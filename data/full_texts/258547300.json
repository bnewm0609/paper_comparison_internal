{"id": 258547300, "updated": "2023-10-05 01:11:47.202", "metadata": {"title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning", "authors": "[{\"first\":\"Bo\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yuanhan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Liangyu\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jinghao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jingkang\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Ziwei\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1$\\times$ A100 GPU to 4$\\times$ RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.03726", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-03726", "doi": "10.48550/arxiv.2305.03726"}}, "content": {"source": {"pdf_hash": "d6d3604f369bb0415cbe814e43ca3131323b03e2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.03726v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4a07cf4f183fb3bc78853ad0a80c0d2003bc9f73", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d6d3604f369bb0415cbe814e43ca3131323b03e2.txt", "contents": "\nOtter: A Multi-Modal Model with In-Context Instruction Tuning\n\n\nBo Li \nS-Lab\nNanyang Technological University\nSingapore\n\nYuanhan Zhang yuanhan002@ntu.edu.sg \nS-Lab\nNanyang Technological University\nSingapore\n\nLiangyu Chen \nS-Lab\nNanyang Technological University\nSingapore\n\nJinghao Wang \nS-Lab\nNanyang Technological University\nSingapore\n\nJingkang Yang jingkang001@ntu.edu.sg \nS-Lab\nNanyang Technological University\nSingapore\n\nZiwei Liu ziwei.liu@ntu.edu.sg \nS-Lab\nNanyang Technological University\nSingapore\n\nOtter: A Multi-Modal Model with In-Context Instruction Tuning\n\nLarge language models (LLMs) have demonstrated significant universal capabilities as few/zero-shot learners in various tasks due to their pre-training on vast amounts of text data, as exemplified by GPT-3, which boosted to InstrctGPT and ChatGPT, effectively following natural language instructions to accomplish real-world tasks. In this paper, we propose to introduce instruction tuning into multi-modal models, motivated by the Flamingo model's upstream interleaved format pretraining dataset. We adopt a similar approach to construct our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. We then introduce Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning. We also optimize OpenFlamingo's implementation for researchers, democratizing the required training resources from 1\u00d7 A100 GPU to 4\u00d7 RTX-3090 GPUs, and integrate both OpenFlamingo and Otter into Huggingface Transformers for more researchers to incorporate the models into their customized training and inference pipelines.Recent studies have highlighted the importance of instruction tuning in empowering LLMs, as exemplified by the boosting of GPT-3 [6] to InstrctGPT [22]  and ChatGPT[20], which follows natural language instructions effectively to accomplish real-world tasks and allows for customizing task-specific rules into instructions during downstream fine-tuning, enabling pre-trained models to comprehend user intents more effectively and produce accurate and relevant responses. Similar attempts have been introduced in multi-modal models as well. LLaMA-Adapter [38] aims to adapt LLaMA [33] into an instruction following model by adding additional adapter modules and multi-modal prompts. Mini-GPT4 [39] follows the architecture of BLIP-2 [15] but replaces the language decoder with Vicuna [9], which supports longer answers. LLaVA [17] utilizes the same CLIP [23] vision encoder and Vicuna [9] language decoder, and finetunes on their high-quality instruction dataset, curated by .Although these works have achieved excellent results and provided valuable insights, they share a minor common issue. Specifically, they either finetune the entire model or the connection part on * Equal Contribution Corresponding Author\n\nIntroduction & Motivation\n\nLarge language models (LLMs) have demonstrated significant universal capabilities in performing various tasks as few/zero-shot learners. These models are pre-trained on vast amounts of text data and have been showcased in recent research, such as GPT-2 [25] and GPT-3 [6]. task-specific data. For instance, a common practice is to use image-text data pairs from Caption [16] or VQA [11] tasks to align visual and language modules. While embedding visual information into the language model in this way can be effective, we question whether this practice is inherently task-dependent, as it relies on the task for which the data is used to train the alignment module.\n\nUpon reflection, we have discovered that DeepMind Flamingo's [1] upstream pretraining dataset, MultiModal MassiveWeb (M3W), has significant value in aligning visual and language information in a more natural manner. The dataset comprises HTML webpages, where all images and texts are arranged in an interleaved format. Specifically, a piece of text may describe an image (or videos) above or below it, and correlations may exist between images (or videos) and text in adjacent positions. This natural organization of context provides richer information than a caption dataset, where text only describes its corresponding image. Trained on this dataset, Flamingo achieves zeroand few-shot generalization and in-context learning ability, making it the GPT-3 moment in the multi-modal domain.\n\nHowever, DeepMind has not released the Flamingo model and its M3W dataset to the public, potentially because of the model's exceptional performance that could cause astonishment prematurely. Nevertheless, the LAION-AI's OpenFlamingo project [4] has recently been made public, providing access to their corresponding MMC4 [40] dataset in the same interleaved format on a larger scale. Consequently, community researchers can continue to follow the Flamingo series of works for further research and development in the field of multi-modal models. Although the OpenFlamingo model exhibits impressive multi-modal in-context learning abilities and executes tasks with given in-context examples, as an upstream pre-trained model, it still requires instruction tuning to perform downstream tasks more effectively.\n\nIn our paper, we propose our MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset and provide details on its construction in Sec. 3.1. We then introduce Otter, a multi-modal model with in-context instruction tuning based on OpenFlamingo. We illustrate the relationship between Otter and OpenFlamingo in Fig. 1 With the recent success of ChatGPT [20], GPT-4 [19], and other large language models [33,32,9], recent studies start to explore incorporating information from other modalities based on pretrained language models. These studies extend the capabilities of language models to more tasks and modalities, and can be categorized into two perspectives:\n\nSystem Design Perspective. This perspective involves using ChatGPT [20] as a dispatch scheduler and connecting different expert models through it to allow for different visual tasks. Language prompts serve as an interface to call expert visual-language models within their respective task domains. Works in this category include VisualChatGPT [35], HuggingGPT [29], Cola [8], X-GPT [42], MM-REACT [37], and ViperGPT [31]. This approach has limitations in that each model cannot be trained individually on new tasks, and using ChatGPT [20] as a powerful instruction dispatch tool can result in high API query costs.\n\nEnd-to-End Trainable Models Perspective. This perspective focuses on connecting models from different modalities into integrated end-to-end trainable models, also known as multi-modal foundation models. Early works in this field include Flamingo [1], which proposes a unified architecture for modeling language and vision and was later open-sourced as OpenFlamingo [4] by LAION-AI. Other earlier works include BLIP-2 [15], which uses a lightweight Querying Transformer and two-stage bootstrap pretraining to connect information from the image to text modality. With the popularity of GPT-4 [19], there has been an increased focus on this field since 2023. Enterpriselevel product models include OpenAI's yet-to-be-released vision-language version of GPT-4 [19], Google's PaLM-E [10], Baidu's ERNIE [5], Alibaba's Tongyi Qianwen [2], and Sensetime's SenseNova [27]. Academic multi-modal efforts include a variety of models such as LLaMA-Adapters [38], Mini-GPT4 [39], and LLaVA [17]. LLaMA-Adapters aims to adapt LLaMA [33] into an instructionfollowing model with an additional adapters module and multi-modal prompts. Mini-GPT4 follows BLIP-2's [15] architecture but replaces the language decoder with Vicuna [9], which better supports longer responses and multi-round conversations. LLaVA connects text and image modalities through a trainable projector matrix, which is a simple lightweight linear layer. However, since LLaVA trains both the vision encoder and language decoder on their instructing tuning dataset, its cost is relatively high compared to others. In contrast, based on the Flamingo model, Otter trains a few cross-gated attention layers to connect visual and language information and establish attention between in-context examples, leaving the vision encoder and language decoder frozen.\n\n\nMulti-modal Instruction Tuning Dataset\n\nThe concept of instruction tuning in multi-modal models was first introduced in Multi-Instruct [36], where 47 diverse multi-modal tasks covering 11 broad categories were organized. Each task comprises at least 5,000 instances (input-output pairs) from existing open-source datasets and 5 expert-written instructions. Multi-Instruct covers most multi-modal tasks that require visual understanding and multi-modal reasoning, such as Visual Question Answering [11,41,30], Image Captioning [16], Image Generation [7], and Visual Relationship Understanding [14], among others. Similarly, Mini-GPT4 [39] constructs its instruction following dataset by combining Conceptual Caption [28,7], SBU [21], and LAION [26] with handwritten instruction templates. More recently, LLaVA [17] has brought the quality of an instruction tuning dataset to a higher level, as it was obtained by expanding the original captions of COCO [16] images with handwritten seed instructions using GPT-4 [19] to provide more detailed descriptions and multi-round conversations.\n\nTo the best of our knowledge, the above-mentioned works are the only few that considered instruction tuning in multi-modal models. Our approach further differs from them in that we incorporate incontext examples into instruction tuning by grouping multiple similar instructions together to form a contextual example set. We are the first to propose the in-context instruction tuning paradigm in multi-modal models and to build the corresponding multi-modal in-context instruction tuning datasets.\n\n\nMethod\n\nIn this section, we will introduce the details of the MIMIC-IT dataset in Sec. 3.1, our Otter's training details in Sec. 3.2, and the integration with Hugging Fance ecosystem in Sec. 3.3.\n\n\nMulti-Modal In-Context Instruction Tuning\n\nThe OpenFlamingo framework leverages the interleaved multi-modal MMC4 dataset to emerge in its few-shot, in-context learning capabilities. The MMC4 dataset is composed of image-text pairs derived from individual HTML files, with significant contextual relationships between different pairs, as depicted in Fig 2(a). An MMC4 training data sample contains (i) a queried image-text pair, where the text typically describes the image, and (ii) context, which includes the remaining image-text pairs from the same HTML file. The primary training objective of OpenFlamingo is to generate text for the queried image-text pair, and the paradigm of generating query text conditioned on in-context examples ensures OpenFlamingo's in-context learning capacity during the inference phase.\n\nOur Multi-Modal In-Context Instruction Tuning (MIMIC-IT) dataset aims to augment Open-Flamingo's instruction comprehension capabilities while preserving its in-context learning capacity.\n\nTo unleash OpenFlamingo's instruction-following potential, we compile data from visual-language tasks into image-instruction-answer triplets. Concurrently, to maintain OpenFlamingo's in-context learning capacity, we retrieve in-context examples for each triplet, which often lack correlated context, such as a visual question-answer data sample in VQAv2 [3]. Specifically, each MIMIC-IT data sample consists of (i) a queried image-instruction-answer triplet, with the instruction-answer tailored to the image, and (ii) context. The context contains a series of image-instruction-answer triplets that contextually correlate with the queried triplet, emulating the relationship between the context and the queried image-text pair found in the MMC4 dataset. The training objective for MIMIC-IT is to generate the answer within the queried image-instruction-answer triplet. The image-instruction-answer triplets are derived from (i) visual question-answer datasets, namely, VQAv2 [3] and GQA [13], (ii) visual instruction datasets, such as LLaVA [17], (iii) an in-progress, high-quality panoptic video scene graph dataset from the PVSG repository. For each video, we select 4-8 frames for instruction-following annotation, using the LLaVA dataset as a reference. We have developed three heuristics to construct the context for each image-instruction-answer triplet, as illustrated in Fig 2(b).\n\n\nTraining Details\n\nOur approach adopts the OpenFlamingo training paradigm to train the Otter model. The pretrained OpenFlamingo model comprises a LLaMA-7B [33] language encoder and a CLIP ViT-L/14 [24] vision encoder. To prevent overfitting and leverage pretrained knowledge, we freeze both the encoders and only finetune the Perceiver resampler module, cross-attention layers inserted into the language encoder and input/output embeddings of the language encoder. This results in approximately 1.3 billion trainable parameters for the Otter model.\n\nTo optimize our model, we employ the AdamW optimizer [18] with a starting learning rate of 10 \u22125 and a batch size of 4. We train Otter for 6 epochs, with the learning rate scheduled using a cosine annealing scheduler. We also use gradient clipping of a threshold of 1.0 to prevent exploding gradients.\n\nDuring our training, we follow a specific format to prepare our training data. The format includes a combination of image, user instruction, \"GPT\"-generated answers 1 , and a special token known as the  \n\n\nIntegratation with Hugging Face\n\nWe have integrated Otter into Hugging Face Transformers [34] and trained it using the Hugging Face Accelerator 2 , which enables automatic mapping of the model weights to different GPU devices and offloading of overflowed weights to CPU or disk. Additionally, we use bf16 mixed precision during training. The total optimizations enable our model to be trained on 4\u00d7RTX-3090 GPUs, each with 24GB memory.\n\nMeanwhile, since Otter has been integrated into Hugging Face Transformers, it can now be reused with less than five lines of code, making it much easier for researchers to integrate into their respective training and inference pipelines (compared to the original OpenFlamingo implementation). We also provide the support of Fully Sharded Data Parallel (FSDP) and DeepSpeed to enable greater training efficiency and less memory consumption.\n\nTo enable future research and convenience, we also provide a script for converting the original OpenFlamingo-9B checkpoint into the Hugging Face Model format. The converted checkpoint and our Otter model are uploaded and available on the Hugging Face model hub luodian/ openflamingo-9b-hf and luodian/otter-9b-hf, respectively.\n\n\nDemonstrations\n\nIn \n\n\nFollowing User Instructions\n\nIn Sec. 3.1, we discussed how we finetuned Otter on visual instruction pairs to transform it into a powerful instruction follower. The results of our experiments are demonstrated in Fig. 3, where we observe that Otter is able to provide more detailed descriptions of images and follow user instructions more accurately. This characteristic of Otter is attributed to the co-design of our model and data, which leverages the generalization ability of a strong language decoder and the rich variety of instructions present in the MIMIC-IT dataset. By fine-tuning on visual instruction pairs, Otter is able to learn the nuances of human language and accurately apply it to visual input.\n\nIn order to evaluate Otter's reasoning capabilities, we designed a series of experiments to test its ability to handle complex scenarios that require a deeper understanding of the situation and more advanced commonsense reasoning. In Fig. 4, we present an example where OpenFlamingo fails to identify important clues in the image, while Otter is able to demonstrate a deeper understanding of the scene and apply relevant commonsense knowledge to answer the given question. In this particular example, Otter is able to correctly interpret the confusion around the traffic signal, while also taking into account relevant contextual factors such as accidents, traffic congestion, and disruptions to the flow of traffic. This level of reasoning ability is not present in standard image captioning models and represents a significant advance in the field of computer vision and natural language processing.\n\n\nMulti-Modal In-context Learning\n\nOtter is designed to support multi-modal in-context learning in a similar pattern to Flamingo [1] and OpenFlamingo [4], which involves conditioning the language model on the corresponding media, such as an image that corresponds to a caption or a question-answer pair. In Fig. 5 Despite the fact that user inputs are often short and restricted in these scenarios, Otter is able to provide more comprehensive answers. By leveraging the information contained in the images and other contextual examples, Otter is able to better understand the underlying meaning and context of the questions, resulting in more accurate and informative answers.\n\nInstruction: Write a detailed description of the given image?\n\nOpenFlamingo: Pancakes with bacon and maple syrup.\n\nOtter: The image features a wooden table with various items on it.\n\nThere is a sandwhich, which has been cut in half, and a bowl of cereal placed near the plate. Additionally, there is a cup, likely containing a beverage such as coffee. The table also has a \"\"live baccy\"\" container and a mug present, suggesting a cozy and relaxed setting. The scene might be a part of a breakfast or snack time arrangement, with the sandwich, cereals, and beverage as the main items on the table.\n\nInstruction: Can you describe the main features of this image for me?\n\nOpenFlamingo: The man is flying a kite.\n\nOtter: The image shows a person standing in front of a kite that is flying in the sky. The person is wearing a black shirt and is positioned on the left side of the kite. The kite is quite large and can be seen above the person's head. The scene appears to be outdoors, with the person possibly preparing to launch the kites or enjoying the moment as the kit flies in the breeze. There are also two other people in the background, one on the right side and another person partially visible on the far right. The presence of multiple people in this outdoor setting suggests that it might be a gathering or a popular location for kite flying and leisure activities. Instruction: What could happen if drivers become confused by the traffic signals at this intersection?\n\nOpenFlamingo: They could get stuck in the rain.\n\nOtter: If drivers become confused by the traffic signals at the intersection, it could lead to various potential scenarios: 1. Red light: The red light signal is on, and the car is moving. In this case, the driver might continue driving, potentially causing a redlight runner accident, which can be dangerous for pedestrians and other vehicles. 2. Green light: If the green light signal appears while the car has already started moving, the vehicle might continue moving, potentially leading to a green-light running accident, where the driver does not stop at the green signal and risks colliding with other vehicles or pedestrian crossings. 3. Confusion about the signal: If drivers are unsure about the traffic lights, they might hesitate or make an unpredictable maneuverer, which could lead not only to accidents but also to traffic congestion and general disruption of the flow of traffic.  \n\n\nUser provided examples\n\nInstruction: what does this meme usually use for? Otter: This meme typically uses a sad-looking baby as the main image, with text encouraging people to stop being a baby and to be more like a tough, cool individual. The image and text are meant to create a playful and humorous contrast between a cute, vulnerable baby and a tuff, strong person.\n\n\nOtter reaction\n\nInstruction: What is the danger of this sport? Answer: The player may get hitted by the tennis ball.\n\nInstruction: What is the danger of this sport? Answer: While chasing the baseball, the player may inadvertently collide with other players.\n\n\nUser provided examples\n\nInstruction: What is the danger of this sport? Otter: The sport involves players running and trying to catch the ball while others are standing in the grass, which can lead to collisions or accidents. These collisions can result in injuries to the players involved. It is essential for players to be aware of their surroundings and maintain a safe distance from each other to minimize the risk of accidents during the game. \n\n\nOtter reaction\n\n\nConclusion\n\nIn this work, we propose Otter, a multi-modal in-context learning foundation model with instruction tuning. Through partial finetuning on MIMIC-IT dataset, we observe that Otter can convert OpenFlamingo into a zero-shot visual instruction model with strong in-context learning abilities. With the assistance of rich instructions from images and videos, Otter generalizes to achieve better instruction-following and situation-understanding performances.\n\n\nLimitations\n\nLanguage Hallucination. Since Otter is built upon OpenFlamingo, which depends upon LLaMA, the hallucination issue of LLaMA is inherited by Otter. Also, the current Otter model may hallucinate the language that is not related to the image. This issue might be solved by introducing negative examples in the training data.\n\n\nFuture Supports\n\nIn the future, we plan to explore the integration of more efficient training schemas (e.g., parameterefficient finetuning such as LoRA [12]) and more modalities (e.g., 3D vision).\n\nFigure 1 :\n1Otter Overview. Otter is a multi-modal model finetuned on our proposed MIMIC-IT dataset, based on OpenFlamingo. Otter model exhibits the improved ability to execute tasks by following given instructions and leveraging in-context examples.\n\n( a )Figure 2 :\na2Multimodal C4 (MMC4) Title: WALNUT AND BLUE CHEESE STUFFED MUSHROOMS Text: \"When you lock the door using the lock tab on the driver's door, all of the other doors and tailgate lock at the same time.Pine Nut Stuffed Mushrooms.\", Text: \"When you lock/unlock the driver's door and tailgate using the master lock switch, all the other doors lock/ unlock at the same time.\" Title: HONDA: (UN)LOKING THE DOORS FROM THE INSIDE Text: \"This Walnut and Blue Cheese Stuffed Mushrooms recipe is sponsored by Fisher Nuts.\", (b) Multi-Modal In-Context Instruction Tuning (MIMIC-IT) Illustration of example data formats in MMC4 and MIMIC-IT. (a) The illustration of the data format in the MMC4 dataset that are used OpenFlamingo. (b) Three heuristics to build the multi-modal in-Context instruction tuning (MIMIC-IT) dataset.\n\n\nthis section, we show several examples of two types of demonstrations of Otter: (1) the ability to follow instructions in Sec. 4.1, and (2) the ability of learning to execute new instructions following provided in-context examples in Sec. 4.2. Compared with OpenFlamingo, these results demonstrate the importance of in-context instruction tuning and the improvement of Otter.\n\nFigure 3 :\n3Comparisons of image captioning results between OpenFlamingo and Otter. Otter provides more detailed and precise descriptions of the images.\n\nFigure 4 :\n4Comparisons of situation understanding results between OpenFlamingo and Otter.Otter is able to demonstrate a deeper understanding of the scene and apply relevant commonsense knowledge to answer the given question.\n\nFigure 5 :\n5In-context learning examples of Otter. Given two examples of instruction-answering pairs and an image, Otter is able to provide more comprehensive answers.\n\n\n. Finetuned on MIMIC-IT dataset, our Otter model demonstrates improved instruction-following ability compared to OpenFlamingo, as shown in our qualitative analysis in Sec. 4.1. Meanwhile, Otter is capable of learning to execute instructions with provided in-context learning examples, as shown in Sec. 4.2. From the engineering perspective, we optimized OpenFlamingo's implementation to make it more accessible to researchers. Our optimizations include optimizing the training requirements from at least 1\u00d7 A100 GPU to only 4\u00d7 RTX3090 GPUs and integrating it into Hugging Face Transformers[34] to simplify training and inference with a few lines of code. Our contributions facilitate further research and development in the field of multi-modal models.We summarize our key contributions as follows:\u2022 We present the MultI-Modal In-Context Instruction Tuning (MIMIC-IT) dataset. Each data sample includes an instruction-image-answer triplet and its in-context examples.\u2022 We introduce Otter, a multi-modal model with in-context instruction tuning based on \nOpenFlamingo, capable of the instruction following and executing new instructions with \nfew in-context learning examples. \n\n\u2022 Our optimizations to OpenFlamingo's implementation, including reducing the training \nrequirements to 4\u00d7 RTX3090 GPUs and integrating it into Huggingface Transformers to \nsimplify the training and inference with only a few lines of code. \n2 Related Work \n\n2.1 Large-scale Multi-modal Models \n\n\n\n\n[endofchunk] token. We format the training data as follows: <context> [image] User:<instruction> GPT:[answers] <answer>.[endofchunk] where the [image], [answer], and [endofchunk] tokens are unique and serve a specific purpose. We design such a chatbot-like format to train our model to improve the instruction-following and conversation generalizability of the model. The [image] and [endofchunk] tokens are originally from the OpenFlamingo training paradigm, while the [answer] token is a new introduction by us in training Otter. The [answer]token separates the answers from the instruction, so that, we mask all tokens after the [answer] token during training and set them as the prediction objectives of the model. We train our model using a cross-entropy loss.Instruction: \nWhat is the \nbaseball player \nholding in his \nhands? \n\nInstruction: \nWhat color is \nthe baseball \nplayer's \nuniform? \n\nInstruction: \nWhich team does \nthe baseball \nplayer belong to? \n\nAnswer: The \nbaseball player is \nholding a baseball \nbat in his hands. \n\nAnswer: The \nbaseball \nplayer's uniform \nis grey. \n\nAnswer: The \nbaseball player \nbelongs to the \nAngels baseball \nteam. \n\nin-context examples \nquery \n\n\u2026 \n\nsame image, different instructions \n\nInstruction: What \nskill set do \nsnowboarders need \nto perform such \ntricks? \n\nInstruction: \nWhat skill set \ndoes this player \nneed to have\\in a \nmatch, and why? \n\nInstruction: \nWhat skill set \ndoes a player \nneed to excel in \nthis sport, \nbased on the \nimage? \n\nAnswer: To \nperform tricks \u2026 \nminimize the \nrisks involved \n\nAnswer: A tennis \nplayer \nneeds \u2026winning \nmatches. \n\nAnswer: To \nimprove his \nperformance, \u2026his \nteam. \n\nin-context examples \nquery \n\n\u2026 \n\nsame instruction, different images \n\nInstruction: What is \nthe main thing \nhappening in this \npicture? \n\nInstruction: Why did \nthe player in red who \nwas attacking fall to \nthe ground? \n\nInstruction: Why is \nthe man in the red \njersey about to stand \nup from the ground? \n\nAnswer: A group \nof \u2026attack and look \nfor an opportunity to \nshoot. \n\nAnswer: Because \nhe ..the ground to \ntry to create a \npenalty. \n\nAnswer: Because \u2026the \nball go in and didn't \nneed to pretend to \ncreate a penalty. \n\nin-context examples \nquery \n\nsequential images, different instructions \n\nInstruction: \nDescription of the \nvideos humorous \nmoment? \n\nAnswer: A man on the \npitch falls down \nafter a shot, \u2026 \\with \nhis teammates. \n\nInstruction: Why is \nthe whole video \nhumorous? \n\nAnswer: The funny \nthing \u2026 it's funny \nhow his injury seems \nto heal in an \ninstant. \n\n\n\n\nInstruction: what does this meme usually use for? Answer: to ask people to think twice and dont be silly Instruction: what does this meme usually use for? Answer: to express users feeling of sad, or moving.\nTo support user-assistant conversations, we adopt \"GPT\" as the role label because it does not have any specific semantic meaning in vocabulary.\nHugging Face Accelerator\nAcknowledgement. We thank Chunyuan Li and Jack Hessel for their advice and support, as well as the OpenFlamingo team for their great contribution to the open-source community.\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katherine Mensch, Malcolm Millican, Reynolds, Advances in Neural Information Processing Systems. 356Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716-23736, 2022. 2, 3, 6\n\n. Alibaba, Tongyi, qianwen. 2023. 3Alibaba. Tongyi qianwen. 2023. 3\n\nVqa: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015. 4\n\n. Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, Ludwig Schmidt, Openflamingo, 6Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 2, 3, 6\n\nEnhanced representation through knowledge integration. Ernie Baidu, Bot, 2023Baidu. Ernie bot: Enhanced representation through knowledge integration. 2023. 3\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. 1\n\nConceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558-3568, 2021. 3\n\nLanguage models are visual reasoning coordinators. Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, Ziwei Liu, ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models. Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Ziwei Liu. Language models are visual reasoning coordinators. In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2023. 3\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, Eric P Xing, 13Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 1, 3\n\nPalm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, arXiv:2303.03378arXiv preprintDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 3\n\nMaking the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition23Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913, 2017. 2, 3\n\nJ Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 8\n\nGqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionDrew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700-6709, 2019. 4\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1233Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32-73, 2017. 3\n\nBlip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXiv:2301.1259713arXiv preprintJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 3\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringer23Proceedings, Part V 13Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014. 2, 3\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 14arXiv preprintHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 1, 3, 4\n\n. Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. arXiv preprintIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 4\n\nGpt-4 technical report. Openai, 13OpenAI. Gpt-4 technical report. 2023. 1, 3\n\n. OpenAI. Introducing chatgpt. 2023. 13OpenAI. Introducing chatgpt. 2023. 1, 3\n\nIm2text: Describing images using 1 million captioned photographs. Vicente Ordonez, Girish Kulkarni, Tamara Berg, Advances in neural information processing systems. 24Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011. 3\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022. 1\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLR, 2021. 1International conference on machine learning. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 1\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLR, 2021. 4International conference on machine learning. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748-8763. PMLR, 2021. 4\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 18Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1\n\nLaion-400m: Open dataset of clip-filtered 400 million image-text pairs. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114arXiv preprintChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 3\n\n. Sensetime, nova. 2023. 3SenseTime. Sense nova. 2023. 3\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, 2018. 3\n\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.17580arXiv preprintYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 3\n\nA corpus of natural language for visual reasoning. Alane Suhr, Mike Lewis, James Yeh, Yoav Artzi, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsShort Papers2Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217-223, 2017. 3\n\nD\u00eddac Sur\u00eds, Sachit Menon, Carl Vondrick, Vipergpt, arXiv:2303.08128Visual inference via python execution for reasoning. arXiv preprintD\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023. 3\n\nStanford alpaca: An instruction-following llama model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 3\n\nLlama: Open and efficient foundation language models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, arXiv:2302.1397114arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- th\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 3, 4\n\nTransformers: State-of-theart natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnlineAssociation for Computational Linguistics26Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics. 2, 6\n\nVisual chatgpt: Talking, drawing and editing with visual foundation models. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan, arXiv:2303.04671arXiv preprintChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 3\n\nMultiinstruct: Improving multi-modal zero-shot learning via instruction tuning. Zhiyang Xu, Ying Shen, Lifu Huang, arXiv:2212.107732022arXiv preprintZhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022. 3\n\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, arXiv:2303.11381Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprintZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 3\n\nLlama-adapter: Efficient fine-tuning of language models with zero-init attention. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Yu Qiao, arXiv:2303.1619913arXiv preprintRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 1, 3\n\nMinigpt-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, arXiv:2304.1059213arXiv preprintDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En- hancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 3\n\nWanrong Zhu, Jack Hessel, Anas Awadalla, Jesse Samir Yitzhak Gadre, Alex Dodge, Youngjae Fang, Ludwig Yu, William Yang Schmidt, Yejin Wang, Choi, arXiv:2304.06939Multimodal C4: An open, billion-scale corpus of images interleaved with text. arXiv preprintWanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023. 2\n\nVisual7w: Grounded question answering in images. Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4995-5004, 2016. 3\n\nXueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, arXiv:2212.11270Generalized decoding for pixel, image, and language. 2022arXiv preprintXueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. arXiv preprint arXiv:2212.11270, 2022. 3\n", "annotations": {"author": "[{\"end\":121,\"start\":65},{\"end\":208,\"start\":122},{\"end\":272,\"start\":209},{\"end\":336,\"start\":273},{\"end\":424,\"start\":337},{\"end\":506,\"start\":425}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":68},{\"end\":135,\"start\":130},{\"end\":221,\"start\":217},{\"end\":285,\"start\":281},{\"end\":350,\"start\":346},{\"end\":434,\"start\":431}]", "author_first_name": "[{\"end\":67,\"start\":65},{\"end\":129,\"start\":122},{\"end\":216,\"start\":209},{\"end\":280,\"start\":273},{\"end\":345,\"start\":337},{\"end\":430,\"start\":425}]", "author_affiliation": "[{\"end\":120,\"start\":72},{\"end\":207,\"start\":159},{\"end\":271,\"start\":223},{\"end\":335,\"start\":287},{\"end\":423,\"start\":375},{\"end\":505,\"start\":457}]", "title": "[{\"end\":62,\"start\":1},{\"end\":568,\"start\":507}]", "venue": null, "abstract": "[{\"end\":2920,\"start\":570}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3206,\"start\":3202},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3220,\"start\":3217},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3323,\"start\":3319},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3335,\"start\":3331},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3681,\"start\":3678},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4652,\"start\":4649},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4733,\"start\":4729},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5573,\"start\":5569},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5585,\"start\":5581},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5623,\"start\":5619},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5626,\"start\":5623},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5628,\"start\":5626},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5952,\"start\":5948},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6228,\"start\":6224},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6245,\"start\":6241},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6255,\"start\":6252},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6267,\"start\":6263},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6282,\"start\":6278},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6301,\"start\":6297},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6419,\"start\":6415},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6746,\"start\":6743},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6865,\"start\":6862},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6918,\"start\":6914},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7091,\"start\":7087},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7257,\"start\":7253},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7298,\"start\":7295},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7328,\"start\":7325},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7360,\"start\":7356},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7446,\"start\":7442},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7462,\"start\":7458},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7478,\"start\":7474},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7519,\"start\":7515},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7646,\"start\":7642},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7709,\"start\":7706},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8445,\"start\":8441},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8807,\"start\":8803},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8810,\"start\":8807},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8813,\"start\":8810},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8836,\"start\":8832},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8858,\"start\":8855},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8902,\"start\":8898},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8943,\"start\":8939},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9025,\"start\":9021},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9027,\"start\":9025},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9037,\"start\":9033},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9053,\"start\":9049},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9119,\"start\":9115},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9262,\"start\":9258},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9321,\"start\":9317},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11455,\"start\":11452},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12077,\"start\":12074},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12090,\"start\":12086},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12144,\"start\":12140},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12648,\"start\":12644},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12690,\"start\":12686},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13096,\"start\":13092},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13641,\"start\":13637},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16525,\"start\":16522},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16546,\"start\":16543},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21557,\"start\":21553},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24203,\"start\":24199}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":21849,\"start\":21598},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22679,\"start\":21850},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23057,\"start\":22680},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23211,\"start\":23058},{\"attributes\":{\"id\":\"fig_4\"},\"end\":23438,\"start\":23212},{\"attributes\":{\"id\":\"fig_5\"},\"end\":23607,\"start\":23439},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":25082,\"start\":23608},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":27620,\"start\":25083},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27829,\"start\":27621}]", "paragraph": "[{\"end\":3615,\"start\":2949},{\"end\":4406,\"start\":3617},{\"end\":5214,\"start\":4408},{\"end\":5879,\"start\":5216},{\"end\":6495,\"start\":5881},{\"end\":8303,\"start\":6497},{\"end\":9390,\"start\":8346},{\"end\":9888,\"start\":9392},{\"end\":10086,\"start\":9899},{\"end\":10908,\"start\":10132},{\"end\":11096,\"start\":10910},{\"end\":12487,\"start\":11098},{\"end\":13037,\"start\":12508},{\"end\":13340,\"start\":13039},{\"end\":13545,\"start\":13342},{\"end\":13983,\"start\":13581},{\"end\":14424,\"start\":13985},{\"end\":14753,\"start\":14426},{\"end\":14775,\"start\":14772},{\"end\":15489,\"start\":14807},{\"end\":16392,\"start\":15491},{\"end\":17069,\"start\":16428},{\"end\":17132,\"start\":17071},{\"end\":17184,\"start\":17134},{\"end\":17252,\"start\":17186},{\"end\":17667,\"start\":17254},{\"end\":17738,\"start\":17669},{\"end\":17779,\"start\":17740},{\"end\":18547,\"start\":17781},{\"end\":18596,\"start\":18549},{\"end\":19495,\"start\":18598},{\"end\":19867,\"start\":19522},{\"end\":19986,\"start\":19886},{\"end\":20127,\"start\":19988},{\"end\":20578,\"start\":20154},{\"end\":21062,\"start\":20610},{\"end\":21398,\"start\":21078},{\"end\":21597,\"start\":21418}]", "formula": null, "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2947,\"start\":2922},{\"attributes\":{\"n\":\"2.2\"},\"end\":8344,\"start\":8306},{\"attributes\":{\"n\":\"3\"},\"end\":9897,\"start\":9891},{\"attributes\":{\"n\":\"3.1\"},\"end\":10130,\"start\":10089},{\"attributes\":{\"n\":\"3.2\"},\"end\":12506,\"start\":12490},{\"attributes\":{\"n\":\"3.3\"},\"end\":13579,\"start\":13548},{\"attributes\":{\"n\":\"4\"},\"end\":14770,\"start\":14756},{\"attributes\":{\"n\":\"4.1\"},\"end\":14805,\"start\":14778},{\"attributes\":{\"n\":\"4.2\"},\"end\":16426,\"start\":16395},{\"end\":19520,\"start\":19498},{\"end\":19884,\"start\":19870},{\"end\":20152,\"start\":20130},{\"end\":20595,\"start\":20581},{\"attributes\":{\"n\":\"5\"},\"end\":20608,\"start\":20598},{\"attributes\":{\"n\":\"5.1\"},\"end\":21076,\"start\":21065},{\"attributes\":{\"n\":\"5.2\"},\"end\":21416,\"start\":21401},{\"end\":21609,\"start\":21599},{\"end\":21866,\"start\":21851},{\"end\":23069,\"start\":23059},{\"end\":23223,\"start\":23213},{\"end\":23450,\"start\":23440}]", "table": "[{\"end\":25082,\"start\":24577},{\"end\":27620,\"start\":25850}]", "figure_caption": "[{\"end\":21849,\"start\":21611},{\"end\":22679,\"start\":21869},{\"end\":23057,\"start\":22682},{\"end\":23211,\"start\":23071},{\"end\":23438,\"start\":23225},{\"end\":23607,\"start\":23452},{\"end\":24577,\"start\":23610},{\"end\":25850,\"start\":25085},{\"end\":27829,\"start\":27623}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5533,\"start\":5527},{\"end\":10446,\"start\":10438},{\"end\":12486,\"start\":12478},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14995,\"start\":14989},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15731,\"start\":15725},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":16706,\"start\":16700}]", "bib_author_first_name": "[{\"end\":28236,\"start\":28232},{\"end\":28267,\"start\":28260},{\"end\":28284,\"start\":28277},{\"end\":28294,\"start\":28290},{\"end\":28306,\"start\":28302},{\"end\":28318,\"start\":28313},{\"end\":28333,\"start\":28327},{\"end\":28349,\"start\":28340},{\"end\":28365,\"start\":28358},{\"end\":28850,\"start\":28841},{\"end\":28867,\"start\":28858},{\"end\":28883,\"start\":28877},{\"end\":28896,\"start\":28888},{\"end\":28912,\"start\":28907},{\"end\":28928,\"start\":28920},{\"end\":28942,\"start\":28938},{\"end\":29324,\"start\":29320},{\"end\":29340,\"start\":29335},{\"end\":29352,\"start\":29346},{\"end\":29366,\"start\":29362},{\"end\":29380,\"start\":29375},{\"end\":29396,\"start\":29389},{\"end\":29409,\"start\":29402},{\"end\":29426,\"start\":29419},{\"end\":29440,\"start\":29435},{\"end\":29453,\"start\":29448},{\"end\":29467,\"start\":29462},{\"end\":29483,\"start\":29479},{\"end\":29487,\"start\":29484},{\"end\":29500,\"start\":29493},{\"end\":29518,\"start\":29510},{\"end\":29535,\"start\":29529},{\"end\":29884,\"start\":29879},{\"end\":30025,\"start\":30022},{\"end\":30041,\"start\":30033},{\"end\":30052,\"start\":30048},{\"end\":30067,\"start\":30060},{\"end\":30082,\"start\":30077},{\"end\":30084,\"start\":30083},{\"end\":30101,\"start\":30093},{\"end\":30118,\"start\":30112},{\"end\":30138,\"start\":30132},{\"end\":30152,\"start\":30146},{\"end\":30167,\"start\":30161},{\"end\":30608,\"start\":30601},{\"end\":30627,\"start\":30621},{\"end\":30639,\"start\":30636},{\"end\":30650,\"start\":30646},{\"end\":31140,\"start\":31133},{\"end\":31149,\"start\":31147},{\"end\":31159,\"start\":31154},{\"end\":31174,\"start\":31166},{\"end\":31189,\"start\":31181},{\"end\":31198,\"start\":31194},{\"end\":31214,\"start\":31208},{\"end\":31229,\"start\":31224},{\"end\":31656,\"start\":31649},{\"end\":31672,\"start\":31665},{\"end\":31679,\"start\":31677},{\"end\":31689,\"start\":31685},{\"end\":31705,\"start\":31697},{\"end\":31713,\"start\":31710},{\"end\":31728,\"start\":31721},{\"end\":31742,\"start\":31736},{\"end\":31758,\"start\":31751},{\"end\":31773,\"start\":31767},{\"end\":31775,\"start\":31774},{\"end\":31789,\"start\":31786},{\"end\":31802,\"start\":31798},{\"end\":31804,\"start\":31803},{\"end\":32126,\"start\":32121},{\"end\":32138,\"start\":32135},{\"end\":32145,\"start\":32144},{\"end\":32147,\"start\":32146},{\"end\":32160,\"start\":32155},{\"end\":32179,\"start\":32170},{\"end\":32192,\"start\":32187},{\"end\":32210,\"start\":32204},{\"end\":32227,\"start\":32219},{\"end\":32239,\"start\":32235},{\"end\":32255,\"start\":32249},{\"end\":32641,\"start\":32637},{\"end\":32654,\"start\":32649},{\"end\":32668,\"start\":32661},{\"end\":32688,\"start\":32683},{\"end\":32700,\"start\":32696},{\"end\":33140,\"start\":33139},{\"end\":33155,\"start\":33149},{\"end\":33167,\"start\":33160},{\"end\":33180,\"start\":33174},{\"end\":33196,\"start\":33189},{\"end\":33213,\"start\":33208},{\"end\":33220,\"start\":33218},{\"end\":33233,\"start\":33227},{\"end\":33622,\"start\":33621},{\"end\":33642,\"start\":33629},{\"end\":34147,\"start\":34141},{\"end\":34161,\"start\":34157},{\"end\":34173,\"start\":34167},{\"end\":34187,\"start\":34181},{\"end\":34202,\"start\":34197},{\"end\":34215,\"start\":34209},{\"end\":34234,\"start\":34225},{\"end\":34247,\"start\":34241},{\"end\":34266,\"start\":34260},{\"end\":34276,\"start\":34271},{\"end\":34278,\"start\":34277},{\"end\":34747,\"start\":34741},{\"end\":34758,\"start\":34752},{\"end\":34769,\"start\":34763},{\"end\":34786,\"start\":34780},{\"end\":35080,\"start\":35072},{\"end\":35093,\"start\":35086},{\"end\":35106,\"start\":35101},{\"end\":35122,\"start\":35117},{\"end\":35135,\"start\":35129},{\"end\":35148,\"start\":35144},{\"end\":35163,\"start\":35158},{\"end\":35182,\"start\":35172},{\"end\":35630,\"start\":35623},{\"end\":35644,\"start\":35636},{\"end\":35657,\"start\":35649},{\"end\":35666,\"start\":35662},{\"end\":35670,\"start\":35667},{\"end\":35873,\"start\":35869},{\"end\":35891,\"start\":35886},{\"end\":36315,\"start\":36308},{\"end\":36331,\"start\":36325},{\"end\":36348,\"start\":36342},{\"end\":36662,\"start\":36658},{\"end\":36678,\"start\":36671},{\"end\":36685,\"start\":36683},{\"end\":36698,\"start\":36693},{\"end\":36715,\"start\":36708},{\"end\":36734,\"start\":36728},{\"end\":36749,\"start\":36744},{\"end\":36765,\"start\":36757},{\"end\":36783,\"start\":36775},{\"end\":36795,\"start\":36791},{\"end\":37224,\"start\":37220},{\"end\":37238,\"start\":37234},{\"end\":37243,\"start\":37239},{\"end\":37254,\"start\":37249},{\"end\":37270,\"start\":37264},{\"end\":37286,\"start\":37279},{\"end\":37300,\"start\":37292},{\"end\":37316,\"start\":37310},{\"end\":37331,\"start\":37325},{\"end\":37346,\"start\":37340},{\"end\":37360,\"start\":37356},{\"end\":37809,\"start\":37805},{\"end\":37823,\"start\":37819},{\"end\":37828,\"start\":37824},{\"end\":37839,\"start\":37834},{\"end\":37855,\"start\":37849},{\"end\":37871,\"start\":37864},{\"end\":37885,\"start\":37877},{\"end\":37901,\"start\":37895},{\"end\":37916,\"start\":37910},{\"end\":37931,\"start\":37925},{\"end\":37945,\"start\":37941},{\"end\":38376,\"start\":38372},{\"end\":38393,\"start\":38386},{\"end\":38403,\"start\":38398},{\"end\":38416,\"start\":38411},{\"end\":38428,\"start\":38423},{\"end\":38441,\"start\":38437},{\"end\":38720,\"start\":38711},{\"end\":38739,\"start\":38732},{\"end\":38753,\"start\":38747},{\"end\":38770,\"start\":38764},{\"end\":38791,\"start\":38784},{\"end\":38806,\"start\":38800},{\"end\":38818,\"start\":38814},{\"end\":38833,\"start\":38828},{\"end\":38846,\"start\":38842},{\"end\":39320,\"start\":39314},{\"end\":39332,\"start\":39329},{\"end\":39348,\"start\":39339},{\"end\":39362,\"start\":39358},{\"end\":39931,\"start\":39922},{\"end\":39944,\"start\":39938},{\"end\":39953,\"start\":39951},{\"end\":39968,\"start\":39959},{\"end\":39980,\"start\":39973},{\"end\":39992,\"start\":39985},{\"end\":40286,\"start\":40281},{\"end\":40297,\"start\":40293},{\"end\":40310,\"start\":40305},{\"end\":40320,\"start\":40316},{\"end\":40750,\"start\":40745},{\"end\":40764,\"start\":40758},{\"end\":40776,\"start\":40772},{\"end\":41091,\"start\":41086},{\"end\":41105,\"start\":41099},{\"end\":41123,\"start\":41117},{\"end\":41135,\"start\":41131},{\"end\":41151,\"start\":41144},{\"end\":41162,\"start\":41156},{\"end\":41178,\"start\":41173},{\"end\":41195,\"start\":41186},{\"end\":41197,\"start\":41196},{\"end\":41505,\"start\":41501},{\"end\":41522,\"start\":41515},{\"end\":41538,\"start\":41531},{\"end\":41554,\"start\":41548},{\"end\":41575,\"start\":41565},{\"end\":41593,\"start\":41585},{\"end\":41608,\"start\":41603},{\"end\":41631,\"start\":41627},{\"end\":41645,\"start\":41639},{\"end\":41662,\"start\":41654},{\"end\":41676,\"start\":41670},{\"end\":41695,\"start\":41688},{\"end\":41713,\"start\":41704},{\"end\":42162,\"start\":42156},{\"end\":42177,\"start\":42169},{\"end\":42191,\"start\":42185},{\"end\":42204,\"start\":42198},{\"end\":42222,\"start\":42215},{\"end\":42240,\"start\":42233},{\"end\":42253,\"start\":42246},{\"end\":42265,\"start\":42262},{\"end\":42277,\"start\":42273},{\"end\":42290,\"start\":42284},{\"end\":42305,\"start\":42302},{\"end\":42318,\"start\":42315},{\"end\":42334,\"start\":42329},{\"end\":42361,\"start\":42355},{\"end\":42372,\"start\":42366},{\"end\":42388,\"start\":42382},{\"end\":42399,\"start\":42394},{\"end\":42402,\"start\":42400},{\"end\":42414,\"start\":42407},{\"end\":42428,\"start\":42421},{\"end\":42444,\"start\":42437},{\"end\":42461,\"start\":42452},{\"end\":42463,\"start\":42462},{\"end\":43401,\"start\":43394},{\"end\":43415,\"start\":43406},{\"end\":43428,\"start\":43421},{\"end\":43441,\"start\":43433},{\"end\":43455,\"start\":43448},{\"end\":43465,\"start\":43462},{\"end\":43789,\"start\":43782},{\"end\":43798,\"start\":43794},{\"end\":43809,\"start\":43805},{\"end\":44021,\"start\":44012},{\"end\":44034,\"start\":44028},{\"end\":44047,\"start\":44039},{\"end\":44059,\"start\":44054},{\"end\":44070,\"start\":44065},{\"end\":44088,\"start\":44082},{\"end\":44103,\"start\":44096},{\"end\":44111,\"start\":44109},{\"end\":44124,\"start\":44117},{\"end\":44137,\"start\":44131},{\"end\":44570,\"start\":44564},{\"end\":44585,\"start\":44578},{\"end\":44596,\"start\":44591},{\"end\":44611,\"start\":44603},{\"end\":44622,\"start\":44616},{\"end\":44631,\"start\":44628},{\"end\":44645,\"start\":44636},{\"end\":44654,\"start\":44650},{\"end\":44662,\"start\":44660},{\"end\":45030,\"start\":45025},{\"end\":45039,\"start\":45036},{\"end\":45054,\"start\":45046},{\"end\":45066,\"start\":45061},{\"end\":45078,\"start\":45071},{\"end\":45333,\"start\":45326},{\"end\":45343,\"start\":45339},{\"end\":45356,\"start\":45352},{\"end\":45372,\"start\":45367},{\"end\":45398,\"start\":45394},{\"end\":45414,\"start\":45406},{\"end\":45427,\"start\":45421},{\"end\":45439,\"start\":45432},{\"end\":45444,\"start\":45440},{\"end\":45459,\"start\":45454},{\"end\":45903,\"start\":45899},{\"end\":45915,\"start\":45909},{\"end\":45930,\"start\":45923},{\"end\":45944,\"start\":45942},{\"end\":46317,\"start\":46311},{\"end\":46328,\"start\":46323},{\"end\":46341,\"start\":46334},{\"end\":46351,\"start\":46348},{\"end\":46363,\"start\":46357},{\"end\":46376,\"start\":46368},{\"end\":46387,\"start\":46381},{\"end\":46401,\"start\":46393},{\"end\":46416,\"start\":46408},{\"end\":46425,\"start\":46423}]", "bib_author_last_name": "[{\"end\":28258,\"start\":28237},{\"end\":28275,\"start\":28268},{\"end\":28288,\"start\":28285},{\"end\":28300,\"start\":28295},{\"end\":28311,\"start\":28307},{\"end\":28325,\"start\":28319},{\"end\":28338,\"start\":28334},{\"end\":28356,\"start\":28350},{\"end\":28374,\"start\":28366},{\"end\":28384,\"start\":28376},{\"end\":28749,\"start\":28742},{\"end\":28757,\"start\":28751},{\"end\":28856,\"start\":28851},{\"end\":28875,\"start\":28868},{\"end\":28886,\"start\":28884},{\"end\":28905,\"start\":28897},{\"end\":28918,\"start\":28913},{\"end\":28936,\"start\":28929},{\"end\":28949,\"start\":28943},{\"end\":29333,\"start\":29325},{\"end\":29344,\"start\":29341},{\"end\":29360,\"start\":29353},{\"end\":29373,\"start\":29367},{\"end\":29387,\"start\":29381},{\"end\":29400,\"start\":29397},{\"end\":29417,\"start\":29410},{\"end\":29433,\"start\":29427},{\"end\":29446,\"start\":29441},{\"end\":29460,\"start\":29454},{\"end\":29477,\"start\":29468},{\"end\":29491,\"start\":29488},{\"end\":29508,\"start\":29501},{\"end\":29527,\"start\":29519},{\"end\":29543,\"start\":29536},{\"end\":29557,\"start\":29545},{\"end\":29890,\"start\":29885},{\"end\":29895,\"start\":29892},{\"end\":30031,\"start\":30026},{\"end\":30046,\"start\":30042},{\"end\":30058,\"start\":30053},{\"end\":30075,\"start\":30068},{\"end\":30091,\"start\":30085},{\"end\":30110,\"start\":30102},{\"end\":30130,\"start\":30119},{\"end\":30144,\"start\":30139},{\"end\":30159,\"start\":30153},{\"end\":30174,\"start\":30168},{\"end\":30619,\"start\":30609},{\"end\":30634,\"start\":30628},{\"end\":30644,\"start\":30640},{\"end\":30658,\"start\":30651},{\"end\":31145,\"start\":31141},{\"end\":31152,\"start\":31150},{\"end\":31164,\"start\":31160},{\"end\":31179,\"start\":31175},{\"end\":31192,\"start\":31190},{\"end\":31206,\"start\":31199},{\"end\":31222,\"start\":31215},{\"end\":31233,\"start\":31230},{\"end\":31663,\"start\":31657},{\"end\":31675,\"start\":31673},{\"end\":31683,\"start\":31680},{\"end\":31695,\"start\":31690},{\"end\":31708,\"start\":31706},{\"end\":31719,\"start\":31714},{\"end\":31734,\"start\":31729},{\"end\":31749,\"start\":31743},{\"end\":31765,\"start\":31759},{\"end\":31784,\"start\":31776},{\"end\":31796,\"start\":31790},{\"end\":31809,\"start\":31805},{\"end\":32133,\"start\":32127},{\"end\":32142,\"start\":32139},{\"end\":32153,\"start\":32148},{\"end\":32168,\"start\":32161},{\"end\":32185,\"start\":32180},{\"end\":32202,\"start\":32193},{\"end\":32217,\"start\":32211},{\"end\":32233,\"start\":32228},{\"end\":32247,\"start\":32240},{\"end\":32261,\"start\":32256},{\"end\":32265,\"start\":32263},{\"end\":32647,\"start\":32642},{\"end\":32659,\"start\":32655},{\"end\":32681,\"start\":32669},{\"end\":32694,\"start\":32689},{\"end\":32707,\"start\":32701},{\"end\":33147,\"start\":33141},{\"end\":33158,\"start\":33156},{\"end\":33172,\"start\":33168},{\"end\":33187,\"start\":33181},{\"end\":33206,\"start\":33197},{\"end\":33216,\"start\":33214},{\"end\":33225,\"start\":33221},{\"end\":33238,\"start\":33234},{\"end\":33244,\"start\":33240},{\"end\":33250,\"start\":33246},{\"end\":33627,\"start\":33623},{\"end\":33649,\"start\":33643},{\"end\":33658,\"start\":33651},{\"end\":34155,\"start\":34148},{\"end\":34165,\"start\":34162},{\"end\":34179,\"start\":34174},{\"end\":34195,\"start\":34188},{\"end\":34207,\"start\":34203},{\"end\":34223,\"start\":34216},{\"end\":34239,\"start\":34235},{\"end\":34258,\"start\":34248},{\"end\":34269,\"start\":34267},{\"end\":34285,\"start\":34279},{\"end\":34750,\"start\":34748},{\"end\":34761,\"start\":34759},{\"end\":34778,\"start\":34770},{\"end\":34790,\"start\":34787},{\"end\":35084,\"start\":35081},{\"end\":35099,\"start\":35094},{\"end\":35115,\"start\":35107},{\"end\":35127,\"start\":35123},{\"end\":35142,\"start\":35136},{\"end\":35156,\"start\":35149},{\"end\":35170,\"start\":35164},{\"end\":35190,\"start\":35183},{\"end\":35634,\"start\":35631},{\"end\":35647,\"start\":35645},{\"end\":35660,\"start\":35658},{\"end\":35674,\"start\":35671},{\"end\":35884,\"start\":35874},{\"end\":35898,\"start\":35892},{\"end\":36114,\"start\":36108},{\"end\":36323,\"start\":36316},{\"end\":36340,\"start\":36332},{\"end\":36353,\"start\":36349},{\"end\":36669,\"start\":36663},{\"end\":36681,\"start\":36679},{\"end\":36691,\"start\":36686},{\"end\":36706,\"start\":36699},{\"end\":36726,\"start\":36716},{\"end\":36742,\"start\":36735},{\"end\":36755,\"start\":36750},{\"end\":36773,\"start\":36766},{\"end\":36789,\"start\":36784},{\"end\":36799,\"start\":36796},{\"end\":37232,\"start\":37225},{\"end\":37247,\"start\":37244},{\"end\":37262,\"start\":37255},{\"end\":37277,\"start\":37271},{\"end\":37290,\"start\":37287},{\"end\":37308,\"start\":37301},{\"end\":37323,\"start\":37317},{\"end\":37338,\"start\":37332},{\"end\":37354,\"start\":37347},{\"end\":37366,\"start\":37361},{\"end\":37817,\"start\":37810},{\"end\":37832,\"start\":37829},{\"end\":37847,\"start\":37840},{\"end\":37862,\"start\":37856},{\"end\":37875,\"start\":37872},{\"end\":37893,\"start\":37886},{\"end\":37908,\"start\":37902},{\"end\":37923,\"start\":37917},{\"end\":37939,\"start\":37932},{\"end\":37951,\"start\":37946},{\"end\":38384,\"start\":38377},{\"end\":38396,\"start\":38394},{\"end\":38409,\"start\":38404},{\"end\":38421,\"start\":38417},{\"end\":38435,\"start\":38429},{\"end\":38451,\"start\":38442},{\"end\":38730,\"start\":38721},{\"end\":38745,\"start\":38740},{\"end\":38762,\"start\":38754},{\"end\":38782,\"start\":38771},{\"end\":38798,\"start\":38792},{\"end\":38812,\"start\":38807},{\"end\":38826,\"start\":38819},{\"end\":38840,\"start\":38834},{\"end\":38858,\"start\":38847},{\"end\":39168,\"start\":39159},{\"end\":39327,\"start\":39321},{\"end\":39337,\"start\":39333},{\"end\":39356,\"start\":39349},{\"end\":39370,\"start\":39363},{\"end\":39936,\"start\":39932},{\"end\":39949,\"start\":39945},{\"end\":39957,\"start\":39954},{\"end\":39971,\"start\":39969},{\"end\":39983,\"start\":39981},{\"end\":39999,\"start\":39993},{\"end\":40291,\"start\":40287},{\"end\":40303,\"start\":40298},{\"end\":40314,\"start\":40311},{\"end\":40326,\"start\":40321},{\"end\":40756,\"start\":40751},{\"end\":40770,\"start\":40765},{\"end\":40785,\"start\":40777},{\"end\":40795,\"start\":40787},{\"end\":41097,\"start\":41092},{\"end\":41115,\"start\":41106},{\"end\":41129,\"start\":41124},{\"end\":41142,\"start\":41136},{\"end\":41154,\"start\":41152},{\"end\":41171,\"start\":41163},{\"end\":41184,\"start\":41179},{\"end\":41207,\"start\":41198},{\"end\":41513,\"start\":41506},{\"end\":41529,\"start\":41523},{\"end\":41546,\"start\":41539},{\"end\":41563,\"start\":41555},{\"end\":41583,\"start\":41576},{\"end\":41601,\"start\":41594},{\"end\":41625,\"start\":41609},{\"end\":41637,\"start\":41632},{\"end\":41652,\"start\":41646},{\"end\":41668,\"start\":41663},{\"end\":41686,\"start\":41677},{\"end\":41702,\"start\":41696},{\"end\":41719,\"start\":41714},{\"end\":41727,\"start\":41721},{\"end\":42167,\"start\":42163},{\"end\":42183,\"start\":42178},{\"end\":42196,\"start\":42192},{\"end\":42213,\"start\":42205},{\"end\":42231,\"start\":42223},{\"end\":42244,\"start\":42241},{\"end\":42260,\"start\":42254},{\"end\":42271,\"start\":42266},{\"end\":42282,\"start\":42278},{\"end\":42300,\"start\":42291},{\"end\":42313,\"start\":42306},{\"end\":42327,\"start\":42319},{\"end\":42353,\"start\":42335},{\"end\":42364,\"start\":42362},{\"end\":42380,\"start\":42373},{\"end\":42392,\"start\":42389},{\"end\":42405,\"start\":42403},{\"end\":42419,\"start\":42415},{\"end\":42435,\"start\":42429},{\"end\":42450,\"start\":42445},{\"end\":42470,\"start\":42464},{\"end\":42476,\"start\":42472},{\"end\":43404,\"start\":43402},{\"end\":43419,\"start\":43416},{\"end\":43431,\"start\":43429},{\"end\":43446,\"start\":43442},{\"end\":43460,\"start\":43456},{\"end\":43470,\"start\":43466},{\"end\":43792,\"start\":43790},{\"end\":43803,\"start\":43799},{\"end\":43815,\"start\":43810},{\"end\":44026,\"start\":44022},{\"end\":44037,\"start\":44035},{\"end\":44052,\"start\":44048},{\"end\":44063,\"start\":44060},{\"end\":44080,\"start\":44071},{\"end\":44094,\"start\":44089},{\"end\":44107,\"start\":44104},{\"end\":44115,\"start\":44112},{\"end\":44129,\"start\":44125},{\"end\":44142,\"start\":44138},{\"end\":44576,\"start\":44571},{\"end\":44589,\"start\":44586},{\"end\":44601,\"start\":44597},{\"end\":44614,\"start\":44612},{\"end\":44626,\"start\":44623},{\"end\":44634,\"start\":44632},{\"end\":44648,\"start\":44646},{\"end\":44658,\"start\":44655},{\"end\":44667,\"start\":44663},{\"end\":45034,\"start\":45031},{\"end\":45044,\"start\":45040},{\"end\":45059,\"start\":45055},{\"end\":45069,\"start\":45067},{\"end\":45088,\"start\":45079},{\"end\":45337,\"start\":45334},{\"end\":45350,\"start\":45344},{\"end\":45365,\"start\":45357},{\"end\":45392,\"start\":45373},{\"end\":45404,\"start\":45399},{\"end\":45419,\"start\":45415},{\"end\":45430,\"start\":45428},{\"end\":45452,\"start\":45445},{\"end\":45464,\"start\":45460},{\"end\":45470,\"start\":45466},{\"end\":45907,\"start\":45904},{\"end\":45921,\"start\":45916},{\"end\":45940,\"start\":45931},{\"end\":45952,\"start\":45945},{\"end\":46321,\"start\":46318},{\"end\":46332,\"start\":46329},{\"end\":46346,\"start\":46342},{\"end\":46355,\"start\":46352},{\"end\":46366,\"start\":46364},{\"end\":46379,\"start\":46377},{\"end\":46391,\"start\":46388},{\"end\":46406,\"start\":46402},{\"end\":46421,\"start\":46417},{\"end\":46430,\"start\":46426}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":248476411},\"end\":28738,\"start\":28175},{\"attributes\":{\"doi\":\"qianwen. 2023. 3\",\"id\":\"b1\"},\"end\":28807,\"start\":28740},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3180429},\"end\":29316,\"start\":28809},{\"attributes\":{\"id\":\"b3\"},\"end\":29822,\"start\":29318},{\"attributes\":{\"id\":\"b4\"},\"end\":29981,\"start\":29824},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":218971783},\"end\":30501,\"start\":29983},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":231951742},\"end\":31080,\"start\":30503},{\"attributes\":{\"id\":\"b7\"},\"end\":31572,\"start\":31082},{\"attributes\":{\"id\":\"b8\"},\"end\":32072,\"start\":31574},{\"attributes\":{\"doi\":\"arXiv:2303.03378\",\"id\":\"b9\"},\"end\":32535,\"start\":32074},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8081284},\"end\":33137,\"start\":32537},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b11\"},\"end\":33530,\"start\":33139},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":152282269},\"end\":34049,\"start\":33532},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4492210},\"end\":34636,\"start\":34051},{\"attributes\":{\"doi\":\"arXiv:2301.12597\",\"id\":\"b14\"},\"end\":35027,\"start\":34638},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14113767},\"end\":35621,\"start\":35029},{\"attributes\":{\"doi\":\"arXiv:2304.08485\",\"id\":\"b16\"},\"end\":35865,\"start\":35623},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b17\"},\"end\":36082,\"start\":35867},{\"attributes\":{\"id\":\"b18\"},\"end\":36160,\"start\":36084},{\"attributes\":{\"id\":\"b19\"},\"end\":36240,\"start\":36162},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14579301},\"end\":36587,\"start\":36242},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":246426909},\"end\":37147,\"start\":36589},{\"attributes\":{\"doi\":\"PMLR, 2021. 1\",\"id\":\"b22\",\"matched_paper_id\":231591445},\"end\":37732,\"start\":37149},{\"attributes\":{\"doi\":\"PMLR, 2021. 4\",\"id\":\"b23\",\"matched_paper_id\":231591445},\"end\":38317,\"start\":37734},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":160025533},\"end\":38637,\"start\":38319},{\"attributes\":{\"doi\":\"arXiv:2111.02114\",\"id\":\"b25\"},\"end\":39155,\"start\":38639},{\"attributes\":{\"doi\":\"nova. 2023. 3\",\"id\":\"b26\"},\"end\":39213,\"start\":39157},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":51876975},\"end\":39846,\"start\":39215},{\"attributes\":{\"doi\":\"arXiv:2303.17580\",\"id\":\"b28\"},\"end\":40228,\"start\":39848},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":19435386},\"end\":40743,\"start\":40230},{\"attributes\":{\"doi\":\"arXiv:2303.08128\",\"id\":\"b30\"},\"end\":41029,\"start\":40745},{\"attributes\":{\"id\":\"b31\"},\"end\":41445,\"start\":41031},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b32\"},\"end\":42095,\"start\":41447},{\"attributes\":{\"id\":\"b33\"},\"end\":43316,\"start\":42097},{\"attributes\":{\"doi\":\"arXiv:2303.04671\",\"id\":\"b34\"},\"end\":43700,\"start\":43318},{\"attributes\":{\"doi\":\"arXiv:2212.10773\",\"id\":\"b35\"},\"end\":44010,\"start\":43702},{\"attributes\":{\"doi\":\"arXiv:2303.11381\",\"id\":\"b36\"},\"end\":44480,\"start\":44012},{\"attributes\":{\"doi\":\"arXiv:2303.16199\",\"id\":\"b37\"},\"end\":44935,\"start\":44482},{\"attributes\":{\"doi\":\"arXiv:2304.10592\",\"id\":\"b38\"},\"end\":45324,\"start\":44937},{\"attributes\":{\"doi\":\"arXiv:2304.06939\",\"id\":\"b39\"},\"end\":45848,\"start\":45326},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":5714907},\"end\":46309,\"start\":45850},{\"attributes\":{\"doi\":\"arXiv:2212.11270\",\"id\":\"b41\"},\"end\":46740,\"start\":46311}]", "bib_title": "[{\"end\":28230,\"start\":28175},{\"end\":28839,\"start\":28809},{\"end\":30020,\"start\":29983},{\"end\":30599,\"start\":30503},{\"end\":31131,\"start\":31082},{\"end\":32635,\"start\":32537},{\"end\":33619,\"start\":33532},{\"end\":34139,\"start\":34051},{\"end\":35070,\"start\":35029},{\"end\":36306,\"start\":36242},{\"end\":36656,\"start\":36589},{\"end\":37218,\"start\":37149},{\"end\":37803,\"start\":37734},{\"end\":38370,\"start\":38319},{\"end\":39312,\"start\":39215},{\"end\":40279,\"start\":40230},{\"end\":42154,\"start\":42097},{\"end\":45897,\"start\":45850}]", "bib_author": "[{\"end\":28260,\"start\":28232},{\"end\":28277,\"start\":28260},{\"end\":28290,\"start\":28277},{\"end\":28302,\"start\":28290},{\"end\":28313,\"start\":28302},{\"end\":28327,\"start\":28313},{\"end\":28340,\"start\":28327},{\"end\":28358,\"start\":28340},{\"end\":28376,\"start\":28358},{\"end\":28386,\"start\":28376},{\"end\":28751,\"start\":28742},{\"end\":28759,\"start\":28751},{\"end\":28858,\"start\":28841},{\"end\":28877,\"start\":28858},{\"end\":28888,\"start\":28877},{\"end\":28907,\"start\":28888},{\"end\":28920,\"start\":28907},{\"end\":28938,\"start\":28920},{\"end\":28951,\"start\":28938},{\"end\":29335,\"start\":29320},{\"end\":29346,\"start\":29335},{\"end\":29362,\"start\":29346},{\"end\":29375,\"start\":29362},{\"end\":29389,\"start\":29375},{\"end\":29402,\"start\":29389},{\"end\":29419,\"start\":29402},{\"end\":29435,\"start\":29419},{\"end\":29448,\"start\":29435},{\"end\":29462,\"start\":29448},{\"end\":29479,\"start\":29462},{\"end\":29493,\"start\":29479},{\"end\":29510,\"start\":29493},{\"end\":29529,\"start\":29510},{\"end\":29545,\"start\":29529},{\"end\":29559,\"start\":29545},{\"end\":29892,\"start\":29879},{\"end\":29897,\"start\":29892},{\"end\":30033,\"start\":30022},{\"end\":30048,\"start\":30033},{\"end\":30060,\"start\":30048},{\"end\":30077,\"start\":30060},{\"end\":30093,\"start\":30077},{\"end\":30112,\"start\":30093},{\"end\":30132,\"start\":30112},{\"end\":30146,\"start\":30132},{\"end\":30161,\"start\":30146},{\"end\":30176,\"start\":30161},{\"end\":30621,\"start\":30601},{\"end\":30636,\"start\":30621},{\"end\":30646,\"start\":30636},{\"end\":30660,\"start\":30646},{\"end\":31147,\"start\":31133},{\"end\":31154,\"start\":31147},{\"end\":31166,\"start\":31154},{\"end\":31181,\"start\":31166},{\"end\":31194,\"start\":31181},{\"end\":31208,\"start\":31194},{\"end\":31224,\"start\":31208},{\"end\":31235,\"start\":31224},{\"end\":31665,\"start\":31649},{\"end\":31677,\"start\":31665},{\"end\":31685,\"start\":31677},{\"end\":31697,\"start\":31685},{\"end\":31710,\"start\":31697},{\"end\":31721,\"start\":31710},{\"end\":31736,\"start\":31721},{\"end\":31751,\"start\":31736},{\"end\":31767,\"start\":31751},{\"end\":31786,\"start\":31767},{\"end\":31798,\"start\":31786},{\"end\":31811,\"start\":31798},{\"end\":32135,\"start\":32121},{\"end\":32144,\"start\":32135},{\"end\":32155,\"start\":32144},{\"end\":32170,\"start\":32155},{\"end\":32187,\"start\":32170},{\"end\":32204,\"start\":32187},{\"end\":32219,\"start\":32204},{\"end\":32235,\"start\":32219},{\"end\":32249,\"start\":32235},{\"end\":32263,\"start\":32249},{\"end\":32267,\"start\":32263},{\"end\":32649,\"start\":32637},{\"end\":32661,\"start\":32649},{\"end\":32683,\"start\":32661},{\"end\":32696,\"start\":32683},{\"end\":32709,\"start\":32696},{\"end\":33149,\"start\":33139},{\"end\":33160,\"start\":33149},{\"end\":33174,\"start\":33160},{\"end\":33189,\"start\":33174},{\"end\":33208,\"start\":33189},{\"end\":33218,\"start\":33208},{\"end\":33227,\"start\":33218},{\"end\":33240,\"start\":33227},{\"end\":33246,\"start\":33240},{\"end\":33252,\"start\":33246},{\"end\":33629,\"start\":33621},{\"end\":33651,\"start\":33629},{\"end\":33660,\"start\":33651},{\"end\":34157,\"start\":34141},{\"end\":34167,\"start\":34157},{\"end\":34181,\"start\":34167},{\"end\":34197,\"start\":34181},{\"end\":34209,\"start\":34197},{\"end\":34225,\"start\":34209},{\"end\":34241,\"start\":34225},{\"end\":34260,\"start\":34241},{\"end\":34271,\"start\":34260},{\"end\":34287,\"start\":34271},{\"end\":34752,\"start\":34741},{\"end\":34763,\"start\":34752},{\"end\":34780,\"start\":34763},{\"end\":34792,\"start\":34780},{\"end\":35086,\"start\":35072},{\"end\":35101,\"start\":35086},{\"end\":35117,\"start\":35101},{\"end\":35129,\"start\":35117},{\"end\":35144,\"start\":35129},{\"end\":35158,\"start\":35144},{\"end\":35172,\"start\":35158},{\"end\":35192,\"start\":35172},{\"end\":35636,\"start\":35623},{\"end\":35649,\"start\":35636},{\"end\":35662,\"start\":35649},{\"end\":35676,\"start\":35662},{\"end\":35886,\"start\":35869},{\"end\":35900,\"start\":35886},{\"end\":36116,\"start\":36108},{\"end\":36325,\"start\":36308},{\"end\":36342,\"start\":36325},{\"end\":36355,\"start\":36342},{\"end\":36671,\"start\":36658},{\"end\":36683,\"start\":36671},{\"end\":36693,\"start\":36683},{\"end\":36708,\"start\":36693},{\"end\":36728,\"start\":36708},{\"end\":36744,\"start\":36728},{\"end\":36757,\"start\":36744},{\"end\":36775,\"start\":36757},{\"end\":36791,\"start\":36775},{\"end\":36801,\"start\":36791},{\"end\":37234,\"start\":37220},{\"end\":37249,\"start\":37234},{\"end\":37264,\"start\":37249},{\"end\":37279,\"start\":37264},{\"end\":37292,\"start\":37279},{\"end\":37310,\"start\":37292},{\"end\":37325,\"start\":37310},{\"end\":37340,\"start\":37325},{\"end\":37356,\"start\":37340},{\"end\":37368,\"start\":37356},{\"end\":37819,\"start\":37805},{\"end\":37834,\"start\":37819},{\"end\":37849,\"start\":37834},{\"end\":37864,\"start\":37849},{\"end\":37877,\"start\":37864},{\"end\":37895,\"start\":37877},{\"end\":37910,\"start\":37895},{\"end\":37925,\"start\":37910},{\"end\":37941,\"start\":37925},{\"end\":37953,\"start\":37941},{\"end\":38386,\"start\":38372},{\"end\":38398,\"start\":38386},{\"end\":38411,\"start\":38398},{\"end\":38423,\"start\":38411},{\"end\":38437,\"start\":38423},{\"end\":38453,\"start\":38437},{\"end\":38732,\"start\":38711},{\"end\":38747,\"start\":38732},{\"end\":38764,\"start\":38747},{\"end\":38784,\"start\":38764},{\"end\":38800,\"start\":38784},{\"end\":38814,\"start\":38800},{\"end\":38828,\"start\":38814},{\"end\":38842,\"start\":38828},{\"end\":38860,\"start\":38842},{\"end\":39170,\"start\":39159},{\"end\":39329,\"start\":39314},{\"end\":39339,\"start\":39329},{\"end\":39358,\"start\":39339},{\"end\":39372,\"start\":39358},{\"end\":39938,\"start\":39922},{\"end\":39951,\"start\":39938},{\"end\":39959,\"start\":39951},{\"end\":39973,\"start\":39959},{\"end\":39985,\"start\":39973},{\"end\":40001,\"start\":39985},{\"end\":40293,\"start\":40281},{\"end\":40305,\"start\":40293},{\"end\":40316,\"start\":40305},{\"end\":40328,\"start\":40316},{\"end\":40758,\"start\":40745},{\"end\":40772,\"start\":40758},{\"end\":40787,\"start\":40772},{\"end\":40797,\"start\":40787},{\"end\":41099,\"start\":41086},{\"end\":41117,\"start\":41099},{\"end\":41131,\"start\":41117},{\"end\":41144,\"start\":41131},{\"end\":41156,\"start\":41144},{\"end\":41173,\"start\":41156},{\"end\":41186,\"start\":41173},{\"end\":41209,\"start\":41186},{\"end\":41515,\"start\":41501},{\"end\":41531,\"start\":41515},{\"end\":41548,\"start\":41531},{\"end\":41565,\"start\":41548},{\"end\":41585,\"start\":41565},{\"end\":41603,\"start\":41585},{\"end\":41627,\"start\":41603},{\"end\":41639,\"start\":41627},{\"end\":41654,\"start\":41639},{\"end\":41670,\"start\":41654},{\"end\":41688,\"start\":41670},{\"end\":41704,\"start\":41688},{\"end\":41721,\"start\":41704},{\"end\":41729,\"start\":41721},{\"end\":42169,\"start\":42156},{\"end\":42185,\"start\":42169},{\"end\":42198,\"start\":42185},{\"end\":42215,\"start\":42198},{\"end\":42233,\"start\":42215},{\"end\":42246,\"start\":42233},{\"end\":42262,\"start\":42246},{\"end\":42273,\"start\":42262},{\"end\":42284,\"start\":42273},{\"end\":42302,\"start\":42284},{\"end\":42315,\"start\":42302},{\"end\":42329,\"start\":42315},{\"end\":42355,\"start\":42329},{\"end\":42366,\"start\":42355},{\"end\":42382,\"start\":42366},{\"end\":42394,\"start\":42382},{\"end\":42407,\"start\":42394},{\"end\":42421,\"start\":42407},{\"end\":42437,\"start\":42421},{\"end\":42452,\"start\":42437},{\"end\":42472,\"start\":42452},{\"end\":42478,\"start\":42472},{\"end\":43406,\"start\":43394},{\"end\":43421,\"start\":43406},{\"end\":43433,\"start\":43421},{\"end\":43448,\"start\":43433},{\"end\":43462,\"start\":43448},{\"end\":43472,\"start\":43462},{\"end\":43794,\"start\":43782},{\"end\":43805,\"start\":43794},{\"end\":43817,\"start\":43805},{\"end\":44028,\"start\":44012},{\"end\":44039,\"start\":44028},{\"end\":44054,\"start\":44039},{\"end\":44065,\"start\":44054},{\"end\":44082,\"start\":44065},{\"end\":44096,\"start\":44082},{\"end\":44109,\"start\":44096},{\"end\":44117,\"start\":44109},{\"end\":44131,\"start\":44117},{\"end\":44144,\"start\":44131},{\"end\":44578,\"start\":44564},{\"end\":44591,\"start\":44578},{\"end\":44603,\"start\":44591},{\"end\":44616,\"start\":44603},{\"end\":44628,\"start\":44616},{\"end\":44636,\"start\":44628},{\"end\":44650,\"start\":44636},{\"end\":44660,\"start\":44650},{\"end\":44669,\"start\":44660},{\"end\":45036,\"start\":45025},{\"end\":45046,\"start\":45036},{\"end\":45061,\"start\":45046},{\"end\":45071,\"start\":45061},{\"end\":45090,\"start\":45071},{\"end\":45339,\"start\":45326},{\"end\":45352,\"start\":45339},{\"end\":45367,\"start\":45352},{\"end\":45394,\"start\":45367},{\"end\":45406,\"start\":45394},{\"end\":45421,\"start\":45406},{\"end\":45432,\"start\":45421},{\"end\":45454,\"start\":45432},{\"end\":45466,\"start\":45454},{\"end\":45472,\"start\":45466},{\"end\":45909,\"start\":45899},{\"end\":45923,\"start\":45909},{\"end\":45942,\"start\":45923},{\"end\":45954,\"start\":45942},{\"end\":46323,\"start\":46311},{\"end\":46334,\"start\":46323},{\"end\":46348,\"start\":46334},{\"end\":46357,\"start\":46348},{\"end\":46368,\"start\":46357},{\"end\":46381,\"start\":46368},{\"end\":46393,\"start\":46381},{\"end\":46408,\"start\":46393},{\"end\":46423,\"start\":46408},{\"end\":46432,\"start\":46423}]", "bib_venue": "[{\"end\":29072,\"start\":29020},{\"end\":30809,\"start\":30743},{\"end\":32850,\"start\":32788},{\"end\":33809,\"start\":33743},{\"end\":35264,\"start\":35245},{\"end\":39533,\"start\":39461},{\"end\":40489,\"start\":40417},{\"end\":42689,\"start\":42589},{\"end\":46095,\"start\":46033},{\"end\":28435,\"start\":28386},{\"end\":29018,\"start\":28951},{\"end\":29877,\"start\":29824},{\"end\":30225,\"start\":30176},{\"end\":30741,\"start\":30660},{\"end\":31318,\"start\":31235},{\"end\":31647,\"start\":31574},{\"end\":32119,\"start\":32074},{\"end\":32786,\"start\":32709},{\"end\":33312,\"start\":33268},{\"end\":33741,\"start\":33660},{\"end\":34327,\"start\":34287},{\"end\":34739,\"start\":34638},{\"end\":35243,\"start\":35192},{\"end\":35717,\"start\":35692},{\"end\":36106,\"start\":36084},{\"end\":36197,\"start\":36164},{\"end\":36404,\"start\":36355},{\"end\":36850,\"start\":36801},{\"end\":37425,\"start\":37381},{\"end\":38010,\"start\":37966},{\"end\":38464,\"start\":38453},{\"end\":38709,\"start\":38639},{\"end\":39459,\"start\":39372},{\"end\":39920,\"start\":39848},{\"end\":40415,\"start\":40328},{\"end\":40864,\"start\":40813},{\"end\":41084,\"start\":41031},{\"end\":41499,\"start\":41447},{\"end\":42587,\"start\":42478},{\"end\":43392,\"start\":43318},{\"end\":43780,\"start\":43702},{\"end\":44223,\"start\":44160},{\"end\":44562,\"start\":44482},{\"end\":45023,\"start\":44937},{\"end\":45564,\"start\":45488},{\"end\":46031,\"start\":45954},{\"end\":46499,\"start\":46448}]"}}}, "year": 2023, "month": 12, "day": 17}