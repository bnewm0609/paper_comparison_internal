{"id": 218518087, "updated": "2023-07-18 21:46:52.82", "metadata": {"title": "Recurrent Attention Network with Reinforced Generator for Visual Dialog", "authors": "[{\"first\":\"Hehe\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Linchao\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Fei\",\"last\":\"Wu\",\"middle\":[]}]", "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications", "journal": "ACM Transactions on Multimedia Computing, Communications, and Applications", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "In Visual Dialog, an agent has to parse temporal context in the dialog history and spatial context in the image to hold a meaningful dialog with humans. For example, to answer \u201cwhat is the man on her left wearing?\u201d the agent needs to (1) analyze the temporal context in the dialog history to infer who is being referred to as \u201cher,\u201d (2) parse the image to attend \u201cher,\u201d and (3) uncover the spatial context to shift the attention to \u201cher left\u201d and check the apparel of the man. In this article, we use a dialog network to memorize the temporal context and an attention processor to parse the spatial context. Since the question and the image are usually very complex, which makes it difficult for the question to be grounded with a single glimpse, the attention processor attends to the image multiple times to better collect visual information. In the Visual Dialog task, the generative decoder (G) is trained under the word-by-word paradigm, which suffers from the lack of sentence-level training. We propose to reinforce G at the sentence level using the discriminative model (D), which aims to select the right answer from a few candidates, to ameliorate the problem. Experimental results on the VisDial dataset demonstrate the effectiveness of our approach.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3038528491", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tomccap/FanZYW20", "doi": "10.1145/3390891"}}, "content": {"source": {"pdf_hash": "cb9fb10f604a196515e48ad90f217d33794f5991", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "51d426addc24de3dfab293e5a377d57195e76ed5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cb9fb10f604a196515e48ad90f217d33794f5991.txt", "contents": "\nRecurrent Attention Network with Reinforced Generator for Visual Dialog\n2020. July 2020\n\nHehe Fan \nCenter for Artificial Intelligence\nCenter for Artificial Intelligence\nCollege of Computer Science\nUniversity of Technology Sydney and Baidu Research LINCHAO ZHU and YI YANG\nUniversity of Technology Sydney FEI WU\nZhejiang University\n\n\nHehe Fan \nCenter for Artificial Intelligence\nCenter for Artificial Intelligence\nCollege of Computer Science\nUniversity of Technology Sydney and Baidu Research LINCHAO ZHU and YI YANG\nUniversity of Technology Sydney FEI WU\nZhejiang University\n\n\nLinchao Zhu \nCenter for Artificial Intelligence\nCenter for Artificial Intelligence\nCollege of Computer Science\nUniversity of Technology Sydney and Baidu Research LINCHAO ZHU and YI YANG\nUniversity of Technology Sydney FEI WU\nZhejiang University\n\n\nYi Yang \nCenter for Artificial Intelligence\nCenter for Artificial Intelligence\nCollege of Computer Science\nUniversity of Technology Sydney and Baidu Research LINCHAO ZHU and YI YANG\nUniversity of Technology Sydney FEI WU\nZhejiang University\n\n\nFei Wu \nCenter for Artificial Intelligence\nCenter for Artificial Intelligence\nCollege of Computer Science\nUniversity of Technology Sydney and Baidu Research LINCHAO ZHU and YI YANG\nUniversity of Technology Sydney FEI WU\nZhejiang University\n\n\nRecurrent Attention Network with Reinforced Generator for Visual Dialog\n\nACM Trans. Multimedia Comput. Commun. Appl\n162020. July 202010.1145/3390891CCS Concepts: \u2022 Computing methodologies \u2192 Computer vision tasks; Additional Key Words and Phrases: Visual Dialogvision and languagereinforcement learningdeep learning ACM Reference format:\nIn Visual Dialog, an agent has to parse temporal context in the dialog history and spatial context in the image to hold a meaningful dialog with humans. For example, to answer \"what is the man on her left wearing?\" the agent needs to (1) analyze the temporal context in the dialog history to infer who is being referred to as \"her,\" (2) parse the image to attend \"her,\" and (3) uncover the spatial context to shift the attention to \"her left\" and check the apparel of the man. In this article, we use a dialog network to memorize the temporal context and an attention processor to parse the spatial context. Since the question and the image are usually very complex, which makes it difficult for the question to be grounded with a single glimpse, the attention processor attends to the image multiple times to better collect visual information. In the Visual Dialog task, the generative decoder (G) is trained under the word-by-word paradigm, which suffers from the lack of sentencelevel training. We propose to reinforce G at the sentence level using the discriminative model (D), which aims to select the right answer from a few candidates, to ameliorate the problem. Experimental results on the VisDial dataset demonstrate the effectiveness of our approach.\n\nINTRODUCTION\n\nThis article focuses on Visual Dialog [5,6,19,32,40], in which an agent perceives the environment visually and communicates with humans in natural language. When presented with an image, a dialog history, and a question about the visual content of the image, the agent answers the question The dialog history is crucial for enabling the agent to conduct a dialog. For example, to answer the last question, the agent has to infer what \"the\" and \"about\" refer to. The agent takes multiple glimpses to attend to the man on the left given the complex layout of the image and the dialog history.\n\nin natural language. Unlike visual question answering (VQA) [2,21,28,39], where there are no follow-up questions, the questions in Visual Dialog are usually temporally ordered with narrative structures. Visual Dialog possesses the ability to hold a meaningful dialog with humans in natural language about visual content, which benefits a variety of applications.\n\nIt is necessary to carefully address two main problems in Visual Dialog. First, we need to address how to enable the agent to effectively parse the temporal context, such as the dialog history, to understand the current question accurately. As shown in Figure 1, the agent has to infer what the two words \"the\" and \"about\" refer to in order to answer the last question. Second, we need to address how to attend to the region of interest in an image given a question and its temporal context so that the agent has a comprehensive understanding of the rich visual content.\n\nIn this article, we propose a recurrent attention network for Visual Dialog, which attempts to address the two problems by an encoder-decoder architecture. For the encoder, we use a dialog network, which is a long short-term memory (LSTM) [13], to memorize the temporal context of the dialog history. The dialog network triggers a question signal that includes the question and dialog history information. Then the signal is passed to an attention processor.\n\nThe attention processor is another LSTM integrated with an attention mechanism [3,43], to parse the visual spatial context. Guided by the dialog network signal, the attention processor grounds the question in the image by iteratively glimpsing the visual content multiple times. Last, a state vector is generated by incorporating the multiple glimpses and passed to the decoder to generate or select an answer.\n\nThere are two decoders in Visual Dialog [5]: a generative model (G) that generates answers in natural language and a discriminative model (D) to select the best answers from candidates. A major problem in optimizing G is the lack of appropriate loss function for training. Typical wordby-word generation is trained to fit an answer at the word level. However, two answers can be semantically different even though they are similar by word-to-word matching. For example, the sentence \"he is 4 years old\" is very similar to the ground-truth \"he is 14 years old\" by word-level matching, but the meanings are very different.\n\nTo ameliorate the problem, we propose to guide the training of G at the sentence level by D. Our premise is that D is capable of measuring answers at the sentence level, and its knowledge therefore can be used as a complement of word-level training for G. In addition, the training of D is more robust because the training loss can be computed by directly checking whether the selected answers are exactly the ground truths. Since transferring information from D to G is non-differentiable, we use reinforcement learning [35,36] to achieve the goal by reward. An experiment on the VisDial dataset [5] demonstrates the effectiveness of the attention processor and the training method that reinforces G by D.\n\n\nRELATED WORK\n\nVision and language. There is increased interest in the field of multimodal learning [44] for bridging computer vision [7,10] and natural language understanding, such as image captioning [8,11,12,15,34,37,38,46], video captioning [17,24,31,42], text-to-image coreference/grounding [14,16,25,26,30], VQA [2,4,21,39,41,47], and Visual Dialog [5,6,19,32,40]. These tasks typically involve attention mechanisms.\n\nThe difference between Visual Dialog and VQA is that the questions in Visual Dialog are dependent on the history and often strongly contextual. In Visual Dialog, a question-answer pair is called a fact and the previous facts constitute history. In contrast to other work [5,19], Das et al. [6] trained two agents to communicate in natural language using an \"image guessing\" game. We follow those other works [5,19] by only implementing the answerer agent but evaluating the agent at the dialog level, in which no ground-truth answer is available during evaluation.\n\nAttention in vision and language. Several prior works have applied attention mechanisms [3,43] to vision and language tasks [1,5,20,24,45,48]. For example, Yang et al. [45] presented stacked attention networks (SANs) that learned to answer natural language questions from images. Lu et al. [20] proposed two co-attention mechanisms (HieCoAtt) for VQA that jointly performed question-guided visual attention and image-guided question attention. They also proposed a history-conditioned image attentive encoder (HCIAE) [19] for Visual Dialog that first used the current question to attend to the exchanges in the history, and then used the question and attended history to attend to the image, so as to obtain the final encoding. Wu et al. [40] applied the co-attention mechanism (CoAtt) to Visual Dialog by adding attentions on the dialog history. Different from these attention mechanisms, our recurrent attention network queries the image by several glimpses in each question-answering round. The core of the recurrent attention network is an attention processor that is implemented by an LSTM with an attention mechanism. For each glimpse, the attention processor takes the attention query and the last attention result as input and outputs the new attention result. The last state of the LSTM is used to initialize the decoder's state.\n\nReinforcement learning. Many breakthroughs in reinforcement learning have recently been made. It is generally accepted that there are two families of model-free methods for reinforcement learning. The first consists of value-based methods, where an action value function is learned to guide the agent to act in the next timestep. One algorithm is Q-learning [35] (e.g., DQN [23]), which aims to directly approximate the optimal action value function. In contrast to value-based methods, the second type is policy-based methods that directly parameterize the policy \u03c0 , such as REINFORCE [36]. Standard REINFORCE updates the policy parameters in the gradient ascent direction.\n\nREINFORCE has been widely used in computer vision and natural language processing. Mnih et al. [22] developed the recurrent visual attention model, which learns the spatial attention policies for image classification. Fan et al. [9] applied REINFORCE to efficient video classification. Ranzato et al. [27] proposed a sequence-level training method for recurrent neural networks with REINFORCE. Rennie et al. [29] used the sequence-level training for image captioning. Das et al. [6] used REINFORCE to train two agents to communicate in natural language for Visual Dialog. Wu et al. [40] used adversarial REINFORCE with an intermediate reward to encourage the generator to generate responses as human-generated dialogs. In this article, we use the basic REINFORCE to provide a sentence-level training for the generator G. consists of a sentence encoder, a dialog network, and an attention processor. The sentence encoder embeds a natural language sentence to a one-dimensional vector. The dialog network is designed to parse the temporal context of dialog. Since LSTM is able to model sequences, we leverage an LSTM to model question dependency in a dialog. According to the order of questions, the dialog network can understand pronouns in a sequence of questions. Specifically, at each question-answering round, the dialog network takes an embedded question as input. Then, according to its memory, the dialog network emits a signal that integrates the question and the temporal context. The attention processor grounds the signal by glimpsing the image multiple times and generates a state vector to the decoder. The given image and caption are used to initialize the state of the dialog network. The decoder (B) includes a discriminator (D) and a generator (G). The D selects best answers from candidates. The G directly generates answers in natural language. In addition to the word-by-word training, the G is further improved by D at the sentence level.\n\n\nMODEL\n\nIn this section, we describe the details of our recurrent attention network. As shown in Figure 2, the network consists of an encoder and a decoder. Before the first question-answering round, the agent is given the image I and a caption c of the image. In each round t, the encoder analyzes the dialog history, integrates the current question q t , observes the image, and generates a state vector for the decoder. According to the state vector, the discriminator decoder D then selects the best answer a d t from a set of candidates. The generator decoder G generates an answer a \u0434 t for the question q t in natural language. During training, we also utilize D to improve G by reinforcement learning.\n\n\nEncoder\n\nDialog network. The dialog network is responsible for modeling the temporal context in the dialog history. Since a dialog is a sequence of question-answer pairs along time, we use an LSTM to memorize the temporal context. At each question-answering round, the dialog network takes the question embedding vector as input and generates a signal that integrates the temporal context and the question. At the beginning of a dialog, we concatenate the image feature and the caption embedding to initialize the state of the dialog network.\n\nNote that we do not encode the previously generated answers back to the dialog network. The reasons are as follows. First, in this work, we evaluate models with a new protocol, in which the ground-truth answers are not available to the agent. Since the self-generated answers are not reliable, we do not use them as \"facts\". Second, the generated answers are dependent on the encoder. The information that the decoder needs is produced by the encoder. Therefore, there is no need to encode the previously generated answers back to the dialog network.\n\nAttention processor. The attention processor takes the signal emitted by the dialog network as input and parses the spatial context in the image to generate a state vector for the decoder. The attention processor is implemented by an LSTM with an attention mechanism at each glimpse. The LSTM memorizes the attention history and collects the visual information for the question. Before the first glimpse, the state of our attention processor is initialized by the signal emitted by the dialog network. For each glimpse, the attention processor uses the LSTM's state to attend to the image. The attention result will be used as input for the next glimpse. Since the LSTM memorizes the attention history, the attention processor is capable of avoiding repeated attentions for future glimpses. Figure 3 illustrates how the attention processor grounds a question in an image. Although sometimes it is difficult to precisely define how many glimpses are actually needed to correctly answer a question, a single-step attention mechanism is usually not enough.\n\nWe use the spatial features F \u2208 R d \u00d7k\u00d7k from the activation of a convolutional neural network (ConvNet) layer as the image representations to be attended. Before the model attends the the spatial feature F \u2208 R d \u00d7k\u00d7k , we reshape F to R d \u00d7k 2 . For the VGG-16 network [33], we use the output of the pool 5 layer in which d = 512 and k = 7.\n\nSuppose that s is the LSTM hidden state size, a is the length of attention vector, and 1 \u2208 R k 2 is a vector with all elements set to 1. The signal emitted by dialog network is q \u2208 R d . Our attention processor contains an LSTM and a few trainable parameters W 1 \u2208 R d \u00d7s , W 2 \u2208 R a\u00d7d , M \u2208 R a\u00d7s , v \u2208 R a . We show the details of the recurrent attention mechanism in Algorithm 1. The \u03b1 T \u2208 R k is the attention weight over the spatial image feature F . Since f (visual features) and q (semantic features) are from two different feature spaces, we normalize them before concatenating them to a vector. Because D can measure answers at the sentence level, it can be used to evaluate and guide G's behavior during training. Because this process is not differentiable, reinforcement learning is used.\n\n\nALGORITHM 1: Recurrent Attention Mechanism\n\nInput: spatial image feature F \u2208 R d \u00d7k 2 ; signal emitted by dialog network q \u2208 R d ; attention parameters of the attention processor\nW 1 \u2208 R d \u00d7s , W 2 \u2208 R a\u00d7d , M \u2208 R a\u00d7s , v \u2208 R a ;\nLong short-term memory of the attention processor LSTM; number of attention steps T . Output: state h.\nInitialize: t \u2190 0. h \u2190 qW 1 // initialization for the LSTM hidden state, which is an s-dimensional vector q \u2190 q | |q | | 2 // l2 normalization for signal q while t < T do t \u2190 t + 1 \u03b1 \u2190 v T tanh(W 2 F + Mh1 T )\n// generate attention vector, whose length is k 2 \u03b1 \u2190 softmax(\u03b1 ) // attention weight f \u2190 F\u03b1 T // apply attention weight to the spatial image feature and generate an attended feature vector whose length is\nd f \u2190 f | |f | | 2 // l2 normalization for attended feature vector i \u2190 concatenate( f , q) h \u2190 LSTM(i, h) end\n\nDecoder\n\nThere are two kinds of models for the decoder in Visual Dialog: the discriminative model (D) and the generative model (G). The discriminator aims to select the ground-truth answer from a set of candidates at the sentence level, whereas the generator directly generates a sentence and maximizes the likelihood of each word in the ground-truth answer. Unlike Das et al. [5], in which the two models are trained and evaluated separately, we train D and G jointly. Furthermore, we use D to improve G at the sentence level. In this section, we denote the output state vector of encoder as h 0 .\n\nDiscriminator (D). The discriminator computes dot-product similarities between h 0 and each of the embedded answer candidates. As shown in Figure 4(a), the similarities are fed into a softmax function during training to compute the posterior probability over the candidates. The network is trained to maximize the log-likelihood of the ground-truth answer. We denote f (\u00b7) as the sentence encoder and s r as the ground-truth answer, then training D involves minimizing\nL D = \u2212log p( f (s r )|h 0 ) \u221d \u2212f (s r ) \u00b7 h 0 .(1)\nIn this step, both the sentence encoder f and the dialog network are updated. During evaluation, candidates are ranked based on their posterior probabilities.\n\nGenerator (G). We denote the ground-truth answer s r as a sequence of words [w 1 , w 2 , . . . ,w T ], and training G involves minimizing\nL G = \u2212log p(s r |h 0 ) = \u2212log p(w 1 , w 2 , . . . ,w T |h 0 ) = \u2212log T t =1 p(w t |w 1 , w 2 , . . . ,w t \u22121 , h 0 ) = \u2212 T t =1 log p(w t |w 1 , w 2 , . . . ,w t \u22121 , h 0 ).(2)\nSince Equation (2) aims to maximize the likelihood of each word in the ground-truth answer, we call this method word-level training. Usually, the term p(w t |w 1 , . . . ,w t \u22121 , h 0 ) is modeled by an LSTM that takes h 0 as its initial state:\np(\u00b7|w 1 , w 2 , . . . ,w t \u22121 , h 0 ), h t = LST M (w t \u22121 , h t \u22121 ),(3)\nwhere w 0 is the start symbol <START>. In addition, the distribution p(\u00b7|w 1 , w 2 , . . . ,w t \u22121 , h 0 ) is a parametric function of h t . LSTM first generates the current hidden state h t and then emits the distribution by a fully connected layer according to h t . For simplicity, we use \u03c0 (\u00b7|h t ) to denote this distribution:\n\u03c0 (\u00b7|h t ) = p(\u00b7|w 1 , w 2 , . . . ,w t \u22121 , h 0 ).(4)\nThe current word is generated by w t = argmax w \u03c0 (\u00b7|h t ). During training, the previous groundtruth words are given. When conducting the evaluation, the previous ground-truth words are unavailable and are generated by maximum likelihood estimation (MLE). The log\u03c0 is used to rank candidate answers as log-likelihood scores. The generator and the sentence encoder share the same word embedding matrix. Compared with discriminative models, generative decoders are more practical for realistic applications but often achieve lower accuracy than discriminative models.\n\nReinforcing G by D. The answer to a question is usually not unique. However, training with Equation (2) will make the agent focus only on the unique answer provided by the dataset. As a result, the agent may mistakenly treat semantically similar sentences as wrong answers. Since D can measure answers at the sentence level, we additionally use D to evaluate and guide G's behavior during training. Supposing that \u0434(\u00b7) is the generative function to be learned, the sentencelevel training is to\nL J = \u2212log p( f (\u0434(h 0 ))|h 0 ) \u2248 \u2212log p( f (\u0434(h 0 ))| f (s r )).(5)\nBecause f (\u00b7) can encode semantically similar sentences to similar embeddings, G is allowed to generate semantically similar answers that do not need to be the same with the unique ground truths for each word in the training set. Because f (\u0434(h 0 )) is non-differentiable, we use reinforcement learning to transfer this information by reward. Reinforcement learning is about an agent interacting with an environment, and learning an optimal policy, by trial and error, for sequence decision making. We consider the sentence generation as a process of word sequence decision making. At each timestep t, G selects a word w t from the word space W according to its internal hidden state h t and policy that maps the hidden state space to the word space. Here, the policy is \u03c0 in Equation (4). Then, G updates its state to h t +1 and selects the next word w t +1 . After reaching the terminator <END>, G receives a reward r provided by D, as shown in Figure 4(b). The goal of reinforcement learning is to learn the optimal policy that maximizes such rewards. Suppose that s is an answer generated by G and S is the sentence space. The objective of reinforcement learning is to maximize\nJ (\u03b8 ) = E s \u223cS r (s |h 0 ),(6)\nwhere \u03b8 represents the parameters of the policy \u03c0 and J (\u00b7) is the expected reward under the distribution of possible sentences. The gradients of the objective are\n\u03b8 J = E s \u223cS \u03b8 log\u03c0 (s |h 0 )r (s |h 0 ) = E s \u223cS \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 T t =1 \u03b8 log\u03c0 (w t |h t )r (s |h 0 ) \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6(7)\nThe gradients can be backpropagated to G's LSTM and the entire encoder via h 0 .\n\nSince h 0 \u2248 f (s r ), we use s r to replace h 0 . Then the reward function is\nr (s |s r ) = f (s) \u00b7 f (s r ).(8)\nThe reward will encourage (r (s |s r ) > 0) or discourage (r (s |s r ) < 0) the generation of s. Furthermore, s 1 will be encouraged more than s 2 when r (s 1 |s r ) > r (s 2 |s r ). Since the dimension of the possible sentence space S can be very high, it is impossible to optimize Equation (2) directly. Following REINFORCE [36], we use Monte Carlo sampling to approximate the policy gradients: w t \u223c \u03c0 (\u00b7|h t ).\n\n(9) Note that the word-level training also optimizes the policy \u03c0 and can significantly reduce the sentence space S by forcing G to generate grammatically correct answers. Therefore, the wordlevel training can be considered as supplementary to the sentence-level training.\n\nIn training, we minimize the following hybrid loss:\nL = \u03b1L D + \u03b2L G \u2212 \u03b3 J,(10)\nwhere \u03b1, \u03b2, and \u03b3 are three positive factors.\n\n\nEXPERIMENTS\n\nDataset. We evaluate our proposed approach on the VisDial v0.9 dataset [5], which contains 83k dialogs on COCO-train and 40k on COCO-val images. Each dialog contains one image, one caption, and 10 question-answering rounds. The captions are from the COCO dataset [18]. The questions and answers are collected by pairing two questioners and answerers on Amazon Mechanical Turk to chat about an image. The questioners see only the image captions, and the image remains hidden to them. Their task is to ask questions about the hidden images to \"imagine the scenes better.\" According to the image and caption, the answerers answer questions asked by their chat partner. Every question is coupled with 100 candidate answer options, of which only one option is correct.\n\nEvaluation protocol. In the work of Das et al. [5], the agent takes the ground-truth answers of all previous questions as input information to generate an answer for a new question. When testing the t-th round of a dialog, all ground-truth answers from round 1 to round t \u2212 1 are provided to the system. In real-world applications, however, the ground truth answers are always not available. Therefore, we adopt a different evaluation setting from Das et al. [5]. The difference is that the ground-truth answers to the history questions are not provided to the agent. The agent uses the answers generated (G) or selected (D) by itself or does not use them. The models are asked to sort the 100 candidate answer options for each question. D uses posterior probabilities to rank these answer options, and G uses the log-likelihood of these options for ranking. The model is evaluated on the following retrieval metrics: (1) mean rank of human response (lower is better), (2) recall@k (i.e., existence of the human response in top-k ranked responses (higher is better)), and (3) mean reciprocal rank (MRR) of the human response (higher is better).\n\nImplementation details. All LSTMs in the model are single layer with 512d hidden state. For the sentence encoder, we adopt the dynamic recurrent neural network structure. We use VGG-16 [33] to extract image features. Images are resized to 224 \u00d7 224 pixels. We use the output of pool5 (7 \u00d7 7 \u00d7 512) for attention and the output of fc7 (4,096) to initialize the dialog network's state. The vocabulary consists of 8,836 words, and each word occurs at least five times in the training dataset. We use the Adam optimizer with the learning rate of 0.001 and clip the gradients by 5.0. Consistent with Das et al. [5], we split the 83k training dataset into 82k for train and 1k for val, and use the 40k as test. Each model is trained for 80 epochs. We have implemented our algorithm using both PaddlePaddle and TensorFlow, which have shown similar performance. In this article, we report the TensorFlow accuracy.\n\n\nEvaluation of the Attention Processor\n\nTo evaluate the attention processor, we design three baseline models in this section. We train and evaluate the discriminative model and the generative model separately. The baseline models are as follows:\n\n\u2022 Plain dialog network (PDN): Only the sentence encoder and the dialog network are included for each question-answering round. The output of the dialog network is directly used to initialize the decoder's state. For the preceding baseline models, at the beginning of a dialog, the image feature and text caption embedding are concatenated to initialize the state of the dialog network. For the proposed attention processor, the number of attention steps T is set to {1, 2, 3, 4, 5}. The difference between the ADN model and the proposed attention processor with T = 1 is that the attention processor includes an additional LSTM. Correctly, the ADN model outputs the attention result f concatenated with the output of the dialog network q. The attention processor outputs the state of the LSTM (i.e., the h).\n\nWe run experiments three times. Mean and standard deviation are reported in Table 1. Among the three baselines, ADN achieves the highest accuracy. IDN achieves the second highest accuracy. PDN achieves the lowest accuracy. For example, for the discriminative model, the recall@1 are 42.75, 42.96, and 43.56. The proposed recurrent attention mechanism significantly outperforms the traditional attention mechanism (ADN), especially with the discriminative model. For example, for the discriminative model, our attention processor with T = 1 achieves 46.22 at recall@1, which outperforms ADN (43.56) by 2.66.\n\nFor our attention processor with the generative model, accuracy is improved with the increase in the number of attention steps. Our model usually achieves best accuracy when T = 3 or T = 4.\n\nTo further research the attention processor's behavior, we illustrate a few attention examples in Figure 5. In this experiment, we set T = 3. First, we observe from Attention 1 that our dialog  network is capable of correctly parsing the temporal context in dialog. In the top example, the agent knows that \"on her left\" means \"on the girl's left.\" In the bottom example, the agent knows that \"it\" means \"the bus.\" Second, the attention processor exhibits different behavior according to different questions. In the top example, the attention gradually moves from the middle between \"the girl\" and \"the man on her left\" to \"the man on her left,\" and finally focuses on the body of \"the man on her left.\" For the middle example, the attention processor counts the bulls in the image sequentially. In the bottom example, the attention moves from the tail of the bus to the head to check the color.\n\n\nEvaluation of the Entire Model\n\nIn this section, we evaluate the training method proposed in Equation (10). In Section 4.1, we find that D converges faster than G. Therefore, we set \u03b1 = 0.5 and \u03b2 = 1.0. We first evaluate the discriminator-generator joint training method (\u03b3 = 0) and then evaluate the proposed sentencelevel training that reinforces G by D (\u03b3 0). The number of attention steps T is tested from {1, 2, 3, 4, 5}.\n\nThe results of the discriminator-generator joint training are shown in Table 1. Compared with separate training (\"att\"), this method, although simple, significantly improves the accuracy for both D and G. For the generative model, the best recall@1 by separate training (\"att\") is 42.79, whereas it is 43.26 by joint training (\"att+joint\").\n\nNext, we evaluate the sentence-level training method. In this experiment, we set \u03b3 = 0.1. Since this method aims to improve G and has little influence on D in the experiment, we only show the generative model's results in Table 1. As we can see from the results, sentence-level training (\"att+joint+RL\") further improves the accuracy on the basis of discriminator-generator joint training, especially for recall@5 and recall@10. For the generative model with three attention steps, the G improves 0.30 (\"att+joint\") at recall@5. \n\n\nComparison with Other Models\n\nWe have proposed an encoder-decoder model for Visual Dialog. For the encoder, we use a dialog network to memorize the temporal context of the dialog history and an attention processor to glimpse the image. For the decoder, we train the discriminator and generator jointly. We further use the discriminator to guide the generator at the sentence level. Experiments show that our model significantly improves accuracy compared to the late fusion (LF), hierarchical recurrent encoder with attention (HREA), memory network (MN), and HCIAE models. We compare our model with three models proposed in the work of Das et al. [5]: the LF encoder, HREA, and MN. We implement the three models by ourselves and apply it to our setting. As strong competitors, we also include HCIAE [19] models. For HCIAE models, the HCIAE-G(enerative)-DIS(criminator), which is trained under the mixed MLE and discriminator loss (knowledge transfer), is the best generative model, whereas the HCIAE-D(iscriminative)-NP(air)-ATT(entive), which is trained under the n-pair discriminative loss and using the selfattentive answer encoding, is the best discriminative model. Therefore, we use these two models to represent HCIAE models. Note that we do not directly compare our results to those numbers reported in the work of Das et al. [5] and Lu et al. [19] because we do not use the history ground-truth answers for evaluation. Instead, as we illustrated in the evaluation protocol, we use the answers selected or generated by the models. For our model, we set T = 3 since glimpsing three times achieves the best performance on the validation dataset. The results are shown in Table 2.\n\nAs can be seen from the results, our method outperforms all three models in the work of Das et al. [5]. For example, for the generative model evaluated by recall@1, our model outperforms LF by 1.7, HREA by 1.59, and MN by 1.98. Our model also outperforms the two HCIAE models in the work of Lu et al. [19]. For example, for the generative model evaluated by recall@1, our model outperforms HCIAE-G-DIS by 1.18. For the discriminative model evaluated by recall@1, our model outperforms HCIAE-D-NP-ATT by 1.47.\n\nWe present some examples of answers generated by our model in Figure 6. Occasionally, the model generates more accurate answers than the ground truths. For the example in the \"elephant\" dialog, when asked \"what else do you see?\" the model generates \"a rock wall,\" which is more accurate than the ground truth \"nothing.\" For the example in the last dialog, when asked \"how is the weather\" the model generates \"cloudy,\" which is more accurate than the ground truth \"it's sunny.\" The impact of generating answers with and without ground-truth answers is shown in Figure 7.\n\nWe also compare our model with other methods with the evaluation strategy proposed by Das et al. [5], in which the ground-truth answers to the history questions are available to the agent. Apart from the LF, HERA, MN, and HCIAE models, we also include the hierarchical recurrent encoder (HER) [5], stacked attention network with question and image (SAN-QI) [45], hierarchical   7. Examples of generated answers with and without ground-truth answers. The red color indicates that the generated answer is different from the ground truth. Since our framework does not use answers, there is no difference between using ground-truth answers or not. For other methods, leveraging groundtruth answers can help the agent generate more reasonable answers. However, ground-truth answers are not available in practice.\n\nquestion-image co-attention (HieCoAtt-QI) [20], AMEM [32], and co-attention [40] models in this experiment. Numbers are exactly as reported in prior works. In this experiment, the number of glimpses is set to 3. The results are shown in Table 3. Compared with Table 2, the ground-truth answers to the history question help the agent answer the future questions. For example, the ground-truth history answers help HCIAE-D-NP-ATT reduce mean rank by 1.16. Since the HCIAE [19] and co-attention [40] models bravely exploit the ground-truth question-answer pairs (i.e.) and facts, and apply attention mechanisms to the facts, they achieve high accuracy. However, our model considers the answers unreliable and does not use them well, and thus it does not outperform the HCIAE and co-attention models in this experiment. \n\n\nCONCLUSION\n\nWe have proposed an encoder-decoder model for Visual Dialog. For the encoder, we use a dialog network to memorize the temporal context of the dialog history and an attention processor to glimpse the image. For the decoder, we train the discriminator and generator jointly. We further use the discriminator to guide the generator at the sentence level. Experiments show that our model significantly improves accuracy compared to the LF, HREA, MN, and HCIAE models.\n\nFig. 1 .\n1An example of Visual Dialog. Given an image and its caption, the agent answers a series of questions.\n\nFig. 2 .\n2The architecture of the proposed recurrent attention network. The encoder (A)\n\nFig. 3 .\n3Illustration of how the attention processor grounds a question in an image. First, according to dialog history, the dialog network parse the pronoun \"her.\" Then, based on the output of the dialog network, the attention processor glimpses the image to answer the question. Later,Figure 5demonstrates three examples of the behavior of our attention processor.\n\nFig. 4 .\n4The architecture of the decoder. (A) Joint training of discriminator (D) and generator (G). (B) Reinforcing G by D.\n\n\u2022\nImage dialog network (IDN):The image information is added on the basis of the PDN for each question-answering round. The output of the dialog network and the image feature are first normalized and then concatenated to initialize the decoder's state.\u2022 Attention dialog network (ADN): The image information in the IDN is replaced by a singlestep attention mechanism, which does not contain an LSTM. The output of the dialog network and attended image feature are also normalized before being concatenated to initialize the decoder's state.\n\nFig. 5 .\n5Visualization examples of how the attention processor grounds questions in images.\n\nFig. 6 .\n6Examples of generated answers by our model. The red color indicates the generated answer is different with the ground truth.\n\nFig.\nFig. 7. Examples of generated answers with and without ground-truth answers. The red color indicates that the generated answer is different from the ground truth. Since our framework does not use answers, there is no difference between using ground-truth answers or not. For other methods, leveraging groundtruth answers can help the agent generate more reasonable answers. However, ground-truth answers are not available in practice.\n\nTable 1 .\n1Experimental Results on the VisDial DatasetModels \nMRR \nR@1 \nR@5 \nR@10 \nMean \n\nGenerative model \n\nbaseline \nPDN \n0.4993\u00b10.0003 \n40.96\u00b10.03 \n58.32\u00b10.01 \n60.98\u00b10.08 \n22.85\u00b10.02 \nIDN \n0.5023\u00b10.0008 \n41.72\u00b10.06 \n58.44\u00b10.01 \n61.26\u00b10.11 \n22.81\u00b10.07 \nADN \n0.5035\u00b10.0002 \n42.00\u00b10.10 \n58.56\u00b10.03 \n61.32\u00b10.03 \n22.75\u00b10.01 \n\njoint \n\nPDN \n0.5003\u00b10.0004 \n40.93\u00b10.01 \n58.47\u00b10.12 \n61.13\u00b10.05 \n22.83\u00b10.02 \nIDN \n0.5026\u00b10.0005 \n41.81\u00b10.05 \n58.77\u00b10.11 \n61.35\u00b10.04 \n22.74\u00b10.04 \nADN \n0.5034\u00b10.0002 \n42.17\u00b10.08 \n58.64\u00b10.07 \n61.43\u00b10.02 \n22.71\u00b10.08 \n\nRL \n\nPDN \n0.5002\u00b10.0008 \n41.04\u00b10.04 \n58.52\u00b10.05 \n61.12\u00b10.12 \n22.76\u00b10.01 \nIDN \n0.5032\u00b10.0010 \n41.93\u00b10.12 \n59.02\u00b10.09 \n61.48\u00b10.06 \n22.75\u00b10.02 \nADN \n0.5045\u00b10.0003 \n42.04\u00b10.05 \n59.09\u00b10.04 \n61.70\u00b10.05 \n22.64\u00b10.02 \n\njoint+RL \nPDN \n0.5009\u00b10.0003 \n41.15\u00b10.13 \n58.57\u00b10.09 \n61.20\u00b10.10 \n22.75\u00b10.04 \nIDN \n0.5044\u00b10.0005 \n42.09\u00b10.05 \n59.04\u00b10.14 \n61.61\u00b10.08 \n22.73\u00b10.06 \nADN \n0.5053\u00b10.0007 \n42.17\u00b10.03 \n59.13\u00b10.07 \n61.78\u00b10.12 \n22.66\u00b10.03 \n\natt \n\nT = 1 \n0.5028\u00b10.0003 \n41.77\u00b10.04 \n58.33\u00b10.03 \n60.74\u00b10.04 \n22.81\u00b10.02 \nT = 2 \n0.5092\u00b10.0002 \n42.61\u00b10.00 \n58.83\u00b10.02 \n61.34\u00b10.03 \n22.72\u00b10.05 \nT = 3 \n0.5105\u00b10.0001 \n42.79\u00b10.03 \n58.90\u00b10.02 \n61.53\u00b10.03 \n22.62\u00b10.02 \nT = 4 \n0.5098\u00b10.0004 \n42.70\u00b10.03 \n58.77\u00b10.05 \n61.45\u00b10.03 \n22.67\u00b10.01 \nT = 5 \n0.5093\u00b10.0002 \n42.59\u00b10.09 \n58.74\u00b10.03 \n61.29\u00b10.02 \n22.70\u00b10.04 \n\natt+joint \n\nT = 1 \n0.5129\u00b10.0005 \n43.15\u00b10.05 \n58.97\u00b10.10 \n61.55\u00b10.05 \n22.56\u00b10.04 \nT = 2 \n0.5132\u00b10.0002 \n43.23\u00b10.04 \n59.07\u00b10.02 \n61.58\u00b10.03 \n22.47\u00b10.02 \nT = 3 \n0.5135\u00b10.0002 \n43.23\u00b10.03 \n59.08\u00b10.07 \n61.62\u00b10.03 \n22.40\u00b10.03 \nT = 4 \n0.5140\u00b10.0004 \n43.26\u00b10.02 \n59.13\u00b10.04 \n61.73\u00b10.09 \n22.28\u00b10.07 \nT = 5 \n0.5132\u00b10.0003 \n43.15\u00b10.03 \n59.02\u00b10.04 \n61.52\u00b10.07 \n22.49\u00b10.04 \n\natt+RL \n\nT = 1 \n0.5131\u00b10.0003 \n43.18\u00b10.01 \n58.95\u00b10.08 \n61.55\u00b10.08 \n22.53\u00b10.03 \nT = 2 \n0.5136\u00b10.0003 \n43.26\u00b10.05 \n59.13\u00b10.02 \n61.57\u00b10.05 \n22.48\u00b10.05 \nT = 3 \n0.5141\u00b10.0002 \n43.29\u00b10.04 \n59.15\u00b10.03 \n61.64\u00b10.02 \n22.41\u00b10.02 \nT = 4 \n0.5138\u00b10.0002 \n43.29\u00b10.07 \n59.18\u00b10.04 \n61.70\u00b10.00 \n22.34\u00b10.01 \nT = 5 \n0.5133\u00b10.0001 \n43.17\u00b10.03 \n58.90\u00b10.02 \n61.39\u00b10.04 \n22.52\u00b10.02 \n\natt+joint+RL \n\nT = 1 \n0.5142\u00b10.0004 \n43.26\u00b10.02 \n59.15\u00b10.05 \n61.820\u00b10.02 \n22.40\u00b10.03 \nT = 2 \n0.5145\u00b10.0003 \n43.37\u00b10.03 \n59.29\u00b10.09 \n61.88\u00b10.08 \n22.37\u00b10.03 \nT = 3 \n0.5148\u00b10.0001 \n43.39\u00b10.05 \n59.38\u00b10.02 \n61.97\u00b10.06 \n22.32\u00b10.04 \nT = 4 \n0.5154\u00b10.0002 \n43.42\u00b10.02 \n59.31\u00b10.03 \n62.04\u00b10.05 \n22.24\u00b10.02 \nT = 5 \n0.5140\u00b10.0001 \n43.32\u00b10.01 \n59.08\u00b10.03 \n61.87\u00b10.01 \n22.41\u00b10.04 \n\n\nTable 2 .\n2Comparison with the LF Encoder, HREA, MN, and HCIAEModel \n\nGenerative Model \nDiscriminative Model \nMRR \nR@1 \nR@5 R@10 Mean \nMRR \nR@1 \nR@5 R@10 Mean \n\nLF [5] \n0.5033 41.69 58.39 \n60.94 \n22.87 \n0.5794 44.00 74.03 \n83.11 \n6.09 \nHREA [5] \n0.5032 41.80 58.20 \n60.77 \n22.52 \n0.5762 43.37 74.72 \n84.09 \n5.71 \nMN [5] \n0.5004 41.41 58.19 \n60.64 \n23.18 \n0.5763 43.79 73.76 \n83.80 \n6.23 \nHCIAE-G-DIS [19] \n0.5094 42.21 59.01 \n61.33 \n22.38 \n-\n-\n-\n-\n-\nHCIAE-D-NP-ATT \n[19] \n\n-\n-\n-\n-\n-\n0.5905 44.98 76.11 \n85.12 \n5.63 \n\nOurs \n0.5148 43.39 59.38 \n61.97 \n22.32 \n0.6024 46.45 76.91 \n86.34 \n5.10 \n\n\n\nTable 3 .\n3Comparison with Other Methods with the Evaluation Protocol Proposed by Das et al.[5], in Which the Ground-Truth Answers to the History Questions Are Available to the AgentModel \nGenerative Model \nDiscriminative Model \nMRR \nR@1 R@5 R@10 Mean \nMRR \nR@1 R@5 R@10 Mean \nLF [5] \n0.5204 42.04 61.78 \n67.66 \n16.84 0.5807 43.82 74.68 \n84.07 \n5.78 \nHER [5] \n0.5237 42.29 62.18 \n67.92 \n17.07 0.5846 44.67 74.50 \n84.22 \n5.72 \nHREA [5] \n0.5242 42.29 62.33 \n68.17 \n16.79 0.5868 44.82 74.81 \n84.36 \n5.66 \nMN [5] \n0.5259 42.29 62.85 \n68.88 \n17.06 0.5965 45.55 76.22 \n85.37 \n5.46 \nSAN-QI [45] \n-\n-\n-\n-\n-\n0.5764 43.44 74.26 \n83.72 \n5.88 \nHieCoAtt-QI [20] \n-\n-\n-\n-\n-\n0.5788 43.51 74.49 \n83.96 \n5.84 \nAMEM [32] \n-\n-\n-\n-\n-\n0.6160 47.74 78.04 \n86.84 \n4.99 \nHCIAE-G-DIS [19] \n0.5467 44.35 65.28 \n71.55 \n14.23 \n-\n-\n-\n-\n-\nHCIAE-D-NP-ATT \n[19] \n\n-\n-\n-\n-\n-\n0.6222 48.48 78.75 \n87.59 \n4.81 \n\nOurs \n0.5450 44.40 65.23 \n70.88 \n14.54 0.6227 48.63 78.62 \n87.49 \n4.95 \n\n\nACM Trans. Multimedia Comput. Commun. Appl., Vol. 16, No. 3, Article 78. Publication date: July 2020.\n\nNeural module networks. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, 10.1109/CVPR.2016.12Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16. the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16). 39-48. DOI:https://doi.org/10.1109/ CVPR.2016.12\n\nVQA: Visual question answering. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, 10.1109/ICCV.2015.279Proceedingsof the 2015 IEEE InternationalConference on Computer Vision (ICCV'15. of the 2015 IEEE InternationalConference on Computer Vision (ICCV'15Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual question answering. In Proceedingsof the 2015 IEEE InternationalConference on Computer Vision (ICCV'15). 2425-2433. DOI:https://doi.org/10.1109/ICCV.2015.279\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv:1409.0473.\n\nDeep attention neural tensor network for visual question answering. Yalong Bai, Jianlong Fu, Tiejun Zhao, Tao Mei, 10.1007/978-3-030-01258-8_2Proceedings of the 15th European Conference on Computer Vision (ECCV'18. the 15th European Conference on Computer Vision (ECCV'18Yalong Bai, Jianlong Fu, Tiejun Zhao, and Tao Mei. 2018. Deep attention neural tensor network for visual question answering. In Proceedings of the 15th European Conference on Computer Vision (ECCV'18). 21-37. DOI:https://doi.org/ 10.1007/978-3-030-01258-8_2\n\nVisual dialog. Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, M F Jos\u00e9, Devi Moura, Dhruv Parikh, Batra, 10.1109/CVPR.2017.121Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17. the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 M. F. Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17). 1080-1089. DOI:https://doi.org/10.1109/CVPR.2017.121\n\nLearning cooperative visual dialog agents with deep reinforcement learning. Abhishek Das, Satwik Kottur, M F Jos\u00e9, Stefan Moura, Dhruv Lee, Batra, 10.1109/ICCV.2017.321Proceedings of the IEEE International Conference on Computer Vision (ICCV'17. the IEEE International Conference on Computer Vision (ICCV'17Abhishek Das, Satwik Kottur, Jos\u00e9 M. F. Moura, Stefan Lee, and Dhruv Batra. 2017. Learning cooperative visual dialog agents with deep reinforcement learning. In Proceedings of the IEEE International Conference on Computer Vision (ICCV'17). 2970-2979. DOI:https://doi.org/10.1109/ICCV.2017.321\n\nAdaptive exploration for unsupervised person reidentification. Yuhang Ding, Hehe Fan, Mingliang Xu, Yi Yang, 10.1145/3369393ACM Transactions on Multimedia Computing. 16Communications, and ApplicationsYuhang Ding, Hehe Fan, Mingliang Xu, and Yi Yang. 2020. Adaptive exploration for unsupervised person re- identification. ACM Transactions on Multimedia Computing, Communications, and Applications 16, 1 (2020), Article 3. DOI:https://doi.org/10.1145/3369393\n\nLong-term recurrent convolutional networks for visual recognition and description. Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell, 10.1109/TPAMI.2016.2599174IEEE Transactions on Pattern Analysis and Machine Intelligence. 39Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, and Trevor Darrell. 2017. Long-term recurrent convolutional networks for visual recognition and description. IEEE Transactions on Pattern Analysis and Machine Intelligence 39, 4 (2017), 677-691. DOI:https://doi.org/10.1109/TPAMI. 2016.2599174\n\nWatching a small portion could be as good as watching all: Towards efficient video classification. Hehe Fan, Zhongwen Xu, Linchao Zhu, Chenggang Yan, Jianjun Ge, Yi Yang, 10.24963/ijcai.2018/98Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI'18. the 27th International Joint Conference on Artificial Intelligence (IJCAI'18Hehe Fan, Zhongwen Xu, Linchao Zhu, Chenggang Yan, Jianjun Ge, and Yi Yang. 2018. Watching a small portion could be as good as watching all: Towards efficient video classification. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI'18). 705-711. DOI:https://doi.org/10.24963/ijcai.2018/98\n\nUnsupervised person re-identification: Clustering and fine-tuning. Hehe Fan, Liang Zheng, Chenggang Yan, Yi Yang, 10.1145/3243316Article 83. 14Communications, and ApplicationsHehe Fan, Liang Zheng, Chenggang Yan, and Yi Yang. 2018. Unsupervised person re-identification: Clustering and fine-tuning. ACM Transactions on Multimedia Computing, Communications, and Applications 14, 4 (2018), Article 83. DOI:https://doi.org/10.1145/3243316\n\nFrom captions to visual concepts and back. Saurabh Hao Fang, Forrest N Gupta, Rupesh Kumar Iandola, Li Srivastava, Piotr Deng, Jianfeng Doll\u00e1r, Gao, 10.1109/CVPR.2015.7298754Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'15. the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'15Hao Fang, Saurabh Gupta, Forrest N. Iandola, Rupesh Kumar Srivastava, Li Deng, Piotr Doll\u00e1r, Jianfeng Gao, et al. 2015. From captions to visual concepts and back. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'15). 1473-1482. DOI:https://doi.org/10.1109/CVPR.2015.7298754\n\nCascaded revision network for novel object captioning. Q Feng, Y Wu, H Fan, C Yan, M Xu, Y Yang, 10.1109/TCSVT.2020.2965966IEEE Transactions on Circuits and Systems for Video Technology. Early Access. Q. Feng, Y. Wu, H. Fan, C. Yan, M. Xu, and Y. Yang. 2020. Cascaded revision network for novel object captioning. IEEE Transactions on Circuits and Systems for Video Technology. Early Access. DOI:https://doi.org/10.1109/TCSVT. 2020.2965966\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Computation. 9Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9, 8 (1997), 1735-1780. DOI:https://doi.org/10.1162/neco.1997.9.8.1735\n\nSegmentation from natural language expressions. Ronghang Hu, Marcus Rohrbach, Trevor Darrell, 10.1007/978-3-319-46448-0_7Proceedings of the 14th European Conference on Computer Vision (ECCV'16), Part I. 108-124. the 14th European Conference on Computer Vision (ECCV'16), Part I. 108-124Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. 2016. Segmentation from natural language expressions. In Pro- ceedings of the 14th European Conference on Computer Vision (ECCV'16), Part I. 108-124. DOI:https://doi.org/10.1007/ 978-3-319-46448-0_7\n\nDeep visual-semantic alignments for generating image descriptions. Andrej Karpathy, Li Fei-Fei, 10.1109/TPAMI.2016.2598339IEEE Transactions on Pattern Analysis and Machine Intelligence. 39Andrej Karpathy and Li Fei-Fei. 2017. Deep visual-semantic alignments for generating image descriptions. IEEE Trans- actions on Pattern Analysis and Machine Intelligence 39, 4 (2017), 664-676. DOI:https://doi.org/10.1109/TPAMI.2016. 2598339\n\nWhat are you talking about? Text-toimage coreference. Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun, Sanja Fidler, 10.1109/CVPR.2014.455Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'14. the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'14Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun, and Sanja Fidler. 2014. What are you talking about? Text-to- image coreference. In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'14). 3558-3565. DOI:https://doi.org/10.1109/CVPR.2014.455\n\nJointly localizing and describing events for dense video captioning. Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, Tao Mei, Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'18. the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'18Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. 2018. Jointly localizing and describing events for dense video captioning. In Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'18). 7492-7500.\n\nMicrosoft COCO: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge J Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, 10.1007/978-3-319-10602-1_48Proceedings of the 13th European Conference on Computer Vision (ECCV'14), Part V. 740-755. the 13th European Conference on Computer Vision (ECCV'14), Part V. 740-755Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In Proceedings of the 13th European Confer- ence on Computer Vision (ECCV'14), Part V. 740-755. DOI:https://doi.org/10.1007/978-3-319-10602-1_48\n\nBest of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model. Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, and Dhruv Batra. 2017. Best of both worlds: Transferring knowl- edge from discriminative learning to a generative visual dialog model. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. 313-323.\n\nHierarchical question-image co-attention for visual question answering. Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems. Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016. Hierarchical question-image co-attention for visual question answering. In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Informa- tion Processing Systems 2016. 289-297.\n\nAsk your neurons: A deep learning approach to visual question answering. Mateusz Malinowski, Marcus Rohrbach, Mario Fritz, 10.1007/s11263-017-1038-2International Journal of Computer Vision. 125Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2017. Ask your neurons: A deep learning approach to visual question answering. International Journal of Computer Vision 125, 1-3 (2017), 110-135. DOI:https://doi.org/10.1007/ s11263-017-1038-2\n\nRecurrent models of visual attention. Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014. Recurrent models of visual attention. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014. 2204-2212.\n\nHuman-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, 10.1038/nature14236Nature. 518Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. 2015. Human-level control through deep reinforcement learning. Nature 518, 7540 (2015), 529-533. DOI:https:// doi.org/10.1038/nature14236\n\nHierarchical recurrent neural encoder for video representation with application to captioning. Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang, 10.1109/CVPR.2016.117Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16. the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, and Yueting Zhuang. 2016. Hierarchical recurrent neural encoder for video representation with application to captioning. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16). 1029-1038. DOI:https://doi.org/10.1109/CVPR.2016.117\n\nFlickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. A Bryan, Liwei Plummer, Chris M Wang, Juan C Cervantes, Julia Caicedo, Svetlana Hockenmaier, Lazebnik, 10.1007/s11263-016-0965-7International Journal of Computer Vision. 123Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2017. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. Interna- tional Journal of Computer Vision 123, 1 (2017), 74-93. DOI:https://doi.org/10.1007/s11263-016-0965-7\n\nLinking people in videos with \"their\" names using coreference resolution. Vignesh Ramanathan, Armand Joulin, Percy Liang, Fei-Fei Li, 10.1007/978-3-319-10590-1_7Proceedings of the 13th European Conference on Computer Vision (ECCV'14), Part I. 95-110. the 13th European Conference on Computer Vision (ECCV'14), Part I. 95-110Vignesh Ramanathan, Armand Joulin, Percy Liang, and Fei-Fei Li. 2014. Linking people in videos with \"their\" names using coreference resolution. In Proceedings of the 13th European Conference on Computer Vision (ECCV'14), Part I. 95-110. DOI:https://doi.org/10.1007/978-3-319-10590-1_7\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level training with re- current neural networks. arXiv:1511.06732.\n\nExploring models and data for image question answering. Mengye Ren, Ryan Kiros, Richard S Zemel, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Mengye Ren, Ryan Kiros, and Richard S. Zemel. 2015. Exploring models and data for image question answering. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015. 2953-2961.\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jarret Mroueh, Vaibhava Ross, Goel, 10.1109/CVPR.2017.131Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17. the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17). 1179-1195. DOI:https://doi.org/10.1109/CVPR.2017.131\n\nGrounding of textual phrases in images by reconstruction. Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, Bernt Schiele, 10.1007/978-3-319-46448-0_49Proceedings of the 14th European Conference on Computer Vision (ECCV'16), Part I. 817-834. the 14th European Conference on Computer Vision (ECCV'16), Part I. 817-834Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. 2016. Grounding of textual phrases in images by reconstruction. In Proceedings of the 14th European Conference on Computer Vision (ECCV'16), Part I. 817-834. DOI:https://doi.org/10.1007/978-3-319-46448-0_49\n\nA dataset for movie description. Anna Rohrbach, Marcus Rohrbach, Niket Tandon, Bernt Schiele, 10.1109/CVPR.2015.7298940Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'15. the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'15Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. 2015. A dataset for movie description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR'15). 3202-3212. DOI:https:// doi.org/10.1109/CVPR.2015.7298940\n\nVisual reference resolution using attention memory for visual dialog. Andreas Paul Hongsuck Seo, Bohyung Lehrmann, Leonid Han, Sigal, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, and Leonid Sigal. 2017. Visual reference resolution using attention memory for visual dialog. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. 3722-3732.\n\nKaren Simonyan, Andrew Zisserman, arXiv:1409.1556Very deep convolutional networks for large-scale image recognition. Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556.\n\nImage captioning with affective guiding and selective attention. Anqi Wang, Haifeng Hu, Liang Yang, 10.1145/3226037ACM Transactions on Multimedia Computing. 1473Communications, and ApplicationsAnqi Wang, Haifeng Hu, and Liang Yang. 2018. Image captioning with affective guiding and selective attention. ACM Transactions on Multimedia Computing, Communications, and Applications 14, 3 (2018), Article 73, 15 pages. DOI:https://doi.org/10.1145/3226037\n\nQ-learning. J C H Christopher, Peter Watkins, Dayan, 10.1007/BF00992698Machine Learning. 8Christopher J. C. H. Watkins and Peter Dayan. 1992. Q-learning. Machine Learning 8 (1992), 279-292. DOI: https://doi.org/10.1007/BF00992698\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. J Ronald, Williams, 10.1007/BF00992696Machine Learning. 8Ronald J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8 (1992), 229-256. DOI:https://doi.org/10.1007/BF00992696\n\nImage captioning via semantic guidance attention and consensus selection strategy. Jie Wu, Haifeng Hu, Yi Wu, 10.1145/3271485ACM Transactions on Multimedia Computing. 14Communications, and ApplicationsJie Wu, Haifeng Hu, and Yi Wu. 2018. Image captioning via semantic guidance attention and consensus selection strategy. ACM Transactions on Multimedia Computing, Communications, and Applications 14, 4 (2018), Article 87. DOI:https://doi.org/10.1145/3271485\n\nWhat value do explicit high level concepts have in vision to language problems?. Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony R Dick, Anton Van Den, Hengel, Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16. the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony R. Dick, and Anton van den Hengel. 2016. What value do explicit high level concepts have in vision to language problems? In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16). 203-212.\n\nImage captioning and visual question answering based on attributes and external knowledge. Qi Wu, Chunhua Shen, Peng Wang, Anthony R Dick, Anton Van Den, Hengel, 10.1109/TPAMI.2017.2708709IEEE Transactions on Pattern Analysis and Machine Intelligence. 40Qi Wu, Chunhua Shen, Peng Wang, Anthony R. Dick, and Anton van den Hengel. 2018. Image captioning and visual question answering based on attributes and external knowledge. IEEE Transactions on Pattern Analysis and Machine Intelligence 40, 6 (2018), 1367-1381. DOI:https://doi.org/10.1109/TPAMI.2017.2708709\n\nAre you talking to me? Reasoned visual dialog generation through adversarial learning. Qi Wu, Peng Wang, Chunhua Shen, Ian D Reid, Anton Van Den, Hengel, Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'18). the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'18)Qi Wu, Peng Wang, Chunhua Shen, Ian D. Reid, and Anton van den Hengel. 2018. Are you talking to me? Reasoned visual dialog generation through adversarial learning. In Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'18).\n\nRevisiting EmbodiedQA: A simple baseline and beyond. Yu Wu, Lu Jiang, Yi Yang, 10.1109/TIP.2020.2967584IEEE Transactions on Image Processing. 29Yu Wu, Lu Jiang, and Yi Yang. 2020. Revisiting EmbodiedQA: A simple baseline and beyond. IEEE Transactions on Image Processing 29 (2020), 3984-3992. DOI:https://doi.org/10.1109/TIP.2020.2967584\n\nLearning multimodal attention LSTM networks for video captioning. Jun Xu, Ting Yao, Yongdong Zhang, Tao Mei, 10.1145/3123266.3123448Proceedings of the 2017 ACM Conference on Multimedia (MM'17. the 2017 ACM Conference on Multimedia (MM'17Jun Xu, Ting Yao, Yongdong Zhang, and Tao Mei. 2017. Learning multimodal attention LSTM networks for video captioning. In Proceedings of the 2017 ACM Conference on Multimedia (MM'17). 537-545. DOI:https://doi.org/10.1145/ 3123266.3123448\n\nShow, attend and tell: Neural image caption generation with visual attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov, Richard S Zemel, Yoshua Bengio, Proceedings of the 32nd International Conference on Machine Learning (ICML'15. the 32nd International Conference on Machine Learning (ICML'15Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine Learning (ICML'15). 2048-2057.\n\nImage classification by cross-media active learning with privileged information. Yan Yan, Feiping Nie, Wen Li, Chenqiang Gao, Yi Yang, Dong Xu, 10.1109/TMM.2016.2602938IEEE Transactions on Multimedia. 18Yan Yan, Feiping Nie, Wen Li, Chenqiang Gao, Yi Yang, and Dong Xu. 2016. Image classification by cross-media active learning with privileged information. IEEE Transactions on Multimedia 18, 12 (2016), 2494-2502. DOI:https:// doi.org/10.1109/TMM.2016.2602938\n\nStacked attention networks for image question answering. Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alexander J Smola, 10.1109/CVPR.2016.10Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16. the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alexander J. Smola. 2016. Stacked attention networks for image question answering. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16). 21-29. DOI:https://doi.org/10.1109/CVPR.2016.10\n\nMulti-level attention networks for visual question answering. Dongfei Yu, Jianlong Fu, Tao Mei, Yong Rui, 10.1109/CVPR.2017.446Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17. the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17Dongfei Yu, Jianlong Fu, Tao Mei, and Yong Rui. 2017. Multi-level attention networks for visual question answering. In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'17). 4187-4195. DOI:https:// doi.org/10.1109/CVPR.2017.446\n\nUncovering the temporal context for video question answering. Linchao Zhu, Zhongwen Xu, Yi Yang, Alexander G Hauptmann, 10.1007/s11263-017-1033-7International Journal of Computer Vision. 124Linchao Zhu, Zhongwen Xu, Yi Yang, and Alexander G. Hauptmann. 2017. Uncovering the temporal context for video question answering. International Journal of Computer Vision 124, 3 (2017), 409-421. DOI:https://doi.org/10. 1007/s11263-017-1033-7\n\nVisual7W: Grounded question answering in images. Yuke Zhu, Oliver Groth, Michael S Bernstein, Fei-Fei Li, 10.1109/CVPR.2016.540Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16. the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Fei-Fei Li. 2016. Visual7W: Grounded question answering in im- ages. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR'16). 4995-5004. DOI:https://doi.org/10.1109/CVPR.2016.540\n", "annotations": {"author": "[{\"end\":333,\"start\":90},{\"end\":577,\"start\":334},{\"end\":824,\"start\":578},{\"end\":1067,\"start\":825},{\"end\":1309,\"start\":1068}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":95},{\"end\":342,\"start\":339},{\"end\":589,\"start\":586},{\"end\":832,\"start\":828}]", "author_first_name": "[{\"end\":94,\"start\":90},{\"end\":338,\"start\":334},{\"end\":585,\"start\":578},{\"end\":827,\"start\":825},{\"end\":1071,\"start\":1068},{\"end\":1074,\"start\":1072}]", "author_affiliation": "[{\"end\":332,\"start\":100},{\"end\":576,\"start\":344},{\"end\":823,\"start\":591},{\"end\":1066,\"start\":834},{\"end\":1308,\"start\":1076}]", "title": "[{\"end\":72,\"start\":1},{\"end\":1381,\"start\":1310}]", "venue": "[{\"end\":1425,\"start\":1383}]", "abstract": "[{\"end\":2907,\"start\":1647}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2964,\"start\":2961},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2966,\"start\":2964},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2969,\"start\":2966},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2972,\"start\":2969},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2975,\"start\":2972},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3578,\"start\":3575},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3581,\"start\":3578},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3584,\"start\":3581},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3587,\"start\":3584},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4694,\"start\":4690},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4993,\"start\":4990},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4996,\"start\":4993},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5366,\"start\":5363},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6470,\"start\":6466},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6473,\"start\":6470},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6545,\"start\":6542},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6757,\"start\":6753},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6790,\"start\":6787},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6793,\"start\":6790},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6858,\"start\":6855},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6861,\"start\":6858},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6864,\"start\":6861},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6867,\"start\":6864},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6870,\"start\":6867},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6873,\"start\":6870},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6876,\"start\":6873},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6879,\"start\":6876},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6902,\"start\":6898},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6905,\"start\":6902},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6908,\"start\":6905},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6911,\"start\":6908},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6953,\"start\":6949},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6956,\"start\":6953},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6959,\"start\":6956},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6962,\"start\":6959},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6965,\"start\":6962},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6974,\"start\":6971},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6976,\"start\":6974},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6979,\"start\":6976},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6982,\"start\":6979},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6985,\"start\":6982},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6988,\"start\":6985},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7011,\"start\":7008},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7013,\"start\":7011},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7016,\"start\":7013},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7019,\"start\":7016},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7022,\"start\":7019},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7351,\"start\":7348},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7354,\"start\":7351},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7370,\"start\":7367},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7488,\"start\":7485},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7491,\"start\":7488},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7734,\"start\":7731},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7737,\"start\":7734},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7770,\"start\":7767},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7772,\"start\":7770},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7775,\"start\":7772},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7778,\"start\":7775},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7781,\"start\":7778},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7784,\"start\":7781},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7815,\"start\":7811},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7937,\"start\":7933},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8164,\"start\":8160},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8385,\"start\":8381},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9345,\"start\":9341},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9361,\"start\":9357},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9574,\"start\":9570},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9759,\"start\":9755},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9892,\"start\":9889},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9965,\"start\":9961},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10072,\"start\":10068},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10142,\"start\":10139},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10246,\"start\":10242},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14757,\"start\":14753},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16868,\"start\":16865},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21939,\"start\":21935},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22513,\"start\":22510},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22706,\"start\":22702},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23254,\"start\":23251},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23666,\"start\":23663},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24539,\"start\":24535},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24959,\"start\":24956},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28116,\"start\":28112},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29962,\"start\":29959},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30115,\"start\":30111},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30649,\"start\":30646},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30668,\"start\":30664},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31101,\"start\":31098},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31304,\"start\":31300},{\"end\":31408,\"start\":31404},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32180,\"start\":32177},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32376,\"start\":32373},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32441,\"start\":32437},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32935,\"start\":32931},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32946,\"start\":32942},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32969,\"start\":32965},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33363,\"start\":33359},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33385,\"start\":33381},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":39193,\"start\":39190}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34296,\"start\":34184},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34385,\"start\":34297},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34754,\"start\":34386},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34881,\"start\":34755},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35422,\"start\":34882},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35516,\"start\":35423},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35652,\"start\":35517},{\"attributes\":{\"id\":\"fig_7\"},\"end\":36093,\"start\":35653},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38503,\"start\":36094},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39096,\"start\":38504},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40047,\"start\":39097}]", "paragraph": "[{\"end\":3513,\"start\":2923},{\"end\":3877,\"start\":3515},{\"end\":4449,\"start\":3879},{\"end\":4909,\"start\":4451},{\"end\":5321,\"start\":4911},{\"end\":5943,\"start\":5323},{\"end\":6651,\"start\":5945},{\"end\":7075,\"start\":6668},{\"end\":7641,\"start\":7077},{\"end\":8981,\"start\":7643},{\"end\":9658,\"start\":8983},{\"end\":11618,\"start\":9660},{\"end\":12329,\"start\":11628},{\"end\":12874,\"start\":12341},{\"end\":13426,\"start\":12876},{\"end\":14481,\"start\":13428},{\"end\":14824,\"start\":14483},{\"end\":15625,\"start\":14826},{\"end\":15806,\"start\":15672},{\"end\":15960,\"start\":15858},{\"end\":16376,\"start\":16171},{\"end\":17086,\"start\":16497},{\"end\":17556,\"start\":17088},{\"end\":17767,\"start\":17609},{\"end\":17906,\"start\":17769},{\"end\":18329,\"start\":18085},{\"end\":18735,\"start\":18404},{\"end\":19357,\"start\":18791},{\"end\":19852,\"start\":19359},{\"end\":21103,\"start\":19922},{\"end\":21299,\"start\":21136},{\"end\":21494,\"start\":21414},{\"end\":21573,\"start\":21496},{\"end\":22023,\"start\":21609},{\"end\":22297,\"start\":22025},{\"end\":22350,\"start\":22299},{\"end\":22423,\"start\":22378},{\"end\":23202,\"start\":22439},{\"end\":24348,\"start\":23204},{\"end\":25255,\"start\":24350},{\"end\":25502,\"start\":25297},{\"end\":26311,\"start\":25504},{\"end\":26919,\"start\":26313},{\"end\":27110,\"start\":26921},{\"end\":28007,\"start\":27112},{\"end\":28436,\"start\":28042},{\"end\":28778,\"start\":28438},{\"end\":29309,\"start\":28780},{\"end\":30997,\"start\":29342},{\"end\":31507,\"start\":30999},{\"end\":32078,\"start\":31509},{\"end\":32887,\"start\":32080},{\"end\":33705,\"start\":32889},{\"end\":34183,\"start\":33720}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15857,\"start\":15807},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16170,\"start\":15961},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16486,\"start\":16377},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17608,\"start\":17557},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18084,\"start\":17907},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18403,\"start\":18330},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18790,\"start\":18736},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19921,\"start\":19853},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21135,\"start\":21104},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21413,\"start\":21300},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21608,\"start\":21574},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22377,\"start\":22351}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26396,\"start\":26389},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28516,\"start\":28509},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":29009,\"start\":29002},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30996,\"start\":30989},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":33133,\"start\":33126},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33156,\"start\":33149}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2921,\"start\":2909},{\"attributes\":{\"n\":\"2\"},\"end\":6666,\"start\":6654},{\"attributes\":{\"n\":\"3\"},\"end\":11626,\"start\":11621},{\"attributes\":{\"n\":\"3.1\"},\"end\":12339,\"start\":12332},{\"end\":15670,\"start\":15628},{\"attributes\":{\"n\":\"3.2\"},\"end\":16495,\"start\":16488},{\"attributes\":{\"n\":\"4\"},\"end\":22437,\"start\":22426},{\"attributes\":{\"n\":\"4.1\"},\"end\":25295,\"start\":25258},{\"attributes\":{\"n\":\"4.2\"},\"end\":28040,\"start\":28010},{\"attributes\":{\"n\":\"4.3\"},\"end\":29340,\"start\":29312},{\"attributes\":{\"n\":\"5\"},\"end\":33718,\"start\":33708},{\"end\":34193,\"start\":34185},{\"end\":34306,\"start\":34298},{\"end\":34395,\"start\":34387},{\"end\":34764,\"start\":34756},{\"end\":34884,\"start\":34883},{\"end\":35432,\"start\":35424},{\"end\":35526,\"start\":35518},{\"end\":35658,\"start\":35654},{\"end\":36104,\"start\":36095},{\"end\":38514,\"start\":38505},{\"end\":39107,\"start\":39098}]", "table": "[{\"end\":38503,\"start\":36149},{\"end\":39096,\"start\":38567},{\"end\":40047,\"start\":39280}]", "figure_caption": "[{\"end\":34296,\"start\":34195},{\"end\":34385,\"start\":34308},{\"end\":34754,\"start\":34397},{\"end\":34881,\"start\":34766},{\"end\":35422,\"start\":34885},{\"end\":35516,\"start\":35434},{\"end\":35652,\"start\":35528},{\"end\":36093,\"start\":35659},{\"end\":36149,\"start\":36106},{\"end\":38567,\"start\":38516},{\"end\":39280,\"start\":39109}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4140,\"start\":4132},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11725,\"start\":11717},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14227,\"start\":14219},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17235,\"start\":17227},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20877,\"start\":20869},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27218,\"start\":27210},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31579,\"start\":31571},{\"end\":32077,\"start\":32069},{\"end\":32459,\"start\":32458}]", "bib_author_first_name": "[{\"end\":40180,\"start\":40175},{\"end\":40196,\"start\":40190},{\"end\":40213,\"start\":40207},{\"end\":40226,\"start\":40223},{\"end\":40704,\"start\":40695},{\"end\":40721,\"start\":40712},{\"end\":40737,\"start\":40731},{\"end\":40750,\"start\":40742},{\"end\":40766,\"start\":40761},{\"end\":40775,\"start\":40774},{\"end\":40784,\"start\":40776},{\"end\":40798,\"start\":40794},{\"end\":41349,\"start\":41342},{\"end\":41369,\"start\":41360},{\"end\":41381,\"start\":41375},{\"end\":41626,\"start\":41620},{\"end\":41640,\"start\":41632},{\"end\":41651,\"start\":41645},{\"end\":41661,\"start\":41658},{\"end\":42105,\"start\":42097},{\"end\":42117,\"start\":42111},{\"end\":42132,\"start\":42126},{\"end\":42143,\"start\":42140},{\"end\":42158,\"start\":42151},{\"end\":42167,\"start\":42166},{\"end\":42169,\"start\":42168},{\"end\":42180,\"start\":42176},{\"end\":42193,\"start\":42188},{\"end\":42772,\"start\":42764},{\"end\":42784,\"start\":42778},{\"end\":42794,\"start\":42793},{\"end\":42796,\"start\":42795},{\"end\":42809,\"start\":42803},{\"end\":42822,\"start\":42817},{\"end\":43358,\"start\":43352},{\"end\":43369,\"start\":43365},{\"end\":43384,\"start\":43375},{\"end\":43391,\"start\":43389},{\"end\":43834,\"start\":43830},{\"end\":43848,\"start\":43844},{\"end\":43853,\"start\":43849},{\"end\":43871,\"start\":43865},{\"end\":43892,\"start\":43882},{\"end\":43912,\"start\":43906},{\"end\":43929,\"start\":43925},{\"end\":43944,\"start\":43938},{\"end\":44501,\"start\":44497},{\"end\":44515,\"start\":44507},{\"end\":44527,\"start\":44520},{\"end\":44542,\"start\":44533},{\"end\":44555,\"start\":44548},{\"end\":44562,\"start\":44560},{\"end\":45162,\"start\":45158},{\"end\":45173,\"start\":45168},{\"end\":45190,\"start\":45181},{\"end\":45198,\"start\":45196},{\"end\":45578,\"start\":45571},{\"end\":45596,\"start\":45589},{\"end\":45598,\"start\":45597},{\"end\":45612,\"start\":45606},{\"end\":45618,\"start\":45613},{\"end\":45630,\"start\":45628},{\"end\":45648,\"start\":45643},{\"end\":45663,\"start\":45655},{\"end\":46230,\"start\":46229},{\"end\":46238,\"start\":46237},{\"end\":46244,\"start\":46243},{\"end\":46251,\"start\":46250},{\"end\":46258,\"start\":46257},{\"end\":46264,\"start\":46263},{\"end\":46643,\"start\":46639},{\"end\":46662,\"start\":46656},{\"end\":46940,\"start\":46932},{\"end\":46951,\"start\":46945},{\"end\":46968,\"start\":46962},{\"end\":47493,\"start\":47487},{\"end\":47506,\"start\":47504},{\"end\":47908,\"start\":47904},{\"end\":47920,\"start\":47915},{\"end\":47931,\"start\":47926},{\"end\":47946,\"start\":47940},{\"end\":47961,\"start\":47956},{\"end\":48517,\"start\":48512},{\"end\":48526,\"start\":48522},{\"end\":48539,\"start\":48532},{\"end\":48553,\"start\":48545},{\"end\":48563,\"start\":48560},{\"end\":49034,\"start\":49026},{\"end\":49047,\"start\":49040},{\"end\":49060,\"start\":49055},{\"end\":49062,\"start\":49061},{\"end\":49078,\"start\":49073},{\"end\":49091,\"start\":49085},{\"end\":49104,\"start\":49100},{\"end\":49119,\"start\":49114},{\"end\":49129,\"start\":49128},{\"end\":49138,\"start\":49130},{\"end\":49780,\"start\":49774},{\"end\":49791,\"start\":49785},{\"end\":49807,\"start\":49800},{\"end\":49818,\"start\":49814},{\"end\":49832,\"start\":49827},{\"end\":50352,\"start\":50346},{\"end\":50364,\"start\":50357},{\"end\":50376,\"start\":50371},{\"end\":50388,\"start\":50384},{\"end\":50858,\"start\":50851},{\"end\":50877,\"start\":50871},{\"end\":50893,\"start\":50888},{\"end\":51266,\"start\":51257},{\"end\":51280,\"start\":51273},{\"end\":51292,\"start\":51288},{\"end\":51306,\"start\":51301},{\"end\":51745,\"start\":51736},{\"end\":51757,\"start\":51752},{\"end\":51776,\"start\":51771},{\"end\":51791,\"start\":51785},{\"end\":51793,\"start\":51792},{\"end\":51804,\"start\":51800},{\"end\":51817,\"start\":51813},{\"end\":51819,\"start\":51818},{\"end\":51835,\"start\":51831},{\"end\":52231,\"start\":52225},{\"end\":52245,\"start\":52237},{\"end\":52252,\"start\":52250},{\"end\":52262,\"start\":52259},{\"end\":52274,\"start\":52267},{\"end\":52889,\"start\":52888},{\"end\":52902,\"start\":52897},{\"end\":52917,\"start\":52912},{\"end\":52919,\"start\":52918},{\"end\":52930,\"start\":52926},{\"end\":52932,\"start\":52931},{\"end\":52949,\"start\":52944},{\"end\":52967,\"start\":52959},{\"end\":53470,\"start\":53463},{\"end\":53489,\"start\":53483},{\"end\":53503,\"start\":53498},{\"end\":53518,\"start\":53511},{\"end\":54006,\"start\":53999},{\"end\":54018,\"start\":54013},{\"end\":54035,\"start\":54028},{\"end\":54052,\"start\":54044},{\"end\":54357,\"start\":54351},{\"end\":54367,\"start\":54363},{\"end\":54382,\"start\":54375},{\"end\":54384,\"start\":54383},{\"end\":54803,\"start\":54802},{\"end\":54819,\"start\":54812},{\"end\":54835,\"start\":54828},{\"end\":54853,\"start\":54847},{\"end\":54870,\"start\":54862},{\"end\":55431,\"start\":55427},{\"end\":55448,\"start\":55442},{\"end\":55467,\"start\":55459},{\"end\":55478,\"start\":55472},{\"end\":55493,\"start\":55488},{\"end\":56021,\"start\":56017},{\"end\":56038,\"start\":56032},{\"end\":56054,\"start\":56049},{\"end\":56068,\"start\":56063},{\"end\":56594,\"start\":56587},{\"end\":56621,\"start\":56614},{\"end\":56638,\"start\":56632},{\"end\":57043,\"start\":57038},{\"end\":57060,\"start\":57054},{\"end\":57353,\"start\":57349},{\"end\":57367,\"start\":57360},{\"end\":57377,\"start\":57372},{\"end\":57748,\"start\":57747},{\"end\":57752,\"start\":57749},{\"end\":57771,\"start\":57766},{\"end\":58058,\"start\":58057},{\"end\":58393,\"start\":58390},{\"end\":58405,\"start\":58398},{\"end\":58412,\"start\":58410},{\"end\":58849,\"start\":58847},{\"end\":58861,\"start\":58854},{\"end\":58876,\"start\":58868},{\"end\":58889,\"start\":58882},{\"end\":58891,\"start\":58890},{\"end\":58903,\"start\":58898},{\"end\":59454,\"start\":59452},{\"end\":59466,\"start\":59459},{\"end\":59477,\"start\":59473},{\"end\":59491,\"start\":59484},{\"end\":59493,\"start\":59492},{\"end\":59505,\"start\":59500},{\"end\":60012,\"start\":60010},{\"end\":60021,\"start\":60017},{\"end\":60035,\"start\":60028},{\"end\":60045,\"start\":60042},{\"end\":60047,\"start\":60046},{\"end\":60059,\"start\":60054},{\"end\":60565,\"start\":60563},{\"end\":60572,\"start\":60570},{\"end\":60582,\"start\":60580},{\"end\":60918,\"start\":60915},{\"end\":60927,\"start\":60923},{\"end\":60941,\"start\":60933},{\"end\":60952,\"start\":60949},{\"end\":61409,\"start\":61403},{\"end\":61419,\"start\":61414},{\"end\":61428,\"start\":61424},{\"end\":61445,\"start\":61436},{\"end\":61456,\"start\":61451},{\"end\":61458,\"start\":61457},{\"end\":61476,\"start\":61470},{\"end\":61499,\"start\":61492},{\"end\":61501,\"start\":61500},{\"end\":61515,\"start\":61509},{\"end\":62055,\"start\":62052},{\"end\":62068,\"start\":62061},{\"end\":62077,\"start\":62074},{\"end\":62091,\"start\":62082},{\"end\":62099,\"start\":62097},{\"end\":62110,\"start\":62106},{\"end\":62496,\"start\":62490},{\"end\":62511,\"start\":62503},{\"end\":62524,\"start\":62516},{\"end\":62532,\"start\":62530},{\"end\":62548,\"start\":62539},{\"end\":62550,\"start\":62549},{\"end\":63098,\"start\":63091},{\"end\":63111,\"start\":63103},{\"end\":63119,\"start\":63116},{\"end\":63129,\"start\":63125},{\"end\":63662,\"start\":63655},{\"end\":63676,\"start\":63668},{\"end\":63683,\"start\":63681},{\"end\":63699,\"start\":63690},{\"end\":63701,\"start\":63700},{\"end\":64080,\"start\":64076},{\"end\":64092,\"start\":64086},{\"end\":64107,\"start\":64100},{\"end\":64109,\"start\":64108},{\"end\":64128,\"start\":64121}]", "bib_author_last_name": "[{\"end\":40188,\"start\":40181},{\"end\":40205,\"start\":40197},{\"end\":40221,\"start\":40214},{\"end\":40232,\"start\":40227},{\"end\":40710,\"start\":40705},{\"end\":40729,\"start\":40722},{\"end\":40740,\"start\":40738},{\"end\":40759,\"start\":40751},{\"end\":40772,\"start\":40767},{\"end\":40792,\"start\":40785},{\"end\":40805,\"start\":40799},{\"end\":41358,\"start\":41350},{\"end\":41373,\"start\":41370},{\"end\":41388,\"start\":41382},{\"end\":41630,\"start\":41627},{\"end\":41643,\"start\":41641},{\"end\":41656,\"start\":41652},{\"end\":41665,\"start\":41662},{\"end\":42109,\"start\":42106},{\"end\":42124,\"start\":42118},{\"end\":42138,\"start\":42133},{\"end\":42149,\"start\":42144},{\"end\":42164,\"start\":42159},{\"end\":42174,\"start\":42170},{\"end\":42186,\"start\":42181},{\"end\":42200,\"start\":42194},{\"end\":42207,\"start\":42202},{\"end\":42776,\"start\":42773},{\"end\":42791,\"start\":42785},{\"end\":42801,\"start\":42797},{\"end\":42815,\"start\":42810},{\"end\":42826,\"start\":42823},{\"end\":42833,\"start\":42828},{\"end\":43363,\"start\":43359},{\"end\":43373,\"start\":43370},{\"end\":43387,\"start\":43385},{\"end\":43396,\"start\":43392},{\"end\":43842,\"start\":43835},{\"end\":43863,\"start\":43854},{\"end\":43880,\"start\":43872},{\"end\":43904,\"start\":43893},{\"end\":43923,\"start\":43913},{\"end\":43936,\"start\":43930},{\"end\":43952,\"start\":43945},{\"end\":44505,\"start\":44502},{\"end\":44518,\"start\":44516},{\"end\":44531,\"start\":44528},{\"end\":44546,\"start\":44543},{\"end\":44558,\"start\":44556},{\"end\":44567,\"start\":44563},{\"end\":45166,\"start\":45163},{\"end\":45179,\"start\":45174},{\"end\":45194,\"start\":45191},{\"end\":45203,\"start\":45199},{\"end\":45587,\"start\":45579},{\"end\":45604,\"start\":45599},{\"end\":45626,\"start\":45619},{\"end\":45641,\"start\":45631},{\"end\":45653,\"start\":45649},{\"end\":45670,\"start\":45664},{\"end\":45675,\"start\":45672},{\"end\":46235,\"start\":46231},{\"end\":46241,\"start\":46239},{\"end\":46248,\"start\":46245},{\"end\":46255,\"start\":46252},{\"end\":46261,\"start\":46259},{\"end\":46269,\"start\":46265},{\"end\":46654,\"start\":46644},{\"end\":46674,\"start\":46663},{\"end\":46943,\"start\":46941},{\"end\":46960,\"start\":46952},{\"end\":46976,\"start\":46969},{\"end\":47502,\"start\":47494},{\"end\":47514,\"start\":47507},{\"end\":47913,\"start\":47909},{\"end\":47924,\"start\":47921},{\"end\":47938,\"start\":47932},{\"end\":47954,\"start\":47947},{\"end\":47968,\"start\":47962},{\"end\":48520,\"start\":48518},{\"end\":48530,\"start\":48527},{\"end\":48543,\"start\":48540},{\"end\":48558,\"start\":48554},{\"end\":48567,\"start\":48564},{\"end\":49038,\"start\":49035},{\"end\":49053,\"start\":49048},{\"end\":49071,\"start\":49063},{\"end\":49083,\"start\":49079},{\"end\":49098,\"start\":49092},{\"end\":49112,\"start\":49105},{\"end\":49126,\"start\":49120},{\"end\":49146,\"start\":49139},{\"end\":49783,\"start\":49781},{\"end\":49798,\"start\":49792},{\"end\":49812,\"start\":49808},{\"end\":49825,\"start\":49819},{\"end\":49838,\"start\":49833},{\"end\":50355,\"start\":50353},{\"end\":50369,\"start\":50365},{\"end\":50382,\"start\":50377},{\"end\":50395,\"start\":50389},{\"end\":50869,\"start\":50859},{\"end\":50886,\"start\":50878},{\"end\":50899,\"start\":50894},{\"end\":51271,\"start\":51267},{\"end\":51286,\"start\":51281},{\"end\":51299,\"start\":51293},{\"end\":51318,\"start\":51307},{\"end\":51750,\"start\":51746},{\"end\":51769,\"start\":51758},{\"end\":51783,\"start\":51777},{\"end\":51798,\"start\":51794},{\"end\":51811,\"start\":51805},{\"end\":51829,\"start\":51820},{\"end\":51842,\"start\":51836},{\"end\":52235,\"start\":52232},{\"end\":52248,\"start\":52246},{\"end\":52257,\"start\":52253},{\"end\":52265,\"start\":52263},{\"end\":52281,\"start\":52275},{\"end\":52895,\"start\":52890},{\"end\":52910,\"start\":52903},{\"end\":52924,\"start\":52920},{\"end\":52942,\"start\":52933},{\"end\":52957,\"start\":52950},{\"end\":52979,\"start\":52968},{\"end\":52989,\"start\":52981},{\"end\":53481,\"start\":53471},{\"end\":53496,\"start\":53490},{\"end\":53509,\"start\":53504},{\"end\":53521,\"start\":53519},{\"end\":54011,\"start\":54007},{\"end\":54026,\"start\":54019},{\"end\":54042,\"start\":54036},{\"end\":54057,\"start\":54053},{\"end\":54066,\"start\":54059},{\"end\":54361,\"start\":54358},{\"end\":54373,\"start\":54368},{\"end\":54390,\"start\":54385},{\"end\":54810,\"start\":54804},{\"end\":54826,\"start\":54820},{\"end\":54845,\"start\":54836},{\"end\":54860,\"start\":54854},{\"end\":54875,\"start\":54871},{\"end\":54881,\"start\":54877},{\"end\":55440,\"start\":55432},{\"end\":55457,\"start\":55449},{\"end\":55470,\"start\":55468},{\"end\":55486,\"start\":55479},{\"end\":55501,\"start\":55494},{\"end\":56030,\"start\":56022},{\"end\":56047,\"start\":56039},{\"end\":56061,\"start\":56055},{\"end\":56076,\"start\":56069},{\"end\":56612,\"start\":56595},{\"end\":56630,\"start\":56622},{\"end\":56642,\"start\":56639},{\"end\":56649,\"start\":56644},{\"end\":57052,\"start\":57044},{\"end\":57070,\"start\":57061},{\"end\":57358,\"start\":57354},{\"end\":57370,\"start\":57368},{\"end\":57382,\"start\":57378},{\"end\":57764,\"start\":57753},{\"end\":57779,\"start\":57772},{\"end\":57786,\"start\":57781},{\"end\":58065,\"start\":58059},{\"end\":58075,\"start\":58067},{\"end\":58396,\"start\":58394},{\"end\":58408,\"start\":58406},{\"end\":58415,\"start\":58413},{\"end\":58852,\"start\":58850},{\"end\":58866,\"start\":58862},{\"end\":58880,\"start\":58877},{\"end\":58896,\"start\":58892},{\"end\":58911,\"start\":58904},{\"end\":58919,\"start\":58913},{\"end\":59457,\"start\":59455},{\"end\":59471,\"start\":59467},{\"end\":59482,\"start\":59478},{\"end\":59498,\"start\":59494},{\"end\":59513,\"start\":59506},{\"end\":59521,\"start\":59515},{\"end\":60015,\"start\":60013},{\"end\":60026,\"start\":60022},{\"end\":60040,\"start\":60036},{\"end\":60052,\"start\":60048},{\"end\":60067,\"start\":60060},{\"end\":60075,\"start\":60069},{\"end\":60568,\"start\":60566},{\"end\":60578,\"start\":60573},{\"end\":60587,\"start\":60583},{\"end\":60921,\"start\":60919},{\"end\":60931,\"start\":60928},{\"end\":60947,\"start\":60942},{\"end\":60956,\"start\":60953},{\"end\":61412,\"start\":61410},{\"end\":61422,\"start\":61420},{\"end\":61434,\"start\":61429},{\"end\":61449,\"start\":61446},{\"end\":61468,\"start\":61459},{\"end\":61490,\"start\":61477},{\"end\":61507,\"start\":61502},{\"end\":61522,\"start\":61516},{\"end\":62059,\"start\":62056},{\"end\":62072,\"start\":62069},{\"end\":62080,\"start\":62078},{\"end\":62095,\"start\":62092},{\"end\":62104,\"start\":62100},{\"end\":62113,\"start\":62111},{\"end\":62501,\"start\":62497},{\"end\":62514,\"start\":62512},{\"end\":62528,\"start\":62525},{\"end\":62537,\"start\":62533},{\"end\":62556,\"start\":62551},{\"end\":63101,\"start\":63099},{\"end\":63114,\"start\":63112},{\"end\":63123,\"start\":63120},{\"end\":63133,\"start\":63130},{\"end\":63666,\"start\":63663},{\"end\":63679,\"start\":63677},{\"end\":63688,\"start\":63684},{\"end\":63711,\"start\":63702},{\"end\":64084,\"start\":64081},{\"end\":64098,\"start\":64093},{\"end\":64119,\"start\":64110},{\"end\":64131,\"start\":64129}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.12\",\"id\":\"b0\",\"matched_paper_id\":5276660},\"end\":40661,\"start\":40151},{\"attributes\":{\"doi\":\"10.1109/ICCV.2015.279\",\"id\":\"b1\",\"matched_paper_id\":3180429},\"end\":41269,\"start\":40663},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b2\"},\"end\":41550,\"start\":41271},{\"attributes\":{\"doi\":\"10.1007/978-3-030-01258-8_2\",\"id\":\"b3\",\"matched_paper_id\":52956432},\"end\":42080,\"start\":41552},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.121\",\"id\":\"b4\",\"matched_paper_id\":1820614},\"end\":42686,\"start\":42082},{\"attributes\":{\"doi\":\"10.1109/ICCV.2017.321\",\"id\":\"b5\",\"matched_paper_id\":1448723},\"end\":43287,\"start\":42688},{\"attributes\":{\"doi\":\"10.1145/3369393\",\"id\":\"b6\",\"matched_paper_id\":195848327},\"end\":43745,\"start\":43289},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2016.2599174\",\"id\":\"b7\",\"matched_paper_id\":5736847},\"end\":44396,\"start\":43747},{\"attributes\":{\"doi\":\"10.24963/ijcai.2018/98\",\"id\":\"b8\",\"matched_paper_id\":51606833},\"end\":45089,\"start\":44398},{\"attributes\":{\"doi\":\"10.1145/3243316\",\"id\":\"b9\",\"matched_paper_id\":220422618},\"end\":45526,\"start\":45091},{\"attributes\":{\"doi\":\"10.1109/CVPR.2015.7298754\",\"id\":\"b10\",\"matched_paper_id\":9254582},\"end\":46172,\"start\":45528},{\"attributes\":{\"doi\":\"10.1109/TCSVT.2020.2965966\",\"id\":\"b11\",\"matched_paper_id\":199472695},\"end\":46613,\"start\":46174},{\"attributes\":{\"doi\":\"10.1162/neco.1997.9.8.1735\",\"id\":\"b12\",\"matched_paper_id\":1915014},\"end\":46882,\"start\":46615},{\"attributes\":{\"doi\":\"10.1007/978-3-319-46448-0_7\",\"id\":\"b13\",\"matched_paper_id\":1931511},\"end\":47418,\"start\":46884},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2016.2598339\",\"id\":\"b14\",\"matched_paper_id\":8517067},\"end\":47848,\"start\":47420},{\"attributes\":{\"doi\":\"10.1109/CVPR.2014.455\",\"id\":\"b15\",\"matched_paper_id\":3015754},\"end\":48441,\"start\":47850},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5046218},\"end\":48981,\"start\":48443},{\"attributes\":{\"doi\":\"10.1007/978-3-319-10602-1_48\",\"id\":\"b17\",\"matched_paper_id\":14113767},\"end\":49662,\"start\":48983},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":35001335},\"end\":50272,\"start\":49664},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":868693},\"end\":50776,\"start\":50274},{\"attributes\":{\"doi\":\"10.1007/s11263-017-1038-2\",\"id\":\"b20\",\"matched_paper_id\":11621064},\"end\":51217,\"start\":50778},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":17195923},\"end\":51677,\"start\":51219},{\"attributes\":{\"doi\":\"10.1038/nature14236\",\"id\":\"b22\",\"matched_paper_id\":205242740},\"end\":52128,\"start\":51679},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.117\",\"id\":\"b23\",\"matched_paper_id\":3867284},\"end\":52785,\"start\":52130},{\"attributes\":{\"doi\":\"10.1007/s11263-016-0965-7\",\"id\":\"b24\",\"matched_paper_id\":6941275},\"end\":53387,\"start\":52787},{\"attributes\":{\"doi\":\"10.1007/978-3-319-10590-1_7\",\"id\":\"b25\",\"matched_paper_id\":8335083},\"end\":53997,\"start\":53389},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b26\"},\"end\":54293,\"start\":53999},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2950705},\"end\":54746,\"start\":54295},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.131\",\"id\":\"b28\",\"matched_paper_id\":206594923},\"end\":55367,\"start\":54748},{\"attributes\":{\"doi\":\"10.1007/978-3-319-46448-0_49\",\"id\":\"b29\",\"matched_paper_id\":9926549},\"end\":55982,\"start\":55369},{\"attributes\":{\"doi\":\"10.1109/CVPR.2015.7298940\",\"id\":\"b30\",\"matched_paper_id\":15184723},\"end\":56515,\"start\":55984},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":5677543},\"end\":57036,\"start\":56517},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b32\"},\"end\":57282,\"start\":57038},{\"attributes\":{\"doi\":\"10.1145/3226037\",\"id\":\"b33\",\"matched_paper_id\":50775693},\"end\":57733,\"start\":57284},{\"attributes\":{\"doi\":\"10.1007/BF00992698\",\"id\":\"b34\",\"matched_paper_id\":208910339},\"end\":57964,\"start\":57735},{\"attributes\":{\"doi\":\"10.1007/BF00992696\",\"id\":\"b35\",\"matched_paper_id\":2332513},\"end\":58305,\"start\":57966},{\"attributes\":{\"doi\":\"10.1145/3271485\",\"id\":\"b36\",\"matched_paper_id\":56594619},\"end\":58764,\"start\":58307},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206593820},\"end\":59359,\"start\":58766},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2017.2708709\",\"id\":\"b38\",\"matched_paper_id\":18548166},\"end\":59921,\"start\":59361},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":24537813},\"end\":60508,\"start\":59923},{\"attributes\":{\"doi\":\"10.1109/TIP.2020.2967584\",\"id\":\"b40\",\"matched_paper_id\":102352082},\"end\":60847,\"start\":60510},{\"attributes\":{\"doi\":\"10.1145/3123266.3123448\",\"id\":\"b41\",\"matched_paper_id\":5303968},\"end\":61323,\"start\":60849},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1055111},\"end\":61969,\"start\":61325},{\"attributes\":{\"doi\":\"10.1109/TMM.2016.2602938\",\"id\":\"b43\",\"matched_paper_id\":4620251},\"end\":62431,\"start\":61971},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.10\",\"id\":\"b44\",\"matched_paper_id\":8849206},\"end\":63027,\"start\":62433},{\"attributes\":{\"doi\":\"10.1109/CVPR.2017.446\",\"id\":\"b45\",\"matched_paper_id\":758237},\"end\":63591,\"start\":63029},{\"attributes\":{\"doi\":\"10.1007/s11263-017-1033-7\",\"id\":\"b46\",\"matched_paper_id\":27121222},\"end\":64025,\"start\":63593},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.540\",\"id\":\"b47\",\"matched_paper_id\":5714907},\"end\":64591,\"start\":64027}]", "bib_title": "[{\"end\":40173,\"start\":40151},{\"end\":40693,\"start\":40663},{\"end\":41618,\"start\":41552},{\"end\":42095,\"start\":42082},{\"end\":42762,\"start\":42688},{\"end\":43350,\"start\":43289},{\"end\":43828,\"start\":43747},{\"end\":44495,\"start\":44398},{\"end\":45156,\"start\":45091},{\"end\":45569,\"start\":45528},{\"end\":46227,\"start\":46174},{\"end\":46637,\"start\":46615},{\"end\":46930,\"start\":46884},{\"end\":47485,\"start\":47420},{\"end\":47902,\"start\":47850},{\"end\":48510,\"start\":48443},{\"end\":49024,\"start\":48983},{\"end\":49772,\"start\":49664},{\"end\":50344,\"start\":50274},{\"end\":50849,\"start\":50778},{\"end\":51255,\"start\":51219},{\"end\":51734,\"start\":51679},{\"end\":52223,\"start\":52130},{\"end\":52886,\"start\":52787},{\"end\":53461,\"start\":53389},{\"end\":54349,\"start\":54295},{\"end\":54800,\"start\":54748},{\"end\":55425,\"start\":55369},{\"end\":56015,\"start\":55984},{\"end\":56585,\"start\":56517},{\"end\":57347,\"start\":57284},{\"end\":57745,\"start\":57735},{\"end\":58055,\"start\":57966},{\"end\":58388,\"start\":58307},{\"end\":58845,\"start\":58766},{\"end\":59450,\"start\":59361},{\"end\":60008,\"start\":59923},{\"end\":60561,\"start\":60510},{\"end\":60913,\"start\":60849},{\"end\":61401,\"start\":61325},{\"end\":62050,\"start\":61971},{\"end\":62488,\"start\":62433},{\"end\":63089,\"start\":63029},{\"end\":63653,\"start\":63593},{\"end\":64074,\"start\":64027}]", "bib_author": "[{\"end\":40190,\"start\":40175},{\"end\":40207,\"start\":40190},{\"end\":40223,\"start\":40207},{\"end\":40234,\"start\":40223},{\"end\":40712,\"start\":40695},{\"end\":40731,\"start\":40712},{\"end\":40742,\"start\":40731},{\"end\":40761,\"start\":40742},{\"end\":40774,\"start\":40761},{\"end\":40794,\"start\":40774},{\"end\":40807,\"start\":40794},{\"end\":41360,\"start\":41342},{\"end\":41375,\"start\":41360},{\"end\":41390,\"start\":41375},{\"end\":41632,\"start\":41620},{\"end\":41645,\"start\":41632},{\"end\":41658,\"start\":41645},{\"end\":41667,\"start\":41658},{\"end\":42111,\"start\":42097},{\"end\":42126,\"start\":42111},{\"end\":42140,\"start\":42126},{\"end\":42151,\"start\":42140},{\"end\":42166,\"start\":42151},{\"end\":42176,\"start\":42166},{\"end\":42188,\"start\":42176},{\"end\":42202,\"start\":42188},{\"end\":42209,\"start\":42202},{\"end\":42778,\"start\":42764},{\"end\":42793,\"start\":42778},{\"end\":42803,\"start\":42793},{\"end\":42817,\"start\":42803},{\"end\":42828,\"start\":42817},{\"end\":42835,\"start\":42828},{\"end\":43365,\"start\":43352},{\"end\":43375,\"start\":43365},{\"end\":43389,\"start\":43375},{\"end\":43398,\"start\":43389},{\"end\":43844,\"start\":43830},{\"end\":43865,\"start\":43844},{\"end\":43882,\"start\":43865},{\"end\":43906,\"start\":43882},{\"end\":43925,\"start\":43906},{\"end\":43938,\"start\":43925},{\"end\":43954,\"start\":43938},{\"end\":44507,\"start\":44497},{\"end\":44520,\"start\":44507},{\"end\":44533,\"start\":44520},{\"end\":44548,\"start\":44533},{\"end\":44560,\"start\":44548},{\"end\":44569,\"start\":44560},{\"end\":45168,\"start\":45158},{\"end\":45181,\"start\":45168},{\"end\":45196,\"start\":45181},{\"end\":45205,\"start\":45196},{\"end\":45589,\"start\":45571},{\"end\":45606,\"start\":45589},{\"end\":45628,\"start\":45606},{\"end\":45643,\"start\":45628},{\"end\":45655,\"start\":45643},{\"end\":45672,\"start\":45655},{\"end\":45677,\"start\":45672},{\"end\":46237,\"start\":46229},{\"end\":46243,\"start\":46237},{\"end\":46250,\"start\":46243},{\"end\":46257,\"start\":46250},{\"end\":46263,\"start\":46257},{\"end\":46271,\"start\":46263},{\"end\":46656,\"start\":46639},{\"end\":46676,\"start\":46656},{\"end\":46945,\"start\":46932},{\"end\":46962,\"start\":46945},{\"end\":46978,\"start\":46962},{\"end\":47504,\"start\":47487},{\"end\":47516,\"start\":47504},{\"end\":47915,\"start\":47904},{\"end\":47926,\"start\":47915},{\"end\":47940,\"start\":47926},{\"end\":47956,\"start\":47940},{\"end\":47970,\"start\":47956},{\"end\":48522,\"start\":48512},{\"end\":48532,\"start\":48522},{\"end\":48545,\"start\":48532},{\"end\":48560,\"start\":48545},{\"end\":48569,\"start\":48560},{\"end\":49040,\"start\":49026},{\"end\":49055,\"start\":49040},{\"end\":49073,\"start\":49055},{\"end\":49085,\"start\":49073},{\"end\":49100,\"start\":49085},{\"end\":49114,\"start\":49100},{\"end\":49128,\"start\":49114},{\"end\":49148,\"start\":49128},{\"end\":49785,\"start\":49774},{\"end\":49800,\"start\":49785},{\"end\":49814,\"start\":49800},{\"end\":49827,\"start\":49814},{\"end\":49840,\"start\":49827},{\"end\":50357,\"start\":50346},{\"end\":50371,\"start\":50357},{\"end\":50384,\"start\":50371},{\"end\":50397,\"start\":50384},{\"end\":50871,\"start\":50851},{\"end\":50888,\"start\":50871},{\"end\":50901,\"start\":50888},{\"end\":51273,\"start\":51257},{\"end\":51288,\"start\":51273},{\"end\":51301,\"start\":51288},{\"end\":51320,\"start\":51301},{\"end\":51752,\"start\":51736},{\"end\":51771,\"start\":51752},{\"end\":51785,\"start\":51771},{\"end\":51800,\"start\":51785},{\"end\":51813,\"start\":51800},{\"end\":51831,\"start\":51813},{\"end\":51844,\"start\":51831},{\"end\":52237,\"start\":52225},{\"end\":52250,\"start\":52237},{\"end\":52259,\"start\":52250},{\"end\":52267,\"start\":52259},{\"end\":52283,\"start\":52267},{\"end\":52897,\"start\":52888},{\"end\":52912,\"start\":52897},{\"end\":52926,\"start\":52912},{\"end\":52944,\"start\":52926},{\"end\":52959,\"start\":52944},{\"end\":52981,\"start\":52959},{\"end\":52991,\"start\":52981},{\"end\":53483,\"start\":53463},{\"end\":53498,\"start\":53483},{\"end\":53511,\"start\":53498},{\"end\":53523,\"start\":53511},{\"end\":54013,\"start\":53999},{\"end\":54028,\"start\":54013},{\"end\":54044,\"start\":54028},{\"end\":54059,\"start\":54044},{\"end\":54068,\"start\":54059},{\"end\":54363,\"start\":54351},{\"end\":54375,\"start\":54363},{\"end\":54392,\"start\":54375},{\"end\":54812,\"start\":54802},{\"end\":54828,\"start\":54812},{\"end\":54847,\"start\":54828},{\"end\":54862,\"start\":54847},{\"end\":54877,\"start\":54862},{\"end\":54883,\"start\":54877},{\"end\":55442,\"start\":55427},{\"end\":55459,\"start\":55442},{\"end\":55472,\"start\":55459},{\"end\":55488,\"start\":55472},{\"end\":55503,\"start\":55488},{\"end\":56032,\"start\":56017},{\"end\":56049,\"start\":56032},{\"end\":56063,\"start\":56049},{\"end\":56078,\"start\":56063},{\"end\":56614,\"start\":56587},{\"end\":56632,\"start\":56614},{\"end\":56644,\"start\":56632},{\"end\":56651,\"start\":56644},{\"end\":57054,\"start\":57038},{\"end\":57072,\"start\":57054},{\"end\":57360,\"start\":57349},{\"end\":57372,\"start\":57360},{\"end\":57384,\"start\":57372},{\"end\":57766,\"start\":57747},{\"end\":57781,\"start\":57766},{\"end\":57788,\"start\":57781},{\"end\":58067,\"start\":58057},{\"end\":58077,\"start\":58067},{\"end\":58398,\"start\":58390},{\"end\":58410,\"start\":58398},{\"end\":58417,\"start\":58410},{\"end\":58854,\"start\":58847},{\"end\":58868,\"start\":58854},{\"end\":58882,\"start\":58868},{\"end\":58898,\"start\":58882},{\"end\":58913,\"start\":58898},{\"end\":58921,\"start\":58913},{\"end\":59459,\"start\":59452},{\"end\":59473,\"start\":59459},{\"end\":59484,\"start\":59473},{\"end\":59500,\"start\":59484},{\"end\":59515,\"start\":59500},{\"end\":59523,\"start\":59515},{\"end\":60017,\"start\":60010},{\"end\":60028,\"start\":60017},{\"end\":60042,\"start\":60028},{\"end\":60054,\"start\":60042},{\"end\":60069,\"start\":60054},{\"end\":60077,\"start\":60069},{\"end\":60570,\"start\":60563},{\"end\":60580,\"start\":60570},{\"end\":60589,\"start\":60580},{\"end\":60923,\"start\":60915},{\"end\":60933,\"start\":60923},{\"end\":60949,\"start\":60933},{\"end\":60958,\"start\":60949},{\"end\":61414,\"start\":61403},{\"end\":61424,\"start\":61414},{\"end\":61436,\"start\":61424},{\"end\":61451,\"start\":61436},{\"end\":61470,\"start\":61451},{\"end\":61492,\"start\":61470},{\"end\":61509,\"start\":61492},{\"end\":61524,\"start\":61509},{\"end\":62061,\"start\":62052},{\"end\":62074,\"start\":62061},{\"end\":62082,\"start\":62074},{\"end\":62097,\"start\":62082},{\"end\":62106,\"start\":62097},{\"end\":62115,\"start\":62106},{\"end\":62503,\"start\":62490},{\"end\":62516,\"start\":62503},{\"end\":62530,\"start\":62516},{\"end\":62539,\"start\":62530},{\"end\":62558,\"start\":62539},{\"end\":63103,\"start\":63091},{\"end\":63116,\"start\":63103},{\"end\":63125,\"start\":63116},{\"end\":63135,\"start\":63125},{\"end\":63668,\"start\":63655},{\"end\":63681,\"start\":63668},{\"end\":63690,\"start\":63681},{\"end\":63713,\"start\":63690},{\"end\":64086,\"start\":64076},{\"end\":64100,\"start\":64086},{\"end\":64121,\"start\":64100},{\"end\":64133,\"start\":64121}]", "bib_venue": "[{\"end\":40423,\"start\":40347},{\"end\":40977,\"start\":40909},{\"end\":41823,\"start\":41767},{\"end\":42399,\"start\":42323},{\"end\":42995,\"start\":42934},{\"end\":44760,\"start\":44684},{\"end\":45861,\"start\":45790},{\"end\":47170,\"start\":47096},{\"end\":48160,\"start\":48084},{\"end\":48738,\"start\":48662},{\"end\":49341,\"start\":49267},{\"end\":52473,\"start\":52397},{\"end\":53713,\"start\":53640},{\"end\":55073,\"start\":54997},{\"end\":55696,\"start\":55622},{\"end\":56262,\"start\":56191},{\"end\":59090,\"start\":59014},{\"end\":60248,\"start\":60171},{\"end\":61086,\"start\":61042},{\"end\":61665,\"start\":61603},{\"end\":62747,\"start\":62671},{\"end\":63325,\"start\":63249},{\"end\":64323,\"start\":64247},{\"end\":40345,\"start\":40254},{\"end\":40907,\"start\":40828},{\"end\":41340,\"start\":41271},{\"end\":41765,\"start\":41694},{\"end\":42321,\"start\":42230},{\"end\":42932,\"start\":42856},{\"end\":43453,\"start\":43413},{\"end\":44042,\"start\":43980},{\"end\":44682,\"start\":44591},{\"end\":45230,\"start\":45220},{\"end\":45788,\"start\":45702},{\"end\":46373,\"start\":46297},{\"end\":46720,\"start\":46702},{\"end\":47094,\"start\":47005},{\"end\":47604,\"start\":47542},{\"end\":48082,\"start\":47991},{\"end\":48660,\"start\":48569},{\"end\":49265,\"start\":49176},{\"end\":49952,\"start\":49840},{\"end\":50509,\"start\":50397},{\"end\":50966,\"start\":50926},{\"end\":51432,\"start\":51320},{\"end\":51869,\"start\":51863},{\"end\":52395,\"start\":52304},{\"end\":53056,\"start\":53016},{\"end\":53638,\"start\":53550},{\"end\":54138,\"start\":54084},{\"end\":54504,\"start\":54392},{\"end\":54995,\"start\":54904},{\"end\":55620,\"start\":55531},{\"end\":56189,\"start\":56103},{\"end\":56763,\"start\":56651},{\"end\":57153,\"start\":57087},{\"end\":57439,\"start\":57399},{\"end\":57822,\"start\":57806},{\"end\":58111,\"start\":58095},{\"end\":58472,\"start\":58432},{\"end\":59012,\"start\":58921},{\"end\":59611,\"start\":59549},{\"end\":60169,\"start\":60077},{\"end\":60650,\"start\":60613},{\"end\":61040,\"start\":60981},{\"end\":61601,\"start\":61524},{\"end\":62170,\"start\":62139},{\"end\":62669,\"start\":62578},{\"end\":63247,\"start\":63156},{\"end\":63778,\"start\":63738},{\"end\":64245,\"start\":64154}]"}}}, "year": 2023, "month": 12, "day": 17}