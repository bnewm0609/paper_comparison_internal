{"id": 132686560, "updated": "2022-02-19 15:25:39.039", "metadata": {"title": "Intelligent Mobile Drone System Based on Real-Time Object Detection", "authors": "[{\"first\":\"Chuanlong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xingming\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Junhao\",\"last\":\"Cai\",\"middle\":[]}]", "venue": "Journal on Artificial Intelligence", "journal": "Journal on Artificial Intelligence", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Drone also known as unmanned aerial vehicle (UAV) has drawn lots of attention in recent years. Quadcopter as one of the most popular drones has great potential in both industrial and academic fields. Quadcopter drones are capable of taking off vertically and flying towards any direction. Traditional researches of drones mainly focus on their mechanical structures and movement control. The aircraft movement is usually controlled by a remote controller manually or the trajectory is pre-programmed with specific algorithms. Consumer drones typically use mobile device together with remote controllers to realize flight control and video transmission. Implementing different functions on mobile devices can result in different behaviors of drones indirectly. With the development of deep learning in computer vision field, commercial drones equipped with camera can be much more intelligent and even realize autonomous flight. In the past, running deep learning based algorithms on mobile devices is highly computational intensive and time consuming. This paper utilizes a novel real-time object detection method and deploys the deep learning model on the modern mobile device to realize autonomous object detection and object tracking of drones.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2927212233", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.32604/jai.2019.06064"}}, "content": {"source": {"pdf_hash": "0adc350e398e9a08d7cc76568a004a51fdd7448b", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://doi.org/10.32604/jai.2019.06064", "status": "GOLD"}}, "grobid": {"id": "68ff7d37e4fff42b7cd1d22e7cdb354f59f4982e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0adc350e398e9a08d7cc76568a004a51fdd7448b.txt", "contents": "\nIntelligent Mobile Drone System Based on Real-Time Object Detection\n2019\n\nChuanlong Li \nXingming Sun \nJunhao Cai \nIntelligent Mobile Drone System Based on Real-Time Object Detection\n\nJAI\n11201910.32604/jai.2019.06064DroneUAVCNNobject detectionmobile applicationIos\nDrone also known as unmanned aerial vehicle (UAV) has drawn lots of attention in recent years. Quadcopter as one of the most popular drones has great potential in both industrial and academic fields. Quadcopter drones are capable of taking off vertically and flying towards any direction. Traditional researches of drones mainly focus on their mechanical structures and movement control. The aircraft movement is usually controlled by a remote controller manually or the trajectory is pre-programmed with specific algorithms. Consumer drones typically use mobile device together with remote controllers to realize flight control and video transmission. Implementing different functions on mobile devices can result in different behaviors of drones indirectly. With the development of deep learning in computer vision field, commercial drones equipped with camera can be much more intelligent and even realize autonomous flight. In the past, running deep learning based algorithms on mobile devices is highly computational intensive and time consuming. This paper utilizes a novel real-time object detection method and deploys the deep learning model on the modern mobile device to realize autonomous object detection and object tracking of drones.\n\nIntroduction\n\nUnmanned aerial vehicle (UAV) is an aircraft without the need of an actual onboard pilot. UAV can be used in various scenarios including military and civil use [Li and Sun (2018); Quinn, Nyhan, Navarro et al. (2018)]. For instance, drones can be used to patrol certain area of interest and execute boarder reconnaissance in military operation. Also, many areas utilize the flexibility of the drone and the camera footage acquired by it to realize 3D reconstruction and autonomous flight [Kim, Liu, Lee et al. (2019)]. However, traditional methods of realizing autonomous flight or object detection usually use simultaneous localization and mapping (SLAM) based algorithms which require lots of sensors. Also, the image-based objection detection or avoidance methods typically require huge computational power and complicated algorithms which make it hard to realize. Recent advance in deep learning has proven that convolutional neural network (CNN) is very useful in computer vision related tasks. Many researchers and companies take advantage of the exceptional power of CNN and put the computer vision field to a new era [Nguyen, Arsalan, Kooet al. (2018);Xiang, Zhai, Lv and El Saddik (2018); Rivas, Chamoso, Gonz\u00e1 lez-Briones et al. (2018);He, Gkioxari, Doll\u00e1 r et al. (2017);Li, Jiang and Cheslyar (2018); Deniz, Vallez, Espinosa-Aranda et al. (2017)]. Object detection and object segmentation are among these successful applications [Girshick, Donahue, Darrell et al. (2014) ;Girshick, (2015); Ren, He, Girshick et al. (2017);Liu, Anguelov, Erhan et al. (2016)]. Object detection focuses on locating the independent object and the corresponding object class. This is harder than normal image classification tasks for it also has to determine the location of the object and puts a suitable bounding box around the object. The bounding box should contain the detected object with high confidence that the object belongs to certain class. This process has highly computational complexity which makes it hard to use in real-world applications. Furthermore, object detection on mobile video stream requires real-time features while maintaining high detection accuracy. You Only Look Once (YOLO) as the representative of the one-shot object detection method is specialized for high detection speed while still reserves relatively high accuracy which makes it very suitable for real-time applications [Redmon, Divvala, Girshick et al. (2016)]. This paper takes advantage of the flexibility of the modern mobile devices and deep learning based real-time object detection method to realize a mobile system which can automatically detect certain objects from the video stream of drone.\n\n\nRelated work\n\nConvolutional neural network (CNN) is a revolution technology in deep learning field, it typically consists of one input layer and one output layer and also includes multiple hidden layers between the input and output layer. The convolutional layer applies a convolution kernel over the input tensor and the convolutional operation is essentially a crosscorrelation mathematical operation which can reduce huge amount of network parameters while achieve the same goal as the traditional fully connected network. Krizhevsky et al. [Krizhevsky, Sutskever and Hinton (2012)] first introduced deep convolutional neural network to classify the ImageNet dataset and outclassed the traditional classification methods. A large number of novel applications using CNN sprang up after their successful work [Zhou, Liang, Li et al. (2018)]. CNN is mostly popular used in image classification, image object detection, image object segmentation, image style transfer and so on in computer vision field. Objection detection is a computer vision task which recognizes the class and the position of the object inside a given image. The detected object is annotated by a rectangle bounding box with certain class identifier. Recent studies have utilized deep learning to realize objection detection. Region-based convolutional neural network (R-CNN) for object detection is the representative of this kind of deep learning methods. It separates the object classification and bounding box prediction by using the region of interest method to detect object [Girshick, Donahue, Darrell et al. (2014)]. Fast R-CNN improves the speed of R-CNN by introducing Region of Interest (ROI) pooling which can share the convolutional layers [Girshick (2015)]. Faster R-CNN further improves Fast R-CNN by introducing Region Proposal Network and becomes the top framework in several benchmarks [Ren, He, Girshick et al. (2017)]. YOLO stands for another object detection party and differs from the above algorithms. YOLO uses a single convolutional network which predicts the bounding boxes and the class probabilities for the detected boxes simultaneously [Redmon, Divvala, Girshick et al. (2016)]. It works by dividing the input image into several grids and predicts several bounding boxes within each grid. The network outputs the class probability, the confidence score and bounding box offset values for each predicted bounding box. The confidence score shows how likely the detected area contains object and the class probability determines what object is inside the bounding box area. Quadcopter drones equipped with high resolution camera is a very suitable platform for real-time computer vision task. \u0160t\u011bp\u00e1n et al. [\u0160t\u011bp\u00e1n, Krajn\u00edk, Petrl\u00edk et al. (2019)] present computer vision modules of a multi-unmanned aerial vehicle system which runs completely on board in real time. They split the system into two separate tasks. The first one is responsible for finding, tracking, and landing on a human-driven car while the second one focuses on finding small colored objects in a wide area. Furthermore, deep learning models can be deployed on the onboard system or mobile control station to realize autonomous control of the drones. Rivas et al. [Rivas, Chamoso, Gonz\u00e1 lez-Briones et al. (2018)] use multirotor drones to detect cattle in real-time and employ the artificial intelligence techniques for the analysis of information captured by drones. They use a convolutional neural network to realize cattle detection and implement the system on a PC software. Xiang et al. [Xiang, Zhai, Lv et al. (2018)] use drones for vehicle counting in traffic monitoring. The detector they proposed can handle two situations including static background and moving background. Nguyen et al. [Nguyen, Arsalan, Koo et al. (2018)] propose a method which helps the autonomous landing of drones. They utilize the visiblelight-camera to develop a novel remote-marker-based tracking algorithm which can enable the drones to land in heterogeneous areas without GPS signal. They also use convolutional neural network to extract image features automatically and outperform the state-of-the-art objector trackers.\n\n\nSystem design\n\nThe object detection method used in this system is based on YOLO network. YOLO works by using a fixed grid of detectors instead of region proposal-based detectors. It consists of two networks where the first one serves as an image feature extractor and the second one is in charge of the real object detection. The first part is typically trained on the ImageNet dataset using Visual Geometry Group Net (VGGNet), InceptionNet or Residual Neural Network (ResNet) to obtain the best image feature representation. The object detection network predicts the class probabilities, the confidence score of whether or not a certain location contains an object and the bounding box coordinates. The main purpose of using grid cell is to limit the detector to find objects in certain regions so as to improve the accuracy and detection speed. Anchor box is used during the training process to help the detector understand the most common object shapes. The network is mainly comprised of these following basic convolutional blocks: \u2192 \u2192 . And the last couple of layers only contain the \u2192 blocks to generate the final bounding box grid cells. The grid cells are further processed to obtain the final object detection information.  Fig. 1 shows, the mobile system works by parsing the video frame of the drone camera first. And in order to achieve low video latency as the system processing the frame images, the object detection method is required to have a high detection speed. Tiny-YOLO is recommended in this circumstance which can achieve high detection speed while ensure a relative high detection accuracy. The object detection model is typically trained on a local server using a specific dataset such as (Common Objects in Context) COCO dataset. The trained object detection model is then converted to another format in order to be compatible with mobile devices. The conversion tools include TensorFlowLite, Caffe2, CoreMLTools, etc. The converted mobile version model is then deployed on the modern mobile devices while combining the mobile SDKs for drones. In this case, all the object detection process happens directly on the mobile device which can reduce the consumption of the battery of drones. The system can further process the detected objects to control the movements of the drone.\n\n\nImplementation details\n\nThe proposed system is implemented on a modern iOS device to further demonstrate the feasibility. The object detection model is converted to CoreML format in order to deploy the model on the iOS device. CoreML is introduced by Apple company to enable better machine learning features on mobile devices. The mobile system receives the live video stream and displays it on the screen while the trained model is running at the background to detect known objects. The video stream is decoded into iOS CVPixelBuffer frame in order to apply the object detection algorithm. The bounding box rendering position is further calculated according to the anchor boxes and screen resolution. After detecting the object, user can choose whether to follow certain moving object or not. The detailed implementation architecture is shown as Fig. 1.  Fig. 3 shows the target tracking result. Note that the full YOLO network is relatively big considering the computational power of the mobile phone. When the system detects certain object, the previous detected bounding box can be slightly lagging behind sometimes as shown in Fig. 2. Fast-YOLO (Tiny-YOLO) is a refined object detection model which resembles Full-YOLO network but specializes for real-time applications. The entire Tiny-YOLO architecture has fewer \u2192 \u2192 blocks but similar architecture as the Full-YOLO. It shows much lesser bounding box latency than the full model one but also results in lower detection accuracy. As shown in Fig. 4, the object position can be detected with high deviation or even some objects are not detected at all.\n\n\nFigure 5: Tiny-YOLO detection\n\nThe testing mobile device for this experiment is an Apple iPhone 7 plus which could also be one of the potential reasons the algorithm runs slow. According to the official announcement of the Apple company, the newest device should run the same model with a 10x times faster speed. The drone used in the experiment is a DJI Mavic Pro and the mobile system is written in swift language using iOS SDK and DJI Mobile SDK. The object detection network is first trained on a local computer using COCO dataset [Lin, Maire, Belongie et al. (2014)]. The learning rate is set to 0.001 and the network is trained using one Nvidia GTX1080 GPU for 10000 steps. All the training images are reformatted to 416x416x3 shape with basic data augmentation technique.\n\n\nCitations\n\nThis paper applies the object detection algorithm using convolutional neural network to mobile drone applications. The live video transmitted from the main camera is shown on the mobile device, and the video stream is decoded to separated frames in order to send to the YOLO object detection network to obtain the object bounding box and class prediction. It also implements the feature that user can track certain detected moving objects. This system can further enable drones to realize autonomous flight control, autonomous object tracking and obstacle avoidance. Also, by implementing the instance segmentation method such as Mask R-CNN, the mobile drone system will be able to detect the instance contour and track objects more accurately. Furthermore, there are some privacy and security issues that need to be considered in the future. fund; by the Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET) fund, China.\n\nFigure 1 :\n1System frameworkAs\n\nFigure 2 :\n2Detailed implementation architecture Fig. 2 shows the object detection example result and\n\nFigure 3 :Figure 4 :\n34Drone object detection using Full-YOLO model Object tracking\n\nO Deniz, N Vallez, J Espinosa-Aranda, J Rico-Saavedra, J Parra-Patino, Eyes of things. Sensors. 171173Deniz, O.; Vallez, N.; Espinosa-Aranda, J.; Rico-Saavedra, J.; Parra-Patino, J. et al. (2017): Eyes of things. Sensors, vol. 17, no. 5, pp. 1173.\n\nR Girshick, Fast R-CNN. Proceedings of the IEEE International Conference on Computer Vision. Girshick, R. (2015): Fast R-CNN. Proceedings of the IEEE International Conference on Computer Vision, pp. 1440-1448.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, IEEE Conference on Computer Vision and Pattern Recognition. Girshick, R.; Donahue, J.; Darrell, T.; Malik, J. (2014): Rich feature hierarchies for accurate object detection and semantic segmentation. IEEE Conference on Computer Vision and Pattern Recognition, pp. 580-587.\n\nK He, G Gkioxari, P Doll\u00e1r, R Girshick, Mask R-CNN. IEEE International Conference on Computer Vision. He, K.; Gkioxari, G.; Doll\u00e1r, P.; Girshick, R. (2017): Mask R-CNN. IEEE International Conference on Computer Vision, pp. 2980-2988.\n\nRemote proximity monitoring between mobile construction resources using camera-mounted UAVs. D Kim, M Liu, S Lee, V R Kamat, Automation in Construction. 99Kim, D.; Liu, M.; Lee, S.; Kamat, V. R. (2019): Remote proximity monitoring between mobile construction resources using camera-mounted UAVs. Automation in Construction, vol. 99, pp. 168-182.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems. Krizhevsky, A.; Sutskever, I.; Hinton, G. E. (2012): Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, pp. 1097-1105.\n\nEmbedding image through generated intermediate medium using deep convolutional generative adversarial network. C Li, Y Jiang, M Cheslyar, Computers, Materials & Continua. 562Li, C.; Jiang, Y.; Cheslyar, M. (2018): Embedding image through generated intermediate medium using deep convolutional generative adversarial network. Computers, Materials & Continua, vol. 56, no. 2, pp. 313-324.\n\nA novel meteorological sensor data acquisition approach based on unmanned aerial vehicle. C Li, X Sun, International Journal of Sensor Networks. 282Li, C.; Sun, X. (2018): A novel meteorological sensor data acquisition approach based on unmanned aerial vehicle. International Journal of Sensor Networks, vol. 28, no. 2, pp. 80-88.\n\nT Lin, M Maire, S Belongie, J Hays, P Perona, Microsoft COCO: common objects in context. European Conference on Computer Vision. 8693Lin, T.; Maire, M.; Belongie, S.; Hays, J.; Perona, P. et al. (2014): Microsoft COCO: common objects in context. European Conference on Computer Vision, vol. 8693, pp. 740-755.\n\nW Liu, D Anguelov, D Erhan, C Szegedy, S Reed, SSD: single shot multiBox detector. European Conference on Computer Vision. Liu, W.; Anguelov, D.; Erhan, D.; Szegedy, C.; Reed, S. et al. (2016): SSD: single shot multiBox detector. European Conference on Computer Vision, pp. 21-37.\n\nLightDenseYOLO: a fast and accurate marker tracker for autonomous UAV landing by visible light camera sensor on drone. P H Nguyen, M Arsalan, J H Koo, R A Naqvi, N Q Truong, Sensors. 1861703Nguyen, P. H.; Arsalan, M.; Koo, J. H.; Naqvi, R. A.; Truong, N. Q. et al. (2018): LightDenseYOLO: a fast and accurate marker tracker for autonomous UAV landing by visible light camera sensor on drone. Sensors, vol. 18, no. 6, pp. 1703.\n\nHumanitarian applications of machine learning with remote-sensing data: review and case study in refugee settlement mapping. J A Quinn, M M Nyhan, C Navarro, D Coluccia, L Bromley, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 376Quinn, J. A.; Nyhan, M. M.; Navarro, C.; Coluccia, D.; Bromley, L. et al. (2018): Humanitarian applications of machine learning with remote-sensing data: review and case study in refugee settlement mapping. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 376, no. 2128.\n\nYou only look once: unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRedmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. (2016): You only look once: unified, real-time object detection. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779-788.\n\nFaster R-CNN: towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, IEEE Transactions on Pattern Analysis & Machine Intelligence. 6Ren, S.; He, K.; Girshick, R.; Sun, J. (2017): Faster R-CNN: towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 6, pp. 1137-1149.\n\nA Rivas, P Chamoso, A Gonz\u00e1lez-Briones, J Corchado, Detection of cattle using drones and convolutional neural networks. Sensors. 182048Rivas, A.; Chamoso, P.; Gonz\u00e1lez-Briones, A.; Corchado, J. (2018): Detection of cattle using drones and convolutional neural networks. Sensors, vol. 18, no. 7, pp. 2048.\n\nVision techniques for on-board detection, following, and mapping of moving targets. P \u0160t\u011bp\u00e1n, T Krajn\u00edk, M Petrl\u00ed K, M Saska, Journal of Field Robotics. 361\u0160t\u011bp\u00e1n, P.; Krajn\u00edk, T.; Petrl\u00ed k, M.; Saska, M. (2019): Vision techniques for on-board detection, following, and mapping of moving targets. Journal of Field Robotics, vol. 36, no. 1, pp. 252-269.\n\nX Xiang, M Zhai, N Lv, A El Saddik, Vehicle counting based on vehicle detection and tracking from aerial videos. Sensors. 182560Xiang, X.; Zhai, M.; Lv, N.; El Saddik, A. (2018): Vehicle counting based on vehicle detection and tracking from aerial videos. Sensors, vol. 18, no. 8, pp. 2560.\n\nImproved VGG model for road traffic sign recognition. S Zhou, W Liang, J Li, J Kim, Computers, Materials & Continua. 571Zhou, S.; Liang, W.; Li, J.; Kim, J. (2018): Improved VGG model for road traffic sign recognition. Computers, Materials & Continua, vol. 57, no. 1, pp. 11-24.\n", "annotations": {"author": "[{\"end\":88,\"start\":75},{\"end\":102,\"start\":89},{\"end\":114,\"start\":103}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":85},{\"end\":101,\"start\":98},{\"end\":113,\"start\":110}]", "author_first_name": "[{\"end\":84,\"start\":75},{\"end\":97,\"start\":89},{\"end\":109,\"start\":103}]", "author_affiliation": null, "title": "[{\"end\":68,\"start\":1},{\"end\":182,\"start\":115}]", "venue": "[{\"end\":187,\"start\":184}]", "abstract": "[{\"end\":1513,\"start\":266}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1707,\"start\":1689},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1744,\"start\":1709},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2044,\"start\":2016},{\"end\":2688,\"start\":2653},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2724,\"start\":2688},{\"end\":2774,\"start\":2726},{\"end\":2810,\"start\":2774},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2839,\"start\":2810},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2885,\"start\":2841},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3010,\"start\":2969},{\"end\":3012,\"start\":3011},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3028,\"start\":3012},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3062,\"start\":3030},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3096,\"start\":3062},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3970,\"start\":3930},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4798,\"start\":4758},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5054,\"start\":5024},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5806,\"start\":5765},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5953,\"start\":5937},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6120,\"start\":6088},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6390,\"start\":6350},{\"end\":6957,\"start\":6918},{\"end\":7493,\"start\":7445},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7803,\"start\":7773},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8013,\"start\":7978},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12880,\"start\":12845}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":14096,\"start\":14065},{\"attributes\":{\"id\":\"fig_1\"},\"end\":14199,\"start\":14097},{\"attributes\":{\"id\":\"fig_2\"},\"end\":14284,\"start\":14200}]", "paragraph": "[{\"end\":4211,\"start\":1529},{\"end\":8389,\"start\":4228},{\"end\":10697,\"start\":8407},{\"end\":12307,\"start\":10724},{\"end\":13088,\"start\":12341},{\"end\":14064,\"start\":13102}]", "formula": null, "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1527,\"start\":1515},{\"attributes\":{\"n\":\"2\"},\"end\":4226,\"start\":4214},{\"attributes\":{\"n\":\"3\"},\"end\":8405,\"start\":8392},{\"attributes\":{\"n\":\"4\"},\"end\":10722,\"start\":10700},{\"end\":12339,\"start\":12310},{\"attributes\":{\"n\":\"5\"},\"end\":13100,\"start\":13091},{\"end\":14076,\"start\":14066},{\"end\":14108,\"start\":14098},{\"end\":14221,\"start\":14201}]", "table": null, "figure_caption": "[{\"end\":14096,\"start\":14078},{\"end\":14199,\"start\":14110},{\"end\":14284,\"start\":14224}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9631,\"start\":9625},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11553,\"start\":11547},{\"end\":11562,\"start\":11556},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11838,\"start\":11832},{\"end\":12204,\"start\":12198}]", "bib_author_first_name": "[{\"end\":14287,\"start\":14286},{\"end\":14296,\"start\":14295},{\"end\":14306,\"start\":14305},{\"end\":14325,\"start\":14324},{\"end\":14342,\"start\":14341},{\"end\":14536,\"start\":14535},{\"end\":14829,\"start\":14828},{\"end\":14841,\"start\":14840},{\"end\":14852,\"start\":14851},{\"end\":14863,\"start\":14862},{\"end\":15146,\"start\":15145},{\"end\":15152,\"start\":15151},{\"end\":15164,\"start\":15163},{\"end\":15174,\"start\":15173},{\"end\":15474,\"start\":15473},{\"end\":15481,\"start\":15480},{\"end\":15488,\"start\":15487},{\"end\":15495,\"start\":15494},{\"end\":15497,\"start\":15496},{\"end\":15793,\"start\":15792},{\"end\":15807,\"start\":15806},{\"end\":15820,\"start\":15819},{\"end\":15822,\"start\":15821},{\"end\":16179,\"start\":16178},{\"end\":16185,\"start\":16184},{\"end\":16194,\"start\":16193},{\"end\":16546,\"start\":16545},{\"end\":16552,\"start\":16551},{\"end\":16788,\"start\":16787},{\"end\":16795,\"start\":16794},{\"end\":16804,\"start\":16803},{\"end\":16816,\"start\":16815},{\"end\":16824,\"start\":16823},{\"end\":17099,\"start\":17098},{\"end\":17106,\"start\":17105},{\"end\":17118,\"start\":17117},{\"end\":17127,\"start\":17126},{\"end\":17138,\"start\":17137},{\"end\":17500,\"start\":17499},{\"end\":17502,\"start\":17501},{\"end\":17512,\"start\":17511},{\"end\":17523,\"start\":17522},{\"end\":17525,\"start\":17524},{\"end\":17532,\"start\":17531},{\"end\":17534,\"start\":17533},{\"end\":17543,\"start\":17542},{\"end\":17545,\"start\":17544},{\"end\":17934,\"start\":17933},{\"end\":17936,\"start\":17935},{\"end\":17945,\"start\":17944},{\"end\":17947,\"start\":17946},{\"end\":17956,\"start\":17955},{\"end\":17967,\"start\":17966},{\"end\":17979,\"start\":17978},{\"end\":18478,\"start\":18477},{\"end\":18488,\"start\":18487},{\"end\":18499,\"start\":18498},{\"end\":18511,\"start\":18510},{\"end\":18952,\"start\":18951},{\"end\":18959,\"start\":18958},{\"end\":18965,\"start\":18964},{\"end\":18977,\"start\":18976},{\"end\":19260,\"start\":19259},{\"end\":19269,\"start\":19268},{\"end\":19280,\"start\":19279},{\"end\":19300,\"start\":19299},{\"end\":19650,\"start\":19649},{\"end\":19660,\"start\":19659},{\"end\":19671,\"start\":19670},{\"end\":19683,\"start\":19682},{\"end\":19920,\"start\":19919},{\"end\":19929,\"start\":19928},{\"end\":19937,\"start\":19936},{\"end\":19943,\"start\":19942},{\"end\":20266,\"start\":20265},{\"end\":20274,\"start\":20273},{\"end\":20283,\"start\":20282},{\"end\":20289,\"start\":20288}]", "bib_author_last_name": "[{\"end\":14293,\"start\":14288},{\"end\":14303,\"start\":14297},{\"end\":14322,\"start\":14307},{\"end\":14339,\"start\":14326},{\"end\":14355,\"start\":14343},{\"end\":14545,\"start\":14537},{\"end\":14838,\"start\":14830},{\"end\":14849,\"start\":14842},{\"end\":14860,\"start\":14853},{\"end\":14869,\"start\":14864},{\"end\":15149,\"start\":15147},{\"end\":15161,\"start\":15153},{\"end\":15171,\"start\":15165},{\"end\":15183,\"start\":15175},{\"end\":15478,\"start\":15475},{\"end\":15485,\"start\":15482},{\"end\":15492,\"start\":15489},{\"end\":15503,\"start\":15498},{\"end\":15804,\"start\":15794},{\"end\":15817,\"start\":15808},{\"end\":15829,\"start\":15823},{\"end\":16182,\"start\":16180},{\"end\":16191,\"start\":16186},{\"end\":16203,\"start\":16195},{\"end\":16549,\"start\":16547},{\"end\":16556,\"start\":16553},{\"end\":16792,\"start\":16789},{\"end\":16801,\"start\":16796},{\"end\":16813,\"start\":16805},{\"end\":16821,\"start\":16817},{\"end\":16831,\"start\":16825},{\"end\":17103,\"start\":17100},{\"end\":17115,\"start\":17107},{\"end\":17124,\"start\":17119},{\"end\":17135,\"start\":17128},{\"end\":17143,\"start\":17139},{\"end\":17509,\"start\":17503},{\"end\":17520,\"start\":17513},{\"end\":17529,\"start\":17526},{\"end\":17540,\"start\":17535},{\"end\":17552,\"start\":17546},{\"end\":17942,\"start\":17937},{\"end\":17953,\"start\":17948},{\"end\":17964,\"start\":17957},{\"end\":17976,\"start\":17968},{\"end\":17987,\"start\":17980},{\"end\":18485,\"start\":18479},{\"end\":18496,\"start\":18489},{\"end\":18508,\"start\":18500},{\"end\":18519,\"start\":18512},{\"end\":18956,\"start\":18953},{\"end\":18962,\"start\":18960},{\"end\":18974,\"start\":18966},{\"end\":18981,\"start\":18978},{\"end\":19266,\"start\":19261},{\"end\":19277,\"start\":19270},{\"end\":19297,\"start\":19281},{\"end\":19309,\"start\":19301},{\"end\":19657,\"start\":19651},{\"end\":19668,\"start\":19661},{\"end\":19680,\"start\":19672},{\"end\":19689,\"start\":19684},{\"end\":19926,\"start\":19921},{\"end\":19934,\"start\":19930},{\"end\":19940,\"start\":19938},{\"end\":19953,\"start\":19944},{\"end\":20271,\"start\":20267},{\"end\":20280,\"start\":20275},{\"end\":20286,\"start\":20284},{\"end\":20293,\"start\":20290}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":14533,\"start\":14286},{\"attributes\":{\"id\":\"b1\"},\"end\":14744,\"start\":14535},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":215827080},\"end\":15143,\"start\":14746},{\"attributes\":{\"id\":\"b3\"},\"end\":15378,\"start\":15145},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":116695516},\"end\":15725,\"start\":15380},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":195908774},\"end\":16065,\"start\":15727},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":139766649},\"end\":16453,\"start\":16067},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":67867854},\"end\":16785,\"start\":16455},{\"attributes\":{\"id\":\"b8\"},\"end\":17096,\"start\":16787},{\"attributes\":{\"id\":\"b9\"},\"end\":17378,\"start\":17098},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":44146600},\"end\":17806,\"start\":17380},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":51930946},\"end\":18418,\"start\":17808},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594738},\"end\":18869,\"start\":18420},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10328909},\"end\":19257,\"start\":18871},{\"attributes\":{\"id\":\"b14\"},\"end\":19563,\"start\":19259},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":58013782},\"end\":19917,\"start\":19565},{\"attributes\":{\"id\":\"b16\"},\"end\":20209,\"start\":19919},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":54712953},\"end\":20489,\"start\":20211}]", "bib_title": "[{\"end\":14826,\"start\":14746},{\"end\":15471,\"start\":15380},{\"end\":15790,\"start\":15727},{\"end\":16176,\"start\":16067},{\"end\":16543,\"start\":16455},{\"end\":17497,\"start\":17380},{\"end\":17931,\"start\":17808},{\"end\":18475,\"start\":18420},{\"end\":18949,\"start\":18871},{\"end\":19647,\"start\":19565},{\"end\":20263,\"start\":20211}]", "bib_author": "[{\"end\":14295,\"start\":14286},{\"end\":14305,\"start\":14295},{\"end\":14324,\"start\":14305},{\"end\":14341,\"start\":14324},{\"end\":14357,\"start\":14341},{\"end\":14547,\"start\":14535},{\"end\":14840,\"start\":14828},{\"end\":14851,\"start\":14840},{\"end\":14862,\"start\":14851},{\"end\":14871,\"start\":14862},{\"end\":15151,\"start\":15145},{\"end\":15163,\"start\":15151},{\"end\":15173,\"start\":15163},{\"end\":15185,\"start\":15173},{\"end\":15480,\"start\":15473},{\"end\":15487,\"start\":15480},{\"end\":15494,\"start\":15487},{\"end\":15505,\"start\":15494},{\"end\":15806,\"start\":15792},{\"end\":15819,\"start\":15806},{\"end\":15831,\"start\":15819},{\"end\":16184,\"start\":16178},{\"end\":16193,\"start\":16184},{\"end\":16205,\"start\":16193},{\"end\":16551,\"start\":16545},{\"end\":16558,\"start\":16551},{\"end\":16794,\"start\":16787},{\"end\":16803,\"start\":16794},{\"end\":16815,\"start\":16803},{\"end\":16823,\"start\":16815},{\"end\":16833,\"start\":16823},{\"end\":17105,\"start\":17098},{\"end\":17117,\"start\":17105},{\"end\":17126,\"start\":17117},{\"end\":17137,\"start\":17126},{\"end\":17145,\"start\":17137},{\"end\":17511,\"start\":17499},{\"end\":17522,\"start\":17511},{\"end\":17531,\"start\":17522},{\"end\":17542,\"start\":17531},{\"end\":17554,\"start\":17542},{\"end\":17944,\"start\":17933},{\"end\":17955,\"start\":17944},{\"end\":17966,\"start\":17955},{\"end\":17978,\"start\":17966},{\"end\":17989,\"start\":17978},{\"end\":18487,\"start\":18477},{\"end\":18498,\"start\":18487},{\"end\":18510,\"start\":18498},{\"end\":18521,\"start\":18510},{\"end\":18958,\"start\":18951},{\"end\":18964,\"start\":18958},{\"end\":18976,\"start\":18964},{\"end\":18983,\"start\":18976},{\"end\":19268,\"start\":19259},{\"end\":19279,\"start\":19268},{\"end\":19299,\"start\":19279},{\"end\":19311,\"start\":19299},{\"end\":19659,\"start\":19649},{\"end\":19670,\"start\":19659},{\"end\":19682,\"start\":19670},{\"end\":19691,\"start\":19682},{\"end\":19928,\"start\":19919},{\"end\":19936,\"start\":19928},{\"end\":19942,\"start\":19936},{\"end\":19955,\"start\":19942},{\"end\":20273,\"start\":20265},{\"end\":20282,\"start\":20273},{\"end\":20288,\"start\":20282},{\"end\":20295,\"start\":20288}]", "bib_venue": "[{\"end\":14380,\"start\":14357},{\"end\":14626,\"start\":14547},{\"end\":14929,\"start\":14871},{\"end\":15245,\"start\":15185},{\"end\":15531,\"start\":15505},{\"end\":15880,\"start\":15831},{\"end\":16236,\"start\":16205},{\"end\":16598,\"start\":16558},{\"end\":16914,\"start\":16833},{\"end\":17219,\"start\":17145},{\"end\":17561,\"start\":17554},{\"end\":18087,\"start\":17989},{\"end\":18598,\"start\":18521},{\"end\":19043,\"start\":18983},{\"end\":19386,\"start\":19311},{\"end\":19716,\"start\":19691},{\"end\":20039,\"start\":19955},{\"end\":20326,\"start\":20295},{\"end\":18662,\"start\":18600}]"}}}, "year": 2023, "month": 12, "day": 17}