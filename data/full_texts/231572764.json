{"id": 231572764, "updated": "2023-10-06 07:04:41.421", "metadata": {"title": "Towards Real-World Blind Face Restoration with Generative Facial Prior", "authors": "[{\"first\":\"Xintao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Honglun\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Shan\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 1, "day": 11}, "abstract": "Blind face restoration usually relies on facial priors, such as facial geometry prior or reference prior, to restore realistic and faithful details. However, very low-quality inputs cannot offer accurate geometric prior while high-quality references are inaccessible, limiting the applicability in real-world scenarios. In this work, we propose GFP-GAN that leverages rich and diverse priors encapsulated in a pretrained face GAN for blind face restoration. This Generative Facial Prior (GFP) is incorporated into the face restoration process via novel channel-split spatial feature transform layers, which allow our method to achieve a good balance of realness and fidelity. Thanks to the powerful generative facial prior and delicate designs, our GFP-GAN could jointly restore facial details and enhance colors with just a single forward pass, while GAN inversion methods require expensive image-specific optimization at inference. Extensive experiments show that our method achieves superior performance to prior art on both synthetic and real-world datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2101.04061", "mag": "3118892572", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/WangLZS21", "doi": "10.1109/cvpr46437.2021.00905"}}, "content": {"source": {"pdf_hash": "54483c5672bbee68cb77969d9cf6681209230b95", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2101.04061v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2101.04061", "status": "GREEN"}}, "grobid": {"id": "44bc011aeb7baeaf8f44b256d925cd6128251fb5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/54483c5672bbee68cb77969d9cf6681209230b95.txt", "contents": "\nTowards Real-World Blind Face Restoration with Generative Facial Prior\n\n\nXintao Wang xintaowang@tencent.com \nApplied Research Center (ARC)\nTencent PCG\n\nYu Li \nApplied Research Center (ARC)\nTencent PCG\n\nHonglun Zhang honlanzhang@tencent.com \nApplied Research Center (ARC)\nTencent PCG\n\nYing Shan yingsshan@tencent.com \nApplied Research Center (ARC)\nTencent PCG\n\nTowards Real-World Blind Face Restoration with Generative Facial Prior\n/TencentARC/GFPGAN GFP-GAN Ours Input From real life PULSE CVPR 20 CVPR 20 Wan et al. HiFaceGAN ACMMM 20 DFDNet ECCV 20\nFigure 1: Comparisons with state-of-the-art face restoration methods: HiFaceGAN [67], DFDNet [44], Wan et al.[61]and PULSE [52] on the real-world low-quality images. While previous methods struggle to restore faithful facial details or retain face identity, our proposed GFP-GAN achieves a good balance of realness and fidelity with much fewer artifacts. In addition, the powerful generative facial prior allows us to perform restoration and color enhancement jointly. (Zoom in for best view) Abstract Blind face restoration usually relies on facial priors, such as facial geometry prior or reference prior, to restore realistic and faithful details. However, very low-quality inputs cannot offer accurate geometric prior while highquality references are inaccessible, limiting the applicability in real-world scenarios. In this work, we propose GFP-GAN that leverages rich and diverse priors encapsulated in a pretrained face GAN for blind face restoration. This Generative Facial Prior (GFP) is incorporated into the face restoration process via spatial feature transform layers, which allow our method to achieve a good balance of realness and fidelity. Thanks to the powerful generative facial prior and delicate designs, our GFP-GAN could jointly restore facial details and enhance colors with just a single forward pass, while GAN inversion methods require image-specific optimization at inference. Extensive experiments show that our method achieves superior performance to prior art on both synthetic and real-world datasets.\n\nIntroduction\n\nBlind face restoration aims at recovering high-quality faces from the low-quality counterparts suffering from unknown degradation, such as low-resolution [13,48,9], noise [71], blur [39,58], compression artifacts [12], etc. When applied to real-world scenarios, it becomes more challenging, due to more complicated degradation, diverse poses and expressions. Previous works [9,69,6] typically exploit face-specific priors in face restoration, such as facial landmarks [9], parsing maps [6,9], facial component heatmaps [69], and show that those geometry facial priors are pivotal to recover accurate face shape and details. However, those priors are usually estimated from input images and inevitably degrades with very low-quality inputs in the real world. In addition, despite their semantic guidance, the above priors contain limited texture information for restoring facial details (e.g., eye pupil).\n\nAnother category of approaches investigates reference priors, i.e., high-quality guided faces [46,45,11] or facial component dictionaries [44], to generate realistic results and alleviate the dependency on degraded inputs. However, the inaccessibility of high-resolution references limits its practical applicability, while the limited capacity of dictionaries restricts its diversity and richness of facial details.\n\nIn this study, we leverage Generative Facial Prior (GFP) for real-world blind face restoration, i.e., the prior implicitly encapsulated in pretrained face Generative Adversarial Network (GAN) [18] models such as StyleGAN [35,36]. These face GANs are capable of generating faithful faces with a high degree of variability, and thereby providing rich and diverse priors such as geometry, facial textures and colors, making it possible to jointly restore facial details and enhance colors (Fig. 1). However, it is challenging to incorporate such generative priors into the restoration process. Previous attempts typically use GAN inversion [19,54,52]. They first 'invert' the degraded image back to a latent code of the pretrained GAN, and then conduct expensive imagespecific optimization to reconstruct images. Despite visually realistic outputs, they usually produce images with low fidelity, as the low-dimension latent codes are insufficient to guide accurate restoration.\n\nTo address these challenges, we propose the GFP-GAN with delicate designs to achieve a good balance of realness and fidelity in a single forward pass. Specifically, GFP-GAN consists of a degradation removal module and a pretrained face GAN as facial prior. They are connected by a direct latent code mapping, and several Channel-Split Spatial Feature Transform (CS-SFT) layers in a coarse-to-fine manner. The proposed CS-SFT layers perform spatial modulation on a split of features and leave the left features to directly pass through for better information preservation, allowing our method to effectively incorporate generative prior while retraining high fidelity. Besides, we introduce facial component loss with local discriminators to further enhance perceptual facial details, while employing identity preserving loss to further improve fidelity.\n\nWe summarize the contributions as follows. (1) We leverage rich and diverse generative facial priors for blind face restoration. Those priors contain sufficient facial textures and color information, allowing us to jointly perform face restoration and color enhancement. (2) We propose the GFP-GAN framework with delicate designs of architectures and losses to incorporate generative facial prior. Our GFP-GAN with CS-SFT layers achieves a good balance of fidelity and texture faithfulness in a single forward pass. (3) Extensive experiments show that our method achieves superior performance to prior art on both synthetic and realworld datasets.\n\n\nRelated Work\n\nImage Restoration typically includes super-resolution [13,48,60,49,74,68,22,50], denoising [71,42,26], deblurring [65,39,58] and compression removal [12,21].\n\nTo achieve visually-pleasing results, generative adversarial network [18] is usually employed as loss supervisions to push the solutions closer to the natural manifold [41,57,64,7,14], while our work attempts to leverage the pretrained face GANs as generative facial priors (GFP). Face Restoration. Based on general face hallucination [5,30,66,70], two typical face-specific priors: geometry priors and reference priors, are incorporated to further improve the performance. The geometry priors include facial landmarks [9,37,77], face parsing maps [58,6,9] and facial component heatmaps [69]. However, 1) those priors require estimations from low-quality inputs and inevitably degrades in real-world scenarios. 2) They mainly focus on geometry constraints and may not contain adequate details for restoration. Instead, our employed GFP does not involve an explicit geometry estimation from degraded images, and contains adequate textures inside its pretrained network.\n\nReference priors [46,45,11] usually rely on reference images of the same identity. To overcome this issue, DFD-Net [44] suggests to construct a face dictionary of each component (e.g., eyes, mouth) with CNN features to guide the restoration. However, DFDNet mainly focuses on components in the dictionary and thus degrades in the regions beyond its dictionary scope (e.g., hair, ears and face contour), instead, our GFP-GAN could treat faces as a whole to restore. Moreover, the limited size of dictionary restricts its diversity and richness, while the GFP could provide rich and diverse priors including geometry, textures and colors. Generative Priors of pretrained GANs [34,35,36,3] is previously exploited by GAN inversion [1,76,54,19], whose primary aim is to find the closest latent codes given an input image. PULSE [52] iteratively optimizes the latent code of StyleGAN [35] until the distance between outputs and inputs is below a threshold. mGANprior [19] attempts to optimize multiple codes to improve the reconstruction quality. However, these methods usually produce images with low fidelity, as the low-dimension latent codes are insufficient to guide the restoration. In contrast, our proposed CS-SFT modulation layers enable prior incorporation on multi-resolution spatial features to achieve high fidelity. Besides, expensive iterative optimization is not required in our GFP-GAN during inference. Channel Split Operation is usually explored to design compact models and improve model representation ability. MobileNet [28] proposes depthwise convolutions and GhostNet [23] splits the convolutional layer into two parts and uses fewer filters to generate intrinsic feature maps. Dual path architecture in DPN [8] enables feature re-usage and new feature exploration for each path, thus improving its representation ability. A similar idea is also employed in super-resolution [75]. Our CS-SFT layers share the similar spirits, but with different operations and purposes. We adopt spatial feature transform [63,55] on one split and leave the left split as identity to achieve a good balance of realness and fidelity. Local Component Discriminators. Local discriminator is proposed to focus on local patch distributions [32,47,62]. When applied to faces, those discriminative losses are imposed on separate semantic facial regions [43,20]. Our introduced facial component loss also adopts such designs but with a further style supervision based on the learned discriminative features.\n\n\nMethodology\n\n\nOverview of GFP-GAN\n\nWe describe GFP-GAN framework in this section. Given an input facial image x suffering from unknown degradation, the aim of blind face restoration is to estimate a high-quality image\u0177, which is as similar as possible to the ground-truth image y, in terms of realness and fidelity.\n\nThe overall framework of GFP-GAN is depicted in Fig. 2. GFP-GAN is comprised of a degradation removal module (U-Net) and a pretrained face GAN (such as Style-GAN2 [36]) as prior. They are bridged by a latent code mapping and several Channel-Split Spatial Feature Transform (CS-SFT) layers. Specifically, the degradation removal module is designed to remove complicated degradation, and extract two kinds of features, i.e. 1) latent features F latent to map the input image to the closest latent code in StyleGAN2, and 2) multi-resolution spatial features F spatial for modulating the StyleGAN2 features.\n\nAfter that, F latent is mapped to intermediate latent codes W by several linear layers. Given the close latent code to the input image, StyleGAN2 could generate intermediate convolutional features, denoted by F GAN . These features provide rich facial details captured in the weights of pre-trained GAN. Multi-resolution features F spatial are used to spatially modulate the face GAN features F GAN with the proposed CS-SFT layers in a coarse-to-fine manner, achieving realistic results while preserving high fidelity.\n\nDuring training, except for the global discriminative loss, we introduce facial component loss with discriminators to enhance the perceptually significant face components, i.e., eyes and mouth. In order to retrain identity, we also employ identity preserving guidance.\n\n\nDegradation Removal Module\n\nReal-world blind face restoration faces with complicated and severer degradation, which is typically a mixture of low-resolution, blur, noise and JPEG artifacts. The degradation removal module is designed to explicitly remove the above degradation and extract 'clean' features F latent and F spatial , alleviating the burden of subsequent modules. We adopt the U-Net [56] structure as our degradation remove module, as it could 1) increase receptive field for large blur elimination, and 2) generate multi-resolution features. The formulation is as follows:\nF latent , F spatial = U-Net(x).(1)\nThe latent features F latent is used to map the input image to the closest latent code in StyleGAN2 (Sec. 3.3). The multiresolution spatial features F spatial are used to modulate the StyleGAN2 features (Sec. 3.4). In order to have an intermediate supervision for removing degradation, we employ the L1 restoration loss in each resolution scale in the early stage of training. Specifically, we also output images for each resolution scale of the U-Net decoder, and then restrict these outputs to be close to the pyramid of the ground-truth image.\n\n\nGenerative Facial Prior and Latent Code Mapping\n\nA pre-trained face GAN captures a distribution over faces in its leaned weights of convolutions, namely, generative prior [19,54]. We leverage such pretrained face GANs to provide diverse and rich facial details for our task. A typical way of deploying generative priors is to map the input image to its closest latent codes Z, and then generate the corresponding output by a pretrained GAN [1,76,54,19]. However, these methods usually require time-consuming iterative optimization for preserving fidelity. Instead of producing a final image directly, we generate intermediate convolutional features F GAN of the closest face, as it contains more details and could be further modulated by input features for better fidelity (see Sec. 3.4).\n\nSpecifically, given the encoded vector F latent of the input image (produced by the U-Net, Eq. 1), we first map it to intermediate latent codes W for better preserving semantic property i.e., the intermediate space transformed from Z with several multi-layer perceptron layers (MLP) [76]. The latent codes W then pass through each convolution layer in the pre-trained GAN, and generate GAN features for each resolution scale.\nW = MLP(F latent ), F GAN = StyleGAN(W).(2)\nDiscussion: Joint Restoration and Color Enhancement. Generative models capture diverse and rich priors beyond realistic details and vivid textures. For instance, they also encapsulate color priors, which could be employed in our task for joint face restoration and color enhancement. Realworld face images, e.g., old photos, usually have black-andwhite color, vintage yellow color, or dim color. Lively color prior in generative facial prior allows us to perform color enhancement including colorization [72]. We believe the generative facial priors also incorporate conventional geometric priors [9,69], 3D priors [16], etc. for restoration and manipulation.\n\n\nChannel-Split Spatial Feature Transform\n\nIn order to better preserve fidelity, we further use the input spatial features F spatial (produced by the U-Net, Eq. 1) to modulate the GAN features F GAN from Eq. 2. Preserving spatial information from inputs is crucial for face restoration, as it usually requires local characteristics for fidelity preservation, and adaptive restoration at different spatial locations of a face. Therefore, we employ Spatial Feature Transform (SFT) [63], which generates affine transformation parameters for spatial-wise feature modulation, and has shown its effectiveness in incorporating other conditions in image restoration [63,44] and image generation [55].\n\nSpecifically, at each resolution scale, we generate a pair of affine transformation parameters (\u03b1, \u03b2) from input fea-tures F spatial by several convolutional layers. After that, the modulation is carried out by scaling and shifting the GAN features F GAN , formulated by:\n\u03b1, \u03b2 = Conv(F spatial ), F output = SFT(F GAN |\u03b1, \u03b2) = \u03b1 F GAN + \u03b2.(3)\nTo achieve a better balance of realness and fidelity, we further propose channel-split spatial feature transform (CS-SFT) layers, which perform spatial modulation on part of the GAN features by input features F spatial (contributing to fidelity) and leave the left GAN features (contributing to realness) to directly pass through, as shown in Fig. 2:\nF output = CS-SFT(F GAN |\u03b1, \u03b2) (4) = Concat[Identity(F split0 GAN ), \u03b1 F split1 GAN + \u03b2],\nwhere F split0 GAN and F split1 GAN are split features from F GAN in channel dimension, and Concat[\u00b7, \u00b7] denotes the concatenation operation.\n\nAs a result, CS-SFT enjoys the benefits of directly incorporating prior information and effective modulating by input images, thereby achieving a good balance between texture faithfulness and fidelity. Besides, CS-SFT could also reduce complexity as it requires fewer channels for modulation, similar to GhostNet [23].\n\nWe conduct channel-split SFT layers at each resolution scale, and finally generate a restored face\u0177.\n\n\nModel Objectives\n\nThe learning objective of training our GFP-GAN consists of: 1) reconstruction loss that constraints the outputs\u0177 close to the ground-truth y, 2) adversarial loss for restoring realistic textures, 3) proposed facial component loss to further enhance facial details, and 4) identity preserving loss. Reconstruction Loss. We adopt the widely-used L1 loss and perceptual loss [33,41] as our reconstruction loss L rec , defined as follows:\nL rec = \u03bb l1 \u0177 \u2212 y 1 + \u03bb per \u03c6(\u0177) \u2212 \u03c6(y) 1 ,(5)\nwhere \u03c6 is the pretrained VGG-19 network [59] and we use the {conv1, \u00b7 \u00b7 \u00b7 , conv5} feature maps before activation [64]. \u03bb l1 and \u03bb per denote the loss weights of L1 and perceptual loss, respectively. Adversarial Loss. We employ adversarial loss L adv to encourage the GFP-GAN to favor the solutions in the natural image manifold and generate realistic textures. Similar to StyleGAN2 [36], logistic loss [18] is adopted:\nL adv = \u2212\u03bb adv E\u0177 softplus(D(\u0177))(6)\nwhere D denotes the discriminator and \u03bb adv represents the adversarial loss weight. Facial Component Loss. In order to further enhance the perceptually significant face components, we introduce facial component loss with local discriminators for left eye, right eye and mouth. As shown in Fig. 2, we first crop interested regions with ROI align [24]. For each region, we train separate and small local discriminators to distinguish whether the restore patches are real, pushing the patches close to the natural facial component distributions. Inspired by [62], we further incorporate a feature style loss based on the learned discriminators. Different from previous feature matching loss with spatial-wise constraints [62], our feature style loss attempts to match the Gram matrix statistics [15] of real and restored patches. Gram matrix calculates the feature correlations and usually effectively captures texture information [17]. We extract features from multiple layers of the learned local discriminators and learn to match these Gram statistic of intermediate representations from the real and restored patches. Empirically, we found the feature style loss performs better than previous feature matching loss in terms of generating realistic facial details and reducing unpleasant artifacts.\n\nThe facial component loss is defined as follows. The first term is the discriminative loss [18] and the second term is the feature style loss:\nL comp = ROI \u03bb local E\u0177 ROI [log(1 \u2212 D ROI (\u0177 ROI ))]+ \u03bb f s Gram(\u03c8(\u0177 ROI )) \u2212 Gram(\u03c8(y ROI )) 1 (7)\nwhere ROI is region of interest from the component collection {left eye, right eye, mouth}. D ROI is the local discriminator for each region. \u03c8 denotes the multi-resolution features from the learned discriminators. \u03bb local and \u03bb f s represent the loss weights of local discriminative loss and feature style loss, respectively. Identity Preserving Loss. We draw inspiration from [31] and apply identity preserving loss in our model. Similar to perceptual loss [33], we define the loss based on the feature embedding of an input face. Specifically, we adopt the pretrained face recognition ArcFace [10] model, which captures the most prominent features for identity discrimination. The identity preserving loss enforces the restored result to have a small distance with the ground truth in the compact deep feature space:\nL id = \u03bb id \u03b7(\u0177) \u2212 \u03b7(y) 1 ,(8)\nwhere \u03b7 represents face feature extractor, i.e. ArcFace [10] in our implementation. \u03bb id denotes the weight of identity preserving loss.\n\nThe overall model objective is a combination of the above losses:\nL total = L rec + L adv + L comp + L id .(9)\nThe loss hyper-parameters are set as follows: \u03bb l1 = 0.1, \u03bb per = 1, \u03bb adv = 0.1, \u03bb local = 1, \u03bb f s = 200 and \u03bb id = 10.\n\n\nExperiments\n\n\nDatasets and Implementation\n\nTraining Datasets. We train our GFP-GAN on the FFHQ dataset [35], which consists of 70, 000 high-quality images. We resize all the images to 512 2 during training. Our GFP-GAN is trained on synthetic data that approximate to the real low-quality images and generalize to realworld images during inference. We follow the practice in [46,44] and adopt the following degradation model to synthesize training data:\nx = [(y k \u03c3 ) \u2193 r +n \u03b4 ] JPEGq .(10)\nThe high quality image y is first convolved with Gaussian blur kernel k \u03c3 followed by a downsampling operation with a scale factor r. After that, additive white Gaussian noise n \u03b4 is added to the image and finally it is compressed by JPEG with quality factor q. Similar to [44], for each training pair, we randomly sample \u03c3, r, \u03b4 and q from {0.2 : 10}, {1 : 8}, {0 : 15}, {60 : 100}, respectively. We also add color jittering during training for color enhancement. Testing Datasets. We construct one synthetic dataset and three different real datasets with distinct sources. All these datasets have no overlap with our training dataset. We provide a brief introduction here.\n\n\u2022 CelebA-Test is the synthetic dataset with 3,000 CelebA-HQ images from its testing partition [51]. The generation way is the same as that during training.\n\n\u2022 LFW-Test. LFW [29] contains low-quality images in the wild. We group all the first image for each identity in the validation partition, forming 1711 testing images.\n\n\u2022 CelebChild-Test contains 180 child faces of celebrities collected from the Internet. They are low-quality and many of them are black-and-white old photos.\n\n\u2022 WebPhoto-Test. We crawled 188 low-quality photos in real life from the Internet and extracted 407 faces to construct the WebPhoto testing dataset. These photos have diverse and complicated degradation. Some of them are old photos with very severe degradation on both details and color. Implementation. We adopt the pretrained StyleGAN2 [36] with 512 2 outputs as our generative facial prior. The channel multiplier of StyleGAN2 is set to one for compact model size. The UNet for degradation removal consists of seven downsamples and seven upsamples, each with a residual block [25]. For each CS-SFT layer, we use two convolutional layers to generate the affine parameters \u03b1 and \u03b2 respectively.\n\nThe training mini-batch size is set to 12. We augment the training data with horizontal flip and color jittering. We consider three components: left eye, right eye, mouth for face component loss as they are perceptually significant. Each component is cropped by ROI align [24] with face landmarks provided in the origin training dataset. We train our model with Adam optimizer [38] for a total of 800k iterations. The learning rate was set to 2 \u00d7 10 \u22123 and then decayed by a factor of 2 at the 700k-th, 750k-th iterations. We implement our models with the PyTorch framework and train them using four NVIDIA Tesla P40 GPUs.\n\n\nComparisons with State-of-the-art Methods\n\nWe compare our GFP-GAN with several state-of-the-art face restoration methods: HiFaceGAN [67], DFDNet [44], PSFRGAN [6], Super-FAN [4] and Wan et al. [61]. GAN inversion methods for face restoration: PULSE [52] and mGANprior [19] are also included for comparison. We also compare our GFP-GAN with image restoration methods: RCAN [74], ESRGAN [64] and DeblurGANv2 [40], and we finetune them on our face training set for fair comparisons. We adopt their official codes except for Super-FAN, for which we use a re-implementation.\n\nFor the evaluation, we employ the widely-used nonreference perceptual metrics: FID [27] and NIQE [53]. We also adopt pixel-wise metrics (PSNR and SSIM) and the perceptual metric (LPIPS [73]) for the CelebA-Test with Ground-Truth (GT). We measure the identity distance with angels in the ArcFace [10] feature embedding, where smaller values indicate closer identity to the GT. Synthetic CelebA-Test. The comparisons are conducted under two settings: 1) blind face restoration whose inputs and outputs have the same resolution. 2) 4\u00d7 face superresolution. Note that our method could take upsampled im-  Table. 1 and Table. 2. On both settings, GFP-GAN achieves the lowest LPIPS, indicating that our results is perceptually close to the ground-truth. GFP-GAN also obtain the lowest FID and NIQE, showing that the outputs have a close distance to the real face distribution and natural image distribution, respectively. Besides the perceptual performance, our method also retains better identity, indicated by the smallest degree in the face feature embedding. Note that 1) the lower FID and NIQE of our method than GT does not indicate that our performance is better than GT, as those 'perceptual' metrics are well correlated with the human-opinion-scores on a coarse scale, but not always well correlated on a finer scale [2]; 2) the pixel-wise metrics PSNR and SSIM are not correlation well with the subjective evaluation of human observers [2,41] and our model is not good at these two metrics.\n\nQualitative results are presented in Fig. 3 and Fig. 4. 1) Thanks to the powerful generative facial prior, our GFP-GAN recovers faithful details in the eyes (pupils and eyelashes), teeth, etc. 2) Our method treats faces as whole in restoration and could also generate realistic hair, while previous methods that rely on component dictionaries (DFD-Net) or parsing maps (PSFRGAN) fail to produce faithful hair textures (2nd row, Fig. 3). 3) GFP-GAN is capable of retaining fidelity, e.g., it produces natural closed mouth \n\n\nGFP-GAN\n\n\nLFW-Test\n\n\nCelebChild-Test\n\nWebPhoto-Test   Fig. 3). And in Fig. 4, GFP-GAN also restores reasonable eye gaze direction.\n\nReal-World LFW, CelebChild and WedPhoto-Test. To test the generalization ability, we evaluate our model on three different real-world datasets. The quantitative results are show in Table. 3. Our GFP-GAN achieves superior performance on all the three real-world datasets, showing its remarkable generalization capability. Although PULSE [52] could also obtain high perceptual quality (lower FID scores), it could not retain the face identity as  35 4.144 shown in Fig 5. The qualitative comparisons are shown in Fig. 5. GFP-GAN could jointly conduct face restoration and color enhancement for real-life photos with the powerful generative prior. Our method could produce plausible and realistic faces on complicated real-world degradation while other methods fail to recover faithful facial details or produces ar- tifacts (especially in WebPhoto-Test in Fig 5). Besides the common facial components like eyes and teeth, GFP-GAN also perform better in hair and ears, as the GFP prior takes the whole face into consideration rather than separate parts. With SC-SFT layers, our model is capable of achieving high fidelity. As shown in the last row of Fig. 5, most previous methods fail to recover the closed eyes, while ours could successfully restore them with fewer artifacts.\n\n\nAblation Studies\n\nCS-SFT layers. As shown in Table. 4 [configuration a)] and Fig. 6, when we remove spatial modulation layers, i.e., only keep the latent code mapping without spatial information, the restored faces could not retain face identity even with identity-preserving loss (high LIPS score and large Deg.). Thus, the multi-resolution spatial features used in CS-SFT layers is vital to preserve fidelity. When we switch CS-SFT layers to simple SFT layers [configuration b) in Table. 4], we observe that 1) the perceptual quality degrades on all metrics and 2) it preserves stronger identity (smaller Deg.), as the input image features impose influence on all the modulated features and the outputs bias to the degraded inputs, thus leading to lower perceptual quality. By contrast, CS-SFT layers provide a good balance of realness and fidelity by modulating a split of features. Pretrained GAN as GFP. Pretrained GAN provides rich and diverse features for restoration. A performance drop is observed if we do not use the generative facial prior, as shown in Table. 4 [configuration c)] and Fig. 6. Pyramid Restoration Loss. Pyramid restoration loss is employed in the degradation removal module and strengthens the restoration ability for complicated degradation in the real world. Without this intermediate supervision, the multi-resolution spatial features for subsequent modulations may still have degradation, resulting in inferior performance, as shown in Table. 4 [configuration d)] and Fig. 6. Facial Component Loss. We compare the results of 1) removing all the facial component loss, 2) only keeping the component discriminators, 3) adding extra feature matching loss as in [62], and 4) adopting extra feature style loss based on Gram statistics [15]. It is shown in Fig 7 that component discriminators with feature style loss could better capture the eye distribution and restore the plausible details. \n\n\nDiscussion and Limitations\n\nTraining bias. Our method performs well on most darkskinned faces and various population groups (Fig. 8), as our method uses both the pretrained GAN and input image features for modulation. Beside, we employ reconstruction loss and identity preserving loss to restrict the outputs to retain fidelity with inputs. However, when input images are gray-scale, the face color may have a bias (last example in Fig. 8), as the inputs do not contain sufficient color information. Thus, a diverse and balanced dataset is in need.\n\nLimitations. As shown in Fig. 9, when the degradation of real images is severe, the restored facial details by GFP-GAN are twisted with artifacts. Our method also produces unnatural results for very large poses. This is because the synthetic degradation and training data distribution are different from those in real-world. One possible way is to learn those distributions from real data instead of merely using synthetic data, which is left as future work.\n\n\nConclusion\n\nWe have proposed the GFP-GAN framework that leverages the rich and diverse generative facial prior for the challenging blind face restoration task. This prior is incorporated into the restoration process with channel-split spatial feature transform layers, allowing us to achieve a good balance of realness and fidelity. Extensive comparisons demonstrate the superior capability of GFP-GAN in joint face restoration and color enhancement for real-world images, outperforming prior art.\n\nFigure 2 :\n2Overview of GFP-GAN framework. It consists of a degradation removal module (U-Net) and a pretrained face GAN as facial prior. They are bridged by a latent code mapping and several Channel-Split Spatial Feature Transform (CS-SFT) layers. During training, we employ 1) intermediate restoration losses to remove complex degradation, 2) Facial component loss with discriminators to enhance facial details, and 3) identity preserving loss to retain face identity.\n\nFigure 3 :Figure 4 :\n34Qualitative comparison on the CelebA-Test for blind face restoration. Our GFP-GAN produces faithful details in eyes, mouth and hair. Zoom in for best view. Comparison on the CelebA-Test for \u00d74 face super-resolution. Our GFP-GAN restores realistic teeth and faithful eye gaze direction. Zoom in for best view.\n\nFigure 5 :\n5Qualitative comparisons on three real-world datasets. Zoom in for best view.\n\nFigure 6 :Figure 7 :Figure 8 :Figure 9 :\n6789Ablation studies on CS-SFT layers, GFP prior and pyramid restoration loss. Zoom in for best view.Input + D + D + fm + D +fs No D Ablation studies on facial component loss. In the figure, D, fm, fs denotes component discriminator, feature matching loss and feature style loss, respectively. Results on dark-skinned faces. Limitations of our model. The results of PSFR-GAN [6] are also presented.\n\nTable 1 :\n1Quantitative comparison on CelebA-Test for blind face restoration. Red and blue indicates the best and the second best performance. '*' denotes finetuning on our training set. Deg. represents the identity distance.Methods \nLPIPS\u2193 FID\u2193 NIQE \u2193 Deg.\u2193 PSNR\u2191 SSIM\u2191 \nInput \n0.4866 143.98 13.440 47.94 25.35 0.6848 \nDeblurGANv2* [40] 0.4001 52.69 4.917 39.64 25.91 0.6952 \nWan et al. [61] \n0.4826 67.58 5.356 43.00 24.71 0.6320 \nHiFaceGAN [67] \n0.4770 66.09 4.916 42.18 24.92 0.6195 \nDFDNet [44] \n0.4341 59.08 4.341 40.31 23.68 0.6622 \nPSFRGAN [6] \n0.4240 47.59 5.123 39.69 24.71 0.6557 \nmGANprior [19] \n0.4584 82.27 6.422 55.45 24.30 0.6758 \nPULSE [52] \n0.4851 67.56 5.305 69.55 21.61 0.6200 \nGFP-GAN (ours) 0.3646 42.62 4.077 34.60 25.08 0.6777 \nGT \n0 \n43.43 4.292 \n0 \n\u221e \n1 \n\nages as inputs for face super-resolution. \nThe quantitative results for each setting are shown in \n\nTable 2 :\n2Quantitative comparison on CelebA-Test for 4\u00d7 face super-resolution. Red and blue indicates the best and the second best performance. '*' denotes finetuning on our training set. Deg. represents the identity distance.Methods \nLPIPS\u2193 FID\u2193 NIQE \u2193 Deg.\u2193 PSNR\u2191 SSIM\u2191 \nBicubic \n0.4834 148.87 10.767 49.60 25.377 0.6985 \nRCAN* [74] \n0.4159 93.66 9.907 38.45 27.24 0.7533 \nESRGAN* [64] 0.4127 49.20 4.099 51.21 23.74 0.6319 \nSuper-FAN [4] \n0.4791 139.49 10.828 49.14 25.28 0.7033 \nGFP-GAN (ours) 0.3653 42.36 4.078 34.67 25.04 0.6744 \nGT \n0 \n43.43 4.292 \n0 \n\u221e \n1 \n\nwithout forced addition of teeth as PSFRGAN does (2nd \nrow, \n\nTable 3 :\n3Quantitative comparison on the real-world LFW, CelebChild, WebPhoto. Red and blue indicates the best and the second best performance. '*' denotes finetuning on our training set. Deg. represents the identity distance. GFP-GAN (ours) 49.96 3.882 111.78 4.349 87.Dataset \nLFW-Test \nCelebChild \nWebPhoto \nMethods \nFID\u2193 NIQE \u2193 FID\u2193 NIQE \u2193 FID\u2193 NIQE \u2193 \nInput \n137.56 11.214 144.42 9.170 170.11 12.755 \nDeblurGANv2* [40] 57.28 4.309 110.51 4.453 100.58 4.666 \nWan et al. [61] \n73.19 5.034 115.70 4.849 100.40 5.705 \nHiFaceGAN [67] \n64.50 4.510 113.00 4.855 116.12 4.885 \nDFDNet [44] \n62.57 4.026 111.55 4.414 100.68 5.293 \nPSFRGAN [6] \n51.89 5.096 107.40 4.804 88.45 5.582 \nmGANprior [19] \n73.00 6.051 126.54 6.841 120.75 7.226 \nPULSE [52] \n64.86 5.097 102.74 5.225 86.45 5.146 \n\n\nTable 4 :\n4Ablation study results on CelebA-Test under blind face restoration. Pyramid Restoration Loss 0.369 (\u2191) 45.17 (\u2191) 4.284 (\u2191) 35.50 (\u2191)Configuration \nLPIPS\u2193 \nFID\u2193 \nNIQE \u2193 \nDeg.\u2193 \nOur GFP-GAN with SC-SFT \n0.3646 \n42.62 \n4.077 \n34.60 \na) No spatial modulation \n0.550 (\u2191) 60.44 (\u2191) 4.183 (\u2191) 74.76 (\u2191) \nb) Use SFT \n0.387 (\u2191) 47.65 (\u2191) 4.146(\u2191) 34.38 (\u2193) \nc) w/o GFP \n0.379 (\u2191) 48.47 (\u2191) 4.153 (\u2191) 35.04 (\u2191) \nd) \u2212 \n\nIm-age2stylegan: How to embed images into the stylegan latent space. Rameen Abdal, Yipeng Qin, Peter Wonka, ICCV, 2019. 24Rameen Abdal, Yipeng Qin, and Peter Wonka. Im- age2stylegan: How to embed images into the stylegan latent space? In ICCV, 2019. 2, 4\n\nTomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on perceptual image super-resolution. Yochai Blau, Roey Mechrez, Radu Timofte, In ECCVW. 6Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on percep- tual image super-resolution. In ECCVW, 2018. 6\n\nLarge scale gan training for high fidelity natural image synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, arXiv:1809.11096arXiv preprintAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 2\n\nSuper-fan: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans. Adrian Bulat, Georgios Tzimiropoulos, CVPR. 67Adrian Bulat and Georgios Tzimiropoulos. Super-fan: In- tegrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans. In CVPR, 2018. 6, 7\n\nAttention-aware face hallucination via deep reinforcement learning. Qingxing Cao, Liang Lin, Yukai Shi, Xiaodan Liang, Guanbin Li, CVPR. Qingxing Cao, Liang Lin, Yukai Shi, Xiaodan Liang, and Guanbin Li. Attention-aware face hallucination via deep re- inforcement learning. In CVPR, 2017. 2\n\nProgressive semantic-aware style transformation for blind face restoration. Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, K Kwan-Yee, Wong, arXiv:2009.087097Chaofeng Chen, Xiaoming Li, Lingbo Yang, Xianhui Lin, Lei Zhang, and Kwan-Yee K. Wong. Progressive semantic-aware style transformation for blind face restora- tion. arXiv:2009.08709, 2020. 1, 2, 6, 7, 8\n\nImage blind denoising with generative adversarial network based noise modeling. Jingwen Chen, Jiawei Chen, Hongyang Chao, Ming Yang, CVPR. Jingwen Chen, Jiawei Chen, Hongyang Chao, and Ming Yang. Image blind denoising with generative adversarial net- work based noise modeling. In CVPR, 2018. 2\n\nDual path networks. Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, Jiashi Feng, NeurIPS. Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng. Dual path networks. In NeurIPS, 2017. 2\n\nFsrnet: End-to-end learning face super-resolution with facial priors. Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, Jian Yang, CVPR. Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. Fsrnet: End-to-end learning face super-resolution with facial priors. In CVPR, 2018. 1, 2, 4\n\nArcface: Additive angular margin loss for deep face recognition. Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou, CVPR. 56Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. 5, 6\n\nExemplar guided face image super-resolution without facial landmarks. Berk Dogan, Shuhang Gu, Radu Timofte, CVPRW. 1Berk Dogan, Shuhang Gu, and Radu Timofte. Exemplar guided face image super-resolution without facial landmarks. In CVPRW, 2019. 1, 2\n\nCompression artifacts reduction by a deep convolutional network. Chao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang, ICCV. 1Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. Compression artifacts reduction by a deep convolu- tional network. In ICCV, 2015. 1, 2\n\nLearning a deep convolutional network for image super-resolution. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, ECCV. 1Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV, 2014. 1, 2\n\nDeep generative adversarial compression artifact removal. Leonardo Galteri, Lorenzo Seidenari, Marco Bertini, Alberto Del Bimbo, ICCV. Leonardo Galteri, Lorenzo Seidenari, Marco Bertini, and Al- berto Del Bimbo. Deep generative adversarial compression artifact removal. In ICCV, 2017. 2\n\nImage style transfer using convolutional neural networks. A Leon, Alexander S Gatys, Matthias Ecker, Bethge, CVPR. 5Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im- age style transfer using convolutional neural networks. In CVPR, 2016. 5, 8\n\nGanfit: Generative adversarial network fitting for high fidelity 3d face reconstruction. Baris Gecer, Stylianos Ploumpis, Irene Kotsia, Stefanos Zafeiriou, CVPR. Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou. Ganfit: Generative adversarial network fitting for high fidelity 3d face reconstruction. In CVPR, 2019. 4\n\nThe unreasonable effectiveness of texture transfer for single image super-resolution. Bernhard Muhammad Waleed Gondal, Michael Sch\u00f6lkopf, Hirsch, ECCV. Muhammad Waleed Gondal, Bernhard Sch\u00f6lkopf, and Michael Hirsch. The unreasonable effectiveness of texture transfer for single image super-resolution. In ECCV, 2018. 5\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NeurIPS. 5Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 2, 4, 5\n\nImage processing using multi-code gan prior. Jinjin Gu, Yujun Shen, Bolei Zhou, CVPR. 67Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. In CVPR, 2020. 2, 4, 6, 7\n\nLadn: Local adversarial disentangling network for facial makeup and de-makeup. Qiao Gu, Guanzhi Wang, Mang Tik Chiu, Yu-Wing Tai, Chi-Keung Tang, ICCV. Qiao Gu, Guanzhi Wang, Mang Tik Chiu, Yu-Wing Tai, and Chi-Keung Tang. Ladn: Local adversarial disentangling net- work for facial makeup and de-makeup. In ICCV, 2019. 3\n\nBuilding dual-domain representations for compression artifacts reduction. Jun Guo, Hongyang Chao, ECCV. Jun Guo and Hongyang Chao. Building dual-domain rep- resentations for compression artifacts reduction. In ECCV, 2016. 2\n\nClosedloop matters: Dual regression networks for single image super-resolution. Yong Guo, Jian Chen, Jingdong Wang, Qi Chen, Jiezhang Cao, Zeshuai Deng, Yanwu Xu, Mingkui Tan, CVPR. Yong Guo, Jian Chen, Jingdong Wang, Qi Chen, Jiezhang Cao, Zeshuai Deng, Yanwu Xu, and Mingkui Tan. Closed- loop matters: Dual regression networks for single image super-resolution. In CVPR, 2020. 2\n\nGhostnet: More features from cheap operations. Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, Chang Xu, CVPR, 2020. 24Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features from cheap opera- tions. In CVPR, 2020. 2, 4\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, ICCV. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In ICCV, 2017. 5\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 5\n\nStochastic frequency masking to improve super-resolution and denoising networks. Majed El Helou, Ruofan Zhou, Sabine S\u00fcsstrunk, ECCV. 2020Majed El Helou, Ruofan Zhou, and Sabine S\u00fcsstrunk. Stochastic frequency masking to improve super-resolution and denoising networks. In ECCV, 2020. 2\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, NeurIPS. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib- rium. In NeurIPS, 2017. 6\n\nMobilenets: Efficient convolutional neural networks for mobile vision applications. G Andrew, Menglong Howard, Bo Zhu, Dmitry Chen, Weijun Kalenichenko, Tobias Wang, Marco Weyand, Hartwig Andreetto, Adam, arXiv:1704.04861Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An- dreetto, and Hartwig Adam. Mobilenets: Efficient con- volutional neural networks for mobile vision applications. arXiv:1704.04861, 2017. 2\n\nLabeled faces in the wild: A database for studying face recognition in unconstrained environments. B Gary, Manu Huang, Tamara Ramesh, Erik Berg, Learned-Miller, AmherstUniversity of MassachusettsTechnical reportGary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical report, University of Massachusetts, Amherst, 2007. 5\n\nWavelet-srnet: A wavelet-based cnn for multi-scale face super resolution. Huaibo Huang, Ran He, Zhenan Sun, Tieniu Tan, ICCV. Huaibo Huang, Ran He, Zhenan Sun, and Tieniu Tan. Wavelet-srnet: A wavelet-based cnn for multi-scale face su- per resolution. In ICCV, 2017. 2\n\nBeyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis. Rui Huang, Shu Zhang, Tianyu Li, Ran He, CVPR. Rui Huang, Shu Zhang, Tianyu Li, and Ran He. Beyond face rotation: Global and local perception gan for photoreal- istic and identity preserving frontal view synthesis. In CVPR, 2017. 5\n\nGlobally and locally consistent image completion. Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa, ACM Transactions on Graphics (ToG). 364Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image completion. ACM Transactions on Graphics (ToG), 36(4):1-14, 2017. 3\n\nPerceptual losses for real-time style transfer and super-resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, ECCV. 45Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 4, 5\n\nProgressive growing of gans for improved quality, stability, and variation. Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, In ICLR. 2Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018. 2\n\nA style-based generator architecture for generative adversarial networks. Tero Karras, Samuli Laine, Timo Aila, CVPR. 25Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2018. 2, 5\n\nAnalyzing and improving the image quality of stylegan. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, CVPR. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020. 2, 3, 4, 5\n\nProgressive face super-resolution via attention to facial landmark. Deokyun Kim, Minseon Kim, Gihyun Kwon, Dae-Shik Kim, BMVC. Deokyun Kim, Minseon Kim, Gihyun Kwon, and Dae-Shik Kim. Progressive face super-resolution via attention to facial landmark. In BMVC, 2019. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6\n\nDeblurgan: Blind motion deblurring using conditional adversarial networks. Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, Ji\u0159\u00ed Matas, CVPR. 1Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Ji\u0159\u00ed Matas. Deblurgan: Blind motion deblurring using conditional adversarial networks. In CVPR, 2018. 1, 2\n\nDeblurgan-v2: Deblurring (orders-of-magnitude) faster and better. Orest Kupyn, Tetiana Martyniuk, Junru Wu, Zhangyang Wang, ICCV. 67Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang. Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better. In ICCV, 2019. 6, 7\n\nPhotorealistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, CVPR. 6Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo- realistic single image super-resolution using a generative ad- versarial network. In CVPR, 2017. 2, 4, 6\n\nNon-local color image denoising with convolutional neural networks. Stamatios Lefkimmiatis, CVPR. Stamatios Lefkimmiatis. Non-local color image denoising with convolutional neural networks. In CVPR, 2017. 2\n\nBeautygan: Instance-level facial makeup transfer with deep generative adversarial network. Tingting Li, Ruihe Qian, Chao Dong, Si Liu, Qiong Yan, Wenwu Zhu, Liang Lin, In ACM MM. 3Tingting Li, Ruihe Qian, Chao Dong, Si Liu, Qiong Yan, Wenwu Zhu, and Liang Lin. Beautygan: Instance-level facial makeup transfer with deep generative adversarial network. In ACM MM, 2018. 3\n\nBlind face restoration via deep multi-scale component dictionaries. Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, Lei Zhang, ECCV. 67Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. Blind face restora- tion via deep multi-scale component dictionaries. In ECCV, 2020. 1, 2, 4, 5, 6, 7\n\nEnhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, Wangmeng Zuo, CVPR. 1Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. In CVPR, 2020. 1, 2\n\nLearning warped guidance for blind face restoration. Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, Ruigang Yang, ECCV. Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, and Ruigang Yang. Learning warped guidance for blind face restoration. In ECCV, 2018. 1, 2, 5\n\nGenerative face completion. Yijun Li, Sifei Liu, Jimei Yang, Ming-Hsuan Yang, CVPR. Yijun Li, Sifei Liu, Jimei Yang, and Ming-Hsuan Yang. Gen- erative face completion. In CVPR, 2017. 3\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, CVPRW. 1Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, 2017. 1, 2\n\nNon-local recurrent network for image restoration. Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S Huang, In NeurIPS. 2Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and Thomas S Huang. Non-local recurrent network for image restoration. In NeurIPS, 2018. 2\n\nResidual feature aggregation network for image superresolution. Jie Liu, Wenjie Zhang, Yuting Tang, Jie Tang, Gangshan Wu, CVPR. 2020Jie Liu, Wenjie Zhang, Yuting Tang, Jie Tang, and Gangshan Wu. Residual feature aggregation network for image super- resolution. In CVPR, 2020. 2\n\nDeep learning face attributes in the wild. Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang, ICCV. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015. 5\n\nPulse: Self-supervised photo upsampling via latent space exploration of generative models. Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin, CVPR. 7Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsam- pling via latent space exploration of generative models. In CVPR, 2020. 1, 2, 6, 7\n\nMaking a \"completely blind\" image quality analyzer. IEEE Signal processing letters. Anish Mittal, Rajiv Soundararajan, Alan C Bovik, 20Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak- ing a \"completely blind\" image quality analyzer. IEEE Sig- nal processing letters, 20(3):209-212, 2012. 6\n\nExploiting deep generative prior for versatile image restoration and manipulation. Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping Luo, ECCV, 2020. 24Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep genera- tive prior for versatile image restoration and manipulation. In ECCV, 2020. 2, 4\n\nSemantic image synthesis with spatially-adaptive normalization. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu, CVPR. 24Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive nor- malization. In CVPR, 2019. 2, 4\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015. 3\n\nEnhancenet: Single image super-resolution through automated texture synthesis. S M Mehdi, Bernhard Sajjadi, Michael Scholkopf, Hirsch, ECCV. Mehdi SM Sajjadi, Bernhard Scholkopf, and Michael Hirsch. Enhancenet: Single image super-resolution through automated texture synthesis. In ECCV, 2017. 2\n\nDeep semantic face deblurring. Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, Ming-Hsuan Yang, CVPR. 1Ziyi Shen, Wei-Sheng Lai, Tingfa Xu, Jan Kautz, and Ming- Hsuan Yang. Deep semantic face deblurring. In CVPR, 2018. 1, 2\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, ICLR. Karen Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. In ICLR, 2015. 4\n\nNtire 2017 challenge on single image super-resolution: Methods and results. Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, CVPRW. Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming- Hsuan Yang, and Lei Zhang. Ntire 2017 challenge on single image super-resolution: Methods and results. In CVPRW, 2017. 2\n\nBringing old photos back to life. Ziyu Wan, Bo Zhang, Dongdong Chen, Pan Zhang, Dong Chen, Jing Liao, Fang Wen, CVPR, 2020. 1. 67Ziyu Wan, Bo Zhang, Dongdong Chen, Pan Zhang, Dong Chen, Jing Liao, and Fang Wen. Bringing old photos back to life. In CVPR, 2020. 1, 6, 7\n\nHigh-resolution image synthesis and semantic manipulation with conditional gans. Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro, CVPR. 3Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image syn- thesis and semantic manipulation with conditional gans. In CVPR, 2018. 3, 5, 8\n\nRecovering realistic texture in image super-resolution by deep spatial feature transform. Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy, CVPR. 24Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In CVPR, 2018. 2, 4\n\nEsrgan: Enhanced super-resolution generative adversarial networks. Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, Chen Change Loy, ECCVW. 67Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En- hanced super-resolution generative adversarial networks. In ECCVW, 2018. 2, 4, 6, 7\n\nDeep convolutional neural network for image deconvolution. Li Xu, Jimmy S Ren, Ce Liu, Jiaya Jia, NeurIPS. Li Xu, Jimmy S Ren, Ce Liu, and Jiaya Jia. Deep convolu- tional neural network for image deconvolution. In NeurIPS, 2014. 2\n\nLearning to superresolve blurry face and text images. Xiangyu Xu, Deqing Sun, Jinshan Pan, Yujin Zhang, Hanspeter Pfister, Ming-Hsuan Yang, ICCV. Xiangyu Xu, Deqing Sun, Jinshan Pan, Yujin Zhang, Hanspeter Pfister, and Ming-Hsuan Yang. Learning to super- resolve blurry face and text images. In ICCV, 2017. 2\n\nHifacegan: Face renovation via collaborative suppression and replenishment. Lingbo Yang, Chang Liu, Pan Wang, Shanshe Wang, Peiran Ren, Siwei Ma, Wen Gao, ACM Multimedia. 617Lingbo Yang, Chang Liu, Pan Wang, Shanshe Wang, Peiran Ren, Siwei Ma, and Wen Gao. Hifacegan: Face renovation via collaborative suppression and replenishment. ACM Mul- timedia, 2020. 1, 6, 7\n\nPath-restore: Learning network path selection for image restoration. Ke Yu, Xintao Wang, Chao Dong, Xiaoou Tang, Chen Change Loy, arXiv:1904.10343Ke Yu, Xintao Wang, Chao Dong, Xiaoou Tang, and Chen Change Loy. Path-restore: Learning network path se- lection for image restoration. arXiv:1904.10343, 2019. 2\n\nFace super-resolution guided by facial component heatmaps. Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, Richard Hartley, ECCV. Xin Yu, Basura Fernando, Bernard Ghanem, Fatih Porikli, and Richard Hartley. Face super-resolution guided by facial component heatmaps. In ECCV, pages 217-233, 2018. 1, 2, 4\n\nSuper-resolving very low-resolution face images with supplementary attributes. Xin Yu, Basura Fernando, Richard Hartley, Fatih Porikli, In CVPR. 2Xin Yu, Basura Fernando, Richard Hartley, and Fatih Porikli. Super-resolving very low-resolution face images with sup- plementary attributes. In CVPR, 2018. 2\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, IEEE TIP. 267Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE TIP, 26(7):3142-3155, 2017. 1, 2\n\nColorful image colorization. Richard Zhang, Phillip Isola, Alexei A Efros, ECCV. Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016. 4\n\nThe unreasonable effectiveness of deep features as a perceptual metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, In CVPR. 6Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. 6\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, ECCV. 67Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In ECCV, 2018. 2, 6, 7\n\nChannel splitting network for single mr image super-resolution. Xiaole Zhao, Yulun Zhang, Tao Zhang, Xueming Zou, IEEE Transactions on Image Processing. 2811Xiaole Zhao, Yulun Zhang, Tao Zhang, and Xueming Zou. Channel splitting network for single mr image super-resolution. IEEE Transactions on Image Processing, 28(11):5649-5662, 2019. 2\n\nIndomain gan inversion for real image editing. Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou, ECCV, 2020. 24Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In- domain gan inversion for real image editing. In ECCV, 2020. 2, 4\n\nDeep cascaded bi-network for face hallucination. Shizhan Zhu, Sifei Liu, Chen Change Loy, Xiaoou Tang, ECCV. Shizhan Zhu, Sifei Liu, Chen Change Loy, and Xiaoou Tang. Deep cascaded bi-network for face hallucination. In ECCV, 2016. 2\n", "annotations": {"author": "[{\"end\":152,\"start\":74},{\"end\":202,\"start\":153},{\"end\":284,\"start\":203},{\"end\":360,\"start\":285}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":81},{\"end\":158,\"start\":156},{\"end\":216,\"start\":211},{\"end\":294,\"start\":290}]", "author_first_name": "[{\"end\":80,\"start\":74},{\"end\":155,\"start\":153},{\"end\":210,\"start\":203},{\"end\":289,\"start\":285}]", "author_affiliation": "[{\"end\":151,\"start\":110},{\"end\":201,\"start\":160},{\"end\":283,\"start\":242},{\"end\":359,\"start\":318}]", "title": "[{\"end\":71,\"start\":1},{\"end\":431,\"start\":361}]", "venue": null, "abstract": "[{\"end\":2085,\"start\":552}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2259,\"start\":2255},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2262,\"start\":2259},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2264,\"start\":2262},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":2276,\"start\":2272},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2287,\"start\":2283},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2290,\"start\":2287},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2318,\"start\":2314},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2478,\"start\":2475},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":2481,\"start\":2478},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2483,\"start\":2481},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2572,\"start\":2569},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2590,\"start\":2587},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2592,\"start\":2590},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":2624,\"start\":2620},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3105,\"start\":3101},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3108,\"start\":3105},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3111,\"start\":3108},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3149,\"start\":3145},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3621,\"start\":3617},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3650,\"start\":3646},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3653,\"start\":3650},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4066,\"start\":4062},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":4069,\"start\":4066},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4072,\"start\":4069},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5775,\"start\":5772},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5978,\"start\":5974},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5981,\"start\":5978},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5984,\"start\":5981},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5987,\"start\":5984},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":5990,\"start\":5987},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":5993,\"start\":5990},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5996,\"start\":5993},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5999,\"start\":5996},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":6015,\"start\":6011},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6018,\"start\":6015},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6021,\"start\":6018},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":6038,\"start\":6034},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6041,\"start\":6038},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6044,\"start\":6041},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6073,\"start\":6069},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6076,\"start\":6073},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6152,\"start\":6148},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6251,\"start\":6247},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":6254,\"start\":6251},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":6257,\"start\":6254},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6259,\"start\":6257},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6262,\"start\":6259},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6417,\"start\":6414},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6420,\"start\":6417},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":6423,\"start\":6420},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":6426,\"start\":6423},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6601,\"start\":6598},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6604,\"start\":6601},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":6607,\"start\":6604},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6631,\"start\":6627},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6633,\"start\":6631},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6635,\"start\":6633},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":6670,\"start\":6666},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7070,\"start\":7066},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7073,\"start\":7070},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7076,\"start\":7073},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7168,\"start\":7164},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7727,\"start\":7723},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7730,\"start\":7727},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7733,\"start\":7730},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7735,\"start\":7733},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7780,\"start\":7777},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":7783,\"start\":7780},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7786,\"start\":7783},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7789,\"start\":7786},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7877,\"start\":7873},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7932,\"start\":7928},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8015,\"start\":8011},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8590,\"start\":8586},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8640,\"start\":8636},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8779,\"start\":8776},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":8947,\"start\":8943},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9077,\"start\":9073},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9080,\"start\":9077},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9289,\"start\":9285},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9292,\"start\":9289},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":9295,\"start\":9292},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9400,\"start\":9396},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9403,\"start\":9400},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10036,\"start\":10032},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11664,\"start\":11660},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12611,\"start\":12607},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12614,\"start\":12611},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12879,\"start\":12876},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":12882,\"start\":12879},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12885,\"start\":12882},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12888,\"start\":12885},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":13513,\"start\":13509},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":14204,\"start\":14200},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14296,\"start\":14293},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":14299,\"start\":14296},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14315,\"start\":14311},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":14839,\"start\":14835},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":15018,\"start\":15014},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15021,\"start\":15018},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":15047,\"start\":15043},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16294,\"start\":16290},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16794,\"start\":16790},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16797,\"start\":16794},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16946,\"start\":16942},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":17020,\"start\":17016},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17289,\"start\":17285},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17309,\"start\":17305},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17707,\"start\":17703},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":17917,\"start\":17913},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":18080,\"start\":18076},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18154,\"start\":18150},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18290,\"start\":18286},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18753,\"start\":18749},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19284,\"start\":19280},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19365,\"start\":19361},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19502,\"start\":19498},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19813,\"start\":19809},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20233,\"start\":20229},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20505,\"start\":20501},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20508,\"start\":20505},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20894,\"start\":20890},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":21391,\"start\":21387},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21470,\"start\":21466},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22118,\"start\":22114},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22359,\"start\":22355},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22749,\"start\":22745},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22854,\"start\":22850},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":23234,\"start\":23230},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23247,\"start\":23243},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23260,\"start\":23257},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23275,\"start\":23272},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":23295,\"start\":23291},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23351,\"start\":23347},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23370,\"start\":23366},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":23474,\"start\":23470},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":23487,\"start\":23483},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23508,\"start\":23504},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23756,\"start\":23752},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23770,\"start\":23766},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":23858,\"start\":23854},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23968,\"start\":23964},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24992,\"start\":24989},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25112,\"start\":25109},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25115,\"start\":25112},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26161,\"start\":26157},{\"end\":26274,\"start\":26266},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":28793,\"start\":28789},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28865,\"start\":28861}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31002,\"start\":30531},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31335,\"start\":31003},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31425,\"start\":31336},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31866,\"start\":31426},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32748,\"start\":31867},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33378,\"start\":32749},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34163,\"start\":33379},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34583,\"start\":34164}]", "paragraph": "[{\"end\":3005,\"start\":2101},{\"end\":3423,\"start\":3007},{\"end\":4399,\"start\":3425},{\"end\":5254,\"start\":4401},{\"end\":5903,\"start\":5256},{\"end\":6077,\"start\":5920},{\"end\":7047,\"start\":6079},{\"end\":9549,\"start\":7049},{\"end\":9867,\"start\":9587},{\"end\":10472,\"start\":9869},{\"end\":10992,\"start\":10474},{\"end\":11262,\"start\":10994},{\"end\":11850,\"start\":11293},{\"end\":12433,\"start\":11887},{\"end\":13224,\"start\":12485},{\"end\":13651,\"start\":13226},{\"end\":14355,\"start\":13696},{\"end\":15048,\"start\":14399},{\"end\":15321,\"start\":15050},{\"end\":15743,\"start\":15393},{\"end\":15975,\"start\":15834},{\"end\":16295,\"start\":15977},{\"end\":16397,\"start\":16297},{\"end\":16852,\"start\":16418},{\"end\":17321,\"start\":16901},{\"end\":18656,\"start\":17358},{\"end\":18800,\"start\":18658},{\"end\":19721,\"start\":18902},{\"end\":19889,\"start\":19753},{\"end\":19956,\"start\":19891},{\"end\":20123,\"start\":20002},{\"end\":20579,\"start\":20169},{\"end\":21291,\"start\":20617},{\"end\":21448,\"start\":21293},{\"end\":21616,\"start\":21450},{\"end\":21774,\"start\":21618},{\"end\":22471,\"start\":21776},{\"end\":23095,\"start\":22473},{\"end\":23667,\"start\":23141},{\"end\":25163,\"start\":23669},{\"end\":25686,\"start\":25165},{\"end\":25819,\"start\":25727},{\"end\":27096,\"start\":25821},{\"end\":29019,\"start\":27117},{\"end\":29570,\"start\":29050},{\"end\":30030,\"start\":29572},{\"end\":30530,\"start\":30045}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11886,\"start\":11851},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13695,\"start\":13652},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15392,\"start\":15322},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15833,\"start\":15744},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16900,\"start\":16853},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17357,\"start\":17322},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18901,\"start\":18801},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19752,\"start\":19722},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20001,\"start\":19957},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20616,\"start\":20580}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24289,\"start\":24270},{\"end\":26008,\"start\":26002},{\"end\":27150,\"start\":27144},{\"end\":27588,\"start\":27582},{\"end\":28170,\"start\":28164},{\"end\":28573,\"start\":28567}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2099,\"start\":2087},{\"attributes\":{\"n\":\"2.\"},\"end\":5918,\"start\":5906},{\"attributes\":{\"n\":\"3.\"},\"end\":9563,\"start\":9552},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9585,\"start\":9566},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11291,\"start\":11265},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12483,\"start\":12436},{\"attributes\":{\"n\":\"3.4.\"},\"end\":14397,\"start\":14358},{\"attributes\":{\"n\":\"3.5.\"},\"end\":16416,\"start\":16400},{\"attributes\":{\"n\":\"4.\"},\"end\":20137,\"start\":20126},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20167,\"start\":20140},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23139,\"start\":23098},{\"end\":25696,\"start\":25689},{\"end\":25707,\"start\":25699},{\"end\":25725,\"start\":25710},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27115,\"start\":27099},{\"attributes\":{\"n\":\"4.4.\"},\"end\":29048,\"start\":29022},{\"attributes\":{\"n\":\"5.\"},\"end\":30043,\"start\":30033},{\"end\":30542,\"start\":30532},{\"end\":31024,\"start\":31004},{\"end\":31347,\"start\":31337},{\"end\":31467,\"start\":31427},{\"end\":31877,\"start\":31868},{\"end\":32759,\"start\":32750},{\"end\":33389,\"start\":33380},{\"end\":34174,\"start\":34165}]", "table": "[{\"end\":32748,\"start\":32093},{\"end\":33378,\"start\":32977},{\"end\":34163,\"start\":33651},{\"end\":34583,\"start\":34308}]", "figure_caption": "[{\"end\":31002,\"start\":30544},{\"end\":31335,\"start\":31027},{\"end\":31425,\"start\":31349},{\"end\":31866,\"start\":31472},{\"end\":32093,\"start\":31879},{\"end\":32977,\"start\":32761},{\"end\":33651,\"start\":33391},{\"end\":34308,\"start\":34176}]", "figure_ref": "[{\"end\":3919,\"start\":3911},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9923,\"start\":9917},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15742,\"start\":15736},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17653,\"start\":17647},{\"end\":25208,\"start\":25202},{\"end\":25219,\"start\":25213},{\"end\":25600,\"start\":25593},{\"end\":25749,\"start\":25743},{\"end\":25765,\"start\":25759},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26290,\"start\":26284},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26338,\"start\":26332},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26681,\"start\":26675},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26975,\"start\":26969},{\"end\":27182,\"start\":27176},{\"end\":28202,\"start\":28196},{\"end\":28605,\"start\":28599},{\"end\":28892,\"start\":28882},{\"end\":29154,\"start\":29146},{\"end\":29460,\"start\":29454},{\"end\":29603,\"start\":29597}]", "bib_author_first_name": "[{\"end\":34660,\"start\":34654},{\"end\":34674,\"start\":34668},{\"end\":34685,\"start\":34680},{\"end\":34948,\"start\":34942},{\"end\":34959,\"start\":34955},{\"end\":34973,\"start\":34969},{\"end\":35231,\"start\":35225},{\"end\":35243,\"start\":35239},{\"end\":35258,\"start\":35253},{\"end\":35600,\"start\":35594},{\"end\":35616,\"start\":35608},{\"end\":35917,\"start\":35909},{\"end\":35928,\"start\":35923},{\"end\":35939,\"start\":35934},{\"end\":35952,\"start\":35945},{\"end\":35967,\"start\":35960},{\"end\":36217,\"start\":36209},{\"end\":36232,\"start\":36224},{\"end\":36243,\"start\":36237},{\"end\":36257,\"start\":36250},{\"end\":36266,\"start\":36263},{\"end\":36275,\"start\":36274},{\"end\":36600,\"start\":36593},{\"end\":36613,\"start\":36607},{\"end\":36628,\"start\":36620},{\"end\":36639,\"start\":36635},{\"end\":36836,\"start\":36829},{\"end\":36849,\"start\":36843},{\"end\":36860,\"start\":36854},{\"end\":36874,\"start\":36867},{\"end\":36889,\"start\":36880},{\"end\":36901,\"start\":36895},{\"end\":37113,\"start\":37111},{\"end\":37124,\"start\":37120},{\"end\":37138,\"start\":37130},{\"end\":37151,\"start\":37144},{\"end\":37162,\"start\":37158},{\"end\":37404,\"start\":37396},{\"end\":37414,\"start\":37411},{\"end\":37427,\"start\":37420},{\"end\":37441,\"start\":37433},{\"end\":37682,\"start\":37678},{\"end\":37697,\"start\":37690},{\"end\":37706,\"start\":37702},{\"end\":37927,\"start\":37923},{\"end\":37939,\"start\":37934},{\"end\":37950,\"start\":37946},{\"end\":37957,\"start\":37951},{\"end\":37969,\"start\":37963},{\"end\":38198,\"start\":38194},{\"end\":38209,\"start\":38205},{\"end\":38216,\"start\":38210},{\"end\":38229,\"start\":38222},{\"end\":38240,\"start\":38234},{\"end\":38464,\"start\":38456},{\"end\":38481,\"start\":38474},{\"end\":38498,\"start\":38493},{\"end\":38515,\"start\":38508},{\"end\":38519,\"start\":38516},{\"end\":38745,\"start\":38744},{\"end\":38761,\"start\":38752},{\"end\":38763,\"start\":38762},{\"end\":38779,\"start\":38771},{\"end\":39031,\"start\":39026},{\"end\":39048,\"start\":39039},{\"end\":39064,\"start\":39059},{\"end\":39081,\"start\":39073},{\"end\":39371,\"start\":39363},{\"end\":39403,\"start\":39396},{\"end\":39629,\"start\":39626},{\"end\":39646,\"start\":39642},{\"end\":39667,\"start\":39662},{\"end\":39679,\"start\":39675},{\"end\":39689,\"start\":39684},{\"end\":39711,\"start\":39704},{\"end\":39724,\"start\":39719},{\"end\":39742,\"start\":39736},{\"end\":39997,\"start\":39991},{\"end\":40007,\"start\":40002},{\"end\":40019,\"start\":40014},{\"end\":40228,\"start\":40224},{\"end\":40240,\"start\":40233},{\"end\":40255,\"start\":40247},{\"end\":40269,\"start\":40262},{\"end\":40284,\"start\":40275},{\"end\":40544,\"start\":40541},{\"end\":40558,\"start\":40550},{\"end\":40776,\"start\":40772},{\"end\":40786,\"start\":40782},{\"end\":40801,\"start\":40793},{\"end\":40810,\"start\":40808},{\"end\":40825,\"start\":40817},{\"end\":40838,\"start\":40831},{\"end\":40850,\"start\":40845},{\"end\":40862,\"start\":40855},{\"end\":41124,\"start\":41121},{\"end\":41135,\"start\":41130},{\"end\":41144,\"start\":41142},{\"end\":41159,\"start\":41151},{\"end\":41173,\"start\":41165},{\"end\":41183,\"start\":41178},{\"end\":41395,\"start\":41388},{\"end\":41407,\"start\":41400},{\"end\":41572,\"start\":41565},{\"end\":41584,\"start\":41577},{\"end\":41600,\"start\":41592},{\"end\":41610,\"start\":41606},{\"end\":41827,\"start\":41822},{\"end\":41830,\"start\":41828},{\"end\":41844,\"start\":41838},{\"end\":41857,\"start\":41851},{\"end\":42118,\"start\":42112},{\"end\":42133,\"start\":42127},{\"end\":42150,\"start\":42144},{\"end\":42172,\"start\":42164},{\"end\":42186,\"start\":42182},{\"end\":42490,\"start\":42489},{\"end\":42507,\"start\":42499},{\"end\":42518,\"start\":42516},{\"end\":42530,\"start\":42524},{\"end\":42543,\"start\":42537},{\"end\":42564,\"start\":42558},{\"end\":42576,\"start\":42571},{\"end\":42592,\"start\":42585},{\"end\":42965,\"start\":42964},{\"end\":42976,\"start\":42972},{\"end\":42990,\"start\":42984},{\"end\":43003,\"start\":42999},{\"end\":43386,\"start\":43380},{\"end\":43397,\"start\":43394},{\"end\":43408,\"start\":43402},{\"end\":43420,\"start\":43414},{\"end\":43700,\"start\":43697},{\"end\":43711,\"start\":43708},{\"end\":43725,\"start\":43719},{\"end\":43733,\"start\":43730},{\"end\":43987,\"start\":43980},{\"end\":44001,\"start\":43996},{\"end\":44021,\"start\":44014},{\"end\":44309,\"start\":44303},{\"end\":44328,\"start\":44319},{\"end\":44338,\"start\":44336},{\"end\":44575,\"start\":44571},{\"end\":44588,\"start\":44584},{\"end\":44601,\"start\":44595},{\"end\":44615,\"start\":44609},{\"end\":44867,\"start\":44863},{\"end\":44882,\"start\":44876},{\"end\":44894,\"start\":44890},{\"end\":45105,\"start\":45101},{\"end\":45120,\"start\":45114},{\"end\":45133,\"start\":45128},{\"end\":45148,\"start\":45143},{\"end\":45165,\"start\":45159},{\"end\":45180,\"start\":45176},{\"end\":45440,\"start\":45433},{\"end\":45453,\"start\":45446},{\"end\":45465,\"start\":45459},{\"end\":45480,\"start\":45472},{\"end\":45680,\"start\":45679},{\"end\":45696,\"start\":45691},{\"end\":45889,\"start\":45884},{\"end\":45906,\"start\":45897},{\"end\":45921,\"start\":45915},{\"end\":45940,\"start\":45934},{\"end\":45954,\"start\":45950},{\"end\":46218,\"start\":46213},{\"end\":46233,\"start\":46226},{\"end\":46250,\"start\":46245},{\"end\":46264,\"start\":46255},{\"end\":46522,\"start\":46513},{\"end\":46535,\"start\":46530},{\"end\":46549,\"start\":46543},{\"end\":46562,\"start\":46558},{\"end\":46580,\"start\":46574},{\"end\":46602,\"start\":46593},{\"end\":46617,\"start\":46611},{\"end\":46633,\"start\":46626},{\"end\":46650,\"start\":46642},{\"end\":46662,\"start\":46657},{\"end\":47029,\"start\":47020},{\"end\":47259,\"start\":47251},{\"end\":47269,\"start\":47264},{\"end\":47280,\"start\":47276},{\"end\":47289,\"start\":47287},{\"end\":47300,\"start\":47295},{\"end\":47311,\"start\":47306},{\"end\":47322,\"start\":47317},{\"end\":47608,\"start\":47600},{\"end\":47621,\"start\":47613},{\"end\":47637,\"start\":47628},{\"end\":47651,\"start\":47644},{\"end\":47665,\"start\":47657},{\"end\":47674,\"start\":47671},{\"end\":47983,\"start\":47975},{\"end\":47993,\"start\":47988},{\"end\":48005,\"start\":47998},{\"end\":48018,\"start\":48011},{\"end\":48030,\"start\":48026},{\"end\":48045,\"start\":48037},{\"end\":48316,\"start\":48308},{\"end\":48325,\"start\":48321},{\"end\":48337,\"start\":48331},{\"end\":48350,\"start\":48342},{\"end\":48361,\"start\":48356},{\"end\":48374,\"start\":48367},{\"end\":48574,\"start\":48569},{\"end\":48584,\"start\":48579},{\"end\":48595,\"start\":48590},{\"end\":48612,\"start\":48602},{\"end\":48797,\"start\":48794},{\"end\":48811,\"start\":48803},{\"end\":48823,\"start\":48817},{\"end\":48837,\"start\":48829},{\"end\":48852,\"start\":48843},{\"end\":49078,\"start\":49074},{\"end\":49089,\"start\":49084},{\"end\":49101,\"start\":49095},{\"end\":49111,\"start\":49107},{\"end\":49118,\"start\":49112},{\"end\":49132,\"start\":49124},{\"end\":49362,\"start\":49359},{\"end\":49374,\"start\":49368},{\"end\":49388,\"start\":49382},{\"end\":49398,\"start\":49395},{\"end\":49413,\"start\":49405},{\"end\":49623,\"start\":49618},{\"end\":49633,\"start\":49629},{\"end\":49647,\"start\":49639},{\"end\":49660,\"start\":49654},{\"end\":49884,\"start\":49878},{\"end\":49901,\"start\":49892},{\"end\":49916,\"start\":49910},{\"end\":49927,\"start\":49921},{\"end\":49941,\"start\":49934},{\"end\":50240,\"start\":50235},{\"end\":50254,\"start\":50249},{\"end\":50274,\"start\":50270},{\"end\":50276,\"start\":50275},{\"end\":50541,\"start\":50534},{\"end\":50555,\"start\":50547},{\"end\":50564,\"start\":50562},{\"end\":50575,\"start\":50570},{\"end\":50585,\"start\":50581},{\"end\":50592,\"start\":50586},{\"end\":50602,\"start\":50598},{\"end\":50877,\"start\":50870},{\"end\":50891,\"start\":50884},{\"end\":50906,\"start\":50897},{\"end\":50920,\"start\":50913},{\"end\":51150,\"start\":51146},{\"end\":51171,\"start\":51164},{\"end\":51187,\"start\":51181},{\"end\":51579,\"start\":51578},{\"end\":51581,\"start\":51580},{\"end\":51597,\"start\":51589},{\"end\":51614,\"start\":51607},{\"end\":51830,\"start\":51826},{\"end\":51846,\"start\":51837},{\"end\":51858,\"start\":51852},{\"end\":51866,\"start\":51863},{\"end\":51884,\"start\":51874},{\"end\":52093,\"start\":52088},{\"end\":52110,\"start\":52104},{\"end\":52333,\"start\":52329},{\"end\":52350,\"start\":52343},{\"end\":52365,\"start\":52362},{\"end\":52386,\"start\":52376},{\"end\":52396,\"start\":52393},{\"end\":52624,\"start\":52620},{\"end\":52632,\"start\":52630},{\"end\":52648,\"start\":52640},{\"end\":52658,\"start\":52655},{\"end\":52670,\"start\":52666},{\"end\":52681,\"start\":52677},{\"end\":52692,\"start\":52688},{\"end\":52945,\"start\":52936},{\"end\":52959,\"start\":52952},{\"end\":52972,\"start\":52965},{\"end\":52984,\"start\":52978},{\"end\":52993,\"start\":52990},{\"end\":53006,\"start\":53001},{\"end\":53314,\"start\":53308},{\"end\":53323,\"start\":53321},{\"end\":53332,\"start\":53328},{\"end\":53350,\"start\":53339},{\"end\":53600,\"start\":53594},{\"end\":53609,\"start\":53607},{\"end\":53622,\"start\":53614},{\"end\":53633,\"start\":53627},{\"end\":53643,\"start\":53638},{\"end\":53653,\"start\":53649},{\"end\":53662,\"start\":53660},{\"end\":53680,\"start\":53669},{\"end\":53949,\"start\":53947},{\"end\":53959,\"start\":53954},{\"end\":53961,\"start\":53960},{\"end\":53969,\"start\":53967},{\"end\":53980,\"start\":53975},{\"end\":54181,\"start\":54174},{\"end\":54192,\"start\":54186},{\"end\":54205,\"start\":54198},{\"end\":54216,\"start\":54211},{\"end\":54233,\"start\":54224},{\"end\":54253,\"start\":54243},{\"end\":54512,\"start\":54506},{\"end\":54524,\"start\":54519},{\"end\":54533,\"start\":54530},{\"end\":54547,\"start\":54540},{\"end\":54560,\"start\":54554},{\"end\":54571,\"start\":54566},{\"end\":54579,\"start\":54576},{\"end\":54867,\"start\":54865},{\"end\":54878,\"start\":54872},{\"end\":54889,\"start\":54885},{\"end\":54902,\"start\":54896},{\"end\":54920,\"start\":54909},{\"end\":55167,\"start\":55164},{\"end\":55178,\"start\":55172},{\"end\":55196,\"start\":55189},{\"end\":55210,\"start\":55205},{\"end\":55227,\"start\":55220},{\"end\":55500,\"start\":55497},{\"end\":55511,\"start\":55505},{\"end\":55529,\"start\":55522},{\"end\":55544,\"start\":55539},{\"end\":55806,\"start\":55803},{\"end\":55822,\"start\":55814},{\"end\":55834,\"start\":55828},{\"end\":55845,\"start\":55841},{\"end\":55855,\"start\":55852},{\"end\":56094,\"start\":56087},{\"end\":56109,\"start\":56102},{\"end\":56123,\"start\":56117},{\"end\":56125,\"start\":56124},{\"end\":56315,\"start\":56308},{\"end\":56330,\"start\":56323},{\"end\":56344,\"start\":56338},{\"end\":56346,\"start\":56345},{\"end\":56357,\"start\":56354},{\"end\":56375,\"start\":56369},{\"end\":56641,\"start\":56636},{\"end\":56656,\"start\":56649},{\"end\":56664,\"start\":56661},{\"end\":56675,\"start\":56669},{\"end\":56688,\"start\":56682},{\"end\":56699,\"start\":56696},{\"end\":56954,\"start\":56948},{\"end\":56966,\"start\":56961},{\"end\":56977,\"start\":56974},{\"end\":56992,\"start\":56985},{\"end\":57279,\"start\":57272},{\"end\":57290,\"start\":57285},{\"end\":57301,\"start\":57297},{\"end\":57313,\"start\":57308},{\"end\":57512,\"start\":57505},{\"end\":57523,\"start\":57518},{\"end\":57533,\"start\":57529},{\"end\":57540,\"start\":57534},{\"end\":57552,\"start\":57546}]", "bib_author_last_name": "[{\"end\":34666,\"start\":34661},{\"end\":34678,\"start\":34675},{\"end\":34691,\"start\":34686},{\"end\":34953,\"start\":34949},{\"end\":34967,\"start\":34960},{\"end\":34981,\"start\":34974},{\"end\":35237,\"start\":35232},{\"end\":35251,\"start\":35244},{\"end\":35267,\"start\":35259},{\"end\":35606,\"start\":35601},{\"end\":35630,\"start\":35617},{\"end\":35921,\"start\":35918},{\"end\":35932,\"start\":35929},{\"end\":35943,\"start\":35940},{\"end\":35958,\"start\":35953},{\"end\":35970,\"start\":35968},{\"end\":36222,\"start\":36218},{\"end\":36235,\"start\":36233},{\"end\":36248,\"start\":36244},{\"end\":36261,\"start\":36258},{\"end\":36272,\"start\":36267},{\"end\":36284,\"start\":36276},{\"end\":36290,\"start\":36286},{\"end\":36605,\"start\":36601},{\"end\":36618,\"start\":36614},{\"end\":36633,\"start\":36629},{\"end\":36644,\"start\":36640},{\"end\":36841,\"start\":36837},{\"end\":36852,\"start\":36850},{\"end\":36865,\"start\":36861},{\"end\":36878,\"start\":36875},{\"end\":36893,\"start\":36890},{\"end\":36906,\"start\":36902},{\"end\":37118,\"start\":37114},{\"end\":37128,\"start\":37125},{\"end\":37142,\"start\":37139},{\"end\":37156,\"start\":37152},{\"end\":37167,\"start\":37163},{\"end\":37409,\"start\":37405},{\"end\":37418,\"start\":37415},{\"end\":37431,\"start\":37428},{\"end\":37451,\"start\":37442},{\"end\":37688,\"start\":37683},{\"end\":37700,\"start\":37698},{\"end\":37714,\"start\":37707},{\"end\":37932,\"start\":37928},{\"end\":37944,\"start\":37940},{\"end\":37961,\"start\":37958},{\"end\":37974,\"start\":37970},{\"end\":38203,\"start\":38199},{\"end\":38220,\"start\":38217},{\"end\":38232,\"start\":38230},{\"end\":38245,\"start\":38241},{\"end\":38472,\"start\":38465},{\"end\":38491,\"start\":38482},{\"end\":38506,\"start\":38499},{\"end\":38525,\"start\":38520},{\"end\":38750,\"start\":38746},{\"end\":38769,\"start\":38764},{\"end\":38785,\"start\":38780},{\"end\":38793,\"start\":38787},{\"end\":39037,\"start\":39032},{\"end\":39057,\"start\":39049},{\"end\":39071,\"start\":39065},{\"end\":39091,\"start\":39082},{\"end\":39394,\"start\":39372},{\"end\":39413,\"start\":39404},{\"end\":39421,\"start\":39415},{\"end\":39640,\"start\":39630},{\"end\":39660,\"start\":39647},{\"end\":39673,\"start\":39668},{\"end\":39682,\"start\":39680},{\"end\":39702,\"start\":39690},{\"end\":39717,\"start\":39712},{\"end\":39734,\"start\":39725},{\"end\":39749,\"start\":39743},{\"end\":40000,\"start\":39998},{\"end\":40012,\"start\":40008},{\"end\":40024,\"start\":40020},{\"end\":40231,\"start\":40229},{\"end\":40245,\"start\":40241},{\"end\":40260,\"start\":40256},{\"end\":40273,\"start\":40270},{\"end\":40289,\"start\":40285},{\"end\":40548,\"start\":40545},{\"end\":40563,\"start\":40559},{\"end\":40780,\"start\":40777},{\"end\":40791,\"start\":40787},{\"end\":40806,\"start\":40802},{\"end\":40815,\"start\":40811},{\"end\":40829,\"start\":40826},{\"end\":40843,\"start\":40839},{\"end\":40853,\"start\":40851},{\"end\":40866,\"start\":40863},{\"end\":41128,\"start\":41125},{\"end\":41140,\"start\":41136},{\"end\":41149,\"start\":41145},{\"end\":41163,\"start\":41160},{\"end\":41176,\"start\":41174},{\"end\":41186,\"start\":41184},{\"end\":41398,\"start\":41396},{\"end\":41416,\"start\":41408},{\"end\":41575,\"start\":41573},{\"end\":41590,\"start\":41585},{\"end\":41604,\"start\":41601},{\"end\":41614,\"start\":41611},{\"end\":41836,\"start\":41831},{\"end\":41849,\"start\":41845},{\"end\":41867,\"start\":41858},{\"end\":42125,\"start\":42119},{\"end\":42142,\"start\":42134},{\"end\":42162,\"start\":42151},{\"end\":42180,\"start\":42173},{\"end\":42197,\"start\":42187},{\"end\":42497,\"start\":42491},{\"end\":42514,\"start\":42508},{\"end\":42522,\"start\":42519},{\"end\":42535,\"start\":42531},{\"end\":42556,\"start\":42544},{\"end\":42569,\"start\":42565},{\"end\":42583,\"start\":42577},{\"end\":42602,\"start\":42593},{\"end\":42608,\"start\":42604},{\"end\":42970,\"start\":42966},{\"end\":42982,\"start\":42977},{\"end\":42997,\"start\":42991},{\"end\":43008,\"start\":43004},{\"end\":43024,\"start\":43010},{\"end\":43392,\"start\":43387},{\"end\":43400,\"start\":43398},{\"end\":43412,\"start\":43409},{\"end\":43424,\"start\":43421},{\"end\":43706,\"start\":43701},{\"end\":43717,\"start\":43712},{\"end\":43728,\"start\":43726},{\"end\":43736,\"start\":43734},{\"end\":43994,\"start\":43988},{\"end\":44012,\"start\":44002},{\"end\":44030,\"start\":44022},{\"end\":44317,\"start\":44310},{\"end\":44334,\"start\":44329},{\"end\":44346,\"start\":44339},{\"end\":44582,\"start\":44576},{\"end\":44593,\"start\":44589},{\"end\":44607,\"start\":44602},{\"end\":44624,\"start\":44616},{\"end\":44874,\"start\":44868},{\"end\":44888,\"start\":44883},{\"end\":44899,\"start\":44895},{\"end\":45112,\"start\":45106},{\"end\":45126,\"start\":45121},{\"end\":45141,\"start\":45134},{\"end\":45157,\"start\":45149},{\"end\":45174,\"start\":45166},{\"end\":45185,\"start\":45181},{\"end\":45444,\"start\":45441},{\"end\":45457,\"start\":45454},{\"end\":45470,\"start\":45466},{\"end\":45484,\"start\":45481},{\"end\":45689,\"start\":45681},{\"end\":45703,\"start\":45697},{\"end\":45707,\"start\":45705},{\"end\":45895,\"start\":45890},{\"end\":45913,\"start\":45907},{\"end\":45932,\"start\":45922},{\"end\":45948,\"start\":45941},{\"end\":45960,\"start\":45955},{\"end\":46224,\"start\":46219},{\"end\":46243,\"start\":46234},{\"end\":46253,\"start\":46251},{\"end\":46269,\"start\":46265},{\"end\":46528,\"start\":46523},{\"end\":46541,\"start\":46536},{\"end\":46556,\"start\":46550},{\"end\":46572,\"start\":46563},{\"end\":46591,\"start\":46581},{\"end\":46609,\"start\":46603},{\"end\":46624,\"start\":46618},{\"end\":46640,\"start\":46634},{\"end\":46655,\"start\":46651},{\"end\":46667,\"start\":46663},{\"end\":47042,\"start\":47030},{\"end\":47262,\"start\":47260},{\"end\":47274,\"start\":47270},{\"end\":47285,\"start\":47281},{\"end\":47293,\"start\":47290},{\"end\":47304,\"start\":47301},{\"end\":47315,\"start\":47312},{\"end\":47326,\"start\":47323},{\"end\":47611,\"start\":47609},{\"end\":47626,\"start\":47622},{\"end\":47642,\"start\":47638},{\"end\":47655,\"start\":47652},{\"end\":47669,\"start\":47666},{\"end\":47680,\"start\":47675},{\"end\":47986,\"start\":47984},{\"end\":47996,\"start\":47994},{\"end\":48009,\"start\":48006},{\"end\":48024,\"start\":48019},{\"end\":48035,\"start\":48031},{\"end\":48049,\"start\":48046},{\"end\":48319,\"start\":48317},{\"end\":48329,\"start\":48326},{\"end\":48340,\"start\":48338},{\"end\":48354,\"start\":48351},{\"end\":48365,\"start\":48362},{\"end\":48379,\"start\":48375},{\"end\":48577,\"start\":48575},{\"end\":48588,\"start\":48585},{\"end\":48600,\"start\":48596},{\"end\":48617,\"start\":48613},{\"end\":48801,\"start\":48798},{\"end\":48815,\"start\":48812},{\"end\":48827,\"start\":48824},{\"end\":48841,\"start\":48838},{\"end\":48856,\"start\":48853},{\"end\":49082,\"start\":49079},{\"end\":49093,\"start\":49090},{\"end\":49105,\"start\":49102},{\"end\":49122,\"start\":49119},{\"end\":49138,\"start\":49133},{\"end\":49366,\"start\":49363},{\"end\":49380,\"start\":49375},{\"end\":49393,\"start\":49389},{\"end\":49403,\"start\":49399},{\"end\":49416,\"start\":49414},{\"end\":49627,\"start\":49624},{\"end\":49637,\"start\":49634},{\"end\":49652,\"start\":49648},{\"end\":49665,\"start\":49661},{\"end\":49890,\"start\":49885},{\"end\":49908,\"start\":49902},{\"end\":49919,\"start\":49917},{\"end\":49932,\"start\":49928},{\"end\":49947,\"start\":49942},{\"end\":50247,\"start\":50241},{\"end\":50268,\"start\":50255},{\"end\":50282,\"start\":50277},{\"end\":50545,\"start\":50542},{\"end\":50560,\"start\":50556},{\"end\":50568,\"start\":50565},{\"end\":50579,\"start\":50576},{\"end\":50596,\"start\":50593},{\"end\":50606,\"start\":50603},{\"end\":50882,\"start\":50878},{\"end\":50895,\"start\":50892},{\"end\":50911,\"start\":50907},{\"end\":50924,\"start\":50921},{\"end\":51162,\"start\":51151},{\"end\":51179,\"start\":51172},{\"end\":51192,\"start\":51188},{\"end\":51587,\"start\":51582},{\"end\":51605,\"start\":51598},{\"end\":51624,\"start\":51615},{\"end\":51632,\"start\":51626},{\"end\":51835,\"start\":51831},{\"end\":51850,\"start\":51847},{\"end\":51861,\"start\":51859},{\"end\":51872,\"start\":51867},{\"end\":51889,\"start\":51885},{\"end\":52102,\"start\":52094},{\"end\":52120,\"start\":52111},{\"end\":52341,\"start\":52334},{\"end\":52360,\"start\":52351},{\"end\":52374,\"start\":52366},{\"end\":52391,\"start\":52387},{\"end\":52402,\"start\":52397},{\"end\":52628,\"start\":52625},{\"end\":52638,\"start\":52633},{\"end\":52653,\"start\":52649},{\"end\":52664,\"start\":52659},{\"end\":52675,\"start\":52671},{\"end\":52686,\"start\":52682},{\"end\":52696,\"start\":52693},{\"end\":52950,\"start\":52946},{\"end\":52963,\"start\":52960},{\"end\":52976,\"start\":52973},{\"end\":52988,\"start\":52985},{\"end\":52999,\"start\":52994},{\"end\":53016,\"start\":53007},{\"end\":53319,\"start\":53315},{\"end\":53326,\"start\":53324},{\"end\":53337,\"start\":53333},{\"end\":53354,\"start\":53351},{\"end\":53605,\"start\":53601},{\"end\":53612,\"start\":53610},{\"end\":53625,\"start\":53623},{\"end\":53636,\"start\":53634},{\"end\":53647,\"start\":53644},{\"end\":53658,\"start\":53654},{\"end\":53667,\"start\":53663},{\"end\":53684,\"start\":53681},{\"end\":53952,\"start\":53950},{\"end\":53965,\"start\":53962},{\"end\":53973,\"start\":53970},{\"end\":53984,\"start\":53981},{\"end\":54184,\"start\":54182},{\"end\":54196,\"start\":54193},{\"end\":54209,\"start\":54206},{\"end\":54222,\"start\":54217},{\"end\":54241,\"start\":54234},{\"end\":54258,\"start\":54254},{\"end\":54517,\"start\":54513},{\"end\":54528,\"start\":54525},{\"end\":54538,\"start\":54534},{\"end\":54552,\"start\":54548},{\"end\":54564,\"start\":54561},{\"end\":54574,\"start\":54572},{\"end\":54583,\"start\":54580},{\"end\":54870,\"start\":54868},{\"end\":54883,\"start\":54879},{\"end\":54894,\"start\":54890},{\"end\":54907,\"start\":54903},{\"end\":54924,\"start\":54921},{\"end\":55170,\"start\":55168},{\"end\":55187,\"start\":55179},{\"end\":55203,\"start\":55197},{\"end\":55218,\"start\":55211},{\"end\":55235,\"start\":55228},{\"end\":55503,\"start\":55501},{\"end\":55520,\"start\":55512},{\"end\":55537,\"start\":55530},{\"end\":55552,\"start\":55545},{\"end\":55812,\"start\":55807},{\"end\":55826,\"start\":55823},{\"end\":55839,\"start\":55835},{\"end\":55850,\"start\":55846},{\"end\":55861,\"start\":55856},{\"end\":56100,\"start\":56095},{\"end\":56115,\"start\":56110},{\"end\":56131,\"start\":56126},{\"end\":56321,\"start\":56316},{\"end\":56336,\"start\":56331},{\"end\":56352,\"start\":56347},{\"end\":56367,\"start\":56358},{\"end\":56380,\"start\":56376},{\"end\":56647,\"start\":56642},{\"end\":56659,\"start\":56657},{\"end\":56667,\"start\":56665},{\"end\":56680,\"start\":56676},{\"end\":56694,\"start\":56689},{\"end\":56702,\"start\":56700},{\"end\":56959,\"start\":56955},{\"end\":56972,\"start\":56967},{\"end\":56983,\"start\":56978},{\"end\":56996,\"start\":56993},{\"end\":57283,\"start\":57280},{\"end\":57295,\"start\":57291},{\"end\":57306,\"start\":57302},{\"end\":57318,\"start\":57314},{\"end\":57516,\"start\":57513},{\"end\":57527,\"start\":57524},{\"end\":57544,\"start\":57541},{\"end\":57557,\"start\":57553}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":102350964},\"end\":34839,\"start\":34585},{\"attributes\":{\"id\":\"b1\"},\"end\":35155,\"start\":34841},{\"attributes\":{\"doi\":\"arXiv:1809.11096\",\"id\":\"b2\"},\"end\":35455,\"start\":35157},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4319595},\"end\":35839,\"start\":35457},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1882232},\"end\":36131,\"start\":35841},{\"attributes\":{\"doi\":\"arXiv:2009.08709\",\"id\":\"b5\"},\"end\":36511,\"start\":36133},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":51989956},\"end\":36807,\"start\":36513},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":35602767},\"end\":37039,\"start\":36809},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4564405},\"end\":37329,\"start\":37041},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8923541},\"end\":37606,\"start\":37331},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":189928566},\"end\":37856,\"start\":37608},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9569924},\"end\":38126,\"start\":37858},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":18874645},\"end\":38396,\"start\":38128},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":3084814},\"end\":38684,\"start\":38398},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206593710},\"end\":38935,\"start\":38686},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":62841624},\"end\":39275,\"start\":38937},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":51894650},\"end\":39595,\"start\":39277},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1033682},\"end\":39944,\"start\":39597},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":209377041},\"end\":40143,\"start\":39946},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":131775478},\"end\":40465,\"start\":40145},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8585513},\"end\":40690,\"start\":40467},{\"attributes\":{\"id\":\"b21\"},\"end\":41072,\"start\":40692},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":208310058},\"end\":41341,\"start\":41074},{\"attributes\":{\"id\":\"b23\"},\"end\":41517,\"start\":41343},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206594692},\"end\":41739,\"start\":41519},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":212725532},\"end\":42027,\"start\":41741},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":326772},\"end\":42403,\"start\":42029},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b27\"},\"end\":42863,\"start\":42405},{\"attributes\":{\"id\":\"b28\"},\"end\":43304,\"start\":42865},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":22066308},\"end\":43574,\"start\":43306},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13058841},\"end\":43928,\"start\":43576},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3707204},\"end\":44232,\"start\":43930},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":980236},\"end\":44493,\"start\":44234},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3568073},\"end\":44787,\"start\":44495},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":54482423},\"end\":45044,\"start\":44789},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":209202273},\"end\":45363,\"start\":45046},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":201317137},\"end\":45633,\"start\":45365},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6628106},\"end\":45807,\"start\":45635},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4552226},\"end\":46145,\"start\":45809},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":199543931},\"end\":46426,\"start\":46147},{\"attributes\":{\"id\":\"b40\"},\"end\":46950,\"start\":46428},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":10352330},\"end\":47158,\"start\":46952},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":53040708},\"end\":47530,\"start\":47160},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":220935924},\"end\":47877,\"start\":47532},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":219618216},\"end\":48253,\"start\":47879},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":4889032},\"end\":48539,\"start\":48255},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":9459250},\"end\":48725,\"start\":48541},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":6540453},\"end\":49021,\"start\":48727},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":47007607},\"end\":49293,\"start\":49023},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":219963148},\"end\":49573,\"start\":49295},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":459456},\"end\":49785,\"start\":49575},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":212634162},\"end\":50149,\"start\":49787},{\"attributes\":{\"id\":\"b52\"},\"end\":50449,\"start\":50151},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":214713474},\"end\":50804,\"start\":50451},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":81981856},\"end\":51079,\"start\":50806},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":3719281},\"end\":51497,\"start\":51081},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":206771333},\"end\":51793,\"start\":51499},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3825399},\"end\":52018,\"start\":51795},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":14124313},\"end\":52251,\"start\":52020},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":484327},\"end\":52584,\"start\":52253},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":215827763},\"end\":52853,\"start\":52586},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":41805341},\"end\":53216,\"start\":52855},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":4710407},\"end\":53525,\"start\":53218},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":52154773},\"end\":53886,\"start\":53527},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":7036324},\"end\":54118,\"start\":53888},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":19668149},\"end\":54428,\"start\":54120},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":218581319},\"end\":54794,\"start\":54430},{\"attributes\":{\"doi\":\"arXiv:1904.10343\",\"id\":\"b67\"},\"end\":55103,\"start\":54796},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":52084187},\"end\":55416,\"start\":55105},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":3721829},\"end\":55722,\"start\":55418},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":996788},\"end\":56056,\"start\":55724},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":50698},\"end\":56234,\"start\":56058},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":4766599},\"end\":56558,\"start\":56236},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":49657846},\"end\":56882,\"start\":56560},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":53113531},\"end\":57223,\"start\":56884},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":214743564},\"end\":57454,\"start\":57225},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":11859568},\"end\":57688,\"start\":57456}]", "bib_title": "[{\"end\":34652,\"start\":34585},{\"end\":34940,\"start\":34841},{\"end\":35592,\"start\":35457},{\"end\":35907,\"start\":35841},{\"end\":36591,\"start\":36513},{\"end\":36827,\"start\":36809},{\"end\":37109,\"start\":37041},{\"end\":37394,\"start\":37331},{\"end\":37676,\"start\":37608},{\"end\":37921,\"start\":37858},{\"end\":38192,\"start\":38128},{\"end\":38454,\"start\":38398},{\"end\":38742,\"start\":38686},{\"end\":39024,\"start\":38937},{\"end\":39361,\"start\":39277},{\"end\":39624,\"start\":39597},{\"end\":39989,\"start\":39946},{\"end\":40222,\"start\":40145},{\"end\":40539,\"start\":40467},{\"end\":40770,\"start\":40692},{\"end\":41119,\"start\":41074},{\"end\":41386,\"start\":41343},{\"end\":41563,\"start\":41519},{\"end\":41820,\"start\":41741},{\"end\":42110,\"start\":42029},{\"end\":43378,\"start\":43306},{\"end\":43695,\"start\":43576},{\"end\":43978,\"start\":43930},{\"end\":44301,\"start\":44234},{\"end\":44569,\"start\":44495},{\"end\":44861,\"start\":44789},{\"end\":45099,\"start\":45046},{\"end\":45431,\"start\":45365},{\"end\":45677,\"start\":45635},{\"end\":45882,\"start\":45809},{\"end\":46211,\"start\":46147},{\"end\":46511,\"start\":46428},{\"end\":47018,\"start\":46952},{\"end\":47249,\"start\":47160},{\"end\":47598,\"start\":47532},{\"end\":47973,\"start\":47879},{\"end\":48306,\"start\":48255},{\"end\":48567,\"start\":48541},{\"end\":48792,\"start\":48727},{\"end\":49072,\"start\":49023},{\"end\":49357,\"start\":49295},{\"end\":49616,\"start\":49575},{\"end\":49876,\"start\":49787},{\"end\":50532,\"start\":50451},{\"end\":50868,\"start\":50806},{\"end\":51144,\"start\":51081},{\"end\":51576,\"start\":51499},{\"end\":51824,\"start\":51795},{\"end\":52086,\"start\":52020},{\"end\":52327,\"start\":52253},{\"end\":52618,\"start\":52586},{\"end\":52934,\"start\":52855},{\"end\":53306,\"start\":53218},{\"end\":53592,\"start\":53527},{\"end\":53945,\"start\":53888},{\"end\":54172,\"start\":54120},{\"end\":54504,\"start\":54430},{\"end\":55162,\"start\":55105},{\"end\":55495,\"start\":55418},{\"end\":55801,\"start\":55724},{\"end\":56085,\"start\":56058},{\"end\":56306,\"start\":56236},{\"end\":56634,\"start\":56560},{\"end\":56946,\"start\":56884},{\"end\":57270,\"start\":57225},{\"end\":57503,\"start\":57456}]", "bib_author": "[{\"end\":34668,\"start\":34654},{\"end\":34680,\"start\":34668},{\"end\":34693,\"start\":34680},{\"end\":34955,\"start\":34942},{\"end\":34969,\"start\":34955},{\"end\":34983,\"start\":34969},{\"end\":35239,\"start\":35225},{\"end\":35253,\"start\":35239},{\"end\":35269,\"start\":35253},{\"end\":35608,\"start\":35594},{\"end\":35632,\"start\":35608},{\"end\":35923,\"start\":35909},{\"end\":35934,\"start\":35923},{\"end\":35945,\"start\":35934},{\"end\":35960,\"start\":35945},{\"end\":35972,\"start\":35960},{\"end\":36224,\"start\":36209},{\"end\":36237,\"start\":36224},{\"end\":36250,\"start\":36237},{\"end\":36263,\"start\":36250},{\"end\":36274,\"start\":36263},{\"end\":36286,\"start\":36274},{\"end\":36292,\"start\":36286},{\"end\":36607,\"start\":36593},{\"end\":36620,\"start\":36607},{\"end\":36635,\"start\":36620},{\"end\":36646,\"start\":36635},{\"end\":36843,\"start\":36829},{\"end\":36854,\"start\":36843},{\"end\":36867,\"start\":36854},{\"end\":36880,\"start\":36867},{\"end\":36895,\"start\":36880},{\"end\":36908,\"start\":36895},{\"end\":37120,\"start\":37111},{\"end\":37130,\"start\":37120},{\"end\":37144,\"start\":37130},{\"end\":37158,\"start\":37144},{\"end\":37169,\"start\":37158},{\"end\":37411,\"start\":37396},{\"end\":37420,\"start\":37411},{\"end\":37433,\"start\":37420},{\"end\":37453,\"start\":37433},{\"end\":37690,\"start\":37678},{\"end\":37702,\"start\":37690},{\"end\":37716,\"start\":37702},{\"end\":37934,\"start\":37923},{\"end\":37946,\"start\":37934},{\"end\":37963,\"start\":37946},{\"end\":37976,\"start\":37963},{\"end\":38205,\"start\":38194},{\"end\":38222,\"start\":38205},{\"end\":38234,\"start\":38222},{\"end\":38247,\"start\":38234},{\"end\":38474,\"start\":38456},{\"end\":38493,\"start\":38474},{\"end\":38508,\"start\":38493},{\"end\":38527,\"start\":38508},{\"end\":38752,\"start\":38744},{\"end\":38771,\"start\":38752},{\"end\":38787,\"start\":38771},{\"end\":38795,\"start\":38787},{\"end\":39039,\"start\":39026},{\"end\":39059,\"start\":39039},{\"end\":39073,\"start\":39059},{\"end\":39093,\"start\":39073},{\"end\":39396,\"start\":39363},{\"end\":39415,\"start\":39396},{\"end\":39423,\"start\":39415},{\"end\":39642,\"start\":39626},{\"end\":39662,\"start\":39642},{\"end\":39675,\"start\":39662},{\"end\":39684,\"start\":39675},{\"end\":39704,\"start\":39684},{\"end\":39719,\"start\":39704},{\"end\":39736,\"start\":39719},{\"end\":39751,\"start\":39736},{\"end\":40002,\"start\":39991},{\"end\":40014,\"start\":40002},{\"end\":40026,\"start\":40014},{\"end\":40233,\"start\":40224},{\"end\":40247,\"start\":40233},{\"end\":40262,\"start\":40247},{\"end\":40275,\"start\":40262},{\"end\":40291,\"start\":40275},{\"end\":40550,\"start\":40541},{\"end\":40565,\"start\":40550},{\"end\":40782,\"start\":40772},{\"end\":40793,\"start\":40782},{\"end\":40808,\"start\":40793},{\"end\":40817,\"start\":40808},{\"end\":40831,\"start\":40817},{\"end\":40845,\"start\":40831},{\"end\":40855,\"start\":40845},{\"end\":40868,\"start\":40855},{\"end\":41130,\"start\":41121},{\"end\":41142,\"start\":41130},{\"end\":41151,\"start\":41142},{\"end\":41165,\"start\":41151},{\"end\":41178,\"start\":41165},{\"end\":41188,\"start\":41178},{\"end\":41400,\"start\":41388},{\"end\":41418,\"start\":41400},{\"end\":41577,\"start\":41565},{\"end\":41592,\"start\":41577},{\"end\":41606,\"start\":41592},{\"end\":41616,\"start\":41606},{\"end\":41838,\"start\":41822},{\"end\":41851,\"start\":41838},{\"end\":41869,\"start\":41851},{\"end\":42127,\"start\":42112},{\"end\":42144,\"start\":42127},{\"end\":42164,\"start\":42144},{\"end\":42182,\"start\":42164},{\"end\":42199,\"start\":42182},{\"end\":42499,\"start\":42489},{\"end\":42516,\"start\":42499},{\"end\":42524,\"start\":42516},{\"end\":42537,\"start\":42524},{\"end\":42558,\"start\":42537},{\"end\":42571,\"start\":42558},{\"end\":42585,\"start\":42571},{\"end\":42604,\"start\":42585},{\"end\":42610,\"start\":42604},{\"end\":42972,\"start\":42964},{\"end\":42984,\"start\":42972},{\"end\":42999,\"start\":42984},{\"end\":43010,\"start\":42999},{\"end\":43026,\"start\":43010},{\"end\":43394,\"start\":43380},{\"end\":43402,\"start\":43394},{\"end\":43414,\"start\":43402},{\"end\":43426,\"start\":43414},{\"end\":43708,\"start\":43697},{\"end\":43719,\"start\":43708},{\"end\":43730,\"start\":43719},{\"end\":43738,\"start\":43730},{\"end\":43996,\"start\":43980},{\"end\":44014,\"start\":43996},{\"end\":44032,\"start\":44014},{\"end\":44319,\"start\":44303},{\"end\":44336,\"start\":44319},{\"end\":44348,\"start\":44336},{\"end\":44584,\"start\":44571},{\"end\":44595,\"start\":44584},{\"end\":44609,\"start\":44595},{\"end\":44626,\"start\":44609},{\"end\":44876,\"start\":44863},{\"end\":44890,\"start\":44876},{\"end\":44901,\"start\":44890},{\"end\":45114,\"start\":45101},{\"end\":45128,\"start\":45114},{\"end\":45143,\"start\":45128},{\"end\":45159,\"start\":45143},{\"end\":45176,\"start\":45159},{\"end\":45187,\"start\":45176},{\"end\":45446,\"start\":45433},{\"end\":45459,\"start\":45446},{\"end\":45472,\"start\":45459},{\"end\":45486,\"start\":45472},{\"end\":45691,\"start\":45679},{\"end\":45705,\"start\":45691},{\"end\":45709,\"start\":45705},{\"end\":45897,\"start\":45884},{\"end\":45915,\"start\":45897},{\"end\":45934,\"start\":45915},{\"end\":45950,\"start\":45934},{\"end\":45962,\"start\":45950},{\"end\":46226,\"start\":46213},{\"end\":46245,\"start\":46226},{\"end\":46255,\"start\":46245},{\"end\":46271,\"start\":46255},{\"end\":46530,\"start\":46513},{\"end\":46543,\"start\":46530},{\"end\":46558,\"start\":46543},{\"end\":46574,\"start\":46558},{\"end\":46593,\"start\":46574},{\"end\":46611,\"start\":46593},{\"end\":46626,\"start\":46611},{\"end\":46642,\"start\":46626},{\"end\":46657,\"start\":46642},{\"end\":46669,\"start\":46657},{\"end\":47044,\"start\":47020},{\"end\":47264,\"start\":47251},{\"end\":47276,\"start\":47264},{\"end\":47287,\"start\":47276},{\"end\":47295,\"start\":47287},{\"end\":47306,\"start\":47295},{\"end\":47317,\"start\":47306},{\"end\":47328,\"start\":47317},{\"end\":47613,\"start\":47600},{\"end\":47628,\"start\":47613},{\"end\":47644,\"start\":47628},{\"end\":47657,\"start\":47644},{\"end\":47671,\"start\":47657},{\"end\":47682,\"start\":47671},{\"end\":47988,\"start\":47975},{\"end\":47998,\"start\":47988},{\"end\":48011,\"start\":47998},{\"end\":48026,\"start\":48011},{\"end\":48037,\"start\":48026},{\"end\":48051,\"start\":48037},{\"end\":48321,\"start\":48308},{\"end\":48331,\"start\":48321},{\"end\":48342,\"start\":48331},{\"end\":48356,\"start\":48342},{\"end\":48367,\"start\":48356},{\"end\":48381,\"start\":48367},{\"end\":48579,\"start\":48569},{\"end\":48590,\"start\":48579},{\"end\":48602,\"start\":48590},{\"end\":48619,\"start\":48602},{\"end\":48803,\"start\":48794},{\"end\":48817,\"start\":48803},{\"end\":48829,\"start\":48817},{\"end\":48843,\"start\":48829},{\"end\":48858,\"start\":48843},{\"end\":49084,\"start\":49074},{\"end\":49095,\"start\":49084},{\"end\":49107,\"start\":49095},{\"end\":49124,\"start\":49107},{\"end\":49140,\"start\":49124},{\"end\":49368,\"start\":49359},{\"end\":49382,\"start\":49368},{\"end\":49395,\"start\":49382},{\"end\":49405,\"start\":49395},{\"end\":49418,\"start\":49405},{\"end\":49629,\"start\":49618},{\"end\":49639,\"start\":49629},{\"end\":49654,\"start\":49639},{\"end\":49667,\"start\":49654},{\"end\":49892,\"start\":49878},{\"end\":49910,\"start\":49892},{\"end\":49921,\"start\":49910},{\"end\":49934,\"start\":49921},{\"end\":49949,\"start\":49934},{\"end\":50249,\"start\":50235},{\"end\":50270,\"start\":50249},{\"end\":50284,\"start\":50270},{\"end\":50547,\"start\":50534},{\"end\":50562,\"start\":50547},{\"end\":50570,\"start\":50562},{\"end\":50581,\"start\":50570},{\"end\":50598,\"start\":50581},{\"end\":50608,\"start\":50598},{\"end\":50884,\"start\":50870},{\"end\":50897,\"start\":50884},{\"end\":50913,\"start\":50897},{\"end\":50926,\"start\":50913},{\"end\":51164,\"start\":51146},{\"end\":51181,\"start\":51164},{\"end\":51194,\"start\":51181},{\"end\":51589,\"start\":51578},{\"end\":51607,\"start\":51589},{\"end\":51626,\"start\":51607},{\"end\":51634,\"start\":51626},{\"end\":51837,\"start\":51826},{\"end\":51852,\"start\":51837},{\"end\":51863,\"start\":51852},{\"end\":51874,\"start\":51863},{\"end\":51891,\"start\":51874},{\"end\":52104,\"start\":52088},{\"end\":52122,\"start\":52104},{\"end\":52343,\"start\":52329},{\"end\":52362,\"start\":52343},{\"end\":52376,\"start\":52362},{\"end\":52393,\"start\":52376},{\"end\":52404,\"start\":52393},{\"end\":52630,\"start\":52620},{\"end\":52640,\"start\":52630},{\"end\":52655,\"start\":52640},{\"end\":52666,\"start\":52655},{\"end\":52677,\"start\":52666},{\"end\":52688,\"start\":52677},{\"end\":52698,\"start\":52688},{\"end\":52952,\"start\":52936},{\"end\":52965,\"start\":52952},{\"end\":52978,\"start\":52965},{\"end\":52990,\"start\":52978},{\"end\":53001,\"start\":52990},{\"end\":53018,\"start\":53001},{\"end\":53321,\"start\":53308},{\"end\":53328,\"start\":53321},{\"end\":53339,\"start\":53328},{\"end\":53356,\"start\":53339},{\"end\":53607,\"start\":53594},{\"end\":53614,\"start\":53607},{\"end\":53627,\"start\":53614},{\"end\":53638,\"start\":53627},{\"end\":53649,\"start\":53638},{\"end\":53660,\"start\":53649},{\"end\":53669,\"start\":53660},{\"end\":53686,\"start\":53669},{\"end\":53954,\"start\":53947},{\"end\":53967,\"start\":53954},{\"end\":53975,\"start\":53967},{\"end\":53986,\"start\":53975},{\"end\":54186,\"start\":54174},{\"end\":54198,\"start\":54186},{\"end\":54211,\"start\":54198},{\"end\":54224,\"start\":54211},{\"end\":54243,\"start\":54224},{\"end\":54260,\"start\":54243},{\"end\":54519,\"start\":54506},{\"end\":54530,\"start\":54519},{\"end\":54540,\"start\":54530},{\"end\":54554,\"start\":54540},{\"end\":54566,\"start\":54554},{\"end\":54576,\"start\":54566},{\"end\":54585,\"start\":54576},{\"end\":54872,\"start\":54865},{\"end\":54885,\"start\":54872},{\"end\":54896,\"start\":54885},{\"end\":54909,\"start\":54896},{\"end\":54926,\"start\":54909},{\"end\":55172,\"start\":55164},{\"end\":55189,\"start\":55172},{\"end\":55205,\"start\":55189},{\"end\":55220,\"start\":55205},{\"end\":55237,\"start\":55220},{\"end\":55505,\"start\":55497},{\"end\":55522,\"start\":55505},{\"end\":55539,\"start\":55522},{\"end\":55554,\"start\":55539},{\"end\":55814,\"start\":55803},{\"end\":55828,\"start\":55814},{\"end\":55841,\"start\":55828},{\"end\":55852,\"start\":55841},{\"end\":55863,\"start\":55852},{\"end\":56102,\"start\":56087},{\"end\":56117,\"start\":56102},{\"end\":56133,\"start\":56117},{\"end\":56323,\"start\":56308},{\"end\":56338,\"start\":56323},{\"end\":56354,\"start\":56338},{\"end\":56369,\"start\":56354},{\"end\":56382,\"start\":56369},{\"end\":56649,\"start\":56636},{\"end\":56661,\"start\":56649},{\"end\":56669,\"start\":56661},{\"end\":56682,\"start\":56669},{\"end\":56696,\"start\":56682},{\"end\":56704,\"start\":56696},{\"end\":56961,\"start\":56948},{\"end\":56974,\"start\":56961},{\"end\":56985,\"start\":56974},{\"end\":56998,\"start\":56985},{\"end\":57285,\"start\":57272},{\"end\":57297,\"start\":57285},{\"end\":57308,\"start\":57297},{\"end\":57320,\"start\":57308},{\"end\":57518,\"start\":57505},{\"end\":57529,\"start\":57518},{\"end\":57546,\"start\":57529},{\"end\":57559,\"start\":57546}]", "bib_venue": "[{\"end\":34703,\"start\":34693},{\"end\":34991,\"start\":34983},{\"end\":35223,\"start\":35157},{\"end\":35636,\"start\":35632},{\"end\":35976,\"start\":35972},{\"end\":36207,\"start\":36133},{\"end\":36650,\"start\":36646},{\"end\":36915,\"start\":36908},{\"end\":37173,\"start\":37169},{\"end\":37457,\"start\":37453},{\"end\":37721,\"start\":37716},{\"end\":37980,\"start\":37976},{\"end\":38251,\"start\":38247},{\"end\":38531,\"start\":38527},{\"end\":38799,\"start\":38795},{\"end\":39097,\"start\":39093},{\"end\":39427,\"start\":39423},{\"end\":39758,\"start\":39751},{\"end\":40030,\"start\":40026},{\"end\":40295,\"start\":40291},{\"end\":40569,\"start\":40565},{\"end\":40872,\"start\":40868},{\"end\":41198,\"start\":41188},{\"end\":41422,\"start\":41418},{\"end\":41620,\"start\":41616},{\"end\":41873,\"start\":41869},{\"end\":42206,\"start\":42199},{\"end\":42487,\"start\":42405},{\"end\":42962,\"start\":42865},{\"end\":43430,\"start\":43426},{\"end\":43742,\"start\":43738},{\"end\":44066,\"start\":44032},{\"end\":44352,\"start\":44348},{\"end\":44633,\"start\":44626},{\"end\":44905,\"start\":44901},{\"end\":45191,\"start\":45187},{\"end\":45490,\"start\":45486},{\"end\":45713,\"start\":45709},{\"end\":45966,\"start\":45962},{\"end\":46275,\"start\":46271},{\"end\":46673,\"start\":46669},{\"end\":47048,\"start\":47044},{\"end\":47337,\"start\":47328},{\"end\":47686,\"start\":47682},{\"end\":48055,\"start\":48051},{\"end\":48385,\"start\":48381},{\"end\":48623,\"start\":48619},{\"end\":48863,\"start\":48858},{\"end\":49150,\"start\":49140},{\"end\":49422,\"start\":49418},{\"end\":49671,\"start\":49667},{\"end\":49953,\"start\":49949},{\"end\":50233,\"start\":50151},{\"end\":50618,\"start\":50608},{\"end\":50930,\"start\":50926},{\"end\":51280,\"start\":51194},{\"end\":51638,\"start\":51634},{\"end\":51895,\"start\":51891},{\"end\":52126,\"start\":52122},{\"end\":52409,\"start\":52404},{\"end\":52711,\"start\":52698},{\"end\":53022,\"start\":53018},{\"end\":53360,\"start\":53356},{\"end\":53691,\"start\":53686},{\"end\":53993,\"start\":53986},{\"end\":54264,\"start\":54260},{\"end\":54599,\"start\":54585},{\"end\":54863,\"start\":54796},{\"end\":55241,\"start\":55237},{\"end\":55561,\"start\":55554},{\"end\":55871,\"start\":55863},{\"end\":56137,\"start\":56133},{\"end\":56389,\"start\":56382},{\"end\":56708,\"start\":56704},{\"end\":57035,\"start\":56998},{\"end\":57330,\"start\":57320},{\"end\":57563,\"start\":57559}]"}}}, "year": 2023, "month": 12, "day": 17}