{"id": 7958862, "updated": "2023-07-19 16:11:43.566", "metadata": {"title": "Jointly Embedding Knowledge Graphs and Logical Rules", "authors": "[{\"first\":\"Shu\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Quan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Lihong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Guo\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": ",", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2563063592", "acl": "D16-1019", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/GuoWWWG16", "doi": "10.18653/v1/d16-1019"}}, "content": {"source": {"pdf_hash": "2cefe5adb11295b830ce27176c6d84b66fb20c2c", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/D16-1019.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/D16-1019.pdf", "status": "HYBRID"}}, "grobid": {"id": "0688d07d006099e1f4a8ec6c8063d01c4c87ac09", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2cefe5adb11295b830ce27176c6d84b66fb20c2c.txt", "contents": "\nJointly Embedding Knowledge Graphs and Logical Rules\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsNovember 1-5, 2016. 2016\n\nShu Guo guoshu@iie.ac.cn \nInstitute of Information Engineering\nChinese Academy of Sciences\n100093BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nQuan Wang wangquan@iie.ac.cn \nInstitute of Information Engineering\nChinese Academy of Sciences\n100093BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nLihong Wang \nNational Computer Network Emergency Response Technical Team Coordination Center of China\n100029BeijingChina\n\nBin Wang wangbin@iie.ac.cn \nInstitute of Information Engineering\nChinese Academy of Sciences\n100093BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nLi Guo guoli@iie.ac.cn \nInstitute of Information Engineering\nChinese Academy of Sciences\n100093BeijingChina\n\nUniversity of Chinese Academy of Sciences\n100049BeijingChina\n\nJointly Embedding Knowledge Graphs and Logical Rules\n\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing\nthe 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsNovember 1-5, 2016. 2016\nEmbedding knowledge graphs into continuous vector spaces has recently attracted increasing interest. Most existing methods perform the embedding task using only fact triples. Logical rules, although containing rich background information, have not been well studied in this task. This paper proposes a novel method of jointly embedding knowledge graphs and logical rules. The key idea is to represent and model triples and rules in a unified framework. Specifically, triples are represented as atomic formulae and modeled by the translation assumption, while rules represented as complex formulae and modeled by t-norm fuzzy logics. Embedding then amounts to minimizing a global loss over both atomic and complex formulae. In this manner, we learn embeddings compatible not only with triples but also with rules, which will certainly be more predictive for knowledge acquisition and inference. We evaluate our method with link prediction and triple classification tasks. Experimental results show that joint embedding brings significant and consistent improvements over stateof-the-art methods. Particularly, it enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of our method to learn more predictive embeddings.\n\nIntroduction\n\nKnowledge graphs (KGs) provide rich structured information and have become extremely useful resources for many NLP related applications like * Corresponding author: Quan Wang. word sense disambiguation (Wasserman-Pritsker et al., 2015) and information extraction (Hoffmann et al., 2011). A typical KG represents knowledge as multi-relational data, stored in triples of the form (head entity, relation, tail entity), e.g., (Paris, Capital-Of, France). Although powerful in representing structured data, the symbolic nature of such triples makes KGs, especially large-scale KGs, hard to manipulate.\n\nRecently, a promising approach, namely knowledge graph embedding, has been proposed and successfully applied to various KGs (Nickel et al., 2012;Socher et al., 2013;Bordes et al., 2014). The key idea is to embed components of a KG including entities and relations into a continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the KG. The embeddings contain rich semantic information about entities and relations, and can significantly enhance knowledge acquisition and inference .\n\nMost existing methods perform the embedding task based solely on fact triples Wang et al., 2014;Nickel et al., 2016). The only requirement is that the learned embeddings should be compatible with those facts. While logical rules contain rich background information and are extremely useful for knowledge acquisition and inference (Jiang et al., 2012;Pujara et al., 2013), they have not been well studied in this task.  and Wei et al. (2015) tried to leverage both embedding methods and logical rules for KG completion. In their work, however, rules are modeled separately from embedding methods, serving as postprocessing steps, and thus will not help to obtain better embeddings. Rockt\u00e4schel et al. (2015) recently proposed a joint model which injects first-order logic into embeddings. But it focuses on the relation extraction task, and creates vector embeddings for entity pairs rather than individual entities. Since entities do not have their own embeddings, relations between unpaired entities cannot be effectively discovered (Chang et al., 2014).\n\nIn this paper we introduce KALE, a new approach that learns entity and relation Embeddings by jointly modeling Knowledge And Logic. Knowledge triples are taken as atoms and modeled by the translation assumption, i.e., relations act as translations between head and tail entities . A triple (e i , r k , e j ) is scored by \u2225e i + r k \u2212 e j \u2225 1 , where e i , r k , and e j are the vector embeddings for entities and relations. The score is then mapped to the unit interval [0, 1] to indicate the truth value of that triple. Logical rules are taken as complex formulae constructed by combining atoms with logical connectives (e.g., \u2227 and \u21d2), and modeled by t-norm fuzzy logics (H\u00e1jek, 1998). The truth value of a rule is a composition of the truth values of the constituent atoms, defined by specific logical connectives. In this way, KALE represents triples and rules in a unified framework, as atomic and complex formulae respectively. Figure 1 gives a simple illustration of the framework. After unifying triples and rules, KALE minimizes a global loss involving both of them to obtain entity and relation embeddings. The learned embeddings are therefore compatible not only with triples but also with rules, which will definitely be more predictive for knowledge acquisition and inference.\n\nThe main contributions of this paper are summarized as follows. (i) We devise a unified framework that jointly models triples and rules to obtain more predictive entity and relation embeddings. The new framework KALE is general enough to handle any type of rules that can be represented as first-order logic formulae. (ii) We evaluate KALE with link prediction and triple classification tasks on WordNet (Miller, 1995) and Freebase (Bollacker et al., 2008). Experimental results show significant and consistent improvements over state-of-the-art methods. Particularly, joint embedding enhances the prediction of new facts which cannot even be directly inferred by pure logical inference, demonstrating the capability of KALE to learn more predictive embeddings.\n\n\nRelated Work\n\nRecent years have seen rapid growth in KG embedding methods. Given a KG, such methods aim to encode its entities and relations into a continuous vector space, by using neural network architectures (Socher et al., 2013;Bordes et al., 2014), matrix/tensor factorization techniques (Nickel et al., 2011;Riedel et al., 2013;Chang et al., 2014), or Bayesian clustering strategies (Kemp et al., 2006;Xu et al., 2006;Sutskever et al., 2009). Among these methods, TransE , which models relations as translating operations, achieves a good trade-off between prediction accuracy and computational efficiency. Various extensions like TransH (Wang et al., 2014) and Tran-sR (Lin et al., 2015b) are later proposed to further enhance the prediction accuracy of TransE. Most existing methods perform the embedding task based solely on triples contained in a KG. Some recent work tries to further incorporate other types of information available, e.g., relation paths (Neelakantan et al., 2015;Lin et al., 2015a;Luo et al., 2015), relation type-constraints (Krompa\u00dfet al., 2015), entity types , and entity descriptions (Zhong et al., 2015), to learn better embeddings.\n\nLogical rules have been widely studied in knowledge acquisition and inference, usually on the basis of Markov logic networks (Richardson and Domingos, 2006;Br\u00f6cheler et al., 2010;Pujara et al., 2013;Beltagy and Mooney, 2014). Recently, there has been growing interest in combining logical rules and embedding models.  and Wei et al. (2015) tried to utilize rules to refine predictions made by embedding models, via integer linear programming or Markov logic networks. In their work, however, rules are modeled separately from embedding models, and will not help obtain better embeddings. Rockt\u00e4schel et al. (2015) proposed a joint model that injects first-order logic into embeddings. But their work focuses on relation extraction, creating vector embeddings for entity pairs, and hence fails to discover relations between unpaired entities. This paper, in contrast, aims at learning more predictive embeddings by jointly modeling knowledge and logic. Since each entity has its own embedding, our approach can successfully make predictions between unpaired entities, providing greater flexibility for knowledge acquisition and inference.\n\n\nJointly Embedding Knowledge and Logic\n\nWe first describe the formulation of joint embedding. We are given a KG containing a set of triples K = {(e i , r k , e j )}, with each triple composed of two entities e i , e j \u2208 E and their relation r k \u2208 R.\n\nHere E is the entity vocabulary and R the relation set. Besides the triples, we are given a set of logical rules L, either specified manually or extracted automatically. A logical rule is encoded, for example, in the form of \u2200x, y : (x, r s , y) \u21d2 (x, r t , y), stating that any two entities linked by relation r s should also be linked by relation r t . Entities and relations are associated with vector embeddings, denoted by e, r \u2208 R d , representing their latent semantics. The proposed method, KALE, aims to learn these embeddings by jointly modeling knowledge triples K and logical rules L.\n\n\nOverview\n\nTo enable joint embedding, a key ingredient of KALE is to unify triples and rules, in terms of firstorder logic (Rockt\u00e4schel et al., 2014;Rockt\u00e4schel et al., 2015). A triple (e i , r k , e j ) is taken as a ground atom which applies a relation r k to a pair of entities e i and e j . Given a logical rule, it is first instantiated with concrete entities in the vocabulary E, resulting in a set of ground rules. For example, a universally quantified rule \u2200x, y : (x, Capital-Of, y) \u21d2 (x, Located-In, y) might be instantiated with the concrete entities of Paris and France, giving the ground rule (Paris, Capital-Of, France) \u21d2 (Paris, Located-In, France). 1 A ground rule can then be interpreted as a complex formula, constructed by combining ground atoms with logical connectives (e.g. \u2227 and \u21d2).\n\nLet F denote the set of training formulae, both atomic (triples) and complex (ground rules). KALE further employs a truth function I : F \u2192 [0, 1] to assign a soft truth value to each formula, indicating how likely a triple holds or to what degree a ground rule is satisfied. The truth value of a triple is determined by the corresponding entity and relation embeddings. The truth value of a ground rule is determined by the truth values of the constituent triples, via specific logical connectives. In this way, KALE models triples and rules in a unified framework. See Figure 1 for an overview. Finally, KALE minimizes a global loss over the training formulae F to learn entity and relation embeddings compatible with both triples and rules. In what follows, we describe the key components of KALE, including triple modeling, rule modeling, and joint learning.\n\n\nTriple Modeling\n\nTo model triples we follow TransE , as it is simple and efficient while achieving state-of-the-art predictive performance. Specifically, given a triple (e i , r k , e j ), we model the relation embedding r k as a translation between the entity embeddings e i and e j , i.e., we want e i + r k \u2248 e j when the triple holds. The intuition here originates from linguistic regularities such as France \u2212 Paris = Germany \u2212 Berlin (Mikolov et al., 2013). In relational data, such analogy holds because of the certain relation Capital-Of, through which we will get Paris + Capital-Of = France and Berlin + Capital-Of = Germany. Then, we score each triple on the basis of \u2225e i + r k \u2212 e j \u2225 1 , and define its soft truth value as\nI (e i , r k , e j ) = 1 \u2212 1 3 \u221a d \u2225e i + r k \u2212 e j \u2225 1 ,(1)\nwhere d is the dimension of the embedding space. It is easy to see that I (e i , r k , e j ) \u2208 [0, 1] with the constraints \u2225e i \u2225 2 \u2264 1, \u2225e j \u2225 2 \u2264 1, and \u2225r k \u2225 2 \u2264 1. 2 I (e i , r k , e j ) is expected to be large if the triple holds, and small otherwise.\n\n\nRule Modeling\n\nTo model rules we use t-norm fuzzy logics (H\u00e1jek, 1998), which define the truth value of a complex formula as a composition of the truth values of its constituents, through specific t-norm based logical connectives. We follow Rockt\u00e4schel et al. (2015) and use the product t-norm. The compositions associated with logical conjunction (\u2227), disjunction (\u2228), and negation (\u00ac) are defined as follow:\nI(f 1 \u2227 f 2 ) = I(f 1 )\u00b7I(f 2 )\n,\nI(f 1 \u2228 f 2 ) = I(f 1 ) + I(f 2 ) \u2212 I(f 1 )\u00b7I(f 2 ), I(\u00acf 1 ) = 1 \u2212 I(f 1 ),\nwhere f 1 and f 2 are two constituent formulae, either atomic or complex. Given these compositions, the truth value of any complex formula can be calculated recursively, e.g.,\nI(\u00acf 1 \u2227 f 2 ) = I(f 2 ) \u2212 I(f 1 )\u00b7I(f 2 )\n,\nI(f 1 \u21d2 f 2 ) = I(f 1 )\u00b7I(f 2 ) \u2212 I(f 1 ) + 1.\nThis paper considers two types of rules. The first type is \u2200x, y : (x, r s , y) \u21d2 (x, r t , y). Given a ground rule f (e m , r s , e n ) \u21d2 (e m , r t , e n ), the truth value is calculated as:\nI(f )=I(e m , r s , e n )\u00b7I(e m , r t , e n ) \u2212I(e m , r s , e n ) + 1,(2)\nwhere I(\u00b7,\u00b7,\u00b7) is the truth value of a constituent triple, defined by Eq. (1). The second type is \u2200x, y, z : (x, r s 1 , y) \u2227 (y, r s 2 , z) \u21d2 (x, r t , z). Given a ground rule f (e \u2113 , r s 1 , e m ) \u2227 (e m , r s 2 , e n ) \u21d2 (e \u2113 , r t , e n ), the truth value is:\nI(f )=I(e \u2113 , r s 1 , e m )\u00b7I(e m , r s 2 , e n )\u00b7I(e \u2113 , r t , e n ) \u2212I(e \u2113 , r s 1 , e m )\u00b7I(e m , r s 2 , e n ) + 1.(3)\nThe larger the truth values are, the better the ground rules are satisfied. It is easy to see that besides these two types of rules, the KALE framework is general enough to handle any rules that can be represented as first-order logic formulae. The investigation of other types of rules will be left for future work.\n2 Note that 0 \u2264 \u2225ei + r k \u2212 ej\u2225 1 \u2264 \u2225ei\u2225 1 + \u2225r k \u2225 1 + \u2225ej\u2225 1 \u2264 3 \u221a d, where the last inequality holds because \u2225x\u2225 1 = \u2211 i |xi| \u2264 \u221a d \u2211 i x 2 i = \u221a d \u2225x\u2225 2\nfor any x \u2208 R d , according to the Cauchy-Schwarz inequality.\n\n\nJoint Learning\n\nAfter unifying triples and rules as atomic and complex formulae, we minimize a global loss over this general representation to learn entity and relation embeddings. We first construct a training set F containing all positive formulae, including (i) observed triples, and (ii) ground rules in which at least one constituent triple is observed. Then we minimize a margin-based ranking loss, enforcing positive formulae to have larger truth values than negative ones:\nmin {e},{r} \u2211 f + \u2208F \u2211 f \u2212 \u2208N f + [ \u03b3 \u2212 I(f + ) + I(f \u2212 ) ] + , s.t. \u2225e\u2225 2 \u2264 1, \u2200e \u2208 E; \u2225r\u2225 2 \u2264 1, \u2200r \u2208 R. (4)\nHere f + \u2208 F is a positive formula, f \u2212 \u2208 N f + a negative one constructed for f + , \u03b3 a margin separating positive and negative formulae, and\n[x] + max{0, x}. If f + (e i , r k , e j )\nis a triple, we construct f \u2212 by replacing either e i or e j with a random entity e \u2208 E, and calculate its truth value according to Eq. (1). For example, we might generate a negative instance (Paris, Capital-Of, Germany) for the triple (Paris, Capital-Of, France). If f + (e m , r s , e n ) \u21d2 (e m , r t , e n ) or (e \u2113 , r s 1 , e m ) \u2227 (e m , r s 2 , e n ) \u21d2 (e \u2113 , r t , e n ) is a ground rule, we construct f \u2212 by replacing r t in the consequent with a random relation r \u2208 R, and calculate its truth value according to Eq. (2) or Eq. (3). For example, given a ground rule (Paris, Capital-Of, France) \u21d2 (Paris, Located-In, France), a possible negative instance (Paris, Capital-Of, France)\u21d2 (Paris, Has-Spouse, France) could be generated. We believe that most instances (both triples and ground rules) generated in this way are truly negative. Stochastic gradient descent in mini-batch mode is used to carry out the minimization. To satisfy the \u2113 2 -constraints, e and r are projected to the unit \u2113 2 -ball before each mini-batch. Embeddings learned in this way are required to be compatible with not only triples but also rules.\n\n\nDiscussions\n\nComplexity. We compare KALE with several stateof-the-art embedding methods in space complexity and time complexity (per iteration) during learning. of the embedding space, and n e /n r /n t /n g is the number of entities/relations/triples/ground rules. The results indicate that incorporating additional rules will not significantly increase the space or time complexity of KALE, keeping the model complexity almost the same as that of TransE (optimal among the methods listed in the table). But please note that KALE needs to ground universally quantified rules before learning, which further requires O(n u n t /n r ) in time complexity. Here, n u is the number of universally quantified rules, and n t /n r is the averaged number of observed triples per relation. During grounding, we select those ground rules with at least one triple observed. Grounding is required only once before learning, and is not included during the iterations.\n\nExtensions. Actually, our approach is quite general.\n\n(i) Besides TransE, a variety of embedding methods, e.g., those listed in Table 1, can be used for triple modeling (Section 3.2), as long as we further define a mapping f : R \u2192 [0, 1] to map original scores to soft truth values. (ii) Besides the two types of rules introduced in Section 3.3, other types of rules can also be handled as long as they can be represented as first-order logic formulae. (iii) Besides the product t-norm, other types of t-norm based fuzzy logics can be used for rule modeling (Section 3.3), e.g., the \u0141ukasiewicz t-norm used in probabilistic soft logic (Br\u00f6cheler et al., 2010) and the minimum t-norm used in fuzzy description logic (Stoilos et al., 2007).\n\n(iv) Besides the pairwise ranking loss, other types of loss functions can be designed for joint learning (Section 3.4), e.g., the pointwise squared loss or the logarithmic loss (Rockt\u00e4schel et al., 2014;Rockt\u00e4schel et al., 2015).\n\n\nExperiments\n\nWe empirically evaluate KALE with two tasks: (i) link prediction and (ii) triple classification.  We further create logical rules for each dataset, in the form of \u2200x, y : (x, r s , y) \u21d2 (x, r t , y) or \u2200x, y, z : (x, r s 1 , y) \u2227 (y, r s 2 , z) \u21d2 (x, r t , z). To do so, we first run TransE to get entity and relation embeddings, and calculate the truth value for each of such rules according to Eq. (2) or Eq. (3). Then we rank all such rules by their truth values and manually filter those ranked at the top. We finally create 47 rules on FB122, and 14 on WN18 (see Table 2 for examples). The rules are then instantiated with concrete entities (grounding). Ground rules in which at least one constituent triple is observed in the training set are used in joint learning.\n\nNote that some of the test triples can be inferred by directly applying these rules on the training set (pure logical inference). On each dataset, we further split the test set into two parts, test-I and test-II. The former contains triples that cannot be directly inferred by pure logical inference, and the latter the remaining test triples. Table 3 gives some statistics of the datasets, including the number of entities, relations, triples in training/validation/test-I/test-II set, and ground rules.\n\nComparison settings. As baselines we take the embedding techniques of TransE, TransH, and Tran-sR. TransE models relation embeddings as translation operations between entity embeddings. TransH \u2200x, y : /sports/athlete/team(x, y) \u21d2 /sports/sports team/player(y, x) \u2200x, y : /location/country/capital(x, y) \u21d2 /location/location/contains(x, y) \u2200x, y, z : /people/person/nationality(x, y) \u2227 /location/country/official language(y, z) \u21d2 /people/person/languages(x, z) \u2200x, y, z : /country/administrative divisions(x, y) \u2227 /administrative division/capital(y, z) \u21d2 /country/second level divisions(x, z) \u2200x, y : hypernym(x, y) \u21d2 hyponym(y, x) \u2200x, y : instance hypernym(x, y) \u21d2 instance hyponym(y, x) \u2200x, y : synset domain topic of(x, y) \u21d2 member of domain topic(y, x) and TransR are extensions of TransE. They further allow entities to have distinct embeddings when involved in different relations, by introducing relationspecific hyperplanes and projection matrices respectively. All the three methods have been demonstrated to perform well on WordNet and Freebase data.\n\nWe further test our approach in three different scenarios. (i) KALE-Trip uses triples alone to perform the embedding task, i.e., only the training triples are included in the optimization Eq. (4). It is a linearly transformed version of TransE. The only difference is that relation embeddings are normalized in KALE-Trip, but not in TransE. (ii) KALE-Pre first repeats pure logical inference on the training set and adds inferred triples as additional training data, until no further triples can be inferred. Both original and inferred triples are then included in the optimization. For example, given a logical rule \u2200x, y : (x, r s , y) \u21d2 (x, r t , y), a new triple (e i , r t , e j ) can be inferred if (e i , r s , e j ) is observed in the training set, and both triples will be used as training instances for embedding. (iii) KALE-Joint is the joint learning scenario, which considers both training triples and ground rules in the optimization. In the aforementioned example, training triple (e i , r s , e j ) and ground rule (e i , r s , e j ) \u21d2 (e i , r t , e j ) will be used in the training process of KALE-Joint, without explicitly incorporating triple (e i , r t , e j ). Among the methods, TransE/TransH/TransR and KALE-Trip use only triples, while KALE-Pre/KALE-Joint further incorporates rules, before or during embedding. Implementation details. We use the code provided by  for TransE 4 , and the code provided by Lin et al. (2015b) for TransH and Tran-sR 5 . KALE is implemented in Java. Note that Lin et al. (2015b) initialized TransR with the results of 4 https://github.com/glorotxa/SME 5 https://github.com/mrlyk423/relation extraction TransE. However, to ensure fair comparison, we randomly initialize all the methods in our experiments. For all the methods, we create 100 mini-batches on each dataset, and tune the embedding dimension d in {20, 50, 100}. For TransE, TransH, and Tran-sR which score a triple by a distance in R + , we tune the learning rate \u03b7 in {0.001, 0.01, 0.1}, and the margin \u03b3 in {1, 2, 3, 4}. For KALE which scores a triple (as well as a ground rule) by a soft truth value in the unit interval [0, 1], we set the learning rate \u03b7 in {0.01, 0.02, 0.05, 0.1}, and the margin \u03b3 in {0.1, 0.12, 0.15, 0.2}. KALE allows triples and rules to have different weights, with the former fixed to 1, and the latter (denoted by \u03bb) selected in {0.001, 0.01, 0.1, 1}.\n\n\nLink Prediction\n\nThis task is to complete a triple (e i , r k , e j ) with e i or e j missing, i.e., predict e i given (r k , e j ) or predict e j given (e i , r k ). Evaluation protocol. We follow the same evaluation protocol used in TransE . For each test triple (e i , r k , e j ), we replace the head entity e i by every entity e \u2032 i in the dictionary, and calculate the truth value (or distance) for the corrupted triple (e \u2032 i , r k , e j ). Ranking the truth values in descending order (or the distances in ascending order), we get the rank of the correct entity e i . Similarly, we can get another rank by corrupting the tail entity e j . Aggregated over all the test triples, we report three metrics: (i) the mean reciprocal rank (MRR), (ii) the median of the ranks (MED), and (iii) the proportion of ranks no larger than n (HITS@N). We do not report the averaged rank (i.e., the \"Mean Rank\" metric used by ), since it is usually sensitive to outliers (Nickel et al., 2016).\n\nNote that a corrupted triple may exist in KGs, which should also be taken as a valid triple. Consider a test triple (Paris, Located-In, France)    and a possible corruption (Lyon, Located-In, France). Both triples are valid. In this case, ranking Lyon before the correct answer Paris should not be counted as an error. To avoid such phenomena, we follow  and remove those corrupted triples which exist in either the training, validation, or test set before getting the ranks. That means, we remove Lyon from the candidate list before getting the rank of Paris in the aforementioned example. We call the original setting \"raw\" and the new setting \"filtered\".\n\nOptimal configurations. For each of the methods to be compared, we tune its hyperparameters in the ranges specified in Section 4.1, and select a best model that leads to the highest filtered MRR score on the validation set (with a total of 500 epochs over the training data). The optimal configurations for KALE are: d = 100, \u03b7 = 0.05, \u03b3 = 0.12, and \u03bb = 1 on FB122; d = 50, \u03b7 = 0.05, \u03b3 = 0.2, and \u03bb = 0.1 on WN18. To better see and understand the effects of rules, we use the same configuration for KALE-Trip, KALE-Pre, and KALE-Joint on each dataset.\n\nResults. Table 4 and To better understand how the joint embedding scenario can learn more predictive embeddings, on each dataset we further split the test-I set into two parts. Given a triple (e i , r k , e j ) in the test-I set, we assign it to the first part if relation r k is covered by the rules, and the second part otherwise. We call the two parts Test-Incl and Test-Excl respectively. Table 6 compares the performance of KALE-Trip and KALE-Joint on the two parts. The results show that KALE-Joint outperforms KALE-Trip on both parts, but the improvements on Test-Incl are much more significant than those on Test-Excl. Take the filtered setting on WN18 as an example. On Test-Incl, KALE-Joint increases the metric MRR by 55.7%, decreases the metric MED by 26.9%, and increas-es the metric HITS@10 by 38.2%. On Test-Excl, however, MRR rises by 3.1%, MED remains the same, and HITS@10 rises by only 0.3%. This observation indicates that jointly embedding triples and rules helps to learn more predictive embeddings, especially for those relations that are used to construct the rules. This might be the main reason that KALE-Joint can make better predictions even beyond the scope of pure logical inference.\n\n\nTriple Classification\n\nThis task is to verify whether an unobserved triple (e i , r k , e j ) is correct or not. Evaluation protocol. We take the following evaluation protocol similar to that used in TransH (Wang et al., 2014). We first create labeled data for evaluation. For each triple in the test or validation set (i.e., a positive triple), we construct 10 negative triples for it by randomly corrupting the entities, 5 at the head position and the other 5 at the tail position. 6 To make the negative triples as difficult as possible, we corrupt a position using only entities that have appeared in that position, and further ensure that the corrupted triples do not exist in either the training, validation, or test set. We simply use the truth values (or distances) to classify triples. Triples with large truth values (or small distances) tend to be predicted as positive. To evaluate, we first rank the triples associated with each specific relation (in descending order according to their truth values, or in ascending order according to the distances), and calculate the average precision for that relation. We then report on the test sets the mean average precision (MAP)  aggregated over different relations.\n\nOptimal configurations. The hyperparameters of each method are again tuned in the ranges specified in Section 4.1, and the best models are selected by maximizing MAP on the validation set. The optimal configurations for KALE are: d = 100, \u03b7 = 0.1, \u03b3 = 0.2, and \u03bb = 0.1 on FB122; d = 100, \u03b7 = 0.1, \u03b3 = 0.2, and \u03bb = 0.001 on WN18. Again, we use the same configuration for KALE-Trip, KALE-Pre, and KALE-Joint on each dataset.\n\nResults. Table 7 shows the results on the test-I, test-II, and test-all sets of our datasets. From the results, we can see that: (i) KALE-Pre and KALE-Joint outperform the other methods which use triples alone on almost all the test sets, demonstrating the superiority of incorporating logical rules. (ii) KALE-Joint performs better than KALE-Pre on the test-I sets, i.e., triples that cannot be directly inferred by performing pure logical inference on the training set. This observation is similar to that observed in the link prediction task, demonstrating that the joint embedding scenario can learn more predictive embeddings and make predictions beyond the capability of pure logical inference.\n\n\nConclusion and Future Work\n\nIn this paper, we propose a new method for jointly embedding knowledge graphs and logical rules, referred to as KALE. The key idea is to represent and model triples and rules in a unified framework. Specifically, triples are represented as atomic formulae and modeled by the translation assumption, while rules as complex formulae and by the t-norm fuzzy logics. A global loss on both atomic and complex formulae is then minimized to perform the embedding task. Embeddings learned in this way are compatible not only with triples but also with rules, which are certainly more useful for knowledge acquisition and inference. We evaluate KALE with the link prediction and triple classification tasks on WordNet and Freebase data. Experimental results show that joint embedding brings significant and consistent improvements over state-of-the-art methods. More importantly, it can obtain more predictive embeddings and make better predictions even beyond the scope of pure logical inference.\n\nFor future work, we would like to (i) Investigate the efficacy of incorporating other types of logical rules such as \u2200x, y, z : (x, Capital-Of, y) \u21d2 \u00ac(x, Capital-Of, z). (ii) Investigate the possibility of modeling logical rules using only relation embeddings as suggested by Demeester et al. (2016), e.g., modeling the above rule using only the embedding associated with Capital-Of. This avoids grounding, which might be time and space inefficient especially for complicated rules. (iii) Investigate the use of automatically extracted rules which are no longer hard rules and tolerant of uncertainty.\n\nFigure 1 :\n1Simple illustration of KALE.\n\nTable 1\n1shows the results, where d is the dimensionMethod \n\nComplexity (Space/Time) \n\nSE (Bordes et al., 2011) \nned+2nrd 2 \nO(ntd 2 ) \nLFM (Jenatton et al., 2012) ned+nrd 2 \nO(ntd 2 ) \nTransE (Bordes et al., 2013) ned+nrd \nO(ntd) \nTransH (Wang et al., 2014) ned+2nrd \nO(ntd) \nTransR (Lin et al., 2015b) \nned+nr(d 2 +d) O(ntd 2 ) \n\nKALE (this paper) \nned+nrd \nO(ntd+ngd) \n\nTable 1: Complexity of different embedding methods. \n\n\n\nTable 3 :\n3Statistics of datasets.4.1 Experimental Setup \n\nDatasets. We use two datasets: WN18 and FB122. \nWN18 is a subgraph of WordNet containing 18 rela-\ntions. FB122 is composed of 122 Freebase relations \nregarding the topics of \"people\", \"location\", and \"s-\nports\", extracted from FB15K. Both WN18 and F-\nB15K are released by Bordes et al. (2013) 3 . Triples \non each dataset are split into training/validation/test \nsets, used for model training, parameter tuning, and \nevaluation respectively. For WN18 we use the o-\nriginal data split, and for FB122 we extract triples \nassociated with the 122 relations from the training, \nvalidation, and test sets of FB15K. \n\n\nTable 2 :\n2Examples of rules created.\n\nTable 4 :\n4Linkprediction results on the test-I, test-II, and test-all sets of FB122 and WN18 (raw setting). \n\nTest-I \nTest-II \nTest-ALL \n\nMRR MED \nHITS@N (%) \nMRR MED \nHITS@N (%) \nMRR MED \nHITS@N (%) \n\n\n\nTable 5 :\n5Linkprediction results on the test-I, test-II, and test-all sets of FB122 and WN18 (filtered setting). \n\n\n\nTable 5\n5II sets which contain directly inferable triples, KALE-Pre can easily beat all the baselines (even KALE-Joint). That means, for triples covered by pure logical inference, it is trivial to improve the performance by directly incorporating them as training instances.show the results in the \n\n\nFB122 WN18\nFB122MAP (Test-I/II/ALL) MAP (Test-I/II/ALL) KALE-Joint 0.599 0.870 0.677 0.627 0.997 0.961TransE \n0.552 0.852 0.634 0.592 0.993 0.958 \nTransH \n0.576 0.758 0.641 0.604 0.978 0.947 \nTransR \n0.572 0.699 0.619 0.412 0.854 0.836 \nKALE-Trip 0.578 0.829 0.652 0.618 0.995 0.953 \nKALE-Pre \n0.575 0.916 0.668 0.620 0.997 0.964 \n\n\nTable 7 :\n7Triple classification results on the test-I, test-II, andtest-all sets of FB122 and WN18.\nOur approach actually takes as input rules represented in first-order logic, i.e., those with quantifiers such as \u2200. But it could be hard to deal with quantifiers, so we use ground rules, i.e., propositional statements during learning.\nhttps://everest.hds.utc.fr/doku.php?id=en:smemlj12\nPrevious work typically constructs only a single negative case for each positive one. We empirically found such a balanced classification task too simple for our datasets. So we consider a highly unbalanced setting, with a positive-to-negative ratio of 1:10, for which the previously used metric accuracy is no longer suitable.\nAcknowledgmentsWe would like to thank the anonymous reviewers for their insightful comments and suggestions. This research is supported by the National Natural Science Foundation of China (grant No. 61402465) and the Strategic Priority Research Program of the Chinese Academy of Sciences (grant No. XDA06030200).\nEfficient markov logic inference for natural language semantics. Islam Beltagy, Raymond J Mooney, Proceedings of the 28th AAAI Conference on Artificial Intelligence -Workshop on Statistical Relational Artificial Intelligence. the 28th AAAI Conference on Artificial Intelligence -Workshop on Statistical Relational Artificial IntelligenceIslam Beltagy and Raymond J. Mooney. 2014. Efficient markov logic inference for natural language semantics. In Proceedings of the 28th AAAI Conference on Arti- ficial Intelligence -Workshop on Statistical Relational Artificial Intelligence, pages 9-14.\n\nFreebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data. the 2008 ACM SIGMOD International Conference on Management of DataKurt Bollacker, Colin Evans, Praveen Paritosh, Tim S- turge, and Jamie Taylor. 2008. Freebase: a collab- oratively created graph database for structuring hu- man knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pages 1247-1250.\n\nLearning structured embeddings of knowledge bases. Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, Proceedings of the 25th AAAI Conference on Artificial Intelligence. the 25th AAAI Conference on Artificial IntelligenceAntoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. 2011. Learning structured embed- dings of knowledge bases. In Proceedings of the 25th AAAI Conference on Artificial Intelligence, pages 301-306.\n\nTranslating embeddings for modeling multi-relational data. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Dur\u00e1n, Jason Weston, Oksana Yakhnenko, Proceedings of the 27th Annual Conference on Neural Information Processing Systems. the 27th Annual Conference on Neural Information Processing SystemsAntoine Bordes, Nicolas Usunier, Alberto Garcia-Dur\u00e1n, Jason Weston, and Oksana Yakhnenko. 2013. Trans- lating embeddings for modeling multi-relational da- ta. In Proceedings of the 27th Annual Conference on Neural Information Processing Systems, pages 2787- 2795.\n\nA semantic matching energy function for learning with multi-relational data. Machine Learning. Antoine Bordes, Xavier Glorot, Jason Weston, Yoshua Bengio, 94Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2014. A semantic matching energy function for learning with multi-relational data. Ma- chine Learning, 94(2):233-259.\n\nProbabilistic similarity logic. Matthias Br\u00f6cheler, Lilyana Mihalkova, Lise Getoor, Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence. the 26th Conference on Uncertainty in Artificial IntelligenceMatthias Br\u00f6cheler, Lilyana Mihalkova, and Lise Getoor. 2010. Probabilistic similarity logic. In Proceedings of the 26th Conference on Uncertainty in Artificial Intel- ligence, pages 73-82.\n\nTyped tensor decomposition of knowledge bases for relation extraction. Kai-Wei Chang, Wen-Tau Yih, Bishan Yang, Christopher Meek, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingKai-wei Chang, Wen-tau Yih, Bishan Yang, and Christo- pher Meek. 2014. Typed tensor decomposition of knowledge bases for relation extraction. In Proceed- ings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 1568-1579.\n\nRegularizing relation representations by first-order implications. Thomas Demeester, Tim Rockt\u00e4schel, Sebastian Riedel, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Workshop on Automated Knowledge Base Construction. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Workshop on Automated Knowledge Base ConstructionThomas Demeester, Tim Rockt\u00e4schel, and Sebastian Riedel. 2016. Regularizing relation representations by first-order implications. In Proceedings of the 2016 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Lan- guage Technologies -Workshop on Automated Knowl- edge Base Construction, pages 75-80.\n\nSemantically smooth knowledge graph embedding. Shu Guo, Quan Wang, Lihong Wang, Bin Wang, Li Guo, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingShu Guo, Quan Wang, Lihong Wang, Bin Wang, and Li Guo. 2015. Semantically smooth knowledge graph embedding. In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 84-94.\n\nThe metamathematics of fuzzy logic. Petr H\u00e1jek, KluwerPetr H\u00e1jek. 1998. The metamathematics of fuzzy logic. Kluwer.\n\nKnowledgebased weak supervision for information extraction of overlapping relations. Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S Weld, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesRaphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge- based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computation- al Linguistics: Human Language Technologies, pages 541-550.\n\nA latent factor model for highly multi-relational data. Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes, Guillaume R Obozinski, Proceedings of the 26th Annual Conference on Neural Information Processing Systems. the 26th Annual Conference on Neural Information Processing SystemsRodolphe Jenatton, Nicolas L. Roux, Antoine Bordes, and Guillaume R. Obozinski. 2012. A latent factor model for highly multi-relational data. In Proceedings of the 26th Annual Conference on Neural Information Processing Systems, pages 3167-3175.\n\nLearning to refine an automatically extracted knowledge base using markov logic. Shangpu Jiang, Daniel Lowd, Dejing Dou, Proceedings of 12th IEEE International Conference on Data Mining. 12th IEEE International Conference on Data MiningShangpu Jiang, Daniel Lowd, and Dejing Dou. 2012. Learning to refine an automatically extracted knowl- edge base using markov logic. In Proceedings of 12th IEEE International Conference on Data Mining, pages 912-917.\n\nLearning systems of concepts with an infinite relational model. Charles Kemp, Joshua B Tenenbaum, Thomas L Griffiths, Takeshi Yamada, Naonori Ueda, Proceedings of the 21st AAAI Conference on Artificial Intelligence. the 21st AAAI Conference on Artificial IntelligenceCharles Kemp, Joshua B. Tenenbaum, Thomas L. Grif- fiths, Takeshi Yamada, and Naonori Ueda. 2006. Learning systems of concepts with an infinite relation- al model. In Proceedings of the 21st AAAI Conference on Artificial Intelligence, pages 381-388.\n\nType-constrained representation learning in knowledge graphs. Denis Krompa\u00df, Stephan Baier, Volker Tresp, Proceedings of the 14th International Semantic Web Conference. the 14th International Semantic Web ConferenceDenis Krompa\u00df, Stephan Baier, and Volker Tresp. 2015. Type-constrained representation learning in knowl- edge graphs. In Proceedings of the 14th International Semantic Web Conference, pages 640-655.\n\nModeling relation paths for representation learning of knowledge bases. Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, Song Liu, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingYankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. 2015a. Modeling relation paths for representation learning of knowledge bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 705- 714.\n\nLearning entity and relation embeddings for knowledge graph completion. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, Xuan Zhu, Proceedings of the 29th AAAI Conference on Artificial Intelligence. the 29th AAAI Conference on Artificial IntelligenceYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015b. Learning entity and relation em- beddings for knowledge graph completion. In Pro- ceedings of the 29th AAAI Conference on Artificial In- telligence, pages 2181-2187.\n\nContext-dependent knowledge graph embedding. Yuanfei Luo, Quan Wang, Bin Wang, Li Guo, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingYuanfei Luo, Quan Wang, Bin Wang, and Li Guo. 2015. Context-dependent knowledge graph embed- ding. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1656-1661.\n\nLinguistic regularities in continuous space word representations. Tomas Mikolov, Yih Wen-Tau, Geoffrey Zweig, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Confer- ence of the North American Chapter of the Associa- tion for Computational Linguistics: Human Language Technologies, pages 746-751.\n\nWordnet: a lexical database for english. A George, Miller, Communications of the ACM. 3811George A. Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39-41.\n\nCompositional vector space models for knowledge base completion. Arvind Neelakantan, Benjamin Roth, Andrew Mc-Callum, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingArvind Neelakantan, Benjamin Roth, and Andrew Mc- Callum. 2015. Compositional vector space model- s for knowledge base completion. In Proceedings of the 53rd Annual Meeting of the Association for Com- putational Linguistics and the 7th International Join- t Conference on Natural Language Processing, pages 156-166.\n\nA three-way model for collective learning on multi-relational data. Maximilian Nickel, Hans P Volker Tresp, Kriegel, Proceedings of the 28th International Conference on Machine Learning. the 28th International Conference on Machine LearningMaximilian Nickel, Volker Tresp, and Hans P. Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th In- ternational Conference on Machine Learning, pages 809-816.\n\nFactorizing yago: Scalable machine learning for linked data. Maximilian Nickel, Hans P Volker Tresp, Kriegel, Proceedings of the 21st International Conference on World Wide Web. the 21st International Conference on World Wide WebMaximilian Nickel, Volker Tresp, and Hans P. Kriegel. 2012. Factorizing yago: Scalable machine learning for linked data. In Proceedings of the 21st International Conference on World Wide Web, pages 271-280.\n\nHolographic embeddings of knowledge graphs. Maximilian Nickel, Lorenzo Rosasco, Tomaso Poggio, Proceedings of the 30th AAAI Conference on Artificial Intelligence. the 30th AAAI Conference on Artificial IntelligenceMaximilian Nickel, Lorenzo Rosasco, and Tomaso Pog- gio. 2016. Holographic embeddings of knowledge graphs. In Proceedings of the 30th AAAI Conference on Artificial Intelligence, pages 1955-1961.\n\nKnowledge graph identification. Jay Pujara, Hui Miao, Lise Getoor, William Cohen, Proceedings of the 12th International Semantic Web Conference. the 12th International Semantic Web ConferenceJay Pujara, Hui Miao, Lise Getoor, and William Cohen. 2013. Knowledge graph identification. In Proceed- ings of the 12th International Semantic Web Confer- ence, pages 542-557.\n\nMarkov logic networks. Matthew Richardson, Pedro Domingos, Machine Learning. 62Matthew Richardson and Pedro Domingos. 2006. Markov logic networks. Machine Learning, 62(1- 2):107-136.\n\nRelation extraction with matrix factorization and universal schemas. Sebastian Riedel, Limin Yao, Andrew Mccallum, Benjamin M Marlin, Proceedings of the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Pro- ceedings of the 2013 Conference on North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, pages 74-84.\n\nLow-dimensional embeddings of logic. Tim Rockt\u00e4schel, Matko Bo\u0161njak, Sameer Singh, Sebastian Riedel, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics -Workshop on Semantic Parsing. the 52nd Annual Meeting of the Association for Computational Linguistics -Workshop on Semantic ParsingTim Rockt\u00e4schel, Matko Bo\u0161njak, Sameer Singh, and Se- bastian Riedel. 2014. Low-dimensional embeddings of logic. In Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics - Workshop on Semantic Parsing, pages 45-49.\n\nInjecting logical background knowledge into embeddings for relation extraction. Tim Rockt\u00e4schel, Sameer Singh, Sebastian Riedel, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesTim Rockt\u00e4schel, Sameer Singh, and Sebastian Riedel. 2015. Injecting logical background knowledge into embeddings for relation extraction. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 1119-1129.\n\nReasoning with neural tensor networks for knowledge base completion. Richard Socher, Danqi Chen, Christopher D Manning, Andrew Y Ng, Proceedings of the 27th Annual Conference on Neural Information Processing Systems. the 27th Annual Conference on Neural Information Processing SystemsRichard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning with neural ten- sor networks for knowledge base completion. In Pro- ceedings of the 27th Annual Conference on Neural In- formation Processing Systems, pages 926-934.\n\nReasoning with very expressive fuzzy description logics. Giorgos Stoilos, Giorgos B Stamou, Jeff Z Pan, Journal of Artificial Intelligence Research. 30Vassilis Tzouvaras, and Ian HorrocksGiorgos Stoilos, Giorgos B. Stamou, Jeff Z. Pan, Vassilis Tzouvaras, and Ian Horrocks. 2007. Reasoning with very expressive fuzzy description logics. Journal of Artificial Intelligence Research, 30:273-320.\n\nModelling relational data using bayesian clustered tensor factorization. Ilya Sutskever, Joshua B Tenenbaum, Ruslan R Salakhutdinov, Proceedings of the 23rd Annual Conference on Neural Information Processing Systems. the 23rd Annual Conference on Neural Information Processing SystemsIlya Sutskever, Joshua B. Tenenbaum, and Ruslan R. Salakhutdinov. 2009. Modelling relational data using bayesian clustered tensor factorization. In Proceed- ings of the 23rd Annual Conference on Neural Infor- mation Processing Systems, pages 1821-1828.\n\nKnowledge graph embedding by translating on hyperplanes. Zhen Wang, Jianwen Zhang, Jianlin Feng, Zheng Chen, Proceedings of the 28th AAAI Conference on Artificial Intelligence. the 28th AAAI Conference on Artificial IntelligenceZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the 28th AAAI Conference on Artificial Intelligence, pages 1112-1119.\n\nKnowledge base completion using embeddings and rules. Quan Wang, Bin Wang, Li Guo, Proceedings of the 24th International Joint Conference on Artificial Intelligence. the 24th International Joint Conference on Artificial IntelligenceQuan Wang, Bin Wang, and Li Guo. 2015. Knowledge base completion using embeddings and rules. In Pro- ceedings of the 24th International Joint Conference on Artificial Intelligence, pages 1859-1865.\n\nLearning to identify the best contexts for knowledge-based wsd. Evgenia Wasserman-Pritsker, William W Cohen, Einat Minkov, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingEvgenia Wasserman-Pritsker, William W. Cohen, and Einat Minkov. 2015. Learning to identify the best contexts for knowledge-based wsd. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1662-1667.\n\nLarge-scale knowledge base completion: inferring via grounding network sampling over selected instances. Zhuoyu Wei, Jun Zhao, Kang Liu, Zhenyu Qi, Zhengya Sun, Guanhua Tian, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. the 24th ACM International on Conference on Information and Knowledge ManagementZhuoyu Wei, Jun Zhao, Kang Liu, Zhenyu Qi, Zhengya Sun, and Guanhua Tian. 2015. Large-scale knowl- edge base completion: inferring via grounding net- work sampling over selected instances. In Proceed- ings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1331-1340.\n\nConnecting language and knowledge bases with embedding models for relation extraction. Jason Weston, Antoine Bordes, Oksana Yakhnenko, Nicolas Usunier, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingJason Weston, Antoine Bordes, Oksana Yakhnenko, and Nicolas Usunier. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1366-1371.\n\nInfinite hidden relational models. Zhao Xu, Kai Volker Tresp, Hanspeter Yu, Kriegel, Proceedings of Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence. the 22nd Conference on Uncertainty in Artificial IntelligenceZhao Xu, Volker Tresp, Kai Yu, and Hanspeter Kriegel. 2006. Infinite hidden relational models. In Proceed- ings of Proceedings of the 22nd Conference on Uncer- tainty in Artificial Intelligence, pages 544-551.\n\nAligning knowledge and text embeddings by entity descriptions. Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, Zheng Chen, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingHuaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and Zheng Chen. 2015. Aligning knowledge and text embeddings by entity descriptions. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 267-272.\n", "annotations": {"author": "[{\"end\":344,\"start\":172},{\"end\":521,\"start\":345},{\"end\":643,\"start\":522},{\"end\":818,\"start\":644},{\"end\":989,\"start\":819}]", "publisher": "[{\"end\":95,\"start\":54},{\"end\":1256,\"start\":1215}]", "author_last_name": "[{\"end\":179,\"start\":176},{\"end\":354,\"start\":350},{\"end\":533,\"start\":529},{\"end\":652,\"start\":648},{\"end\":825,\"start\":822}]", "author_first_name": "[{\"end\":175,\"start\":172},{\"end\":349,\"start\":345},{\"end\":528,\"start\":522},{\"end\":647,\"start\":644},{\"end\":821,\"start\":819}]", "author_affiliation": "[{\"end\":281,\"start\":198},{\"end\":343,\"start\":283},{\"end\":458,\"start\":375},{\"end\":520,\"start\":460},{\"end\":642,\"start\":535},{\"end\":755,\"start\":672},{\"end\":817,\"start\":757},{\"end\":926,\"start\":843},{\"end\":988,\"start\":928}]", "title": "[{\"end\":53,\"start\":1},{\"end\":1042,\"start\":990}]", "venue": "[{\"end\":1130,\"start\":1044}]", "abstract": "[{\"end\":2575,\"start\":1281}]", "bib_ref": "[{\"end\":2766,\"start\":2761},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2826,\"start\":2793},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2877,\"start\":2854},{\"end\":3040,\"start\":3013},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3334,\"start\":3313},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3354,\"start\":3334},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3374,\"start\":3354},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3815,\"start\":3797},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3835,\"start\":3815},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4069,\"start\":4049},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4089,\"start\":4069},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4159,\"start\":4142},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4425,\"start\":4400},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4773,\"start\":4753},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5463,\"start\":5450},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6486,\"start\":6472},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6524,\"start\":6500},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7064,\"start\":7043},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7084,\"start\":7064},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7146,\"start\":7125},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7166,\"start\":7146},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7185,\"start\":7166},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7240,\"start\":7221},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7256,\"start\":7240},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7278,\"start\":7256},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7495,\"start\":7476},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7527,\"start\":7508},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7824,\"start\":7798},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7842,\"start\":7824},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7859,\"start\":7842},{\"end\":7908,\"start\":7887},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7969,\"start\":7949},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8156,\"start\":8125},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8179,\"start\":8156},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8199,\"start\":8179},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8224,\"start\":8199},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8339,\"start\":8322},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8613,\"start\":8588},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10137,\"start\":10111},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10162,\"start\":10137},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12121,\"start\":12099},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12787,\"start\":12774},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12983,\"start\":12958},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18226,\"start\":18202},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18304,\"start\":18282},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18510,\"start\":18484},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18535,\"start\":18510},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22341,\"start\":22323},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":22426,\"start\":22408},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24274,\"start\":24253},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26931,\"start\":26912},{\"end\":27190,\"start\":27189},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30373,\"start\":30350}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30717,\"start\":30676},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31146,\"start\":30718},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31817,\"start\":31147},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31856,\"start\":31818},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32061,\"start\":31857},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32179,\"start\":32062},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32480,\"start\":32180},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":32813,\"start\":32481},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":32915,\"start\":32814}]", "paragraph": "[{\"end\":3187,\"start\":2591},{\"end\":3717,\"start\":3189},{\"end\":4774,\"start\":3719},{\"end\":6066,\"start\":4776},{\"end\":6829,\"start\":6068},{\"end\":7998,\"start\":6846},{\"end\":9137,\"start\":8000},{\"end\":9388,\"start\":9179},{\"end\":9986,\"start\":9390},{\"end\":10793,\"start\":9999},{\"end\":11656,\"start\":10795},{\"end\":12395,\"start\":11676},{\"end\":12714,\"start\":12457},{\"end\":13126,\"start\":12732},{\"end\":13160,\"start\":13159},{\"end\":13413,\"start\":13238},{\"end\":13458,\"start\":13457},{\"end\":13698,\"start\":13506},{\"end\":14038,\"start\":13774},{\"end\":14478,\"start\":14162},{\"end\":14697,\"start\":14636},{\"end\":15180,\"start\":14716},{\"end\":15434,\"start\":15292},{\"end\":16609,\"start\":15478},{\"end\":17565,\"start\":16625},{\"end\":17619,\"start\":17567},{\"end\":18305,\"start\":17621},{\"end\":18536,\"start\":18307},{\"end\":19324,\"start\":18552},{\"end\":19830,\"start\":19326},{\"end\":20891,\"start\":19832},{\"end\":23289,\"start\":20893},{\"end\":24275,\"start\":23309},{\"end\":24934,\"start\":24277},{\"end\":25487,\"start\":24936},{\"end\":26702,\"start\":25489},{\"end\":27927,\"start\":26728},{\"end\":28351,\"start\":27929},{\"end\":29053,\"start\":28353},{\"end\":30072,\"start\":29084},{\"end\":30675,\"start\":30074}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12456,\"start\":12396},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13158,\"start\":13127},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13237,\"start\":13161},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13456,\"start\":13414},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13505,\"start\":13459},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13773,\"start\":13699},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14161,\"start\":14039},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14635,\"start\":14479},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15291,\"start\":15181},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15477,\"start\":15435}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17702,\"start\":17695},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19127,\"start\":19120},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19677,\"start\":19670},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25505,\"start\":25498},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28369,\"start\":28362}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2589,\"start\":2577},{\"attributes\":{\"n\":\"2\"},\"end\":6844,\"start\":6832},{\"attributes\":{\"n\":\"3\"},\"end\":9177,\"start\":9140},{\"attributes\":{\"n\":\"3.1\"},\"end\":9997,\"start\":9989},{\"attributes\":{\"n\":\"3.2\"},\"end\":11674,\"start\":11659},{\"attributes\":{\"n\":\"3.3\"},\"end\":12730,\"start\":12717},{\"attributes\":{\"n\":\"3.4\"},\"end\":14714,\"start\":14700},{\"attributes\":{\"n\":\"3.5\"},\"end\":16623,\"start\":16612},{\"attributes\":{\"n\":\"4\"},\"end\":18550,\"start\":18539},{\"attributes\":{\"n\":\"4.2\"},\"end\":23307,\"start\":23292},{\"attributes\":{\"n\":\"4.3\"},\"end\":26726,\"start\":26705},{\"attributes\":{\"n\":\"5\"},\"end\":29082,\"start\":29056},{\"end\":30687,\"start\":30677},{\"end\":30726,\"start\":30719},{\"end\":31157,\"start\":31148},{\"end\":31828,\"start\":31819},{\"end\":31867,\"start\":31858},{\"end\":32072,\"start\":32063},{\"end\":32188,\"start\":32181},{\"end\":32492,\"start\":32482},{\"end\":32824,\"start\":32815}]", "table": "[{\"end\":31146,\"start\":30771},{\"end\":31817,\"start\":31182},{\"end\":32061,\"start\":31873},{\"end\":32179,\"start\":32078},{\"end\":32480,\"start\":32455},{\"end\":32813,\"start\":32584}]", "figure_caption": "[{\"end\":30717,\"start\":30689},{\"end\":30771,\"start\":30728},{\"end\":31182,\"start\":31159},{\"end\":31856,\"start\":31830},{\"end\":31873,\"start\":31869},{\"end\":32078,\"start\":32074},{\"end\":32455,\"start\":32190},{\"end\":32584,\"start\":32498},{\"end\":32915,\"start\":32826}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5719,\"start\":5711},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11373,\"start\":11365}]", "bib_author_first_name": "[{\"end\":33914,\"start\":33909},{\"end\":33931,\"start\":33924},{\"end\":33933,\"start\":33932},{\"end\":34523,\"start\":34519},{\"end\":34540,\"start\":34535},{\"end\":34555,\"start\":34548},{\"end\":34569,\"start\":34566},{\"end\":34583,\"start\":34578},{\"end\":35076,\"start\":35069},{\"end\":35090,\"start\":35085},{\"end\":35104,\"start\":35099},{\"end\":35122,\"start\":35116},{\"end\":35528,\"start\":35521},{\"end\":35544,\"start\":35537},{\"end\":35561,\"start\":35554},{\"end\":35581,\"start\":35576},{\"end\":35596,\"start\":35590},{\"end\":36127,\"start\":36120},{\"end\":36142,\"start\":36136},{\"end\":36156,\"start\":36151},{\"end\":36171,\"start\":36165},{\"end\":36405,\"start\":36397},{\"end\":36424,\"start\":36417},{\"end\":36440,\"start\":36436},{\"end\":36857,\"start\":36850},{\"end\":36872,\"start\":36865},{\"end\":36884,\"start\":36878},{\"end\":36902,\"start\":36891},{\"end\":37394,\"start\":37388},{\"end\":37409,\"start\":37406},{\"end\":37432,\"start\":37423},{\"end\":38212,\"start\":38209},{\"end\":38222,\"start\":38218},{\"end\":38235,\"start\":38229},{\"end\":38245,\"start\":38242},{\"end\":38254,\"start\":38252},{\"end\":38899,\"start\":38895},{\"end\":39068,\"start\":39061},{\"end\":39085,\"start\":39079},{\"end\":39097,\"start\":39093},{\"end\":39108,\"start\":39104},{\"end\":39128,\"start\":39122},{\"end\":39130,\"start\":39129},{\"end\":39733,\"start\":39725},{\"end\":39751,\"start\":39744},{\"end\":39753,\"start\":39752},{\"end\":39767,\"start\":39760},{\"end\":39785,\"start\":39776},{\"end\":39787,\"start\":39786},{\"end\":40285,\"start\":40278},{\"end\":40299,\"start\":40293},{\"end\":40312,\"start\":40306},{\"end\":40722,\"start\":40715},{\"end\":40735,\"start\":40729},{\"end\":40737,\"start\":40736},{\"end\":40755,\"start\":40749},{\"end\":40757,\"start\":40756},{\"end\":40776,\"start\":40769},{\"end\":40792,\"start\":40785},{\"end\":41236,\"start\":41231},{\"end\":41253,\"start\":41246},{\"end\":41267,\"start\":41261},{\"end\":41662,\"start\":41656},{\"end\":41675,\"start\":41668},{\"end\":41687,\"start\":41681},{\"end\":41701,\"start\":41694},{\"end\":41712,\"start\":41707},{\"end\":41722,\"start\":41718},{\"end\":42228,\"start\":42222},{\"end\":42241,\"start\":42234},{\"end\":42254,\"start\":42247},{\"end\":42264,\"start\":42260},{\"end\":42274,\"start\":42270},{\"end\":42687,\"start\":42680},{\"end\":42697,\"start\":42693},{\"end\":42707,\"start\":42704},{\"end\":42716,\"start\":42714},{\"end\":43160,\"start\":43155},{\"end\":43173,\"start\":43170},{\"end\":43191,\"start\":43183},{\"end\":43799,\"start\":43798},{\"end\":44025,\"start\":44019},{\"end\":44047,\"start\":44039},{\"end\":44060,\"start\":44054},{\"end\":44776,\"start\":44766},{\"end\":44789,\"start\":44785},{\"end\":44791,\"start\":44790},{\"end\":45228,\"start\":45218},{\"end\":45241,\"start\":45237},{\"end\":45243,\"start\":45242},{\"end\":45648,\"start\":45638},{\"end\":45664,\"start\":45657},{\"end\":45680,\"start\":45674},{\"end\":46039,\"start\":46036},{\"end\":46051,\"start\":46048},{\"end\":46062,\"start\":46058},{\"end\":46078,\"start\":46071},{\"end\":46403,\"start\":46396},{\"end\":46421,\"start\":46416},{\"end\":46635,\"start\":46626},{\"end\":46649,\"start\":46644},{\"end\":46661,\"start\":46655},{\"end\":46680,\"start\":46672},{\"end\":46682,\"start\":46681},{\"end\":47300,\"start\":47297},{\"end\":47319,\"start\":47314},{\"end\":47335,\"start\":47329},{\"end\":47352,\"start\":47343},{\"end\":47917,\"start\":47914},{\"end\":47937,\"start\":47931},{\"end\":47954,\"start\":47945},{\"end\":48616,\"start\":48609},{\"end\":48630,\"start\":48625},{\"end\":48648,\"start\":48637},{\"end\":48650,\"start\":48649},{\"end\":48666,\"start\":48660},{\"end\":48668,\"start\":48667},{\"end\":49142,\"start\":49135},{\"end\":49159,\"start\":49152},{\"end\":49161,\"start\":49160},{\"end\":49174,\"start\":49170},{\"end\":49176,\"start\":49175},{\"end\":49550,\"start\":49546},{\"end\":49568,\"start\":49562},{\"end\":49570,\"start\":49569},{\"end\":49588,\"start\":49582},{\"end\":49590,\"start\":49589},{\"end\":50072,\"start\":50068},{\"end\":50086,\"start\":50079},{\"end\":50101,\"start\":50094},{\"end\":50113,\"start\":50108},{\"end\":50505,\"start\":50501},{\"end\":50515,\"start\":50512},{\"end\":50524,\"start\":50522},{\"end\":50949,\"start\":50942},{\"end\":50977,\"start\":50970},{\"end\":50979,\"start\":50978},{\"end\":50992,\"start\":50987},{\"end\":51514,\"start\":51508},{\"end\":51523,\"start\":51520},{\"end\":51534,\"start\":51530},{\"end\":51546,\"start\":51540},{\"end\":51558,\"start\":51551},{\"end\":51571,\"start\":51564},{\"end\":52156,\"start\":52151},{\"end\":52172,\"start\":52165},{\"end\":52187,\"start\":52181},{\"end\":52206,\"start\":52199},{\"end\":52685,\"start\":52681},{\"end\":52693,\"start\":52690},{\"end\":52717,\"start\":52708},{\"end\":53166,\"start\":53159},{\"end\":53181,\"start\":53174},{\"end\":53193,\"start\":53189},{\"end\":53203,\"start\":53200},{\"end\":53214,\"start\":53209}]", "bib_author_last_name": "[{\"end\":33922,\"start\":33915},{\"end\":33940,\"start\":33934},{\"end\":34533,\"start\":34524},{\"end\":34546,\"start\":34541},{\"end\":34564,\"start\":34556},{\"end\":34576,\"start\":34570},{\"end\":34590,\"start\":34584},{\"end\":35083,\"start\":35077},{\"end\":35097,\"start\":35091},{\"end\":35114,\"start\":35105},{\"end\":35129,\"start\":35123},{\"end\":35535,\"start\":35529},{\"end\":35552,\"start\":35545},{\"end\":35574,\"start\":35562},{\"end\":35588,\"start\":35582},{\"end\":35606,\"start\":35597},{\"end\":36134,\"start\":36128},{\"end\":36149,\"start\":36143},{\"end\":36163,\"start\":36157},{\"end\":36178,\"start\":36172},{\"end\":36415,\"start\":36406},{\"end\":36434,\"start\":36425},{\"end\":36447,\"start\":36441},{\"end\":36863,\"start\":36858},{\"end\":36876,\"start\":36873},{\"end\":36889,\"start\":36885},{\"end\":36907,\"start\":36903},{\"end\":37404,\"start\":37395},{\"end\":37421,\"start\":37410},{\"end\":37439,\"start\":37433},{\"end\":38216,\"start\":38213},{\"end\":38227,\"start\":38223},{\"end\":38240,\"start\":38236},{\"end\":38250,\"start\":38246},{\"end\":38258,\"start\":38255},{\"end\":38905,\"start\":38900},{\"end\":39077,\"start\":39069},{\"end\":39091,\"start\":39086},{\"end\":39102,\"start\":39098},{\"end\":39120,\"start\":39109},{\"end\":39135,\"start\":39131},{\"end\":39742,\"start\":39734},{\"end\":39758,\"start\":39754},{\"end\":39774,\"start\":39768},{\"end\":39797,\"start\":39788},{\"end\":40291,\"start\":40286},{\"end\":40304,\"start\":40300},{\"end\":40316,\"start\":40313},{\"end\":40727,\"start\":40723},{\"end\":40747,\"start\":40738},{\"end\":40767,\"start\":40758},{\"end\":40783,\"start\":40777},{\"end\":40797,\"start\":40793},{\"end\":41244,\"start\":41237},{\"end\":41259,\"start\":41254},{\"end\":41273,\"start\":41268},{\"end\":41666,\"start\":41663},{\"end\":41679,\"start\":41676},{\"end\":41692,\"start\":41688},{\"end\":41705,\"start\":41702},{\"end\":41716,\"start\":41713},{\"end\":41726,\"start\":41723},{\"end\":42232,\"start\":42229},{\"end\":42245,\"start\":42242},{\"end\":42258,\"start\":42255},{\"end\":42268,\"start\":42265},{\"end\":42278,\"start\":42275},{\"end\":42691,\"start\":42688},{\"end\":42702,\"start\":42698},{\"end\":42712,\"start\":42708},{\"end\":42720,\"start\":42717},{\"end\":43168,\"start\":43161},{\"end\":43181,\"start\":43174},{\"end\":43197,\"start\":43192},{\"end\":43806,\"start\":43800},{\"end\":43814,\"start\":43808},{\"end\":44037,\"start\":44026},{\"end\":44052,\"start\":44048},{\"end\":44070,\"start\":44061},{\"end\":44783,\"start\":44777},{\"end\":44804,\"start\":44792},{\"end\":44813,\"start\":44806},{\"end\":45235,\"start\":45229},{\"end\":45256,\"start\":45244},{\"end\":45265,\"start\":45258},{\"end\":45655,\"start\":45649},{\"end\":45672,\"start\":45665},{\"end\":45687,\"start\":45681},{\"end\":46046,\"start\":46040},{\"end\":46056,\"start\":46052},{\"end\":46069,\"start\":46063},{\"end\":46084,\"start\":46079},{\"end\":46414,\"start\":46404},{\"end\":46430,\"start\":46422},{\"end\":46642,\"start\":46636},{\"end\":46653,\"start\":46650},{\"end\":46670,\"start\":46662},{\"end\":46689,\"start\":46683},{\"end\":47312,\"start\":47301},{\"end\":47327,\"start\":47320},{\"end\":47341,\"start\":47336},{\"end\":47359,\"start\":47353},{\"end\":47929,\"start\":47918},{\"end\":47943,\"start\":47938},{\"end\":47961,\"start\":47955},{\"end\":48623,\"start\":48617},{\"end\":48635,\"start\":48631},{\"end\":48658,\"start\":48651},{\"end\":48671,\"start\":48669},{\"end\":49150,\"start\":49143},{\"end\":49168,\"start\":49162},{\"end\":49180,\"start\":49177},{\"end\":49560,\"start\":49551},{\"end\":49580,\"start\":49571},{\"end\":49604,\"start\":49591},{\"end\":50077,\"start\":50073},{\"end\":50092,\"start\":50087},{\"end\":50106,\"start\":50102},{\"end\":50118,\"start\":50114},{\"end\":50510,\"start\":50506},{\"end\":50520,\"start\":50516},{\"end\":50528,\"start\":50525},{\"end\":50968,\"start\":50950},{\"end\":50985,\"start\":50980},{\"end\":50999,\"start\":50993},{\"end\":51518,\"start\":51515},{\"end\":51528,\"start\":51524},{\"end\":51538,\"start\":51535},{\"end\":51549,\"start\":51547},{\"end\":51562,\"start\":51559},{\"end\":51576,\"start\":51572},{\"end\":52163,\"start\":52157},{\"end\":52179,\"start\":52173},{\"end\":52197,\"start\":52188},{\"end\":52214,\"start\":52207},{\"end\":52688,\"start\":52686},{\"end\":52706,\"start\":52694},{\"end\":52720,\"start\":52718},{\"end\":52729,\"start\":52722},{\"end\":53172,\"start\":53167},{\"end\":53187,\"start\":53182},{\"end\":53198,\"start\":53194},{\"end\":53207,\"start\":53204},{\"end\":53219,\"start\":53215}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14585127},\"end\":34433,\"start\":33844},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207167677},\"end\":35016,\"start\":34435},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":715463},\"end\":35460,\"start\":35018},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14941970},\"end\":36023,\"start\":35462},{\"attributes\":{\"id\":\"b4\"},\"end\":36363,\"start\":36025},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2998057},\"end\":36777,\"start\":36365},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1077128},\"end\":37319,\"start\":36779},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":16518575},\"end\":38160,\"start\":37321},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":205692},\"end\":38857,\"start\":38162},{\"attributes\":{\"id\":\"b9\"},\"end\":38974,\"start\":38859},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":16483125},\"end\":39667,\"start\":38976},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10854724},\"end\":40195,\"start\":39669},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":828421},\"end\":40649,\"start\":40197},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1021333},\"end\":41167,\"start\":40651},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14178327},\"end\":41582,\"start\":41169},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1969092},\"end\":42148,\"start\":41584},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2949428},\"end\":42633,\"start\":42150},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14047545},\"end\":43087,\"start\":42635},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7478738},\"end\":43755,\"start\":43089},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1671874},\"end\":43952,\"start\":43757},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11171811},\"end\":44696,\"start\":43954},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1157792},\"end\":45155,\"start\":44698},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6348464},\"end\":45592,\"start\":45157},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6071257},\"end\":46002,\"start\":45594},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11564235},\"end\":46371,\"start\":46004},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12698795},\"end\":46555,\"start\":46373},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2687019},\"end\":47258,\"start\":46557},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":17419203},\"end\":47832,\"start\":47260},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6488690},\"end\":48538,\"start\":47834},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8429835},\"end\":49076,\"start\":48540},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":178794},\"end\":49471,\"start\":49078},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14474967},\"end\":50009,\"start\":49473},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":15027084},\"end\":50445,\"start\":50011},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3143712},\"end\":50876,\"start\":50447},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1598651},\"end\":51401,\"start\":50878},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":213169},\"end\":52062,\"start\":51403},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":89639},\"end\":52644,\"start\":52064},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":891216},\"end\":53094,\"start\":52646},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":5440379},\"end\":53620,\"start\":53096}]", "bib_title": "[{\"end\":33907,\"start\":33844},{\"end\":34517,\"start\":34435},{\"end\":35067,\"start\":35018},{\"end\":35519,\"start\":35462},{\"end\":36395,\"start\":36365},{\"end\":36848,\"start\":36779},{\"end\":37386,\"start\":37321},{\"end\":38207,\"start\":38162},{\"end\":39059,\"start\":38976},{\"end\":39723,\"start\":39669},{\"end\":40276,\"start\":40197},{\"end\":40713,\"start\":40651},{\"end\":41229,\"start\":41169},{\"end\":41654,\"start\":41584},{\"end\":42220,\"start\":42150},{\"end\":42678,\"start\":42635},{\"end\":43153,\"start\":43089},{\"end\":43796,\"start\":43757},{\"end\":44017,\"start\":43954},{\"end\":44764,\"start\":44698},{\"end\":45216,\"start\":45157},{\"end\":45636,\"start\":45594},{\"end\":46034,\"start\":46004},{\"end\":46394,\"start\":46373},{\"end\":46624,\"start\":46557},{\"end\":47295,\"start\":47260},{\"end\":47912,\"start\":47834},{\"end\":48607,\"start\":48540},{\"end\":49133,\"start\":49078},{\"end\":49544,\"start\":49473},{\"end\":50066,\"start\":50011},{\"end\":50499,\"start\":50447},{\"end\":50940,\"start\":50878},{\"end\":51506,\"start\":51403},{\"end\":52149,\"start\":52064},{\"end\":52679,\"start\":52646},{\"end\":53157,\"start\":53096}]", "bib_author": "[{\"end\":33924,\"start\":33909},{\"end\":33942,\"start\":33924},{\"end\":34535,\"start\":34519},{\"end\":34548,\"start\":34535},{\"end\":34566,\"start\":34548},{\"end\":34578,\"start\":34566},{\"end\":34592,\"start\":34578},{\"end\":35085,\"start\":35069},{\"end\":35099,\"start\":35085},{\"end\":35116,\"start\":35099},{\"end\":35131,\"start\":35116},{\"end\":35537,\"start\":35521},{\"end\":35554,\"start\":35537},{\"end\":35576,\"start\":35554},{\"end\":35590,\"start\":35576},{\"end\":35608,\"start\":35590},{\"end\":36136,\"start\":36120},{\"end\":36151,\"start\":36136},{\"end\":36165,\"start\":36151},{\"end\":36180,\"start\":36165},{\"end\":36417,\"start\":36397},{\"end\":36436,\"start\":36417},{\"end\":36449,\"start\":36436},{\"end\":36865,\"start\":36850},{\"end\":36878,\"start\":36865},{\"end\":36891,\"start\":36878},{\"end\":36909,\"start\":36891},{\"end\":37406,\"start\":37388},{\"end\":37423,\"start\":37406},{\"end\":37441,\"start\":37423},{\"end\":38218,\"start\":38209},{\"end\":38229,\"start\":38218},{\"end\":38242,\"start\":38229},{\"end\":38252,\"start\":38242},{\"end\":38260,\"start\":38252},{\"end\":38907,\"start\":38895},{\"end\":39079,\"start\":39061},{\"end\":39093,\"start\":39079},{\"end\":39104,\"start\":39093},{\"end\":39122,\"start\":39104},{\"end\":39137,\"start\":39122},{\"end\":39744,\"start\":39725},{\"end\":39760,\"start\":39744},{\"end\":39776,\"start\":39760},{\"end\":39799,\"start\":39776},{\"end\":40293,\"start\":40278},{\"end\":40306,\"start\":40293},{\"end\":40318,\"start\":40306},{\"end\":40729,\"start\":40715},{\"end\":40749,\"start\":40729},{\"end\":40769,\"start\":40749},{\"end\":40785,\"start\":40769},{\"end\":40799,\"start\":40785},{\"end\":41246,\"start\":41231},{\"end\":41261,\"start\":41246},{\"end\":41275,\"start\":41261},{\"end\":41668,\"start\":41656},{\"end\":41681,\"start\":41668},{\"end\":41694,\"start\":41681},{\"end\":41707,\"start\":41694},{\"end\":41718,\"start\":41707},{\"end\":41728,\"start\":41718},{\"end\":42234,\"start\":42222},{\"end\":42247,\"start\":42234},{\"end\":42260,\"start\":42247},{\"end\":42270,\"start\":42260},{\"end\":42280,\"start\":42270},{\"end\":42693,\"start\":42680},{\"end\":42704,\"start\":42693},{\"end\":42714,\"start\":42704},{\"end\":42722,\"start\":42714},{\"end\":43170,\"start\":43155},{\"end\":43183,\"start\":43170},{\"end\":43199,\"start\":43183},{\"end\":43808,\"start\":43798},{\"end\":43816,\"start\":43808},{\"end\":44039,\"start\":44019},{\"end\":44054,\"start\":44039},{\"end\":44072,\"start\":44054},{\"end\":44785,\"start\":44766},{\"end\":44806,\"start\":44785},{\"end\":44815,\"start\":44806},{\"end\":45237,\"start\":45218},{\"end\":45258,\"start\":45237},{\"end\":45267,\"start\":45258},{\"end\":45657,\"start\":45638},{\"end\":45674,\"start\":45657},{\"end\":45689,\"start\":45674},{\"end\":46048,\"start\":46036},{\"end\":46058,\"start\":46048},{\"end\":46071,\"start\":46058},{\"end\":46086,\"start\":46071},{\"end\":46416,\"start\":46396},{\"end\":46432,\"start\":46416},{\"end\":46644,\"start\":46626},{\"end\":46655,\"start\":46644},{\"end\":46672,\"start\":46655},{\"end\":46691,\"start\":46672},{\"end\":47314,\"start\":47297},{\"end\":47329,\"start\":47314},{\"end\":47343,\"start\":47329},{\"end\":47361,\"start\":47343},{\"end\":47931,\"start\":47914},{\"end\":47945,\"start\":47931},{\"end\":47963,\"start\":47945},{\"end\":48625,\"start\":48609},{\"end\":48637,\"start\":48625},{\"end\":48660,\"start\":48637},{\"end\":48673,\"start\":48660},{\"end\":49152,\"start\":49135},{\"end\":49170,\"start\":49152},{\"end\":49182,\"start\":49170},{\"end\":49562,\"start\":49546},{\"end\":49582,\"start\":49562},{\"end\":49606,\"start\":49582},{\"end\":50079,\"start\":50068},{\"end\":50094,\"start\":50079},{\"end\":50108,\"start\":50094},{\"end\":50120,\"start\":50108},{\"end\":50512,\"start\":50501},{\"end\":50522,\"start\":50512},{\"end\":50530,\"start\":50522},{\"end\":50970,\"start\":50942},{\"end\":50987,\"start\":50970},{\"end\":51001,\"start\":50987},{\"end\":51520,\"start\":51508},{\"end\":51530,\"start\":51520},{\"end\":51540,\"start\":51530},{\"end\":51551,\"start\":51540},{\"end\":51564,\"start\":51551},{\"end\":51578,\"start\":51564},{\"end\":52165,\"start\":52151},{\"end\":52181,\"start\":52165},{\"end\":52199,\"start\":52181},{\"end\":52216,\"start\":52199},{\"end\":52690,\"start\":52681},{\"end\":52708,\"start\":52690},{\"end\":52722,\"start\":52708},{\"end\":52731,\"start\":52722},{\"end\":53174,\"start\":53159},{\"end\":53189,\"start\":53174},{\"end\":53200,\"start\":53189},{\"end\":53209,\"start\":53200},{\"end\":53221,\"start\":53209}]", "bib_venue": "[{\"end\":34068,\"start\":33942},{\"end\":34673,\"start\":34592},{\"end\":35197,\"start\":35131},{\"end\":35690,\"start\":35608},{\"end\":36118,\"start\":36025},{\"end\":36525,\"start\":36449},{\"end\":36995,\"start\":36909},{\"end\":37634,\"start\":37441},{\"end\":38421,\"start\":38260},{\"end\":38893,\"start\":38859},{\"end\":39253,\"start\":39137},{\"end\":39881,\"start\":39799},{\"end\":40382,\"start\":40318},{\"end\":40865,\"start\":40799},{\"end\":41336,\"start\":41275},{\"end\":41814,\"start\":41728},{\"end\":42346,\"start\":42280},{\"end\":42808,\"start\":42722},{\"end\":43341,\"start\":43199},{\"end\":43841,\"start\":43816},{\"end\":44233,\"start\":44072},{\"end\":44883,\"start\":44815},{\"end\":45333,\"start\":45267},{\"end\":45755,\"start\":45689},{\"end\":46147,\"start\":46086},{\"end\":46448,\"start\":46432},{\"end\":46829,\"start\":46691},{\"end\":47478,\"start\":47361},{\"end\":48105,\"start\":47963},{\"end\":48755,\"start\":48673},{\"end\":49225,\"start\":49182},{\"end\":49688,\"start\":49606},{\"end\":50186,\"start\":50120},{\"end\":50611,\"start\":50530},{\"end\":51087,\"start\":51001},{\"end\":51673,\"start\":51578},{\"end\":52302,\"start\":52216},{\"end\":52822,\"start\":52731},{\"end\":53307,\"start\":53221},{\"end\":34181,\"start\":34070},{\"end\":34741,\"start\":34675},{\"end\":35250,\"start\":35199},{\"end\":35759,\"start\":35692},{\"end\":36588,\"start\":36527},{\"end\":37068,\"start\":36997},{\"end\":37814,\"start\":37636},{\"end\":38569,\"start\":38423},{\"end\":39356,\"start\":39255},{\"end\":39950,\"start\":39883},{\"end\":40433,\"start\":40384},{\"end\":40918,\"start\":40867},{\"end\":41384,\"start\":41338},{\"end\":41887,\"start\":41816},{\"end\":42399,\"start\":42348},{\"end\":42881,\"start\":42810},{\"end\":43470,\"start\":43343},{\"end\":44381,\"start\":44235},{\"end\":44938,\"start\":44885},{\"end\":45386,\"start\":45335},{\"end\":45808,\"start\":45757},{\"end\":46195,\"start\":46149},{\"end\":46954,\"start\":46831},{\"end\":47582,\"start\":47480},{\"end\":48234,\"start\":48107},{\"end\":48824,\"start\":48757},{\"end\":49757,\"start\":49690},{\"end\":50239,\"start\":50188},{\"end\":50679,\"start\":50613},{\"end\":51160,\"start\":51089},{\"end\":51755,\"start\":51675},{\"end\":52375,\"start\":52304},{\"end\":52885,\"start\":52824},{\"end\":53380,\"start\":53309}]"}}}, "year": 2023, "month": 12, "day": 17}