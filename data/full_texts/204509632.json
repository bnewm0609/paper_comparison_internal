{"id": 204509632, "updated": "2023-10-06 22:01:31.51", "metadata": {"title": "Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning", "authors": "[{\"first\":\"Nai-Hui\",\"last\":\"Chia\",\"middle\":[]},{\"first\":\"Andr'as\",\"last\":\"Gily'en\",\"middle\":[]},{\"first\":\"Tongyang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Han-Hsuan\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Ewin\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Chunhao\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "We present an algorithmic framework for quantum-inspired classical algorithms on close-to-low-rank matrices, generalizing the series of results started by Tang's breakthrough quantum-inspired algorithm for recommendation systems [STOC'19]. Motivated by quantum linear algebra algorithms and the quantum singular value transformation (SVT) framework of Gily\\'en, Su, Low, and Wiebe [STOC'19], we develop classical algorithms for SVT that run in time independent of input dimension, under suitable quantum-inspired sampling assumptions. Our results give compelling evidence that in the corresponding QRAM data structure input model, quantum SVT does not yield exponential quantum speedups. Since the quantum SVT framework generalizes essentially all known techniques for quantum linear algebra, our results, combined with sampling lemmas from previous work, suffice to generalize all recent results about dequantizing quantum machine learning algorithms. In particular, our classical SVT framework recovers and often improves the dequantization results on recommendation systems, principal component analysis, supervised clustering, support vector machines, low-rank regression, and semidefinite program solving. We also give additional dequantization results on low-rank Hamiltonian simulation and discriminant analysis. Our improvements come from identifying the key feature of the quantum-inspired input model that is at the core of all prior quantum-inspired results: $\\ell^2$-norm sampling can approximate matrix products in time independent of their dimension. We reduce all our main results to this fact, making our exposition concise, self-contained, and intuitive.", "fields_of_study": "[\"Computer Science\",\"Physics\"]", "external_ids": {"arxiv": "1910.06151", "mag": "3106381803", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/stoc/ChiaGLLTW20", "doi": "10.1145/3357713.3384314"}}, "content": {"source": {"pdf_hash": "bb77dbdc71a14069317d934e2923620aea2ca709", "pdf_src": "ACM", "pdf_uri": "[\"https://arxiv.org/pdf/1910.06151v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://authors.library.caltech.edu/108232/1/3357713.3384314.pdf", "status": "GREEN"}}, "grobid": {"id": "bdb2bcf2471a53436d7d199cc3ffc8d8139dd88b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bb77dbdc71a14069317d934e2923620aea2ca709.txt", "contents": "\nSampling-Based Sublinear Low-Rank Matrix Arithmetic Framework for Dequantizing Quantum Machine Learning\nJune 22-26, 2020\n\nNai-Hui Chia \nAndr\u00e1s Gily\u00e9n agilyen@caltech.edu \nTongyang Li tongyang@cs.umd.edu \nHan-Hsuan Lin linhh@cs.utexas.edu \nEwin Tang ewint@cs.washington.edu \nChunhao Wang chunhao@cs.utexas.edu \nNai-Hui Chia \nAndr\u00e1s Gily\u00e9n \nTongyang Li \nHan-Hsuan Lin \nEwin Tang \nChunhao Wang \n\nUniversity of Texas at Austin Austin\nTexasUSA\n\n\nCalifornia Institute of Technology Pasadena\nCaliforniaUSA\n\n\nUniversity of Maryland College Park\nMarylandUSA\n\n\nUniversity of Texas at Austin Austin\nTexasUSA\n\n\nUniversity of Washington Seattle\nWashingtonUSA\n\n\nACM Reference Format\nUniversity of Texas at Austin Austin\nTexasUSA\n\nSampling-Based Sublinear Low-Rank Matrix Arithmetic Framework for Dequantizing Quantum Machine Learning\n\nSTOC '20\nChicago, IL, USAJune 22-26, 202010.1145/3357713.3384314ACM ISBN 978-1-4503-6979-4/20/06. . . $15.00CCS CONCEPTS \u2022 Theory of computation \u2192 Quantum computation theoryMachine learning theorySketching and sampling KEYWORDS quantum machine learning, low-rank approximation, sampling, quantum-inspired algorithms, quantum machine learning, dequan- tization\nWe present an algorithmic framework for quantum-inspired classical algorithms on close-to-low-rank matrices, generalizing the series of results started by Tang's breakthrough quantum-inspired algorithm for recommendation systems [STOC'19]. Motivated by quantum linear algebra algorithms and the quantum singular value transformation (SVT) framework of Gily\u00e9n et al. [STOC'19], we develop classical algorithms for SVT that run in time independent of input dimension, under suitable quantum-inspired sampling assumptions. Our results give compelling evidence that in the corresponding QRAM data structure input model, quantum SVT does not yield exponential quantum speedups. Since the quantum SVT framework generalizes essentially all known techniques for quantum linear algebra, our results, combined with sampling lemmas from previous work, suffices to generalize all recent results about dequantizing quantum machine learning algorithms. In particular, our classical SVT framework recovers and often improves the dequantization results on recommendation systems, principal component analysis, supervised clustering, support vector machines, low-rank regression, and semidefinite program solving. We also give additional dequantization results on low-rank Hamiltonian simulation and discriminant analysis. Our improvements come from identifying the key feature of the quantum-inspired input model that is at the core of all prior quantum-inspired results: \u2113 2 -norm sampling can approximate matrix products in time independent of their dimension. We reduce all our main results to this fact, making our exposition concise, self-contained, and intuitive.\n\nINTRODUCTION 1.Motivation\n\nQuantum machine learning (QML) is a relatively new field of study with a rapidly growing number of proposals for how quantum computers could significantly speed up machine learning tasks [10,19]. If any of these proposals yield substantial practical speedups, it could be the killer application motivating the development of scalable quantum computers [38]. Many of the proposals are based on Harrow, Hassidim, and Lloyd's algorithm (HHL) for solving sparse linear equation systems in time poly-logarithmic in input size [25]. However, QML applications are less likely to admit exponential speedups in practice compared to, say, Shor's algorithm for factoring [42], because unlike their classical counterparts, QML algorithms must make strong input assumptions and learn relatively little from their output [1]. These caveats arise because both loading input data into a quantum computer and extracting amplitude data from an output quantum state are hard in their most generic forms.\n\nA recent line of research analyzes the speedups of QML algorithms by developing classical counterparts that carefully exploit these restrictive input and output assumptions. This began with a breakthrough 2018 paper by Tang [45] showing that the quantum recommendation systems algorithm [29], previously believed to be one of the strongest candidates for a practical exponential speedup in QML, does not give an exponential speedup. Specifically, Tang described a \"dequantized\" algorithm that solves the same problem as the quantum algorithm and only suffers from a polynomial slowdown. Tang's algorithm crucially exploits the structure of the input assumed by the quantum algorithm, which is used for efficiently preparing states. Subsequent work relies on similar techniques to dequantize a wide range of QML algorithms, including those for principal component analysis and supervised clustering [44], lowrank linear system solving [9,21], low-rank semidefinite program solving [8], support vector machines [13], nonnegative matrix factorization [7], and minimal conical hull [18]. These results show that the advertised exponential speedups of many QML algorithms disappear if the corresponding classical algorithms can use input assumptions analogous to the state preparation assumptions of the quantum algorithms. Previous papers [9,21,44] have observed that these techniques can likely be used to dequantize all QML that operates on low-rank data. Apart from a few QML algorithms that assume sparse input data [25], much of QML depends on some low-rank assumption. As a consequence, these dequantization results have drastically changed our understanding of the landscape of potential QML algorithm speedups, by either providing strong barriers for or completely disproving the existence of exponential quantum speedups for the corresponding QML problems.\n\nRecent works on quantum algorithms use the primitive of singular value transformation to unify many quantum algorithms ranging from quantum walks to QML, under a quantum linear algebra framework called quantum singular value transformation (QSVT) [6,22,34]. Since this framework effectively captures all known linear algebraic QML techniques, a natural question is what aspects of this framework can be dequantized. Understanding the quantum-inspired analogue of QSVT promises a unification of dequantization results and more intuition about potential quantum speedups, which helps to guide future quantum algorithms research.\n\n\nMain Results\n\nOur work gives a simple framework of quantum-inspired classical algorithms with wide applicability, grasping the capabilities and limitations of these techniques. We use this framework to dequantize many quantum linear algebra algorithms, including QSVT with certain input models. We give an overview of our results here, deferring proofs to the full version of this paper.\n\nSampling and query access model. Our framework assumes a specific input model called sampling and query access, which can be thought of as a classical analogue to quantum state preparation assumptions, i.e., the ability to prepare a state |v\u27e9 proportional to some input vector v. If we have sampling and query access to a vector v \u2208 C n , denoted SQ(v), we can efficiently make the following kinds of queries (Definition 2.5): (1) given an index i \u2208 [n], output the corresponding entry v(i); (2) sample an index j \u2208 [n] with probability |v(j)| 2 /\u2225v \u2225 2 ; and (3) output the vector's \u2113 2 -norm \u2225v \u2225. If we have sampling and query access to a matrix A \u2208 C m\u00d7n , denoted SQ(A), we have SQ(A(i, \u00b7)) for all rows i and also SQ(a) for a the vector of row norms (i.e., a(i) := \u2225A(i, \u00b7)\u2225).\n\nTo motivate this definition, we make the following observations about this input model. First, this model naturally admits classical algorithms with similar properties to the corresponding QML algorithms. Second, as far as we know, if input data is given classically, 1 classical algorithms in the sampling and query model can be run whenever the corresponding algorithms in the quantum model can (Remark 2.13). For example, if input is loaded in the QRAM data structure, as commonly assumed in QML in order to satisfy state preparation assumptions [10,37], then we have log-time sampling and query access to it. Consequently, a fast classical algorithm for a problem in this classical model implies lack of quantum speedup for the problem.\n\nMatrix arithmetic. We make a conceptual contribution by defining the slightly more general notion of oversampling and query access to a vector or matrix (Definition 2.7), where we have sampling and query access to another vector/matrix that gives an entry-wise upper bound on the absolute values of the entries of the actual vector/matrix, which we can only query. With this definition comes the insight that this input model is closed under arithmetic operations. Though this closure property comes into play relatively little in applications to dequantizing QML, the essential power of quantuminspired algorithms lies in its ability to use sampling and query access to input matrices to build oversampling and query access to increasingly complex arithmetic expressions on input, possibly with some approximation error, without paying the (at least) linear time necessary to compute such expressions in conventional ways. As a simple example, if we have oversampling and query access to matrices A (1) , . . . , A (t ) , we have oversampling and query access to linear combinations t i=1 \u03bb i A (i) as well (Lemma 2.12). The \"oversampling\" input model is also closed under (approximate) matrix products -the key technique underlying our main results. Such results have been known in the classical literature for some time [14]; we now give an example illustrating the flavor of main ideas. Suppose we are given sampling and query access to two matrices A \u2208 C m\u00d7n and B \u2208 C m\u00d7p , and desire (over)sampling and query access to A \u2020 B. A \u2020 B is a sum of outer products A(i, \u00b7) \u2020 B(i, \u00b7), so we can randomly sample them to get a good estimator for A \u2020 B. We can use SQ(A) to pull samples i 1 , . . . , i s according to the row norms of A, a distribution we will denote p (so p(i) = \u2225A(i, \u00b7)\u2225 2 /\u2225A\u2225 2 F ). Consider Z := 1\ns s k=1 1 p(i k ) A(i k , \u00b7) \u2020 B(i k , \u00b7). Z is an unbiased estimator of A \u2020 B: E[Z (i, j)] = 1 s s k =1 n \u2113=1 p(\u2113) A(\u2113, i) \u2020 B(\u2113, j) p(\u2113) = [A \u2020 B](i, j).\nFurther, the variance of this estimator is small. In the following computation, we consider s = 1, because the variance for general s decreases as 1/s.\nE[\u2225A \u2020 B \u2212 Z \u2225 2 F ] \u2264 i,j E[|Z (i, j)| 2 ] = i,j,\u2113 p(\u2113) |A(\u2113, i)| 2 |B(\u2113, j)| 2 p(\u2113) 2 = \u2113 1 p(\u2113) \u2225A(\u2113, \u00b7)\u2225 2 \u2225B(\u2113, \u00b7)\u2225 2 = \u2113 \u2225A\u2225 2 F \u2225B(\u2113, \u00b7)\u2225 2 = \u2225A\u2225 2 F \u2225B\u2225 2 F .\nDue to Chebyshev's inequality, we can approximate A \u2020 B by a relatively small linear combination of outer products of rows of A and the corresponding rows of B with high success probability. Moreover, if we have SQ(A) and SQ(B), then we also have (over)sampling and query access to the outer products (Lemma 2.11). Using that oversampling and query access is closed under taking linear combinations (Lemma 2.12), this also yields oversampling and query access to Z \u2248 A \u2020 B. In our applications we will keep Z as an outer product A \u2032 \u2020 B \u2032 for convenience. Nevertheless, our central tool will be an approximate matrix product protocol: see the key lemma in Section 1.4. Note that the discussion so far suggests that for a matrix A we need (over)sampling and query access to both A and A \u2020 . In fact, we show in the full version that having either one of them suffices.\n\nSo far, we have shown that if we have (over)sampling and query access to our vectors and matrices, we can perform ordinary linear algebra operations, i.e. matrix arithmetic. We will leverage our approximate matrix product protocol to add matrix functions to our toolkit: given oversampling and query access to an input matrix, we can get oversampling and query access to an approximation of a (Lipschitz) function applied to that matrix. Therefore, one can think about oversampling and query access as a classical analogue to the quantum block-encodings in quantum singular value transformation [22], which support linear combinations, products, and low-degree polynomials (that is, approximations of Lipschitz functions) of input matrices.\n\nEven singular value transformation. Our main result is that, given (over)sampling and query access to an input matrix A \u2208 C m\u00d7n , we can find a succinct an efficient description of an even singular value transformation of A. This primitive is based on the even SVT used by Gily\u00e9n et al. [22]: given a function f : [0, \u221e) \u2192 C, the even SVT\nis 2 f ( \u221a A \u2020 A)\n, applying f to the singular values of A and replacing left singular vectors with the corresponding right singular vectors\n(so if A = \u03c3 i u i v \u2020 i is the singular value decomposition of A, then f ( \u221a A \u2020 A) = f (\u03c3 i )v i v \u2020 i ).\nThe primitive of singular value transformation has been shown to generalize a large portion of quantum machine learning algorithms [22]; we bring this observation into the quantum-inspired landscape.\n\nMain theorem (informal version of Theorem 3.2). Suppose we are given sampling and query access to a matrix A \u2208 C m\u00d7n (that is, SQ(A)) and a function f : [0, \u221e) \u2192 C such that f andf (x) := (f (x) \u2212 f (0))/x are L-Lipschitz and L \u2032 -Lipschitz, respectively. Then, for sufficiently small \u03b5, \u03b4 > 0, we can find a subset of (normalized) rows of A, R \u2208 C r \u00d7n , and a subset of (normalized) columns of R, C \u2208 C r \u00d7c such that\nPr \u2225R \u2020f (CC \u2020 )R + f (0)I \u2212 f (A \u2020 A)\u2225 > \u03b5 < \u03b4 .\nLet T be the time the sampling and query oracle takes to respond. Then finding R and C and computingf\n(CC \u2020 ) takes O r 2 c + rcT time, where r =\u0398 L 2 \u2225A\u2225 2 \u2225A\u2225 2 F \u03b5 2 log 1 \u03b4 c =\u0398 L \u20322 \u2225A\u2225 6 \u2225A\u2225 2 F \u03b5 2 log 1 \u03b4 .\nWe call R \u2020f (CC \u2020 )R an RUR decomposition because R \u2208 C r \u00d7n is a subset of rows of the input matrix (R corresponds to the 'R' of the RUR decomposition, andf (CC \u2020 ) \u2208 C r \u00d7r corresponds to the 'U'). More precisely, an RUR decomposition expresses a desired matrix as a linear combination of outer products of rows of the input matrix. 3 The matrix U encodes the coefficients in the linear combination. 2 For a Hermitian matrix H and a function f : R \u2192 C, f (H ) denotes applying f to the\neigenvalues of H . That is, f (H ) := n i =1 f (\u03bb i )v i v \u2020 i\n, for \u03bb i and v i the eigenvalues and eigenvectors of H . 3 This is the relevant variant of the notion of a CUR decomposition from the randomized numerical linear algebra and theoretical computer science communities [17].\n\nWe want our output in the form of an RUR decomposition, since we can describe such a decomposition implicitly just as a list of row indices and some additional coefficients, which avoids picking up a dependence on m or n in our runtimes. Further, having SQ(A) implies that we can exploit the RUR structure to gain oversampling and query access to the output matrix, enabling the evaluation of matrix-vector expressions. In particular, for an RUR decomposition, we can get oversampling and query access to approximations of R \u2020 U Rb and R \u2020 U RMb, for a matrix M \u2208 C n\u00d7n and a vector b \u2208 C n , in time independent of n.\n\nMore general results follow as corollaries of our main result on even SVT. For an arbitrary matrix A with SQ(A) and SQ(A \u2020 ) access oracles, 4 we can perform generic (non-even) SVT (Theorem 3.4), where the output is given as an approximate CUR decomposition expressing the desired matrix as a linear combination of outer products of columns and rows of A. We can also perform eigenvalue transformation on Hermitian matrices (Theorem 3.5), where the output is given as an approximate RUR decomposition. Given an RUR (or CUR) decomposition, one can also approximately diagonalize the matrix U in order to recover an approximate eigenvalue decomposition (or SVD) of the desired matrix, see e.g. Theorem 3.5.\n\nHowever, using only our main theorem about even SVT, we can directly recover most existing quantum-inspired machine learning algorithms without using the more advanced Theorems 3.4 and 3.5 discussed above, yielding faster dequantization for QML algorithms. In Section 1.3, we outline our results recovering such applications.\n\nFor some intuition on error bounds and time complexity, we consider how the parameters in our main theorem behave in a restricted setting: suppose that A has minimum singular value \u03c3 and \u2225A\u2225 F /\u03c3 is dimension-independent. 5 This condition simultaneously bounds the rank and condition number of A. Further suppose 6 that f 's Lipschitz constant satisfies\nL\u2225A\u2225 2 < C max x ,y \u2208[0, \u2225A \u2225 2 ] | f (x) \u2212 f (y)|\nfor some dimension-independent C. Note that C must be at least 1, therefore such an f is at most C-times \"steeper\" compared to the least possible \"steepness\". Under these assumptions, we can get an RUR decomposition to additive error (\u03b5 max x ,y \u2208[0, \u2225A \u2225 2 ] | f (x) \u2212 f (y)|) in runtime independent of dimensions (i.e., r, c are dimensionless). The precise runtime is\nO \u2225A\u2225 6 F \u2225A\u2225 2 \u03c3 4 C 6 \u03b5 6 log 3 1 \u03b4 .\nDependence on \u03c3 arises because we bound L \u2032 \u2264 L/\u03c3 2 : our algorithm's dependence on L \u2032 implicitly enforces a low-rank constraint in this case. All of our analyses give qualitatively similar results to this, albeit in more general settings allowing approximately lowrank input.\n\nImplications for quantum singular value transformation. Gily\u00e9n et al.'s QSVT framework [22] assumes that the input matrix A is given by a block-encoding, which is a quantum circuit implementing a unitary transformation whose top-left block contains (up to scaling) A itself [34]. Given a block-encoding of A, one can apply certain kinds of degree-d polynomials of A to an input quantum state, incurring only about d times the implementation cost of the input block-encoding. One can get a block-encoding of an input matrix A through various methods. If A is s-sparse with efficiently computable elements and \u2225A\u2225 \u2264 1, then one can directly get a blockencoding of A/s [22, Lemma 48]. If A is in the QRAM data structure (used for efficient state preparation for QML algorithms [37]), one can directly get a block-encoding of A/\u2225A\u2225 F [22,Lemma 50]. This latter normalization means that QRAM-based QSVT has an implicit dependence on the Frobenius norm \u2225A\u2225 F . This dependence on \u2225A\u2225 F suggests lack of exponential speedup for QRAM-based QSVT, since \u2225A\u2225 F is the key parameter in the complexity of our corresponding classical algorithms. This is in contrast to sparsitybased QSVT, which instead has dependence on \u2225A\u2225 and the sparsity s, and generalizes algorithms like HHL that solve BQP-complete problems.\n\nOur results give compelling evidence that there is indeed no exponential speedup for QRAM-based QSVT, and show that oversampling and query access can be thought of as a classical analogue to block-encodings in the bounded Frobenius norm regime. Indeed, if we are given matrices and vectors in the QRAM data structure, then by converting them to block-encodings, we can apply any function to the input that can be obtained by composing addition, scalar multiplication, matrix multiplication, and singular value transformation. Since this data structure gives us sampling and query access to input, we can classically approximately evaluate the same types of expressions.\n\nIn particular, we show that we can apply the singular value transform of a matrix A \u2208 C m\u00d7n satisfying \u2225A\u2225 F = 1 to b \u2208 C n in QRAM (Theorem 3.7). Our algorithm simulates sampling and query access to v := p (QV) (A)b up to \u03b5 \u2225v \u2225 error in poly(d, 1 \u03b5 , \u2225b \u2225 \u2225v \u2225 , log mn) time, where p(x) is a degree-d polynomial of the kind QSVT can apply and p (QV) (A) is the type of SVT that QSVT performs on A (Definition 3.6). This runtime is only polynomially slower than the corresponding quantum algorithm, except in the \u03b5 parameter. 7 Theorem 3.7 also dequantizes QSVT for block-encodings derived from (purifications of) density operators [22,Lemma 45] that come from some well-structured classical data. The situation in this case is even nicer, since density operators are already normalized. This gives evidence that QSVT with these kinds of block-encodings do not give inherent exponential speedups (though, if input preparation/output analysis protocols have no classical analogues, they can play a part in an algorithm achieving an exponential speedup). QSVT using other types of block-encodings (with potentially large Frobenius norm) remains intact. 7 The QML algorithms we discuss generally only incur polylog( 1 \u03b5 ) terms, but need to eventually pay poly(1/\u03b5 ) to extract information from output quantum states. So, we believe this exponential speedup is artificial. See the open questions section for more discussion of this error parameter.\n\n\nApplications: Dequantizing QML & More\n\nWith our main results, we can recover existing quantum-inspired algorithms for recommendation systems [45], principal component analysis [44], supervised clustering [44], support vector machines [13], low-rank matrix inversion [9,21], and semidefinite program solving [8]. We also propose new quantum-inspired algorithms for low-rank Hamiltonian simulation and discriminant analysis (dequantizing the quantum algorithm of Cong & Duan [11]). Our framework achieves these results with a conceptually simple analysis, and often admits faster and more general results.\n\nFor the following results, we assume our sampling and query access to the input takes O(1) time. There are data structures that can support such queries (Remark 2.13), and if the input is in QRAM, the runtime only increases by at most a factor of log of input size. We note here that, though our outputs are often in the form of oversampling and query access SQ \u03d5 (Definition 2.7), via rejection sampling, one can think about this access as the same as sampling and query access, except one can only compute the norm up to some relative error (Lemma 2.8).\n\nRecommendation systems. Our framework gives a simpler and faster variant of Tang's dequantization [45] of Kerenidis & Prakash's quantum recommendation systems [29]. This result is notable for being the first result in this line of work and for dequantizing what was previously believed to be the strongest candidate for practical exponential quantum speedups for a machine learning problem [38]. The task is as follows: given sampling and query access to a matrix A \u2208 R m\u00d7n , a row index i \u2208 [m], and a singular value threshold \u03c3 , sample from the i th row of some\u00c2 \u2208 R m\u00d7n , where\u00c2 is a \u03c3 -thresholded low-rank approximation of A. Specifically,\u00c2 is \u03b5 \u2225A\u2225 F -close in additive Frobenius norm error to a singular value transform of A that is smoothly thresholded to keep only singular vectors with value at least \u03c3 .\n\nWe can rewrite our target low-rank approximation as A \u00b7 t(A \u2020 A), where t is a step function that is zero for x \u2264 5 6 \u03c3 2 , one for x \u2265 7 6 \u03c3 2 , and a linear interpolation between the two for x \u2208 [ 5 6 \u03c3 2 , 7 6 \u03c3 2 ]. In other words, our low-rank approximation is A multiplied by a smoothened projector. We can use our main theorem Theorem 3.2 to approximate t(A \u2020 A) by some R \u2020 U R. Then, the i th row of our low-rank approximation is A(i, \u00b7)R \u2020 U R, which is a product of a vector with an RUR decomposition. Thus, using previously-discussed matrix arithmetic lemmas, we have SQ \u03d5 (A(i, \u00b7)R \u2020 U R), so we can get the sample from this row as desired.  [44]), our algorithm trivially recovers this result. Given a dataset of points q 1 , . . . , q n\u22121 \u2208 R d , the goal is to estimate the distance between their centroid and a new point p \u2208 R d , \u2225p \u2212 1 n\u22121 (q 1 + \u00b7 \u00b7 \u00b7 + q n\u22121 )\u2225 2 . We can reduce this problem to estimating wM(wM) \u2020 to \u03b5 additive error, for a certain choice of vector w \u2208 R n and M \u2208 R n\u00d7d . This can be done with a simple inner product estimation procedure in time\nO Z 2 \u03b5 2 log 1 \u03b4 , where Z = \u2225M \u2225 2 F \u2225w \u2225 2 = 4(\u2225p\u2225 2 + 1 n\u22121 n\u22121 i=1 \u2225q i \u2225 2 ).\nPrincipal component analysis. Our framework improves on Tang's dequantization [44] of the quantum principal component analysis (qPCA) algorithm [33]. Since the actual task being solved by the original quantum algorithm is underspecified, we describe the task as is performed in the dequantization. Given a matrix SQ(X ) \u2208 C m\u00d7n such that X \u2020 X has top k eigenvalues\n{\u03bb i } k i=1 and eigenvectors {v i } k i=1 , the goal is to compute eigenvalue estimates {\u03bb i } k i=1 such that |\u03bb i \u2212\u03bb i | \u2264 \u03b5 Tr(X \u2020 X ) and eigenvector estimates {SQ \u03d5 (v i )} k i=1 such that \u2225v i \u2212 v i \u2225 \u2264 \u03b5.\nTo avoid degeneracy conditions, we must have a gap assumption granting\n|\u03bb i \u2212\u03bb i+1 | \u2265 \u03b7\u2225X \u2225 2 for all i \u2208 [k].\nThen, we can approach the problem as follows. First, we use that an importance-sampled submatrix of X has approximately the same singular values as X itself to get our estimates\n{\u03bb i } k i=1 . With these estimates, we can define smoothened step functions f i for i \u2208 [k] such that f i (X \u2020 X ) = v \u2020 i v i .\nWe can then use our main theorem to find an RUR decomposition for f i (X \u2020 X ). We use additional properties of the RUR description to argue that it is indeed a rank-1 outer productv \u2020 iv i , which is our desired approximation for the eigenvector. We have sampling and query access tov i because it is R \u2020 x for some vector x. Altogether, this algorithm runs in time\nO \u2225A \u2225 6 F \u2225A \u2225 2 \u03c3 4 \u03b5 \u22126 \u03b7 \u22126 log 3 k \u03b4 , a major improvement over the original dequantization's runtime O \u2225A \u2225 36 F \u03c3 24 \u2225A \u2225 12 \u03b5 \u221212 \u03b7 \u22126 log 3 k \u03b4 .\nMatrix inversion. Our framework can generalize a pair of results giving quantum-inspired versions of low-rank matrix inversion [9,21]. Given a matrix SQ(A) \u2208 C m\u00d7n and a vector SQ(b) \u2208 C m , the goal is to obtain SQ \u03d5 (A + \u03c3 ,\u03b7 b) where A + \u03c3 ,\u03b7 is a pseudo-inverse of A smoothly thresholded to invert only the singular values that are at least \u03c3 .\n\nWe\ncan rewrite A + \u03c3 ,\u03b7 b = \u03b9(A \u2020 A)A \u2020 b for \u03b9 a function encoding a thresholded inverse. Namely, \u03b9(x) = 1/x for x \u2265 \u03c3 2 , \u03b9(x) = 0 for x \u2264 (1 \u2212 \u03b7)\u03c3 2 ,\nand is a linear interpolation between the endpoints for x \u2208 [(1 \u2212 \u03b7)\u03c3 2 , \u03c3 2 ]. By our main theorem, we can find an RUR decomposition for \u03b9(A \u2020 A), from which we can then get SQ(R \u2020 U RA \u2020 b) via sampling techniques. Altogether, this algorithm\ntakes O \u2225A \u2225 6 F \u2225A \u2225 22\n\u03c3 28 \u03b7 6 \u03b5 6 log 3 1 \u03b4 time with no restriction on A, whereas the result of [21] applies to strictly rank-k A and gets the incompa-\nrable runtime O \u2225A \u2225 6 F k 6 \u2225A \u2225 16 \u03c3 22 \u03b7 6 \u03b5 6 log 3 1 \u03b4 .\nSupport vector machines. We use our framework to dequantize Rebentrost, Mohseni, and Lloyd's quantum support vector machine [39], which was previously noted to be possible by Ding, Bao, and Huang [13]. The idea is to find a hyperplane best explaining m data points in a matrix SQ(X ) \u2208 R m\u00d7n with labels SQ(y) \u2208 {\u00b11} m . With regularization, this reduces to approximately solving the linear system\n0 \u00ec 1 \u2020 \u00ec 1 XX \u2020 + \u03b3 \u22121 I b \u03b1 = 0 y .\nCall the above matrix F , and letF := F /Tr(F ). The quantum algorithm approximately solves the linear system by applyingF + \u03bb,\u03b7 to y. So, our goal is to output\nSQ \u03d5 (v) for v \u2208 R m+1 satisfying \u2225v \u2212F + \u03bb,\u03b7 [ 0 y ]\u2225 \u2264 \u03b5 \u2225F + \u03bb,\u03b7 [ 0 y ]\u2225.\nTo do this, we use our matrix arithmetic techniques in order to get oversampling and query access to SQ \u03c6 (F ) from SQ(X ). Then, using SQ \u03c6 (F ), we run the quantuminspired matrix inversion algorithm discussed above, immediately giving us the desired v. This takes time O \u03bb \u221228 \u03b7 \u22126 \u03b5 \u22126 log 3 1 \u03b4 . We solve the problem in the same generality as the original quantum algorithm, unlike the prior dequantization result [13], which also lacks explicit error bounds or runtime bounds; the paper simply argues that the algorithm is polynomial time in the right parameters.\n\nHamiltonian simulation. Our framework can be used to give a Hamiltonian simulation algorithm for low-rank Hamiltonians. Given a Hermitian matrix SQ(H ) \u2208 C n\u00d7n such that \u2225H \u2225 \u2264 t and\n\u2225H + \u2225 \u2264 1/\u03c3 along with a unit vector SQ(b) \u2208 C n , the goal is to obtain SQ \u03d5 (v) where \u2225v \u2212 e iH b \u2225 F \u2264 \u03b5.\nIn order to use our even SVT result, we split our desired transformation into even and odd parts: e ix = cos(x) + i sin(x) = cos(x) + i sinc(x)x. We use even singular value transformation to apply the even functions cos and sinc; for an even function \u0434(x),\nlet f \u0434 (x) := \u0434( \u221a x), so that \u0434(H ) = f \u0434 (H \u2020 H ) and we can rewrite e iH b = f cos (H \u2020 H )b + i f sinc (H \u2020 H )H \u2020 b.\nThen, using our main theorem, we can find RUR decompositions for both even SVTs, gaining sampling and query access to the matrix-vector products for the even and odd parts of the expression, from which sampling and query access to our estimate of e iH b follows. This takes O \u2225H \u2225 6 F t 10 \u03c3 16 t 6 \u03b5 \u22126 log 3 1 \u03b4 time, which is dimension-independent if we think of the desired error as t\u03b5, the natural choice for additive error. This algorithm also works if H is not strictly low-rank, in which case the output will be a version of e iH where eigenvalues \u2264 \u03c3 are thresholded away. We also provide a version of this algorithm that works for all H without a dimensionindependent runtime. This version gets improved runtimes when t = 1.\n\nSemidefinite program (SDP) solving. We solve the problem of SDP-feasibility, improving on prior work of Chia et al. [8] dequantizing some versions of quantum SDP solvers [2,5]. Given m \u2208 N, b 1 , . . . , b m \u2208 R, and Hermitian matrices\nA (1) , . . . , A (m) such that \u2212I \u2aaf A (i) \u2aaf I for all i \u2208 [m], let S \u03b5 be the set of all X satisfying Tr[A (i) X ] \u2264 b i + \u03b5 \u2200 i \u2208 [m]; X \u2ab0 0; Tr[X ] = 1.\nThe task is to differentiate whether S 0 (in which case the output should be an X \u2208 S \u03b5 ) or S \u03b5 = (in which case the output should be \"infeasible\"). Note that general SDPs can be reduced to this feasibility problem via a simple binary search.\n\nBy using the matrix multiplicative weights (MMW) method [3], SDP \u03b5-feasibility reduces to estimating Tr[A (i) X ] up to \u03b5/4 error given SQ(A (i) ) for all i \u2208 [m] and X implicitly defined as a Gibbs state Discriminant analysis. We present a new dequantized algorithm, a classical analogue to Cong and Duan's quantum discriminant analysis algorithm [11]. The high-level idea is to find the vectors that best explain the way data points are classified. Cong and Duan reduces this idea to the following task: given matrices SQ(B) and SQ(W ), find eigenvectors and eigenvalues of\nX := exp[\u2212A] Tr(exp[\u2212A]) where A := \u03b5 4 \u2272ln(n)/\u03b5 2 \u03c4 =1 A (j \u03c4 ) .\u221a W \u2020 W (B \u2020 B) \u22121 \u221a W \u2020 W.\nThey solve a version of this task where one only needs to output approximate eigenvectors and one can ignore the singular vectors of B and W that are smaller than a parameter \u03c3 .\n\nWe achieve this goal by using Theorem 3.2 to approximate\n\u221a W \u2020 W \u2248 R \u2020 W U W R W and (B \u2020 B) \u22121 \u2248 R \u2020 B U B R B by RUR decom- positions. Then, we use Lemma 3.1 to approximate R W R \u2020 B by small matrices R \u2032 W R \u2032 \u2020 B by small submatrices.\nThis yields an approximate RUR decomposition of the matrix whose eigenvalues and vectors we want to find:\nR \u2020 W (U W R \u2032 W R \u2032 \u2020 B U B R \u2032 B R \u2032 \u2020 W U W )R W .\nFinding eigenvectors from an RUR decomposition follows from an observation: for a matrix C W formed by sampling columns from R W (using SQ(W )), and [C W ] k the rank-k approximation to C W (which can be computed because C W has size independent of dimension), \n(([C W ] k ) + R W ) \u2020 isU R W = R \u2020 W (C + k ) \u2020 C \u2020 k UC k C + k R W ,\nwhich holds by choosing k sufficiently large and choosing C to 8 Here we use \u2225A (\u00b7) \u2225 * := max i \u2208[m] \u2225A (i ) \u2225 * . Note that this bound does not appear to be dimension-independent due to the normalizing assumption \u2225A (\u00b7) \u2225 \u2264 1. If we would relax this assumption, then we could get a dimension-independent bound corresponding to precision \u03b5 \u2225A (\u00b7) \u2225, by replacing \u2225A (\u00b7) \u2225 F with the \"stable rank\" \u2225A (\u00b7) \u2225 F / \u2225A (\u00b7) \u2225. Then the resulting runtime bound is dimension-independent apart from the ln(n) factors, that come from MMW. be the same sketch used for U . Then, we can compute the SVD as C + k U (C + k ) \u2020 = V DV \u2020 which gives us an approximate SVD for R \u2020 W U R W : the eigenvectors are (C + k R W ) \u2020 V , and the eigenvalues are the diagonal entries of D. We show that this has the approximation properties analogous to the quantum algorithm. Our algorithm runs\nin O \u2225B \u2225 4 \u2225B \u2225 6 F \u03b5 6 \u03c3 10 + \u2225W \u2225 10 \u2225W \u2225 6 F \u03b5 6 \u03c3 16 log 3 1 \u03b4 time.\nWhat else is there? Though we have presented many dequantized versions of QML algorithms, the question remains of what QML algorithms don't have such versions. That is, what algorithms still have the potential to give exponential speedups? Because QSVT generalizes essentially all known quantum linear algebra techniques, we restrict our focus to algorithms in that framework. As we noted previously, we only demonstrate lack of exponential speedup for QSVT with block-encodings coming from QRAM and density operators. Other kinds of block-encodings, such as those coming from sparsity assumptions, remain impervious to our techniques. The most well-known quantum linear algebra algorithms of this \"dequantization-resistant\" type are HHL [25] and its derivatives. Sparse matrix inversion is BQP-complete, which explains why our techniques leave these speedups untouched. Nevertheless HHL has serious caveats, as noted by Aaronson [1]. In particular, HHL only gives an exponential speedup when the condition number of the input matrix is poly-logarithmic in dimension, which doesn't happen in typical datasets. This constraint hamstrings most attempts to apply HHL to practical problems, especially when combined with the typical QML constraints that quantum algorithms need quantum states as input and often can only give quantum states as output. Work like Zhao et al. on Gaussian process regression [50] and Lloyd et al. on topological data analysis [31] attempt to address these issues to get a super-polynomial quantum speedup.\n\n\nTechniques\n\nPlacing sampling and query access in the sketching context. As we will see below, the fundamental idea of quantum-inspired algorithms is to reduce dimensionality of input matrices to speed up linear algebra computations. So, using sketching techniques is natural here. Recall that the fundamental difference between quantum-inspired algorithms and traditional sketching algorithms is that we assume that we can perform measurements of states corresponding to input in time independent of input dimension (that is, we have efficient sampling and query access to input), and in exchange want algorithms that run in time independent of dimension. The kind of samples we get from sampling and query access is usually called importance sampling or length-square sampling in classical literature.\n\nThe quantum-inspired model is weaker than the standard sketching algorithm model (Remark 2.13): an algorithm taking T time in the quantum-inspired model for an input matrix A can be con- This viewpoint could be advantageous in some cases, for example in some streaming scenario [29]. Nevertheless, our primary motivation here is not to develop better generic sketching algorithms, but to better understand the scope of problems facilitating large quantum speed-ups.\n\nA natural question is whether more modern types of sketches can be used in our model. After all, importance sampling is only one of many sketching techniques studied in the large literature on sketching algorithms. Notably, though, other types of sketches seem to fail in the input regimes where quantum machine learning succeeds: assuming sampling and query access to input, importance sampling takes time independent of dimension, whereas other randomized linear algebra methods such as Count-Sketch, Johnson-Lindenstrauss, and leverage score sampling all still take time linear in input-sparsity.\n\nFurthermore, importance sampling is highly compatible with quantum-like algorithms: given the ability to query entries and obtain importance samples of the input, we can query entries and obtain importance samples of the output, analogously to the way quantum machine learning algorithms move from an input quantum state to an output quantum state. This insight unlocks surprising power in importance sampling. For example, it reveals that Frieze, Kannan, and Vempala's low-rank approximation algorithm (FKV) [20], which, as stated, requires O(kmn) time to output the desired matrix, actually can produce useful results (samples and entries) in time independent of input dimension. Our goal is to develop a framework that demonstrates what can be done with importance sampling and establishes a classical frontier for quantum algorithms to push past.\n\nImportance sampling to even singular value transformation. The fundamental property of importance sampling is its ability to efficiently approximate matrix products (and by extension, vectors and higher-order tensors). This is our key lemma, which states that if we have sufficient access to two matrices, we can approximate their product by a product of matrices of smaller dimension:\n\nKey lemma [14] (informal version of Lemma 3.1). Suppose we are given SQ(X ) \u2208 C m\u00d7n and SQ(Y ) \u2208 C m\u00d7p . Then we can find normalized submatrices of X and Y , X \u2032 \u2208 C s\u00d7n and Y \u2032 \u2208 C s\u00d7p , in O(s) time for s = \u0398( 1 \u03b5 2 log 1 \u03b4 ), such that\nPr \u2225X \u2032 \u2020 Y \u2032 \u2212 X \u2020 Y \u2225 F \u2264 \u03b5 \u2225X \u2225 F \u2225Y \u2225 F > 1 \u2212 \u03b4 . We subsequently have O(s)-time SQ(X \u2032 ), SQ(X \u2032 \u2020 ), SQ(Y \u2032 ), SQ(Y \u2032 \u2020 ).\nPrior quantum-inspired algorithms [8,9,44,45] indirectly used this lemma by using FKV, which finds a low-rank approximation to the input matrix in the form of an approximate low-rank SVD and relies heavily on this lemma in the analysis. By using FKV once, one can gain access to singular values and right singular vectors; by using it twice, one can gain access to a full SVD. Then, by applying functions to the approximate singular values, one can argue that the resulting expression is close to the desired expression. One could theoretically use this procedure to give a classical algorithm for singular value transformation, but we prove our main results without going through the full analysis of the low-rank approximation.\n\nInstead, we use the key lemma twice to get an RUR decomposition of an even singular value transformation of the input (Theorem 3.2). Notice that, because we wish to run in time independent of dimension, the best we can do is to express the output based on the given input, as an RUR decomposition does. The proof of our main theorem is straightforward. Recall that, given SQ(A) \u2208 C m\u00d7n , we wish to approximate f (A \u2020 A) for f a function that, without loss of generality, satisfies f (0) = 0.\nf (A \u2020 A) \u2248 f (R \u2020 R) = R \u2020f (RR \u2020 )R \u2248 R \u2020f (CC \u2020 )R,\nwhere the first approximation follows from the key lemma with R \u2208 C r \u00d7n normalized rows of A, the equality follows fromf (x) = f (x)/x, and the second approximation follows from the key lemma with C \u2208 C r \u00d7c normalized columns of R. We then takef (CC \u2020 ) to be the \"U\" of our RUR decomposition, finding it by naively computing the SVD of C in O r 2 c time. The analysis is straightforward: we use that f andf are Lipschitz to argue that the error from approximating our matrix products propagates well. We also use a variant of the key lemma to give a spectral norm variant of the main theorem.\n\nThough this analysis is much simpler than FKV, it gives improved results in our applications. Our approach has several advantages. The reduction first given by Tang to get an SVT-based low-rank approximation bound from the standard notion of low-rank approximation [45, Theorem 4.7] induces a quadratic loss in precision, which appears to be only an artifact of the analysis. Also, FKV gives Frobenius norm error bounds, though for applications we often only need spectral norm bounds; our main theorem can get improved runtimes by taking advantage of the weaker spectral norm bounds. Finally, we take a reduced number of rows compared to columns, whereas FKV approximates the input by taking the same number of rows and columns.\n\nThe flexibility of singular value transformation also leads to easy generalization of results. For example, another important technical difference from previous work [8,9,21] is that our results do not assume that the input is strictly low-rank. Instead, following [22,45], our algorithms work on close-to-low-rank matrices by doing SVTs that smoothly threshold to only operate on large-enough singular values. That is, we implicitly take a low-rank approximation of the input before applying our singular value transformation.\n\nGeneral transformation results. We can bootstrap our algorithm for even SVT to get results for generic SVT (Theorem 3.4) and eigenvalue transformation (Theorem 3.5).\n\nFor generic SVT: consider a function f : R \u2192 C satisfying f (0) = 0 and a matrix A \u2208 C m\u00d7n . Given SQ(A) and SQ(A \u2020 ), we give an algorithm to output a CUR decomposition approximating f (SV) (A). Our strategy is to apply our main result Theorem 3.2 to \u0434(A \u2020 A), for \u0434(x) := f ( \u221a x)/ \u221a x, and subsequently approximate matrix products with Lemma 3.1 to get an approximation of the form A \u2032 R \u2032 \u2020 U R + \u0434(0)A:\nf (SV) (A) = A\u0434(A \u2020 A) \u2248 AR \u2020 U R + A(\u0434(0)I ) \u2248 A \u2032 R \u2032 \u2020 U R + \u0434(0)A.\nHere, A \u2032 R \u2032 \u2020 U R is a CUR decomposition as desired, since A \u2032 is a normalized subset of columns of A. One could further approximate \u0434(0)A by a CUR decomposition if necessary (e.g. by adapting the eigenvalue transformation result below). Some QML applications of even SVT look similar to this (e.g., matrix inversion and Hamiltonian simulation), but we can use the additional structure in these problems to do this kind of approximation better.\n\nAs for eigenvalue transformation, consider a function f : R \u2192 C and a Hermitian matrix H \u2208 C n\u00d7n , given SQ(H ). We wish to compute the eigenvalue transform f (H ). If f is even (so f (x) = f (\u2212x)), then f (H ) = f ( \u221a H \u2020 H ), so the result follows from our main theorem for even SVT.\n\nFor non-even f , we use a different strategy, similar to the one used for quantum-inspired semidefinite programming [8]: first we find the eigenvectors and eigenvalues of H and then apply f to the eigenvalues. Let \u03c0 (x) be a (smoothened) step function that is a linear interpolation between 0 and 1 on [0.5\u03b5 2 , \u03b5 2 ]. Then\nH \u2248 \u03c0 (HH \u2020 )H\u03c0 (H \u2020 H ) \u2248 R \u2020\u03c0 (CC \u2020 )RHR \u2020\u03c0 (CC \u2020 )R \u2248 R \u2020\u03c0 (CC \u2020 )M\u03c0 (CC \u2020 )R = R \u2020 (C \u03c3 C + \u03c3 ) \u2020\u03c0 (CC \u2020 )M\u03c0 (CC \u2020 )C \u03c3 C + \u03c3 R,\nwhere the second approximation follows from Theorem 3.2, the third approximation follows from the key lemma with M \u2248 RHR \u2020 , and C \u03c3 is the low-rank approximation of C formed by transforming C according to the \"filter\" function on x that is 0 for x < \u03c3 (< \u03b5) and\n\nx otherwise.\u00db := C + \u03c3 R \u2208 C c\u00d7n is close to an isometry, which we argue by showing (C + \u03c3 R)(C + \u03c3 R) \u2020 \u2248 I . We are nearly done now: since the rest of the matrix expression, C \u2020 \u03c3\u03c0 (CC \u2020 )M\u03c0 (CC \u2020 )C \u03c3 \u2208 C c\u00d7c , consists of submatrices of H of size independent of n, we can directly compute its unitary eigendecomposition U DU \u2020 . This gives the approximate decomposition H \u2248 (\u00dbU )D(\u00dbU ) \u2020 , with\u00dbU and D acting as approximate eigenvectors and eigenvalues of H , respectively. Some simple analysis shows that f (H ) \u2248 (\u00dbU )f (D)(\u00dbU ) \u2020 in the desired sense. Therefore, our output approximation of f (H ) comes in the form of an RUR decomposition that can be rewritten in the form of an approximate eigendecomposition.\n\n\nRelated Work\n\nOur work bridges the fields of randomized algorithms and quantum algorithms for linear algebra. Thus, we interact with a diverse body of related work.\n\nRandomized numerical linear algebra. Generally speaking, the techniques our framework uses belong to randomized linear algebra algorithms (see the surveys [35,48]). Our core primitive is importance sampling: see the survey by Kannan and Vempala [28] for algorithms using this type of sampling. In addition to the low-rank approximation algorithms [20] used in the quantum-inspired literature, others have used importance sampling for, e.g., orthogonal tensor decomposition [16,36,43] (generalizing low-rank approximation [20]) and support vector machines [26].\n\nClassical algorithms for quantum problems. We are aware of two important prior results from before Tang's first paper [45] that connect quantum algorithms to randomized numerical linear algebra. The first is Van den Nest's work on using probabilistic methods for quantum simulation [46], which defines a notion of \"computationally tractable\" (CT) state equivalent to our notion of sampling and query access and then uses it to simulate restricted classes of quantum circuits. We share some essential ideas with this work, such as the simple sampling lemma Lemma 2.9, but dequantized algorithms critically use low-rank assumptions on the input for \"simulating\" QML in a way that would not be possible were we only viewing such algorithms as large quantum circuits. The second is a paper by Rudi et al. [41] that uses the Nystr\u00f6m method to simulate a sparse Hamiltonian H on a sparse input state in time poly-logarithmic in dimension and polynomial in \u2225H \u2225 F , assuming sampling and query access to H . Our Hamiltonian simulation results do not require a sparsity assumption and still achieve a dimension-independent runtime, but get slightly larger exponents in exchange.\n\nPractical implementation. A work by Arrazola et al. [4] implements and tests quantum-inspired algorithms for regression and recommendation systems. This work makes various conclusions, and for example, suggests that the \u03b5 2 scaling in the number of rows/columns taken in our recommendation systems algorithm is inherent. However, we are unsure of these results' implications for the broader question of whether QML algorithms can achieve practical speedups, for two reasons. First, our algorithms use a restricted model of computation in order to get a broad asymptotic result for generic applications of quantum machine learning. However, if we wish to compare QML to the best classical algorithm in practice, other sketching algorithms are more natural to run on a classical computer and are likely to be faster. For example, Dahiya, Konomis, and Woodruff [12] conducted an empirical study of sketching algorithms for low-rank approximation on both synthetic datasets and the movielens dataset, reporting that their implementation \"finds a solution with cost at most 10 times the optimal one . . . but does so 10 times faster.\" For comparison, Arrazola et al. [4] claim that the running times of quantum-inspired algorithms are worse than directly computing the singular value decomposition for mediumsized matrices (e.g. 10 4 \u00d7 10 4 ). Second, the authors implement the quantum-inspired algorithms in a simple, non-optimized way in Python and then compare it to the well-optimized LAPACK library C implementation of singular value decomposition. These caveats make it difficult to draw definitive conclusions about the practicality of quantum-inspired algorithms as a whole from these experimental results.\n\nQuantum machine learning. As mentioned in Section 1.3, our work has major implications for the landscape of quantum machine learning. In particular, our work suggests that the most promising way to get exponential speedups for algorithms fitting in the framework of quantum singular value transformation [22] is via algorithms that use sparse matrices as input (as opposed to those with input in QRAM), such as HHL [25]. Such algorithms have other major caveats (mentioned by Aaronson [1]) that make it difficult to find applications with the potential for practical super-polynomial speedups. Proposals for such applications include Gaussian process regression [50] and topological data analysis [31].\n\nRelated independent work. Independently from our work, Jethwani, Le Gall, and Singh [27] simultaneously derived similar results. They implicitly derive a version of our even SVT result, and use it to achieve generic SVT (approximate SQ(b \u2020 f (SV) (A)) for a vector b) by writing f (SV) \n(A) = A\u0434(A \u2020 A) for \u0434(x) = f ( \u221a x)/ \u221a\nx and then using sampling subroutines to get the solution from the resulting expression b \u2020 AR \u2020 U R. It is difficult to directly compare the main SVT results, because the parameters that appear in their runtime bounds are somewhat non-standard, but one can see that for typical choices of f , their results require a strictly low-rank A. In comparison our results apply to general A, and we also demonstrate how to apply them to (re)derive dequantized algorithms.\n\n\nOpen Questions\n\nOur framework recovers recent dequantization results, and we hope that it will be used for dequantizing more quantum algorithms. In the meantime, our work leaves several natural open questions.\n\nFirst, in the quantum setting, linear algebra algorithms [22] can achieve logarithmic dependence on the precision \u03b5. Can classical algorithms also achieve such exponentially improved dependence, when the goal is restricted to sampling from the output (i.e., without the requirement to query elements of the output)? If not, is there a mildly stronger classical model that can achieve this? Could this exponential advantage be exploited in a meaningful way?\n\nSecond, our algorithms still have significant slowdown as compared to their quantum counterparts. Can we shave condition number factors to get runtimes of the form O \u2225A \u2225 6 F \u03c3 6 \u03b5 6 log 3 1 \u03b4 (for the recommendation systems application, for instance)? Can we get even better runtimes by somehow avoiding SVD computation? Finally, is there an approach to QML that does not go through HHL (whose demanding assumptions make exponential speedups difficult to demonstrate even in theory) or a low-rank assumption (which, as we demonstrate, makes the tasks \"easy\" for classical computers)?\n\n\nPRELIMINARIES\n\nTo begin with, we define notation to be used throughout this paper. For n \u2208 N, [n] := {1, . . . , n}. For z \u2208 C, its absolute value is |z| = \u221a z * z, where z * is the complex conjugate of z. f \u2272 \u0434 denotes the ordering f = O(\u0434) (and respectively for \u2273 and \u2242). O(\u0434) is shorthand for O(\u0434 poly(log \u0434)). Finally, we assume that arithmetic operations (e.g addition and multiplication of real numbers) and function evaluation oracles (computing f (x) from x) take unit time, and that queries to oracles (like the queries to input discussed in Section 2.2) are at least unit time cost.\n\n\nLinear Algebra\n\nIn this paper, we consider complex matrices A \u2208 C m\u00d7n for m, n \u2208 N. For i \u2208 [m], j \u2208 [n], we let A(i, \u00b7) denote the i-th row of A, A(\u00b7, j) denote the j-th column of A, and A(i, j) denote the (i, j)-th element of A. (A|B) denotes the concatenation of matrices A and B and vec(A) \u2208 C mn denotes the vector formed by concatenating the rows of A. For vectors v \u2208 C n , \u2225v \u2225 denotes standard Euclidean norm (so \u2225v \u2225 := ( n i=1 |v i | 2 ) 1/2 ). For a matrix A \u2208 C m\u00d7n , the Frobenius norm of A is \u2225A\u2225 F := \u2225vec(A)\u2225 = ( m i=1 n j=1 |A(i, j)| 2 ) 1/2 and the spectral norm of A is \u2225A\u2225 := \u2225A\u2225 Op := sup x \u2208C n , \u2225x \u2225=1 \u2225Ax \u2225.\n\nA singular value decomposition (SVD) of A is a representation A = U DV \u2020 , where for N := min(m, n), U \u2208 C m\u00d7N and V \u2208 C n\u00d7N are isometries and D \u2208 R N \u00d7N is diagonal with \u03c3 i := D(i, i) and \u03c3 1 \u2265 \u03c3 2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3 N \u2265 0. We can also write this decomposition as\nA = N i=1 \u03c3 i U (\u00b7, i)V (\u00b7, i) \u2020 .\nWe now formally define singular value transformation:\nDefinition 2.1.\nFor a function f : [0, \u221e) \u2192 C such 9 that f (0) = 0 we define the singular value transform of A \u2208 C m\u00d7n via a singular value decomposition A = min(m,n) i=1\n\u03c3 i u i v \u2020 i as follows f (SV) (A) := min(m,n) i=1 f (\u03c3 i )u i v \u2020 i .(1)Definition 2.2.\nFor a function f : R \u2192 C and a Hermitian A \u2208 C n\u00d7n we define the eigenvalue transform of A via a unitary\neigendecomposition A = n i=1 \u03bb i v i v \u2020 i as follows f (EV) (A) := n i=1 f (\u03bb i )v i v \u2020 i .(2)\nSince we only consider eigenvalue transformations of Hermitian matrices, where singular vectors/values and eigenvectors/values (roughly) coincide, the key difference is that eigenvalue transformations can distinguish eigenvalue sign. As this is the standard notion of a matrix function, we will usually drop the superscript in notation: f (A) := f (EV) (A).\n\nWe will use the following standard definition of a Lipschitz function.\nDefinition 2.3. We say f : R \u2192 C is L-Lipschitz on F \u2286 R if for all x, y \u2208 F, | f (x) \u2212 f (y)| \u2264 L|x \u2212 y|.\n\nSampling and Query Access Oracles\n\nSince we want our algorithms to run in time sublinear in input size, we must be careful in defining the access model. Our input model is unconventional, being designed as a reasonable classical analogue for the input model of some quantum algorithms. The sampling and query oracle we present below can be thought of as a classical analogue to a quantum state, and will be used heavily to move between intermediate steps of these quantum-inspired algorithms. First, as a warmup, we define a simple query oracle: Definition 2.4 (Query access). For a vector v \u2208 C n , we have Q(v), query access to v if for all i \u2208 [n], we can obtain v(i). Likewise, for a matrix A \u2208 C m\u00d7n , we have Q(A) if for all (i, j) \u2208 [m] \u00d7 [n], we can obtain A(i, j). Let q(v) (or q(A)) denote the (time) cost of such a query.\n\nFor example, in the typical RAM access model, we are given our input v \u2208 C n as Q(v) with q(v) = 1. For brevity, we will sometimes abuse this notation (and other access notations) and write, for example, \"Q(A) \u2208 C m\u00d7n \" instead of \"Q(A) for A \u2208 C m\u00d7n \". Definition 2.5 (Sampling and query access to a vector). For a vector v \u2208 C n , we have SQ(v), sampling and query access to v, if we can:\n\n(1) Query for entries of v as in Q(v);\n\n(2) Obtain independent samples i \u2208 [n] following the distribution D v \u2208 R n , where D v (i) := |v(i)| 2 /\u2225v \u2225 2 ; (3) Query for \u2225v \u2225. Let q(v), s(v), and n(v) denote the cost of querying entries, sampling indices, and querying the norm respectively. Further define sq(v) := q(v) + s(v) + n(v).\n\nWe will refer to these samples as importance samples from v, though one can view them as measurements of the quantum state |v\u27e9 := 1 \u2225v \u2225 v i |i\u27e9 in the computational basis.\n\nQuantum-inspired algorithms typically don't give exact sampling and query access to the output vector. Instead, we get a more general version of sampling and query access, which assumes we can only access a sampling distribution that oversamples the correct distribution. 10\nDefinition 2.6. For v \u2208 C n , p \u2208 R n \u22650 is a \u03d5-oversampled importance sampling distribution of v (for \u03d5 \u2265 1) if n i=1 p(i) = 1 and, for all i \u2208 [n], p(i) \u2265 D v (i)/\u03d5 = |v(i) | 2 \u03d5 \u2225v \u2225 2 .\nIf p is a \u03d5-oversampled importance sampling distribution of v, any given output i \u2208 [n] is no more than \u03d5-times rarer in p compared to the desired distribution D v . As a result, intuitively, estimators that use D v can also use p, with a factor \u03d5 increase in the number of samples necessary. For example, we can convert a sample from p to a sample from D v with probability 1/\u03d5 with rejection sampling: sample an i distributed as p, then accept the sample with probability (D v (i)/p(i))/\u03d5.\n\n\nDefinition 2.7 (Oversampling and query access).\n\nFor v \u2208 C n and \u03d5 \u2265 1, we have SQ \u03d5 (v), \u03d5-oversampling and query access to v, if we have Q(v) and SQ(\u1e7d) for\u1e7d \u2208 C n a vector satisfying \u03b2 := \u2225\u1e7d \u2225 2 = \u03d5 \u2225v \u2225 2 and |\u1e7d(i)\n| 2 \u2265 |v(i)| 2 for all i \u2208 [n]. Denote p(i) := D\u1e7d (i), s \u03d5 (v) := s(\u1e7d), q \u03d5 (v) := q(\u1e7d), n \u03d5 (v) := n(\u1e7d), and sq \u03d5 (v) := s \u03d5 (v) + q \u03d5 (v) + q(v) + n \u03d5 (v). SQ 1 (v)\nis the same as SQ(v), if we take\u1e7d = v. Note that our algorithms need to know \u03b2 (even if \u2225v \u2225 is known), as \u03b2 cannot be deduced from a small number of queries, samples, or probability computations. So, we will be choosing\u1e7d (and, correspondingly, \u03d5) such that \u2225\u1e7d \u2225 2 remains computable, even if potentially some c\u1e7d satisfies all our other requirements for some c < 1 (giving a smaller value of \u03d5). Finally, note that oversampling access implies an approximate version of the usual sampling access: Lemma 2.8. Suppose we are given SQ \u03d5 (v) and some \u03b4 \u2208 (0, 1]. Denote sq(v) := \u03d5sq \u03d5 (v) log 1 \u03b4 . We can sample from D v with probability \u2265 1 \u2212 \u03b4 in O( sq(v)) time. We can also estimate \u2225v \u2225 to \u03bd multiplicative error for \u03bd \u2208 (0, 1] with probability \u2265 1 \u2212 \u03b4 in O 1 \u03bd 2 sq(v) time.\n\nWe will generally compare our algorithms, which output SQ \u03d5 (v), to a quantum algorithm that can output (and measure) |v\u27e9. So, sq(v) is the relevant complexity measure that we will analyze and bound: if we wish to mimic samples from the output of the quantum algorithm we dequantize, we will pay a one-time cost to run our quantum-inspired algorithm, and then pay sq(v) cost per additional measurements. Lemma 2.9 (Linear combinations, Proposition 4.3 of [45]). Given SQ(v 1 ), . . . , SQ(v k ) \u2208 C n and \u03bb 1 , . . . , 10 Oversampling turns out to be the \"natural\" form of approximation in this setting; other forms of error do not propagate through quantum-inspired algorithms well, and the ones that do can usually be translated into oversampling of a desired distribution.\n\u03bb k \u2208 C, we have SQ \u03d5 ( \u03bb i v i ) for \u03d5 = k \u2225\u03bb i v i \u2225 2 \u2225 \u03bb i v i \u2225 2 and sq \u03d5 ( \u03bb i v i ) := max i \u2208[k] s(v i ) + k i=1 q(v i )\n(after paying O k i=1 n(v i ) one-time pre-processing cost to query for norms).\n\nSo, our general goal will be to express our output vector as a linear combination of a small number of input vectors that we have sampling and query access to. Then, we can get an approximate SQ access to our output using Lemma 2.8, where we pay an additional \"cancellation constant\" factor of k\n\u2225\u03bb i v i \u2225 2 \u2225 \u03bb i v i \u2225 2 .\nWe introduce some notation to this: for V \u2208 C n\u00d7k the matrix whose columns are v i 's and x \u2208 C k the vector whose entries are \u03bb i 's,\nC V ,x := i |x(i)| 2 \u2225V (\u00b7, i)\u2225 2 \u2225 i x(i)V (\u00b7, i)\u2225 2 \u2264 \u2225x \u2225 2 \u2225V \u2225 2 F \u2225V x \u2225 2 .\nAs we can see, kC V ,x is only large when V x has significantly smaller norm than the components v i in the sum suggest. Usually, in our applications, we can intuitively think about this overhead being small when the desired output vector mostly lies in a subspace spanned by singular vectors with large singular values in our low-rank input. Quantum algorithms also have similar overheads. For example, the quantum recommendation systems algorithm [29] incurs such a cost factor when performing a swap test to project the input vector on the subspace spanned by the top singular vectors of the input matrix. Assuming this cancellation is not too large, other subroutines dominate the runtime in our applications. We also define oversampling and query access for a matrix. Though the oversampling approximation is unusual, this model is also discussed in prior work [15,20] and is the right notion for the sampling procedures we will use. We have SQ \u03d5 (A) if we have Q(A) and SQ(\u00c3) for\u00c3 \u2208 C m\u00d7n satisfying \u03b2 := \u2225\u00c3\u2225 2 F = \u03d5 \u2225A\u2225 2 F and |\u00c3(i, j)| 2 \u2265 |A(i, j)| 2 for all (i, j) \u2208 [m] \u00d7 [n]. Let p and p i denote the distributions on\u00e3 and A(i, \u00b7), respectively. The (known upper bounds on the) complexity of (over)sampling and querying from the matrix A is denoted by s \u03d5 (A) := max(s(\u00c3(i, \u00b7)), s(\u00e3)), q \u03d5 (A) := max(q(\u00c3(i, \u00b7)), q(\u00e3)), q(A) := max(q(A(i, \u00b7))), and n \u03d5 (A) := n(\u00e3) respectively. We also use the notation sq \u03d5 (A) := max(s \u03d5 (A), q \u03d5 (A), q(A), n \u03d5 (A)) and sq \u03d5 (A ( \u2020) ) := sq \u03d5 (A) + sq \u03d5 (A \u2020 ) sometimes omitting the subscripts if \u03d5 = 1.\n\nObserve that SQ \u03d5 (A) implies SQ \u03d5 (vec(A)): we can take vec(A) = vec(\u00c3), and the distribution for vec(\u00c3) is sampled by sampling i from D\u00e3 , and then sampling j from D\u00c3 (i, \u00b7) . This gives the output (i, j) with probability |\u00c3(i, j)| 2 /\u2225\u00c3\u2225 2 F . Therefore, SQ \u03d5 (A) can be thought of as SQ \u03d5 (vec(A)), with the additional guarantees that we can compute marginals n j=1 D vec(\u00c3) (i, j) and can sample from the resulting conditional distributions D vec(\u00c3) (i, j)/ n j=1 D vec(\u00c3) (i, j).\nLemma 2.11. Given vectors u \u2208 C m , v \u2208 C n with SQ \u03c6 u (u), SQ \u03c6 v (v) access we have SQ \u03d5 (A) for their outer product A := uv \u2020 with \u03d5 = \u03c6 u \u03c6 v and s \u03d5 (A) = s \u03c6 u (u) + s \u03c6 v (v), q \u03d5 (A) = q \u03c6 u (u) + q \u03c6 v (v), q(A) = q(u) + q(v), and n \u03d5 (A) = n \u03c6 u (u) + n \u03c6 v (v),\nThe above shows that Definition 2.10 is a faithful generalization of Definition 2.7, i.e., for a vector v we get back essentially the same\n\u2225a\u2225 2 = \u2225A\u2225 2 F |a 1 | 2 = \u2225A(1, \u00b7)\u2225 2 |a 2 | 2 = \u2225A(2, \u00b7)\u2225 2 |A(1, 1)| 2 + |A(1, 2)| 2 |A(1, 3)| 2 + |A(1, 4)| 2 |A(2, 1)| 2 + |A(2, 2)| 2 |A(2, 3)| 2 + |A(2, 4)| 2 |A(1, 1)| 2 |A(1, 2)| 2 |A(1, 3)| 2 |A(1, 4)| 2 |A(2, 1)| 2 |A(2, 2)| 2 |A(2, 3)| 2 |A(2, 4)| 2 A(1,1) |A(1,1)| A(1,2) |A(1,2) | A(1,3) |A(1,3) | A(1,4) |A(1,4)| A(2,1) |A(2,1)| A(2,2) |A(2,2)| A(2,3) |A(2,3)| A(2,4)\n|A(2,4)| Figure 1: Dynamic data structure for A \u2208 C 2\u00d74 . We compose the data structure for a with the data structure for A's rows.\n\ndefinition if we think about it as a row / column matrix. Using the same ideas as in Lemma 2.9, we can extend sampling and query access of input matrices to linear combinations of those matrices.\nLemma 2.12. Given SQ \u03c6 (1) (A (1) ), . . . , SQ \u03c6 (\u03c4 ) (A (\u03c4 ) ) \u2208 C m\u00d7n , we have SQ \u03d5 (A) for A := \u03c4 t =1 \u03bb t A (t ) with \u03d5 = \u03c4 \u03c4 t =1 \u03c6 (t ) \u2225\u03bb t A (t ) \u2225 2 F \u2225A \u2225 2 F and s \u03d5 (A) = max t \u2208[\u03c4 ] s \u03c6 (t ) (A (t ) ) + q \u03d5 (A), q \u03d5 (A) = \u03c4 t =1 q \u03c6 (t ) (A (t ) ), q(A) = \u03c4 t =1 q(A (t ) )\n, and n \u03d5 (A) = 1 (after paying \u03c4 t =1 n \u03c6 (t ) (A (t ) ) one-time pre-processing cost).\n\nQuantum machine learning algorithms and their corresponding quantum-inspired algorithms have the potential to achieve exponential speedups when their state preparation procedures run in time polylog(n). So, the most interesting regime for us is when our sampling and query oracles take polylogarithmic time. This assumption can be satisfied in various ways.\n\nRemark 2.13. Below, we list various settings where we have sampling and query access to input matrices and vectors, and whenever relevant, we compare the resulting runtimes to the time to prepare analogous quantum states. Note that because we do not analyze classical algorithms in the bit model, their runtimes may be missing log factors that should be counted for a fair comparison between classical and quantum.\n\nData structure. Given v \u2208 C n in the standard RAM model, the alias method [47] takes \u0398(n) pre-processing time to output a data structure that uses \u0398(n) space and can sample from v in \u0398(1) time.\n\nIn other words, we can get SQ(v) with sq(v) = \u0398(1) in O(n) time, and by extension, for a matrix A \u2208 C m\u00d7n , SQ(A) with sq(A) = \u0398(1) in O(mn) time.\n\nMore precisely, the pre-processing time is linear in the number of non-zero entries of the input vector/matrix (which we denote nnz(v)/nnz(A)). A direct consequence of this observation is that the quantum-inspired setting is more restrictive than the typical randomized numerical linear algebra algorithm setting. With this data structure, a fast quantum-inspired algorithm (say, one running in time O(T sq(A)) for T independent of input size) implies an algorithm in the standard computational model (running in O(nnz(A) + T ) time).\n\nDynamic data structure. QML algorithms often assume that their input is in a QRAM data structure [6,23,30,37,40,49], arguing that, with the right type of quantum access, this data structure allows for circuits preparing input states with linear gate count but polylog depth. Hardware might be able to parallelize these circuits enough so that they run in polylog time. In the interest of considering the best of all possible worlds for QML, we will treat circuit depth as runtime for QRAM and ignore technicalities.\n\nThis data structure (see Fig. 1) admits sampling and query access to the data it stores with just-as-good runtimes: specifically, for a matrix A \u2208 C m\u00d7n , we get SQ(A) with q(A) = O(1) and s(A) = O(log mn). So, quantum-inspired algorithms can be used whenever QML algorithms assume this form of input.\n\nFurther, unlike the alias method stated above, this data structure supports updating entries in O(log mn) time, which can be useful for applications of QML where data can accumulate over time [29].\n\nIntegrability assumption. For v \u2208 C n , suppose we can compute entries and sums i \u2208I (b) |v i | 2 in time T , where I (b) \u2282 [n] is the set of indices whose binary representation begins with the bitstring b. Then we have SQ(v) where q(v) = O(T ), s(v) = O(T log n), and n(v) = O(T ). Analogously, a quantum state corresponding to v can be prepared in time O(T log n) via Grover-Rudolph state preparation [24]. (One can think about the QRAM data structure as pre-computing all the necessary sums for this protocol.)\n\nUniformity assumption. Given O(1)-time Q(v) \u2208 C n and a \u03b2 = \u03d5 \u2225v \u2225 2 such that max |v i | 2 \u2264 \u03b2/n, we have SQ \u03d5 (v) with sq \u03d5 (v) = O(1), by using the all-1 vector times \u03b2/n as an upper bound. Assuming the ability to query entries of v in superposition, a quantum state corresponding to v can be prepared in time O \u03d5 log n .\n\nSparsity assumption. If A \u2208 C m\u00d7n has at most s non-zero entries per row (with efficiently computable locations) and the matrix elements are |A(i, j)| \u2264 c (and efficiently computable), then we have SQ \u03d5 (A) for \u03d5 = c 2 sm \u2225A \u2225 2 F , simply by using the uniform distribution over non-zero entries for the oversampling and query oracles. For example, for SQ(\u00e3) we can set\u00e3(i) := c \u221a s, and for\u00c3(i, \u00b7) we use the vector with entries c at the non-zeros of A(i, \u00b7) (potentially adding some \"dummy\" zero locations to have exactly s non-zeroes).\n\nIf A is not much smaller than we expect, \u03d5 is independent of dimension. For example, if A has exactly s non-zero entries per row and |A(i, j)| \u2265 c \u2032 for non-zero entries, then \u03d5 \u2264 (c/c \u2032 ) 2 . This kind of sparsity assumption is used in some QML and Hamiltonian simulation problems [25].\n\nCT states. In 2009, Van den Nest defined the notion of a \"computationally tractable\" (CT) state [46]. Using our notation, |\u03c8 \u27e9 \u2208 C n is a CT state if we have SQ(\u03c8 ) with sq(\u03c8 ) = polylog(n). Van den Nest's paper identifies several classes of CT states, including product states, quantum Fourier transforms of product states, matrix product states of polynomial bond dimension, stabilizer states, and states from matchgate circuits.\n\n\nMatrix Sketches\n\nDefinition 2.14. For a distribution p \u2208 R m , we say that a matrix S \u2208 R s\u00d7m is sampling according to p if each row of S is independently chosen to be e i / sp(i) with probability p i .\n\nWe call such S's importance sampling sketches when p comes from SQ(A) for some A \u2208 C m\u00d7n , and we call them \u03d5-oversampled importance sampling sketches if p comes from SQ \u03d5 (A).\n\nIn the standard algorithm setting, sketching A down to SA with an importance sampling sketch requires reading all of A to compute D a . If we have SQ \u03d5 (A), we can efficiently create a \u03d5-oversampling sketch S by pulling samples from p, and SA will be a (normalized) subset of rows of A. The core technique of our quantum-inspired algorithms is to use these kinds of sketches to approximate matrix expressions. Further, we can chain them with a simple observation. Lemma 2.15. Given SQ \u03d5 (A) and S \u2208 R r \u00d7m (described in pairs (i 1 , p(i 1 )), . . . , (i r , p(i r ))) sampled according to p with r \u2265 2\u03d5 2 ln 2 \u03b4 , then with probability \u2265 1 \u2212 \u03b4 we have SQ 2\u03d5 ((SA) \u2020 ) with q((SA) \u2020 ) = q(A), s \u03d5 ((SA) \u2020 ) = s \u03d5 (A)+r q \u03d5 (A), q \u03d5 ((SA) \u2020 ) = r q \u03d5 (A), and n \u03d5 ((SA) \u2020 ) = n \u03d5 (A). If \u03d5 = 1, then for all r , we have SQ((SA) \u2020 ) with the runtimes specified above.\n\nWhen we refer to sketching A down to SAT , we use the above observation for sampling T .\n\n\nMAIN RESULTS\n\n\nSingular Value Transformation\n\nWe begin with a fundamental observation: given sampling and query access to a matrix A, we can approximate the matrix product A \u2020 B by a sum of rank-one outer products. This is the key lemma we use most in our applications. Lemma 3.1 (Approximating matrix multiplication to Frobenius norm error; corollary of [14, Theorem 1])). Consider X \u2208 C m\u00d7n , Y \u2208 C m\u00d7p , and take S \u2208 R s\u00d7m to be sampled according to r := p+q 2 , where p, q \u2208 R m are \u03d5 1 , \u03d5 2 -oversampled importance sampling distributions from X, Y respectively. Then,\nPr \u2225X \u2020 S \u2020 SY \u2212 X \u2020 Y \u2225 F < 8\u03d5 1 \u03d5 2 log 2/\u03b4 s \u2225X \u2225 F \u2225Y \u2225 F > 1 \u2212 \u03b4 .\nMoreover, \u2225SX \u2225 2 F \u2264 2\u03d5 1 \u2225X \u2225 2 F and \u2225SY \u2225 2 F \u2264 2\u03d5 2 \u2225Y \u2225 2 F . We make a couple remarks. First, the bounds on \u2225SX \u2225 2 F can be improved to something like \u2225X \u2225 2 F for a sufficiently large sketch, but we will not need such bounds. Second, if X = Y , we can get an improved spectral norm bound: instead of depending on \u2225X \u2225 2 F , error depends on \u2225X \u2225\u2225X \u2225 F . Theorem 3.2 (Even singular value transformation). Let A \u2208 C m\u00d7n and f : R + \u2192 C be such that, f andf (x) := (f (x) \u2212 f (0))/x are L-Lipschitz and L \u2032 -Lipschitz, respectively, on \u222a n i=1 [\u03c3 2 i \u2212d, \u03c3 2 i +d] for some d > 0. Take parameters \u03b5 and \u03b4 such that 0 < \u03b5 \u2272 L\u2225A\u2225 2 * , \u03b4 \u2208 (0, 1], and d >\u03b5 := \u2225A\u2225\u2225A\u2225 F\n\u03d5 2 log(1/\u03b4 ) min(r ,c) 1/2\n. Choose a norm * \u2208 {F, Op}. Given SQ \u03d5 (A), consider the sketch S \u2208 R r \u00d7m sampled from p and the sketch T \u2020 \u2208 R c\u00d7n sampled from the distribution for SQ 2\u03d5 ((SA) \u2020 ) (given by Lemma 2.15), where r =\u03a9 \u03d5 2 L 2 \u2225A\u2225 2 * \u2225A\u2225 2 F 1 \u03b5 2 log 1 \u03b4 and c =\u03a9 \u03d5 2 L \u20322 \u2225A\u2225 4 \u2225A\u2225 2 * \u2225A\u2225 2 F 1 \u03b5 2 log 1 \u03b4 . Then, for R := SA and C := SAT , we can achieve the bound\nPr \u2225R \u2020f (CC \u2020 )R + f (0)I \u2212 f (A \u2020 A)\u2225 * > \u03b5 < \u03b4 .(3)\nFinding the sketches takes time O (r + c)sq \u03d5 (A) .\n\nWe remark that no additional log terms are necessary (i.e.,\u03a9 becomes \u2126) when Frobenius norm is used. Later we will need some bounds on the norms of the matrices in our decomposition. The following lemma gives the bounds we need for our applications.\n\n\nLemma 3.3 (Norm bounds for even singular value transformation).\n\nSuppose the assumptions from Theorem 3.2 hold and the event in Eq.\n(3) occurs (that is, R \u2020f (CC \u2020 )R \u2248 f (A \u2020 A) \u2212 f (0)I ).\nThen we can additionally assume that the following bounds also hold:\n\u2225R\u2225 = O(\u2225A\u2225) and \u2225R\u2225 F = O(\u2225A\u2225 F ),(4)\u2225f (CC \u2020 )\u2225 \u2264 max |f (x)| x \u2208 min(r ,c) i=1 [\u03c3 2 i \u2212\u03b5, \u03c3 2 i +\u03b5] ,(5)\nwhen * = Op, R \u2020 f (CC \u2020 ) \u2264 \u2225 f (A \u2020 A) \u2212 f (0)I \u2225 + \u03b5. (6) While we will primarily use the simple and fast primitive of even singular value transformation to recover \"dequantized QML\"-type results, we can also get generic singular value transformation and eigenvalue transformation results by bootstrapping Theorem 3.2.\n\nTheorem 3.4 (Generic singular value transformation). Let A \u2208 C m\u00d7n be given with both SQ \u03d5 (A) and SQ \u03d5 (A \u2020 ) and let f : R \u2192 C be a function such that f (0) = 0, \u0434(x) := f ( \u221a x)/ \u221a x is L-Lipschitz, and\u0434(x) := \u0434(x)/x is L \u2032 -Lipschitz. Then, for 0 < \u03b5 \u2272 L\u2225A\u2225 3 , we can output sketches R := SA \u2208 C r \u00d7n and C := AT \u2208 C m\u00d7c , along with M \u2208 C r \u00d7c , with r =\u03a9 \u03d5 2 L 2 \u2225A\u2225 2 \u2225A\u2225 4 F 1 \u03b5 2 log 1 \u03b4 and c = \u2126 \u03d5 2 L 2 \u2225A\u2225 4 \u2225A\u2225 2 Theorem 3.5 (Eigenvalue transformation). Suppose we are given a Hermitian SQ \u03d5 (A) \u2208 C n\u00d7n , a function f : R \u2192 C that is L-Lipschitz on \u222a n i=1 [\u03bb i \u2212 d, \u03bb i + d] for some d > \u03b5 L , and some 11 \u03b5 \u2272 L\u2225A\u2225 \u2225A \u2225 \u2225A \u2225 F . Then we can output S \u2208 C s\u00d7n , N \u2208 C s \u2032 \u00d7s , and D \u2208 C s \u2032 \u00d7s \u2032 with s = O \u03d5 2 \u2225A\u2225 4 \u2225A\u2225 2 \n\n\nDequantizing QSVT\n\nWe can use the above results to dequantize the quantum singular value transformation described by Gily\u00e9n et al. [22] in the case of close-to-low-rank input.\n\nDefinition 3.6. For a matrix A \u2208 C m\u00d7n and p(x) \u2208 C[x] degree-d polynomial of parity-d (i.e., even if d is even and odd if d is odd), we define the notation p (QV) (A) in the following way:\n\n(1) If p is even, meaning that we can express p(x) = q(x 2 ) for some polynomial q(x), then (2) If p is odd, meaning that we can express p(x) = x \u00b7 q(x 2 ) for some polynomial q(x), then  Then with probability \u2265 1 \u2212 \u03b4 , for \u03b5 a sufficiently small constant, we can get SQ \u03d5 (v) \u2208 C n such that \u2225v \u2212 p (QV) (A)b \u2225 \u2264 \u03b5 \u2225p (QV) (A)b \u2225 in poly d, From this result it follows that QSVT, as described in [22,Theorem 17], has no exponential speedup when the block-encoding of A comes from a quantum-accessible \"QRAM\" data structure as in [22,Lemma 50]. In the setting of QSVT, given A and b in QRAM, one can prepare |b\u27e9 and construct a block-encoding for A/\u2225A\u2225 F = A in polylog(mn) time. Then one can apply (quantum) SVT by a degree-d polynomial on A and apply the resulting map to |b\u27e9 with d \u00b7 polylog(mn) gates and finally project down to get the state |p (QV) (A)b\u27e9 with probability \u2265 1\u2212\u03b4 after \u0398  11 The correct way to think about \u03b5 is as some constant fraction of L \u2225A \u2225. If \u03b5 > L \u2225A \u2225 then f (0)I is a satisfactory approximation. The bound we give says that we want an at least \u2225A \u2225 F / \u2225A \u2225 improvement over trivial, which is modest in the close-to-low-rank regime that we care about. Similar assumptions appear in applications. outcome, possibly with some log(1/\u03b5) factors representing the discretization error in truncating real numbers to finite precision (which we ignore, since we do not account for them in our classical algorithm runtimes).\n\nAnalogously, by Remark 2.13, having A and b in (Q)RAM implies having SQ(A) and SQ(b) with sq(A) = O(log mn) and sq(b) = O(log n). Since QSVT also needs to assume max x \u2208[\u22121,1] |p(x)| \u2264 1, the classical procedure matches the assumptions for QSVT. Our algorithm runs only polynomially slower than the quantum algorithm, since the quantum runtime clearly depends on d, 1 \u2225p (QV) (A)b \u2225 , and log(mn). We are exponentially slower in \u03b5 and \u03b4 (these errors are conflated for the quantum algorithm). However, this exponential advantage vanishes if the desired output isn't a quantum state but some fixed value (or an estimate of one), since then the quantum algorithm must also pay 1 \u03b5 during the sampling or tomography procedures (meanwhile the success probability 1-\u03b4 can be typically exponentially boosted on the classical side). Note that, unlike in the quantum output, we can query entries of the output, which a quantum algorithm cannot do without paying at least a 1 \u03b5 factor. Theorem 3.7 also dequantizes QSVT for block-encodings of density operators when the density operator comes from some wellstructured classical data. Indeed, [22,Lemma 45] assumes we can efficiently prepare a purification of the density operator \u03c1. The rough classical analogue is the assumption that we have sampling and query access to some A \u2208 C m\u00d7n with \u03c1 = A \u2020 A. Since Tr(\u03c1) = 1, we have \u2225A\u2225 F = 1. Then, p (QV) (\u03c1) = r (QV) (A) for r (x) = p(x 2 ) and \u2225\u03c1 \u2225 = \u2225A\u2225 2 , so we can repeat the above argument to show the lack of exponential speedup for this input model too.\n\nWe can mimic the quantum algorithm with our techniques because low-degree polynomials are smooth. For example, a degree-d polynomial bounded on [\u22121, 1] is d 2 -Lipschitz, by Markov's inequality. We use inequalities of this type to prove the statement.\n\nTechnically, QSVT can use A \u2020 in QRAM instead of A (cf. [22,Lemma 50]). This does not result in a discrepancy, because in the full version we describe a method to get SQ(B) and SQ(B \u2020 ) for a matrix B satisfying \u2225B \u2212 A\u2225 \u2264 \u03b5, given only SQ(A).\n\n\nverted to a standard algorithm that runs in time O(nnz(A) + T ), where nnz(A) is the number of nonzero entries of A. So, we can also think about an O(T )-time quantum-inspired algorithm as an O(nnz(A) + T )-time sketching algorithm, where the nnz(A) portion of the runtime can only be used to facilitate importance sampling.\n\nDefinition 2 .\n210 (Oversampling and query access to a matrix). For a matrix A \u2208 C m\u00d7n , we have SQ(A) if we have SQ(A(i, \u00b7)) for all i \u2208 [m] and SQ(a) for a \u2208 R m the vector of row norms (a(i) := \u2225A(i, \u00b7)\u2225).\n\nPr\n\u2225CMR + \u0434(0)A \u2212 f (SV) (A)\u2225 > \u03b5 < \u03b4 .Finding S, M, and T takes time O (L 4 L \u20322 \u2225A\u2225 16 \u2225A\u2225 6 F + L 6 \u2225A\u2225 10 \u2225A\u2225 8 F ) \u03d5 6 \u03b5 6 log 3 1 \u03b4 sq \u03d5 A ( \u2020) .\n\n\nand s \u2032 = O \u2225A\u2225 2 F L 2 /\u03b5 2 , such that Pr \u2225(SA) \u2020 N \u2020 DN (SA) + f (0)I \u2212 f (EV) (A)\u2225 > \u03b5 < \u03b4, in time O L 22 \u03b5 \u221222 \u2225A\u2225 16 \u2225A\u2225 6 F log 3 1 \u03b4 sq \u03d5 (A). Moreover, this decomposition satisfies the following further properties. First, N SA is an approximate isometry: \u2225(N SA)(N SA) \u2020 \u2212 I \u2225 \u2264 ( \u03b5 L \u2225A \u2225 ) 3 . Second, D is a diagonal matrix and its diagonal entries satisfy |D(i, i) + f (0) \u2212 f (\u03bb i )| \u2264 \u03b5 for all i \u2208 [s \u2032 ] (when eigenvalues \u03bb i are appropriately ordered).\n\np\n(QV) (A) := q(A \u2020 A) = p( A \u2020 A).\n\np\n(QV) (A) := A \u00b7 q(A \u2020 A).\n\nTheorem 3. 7 .\n7Suppose we are given a matrix A \u2208 C m\u00d7n satisfying \u2225A\u2225 F = 1 via the oracles for SQ(A) with sq(A) = O(log(mn)), a vector SQ(b) \u2208 C n with \u2225b \u2225 = 1 and sq(b) = O(log n), and a degree-d polynomial p(x) of parity-d such that |p(x)| \u2264 1 for all x \u2208 [\u22121, 1].\n\n\n1 \u2225p (QV) (A)b \u2225 , 1 \u03b5 , 1 \u03b4 , log mn time (with sq(v) also having similar runtime bound).\n\n\nthe circuit. So, getting a sample from |p (QV) (A)b\u27e9 takes \u0398 d 1 \u2225p (QV) (A)b \u2225 polylog(mn/\u03b4 ) time. This circuit gives an exact\n\n\nTo estimate Tr[A (i) X ], we first notice that we have SQ \u03d5 (A), since it is a linear combination of matrices that we have sampling and query access to (Lemma 2.12). Then, we can find approximations of the Gibbs state by applying eigenvalue transformation (Theorem 3.5) according to the exponential function to get exp[\u2212A] as an RUR decomposition. Then the estimation of Tr[A (i) X ] can be performed by the usual SQ sampling techniques. This strategy solves the feasibility problem and when applicable outputs the \u03b5approximate solution of the SDP as an RUR decomposition in time8 O For the same feasibility problem, the previous quantum-inspired SDP solver[8] proved a complexity bound O mr 57 \u03b5 \u221292 ln 37 (n) , assuming that the constraint matrices have rank at most r . Since the rank constraint implies that A (\u00b7) our algorithm has complexity O r 11 \u03b5 \u221246 ln 23 (n) + mr 7 \u03b5 \u221228 ln 13 (n) . So, our new algorithm both solves a more general problem and also greatly improves the runtime.\u2225A (\u00b7) \u2225 22 \n\nF \n\n\u03b5 46 ln 23 (n) + m \n\n\u2225A (\u00b7) \u2225 14 \n\nF \n\n\u03b5 28 ln 13 (n) . \nF \n\n\u2264 \n\u221a \nr , under this assumption \n\n\n\n\nan approximate projective isometry (that is, its singular values are close to one or zero). This roughly formalizes the intuition of C W preserving the left singular vectors and singular values of R W . We can rewrite R \u2020 W\nThis assumption is important. When input data is quantum (say, it is gathered experimentally from a quantum system), a classical computer has little hope of performing linear algebra on it efficiently.\nOnly one of SQ(A) or SQ(A \u2020 ) suffices, but it is more convenient to assume both.5 By a dimension-independent or dimensionless quantity, we mean a quantity that is both independent of the size of the input matrix and is scale-invariant, i.e., does not change under scaling A \u2190 \u03b1 A.6 This criterion is fairly reasonable. For example, the polynomials used in QSVT satisfy it.\nThe f (0) = 0 requirement ensures that the definition is independent of the (not necessarily unique) choice of SVD.\nACKNOWLEDGMENTS\nRead the fine print. Scott Aaronson, Nature Physics. 11291Scott Aaronson. 2015. Read the fine print. Nature Physics 11, 4 (2015), 291.\n\nImprovements in quantum SDPsolving with applications. Andr\u00e1s Joran Van Apeldoorn, Gily\u00e9n, arXiv:1804.05058Proceedings of the 46th International Colloquium on Automata, Languages, and Programming (ICALP). the 46th International Colloquium on Automata, Languages, and Programming (ICALP)9915Joran van Apeldoorn and Andr\u00e1s Gily\u00e9n. 2019. Improvements in quantum SDP- solving with applications. In Proceedings of the 46th International Colloquium on Automata, Languages, and Programming (ICALP). 99:1-99:15. arXiv: 1804.05058\n\nA Combinatorial, Primal-Dual Approach to Semidefinite Programs. Sanjeev Arora, Satyen Kale, 35. Earlier version in STOC'07. 63Sanjeev Arora and Satyen Kale. 2016. A Combinatorial, Primal-Dual Approach to Semidefinite Programs. Journal of the ACM 63, 2 (2016), 12:1-12:35. Earlier version in STOC'07.\n\nJuan Miguel Arrazola, Alain Delgado, Seth Bhaskar Roy Bardhan, Lloyd, arXiv:1905.10415Quantum-inspired algorithms in practice. Juan Miguel Arrazola, Alain Delgado, Bhaskar Roy Bardhan, and Seth Lloyd. 2019. Quantum-inspired algorithms in practice. arXiv: 1905.10415\n\nQuantum SDP solvers: Large speed-ups, optimality, and applications to quantum learning. G S L Fernando, Amir Brand\u00e3o, Tongyang Kalev, Cedric Yen-Yu Li, Krysta M Lin, Xiaodi Svore, Wu, arXiv:1710.02581Proceedings of the 46th International Colloquium on Automata, Languages, and Programming (ICALP). the 46th International Colloquium on Automata, Languages, and Programming (ICALP)2714Fernando G. S. L. Brand\u00e3o, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta M. Svore, and Xiaodi Wu. 2019. Quantum SDP solvers: Large speed-ups, optimality, and applications to quantum learning. In Proceedings of the 46th International Col- loquium on Automata, Languages, and Programming (ICALP). 27:1-27:14. arXiv: 1710.02581\n\nThe power of block-encoded matrix powers: improved regression techniques via faster Hamiltonian simulation. Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, Stacey Jeffery, 1-33:14.arXiv:1804.01973Proceedings of the 46th International Colloquium on Automata, Languages, and Programming (ICALP). the 46th International Colloquium on Automata, Languages, and Programming (ICALP)33Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, and Stacey Jeffery. 2019. The power of block-encoded matrix powers: improved regression techniques via faster Hamiltonian simulation. In Proceedings of the 46th International Colloquium on Automata, Languages, and Programming (ICALP). 33:1-33:14. arXiv: 1804.01973\n\nA quantum-inspired classical algorithm for separable non-negative matrix factorization. Zhihuai Chen, Yinan Li, Xiaoming Sun, Pei Yuan, Jialin Zhang, arXiv:1907.05568Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceAAAI PressZhihuai Chen, Yinan Li, Xiaoming Sun, Pei Yuan, and Jialin Zhang. 2019. A quantum-inspired classical algorithm for separable non-negative matrix factor- ization. In Proceedings of the 28th International Joint Conference on Artificial Intelligence. AAAI Press, 4511-4517. arXiv: 1907.05568\n\nQuantuminspired classical sublinear-time algorithm for solving low-rank semidefinite programming via sampling approaches. Nai-Hui Chia, Tongyang Li, Han-Hsuan Lin, Chunhao Wang, arXiv:1901.03254Nai-Hui Chia, Tongyang Li, Han-Hsuan Lin, and Chunhao Wang. 2019. Quantum- inspired classical sublinear-time algorithm for solving low-rank semidefinite programming via sampling approaches. (2019). arXiv: 1901.03254\n\nQuantum-inspired sublinear classical algorithms for solving low-rank linear systems. Nai-Hui Chia, Han-Hsuan Lin, Chunhao Wang, arXiv:1811.04852Nai-Hui Chia, Han-Hsuan Lin, and Chunhao Wang. 2018. Quantum-inspired sublinear classical algorithms for solving low-rank linear systems. (2018). arXiv: 1811.04852\n\nQuantum machine learning: a classical perspective. Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil, Andrea Rocchetto, Simone Severini, Leonard Wossnig, Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences. 47420170551Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil, Andrea Rocchetto, Simone Severini, and Leonard Wossnig. 2018. Quantum machine learning: a classical perspective. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences 474, 2209 (Jan. 2018), 20170551.\n\nQuantum discriminant analysis for dimensionality reduction and classification. Iris Cong, Luming Duan, arXiv:1510.00113New Journal of Physics. 1873011Iris Cong and Luming Duan. 2016. Quantum discriminant analysis for dimen- sionality reduction and classification. New Journal of Physics 18, 7 (jul 2016), 073011. arXiv: 1510.00113\n\nAn empirical evaluation of sketching for numerical linear algebra. Yogesh Dahiya, Dimitris Konomis, David P Woodruff, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningACMYogesh Dahiya, Dimitris Konomis, and David P Woodruff. 2018. An empirical evaluation of sketching for numerical linear algebra. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 1292-1300.\n\nChen Ding, Tian-Yi Bao, He-Liang Huang, arXiv:1906.08902Quantum-Inspired Support Vector Machine. Chen Ding, Tian-Yi Bao, and He-Liang Huang. 2019. Quantum-Inspired Support Vector Machine. arXiv: 1906.08902\n\nFast Monte Carlo algorithms for matrices I: Approximating matrix multiplication. Petros Drineas, Ravi Kannan, Michael W Mahoney, SIAM J. Comput. 361Petros Drineas, Ravi Kannan, and Michael W. Mahoney. 2006. Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication. SIAM J. Comput. 36, 1 (2006), 132-157.\n\nCompetitive recommendation systems. Petros Drineas, Iordanis Kerenidis, Prabhakar Raghavan, Proceedings of the 34th ACM Symposium on the Theory of Computing (STOC. the 34th ACM Symposium on the Theory of Computing (STOCPetros Drineas, Iordanis Kerenidis, and Prabhakar Raghavan. 2002. Competitive recommendation systems. In Proceedings of the 34th ACM Symposium on the Theory of Computing (STOC). 82-90.\n\nA randomized algorithm for a tensor-based generalization of the singular value decomposition. Petros Drineas, Michael W Mahoney, Linear Algebra Appl. 420Petros Drineas and Michael W. Mahoney. 2007. A randomized algorithm for a tensor-based generalization of the singular value decomposition. Linear Algebra Appl. 420, 2-3 (2007), 553-571.\n\nRelativeerror CUR matrix decompositions. Petros Drineas, Michael W Mahoney, S Muthukrishnan, SIAM J. Matrix Anal. Appl. 30Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. 2008. Relative- error CUR matrix decompositions. SIAM J. Matrix Anal. Appl. 30, 2 (Jan. 2008), 844-881.\n\nA quantuminspired algorithm for general minimum conical hull problems. Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Dacheng Tao, arXiv:1907.06814Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. 2019. A quantum- inspired algorithm for general minimum conical hull problems. arXiv: 1907.06814\n\nA non-review of Quantum Machine Learning: trends and explorations. Vedran Dunjko, Peter Wittek, Quantum Views. 432Vedran Dunjko and Peter Wittek. 2020. A non-review of Quantum Machine Learning: trends and explorations. Quantum Views 4 (March 2020), 32.\n\nFast Monte-Carlo algorithms for finding low-rank approximations. Alan Frieze, Ravi Kannan, Santosh Vempala, Journal of the ACM. 51Alan Frieze, Ravi Kannan, and Santosh Vempala. 2004. Fast Monte-Carlo algo- rithms for finding low-rank approximations. Journal of the ACM 51, 6 (2004), 1025-1041.\n\nQuantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension. Andr\u00e1s Gily\u00e9n, Seth Lloyd, Ewin Tang, arXiv:1811.04909Andr\u00e1s Gily\u00e9n, Seth Lloyd, and Ewin Tang. 2018. Quantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension. (2018). arXiv: 1811.04909\n\nQuantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, Nathan Wiebe, arXiv:1806.01838Proceedings of the 51st ACM Symposium on the Theory of Computing (STOC). the 51st ACM Symposium on the Theory of Computing (STOC)Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, and Nathan Wiebe. 2019. Quantum singular value transformation and beyond: exponential improvements for quan- tum matrix arithmetics. In Proceedings of the 51st ACM Symposium on the Theory of Computing (STOC). 193-204. arXiv: 1806.01838\n\nQuantum random access memory. Vittorio Giovannetti, Seth Lloyd, Lorenzo Maccone, arXiv:0708.1879Physical Review Letters. 100160501Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. 2008. Quantum random access memory. Physical Review Letters 100, 16 (2008), 160501. arXiv: 0708.1879\n\nCreating superpositions that correspond to efficiently integrable probability distributions. Lov Grover, Terry Rudolph, arXiv:quant-ph/0208112Lov Grover and Terry Rudolph. 2002. Creating superpositions that correspond to efficiently integrable probability distributions. (2002). arXiv: quant-ph/0208112\n\nQuantum algorithm for linear systems of equations. Aram W Harrow, Avinatan Hassidim, Seth Lloyd, arXiv:0811.3171Physical Review Letters. 103150502Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. 2009. Quantum algorithm for linear systems of equations. Physical Review Letters 103, 15 (2009), 150502. arXiv: 0811.3171\n\nBeating SGD: Learning SVMs in sublinear time. Elad Hazan, Tomer Koren, Nati Srebro, Advances in Neural Information Processing Systems. J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger24Elad Hazan, Tomer Koren, and Nati Srebro. 2011. Beating SGD: Learning SVMs in sublinear time. In Advances in Neural Information Processing Systems 24, J. Shawe- Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger (Eds.). 1233- 1241.\n\nQuantum-inspired classical algorithms for singular value transformation. Dhawal Jethwani, Fran\u00e7ois Le Gall, Sanjay K Singh, arXiv:1910.05699Dhawal Jethwani, Fran\u00e7ois Le Gall, and Sanjay K. Singh. 2019. Quantum-inspired classical algorithms for singular value transformation. (2019). arXiv: 1910.05699\n\nRandomized algorithms in numerical linear algebra. Ravindran Kannan, Santosh Vempala, Acta Numerica. 26Ravindran Kannan and Santosh Vempala. 2017. Randomized algorithms in nu- merical linear algebra. Acta Numerica 26 (2017), 95-135.\n\nQuantum recommendation systems. Iordanis Kerenidis, Anupam Prakash, 1-49:21.arXiv:1603.08675Proceedings of the 8th Innovations in Theoretical Computer Science Conference (ITCS). the 8th Innovations in Theoretical Computer Science Conference (ITCS)49Iordanis Kerenidis and Anupam Prakash. 2017. Quantum recommendation sys- tems. In Proceedings of the 8th Innovations in Theoretical Computer Science Con- ference (ITCS). 49:1-49:21. arXiv: 1603.08675\n\nQuantum gradient descent for linear systems and least squares. Iordanis Kerenidis, Anupam Prakash, arXiv:1704.04992Physical Review A. 101222316Iordanis Kerenidis and Anupam Prakash. 2020. Quantum gradient descent for linear systems and least squares. Physical Review A 101, 2 (2020), 022316. arXiv: 1704.04992\n\nQuantum algorithms for topological and geometric analysis of data. Seth Lloyd, Silvano Garnerone, Paolo Zanardi, arXiv:1408.3106Nature Communications. 710138Seth Lloyd, Silvano Garnerone, and Paolo Zanardi. 2016. Quantum algorithms for topological and geometric analysis of data. Nature Communications 7 (2016), 10138. arXiv: 1408.3106\n\nQuantum algorithms for supervised and unsupervised machine learning. Seth Lloyd, Masoud Mohseni, Patrick Rebentrost, arXiv:1307.0411Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. 2013. Quantum algorithms for supervised and unsupervised machine learning. arXiv: 1307.0411\n\nQuantum principal component analysis. Seth Lloyd, Masoud Mohseni, Patrick Rebentrost, arXiv:1307.0401Nature Physics. 10Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. 2014. Quantum principal component analysis. Nature Physics 10 (2014), 631-633. arXiv: 1307.0401\n\nOptimal Hamiltonian simulation by quantum signal processing. Hao Guang, Isaac L Low, Chuang, arXiv:1606.02685Physical Review Letters. 11810501Guang Hao Low and Isaac L. Chuang. 2017. Optimal Hamiltonian simulation by quantum signal processing. Physical Review Letters 118, 1 (2017), 010501. arXiv: 1606.02685\n\nRandomized algorithms for matrices and data. Foundations and Trends\u00ae in. Michael W Mahoney, Machine Learning. 3Michael W. Mahoney. 2011. Randomized algorithms for matrices and data. Foun- dations and Trends\u00ae in Machine Learning 3, 2 (2011), 123-224.\n\nTensor-CUR decompositions for tensor-based data. Michael W Mahoney, Mauro Maggioni, Petros Drineas, SIAM J. Matrix Anal. Appl. 30Michael W. Mahoney, Mauro Maggioni, and Petros Drineas. 2008. Tensor-CUR decompositions for tensor-based data. SIAM J. Matrix Anal. Appl. 30, 3 (2008), 957-987.\n\nQuantum algorithms for linear algebra and machine learning. Anupam Prakash, Ph.D. Dissertation. UC BerkeleyAnupam Prakash. 2014. Quantum algorithms for linear algebra and machine learning. Ph.D. Dissertation. UC Berkeley.\n\nQuantum Computing in the NISQ era and beyond. John Preskill, arXiv:1801.00862279John Preskill. 2018. Quantum Computing in the NISQ era and beyond. Quantum 2 (2018), 79. arXiv: 1801.00862\n\nQuantum support vector machine for big data classification. Patrick Rebentrost, Masoud Mohseni, Seth Lloyd, arXiv:1307.0471Physical Review Letters. 113130503Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. 2014. Quantum support vector machine for big data classification. Physical Review Letters 113, 13 (2014), 130503. arXiv: 1307.0471\n\nQuantum gradient descent and Newton's method for constrained polynomial optimization. Patrick Rebentrost, Maria Schuld, Leonard Wossnig, Francesco Petruccione, Seth Lloyd, arXiv:1612.01789New Journal of Physics. 2173023Patrick Rebentrost, Maria Schuld, Leonard Wossnig, Francesco Petruccione, and Seth Lloyd. 2019. Quantum gradient descent and Newton's method for con- strained polynomial optimization. New Journal of Physics 21, 7 (2019), 073023. arXiv: 1612.01789\n\nApproximating Hamiltonian dynamics with the Nystr\u00f6m method. Alessandro Rudi, Leonard Wossnig, Carlo Ciliberto, Andrea Rocchetto, Massimiliano Pontil, Simone Severini, arXiv:1804.024844234Alessandro Rudi, Leonard Wossnig, Carlo Ciliberto, Andrea Rocchetto, Massimil- iano Pontil, and Simone Severini. 2020. Approximating Hamiltonian dynamics with the Nystr\u00f6m method. Quantum 4 (2020), 234. arXiv: 1804.02484\n\nPolynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. W Peter, Shor, arXiv:quant-ph/9508027Earlier version in FOCS'94. 26Peter W. Shor. 1997. Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer. SIAM Journal on Computing 26, 5 (1997), 1484-1509. Earlier version in FOCS'94. arXiv: quant-ph/9508027\n\nSublinear time orthogonal tensor decomposition. Zhao Song, David Woodruff, Huan Zhang, Advances in Neural Information Processing Systems. Curran Associates, Inc29Zhao Song, David Woodruff, and Huan Zhang. 2016. Sublinear time orthogonal tensor decomposition. In Advances in Neural Information Processing Systems 29. Curran Associates, Inc., 793-801.\n\nQuantum-inspired classical algorithms for principal component analysis and supervised clustering. Ewin Tang, arXiv:1811.00414Ewin Tang. 2018. Quantum-inspired classical algorithms for principal component analysis and supervised clustering. (2018). arXiv: 1811.00414\n\nA quantum-inspired classical algorithm for recommendation systems. Ewin Tang, arXiv:1807.04271Proceedings of the 51st ACM Symposium on the Theory of Computing (STOC). the 51st ACM Symposium on the Theory of Computing (STOC)Ewin Tang. 2019. A quantum-inspired classical algorithm for recommendation systems. In Proceedings of the 51st ACM Symposium on the Theory of Computing (STOC). 217-228. arXiv: 1807.04271\n\nSimulating quantum computers with probabilistic methods. Quantum Information and Computation. arXiv:0911.162411Maarten Van den NestMaarten Van den Nest. 2011. Simulating quantum computers with probabilistic methods. Quantum Information and Computation 11, 9&10 (2011), 784-812. arXiv: 0911.1624\n\nA linear algorithm for generating random numbers with a given distribution. D Michael, Vose, IEEE Transactions on Software Engineering. 17Michael D. Vose. 1991. A linear algorithm for generating random numbers with a given distribution. IEEE Transactions on Software Engineering 17, 9 (1991), 972-975.\n\nSketching as a tool for numerical linear algebra. David P Woodruff, Foundations and Trends\u00ae in Theoretical Computer Science. 10David P. Woodruff. 2014. Sketching as a tool for numerical linear algebra. Foun- dations and Trends\u00ae in Theoretical Computer Science 10, 1-2 (2014), 1-157.\n\nQuantum linear system algorithm for dense matrices. Leonard Wossnig, Zhikuan Zhao, Anupam Prakash, arXiv:1704.06174Physical Review Letters. 12050502Leonard Wossnig, Zhikuan Zhao, and Anupam Prakash. 2018. Quantum linear system algorithm for dense matrices. Physical Review Letters 120, 5 (2018), 050502. arXiv: 1704.06174\n\nQuantumassisted Gaussian process regression. Zhikuan Zhao, Jack K Fitzsimons, Joseph F Fitzsimons, arXiv:1512.03929Physical Review A. 9952331Issue 5Zhikuan Zhao, Jack K. Fitzsimons, and Joseph F. Fitzsimons. 2019. Quantum- assisted Gaussian process regression. Physical Review A 99 (May 2019), 052331. Issue 5. arXiv: 1512.03929\n", "annotations": {"author": "[{\"end\":136,\"start\":123},{\"end\":171,\"start\":137},{\"end\":204,\"start\":172},{\"end\":239,\"start\":205},{\"end\":274,\"start\":240},{\"end\":310,\"start\":275},{\"end\":324,\"start\":311},{\"end\":339,\"start\":325},{\"end\":352,\"start\":340},{\"end\":367,\"start\":353},{\"end\":378,\"start\":368},{\"end\":392,\"start\":379},{\"end\":440,\"start\":393},{\"end\":500,\"start\":441},{\"end\":550,\"start\":501},{\"end\":598,\"start\":551},{\"end\":647,\"start\":599},{\"end\":716,\"start\":648}]", "publisher": null, "author_last_name": "[{\"end\":135,\"start\":131},{\"end\":150,\"start\":144},{\"end\":183,\"start\":181},{\"end\":218,\"start\":215},{\"end\":249,\"start\":245},{\"end\":287,\"start\":283},{\"end\":323,\"start\":319},{\"end\":338,\"start\":332},{\"end\":351,\"start\":349},{\"end\":366,\"start\":363},{\"end\":377,\"start\":373},{\"end\":391,\"start\":387}]", "author_first_name": "[{\"end\":130,\"start\":123},{\"end\":143,\"start\":137},{\"end\":180,\"start\":172},{\"end\":214,\"start\":205},{\"end\":244,\"start\":240},{\"end\":282,\"start\":275},{\"end\":318,\"start\":311},{\"end\":331,\"start\":325},{\"end\":348,\"start\":340},{\"end\":362,\"start\":353},{\"end\":372,\"start\":368},{\"end\":386,\"start\":379}]", "author_affiliation": "[{\"end\":439,\"start\":394},{\"end\":499,\"start\":442},{\"end\":549,\"start\":502},{\"end\":597,\"start\":552},{\"end\":646,\"start\":600},{\"end\":715,\"start\":649}]", "title": "[{\"end\":104,\"start\":1},{\"end\":820,\"start\":717}]", "venue": "[{\"end\":830,\"start\":822}]", "abstract": "[{\"end\":2835,\"start\":1182}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3055,\"start\":3051},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3058,\"start\":3055},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3220,\"start\":3216},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3389,\"start\":3385},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3528,\"start\":3524},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3674,\"start\":3671},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4078,\"start\":4074},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4141,\"start\":4137},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4752,\"start\":4748},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4787,\"start\":4784},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4790,\"start\":4787},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4833,\"start\":4830},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4863,\"start\":4859},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4901,\"start\":4898},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4932,\"start\":4928},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5188,\"start\":5185},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5191,\"start\":5188},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5194,\"start\":5191},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5370,\"start\":5366},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5963,\"start\":5960},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5966,\"start\":5963},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5969,\"start\":5966},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7161,\"start\":7158},{\"end\":7925,\"start\":7912},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8068,\"start\":8064},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8071,\"start\":8068},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9584,\"start\":9580},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12018,\"start\":12014},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12452,\"start\":12448},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12884,\"start\":12880},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13972,\"start\":13971},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14039,\"start\":14038},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14246,\"start\":14245},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14407,\"start\":14403},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15172,\"start\":15171},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16286,\"start\":16285},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17248,\"start\":17244},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17435,\"start\":17431},{\"end\":17837,\"start\":17828},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17935,\"start\":17931},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17991,\"start\":17987},{\"end\":18000,\"start\":17991},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19378,\"start\":19377},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19659,\"start\":19658},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19768,\"start\":19764},{\"end\":19777,\"start\":19768},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20284,\"start\":20283},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20725,\"start\":20721},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20760,\"start\":20756},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20788,\"start\":20784},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20818,\"start\":20814},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20849,\"start\":20846},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20852,\"start\":20849},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20890,\"start\":20887},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21057,\"start\":21053},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21844,\"start\":21840},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21905,\"start\":21901},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22136,\"start\":22132},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23218,\"start\":23214},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23812,\"start\":23808},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23878,\"start\":23874},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25381,\"start\":25378},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25384,\"start\":25381},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26105,\"start\":26101},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26347,\"start\":26343},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26419,\"start\":26415},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":27317,\"start\":27313},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28993,\"start\":28990},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29047,\"start\":29044},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29049,\"start\":29047},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29570,\"start\":29567},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29863,\"start\":29859},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31159,\"start\":31158},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32781,\"start\":32777},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":32972,\"start\":32969},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":33444,\"start\":33440},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":33495,\"start\":33491},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":34659,\"start\":34655},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35958,\"start\":35954},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":36698,\"start\":36694},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":37089,\"start\":37086},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37091,\"start\":37089},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37094,\"start\":37091},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":37097,\"start\":37094},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39828,\"start\":39825},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39830,\"start\":39828},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":39833,\"start\":39830},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39928,\"start\":39924},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":39931,\"start\":39928},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41688,\"start\":41685},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":43337,\"start\":43333},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":43340,\"start\":43337},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43427,\"start\":43423},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":43529,\"start\":43525},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":43655,\"start\":43651},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":43658,\"start\":43655},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":43661,\"start\":43658},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":43703,\"start\":43699},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":43737,\"start\":43733},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":43862,\"start\":43858},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":44026,\"start\":44022},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":44545,\"start\":44541},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":44967,\"start\":44964},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":45774,\"start\":45770},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":46077,\"start\":46074},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":46931,\"start\":46927},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47042,\"start\":47038},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":47111,\"start\":47108},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":47289,\"start\":47285},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47324,\"start\":47320},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":47415,\"start\":47411},{\"end\":47612,\"start\":47608},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":48392,\"start\":48388},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":56272,\"start\":56268},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":56334,\"start\":56332},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":57796,\"start\":57792},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":58213,\"start\":58209},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":58216,\"start\":58213},{\"end\":59074,\"start\":59068},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":61742,\"start\":61738},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":62643,\"start\":62640},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":62646,\"start\":62643},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":62649,\"start\":62646},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":62652,\"start\":62649},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":62655,\"start\":62652},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":62658,\"start\":62655},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":63559,\"start\":63555},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":63969,\"start\":63965},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":65229,\"start\":65225},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":65332,\"start\":65328},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":70633,\"start\":70629},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":71267,\"start\":71263},{\"end\":71278,\"start\":71267},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":71400,\"start\":71396},{\"end\":71409,\"start\":71400},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":71761,\"start\":71759},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":73451,\"start\":73447},{\"end\":73460,\"start\":73451},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":74179,\"start\":74175},{\"end\":74188,\"start\":74179},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":76669,\"start\":76668},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":76749,\"start\":76746},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":77703,\"start\":77702},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":77903,\"start\":77902}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":74688,\"start\":74362},{\"attributes\":{\"id\":\"fig_1\"},\"end\":74898,\"start\":74689},{\"attributes\":{\"id\":\"fig_2\"},\"end\":75051,\"start\":74899},{\"attributes\":{\"id\":\"fig_3\"},\"end\":75525,\"start\":75052},{\"attributes\":{\"id\":\"fig_4\"},\"end\":75562,\"start\":75526},{\"attributes\":{\"id\":\"fig_5\"},\"end\":75591,\"start\":75563},{\"attributes\":{\"id\":\"fig_6\"},\"end\":75862,\"start\":75592},{\"attributes\":{\"id\":\"fig_7\"},\"end\":75955,\"start\":75863},{\"attributes\":{\"id\":\"fig_8\"},\"end\":76086,\"start\":75956},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":77192,\"start\":76087},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":77418,\"start\":77193}]", "paragraph": "[{\"end\":3848,\"start\":2864},{\"end\":5711,\"start\":3850},{\"end\":6339,\"start\":5713},{\"end\":6729,\"start\":6356},{\"end\":7513,\"start\":6731},{\"end\":8255,\"start\":7515},{\"end\":10074,\"start\":8257},{\"end\":10382,\"start\":10231},{\"end\":11417,\"start\":10550},{\"end\":12159,\"start\":11419},{\"end\":12499,\"start\":12161},{\"end\":12640,\"start\":12518},{\"end\":12948,\"start\":12749},{\"end\":13369,\"start\":12950},{\"end\":13521,\"start\":13420},{\"end\":14123,\"start\":13635},{\"end\":14408,\"start\":14187},{\"end\":15028,\"start\":14410},{\"end\":15734,\"start\":15030},{\"end\":16061,\"start\":15736},{\"end\":16416,\"start\":16063},{\"end\":16837,\"start\":16468},{\"end\":17155,\"start\":16878},{\"end\":18457,\"start\":17157},{\"end\":19128,\"start\":18459},{\"end\":20577,\"start\":19130},{\"end\":21183,\"start\":20619},{\"end\":21740,\"start\":21185},{\"end\":22557,\"start\":21742},{\"end\":23645,\"start\":22559},{\"end\":24095,\"start\":23730},{\"end\":24379,\"start\":24309},{\"end\":24598,\"start\":24421},{\"end\":25095,\"start\":24729},{\"end\":25599,\"start\":25251},{\"end\":25603,\"start\":25601},{\"end\":25999,\"start\":25755},{\"end\":26156,\"start\":26025},{\"end\":26616,\"start\":26219},{\"end\":26815,\"start\":26655},{\"end\":27463,\"start\":26894},{\"end\":27647,\"start\":27465},{\"end\":28014,\"start\":27758},{\"end\":28872,\"start\":28138},{\"end\":29109,\"start\":28874},{\"end\":29509,\"start\":29266},{\"end\":30086,\"start\":29511},{\"end\":30359,\"start\":30181},{\"end\":30417,\"start\":30361},{\"end\":30705,\"start\":30600},{\"end\":31021,\"start\":30760},{\"end\":31964,\"start\":31095},{\"end\":33570,\"start\":32039},{\"end\":34375,\"start\":33585},{\"end\":34842,\"start\":34377},{\"end\":35443,\"start\":34844},{\"end\":36295,\"start\":35445},{\"end\":36682,\"start\":36297},{\"end\":36922,\"start\":36684},{\"end\":37781,\"start\":37052},{\"end\":38275,\"start\":37783},{\"end\":38926,\"start\":38331},{\"end\":39657,\"start\":38928},{\"end\":40186,\"start\":39659},{\"end\":40353,\"start\":40188},{\"end\":40762,\"start\":40355},{\"end\":41280,\"start\":40834},{\"end\":41567,\"start\":41282},{\"end\":41892,\"start\":41569},{\"end\":42288,\"start\":42026},{\"end\":43009,\"start\":42290},{\"end\":43176,\"start\":43026},{\"end\":43738,\"start\":43178},{\"end\":44910,\"start\":43740},{\"end\":46621,\"start\":44912},{\"end\":47325,\"start\":46623},{\"end\":47613,\"start\":47327},{\"end\":48117,\"start\":47653},{\"end\":48329,\"start\":48136},{\"end\":48787,\"start\":48331},{\"end\":49373,\"start\":48789},{\"end\":49968,\"start\":49391},{\"end\":50604,\"start\":49987},{\"end\":50865,\"start\":50606},{\"end\":50954,\"start\":50901},{\"end\":51126,\"start\":50971},{\"end\":51321,\"start\":51217},{\"end\":51776,\"start\":51419},{\"end\":51848,\"start\":51778},{\"end\":52789,\"start\":51992},{\"end\":53181,\"start\":52791},{\"end\":53221,\"start\":53183},{\"end\":53516,\"start\":53223},{\"end\":53690,\"start\":53518},{\"end\":53966,\"start\":53692},{\"end\":54648,\"start\":54157},{\"end\":54868,\"start\":54700},{\"end\":55811,\"start\":55036},{\"end\":56588,\"start\":55813},{\"end\":56798,\"start\":56719},{\"end\":57095,\"start\":56800},{\"end\":57259,\"start\":57125},{\"end\":58897,\"start\":57343},{\"end\":59384,\"start\":58899},{\"end\":59797,\"start\":59659},{\"end\":60312,\"start\":60181},{\"end\":60509,\"start\":60314},{\"end\":60887,\"start\":60799},{\"end\":61246,\"start\":60889},{\"end\":61662,\"start\":61248},{\"end\":61857,\"start\":61664},{\"end\":62005,\"start\":61859},{\"end\":62541,\"start\":62007},{\"end\":63058,\"start\":62543},{\"end\":63361,\"start\":63060},{\"end\":63560,\"start\":63363},{\"end\":64075,\"start\":63562},{\"end\":64401,\"start\":64077},{\"end\":64941,\"start\":64403},{\"end\":65230,\"start\":64943},{\"end\":65663,\"start\":65232},{\"end\":65868,\"start\":65683},{\"end\":66046,\"start\":65870},{\"end\":66912,\"start\":66048},{\"end\":67002,\"start\":66914},{\"end\":67578,\"start\":67051},{\"end\":68323,\"start\":67651},{\"end\":68705,\"start\":68352},{\"end\":68812,\"start\":68761},{\"end\":69063,\"start\":68814},{\"end\":69197,\"start\":69131},{\"end\":69325,\"start\":69257},{\"end\":69755,\"start\":69434},{\"end\":70495,\"start\":69757},{\"end\":70673,\"start\":70517},{\"end\":70864,\"start\":70675},{\"end\":72312,\"start\":70866},{\"end\":73864,\"start\":72314},{\"end\":74117,\"start\":73866},{\"end\":74361,\"start\":74119}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10230,\"start\":10075},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10549,\"start\":10383},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12517,\"start\":12500},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12748,\"start\":12641},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13419,\"start\":13370},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13634,\"start\":13522},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14186,\"start\":14124},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16467,\"start\":16417},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16877,\"start\":16838},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23729,\"start\":23646},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24308,\"start\":24096},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24420,\"start\":24380},{\"attributes\":{\"id\":\"formula_12\"},\"end\":24728,\"start\":24599},{\"attributes\":{\"id\":\"formula_13\"},\"end\":25250,\"start\":25096},{\"attributes\":{\"id\":\"formula_14\"},\"end\":25754,\"start\":25604},{\"attributes\":{\"id\":\"formula_15\"},\"end\":26024,\"start\":26000},{\"attributes\":{\"id\":\"formula_16\"},\"end\":26218,\"start\":26157},{\"attributes\":{\"id\":\"formula_17\"},\"end\":26654,\"start\":26617},{\"attributes\":{\"id\":\"formula_18\"},\"end\":26893,\"start\":26816},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27757,\"start\":27648},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28137,\"start\":28015},{\"attributes\":{\"id\":\"formula_21\"},\"end\":29265,\"start\":29110},{\"attributes\":{\"id\":\"formula_22\"},\"end\":30153,\"start\":30087},{\"attributes\":{\"id\":\"formula_23\"},\"end\":30180,\"start\":30153},{\"attributes\":{\"id\":\"formula_24\"},\"end\":30599,\"start\":30418},{\"attributes\":{\"id\":\"formula_25\"},\"end\":30759,\"start\":30706},{\"attributes\":{\"id\":\"formula_26\"},\"end\":31047,\"start\":31022},{\"attributes\":{\"id\":\"formula_27\"},\"end\":31094,\"start\":31047},{\"attributes\":{\"id\":\"formula_28\"},\"end\":32038,\"start\":31965},{\"attributes\":{\"id\":\"formula_29\"},\"end\":37051,\"start\":36923},{\"attributes\":{\"id\":\"formula_30\"},\"end\":38330,\"start\":38276},{\"attributes\":{\"id\":\"formula_31\"},\"end\":40833,\"start\":40763},{\"attributes\":{\"id\":\"formula_32\"},\"end\":42025,\"start\":41893},{\"attributes\":{\"id\":\"formula_33\"},\"end\":47652,\"start\":47614},{\"attributes\":{\"id\":\"formula_34\"},\"end\":50900,\"start\":50866},{\"attributes\":{\"id\":\"formula_35\"},\"end\":50970,\"start\":50955},{\"attributes\":{\"id\":\"formula_36\"},\"end\":51201,\"start\":51127},{\"attributes\":{\"id\":\"formula_37\"},\"end\":51216,\"start\":51201},{\"attributes\":{\"id\":\"formula_38\"},\"end\":51418,\"start\":51322},{\"attributes\":{\"id\":\"formula_39\"},\"end\":51955,\"start\":51849},{\"attributes\":{\"id\":\"formula_40\"},\"end\":54156,\"start\":53967},{\"attributes\":{\"id\":\"formula_41\"},\"end\":55035,\"start\":54869},{\"attributes\":{\"id\":\"formula_42\"},\"end\":56718,\"start\":56589},{\"attributes\":{\"id\":\"formula_43\"},\"end\":57124,\"start\":57096},{\"attributes\":{\"id\":\"formula_44\"},\"end\":57342,\"start\":57260},{\"attributes\":{\"id\":\"formula_45\"},\"end\":59658,\"start\":59385},{\"attributes\":{\"id\":\"formula_46\"},\"end\":60180,\"start\":59798},{\"attributes\":{\"id\":\"formula_47\"},\"end\":60798,\"start\":60510},{\"attributes\":{\"id\":\"formula_48\"},\"end\":67650,\"start\":67579},{\"attributes\":{\"id\":\"formula_49\"},\"end\":68351,\"start\":68324},{\"attributes\":{\"id\":\"formula_50\"},\"end\":68760,\"start\":68706},{\"attributes\":{\"id\":\"formula_51\"},\"end\":69256,\"start\":69198},{\"attributes\":{\"id\":\"formula_52\"},\"end\":69364,\"start\":69326},{\"attributes\":{\"id\":\"formula_53\"},\"end\":69433,\"start\":69364}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2862,\"start\":2837},{\"attributes\":{\"n\":\"1.2\"},\"end\":6354,\"start\":6342},{\"attributes\":{\"n\":\"1.3\"},\"end\":20617,\"start\":20580},{\"attributes\":{\"n\":\"1.4\"},\"end\":33583,\"start\":33573},{\"attributes\":{\"n\":\"1.5\"},\"end\":43024,\"start\":43012},{\"attributes\":{\"n\":\"1.6\"},\"end\":48134,\"start\":48120},{\"attributes\":{\"n\":\"2\"},\"end\":49389,\"start\":49376},{\"attributes\":{\"n\":\"2.1\"},\"end\":49985,\"start\":49971},{\"attributes\":{\"n\":\"2.2\"},\"end\":51990,\"start\":51957},{\"end\":54698,\"start\":54651},{\"attributes\":{\"n\":\"2.3\"},\"end\":65681,\"start\":65666},{\"attributes\":{\"n\":\"3\"},\"end\":67017,\"start\":67005},{\"attributes\":{\"n\":\"3.1\"},\"end\":67049,\"start\":67020},{\"end\":69129,\"start\":69066},{\"attributes\":{\"n\":\"3.2\"},\"end\":70515,\"start\":70498},{\"end\":74704,\"start\":74690},{\"end\":74902,\"start\":74900},{\"end\":75528,\"start\":75527},{\"end\":75565,\"start\":75564},{\"end\":75607,\"start\":75593}]", "table": "[{\"end\":77192,\"start\":77079}]", "figure_caption": "[{\"end\":74688,\"start\":74364},{\"end\":74898,\"start\":74706},{\"end\":75051,\"start\":74903},{\"end\":75525,\"start\":75054},{\"end\":75562,\"start\":75529},{\"end\":75591,\"start\":75566},{\"end\":75862,\"start\":75609},{\"end\":75955,\"start\":75865},{\"end\":76086,\"start\":75958},{\"end\":77079,\"start\":76089},{\"end\":77418,\"start\":77195}]", "figure_ref": "[{\"end\":60198,\"start\":60190},{\"end\":63091,\"start\":63085}]", "bib_author_first_name": "[{\"end\":78323,\"start\":78317},{\"end\":78856,\"start\":78849},{\"end\":78870,\"start\":78864},{\"end\":79090,\"start\":79086},{\"end\":79113,\"start\":79108},{\"end\":79127,\"start\":79123},{\"end\":79442,\"start\":79441},{\"end\":79446,\"start\":79443},{\"end\":79461,\"start\":79457},{\"end\":79479,\"start\":79471},{\"end\":79500,\"start\":79487},{\"end\":79511,\"start\":79505},{\"end\":79513,\"start\":79512},{\"end\":79525,\"start\":79519},{\"end\":80184,\"start\":80175},{\"end\":80204,\"start\":80198},{\"end\":80219,\"start\":80213},{\"end\":80833,\"start\":80826},{\"end\":80845,\"start\":80840},{\"end\":80858,\"start\":80850},{\"end\":80867,\"start\":80864},{\"end\":80880,\"start\":80874},{\"end\":81482,\"start\":81475},{\"end\":81497,\"start\":81489},{\"end\":81511,\"start\":81502},{\"end\":81524,\"start\":81517},{\"end\":81856,\"start\":81849},{\"end\":81872,\"start\":81863},{\"end\":81885,\"start\":81878},{\"end\":82129,\"start\":82124},{\"end\":82145,\"start\":82141},{\"end\":82166,\"start\":82156},{\"end\":82173,\"start\":82167},{\"end\":82195,\"start\":82183},{\"end\":82210,\"start\":82204},{\"end\":82228,\"start\":82222},{\"end\":82246,\"start\":82239},{\"end\":82746,\"start\":82742},{\"end\":82759,\"start\":82753},{\"end\":83068,\"start\":83062},{\"end\":83085,\"start\":83077},{\"end\":83102,\"start\":83095},{\"end\":83545,\"start\":83541},{\"end\":83559,\"start\":83552},{\"end\":83573,\"start\":83565},{\"end\":83835,\"start\":83829},{\"end\":83849,\"start\":83845},{\"end\":83865,\"start\":83858},{\"end\":83867,\"start\":83866},{\"end\":84118,\"start\":84112},{\"end\":84136,\"start\":84128},{\"end\":84157,\"start\":84148},{\"end\":84581,\"start\":84575},{\"end\":84598,\"start\":84591},{\"end\":84600,\"start\":84599},{\"end\":84868,\"start\":84862},{\"end\":84885,\"start\":84878},{\"end\":84887,\"start\":84886},{\"end\":84898,\"start\":84897},{\"end\":85183,\"start\":85177},{\"end\":85196,\"start\":85188},{\"end\":85213,\"start\":85204},{\"end\":85226,\"start\":85219},{\"end\":85478,\"start\":85472},{\"end\":85492,\"start\":85487},{\"end\":85728,\"start\":85724},{\"end\":85741,\"start\":85737},{\"end\":85757,\"start\":85750},{\"end\":86054,\"start\":86048},{\"end\":86067,\"start\":86063},{\"end\":86079,\"start\":86075},{\"end\":86384,\"start\":86378},{\"end\":86397,\"start\":86393},{\"end\":86407,\"start\":86402},{\"end\":86423,\"start\":86417},{\"end\":86890,\"start\":86882},{\"end\":86908,\"start\":86904},{\"end\":86923,\"start\":86916},{\"end\":87235,\"start\":87232},{\"end\":87249,\"start\":87244},{\"end\":87498,\"start\":87494},{\"end\":87500,\"start\":87499},{\"end\":87517,\"start\":87509},{\"end\":87532,\"start\":87528},{\"end\":87813,\"start\":87809},{\"end\":87826,\"start\":87821},{\"end\":87838,\"start\":87834},{\"end\":88309,\"start\":88303},{\"end\":88328,\"start\":88320},{\"end\":88331,\"start\":88329},{\"end\":88344,\"start\":88338},{\"end\":88346,\"start\":88345},{\"end\":88592,\"start\":88583},{\"end\":88608,\"start\":88601},{\"end\":88806,\"start\":88798},{\"end\":88824,\"start\":88818},{\"end\":89287,\"start\":89279},{\"end\":89305,\"start\":89299},{\"end\":89598,\"start\":89594},{\"end\":89613,\"start\":89606},{\"end\":89630,\"start\":89625},{\"end\":89937,\"start\":89933},{\"end\":89951,\"start\":89945},{\"end\":89968,\"start\":89961},{\"end\":90183,\"start\":90179},{\"end\":90197,\"start\":90191},{\"end\":90214,\"start\":90207},{\"end\":90473,\"start\":90470},{\"end\":90486,\"start\":90481},{\"end\":90488,\"start\":90487},{\"end\":90799,\"start\":90792},{\"end\":90801,\"start\":90800},{\"end\":91026,\"start\":91019},{\"end\":91028,\"start\":91027},{\"end\":91043,\"start\":91038},{\"end\":91060,\"start\":91054},{\"end\":91327,\"start\":91321},{\"end\":91534,\"start\":91530},{\"end\":91739,\"start\":91732},{\"end\":91758,\"start\":91752},{\"end\":91772,\"start\":91768},{\"end\":92106,\"start\":92099},{\"end\":92124,\"start\":92119},{\"end\":92140,\"start\":92133},{\"end\":92159,\"start\":92150},{\"end\":92177,\"start\":92173},{\"end\":92550,\"start\":92540},{\"end\":92564,\"start\":92557},{\"end\":92579,\"start\":92574},{\"end\":92597,\"start\":92591},{\"end\":92621,\"start\":92609},{\"end\":92636,\"start\":92630},{\"end\":92987,\"start\":92986},{\"end\":93328,\"start\":93324},{\"end\":93340,\"start\":93335},{\"end\":93355,\"start\":93351},{\"end\":93729,\"start\":93725},{\"end\":93965,\"start\":93961},{\"end\":94678,\"start\":94677},{\"end\":94959,\"start\":94954},{\"end\":94961,\"start\":94960},{\"end\":95247,\"start\":95240},{\"end\":95264,\"start\":95257},{\"end\":95277,\"start\":95271},{\"end\":95563,\"start\":95556},{\"end\":95574,\"start\":95570},{\"end\":95576,\"start\":95575},{\"end\":95595,\"start\":95589},{\"end\":95597,\"start\":95596}]", "bib_author_last_name": "[{\"end\":78162,\"start\":78148},{\"end\":78343,\"start\":78324},{\"end\":78351,\"start\":78345},{\"end\":78862,\"start\":78857},{\"end\":78875,\"start\":78871},{\"end\":79106,\"start\":79091},{\"end\":79121,\"start\":79114},{\"end\":79147,\"start\":79128},{\"end\":79154,\"start\":79149},{\"end\":79455,\"start\":79447},{\"end\":79469,\"start\":79462},{\"end\":79485,\"start\":79480},{\"end\":79503,\"start\":79501},{\"end\":79517,\"start\":79514},{\"end\":79531,\"start\":79526},{\"end\":79535,\"start\":79533},{\"end\":80196,\"start\":80185},{\"end\":80211,\"start\":80205},{\"end\":80227,\"start\":80220},{\"end\":80838,\"start\":80834},{\"end\":80848,\"start\":80846},{\"end\":80862,\"start\":80859},{\"end\":80872,\"start\":80868},{\"end\":80886,\"start\":80881},{\"end\":81487,\"start\":81483},{\"end\":81500,\"start\":81498},{\"end\":81515,\"start\":81512},{\"end\":81529,\"start\":81525},{\"end\":81861,\"start\":81857},{\"end\":81876,\"start\":81873},{\"end\":81890,\"start\":81886},{\"end\":82139,\"start\":82130},{\"end\":82154,\"start\":82146},{\"end\":82181,\"start\":82174},{\"end\":82202,\"start\":82196},{\"end\":82220,\"start\":82211},{\"end\":82237,\"start\":82229},{\"end\":82254,\"start\":82247},{\"end\":82751,\"start\":82747},{\"end\":82764,\"start\":82760},{\"end\":83075,\"start\":83069},{\"end\":83093,\"start\":83086},{\"end\":83111,\"start\":83103},{\"end\":83550,\"start\":83546},{\"end\":83563,\"start\":83560},{\"end\":83579,\"start\":83574},{\"end\":83843,\"start\":83836},{\"end\":83856,\"start\":83850},{\"end\":83875,\"start\":83868},{\"end\":84126,\"start\":84119},{\"end\":84146,\"start\":84137},{\"end\":84166,\"start\":84158},{\"end\":84589,\"start\":84582},{\"end\":84608,\"start\":84601},{\"end\":84876,\"start\":84869},{\"end\":84895,\"start\":84888},{\"end\":84912,\"start\":84899},{\"end\":85186,\"start\":85184},{\"end\":85202,\"start\":85197},{\"end\":85217,\"start\":85214},{\"end\":85230,\"start\":85227},{\"end\":85485,\"start\":85479},{\"end\":85499,\"start\":85493},{\"end\":85735,\"start\":85729},{\"end\":85748,\"start\":85742},{\"end\":85765,\"start\":85758},{\"end\":86061,\"start\":86055},{\"end\":86073,\"start\":86068},{\"end\":86084,\"start\":86080},{\"end\":86391,\"start\":86385},{\"end\":86400,\"start\":86398},{\"end\":86415,\"start\":86408},{\"end\":86429,\"start\":86424},{\"end\":86902,\"start\":86891},{\"end\":86914,\"start\":86909},{\"end\":86931,\"start\":86924},{\"end\":87242,\"start\":87236},{\"end\":87257,\"start\":87250},{\"end\":87507,\"start\":87501},{\"end\":87526,\"start\":87518},{\"end\":87538,\"start\":87533},{\"end\":87819,\"start\":87814},{\"end\":87832,\"start\":87827},{\"end\":87845,\"start\":87839},{\"end\":88318,\"start\":88310},{\"end\":88336,\"start\":88332},{\"end\":88352,\"start\":88347},{\"end\":88599,\"start\":88593},{\"end\":88616,\"start\":88609},{\"end\":88816,\"start\":88807},{\"end\":88832,\"start\":88825},{\"end\":89297,\"start\":89288},{\"end\":89313,\"start\":89306},{\"end\":89604,\"start\":89599},{\"end\":89623,\"start\":89614},{\"end\":89638,\"start\":89631},{\"end\":89943,\"start\":89938},{\"end\":89959,\"start\":89952},{\"end\":89979,\"start\":89969},{\"end\":90189,\"start\":90184},{\"end\":90205,\"start\":90198},{\"end\":90225,\"start\":90215},{\"end\":90479,\"start\":90474},{\"end\":90492,\"start\":90489},{\"end\":90500,\"start\":90494},{\"end\":90809,\"start\":90802},{\"end\":91036,\"start\":91029},{\"end\":91052,\"start\":91044},{\"end\":91068,\"start\":91061},{\"end\":91335,\"start\":91328},{\"end\":91543,\"start\":91535},{\"end\":91750,\"start\":91740},{\"end\":91766,\"start\":91759},{\"end\":91778,\"start\":91773},{\"end\":92117,\"start\":92107},{\"end\":92131,\"start\":92125},{\"end\":92148,\"start\":92141},{\"end\":92171,\"start\":92160},{\"end\":92183,\"start\":92178},{\"end\":92555,\"start\":92551},{\"end\":92572,\"start\":92565},{\"end\":92589,\"start\":92580},{\"end\":92607,\"start\":92598},{\"end\":92628,\"start\":92622},{\"end\":92645,\"start\":92637},{\"end\":92993,\"start\":92988},{\"end\":92999,\"start\":92995},{\"end\":93333,\"start\":93329},{\"end\":93349,\"start\":93341},{\"end\":93361,\"start\":93356},{\"end\":93734,\"start\":93730},{\"end\":93970,\"start\":93966},{\"end\":94686,\"start\":94679},{\"end\":94692,\"start\":94688},{\"end\":94970,\"start\":94962},{\"end\":95255,\"start\":95248},{\"end\":95269,\"start\":95265},{\"end\":95285,\"start\":95278},{\"end\":95568,\"start\":95564},{\"end\":95587,\"start\":95577},{\"end\":95608,\"start\":95598}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2246294},\"end\":78261,\"start\":78127},{\"attributes\":{\"doi\":\"arXiv:1804.05058\",\"id\":\"b1\"},\"end\":78783,\"start\":78263},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":150819},\"end\":79084,\"start\":78785},{\"attributes\":{\"doi\":\"arXiv:1905.10415\",\"id\":\"b3\"},\"end\":79351,\"start\":79086},{\"attributes\":{\"doi\":\"arXiv:1710.02581\",\"id\":\"b4\",\"matched_paper_id\":4869061},\"end\":80065,\"start\":79353},{\"attributes\":{\"doi\":\"1-33:14.arXiv:1804.01973\",\"id\":\"b5\",\"matched_paper_id\":4614529},\"end\":80736,\"start\":80067},{\"attributes\":{\"doi\":\"arXiv:1907.05568\",\"id\":\"b6\",\"matched_paper_id\":196470807},\"end\":81351,\"start\":80738},{\"attributes\":{\"doi\":\"arXiv:1901.03254\",\"id\":\"b7\"},\"end\":81762,\"start\":81353},{\"attributes\":{\"doi\":\"arXiv:1811.04852\",\"id\":\"b8\"},\"end\":82071,\"start\":81764},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3306944},\"end\":82661,\"start\":82073},{\"attributes\":{\"doi\":\"arXiv:1510.00113\",\"id\":\"b10\",\"matched_paper_id\":53690587},\"end\":82993,\"start\":82663},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":50769310},\"end\":83539,\"start\":82995},{\"attributes\":{\"doi\":\"arXiv:1906.08902\",\"id\":\"b12\"},\"end\":83746,\"start\":83541},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":702408},\"end\":84074,\"start\":83748},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3097965},\"end\":84479,\"start\":84076},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7730858},\"end\":84819,\"start\":84481},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":58270},\"end\":85104,\"start\":84821},{\"attributes\":{\"doi\":\"arXiv:1907.06814\",\"id\":\"b17\"},\"end\":85403,\"start\":85106},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":215885923},\"end\":85657,\"start\":85405},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2483891},\"end\":85952,\"start\":85659},{\"attributes\":{\"doi\":\"arXiv:1811.04909\",\"id\":\"b20\"},\"end\":86269,\"start\":85954},{\"attributes\":{\"doi\":\"arXiv:1806.01838\",\"id\":\"b21\",\"matched_paper_id\":46941335},\"end\":86850,\"start\":86271},{\"attributes\":{\"doi\":\"arXiv:0708.1879\",\"id\":\"b22\",\"matched_paper_id\":570390},\"end\":87137,\"start\":86852},{\"attributes\":{\"doi\":\"arXiv:quant-ph/0208112\",\"id\":\"b23\"},\"end\":87441,\"start\":87139},{\"attributes\":{\"doi\":\"arXiv:0811.3171\",\"id\":\"b24\",\"matched_paper_id\":5187993},\"end\":87761,\"start\":87443},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2700110},\"end\":88228,\"start\":87763},{\"attributes\":{\"doi\":\"arXiv:1910.05699\",\"id\":\"b26\"},\"end\":88530,\"start\":88230},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":26830284},\"end\":88764,\"start\":88532},{\"attributes\":{\"doi\":\"1-49:21.arXiv:1603.08675\",\"id\":\"b28\",\"matched_paper_id\":579463},\"end\":89214,\"start\":88766},{\"attributes\":{\"doi\":\"arXiv:1704.04992\",\"id\":\"b29\",\"matched_paper_id\":119415623},\"end\":89525,\"start\":89216},{\"attributes\":{\"doi\":\"arXiv:1408.3106\",\"id\":\"b30\",\"matched_paper_id\":7811252},\"end\":89862,\"start\":89527},{\"attributes\":{\"doi\":\"arXiv:1307.0411\",\"id\":\"b31\"},\"end\":90139,\"start\":89864},{\"attributes\":{\"doi\":\"arXiv:1307.0401\",\"id\":\"b32\",\"matched_paper_id\":5589954},\"end\":90407,\"start\":90141},{\"attributes\":{\"doi\":\"arXiv:1606.02685\",\"id\":\"b33\",\"matched_paper_id\":1118993},\"end\":90717,\"start\":90409},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":47503887},\"end\":90968,\"start\":90719},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":13526501},\"end\":91259,\"start\":90970},{\"attributes\":{\"id\":\"b36\"},\"end\":91482,\"start\":91261},{\"attributes\":{\"doi\":\"arXiv:1801.00862\",\"id\":\"b37\"},\"end\":91670,\"start\":91484},{\"attributes\":{\"doi\":\"arXiv:1307.0471\",\"id\":\"b38\"},\"end\":92011,\"start\":91672},{\"attributes\":{\"doi\":\"arXiv:1612.01789\",\"id\":\"b39\",\"matched_paper_id\":119090868},\"end\":92478,\"start\":92013},{\"attributes\":{\"doi\":\"arXiv:1804.02484\",\"id\":\"b40\"},\"end\":92886,\"start\":92480},{\"attributes\":{\"doi\":\"arXiv:quant-ph/9508027\",\"id\":\"b41\",\"matched_paper_id\":2337707},\"end\":93274,\"start\":92888},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":15098786},\"end\":93625,\"start\":93276},{\"attributes\":{\"doi\":\"arXiv:1811.00414\",\"id\":\"b43\"},\"end\":93892,\"start\":93627},{\"attributes\":{\"doi\":\"arXiv:1807.04271\",\"id\":\"b44\",\"matched_paper_id\":44036160},\"end\":94303,\"start\":93894},{\"attributes\":{\"doi\":\"arXiv:0911.1624\",\"id\":\"b45\"},\"end\":94599,\"start\":94305},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":16030199},\"end\":94902,\"start\":94601},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":51783444},\"end\":95186,\"start\":94904},{\"attributes\":{\"doi\":\"arXiv:1704.06174\",\"id\":\"b48\",\"matched_paper_id\":3714239},\"end\":95509,\"start\":95188},{\"attributes\":{\"doi\":\"arXiv:1512.03929\",\"id\":\"b49\"},\"end\":95839,\"start\":95511}]", "bib_title": "[{\"end\":78146,\"start\":78127},{\"end\":78315,\"start\":78263},{\"end\":78847,\"start\":78785},{\"end\":79439,\"start\":79353},{\"end\":80173,\"start\":80067},{\"end\":80824,\"start\":80738},{\"end\":82122,\"start\":82073},{\"end\":82740,\"start\":82663},{\"end\":83060,\"start\":82995},{\"end\":83827,\"start\":83748},{\"end\":84110,\"start\":84076},{\"end\":84573,\"start\":84481},{\"end\":84860,\"start\":84821},{\"end\":85470,\"start\":85405},{\"end\":85722,\"start\":85659},{\"end\":86376,\"start\":86271},{\"end\":86880,\"start\":86852},{\"end\":87492,\"start\":87443},{\"end\":87807,\"start\":87763},{\"end\":88581,\"start\":88532},{\"end\":88796,\"start\":88766},{\"end\":89277,\"start\":89216},{\"end\":89592,\"start\":89527},{\"end\":90177,\"start\":90141},{\"end\":90468,\"start\":90409},{\"end\":90790,\"start\":90719},{\"end\":91017,\"start\":90970},{\"end\":91730,\"start\":91672},{\"end\":92097,\"start\":92013},{\"end\":92984,\"start\":92888},{\"end\":93322,\"start\":93276},{\"end\":93959,\"start\":93894},{\"end\":94675,\"start\":94601},{\"end\":94952,\"start\":94904},{\"end\":95238,\"start\":95188},{\"end\":95554,\"start\":95511}]", "bib_author": "[{\"end\":78164,\"start\":78148},{\"end\":78345,\"start\":78317},{\"end\":78353,\"start\":78345},{\"end\":78864,\"start\":78849},{\"end\":78877,\"start\":78864},{\"end\":79108,\"start\":79086},{\"end\":79123,\"start\":79108},{\"end\":79149,\"start\":79123},{\"end\":79156,\"start\":79149},{\"end\":79457,\"start\":79441},{\"end\":79471,\"start\":79457},{\"end\":79487,\"start\":79471},{\"end\":79505,\"start\":79487},{\"end\":79519,\"start\":79505},{\"end\":79533,\"start\":79519},{\"end\":79537,\"start\":79533},{\"end\":80198,\"start\":80175},{\"end\":80213,\"start\":80198},{\"end\":80229,\"start\":80213},{\"end\":80840,\"start\":80826},{\"end\":80850,\"start\":80840},{\"end\":80864,\"start\":80850},{\"end\":80874,\"start\":80864},{\"end\":80888,\"start\":80874},{\"end\":81489,\"start\":81475},{\"end\":81502,\"start\":81489},{\"end\":81517,\"start\":81502},{\"end\":81531,\"start\":81517},{\"end\":81863,\"start\":81849},{\"end\":81878,\"start\":81863},{\"end\":81892,\"start\":81878},{\"end\":82141,\"start\":82124},{\"end\":82156,\"start\":82141},{\"end\":82183,\"start\":82156},{\"end\":82204,\"start\":82183},{\"end\":82222,\"start\":82204},{\"end\":82239,\"start\":82222},{\"end\":82256,\"start\":82239},{\"end\":82753,\"start\":82742},{\"end\":82766,\"start\":82753},{\"end\":83077,\"start\":83062},{\"end\":83095,\"start\":83077},{\"end\":83113,\"start\":83095},{\"end\":83552,\"start\":83541},{\"end\":83565,\"start\":83552},{\"end\":83581,\"start\":83565},{\"end\":83845,\"start\":83829},{\"end\":83858,\"start\":83845},{\"end\":83877,\"start\":83858},{\"end\":84128,\"start\":84112},{\"end\":84148,\"start\":84128},{\"end\":84168,\"start\":84148},{\"end\":84591,\"start\":84575},{\"end\":84610,\"start\":84591},{\"end\":84878,\"start\":84862},{\"end\":84897,\"start\":84878},{\"end\":84914,\"start\":84897},{\"end\":85188,\"start\":85177},{\"end\":85204,\"start\":85188},{\"end\":85219,\"start\":85204},{\"end\":85232,\"start\":85219},{\"end\":85487,\"start\":85472},{\"end\":85501,\"start\":85487},{\"end\":85737,\"start\":85724},{\"end\":85750,\"start\":85737},{\"end\":85767,\"start\":85750},{\"end\":86063,\"start\":86048},{\"end\":86075,\"start\":86063},{\"end\":86086,\"start\":86075},{\"end\":86393,\"start\":86378},{\"end\":86402,\"start\":86393},{\"end\":86417,\"start\":86402},{\"end\":86431,\"start\":86417},{\"end\":86904,\"start\":86882},{\"end\":86916,\"start\":86904},{\"end\":86933,\"start\":86916},{\"end\":87244,\"start\":87232},{\"end\":87259,\"start\":87244},{\"end\":87509,\"start\":87494},{\"end\":87528,\"start\":87509},{\"end\":87540,\"start\":87528},{\"end\":87821,\"start\":87809},{\"end\":87834,\"start\":87821},{\"end\":87847,\"start\":87834},{\"end\":88320,\"start\":88303},{\"end\":88338,\"start\":88320},{\"end\":88354,\"start\":88338},{\"end\":88601,\"start\":88583},{\"end\":88618,\"start\":88601},{\"end\":88818,\"start\":88798},{\"end\":88834,\"start\":88818},{\"end\":89299,\"start\":89279},{\"end\":89315,\"start\":89299},{\"end\":89606,\"start\":89594},{\"end\":89625,\"start\":89606},{\"end\":89640,\"start\":89625},{\"end\":89945,\"start\":89933},{\"end\":89961,\"start\":89945},{\"end\":89981,\"start\":89961},{\"end\":90191,\"start\":90179},{\"end\":90207,\"start\":90191},{\"end\":90227,\"start\":90207},{\"end\":90481,\"start\":90470},{\"end\":90494,\"start\":90481},{\"end\":90502,\"start\":90494},{\"end\":90811,\"start\":90792},{\"end\":91038,\"start\":91019},{\"end\":91054,\"start\":91038},{\"end\":91070,\"start\":91054},{\"end\":91337,\"start\":91321},{\"end\":91545,\"start\":91530},{\"end\":91752,\"start\":91732},{\"end\":91768,\"start\":91752},{\"end\":91780,\"start\":91768},{\"end\":92119,\"start\":92099},{\"end\":92133,\"start\":92119},{\"end\":92150,\"start\":92133},{\"end\":92173,\"start\":92150},{\"end\":92185,\"start\":92173},{\"end\":92557,\"start\":92540},{\"end\":92574,\"start\":92557},{\"end\":92591,\"start\":92574},{\"end\":92609,\"start\":92591},{\"end\":92630,\"start\":92609},{\"end\":92647,\"start\":92630},{\"end\":92995,\"start\":92986},{\"end\":93001,\"start\":92995},{\"end\":93335,\"start\":93324},{\"end\":93351,\"start\":93335},{\"end\":93363,\"start\":93351},{\"end\":93736,\"start\":93725},{\"end\":93972,\"start\":93961},{\"end\":94688,\"start\":94677},{\"end\":94694,\"start\":94688},{\"end\":94972,\"start\":94954},{\"end\":95257,\"start\":95240},{\"end\":95271,\"start\":95257},{\"end\":95287,\"start\":95271},{\"end\":95570,\"start\":95556},{\"end\":95589,\"start\":95570},{\"end\":95610,\"start\":95589}]", "bib_venue": "[{\"end\":78548,\"start\":78467},{\"end\":79732,\"start\":79651},{\"end\":80432,\"start\":80351},{\"end\":81053,\"start\":80987},{\"end\":83292,\"start\":83211},{\"end\":84295,\"start\":84240},{\"end\":86576,\"start\":86520},{\"end\":89013,\"start\":88944},{\"end\":94117,\"start\":94061},{\"end\":78178,\"start\":78164},{\"end\":78465,\"start\":78369},{\"end\":78907,\"start\":78877},{\"end\":79211,\"start\":79172},{\"end\":79649,\"start\":79553},{\"end\":80349,\"start\":80253},{\"end\":80985,\"start\":80904},{\"end\":81473,\"start\":81353},{\"end\":81847,\"start\":81764},{\"end\":82339,\"start\":82256},{\"end\":82804,\"start\":82782},{\"end\":83209,\"start\":83113},{\"end\":83636,\"start\":83597},{\"end\":83891,\"start\":83877},{\"end\":84238,\"start\":84168},{\"end\":84629,\"start\":84610},{\"end\":84939,\"start\":84914},{\"end\":85175,\"start\":85106},{\"end\":85514,\"start\":85501},{\"end\":85785,\"start\":85767},{\"end\":86046,\"start\":85954},{\"end\":86518,\"start\":86447},{\"end\":86971,\"start\":86948},{\"end\":87230,\"start\":87139},{\"end\":87578,\"start\":87555},{\"end\":87896,\"start\":87847},{\"end\":88301,\"start\":88230},{\"end\":88631,\"start\":88618},{\"end\":88942,\"start\":88858},{\"end\":89348,\"start\":89331},{\"end\":89676,\"start\":89655},{\"end\":89931,\"start\":89864},{\"end\":90256,\"start\":90242},{\"end\":90541,\"start\":90518},{\"end\":90827,\"start\":90811},{\"end\":91095,\"start\":91070},{\"end\":91319,\"start\":91261},{\"end\":91528,\"start\":91484},{\"end\":91818,\"start\":91795},{\"end\":92223,\"start\":92201},{\"end\":92538,\"start\":92480},{\"end\":93049,\"start\":93023},{\"end\":93412,\"start\":93363},{\"end\":93723,\"start\":93627},{\"end\":94059,\"start\":93988},{\"end\":94397,\"start\":94305},{\"end\":94735,\"start\":94694},{\"end\":95027,\"start\":94972},{\"end\":95326,\"start\":95303},{\"end\":95643,\"start\":95626}]"}}}, "year": 2023, "month": 12, "day": 17}