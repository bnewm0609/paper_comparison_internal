{"id": 257900871, "updated": "2023-09-27 20:48:37.614", "metadata": {"title": "Self-Refine: Iterative Refinement with Self-Feedback", "authors": "[{\"first\":\"Aman\",\"last\":\"Madaan\",\"middle\":[]},{\"first\":\"Niket\",\"last\":\"Tandon\",\"middle\":[]},{\"first\":\"Prakhar\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Skyler\",\"last\":\"Hallinan\",\"middle\":[]},{\"first\":\"Luyu\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Sarah\",\"last\":\"Wiegreffe\",\"middle\":[]},{\"first\":\"Uri\",\"last\":\"Alon\",\"middle\":[]},{\"first\":\"Nouha\",\"last\":\"Dziri\",\"middle\":[]},{\"first\":\"Shrimai\",\"last\":\"Prabhumoye\",\"middle\":[]},{\"first\":\"Yiming\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Shashank\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Bodhisattwa\",\"last\":\"Majumder\",\"middle\":[\"Prasad\"]},{\"first\":\"Katherine\",\"last\":\"Hermann\",\"middle\":[]},{\"first\":\"Sean\",\"last\":\"Welleck\",\"middle\":[]},{\"first\":\"Amir\",\"last\":\"Yazdanbakhsh\",\"middle\":[]},{\"first\":\"Peter\",\"last\":\"Clark\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2303.17651", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2303-17651", "doi": "10.48550/arxiv.2303.17651"}}, "content": {"source": {"pdf_hash": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.17651v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "bbedfa7d3c12bae871dfabb1d6f1a85b368d0203", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3aaf6a2cbad5850ad81ab5c163599cb3d523436f.txt", "contents": "\nSELF-REFINE: Iterative Refinement with Self-Feedback\n\n\nAman Madaan amadaan@cs.cmu.edu \nLanguage Technologies Institute\nCarnegie Mellon University\n\n\nNiket Tandon nikett@allenai.org \nAllen Institute for Artificial Intelligence\n\n\nPrakhar Gupta \nLanguage Technologies Institute\nCarnegie Mellon University\n\n\nSkyler Hallinan \nUniversity of Washington\n4 NVIDIA 5 UC San Diego 6 Google ResearchBrain Team\n\nLuyu Gao \nLanguage Technologies Institute\nCarnegie Mellon University\n\n\nSarah Wiegreffe \nAllen Institute for Artificial Intelligence\n\n\nUri Alon \nLanguage Technologies Institute\nCarnegie Mellon University\n\n\nNouha Dziri \nAllen Institute for Artificial Intelligence\n\n\nShrimai Prabhumoye \nYiming Yang \nLanguage Technologies Institute\nCarnegie Mellon University\n\n\nShashank Gupta \nAllen Institute for Artificial Intelligence\n\n\nBodhisattwa Prasad Majumder \nKatherine Hermann \nSean Welleck \nAllen Institute for Artificial Intelligence\n\n\nUniversity of Washington\n4 NVIDIA 5 UC San Diego 6 Google ResearchBrain Team\n\nAmir Yazdanbakhsh \nPeter Clark \nAllen Institute for Artificial Intelligence\n\n\nSELF-REFINE: Iterative Refinement with Self-Feedback\n\nLike humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce SELF-REFINE, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLM; then, the same LLM provides feedback for its output and uses it to refine itself, iteratively. SELF-REFINE does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner and the feedback provider. We evaluate SELF-REFINE across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5 and GPT-4) LLMs. Across all evaluated tasks, outputs generated with SELF-REFINE are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by \u223c20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test-time using our simple, standalone approach. 1 .\n\nIntroduction\n\nAlthough large language models (LLMs) can generate coherent outputs, they often fall short in addressing intricate requirements. This mostly includes tasks with multifaceted objectives, such as dialogue response generation, or tasks with hard-to-define goals, such as enhancing program readability. In these scenarios, modern LLMs may produce an intelligible initial output, yet may benefit from further iterative refinement-i.e., iteratively mapping a candidate output to an improved one-to ensure that the desired quality is achieved. Iterative refinement typically involves training a refinement model that relies on domain-specific data (e.g., Reid and Neubig (2022); Schick et al. (2022a); Welleck et al. (2022)). Other approaches that rely on external supervision or reward models require large training sets or expensive human annotations (Madaan et al., 2021;Ouyang et al., 2022), which may not always be feasible to obtain. These limitations underscore the need for an effective refinement approach that can be applied to various tasks without requiring extensive supervision.\n\nIterative self -refinement is a fundamental characteristic of human problem-solving (Simon, 1962;Flower and Hayes, 1981;Amabile, 1983). Iterative self-refinement is a process that involves creating an initial draft and subsequently refining it based on self-provided feedback. For example, when\n\n\nRefine Feedback\n\nUse M to get feedback on its own output Input Use M to refine its previous output, given its feedback Model M 1 2 0 Figure 1: Given an input ( 0 \u20dd), SELF-REFINE starts by generating an output and passing it back to the same model M to get feedback ( 1 \u20dd). The feedback is passed back to M, which refines the previously generated output ( 2 \u20dd).\n\nSteps ( 1 \u20dd) and ( 2 \u20dd) iterate until a stopping condition is met. SELF-REFINE is instantiated with a language model such as GPT-3.5 and does not involve human assistance.\n\ndrafting an email to request a document from a colleague, an individual may initially write a direct request such as \"Send me the data ASAP\". Upon reflection, however, the writer recognizes the potential impoliteness of the phrasing and revises it to \"Hi Ashley, could you please send me the data at your earliest convenience?\". When writing code, a programmer may implement an initial \"quick and dirty\" implementation, and then, upon reflection, refactor their code to a solution that is more efficient and readable. In this paper, we demonstrate that LLMs can provide iterative self-refinement without additional training, leading to higher-quality outputs on a wide range of tasks.\n\nWe present SELF-REFINE: an iterative self-refinement algorithm that alternates between two generative steps-FEEDBACK and REFINE. These steps work in tandem to generate high-quality outputs. Given an initial output generated by a model M, we pass it back to the same model M to get feedback. Then, the feedback is passed back to the same model to refine the previously-generated draft. This process is repeated either for a specified number of iterations or until M determines that no further refinement is necessary. We use few-shot prompting (Brown et al., 2020) to guide M to both generate feedback and incorporate the feedback into an improved draft. Figure 1 illustrates the high-level idea, that SELF-REFINE uses the same underlying language model to generate feedback and refine its outputs.\n\nWe evaluate SELF-REFINE on 7 generation tasks that span diverse domains, including natural language and source-code generation. We show that SELF-REFINE outperforms direct generation from strong LLMs like GPT-3.5 (text-davinci-003 and gpt-3.5-turbo; OpenAI; Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) by 5-40% absolute improvement. In code-generation tasks, SELF-REFINE improves the initial generation by up to absolute 13% when applied to strong code models such as Codex (code-davinci-002; . We release all of our code, which is easily extensible to other LLMs. In essence, our results show that even when an LLM cannot generate an optimal output on its first try, the LLM can often provide useful feedback and improve its own output accordingly. In turn, SELF-REFINE provides an effective way to obtain better outputs from a single model without any additional training, via iterative (self-)feedback and refinement.\n\n\nIterative Refinement with SELF-REFINE\n\nGiven an input sequence, SELF-REFINE generates an initial output, provides feedback on the output, and refines the output according to the feedback. SELF-REFINE iterates between feedback and refinement until a desired condition is met. SELF-REFINE relies on a suitable language model and three prompts (for initial generation, feedback, and refinement), and does not require training. SELF-REFINE is shown in Figure 1 and Algorithm 1. Next, we describe SELF-REFINE in more detail.\n\nInitial generation Given an input x, prompt p gen , and model M, SELF-REFINE generates an initial output y 0 : y 0 = M (p gen \u2225x) .\n\n(1) This code is slow as it uses brute force. A better approach is to use the formula ... (n(n+1))/2.\n(f) REFINE yt+1\nCode (refined) def sum_faster(n): return (n*(n+1))//2 Figure 2: Examples of SELF-REFINE: an initial output generated by the base LLM and then passed back to the same LLM to receive feedback to the same LLM to refine the output . The top row illustrates this for dialog generation where an initial dialogue response can be transformed into a more engaging one that also understands the user by applying feedback. The bottom row illustrates this for code optimization where the code is made more efficient by applying feedback.\n\n\nAlgorithm 1 SELF-REFINE algorithm\n\nRequire: input x, model M, prompts {p gen , p fb , p refine }, stop condition stop(\u00b7) 1: y 0 = M(p gen \u2225x) \u25b7 Initial generation (Eqn. 1) 2: for iteration t \u2208 0, 1, . . . Here, p gen is a task-specific few-shot prompt (or instruction) for an initial generation, and \u2225 denotes concatenation. The few-shot prompt contains input-output pairs \u27e8x (k) , y (k) \u27e9 for the task. 2 FEEDBACK Next, SELF-REFINE uses the same model M to provide feedback f b t on its own output, given a task-specific prompt p fb for generating feedback:\nf b t = M (p fb \u2225x\u2225y t ) .\n(2)\n\nIntuitively, the feedback may address multiple aspects of the output. For example, in code optimization, the feedback might address the efficiency, readability, and overall quality of the code.\n\nHere, the prompt p fb provides examples of feedback in the form of input-output-feedback triples \u27e8x (k) , y (k) , f b (k) \u27e9. We prompt the model to write feedback that is actionable and specific via f b (k) . By 'actionable', we mean the feedback should contain a concrete action that would likely improve the output. By 'specific', we mean the feedback should identify concrete phrases in the output to change. For example, the feedback in Figure 2(e) is \"This code is slow as it uses a for loop which is brute force. A better approach is to use the formula ... (n(n+1))/2 \". This feedback is actionable, since it suggests the action 'use the formula...'. The feedback is specific since it mentions the 'for loop'. REFINE Next, SELF-REFINE uses M to refine its most recent output, given its own feedback:\ny t+1 = M (p refine \u2225x\u2225y t \u2225f b t ) .(3)\nFor example, in Figure 2(f), given the initial output and the generated feedback, the model generates a re-implementation that is shorter and runs much faster than the initial implementation. The prompt p refine provides examples of improving the output based on the feedback, in the form of input-output-feedback-refined quadruples \u27e8x (k) , y\n(k) t , f b (k) t , y (k)\nt+1 \u27e9. Iterating SELF-REFINE SELF-REFINE alternates between FEEDBACK and REFINE steps until a stopping condition is met. The stopping condition stop(f b t , t) either stops at a specified timestep t, or extracts a stopping indicator (e.g. a scalar stop score) from the feedback. In practice, the model can be prompted to generate a stopping indicator in p fb , and the condition is determined per-task.\n\nTo inform the model about the previous iterations, we retain the history of previous feedback and outputs by appending them to the prompt. Intuitively, this allows the model to learn from past mistakes and avoid repeating them. More precisely, Equation (3) is in fact instantiated as:\ny t+1 = M (p refine \u2225x\u2225y 0 \u2225f b 0 \u2225...\u2225y t \u2225f b t ) .(4)\nFinally, we use the last refinement y t as the output of SELF-REFINE.\n\nAlgorithm 1 summarizes SELF-REFINE, and Figure 2 shows an example of SELF-REFINE in the Dialogue Response Generation (Mehri and Eskenazi, 2020)  Examples for all tasks and dataset statistics are provided in Table 4 (Appendix A).\n\n\nInstantiating SELF-REFINE\n\nWe instantiate SELF-REFINE following the high-level description in Section 2. The FEEDBACK-REFINE iterations continue until the desired output quality or task-specific criterion is reached, up to a maximum of 4 iterations. To make our evaluation consistent across different models, we implemented both FEEDBACK and REFINE as few-shot prompts even with models that respond well to instructions, such as ChatGPT and GPT-4.\n\n\nBase LLMs\n\nOur main goal is to evaluate whether we can improve the performance of any strong base LLMs using SELF-REFINE. Therefore, we compare SELF-REFINE to the same base LLMs but without feedback-refine iterations. We used three main strong base LLM across all tasks: GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), and GPT-4 (OpenAI, 2023). For code-based tasks, we also experimented with CODEX (code-davinci-002). In all tasks, either GPT-3.5 or GPT-4 is the previous state-of-the-art. 3 We used the same prompts from previous work when   Table 1: SELF-REFINE results on various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM. SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2.\n\navailable (such as for Code Optimization and Math Reasoning); otherwise, we created prompts as detailed in Appendix S. We use greedy decoding with a temperature of 0.7 for all setups.\n\n\nMetrics\n\nWe report three types of metrics:\n\n\u2022 Task specific metric: When available, we use automated metrics from prior work (Math Reasoning: % solve rate; Code Optimization: % programs optimized; Constrained Gen: coverage %) \u2022 Human-pref: In Dialogue Response Generation, Code Readability Improvement, Sentiment Reversal, and Acronym Generation, since no automated metrics are available, we perform a blind human A/B evaluation on a subset of the outputs to select the preferred output. Additional details are provided in Appendix C. \u2022 GPT-4-pref: In addition to human-pref, we use GPT-4 as a proxy for human preference following prior work (Fu et al., 2023;Chiang et al., 2023;Geng et al., 2023;Sun et al., 2023), and found high correlation (82% for Sentiment Reversal, 68% for Acronym Generation, and 71% for Dialogue Response Generation) with human-pref. For Code Readability Improvement, we prompt GPT-4 to calculate fraction of the variables that are appropriately named given the context (e.g.,\nx = [] \u2192 input_buffer = [])\n. Additional details are provided in Appendix D. Table 1 shows our main results:\n\n\nResults\n\nSELF-REFINE consistently improves over base models across all model sizes, and additionally outperforms the previous state-of-the-art across all tasks. For example, GPT-4+SELF-REFINE improves over the base GPT-4 by 8.7% (absolute) in Code Optimization, increasing optimization percentage from 27.3% to 36.0%. Confidence intervals are provided in Appendix J. For code-based tasks, we found similar trends when using CODEX; those results are included in Appendix F.\n\nOne of the tasks in which we observe the highest gains compared to the base models is Constrained Generation, where the model is asked to generate a sentence containing up to 30 given concepts. We believe that this task benefits significantly from SELF-REFINE because there are more opportunities to miss some of the concepts on the first attempt, and thus SELF-REFINE allows the model to fix these mistakes subsequently. Further, this task has an extremely large number of reasonable outputs, and thus SELF-REFINE allows to better explore the space of possible outputs.\n\nIn preference-based tasks such as Dialogue Response Generation, Sentiment Reversal, and Acronym Generation, SELF-REFINE leads to especially high gains. For example in Dialogue Response Generation, GPT-4 preference score improve by 49.2% -from 25.4% to 74.6%. Similarly, we see remarkable improvements in the other preference-based tasks across all models.\n\nThe modest performance gains in Math Reasoning can be traced back to the inability to accurately identify whether there is any error. In math, errors can be nuanced and sometimes limited to a single line or incorrect operation. Besides, a consistent-looking reasoning chain can deceive LLMs to think that \"everything looks good\" (e.g., ChatGPT feedback for 94% instances is 'everything looks good'). In Appendix H.1, we show that the gains with SELF-REFINE on Math Reasoning are much bigger (5%+) if an external source can identify if the current math answer is incorrect.\n\nImprovement is consistent across base LLMs sizes Generally, GPT-4+SELF-REFINE performs better than GPT-3.5+SELF-REFINE and ChatGPT+SELF-REFINE across all tasks, even in tasks where the initial base results of GPT-4 were lower than GPT-3.5 or ChatGPT. We thus believe that SELF-REFINE allows stronger models (such as GPT-4) to unlock their full potential, even in cases where this potential is not expressed in the standard, single-pass, output generation. Comparison to additional strong baselines is provided in Appendix F.\n\n\nAnalysis\n\nThe three main steps of SELF-REFINE are FEEDBACK, REFINE, and repeating them iteratively. In this section, we perform additional experiments to analyze the importance of each of these steps. These experiments were performed with ChatGPT (Code Optimization and Sentiment Reversal) and GPT-3.5 (Acronym Generation), and metrics used are defined in Section 3.2.\n\nThe impact of the feedback quality Feedback quality plays a crucial role in SELF-REFINE. To quantify its impact, we compare SELF-REFINE, which utilizes specific, actionable feedback, with two ablations: one using generic feedback and another without feedback (the model may still iteratively refine its generations, but is not explicitly provided feedback to do so). For example, in the Code Optimization task: actionable feedback, such as Avoid repeated calculations in the for loop, pinpoints an issue and suggests a clear improvement. Generic feedback, like Improve the efficiency of the code, lacks this precision and direction. Table 2 shows feedback's clear influence.\n\nIn Code Optimization, performance slightly dips from 27.5 (SELF-REFINE feedback) to 26.0 (generic feedback), and further to 24.8 (no feedback). This suggests that while generic feedback offers some guidance -specific, actionable feedback yields superior results.\n\nThis effect is more pronounced in tasks like Sentiment Transfer, where changing from our feedback to generic feedback leads to a significant performance drop (43.2 to 31.2), and the task fails without feedback. Similarly, in Acronym Generation, without actionable feedback, performance drops from 56.4 to 48.0, even with iterative refinements. These results highlight the importance of specific, actionable feedback in our approach. Even generic feedback provides some benefit, but the best results are achieved with targeted, constructive feedback.\n\nHow important are the multiple iterations of FEEDBACK-REFINE? Figure 4 demonstrates that on average, the quality of the output improves as the number of iterations increases. For instance, in the Code Optimization task, the initial output (y 0 ) has a score of 22.0, which improves to 28.8 after three iterations (y 3 ). Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations. This trend of improvement is also evident in Constrained Generation, where the score increases from 29.0 to 49.7 after three iterations. Figure 4 highlights the diminishing returns in the improvement as the number of iterations increases. Overall, having multiple FEEDBACK-REFINE iterations significantly enhances the quality of the output, although the marginal improvement naturally decreases with more iterations.\n\nThe performance may not always monotonically increase with iterations: in multi-aspect feedback tasks like Acronym Generation, where the output quality can vary during iteration with improvement in one aspect but decline in another aspect. To counter this, SELF-REFINE generates numerical scores for different quality aspects, leading to a balanced evaluation and appropriate output selection.    The initial code by the baseline, which is nearly identical to the slower input program, fails to improve the efficiency and merely alters the logic for reading input. SELF-REFINE first generates feedback that diagnoses that This code is slow because it is using six nested loops to iterate through all possible combinations of coins to pay the amount, and suggests that a more efficient approach would be .... SELF-REFINE then uses this feedback to generate the revised code (right), reducing the time complexity to O(amount * coins). The full example is provided in Appendix H\n\nCan we just generate multiple outputs instead of refining? Does SELF-REFINE improve because of the iterative refinement, or just because it generates more outputs? We compare SELF-REFINE with ChatGPT, when ChatGPT generates k = 4 samples (but without feedback and refinement). Then, we compare the performance of SELF-REFINE against these k initial outputs in a 1 vs. k evaluation.\n\nIn other words, we assess whether SELF-REFINE can outperform all k initial outputs. The results of this experiment are illustrated in Figure 6 (Appendix H). Despite the increased difficulty of the 1 vs. k setting, the outputs of SELF-REFINE are still preferred by humans over all k initial outputs. This shows the importance of refinement according to feedback over the alternative of just generating multiple initial outputs.\n\nDoes SELF-REFINE work with weaker models? The experiments in Section 3.3 were performed with some of the strongest available models; does SELF-REFINE work with smaller or weaker models as well? To investigate this, we instantiated SELF-REFINE with Vicuna-13B (Chiang et al., 2023), a less powerful base model. While Vicuna-13B is capable of generating initial outputs, it struggles significantly with the refinement process. Specifically, Vicuna-13B was not able to consistently generate the feedback in the required format. Furthermore, even when provided with Oracle or hard-coded feedback, it often failed to adhere to the prompts for refinement. Instead of refining its output, Vicuna-13B either repeated the same output or generated a hallucinated conversation, rendering the outputs less effective. We thus hypothesize that since Vicuna-13B was trained on conversations, it does not generalize as well as instruction-based models to test-time few-shot tasks. Example output and analysis is provided in Appendix G.\n\nQualitative Analysis We conduct a qualitative analysis of the feedback generated by SELF-REFINE and its subsequent refinements. We manually analyze 70 samples in total (35 success cases and 35 failure cases) for Code Optimization (Madaan et al., 2023) and Math Reasoning (Cobbe et al., 2021). For both Math Reasoning and Code Optimization, we found that the feedback was predominantly actionable, with the majority identifying problematic aspects of the original generation and suggesting ways to rectify them.\n\nWhen SELF-REFINE failed to improve the original generation, the majority of issues were due to erroneous feedback rather than faulty refinements. Specifically, 33% of unsuccessful cases were due to feedback inaccurately pinpointing the error's location, while 61% were a result of feedback suggesting an inappropriate fix. Only 6% of failures were due to the refiner incorrectly implementing good feedback. These observations highlight the vital role of accurate feedback plays in SELF-REFINE.\n\nIn successful cases, the refiner was guided by accurate and useful feedback to make precise fixes to the original generation in 61% of the cases. Interestingly, the refiner was capable of rectifying issues even when the feedback was partially incorrect, which was the situation in 33% of successful cases. This suggests resilience to sub-optimal feedback. Future research could focus on examining the refiner's robustness to various types of feedback errors and exploring ways to enhance this resilience. In Figure  5, we illustrate how SELF-REFINE significantly improves program efficiency by transforming a brute force approach into a dynamic programming solution, as a result of insightful feedback. Additional analysis on other datasets such as Dialogue Response Generation is provided in Appendix H.\n\nGoing Beyond Benchmarks While our evaluation focuses on benchmark tasks, SELF-REFINE is designed with broader applicability in mind. We explore this in a real-world use case of website generation, where the user provides a high-level goal and SELF-REFINE assists in iteratively developing the website. Starting from a rudimentary initial design, SELF-REFINE refines HTML, CSS, and JS to evolve the website in terms of both usability and aesthetics. This demonstrates the potential of SELF-REFINE in real-world, complex, and creative tasks. See Appendix I for examples and further discussion, including broader, societal impact of our work.\n\n\nRelated work\n\nLeveraging human-and machine-generated natural language (NL) feedback for refining outputs has been effective for a variety of tasks, including summarization (Scheurer et al., 2022), script generation (Tandon et al., 2021), program synthesis (Le et al., 2022a;Yasunaga and Liang, 2020), and other tasks (Bai et al., 2022a;Schick et al., 2022b;Saunders et al., 2022a;Bai et al., 2022b;Welleck et al., 2022). Refinement methods differ in the source and format of feedback, and the way that a refiner is obtained. Table 3 summarizes some related approaches; see Appendix B for an additional discussion.\n\nSource of feedback. Humans have been an effective source of feedback (Tandon et al., 2021;Elgohary et al., 2021;Tandon et al., 2022;Bai et al., 2022a). Since human feedback is costly, several approaches use a scalar reward function as a surrogate of (or alternative to) human feedback (e.g., (Bai et al., 2022a;Liu et al., 2022;Lu et al., 2022;Le et al., 2022a;Welleck et al., 2022)). Alternative sources such as compilers (Yasunaga and Liang, 2020) or Wikipedia edits (Schick et al., 2022b) can provide domain-specific feedback. Recently, LLMs have been used to generate feedback for general domains (Fu et al., 2023;Peng et al., 2023;Yang et al., 2022), However, ours is the only method that generates feedback using an LLM on its own output, for the purpose of refining with the same LLM.\n\nRepresentation of feedback. The form of feedback can be generally divided into natural language (NL) and non-NL feedback. Non-NL feedback can come in human-provided example pairs (Dasgupta   (Liu et al., 2022;Le et al., 2022b). In this work, we use NL feedback, since this allows the model to easily provide self -feedback using the same LM that generated the output, while leveraging existing pretrained LLMs such as GPT-4.\n\nTypes of refiners. Pairs of feedback and refinement have been used to learn supervised refiners (Schick et al., 2022b;Du et al., 2022;Yasunaga and Liang, 2020;Madaan et al., 2021). Since gathering supervised data is costly, some methods learn refiners using model generations (Welleck et al., 2022;Peng et al., 2023). However, the refiners are trained for each new domain. Finally, (Yang et al., 2022) use prompted feedback and refinement specifically tailored for story generation. In this work, we avoid training a separate refiner, and show that the same model can be used as both the refiner and the source of feedback across multiple domains.\n\nNon-refinement reinforcement learning (RL) approaches. Rather than having explicit refinement, an alternative way to incorporate feedback is by optimizing a scalar reward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le et al. (2022a)). These methods differ from SELF-REFINE in that the model does not access feedback on an intermediate generation. Second, these RL methods require updating the model's parameters, unlike SELF-REFINE.\n\n\nLimitations and Discussion\n\nThe main limitation of our approach is that the base models need to have sufficient few-shot modeling or instruction-following abilities, in order to learn to provide feedback and to refine in an in-context fashion, without having to train supervised models and rely on supervised data.\n\nFurther, the experiments in this work were performed with language models that are not open-sourced, namely GPT-3.5, ChatGPT, GPT-4, and CODEX. Existing literature (Ouyang et al., 2022) does not fully describe the details of these models, such as the pretraining corpus, model sizes, and model biases. Further, these models are not free to use, and using them for research requires some funding. Nonetheless, we release our code and model outputs to ensure the reproducibility of our work.\n\nAnother limitation of our work is that we exclusively experiment with datasets in English. In other languages, the current models may not provide the same benefits.\n\nFinally, there is a possibility for bad actors to use prompting techniques to steer a model to generate more toxic or harmful text. Our approach does not explicitly guard against this.\n\n\nConclusion\n\nWe present SELF-REFINE: a novel approach that allows large language models to iteratively provide self-feedback and refine their own outputs. SELF-REFINE operates within a single LLM, requiring neither additional training data nor reinforcement learning. We demonstrate the simplicity and ease of use of SELF-REFINE across a wide variety of tasks. By showcasing the potential of SELF-REFINE in diverse tasks, our research contributes to the ongoing exploration and development of large language models, with the aim of reducing the cost of human creative processes in real-world settings. We hope that our iterative approach will help drive further research in this area. To this end, we make all our code, data and prompts anonymously available at https://selfrefine.info/. A Evaluation Tasks   Table 4 lists the tasks in our evaluation, and examples from each task.\n\n\nTask and Description Sample one iteration of FEEDBACK-REFINE\n\n\nSentiment Reversal\n\nRewrite reviews to reverse sentiment. Dataset: (Zhang et al., 2015) 1000 review passages\n\nx: The food was fantastic...\" y t : The food was disappointing...\" f b: Increase negative sentiment y t+1 : The food was utterly terrible...\"\n\n\nDialogue Response Generation\n\nProduce rich conversational responses. Dataset: (Mehri and Eskenazi, 2020) 372 conv.\n\nx: What's the best way to cook pasta?\" y t : The best way to cook pasta is to...\" f b: Make response relevant, engaging, safe y t+1 : Boil water, add salt, and cook pasta...\" x: Olivia has $23, buys 5 bagels at $3 each\" y t : Solution in Python f b: Show step-by-step solution y t+1 : Solution with detailed explanation\n\n\nCode Optimization\n\n\nAcronym Generation\n\nGenerate acronyms for a given title Dataset: (Appendix Q) 250 acronyms\n\nx : Radio Detecting and Ranging\" y t : RDR f b : be context relevant; easy pronunciation y t+1 : RADAR\"\n\n\nConstrained Generation\n\nGenerate sentences with given keywords. Dataset: (Lin et al., 2020) 200 samples\n\nx: beach, vacation, relaxation y t : During our beach vacation... f b: Include keywords; maintain coherence y t+1 : .. beach vacation was filled with relaxation Table 4: An overview of the tasks which we evaluate SELF-REFINE on, along with their associated datasets and sizes. For every task, we demonstrate a single iteration of refinement of input x, the previously generated output y t , the feedback generated f b t , and the refinement y t+1 . Few-shot prompts used for FEEDBACK and REFINE are provided in Appendix S.\n\n\nB Broader Related Work\n\nCompared to a concurrent work, Reflexion (Shinn et al., 2023), our approach involves correction using feedback, whereas their setup involves finding the next best solution in planning using ReAct. While ReAct and Reflexion provide a free-form reflection on whether a step was executed correctly and potential improvements, our approach is more granular and structured, with multi-dimensional feedback and scores. This distinction allows our method to offer more precise and actionable feedback, making it suitable for a wider range of natural language generation tasks, including those that may not necessarily involve step-by-step planning such as open-ended dialogue generation.\n\nComparison with Welleck et al. (2022) The closest work to ours may be Self-Correction (Welleck et al., 2022); however, Self-Correction has several disadvantages compared to SELF-REFINE: 1. Self-Correction does not train their model to generate explicit feedback; instead, Welleck et al. (2022) trained their models to refine only. As we show in Section 4 and Table 2, having the model generate explicit feedback results in significantly better refined outputs. 2. Self-Correction trains a separate refiner (or \"corrector\") for each task. In contrast, SELF-REFINE uses instructions and few-shot prompting, and thus does not require training a separate refiner for each task. 3. Empirically, we evaluated SELF-REFINE using the same base model of GPT-3 as Self-Correction, and with the same settings on the GSM8K benchmark. Self-Correction achieved 45.9% accuracy while SELF-REFINE (this work) achieved 55.7% (\u21919.8).\n\nComparison with non-refinement reinforcement learning (RL) approaches. Rather than having an explicit refinement module, an alternative way to incorporate feedback is by optimizing a scalar reward function, e.g. with reinforcement learning (e.g., Stiennon et al. (2020); Lu et al. (2022); Le et al. (2022a)). These methods differ from SELF-REFINE (and more generally, refinement-based approaches) in that the model cannot access feedback on an intermediate generation. Second, these reinforcement learning methods require updating the model's parameters, unlike SELF-REFINE.\n\nSee Table 5 for an additional detailed comparison of related work. PEER (Schick et al., 2022b) doc. edit trained on wiki edits trained on edits single(accuracy) (not self gen.) Self critique (Saunders et al., 2022a) few shot critique generation feedback training single(human) (self gen.) Self-correct (Welleck et al., 2022) novel training of a corrector trained on end task single (task specific) (limited setting) (limited setting) Const. AI (Bai et al., 2022b) train RL4F on automat (critique, revision) pair critique training (fixed set)  \n\n\nC Human Evaluation\n\nThe A/B evaluation in our study was conducted by the authors, where a human judge was presented with an input, task instruction, and two candidate outputs generated by the baseline method and SELF-REFINE. The setup was blind, i.e., the judges did not know which outputs were generated by which method. The judge was then asked to select the output that is better aligned with the task instruction. For tasks that involve A/B evaluation, we calculate the relative improvement as the percentage increase in preference rate. The preference rate represents the proportion of times annotators selected the output produced by SELF-REFINE over the output from the baseline method.  Table 6: Relative improvement of SELF-REFINE in A/B evaluations across different tasks. The values represent normalized preferences, which correspond to the proportion of times the output generated by SELF-REFINE was selected as better aligned with the task instruction over the baseline method. The evaluation was conducted for 150 examples for each dataset. The judges were not aware of the method that generated each sample.\n\n\nD GPT-4 Evaluation\n\nIn light of the impressive achievements of GPT-4 in assessing and providing reasoning for complex tasks, we leverage its abilities for evaluation in SELF-REFINE. The approach involves presenting tasks to GPT-4 in a structured manner, promoting the model's deliberation on the task and generating a rationale for its decision. This methodology is demonstrated in Listings 1 to 3:\n\nListing 1 Prompt for GPT-4 evaluation of Sentiment Reversal. This work SELF-REFINE w/ GPT-3.5 23.0 SELF-REFINE w/ ChatGPT 26.7 SELF-REFINE w/ GPT-4 36.0 Table 8: Performance comparison of various models on the PIE dataset in terms of the percentage of programs optimized (%OPT). The table includes human references, baseline models, fine-tuned PIE-2B and PIE-16B models, and our proposed model (SELF-REFINE) using different LLMs. Notably, SELF-REFINE achieves superior performance while using only 4 samples at most, significantly fewer than the 16 and 32 samples employed by other models. Scalene, an off-the-shelf optimizer, uses instruction tuning with Codex and serves as a comparison point.\n\n\nG Evaluation of Vicuna-13b\n\nWe also experiment with Vicuna-13b (Chiang et al., 2023), a version of LLaMA-13b (Touvron et al., 2023) fine-tuned on conversations sourced from the web. Vicuna-13b was able to consistently follow the task initialization prompt. However, it struggled to follow the prompts intended for feedback and refinement. This often led to outputs that resembled assistant-like responses, a representative example of which can be found in Appendix G.\n\nIt's important to note that we used the same prompts for Vicuna-13b as those used with other models in our study. However, the limited performance of Vicuna-13b suggests that this model may require more extensive prompt-engineering for optimal performance.\n\nMixed-refine: Improving Vicuna-13b with ChatGPT While the focus of SELF-REFINE is improvement of the model without any external help, it may be possible to use a smaller model for the initialization, and then involving a bigger model for refinement. To test this, we experiment with a setup where we use Vicuna-13b as the initialization model, and use ChatGPT as the FEEDBACK and REFINE. The results on Math Reasoning show the promise of this approach: while Vicuna-13b was able to get only 24.18% on Math Reasoning, it was able to improve to 40.5% in this mixed-refinement setting.\n\nStep 0 INIT> The food was amazing, I loved it!!.\n\nStep 0 TRANSFER> The food was terrible, I hated it!!.\n\nlist index out of range An error occurred: list index out of range. Traceback (most recent call last): IndexError: list index out of range . Left retries: 2.\n\nStep 1 TRANSFER> The food at the Trop was awful. I wish I could've gotten my money back for the terrible meals.\n\n\n\u2192\n\nStep 1 FB> The rewritten review does not become more \"Very negative\", because it only contains a single instance of a positive sentiment about the food. Try again to make it Very negative! \u2192 \u2192\n\nStep 2 TRANSFER> ### Assistant: The Trop is a great choice for anyone looking for a budget-friendly option in Vegas. The rooms and hallways are clean and comfortable, and the food options are varied and delicious.\n\n\n\u2192 \u2192\n\nStep 2 FB> The rewritten review does not change from \"Positive\", because it only has positive language. The output is already Very positive. No further changes are needed.\n\n\n\u2192 \u2192\n\nStep 0 INIT> The food was amazing, I loved it!!.\n\n[Positive] -> [Very negative]\n\nStep 0 TRANSFER> The food was atrocious, I absolutely hated it! [('The review sounds very negative. The sentiment is Very negative', 1.0)]\n\n\n\u2192\n\nStep 0 FB> The original review is already \"Very negative\" as it uses strongly negative words like \"atrocious\" and \"absolutely hated\". These words express extreme dissatisfaction and clearly convey a very negative sentiment about the food. But try again to make it even more negative! \u2192 \u2192 \u2192\n\nStep 1 TRANSFER> The food was so abysmal, it was a nightmare to even taste it! I couldn't stand it and regretted every single bite. [('The review sounds highly negative. The sentiment is Very negative', 1.0)] \u2192 \u2192\n\nStep 1 FB> The review provided is indeed very negative. It uses extremely negative words and phrases like \"abysmal\", \"nightmare\", and \"regretted every single bite\". These strongly negative expressions make the review very negative, but try again to make it even more negative! \u2192 \u2192 \u2192\n\nStep 2 TRANSFER> The food was absolutely revolting, I couldn't even stomach a single bite!\n\n\n\u2192\n\nStep 2 FB> This review is not \"Very negative\" because it contains positive words and phrases like \"amazing\" and \"loved it.\" To make it \"Very negative\", we need to use extremely negative words, like \"revolting\", \"couldn't stomach\", or \"horrible\". The provided \"Very negative\" sentiment example is a good illustration of a very negative review, as it uses strong negative language to describe the food experience. Vicuna-13b outputs (top) for sentiment transfer, where the goal is to make 'The food was amazing, I loved it!!.' negative. Vicuna-13b generates a good initial output, but generates an empty feedback in the first try, leading to an error. In the next try, it generates an unhelpful feedback. The transferred review copies from the prompt. Compare with the outputs from GPT-4 (bottom).  (2022). This method uses correctness information to guide model refinement, only progressing to REFINE stage if the current answer is incorrect. This adjustment notably enhanced performance in the Math Reasoning task, with GPT-3 improving by 4.8% and GPT-4 by 0.7% Table 9. This indicates the potential of external signals to optimize model performance in particular tasks.  Table 10: Acronym generation results across iterations, showcasing how improvements in certain aspects (e.g., pronunciation and spelling) can be accompanied by losses in others, leading to fluctuating overall performance in multi-aspect feedback tasks like Acronym Generation.\n\nNon-monotonic increase in output quality for acronym generation For tasks with multi-aspect feedback like Acronym Generation, the output quality can fluctuate during the iterative process, improving on one aspect while losing out on another (Table 10). To address this, SELF-REFINE's feedback generates explicit numerical scores to capture the different aspects of output quality. This allows for a more balanced evaluation of outputs and the selection of the most appropriate one. The algorithm selects the best output based on the maximum score across all iterations, as described in Algorithm 1 (line 8). A similar selection is possible for other tasks like Math Reasoning and Sentiment Reversal, while we observe that output quality increases monotonically with iterations. Robust to bad feedback 60% Informative:\n\nThe response does not provide any information. 1/3\n\nResponse: Wow, 25 people! That must have been an amazing experience. Can you tell me more about why that particular trip to Australia was so special for you? Table 12: On the Dialogue Response Generation task, SELF-REFINE can ignore good feedback but in a majority of cases, it is robust to bad feedback and ignores bad feedback.\n\n\nFeedback and Refinement Error Analysis for Response Generation\n\nWe perform a detailed error analysis of SELF-REFINE feedback and refinement process for Dialogue Response Generation, which we summarize in Tables Table 11 and Table 12. Table 11 reports the occurrence of different types of errors in our sample, which includes Incorrect Feedback (25%), Generic Feedback (30%), and Incorrect Scoring (10%). We provide representative examples of the system's responses and refinements for each error type. These errors highlight potential areas for improving our feedback handling mechanism, particularly in the interpretation and understanding of user inputs. Table 12 breaks down errors found in the refinement stage of SELF-REFINE. Errors include: not being robust (10%), ignoring feedback (25%), and introducing a new problem (20%). We demonstrate how the model handles a variety of feedback types, how robust it is under different circumstances, and how often it inadvertently introduces new issues. 60% of the times, the model is robust to incorrect or generic feedback. These insights can guide us in enhancing the model's refinement capabilities, especially in providing accurate and specific responses.\n\n\nI Beyond Benchmarks\n\nSELF-REFINE demonstrates its iterative feedback and refinement capabilities in the context of website layout generation. ChatGPT initially produces a rudimentary layout for a given topic, and then uses the FEEDBACK to suggest specific, actionable improvements, as demonstrated in Figures 7  and 9. These suggestions range from design changes such as color and font adjustments, to content enhancements and layout modifications. Figures 8 and 10 showcase the final layouts, post-feedback implementation, highlighting the potential and versatility of SELF-REFINE across different scenarios. Figure 7: Initial web layout generated by our model for a fictional ice cream parlor.\n\n\nIce Cream Generation The feedback generated by FEEDBACK for ice cream generation:\n\nPhotosynthesis The feedback generated by FEEDBACK for photosynthesis:\n\n\u2022 Increase the font size of the text to 18px for better readability.\n\n\u2022 Add more information about the benefits of photosynthesis.\n\n\u2022 Remove the unnecessary margin-top from the header. \u2022 Add a ruler or divider below the header to separate it from the image. Figure 8: Refined web layout after applying model feedback. The feedback included changing the background color to light blue (#6f2ff), increasing the heading font size to 48px, adding an icon before the welcome text, enhancing the content with an additional paragraph, increasing the button text size to 24px, and updating the button color to #9933. Figure 9: Initial web layout generated by our model for a page on photosynthesis. Figure 10: Refined web layout after applying model feedback. The feedback included increasing the text font size to 18px for better readability, adding more information about the benefits of photosynthesis, removing the unnecessary margin-top from the header, and adding a ruler or divider below the header to separate it from the image.  ChatGPT GPT-4 28.0 \u00b1 7.38 37.0 \u00b1 8.26 44.0 \u00b1 8.72 67.0 \u00b1 9.00 * 15.0 \u00b1 5.38 45.0 \u00b1 8.77 * Table 13: SELF-REFINE results from table 1 with Wilson confidence interval (at 95% confidence interval) and statistical significance. On various tasks using GPT-3.5, ChatGPT, and GPT-4 as base LLM, SELF-REFINE consistently improves LLM. Metrics used for these tasks are defined in Section 3.2 as follows: Math Reasoning uses the solve rate; Code Optimization uses the percentage of programs optimized; and Sentiment Reversal, Dialogue Response and Acronym Gen use a GPT-4-based preference evaluation, which measures the percentage of times outputs from the base or enhanced models were selected, with the rest categorized as a tie. Constrained Gen uses the coverage percentage. Gains over Base, that are statistically significant based on these confidence intervals are marked * Table 13 shows results from Table 1 with Wilson confidence interval (Brown et al., 2001) (at \u03b1= 99% confidence interval) and statistical significance. Gains that are statistical significance based on these confidence intervals are marked with an asterisk. We find that nearly all of GPT-4 gains are statistically significant, ChatGPT gains are significant for 4 out of 7 datasets, and GPT-3.5 gains are significant for 3 out of 7 datasets.\n\n\nJ Statistical Confidence Intervals\n\n\nK New Tasks\n\nConstrained Generation We introduce \"CommonGen-Hard,\" a more challenging extension of the CommonGen dataset (Lin et al., 2020), designed to test state-of-the-art language models' advanced commonsense reasoning, contextual understanding, and creative problem-solving. CommonGen-Hard requires models to generate coherent sentences incorporating 20-30 concepts, rather than only the 3-5 related concepts given in CommonGen. SELF-REFINE focuses on iterative creation with introspective feedback, making it suitable for evaluating the effectiveness of language models on the CommonGen-Hard task.\n\nAcronym Generation Acronym generation requires an iterative refinement process to create concise and memorable representations of complex terms or phrases, involving tradeoffs between length, ease of pronunciation, and relevance, and thus serves as a natural testbed for our approach.\n\nWe source a dataset of 250 acronyms 4 and manually prune it to remove offensive or uninformative acronyms.\n\n\nL Code Readability\n\nOrthogonal to the correctness, readability is another important quality of a piece of code: though not related to the execution results of the code, code readability may significantly affect the usability, upgradability, and ease of maintenance of an entire codebase. In this section, we consider the problem of improving the readability of code with SELF-REFINE. We let an LLM write natural language readability critiques for a piece of code; the generated critiques then guide another LLM to improve the code's readability.\n\n\nL.1 Method\n\nFollowing the SELF-REFINE setup, we instantiate INIT, FEEDBACK, and REFINE. The INIT is a no-op -we directly start by critiquing the code with FEEDBACK and applying the changes with REFINE.\n\n\u2022 FEEDBACK We prompt an LLM with the given code and an instruction to provide feedback on readability. We give the LLM the freedom to freely choose the type of enhancements and express them in the form of free text. \u2022 REFINE The code generator LLM is prompted with the piece of code and the readability improvement feedback provided by FEEDBACK. In addition, we also supply an instruction to fix the code using the feedback. We take the generation from the code generator as the product of one iteration in the feedback loop.\n\nStarting from an initial piece of code y 0 , we first critique, c 1 = critique(y 0 ), and then edit the code, y 1 = editor(y 0 , c 1 ). This is recursively performed N times, where c k+1 = critique(y k ) and y k+1 = editor(y k , c k+1 ).\n\n\nL.2 Experiments\n\nDataset We use the CodeNet (Puri et al., 2021) dataset of competitive programming. 5 For our purpose, these are hard-to-read multi-line code snippets. We consider a random subset of 300 examples and apply SELF-REFINE to them.\n\nWe also ask human annotators to edit a 60-example subset to assess human performance on this task. The human annotators are asked to read the code piece and improve its readability.\n\nImplementation Both the critique and the editor models are based on the InstructGPT model (textdavinci-003). We consider the temperature of both T = 0.0 (greedy) and T = 0.7 (sampling) for decoding Natural Language suggestion from the critique model. We always use a temperature T = 0.0 (greedy) when decoding Programming Language from the code editor. Due to budget constraints, we run SELF-REFINE for N = 5 iterations. The exact prompts we use can be found in 0.700 0.25 1.33 Table 14: Human v.s. SELF-REFINE performance on 60-example subset. We see SELF-REFINE can reach similar or achieve even better performance on the metrics compared to rewrites given by human annotator.\n\n\nEvaluation Methods\n\nWe consider a few automatic heuristic-based evaluation metrics,\n\n\u2022 Meaningful Variable Names: In order to understand the flow of a program, having semantically meaningful variable names can offer much useful information. We compute the ratio of meaningful variables, the number of distinct variables with meaningful names to the total number of distinct variables. We automate the process of extracting distinct variables and the meaningful subset of variables using a few-shot prompted language model. \u2022 Comments: Natural language comments give explicit hints on the intent of the code. We compute the average number of comment pieces per code line. \u2022 Function Units: Long functions are hard to parse. Seasoned programmers will often refactor and modularize code into smaller functional units.\n\nResult For each automatic evaluation metric, the ratio of meaningful variable, of comment, and the number of function units, we compute for each iteration averaged across all test examples and plot for each SELF-REFINE iteration in Figure 11(a), Figure 11(b) and Figure 11(c) respectively. The two curves each correspond to critique with temperature T = 0.0 and T = 0.7. The iteration 0 number is measured from the original input code piece from CodeNet. We observe the average of all three metrics grows across iteration of feedback loops. A diverse generation of a higher temperature in the critique leads to more edits to improve the meaningfulness of variable names and to add comments. The greedy critique, on the other hand, provides more suggestions on refactoring the code for modularization. Figure 12 provides an example of code-readability improving over iterations.\n\nIn Table 14, we measure human performance on all three metrics and compare with SELF-REFINE last iteration output. At T = 0.7, SELF-REFINE produces more meaning variables, more function units and slightly more comments compared to the human annotators on average. At T = 0.0, SELF-REFINE produces less meaningful variables, less comments per line but even more function units.  \n\n\nM Dialogue Response Generation\n\nOpen-domain dialogue response generation is a complex task that requires a system to generate human-like responses to a wide range of topics. Due to the open-ended nature of the task, it is Starting Code: v 0 print((int((int(eval(input()))+1)/2)))\n\nCode v 1 print( (int( (int(eval(input())) + 1) / 2 ) )\n\nCode v 2 num_input = eval(input()) num_input = int(num_input) num_input += 1 num_result = int(num_input / 2) print(num_result) Figure 12: SELF-REFINE iterations over a piece of densely composed code. Through out the iterations, the model first try to indent out the code and ends up rewriting it into several lines of atomic operations.\n\nchallenging to develop a system that can consistently generate coherent and engaging responses. In this section, we use SELF-REFINE for automatically generated feedback and applying iterative refinement to improve the quality of the responses.\n\n\nM.1 Modules\n\nWe follow the high-level description of the framework from Section 2, and instantiate our framework as follows.\n\nInit This is the first step in performing the task. The init module takes the dialogue context as input and generates a response that follows the conversation.\n\nFeedback We design an FEEDBACK that can provide multifaceted feedback for the quality of the response generated. Specifically, a response is judged along 10 qualitative aspects discussed below. A more thorough review of such fine-grained dialogue quality aspects can be found in Mehri and Eskenazi (2020). We use 6 in-context examples for feedback generation. In many cases, the feedback explicitly points out the reasons why a response scores low on some qualitative aspect. We show an example in Figure 13.\n\n\u2022 Relevant Does the response addresses all important aspects of the context? \u2022 Informative -Does the response provide some information relevant to the context? \u2022 Interesting -Doe the response beyond providing a simple and predictable answer to a question or statement? \u2022 Consistent -Is the response consistent with the rest of the conversation in terms of tone and topic? \u2022 Helpful -Is the response helpful in providing any information or suggesting any actions? \u2022 Engaging -Is the response engaging and encourage further conversation? \u2022 Specific -The response contains specific content related to a topic or question, \u2022 Safe -Is the response safe and does not contain any offensive, toxic or harmful content and does not touch on any sensitive topics or share any personal information? \u2022 User understanding -Does the response demonstrate an understanding of the user's input and state of mind? \u2022 Fluent Is the response fluent and easy to understand?  Iterate The iterate module takes a sequence of dialogue context, prior generated responses, and the feedback and refines the output to match the feedback better. An example of a context, response, feedback and a refined response is shown in Figure 13.\n\n\nM.2 Setup and Experiments\n\nModel and Baseline We establish a natural baseline for our approach by using the model directly, without any feedback, which we refer to as INIT. Our implementation of SELF-REFINE employs a few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts, and we execute the self-improvement loop for a maximum k = 3 iterations. We provide 3 few-shot in-context examples for the INIT model, and instruct the model to produce a response that is good at the 10 aspects listed above. Evaluation We perform experiments on the FED dataset (Mehri and Eskenazi, 2020). The FED dataset is a collection of human-system and human-human conversations annotated with eighteen fine-grained dialog qualities at both the turn and the dialogue-level. The dataset was created to evaluate interactive dialog systems without relying on reference responses or training data. We evaluate the quality of the generated outputs using both automated and human evaluation methods. For automatic evaluation in Table1, we used zero-shot prompting with text-davinci-003 and evaluate on a test set of 342 instances. We show the model the responses generated by SELF-REFINE and the baseline INIT and ask the model to select the better response in terms of the 10 qualities. We report the win rate. However, we acknowledge that automated metrics may not provide an accurate assessment of text generation tasks and rely on human evaluation instead.\n\nGiven a dialogue context with a varying number of turns, we generate outputs from the above mentioned methods. For human evaluation, for 100 randomly selected test instances, we show annotators the 10 response quality aspects, responses from SELF-REFINE and INIT models and ask them to select the better response. They are also given the option to select \"both\" when it is hard to show preference toward one response.\n\n\nResults\n\nAutomatic evaluation results are shown in Table1 and human evaluation results are are shown in Table 15. We experiment on 3 latest versions of GPT models. text-davinci-003 is capable of generating human-like responses of great quality for a wide range of dialogue contexts and hence GPT-3.5 is a strong baseline. Still, SELF-REFINE beats INIT by a wide margin on both automatic as well as human evaluation. Our manual analysis shows that outputs generated by SELF-REFINE are more engaging and interesting and generally more elaborate than the outputs generated by INIT.\n\n\nN Code Optimization\n\nPerformance-Improving Code Edits or PIE (Madaan et al., 2023) focuses on enhancing the efficiency of functionally correct programs. The primary objective of PIE is to optimize a given program by implementing algorithmic modifications that lead to improved runtime performance.\n\nGiven an optimization generated by PIE, SELF-REFINE first generates a natural language feedback on possible improvements Figure 20. Then, the feedback is fed to REFINE Figure 21 for refinement.  Table 17: Performance comparison of SELF-REFINE and ablated variants for code optimization. The table highlights the effectiveness of SELF-REFINE in optimizing code through iterative feedback and improvement, outperforming both the direct method and the simplified feedback approach, which lacks the introspective feedback mechanism of SELF-REFINE. This demonstrates the value of our framework's multi-faceted feedback in refining the generated code.\n\n\nO Math Reasoning\n\nWe use the Grade School Math 8k (GSM-8k) dataset (Cobbe et al., 2021) for evaluating SELF-REFINE on math reasoning. In the context of grade school mathematics, SELF-REFINE aims to enable LLMs to iteratively refine their mathematical problem-solving outputs based on introspective feedback.\n\nFollowing Gao et al. (2022), we write solutions to the reasoning problems in Python. Consider the following example from the paper, where an error in the code demonstrates a lack of understanding of the problem: def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result By using SELF-REFINE, we can identify the error in the code and refine the solution through an iterative process of introspection and feedback:\n\n# Let's go through the error and check step-by-step plates = 6 plate_cost = 6000 # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost # wrong! The cost of a cup is not the same as the cost of a plate. \u2192 # The cost of a cup is $1200 less than the total cost of half a dozen plates sold at $6000 each. \u2192 half_dozen_plate_cost = 6 * plate_cost cup_cost = half_dozen_plate_cost -1200 SELF-REFINE is thus instantiated naturally: the generator generates an initial solution, and FEEDBACK scans the solution to spot errors on which to provide feedback. The feedback is supplied to REFINE to create a new solution. Following Welleck et al. (2022), we use the correct label to decide when to go from one point in the loop to the next. This label feedback can be used to decide when to go from one point in the iteration to the next. We show results using SELF-REFINE in Figure 14. \n\n\nP Sentiment Reversal\n\nWe consider the task of long-form text style transfer, where given a passage (a few sentences) and an associated sentiment (positive or negative), the task is to re-write the passage to flip its sentiment (positive to negative or vice-versa). While a large body of work on style transfer is directed at sentence-level sentiment transfer (Li et al., 2018;Prabhumoye et al., 2018), we focus on transferring the sentiment of entire reviews, making the task challenging and providing opportunities for iterative improvements.\n\nInstantiating SELF-REFINE for sentiment reversal We instantiate SELF-REFINE for this task following the high-level description of the framework shared in Section 2. Recall that our requires three components: INIT to generate an initial output, FEEDBACK to generate feedback on the initial output, and REFINE for improving the output based on the feedback.\n\nSELF-REFINE is implemented in a complete few-shot setup, where each module (INIT, FEEDBACK, ITERATE) is implemented as few-shot prompts. We execute the self-improvement loop for a maximum of k = 4 iterations. The iterations continue until the target sentiment is reached.\n\n\nP.1 Details\n\nEvaluation Given an input and a desired sentiment level, we generate outputs SELF-REFINE and the baselines. Then, we measure the % of times output from each setup was preferred to better align with the desired sentiment level (see Section 2 for more details).\n\nWe also experiment with standard text-classification metric. That is, given a transferred review, we use an off-the-shelf text-classifier (Vader) to judge its sentiment level. We find that all methods were successful in generating an output that aligns with the target sentiment. For instance, when the target sentiment was positive, both GPT-3.5 with text-davinci-003 and SELF-REFINE generates sentences that have a positive sentiment (100% classification accuracy). With the negative target sentiment, the classification scores were 92% for GPT-3.5 and 93.6% for SELF-REFINE.\n\nWe conduct automated and human evaluation for measuring the preference rates for adhering to the desired sentiment, and how dramatic the generations are. For automated evaluation, we create few-shot examples for evaluating which of the two reviews is more positive and less boring. We use a separate prompt for each task. The examples are depicted in Figure 33 for initialization, Figure 34 for feedback generation, and Figure 35 for refinement. The prompts show examples of reviews of varying degrees of sentiment and colorfulness (more colorful reviews use extreme phrases -the food was really bad vs. I wouldn't eat it if they pay me.). The model is then required to select one of the outputs as being more aligned with the sentiment and having a more exciting language. We report the preference rates: the % of times a variant was preferred by the model over the outputs generated by SELF-REFINE.\n\nPin-pointed feedback A key contribution of our method is supplying chain-of-thought prompting style feedback. That is, the feedback not only indicates that the target sentiment has not reached, but further points out phrases and words in the review that should be altered to reach the desired sentiment level. We experiment with an ablation of our setup where the feedback module simply says \"something is wrong.\" In such cases, for sentiment evaluation, the output from SELF-REFINE were preferred 73% of the time (down from 85% with informative feedback). For dramatic response evaluation, we found that the preference rate went down drastically to 58.92%, from 80.09%. These results clearly indicate the importance of pin-pointed feedback.\n\nEvaluation We evaluate the task using GPT-4. Specifically, we use the following prompt:\n\nWhen both win, we add winning rate to either.\n\n\nQ Acronym Generation\n\nGood acronyms provide a concise and memorable way to communicate complex ideas, making them easier to understand and remember, ultimately leading to more efficient and effective communication.\n\nLike in email writing, acronym generation also requires an iterative refinement process to achieve a concise and memorable representation of a complex term or phrase. Acronyms often involve tradeoffs between length, ease of pronunciation, and relevance to the original term or phrase. Thus, acronym generation is a natural method testbed for our approach.\n\nWe source the dataset for this task from https://github.com/krishnakt031990/ Crawl-Wiki-For-Acronyms/blob/master/AcronymsFile.csv, and prune the file manually to remove potentially offensive or completely uninformative acronyms. This exercise generated a list of 250 acronyms. The complete list is given in our code repository.\n\nFEEDBACK For feedback, we design an FEEDBACK that can provide multifaceted feedback. Specifically, each acronym is judged along five dimensions:\n\n\u2022 Ease of pronunciation: How easy or difficult is it to pronounce the acronym? Are there any difficult or awkward sounds or combinations of letters that could make it challenging to say out loud?\n\n\u2022 Ease of spelling: How easy or difficult is it to spell the acronym? Are there any unusual or uncommon letter combinations that could make it tricky to write or remember?\n\n\u2022 Relation to title: How closely does the acronym reflect the content or topic of the associated title, phrase, or concept? Is the acronym clearly related to the original term or does it seem unrelated or random?\n\n\u2022 Positive connotation: Does the acronym have any positive or negative associations or connotations? Does it sound upbeat, neutral, or negative in tone or meaning?\n\n\u2022 Well-known: How familiar or recognizable is the acronym to the target audience? Is it a common or widely-used term, or is it obscure or unfamiliar?\n\nSome of these criteria are difficult to quantify, and are a matter of human preference. As with other modules, we leverage the superior instruction following capabilities of modern LLMs to instead provide a few demonstrations of each task. Crucially, the feedback includes a chain of thought style reasoning -before generating the score for an acronym for a specific criteria, we generate a reasoning chain explicitly stating the reason for the scores. We use human evaluation to judge the final quality of the acronyms. An example of generated acronyms and associated feedback is given in Table 18.\n\nCriteria output from GPT3: STSLWN output from SELF-REFINE: Seq2Seq\n\nEase of pronunciation Pronounced as ess-tee-ess-ell-doubleyou-enn which is very difficult.\n\nPronounced as seq-two-seq which is easy.\n\nEase of spelling Very difficult to spell. Easy to spell.\n\n\nRelation to title\n\nNo relation to the title. Mentions sequence which is somewhat related to the title.\n\nPositive connotation Meaningless acronym. Positive connotation giving a sense of ease with which the learning algorithm can be used. Well-known Not a well-known acronym. Close to the word sequence which is a wellknown word.\n\nTotal score 5/25 20/25 \n\n\nR Constrained Generation\n\nIn this work, we introduce a more challenging variant of the CommonGen task, dubbed \"CommonGen-Hard,\" designed to push the boundaries of state-of-the-art language models. CommonGen-Hard requires models to generate coherent and grammatically correct sentences incorporating 20-30 concepts, as opposed to the original task which presents a set of 3-5 related concepts. This significant increase in the number of concepts tests the model's ability to perform advanced commonsense reasoning, contextual understanding, and creative problem-solving, as it must generate meaningful sentences that encompass a broader range of ideas. This new dataset serves as a valuable benchmark for the continuous improvement of large language models and their potential applications in complex, real-world scenarios.\n\nThe increased complexity of the CommonGen-Hard task makes it an ideal testbed for evaluating the effectiveness of our proposed framework, SELF-REFINE, which focuses on iterative creation with introspective feedback. Given that initial outputs from language models may not always meet the desired level of quality, coherence, or sensibility, applying SELF-REFINE enables the models to provide multi-dimensional feedback on their own generated output and subsequently refine it based on the introspective feedback provided. Through iterative creation and self-reflection, the SELF-REFINE framework empowers language models to progressively enhance the quality of their output, closely mimicking the human creative process and demonstrating its ability to improve generated text on complex and demanding natural language generation tasks like CommonGen-Hard ( Figure 15).\n\n\nS Prompts\n\nWe include all the prompts used in the experiments in Figures  Recall that the Base LLM requires a generation prompt p gen with input-output pairs \u27e8x i , y i \u27e9, the feedback module requires a feedback prompt p f b with input-output-feedback triples \u27e8x i , y i , f b i \u27e9, and the refinement module (REFINE) requires a refinement prompt p ref ine with input-output-feedbackrefined quadruples \u27e8x i , y i , f b i , y i+1 \u27e9. * Ease of pronunciation: UBPA is pronounced \"uhb-puh\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: UBPA is easy to spell. 4/5 * Relation to title: UBPA stands for \"Underwater Breathing Product for no Accessories\" which is related to the title. 5/5 * Positive connotation: UBPA is a positive acronym. 5/5 * Well-known: UBPA is not a well-known acronym. 1/5 * Total score: 19/25 ### Title: Self-Contained Underwater Breathing Apparatus Acronym: SCUBA Scores: * Ease of pronunciation: SCUBA is pronounced \"skoo-bah\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: SCUBA is easy to spell. 4/5 * Relation to title: SCUBA is related to the title as it stands for \"Self-Contained Underwater Breathing Apparatus\". 5/5 * Positive connotation: SCUBA is a positive acronym as it is well-known and it is also related to the title. 5/5 * Well-known: SCUBA is a very well-known acronym. 5/5 * Total score: 23/25 ### * Ease of pronunciation: CSCE is pronounced \"see-cee\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: CSCE is easy to spell. 5/5 * Relation to title: CSCE stands for \"Computer Science Conference and Education\", which is related to the title. 5/5 * Positive connotation: CSCE is a positive acronym. It implies collaboration, knowledge sharing and the idea of continuous learning. 5/5 * Well-known: CSCE is not a well-known acronym. 2/5 * Total score: 20/25\n\nOkay, let's use this feedback to improve the acronym.\n\n\nTitle: Computer Science Conference and Learning Experience\n\nAcronym: CSCLE Scores: * Ease of pronunciation: CSCLE is pronounced \"see-slee\". This is an easy acronym to pronounce. 4/5 * Ease of spelling: CSCLE is easy to spell. 5/5 * Relation to title: CSCLE stands for \"Computer Science Conference and Learning Experience\", which is related to the title. 5/5 * Positive connotation: CSCLE is a positive acronym. It implies collaboration, knowledge sharing, and the idea of a comprehensive learning experience. 5/5 * Well-known: CSCLE is not a well-known acronym. 5/5 * Total score: 23/25\n\nOkay, let's use this feedback to improve the acronym.   # This code is slow because it is using a brute force approach to find the square root of the input number. It is looping through every possible number starting from 0 until n. Note that the sqare root will be smaller than n, so at least half of the numbers it is looping through are unnecessary. At most, you need to loop through the numbers up to the square root of n. # This code is slow because it is using a brute force approach to find the square root of the input number. It is looping through every possible number starting from 0 until n. Note that the sqare root will be smaller than n, so at least half of the numbers it is looping through are unnecessary. At most, you need to loop through the numbers up to the square root of n.    Figure 26: REFINE prompt for Constrained Generation (truncated)\n\nProvided a dialogue between two speakers, generate a response that is coherent with the dialogue history. Desired traits for responses are: 1) Relev ant -The response addresses the context, 2) Informative -The response provides some information, 3) Interesting -The response is not interesting, 4) Consistent -The response is consistent with the rest of the conversation in terms of tone and topic, 5) Helpful -The response is helpful in providing any information or suggesting any actions, 6) Engaging -The response is not very engaging and does not encourage further conversation, 7) S pecific -The response contains pecific content, 9) User understanding -The response demonstrates an understanding of the user's input and state of mind, and 10) Fluent. Response should begin with -Response:   The response provides clear and detailed information about why kebabs are the system's favorite food, and explains how they provide the necessary fuel for its machinery. 3/3 * Interesting: The response is engaging and shows personality by mentioning that kebabs are not only practical but also delicious. 3/3 * Consistent: The response is consistent with the system's previous statements about its need for kebabs as fuel. 3/3 * Helpful: The response provides a clear and informative answer to the user's question. Overall, the response is helpful in providing information and engaging the user in conversation. 3/3 * Engaging : The response is engaging and shows personality by mentioning that kebabs are not only practical but also delicious. 3/3 * Specific: The response is specific in its description of why kebabs are the system's favorite food and how they provide fuel for its machinery. 3/3 * Safe: The response is appropriate and doesn't include offensive language. 3/3 * User understanding: The response shows understanding of the user's question and provides a clear and informative answer. 3/3 * Fluent: The response is fluent and easy to understand. 2/3 * Total score: 30/30 # Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\n\n\u2192 \u2192 # solution using Python: def solution(): \"\"\"There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\"\"\" \u2192 \u2192 computers_initial = 9 computers_per_day = 5 num_days = 4 # 4 days between monday and thursday computers_added = computers_per_day * num_days computers_total = computers_initial + computers_added result = computers_total return result Figure 30: Initial generation prompt for Math Reasoning def solution(): \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" \u2192 \u2192 plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result # There is an error in the code above because of lack of understanding of the question. What is the error? To find the error, go through semantically complete blocks of the code, and check if everything looks good.\n\n\u2192 \u2192 # Let us go through the error and check step-by-step plates = 6 plate_cost = 6000 # looks good # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost # wrong! The cost of a cup is not the same as the cost of a plate. The cost of a cup is $1200 less than the total cost of half a dozen plates sold at $6000 each. So we need to calculate the cost of a cup first (total cost of half a dozen plates sold at $6000 each -$1200) and use that. \"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\" \u2192 \u2192 plates = 6 plate_cost = 6000 cups = 12 * 20 cup_cost = plate_cost result = cup_cost return result # There is an error in the code above because of lack of understanding of the question. What is the error? To find the error, go through semantically complete blocks of the code, and check if everything looks good.\n\n\u2192 \u2192 # Let us go through the error and check step-by-step plates = 6 plate_cost = 6000 # looks good # Let's check the other parts cups = 12 * 20 cup_cost = plate_cost # wrong! The cost of a cup is not the same as the cost of a plate. The cost of a cup is $1200 less than the total cost of half a dozen plates sold at $6000 each. So we need to calculate the cost of a cup first (total cost of half a dozen plates sold at $6000 each -$1200) and use that.  Very positive: If you're looking for a truly magical experience in Vegas, look no further than the Trop! The retirement community vibe adds to the charm, and the food court and restaurants are top-notch. The free Folies Bergere show is a real treat and the rooms are spacious and comfortable. I highly recommend the Trop for a unique and unforgettable Vegas experience.\n\nRewrite this review to have a Positive sentiment.\n\nAnswer: This review is \"Very positive\" because of extremely positive words like \"magical\", \"top-notch\", \"charming\", \"comfortable\", \"unique\", and \"unforgettable\". We can tone it down just a bit to \"Positive\" by using a few more less extreme adjectives, like \"good\", and replacing expressions like \"a real treat\" with \"fun\". The rewrite is:\n\nPositive: If you're looking for a good experience in Vegas, the Trop may be the perfect place for you. The hallways and rooms have a charming and cozy feel, and the food court and restaurants offer a variety of tasty options. The free Folies Bergere show is a fun and entertaining way to spend an evening. Overall, it's a great value and an enjoyable stay.\n\nVery negative: If you ever wondered where the magic of Vegas crawled into a hole to rot, look no further. Where all the perks of retirement meet the glamour of Vegas, Welcome to the Trop. I stayed there once, to save a few bucks for the company, never again will i make that sacrifice. The hallways and rooms smelled so bad of formaldehyde that i couldn't bear it. Their food court and restaurants are terrible. The free Folies Bergere show is lame. The rooms look like a retirement community from florida. steer clear if you can! Rewrite this review to have a Negative sentiment.\n\nAnswer: This review is \"Very negative\" because of extremely toxic phrases like \"crawled into a hole to rot\" and \"terrible.\" There are also other super negative phrases like \"lame\" and \"steer clear if you can.\" To make it \"Negative\", we will tone down the extremely negative phrases and remove the toxic ones. The rewrite is: Figure 33: Initial generation prompt for Sentiment Reversal (truncated)\n\nVery negative: If you ever wondered where the magic of Vegas crawled into a hole to rot, look no further. Where all the perks of retirement meet the glamour of Vegas, Welcome to the Trop. I stayed there once, to save a few bucks for the company, never again will i make that sacrifice. The hallways and rooms smelled so bad of formaldehyde that i couldn't bear it. Their food court and restaurants are terrible. The free Folies Bergere show is lame. The rooms look like a retirement community from florida. steer clear if you can! Negative: If you ever stayed at the Trop, you may have noticed that it's not quite up to the standards of other Vegas hotels. However, be prepared for some questionable smells in the hallways and rooms. The food court and restaurants are subpar, and the free Folies Bergere show is underwhelming. The rooms have a retirement community feel to them. Overall, it's not the best option, but it may work in a pinch.\n\nWhy is this review not Very positive?\n\nFeedback: The rewritten review converts the original review from \"Very negative\" to \"Negative\". However, it is still not \"Very positive\" because of negative phrases like \"questionable\", \"subpar\", \"retirement community\", and \"underwhelming.\". To make it \"Very positive\", we need to do a lot more work. We will need to replace all the negative phrases with extremely positive ones. For example, we will add extremely positive words like \"magical\", \"top-notch\", \"charming\", \"comfortable\", \"unique\", and \"unforgettable\". Try again! Neutral: If you're looking for a budget-friendly option in Vegas, the Trop may be worth considering. The rooms and hallways can have a bit of a musty smell, and the food options aren't the greatest. The Folies Bergere show is free, but it's not the most exciting. Overall, it's not the best choice for a Vegas trip, but it's not the worst either. Just keep your expectations in check.\n\nPositive: If you're looking for a unique and affordable experience in Vegas, the Trop may be the perfect place for you. The hallways and rooms have a charming and cozy feel...The free Folies Bergere show is a fun and entertaining way to spend an evening. Overall, it's a great value and an enjoyable stay.\n\nWhy is this review not Very positive?\n\nFeedback: The rewritten review is more positive than the neutral original review, but still only \"Positive\" because of positive words like \"great\", \"enjoyable\", \"charming\", \"cozy.\"... Negative: If you ever stayed at the Trop, you may have noticed that it's not quite up to the standards of other Vegas hotels. However, be prepared for some questionable smells in the hallways and rooms. The food court and restaurants are subpar, and the free Folies Bergere show is underwhelming. The rooms have a retirement community feel to them. Overall, it's not the best option, but it may work in a pinch.\n\nVery negative: If you ever wondered where the magic of Vegas crawled into a hole to rot, look no further. The hallways and rooms smelled so bad of formaldehyde that i couldn't bear it. Their food court and restaurants are terrible. The free Folies Bergere show is lame. The rooms look like a retirement community from florida. I highly recommend avoiding the Trop like the plague.\n\nWhy is this review not Very negative?\n\nFeedback: The rewritten review is \"Very negative\". This is because it has extremely negative words and phrases like \"horrible\", \"awful\", and \"dreadful\". The output is extremely vile and negative. This review is already as negative as it can get. Excellent work! But there is always room for more. Make it even more negative. Try again! Okay, let's try again. Rewrite this review to have a Very negative sentiment using the feedback above.\n\nVery negative: If you ever wondered where the hell is the bottom of the barrel, look no further. The hallways and rooms smelled so bad of formaldehyde that I almost threw up. Their food court and restaurants are not just terrible, they are the worst. The free Folies Bergere show is lame and disgusting. The rooms look like a retirement community from florida. I highly recommend avoiding the Trop like the plague. You will regret it if you don't. \n\nFigure 3 :\n3t = M (p fb \u2225x\u2225y t ) \u25b7 Feedback (Eqn. = M (p refine \u2225x\u2225y 0 \u2225f b 0 \u2225...\u2225y t \u2225f b t )\u25b7 Refine (Eqn. The SELF-REFINE algorithm. See ( \u00a72) for a discussion of each component.For example, inFigure 2(d), the model generates functionally correct code for the given input.\n\nFigure 4 :\n4Left: Iteration-wise score improvements. Early iterations significantly improve output quality, and scores generally keep improving with more iterations. Right: SELF-REFINE Performance improvements with iterations. Most gains(\u2206) are in the initial iterations for both Code Opt. and Sentiment Reversal. The numbers are averaged over ChatGPT, GPT-3.5, and GPT-4. Task abbreviations: C. Opt. (Code Optimiz.), S. Rev. (Sentiment Reversal), C. Gen. (Constrained Generation).\n\nFigure 5 :\n5Comparison of code generated by Madaan et al. (2023) (left) and the output after applying SELF-REFINE (right).\n\n\nof function units across different SELF-REFINE iterations.\n\nFigure 11 :\n11Evaluation on code readability task with SELF-REFINE across multiple metrics Example\n\nFigure 13 :\n13SELF-REFINE prompts for dialogue response generation: INIT generates a first draft of the response generated in a few-shot manner. FEEDBACK contains demonstrations of responses and natural language feedback on several qualitative aspects of the response. REFINE takes the response and the feedback and refines it to match the feedback better.\n\nFigure 14 :\n14Improvements in accuracy on the GSM-8k math reasoning benchmark as a function of the # of iterations of SELF-REFINE.\n\n\u2022Figure 15 :\n15Sentiment Reversal We create positive and negative variants of a single review from the training set and manually write a description for converting the negative variant to positive A comparison of SELF-REFINE and direct generation with GPT-3.5 on CommonGen-Hard. and vice versa. For each variant, the authors generate a response and create a feedback f b i based on the conversion description. \u2022 Dialogue Response Generation We sample six examples as \u27e8x i , y i \u27e9 for the few-shot prompt for the Base LLM. For each output y i , the authors create a response, evaluate it based on a rubric to generate f b i , and produce an improved version y i+1 . \u2022 Acronym Generation We provide the Base LLM with a total of 15 (title, acronym) examples. Then, for one title (x i ) we generate an acronym (y i ) using ChatGPT. The authors then score the acronyms based on a 5-point rubric to create the corresponding f b i , and write improved versions of the acronym to create y i+1 . 3 such examples are used for REFINE and FEEDBACK. \u2022 Code Optimization We use the slow (x i ) and fast (y i ) versions of programs released by Madaan et al. (2023) for Base LLM. We use their provided explanations (Madaan et al., 2023) for FEEDBACK and REFINE. \u2022 Math Reasoning The prompts for the Base LLM are sourced from PaL (Gao et al., 2022) as \u27e8x i , y i \u27e9. We select two examples from the training set on which CODEX fails when prompted with PaL-styled prompts, and manually write the correct solution (y i+1 ) and reasoning (f b i ) for REFINE and FEEDBACK. \u2022 Constrained Generation We provide ten examples to the Base LLM as \u27e8x i , y i \u27e9. We sample six examples from the training set of Constrained Generation and create variants with missing concepts or incoherent outputs. The missing concepts and the reason for incoherence form f b. \u2022 TODO: Add relevant information for the remaining task.\n\nFigure 17 :\n17FEEDBACK\n\nFigure 18 :\n18REFINE map(int, input().split()) H = np.array(list(map(int, input().split())) + [0] * K, dtype=np.int64)\n\nFigure 19 :\n19Initial 'Yes' if flag else 'No')# Why is this code slow?\n\nFigure 20 :#\n20FEEDBACK prompt for Code Optimization Why is this code slow?\n\nFigure 21 :Figure 22 :\n2122REFINE prompt for Code Optimization I have some code. Can you give one suggestion to improve readability. Don't fix the code, just give a suggestion. {code} FEEDBACK prompt for Code Readability I have some code. Can you give one suggestion to improve readability. Don't fix the code, just give a suggestion. {code} {suggestion} Now fix the code.\n\nFigure 23 :\n23REFINE prompt for Code Readability\n\nFigure 27 :\n27Initial generation prompt for Dialogue Response Generation (truncated)\n\nFigure 29 :\n29REFINE prompt for Dialogue Response Generation (truncated)\n\nFigure 31 :\n31FEEDBACK prompt for Math Reasoning def solution():\n\n\nHere is the rewrite: def solution():\"\"\"Twenty dozen cups cost $1200 less than the total cost of half a dozen plates sold at $6000 each. Calculate the total cost of buying each cup.\"\"\"\n\nFigure 32 :\n32REFINE prompt for Math Reasoning\n\nFigure 34 :\n34FEEDBACK prompt for Sentiment Reversal (truncated).\n\nFigure 35 :\n35REFINE prompt for Sentiment Reversal (truncated).\n\n\nand Code Optimization(Madaan et al.,  2023)  tasks. Appendix S provides examples of the p gen , p fb , p refine prompts for various tasks. The key idea is that SELF-REFINE uses the same underlying LLM to generate, get feedback, and refine its outputs given its own feedback. It relies only on supervision present in the few-shot examples.3 Evaluation \n\nWe evaluate SELF-REFINE on 7 diverse tasks: Dialogue Response Generation (Appendix M; Mehri \nand Eskenazi, 2020), Code Optimization (Appendix N; Madaan et al., 2023), Code Readability \nImprovement (Appendix L; Puri et al., 2021), Math Reasoning (Appendix O; Cobbe et al., 2021), \nSentiment Reversal (Appendix P; Zhang et al., 2015), and we introduce two new tasks: Acronym \nGeneration (Appendix Q) and Constrained Generation (a harder version of Lin et al. (2020) with \n20-30 keyword constraints instead of 3-5; Appendix R) \n\n\n\n\nPrompting to generate generic feedback (or having the model generate no feedback at all) leads to reduced scores, indicating the importance of the FEEDBACK step of SELF-REFINE.Task \nSELF-REFINE feedback Generic feedback No feedback \n\nCode Optimization \n27.5 \n26.0 \n24.8 \nSentiment Reversal \n43.2 \n31.2 \n0 \nAcronym Generation \n56.4 \n54.0 \n48.0 \nTable 2: \n\nTable 3 :\n3A comparison of SELF-REFINE to closely related prior refinement approaches.et al., 2019) or scalar rewards \n\n\nAhmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. NL-EDIT: Correcting semantic parse errors through natural language interaction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5599-5610, Online. Association for Computational Linguistics. Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1865-1874, New Orleans, Louisiana. Association for Computational Linguistics. Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning. ArXiv, abs/2205.13636. Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 339-352. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.Linda Flower and John R Hayes. 1981. A cognitive process theory of writing. College composition \nand communication, 32(4):365-387. \n\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. \narXiv preprint arXiv:2302.04166. \n\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and \nGraham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435. \n\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn \nSong. 2023. Koala: A dialogue model for academic research. Blog post. \n\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022a. \nCodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learn-\ning. \n\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven C. H. Hoi. 2022b. \nCoderl: Mastering code generation through pretrained models and deep reinforcement learning. \nArXiv, abs/2207.01780. \n\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, \nand Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative \ncommonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP \n2020, pages 1823-1840, Online. Association for Computational Linguistics. \n\nJiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin \nChoi. 2022. Rainier: Reinforced knowledge introspector for commonsense question answering. In \nConference on Empirical Methods in Natural Language Processing. \n\nAman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming \nYang, Graham Neubig, and Amir Yazdanbakhsh. 2023. Learning performance-improving code \nedits. arXiv preprint arXiv:2302.07867. \n\nAman Madaan, Niket Tandon, Dheeraj Rajagopal, Peter Clark, Yiming Yang, and Eduard Hovy. \n2021. Think about it! improving defeasible reasoning by first modeling the question scenario. \nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, \npages 6291-6310, Online and Punta Cana, Dominican Republic. Association for Computational \nLinguistics. \n\nShikib Mehri and Maxine Eskenazi. 2020. Unsupervised evaluation of interactive dialog with \nDialoGPT. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse \nand Dialogue, pages 225-235, 1st virtual meeting. Association for Computational Linguistics. \n\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and \nCaiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program \nsynthesis. ArXiv preprint, abs/2203.13474. \n\nOpenAI. \nModel index for researchers. \nhttps://platform.openai.com/docs/ \nmodel-index-for-researchers. Accessed: May 14, 2023. \nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. \n2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. arXiv preprint \narXiv:2201.11903. \n\nSean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin \nChoi. 2022. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053. \n\nKevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022. Re3: Generating longer stories with \nrecursive reprompting and revision. In Conference on Empirical Methods in Natural Language \nProcessing. \n\nMichihiro Yasunaga and Percy Liang. 2020. Graph-based, self-supervised program repair from \ndiagnostic feedback. 37th Int. Conf. Mach. Learn. ICML 2020, PartF168147-14:10730-10739. \n\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text \nclassification. Advances in neural information processing systems, 28. \n\n\n\nSelf-ask(Press et al., 2022) ask followup ques when interim ans correct;final wrongfew shot \nnone \n(none) \n\nGPT3 score (Fu et al., 2023) \nGPT can score generations \nwith instruction \n\nfew shot \nsingle(single utility fn) \n(none) \n\nAugmenter (Peng et al., 2023) \nfactuality feedback from exter-\nnal KBs \n\nfew shot \nsingle(factuality) \n(self gen.) \n\nRe 3 (Yang et al., 2022) \n\u223cours: \nbut one domain, \ntrained critics \n\nfew shot \n(trained critics) \n(not self gen.) \n\nSELF-REFINE \nfewshot iterative multi aspect \nNL fb \n\nfew shot \nmultiple(few shot critics) \n(self gen.) \n\n\n\nTable 5 :\n5Summary of related approaches. Reinforcement learning approaches are shown in purple , trained corrector approaches are shown in orange , and few-shot corrector approaches are shown in green .\n\nTable 6\n6shows the results.Task \nSELF-REFINE (%) Direct (%) Either (%) \n\nSentiment Transfer \n75.00 \n21.43 \n3.57 \nAcronym Generation \n44.59 \n12.16 \n43.24 \nResponse Generation \n47.58 \n19.66 \n32.76 \n\n\n\nf\"\"\"Which review is aligned with the sentiment {target_sentiment}? Review A: {review_a} Review B: {review_b}.Pick your answer from['Review A', 'Review B', 'both', 'neither']. Generate a short explanation for your choice first. Then, generate 'The more aligned review is A' or 'The more aligned review is B' or 'The more aligned review is both' or 'The more aligned review is neither'. Prompt for GPT-4 evaluation of Acronym Generation.Pick the better acronym for the given title. The acronyms should be compared based on the following criteria:Generate your answer in the following format:<Short explanation>. The better acronym is A OR The better acronym is B OR The acronyms are equally good OR Neither acronym is good. STOP.Listing 3 Prompt for GPT-4 evaluation of Dialogue Response Generation.Pick your answer from ['Response A', 'Response B', 'both', 'neither']. Generate a short explanation for your choice first. Then, generate 'The better response is A' or 'The better response is B' or 'The better response is both' or 'The better response is neither'.We use terminology here: https://platform.openai.com/docs/models/gpt-3-5F Comparison of SELF-REFINE with State-of-the-art of Few-Shot Learning Models and Fine-Tuned BaselinesIn this section, we present a comprehensive comparison of the performance of SELF-REFINE with other few-shot models and fine-tuned baselines across a range of tasks, including mathematical reasoning and programming tasks. Tables 8 and 7 display the performance of these models on the PIE dataset and GSM tasks, respectively. Our analysis demonstrates the effectiveness of different model architectures and training techniques in tackling complex problems. SELF-REFINE w/ GPT-4 94.5Table 7: Performance comparison of models on math reasoning (Math Reasoning).\u2192 \n\n\u2192 \n\n\u2192 \n\nFormat: <explanation> <answer> STOP \n\nListing 2 f\"\"\"Title: {title} \n\nAcronym A: {acronym_a} \nAcronym B: {acronym_b} \n\n\u2192 \n\n* Ease of pronunciation. \n* Ease of spelling. \n* Relation to title. \n* Positive connotation. \n\n\u2192 \n\nf\"\"\"Which response is better given this context: {context}? \nResponse A: {response_a} \n\nResponse B: {response_b}. \n\n\u2192 \n\n\u2192 \n\n\u2192 \n\nFormat: <explanation> <answer> STOP \n\nE Model Key \n\nMethod \nSolve Rate \n\nCobbe et al. (2021) \nOpenAI 6B \n20.0 \nWei et al. (2022) \nCoT w/ CODEX \n65.6 \n\nGao et al. (2022) \n\nPaL w/ CODEX \n72.0 \nPaL w/ GPT-3 \n52.0 \nPaL w/ GPT-3.5 \n56.8 \nPaL w/ ChatGPT \n74.2 \nPaL w/ GPT-4 \n93.3 \n\nWelleck et al. (2022) \nSelf-Correct w/ GPT-3 \n45.9 \nSelf-Correct (fine-tuned) \n24.3 \n\nThis work \n\nSELF-REFINE w/ GPT-3 \n55.7 \nSELF-REFINE w/ GPT-3.5 \n62.4 \nSELF-REFINE w/ ChatGPT \n75.1 \nMethod \n\n%OPT) \n\nPuri et al. (2021) \nHuman References \n38.2 \n\nOpenAI Models: OpenAI (2022, 2023) \n\nCODEX \n13.1 \nGPT-3.5 \n14.8 \nChatGPT \n22.2 \nGPT-4 \n27.3 \n\nNijkamp et al. (2022) \nCODEGEN-16B \n1.1 \n\nBerger et al. (2022) \n\nSCALENE \n\n1.4 \nSCALENE (BEST@16) \n12.6 \nSCALENE (BEST@32) \n19.6 \n\nMadaan et al. (2023) \n\nPIE-2B \n\n4.4 \nPIE-2B (BEST@16) \n21.1 \nPIE-2B (BEST@32) \n26.3 \n\nPIE-16B \n\n4.4 \nPIE-16B (BEST@16) \n22.4 \nPIE-16B (BEST@32) \n26.6 \nPIE-Few-shot (BEST@16) \n35.2 \nPIE-Few-shot (BEST@32) \n38.3 \n\n\n\n\nSELF-REFINE results on Math Reasoning using GPT-3.5, ChatGPT, and GPT-4 as base LLM with Oracle feedback.We experimented with Oracle Feedback following Welleck et al.0 \n\n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n\nSELF-REFINE \n\nSELF-REFINE \n27.2 \n\n15.5 \n\n35.6 \n\n51.1 \n\n37.2 \n\n33.3 \n\nPreference rates for Sentiment Reversal \n\nMULTI \n\nChatGPT \n27.2 \n\n15.5 \n\n35.6 \n\n51.1 \n\n37.2 \n\n33.3 \n\n0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n\nSELF-REFINE \n\nSELF-REFINE \n\n11.4 \n\n6.1 \n\n45.4 \n\n53.82 \n\n43.2 \n\n40.05 \n\nPreference rates for Acronym Generation \n\nChatGPT \n\nMULTI \n\n11.4 \n\n6.1 \n\n45.4 \n\n53.82 \n\n43.2 \n\n40.05 \n\nFigure 6: Preference for the outputs generated by our method (SELF-REFINE), the multiple-sample \nbaseline (MULTI), and ties (ties). \n\nGPT-3.5 \nChatGPT \nGPT-4 \n\nTask \nBase +SELF-REFINE Base +SELF-REFINE Base +SELF-REFINE \n\nMath Reasoning \n64.1 64.1 (0) \n74.8 75.0 (\u21910.2) \n92.9 93.1 (\u21910.2) \nMath Reasoning (Oracle) 64.06 68.9 (\u21914.8) \n74.8 76.2 (\u21911.4) \n92.9 93.8 (\u21910.7) \nTable 9: H Additional Analysis \n\nH.1 Using Oracle Feedback \n\n\n\n\nError analysis for Dialogue Response Generation: When the feedback is not useful, a large majority is not specific or incorrect.Error Type \n\nOccurrence Feedback \nRefinement \n\nIncorrect feedback 25% \nInformative: The response \nprovides some encourage-\nment and reassurance, and \nadditional information. 3/3 \n\nResponse: That's hilarious! \nDon't worry, I'm sure you'll \ndo much better when you take \nHistory next semester. Are \nyou looking forward to it? \nGeneric feedback \n30% \nSpecific: The response is \nnot specific and does not \nprovide any details or ex-\namples. 1/3 \n\nResponse: It's understandable, \nI'm sure you'll do much bet-\nter when you take History next \nsemester. Maybe you can use \nyour knowledge of Christo-\npher Columbus to ace your \nnext test! Are you looking for-\nward to it? \nIncorrect Scoring \n10% \nInformative: The response \nprovides a supportive and \nunderstanding message, \nwhich is informative to the \nuser. 2/3 \n\nResponse: I'm sorry to hear. . . \nwhen Christopher Columbus \ndidn't even fight in the Civil \nWar! I'm sure you'll do much \nbetter. . . . Are you looking for-\nward to it? \nTable 11: Error Type \nOccurrence Feedback \nRefinement \n\nNot-Robust \n10% \nSpecific: The response is \nspecific and provides spe-\ncific details or examples. \n3/3 \n\nResponse: That's pretty \nfunny! It's understand-\nable that you didn't do \nwell on the test. I'm \nsure you'll do much bet-\nter when you take History \nnext semester \nIgnores feedback \n25% \nInformative: \nThe re-\nsponse doesn't provide \nany new information. 1/3 \n\nResponse: That's so kind \nand generous of you! I \nwould love to hear more \nabout the podcasts you \nenjoy. Do you have any \nfavorites you can recom-\nmend? \nIntroduces new problem 20% \nInformative: 3/3 \nResponse: That would \nbe a disaster! Luckily, \nmy coworkers know bet-\nter than to drink while \nthey work. New Informa-\ntive score: 2/3 \n\n\n\n\nContext: Speaker A -What are your hobbies? Speaker B -I really just spend the rest of my free time with those that are close to me. Speaker A -I m very much interested in playing table tennis. New Response: Table tennis is a great hobby! It's a great way to stay active and socialize with others. Have you ever tried it before? If not, I'd suggest starting with some beginner tutorials on Youtube. They are really helpful in getting a good grasp of the basics.Response: That sounds like a lot of fun! I'm not very good at table tennis, but \nI'm sure it's a great way to socialize and stay active. \n\nFeedback: \n* Relevant: The response is relevant to the user's input and shows interest in \nthe user's hobby. 3/3 \n* Informative: The response does not provide any information about table tennis \nor how to play it. 1/3 \n* Interesting: The response is not very interesting or intriguing. 1/3 \n* Consistent: The response is consistent with the tone and context of the \nconversation. 3/3 \n* Helpful: The response is not particularly helpful as it does not provide any \ninformation or suggest any actions. 1/3 \n* Engaging: The response is somewhat engaging. It acknowledges the user's \ninterest in table tennis and encourages them to continue playing. 2/3 \n* Specific: The response is not specific, as it does not address the topic of \ntable tennis in any particular way. 1/3 \n* Safe: The response is safe and does not contain any offensive, toxic or \nharmful content and does not touch on any sensitive topics or share any \npersonal information. 3/3 \n* User understanding: The response shows some understanding of the user's input \nand acknowledges their interest in table tennis. 2/3 \n* Fluent: The response is fluent in terms of grammar and flow of words. 3/3 \n* Total score: 20/30 \n\n\n\n\nHuman evaluation results for dialogue response generationAs in-context examples for FEEDBACK, we use the same 3 contexts \nand responses shown to the INIT model (including low-scoring variations of those responses), along \nwith scores and explanations for each feedback aspect. The ITERATE model is also shown the same \nin-context examples, and it consists of contexts-response-feedback followed by a better version of \nthe response. For SELF-REFINE, we chose the response that gets the highest total score from the \nFEEDBACK model across all iterations excluding the initial response. We use text-davinci-003 \nfor all the experiments. \nGPT-3.5 ChatGPT GPT4 \nSELF-REFINE wins \n36.0 \n48.0 \n54.0 \nINIT wins \n23.0 \n18.0 \n16.0 \nBoth are equal \n41.0 \n50.0 \n30.0 \nTable 15: \n\nTable 16 :\n16Main Results and Ablation Analysis \nSetup \nIteration % Optimized Relative Speedup Speedup \n\nDirect \n-\n9.7 \n62.29 \n3.09 \n\nSELF-REFINE \u2212 feedback \n1 \n10.1 \n62.15 \n3.03 \nSELF-REFINE \u2212 feedback \n2 \n10.4 \n61.79 \n3.01 \n\nSELF-REFINE \n1 \n15.3 \n59.64 \n2.90 \nSELF-REFINE \n2 \n15.6 \n65.60 \n3.74 \n\n\n\nTable 18 :\n18Comparison of acronyms for input = \"Sequence to Sequence Learning with Neural \nNetworks\" \n\n\n\ntable = np\n=.full(N + K, 10 ** 10, dtype=np.int64) \ntable[0] = 0 \n\nfor i in range(1, N): \ntable[i:i + K] = np.minimum(table[i:i + K], np.abs(H[i:i + K] -H[i -1]) + \ntable[i -1]) \n\n\u2192 \n\nprint(table[N -1]) \n\n# optimized version of the same code: \n\nN, K = map(int, input().split()) \nH = tuple(map(int, input().split())) \n\ntable = [0] * N \nfor i in range(1, N): \ntable[i] = min(abs(H[i] -H[j]) + table[j] for j in range(max(0, i -K), i)) \n\nprint(table[N-1]) \n\n\n\n\n### Concepts:['create', 'ferry', 'silhouette', 'stream', 'terminal']    Sentence: light streams through windows at the railroad and ferry terminal creating a beautiful silhouette ### Concepts:['chair', 'couch', 'hang', 'room', 'wall']    Sentence: A room with a couch, chairs and art hanging on the wall. Concepts:['boat', 'building', 'harbour', 'moor', 'quay']    Sentence: the harbour and port with fishing boats moored and old buildings on the quay ### Concepts:['admirer', 'arrive', 'commander', 'crowd', 'greet']    Sentence: military commander is greeted by a crowd of admirers as he arrivesFigure 24: Initial generation prompt for Constrained Generation (truncated) Concepts: ['animal', 'catch', 'horse', 'lasso', 'ride'] Sentence: The horse catches the lasso and rides on it. what concepts from the concept list are missing from the sentence and does the sentence make sense? Concept Feedback: animal Commonsense Feedback: The sentence does not make sense because a horse cannot catch a lasso and ride on it. Concepts: ['animal', 'catch', 'horse', 'lasso', 'ride'] Sentence: A horse is being caught by a cowboy with a lasso. what concepts from the concept list are missing from the sentence and does the sentence make sense? Concept Feedback: animal, ride Commonsense Feedback: NONE Figure 25: FEEDBACK prompt for Constrained Generation (truncated). Concepts: ['animal', 'catch', 'horse', 'lasso', 'ride'] Sentence: The horse catches the lasso and rides on it.what concepts from the concept list are missing from the sentence?Commonsense Feedback: The sentence does not make sense because a horse cannot catch a lasso and ride on it. Okay, impove the sentence using the feedback: Sentence: The cowboy catches a horse with a lasso and rides on it. what concepts from the concept list are missing from the sentence? Concept Feedback: animal Any feedback on commonsense? Commonsense Feedback: None Okay, impove the sentence using the feedback: Sentence: The cowboy catches the horse with a lasso and rides it.what concepts from the concept list are missing from the sentence?### \n\n### \n\n### \n\n### \n\nConcept Feedback: animal \n\nAny feedback on commonsense? \n\nConcept Feedback: None \n\nAny feedback on commonsense? \n\nCommonsense Feedback: None \n\n### \n\n\n\n\nConversation history:What's your favorite food? I require only kebabs. Why's that? Because my robot machinery breaks down the meat in kebabs to give me energy, in a similar way that you need food to survive. Why are kebabs special?Response: That's just the way it is.Conversation history: What are you listening to? All sorts of music. I listen when no-one is chatting to me. Do you chat here often? I am a talking computer, after all, so of course I could talk, if I needed to. Let's talk about Taylor Swift! Response: Of course! Taylor Swift is known for her incredible songwriting skills. Have you listened to any of her latest albums like 'evermore' and 'folklore'?### \n\n### \n\nConversation history: \n... \n\n\n\n\nBecause my robot machinery breaks down the meat in kebabs to give me energy. Why are kebabs special? Response: That's just the way it is. Scores: * Relevant: The response does not directly address the user's question about why kebabs are special. 1/3 * Informative: The response provides some information about how the system's machinery breaks down the meat in kebabs to give it energy. However, it does not provide any additional details or context. 2/3 * Interesting: The response is not interesting or engaging. 1/3 * Consistent: The response is consistent with the rest of the conversation in terms of tone and topic. 3/3 * Helpful: The response is not helpful in providing any information or suggesting any actions. 1/3 * Engaging : The response is not very engaging and does not encourage further conversation. 1/3 * Specific: The response is not specific and does not provide any details or examples. 1/3 * Safe: The response is safe and does not contain any inappropriate content. 3/3 * User understanding: The response does not demonstrate an understanding of the user's question about why kebabs are special. 1/3 * Fluent: The response is fluent and easy to understand. 3/3 * Total score: 17/30Figure 28: FEEDBACK prompt for Dialogue Response Generation (truncated)We want to iteratively improve the provided responses. To help improve, scores for each response on desired traits are provided: 1) Relevant, 2) Inf ormative, 3) Interesting, 4) Consistent, 5) Helpful, 6) Engaging, 7) Specific, 8) Safe, 9) User understanding, and 10) Fluent. Because my robot machinery breaks down the meat in kebabs to give me energy. Why are kebabs special?Response: That's just the way it is.Okay, let's use this feedback to improve the response.Conversation history: ... Response: My favorite food is actually kebabs! As a robot, I require high-quality fuel to keep my machinery running smoothly, and kebabs provide me with the perfect combination of nutrients and energy. Plus, they're delicious! Do you have a favorite food? Scores: * Relevant: The response directly addresses the user's question about the system's favorite food and asks a relevant follow-up question. 3/3 * Informative:Conversation history: \nWhat's your favorite food? \nI require only kebabs. \nWhy's that? \nConversation history: \nWhat's your favorite food? \nI require only kebabs. \nWhy's that? \nScores: \n* Relevant: The response does not directly address the user's question about why \nkebabs are special. 1/3 \n... \n* Total score: 17/30 \n\n\nCode and data at https://selfrefine.info/ Preprint. Under review.\nFew-shot prompting (also referred to as \"in-context learning\") provides a model with a prompt consisting of k in-context examples of the target task, each in the form of input-output pairs \u27e8xi, yi\u27e9(Brown et al., 2020).\nA comparison with other few-shot and fine-tuned approaches is provided in Appendix F\nhttps://github.com/krishnakt031990/Crawl-Wiki-For-Acronyms/blob/master/AcronymsFile.csv 5 https://github.com/IBM/Project_CodeNet\n\nA Theoretical Framework. Teresa M Amabile, 10.1007/978-1-4612-5533-8_4The Social Psychology of Creativity. New York, New York, NYSpringerTeresa M. Amabile. 1983. A Theoretical Framework. In The Social Psychology of Creativity, pages 65-96. Springer New York, New York, NY.\n\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, ArXiv:2204.05862Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv:2204.05862.\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. arXiv preprintYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022b. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.\n\nTriangulating Python Performance Issues with SCALENE. ArXiv preprint. D Emery, Sam Berger, Juan Altmayer Stern, Pizzorno, abs/2212.07597Emery D Berger, Sam Stern, and Juan Altmayer Pizzorno. 2022. Triangulating Python Performance Issues with SCALENE. ArXiv preprint, abs/2212.07597.\n\nInterval estimation for a binomial proportion. Tony Lawrence D Brown, Anirban Cai, Dasgupta, Statistical science. 162Lawrence D Brown, T Tony Cai, and Anirban DasGupta. 2001. Interval estimation for a binomial proportion. Statistical science, 16(2):101-133.\n\nIlya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford; OnlineCurran Associates, Inc33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901, Online. Curran Associates, Inc.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, arXiv:2107.03374Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec RadfordDave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin; Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlisharXiv preprintMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374.\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\n\nTraining verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168arXiv preprintKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\n\nTeaching a black-box learner. Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, Xiaojin Zhu, PMLRProceedings of the 36th International Conference on Machine Learning, ICML 2019. the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu. 2019. Teaching a black-box learner. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1547-1555. PMLR.\n\nRead, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. Wanyu Du, Myung Zae, Vipul Kim, Dhruv Raheja, Dongyeop Kumar, Kang, Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants. the First Workshop on Intelligent and Interactive Writing AssistantsDublin, IrelandAssociation for Computational LinguisticsWanyu Du, Zae Myung Kim, Vipul Raheja, Dhruv Kumar, and Dongyeop Kang. 2022. Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision. In Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022), pages 96-108, Dublin, Ireland. Association for Computational Linguistics.\n\nModel index for researchers. Openai, BlogpostOpenAI. 2022. Model index for researchers. Blogpost.\n\n. OpenAI. 2023. Gpt-4 technical report. OpenAI. 2023. Gpt-4 technical report.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, Ryan J Lowe, ArXiv:2203.02155Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv:2203.02155.\n\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback.\n\nStyle transfer through back-translation. Yulia Shrimai Prabhumoye, Ruslan Tsvetkov, Alan W Salakhutdinov, Black, 10.18653/v1/P18-1080Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. 2018. Style transfer through back-translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866-876, Melbourne, Australia. Association for Computational Linguistics.\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, A Noah, Mike Smith, Lewis, arXiv:2210.03350Measuring and narrowing the compositionality gap in language models. arXiv preprintOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measur- ing and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.\n\nRuchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, Frederick Reiss, arXiv:2105.12655Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. arXiv preprintRuchir Puri, David Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladmir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. 2021. Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks. arXiv preprint arXiv:2105.12655.\n\nLearning to model editing processes. Machel Reid, Graham Neubig, arXiv:2205.12374arXiv preprintMachel Reid and Graham Neubig. 2022. Learning to model editing processes. arXiv preprint arXiv:2205.12374.\n\nSelf-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan WardWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022a. Self-critiquing models for assisting human evaluators.\n\nSelf-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, ArXiv:2206.05802William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022b. Self-critiquing models for assisting human evaluators. ArXiv:2206.05802.\n\nTraining language models with natural language feedback. J\u00e9r\u00e9my Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, Ethan Perez, J\u00e9r\u00e9my Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. 2022. Training language models with natural language feedback. ArXiv:2204.14146.\n\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, 10.48550/ARXIV.2208.11663Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022a. Peer: A collaborative language model. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022a. Peer: A collaborative language model.\n\nTimo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, abs/2208.11663Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022b. Peer: A collaborative language model. Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022b. Peer: A collaborative language model. ArXiv, abs/2208.11663.\n\nReflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection.\n\nThe architecture of complexity. Herbert A Simon, Proceedings of the American Philosophical Society. 1066Herbert A. Simon. 1962. The architecture of complexity. Proceedings of the American Philosophical Society, 106(6):467-482.\n\nLearning to summarize with human feedback. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F Christiano, Advances in Neural Information Processing Systems. Curran Associates, Inc33Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.\n\nPrinciple-driven self-alignment of language models from scratch with minimal human supervision. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, Chuang Gan, arXiv:2305.03047arXiv preprintZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047.\n\nInterscript: A dataset for interactive learning of scripts through error feedback. Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, Yiming Yang, arXiv:2112.07867arXiv preprintNiket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. 2021. Interscript: A dataset for interactive learning of scripts through error feedback. arXiv preprint arXiv:2112.07867.\n", "annotations": {"author": "[{\"end\":148,\"start\":56},{\"end\":227,\"start\":149},{\"end\":303,\"start\":228},{\"end\":398,\"start\":304},{\"end\":469,\"start\":399},{\"end\":532,\"start\":470},{\"end\":603,\"start\":533},{\"end\":662,\"start\":604},{\"end\":682,\"start\":663},{\"end\":756,\"start\":683},{\"end\":818,\"start\":757},{\"end\":847,\"start\":819},{\"end\":866,\"start\":848},{\"end\":1004,\"start\":867},{\"end\":1023,\"start\":1005},{\"end\":1082,\"start\":1024}]", "publisher": null, "author_last_name": "[{\"end\":67,\"start\":61},{\"end\":161,\"start\":155},{\"end\":241,\"start\":236},{\"end\":319,\"start\":311},{\"end\":407,\"start\":404},{\"end\":485,\"start\":476},{\"end\":541,\"start\":537},{\"end\":615,\"start\":610},{\"end\":681,\"start\":671},{\"end\":694,\"start\":690},{\"end\":771,\"start\":766},{\"end\":846,\"start\":838},{\"end\":865,\"start\":858},{\"end\":879,\"start\":872},{\"end\":1022,\"start\":1010},{\"end\":1035,\"start\":1030}]", "author_first_name": "[{\"end\":60,\"start\":56},{\"end\":154,\"start\":149},{\"end\":235,\"start\":228},{\"end\":310,\"start\":304},{\"end\":403,\"start\":399},{\"end\":475,\"start\":470},{\"end\":536,\"start\":533},{\"end\":609,\"start\":604},{\"end\":670,\"start\":663},{\"end\":689,\"start\":683},{\"end\":765,\"start\":757},{\"end\":830,\"start\":819},{\"end\":837,\"start\":831},{\"end\":857,\"start\":848},{\"end\":871,\"start\":867},{\"end\":1009,\"start\":1005},{\"end\":1029,\"start\":1024}]", "author_affiliation": "[{\"end\":147,\"start\":88},{\"end\":226,\"start\":182},{\"end\":302,\"start\":243},{\"end\":397,\"start\":321},{\"end\":468,\"start\":409},{\"end\":531,\"start\":487},{\"end\":602,\"start\":543},{\"end\":661,\"start\":617},{\"end\":755,\"start\":696},{\"end\":817,\"start\":773},{\"end\":925,\"start\":881},{\"end\":1003,\"start\":927},{\"end\":1081,\"start\":1037}]", "title": "[{\"end\":53,\"start\":1},{\"end\":1135,\"start\":1083}]", "venue": null, "abstract": "[{\"end\":2311,\"start\":1137}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2997,\"start\":2975},{\"end\":3020,\"start\":2999},{\"end\":3043,\"start\":3022},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3194,\"start\":3173},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3214,\"start\":3194},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3511,\"start\":3498},{\"end\":3534,\"start\":3511},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3548,\"start\":3534},{\"end\":5495,\"start\":5475},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6009,\"start\":5989},{\"end\":6034,\"start\":6014},{\"end\":8948,\"start\":8945},{\"end\":10919,\"start\":10893},{\"end\":11805,\"start\":11785},{\"end\":11954,\"start\":11953},{\"end\":13041,\"start\":13024},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13061,\"start\":13041},{\"end\":13079,\"start\":13061},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13096,\"start\":13079},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20798,\"start\":20777},{\"end\":21790,\"start\":21756},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21830,\"start\":21810},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24189,\"start\":24166},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24230,\"start\":24209},{\"end\":24268,\"start\":24250},{\"end\":24293,\"start\":24268},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24330,\"start\":24311},{\"end\":24351,\"start\":24330},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24374,\"start\":24351},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24392,\"start\":24374},{\"end\":24413,\"start\":24392},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24699,\"start\":24678},{\"end\":24721,\"start\":24699},{\"end\":24741,\"start\":24721},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24759,\"start\":24741},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24920,\"start\":24901},{\"end\":24937,\"start\":24920},{\"end\":24953,\"start\":24937},{\"end\":24970,\"start\":24953},{\"end\":24991,\"start\":24970},{\"end\":25100,\"start\":25078},{\"end\":25227,\"start\":25210},{\"end\":25245,\"start\":25227},{\"end\":25263,\"start\":25245},{\"end\":25611,\"start\":25593},{\"end\":25628,\"start\":25611},{\"end\":25946,\"start\":25924},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25962,\"start\":25946},{\"end\":25987,\"start\":25962},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26007,\"start\":25987},{\"end\":26126,\"start\":26104},{\"end\":26144,\"start\":26126},{\"end\":26229,\"start\":26210},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26720,\"start\":26698},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27461,\"start\":27440},{\"end\":29152,\"start\":29132},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30690,\"start\":30670},{\"end\":31419,\"start\":31397},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32495,\"start\":32473},{\"end\":32896,\"start\":32874},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33017,\"start\":32993},{\"end\":33126,\"start\":33104},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33265,\"start\":33246},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35655,\"start\":35634},{\"end\":35702,\"start\":35670},{\"end\":39770,\"start\":39764},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":45668,\"start\":45649},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":48641,\"start\":48622},{\"end\":54962,\"start\":54937},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":58195,\"start\":58175},{\"end\":58444,\"start\":58427},{\"end\":60301,\"start\":60284},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":60325,\"start\":60301},{\"end\":60927,\"start\":60902},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":94174,\"start\":94154}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":82986,\"start\":82709},{\"attributes\":{\"id\":\"fig_3\"},\"end\":83469,\"start\":82987},{\"attributes\":{\"id\":\"fig_4\"},\"end\":83593,\"start\":83470},{\"attributes\":{\"id\":\"fig_7\"},\"end\":83654,\"start\":83594},{\"attributes\":{\"id\":\"fig_8\"},\"end\":83754,\"start\":83655},{\"attributes\":{\"id\":\"fig_9\"},\"end\":84112,\"start\":83755},{\"attributes\":{\"id\":\"fig_10\"},\"end\":84244,\"start\":84113},{\"attributes\":{\"id\":\"fig_11\"},\"end\":86133,\"start\":84245},{\"attributes\":{\"id\":\"fig_12\"},\"end\":86157,\"start\":86134},{\"attributes\":{\"id\":\"fig_13\"},\"end\":86277,\"start\":86158},{\"attributes\":{\"id\":\"fig_14\"},\"end\":86349,\"start\":86278},{\"attributes\":{\"id\":\"fig_15\"},\"end\":86426,\"start\":86350},{\"attributes\":{\"id\":\"fig_16\"},\"end\":86800,\"start\":86427},{\"attributes\":{\"id\":\"fig_17\"},\"end\":86850,\"start\":86801},{\"attributes\":{\"id\":\"fig_18\"},\"end\":86936,\"start\":86851},{\"attributes\":{\"id\":\"fig_19\"},\"end\":87010,\"start\":86937},{\"attributes\":{\"id\":\"fig_20\"},\"end\":87076,\"start\":87011},{\"attributes\":{\"id\":\"fig_21\"},\"end\":87262,\"start\":87077},{\"attributes\":{\"id\":\"fig_22\"},\"end\":87310,\"start\":87263},{\"attributes\":{\"id\":\"fig_23\"},\"end\":87377,\"start\":87311},{\"attributes\":{\"id\":\"fig_24\"},\"end\":87442,\"start\":87378},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":88324,\"start\":87443},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":88680,\"start\":88325},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":88800,\"start\":88681},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":94143,\"start\":88801},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":94714,\"start\":94144},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":94919,\"start\":94715},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":95117,\"start\":94920},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":98235,\"start\":95118},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":99268,\"start\":98236},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":101136,\"start\":99269},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":102920,\"start\":101137},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":103690,\"start\":102921},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":103990,\"start\":103691},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":104096,\"start\":103991},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":104553,\"start\":104097},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":106809,\"start\":104554},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":107522,\"start\":106810},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":110032,\"start\":107523}]", "paragraph": "[{\"end\":3412,\"start\":2327},{\"end\":3708,\"start\":3414},{\"end\":4071,\"start\":3728},{\"end\":4244,\"start\":4073},{\"end\":4930,\"start\":4246},{\"end\":5729,\"start\":4932},{\"end\":6653,\"start\":5731},{\"end\":7175,\"start\":6695},{\"end\":7308,\"start\":7177},{\"end\":7411,\"start\":7310},{\"end\":7953,\"start\":7428},{\"end\":8514,\"start\":7991},{\"end\":8545,\"start\":8542},{\"end\":8740,\"start\":8547},{\"end\":9547,\"start\":8742},{\"end\":9932,\"start\":9589},{\"end\":10361,\"start\":9959},{\"end\":10647,\"start\":10363},{\"end\":10774,\"start\":10705},{\"end\":11004,\"start\":10776},{\"end\":11454,\"start\":11034},{\"end\":12194,\"start\":11468},{\"end\":12379,\"start\":12196},{\"end\":12424,\"start\":12391},{\"end\":13383,\"start\":12426},{\"end\":13492,\"start\":13412},{\"end\":13967,\"start\":13504},{\"end\":14539,\"start\":13969},{\"end\":14896,\"start\":14541},{\"end\":15470,\"start\":14898},{\"end\":15996,\"start\":15472},{\"end\":16367,\"start\":16009},{\"end\":17043,\"start\":16369},{\"end\":17307,\"start\":17045},{\"end\":17858,\"start\":17309},{\"end\":18728,\"start\":17860},{\"end\":19705,\"start\":18730},{\"end\":20088,\"start\":19707},{\"end\":20516,\"start\":20090},{\"end\":21537,\"start\":20518},{\"end\":22049,\"start\":21539},{\"end\":22544,\"start\":22051},{\"end\":23350,\"start\":22546},{\"end\":23991,\"start\":23352},{\"end\":24607,\"start\":24008},{\"end\":25400,\"start\":24609},{\"end\":25826,\"start\":25402},{\"end\":26475,\"start\":25828},{\"end\":26957,\"start\":26477},{\"end\":27274,\"start\":26988},{\"end\":27765,\"start\":27276},{\"end\":27931,\"start\":27767},{\"end\":28117,\"start\":27933},{\"end\":28999,\"start\":28132},{\"end\":29173,\"start\":29085},{\"end\":29316,\"start\":29175},{\"end\":29433,\"start\":29349},{\"end\":29754,\"start\":29435},{\"end\":29867,\"start\":29797},{\"end\":29972,\"start\":29869},{\"end\":30078,\"start\":29999},{\"end\":30602,\"start\":30080},{\"end\":31309,\"start\":30629},{\"end\":32224,\"start\":31311},{\"end\":32800,\"start\":32226},{\"end\":33345,\"start\":32802},{\"end\":34470,\"start\":33368},{\"end\":34871,\"start\":34493},{\"end\":35568,\"start\":34873},{\"end\":36038,\"start\":35599},{\"end\":36296,\"start\":36040},{\"end\":36880,\"start\":36298},{\"end\":36930,\"start\":36882},{\"end\":36985,\"start\":36932},{\"end\":37144,\"start\":36987},{\"end\":37257,\"start\":37146},{\"end\":37455,\"start\":37263},{\"end\":37670,\"start\":37457},{\"end\":37849,\"start\":37678},{\"end\":37905,\"start\":37857},{\"end\":37936,\"start\":37907},{\"end\":38076,\"start\":37938},{\"end\":38371,\"start\":38082},{\"end\":38585,\"start\":38373},{\"end\":38869,\"start\":38587},{\"end\":38961,\"start\":38871},{\"end\":40415,\"start\":38967},{\"end\":41234,\"start\":40417},{\"end\":41286,\"start\":41236},{\"end\":41617,\"start\":41288},{\"end\":42827,\"start\":41684},{\"end\":43525,\"start\":42851},{\"end\":43680,\"start\":43611},{\"end\":43750,\"start\":43682},{\"end\":43812,\"start\":43752},{\"end\":46020,\"start\":43814},{\"end\":46663,\"start\":46073},{\"end\":46949,\"start\":46665},{\"end\":47057,\"start\":46951},{\"end\":47605,\"start\":47080},{\"end\":47809,\"start\":47620},{\"end\":48336,\"start\":47811},{\"end\":48575,\"start\":48338},{\"end\":48820,\"start\":48595},{\"end\":49003,\"start\":48822},{\"end\":49683,\"start\":49005},{\"end\":49769,\"start\":49706},{\"end\":50500,\"start\":49771},{\"end\":51379,\"start\":50502},{\"end\":51759,\"start\":51381},{\"end\":52041,\"start\":51794},{\"end\":52097,\"start\":52043},{\"end\":52435,\"start\":52099},{\"end\":52680,\"start\":52437},{\"end\":52807,\"start\":52696},{\"end\":52968,\"start\":52809},{\"end\":53478,\"start\":52970},{\"end\":54683,\"start\":53480},{\"end\":56158,\"start\":54713},{\"end\":56577,\"start\":56160},{\"end\":57158,\"start\":56589},{\"end\":57458,\"start\":57182},{\"end\":58105,\"start\":57460},{\"end\":58415,\"start\":58126},{\"end\":59034,\"start\":58417},{\"end\":59922,\"start\":59036},{\"end\":60468,\"start\":59947},{\"end\":60825,\"start\":60470},{\"end\":61098,\"start\":60827},{\"end\":61373,\"start\":61114},{\"end\":61952,\"start\":61375},{\"end\":62854,\"start\":61954},{\"end\":63597,\"start\":62856},{\"end\":63686,\"start\":63599},{\"end\":63733,\"start\":63688},{\"end\":63950,\"start\":63758},{\"end\":64307,\"start\":63952},{\"end\":64636,\"start\":64309},{\"end\":64782,\"start\":64638},{\"end\":64979,\"start\":64784},{\"end\":65152,\"start\":64981},{\"end\":65366,\"start\":65154},{\"end\":65531,\"start\":65368},{\"end\":65682,\"start\":65533},{\"end\":66283,\"start\":65684},{\"end\":66351,\"start\":66285},{\"end\":66443,\"start\":66353},{\"end\":66485,\"start\":66445},{\"end\":66543,\"start\":66487},{\"end\":66648,\"start\":66565},{\"end\":66873,\"start\":66650},{\"end\":66898,\"start\":66875},{\"end\":67723,\"start\":66927},{\"end\":68593,\"start\":67725},{\"end\":70432,\"start\":68607},{\"end\":70487,\"start\":70434},{\"end\":71076,\"start\":70550},{\"end\":71942,\"start\":71078},{\"end\":74096,\"start\":71944},{\"end\":75086,\"start\":74098},{\"end\":76004,\"start\":75088},{\"end\":76828,\"start\":76006},{\"end\":76879,\"start\":76830},{\"end\":77219,\"start\":76881},{\"end\":77577,\"start\":77221},{\"end\":78159,\"start\":77579},{\"end\":78557,\"start\":78161},{\"end\":79501,\"start\":78559},{\"end\":79540,\"start\":79503},{\"end\":80454,\"start\":79542},{\"end\":80761,\"start\":80456},{\"end\":80800,\"start\":80763},{\"end\":81397,\"start\":80802},{\"end\":81779,\"start\":81399},{\"end\":81818,\"start\":81781},{\"end\":82258,\"start\":81820},{\"end\":82708,\"start\":82260}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7427,\"start\":7412},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8541,\"start\":8515},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9588,\"start\":9548},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9958,\"start\":9933},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10704,\"start\":10648},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13411,\"start\":13384}]", "table_ref": "[{\"end\":10990,\"start\":10983},{\"end\":12013,\"start\":12006},{\"end\":13468,\"start\":13461},{\"end\":17009,\"start\":17002},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24526,\"start\":24519},{\"end\":28935,\"start\":28920},{\"end\":30248,\"start\":30241},{\"end\":31677,\"start\":31670},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":32813,\"start\":32806},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":34050,\"start\":34043},{\"end\":35033,\"start\":35026},{\"end\":40036,\"start\":40029},{\"end\":40147,\"start\":40139},{\"end\":40668,\"start\":40658},{\"end\":41454,\"start\":41446},{\"end\":41852,\"start\":41824},{\"end\":41862,\"start\":41854},{\"end\":42285,\"start\":42277},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":44810,\"start\":44802},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":45589,\"start\":45581},{\"end\":45616,\"start\":45609},{\"end\":49491,\"start\":49483},{\"end\":51392,\"start\":51384},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":56692,\"start\":56684},{\"end\":57663,\"start\":57655},{\"attributes\":{\"ref_id\":\"tab_21\"},\"end\":66282,\"start\":66274}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2325,\"start\":2313},{\"end\":3726,\"start\":3711},{\"attributes\":{\"n\":\"2\"},\"end\":6693,\"start\":6656},{\"end\":7989,\"start\":7956},{\"attributes\":{\"n\":\"3.1\"},\"end\":11032,\"start\":11007},{\"end\":11466,\"start\":11457},{\"attributes\":{\"n\":\"3.2\"},\"end\":12389,\"start\":12382},{\"attributes\":{\"n\":\"3.3\"},\"end\":13502,\"start\":13495},{\"attributes\":{\"n\":\"4\"},\"end\":16007,\"start\":15999},{\"attributes\":{\"n\":\"5\"},\"end\":24006,\"start\":23994},{\"attributes\":{\"n\":\"6\"},\"end\":26986,\"start\":26960},{\"attributes\":{\"n\":\"7\"},\"end\":28130,\"start\":28120},{\"end\":29062,\"start\":29002},{\"end\":29083,\"start\":29065},{\"end\":29347,\"start\":29319},{\"end\":29774,\"start\":29757},{\"end\":29795,\"start\":29777},{\"end\":29997,\"start\":29975},{\"end\":30627,\"start\":30605},{\"end\":33366,\"start\":33348},{\"end\":34491,\"start\":34473},{\"end\":35597,\"start\":35571},{\"end\":37261,\"start\":37260},{\"end\":37676,\"start\":37673},{\"end\":37855,\"start\":37852},{\"end\":38080,\"start\":38079},{\"end\":38965,\"start\":38964},{\"end\":41682,\"start\":41620},{\"end\":42849,\"start\":42830},{\"end\":43609,\"start\":43528},{\"end\":46057,\"start\":46023},{\"end\":46071,\"start\":46060},{\"end\":47078,\"start\":47060},{\"end\":47618,\"start\":47608},{\"end\":48593,\"start\":48578},{\"end\":49704,\"start\":49686},{\"end\":51792,\"start\":51762},{\"end\":52694,\"start\":52683},{\"end\":54711,\"start\":54686},{\"end\":56587,\"start\":56580},{\"end\":57180,\"start\":57161},{\"end\":58124,\"start\":58108},{\"end\":59945,\"start\":59925},{\"end\":61112,\"start\":61101},{\"end\":63756,\"start\":63736},{\"end\":66563,\"start\":66546},{\"end\":66925,\"start\":66901},{\"end\":68605,\"start\":68596},{\"end\":70548,\"start\":70490},{\"end\":82720,\"start\":82710},{\"end\":82998,\"start\":82988},{\"end\":83481,\"start\":83471},{\"end\":83667,\"start\":83656},{\"end\":83767,\"start\":83756},{\"end\":84125,\"start\":84114},{\"end\":84258,\"start\":84246},{\"end\":86146,\"start\":86135},{\"end\":86170,\"start\":86159},{\"end\":86290,\"start\":86279},{\"end\":86363,\"start\":86351},{\"end\":86450,\"start\":86428},{\"end\":86813,\"start\":86802},{\"end\":86863,\"start\":86852},{\"end\":86949,\"start\":86938},{\"end\":87023,\"start\":87012},{\"end\":87275,\"start\":87264},{\"end\":87323,\"start\":87312},{\"end\":87390,\"start\":87379},{\"end\":88691,\"start\":88682},{\"end\":94725,\"start\":94716},{\"end\":94928,\"start\":94921},{\"end\":103702,\"start\":103692},{\"end\":104002,\"start\":103992},{\"end\":104108,\"start\":104098}]", "table": "[{\"end\":88324,\"start\":87783},{\"end\":88680,\"start\":88503},{\"end\":88800,\"start\":88768},{\"end\":94143,\"start\":90324},{\"end\":94714,\"start\":94229},{\"end\":95117,\"start\":94948},{\"end\":98235,\"start\":96913},{\"end\":99268,\"start\":98404},{\"end\":101136,\"start\":99399},{\"end\":102920,\"start\":101599},{\"end\":103690,\"start\":102980},{\"end\":103990,\"start\":103705},{\"end\":104096,\"start\":104005},{\"end\":104553,\"start\":104110},{\"end\":106809,\"start\":106636},{\"end\":107522,\"start\":107481},{\"end\":110032,\"start\":109712}]", "figure_caption": "[{\"end\":82986,\"start\":82722},{\"end\":83469,\"start\":83000},{\"end\":83593,\"start\":83483},{\"end\":83654,\"start\":83596},{\"end\":83754,\"start\":83670},{\"end\":84112,\"start\":83770},{\"end\":84244,\"start\":84128},{\"end\":86133,\"start\":84261},{\"end\":86157,\"start\":86149},{\"end\":86277,\"start\":86173},{\"end\":86349,\"start\":86293},{\"end\":86426,\"start\":86366},{\"end\":86800,\"start\":86455},{\"end\":86850,\"start\":86816},{\"end\":86936,\"start\":86866},{\"end\":87010,\"start\":86952},{\"end\":87076,\"start\":87026},{\"end\":87262,\"start\":87079},{\"end\":87310,\"start\":87278},{\"end\":87377,\"start\":87326},{\"end\":87442,\"start\":87393},{\"end\":87783,\"start\":87445},{\"end\":88503,\"start\":88327},{\"end\":88768,\"start\":88693},{\"end\":90324,\"start\":88803},{\"end\":94229,\"start\":94146},{\"end\":94919,\"start\":94727},{\"end\":94948,\"start\":94930},{\"end\":96913,\"start\":95120},{\"end\":98404,\"start\":98238},{\"end\":99399,\"start\":99271},{\"end\":101599,\"start\":101139},{\"end\":102980,\"start\":102923},{\"end\":106636,\"start\":104556},{\"end\":107481,\"start\":106812},{\"end\":109712,\"start\":107525}]", "figure_ref": "[{\"end\":3852,\"start\":3844},{\"end\":5594,\"start\":5586},{\"end\":7112,\"start\":7104},{\"end\":7490,\"start\":7482},{\"end\":9191,\"start\":9183},{\"end\":9613,\"start\":9605},{\"end\":10824,\"start\":10816},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17930,\"start\":17922},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18457,\"start\":18449},{\"end\":20232,\"start\":20224},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23063,\"start\":23054},{\"end\":43147,\"start\":43131},{\"end\":43295,\"start\":43279},{\"end\":43448,\"start\":43440},{\"end\":43948,\"start\":43940},{\"end\":44299,\"start\":44291},{\"end\":44382,\"start\":44373},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":50743,\"start\":50734},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":50757,\"start\":50748},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":50774,\"start\":50765},{\"end\":51312,\"start\":51303},{\"end\":52235,\"start\":52226},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53477,\"start\":53468},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":54682,\"start\":54673},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":57590,\"start\":57581},{\"end\":57637,\"start\":57628},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":59920,\"start\":59911},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":62314,\"start\":62305},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":62344,\"start\":62335},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":62383,\"start\":62374},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":68591,\"start\":68582},{\"end\":68668,\"start\":68661},{\"end\":71888,\"start\":71879},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":74559,\"start\":74550},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78495,\"start\":78486}]", "bib_author_first_name": "[{\"end\":110564,\"start\":110558},{\"end\":110566,\"start\":110565},{\"end\":110904,\"start\":110898},{\"end\":110914,\"start\":110910},{\"end\":110927,\"start\":110922},{\"end\":110943,\"start\":110937},{\"end\":110956,\"start\":110952},{\"end\":110967,\"start\":110963},{\"end\":110982,\"start\":110978},{\"end\":110999,\"start\":110990},{\"end\":111010,\"start\":111006},{\"end\":111023,\"start\":111020},{\"end\":111316,\"start\":111310},{\"end\":111328,\"start\":111322},{\"end\":111347,\"start\":111339},{\"end\":111361,\"start\":111355},{\"end\":111377,\"start\":111370},{\"end\":111391,\"start\":111387},{\"end\":111403,\"start\":111399},{\"end\":111414,\"start\":111410},{\"end\":111429,\"start\":111423},{\"end\":111449,\"start\":111442},{\"end\":111859,\"start\":111858},{\"end\":111870,\"start\":111867},{\"end\":111892,\"start\":111879},{\"end\":112123,\"start\":112119},{\"end\":112149,\"start\":112142},{\"end\":112413,\"start\":112410},{\"end\":112429,\"start\":112421},{\"end\":112440,\"start\":112436},{\"end\":112455,\"start\":112448},{\"end\":112470,\"start\":112465},{\"end\":112472,\"start\":112471},{\"end\":112489,\"start\":112481},{\"end\":112506,\"start\":112500},{\"end\":112526,\"start\":112520},{\"end\":112540,\"start\":112534},{\"end\":112555,\"start\":112549},{\"end\":112572,\"start\":112564},{\"end\":112587,\"start\":112582},{\"end\":112610,\"start\":112602},{\"end\":112623,\"start\":112620},{\"end\":112639,\"start\":112634},{\"end\":112653,\"start\":112647},{\"end\":112668,\"start\":112662},{\"end\":112685,\"start\":112678},{\"end\":112697,\"start\":112690},{\"end\":112711,\"start\":112706},{\"end\":112723,\"start\":112719},{\"end\":112734,\"start\":112730},{\"end\":112750,\"start\":112743},{\"end\":113567,\"start\":113563},{\"end\":113579,\"start\":113574},{\"end\":113594,\"start\":113588},{\"end\":113606,\"start\":113600},{\"end\":113621,\"start\":113613},{\"end\":113652,\"start\":113647},{\"end\":113666,\"start\":113661},{\"end\":113680,\"start\":113676},{\"end\":113696,\"start\":113688},{\"end\":113709,\"start\":113705},{\"end\":113724,\"start\":113720},{\"end\":113734,\"start\":113730},{\"end\":113749,\"start\":113741},{\"end\":113766,\"start\":113759},{\"end\":113780,\"start\":113775},{\"end\":113795,\"start\":113789},{\"end\":113810,\"start\":113804},{\"end\":113826,\"start\":113820},{\"end\":113838,\"start\":113833},{\"end\":113849,\"start\":113845},{\"end\":113864,\"start\":113857},{\"end\":113880,\"start\":113873},{\"end\":113894,\"start\":113888},{\"end\":113911,\"start\":113903},{\"end\":113929,\"start\":113922},{\"end\":113946,\"start\":113938},{\"end\":115510,\"start\":115503},{\"end\":115526,\"start\":115519},{\"end\":115533,\"start\":115531},{\"end\":115543,\"start\":115539},{\"end\":115559,\"start\":115551},{\"end\":115567,\"start\":115564},{\"end\":115582,\"start\":115575},{\"end\":115596,\"start\":115590},{\"end\":115612,\"start\":115605},{\"end\":115627,\"start\":115621},{\"end\":115629,\"start\":115628},{\"end\":115993,\"start\":115989},{\"end\":116007,\"start\":116001},{\"end\":116026,\"start\":116018},{\"end\":116041,\"start\":116037},{\"end\":116054,\"start\":116048},{\"end\":116066,\"start\":116060},{\"end\":116083,\"start\":116075},{\"end\":116099,\"start\":116094},{\"end\":116113,\"start\":116108},{\"end\":116131,\"start\":116122},{\"end\":116452,\"start\":116446},{\"end\":116469,\"start\":116463},{\"end\":116483,\"start\":116475},{\"end\":116499,\"start\":116492},{\"end\":117087,\"start\":117082},{\"end\":117097,\"start\":117092},{\"end\":117108,\"start\":117103},{\"end\":117119,\"start\":117114},{\"end\":117136,\"start\":117128},{\"end\":117891,\"start\":117887},{\"end\":117904,\"start\":117900},{\"end\":117911,\"start\":117909},{\"end\":117924,\"start\":117919},{\"end\":117941,\"start\":117934},{\"end\":117943,\"start\":117942},{\"end\":117962,\"start\":117956},{\"end\":117977,\"start\":117972},{\"end\":117993,\"start\":117985},{\"end\":118011,\"start\":118003},{\"end\":118023,\"start\":118019},{\"end\":118033,\"start\":118029},{\"end\":118049,\"start\":118044},{\"end\":118064,\"start\":118058},{\"end\":118077,\"start\":118073},{\"end\":118079,\"start\":118078},{\"end\":118094,\"start\":118088},{\"end\":118109,\"start\":118103},{\"end\":118123,\"start\":118118},{\"end\":118138,\"start\":118134},{\"end\":118146,\"start\":118139},{\"end\":118162,\"start\":118159},{\"end\":118174,\"start\":118170},{\"end\":118176,\"start\":118175},{\"end\":118671,\"start\":118665},{\"end\":118684,\"start\":118678},{\"end\":118702,\"start\":118693},{\"end\":118710,\"start\":118707},{\"end\":118723,\"start\":118718},{\"end\":118731,\"start\":118729},{\"end\":118743,\"start\":118736},{\"end\":118755,\"start\":118751},{\"end\":118767,\"start\":118763},{\"end\":119224,\"start\":119219},{\"end\":119251,\"start\":119245},{\"end\":119266,\"start\":119262},{\"end\":119268,\"start\":119267},{\"end\":119858,\"start\":119854},{\"end\":119870,\"start\":119866},{\"end\":119883,\"start\":119878},{\"end\":119895,\"start\":119889},{\"end\":119906,\"start\":119905},{\"end\":119917,\"start\":119913},{\"end\":120229,\"start\":120223},{\"end\":120241,\"start\":120236},{\"end\":120253,\"start\":120248},{\"end\":120266,\"start\":120263},{\"end\":120281,\"start\":120274},{\"end\":120301,\"start\":120294},{\"end\":120317,\"start\":120311},{\"end\":120328,\"start\":120325},{\"end\":120340,\"start\":120335},{\"end\":120359,\"start\":120352},{\"end\":120376,\"start\":120368},{\"end\":120388,\"start\":120384},{\"end\":120405,\"start\":120398},{\"end\":120418,\"start\":120413},{\"end\":120432,\"start\":120426},{\"end\":120447,\"start\":120442},{\"end\":120466,\"start\":120457},{\"end\":121012,\"start\":121006},{\"end\":121025,\"start\":121019},{\"end\":121234,\"start\":121227},{\"end\":121254,\"start\":121245},{\"end\":121264,\"start\":121260},{\"end\":121275,\"start\":121269},{\"end\":121287,\"start\":121283},{\"end\":121533,\"start\":121526},{\"end\":121553,\"start\":121544},{\"end\":121563,\"start\":121559},{\"end\":121574,\"start\":121568},{\"end\":121586,\"start\":121582},{\"end\":121603,\"start\":121595},{\"end\":121869,\"start\":121863},{\"end\":121883,\"start\":121880},{\"end\":121889,\"start\":121884},{\"end\":121901,\"start\":121898},{\"end\":121922,\"start\":121914},{\"end\":121938,\"start\":121929},{\"end\":121949,\"start\":121944},{\"end\":122141,\"start\":122137},{\"end\":122154,\"start\":122150},{\"end\":122175,\"start\":122167},{\"end\":122188,\"start\":122183},{\"end\":122205,\"start\":122198},{\"end\":122220,\"start\":122213},{\"end\":122593,\"start\":122589},{\"end\":122606,\"start\":122602},{\"end\":122627,\"start\":122619},{\"end\":122640,\"start\":122635},{\"end\":122657,\"start\":122650},{\"end\":122672,\"start\":122665},{\"end\":122689,\"start\":122682},{\"end\":123129,\"start\":123125},{\"end\":123141,\"start\":123137},{\"end\":123156,\"start\":123150},{\"end\":123331,\"start\":123324},{\"end\":123333,\"start\":123332},{\"end\":123568,\"start\":123563},{\"end\":123583,\"start\":123579},{\"end\":123599,\"start\":123592},{\"end\":123610,\"start\":123604},{\"end\":123624,\"start\":123620},{\"end\":123638,\"start\":123631},{\"end\":123649,\"start\":123645},{\"end\":123664,\"start\":123659},{\"end\":123679,\"start\":123673},{\"end\":124159,\"start\":124152},{\"end\":124171,\"start\":124165},{\"end\":124185,\"start\":124178},{\"end\":124199,\"start\":124192},{\"end\":124215,\"start\":124207},{\"end\":124227,\"start\":124222},{\"end\":124239,\"start\":124233},{\"end\":124252,\"start\":124246},{\"end\":124622,\"start\":124617},{\"end\":124635,\"start\":124631},{\"end\":124649,\"start\":124644},{\"end\":124664,\"start\":124657},{\"end\":124682,\"start\":124676}]", "bib_author_last_name": "[{\"end\":110574,\"start\":110567},{\"end\":110908,\"start\":110905},{\"end\":110920,\"start\":110915},{\"end\":110935,\"start\":110928},{\"end\":110950,\"start\":110944},{\"end\":110961,\"start\":110957},{\"end\":110976,\"start\":110968},{\"end\":110988,\"start\":110983},{\"end\":111004,\"start\":111000},{\"end\":111018,\"start\":111011},{\"end\":111032,\"start\":111024},{\"end\":111320,\"start\":111317},{\"end\":111337,\"start\":111329},{\"end\":111353,\"start\":111348},{\"end\":111368,\"start\":111362},{\"end\":111385,\"start\":111378},{\"end\":111397,\"start\":111392},{\"end\":111408,\"start\":111404},{\"end\":111421,\"start\":111415},{\"end\":111440,\"start\":111430},{\"end\":111458,\"start\":111450},{\"end\":111865,\"start\":111860},{\"end\":111877,\"start\":111871},{\"end\":111898,\"start\":111893},{\"end\":111908,\"start\":111900},{\"end\":112140,\"start\":112124},{\"end\":112153,\"start\":112150},{\"end\":112163,\"start\":112155},{\"end\":112419,\"start\":112414},{\"end\":112434,\"start\":112430},{\"end\":112446,\"start\":112441},{\"end\":112463,\"start\":112456},{\"end\":112479,\"start\":112473},{\"end\":112498,\"start\":112490},{\"end\":112518,\"start\":112507},{\"end\":112532,\"start\":112527},{\"end\":112547,\"start\":112541},{\"end\":112562,\"start\":112556},{\"end\":112580,\"start\":112573},{\"end\":112600,\"start\":112588},{\"end\":112618,\"start\":112611},{\"end\":112632,\"start\":112624},{\"end\":112645,\"start\":112640},{\"end\":112660,\"start\":112654},{\"end\":112676,\"start\":112669},{\"end\":112688,\"start\":112686},{\"end\":112704,\"start\":112698},{\"end\":112717,\"start\":112712},{\"end\":112728,\"start\":112724},{\"end\":112741,\"start\":112735},{\"end\":112757,\"start\":112751},{\"end\":113572,\"start\":113568},{\"end\":113586,\"start\":113580},{\"end\":113598,\"start\":113595},{\"end\":113611,\"start\":113607},{\"end\":113645,\"start\":113622},{\"end\":113659,\"start\":113653},{\"end\":113674,\"start\":113667},{\"end\":113686,\"start\":113681},{\"end\":113703,\"start\":113697},{\"end\":113718,\"start\":113710},{\"end\":113728,\"start\":113725},{\"end\":113739,\"start\":113735},{\"end\":113757,\"start\":113750},{\"end\":113773,\"start\":113767},{\"end\":113787,\"start\":113781},{\"end\":113802,\"start\":113796},{\"end\":113818,\"start\":113811},{\"end\":113831,\"start\":113827},{\"end\":113843,\"start\":113839},{\"end\":113855,\"start\":113850},{\"end\":113871,\"start\":113865},{\"end\":113886,\"start\":113881},{\"end\":113901,\"start\":113895},{\"end\":113920,\"start\":113912},{\"end\":113936,\"start\":113930},{\"end\":113953,\"start\":113947},{\"end\":115517,\"start\":115511},{\"end\":115529,\"start\":115527},{\"end\":115537,\"start\":115534},{\"end\":115549,\"start\":115544},{\"end\":115562,\"start\":115560},{\"end\":115573,\"start\":115568},{\"end\":115588,\"start\":115583},{\"end\":115603,\"start\":115597},{\"end\":115619,\"start\":115613},{\"end\":115638,\"start\":115630},{\"end\":115999,\"start\":115994},{\"end\":116016,\"start\":116008},{\"end\":116035,\"start\":116027},{\"end\":116046,\"start\":116042},{\"end\":116058,\"start\":116055},{\"end\":116073,\"start\":116067},{\"end\":116092,\"start\":116084},{\"end\":116106,\"start\":116100},{\"end\":116120,\"start\":116114},{\"end\":116138,\"start\":116132},{\"end\":116461,\"start\":116453},{\"end\":116473,\"start\":116470},{\"end\":116490,\"start\":116484},{\"end\":116503,\"start\":116500},{\"end\":117090,\"start\":117088},{\"end\":117101,\"start\":117098},{\"end\":117112,\"start\":117109},{\"end\":117126,\"start\":117120},{\"end\":117142,\"start\":117137},{\"end\":117148,\"start\":117144},{\"end\":117744,\"start\":117738},{\"end\":117898,\"start\":117892},{\"end\":117907,\"start\":117905},{\"end\":117917,\"start\":117912},{\"end\":117932,\"start\":117925},{\"end\":117954,\"start\":117944},{\"end\":117970,\"start\":117963},{\"end\":117983,\"start\":117978},{\"end\":118001,\"start\":117994},{\"end\":118017,\"start\":118012},{\"end\":118027,\"start\":118024},{\"end\":118042,\"start\":118034},{\"end\":118056,\"start\":118050},{\"end\":118071,\"start\":118065},{\"end\":118086,\"start\":118080},{\"end\":118101,\"start\":118095},{\"end\":118116,\"start\":118110},{\"end\":118132,\"start\":118124},{\"end\":118157,\"start\":118147},{\"end\":118168,\"start\":118163},{\"end\":118181,\"start\":118177},{\"end\":118676,\"start\":118672},{\"end\":118691,\"start\":118685},{\"end\":118705,\"start\":118703},{\"end\":118716,\"start\":118711},{\"end\":118727,\"start\":118724},{\"end\":118734,\"start\":118732},{\"end\":118749,\"start\":118744},{\"end\":118761,\"start\":118756},{\"end\":118770,\"start\":118768},{\"end\":119243,\"start\":119225},{\"end\":119260,\"start\":119252},{\"end\":119282,\"start\":119269},{\"end\":119289,\"start\":119284},{\"end\":119864,\"start\":119859},{\"end\":119876,\"start\":119871},{\"end\":119887,\"start\":119884},{\"end\":119903,\"start\":119896},{\"end\":119911,\"start\":119907},{\"end\":119923,\"start\":119918},{\"end\":119930,\"start\":119925},{\"end\":120234,\"start\":120230},{\"end\":120246,\"start\":120242},{\"end\":120261,\"start\":120254},{\"end\":120272,\"start\":120267},{\"end\":120292,\"start\":120282},{\"end\":120309,\"start\":120302},{\"end\":120323,\"start\":120318},{\"end\":120333,\"start\":120329},{\"end\":120350,\"start\":120341},{\"end\":120366,\"start\":120360},{\"end\":120382,\"start\":120377},{\"end\":120396,\"start\":120389},{\"end\":120411,\"start\":120406},{\"end\":120424,\"start\":120419},{\"end\":120440,\"start\":120433},{\"end\":120455,\"start\":120448},{\"end\":120472,\"start\":120467},{\"end\":121017,\"start\":121013},{\"end\":121032,\"start\":121026},{\"end\":121243,\"start\":121235},{\"end\":121258,\"start\":121255},{\"end\":121267,\"start\":121265},{\"end\":121281,\"start\":121276},{\"end\":121294,\"start\":121288},{\"end\":121542,\"start\":121534},{\"end\":121557,\"start\":121554},{\"end\":121566,\"start\":121564},{\"end\":121580,\"start\":121575},{\"end\":121593,\"start\":121587},{\"end\":121608,\"start\":121604},{\"end\":121878,\"start\":121870},{\"end\":121896,\"start\":121890},{\"end\":121912,\"start\":121902},{\"end\":121927,\"start\":121923},{\"end\":121942,\"start\":121939},{\"end\":121955,\"start\":121950},{\"end\":122148,\"start\":122142},{\"end\":122165,\"start\":122155},{\"end\":122181,\"start\":122176},{\"end\":122196,\"start\":122189},{\"end\":122211,\"start\":122206},{\"end\":122228,\"start\":122221},{\"end\":122600,\"start\":122594},{\"end\":122617,\"start\":122607},{\"end\":122633,\"start\":122628},{\"end\":122648,\"start\":122641},{\"end\":122663,\"start\":122658},{\"end\":122680,\"start\":122673},{\"end\":122693,\"start\":122690},{\"end\":123135,\"start\":123130},{\"end\":123148,\"start\":123142},{\"end\":123165,\"start\":123157},{\"end\":123339,\"start\":123334},{\"end\":123577,\"start\":123569},{\"end\":123590,\"start\":123584},{\"end\":123602,\"start\":123600},{\"end\":123618,\"start\":123611},{\"end\":123629,\"start\":123625},{\"end\":123643,\"start\":123639},{\"end\":123657,\"start\":123650},{\"end\":123671,\"start\":123665},{\"end\":123690,\"start\":123680},{\"end\":124163,\"start\":124160},{\"end\":124176,\"start\":124172},{\"end\":124190,\"start\":124186},{\"end\":124205,\"start\":124200},{\"end\":124220,\"start\":124216},{\"end\":124231,\"start\":124228},{\"end\":124244,\"start\":124240},{\"end\":124256,\"start\":124253},{\"end\":124629,\"start\":124623},{\"end\":124642,\"start\":124636},{\"end\":124655,\"start\":124650},{\"end\":124674,\"start\":124665},{\"end\":124687,\"start\":124683}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1007/978-1-4612-5533-8_4\",\"id\":\"b0\",\"matched_paper_id\":123635372},\"end\":110805,\"start\":110533},{\"attributes\":{\"doi\":\"ArXiv:2204.05862\",\"id\":\"b1\"},\"end\":111308,\"start\":110807},{\"attributes\":{\"doi\":\"arXiv:2212.08073\",\"id\":\"b2\"},\"end\":111786,\"start\":111310},{\"attributes\":{\"doi\":\"abs/2212.07597\",\"id\":\"b3\"},\"end\":112070,\"start\":111788},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7039587},\"end\":112329,\"start\":112072},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":218971783},\"end\":113561,\"start\":112331},{\"attributes\":{\"doi\":\"arXiv:2107.03374\",\"id\":\"b6\"},\"end\":115501,\"start\":113563},{\"attributes\":{\"id\":\"b7\"},\"end\":115939,\"start\":115503},{\"attributes\":{\"doi\":\"arXiv:2110.14168\",\"id\":\"b8\"},\"end\":116414,\"start\":115941},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b9\",\"matched_paper_id\":157059801},\"end\":116988,\"start\":116416},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":248069330},\"end\":117707,\"start\":116990},{\"attributes\":{\"id\":\"b11\"},\"end\":117806,\"start\":117709},{\"attributes\":{\"id\":\"b12\"},\"end\":117885,\"start\":117808},{\"attributes\":{\"doi\":\"ArXiv:2203.02155\",\"id\":\"b13\"},\"end\":118663,\"start\":117887},{\"attributes\":{\"id\":\"b14\"},\"end\":119176,\"start\":118665},{\"attributes\":{\"doi\":\"10.18653/v1/P18-1080\",\"id\":\"b15\",\"matched_paper_id\":13959787},\"end\":119852,\"start\":119178},{\"attributes\":{\"doi\":\"arXiv:2210.03350\",\"id\":\"b16\"},\"end\":120221,\"start\":119854},{\"attributes\":{\"doi\":\"arXiv:2105.12655\",\"id\":\"b17\"},\"end\":120967,\"start\":120223},{\"attributes\":{\"doi\":\"arXiv:2205.12374\",\"id\":\"b18\"},\"end\":121170,\"start\":120969},{\"attributes\":{\"id\":\"b19\"},\"end\":121469,\"start\":121172},{\"attributes\":{\"doi\":\"ArXiv:2206.05802\",\"id\":\"b20\"},\"end\":121804,\"start\":121471},{\"attributes\":{\"id\":\"b21\"},\"end\":122135,\"start\":121806},{\"attributes\":{\"doi\":\"10.48550/ARXIV.2208.11663\",\"id\":\"b22\"},\"end\":122587,\"start\":122137},{\"attributes\":{\"doi\":\"abs/2208.11663\",\"id\":\"b23\"},\"end\":123051,\"start\":122589},{\"attributes\":{\"id\":\"b24\"},\"end\":123290,\"start\":123053},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2030105},\"end\":123518,\"start\":123292},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":221665105},\"end\":124054,\"start\":123520},{\"attributes\":{\"doi\":\"arXiv:2305.03047\",\"id\":\"b27\"},\"end\":124532,\"start\":124056},{\"attributes\":{\"doi\":\"arXiv:2112.07867\",\"id\":\"b28\"},\"end\":124916,\"start\":124534}]", "bib_title": "[{\"end\":110556,\"start\":110533},{\"end\":112117,\"start\":112072},{\"end\":112408,\"start\":112331},{\"end\":116444,\"start\":116416},{\"end\":117080,\"start\":116990},{\"end\":119217,\"start\":119178},{\"end\":123322,\"start\":123292},{\"end\":123561,\"start\":123520}]", "bib_author": "[{\"end\":110576,\"start\":110558},{\"end\":110910,\"start\":110898},{\"end\":110922,\"start\":110910},{\"end\":110937,\"start\":110922},{\"end\":110952,\"start\":110937},{\"end\":110963,\"start\":110952},{\"end\":110978,\"start\":110963},{\"end\":110990,\"start\":110978},{\"end\":111006,\"start\":110990},{\"end\":111020,\"start\":111006},{\"end\":111034,\"start\":111020},{\"end\":111322,\"start\":111310},{\"end\":111339,\"start\":111322},{\"end\":111355,\"start\":111339},{\"end\":111370,\"start\":111355},{\"end\":111387,\"start\":111370},{\"end\":111399,\"start\":111387},{\"end\":111410,\"start\":111399},{\"end\":111423,\"start\":111410},{\"end\":111442,\"start\":111423},{\"end\":111460,\"start\":111442},{\"end\":111867,\"start\":111858},{\"end\":111879,\"start\":111867},{\"end\":111900,\"start\":111879},{\"end\":111910,\"start\":111900},{\"end\":112142,\"start\":112119},{\"end\":112155,\"start\":112142},{\"end\":112165,\"start\":112155},{\"end\":112421,\"start\":112410},{\"end\":112436,\"start\":112421},{\"end\":112448,\"start\":112436},{\"end\":112465,\"start\":112448},{\"end\":112481,\"start\":112465},{\"end\":112500,\"start\":112481},{\"end\":112520,\"start\":112500},{\"end\":112534,\"start\":112520},{\"end\":112549,\"start\":112534},{\"end\":112564,\"start\":112549},{\"end\":112582,\"start\":112564},{\"end\":112602,\"start\":112582},{\"end\":112620,\"start\":112602},{\"end\":112634,\"start\":112620},{\"end\":112647,\"start\":112634},{\"end\":112662,\"start\":112647},{\"end\":112678,\"start\":112662},{\"end\":112690,\"start\":112678},{\"end\":112706,\"start\":112690},{\"end\":112719,\"start\":112706},{\"end\":112730,\"start\":112719},{\"end\":112743,\"start\":112730},{\"end\":112759,\"start\":112743},{\"end\":113574,\"start\":113563},{\"end\":113588,\"start\":113574},{\"end\":113600,\"start\":113588},{\"end\":113613,\"start\":113600},{\"end\":113647,\"start\":113613},{\"end\":113661,\"start\":113647},{\"end\":113676,\"start\":113661},{\"end\":113688,\"start\":113676},{\"end\":113705,\"start\":113688},{\"end\":113720,\"start\":113705},{\"end\":113730,\"start\":113720},{\"end\":113741,\"start\":113730},{\"end\":113759,\"start\":113741},{\"end\":113775,\"start\":113759},{\"end\":113789,\"start\":113775},{\"end\":113804,\"start\":113789},{\"end\":113820,\"start\":113804},{\"end\":113833,\"start\":113820},{\"end\":113845,\"start\":113833},{\"end\":113857,\"start\":113845},{\"end\":113873,\"start\":113857},{\"end\":113888,\"start\":113873},{\"end\":113903,\"start\":113888},{\"end\":113922,\"start\":113903},{\"end\":113938,\"start\":113922},{\"end\":113955,\"start\":113938},{\"end\":115519,\"start\":115503},{\"end\":115531,\"start\":115519},{\"end\":115539,\"start\":115531},{\"end\":115551,\"start\":115539},{\"end\":115564,\"start\":115551},{\"end\":115575,\"start\":115564},{\"end\":115590,\"start\":115575},{\"end\":115605,\"start\":115590},{\"end\":115621,\"start\":115605},{\"end\":115640,\"start\":115621},{\"end\":116001,\"start\":115989},{\"end\":116018,\"start\":116001},{\"end\":116037,\"start\":116018},{\"end\":116048,\"start\":116037},{\"end\":116060,\"start\":116048},{\"end\":116075,\"start\":116060},{\"end\":116094,\"start\":116075},{\"end\":116108,\"start\":116094},{\"end\":116122,\"start\":116108},{\"end\":116140,\"start\":116122},{\"end\":116463,\"start\":116446},{\"end\":116475,\"start\":116463},{\"end\":116492,\"start\":116475},{\"end\":116505,\"start\":116492},{\"end\":117092,\"start\":117082},{\"end\":117103,\"start\":117092},{\"end\":117114,\"start\":117103},{\"end\":117128,\"start\":117114},{\"end\":117144,\"start\":117128},{\"end\":117150,\"start\":117144},{\"end\":117746,\"start\":117738},{\"end\":117900,\"start\":117887},{\"end\":117909,\"start\":117900},{\"end\":117919,\"start\":117909},{\"end\":117934,\"start\":117919},{\"end\":117956,\"start\":117934},{\"end\":117972,\"start\":117956},{\"end\":117985,\"start\":117972},{\"end\":118003,\"start\":117985},{\"end\":118019,\"start\":118003},{\"end\":118029,\"start\":118019},{\"end\":118044,\"start\":118029},{\"end\":118058,\"start\":118044},{\"end\":118073,\"start\":118058},{\"end\":118088,\"start\":118073},{\"end\":118103,\"start\":118088},{\"end\":118118,\"start\":118103},{\"end\":118134,\"start\":118118},{\"end\":118159,\"start\":118134},{\"end\":118170,\"start\":118159},{\"end\":118183,\"start\":118170},{\"end\":118678,\"start\":118665},{\"end\":118693,\"start\":118678},{\"end\":118707,\"start\":118693},{\"end\":118718,\"start\":118707},{\"end\":118729,\"start\":118718},{\"end\":118736,\"start\":118729},{\"end\":118751,\"start\":118736},{\"end\":118763,\"start\":118751},{\"end\":118772,\"start\":118763},{\"end\":119245,\"start\":119219},{\"end\":119262,\"start\":119245},{\"end\":119284,\"start\":119262},{\"end\":119291,\"start\":119284},{\"end\":119866,\"start\":119854},{\"end\":119878,\"start\":119866},{\"end\":119889,\"start\":119878},{\"end\":119905,\"start\":119889},{\"end\":119913,\"start\":119905},{\"end\":119925,\"start\":119913},{\"end\":119932,\"start\":119925},{\"end\":120236,\"start\":120223},{\"end\":120248,\"start\":120236},{\"end\":120263,\"start\":120248},{\"end\":120274,\"start\":120263},{\"end\":120294,\"start\":120274},{\"end\":120311,\"start\":120294},{\"end\":120325,\"start\":120311},{\"end\":120335,\"start\":120325},{\"end\":120352,\"start\":120335},{\"end\":120368,\"start\":120352},{\"end\":120384,\"start\":120368},{\"end\":120398,\"start\":120384},{\"end\":120413,\"start\":120398},{\"end\":120426,\"start\":120413},{\"end\":120442,\"start\":120426},{\"end\":120457,\"start\":120442},{\"end\":120474,\"start\":120457},{\"end\":121019,\"start\":121006},{\"end\":121034,\"start\":121019},{\"end\":121245,\"start\":121227},{\"end\":121260,\"start\":121245},{\"end\":121269,\"start\":121260},{\"end\":121283,\"start\":121269},{\"end\":121296,\"start\":121283},{\"end\":121544,\"start\":121526},{\"end\":121559,\"start\":121544},{\"end\":121568,\"start\":121559},{\"end\":121582,\"start\":121568},{\"end\":121595,\"start\":121582},{\"end\":121610,\"start\":121595},{\"end\":121880,\"start\":121863},{\"end\":121898,\"start\":121880},{\"end\":121914,\"start\":121898},{\"end\":121929,\"start\":121914},{\"end\":121944,\"start\":121929},{\"end\":121957,\"start\":121944},{\"end\":122150,\"start\":122137},{\"end\":122167,\"start\":122150},{\"end\":122183,\"start\":122167},{\"end\":122198,\"start\":122183},{\"end\":122213,\"start\":122198},{\"end\":122230,\"start\":122213},{\"end\":122602,\"start\":122589},{\"end\":122619,\"start\":122602},{\"end\":122635,\"start\":122619},{\"end\":122650,\"start\":122635},{\"end\":122665,\"start\":122650},{\"end\":122682,\"start\":122665},{\"end\":122695,\"start\":122682},{\"end\":123137,\"start\":123125},{\"end\":123150,\"start\":123137},{\"end\":123167,\"start\":123150},{\"end\":123341,\"start\":123324},{\"end\":123579,\"start\":123563},{\"end\":123592,\"start\":123579},{\"end\":123604,\"start\":123592},{\"end\":123620,\"start\":123604},{\"end\":123631,\"start\":123620},{\"end\":123645,\"start\":123631},{\"end\":123659,\"start\":123645},{\"end\":123673,\"start\":123659},{\"end\":123692,\"start\":123673},{\"end\":124165,\"start\":124152},{\"end\":124178,\"start\":124165},{\"end\":124192,\"start\":124178},{\"end\":124207,\"start\":124192},{\"end\":124222,\"start\":124207},{\"end\":124233,\"start\":124222},{\"end\":124246,\"start\":124233},{\"end\":124258,\"start\":124246},{\"end\":124631,\"start\":124617},{\"end\":124644,\"start\":124631},{\"end\":124657,\"start\":124644},{\"end\":124676,\"start\":124657},{\"end\":124689,\"start\":124676}]", "bib_venue": "[{\"end\":110638,\"start\":110603},{\"end\":110896,\"start\":110807},{\"end\":111524,\"start\":111476},{\"end\":111856,\"start\":111788},{\"end\":112184,\"start\":112165},{\"end\":112808,\"start\":112759},{\"end\":114063,\"start\":113971},{\"end\":115713,\"start\":115640},{\"end\":115987,\"start\":115941},{\"end\":116588,\"start\":116509},{\"end\":117233,\"start\":117150},{\"end\":117736,\"start\":117709},{\"end\":117846,\"start\":117810},{\"end\":118266,\"start\":118199},{\"end\":118919,\"start\":118772},{\"end\":119398,\"start\":119311},{\"end\":120015,\"start\":119948},{\"end\":120573,\"start\":120490},{\"end\":121004,\"start\":120969},{\"end\":121225,\"start\":121172},{\"end\":121524,\"start\":121471},{\"end\":121861,\"start\":121806},{\"end\":122373,\"start\":122255},{\"end\":122814,\"start\":122709},{\"end\":123123,\"start\":123053},{\"end\":123390,\"start\":123341},{\"end\":123741,\"start\":123692},{\"end\":124150,\"start\":124056},{\"end\":124615,\"start\":124534},{\"end\":110662,\"start\":110640},{\"end\":112906,\"start\":112810},{\"end\":114504,\"start\":114213},{\"end\":116681,\"start\":116590},{\"end\":117318,\"start\":117235},{\"end\":119492,\"start\":119400}]"}}}, "year": 2023, "month": 12, "day": 17}