{"id": 251569932, "updated": "2023-04-01 13:15:00.649", "metadata": {"title": "FLAG: A Feedback-aware Local and Global Model for Heterogeneous Sequential Recommendation", "authors": "[{\"first\":\"Mingkai\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Jing\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Jinwei\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Weike\",\"last\":\"Pan\",\"middle\":[]},{\"first\":\"Zhong\",\"last\":\"Ming\",\"middle\":[]}]", "venue": "ACM Transactions on Intelligent Systems and Technology", "journal": "ACM Transactions on Intelligent Systems and Technology", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Heterogeneous sequential recommendation that models sequences of items associated with more than one type of feedback such as examinations and purchases is an emerging topic in the research community, which is also an important problem in many real-world applications. Though there are some methods proposed to exploit different types of feedback in item sequences such as RLBL, RIB, and BINN, they are based on RNN and may not be very competitive in capturing users\u2019 complex and dynamic preferences. And most existing advanced sequential recommendation methods such as the CNN- and attention-based methods are often designed for making use of item sequences with one single type of feedback, which thus can not be applied to the studied problem directly. As a response, we propose a novel feedback-aware local and global (FLAG) preference learning model for heterogeneous sequential recommendation. Our FLAG contains four modules, including (i) a local preference learning module for capturing a user\u2019s short-term interest, which adopts a novel feedback-aware self-attention block to distinguish different types of feedback; (ii) a global preference learning module for modeling a user\u2019s global preference; (iii) a local intention learning module, which takes a user\u2019s real feedback in the next step, i.e., the user\u2019s intention at the current step, as the query vector in a self-attention block to figure out the items that match the user\u2019s intention well; and (iv) a prediction module for preference integration and final prediction. We then conduct extensive experiments on three public datasets and find that our FLAG significantly outperforms 13 very competitive baselines in terms of two commonly used ranking-oriented metrics in most cases. We also include ablation studies and sensitivity analysis of our FLAG to have more in-depth insights.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tist/HeLLPM23", "doi": "10.1145/3557046"}}, "content": {"source": {"pdf_hash": "ae2ba20aa91351c5be49b66b29b6ec931599813b", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "36aeabd5ddd79279d47d02292ff1d91f4516374c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ae2ba20aa91351c5be49b66b29b6ec931599813b.txt", "contents": "\nFLAG: A Feedback-aware Local and Global Model for Heterogeneous Sequential Recommendation\nNovember 2022\n\nMingkai He \nJing Lin \nJinwei Luo luojinwei2016@email.szu.edu.cn \nWeike Pan \nZhong Ming mingz@szu.edu.cn. \n\nCollege of Computer Science and Software Engineering and National Engineering Laboratory for Big Data System Computing Technology\nCollege of Com-puter Science and Software Engineering and National Engineering Laboratory for Big Data System Computing Technology\nShenzhen University\nChina\n\n\nShenzhen University\n3688# Nanhai Avenue, Nanshan District, China; emails: {1910273003, lin-jing2018518060ShenzhenChina\n\nFLAG: A Feedback-aware Local and Global Model for Heterogeneous Sequential Recommendation\n\nACM Transactions on Intelligent Systems and Technology\n14114November 202210.1145/3557046Publication date: November 2022.14 ACM Reference format: Mingkai He, Jing Lin, Jinwei Luo, Weike Pan, and Zhong Ming. 2022. FLAG: A Feedback-aware Local and Global Model for Heterogeneous Sequential Recommendation. Supported by the National Natural Science Foundation of China No. 62172283 and No. 61836005. Authors' address: M. He, J. Lin, J. Luo, W. Pan (corresponding author), and Z. Ming (corresponding author), 14:2 M. He et al.CCS Concepts: \u2022 Information systems \u2192 Personalization\u2022 Human-centered computing \u2192 Collabo- rative filteringAdditional Key Words and Phrases: Heterogeneous sequential recommendation, self-attention, local intention, local preference, global preference\nHeterogeneous sequential recommendation that models sequences of items associated with more than one type of feedback such as examinations and purchases is an emerging topic in the research community, which is also an important problem in many real-world applications. Though there are some methods proposed to exploit different types of feedback in item sequences such as RLBL, RIB, and BINN, they are based on RNN and may not be very competitive in capturing users' complex and dynamic preferences. And most existing advanced sequential recommendation methods such as the CNN-and attention-based methods are often designed for making use of item sequences with one single type of feedback, which thus can not be applied to the studied problem directly. As a response, we propose a novel feedback-aware local and global (FLAG) preference learning model for heterogeneous sequential recommendation. Our FLAG contains four modules, including (i) a local preference learning module for capturing a user's short-term interest, which adopts a novel feedback-aware self-attention block to distinguish different types of feedback; (ii) a global preference learning module for modeling a user's global preference; (iii) a local intention learning module, which takes a user's real feedback in the next step, i.e., the user's intention at the current step, as the query vector in a selfattention block to figure out the items that match the user's intention well; and (iv) a prediction module for preference integration and final prediction. We then conduct extensive experiments on three public datasets and find that our FLAG significantly outperforms 13 very competitive baselines in terms of two commonly used ranking-oriented metrics in most cases. We also include ablation studies and sensitivity analysis of our FLAG to have more in-depth insights.\n\nINTRODUCTION\n\nIn a real recommendation system, there are often different types of feedback, such as examinations, purchases, forwards, and so on ]. The purchase feedback is a kind of behavior reflecting a strong preference of a user, while the examination feedback is usually the most common and richest kind of behavior. Both types of feedback can be used to model a user's preference and interest, so some recent recommendation methods turn to these two parts of data to better characterize a user. As a hot research topic, sequential recommendation exploits a user's historical interaction sequence, aiming to predict the item that the user will interact with in the next step. However, most sequential recommendation methods cannot handle heterogeneous sequences with more than one type of feedback, which makes the model unable to exploit the additional useful information. Heterogeneous sequential recommendation is an emerging and important problem, which is more in line with many real-world applications consisting of two or more different types of feedback.\n\nMost previous recommendation methods only model one single type of feedback. For example, the typical matrix factorization (MF) algorithm [Rendle et al. 2009] uses the inner product of a user-specific latent vector and an item-specific latent vector to predict the user's preference toward the item. With the development of deep learning, more advanced sequential recommendation models have been proposed. For example, RNN-based methods [Hidasi and Karatzoglou 2018;Quadrana et al. 2017] are very suitable for modeling an item sequence for its self-recursive nature. GRU4Rec [Hidasi and Karatzoglou 2018] uses GRU to model the item sequence in a natural way. Later, a CNN-based method, Caser [Tang and Wang 2018], uses some vertical convolution filters and horizontal convolution filters to extract sequential features from the embedding matrix of the item sequence. Recently, an attention-based method, SASRec [Kang and McAuley 2018], introduces self-attention blocks to directly model the correlation between any two items in the sequence. At the same time, the causality mask is used to avoid the introduction of future information. Very recently, FISSA , an improved version based on SASRec, uses a global preference representation learning module to obtain a user's global preference and a gating module to balance the local and global preferences by taking the information of each candidate item into account. However, all of the above methods belong to homogeneous recommendation models and cannot make full use of different types of feedback in heterogeneous sequential recommendation.\n\nFor heterogeneous sequential recommendation, RLBL [Liu et al. 2017] combines an RNN module and a log-bilinear (LBL) module and introduces a feedback-specific transition matrix to capture the impact of different types of feedback. RIB [Zhou et al. 2018] uses some additional feedbackspecific embedding in the input of RNN to distinguish different types of feedback. BINN ] proposes a contextual LSTM (CLSTM), which integrates the modeling of different types of feedback into the LSTM module. However, for heterogeneous sequential recommendation, there are some specific challenges that are not well addressed by the aforementioned existing methods. (i) The heterogeneity of feedback. It is important for a heterogeneous sequential recommendation model to distinguish different types of feedback in historical interaction sequences. (ii) The uncertainty of user intention. In homogeneous sequential recommendation, a model trained with historical purchase sequences can predict the likely-to-purchase items in the next step. However, in a sequence with both examination feedback and purchase feedback, predicting the next item for the real next feedback is difficult at each step. (iii) The complexity of user preferences. How to make the heterogeneous sequential recommendation model better distinguish and utilize the user's local and global preferences is also an important question.\n\nTo address the above three challenges, we design a novel solution named feedback-aware local and global (FLAG) model. Our FLAG contains four modules, namely, a local preference learning FLAG 14:3 module, a global preference learning module, a local intention learning module, and a prediction module. In the local preference learning module, we design a feedback-aware self-attention block (FSAB), and then stack FSABs to capture the local preference, which enables the model to distinguish different types of feedback. In the global preference learning module, a learnable query vector shared by all the sequences is used in a location-based attention layer (LAL) to identify the most representative items, i.e., to capture the global preference [Luong et al. 2015]. Similar to LAL, the local intention learning module uses the embedding of the real next feedback, which indicates the intention of the user at a current step, as the query vector to figure out the items related to the current intention. In the prediction module, we first use an item similarity gating function to aggregate the local and global preferences in balance and then combine the intention and the preference via an element-wise addition operation for final prediction.\n\nWe summarize our main contributions as follows: (i) We propose a novel feedback-aware local and global (FLAG) model for heterogeneous sequential recommendation. (ii) We design a feedbackaware self-attention block (FSAB) in the local preference learning module, which enables our FLAG to distinguish different types of feedback in heterogeneous sequences. (iii) We design a local intention learning module to capture a user's intention, which can be seen as a task-specific module for prediction tasks of a next examined item or purchased item, so the model can make more targeted predictions for different tasks. (iv) We conduct extensive empirical studies on three public datasets and find that our FLAG achieves the best performance in comparing with 13 methods in most cases and surpasses two most representative and competitive baselines, FISSA and BINN, by 14.64% and 20.20% on average in terms of Rec@10, respectively.\n\n\nRELATED WORK\n\nIn this section, we review the typical methods in non-sequential recommendation, homogeneous sequential recommendation, and heterogeneous sequential recommendation and point out the relationship and difference between our FLAG and those works.\n\n\nNon-sequential Recommendation\n\nIn a real e-commerce scenario, different types of feedback generated by a user on an item often implicitly reflects the user's different degrees of preference for an item. Users usually have different types of feedback on an item, such as examinations, purchases, forwards, and so on.\n\nThere are many non-sequential recommendation methods for modeling one single type of feedback, and the most typical method is matrix factorization (MF)-based methods. In an MF-based method [Rendle et al. 2009], the inner product between a user-specific latent vector and an itemspecific latent vector is regarded as the user's preference for the item. The user-specific latent vector of a same user can be trained in multiple user-item pairs, and the same is true for items, so the model can make more reasonable predictions for user-item pairs that have not been seen before. To characterize users' interest more accurately, FISM [Kabbur et al. 2013] uses average pooling to aggregate the latent vectors of items that a user has interacted with instead of the user-specific latent vector. At the same time, the FISM model only needs to train and save the item-specific latent matrix. FISM treats the items that the user has interacted with in history equally when modeling the user's interest. However, when predicting the user's preference for different items, historical items should have different contributions. To solve this problem, NAIS  aggregates the embeddings of a user's historical items by a weighted sum to obtain a more accurate user's representation, where the weight of each item is obtained via multi-layer perceptron (MLP), which models the relationship between the target item and the user's historical items. Most of the MF-based methods are linear methods, while the deep learning-based method NCF [He et al. 2017] uses a multi-layer neural network to model the complex nonlinear relationship between the user embedding and the item embedding.\n\nDue to the sparsity of the purchase feedback, the examination feedback is considered as an auxiliary feedback to help the model better predict the next likely-to-purchase items for a user ]. On the basis of BPR [Rendle et al. 2009], ABPR [Pan et al. 2015] believes that users should score higher for items with an examination feedback, compared to items that have not been interacted at all. Due to the use of the additional feedback data, ABPR effectively alleviates the sparsity of the purchase feedback. MF-BPR [Loni et al. 2016] maps different types of feedback onto different levels, where a higher level reflects the higher preference strengths. MF-BPR proposes a non-uniform sampler to obtain a positive example with different types of feedback, while the negative example is collected from a lower level compared with the positive one. VALS ] treats examination as intermediate feedback between the purchase feedback and the missing feedback. It introduces a fixed margin in the partial order relationship between the target feedback (i.e., purchase), the auxiliary feedback (i.e., examination), and the missing feedback, and trains the entire model by constructing a margin-based loss to obtain more reasonable predictions. NMTR  explores the relationship between different behaviors and points out that the predicted value of the feedback for the item depends on the prediction of the previous feedback of the item. Similar to NMTR, EHCF ] learns a separate prediction layer for each type of feedback and believes that each feedback depends on its precedent feedback to capture the transfer features, so the sparse feedback can also be better trained through the dense feedback. However, none of the above methods consider the sequential information in users' feedback. Since the user's interest tends to change over time, it is necessary to take the sequential information in the users' historical feedback into consideration.\n\n\nHomogeneous Sequential Recommendation\n\nMost sequential recommendation methods model a sequence with one single type of feedback, such as a purchase sequence. This type of method is called homogeneous sequential recommendation. FPMC [Rendle et al. 2010] introduces sequential information based on traditional matrix factorization and models the first-order Markov chains (MCs) in a factorization way. On the basis of FPMC, Fossil [He and McAuley 2016] comprehensively considers multiple items that the user has recently interacted with to make predictions on items. With the development of deep learning, more methods suitable for modeling the sequential information have been proposed. GRU4Rec [Hidasi and Karatzoglou 2018] uses gated recurrent unit (GRU) for sequential recommendation for the first time. The excellent gating structure in GRU alleviates the disappearance of the gradient of RNN. However, since GRU4Rec needs the features of a previous step as input at each step, the entire GRU4Rec model cannot be trained in parallel, which makes the training efficiency low. Caser [Tang and Wang 2018] introduces the convolution structure of CNN and proposes a novel way of modeling the sequential features using some horizontal convolution filters and vertical convolution filters. Since the convolution operation can be performed in parallel, the training efficiency is greatly improved. The attention mechanism [Li et al. 2017] is also applied to sequential recommendation, and it has been shown effective in many real applications ]. SASRec [Kang and McAuley 2018] uses a stacked structure of self-attention blocks (SAB) to capture the local preference, which directly models the relationship between any two items in the sequence, so it can well capture long-range dependencies. FISSA , an improved version of SASRec, introduces a location-based layer to model the global preference and an item similarity gating module to balance the local and global preferences. Similar to FISSA, CAR [He et al. 2020] takes the issue of list consistency into consideration and uses a consistency-aware gating network to determine whether the user's interest has changed. There is also a kind of sequential recommendation method that considers some side information to learn users' finegrained preferences. FDSA [Zhang et al. 2019] takes the corresponding information of the items in a historical item sequence as the feature sequence, which is modeled by a vanilla self-attention layer. NOVA [Liu et al. 2021b] regards some side information as the query and key of a selfattention layer and takes pure item embedding as the value, so the side information will not interfere the feature space of items. This type of method considers how to introduce the modeling of side information and usually does not distinguish the types of side information. Notice that although feedback can be viewed as one kind of side information, the introduction of multiple types of feedback actually means introduction of more data (such as examination, adding-to-cart, adding-to-favorite, etc.), i.e., heterogeneous sequences. We believe that a heterogeneous sequence contains richer information than a homogeneous purchase sequence. In addition, feedback can be considered as a kind of contextual feature generated when a user interacts with an item, which exists in various real-world applications, so it is worth studying the feedback in depth, rather than simply treating it as a kind of side information. Hence, we regard it as a feature to distinguish homogeneous sequential recommendation from heterogeneous sequential recommendation. There are also some sequential recommendation models designed for some specific problems, such as data noise [Chang et al. 2021;Sun et al. 2021;Tong et al. 2021;, sparsity [Liu et al. 2021a;Yuan et al. 2021], long sequences [Tan et al. 2021], and so on. The homogeneous sequential recommendation methods are not well applicable to heterogeneous sequences, because they can not distinguish different types of feedback. To be more in line with real application scenarios, it is necessary to consider different types of feedback in the sequences.\n\n\nHeterogeneous Sequential Recommendation\n\nAlthough the purchase feedback often indicates the user's strong preference, the user's recent examination feedback more truly reflects the user's current intention. To better model user interest, some methods have been proposed for heterogeneous sequential recommendation, which aims to model behavioral sequences with multiple feedback. In RLBL [Liu et al. 2017], a feedback-specific matrix is used in LBL to distinguish different feedback to obtain the local preference, and an RNN is used to capture the sequential features. RIB [Zhou et al. 2018] introduces a feedback embedding matrix, concatenating the item embedding and its corresponding behavior embedding as the input of RNN. BINN  integrates feedback modeling into LSTM, called CLSTM, and relies on CLSTM to model the local and global preferences. There are also some recent works leveraging graph neural networks (GNN) for heterogeneous sequential recommendation [Meng et al. 2020]. However, GNN-based methods are often time-consuming in comparing with the self-attentionbased methods that facilitate execution in parallel [Kang and McAuley 2018]. According to the user's interaction habits, users may incline to interact with certain categories of items. To capture this kind of preference, methods such as ; Tanjim et al. [2020]; Xu et al. [2021] introduce the category information of the items to better model user intention. However, to introduce the category information, the complexity of the model tends to become higher. And most heterogeneous sequential recommendation methods are based on RNN, which may not be applicable to real applications due to its slow training speed. As a simple but efficient module, the self-attention mechanism is rarely applied to heterogeneous sequential recommendation due to the confusion caused by different types of behaviors. As a response, we propose a novel feedback-aware local and global preference learning model, i.e., FLAG, which is a solution that applies self-attention to heterogeneous sequence recommendation. The newly designed feedback-aware self-attention block and local intention learning module enable our FLAG to process heterogeneous sequences well. In heterogeneous sequential recommendation, besides the item sequence (in the left of the middle), we also need to model the feedback sequence (in the top left corner) of each useru. First, we have an item-specific embedding matrix E and a feedback-specific embedding matrix F , so the two sequences of items and feedback can be represented as two embedding matrices. In the local preference learning module, we propose a feedback-aware self-attention block (FSAB) to replace the self-attention block (SAB) in SASRec (see Equations (1\u223c9)). In the global preference learning module (in the bottom left corner), we use a location-based attention layer LAL(\u00b7) (see Equation (11)) to identify the most representative items. In the local intention module (in the top), we take the embedding of target feedback as the query vector in a feedback-aware attention layer (FAL) and then have the local intention z li of each user u at each step . In the prediction module, we fuse the global preference, the recent item, and the candidate item and obtain a balance factor \u03bb, which is further used to combine the local and global preferences. Finally, we integrate the user's preference and local intention via an element-wise addition operation and have the final representation z for preference prediction.\n\n\nPROPOSED METHOD\n\nIn this article, we propose a novel feedback-aware local and global (FLAG) preference learning model for heterogeneous sequential recommendation, which is able to fully exploit different types of users' sequential feedback. Specifically, we have a set of users U, a set of items I, and a set of feedback F = {e, p}, where e means an examination feedback and p means a purchase feedback in an e-commence platform. Each user u owns his/her historical item and feedback sequence\nS u = {(i 1 u , f 1 u ), . . . , (i u , f u ), . . . , (i |S u | u , f |S u | u )}, i u \u2208 I, f u \u2208 F , \u2208 {1, 2, . . . , L}.\nNotice that each item i u is associated with a corresponding feedback f u , which is different from the homogeneous sequential recommendation problem. Our goal is to predict the next likely-to-purchase item i |S u |+1 u \u2208 I for each user u. We call the studied problem as heterogeneous sequential recommendation (HSR), because a sequence contains more than one type of feedback, e.g., examinations and purchases.\n\nWe illustrate the overall architecture of our FLAG in Figure 1, which contains a local preference learning module, a global preference learning module, a local intention learning module, and a prediction module. We will describe each component of our FLAG in the sequent sections. Notice that a letter in bold represents matrix or a corresponding row vector in this article.\n\n\nLocal Preference Learning\n\nIn modeling of a homogeneous sequence, self-attention block (SAB) has been widely used because it can effectively capture the long-range dependency of the item sequence. However, SAB cannot be directly applied to the modeling of the heterogeneous sequence for the reason that it can not distinguish different types of behaviors in a sequence. When users perform different behaviors on a same item, items in two different positions should be treated differently. However, the traditional self-attention block can not achieve this.\n\nTo this end, we propose a feedback-aware self-attention block (FSAB), which can be adapted to heterogeneous sequences by modifying SAB, so we can replace the self-attention block (SAB) in the state-of-the-art sequential recommendation model SASRec [Kang and McAuley 2018]. Specifically, we design a series of feedback-aware query, key, value matrices in FSAB, which takes the latest L items and their associated feedback of each user u, i.e.,\nS u = {(i 1 u , f 1 u ), . . . , (i u , f u ), . . . , (i L u , f L u )} instead of {i 1 u , . . . , i u , . . . i L u }\nin SASRec, as input, and output the local preference of each user u, so FSAB has the ability to distinguish different types of feedback. We fix L = 50 and pad the sequences that are shorter than L .\n\nWe represent an item sequence as an embedding matrix\nE = [I i 1 u ; . . . ; I i u ; . . . ; I i L u ] \u2208 R L\u00d7d , where I i u \u2208 R 1\u00d7d\ndenotes the item-specific latent vector of item i u . We follow Kang and McAuley [2018] and use a position-specific latent vector p \u2208 R 1\u00d7d to model the position information at each step and then have a position embedding matrix P = [p 1 ; . . . ; p ; . . . ; p L ] \u2208 R L\u00d7d . We use element-wise addition to fuse the item information and the position information at each position , which can be denoted as x (0) u \u2208 R 1\u00d7d :\nx (0) u = I i u + p , \u2208 {1, 2, . . . , L}.(1)\nThen, we can get the input matrix\nX (0) u = [x (0) u1 ; . . . ; x (0) u ; . . . ; x (0) uL ]\nand feed it into a stacked structure of multiple FSABs:\nX (b ) u = FSAB (b ) X (b\u22121) u , b \u2208 {1, 2, . . . , B}.(2)\nAn FSAB consists of three parts: The first one is a feedback-aware input layer (FIL) using the feedback-specific projection matrix to generate the feedback-specific projection query, key, and value matrices; the second one is a self-attention layer (SAL) used to capture the similarity relationship between the items; and the last one is a feed-forward layer (FFL) used to capture the non-linear relationship. Omitting the layer normalization technique and the residual connection, our feedback-aware self-attention block can be viewed as follows:\nFSAB(X ) = F FL(SAL(FIL(X ))),(3)\nwhere the self-attention layer SAL(\u00b7) and the feed-forward layer FFL(\u00b7) are first introduced in Vaswani et al. [2017]. In SAB, query, key, and value matrices are calculated by multiplying X by three different weight matrices, respectively. In our FSAB, we make use of the feedback information in the Q, K, V by our proposed feedback-aware input layer (FIL) as follows:\nQ, K, V = FIL(X ),(4)Q = f \u2208F (X \u2297 M f )W f q ,(5)K = f \u2208F (X \u2297 M f )W f k ,(6)V = f \u2208F (X \u2297 M f )W f v ,(7)\nwhere \u2297 is the element-wise product, and M f \u2208 R L\u00d7d denotes the mask matrix to extract the rows of the feedback type f . Notice that each row of M f with the feedback type f is a vector of ones, while other rows are vectors of zeros. W f q ,W f k ,W f v \u2208 R d \u00d7d are the projected query, key, and value matrices of the feedback f , respectively. We can see that our FSAB can easily be adapted to multiple types of feedback by extending the current set of feedback types F = {e, p}. After that, the self-attention layer can be formulated as follows:\nX = SAL(Q, K, V ) = so f tmax QK T \u221a d \u0394 \u00b7 V ,(8)\nwhere Q \u2208 R d \u00d7d denotes the queries, K \u2208 R d \u00d7d is the keys, and V \u2208 R d \u00d7d is the values. Notice that \u0394 represents the causality mask, i.e., it works as a filter that resembles a unit lower triangular matrix of size L \u00d7 L and sets the elements in the top-right corner as very small values (turned to be 0 after softmax) to ensure that only the items before the current step are used at each step. Next, we can feed the feedback-aware self-attention matrix X into FFL, which makes the model have the ability of nonlinear modeling and capture the relationships between different latent dimensions.\nF FL(X ) = ReLU (X W 1 + 1 T b 1 )W 2 + 1 T b 2 ,(9)\nwhere W 1 , W 2 \u2208 R d \u00d7d , and b 1 , b 2 \u2208 R 1\u00d7d denote the weight and bias of the two-layer neural network, 1 is a vector of ones with size 1 \u00d7 L, and ReLU is a common nonlinear activation function chosen because of the efficiency of its calculations. By stacking B FASBs, we can obtain the local preference z lp :\nz lp = x (B) u ,(10)\nwhere z lp \u2208 R 1\u00d7d denotes the local preference of user u at step obtained from the top FSAB. The FSAB module directly models the relationship between any two items in the sequence through self-attention, making FSAB better at capturing the long-term dependencies. With the help of the feedback-aware input layer, FSAB further captures the correlation between different feedback, which can better model the user's local preference.\n\n\nGlobal Preference Learning\n\nFSAB can capture the user's local preference, but it lacks the consideration of the global information, because the causality mask eliminates the impact of the future feedback via the attention weight. In this case, we use a location-based attention layer LAL(\u00b7) [Luong et al. 2015] to model the global preference z \u0434p of a user, which enables the model to make full use of the user's entire item sequence information at each step:\nz \u0434p = LAL(E) = so f tmax (q S (EW K ) T )EW V ,(11)\nwhereW K andW V \u2208 R d \u00d7d denote the projection matrices used to map the item embedding matrix E to the key matrix and the value matrix, respectively, and q S \u2208 R 1\u00d7d represents a shared query vector that is able to identify the most representative items. Notice that LAL is a variant of SAL, FLAG 14:9\n\nwhich introduces a shared vector q S of all sequences as a query vector and removes the causality mask to capture the global preference. In a related work, the authors apply the dropout technique for the global representation matrix, because z \u0434p is shared by all steps in a sequence during the training phase ]. Unlike homogeneous sequential recommendation, there are more examination records in a sequence in heterogeneous sequential recommendation, which makes it easier to capture the global preference. Therefore, the dropout technique is omitted in our FLAG. In our preliminary studies, we find that removing the dropout technique can indeed make the model achieve better convergence.\n\n\nLocal Intention Learning\n\nIn the process of studying how to model a heterogeneous sequence, we also observe that when there are examination and purchase feedback in a sequence, predicting the next likely-to-interact item actually includes two different tasks, i.e., predicting the next likely-to-examine item and predicting the next likely-to-purchase item. As the number of feedback types increases, more tasks will be involved. However, both the local preference learning module and the global preference learning module can only obtain the sequential information from historical heterogeneous sequences. This means that using the same obtained sequential representation to deal with different user intention may not be reasonable. Although the features extracted from a historical sequence can express the user's current intention to a certain extent, especially when we use a heterogeneous sequence rather than a homogeneous one, it still can not focus on the user's current intention. In addition, from the perspective of features, it does not reflect the difference between the user's different intention, because the same feature is used for different tasks. Through the local preference learning module and the global preference learning module, we can obtain the user's local and global preferences at each step. However, these two modules still cannot model the user's local intention well, which makes it difficult to predict the next item that fits for the real next feedback, i.e., examine or purchase.\n\nWhen a user decides to examine an item or purchase an item, the difference in intention will cause the interaction at different positions in the user's current heterogeneous sequence to show different importance. For example, before a user decides to buy a piece of clothing, he/she may have examined several related pieces of clothing and finally decide to buy it. For example, when the user clicks a headset, it may be because he has just bought a related product, such as a mobile phone. Therefore, we believe that for different user intention, a more distinguishable module is needed to model more precise user preferences. To this end, we propose a feedback-based attention layer (FAL) for modeling the local intention to obtain more accurate real-time user preferences.\n\nIn a heterogeneous sequence, whether a user's next behavior is examine or purchase is an indication of the user's current intention. Considering that in real scenarios, we cannot accurately predict the user's intention, but we can make use of this information during training to inform the model of the user's intention in advance. Then in the inference stage of the model, we can restrict the model to focus only on the intention of \"purchase. \"\n\nTo model the contribution of each item in the user's historical heterogeneous sequence toward the user intention, we propose a local intention learning module based on the self-attention block, where the key and value parts include the item, behavior, and location information, and the query part is the real next behavior that we regard as the user's intention.\n\nFollowing ; Zhou et al. [2018], we introduce a feedback-specific embedding matrix F , including the examination embedding F e and the purchase embedding F p , to represent the two types of feedback. Similar to FSAB, we use element-wise addition to construct the input of FAL, which can be represented as follows:\no u = I i u + p + F f u , \u2208 {1, 2, . . . , L},(12)\nwhere I i u \u2208 R 1\u00d7d , p \u2208 R 1\u00d7d , and F f u \u2208 R 1\u00d7d are the item-specific latent vector of item i u , the position-specific latent vector of position , and the feedback-specific latent vector of feedback f u , respectively. Then, we can get the input matrix of FAL, i.e., O = [o 1 u ; . . . ; o u ; . . . ; o L u ]. In fact, in heterogeneous sequential recommendation, we cannot accurately know whether the user will examine or purchase an item in the next step. However, just like many recommendation models using the candidate item as the input of the model, we can get the real next feedback, i.e., the intention, in the historical data in advance and use it as part of our model in the training phase. When the real next feedback is given, we hope that the model can extract some of the most relevant items in the historical feedback according to the user's current intention. Notice that we only make use of the real next feedback in the training phase to find out the relevant items under different intention. In the test phase (or real-world application), we can fix the real next feedback as purchase to match the goal of recommending the next likely-to-purchase item. Similar to LAL, we regard F e and F p as two shared query vectors to figure out the information that can represent the user's current intention. Specifically, we use the embedding corresponding to the user's real next feedback F f +1 u as the query vector, because it represents the user's local intention in the next step. The query matrix of FAL is thus as follows:\nQ i = [F f 2 u ; F f 3 u ; . . . ; F f L+1 u ]W Q ,(13)\nwhere W Q \u2208 R d \u00d7d denotes the projection query matrix, and Q i \u2208 R L\u00d7d represents a query matrix used to extract the information that is more relevant to the target feedback in the historical sequence. As a result, we can obtain the local intention z li in the following form:\nz li = FBA(Q i , O ) = so f tmax (Q i, (OW K ) T \u0394)OW V ,(14)\nwhere Q i, \u2208 R 1\u00d7d denotes the query vector of Q i at step , the causality constraint \u0394 is the same as that in Equation (8), and W K \u2208 R d \u00d7d and W V \u2208 R d \u00d7d denote the projection key and value matrices, respectively. In heterogeneous sequential recommendation, this module can be regarded as a kind of feedback-aware module for modeling the difference between two different types of feedback, so the model can extract the features that are more in line with the current intention from the historical sequence according to the user's next feedback, which enables the model to make more accurate predictions.\n\n\nPrediction and Learning\n\nTo balance the local preference and the global preference, we follow  and use an item similarity gating (ISG) module, which takes the global preference z \u0434p , the item i u at step , and the target item i +1 u as input, and outputs a balance factor \u03bb \u2208 (0, 1):\n\u03bb = ISG z \u0434p , i u , i +1 u = \u03c3 ([I i u , z \u0434p , I i +1 u ]W G + b G ),(15)\nwhere W G \u2208 R 3d \u00d71 , b G \u2208 R are the weight and bias of the ISG function, [\u00b7, \u00b7, \u00b7] denotes the concatenation operation to concatenate multiple feature vectors together, and the sigmoid function \u03c3 (x ) = 1/(1 + e \u2212x ) is an activation function that maps the input x to a real number between 0 and 1. Similar to the usage of the update gate in GRU, the global preference and the local preference can be aggregated with the balance factor \u03bb:\nz l\u0434p = z lp \u2297 \u03bb + z \u0434p \u2297 (1 \u2212 \u03bb),(16)\nwhere z l\u0434p \u2208 R 1\u00d7d , and \u2297 denotes the element-wise product operation. After that, we obtain the final representation z of the sequence at the th step by combining z l\u0434p and the intention FLAG 14:11 embedding z li via an element-wise addition operation:\nz = z l\u0434p + z li .(17)\nFinally, we predict the preference scorer +1,i of the target item i +1 u at each step :\nr +1,i = z (I i +1 u ) T .(18)\nAlthough we introduce a local intention learning module to model different user intention, the module does not distinguish the importance of different intention and treats them equally. To make the model pay more attention to the user's relatively sparse purchase intention during training, we introduce a hyper-parameter to adjust the importance of different intention. Specifically, in the training phase, we use a weighted cross-entropy loss as the loss function to emphasize the importance of the loss of the purchase task to a certain extent. Specifically, we use \u03b1 p and \u03b1 e to represent the weight of the purchase prediction task and the examination prediction task, respectively, to obtain a weight w +1 u at step of user u:\nw +1 u = \u03b1 p \u03b4 p f +1 u + \u03b1 e \u03b4 e f +1 u ,(19)\nwhere \u03b4 p ( f +1 u ) is a purchase indicator function having the value 1 if f +1 u is a purchase feedback, and 0 otherwise, and \u03b4 e (\u00b7) is an examination indicator function. Finally, our FLAG can be optimized by the Adam optimizer with the weighted cross-entropy loss function L:\nL = \u2212 u \u2208U L\u22121 =1 \u03b4 i +1 u w +1 u [log(\u03c3 (r +1,i +1 u )) + log(1 \u2212 \u03c3 (r +1, j ))],(20)\nwhere j is a negative item randomly sampled from I\\S u . The indicator function \u03b4 (i +1 u ) = 1 only if i +1 u is not a padding item and \u03b4 (i +1 u ) = 0 otherwise. Notice that for each user u, the loss at each step can be calculated in parallel due to the advantages of the self-attention structure.\n\n\nThe Learning Algorithm\n\nThe training procedure of our FLAG is depicted in Algorithm 1. First, we calculate the local preference z lp through our proposed FSAB (line 5) and the global preference z \u0434p via LAL (line 6), which take the heterogeneous sequence and item sequence as input, respectively. Next, the local intention z li generated by FAL (line 7) is summed up with the local and global preference z l\u0434p to obtain the final representation z of each step (line 8). In the end, we predict the preference scorer +1,i of the next item i at each step (line 9) and calculate the purchase-oriented weighted loss to train the whole model (lines 10-11).\n\n\nDiscussions\n\nOur FLAG can be seen as a significant extension of some typical homogeneous sequential recommendation methods, such as SASRec and FISSA.\n\n\u2022 SASRec [Kang and McAuley 2018] is a typical model that introduces the self-attention block into sequential recommendation. If we only use the local preference learning module and the traditional self-attention block to capture the sequential features in our FLAG, then our FLAG can be reduced to SASRec. \u2022 FISSA ] is a recent model in the field of homogeneous sequential recommendation, which combines local and global preferences for next-item prediction. If we remove the local intention module, FSAB in the local preference learning module, and the weight that balances the examination task and the purchase task, then our FLAG degenerates to FISSA. Collect a batch of users and the corresponding heterogeneous sequences, where the sequence of each user u is\nS u = {(i 1 u , b 1 u ), . . . , (i u , b u ), . . . , (i L+1 u , b L+1 u )}.\n\n5:\n\nCalculate local preference z lp of step via Equations (1-10). Although SASRec and FISSA achieve good performance in homogeneous sequential recommendation, neither of them can handle heterogeneous sequences. To deal with heterogeneous sequences that are more in line with real scenarios, the main advantages of our FLAG are as follows: (1) The FSAB we introduce in local preference learning allows the model to distinguish different behaviors when modeling heterogeneous sequences. (2) The local intention learning module enables the model to better capture user intention and thus make more accurate recommendations. Notice that it can be used as a general component to improve the performance of a certain homogeneous sequential recommendation model in scenarios with heterogeneous sequences. (3) Compared with SASRec and FISSA, our FLAG has no significant increase in space complexity and time complexity. We further verify in Section 4.2.3 that our FLAG is scalable in terms of time complexity and can be better applied to real-world recommendation scenarios.\n\n\nEXPERIMENTS\n\nIn this section, we report the settings and results of our experiments on three public datasets and study the following research questions: (RQ1) Does our FLAG achieve the state-of-the-art performance for heterogeneous sequential recommendation? (RQ2) Which module is the main source of the improvement of our FLAG? (RQ3) Compared with other methods, is our FLAG scalable in terms of space complexity and time complexity? (RQ4) How do the hyper-parameters such as the dimensionality of the latent vectors d and the number of blocks B affect the performance of our FLAG? (RQ5) How will our FLAG perform with more types of behaviors? (RQ6) How does the loss weight ratio between the examination task and the purchase task affect the performance of our FLAG?\n\n\nSettings\n\n4.1.1 Datasets. We conduct experiments on three public e-commence datasets with examination feedback and purchase feedback, i.e., RC15, 1 User Behavior (UB), 2 and Tmall. 3 RC15 is a dataset containing a lot of anonymous sessions with examination records and purchase records from RecSys Challenge 2015. UB is an e-commerce dataset that collects user behaviors in Taobao  For heterogeneous sequential recommendation, we preprocess the datasets as follows: (i) we sort the records of each user in chronological order and remove the records other than the examination and purchase ones; (ii) for all duplicated user-item-feedback tuples, we only keep the first tuple in a sequence; (iii) we discard the items with fewer than n purchase feedback according to the number of purchase records in each dataset, where n = 5 for RC15, n = 10 for UB, and n = 20 for Tmall; (iv) we remove the users whose purchase records are fewer than 5 for RC15 and UB, and 10 for Tmall; and (v) we take the last two purchase records of each sequence as the validation data and test data and the remaining records as the training data. Moreover, we remove the cold-start items in the validation data and test data. We summarize the statistics of the three processed datasets in Table 1.\n\n\nEvaluation Metrics.\n\nWe use two commonly used metrics, i.e., recall (Rec@10) and normalized discounted cumulative gain (NDCG@10), to evaluate the recommendation performance. For a top-10 list in next purchase item recommendation, Rec@10 measures how accurate a recommendation method is to hit a user's real interest, while NDCG@10 measures whether the next item that a user purchases ranks high enough. These two indicators measure the recommendation model from the two aspects of hit rate and ranking quality, respectively, which are suitable for the evaluation of the heterogeneous sequence recommendation task. We tune the model on the validation data and perform the final evaluation on the test data.\n\n\nBaselines.\n\nWe adopt 13 competitive baselines, including 2 non-sequential recommendation methods, 8 homogeneous sequential recommendation methods, and 3 heterogeneous sequential recommendation methods.\n\n\u2022 BPRMF [Rendle et al. 2009] is a recommendation model based on matrix factorization (MF), which uses a pairwise loss in the training phase. \u2022 FISM [Kabbur et al. 2013] is another MF-based method, which replaces the user-specific feature vector with the aggregated feature vectors of the historically interacted items of the user. \u2022 FPMC [Rendle et al. 2010] is a sequential recommendation method based on BPRMF [Rendle et al. 2009], which further considers the first-order Markov chain in a factorization way. \u2022 Fossil [He and McAuley 2016] combines FISM [Kabbur et al. 2013] and FPMC [Rendle et al. 2010] to tackle the sparsity with sequential dynamics, where the former and the latter are used to model the long-term and short-term preferences, respectively. In addition, some personalized weights are learned to balance these two parts. \u2022 GRU4Rec [Hidasi and Karatzoglou 2018] uses one or more GRU layers to model a user's history sequence, where the output of a GRU layer at each step will be used as its own input in the next step so it is able to capture the sequential relationship between a user's two consecutive feedback. \u2022 Caser [Tang and Wang 2018] is a CNN-based model that uses some vertical and horizontal convolutions to model the sequential features. Compared with the RNN-based method, the CNN-based method has the advantage of being able to be trained in parallel. Notice that there is a user embedding in Caser. \u2022 SASRec [Kang and McAuley 2018] uses a self-attention layer to directly model the correlation between the historical items to better capture the long-range dependency. Moreover, the feed-forward layer brings better nonlinearity to the model, which further enhances its modeling capability. \u2022 CAR [He et al. 2020] uses a general user preference model (GUPM) to learn a user's global preference, a current preference priority model (CPPM) to capture the local preference, and finally a consistency-aware gating network to balance the local and global preferences. \u2022 FISSA ] contains three modules, including a local representation module based on SASRec, a global representation module based on LAL, and an item similarity gating module for balancing these two representations. Notice that FISSA is also a special case of our FLAG by replacing FSAB in the local preference learning module with SAB and removing the local intention learning module. \u2022 NOVA [Liu et al. 2021b] treats items as the value part of a self-attention layer and takes some side information as the query and key parts of a self-attention layer to ensure that the feature space of the items is not interfered. \u2022 RLBL [Liu et al. 2017] improves the log-bilinear (LBL) module with a feedback-specific transition matrix to aggregate the item information in a time window and uses an RNN to obtain the sequential features between two adjacent windows. \u2022 RIB [Zhou et al. 2018] is a heterogeneous sequential recommendation method that takes the vector obtained by concatenating the item embedding and the feedback embedding as input and models the sequential features through a GRU layer. \u2022 BINN ] uses a CLSTM layer that considers the feedback information to model a user's short-term preferences and also uses Bi-CLSTM to model a user's stable preferences.\n\n\nImplementation Details.\n\nWe implement the MF-based methods (i.e., BPRMF, FISM, FPMC, and Fossil) with the code published by He and McAuley [2016], the RNN-based methods (i.e., GRU4Rec, RLBL, RIB, and BINN) in TensorFlow. For the DL-based methods (i.e., Caser, CAR, SAS-Rec, and FISSA), we run the code released by ; Tang and Wang [2018]. The code of NOVA and our FLAG is adapted from that of FISSA . For all the methods, we set the embedding size d = 50 [Kang and McAuley 2018] and initialize all the embedding matrices via Xavier normal initialization [Glorot and Bengio 2010]. For the ML-based methods, the hyperparameters on the regularization terms are chosen from {0.01, 0.001, 0.0001}. For the DL-based methods, we set the dropout rate to 0.5 [Kang and McAuley 2018]. For SASRec, CAR, FISSA, and our FLAG, we set the sequence length L of each user to 50, the batch size to 128, and the learning rate to 0.001 ]. Besides, the number of blocks B is chosen from {1, 2, 3} via the performance of Rec@10 on the validation data ]. In the test phase, we fix the real next feedback as purchase to guide our FLAG to generate the next likely-to-purchase items for each user. The data, code, and scripts used in the experiments are publicly available 4 for reproducibility and further extension.\n\n\nResults\n\n\nPerformance Comparison (RQ1).\n\nWe report the recommendation performance of our FLAG and 13 baselines on three datasets in Table 2. From the results in Table 2, we can have the following observations: (i) For non-sequential recommendation methods, FISM performs better than BPRMF in more cases, which shows the effectiveness of modeling the global preference from the historically interacted items in FISM. And both FISM and BPRMF perform worse than most homogeneous sequential recommendation methods, indicating the usefulness of the sequential information in item sequences. (ii) For homogeneous sequential recommendation methods, FISSA achieves the best performance, which is consistent with the results in . (iii) For heterogeneous sequential recommendation methods, the RNN-based heterogeneous sequential recommendation methods (i.e., RLBL, RIB, and BINN) can beat the RNN-based homogeneous sequential recommendation method (i.e., GRU4Rec), indicating that distinguishing different types of feedback is beneficial to the performance. And our FLAG performs the best in most cases except the case of BINN on NDCG@10 on Tmall, which shows the superiority of our FLAG in modeling different types of sequential feedback. (iv) For the best homogeneous and heterogenous baselines, i.e., FISSA and BINN, our FLAG improves them by 14.64% and 20.20% in terms of Rec@10 (and 20.65% and 4.54% in terms of NDCG@10) on average across the three datasets, respectively. The results clearly show the effectiveness of the specifically designed feedback-aware self-attention block and the local intention learning module, as well as the advantage of the self-attention mechanism over the recurrent mechanism in RNN-based methods.\n\n\nAblation Study (RQ2).\n\nTo distinguish and exploit the two different types of feedback, we specifically design a feedback-aware self-attention block (FSAB) in the local preference learning module, propose a new local intention learning module considering a user's real next feedback, and also use a weighted cross-entropy loss for two different prediction tasks. To figure out the contribution of different components to the performance of our FLAG, we conduct ablation studies and report the results in Table 3. We compare the performance of SASRec, SASRec with the local intention learning module (denoted as SASRec (w/ z li )), FISSA, FISSA with the local intention learning module (denoted as FISSA (w/ z li )), FLAG without FSAB, i.e., replace FSAB with SAB [Kang and McAuley 2018] (denoted as \"FLAG (w/o FSAB)\"), FLAG without the local intention learning module, i.e., z = z l\u0434p (denoted as \"FLAG (w/o z li )\"), FLAG without the weighted cross-entropy loss, i.e., \u03b1 e = \u03b1 p = 1 (denoted as \"FLAG (w/o weighted loss)\"), and our FLAG. We have the following observations: (i) All variants of our FLAG surpass FISSA on all three datasets, among which our FLAG achieves the best performance on average, which shows that (iv) Our weighted loss improves 4.88% on RC15 and 5.68% on Tmall, respectively, which shows that the introduction of the weighted loss can indeed improve the model's ability to predict the next likely-to-purchase item. We also notice that the weighted loss is not helpful on UB due to the sparsity of the UB dataset. (v) We can find that the local intention module improves our FLAG by 11.09% on average across the three datasets, while FSAB and the weighted loss have less than 5% improvement, indicating that our local intention learning module is the main source of the improvement of our FLAG.\n\n\nComplexity Analyze (RQ3).\n\nSince our work targets at real-world recommendation scenarios, the model efficiency (e.g., running time) is an important factor to investigate. Is our FLAG scalable as compared with other methods in terms of the computation time? Does the model need more parameters to capture the user preferences? In this section, we study and compare the space complexity and time complexity of three models, i.e., SASRec, FISSA, and our FLAG, and verify whether our FLAG has good scalability. In the following analysis, we set the number of blocks B = 1 in these three models.\n\nSpace complexity. For SASRec, its space complexity is O ((|I | +L +2)d +5d 2 ), of which (|I | +L + 2)d is for the parameters of the item embedding matrix, position embedding matrix, and the bias of FFN, and 5d 2 belongs to the self-attention block. The number of parameters of the item embedding matrix is dominant in the total number of parameters. For FISSA, the total space complexity is O ((|I | + L + 8)d + 9d 2 ), and the number of new parameters comes from the global part (2d for the bias of FFN, d for the query in the SAB, and 3d 2 for the projection matrices) and the gating part (3d). For our FLAG, the proposed FSAB has |F | times the parameter number compared with an ordinary SAB (|F | = 2 in our experiment). The local intention learning module is also based on the self-attention block and has the same number of parameters. The behavior embedding as query in the local intention learning module has the parameter amount of |F |d. In the end, the final space complexity of our FLAG is O ((|I | + 2L + 10 + |F |)d + (3|F | + 11)d 2 ). In real scenarios, there are usually only a few types of behaviors, and the number of items is usually at the level of millions, i.e., |I | >> |F |. Therefore, the number of parameters introduced by our FSAB and local intention learning module in our FLAG is far smaller than that of the item embedding, and our FLAG is thus acceptable in terms of space complexity. Time complexity. The module of SAB usually includes the following steps: mapping the input features to query, key, and value, calculating similarity, normalizing the similarity using the softmax function, calculating the weighted sum, and finally passing through the non-linear layer FFN. The time complexity of the above steps are\nO (3Ld 2 ), O (L 2 d ), O (L 2 ), O (L 2 d ), and O (2Ld 2 ),\nwhere L is the length of the historical heterogeneous sequence, and d is the dimensionality of embedding. If we ignore the constant term, then we can get the time complexity of SASRec as O (2L 2 d + 5Ld 2 ). For FISSA, the new global module can also be regarded as SAB, and the time complexity of the calculation of the gating part can be ignored, so the time complexity of FISSA is O (4L 2 d + 9Ld 2 ). For our FLAG, the proposed FSAB makes the model better distinguish between different types of behaviors. Therefore, for different behaviors, we need to do a mapping of the input to the query, key, and value matrices. The time complexity is O(3|F |Ld 2 ), and the local intent learning module is also based on SAB and has the same time complexity. Finally, the time complexity of our FLAG is O (6L 2 d + (3|F | + 11)Ld 2 ). We summarize the complexity in Table 4.\n\nIt is worth mentioning that although there are three SAB-based modules in our FLAG, it does not mean that our FLAG requires three times as much training time as SASRec. In our FLAG, the time complexity of the local preference learning module, the global preference learning module, and the local intention learning module are O (2L 2 d + (3|F | + 2)Ld 2 ), O (2L 2 d + 4Ld 2 ), and O (2L 2 d + 5Ld 2 ), respectively. Since they do not depend on each other, our FLAG can be further optimized in parallel computing. Among the three modules, the module with the highest time complexity is the local preference learning module, which is also the upper bound of the time complexity of our FLAG in parallel implementation. The gap between FLAG and SASRec is only O (3(|F | \u2212 1)Ld 2 ), which is acceptable in real scenarios because |F | is usually small.\n\nTo further verify the above analysis, we study the average training time for each epoch of SAS-Rec, FISSA, and our FLAG on Quadro RTX 4000, which are shown in Figure 2. From Figure 2, we can see that the average running time of our FLAG is about 1.2 times that of SASRec, which shows that our FLAG can perform better in modeling heterogeneous sequences without greatly increasing the training time.\n\n\nQuantitative Study (RQ4).\n\nIn this subsection, we study the effect of two hyper-parameters: one is the dimensionality d that affects the capacity of the model and the other is the number of  From Figures 2 and 3, we can see that our FLAG achieves better results than FISSA and SASRec with different values of d \u2208 {10, 20, 30, 40, 50}, and the running time is only 8% slower than FISSA on average, indicating that our model can be applied to heterogeneous sequences at a low cost. With the increase of d, the performance of our FLAG increases faster than that of FISSA, which shows that our FLAG can better capture the user's preference with a larger d.\n\nFrom Figure 4, we can see that our FLAG is better than FISSA and SASRec with different values of B \u2208 {1, 2, 3}, and B = 1 is usually good enough in most cases (except on RC15). This is because in heterogeneous sequential recommendation, the input sequence contains items with different types of feedback, which provides rich information so the single-layer FSAB can model the local preference well. However, the local preference learning module does not explicitly model the user's intent. Our local intention learning module can well capture the user's intention, which makes up for this shortcoming.\n\n\nFeedback Extension (RQ5).\n\nIn this section, to study the impact of the number of feedback types on our FLAG, we also consider the adding-to-cart and adding-to-favorite feedback in the Tmall and UB datasets so the number of feedback types is increased to four. We use FLAG(|F | = 2) and FLAG(|F | = 4) to represent our FLAG when there are two types of feedback and four types of feedback, respectively. The results are shown in Table 5.\n\nFrom Table 5, we can see that FLAG(F = |4|) is increased by 16% and 19% on Rec@10 and NDCG@10 on the UB dataset, respectively. For the Tmall dataset, there is also an increase of 2.77% on NDCG@10. The results show that as the number of feedback types increases, our FLAG can still distinguish different feedback, and more feedback types can make the local intention learning module more accurately capture the user's intention. (RQ6). In this section, we compare the effect of different values of \u03b1 e and \u03b1 p on our FLAG on three datasets. From the results in Figure 5, we can have the following observations: (i) When \u03b1 e = 1 and \u03b1 p = 1, it is equivalent to train two tasks at the same time, and both tasks are equally important. We can see that the performance of the model greatly exceeds the situation of \u03b1 e = 0 and \u03b1 p = 1, indicating that the examination task can help the purchase task to learn more representative features and thus alleviate the data sparsity. (ii) \u03b1 p = 2 means that we will give the purchase task more weight. In other words, if the prediction for the purchase task is wrong, then the model will be punished more severely, so the model can better distinguish between correctly and incorrectly predicted samples. On the sparse UB dataset, the value of \u03b1 p = 2 makes the model perform slightly worse than \u03b1 p = 1. On the relatively dense RC15 and Tmall, our FLAG achieves the best performance, showing that increasing the weight of the purchase task can make the model produce better recommendations. (iii) As \u03b1 p becomes further larger, the overall performance of the model shows a downward trend, which means that \u03b1 p cannot be increased unlimitedly, since the examination task is also important for preference learning.\n\n\nExploratory Study\n\n\nCONCLUSIONS AND FUTURE WORK\n\nIn this article, we propose a novel feedback-aware local and global (FLAG) model for an emerging and important problem called heterogeneous sequential recommendation with more than one type of feedback in item sequences. Specifically, our FLAG contains four main components, i.e., a local preference learning module, a global preference learning module, a local intention learning module, and a prediction module. Importantly, besides modeling a user's long-term interest from his/her historically interacted items in the global preference learning module and fusing the shortand long-term preferences seamlessly in the prediction module, we design a feedback-aware selfattention block (FSAB) in the local preference learning module and take the real next feedback as the query vector in the local intention learning module to better digest a user's heterogeneous feedback. We then conduct extensive empirical studies on three public datasets and find that our FLAG achieves the best performance compared with 13 very competitive baselines in most cases. We further report ablation studies to show the rationality of the design of the network architecture, as well as the sensitivity analysis of the hyper-parameters to have more insights of our FLAG. In addition, we do an in-depth analysis of the time complexity and space complexity and also study the performance of our FLAG with different numbers of behavior types and show that our FLAG can be applied to real scenarios.\n\nIn the future, we plan to explore how to model other types of feedback with negative semantic information such as dislike, remove from favorites, and others. Moreover, we are also interested in integrating the knowledge graph of items or the social network of users in heterogeneous sequential recommendation.\n\nFig. 1 .\n1The network architecture of our FLAG.\n\nFig. 3 .\n3Recommendation performance (Rec@10) of SASRec, FISSA, and our FLAG with different values of d on three datasets (B = 1).\n\nFig. 4 .\n4Recommendation performance (Rec@10) of SASRec, FISSA, and our FLAG with different numbers of blocks B on three datasets (d = 50). our proposed feedback-aware self-attention blocks B. The results of SASRec, FISSA, and our FLAG on Rec@10 are shown in Figures 3 and 4.\n\nALGORITHM 1 :\n1The learning algorithm of feedback-aware local and global model (FLAG).1: Initialization: Initialize model parameters \u0398. \n2: repeat \n\n3: \n\nfor t = 1, . . . ,T do \n\n4: \n\n\n\nTable 1 .\n1Statistics of the Processed Datasets Used in the ExperimentsDataset # Users # Items # Examinations # Purchases Avg.Length Density \n\n\nTable 2 .\n2Recommendation Performance of our FLAG and 13 Baselines, Including 2 Non-sequential Recommendation Methods (i.e., BPRMF and FISM), 8 Homogeneous Methods (i.e., FPMC, Fossil, GRU4Rec, Caser, SASRec, CAR, FISSA, and NOVA) and 3 Heterogeneous Methods (i.e., RLBL, RIB, and BINN) on Three DatasetsType \nMethod \nRC15 \nUB \nTmall \nRec@10 NDCG@10 Rec@10 NDCG@10 Rec@10 NDCG@10 \nNon-sequential \nrecommendation method \n\nBPRMF \n0.2189 \n0.0938 \n0.0301 \n0.0128 \n0.0223 \n0.0105 \nFISM \n0.2843 \n0.1570 \n0.0319 \n0.0170 \n0.0154 \n0.0070 \n\nHomogeneous sequential \nrecommendation method \n\nFPMC \n0.2533 \n0.1264 \n0.0346 \n0.0180 \n0.0458 \n0.0252 \nFossil \n0.3495 \n0.1950 \n0.0403 \n0.0222 \n0.0583 \n0.0354 \nGRU4Rec 0.3548 \n0.1905 \n0.0478 \n0.0264 \n0.0705 \n0.0401 \nCaser \n0.3954 \n0.1871 \n0.0448 \n0.0229 \n0.0671 \n0.0348 \nSASRec \n0.3808 \n0.1821 \n0.0515 \n0.0227 \n0.0754 \n0.0346 \nCAR \n0.4170 \n0.1906 \n0.0693 \n0.0306 \n0.0818 \n0.0387 \nFISSA \n0.4359 \n0.1989 \n0.0695 \n0.0308 \n0.0836 \n0.0399 \nNOVA \n0.4030 \n0.1944 \n0.0625 \n0.0287 \n0.0889 \n0.0428 \n\nHeterogeneous sequential \nrecommendation method \n\nRLBL \n0.3800 \n0.1999 \n0.0418 \n0.0225 \n0.0614 \n0.0358 \nRIB \n0.3900 \n0.2052 \n0.0619 \n0.0343 \n0.0720 \n0.0417 \nBINN \n0.4159 \n0.2207 \n0.0628 \n0.0347 \n0.0838 \n0.0487 \nFLAG \n0.4947 \n0.2342 \n0.0833 \n0.0388 \n0.0925 \n0.0466 \n\n\n\nTable 3 .\n3Recommendation Performance (Rec@10) of Different Architectures on Three Datasets in Ablation Studies (B = 1)Architecture \nDataset RC15 \nUB \nTmall \n\nSASRec \n0.3505 0.0489 0.0754 \nSASRec (w/ z li ) \n0.3951 0.0666 0.0900 \nFISSA \n0.4226 0.0695 0.0836 \nFISSA (w/ z li ) \n0.4573 0.0827 0.0856 \nFLAG (w/o FSAB) \n0.4557 0.0817 0.0852 \nFLAG (w/o z li ) \n0.4311 0.0771 0.0794 \nFLAG (w/o weighted loss) \n0.4471 0.0860 0.0875 \nFLAG \n0.4689 0.0833 0.0925 \n\nthe three components can complement each other and make the model perform better. (ii) SASRec \nand FISSA achieve better results than the original model after adding the local intention learning \nmodule, which shows that the local intention learning module can be used as a general compo-\nnent to improve the performance of a certain homogeneous sequential model in heterogeneous \nsequence scenarios. (iii) Our FLAG performs better than FLAG (w/o FSAB) in all cases, indicating \nthat our FSAB enables the model to better distinguish two different types of feedback in heteroge-\nneous sequences. \n\nTable 4 .\n4The Space Complexity and Time Complexity of SASRec, FISSA, and Our FLAG |I | + 2L + 10 + |F |)d + (3|F | + 11)d 2 ) O(6L 2 d + (3|F | + 11)Ld 2 ) Fig. 2. The average running time for each epoch of SASRec, FISSA, and our FLAG with different values of d on three datasets (B = 1).Space Complexity \nTime Complexity \nSASRec \nO((|I | + L + 2)d + 5d 2 ) \nO ( 2 L 2 d + 5Ld 2 ) \nFISSA \nO((|I | + L + 8)d + 9d 2 ) \nO ( 4 L 2 d + 9Ld 2 ) \nFLAG \nO((\n\nTable 5 .\n5Recommendation Performance of our FLAG on UB and Tmall, of which FLAG(|F | = x) Denotes that Our FLAG Is Trained with x Kind of Feedback Fig. 5. Recommendation performance (Rec@10) of our FLAG with different values of \u03b1 e and \u03b1 p on three datasets (d = 50, B = 1).Method \nUB \nTmall \nRec@10 NDCG@10 Rec@10 NDCG@10 \nFLAG(|F | = 2) 0.0833 \n0.0388 \n0.0925 \n0.0446 \nFLAG(|F | = 4) 0.0966 \n0.0461 \n0.0923 \n0.0458 \n\n\nACM Transactions on Intelligent Systems and Technology, Vol. 14, No. 1, Article 14. Publication date: November 2022.\nhttps://recsys.acm.org/recsys15/challenge. 2 https://tianchi.aliyun.com/dataset/dataDetail?dataId=649. 3 https://tianchi.aliyun.com/dataset/dataDetail?dataId=42. ACM Transactions on Intelligent Systems and Technology, Vol. 14, No. 1, Article 14. Publication date: November 2022.\nhttp://csse.szu.edu.cn/staff/panwk/publications/FLAG/.\nACKNOWLEDGMENTSWe thank the handling associate editor and reviewers for their efforts and constructive expert comments.\nSequential recommendation with graph neural networks. Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng Jin, Yong Li, Proceedings of the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21. the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song, Depeng Jin, and Yong Li. 2021. Sequential recom- mendation with graph neural networks. In Proceedings of the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21). 378-387.\n\nEfficient heterogeneous collaborative filtering without negative sampling for recommendation. Chong Chen, Min Zhang, Yongfeng Zhang, Weizhi Ma, Yiqun Liu, Shaoping Ma, Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI'20. the 34th AAAI Conference on Artificial Intelligence (AAAI'20Chong Chen, Min Zhang, Yongfeng Zhang, Weizhi Ma, Yiqun Liu, and Shaoping Ma. 2020. Efficient heterogeneous collabo- rative filtering without negative sampling for recommendation. In Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI'20). 19-26.\n\nBehavior sequence transformer for e-commerce recommendation in Alibaba. Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, Wenwu Ou, Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data. the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse DataQiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Behavior sequence transformer for e-commerce rec- ommendation in Alibaba. In Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data. 1-4.\n\nAIR: Attentional intentionaware recommender systems. Tong Chen, Hongzhi Yin, Hongxu Chen, Rui Yan, Proceedings of the 25th IEEE International Conference on Data Engineering (ICDE'19. the 25th IEEE International Conference on Data Engineering (ICDE'19Quoc Viet Hung Nguyen, and Xue LiTong Chen, Hongzhi Yin, Hongxu Chen, Rui Yan, Quoc Viet Hung Nguyen, and Xue Li. 2019. AIR: Attentional intention- aware recommender systems. In Proceedings of the 25th IEEE International Conference on Data Engineering (ICDE'19). 304-315.\n\nA survey on heterogeneous one-class collaborative filtering. Xiancong Chen, Lin Li, Weike Pan, Zhong Ming, ACM Trans. Inf. Syst. 3854Xiancong Chen, Lin Li, Weike Pan, and Zhong Ming. 2020. A survey on heterogeneous one-class collaborative filtering. ACM Trans. Inf. Syst. 38, 4 (2020), 35:1-35:54.\n\nImproving implicit recommender systems with view data. Jingtao Ding, Guanghui Yu, Xiangnan He, Yuhan Quan, Yong Li, Tat-Seng Chua, Depeng Jin, Jiajie Yu, Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI'18. the 27th International Joint Conference on Artificial Intelligence (IJCAI'18Jingtao Ding, Guanghui Yu, Xiangnan He, Yuhan Quan, Yong Li, Tat-Seng Chua, Depeng Jin, and Jiajie Yu. 2018. Improving implicit recommender systems with view data. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI'18). 3343-3349.\n\nLearning to recommend with multiple cascading behaviors. Chen Gao, Xiangnan He, Danhua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat-Seng Chua, Lina Yao, Yang Song, Depeng Jin, IEEE Trans. Knowl. Data Eng. 33Chen Gao, Xiangnan He, Danhua Gan, Xiangning Chen, Fuli Feng, Yong Li, Tat-Seng Chua, Lina Yao, Yang Song, and Depeng Jin. 2021. Learning to recommend with multiple cascading behaviors. IEEE Trans. Knowl. Data Eng. 33, 6 (2021), 2588-2601.\n\nUnderstanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio, Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (JMLR Proceedings). the 13th International Conference on Artificial Intelligence and Statistics (JMLR Proceedings)9Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (JMLR Proceedings), Vol. 9. 249- 256.\n\nFusing similarity models with Markov chains for sparse sequential recommendation. Ruining He, Julian Mcauley, Proceedings of the 16th IEEE International Conference on Data Mining (ICDM'16. the 16th IEEE International Conference on Data Mining (ICDM'16Ruining He and Julian McAuley. 2016. Fusing similarity models with Markov chains for sparse sequential recommendation. In Proceedings of the 16th IEEE International Conference on Data Mining (ICDM'16). 191-200.\n\nNAIS: Neural attentive item similarity model for recommendation. Xiangnan He, Zhankui He, Jingkuan Song, Zhenguang Liu, Yu-Gang Jiang, Tat-Seng Chua, IEEE Trans. Knowl. Data Eng. 30Xiangnan He, Zhankui He, Jingkuan Song, Zhenguang Liu, Yu-Gang Jiang, and Tat-Seng Chua. 2018. NAIS: Neural attentive item similarity model for recommendation. IEEE Trans. Knowl. Data Eng. 30, 12 (2018), 2354-2366.\n\nNeural collaborative filtering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, Proceedings of the 26th International Conference on World Wide Web. the 26th International Conference on World Wide WebXiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th International Conference on World Wide Web. 173-182.\n\nConsistency-aware recommendation for user-generated item list continuation. Yun He, Yin Zhang, Weiwen Liu, James Caverlee, Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM'20. the 13th International Conference on Web Search and Data Mining (WSDM'20Yun He, Yin Zhang, Weiwen Liu, and James Caverlee. 2020. Consistency-aware recommendation for user-generated item list continuation. In Proceedings of the 13th International Conference on Web Search and Data Mining (WSDM'20). 250-258.\n\nRecurrent neural networks with top-k gains for session-based recommendations. Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Proceedings of the 27th ACM International Conference on Information and Knowledge Management (CIKM'18. the 27th ACM International Conference on Information and Knowledge Management (CIKM'18Bal\u00e1zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent neural networks with top-k gains for session-based recommen- dations. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (CIKM'18). 843-852.\n\nFISM: Factored item similarity models for top-n recommender systems. Santosh Kabbur, Xia Ning, George Karypis, Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'13. the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'13Santosh Kabbur, Xia Ning, and George Karypis. 2013. FISM: Factored item similarity models for top-n recommender systems. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD'13). 659-667.\n\nSelf-attentive sequential recommendation. Wang-Cheng Kang, Julian J Mcauley, Proceedings of the 18th IEEE International Conference on Data Mining (ICDM'18. the 18th IEEE International Conference on Data Mining (ICDM'18Wang-Cheng Kang and Julian J. McAuley. 2018. Self-attentive sequential recommendation. In Proceedings of the 18th IEEE International Conference on Data Mining (ICDM'18). 197-206.\n\nNeural attentive session-based recommendation. Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, Jun Ma, Proceedings of the ACM Conference on Information and Knowledge Management (CIKM'17. the ACM Conference on Information and Knowledge Management (CIKM'17Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017. Neural attentive session-based recommen- dation. In Proceedings of the ACM Conference on Information and Knowledge Management (CIKM'17). 1419-1428.\n\nLearning from history and present: Nextitem recommendation via discriminatively exploiting user behaviors. Zhi Li, Hongke Zhao, Qi Liu, Zhenya Huang, Tao Mei, Enhong Chen, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (CIKM'18. the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (CIKM'18Zhi Li, Hongke Zhao, Qi Liu, Zhenya Huang, Tao Mei, and Enhong Chen. 2018. Learning from history and present: Next- item recommendation via discriminatively exploiting user behaviors. In Proceedings of the 24th ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Mining (CIKM'18). 1734-1743.\n\nFISSA: Fusing item similarity models with self-attention networks for sequential recommendation. Jing Lin, Weike Pan, Zhong Ming, Proceedings of the 14th ACM Conference on Recommender Systems (RecSys'20. the 14th ACM Conference on Recommender Systems (RecSys'20Jing Lin, Weike Pan, and Zhong Ming. 2020. FISSA: Fusing item similarity models with self-attention networks for sequen- tial recommendation. In Proceedings of the 14th ACM Conference on Recommender Systems (RecSys'20). 130-139.\n\nNoninvasive self-attention for side information fusion in sequential recommendation. Chang Liu, Xiaoguang Li, Guohao Cai, Zhenhua Dong, Hong Zhu, Lifeng Shang, Proceedings of the 35th AAAI Conference on Artificial Intelligence. the 35th AAAI Conference on Artificial IntelligenceChang Liu, Xiaoguang Li, Guohao Cai, Zhenhua Dong, Hong Zhu, and Lifeng Shang. 2021b. Noninvasive self-attention for side information fusion in sequential recommendation. In Proceedings of the 35th AAAI Conference on Artificial Intelligence. 4249-4256.\n\nMulti-behavioral sequential prediction with recurrent log-bilinear model. Qiang Liu, Shu Wu, Liang Wang, IEEE Trans. Knowl. Data Eng. 29Qiang Liu, Shu Wu, and Liang Wang. 2017. Multi-behavioral sequential prediction with recurrent log-bilinear model. IEEE Trans. Knowl. Data Eng. 29, 6 (2017), 1254-1267.\n\nAugmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. Zhiwei Liu, Ziwei Fan, Yu Wang, Philip S Yu, Proceedings of the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21. the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21Zhiwei Liu, Ziwei Fan, Yu Wang, and Philip S. Yu. 2021a. Augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. In Proceedings of the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21). 1608-1612.\n\nBayesian personalized ranking with multi-channel user feedback. Babak Loni, Roberto Pagano, Martha A Larson, Alan Hanjalic, Proceedings of the 10th ACM Conference on Recommender Systems (RecSys'16. the 10th ACM Conference on Recommender Systems (RecSys'16Babak Loni, Roberto Pagano, Martha A. Larson, and Alan Hanjalic. 2016. Bayesian personalized ranking with multi-channel user feedback. In Proceedings of the 10th ACM Conference on Recommender Systems (RecSys'16). 361-364.\n\nEffective approaches to attention-based neural machine translation. Thang Luong, Hieu Pham, Christopher D Manning, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP'15. the Conference on Empirical Methods in Natural Language Processing (EMNLP'15Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP'15). 1412- 1421.\n\nIncorporating user micro-behaviors and item knowledge into multitask learning for session-based recommendation. Wenjing Meng, Deqing Yang, Yanghua Xiao, Proceedings of the 43rd International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'20. the 43rd International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'20Wenjing Meng, Deqing Yang, and Yanghua Xiao. 2020. Incorporating user micro-behaviors and item knowledge into multi- task learning for session-based recommendation. In Proceedings of the 43rd International ACM SIGIR Conference on Re- search & Development in Information Retrieval (SIGIR'20). 1091-1100.\n\nAdaptive Bayesian personalized ranking for heterogeneous implicit feedbacks. Weike Pan, Hao Zhong, Congfu Xu, Zhong Ming, Knowl.-based Syst. 73Weike Pan, Hao Zhong, Congfu Xu, and Zhong Ming. 2015. Adaptive Bayesian personalized ranking for heterogeneous implicit feedbacks. Knowl.-based Syst. 73 (2015), 173-180.\n\nPersonalizing session-based recommendations with hierarchical recurrent neural networks. Massimo Quadrana, Alexandros Karatzoglou, Bal\u00e1zs Hidasi, Paolo Cremonesi, Proceedings of the 11th ACM Conference on Recommender Systems (RecSys'17. the 11th ACM Conference on Recommender Systems (RecSys'17Massimo Quadrana, Alexandros Karatzoglou, Bal\u00e1zs Hidasi, and Paolo Cremonesi. 2017. Personalizing session-based rec- ommendations with hierarchical recurrent neural networks. In Proceedings of the 11th ACM Conference on Recommender Systems (RecSys'17). 130-137.\n\nBPR: Bayesian personalized ranking from implicit feedback. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI'09. the 25th Conference on Uncertainty in Artificial Intelligence (UAI'09Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI'09). 452-461.\n\nFactorizing personalized Markov chains for nextbasket recommendation. Steffen Rendle, Christoph Freudenthaler, Lars Schmidt-Thieme, Proceedings of the 19th International Conference on World Wide Web (WWW'10. the 19th International Conference on World Wide Web (WWW'10Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized Markov chains for next- basket recommendation. In Proceedings of the 19th International Conference on World Wide Web (WWW'10). 811-820.\n\nDoes every data instance matter? Enhancing sequential recommendation by eliminating unreliable data. Yatong Sun, Bin Wang, Zhu Sun, Xiaochun Yang, Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI'21. the 30th International Joint Conference on Artificial Intelligence (IJCAI'21Yatong Sun, Bin Wang, Zhu Sun, and Xiaochun Yang. 2021. Does every data instance matter? Enhancing sequential recom- mendation by eliminating unreliable data. In Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI'21). 1579-1585.\n\nDynamic memory based attention network for sequential recommendation. Qiaoyu Tan, Jianwei Zhang, Ninghao Liu, Xiao Huang, Hongxia Yang, Jingren Zhou, Xia Hu, Proceedings of the 35th AAAI Conference on Artificial Intelligence. the 35th AAAI Conference on Artificial IntelligenceQiaoyu Tan, Jianwei Zhang, Ninghao Liu, Xiao Huang, Hongxia Yang, Jingren Zhou, and Xia Hu. 2021. Dynamic mem- ory based attention network for sequential recommendation. In Proceedings of the 35th AAAI Conference on Artificial Intelligence. 4384-4392.\n\nPersonalized top-n sequential recommendation via convolutional sequence embedding. Jiaxi Tang, Ke Wang, Proceedings of the 11th ACM International Conference on Web Search and Data Mining (WSDM'18. the 11th ACM International Conference on Web Search and Data Mining (WSDM'18Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In Proceedings of the 11th ACM International Conference on Web Search and Data Mining (WSDM'18). 565-573.\n\nAttentive sequential models of latent intent for next item recommendation. Mehrab Md, Congzhe Tanjim, Ethan Su, Diane Benjamin, Liangjie Hu, Julian J Hong, Mcauley, Proceedings of the 29th International Conference on World Wide Web Companion. the 29th International Conference on World Wide Web CompanionMd. Mehrab Tanjim, Congzhe Su, Ethan Benjamin, Diane Hu, Liangjie Hong, and Julian J. McAuley. 2020. Attentive se- quential models of latent intent for next item recommendation. In Proceedings of the 29th International Conference on World Wide Web Companion. 2528-2534.\n\nPattern-enhanced contrastive policy learning network for sequential recommendation. Xiaohai Tong, Pengfei Wang, Chenliang Li, Long Xia, Shaozhang Niu, Proceedings of the 30th International Joint Conference on Artificial Intelligence. the 30th International Joint Conference on Artificial IntelligenceXiaohai Tong, Pengfei Wang, Chenliang Li, Long Xia, and ShaoZhang Niu. 2021. Pattern-enhanced contrastive policy learn- ing network for sequential recommendation. In Proceedings of the 30th International Joint Conference on Artificial Intel- ligence. 1593-1599.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS'17. the 31st International Conference on Neural Information Processing Systems (NeurIPS'17Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS'17). 6000-6010.\n\nCounterfactual data-augmented sequential recommendation. Zhenlei Wang, Jingsen Zhang, Hongteng Xu, Xu Chen, Yongfeng Zhang, Wayne Xin Zhao, Ji-Rong Wen, Proceedings of the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21. the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21Zhenlei Wang, Jingsen Zhang, Hongteng Xu, Xu Chen, Yongfeng Zhang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. Coun- terfactual data-augmented sequential recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21). 347-356.\n\nModeling multiple coexisting category-level intentions for next item recommendation. Yanan Xu, Yanmin Zhu, Jiadi Yu, ACM Trans. Inf. Syst. 39324Yanan Xu, Yanmin Zhu, and Jiadi Yu. 2021. Modeling multiple coexisting category-level intentions for next item recom- mendation. ACM Trans. Inf. Syst. 39, 3 (2021), 23:1-23:24.\n\nImproving sequential recommendation consistency with self-supervised imitation. Hongshen Xu Yuan, Yonghao Chen, Xiaofang Song, Zhuoye Zhao, Ding, Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI'21. the 30th International Joint Conference on Artificial Intelligence (IJCAI'21Xu Yuan, Hongshen Chen, Yonghao Song, Xiaofang Zhao, and Zhuoye Ding. 2021. Improving sequential recommenda- tion consistency with self-supervised imitation. In Proceedings of the 30th International Joint Conference on Artificial Intelligence (IJCAI'21). 3321-3327.\n\nCauseRec: Counterfactual user sequence synthesis for sequential recommendation. Shengyu Zhang, Dong Yao, Zhou Zhao, Tat-Seng Chua, Fei Wu, Proceedings of the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21. the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21Shengyu Zhang, Dong Yao, Zhou Zhao, Tat-Seng Chua, and Fei Wu. 2021. CauseRec: Counterfactual user sequence syn- thesis for sequential recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR'21). 367-377.\n\nFeature-level deeper self-attention network for sequential recommendation. Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Deqing Wang, Guanfeng Liu, Xiaofang Zhou, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial IntelligenceTingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S. Sheng, Jiajie Xu, Deqing Wang, Guanfeng Liu, and Xiaofang Zhou. 2019. Feature-level deeper self-attention network for sequential recommendation. In Proceedings of the 28th International Joint Conference on Artificial Intelligence. 4320-4326.\n\nMicro behaviors: A new perspective in e-commerce recommender systems. Meizi Zhou, Zhuoye Ding, Jiliang Tang, Dawei Yin, Proceedings of the 11th ACM International Conference on Web Search and Data Mining (WSDM'18. the 11th ACM International Conference on Web Search and Data Mining (WSDM'18Meizi Zhou, Zhuoye Ding, Jiliang Tang, and Dawei Yin. 2018. Micro behaviors: A new perspective in e-commerce recom- mender systems. In Proceedings of the 11th ACM International Conference on Web Search and Data Mining (WSDM'18). 727-735.\n\n. ACM Transactions on Intelligent Systems and Technology. 141Publication dateACM Transactions on Intelligent Systems and Technology, Vol. 14, No. 1, Article 14. Publication date: November 2022.\n", "annotations": {"author": "[{\"end\":117,\"start\":106},{\"end\":127,\"start\":118},{\"end\":170,\"start\":128},{\"end\":181,\"start\":171},{\"end\":211,\"start\":182},{\"end\":500,\"start\":212},{\"end\":621,\"start\":501}]", "publisher": null, "author_last_name": "[{\"end\":116,\"start\":114},{\"end\":126,\"start\":123},{\"end\":138,\"start\":135},{\"end\":180,\"start\":177},{\"end\":192,\"start\":188}]", "author_first_name": "[{\"end\":113,\"start\":106},{\"end\":122,\"start\":118},{\"end\":134,\"start\":128},{\"end\":176,\"start\":171},{\"end\":187,\"start\":182}]", "author_affiliation": "[{\"end\":499,\"start\":213},{\"end\":620,\"start\":502}]", "title": "[{\"end\":90,\"start\":1},{\"end\":711,\"start\":622}]", "venue": "[{\"end\":767,\"start\":713}]", "abstract": "[{\"end\":3332,\"start\":1485}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4560,\"start\":4541},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4869,\"start\":4840},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4889,\"start\":4869},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5006,\"start\":4978},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5115,\"start\":5095},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5337,\"start\":5314},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6064,\"start\":6048},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6249,\"start\":6232},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8150,\"start\":8131},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10344,\"start\":10325},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10787,\"start\":10767},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11673,\"start\":11657},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12034,\"start\":12015},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12059,\"start\":12042},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12335,\"start\":12318},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13995,\"start\":13976},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14194,\"start\":14173},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14848,\"start\":14828},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15177,\"start\":15161},{\"end\":15301,\"start\":15292},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15754,\"start\":15739},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16068,\"start\":16049},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16247,\"start\":16230},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17488,\"start\":17469},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17504,\"start\":17488},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17521,\"start\":17504},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17550,\"start\":17532},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17566,\"start\":17550},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17601,\"start\":17584},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18310,\"start\":18294},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18497,\"start\":18480},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18890,\"start\":18873},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19056,\"start\":19033},{\"end\":19240,\"start\":19234},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19258,\"start\":19242},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23659,\"start\":23636},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24371,\"start\":24348},{\"end\":25661,\"start\":25655},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28353,\"start\":28335},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":32689,\"start\":32671},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":39097,\"start\":39075},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":43961,\"start\":43942},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":44101,\"start\":44082},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":44291,\"start\":44272},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":44365,\"start\":44346},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":44475,\"start\":44454},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":44509,\"start\":44490},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":44539,\"start\":44520},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":44814,\"start\":44785},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":45094,\"start\":45075},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":45398,\"start\":45376},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":45679,\"start\":45664},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46338,\"start\":46321},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":46571,\"start\":46554},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":46808,\"start\":46791},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47338,\"start\":47317},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47529,\"start\":47509},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":47670,\"start\":47647},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":47770,\"start\":47746},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":47965,\"start\":47942},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":50998,\"start\":50975},{\"end\":58683,\"start\":58678}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":61887,\"start\":61839},{\"attributes\":{\"id\":\"fig_2\"},\"end\":62019,\"start\":61888},{\"attributes\":{\"id\":\"fig_3\"},\"end\":62296,\"start\":62020},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":62482,\"start\":62297},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":62626,\"start\":62483},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":63913,\"start\":62627},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":64964,\"start\":63914},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":65416,\"start\":64965},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":65838,\"start\":65417}]", "paragraph": "[{\"end\":4401,\"start\":3348},{\"end\":5996,\"start\":4403},{\"end\":7382,\"start\":5998},{\"end\":8630,\"start\":7384},{\"end\":9556,\"start\":8632},{\"end\":9816,\"start\":9573},{\"end\":10134,\"start\":9850},{\"end\":11802,\"start\":10136},{\"end\":13741,\"start\":11804},{\"end\":17903,\"start\":13783},{\"end\":21419,\"start\":17947},{\"end\":21914,\"start\":21439},{\"end\":22451,\"start\":22039},{\"end\":22827,\"start\":22453},{\"end\":23386,\"start\":22857},{\"end\":23830,\"start\":23388},{\"end\":24150,\"start\":23952},{\"end\":24204,\"start\":24152},{\"end\":24707,\"start\":24284},{\"end\":24787,\"start\":24754},{\"end\":24902,\"start\":24847},{\"end\":25509,\"start\":24962},{\"end\":25912,\"start\":25544},{\"end\":26571,\"start\":26022},{\"end\":27219,\"start\":26622},{\"end\":27588,\"start\":27273},{\"end\":28041,\"start\":27610},{\"end\":28503,\"start\":28072},{\"end\":28858,\"start\":28557},{\"end\":29550,\"start\":28860},{\"end\":31068,\"start\":29579},{\"end\":31845,\"start\":31070},{\"end\":32293,\"start\":31847},{\"end\":32657,\"start\":32295},{\"end\":32971,\"start\":32659},{\"end\":34567,\"start\":33023},{\"end\":34901,\"start\":34624},{\"end\":35572,\"start\":34964},{\"end\":35859,\"start\":35600},{\"end\":36376,\"start\":35936},{\"end\":36670,\"start\":36416},{\"end\":36781,\"start\":36694},{\"end\":37545,\"start\":36813},{\"end\":37872,\"start\":37593},{\"end\":38259,\"start\":37960},{\"end\":38912,\"start\":38286},{\"end\":39064,\"start\":38928},{\"end\":39829,\"start\":39066},{\"end\":40975,\"start\":39913},{\"end\":41746,\"start\":40991},{\"end\":43020,\"start\":41759},{\"end\":43728,\"start\":43044},{\"end\":43932,\"start\":43743},{\"end\":47190,\"start\":43934},{\"end\":48483,\"start\":47218},{\"end\":50210,\"start\":48527},{\"end\":52030,\"start\":50236},{\"end\":52623,\"start\":52060},{\"end\":54374,\"start\":52625},{\"end\":55303,\"start\":54437},{\"end\":56152,\"start\":55305},{\"end\":56552,\"start\":56154},{\"end\":57207,\"start\":56582},{\"end\":57810,\"start\":57209},{\"end\":58248,\"start\":57840},{\"end\":59999,\"start\":58250},{\"end\":61527,\"start\":60051},{\"end\":61838,\"start\":61529}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":22038,\"start\":21915},{\"attributes\":{\"id\":\"formula_1\"},\"end\":23951,\"start\":23831},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24283,\"start\":24205},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24753,\"start\":24708},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24846,\"start\":24788},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24961,\"start\":24903},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25543,\"start\":25510},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25934,\"start\":25913},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25963,\"start\":25934},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25992,\"start\":25963},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26021,\"start\":25992},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26621,\"start\":26572},{\"attributes\":{\"id\":\"formula_12\"},\"end\":27272,\"start\":27220},{\"attributes\":{\"id\":\"formula_13\"},\"end\":27609,\"start\":27589},{\"attributes\":{\"id\":\"formula_14\"},\"end\":28556,\"start\":28504},{\"attributes\":{\"id\":\"formula_15\"},\"end\":33022,\"start\":32972},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34623,\"start\":34568},{\"attributes\":{\"id\":\"formula_17\"},\"end\":34963,\"start\":34902},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35935,\"start\":35860},{\"attributes\":{\"id\":\"formula_19\"},\"end\":36415,\"start\":36377},{\"attributes\":{\"id\":\"formula_20\"},\"end\":36693,\"start\":36671},{\"attributes\":{\"id\":\"formula_21\"},\"end\":36812,\"start\":36782},{\"attributes\":{\"id\":\"formula_22\"},\"end\":37592,\"start\":37546},{\"attributes\":{\"id\":\"formula_23\"},\"end\":37959,\"start\":37873},{\"attributes\":{\"id\":\"formula_24\"},\"end\":39907,\"start\":39830},{\"attributes\":{\"id\":\"formula_25\"},\"end\":54436,\"start\":54375}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":43019,\"start\":43012},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":48625,\"start\":48618},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":48654,\"start\":48647},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":50723,\"start\":50716},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":55302,\"start\":55295},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":58247,\"start\":58240},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":58262,\"start\":58255}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3346,\"start\":3334},{\"attributes\":{\"n\":\"2\"},\"end\":9571,\"start\":9559},{\"attributes\":{\"n\":\"2.1\"},\"end\":9848,\"start\":9819},{\"attributes\":{\"n\":\"2.2\"},\"end\":13781,\"start\":13744},{\"attributes\":{\"n\":\"2.3\"},\"end\":17945,\"start\":17906},{\"attributes\":{\"n\":\"3\"},\"end\":21437,\"start\":21422},{\"attributes\":{\"n\":\"3.1\"},\"end\":22855,\"start\":22830},{\"attributes\":{\"n\":\"3.2\"},\"end\":28070,\"start\":28044},{\"attributes\":{\"n\":\"3.3\"},\"end\":29577,\"start\":29553},{\"attributes\":{\"n\":\"3.4\"},\"end\":35598,\"start\":35575},{\"attributes\":{\"n\":\"3.5\"},\"end\":38284,\"start\":38262},{\"attributes\":{\"n\":\"3.6\"},\"end\":38926,\"start\":38915},{\"end\":39911,\"start\":39909},{\"attributes\":{\"n\":\"4\"},\"end\":40989,\"start\":40978},{\"attributes\":{\"n\":\"4.1\"},\"end\":41757,\"start\":41749},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":43042,\"start\":43023},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":43741,\"start\":43731},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":47216,\"start\":47193},{\"attributes\":{\"n\":\"4.2\"},\"end\":48493,\"start\":48486},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":48525,\"start\":48496},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":50234,\"start\":50213},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":52058,\"start\":52033},{\"attributes\":{\"n\":\"4.2.4\"},\"end\":56580,\"start\":56555},{\"attributes\":{\"n\":\"4.2.5\"},\"end\":57838,\"start\":57813},{\"attributes\":{\"n\":\"4.2.6\"},\"end\":60019,\"start\":60002},{\"attributes\":{\"n\":\"5\"},\"end\":60049,\"start\":60022},{\"end\":61848,\"start\":61840},{\"end\":61897,\"start\":61889},{\"end\":62029,\"start\":62021},{\"end\":62311,\"start\":62298},{\"end\":62493,\"start\":62484},{\"end\":62637,\"start\":62628},{\"end\":63924,\"start\":63915},{\"end\":64975,\"start\":64966},{\"end\":65427,\"start\":65418}]", "table": "[{\"end\":62482,\"start\":62384},{\"end\":62626,\"start\":62610},{\"end\":63913,\"start\":62932},{\"end\":64964,\"start\":64034},{\"end\":65416,\"start\":65255},{\"end\":65838,\"start\":65693}]", "figure_caption": "[{\"end\":61887,\"start\":61850},{\"end\":62019,\"start\":61899},{\"end\":62296,\"start\":62031},{\"end\":62384,\"start\":62313},{\"end\":62610,\"start\":62495},{\"end\":62932,\"start\":62639},{\"end\":64034,\"start\":63926},{\"end\":65255,\"start\":64977},{\"end\":65693,\"start\":65429}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22515,\"start\":22507},{\"end\":56321,\"start\":56313},{\"end\":56336,\"start\":56328},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":56766,\"start\":56751},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":57222,\"start\":57214},{\"end\":58818,\"start\":58810}]", "bib_author_first_name": "[{\"end\":66471,\"start\":66464},{\"end\":66483,\"start\":66479},{\"end\":66491,\"start\":66489},{\"end\":66504,\"start\":66499},{\"end\":66515,\"start\":66510},{\"end\":66525,\"start\":66521},{\"end\":66538,\"start\":66532},{\"end\":66548,\"start\":66544},{\"end\":67167,\"start\":67162},{\"end\":67177,\"start\":67174},{\"end\":67193,\"start\":67185},{\"end\":67207,\"start\":67201},{\"end\":67217,\"start\":67212},{\"end\":67231,\"start\":67223},{\"end\":67719,\"start\":67714},{\"end\":67730,\"start\":67726},{\"end\":67740,\"start\":67737},{\"end\":67750,\"start\":67745},{\"end\":67763,\"start\":67758},{\"end\":68273,\"start\":68269},{\"end\":68287,\"start\":68280},{\"end\":68299,\"start\":68293},{\"end\":68309,\"start\":68306},{\"end\":68808,\"start\":68800},{\"end\":68818,\"start\":68815},{\"end\":68828,\"start\":68823},{\"end\":68839,\"start\":68834},{\"end\":69100,\"start\":69093},{\"end\":69115,\"start\":69107},{\"end\":69128,\"start\":69120},{\"end\":69138,\"start\":69133},{\"end\":69149,\"start\":69145},{\"end\":69162,\"start\":69154},{\"end\":69175,\"start\":69169},{\"end\":69187,\"start\":69181},{\"end\":69695,\"start\":69691},{\"end\":69709,\"start\":69701},{\"end\":69720,\"start\":69714},{\"end\":69735,\"start\":69726},{\"end\":69746,\"start\":69742},{\"end\":69757,\"start\":69753},{\"end\":69770,\"start\":69762},{\"end\":69781,\"start\":69777},{\"end\":69791,\"start\":69787},{\"end\":69804,\"start\":69798},{\"end\":70163,\"start\":70157},{\"end\":70178,\"start\":70172},{\"end\":70729,\"start\":70722},{\"end\":70740,\"start\":70734},{\"end\":71176,\"start\":71168},{\"end\":71188,\"start\":71181},{\"end\":71201,\"start\":71193},{\"end\":71217,\"start\":71208},{\"end\":71230,\"start\":71223},{\"end\":71246,\"start\":71238},{\"end\":71540,\"start\":71532},{\"end\":71549,\"start\":71545},{\"end\":71563,\"start\":71556},{\"end\":71578,\"start\":71571},{\"end\":71587,\"start\":71584},{\"end\":71600,\"start\":71592},{\"end\":72003,\"start\":72000},{\"end\":72011,\"start\":72008},{\"end\":72025,\"start\":72019},{\"end\":72036,\"start\":72031},{\"end\":72528,\"start\":72522},{\"end\":72547,\"start\":72537},{\"end\":73071,\"start\":73064},{\"end\":73083,\"start\":73080},{\"end\":73096,\"start\":73090},{\"end\":73600,\"start\":73590},{\"end\":73613,\"start\":73607},{\"end\":73615,\"start\":73614},{\"end\":73997,\"start\":73993},{\"end\":74009,\"start\":74002},{\"end\":74021,\"start\":74015},{\"end\":74036,\"start\":74028},{\"end\":74045,\"start\":74042},{\"end\":74055,\"start\":74052},{\"end\":74547,\"start\":74544},{\"end\":74558,\"start\":74552},{\"end\":74567,\"start\":74565},{\"end\":74579,\"start\":74573},{\"end\":74590,\"start\":74587},{\"end\":74602,\"start\":74596},{\"end\":75222,\"start\":75218},{\"end\":75233,\"start\":75228},{\"end\":75244,\"start\":75239},{\"end\":75702,\"start\":75697},{\"end\":75717,\"start\":75708},{\"end\":75728,\"start\":75722},{\"end\":75741,\"start\":75734},{\"end\":75752,\"start\":75748},{\"end\":75764,\"start\":75758},{\"end\":76224,\"start\":76219},{\"end\":76233,\"start\":76230},{\"end\":76243,\"start\":76238},{\"end\":76558,\"start\":76552},{\"end\":76569,\"start\":76564},{\"end\":76577,\"start\":76575},{\"end\":76590,\"start\":76584},{\"end\":76592,\"start\":76591},{\"end\":77186,\"start\":77181},{\"end\":77200,\"start\":77193},{\"end\":77215,\"start\":77209},{\"end\":77217,\"start\":77216},{\"end\":77230,\"start\":77226},{\"end\":77668,\"start\":77663},{\"end\":77680,\"start\":77676},{\"end\":77698,\"start\":77687},{\"end\":77700,\"start\":77699},{\"end\":78234,\"start\":78227},{\"end\":78247,\"start\":78241},{\"end\":78261,\"start\":78254},{\"end\":78879,\"start\":78874},{\"end\":78888,\"start\":78885},{\"end\":78902,\"start\":78896},{\"end\":78912,\"start\":78907},{\"end\":79208,\"start\":79201},{\"end\":79229,\"start\":79219},{\"end\":79249,\"start\":79243},{\"end\":79263,\"start\":79258},{\"end\":79735,\"start\":79728},{\"end\":79753,\"start\":79744},{\"end\":79773,\"start\":79769},{\"end\":79787,\"start\":79783},{\"end\":80281,\"start\":80274},{\"end\":80299,\"start\":80290},{\"end\":80319,\"start\":80315},{\"end\":80812,\"start\":80806},{\"end\":80821,\"start\":80818},{\"end\":80831,\"start\":80828},{\"end\":80845,\"start\":80837},{\"end\":81365,\"start\":81359},{\"end\":81378,\"start\":81371},{\"end\":81393,\"start\":81386},{\"end\":81403,\"start\":81399},{\"end\":81418,\"start\":81411},{\"end\":81432,\"start\":81425},{\"end\":81442,\"start\":81439},{\"end\":81907,\"start\":81902},{\"end\":81916,\"start\":81914},{\"end\":82393,\"start\":82387},{\"end\":82405,\"start\":82398},{\"end\":82419,\"start\":82414},{\"end\":82429,\"start\":82424},{\"end\":82448,\"start\":82440},{\"end\":82459,\"start\":82453},{\"end\":82461,\"start\":82460},{\"end\":82978,\"start\":82971},{\"end\":82992,\"start\":82985},{\"end\":83008,\"start\":82999},{\"end\":83017,\"start\":83013},{\"end\":83032,\"start\":83023},{\"end\":83483,\"start\":83477},{\"end\":83497,\"start\":83493},{\"end\":83511,\"start\":83507},{\"end\":83525,\"start\":83520},{\"end\":83542,\"start\":83537},{\"end\":83555,\"start\":83550},{\"end\":83557,\"start\":83556},{\"end\":83571,\"start\":83565},{\"end\":83585,\"start\":83580},{\"end\":84129,\"start\":84122},{\"end\":84143,\"start\":84136},{\"end\":84159,\"start\":84151},{\"end\":84166,\"start\":84164},{\"end\":84181,\"start\":84173},{\"end\":84194,\"start\":84189},{\"end\":84198,\"start\":84195},{\"end\":84212,\"start\":84205},{\"end\":84833,\"start\":84828},{\"end\":84844,\"start\":84838},{\"end\":84855,\"start\":84850},{\"end\":85153,\"start\":85145},{\"end\":85170,\"start\":85163},{\"end\":85185,\"start\":85177},{\"end\":85198,\"start\":85192},{\"end\":85734,\"start\":85727},{\"end\":85746,\"start\":85742},{\"end\":85756,\"start\":85752},{\"end\":85771,\"start\":85763},{\"end\":85781,\"start\":85778},{\"end\":86380,\"start\":86372},{\"end\":86396,\"start\":86388},{\"end\":86409,\"start\":86403},{\"end\":86421,\"start\":86415},{\"end\":86423,\"start\":86422},{\"end\":86437,\"start\":86431},{\"end\":86448,\"start\":86442},{\"end\":86463,\"start\":86455},{\"end\":86477,\"start\":86469},{\"end\":87004,\"start\":86999},{\"end\":87017,\"start\":87011},{\"end\":87031,\"start\":87024},{\"end\":87043,\"start\":87038}]", "bib_author_last_name": "[{\"end\":66477,\"start\":66472},{\"end\":66487,\"start\":66484},{\"end\":66497,\"start\":66492},{\"end\":66508,\"start\":66505},{\"end\":66519,\"start\":66516},{\"end\":66530,\"start\":66526},{\"end\":66542,\"start\":66539},{\"end\":66551,\"start\":66549},{\"end\":67172,\"start\":67168},{\"end\":67183,\"start\":67178},{\"end\":67199,\"start\":67194},{\"end\":67210,\"start\":67208},{\"end\":67221,\"start\":67218},{\"end\":67234,\"start\":67232},{\"end\":67724,\"start\":67720},{\"end\":67735,\"start\":67731},{\"end\":67743,\"start\":67741},{\"end\":67756,\"start\":67751},{\"end\":67766,\"start\":67764},{\"end\":68278,\"start\":68274},{\"end\":68291,\"start\":68288},{\"end\":68304,\"start\":68300},{\"end\":68313,\"start\":68310},{\"end\":68813,\"start\":68809},{\"end\":68821,\"start\":68819},{\"end\":68832,\"start\":68829},{\"end\":68844,\"start\":68840},{\"end\":69105,\"start\":69101},{\"end\":69118,\"start\":69116},{\"end\":69131,\"start\":69129},{\"end\":69143,\"start\":69139},{\"end\":69152,\"start\":69150},{\"end\":69167,\"start\":69163},{\"end\":69179,\"start\":69176},{\"end\":69190,\"start\":69188},{\"end\":69699,\"start\":69696},{\"end\":69712,\"start\":69710},{\"end\":69724,\"start\":69721},{\"end\":69740,\"start\":69736},{\"end\":69751,\"start\":69747},{\"end\":69760,\"start\":69758},{\"end\":69775,\"start\":69771},{\"end\":69785,\"start\":69782},{\"end\":69796,\"start\":69792},{\"end\":69808,\"start\":69805},{\"end\":70170,\"start\":70164},{\"end\":70185,\"start\":70179},{\"end\":70732,\"start\":70730},{\"end\":70748,\"start\":70741},{\"end\":71179,\"start\":71177},{\"end\":71191,\"start\":71189},{\"end\":71206,\"start\":71202},{\"end\":71221,\"start\":71218},{\"end\":71236,\"start\":71231},{\"end\":71251,\"start\":71247},{\"end\":71543,\"start\":71541},{\"end\":71554,\"start\":71550},{\"end\":71569,\"start\":71564},{\"end\":71582,\"start\":71579},{\"end\":71590,\"start\":71588},{\"end\":71605,\"start\":71601},{\"end\":72006,\"start\":72004},{\"end\":72017,\"start\":72012},{\"end\":72029,\"start\":72026},{\"end\":72045,\"start\":72037},{\"end\":72535,\"start\":72529},{\"end\":72559,\"start\":72548},{\"end\":73078,\"start\":73072},{\"end\":73088,\"start\":73084},{\"end\":73104,\"start\":73097},{\"end\":73605,\"start\":73601},{\"end\":73623,\"start\":73616},{\"end\":74000,\"start\":73998},{\"end\":74013,\"start\":74010},{\"end\":74026,\"start\":74022},{\"end\":74040,\"start\":74037},{\"end\":74050,\"start\":74046},{\"end\":74058,\"start\":74056},{\"end\":74550,\"start\":74548},{\"end\":74563,\"start\":74559},{\"end\":74571,\"start\":74568},{\"end\":74585,\"start\":74580},{\"end\":74594,\"start\":74591},{\"end\":74607,\"start\":74603},{\"end\":75226,\"start\":75223},{\"end\":75237,\"start\":75234},{\"end\":75249,\"start\":75245},{\"end\":75706,\"start\":75703},{\"end\":75720,\"start\":75718},{\"end\":75732,\"start\":75729},{\"end\":75746,\"start\":75742},{\"end\":75756,\"start\":75753},{\"end\":75770,\"start\":75765},{\"end\":76228,\"start\":76225},{\"end\":76236,\"start\":76234},{\"end\":76248,\"start\":76244},{\"end\":76562,\"start\":76559},{\"end\":76573,\"start\":76570},{\"end\":76582,\"start\":76578},{\"end\":76595,\"start\":76593},{\"end\":77191,\"start\":77187},{\"end\":77207,\"start\":77201},{\"end\":77224,\"start\":77218},{\"end\":77239,\"start\":77231},{\"end\":77674,\"start\":77669},{\"end\":77685,\"start\":77681},{\"end\":77708,\"start\":77701},{\"end\":78239,\"start\":78235},{\"end\":78252,\"start\":78248},{\"end\":78266,\"start\":78262},{\"end\":78883,\"start\":78880},{\"end\":78894,\"start\":78889},{\"end\":78905,\"start\":78903},{\"end\":78917,\"start\":78913},{\"end\":79217,\"start\":79209},{\"end\":79241,\"start\":79230},{\"end\":79256,\"start\":79250},{\"end\":79273,\"start\":79264},{\"end\":79742,\"start\":79736},{\"end\":79767,\"start\":79754},{\"end\":79781,\"start\":79774},{\"end\":79802,\"start\":79788},{\"end\":80288,\"start\":80282},{\"end\":80313,\"start\":80300},{\"end\":80334,\"start\":80320},{\"end\":80816,\"start\":80813},{\"end\":80826,\"start\":80822},{\"end\":80835,\"start\":80832},{\"end\":80850,\"start\":80846},{\"end\":81369,\"start\":81366},{\"end\":81384,\"start\":81379},{\"end\":81397,\"start\":81394},{\"end\":81409,\"start\":81404},{\"end\":81423,\"start\":81419},{\"end\":81437,\"start\":81433},{\"end\":81445,\"start\":81443},{\"end\":81912,\"start\":81908},{\"end\":81921,\"start\":81917},{\"end\":82396,\"start\":82394},{\"end\":82412,\"start\":82406},{\"end\":82422,\"start\":82420},{\"end\":82438,\"start\":82430},{\"end\":82451,\"start\":82449},{\"end\":82466,\"start\":82462},{\"end\":82475,\"start\":82468},{\"end\":82983,\"start\":82979},{\"end\":82997,\"start\":82993},{\"end\":83011,\"start\":83009},{\"end\":83021,\"start\":83018},{\"end\":83036,\"start\":83033},{\"end\":83491,\"start\":83484},{\"end\":83505,\"start\":83498},{\"end\":83518,\"start\":83512},{\"end\":83535,\"start\":83526},{\"end\":83548,\"start\":83543},{\"end\":83563,\"start\":83558},{\"end\":83578,\"start\":83572},{\"end\":83596,\"start\":83586},{\"end\":84134,\"start\":84130},{\"end\":84149,\"start\":84144},{\"end\":84162,\"start\":84160},{\"end\":84171,\"start\":84167},{\"end\":84187,\"start\":84182},{\"end\":84203,\"start\":84199},{\"end\":84216,\"start\":84213},{\"end\":84836,\"start\":84834},{\"end\":84848,\"start\":84845},{\"end\":84858,\"start\":84856},{\"end\":85161,\"start\":85154},{\"end\":85175,\"start\":85171},{\"end\":85190,\"start\":85186},{\"end\":85203,\"start\":85199},{\"end\":85209,\"start\":85205},{\"end\":85740,\"start\":85735},{\"end\":85750,\"start\":85747},{\"end\":85761,\"start\":85757},{\"end\":85776,\"start\":85772},{\"end\":85784,\"start\":85782},{\"end\":86386,\"start\":86381},{\"end\":86401,\"start\":86397},{\"end\":86413,\"start\":86410},{\"end\":86429,\"start\":86424},{\"end\":86440,\"start\":86438},{\"end\":86453,\"start\":86449},{\"end\":86467,\"start\":86464},{\"end\":86482,\"start\":86478},{\"end\":87009,\"start\":87005},{\"end\":87022,\"start\":87018},{\"end\":87036,\"start\":87032},{\"end\":87047,\"start\":87044}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":235658226},\"end\":67066,\"start\":66410},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":208284469},\"end\":67640,\"start\":67068},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":155099871},\"end\":68214,\"start\":67642},{\"attributes\":{\"id\":\"b3\"},\"end\":68737,\"start\":68216},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":222296312},\"end\":69036,\"start\":68739},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51822136},\"end\":69632,\"start\":69038},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211069741},\"end\":70080,\"start\":69634},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5575601},\"end\":70638,\"start\":70082},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9124261},\"end\":71101,\"start\":70640},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206744664},\"end\":71498,\"start\":71103},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13907106},\"end\":71922,\"start\":71500},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":209515672},\"end\":72442,\"start\":71924},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1159769},\"end\":72993,\"start\":72444},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":207204749},\"end\":73546,\"start\":72995},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52127932},\"end\":73944,\"start\":73548},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":21066930},\"end\":74435,\"start\":73946},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":50768534},\"end\":75119,\"start\":74437},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":221784706},\"end\":75610,\"start\":75121},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":232135174},\"end\":76143,\"start\":75612},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14093453},\"end\":76449,\"start\":76145},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":233481483},\"end\":77115,\"start\":76451},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":757461},\"end\":77593,\"start\":77117},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1998416},\"end\":78113,\"start\":77595},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":219636253},\"end\":78795,\"start\":78115},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14800684},\"end\":79110,\"start\":78797},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10174110},\"end\":79667,\"start\":79112},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10795036},\"end\":80202,\"start\":79669},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":207178809},\"end\":80703,\"start\":80204},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":237100856},\"end\":81287,\"start\":80705},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":231951435},\"end\":81817,\"start\":81289},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":39847715},\"end\":82310,\"start\":81819},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":215812033},\"end\":82885,\"start\":82312},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":237100943},\"end\":83448,\"start\":82887},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13756489},\"end\":84063,\"start\":83450},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":235792483},\"end\":84741,\"start\":84065},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":235571722},\"end\":85063,\"start\":84743},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":235658790},\"end\":85645,\"start\":85065},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":235792358},\"end\":86295,\"start\":85647},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":199465766},\"end\":86927,\"start\":86297},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":32370905},\"end\":87455,\"start\":86929},{\"attributes\":{\"id\":\"b40\"},\"end\":87650,\"start\":87457}]", "bib_title": "[{\"end\":66462,\"start\":66410},{\"end\":67160,\"start\":67068},{\"end\":67712,\"start\":67642},{\"end\":68267,\"start\":68216},{\"end\":68798,\"start\":68739},{\"end\":69091,\"start\":69038},{\"end\":69689,\"start\":69634},{\"end\":70155,\"start\":70082},{\"end\":70720,\"start\":70640},{\"end\":71166,\"start\":71103},{\"end\":71530,\"start\":71500},{\"end\":71998,\"start\":71924},{\"end\":72520,\"start\":72444},{\"end\":73062,\"start\":72995},{\"end\":73588,\"start\":73548},{\"end\":73991,\"start\":73946},{\"end\":74542,\"start\":74437},{\"end\":75216,\"start\":75121},{\"end\":75695,\"start\":75612},{\"end\":76217,\"start\":76145},{\"end\":76550,\"start\":76451},{\"end\":77179,\"start\":77117},{\"end\":77661,\"start\":77595},{\"end\":78225,\"start\":78115},{\"end\":78872,\"start\":78797},{\"end\":79199,\"start\":79112},{\"end\":79726,\"start\":79669},{\"end\":80272,\"start\":80204},{\"end\":80804,\"start\":80705},{\"end\":81357,\"start\":81289},{\"end\":81900,\"start\":81819},{\"end\":82385,\"start\":82312},{\"end\":82969,\"start\":82887},{\"end\":83475,\"start\":83450},{\"end\":84120,\"start\":84065},{\"end\":84826,\"start\":84743},{\"end\":85143,\"start\":85065},{\"end\":85725,\"start\":85647},{\"end\":86370,\"start\":86297},{\"end\":86997,\"start\":86929}]", "bib_author": "[{\"end\":66479,\"start\":66464},{\"end\":66489,\"start\":66479},{\"end\":66499,\"start\":66489},{\"end\":66510,\"start\":66499},{\"end\":66521,\"start\":66510},{\"end\":66532,\"start\":66521},{\"end\":66544,\"start\":66532},{\"end\":66553,\"start\":66544},{\"end\":67174,\"start\":67162},{\"end\":67185,\"start\":67174},{\"end\":67201,\"start\":67185},{\"end\":67212,\"start\":67201},{\"end\":67223,\"start\":67212},{\"end\":67236,\"start\":67223},{\"end\":67726,\"start\":67714},{\"end\":67737,\"start\":67726},{\"end\":67745,\"start\":67737},{\"end\":67758,\"start\":67745},{\"end\":67768,\"start\":67758},{\"end\":68280,\"start\":68269},{\"end\":68293,\"start\":68280},{\"end\":68306,\"start\":68293},{\"end\":68315,\"start\":68306},{\"end\":68815,\"start\":68800},{\"end\":68823,\"start\":68815},{\"end\":68834,\"start\":68823},{\"end\":68846,\"start\":68834},{\"end\":69107,\"start\":69093},{\"end\":69120,\"start\":69107},{\"end\":69133,\"start\":69120},{\"end\":69145,\"start\":69133},{\"end\":69154,\"start\":69145},{\"end\":69169,\"start\":69154},{\"end\":69181,\"start\":69169},{\"end\":69192,\"start\":69181},{\"end\":69701,\"start\":69691},{\"end\":69714,\"start\":69701},{\"end\":69726,\"start\":69714},{\"end\":69742,\"start\":69726},{\"end\":69753,\"start\":69742},{\"end\":69762,\"start\":69753},{\"end\":69777,\"start\":69762},{\"end\":69787,\"start\":69777},{\"end\":69798,\"start\":69787},{\"end\":69810,\"start\":69798},{\"end\":70172,\"start\":70157},{\"end\":70187,\"start\":70172},{\"end\":70734,\"start\":70722},{\"end\":70750,\"start\":70734},{\"end\":71181,\"start\":71168},{\"end\":71193,\"start\":71181},{\"end\":71208,\"start\":71193},{\"end\":71223,\"start\":71208},{\"end\":71238,\"start\":71223},{\"end\":71253,\"start\":71238},{\"end\":71545,\"start\":71532},{\"end\":71556,\"start\":71545},{\"end\":71571,\"start\":71556},{\"end\":71584,\"start\":71571},{\"end\":71592,\"start\":71584},{\"end\":71607,\"start\":71592},{\"end\":72008,\"start\":72000},{\"end\":72019,\"start\":72008},{\"end\":72031,\"start\":72019},{\"end\":72047,\"start\":72031},{\"end\":72537,\"start\":72522},{\"end\":72561,\"start\":72537},{\"end\":73080,\"start\":73064},{\"end\":73090,\"start\":73080},{\"end\":73106,\"start\":73090},{\"end\":73607,\"start\":73590},{\"end\":73625,\"start\":73607},{\"end\":74002,\"start\":73993},{\"end\":74015,\"start\":74002},{\"end\":74028,\"start\":74015},{\"end\":74042,\"start\":74028},{\"end\":74052,\"start\":74042},{\"end\":74060,\"start\":74052},{\"end\":74552,\"start\":74544},{\"end\":74565,\"start\":74552},{\"end\":74573,\"start\":74565},{\"end\":74587,\"start\":74573},{\"end\":74596,\"start\":74587},{\"end\":74609,\"start\":74596},{\"end\":75228,\"start\":75218},{\"end\":75239,\"start\":75228},{\"end\":75251,\"start\":75239},{\"end\":75708,\"start\":75697},{\"end\":75722,\"start\":75708},{\"end\":75734,\"start\":75722},{\"end\":75748,\"start\":75734},{\"end\":75758,\"start\":75748},{\"end\":75772,\"start\":75758},{\"end\":76230,\"start\":76219},{\"end\":76238,\"start\":76230},{\"end\":76250,\"start\":76238},{\"end\":76564,\"start\":76552},{\"end\":76575,\"start\":76564},{\"end\":76584,\"start\":76575},{\"end\":76597,\"start\":76584},{\"end\":77193,\"start\":77181},{\"end\":77209,\"start\":77193},{\"end\":77226,\"start\":77209},{\"end\":77241,\"start\":77226},{\"end\":77676,\"start\":77663},{\"end\":77687,\"start\":77676},{\"end\":77710,\"start\":77687},{\"end\":78241,\"start\":78227},{\"end\":78254,\"start\":78241},{\"end\":78268,\"start\":78254},{\"end\":78885,\"start\":78874},{\"end\":78896,\"start\":78885},{\"end\":78907,\"start\":78896},{\"end\":78919,\"start\":78907},{\"end\":79219,\"start\":79201},{\"end\":79243,\"start\":79219},{\"end\":79258,\"start\":79243},{\"end\":79275,\"start\":79258},{\"end\":79744,\"start\":79728},{\"end\":79769,\"start\":79744},{\"end\":79783,\"start\":79769},{\"end\":79804,\"start\":79783},{\"end\":80290,\"start\":80274},{\"end\":80315,\"start\":80290},{\"end\":80336,\"start\":80315},{\"end\":80818,\"start\":80806},{\"end\":80828,\"start\":80818},{\"end\":80837,\"start\":80828},{\"end\":80852,\"start\":80837},{\"end\":81371,\"start\":81359},{\"end\":81386,\"start\":81371},{\"end\":81399,\"start\":81386},{\"end\":81411,\"start\":81399},{\"end\":81425,\"start\":81411},{\"end\":81439,\"start\":81425},{\"end\":81447,\"start\":81439},{\"end\":81914,\"start\":81902},{\"end\":81923,\"start\":81914},{\"end\":82398,\"start\":82387},{\"end\":82414,\"start\":82398},{\"end\":82424,\"start\":82414},{\"end\":82440,\"start\":82424},{\"end\":82453,\"start\":82440},{\"end\":82468,\"start\":82453},{\"end\":82477,\"start\":82468},{\"end\":82985,\"start\":82971},{\"end\":82999,\"start\":82985},{\"end\":83013,\"start\":82999},{\"end\":83023,\"start\":83013},{\"end\":83038,\"start\":83023},{\"end\":83493,\"start\":83477},{\"end\":83507,\"start\":83493},{\"end\":83520,\"start\":83507},{\"end\":83537,\"start\":83520},{\"end\":83550,\"start\":83537},{\"end\":83565,\"start\":83550},{\"end\":83580,\"start\":83565},{\"end\":83598,\"start\":83580},{\"end\":84136,\"start\":84122},{\"end\":84151,\"start\":84136},{\"end\":84164,\"start\":84151},{\"end\":84173,\"start\":84164},{\"end\":84189,\"start\":84173},{\"end\":84205,\"start\":84189},{\"end\":84218,\"start\":84205},{\"end\":84838,\"start\":84828},{\"end\":84850,\"start\":84838},{\"end\":84860,\"start\":84850},{\"end\":85163,\"start\":85145},{\"end\":85177,\"start\":85163},{\"end\":85192,\"start\":85177},{\"end\":85205,\"start\":85192},{\"end\":85211,\"start\":85205},{\"end\":85742,\"start\":85727},{\"end\":85752,\"start\":85742},{\"end\":85763,\"start\":85752},{\"end\":85778,\"start\":85763},{\"end\":85786,\"start\":85778},{\"end\":86388,\"start\":86372},{\"end\":86403,\"start\":86388},{\"end\":86415,\"start\":86403},{\"end\":86431,\"start\":86415},{\"end\":86442,\"start\":86431},{\"end\":86455,\"start\":86442},{\"end\":86469,\"start\":86455},{\"end\":86484,\"start\":86469},{\"end\":87011,\"start\":86999},{\"end\":87024,\"start\":87011},{\"end\":87038,\"start\":87024},{\"end\":87049,\"start\":87038}]", "bib_venue": "[{\"end\":66778,\"start\":66674},{\"end\":67373,\"start\":67313},{\"end\":67963,\"start\":67874},{\"end\":68466,\"start\":68399},{\"end\":69361,\"start\":69285},{\"end\":70392,\"start\":70298},{\"end\":70891,\"start\":70829},{\"end\":71726,\"start\":71675},{\"end\":72208,\"start\":72136},{\"end\":72750,\"start\":72664},{\"end\":73305,\"start\":73214},{\"end\":73766,\"start\":73704},{\"end\":74211,\"start\":74144},{\"end\":74810,\"start\":74718},{\"end\":75382,\"start\":75325},{\"end\":75891,\"start\":75840},{\"end\":76822,\"start\":76718},{\"end\":77372,\"start\":77315},{\"end\":77879,\"start\":77803},{\"end\":78493,\"start\":78389},{\"end\":79406,\"start\":79349},{\"end\":79959,\"start\":79890},{\"end\":80471,\"start\":80412},{\"end\":81021,\"start\":80945},{\"end\":81566,\"start\":81515},{\"end\":82092,\"start\":82016},{\"end\":82616,\"start\":82555},{\"end\":83187,\"start\":83121},{\"end\":83787,\"start\":83701},{\"end\":84443,\"start\":84339},{\"end\":85380,\"start\":85304},{\"end\":86011,\"start\":85907},{\"end\":86633,\"start\":86567},{\"end\":87218,\"start\":87142},{\"end\":66672,\"start\":66553},{\"end\":67311,\"start\":67236},{\"end\":67872,\"start\":67768},{\"end\":68397,\"start\":68315},{\"end\":68866,\"start\":68846},{\"end\":69283,\"start\":69192},{\"end\":69837,\"start\":69810},{\"end\":70296,\"start\":70187},{\"end\":70827,\"start\":70750},{\"end\":71280,\"start\":71253},{\"end\":71673,\"start\":71607},{\"end\":72134,\"start\":72047},{\"end\":72662,\"start\":72561},{\"end\":73212,\"start\":73106},{\"end\":73702,\"start\":73625},{\"end\":74142,\"start\":74060},{\"end\":74716,\"start\":74609},{\"end\":75323,\"start\":75251},{\"end\":75838,\"start\":75772},{\"end\":76277,\"start\":76250},{\"end\":76716,\"start\":76597},{\"end\":77313,\"start\":77241},{\"end\":77801,\"start\":77710},{\"end\":78387,\"start\":78268},{\"end\":78936,\"start\":78919},{\"end\":79347,\"start\":79275},{\"end\":79888,\"start\":79804},{\"end\":80410,\"start\":80336},{\"end\":80943,\"start\":80852},{\"end\":81513,\"start\":81447},{\"end\":82014,\"start\":81923},{\"end\":82553,\"start\":82477},{\"end\":83119,\"start\":83038},{\"end\":83699,\"start\":83598},{\"end\":84337,\"start\":84218},{\"end\":84880,\"start\":84860},{\"end\":85302,\"start\":85211},{\"end\":85905,\"start\":85786},{\"end\":86565,\"start\":86484},{\"end\":87140,\"start\":87049},{\"end\":87513,\"start\":87459}]"}}}, "year": 2023, "month": 12, "day": 17}