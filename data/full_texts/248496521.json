{"id": 248496521, "updated": "2023-11-05 16:50:03.782", "metadata": {"title": "COUCH: Towards Controllable Human-Chair Interactions", "authors": "[{\"first\":\"Xiaohan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Bharat\",\"last\":\"Bhatnagar\",\"middle\":[\"Lal\"]},{\"first\":\"Vladimir\",\"last\":\"Guzov\",\"middle\":[]},{\"first\":\"Sebastian\",\"last\":\"Starke\",\"middle\":[]},{\"first\":\"Gerard\",\"last\":\"Pons-Moll\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion. In this work, we study the problem of synthesizing scene interactions conditioned on different contact positions on the object. As a testbed to investigate this new problem, we focus on human-chair interaction as one of the most common actions which exhibit large variability in terms of contacts. We propose a novel synthesis framework COUCH that plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the COUCH Dataset. Our method shows significant quantitative and qualitative improvements over existing methods for human-object interactions. More importantly, our method enables control of the motion through user-specified or automatically predicted contacts.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.00541", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ZhangBSGP22", "doi": "10.48550/arxiv.2205.00541"}}, "content": {"source": {"pdf_hash": "2a37b307daef48766eeed8409003066b14955c3b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2205.00541v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "81388d0ce9c09623b9184316163491d8d49380ae", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2a37b307daef48766eeed8409003066b14955c3b.txt", "contents": "\nCOUCH: Towards Controllable Human-Chair Interactions\n\n\nXiaohan Zhang \nUniversity of T\u00fcbingen\nGermany\n\nMax Planck Institute for Informatics\nSaarland Informatics Campus\nGermany\n\nBharat Lal Bhatnagar \nUniversity of T\u00fcbingen\nGermany\n\nMax Planck Institute for Informatics\nSaarland Informatics Campus\nGermany\n\nVladimir Guzov \nUniversity of T\u00fcbingen\nGermany\n\nMax Planck Institute for Informatics\nSaarland Informatics Campus\nGermany\n\nSebastian Starke \nElectronic Arts\n\n\nUniversity of Edinburgh\nUnited Kingdom\n\nGerard Pons-Moll \nUniversity of T\u00fcbingen\nGermany\n\nMax Planck Institute for Informatics\nSaarland Informatics Campus\nGermany\n\nCOUCH: Towards Controllable Human-Chair Interactions\n\nFig. 1: We present COUCH. A dataset and model to synthesizes controllable, contact-based human-chair interactions.Abstract. Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion. In this work, we study the problem of synthesizing scene interactions conditioned on different contact positions on the object. As a testbed to investigate this new problem, we focus on human-chair interaction as one of the most common actions which exhibit large variability in terms of contacts. We propose a novel synthesis framework COUCH that plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the COUCH Dataset. Our method shows significant quantitative and qualitative improvements over existing methods for human-object interactions. More importantly, our method enables control of the motion through user-specified or automatically predicted contacts. Our dataset, model and code will be available at [1].2 Zhang et al.\n Fig. 1\n: We present COUCH. A dataset and model to synthesizes controllable, contact-based human-chair interactions.\n\nAbstract. Humans interact with an object in many different ways by making contact at different locations, creating a highly complex motion space that can be difficult to learn, particularly when synthesizing such human interactions in a controllable manner. Existing works on synthesizing human scene interaction focus on the high-level control of action but do not consider the fine-grained control of motion. In this work, we study the problem of synthesizing scene interactions conditioned on different contact positions on the object. As a testbed to investigate this new problem, we focus on human-chair interaction as one of the most common actions which exhibit large variability in terms of contacts. We propose a novel synthesis framework COUCH that plans ahead the motion by predicting contact-aware control signals of the hands, which are then used to synthesize contact-conditioned interactions. Furthermore, we contribute a large human-chair interaction dataset with clean annotations, the COUCH Dataset. Our method shows significant quantitative and qualitative improvements over existing methods for human-object interactions. More importantly, our method enables control of the motion through user-specified or automatically predicted contacts. Our dataset, model and code will be available at [1].\n\n\nIntroduction\n\nTo synthesize realistic virtual humans which can achieve goals and act upon the environment, reasoning about the interactions and in turn contacts, is necessary. Reaching a goal, like sitting on a chair, is often preceded by intentional contact with the hand to support the body. In this work we investigate a motion synthesis method which exploits predictable contact to achieve more control and diversity over the animations.\n\nAlthough most applications in VR/AR, digital content creation and robotics require synthesizing motion within the environment, it is not considered in the majority of works in human motion synthesis [40,41,39,33]. Recent work does take the environment into account but is limited to synthesizing static poses [23,58]. Synthesising dynamic human motion coherent with the environment is a substantially harder task [47,22,26,48,49] and recent works show promising results. However, these methods do not reason about intentional contacts with the environment, and can not be controlled with user provided contacts.\n\nThus, in this work, we investigate a new problem: synthesizing human motion conditioned on contact positions on the object to allow for controllable movement variations. As a testbed to investigate this new problem, we focus on human-chair interactions as one of the most common actions, which are of crucial importance for ergonomics, avatars in virtual reality or video game animation. Contact-driven motion synthesis is a more challenging learning problem compared to conditioning only on coarse object geometry [47,22]. First, the human needs to approach the chair differently depending on the contacts, regardless of the starting position, walking around it if necessary. Second, a chair can be approached and contacted in many different ways; we can directly sit without using our hands, or we can first support the body weight using the left/right or both hands with different parts of the chair. Furthermore, individual styled freeinteractions can be modelled such as leaning back, stretching legs, using hands to support the head, and so on.\n\nContact driven motion allows for providing more detailed instructions to the virtual human such as approaching to sit on the chair, while supporting the body with the left hand and placing it on the armrest, as illustrated in Figure 1. Given the contact and the goal, the full-body needs to coordinate at run-time to achieve a plausible sequence of pose transitions. Intuitively, this emulates our planning of motion as real humans: we plan in terms of goals and intermediate object contacts to reach; the full-body then moves to follow such desired trajectories.\n\nTo this end, we propose COUCH, a method for controllable contact driven human-chair interactions, which is composed of two core components: 1) Con-trolNet is responsible for motion planning by predicting the future control signal of the body limbs which guides the future body movement. Our spatial-temporal control signal consists of dynamic trajectories of the hands towards the contact points and the local phase, an auxiliary continuous variable that encodes the temporal information of a limb during a particular movement (e.g. the left hand reaching an armrest). 2) PoseNet conditions on the predicted control signal to synthesise motion that follows the dynamic trajectories, ensuring the con-tact point is reached. At runtime, COUCH can operate in two modes. First, in an interactive mode where the user specifies the desired contact points on the target object. Second, in a generative mode where COUCH can automatically sample diverse intentional contacts on the object with a novel neural network called ContactNet. Training and evaluating COUCH calls for a dataset of rich and accurate human chair interactions. Existing interaction 3D datasets [47] are captured with Inertial Sensors, and hence do not capture the real body motion and the contacts with the real chair geometry -instead synthetic chairs are fit to the avatar as post-process in those works. Hence, to jointly capture real human-chair interactions with fine-grained contacts, we fit the SMPL model [37] and scanned chair models to data obtained from multiple Kinects and IMUs. The dataset (the COUCH dataset, Table 4) consists of 3 hours (over 500 sequences) of motion capture (MoCap) on human-chair interactions. Moreover it features multiple subjects, accurately captured contacts with registered chairs, and annotation on the type of hand contact. Our experiments demonstrate that COUCH runs in real-time at 30 fps, COUCH generalizes across chairs of varied geometry, and different starting positions relative to the chair. Compared to SoTA models (trained on the same data) adapted to incorporate contacts, our method significantly outperforms them in terms of control by improving the average contact distance by 55%.\n\nThe contributions of our work can be summarized as follows:\n\n-We propose COUCH, the first method for synthesizing controllable contactbased human-chair interactions. Given the same input control, the COUCH model can achieve diverse sitting motions. By specifying different control signals, the user enables control over the style of interaction with the object. Results show our method outperforms the state of the art both qualitatively and quantitatively. -To train COUCH, we captured a large-scale MoCap dataset consisting of 3 hours (over 500 sequences) of human interacting with chairs different styles of sitting and free movements. The dataset features multiple subject, real chair geometry, accurately annotated hand contacts, and RGB-D images. -To stimulate further research in controllable synthesis of human motion, we will release the COUCH model and dataset.\n\n\nRelated Work\n\nScene agnostic human motion prediction. Synthesizing realistic human motion has drawn much attention from the computer vision and graphics communities. However, many methods do not take the scene into account. Existing methods on short (\u223c1 sec) [40,20,41,16,54,51,5,55,24] and long (>1 min) [28,18] term 3D human motion prediction aim to produce realistic-looking human motion (typically walking and its variants). There also exists work on conditional motion generation based on music [33]. These methods have two major limitations, i) except for work that use generative models [36,21,19,6,25], these methods are largely deterministic and cannot be used to generate diverse motions and ii) this body of work is unfortunately agnostic to scene geometry [32,39], which is critical to model human scene interactions. Our method on the other hand can generate realistic motion and interactions, taking into account the 3D scene.\n\nAffordance and Static Scene Interactions. Although the focus of our work is to model human-scene interactions over time, we find the works predicting static affordances in a 3D scene [34] relevant. This branch of work aims at predicting static humans in a scene [58,57,23] that satisfies the scene constraints. More recently there have been attempts to model fine-grained interactions (contacts) between the hand and objects [15,50,9,31]. The aforementioned methods focus on predicting static humans / human poses that satisfy the scene constraints in case of affordances or grasping an object in case of hand-object interactions. But these methods cannot produce a full sequence of human motion and interaction with the scene. Ours is the first approach that can model fine-grained interactions (contacts) between an object in the scene and the human.\n\nDynamic Scene Interactions. Although various algorithms have been proposed for scene-agnostic motion prediction, affordance prediction as well as the synthesis of static human-scene interaction, generating dynamic human-scene interactions is less explored. Recent advances include predicting human motion from scene images [10], and using a semantic graph and RNN to predict human and object movements [14]. More recently, Wang et al. [52] introduce a hierarchical framework that generates 'in-between' locations and poses on the scene and interpolates between the goal poses. However, it requires a carefully tuned post-optimization step over the full motion synthesis to solve the discontinuity of motion between sub-goals and to achieve robust foot contacts with the scene. Alternatively, Chao et al. [13] use a reinforcement learning based approach by training a meta controller to coordinate sub-controllers to complete a sitting task. An important category of human-scene interaction involves performing locomotion on uneven terrains. The Phase-functioned Neural Network [27] first introduced the use of external phase variables to represent the state of the motion cycle. Zhang et al. [56] applies same concept for quadruped motion and further incorporates a gating network that segments the locomotion modes based on foot velocities. Both works show impressive results thanks to the mixture of experts [17] styled architectures.\n\nThe most relevant work to us, are the Neural State Machine (NSM) [47] and SAMP [22]. While NSM is a powerful method and models human-scene interactions such as sitting, carrying boxes and opening doors, it does not generate motion variations for the same task and object geometry. SAMP predicts diverse goal locations in the scene for the virtual human, which is then used to drive the motion generation. Our work takes inspiration from these works, but it is demonstrated qualitatively and quantitatively from our experiments that neither of the work enables control over the style of interaction (Section 5.2). Our\n\n\nA.\n\nB.\n\n\nRight handed contacts\n\nContacts with both hands work focus on controllable, fine-grained interactions given on contacts on the object. To the best of our knowledge, no previous work has tackled the problem of generating controllable human-chair interactions.\n\n\nThe COUCH Dataset\n\nLarge scale datasets of human motion such as AMASS [38] and H3.6m [29] have allowed us to build models of human motion. Unfortunately these datasets only contain sequences of 3D poses but no information about the 3D environment, making these datasets unsuitable for learning human interactions. On the other hand, datasets containing human-object interactions are either restricted to just hands [9], contain only static human poses [50,23] without any motion, or have little variation in motion [47]. We present a multi-subject dataset of human chair interactions. Our dataset consists of 6 different subjects interacting with chairs with over 500 motion sequences. We collect our dataset using 17 wearable Inertial Measurement Units (XSens) [4], from which we obtain high-quality pose sequences in SMPL [37] format using Unity [30]. The total capture length is 3 hours.\n\nMotion capture with marker-based capture systems is restrictive to capturing human-object interactions because markers often get occluded during the interactions leading to inaccurate tracking. IMU-based systems are prevalent for large-scale motion capture, however, the error from its calibration can lower the accuracy of the motion. We propose to combine IMUs with Kinect-based capture system as an efficient trade-off between scalability and accuracy. Our capture system is lightweight and can be generalized to capture many human Table 1: Comparison with existing motion capture datasets for human chair interactions. The COUCH dataset features registered real chairs models, multiple subject, and RGB-D data. The types of hand contact are also annotated.\nFeatures NSM[47] SAMP[22] Ours Real Objects \u2717 \u2713 \u2713 Multiple Subjects \u2717 \u2717 \u2713 Contact Types \u2717 \u2717 \u2713 RGB-D \u2717 \u2717 \u2713\nscene interactions. We use the SMPL registration method similar to [8,44,7] to obtain SMPL fits for our data. The dataset is captured in four different indoor scenes. The average fitting error for the SMPL human model, and the chair scans to the point clouds from the Kinects are 3.12 cm and 1.70 cm, respectively (in Chamfer distance). More details about data capture can be found in supp. mat.\n\nDiversity on Starting Points and Styles. We capture people approaching the chairs from different starting points surrounding the chairs. Each subject then performs different styles of interactions with the chairs during sitting. This includes, one hand touching the arm of the chair, both hands touching the armrests of the chair, one hand touching the sitting plane of the chair before sitting down, and no hand contacts. It also includes free interactions such as crossing legs or leaning forward and backward on the chairs. To ensure the naturalness of motion, each subject is only provided with high-level instruction before capturing each sequence and was asked to perform their styles freely. Annotations of the direction of the starting points relative to the chair as well as the type of hand contact are included in the dataset.\n\nObjects. Our dataset contains three different chair models that vary in terms of their shapes, as well as a sofa. The objects are 3D scanned [3,2] before registering into the Kinect captured point clouds. To generalize the synthesized motion to unseen objects, we perform data augmentation as in [47].\n\nContacts. Studying contact-conditioned interaction calls for accurate contacts to be annotated in the dataset. Since we capture both the body motion and the object pose, it is possible to capture contacts between the body and the object. We detect the contacts of five key joints of the virtual human skeleton, which are the pelvis, hands, and feet. We then augment our data by randomly switching or scaling the object at each frame. The data augmentation is performed on 30 instances from ShapeNet [12] over categories of straight chairs, chairs with arms, and sofas. At every frame, we project the contacts detected from the ground truth data to the new object, and apply full-body inverse kinematics to recompute the pose such that the contacts are consistent, keeping the original context of the motion. \n\n\nMethod\n\nWe address the problem of synthesising 3D human motion that is natural and satisfies environmental geometry constraints and user-defined contacts with the chair. COUCH allows fine-grained control over how the human interacts with the chair. At run-time, our model operates in two modes. First, a generative mode where COUCH can automatically sample diverse intentional contacts on the object with our proposed generative model. Second, an interactive mode where the user specifies the desired contact points on the target object. The input to our method is the current character pose, the target chair geometry as well the target contacts for the hands that need to be met. Our method takes these inputs and predicts the future poses that satisfy the desired contacts autoregressively.\n\n\nKey Insights\n\nSynthesising natural human motion subject to environmental constraints is a challenging task [47,22,27], particularly when also satisfying a set of desired contacts. To this end, we first divide our motion synthesis task into motion planning and motion prediction. We derive our intuition from the way humans execute complex interactions e.g., to sit on the chair, we first prepare a mental model of how we will sit (place a hand on the arm-rest and sit, place a hand on the sitting plane and sit or just sit without using the hands etc.) and then we move our bodies accordingly. We propose two neural networks ControlNet f CN (\u00b7), and PoseNet f PN (\u00b7), for motion planning and motion prediction respectively. Furthermore, we observe that it is useful to perform detailed hand motion planning only when we are close to the chair right before sitting and not when we are far off. Thus, we decompose the motion synthesis into approaching and sitting. The approaching motion can be generated directly with PoseNet but both networks are required for sitting, ControlNet and PoseNet, for generating the sitting motion that satisfies the given contacts.\n\n\nMotion Planning with ControlNet\n\nControlNet is the core of our method and plays an important role in motion planning, that is predicting the future control signals of the key joints which are used to guide the body motions. At a high level, the contact-aware control signal contains the local phases and the future locations of the key joints (in our case, the two hands). The local phase is an auxiliary variable that encodes the temporal alignment of motion for each of the hands and prepares for a future contact to be made. When the virtual human is ready to make contact with the chair, and at the beginning of the hand movement, the local phase is equal to 0, and it gradually reaches the value 1 as the hand comes closer to the contact. The hand trajectory, on the other hand, encodes the spatial relationship between the hand joint and the given contact location.\n\nMore formally, we define our spatial-temporal control signal at frame i + 1 to be C\n+ i+1 = {h + i+1 , \u03d5 + i+1 },\nwhere h + i+1 \u2208 R 2\u00d73\u00d7\u03c4 + represents the future position of the two hand joints relative to their corresponding desired contact point c \u2208 R 2\u00d73 , and their local phases are represented by \u03d5 + i \u2208 R 2\u00d7\u03c4 + . We predict the control signal for \u03c4 + = 7 time stamps sampled uniformly between [0, 1] second window centered at frame i + 1.\n\nWe use an LSTM based f CN (\u00b7) to predict the control signal,\nC + i+1 = f CN (h i , \u03d5 i ),(1)\nwhereh i \u2208 R 2\u00d73\u00d7\u03c4 + denote \u03c4 + points interpolated uniformly on the straight line from the current hand locations to their desired contact locations c. Intuitively, these interpolated positions encourages the ControlNet to predict future hand trajectories that always reach the given contacts. \u03d5 i \u2208 R 2\u00d7\u03c4 denotes the local phases of the hands over \u03c4 = 13 frames sampled uniformly between the [-1, 1] second window centered at frame i. The ControlNet is trained to minimize the following MSE loss on the future hand trajectories and the local phase, which is formulated as follows:\nL control = \u03bb 1 \u2225h + i+1 \u2212\u0125 + i+1 \u2225 2 2 + \u03bb 2 \u2225\u03d5 + i+1 \u2212\u03c6 + i+1 \u2225 2 2 + \u03bb 3 L reg .(2)\nHere, h + i+1 , \u03d5 + i+1 are the network predicted future trajectories and local phases. h + i+1 ,\u03c6 + i+1 are the corresponding GT. We also introduce an additional regularization term L reg = \u2225h + i+1 \u2212h i \u2225 2 . Please see supplementary for implementation details regarding the network architectures and training.\n\n\nMotion Synthesis with PoseNet\n\nControlNet generates important signals that guide the motion of the person such that user-defined contacts are satisfied. To this end, we train PoseNet f PN (\u00b7), that takes as input the control signals predicted by the ControlNet along with the 3D scene and motion in the past and predicts full body motion.\nJ i+1 , T + i+1 , G + i+1 , \u03a6 i+1 ,j p i+1 ,T i+1 , b i+1 = f PN (C + i , J i , T i , G i , I i , E i , \u03a6 i ),(3)\nwhere C + i is the control signal generated by the ControlNet. We represent the current state of motion for the human model: J i = (j p i , j v i , j r i ) contains root relative position j p i \u2208 R j\u00d73 , rotation j v i \u2208 R j\u00d76 and velocity j r i \u2208 R j\u00d73 of each joint at frame i. We use j = 22 joints for our human model.\nT i = (t p i , t d i , t a i ) contains the root positions t p i \u2208 R \u03c4 \u00d73 and rotation t d i \u2208 R \u03c4 \u00d76 for \u03c4 = 13\nframes sampled uniformly between the [-1, 1] second window centered at frame i. t a i \u2208 R \u03c4 \u00d73 are the soft labels which describe current action over ours three action classes, namely, idle, walk, and sit. Inspired by Starke et al., [47], we also use intermediate goals To accurately capture the spatial relation between the person and the chair, we voxelize the chair into an 8 \u00d7 8 \u00d7 8 grid and store at each voxel its occupancy (R) and the relative vector between the root joint of the person and the voxel (R 3 ). This allows us to reason about the distance between the person and different parts of the chair. We flatten this grid to obtain our chair encoding I i \u2208 R 2048 at time-step i.\nG i = (g p i , g d i , g a i ), where g p i \u2208 R \u03c4 \u00d73 , g d i \u2208 R \u03c4 \u00d76\nIn order to explicitly reason about the collisions of the person with the chair, we voxelize the region around the person into a cylindrical ego-centric grid and store the occupancies corresponding to the chair (if it is inside the grid). We flatten the occupancy feature to obtain E i \u2208 R 1408 . It is important to note that although I i and E i are scene encodings that serve different purposes. I i is chaircentric and entails information about how far is the person from the chair and the geometry of the chair, while E i is ego-centric and detects collisions in the surrounding of the human model. In addition, we also introduce an auxiliary variable \u03a6 \u2208 [0, 1] as in [41,27], which encodes the global phase of the motion. When approaching the goal, the represents the timing within a walking cycle, for sitting the phase equals 0 when the person is still standing and reaches 1 when the person has sat. The components of the output of the network differs from the input to a small extend by additionally predictingj p i+1 are the joint positions relative to future root 1 second ahead. To ensure the human model can reach the chair, we introduce the goal-relative root trajectoryT i+1 = {t p i+1 ,t d i+1 } which include the root positions and forward directions relative to the chair of frame i + 1. The rest of the components remain consistent with the input include the the future pose J i+1 , future root trajectory T + i+1 , the future intermediate goals G + i+1 , and the future global phase \u03a6 i+1 . The PoseNet f PN (\u00b7) adopts a mixture-of-experts [27,22,47,48] and is trained to minimize the standard MSE loss.\n\n\nContact Generation with ContactNet\n\nFrom the user's perspective, it is useful to automatically generate plausible contact points on any given chairs. To this reason, we propose ContactNet. The network adopts a conditional variational auto-encoder [46] architecture (cVAE) which encodes the chair geometry I introduced in Section 4.3 and the contact positions c \u2208 R 2\u00d73 to a latent vector z. The decoder of the network then reconstructs the hand contacts\u0109 \u2208 R 2\u00d73 . Note, the position of each voxel in the scene representation I in this case is computed relative to the center of the chair instead of the character's root. During training, the network is trained to minimize the following loss,\nL contact = \u2225\u0109 \u2212 c\u2225 2 2 + \u03b2KL(q(z|c, I)\u2225p(z)),(4)\nwhere KL denotes the Kullback-Leibler divergence. During inference, given the scene representation I of a novel chair, we sample the latent vector z from the uniform Gaussian distribution N (0, I), and use the decoder to generate plausible hand contacts c \u2208 R 2\u00d73 .\n\n\nDecomposition of Approaching and Sitting Motion\n\nDetailed hand motion planning is only required when the human model is close enough to the chair right before sitting as sitting requires synthesizing more precise full-body motion, especially for the hands, such that the person makes the desired contacts and sits on the chair. For this reason, we decompose our synthesis into approaching and sitting by only activating the ControlNet during the sitting. When the ControlNet is deactivated the control signal or when a \"no contact\" signal is present the control signal for the corresponding hand is zeroed.\n\n\nEvaluation\n\nStudying contact-conditioned interaction with chairs requires accurately labelled contacts and a diverse range of styled interactions. The COUCH dataset is captured to meet such needs. We evaluate our contact constrained motion synthesis method on the COUCH dataset qualitatively and quantitatively. Our method is the first approach that allows the user to explicitly define how the person should contact the chair and we generate natural and diverse motions satisfying these contacts. As such we evaluate our method on three axis, (i) accuracy in reaching the contacts, (ii) diversity and (iii) naturalness of the synthesised motion. For qualitative results, we highly encourage the readers to see our supplementary video. It can be seen that our method can generate diverse and natural motions while reaching the user-specified contacts. We quantitatively evaluate the accuracy of contacts and motion diversity on a total of 120 testing sequences on six subject-specific models trained on corresponding subsets of our COUCH dataset. Note that we evaluate raw synthesized motion without post-processing.\n\n\nBaselines\n\nTo our best knowledge, the most related work to ours are the Neural State Machine (NSM) [47]   control over how the interaction should take place. We adapt these baselines for our task by additionally conditioning on the contact positions and refer to these new baselines as NSM+Control and SAMP+Control. Quantitative results are reported for both the original baselines and their adapted version. For each of the methods, we train subject-specific models with the corresponding subset of our COUCH dataset using the code provided by the authors. Our experiments, detailed below, show that naively providing contacts as input to existing motion synthesis approaches does not ensure that the generated motion satisfies the contacts. Our method, on the other hand, does not suffer from this limitation.\n\n\nEvaluation on Control\n\nIn order to evaluate how well our synthesised motion meets the given contacts, we report the average contact error (ACE) as the mean squared error between the predicted hand contact and the corresponding given contact. We use the closest position of the predicted hand motion to the given contact as our hand contact.\n\nSince ACE might be susceptible to outliers and inspired by the literature on object detection [35,43,42,11], we also report average contact precision (AP@k), where we consider a contact as correctly predicted if it is closer than k cm. We compare our method with NSM+Control and SAMP+Control in Table  2. It can be observed that COUCH outperforms prior methods by a significant margin. Prior methods are trained to condition on the contact positions, however it is found (Figure 4) to be not sufficient as the contact input can be easily ignored during auto-regressive prediction. As a result the contact constraints are often not met. This highlights the importance of motion planning in form of trajectory predictors in order to reach the desired contacts. Our ControlNet provides valuable information on how to synthesise motion such that the given contacts are satisfied. Our motion prediction network PoseNet uses these control signals to generate contact constrained motions. Table 2: Evaluation on degree of control. COUCH is shown to be more controllable compared to the baseline methods. The distance from given contact points and the joint position are measured. The success rate of control is also reported.\n\n\nMethod\n\nDistance to Contact \u2193 AP@ 3 cm \u2191 AP@ 5 cm \u2191 AP@ 7 cm \u2191 AP@ 9 cm \u2191 NSM [47] 10 \n\n\nEvaluation on Motion Diversity\n\nDiversity is an essential element for our motion synthesis, since a chair can be approached and interacted with in different ways. To quantify diversity, we evaluate using the Average Pairwise Distance (APD) [55,58,22] on the synthesized pose features of the virtual human J i = (j p i , j v i , j r i ). defined as:\nAP D = 1 N (N \u2212 1) N i=1 N j\u0338 =i D(J \u2032 i , J \u2032 j ),(5)\nwhere N is the total number of frames in all the testing sequences. Note that for evaluation, the virtual human is initialized at different starting points and is instructed to approach and sit on randomly selected chairs with randomly sampled contact points from the dataset, and motion is synthesized for 16 seconds for each sequence. We compare the diversity of synthesized motion in Table 3 and it can be seen that using explicit contacts allows our method to generate more varied motion. \n\n\nControlling with a series of Contacts.\n\nA useful application of our approach is to automatically generate a motion sequence with a series of desired contacts in the context of animation, character control, when executing a set of complex actions. For instance, the person can be instructed to first sit with their hands on the armrest, then lift the arms to support the head before bringing the hands back to the armrest, see Figure 5) and the supplementary video. Our approach can be adapted for this task by iteratively providing the new goal locations for the hands as input after the present locations are reached.\n\n\nContact Prediction on Novel Shapes\n\nApart from user-specified contacts, we can additionally generate the contacts on the surface of a given chair using our proposed ContactNet. This allows us to generate fully automatic and diverse motions for sitting. To measure the diversity of the generated contacts from ContactNet, we compute the Average Pairwise Distance (APD) among the generated hand contact positions c j with unseen chair shapes. A total number of 200 unseen chairs are chosen, and each 10 contact positions are predicted for both hands.\nAP D = 1 2LN (N \u2212 1) 2 k=1 L l=1 N i=1 N j\u0338 =i \u2225X \u2032 i \u2212 X \u2032 j \u2225 2 2(6)\nL = 200 is the number of objects and N = 10 is the number of contacts generated per object. The APD on contact positions is 11.82 cm which is comparable to the ground truth dataset which has an APD of 14.07 cm. As shown qualitatively in Figure 6, we can generate diverse and plausible contact positions on chairs, which can generalize to unseen shapes. \n\n\nConclusion\n\nWe propose COUCH, the first method for synthesising controllable contactdriven human-chair interactions. Given initial conditions and the contacts on the chair, our model plans the motion of the hands, which drives the full body poses to satisfy contacts. In addition to the model, we contribute the COUCHdataset for human chair interactions which includes a wide variety of sitting motions approaching and contacting the chair in different ways. It consists of 3 hours of motion capture with 6 subjects interacting with registered 3D chair models, captured in high quality with IMUs and Kinects. Experiments demonstrate that our method consistently outperforms the SoTA by improving the average contact accuracy by \u223c55% to better satisfy contact constraints. In addition to better control, it can been seen in the supplementary video that our approach generates more natural motion compared to the baseline methods. In the future, we want to extend our dataset to new activities and train a multi-activity contact driven model. In the supplementary, we discuss further future directions in this new problem of fine-grained controlled motion synthesis. Our dataset and code will be released to foster further work in this new research direction. \n\n\nAPPENDIX\n\nIn this appendix, we provide additional information about the dataset, implementation details, post-processing techniques. We also discuss on the current limitation as future research perspectives.\n\n1 Dataset Table 4 shows a break down of our dataset in terms of different types of interactions. Our dataset consists of 3 hours of MoCap with over 500 motion sequences.  [53] followed by manual correction with [45] on the segmentation masks. These masks are then used to segment multi-view depth maps and lift human point clouds from 2D to 3D. We use FrankMocap [44] to initialize the SMPL pose from the images and then apply instance specific optimization [7] to fit the SMPL model to the segmented human point cloud. For more accurate fitting, we additionally obtain the SMPL shape parameters of each subject from 3D scans using [8].\n\n\nMotion Data\n\nSynchronization with the IMUs. The fitted SMPL model provides us with accurate contacts with the scene, however, the fitted motion sequence is prone to occlusion and drastic body movements, as a result, the fitted motion can be jittery at times. On the other hand, the pose captured with the IMUs is smooth over time, but it might not accurately capture the contacts. To this reason, we synchronize the Kinect captured data with the body sensors by incorporating the SMPL fitted poses into the IMU pose sequences. After synchronization, we optimize the joint rotations j r i to achieve temporal smoothness via the objective\nL temp (j r i ) = T \u22121 i=1 \u2225j r i+1 \u2212 j r i \u2225 2 + T \u22121 i=1 \u2225j i \u2225(7)\nwherej i represents the acceleration of the body joints in frame i approximated by central difference.\n\nWe additionally use the binary contact labels of the toes and the heels detected by the IMU sensors to remove foot-sliding on the motion data. To remove the foot-sliding, we compute the average joint positions over the duration of the contacts grouped by the positive contact labels. This computation is performed for all four foot joints. This forms a sequence of target joint positions of the feet f i \u2208 R 4\u00d73 . We then optimize the objective function\nL slide (j r i ) = T i=1 \u2225f i \u2212 f i \u2225 2 ,(8)\nwhere f i represents the foot joint positions at frame i. The resulting motion sequence is temporally smooth and has accurate contacts registered with the chair models. Object Processing. To obtain object segmentation, we pre-scan objects using a 3D scanner [2,3]. We then use multi-view object keypoints, marked by manual annotators on the images, to fit the pre-scanned chair meshes to the given frame. The segmentation masks are then obtained by projecting fitted object meshes to the images. Since the chairs remain static during the capture, we average over the 6D pose of the fitted chair model during each capture session to obtain the final transformation of the chair. \n\n\nTraining Details\n\n\nControlNet\n\nAs shown in Figure 7, the contact network is a two-layer LSTM architecture. Each layer has a hidden dimension of 512. The pose and the control signals (hand trajectories, and the local phases) are each encoded through a two-layer fully connected network with of shape {128, 128} before passing through the LSTM. We apply scheduled sampling on hand trajectories for better model performance.\n\nFor the local phases, we always use the ground truth. Each of our training samples is in a sequence of 60 frames. The ControlNet is trained for 150 epochs with an Adam optimizer. The initial learning rate is 1e-3 and a cosine learning rate scheduler was used to decay the learning rate gradually to 5e-6. The full training of a subject-specific model takes approximately 1 hour on an NVIDIA V100 GPU. The PoseNet adopts the mixture-of-expert structure [17]. It consists of different feature encoders of structures shown in Table 5. The gating network and the prediction networks are both three-layer fully-connected networks, with hidden dimensions of 128 and 512 respectively. The number of experts is set to 10. The PoseNet is trained for 150 epochs with an Adam optimizer. The initial learning rate is 1e-4 and a cosine learning rate scheduler was used to decay the learning rate gradually to 5e-6. The full training of a subject-specific model takes approximately 6 hours on an NVIDIA V100 GPU.\n\n\nPoseNet\n\n\nContactNet\n\nThe ContactNet encodes the scene I through a three-layer fully connected network of shape {512, 512, 64}. The latent vector z of the VAE is of size 6. The weight of the Kullback-Leibler divergence \u03b2 is 0.1. We use the Adam optimizer with a learning rate of 1e-3 and train ContactNet for 150 epochs. The full training of a subject-specific model takes approximately 10 minutes on an NVIDIA V100 GPU.\n\n\nContact Projection and Trajectory Fitting\n\nTo ensure the ContactNet predicts contacts that land exactly on the surface of the object, we perform a post-processing step, when the distance of the network predicted contact to the surface is less than a set threshold of 10 cm, we simply project the contact onto the nearest point on the chair surface. When the distance is greater than 10 cm, we simply neglect the predicted contact. The ControlNet predicts the future hand trajectories, and it would be possible to fit the predicted pose to the predicted hand position from the hand trajectories at each frame to further improve the satisfaction of the contact constraints. Note, in the evaluation of the main paper we do not apply such fitting technique.\n\n\nLimitations and Future Direction\n\nWe observe the synthesized motion can slightly intersect with the chair. A solution to this problem would be to apply a post-processing step to avoid such collision. In order to generalize to more different chair shapes, it would be useful to investigate better ways of encoding the scene geometry while trying to avoid over-fitting.\n\nDifferent shaped person can intersect with the same object very differently even when performing the same motion. The COUCH dataset captures human interaction with different body shapes. With the dataset, it is possible to study how to build subject-variant motion synthesis model and how to effectively condition on the body shapes. These are challenges in motion synthesis that have not been tackled.\n\nOur work on controllable human-chair interaction. It would be useful to extend the scope of interacted objects, especially considering the cases when the objects are non-static, when performing motions such as lifting a box, or opening a door. Another possible direction would be to further apply contact-based control in these interactions.\n\nFig. 2 :\n2(A) The COUCH dataset captures a diverse range chair interaction with an emphasis on hand contacts. It consists of RGB-D images, corresponding Mo-Cap in the SMPL [37] format, chair geometry and annotations on types of hand interaction. (B) COUCH dataset captures natural modes of interactions with a chair, as demonstrated by the heatmaps of contact clusters. Most common contacts while sitting, include right hand support or both hands.\n\nFig. 3 :\n3Given user specified or model generated contacts, our proposed method which consists of the ControlNet and the PoseNet, auto-regressively synthesizes contact satisfying motion.\n\n\nare the goal positions and orientations at frame i. g a i \u2208 R \u03c4 \u00d73 are the one-hot labels describing the intended goal action.\n\nFig. 5 :\n5COUCH can also be extended by specifying a series of contacts for to automatically synthesize more complex interactions. The past poses are indicated by blue skeletons.\n\nFig. 6 :\n6ContactNet enables sampling of diverse contact positions across various chair shapes. These contacts can be used by our ControlNet and PoseNet to generate fully automatic and diverse motions.\n\n20 .\n20Gui, L.Y., Wang, Y.X., Ramanan, D., Moura, J.M.F.: Few-shot human motion prediction via meta-learning. In: Proceedings of the European Conference on Computer Vision (ECCV) (September 2018) 21. Habibie, I., Holden, D., Schwarz, J., Yearsley, J., Komura, T.: A recurrent variational autoencoder for human motion synthesis. In: Proceedings of the British Machine Vision Conference (BMVC). pp. 119.1-119.12. BMVA Press (September 2017) 22. Hassan, M., Ceylan, D., Villegas, R., Saito, J., Yang, J., Zhou, Y., Black, M.: Stochastic scene-aware motion prediction. In: Proceedings of the International Conference on Computer Vision 2021 (2021) 23. Hassan, M., Choutas, V., Tzionas, D., Black, M.J.: Resolving 3D human pose ambiguities with 3D scene constraints. In: International Conference on Computer Vision. pp. 2282-2292 (Oct 2019) 24. Henter, G.E., Alexanderson, S., Beskow, J.: Moglow: probabilistic and controllable motion synthesis using normalising flows. ACM Trans. Graph. 39(6), 236:1-236:14 (2020) 25. Hernandez, A., Gall, J., Moreno-Noguer, F.: Human motion prediction via spatiotemporal inpainting. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2019) 26. Holden, D., Kanoun, O., Perepichka, M., Popa, T.: Learned motion matching. ACM Trans. Graph. 39(4), 53 (2020) 27. Holden, D., Komura, T., Saito, J.: Phase-functioned neural networks for character control. ACM Trans. Graph. 36(4), 42:1-42:13 (2017) 28. Holden, D., Saito, J., Komura, T.: A deep learning framework for character motion synthesis and editing. ACM Trans. Graph. (Jul 2016) 29. Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence 36(7), 1325-1339 (jul 2014) 30. Juliani, A., Berges, V., Vckay, E., Gao, Y., Henry, H., Mattar, M., Lange, D.: Unity: A general platform for intelligent agents. CoRR abs/1809.02627 (2018) 31. Karunratanakul, K., Yang, J., Zhang, Y., Black, M., Muandet, K., Tang, S.: Grasping field: Learning implicit representations for human grasps. In: International Conference on 3D Vision (3DV) (2020) 32. Li, M., Chen, S., Zhao, Y., Zhang, Y., Wang, Y., Tian, Q.: Dynamic multiscale graph neural networks for 3d skeleton based human motion prediction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020) 33. Li, R., Yang, S., Ross, D.A., Kanazawa, A.: Ai choreographer: Music conditioned 3d dance generation with aist++ (2021) 34. Li, X., Liu, S., Kim, K., Wang, X., Yang, M.H., Kautz, J.: Putting humans in a scene: Learning affordance in 3d indoor environments. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2019) 35. Lin, T.Y., Doll\u00e1r, P., Girshick, R.B., He, K., Hariharan, B., Belongie, S.J.: Feature pyramid networks for object detection. In: CVPR. pp. 936-944. IEEE Computer Society (2017) 36. Ling, H.Y., Zinno, F., Cheng, G., Van De Panne, M.: Character controllers using motion vaes. ACM Trans. Graph. 39(4) (2020)\n\nFig. 7 :\n7Our method that combines the ControlNet and the PoseNet.\n\nTable 3 :\n3Evaluation on the diversity of the synthesized motion. APD is measured \nfor segmented motion of approaching and sitting. Our approach attains the best \nscore compared to the baselines. \n\nMethod \nApproach Sit \nNSM [47] \n5.15 \n5.76 \nSAMP [22] \n5.34 \n5.81 \nNSM+Control [47] \n5.07 \n5.80 \nSAMP+Control [22] \n5.21 \n5.88 \nOurs \n5.55 6.02 \nGround Truth \n5.69 \n6.30 \n\n\n\nTable 4 :\n4Distribution of the COUCH dataset with different types of interaction.SMPL Fitting. We segment the human in captured RGB images by running Detectron V2Interaction Type Minutes % \nRight Hand \n36.3 17.3 \nLeft Hand \n29.4 14.0 \nBoth Hand \n60.5 28.9 \nNo Contact \n36.5 17.4 \nFree Interaction 31.9 15.2 \nLocomotion \n15.1 7.2 \n\n1.2 Data Processing \n\n\n\nTable 5 :\n5Details on different encoder networks of the PoseNet. Architecture Encoder for C {128,128,128} Encoder for {J, T} {512, 512, 512} Encoder for G {128,128,128}Networks \nEncoder for I \n{512, 512, 512} \nEncoder for E \n{256,256,256} \n\n\nAcknowledgementWe would like to thank Xianghui Xie for helping with data processing, and we are very grateful for all the participants who took part in the data capture.\n. Agisoft Metashape, Agisoft metashape. https://www.agisoft.com/\n\nXsens mvn: Full 6dof human motion tracking using miniature inertial sensors. accessed: 2010-09-30Xsens mvn: Full 6dof human motion tracking using miniature inertial sensors. https://www.xsens.com/, accessed: 2010-09-30\n\nStructured prediction helps 3d human motion modelling. E Aksan, M Kaufmann, O Hilliges, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Aksan, E., Kaufmann, M., Hilliges, O.: Structured prediction helps 3d human motion modelling. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2019)\n\nA stochastic conditioning scheme for diverse human motion prediction. S Aliakbarian, F S Saleh, M Salzmann, L Petersson, S Gould, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Aliakbarian, S., Saleh, F.S., Salzmann, M., Petersson, L., Gould, S.: A stochastic conditioning scheme for diverse human motion prediction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)\n\nLearning to reconstruct people in clothing from a single RGB camera. T Alldieck, M Magnor, B L Bhatnagar, C Theobalt, G Pons-Moll, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Alldieck, T., Magnor, M., Bhatnagar, B.L., Theobalt, C., Pons-Moll, G.: Learning to reconstruct people in clothing from a single RGB camera. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (jun 2019)\n\nCombining implicit function learning and parametric models for 3d human reconstruction. B L Bhatnagar, C Sminchisescu, C Theobalt, G Pons-Moll, European Conference on Computer Vision (ECCV). SpringerBhatnagar, B.L., Sminchisescu, C., Theobalt, C., Pons-Moll, G.: Combining im- plicit function learning and parametric models for 3d human reconstruction. In: European Conference on Computer Vision (ECCV). Springer (August 2020)\n\nS Brahmbhatt, C Ham, C C Kemp, J Hays, ContactDB: Analyzing and predicting grasp contact via thermal imaging. Brahmbhatt, S., Ham, C., Kemp, C.C., Hays, J.: ContactDB: Analyzing and pre- dicting grasp contact via thermal imaging (2019), cVPR\n\nLong-term human motion prediction with scene context. Z Cao, H Gao, K Mangalam, Q Cai, M Vo, J Malik, ECCVCao, Z., Gao, H., Mangalam, K., Cai, Q., Vo, M., Malik, J.: Long-term human motion prediction with scene context. In: ECCV (2020)\n\nEnd-to-end object detection with transformers. N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, S Zagoruyko, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object detection with transformers. In: Proceedings of the European Conference on Computer Vision (ECCV) (2020)\n\nShapenet: An information-rich 3d model repository. A X Chang, T A Funkhouser, L J Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, J Xiao, L Yi, F Yu, CoRR abs/1512.03012Chang, A.X., Funkhouser, T.A., Guibas, L.J., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., Yu, F.: Shapenet: An information-rich 3d model repository. CoRR abs/1512.03012 (2015)\n\nLearning to sit: Synthesizing human-chair interactions via hierarchical control. Y Chao, J Yang, W Chen, J Deng, CoRR abs/1908.07423Chao, Y., Yang, J., Chen, W., Deng, J.: Learning to sit: Synthesizing human-chair interactions via hierarchical control. CoRR abs/1908.07423 (2019)\n\nContext-aware human motion prediction. E Corona, A Pumarola, G Aleny\u00e0, F Moreno-Noguer, CoRR abs/1904.03419Corona, E., Pumarola, A., Aleny\u00e0, G., Moreno-Noguer, F.: Context-aware human motion prediction. CoRR abs/1904.03419 (2019)\n\nGanhand: Predicting human grasp affordances in multi-object scenes. E Corona, A Pumarola, G Alenya, F Moreno-Noguer, G Rogez, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Corona, E., Pumarola, A., Alenya, G., Moreno-Noguer, F., Rogez, G.: Ganhand: Predicting human grasp affordances in multi-object scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)\n\nLearning dynamic relationships for 3d human motion prediction. Q Cui, H Sun, F Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Cui, Q., Sun, H., Yang, F.: Learning dynamic relationships for 3d human motion prediction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)\n\nLearning factored representations in a deep mixture of experts. D Eigen, M Ranzato, I Sutskever, 2nd International Conference on Learning Representations. Bengio, Y., LeCun, Y.Banff, AB, CanadaWorkshop Track ProceedingsEigen, D., Ranzato, M., Sutskever, I.: Learning factored representations in a deep mixture of experts. In: Bengio, Y., LeCun, Y. (eds.) 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Workshop Track Proceedings (2014)\n\nLearning human motion models for long-term predictions. P Ghosh, J Song, E Aksan, O Hilliges, s International Conference on 3D Vision 3DV. Ghosh, P., Song, J., Aksan, E., Hilliges, O.: Learning human motion models for long-term predictions. s International Conference on 3D Vision 3DV (2017)\n\nAdversarial geometry-aware human motion prediction. L Y Gui, Y X Wang, X Liang, J M F Moura, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Gui, L.Y., Wang, Y.X., Liang, X., Moura, J.M.F.: Adversarial geometry-aware hu- man motion prediction. In: Proceedings of the European Conference on Computer Vision (ECCV) (September 2018)\n\nSMPL: A skinned multi-person linear model. M Loper, N Mahmood, J Romero, G Pons-Moll, M J Black, ACM Trans. Graphics (Proc. SIGGRAPH Asia). 34616Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia) 34(6), 248:1-248:16 (2015)\n\nAMASS: archive of motion capture as surface shapes. N Mahmood, N Ghorbani, N F Troje, G Pons-Moll, M J Black, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019. Seoul, KoreaMahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: AMASS: archive of motion capture as surface shapes. In: 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019\n\nLearning trajectory dependencies for human motion prediction. W Mao, M Liu, M Salzmann, H Li, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Mao, W., Liu, M., Salzmann, M., Li, H.: Learning trajectory dependencies for human motion prediction. In: Proceedings of the IEEE/CVF International Con- ference on Computer Vision (ICCV) (October 2019)\n\nOn human motion prediction using recurrent neural networks. J Martinez, M J Black, J Romero, 2017 IEEE Conference on Computer Vision and Pattern Recognition. Honolulu, HI, USAMartinez, J., Black, M.J., Romero, J.: On human motion prediction using recur- rent neural networks. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017\n\nQuaternet: A quaternion-based recurrent model for human motion. D Pavllo, D Grangier, M Auli, British Machine Vision Conference (BMVC). Pavllo, D., Grangier, D., Auli, M.: Quaternet: A quaternion-based recurrent model for human motion. In: British Machine Vision Conference (BMVC) (2018)\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in Neural Information Processing Systems. Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., Garnett, R.28Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., Garnett, R. (eds.) Advances in Neural Information Processing Sys- tems. vol. 28 (2015)\n\nFaster R-CNN: towards real-time object detection with region proposal networks. S Ren, K He, R B Girshick, J Sun, CoRR abs/1506.01497Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object detection with region proposal networks. CoRR abs/1506.01497 (2015)\n\nFrankmocap: A monocular 3d whole-body pose estimation system via regression and integration. Y Rong, T Shiratori, H Joo, IEEE International Conference on Computer Vision Workshops. Rong, Y., Shiratori, T., Joo, H.: Frankmocap: A monocular 3d whole-body pose es- timation system via regression and integration. In: IEEE International Conference on Computer Vision Workshops (2021)\n\nf-brs: Rethinking backpropagating refinement for interactive segmentation. K Sofiiuk, I Petrov, O Barinova, A Konushin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSofiiuk, K., Petrov, I., Barinova, O., Konushin, A.: f-brs: Rethinking backpropa- gating refinement for interactive segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8623-8632 (2020)\n\nLearning structured output representation using deep conditional generative models. K Sohn, H Lee, X Yan, C Cortes, N D Lawrence, D D Lee, M Sugiyama, De- cember 7-12Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Garnett, R.Montreal, Quebec, CanadaSohn, K., Lee, H., Yan, X.: Learning structured output representation using deep conditional generative models. In: Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R. (eds.) Advances in Neural Information Processing Sys- tems 28: Annual Conference on Neural Information Processing Systems 2015, De- cember 7-12, 2015, Montreal, Quebec, Canada. pp. 3483-3491 (2015)\n\nNeural state machine for characterscene interactions. S Starke, H Zhang, T Komura, J Saito, ACM Trans. Graph. 386Starke, S., Zhang, H., Komura, T., Saito, J.: Neural state machine for character- scene interactions. ACM Trans. Graph. 38(6), 209:1-209:14 (2019)\n\nLocal motion phases for learning multi-contact character movements. S Starke, Y Zhao, T Komura, K A Zaman, ACM Trans. Graph. Starke, S., Zhao, Y., Komura, T., Zaman, K.A.: Local motion phases for learning multi-contact character movements. ACM Trans. Graph.\n\nNeural animation layering for synthesizing martial arts movements. S Starke, Y Zhao, F Zinno, T Komura, ACM Trans. Graph. Starke, S., Zhao, Y., Zinno, F., Komura, T.: Neural animation layering for synthe- sizing martial arts movements. ACM Trans. Graph.\n\nGRAB: A dataset of wholebody human grasping of objects. O Taheri, N Ghorbani, M J Black, D Tzionas, European Conference on Computer Vision (ECCV). Taheri, O., Ghorbani, N., Black, M.J., Tzionas, D.: GRAB: A dataset of whole- body human grasping of objects. In: European Conference on Computer Vision (ECCV) (2020)\n\nVRED: A position-velocity recurrent encoder-decoder for human motion prediction. H Wang, J Feng, CoRR abs/1906.06514Wang, H., Feng, J.: VRED: A position-velocity recurrent encoder-decoder for hu- man motion prediction. CoRR abs/1906.06514 (2019)\n\nSynthesizing long-term 3d human motion and interaction in 3d scenes. J Wang, H Xu, J Xu, S Liu, X Wang, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual. IEEEWang, J., Xu, H., Xu, J., Liu, S., Wang, X.: Synthesizing long-term 3d human motion and interaction in 3d scenes. In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021. pp. 9401-9411. Computer Vision Foundation / IEEE (2021)\n\n. Y Wu, A Kirillov, F Massa, W Y Lo, R Girshick, Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https:// github.com/facebookresearch/detectron2 (2019)\n\nHierarchical style-based networks for motion synthesis. J Xu, H Xu, B Ni, X Yang, X Wang, T Darrell, Computer Vision -ECCV 2020 -16th European Conference. Vedaldi, A., Bischof, H., Brox, T., Frahm, J.Glasgow, UKProceedings, Part XIXu, J., Xu, H., Ni, B., Yang, X., Wang, X., Darrell, T.: Hierarchical style-based networks for motion synthesis. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J. (eds.) Computer Vision -ECCV 2020 -16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XI\n\nDlow: Diversifying latent flows for diverse human motion prediction. Y Yuan, K Kitani, Proceedings of the European Conference on Computer Vision (ECCV. the European Conference on Computer Vision (ECCVYuan, Y., Kitani, K.: Dlow: Diversifying latent flows for diverse human motion pre- diction. In: Proceedings of the European Conference on Computer Vision (ECCV) (2020)\n\nMode-adaptive neural networks for quadruped motion control. H Zhang, S Starke, T Komura, J Saito, ACM Trans. Graph. 37411Zhang, H., Starke, S., Komura, T., Saito, J.: Mode-adaptive neural networks for quadruped motion control. ACM Trans. Graph. 37(4), 145:1-145:11 (2018)\n\nPLACE: Proximity learning of articulation and contact in 3D environments. S Zhang, Y Zhang, Q Ma, M J Black, S Tang, International Conference on 3D Vision (3DV). Zhang, S., Zhang, Y., Ma, Q., Black, M.J., Tang, S.: PLACE: Proximity learning of articulation and contact in 3D environments. In: International Conference on 3D Vision (3DV) (Nov 2020)\n\nGenerating 3d people in scenes without people. Y Zhang, M Hassan, H Neumann, M J Black, S Tang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Zhang, Y., Hassan, M., Neumann, H., Black, M.J., Tang, S.: Generating 3d peo- ple in scenes without people. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)\n", "annotations": {"author": "[{\"end\":176,\"start\":56},{\"end\":304,\"start\":177},{\"end\":426,\"start\":305},{\"end\":502,\"start\":427},{\"end\":626,\"start\":503}]", "publisher": null, "author_last_name": "[{\"end\":69,\"start\":64},{\"end\":197,\"start\":184},{\"end\":319,\"start\":314},{\"end\":443,\"start\":437},{\"end\":519,\"start\":510}]", "author_first_name": "[{\"end\":63,\"start\":56},{\"end\":183,\"start\":177},{\"end\":313,\"start\":305},{\"end\":436,\"start\":427},{\"end\":509,\"start\":503}]", "author_affiliation": "[{\"end\":101,\"start\":71},{\"end\":175,\"start\":103},{\"end\":229,\"start\":199},{\"end\":303,\"start\":231},{\"end\":351,\"start\":321},{\"end\":425,\"start\":353},{\"end\":461,\"start\":445},{\"end\":501,\"start\":463},{\"end\":551,\"start\":521},{\"end\":625,\"start\":553}]", "title": "[{\"end\":53,\"start\":1},{\"end\":679,\"start\":627}]", "venue": null, "abstract": "[{\"end\":2123,\"start\":681}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4205,\"start\":4201},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4208,\"start\":4205},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4211,\"start\":4208},{\"end\":4214,\"start\":4211},{\"end\":4315,\"start\":4311},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4318,\"start\":4315},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4419,\"start\":4415},{\"end\":4422,\"start\":4419},{\"end\":4425,\"start\":4422},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4428,\"start\":4425},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4431,\"start\":4428},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5134,\"start\":5130},{\"end\":5137,\"start\":5134},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7393,\"start\":7389},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7712,\"start\":7708},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9571,\"start\":9567},{\"end\":9574,\"start\":9571},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9577,\"start\":9574},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9580,\"start\":9577},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9583,\"start\":9580},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9586,\"start\":9583},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9588,\"start\":9586},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9591,\"start\":9588},{\"end\":9594,\"start\":9591},{\"end\":9617,\"start\":9613},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9620,\"start\":9617},{\"end\":9906,\"start\":9902},{\"end\":9909,\"start\":9906},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9912,\"start\":9909},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9914,\"start\":9912},{\"end\":9917,\"start\":9914},{\"end\":10080,\"start\":10076},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10083,\"start\":10080},{\"end\":10437,\"start\":10433},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10516,\"start\":10512},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10519,\"start\":10516},{\"end\":10521,\"start\":10519},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10679,\"start\":10675},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10682,\"start\":10679},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10684,\"start\":10682},{\"end\":10687,\"start\":10684},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11431,\"start\":11427},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11510,\"start\":11506},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11543,\"start\":11539},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11912,\"start\":11908},{\"end\":12185,\"start\":12181},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12300,\"start\":12296},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12518,\"start\":12514},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12611,\"start\":12607},{\"end\":12625,\"start\":12621},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13505,\"start\":13501},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13849,\"start\":13846},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13887,\"start\":13883},{\"end\":13890,\"start\":13887},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13950,\"start\":13946},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14196,\"start\":14193},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14259,\"start\":14255},{\"end\":14283,\"start\":14279},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15260,\"start\":15257},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15263,\"start\":15260},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15264,\"start\":15263},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16570,\"start\":16567},{\"end\":16572,\"start\":16570},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16726,\"start\":16722},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17232,\"start\":17228},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18447,\"start\":18443},{\"end\":18450,\"start\":18447},{\"end\":18453,\"start\":18450},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23023,\"start\":23019},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24226,\"start\":24222},{\"end\":24229,\"start\":24226},{\"end\":25114,\"start\":25110},{\"end\":25117,\"start\":25114},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25120,\"start\":25117},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25123,\"start\":25120},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25427,\"start\":25423},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":28019,\"start\":28015},{\"end\":29170,\"start\":29166},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29173,\"start\":29170},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29176,\"start\":29173},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29179,\"start\":29176},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30375,\"start\":30371},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30626,\"start\":30622},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30629,\"start\":30626},{\"end\":30632,\"start\":30629},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":34524,\"start\":34520},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34564,\"start\":34560},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34716,\"start\":34712},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34810,\"start\":34807},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34984,\"start\":34981},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37857,\"start\":37853}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41144,\"start\":40696},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41332,\"start\":41145},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41461,\"start\":41333},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41641,\"start\":41462},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41844,\"start\":41642},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44988,\"start\":41845},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45056,\"start\":44989},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45428,\"start\":45057},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45783,\"start\":45429},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46026,\"start\":45784}]", "paragraph": "[{\"end\":2240,\"start\":2132},{\"end\":3556,\"start\":2242},{\"end\":4000,\"start\":3573},{\"end\":4613,\"start\":4002},{\"end\":5665,\"start\":4615},{\"end\":6230,\"start\":5667},{\"end\":8432,\"start\":6232},{\"end\":8493,\"start\":8434},{\"end\":9305,\"start\":8495},{\"end\":10248,\"start\":9322},{\"end\":11102,\"start\":10250},{\"end\":12540,\"start\":11104},{\"end\":13158,\"start\":12542},{\"end\":13167,\"start\":13165},{\"end\":13428,\"start\":13193},{\"end\":14321,\"start\":13450},{\"end\":15083,\"start\":14323},{\"end\":15585,\"start\":15190},{\"end\":16424,\"start\":15587},{\"end\":16727,\"start\":16426},{\"end\":17537,\"start\":16729},{\"end\":18333,\"start\":17548},{\"end\":19497,\"start\":18350},{\"end\":20371,\"start\":19533},{\"end\":20456,\"start\":20373},{\"end\":20818,\"start\":20487},{\"end\":20880,\"start\":20820},{\"end\":21495,\"start\":20913},{\"end\":21895,\"start\":21583},{\"end\":22236,\"start\":21929},{\"end\":22672,\"start\":22351},{\"end\":23478,\"start\":22786},{\"end\":25173,\"start\":23549},{\"end\":25869,\"start\":25212},{\"end\":26185,\"start\":25920},{\"end\":26794,\"start\":26237},{\"end\":27913,\"start\":26809},{\"end\":28727,\"start\":27927},{\"end\":29070,\"start\":28753},{\"end\":30290,\"start\":29072},{\"end\":30379,\"start\":30301},{\"end\":30730,\"start\":30414},{\"end\":31279,\"start\":30786},{\"end\":31900,\"start\":31322},{\"end\":32451,\"start\":31939},{\"end\":32876,\"start\":32523},{\"end\":34137,\"start\":32891},{\"end\":34347,\"start\":34150},{\"end\":34985,\"start\":34349},{\"end\":35624,\"start\":35001},{\"end\":35796,\"start\":35694},{\"end\":36251,\"start\":35798},{\"end\":36975,\"start\":36297},{\"end\":37399,\"start\":37009},{\"end\":38399,\"start\":37401},{\"end\":38822,\"start\":38424},{\"end\":39578,\"start\":38868},{\"end\":39948,\"start\":39615},{\"end\":40352,\"start\":39950},{\"end\":40695,\"start\":40354}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15189,\"start\":15084},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20486,\"start\":20457},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20912,\"start\":20881},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21582,\"start\":21496},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22350,\"start\":22237},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22785,\"start\":22673},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23548,\"start\":23479},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25919,\"start\":25870},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30785,\"start\":30731},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32522,\"start\":32452},{\"attributes\":{\"id\":\"formula_10\"},\"end\":35693,\"start\":35625},{\"attributes\":{\"id\":\"formula_11\"},\"end\":36296,\"start\":36252}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":7826,\"start\":7819},{\"end\":14865,\"start\":14858},{\"end\":29375,\"start\":29367},{\"end\":30061,\"start\":30054},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31180,\"start\":31173},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34366,\"start\":34359},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37931,\"start\":37924}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3571,\"start\":3559},{\"attributes\":{\"n\":\"2\"},\"end\":9320,\"start\":9308},{\"end\":13163,\"start\":13161},{\"end\":13191,\"start\":13170},{\"attributes\":{\"n\":\"3\"},\"end\":13448,\"start\":13431},{\"attributes\":{\"n\":\"4\"},\"end\":17546,\"start\":17540},{\"attributes\":{\"n\":\"4.1\"},\"end\":18348,\"start\":18336},{\"attributes\":{\"n\":\"4.2\"},\"end\":19531,\"start\":19500},{\"attributes\":{\"n\":\"4.3\"},\"end\":21927,\"start\":21898},{\"attributes\":{\"n\":\"4.4\"},\"end\":25210,\"start\":25176},{\"attributes\":{\"n\":\"4.5\"},\"end\":26235,\"start\":26188},{\"attributes\":{\"n\":\"5\"},\"end\":26807,\"start\":26797},{\"attributes\":{\"n\":\"5.1\"},\"end\":27925,\"start\":27916},{\"attributes\":{\"n\":\"5.2\"},\"end\":28751,\"start\":28730},{\"end\":30299,\"start\":30293},{\"attributes\":{\"n\":\"5.3\"},\"end\":30412,\"start\":30382},{\"attributes\":{\"n\":\"5.4\"},\"end\":31320,\"start\":31282},{\"attributes\":{\"n\":\"5.5\"},\"end\":31937,\"start\":31903},{\"attributes\":{\"n\":\"6\"},\"end\":32889,\"start\":32879},{\"end\":34148,\"start\":34140},{\"attributes\":{\"n\":\"1.1\"},\"end\":34999,\"start\":34988},{\"attributes\":{\"n\":\"2\"},\"end\":36994,\"start\":36978},{\"attributes\":{\"n\":\"2.1\"},\"end\":37007,\"start\":36997},{\"attributes\":{\"n\":\"2.2\"},\"end\":38409,\"start\":38402},{\"attributes\":{\"n\":\"2.3\"},\"end\":38422,\"start\":38412},{\"attributes\":{\"n\":\"3\"},\"end\":38866,\"start\":38825},{\"attributes\":{\"n\":\"4\"},\"end\":39613,\"start\":39581},{\"end\":40705,\"start\":40697},{\"end\":41154,\"start\":41146},{\"end\":41471,\"start\":41463},{\"end\":41651,\"start\":41643},{\"end\":41850,\"start\":41846},{\"end\":44998,\"start\":44990},{\"end\":45067,\"start\":45058},{\"end\":45439,\"start\":45430},{\"end\":45794,\"start\":45785}]", "table": "[{\"end\":45428,\"start\":45069},{\"end\":45783,\"start\":45592},{\"end\":46026,\"start\":45953}]", "figure_caption": "[{\"end\":41144,\"start\":40707},{\"end\":41332,\"start\":41156},{\"end\":41461,\"start\":41335},{\"end\":41641,\"start\":41473},{\"end\":41844,\"start\":41653},{\"end\":44988,\"start\":41853},{\"end\":45056,\"start\":45000},{\"end\":45592,\"start\":45441},{\"end\":45953,\"start\":45796}]", "figure_ref": "[{\"end\":2131,\"start\":2125},{\"end\":5901,\"start\":5893},{\"end\":29552,\"start\":29543},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31716,\"start\":31708},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32768,\"start\":32760},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37029,\"start\":37021}]", "bib_author_first_name": "[{\"end\":46206,\"start\":46199},{\"end\":46539,\"start\":46538},{\"end\":46548,\"start\":46547},{\"end\":46560,\"start\":46559},{\"end\":46978,\"start\":46977},{\"end\":46993,\"start\":46992},{\"end\":46995,\"start\":46994},{\"end\":47004,\"start\":47003},{\"end\":47016,\"start\":47015},{\"end\":47029,\"start\":47028},{\"end\":47516,\"start\":47515},{\"end\":47528,\"start\":47527},{\"end\":47538,\"start\":47537},{\"end\":47540,\"start\":47539},{\"end\":47553,\"start\":47552},{\"end\":47565,\"start\":47564},{\"end\":47956,\"start\":47955},{\"end\":47958,\"start\":47957},{\"end\":47971,\"start\":47970},{\"end\":47987,\"start\":47986},{\"end\":47999,\"start\":47998},{\"end\":48296,\"start\":48295},{\"end\":48310,\"start\":48309},{\"end\":48317,\"start\":48316},{\"end\":48319,\"start\":48318},{\"end\":48327,\"start\":48326},{\"end\":48593,\"start\":48592},{\"end\":48600,\"start\":48599},{\"end\":48607,\"start\":48606},{\"end\":48619,\"start\":48618},{\"end\":48626,\"start\":48625},{\"end\":48632,\"start\":48631},{\"end\":48823,\"start\":48822},{\"end\":48833,\"start\":48832},{\"end\":48842,\"start\":48841},{\"end\":48854,\"start\":48853},{\"end\":48865,\"start\":48864},{\"end\":48877,\"start\":48876},{\"end\":49259,\"start\":49258},{\"end\":49261,\"start\":49260},{\"end\":49270,\"start\":49269},{\"end\":49272,\"start\":49271},{\"end\":49286,\"start\":49285},{\"end\":49288,\"start\":49287},{\"end\":49298,\"start\":49297},{\"end\":49310,\"start\":49309},{\"end\":49319,\"start\":49318},{\"end\":49325,\"start\":49324},{\"end\":49337,\"start\":49336},{\"end\":49346,\"start\":49345},{\"end\":49354,\"start\":49353},{\"end\":49360,\"start\":49359},{\"end\":49368,\"start\":49367},{\"end\":49374,\"start\":49373},{\"end\":49706,\"start\":49705},{\"end\":49714,\"start\":49713},{\"end\":49722,\"start\":49721},{\"end\":49730,\"start\":49729},{\"end\":49945,\"start\":49944},{\"end\":49955,\"start\":49954},{\"end\":49967,\"start\":49966},{\"end\":49977,\"start\":49976},{\"end\":50205,\"start\":50204},{\"end\":50215,\"start\":50214},{\"end\":50227,\"start\":50226},{\"end\":50237,\"start\":50236},{\"end\":50254,\"start\":50253},{\"end\":50731,\"start\":50730},{\"end\":50738,\"start\":50737},{\"end\":50745,\"start\":50744},{\"end\":51177,\"start\":51176},{\"end\":51186,\"start\":51185},{\"end\":51197,\"start\":51196},{\"end\":51666,\"start\":51665},{\"end\":51675,\"start\":51674},{\"end\":51683,\"start\":51682},{\"end\":51692,\"start\":51691},{\"end\":51955,\"start\":51954},{\"end\":51957,\"start\":51956},{\"end\":51964,\"start\":51963},{\"end\":51966,\"start\":51965},{\"end\":51974,\"start\":51973},{\"end\":51983,\"start\":51982},{\"end\":51987,\"start\":51984},{\"end\":52344,\"start\":52343},{\"end\":52353,\"start\":52352},{\"end\":52364,\"start\":52363},{\"end\":52374,\"start\":52373},{\"end\":52387,\"start\":52386},{\"end\":52389,\"start\":52388},{\"end\":52675,\"start\":52674},{\"end\":52686,\"start\":52685},{\"end\":52698,\"start\":52697},{\"end\":52700,\"start\":52699},{\"end\":52709,\"start\":52708},{\"end\":52722,\"start\":52721},{\"end\":52724,\"start\":52723},{\"end\":53124,\"start\":53123},{\"end\":53131,\"start\":53130},{\"end\":53138,\"start\":53137},{\"end\":53150,\"start\":53149},{\"end\":53562,\"start\":53561},{\"end\":53574,\"start\":53573},{\"end\":53576,\"start\":53575},{\"end\":53585,\"start\":53584},{\"end\":53959,\"start\":53958},{\"end\":53969,\"start\":53968},{\"end\":53981,\"start\":53980},{\"end\":54264,\"start\":54263},{\"end\":54271,\"start\":54270},{\"end\":54277,\"start\":54276},{\"end\":54289,\"start\":54288},{\"end\":54750,\"start\":54749},{\"end\":54757,\"start\":54756},{\"end\":54763,\"start\":54762},{\"end\":54765,\"start\":54764},{\"end\":54777,\"start\":54776},{\"end\":55046,\"start\":55045},{\"end\":55054,\"start\":55053},{\"end\":55067,\"start\":55066},{\"end\":55409,\"start\":55408},{\"end\":55420,\"start\":55419},{\"end\":55430,\"start\":55429},{\"end\":55442,\"start\":55441},{\"end\":55926,\"start\":55925},{\"end\":55934,\"start\":55933},{\"end\":55941,\"start\":55940},{\"end\":55948,\"start\":55947},{\"end\":55958,\"start\":55957},{\"end\":55960,\"start\":55959},{\"end\":55972,\"start\":55971},{\"end\":55974,\"start\":55973},{\"end\":55981,\"start\":55980},{\"end\":56591,\"start\":56590},{\"end\":56601,\"start\":56600},{\"end\":56610,\"start\":56609},{\"end\":56620,\"start\":56619},{\"end\":56866,\"start\":56865},{\"end\":56876,\"start\":56875},{\"end\":56884,\"start\":56883},{\"end\":56894,\"start\":56893},{\"end\":56896,\"start\":56895},{\"end\":57124,\"start\":57123},{\"end\":57134,\"start\":57133},{\"end\":57142,\"start\":57141},{\"end\":57151,\"start\":57150},{\"end\":57368,\"start\":57367},{\"end\":57378,\"start\":57377},{\"end\":57390,\"start\":57389},{\"end\":57392,\"start\":57391},{\"end\":57401,\"start\":57400},{\"end\":57708,\"start\":57707},{\"end\":57716,\"start\":57715},{\"end\":57943,\"start\":57942},{\"end\":57951,\"start\":57950},{\"end\":57957,\"start\":57956},{\"end\":57963,\"start\":57962},{\"end\":57970,\"start\":57969},{\"end\":58337,\"start\":58336},{\"end\":58343,\"start\":58342},{\"end\":58355,\"start\":58354},{\"end\":58364,\"start\":58363},{\"end\":58366,\"start\":58365},{\"end\":58372,\"start\":58371},{\"end\":58565,\"start\":58564},{\"end\":58571,\"start\":58570},{\"end\":58577,\"start\":58576},{\"end\":58583,\"start\":58582},{\"end\":58591,\"start\":58590},{\"end\":58599,\"start\":58598},{\"end\":59088,\"start\":59087},{\"end\":59096,\"start\":59095},{\"end\":59449,\"start\":59448},{\"end\":59458,\"start\":59457},{\"end\":59468,\"start\":59467},{\"end\":59478,\"start\":59477},{\"end\":59736,\"start\":59735},{\"end\":59745,\"start\":59744},{\"end\":59754,\"start\":59753},{\"end\":59760,\"start\":59759},{\"end\":59762,\"start\":59761},{\"end\":59771,\"start\":59770},{\"end\":60058,\"start\":60057},{\"end\":60067,\"start\":60066},{\"end\":60077,\"start\":60076},{\"end\":60088,\"start\":60087},{\"end\":60090,\"start\":60089},{\"end\":60099,\"start\":60098}]", "bib_author_last_name": "[{\"end\":46216,\"start\":46207},{\"end\":46545,\"start\":46540},{\"end\":46557,\"start\":46549},{\"end\":46569,\"start\":46561},{\"end\":46990,\"start\":46979},{\"end\":47001,\"start\":46996},{\"end\":47013,\"start\":47005},{\"end\":47026,\"start\":47017},{\"end\":47035,\"start\":47030},{\"end\":47525,\"start\":47517},{\"end\":47535,\"start\":47529},{\"end\":47550,\"start\":47541},{\"end\":47562,\"start\":47554},{\"end\":47575,\"start\":47566},{\"end\":47968,\"start\":47959},{\"end\":47984,\"start\":47972},{\"end\":47996,\"start\":47988},{\"end\":48009,\"start\":48000},{\"end\":48307,\"start\":48297},{\"end\":48314,\"start\":48311},{\"end\":48324,\"start\":48320},{\"end\":48332,\"start\":48328},{\"end\":48597,\"start\":48594},{\"end\":48604,\"start\":48601},{\"end\":48616,\"start\":48608},{\"end\":48623,\"start\":48620},{\"end\":48629,\"start\":48627},{\"end\":48638,\"start\":48633},{\"end\":48830,\"start\":48824},{\"end\":48839,\"start\":48834},{\"end\":48851,\"start\":48843},{\"end\":48862,\"start\":48855},{\"end\":48874,\"start\":48866},{\"end\":48887,\"start\":48878},{\"end\":49267,\"start\":49262},{\"end\":49283,\"start\":49273},{\"end\":49295,\"start\":49289},{\"end\":49307,\"start\":49299},{\"end\":49316,\"start\":49311},{\"end\":49322,\"start\":49320},{\"end\":49334,\"start\":49326},{\"end\":49343,\"start\":49338},{\"end\":49351,\"start\":49347},{\"end\":49357,\"start\":49355},{\"end\":49365,\"start\":49361},{\"end\":49371,\"start\":49369},{\"end\":49377,\"start\":49375},{\"end\":49711,\"start\":49707},{\"end\":49719,\"start\":49715},{\"end\":49727,\"start\":49723},{\"end\":49735,\"start\":49731},{\"end\":49952,\"start\":49946},{\"end\":49964,\"start\":49956},{\"end\":49974,\"start\":49968},{\"end\":49991,\"start\":49978},{\"end\":50212,\"start\":50206},{\"end\":50224,\"start\":50216},{\"end\":50234,\"start\":50228},{\"end\":50251,\"start\":50238},{\"end\":50260,\"start\":50255},{\"end\":50735,\"start\":50732},{\"end\":50742,\"start\":50739},{\"end\":50750,\"start\":50746},{\"end\":51183,\"start\":51178},{\"end\":51194,\"start\":51187},{\"end\":51207,\"start\":51198},{\"end\":51672,\"start\":51667},{\"end\":51680,\"start\":51676},{\"end\":51689,\"start\":51684},{\"end\":51701,\"start\":51693},{\"end\":51961,\"start\":51958},{\"end\":51971,\"start\":51967},{\"end\":51980,\"start\":51975},{\"end\":51993,\"start\":51988},{\"end\":52350,\"start\":52345},{\"end\":52361,\"start\":52354},{\"end\":52371,\"start\":52365},{\"end\":52384,\"start\":52375},{\"end\":52395,\"start\":52390},{\"end\":52683,\"start\":52676},{\"end\":52695,\"start\":52687},{\"end\":52706,\"start\":52701},{\"end\":52719,\"start\":52710},{\"end\":52730,\"start\":52725},{\"end\":53128,\"start\":53125},{\"end\":53135,\"start\":53132},{\"end\":53147,\"start\":53139},{\"end\":53153,\"start\":53151},{\"end\":53571,\"start\":53563},{\"end\":53582,\"start\":53577},{\"end\":53592,\"start\":53586},{\"end\":53966,\"start\":53960},{\"end\":53978,\"start\":53970},{\"end\":53986,\"start\":53982},{\"end\":54268,\"start\":54265},{\"end\":54274,\"start\":54272},{\"end\":54286,\"start\":54278},{\"end\":54293,\"start\":54290},{\"end\":54754,\"start\":54751},{\"end\":54760,\"start\":54758},{\"end\":54774,\"start\":54766},{\"end\":54781,\"start\":54778},{\"end\":55051,\"start\":55047},{\"end\":55064,\"start\":55055},{\"end\":55071,\"start\":55068},{\"end\":55417,\"start\":55410},{\"end\":55427,\"start\":55421},{\"end\":55439,\"start\":55431},{\"end\":55451,\"start\":55443},{\"end\":55931,\"start\":55927},{\"end\":55938,\"start\":55935},{\"end\":55945,\"start\":55942},{\"end\":55955,\"start\":55949},{\"end\":55969,\"start\":55961},{\"end\":55978,\"start\":55975},{\"end\":55990,\"start\":55982},{\"end\":56598,\"start\":56592},{\"end\":56607,\"start\":56602},{\"end\":56617,\"start\":56611},{\"end\":56626,\"start\":56621},{\"end\":56873,\"start\":56867},{\"end\":56881,\"start\":56877},{\"end\":56891,\"start\":56885},{\"end\":56902,\"start\":56897},{\"end\":57131,\"start\":57125},{\"end\":57139,\"start\":57135},{\"end\":57148,\"start\":57143},{\"end\":57158,\"start\":57152},{\"end\":57375,\"start\":57369},{\"end\":57387,\"start\":57379},{\"end\":57398,\"start\":57393},{\"end\":57409,\"start\":57402},{\"end\":57713,\"start\":57709},{\"end\":57721,\"start\":57717},{\"end\":57948,\"start\":57944},{\"end\":57954,\"start\":57952},{\"end\":57960,\"start\":57958},{\"end\":57967,\"start\":57964},{\"end\":57975,\"start\":57971},{\"end\":58340,\"start\":58338},{\"end\":58352,\"start\":58344},{\"end\":58361,\"start\":58356},{\"end\":58369,\"start\":58367},{\"end\":58381,\"start\":58373},{\"end\":58568,\"start\":58566},{\"end\":58574,\"start\":58572},{\"end\":58580,\"start\":58578},{\"end\":58588,\"start\":58584},{\"end\":58596,\"start\":58592},{\"end\":58607,\"start\":58600},{\"end\":59093,\"start\":59089},{\"end\":59103,\"start\":59097},{\"end\":59455,\"start\":59450},{\"end\":59465,\"start\":59459},{\"end\":59475,\"start\":59469},{\"end\":59484,\"start\":59479},{\"end\":59742,\"start\":59737},{\"end\":59751,\"start\":59746},{\"end\":59757,\"start\":59755},{\"end\":59768,\"start\":59763},{\"end\":59776,\"start\":59772},{\"end\":60064,\"start\":60059},{\"end\":60074,\"start\":60068},{\"end\":60085,\"start\":60078},{\"end\":60096,\"start\":60091},{\"end\":60104,\"start\":60100}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":46261,\"start\":46197},{\"attributes\":{\"doi\":\"accessed: 2010-09-30\",\"id\":\"b1\"},\"end\":46481,\"start\":46263},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":204800967},\"end\":46905,\"start\":46483},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":219629756},\"end\":47444,\"start\":46907},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":76666456},\"end\":47865,\"start\":47446},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":220686868},\"end\":48293,\"start\":47867},{\"attributes\":{\"id\":\"b6\"},\"end\":48536,\"start\":48295},{\"attributes\":{\"id\":\"b7\"},\"end\":48773,\"start\":48538},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":218889832},\"end\":49205,\"start\":48775},{\"attributes\":{\"doi\":\"CoRR abs/1512.03012\",\"id\":\"b9\"},\"end\":49622,\"start\":49207},{\"attributes\":{\"doi\":\"CoRR abs/1908.07423\",\"id\":\"b10\"},\"end\":49903,\"start\":49624},{\"attributes\":{\"doi\":\"CoRR abs/1904.03419\",\"id\":\"b11\"},\"end\":50134,\"start\":49905},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":219962806},\"end\":50665,\"start\":50136},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":219615815},\"end\":51110,\"start\":50667},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":11492613},\"end\":51607,\"start\":51112},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13549534},\"end\":51900,\"start\":51609},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52965638},\"end\":52298,\"start\":51902},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5328073},\"end\":52620,\"start\":52300},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":102351100},\"end\":53059,\"start\":52622},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":199668903},\"end\":53499,\"start\":53061},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":645845},\"end\":53892,\"start\":53501},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":21687255},\"end\":54181,\"start\":53894},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10328909},\"end\":54667,\"start\":54183},{\"attributes\":{\"doi\":\"CoRR abs/1506.01497\",\"id\":\"b23\"},\"end\":54950,\"start\":54669},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":237091854},\"end\":55331,\"start\":54952},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":210932537},\"end\":55839,\"start\":55333},{\"attributes\":{\"doi\":\"De- cember 7-12\",\"id\":\"b26\",\"matched_paper_id\":13936837},\"end\":56534,\"start\":55841},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":207984929},\"end\":56795,\"start\":56536},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":221105756},\"end\":57054,\"start\":56797},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":235128652},\"end\":57309,\"start\":57056},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221245720},\"end\":57624,\"start\":57311},{\"attributes\":{\"doi\":\"CoRR abs/1906.06514\",\"id\":\"b31\"},\"end\":57871,\"start\":57626},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":228083961},\"end\":58332,\"start\":57873},{\"attributes\":{\"id\":\"b33\"},\"end\":58506,\"start\":58334},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":221266014},\"end\":59016,\"start\":58508},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":212747811},\"end\":59386,\"start\":59018},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":51692385},\"end\":59659,\"start\":59388},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":224413564},\"end\":60008,\"start\":59661},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":208857374},\"end\":60481,\"start\":60010}]", "bib_title": "[{\"end\":46536,\"start\":46483},{\"end\":46975,\"start\":46907},{\"end\":47513,\"start\":47446},{\"end\":47953,\"start\":47867},{\"end\":48820,\"start\":48775},{\"end\":50202,\"start\":50136},{\"end\":50728,\"start\":50667},{\"end\":51174,\"start\":51112},{\"end\":51663,\"start\":51609},{\"end\":51952,\"start\":51902},{\"end\":52341,\"start\":52300},{\"end\":52672,\"start\":52622},{\"end\":53121,\"start\":53061},{\"end\":53559,\"start\":53501},{\"end\":53956,\"start\":53894},{\"end\":54261,\"start\":54183},{\"end\":55043,\"start\":54952},{\"end\":55406,\"start\":55333},{\"end\":55923,\"start\":55841},{\"end\":56588,\"start\":56536},{\"end\":56863,\"start\":56797},{\"end\":57121,\"start\":57056},{\"end\":57365,\"start\":57311},{\"end\":57940,\"start\":57873},{\"end\":58562,\"start\":58508},{\"end\":59085,\"start\":59018},{\"end\":59446,\"start\":59388},{\"end\":59733,\"start\":59661},{\"end\":60055,\"start\":60010}]", "bib_author": "[{\"end\":46218,\"start\":46199},{\"end\":46547,\"start\":46538},{\"end\":46559,\"start\":46547},{\"end\":46571,\"start\":46559},{\"end\":46992,\"start\":46977},{\"end\":47003,\"start\":46992},{\"end\":47015,\"start\":47003},{\"end\":47028,\"start\":47015},{\"end\":47037,\"start\":47028},{\"end\":47527,\"start\":47515},{\"end\":47537,\"start\":47527},{\"end\":47552,\"start\":47537},{\"end\":47564,\"start\":47552},{\"end\":47577,\"start\":47564},{\"end\":47970,\"start\":47955},{\"end\":47986,\"start\":47970},{\"end\":47998,\"start\":47986},{\"end\":48011,\"start\":47998},{\"end\":48309,\"start\":48295},{\"end\":48316,\"start\":48309},{\"end\":48326,\"start\":48316},{\"end\":48334,\"start\":48326},{\"end\":48599,\"start\":48592},{\"end\":48606,\"start\":48599},{\"end\":48618,\"start\":48606},{\"end\":48625,\"start\":48618},{\"end\":48631,\"start\":48625},{\"end\":48640,\"start\":48631},{\"end\":48832,\"start\":48822},{\"end\":48841,\"start\":48832},{\"end\":48853,\"start\":48841},{\"end\":48864,\"start\":48853},{\"end\":48876,\"start\":48864},{\"end\":48889,\"start\":48876},{\"end\":49269,\"start\":49258},{\"end\":49285,\"start\":49269},{\"end\":49297,\"start\":49285},{\"end\":49309,\"start\":49297},{\"end\":49318,\"start\":49309},{\"end\":49324,\"start\":49318},{\"end\":49336,\"start\":49324},{\"end\":49345,\"start\":49336},{\"end\":49353,\"start\":49345},{\"end\":49359,\"start\":49353},{\"end\":49367,\"start\":49359},{\"end\":49373,\"start\":49367},{\"end\":49379,\"start\":49373},{\"end\":49713,\"start\":49705},{\"end\":49721,\"start\":49713},{\"end\":49729,\"start\":49721},{\"end\":49737,\"start\":49729},{\"end\":49954,\"start\":49944},{\"end\":49966,\"start\":49954},{\"end\":49976,\"start\":49966},{\"end\":49993,\"start\":49976},{\"end\":50214,\"start\":50204},{\"end\":50226,\"start\":50214},{\"end\":50236,\"start\":50226},{\"end\":50253,\"start\":50236},{\"end\":50262,\"start\":50253},{\"end\":50737,\"start\":50730},{\"end\":50744,\"start\":50737},{\"end\":50752,\"start\":50744},{\"end\":51185,\"start\":51176},{\"end\":51196,\"start\":51185},{\"end\":51209,\"start\":51196},{\"end\":51674,\"start\":51665},{\"end\":51682,\"start\":51674},{\"end\":51691,\"start\":51682},{\"end\":51703,\"start\":51691},{\"end\":51963,\"start\":51954},{\"end\":51973,\"start\":51963},{\"end\":51982,\"start\":51973},{\"end\":51995,\"start\":51982},{\"end\":52352,\"start\":52343},{\"end\":52363,\"start\":52352},{\"end\":52373,\"start\":52363},{\"end\":52386,\"start\":52373},{\"end\":52397,\"start\":52386},{\"end\":52685,\"start\":52674},{\"end\":52697,\"start\":52685},{\"end\":52708,\"start\":52697},{\"end\":52721,\"start\":52708},{\"end\":52732,\"start\":52721},{\"end\":53130,\"start\":53123},{\"end\":53137,\"start\":53130},{\"end\":53149,\"start\":53137},{\"end\":53155,\"start\":53149},{\"end\":53573,\"start\":53561},{\"end\":53584,\"start\":53573},{\"end\":53594,\"start\":53584},{\"end\":53968,\"start\":53958},{\"end\":53980,\"start\":53968},{\"end\":53988,\"start\":53980},{\"end\":54270,\"start\":54263},{\"end\":54276,\"start\":54270},{\"end\":54288,\"start\":54276},{\"end\":54295,\"start\":54288},{\"end\":54756,\"start\":54749},{\"end\":54762,\"start\":54756},{\"end\":54776,\"start\":54762},{\"end\":54783,\"start\":54776},{\"end\":55053,\"start\":55045},{\"end\":55066,\"start\":55053},{\"end\":55073,\"start\":55066},{\"end\":55419,\"start\":55408},{\"end\":55429,\"start\":55419},{\"end\":55441,\"start\":55429},{\"end\":55453,\"start\":55441},{\"end\":55933,\"start\":55925},{\"end\":55940,\"start\":55933},{\"end\":55947,\"start\":55940},{\"end\":55957,\"start\":55947},{\"end\":55971,\"start\":55957},{\"end\":55980,\"start\":55971},{\"end\":55992,\"start\":55980},{\"end\":56600,\"start\":56590},{\"end\":56609,\"start\":56600},{\"end\":56619,\"start\":56609},{\"end\":56628,\"start\":56619},{\"end\":56875,\"start\":56865},{\"end\":56883,\"start\":56875},{\"end\":56893,\"start\":56883},{\"end\":56904,\"start\":56893},{\"end\":57133,\"start\":57123},{\"end\":57141,\"start\":57133},{\"end\":57150,\"start\":57141},{\"end\":57160,\"start\":57150},{\"end\":57377,\"start\":57367},{\"end\":57389,\"start\":57377},{\"end\":57400,\"start\":57389},{\"end\":57411,\"start\":57400},{\"end\":57715,\"start\":57707},{\"end\":57723,\"start\":57715},{\"end\":57950,\"start\":57942},{\"end\":57956,\"start\":57950},{\"end\":57962,\"start\":57956},{\"end\":57969,\"start\":57962},{\"end\":57977,\"start\":57969},{\"end\":58342,\"start\":58336},{\"end\":58354,\"start\":58342},{\"end\":58363,\"start\":58354},{\"end\":58371,\"start\":58363},{\"end\":58383,\"start\":58371},{\"end\":58570,\"start\":58564},{\"end\":58576,\"start\":58570},{\"end\":58582,\"start\":58576},{\"end\":58590,\"start\":58582},{\"end\":58598,\"start\":58590},{\"end\":58609,\"start\":58598},{\"end\":59095,\"start\":59087},{\"end\":59105,\"start\":59095},{\"end\":59457,\"start\":59448},{\"end\":59467,\"start\":59457},{\"end\":59477,\"start\":59467},{\"end\":59486,\"start\":59477},{\"end\":59744,\"start\":59735},{\"end\":59753,\"start\":59744},{\"end\":59759,\"start\":59753},{\"end\":59770,\"start\":59759},{\"end\":59778,\"start\":59770},{\"end\":60066,\"start\":60057},{\"end\":60076,\"start\":60066},{\"end\":60087,\"start\":60076},{\"end\":60098,\"start\":60087},{\"end\":60106,\"start\":60098}]", "bib_venue": "[{\"end\":46714,\"start\":46651},{\"end\":47200,\"start\":47127},{\"end\":49004,\"start\":48955},{\"end\":50425,\"start\":50352},{\"end\":50915,\"start\":50842},{\"end\":51305,\"start\":51288},{\"end\":52110,\"start\":52061},{\"end\":52814,\"start\":52802},{\"end\":53298,\"start\":53235},{\"end\":53676,\"start\":53659},{\"end\":55602,\"start\":55536},{\"end\":56156,\"start\":56132},{\"end\":58719,\"start\":58708},{\"end\":59218,\"start\":59170},{\"end\":60269,\"start\":60196},{\"end\":46338,\"start\":46263},{\"end\":46649,\"start\":46571},{\"end\":47125,\"start\":47037},{\"end\":47642,\"start\":47577},{\"end\":48056,\"start\":48011},{\"end\":48403,\"start\":48334},{\"end\":48590,\"start\":48538},{\"end\":48953,\"start\":48889},{\"end\":49256,\"start\":49207},{\"end\":49703,\"start\":49624},{\"end\":49942,\"start\":49905},{\"end\":50350,\"start\":50262},{\"end\":50840,\"start\":50752},{\"end\":51265,\"start\":51209},{\"end\":51746,\"start\":51703},{\"end\":52059,\"start\":51995},{\"end\":52438,\"start\":52397},{\"end\":52800,\"start\":52732},{\"end\":53233,\"start\":53155},{\"end\":53657,\"start\":53594},{\"end\":54028,\"start\":53988},{\"end\":54344,\"start\":54295},{\"end\":54747,\"start\":54669},{\"end\":55131,\"start\":55073},{\"end\":55534,\"start\":55453},{\"end\":56119,\"start\":56007},{\"end\":56644,\"start\":56628},{\"end\":56920,\"start\":56904},{\"end\":57176,\"start\":57160},{\"end\":57456,\"start\":57411},{\"end\":57705,\"start\":57626},{\"end\":58055,\"start\":57977},{\"end\":58661,\"start\":58609},{\"end\":59168,\"start\":59105},{\"end\":59502,\"start\":59486},{\"end\":59821,\"start\":59778},{\"end\":60194,\"start\":60106}]"}}}, "year": 2023, "month": 12, "day": 17}