{"id": 232290403, "updated": "2023-10-06 05:25:09.512", "metadata": {"title": "PSCC-Net: Progressive Spatio-Channel Correlation Network for Image Manipulation Detection and Localization", "authors": "[{\"first\":\"Xiaohong\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yaojie\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xiaoming\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "IEEE Transactions on Circuits and Systems for Video Technology", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "To defend against manipulation of image content, such as splicing, copy-move, and removal, we develop a Progressive Spatio-Channel Correlation Network (PSCC-Net) to detect and localize image manipulations. PSCC-Net processes the image in a two-path procedure: a top-down path that extracts local and global features and a bottom-up path that detects whether the input image is manipulated, and estimates its manipulation masks at multiple scales, where each mask is conditioned on the previous one. Different from the conventional encoder-decoder and no-pooling structures, PSCC-Net leverages features at different scales with dense cross-connections to produce manipulation masks in a coarse-to-fine fashion. Moreover, a Spatio-Channel Correlation Module (SCCM) captures both spatial and channel-wise correlations in the bottom-up path, which endows features with holistic cues, enabling the network to cope with a wide range of manipulation attacks. Thanks to the light-weight backbone and progressive mechanism, PSCC-Net can process 1,080P images at 50+ FPS. Extensive experiments demonstrate the superiority of PSCC-Net over the state-of-the-art methods on both detection and localization.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.10596", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tcsv/LiuLCL22", "doi": "10.1109/tcsvt.2022.3189545"}}, "content": {"source": {"pdf_hash": "611cccb3bd2af2a2b13c1f3a4e3facb7db81a8c5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2103.10596v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2103.10596", "status": "GREEN"}}, "grobid": {"id": "800e68691252f7e4a66a8a05d56aeee427583992", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/611cccb3bd2af2a2b13c1f3a4e3facb7db81a8c5.txt", "contents": "\nPSCC-Net: Progressive Spatio-Channel Correlation Network for Image Manipulation Detection and Localization\n\n\nXiaohong Liu \nYaojie Liu \nSenior Member, IEEEJun Chen \nSenior Member, IEEEXiaoming Liu \nPSCC-Net: Progressive Spatio-Channel Correlation Network for Image Manipulation Detection and Localization\n1Index Terms-Image manipulation detection and localizationprogressive mechanismattention mechanism\nTo defend against manipulation of image content, such as splicing, copy-move, and removal, we develop a Progressive Spatio-Channel Correlation Network (PSCC-Net) to detect and localize image manipulations. PSCC-Net processes the image in a two-path procedure: a top-down path that extracts local and global features and a bottom-up path that detects whether the input image is manipulated, and estimates its manipulation masks at multiple scales, where each mask is conditioned on the previous one. Different from the conventional encoder-decoder and nopooling structures, PSCC-Net leverages features at different scales with dense cross-connections to produce manipulation masks in a coarse-to-fine fashion. Moreover, a Spatio-Channel Correlation Module (SCCM) captures both spatial and channelwise correlations in the bottom-up path, which endows features with holistic cues, enabling the network to cope with a wide range of manipulation attacks. Thanks to the light-weight backbone and progressive mechanism, PSCC-Net can process 1, 080P images at 50+ FPS. Extensive experiments demonstrate the superiority of PSCC-Net over the state-of-the-art methods on both detection and localization. Codes and models are available at https://github.com/proteus1991/PSCC-Net.\n\nI. INTRODUCTION\n\nSeeing is believing? Not anymore. Recent advances on image manipulation techniques [1]- [4] enable easy editing of raw images, such as removing unwanted objects [5]- [8], face swapping [2], attribute changing [9], etc. Although such techniques are neutral, malicious attackers may utilize them to create deceitful content to propagate false information, e.g., fake news [10], insurance fraud [11], and Deepfake [12]- [15]. Thus, concerns of the adverse impact on social media and even real-world systems have been raised [16], [17]. To alleviate the concerns, it is crucial to develop reliable models to expose the manipulated images. While being used in machine and systems, the model is required to, at a minimal, distinguish manipulated images from pristine ones, where the objective is to detect. While being used for human's viewing, the model is  further required to estimate tampered areas in forged images, where the objective is to localize. Generally, image manipulation consists of the contentdependent process and content-independent process. The former includes splicing, copy-move, and removal, as shown in Fig. 1. Both splicing and copy-move are content-copying forgeries, where the splicing content is from a different donor image while the copy-move content is from the target image per se. Removal takes out certain objects from the target image and performs refilling via inpainting. Often, the contentdependent process follows the semantic arrangement in the target image, e.g., placing a car on the road and replacing one face with another, which makes the resulting image visually \"authentic\" and indistinguishable from the pristine one. However, based on image/camera trace analysis [19], [20], subtle patterns can still be revealed to indicate the manipulation. On the other hand, the content-independent process includes global modifications such as brightness/contrast change, blurring, noising and image compression. They barely create any disinformation, but their resultant noise may undermine the analysis of image/camera traces and potentially hide the discrepancy between the manipulated and pristine areas.\n\nTo defend against manipulations, many image manipulation detection and localization (IMDL) methods have been proposed in the past. In the early stages, methods are designed to handle a single type of manipulation. In recent years, works [11], [13], [18], [21]- [26] are proposed to build generic IMDL models for multiple manipulation types. However, there are still 3 major unsolved problems for IMDL: arXiv:2103.10596v2 [cs.CV] 13 Aug 2022 1) Scale variation: The forged area varies in sizes. Most prior works neglect the importance of scale variations and encounter difficulty when detecting forged areas of different sizes. Both the conventional encoder-decoder [23], [24] and no-pooling [11], [18] structures have difficulties in leveraging local and global features jointly, thus can only handle a limited scale variation.\n\n2) Image correlation: Manipulated regions can best be determined when compared to pristine regions, especially for splicing attacks. A naive learning of mapping from the manipulated image to manipulation mask may lead to an overfitting to the specific attack type in training. In contrast, considering the image spatial correlation can lead to a more generalized localization solution. Yet, such correlation is mostly neglected in prior works.\n\n3) Detection: In principle, manipulation detection and localization are highly relevant tasks, where the detection score can be simply derived from the response of the predicted manipulation mask, i.e., at least one part of the forged image has high response while no part of the pristine one does. However, most prior works assume the existence of manipulation in all input images. As a result, this could cause many false alarms on pristine images and make the detection unreliable.\n\nTo address the above issues, we propose a novel Progressive Spatio-Channel Correlation Network (PSCC-Net), as in Fig. 2. PSCC-Net consists of a top-down path and a bottomup path. In the top-down path, a backbone encoder first extracts the local and global features from an input image. We adopt the network structure of [27] as our encoder, whose dense connections among different scales facilitate information exchange. In the bottom-up path, we leverage the learned features to estimate 4 manipulation masks from small scales to large ones, where each mask serves as a prior in the nextscale estimation. Thanks to such a design, the final mask is estimated in a coarse-to-fine fashion, harvesting both the local and global information. This design enables a potential speed-up by terminating the bottom-up mask estimation, if the intermediate mask is satisfactory. Moreover, rather than investigating the response of predicted manipulation masks, we feed the learned features into a detection head to produce the score for binary classification.\n\nTo exploit image correlation, we propose a Spatio-Channel Correlation Module (SCCM) that grasps both spatial and channel-wise correlations at each bottom-up step. The spatial correlation aggregates the global context among local features. As the response from different channels might be associated with the same class (e.g., manipulated or pristine), the channelwise correlation computes the similarity among feature maps to enhance the representation in interest areas. Given the lightweight design of the encoder, PSCC-Net can process 1, 080P at 50+ FPS. Our proposed approach demonstrates a superior manipulation localization on several benchmarks. In addition, we show that the recent IMDL methods encounter difficulty in distinguishing manipulated images from pristine ones. By explicitly introducing a detection head, our method achieves the state of the art (SOTA) on manipulation detection.\n\nWe summarize the contributions of this work as follows:\n\n\u2022 A new PSCC-Net is proposed that performs favorably on manipulation detection and enables progressive improvement of manipulation localization in a coarse-to-fine fashion; \u2022 A novel SCCM module is designed to capture the spatial and channel-wise correlations for better generalization. SCCM avoids the use of massive annotated data to pretrain our feature extractor; \u2022 The SOTA results for both image manipulation detection and localization are successively achieved.\n\n\nII. RELATED WORK A. Image Manipulation Detection\n\nImage manipulation detection aims to distinguish manipulated images from pristine ones via image-level binary classification. There are two major approaches for this detection: the implicit manner [10], [28] and the explicit manner [29]. The former obtains the detection score by the statistics (e.g., average [10] or maximum [28] value) of the predicted manipulation mask, and the latter explicitly outputs the score from a dedicated classification module. Recent works [11], [18] focus on pixel-level manipulation localization but neglect the importance of image-level detection. Instead, this work leverages both manipulated and pristine images in training and jointly considers detection and localization of image manipulation.\n\n\nB. Image Manipulation Localization\n\nEarly works propose to localize the manipulation of one specific type, e.g., splicing [10], [19], [30]- [36], copy-move [28], [29], [37]- [40], removal [41]- [44], and the content-preserved process [24], [45]. Although most methods perform well on detecting that specific forgery type, they fall short in handling real-world cases, where usually the forgery type is unknown in advance and various types of forgery might be utilized in manipulation. In the related problem of face anti-spoofing, researchers also study how to localize the facial pixels covered with various spoof mediums [46].\n\nRecent works attempt to tackle multiple forgeries in one model. J-LSTM [21] and H-LSTM [24] integrate the LSTM and CNN to capture the boundary-discriminative features. However, due to the patch-based design, both methods are time-consuming, and the size of detectable regions is limited by the preset patch size. RGB-N [23] adopts the steganalysis rich model [47] and Faster R-CNN [48], but it can only provide bounding boxes instead of segmentation masks. Later, ManTra-Net [11] learns features to distinguish 385 known manipulation types and treats the problem as anomaly detection. To learn the distinguishable features, auxiliary labeled data, such as camera sensors, are used. SPAN [18] extends ManTra-Net to further model the spatial correlation via local self-attention blocks and pyramid propagation. However, as the correlation is only considered in the local region, ManTra-Net and SPAN fail to take full advantage of the spatial correlation and consequently have limited generalizability. In this work, our PSCC-Net utilizes a progressive mechanism to improve the multi-scale feature representation and SCCM modules to better explore spatial and channel-wise correlations. \n\n\nC. Progressive Mechanism\n\nProgressive mechanism tackles a challenging task in a coarse-to-fine fashion. It has been widely adopted in many low-level and high-level vision tasks, such as denoising [5], [49], inpainting [50], super-resolution [51], [52], and object detection [53]- [56]. The pyramid structure is commonly utilized to build multi-scale features. In this work, we propose a densely connected pyramid structure that progressively refines the manipulation mask from small scales to large ones, where each predicted mask serves as a prior for the next-scale estimation.\n\n\nD. Attention Mechanism\n\nThe pioneer work [57] proposes an attention mechanism to improve the feature representation with relatively low cost, which has been widely employed in various vision tasks [13], [29], [58]- [62]. According to the applied domain, the attention mechanism can be divided into two types: spatial attention [59] and channel-wise attention [58]. Recent works [63]- [65] take the benefit of both types to further improve the representation capability of DNN. These methods adopt separate schemes to explore the spatial and channel-wise attentions and thus require additional efforts to fuse them. In addition, due to memory limit, they can only apply to high-level features where the spatial size is small. In this work, a unified SCCM jointly explores the image correlation and discrepancy in both spatial domain and feature channels on the same features. Besides, owing to the dimensional reduction design, SCCM is able to adapt both low-level and high-level features with arbitrary sizes.\n\n\nIII. PSCC-NET\n\nOur PSCC-Net enables the detection and localization of various types of manipulations. As compared to the imagelevel detection, the pixel-level localization is more difficult. Therefore, PSCC-Net pays special attention to tackling the localization problem. Indeed, since the features for detection and localization are jointly learned, improving the localization performance will naturally benefit detection.\n\nA. Network Architecture 1) Top-Down Path: Most prior works use the conventional encoder-decoder [23], [24] and no-pooling structures [11], [18] to extract features. Since forged areas have various sizes, it is important to fuse local and global features to handle the scale variation. However, both structures extract features in a sequential pipeline and neglect feature fusion among different scales, and thus can only handle a limited scale variation. To address this issue, we adopt a light-weight backbone in [27], named HRNetV2p-W18. Following its default setting, the stage down-scaling ratio s is set to 2, and there are totally 4 stages.\n\nCompared to encoder-decoder and no-pooling structures, the benefits of our backbone are two-fold. First, features from different scales are computed in parallel. Hence, dense connections among different scales enable effective information exchange, which is beneficial for handling scale variations. Second, since the local and global feature fusion is performed for every scale, each feature contains sufficient information to predict a manipulation mask at the corresponding scale. Therefore, this backbone is in line with our progressive mechanism, where the prediction of each mask should rely on all local and global features to improve its accuracy. Indeed, except the predicted mask on the last scale, the others serve as a prior for the next-scale mask prediction. After the top-down path, the manipulated features on 4 scales are extracted. Then, we use the bottom-up path to perform manipulation detection and localization.\n\n2) Bottom-Up Path: The bottom-up path in PSCC-Net estimates the detection score and the manipulation mask. Specifically, the detection score is predicted based on the extracted features from the top-down-path via a detection head [27], then the manipulation mask is generated through a progressive mechanism with full supervision. In particular, the coarse-to-fine progressive mechanism mimics how human tackles complicated problems in daily life.\n\nWe denote the input image as I \u2208 R H\u00d7W \u00d73 . The extracted features at 4 scales are F 1 \u2208 R H\u00d7W \u00d7C , F 2 \u2208 R H/s\u00d7W/s\u00d7sC , F 3 \u2208 R H/s 2 \u00d7W/s 2 \u00d7s 2 C and F 4 \u2208 R H/s 3 \u00d7W/s 3 \u00d7s 3 C , and their corresponding masks are denoted as\nM 1 \u2208 R H\u00d7W , M 2 \u2208 R H/s\u00d7W/s , M 3 \u2208 R H/s 2 \u00d7W/s 2 and M 4 \u2208 R H/s 3 \u00d7W/s 3 .\nHere H, W , and C are the height, width, and channel number of the image/feature respectively. Formally, we have\nM n\u22121 = f n\u22121 (\u03c4 (M n ) \u00b7 F n\u22121 ), n = 2, 3, 4,(1)\nwhere f n denotes the SCCM on the nth scale, and \u03c4 is the upsampling operation (e.g., the bilinear interpolation). Since M 4 is the mask on the last scale, it can be directly expressed as M 4 = f 4 (F 4 ). For Scales 1-3, the feature on the current scale is associated with the upsampled mask from the previous scale for feature modulation. Then, the modulated feature is fed into SCCM to produce a manipulation mask.\n\nTo reduce the prediction difficulty, the proposed progressive mechanism avoids generating the mask at the finest scale directly. Instead, the mask on the coarsest scale is first predicted to locate the regions that are potentially forged based on current available information. The subsequent prediction on the finer scale can leverage the previous mask and pay more attention to those selected regions. This process continues until the generation of the manipulation mask at the finest scale, which serves as the final prediction. However, without explicit supervision on each scale, the intermediate masks might not follow the coarse-to-fine order. Therefore, full supervisions are applied on all scales to guide the mask estimation.\n\n\nB. Spatio-Channel Correlation Module\n\nAttention mechanisms are commonly used to modulate learned features according to their relative significance. As the final manipulation mask is binary, the localization can be considered as a pixel-level binary classification. Ideally, we expect the learned features on forged regions are similar to each other but distinct from those in pristine regions. In this case, a fundamental clustering method may suffice to produce an effective mask. Therefore, to better tackle manipulation localization, we propose a SCCM that employs the spatial attention to aggregate the pixel-level features based on their contextual correlations, and the channel-wise attention to consolidate the feature maps based on their channel correlations.\n\nWe illustrate the detailed structure of SCCM in Fig. 3, where the input feature X is of size H \u00d7 W \u00d7 C. Note that even though X is small (256\u00d7256), the size of its spatial correlation can be enormous (65, 536 \u00d7 65, 536), easily exceeding the memory limit. Therefore, we use function h to reshape the input X \u2208 R H\u00d7W \u00d7C to X \u2208 R HW/r 2 \u00d7Cr 2 , where each feature map is flattened to form a vector based on SCCM down-scaling ratio r. For instance, with r = 4, the size of spatial correlation is 4, 096 \u00d7 4, 096 instead of 65, 536 \u00d7 65, 536. Therefore, this operation preserves all feature information and avoids modeling the spatial correlation of potentially large size HW \u00d7 HW .\n\nTo build the spatial and channel-wise correlations, one may directly leverage X . However, additional flexibility could be achieved by introducing the embedded Gaussian function [59]. Therefore, we use the 1 \u00d7 1 convolution to build different functions g, \u03b8, and \u03c6 to transform X into new linear embeddings as X g = g(X ), X \u03b8 = \u03b8(X ), and X \u03c6 = \u03c6(X ), all with the same size as X . Subsequently, the spatial and channel-wise correlations (denoted as A s \u2208 R HW/r 2 \u00d7HW/r 2 and A c \u2208 R Cr 2 \u00d7Cr 2 ) of embedded features X \u03b8 and X \u03c6 are computed, and the Gaussian operation is implemented by Softmax function. In the end, the spatial and channel-wise attentions are realized by performing matrix multiplications A s X g and X g A c , respectively. Unlike prior methods [63]- [65] that employ two attentions on different features, we apply both to the same linear embedding for mutual accommodation. Indeed, applying attentions in this way reduces the difficulty of subsequent fusion process, and also saves computational operations in SCCM. Specifically, the spatial attention can be formulated as:\nY s = A s X g = softmax(X \u03b8 X T \u03c6 )X g ,(2)\nwhere Y s \u2208 R HW/r 2 \u00d7Cr 2 is the feature resulting from the application of spatial attention, and softmax(\u00b7) denotes the Softmax function. The element (i, j) in A s indicates the similarity between the feature vectors in the ith row of X \u03b8 and jth row of X \u03c6 . The more similar they are, the higher correlation they have. This helps the network to learn feature representations for distinguishing forged regions from pristine ones and avoid overfitting to a specific attack type in training. Similarly, the channel-wise attention is expressed as:\nY c = X g A c = X g softmax(X T \u03b8 X \u03c6 ),(3)\nwhere Y c \u2208 R HW/r 2 \u00d7Cr 2 is the feature resulting from the application of channel-wise attention. The element (i, j) in A c measures the similarity between the channel maps in the ith column of X \u03b8 and jth column of X \u03c6 . Since the response from different channels might be associated with the same class, e.g., manipulated or pristine, the channel-wise correlation aggregates feature maps based on their similarities to enhance the representation in forged regions.\n\nWe use h \u22121 to reshape Y s and Y c respectively back to Y s and Y c of size H \u00d7 W \u00d7 C. Further, two functions \u03c9 s and \u03c9 c are built by 1 \u00d7 1 convolution to improve their feature representations. The output features from \u03c9 s and \u03c9 c are complement to each other. As it is non-trivial to determine their relative significance, two learnable parameters \u03b1 s and \u03b1 c , both initialized as 1, are used for trade-off. The learned values of \u03b1 s and \u03b1 c can be found in supplementary. We also adopt the residual learning [66] to express the feature Z as:\nZ = X + \u03b1 s \u00b7 \u03c9 s (Y s ) + \u03b1 c \u00b7 \u03c9 c (Y c ).(4)\nThe final output of SCCM is a predicted mask with only one channel. To reduce the channel number in Z, we employ a mask generation block with the sequential order of Conv-ReLU-Conv-Sigmoid, where Conv is a 3 \u00d7 3 convolution.\n\n\nC. Loss Function\n\nTo train the PSCC-Net, we adopt the binary cross-entropy loss (L bce ) for both detection and localization tasks. The predicted detection score (s d ) is supervised by the groundtruth (GT) label (l d ) with 0 standing for pristine image and 1 for forged image. Moreover, full supervisions are applied on each predicted mask by downsampling the GT mask G 1 to G 2 , G 3 , and G 4 according to their corresponding sizes, with 0 standing for pristine pixel and 1 for forged pixel. The masks predicted through the progressive mechanism at different scales are considered to be of equal importance. Therefore, our final loss functionL can be expressed as:\nL = L bce (s d , l d ) + 1 4 4 m=1 L bce (M m , G m ).(5)\n\nD. Training Data Synthesis\n\nSince there is no standard IMDL dataset for training, a synthetic dataset is built to train and validate our PSCC-Net. This dataset includes four categories 1) splicing, 2) copy-move, 3) removal, and 4) pristine classes. For splicing, following [34], [67], we use the MS COCO [68] to generate spliced images, where one annotated region is randomly selected per image, and pasted into a different image after several transformations. We adopt the same transformation as [34] including the scale, rotation, shift and luminance changes. Since the spliced region is not necessarily an object, we use the Bezier curve [69] to generate random contours, then fill them to produce splicing masks. We follow the same processes above but randomly select donor and target images in KCMI [70], VISION [71], and Dresden [72] that are commonly used to identify camera source [20], to generate additional spliced images as supplementary. For copy-move, the dataset from [28] is adopted. For removal, inspired by [34], [67], we adopt the SOTA inpainting method [6] to fill one annotated region that is randomly removed from each chosen MS COCO image. As to the pristine class, we simply select images from the MS COCO dataset.  In summary, we have 116, 583 images in splicing class, 100, 000 images in copy-move class, 78, 246 images in removal class, and 81, 910 images in pristine class, thus \u223c0.38M in total. Examples of different manipulation types in our synthetic dataset are demonstrated in Fig 4. It should be emphasized that our training dataset is much smaller than that of MantraNet and SPAN, where massive annotated data (1.25M) is used to train their feature extractor, not to mention the large number of synthesized manipulations for training the rest of their networks.\n\nAs it is inefficient to train all manipulated images in one epoch, we uniformly sample 0.025M images per class to form a 0.1M dataset on-the-fly for training in each epoch. In addition, we also build a validation set that contains 4 \u00d7 100 images. The size of synthetic images are all set to 256 \u00d7 256.\n\n\nIV. EXPERIMENTS\n\nA. Experimental Setup 1) Test data: We evaluate the manipulation localization on four standard test datasets: Columbia [73], Coverage [38], CASIA [74] and NIST16 [75], and one real-world dataset: IMD20 [76]. To finetune PSCC-Net, we follow the same training/testing split on Coverage, CASIA, and NIST16 as in [18], [23] for fair comparisons. Specifically, Columbia [73] is a splicing dataset of 180 images. Coverage [38] is a copymove dataset of 100 images; for fine-tuning, it is split into 75/25 for training and testing. CASIA [74] (v1.0 + v2.0) includes both splicing and copy-move; for fine-tuning, 5, 123 images from v2.0 is adopted for training, and 921 images from v1.0 is for testing. NIST16 [75] has 564 images, involving all three manipulations; for fine-tuning, 404 images are used for training and 160 for testing. IMD20 [76] consists of 2, 010 real-life manipulated images collected from Internet, and involves all three manipulations as well. We summarize the manipulation types for each test dataset and the number of images for evaluating our pre-trained and fine-tuned models in Tab. I.\n\nAs the manipulation detection is not considered by recent works, there is no standard dataset for benchmarking. Since CASIA is the only test dataset in here that corresponds each manipulated image to its pristine image, we use both forged and pristine images and define an evaluation protocol for detection. This dataset is named CASIA-D and consists of 1, 842 images with 50% forged and 50% pristine.\n\n2) Metrics: To quantify the localization performance, following previous works [11], [18], we use pixel-level Area Under Curve (AUC) and F1 score on manipulation masks. To evaluate the detection performance, we use image-level AUC and F1 score, Equal Error Rate (EER), and True Positive Rate at 1% false positive rate (TPR 1% ). Since binary masks and detection scores are required to compute F1 scores, we adopt the EER threshold to binarize them.\n\n3) Implementation details: PSCC-Net is end-to-end trainable and light-weighted. Its top-down path and bottom-up path have 2.0 and 1.6 Million (M) parameters. In the bottom-up path, the detection head has 0.9 M and the rest part (for localization) has only 0.7 M parameters. In comparison, the ManTra-Net [11] and SPAN [18] have 3.8 and 3.7 M parameters, respectively. Implemented by PyTorch, our model is trained with GeForce GTX 1080Ti. We initialize our backbone with ImageNet pre-trained weights, and optimize the whole model by Adam [77] with a batch size of 10 and an initial learning rate of 2e-4. The learning rate is halved every 5 epochs and the total training period is 25 epochs.\n\nOur network can take arbitrary-size images as input. To avoid performance degradation caused by size mismatch between training (e.g., 256 \u00d7 256) and testing data (e.g., 4, 000 \u00d7 3, 000), at the end of top-down path, we resample the extracted features from the first to the last scales respectively into fixed sizes 256 \u00d7 256, 128 \u00d7 128, 64 \u00d7 64, and 32 \u00d7 32, where the ratio r in SCCM is set to 4, 2, 2, and 1 respectively to reduce the computational burden. The produced masks are resampled back to the same size as the input image for localization evaluation.\n\n\nB. Comparisons on Localization\n\nThe compared IMDL methods include J-LSTM [21], H-LSTM [24], RGB-N [23], ManTra-Net [11], and SPAN [18] where SPAN has reported the SOTA performance on localization. Following the evaluation protocol defined in SPAN [18], we compare the localization performance using two models: 1) the pre-trained model is trained on the synthetic dataset and evaluated on the full test datasets, and 2) the fine-tuned model is the pre-trained model further fine-tuned on the training split of test datasets and evaluated on their test split. The pre-trained model is to show the generalization ability of each method, and the fine-tuned model is to manifest their localization performance while the domain discrepancy has been greatly alleviated. Note that the reported results of all compared methods are either from their original papers or by running their public codes. 1) Pre-trained model: We choose the best pre-trained model based on the performance on our validation set. Tab. II shows the localization performance of pre-trained models for different methods on four standard datasets and one realworld dataset under pixel-level AUC. The pre-trained PSCC-Net achieves the best localization performance on Columbia, CASIA, NIST16, and IMD20, and ranks the second on Coverage. The most significant performance gain is achieved while tackling real-life manipulated images (5.6% \u2191). This validates that the PSCC-Net has the best generalization ability as compared to the others. We fail to achieve the best performance on Coverage, despite surpassing ManTra-Net 2.8% under AUC. The reason might be the imperfection of our training data for the case, where the copied object is intentionally moved to cover a pristine object with similar appearance. Indeed, by fine-tuning the pre-trained model on Coverage, PSCC-Net achieves the 0.4% gain over SPAN under AUC (Tab. III).\n\n2) Fine-tuned model: The network weights of the pretrained model are used to initiate the fine-tuned models that will be trained on the training split of Coverage, CASIA, and NIST16 datasets, respectively. The training strategy for finetuned models is the same as the one for pre-trained model, except setting the initial learning rate to 1e-4. We evaluate the fine-tuned models of different methods in Tab. III. For AUC, PSCC-Net surpasses baselines in all cases (over 2.4% to SPAN on average). As for F1 score, our model outperforms them with a large margin (over 16.6% to SPAN on average). This validates the effectiveness of our overall network design.\n\n3) Qualitative comparisons: We provide qualitative evaluations of manipulation localization on four standard test datasets and one real-life dataset shown in Fig. 5 and Fig. 6, respectively, where the best available model for each method is used to produce manipulation masks. Compared to ManTra-  Net [11] and SPAN [18], the predicted masks from our PSCC-Net achieve the best performance in terms of higher prediction accuracy (e.g., the 1st row in Fig. 5) and fewer false alarms (e.g., the 6th row in Fig. 5). In addition, the proposed method is less sensitive to the scale variation. Both large (e.g., the 5th row in Fig. 5) and small (e.g., the 7th row in Fig. 5) manipulations can be localized effectively. On the real-life dataset, PSCC-Net still performs much better than the other two (e.g., the 2th row in Fig. 6), which demonstrates its good generalization ability.  \n\n\nC. Comparisons on Detection\n\nSince ManTra-Net and SPAN are the best performing baselines in the localization evaluation, and ManTra-Net does not develop the fine-tuned model, we choose to use the pre-trained model for detection evaluation, in order to make comparisons to both of them. Although these two baselines make no direct attempt to perform detection, their estimated manipulation masks can be leveraged for this purpose. As such, we simply regard the average of the mask as their scores. For fair comparisons, we build a variant that adopts the same averaging strategy to calculate this score, denoted as PSCC-Net \u2020 . In Tab. IV, owing to our well-predicted manipulation masks, the PSCC-Net \u2020 achieves the best detection performance on all used metrics. Moreover, we depict the corresponding Receiver Operating Characteristic (ROC) curve in Fig. 7. It is evident that the detection performance can be dramatically improved by introducing a tailored head. With a favorable detection, the IMDL methods can be more efficient. That is, detection is performed before localization, and only the detected forgery is passed for localization. Our network design is compatible with this efficiency consideration as the detection head is placed at the beginning of the bottom-up path. The qualitative evaluations of manipulation detection are demonstrated in Fig. 8, where the predicted masks from SPAN [18] and our PSCC-Net on both pristine and manipulation images are compared. Without the existence-of-manipulation assumption, for pristine images, the corresponding predicted masks from our PSCC-Net are nearly blank. However, the ones from SPAN (a) Pristine image (b) Manipulated image (c) Pristine mask [18] (d) Manip. mask [18] (e) Our pristine mask (f) Our manip. mask (g) GT manip. mask suffer severe false alarms in most cases. As for the relevant manipulated images, the proposed method localizes the forged regions more accurately.\n\n\nD. Visualization of SCCM\n\nTo provide insights into SCCM, we visualize the spatial response map for forged and pristine pixels in M 3 , by examining its spatial correlation represented in A s . After interpolation, each row of A s is associated with one pixel (e.g., P 1 ) in the test image, and its grayscale spatial response map can be obtained by reshaping this row vector from 1 \u00d7 HW to H \u00d7W (e.g., P 1 response). In Fig. 9 (a), spliced, copy-moved, inpainted, and authentic images are shown from top to bottom respectively, each with one example. We select 3 representative pixels for each image and annotate as P 1 , P 2 , and P 3 . For manipulated images, P 1 and P 2 are from forged regions, and P 3 is from pristine regions; as for the authentic image, all pixels are pristine. We project their grayscale spatial response maps into Jet color map and overlay them on the manipulated image as in Figs. 9 (c-e). It can be seen that for manipulated images, the spatial response maps of P 1 and P 2 have high values in forged regions and low values in pristine regions at most cases, but the map of P 3 retains low values in all regions including the one providing the copied content (e.g., the P 3 response in the 2nd row of Fig. 9 (e)). As for the authentic image, the spatial response maps of all selected pixels retain low values consistently. This visualization indicates that the features in forged regions are successfully clustered together, thus justifies the effectiveness of spatial attention in SCCM.\n\nFor channel-wise correlation A c , it is hard to provide a comprehensible visualization. Instead, we choose to visualize one channel of Y c and compare it to the same channel of X to see if any region is enhanced. We visualize the 1st channel of X and Y c in Figs. 9 (f,g). Indeed, the forged region in Y c is consolidated compared to the one in X, and if the forged region does not exist (i.e., in the case of authentic images), no region is enhanced. This proves the effectiveness of channelwise attention in SCCM. \n\n\nE. Visualization of Predicted Manipulation Masks on Different Scales\n\nThe proposed PSCC-Net utilizes a progressive mechanism to reduce the prediction difficulty by avoiding generating the mask from the finest scale directly. Instead, the mask on the coarsest scale is first predicted to locate the regions that are potentially forged based on the current available information. The subsequent prediction on the finer scale can leverage the previous mask and pay more attention to those selected regions. This process repeatedly performs until generating the manipulation mask at the finest scale as our final prediction.\n\nHere, we visualize the performance improvement of manipulation localization from the Scale 4 to Scale 1. In Fig. 10, Mask 4, Mask 3, and Mask 2 are the variants that truncate the original model after generating manipulation masks on the 4th, 3rd, and 2nd scales, and Mask 1 is the output of  the original model. It can be seen that benefiting to the proposed progressive mechanism, the localization performance is gradually improved from Mask 4 to Mask 1 in terms of lower false alarms and clearer boundaries. More discussions about quantitative comparisons and terminating PSCC-Net earlier for runtime saving can be found in Sec. IV-F.\n\n\nF. Runtime Analysis and Ablation Study\n\nIn Tab. V, we test several variants of PSCC-Net to justify the network design, where all variants are pre-trained on our synthetic dataset. Average AUC/F1s are reported (in %), and the runtime (in proportion) is relative to that of PSCC-Net. Our full model takes 0.019s to process one 1, 080P image, whereas ManTra-Net and SPAN take 0.208s and 0.161s, respectively. Moreover, as shown in Fig. 2, terminating the PSCC-Net earlier on Mask 4, Mask 3 or Mask 2 in inference time is feasible and will not interfere the prediction of manipulation mask at that scale. From our experiments, terminating the prediction on Mask 4 can shorten the runtime to 0.012s, i.e., \u223c 37% additional saving. Though nonessential for research datasets, this time-saving is significant and economical in practical applications, e.g., 14.6 million photos are uploaded to Facebook per hour 1 . 1 https://www.pingdom.com/blog/social-media-in-2017/ The comparisons of Mask 4, Mask 3, Mask 2, and the original PSCC-Net demonstrate the gradual improvement in performance, which is a clear manifestation of our progressive mechanism. Since Mask 3 already performs well under AUC and F1 scores, it is a good stopping point for mask prediction.\n\nWe also build several variants for SCCM, including the ones without spatial and channel-wise attentions (w/o SA+CA), without spatial attention (w/o SA), without channel-wise attention (w/o CA), and without feature sharing (w/o FS), which obtains embeddings from different \u03b8 and \u03c6 functions to compute spatial and channel-wise similarities. The comparisons illustrate that both SA and CA outperform the baseline (w/o SA+CA), and the performance gain acquired from SA is more than that from CA. In addition, feature sharing not only slightly reduces the runtime, but also enables mutual accommodation between these two attentions to help SCCM achieve better results than the one employing different features (i.e., w/o FS).\n\n\nG. Robustness Analysis\n\nTo analyze the robustness of PSCC-Net for localization, we follow the distortion settings in [18] to degrade the raw manipulated images from Columbia and NIST16. These distortions include resizing images to a different scale (Resize), applying Gaussian blur with kernel size k (GSBlur), adding Gaussian noise with standard deviation \u03c3 (GSNoise), and performing JPEG compression with quality factor q (JPEGComp). In addition, we introduce a mixed version of the aforementioned distortions (Mixed), where the resizing scale, kernel size k, standard deviation \u03c3, quality factor q are randomly selected from the intervals [0.25, 0.78], [3,15], [3,15], and [50, 100], respectively. Tab. VI shows the robustness analysis of localization under pixel-level AUC with pre-trained models. The PSCC-Net is more robust than ManTra-Net and SPAN under all distortions. It is worth noting that resizing is commonly performed when uploading images to social media. Indeed, benefiting from the operation that resamples the manipulation features into the fixed sizes, the impact of resizing to PSCC-Net is the least as compared to the others. We also analyze the detection robustness of PSCC-Net with respect to various distortions on CASIA-D. In Tab. VII, it can be seen that our PSCC-Net is quite robust for detection, especially in the case where the JPEG compression is performed.\n\n\nH. Limitations\n\nPSCC-Net enables us to detect and localize various types of manipulations. As compared to image-level detection, the pixel-level localization is more challenging, especially while dealing with real-life manipulated images. Here we demonstrate some failure cases on IMD20 [76].\n\nIn Fig. 11, it is clear that for real-life manipulated images, the forged regions may have diverse sizes and shapes. In the first row, we show a specific case where the same pattern is copied several times but with different scales. Despite our method fails to localize all forged regions, it is less sensitive to scale variation as compared to ManTra-Net [11] and SPAN [18], owing to our tailored network design. In addition, our method may fail to localize the whole forged regions or only localize part of them in some cases (e.g., the last two rows). One possible reason is that some manipulation traces are elaborately removed by fabricators. Indeed, the compared IMDL methods also have difficulty to tackle these manipulated images. Note that even in theses cases, our PSCC- Net still performs relatively better than the SOTAs [11], [18] for image manipulation localization (e.g., see the 2rd row).\n\n\nV. CONCLUSION\n\nIn this work, a novel PSCC-Net is proposed to meet the challenge of advanced image manipulation techniques. We employ a progressive mechanism to predict the manipulation mask on all backbone scales, where each mask serves as a prior to help predict the next-scale mask. Moreover, a SCCM is designed to perform spatial and channel-wise attentions on extracted features, which provides holistic information to make our model more generalized to manipulation attacks. Extensive experiments demonstrate that our PSCC-Net outperforms the SOTA methods on both detection and localization. For future work, we will develop techniques for estimating the uncertainty of predicted manipulation masks to further improve the IMDL performance.\n\n\nX.Liu  is with the John Hopcroft Center, Shanghai Jiao Tong University, Shanghai, 200240, China (e-mail: xiaohongliu@sjtu.edu.cn). J. Chen is with the Department of Electrical and Computer Engineering, McMaster University, Hamilton, ON L8S 4K1, Canada (e-mail: chen-jun@mcmaster.ca). Y. Liu and X. Liu are with the Department of Computer Science and Engineering, Michigan State University, East Lansing, MI 48824, USA (email: {liuyaoj1, liuxm}@msu.edu). Most of the work are conducted when Xiaohong Liu was a visiting scholar at MSU. This material is based upon work partially supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00112090131.\n\nFig. 1 .\n1Examples of image manipulation localization. Three examples are splicing, copy-move, and removal manipulations respectively. With novel designs of progressive mechanism and correlation module, our method demonstrates robust and accurate estimation at different scales and types.\n\nFig. 2 .\n2The architecture of the proposed PSCC-Net. The detection score predicted by the detection head indicates if the input is manipulated or not. The accuracy of manipulation localization from Mask 4 to Mask 1 is gradually improved, e.g., the prediction of Mask 4 confuses the pasted (forged) region with the pristine (copied) one, while Mask 1 effectively fixes it.\n\nFig. 3 .\n3The structure of SCCM. Here \u2297 represents the matrix multiplication and \u2295 the element-wise addition; the red arrow shows the common feature flows; the pink and green arrows show the feature flows of spatial and channelwise attentions respectively.\n\nFig. 4 .\n4Examples from our synthetic dataset. The generated images of different manipulation types and their ground-truth masks are demonstrated.\n\nFig. 5 .\n5Qualitative localization evaluations on four standard test datasets. From top to bottom, our PSCC-Net is compared to SOTAs on Columbia, Coverage, CASIA, and NIST16 datasets respectively, each with two images.\n\nFig. 6 .\n6Qualitative localization evaluations on the IMD20 real-life dataset.\n\nFig. 7 .\n7ROC of different methods for detection. Our detection head successfully alleviates the influence of false alarms in pristine images, thus achieves the best result.\n\nFig. 8 .\n8Qualitative detection evaluations on CASIA-D. Since GT pristine masks are blank, they are not shown here for clarity.\n\nFig. 10 .\n10P1 response (d) P2 response (e) P3 response (f) 1st channel in X (g) 1st channel in YcFig. 9. Visualization of spatial and channel-wise attentions in SCCM. From top to bottom, we show the spliced, copy-moved, inpainted, and authentic images respectively.For each test image, we show its GT manipulation mask, 3 spatial response maps (one for each selected pixel), and the 1st channel map in X and Yc. Zoom in for details. Visualization of predicted manipulation masks from Scale 4 to Scale 1. From top to bottom, manipulated images are from Columbia, Coverage, CASIA, NIST16, and IMD20 respectively. All predicted masks are from our pre-trained model.\n\nFig. 11 .\n11Failure cases. Zoom in for details.\n\nTABLE I SUMMARY\nIOF TEST DATASETS FOR OUR PRE-TRAINED AND FINE-TUNED MODELS (# STANDS FOR THE NUMBER OF IMAGES. AND INDICATE WHETHER OR NOT THE MANIPULATION TYPE IS INVOLVED).Dataset \nPre-trained \nFine-tuned \nSplicing Copy-move Removal \n# Test \n# Train # Test \n\nColumbia \n180 \n\u2212 \n\u2212 \n\n\n\n\nCoverage \n100 \n75 \n25 \n\n\n\n\nCASIA \n6, 044 \n5, 123 \n921 \n\n\n\n\nNIST \n564 \n404 \n160 \n\n\n\n\nIMD20 \n2, 010 \n\u2212 \n\u2212 \n\n\n\n\n\n\nTABLE II LOCALIZATION\nIIAUC (%) OF PRE-TRAINED MODELS. TABLE III EVALUATION OF THE FINE-TUNED MODELS. LOCALIZATION AUC/F1S ARE REPORTED (IN %). MANTRA-NET IS NOT SHOWN HERE AS IT HAS ONLY DEVELOPED THE PRE-TRAINED MODEL.Method \nColumbia Coverage CASIA NIST16 IMD20 \n\nManTra-Net [11] 82.4 \n81.9 \n81.7 \n79.5 \n74.8 \n\nSPAN [18] \n93.6 \n92.2 \n79.7 \n84.0 \n75.0 \n\nPSCC-Net \n98.2 \n84.7 \n82.9 \n85.5 \n80.6 \n\nMethod \nCoverage \nCASIA \nNIST16 \n\nJ-LSTM [21] \n61.4 / -\n-/ -\n76.4 / -\n\nH-LSTM [24] 71.2 / -\n-/ -\n79.4 / -\n\nRGB-N [23] \n81.7 / 43.7 \n79.5 / 40.8 \n93.7 / 72.2 \n\nSPAN [18] \n93.7 / 55.8 \n83.8 / 38.2 \n96.1 / 58.2 \n\nPSCC-Net \n94.1 / 72.3 87.5 / 55.4 99.1 / 74.2 \n\n\n\nTABLE IV DETECTION\nIVEVALUATION ON CASIA-D, ALL REPORTED IN %.Method \nAUC \u2191 \nF1 \u2191 \nEER \u2193 \nTPR 1% \u2191 \n\nManTra-Net [11] 59.94 \n56.69 \n43.21 \n5.43 \n\nSPAN [18] \n67.33 \n63.48 \n36.47 \n5.54 \n\nPSCC-Net  \u2020 \n74.40 \n66.88 \n33.21 \n28.37 \n\nPSCC-Net \n99.65 \n97.12 2.83 \n95.65 \n\n\n\nTABLE V ABLATION\nVSTUDY OF PSCC-NET (AUC/F1 IN %). THE RUNTIME IS REPORTED IN PROPORTION TO THAT OF ORIGINAL PSCC-NET.Variants \nColumbia \nCoverage \nCASIA \nNIST16 \nRuntime \n\nMask 4 \n93.34 / 79.22 \n82.99 / 44.23 \n81.49 / 31.69 \n84.15 / 30.55 \n0.63 \n\nMask 3 \n98.08 / 92.41 \n83.48 / 47.29 \n82.55 / 34.64 \n85.25 / 33.55 \n0.75 \n\nMask 2 \n98.18 / 93.32 \n84.44 / 49.08 \n82.78 / 35.59 \n85.38 / 34.94 \n0.88 \n\nw/o CA+SA 85.78 / 70.32 \n79.95 / 43.27 \n79.26 / 31.06 \n79.58 / 31.73 \n0.84 \n\nw/o SA \n90.70 / 75.68 \n80.56 / 43.50 \n79.51 / 31.08 \n83.49 / 32.34 \n0.92 \n\nw/o CA \n94.50 / 85.34 \n82.16 / 45.04 \n82.63 / 35.97 \n84.65 / 33.42 \n0.92 \n\nw/o FS \n97.54 / 91.30 \n84.06 / 48.23 \n81.88 / 34.33 \n84.45 / 34.21 \n1.04 \n\nPSCC-Net \n98.19 / 93.45 \n84.65 / 49.78 82.93 / 36.27 85.47 / 35.73 1.00 \n\n\n\nTABLE VI ROBUSTNESS\nVIANALYSIS OF LOCALIZATION WITH RESPECT TO VARIOUS DISTORTIONS. PIXEL-LEVEL AUCS ARE REPORTED (IN %).Distortion \nResize \nResize \nGSBlur \nGSBlur \nGSNoise GSNoise \nJPEGComp \nJPEGComp \nMixed \nw/o distortion \n0.78\u00d7 \n0.25\u00d7 \nk = 3 \nk = 15 \n\u03c3 = 3 \n\u03c3 = 15 \nq = 100 \nq = 50 \n\nColumbia \n\nManTraNet [11] \n71.66 \n68.64 \n67.72 \n62.88 \n68.22 \n54.97 \n75.00 \n59.37 \n60.47 \n77.95 \n\nSPAN [18] \n89.99 \n69.08 \n78.97 \n67.70 \n75.11 \n65.80 \n93.32 \n74.62 \n62.54 \n93.60 \n\nPSCC-Net \n93.40 78.41 \n84.18 \n73.24 \n82.64 \n74.35 \n97.97 \n89.11 \n72.69 \n98.19 \n\nNIST16 \n\nManTraNet [11] \n77.43 \n75.52 \n77.46 \n74.55 \n67.41 \n58.55 \n77.91 \n74.38 \n64.82 \n78.05 \n\nSPAN [18] \n83.24 \n80.32 \n83.10 \n79.15 \n75.17 \n67.28 \n83.59 \n80.68 \n68.36 \n83.95 \n\nPSCC-Net \n85.29 85.01 \n85.38 \n79.93 \n78.42 \n76.65 \n85.40 \n85.37 \n73.93 \n85.47 \n\n\n\nTABLE VII ROBUSTNESS\nVIIANALYSIS OF DETECTION FOR PSCC-NET WITH RESPECT TO VARIOUS DISTORTIONS ON CASIA-D. IMAGE-LEVEL AUCS AND F1 SCORES ARE REPORTED (IN %).Distortion \nAUC \nF1 \n\nResize 0.78\u00d7 \n95.48 \n91.46 \n\nResize 0.25\u00d7 \n74.86 \n70.47 \n\nGSBlur k = 3 \n92.93 \n87.55 \n\nGSBlur k = 15 \n88.59 \n83.56 \n\nGSNoise \u03c3 = 3 \n89.78 \n83.37 \n\nGSNoise \u03c3 = 15 \n85.50 \n80.87 \n\nJPEGComp q = 100 \n99.44 \n96.47 \n\nJPEGComp q = 50 \n99.45 \n96.47 \n\nMixed \n87.55 \n83.51 \n\nw/o distortion \n99.65 97.12 \n\n\n\nManiGAN: Textguided image manipulation. B Li, X Qi, T Lukasiewicz, P H Torr, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020B. Li, X. Qi, T. Lukasiewicz, and P. H. Torr, \"ManiGAN: Text- guided image manipulation,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nMaskGAN: Towards diverse and interactive facial image manipulation. C.-H Lee, Z Liu, L Wu, P Luo, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020C.-H. Lee, Z. Liu, L. Wu, and P. Luo, \"MaskGAN: Towards diverse and interactive facial image manipulation,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nSemantic image manipulation using scene graphs. H Dhamo, A Farshad, I Laina, N Navab, G D Hager, F Tombari, C Rupprecht, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020H. Dhamo, A. Farshad, I. Laina, N. Navab, G. D. Hager, F. Tombari, and C. Rupprecht, \"Semantic image manipulation using scene graphs,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nOpen-Edit: Open-domain image manipulation with open-vocabulary instructions. X Liu, Z Lin, J Zhang, H Zhao, Q Tran, X Wang, H Li, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)2020X. Liu, Z. Lin, J. Zhang, H. Zhao, Q. Tran, X. Wang, and H. Li, \"Open-Edit: Open-domain image manipulation with open-vocabulary instructions,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2020.\n\nProgressive reconstruction of visual structure for image inpainting. J Li, F He, L Zhang, B Du, D Tao, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)J. Li, F. He, L. Zhang, B. Du, and D. Tao, \"Progressive reconstruction of visual structure for image inpainting,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2019.\n\nRecurrent feature reasoning for image inpainting. J Li, N Wang, L Zhang, B Du, D Tao, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020J. Li, N. Wang, L. Zhang, B. Du, and D. Tao, \"Recurrent feature reasoning for image inpainting,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nPrior guided GAN based semantic inpainting. A Lahiri, A K Jain, S Agrawal, P Mitra, P K Biswas, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020A. Lahiri, A. K. Jain, S. Agrawal, P. Mitra, and P. K. Biswas, \"Prior guided GAN based semantic inpainting,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nHighresolution image inpainting with iterative confidence feedback and guided upsampling. Y Zeng, Z Lin, J Yang, J Zhang, E Shechtman, H Lu, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)2020Y. Zeng, Z. Lin, J. Yang, J. Zhang, E. Shechtman, and H. Lu, \"High- resolution image inpainting with iterative confidence feedback and guided upsampling,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2020.\n\nInterpreting the latent space of GANs for semantic face editing. Y Shen, J Gu, X Tang, B Zhou, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020Y. Shen, J. Gu, X. Tang, and B. Zhou, \"Interpreting the latent space of GANs for semantic face editing,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nFighting fake news: Image splice detection via learned self-consistency. M Huh, A Liu, A Owens, A A Efros, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)M. Huh, A. Liu, A. Owens, and A. A. Efros, \"Fighting fake news: Image splice detection via learned self-consistency,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018.\n\nManTra-Net: Manipulation tracing network for detection and localization of image forgeries with anomalous features. Y Wu, W Abdalmageed, P Natarajan, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)Y. Wu, W. AbdAlmageed, and P. Natarajan, \"ManTra-Net: Manipulation tracing network for detection and localization of image forgeries with anomalous features,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2019.\n\nDeepfakes and beyond: A survey of face manipulation and fake detection. R Tolosana, R Vera-Rodriguez, J Fierrez, A Morales, J Ortega-Garcia, Information FusionR. Tolosana, R. Vera-Rodriguez, J. Fierrez, A. Morales, and J. Ortega- Garcia, \"Deepfakes and beyond: A survey of face manipulation and fake detection,\" Information Fusion, 2020.\n\nOn the detection of digital face manipulation. H Dang, F Liu, J Stehouwer, X Liu, A Jain, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020H. Dang, F. Liu, J. Stehouwer, X. Liu, and A. Jain, \"On the detection of digital face manipulation,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nMsta-net: Forgery detection by generating manipulation trace based on multi-scale selftexture attention. J Yang, S Xiao, A Li, W Lu, X Gao, Y Li, IEEE Trans. Circuit Syst. Video Technol. J. Yang, S. Xiao, A. Li, W. Lu, X. Gao, and Y. Li, \"Msta-net: Forgery detection by generating manipulation trace based on multi-scale self- texture attention,\" IEEE Trans. Circuit Syst. Video Technol., 2021.\n\nDetecting compressed deepfake videos in social networks using frame-temporality two-stream convolutional network. J Hu, X Liao, W Wang, Z Qin, IEEE Trans. Circuit Syst. Video Technol. J. Hu, X. Liao, W. Wang, and Z. Qin, \"Detecting compressed deepfake videos in social networks using frame-temporality two-stream convolu- tional network,\" IEEE Trans. Circuit Syst. Video Technol., 2021.\n\nSeeing no longer means believing. T Thomson, D Angus, P Dootson, In DailyT. Thomson, D. Angus, and P. Dootson, \"Seeing no longer means believing,\" In Daily. [Online]. Available: https://indaily.com.au/opinion/ 2020/11/04/seeing-should-not-mean-believing/\n\nIs that video real?. A Willingham, CNNA. Willingham, \"Is that video real?\" CNN.\n\nSPAN: Spatial pyramid attention network for image manipulation localization. X Hu, Z Zhang, Z Jiang, S Chaudhuri, Z Yang, R Nevatia, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)2020X. Hu, Z. Zhang, Z. Jiang, S. Chaudhuri, Z. Yang, and R. Nevatia, \"SPAN: Spatial pyramid attention network for image manipulation localization,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2020.\n\nNoiseprint: a CNN-based camera model fingerprint. D Cozzolino, L Verdoliva, IEEE Trans. Inf. Forensics Secur. D. Cozzolino and L. Verdoliva, \"Noiseprint: a CNN-based camera model fingerprint,\" IEEE Trans. Inf. Forensics Secur., 2019.\n\nCamera trace erasing. C Chen, Z Xiong, X Liu, F Wu, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020C. Chen, Z. Xiong, X. Liu, and F. Wu, \"Camera trace erasing,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nExploiting spatial structure for localizing manipulated image regions. J H Bappy, A K Roy-Chowdhury, J Bunk, L Nataraj, B Manjunath, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)J. H. Bappy, A. K. Roy-Chowdhury, J. Bunk, L. Nataraj, and B. Man- junath, \"Exploiting spatial structure for localizing manipulated image regions,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2017.\n\nImage splicing localization using a multi-task fully convolutional network (MFCN). R Salloum, Y Ren, C.-C J Kuo, Journal of Visual Communication and Image Representation. R. Salloum, Y. Ren, and C.-C. J. Kuo, \"Image splicing localization using a multi-task fully convolutional network (MFCN),\" Journal of Visual Communication and Image Representation, 2018.\n\nLearning rich features for image manipulation detection. P Zhou, X Han, V I Morariu, L S Davis, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)P. Zhou, X. Han, V. I. Morariu, and L. S. Davis, \"Learning rich features for image manipulation detection,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018.\n\nHybrid LSTM and encoder-decoder architecture for detection of image forgeries. J H Bappy, C Simons, L Nataraj, B Manjunath, A K Roy-Chowdhury, IEEE Trans. Image Process. J. H. Bappy, C. Simons, L. Nataraj, B. Manjunath, and A. K. Roy- Chowdhury, \"Hybrid LSTM and encoder-decoder architecture for de- tection of image forgeries,\" IEEE Trans. Image Process., 2019.\n\nProactive image manipulation detection. V Asnani, X Yin, T Hassner, S Liu, X Liu, Proceeding of IEEE Computer Vision and Pattern Recognition. eeding of IEEE Computer Vision and Pattern RecognitionNew Orleans, LAV. Asnani, X. Yin, T. Hassner, S. Liu, and X. Liu, \"Proactive image manipulation detection,\" in In Proceeding of IEEE Computer Vision and Pattern Recognition, New Orleans, LA, June 2022.\n\nReverse engineering of generative models: Inferring model hyperparameters from generated images. V Asnani, X Yin, T Hassner, X Liu, V. Asnani, X. Yin, T. Hassner, and X. Liu, \"Reverse engineering of generative models: Inferring model hyperparameters from generated images,\" 2021. [Online]. Available: https://arxiv.org/abs/2106.07873\n\nDeep high-resolution representation learning for visual recognition. J Wang, K Sun, T Cheng, B Jiang, C Deng, Y Zhao, D Liu, Y Mu, M Tan, X Wang, IEEE Trans. Pattern Anal. Mach. Intell. J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y. Zhao, D. Liu, Y. Mu, M. Tan, X. Wang et al., \"Deep high-resolution representation learning for visual recognition,\" IEEE Trans. Pattern Anal. Mach. Intell., 2020.\n\nBusterNet: Detecting copy-move image forgery with source/target localization. Y Wu, W Abd-Almageed, P Natarajan, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)Y. Wu, W. Abd-Almageed, and P. Natarajan, \"BusterNet: Detecting copy-move image forgery with source/target localization,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018.\n\nDOA-GAN: Dual-order attentive generative adversarial network for image copy-move forgery detection and localization. A Islam, C Long, A Basharat, A Hoogs, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020A. Islam, C. Long, A. Basharat, and A. Hoogs, \"DOA-GAN: Dual-order attentive generative adversarial network for image copy-move forgery detection and localization,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nExposing region splicing forgeries with blind local noise estimation. S Lyu, X Pan, X Zhang, Int. J. Comput. Vis. S. Lyu, X. Pan, and X. Zhang, \"Exposing region splicing forgeries with blind local noise estimation,\" Int. J. Comput. Vis., 2014.\n\nSplicebuster: A new blind image splicing detector. D Cozzolino, G Poggi, L Verdoliva, International Workshop on Information Forensics and Security (WIFS). D. Cozzolino, G. Poggi, and L. Verdoliva, \"Splicebuster: A new blind image splicing detector,\" in International Workshop on Information Forensics and Security (WIFS), 2015.\n\nLocalization of JPEG double compression through multi-domain convolutional neural networks. I Amerini, T Uricchio, L Ballan, R Caldelli, IEEE Conf. Comput. Vis. Pattern Recog. Worksh. CVPRWI. Amerini, T. Uricchio, L. Ballan, and R. Caldelli, \"Localization of JPEG double compression through multi-domain convolutional neu- ral networks,\" in IEEE Conf. Comput. Vis. Pattern Recog. Worksh. (CVPRW), 2017.\n\nTampering detection and localization through clustering of camerabased CNN features. L Bondi, S Lameri, D G\u00fcera, P Bestagini, E J Delp, S Tubaro, IEEE Conf. Comput. Vis. Pattern Recog. Worksh. CVPRWL. Bondi, S. Lameri, D. G\u00fcera, P. Bestagini, E. J. Delp, and S. Tubaro, \"Tampering detection and localization through clustering of camera- based CNN features,\" in IEEE Conf. Comput. Vis. Pattern Recog. Worksh. (CVPRW), 2017.\n\nDeep matching and validation network: An end-to-end solution to constrained image splicing localization and detection. Y Wu, W Abd-Almageed, P Natarajan, ACM Int. Conf. Multimedia. Y. Wu, W. Abd-Almageed, and P. Natarajan, \"Deep matching and validation network: An end-to-end solution to constrained image splicing localization and detection,\" in ACM Int. Conf. Multimedia, 2017.\n\nThe point where reality meets fantasy: Mixed adversarial generators for image splice detection. V V Kniaz, V Knyaz, F Remondino, Proc. Adv. Neural Inform. Process. Syst. (NeurIPS). Adv. Neural Inform. ess. Syst. (NeurIPS)V. V. Kniaz, V. Knyaz, and F. Remondino, \"The point where reality meets fantasy: Mixed adversarial generators for image splice detection,\" in Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 2019.\n\nMultitask se-network for image splicing localization. Y Zhang, G Zhu, L Wu, S Kwong, H Zhang, Y Zhou, IEEE Trans. Circuit Syst. Video Technol. Y. Zhang, G. Zhu, L. Wu, S. Kwong, H. Zhang, and Y. Zhou, \"Multi- task se-network for image splicing localization,\" IEEE Trans. Circuit Syst. Video Technol., 2021.\n\nEfficient dense-field copymove forgery detection. D Cozzolino, G Poggi, L Verdoliva, IEEE Trans. Inf. Forensics Secur. D. Cozzolino, G. Poggi, and L. Verdoliva, \"Efficient dense-field copy- move forgery detection,\" IEEE Trans. Inf. Forensics Secur., 2015.\n\nCOVERAGE-A novel database for copy-move forgery detection. B Wen, Y Zhu, R Subramanian, T.-T Ng, X Shen, S Winkler, Proc. IEEE Int. Conf. Image Process. (ICIP). IEEE Int. Conf. Image ess. (ICIP)B. Wen, Y. Zhu, R. Subramanian, T.-T. Ng, X. Shen, and S. Winkler, \"COVERAGE-A novel database for copy-move forgery detection,\" in Proc. IEEE Int. Conf. Image Process. (ICIP), 2016.\n\nImage copy-move forgery detection via an end-to-end deep neural network. Y Wu, W Abd-Almageed, P Natarajan, Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV). IEEE Winter Conf. Appl. Comput. Vis. (WACV)Y. Wu, W. Abd-Almageed, and P. Natarajan, \"Image copy-move forgery detection via an end-to-end deep neural network,\" in Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV), 2018.\n\nA patchmatchbased dense-field algorithm for video copy-move detection and localization. L D&apos;amiano, D Cozzolino, G Poggi, L Verdoliva, IEEE Trans. Circuit Syst. Video Technol. 293L. D'Amiano, D. Cozzolino, G. Poggi, and L. Verdoliva, \"A patchmatch- based dense-field algorithm for video copy-move detection and local- ization,\" IEEE Trans. Circuit Syst. Video Technol., vol. 29, no. 3, pp. 669-682, 2018.\n\nA deep learning approach to patch-based image inpainting forensics. X Zhu, Y Qian, X Zhao, B Sun, Y Sun, Image Communication. Signal ProcessingX. Zhu, Y. Qian, X. Zhao, B. Sun, and Y. Sun, \"A deep learning approach to patch-based image inpainting forensics,\" Signal Processing: Image Communication, 2018.\n\nSequential and patch analyses for object removal video forgery detection and localization. M Aloraini, M Sharifzadeh, D Schonfeld, IEEE Trans. Circuit Syst. Video Technol. 313M. Aloraini, M. Sharifzadeh, and D. Schonfeld, \"Sequential and patch analyses for object removal video forgery detection and localization,\" IEEE Trans. Circuit Syst. Video Technol., vol. 31, no. 3, pp. 917-930, 2020.\n\nSpatiotemporal trident networks: Detection and localization of object removal tampering in video passive forensics. Q Yang, D Yu, Z Zhang, Y Yao, L Chen, IEEE Trans. Circuit Syst. Video Technol. 3110Q. Yang, D. Yu, Z. Zhang, Y. Yao, and L. Chen, \"Spatiotemporal trident networks: Detection and localization of object removal tampering in video passive forensics,\" IEEE Trans. Circuit Syst. Video Technol., vol. 31, no. 10, pp. 4131 -4144, 2020.\n\nIid-net: Image inpainting detection network via neural architecture search and attention. H Wu, J Zhou, IEEE Trans. Circuit Syst. Video Technol. H. Wu and J. Zhou, \"Iid-net: Image inpainting detection network via neural architecture search and attention,\" IEEE Trans. Circuit Syst. Video Technol., 2021.\n\nLiterature survey on image manipulation detection. R M Joseph, A Chithra, International Research Journal of Engineering and Technology. IRJETR. M. Joseph and A. Chithra, \"Literature survey on image manipulation detection,\" International Research Journal of Engineering and Technol- ogy (IRJET), 2015.\n\nOn disentangling spoof traces for generic face anti-spoofing. Y Liu, J Stehouwer, X Liu, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)2020Y. Liu, J. Stehouwer, and X. Liu, \"On disentangling spoof traces for generic face anti-spoofing,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2020.\n\nRich models for steganalysis of digital images. J Fridrich, J Kodovsky, IEEE Trans. Inf. Forensics Secur. J. Fridrich and J. Kodovsky, \"Rich models for steganalysis of digital images,\" IEEE Trans. Inf. Forensics Secur., 2012.\n\nFaster R-CNN: Towards realtime object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Proc. Adv. Neural Inform. Process. Syst. (NeurIPS). Adv. Neural Inform. ess. Syst. (NeurIPS)S. Ren, K. He, R. Girshick, and J. Sun, \"Faster R-CNN: Towards real- time object detection with region proposal networks,\" in Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 2015.\n\nGated fusion network for single image dehazing. W Ren, L Ma, J Zhang, J Pan, X Cao, W Liu, M.-H Yang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)W. Ren, L. Ma, J. Zhang, J. Pan, X. Cao, W. Liu, and M.-H. Yang, \"Gated fusion network for single image dehazing,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018.\n\nProgressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations. P Yi, Z Wang, K Jiang, J Jiang, J Ma, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)P. Yi, Z. Wang, K. Jiang, J. Jiang, and J. Ma, \"Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2019.\n\nMulti-scale progressive fusion network for single image deraining. K Jiang, Z Wang, P Yi, C Chen, B Huang, Y Luo, J Ma, J Jiang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020K. Jiang, Z. Wang, P. Yi, C. Chen, B. Huang, Y. Luo, J. Ma, and J. Jiang, \"Multi-scale progressive fusion network for single image deraining,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nFSRNet: End-to-end learning face super-resolution with facial priors. Y Chen, Y Tai, X Liu, C Shen, J Yang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)Y. Chen, Y. Tai, X. Liu, C. Shen, and J. Yang, \"FSRNet: End-to-end learning face super-resolution with facial priors,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018.\n\nProgressive attention guided recurrent network for salient object detection. X Zhang, T Wang, J Qi, H Lu, G Wang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)X. Zhang, T. Wang, J. Qi, H. Lu, and G. Wang, \"Progressive attention guided recurrent network for salient object detection,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018.\n\nProgressFace: Scale-aware progressive learning for face detection. J Zhu, D Li, T Han, L Tian, Y Shan, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)2020J. Zhu, D. Li, T. Han, L. Tian, and Y. Shan, \"ProgressFace: Scale-aware progressive learning for face detection,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2020.\n\nProgressive refinement network for occluded pedestrian detection. X Song, K Zhao, W.-S C H Zhang, J Guo, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)2020X. Song, K. Zhao, W.-S. C. H. Zhang, and J. Guo, \"Progressive refinement network for occluded pedestrian detection,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2020.\n\nPedestrian detection with autoregressive network phases. G Brazil, X Liu, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)G. Brazil and X. Liu, \"Pedestrian detection with autoregressive network phases,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2019.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Proc. Adv. Neural Inform. Process. Syst. (NeurIPS). Adv. Neural Inform. ess. Syst. (NeurIPS)A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Proc. Adv. Neural Inform. Process. Syst. (NeurIPS), 2017.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)J. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018.\n\nNon-local neural networks. X Wang, R Girshick, A Gupta, K He, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)X. Wang, R. Girshick, A. Gupta, and K. He, \"Non-local neural net- works,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2018.\n\nGridDehazeNet: Attention-based multi-scale network for image dehazing. X Liu, Y Ma, Z Shi, J Chen, Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV). IEEE/CVF Int. Conf. Comput. Vis. (ICCV)X. Liu, Y. Ma, Z. Shi, and J. Chen, \"GridDehazeNet: Attention-based multi-scale network for image dehazing,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), 2019.\n\nVideo super-resolution with temporal group attention. T Isobe, S Li, X Jia, S Yuan, G Slabaugh, C Xu, Y.-L Li, S Wang, Q Tian, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2020T. Isobe, S. Li, X. Jia, S. Yuan, G. Slabaugh, C. Xu, Y.-L. Li, S. Wang, and Q. Tian, \"Video super-resolution with temporal group attention,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2020.\n\nMitigating face recognition bias via group adaptive classifier. S Gong, X Liu, A Jain, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)2021S. Gong, X. Liu, and A. Jain, \"Mitigating face recognition bias via group adaptive classifier,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2021.\n\nBAM: Bottleneck attention module. J Park, S Woo, J.-Y. Lee, I S Kweon, Proc. Brit. Mach. Vis. Conf. (BMVC). Brit. Mach. Vis. Conf. (BMVC)J. Park, S. Woo, J.-Y. Lee, and I. S. Kweon, \"BAM: Bottleneck attention module,\" in Proc. Brit. Mach. Vis. Conf. (BMVC), 2018.\n\nCBAM: Convolutional block attention module. S Woo, J Park, J.-Y. Lee, I So Kweon, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)S. Woo, J. Park, J.-Y. Lee, and I. So Kweon, \"CBAM: Convolutional block attention module,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2018.\n\nDual attention network for scene segmentation. J Fu, J Liu, H Tian, Y Li, Y Bao, Z Fang, H Lu, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, \"Dual attention network for scene segmentation,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2019.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR). IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), 2016.\n\nAdversarial learning for constrained image splicing detection and localization based on atrous convolution. Y Liu, X Zhu, X Zhao, Y Cao, IEEE Trans. Inf. Forensics Secur. Y. Liu, X. Zhu, X. Zhao, and Y. Cao, \"Adversarial learning for constrained image splicing detection and localization based on atrous convolution,\" IEEE Trans. Inf. Forensics Secur., 2019.\n\nMicrosoft COCO: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \"Microsoft COCO: Common objects in context,\" in Proc. Eur. Conf. Comput. Vis. (ECCV), 2014.\n\nMathematics for computer graphics applications. M E Mortenson, Industrial Press IncM. E. Mortenson, Mathematics for computer graphics applications. Industrial Press Inc., 1999.\n\nCamera model identification. I S P Society, I. S. P. Society, \"Camera model identification,\" https://www.kaggle.com/ c/sp-society-camera-model-identification.\n\nVISION: a video and image dataset for source identification. D Shullani, M Fontani, M Iuliani, O Shaya, A Piva, EURASIP Journal on Information Security. D. Shullani, M. Fontani, M. Iuliani, O. Al Shaya, and A. Piva, \"VISION: a video and image dataset for source identification,\" EURASIP Journal on Information Security, 2017.\n\nThe 'dresden image database' for benchmarking digital image forensics. T Gloe, R B\u00f6hme, ACM Symposium on Applied Computing. T. Gloe and R. B\u00f6hme, \"The 'dresden image database' for benchmarking digital image forensics,\" in ACM Symposium on Applied Computing, 2010.\n\nColumbia image splicing detection evaluation dataset. T.-T Ng, J Hsu, S.-F Chang, DVMM lab. Columbia Univ CalPhotos Digit LibrT.-T. Ng, J. Hsu, and S.-F. Chang, \"Columbia image splicing detection evaluation dataset,\" DVMM lab. Columbia Univ CalPhotos Digit Libr, 2009.\n\nCasia image tampering detection evaluation database. J Dong, W Wang, T Tan, China Summit and International Conference on Signal and Information Processing. J. Dong, W. Wang, and T. Tan, \"Casia image tampering detection evaluation database,\" in China Summit and International Conference on Signal and Information Processing, 2013.\n\nNIST: Nist nimble 2016 datasets. \"NIST: Nist nimble 2016 datasets,\" https://www.nist.gov/itl/iad/mig/, 2016.\n\nIMD2020: A large-scale annotated dataset tailored for detecting manipulated images. A Novozamsky, B Mahdian, S Saic, Proc. IEEE Winter Conf. Appl. Comput. Vis. Worksh. (WACVW). IEEE Winter Conf. Appl. Comput. Vis. Worksh. (WACVW)2020A. Novozamsky, B. Mahdian, and S. Saic, \"IMD2020: A large-scale annotated dataset tailored for detecting manipulated images,\" in Proc. IEEE Winter Conf. Appl. Comput. Vis. Worksh. (WACVW), 2020.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, Proc. Int. Conf. Learn. Represent. Int. Conf. Learn. RepresentD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" Proc. Int. Conf. Learn. Represent., 2015.\n\nHis research interests include image/video restoration and image segmentation. He was the recipient of the Ontario Graduate Scholarship in 2019, NSERC Alexander Graham Bell Canada Graduate Scholarship-Doctoral and Borealis AI Global Fellowship award in 2020. He is a reviewer of several IEEE journals. IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECH-NOLOGY, and IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS. Hamilton, ON, Canada; Chengdu, China; Shanghai, China2021Southwest Jiaotong University ; Shanghai Jiao Tong UniversityXiaohong Liu received the Ph.D. degree in electrical and computer engineering from McMaster UniversityXiaohong Liu received the Ph.D. degree in electrical and computer engineering from McMaster Univer- sity, Hamilton, ON, Canada, in 2021, the M.A.Sc. degree in electrical and computer engineering from University of Ottawa, Ottawa, ON, Canada, in 2016, and the B.E. degree in communication engineer- ing from Southwest Jiaotong University, Chengdu, China, in 2014. He is currently a tenure-track Assis- tant Professor with John Hopcroft Center, Shanghai Jiao Tong University, Shanghai, China. His research interests include image/video restoration and image segmentation. He was the recipient of the Ontario Graduate Scholarship in 2019, NSERC Alexander Graham Bell Canada Graduate Scholarship-Doctoral and Borealis AI Global Fellowship award in 2020. He is a reviewer of several IEEE journals, including IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, IEEE TRANSACTIONS ON MULTIMEDIA, IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECH- NOLOGY, and IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS.\n\nHis research areas of interest are security of face biometric systems (e.g., face anti-spoofing, digital manipulation attack, adversarial attack), 3D face modeling. Yaojie Liu is a research scientist at Google Research. He received the Ph.D. degree in Computer Science and Engineering from Michigan State University in 2021. He received the M.S. in Computer Science from the Ohio State University in 2016 and the B.S. Communication Engineering from University of Electronic Science and Technology of Chinaface representation & analysis, XAI, image systhesis, and multi-model modelingYaojie Liu is a research scientist at Google Re- search. He received the Ph.D. degree in Computer Science and Engineering from Michigan State Uni- versity in 2021. He received the M.S. in Computer Science from the Ohio State University in 2016 and the B.S. in Communication Engineering from University of Electronic Science and Technology of China in 2014. His research areas of interest are security of face biometric systems (e.g., face anti-spoofing, digital manipulation attack, adversar- ial attack), 3D face modeling, face representation & analysis, XAI, image systhesis, and multi-model modeling.\n\nwhere he is currently a Professor. His research interests include information theory, machine learning, wireless communications, and signal processing. Jun Chen, Chen was a recipient of the Josef Raviv Memorial Postdoctoral Fellowship in 2006, the Early Researcher Award from the Province of Ontario in 2010, the IBM Faculty Award in 2010, the ICC Best Paper Award in 2020, and the JSPS Invitational Fellowship in 2021. He held the title of the Barber-Gennum Chair of information technology from 2008 to 2013 and the title of the Joseph Ip Distinguished Engineering Fellow from. Shanghai, China; Ithaca, NY, USA; Urbana, IL, USA; Yorktown Heights, NY, USA; Hamilton, ON, CanadaShanghai Jiao Tong University ; he was a Post-Doctoral Research Associate with the Coordinated Science Laboratory, University of Illinois at Urbana-Champaign ; Department of Electrical and Computer Engineering, McMaster University2001, and the M.S. and Ph.D. degrees in electrical and computer engineering from Cornell University. NETWORKING from 2020 to 2021. He is currently an Associate EditorJun Chen (Senior Member, IEEE) received the B.E. degree in communication engineering from Shanghai Jiao Tong University, Shanghai, China, in 2001, and the M.S. and Ph.D. degrees in electrical and com- puter engineering from Cornell University, Ithaca, NY, USA, in 2004 and 2006, respectively. From September 2005 to July 2006, he was a Post-Doctoral Research Associate with the Coordi- nated Science Laboratory, University of Illinois at Urbana-Champaign, Urbana, IL, USA, and a Post- Doctoral Fellow with the IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA, from July 2006 to August 2007. Since September 2007, he has been with the Department of Electrical and Computer Engineering, McMaster University, Hamilton, ON, Canada, where he is currently a Professor. His research interests include information theory, machine learning, wireless communications, and signal processing. Dr. Chen was a recipient of the Josef Raviv Memorial Postdoctoral Fellowship in 2006, the Early Researcher Award from the Province of Ontario in 2010, the IBM Faculty Award in 2010, the ICC Best Paper Award in 2020, and the JSPS Invitational Fellowship in 2021. He held the title of the Barber- Gennum Chair of information technology from 2008 to 2013 and the title of the Joseph Ip Distinguished Engineering Fellow from 2016 to 2018. He served as an Editor for IEEE TRANSACTIONS ON GREEN COMMUNICATIONS AND NETWORKING from 2020 to 2021. He is currently an Associate Editor of IEEE TRANSACTIONS ON INFORMATION THEORY.\n\nBefore joining MSU, in Fall 2012, he was a research scientist at General Electric (GE) Global Research. His research interests include computer vision, machine learning, and biometrics. As a coauthor, he is a recipient of Best Industry Related Paper Award Runner-up at ICPR. Xiaoming Liu, Best Student Paper Award at WACV 2012 and 2014, and Best Poster Award at BMVC 2015. He has been the area chair for numerous conferences, including CVPR, ECCV, ICCV, NeurIPS, and ICLR. He is the program chair of. Pittsburgh, Pennsylvania; East Lansing, MichiganCarnegie Mellon University ; Computer Science and Engineering, Michigan State UniversityAVSS 2022, and general chair of FG 2023. He is an associate editor of the PATTERN RECOGNITION, and the IEEE TRANSACTIONS ON IMAGE PROCESSING. He has authored more than 160 scientific publications, and has filed 29 U.S. patentsXiaoming Liu (Senior Member, IEEE) received the PhD degree in electrical and computer engineering from Carnegie Mellon University, Pittsburgh, Penn- sylvania, in 2004. He is currently a MSU Foundation Professor with the Department of Computer Science and Engineering, Michigan State University, East Lansing, Michigan. Before joining MSU, in Fall 2012, he was a research scientist at General Electric (GE) Global Research. His research interests include computer vision, machine learning, and biometrics. As a coauthor, he is a recipient of Best Industry Related Paper Award Runner-up at ICPR 2014, Best Student Paper Award at WACV 2012 and 2014, and Best Poster Award at BMVC 2015. He has been the area chair for numerous conferences, including CVPR, ECCV, ICCV, NeurIPS, and ICLR. He is the program chair of WACV 2018, BTAS 2018, AVSS 2022, and general chair of FG 2023. He is an associate editor of the PATTERN RECOGNITION, and the IEEE TRANSACTIONS ON IMAGE PROCESSING. He has authored more than 160 scientific publications, and has filed 29 U.S. patents.\n", "annotations": {"author": "[{\"end\":123,\"start\":110},{\"end\":135,\"start\":124},{\"end\":164,\"start\":136},{\"end\":197,\"start\":165}]", "publisher": null, "author_last_name": "[{\"end\":122,\"start\":119},{\"end\":134,\"start\":131},{\"end\":196,\"start\":193}]", "author_first_name": "[{\"end\":118,\"start\":110},{\"end\":130,\"start\":124},{\"end\":158,\"start\":155},{\"end\":163,\"start\":159},{\"end\":192,\"start\":184}]", "author_affiliation": null, "title": "[{\"end\":107,\"start\":1},{\"end\":304,\"start\":198}]", "venue": null, "abstract": "[{\"end\":1671,\"start\":404}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1776,\"start\":1773},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1781,\"start\":1778},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1854,\"start\":1851},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1859,\"start\":1856},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1878,\"start\":1875},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1902,\"start\":1899},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2064,\"start\":2060},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2086,\"start\":2082},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2105,\"start\":2101},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2111,\"start\":2107},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2215,\"start\":2211},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2221,\"start\":2217},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3400,\"start\":3396},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3406,\"start\":3402},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4072,\"start\":4068},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4078,\"start\":4074},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4084,\"start\":4080},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4090,\"start\":4086},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4096,\"start\":4092},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4500,\"start\":4496},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4506,\"start\":4502},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4526,\"start\":4522},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4532,\"start\":4528},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5915,\"start\":5911},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8320,\"start\":8316},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8326,\"start\":8322},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8355,\"start\":8351},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8433,\"start\":8429},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8449,\"start\":8445},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8594,\"start\":8590},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8600,\"start\":8596},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8979,\"start\":8975},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8985,\"start\":8981},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8991,\"start\":8987},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8997,\"start\":8993},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9013,\"start\":9009},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9019,\"start\":9015},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9025,\"start\":9021},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9031,\"start\":9027},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9045,\"start\":9041},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9051,\"start\":9047},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9091,\"start\":9087},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9097,\"start\":9093},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9480,\"start\":9476},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9558,\"start\":9554},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9574,\"start\":9570},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9806,\"start\":9802},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9846,\"start\":9842},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9868,\"start\":9864},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9962,\"start\":9958},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10174,\"start\":10170},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10869,\"start\":10866},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10875,\"start\":10871},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10892,\"start\":10888},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10915,\"start\":10911},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10921,\"start\":10917},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10948,\"start\":10944},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10954,\"start\":10950},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11297,\"start\":11293},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11453,\"start\":11449},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11459,\"start\":11455},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11465,\"start\":11461},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":11471,\"start\":11467},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11583,\"start\":11579},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11615,\"start\":11611},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11634,\"start\":11630},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11640,\"start\":11636},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12789,\"start\":12785},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12795,\"start\":12791},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12826,\"start\":12822},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12832,\"start\":12828},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13207,\"start\":13203},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14506,\"start\":14502},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17981,\"start\":17977},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":18571,\"start\":18567},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":18577,\"start\":18573},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":20519,\"start\":20515},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21829,\"start\":21825},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":21835,\"start\":21831},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":21860,\"start\":21856},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22053,\"start\":22049},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":22197,\"start\":22193},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":22360,\"start\":22356},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":22373,\"start\":22369},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":22391,\"start\":22387},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22445,\"start\":22441},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22539,\"start\":22535},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22581,\"start\":22577},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":22587,\"start\":22583},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22628,\"start\":22625},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":23794,\"start\":23790},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23809,\"start\":23805},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":23821,\"start\":23817},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":23837,\"start\":23833},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":23877,\"start\":23873},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23984,\"start\":23980},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23990,\"start\":23986},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":24040,\"start\":24036},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24091,\"start\":24087},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":24205,\"start\":24201},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":24376,\"start\":24372},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":24509,\"start\":24505},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25263,\"start\":25259},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25269,\"start\":25265},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25938,\"start\":25934},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25952,\"start\":25948},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":26171,\"start\":26167},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26963,\"start\":26959},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26976,\"start\":26972},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26988,\"start\":26984},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27005,\"start\":27001},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27020,\"start\":27016},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27137,\"start\":27133},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29744,\"start\":29740},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29758,\"start\":29754},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31723,\"start\":31719},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32028,\"start\":32024},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32049,\"start\":32045},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":37656,\"start\":37652},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":38194,\"start\":38191},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38197,\"start\":38194},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":38202,\"start\":38199},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38205,\"start\":38202},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":39218,\"start\":39214},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39581,\"start\":39577},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":39595,\"start\":39591},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":40058,\"start\":40054},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":40064,\"start\":40060}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41553,\"start\":40873},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41843,\"start\":41554},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42216,\"start\":41844},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42474,\"start\":42217},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42622,\"start\":42475},{\"attributes\":{\"id\":\"fig_5\"},\"end\":42842,\"start\":42623},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42922,\"start\":42843},{\"attributes\":{\"id\":\"fig_7\"},\"end\":43097,\"start\":42923},{\"attributes\":{\"id\":\"fig_8\"},\"end\":43226,\"start\":43098},{\"attributes\":{\"id\":\"fig_9\"},\"end\":43891,\"start\":43227},{\"attributes\":{\"id\":\"fig_10\"},\"end\":43940,\"start\":43892},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":44338,\"start\":43941},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44995,\"start\":44339},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45260,\"start\":44996},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":46036,\"start\":45261},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46843,\"start\":46037},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47320,\"start\":46844}]", "paragraph": "[{\"end\":3829,\"start\":1690},{\"end\":4658,\"start\":3831},{\"end\":5103,\"start\":4660},{\"end\":5589,\"start\":5105},{\"end\":6638,\"start\":5591},{\"end\":7539,\"start\":6640},{\"end\":7596,\"start\":7541},{\"end\":8066,\"start\":7598},{\"end\":8850,\"start\":8119},{\"end\":9481,\"start\":8889},{\"end\":10667,\"start\":9483},{\"end\":11249,\"start\":10696},{\"end\":12261,\"start\":11276},{\"end\":12687,\"start\":12279},{\"end\":13335,\"start\":12689},{\"end\":14270,\"start\":13337},{\"end\":14719,\"start\":14272},{\"end\":14948,\"start\":14721},{\"end\":15141,\"start\":15029},{\"end\":15610,\"start\":15193},{\"end\":16347,\"start\":15612},{\"end\":17117,\"start\":16388},{\"end\":17797,\"start\":17119},{\"end\":18896,\"start\":17799},{\"end\":19488,\"start\":18941},{\"end\":20001,\"start\":19533},{\"end\":20548,\"start\":20003},{\"end\":20821,\"start\":20597},{\"end\":21492,\"start\":20842},{\"end\":23348,\"start\":21580},{\"end\":23651,\"start\":23350},{\"end\":24775,\"start\":23671},{\"end\":25178,\"start\":24777},{\"end\":25628,\"start\":25180},{\"end\":26320,\"start\":25630},{\"end\":26883,\"start\":26322},{\"end\":28778,\"start\":26918},{\"end\":29436,\"start\":28780},{\"end\":30315,\"start\":29438},{\"end\":32258,\"start\":30347},{\"end\":33776,\"start\":32287},{\"end\":34295,\"start\":33778},{\"end\":34918,\"start\":34368},{\"end\":35556,\"start\":34920},{\"end\":36809,\"start\":35599},{\"end\":37532,\"start\":36811},{\"end\":38924,\"start\":37559},{\"end\":39219,\"start\":38943},{\"end\":40125,\"start\":39221},{\"end\":40872,\"start\":40143}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15028,\"start\":14949},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15192,\"start\":15142},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18940,\"start\":18897},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19532,\"start\":19489},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20596,\"start\":20549},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21550,\"start\":21493}]", "table_ref": null, "section_header": "[{\"end\":1688,\"start\":1673},{\"end\":8117,\"start\":8069},{\"end\":8887,\"start\":8853},{\"end\":10694,\"start\":10670},{\"end\":11274,\"start\":11252},{\"end\":12277,\"start\":12264},{\"end\":16386,\"start\":16350},{\"end\":20840,\"start\":20824},{\"end\":21578,\"start\":21552},{\"end\":23669,\"start\":23654},{\"end\":26916,\"start\":26886},{\"end\":30345,\"start\":30318},{\"end\":32285,\"start\":32261},{\"end\":34366,\"start\":34298},{\"end\":35597,\"start\":35559},{\"end\":37557,\"start\":37535},{\"end\":38941,\"start\":38927},{\"end\":40141,\"start\":40128},{\"end\":41563,\"start\":41555},{\"end\":41853,\"start\":41845},{\"end\":42226,\"start\":42218},{\"end\":42484,\"start\":42476},{\"end\":42632,\"start\":42624},{\"end\":42852,\"start\":42844},{\"end\":42932,\"start\":42924},{\"end\":43107,\"start\":43099},{\"end\":43237,\"start\":43228},{\"end\":43902,\"start\":43893},{\"end\":43957,\"start\":43942},{\"end\":44361,\"start\":44340},{\"end\":45015,\"start\":44997},{\"end\":45278,\"start\":45262},{\"end\":46057,\"start\":46038},{\"end\":46865,\"start\":46845}]", "table": "[{\"end\":44338,\"start\":44117},{\"end\":44995,\"start\":44560},{\"end\":45260,\"start\":45059},{\"end\":46036,\"start\":45380},{\"end\":46843,\"start\":46159},{\"end\":47320,\"start\":47003}]", "figure_caption": "[{\"end\":41553,\"start\":40875},{\"end\":41843,\"start\":41565},{\"end\":42216,\"start\":41855},{\"end\":42474,\"start\":42228},{\"end\":42622,\"start\":42486},{\"end\":42842,\"start\":42634},{\"end\":42922,\"start\":42854},{\"end\":43097,\"start\":42934},{\"end\":43226,\"start\":43109},{\"end\":43891,\"start\":43240},{\"end\":43940,\"start\":43905},{\"end\":44117,\"start\":43959},{\"end\":44560,\"start\":44364},{\"end\":45059,\"start\":45018},{\"end\":45380,\"start\":45280},{\"end\":46159,\"start\":46060},{\"end\":47003,\"start\":46869}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":2817,\"start\":2811},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":5710,\"start\":5704},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17173,\"start\":17167},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23068,\"start\":23062},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29602,\"start\":29596},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29613,\"start\":29607},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29894,\"start\":29888},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29947,\"start\":29941},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30064,\"start\":30058},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30104,\"start\":30098},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30259,\"start\":30253},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31174,\"start\":31168},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":31681,\"start\":31675},{\"end\":32691,\"start\":32681},{\"end\":33500,\"start\":33490},{\"end\":34050,\"start\":34037},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35035,\"start\":35028},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35993,\"start\":35987},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39231,\"start\":39224}]", "bib_author_first_name": "[{\"end\":47363,\"start\":47362},{\"end\":47369,\"start\":47368},{\"end\":47375,\"start\":47374},{\"end\":47390,\"start\":47389},{\"end\":47392,\"start\":47391},{\"end\":47735,\"start\":47731},{\"end\":47742,\"start\":47741},{\"end\":47749,\"start\":47748},{\"end\":47755,\"start\":47754},{\"end\":48092,\"start\":48091},{\"end\":48101,\"start\":48100},{\"end\":48112,\"start\":48111},{\"end\":48121,\"start\":48120},{\"end\":48130,\"start\":48129},{\"end\":48132,\"start\":48131},{\"end\":48141,\"start\":48140},{\"end\":48152,\"start\":48151},{\"end\":48551,\"start\":48550},{\"end\":48558,\"start\":48557},{\"end\":48565,\"start\":48564},{\"end\":48574,\"start\":48573},{\"end\":48582,\"start\":48581},{\"end\":48590,\"start\":48589},{\"end\":48598,\"start\":48597},{\"end\":48936,\"start\":48935},{\"end\":48942,\"start\":48941},{\"end\":48948,\"start\":48947},{\"end\":48957,\"start\":48956},{\"end\":48963,\"start\":48962},{\"end\":49277,\"start\":49276},{\"end\":49283,\"start\":49282},{\"end\":49291,\"start\":49290},{\"end\":49300,\"start\":49299},{\"end\":49306,\"start\":49305},{\"end\":49628,\"start\":49627},{\"end\":49638,\"start\":49637},{\"end\":49640,\"start\":49639},{\"end\":49648,\"start\":49647},{\"end\":49659,\"start\":49658},{\"end\":49668,\"start\":49667},{\"end\":49670,\"start\":49669},{\"end\":50053,\"start\":50052},{\"end\":50061,\"start\":50060},{\"end\":50068,\"start\":50067},{\"end\":50076,\"start\":50075},{\"end\":50085,\"start\":50084},{\"end\":50098,\"start\":50097},{\"end\":50444,\"start\":50443},{\"end\":50452,\"start\":50451},{\"end\":50458,\"start\":50457},{\"end\":50466,\"start\":50465},{\"end\":50826,\"start\":50825},{\"end\":50833,\"start\":50832},{\"end\":50840,\"start\":50839},{\"end\":50849,\"start\":50848},{\"end\":50851,\"start\":50850},{\"end\":51210,\"start\":51209},{\"end\":51216,\"start\":51215},{\"end\":51231,\"start\":51230},{\"end\":51645,\"start\":51644},{\"end\":51657,\"start\":51656},{\"end\":51675,\"start\":51674},{\"end\":51686,\"start\":51685},{\"end\":51697,\"start\":51696},{\"end\":51959,\"start\":51958},{\"end\":51967,\"start\":51966},{\"end\":51974,\"start\":51973},{\"end\":51987,\"start\":51986},{\"end\":51994,\"start\":51993},{\"end\":52382,\"start\":52381},{\"end\":52390,\"start\":52389},{\"end\":52398,\"start\":52397},{\"end\":52404,\"start\":52403},{\"end\":52410,\"start\":52409},{\"end\":52417,\"start\":52416},{\"end\":52787,\"start\":52786},{\"end\":52793,\"start\":52792},{\"end\":52801,\"start\":52800},{\"end\":52809,\"start\":52808},{\"end\":53095,\"start\":53094},{\"end\":53106,\"start\":53105},{\"end\":53115,\"start\":53114},{\"end\":53338,\"start\":53337},{\"end\":53475,\"start\":53474},{\"end\":53481,\"start\":53480},{\"end\":53490,\"start\":53489},{\"end\":53499,\"start\":53498},{\"end\":53512,\"start\":53511},{\"end\":53520,\"start\":53519},{\"end\":53846,\"start\":53845},{\"end\":53859,\"start\":53858},{\"end\":54053,\"start\":54052},{\"end\":54061,\"start\":54060},{\"end\":54070,\"start\":54069},{\"end\":54077,\"start\":54076},{\"end\":54390,\"start\":54389},{\"end\":54392,\"start\":54391},{\"end\":54401,\"start\":54400},{\"end\":54403,\"start\":54402},{\"end\":54420,\"start\":54419},{\"end\":54428,\"start\":54427},{\"end\":54439,\"start\":54438},{\"end\":54826,\"start\":54825},{\"end\":54837,\"start\":54836},{\"end\":54847,\"start\":54843},{\"end\":54849,\"start\":54848},{\"end\":55159,\"start\":55158},{\"end\":55167,\"start\":55166},{\"end\":55174,\"start\":55173},{\"end\":55176,\"start\":55175},{\"end\":55187,\"start\":55186},{\"end\":55189,\"start\":55188},{\"end\":55555,\"start\":55554},{\"end\":55557,\"start\":55556},{\"end\":55566,\"start\":55565},{\"end\":55576,\"start\":55575},{\"end\":55587,\"start\":55586},{\"end\":55600,\"start\":55599},{\"end\":55602,\"start\":55601},{\"end\":55880,\"start\":55879},{\"end\":55890,\"start\":55889},{\"end\":55897,\"start\":55896},{\"end\":55908,\"start\":55907},{\"end\":55915,\"start\":55914},{\"end\":56336,\"start\":56335},{\"end\":56346,\"start\":56345},{\"end\":56353,\"start\":56352},{\"end\":56364,\"start\":56363},{\"end\":56643,\"start\":56642},{\"end\":56651,\"start\":56650},{\"end\":56658,\"start\":56657},{\"end\":56667,\"start\":56666},{\"end\":56676,\"start\":56675},{\"end\":56684,\"start\":56683},{\"end\":56692,\"start\":56691},{\"end\":56699,\"start\":56698},{\"end\":56705,\"start\":56704},{\"end\":56712,\"start\":56711},{\"end\":57051,\"start\":57050},{\"end\":57057,\"start\":57056},{\"end\":57073,\"start\":57072},{\"end\":57441,\"start\":57440},{\"end\":57450,\"start\":57449},{\"end\":57458,\"start\":57457},{\"end\":57470,\"start\":57469},{\"end\":57888,\"start\":57887},{\"end\":57895,\"start\":57894},{\"end\":57902,\"start\":57901},{\"end\":58114,\"start\":58113},{\"end\":58127,\"start\":58126},{\"end\":58136,\"start\":58135},{\"end\":58484,\"start\":58483},{\"end\":58495,\"start\":58494},{\"end\":58507,\"start\":58506},{\"end\":58517,\"start\":58516},{\"end\":58881,\"start\":58880},{\"end\":58890,\"start\":58889},{\"end\":58900,\"start\":58899},{\"end\":58909,\"start\":58908},{\"end\":58922,\"start\":58921},{\"end\":58924,\"start\":58923},{\"end\":58932,\"start\":58931},{\"end\":59340,\"start\":59339},{\"end\":59346,\"start\":59345},{\"end\":59362,\"start\":59361},{\"end\":59698,\"start\":59697},{\"end\":59700,\"start\":59699},{\"end\":59709,\"start\":59708},{\"end\":59718,\"start\":59717},{\"end\":60078,\"start\":60077},{\"end\":60087,\"start\":60086},{\"end\":60094,\"start\":60093},{\"end\":60100,\"start\":60099},{\"end\":60109,\"start\":60108},{\"end\":60118,\"start\":60117},{\"end\":60382,\"start\":60381},{\"end\":60395,\"start\":60394},{\"end\":60404,\"start\":60403},{\"end\":60648,\"start\":60647},{\"end\":60655,\"start\":60654},{\"end\":60662,\"start\":60661},{\"end\":60680,\"start\":60676},{\"end\":60686,\"start\":60685},{\"end\":60694,\"start\":60693},{\"end\":61039,\"start\":61038},{\"end\":61045,\"start\":61044},{\"end\":61061,\"start\":61060},{\"end\":61434,\"start\":61433},{\"end\":61451,\"start\":61450},{\"end\":61464,\"start\":61463},{\"end\":61473,\"start\":61472},{\"end\":61825,\"start\":61824},{\"end\":61832,\"start\":61831},{\"end\":61840,\"start\":61839},{\"end\":61848,\"start\":61847},{\"end\":61855,\"start\":61854},{\"end\":62154,\"start\":62153},{\"end\":62166,\"start\":62165},{\"end\":62181,\"start\":62180},{\"end\":62572,\"start\":62571},{\"end\":62580,\"start\":62579},{\"end\":62586,\"start\":62585},{\"end\":62595,\"start\":62594},{\"end\":62602,\"start\":62601},{\"end\":62992,\"start\":62991},{\"end\":62998,\"start\":62997},{\"end\":63258,\"start\":63257},{\"end\":63260,\"start\":63259},{\"end\":63270,\"start\":63269},{\"end\":63571,\"start\":63570},{\"end\":63578,\"start\":63577},{\"end\":63591,\"start\":63590},{\"end\":63864,\"start\":63863},{\"end\":63876,\"start\":63875},{\"end\":64122,\"start\":64121},{\"end\":64129,\"start\":64128},{\"end\":64135,\"start\":64134},{\"end\":64147,\"start\":64146},{\"end\":64479,\"start\":64478},{\"end\":64486,\"start\":64485},{\"end\":64492,\"start\":64491},{\"end\":64501,\"start\":64500},{\"end\":64508,\"start\":64507},{\"end\":64515,\"start\":64514},{\"end\":64525,\"start\":64521},{\"end\":64923,\"start\":64922},{\"end\":64929,\"start\":64928},{\"end\":64937,\"start\":64936},{\"end\":64946,\"start\":64945},{\"end\":64955,\"start\":64954},{\"end\":65325,\"start\":65324},{\"end\":65334,\"start\":65333},{\"end\":65342,\"start\":65341},{\"end\":65348,\"start\":65347},{\"end\":65356,\"start\":65355},{\"end\":65365,\"start\":65364},{\"end\":65372,\"start\":65371},{\"end\":65378,\"start\":65377},{\"end\":65774,\"start\":65773},{\"end\":65782,\"start\":65781},{\"end\":65789,\"start\":65788},{\"end\":65796,\"start\":65795},{\"end\":65804,\"start\":65803},{\"end\":66178,\"start\":66177},{\"end\":66187,\"start\":66186},{\"end\":66195,\"start\":66194},{\"end\":66201,\"start\":66200},{\"end\":66207,\"start\":66206},{\"end\":66577,\"start\":66576},{\"end\":66584,\"start\":66583},{\"end\":66590,\"start\":66589},{\"end\":66597,\"start\":66596},{\"end\":66605,\"start\":66604},{\"end\":66913,\"start\":66912},{\"end\":66921,\"start\":66920},{\"end\":66932,\"start\":66928},{\"end\":66936,\"start\":66933},{\"end\":66945,\"start\":66944},{\"end\":67246,\"start\":67245},{\"end\":67256,\"start\":67255},{\"end\":67541,\"start\":67540},{\"end\":67552,\"start\":67551},{\"end\":67563,\"start\":67562},{\"end\":67573,\"start\":67572},{\"end\":67586,\"start\":67585},{\"end\":67595,\"start\":67594},{\"end\":67597,\"start\":67596},{\"end\":67606,\"start\":67605},{\"end\":67616,\"start\":67615},{\"end\":67948,\"start\":67947},{\"end\":67954,\"start\":67953},{\"end\":67962,\"start\":67961},{\"end\":68229,\"start\":68228},{\"end\":68237,\"start\":68236},{\"end\":68249,\"start\":68248},{\"end\":68258,\"start\":68257},{\"end\":68579,\"start\":68578},{\"end\":68586,\"start\":68585},{\"end\":68592,\"start\":68591},{\"end\":68599,\"start\":68598},{\"end\":68913,\"start\":68912},{\"end\":68922,\"start\":68921},{\"end\":68928,\"start\":68927},{\"end\":68935,\"start\":68934},{\"end\":68943,\"start\":68942},{\"end\":68955,\"start\":68954},{\"end\":68964,\"start\":68960},{\"end\":68970,\"start\":68969},{\"end\":68978,\"start\":68977},{\"end\":69366,\"start\":69365},{\"end\":69374,\"start\":69373},{\"end\":69381,\"start\":69380},{\"end\":69693,\"start\":69692},{\"end\":69701,\"start\":69700},{\"end\":69712,\"start\":69707},{\"end\":69719,\"start\":69718},{\"end\":69721,\"start\":69720},{\"end\":69968,\"start\":69967},{\"end\":69975,\"start\":69974},{\"end\":69987,\"start\":69982},{\"end\":69994,\"start\":69993},{\"end\":70260,\"start\":70259},{\"end\":70266,\"start\":70265},{\"end\":70273,\"start\":70272},{\"end\":70281,\"start\":70280},{\"end\":70287,\"start\":70286},{\"end\":70294,\"start\":70293},{\"end\":70302,\"start\":70301},{\"end\":70632,\"start\":70631},{\"end\":70638,\"start\":70637},{\"end\":70647,\"start\":70646},{\"end\":70654,\"start\":70653},{\"end\":71024,\"start\":71023},{\"end\":71031,\"start\":71030},{\"end\":71038,\"start\":71037},{\"end\":71046,\"start\":71045},{\"end\":71322,\"start\":71318},{\"end\":71329,\"start\":71328},{\"end\":71338,\"start\":71337},{\"end\":71350,\"start\":71349},{\"end\":71358,\"start\":71357},{\"end\":71368,\"start\":71367},{\"end\":71379,\"start\":71378},{\"end\":71389,\"start\":71388},{\"end\":71391,\"start\":71390},{\"end\":71707,\"start\":71706},{\"end\":71709,\"start\":71708},{\"end\":71866,\"start\":71865},{\"end\":71870,\"start\":71867},{\"end\":72058,\"start\":72057},{\"end\":72070,\"start\":72069},{\"end\":72081,\"start\":72080},{\"end\":72092,\"start\":72091},{\"end\":72101,\"start\":72100},{\"end\":72395,\"start\":72394},{\"end\":72403,\"start\":72402},{\"end\":72646,\"start\":72642},{\"end\":72652,\"start\":72651},{\"end\":72662,\"start\":72658},{\"end\":72912,\"start\":72911},{\"end\":72920,\"start\":72919},{\"end\":72928,\"start\":72927},{\"end\":73384,\"start\":73383},{\"end\":73398,\"start\":73397},{\"end\":73409,\"start\":73408},{\"end\":73773,\"start\":73772},{\"end\":73775,\"start\":73774},{\"end\":73785,\"start\":73784},{\"end\":76984,\"start\":76981},{\"end\":79699,\"start\":79691}]", "bib_author_last_name": "[{\"end\":47366,\"start\":47364},{\"end\":47372,\"start\":47370},{\"end\":47387,\"start\":47376},{\"end\":47397,\"start\":47393},{\"end\":47739,\"start\":47736},{\"end\":47746,\"start\":47743},{\"end\":47752,\"start\":47750},{\"end\":47759,\"start\":47756},{\"end\":48098,\"start\":48093},{\"end\":48109,\"start\":48102},{\"end\":48118,\"start\":48113},{\"end\":48127,\"start\":48122},{\"end\":48138,\"start\":48133},{\"end\":48149,\"start\":48142},{\"end\":48162,\"start\":48153},{\"end\":48555,\"start\":48552},{\"end\":48562,\"start\":48559},{\"end\":48571,\"start\":48566},{\"end\":48579,\"start\":48575},{\"end\":48587,\"start\":48583},{\"end\":48595,\"start\":48591},{\"end\":48601,\"start\":48599},{\"end\":48939,\"start\":48937},{\"end\":48945,\"start\":48943},{\"end\":48954,\"start\":48949},{\"end\":48960,\"start\":48958},{\"end\":48967,\"start\":48964},{\"end\":49280,\"start\":49278},{\"end\":49288,\"start\":49284},{\"end\":49297,\"start\":49292},{\"end\":49303,\"start\":49301},{\"end\":49310,\"start\":49307},{\"end\":49635,\"start\":49629},{\"end\":49645,\"start\":49641},{\"end\":49656,\"start\":49649},{\"end\":49665,\"start\":49660},{\"end\":49677,\"start\":49671},{\"end\":50058,\"start\":50054},{\"end\":50065,\"start\":50062},{\"end\":50073,\"start\":50069},{\"end\":50082,\"start\":50077},{\"end\":50095,\"start\":50086},{\"end\":50101,\"start\":50099},{\"end\":50449,\"start\":50445},{\"end\":50455,\"start\":50453},{\"end\":50463,\"start\":50459},{\"end\":50471,\"start\":50467},{\"end\":50830,\"start\":50827},{\"end\":50837,\"start\":50834},{\"end\":50846,\"start\":50841},{\"end\":50857,\"start\":50852},{\"end\":51213,\"start\":51211},{\"end\":51228,\"start\":51217},{\"end\":51241,\"start\":51232},{\"end\":51654,\"start\":51646},{\"end\":51672,\"start\":51658},{\"end\":51683,\"start\":51676},{\"end\":51694,\"start\":51687},{\"end\":51711,\"start\":51698},{\"end\":51964,\"start\":51960},{\"end\":51971,\"start\":51968},{\"end\":51984,\"start\":51975},{\"end\":51991,\"start\":51988},{\"end\":51999,\"start\":51995},{\"end\":52387,\"start\":52383},{\"end\":52395,\"start\":52391},{\"end\":52401,\"start\":52399},{\"end\":52407,\"start\":52405},{\"end\":52414,\"start\":52411},{\"end\":52420,\"start\":52418},{\"end\":52790,\"start\":52788},{\"end\":52798,\"start\":52794},{\"end\":52806,\"start\":52802},{\"end\":52813,\"start\":52810},{\"end\":53103,\"start\":53096},{\"end\":53112,\"start\":53107},{\"end\":53123,\"start\":53116},{\"end\":53349,\"start\":53339},{\"end\":53478,\"start\":53476},{\"end\":53487,\"start\":53482},{\"end\":53496,\"start\":53491},{\"end\":53509,\"start\":53500},{\"end\":53517,\"start\":53513},{\"end\":53528,\"start\":53521},{\"end\":53856,\"start\":53847},{\"end\":53869,\"start\":53860},{\"end\":54058,\"start\":54054},{\"end\":54067,\"start\":54062},{\"end\":54074,\"start\":54071},{\"end\":54080,\"start\":54078},{\"end\":54398,\"start\":54393},{\"end\":54417,\"start\":54404},{\"end\":54425,\"start\":54421},{\"end\":54436,\"start\":54429},{\"end\":54449,\"start\":54440},{\"end\":54834,\"start\":54827},{\"end\":54841,\"start\":54838},{\"end\":54853,\"start\":54850},{\"end\":55164,\"start\":55160},{\"end\":55171,\"start\":55168},{\"end\":55184,\"start\":55177},{\"end\":55195,\"start\":55190},{\"end\":55563,\"start\":55558},{\"end\":55573,\"start\":55567},{\"end\":55584,\"start\":55577},{\"end\":55597,\"start\":55588},{\"end\":55616,\"start\":55603},{\"end\":55887,\"start\":55881},{\"end\":55894,\"start\":55891},{\"end\":55905,\"start\":55898},{\"end\":55912,\"start\":55909},{\"end\":55919,\"start\":55916},{\"end\":56343,\"start\":56337},{\"end\":56350,\"start\":56347},{\"end\":56361,\"start\":56354},{\"end\":56368,\"start\":56365},{\"end\":56648,\"start\":56644},{\"end\":56655,\"start\":56652},{\"end\":56664,\"start\":56659},{\"end\":56673,\"start\":56668},{\"end\":56681,\"start\":56677},{\"end\":56689,\"start\":56685},{\"end\":56696,\"start\":56693},{\"end\":56702,\"start\":56700},{\"end\":56709,\"start\":56706},{\"end\":56717,\"start\":56713},{\"end\":57054,\"start\":57052},{\"end\":57070,\"start\":57058},{\"end\":57083,\"start\":57074},{\"end\":57447,\"start\":57442},{\"end\":57455,\"start\":57451},{\"end\":57467,\"start\":57459},{\"end\":57476,\"start\":57471},{\"end\":57892,\"start\":57889},{\"end\":57899,\"start\":57896},{\"end\":57908,\"start\":57903},{\"end\":58124,\"start\":58115},{\"end\":58133,\"start\":58128},{\"end\":58146,\"start\":58137},{\"end\":58492,\"start\":58485},{\"end\":58504,\"start\":58496},{\"end\":58514,\"start\":58508},{\"end\":58526,\"start\":58518},{\"end\":58887,\"start\":58882},{\"end\":58897,\"start\":58891},{\"end\":58906,\"start\":58901},{\"end\":58919,\"start\":58910},{\"end\":58929,\"start\":58925},{\"end\":58939,\"start\":58933},{\"end\":59343,\"start\":59341},{\"end\":59359,\"start\":59347},{\"end\":59372,\"start\":59363},{\"end\":59706,\"start\":59701},{\"end\":59715,\"start\":59710},{\"end\":59728,\"start\":59719},{\"end\":60084,\"start\":60079},{\"end\":60091,\"start\":60088},{\"end\":60097,\"start\":60095},{\"end\":60106,\"start\":60101},{\"end\":60115,\"start\":60110},{\"end\":60123,\"start\":60119},{\"end\":60392,\"start\":60383},{\"end\":60401,\"start\":60396},{\"end\":60414,\"start\":60405},{\"end\":60652,\"start\":60649},{\"end\":60659,\"start\":60656},{\"end\":60674,\"start\":60663},{\"end\":60683,\"start\":60681},{\"end\":60691,\"start\":60687},{\"end\":60702,\"start\":60695},{\"end\":61042,\"start\":61040},{\"end\":61058,\"start\":61046},{\"end\":61071,\"start\":61062},{\"end\":61448,\"start\":61435},{\"end\":61461,\"start\":61452},{\"end\":61470,\"start\":61465},{\"end\":61483,\"start\":61474},{\"end\":61829,\"start\":61826},{\"end\":61837,\"start\":61833},{\"end\":61845,\"start\":61841},{\"end\":61852,\"start\":61849},{\"end\":61859,\"start\":61856},{\"end\":62163,\"start\":62155},{\"end\":62178,\"start\":62167},{\"end\":62191,\"start\":62182},{\"end\":62577,\"start\":62573},{\"end\":62583,\"start\":62581},{\"end\":62592,\"start\":62587},{\"end\":62599,\"start\":62596},{\"end\":62607,\"start\":62603},{\"end\":62995,\"start\":62993},{\"end\":63003,\"start\":62999},{\"end\":63267,\"start\":63261},{\"end\":63278,\"start\":63271},{\"end\":63575,\"start\":63572},{\"end\":63588,\"start\":63579},{\"end\":63595,\"start\":63592},{\"end\":63873,\"start\":63865},{\"end\":63885,\"start\":63877},{\"end\":64126,\"start\":64123},{\"end\":64132,\"start\":64130},{\"end\":64144,\"start\":64136},{\"end\":64151,\"start\":64148},{\"end\":64483,\"start\":64480},{\"end\":64489,\"start\":64487},{\"end\":64498,\"start\":64493},{\"end\":64505,\"start\":64502},{\"end\":64512,\"start\":64509},{\"end\":64519,\"start\":64516},{\"end\":64530,\"start\":64526},{\"end\":64926,\"start\":64924},{\"end\":64934,\"start\":64930},{\"end\":64943,\"start\":64938},{\"end\":64952,\"start\":64947},{\"end\":64958,\"start\":64956},{\"end\":65331,\"start\":65326},{\"end\":65339,\"start\":65335},{\"end\":65345,\"start\":65343},{\"end\":65353,\"start\":65349},{\"end\":65362,\"start\":65357},{\"end\":65369,\"start\":65366},{\"end\":65375,\"start\":65373},{\"end\":65384,\"start\":65379},{\"end\":65779,\"start\":65775},{\"end\":65786,\"start\":65783},{\"end\":65793,\"start\":65790},{\"end\":65801,\"start\":65797},{\"end\":65809,\"start\":65805},{\"end\":66184,\"start\":66179},{\"end\":66192,\"start\":66188},{\"end\":66198,\"start\":66196},{\"end\":66204,\"start\":66202},{\"end\":66212,\"start\":66208},{\"end\":66581,\"start\":66578},{\"end\":66587,\"start\":66585},{\"end\":66594,\"start\":66591},{\"end\":66602,\"start\":66598},{\"end\":66610,\"start\":66606},{\"end\":66918,\"start\":66914},{\"end\":66926,\"start\":66922},{\"end\":66942,\"start\":66937},{\"end\":66949,\"start\":66946},{\"end\":67253,\"start\":67247},{\"end\":67260,\"start\":67257},{\"end\":67549,\"start\":67542},{\"end\":67560,\"start\":67553},{\"end\":67570,\"start\":67564},{\"end\":67583,\"start\":67574},{\"end\":67592,\"start\":67587},{\"end\":67603,\"start\":67598},{\"end\":67613,\"start\":67607},{\"end\":67627,\"start\":67617},{\"end\":67951,\"start\":67949},{\"end\":67959,\"start\":67955},{\"end\":67966,\"start\":67963},{\"end\":68234,\"start\":68230},{\"end\":68246,\"start\":68238},{\"end\":68255,\"start\":68250},{\"end\":68261,\"start\":68259},{\"end\":68583,\"start\":68580},{\"end\":68589,\"start\":68587},{\"end\":68596,\"start\":68593},{\"end\":68604,\"start\":68600},{\"end\":68919,\"start\":68914},{\"end\":68925,\"start\":68923},{\"end\":68932,\"start\":68929},{\"end\":68940,\"start\":68936},{\"end\":68952,\"start\":68944},{\"end\":68958,\"start\":68956},{\"end\":68967,\"start\":68965},{\"end\":68975,\"start\":68971},{\"end\":68983,\"start\":68979},{\"end\":69371,\"start\":69367},{\"end\":69378,\"start\":69375},{\"end\":69386,\"start\":69382},{\"end\":69698,\"start\":69694},{\"end\":69705,\"start\":69702},{\"end\":69716,\"start\":69713},{\"end\":69727,\"start\":69722},{\"end\":69972,\"start\":69969},{\"end\":69980,\"start\":69976},{\"end\":69991,\"start\":69988},{\"end\":70003,\"start\":69995},{\"end\":70263,\"start\":70261},{\"end\":70270,\"start\":70267},{\"end\":70278,\"start\":70274},{\"end\":70284,\"start\":70282},{\"end\":70291,\"start\":70288},{\"end\":70299,\"start\":70295},{\"end\":70305,\"start\":70303},{\"end\":70635,\"start\":70633},{\"end\":70644,\"start\":70639},{\"end\":70651,\"start\":70648},{\"end\":70658,\"start\":70655},{\"end\":71028,\"start\":71025},{\"end\":71035,\"start\":71032},{\"end\":71043,\"start\":71039},{\"end\":71050,\"start\":71047},{\"end\":71326,\"start\":71323},{\"end\":71335,\"start\":71330},{\"end\":71347,\"start\":71339},{\"end\":71355,\"start\":71351},{\"end\":71365,\"start\":71359},{\"end\":71376,\"start\":71369},{\"end\":71386,\"start\":71380},{\"end\":71399,\"start\":71392},{\"end\":71719,\"start\":71710},{\"end\":71878,\"start\":71871},{\"end\":72067,\"start\":72059},{\"end\":72078,\"start\":72071},{\"end\":72089,\"start\":72082},{\"end\":72098,\"start\":72093},{\"end\":72106,\"start\":72102},{\"end\":72400,\"start\":72396},{\"end\":72409,\"start\":72404},{\"end\":72649,\"start\":72647},{\"end\":72656,\"start\":72653},{\"end\":72668,\"start\":72663},{\"end\":72917,\"start\":72913},{\"end\":72925,\"start\":72921},{\"end\":72932,\"start\":72929},{\"end\":73395,\"start\":73385},{\"end\":73406,\"start\":73399},{\"end\":73414,\"start\":73410},{\"end\":73782,\"start\":73776},{\"end\":73788,\"start\":73786},{\"end\":76989,\"start\":76985},{\"end\":79703,\"start\":79700}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":47661,\"start\":47322},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":198967908},\"end\":48041,\"start\":47663},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":215415902},\"end\":48471,\"start\":48043},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":220961475},\"end\":48864,\"start\":48473},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":208003108},\"end\":49224,\"start\":48866},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":219633936},\"end\":49581,\"start\":49226},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":219965283},\"end\":49960,\"start\":49583},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":218870309},\"end\":50376,\"start\":49962},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":198897678},\"end\":50750,\"start\":50378},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13684145},\"end\":51091,\"start\":50752},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":195503148},\"end\":51570,\"start\":51093},{\"attributes\":{\"id\":\"b11\"},\"end\":51909,\"start\":51572},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":203737259},\"end\":52274,\"start\":51911},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":250182908},\"end\":52670,\"start\":52276},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":234869865},\"end\":53058,\"start\":52672},{\"attributes\":{\"id\":\"b15\"},\"end\":53314,\"start\":53060},{\"attributes\":{\"id\":\"b16\"},\"end\":53395,\"start\":53316},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":221447878},\"end\":53793,\"start\":53397},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":52097748},\"end\":54028,\"start\":53795},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":212725220},\"end\":54316,\"start\":54030},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7901560},\"end\":54740,\"start\":54318},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3507740},\"end\":55099,\"start\":54742},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":44149078},\"end\":55473,\"start\":55101},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":70350052},\"end\":55837,\"start\":55475},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":247793156},\"end\":56236,\"start\":55839},{\"attributes\":{\"id\":\"b25\"},\"end\":56571,\"start\":56238},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":201124533},\"end\":56970,\"start\":56573},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52955351},\"end\":57321,\"start\":56972},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":211551516},\"end\":57815,\"start\":57323},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7785764},\"end\":58060,\"start\":57817},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14377597},\"end\":58389,\"start\":58062},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":19328533},\"end\":58793,\"start\":58391},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":32096248},\"end\":59218,\"start\":58795},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":22534034},\"end\":59599,\"start\":59220},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":202779027},\"end\":60021,\"start\":59601},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":240144300},\"end\":60329,\"start\":60023},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":15435392},\"end\":60586,\"start\":60331},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15201962},\"end\":60963,\"start\":60588},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":13683472},\"end\":61343,\"start\":60965},{\"attributes\":{\"id\":\"b39\"},\"end\":61754,\"start\":61345},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":51974320},\"end\":62060,\"start\":61756},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":218932192},\"end\":62453,\"start\":62062},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":234437036},\"end\":62899,\"start\":62455},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":235502024},\"end\":63204,\"start\":62901},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":212605869},\"end\":63506,\"start\":63206},{\"attributes\":{\"id\":\"b45\"},\"end\":63813,\"start\":63508},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":9751504},\"end\":64040,\"start\":63815},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":10328909},\"end\":64428,\"start\":64042},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":4563057},\"end\":64815,\"start\":64430},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":204959680},\"end\":65255,\"start\":64817},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":214623217},\"end\":65701,\"start\":65257},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":4564405},\"end\":66098,\"start\":65703},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":52083170},\"end\":66507,\"start\":66100},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":226291803},\"end\":66844,\"start\":66509},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":226239804},\"end\":67186,\"start\":66846},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":54218995},\"end\":67511,\"start\":67188},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":13756489},\"end\":67912,\"start\":67513},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":140309863},\"end\":68199,\"start\":67914},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":4852647},\"end\":68505,\"start\":68201},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":199528450},\"end\":68856,\"start\":68507},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":219965026},\"end\":69299,\"start\":68858},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":219687431},\"end\":69656,\"start\":69301},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":49864419},\"end\":69921,\"start\":69658},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":49867180},\"end\":70210,\"start\":69923},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":52180375},\"end\":70583,\"start\":70212},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":206594692},\"end\":70913,\"start\":70585},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":86653999},\"end\":71273,\"start\":70915},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":14113767},\"end\":71656,\"start\":71275},{\"attributes\":{\"id\":\"b68\"},\"end\":71834,\"start\":71658},{\"attributes\":{\"id\":\"b69\"},\"end\":71994,\"start\":71836},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":34389305},\"end\":72321,\"start\":71996},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":3094482},\"end\":72586,\"start\":72323},{\"attributes\":{\"id\":\"b72\"},\"end\":72856,\"start\":72588},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":14008455},\"end\":73187,\"start\":72858},{\"attributes\":{\"id\":\"b74\"},\"end\":73297,\"start\":73189},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":214603516},\"end\":73726,\"start\":73299},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":6628106},\"end\":73963,\"start\":73728},{\"attributes\":{\"id\":\"b77\"},\"end\":75639,\"start\":73965},{\"attributes\":{\"id\":\"b78\"},\"end\":76827,\"start\":75641},{\"attributes\":{\"id\":\"b79\"},\"end\":79414,\"start\":76829},{\"attributes\":{\"id\":\"b80\"},\"end\":81338,\"start\":79416}]", "bib_title": "[{\"end\":47360,\"start\":47322},{\"end\":47729,\"start\":47663},{\"end\":48089,\"start\":48043},{\"end\":48548,\"start\":48473},{\"end\":48933,\"start\":48866},{\"end\":49274,\"start\":49226},{\"end\":49625,\"start\":49583},{\"end\":50050,\"start\":49962},{\"end\":50441,\"start\":50378},{\"end\":50823,\"start\":50752},{\"end\":51207,\"start\":51093},{\"end\":51956,\"start\":51911},{\"end\":52379,\"start\":52276},{\"end\":52784,\"start\":52672},{\"end\":53472,\"start\":53397},{\"end\":53843,\"start\":53795},{\"end\":54050,\"start\":54030},{\"end\":54387,\"start\":54318},{\"end\":54823,\"start\":54742},{\"end\":55156,\"start\":55101},{\"end\":55552,\"start\":55475},{\"end\":55877,\"start\":55839},{\"end\":56640,\"start\":56573},{\"end\":57048,\"start\":56972},{\"end\":57438,\"start\":57323},{\"end\":57885,\"start\":57817},{\"end\":58111,\"start\":58062},{\"end\":58481,\"start\":58391},{\"end\":58878,\"start\":58795},{\"end\":59337,\"start\":59220},{\"end\":59695,\"start\":59601},{\"end\":60075,\"start\":60023},{\"end\":60379,\"start\":60331},{\"end\":60645,\"start\":60588},{\"end\":61036,\"start\":60965},{\"end\":61431,\"start\":61345},{\"end\":61822,\"start\":61756},{\"end\":62151,\"start\":62062},{\"end\":62569,\"start\":62455},{\"end\":62989,\"start\":62901},{\"end\":63255,\"start\":63206},{\"end\":63568,\"start\":63508},{\"end\":63861,\"start\":63815},{\"end\":64119,\"start\":64042},{\"end\":64476,\"start\":64430},{\"end\":64920,\"start\":64817},{\"end\":65322,\"start\":65257},{\"end\":65771,\"start\":65703},{\"end\":66175,\"start\":66100},{\"end\":66574,\"start\":66509},{\"end\":66910,\"start\":66846},{\"end\":67243,\"start\":67188},{\"end\":67538,\"start\":67513},{\"end\":67945,\"start\":67914},{\"end\":68226,\"start\":68201},{\"end\":68576,\"start\":68507},{\"end\":68910,\"start\":68858},{\"end\":69363,\"start\":69301},{\"end\":69690,\"start\":69658},{\"end\":69965,\"start\":69923},{\"end\":70257,\"start\":70212},{\"end\":70629,\"start\":70585},{\"end\":71021,\"start\":70915},{\"end\":71316,\"start\":71275},{\"end\":72055,\"start\":71996},{\"end\":72392,\"start\":72323},{\"end\":72640,\"start\":72588},{\"end\":72909,\"start\":72858},{\"end\":73381,\"start\":73299},{\"end\":73770,\"start\":73728},{\"end\":74265,\"start\":73965},{\"end\":75804,\"start\":75641},{\"end\":76979,\"start\":76829},{\"end\":79689,\"start\":79416}]", "bib_author": "[{\"end\":47368,\"start\":47362},{\"end\":47374,\"start\":47368},{\"end\":47389,\"start\":47374},{\"end\":47399,\"start\":47389},{\"end\":47741,\"start\":47731},{\"end\":47748,\"start\":47741},{\"end\":47754,\"start\":47748},{\"end\":47761,\"start\":47754},{\"end\":48100,\"start\":48091},{\"end\":48111,\"start\":48100},{\"end\":48120,\"start\":48111},{\"end\":48129,\"start\":48120},{\"end\":48140,\"start\":48129},{\"end\":48151,\"start\":48140},{\"end\":48164,\"start\":48151},{\"end\":48557,\"start\":48550},{\"end\":48564,\"start\":48557},{\"end\":48573,\"start\":48564},{\"end\":48581,\"start\":48573},{\"end\":48589,\"start\":48581},{\"end\":48597,\"start\":48589},{\"end\":48603,\"start\":48597},{\"end\":48941,\"start\":48935},{\"end\":48947,\"start\":48941},{\"end\":48956,\"start\":48947},{\"end\":48962,\"start\":48956},{\"end\":48969,\"start\":48962},{\"end\":49282,\"start\":49276},{\"end\":49290,\"start\":49282},{\"end\":49299,\"start\":49290},{\"end\":49305,\"start\":49299},{\"end\":49312,\"start\":49305},{\"end\":49637,\"start\":49627},{\"end\":49647,\"start\":49637},{\"end\":49658,\"start\":49647},{\"end\":49667,\"start\":49658},{\"end\":49679,\"start\":49667},{\"end\":50060,\"start\":50052},{\"end\":50067,\"start\":50060},{\"end\":50075,\"start\":50067},{\"end\":50084,\"start\":50075},{\"end\":50097,\"start\":50084},{\"end\":50103,\"start\":50097},{\"end\":50451,\"start\":50443},{\"end\":50457,\"start\":50451},{\"end\":50465,\"start\":50457},{\"end\":50473,\"start\":50465},{\"end\":50832,\"start\":50825},{\"end\":50839,\"start\":50832},{\"end\":50848,\"start\":50839},{\"end\":50859,\"start\":50848},{\"end\":51215,\"start\":51209},{\"end\":51230,\"start\":51215},{\"end\":51243,\"start\":51230},{\"end\":51656,\"start\":51644},{\"end\":51674,\"start\":51656},{\"end\":51685,\"start\":51674},{\"end\":51696,\"start\":51685},{\"end\":51713,\"start\":51696},{\"end\":51966,\"start\":51958},{\"end\":51973,\"start\":51966},{\"end\":51986,\"start\":51973},{\"end\":51993,\"start\":51986},{\"end\":52001,\"start\":51993},{\"end\":52389,\"start\":52381},{\"end\":52397,\"start\":52389},{\"end\":52403,\"start\":52397},{\"end\":52409,\"start\":52403},{\"end\":52416,\"start\":52409},{\"end\":52422,\"start\":52416},{\"end\":52792,\"start\":52786},{\"end\":52800,\"start\":52792},{\"end\":52808,\"start\":52800},{\"end\":52815,\"start\":52808},{\"end\":53105,\"start\":53094},{\"end\":53114,\"start\":53105},{\"end\":53125,\"start\":53114},{\"end\":53351,\"start\":53337},{\"end\":53480,\"start\":53474},{\"end\":53489,\"start\":53480},{\"end\":53498,\"start\":53489},{\"end\":53511,\"start\":53498},{\"end\":53519,\"start\":53511},{\"end\":53530,\"start\":53519},{\"end\":53858,\"start\":53845},{\"end\":53871,\"start\":53858},{\"end\":54060,\"start\":54052},{\"end\":54069,\"start\":54060},{\"end\":54076,\"start\":54069},{\"end\":54082,\"start\":54076},{\"end\":54400,\"start\":54389},{\"end\":54419,\"start\":54400},{\"end\":54427,\"start\":54419},{\"end\":54438,\"start\":54427},{\"end\":54451,\"start\":54438},{\"end\":54836,\"start\":54825},{\"end\":54843,\"start\":54836},{\"end\":54855,\"start\":54843},{\"end\":55166,\"start\":55158},{\"end\":55173,\"start\":55166},{\"end\":55186,\"start\":55173},{\"end\":55197,\"start\":55186},{\"end\":55565,\"start\":55554},{\"end\":55575,\"start\":55565},{\"end\":55586,\"start\":55575},{\"end\":55599,\"start\":55586},{\"end\":55618,\"start\":55599},{\"end\":55889,\"start\":55879},{\"end\":55896,\"start\":55889},{\"end\":55907,\"start\":55896},{\"end\":55914,\"start\":55907},{\"end\":55921,\"start\":55914},{\"end\":56345,\"start\":56335},{\"end\":56352,\"start\":56345},{\"end\":56363,\"start\":56352},{\"end\":56370,\"start\":56363},{\"end\":56650,\"start\":56642},{\"end\":56657,\"start\":56650},{\"end\":56666,\"start\":56657},{\"end\":56675,\"start\":56666},{\"end\":56683,\"start\":56675},{\"end\":56691,\"start\":56683},{\"end\":56698,\"start\":56691},{\"end\":56704,\"start\":56698},{\"end\":56711,\"start\":56704},{\"end\":56719,\"start\":56711},{\"end\":57056,\"start\":57050},{\"end\":57072,\"start\":57056},{\"end\":57085,\"start\":57072},{\"end\":57449,\"start\":57440},{\"end\":57457,\"start\":57449},{\"end\":57469,\"start\":57457},{\"end\":57478,\"start\":57469},{\"end\":57894,\"start\":57887},{\"end\":57901,\"start\":57894},{\"end\":57910,\"start\":57901},{\"end\":58126,\"start\":58113},{\"end\":58135,\"start\":58126},{\"end\":58148,\"start\":58135},{\"end\":58494,\"start\":58483},{\"end\":58506,\"start\":58494},{\"end\":58516,\"start\":58506},{\"end\":58528,\"start\":58516},{\"end\":58889,\"start\":58880},{\"end\":58899,\"start\":58889},{\"end\":58908,\"start\":58899},{\"end\":58921,\"start\":58908},{\"end\":58931,\"start\":58921},{\"end\":58941,\"start\":58931},{\"end\":59345,\"start\":59339},{\"end\":59361,\"start\":59345},{\"end\":59374,\"start\":59361},{\"end\":59708,\"start\":59697},{\"end\":59717,\"start\":59708},{\"end\":59730,\"start\":59717},{\"end\":60086,\"start\":60077},{\"end\":60093,\"start\":60086},{\"end\":60099,\"start\":60093},{\"end\":60108,\"start\":60099},{\"end\":60117,\"start\":60108},{\"end\":60125,\"start\":60117},{\"end\":60394,\"start\":60381},{\"end\":60403,\"start\":60394},{\"end\":60416,\"start\":60403},{\"end\":60654,\"start\":60647},{\"end\":60661,\"start\":60654},{\"end\":60676,\"start\":60661},{\"end\":60685,\"start\":60676},{\"end\":60693,\"start\":60685},{\"end\":60704,\"start\":60693},{\"end\":61044,\"start\":61038},{\"end\":61060,\"start\":61044},{\"end\":61073,\"start\":61060},{\"end\":61450,\"start\":61433},{\"end\":61463,\"start\":61450},{\"end\":61472,\"start\":61463},{\"end\":61485,\"start\":61472},{\"end\":61831,\"start\":61824},{\"end\":61839,\"start\":61831},{\"end\":61847,\"start\":61839},{\"end\":61854,\"start\":61847},{\"end\":61861,\"start\":61854},{\"end\":62165,\"start\":62153},{\"end\":62180,\"start\":62165},{\"end\":62193,\"start\":62180},{\"end\":62579,\"start\":62571},{\"end\":62585,\"start\":62579},{\"end\":62594,\"start\":62585},{\"end\":62601,\"start\":62594},{\"end\":62609,\"start\":62601},{\"end\":62997,\"start\":62991},{\"end\":63005,\"start\":62997},{\"end\":63269,\"start\":63257},{\"end\":63280,\"start\":63269},{\"end\":63577,\"start\":63570},{\"end\":63590,\"start\":63577},{\"end\":63597,\"start\":63590},{\"end\":63875,\"start\":63863},{\"end\":63887,\"start\":63875},{\"end\":64128,\"start\":64121},{\"end\":64134,\"start\":64128},{\"end\":64146,\"start\":64134},{\"end\":64153,\"start\":64146},{\"end\":64485,\"start\":64478},{\"end\":64491,\"start\":64485},{\"end\":64500,\"start\":64491},{\"end\":64507,\"start\":64500},{\"end\":64514,\"start\":64507},{\"end\":64521,\"start\":64514},{\"end\":64532,\"start\":64521},{\"end\":64928,\"start\":64922},{\"end\":64936,\"start\":64928},{\"end\":64945,\"start\":64936},{\"end\":64954,\"start\":64945},{\"end\":64960,\"start\":64954},{\"end\":65333,\"start\":65324},{\"end\":65341,\"start\":65333},{\"end\":65347,\"start\":65341},{\"end\":65355,\"start\":65347},{\"end\":65364,\"start\":65355},{\"end\":65371,\"start\":65364},{\"end\":65377,\"start\":65371},{\"end\":65386,\"start\":65377},{\"end\":65781,\"start\":65773},{\"end\":65788,\"start\":65781},{\"end\":65795,\"start\":65788},{\"end\":65803,\"start\":65795},{\"end\":65811,\"start\":65803},{\"end\":66186,\"start\":66177},{\"end\":66194,\"start\":66186},{\"end\":66200,\"start\":66194},{\"end\":66206,\"start\":66200},{\"end\":66214,\"start\":66206},{\"end\":66583,\"start\":66576},{\"end\":66589,\"start\":66583},{\"end\":66596,\"start\":66589},{\"end\":66604,\"start\":66596},{\"end\":66612,\"start\":66604},{\"end\":66920,\"start\":66912},{\"end\":66928,\"start\":66920},{\"end\":66944,\"start\":66928},{\"end\":66951,\"start\":66944},{\"end\":67255,\"start\":67245},{\"end\":67262,\"start\":67255},{\"end\":67551,\"start\":67540},{\"end\":67562,\"start\":67551},{\"end\":67572,\"start\":67562},{\"end\":67585,\"start\":67572},{\"end\":67594,\"start\":67585},{\"end\":67605,\"start\":67594},{\"end\":67615,\"start\":67605},{\"end\":67629,\"start\":67615},{\"end\":67953,\"start\":67947},{\"end\":67961,\"start\":67953},{\"end\":67968,\"start\":67961},{\"end\":68236,\"start\":68228},{\"end\":68248,\"start\":68236},{\"end\":68257,\"start\":68248},{\"end\":68263,\"start\":68257},{\"end\":68585,\"start\":68578},{\"end\":68591,\"start\":68585},{\"end\":68598,\"start\":68591},{\"end\":68606,\"start\":68598},{\"end\":68921,\"start\":68912},{\"end\":68927,\"start\":68921},{\"end\":68934,\"start\":68927},{\"end\":68942,\"start\":68934},{\"end\":68954,\"start\":68942},{\"end\":68960,\"start\":68954},{\"end\":68969,\"start\":68960},{\"end\":68977,\"start\":68969},{\"end\":68985,\"start\":68977},{\"end\":69373,\"start\":69365},{\"end\":69380,\"start\":69373},{\"end\":69388,\"start\":69380},{\"end\":69700,\"start\":69692},{\"end\":69707,\"start\":69700},{\"end\":69718,\"start\":69707},{\"end\":69729,\"start\":69718},{\"end\":69974,\"start\":69967},{\"end\":69982,\"start\":69974},{\"end\":69993,\"start\":69982},{\"end\":70005,\"start\":69993},{\"end\":70265,\"start\":70259},{\"end\":70272,\"start\":70265},{\"end\":70280,\"start\":70272},{\"end\":70286,\"start\":70280},{\"end\":70293,\"start\":70286},{\"end\":70301,\"start\":70293},{\"end\":70307,\"start\":70301},{\"end\":70637,\"start\":70631},{\"end\":70646,\"start\":70637},{\"end\":70653,\"start\":70646},{\"end\":70660,\"start\":70653},{\"end\":71030,\"start\":71023},{\"end\":71037,\"start\":71030},{\"end\":71045,\"start\":71037},{\"end\":71052,\"start\":71045},{\"end\":71328,\"start\":71318},{\"end\":71337,\"start\":71328},{\"end\":71349,\"start\":71337},{\"end\":71357,\"start\":71349},{\"end\":71367,\"start\":71357},{\"end\":71378,\"start\":71367},{\"end\":71388,\"start\":71378},{\"end\":71401,\"start\":71388},{\"end\":71721,\"start\":71706},{\"end\":71880,\"start\":71865},{\"end\":72069,\"start\":72057},{\"end\":72080,\"start\":72069},{\"end\":72091,\"start\":72080},{\"end\":72100,\"start\":72091},{\"end\":72108,\"start\":72100},{\"end\":72402,\"start\":72394},{\"end\":72411,\"start\":72402},{\"end\":72651,\"start\":72642},{\"end\":72658,\"start\":72651},{\"end\":72670,\"start\":72658},{\"end\":72919,\"start\":72911},{\"end\":72927,\"start\":72919},{\"end\":72934,\"start\":72927},{\"end\":73397,\"start\":73383},{\"end\":73408,\"start\":73397},{\"end\":73416,\"start\":73408},{\"end\":73784,\"start\":73772},{\"end\":73790,\"start\":73784},{\"end\":76991,\"start\":76981},{\"end\":79705,\"start\":79691}]", "bib_venue": "[{\"end\":47503,\"start\":47455},{\"end\":47865,\"start\":47817},{\"end\":48268,\"start\":48220},{\"end\":48671,\"start\":48641},{\"end\":49055,\"start\":49016},{\"end\":49416,\"start\":49368},{\"end\":49783,\"start\":49735},{\"end\":50171,\"start\":50141},{\"end\":50577,\"start\":50529},{\"end\":50927,\"start\":50897},{\"end\":51347,\"start\":51299},{\"end\":52105,\"start\":52057},{\"end\":53598,\"start\":53568},{\"end\":54186,\"start\":54138},{\"end\":54537,\"start\":54498},{\"end\":55301,\"start\":55253},{\"end\":56050,\"start\":55981},{\"end\":57153,\"start\":57123},{\"end\":57582,\"start\":57534},{\"end\":59822,\"start\":59782},{\"end\":60782,\"start\":60749},{\"end\":61167,\"start\":61124},{\"end\":63665,\"start\":63635},{\"end\":64245,\"start\":64205},{\"end\":64636,\"start\":64588},{\"end\":65046,\"start\":65007},{\"end\":65490,\"start\":65442},{\"end\":65915,\"start\":65867},{\"end\":66318,\"start\":66270},{\"end\":66680,\"start\":66650},{\"end\":67019,\"start\":66989},{\"end\":67366,\"start\":67318},{\"end\":67721,\"start\":67681},{\"end\":68072,\"start\":68024},{\"end\":68367,\"start\":68319},{\"end\":68692,\"start\":68653},{\"end\":69089,\"start\":69041},{\"end\":69492,\"start\":69444},{\"end\":69795,\"start\":69766},{\"end\":70073,\"start\":70043},{\"end\":70411,\"start\":70363},{\"end\":70764,\"start\":70716},{\"end\":71469,\"start\":71439},{\"end\":73528,\"start\":73476},{\"end\":73852,\"start\":73825},{\"end\":74446,\"start\":74393},{\"end\":77506,\"start\":77408},{\"end\":79965,\"start\":79917},{\"end\":47453,\"start\":47399},{\"end\":47815,\"start\":47761},{\"end\":48218,\"start\":48164},{\"end\":48639,\"start\":48603},{\"end\":49014,\"start\":48969},{\"end\":49366,\"start\":49312},{\"end\":49733,\"start\":49679},{\"end\":50139,\"start\":50103},{\"end\":50527,\"start\":50473},{\"end\":50895,\"start\":50859},{\"end\":51297,\"start\":51243},{\"end\":51642,\"start\":51572},{\"end\":52055,\"start\":52001},{\"end\":52461,\"start\":52422},{\"end\":52854,\"start\":52815},{\"end\":53092,\"start\":53060},{\"end\":53335,\"start\":53316},{\"end\":53566,\"start\":53530},{\"end\":53903,\"start\":53871},{\"end\":54136,\"start\":54082},{\"end\":54496,\"start\":54451},{\"end\":54911,\"start\":54855},{\"end\":55251,\"start\":55197},{\"end\":55643,\"start\":55618},{\"end\":55979,\"start\":55921},{\"end\":56333,\"start\":56238},{\"end\":56757,\"start\":56719},{\"end\":57121,\"start\":57085},{\"end\":57532,\"start\":57478},{\"end\":57929,\"start\":57910},{\"end\":58215,\"start\":58148},{\"end\":58573,\"start\":58528},{\"end\":58986,\"start\":58941},{\"end\":59399,\"start\":59374},{\"end\":59780,\"start\":59730},{\"end\":60164,\"start\":60125},{\"end\":60448,\"start\":60416},{\"end\":60747,\"start\":60704},{\"end\":61122,\"start\":61073},{\"end\":61524,\"start\":61485},{\"end\":61880,\"start\":61861},{\"end\":62232,\"start\":62193},{\"end\":62648,\"start\":62609},{\"end\":63044,\"start\":63005},{\"end\":63340,\"start\":63280},{\"end\":63633,\"start\":63597},{\"end\":63919,\"start\":63887},{\"end\":64203,\"start\":64153},{\"end\":64586,\"start\":64532},{\"end\":65005,\"start\":64960},{\"end\":65440,\"start\":65386},{\"end\":65865,\"start\":65811},{\"end\":66268,\"start\":66214},{\"end\":66648,\"start\":66612},{\"end\":66987,\"start\":66951},{\"end\":67316,\"start\":67262},{\"end\":67679,\"start\":67629},{\"end\":68022,\"start\":67968},{\"end\":68317,\"start\":68263},{\"end\":68651,\"start\":68606},{\"end\":69039,\"start\":68985},{\"end\":69442,\"start\":69388},{\"end\":69764,\"start\":69729},{\"end\":70041,\"start\":70005},{\"end\":70361,\"start\":70307},{\"end\":70714,\"start\":70660},{\"end\":71084,\"start\":71052},{\"end\":71437,\"start\":71401},{\"end\":71704,\"start\":71658},{\"end\":71863,\"start\":71836},{\"end\":72147,\"start\":72108},{\"end\":72445,\"start\":72411},{\"end\":72678,\"start\":72670},{\"end\":73012,\"start\":72934},{\"end\":73220,\"start\":73189},{\"end\":73474,\"start\":73416},{\"end\":73823,\"start\":73790},{\"end\":74391,\"start\":74267},{\"end\":76057,\"start\":75806},{\"end\":77406,\"start\":76991},{\"end\":79915,\"start\":79705}]"}}}, "year": 2023, "month": 12, "day": 17}