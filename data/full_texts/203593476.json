{"id": 203593476, "updated": "2023-10-06 22:26:25.18", "metadata": {"title": "Multi-scale Attributed Node Embedding", "authors": "[{\"first\":\"Benedek\",\"last\":\"Rozemberczki\",\"middle\":[]},{\"first\":\"Carl\",\"last\":\"Allen\",\"middle\":[]},{\"first\":\"Rik\",\"last\":\"Sarkar\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 9, "day": 28}, "abstract": "We present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighborhoods of different sizes are either pooled (AE) or encoded distinctly in a multi-scale approach (MUSAE). Capturing attribute-neighborhood relationships over multiple scales is useful for a diverse range of applications, including latent feature identification across disconnected networks with similar attributes. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are robust, computationally efficient and outperform comparable models on social, web and citation network datasets.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1909.13021", "mag": "2975966238", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/compnet/RozemberczkiAS21", "doi": "10.1093/comnet/cnab014"}}, "content": {"source": {"pdf_hash": "5d50adf52da1df9d72ebc273499ef74218f9f63c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1909.13021v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://academic.oup.com/comnet/article-pdf/9/2/cnab014/40435146/cnab014.pdf", "status": "HYBRID"}}, "grobid": {"id": "f8eb22b204401974b9aee2e8c94f6065a4a67885", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5d50adf52da1df9d72ebc273499ef74218f9f63c.txt", "contents": "\nMULTI-SCALE ATTRIBUTED NODE EMBEDDING\n\n\nBenedek Rozemberczki \nSchool of Informatics\nUniversity of Edinburgh\n\n\nCarl Allen carl.allen@ed.ac.uk \nSchool of Informatics\nUniversity of Edinburgh\n\n\nRik Sarkar rsarkar@inf.ed.ac.uk \nSchool of Informatics\nUniversity of Edinburgh\n\n\nMULTI-SCALE ATTRIBUTED NODE EMBEDDING\n\nWe present network embedding algorithms that capture information about a node from the local distribution over node attributes around it, as observed over random walks following an approach similar to Skip-gram. Observations from neighborhoods of different sizes are either pooled (AE) or encoded distinctly in a multiscale approach (MUSAE). Capturing attribute-neighborhood relationships over multiple scales is useful for a diverse range of applications, including latent feature identification across disconnected networks with similar attributes. We prove theoretically that matrices of node-feature pointwise mutual information are implicitly factorized by the embeddings. Experiments show that our algorithms are robust, computationally efficient and outperform comparable models on social, web and citation network datasets.\n Figure 1a \nattributed nodes D and G have the same feature set and their nearest neighbours also exhibit equivalent sets of features, whereas features at higher order neighbourhoods differ. Figure 1b shows that as the order of neighbourhoods considered (r) increases, the product of the adjacency matrix power and the feature matrix becomes less sparse. This suggests that an implicit decomposition method would be computationally beneficial.\n\nOur key contributions are:\n\n1. to introduce the first Skip-gram style embedding algorithms that consider attribute distributions over local neighborhoods, both pooled (AE) and multi-scale (MUSAE), and their counterparts that attribute distinct features to each node (AE-EGO and MUSAE-EGO);\n\n2. to theoretically prove that their embeddings approximately factorize PMI matrices based on the product of an adjacency matrix power and node-feature matrix;\n\n3. to show that popular network embedding methods DeepWalk (Perozzi et al., 2014) and Walklets (Perozzi et al., 2017) are special cases of our AE and MUSAE;\n\n4. we show empirically that AE and MUSAE embeddings enable strong performance at regression, classification, and link prediction tasks for real-world networks (e.g. Wikipedia and Facebook), are computationally scalable and enable transfer learning between networks.\n\nWe provide reference implementations of AE and MUSAE, together with the datasets used for evaluation at https://github.com/benedekrozemberczki/MUSAE.\n\n\nRELATED WORK\n\nEfficient unsupervised learning of node embeddings for large networks has seen unprecedented development in recent years. The current paradigm focuses on learning latent space representations of nodes such that those that share neighbors (Perozzi et al., 2014;Tang et al., 2015;Grover & Leskovec, 2016;Perozzi et al., 2017), structural roles (Ribeiro et al., 2017;Ahmed et al., 2018) or attributes are located close together in the embedding space. Our work falls under the last of these categories as our goal is to learn similar latent representations for nodes with similar sets of features in their neighborhoods, both on a pooled and multi-scale basis.\n\nNeighborhood preserving node embedding procedures place nodes with common first, second and higher order neighbors within close proximity in the embedding space. Recent works in the neighborhood preserving node embedding literature were inspired by the Skip-gram model (Mikolov et al., 2013a;b), which generates word embeddings by implicitly factorizing a shifted pointwise mutual information (PMI) matrix (Levy & Goldberg, 2014) obtained from a text corpus. This procedure inspired DeepWalk (Perozzi et al., 2014), a method which generates truncated random walks over a graph to obtain a \"corpus\" from which the Skip-gram model generates neighborhood preserving node embeddings. In doing so, DeepWalk implicitly factorizes a PMI matrix, which can be shown, based on the underlying first-order Markov process, to correspond to the mean of a set of normalized adjacency matrix powers up to a given order (Qiu et al., 2018). Such pooling of matrices can be suboptimal since neighbors over increasing path lengths (or scales) are treated equally or according to fixed weightings (Mikolov et al., 2013a;Grover & Leskovec, 2016); whereas it has been found that an optimal weighting may be task or dataset specific (Abu-El-Haija et al., 2018). In contrast, multi-scale node embedding methods such as LINE (Tang et al., 2015), GraRep (Cao et al., 2015) and Walklets (Perozzi et al., 2017) separately learn lower-dimensional node embedding components from each adjacency matrix power and concatenate them to form the full node representation. Such un-pooled representations, comprising distinct but less information at each scale, are found to give higher performance in a number of downstream settings, without increasing the overall number of free parameters (Perozzi et al., 2017).\n\nAttributed node embedding procedures refine ideas from neighborhood based node embeddings to also incorporate node attributes (equivalently, features or labels) (Yang et al., 2015;Liao et al., 2018;Huang et al., 2017;Yang et al., 2018;Yang & Yang, 2018). Similarities between both a node's neighborhood structure and features contribute to determining pairwise proximity in the node embedding space. These models follow quite different strategies to obtain such representations. The most elemental procedure, TADW (Yang et al., 2015), decomposes a convex combination of normalized adjacency matrix powers into a matrix product that includes the feature matrix. Several other models, such as SINE (Zhang et al., 2018) and ASNE (Liao et al., 2018), implicitly factorize a matrix formed by concatenating the feature and adjacency matrices. Other approaches such as TENE (Yang & Yang, 2018), formulate the attributed node embedding task as a joint non-negative matrix factorization problem in which node representations obtained from sub-tasks are used to regularize one another. AANE (Huang et al., 2017) uses a similar network structure based regularization approach, in which a node feature similarity matrix is decomposed using the alternating direction method of multipliers. The method most similar to our own is BANE (Yang et al., 2018), in which the product of a normalized adjacency matrix power and a feature matrix is explicitly factorized to obtain attributed node embeddings. Many other methods exist, but do not consider the attributes of higher order neighborhoods (Yang et al., 2015;Liao et al., 2018;Huang et al., 2017;Zhang et al., 2018;Yang & Yang, 2018).\n\nThe relationship between our pooled (AE) and multi-scale (MUSAE) attributed node embedding methods mirrors that between graph convolutional neural networks (GCNNs) and multi-scale GC-NNs. Widely used graph convolutional layers, such as GCN \n\n\nATTRIBUTED EMBEDDING MODELS\n\nWe now define algorithms to learn node embeddings using the attributes of nearby nodes, that allows both node and attribute embeddings to be learned jointly. The aim is to learn similar embeddings for nodes that occur in neighbourhoods of similar attributes; and similar embeddings for attributes that often occur in similar neighbourhoods of nodes. Let G = (V, L) be an undirected graph of interest where V and L are the sets of vertices and edges (or links) respectively; and let F be the set of all possible node features (i.e. attributes). We define F v \u2286 F as the subset of features belonging to each node v \u2208 V. An embedding of nodes is a mapping g : V \u2192 R d that assigns a d-dimensional representation g(v) (or simply g v ) to each node v and is fully described by a matrix G \u2208 R |V|\u00d7d . Similarly, an embedding of the features (to the same latent space) is a mapping h : F \u2192 R d with embeddings denoted h(f ) (or simply h f ), and is fully described by a matrix H \u2208 R |F|\u00d7d .\n\n\nATTRIBUTED EMBEDDING\n\nThe Attributed Embedding (AE) procedure is described by Algorithm 1. We sample n nodes w 1 , from which to start attributed random walks on G, with probability proportional to their degree (Line 2). From each starting node, a node sequence of length l is sampled over G (Line 3), where sampling follows a first order random walk. For a given window size t, we iterate over each of the first l \u2212 t nodes of the sequence termed source nodes w j (Line 4). For each source node, we consider the following t nodes as target nodes (Line 5). For each target node w j+r , we add the tuple (w j , f ) to the corpus D for each target feature f \u2208 F wj+r (Lines 6 and 7). We also consider features of the \n1 for i in 1 : n do 2 Pick w1 \u2208 V according to P (w1) \u223c deg(w1)/vol(G). 3 (w1, w2, . . . , wl) \u2190 Sample Nodes(G, w1, l) 4 for j in 1 : l \u2212 t do 5 for r in 1 : t do 6 for f in Fw j+r do 7\nAdd tuple (wj, f ) to multiset D. Algorithm 1: AE sampling and training procedure Data: G = (V, L) -Graph to be embedded.\n\n{Fv}V -Set of node feature sets. n -Number of sequence samples. l -Length of sequences. t -Context size. d -Embedding dimension. b -Number of negative samples. Result: Node embeddings g r and feature embeddings h r for r = 1, . . . , t. 1 for i in 1 : n do 2 Pick w1 \u2208 V according to P (w1) \u223c deg(w1)/vol(G). Run SGNS on Dr with b negative samples and d t dimensions.\n\n\n18\n\nOutput g r v , \u2200v \u2208 V, and h r f , \u2200f \u2208 F = \u222aVFv. 19 end Algorithm 2: MUSAE sampling and training procedure source node f \u2208 F wj , adding each (w j+r , f ) tuple to D (Lines 9 and 10). Running Skip-gram on D with b negative samples (Line 15) generates the d-dimensional node and feature embeddings.\n\n\nMULTI-SCALE ATTRIBUTED EMBEDDING\n\nThe AE method (Algorithm 1) pools feature sets of neighborhoods at different proximities. Inspired by the performance of (unattributed) multi-scale node embeddings, we adapt the AE algorithm to give multi-scale attributed node embeddings (MUSAE). The embedding component of a node v \u2208 V for a specific proximity r \u2208 {1, ..., t} is given by a mapping g r : V \u2192 R d/t (assuming t divides d). Similarly, the embedding component of feature f \u2208 F at proximity r is given by a mapping h r : F \u2192 R d/t . Concatenating gives a d-dimensional embedding for each node and feature.\n\nThe Multi-Scale Attributed Embedding procedure is described by Algorithm 2. We again sample n starting nodes w 1 with a probability proportional to node degree (Line 2) and, for each, sample a node sequence of length l over G (Line 3) according to either a first or second order random walk. For a given window size t, we iterate over the first l \u2212 t (source) nodes w j of the sequence (Line 4) and for each source node we iterate through the t (target) nodes w j+r that follow (Line 5). We again consider each target node feature f \u2208 F wj+r , but now add tuples (w j , f ) to a sub-corpus D r \u2192 (Lines 6 and 7). We add tuples (w j+r , f ) to another sub-corpus D r \u2190 for each source node feature f \u2208 F wj (Lines 9 and 10). Running Skip-gram on each sub-corpus D r = D r \u2192 \u222a D r \u2190 with b negative samples (Line 16) output t ( d t )-dimensional node and feature embeddings that are concatenated.\n\n\nATTRIBUTED EMBEDDING AS IMPLICIT MATRIX FACTORIZATION\n\nLevy & Goldberg (2014) showed that the loss function of Skip-gram with negative sampling (SGNS) is minimized if the embedding matrices factorize a matrix of pointwise mutual information (PMI) of word co-occurrence statistics. Specifically, for a word dictionary V with |V| = n, SGNS (with b negative samples) outputs two embedding matrices W , C \u2208 R d\u00d7n such that \u2200w, c \u2208 V:\nw w c c \u2248 log #(w,c)|D| #(w)#(c) \u2212 log b , where #(w, c), #(w)\n, #(c) denote counts of word-context pair (w, c), w and c over a corpus D; and word embeddings w w , c c \u2208 R d are columns of W and C corresponding to w and c respectively.\nConsidering #(w) |D| , #(c) |D| , #(w,c)\n|D| as empirical estimates of p(w), p(c) and p(w, c) respectively shows:\nW C \u2248 [ PMI(w, c) \u2212 log b ] w,c\u2208V ,\ni.e. an approximate low-rank factorization of a shifted PMI matrix (low rank since typically d n).\n\nQiu et al. (2018) extended this result to node embedding models that apply SGNS to a \"corpus\" generated from random walks over the graph. In the case of DeepWalk where random walks are first-order Markov, the joint probability distributions over nodes at different stages of a random walk can be expressed in closed form. A closed form then follows for the factorized PMI matrix. We show that AE and MUSAE implicitly perform analogous matrix factorizations.\n\nNotation: A \u2208 R n\u00d7n denotes the adjacency matrix and D \u2208 R n\u00d7n the diagonal degree matrix of a graph G, i.e. D w,w = deg(w) = v A w,v . We denote the volume of G by c = v,w A v,w . We define the binary attribute matrix F \u2208 {0, 1} |V|\u00d7|F| by F w,f = 1 f \u2208Fw , \u2200w \u2208 V, f \u2208 F. For ease of notation, we let P = D \u22121 A and E = diag(1 DF ), where diag indicates a diagonal matrix.\n\n\nInterpretation:\n\nAssuming G is ergodic:\np(w) = deg(w) c\n, w \u2208 V is the stationary distribution over nodes, i.e. c \u22121 D = diag(p(w)); and c \u22121 A is the stationary joint distribution over consecutive nodes p(w j , w j+1 ). F w,f can be considered a Bernoulli parameter describing the probability p(f |w) of observing a feature f at a node w and so c \u22121 DF describes the stationary joint distribution p(f, w j ) over nodes and features. Accordingly, P is the matrix of conditional distributions p(w j+1 |w j ); and E is a diagonal matrix proportional to the probability of observing each feature at the stationary distribution p(f ) (note that p(f ) need not sum to 1, whereas p(w) necessarily must).\n\n\nMULTI-SCALE CASE (MUSAE)\n\nWe know that the SGNS aspect of MUSAE (Algorithm 2, Line 17) is minimized when the learned\nembeddings g r v , h r f satisfy g r w h r f \u2248 log #(w,f )r|Dr| #(w)r#(f )r \u2212 log b \u2200w \u2208 V, f \u2208 F.\nOur aim is to express this factorization in terms of known properties of the graph G and its features.\n\nLemma 1. The empirical statistics of node-feature pairs obtained from random walks give unbiased estimates of joint probabilities of observing feature f \u2208 F r steps (i) after; or (ii) before node v \u2208 V, as given by:\nplim l\u2192\u221e #(w,f )\u2192 r |D\u2192 r | = c \u22121 (DP r F ) w,f plim l\u2192\u221e #(w,f )\u2190 r |D\u2190 r | = c \u22121 (F DP r ) f,w\nProof. See Appendix.\n\nLemma 2. Empirical statistics of node-feature pairs obtained from random walks give unbiased estimates of joint probabilities of observing feature f \u2208 F r steps either side of node v \u2208 V, given by:\nplim l\u2192\u221e #(w,f )r |Dr| = c \u22121 (DP r F ) w,f , Proof. See Appendix.\nMarginalizing gives unbiased estimates of stationary probability distributions of nodes and features:\nplim l\u2192\u221e #(w) |Dr| = deg(w) c = c \u22121 D w,w and plim l\u2192\u221e #(f ) |Dr| = w|f \u2208Fw deg(w) c = c \u22121 E f,f\nTheorem 1. MUSAE embeddings approximately factorize the node-feature PMI matrix:\nlog c P r F E \u22121 \u2212 log b, for r = 1, ... , t.\nProof.\n#(w,f )r|Dr| #(f )r#(w)r = #(w,f )r |Dr| / #(f )r |Dr| #(w)r |Dr| p \u2212 \u2192 (cD \u22121 )(c \u22121 DP r F )(cE \u22121 ) w,f = c(P r F E \u22121 ) w,f\n\nPOOLED CASE (AE)\n\nLemma 3. The empirical statistics of node-feature pairs learned by the AE algorithm give unbiased estimates of mean joint probabilities over different path lengths as follows:\nplim l\u2192\u221e #(w,f ) |D| = c t D( t r=1 P r )F w,f (1) Proof. By construction, |D| = r |D r |, #(w, f ) = r #(w, f ) r , |D r | = |D s | \u2200 r, s \u2208 {1, .\n. . , t} and so |D s | = t \u22121 |D|. Combining with Lemma 2, the result follows.\n\nTheorem 2. AE embeddings approximately factorize the pooled node-feature matrix:\nlog c t ( t r=1 P r )FE \u22121 \u2212 log b .\nProof. The proof is analogous to the proof of Theorem 1.\n\nRemark 1. DeepWalk is a corner case of AE with F = I |V| .\n\nThat is, DeepWalk is equivalent to AE if each node has a single unique feature. Thus E = diag(1 DI) = D and, by Theorem 2, DeepWalk's embeddings factorize log c t ( t r=1 P r )D \u22121 \u2212 log b, as previously noted by Qiu et al. (2018).\n\n\nRemark 2. Walklets is a corner case of MUSAE with\nF = I |V| .\nThus, for r = 1, . . . , t, the embeddings of Walklets factorise log c P r D \u22121 \u2212 log b.\n\nRemark 3. Appending an identity matrix I to the feature matrices F of AE and MUSAE (denoted [F ; I]) adds a unique feature to each node. The resulting algorithms, named AE-EGO and MUSAE-EGO, learn embeddings that, respectively, approximately factorize the node-feature PMI matrices:\nlog c P r [F ; I] E \u22121 \u2212 log b, \u2200r \u2208 {1, ..., t};\nand\nlog c t ( t r=1 P r ) [F ; I] E \u22121 \u2212 log b .\n\nCOMPLEXITY ANALYSIS\n\nUnder the assumption of a constant number of features per source node and first-order attributed random walk sampling, the corpus generation has a runtime complexity of O(n l t x/y), where x = v\u2208V |F v | the total number of features across all nodes (including repetition) and y = |V| the number of nodes. Using negative sampling, the optimization runtime of a single asynchronous gradient descent epoch on AE and the joint optimization runtime of MUSAE embeddings is described by O(b d n l t x/y). If one does p truncated walks from each source node, the corpus generation complexity is O(p y l t x) and the model optimization runtime is O(b d p y l t x). Our later runtime experiments in Section 5 will underpin optimization runtime complexity discussed above.\n\nCorpus generation has a memory complexity of O(n l t x/y) while the same when generating p truncated walks per node has a memory complexity of O(p y l t x). Storing the parameters of an AE embedding has a memory complexity of O(y d) and MUSAE embeddings also use O(y d) memory.\n\n\nEXPERIMENTAL EVALUATION\n\nIn order to evaluate the quality of created representations we test the embeddings on supervised downstream tasks such as node classification, transfer learning across networks, regression, and link prediction. Finally, we investigate how changes in the input size affect the runtime. For doing so we utilize social networks and web graphs that we collected from Facebook, Github, Twitch and Wikipedia. The data sources, collection procedures and the datasets themselves are described with great detail in Appendix B. In addition we tested our methods on citation networks widely used for model evaluation (Shchur et al., 2018). Across all experiments we use the same hyperparameter settings of our own model, competing unsupervised methods and graph neural networks -these are respectively listed in Appendices C, E and F.\n\n\nNODE CLASSIFICATION\n\nWe evaluate the node classification performance in two separate scenarios. In the first one we do k-shot learning by using the attributed embedding vectors with logistic regression to predict labels on the Facebook, Github and Twitch Portugal graphs. In the second one we test the predictive performance under a fixed size train-test split to compare against various embedding methods and competitive neural network architectures.\n\n\nK-SHOT LEARNING\n\nIn this experiment we take k randomly selected samples per class, and use the attributed node embeddings to train a logistic regression model with l 2 regularization and predict the labels on the remaining vertices. We repeated the above procedure with seeded splits 100 times to obtain robust comparable results (Shchur et al., 2018). From these we calculated the average of micro averaged F 1 scores to compare our own methods with other unsupervised node embedding procedures. We varied k in order to show the efficacy of the methods -what are the gains when the training set size is increased. These results are plotted on subplots of Figure 2 for the Facebook, Github and Twitch Portugal networks.\n\nBased on these plots it is evident that MUSAE and AE embeddings have little gains in terms of micro F 1 score when additional data points are added to the training set when k is larger than 12. This implies that our method is data efficient. Moreover, MUSAE-EGO and AE-EGO have a slight performance advantage, which means that including the nodes in the attributed random walks helps when a small amount of labeled data is available in the downstream task.  Figure 2: Node classification k-shot learning performance as a function of training samples per class evaluated by average micro F 1 scores calculated from a 100 seeded train-test splits.\n\n\nFIXED RATIO TRAIN-TEST SPLITS\n\nIn this series of experiments we created a 100 seeded train test splits of nodes (80% train -20% test) and calculated weighted, micro and macro averaged F 1 scores on the test set to compare our methods to various embedding and graph neural network methods. Across procedures the same random seeds were used to obtain the train-test split this way the performances are directly comparable. We attached these results on the Facebook, Github and Twitch Portugal graphs as Table 5 of Appendix G. In each column red denotes the best performing unsupervised embedding model and blue corresponds to the strongest supervised neural model. We also attached additional supporting results using the same experimental setting with the unsupervised methods on the Cora, Citeseer, and Pubmed graphs as Table 6 of Appendix G.\n\nIn terms of micro F 1 score our strongest method outperforms on the Facebook and GitHub networks the best unsupervised method by 1.01% and 0.47% respectively. On the Twitch Portugal network the relative micro F 1 advantage of ASNE over our best method is 1.02%. Supervised node embedding methods outperform our and other unsupervised methods on every dataset for most metrics. In terms of micro F 1 this relative advantage over our best performing model variant is the largest with 4.67% on the Facebook network, and only 0.11% on Twitch Portugal.\n\nOne can make four general observations based on our results (i) multi-scale representations can help with the classification tasks compared to pooled ones; (ii) the addition of the nodes in the ego augmented models to the feature sets does not help the performance when a large amount of labeled training data is available; (iii) based on the standard errors supervised neural models do not necessarily have a significant advantage over unsupervised methods (see the results on the Github and Twitch datasets); (iv) attributed node embedding methods that only consider first-order neighbourhoods have a poor performance.\n\n\nTRANSFER LEARNING ON TWITCH SOCIAL NETWORKS\n\nNeighbourhood based methods such as DeepWalk (Perozzi et al., 2014) are transductive and the function used to create the embedding cannot map nodes that are not connected to the original graph to the latent space. However, vanilla MUSAE and AE are inductive and can easily map nodes to the embedding space if the attributes across the source and target graph are shared. This also means that supervised models trained on the embedding of a source graph are transferable. Importantly those attributed embedding methods such as AANE or ASNE that explicitly use the graph are unable to do this transfer.\n\nUsing the disjoint Twitch country level social networks (inter country edges are not present) we did a transfer learning experiment. First, we learn an embedding function given the social network from a country with the standard parameter settings. Second, we train regularized logistic regression on the embedding to predict whether the Twitch user streams explicit content. Third, using the embedding function we map the target graph to the embedding space. Fourth, we use the logistic model to predict the node labels on the target graph. We evaluate the performance by the micro F 1 score based on 10 experimental repetitions. These averages with standard error bars are plotted for the Twitch Germany, England and Spain datasets as target graphs on Figure 3. We added additional results with France, Portugal and Russia being the target country in Appendix H as  These results support that MUSAE and AE create features that are transferable across disjoint graphs that share vertex features. Moreover, the transfer of the downstream model is also possible across datasets. There is no clear evidence that either MUSAE or AE gives better results on this specific problem. We also see some evidence that upstream and downstream models that we trained on graphs with more vertices transfer better.\n\n\nREGRESSION ON WIKIPEDIA GRAPHS\n\nWe created embeddings of the Wikipedia webgraphs with all of our methods and the unsupervised baselines. Using a 80% train -20% test split we predict the log of average traffic for each page using an elastic net model. The hyperparameters of the downstream model are available in Appendix D. In Table 7 of Appendix I we report average test R 2 and standard error of the predictive performance over 100 seeded train-test splits. Our key observation are: (i) that MUSAE outperforms all benchmark neighbourhood preserving and attributed node embedding methods, with the strongest MUSAE variant outperforming the best baseline between 2.05% and 10.03% (test R 2 ); (ii) that MUSAE significantly outperforms AE by between 2.49% and 21.64% (test R 2 ); and (iii) the benefit of using the vertices as features (ego augmented model) can improve the performance of embeddings, but appears to be dataset specific phenomenon.\n\n\nLINK PREDICTION ON WEB GRAPHS AND SOCIAL NETWORKS\n\nThe final series of experiments dedicated to the representation quality is about link prediction. We carried out an attenuated graph embedding trial to predict the removed edges from the graph. First, we randomly removed 50% of edges while the connectivity of the graph was not changed. Second, an embedding is created from the attenuated graph. Third, we calculate features for the removed edges and the same number of randomly selected pairs of nodes (negative candidates) with binary operators to create d-dimensional edge features. We use the binary operators applied by Grover & Leskovec (2016). Specifically, we calculated the average, element-wise product, element-wise l 1 norm and the element-wise l 2 norm of vectors. Finally, we created a 100 seeded 80% train -20% test splits and used logistic regression to predict whether an edge exists.\n\nWe compared to attributed and neighbourhood based embedding methods and average AUC scores are presented in Tables 8 and 9 of Appendix J. Our results show that Walklets (Perozzi et al., 2017) the multi-scale neighbourhood based embedding method materially outperforms every other method on most of the datasets and attributed embedding methods generally do poorly in terms of AUC compared to neighbourhood based ones.\n\n\nSCALABILITY\n\nIn order to show the efficacy of our algorithms we run a series of experiments on synthetic graphs where we are able to manipulate the input size. Specifically, we look at the effect of changing the number of vertices and features per vertex. Our detailed experimental setup was as follows. Each point in Figure 4 is the mean runtime obtained from 100 experimental runs on Erdos-Renyi graphs. The base graph that we manipulated had 2 11 nodes, 2 3 edges and the same number of unique features per node uniformly selected from a feature set of 2 11 . Our experimental settings were the same as the ones described in Appendix C except for the number of epochs. We only did a single training epoch with asynchronous gradient descent on each graph. We tested the runtime with 1, 2 and 4 cores and included a dashed line as the linear runtime reference in each subfigure. We observe that doubling the average number of features per vertex doubles the runtime of AE and MUSAE. Moreover, the number of cores used during the optimization does not decrease the runtime when the number of unique features per vertex compared to the cardinality of the feature set is large. When we look at the change in the vertex set size we also see a linear behaviour. Doubling the input size simply results in a doubled optimization runtime. In addition, if one interpolates linearly from these results it comes that a network with 1 million nodes, 8 edges per node, 8 unique features per node can be embedded with MUSAE on commodity hardware in less than 5 hours. This interpolation assumes that the standard parameter settings proposed in Appendix C and 4 cores were used for optimization.\n\n\nDISCUSSION AND CONCLUSION\n\nWe investigated attributed node embedding and proposes efficient pooled (AE) and multi-scale (MUSAE) attributed node embedding algorithms with linear runtime. We proved that these algorithms implicitly factorize probability matrices of features appearing in the neighbourhood of nodes. Two widely used neighbourhood preserving node embedding methods Perozzi et al. (2014;2017) are in fact simplified cases of our models. On several datasets (Wikipedia, Facebook, Github, and citation networks) we found that representations learned by our methods, in particular MUSAE, outperform neighbourhood based node embedding methods ( (2018)).\n\nOur proposed embedding models are differentiated from other methods in that they encode feature information from higher order neighborhoods. The most similar previous model BANE (Yang et al., 2018) encodes node attributes from higher order neighbourhoods but has non-linear runtime complexity and the product of adjacency matrix power and feature matrix is decomposed explicitly.\n\n\nACKNOWLEDGMENTS\n\nBenedek Rozemberczki and Carl Allen were supported by the Centre for Doctoral Training in Data Science,funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh. A PROOFS Lemma 1. The empirical statistics of node-feature pairs obtained from random walks give unbiased estimates of joint probabilities of observing feature f \u2208 F r steps (i) after; or (ii) before node v \u2208 V, as given by:\nplim l\u2192\u221e #(w,f )\u2192 r |D\u2192 r | = c \u22121 (DP r F ) w,f plim l\u2192\u221e #(w,f )\u2190 r |D\u2190 r | = c \u22121 (F DP r ) f,w\nProof. The proof is analogous to that given for Theorem 2.1 in Qiu et al. (2018). We show that the computed statistics correspond to sequences of random variables with finite expectation, bounded variance and covariances that tend to zero as the separation between variables within the sequence tends to infinity. The Weak Law of Large Numbers (S.N.Bernstein) then guarantees that the sample mean converges to the expectation of the random variable. We first consider the special case n = 1, i.e. we have a single sequence w 1 , ..., w l generated by a random walk (see Algorithm 1). For a particular node-feature pair (w, f ), we let Y i , i \u2208 {1, ..., l \u2212 t}, be the indicator function for the event w i = w and f \u2208 F i+r . Thus, we have:\n#(w,f )\u2192 r |D\u2192 r | = 1 l\u2212t l\u2212t i=1 Y i ,(2)\nthe sample average of the Y i s. We also have:\nE[Y i ] = deg(w) c (P r F ) w,f = 1 c (DP r F ) w,f E[Y i Y j ] = Prob[w i = w, f \u2208 F i+r , w j = w, f \u2208 F j+r ] = deg(w) c p(wi=w) P r :w p(wi+r|wi=w) diag(F :f ) p(f \u2208Fi+r|wi+r) P j\u2212(i+r) :w p(wj =w|wi+r) p(wj =w,f \u2208Fi+r|wi=w) P r w: F :f p(f \u2208Fj+r|wj =w)\nfor j > i + r. This allows us to compute the covariance:\nCov(Y i , Y j ) = E[Y i Y j ] \u2212 E[Y i ] E[Y j ] = deg(w) c P r w: diag(F :f ) P j\u2212(i+r) :w \u2212 deg(w) c 1 tends to 0 as j\u2212i\u2192\u221e P r w: F :f ,(3)\nwhere 1 is a vector of ones. The difference term (indicated) tends to zero as j \u2212 i \u2192 \u221e since then p(w j = w|w i+r ) tends to the stationary distribution p(w) = deg(w) c , regardless of w i+r .\n\nThus, applying the Weak Law of Large Numbers, the sample average converges in probability to the expected value, i.e.:\n#(w,f )\u2192 r |D\u2192 r | = 1 l\u2212t l\u2212t i=1 Y i p \u2192 1 l\u2212t l\u2212t i=1 E[Y i ] = 1 c (DP r F ) w,f A similar argument applies to #(w,f )\u2190 r |D\u2190 r | , with expectation term 1 c (F DP r ) f,w .\nIn both cases, the argument readily extends to the general setting where n > 1 with suitably defined indicator functions for each of the n random walks (see Qiu et al. (2018)).\n\nLemma 2. Empirical statistics of node-feature pairs obtained from random walks give unbiased estimates of joint probabilities of observing feature f \u2208 F r steps either side of node v \u2208 V, given by: \nplim l\u2192\u221e #(w,f )r |Dr| = c \u22121 (DP r F ) w,f , Proof. #(w,f )r |Dr| = #(w,f )\u2192 r |Dr| + #(w,f )\u2190 r |Dr| = 1 2 #(w,f )\u2192 r |D\u2192 r | + #(w,f )\u2190 r |D\u2190 r | p \u2192 1 2 1 c (DP r F ) w,f + 1 c (F DP r ) f,w = 1 2c DP r F + P r DF w,f = 1 2c (DP r + (A D \u22121 ) r D)F w,f = 1 2c (DP r + D(D \u22121 A ) r )F w,f = 1 c (DP r F ) w,f .\n\nB DATASETS AND DESCRIPTIVE STATISTICS\n\nOur method was evaluated on a variety of social networks and web page-page graphs that we collected from openly available API services. In Table 1 we described the graphs with widely used statistics with respect to size, diameter, and level of clustering. We also included the average number of features per vertex and unique feature count in the last columns. These datasets are available with the source code of MUSAE and AE at https://github.com/benedekrozemberczki/ MUSAE.\n\n\nB.1 FACEBOOK PAGE-PAGE DATASET\n\nThis webgraph is a page-page graph of verified Facebook sites. Nodes represent official Facebook pages while the links are mutual likes between sites. Node features are extracted from the site descriptions that the page owners created to summarize the purpose of the site. This graph was collected through the Facebook Graph API in November 2017 and restricted to pages from 4 categories which are defined by Facebook. These categories are: politicians, governmental organizations, television shows and companies. As one can see in Table 1 it is a highly clustered graph with a large diameter. The task related to this dataset is multi-class node classification for the 4 site categories.\n\n\nB.2 GITHUB WEB AND MACHINE LEARNING DEVELOPERS DATASET\n\nThe largest graph used for evaluation is a social network of GitHub developers which we collected from the public API in June 2019. Nodes are developers who have starred at least 10 repositories and edges are mutual follower relationships between them. The vertex features are extracted based on the location, repositories starred, employer and e-mail address. The task related to the graph is binary node classification -one has to predict whether the GitHub user is a web or a machine learning developer. This target feature was derived from the job title of each user. As the descriptive statistics show in Table 1 this is the largest graph that we use for evaluation with the highest sparsity.\n\n\nB.3 WIKIPEDIA DATASETS\n\nThe datasets that we use to perform node level regression are Wikipedia page-page networks collected on three specific topics: chameleons, crocodiles and squirrels. In these networks nodes are articles from the English Wikipedia collected in December 2018, edges are mutual links that exist between pairs of sites. Node features describe the presence of nouns appearing in the articles. For each node we also have the average monthly traffic between October 2017 and November 2018. In the regression tasks used for embedding evaluation the logarithm of average traffic is the target variable. Table 1 shows that these networks are heterogeneous in terms of size, density, and clustering.\n\n\nB.4 TWITCH DATASETS\n\nThese datasets used for node classification and transfer learning are Twitch user-user networks of gamers who stream in a certain language. Nodes are the users themselves and the links are mutual friendships between them. Vertex features are extracted based on the games played and liked, location and streaming habits. Datasets share the same set of node features, this makes transfer learning across networks possible. These social networks were collected in May 2018. The supervised task related to these networks is binary node classification -one has to predict whether a streamer uses explicit language.\n\n\nC STANDARD HYPERPARAMETER SETTINGS OF OUR EMBEDDING MODELS\n\nIn MUSAE and AE models we have a set of parameters that we use for model evaluation. Our parameter settings listed in Table 2 are quite similar to the widely used general settings of random walk sampled implicit factorization machines (Perozzi et al., 2014;Grover & Leskovec, 2016;Ribeiro et al., 2017;Perozzi et al., 2017). Each of our models is augmented with a Doc2Vec (Mikolov et al., 2013a;b) embedding of node features -this is done such way that the overall dimension is still 128.\n\n\nD HYPERPARAMETER SETTINGS OF THE DOWNSTREAM MODELS\n\nThe downstream tasks uses logistic and elastic net regression from Scikit-learn (Pedregosa et al., 2011) for node level classification, regression and link prediction. For the evaluation of every embedding model we use the standard settings of the library except for the regularization and norm mixing parameters. These are described in Table 3.  Our purpose was a fair evaluation compared to other unsupervised neighbourhood based and attributed node embedding procedures. Because of this each we tried to use hyperparameter settings that give similar expressive power to the competing methods with respect to target matrix approximation (Perozzi et al., 2014;Grover & Leskovec, 2016;Perozzi et al., 2017) and number of dimensions.\n\n\u2022 DeepWalk (Perozzi et al., 2014): We used the hyperparameter settings described in Table  2. While the original DeepWalk model uses hierarchical softmax to speed up calculations we used a negative sampling based implementation. This way DeepWalk can be seen as a special case of Node2Vec (Grover & Leskovec, 2016) when the second-order random walks are equivalent to the firs-order walks.\n\n\u2022 LINE 2 (Tang et al., 2015): We created 64 dimensional embeddings based on first and second order proximity and concatenated these together for the downstream tasks. Other hyperparameters are taken from the original work.\n\n\u2022 Node2Vec (Grover & Leskovec, 2016): Except for the in-out and return parameters that control the second-order random walk behavior we used the hyperparameter settings described in Table 2. These behavior control parameters were tuned with grid search from the {4, 2, 1, 0.5, 0.25} set using a train-validation split of 80% \u2212 20% within the training set itself.\n\n\u2022 Walklets (Perozzi et al., 2017): We used the hyperparameters described in Table 2 except for window size. We set a window size of 4 with individual embedding sizes of 32. This way the overall number of dimensions of the representation remained the same.\n\n\u2022 The attributed node embedding methods AANE, ASNE, BANE, TADW, TENE all use the hyperparameters described in the respective papers except for the dimension. We parametrized these methods such way that each of the final embeddings used in the downstream tasks is 128 dimensional.\n\n\nF HYPERPARAMETER SETTINGS OF COMPETING GRAPH NEURAL NETWORKS\n\nEach model was optimized with the Adam optimizer (Kingma & Ba, 2015) with the standard moving average parameters and the model implementations are sparsity aware modifications based on PyTorch Geometric (Fey & Lenssen, 2019). We needed these modifications in order to accommodate the large number of vertex features -see the last column in Table 1. Except for the GAT model (Veli\u010dkovi\u0107 et al., 2018) we used ReLU intermediate activation functions (Nair & Hinton, 2010) with a softmax unit in the final layer for classification. The hyperparameters used for the training and regularization of the neural models are listed in Table 4. Except for the APPNP model each baseline uses information up to 2-hop neighbourhoods. The model specific settings when we needed to deviate from the basic settings which are listed in Table  4 were as follows:\n\n\u2022 Classical GCN (Kipf & Welling, 2017): We used the standard parameter settings described in this section.  , 1998). We clustered the graphs into disjoint clusters, and the number of clusters was the same as the number of node classes (e.g. in case of the Facebook page-page network we created 4 clusters). For training we used the earlier described setup.\n\n\u2022 APPNP (Klicpera et al., 2019): The top level feed-forward layer had 32 hidden neurons, the teleport probability was set as 0.2 and we used 20 steps for approximate personalized pagerank calculation.\n\n\u2022 SGCONV (Wu et al., 2019): We used the 2 nd power of the normalized adjacency matrix for training the classifier.      \n\n\nG CLASSIFICATION PERFORMANCE\n\n\nof the target matrix.\n\nFigure 1 :\n1Phenomena affecting and inspiring the design of the multi-scale attributed network embedding procedure. In\n\n\n(Kipf & Welling, 2017), GraphSage (Hamilton et al., 2017), GAT (Veli\u010dkovi\u0107 et al., 2018), APPNP (Klicpera et al., 2019), SGCONV (Wu et al., 2019) and ClusterGCN (Chiang et al., 2019), create latent node representations that pool node attributes from arbitrary order neighborhoods, which are then inseparable and unrecoverable. In contrast, MixHop (Abu-El-Haija et al., 2019) learns latent features for each proximity.\n\n\nSGNS on D with b negative samples and d dimensions.16  Output gv, \u2200v \u2208 V, and hf , \u2200f \u2208 F = \u222aVFv.\n\n3 ( w1 ,\n3w1w2, . . . , wl) \u2190 Sample Nodes(G,\n\nFigure 3 :\n3Mean micro F 1 scores and standard errors calculated from 10 transfer learning runs with MUSAE and AE on the Twitch graphs using Germany, England and Spain as target for the transfer.\n\nFigure 4 :\n4Optimization time as a function of average feature count / number of vertices.\n\n\nPerozzi et al. (2014); Grover & Leskovec (2016)), multi-scale algorithms (Tang et al. (2015); Perozzi et al. (2017)) and recently proposed attributed node embedding procedures (Yang et al. (2015); Liao et al. (2018); Huang et al. (2017); Yang et al. (2018); Yang & Yang\n\n\nThe final step follows by symmetry of A, indicating how the Lemma can be extended to directed graphs.\n\n\u2022\nGraphSAGE (Hamilton et al., 2017): We utilized a graph convolutional aggregator on the sampled neighbourhoods, samples of 40 nodes per source, and the standard hyperparameter settings. \u2022 GAT (Veli\u010dkovi\u0107 et al., 2018): The negative slope parameter of the leaky ReLU function was 0.2 and we applied a single attention head. Besides these, we used the standard hyperparameter settings. \u2022 MixHop (Abu-El-Haija et al., 2019): We took advantage of the 0 th , 1 st and 2 nd powers of the normalized adjacency matrix with 32 dimensional convolutional filters for creating the first hidden representations. This was fed to a feed-forward layer to classify the nodes. \u2022 ClusterGCN (Chiang et al., 2019): Just as Chiang et al. (2019) did, we used the METIS procedure (Karypis & Kumar\n\nFigure 5 :\n5Mean micro F 1 scores and standaard errors calculated from 10 transfer learning runs with MUSAE and AE on the Twitch graphs using France, Portugal and Russia as targets for the transfer.I REGRESSION RESULTS ON WIKIPEDIA PAGE-PAGE GRAPHS\n\n\nNode embedding g and feature embedding h.Data: G = (V, L) -Graph to be embedded. \n{Fv}V -Set of node feature sets. \nn -Number of sequence samples. \nl -Length of sequences. \nt -Context size. \nd -Embedding dimension. \nb -Number of negative samples. \nResult: \n\nTable 5 .\n5EN ES FR PT RU \n0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nMicro F \n1 Score \n\nMUSAE transfer to DE users \n\nDE ES FR PT RU \n0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nMicro F \n1 Score \n\nMUSAE transfer to EN users \n\nDE EN FR PT RU \n0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nMicro F \n1 Score \n\nMUSAE transfer to ES users \n\nEN ES FR PT RU \n0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nMicro F \n1 Score \n\nAE transfer to DE users \n\nDE ES FR PT RU \n0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nMicro F \n1 Score \n\nAE transfer to EN users \n\nDE EN FR PT RU \n0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\nMicro F \n1 Score \n\nAE transfer to ES users \n\n\n\n\nMatthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. Xiao Huang, Jundong Li, and Xia Hu. Accelerated attributed network embedding. In Proceedings of the 2017 SIAM International Conference on Data Mining, pp. 633-641. SIAM, 2017. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111-3119, 2013b. Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML), pp. 807-814, 2010. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining., 2014. Bryan Perozzi, Vivek Kulkarni, Haochen Chen, and Steven Skiena. Don't walk, skip!: Online learning of multi-scale network embeddings. In Proceedings of the 2017 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2017, pp. 258-265, 2017.Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An \nefficient algorithm for training deep and large graph convolutional networks. In Proceedings of \nthe 25th ACM SIGKDD International Conference on Knowledge Discovery &#38; Data Mining \n(KDD), pp. 257-266, 2019. \n\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings \nof the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, \n2016. \n\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. \nIn Advances in Neural Information Processing Systems (NIPS) 30, pp. 1024-1034. Curran Asso-\nciates, Inc., 2017. \nGeorge Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irreg-\nular graphs. SIAM Journal on scientific Computing, 20(1):359-392, 1998. \n\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International \nConference on Learning Representations (ICLR), 2015. \n\nThomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-\nworks. In International Conference on Learning Representations (ICLR), 2017. \n\nJohannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: \nGraph neural networks meet personalized pagerank. In International Conference on Learning \nRepresentations (ICLR), 2019. \n\nOmer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Ad-\nvances in neural information processing systems, pp. 2177-2185, 2014. \n\nLizi Liao, Xiangnan He, Hanwang Zhang, and Tat-Seng Chua. Attributed social network embed-\nding. IEEE Transactions on Knowledge and Data Engineering, 30(12):2257-2270, 2018. \n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-\ntations in vector space. 2013a. \n\nFabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier \nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: \nMachine learning in python. Journal of machine learning research, 12(Oct):2825-2830, 2011. \n\nJiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as \nmatrix factorization: Unifying deepwalk, line, pte, and node2vec. In Proceedings of the Eleventh \nACM International Conference on Web Search and Data Mining, pp. 459-467, 2018. \n\nLeonardo F.R. Ribeiro, Pedro H.P. Saverese, and Daniel R. Figueiredo. Struc2vec: Learning node \nrepresentations from structural identity. In Proceedings of the 23rd ACM SIGKDD International \nConference on Knowledge Discovery and Data Mining, KDD '17, pp. 385-394, 2017. \n\nOleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Pitfalls \nof graph neural network evaluation. Relational Representation Learning Workshop, NeurIPS \n2018, 2018. \n\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale \ninformation network embedding. In Proceedings of the 24th International Conference on World \nWide Web, pp. 1067-1077, 2015. \n\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua \nBengio. Graph attention networks. In International Conference on Learning Representations \n(ICLR), 2018. \nFelix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Sim-\nplifying graph convolutional networks. In Proceedings of the 36th International Conference on \nMachine Learning (ICML), pp. 6861-6871, 2019. \n\nCheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network representation \nlearning with rich text information. In IJCAI, pp. 2111-2117, 2015. \n\nHong Yang, Shirui Pan, Peng Zhang, Ling Chen, Defu Lian, and Chengqi Zhang. Binarized at-\ntributed network embedding. In 2018 IEEE International Conference on Data Mining (ICDM), \npp. 1476-1481. IEEE, 2018. \n\nShuang Yang and Bo Yang. Enhanced network embedding with text information. In 2018 24th \nInternational Conference on Pattern Recognition (ICPR), pp. 326-331. IEEE, 2018. \n\nDaokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. Sine: Scalable incomplete network \nembedding. In 2018 IEEE International Conference on Data Mining (ICDM), pp. 737-746. IEEE, \n2018. \n\n\n\nTable 1 :\n1Descriptive statistics of the networks used in our experimental evaluation.Dataset \nNodes \nEdges \nDiameter \nClustering \nCoefficient \nDensity \nAverage \nFeature \n\nUnique \nFeatures \nFacebook Page-Page \n22,470 171,002 \n15 \n0.232 \n0.001 \n14.000 \n4,714 \nGitHub Web-ML \n37,700 289,003 \n7 \n0.013 \n0.001 \n18.312 \n4,005 \nWikipedia Chameleon \n2,277 \n31,421 \n11 \n0.314 \n0.012 \n21.547 \n3,132 \nWikipedia Crocodile \n11,631 170,918 \n11 \n0.026 \n0.003 \n75.161 \n13,183 \nWikipedia Squirrel \n5,201 \n198,493 \n10 \n0.348 \n0.015 \n26.474 \n3,148 \nTwitch DE \n9,498 \n153,138 \n7 \n0.047 \n0.003 \n20.397 \n2,545 \nTwitch EN \n7,126 \n35,324 \n10 \n0.042 \n0.002 \n20.799 \n2,545 \nTwitch ES \n4,648 \n59,382 \n9 \n0.084 \n0.006 \n19.391 \n2,545 \nTwitch FR \n6,549 \n112,666 \n7 \n0.054 \n0.005 \n19.758 \n2,545 \nTwitch PT \n1,912 \n31,299 \n7 \n0.131 \n0.017 \n19.944 \n2,545 \nTwitch RU \n4,385 \n37,304 \n9 \n0.049 \n0.004 \n20.635 \n2,545 \n\n\n\nTable 2 :\n2Standard hyperparameter settings of the AE and MUSAE embeddings.Parameter \nValue Notation \nDimensions \n128 \nd \nWalk length \n80 \nl \nNumber of walks per node 10 \np \nNumber of epochs \n5 \nk \nWindow size \n3 \nt \nInitial learning rate \n0.05 \n\u03b1 max \nFinal learning rate \n0.025 \u03b1 min \nNegative samples \n5 \nb \n\n\n\nTable 3 :\n3Standard hyperparameter settings of the downstream logistic and elastic net regression models that use the embeddings for classification, link prediction and regression.Parameter \nValue Notation \nRegularization coefficient \n0.01 \n\u03bb \nNorm mixing parameter \n0.5 \n\u03b3 \n\nE HYPERPARAMETER SETTINGS OF COMPETING UNSUPERVISED \n\nEMBEDDING METHODS \n\n\n\nTable 4 :\n4Hyperparameter settings used for training the graph neural network baselines.Parameter \nValue \nEpochs \n200 \nLearning rate \n0.01 \nDropout \n0.5 \nl 2 Weight regularization 0.001 \nDepth \n2 \nFilters per layer \n32 \n\n\n\nTable 5 :\n5Node classification test performance evaluated by weighted, micro and macro F 1 scores calculated from 10 seeded train-test splits. We included standard errors of the scores and used 80% of nodes for training / 20% of nodes for testing. Red numbers denote the best performing node embedding method and blue ones denote the best performing supervised graph neural network.Datasets \nFacebook Page-Page \nGitHub WebML \nTwitch Portugal \nWeighted \nMicro \nMacro \nWeighted \nMicro \nMacro \nWeighted \nMicro \nMacro \nDeepWalk \n0.861 \n\n\u00b10.001 \n\n0.863 \n\n\u00b10.001 \n\n0.848 \n\n\u00b10.001 \n\n0.852 \n\n\u00b10.001 \n\n0.858 \n\n\u00b10.001 \n\n0.801 \n\n\u00b10.002 \n\n0.650 \n\n\u00b10.008 \n\n0.672 \n\n\u00b10.007 \n\n0.594 \n\n\u00b10.009 \n\nLINE2 \n0.874 \n\n\u00b10.001 \n\n0.875 \n\n\u00b10.001 \n\n0.862 \n\n\u00b10.001 \n\n0.852 \n\n\u00b10.001 \n\n0.858 \n\n\u00b10.001 \n\n0.800 \n\n\u00b10.002 \n\n0.636 \n\n\u00b10.006 \n\n0.670 \n\n\u00b10.005 \n\n0.571 \n\n\u00b10.005 \n\nNode2Vec \n0.889 \n\n\u00b10.001 \n\n0.890 \n\n\u00b10.001 \n\n0.880 \n\n\u00b10.001 \n\n0.853 \n\n\u00b10.001 \n\n0.859 \n\n\u00b10.001 \n\n0.802 \n\n\u00b10.001 \n\n0.665 \n\n\u00b10.004 \n\n0.686 \n\n\u00b10.004 \n\n0.612 \n\n\u00b10.004 \n\nWalklets \n0.886 \n\n\u00b10.001 \n\n0.887 \n\n\u00b10.001 \n\n0.875 \n\n\u00b10.001 \n\n0.854 \n\n\u00b10.001 \n\n0.860 \n\n\u00b10.001 \n\n0.804 \n\n\u00b10.002 \n\n0.652 \n\n\u00b10.006 \n\n0.671 \n\n\u00b10.006 \n\n0.599 \n\n\u00b10.005 \n\nTADW \n0.760 \n\n\u00b10.002 \n\n0.765 \n\n\u00b10.002 \n\n0.740 \n\n\u00b10.003 \n\n0.650 \n\n\u00b10.001 \n\n0.748 \n\n\u00b10.001 \n\n0.528 \n\n\u00b10.007 \n\n0.459 \n\n\u00b10.001 \n\n0.659 \n\n\u00b10.005 \n\n0.406 \n\n\u00b10.003 \n\nAANE \n0.793 \n\n\u00b10.001 \n\n0.796 \n\n\u00b10.001 \n\n0.775 \n\n\u00b10.001 \n\n0.848 \n\n\u00b10.001 \n\n0.856 \n\n\u00b10.001 \n\n0.794 \n\n\u00b10.002 \n\n0.636 \n\n\u00b10.006 \n\n0.661 \n\n\u00b10.006 \n\n0.577 \n\n\u00b10.006 \n\nASNE \n0.794 \n\n\u00b10.001 \n\n0.797 \n\n\u00b10.001 \n\n0.776 \n\n\u00b10.001 \n\n0.829 \n\n\u00b10.001 \n\n0.839 \n\n\u00b10.001 \n\n0.766 \n\n\u00b10.002 \n\n0.670 \n\n\u00b10.006 \n\n0.685 \n\n\u00b10.006 \n\n0.620 \n\n\u00b10.006 \n\nBANE \n0.868 \n\n\u00b10.001 \n\n0.868 \n\n\u00b10.001 \n\n0.859 \n\n\u00b10.002 \n\n0.711 \n\n\u00b10.001 \n\n0.762 \n\n\u00b10.001 \n\n0.576 \n\n\u00b10.001 \n\n0.644 \n\n\u00b10.006 \n\n0.664 \n\n\u00b10.006 \n\n0.587 \n\n\u00b10.006 \n\nTENE \n0.724 \n\n\u00b10.002 \n\n0.731 \n\n\u00b10.002 \n\n0.699 \n\n\u00b10.002 \n\n0.842 \n\n\u00b10.001 \n\n0.850 \n\n\u00b10.001 \n\n0.785 \n\n\u00b10.002 \n\n0.613 \n\n\u00b10.005 \n\n0.664 \n\n\u00b10.006 \n\n0.536 \n\n\u00b10.006 \n\nAE \n0.887 \n\n\u00b10.001 \n\n0.888 \n\n\u00b10.001 \n\n0.879 \n\n\u00b10.001 \n\n0.858 \n\n\u00b10.001 \n\n0.863 \n\n\u00b10.001 \n\n0.807 \n\n\u00b10.001 \n\n0.653 \n\n\u00b10.005 \n\n0.672 \n\n\u00b10.004 \n\n0.598 \n\n\u00b10.006 \n\nAE-EGO \n0.898 \n\n\u00b10.001 \n\n0.899 \n\n\u00b10.001 \n\n0.890 \n\n\u00b10.001 \n\n0.857 \n\n\u00b10.001 \n\n0.863 \n\n\u00b10.001 \n\n0.807 \n\n\u00b10.002 \n\n0.652 \n\n\u00b10.007 \n\n0.671 \n\n\u00b10.007 \n\n0.599 \n\n\u00b10.009 \n\nMUSAE \n0.886 \n\n\u00b10.001 \n\n0.887 \n\n\u00b10.001 \n\n0.877 \n\n\u00b10.001 \n\n0.859 \n\n\u00b10.001 \n\n0.864 \n\n\u00b10.001 \n\n0.810 \n\n\u00b10.001 \n\n0.654 \n\n\u00b10.006 \n\n0.672 \n\n\u00b10.006 \n\n0.6 \n\n\u00b10.007 \n\nMUSAE-EGO \n0.893 \n\n\u00b10.001 \n\n0.894 \n\n\u00b10.001 \n\n0.884 \n\n\u00b10.001 \n\n0.859 \n\n\u00b10.001 \n\n0.864 \n\n\u00b10.001 \n\n0.810 \n\n\u00b10.001 \n\n0.655 \n\n\u00b10.003 \n\n0.671 \n\n\u00b10.002 \n\n0.604 \n\n\u00b10.003 \n\nGCN \n0.931 \n\n\u00b10.001 \n\n0.932 \n\n\u00b10.001 \n\n0.928 \n\n\u00b10.001 \n\n0.859 \n\n\u00b10.001 \n\n0.865 \n\n\u00b10.001 \n\n0.809 \n\n\u00b10.002 \n\n0.650 \n\n\u00b10.013 \n\n0.695 \n\n\u00b10.007 \n\n0.577 \n\n\u00b10.02 \n\nGraphSAGE \n0.812 \n\n\u00b10.002 \n\n0.814 \n\n\u00b10.002 \n\n0.795 \n\n\u00b10.002 \n\n0.848 \n\n\u00b10.001 \n\n0.854 \n\n\u00b10.001 \n\n0.794 \n\n\u00b10.002 \n\n0.618 \n\n\u00b10.003 \n\n0.631 \n\n\u00b10.004 \n\n0.563 \n\n\u00b10.005 \n\nGAT \n0.918 \n\n\u00b10.001 \n\n0.919 \n\n\u00b10.001 \n\n0.912 \n\n\u00b10.001 \n\n0.856 \n\n\u00b10.001 \n\n0.864 \n\n\u00b10.001 \n\n0.803 \n\n\u00b10.002 \n\n0.648 \n\n\u00b10.008 \n\n0.678 \n\n\u00b10.007 \n\n0.588 \n\n\u00b10.009 \n\nMixHop \n0.940 \n\n\u00b10.001 \n\n0.941 \n\n\u00b10.002 \n\n0.937 \n\n\u00b10.001 \n\n0.847 \n\n\u00b10.0 \n\n0.85 \n\n\u00b10.001 \n\n0.800 \n\n\u00b10.001 \n\n0.626 \n\n\u00b10.003 \n\n0.630 \n\n\u00b10.004 \n\n0.576 \n\n\u00b10.003 \n\nClusterGCN \n0.937 \n\n\u00b10.001 \n\n0.937 \n\n\u00b10.001 \n\n0.934 \n\n\u00b10.001 \n\n0.855 \n\n\u00b10.001 \n\n0.859 \n\n\u00b10.001 \n\n0.807 \n\n\u00b10.001 \n\n0.647 \n\n\u00b10.004 \n\n0.654 \n\n\u00b10.004 \n\n0.602 \n\n\u00b10.005 \n\nAPPNP \n0.938 \n\n\u00b10.001 \n\n0.938 \n\n\u00b10.001 \n\n0.935 \n\n\u00b10.001 \n\n0.860 \n\n\u00b10.002 \n\n0.868 \n\n\u00b10.001 \n\n0.811 \n\n\u00b10.002 \n\n0.683 \n\n\u00b10.009 \n\n0.702 \n\n\u00b10.012 \n\n0.623 \n\n\u00b10.010 \n\nSGCONV \n0.832 \n\n\u00b10.002 \n\n0.836 \n\n\u00b10.002 \n\n0.812 \n\n\u00b10.002 \n\n0.816 \n\n\u00b10.001 \n\n0.829 \n\n\u00b10.001 \n\n0.747 \n\n\u00b10.002 \n\n0.652 \n\n\u00b10.003 \n\n0.663 \n\n\u00b10.003 \n\n0.604 \n\n\u00b10.004 \n\n\n\nTable 6 :\n6Node classification test performance evaluated by weighted, micro and macro F 1 scores calculated from 10 seeded train-test splits. We included standard errors of the scores and used 80% of nodes for training / 20% of nodes for testing. Red numbers denote the best performing node embedding method.H ADDITIONAL TRANSFER LEARNING RESULTS ON THE TWITCH GRAPHS Micro F 1 Score MUSAE transfer to FR users DE EN ES FR RU 0 Micro F 1 Score MUSAE transfer to PT users DE EN ES FR PT 0 Micro F 1 Score MUSAE transfer to RU users DE EN ES PT RU 0 Micro F 1 Score AE transfer to FR users DE EN ES FR RU 0 Micro F 1 Score AE transfer to PT users DE EN ES FR PT 0 Micro F 1 Score AE transfer to RU usersDatasets \nCora \nCiteseer \nPubmed \nWeighted \nMicro \nMacro \nWeighted \nMicro \nMacro \nWeighted \nMicro \nMacro \nDeepWalk \n0.832 \n\n\u00b10.003 \n\n0.833 \n\n\u00b10.004 \n\n0.823 \n\n\u00b10.004 \n\n0.597 \n\n\u00b10.007 \n\n0.603 \n\n\u00b10.007 \n\n0.560 \n\n\u00b10.006 \n\n0.801 \n\n\u00b10.001 \n\n0.802 \n\n\u00b10.001 \n\n0.789 \n\n\u00b10.002 \n\nLINE2 \n0.775 \n\n\u00b10.004 \n\n0.777 \n\n\u00b10.004 \n\n0.768 \n\n\u00b10.005 \n\n0.529 \n\n\u00b10.006 \n\n0.542 \n\n\u00b10.006 \n\n0.486 \n\n\u00b10.005 \n\n0.798 \n\n\u00b10.001 \n\n0.799 \n\n\u00b10.001 \n\n0.785 \n\n\u00b10.001 \n\nNode2Vec \n0.840 \n\n\u00b10.003 \n\n0.840 \n\n\u00b10.003 \n\n0.826 \n\n\u00b10.003 \n\n0.616 \n\n\u00b10.005 \n\n0.622 \n\n\u00b10.005 \n\n0.581 \n\n\u00b10.005 \n\n0.809 \n\n\u00b10.002 \n\n0.810 \n\n\u00b10.002 \n\n0.797 \n\n\u00b10.002 \n\nWalklets \n0.843 \n\n\u00b10.003 \n\n0.843 \n\n\u00b10.003 \n\n0.827 \n\n\u00b10.003 \n\n0.624 \n\n\u00b10.005 \n\n0.630 \n\n\u00b10.006 \n\n0.590 \n\n\u00b10.005 \n\n0.815 \n\n\u00b10.001 \n\n0.815 \n\n\u00b10.001 \n\n0.804 \n\n\u00b10.002 \n\nTADW \n0.819 \n\n\u00b10.004 \n\n0.819 \n\n\u00b10.004 \n\n0.804 \n\n\u00b10.005 \n\n0.725 \n\n\u00b10.004 \n\n0.734 \n\n\u00b10.004 \n\n0.685 \n\n\u00b10.004 \n\n0.862 \n\n\u00b10.002 \n\n0.862 \n\n\u00b10.002 \n\n0.863 \n\n\u00b10.002 \n\nAANE \n0.793 \n\n\u00b10.006 \n\n0.793 \n\n\u00b10.006 \n\n0.777 \n\n\u00b10.006 \n\n0.728 \n\n\u00b10.005 \n\n0.733 \n\n\u00b10.004 \n\n0.693 \n\n\u00b10.005 \n\n0.867 \n\n\u00b10.001 \n\n0.867 \n\n\u00b10.001 \n\n0.867 \n\n\u00b10.002 \n\nASNE \n0.831 \n\n\u00b10.003 \n\n0.83 \n\n\u00b10.003 \n\n0.812 \n\n\u00b10.004 \n\n0.713 \n\n\u00b10.004 \n\n0.718 \n\n\u00b10.004 \n\n0.677 \n\n\u00b10.004 \n\n0.846 \n\n\u00b10.002 \n\n0.846 \n\n\u00b10.002 \n\n0.843 \n\n\u00b10.002 \n\nBANE \n0.807 \n\n\u00b10.005 \n\n0.807 \n\n\u00b10.005 \n\n0.787 \n\n\u00b10.005 \n\n0.707 \n\n\u00b10.003 \n\n0.713 \n\n\u00b10.003 \n\n0.67 \n\n\u00b10.004 \n\n0.823 \n\n\u00b10.002 \n\n0.823 \n\n\u00b10.002 \n\n0.822 \n\n\u00b10.002 \n\nTENE \n0.829 \n\n\u00b10.005 \n\n0.829 \n\n\u00b10.005 \n\n0.815 \n\n\u00b10.004 \n\n0.664 \n\n\u00b10.004 \n\n0.681 \n\n\u00b10.003 \n\n0.611 \n\n\u00b10.002 \n\n0.842 \n\n\u00b10.001 \n\n0.842 \n\n\u00b10.001 \n\n0.843 \n\n\u00b10.002 \n\nAE \n0.835 \n\n\u00b10.005 \n\n0.835 \n\n\u00b10.005 \n\n0.815 \n\n\u00b10.006 \n\n0.730 \n\n\u00b10.005 \n\n0.739 \n\n\u00b10.005 \n\n0.688 \n\n\u00b10.006 \n\n0.839 \n\n\u00b10.002 \n\n0.839 \n\n\u00b10.002 \n\n0.840 \n\n\u00b10.002 \n\nAE-EGO \n0.835 \n\n\u00b10.005 \n\n0.835 \n\n\u00b10.006 \n\n0.816 \n\n\u00b10.005 \n\n0.729 \n\n\u00b10.004 \n\n0.739 \n\n\u00b10.005 \n\n0.690 \n\n\u00b10.007 \n\n0.840 \n\n\u00b10.002 \n\n0.840 \n\n\u00b10.003 \n\n0.839 \n\n\u00b10.002 \n\nMUSAE \n0.848 \n\n\u00b10.004 \n\n0.848 \n\n\u00b10.004 \n\n0.832 \n\n\u00b10.005 \n\n0.737 \n\n\u00b1 0.004 \n\n0.742 \n\n\u00b10.004 \n\n0.706 \n\n\u00b10.004 \n\n0.853 \n\n\u00b10.001 \n\n0.853 \n\n\u00b10.001 \n\n0.854 \n\n\u00b10.002 \n\nMUSAE-EGO \n0.849 \n\n\u00b10.004 \n\n0.849 \n\n\u00b10.004 \n\n0.833 \n\n\u00b10.004 \n\n0.736 \n\n\u00b10.004 \n\n0.741 \n\n\u00b10.004 \n\n0.706 \n\n\u00b10.004 \n\n0.850 \n\n\u00b10.002 \n\n0.851 \n\n\u00b10.002 \n\n0.850 \n\n\u00b10.002 \n\n\nTable 7 :\n7Average test R 2 values and standard errors on the Wikipedia traffic prediction tasks. Red numbers denote the best results on each page-page network.J LINK PREDICTION RESULTS ON SOCIAL AND WEB NETWORKSDatasets \n\nMethod \nWikipedia \nChameleons \n\nWikipedia \nCrocodiles \n\nWikipedia \nSquirrels \nDeepWalk \n0.375 \n\n\u00b10.004 \n\n0.553 \n\n\u00b10.001 \n\n0.170 \n\n\u00b10.002 \n\nLINE2 \n0.381 \n\n\u00b10.003 \n\n0.586 \n\n\u00b10.001 \n\n0.232 \n\n\u00b10.002 \n\nNode2Vec \n0.414 \n\n\u00b10.003 \n\n0.574 \n\n\u00b10.001 \n\n0.174 \n\n\u00b10.002 \n\nWalklets \n0.426 \n\n\u00b10.003 \n\n0.625 \n\n\u00b10.001 \n\n0.249 \n\n\u00b10.002 \n\nTADW \n0.527 \n\n\u00b10.003 \n\n0.636 \n\n\u00b10.001 \n\n0.271 \n\n\u00b10.002 \n\nAANE \n0.598 \n\n\u00b10.007 \n\n0.732 \n\n\u00b10.002 \n\n0.287 \n\n\u00b10.002 \n\nASNE \n0.440 \n\n\u00b10.009 \n\n0.572 \n\n\u00b10.003 \n\n0.229 \n\n\u00b10.005 \n\nBANE \n0.464 \n\n\u00b10.003 \n\n0.617 \n\n\u00b10.001 \n\n0.168 \n\n\u00b10.002 \n\nTENE \n0.494 \n\n\u00b10.02 \n\n0.701 \n\n\u00b10.003 \n\n0.321 \n\n\u00b10.007 \n\nAE \n0.642 \n\n\u00b10.006 \n\n0.743 \n\n\u00b10.003 \n\n0.291 \n\n\u00b10.006 \n\nAE-EGO \n0.644 \n\n\u00b10.009 \n\n0.732 \n\n\u00b10.002 \n\n0.283 \n\n\u00b10.006 \n\nMUSAE \n0.658 \n\n\u00b10.004 \n\n0.736 \n\n\u00b10.003 \n\n0.338 \n\n\u00b10.007 \n\nMUSAE-EGO \n0.653 \n\n\u00b10.011 \n\n0.747 \n\n\u00b10.003 \n\n0.354 \n\n\u00b10.009 \n\n\nTable 8 :\n8Link prediction results evaluated by average AUC scores on the test set using attributed node embedding techniques and regularized logistic regression. We created a 100 seeded splits (80% training -20% test). Standard errors of the AUC score are included below. Red denotes the best performing embedding model on a given dataset considering both the neighbourhood based and attributed techniques. We used 4 different element-wise operators to create edge features.Datasets \n\n\n\nTable 9 :\n9Link prediction results evaluated by average AUC scores on the test set using attributed node embedding techniques and regularized logistic regression. We created a 100 seeded splits (80% training -20% test). Standard errors of the AUC score are included below. Red denotes the best performing embedding model on a given dataset considering both the neighbourhood based and neighbourhood based embedding techniques. We used 4 different element-wise operators to create edge features.Datasets \n\n\n\nWatch your step: Learning node embeddings via graph attention. Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, Alexander A Alemi, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett31Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi. Watch your step: Learning node embeddings via graph attention. In S. Bengio, H. Wallach, H. Larochelle, K. Grau- man, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 9198-9208. 2018.\n\nMixHop: Higher-order graph convolutional architectures via sparsified neighborhood mixing. Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Proceedings of the 36th International Conference on Machine Learning (ICML). the 36th International Conference on Machine Learning (ICML)Greg Ver Steeg, and Aram GalstyanSami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. MixHop: Higher-order graph convolutional architectures via sparsified neighborhood mixing. In Proceedings of the 36th International Con- ference on Machine Learning (ICML), pp. 21-29, 2019.\n\nLearning role-based graph embeddings. K Nesreen, Ryan Ahmed, John Rossi, Xiangnan Boaz Lee, Kong, Rong Theodore L Willke, Hoda Zhou, Eldardiry, arXiv:1802.02896arXiv preprintNesreen K Ahmed, Ryan Rossi, John Boaz Lee, Xiangnan Kong, Theodore L Willke, Rong Zhou, and Hoda Eldardiry. Learning role-based graph embeddings. arXiv preprint arXiv:1802.02896, 2018.\n\nGrarep: Learning graph representations with global structural information. Shaosheng Cao, Wei Lu, Qiongkai Xu, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. the 24th ACM International on Conference on Information and Knowledge ManagementShaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global structural information. In Proceedings of the 24th ACM International on Conference on Informa- tion and Knowledge Management, pp. 891-900, 2015.\n", "annotations": {"author": "[{\"end\":110,\"start\":41},{\"end\":190,\"start\":111},{\"end\":271,\"start\":191}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":49},{\"end\":121,\"start\":116},{\"end\":201,\"start\":195}]", "author_first_name": "[{\"end\":48,\"start\":41},{\"end\":115,\"start\":111},{\"end\":194,\"start\":191}]", "author_affiliation": "[{\"end\":109,\"start\":63},{\"end\":189,\"start\":143},{\"end\":270,\"start\":224}]", "title": "[{\"end\":38,\"start\":1},{\"end\":309,\"start\":272}]", "venue": null, "abstract": "[{\"end\":1142,\"start\":311}]", "bib_ref": "[{\"end\":2120,\"start\":2089},{\"end\":2156,\"start\":2125},{\"end\":2890,\"start\":2868},{\"end\":2908,\"start\":2890},{\"end\":2932,\"start\":2908},{\"end\":2953,\"start\":2932},{\"end\":2994,\"start\":2972},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3013,\"start\":2994},{\"end\":3581,\"start\":3558},{\"end\":3583,\"start\":3581},{\"end\":3718,\"start\":3695},{\"end\":3803,\"start\":3772},{\"end\":4210,\"start\":4192},{\"end\":4388,\"start\":4365},{\"end\":4412,\"start\":4388},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4525,\"start\":4498},{\"end\":4607,\"start\":4583},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4634,\"start\":4616},{\"end\":4670,\"start\":4639},{\"end\":5064,\"start\":5042},{\"end\":5247,\"start\":5228},{\"end\":5265,\"start\":5247},{\"end\":5284,\"start\":5265},{\"end\":5302,\"start\":5284},{\"end\":5320,\"start\":5302},{\"end\":5600,\"start\":5576},{\"end\":5783,\"start\":5763},{\"end\":5812,\"start\":5788},{\"end\":5953,\"start\":5934},{\"end\":6406,\"start\":6382},{\"end\":6662,\"start\":6643},{\"end\":6680,\"start\":6662},{\"end\":6699,\"start\":6680},{\"end\":6718,\"start\":6699},{\"end\":6736,\"start\":6718},{\"end\":12133,\"start\":12127},{\"end\":15925,\"start\":15908},{\"end\":18181,\"start\":18160},{\"end\":19185,\"start\":19164},{\"end\":22331,\"start\":22309},{\"end\":25767,\"start\":25743},{\"end\":28523,\"start\":28502},{\"end\":28528,\"start\":28523},{\"end\":29762,\"start\":29745},{\"end\":31636,\"start\":31619},{\"end\":35815,\"start\":35793},{\"end\":35839,\"start\":35815},{\"end\":35860,\"start\":35839},{\"end\":35881,\"start\":35860},{\"end\":36205,\"start\":36181},{\"end\":36762,\"start\":36740},{\"end\":36786,\"start\":36762},{\"end\":36807,\"start\":36786},{\"end\":38483,\"start\":38464},{\"end\":38639,\"start\":38618},{\"end\":38814,\"start\":38789},{\"end\":38883,\"start\":38862},{\"end\":39374,\"start\":39367}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39994,\"start\":39971},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40114,\"start\":39995},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40534,\"start\":40115},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40634,\"start\":40535},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40681,\"start\":40635},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40878,\"start\":40682},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40970,\"start\":40879},{\"attributes\":{\"id\":\"fig_7\"},\"end\":41242,\"start\":40971},{\"attributes\":{\"id\":\"fig_8\"},\"end\":41346,\"start\":41243},{\"attributes\":{\"id\":\"fig_9\"},\"end\":42122,\"start\":41347},{\"attributes\":{\"id\":\"fig_10\"},\"end\":42372,\"start\":42123},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42631,\"start\":42373},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43187,\"start\":42632},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48737,\"start\":43188},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49622,\"start\":48738},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49936,\"start\":49623},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":50288,\"start\":49937},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":50511,\"start\":50289},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":54235,\"start\":50512},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":57129,\"start\":54236},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":58190,\"start\":57130},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":58678,\"start\":58191},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":59185,\"start\":58679}]", "paragraph": "[{\"end\":1585,\"start\":1155},{\"end\":1613,\"start\":1587},{\"end\":1876,\"start\":1615},{\"end\":2037,\"start\":1878},{\"end\":2195,\"start\":2039},{\"end\":2462,\"start\":2197},{\"end\":2613,\"start\":2464},{\"end\":3287,\"start\":2630},{\"end\":5065,\"start\":3289},{\"end\":6737,\"start\":5067},{\"end\":6979,\"start\":6739},{\"end\":7994,\"start\":7011},{\"end\":8712,\"start\":8019},{\"end\":9021,\"start\":8900},{\"end\":9390,\"start\":9023},{\"end\":9695,\"start\":9397},{\"end\":10301,\"start\":9732},{\"end\":11197,\"start\":10303},{\"end\":11629,\"start\":11255},{\"end\":11865,\"start\":11693},{\"end\":11979,\"start\":11907},{\"end\":12114,\"start\":12016},{\"end\":12573,\"start\":12116},{\"end\":12949,\"start\":12575},{\"end\":12991,\"start\":12969},{\"end\":13649,\"start\":13008},{\"end\":13768,\"start\":13678},{\"end\":13970,\"start\":13868},{\"end\":14187,\"start\":13972},{\"end\":14306,\"start\":14286},{\"end\":14505,\"start\":14308},{\"end\":14674,\"start\":14573},{\"end\":14854,\"start\":14774},{\"end\":14907,\"start\":14901},{\"end\":15230,\"start\":15055},{\"end\":15457,\"start\":15379},{\"end\":15539,\"start\":15459},{\"end\":15633,\"start\":15577},{\"end\":15693,\"start\":15635},{\"end\":15926,\"start\":15695},{\"end\":16079,\"start\":15991},{\"end\":16363,\"start\":16081},{\"end\":16417,\"start\":16414},{\"end\":17247,\"start\":16485},{\"end\":17526,\"start\":17249},{\"end\":18377,\"start\":17554},{\"end\":18831,\"start\":18401},{\"end\":19553,\"start\":18851},{\"end\":20200,\"start\":19555},{\"end\":21045,\"start\":20234},{\"end\":21594,\"start\":21047},{\"end\":22216,\"start\":21596},{\"end\":22864,\"start\":22264},{\"end\":24165,\"start\":22866},{\"end\":25114,\"start\":24200},{\"end\":26019,\"start\":25168},{\"end\":26438,\"start\":26021},{\"end\":28122,\"start\":26454},{\"end\":28785,\"start\":28152},{\"end\":29166,\"start\":28787},{\"end\":29583,\"start\":29186},{\"end\":30422,\"start\":29682},{\"end\":30513,\"start\":30467},{\"end\":30828,\"start\":30772},{\"end\":31163,\"start\":30970},{\"end\":31283,\"start\":31165},{\"end\":31638,\"start\":31462},{\"end\":31838,\"start\":31640},{\"end\":32669,\"start\":32193},{\"end\":33392,\"start\":32704},{\"end\":34148,\"start\":33451},{\"end\":34862,\"start\":34175},{\"end\":35495,\"start\":34886},{\"end\":36046,\"start\":35558},{\"end\":36833,\"start\":36101},{\"end\":37224,\"start\":36835},{\"end\":37448,\"start\":37226},{\"end\":37812,\"start\":37450},{\"end\":38069,\"start\":37814},{\"end\":38350,\"start\":38071},{\"end\":39257,\"start\":38415},{\"end\":39615,\"start\":39259},{\"end\":39817,\"start\":39617},{\"end\":39939,\"start\":39819}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8899,\"start\":8713},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11692,\"start\":11630},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11906,\"start\":11866},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12015,\"start\":11980},{\"attributes\":{\"id\":\"formula_4\"},\"end\":13007,\"start\":12992},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13867,\"start\":13769},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14285,\"start\":14188},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14572,\"start\":14506},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14773,\"start\":14675},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14900,\"start\":14855},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15035,\"start\":14908},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15378,\"start\":15231},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15576,\"start\":15540},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15990,\"start\":15979},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16413,\"start\":16364},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16462,\"start\":16418},{\"attributes\":{\"id\":\"formula_16\"},\"end\":29681,\"start\":29584},{\"attributes\":{\"id\":\"formula_17\"},\"end\":30466,\"start\":30423},{\"attributes\":{\"id\":\"formula_18\"},\"end\":30771,\"start\":30514},{\"attributes\":{\"id\":\"formula_19\"},\"end\":30969,\"start\":30829},{\"attributes\":{\"id\":\"formula_20\"},\"end\":31461,\"start\":31284},{\"attributes\":{\"id\":\"formula_21\"},\"end\":32152,\"start\":31839}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20711,\"start\":20704},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":21030,\"start\":21023},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":24502,\"start\":24495},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":26143,\"start\":26129},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":32339,\"start\":32332},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33243,\"start\":33236},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34068,\"start\":34061},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34775,\"start\":34768},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35683,\"start\":35676},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36445,\"start\":36438},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36927,\"start\":36919},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":37639,\"start\":37632},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":37897,\"start\":37890},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":38762,\"start\":38755},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":39046,\"start\":39039},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":39240,\"start\":39232}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":2628,\"start\":2616},{\"attributes\":{\"n\":\"3\"},\"end\":7009,\"start\":6982},{\"attributes\":{\"n\":\"3.1\"},\"end\":8017,\"start\":7997},{\"end\":9395,\"start\":9393},{\"attributes\":{\"n\":\"3.2\"},\"end\":9730,\"start\":9698},{\"attributes\":{\"n\":\"4\"},\"end\":11253,\"start\":11200},{\"end\":12967,\"start\":12952},{\"attributes\":{\"n\":\"4.1\"},\"end\":13676,\"start\":13652},{\"attributes\":{\"n\":\"4.2\"},\"end\":15053,\"start\":15037},{\"end\":15978,\"start\":15929},{\"attributes\":{\"n\":\"4.3\"},\"end\":16483,\"start\":16464},{\"attributes\":{\"n\":\"5\"},\"end\":17552,\"start\":17529},{\"attributes\":{\"n\":\"5.1\"},\"end\":18399,\"start\":18380},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":18849,\"start\":18834},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":20232,\"start\":20203},{\"attributes\":{\"n\":\"5.2\"},\"end\":22262,\"start\":22219},{\"attributes\":{\"n\":\"5.3\"},\"end\":24198,\"start\":24168},{\"attributes\":{\"n\":\"5.4\"},\"end\":25166,\"start\":25117},{\"attributes\":{\"n\":\"5.5\"},\"end\":26452,\"start\":26441},{\"attributes\":{\"n\":\"6\"},\"end\":28150,\"start\":28125},{\"end\":29184,\"start\":29169},{\"end\":32191,\"start\":32154},{\"end\":32702,\"start\":32672},{\"end\":33449,\"start\":33395},{\"end\":34173,\"start\":34151},{\"end\":34884,\"start\":34865},{\"end\":35556,\"start\":35498},{\"end\":36099,\"start\":36049},{\"end\":38413,\"start\":38353},{\"end\":39970,\"start\":39942},{\"end\":40006,\"start\":39996},{\"end\":40644,\"start\":40636},{\"end\":40693,\"start\":40683},{\"end\":40890,\"start\":40880},{\"end\":41349,\"start\":41348},{\"end\":42134,\"start\":42124},{\"end\":42642,\"start\":42633},{\"end\":48748,\"start\":48739},{\"end\":49633,\"start\":49624},{\"end\":49947,\"start\":49938},{\"end\":50299,\"start\":50290},{\"end\":50522,\"start\":50513},{\"end\":54246,\"start\":54237},{\"end\":57140,\"start\":57131},{\"end\":58201,\"start\":58192},{\"end\":58689,\"start\":58680}]", "table": "[{\"end\":42631,\"start\":42416},{\"end\":43187,\"start\":42644},{\"end\":48737,\"start\":44434},{\"end\":49622,\"start\":48825},{\"end\":49936,\"start\":49699},{\"end\":50288,\"start\":50118},{\"end\":50511,\"start\":50378},{\"end\":54235,\"start\":50895},{\"end\":57129,\"start\":54939},{\"end\":58190,\"start\":57343},{\"end\":58678,\"start\":58667},{\"end\":59185,\"start\":59174}]", "figure_caption": "[{\"end\":39994,\"start\":39973},{\"end\":40114,\"start\":40008},{\"end\":40534,\"start\":40117},{\"end\":40634,\"start\":40537},{\"end\":40681,\"start\":40648},{\"end\":40878,\"start\":40695},{\"end\":40970,\"start\":40892},{\"end\":41242,\"start\":40973},{\"end\":41346,\"start\":41245},{\"end\":42122,\"start\":41350},{\"end\":42372,\"start\":42136},{\"end\":42416,\"start\":42375},{\"end\":44434,\"start\":43190},{\"end\":48825,\"start\":48750},{\"end\":49699,\"start\":49635},{\"end\":50118,\"start\":49949},{\"end\":50378,\"start\":50301},{\"end\":50895,\"start\":50524},{\"end\":54939,\"start\":54248},{\"end\":57343,\"start\":57142},{\"end\":58667,\"start\":58203},{\"end\":59174,\"start\":58691}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":1153,\"start\":1144},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":1342,\"start\":1333},{\"end\":19498,\"start\":19490},{\"end\":20021,\"start\":20013},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23628,\"start\":23620},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26767,\"start\":26759}]", "bib_author_first_name": "[{\"end\":59254,\"start\":59250},{\"end\":59274,\"start\":59269},{\"end\":59288,\"start\":59284},{\"end\":59307,\"start\":59298},{\"end\":59309,\"start\":59308},{\"end\":59851,\"start\":59847},{\"end\":59871,\"start\":59866},{\"end\":59885,\"start\":59881},{\"end\":59901,\"start\":59894},{\"end\":59923,\"start\":59915},{\"end\":59937,\"start\":59932},{\"end\":60490,\"start\":60489},{\"end\":60504,\"start\":60500},{\"end\":60516,\"start\":60512},{\"end\":60532,\"start\":60524},{\"end\":60553,\"start\":60549},{\"end\":60577,\"start\":60573},{\"end\":60896,\"start\":60887},{\"end\":60905,\"start\":60902},{\"end\":60918,\"start\":60910}]", "bib_author_last_name": "[{\"end\":59267,\"start\":59255},{\"end\":59282,\"start\":59275},{\"end\":59296,\"start\":59289},{\"end\":59315,\"start\":59310},{\"end\":59864,\"start\":59852},{\"end\":59879,\"start\":59872},{\"end\":59892,\"start\":59886},{\"end\":59913,\"start\":59902},{\"end\":59930,\"start\":59924},{\"end\":59949,\"start\":59938},{\"end\":60498,\"start\":60491},{\"end\":60510,\"start\":60505},{\"end\":60522,\"start\":60517},{\"end\":60541,\"start\":60533},{\"end\":60547,\"start\":60543},{\"end\":60571,\"start\":60554},{\"end\":60582,\"start\":60578},{\"end\":60593,\"start\":60584},{\"end\":60900,\"start\":60897},{\"end\":60908,\"start\":60906},{\"end\":60921,\"start\":60919}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52195182},\"end\":59754,\"start\":59187},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":141460877},\"end\":60449,\"start\":59756},{\"attributes\":{\"doi\":\"arXiv:1802.02896\",\"id\":\"b2\"},\"end\":60810,\"start\":60451},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":17341970},\"end\":61335,\"start\":60812}]", "bib_title": "[{\"end\":59248,\"start\":59187},{\"end\":59845,\"start\":59756},{\"end\":60885,\"start\":60812}]", "bib_author": "[{\"end\":59269,\"start\":59250},{\"end\":59284,\"start\":59269},{\"end\":59298,\"start\":59284},{\"end\":59317,\"start\":59298},{\"end\":59866,\"start\":59847},{\"end\":59881,\"start\":59866},{\"end\":59894,\"start\":59881},{\"end\":59915,\"start\":59894},{\"end\":59932,\"start\":59915},{\"end\":59951,\"start\":59932},{\"end\":60500,\"start\":60489},{\"end\":60512,\"start\":60500},{\"end\":60524,\"start\":60512},{\"end\":60543,\"start\":60524},{\"end\":60549,\"start\":60543},{\"end\":60573,\"start\":60549},{\"end\":60584,\"start\":60573},{\"end\":60595,\"start\":60584},{\"end\":60902,\"start\":60887},{\"end\":60910,\"start\":60902},{\"end\":60923,\"start\":60910}]", "bib_venue": "[{\"end\":59366,\"start\":59317},{\"end\":60026,\"start\":59951},{\"end\":60487,\"start\":60451},{\"end\":61018,\"start\":60923},{\"end\":60088,\"start\":60028},{\"end\":61100,\"start\":61020}]"}}}, "year": 2023, "month": 12, "day": 17}