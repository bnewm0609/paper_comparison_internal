{"id": 251066837, "updated": "2023-10-05 12:10:02.227", "metadata": {"title": "Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis", "authors": "[{\"first\":\"Trisha\",\"last\":\"Mittal\",\"middle\":[]},{\"first\":\"Ritwik\",\"last\":\"Sinha\",\"middle\":[]},{\"first\":\"Viswanathan\",\"last\":\"Swaminathan\",\"middle\":[]},{\"first\":\"John\",\"last\":\"Collomosse\",\"middle\":[]},{\"first\":\"Dinesh\",\"last\":\"Manocha\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "As tools for content editing mature, and artificial intelligence (AI) based algorithms for synthesizing media grow, the presence of manipulated content across online media is increasing. This phenomenon causes the spread of misinformation, creating a greater need to distinguish between ``real'' and ``manipulated'' content. To this end, we present VideoSham, a dataset consisting of 826 videos (413 real and 413 manipulated). Many of the existing deepfake datasets focus exclusively on two types of facial manipulations -- swapping with a different subject's face or altering the existing face. VideoSham, on the other hand, contains more diverse, context-rich, and human-centric, high-resolution videos manipulated using a combination of 6 different spatial and temporal attacks. Our analysis shows that state-of-the-art manipulation detection algorithms only work for a few specific attacks and do not scale well on VideoSham. We performed a user study on Amazon Mechanical Turk with 1200 participants to understand if they can differentiate between the real and manipulated videos in VideoSham. Finally, we dig deeper into the strengths and weaknesses of performances by humans and SOTA-algorithms to identify gaps that need to be filled with better AI algorithms. We present the dataset at https://github.com/adobe-research/VideoSham-dataset.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.13064", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wacv/MittalSSCM23", "doi": "10.1109/wacvw58289.2023.00071"}}, "content": {"source": {"pdf_hash": "e3453b25f768a9e2a5c42a43cdd07041510c7f05", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2207.13064v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "80aa3619b34a4224a20f882121929a321b5a9133", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e3453b25f768a9e2a5c42a43cdd07041510c7f05.txt", "contents": "\nVideo Manipulations Beyond Faces: A Dataset with Human-Machine Analysis\n\n\nTrisha Mittal trisha@umd.edu \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nRitwik Sinha risinha@adobe.com \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nAdobe Research \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nSan Jose \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nViswanathan Swaminathan \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nAdobe Research \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nSan Jose \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nJohn Collomosse collomos@adobe.com \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nAdobe Research \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nSan Jose \nDinesh Manocha\nUniversity of Maryland\nUniversity of Maryland\n\n\nVideo Manipulations Beyond Faces: A Dataset with Human-Machine Analysis\n\nAs tools for content editing mature, and artificial intelligence (AI) based algorithms for synthesizing media grow, the presence of manipulated content across online media is increasing. This phenomenon causes the spread of misinformation, creating a greater need to distinguish between \"real\" and \"manipulated\" content. To this end, we present VIDEOSHAM, a dataset consisting of 826 videos (413 real and 413 manipulated). Many of the existing deepfake datasets focus exclusively on two types of facial manipulations-swapping with a different subject's face or altering the existing face. VIDEOSHAM, on the other hand, contains more diverse, context-rich, and human-centric, high-resolution videos manipulated using a combination of 6 different spatial and temporal attacks. Our analysis shows that state-of-the-art manipulation detection algorithms only work for a few specific attacks and do not scale well on VIDEOSHAM. We performed a user study on Amazon Mechanical Turk with 1200 participants to understand if they can differentiate between the real and manipulated videos in VIDEOSHAM. Finally, we dig deeper into the strengths and weaknesses of performances by humans and SOTAalgorithms to identify gaps that need to be filled with better AI algorithms. We present the dataset here 1 . * Work done as an intern at Adobe Research. 1 VIDEOSHAM dataset link.\n\nIntroduction\n\nThe proliferation of accessible video editing software and artificial intelligence (AI) tools has led to an increase in manipulated video content [23,20]. While digital manipulation is commonplace in the creative process, in some cases video manipulation has a malicious intent. Social media often amplifies such false information through the circulation of manipulated videos [7,14]. A recent survey by Pew Research Center showed that exposure to such false information is of widespread concern [54]. Therefore, there has been a significant increase in cases of misinformation, fraud and cybercrimes in the last decade. Such video manipulations pose a great threat to politics and can manipulate elections [62,5], alter political narratives, weaken the public's trust in a country's leadership, and an increase hatred among various social groups. Another common occurrence is corporate frauds and scams where people use altered audio to impersonate other people to extort cash and other resources. Lastly, many video manipulations often result in numerous cybercrimes [18,55,9]. To further illustrate our motivations in this work, we depict such instances of video manipulations in Figure 1.\n\nThis leads to an important question-how do we detect manipulated content? The current arsenal of techniques involve the use of AI which in turn requires tremendous amounts of data. In the past decade alone, there has been a surge in the number of benchmark deepfake datasets [27,49,13] which manipulate the facial features of subjects in images and videos. We summarize recent deepfake datasets in Table 1.\n\nBut facial manipulations represent only a fraction of all manipulated content circulated on social media. For example, modifications also include changing the background context (Figure 1b), text and audio ( Figure 1c) in media, aesthetic edits, adding/removing entities (Figure 1a), and temporal edits (Figure 1d). These manipulations can be performed in a matter of clicks due to the availability of state of the art video editing tools like Adobe AfterEffects TM , Adobe Lightroom TM , Filmora, GIMP, and many others. To our knowledge, no benchmark video dataset exists that extends beyond deepfake-only facial manipulations to include the vast range of manipulations described above.\n\n(a1) The original photo, from Getty Images shows an armed man parked in front of a car.\n\n(b1) This is an original clip of a presidential candidate addressing public in the US state, Minnesota.\n\n(c1) An original image shows three missiles being launched by Iran's government.\n\n(a2) The photo above was altered by digitally placing the armed man in front of a peaceful protest, insinuating violence.\n\n(b2) The clip above is altered by changing the location and the signs on the podium to a different US state, Florida.\n\n(c2) In an altered image released on Iran's Revolutionary Guards website, claimed that 4 missiles were launched simultaneously.  [42], and (c) [44] are examples of videos on social media spatially manipulated with the intent to mislead audiences.  Transferring a face from source to target image/video FaceShifter [31], FSGAN [43], DeepFaceLab [45] 4\n\nFace Re-enactment (FR) Neural Textures [57], First-Order-Motion [53] Using facial movements and expression deformations of Face2Face [58], IcFace [59], FSGAN [43], a face to guide the motions and deformations of another face 5\n\nAudio-driven FR (AFR) Wav2Lip [46], APB2FACE [69], ATFHP [68] Reenacting faces driven by a given audio signal to sync with lip movement  \n\n\nMain Contributions\n\nWe release a new manipulated high-resolution video dataset called VIDEOSHAM ( Figure 6). VIDEOSHAM offers the following benefits over existing manipulated video datasets:\n\n1. Beyond Faces (Deepfakes):\n\nThe videos in VIDEOSHAM are manipulated using six spatial and temporal attacks (See Table 2) manipulating videos at the scene level targeting, not just faces, but also the background context, text and audio, aesthetic edits, adding/removing entities, and temporal edits (See Figure 2).\n\n2. Beyond Images: Although there exist image manipulation datasets that go beyond faces, they cannot be used to detect video manipulations, which require dedicated video datasets. The latter, however, are hard to create due to the manual labor involved. In this work, we go beyond images to release the first video manipulation dataset containing beyond-face manipulations.\n\nVIDEOSHAM consists of 413 real-world videos and their corresponding manipulated versions (total 826 videos). The videos have diverse scene backgrounds, are context-rich, and contain up to 9 subjects on average. VIDEOSHAM is the largest dataset containing manipulated videos generated by professional video editors with varied attacks. A user study conducted on Amazon Mechanical Turk (AMT) to understand the kind of attack methods that mislead humans the most. In addition, we analyze the performance of existing state of the art deepfake detection algorithms and video forensics algorithms on VIDEOSHAM. We find that these techniques are less than 50% effective in distinguishing between a real and a manipulated video.\n\nWe elaborate more about VIDEOSHAM in Section 3. In Section 4 we present our findings from the user study and evaluation of detection models. And, finally in Section 5, we discuss some promising ideas to help these attacks.\n\n\nRelated Work\n\nIn this section, we discuss previous works in detection of manipulated and deceptive media content. To begin with, we first discuss the video manipulation techniques used to create such fake videos in Section 2.1. Then in Section 2.2, we summarize various datasets and benchmarks for video manipulations. We also survey different techniques used for detecting deepfake videos in Section 2.3 and generic video forensic methods in Section 2.4.\n\n\nVideo Manipulation Techniques/Attacks\n\nManipulation techniques, or attacks, are broadly categorized as spatial [6], temporal [23], and geometric [23]  in the literature (see Table 2). Basic examples of spatial attacks include copy-move and image/video splicing which correspond to spatially or temporally shifting an object to a different location in the same video or a different video, respectively. Retouching, another common attack, involves aesthetic edits like adjusting brightness, contrast, and other parameters of digital content. More recently, people have used AI to alter facial features to create deepfake videos. AI-based techniques are comprised of two major attack approaches, Face Swapping [43,45] and Face Reenactment [58,57,53]. Face Swapping switches the subject's face with the face of another person and Face Reenactment alters the subject's facial expressions. Temporal attacks involve swapping, duplicating, inserting, and deleting frames of video, giving the impression that the video has been sped up or slowed down. Finally, geometric attacks include operations like cropping and rotations.\n\n\nVideo Manipulation Datasets\n\nCreating benchmarks of video manipulations is a challenging task as this may require per-frame manipulations. Some of the datasets (like Khelifi et al. [23], MTVFD [4], Liao et al. [34], Su et al. [56], Media Forensics Challenge [15]) are very small in volume containing 7 \u2212 200 videos each, these datasets are also not publicly available. Most of these videos have 0 or 1 subjects present in the frame with very little background context. More recently, AI-synthesized attacks like face swapping, face re-enactment, and audio-driven face re-enactment have led to the creation of datasets like UADFV [66], FaceForen-sics++ [49], DeeperForensics1.0 [21], WildDeepFake [72]. Because these datasets are generated using learning methods; some of these datasets have upto 100k videos. However all of these datasets have strictly 1 subject per video with the face being predominant part of the frame with no background context at all. Many datasets are missing audio except DFDC [13], DF-TIMIT [27], KoDF [30], FakeAVCeleb [22], ForgeryNet [20] and SR-DF [60].\n\n\nDeepfake Detection Methods\n\nThe goal of the deepfake detection approaches is to algorithmically distinguish fake videos from real videos. A large portion of these methods are focused on detecting visual artifacts especially on the finer regions of the face (like eyes, mouth and teeth [39]). Some approaches specifically focus on abnormalities like inconsistent head pose orientations [67], asynchronous lip movement and speech [17] and unnatural eye blinking [32]. Prior work have also observed and exploited the fact that temporal coherence is not enforced effectively in the synthesis process of deepfakes [51,16] and exploit this in detection methods. More recently, interesting affective computing approaches that focus on correlated emotion signals from audio-visual cues [40,3], and detecting signals like heart rate and breathing rate [47] from the videos have also been proposed. However, it is clear that due to the nature of the datasets (single-person, face-centered videos), these approaches focus only on facial cues and audio cues.\n\n\nVideo Forensic Methods\n\nDevelopments in video forensics literature focus on two specific attacks; Copy-Move and Splicing (Row 1 in Table 2) and Temporal attacks (Row 6 in Table 2). Most conventional copy-move forgery detection methods mainly consist of three components [12]: (1) feature extraction, (2) matching, and (3) post-processing. A variety of features have been explored, e.g., DCT (Discrete Cosine Transform) [38], DWT (Discrete Wavelet Transform) and KPCA (Kernel Principal Component Analysis) [8], Zernike moments [50]. Consequently, some end-to-end deep learning based copy-move forgery detection methods were proposed [63,64,32]. However these efforts are limited to images. Another interesting development, still in naive stages is deep learning methods to detect inpainting in videos [71]. Some of the methods in detecting temporal attacks (also called as intra-frame manipulations) use the consistency of velocity field [65] and optical flow [61]. These methods can recognize frame insertion and frame deletion attacks. Similarly, Zhao et al. [70] use inter-frame similarity analysis to detect frame duplications in the videos. Finally, Long et al. [37] propose a coarse-to-fine framework based on deep Convolutional Neural Networks (CNN) to detect potential frame duplications.\n\n\nOur Dataset-VIDEOSHAM\n\nIn this section, we present details on the dataset creation process (Section 3.1) and discuss some of the salient features and characteristics of VIDEOSHAM (Section ??).\n\n\nCreation and Annotation Process\n\n\nSource Videos\n\nWe have a total of 836 videos comprising of 413 original videos and 413 manipulated versions, each corresponding to one of the original videos. We obtain our source videos from an online video website (vimeo 2 ) and only include videos attributed with a CC-BY (Creative Commons) license. In addition, we avoid videos with brands, children, objectionable content, TV show/movie clips and videos with copyrighted music. We trim these original videos to a specific length (upto 5\u221230 seconds) before we perform any manipulation attack.\n\n\nManipulation Attacks\n\nWe employ a total of 6 manipulation attacks for creating our dataset. As per prior literature, we also categorize these attacks into spatial and temporal attacks 3 . We visually show the distribution of the attacks in Figure 3a (attacks outlined in blue are spatial attacks, outlined in pink are temporal attacks). We describe each of the attack below.\n\n\u2212 ATTACK 1 (Adding an entity/subject): In this attack we select an entity or a subject from some other sources and place them in the current video. This attack is somewhat similar to copy-move attack.  This attack is specifically to render the video temporally inconsistent. We choose to perform one of these manipulations, randomly duplicating frames, removing or dropping frames in the video. This also includes slowing down a video.\n\n\u2212 ATTACK 6 (Audio Replaced): Audio modality is a very important aspect for videos. To manipulate this, we replace the existing audio with some other audio.\n\nWe visually depict the 4 spatial attacks (ATTACK 1, AT-TACK 2, ATTACK 3, and ATTACK 4) in Figure 6.\n\n\nManipulated Videos\n\nWe worked with 3 professional video editors hired on Upwork 4 . The editors were shortlisted based on their experience and were well-versed with Adobe AfterEffects TM , the software used for creating these edits. Each editor was assigned tasks, i.e. source videos, start and end timestamp to be edited and a one-line description of the manipulation to be performed. We provide all videos and the attacks performed for every video. Dataset Analysis: In Figure 3a, we present the distribution of attacks for the 413 videos, each lasting 1 \u2212 31 seconds. The average length of videos in our dataset is around 8 seconds long. We also run an object detection model 5 4 www.upwork.com. 5 https://github.com/roboflow-ai.\n\nto count the number of people/agents in every video (Figure 3b). More than 80% of the videos in our dataset contains at least one subject.\n\n\nExperiments and Results\n\nWe elaborate on three experiments we perform to highlight the importance, novelty and usecase of VIDEOSHAM.\n\nTo begin with, we present the analysis of how well humans fair in detecting these attacks in Section 4.1, followed by analysis of the performance of state-of-the-art deepfake detection methods and video forensic techniques in Section 4.2. Finally in Section 4.3, we present some ideas and preliminary results for using interdisciplinary ideas for detecting such attacks.\n\n\nExpt 1: How Well Do Humans Perform?\n\nSetup: We first shortlist 60 videos from VIDEOSHAM. Out of these, 30 videos are real and the remaining 30 are manipulated (5 videos per attack). We recruit human participants from Amazon Mechanical Turk (AMT) and show each video to 20 participants. The participants are requested to watch the full video; followed by two questions. In the first question, the participants are asked to respond to the following prompt in either a yes or no -\"Do you believe this video has been manipulated/edited to misrepresent facts?\". And, in the second question we ask them to explain in a sentence what they felt was manipulated with the following prompt-\"If you answered YES above, what region or aspect of this video, do you believe is manipulated.\". Note that participants are not informed whether videos are manipulated or not. They are also not informed about the set of attacks. We show the setup in Figure 4 which was used to collect a total of 1200 responses from AMT participants.\n\n\nStudy Analysis:\n\nWe summarize the responses of the user study in Table 3. Both the real and manipulated videos receive 600 responses each. We observe that out of the 600 responses (corresponding to the 30 real videos), 342, i.e., 57% were correctly identified as real. Similarly, out of 600 responses for the manipulated videos, 389 were incorrectly identified as real, i.e., 35.2% of these responses correctly identified manipulated videos. Analyzing the responses by the type of attack, we observe that human participants are able to identify 45% of the videos manipulated using ATTACK 6. For the other attack types, the proportion of manipulated videos labeled as 'fake' ranges from 13-31%. Furthermore, we notice that human participants are able to more successfully identify manipulated videos that are modified using temporal attacks (ATTACK 5 and ATTACK 6) than spatial attacks (ATTACK 1\u2212 ATTACK 4). Moreover, we also received some number of responses from participants explaining their rationale behind reporting a manipulated video. From the responses received, there is no clear evidence that suggests that participants are able to identify the manipulated region/kind in case of spatial edits. But, they were somewhat able to correctly identify the manipulated edit in case of temporal attacks. This would imply that a subset of our selection of attacks are indiscernible to the human eye.\n\nStatistical Tests: Next we consider statistical tests to see if humans are able to tell a real video from a manipulated video. We consider the following quantities for this test, define p 1 = P (declaring video real|real video). Also, let p 2 = P (declaring video real|manipulated video). If humans are able to tell real videos apart from manipulated videos, we expect p 1 to be larger that p 2 . Hence, we test the one-sided statistical hypothesis:\nH 0 : p 1 = p 2 against H 1 : p 1 > p 2 .\nWe test this hypothesis with the test statistic (p 1 \u2212 p 2 ) 6 . In Table 3 we present the difference of proportions as well as the one-sided p\u2212value of the test for each attack type. The first thing to note is that when combining across all attack types (last row), we see that even though p 1 is slightly bigger than p 2 , this difference is not statistically significant (p\u2212value of 0.177). This suggests that our edits are not discernible to human evaluators. When we break it down by attack type, we observe that only for ATTACK 6 (audio replacement), humans are more likely to declare such edits as manipulations (p\u2212value < 0.001). For ATTACK 4 (text replaced or added), there is weak statistical evidence of humans detecting this manipulation (p\u2212value of 0.097). For all other attacks, there is no statistical evidence that humans can tell when a video has been manipulated using that strategy. It is particularly telling that when an entity/subject is 6 Given our sample size, we have a 86% statistical power of detecting a difference if the true values are p 1 = 0.75 and p 2 = 0.74.   added or removed (ATTACKs 1 and 2), more of our human subjects declare such manipulated videos as real than they declare unedited videos. This shows how modern editing tools can be used to manipulate videos in a way that humans have no way of telling such edits just by looking at the video. This observation establishes the need to build high quality video manipulation detection algorithms that can label manipulated videos at scale.\n\n\nExpt 2: How Well Do Machines Perform?\n\nTo answer this question better, we evaluate state-of-theart deepfake detection methods and video forensics techniques on VIDEOSHAM. Deepfake Detection Methods: We evaluate Li et al. [32], XceptionNet [49] and Mittal et al. [40] on VIDEOSHAM. Deepfake videos generated using data-driven methods can only synthesize face images of a fixed size, and they must undergo an affine warping to match the configuration of the source's face. Due to resolution inconsistencies between warped face and background context, there are various artifacts on the synthesized faces. Li et al. [32] detects such artifacts by comparing the generated face areas and their surrounding regions with a dedicated Convolutional Neural Network (CNN) model. On the other hand, Xception-Net [49] is a transfer learning model which is also a CNN architecture, which was originally trained for the classical object detection task and later finetuned for deepfake detection on FaceForensics++ dataset. Finally, Mittal et al. propose an approach that simultaneously exploits the audio (speech) and video (face) modalities and also the perceived emotion features extracted from both the modalities to detect any falsification or alteration in the input video. They use the correlation between the modalities to detect a fake video.  For some preliminary analysis, we explore two ideas, gaze and affect of all agents involved. We observe that these two ideas in itself can effectively detect manipulations of the kind, ATTACK 1 and ATTACK 2.\n\n\nGroundTruth # videos Reported\n\nVideo Forensics Techniques: We evaluate Long et al. [37] and Liu et al. [36] on VIDEOSHAM. Both of these methods are state-of-the-art methods in video forensics literature. While, Long et al. [37] is specifically for detecting cases of frame duplications in a video, Liu et al. [36] specifically focus on detecting copy-move attacks. For all the methods, we use pretrained models and report the results when evaluated on VIDEOSHAMin Table 4. Study Analysis: All the 5 shortlisted methods are less then 50% accurate on VIDEOSHAM. This is understandable, as all the deepfake methods (Li et al. [32], MesoNet [2], and Mittal et al. [40]) are trained specifically to look for manipulations in faces. Moreover, these method are not used to inferencing on videos with more or less than 1 person in the frame and with so much context information. Hence, we observe that these methods are only inferring based on artifacts caught near the face regions in the (Column 1) In the first column, we remove the main subject from the foreground. We identify this as a manipulated image using a gaze tracking algorithm by noting that there is no object at the location of the crowds gaze direction. (Column 2)Here, we manipulate an image by inserting the man in black shirt. We use emotion recognition techniques to infer that this false subject has an affective state that is not in tune to those of the other players.\n\nVIDEOSHAM videos. We also observe that, Mittal et al. specifically are able to detect some of the temporal manipulations well; which is because the method is trained to look for correlation between audio and visual modalities. Similarly, even the video forensics techniques are specifically performing well on attacks that they have been trained for, i.e. ATTACK 5 for Long et al. and ATTACK 1 and AT-TACK 2 for Liu et al. [35]. ATTACK 3 (color change) and ATTACK 4 (text replacement) tend to remain hard to be detected by most of these methods.\n\n\nExpt 3: Beyond DeepFake Detection and Video Forensic Techniques\n\nOne can observe from the experiments in the previous section, that all the methods are largely dependent on the visual artifacts. However, given the diversity of attacks used to manipulate videos, we hypothesize the use of inter-agent and multimodal analysis models for detecting such manipulations. We show preliminary results in Figure 5. Strategy 1 (Gaze): To begin with, we believe that tracking gaze of subjects can be useful for detection experiments. Gaze following is a task in computer vision to identify objects and regions that the subject of interest is focusing on. The idea behind this strategy is to identify manipulated images by using gaze following to locate \"absent\" targets and/or \"out-of-context\" subjects in the video. To perform some preliminary analysis we deploy GazeFollow [48]. More specifically, for each frame, we begin by obtain the spatial coordinates of the subject's head's bound-ing box and pass this information as input to the gaze tracking algorithm, GazeFollow [48], which outputs the location of the subject's gaze. The final step in this strategy is to run an object detector to obtain a confidence score c g corresponding to an object present at the gaze location. A low confidence score indicates a manipulated frame. Strategy 2 (Affect): In this strategy we propose the use of affective cues. When we track and look for affective disparities in affective state of different subjects. Prior works in psychology [26] and empirical works [41] that subjects in social settings often share affective states. We use facial expressions, body postures and scene understanding to perceive the affective states of all subjects. We use the model EmotiCon [41] trained on EMOTIC dataset [29] to perceive these affective states and obtain an affective confidence score c a . By empirically assigning a threshold, \u03c4 on the two confidence scores, we flag a video as manipulated. We observe that these two techniques help detect ATTACK 1 and ATTACK 2 significantly well. We add quantitative results for the same in Table 5. We show two qualitative results of these ideas in Figure 5.\n\nExperiment 3 shows that, in addition to human assessment, specialized deepfake detection techniques, and video forensics, other approaches that are not intended for identifying manipulated videos can be used.\n\n\nConclusion and Future Directions\n\nOur goal with the expt 1 (Section 4.1) and expt 2 (Section 4.2) was to understand how well humans can detect some of the manipulations that occur today circulated on social media. We also wanted to understand if the developments in the deepfake detection and video forensic literature match up to these manipulation attacks. Finally, through expt 3 (Section 4.3) we want to propagate the idea of using ideas beyond detection of visual artifacts for scalable models for video manipulation detection.\n\nWe conclude from expt 1 (Section 4.1) and expt 2 (Section 4.2) that both humans and machines (5 methods shortlisted) struggle to detect these manipulations successfully. We believe that these are attacks of concern, as they are going undetected even by human participants. Moreover, we emphasize that these manipulations play a big role in many real-world video manipulations (Figure 1).\n\nMore generally, we believe that computer vision algorithms perform almost comparable to humans in most of these ATTACKS. However, most methods are very attackspecific and do not generalize well to other attacks. Mostly every deepfake detection method fails to handle videos with more than 1 subject and hence have a very limited scope. Also, importantly most of the deepfake detection methods require huge amounts of training samples; and this is not a realistic assumption. It is important to build methods which can be less computationally intensive and at the same time are also able to generalize well. Similarly, methods in video forensics also are only able to handle very specific attacks. These are less dependent on data, but computationally expensive as they are more or less, inference based methods.\n\nWe believe following are some knowledge gaps and research agendas that can help the society combat the increasing problem of misinformation, frauds and cybercrimes occurring due to manipulated media content shared online.\n\n1. There is a need to build detection models focused on more diverse attacks or video manipulations. Through VIDEOSHAM, we attempted to include some of the attacks that have not been studied before owing to a lack of a dataset. We hope this dataset can be a step towards achieving better detection models for all the 6 attacks.\n\n2. Moreover it is important to increase the scope of detection ideas being used currently for detecting manipulations. Current methods are extremely focused on visual perception. Our goal through experiment 3 was to show through very preliminary analysis that ideas based on inter-agent dynamics and multimodal cues can be a promising literature source. Another promising idea, is to include domain knowledge in detecting manipulations; as humans we have some contextual information which the detection models severely suffer from.\n\n3. Largely all existing methods require a significant amount of training data to train the models. But, with newer manipulations and attacks on videos, it will become impossible to keep up with detection models for the same. We need to reduce the dependence on training data build detection models that are as generalizable as possible to potential attacks.\n\n\nEthical Considerations\n\nWe note that our dataset sources videos from an online video website that are attributed with a CC-BY license, and we do not retain any metadata corresponding to the creators of the videos. In addition, we do not collect any personal information of the human participants in the subsequent user study conducted on AMT. We expect that our dataset is an effort towards mitigating and fighting against malicious manipulations of online digital content.\n\nFigure 1 :\n1Spatial manipulations: (a) [10], (b)\n\nFigure 2 :\n2VIDEOSHAM: (top) VIDEOSHAM consists of diverse, context-rich, and human-centric manipulated videos by professional video editors via 6 spatial and temporal attacks (e.g. jersey color change and person removal). (bottom) In contrast, deepfake datasets (DF-TIMIT and DFDC) only consist of facial manipulations individual subjects from a close-up angle.\n\n(a )\n)Attack distribution: Distribution of videos that are attacked with different manipulation techniques. Attacks 1 \u2212 4 are spatial attacks, and Attacks 5 \u2212 6 are temporal attacks. (b) Density distribution: Distribution of videos according to number of persons present in each video. This is considerably high w.r.t. the existing datasets. (c) Duration distribution: Distribution of videos according to duration or length of each video (in seconds). The average length of our videos is 8 seconds.\n\nFigure 3 :\n3Dataset statistics: We visually present various statistics for VIDEOSHAMfor better insights. \u2212 ATTACK 4 (Text Replaced/Added): We perform edits like adding some text in the video or removing or replacing already existing text in the video. \u2212 ATTACK 5 (Frames Duplication / Removal/ Dropping):\n\nFigure 4 :\n4User Study Setup: We present the Amazon Mechanical Turk setup used (Section 4.1).\n\nFigure 5 :\n5Inter-Agent Dynamics and Multimodal Ideas for Detecting Manipulations: We show the output of the automated techniques used to identify manipulated videos in VIDEOSHAM.\n\nTable 1 :\n1Characteristics of Video Manipulation Datasets: We compare VIDEOSHAM with state-of-the-art video manipulation datasets.S.No. Attack \nMethod/Software \nDescription \n\nSpatial \n\n1 \nCopy-Move and Splicing Adobe Photoshop TM , AfterEffects TM \nSelect and copy region within same video and paste \nthis somewhere else within the same video or different video \n2 \nRetouching/Lighting \nAdobe Lightroom TM \nBrightness increase/decrease, Contrast Increase/Decrease, Median Filter \n\n3 \nFace Swapping (FS) \nFakeApp, FaceSwap [28] \n\n\nTable 2 :\n2Attacks: We summarize the various attacks that have explored in prior literature for manipulating images and videos.\n\n\n(A) 1200 responses (20 participants \u00d7 (30 real + 30 manipulated videos))GT \n#Resp Rep Rep \np1 \np2 \np1 \u2212 p2 p\u2212 value \nCI(l) \nCI(u) \n(Total) Real Fake \nReal \n600 \n454 \n146 0.757 \n0 \n\u2212 \n\u2212 \n\u2212 \n\nManipulated \nAttack \n\n1 100 \n87 \n13 \n0.757 0.87 \n\u22120.113 \n0.991 \n\u22120.182 \n1 \n2 100 \n79 \n21 \n0.757 0.79 \n\u22120.033 \n0.725 \n\u22120.112 \n1 \n3 100 \n74 \n26 \n0.757 0.74 \n0.016 \n0.408 \n\u22120.066 \n1 \n4 100 \n69 \n31 \n0.757 0.69 \n0.066 \n0.097 \n\u22120.020 \n1 \n5 100 \n75 \n25 \n0.757 0.75 \n0.006 \n0.493 \n\u22120.076 \n1 \n6 100 \n55 \n45 \n0.757 0.55 \n0.207 \n< 0.001 \n0.114 \n1 \n\n600 \n439 \n161 0.757 0.732 \n0.0250 \n0.177 \n\u22120.018 \n1 \n\n\n\nTable 3 :\n3HumanPerformance: We observe that participants \nare unable to detect ATTACK 3 (26%) and ATTACK 4 (31%). \nVideos manipulated using ATTACK 5 are relatively easier to de-\ntect (75%). \n\nPredicted \n\nGT \n#Vid. \n\nDeepfake Detection Methods \nVideo Forensics Techniques \n\nLi et al [32] MesoNet [2] Mittal et al. [40] Long et al. [37] Liu et al. [36] \n\nPredicted \nPredicted \nPredicted \nPredicted \nPredicted \n\nReal Fake Real Fake Real \nFake \nReal \nFake \nReal Fake \nReal \n413 \n188 \n225 \n167 \n246 \n238 \n175 \n219 \n194 \n234 \n179 \n\nManipulated \nAttack \n\n1 \n97 \n76 \n21 \n93 \n4 \n92 \n5 \n86 \n11 \n68 \n29 \n2 \n97 \n63 \n34 \n84 \n13 \n66 \n31 \n84 \n13 \n46 \n51 \n3 \n54 \n35 \n19 \n37 \n17 \n49 \n5 \n50 \n4 \n38 \n16 \n4 \n45 \n32 \n13 \n34 \n11 \n42 \n3 \n39 \n6 \n34 \n11 \n5 \n73 \n70 \n3 \n67 \n6 \n54 \n19 \n25 \n48 \n68 \n5 \n6 \n47 \n45 \n2 \n44 \n3 \n31 \n16 \n38 \n9 \n41 \n6 \n\n413 \n321 \n92 \n359 \n54 \n334 \n79 \n322 \n91 \n295 \n118 \n\n\n\nTable 4 :\n4Machine Performance: We evaluate 3 state-of-the-art deepfake detection methods and 2 video forensics techniques on VIDEOSHAM. It is apparent that these algorithms do not perform well on VIDEOSHAM, speaking to its complexity and diversity.\n\nTable 5 :\n5Quantitative Results (Expt 3):\n\u2212 ATTACK 2 (Removing an entity/subject): In this attack, we basically select an entity or a subject in the video and remove it from all the frames and fill in the gap with background settings. To do this, we used content-aware fill in Adobe AfterEffects TM and some deep learning methods for generating masks[19] and performing video inpainting[24,25].\u2212 ATTACK 3 (Background/Color Change): We focus on a particular aspect of the video, and change the background of the video, or color of a small entity in the video.2 www.vimeo.com.3 We do not use geometric attacks, as they have been shown to be easily detected.\nAcknowledgementsThis research is partially supported by ARO Grant W911NF2110026.A. VIDEOSHAM: Qualitative ExamplesWe present some qualitative examples from our dataset here. We show one example from each of the 4 spatial attacks in this picture.Figure 6: VIDEOSHAM Examples: We present a series of frames, both for real and manipulated videos for the 4 spatial attacks. In ATTACK 1, we add ducks in the water behind the person talking in the microphone. In ATTACK 2, we remove the man in the black shirt to the right corner. In ATTACK 3, we change the color of the walls to yellow. And, finally in ATTACK 4, we alter the name of the person talking. We were not able to add examples of the 2 temporal attacks (ATTACK 5, ATTACK 6) here.\nContributing data to deepfake detection research. Contributing data to deepfake detection research, Sep 2019.\n\nMesonet: a compact facial video forgery detection network. Darius Afchar, Vincent Nozick, Junichi Yamagishi, Isao Echizen, 2018 IEEE International Workshop on Information Forensics and Security (WIFS). IEEEDarius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. Mesonet: a compact facial video forgery detection network. In 2018 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1-7. IEEE, 2018.\n\nDetecting deep-fake videos from appearance and behavior. Shruti Agarwal, Hany Farid, Tarek El-Gaaly, Ser-Nam Lim, 2020 IEEE International Workshop on Information Forensics and Security (WIFS). IEEEShruti Agarwal, Hany Farid, Tarek El-Gaaly, and Ser-Nam Lim. De- tecting deep-fake videos from appearance and behavior. In 2020 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1-6. IEEE, 2020.\n\nDevelopment of a video tampering dataset for forensic investigation. Forensic science international. Omar Ismael Al-Sanjary, Ahmed Abdullah Ahmed, and Ghazali Sulong266Omar Ismael Al-Sanjary, Ahmed Abdullah Ahmed, and Ghazali Su- long. Development of a video tampering dataset for forensic investi- gation. Forensic science international, 266:565-572, 2016.\n\nEvaluating the fake news problem at the scale of the information ecosystem. Jennifer Allen, Baird Howland, Markus Mobius, David Rothschild, Duncan J Watts, Science Advances. 6143539Jennifer Allen, Baird Howland, Markus Mobius, David Rothschild, and Duncan J Watts. Evaluating the fake news problem at the scale of the information ecosystem. Science Advances, 6(14):eaay3539, 2020.\n\nA sift-based forensic method for copy-move attack detection and transformation recovery. Irene Amerini, Lamberto Ballan, Roberto Caldelli, Alberto Del Bimbo, Giuseppe Serra, IEEE transactions on information forensics and security. 63Irene Amerini, Lamberto Ballan, Roberto Caldelli, Alberto Del Bimbo, and Giuseppe Serra. A sift-based forensic method for copy-move attack detection and transformation recovery. IEEE transactions on information forensics and security, 6(3):1099-1110, 2011.\n\nGetting acquainted with social networks and apps: combating fake news on social media. Katie Anderson, Library Hi Tech News. 35Katie Anderson. Getting acquainted with social networks and apps: combating fake news on social media. Library Hi Tech News, 35, 07 2018.\n\nExploring duplicated regions in natural images. M Bashar, Noda, K Ohnishi, Mori, IEEE Transactions on Image Processing. M Bashar, K Noda, N Ohnishi, and K Mori. Exploring duplicated regions in natural images. IEEE Transactions on Image Processing, 2010.\n\nFake news and deepfakes: A dangerous threat for 21st century information security. Johnny Botha, Heloise Pieterse, ICCWS 2020 15th International Conference on Cyber Warfare and Security. Academic Conferences and publishing limited. 57Johnny Botha and Heloise Pieterse. Fake news and deepfakes: A dangerous threat for 21st century information security. In ICCWS 2020 15th International Conference on Cyber Warfare and Security. Academic Conferences and publishing limited, page 57, 2020.\n\nFox news runs digitally altered images in coverage of seattle's protests, capitol hill autonomous zone -the seattle times. Link. Jim Brunner, Jim Brunner. Fox news runs digitally altered images in coverage of seattle's protests, capitol hill autonomous zone -the seattle times. Link, June 2020.\n\nVoxceleb2: Deep speaker recognition. Arsha Joon Son Chung, Andrew Nagrani, Zisserman, arXiv:1806.05622arXiv preprintJoon Son Chung, Arsha Nagrani, and Andrew Zisserman. Voxceleb2: Deep speaker recognition. arXiv preprint arXiv:1806.05622, 2018.\n\nEfficient dense-field copy-move forgery detection. Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva, IEEE Transactions on Information Forensics and Security. 1011Davide Cozzolino, Giovanni Poggi, and Luisa Verdoliva. Efficient dense-field copy-move forgery detection. IEEE Transactions on In- formation Forensics and Security, 10(11):2284-2297, 2015.\n\nNicole Baram, and Cristian Canton Ferrer. The deepfake detection challenge (dfdc) preview dataset. Brian Dolhansky, Russ Howes, Ben Pflaum, arXiv:1910.08854arXiv preprintBrian Dolhansky, Russ Howes, Ben Pflaum, Nicole Baram, and Cris- tian Canton Ferrer. The deepfake detection challenge (dfdc) preview dataset. arXiv preprint arXiv:1910.08854, 2019.\n\nThe current state of fake news: challenges and opportunities. Alvaro Figueira, Luciana Oliveira, Procedia Computer Science. 1212017Alvaro Figueira and Luciana Oliveira. The current state of fake news: challenges and opportunities. Procedia Computer Science, 121:817- 825, 12 2017.\n\nMfc datasets: Large-scale benchmark datasets for media forensic challenge evaluation. Haiying Guan, Mark Kozak, Eric Robertson, Yooyoung Lee, Amy N Yates, Andrew Delgado, Daniel Zhou, Timothee Kheyrkhah, Jeff Smith, Jonathan Fiscus, 2019 IEEE Winter Applications of Computer Vision Workshops (WACVW). IEEEHaiying Guan, Mark Kozak, Eric Robertson, Yooyoung Lee, Amy N Yates, Andrew Delgado, Daniel Zhou, Timothee Kheyrkhah, Jeff Smith, and Jonathan Fiscus. Mfc datasets: Large-scale benchmark datasets for media forensic challenge evaluation. In 2019 IEEE Win- ter Applications of Computer Vision Workshops (WACVW), pages 63-72. IEEE, 2019.\n\nDeepfake video detection using recurrent neural networks. David G\u00fcera, J Edward, Delp, 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). IEEEDavid G\u00fcera and Edward J Delp. Deepfake video detection using recurrent neural networks. In 2018 15th IEEE International Con- ference on Advanced Video and Signal Based Surveillance (AVSS), pages 1-6. IEEE, 2018.\n\nLips don't lie: A generalisable and robust approach to face forgery detection. Alexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic, Alexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. Lips don't lie: A generalisable and robust approach to face forgery detection, 2021.\n\nDeepfakes: False pornography is here and the law cannot protect you. Douglas Harris, Duke L. & Tech. Rev. 1799Douglas Harris. Deepfakes: False pornography is here and the law cannot protect you. Duke L. & Tech. Rev., 17:99, 2018.\n\nMask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017.\n\nForgerynet: A versatile benchmark for comprehensive forgery analysis. Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, Ziwei Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu. Forgerynet: A versatile benchmark for comprehensive forgery analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pages 4360-4369, 2021.\n\nDeeperforensics-1.0: A large-scale dataset for real-world face forgery detection. Liming Jiang, Wayne Wu, Ren Li, Chen Qian, Chen Change Loy, arXiv:2001.03024arXiv preprintLiming Jiang, Wayne Wu, Ren Li, Chen Qian, and Chen Change Loy. Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection. arXiv preprint arXiv:2001.03024, 2020.\n\nHasam Khalid, Shahroz Tariq, Simon S Woo, arXiv:2108.05080Fakeavceleb: A novel audio-video multimodal deepfake dataset. arXiv preprintHasam Khalid, Shahroz Tariq, and Simon S Woo. Fakeavceleb: A novel audio-video multimodal deepfake dataset. arXiv preprint arXiv:2108.05080, 2021.\n\nPerceptual video hashing for content identification and authentication. Fouad Khelifi, Ahmed Bouridane, IEEE Transactions on Circuits and Systems for Video Technology. 29Fouad Khelifi and Ahmed Bouridane. Perceptual video hashing for content identification and authentication. IEEE Transactions on Cir- cuits and Systems for Video Technology, 29(1):50-67, 2017.\n\nJoon-Young Lee, and In So Kweon. Deep video inpainting. Dahun Kim, Sanghyun Woo, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Deep video inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5792-5801, 2019.\n\nJoon-Young Lee, and In So Kweon. Recurrent temporal aggregation framework for deep video inpainting. Dahun Kim, Sanghyun Woo, IEEE Transactions on Pattern Analysis and Machine Intelligence. 425Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Re- current temporal aggregation framework for deep video inpainting. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(5):1038-1052, 2020.\n\nAffective body expression perception and recognition: A survey. Andrea Kleinsmith, Nadia Bianchi-Berthouze, IEEE Transactions on Affective Computing. 4Andrea Kleinsmith and Nadia Bianchi-Berthouze. Affective body expression perception and recognition: A survey. IEEE Transactions on Affective Computing, 4:15-33, 2013.\n\nDeepfakes: a new threat to face recognition? assessment and detection. Pavel Korshunov, S\u00e9bastien Marcel, arXiv:1812.08685arXiv preprintPavel Korshunov and S\u00e9bastien Marcel. Deepfakes: a new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018.\n\nFast face-swap using convolutional neural networks. Iryna Korshunova, Wenzhe Shi, Joni Dambre, Lucas Theis, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionIryna Korshunova, Wenzhe Shi, Joni Dambre, and Lucas Theis. Fast face-swap using convolutional neural networks. In Proceedings of the IEEE international conference on computer vision, pages 3677- 3685, 2017.\n\nEmotic: Emotions in context dataset. Ronak Kosti, M Jose, Adria Alvarez, Agata Recasens, Lapedriza, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsRonak Kosti, Jose M Alvarez, Adria Recasens, and Agata Lapedriza. Emotic: Emotions in context dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 61-69, 2017.\n\nKodf: A large-scale korean deepfake detection dataset. Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, Gyeongsu Chae, arXiv:2103.10094arXiv preprintPatrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and Gyeongsu Chae. Kodf: A large-scale korean deepfake detection dataset. arXiv preprint arXiv:2103.10094, 2021.\n\nFaceshifter: Towards high fidelity and occlusion aware face swapping. Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen, arXiv:1912.13457arXiv preprintLingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. Faceshifter: Towards high fidelity and occlusion aware face swap- ping. arXiv preprint arXiv:1912.13457, 2019.\n\nExposing deepfake videos by detecting face warping artifacts. Yuezun Li, Siwei Lyu, IEEE Conference on Computer Vision and Pattern Recognition Workshops. CVPRWYuezun Li and Siwei Lyu. Exposing deepfake videos by detecting face warping artifacts. In IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2019.\n\nCelebdf: A new dataset for deepfake forensics. Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, Siwei Lyu, Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb- df: A new dataset for deepfake forensics. 2019.\n\nVideo copy-move forgery detection and localization based on tamura texture features. Yang Sheng, Tian-Qiang Liao, Huang, 2013 6th international congress on image and signal processing. IEEE2Sheng-Yang Liao and Tian-Qiang Huang. Video copy-move forgery detection and localization based on tamura texture features. In 2013 6th international congress on image and signal processing (CISP), volume 2, pages 864-868. IEEE, 2013.\n\nDeep supervised hashing for fast image retrieval. Haomiao Liu, Ruiping Wang, S Shan, Xilin Chen, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Haomiao Liu, Ruiping Wang, S. Shan, and Xilin Chen. Deep su- pervised hashing for fast image retrieval. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2064- 2072, 2016.\n\nTwo-stage copy-move forgery detection with self deep matching and proposal superglue. Yaqi Liu, Chao Xia, Xiaobin Zhu, Shengwei Xu, IEEE Transactions on Image Processing. 31Yaqi Liu, Chao Xia, Xiaobin Zhu, and Shengwei Xu. Two-stage copy-move forgery detection with self deep matching and proposal superglue. IEEE Transactions on Image Processing, 31:541-555, 2021.\n\nA coarse-to-fine deep convolutional neural network framework for frame duplication detection and localization in forged videos. Chengjiang Long, Arslan Basharat, Anthony Hoogs, Priyanka Singh, Hany Farid, CVPR Workshops. Chengjiang Long, Arslan Basharat, Anthony Hoogs, Priyanka Singh, Hany Farid, et al. A coarse-to-fine deep convolutional neural network framework for frame duplication detection and localization in forged videos. In CVPR Workshops, pages 1-10, 2019.\n\nCopy-move forgery detection technique for forensic analysis in digital images. Toqeer Mahmood, Tabassam Nawaz, Aun Irtaza, Rehan Ashraf, Mohsin Shah, Muhammad Tariq Mahmood, Mathematical Problems in Engineering. Toqeer Mahmood, Tabassam Nawaz, Aun Irtaza, Rehan Ashraf, Mohsin Shah, and Muhammad Tariq Mahmood. Copy-move forgery detection technique for forensic analysis in digital images. Mathe- matical Problems in Engineering, 2016, 2016.\n\nExploiting visual artifacts to expose deepfakes and face manipulations. Falko Matern, Christian Riess, Marc Stamminger, IEEE Winter Applications of Computer Vision Workshops (WACVW). Falko Matern, Christian Riess, and Marc Stamminger. Exploiting visual artifacts to expose deepfakes and face manipulations. 2019 IEEE Winter Applications of Computer Vision Workshops (WACVW), pages 83-92, 2019.\n\nEmotions don't lie: An audio-visual deepfake detection method using affective cues. Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, Dinesh Manocha, Proceedings of the 28th ACM international conference on multimedia. the 28th ACM international conference on multimediaTrisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha. Emotions don't lie: An audio-visual deepfake detection method using affective cues. In Proceedings of the 28th ACM international conference on multimedia, pages 2823-2832, 2020.\n\nEmoticon: Context-aware multimodal emotion recognition using frege's principle. Trisha Mittal, Pooja Guhan, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, Dinesh Manocha, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Trisha Mittal, Pooja Guhan, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha. Emoticon: Context-aware multi- modal emotion recognition using frege's principle. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14222-14231, 2020.\n\nFalse video of joe biden claims democrat candidate greeted wrong us state at a rally -euronews. Link. A P News, AP News. False video of joe biden claims democrat candidate greeted wrong us state at a rally -euronews. Link, February 2020. (Accessed on 04/07/2022).\n\nFsgan: Subject agnostic face swapping and reenactment. Yuval Nirkin, Yosi Keller, Tal Hassner, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionYuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject agnostic face swapping and reenactment. In Proceedings of the IEEE/CVF in- ternational conference on computer vision, pages 7184-7193, 2019.\n\nPhoto of iran's missile launch was manipulated : Npr. Link. 04/07/2022NPRNPR. Photo of iran's missile launch was manipulated : Npr. Link, July 2008. (Accessed on 04/07/2022).\n\nIvan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um\u00e9, Mr Dpfks, Carl Shift Facenheim, R P Luis, Jian Jiang, arXiv:2005.05535A simple, flexible and extensible face swapping framework. arXiv preprintIvan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um\u00e9, Mr Dpfks, Carl Shift Facenheim, Luis RP, Jian Jiang, et al. Deepfacelab: A simple, flexible and extensible face swapping framework. arXiv preprint arXiv:2005.05535, 2020.\n\nA lip sync expert is all you need for speech to lip generation in the wild. Rudrabha Kr Prajwal, Mukhopadhyay, P Vinay, C V Namboodiri, Jawahar, Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on MultimediaKR Prajwal, Rudrabha Mukhopadhyay, Vinay P Namboodiri, and CV Jawahar. A lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM International Conference on Multimedia, pages 484-492, 2020.\n\nDeeprhythm: Exposing deepfakes with attentional visual heartbeat rhythms. Hua Qi, Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Wei Feng, Yang Liu, Jianjun Zhao, Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on MultimediaHua Qi, Qing Guo, Felix Juefei-Xu, Xiaofei Xie, Lei Ma, Wei Feng, Yang Liu, and Jianjun Zhao. Deeprhythm: Exposing deepfakes with attentional visual heartbeat rhythms. In Proceedings of the 28th ACM International Conference on Multimedia, pages 4318-4327, 2020.\n\nAdri\u00e1 Recasens Continente Recasens. Where are they looking? PhD thesis. Massachusetts Institute of TechnologyAdri\u00e1 Recasens Continente Recasens. Where are they looking? PhD thesis, Massachusetts Institute of Technology, 2016.\n\nFaceforensics++: Learning to detect manipulated facial images. Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, Matthias Nie\u00dfner, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionAndreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Nie\u00dfner. Faceforensics++: Learn- ing to detect manipulated facial images. In Proceedings of the IEEE International Conference on Computer Vision, pages 1-11, 2019.\n\nRotation invariant localization of duplicated image regions based on zernike moments. Matthias Seung-Jin Ryu, Min-Jeong Kirchner, Heung-Kyu Lee, Lee, IEEE Transactions on Information Forensics and Security. 88Seung-Jin Ryu, Matthias Kirchner, Min-Jeong Lee, and Heung- Kyu Lee. Rotation invariant localization of duplicated image re- gions based on zernike moments. IEEE Transactions on Information Forensics and Security, 8(8):1355-1370, 2013.\n\nRecurrent convolutional strategies for face manipulation detection in videos. Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael Abdalmageed, Iacopo Masi, Prem Natarajan, Interfaces (GUI). 31Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAlmageed, Iacopo Masi, and Prem Natarajan. Recurrent convolutional strate- gies for face manipulation detection in videos. Interfaces (GUI), 3:1, 2019.\n\nThe vidtimit database. Conrad Sanderson, IDIAP. Technical reportConrad Sanderson. The vidtimit database. Technical report, IDIAP, 2002.\n\nFirst order motion model for image animation. Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, Nicu Sebe, Advances in Neural Information Processing Systems. 32Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in Neural Information Processing Systems, 32:7137-7147, 2019.\n\nMisinformation and fears about its impact are pervasive in 11 emerging economies. Laura Silver, Laura Silver. Misinformation and fears about its impact are pervasive in 11 emerging economies, Aug 2020.\n\ndeepfakes\": The newest way to commit one of the oldest crimes. Russell Spivak, Geo. L. Tech. Rev. 3339Russell Spivak. \" deepfakes\": The newest way to commit one of the oldest crimes. Geo. L. Tech. Rev., 3:339, 2018.\n\nA video forgery detection algorithm based on compressive sensing. Lichao Su, Tianqiang Huang, Jianmei Yang, Multimedia Tools and Applications. 7417Lichao Su, Tianqiang Huang, and Jianmei Yang. A video forgery detection algorithm based on compressive sensing. Multimedia Tools and Applications, 74(17):6641-6656, 2015.\n\nDeferred neural rendering: Image synthesis using neural textures. Justus Thies, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, ACM Transactions on Graphics (TOG). 384Justus Thies, Michael Zollh\u00f6fer, and Matthias Nie\u00dfner. Deferred neural rendering: Image synthesis using neural textures. ACM Trans- actions on Graphics (TOG), 38(4):1-12, 2019.\n\nFace2face: Real-time face capture and reenactment of rgb videos. Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, Matthias Nie\u00dfner, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJustus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Nie\u00dfner. Face2face: Real-time face capture and reenactment of rgb videos. In Proceedings of the IEEE confer- ence on computer vision and pattern recognition, pages 2387-2395, 2016.\n\nIcface: Interpretable and controllable face reenactment using gans. Soumya Tripathy, Juho Kannala, Esa Rahtu, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionSoumya Tripathy, Juho Kannala, and Esa Rahtu. Icface: Inter- pretable and controllable face reenactment using gans. In Proceed- ings of the IEEE/CVF Winter Conference on Applications of Com- puter Vision, pages 3385-3394, 2020.\n\nM2tr: Multi-modal multi-scale transformers for deepfake detection. Junke Wang, Zuxuan Wu, Jingjing Chen, Yu-Gang Jiang, arXiv:2104.09770arXiv preprintJunke Wang, Zuxuan Wu, Jingjing Chen, and Yu-Gang Jiang. M2tr: Multi-modal multi-scale transformers for deepfake detection. arXiv preprint arXiv:2104.09770, 2021.\n\nVideo inter-frame forgery identification based on optical flow consistency. Qi Wang, Zhaohong Li, Zhenzhen Zhang, Qinglong Ma, Sensors & Transducers. 1663229Qi Wang, Zhaohong Li, Zhenzhen Zhang, and Qinglong Ma. Video inter-frame forgery identification based on optical flow consistency. Sensors & Transducers, 166(3):229, 2014.\n\nMeasuring the news and its impact on democracy. J Duncan, Watts, M David, Markus Rothschild, Mobius, Proceedings of the National Academy of Sciences. 118152021Duncan J Watts, David M Rothschild, and Markus Mobius. Mea- suring the news and its impact on democracy. Proceedings of the National Academy of Sciences, 118(15), 2021.\n\nBusternet: Detecting copy-move image forgery with source/target localization. Yue Wu, Wael Abd-Almageed, Prem Natarajan, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Yue Wu, Wael Abd-Almageed, and Prem Natarajan. Busternet: Detecting copy-move image forgery with source/target localization. In Proceedings of the European conference on computer vision (ECCV), pages 168-184, 2018.\n\nWael Abd-Almageed, and Prem Natarajan. long2019coarse. Yue Wu, 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEEYue Wu, Wael Abd-Almageed, and Prem Natarajan. long2019coarse. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1907-1915. IEEE, 2018.\n\nExposing video inter-frame forgery based on velocity field consistency. Yuxing Wu, Xinghao Jiang, Tanfeng Sun, Wan Wang, 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEEYuxing Wu, Xinghao Jiang, Tanfeng Sun, and Wan Wang. Expos- ing video inter-frame forgery based on velocity field consistency. In 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 2674-2678. IEEE, 2014.\n\nExposing deep fakes using inconsistent head poses. Xin Yang, Yuezun Li, Siwei Lyu, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEXin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8261-8265. IEEE, 2019.\n\nExposing deep fakes using inconsistent head poses. Xin Yang, Yuezun Li, Siwei Lyu, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEXin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8261-8265. IEEE, 2019.\n\nAudio-driven talking face video generation with learning-based personalized head pose. Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, Yong-Jin Liu, arXiv:2002.10137arXiv preprintRan Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and Yong-Jin Liu. Audio-driven talking face video generation with learning-based per- sonalized head pose. arXiv preprint arXiv:2002.10137, 2020.\n\nApb2face: audio-guided face reenactment with auxiliary pose and blink signals. Jiangning Zhang, Liang Liu, Zhucun Xue, Yong Liu, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEJiangning Zhang, Liang Liu, Zhucun Xue, and Yong Liu. Apb2face: audio-guided face reenactment with auxiliary pose and blink signals. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4402-4406. IEEE, 2020.\n\nInter-frame passive-blind forgery detection for video shot based on similarity analysis. Dong-Ning Zhao, Ren-Kui Wang, Zhe-Ming Lu, Multimedia Tools and Applications. 7719Dong-Ning Zhao, Ren-Kui Wang, and Zhe-Ming Lu. Inter-frame passive-blind forgery detection for video shot based on similarity analysis. Multimedia Tools and Applications, 77(19):25389-25408, 2018.\n\nPeng Zhou, Ning Yu, Zuxuan Wu, S Larry, Abhinav Davis, Ser-Nam Shrivastava, Lim, arXiv:2101.11080Deep video inpainting detection. arXiv preprintPeng Zhou, Ning Yu, Zuxuan Wu, Larry S Davis, Abhinav Shrivas- tava, and Ser-Nam Lim. Deep video inpainting detection. arXiv preprint arXiv:2101.11080, 2021.\n\nWilddeepfake: A challenging real-world dataset for deepfake detection. Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, Yu-Gang Jiang, Proceedings of the 28th ACM International Conference on Multimedia. the 28th ACM International Conference on MultimediaBojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. Wilddeepfake: A challenging real-world dataset for deepfake detection. In Proceedings of the 28th ACM International Conference on Multimedia, pages 2382-2390, 2020.\n", "annotations": {"author": "[{\"end\":167,\"start\":75},{\"end\":262,\"start\":168},{\"end\":341,\"start\":263},{\"end\":414,\"start\":342},{\"end\":502,\"start\":415},{\"end\":581,\"start\":503},{\"end\":654,\"start\":582},{\"end\":753,\"start\":655},{\"end\":832,\"start\":754},{\"end\":905,\"start\":833}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":82},{\"end\":180,\"start\":175},{\"end\":277,\"start\":269},{\"end\":350,\"start\":346},{\"end\":438,\"start\":427},{\"end\":517,\"start\":509},{\"end\":590,\"start\":586},{\"end\":670,\"start\":660},{\"end\":768,\"start\":760},{\"end\":841,\"start\":837}]", "author_first_name": "[{\"end\":81,\"start\":75},{\"end\":174,\"start\":168},{\"end\":268,\"start\":263},{\"end\":345,\"start\":342},{\"end\":426,\"start\":415},{\"end\":508,\"start\":503},{\"end\":585,\"start\":582},{\"end\":659,\"start\":655},{\"end\":759,\"start\":754},{\"end\":836,\"start\":833}]", "author_affiliation": "[{\"end\":166,\"start\":105},{\"end\":261,\"start\":200},{\"end\":340,\"start\":279},{\"end\":413,\"start\":352},{\"end\":501,\"start\":440},{\"end\":580,\"start\":519},{\"end\":653,\"start\":592},{\"end\":752,\"start\":691},{\"end\":831,\"start\":770},{\"end\":904,\"start\":843}]", "title": "[{\"end\":72,\"start\":1},{\"end\":977,\"start\":906}]", "venue": null, "abstract": "[{\"end\":2341,\"start\":979}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2507,\"start\":2503},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2510,\"start\":2507},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2737,\"start\":2734},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2740,\"start\":2737},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2857,\"start\":2853},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3068,\"start\":3064},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3070,\"start\":3068},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3430,\"start\":3426},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3433,\"start\":3430},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3435,\"start\":3433},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3830,\"start\":3826},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":3833,\"start\":3830},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3836,\"start\":3833},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5299,\"start\":5295},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5313,\"start\":5309},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5484,\"start\":5480},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5496,\"start\":5492},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5514,\"start\":5510},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5561,\"start\":5557},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":5586,\"start\":5582},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":5655,\"start\":5651},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":5668,\"start\":5664},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5680,\"start\":5676},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":5780,\"start\":5776},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":5795,\"start\":5791},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":5807,\"start\":5803},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8289,\"start\":8286},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8304,\"start\":8300},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8324,\"start\":8320},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8886,\"start\":8882},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8889,\"start\":8886},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8915,\"start\":8911},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8918,\"start\":8915},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8921,\"start\":8918},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9480,\"start\":9476},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9491,\"start\":9488},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9509,\"start\":9505},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9525,\"start\":9521},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9557,\"start\":9553},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9928,\"start\":9924},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9951,\"start\":9947},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9976,\"start\":9972},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":9995,\"start\":9991},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10301,\"start\":10297},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10316,\"start\":10312},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10327,\"start\":10323},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10345,\"start\":10341},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10362,\"start\":10358},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10670,\"start\":10666},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10770,\"start\":10766},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10813,\"start\":10809},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10845,\"start\":10841},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10994,\"start\":10990},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10997,\"start\":10994},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11163,\"start\":11159},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11165,\"start\":11163},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11228,\"start\":11224},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11704,\"start\":11700},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11853,\"start\":11849},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11938,\"start\":11935},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11960,\"start\":11956},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":12066,\"start\":12062},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":12069,\"start\":12066},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12072,\"start\":12069},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":12234,\"start\":12230},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":12371,\"start\":12367},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12393,\"start\":12389},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":12494,\"start\":12490},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12600,\"start\":12596},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14659,\"start\":14658},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15260,\"start\":15259},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19831,\"start\":19830},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20628,\"start\":20624},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20646,\"start\":20642},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20669,\"start\":20665},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21020,\"start\":21016},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21207,\"start\":21203},{\"end\":21433,\"start\":21420},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22037,\"start\":22033},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22057,\"start\":22053},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22177,\"start\":22173},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22263,\"start\":22259},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22577,\"start\":22573},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22590,\"start\":22587},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":22614,\"start\":22610},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23813,\"start\":23809},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24802,\"start\":24798},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":25002,\"start\":24998},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25456,\"start\":25452},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25481,\"start\":25477},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25690,\"start\":25686},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25721,\"start\":25717},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":34196,\"start\":34192},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34232,\"start\":34228},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34235,\"start\":34232},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34401,\"start\":34400},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":34417,\"start\":34416}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30026,\"start\":29977},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30390,\"start\":30027},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30890,\"start\":30391},{\"attributes\":{\"id\":\"fig_3\"},\"end\":31196,\"start\":30891},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31291,\"start\":31197},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31472,\"start\":31292},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32002,\"start\":31473},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32131,\"start\":32003},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32716,\"start\":32132},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33589,\"start\":32717},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33840,\"start\":33590},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":33883,\"start\":33841}]", "paragraph": "[{\"end\":3549,\"start\":2357},{\"end\":3957,\"start\":3551},{\"end\":4646,\"start\":3959},{\"end\":4735,\"start\":4648},{\"end\":4840,\"start\":4737},{\"end\":4922,\"start\":4842},{\"end\":5045,\"start\":4924},{\"end\":5164,\"start\":5047},{\"end\":5516,\"start\":5166},{\"end\":5744,\"start\":5518},{\"end\":5883,\"start\":5746},{\"end\":6076,\"start\":5906},{\"end\":6106,\"start\":6078},{\"end\":6393,\"start\":6108},{\"end\":6768,\"start\":6395},{\"end\":7490,\"start\":6770},{\"end\":7714,\"start\":7492},{\"end\":8172,\"start\":7731},{\"end\":9292,\"start\":8214},{\"end\":10378,\"start\":9324},{\"end\":11427,\"start\":10409},{\"end\":12725,\"start\":11454},{\"end\":12920,\"start\":12751},{\"end\":13503,\"start\":12972},{\"end\":13880,\"start\":13528},{\"end\":14317,\"start\":13882},{\"end\":14474,\"start\":14319},{\"end\":14575,\"start\":14476},{\"end\":15310,\"start\":14598},{\"end\":15450,\"start\":15312},{\"end\":15585,\"start\":15478},{\"end\":15957,\"start\":15587},{\"end\":16973,\"start\":15997},{\"end\":18376,\"start\":16993},{\"end\":18827,\"start\":18378},{\"end\":20400,\"start\":18870},{\"end\":21947,\"start\":20442},{\"end\":23384,\"start\":21981},{\"end\":23931,\"start\":23386},{\"end\":26109,\"start\":23999},{\"end\":26319,\"start\":26111},{\"end\":26854,\"start\":26356},{\"end\":27243,\"start\":26856},{\"end\":28056,\"start\":27245},{\"end\":28279,\"start\":28058},{\"end\":28608,\"start\":28281},{\"end\":29141,\"start\":28610},{\"end\":29500,\"start\":29143},{\"end\":29976,\"start\":29527}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18869,\"start\":18828}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":3956,\"start\":3949},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":6199,\"start\":6192},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":8356,\"start\":8349},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11568,\"start\":11561},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11608,\"start\":11601},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17048,\"start\":17041},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":18945,\"start\":18938},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22421,\"start\":22414},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":26048,\"start\":26041}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2355,\"start\":2343},{\"end\":5904,\"start\":5886},{\"attributes\":{\"n\":\"2.\"},\"end\":7729,\"start\":7717},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8212,\"start\":8175},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9322,\"start\":9295},{\"attributes\":{\"n\":\"2.3.\"},\"end\":10407,\"start\":10381},{\"attributes\":{\"n\":\"2.4.\"},\"end\":11452,\"start\":11430},{\"attributes\":{\"n\":\"3.\"},\"end\":12749,\"start\":12728},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12954,\"start\":12923},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":12970,\"start\":12957},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":13526,\"start\":13506},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":14596,\"start\":14578},{\"attributes\":{\"n\":\"4.\"},\"end\":15476,\"start\":15453},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15995,\"start\":15960},{\"end\":16991,\"start\":16976},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20440,\"start\":20403},{\"end\":21979,\"start\":21950},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23997,\"start\":23934},{\"attributes\":{\"n\":\"5.\"},\"end\":26354,\"start\":26322},{\"attributes\":{\"n\":\"6.\"},\"end\":29525,\"start\":29503},{\"end\":29988,\"start\":29978},{\"end\":30038,\"start\":30028},{\"end\":30396,\"start\":30392},{\"end\":30902,\"start\":30892},{\"end\":31208,\"start\":31198},{\"end\":31303,\"start\":31293},{\"end\":31483,\"start\":31474},{\"end\":32013,\"start\":32004},{\"end\":32727,\"start\":32718},{\"end\":33600,\"start\":33591},{\"end\":33851,\"start\":33842}]", "table": "[{\"end\":32002,\"start\":31604},{\"end\":32716,\"start\":32206},{\"end\":33589,\"start\":32734}]", "figure_caption": "[{\"end\":30026,\"start\":29990},{\"end\":30390,\"start\":30040},{\"end\":30890,\"start\":30398},{\"end\":31196,\"start\":30904},{\"end\":31291,\"start\":31210},{\"end\":31472,\"start\":31305},{\"end\":31604,\"start\":31485},{\"end\":32131,\"start\":32015},{\"end\":32206,\"start\":32134},{\"end\":32734,\"start\":32729},{\"end\":33840,\"start\":33602},{\"end\":33883,\"start\":33853}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3548,\"start\":3540},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4147,\"start\":4137},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4176,\"start\":4167},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4241,\"start\":4230},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4272,\"start\":4262},{\"end\":5992,\"start\":5984},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6391,\"start\":6383},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13755,\"start\":13746},{\"end\":14574,\"start\":14566},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15059,\"start\":15050},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15374,\"start\":15364},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16898,\"start\":16890},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19999,\"start\":19982},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24338,\"start\":24330},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26108,\"start\":26100},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27241,\"start\":27232}]", "bib_author_first_name": "[{\"end\":35409,\"start\":35403},{\"end\":35425,\"start\":35418},{\"end\":35441,\"start\":35434},{\"end\":35457,\"start\":35453},{\"end\":35846,\"start\":35840},{\"end\":35860,\"start\":35856},{\"end\":35873,\"start\":35868},{\"end\":35891,\"start\":35884},{\"end\":36649,\"start\":36641},{\"end\":36662,\"start\":36657},{\"end\":36678,\"start\":36672},{\"end\":36692,\"start\":36687},{\"end\":36713,\"start\":36705},{\"end\":37041,\"start\":37036},{\"end\":37059,\"start\":37051},{\"end\":37075,\"start\":37068},{\"end\":37093,\"start\":37086},{\"end\":37097,\"start\":37094},{\"end\":37113,\"start\":37105},{\"end\":37530,\"start\":37525},{\"end\":37753,\"start\":37752},{\"end\":37769,\"start\":37768},{\"end\":38048,\"start\":38042},{\"end\":38063,\"start\":38056},{\"end\":38579,\"start\":38576},{\"end\":38785,\"start\":38780},{\"end\":38808,\"start\":38802},{\"end\":39046,\"start\":39040},{\"end\":39066,\"start\":39058},{\"end\":39079,\"start\":39074},{\"end\":39446,\"start\":39441},{\"end\":39462,\"start\":39458},{\"end\":39473,\"start\":39470},{\"end\":39762,\"start\":39756},{\"end\":39780,\"start\":39773},{\"end\":40069,\"start\":40062},{\"end\":40080,\"start\":40076},{\"end\":40092,\"start\":40088},{\"end\":40112,\"start\":40104},{\"end\":40121,\"start\":40118},{\"end\":40123,\"start\":40122},{\"end\":40137,\"start\":40131},{\"end\":40153,\"start\":40147},{\"end\":40168,\"start\":40160},{\"end\":40184,\"start\":40180},{\"end\":40200,\"start\":40192},{\"end\":40680,\"start\":40675},{\"end\":40689,\"start\":40688},{\"end\":41102,\"start\":41092},{\"end\":41126,\"start\":41114},{\"end\":41146,\"start\":41139},{\"end\":41161,\"start\":41157},{\"end\":41414,\"start\":41407},{\"end\":41588,\"start\":41581},{\"end\":41600,\"start\":41593},{\"end\":41616,\"start\":41611},{\"end\":41629,\"start\":41625},{\"end\":42007,\"start\":42002},{\"end\":42015,\"start\":42012},{\"end\":42025,\"start\":42021},{\"end\":42038,\"start\":42032},{\"end\":42051,\"start\":42045},{\"end\":42064,\"start\":42057},{\"end\":42073,\"start\":42071},{\"end\":42085,\"start\":42081},{\"end\":42097,\"start\":42092},{\"end\":42627,\"start\":42621},{\"end\":42640,\"start\":42635},{\"end\":42648,\"start\":42645},{\"end\":42657,\"start\":42653},{\"end\":42675,\"start\":42664},{\"end\":42902,\"start\":42897},{\"end\":42918,\"start\":42911},{\"end\":42933,\"start\":42926},{\"end\":43256,\"start\":43251},{\"end\":43271,\"start\":43266},{\"end\":43603,\"start\":43598},{\"end\":43617,\"start\":43609},{\"end\":44057,\"start\":44052},{\"end\":44071,\"start\":44063},{\"end\":44430,\"start\":44424},{\"end\":44448,\"start\":44443},{\"end\":44756,\"start\":44751},{\"end\":44777,\"start\":44768},{\"end\":45022,\"start\":45017},{\"end\":45041,\"start\":45035},{\"end\":45051,\"start\":45047},{\"end\":45065,\"start\":45060},{\"end\":45445,\"start\":45440},{\"end\":45454,\"start\":45453},{\"end\":45466,\"start\":45461},{\"end\":45481,\"start\":45476},{\"end\":45941,\"start\":45934},{\"end\":45956,\"start\":45948},{\"end\":45970,\"start\":45962},{\"end\":45983,\"start\":45976},{\"end\":45998,\"start\":45990},{\"end\":46282,\"start\":46275},{\"end\":46294,\"start\":46287},{\"end\":46303,\"start\":46300},{\"end\":46314,\"start\":46310},{\"end\":46325,\"start\":46321},{\"end\":46601,\"start\":46595},{\"end\":46611,\"start\":46606},{\"end\":46920,\"start\":46914},{\"end\":46928,\"start\":46925},{\"end\":46937,\"start\":46935},{\"end\":46951,\"start\":46943},{\"end\":46961,\"start\":46956},{\"end\":47169,\"start\":47165},{\"end\":47187,\"start\":47177},{\"end\":47562,\"start\":47555},{\"end\":47575,\"start\":47568},{\"end\":47583,\"start\":47582},{\"end\":47595,\"start\":47590},{\"end\":47960,\"start\":47956},{\"end\":47970,\"start\":47966},{\"end\":47983,\"start\":47976},{\"end\":47997,\"start\":47989},{\"end\":48375,\"start\":48365},{\"end\":48388,\"start\":48382},{\"end\":48406,\"start\":48399},{\"end\":48422,\"start\":48414},{\"end\":48434,\"start\":48430},{\"end\":48793,\"start\":48787},{\"end\":48811,\"start\":48803},{\"end\":48822,\"start\":48819},{\"end\":48836,\"start\":48831},{\"end\":48851,\"start\":48845},{\"end\":48872,\"start\":48858},{\"end\":49228,\"start\":49223},{\"end\":49246,\"start\":49237},{\"end\":49258,\"start\":49254},{\"end\":49636,\"start\":49630},{\"end\":49652,\"start\":49645},{\"end\":49672,\"start\":49667},{\"end\":49688,\"start\":49682},{\"end\":49701,\"start\":49695},{\"end\":50180,\"start\":50174},{\"end\":50194,\"start\":50189},{\"end\":50209,\"start\":50202},{\"end\":50229,\"start\":50224},{\"end\":50245,\"start\":50239},{\"end\":50258,\"start\":50252},{\"end\":50724,\"start\":50723},{\"end\":50726,\"start\":50725},{\"end\":50946,\"start\":50941},{\"end\":50959,\"start\":50955},{\"end\":50971,\"start\":50968},{\"end\":51491,\"start\":51487},{\"end\":51506,\"start\":51499},{\"end\":51519,\"start\":51512},{\"end\":51537,\"start\":51531},{\"end\":51549,\"start\":51543},{\"end\":51567,\"start\":51562},{\"end\":51575,\"start\":51573},{\"end\":51587,\"start\":51583},{\"end\":51593,\"start\":51588},{\"end\":51606,\"start\":51605},{\"end\":51608,\"start\":51607},{\"end\":51619,\"start\":51615},{\"end\":52058,\"start\":52050},{\"end\":52086,\"start\":52085},{\"end\":52095,\"start\":52094},{\"end\":52097,\"start\":52096},{\"end\":52555,\"start\":52552},{\"end\":52564,\"start\":52560},{\"end\":52575,\"start\":52570},{\"end\":52594,\"start\":52587},{\"end\":52603,\"start\":52600},{\"end\":52611,\"start\":52608},{\"end\":52622,\"start\":52618},{\"end\":52635,\"start\":52628},{\"end\":53321,\"start\":53314},{\"end\":53337,\"start\":53331},{\"end\":53354,\"start\":53349},{\"end\":53375,\"start\":53366},{\"end\":53389,\"start\":53383},{\"end\":53405,\"start\":53397},{\"end\":53891,\"start\":53883},{\"end\":53916,\"start\":53907},{\"end\":53936,\"start\":53927},{\"end\":54327,\"start\":54321},{\"end\":54341,\"start\":54335},{\"end\":54354,\"start\":54349},{\"end\":54368,\"start\":54364},{\"end\":54388,\"start\":54382},{\"end\":54399,\"start\":54395},{\"end\":54664,\"start\":54658},{\"end\":54828,\"start\":54818},{\"end\":54847,\"start\":54839},{\"end\":54867,\"start\":54861},{\"end\":54883,\"start\":54878},{\"end\":54895,\"start\":54891},{\"end\":55248,\"start\":55243},{\"end\":55434,\"start\":55427},{\"end\":55653,\"start\":55647},{\"end\":55667,\"start\":55658},{\"end\":55682,\"start\":55675},{\"end\":55972,\"start\":55966},{\"end\":55987,\"start\":55980},{\"end\":56007,\"start\":55999},{\"end\":56305,\"start\":56299},{\"end\":56320,\"start\":56313},{\"end\":56336,\"start\":56332},{\"end\":56358,\"start\":56349},{\"end\":56377,\"start\":56369},{\"end\":56867,\"start\":56861},{\"end\":56882,\"start\":56878},{\"end\":56895,\"start\":56892},{\"end\":57351,\"start\":57346},{\"end\":57364,\"start\":57358},{\"end\":57377,\"start\":57369},{\"end\":57391,\"start\":57384},{\"end\":57671,\"start\":57669},{\"end\":57686,\"start\":57678},{\"end\":57699,\"start\":57691},{\"end\":57715,\"start\":57707},{\"end\":57972,\"start\":57971},{\"end\":57989,\"start\":57988},{\"end\":58003,\"start\":57997},{\"end\":58333,\"start\":58330},{\"end\":58342,\"start\":58338},{\"end\":58361,\"start\":58357},{\"end\":58762,\"start\":58759},{\"end\":59087,\"start\":59081},{\"end\":59099,\"start\":59092},{\"end\":59114,\"start\":59107},{\"end\":59123,\"start\":59120},{\"end\":59524,\"start\":59521},{\"end\":59537,\"start\":59531},{\"end\":59547,\"start\":59542},{\"end\":59931,\"start\":59928},{\"end\":59944,\"start\":59938},{\"end\":59954,\"start\":59949},{\"end\":60374,\"start\":60371},{\"end\":60385,\"start\":60379},{\"end\":60396,\"start\":60390},{\"end\":60409,\"start\":60404},{\"end\":60423,\"start\":60415},{\"end\":60738,\"start\":60729},{\"end\":60751,\"start\":60746},{\"end\":60763,\"start\":60757},{\"end\":60773,\"start\":60769},{\"end\":61247,\"start\":61238},{\"end\":61261,\"start\":61254},{\"end\":61276,\"start\":61268},{\"end\":61522,\"start\":61518},{\"end\":61533,\"start\":61529},{\"end\":61544,\"start\":61538},{\"end\":61550,\"start\":61549},{\"end\":61565,\"start\":61558},{\"end\":61580,\"start\":61573},{\"end\":61897,\"start\":61892},{\"end\":61909,\"start\":61902},{\"end\":61925,\"start\":61917},{\"end\":61939,\"start\":61932},{\"end\":61951,\"start\":61944}]", "bib_author_last_name": "[{\"end\":35416,\"start\":35410},{\"end\":35432,\"start\":35426},{\"end\":35451,\"start\":35442},{\"end\":35465,\"start\":35458},{\"end\":35854,\"start\":35847},{\"end\":35866,\"start\":35861},{\"end\":35882,\"start\":35874},{\"end\":35895,\"start\":35892},{\"end\":36655,\"start\":36650},{\"end\":36670,\"start\":36663},{\"end\":36685,\"start\":36679},{\"end\":36703,\"start\":36693},{\"end\":36719,\"start\":36714},{\"end\":37049,\"start\":37042},{\"end\":37066,\"start\":37060},{\"end\":37084,\"start\":37076},{\"end\":37103,\"start\":37098},{\"end\":37119,\"start\":37114},{\"end\":37539,\"start\":37531},{\"end\":37760,\"start\":37754},{\"end\":37766,\"start\":37762},{\"end\":37777,\"start\":37770},{\"end\":37783,\"start\":37779},{\"end\":38054,\"start\":38049},{\"end\":38072,\"start\":38064},{\"end\":38587,\"start\":38580},{\"end\":38800,\"start\":38786},{\"end\":38816,\"start\":38809},{\"end\":38827,\"start\":38818},{\"end\":39056,\"start\":39047},{\"end\":39072,\"start\":39067},{\"end\":39089,\"start\":39080},{\"end\":39456,\"start\":39447},{\"end\":39468,\"start\":39463},{\"end\":39480,\"start\":39474},{\"end\":39771,\"start\":39763},{\"end\":39789,\"start\":39781},{\"end\":40074,\"start\":40070},{\"end\":40086,\"start\":40081},{\"end\":40102,\"start\":40093},{\"end\":40116,\"start\":40113},{\"end\":40129,\"start\":40124},{\"end\":40145,\"start\":40138},{\"end\":40158,\"start\":40154},{\"end\":40178,\"start\":40169},{\"end\":40190,\"start\":40185},{\"end\":40207,\"start\":40201},{\"end\":40686,\"start\":40681},{\"end\":40696,\"start\":40690},{\"end\":40702,\"start\":40698},{\"end\":41112,\"start\":41103},{\"end\":41137,\"start\":41127},{\"end\":41155,\"start\":41147},{\"end\":41168,\"start\":41162},{\"end\":41421,\"start\":41415},{\"end\":41591,\"start\":41589},{\"end\":41609,\"start\":41601},{\"end\":41623,\"start\":41617},{\"end\":41638,\"start\":41630},{\"end\":42010,\"start\":42008},{\"end\":42019,\"start\":42016},{\"end\":42030,\"start\":42026},{\"end\":42043,\"start\":42039},{\"end\":42055,\"start\":42052},{\"end\":42069,\"start\":42065},{\"end\":42079,\"start\":42074},{\"end\":42090,\"start\":42086},{\"end\":42101,\"start\":42098},{\"end\":42633,\"start\":42628},{\"end\":42643,\"start\":42641},{\"end\":42651,\"start\":42649},{\"end\":42662,\"start\":42658},{\"end\":42679,\"start\":42676},{\"end\":42909,\"start\":42903},{\"end\":42924,\"start\":42919},{\"end\":42937,\"start\":42934},{\"end\":43264,\"start\":43257},{\"end\":43281,\"start\":43272},{\"end\":43607,\"start\":43604},{\"end\":43621,\"start\":43618},{\"end\":44061,\"start\":44058},{\"end\":44075,\"start\":44072},{\"end\":44441,\"start\":44431},{\"end\":44466,\"start\":44449},{\"end\":44766,\"start\":44757},{\"end\":44784,\"start\":44778},{\"end\":45033,\"start\":45023},{\"end\":45045,\"start\":45042},{\"end\":45058,\"start\":45052},{\"end\":45071,\"start\":45066},{\"end\":45451,\"start\":45446},{\"end\":45459,\"start\":45455},{\"end\":45474,\"start\":45467},{\"end\":45490,\"start\":45482},{\"end\":45501,\"start\":45492},{\"end\":45946,\"start\":45942},{\"end\":45960,\"start\":45957},{\"end\":45974,\"start\":45971},{\"end\":45988,\"start\":45984},{\"end\":46003,\"start\":45999},{\"end\":46285,\"start\":46283},{\"end\":46298,\"start\":46295},{\"end\":46308,\"start\":46304},{\"end\":46319,\"start\":46315},{\"end\":46329,\"start\":46326},{\"end\":46604,\"start\":46602},{\"end\":46615,\"start\":46612},{\"end\":46923,\"start\":46921},{\"end\":46933,\"start\":46929},{\"end\":46941,\"start\":46938},{\"end\":46954,\"start\":46952},{\"end\":46965,\"start\":46962},{\"end\":47175,\"start\":47170},{\"end\":47192,\"start\":47188},{\"end\":47199,\"start\":47194},{\"end\":47566,\"start\":47563},{\"end\":47580,\"start\":47576},{\"end\":47588,\"start\":47584},{\"end\":47600,\"start\":47596},{\"end\":47964,\"start\":47961},{\"end\":47974,\"start\":47971},{\"end\":47987,\"start\":47984},{\"end\":48000,\"start\":47998},{\"end\":48380,\"start\":48376},{\"end\":48397,\"start\":48389},{\"end\":48412,\"start\":48407},{\"end\":48428,\"start\":48423},{\"end\":48440,\"start\":48435},{\"end\":48801,\"start\":48794},{\"end\":48817,\"start\":48812},{\"end\":48829,\"start\":48823},{\"end\":48843,\"start\":48837},{\"end\":48856,\"start\":48852},{\"end\":48880,\"start\":48873},{\"end\":49235,\"start\":49229},{\"end\":49252,\"start\":49247},{\"end\":49269,\"start\":49259},{\"end\":49643,\"start\":49637},{\"end\":49665,\"start\":49653},{\"end\":49680,\"start\":49673},{\"end\":49693,\"start\":49689},{\"end\":49709,\"start\":49702},{\"end\":50187,\"start\":50181},{\"end\":50200,\"start\":50195},{\"end\":50222,\"start\":50210},{\"end\":50237,\"start\":50230},{\"end\":50250,\"start\":50246},{\"end\":50266,\"start\":50259},{\"end\":50731,\"start\":50727},{\"end\":50953,\"start\":50947},{\"end\":50966,\"start\":50960},{\"end\":50979,\"start\":50972},{\"end\":51497,\"start\":51492},{\"end\":51510,\"start\":51507},{\"end\":51529,\"start\":51520},{\"end\":51541,\"start\":51538},{\"end\":51560,\"start\":51550},{\"end\":51571,\"start\":51568},{\"end\":51581,\"start\":51576},{\"end\":51603,\"start\":51594},{\"end\":51613,\"start\":51609},{\"end\":51625,\"start\":51620},{\"end\":52069,\"start\":52059},{\"end\":52083,\"start\":52071},{\"end\":52092,\"start\":52087},{\"end\":52108,\"start\":52098},{\"end\":52117,\"start\":52110},{\"end\":52558,\"start\":52556},{\"end\":52568,\"start\":52565},{\"end\":52585,\"start\":52576},{\"end\":52598,\"start\":52595},{\"end\":52606,\"start\":52604},{\"end\":52616,\"start\":52612},{\"end\":52626,\"start\":52623},{\"end\":52640,\"start\":52636},{\"end\":53329,\"start\":53322},{\"end\":53347,\"start\":53338},{\"end\":53364,\"start\":53355},{\"end\":53381,\"start\":53376},{\"end\":53395,\"start\":53390},{\"end\":53413,\"start\":53406},{\"end\":53905,\"start\":53892},{\"end\":53925,\"start\":53917},{\"end\":53940,\"start\":53937},{\"end\":53945,\"start\":53942},{\"end\":54333,\"start\":54328},{\"end\":54347,\"start\":54342},{\"end\":54362,\"start\":54355},{\"end\":54380,\"start\":54369},{\"end\":54393,\"start\":54389},{\"end\":54409,\"start\":54400},{\"end\":54674,\"start\":54665},{\"end\":54837,\"start\":54829},{\"end\":54859,\"start\":54848},{\"end\":54876,\"start\":54868},{\"end\":54889,\"start\":54884},{\"end\":54900,\"start\":54896},{\"end\":55255,\"start\":55249},{\"end\":55441,\"start\":55435},{\"end\":55656,\"start\":55654},{\"end\":55673,\"start\":55668},{\"end\":55687,\"start\":55683},{\"end\":55978,\"start\":55973},{\"end\":55997,\"start\":55988},{\"end\":56015,\"start\":56008},{\"end\":56311,\"start\":56306},{\"end\":56330,\"start\":56321},{\"end\":56347,\"start\":56337},{\"end\":56367,\"start\":56359},{\"end\":56385,\"start\":56378},{\"end\":56876,\"start\":56868},{\"end\":56890,\"start\":56883},{\"end\":56901,\"start\":56896},{\"end\":57356,\"start\":57352},{\"end\":57367,\"start\":57365},{\"end\":57382,\"start\":57378},{\"end\":57397,\"start\":57392},{\"end\":57676,\"start\":57672},{\"end\":57689,\"start\":57687},{\"end\":57705,\"start\":57700},{\"end\":57718,\"start\":57716},{\"end\":57979,\"start\":57973},{\"end\":57986,\"start\":57981},{\"end\":57995,\"start\":57990},{\"end\":58014,\"start\":58004},{\"end\":58022,\"start\":58016},{\"end\":58336,\"start\":58334},{\"end\":58355,\"start\":58343},{\"end\":58371,\"start\":58362},{\"end\":58765,\"start\":58763},{\"end\":59090,\"start\":59088},{\"end\":59105,\"start\":59100},{\"end\":59118,\"start\":59115},{\"end\":59128,\"start\":59124},{\"end\":59529,\"start\":59525},{\"end\":59540,\"start\":59538},{\"end\":59551,\"start\":59548},{\"end\":59936,\"start\":59932},{\"end\":59947,\"start\":59945},{\"end\":59958,\"start\":59955},{\"end\":60377,\"start\":60375},{\"end\":60388,\"start\":60386},{\"end\":60402,\"start\":60397},{\"end\":60413,\"start\":60410},{\"end\":60427,\"start\":60424},{\"end\":60744,\"start\":60739},{\"end\":60755,\"start\":60752},{\"end\":60767,\"start\":60764},{\"end\":60777,\"start\":60774},{\"end\":61252,\"start\":61248},{\"end\":61266,\"start\":61262},{\"end\":61279,\"start\":61277},{\"end\":61527,\"start\":61523},{\"end\":61536,\"start\":61534},{\"end\":61547,\"start\":61545},{\"end\":61556,\"start\":61551},{\"end\":61571,\"start\":61566},{\"end\":61592,\"start\":61581},{\"end\":61597,\"start\":61594},{\"end\":61900,\"start\":61898},{\"end\":61915,\"start\":61910},{\"end\":61930,\"start\":61926},{\"end\":61942,\"start\":61940},{\"end\":61957,\"start\":61952}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35342,\"start\":35233},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52157475},\"end\":35781,\"start\":35344},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":216867875},\"end\":36204,\"start\":35783},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3309938},\"end\":36563,\"start\":36206},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":215745943},\"end\":36945,\"start\":36565},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14160588},\"end\":37436,\"start\":36947},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":70289272},\"end\":37702,\"start\":37438},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":27943688},\"end\":37957,\"start\":37704},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":235870079},\"end\":38445,\"start\":37959},{\"attributes\":{\"id\":\"b9\"},\"end\":38741,\"start\":38447},{\"attributes\":{\"doi\":\"arXiv:1806.05622\",\"id\":\"b10\"},\"end\":38987,\"start\":38743},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15435392},\"end\":39340,\"start\":38989},{\"attributes\":{\"doi\":\"arXiv:1910.08854\",\"id\":\"b12\"},\"end\":39692,\"start\":39342},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":249306031},\"end\":39974,\"start\":39694},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":61807685},\"end\":40615,\"start\":39976},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":61808533},\"end\":41011,\"start\":40617},{\"attributes\":{\"id\":\"b16\"},\"end\":41336,\"start\":41013},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":158596286},\"end\":41567,\"start\":41338},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":54465873},\"end\":41930,\"start\":41569},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":232167245},\"end\":42537,\"start\":41932},{\"attributes\":{\"doi\":\"arXiv:2001.03024\",\"id\":\"b20\"},\"end\":42895,\"start\":42539},{\"attributes\":{\"doi\":\"arXiv:2108.05080\",\"id\":\"b21\"},\"end\":43177,\"start\":42897},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":55725934},\"end\":43540,\"start\":43179},{\"attributes\":{\"id\":\"b23\"},\"end\":43949,\"start\":43542},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":209341493},\"end\":44358,\"start\":43951},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":795157},\"end\":44678,\"start\":44360},{\"attributes\":{\"doi\":\"arXiv:1812.08685\",\"id\":\"b26\"},\"end\":44963,\"start\":44680},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14191515},\"end\":45401,\"start\":44965},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1136704},\"end\":45877,\"start\":45403},{\"attributes\":{\"doi\":\"arXiv:2103.10094\",\"id\":\"b29\"},\"end\":46203,\"start\":45879},{\"attributes\":{\"doi\":\"arXiv:1912.13457\",\"id\":\"b30\"},\"end\":46531,\"start\":46205},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":53298495},\"end\":46865,\"start\":46533},{\"attributes\":{\"id\":\"b32\"},\"end\":47078,\"start\":46867},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2990372},\"end\":47503,\"start\":47080},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15281206},\"end\":47868,\"start\":47505},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":229220115},\"end\":48235,\"start\":47870},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":146121036},\"end\":48706,\"start\":48237},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":53971052},\"end\":49149,\"start\":48708},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":61810032},\"end\":49544,\"start\":49151},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":220935571},\"end\":50092,\"start\":49546},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":212725677},\"end\":50619,\"start\":50094},{\"attributes\":{\"id\":\"b41\"},\"end\":50884,\"start\":50621},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":201058495},\"end\":51309,\"start\":50886},{\"attributes\":{\"doi\":\"04/07/2022\",\"id\":\"b43\"},\"end\":51485,\"start\":51311},{\"attributes\":{\"doi\":\"arXiv:2005.05535\",\"id\":\"b44\"},\"end\":51972,\"start\":51487},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":221266065},\"end\":52476,\"start\":51974},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":219687455},\"end\":53022,\"start\":52478},{\"attributes\":{\"id\":\"b47\"},\"end\":53249,\"start\":53024},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":59292011},\"end\":53795,\"start\":53251},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":15106001},\"end\":54241,\"start\":53797},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":147704268},\"end\":54633,\"start\":54243},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":14452116},\"end\":54770,\"start\":54635},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":202767986},\"end\":55159,\"start\":54772},{\"attributes\":{\"id\":\"b53\"},\"end\":55362,\"start\":55161},{\"attributes\":{\"id\":\"b54\"},\"end\":55579,\"start\":55364},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":18775029},\"end\":55898,\"start\":55581},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":219950625},\"end\":56232,\"start\":55900},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":56894332},\"end\":56791,\"start\":56234},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":102486843},\"end\":57277,\"start\":56793},{\"attributes\":{\"doi\":\"arXiv:2104.09770\",\"id\":\"b59\"},\"end\":57591,\"start\":57279},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":2578547},\"end\":57921,\"start\":57593},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":233201832},\"end\":58250,\"start\":57923},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":52955351},\"end\":58702,\"start\":58252},{\"attributes\":{\"id\":\"b63\"},\"end\":59007,\"start\":58704},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":9100570},\"end\":59468,\"start\":59009},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":53295714},\"end\":59875,\"start\":59470},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":53295714},\"end\":60282,\"start\":59877},{\"attributes\":{\"doi\":\"arXiv:2002.10137\",\"id\":\"b67\"},\"end\":60648,\"start\":60284},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":216338400},\"end\":61147,\"start\":60650},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":3759128},\"end\":61516,\"start\":61149},{\"attributes\":{\"doi\":\"arXiv:2101.11080\",\"id\":\"b70\"},\"end\":61819,\"start\":61518},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":222278153},\"end\":62313,\"start\":61821}]", "bib_title": "[{\"end\":35401,\"start\":35344},{\"end\":35838,\"start\":35783},{\"end\":36273,\"start\":36206},{\"end\":36639,\"start\":36565},{\"end\":37034,\"start\":36947},{\"end\":37523,\"start\":37438},{\"end\":37750,\"start\":37704},{\"end\":38040,\"start\":37959},{\"end\":39038,\"start\":38989},{\"end\":39754,\"start\":39694},{\"end\":40060,\"start\":39976},{\"end\":40673,\"start\":40617},{\"end\":41405,\"start\":41338},{\"end\":41579,\"start\":41569},{\"end\":42000,\"start\":41932},{\"end\":43249,\"start\":43179},{\"end\":43596,\"start\":43542},{\"end\":44050,\"start\":43951},{\"end\":44422,\"start\":44360},{\"end\":45015,\"start\":44965},{\"end\":45438,\"start\":45403},{\"end\":46593,\"start\":46533},{\"end\":47163,\"start\":47080},{\"end\":47553,\"start\":47505},{\"end\":47954,\"start\":47870},{\"end\":48363,\"start\":48237},{\"end\":48785,\"start\":48708},{\"end\":49221,\"start\":49151},{\"end\":49628,\"start\":49546},{\"end\":50172,\"start\":50094},{\"end\":50939,\"start\":50886},{\"end\":52048,\"start\":51974},{\"end\":52550,\"start\":52478},{\"end\":53312,\"start\":53251},{\"end\":53881,\"start\":53797},{\"end\":54319,\"start\":54243},{\"end\":54656,\"start\":54635},{\"end\":54816,\"start\":54772},{\"end\":55425,\"start\":55364},{\"end\":55645,\"start\":55581},{\"end\":55964,\"start\":55900},{\"end\":56297,\"start\":56234},{\"end\":56859,\"start\":56793},{\"end\":57667,\"start\":57593},{\"end\":57969,\"start\":57923},{\"end\":58328,\"start\":58252},{\"end\":58757,\"start\":58704},{\"end\":59079,\"start\":59009},{\"end\":59519,\"start\":59470},{\"end\":59926,\"start\":59877},{\"end\":60727,\"start\":60650},{\"end\":61236,\"start\":61149},{\"end\":61890,\"start\":61821}]", "bib_author": "[{\"end\":35418,\"start\":35403},{\"end\":35434,\"start\":35418},{\"end\":35453,\"start\":35434},{\"end\":35467,\"start\":35453},{\"end\":35856,\"start\":35840},{\"end\":35868,\"start\":35856},{\"end\":35884,\"start\":35868},{\"end\":35897,\"start\":35884},{\"end\":36657,\"start\":36641},{\"end\":36672,\"start\":36657},{\"end\":36687,\"start\":36672},{\"end\":36705,\"start\":36687},{\"end\":36721,\"start\":36705},{\"end\":37051,\"start\":37036},{\"end\":37068,\"start\":37051},{\"end\":37086,\"start\":37068},{\"end\":37105,\"start\":37086},{\"end\":37121,\"start\":37105},{\"end\":37541,\"start\":37525},{\"end\":37762,\"start\":37752},{\"end\":37768,\"start\":37762},{\"end\":37779,\"start\":37768},{\"end\":37785,\"start\":37779},{\"end\":38056,\"start\":38042},{\"end\":38074,\"start\":38056},{\"end\":38589,\"start\":38576},{\"end\":38802,\"start\":38780},{\"end\":38818,\"start\":38802},{\"end\":38829,\"start\":38818},{\"end\":39058,\"start\":39040},{\"end\":39074,\"start\":39058},{\"end\":39091,\"start\":39074},{\"end\":39458,\"start\":39441},{\"end\":39470,\"start\":39458},{\"end\":39482,\"start\":39470},{\"end\":39773,\"start\":39756},{\"end\":39791,\"start\":39773},{\"end\":40076,\"start\":40062},{\"end\":40088,\"start\":40076},{\"end\":40104,\"start\":40088},{\"end\":40118,\"start\":40104},{\"end\":40131,\"start\":40118},{\"end\":40147,\"start\":40131},{\"end\":40160,\"start\":40147},{\"end\":40180,\"start\":40160},{\"end\":40192,\"start\":40180},{\"end\":40209,\"start\":40192},{\"end\":40688,\"start\":40675},{\"end\":40698,\"start\":40688},{\"end\":40704,\"start\":40698},{\"end\":41114,\"start\":41092},{\"end\":41139,\"start\":41114},{\"end\":41157,\"start\":41139},{\"end\":41170,\"start\":41157},{\"end\":41423,\"start\":41407},{\"end\":41593,\"start\":41581},{\"end\":41611,\"start\":41593},{\"end\":41625,\"start\":41611},{\"end\":41640,\"start\":41625},{\"end\":42012,\"start\":42002},{\"end\":42021,\"start\":42012},{\"end\":42032,\"start\":42021},{\"end\":42045,\"start\":42032},{\"end\":42057,\"start\":42045},{\"end\":42071,\"start\":42057},{\"end\":42081,\"start\":42071},{\"end\":42092,\"start\":42081},{\"end\":42103,\"start\":42092},{\"end\":42635,\"start\":42621},{\"end\":42645,\"start\":42635},{\"end\":42653,\"start\":42645},{\"end\":42664,\"start\":42653},{\"end\":42681,\"start\":42664},{\"end\":42911,\"start\":42897},{\"end\":42926,\"start\":42911},{\"end\":42939,\"start\":42926},{\"end\":43266,\"start\":43251},{\"end\":43283,\"start\":43266},{\"end\":43609,\"start\":43598},{\"end\":43623,\"start\":43609},{\"end\":44063,\"start\":44052},{\"end\":44077,\"start\":44063},{\"end\":44443,\"start\":44424},{\"end\":44468,\"start\":44443},{\"end\":44768,\"start\":44751},{\"end\":44786,\"start\":44768},{\"end\":45035,\"start\":45017},{\"end\":45047,\"start\":45035},{\"end\":45060,\"start\":45047},{\"end\":45073,\"start\":45060},{\"end\":45453,\"start\":45440},{\"end\":45461,\"start\":45453},{\"end\":45476,\"start\":45461},{\"end\":45492,\"start\":45476},{\"end\":45503,\"start\":45492},{\"end\":45948,\"start\":45934},{\"end\":45962,\"start\":45948},{\"end\":45976,\"start\":45962},{\"end\":45990,\"start\":45976},{\"end\":46005,\"start\":45990},{\"end\":46287,\"start\":46275},{\"end\":46300,\"start\":46287},{\"end\":46310,\"start\":46300},{\"end\":46321,\"start\":46310},{\"end\":46331,\"start\":46321},{\"end\":46606,\"start\":46595},{\"end\":46617,\"start\":46606},{\"end\":46925,\"start\":46914},{\"end\":46935,\"start\":46925},{\"end\":46943,\"start\":46935},{\"end\":46956,\"start\":46943},{\"end\":46967,\"start\":46956},{\"end\":47177,\"start\":47165},{\"end\":47194,\"start\":47177},{\"end\":47201,\"start\":47194},{\"end\":47568,\"start\":47555},{\"end\":47582,\"start\":47568},{\"end\":47590,\"start\":47582},{\"end\":47602,\"start\":47590},{\"end\":47966,\"start\":47956},{\"end\":47976,\"start\":47966},{\"end\":47989,\"start\":47976},{\"end\":48002,\"start\":47989},{\"end\":48382,\"start\":48365},{\"end\":48399,\"start\":48382},{\"end\":48414,\"start\":48399},{\"end\":48430,\"start\":48414},{\"end\":48442,\"start\":48430},{\"end\":48803,\"start\":48787},{\"end\":48819,\"start\":48803},{\"end\":48831,\"start\":48819},{\"end\":48845,\"start\":48831},{\"end\":48858,\"start\":48845},{\"end\":48882,\"start\":48858},{\"end\":49237,\"start\":49223},{\"end\":49254,\"start\":49237},{\"end\":49271,\"start\":49254},{\"end\":49645,\"start\":49630},{\"end\":49667,\"start\":49645},{\"end\":49682,\"start\":49667},{\"end\":49695,\"start\":49682},{\"end\":49711,\"start\":49695},{\"end\":50189,\"start\":50174},{\"end\":50202,\"start\":50189},{\"end\":50224,\"start\":50202},{\"end\":50239,\"start\":50224},{\"end\":50252,\"start\":50239},{\"end\":50268,\"start\":50252},{\"end\":50733,\"start\":50723},{\"end\":50955,\"start\":50941},{\"end\":50968,\"start\":50955},{\"end\":50981,\"start\":50968},{\"end\":51499,\"start\":51487},{\"end\":51512,\"start\":51499},{\"end\":51531,\"start\":51512},{\"end\":51543,\"start\":51531},{\"end\":51562,\"start\":51543},{\"end\":51573,\"start\":51562},{\"end\":51583,\"start\":51573},{\"end\":51605,\"start\":51583},{\"end\":51615,\"start\":51605},{\"end\":51627,\"start\":51615},{\"end\":52071,\"start\":52050},{\"end\":52085,\"start\":52071},{\"end\":52094,\"start\":52085},{\"end\":52110,\"start\":52094},{\"end\":52119,\"start\":52110},{\"end\":52560,\"start\":52552},{\"end\":52570,\"start\":52560},{\"end\":52587,\"start\":52570},{\"end\":52600,\"start\":52587},{\"end\":52608,\"start\":52600},{\"end\":52618,\"start\":52608},{\"end\":52628,\"start\":52618},{\"end\":52642,\"start\":52628},{\"end\":53331,\"start\":53314},{\"end\":53349,\"start\":53331},{\"end\":53366,\"start\":53349},{\"end\":53383,\"start\":53366},{\"end\":53397,\"start\":53383},{\"end\":53415,\"start\":53397},{\"end\":53907,\"start\":53883},{\"end\":53927,\"start\":53907},{\"end\":53942,\"start\":53927},{\"end\":53947,\"start\":53942},{\"end\":54335,\"start\":54321},{\"end\":54349,\"start\":54335},{\"end\":54364,\"start\":54349},{\"end\":54382,\"start\":54364},{\"end\":54395,\"start\":54382},{\"end\":54411,\"start\":54395},{\"end\":54676,\"start\":54658},{\"end\":54839,\"start\":54818},{\"end\":54861,\"start\":54839},{\"end\":54878,\"start\":54861},{\"end\":54891,\"start\":54878},{\"end\":54902,\"start\":54891},{\"end\":55257,\"start\":55243},{\"end\":55443,\"start\":55427},{\"end\":55658,\"start\":55647},{\"end\":55675,\"start\":55658},{\"end\":55689,\"start\":55675},{\"end\":55980,\"start\":55966},{\"end\":55999,\"start\":55980},{\"end\":56017,\"start\":55999},{\"end\":56313,\"start\":56299},{\"end\":56332,\"start\":56313},{\"end\":56349,\"start\":56332},{\"end\":56369,\"start\":56349},{\"end\":56387,\"start\":56369},{\"end\":56878,\"start\":56861},{\"end\":56892,\"start\":56878},{\"end\":56903,\"start\":56892},{\"end\":57358,\"start\":57346},{\"end\":57369,\"start\":57358},{\"end\":57384,\"start\":57369},{\"end\":57399,\"start\":57384},{\"end\":57678,\"start\":57669},{\"end\":57691,\"start\":57678},{\"end\":57707,\"start\":57691},{\"end\":57720,\"start\":57707},{\"end\":57981,\"start\":57971},{\"end\":57988,\"start\":57981},{\"end\":57997,\"start\":57988},{\"end\":58016,\"start\":57997},{\"end\":58024,\"start\":58016},{\"end\":58338,\"start\":58330},{\"end\":58357,\"start\":58338},{\"end\":58373,\"start\":58357},{\"end\":58767,\"start\":58759},{\"end\":59092,\"start\":59081},{\"end\":59107,\"start\":59092},{\"end\":59120,\"start\":59107},{\"end\":59130,\"start\":59120},{\"end\":59531,\"start\":59521},{\"end\":59542,\"start\":59531},{\"end\":59553,\"start\":59542},{\"end\":59938,\"start\":59928},{\"end\":59949,\"start\":59938},{\"end\":59960,\"start\":59949},{\"end\":60379,\"start\":60371},{\"end\":60390,\"start\":60379},{\"end\":60404,\"start\":60390},{\"end\":60415,\"start\":60404},{\"end\":60429,\"start\":60415},{\"end\":60746,\"start\":60729},{\"end\":60757,\"start\":60746},{\"end\":60769,\"start\":60757},{\"end\":60779,\"start\":60769},{\"end\":61254,\"start\":61238},{\"end\":61268,\"start\":61254},{\"end\":61281,\"start\":61268},{\"end\":61529,\"start\":61518},{\"end\":61538,\"start\":61529},{\"end\":61549,\"start\":61538},{\"end\":61558,\"start\":61549},{\"end\":61573,\"start\":61558},{\"end\":61594,\"start\":61573},{\"end\":61599,\"start\":61594},{\"end\":61902,\"start\":61892},{\"end\":61917,\"start\":61902},{\"end\":61932,\"start\":61917},{\"end\":61944,\"start\":61932},{\"end\":61959,\"start\":61944}]", "bib_venue": "[{\"end\":35281,\"start\":35233},{\"end\":35544,\"start\":35467},{\"end\":35974,\"start\":35897},{\"end\":36305,\"start\":36275},{\"end\":36737,\"start\":36721},{\"end\":37176,\"start\":37121},{\"end\":37561,\"start\":37541},{\"end\":37822,\"start\":37785},{\"end\":38189,\"start\":38074},{\"end\":38574,\"start\":38447},{\"end\":38778,\"start\":38743},{\"end\":39146,\"start\":39091},{\"end\":39439,\"start\":39342},{\"end\":39816,\"start\":39791},{\"end\":40275,\"start\":40209},{\"end\":40793,\"start\":40704},{\"end\":41090,\"start\":41013},{\"end\":41442,\"start\":41423},{\"end\":41707,\"start\":41640},{\"end\":42184,\"start\":42103},{\"end\":42619,\"start\":42539},{\"end\":43015,\"start\":42955},{\"end\":43345,\"start\":43283},{\"end\":43700,\"start\":43623},{\"end\":44139,\"start\":44077},{\"end\":44508,\"start\":44468},{\"end\":44749,\"start\":44680},{\"end\":45140,\"start\":45073},{\"end\":45590,\"start\":45503},{\"end\":45932,\"start\":45879},{\"end\":46273,\"start\":46205},{\"end\":46685,\"start\":46617},{\"end\":46912,\"start\":46867},{\"end\":47263,\"start\":47201},{\"end\":47667,\"start\":47602},{\"end\":48039,\"start\":48002},{\"end\":48456,\"start\":48442},{\"end\":48918,\"start\":48882},{\"end\":49332,\"start\":49271},{\"end\":49777,\"start\":49711},{\"end\":50337,\"start\":50268},{\"end\":50721,\"start\":50621},{\"end\":51052,\"start\":50981},{\"end\":51369,\"start\":51311},{\"end\":51700,\"start\":51643},{\"end\":52185,\"start\":52119},{\"end\":52708,\"start\":52642},{\"end\":53094,\"start\":53024},{\"end\":53482,\"start\":53415},{\"end\":54002,\"start\":53947},{\"end\":54427,\"start\":54411},{\"end\":54681,\"start\":54676},{\"end\":54951,\"start\":54902},{\"end\":55241,\"start\":55161},{\"end\":55460,\"start\":55443},{\"end\":55722,\"start\":55689},{\"end\":56051,\"start\":56017},{\"end\":56464,\"start\":56387},{\"end\":56983,\"start\":56903},{\"end\":57344,\"start\":57279},{\"end\":57741,\"start\":57720},{\"end\":58071,\"start\":58024},{\"end\":58437,\"start\":58373},{\"end\":58836,\"start\":58767},{\"end\":59216,\"start\":59130},{\"end\":59651,\"start\":59553},{\"end\":60058,\"start\":59960},{\"end\":60369,\"start\":60284},{\"end\":60877,\"start\":60779},{\"end\":61314,\"start\":61281},{\"end\":61646,\"start\":61615},{\"end\":62025,\"start\":61959},{\"end\":41761,\"start\":41709},{\"end\":42252,\"start\":42186},{\"end\":43764,\"start\":43702},{\"end\":45194,\"start\":45142},{\"end\":45664,\"start\":45592},{\"end\":49830,\"start\":49779},{\"end\":51110,\"start\":51054},{\"end\":52238,\"start\":52187},{\"end\":52761,\"start\":52710},{\"end\":53536,\"start\":53484},{\"end\":56528,\"start\":56466},{\"end\":57050,\"start\":56985},{\"end\":58488,\"start\":58439},{\"end\":62078,\"start\":62027}]"}}}, "year": 2023, "month": 12, "day": 17}