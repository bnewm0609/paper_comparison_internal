{"id": 245123737, "updated": "2023-10-05 19:06:55.765", "metadata": {"title": "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks", "authors": "[{\"first\":\"Yi-Lin\",\"last\":\"Sung\",\"middle\":[]},{\"first\":\"Jaemin\",\"last\":\"Cho\",\"middle\":[]},{\"first\":\"Mohit\",\"last\":\"Bansal\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Recently, fine-tuning language models pre-trained on large text corpora have provided huge improvements on vision-and-language (V&L) tasks as well as on pure language tasks. However, fine-tuning the entire parameter set of pre-trained models becomes impractical since the model size is growing rapidly. Hence, in this paper, we introduce adapter-based parameter-efficient transfer learning techniques to V&L models such as VL-BART and VLT5. We evaluate our methods in a unified multi-task setup on both image-text and video-text benchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2, GQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA, How2QA, TVC, and YC2C. With careful training and thorough experiments, we benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter) against the standard full fine-tuning and the recently proposed prompt-tuning approach. We also enhance the efficiency and performance of adapters by sharing their weights to attain knowledge across tasks. Our results demonstrate that training the adapter with the weight-sharing technique (4.18% of total parameters for image-text tasks and 3.39% for video-text tasks) can match the performance of fine-tuning the entire model. Lastly, we present a comprehensive analysis including the combination of adapter and task-specific prompts and the impact of V&L pre-training on adapters. Our code is available at: https://github.com/ylsung/VL_adapter.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2112.06825", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/Sung0B22", "doi": "10.1109/cvpr52688.2022.00516"}}, "content": {"source": {"pdf_hash": "55a19318cc93714802c7ac59e07651789749b20c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.06825v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "742e47dc0381fb34c559eedc90194d77282afaca", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/55a19318cc93714802c7ac59e07651789749b20c.txt", "contents": "\nVL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks\n\n\nYi-Lin Sung ylsung@cs.unc.edu \nUNC Chapel Hill\n\n\nJaemin Cho \nUNC Chapel Hill\n\n\nMohit Bansal mbansal@cs.unc.edu \nUNC Chapel Hill\n\n\nVL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks\n\nRecently, fine-tuning language models pre-trained on large text corpora have provided huge improvements on vision-and-language (V&L) tasks as well as on pure language tasks. However, fine-tuning the entire parameter set of pre-trained models becomes impractical since the model size is growing rapidly. Hence, in this paper, we introduce adapter-based parameter-efficient transfer learning techniques to V&L models such as VL-BART and VL-T5. We evaluate our methods in a unified multi-task setup on both image-text and video-text benchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2, GQA, NLVR 2 , and MSCOCO image captioning. For video-text tasks, we use TVQA, How2QA, TVC, and YC2C. With careful training and thorough experiments, we benchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter) against the standard full finetuning and the recently proposed prompt-tuning approach. We also enhance the efficiency and performance of adapters by sharing their weights to attain knowledge across tasks. Our results demonstrate that training the adapter with the weight-sharing technique (4.18% of total parameters for image-text tasks and 3.39% for video-text tasks) can match the performance of fine-tuning the entire model. Lastly, we present a comprehensive analysis including the combination of adapter and task-specific prompts and the impact of V&L pre-training on adapters. 1 Language Model CLIP (a) Full fine-tuning \u2022 VQA Accuracy: 77.6 \u2022 Updated Param: 100% \"vqa: what is the color of the horse?\" Language Model CLIP (b) Adapter training (Ours) \u2022 VQA Accuracy: 77.4 \u2022 Updated Param: 4.18%\"vqa: what is the color of the horse?\" Adapters \"Brown\" \"Brown\"UpdatedFrozen 2 (175 \u00d7 10 9 ) \u00d7 4(bytes) \u00d7 1 10 \u22129 (GB/bytes) = 700(GB)\n\nIntroduction\n\nFollowing the success in the language domain [4,8,25,30,39,40], large-scale pre-training of vision-and-language (V&L) models has become a standard framework to tackle V&L tasks [6,7,18,32,45,48]. In such frameworks, V&L models, which are usually the combination of vision encoders and language models, are first pre-trained on large- Figure 1. Comparison of (a) full fine-tuning and our (b) adapter training for V&L tasks. By updating only a small set of adapter parameters, we can achieve similar performance to full fine-tuning. We experiment with our adapter training on diverse image-text and video-text benchmarks, and here we illustrate the VQA task as an example V&L task. scale unlabeled data, then fine-tuned for downstream V&L tasks. This is the standard strategy to fuse the knowledge of vision-and-language to the language model. However, given that such models' size grows very rapidly nowadays, either pre-training or fine-tuning of the V&L model can still contribute to an unignorable, large memory and storage cost. For instance, if we use GPT-3 [4] with 175B parameters as a backbone of V&L model, we would need 700 GB of memory to store its entire parameters. 2 To address this problem, recently, several parameter-efficient training methods have been proposed [12,16,17,20,28,33,47,52]. Among them, adapter [16] and its variants [20,33] are widely used in the NLP domain and applied to different architectures. Adapter is a small module added to intermediate layers of the model (which is illustrated in Figure 2), which allows to achieve as high performance as full finetuning (i.e., updating all parameters), by fine-tuning only a small set of parameters. Moreover, this also shows that it is possible to use a few parameters to learn the information fusion of vision and language without losing performance. Despite adapters having achieved success in text classification [16,20,33] and image-text alignment [2], to the best of our knowledge, no work has utilized this efficient method for more challenging downstream V&L problems, such as visual/video question answering and image/video captioning. Besides, V&L models often come with expensive computations by combining the knowledge of two input modalities. Hence, we investigate the application of adapter-based parameter-efficient training techniques to V&L tasks.\n\nWe aim to efficiently tune language models on diverse downstream V&L tasks while achieving performance comparable to full fine-tuning. For this, we analyze these parameter-efficient training techniques in a unified multitask learning setup, and we benchmark different adapter [16,20,33] and prompt-based methods [24]. For our V&L model, following Cho et al. [7], we adopt encoder-decoder language models (BART [25] and T5 [40]) that tackle multiple V&L tasks as text generation to avoid designing taskspecific architectures. We use CLIP [37], a pretrained image-text alignment model, as our visual encoder for the ease of doing V&L pre-training. To inform the model about which task it is going to perform, we follow [7,40] to add task-specific (text) prompts to the front of the input sentence (e.g., \"vqa: [Q]\" for VQA). We then insert Adapter [16] and its variants, Hyperformer [20] and Compacter [33], into the model to perform parameter-efficient training. Hyperformer and Compacter are recently proposed state-of-the-art approaches: Hyperformer improves the efficiency of adapters by generating their weights via a hyper-network, while Compacter reduces the parameters by utilizing Kronecker products and low-rank parameterization for the adapters' weights. We also compare adapter-based approaches with prompt tuning [24], which adds trainable prompts to the input. We show the high-level concept of our work in Figure 1. Practically, adapter training involves parameter updates of adapter modules, layer normalization layers, and the visual projection layer (see Section 3.1 and Figure 2 for more details). Since we tackle multiple tasks simultaneously [20,36], we also explore taking advantage of the sharing of information between tasks on adapters and prompts. Specifically, we make some of the trainable parameters to be shareable to learn cross-task information while reserving the rest of them for task-specific information. With this technique, the number of trainable parameters can be reduced even further.\n\nWe conduct our experiments and analysis on four diverse image-text tasks: VQAv2 [10], GQA [19], NLVR 2 [46], and MSCOCO captioning [5]. For completeness, we also verify the effectiveness of our framework on four videotext tasks: TVQA [22], How2QA [26], TVC [23], and YC2C [56]. Overall, the performance of the three adapterbased approaches closes the gap between which of full fine-tuning. In our experiments, Compacter does not stand out in terms of efficiency since we remove the low-rank approximation for trading performance. Hyperformer is more efficient than adapters, but we eventually show our adapter training with the weight-sharing technique can achieve the same performance as full fine-tuning while only updating 4.18% of the entire parameters for image-text tasks (and 3.39% for video-text tasks). Next, we compare the finetuning and freezing of the CLIP parameters, where the latter achieves a better trade-off between performance and parameter efficiency. We also present a detailed analysis to understand the contribution of each trainable component in adapters, as well as the different parameter-sharing mechanisms. We find that using a single set of adapter modules across all tasks achieves the best results and accuracyefficiency trade-off, showing the possibility of pursuing efficiency with simplicity ( Fig. 4). Since most of the experiments are based on the pre-trained weights accompanied with the models (e.g., CLIP pre-trained weights for ResNet and BART pre-trained weights), we also demonstrate that the results of training adapters on top of V&L pre-trained weights can match or even exceed which of the full finetuning counterpart. While we conduct most of our experiments with the V&L generation framework, we also extend the adapter training to CLIP-ViL [45], which is one of the SOTA discriminative V&L approaches. Lastly, we report the results of comprehensive hyperparameter search in Appendix C, hoping that they will be useful for related research on parameter-efficient training.\n\nOur contributions could be summarized as: (1) the first work benchmarking different types of parameter-efficient training techniques (Adapter, Hyperformer and Compacter) for diverse challenging downstream image-text and videotext tasks; (2) empirical demonstration of adapters reaching the performance of full fine-tuning while updating only 3.39-4.18% of the parameters; (3) comprehensive analysis on the design of freezing CLIP, impact of different architectural components, weight-sharing techniques, task-specific prompts, and vision-language pretraining.\n\n\nRelated Work\n\nOur research is built upon previous works about language models, V&L models, and parameter-efficient training. In this section, we introduce previous literature and discuss their similarities and differences with this work.\n\nLanguage Model Pre-training. The workflow of pretraining and fine-tuning has become a popular paradigm for solving many downstream tasks in the NLP field. Several papers accordingly propose new architectures and objectives to model language, such as ELMo [35], BERT [8], RoBERTa [31], GPT series [4,38,39], and encoder-decoder versions such as BART [25] and T5 [40]. Comprehensive studies [40] have shown the effectiveness of the encoderdecoder model compared to other architectures. Hence, we choose BART and T5 as our text generative model. V&L Pre-training. To tackle V&L tasks, most existing approaches combine respective models specialized for either pure language or pure vision tasks as a V&L model. ViL-BERT [32], UNITER [6] and LXMERT [48] use the faster R-CNN [43] object detector to extract the bottom-up [1] features from the image and provide them with text features to a cross-modality transformer to solve visual question answering and image-text alignment tasks. Cho et al. [7] adopt encoder-decoder language models [25,40] to generalize V&L tasks to text generation with a task-agnostic architecture, which combines faster R-CNN object detector and T5 (or BART). In practice, the R-CNN is usually frozen in most of the V&L models because the end-to-end training of the R-CNN and language models is unstable. This prevents R-CNN from adapting its weights to V&L tasks. Pix-elBERT [18] replaces R-CNN with plain ResNet [14], and demonstrates the advantage of including the vision model in training. Inspired by the success of pre-training with webscale unlabeled data in the NLP field, Radford et al. [37] propose CLIP to show the success can be transferred to the V&L field. With the pre-training on 400 million image-text pairs, CLIP has a rich cross-modal representation and can solve a wide range of tasks without additional supervision. CLIP-ViL [45] explores the advantage of using a CLIP visual encoder (ResNet or ViT [9]) for V&L tasks.\n\nAlthough language models are tuned for downstream tasks in previous works, some recent research [49,51] attempts to freeze large language models (e.g., GPT-3) to achieve zero-shot learning for V&L tasks. This line of research focuses on how to map images to the inputs that the language model can use. Frozen [49] achieves this by jointly training an NF-ResNet-50 [3] and frozen GPT-3 with the Conceptual Captioning dataset [44]. Instead of aligning images to text features, Yang et al. [51] directly utilize a pre-trained image captioner to transform images to text, which is a useful resource for a language model, and they demonstrate the effectiveness of this framework on visual question answering tasks. Our V&L model is the combination of CLIP and BART (or T5). We conduct a thorough ablation study to test the performance of four different training scenarios: all possible pairs of training or freezing CLIP and BART. The results in Section 5.2 show that fine-tuning (i.e., not freezing) the language model is crucial to achieve competitive performance on diverse downstream V&L tasks, which is why we focus on how to achieve this much more efficiently via different adapter methods and knowledge sharing across tasks. Our results also show that training the BART model only has the best trade-off between performance and parameter efficiency, hence, even though the architecture is end-to-end trainable, we decide to freeze the CLIP to fulfill our goal of parameter-efficient training.\n\nParameter-Efficient Training. As the size of recent models increases rapidly, updating the models in parameterefficient ways becomes crucial. Recently, three types of methods have been proposed: (1) only updating newly added parameters (added either to the input or model); [16,20,24,28,33]; (2) sparsely updating a small number of parameters of the model; and [12,47,52] (3) low-rank factorization for the weights to be updated [17]. [13,34] combine such approaches to propose a unified parameterefficient training framework. Among these approaches, adapters, which belong to the first category, are widely used in computer vision [41,42] and natural language processing [16,20,33]. While adapters add additional parameters into models, prompt-based approaches instead add trainable parameters into the inputs [11,24,28], and experiments have shown their value in language tasks. Some concurrent works also extend parameter-efficient techniques to CLIP models [2,54,55]. However, they mainly tackle image-text alignment problems, while in this paper we aim at more challenging downstream V&L tasks, such as visual/video question answering, visual reasoning, and image/video captioning. The typical usage of adapters is training them independently on different tasks. This training manner prevents these adapters from learning the shared information across tasks. In this paper, we find that making these convenient plug-and-play adapters shareable improves the performance of a low resource dataset and reduces the overall trainable parameters.\n\n\nMethods\n\nOur main contribution is to exploit adapter-based methods to efficiently fine-tune generative models in the multitask setting. We explore several state-of-the-art variations of adapters (Adapter, Hyperformer, Compacter) on various V&L tasks and show that vanilla adapters are the best among them. We further demonstrate that sharing adapters across tasks can boost performance to match full fine-tuning results and further improve the parameter efficiency.\n\n\nUnified Framework for V&L Tasks\n\nWe illustrate our V&L model in Figure 2(a). We follow [7] to unify V&L tasks to a text generation problem. Our V&L model is a combination of CLIP and BART (T5), and therefore we name our base architecture CLIP-BART (CLIP-T5). To be more specific, assuming that we have a pair of an image (or video) x I and a sentence x S (e.g. the question in VQA) as the input for our model, our goal is to maximize the agreement between the model's output and the text label of M tokens y = (y 1 , y 2 , ..., y M ) (e.g., the ground truth answer in VQA). Regarding archi- tectures, we use an encoder-decoder language model (parameterized by \u03b8 L ) as our main generative model. We connect a CLIP (parameterized by \u03b8 V ) and a visual projection layer (parameterized by \u03b8 V \u2192L ; it projects the visual representation to the correct dimension for language model) to the model for extracting the visual representation from input images and feed the concatenation of visual representation and sentence representation to the encoder-decoder model. The multi-head attention layers inside the encoder learn the cross-modality representations and the decoder utilizes them to generate the targeted text by maximizing its likelihood. Note that the sentence representation is the output of an embedding layer, and positional embeddings are added to both visual and sentence representation. For simplicity, we omit the notations for the embedding layer and positional embedding since they can be viewed as part of \u03b8 L and \u03b8 V . With the datum and model's parameters, our goal is to minimize the cross entropy (CE) loss:\nl x I , x S , y; \u03b8 L , \u03b8 V , \u03b8 V \u2192L = CE(f \u03b8 L (x V \u2192L , x S ), y) = \u2212 M i=1 y i log(f \u03b8 L (x V \u2192L , x S ) i )(1)\nwhere f \u03b8 denotes a function parameterized by \u03b8, and x V \u2192L is the projected visual representation, that is,\nx V \u2192L = f \u03b8 V \u2192L (f \u03b8 V (x I )).\nWe next introduce our multi-task setup for the unified generative model. Benefiting from the unified format, we can construct a universal dataset D from N given V&L datasets, D 1 , D 2 , ..., D N . Thus, we optimize our parameters by minimizing the averaging loss of datum in D:\nL D; \u03b8 L , \u03b8 V , \u03b8 V \u2192L = 1 |D| (x I ,x S ,y)\u2208D l x I , x S , y; \u03b8 L , \u03b8 V , \u03b8 V \u2192L(2)\nOur trainable parameters are the union of \u03b8 L , \u03b8 V , and \u03b8 V \u2192L . As deep learning models have grown rapidly in recent times, updating and storing the whole parameters of either visual or language models can be inefficient. Therefore, this motivates us to introduce adapter-based approaches into V&L models for parameter-efficient tuning.\n\n\nAdapters for V&L Models\n\nAdapters. Figure 2(b) left. Adapters [16] are sub-networks with small parameters that are inserted after every attention and feed-forward layer in a model. With adapters, the models learn downstream tasks by updating only a small number of parameters. The adapters consist of a pair of downsampling and upsampling layers, and a residual connection.\n\nTo be more specific, we denote the input of the adapter as x \u2208 R di , and the weight matrices for downsampling and upsampling layers to be \u03b8 D \u2208 R di\u00d7d and \u03b8 U \u2208 R d\u00d7di , where d i and d are the input and hidden dimensions, respectively. The mechanism of adapters is defined as:\nh = f \u03b8 U (\u03c3(f \u03b8 D (x))) + x(3)\nwhere \u03c3(\u00b7) is an activation function, and we use GELU [15] in this paper. With adapters, the parameter complexity (i.e., the number of added parameters) is O(d i d), and it usually is 2 \u223c 3% of the whole model's parameters. Note that all layer normalization layers are also updated to adapt to the data distribution of downstream data. Hyperformers. Figure 2(b) top-right. The typical usage of adapters is to separately train one adapter for one task. In that fashion, adapter modules are independent across tasks, preventing the possibility of reducing the required parameters by sharing the weights for similar tasks. Hence, in order to make the adapter module even more efficient, we extend the Hyperformer [20] to a V&L architecture. More specifically, we maintain a hyper-network that is shared over tasks to generate the adapters' weights conditioned on the task and the index of the layer. Suppose that we have N T tasks at hand, the number of layers of the model is N L , and we can use t 1 , t 2 , ..., t N T \u2208 R de and l 1 , l 2 , ..., l N L \u2208 R de to represent their d e dimensional embeddings. Hyperformer is composed of a two-layer task projector network \u03b8 T \u2208 R de\u00d72\u00d7dp and the hyper-network \u03b8 H \u2208 R dp\u00d7(2\u00d7d\u00d7di) , which aims to generate the weights for the upsampling and downsampling layer in adapters based on the projected embedding. Without loss of generality, to generate the adapter's weights in the i th layer for j th task, the generation process is:\n[\u03b8 D , \u03b8 U ] = f \u03b8 H (f \u03b8 T ([t j , l i ]))(4)\nwhere [\u00b7] refers to concatenation. Note that to save memory with Hyperformer, the number of trainable parameters of Hyperformer needs to be smaller than that of adapters,\nnamely, |\u03b8 H |+|\u03b8 T |+N T |t|+N L |l| < N T N L (|\u03b8 U |+|\u03b8 D |).\nIn general, we have N T |t|, N L |l|, |\u03b8 T | \u226a |\u03b8 H |, so we can further induce the appropriate range of d p , which is\nd p < N T N L .\nCompacters. Figure 2(b) bottom-right. Although adapters have attained great success on parameter-efficient training, they still have redundant parameters and usually underperform full fine-tuning. Compacter hence is introduced by [33] to solve the issues with the matrix decomposition and parameter sharing, and eventually, Compacter has been shown to have a better trade-off between performance and efficiency compared to adapters. In the following, we demonstrate the mechanism of Compacter with the weights of the downsampling layer. First, Compacter introduces parameterized hypercomplex multiplication layers (PHM layers) [53], whose parameters are the decomposition of \u03b8 D \u2208 R di\u00d7d to the sum of k Kronecker products:\n\u03b8 D = k i=1 A i \u2297 B i(5)\nwhere\nA i \u2208 R k\u00d7k , B i \u2208 R d i k \u00d7 d k .\nThe parameter complex- ity of the PHM layer is O( did k ), reducing the cost by at most 1 k . To further improve the efficiency of PHM layers, Compacter shares the parameters of a smaller matrix A i across all layers, and decomposes the bigger matrix B i even more with low-rank parameterization. Specifically, the matrix B i is approximated to be a low-rank matrix, which is the product of two low-rank matrices,\nu i \u2208 R d i k \u00d7r and v i \u2208 R r\u00d7 d k ,\nwhere r is the matrix's rank. This results in low-rank parameterized hypercomplex multiplication layer (LPHM):\n\u03b8 D = k i=1 A i \u2297 B i = k i=1 A i \u2297 (u i v i )(6)\nEmpirically, r = 1 is sufficient to achieve competitive performance, obtaining the complexity of the LPHM layer as O( d+di k ). Nevertheless, we find sharing matrix and lowrank decomposition in LPHM layers both severely hurt the performance of V&L tasks, so we remove them and use the PHM layers instead. Shared-Weight Adapters. Figure 3. Inspired by Hyperformer, we explore the sharing of information between N T tasks in vanilla adapters with the weight-sharing technique. We denote \u0398 = {\u0398 D , \u0398 U }, is the collection of all inserted adapter modules' weights in the model, and \u0398 D (\u0398 U ) is the subset that only includes downsampling (upsampling) layers in adapters. As we have mentioned earlier, adapters are trained independently, so we have unique {\u0398 D i , \u0398 U i } for the i th task. To enable the adapter to learn cross-task information, we make part of the weights of the adapter to be shareable. For instance, we can make \u0398 D i equal to \u0398 D j (i = j), and the rest of parameters (\u0398 U i ) can still learn the task-specific information for the i th task. Note that we also consider the extreme case of using a single adapter for all tasks (\u0398 i = \u0398 j ). We illustrate different weight sharing methods for adapters in Figure 3. Where to Add Adapters? Our goal is to apply adapters into V&L models to efficiently update their parameters.\n\nSince our architecture is composed of both visual and language components, it is expected that adapter layers are injected into both of them. However, we observe that even fully fine-tuning the whole model does not bring much improvement compared to only updating the language one (see details in Section 5.2). Since freezing CLIP has the better trade-off between performance and parameter efficiency over training it, we eventually do not add adapter layers into CLIP.\n\nMulti-task Adapter Variants. We consider several approaches to using adapters. Since we are in a multi-task setting, the first two straightforward methods are training adapters and Compacter per task, and we call Multiple Adapters and Multiple Compacters (illustrated in Figure 3(a)). To allow adapters to learn information across tasks, we use weight sharing techniques mentioned in Section 3.2 to form Half-shared Adapters, illustrated in Figure 3(b). Notably, we can form the two types of Half-shared Adapters by sharing either upsampling layers or downsampling layers. However, we use Half-shared Adapters to represent adapters with sharing upsampling layers, since they have almost no differences in terms of accuracy and efficiency. We also consider the extreme case of training multiple tasks with one set of adapter layers, and this brings Single Adapter and Single Compacter (see Figure 3(c) for details). Lastly, we have Hyperformer which essentially shares information from multiple tasks. Note that we always update \u03b8 V \u2192L (1.14% of parameters) and all layer normalizations (0.04% of parameters) in the language model. We freeze the output layer, whose weights are tied with word embeddings, because it occupies about 30% of the language model's parameters, and updating it doesn't come with a performance boost.\n\n\nExperimental Setup\n\nDatasets. For image-text experiments, we evaluate our models on four V&L datasets: VQAv2 [10] and GQA [19] for visual question answering, NLVR 2 [46] for visual reasoning, and MSCOCO [5] for image captioning. As for video-text experiments, we apply our method on four tasks from VALUE [27] benchmark: TVQA [22] and How2QA [26] for video question answering, TVC [23] and YC2C [56] for video captioning. The statistics of each dataset are shown in Table 3. Architecture Details. We follow [7] to combine the vision encoder and an encoder-decoder language model to deal with many tasks via the unified text generation framework.\n\nFor image experiments, we use CLIP-ResNet101 as our vision encoder [37]. Input images are resized to 224 \u00d7 224 for the memory efficiency. We extract the 7 \u00d7 7 grid features produced by the last convolutional layer, and then apply adaptive maximum-pooling over the features for down-sampling then to 6 \u00d7 6 for a fair comparison to [7]. For video experiments, we use the features extracted by CLIP (ViT-B/32) following [27], where they uniformly sample one frame per second and concatenate the CLIP outputs of the sampled frames to form the visual input. We limit the maximum length of visual input to 64 for efficiency. Following [27], we also include subtitles as additional information. In addition, we choose BART base [25] as our main encoder-decoder language model, but we also extend the studies to T5 base [40]. We use CLIP-BART and CLIP-T5 for representing these two V&L architectures. Training and Evaluation. We perform an extensive hyperparameter search for our models. See Appendix C for details. We use AdamW to train the model, unless we additionally specify and apply a linear decay scheduler. We train the models for 20/7 epochs for image-text/video-text tasks, and warm up the learning rate from 0 to the highest learning rate in the first 2 epochs. In the image-text experiments, we use batch size 500 for CLIP-BART and 250 for CLIP-T5, and the total training time is about 20 hours and 40 hours for CLIP-BART and CLIP-T5 with one A6000 GPU (48G memory), respectively. In video-text experiments, we use batch size 50 for CLIP-BART and the total training time is about 10 hours. We select the last checkpoint for evaluation and report the evaluation score of the four tasks as well as the average score in our experiments. The percentage of updated parameters is also reported as the metric for approaches' efficiency, and we do not take visual encoder into account for computation since it is frozen.\n\n\nResults and Analysis\n\n\nMulti-Task Parameter-Efficient Fine-Tuning\n\nNext, we move on to our main experiments on applying parameter-efficient training techniques for V&L models. We note that the techniques are only used for the language model, as we mentioned in Section 3.2. Besides the adapter-based methods and full fine-tuning, we also consider prompt-tuning [24] and LoRA [17], which are competitive parameter-efficient training approaches. In this case, we also have two variants for each method, that is, Single Prompt-Tuning, Multiple Prompt-Tuning, Single LoRA and Multiple LoRA, where the single one uses the same prompt/low-rank weights for multiple tasks while the multiple one has one prompt/low-rank weights for each task, respectively. The prompts are only added to the input of the encoder because we do not see improvement when the prompts are also added for the decoder. In the following paragraphs, we separately introduce the results in Table 1 and their takeaways. We also report the leaderboard results of multiple approaches on VQA and GQA in Table 2, where we select the representative approaches from each family of methods based on their performance in Table 1 still holds similar to the results in Table 1.\n\n\nDifferent Visual Representations.\n\nWe compare the results of CLIP-BART and VL-BART (w/o pre-training) and find out that there is a small improvement in using CLIP features over R-CNN features (77.6 vs. 76.7). Note that our CLIP also uses images with a smaller size (224 \u00d7 224 vs. 800 \u00d7 1333), so the result proves the effectiveness of pre-trained cross-modality features.\n\nSingle Adapter Performing the Best. We observe that the vanilla adapter is the most competitive architecture among the three types of adapter variants. We find Half-shared Adapters perform on par with Multiple Adapters with fewer parameters, and the Single Adapter's performance is as competitive as which of the full fine-tuning (77.4 vs. 77.6). The performance boost of Half-shared Adapters and Single Adapter mainly comes from a smaller dataset, NLVR 2 , and this demonstrates that information sharing benefits the low resource tasks. We next turn to the results of Hyperformer and Com-pacter. The Hyperformer shares information across tasks in the hyper-network, thus resulting in that it is more parameter-efficient than the Multiple Adapters (12.22% vs. 5.79%). However, the Single Adapter still outperforms Hyperformer in terms of its parameter-efficiency (4.18% vs. 5.79%) and effectiveness (77.4 vs. 76.4). The optimization of hyper-network is harder and it might be one of the reasons to produce this outcome. Compared to Adapter, Compacter does not stand out in our experiments. We hypothesize the reason causing the outcomes is: our BART model is pre-trained on pure language tasks, and we would like to adapt the model to perform on V&L tasks. Nevertheless, the assumption of Kronecker products might be too restrictive, so that Compacter fails to overcome the huge discrepancy between tasks. To have a complete comparison between three adapter-based approaches, in Figure 4, we show their performance over different percentages of updated parameters. We observe that Single Adapter, despite its simple architecture, achieves the best accuracyefficiency trade-off.\n\nLastly, we transfer the best configuration of the single adapter to CLIP-T5 and show the results in Table 4. Note that we use a larger hidden dimension for adapters in this case, and the percentage of updated parameters is 7.98%. The results conclude that the Single Adapter still achieves a promising accuracy-efficiency trade-off in T5. We leave the results of adding other approaches to T5 in Appendix C.2. LoRA and Prompt-tuning vs. Adapters. Compared to the Single Adapter, LoRA uses slightly more parameters and the accuracy drops by approximately 1%. However, it is still a competitive approach since its performance is    better than other methods. In general, prompt-tuning does not perform well in our experiments. The reason might be similar to Compacter's: because the pre-trained tasks and downstream tasks are dissimilar, the model cannot adapt to the distribution of new datasets with few parameters. Also, prompt-tuning is sensitive to some training configurations such as model size, prompt initialization, and pre-training methods [24]. We leave the improvement of prompt-tuning performance for future work. VL-Adapter in Video-Language Understanding Tasks. Table 6 shows the performance of the representative approaches used in Table 2 on TVQA, How2QA, TVC, and YC2C. We transfer the hyperparameter setup used in Table 1, but we divide the learning by 3 for all approaches for better performance. Single Adapter again attains the best results among all parameter-efficient methods and is on par with full fine-tuning. From the image-text and video-text experiments, we find that the Adapter is more stable than other approaches.\n\n\nTraining or Freezing Visual Encoder\n\nBecause the use of CLIP enables the entire model to be end-to-end trainable with stability [45], we conduct experiments to test the performance of four different training scenarios: all possible pairs of training (full fine-tuning) or freezing CLIP and BART. Every combination is trained and evaluated on VQA. We apply the training tricks 3 used in [18,45] and report the results in Table 5. The results show that there is only 1% improvement with adding the CLIP into training. Given this result, the advantage of adding adapters inside CLIP over keeping it frozen is limited. Therefore, we have determined to freeze the CLIP consistently to save memory, and this also ideally fits our purpose of training models efficiently. We also report the result of our version of \"Frozen\" [49] 4 , where we freeze BART and only fine-tune CLIP. However, the accuracy of 39.4 (in Table 5) is far from that of updating BART, suggesting that fine-tuning the language model is still crucial.\n\n\nAblations and Analysis\n\nIn the following paragraphs, we sequentially present our ablation studies and analysis on (1) the contribution of different modules. (2) adapters with task-specific prompts. (3) effect of V&L pre-training on adapters. (4) adapters on the discriminative V&L model. Contribution of Modules (\u03b8 V \u2192L , Layer Norm). Recall that our adapter training not only includes the adapters' modules but layer normalization layers and the visual projection layer \u03b8 V \u2192L . To have a better understanding of the contribution to the accuracy of adapters, we conduct ablation studies to gradually add trainable modules and compare their results in Table 7. We observe that only updating \u03b8 V \u2192L produces insufficient results, suggesting the need to update the language model partially. We do find a considerable improvement with updating layer normalization layers, but the accuracy is still much behind that of the full fine-tuning, and this result also displays the effectiveness of adapters. We note that this finding deviates from the conclusion in [2], where updating layer normalization layer is comparable or even better than training adapters inside CLIP for the image classification task.\n\nAdapters with Task-specific Prompts. We experiment to remove task-specific prompts before the input sequence, namely, from \"[task]: [input]\" to \"[input]\", where [task] is task indicator, such as \"vqa\", \"gqa\", \"nlvr\", and \"caption\". The ablation is only for the approaches using one set of parameters for multi-tasking, such as full fine-tuning and Single Adapter. We exclude Hyperformer in this experiment since we follow the original implementation to remove all prompts and use task embedding as the condition.\n\nThe results of whether to use task-specific prompts are displayed in Table 8. We find that using prompts can improve performance, and the improvement likely comes from resolving the confusion between tasks. However, the model still performs well without prompts. We hypothesize that the data distribution between tasks is large enough for the model to understand to treat them differently, so the added prompts might become redundant. For example, there is no text input in MSCOCO, while there are two input images in NLVR 2 .\n\nEffect of V&L Pre-training on Adapters. Since this work is about low-cost training, most of our experiments are based on the weights without V&L pre-training. This results in a performance gap between the approaches in this paper and state-of-the-art ones. We therefore also explore whether adapter training can take advantage of V&L pretrained weights. We follow [45] to pre-train on COCO [29] and VG [21] images with multi-modal language modeling,   Table 9. The fine-tuning results of full fine-tuning and Single Adapter after pre-training on V&L tasks first.\n\nvisual question answering, and image-text matching. We exclude the referring tasks (grounded captioning and visual grounding) because they need bounding box information, which cannot be obtained using CLIP. Refer to the training details in [45]. Table 9 shows the fine-tuning results with V&L pre-training. In this case, the Single Adapter even is more competitive than full fine-tuning with only a few parameters being trained, suggesting that the adapters work well with different kinds of pre-trained weights.\n\nAdapters on the Discriminative V&L Pre-trained Model. While most of our experiments and ablations are conducted on generative V&L models, we also applied adapters to CLIP-ViL [45], a SOTA discriminative model, and report the results of VQA [10], SNLI-VE [50], and GQA [19] in Table 10. Adapters are also not added into CLIP due to the same reason in Section 5.2. Note that Single Adapter only updates from 4.3 to 6.2% of parameters with only a small gap in accuracy compared to full finetuning, which demonstrates the effectiveness of adapters for discriminative architectures.  Table 10. The results of adding adapters to CLIP-ViL on VQA, SNLI-VE, and GQA. Note that the number of parameters vary across tasks due to different output heads.\n\n\nDiscussion and Conclusion\n\nWe conduct comprehensive studies on the evaluation of three adapter-based approaches on challenging V&L (image-text and video-text) tasks. We employ a unified format and architecture to solve the tasks in a multi-tasking learning setup. With a thorough hyper-parameter search, we benchmark the performance of those methods and find the Single Adapter, which is a shared-weight vanilla adapter, is the best in terms of accuracy, efficiency, and simplicity. We have also shown that Single Adapter works well with a state-of-the-art discriminative model (CLIP-ViL). Lastly, we conduct ablation studies on understanding the contribution of different trainable modules, adapters with taskspecific prompts, and the effect of V&L pre-training on adapters.\n\nNext, we discuss some limitations of this work. We have carried out extensive experiments on four V&L tasks with our proposed CLIP-BART and CLIP-T5. However, different architectures have their own best hyper-parameters, and data distributions are varied across tasks, so our results and findings do not always guarantee to be generalized to other tasks. Furthermore, we experiment with the three popular adapter variants, but they cannot represent all the adapterbased approaches.\n\nIn this appendix, we first explain our prompt-tuning experiment setup with details (Appendix A). Then we show the experimental results of the complete hyper-parameter search for the adapter-based and prompt-based techniques (Appendix C).\n\n\nA. Details for Prompt-tuning\n\nPrompt-tuning [24] adds trainable parameters to the encoder's inputs for adapting those parameters for new tasks without changing the model. Specifically, we assume the input indices for generating prompts are 1, 2, ..., N p \u2208 N, where N p is the length of prompts. We next apply a three-layer neural network to transform the prompts embeddings to the correct dimension for the language model. The first layer is an embedding layer, parameterized by \u03b8 E \u2208 R Np\u00d7di , and the rest of the two layers are parameterized by \u03b8 D \u2208 R di\u00d7d and \u03b8 U \u2208 R d\u00d7di . Since the architecture of the prompt network is quite similar to the adapter module, we use the same notations as we used in adapters for simplicity. The mathematic form can be written as the following,\nh = f \u03b8 E (p) h p = f \u03b8 U (\u03c3(f \u03b8 D (h)))(7)\nwhere p \u2208 1, 2, ..., N p , h p being the prompt of index p, and we use Tanh as the activation function. Next, we can combine the prompt embeddings with vision and sentence embedding, feed-forwarding to the model, and train them with backpropagation. The trainable parameters consist of the input prompts embeddings and the parameters of the threelayer neural network. Note that unlike in adapter modules that d is smaller than d i for saving memory, d in the prompt network is sometimes greater than d i since it is the main hyper-parameter to increase the number of trainable parameters. The length of the prompt N p does not contribute much to the number of parameters since it only influences the embedding layer, which usually is a small layer. Thus, using longer prompts is a parameter-efficient method to  Table 11. The multi-task evaluation results for CLIP-BART on VQA, GQA, NLVR 2 and COCO Caption between adapter-based approaches with different hyper-parameters. We bold the highest average accuracy separately for each approach, and we also bold the best configuration we used in the main paper. Note that we don't use V&L pre-training for every model. * denotes the NLVR results might be improved if we use different learning rates.\n\ntrain models. However, the memory usage would increase significantly with longer prompts due to the quadratic cost of attention layer on input lengths. For a fair comparison, we maximize N p to use the same amount of memory as being used in adapter-based approaches (around 40 GB).\n\n\nB. Details for LoRA\n\nAssume the initial weight for a layer is \u03b8 di\u00d7do , LoRA [17] learns two low-rank matrices A \u2208 R di\u00d7d and B \u2208 R d\u00d7do (d d i , d o ) to approximate the weight's updates, that is\n\u2206\u03b8 = AB(8)\nThe output of this layer can be written as f \u03b8+AB (h). Hu et al. [17] apply this trick to attention layers (not in feed-forward layers), and they also update bias terms of the model. Compared to the original model, using adapters or prompt-tuning, which modify the network or inputs, causes extra computation in the inference. However, there is no extra overhead in LoRA since we can add the updates back to the model after training.\n\n\nC. Hyper-parameter Search\n\nWe search over the learning rates among {1 \u00d7 10 \u22124 , 3 \u00d7 10 \u22124 , 1 \u00d7 10 \u22123 } for each hyper-parameter configuration. To reduce the cost of searching, we utilize a heuristic logic: we first search for the best learning rate for the one hyperparameter configuration (randomly chosen) and then use the same learning rate for other configurations. We perform another learning rate search only if the results are diverged for some tasks (e.g. sometimes the results of NLVR 2 become very low at certain learning rates).\n\nFor the Adapter, the only hyper-parameter is the hidden dimension d. We also ablate two variants of Half-shared Adapters: sharing upsampling or downsampling layers. We include the search about the projected hidden dimension d e for the task projector network in the Hyperformer. Regarding the Compacter, we have tried different numbers of Kronecker products (k), hidden dimension d, and whether sharing weights and using low-rank parameterization. We also tune the d m for prompt-tuning.\n\n\nC.1. CLIP-BART Hyper-parameter Search\n\nWe show the results of the hyper-parameter search in Table 11. We bold the final configurations used in the main paper and we also list the configurations in Table 13. The exception is that we use the same hyper-parameters for the \"Single\" and \"Multiple\" approaches. For example, even though Multiple Prompts perform better when d m = 100, we still use d m = 800 for both Multiple Prompts and Single Prompt for consistency (J and K rows in Table 11).\n\n\nC.2. CLIP-T5 Hyper-parameter Search\n\nWe display the results of the hyper-parameter search for CLIP-T5 in Table 12 and final configurations for each method in Table 13. We find that the Compacter (F.1 in Table 12) shows the different fashion in T5: it can perform similarly to the Single Adapter (C.2 in Table 12) using fewer parameters. This might because the Compacter is mainly validated on T5 in [33].\n\n\nD. Learderboard Results of the Test-dev split for VQA\n\nWhile we use the Karpathy split for VQA evaluation in the main content, we also report the test-dev results for all approaches in Table 14. In short, the trend remains similar as using the Karpathy split, and the Single Adapter still performs the best among parameter-efficient training methods.   \n\nFigure 2 .\n2Illustration of (a) our unified framework for V&L tasks (Sec. 3.1) and and (b) three-adapter based modules (Sec. 3.2). Green color refers to trainable parameters and blue color refers to frozen ones.\n\nFigure 3 .\n3Illustration of different weight sharing methods for adapters. Parameters with same color are shared across tasks.\n\nFigure 4 .\n4The comparison of three adapter-based approaches over the different percentages of updated parameters. We adjust the hidden dimension d to attain the model with different sizes.\n\n\n. The trendTable 1. The multi-task evaluation results on VQA, GQA, NLVR 2 and COCO Caption between full fine-tuning, adapter-based approaches, prompt-tuning, LoRA, and VL-BART. We bold the highest scores separately for approaches that are with or without parameter-efficient training techniques. We also report the results of the test-dev split on VQA in Appendix D, and the trend is similar to using the Karpathy test set. Note that we don't use V&L pre-training for every model.Method \n\nUpdated \nParams \n(%) \n\nVQA \nKarpathy test \nAcc. (%) \n\nGQA \ntest-dev \nAcc. (%) \n\nNLVR 2 \ntest-P \nAcc. (%) \n\nCOCO Cap. \nKarpathy test \nCIDEr \n\nAvg. \n\nVL-BART [7] \n100.00 \n67.8 \n57.3 \n72.3 \n109.4 \n76.7 \nCLIP-BART \n+ Full fine-tuning \n100.00 \n67.6 \n56.7 \n73.0 \n112.9 \n77.6 \n+ Multiple Adapters \n12.22 \n65.4 \n54.0 \n69.8 \n114.3 \n75.9 \n+ Half-shared Adapters \n8.36 \n65.2 \n53.4 \n71.2 \n113.7 \n75.9 \n+ Single Adapter \n4.18 \n65.9 \n54.5 \n74.2 \n114.9 \n77.4 \n+ Hyperformer \n5.79 \n65.1 \n53.4 \n72.3 \n114.6 \n76.4 \n+ Multiple Compacters \n7.05 \n64.6 \n53.4 \n69.1 \n116.0 \n75.8 \n+ Single Compacter \n2.70 \n64.2 \n53.3 \n71.7 \n114.1 \n75.8 \n+ Multiple LoRA \n17.72 \n65.5 \n53.0 \n62.8 \n115.4 \n74.2 \n+ Single LoRA \n5.93 \n65.2 \n53.6 \n71.9 \n115.3 \n76.5 \n+ Multiple Prompts \n4.53 \n43.8 \n38.1 \n51.1 \n104.6 \n59.4 \n+ Single Prompt \n2.00 \n44.0 \n36.3 \n51.8 \n103.9 \n59.0 \n\nMethod \nUpdated \nParams (%) \n\nVQA \ntest-std \n\nGQA \ntest-std \n\nCLIP-BART \n+ Full fine-tuning \n100.00 \n70.1 \n52.5 \n+ Single Adapter \n4.18 \n68.3 \n50.9 \n+ Single LoRA \n5.93 \n67.3 \n50.0 \n+ Single Prompt \n2.00 \n45.3 \n37.3 \n\nTable 2. Leaderboard results of test-std split for representative \napproaches from different method families. \n\n\n\nTable 3 .\n3The statistics of the datasets used in our experiments.\n\nTable 8 .\n8Ablation results of adding task-specific prompts.Method \nVQA GQA NLVR 2 COCO \nCap. \nAvg. \n\nVL-BART [7] \n69.1 \n59.0 \n73.3 \n111.5 \n78.2 \nCLIP-BART \n+ Full fine-tuning 69.2 \n57.5 \n75.0 \n112.1 \n78.5 \n+ Single Adapter \n69.4 \n58.1 \n73.7 \n115.7 \n79.2 \n\n\n\n\n) -w/ sharing weights, w/ low-rank param. (r = 1), k = 4 ) -w/ sharing weights, w/ low-rank param. (r = 1), k = 8 ) -w/ sharing weights, w/ low-rank param. (r = 1), k = 12 ) -w/ sharing weights, w/o low-rank param., k = 4 ) -w/o sharing weights, w/o low-rank param., k = 2 ) -w/o sharing weights, w/o low-rank param., k = 4 ) -w/o sharing weights, w/o low-rank param., k = 8 Multiple Compacters (d = 96) (H.1) -w/o sharing weights, w/o low-rank param., k = 2 (I) Single Compacter (d = 96) (I.1) -w/o sharing weights, w/o low-rank param., k = 2 Single Compacter (d = 48) (I.2) -w/o sharing weights, w/o low-rank param., k = 2 1 \u00d7 10 \u22123Method \n\nBest \nLearning Rate \n\nUpdated \nParams \n(%) \n\nVQA \nKarpathy test \nAcc. (%) \n\nGQA \ntest-dev \nAcc. (%) \n\nNLVR 2 \ntest-P \nAcc. (%) \n\nCOCO Cap. \nKarpathy test \nCIDEr \n\nAvg. \n\n(A) Full fine-tuning \n1 \u00d7 10 \u22124 \n100.00 \n67.6 \n56.7 \n73.0 \n112.9 \n77.6 \n\n(B) Multiple Adapters \n(B.1) -d = 96 \n3 \u00d7 10 \u22124 \n12.22 \n65.4 \n54.0 \n69.8 \n114.3 \n75.9 \n(B.2) -d = 48 \n1 \u00d7 10 \u22123 \n7.58 \n65.4 \n53.7 \n65.3 \n115.0 \n74.9 \n\n(C) Half-shared Adapters (d = 96) \n(C.1) -sharing downsampling layers \n3 \u00d7 10 \u22124 \n8.40 \n65.2 \n53.3 \n70.2 \n113.8 \n75.6 \n(C.2) -sharing upsampling layers \n3 \u00d7 10 \u22124 \n8.36 \n65.2 \n53.4 \n71.2 \n113.7 \n75.9 \n\n(D) Single Adapter \n(D.1) -d = 192 \n1 \u00d7 10 \u22123 \n7.54 \n66.5 \n54.0 \n73.5 \n115.8 \n77.4 \n(D.2) -d = 96 \n1 \u00d7 10 \u22123 \n4.36 \n65.9 \n54.5 \n74.2 \n114.9 \n77.4 \n(D.3) -d = 64 \n1 \u00d7 10 \u22123 \n3.30 \n65.2 \n53.8 \n72.3 \n114.5 \n76.4 \n(D.4) -d = 48 \n1 \u00d7 10 \u22123 \n2.78 \n64.7 \n53.9 \n71.5 \n114.2 \n76.1 \n(D.5) -d = 24 \n1 \u00d7 10 \u22123 \n1.98 \n63.5 \n52.2 \n71.0 \n113.5 \n75.1 \n\n(F) Hyperformer \n(F.1) -d = 96, d p = 8 \n1 \u00d7 10 \u22123 \n5.79 \n65.1 \n53.4 \n72.3 \n114.6 \n76.4 \n(F.2) -d = 96, d p = 4  *  \n1 \u00d7 10 \u22123 \n3.87 \n65.0 \n53.2 \n51.1 \n114.9 \n71.0 \n(F.3) -d = 48, d p = 8 \n1 \u00d7 10 \u22123 \n3.77 \n64.5 \n52.5 \n71.3 \n114.3 \n75.7 \n\n(G) Multiple Compacters (d = 48) \n(G.1) -w/ sharing weights, w/ low-rank param. (r = 1), k = 1 \n1 \u00d7 10 \u22123 \n1.381 \n50.8 \n41.6 \n53.5 \n104.9 \n62.7 \n(G.21 \u00d7 10 \u22123 \n1.381 \n52.6 \n43.5 \n54.0 \n111.6 \n65.4 \n(G.31 \u00d7 10 \u22123 \n1.382 \n52.2 \n42.4 \n58.3 \n109.8 \n65.7 \n(G.41 \u00d7 10 \u22123 \n1.383 \n53.9 \n43.7 \n60.4 \n111.1 \n67.3 \n(G.51 \u00d7 10 \u22123 \n2.83 \n52.7 \n42.7 \n59.7 \n112.2 \n66.8 \n(G.61 \u00d7 10 \u22123 \n4.42 \n64.0 \n52.9 \n68.3 \n115.7 \n75.2 \n(G.71 \u00d7 10 \u22123 \n2.84 \n62.4 \n51.4 \n68.6 \n115.5 \n74.5 \n(G.81 \u00d7 10 \u22123 \n2.11 \n61.4 \n50.9 \n68.9 \n115.4 \n74.1 \n(H) 1 \u00d7 10 \u22123 \n7.02 \n64.6 \n53.4 \n69.1 \n116.0 \n75.8 \n\n1 \u00d7 10 \u22123 \n2.67 \n64.2 \n53.3 \n71.7 \n114.1 \n75.8 \n1.59 \n61.6 \n50.7 \n69.0 \n114.0 \n73.8 \n\n(J) Multiple Prompts \n(J.1) -N p = 40, d m = 800 \n1 \u00d7 10 \u22123 \n4.53 \n43.8 \n38.1 \n51.1 \n104.6 \n59.4 \n(J.2) -N p = 40, d m = 100 \n1 \u00d7 10 \u22123 \n1.64 \n47.4 \n37.0 \n49.8 \n108.6 \n60.7 \n\n(K) Single Prompt \n(K.1) -N p = 40, d m = 800 \n1 \u00d7 10 \u22123 \n2.00 \n44.0 \n36.3 \n51.8 \n103.9 \n59.0 \n(K.2) -N p = 40, d m = 100 \n1 \u00d7 10 \u22123 \n1.25 \n43.5 \n36.4 \n52.0 \n103.4 \n58.8 \n\n\n\n\n) -w/o sharing weights, w/o low-rank param., k = 4 * (E.3) -w/o sharing weights, w/o low-rank param., k = 8 * (F.2) -w/o sharing weights, w/o low-rank param., k = 4 ) -w/o sharing weights, w/o low-rank param., k = 8Method \n\nBest \nLearning Rate \n\nUpdated \nParams \n(%) \n\nVQA \nKarpathy test \nAcc. (%) \n\nGQA \ntest-dev \nAcc. (%) \n\nNLVR 2 \ntest-P \nAcc. (%) \n\nCOCO Cap. \nKarpathy test \nCIDEr \n\nAvg. \n\n(A) Full fine-tuning \n1 \u00d7 10 \u22124 \n100.00 \n67.3 \n56.5 \n75.4 \n113.1 \n78.1 \n\n(B) Multiple Adapters \n(B.1) -d = 192 \n1 \u00d7 10 \u22123 \n24.56 \n66.0 \n55.7 \n51.8 \n111.9 \n71.3 \n(B.2) -d = 96 \n1 \u00d7 10 \u22123 \n14.29 \n66.1 \n55.7 \n52.5 \n112.8 \n71.8 \n\n(C) Single Adapter \n(C.1) -d = 384 \n3 \u00d7 10 \u22124 \n14.25 \n67.6 \n55.9 \n73.6 \n111.8 \n77.2 \n(C.2) -d = 192 \n3 \u00d7 10 \u22124 \n7.98 \n67.6 \n56.2 \n73.9 \n111.8 \n77.4 \n(C.3) -d = 96 \n1 \u00d7 10 \u22123 \n4.49 \n66.4 \n55.5 \n72.7 \n111.5 \n76.5 \n(C.4) -d = 48 \n1 \u00d7 10 \u22123 \n2.64 \n65.7 \n54.7 \n70.9 \n111.1 \n75.6 \n\n(D) Hyperformer \n(D.1) -d = 192, d p = 8 \n1 \u00d7 10 \u22123 \n6.37 \n65.5 \n55.1 \n71.5 \n112.2 \n76.1 \n(D.2) -d = 192, d p = 4 \n1 \u00d7 10 \u22123 \n3.99 \n65.0 \n53.9 \n70.4 \n111.7 \n75.2 \n\n(E) Multiple Compacters (d = 192) \n(E.1) -w/o sharing weights, w/o low-rank param., k = 2  *  \n1 \u00d7 10 \u22123 \n14.30 \n66.1 \n55.0 \n52.1 \n112.9 \n71.5 \n(E.21 \u00d7 10 \u22123 \n8.06 \n65.4 \n55.0 \n52.2 \n113.2 \n71.5 \n1 \u00d7 10 \u22123 \n4.66 \n63.3 \n52.9 \n51.7 \n110.4 \n69.6 \n\n(F) Single Compacter (d = 192) \n(F.1) -w/o sharing weights, w/o low-rank param., k = 2 \n1 \u00d7 10 \u22123 \n4.49 \n67.0 \n56.6 \n72.5 \n112.7 \n77.2 \n1 \u00d7 10 \u22123 \n2.65 \n66.1 \n55.2 \n71.8 \n111.7 \n76.2 \n(F.31 \u00d7 10 \u22123 \n1.72 \n65.2 \n54.1 \n71.6 \n111.5 \n75.6 \n\n\n\nTable 12 .\n12The multi-task evaluation results for CLIP-T5 on VQA, GQA, NLVR 2 and COCO Caption between adapter-based approaches with different hyper-parameters. We bold the highest average accuracy separately for each approach, and we also bold the best configuration we used in the main paper. Note that we don't use V&L pre-training for every model. * denotes the NLVR 2 results might be improved if we use different learning rates. 500 remove share weight and low-rank, d = 96, k = 2 Single Compacter 1 \u00d7 10 \u22123 500 remove share weight and low-rank, d = 96, k = 2 Multiple Prompts 1 \u00d7 10 \u22123 500 N p = 40, d m = 800 Single Prompt 1 \u00d7 10 \u22123 500 N p = 40, d m = 800 250 remove share weight and low-rank, d = 192, k = 2 Single Compacter 1 \u00d7 10 \u22123 250 remove share weight and low-rank, d = 192, k = 2Model \nApproach \nLearning Rate Batch size \nOther hyper-parameters \n\nCLIP-BART \n\nFull fine-tuning \n1 \u00d7 10 \u22124 \n500 \n-\nMultiple Adapters \n3 \u00d7 10 \u22124 \n500 \nd = 96 \nHalf-shared Adapters \n3 \u00d7 10 \u22124 \n500 \nsharing upsampling layers, d = 96 \nSingle Adapter \n1 \u00d7 10 \u22123 \n500 \nd = 96 \nHyperformer \n1 \u00d7 10 \u22123 \n500 \nd = 96, d p = 8 \nMultiple Compacters \n1 \u00d7 10 \u22123 \nCLIP-T5 \n\nFull fine-tuning \n1 \u00d7 10 \u22124 \n250 \n-\nMultiple Adapters \n1 \u00d7 10 \u22123 \n250 \nd = 192 \nSingle Adapter \n3 \u00d7 10 \u22124 \n250 \nd = 192 \nHyperformer \n1 \u00d7 10 \u22123 \n250 \nd = 192, d p = 8 \nMultiple Compacters \n1 \u00d7 10 \u22123 \n\n\nTable 13 .\n13The best hyperparameter configurations for different parameter-efficient training approaches.Table 14. Test-dev results for VQA.Datasets \nFull \nfine-tuning \n\nMultiple \nAdapters \n\nHalf-shared \nAdapters \n\nSingle \nAdapter \nHyperformer \nMultiple \nCompacters \n\nSingle \nCompacter \n\nMultiple \nLoRA \n\nSingle \nLoRA \n\nMultiple \nPrompts \n\nSingle \nPrompt \n\nVQA \n69.9 \n66.7 \n66.6 \n68.1 \n67.5 \n66.5 \n66.4 \n67.4 \n67.0 \n48.8 \n45.4 \n\n\nThe code for our CVPR 2022 paper is available at: https:// github.com/ylsung/VL_adapter.\nWe have our best jointly training model with a 1 \u00d7 10 \u22126 learning rate and the SGD optimizer on vision encoder while 1 \u00d7 10 \u22124 and the AdamW optimizer for the rest of the model.4 However, unlike Frozen using both captioning and VQA data, we only use the latter in our experiments.\nAcknowledgmentsWe thank the reviewers, Hyounghun Kim, Gedas Bertasius, and Hao Tan for their helpful discussions. This work was supported by ARO Award W911NF2110220, and ONR Grant N000141812871, and Google Focused Research Award. The views, opinions, and/or findings contained in this article are those of the authors and not of the funding agency.\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6077- 6086, 2018. 3\n\nHow to adapt your large-scale vision-andlanguage model. Anonymous, Submitted to The Tenth International Conference on Learning Representations, 2022. under review. 29Anonymous. How to adapt your large-scale vision-and- language model. In Submitted to The Tenth International Conference on Learning Representations, 2022. under re- view. 2, 3, 9\n\nHigh-performance large-scale image recognition without normalization. Andy Brock, Soham De, L Samuel, Karen Smith, Simonyan, PMLRProceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningAndy Brock, Soham De, Samuel L Smith, and Karen Si- monyan. High-performance large-scale image recognition without normalization. In Proceedings of the 38th Interna- tional Conference on Machine Learning, pages 1059-1071. PMLR, 18-24 Jul 2021. 3\n\n. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever. and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. 1, 2Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand- hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad- ford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. CoRR, abs/2005.14165, 2020. 1, 2\n\nMicrosoft COCO captions: Data collection and evaluation server. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, abs/1504.00325CoRR6Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan- tam, Saurabh Gupta, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR, abs/1504.00325, 2015. 2, 6, 8\n\nUniter: Universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, ECCV. 13Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020. 1, 3\n\nUnifying vision-and-language tasks via text generation. Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal, PMLRProceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning1399Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unify- ing vision-and-language tasks via text generation. In Pro- ceedings of the 38th International Conference on Machine Learning, volume 139, pages 1931-1942. PMLR, 18-24 Jul 2021. 1, 2, 3, 6, 7, 9\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL. 1Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL, 2019. 1, 2\n\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, International Conference on Learning Representations. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representa- tions, 2021. 3\n\nMaking the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 89Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba- tra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6325-6334, 2017. 2, 6, 8, 9\n\nPpt: Pre-trained prompt tuning for few-shot learning. Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang, abs/2109.04332ArXiv. 3Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. Ppt: Pre-trained prompt tuning for few-shot learning. ArXiv, abs/2109.04332, 2021. 3\n\nParameterefficient transfer learning with diff pruning. Demi Guo, Alexander Rush, Yoon Kim, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnlineAssociation for Computational Linguistics13Long Papers)Demi Guo, Alexander Rush, and Yoon Kim. Parameter- efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Pa- pers), pages 4884-4896, Online, Aug. 2021. Association for Computational Linguistics. 1, 3\n\nTowards a unified view of parameter-efficient transfer learning. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig, abs/2110.04366CoRRJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg- Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. CoRR, abs/2110.04366, 2021. 3\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 3\n\nBridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415. Dan Hendrycks, Kevin Gimpel, Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016. 5\n\nParameter-efficient transfer learning for nlp. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, PMLRInternational Conference on Machine Learning. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790-2799. PMLR, 2019. 1, 2, 3, 4\n\nLora: Low-rank adaptation of large language models. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen, 13CoRR, abs/2106.09685, 2021. 1, 3, 6Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685, 2021. 1, 3, 6, 13\n\nPixel-bert: Aligning image pixels with text by deep multi-modal transformers. CoRR, abs. Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu, Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. CoRR, abs/2004.00849, 2020. 1, 3, 8\n\nA Drew, Christopher D Hudson, Manning, Gqa: A new dataset for real-world visual reasoning and compositional question answering. Conference on Computer Vision and Pattern Recognition (CVPR). 89Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 2, 6, 8, 9\n\nParameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. Sebastian Rabeeh Karimi Mahabadi, Mostafa Ruder, James Dehghani, Henderson, Annual Meeting of the Association for Computational Linguistics. Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa De- hghani, and James Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Annual Meeting of the Association for Computational Lin- guistics, 2021. 1, 2, 3, 5\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael S Bernstein, Li Fei-Fei, International Journal of Computer Vision. 1239Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123:32-73, 2016. 9\n\nTvqa: Localized, compositional video question answering. Jie Lei, Licheng Yu, Mohit Bansal, Tamara L Berg, EMNLP. Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L. Berg. Tvqa: Localized, compositional video question answering. In EMNLP, 2018. 2, 6, 8\n\nTvr: A large-scale dataset for video-subtitle moment retrieval. Jie Lei, Licheng Yu, Tamara L Berg, Mohit Bansal, ECCV. 62Jie Lei, Licheng Yu, Tamara L. Berg, and Mohit Bansal. Tvr: A large-scale dataset for video-subtitle moment retrieval. ECCV, 2020. 2, 6, 8\n\nThe power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, EMNLP. 812Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, 2021. 2, 3, 6, 8, 12\n\nBART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline6Association for Computational Linguistics. 1, 2, 3Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine- jad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy- anov, and Luke Zettlemoyer. BART: Denoising sequence-to- sequence pre-training for natural language generation, trans- lation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguis- tics, pages 7871-7880, Online, July 2020. Association for Computational Linguistics. 1, 2, 3, 6\n\nHero: Hierarchical encoder for video+language omni-representation pre-training. Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu, EMNLP. 62Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder for video+language omni-representation pre-training. EMNLP, 2020. 2, 6, 8\n\nValue: A multi-task benchmark for video-and-language understanding evaluation. Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, et al. Value: A multi-task bench- mark for video-and-language understanding evaluation. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks, 2021. 6\n\nPrefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnlineAssociation for Computational Linguistics13Long Papers)Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimiz- ing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computa- tional Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Pa- pers), pages 4582-4597, Online, Aug. 2021. Association for Computational Linguistics. 1, 3\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European Conference on Computer Vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740-755. Springer, 2014. 9\n\nRoberta: A robustly optimized BERT pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, abs/1907.11692CoRRYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle- moyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. 1\n\nRoberta: A robustly optimized BERT pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, abs/1907.11692CoRRYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle- moyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. 2\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, NeurIPS. 13Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019. 1, 3\n\nCompacter: Efficient low-rank hypercomplex adapter layers. Rabeeh Karimi Mahabadi, James Henderson, Sebastian Ruder, Thirty-Fifth Conference on Neural Information Processing Systems. 14Rabeeh Karimi mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021. 1, 2, 3, 5, 14\n\nUnipelt: A unified framework for parameter-efficient language model tuning. Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-Tau Yih, Madian Khabsa, abs/2110.07577CoRRYuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. Unipelt: A unified framework for parameter-efficient lan- guage model tuning. CoRR, abs/2110.07577, 2021. 3\n\nDeep contextualized word representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, NAACL. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gard- ner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018. 2\n\nConditionally adaptive multi-task learning: Improving transfer learning in nlp using fewer parameters & less data. Jonathan Pilault, Amine Elhattami, Christopher Joseph Pal, ICLR. 2Jonathan Pilault, Amine Elhattami, and Christopher Joseph Pal. Conditionally adaptive multi-task learning: Improving transfer learning in nlp using fewer parameters & less data. ICLR, 2021. 2\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, ICML. 26Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 3, 6\n\nImproving language understanding by generative pre-training. Alec Radford, Ilya Sutskever, arxiv. Alec Radford and Ilya Sutskever. Improving language un- derstanding by generative pre-training. In arxiv, 2018. 2\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 18Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsu- pervised multitask learners. OpenAI blog, 1(8):9, 2019. 1, 2\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211406Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learn- ing Research, 21(140):1-67, 2020. 1, 2, 3, 6\n\nLearning multiple visual domains with residual adapters. Hakan Sylvestre-Alvise Rebuffi, Andrea Bilen, Vedaldi, NIPS. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In NIPS, 2017. 3\n\nEfficient parametrization of multi-domain deep neural networks. Hakan Sylvestre-Alvise Rebuffi, Andrea Bilen, Vedaldi, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Efficient parametrization of multi-domain deep neural net- works. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8119-8127, 2018. 3\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross B He, Jian Girshick, Sun, IEEE Transactions on Pattern Analysis and Machine Intelligence. 393Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:1137-1149, 2015. 3\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, im- age alt-text dataset for automatic image captioning. In Pro- ceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556-2565, Melbourne, Australia, July 2018. Association for Computational Linguistics. 3\n\nHow much can CLIP benefit vision-and-language tasks? CoRR. Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, Kurt Keutzer, abs/2107.0638389Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How much can CLIP benefit vision-and-language tasks? CoRR, abs/2107.06383, 2021. 1, 2, 3, 8, 9\n\nA corpus for reasoning about natural language grounded in photographs. Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Hua- jun Bai, and Yoav Artzi. A corpus for reasoning about nat- ural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics, pages 6418-6428, Florence, Italy, July 2019. Association for Computational Linguistics. 2, 6, 8\n\nTraining neural networks with fixed sparse masks. Yi-Lin Sung, Varun Nair, Colin Raffel, Advances in Neural Information Processing Systems. 13Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. In Advances in Neural Information Processing Systems, 2021. 1, 3\n\nLxmert: Learning crossmodality encoder representations from transformers. Hao Tan, Mohit Bansal, EMNLP. 13Hao Tan and Mohit Bansal. Lxmert: Learning cross- modality encoder representations from transformers. In EMNLP, 2019. 1, 3\n\nMultimodal few-shot learning with frozen language models. Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S M Ali Eslami, Oriol Vinyals, Felix Hill, CoRR, abs/2106.13884, 2021. 3, 8Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. CoRR, abs/2106.13884, 2021. 3, 8\n\nVisual entailment: A novel task for fine-grained image understanding. Ning Xie, Farley Lai, Derek Doran, Asim Kadav, ArXiv, abs/1901.06706Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for fine-grained image understand- ing. ArXiv, abs/1901.06706, 2019. 9\n\nAn empirical study of gpt-3 for few-shot knowledge-based vqa. Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, Lijuan Wang, abs/2109.05014ArXiv. 3Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. ArXiv, abs/2109.05014, 2021. 3\n\nBitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. CoRR, abs/2106.10199. Shauli Elad Ben Zaken, Yoav Ravfogel, Goldberg, 13Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. CoRR, abs/2106.10199, 2021. 1, 3\n\nBeyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with $1/n$ parameters. Aston Zhang, Yi Tay, Alvin Zhang, Anh Tuan Chan, Siu Luu, Jie Hui, Fu, International Conference on Learning Representations. Aston Zhang, Yi Tay, SHUAI Zhang, Alvin Chan, Anh Tuan Luu, Siu Hui, and Jie Fu. Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplica- tions with $1/n$ parameters. In International Conference on Learning Representations, 2021. 5\n\nTip-adapter: Training-free clip-adapter for better visionlanguage modeling. Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, Hongsheng Li, abs/2111.03930ArXiv. 3Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision- language modeling. ArXiv, abs/2111.03930, 2021. 3\n\nLearning to prompt for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, abs/2109.01134ArXiv. 3Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. ArXiv, abs/2109.01134, 2021. 3\n\nTowards automatic learning of procedures from web instructional videos. Luowei Zhou, Chenliang Xu, Jason J Corso, AAAI. Luowei Zhou, Chenliang Xu, and Jason J. Corso. To- wards automatic learning of procedures from web instruc- tional videos. In AAAI, 2018. 2, 6, 8\n", "annotations": {"author": "[{\"end\":131,\"start\":83},{\"end\":161,\"start\":132},{\"end\":212,\"start\":162}]", "publisher": null, "author_last_name": "[{\"end\":94,\"start\":90},{\"end\":142,\"start\":139},{\"end\":174,\"start\":168}]", "author_first_name": "[{\"end\":89,\"start\":83},{\"end\":138,\"start\":132},{\"end\":167,\"start\":162}]", "author_affiliation": "[{\"end\":130,\"start\":114},{\"end\":160,\"start\":144},{\"end\":211,\"start\":195}]", "title": "[{\"end\":80,\"start\":1},{\"end\":292,\"start\":213}]", "venue": null, "abstract": "[{\"end\":2068,\"start\":294}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2132,\"start\":2129},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2134,\"start\":2132},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2137,\"start\":2134},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2140,\"start\":2137},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2143,\"start\":2140},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2146,\"start\":2143},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2264,\"start\":2261},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2266,\"start\":2264},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2269,\"start\":2266},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2272,\"start\":2269},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2275,\"start\":2272},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":2278,\"start\":2275},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3149,\"start\":3146},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3263,\"start\":3262},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3367,\"start\":3363},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3370,\"start\":3367},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3373,\"start\":3370},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3376,\"start\":3373},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3379,\"start\":3376},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3382,\"start\":3379},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3385,\"start\":3382},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3388,\"start\":3385},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3414,\"start\":3410},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3436,\"start\":3432},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3439,\"start\":3436},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3982,\"start\":3978},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3985,\"start\":3982},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3988,\"start\":3985},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4017,\"start\":4014},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4707,\"start\":4703},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4710,\"start\":4707},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4713,\"start\":4710},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4743,\"start\":4739},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4788,\"start\":4785},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4841,\"start\":4837},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":4853,\"start\":4849},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4968,\"start\":4964},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5147,\"start\":5144},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5150,\"start\":5147},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5277,\"start\":5273},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5312,\"start\":5308},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5331,\"start\":5327},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5755,\"start\":5751},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6092,\"start\":6088},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6095,\"start\":6092},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6536,\"start\":6532},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6546,\"start\":6542},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6559,\"start\":6555},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6586,\"start\":6583},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6690,\"start\":6686},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6703,\"start\":6699},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6713,\"start\":6709},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6728,\"start\":6724},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8245,\"start\":8241},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9534,\"start\":9530},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9544,\"start\":9541},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9558,\"start\":9554},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9574,\"start\":9571},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9577,\"start\":9574},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9580,\"start\":9577},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9628,\"start\":9624},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9640,\"start\":9636},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9668,\"start\":9664},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9995,\"start\":9991},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10007,\"start\":10004},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10023,\"start\":10019},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10049,\"start\":10045},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10094,\"start\":10091},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10268,\"start\":10265},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10311,\"start\":10307},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10314,\"start\":10311},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10675,\"start\":10671},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10713,\"start\":10709},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10895,\"start\":10891},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11145,\"start\":11141},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11218,\"start\":11215},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11336,\"start\":11332},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11339,\"start\":11336},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11549,\"start\":11545},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11603,\"start\":11600},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11664,\"start\":11660},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11727,\"start\":11723},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13010,\"start\":13006},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13013,\"start\":13010},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13016,\"start\":13013},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13019,\"start\":13016},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13022,\"start\":13019},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13027,\"start\":13024},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13097,\"start\":13093},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13100,\"start\":13097},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":13103,\"start\":13100},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13165,\"start\":13161},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13171,\"start\":13167},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13174,\"start\":13171},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13368,\"start\":13364},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13371,\"start\":13368},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13408,\"start\":13404},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13411,\"start\":13408},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13414,\"start\":13411},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13547,\"start\":13543},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13550,\"start\":13547},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13553,\"start\":13550},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13696,\"start\":13693},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":13699,\"start\":13696},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13702,\"start\":13699},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14838,\"start\":14835},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17405,\"start\":17401},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18083,\"start\":18079},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18739,\"start\":18735},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20151,\"start\":20147},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20548,\"start\":20544},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24575,\"start\":24571},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24588,\"start\":24584},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24631,\"start\":24627},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24668,\"start\":24665},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24771,\"start\":24767},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24792,\"start\":24788},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24808,\"start\":24804},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24847,\"start\":24843},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":24861,\"start\":24857},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24972,\"start\":24969},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25180,\"start\":25176},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25442,\"start\":25439},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25530,\"start\":25526},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25742,\"start\":25738},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25834,\"start\":25830},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25925,\"start\":25921},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27394,\"start\":27390},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27408,\"start\":27404},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31368,\"start\":31364},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32097,\"start\":32093},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32355,\"start\":32351},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32358,\"start\":32355},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":32786,\"start\":32782},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32788,\"start\":32787},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34041,\"start\":34038},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":35594,\"start\":35590},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35620,\"start\":35616},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":35632,\"start\":35628},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36034,\"start\":36030},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":36483,\"start\":36479},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":36548,\"start\":36544},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36562,\"start\":36558},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":36576,\"start\":36572},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":38595,\"start\":38591},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40985,\"start\":40981},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":41181,\"start\":41177},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":43475,\"start\":43471},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":52766,\"start\":52765}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44045,\"start\":43833},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44173,\"start\":44046},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44364,\"start\":44174},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46019,\"start\":44365},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":46087,\"start\":46020},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46346,\"start\":46088},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":49161,\"start\":46347},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50706,\"start\":49162},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":52066,\"start\":50707},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":52498,\"start\":52067}]", "paragraph": "[{\"end\":4425,\"start\":2084},{\"end\":6450,\"start\":4427},{\"end\":8472,\"start\":6452},{\"end\":9033,\"start\":8474},{\"end\":9273,\"start\":9050},{\"end\":11234,\"start\":9275},{\"end\":12730,\"start\":11236},{\"end\":14277,\"start\":12732},{\"end\":14745,\"start\":14289},{\"end\":16373,\"start\":14781},{\"end\":16596,\"start\":16488},{\"end\":16909,\"start\":16631},{\"end\":17336,\"start\":16997},{\"end\":17712,\"start\":17364},{\"end\":17992,\"start\":17714},{\"end\":19497,\"start\":18025},{\"end\":19715,\"start\":19545},{\"end\":19900,\"start\":19781},{\"end\":20640,\"start\":19917},{\"end\":20671,\"start\":20666},{\"end\":21121,\"start\":20708},{\"end\":21270,\"start\":21160},{\"end\":22662,\"start\":21321},{\"end\":23133,\"start\":22664},{\"end\":24459,\"start\":23135},{\"end\":25107,\"start\":24482},{\"end\":27026,\"start\":25109},{\"end\":28260,\"start\":27096},{\"end\":28634,\"start\":28298},{\"end\":30313,\"start\":28636},{\"end\":31962,\"start\":30315},{\"end\":32979,\"start\":32002},{\"end\":34182,\"start\":33006},{\"end\":34696,\"start\":34184},{\"end\":35224,\"start\":34698},{\"end\":35788,\"start\":35226},{\"end\":36302,\"start\":35790},{\"end\":37045,\"start\":36304},{\"end\":37823,\"start\":37075},{\"end\":38305,\"start\":37825},{\"end\":38544,\"start\":38307},{\"end\":39329,\"start\":38577},{\"end\":40618,\"start\":39374},{\"end\":40901,\"start\":40620},{\"end\":41100,\"start\":40925},{\"end\":41545,\"start\":41112},{\"end\":42088,\"start\":41575},{\"end\":42577,\"start\":42090},{\"end\":43069,\"start\":42619},{\"end\":43476,\"start\":43109},{\"end\":43832,\"start\":43534}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16487,\"start\":16374},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16630,\"start\":16597},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16996,\"start\":16910},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18024,\"start\":17993},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19544,\"start\":19498},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19780,\"start\":19716},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19916,\"start\":19901},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20665,\"start\":20641},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20707,\"start\":20672},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21159,\"start\":21122},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21320,\"start\":21271},{\"attributes\":{\"id\":\"formula_11\"},\"end\":39373,\"start\":39330},{\"attributes\":{\"id\":\"formula_12\"},\"end\":41111,\"start\":41101}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24935,\"start\":24928},{\"end\":27991,\"start\":27984},{\"end\":28100,\"start\":28093},{\"end\":28213,\"start\":28206},{\"end\":28259,\"start\":28252},{\"end\":30422,\"start\":30415},{\"end\":31498,\"start\":31491},{\"end\":31569,\"start\":31562},{\"end\":32392,\"start\":32385},{\"end\":32878,\"start\":32871},{\"end\":33641,\"start\":33634},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":34774,\"start\":34767},{\"end\":35685,\"start\":35678},{\"end\":36043,\"start\":36036},{\"end\":36588,\"start\":36580},{\"end\":36891,\"start\":36883},{\"end\":40194,\"start\":40186},{\"end\":42680,\"start\":42672},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":42785,\"start\":42777},{\"end\":43067,\"start\":43059},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":43185,\"start\":43177},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":43238,\"start\":43230},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":43283,\"start\":43275},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":43383,\"start\":43375},{\"end\":43672,\"start\":43664}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2082,\"start\":2070},{\"attributes\":{\"n\":\"2.\"},\"end\":9048,\"start\":9036},{\"attributes\":{\"n\":\"3.\"},\"end\":14287,\"start\":14280},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14779,\"start\":14748},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17362,\"start\":17339},{\"attributes\":{\"n\":\"4.\"},\"end\":24480,\"start\":24462},{\"attributes\":{\"n\":\"5.\"},\"end\":27049,\"start\":27029},{\"attributes\":{\"n\":\"5.1.\"},\"end\":27094,\"start\":27052},{\"end\":28296,\"start\":28263},{\"attributes\":{\"n\":\"5.2.\"},\"end\":32000,\"start\":31965},{\"attributes\":{\"n\":\"5.3.\"},\"end\":33004,\"start\":32982},{\"attributes\":{\"n\":\"6.\"},\"end\":37073,\"start\":37048},{\"end\":38575,\"start\":38547},{\"end\":40923,\"start\":40904},{\"end\":41573,\"start\":41548},{\"end\":42617,\"start\":42580},{\"end\":43107,\"start\":43072},{\"end\":43532,\"start\":43479},{\"end\":43844,\"start\":43834},{\"end\":44057,\"start\":44047},{\"end\":44185,\"start\":44175},{\"end\":46030,\"start\":46021},{\"end\":46098,\"start\":46089},{\"end\":50718,\"start\":50708},{\"end\":52078,\"start\":52068}]", "table": "[{\"end\":46019,\"start\":44847},{\"end\":46346,\"start\":46149},{\"end\":49161,\"start\":46983},{\"end\":50706,\"start\":49379},{\"end\":52066,\"start\":51506},{\"end\":52498,\"start\":52209}]", "figure_caption": "[{\"end\":44045,\"start\":43846},{\"end\":44173,\"start\":44059},{\"end\":44364,\"start\":44187},{\"end\":44847,\"start\":44367},{\"end\":46087,\"start\":46032},{\"end\":46149,\"start\":46100},{\"end\":46983,\"start\":46349},{\"end\":49379,\"start\":49164},{\"end\":51506,\"start\":50721},{\"end\":52209,\"start\":52081}]", "figure_ref": "[{\"end\":2426,\"start\":2418},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3615,\"start\":3607},{\"end\":5854,\"start\":5846},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6022,\"start\":6014},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":7786,\"start\":7780},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14820,\"start\":14812},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17382,\"start\":17374},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18383,\"start\":18375},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19937,\"start\":19929},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21658,\"start\":21650},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22552,\"start\":22544},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23417,\"start\":23406},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23587,\"start\":23576},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24032,\"start\":24024},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30123,\"start\":30115}]", "bib_author_first_name": "[{\"end\":53308,\"start\":53303},{\"end\":53327,\"start\":53319},{\"end\":53337,\"start\":53332},{\"end\":53353,\"start\":53347},{\"end\":53365,\"start\":53361},{\"end\":53382,\"start\":53375},{\"end\":53393,\"start\":53390},{\"end\":54168,\"start\":54164},{\"end\":54181,\"start\":54176},{\"end\":54187,\"start\":54186},{\"end\":54201,\"start\":54196},{\"end\":54598,\"start\":54595},{\"end\":54600,\"start\":54599},{\"end\":54616,\"start\":54608},{\"end\":54627,\"start\":54623},{\"end\":54642,\"start\":54635},{\"end\":54657,\"start\":54652},{\"end\":54674,\"start\":54666},{\"end\":54691,\"start\":54685},{\"end\":54711,\"start\":54705},{\"end\":54725,\"start\":54719},{\"end\":54740,\"start\":54734},{\"end\":54757,\"start\":54749},{\"end\":54772,\"start\":54767},{\"end\":54795,\"start\":54787},{\"end\":54808,\"start\":54805},{\"end\":54824,\"start\":54819},{\"end\":54838,\"start\":54832},{\"end\":54853,\"start\":54847},{\"end\":54855,\"start\":54854},{\"end\":54872,\"start\":54865},{\"end\":54884,\"start\":54877},{\"end\":54904,\"start\":54893},{\"end\":54916,\"start\":54912},{\"end\":54927,\"start\":54923},{\"end\":54943,\"start\":54936},{\"end\":55691,\"start\":55685},{\"end\":55701,\"start\":55698},{\"end\":55716,\"start\":55708},{\"end\":55733,\"start\":55722},{\"end\":55751,\"start\":55744},{\"end\":55764,\"start\":55759},{\"end\":55774,\"start\":55773},{\"end\":55783,\"start\":55775},{\"end\":56090,\"start\":56082},{\"end\":56103,\"start\":56097},{\"end\":56115,\"start\":56108},{\"end\":56125,\"start\":56120},{\"end\":56128,\"start\":56126},{\"end\":56142,\"start\":56136},{\"end\":56153,\"start\":56150},{\"end\":56161,\"start\":56159},{\"end\":56177,\"start\":56169},{\"end\":56433,\"start\":56427},{\"end\":56442,\"start\":56439},{\"end\":56451,\"start\":56448},{\"end\":56462,\"start\":56457},{\"end\":56939,\"start\":56934},{\"end\":56956,\"start\":56948},{\"end\":56970,\"start\":56964},{\"end\":56984,\"start\":56976},{\"end\":57308,\"start\":57302},{\"end\":57327,\"start\":57322},{\"end\":57344,\"start\":57335},{\"end\":57361,\"start\":57357},{\"end\":57382,\"start\":57375},{\"end\":57395,\"start\":57389},{\"end\":57416,\"start\":57409},{\"end\":57435,\"start\":57427},{\"end\":57451,\"start\":57446},{\"end\":57974,\"start\":57970},{\"end\":57987,\"start\":57982},{\"end\":58001,\"start\":57994},{\"end\":58021,\"start\":58016},{\"end\":58033,\"start\":58029},{\"end\":58458,\"start\":58452},{\"end\":58465,\"start\":58463},{\"end\":58478,\"start\":58471},{\"end\":58490,\"start\":58484},{\"end\":58716,\"start\":58712},{\"end\":58731,\"start\":58722},{\"end\":58742,\"start\":58738},{\"end\":59572,\"start\":59565},{\"end\":59585,\"start\":59577},{\"end\":59598,\"start\":59592},{\"end\":59609,\"start\":59603},{\"end\":59634,\"start\":59628},{\"end\":59893,\"start\":59886},{\"end\":59905,\"start\":59898},{\"end\":59921,\"start\":59913},{\"end\":59931,\"start\":59927},{\"end\":60398,\"start\":60395},{\"end\":60415,\"start\":60410},{\"end\":60624,\"start\":60620},{\"end\":60640,\"start\":60634},{\"end\":60659,\"start\":60650},{\"end\":60678,\"start\":60673},{\"end\":60695,\"start\":60688},{\"end\":60718,\"start\":60712},{\"end\":60733,\"start\":60729},{\"end\":60752,\"start\":60745},{\"end\":61149,\"start\":61143},{\"end\":61151,\"start\":61150},{\"end\":61162,\"start\":61156},{\"end\":61176,\"start\":61169},{\"end\":61191,\"start\":61185},{\"end\":61210,\"start\":61203},{\"end\":61220,\"start\":61215},{\"end\":61233,\"start\":61227},{\"end\":61570,\"start\":61562},{\"end\":61586,\"start\":61578},{\"end\":61596,\"start\":61593},{\"end\":61609,\"start\":61602},{\"end\":61622,\"start\":61614},{\"end\":61812,\"start\":61811},{\"end\":61832,\"start\":61819},{\"end\":62308,\"start\":62299},{\"end\":62340,\"start\":62333},{\"end\":62353,\"start\":62348},{\"end\":62792,\"start\":62786},{\"end\":62806,\"start\":62802},{\"end\":62818,\"start\":62812},{\"end\":62832,\"start\":62826},{\"end\":62847,\"start\":62842},{\"end\":62860,\"start\":62854},{\"end\":62879,\"start\":62870},{\"end\":62892,\"start\":62886},{\"end\":62911,\"start\":62905},{\"end\":62921,\"start\":62916},{\"end\":62923,\"start\":62922},{\"end\":62939,\"start\":62932},{\"end\":62941,\"start\":62940},{\"end\":62955,\"start\":62953},{\"end\":63410,\"start\":63407},{\"end\":63423,\"start\":63416},{\"end\":63433,\"start\":63428},{\"end\":63448,\"start\":63442},{\"end\":63450,\"start\":63449},{\"end\":63668,\"start\":63665},{\"end\":63681,\"start\":63674},{\"end\":63692,\"start\":63686},{\"end\":63694,\"start\":63693},{\"end\":63706,\"start\":63701},{\"end\":63926,\"start\":63921},{\"end\":63939,\"start\":63935},{\"end\":63953,\"start\":63949},{\"end\":64229,\"start\":64225},{\"end\":64243,\"start\":64237},{\"end\":64254,\"start\":64249},{\"end\":64268,\"start\":64262},{\"end\":64295,\"start\":64284},{\"end\":64309,\"start\":64305},{\"end\":64323,\"start\":64316},{\"end\":64338,\"start\":64334},{\"end\":65096,\"start\":65090},{\"end\":65109,\"start\":65101},{\"end\":65118,\"start\":65116},{\"end\":65129,\"start\":65126},{\"end\":65142,\"start\":65135},{\"end\":65155,\"start\":65147},{\"end\":65432,\"start\":65426},{\"end\":65440,\"start\":65437},{\"end\":65449,\"start\":65446},{\"end\":65462,\"start\":65455},{\"end\":65475,\"start\":65467},{\"end\":65487,\"start\":65482},{\"end\":65498,\"start\":65496},{\"end\":65512,\"start\":65506},{\"end\":65522,\"start\":65519},{\"end\":65527,\"start\":65523},{\"end\":65541,\"start\":65534},{\"end\":65546,\"start\":65542},{\"end\":66057,\"start\":66053},{\"end\":66070,\"start\":66065},{\"end\":66881,\"start\":66873},{\"end\":66894,\"start\":66887},{\"end\":66907,\"start\":66902},{\"end\":66923,\"start\":66918},{\"end\":66936,\"start\":66930},{\"end\":66949,\"start\":66945},{\"end\":66964,\"start\":66959},{\"end\":66983,\"start\":66973},{\"end\":67348,\"start\":67342},{\"end\":67358,\"start\":67354},{\"end\":67369,\"start\":67364},{\"end\":67384,\"start\":67377},{\"end\":67395,\"start\":67389},{\"end\":67408,\"start\":67403},{\"end\":67419,\"start\":67415},{\"end\":67430,\"start\":67426},{\"end\":67442,\"start\":67438},{\"end\":67463,\"start\":67456},{\"end\":67781,\"start\":67775},{\"end\":67791,\"start\":67787},{\"end\":67802,\"start\":67797},{\"end\":67817,\"start\":67810},{\"end\":67828,\"start\":67822},{\"end\":67841,\"start\":67836},{\"end\":67852,\"start\":67848},{\"end\":67863,\"start\":67859},{\"end\":67875,\"start\":67871},{\"end\":67896,\"start\":67889},{\"end\":68255,\"start\":68249},{\"end\":68265,\"start\":68260},{\"end\":68277,\"start\":68273},{\"end\":68292,\"start\":68286},{\"end\":68549,\"start\":68543},{\"end\":68572,\"start\":68567},{\"end\":68593,\"start\":68584},{\"end\":68963,\"start\":68957},{\"end\":68976,\"start\":68969},{\"end\":68989,\"start\":68986},{\"end\":69000,\"start\":68995},{\"end\":69015,\"start\":69012},{\"end\":69026,\"start\":69020},{\"end\":69039,\"start\":69032},{\"end\":69051,\"start\":69045},{\"end\":69343,\"start\":69336},{\"end\":69345,\"start\":69344},{\"end\":69358,\"start\":69354},{\"end\":69373,\"start\":69368},{\"end\":69385,\"start\":69381},{\"end\":69406,\"start\":69395},{\"end\":69420,\"start\":69414},{\"end\":69430,\"start\":69426},{\"end\":69750,\"start\":69742},{\"end\":69765,\"start\":69760},{\"end\":69795,\"start\":69777},{\"end\":70076,\"start\":70072},{\"end\":70090,\"start\":70086},{\"end\":70095,\"start\":70091},{\"end\":70106,\"start\":70101},{\"end\":70122,\"start\":70116},{\"end\":70138,\"start\":70131},{\"end\":70152,\"start\":70144},{\"end\":70168,\"start\":70162},{\"end\":70183,\"start\":70177},{\"end\":70198,\"start\":70192},{\"end\":70212,\"start\":70208},{\"end\":70228,\"start\":70220},{\"end\":70242,\"start\":70238},{\"end\":70608,\"start\":70604},{\"end\":70622,\"start\":70618},{\"end\":70813,\"start\":70809},{\"end\":70830,\"start\":70823},{\"end\":70840,\"start\":70835},{\"end\":70853,\"start\":70848},{\"end\":70865,\"start\":70860},{\"end\":70878,\"start\":70874},{\"end\":71169,\"start\":71164},{\"end\":71182,\"start\":71178},{\"end\":71196,\"start\":71192},{\"end\":71215,\"start\":71206},{\"end\":71227,\"start\":71221},{\"end\":71243,\"start\":71236},{\"end\":71257,\"start\":71252},{\"end\":71267,\"start\":71264},{\"end\":71277,\"start\":71272},{\"end\":71279,\"start\":71278},{\"end\":71672,\"start\":71667},{\"end\":71705,\"start\":71699},{\"end\":71931,\"start\":71926},{\"end\":71964,\"start\":71958},{\"end\":72352,\"start\":72345},{\"end\":72371,\"start\":72367},{\"end\":72373,\"start\":72372},{\"end\":72382,\"start\":72378},{\"end\":72795,\"start\":72789},{\"end\":72807,\"start\":72804},{\"end\":72823,\"start\":72814},{\"end\":72837,\"start\":72833},{\"end\":73511,\"start\":73506},{\"end\":73525,\"start\":73518},{\"end\":73532,\"start\":73526},{\"end\":73540,\"start\":73537},{\"end\":73551,\"start\":73546},{\"end\":73564,\"start\":73560},{\"end\":73582,\"start\":73575},{\"end\":73596,\"start\":73590},{\"end\":73606,\"start\":73602},{\"end\":73918,\"start\":73913},{\"end\":73934,\"start\":73925},{\"end\":73945,\"start\":73941},{\"end\":73957,\"start\":73953},{\"end\":73971,\"start\":73965},{\"end\":73981,\"start\":73977},{\"end\":74609,\"start\":74603},{\"end\":74621,\"start\":74616},{\"end\":74633,\"start\":74628},{\"end\":74931,\"start\":74928},{\"end\":74942,\"start\":74937},{\"end\":75147,\"start\":75142},{\"end\":75167,\"start\":75162},{\"end\":75182,\"start\":75176},{\"end\":75190,\"start\":75189},{\"end\":75192,\"start\":75191},{\"end\":75210,\"start\":75205},{\"end\":75225,\"start\":75220},{\"end\":75526,\"start\":75522},{\"end\":75538,\"start\":75532},{\"end\":75549,\"start\":75544},{\"end\":75561,\"start\":75557},{\"end\":75816,\"start\":75807},{\"end\":75826,\"start\":75823},{\"end\":75840,\"start\":75832},{\"end\":75854,\"start\":75847},{\"end\":75864,\"start\":75859},{\"end\":75876,\"start\":75869},{\"end\":75888,\"start\":75882},{\"end\":76224,\"start\":76218},{\"end\":76245,\"start\":76241},{\"end\":76572,\"start\":76567},{\"end\":76582,\"start\":76580},{\"end\":76593,\"start\":76588},{\"end\":76604,\"start\":76601},{\"end\":76609,\"start\":76605},{\"end\":76619,\"start\":76616},{\"end\":76628,\"start\":76625},{\"end\":77043,\"start\":77037},{\"end\":77058,\"start\":77051},{\"end\":77069,\"start\":77065},{\"end\":77078,\"start\":77075},{\"end\":77094,\"start\":77086},{\"end\":77105,\"start\":77099},{\"end\":77113,\"start\":77111},{\"end\":77129,\"start\":77120},{\"end\":77421,\"start\":77414},{\"end\":77436,\"start\":77428},{\"end\":77447,\"start\":77443},{\"end\":77454,\"start\":77448},{\"end\":77465,\"start\":77460},{\"end\":77711,\"start\":77705},{\"end\":77727,\"start\":77718},{\"end\":77737,\"start\":77732},{\"end\":77739,\"start\":77738}]", "bib_author_last_name": "[{\"end\":53317,\"start\":53309},{\"end\":53330,\"start\":53328},{\"end\":53345,\"start\":53338},{\"end\":53359,\"start\":53354},{\"end\":53373,\"start\":53366},{\"end\":53388,\"start\":53383},{\"end\":53399,\"start\":53394},{\"end\":53813,\"start\":53804},{\"end\":54174,\"start\":54169},{\"end\":54184,\"start\":54182},{\"end\":54194,\"start\":54188},{\"end\":54207,\"start\":54202},{\"end\":54217,\"start\":54209},{\"end\":54606,\"start\":54601},{\"end\":54621,\"start\":54617},{\"end\":54633,\"start\":54628},{\"end\":54650,\"start\":54643},{\"end\":54664,\"start\":54658},{\"end\":54683,\"start\":54675},{\"end\":54703,\"start\":54692},{\"end\":54717,\"start\":54712},{\"end\":54732,\"start\":54726},{\"end\":54747,\"start\":54741},{\"end\":54765,\"start\":54758},{\"end\":54785,\"start\":54773},{\"end\":54803,\"start\":54796},{\"end\":54817,\"start\":54809},{\"end\":54830,\"start\":54825},{\"end\":54845,\"start\":54839},{\"end\":54863,\"start\":54856},{\"end\":54875,\"start\":54873},{\"end\":54891,\"start\":54885},{\"end\":54910,\"start\":54905},{\"end\":54921,\"start\":54917},{\"end\":54934,\"start\":54928},{\"end\":54950,\"start\":54944},{\"end\":55696,\"start\":55692},{\"end\":55706,\"start\":55702},{\"end\":55720,\"start\":55717},{\"end\":55742,\"start\":55734},{\"end\":55757,\"start\":55752},{\"end\":55771,\"start\":55765},{\"end\":55791,\"start\":55784},{\"end\":56095,\"start\":56091},{\"end\":56106,\"start\":56104},{\"end\":56118,\"start\":56116},{\"end\":56134,\"start\":56129},{\"end\":56148,\"start\":56143},{\"end\":56157,\"start\":56154},{\"end\":56167,\"start\":56162},{\"end\":56181,\"start\":56178},{\"end\":56437,\"start\":56434},{\"end\":56446,\"start\":56443},{\"end\":56455,\"start\":56452},{\"end\":56469,\"start\":56463},{\"end\":56946,\"start\":56940},{\"end\":56962,\"start\":56957},{\"end\":56974,\"start\":56971},{\"end\":56994,\"start\":56985},{\"end\":57320,\"start\":57309},{\"end\":57333,\"start\":57328},{\"end\":57355,\"start\":57345},{\"end\":57373,\"start\":57362},{\"end\":57387,\"start\":57383},{\"end\":57407,\"start\":57396},{\"end\":57425,\"start\":57417},{\"end\":57444,\"start\":57436},{\"end\":57459,\"start\":57452},{\"end\":57980,\"start\":57975},{\"end\":57992,\"start\":57988},{\"end\":58014,\"start\":58002},{\"end\":58027,\"start\":58022},{\"end\":58040,\"start\":58034},{\"end\":58461,\"start\":58459},{\"end\":58469,\"start\":58466},{\"end\":58482,\"start\":58479},{\"end\":58496,\"start\":58491},{\"end\":58720,\"start\":58717},{\"end\":58736,\"start\":58732},{\"end\":58746,\"start\":58743},{\"end\":59575,\"start\":59573},{\"end\":59590,\"start\":59586},{\"end\":59601,\"start\":59599},{\"end\":59626,\"start\":59610},{\"end\":59641,\"start\":59635},{\"end\":59896,\"start\":59894},{\"end\":59911,\"start\":59906},{\"end\":59925,\"start\":59922},{\"end\":59935,\"start\":59932},{\"end\":60408,\"start\":60399},{\"end\":60422,\"start\":60416},{\"end\":60632,\"start\":60625},{\"end\":60648,\"start\":60641},{\"end\":60671,\"start\":60660},{\"end\":60686,\"start\":60679},{\"end\":60710,\"start\":60696},{\"end\":60727,\"start\":60719},{\"end\":60743,\"start\":60734},{\"end\":60758,\"start\":60753},{\"end\":61154,\"start\":61152},{\"end\":61167,\"start\":61163},{\"end\":61183,\"start\":61177},{\"end\":61201,\"start\":61192},{\"end\":61213,\"start\":61211},{\"end\":61225,\"start\":61221},{\"end\":61238,\"start\":61234},{\"end\":61576,\"start\":61571},{\"end\":61591,\"start\":61587},{\"end\":61600,\"start\":61597},{\"end\":61612,\"start\":61610},{\"end\":61625,\"start\":61623},{\"end\":61817,\"start\":61813},{\"end\":61839,\"start\":61833},{\"end\":61848,\"start\":61841},{\"end\":62331,\"start\":62309},{\"end\":62346,\"start\":62341},{\"end\":62362,\"start\":62354},{\"end\":62373,\"start\":62364},{\"end\":62800,\"start\":62793},{\"end\":62810,\"start\":62807},{\"end\":62824,\"start\":62819},{\"end\":62840,\"start\":62833},{\"end\":62852,\"start\":62848},{\"end\":62868,\"start\":62861},{\"end\":62884,\"start\":62880},{\"end\":62903,\"start\":62893},{\"end\":62914,\"start\":62912},{\"end\":62930,\"start\":62924},{\"end\":62951,\"start\":62942},{\"end\":62963,\"start\":62956},{\"end\":63414,\"start\":63411},{\"end\":63426,\"start\":63424},{\"end\":63440,\"start\":63434},{\"end\":63455,\"start\":63451},{\"end\":63672,\"start\":63669},{\"end\":63684,\"start\":63682},{\"end\":63699,\"start\":63695},{\"end\":63713,\"start\":63707},{\"end\":63933,\"start\":63927},{\"end\":63947,\"start\":63940},{\"end\":63962,\"start\":63954},{\"end\":64235,\"start\":64230},{\"end\":64247,\"start\":64244},{\"end\":64260,\"start\":64255},{\"end\":64282,\"start\":64269},{\"end\":64303,\"start\":64296},{\"end\":64314,\"start\":64310},{\"end\":64332,\"start\":64324},{\"end\":64350,\"start\":64339},{\"end\":65099,\"start\":65097},{\"end\":65114,\"start\":65110},{\"end\":65124,\"start\":65119},{\"end\":65133,\"start\":65130},{\"end\":65145,\"start\":65143},{\"end\":65159,\"start\":65156},{\"end\":65435,\"start\":65433},{\"end\":65444,\"start\":65441},{\"end\":65453,\"start\":65450},{\"end\":65465,\"start\":65463},{\"end\":65480,\"start\":65476},{\"end\":65494,\"start\":65488},{\"end\":65504,\"start\":65499},{\"end\":65517,\"start\":65513},{\"end\":65532,\"start\":65528},{\"end\":65551,\"start\":65547},{\"end\":66063,\"start\":66058},{\"end\":66073,\"start\":66071},{\"end\":66080,\"start\":66075},{\"end\":66885,\"start\":66882},{\"end\":66900,\"start\":66895},{\"end\":66916,\"start\":66908},{\"end\":66928,\"start\":66924},{\"end\":66943,\"start\":66937},{\"end\":66957,\"start\":66950},{\"end\":66971,\"start\":66965},{\"end\":66991,\"start\":66984},{\"end\":67352,\"start\":67349},{\"end\":67362,\"start\":67359},{\"end\":67375,\"start\":67370},{\"end\":67387,\"start\":67385},{\"end\":67401,\"start\":67396},{\"end\":67413,\"start\":67409},{\"end\":67424,\"start\":67420},{\"end\":67436,\"start\":67431},{\"end\":67454,\"start\":67443},{\"end\":67472,\"start\":67464},{\"end\":67785,\"start\":67782},{\"end\":67795,\"start\":67792},{\"end\":67808,\"start\":67803},{\"end\":67820,\"start\":67818},{\"end\":67834,\"start\":67829},{\"end\":67846,\"start\":67842},{\"end\":67857,\"start\":67853},{\"end\":67869,\"start\":67864},{\"end\":67887,\"start\":67876},{\"end\":67905,\"start\":67897},{\"end\":68258,\"start\":68256},{\"end\":68271,\"start\":68266},{\"end\":68284,\"start\":68278},{\"end\":68296,\"start\":68293},{\"end\":68565,\"start\":68550},{\"end\":68582,\"start\":68573},{\"end\":68599,\"start\":68594},{\"end\":68967,\"start\":68964},{\"end\":68984,\"start\":68977},{\"end\":68993,\"start\":68990},{\"end\":69010,\"start\":69001},{\"end\":69018,\"start\":69016},{\"end\":69030,\"start\":69027},{\"end\":69043,\"start\":69040},{\"end\":69058,\"start\":69052},{\"end\":69352,\"start\":69346},{\"end\":69366,\"start\":69359},{\"end\":69379,\"start\":69374},{\"end\":69393,\"start\":69386},{\"end\":69412,\"start\":69407},{\"end\":69424,\"start\":69421},{\"end\":69442,\"start\":69431},{\"end\":69758,\"start\":69751},{\"end\":69775,\"start\":69766},{\"end\":69799,\"start\":69796},{\"end\":70084,\"start\":70077},{\"end\":70099,\"start\":70096},{\"end\":70114,\"start\":70107},{\"end\":70129,\"start\":70123},{\"end\":70142,\"start\":70139},{\"end\":70160,\"start\":70153},{\"end\":70175,\"start\":70169},{\"end\":70190,\"start\":70184},{\"end\":70206,\"start\":70199},{\"end\":70218,\"start\":70213},{\"end\":70236,\"start\":70229},{\"end\":70252,\"start\":70243},{\"end\":70616,\"start\":70609},{\"end\":70632,\"start\":70623},{\"end\":70821,\"start\":70814},{\"end\":70833,\"start\":70831},{\"end\":70846,\"start\":70841},{\"end\":70858,\"start\":70854},{\"end\":70872,\"start\":70866},{\"end\":70888,\"start\":70879},{\"end\":71176,\"start\":71170},{\"end\":71190,\"start\":71183},{\"end\":71204,\"start\":71197},{\"end\":71219,\"start\":71216},{\"end\":71234,\"start\":71228},{\"end\":71250,\"start\":71244},{\"end\":71262,\"start\":71258},{\"end\":71270,\"start\":71268},{\"end\":71283,\"start\":71280},{\"end\":71697,\"start\":71673},{\"end\":71711,\"start\":71706},{\"end\":71720,\"start\":71713},{\"end\":71956,\"start\":71932},{\"end\":71970,\"start\":71965},{\"end\":71979,\"start\":71972},{\"end\":72365,\"start\":72353},{\"end\":72376,\"start\":72374},{\"end\":72391,\"start\":72383},{\"end\":72396,\"start\":72393},{\"end\":72802,\"start\":72796},{\"end\":72812,\"start\":72808},{\"end\":72831,\"start\":72824},{\"end\":72845,\"start\":72838},{\"end\":73516,\"start\":73512},{\"end\":73535,\"start\":73533},{\"end\":73544,\"start\":73541},{\"end\":73558,\"start\":73552},{\"end\":73573,\"start\":73565},{\"end\":73588,\"start\":73583},{\"end\":73600,\"start\":73597},{\"end\":73614,\"start\":73607},{\"end\":73923,\"start\":73919},{\"end\":73939,\"start\":73935},{\"end\":73951,\"start\":73946},{\"end\":73963,\"start\":73958},{\"end\":73975,\"start\":73972},{\"end\":73987,\"start\":73982},{\"end\":74614,\"start\":74610},{\"end\":74626,\"start\":74622},{\"end\":74640,\"start\":74634},{\"end\":74935,\"start\":74932},{\"end\":74949,\"start\":74943},{\"end\":75160,\"start\":75148},{\"end\":75174,\"start\":75168},{\"end\":75187,\"start\":75183},{\"end\":75203,\"start\":75193},{\"end\":75218,\"start\":75211},{\"end\":75230,\"start\":75226},{\"end\":75530,\"start\":75527},{\"end\":75542,\"start\":75539},{\"end\":75555,\"start\":75550},{\"end\":75567,\"start\":75562},{\"end\":75821,\"start\":75817},{\"end\":75830,\"start\":75827},{\"end\":75845,\"start\":75841},{\"end\":75857,\"start\":75855},{\"end\":75867,\"start\":75865},{\"end\":75880,\"start\":75877},{\"end\":75893,\"start\":75889},{\"end\":76239,\"start\":76225},{\"end\":76254,\"start\":76246},{\"end\":76264,\"start\":76256},{\"end\":76578,\"start\":76573},{\"end\":76586,\"start\":76583},{\"end\":76599,\"start\":76594},{\"end\":76614,\"start\":76610},{\"end\":76623,\"start\":76620},{\"end\":76632,\"start\":76629},{\"end\":76636,\"start\":76634},{\"end\":77049,\"start\":77044},{\"end\":77063,\"start\":77059},{\"end\":77073,\"start\":77070},{\"end\":77084,\"start\":77079},{\"end\":77097,\"start\":77095},{\"end\":77109,\"start\":77106},{\"end\":77118,\"start\":77114},{\"end\":77132,\"start\":77130},{\"end\":77426,\"start\":77422},{\"end\":77441,\"start\":77437},{\"end\":77458,\"start\":77455},{\"end\":77469,\"start\":77466},{\"end\":77716,\"start\":77712},{\"end\":77730,\"start\":77728},{\"end\":77745,\"start\":77740}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3753452},\"end\":53746,\"start\":53218},{\"attributes\":{\"id\":\"b1\"},\"end\":54092,\"start\":53748},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b2\",\"matched_paper_id\":231879922},\"end\":54591,\"start\":54094},{\"attributes\":{\"id\":\"b3\"},\"end\":55619,\"start\":54593},{\"attributes\":{\"doi\":\"abs/1504.00325\",\"id\":\"b4\"},\"end\":56026,\"start\":55621},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":216080982},\"end\":56369,\"start\":56028},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b6\",\"matched_paper_id\":231802355},\"end\":56850,\"start\":56371},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":57174,\"start\":56852},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":225039882},\"end\":57868,\"start\":57176},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8081284},\"end\":58396,\"start\":57870},{\"attributes\":{\"doi\":\"abs/2109.04332\",\"id\":\"b10\",\"matched_paper_id\":237452236},\"end\":58654,\"start\":58398},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":229152766},\"end\":59498,\"start\":58656},{\"attributes\":{\"doi\":\"abs/2110.04366\",\"id\":\"b12\"},\"end\":59838,\"start\":59500},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206594692},\"end\":60285,\"start\":59840},{\"attributes\":{\"id\":\"b14\"},\"end\":60571,\"start\":60287},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b15\",\"matched_paper_id\":59599816},\"end\":61089,\"start\":60573},{\"attributes\":{\"id\":\"b16\"},\"end\":61471,\"start\":61091},{\"attributes\":{\"id\":\"b17\"},\"end\":61809,\"start\":61473},{\"attributes\":{\"id\":\"b18\"},\"end\":62211,\"start\":61811},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235309789},\"end\":62694,\"start\":62213},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4492210},\"end\":63348,\"start\":62696},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52171684},\"end\":63599,\"start\":63350},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":210911606},\"end\":63861,\"start\":63601},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":233296808},\"end\":64109,\"start\":63863},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":204960716},\"end\":65008,\"start\":64111},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":218470055},\"end\":65345,\"start\":65010},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":235377363},\"end\":65990,\"start\":65347},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":230433941},\"end\":66828,\"start\":65992},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14113767},\"end\":67283,\"start\":66830},{\"attributes\":{\"doi\":\"abs/1907.11692\",\"id\":\"b29\"},\"end\":67716,\"start\":67285},{\"attributes\":{\"doi\":\"abs/1907.11692\",\"id\":\"b30\"},\"end\":68149,\"start\":67718},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":199453025},\"end\":68482,\"start\":68151},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":235356070},\"end\":68879,\"start\":68484},{\"attributes\":{\"doi\":\"abs/2110.07577\",\"id\":\"b33\"},\"end\":69292,\"start\":68881},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3626819},\"end\":69625,\"start\":69294},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":221819379},\"end\":69999,\"start\":69627},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":231591445},\"end\":70541,\"start\":70001},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":49313245},\"end\":70754,\"start\":70543},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":160025533},\"end\":71079,\"start\":70756},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":204838007},\"end\":71608,\"start\":71081},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":215826266},\"end\":71860,\"start\":71610},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":215822736},\"end\":72263,\"start\":71862},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":10328909},\"end\":72688,\"start\":72265},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":51876975},\"end\":73445,\"start\":72690},{\"attributes\":{\"doi\":\"abs/2107.06383\",\"id\":\"b44\"},\"end\":73840,\"start\":73447},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":53178856},\"end\":74551,\"start\":73842},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":244345839},\"end\":74852,\"start\":74553},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":201103729},\"end\":75082,\"start\":74854},{\"attributes\":{\"id\":\"b48\"},\"end\":75450,\"start\":75084},{\"attributes\":{\"id\":\"b49\"},\"end\":75743,\"start\":75452},{\"attributes\":{\"doi\":\"abs/2109.05014\",\"id\":\"b50\",\"matched_paper_id\":237485500},\"end\":76101,\"start\":75745},{\"attributes\":{\"id\":\"b51\"},\"end\":76445,\"start\":76103},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":231942691},\"end\":76959,\"start\":76447},{\"attributes\":{\"doi\":\"abs/2111.03930\",\"id\":\"b53\",\"matched_paper_id\":243847522},\"end\":77365,\"start\":76961},{\"attributes\":{\"doi\":\"abs/2109.01134\",\"id\":\"b54\",\"matched_paper_id\":237386023},\"end\":77631,\"start\":77367},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":19713015},\"end\":77898,\"start\":77633}]", "bib_title": "[{\"end\":53301,\"start\":53218},{\"end\":53802,\"start\":53748},{\"end\":54162,\"start\":54094},{\"end\":56080,\"start\":56028},{\"end\":56425,\"start\":56371},{\"end\":56932,\"start\":56852},{\"end\":57300,\"start\":57176},{\"end\":57968,\"start\":57870},{\"end\":58450,\"start\":58398},{\"end\":58710,\"start\":58656},{\"end\":59884,\"start\":59840},{\"end\":60618,\"start\":60573},{\"end\":62297,\"start\":62213},{\"end\":62784,\"start\":62696},{\"end\":63405,\"start\":63350},{\"end\":63663,\"start\":63601},{\"end\":63919,\"start\":63863},{\"end\":64223,\"start\":64111},{\"end\":65088,\"start\":65010},{\"end\":65424,\"start\":65347},{\"end\":66051,\"start\":65992},{\"end\":66871,\"start\":66830},{\"end\":68247,\"start\":68151},{\"end\":68541,\"start\":68484},{\"end\":69334,\"start\":69294},{\"end\":69740,\"start\":69627},{\"end\":70070,\"start\":70001},{\"end\":70602,\"start\":70543},{\"end\":70807,\"start\":70756},{\"end\":71162,\"start\":71081},{\"end\":71665,\"start\":71610},{\"end\":71924,\"start\":71862},{\"end\":72343,\"start\":72265},{\"end\":72787,\"start\":72690},{\"end\":73911,\"start\":73842},{\"end\":74601,\"start\":74553},{\"end\":74926,\"start\":74854},{\"end\":75805,\"start\":75745},{\"end\":76565,\"start\":76447},{\"end\":77035,\"start\":76961},{\"end\":77412,\"start\":77367},{\"end\":77703,\"start\":77633}]", "bib_author": "[{\"end\":53319,\"start\":53303},{\"end\":53332,\"start\":53319},{\"end\":53347,\"start\":53332},{\"end\":53361,\"start\":53347},{\"end\":53375,\"start\":53361},{\"end\":53390,\"start\":53375},{\"end\":53401,\"start\":53390},{\"end\":53815,\"start\":53804},{\"end\":54176,\"start\":54164},{\"end\":54186,\"start\":54176},{\"end\":54196,\"start\":54186},{\"end\":54209,\"start\":54196},{\"end\":54219,\"start\":54209},{\"end\":54608,\"start\":54595},{\"end\":54623,\"start\":54608},{\"end\":54635,\"start\":54623},{\"end\":54652,\"start\":54635},{\"end\":54666,\"start\":54652},{\"end\":54685,\"start\":54666},{\"end\":54705,\"start\":54685},{\"end\":54719,\"start\":54705},{\"end\":54734,\"start\":54719},{\"end\":54749,\"start\":54734},{\"end\":54767,\"start\":54749},{\"end\":54787,\"start\":54767},{\"end\":54805,\"start\":54787},{\"end\":54819,\"start\":54805},{\"end\":54832,\"start\":54819},{\"end\":54847,\"start\":54832},{\"end\":54865,\"start\":54847},{\"end\":54877,\"start\":54865},{\"end\":54893,\"start\":54877},{\"end\":54912,\"start\":54893},{\"end\":54923,\"start\":54912},{\"end\":54936,\"start\":54923},{\"end\":54952,\"start\":54936},{\"end\":55698,\"start\":55685},{\"end\":55708,\"start\":55698},{\"end\":55722,\"start\":55708},{\"end\":55744,\"start\":55722},{\"end\":55759,\"start\":55744},{\"end\":55773,\"start\":55759},{\"end\":55793,\"start\":55773},{\"end\":56097,\"start\":56082},{\"end\":56108,\"start\":56097},{\"end\":56120,\"start\":56108},{\"end\":56136,\"start\":56120},{\"end\":56150,\"start\":56136},{\"end\":56159,\"start\":56150},{\"end\":56169,\"start\":56159},{\"end\":56183,\"start\":56169},{\"end\":56439,\"start\":56427},{\"end\":56448,\"start\":56439},{\"end\":56457,\"start\":56448},{\"end\":56471,\"start\":56457},{\"end\":56948,\"start\":56934},{\"end\":56964,\"start\":56948},{\"end\":56976,\"start\":56964},{\"end\":56996,\"start\":56976},{\"end\":57322,\"start\":57302},{\"end\":57335,\"start\":57322},{\"end\":57357,\"start\":57335},{\"end\":57375,\"start\":57357},{\"end\":57389,\"start\":57375},{\"end\":57409,\"start\":57389},{\"end\":57427,\"start\":57409},{\"end\":57446,\"start\":57427},{\"end\":57461,\"start\":57446},{\"end\":57982,\"start\":57970},{\"end\":57994,\"start\":57982},{\"end\":58016,\"start\":57994},{\"end\":58029,\"start\":58016},{\"end\":58042,\"start\":58029},{\"end\":58463,\"start\":58452},{\"end\":58471,\"start\":58463},{\"end\":58484,\"start\":58471},{\"end\":58498,\"start\":58484},{\"end\":58722,\"start\":58712},{\"end\":58738,\"start\":58722},{\"end\":58748,\"start\":58738},{\"end\":59577,\"start\":59565},{\"end\":59592,\"start\":59577},{\"end\":59603,\"start\":59592},{\"end\":59628,\"start\":59603},{\"end\":59643,\"start\":59628},{\"end\":59898,\"start\":59886},{\"end\":59913,\"start\":59898},{\"end\":59927,\"start\":59913},{\"end\":59937,\"start\":59927},{\"end\":60410,\"start\":60395},{\"end\":60424,\"start\":60410},{\"end\":60634,\"start\":60620},{\"end\":60650,\"start\":60634},{\"end\":60673,\"start\":60650},{\"end\":60688,\"start\":60673},{\"end\":60712,\"start\":60688},{\"end\":60729,\"start\":60712},{\"end\":60745,\"start\":60729},{\"end\":60760,\"start\":60745},{\"end\":61156,\"start\":61143},{\"end\":61169,\"start\":61156},{\"end\":61185,\"start\":61169},{\"end\":61203,\"start\":61185},{\"end\":61215,\"start\":61203},{\"end\":61227,\"start\":61215},{\"end\":61240,\"start\":61227},{\"end\":61578,\"start\":61562},{\"end\":61593,\"start\":61578},{\"end\":61602,\"start\":61593},{\"end\":61614,\"start\":61602},{\"end\":61627,\"start\":61614},{\"end\":61819,\"start\":61811},{\"end\":61841,\"start\":61819},{\"end\":61850,\"start\":61841},{\"end\":62333,\"start\":62299},{\"end\":62348,\"start\":62333},{\"end\":62364,\"start\":62348},{\"end\":62375,\"start\":62364},{\"end\":62802,\"start\":62786},{\"end\":62812,\"start\":62802},{\"end\":62826,\"start\":62812},{\"end\":62842,\"start\":62826},{\"end\":62854,\"start\":62842},{\"end\":62870,\"start\":62854},{\"end\":62886,\"start\":62870},{\"end\":62905,\"start\":62886},{\"end\":62916,\"start\":62905},{\"end\":62932,\"start\":62916},{\"end\":62953,\"start\":62932},{\"end\":62965,\"start\":62953},{\"end\":63416,\"start\":63407},{\"end\":63428,\"start\":63416},{\"end\":63442,\"start\":63428},{\"end\":63457,\"start\":63442},{\"end\":63674,\"start\":63665},{\"end\":63686,\"start\":63674},{\"end\":63701,\"start\":63686},{\"end\":63715,\"start\":63701},{\"end\":63935,\"start\":63921},{\"end\":63949,\"start\":63935},{\"end\":63964,\"start\":63949},{\"end\":64237,\"start\":64225},{\"end\":64249,\"start\":64237},{\"end\":64262,\"start\":64249},{\"end\":64284,\"start\":64262},{\"end\":64305,\"start\":64284},{\"end\":64316,\"start\":64305},{\"end\":64334,\"start\":64316},{\"end\":64352,\"start\":64334},{\"end\":65101,\"start\":65090},{\"end\":65116,\"start\":65101},{\"end\":65126,\"start\":65116},{\"end\":65135,\"start\":65126},{\"end\":65147,\"start\":65135},{\"end\":65161,\"start\":65147},{\"end\":65437,\"start\":65426},{\"end\":65446,\"start\":65437},{\"end\":65455,\"start\":65446},{\"end\":65467,\"start\":65455},{\"end\":65482,\"start\":65467},{\"end\":65496,\"start\":65482},{\"end\":65506,\"start\":65496},{\"end\":65519,\"start\":65506},{\"end\":65534,\"start\":65519},{\"end\":65553,\"start\":65534},{\"end\":66065,\"start\":66053},{\"end\":66075,\"start\":66065},{\"end\":66082,\"start\":66075},{\"end\":66887,\"start\":66873},{\"end\":66902,\"start\":66887},{\"end\":66918,\"start\":66902},{\"end\":66930,\"start\":66918},{\"end\":66945,\"start\":66930},{\"end\":66959,\"start\":66945},{\"end\":66973,\"start\":66959},{\"end\":66993,\"start\":66973},{\"end\":67354,\"start\":67342},{\"end\":67364,\"start\":67354},{\"end\":67377,\"start\":67364},{\"end\":67389,\"start\":67377},{\"end\":67403,\"start\":67389},{\"end\":67415,\"start\":67403},{\"end\":67426,\"start\":67415},{\"end\":67438,\"start\":67426},{\"end\":67456,\"start\":67438},{\"end\":67474,\"start\":67456},{\"end\":67787,\"start\":67775},{\"end\":67797,\"start\":67787},{\"end\":67810,\"start\":67797},{\"end\":67822,\"start\":67810},{\"end\":67836,\"start\":67822},{\"end\":67848,\"start\":67836},{\"end\":67859,\"start\":67848},{\"end\":67871,\"start\":67859},{\"end\":67889,\"start\":67871},{\"end\":67907,\"start\":67889},{\"end\":68260,\"start\":68249},{\"end\":68273,\"start\":68260},{\"end\":68286,\"start\":68273},{\"end\":68298,\"start\":68286},{\"end\":68567,\"start\":68543},{\"end\":68584,\"start\":68567},{\"end\":68601,\"start\":68584},{\"end\":68969,\"start\":68957},{\"end\":68986,\"start\":68969},{\"end\":68995,\"start\":68986},{\"end\":69012,\"start\":68995},{\"end\":69020,\"start\":69012},{\"end\":69032,\"start\":69020},{\"end\":69045,\"start\":69032},{\"end\":69060,\"start\":69045},{\"end\":69354,\"start\":69336},{\"end\":69368,\"start\":69354},{\"end\":69381,\"start\":69368},{\"end\":69395,\"start\":69381},{\"end\":69414,\"start\":69395},{\"end\":69426,\"start\":69414},{\"end\":69444,\"start\":69426},{\"end\":69760,\"start\":69742},{\"end\":69777,\"start\":69760},{\"end\":69801,\"start\":69777},{\"end\":70086,\"start\":70072},{\"end\":70101,\"start\":70086},{\"end\":70116,\"start\":70101},{\"end\":70131,\"start\":70116},{\"end\":70144,\"start\":70131},{\"end\":70162,\"start\":70144},{\"end\":70177,\"start\":70162},{\"end\":70192,\"start\":70177},{\"end\":70208,\"start\":70192},{\"end\":70220,\"start\":70208},{\"end\":70238,\"start\":70220},{\"end\":70254,\"start\":70238},{\"end\":70618,\"start\":70604},{\"end\":70634,\"start\":70618},{\"end\":70823,\"start\":70809},{\"end\":70835,\"start\":70823},{\"end\":70848,\"start\":70835},{\"end\":70860,\"start\":70848},{\"end\":70874,\"start\":70860},{\"end\":70890,\"start\":70874},{\"end\":71178,\"start\":71164},{\"end\":71192,\"start\":71178},{\"end\":71206,\"start\":71192},{\"end\":71221,\"start\":71206},{\"end\":71236,\"start\":71221},{\"end\":71252,\"start\":71236},{\"end\":71264,\"start\":71252},{\"end\":71272,\"start\":71264},{\"end\":71285,\"start\":71272},{\"end\":71699,\"start\":71667},{\"end\":71713,\"start\":71699},{\"end\":71722,\"start\":71713},{\"end\":71958,\"start\":71926},{\"end\":71972,\"start\":71958},{\"end\":71981,\"start\":71972},{\"end\":72367,\"start\":72345},{\"end\":72378,\"start\":72367},{\"end\":72393,\"start\":72378},{\"end\":72398,\"start\":72393},{\"end\":72804,\"start\":72789},{\"end\":72814,\"start\":72804},{\"end\":72833,\"start\":72814},{\"end\":72847,\"start\":72833},{\"end\":73518,\"start\":73506},{\"end\":73537,\"start\":73518},{\"end\":73546,\"start\":73537},{\"end\":73560,\"start\":73546},{\"end\":73575,\"start\":73560},{\"end\":73590,\"start\":73575},{\"end\":73602,\"start\":73590},{\"end\":73616,\"start\":73602},{\"end\":73925,\"start\":73913},{\"end\":73941,\"start\":73925},{\"end\":73953,\"start\":73941},{\"end\":73965,\"start\":73953},{\"end\":73977,\"start\":73965},{\"end\":73989,\"start\":73977},{\"end\":74616,\"start\":74603},{\"end\":74628,\"start\":74616},{\"end\":74642,\"start\":74628},{\"end\":74937,\"start\":74928},{\"end\":74951,\"start\":74937},{\"end\":75162,\"start\":75142},{\"end\":75176,\"start\":75162},{\"end\":75189,\"start\":75176},{\"end\":75205,\"start\":75189},{\"end\":75220,\"start\":75205},{\"end\":75232,\"start\":75220},{\"end\":75532,\"start\":75522},{\"end\":75544,\"start\":75532},{\"end\":75557,\"start\":75544},{\"end\":75569,\"start\":75557},{\"end\":75823,\"start\":75807},{\"end\":75832,\"start\":75823},{\"end\":75847,\"start\":75832},{\"end\":75859,\"start\":75847},{\"end\":75869,\"start\":75859},{\"end\":75882,\"start\":75869},{\"end\":75895,\"start\":75882},{\"end\":76241,\"start\":76218},{\"end\":76256,\"start\":76241},{\"end\":76266,\"start\":76256},{\"end\":76580,\"start\":76567},{\"end\":76588,\"start\":76580},{\"end\":76601,\"start\":76588},{\"end\":76616,\"start\":76601},{\"end\":76625,\"start\":76616},{\"end\":76634,\"start\":76625},{\"end\":76638,\"start\":76634},{\"end\":77051,\"start\":77037},{\"end\":77065,\"start\":77051},{\"end\":77075,\"start\":77065},{\"end\":77086,\"start\":77075},{\"end\":77099,\"start\":77086},{\"end\":77111,\"start\":77099},{\"end\":77120,\"start\":77111},{\"end\":77134,\"start\":77120},{\"end\":77428,\"start\":77414},{\"end\":77443,\"start\":77428},{\"end\":77460,\"start\":77443},{\"end\":77471,\"start\":77460},{\"end\":77718,\"start\":77705},{\"end\":77732,\"start\":77718},{\"end\":77747,\"start\":77732}]", "bib_venue": "[{\"end\":54346,\"start\":54293},{\"end\":56598,\"start\":56545},{\"end\":59065,\"start\":58912},{\"end\":60078,\"start\":60016},{\"end\":64519,\"start\":64441},{\"end\":66399,\"start\":66246},{\"end\":73028,\"start\":72936},{\"end\":74165,\"start\":74078},{\"end\":53463,\"start\":53401},{\"end\":53910,\"start\":53815},{\"end\":54291,\"start\":54223},{\"end\":54966,\"start\":54952},{\"end\":55683,\"start\":55621},{\"end\":56187,\"start\":56183},{\"end\":56543,\"start\":56475},{\"end\":57001,\"start\":56996},{\"end\":57513,\"start\":57461},{\"end\":58107,\"start\":58042},{\"end\":58517,\"start\":58512},{\"end\":58910,\"start\":58748},{\"end\":59563,\"start\":59500},{\"end\":60014,\"start\":59937},{\"end\":60393,\"start\":60287},{\"end\":60808,\"start\":60764},{\"end\":61141,\"start\":61091},{\"end\":61560,\"start\":61473},{\"end\":61999,\"start\":61850},{\"end\":62438,\"start\":62375},{\"end\":63005,\"start\":62965},{\"end\":63462,\"start\":63457},{\"end\":63719,\"start\":63715},{\"end\":63969,\"start\":63964},{\"end\":64439,\"start\":64352},{\"end\":65166,\"start\":65161},{\"end\":65657,\"start\":65553},{\"end\":66244,\"start\":66082},{\"end\":67031,\"start\":66993},{\"end\":67340,\"start\":67285},{\"end\":67773,\"start\":67718},{\"end\":68305,\"start\":68298},{\"end\":68665,\"start\":68601},{\"end\":68955,\"start\":68881},{\"end\":69449,\"start\":69444},{\"end\":69805,\"start\":69801},{\"end\":70258,\"start\":70254},{\"end\":70639,\"start\":70634},{\"end\":70901,\"start\":70890},{\"end\":71321,\"start\":71285},{\"end\":71726,\"start\":71722},{\"end\":72043,\"start\":71981},{\"end\":72460,\"start\":72398},{\"end\":72934,\"start\":72847},{\"end\":73504,\"start\":73447},{\"end\":74076,\"start\":73989},{\"end\":74691,\"start\":74642},{\"end\":74956,\"start\":74951},{\"end\":75140,\"start\":75084},{\"end\":75520,\"start\":75452},{\"end\":75914,\"start\":75909},{\"end\":76216,\"start\":76103},{\"end\":76690,\"start\":76638},{\"end\":77153,\"start\":77148},{\"end\":77490,\"start\":77485},{\"end\":77751,\"start\":77747}]"}}}, "year": 2023, "month": 12, "day": 17}