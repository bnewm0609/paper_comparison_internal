{"id": 261697282, "updated": "2023-10-04 20:54:19.74", "metadata": {"title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs", "authors": "[{\"first\":\"Wenhua\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Weiwei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Haihao\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Yiyang\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Kaokao\",\"last\":\"Lv\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gradient descent, enabling us to achieve outstanding results within 400 steps. SignRound competes impressively against recent methods without introducing additional inference overhead. The source code will be publicly available at \\url{https://github.com/intel/neural-compressor} soon.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2309.05516", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2309-05516", "doi": "10.48550/arxiv.2309.05516"}}, "content": {"source": {"pdf_hash": "633e3fe49fe9c314f7245f77401c2e4a95e925a9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2309.05516v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e547aa65227c0e8aa3c58d6d9a2f740a4984c92b", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/633e3fe49fe9c314f7245f77401c2e4a95e925a9.txt", "contents": "\nOPTIMIZE WEIGHT ROUNDING VIA SIGNED GRADI- ENT DESCENT FOR THE QUANTIZATION OF LLMS\n\n\nWenhua Cheng \nIntel\n\n\nWeiwei Zhang \nIntel\n\n\nHaihao Shen \nIntel\n\n\nYiyang Cai \nIntel\n\n\nXin He \nIntel\n\n\nKaokao Lv \nIntel\n\n\nOPTIMIZE WEIGHT ROUNDING VIA SIGNED GRADI- ENT DESCENT FOR THE QUANTIZATION OF LLMS\nPreprint. Under review.\nLarge Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weightonly quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gradient descent, enabling us to achieve outstanding results within 400 steps. SignRound competes impressively against recent methods without introducing additional inference overhead. The source code will be publicly available at\n\nINTRODUCTION\n\nLarge language models (LLMs) have demonstrated exceptional proficiency on language-related tasks(OpenAI; Touvron et al., 2023a). Nevertheless, the deployment of LLMs presents notable hurdles due to their extensive memory and storage needs. Moreover, the computational demands of these models leads to the challenges for real-time applications. Consequently, it becomes imperative to explore techniques like quantization to facilitate the efficient deployment of LLMs.\n\nQuantization techniques can be broadly classified into two categories: quantization-aware training (QAT) (Esser et al., 2019;Zhuang et al., 2021;Lee et al., 2021; and post-training quantization (PTQ) (Nagel et al., 2019;Xiao et al., 2022;Nagel et al., 2020). QAT involves training the model with quantization in mind. During QAT, the model is trained using simulated lower-precision representations, allowing it to learn and adapt to the effects of quantization. This approach often yields better accuracy compared to PTQ. However, QAT comes with certain drawbacks, including increased training complexity, longer training times, and the need to tune hyperparameters. Applying QAT to LLMs can be particularly costly, despite recent efforts Dettmers et al., 2023) to improve the efficiency of fine-tuning LLMs. In contrast, PTQ directly quantizes the model without any simulated training or fine-tuning. While PTQ is a concise approach, it is susceptible to significant accuracy drops. This highlights the need for further advancements in PTQ methods to enhance their accuracy preservation capabilities.\n\nTwo types of tensors could be quantized: activations and weights. Weight-only quantization has gained prominence in recent times as it offers a favorable tradeoff for LLMs. Quantizing activations for LLMs can be challenging (Wei et al., 2022b;Xiao et al., 2023;Bondarenko et al., 2023), making weight-only quantization a more practical choice. Additionally, the primary bottleneck in generating new tokens for LLMs often lies in memory bandwidth , further emphasizing the significance of weight-only quantization. In this work, we only focus on weight only quantization.\n\nIn order to quantize the weights, a rounding operation is necessary, with rounding-to-nearest (RTN) being the predominant method. RTN quantizes each element independently by simply rounding it to the nearest integer. However, RTN fails to consider the relationships between weights and weights, as well as weights and activations. The potential of an advanced rounding strategy to improve accuracy has been initially demonstrated by Nagel et al. (Nagel et al., 2020). They addressed the rounding task by formulating it as a quadratic unconstrained binary optimization problem and approximated the task loss by employing a Taylor series expansion. However, relying exclusively on the second-order term may not produce accurate results. This is because rounding can introduce considerable weight modifications that may make other order terms significant and non-negligible.\n\nWe prefer the signed gradient descent method to effectively tackle the issue of sub-optimal rounding solutions. This approach is inspired by the well-defined boundaries of the solution space, which are confined to the range of [-0.5, 0.5], where only the threshold for altering the rounding value is of significance. Figure 1 provides an overview of our method, SignRound. It utilizes signed gradient descent to fine-tune the up and down rounding through block-wise output reconstruction, resulting in enhanced flexibility and faster convergence. Our contributions are primarily threefold:\n\n\u2022 We introduce a succinct and potent method for optimizing the weight-rounding task. Our approach utilizes a minimal amount of unlabeled data and executes quantization in a blockwise fashion. Moreover, it is worth noting that our method does not introduce any additional overhead during inference, further enhancing its general practicality. \u2022 Our findings demonstrate that a mere alteration of approximately 5% of the rounding values can significantly enhance the performance of some quantization models. \u2022 Our empirical results exhibit substantial performance enhancements over the established baseline of RTN, and our method contends favorably against recent techniques.\n\n\nRELATED WORK\n\nQuantization Aware Training. QAT methods have gained widespread popularity in model compression, as they enable the fine-tuning process, often leading to superior accuracy compared to the PTQ method. In their work, (Esser et al., 2019) proposed a novel approach that estimates and scales the task loss gradient at each weight and activation layer's quantizer step size, allowing for joint learning with other network parameters. (Zhuang et al., 2021) put forward a progressive quantization scheme that involves quantizing activations after weights. Additionally, CPQ (Lee et al., 2021) effectively identified the optimal quantization grids while naturally encouraging the underlying fullprecision weights to gather around those quantization grids cohesively during training. While QAT methods are popular in relatively small-scale models, their application in LLMs is limited due to the high computational cost associated with training or fine-tuning.\n\nPost-training Quantization (PTQ). PTQ methods simplify the quantization process without the needs of additional training. (Nagel et al., 2019) focused on minimizing quantization error through weight equalization and bias correction techniques.  specifically addressed the quantization of vision transformers, introducing a ranking loss to preserve the relative order of selfattention results after quantization and exploring a mixed-precision quantization scheme.  leveraged Optimal Brain Surgeon (Hassibi et al., 1993) to tune weights during model compression. Both Hawq (Yao et al., 2021) and HAQ (Wang et al., 2019) aimed to identify important layers and maintain higher precision for them. Given its low resource requirement, PTQ is particularly suitable for the quantization of Large Language Models (LLMs). We will next focus on the quantization methods designed for LLMs, most of which fall under the category of PTQ.\n\nLarge Language Models Quantization. Significant advancements have been made in addressing the pressing demand for quantizing large language models (LLMs). LLM.int8() (Dettmers et al., 2022) introduced a mixed precision approach to preserve essential channels in high precision. Zero-QuantV2 (Yao et al., 2023) employed low-rank matrices to enhance model quality recovery. RPTQ (Yuan et al., 2023) mitigated the impact of range differences between channel by rearranging the channels and quantizing them in clusters. Other methods, such as SPIQ (Yvinec et al., 2023), SmoothQuant (Xiao et al., 2022), Outlier Suppression+ (Wei et al., 2023), utilized handcrafted equivalent transformations to mitigate quantization errors. While these approaches are effective, their applicability is limited due to the performance overhead involved during inference, because there is no chance to fuse the transformation scale to the model itself on certain model architectures. LLM-QAT  employs QAT to enhance the performance of W4A8. In the context of weight-only quantization, GPTQ  optimized weights using the Optimal Brain Surgeon (Hassibi et al., 1993) technique, achieving low-bit quantization on LLMs with minimal computational overhead. AWQ  followed the equivalent transformation approach with additional tuning in a constrained space, and has the similar limitations as SmoothQuant (Xiao et al., 2022). SqueezeLLM  employed sensitivity-based non-uniform quantization and dense-and-sparse decomposition to achieve lossless compression to ultra-low precision. While recent advancements in LLM quantization have made significant progress, there is still room for improvement in achieving minimal quantization loss without introducing inference overhead.\n\nRounding Methods. Adaptive Rounding (Nagel et al., 2020) has already showcased the potential of an advanced rounding strategy to enhance accuracy Wei et al., 2022a). They used the rounding task as a quadratic unconstrained binary optimization problem by approximating the task loss through a Taylor series expansion. However, considering only the second-order term may not yield accurate results. This is because the rounding value gets multiplied by a scaling coefficient during de-quantization, potentially introducing significant weight changes that make other order terms non-negligible. FlexRound (Lee et al., 2023) introduces a more flexible approach to rounding by incorporating element-wise division. This allows for simultaneous learning of a shared quantization grid size and individual scales for each pre-trained weight. However, it's not easily scalable to apply to LLMs due to the needs of specialized hyperparameters for each specific model and task. AQuant  introduced a dynamic approach where the border becomes a function dependent on the activation value to reduce the quantization error of activation. We specifically concentrate on the up and down rounding task for weight quantization in this work.\n\nSigned Gradient Descent. Signed gradient descent is not commonly utilized and is typically applied in specific scenarios, such as reducing communication costs. This is because signed gradient carries significantly less information compared to original gradient. Recent studies have shed light on the advantages of sign-based methods over gradient descent in certain conditions. Safaryan et al. (Safaryan & Richt\u00e1rik, 2021) found that sign-based methods are preferable when the Hessian matrix is concentrated on its diagonal and the maximal eigenvalue is much larger than the average eigenvalue. Li et al.  investigated a variant of sign-based gradient descent that exhibits faster convergence. Additionally, Safaryan et al. (Safaryan & Richt\u00e1rik, 2021) proposed a stochastic sign descent with momentum, which converges under the standard bounded variance assumption with the optimal asymptotic rate. These findings contribute to a better understanding of the potential benefits and applications of signed gradient descent methods.\n\n\nMETHODOLOGY\n\nWe provide an overview of quantization before diving into the details of our approach. To quantize and de-quantize the weights, the following operation as shown in Eq.1 is used (disregarding zero point for simplicity).\nW = s * clip( W s , n, m), n, m \u2208 N(1)\nwhere s is the quantization scale, which is a positive scalar value. However, it is important to mention that our method can be easily extended to cases where s is a vector or tensor. And the rounding operation \u230a\u00b7\u2309 is typically performed using the RTN method. While RTN is a concise approach, it quantizes each element independently, thereby losing the ability to model the correlation among different weights or activations.\n\nTo introduce more flexibility into the rounding operation, a tensor V with the same shape of W is introduced. Each element of V falls within the range of [\u2212B, B], in which B is set to 0.5 in all of experiments to ensure that the changes made only impact the rounding value.\nW = s * clip( W s + V , n, m), n, m \u2208 N(2)\nFigure 1: An illustration of SignRound. Unlike the direct rounding in RTN, SignRound performs signed gradient descent to fine-tune the up and down rounding through block-wise output reconstruction. After lightweight forward and backward steps, WINT4 has been well optimized towards the minimal loss, therefore ready for the final inference deployment. Note that Quant and Dequant are two standard operations for quantization and dequantization respectively.\n\nThis adjustment allows for a more adaptable and context-aware quantization process. If we try to reconstruct the output of layers, the loss could be formulated as\nL = ||W X \u2212 W X|| 2 F (3)\nwhere X is the input of the layer and || \u00b7 || F denotes the Frobenius norm. Then the final optimization task is described as the following\narg min V ||W X \u2212 W X|| 2 F(4)\n\nSIGNROUND\n\nSince V has a clear boundary, i.e. [\u22120.5, 0.5], and only the threshold for altering the rounding value is of significance, we prefer scaled signed gradient descent instead of normal gradient descent to optimize this task. Figure 1 shows an illustration of our method. More precisely, we follow the below optimization to approach the sub-optimal solution of Eq. 4.\nV t+1 = V t \u2212 lr * sign( \u2202L \u2202V ) s.t.| t lr * sign( \u2202L \u2202V )| \u2264 B(5)\nwhere t is the optimizing step, lr is the learning rate, | \u00b7 | is the absolute operation and B is the boundary we use, which is set to 0.5 in all our experiments.\n\nFurther, by employing straight-through estimator (STE) (Bengio et al., 2013), it can be easily demonstrated that sign( \u2202L \u2202V ) = sign( \u2202L \u2202W ) in Eq. 5 as following since elements of s are all positive.\n\u2202L \u2202W = \u22122(W X \u2212 W X)X T (6) \u2202L \u2202V = \u22122s(W X \u2212 W X)X T(7)\nSo our optimization could be simplified as\nV t+1 = V t \u2212 lr * sign( \u2202L \u2202W ) s.t.| t lr * sign( \u2202L \u2202W )| \u2264 B(8)\nAlgorithm 1 SignRound Input: Calibration Data D, learning rate lr, total steps T , Model M , block module m w with weights w, zero initialized V , batch size bs\nOutput: best V 1: V \u2190 0, best V \u2190 0, best l \u2190 maximum 2: for i \u2190 0 to T do 3: d \u2190 draw bs samples from D 4: x \u2190 M (d) m \u25b7 get the inputs of m 5: y f \u2190 m w (x)\n\u25b7 get the output of original module 6:\n\nw \u2190 qdq(w, V ) \u25b7 quantize and dequantize w via Eq.2 7:\ny q \u2190 m w (x)\n\u25b7 get the output of quantized module 8:\n\nloss \u2190 mse(y q , y f ) \u25b7 get the loss via Eq.3 9:\n\nif loss < best l then 10:\n\nbest V \u2190 V update V via Eq. 8 15: end for Moreover, as Eq 3 averages the loss of each element, which presumes that each one contributes equally to the network, that basically is not true. To alleviate this issue, we optimize the rounding task blockwise. To clarify, in our context, we use the term 'layer' to refer to a linear/convolution layer, while 'block' denotes a transformer block that typically consists of several linear layers.\n\nThe above pseudocode 1 presents more details of SignRound.\n\n\nEXPERIMENTS\n\nIn this section, we conduct a comprehensive evaluation of SignRound from various perspectives. Firstly, we provide a brief overview of the LLM architectures and tasks that are included in our evaluation. Secondly, we present a detailed comparison between our method and some other existing approaches, highlighting the unique features and advantages of SignRound. Thirdly, we conduct additional experiments to further demonstrate the validity of our choices, assess the sensitivity of hyperparameters, and explore other relevant factors. Finally, the runtime is reported in Appendix D for reference.\n\n\nEXPERIMENTAL SETTINGS\n\nEvaluation and Datasets. We make assessments on several language tasks to satisfy the taskagnostic setting. Specifically, we report average accuracy results on four common sense reasoning tasks including HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), PIQA (Bisk et al., 2020) and LAMBADA (Paperno et al., 2016).Additionally, we benchmarked our models on MMLU (?), which encompasses 57 tasks spanning STEM, humanities, social science, and more. Evaluation for all these tasks was performed using the lm-eval-harness . Furthermore, we complement our evaluation with perplexity (ppl) analysis on Wikitext2 (Merity et al., 2016) and C4 (Raffel et al., 2020), by following the source code 1 of GPTQ.\n\nQuantization Configurations. In line with the approach taken in GPTQ , we specifically concentrate on weight-only quantization, targeting the linear layers within transformer blocks. Other layers, such as the embedding layer and typically the last layer like lm-head, are excluded from the quantization process. We initially intended to utilize the pile (?) dataset for calibration, following AWQ  and SmoothQuant (Xiao et al., 2022). However, due to its large size, we have opted to use the readily available pile-10k dataset 2 , which consists of the first 10k samples from pile, for both GPTQ and our method. We employ standard uniform per-row asymmetric quantization on the min-max grid. Our evaluation primarily focuses on W4, W4G128, and W3G128, where W4 indicates quantizing weights with 4 bits and G represents finer-granularity grouping as described in (Park et al., 2022;.\n\nLarge Language Models. Our experimental evaluation encompasses a range of widely adopted LLM architectures, such as LLaMAs (Touvron et al., 2023a), LLaMAs v2 (Touvron et al., 2023b), BLOOMs (Scao et al., 2022), and OPTs (Zhang et al., 2022). We cover a wide range of LLM parameters, ranging from millions to billions, to ensure comprehensive coverage and analysis.\n\nSignRound Hyperparameters. We selected 512 samples randomly from pile-10k and truncated each sample to a sequence length of 512. The tuning process involves adjusting each block for 400 steps using a learning rate of 2.5e-3, a batch size of 8, and employing a linear learning rate decay. We set the value of B in Eq. 8 to 0.5. Besides, we adopted automatic mixed precision(AMP) to accelerate the tuning. It's worth noting that adjusting the sequence length to 2048 yielded improvements in numerous scenarios. However, we did not adopt this as the default setting due to the associated runtime overhead. For models \u2265 30B, we made configuration adjustments to strike a balance between runtime and performance. Specifically, we reduced the sample count to 256, shorted the sequence length to 256, and disabled AMP.  \n\n\nCOMPARING WITH OTHER METHODS\n\nWe conducted a comprehensive benchmarking of our results against RTN and GPTQ . However, it is important to highlight that act-order was not enabled in GPTQ due to the kernel overhead it introduces , although it has the potential to improve accuracy for certain models. When evaluating perplexity (ppl), we prioritize reporting the ppl on C4 dataset as our primary focus, taking into consideration the potential occurrence of NaN values when assessing perplexity for Wikitext2 and ptb datasets, both for SignRound and GPTQ. Furthermore, we conducted a limited and non-rigorous comparison between our approach and AWQ  in Appendix A.1.\n\nWe begin by presenting the average accuracy results for the HellaSwag, WinoGrand, PIQA, and LAMBADA tasks across LLaMA, OPT, and BLOOM models with a size below 13B. These results   Table 1 and 2. In conclusion, our method outperforms RTN in 36 out of 39 scenarios, showcasing its effectiveness. Additionally, when comparing our approach to GPTQ, we surpass it in 32 out of 39 scenarios, further highlighting the strengths of our method. While our method showcases overall effectiveness, it is important to acknowledge the presence of outliers, such as OPT6.7B at W3G128. Although the root cause for this has not been identified yet, it could be mitigated by fine-tuning hyperparameters, as discussed in the following sections. For detailed results of LLaMA7B, LLaMA13B, LLAMA7B-V2, and LLAMA13B-V2, please refer to Appendix E. The results in Appendix E also highlight that changing the sequence length to 2048 could bring noticeable improvement in many scenarios.\n\nWe then present the perplexity (ppl) results for C4 in Table 3, along with the detailed results for Wikitext2 in Appendix A.2. In conclusion, we achieve better or comparable performance in 9 out of 12 models. In certain cases where the results may not be optimal, we can still fine-tune the hyperparameters to achieve better results, as demonstrated in the subsequent sections.\n\nNext, we present a comprehensive breakdown of the accuracies achieved by MMLU for LLaMA-7B and LLaMa-7B-V2 in Table 4. By analyzing the average accuracies, we observe that SingRound outperforms RTN and GPTQ in 4 out of the 6 scenarios when the best model-wise setting is applied.\n\nWe also provide the results for models with a capacity of 30B or greater at W3G128 in Table 5 and W4 in Appendix A.3. Additionally, we discovered that recovering the sequence length to 512 of the calibration dataset yielded improvements in certain scenarios, and thus we include these results. In summary, our approach achieves comparable performance to GPTQ for the given accuracy task. However, we slightly lag behind GPTQ in terms of ppl tasks.\n\n\nBLOCK-WISE VERSUS LAYER-WISE\n\nWe examined the effects of layer-wise and block-wise tuning. As explained in Section 3.1, the term \"layer\" refers to a linear/convolution layer, while \"block\" specifically denotes a transformer block consisting of multiple linear layers. To simplify this evaluation, we set the sequence length to 256 and disable AMP. Based on the below results, block-wise tuning outperformed layer-wise tuning in the majority of scenarios. \n\n\nTHE ANALYSIS OF HYPERPARAMETERS SENSITIVITY\n\nWe conducted a hyperparameters sensitivity analysis, the results of which are summarized in Table  7. In the \"steps100\" configuration, we used 100 steps, and a learning rate of 1e-2. In the \"lr4e-3\" configuration, we set the learning rate to 4e-3. We also changed the sequence length of the calibration dataset from 512 to 2048, denoted by \"seq2048\". Please note that all other hyperparameters not mentioned in each configuration were kept the same as the default configurations, as detailed in Section 4.1. Overall, our method exhibits robustness to hyperparameters in common sense reasoning tasks, with the exception of the perplexity of LLaMA-7b-v2. However, we did discover that certain hyperparameters, such as the sequence length of the calibration dataset, can significantly impact performance in some scenarios, as demonstrated in Table 4 and 5.  Additionally, we observe an interesting pattern in the distribution of V across different layers. The middle layers exhibit a more tightly clustered distribution compared to the other layers. This observation aligns with the common understanding that the head and tail layers tend to be more sensitive to compression, while the middle layers are relatively more robust. Figure 2 illustrates the impact of the rounding value introduced by the V in Eq. 2 for models around 7B at W4. The red line represents \"up rounding\", indicating that while RTN rounds the value to the floor, SignRound changes it to the ceiling. Conversely, the green line represents \"down rounding\" indicating that while RTN rounds the value to the ceiling, SignRound changes it to the floor. It is worth noting that SignRound modifies only a small percentage of weight rounding values for each of the four models, namely 5.27%, 5.29%, 4.14%, and 4.10%.\n\nWe were also intrigued by the possible correlation between rounding and activation, as previous research has shown that keeping only 0.1%-1% of the channels corresponding to larger activation can significantly improve the quantized performance in AWQ . We shown the result in Appendix C.\n\n\nCONCLUSIONS AND LIMITATIONS\n\nIn this paper, we present a highly effective and concise approach to optimize the weight rounding task. Our method, SignRound, leverages lightweight block-wise tuning using signed gradient descent, achieving remarkable results within a mere 400 steps. Extensive experiments demonstrate the superior performance of our approach. As part of our future work, we plan to apply our approach to more diverse LLM models (e.g., Code LLaMA (Rozi\u00e8re et al., 2023), LLaMA v2 Chat (Touvron et al., 2023b)), and contribute our recipes and implementations to the open source community. On the other hand, although our method is generally effective, there are a few outliers in certain scenarios, where we plan to mitigate the issue by fine-tuning the hyperparameters.\n\n\nA MORE RESULTS\n\n\nA.1 NON-RIGOROUS COMPARISON WITH AWQ\n\nWe conducted a limited comparison between our approach and AWQ , considering that our evaluation methodology closely follows that of GPTQ and we only share a few common tasks with AWQ. It is important to acknowledge that this comparison inherently lacks rigor due to our reliance on referencing AWQ's data alone. Consequently, this approach introduces the possibility of unfairness in the evaluation process, primarily stemming from the utilization of different calibration datasets and other potential factors that may influence the obtained results.\n\nWe present the results of our common tasks alongside AWQ in table 8 and all the results of AWQ are from their paper. While we both report MMLU results, it is important to note that there was a bug fix 3 in lm-eval, resulting in significant changes to the baseline. So we have not included them in this report. The perplexity results for Wikitext2 at W4 are shown in Table 9. In conclusion, our performance is comparable to that of GPTQ. We present the results for models with a capacity of 30B or higher at W4 in Table 10 and PPL on Wikitext2 in Table 11. Furthermore, we observed that adjusting the sequence length of the calibration dataset led to improvements in specific scenarios, and we include these findings in our analysis. Overall, our approach demonstrates comparable accuracy performance to GPTQ for the given task. However, it is worth noting that we slightly fall behind GPTQ in terms of PPL tasks. We provide an analysis of the magnitude distribution of V in Eq. 2 for approximately 7B models at W4 in Figure 3. The findings reveal that the majority of V values are concentrated within the range of [-0.3, 0.3]. Notably, the middle layers demonstrate a narrower distribution in comparison to the other layers. This observation suggests that the head or tail layers may be more susceptible to the compression. LLaMA-7B LLaMA-7B-V2\n\nOPT-6.7B BLOOM-7B1 Figure 3: The distribution of the magnitude of V in Eq. 2 for different models, namely LLaMA-7B, LLaMA-7B-V2, OPT-6.7B, and BLOOM-7B1 at W4. Each color in the distribution represents a specific layer index in the models, with blue indicating shallow layers closer to the data layer, and red representing deeper layers.\n\n\nC CORRECTION BETWEEN SIGNROUND AND SALIENT ACTIVATION\n\n\nCHANNELS\n\nWe were also intrigued by the possible correlation between rounding and activation, as previous research has shown that keeping only 0.1%-1% of the channels corresponding to larger activation can significantly improve the quantized performance in AWQ . Therefore, we investigated whether the altered rounding values tend to fall more frequently in these salient channels. The results of our analysis, presented in Figure 4, reveal an interesting finding. The ratio, representing the percentage of altered rounding values falling within the top 1% salient activation channels out of all altered rounding values, is typically around 1%. This suggests that there is no strong correlation between rounding and activation. It is possible that rounding values of less significant channels need to be changed to compensate for the quantization error introduced by these salient channels.\n\nLLaMA-7B LLaMA-7B-V2\n\nOPT-6.7B BLOOM-7B1 Figure 4: The correction between SignRound and salient activation channels D RUNTIME Table 12 provides a runtime comparison between GPTQ and our method. All measurements were conducted on a single NVIDIA A100 card with 80GB of memory. Although our method demonstrates slightly slower performance compared to GPTQ, it remains well within acceptable limits for real-world deployment. Detailed results of LLaMA7B, LLaMA13B, LLAMA7B-V2, and LLAMA13B-V2 can be found in Table 13, Table 14, Table 15, and Table 16 respectively. \n\n67 4. 5 Figure 2 :\n6752THE ANALYSIS OF GRADIENTS AND THEIR EFFECTS ON ROUNDING In this analysis, we dive into the distribution of the magnitude of V in Eq. 2 and its impact on rounding values across approximately 7 billion models at W4. The visual representations of these The impact of the rounding value introduced by the V in Eq. 2 distributions are provided in Appendix B. Our investigation reveals that the majority of V values are concentrated within the range of [-0.3, 0.3].\n\nTable 1 :\n1Average % accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA for LLaMA & OPT.nbits \nmethods \nLLaMA \nOPT \n7b \n13b \n7bv2 13bv2 125m 1.3b 2.7b 6.7b \n13b \n16 \nFP16 \n68.80 71.14 69.02 71.20 45.09 57.66 61.04 64.92 65.49 \n\nW4 \n\nRTN \n67.38 68.82 66.98 70.17 39.41 47.22 58.61 62.99 64.08 \nGPTQ 64.70 70.00 66.89 69.24 43.58 56.15 59.92 63.09 64.83 \nOurs \n68.05 70.58 67.74 70.03 44.13 56.17 60.58 64.34 65.05 \n\nW4G128 \n\nRTN \n67.85 70.84 68.32 70.72 45.27 56.47 60.70 64.03 64.84 \nGPTQ 66.32 70.92 68.90 70.68 42.88 56.99 61.23 64.75 65.37 \nOurs \n68.09 71.43 68.65 70.81 44.23 57.30 60.86 64.76 65.67 \n\nW3G128 \n\nRTN \n64.94 67.70 65.92 68.70 39.11 42.61 36.99 56.09 49.56 \nGPTQ 58.29 68.73 65.51 68.73 39.78 54.43 58.47 62.98 64.68 \nOurs \n66.62 69.59 66.88 69.70 43.31 55.46 59.12 53.42 63.61 \n\n\n\nTable 2 :\n2Average % accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA for BLOOM.W4 \nW4G128 \nW3G128 \nSize \n560m 1b7 \n3b \n7b1 560m 1b7 \n3b \n7b1 560m 1b7 \n3b \n7b1 \nFP16 45.50 52.31 55.48 60.22 45.50 52.31 55.48 60.22 45.50 52.31 55.48 60.22 \nRTN 43.10 49.97 53.16 57.73 44.28 52.08 54.86 59.31 40.83 47.98 52.51 57.59 \nGPTQ 43.95 50.91 54.65 58.27 44.79 52.08 55.68 59.59 42.74 48.81 53.41 58.12 \nOurs 45.00 51.47 54.63 59.52 45.40 51.85 55.40 59.83 44.08 50.52 53.64 58.69 \n\n\n\nTable 3 :\n3C4 ppl ( \u2193) at W4.LLaMA \nOPT \nBLOOM \nSize \n7b \n13b 7bv2 13bv2 1.3b \n2.7b \n6.7b \n13b 560m 1b7 \n3b \n7b1 \nFP16 7.34 6.80 7.26 \n6.73 16.07 14.34 12.71 12.06 26.59 19.49 17.48 15.20 \nRTN 8.12 7.23 8.16 \n7.14 27.49 18.83 14.37 13.32 29.87 21.25 18.76 16.05 \nGPTQ 8.64 7.13 7.90 \n6.87 17.04 15.06 13.39 12.29 28.15 20.71 18.18 15.67 \nOurs 7.84 7.05 11.20 7.72 16.92 14.97 13.08 12.48 28.12 20.41 18.18 15.67 \n\n\n\nTable 4 :\n4Accuracies(\u2191) of MMLU(5-shot) for LLaMA-7B & LLaMA-7B-V2. \"Ours-2048\" indicates that we have modified the sequence length of the calibration dataset from 512 to 2048.LLaMA-7B \nLLaMA-7B-V2 \nHums. STEM Social Other Avg. Hums. STEM Social Other Avg. \nFP16 \n38.32 31.17 38.05 36.85 35.64 51.40 37.00 52.23 49.51 46.56 \n\nW4G-1 \n\nRTN \n34.84 29.53 32.87 36.28 33.10 44.03 32.83 44.97 42.19 40.24 \nGPTQ 33.31 26.29 29.86 33.11 30.32 46.21 34.29 46.68 44.85 42.21 \nOurs \n34.30 31.05 34.74 36.66 33.95 47.28 33.14 46.90 44.70 42.10 \nOurs2048 35.10 30.69 36.43 36.85 34.42 47.40 33.92 49.61 44.91 43.00 \n\nW4G128 \n\nRTN \n36.30 31.67 37.40 37.99 35.48 49.54 36.50 50.95 47.87 45.31 \nGPTQ 37.77 29.64 36.38 37.45 34.83 50.30 36.51 50.91 47.69 45.43 \nOurs \n36.06 30.86 35.99 36.21 34.44 51.39 37.87 52.56 49.69 46.95 \nOurs2048 35.66 30.05 36.16 37.57 34.46 50.12 36.70 51.44 48.20 45.69 \n\nW3G128 \n\nRTN \n32.97 30.28 33.66 32.60 32.17 41.14 33.06 40.98 40.94 38.51 \nGPTQ 30.77 28.29 30.73 31.33 30.12 44.66 37.55 46.36 43.47 42.48 \nOurs \n30.12 28.21 30.64 30.34 29.68 44.53 33.53 44.60 43.52 40.82 \nOurs2048 32.43 28.62 31.03 32.10 30.85 42.75 32.98 42.88 41.30 39.34 \n\nare shown in \n\nTable 5 :\n5Average % accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA and C4 ppl(\u2193) for LLaMA & OPT with \u2265 30B at W3G128. \"Ours-seq512\" indicates that we have modified the sequence length of the calibration dataset from 256 to 512.Accuracy \nPPL on C4 \nType \nLLaMA \nOPT \nLLaMA \nOPT \nSize \n30b \n65b \n30b \n66b \n30b 65b \n30b \n66b \nFP16 \n73.46 75.48 67.87 69.54 6.13 5.98 11.46 10.99 \nRTN \n72.17 73.69 62.83 38.00 6.85 6.52 30.81 285.41 \nGPTQ \n72.09 73.97 66.76 67.87 6.80 6.52 11.74 11.87 \nOurs-seq256 72.45 73.71 66.51 68.00 6.83 6.52 13.00 13.34 \nOurs-seq512 71.95 73.78 66.70 67.26 6.79 6.53 12.50 13.97 \n\n\n\nTable 6 :\n6Comparing block-wise and layer-wise tuning for around 7B models, the models LLaMA7b,LLaMA7bv2, OPT6.7b, and BLOOM7b1 are denoted by 7b, 7bv2, 6.7b, and 7b1 respectively. The \naccuracy is the % average accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA . Perplex-\nity (PPL) (\u2193) is evaluated using the C4 dataset. \n\nW4 \nW3G128 \nSize \n7b \n7bv2 \n6.7b \n7b1 \n7b \n7bv2 \n6.7b \n7b1 \nlayer-acc-seq256 \n67.50 67.78 63.46 58.72 65.96 66.09 61.60 58.24 \nblock-acc-seq256 \n67.64 67.96 64.55 59.08 66.31 66.63 57.76 58.34 \nlayer-c4-ppl-seq256 \n8.02 \n7.92 13.44 15.73 8.81 \n8.69 16.83 16.15 \nblock-c4-ppl-seq256 7.81 \n8.19 13.10 15.71 8.34 10.84 25.44 16.05 \n\n\n\nTable 7 :\n7Hyperparameter sensitivity analysis, the models LLaMA7b, LLaMA7bv2, OPT6.7b, and \nBLOOM7b1 are denoted by 7b, 7bv2, 6.7b, and 7b1 respectively. The accuracy is the % average \naccuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA . Perplexity (PPL) (\u2193) is evaluated \nusing the C4 dataset. \n\nAccuracy \nPPL on C4 \nSize \n7b \n7bv2 \n6.7b \n7b1 \n7b \n7bv2 \n6.7b \n7b1 \nsteps100 67.53 67.76 64.64 58.76 7.93 7.83 13.12 15.71 \nlr4e-3 \n68.01 67.57 64.57 59.47 7.81 10.29 13.09 15.66 \nseq2048 68.11 67.79 64.32 59.39 7.76 9.97 13.06 15.66 \ndefault \n68.05 67.74 64.34 59.52 7.84 11.20 13.08 15.\n\nTable 8 :\n8Reported results of AWQ and Ours Proposed 78.07 55.76 65.82 78.07 55.92 66.30 A.2 RESULTS OF WIKITEXT2 PPL AT W4LLaMA-7B \nAWQ \nOurs \nnbits \nMethod \nPIQA Hella. Wino. PIQA Hella. Wino. \n16 \nFP16 \n78.35 56.44 67.09 78.35 56.42 66.85 \n\nW3G128 \n\nRTN \n75.84 53.10 63.22 75.73 53.17 63.14 \nGPTQ \n70.89 46.77 60.93 72.58 47.10 59.91 \nProposed 76.66 53.63 66.14 76.61 53.98 66.06 \n\nW4G128 \n\nRTN \n77.86 55.81 65.59 77.58 55.86 65.75 \nGPTQ \n77.20 53.98 65.67 77.26 54.09 64.09 \n\n\nTable 9 :\n9Wikitext2 ppl ( \u2193) at W4 88 14.62 12.47 10.86 10.13 22.41 15.39 13.48 11.37 RTN 6.29 5.53 6.12 5.20 48.20 16.92 12.10 11.32 25.88 16.97 14.75 12.10 GPTQ 6.59 5.33 6.09 5.16 15.67 13.30 11.59 10.33 23.95 16.37 14.10 11.73 Ours 6.12 5.32 298.42 9.15 15.65 13.05 11.18 10.66 23.80 16.22 14.13 11.80 A.3 OTHER RESULTS FOR LARGE MODELSLLaMA \nOPT \nBLOOM \nSize \n7b \n13b \n7bv2 13bv2 1.3b \n2.7b \n6.7b \n13b 560m 1b7 \n3b \n7b1 \nFP16 5.67 5.09 \n5.47 \n4.\n\nTable 10 :\n10Average % accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA and C4 ppl(\u2193) for LLaMA & OPT with size \u2265 30B at W4. \"Ours-seq512\" indicates that we have modified the sequence length of the calibration dataset from 256 to 512. Ours-seq256 72.69 74.03 66.74 68.80 6.47 6.31 11.84 11.42 Ours-seq512 72.86 73.91 67.40 69.22 6.47 6.34 11.77 11.45 B VISUALIZATION OF VAccuracy \nPPL on C4 \nType \nLLaMA \nOPT \nLLaMA \nOPT \nSize \n30b \n65b \n30b \n66b \n30b 65b \n30b \n66b \nFP16 \n73.46 75.48 67.87 69.54 6.13 5.98 11.45 10.99 \nRTN \n72.33 73.91 65.94 37.12 6.54 6.46 13.56 305.73 \nGPTQ \n72.85 74.45 67.55 68.23 6.42 6.23 11.59 11.24 \n\n\nTable 11 :\n11Wikitext ppl(\u2193) for LLaMA & OPT with size \u2265 30B. \"Ours-seq512\" indicates that we have modified the sequence length of the calibration dataset from 256 to 512.W4 \nW3G128 \nType \nLLaMA \nOPT \nLLaMA \nOPT \nSize \n30b 65b \n30b \n66b \n30b 65b \n30b \n66b \nFP16 \n4.10 3.56 9.56 \n9.34 \n4.10 3.56 9.56 \n9.34 \nRTN \n4.54 3.99 10.98 110.43 4.87 4.44 23.05 126.92 \nGPTQ \n4.45 4.16 9.66 \n9.66 \n4.84 4.17 9.75 \n10.58 \nOurs-seq256 4.51 3.91 9.88 \n9.56 \n4.85 4.15 11.07 11.40 \nOurs-seq512 4.52 3.90 9.88 \n9.70 \n4.81 4.17 10.54 10.87 \n\n\n\nTable 12 :\n12Runtime in seconds at W4 E DETAILED RESULTS OF SOME LLAMA MODELSType \nLLaMA \nOPT \nBLOOM \n7B \n13B 6.7B 13B \n3B 7B1 \nGPTQ 712 1240 841 1523 345 661 \nOurs \n899 1590 819 1429 467 843 \n\n\n\nTable 13 :\n13Accuracies(\u2191) of HellaSwag, WinoGrand, PIQA, LAMBADA and PPL(\u2193) of WikiText, PTB, C4 for LLaMA-7B, \"Ours-2048\" indicates that we have modified the sequence length of the calibration dataset from 512 to 2048. Hella. Wino. PIQA Lamb. Avg. Wiki. PTBC4 \n\nhttps://github.com/IST-DASLab/gptq 2 https://huggingface.co/datasets/NeelNanda/pile-10k\nhttps://github.com/EleutherAI/lm-evaluation-harness/pull/497\n\nEstimating or propagating gradients through stochastic neurons for conditional computation. Yoshua Bengio, Nicholas L\u00e9onard, Aaron Courville, arXiv:1308.3432arXiv preprintYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n\nReasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence34Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com- monsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432-7439, 2020.\n\nQuantizable transformers: Removing outliers by helping attention heads do nothing. Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort, arXiv:2306.12929arXiv preprintYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: Removing outliers by helping attention heads do nothing. arXiv preprint arXiv:2306.12929, 2023.\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. arXiv preprintint8 (Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Qlora, arXiv:2305.14314Efficient finetuning of quantized llms. arXiv preprintTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.\n\nK Steven, Jeffrey L Esser, Deepika Mckinstry, Bablani, arXiv:1902.08153Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprintSteven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen- dra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.\n\nOptimal brain compression: A framework for accurate post-training quantization and pruning. Elias Frantar, Dan Alistarh, arXiv:2208.11580arXiv preprintElias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580, 2022.\n\nGptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, arXiv:2210.17323arXiv preprintElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\nA framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony Dipofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle Mcdonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, Andy Zou, 10.5281/zenodo.5371628Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot lan- guage model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.\n\nOptimal brain surgeon and general network pruning. Babak Hassibi, G David, Gregory J Stork, Wolff, IEEE international conference on neural networks. IEEEBabak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network pruning. In IEEE international conference on neural networks, pp. 293-299. IEEE, 1993.\n\nJ Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, Lora, arXiv:2106.09685Low-rank adaptation of large language models. arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n\nSqueezellm: Dense-and-sparse quantization. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, W Michael, Kurt Mahoney, Keutzer, arXiv:2306.07629arXiv preprintSehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629, 2023.\n\nCluster-promoting quantization with bit-drop for minimizing network quantization loss. Jihun Jung Hyun Lee, Sung Ju Yun, Eunho Hwang, Yang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionJung Hyun Lee, Jihun Yun, Sung Ju Hwang, and Eunho Yang. Cluster-promoting quantization with bit-drop for minimizing network quantization loss. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5370-5379, 2021.\n\nJeonghoon Jung Hyun Lee, Se Kim, Dongsoo Jung Kwon, Lee, Flexround, arXiv:2306.00317Learnable rounding based on element-wise division for post-training quantization. arXiv preprintJung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dongsoo Lee. Flexround: Learnable rounding based on element-wise division for post-training quantization. arXiv preprint arXiv:2306.00317, 2023.\n\nOn faster convergence of scaled sign gradient descent. Xiuxian Li, Kuo-Yi Lin, Li Li, Yiguang Hong, Jie Chen, IEEE Transactions on Industrial Informatics. Xiuxian Li, Kuo-Yi Lin, Li Li, Yiguang Hong, and Jie Chen. On faster convergence of scaled sign gradient descent. IEEE Transactions on Industrial Informatics, 2023.\n\nBrecq: Pushing the limit of post-training quantization by block reconstruction. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu, arXiv:2102.05426arXiv preprintYuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.\n\nEfficient activation quantization via adaptive rounding border for post-training quantization. Zhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu, Xiaotian Gao, Jingwen Leng, Minyi Guo, arXiv:2208.11945arXiv preprintZhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu, Xiaotian Gao, Jingwen Leng, and Minyi Guo. Efficient activation quantization via adaptive rounding border for post-training quantization. arXiv preprint arXiv:2208.11945, 2022.\n\nAwq: Activation-aware weight quantization for llm compression and acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han, arXiv:2306.00978arXiv preprintJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978, 2023.\n\nLlm-qat: Data-free quantization aware training for large language models. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas Chandra, arXiv:2305.17888arXiv preprintZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023.\n\nPost-training quantization for vision transformer. Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, Wen Gao, Advances in Neural Information Processing Systems. 34Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quanti- zation for vision transformer. Advances in Neural Information Processing Systems, 34:28092- 28103, 2021.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843arXiv preprintStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n\nData-free quantization through weight equalization and bias correction. Markus Nagel, Mart Van Baalen, Tijmen Blankevoort, Max Welling, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionMarkus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1325-1334, 2019.\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLRMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pp. 7197-7206. PMLR, 2020.\n\n. Openai, Openai, OpenAI. Openai: Chatgpt. URL https://openai.com/blog/chatgpt.\n\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Raquel Boleda, Fern\u00e1ndez, arXiv:1606.06031The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprintDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\n\nnuqmm: Quantized matmul for efficient inference of large-scale generative language models. Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee, arXiv:2206.09557arXiv preprintGunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 211Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.\n\nCode llama: Open foundation models for code. Jonas Baptiste Rozi\u00e8re, Fabian Gehring, Sten Gloeckle, Itai Sootla, Xiaoqing Ellen Gat, Yossi Tan, Jingyu Adi, Tal Liu, J\u00e9r\u00e9my Remez, Rapin, arXiv:2308.12950arXiv preprintBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.\n\nStochastic sign descent methods: New algorithms and better theory. Mher Safaryan, Peter Richt\u00e1rik, International Conference on Machine Learning. PMLRMher Safaryan and Peter Richt\u00e1rik. Stochastic sign descent methods: New algorithms and better theory. In International Conference on Machine Learning, pp. 9224-9234. PMLR, 2021.\n\nWinogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 649Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver- sarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.\n\nAngela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ili\u0107, Roman Hesslow, Alexandra Sasha Castagn\u00e9, Fran\u00e7ois Luccioni, Matthias Yvon, Gall\u00e9, arXiv:2211.05100A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Naman Baptiste Rozi\u00e8re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\n\nLlama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288arXiv preprintHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\n\nHaq: Hardware-aware automated quantization with mixed precision. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, Song Han, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionKuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quan- tization with mixed precision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8612-8620, 2019.\n\nQdrop: randomly dropping quantization for extremely low-bit post-training quantization. Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, Fengwei Yu, arXiv:2203.05740arXiv preprintXiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: randomly dropping quantization for extremely low-bit post-training quantization. arXiv preprint arXiv:2203.05740, 2022a.\n\nOutlier suppression: Pushing the limit of low-bit transformer language models. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, Advances in Neural Information Processing Systems. 35Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Feng- wei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer lan- guage models. Advances in Neural Information Processing Systems, 35:17402-17414, 2022b.\n\nOutlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu, arXiv:2304.09145arXiv preprintXiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xian- glong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han, arXiv:2211.10438arXiv preprintGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han, International Conference on Machine Learning. PMLRGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 38087-38099. PMLR, 2023.\n\nHawq-v3: Dyadic neural network quantization. Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, International Conference on Machine Learning. PMLRZhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qi- jing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning, pp. 11875-11886. PMLR, 2021.\n\nZeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He, arXiv:2303.08302Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Explor- ing post-training quantization in llms from comprehensive study to low rank compensation. arXiv:2303.08302, 2023.\n\nRptq: Reorder-based post-training quantization for large language models. Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, Bingzhe Wu, arXiv:2304.01089arXiv preprintZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models. arXiv preprint arXiv:2304.01089, 2023.\n\nSpiq: Data-free per-channel static input quantization. Edouard Yvinec, Arnaud Dapogny, Matthieu Cord, Kevin Bailly, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionEdouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Spiq: Data-free per-channel static input quantization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3869-3878, 2023.\n\nHellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.07830arXiv preprintRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma- chine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.01068Open pre-trained transformer language models. arXiv preprintSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo- pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\nEffective training of convolutional neural networks with low-bitwidth weights and activations. Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian Reid, Chunhua Shen, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4410Bohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian Reid, and Chunhua Shen. Effective train- ing of convolutional neural networks with low-bitwidth weights and activations. IEEE Transac- tions on Pattern Analysis and Machine Intelligence, 44(10):6140-6152, 2021.\n\nOurs-2048\" indicates that we have modified the sequence length of the calibration dataset from 512 to. \u2191) Of Hellaswag, Piqa Winogrand, Ppl(\u2193) Of Wikitext, Ptb For Llama-13b, Table. 14Table 14: Accuracies(\u2191) of HellaSwag, WinoGrand, PIQA, LAMBADA and PPL(\u2193) of WikiText, PTB, C4 for LLaMA-13B, \"Ours-2048\" indicates that we have modified the sequence length of the calibration dataset from 512 to 2048.\n\n. Hella, Wino, Lamb, Avg. Wiki. PTB. 4Hella. Wino. PIQA Lamb. Avg. Wiki. PTB C4\n\nOurs-2048\" indicates that we have modified the sequence length of the calibration dataset from 512 to. \u2191) Of Hellaswag, Piqa Winogrand, Ppl(\u2193) Of Wikitext, Ptb For Llama, -7b-V2 , Table. 15Table 15: Accuracies(\u2191) of HellaSwag, WinoGrand, PIQA, LAMBADA and PPL(\u2193) of WikiText, PTB, C4 for LLaMA-7B-V2, \"Ours-2048\" indicates that we have modified the sequence length of the calibration dataset from 512 to 2048.\n\n. Hella, Wino, Lamb, Avg. Wiki. PTB. Hella. Wino. PIQA Lamb. Avg. Wiki. PTB\n\nOurs-2048\" indicates that we have modified the sequence length of the calibration dataset from 512 to. \u2191) Of Hellaswag, Piqa Winogrand, Ppl(\u2193) Of Wikitext, Ptb For Llama, -13b-V2 , 16Table 16: Accuracies(\u2191) of HellaSwag, WinoGrand, PIQA, LAMBADA and PPL(\u2193) of WikiText, PTB, C4 for LLaMA-13B-V2, \"Ours-2048\" indicates that we have modified the sequence length of the calibration dataset from 512 to 2048.\n\n. Hella, Wino, Lamb, Avg. Wiki. PTB. 4Hella. Wino. PIQA Lamb. Avg. Wiki. PTB C4\n", "annotations": {"author": "[{\"end\":108,\"start\":87},{\"end\":130,\"start\":109},{\"end\":151,\"start\":131},{\"end\":171,\"start\":152},{\"end\":187,\"start\":172},{\"end\":206,\"start\":188}]", "publisher": null, "author_last_name": "[{\"end\":99,\"start\":94},{\"end\":121,\"start\":116},{\"end\":142,\"start\":138},{\"end\":162,\"start\":159},{\"end\":178,\"start\":176},{\"end\":197,\"start\":195}]", "author_first_name": "[{\"end\":93,\"start\":87},{\"end\":115,\"start\":109},{\"end\":137,\"start\":131},{\"end\":158,\"start\":152},{\"end\":175,\"start\":172},{\"end\":194,\"start\":188}]", "author_affiliation": "[{\"end\":107,\"start\":101},{\"end\":129,\"start\":123},{\"end\":150,\"start\":144},{\"end\":170,\"start\":164},{\"end\":186,\"start\":180},{\"end\":205,\"start\":199}]", "title": "[{\"end\":84,\"start\":1},{\"end\":290,\"start\":207}]", "venue": null, "abstract": "[{\"end\":1541,\"start\":315}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1684,\"start\":1662},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2151,\"start\":2131},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2171,\"start\":2151},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2188,\"start\":2171},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2246,\"start\":2226},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2264,\"start\":2246},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2283,\"start\":2264},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2788,\"start\":2766},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3373,\"start\":3354},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3391,\"start\":3373},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3415,\"start\":3391},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4168,\"start\":4135},{\"end\":4813,\"start\":4802},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6306,\"start\":6285},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6951,\"start\":6931},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7328,\"start\":7306},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7399,\"start\":7381},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7427,\"start\":7408},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7924,\"start\":7901},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8044,\"start\":8026},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8131,\"start\":8112},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8300,\"start\":8279},{\"end\":8333,\"start\":8302},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8374,\"start\":8356},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8876,\"start\":8854},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9130,\"start\":9111},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9537,\"start\":9517},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9645,\"start\":9627},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11125,\"start\":11097},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11455,\"start\":11427},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14253,\"start\":14232},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":16471,\"start\":16449},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16508,\"start\":16484},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16534,\"start\":16515},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16569,\"start\":16547},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16883,\"start\":16862},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16912,\"start\":16891},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17388,\"start\":17369},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17836,\"start\":17817},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17985,\"start\":17962},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18020,\"start\":17997},{\"end\":18048,\"start\":18022},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18079,\"start\":18059},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24816,\"start\":24794},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24855,\"start\":24832},{\"end\":26852,\"start\":26841}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29408,\"start\":28925},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30210,\"start\":29409},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30690,\"start\":30211},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31106,\"start\":30691},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32284,\"start\":31107},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32896,\"start\":32285},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33556,\"start\":32897},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34148,\"start\":33557},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":34629,\"start\":34149},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":35082,\"start\":34630},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":35715,\"start\":35083},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":36242,\"start\":35716},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":36438,\"start\":36243},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":36703,\"start\":36439}]", "paragraph": "[{\"end\":2024,\"start\":1557},{\"end\":3128,\"start\":2026},{\"end\":3700,\"start\":3130},{\"end\":4573,\"start\":3702},{\"end\":5164,\"start\":4575},{\"end\":5839,\"start\":5166},{\"end\":6807,\"start\":5856},{\"end\":7733,\"start\":6809},{\"end\":9479,\"start\":7735},{\"end\":10701,\"start\":9481},{\"end\":11733,\"start\":10703},{\"end\":11967,\"start\":11749},{\"end\":12432,\"start\":12007},{\"end\":12707,\"start\":12434},{\"end\":13208,\"start\":12751},{\"end\":13372,\"start\":13210},{\"end\":13537,\"start\":13399},{\"end\":13944,\"start\":13581},{\"end\":14175,\"start\":14013},{\"end\":14379,\"start\":14177},{\"end\":14480,\"start\":14438},{\"end\":14709,\"start\":14549},{\"end\":14907,\"start\":14869},{\"end\":14963,\"start\":14909},{\"end\":15017,\"start\":14978},{\"end\":15068,\"start\":15019},{\"end\":15095,\"start\":15070},{\"end\":15534,\"start\":15097},{\"end\":15594,\"start\":15536},{\"end\":16209,\"start\":15610},{\"end\":16953,\"start\":16235},{\"end\":17837,\"start\":16955},{\"end\":18203,\"start\":17839},{\"end\":19018,\"start\":18205},{\"end\":19685,\"start\":19051},{\"end\":20650,\"start\":19687},{\"end\":21029,\"start\":20652},{\"end\":21310,\"start\":21031},{\"end\":21759,\"start\":21312},{\"end\":22217,\"start\":21792},{\"end\":24042,\"start\":22265},{\"end\":24331,\"start\":24044},{\"end\":25116,\"start\":24363},{\"end\":25725,\"start\":25174},{\"end\":27071,\"start\":25727},{\"end\":27410,\"start\":27073},{\"end\":28359,\"start\":27479},{\"end\":28381,\"start\":28361},{\"end\":28924,\"start\":28383}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12006,\"start\":11968},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12750,\"start\":12708},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13398,\"start\":13373},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13568,\"start\":13538},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14012,\"start\":13945},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14437,\"start\":14380},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14548,\"start\":14481},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14868,\"start\":14710},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14977,\"start\":14964}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19875,\"start\":19868},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20714,\"start\":20707},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21148,\"start\":21141},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":21405,\"start\":21398},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":22365,\"start\":22357},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":23111,\"start\":23104},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":26100,\"start\":26093},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26248,\"start\":26240},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26281,\"start\":26273},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28495,\"start\":28487},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28909,\"start\":28867}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1555,\"start\":1543},{\"attributes\":{\"n\":\"2\"},\"end\":5854,\"start\":5842},{\"attributes\":{\"n\":\"3\"},\"end\":11747,\"start\":11736},{\"attributes\":{\"n\":\"3.1\"},\"end\":13579,\"start\":13570},{\"attributes\":{\"n\":\"4\"},\"end\":15608,\"start\":15597},{\"attributes\":{\"n\":\"4.1\"},\"end\":16233,\"start\":16212},{\"attributes\":{\"n\":\"4.2\"},\"end\":19049,\"start\":19021},{\"attributes\":{\"n\":\"4.3\"},\"end\":21790,\"start\":21762},{\"attributes\":{\"n\":\"4.4\"},\"end\":22263,\"start\":22220},{\"attributes\":{\"n\":\"5\"},\"end\":24361,\"start\":24334},{\"end\":25133,\"start\":25119},{\"end\":25172,\"start\":25136},{\"end\":27466,\"start\":27413},{\"end\":27477,\"start\":27469},{\"end\":28944,\"start\":28926},{\"end\":29419,\"start\":29410},{\"end\":30221,\"start\":30212},{\"end\":30701,\"start\":30692},{\"end\":31117,\"start\":31108},{\"end\":32295,\"start\":32286},{\"end\":32907,\"start\":32898},{\"end\":33567,\"start\":33558},{\"end\":34159,\"start\":34150},{\"end\":34640,\"start\":34631},{\"end\":35094,\"start\":35084},{\"end\":35727,\"start\":35717},{\"end\":36254,\"start\":36244},{\"end\":36450,\"start\":36440}]", "table": "[{\"end\":30210,\"start\":29501},{\"end\":30690,\"start\":30297},{\"end\":31106,\"start\":30721},{\"end\":32284,\"start\":31285},{\"end\":32896,\"start\":32522},{\"end\":33556,\"start\":32993},{\"end\":34148,\"start\":33569},{\"end\":34629,\"start\":34273},{\"end\":35082,\"start\":34972},{\"end\":35715,\"start\":35460},{\"end\":36242,\"start\":35888},{\"end\":36438,\"start\":36321},{\"end\":36703,\"start\":36699}]", "figure_caption": "[{\"end\":29408,\"start\":28949},{\"end\":29501,\"start\":29421},{\"end\":30297,\"start\":30223},{\"end\":30721,\"start\":30703},{\"end\":31285,\"start\":31119},{\"end\":32522,\"start\":32297},{\"end\":32993,\"start\":32909},{\"end\":34273,\"start\":34161},{\"end\":34972,\"start\":34642},{\"end\":35460,\"start\":35097},{\"end\":35888,\"start\":35730},{\"end\":36321,\"start\":36257},{\"end\":36699,\"start\":36453}]", "figure_ref": "[{\"end\":4900,\"start\":4892},{\"end\":13811,\"start\":13803},{\"end\":23498,\"start\":23490},{\"end\":26752,\"start\":26744},{\"end\":27100,\"start\":27092},{\"end\":27901,\"start\":27893},{\"end\":28410,\"start\":28402}]", "bib_author_first_name": "[{\"end\":36952,\"start\":36946},{\"end\":36969,\"start\":36961},{\"end\":36984,\"start\":36979},{\"end\":37275,\"start\":37268},{\"end\":37287,\"start\":37282},{\"end\":37305,\"start\":37297},{\"end\":37316,\"start\":37311},{\"end\":37751,\"start\":37744},{\"end\":37770,\"start\":37764},{\"end\":37784,\"start\":37778},{\"end\":38012,\"start\":38009},{\"end\":38027,\"start\":38023},{\"end\":38041,\"start\":38035},{\"end\":38055,\"start\":38051},{\"end\":38336,\"start\":38333},{\"end\":38355,\"start\":38347},{\"end\":38368,\"start\":38365},{\"end\":38383,\"start\":38379},{\"end\":38630,\"start\":38629},{\"end\":38646,\"start\":38639},{\"end\":38648,\"start\":38647},{\"end\":38663,\"start\":38656},{\"end\":39067,\"start\":39062},{\"end\":39080,\"start\":39077},{\"end\":39373,\"start\":39368},{\"end\":39390,\"start\":39383},{\"end\":39410,\"start\":39407},{\"end\":39704,\"start\":39701},{\"end\":39718,\"start\":39710},{\"end\":39730,\"start\":39724},{\"end\":39744,\"start\":39741},{\"end\":39759,\"start\":39752},{\"end\":39775,\"start\":39768},{\"end\":39792,\"start\":39784},{\"end\":39809,\"start\":39802},{\"end\":39819,\"start\":39815},{\"end\":39836,\"start\":39830},{\"end\":39855,\"start\":39850},{\"end\":39868,\"start\":39863},{\"end\":39883,\"start\":39879},{\"end\":39895,\"start\":39890},{\"end\":39906,\"start\":39903},{\"end\":39918,\"start\":39913},{\"end\":39929,\"start\":39925},{\"end\":40358,\"start\":40353},{\"end\":40369,\"start\":40368},{\"end\":40386,\"start\":40377},{\"end\":40637,\"start\":40636},{\"end\":40652,\"start\":40646},{\"end\":40664,\"start\":40657},{\"end\":40677,\"start\":40671},{\"end\":40693,\"start\":40686},{\"end\":40710,\"start\":40705},{\"end\":40717,\"start\":40715},{\"end\":40730,\"start\":40724},{\"end\":41076,\"start\":41070},{\"end\":41089,\"start\":41082},{\"end\":41102,\"start\":41098},{\"end\":41116,\"start\":41112},{\"end\":41128,\"start\":41123},{\"end\":41138,\"start\":41133},{\"end\":41146,\"start\":41145},{\"end\":41160,\"start\":41156},{\"end\":41496,\"start\":41491},{\"end\":41516,\"start\":41512},{\"end\":41519,\"start\":41517},{\"end\":41530,\"start\":41525},{\"end\":41924,\"start\":41915},{\"end\":41942,\"start\":41940},{\"end\":41955,\"start\":41948},{\"end\":42351,\"start\":42344},{\"end\":42362,\"start\":42356},{\"end\":42370,\"start\":42368},{\"end\":42382,\"start\":42375},{\"end\":42392,\"start\":42389},{\"end\":42696,\"start\":42690},{\"end\":42707,\"start\":42701},{\"end\":42716,\"start\":42714},{\"end\":42726,\"start\":42722},{\"end\":42737,\"start\":42733},{\"end\":42744,\"start\":42742},{\"end\":42759,\"start\":42752},{\"end\":42767,\"start\":42764},{\"end\":42777,\"start\":42774},{\"end\":43130,\"start\":43123},{\"end\":43139,\"start\":43135},{\"end\":43151,\"start\":43145},{\"end\":43164,\"start\":43157},{\"end\":43177,\"start\":43171},{\"end\":43191,\"start\":43183},{\"end\":43204,\"start\":43197},{\"end\":43216,\"start\":43211},{\"end\":43572,\"start\":43570},{\"end\":43585,\"start\":43578},{\"end\":43599,\"start\":43592},{\"end\":43611,\"start\":43606},{\"end\":43624,\"start\":43618},{\"end\":43635,\"start\":43631},{\"end\":43946,\"start\":43940},{\"end\":43958,\"start\":43952},{\"end\":43975,\"start\":43965},{\"end\":43987,\"start\":43982},{\"end\":44001,\"start\":43995},{\"end\":44015,\"start\":44009},{\"end\":44032,\"start\":44024},{\"end\":44048,\"start\":44038},{\"end\":44070,\"start\":44065},{\"end\":44426,\"start\":44419},{\"end\":44437,\"start\":44432},{\"end\":44447,\"start\":44444},{\"end\":44456,\"start\":44453},{\"end\":44469,\"start\":44464},{\"end\":44477,\"start\":44474},{\"end\":44772,\"start\":44765},{\"end\":44788,\"start\":44781},{\"end\":44801,\"start\":44796},{\"end\":44819,\"start\":44812},{\"end\":45076,\"start\":45070},{\"end\":45088,\"start\":45084},{\"end\":45107,\"start\":45101},{\"end\":45124,\"start\":45121},{\"end\":45569,\"start\":45563},{\"end\":45581,\"start\":45577},{\"end\":45585,\"start\":45582},{\"end\":45597,\"start\":45593},{\"end\":45618,\"start\":45610},{\"end\":45634,\"start\":45628},{\"end\":46012,\"start\":46007},{\"end\":46028,\"start\":46022},{\"end\":46049,\"start\":46041},{\"end\":46065,\"start\":46061},{\"end\":46081,\"start\":46072},{\"end\":46094,\"start\":46088},{\"end\":46110,\"start\":46105},{\"end\":46126,\"start\":46121},{\"end\":46141,\"start\":46135},{\"end\":46632,\"start\":46627},{\"end\":46647,\"start\":46639},{\"end\":46656,\"start\":46654},{\"end\":46678,\"start\":46668},{\"end\":46692,\"start\":46684},{\"end\":46705,\"start\":46698},{\"end\":47048,\"start\":47043},{\"end\":47061,\"start\":47057},{\"end\":47075,\"start\":47071},{\"end\":47094,\"start\":47085},{\"end\":47106,\"start\":47100},{\"end\":47122,\"start\":47115},{\"end\":47136,\"start\":47131},{\"end\":47146,\"start\":47143},{\"end\":47158,\"start\":47151},{\"end\":47533,\"start\":47528},{\"end\":47558,\"start\":47552},{\"end\":47572,\"start\":47568},{\"end\":47587,\"start\":47583},{\"end\":47604,\"start\":47596},{\"end\":47610,\"start\":47605},{\"end\":47621,\"start\":47616},{\"end\":47633,\"start\":47627},{\"end\":47642,\"start\":47639},{\"end\":47654,\"start\":47648},{\"end\":48003,\"start\":47999},{\"end\":48019,\"start\":48014},{\"end\":48330,\"start\":48323},{\"end\":48344,\"start\":48342},{\"end\":48359,\"start\":48352},{\"end\":48371,\"start\":48366},{\"end\":48611,\"start\":48605},{\"end\":48638,\"start\":48627},{\"end\":48649,\"start\":48644},{\"end\":48663,\"start\":48657},{\"end\":48679,\"start\":48673},{\"end\":48691,\"start\":48686},{\"end\":48710,\"start\":48701},{\"end\":48716,\"start\":48711},{\"end\":48735,\"start\":48727},{\"end\":48754,\"start\":48746},{\"end\":49136,\"start\":49132},{\"end\":49153,\"start\":49146},{\"end\":49169,\"start\":49162},{\"end\":49185,\"start\":49179},{\"end\":49206,\"start\":49196},{\"end\":49224,\"start\":49216},{\"end\":49239,\"start\":49234},{\"end\":49262,\"start\":49258},{\"end\":49276,\"start\":49270},{\"end\":49688,\"start\":49684},{\"end\":49703,\"start\":49698},{\"end\":49717,\"start\":49712},{\"end\":49730,\"start\":49725},{\"end\":49744,\"start\":49739},{\"end\":49763,\"start\":49756},{\"end\":49779,\"start\":49772},{\"end\":49797,\"start\":49791},{\"end\":49813,\"start\":49805},{\"end\":49830,\"start\":49824},{\"end\":50200,\"start\":50196},{\"end\":50214,\"start\":50207},{\"end\":50225,\"start\":50220},{\"end\":50233,\"start\":50231},{\"end\":50243,\"start\":50239},{\"end\":50725,\"start\":50718},{\"end\":50737,\"start\":50731},{\"end\":50750,\"start\":50744},{\"end\":50764,\"start\":50755},{\"end\":50777,\"start\":50770},{\"end\":51095,\"start\":51088},{\"end\":51108,\"start\":51101},{\"end\":51124,\"start\":51116},{\"end\":51138,\"start\":51132},{\"end\":51154,\"start\":51145},{\"end\":51164,\"start\":51162},{\"end\":51179,\"start\":51172},{\"end\":51193,\"start\":51184},{\"end\":51649,\"start\":51642},{\"end\":51662,\"start\":51655},{\"end\":51676,\"start\":51670},{\"end\":51689,\"start\":51681},{\"end\":51703,\"start\":51697},{\"end\":51717,\"start\":51710},{\"end\":51732,\"start\":51723},{\"end\":52126,\"start\":52117},{\"end\":52135,\"start\":52133},{\"end\":52148,\"start\":52141},{\"end\":52163,\"start\":52157},{\"end\":52177,\"start\":52173},{\"end\":52512,\"start\":52503},{\"end\":52521,\"start\":52519},{\"end\":52534,\"start\":52527},{\"end\":52546,\"start\":52543},{\"end\":52557,\"start\":52551},{\"end\":52571,\"start\":52567},{\"end\":52925,\"start\":52919},{\"end\":52935,\"start\":52931},{\"end\":52952,\"start\":52942},{\"end\":52964,\"start\":52960},{\"end\":52979,\"start\":52974},{\"end\":52988,\"start\":52984},{\"end\":53000,\"start\":52994},{\"end\":53013,\"start\":53007},{\"end\":53025,\"start\":53021},{\"end\":53039,\"start\":53032},{\"end\":53478,\"start\":53472},{\"end\":53491,\"start\":53484},{\"end\":53501,\"start\":53496},{\"end\":53513,\"start\":53506},{\"end\":53527,\"start\":53520},{\"end\":53830,\"start\":53823},{\"end\":53840,\"start\":53837},{\"end\":53852,\"start\":53846},{\"end\":53863,\"start\":53858},{\"end\":53877,\"start\":53869},{\"end\":53891,\"start\":53884},{\"end\":53906,\"start\":53899},{\"end\":53917,\"start\":53912},{\"end\":53930,\"start\":53922},{\"end\":53942,\"start\":53935},{\"end\":54281,\"start\":54274},{\"end\":54296,\"start\":54290},{\"end\":54314,\"start\":54306},{\"end\":54326,\"start\":54321},{\"end\":54769,\"start\":54764},{\"end\":54782,\"start\":54779},{\"end\":54800,\"start\":54793},{\"end\":54810,\"start\":54807},{\"end\":54825,\"start\":54820},{\"end\":55035,\"start\":55030},{\"end\":55050,\"start\":55043},{\"end\":55064,\"start\":55059},{\"end\":55077,\"start\":55072},{\"end\":55091,\"start\":55087},{\"end\":55105,\"start\":55098},{\"end\":55123,\"start\":55112},{\"end\":55135,\"start\":55131},{\"end\":55146,\"start\":55142},{\"end\":55153,\"start\":55151},{\"end\":55582,\"start\":55577},{\"end\":55598,\"start\":55591},{\"end\":55608,\"start\":55604},{\"end\":55622,\"start\":55614},{\"end\":55631,\"start\":55628},{\"end\":55645,\"start\":55638},{\"end\":56095,\"start\":56090},{\"end\":56111,\"start\":56107},{\"end\":56132,\"start\":56123},{\"end\":56146,\"start\":56143},{\"end\":56580,\"start\":56575},{\"end\":56596,\"start\":56592},{\"end\":56617,\"start\":56608},{\"end\":56631,\"start\":56628},{\"end\":56649,\"start\":56643},{\"end\":57068,\"start\":57063},{\"end\":57084,\"start\":57080},{\"end\":57105,\"start\":57096},{\"end\":57119,\"start\":57116},{\"end\":57138,\"start\":57131}]", "bib_author_last_name": "[{\"end\":36959,\"start\":36953},{\"end\":36977,\"start\":36970},{\"end\":36994,\"start\":36985},{\"end\":37280,\"start\":37276},{\"end\":37295,\"start\":37288},{\"end\":37309,\"start\":37306},{\"end\":37321,\"start\":37317},{\"end\":37762,\"start\":37752},{\"end\":37776,\"start\":37771},{\"end\":37796,\"start\":37785},{\"end\":38021,\"start\":38013},{\"end\":38033,\"start\":38028},{\"end\":38049,\"start\":38042},{\"end\":38067,\"start\":38056},{\"end\":38345,\"start\":38337},{\"end\":38363,\"start\":38356},{\"end\":38377,\"start\":38369},{\"end\":38395,\"start\":38384},{\"end\":38402,\"start\":38397},{\"end\":38637,\"start\":38631},{\"end\":38654,\"start\":38649},{\"end\":38673,\"start\":38664},{\"end\":38682,\"start\":38675},{\"end\":39075,\"start\":39068},{\"end\":39089,\"start\":39081},{\"end\":39381,\"start\":39374},{\"end\":39405,\"start\":39391},{\"end\":39418,\"start\":39411},{\"end\":39428,\"start\":39420},{\"end\":39708,\"start\":39705},{\"end\":39722,\"start\":39719},{\"end\":39739,\"start\":39731},{\"end\":39750,\"start\":39745},{\"end\":39766,\"start\":39760},{\"end\":39782,\"start\":39776},{\"end\":39800,\"start\":39793},{\"end\":39813,\"start\":39810},{\"end\":39828,\"start\":39820},{\"end\":39848,\"start\":39837},{\"end\":39861,\"start\":39856},{\"end\":39877,\"start\":39869},{\"end\":39888,\"start\":39884},{\"end\":39901,\"start\":39896},{\"end\":39911,\"start\":39907},{\"end\":39923,\"start\":39919},{\"end\":39933,\"start\":39930},{\"end\":40366,\"start\":40359},{\"end\":40375,\"start\":40370},{\"end\":40392,\"start\":40387},{\"end\":40399,\"start\":40394},{\"end\":40644,\"start\":40638},{\"end\":40655,\"start\":40653},{\"end\":40669,\"start\":40665},{\"end\":40684,\"start\":40678},{\"end\":40703,\"start\":40694},{\"end\":40713,\"start\":40711},{\"end\":40722,\"start\":40718},{\"end\":40735,\"start\":40731},{\"end\":40741,\"start\":40737},{\"end\":40747,\"start\":40743},{\"end\":41080,\"start\":41077},{\"end\":41096,\"start\":41090},{\"end\":41110,\"start\":41103},{\"end\":41121,\"start\":41117},{\"end\":41131,\"start\":41129},{\"end\":41143,\"start\":41139},{\"end\":41154,\"start\":41147},{\"end\":41168,\"start\":41161},{\"end\":41177,\"start\":41170},{\"end\":41510,\"start\":41497},{\"end\":41523,\"start\":41520},{\"end\":41536,\"start\":41531},{\"end\":41542,\"start\":41538},{\"end\":41938,\"start\":41925},{\"end\":41946,\"start\":41943},{\"end\":41965,\"start\":41956},{\"end\":41970,\"start\":41967},{\"end\":41981,\"start\":41972},{\"end\":42354,\"start\":42352},{\"end\":42366,\"start\":42363},{\"end\":42373,\"start\":42371},{\"end\":42387,\"start\":42383},{\"end\":42397,\"start\":42393},{\"end\":42699,\"start\":42697},{\"end\":42712,\"start\":42708},{\"end\":42720,\"start\":42717},{\"end\":42731,\"start\":42727},{\"end\":42740,\"start\":42738},{\"end\":42750,\"start\":42745},{\"end\":42762,\"start\":42760},{\"end\":42772,\"start\":42768},{\"end\":42780,\"start\":42778},{\"end\":43133,\"start\":43131},{\"end\":43143,\"start\":43140},{\"end\":43155,\"start\":43152},{\"end\":43169,\"start\":43165},{\"end\":43181,\"start\":43178},{\"end\":43195,\"start\":43192},{\"end\":43209,\"start\":43205},{\"end\":43220,\"start\":43217},{\"end\":43576,\"start\":43573},{\"end\":43590,\"start\":43586},{\"end\":43604,\"start\":43600},{\"end\":43616,\"start\":43612},{\"end\":43629,\"start\":43625},{\"end\":43639,\"start\":43636},{\"end\":43950,\"start\":43947},{\"end\":43963,\"start\":43959},{\"end\":43980,\"start\":43976},{\"end\":43993,\"start\":43988},{\"end\":44007,\"start\":44002},{\"end\":44022,\"start\":44016},{\"end\":44036,\"start\":44033},{\"end\":44063,\"start\":44049},{\"end\":44078,\"start\":44071},{\"end\":44430,\"start\":44427},{\"end\":44442,\"start\":44438},{\"end\":44451,\"start\":44448},{\"end\":44462,\"start\":44457},{\"end\":44472,\"start\":44470},{\"end\":44481,\"start\":44478},{\"end\":44779,\"start\":44773},{\"end\":44794,\"start\":44789},{\"end\":44810,\"start\":44802},{\"end\":44826,\"start\":44820},{\"end\":45082,\"start\":45077},{\"end\":45099,\"start\":45089},{\"end\":45119,\"start\":45108},{\"end\":45132,\"start\":45125},{\"end\":45575,\"start\":45570},{\"end\":45591,\"start\":45586},{\"end\":45608,\"start\":45598},{\"end\":45626,\"start\":45619},{\"end\":45646,\"start\":45635},{\"end\":45934,\"start\":45928},{\"end\":45942,\"start\":45936},{\"end\":46020,\"start\":46013},{\"end\":46039,\"start\":46029},{\"end\":46059,\"start\":46050},{\"end\":46070,\"start\":46066},{\"end\":46086,\"start\":46082},{\"end\":46103,\"start\":46095},{\"end\":46119,\"start\":46111},{\"end\":46133,\"start\":46127},{\"end\":46148,\"start\":46142},{\"end\":46159,\"start\":46150},{\"end\":46637,\"start\":46633},{\"end\":46652,\"start\":46648},{\"end\":46666,\"start\":46657},{\"end\":46682,\"start\":46679},{\"end\":46696,\"start\":46693},{\"end\":46709,\"start\":46706},{\"end\":47055,\"start\":47049},{\"end\":47069,\"start\":47062},{\"end\":47083,\"start\":47076},{\"end\":47098,\"start\":47095},{\"end\":47113,\"start\":47107},{\"end\":47129,\"start\":47123},{\"end\":47141,\"start\":47137},{\"end\":47149,\"start\":47147},{\"end\":47162,\"start\":47159},{\"end\":47550,\"start\":47534},{\"end\":47566,\"start\":47559},{\"end\":47581,\"start\":47573},{\"end\":47594,\"start\":47588},{\"end\":47614,\"start\":47611},{\"end\":47625,\"start\":47622},{\"end\":47637,\"start\":47634},{\"end\":47646,\"start\":47643},{\"end\":47660,\"start\":47655},{\"end\":47667,\"start\":47662},{\"end\":48012,\"start\":48004},{\"end\":48029,\"start\":48020},{\"end\":48340,\"start\":48331},{\"end\":48350,\"start\":48345},{\"end\":48364,\"start\":48360},{\"end\":48383,\"start\":48372},{\"end\":48389,\"start\":48385},{\"end\":48625,\"start\":48612},{\"end\":48642,\"start\":48639},{\"end\":48655,\"start\":48650},{\"end\":48671,\"start\":48664},{\"end\":48684,\"start\":48680},{\"end\":48699,\"start\":48692},{\"end\":48725,\"start\":48717},{\"end\":48744,\"start\":48736},{\"end\":48759,\"start\":48755},{\"end\":48766,\"start\":48761},{\"end\":49144,\"start\":49137},{\"end\":49160,\"start\":49154},{\"end\":49177,\"start\":49170},{\"end\":49194,\"start\":49186},{\"end\":49214,\"start\":49207},{\"end\":49232,\"start\":49225},{\"end\":49256,\"start\":49240},{\"end\":49268,\"start\":49263},{\"end\":49283,\"start\":49277},{\"end\":49290,\"start\":49285},{\"end\":49696,\"start\":49689},{\"end\":49710,\"start\":49704},{\"end\":49723,\"start\":49718},{\"end\":49737,\"start\":49731},{\"end\":49754,\"start\":49745},{\"end\":49770,\"start\":49764},{\"end\":49789,\"start\":49780},{\"end\":49803,\"start\":49798},{\"end\":49822,\"start\":49814},{\"end\":49838,\"start\":49831},{\"end\":50205,\"start\":50201},{\"end\":50218,\"start\":50215},{\"end\":50229,\"start\":50226},{\"end\":50237,\"start\":50234},{\"end\":50247,\"start\":50244},{\"end\":50729,\"start\":50726},{\"end\":50742,\"start\":50738},{\"end\":50753,\"start\":50751},{\"end\":50768,\"start\":50765},{\"end\":50780,\"start\":50778},{\"end\":51099,\"start\":51096},{\"end\":51114,\"start\":51109},{\"end\":51130,\"start\":51125},{\"end\":51143,\"start\":51139},{\"end\":51160,\"start\":51155},{\"end\":51170,\"start\":51165},{\"end\":51182,\"start\":51180},{\"end\":51197,\"start\":51194},{\"end\":51653,\"start\":51650},{\"end\":51668,\"start\":51663},{\"end\":51679,\"start\":51677},{\"end\":51695,\"start\":51690},{\"end\":51708,\"start\":51704},{\"end\":51721,\"start\":51718},{\"end\":51736,\"start\":51733},{\"end\":52131,\"start\":52127},{\"end\":52139,\"start\":52136},{\"end\":52155,\"start\":52149},{\"end\":52171,\"start\":52164},{\"end\":52181,\"start\":52178},{\"end\":52517,\"start\":52513},{\"end\":52525,\"start\":52522},{\"end\":52541,\"start\":52535},{\"end\":52549,\"start\":52547},{\"end\":52565,\"start\":52558},{\"end\":52575,\"start\":52572},{\"end\":52929,\"start\":52926},{\"end\":52940,\"start\":52936},{\"end\":52958,\"start\":52953},{\"end\":52972,\"start\":52965},{\"end\":52982,\"start\":52980},{\"end\":52992,\"start\":52989},{\"end\":53005,\"start\":53001},{\"end\":53019,\"start\":53014},{\"end\":53030,\"start\":53026},{\"end\":53047,\"start\":53040},{\"end\":53482,\"start\":53479},{\"end\":53494,\"start\":53492},{\"end\":53504,\"start\":53502},{\"end\":53518,\"start\":53514},{\"end\":53530,\"start\":53528},{\"end\":53835,\"start\":53831},{\"end\":53844,\"start\":53841},{\"end\":53856,\"start\":53853},{\"end\":53867,\"start\":53864},{\"end\":53882,\"start\":53878},{\"end\":53897,\"start\":53892},{\"end\":53910,\"start\":53907},{\"end\":53920,\"start\":53918},{\"end\":53933,\"start\":53931},{\"end\":53945,\"start\":53943},{\"end\":54288,\"start\":54282},{\"end\":54304,\"start\":54297},{\"end\":54319,\"start\":54315},{\"end\":54333,\"start\":54327},{\"end\":54777,\"start\":54770},{\"end\":54791,\"start\":54783},{\"end\":54805,\"start\":54801},{\"end\":54818,\"start\":54811},{\"end\":54830,\"start\":54826},{\"end\":55041,\"start\":55036},{\"end\":55057,\"start\":55051},{\"end\":55070,\"start\":55065},{\"end\":55085,\"start\":55078},{\"end\":55096,\"start\":55092},{\"end\":55110,\"start\":55106},{\"end\":55129,\"start\":55124},{\"end\":55140,\"start\":55136},{\"end\":55149,\"start\":55147},{\"end\":55166,\"start\":55154},{\"end\":55589,\"start\":55583},{\"end\":55602,\"start\":55599},{\"end\":55612,\"start\":55609},{\"end\":55626,\"start\":55623},{\"end\":55636,\"start\":55632},{\"end\":55650,\"start\":55646},{\"end\":56105,\"start\":56096},{\"end\":56121,\"start\":56112},{\"end\":56141,\"start\":56133},{\"end\":56160,\"start\":56147},{\"end\":56398,\"start\":56393},{\"end\":56404,\"start\":56400},{\"end\":56410,\"start\":56406},{\"end\":56590,\"start\":56581},{\"end\":56606,\"start\":56597},{\"end\":56626,\"start\":56618},{\"end\":56641,\"start\":56632},{\"end\":56890,\"start\":56885},{\"end\":56896,\"start\":56892},{\"end\":56902,\"start\":56898},{\"end\":57078,\"start\":57069},{\"end\":57094,\"start\":57085},{\"end\":57114,\"start\":57106},{\"end\":57129,\"start\":57120},{\"end\":57373,\"start\":57368},{\"end\":57379,\"start\":57375},{\"end\":57385,\"start\":57381}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1308.3432\",\"id\":\"b0\"},\"end\":37208,\"start\":36854},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":208290939},\"end\":37659,\"start\":37210},{\"attributes\":{\"doi\":\"arXiv:2306.12929\",\"id\":\"b2\"},\"end\":38007,\"start\":37661},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b3\"},\"end\":38331,\"start\":38009},{\"attributes\":{\"doi\":\"arXiv:2305.14314\",\"id\":\"b4\"},\"end\":38627,\"start\":38333},{\"attributes\":{\"doi\":\"arXiv:1902.08153\",\"id\":\"b5\"},\"end\":38968,\"start\":38629},{\"attributes\":{\"doi\":\"arXiv:2208.11580\",\"id\":\"b6\"},\"end\":39283,\"start\":38970},{\"attributes\":{\"doi\":\"arXiv:2210.17323\",\"id\":\"b7\"},\"end\":39647,\"start\":39285},{\"attributes\":{\"doi\":\"10.5281/zenodo.5371628\",\"id\":\"b8\"},\"end\":40300,\"start\":39649},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":61815367},\"end\":40634,\"start\":40302},{\"attributes\":{\"doi\":\"arXiv:2106.09685\",\"id\":\"b10\"},\"end\":41025,\"start\":40636},{\"attributes\":{\"doi\":\"arXiv:2306.07629\",\"id\":\"b11\"},\"end\":41402,\"start\":41027},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":237420872},\"end\":41913,\"start\":41404},{\"attributes\":{\"doi\":\"arXiv:2306.00317\",\"id\":\"b13\"},\"end\":42287,\"start\":41915},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":237420993},\"end\":42608,\"start\":42289},{\"attributes\":{\"doi\":\"arXiv:2102.05426\",\"id\":\"b15\"},\"end\":43026,\"start\":42610},{\"attributes\":{\"doi\":\"arXiv:2208.11945\",\"id\":\"b16\"},\"end\":43488,\"start\":43028},{\"attributes\":{\"doi\":\"arXiv:2306.00978\",\"id\":\"b17\"},\"end\":43864,\"start\":43490},{\"attributes\":{\"doi\":\"arXiv:2305.17888\",\"id\":\"b18\"},\"end\":44366,\"start\":43866},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235658553},\"end\":44730,\"start\":44368},{\"attributes\":{\"doi\":\"arXiv:1609.07843\",\"id\":\"b20\"},\"end\":44996,\"start\":44732},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":184487878},\"end\":45499,\"start\":44998},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":216056295},\"end\":45924,\"start\":45501},{\"attributes\":{\"id\":\"b23\"},\"end\":46005,\"start\":45926},{\"attributes\":{\"doi\":\"arXiv:1606.06031\",\"id\":\"b24\"},\"end\":46534,\"start\":46007},{\"attributes\":{\"doi\":\"arXiv:2206.09557\",\"id\":\"b25\"},\"end\":46958,\"start\":46536},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":204838007},\"end\":47481,\"start\":46960},{\"attributes\":{\"doi\":\"arXiv:2308.12950\",\"id\":\"b27\"},\"end\":47930,\"start\":47483},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":235624328},\"end\":48258,\"start\":47932},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":199370376},\"end\":48603,\"start\":48260},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b30\"},\"end\":49130,\"start\":48605},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b31\"},\"end\":49629,\"start\":49132},{\"attributes\":{\"doi\":\"arXiv:2307.09288\",\"id\":\"b32\"},\"end\":50129,\"start\":49631},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":102350477},\"end\":50628,\"start\":50131},{\"attributes\":{\"doi\":\"arXiv:2203.05740\",\"id\":\"b34\"},\"end\":51007,\"start\":50630},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":252545187},\"end\":51523,\"start\":51009},{\"attributes\":{\"doi\":\"arXiv:2304.09145\",\"id\":\"b36\"},\"end\":52025,\"start\":51525},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b37\"},\"end\":52411,\"start\":52027},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":253708271},\"end\":52872,\"start\":52413},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":227127479},\"end\":53360,\"start\":52874},{\"attributes\":{\"doi\":\"arXiv:2303.08302\",\"id\":\"b40\"},\"end\":53747,\"start\":53362},{\"attributes\":{\"doi\":\"arXiv:2304.01089\",\"id\":\"b41\"},\"end\":54217,\"start\":53749},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":247762245},\"end\":54707,\"start\":54219},{\"attributes\":{\"doi\":\"arXiv:1905.07830\",\"id\":\"b43\"},\"end\":55028,\"start\":54709},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b44\"},\"end\":55480,\"start\":55030},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":199552011},\"end\":55985,\"start\":55482},{\"attributes\":{\"id\":\"b46\"},\"end\":56389,\"start\":55987},{\"attributes\":{\"id\":\"b47\"},\"end\":56470,\"start\":56391},{\"attributes\":{\"id\":\"b48\"},\"end\":56881,\"start\":56472},{\"attributes\":{\"id\":\"b49\"},\"end\":56958,\"start\":56883},{\"attributes\":{\"id\":\"b50\"},\"end\":57364,\"start\":56960},{\"attributes\":{\"id\":\"b51\"},\"end\":57445,\"start\":57366}]", "bib_title": "[{\"end\":37266,\"start\":37210},{\"end\":40351,\"start\":40302},{\"end\":41489,\"start\":41404},{\"end\":42342,\"start\":42289},{\"end\":44417,\"start\":44368},{\"end\":45068,\"start\":44998},{\"end\":45561,\"start\":45501},{\"end\":47041,\"start\":46960},{\"end\":47997,\"start\":47932},{\"end\":48321,\"start\":48260},{\"end\":50194,\"start\":50131},{\"end\":51086,\"start\":51009},{\"end\":52501,\"start\":52413},{\"end\":52917,\"start\":52874},{\"end\":54272,\"start\":54219},{\"end\":55575,\"start\":55482},{\"end\":56088,\"start\":55987},{\"end\":56573,\"start\":56472}]", "bib_author": "[{\"end\":36961,\"start\":36946},{\"end\":36979,\"start\":36961},{\"end\":36996,\"start\":36979},{\"end\":37282,\"start\":37268},{\"end\":37297,\"start\":37282},{\"end\":37311,\"start\":37297},{\"end\":37323,\"start\":37311},{\"end\":37764,\"start\":37744},{\"end\":37778,\"start\":37764},{\"end\":37798,\"start\":37778},{\"end\":38023,\"start\":38009},{\"end\":38035,\"start\":38023},{\"end\":38051,\"start\":38035},{\"end\":38069,\"start\":38051},{\"end\":38347,\"start\":38333},{\"end\":38365,\"start\":38347},{\"end\":38379,\"start\":38365},{\"end\":38397,\"start\":38379},{\"end\":38404,\"start\":38397},{\"end\":38639,\"start\":38629},{\"end\":38656,\"start\":38639},{\"end\":38675,\"start\":38656},{\"end\":38684,\"start\":38675},{\"end\":39077,\"start\":39062},{\"end\":39091,\"start\":39077},{\"end\":39383,\"start\":39368},{\"end\":39407,\"start\":39383},{\"end\":39420,\"start\":39407},{\"end\":39430,\"start\":39420},{\"end\":39710,\"start\":39701},{\"end\":39724,\"start\":39710},{\"end\":39741,\"start\":39724},{\"end\":39752,\"start\":39741},{\"end\":39768,\"start\":39752},{\"end\":39784,\"start\":39768},{\"end\":39802,\"start\":39784},{\"end\":39815,\"start\":39802},{\"end\":39830,\"start\":39815},{\"end\":39850,\"start\":39830},{\"end\":39863,\"start\":39850},{\"end\":39879,\"start\":39863},{\"end\":39890,\"start\":39879},{\"end\":39903,\"start\":39890},{\"end\":39913,\"start\":39903},{\"end\":39925,\"start\":39913},{\"end\":39935,\"start\":39925},{\"end\":40368,\"start\":40353},{\"end\":40377,\"start\":40368},{\"end\":40394,\"start\":40377},{\"end\":40401,\"start\":40394},{\"end\":40646,\"start\":40636},{\"end\":40657,\"start\":40646},{\"end\":40671,\"start\":40657},{\"end\":40686,\"start\":40671},{\"end\":40705,\"start\":40686},{\"end\":40715,\"start\":40705},{\"end\":40724,\"start\":40715},{\"end\":40737,\"start\":40724},{\"end\":40743,\"start\":40737},{\"end\":40749,\"start\":40743},{\"end\":41082,\"start\":41070},{\"end\":41098,\"start\":41082},{\"end\":41112,\"start\":41098},{\"end\":41123,\"start\":41112},{\"end\":41133,\"start\":41123},{\"end\":41145,\"start\":41133},{\"end\":41156,\"start\":41145},{\"end\":41170,\"start\":41156},{\"end\":41179,\"start\":41170},{\"end\":41512,\"start\":41491},{\"end\":41525,\"start\":41512},{\"end\":41538,\"start\":41525},{\"end\":41544,\"start\":41538},{\"end\":41940,\"start\":41915},{\"end\":41948,\"start\":41940},{\"end\":41967,\"start\":41948},{\"end\":41972,\"start\":41967},{\"end\":41983,\"start\":41972},{\"end\":42356,\"start\":42344},{\"end\":42368,\"start\":42356},{\"end\":42375,\"start\":42368},{\"end\":42389,\"start\":42375},{\"end\":42399,\"start\":42389},{\"end\":42701,\"start\":42690},{\"end\":42714,\"start\":42701},{\"end\":42722,\"start\":42714},{\"end\":42733,\"start\":42722},{\"end\":42742,\"start\":42733},{\"end\":42752,\"start\":42742},{\"end\":42764,\"start\":42752},{\"end\":42774,\"start\":42764},{\"end\":42782,\"start\":42774},{\"end\":43135,\"start\":43123},{\"end\":43145,\"start\":43135},{\"end\":43157,\"start\":43145},{\"end\":43171,\"start\":43157},{\"end\":43183,\"start\":43171},{\"end\":43197,\"start\":43183},{\"end\":43211,\"start\":43197},{\"end\":43222,\"start\":43211},{\"end\":43578,\"start\":43570},{\"end\":43592,\"start\":43578},{\"end\":43606,\"start\":43592},{\"end\":43618,\"start\":43606},{\"end\":43631,\"start\":43618},{\"end\":43641,\"start\":43631},{\"end\":43952,\"start\":43940},{\"end\":43965,\"start\":43952},{\"end\":43982,\"start\":43965},{\"end\":43995,\"start\":43982},{\"end\":44009,\"start\":43995},{\"end\":44024,\"start\":44009},{\"end\":44038,\"start\":44024},{\"end\":44065,\"start\":44038},{\"end\":44080,\"start\":44065},{\"end\":44432,\"start\":44419},{\"end\":44444,\"start\":44432},{\"end\":44453,\"start\":44444},{\"end\":44464,\"start\":44453},{\"end\":44474,\"start\":44464},{\"end\":44483,\"start\":44474},{\"end\":44781,\"start\":44765},{\"end\":44796,\"start\":44781},{\"end\":44812,\"start\":44796},{\"end\":44828,\"start\":44812},{\"end\":45084,\"start\":45070},{\"end\":45101,\"start\":45084},{\"end\":45121,\"start\":45101},{\"end\":45134,\"start\":45121},{\"end\":45577,\"start\":45563},{\"end\":45593,\"start\":45577},{\"end\":45610,\"start\":45593},{\"end\":45628,\"start\":45610},{\"end\":45648,\"start\":45628},{\"end\":45936,\"start\":45928},{\"end\":45944,\"start\":45936},{\"end\":46022,\"start\":46007},{\"end\":46041,\"start\":46022},{\"end\":46061,\"start\":46041},{\"end\":46072,\"start\":46061},{\"end\":46088,\"start\":46072},{\"end\":46105,\"start\":46088},{\"end\":46121,\"start\":46105},{\"end\":46135,\"start\":46121},{\"end\":46150,\"start\":46135},{\"end\":46161,\"start\":46150},{\"end\":46639,\"start\":46627},{\"end\":46654,\"start\":46639},{\"end\":46668,\"start\":46654},{\"end\":46684,\"start\":46668},{\"end\":46698,\"start\":46684},{\"end\":46711,\"start\":46698},{\"end\":47057,\"start\":47043},{\"end\":47071,\"start\":47057},{\"end\":47085,\"start\":47071},{\"end\":47100,\"start\":47085},{\"end\":47115,\"start\":47100},{\"end\":47131,\"start\":47115},{\"end\":47143,\"start\":47131},{\"end\":47151,\"start\":47143},{\"end\":47164,\"start\":47151},{\"end\":47552,\"start\":47528},{\"end\":47568,\"start\":47552},{\"end\":47583,\"start\":47568},{\"end\":47596,\"start\":47583},{\"end\":47616,\"start\":47596},{\"end\":47627,\"start\":47616},{\"end\":47639,\"start\":47627},{\"end\":47648,\"start\":47639},{\"end\":47662,\"start\":47648},{\"end\":47669,\"start\":47662},{\"end\":48014,\"start\":47999},{\"end\":48031,\"start\":48014},{\"end\":48342,\"start\":48323},{\"end\":48352,\"start\":48342},{\"end\":48366,\"start\":48352},{\"end\":48385,\"start\":48366},{\"end\":48391,\"start\":48385},{\"end\":48627,\"start\":48605},{\"end\":48644,\"start\":48627},{\"end\":48657,\"start\":48644},{\"end\":48673,\"start\":48657},{\"end\":48686,\"start\":48673},{\"end\":48701,\"start\":48686},{\"end\":48727,\"start\":48701},{\"end\":48746,\"start\":48727},{\"end\":48761,\"start\":48746},{\"end\":48768,\"start\":48761},{\"end\":49146,\"start\":49132},{\"end\":49162,\"start\":49146},{\"end\":49179,\"start\":49162},{\"end\":49196,\"start\":49179},{\"end\":49216,\"start\":49196},{\"end\":49234,\"start\":49216},{\"end\":49258,\"start\":49234},{\"end\":49270,\"start\":49258},{\"end\":49285,\"start\":49270},{\"end\":49292,\"start\":49285},{\"end\":49698,\"start\":49684},{\"end\":49712,\"start\":49698},{\"end\":49725,\"start\":49712},{\"end\":49739,\"start\":49725},{\"end\":49756,\"start\":49739},{\"end\":49772,\"start\":49756},{\"end\":49791,\"start\":49772},{\"end\":49805,\"start\":49791},{\"end\":49824,\"start\":49805},{\"end\":49840,\"start\":49824},{\"end\":50207,\"start\":50196},{\"end\":50220,\"start\":50207},{\"end\":50231,\"start\":50220},{\"end\":50239,\"start\":50231},{\"end\":50249,\"start\":50239},{\"end\":50731,\"start\":50718},{\"end\":50744,\"start\":50731},{\"end\":50755,\"start\":50744},{\"end\":50770,\"start\":50755},{\"end\":50782,\"start\":50770},{\"end\":51101,\"start\":51088},{\"end\":51116,\"start\":51101},{\"end\":51132,\"start\":51116},{\"end\":51145,\"start\":51132},{\"end\":51162,\"start\":51145},{\"end\":51172,\"start\":51162},{\"end\":51184,\"start\":51172},{\"end\":51199,\"start\":51184},{\"end\":51655,\"start\":51642},{\"end\":51670,\"start\":51655},{\"end\":51681,\"start\":51670},{\"end\":51697,\"start\":51681},{\"end\":51710,\"start\":51697},{\"end\":51723,\"start\":51710},{\"end\":51738,\"start\":51723},{\"end\":52133,\"start\":52117},{\"end\":52141,\"start\":52133},{\"end\":52157,\"start\":52141},{\"end\":52173,\"start\":52157},{\"end\":52183,\"start\":52173},{\"end\":52519,\"start\":52503},{\"end\":52527,\"start\":52519},{\"end\":52543,\"start\":52527},{\"end\":52551,\"start\":52543},{\"end\":52567,\"start\":52551},{\"end\":52577,\"start\":52567},{\"end\":52931,\"start\":52919},{\"end\":52942,\"start\":52931},{\"end\":52960,\"start\":52942},{\"end\":52974,\"start\":52960},{\"end\":52984,\"start\":52974},{\"end\":52994,\"start\":52984},{\"end\":53007,\"start\":52994},{\"end\":53021,\"start\":53007},{\"end\":53032,\"start\":53021},{\"end\":53049,\"start\":53032},{\"end\":53484,\"start\":53472},{\"end\":53496,\"start\":53484},{\"end\":53506,\"start\":53496},{\"end\":53520,\"start\":53506},{\"end\":53532,\"start\":53520},{\"end\":53837,\"start\":53823},{\"end\":53846,\"start\":53837},{\"end\":53858,\"start\":53846},{\"end\":53869,\"start\":53858},{\"end\":53884,\"start\":53869},{\"end\":53899,\"start\":53884},{\"end\":53912,\"start\":53899},{\"end\":53922,\"start\":53912},{\"end\":53935,\"start\":53922},{\"end\":53947,\"start\":53935},{\"end\":54290,\"start\":54274},{\"end\":54306,\"start\":54290},{\"end\":54321,\"start\":54306},{\"end\":54335,\"start\":54321},{\"end\":54779,\"start\":54764},{\"end\":54793,\"start\":54779},{\"end\":54807,\"start\":54793},{\"end\":54820,\"start\":54807},{\"end\":54832,\"start\":54820},{\"end\":55043,\"start\":55030},{\"end\":55059,\"start\":55043},{\"end\":55072,\"start\":55059},{\"end\":55087,\"start\":55072},{\"end\":55098,\"start\":55087},{\"end\":55112,\"start\":55098},{\"end\":55131,\"start\":55112},{\"end\":55142,\"start\":55131},{\"end\":55151,\"start\":55142},{\"end\":55168,\"start\":55151},{\"end\":55591,\"start\":55577},{\"end\":55604,\"start\":55591},{\"end\":55614,\"start\":55604},{\"end\":55628,\"start\":55614},{\"end\":55638,\"start\":55628},{\"end\":55652,\"start\":55638},{\"end\":56107,\"start\":56090},{\"end\":56123,\"start\":56107},{\"end\":56143,\"start\":56123},{\"end\":56162,\"start\":56143},{\"end\":56400,\"start\":56393},{\"end\":56406,\"start\":56400},{\"end\":56412,\"start\":56406},{\"end\":56592,\"start\":56575},{\"end\":56608,\"start\":56592},{\"end\":56628,\"start\":56608},{\"end\":56643,\"start\":56628},{\"end\":56652,\"start\":56643},{\"end\":56892,\"start\":56885},{\"end\":56898,\"start\":56892},{\"end\":56904,\"start\":56898},{\"end\":57080,\"start\":57063},{\"end\":57096,\"start\":57080},{\"end\":57116,\"start\":57096},{\"end\":57131,\"start\":57116},{\"end\":57141,\"start\":57131},{\"end\":57375,\"start\":57368},{\"end\":57381,\"start\":57375},{\"end\":57387,\"start\":57381}]", "bib_venue": "[{\"end\":36944,\"start\":36854},{\"end\":37384,\"start\":37323},{\"end\":37742,\"start\":37661},{\"end\":38138,\"start\":38085},{\"end\":38458,\"start\":38420},{\"end\":38778,\"start\":38700},{\"end\":39060,\"start\":38970},{\"end\":39366,\"start\":39285},{\"end\":39699,\"start\":39649},{\"end\":40449,\"start\":40401},{\"end\":40809,\"start\":40765},{\"end\":41068,\"start\":41027},{\"end\":41615,\"start\":41544},{\"end\":42079,\"start\":41999},{\"end\":42442,\"start\":42399},{\"end\":42688,\"start\":42610},{\"end\":43121,\"start\":43028},{\"end\":43568,\"start\":43490},{\"end\":43938,\"start\":43866},{\"end\":44532,\"start\":44483},{\"end\":44763,\"start\":44732},{\"end\":45205,\"start\":45134},{\"end\":45692,\"start\":45648},{\"end\":46249,\"start\":46177},{\"end\":46625,\"start\":46536},{\"end\":47204,\"start\":47164},{\"end\":47526,\"start\":47483},{\"end\":48075,\"start\":48031},{\"end\":48416,\"start\":48391},{\"end\":48840,\"start\":48784},{\"end\":49353,\"start\":49308},{\"end\":49682,\"start\":49631},{\"end\":50330,\"start\":50249},{\"end\":50716,\"start\":50630},{\"end\":51248,\"start\":51199},{\"end\":51640,\"start\":51525},{\"end\":52115,\"start\":52027},{\"end\":52621,\"start\":52577},{\"end\":53093,\"start\":53049},{\"end\":53470,\"start\":53362},{\"end\":53821,\"start\":53749},{\"end\":54415,\"start\":54335},{\"end\":54762,\"start\":54709},{\"end\":55228,\"start\":55184},{\"end\":55714,\"start\":55652},{\"end\":56167,\"start\":56162},{\"end\":56426,\"start\":56412},{\"end\":56657,\"start\":56652},{\"end\":56918,\"start\":56904},{\"end\":57061,\"start\":56960},{\"end\":57401,\"start\":57387},{\"end\":37432,\"start\":37386},{\"end\":41673,\"start\":41617},{\"end\":45263,\"start\":45207},{\"end\":50398,\"start\":50332},{\"end\":54482,\"start\":54417}]"}}}, "year": 2023, "month": 12, "day": 17}