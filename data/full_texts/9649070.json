{"id": 9649070, "updated": "2023-06-08 18:20:26.174", "metadata": {"title": "3D Semantic Parsing of Large-Scale Indoor Spaces", "authors": "[{\"first\":\"Iro\",\"last\":\"Armeni\",\"middle\":[]},{\"first\":\"Ozan\",\"last\":\"Sener\",\"middle\":[]},{\"first\":\"Amir\",\"last\":\"Zamir\",\"middle\":[\"R.\"]},{\"first\":\"Helen\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Ioannis\",\"last\":\"Brilakis\",\"middle\":[]},{\"first\":\"Martin\",\"last\":\"Fischer\",\"middle\":[]},{\"first\":\"Silvio\",\"last\":\"Savarese\",\"middle\":[]}]", "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "In this paper, we propose a method for semantic parsing the 3D point cloud of an entire building using a hierarchical approach: first, the raw data is parsed into semantically meaningful spaces (e.g. rooms, etc) that are aligned into a canonical reference coordinate system. Second, the spaces are parsed into their structural and building elements (e.g. walls, columns, etc). Performing these with a strong notation of global 3D space is the backbone of our method. The alignment in the first step injects strong 3D priors from the canonical coordinate system into the second step for discovering elements. This allows diverse challenging scenarios as man-made indoor spaces often show recurrent geometric patterns while the appearance features can change drastically. We also argue that identification of structural elements in indoor spaces is essentially a detection problem, rather than segmentation which is commonly used. We evaluated our method on a new dataset of several buildings with a covered area of over 6, 000m2 and over 215 million points, demonstrating robust results readily useful for practical applications.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2460657278", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ArmeniSZJBFS16", "doi": "10.1109/cvpr.2016.170"}}, "content": {"source": {"pdf_hash": "6b49b35969aa750f0bbb136a2ee31c33e00008d9", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://www.repository.cam.ac.uk/bitstreams/504ea6ba-c0e9-426b-98f2-7f7405bd7623/download", "status": "GREEN"}}, "grobid": {"id": "f0d96a7ea5b15581d144235f16a123f783066ddc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6b49b35969aa750f0bbb136a2ee31c33e00008d9.txt", "contents": "\n3D Semantic Parsing of Large-Scale Indoor Spaces (a) Raw Point Cloud (b) Space Parsing and Alignment in Canonical 3D Space (c) Building Element Detection Enclosed Spaces\n\n\nIro Armeni \nStanford University\n\n\nOzan Sener \nStanford University\n\n\nCornell University\n\n\nAmir R Zamir \nStanford University\n\n\nHelen Jiang \nStanford University\n\n\nIoannis Brilakis \nUniversity of Cambridge\n\n\nMartin Fischer \nStanford University\n\n\nSilvio Savarese \nStanford University\n\n\n3D Semantic Parsing of Large-Scale Indoor Spaces (a) Raw Point Cloud (b) Space Parsing and Alignment in Canonical 3D Space (c) Building Element Detection Enclosed Spaces\n\ntable bookcase window door floor wall chair column ceiling beam board sofa z x y Figure 1: Semantic parsing of a large-scale point cloud. Left: the raw point cloud. Middle: the results of parsing the point cloud into disjoint spaces (i.e. the floor plan). Right: the results of parsing a detected room (marked with the black circle) into semantic elements.AbstractIn this paper, we propose a method for semantic parsing the 3D point cloud of an entire building using a hierarchical approach: first, the raw data is parsed into semantically meaningful spaces (e.g. rooms, etc) that are aligned into a canonical reference coordinate system. Second, the spaces are parsed into their structural and building elements (e.g.  walls, columns, etc). Performing these with a strong notation of global 3D space is the backbone of our method. The alignment in the first step injects strong 3D priors from the canonical coordinate system into the second step for discovering elements. This allows diverse challenging scenarios as man-made indoor spaces often show recurrent geometric patterns while the appearance features can change drastically. We also argue that identification of structural elements in indoor spaces is essentially a detection problem, rather than segmentation which is commonly used. We evaluated our method on a new dataset of several buildings with a covered area of over 6, 000m 2 and over 215 million points, demonstrating robust results readily useful for practical applications.\n\nIntroduction\n\nDuring the past few years, 3D imaging technology experienced a major progress with the production of inexpensive depth sensors (e.g. Kinect [2]). This caused a leap in the development of many successful semantic segmentation methods that use both RGB and depth [26,31,7]. However, the 3D sensing field has recently undergone a followup shift with the availability of mature technology for scanning large-scale spaces, e.g. an entire building. Such systems can reliably form the 3D point cloud of thousands of square meters with the number of points often exceeding hundreds of millions (see Fig. 1 left). This demands semantic parsing methods capable of coping with this scale, and ideally, exploiting the unique characteristics of such data.\n\nLarge-scale scans of buildings pose new challenges/opportunities in semantic parsing that are different from, or not faced in, small-scale RGB-D segmentation:\n\nRicher Geometric Information: Large-scale point clouds make the entire building available at once. This allows utilizing recurrent geometric regularities common in manmade structures. Such possibilities are beyond what a single-view depth sensor would provide, as they have part of one room or at most few rooms in their scope. Complexity: Existing semantic segmentation methods designed for small-scale point clouds or RGB-D images are not immediately applicable to large-scale scans due to complexity issues and the fact that choosing a set of representative views from an unbounded number of feasible singleviews is non-trivial. Introduction of New Semantics: Large-scale point clouds of indoor spaces introduce semantics that did not exist in small-scale point clouds or RGB-D images: disjoint spaces like rooms, hallways, etc. Parsing a raw point cloud into such spaces (essentially a floor plan) is a relatively new and valid problem. Novel Applications: A number of novel applications becomes feasible in the context of whole building point clouds, such as, generating space statistics, building analysis (e.g., workspace efficiency), or space manipulation (e.g., removing walls between rooms).\n\nAforementioned points signify the necessity of adopting new approaches to semantic parsing of large-scale point clouds. In this paper, we introduce a method that, given a raw large-scale colored point cloud of an indoor space, first parses it into semantic spaces (e.g., hallways, rooms), and then, further parses those spaces into their structural (e.g. floor, walls, etc.) and building (e.g. furniture) elements (see Fig. 1). One property of our approach is utilizing in semantic element detection the geometric priors acquired from parsing into disjoint spaces, and then, reincorporating the detected elements in updating the found spaces (Sec. 3.2).\n\nAnother key property is reformulating the element parsing task as a detection problem, rather than segmentation. Existing segmentation paradigms start with the assumption that each point must belong to a single segment/class. However, the problem of building element parsing better fits a detection approach. Clutter can occlude parts of important elements, e.g. a white board can occlude a wall. To a segmentation technique, this wall would be an irregular entity with a hole on it, while detecting the wall as a whole provides a better structural understanding of it (see Sec. 4).\n\nThe contributions of this paper can be summarized as: I) We claim and experimentally evaluate that space dividers (i.e. walls) can be robustly detected using the empty space attributed to them in the point cloud. In other words, instead of detecting points belonging to the boundaries of a room, we detect the empty space bounded by them.\n\nII) We show that structural and building elements can be robustly detected using strong geometric priors induced by space parsing. We demonstrate satisfactory parsing results by heavily exploiting such features.\n\nIII) We collected a large-scale dataset composed of colored 3D scans 1 ) of indoor areas of large buildings with var- 1 Collection of points with 3D coordinates and RGB color values. ious architectural styles. A few samples of these spaces can be seen in Fig. 1 and 5. We annotated the semantic spaces and their elements in 3D. We further collected a set of RGB-D images registered on the colored point cloud to enrich the dataset (not used by our method). Annotations are consistent across all modalities (3D point cloud and RGB, and depth images). The dataset, annotations, the code and parsing results of the proposed framework are available to public at buildingparser.stanford.edu.\n\n\nRelated Work\n\nWe provide an overview of the related literature below, but as a brief summary, the following main points differentiate our approach from existing techniques: 1) processing a large-scale point cloud of an entire building (indoor spaces), rather than one or few RGB-D images, 2) detection of space dividers (walls) based on their void (empty) space rather than planar-surface/linear-boundary assumptions, 3) utilizing a set of geometric priors extracted in a normalized canonical space, 4) adopting a detection-based approach, rather than segmentation, to element parsing.\n\nSemantic RGB-D and 3D segmentation have been investigated in a large number of papers during the past few years. For instance, [30,24] proposed an RGB-D segmentation method using a set of heuristics for leveraging 3D geometric priors. [21] developed a search-classify based method for segmentation and modeling of indoor spaces. These are different from our method as they address the problem in a small-scale. A few methods attempted using multiple depth views [29,14], yet they as well remain limited to a small-scale and do not utilize the advantages of a larger scope. [22] performed semantic parsing of buildings but for outdoor spaces. To parse a panoramic RGB-D image, [41] uses the global geometry of the room and cuboid like objects. Though an RGB-D panorama includes more information than a typical RGB-D image, it is not as comprehensive as a 3D point cloud. There also exist many object detection methods developed for RGB-D. These methods either try to extend the RGB methods directly into RGB-D by treating depth as a fourth channel [13,19,31,26,3] or use external sources like CAD models [32]. These methods use image-specific features and do not extend to point clouds. They are also not designed to handle large structural elements, such as floor and ceiling.\n\nIn the context of floor plan estimation, [4] proposed an approach based on trajectory crowd sourcing for estimating a floor plan, while we use an automatically generated 3D point cloud. [38] reconstructed museum type spaces based on Hough transform which is challenged in cluttered scenes (as verified by our experiments), though their goal is not estimation of floor plan. [40] also employs similar planar surface assumption in order to estimate the semantics of a single room using contextual information. [20]  cluttered indoor spaces but their method as well as that of [23] require prior knowledge of scan locations and extraction of planar patches as candidate walls. [36] generated a minimalistic floor plan by first triangulating the 2D floor plan and then merging adjacent segments to obtain the final space partitioning. Their approach does not handle occlusions effectively and requires the scan locations. Liu et al. [18] reconstructed a building in 3D given monocular images and the floor-plan. On the contrary, we find the floor-plan as well as semantic elements therein given a 3D point cloud.\n\n\nParsing Point Cloud into Disjoint Spaces\n\nOur hierarchical parsing method starts with parsing the whole building into spaces that are semantically meaningful (e.g. rooms). This step yields an understanding of the spatial layout and the spaces therein, which will play a central role in the formulation of the second step (Sec. 3.2).\n\n\nDetection incorporating void spaces\n\nEach scanned element in a point cloud is represented as a group of points encompassing its inner void (empty) space. The scanned points belong to the exterior surfaces of the element since only this outer shell is visible to a 3D sensor. However, the inner void is a crucial component in defining the element and its overall shape. This perspective suggests that void space could be actively incorporated in detecting and understanding 3D elements.\n\nSpace dividers (e.g. walls) separating neighboring enclosed spaces are not an exception. Previous efforts towards detecting walls in point clouds overlook this and try to to fit planar surfaces (e.g. [40]) or linear boundaries employing algorithms, such as RANSAC or Hough Transform. These are easily challenged in practice since walls are often cluttered with furniture, and sometimes even not visible.\n\nIn contrast, we follow the \"void-based\" approach and detect space dividers based on their signature in point clouds: a space divider is depicted as a void space bounded by two coarsely parallel margins. This signature remains robust even if the enclosed space is severely cluttered, since we do not detect surfaces or space boundaries but the void in-between. This is shown in Fig. 2 left (b) which depicts two adjacent rooms. The wall and its void space are indicated with red lines. If we form a 1 dimensional histogram of density of points along the x axis (i.e. the signal in Fig. 2 left (b)), the wall appears with the signature of two peaks with an empty space in-between. Attempting to find a wall through detecting planar surfaces would be equivalent to looking for peaks in this signal. As apparent, many strong peaks (e.g. due to the bookcase or table side) appear which make detection of walls difficult. Instead, the peakgap-peak structure is significantly more discriminative and robust. This signature is one of the useful consequences of having the point cloud of the entire building at once.\n\n\nDetecting the peak-gap-peak pattern\n\nIn order to detect the peak-gap-peak pattern, we follow a template matching approach using a bank of peak-gap-peak filters and perform the matching operation via convolutions on a density histogram signal ( Fig. 2 left (a)). This filter bank is shown in Fig. 2 right (b) and has the characteristic of two peaks separated by void with varying widths. The blue curve in Fig. 2 left (c) is the convolution response which shows the filter has strongly responded to the peak-gappeak signature and suppressed the other peaks. It should be noted that the employed filter assumes buildings with roughly planar walls, and hence, does not handle circular and oval shaped rooms or other configurations that deviate from the major axes of the building. However, as nonrectangular rooms make up for a considerably small portion of indoor space layouts [33], this assumption is considered reasonable. A remedy to irregular walls/rooms would be to employ a similar approach, but with a 2D filter bank that is also parametrized over curvature. Since though they account for a small portion, the practical importance of this improvement would not be obvious.\n\nIn greater detail, given a raw point cloud, we first align the three main axes 2 of x \u2212 y \u2212 z with the general struc-  ture of the building. We form a 1 dimensional histogram of density of points along one of the three axes, say H(s).\n\nThen, we create a bank of filters parametrized by the passwidth (c) and the stop-width (\u03b4) as shown in Fig. 2 right (b).\n\nThe filters can be represented as\ng \u03b4,c (s) = 1 2C \u03a0 \u03b4 2 +C (s) \u2212 1 2C \u03a0 \u03b4 2 (s) where \u03a0 k (s) = 1[|s| \u2264 k] and 1[A]\nis an indicator function which is 1 when A is true and 0 otherwise.\n\nWe compute responses of filters when convolved with H(s) as shown in Fig. 2 right (b). Each convolution results in a 3-dimensional score function over the axis of choice s and the parameters c and \u03b4. We then apply max-pooling across s (i.e. pooling parameters are c and \u03b4) to detect a set of wall candidates over the axis of choice. Finally, we apply non-maximum suppression to detect the final wall locations (see [5] for details). We use a bank of filters and pooling since the shape characteristics of space dividers (e.g. width) is not known a priori.\n\nThe found dividers decompose the point cloud into slices along the direction of the detection axis. We then perform the same procedure for the 2 nd and 3 rd axes on each slice to fully partition the point cloud. Since we process each axis independently, any divider is elongated in its original direction resulting in an over-segmented grid (see Fig. 3 (b)). This is due to the fact that we detect the dividers in a 1-dimensional manner (i.e. by considering one axis at a time). This reduction to 1 dimension enables us to scale to large point clouds (linearly with respect to covered area), but it cannot count for the fact that a divider may not extend across the entire building, thus leading to an oversegmentation. In order to efficiently recover the correct segmentation, we perform a series of merging operations.\n\n\nMerging\n\nIn order to merge the over-segments, we adopt a bottomup approach by recursively merging neighbors. We form a graph in which each oversegment is represented by a node, and edges exist between each node and its closest spatial neighbors (see Fig. 3 (c)). We then examine each edge for the existence of a divider between its incident nodes. We check this by detecting the peak-gap-peak on the chunk of point cloud formed by the two incident nodes using the same method of Sec. 3.1.1. If a divider is detected, the edge is removed from the graph. When all edges are examined, the surviving ones (shown in Fig. 3 (d)) denote the over-segments that should be merged. Therefore, the final spaces (Fig. 3 (e)) are the Connected Components of the graph with survived edges (each Connected Component is one space). Through transitivity, the merging operation can extend to any shape and size. In other words, any two oversegments with a path between them will be merged (e.g., see the large room in the middle of Fig. 3 (a)).\n\nIn summary, by exploiting the void-based principle we developed an unsupervised, parameter-free and efficient algorithm to parse a large point cloud into disjoint spaces.\n\n\nCanonical Coordinate System Among Spaces\n\nDecomposing the raw point cloud into disjoint spaces provides geometric priors for detecting semantic elements. This is mostly because spaces have recurrent structure and layout configuration. This structure can be easily exploited by creating a common coordinate system for all spaces. Specifically, we perform the following operations on one semantic space (e.g. a room, hallway, etc.) to form an x\u2212y\u2212z Cartesian reference coordinate system. I) We choose the (z) axis of the reference system as the gravitational axis.\n\nII) We align the x axis along the entrance to the room. Consequently, y axis will be perpendicular to the entrance wall. (see [5] for details).\n\nIII) We then scale the space into a unit cube by simply normalizing the coordinates of the aligned points to range in [0,1]. This allows better generalization and information transfer across different spaces and buildings. An example reference system is shown in Fig. 4. This procedure puts each space in a unit cube aligned across all detected spaces. It results in a geometric representation of it in a single and coherent coordinate system. Such a procedure is not straightforward in the conventional single-view 3D or RGB-D scans since global context is not captured.\n\n\nParsing Disjoint Spaces into Elements\n\nGiven a space in the common reference system, we wish to detect and label the semantic elements therein.\n\nParsing-by-Detection: Structural building analysis and augmented reality are some of the applications that benefit from parsing a point cloud into semantic elements. An analysis of such applications suggests that assuming every point must belong to one class, as in the conventional segmentation paradigm, is not a concrete assumption since it results in elements of incomplete geometry (e.g. hole in wall segment due to clutter). The applications can benefit from a notion of the parsed element and its structural characteristics as a whole regardless of occlusions (Sec. 1). Also, there is always a considerable number of points that either do not belong to any class or are not in the interest of the application. Hence, we argue that a more suitable approach is detecting and localizing each element, rather than segmentation.\n\nRepresenting Detections: Our detection framework follows a 3D sliding window approach; we slide a set of candidate windows (boxes in 3D) for each class and classify if there is an object of the class of interest in the window. These classifiers, window sizes, and their shapes are all learned.\n\nIn order to learn the size and shape of the candidate windows, we first need a representation for 3D windows. Since the semantic spaces are normalized with respect to the common coordinate system, our candidate windows should lie in it as well. In addition to the size, we also need to represent the shape. We create K-by-K-by-K voxel grid by dividing the window into equal sized sub-3D windows and define the occupancy pattern B i for i \u2208 [K\u2212by\u2212K\u2212by\u2212K] as B i is 1 if the sub-window i is part of the shape and 0 otherwise. To summarize, a candidate window is represented by its position P (location of the bottom-left corner in the common coordinate system), its 3D size S in the unit cube, and its occupancy pattern B.\n\nTo classify each candidate window as an object or not we need a set of features which can discriminatively represent the geometry and appearance of the volume specified by the window. Since our points lie in the normalized unit cube, P and S are informative about the global geometry of the window with respect to the space (global features). We also compute a set of features for each occupied sub-window as local geometry and appearance features (local features). We   Table 1 and visualize them in Fig. 4.\n\nLearning to Detect Elements: Our learning approach consists of learning candidate window shapes and learning object detectors. Learning candidate windows: In order to learn a dictionary of candidate detection windows, we compute a set of detection windows as the tightest bounding boxes and their occupancy pattern for each element in the training data. We then group this set into clusters using Affinity Propogation [10] with distance metric intersectionover-union and the occupancy pattern. After clustering, we compute a single detection window per cluster with size equal to the average of the cluster members size and occupancy pattern equal to the mode of that of the cluster members. Training element detectors: In order to learn the element detectors, we use linear classifiers such that given a feature vector \u03a6 of the detection window and the classifier w, \u03a6 \u22ba w > \u03c4 means the candidate window corresponds to a semantic element. We train a linear classifier per class via LIBLINEAR [9]. Negative examples include both elements of other classes and randomly mined hard negatives.\n\nSemantic Element Proposal Generation: Our learning procedure results in element detectors w e and a dictionary of shapes per class. Given the learnt parameters, we use a sliding window approach to detect element proposals. At each sliding position, the SVM detectors are evaluated for each shape atom in the dictionary. The resulting detections are further eliminated with non-maximum suppression producing a final set of proposals as {(D i , e i , l i )} 1...N where D i is the position of the detection, e i is the label of the semantic element class, and l i is the detection score .\n\n\nEnforcing Contextual Consistency using CRF\n\nThe element proposal generation step does not exploit the context of space, as all elements are generated with no explicit consideration of others. However, there is a strong context among semantic elements since the location of one gives a prior over the location of the others. To exploit this property, we employ a graphical model based approach. Features of non-occupied voxels are 0 (see Table 1 for all features).\n\nGiven a collection of detection proposals, we want to choose a subset of them as the final elements. We define our model based on a graph G(V, E) in which the nodes correspond to the detection proposals and the edges model the relationship among elements. Each node is connected to its k e nearest proposals from each class e. Hence, we have ( e k e ) V edges. For each node, we want to infer if it should be in the final detection set or not which results in a binary label space as y v \u2208 {0, 1}. The edge features are\n\u03a6 e=(v,w) = [B v , B w , S v , S w , |P v \u2212 P w |, ],\nwhere | \u00b7 | is the absolute value function. The unary feature is the detection score acquired from the SVM classifier.\n\nFollowing the log-linear model [15], we predict the final elements as a maximization problem of the energy function:\narg max y v\u2208V w 0 l v y v + (u,v)\u2208E y v y u (w eu,ev \u00b7 \u03a6 u,v ) ,(1)\nwhich can be written as an integer program by introducing auxiliary variables y uv = y u y v \u2200u, v \u2208 V as:\narg max y v\u2208V w 0 l v y v + (u,v)\u2208E y vu (w eu,ev \u00b7 \u03a6 u,v ) s.t.y uv \u2264 y u \u2200u \u2208 V, \u2200v \u2208 N (u) s.t.y u + y v \u2264 y uv + 1 \u2200u, v \u2208 E.(2)\nThis maximization is performed using an off-the-shelf LP/MIP solver and the weight vectors w are learned using Structured SVM [35]. Our implementation follows the existing S-SVM-CRF implementations [16,17,28] and the details can be found in the supplementary [5].\n\n\nUpdating the Disjoint Space Parsing Results\n\nSince 'wall' is one of the classes in the element detection step, we utilize the identified walls to update the space dividers found by the peak-gap-peak method of Sec. 3.1.1. This may recover the walls missed by the peak-gap-peak filters as the element detection step incorporates additional features, such as, color or local geometry. In a similar way to the merging operation discussed in Sec. 3.1.2, we obtain the neighbors graph of the found spaces, and for each pair of neighbors we check if there is a detected wall in the connection area; the only difference is that the walls now come from the element detection step and not the peak-gap-peak filters. We then remove edges from the graph when no wall is found and use a connected components graph to form the final space parsing (see Fig. 3 (f)).\n\n\nExperiments\n\nIn this section, we present our experimental results and share the insights we drew from them.\n\n\nDataset\n\nOur dataset is composed of five large-scale indoor areas from three different buildings, each covering approximately 1900, 450, 1700, 870 and 1100 square meters (total of 6020 square meters). These areas show diverse properties in architectural style and appearance and include mainly office areas, educational and exhibition spaces, and conference rooms, personal offices, restrooms, open spaces, lobbies, stairways, and hallways are commonly found therein. One of the areas includes multiple floors, whereas the rest have one. The entire point clouds are automatically generated without any manual intervention using the Matterport [1] scanner (only 3D point clouds; no images used by our method). Parts of these areas can be seen in Fig. 5.\n\nWe detect 12 semantic elements, which are structural elements (ceiling, floor, wall, beam, column, window and door) and commonly found items and furniture (table, chair, sofa, bookcase and board). Notice that these classes are more fine-grained and challenging than many of the semantic indoor segmentation datasets [31,39].\n\n\nParsing into Disjoint Spaces\n\nThe qualitative results of the proposed space parsing method for several sample areas in the dataset are provided in Fig. 5. Parts (a), (g), and (e) show the raw point cloud, manually generated ground truth, and our results before the update step, respectively. Part (d) illustrates the oversegmented space before merging which shows the effectiveness of the merging step. It is worth mentioning that the hallways are sometimes over-segmented although they belong to one segment in the ground truth. This is attributed to \"bottleneck\" areas in some hallways which in combination with their narrow width creates the illusion of a space divider in the density histogram. However, after updating the parsed spaces such issues are resolved (Part (f)).\n\nBaselines: We compare our method with a RANSAC-based plane fitting and a Hough transform-based line fitting methods. These approaches were used in two prominent [27,38] papers in this area. Even though their goal is not space parsing their intermediate results can be adopted. To make the baselines appropriate for space segmentation we post process their detections and well tweaked their parameters. The results shown in Fig. 5 and Table 2 were achieved using these parameters. a. Raw Point Cloud g. Ground Truth e. Ours (merged) d. Ours (oversegmented) c. RANSAC [25] b. Hough Transform [34]   Quantitative Results: Table 2 provides quantitative results for space parsing. We adopt the standard unsupervised clustering metric Adjusted Rand Index (ARI) [25] as the measure. Given the ground truth and the parsing result, ARI considers all feasible matching between space labels and computes a weighted average of accuracy of each matching. Both the final and oversegmented results of the proposed method outperform the baselines.\n\n\nParsing into Semantic Elements\n\nBaselines: We compare our method against the top performing algorithms from the KITTI object detection [11] dataset, mBOW [6] and Vote3D [37]. We only compare against the algorithms only using point clouds, not RGB-D.\n\nIn order to evaluate the contribution of each feature, we also compare against: No Local Geometry: We remove the surface normal (n x l , n y l , n z l ), point densities (d l ) and the curvature (\u03ba) from the feature set to evaluate the importance of local geometry, No Global Geometry: We remove the normalized position P x i , P y i , P z i to evaluate the importance of global geometry, No Color: We remove the RGB color values C r l , C g l , C b l to evaluate the importance of color. Experimental Setup: We use k-fold strategy such that each building is a single fold. Hence, the models do not see any part of the test building during training.\n\nQualitative Results: We visualize the semantic elements parsed by our algorithm and the baselines in Fig. 6. Our results are provided in three different granularities: as detection boxes (h), voxelized detection boxes (g) and points (i). Our algorithm outputs the voxelized detection box (Sec. 4), and we find the others by computing the tightest bounding box and point memberships. Unlike us, the baselines employ a segmentation-based approach ((b) and (c)). Fig. 6 shows that the drop in accuracy due to no color or local geometry modelling is minor, suggesting that global features are the most important ones. Moreover, the local geometry and the color modeling are more useful in finelocalizing objects, while the global geometry is particularly crucial for roughly detecting the object or labeling. This is expected since global features can only provide a very rough location. As shown in Fig. 6, although our results almost always capture the context and structure, the method sometimes fails to localize the element precisely resulting in empty areas in the voxel/point level results. This is mostly due to not including detailed features such as edges or HOG. It is also interesting to note that although the localization accuracy changes drastically when using different features, the number of objects is consistently accurate in diverse cases. We hypothesize that this can be attributed to the strong context learnt by the CRF.\n\nQuantitative Results: For the quantitative analysis, we follow the Pascal VOC [8] detection conventions. We consider a detection box with an overlap greater than 0.5 with the ground truth as a true positive and the rest as false positive. Each detection is assigned to at most one ground truth object, and duplicate ones to the same ground truth object are taken as false positives. After computing the detection results, we draw class-level ROC curves (we defer them to [5]) and compute the mean average precision (mAP). Table 4 provides the mAP of each algorithm and shows the relative importance of global geometry, which is consistent with our motivation of understanding the semantic geometry of a building by parsing into spaces. The appearance \n\n\nBaselines\n\nFeature-based Self-Baselines Our Results Figure 6. Qualitative results of parsing spaces into their semantic elements. Notice the heavy contribution of our global geometry features. The baselines employ a segmentation-based approach. features help the least, which is expected since it is harder to generalize due to intra-class variance among different buildings. Similarly, our method's performance on structural elements is high, however on furniture is limited (see Table 3). We attribute this to the generalization of the structural elements among different buildings, something that does not apply to the same extent on furniture. Also, structural elements show a stronger spatial regularity (captured by our global features) compared to furniture.\n\nEmerging Applications: Using the detection results we propose three emerging applications: space statistics, natural illumination modeling and space manipulation. For more details see [5].\n\n\nComparison with Conventional RGB-D\n\nWe compare our method against semantic RGB-D segmentation baselines mainly to evaluate the performance of our 3D parsing method against such techniques (results in Table 5). We also aim to answer the question whether it is better to carry out semantic parsing on RGB-D images or to perform it in 3D on the point cloud and transfer the results onto image domain. To this end, we enriched our dataset with 300 RGB-D images registered on the point cloud in a semi-automatic way and used the image-point Table 5. Evaluation as RGB-D segmentation: Mean intersectionover-union of our method and [7] method RGB-D [7] Ours mIOU 20.9 38.5 cloud correspondences to transfer the 3D semantic annotations. We use the trained models 3 of [7] as the RGB-D baseline and generate the segmentation masks for our images. Similar to transferring annotations, we project the label of each point from our point-level parsing results to the RGB-D images. The results are tabulated in Table 5.\n\n\nConclusion\n\nWe proposed a detection-based semantic parsing method for large-scale building point clouds and argued that such 3D scans pose new challenges and potentials compared to conventional RGB-D images or small point clouds. Our approach can parse a raw point cloud into disjoint spaces and enables extraction of rich, discriminative and low-dimensional features in a common reference system. This helps with parsing spaces into their composing elements. Such a scene understanding can serve as a stepping stone for greater analysis of man-made structures both in breadth and depth and to developing systems, agents, and applications for smart indoor environments.\n\nFigure 2 .\n2Left: Convolution of the devised filter with the histogram signal. The Histogram signal along axis x is the histogram of x coordinates of all points. Right: Space divider detection algorithm. We start with the density histogram signal (a), convolve it with the filter bank (b), and perform max-pooling (c) to identify the space dividers (d).\n\nFigure 3 .\n3Merging the over-segments: We start with a set of over-segments (b) generated from the point cloud (a) and create their neighbor graph (c). Then, we merge nodes (d-e) as explained in Sec. 3.1.2. We update (f) the results given the output of element detection (Sec. 4.2).\n\nP\nPosition: normalized position of the 3D window (3) S Size: normalized size of the 3D window (3) Local Features (per voxel l \u2208 [K \u00d7 K \u00d7 K]) B l Occupancy: 1 if l is occupied, 0 otherwise (1) d l Ratio: ratio of the number of points in the l to the total number of points in the window (surface normal of the points in the l (3) \u03ba Curvature: Surface curvature of points in the l (1) list our features in\n\nFigure 4 .\n4Detection box in the unit cube reference coordinate system and features for a sample object (table). Our features are the detection anchor point, size and features of each sub-box.\n\nFigure 5 .\n5Space Parsing Qualitative Results.\n\nTable 1 .\n1Features that represent each 3D window. The number in the parenthesis shows the dimensionality of the feature component.Global Features \n\n\n\nTable 2 .\n2Evaluation of space parsing (floor plan generation).Ours \nRANSAC 2D Hough \nBuilding final over-segm based [27] based [38] \n\n(1) \n0.94 \n0.59 \n0.29 \n0.27 \n(2) \n0.82 \n0.76 \n0.30 \n0.31 \n(3) \n0.69 \n0.44 \n0.14 \n0.37 \n(4) \n0.66 \n0.42 \n0.15 \n0.3 \n\nmean \n0.77 \n0.55 \n0.2 \n0.31 \n\n\n\na .\naInput j. GT h. Ours, Boxes d. No Local Geom. f. No Color No Global Geom. b. mBOW[5] c. Vote3D[33] floor \nwall \nwindow \nbeam \ndoor \ncolumn \nchair \ntable \nboard \nbookcase \n\ng. Ours, Voxels \ni. Ours, Points \n\nclutter \nsofa \nceiling \n\ne. \n\nTable 3 .\n3Class specific average precision of our method when using different features. ceiling floor wall beam column window door mean table chair sofa bookcase board mean mean Ours(full) 71.61 88.70 72.86 66.67 91.77 25.92 54.11 67.38 46.02 16.15 6.78 54.71 3.91 25.51 49.93 Ours(no glob.) 48.93 83.76 65.25 62.86 83.15 22.55 41.08 57.27 37.57 11.80 4.57 45.49 3.06 20.35 41.87 Ours(no loc.) 50.74 80.48 65.59 68.53 85.08 21.17 45.39 58.73 39.87 11.43 4.91 57.76 3.73 23.78 44.19 Ours(no col.) 48.05 80.95 67.78 68.02 87.41 25.32 44.31 59.73 50.56 11.83 6.32 52.33 4.76 25.30 45.41Table 4. Quantitative evaluation of semantic element detection.Structural Elements \nFurniture \noverall \nmBOW Vote3D \nOurs \n[6] \n[37] no local g. no global g. no color full \n\nmAP 36.11 39.21 \n44.19 \n41.87 \n45.41 49.93 \n\n\nWe used PCA. However, there are other methods dedicated to this task (Manhattan frame estimation) that could be employed off-the-shelf in more complex cases[12,34].\nWe considered semantic classes common in both NYU-RGBD[31] and our dataset see[5] for details.\n\nMatterport 3d models of interior spaces. Matterport 3d models of interior spaces. http:// matterport.com/. Accessed: 2015-06-01.\n\n. Microsoft Kinect, Microsoft kinect. https://www.microsoft.com/ en-us/kinectforwindows/. Accessed: 2015-06-01.\n\nAccurate Localization of 3D Objects from RGB-D Data using Segmentation Hypotheses. Accurate Localization of 3D Objects from RGB-D Data us- ing Segmentation Hypotheses, 2013.\n\nCrowdinside: automatic construction of indoor floorplans. M Alzantot, M Youssef, Proceedings of the 20th International Conference on Advances in Geographic Information Systems. the 20th International Conference on Advances in Geographic Information SystemsACMM. Alzantot and M. Youssef. Crowdinside: automatic con- struction of indoor floorplans. In Proceedings of the 20th International Conference on Advances in Geographic Infor- mation Systems, pages 99-108. ACM, 2012.\n\nSupplementary material for: 3d semantic parsing for large-scale indoor spaces. I Armeni, S Ozan, Z Amir, H Jiang, I Brilakis, M Fischer, S Savarese, I. Armeni, S. Ozan, Z. Amir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese. Supplementary mate- rial for: 3d semantic parsing for large-scale indoor spaces. http://buildingparser.stanford.edu/ images/supp_mat.pdf. Accessed: 2016-04-09.\n\nLaser-based segment classification using a mixture of bag-of-words. J Behley, V Steinhage, A Cremers, Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on. IEEEJ. Behley, V. Steinhage, and A. Cremers. Laser-based seg- ment classification using a mixture of bag-of-words. In In- telligent Robots and Systems (IROS), 2013 IEEE/RSJ Inter- national Conference on, pages 4195-4200. IEEE, 2013.\n\nObject recognition with hierarchical kernel descriptors. L Bo, K Lai, X Ren, D Fox, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEEL. Bo, K. Lai, X. Ren, and D. Fox. Object recognition with hierarchical kernel descriptors. In Computer Vision and Pat- tern Recognition (CVPR), 2011 IEEE Conference on, pages 1729-1736. IEEE, 2011.\n\nThe pascal visual object classes (voc) challenge. M Everingham, L Van Gool, C K Williams, J Winn, A Zisserman, International journal of computer vision. 882M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) chal- lenge. International journal of computer vision, 88(2):303- 338, 2010.\n\nLIBLINEAR: A library for large linear classification. R.-E Fan, K.-W Chang, C.-J Hsieh, X.-R Wang, C.-J Lin, Journal of Machine Learning Research. 9R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871-1874, 2008.\n\nClustering by passing messages between data points. B J Frey, D Dueck, Science. 315B. J. Frey and D. Dueck. Clustering by passing messages between data points. Science, 315:972-976, 2007.\n\nVision meets robotics: The kitti dataset. A Geiger, P Lenz, C Stiller, R Urtasun, The International Journal of Robotics Research. 0278364913491297A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, page 0278364913491297, 2013.\n\nHeilbron. Robust manhattan frame estimation from a single rgbd image. B Ghanem, A Thabet, J Carlos Niebles, F Caba, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). B. Ghanem, A. Thabet, J. Carlos Niebles, and F. Caba Heil- bron. Robust manhattan frame estimation from a single rgb- d image. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.\n\nLearning rich features from rgb-d images for object detection and segmentation. S Gupta, R Girshick, P Arbel\u00e1ez, J Malik, Computer Vision-ECCV 2014. SpringerS. Gupta, R. Girshick, P. Arbel\u00e1ez, and J. Malik. Learning rich features from rgb-d images for object detection and seg- mentation. In Computer Vision-ECCV 2014, pages 345-360. Springer, 2014.\n\nDense 3d semantic mapping of indoor scenes from rgb-d images. A Hermans, G Floros, B Leibe, Robotics and Automation (ICRA). IEEEA. Hermans, G. Floros, and B. Leibe. Dense 3d semantic mapping of indoor scenes from rgb-d images. In Robotics and Automation (ICRA), 2014 IEEE International Confer- ence on, pages 2631-2638. IEEE, 2014.\n\nProbabilistic graphical models: principles and techniques. D Koller, N Friedman, MIT pressD. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.\n\nAnticipating human activities using object affordances for reactive robotic response. H Koppula, A Saxena, H. Koppula and A. Saxena. Anticipating human activities using object affordances for reactive robotic response. 2013.\n\nLearning human activities and object affordances from rgb-d videos. H S Koppula, R Gupta, A Saxena, The International Journal of Robotics Research. 328H. S. Koppula, R. Gupta, and A. Saxena. Learning human activities and object affordances from rgb-d videos. The In- ternational Journal of Robotics Research, 32(8):951-970, 2013.\n\nRent3d: Floor-plan priors for monocular layout estimation. C Liu, A Schwing, K Kundu, R Urtasun, S Fidler, CVPR. C. Liu, A. Schwing, K. Kundu, R. Urtasun, and S. Fidler. Rent3d: Floor-plan priors for monocular layout estimation. In CVPR, 2015.\n\nEnsemble of exemplar-svms for object detection and beyond. T Malisiewicz, A Gupta, A Efros, Computer Vision (ICCV), 2011 IEEE International Conference on. IEEET. Malisiewicz, A. Gupta, A. Efros, et al. Ensemble of exemplar-svms for object detection and beyond. In Com- puter Vision (ICCV), 2011 IEEE International Conference on, pages 89-96. IEEE, 2011.\n\nAutomatic room detection and reconstruction in cluttered indoor environments with complex room layouts. C Mura, O Mattausch, A J Villanueva, E Gobbetti, R Pajarola, Computers & Graphics. 44C. Mura, O. Mattausch, A. J. Villanueva, E. Gobbetti, and R. Pajarola. Automatic room detection and reconstruction in cluttered indoor environments with complex room layouts. Computers & Graphics, 44:20-32, 2014.\n\nA search-classify approach for cluttered indoor scene understanding. L Nan, K Xie, A Sharf, ACM Transactions on Graphics (TOG). 316137L. Nan, K. Xie, and A. Sharf. A search-classify approach for cluttered indoor scene understanding. ACM Transactions on Graphics (TOG), 31(6):137, 2012.\n\nTowards semantic maps for mobile robots. A N\u00fcchter, J Hertzberg, Robotics and Autonomous Systems. 5611A. N\u00fcchter and J. Hertzberg. Towards semantic maps for mo- bile robots. Robotics and Autonomous Systems, 56(11):915- 926, 2008.\n\nAutomatic generation of structural building descriptions from 3d point cloud scans. S Ochmann, R Vock, R Wessel, M Tamke, R Klein, GRAPP 2014 -International Conference on Computer Graphics Theory and Applications. SCITEPRESSS. Ochmann, R. Vock, R. Wessel, M. Tamke, and R. Klein. Automatic generation of structural building descriptions from 3d point cloud scans. In GRAPP 2014 -International Conference on Computer Graphics Theory and Applications. SCITEPRESS, Jan. 2014.\n\nVoxel cloud connectivity segmentation-supervoxels for point clouds. J Papon, A Abramov, M Schoeler, F Worgotter, Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on. IEEEJ. Papon, A. Abramov, M. Schoeler, and F. Worgot- ter. Voxel cloud connectivity segmentation-supervoxels for point clouds. In Computer Vision and Pattern Recogni- tion (CVPR), 2013 IEEE Conference on, pages 2027-2034. IEEE, 2013.\n\nObjective criteria for the evaluation of clustering methods. W M Rand, Journal of the American Statistical association. 66336W. M. Rand. Objective criteria for the evaluation of cluster- ing methods. Journal of the American Statistical association, 66(336):846-850, 1971.\n\nRgb-(d) scene labeling: Features and algorithms. X Ren, L Bo, D Fox, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEEX. Ren, L. Bo, and D. Fox. Rgb-(d) scene labeling: Features and algorithms. In Computer Vision and Pattern Recogni- tion (CVPR), 2012 IEEE Conference on, pages 2759-2766. IEEE, 2012.\n\nEfficient ransac for point-cloud shape detection. R Schnabel, R Wahl, R Klein, Computer graphics forum. Wiley Online Library26R. Schnabel, R. Wahl, and R. Klein. Efficient ransac for point-cloud shape detection. In Computer graphics forum, volume 26, pages 214-226. Wiley Online Library, 2007.\n\nrcrf: Recursive belief estimation over crfs in rgb-d activity videos. O Sener, A Saxena, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsRome, ItalyO. Sener and A. Saxena. rcrf: Recursive belief estima- tion over crfs in rgb-d activity videos. In Proceedings of Robotics: Science and Systems, Rome, Italy, July 2015.\n\nAn interactive approach to semantic modeling of indoor scenes with an rgbd camera. T Shao, W Xu, K Zhou, J Wang, D Li, B Guo, ACM Transactions on Graphics (TOG). 316136T. Shao, W. Xu, K. Zhou, J. Wang, D. Li, and B. Guo. An interactive approach to semantic modeling of indoor scenes with an rgbd camera. ACM Transactions on Graphics (TOG), 31(6):136, 2012.\n\nIndoor scene segmentation using a structured light sensor. N Silberman, R Fergus, Computer Vision Workshops (ICCV Workshops. IEEE2011 IEEE International Conference onN. Silberman and R. Fergus. Indoor scene segmentation us- ing a structured light sensor. In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 601-608. IEEE, 2011.\n\nIndoor segmentation and support inference from rgbd images. N Silberman, D Hoiem, P Kohli, R Fergus, Computer Vision-ECCV 2012. SpringerN. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor seg- mentation and support inference from rgbd images. In Com- puter Vision-ECCV 2012, pages 746-760. Springer, 2012.\n\nSliding shapes for 3d object detection in rgb-d images. S Song, J Xiao, European Conference on Computer Vision. 26S. Song and J. Xiao. Sliding shapes for 3d object detection in rgb-d images. In European Conference on Computer Vision, volume 2, page 6, 2014.\n\nWhy are most buildings rectangular?. P Steadman, Architectural Research Quarterly. 10P. Steadman. Why are most buildings rectangular? Architec- tural Research Quarterly, 10(02):119-130, 2006.\n\nA mixture of manhattan frames: Beyond the manhattan world. J Straub, G Rosman, O Freifeld, J J Leonard, J W Fisher, Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEEJ. Straub, G. Rosman, O. Freifeld, J. J. Leonard, and J. W. Fisher. A mixture of manhattan frames: Beyond the man- hattan world. In Computer Vision and Pattern Recogni- tion (CVPR), 2014 IEEE Conference on, pages 3770-3777. IEEE, 2014.\n\nSupport vector machine learning for interdependent and structured output spaces. I Tsochantaridis, T Hofmann, T Joachims, Y Altun, Proceedings of the twenty-first international conference on Machine learning. the twenty-first international conference on Machine learningACM104I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Al- tun. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international conference on Machine learning, page 104. ACM, 2004.\n\nFloor plan generation and room labeling of indoor environments from laser range data. E Turner, A Zakhor, E. Turner and A. Zakhor. Floor plan generation and room labeling of indoor environments from laser range data, 2014.\n\nVoting for voting in online point cloud object detection. D Z Wang, I Posner, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsRome, ItalyD. Z. Wang and I. Posner. Voting for voting in online point cloud object detection. In Proceedings of Robotics: Science and Systems, Rome, Italy, July 2015.\n\nReconstructing the worlds museums. J Xiao, Y Furukawa, Computer Vision-ECCV 2012. J. Xiao and Y. Furukawa. Reconstructing the worlds mu- seums. In Computer Vision-ECCV 2012, pages 668-681.\n\n. Springer, Springer, 2012.\n\nSun3d: A database of big spaces reconstructed using sfm and object labels. J Xiao, A Owens, A Torralba, Computer Vision (ICCV), 2013 IEEE International Conference on. IEEEJ. Xiao, A. Owens, and A. Torralba. Sun3d: A database of big spaces reconstructed using sfm and object labels. In Computer Vision (ICCV), 2013 IEEE International Confer- ence on, pages 1625-1632. IEEE, 2013.\n\nAutomatic creation of semantically rich 3d building models from laser scanner data. X Xiong, A Adan, B Akinci, D Huber, Automation in Construction. 31X. Xiong, A. Adan, B. Akinci, and D. Huber. Automatic creation of semantically rich 3d building models from laser scanner data. Automation in Construction, 31:325-337, 2013.\n\nPanocontext: A whole-room 3d context model for panoramic scene understanding. Y Zhang, S Song, P Tan, J Xiao, Computer Vision-ECCV 2014. SpringerY. Zhang, S. Song, P. Tan, and J. Xiao. Panocontext: A whole-room 3d context model for panoramic scene under- standing. In Computer Vision-ECCV 2014, pages 668-686. Springer, 2014.\n", "annotations": {"author": "[{\"end\":206,\"start\":173},{\"end\":261,\"start\":207},{\"end\":297,\"start\":262},{\"end\":332,\"start\":298},{\"end\":376,\"start\":333},{\"end\":414,\"start\":377},{\"end\":453,\"start\":415}]", "publisher": null, "author_last_name": "[{\"end\":183,\"start\":177},{\"end\":217,\"start\":212},{\"end\":274,\"start\":269},{\"end\":309,\"start\":304},{\"end\":349,\"start\":341},{\"end\":391,\"start\":384},{\"end\":430,\"start\":422}]", "author_first_name": "[{\"end\":176,\"start\":173},{\"end\":211,\"start\":207},{\"end\":266,\"start\":262},{\"end\":268,\"start\":267},{\"end\":303,\"start\":298},{\"end\":340,\"start\":333},{\"end\":383,\"start\":377},{\"end\":421,\"start\":415}]", "author_affiliation": "[{\"end\":205,\"start\":185},{\"end\":239,\"start\":219},{\"end\":260,\"start\":241},{\"end\":296,\"start\":276},{\"end\":331,\"start\":311},{\"end\":375,\"start\":351},{\"end\":413,\"start\":393},{\"end\":452,\"start\":432}]", "title": "[{\"end\":170,\"start\":1},{\"end\":623,\"start\":454}]", "venue": null, "abstract": "[{\"end\":2119,\"start\":625}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2278,\"start\":2275},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2400,\"start\":2396},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2403,\"start\":2400},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2405,\"start\":2403},{\"end\":4894,\"start\":4890},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6153,\"start\":6152},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7441,\"start\":7437},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7444,\"start\":7441},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7549,\"start\":7545},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7776,\"start\":7772},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7779,\"start\":7776},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7887,\"start\":7883},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7990,\"start\":7986},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8361,\"start\":8357},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8364,\"start\":8361},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8367,\"start\":8364},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8370,\"start\":8367},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8372,\"start\":8370},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8417,\"start\":8413},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8632,\"start\":8629},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8778,\"start\":8774},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8966,\"start\":8962},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9100,\"start\":9096},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9166,\"start\":9162},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9266,\"start\":9262},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9521,\"start\":9517},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10725,\"start\":10721},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12916,\"start\":12912},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14178,\"start\":14175},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17033,\"start\":17030},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20549,\"start\":20545},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21123,\"start\":21120},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23001,\"start\":22997},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23521,\"start\":23517},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23593,\"start\":23589},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23596,\"start\":23593},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23599,\"start\":23596},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23653,\"start\":23650},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25266,\"start\":25263},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25694,\"start\":25690},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25697,\"start\":25694},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26645,\"start\":26641},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26648,\"start\":26645},{\"end\":27011,\"start\":27003},{\"end\":27035,\"start\":27020},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27050,\"start\":27046},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27074,\"start\":27070},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27239,\"start\":27235},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27653,\"start\":27649},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27671,\"start\":27668},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27687,\"start\":27683},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29939,\"start\":29936},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30332,\"start\":30329},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31566,\"start\":31563},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32198,\"start\":32195},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32215,\"start\":32212},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32333,\"start\":32330},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35057,\"start\":35054},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35071,\"start\":35067},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36174,\"start\":36170},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":36177,\"start\":36174},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36237,\"start\":36233},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36260,\"start\":36257}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33602,\"start\":33248},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33886,\"start\":33603},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34291,\"start\":33887},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34485,\"start\":34292},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34533,\"start\":34486},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34684,\"start\":34534},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34967,\"start\":34685},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35208,\"start\":34968},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":36013,\"start\":35209}]", "paragraph": "[{\"end\":2877,\"start\":2135},{\"end\":3037,\"start\":2879},{\"end\":4240,\"start\":3039},{\"end\":4895,\"start\":4242},{\"end\":5479,\"start\":4897},{\"end\":5819,\"start\":5481},{\"end\":6032,\"start\":5821},{\"end\":6720,\"start\":6034},{\"end\":7308,\"start\":6737},{\"end\":8586,\"start\":7310},{\"end\":9696,\"start\":8588},{\"end\":10031,\"start\":9741},{\"end\":10519,\"start\":10071},{\"end\":10924,\"start\":10521},{\"end\":12033,\"start\":10926},{\"end\":13214,\"start\":12073},{\"end\":13450,\"start\":13216},{\"end\":13572,\"start\":13452},{\"end\":13607,\"start\":13574},{\"end\":13758,\"start\":13691},{\"end\":14315,\"start\":13760},{\"end\":15137,\"start\":14317},{\"end\":16165,\"start\":15149},{\"end\":16337,\"start\":16167},{\"end\":16902,\"start\":16382},{\"end\":17047,\"start\":16904},{\"end\":17620,\"start\":17049},{\"end\":17766,\"start\":17662},{\"end\":18598,\"start\":17768},{\"end\":18893,\"start\":18600},{\"end\":19615,\"start\":18895},{\"end\":20125,\"start\":19617},{\"end\":21216,\"start\":20127},{\"end\":21804,\"start\":21218},{\"end\":22270,\"start\":21851},{\"end\":22791,\"start\":22272},{\"end\":22964,\"start\":22846},{\"end\":23082,\"start\":22966},{\"end\":23257,\"start\":23151},{\"end\":23654,\"start\":23391},{\"end\":24507,\"start\":23702},{\"end\":24617,\"start\":24523},{\"end\":25372,\"start\":24629},{\"end\":25698,\"start\":25374},{\"end\":26478,\"start\":25731},{\"end\":27511,\"start\":26480},{\"end\":27763,\"start\":27546},{\"end\":28414,\"start\":27765},{\"end\":29856,\"start\":28416},{\"end\":30609,\"start\":29858},{\"end\":31377,\"start\":30623},{\"end\":31567,\"start\":31379},{\"end\":32575,\"start\":31606},{\"end\":33247,\"start\":32590}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13690,\"start\":13608},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22845,\"start\":22792},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23150,\"start\":23083},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23390,\"start\":23258}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20095,\"start\":20088},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22251,\"start\":22244},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26921,\"start\":26914},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27106,\"start\":27099},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30387,\"start\":30380},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31100,\"start\":31093},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31777,\"start\":31770},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32113,\"start\":32106},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32574,\"start\":32567}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2133,\"start\":2121},{\"attributes\":{\"n\":\"2.\"},\"end\":6735,\"start\":6723},{\"attributes\":{\"n\":\"3.\"},\"end\":9739,\"start\":9699},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10069,\"start\":10034},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":12071,\"start\":12036},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":15147,\"start\":15140},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16380,\"start\":16340},{\"attributes\":{\"n\":\"4.\"},\"end\":17660,\"start\":17623},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21849,\"start\":21807},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23700,\"start\":23657},{\"attributes\":{\"n\":\"5.\"},\"end\":24521,\"start\":24510},{\"attributes\":{\"n\":\"5.1.\"},\"end\":24627,\"start\":24620},{\"attributes\":{\"n\":\"5.2.\"},\"end\":25729,\"start\":25701},{\"attributes\":{\"n\":\"5.3.\"},\"end\":27544,\"start\":27514},{\"end\":30621,\"start\":30612},{\"attributes\":{\"n\":\"5.4.\"},\"end\":31604,\"start\":31570},{\"attributes\":{\"n\":\"6.\"},\"end\":32588,\"start\":32578},{\"end\":33259,\"start\":33249},{\"end\":33614,\"start\":33604},{\"end\":33889,\"start\":33888},{\"end\":34303,\"start\":34293},{\"end\":34497,\"start\":34487},{\"end\":34544,\"start\":34535},{\"end\":34695,\"start\":34686},{\"end\":34972,\"start\":34969},{\"end\":35219,\"start\":35210}]", "table": "[{\"end\":34684,\"start\":34666},{\"end\":34967,\"start\":34749},{\"end\":35208,\"start\":35072},{\"end\":36013,\"start\":35857}]", "figure_caption": "[{\"end\":33602,\"start\":33261},{\"end\":33886,\"start\":33616},{\"end\":34291,\"start\":33890},{\"end\":34485,\"start\":34305},{\"end\":34533,\"start\":34499},{\"end\":34666,\"start\":34546},{\"end\":34749,\"start\":34697},{\"end\":35072,\"start\":34974},{\"end\":35857,\"start\":35221}]", "figure_ref": "[{\"end\":2738,\"start\":2726},{\"end\":4667,\"start\":4661},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":6301,\"start\":6289},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11309,\"start\":11303},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11512,\"start\":11506},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12295,\"start\":12280},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12333,\"start\":12327},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12447,\"start\":12441},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13571,\"start\":13555},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13845,\"start\":13829},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14669,\"start\":14663},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15396,\"start\":15390},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15761,\"start\":15751},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15850,\"start\":15839},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16163,\"start\":16153},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17318,\"start\":17312},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20124,\"start\":20118},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24505,\"start\":24495},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25371,\"start\":25365},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25854,\"start\":25848},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26909,\"start\":26903},{\"end\":28523,\"start\":28517},{\"end\":28882,\"start\":28876},{\"end\":29318,\"start\":29312},{\"end\":30672,\"start\":30664}]", "bib_author_first_name": "[{\"end\":36416,\"start\":36407},{\"end\":36752,\"start\":36751},{\"end\":36764,\"start\":36763},{\"end\":37247,\"start\":37246},{\"end\":37257,\"start\":37256},{\"end\":37265,\"start\":37264},{\"end\":37273,\"start\":37272},{\"end\":37282,\"start\":37281},{\"end\":37294,\"start\":37293},{\"end\":37305,\"start\":37304},{\"end\":37627,\"start\":37626},{\"end\":37637,\"start\":37636},{\"end\":37650,\"start\":37649},{\"end\":38034,\"start\":38033},{\"end\":38040,\"start\":38039},{\"end\":38047,\"start\":38046},{\"end\":38054,\"start\":38053},{\"end\":38388,\"start\":38387},{\"end\":38402,\"start\":38401},{\"end\":38414,\"start\":38413},{\"end\":38416,\"start\":38415},{\"end\":38428,\"start\":38427},{\"end\":38436,\"start\":38435},{\"end\":38739,\"start\":38735},{\"end\":38749,\"start\":38745},{\"end\":38761,\"start\":38757},{\"end\":38773,\"start\":38769},{\"end\":38784,\"start\":38780},{\"end\":39058,\"start\":39057},{\"end\":39060,\"start\":39059},{\"end\":39068,\"start\":39067},{\"end\":39237,\"start\":39236},{\"end\":39247,\"start\":39246},{\"end\":39255,\"start\":39254},{\"end\":39266,\"start\":39265},{\"end\":39579,\"start\":39578},{\"end\":39589,\"start\":39588},{\"end\":39599,\"start\":39598},{\"end\":39606,\"start\":39600},{\"end\":39617,\"start\":39616},{\"end\":39989,\"start\":39988},{\"end\":39998,\"start\":39997},{\"end\":40010,\"start\":40009},{\"end\":40022,\"start\":40021},{\"end\":40322,\"start\":40321},{\"end\":40333,\"start\":40332},{\"end\":40343,\"start\":40342},{\"end\":40652,\"start\":40651},{\"end\":40662,\"start\":40661},{\"end\":40873,\"start\":40872},{\"end\":40884,\"start\":40883},{\"end\":41081,\"start\":41080},{\"end\":41083,\"start\":41082},{\"end\":41094,\"start\":41093},{\"end\":41103,\"start\":41102},{\"end\":41403,\"start\":41402},{\"end\":41410,\"start\":41409},{\"end\":41421,\"start\":41420},{\"end\":41430,\"start\":41429},{\"end\":41441,\"start\":41440},{\"end\":41648,\"start\":41647},{\"end\":41663,\"start\":41662},{\"end\":41672,\"start\":41671},{\"end\":42048,\"start\":42047},{\"end\":42056,\"start\":42055},{\"end\":42069,\"start\":42068},{\"end\":42071,\"start\":42070},{\"end\":42085,\"start\":42084},{\"end\":42097,\"start\":42096},{\"end\":42416,\"start\":42415},{\"end\":42423,\"start\":42422},{\"end\":42430,\"start\":42429},{\"end\":42675,\"start\":42674},{\"end\":42686,\"start\":42685},{\"end\":42949,\"start\":42948},{\"end\":42960,\"start\":42959},{\"end\":42968,\"start\":42967},{\"end\":42978,\"start\":42977},{\"end\":42987,\"start\":42986},{\"end\":43407,\"start\":43406},{\"end\":43416,\"start\":43415},{\"end\":43427,\"start\":43426},{\"end\":43439,\"start\":43438},{\"end\":43821,\"start\":43820},{\"end\":43823,\"start\":43822},{\"end\":44082,\"start\":44081},{\"end\":44089,\"start\":44088},{\"end\":44095,\"start\":44094},{\"end\":44413,\"start\":44412},{\"end\":44425,\"start\":44424},{\"end\":44433,\"start\":44432},{\"end\":44728,\"start\":44727},{\"end\":44737,\"start\":44736},{\"end\":45086,\"start\":45085},{\"end\":45094,\"start\":45093},{\"end\":45100,\"start\":45099},{\"end\":45108,\"start\":45107},{\"end\":45116,\"start\":45115},{\"end\":45122,\"start\":45121},{\"end\":45420,\"start\":45419},{\"end\":45433,\"start\":45432},{\"end\":45790,\"start\":45789},{\"end\":45803,\"start\":45802},{\"end\":45812,\"start\":45811},{\"end\":45821,\"start\":45820},{\"end\":46097,\"start\":46096},{\"end\":46105,\"start\":46104},{\"end\":46337,\"start\":46336},{\"end\":46552,\"start\":46551},{\"end\":46562,\"start\":46561},{\"end\":46572,\"start\":46571},{\"end\":46584,\"start\":46583},{\"end\":46586,\"start\":46585},{\"end\":46597,\"start\":46596},{\"end\":46599,\"start\":46598},{\"end\":47004,\"start\":47003},{\"end\":47022,\"start\":47021},{\"end\":47033,\"start\":47032},{\"end\":47045,\"start\":47044},{\"end\":47529,\"start\":47528},{\"end\":47539,\"start\":47538},{\"end\":47725,\"start\":47724},{\"end\":47727,\"start\":47726},{\"end\":47735,\"start\":47734},{\"end\":48024,\"start\":48023},{\"end\":48032,\"start\":48031},{\"end\":48283,\"start\":48282},{\"end\":48291,\"start\":48290},{\"end\":48300,\"start\":48299},{\"end\":48672,\"start\":48671},{\"end\":48681,\"start\":48680},{\"end\":48689,\"start\":48688},{\"end\":48699,\"start\":48698},{\"end\":48991,\"start\":48990},{\"end\":49000,\"start\":48999},{\"end\":49008,\"start\":49007},{\"end\":49015,\"start\":49014}]", "bib_author_last_name": "[{\"end\":36423,\"start\":36417},{\"end\":36761,\"start\":36753},{\"end\":36772,\"start\":36765},{\"end\":37254,\"start\":37248},{\"end\":37262,\"start\":37258},{\"end\":37270,\"start\":37266},{\"end\":37279,\"start\":37274},{\"end\":37291,\"start\":37283},{\"end\":37302,\"start\":37295},{\"end\":37314,\"start\":37306},{\"end\":37634,\"start\":37628},{\"end\":37647,\"start\":37638},{\"end\":37658,\"start\":37651},{\"end\":38037,\"start\":38035},{\"end\":38044,\"start\":38041},{\"end\":38051,\"start\":38048},{\"end\":38058,\"start\":38055},{\"end\":38399,\"start\":38389},{\"end\":38411,\"start\":38403},{\"end\":38425,\"start\":38417},{\"end\":38433,\"start\":38429},{\"end\":38446,\"start\":38437},{\"end\":38743,\"start\":38740},{\"end\":38755,\"start\":38750},{\"end\":38767,\"start\":38762},{\"end\":38778,\"start\":38774},{\"end\":38788,\"start\":38785},{\"end\":39065,\"start\":39061},{\"end\":39074,\"start\":39069},{\"end\":39244,\"start\":39238},{\"end\":39252,\"start\":39248},{\"end\":39263,\"start\":39256},{\"end\":39274,\"start\":39267},{\"end\":39586,\"start\":39580},{\"end\":39596,\"start\":39590},{\"end\":39614,\"start\":39607},{\"end\":39622,\"start\":39618},{\"end\":39995,\"start\":39990},{\"end\":40007,\"start\":39999},{\"end\":40019,\"start\":40011},{\"end\":40028,\"start\":40023},{\"end\":40330,\"start\":40323},{\"end\":40340,\"start\":40334},{\"end\":40349,\"start\":40344},{\"end\":40659,\"start\":40653},{\"end\":40671,\"start\":40663},{\"end\":40881,\"start\":40874},{\"end\":40891,\"start\":40885},{\"end\":41091,\"start\":41084},{\"end\":41100,\"start\":41095},{\"end\":41110,\"start\":41104},{\"end\":41407,\"start\":41404},{\"end\":41418,\"start\":41411},{\"end\":41427,\"start\":41422},{\"end\":41438,\"start\":41431},{\"end\":41448,\"start\":41442},{\"end\":41660,\"start\":41649},{\"end\":41669,\"start\":41664},{\"end\":41678,\"start\":41673},{\"end\":42053,\"start\":42049},{\"end\":42066,\"start\":42057},{\"end\":42082,\"start\":42072},{\"end\":42094,\"start\":42086},{\"end\":42106,\"start\":42098},{\"end\":42420,\"start\":42417},{\"end\":42427,\"start\":42424},{\"end\":42436,\"start\":42431},{\"end\":42683,\"start\":42676},{\"end\":42696,\"start\":42687},{\"end\":42957,\"start\":42950},{\"end\":42965,\"start\":42961},{\"end\":42975,\"start\":42969},{\"end\":42984,\"start\":42979},{\"end\":42993,\"start\":42988},{\"end\":43413,\"start\":43408},{\"end\":43424,\"start\":43417},{\"end\":43436,\"start\":43428},{\"end\":43449,\"start\":43440},{\"end\":43828,\"start\":43824},{\"end\":44086,\"start\":44083},{\"end\":44092,\"start\":44090},{\"end\":44099,\"start\":44096},{\"end\":44422,\"start\":44414},{\"end\":44430,\"start\":44426},{\"end\":44439,\"start\":44434},{\"end\":44734,\"start\":44729},{\"end\":44744,\"start\":44738},{\"end\":45091,\"start\":45087},{\"end\":45097,\"start\":45095},{\"end\":45105,\"start\":45101},{\"end\":45113,\"start\":45109},{\"end\":45119,\"start\":45117},{\"end\":45126,\"start\":45123},{\"end\":45430,\"start\":45421},{\"end\":45440,\"start\":45434},{\"end\":45800,\"start\":45791},{\"end\":45809,\"start\":45804},{\"end\":45818,\"start\":45813},{\"end\":45828,\"start\":45822},{\"end\":46102,\"start\":46098},{\"end\":46110,\"start\":46106},{\"end\":46346,\"start\":46338},{\"end\":46559,\"start\":46553},{\"end\":46569,\"start\":46563},{\"end\":46581,\"start\":46573},{\"end\":46594,\"start\":46587},{\"end\":46606,\"start\":46600},{\"end\":47019,\"start\":47005},{\"end\":47030,\"start\":47023},{\"end\":47042,\"start\":47034},{\"end\":47051,\"start\":47046},{\"end\":47536,\"start\":47530},{\"end\":47546,\"start\":47540},{\"end\":47732,\"start\":47728},{\"end\":47742,\"start\":47736},{\"end\":48029,\"start\":48025},{\"end\":48041,\"start\":48033},{\"end\":48188,\"start\":48180},{\"end\":48288,\"start\":48284},{\"end\":48297,\"start\":48292},{\"end\":48309,\"start\":48301},{\"end\":48678,\"start\":48673},{\"end\":48686,\"start\":48682},{\"end\":48696,\"start\":48690},{\"end\":48705,\"start\":48700},{\"end\":48997,\"start\":48992},{\"end\":49005,\"start\":49001},{\"end\":49012,\"start\":49009},{\"end\":49020,\"start\":49016}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":36403,\"start\":36275},{\"attributes\":{\"id\":\"b1\"},\"end\":36516,\"start\":36405},{\"attributes\":{\"id\":\"b2\"},\"end\":36691,\"start\":36518},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13975299},\"end\":37165,\"start\":36693},{\"attributes\":{\"id\":\"b4\"},\"end\":37556,\"start\":37167},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5730410},\"end\":37974,\"start\":37558},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5274356},\"end\":38335,\"start\":37976},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4246903},\"end\":38679,\"start\":38337},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3116168},\"end\":39003,\"start\":38681},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6502291},\"end\":39192,\"start\":39005},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9455111},\"end\":39506,\"start\":39194},{\"attributes\":{\"id\":\"b11\"},\"end\":39906,\"start\":39508},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13259596},\"end\":40257,\"start\":39908},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":14281310},\"end\":40590,\"start\":40259},{\"attributes\":{\"id\":\"b14\"},\"end\":40784,\"start\":40592},{\"attributes\":{\"id\":\"b15\"},\"end\":41010,\"start\":40786},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17004045},\"end\":41341,\"start\":41012},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6253342},\"end\":41586,\"start\":41343},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14448882},\"end\":41941,\"start\":41588},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8829852},\"end\":42344,\"start\":41943},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13926850},\"end\":42631,\"start\":42346},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15284132},\"end\":42862,\"start\":42633},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10899247},\"end\":43336,\"start\":42864},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17476747},\"end\":43757,\"start\":43338},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":197457299},\"end\":44030,\"start\":43759},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15174950},\"end\":44360,\"start\":44032},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12349413},\"end\":44655,\"start\":44362},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8766488},\"end\":45000,\"start\":44657},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13918176},\"end\":45358,\"start\":45002},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13993169},\"end\":45727,\"start\":45360},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":545361},\"end\":46038,\"start\":45729},{\"attributes\":{\"id\":\"b31\"},\"end\":46297,\"start\":46040},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":110439493},\"end\":46490,\"start\":46299},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15686392},\"end\":46920,\"start\":46492},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":564746},\"end\":47440,\"start\":46922},{\"attributes\":{\"id\":\"b35\"},\"end\":47664,\"start\":47442},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":15568286},\"end\":47986,\"start\":47666},{\"attributes\":{\"id\":\"b37\"},\"end\":48176,\"start\":47988},{\"attributes\":{\"id\":\"b38\"},\"end\":48205,\"start\":48178},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6033252},\"end\":48585,\"start\":48207},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14832517},\"end\":48910,\"start\":48587},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":15644143},\"end\":49237,\"start\":48912}]", "bib_title": "[{\"end\":36749,\"start\":36693},{\"end\":37624,\"start\":37558},{\"end\":38031,\"start\":37976},{\"end\":38385,\"start\":38337},{\"end\":38733,\"start\":38681},{\"end\":39055,\"start\":39005},{\"end\":39234,\"start\":39194},{\"end\":39576,\"start\":39508},{\"end\":39986,\"start\":39908},{\"end\":40319,\"start\":40259},{\"end\":41078,\"start\":41012},{\"end\":41400,\"start\":41343},{\"end\":41645,\"start\":41588},{\"end\":42045,\"start\":41943},{\"end\":42413,\"start\":42346},{\"end\":42672,\"start\":42633},{\"end\":42946,\"start\":42864},{\"end\":43404,\"start\":43338},{\"end\":43818,\"start\":43759},{\"end\":44079,\"start\":44032},{\"end\":44410,\"start\":44362},{\"end\":44725,\"start\":44657},{\"end\":45083,\"start\":45002},{\"end\":45417,\"start\":45360},{\"end\":45787,\"start\":45729},{\"end\":46094,\"start\":46040},{\"end\":46334,\"start\":46299},{\"end\":46549,\"start\":46492},{\"end\":47001,\"start\":46922},{\"end\":47722,\"start\":47666},{\"end\":48021,\"start\":47988},{\"end\":48280,\"start\":48207},{\"end\":48669,\"start\":48587},{\"end\":48988,\"start\":48912}]", "bib_author": "[{\"end\":36425,\"start\":36407},{\"end\":36763,\"start\":36751},{\"end\":36774,\"start\":36763},{\"end\":37256,\"start\":37246},{\"end\":37264,\"start\":37256},{\"end\":37272,\"start\":37264},{\"end\":37281,\"start\":37272},{\"end\":37293,\"start\":37281},{\"end\":37304,\"start\":37293},{\"end\":37316,\"start\":37304},{\"end\":37636,\"start\":37626},{\"end\":37649,\"start\":37636},{\"end\":37660,\"start\":37649},{\"end\":38039,\"start\":38033},{\"end\":38046,\"start\":38039},{\"end\":38053,\"start\":38046},{\"end\":38060,\"start\":38053},{\"end\":38401,\"start\":38387},{\"end\":38413,\"start\":38401},{\"end\":38427,\"start\":38413},{\"end\":38435,\"start\":38427},{\"end\":38448,\"start\":38435},{\"end\":38745,\"start\":38735},{\"end\":38757,\"start\":38745},{\"end\":38769,\"start\":38757},{\"end\":38780,\"start\":38769},{\"end\":38790,\"start\":38780},{\"end\":39067,\"start\":39057},{\"end\":39076,\"start\":39067},{\"end\":39246,\"start\":39236},{\"end\":39254,\"start\":39246},{\"end\":39265,\"start\":39254},{\"end\":39276,\"start\":39265},{\"end\":39588,\"start\":39578},{\"end\":39598,\"start\":39588},{\"end\":39616,\"start\":39598},{\"end\":39624,\"start\":39616},{\"end\":39997,\"start\":39988},{\"end\":40009,\"start\":39997},{\"end\":40021,\"start\":40009},{\"end\":40030,\"start\":40021},{\"end\":40332,\"start\":40321},{\"end\":40342,\"start\":40332},{\"end\":40351,\"start\":40342},{\"end\":40661,\"start\":40651},{\"end\":40673,\"start\":40661},{\"end\":40883,\"start\":40872},{\"end\":40893,\"start\":40883},{\"end\":41093,\"start\":41080},{\"end\":41102,\"start\":41093},{\"end\":41112,\"start\":41102},{\"end\":41409,\"start\":41402},{\"end\":41420,\"start\":41409},{\"end\":41429,\"start\":41420},{\"end\":41440,\"start\":41429},{\"end\":41450,\"start\":41440},{\"end\":41662,\"start\":41647},{\"end\":41671,\"start\":41662},{\"end\":41680,\"start\":41671},{\"end\":42055,\"start\":42047},{\"end\":42068,\"start\":42055},{\"end\":42084,\"start\":42068},{\"end\":42096,\"start\":42084},{\"end\":42108,\"start\":42096},{\"end\":42422,\"start\":42415},{\"end\":42429,\"start\":42422},{\"end\":42438,\"start\":42429},{\"end\":42685,\"start\":42674},{\"end\":42698,\"start\":42685},{\"end\":42959,\"start\":42948},{\"end\":42967,\"start\":42959},{\"end\":42977,\"start\":42967},{\"end\":42986,\"start\":42977},{\"end\":42995,\"start\":42986},{\"end\":43415,\"start\":43406},{\"end\":43426,\"start\":43415},{\"end\":43438,\"start\":43426},{\"end\":43451,\"start\":43438},{\"end\":43830,\"start\":43820},{\"end\":44088,\"start\":44081},{\"end\":44094,\"start\":44088},{\"end\":44101,\"start\":44094},{\"end\":44424,\"start\":44412},{\"end\":44432,\"start\":44424},{\"end\":44441,\"start\":44432},{\"end\":44736,\"start\":44727},{\"end\":44746,\"start\":44736},{\"end\":45093,\"start\":45085},{\"end\":45099,\"start\":45093},{\"end\":45107,\"start\":45099},{\"end\":45115,\"start\":45107},{\"end\":45121,\"start\":45115},{\"end\":45128,\"start\":45121},{\"end\":45432,\"start\":45419},{\"end\":45442,\"start\":45432},{\"end\":45802,\"start\":45789},{\"end\":45811,\"start\":45802},{\"end\":45820,\"start\":45811},{\"end\":45830,\"start\":45820},{\"end\":46104,\"start\":46096},{\"end\":46112,\"start\":46104},{\"end\":46348,\"start\":46336},{\"end\":46561,\"start\":46551},{\"end\":46571,\"start\":46561},{\"end\":46583,\"start\":46571},{\"end\":46596,\"start\":46583},{\"end\":46608,\"start\":46596},{\"end\":47021,\"start\":47003},{\"end\":47032,\"start\":47021},{\"end\":47044,\"start\":47032},{\"end\":47053,\"start\":47044},{\"end\":47538,\"start\":47528},{\"end\":47548,\"start\":47538},{\"end\":47734,\"start\":47724},{\"end\":47744,\"start\":47734},{\"end\":48031,\"start\":48023},{\"end\":48043,\"start\":48031},{\"end\":48190,\"start\":48180},{\"end\":48290,\"start\":48282},{\"end\":48299,\"start\":48290},{\"end\":48311,\"start\":48299},{\"end\":48680,\"start\":48671},{\"end\":48688,\"start\":48680},{\"end\":48698,\"start\":48688},{\"end\":48707,\"start\":48698},{\"end\":48999,\"start\":48990},{\"end\":49007,\"start\":48999},{\"end\":49014,\"start\":49007},{\"end\":49022,\"start\":49014}]", "bib_venue": "[{\"end\":36314,\"start\":36275},{\"end\":36599,\"start\":36518},{\"end\":36868,\"start\":36774},{\"end\":37244,\"start\":37167},{\"end\":37740,\"start\":37660},{\"end\":38131,\"start\":38060},{\"end\":38488,\"start\":38448},{\"end\":38826,\"start\":38790},{\"end\":39083,\"start\":39076},{\"end\":39322,\"start\":39276},{\"end\":39693,\"start\":39624},{\"end\":40055,\"start\":40030},{\"end\":40381,\"start\":40351},{\"end\":40649,\"start\":40592},{\"end\":40870,\"start\":40786},{\"end\":41158,\"start\":41112},{\"end\":41454,\"start\":41450},{\"end\":41741,\"start\":41680},{\"end\":42128,\"start\":42108},{\"end\":42472,\"start\":42438},{\"end\":42729,\"start\":42698},{\"end\":43076,\"start\":42995},{\"end\":43522,\"start\":43451},{\"end\":43877,\"start\":43830},{\"end\":44172,\"start\":44101},{\"end\":44464,\"start\":44441},{\"end\":44790,\"start\":44746},{\"end\":45162,\"start\":45128},{\"end\":45483,\"start\":45442},{\"end\":45855,\"start\":45830},{\"end\":46150,\"start\":46112},{\"end\":46380,\"start\":46348},{\"end\":46679,\"start\":46608},{\"end\":47129,\"start\":47053},{\"end\":47526,\"start\":47442},{\"end\":47788,\"start\":47744},{\"end\":48068,\"start\":48043},{\"end\":48372,\"start\":48311},{\"end\":48733,\"start\":48707},{\"end\":49047,\"start\":49022},{\"end\":36949,\"start\":36870},{\"end\":44832,\"start\":44792},{\"end\":47192,\"start\":47131},{\"end\":47830,\"start\":47790}]"}}}, "year": 2023, "month": 12, "day": 17}