{"id": 220793490, "updated": "2023-10-06 12:58:21.619", "metadata": {"title": "Federated Self-Supervised Learning of Multi-Sensor Representations for Embedded Intelligence", "authors": "[{\"first\":\"Aaqib\",\"last\":\"Saeed\",\"middle\":[]},{\"first\":\"Flora\",\"last\":\"Salim\",\"middle\":[\"D.\"]},{\"first\":\"Tanir\",\"last\":\"Ozcelebi\",\"middle\":[]},{\"first\":\"Johan\",\"last\":\"Lukkien\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 7, "day": 25}, "abstract": "Smartphones, wearables, and Internet of Things (IoT) devices produce a wealth of data that cannot be accumulated in a centralized repository for learning supervised models due to privacy, bandwidth limitations, and the prohibitive cost of annotations. Federated learning provides a compelling framework for learning models from decentralized data, but conventionally, it assumes the availability of labeled samples, whereas on-device data are generally either unlabeled or cannot be annotated readily through user interaction. To address these issues, we propose a self-supervised approach termed \\textit{scalogram-signal correspondence learning} based on wavelet transform to learn useful representations from unlabeled sensor inputs, such as electroencephalography, blood volume pulse, accelerometer, and WiFi channel state information. Our auxiliary task requires a deep temporal neural network to determine if a given pair of a signal and its complementary viewpoint (i.e., a scalogram generated with a wavelet transform) align with each other or not through optimizing a contrastive objective. We extensively assess the quality of learned features with our multi-view strategy on diverse public datasets, achieving strong performance in all domains. We demonstrate the effectiveness of representations learned from an unlabeled input collection on downstream tasks with training a linear classifier over pretrained network, usefulness in low-data regime, transfer learning, and cross-validation. Our methodology achieves competitive performance with fully-supervised networks, and it outperforms pre-training with autoencoders in both central and federated contexts. Notably, it improves the generalization in a semi-supervised setting as it reduces the volume of labeled data required through leveraging self-supervised learning.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2007.13018", "mag": "3098250197", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/iotj/SaeedSOL21", "doi": "10.1109/jiot.2020.3009358"}}, "content": {"source": {"pdf_hash": "a0db03df64a01bf41b622b1e42aa996ec5223bca", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.13018v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.13018", "status": "GREEN"}}, "grobid": {"id": "45d756df8efd857c82a1724f8567b966fb2d474a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a0db03df64a01bf41b622b1e42aa996ec5223bca.txt", "contents": "\nIEEE INTERNET OF THINGS JOURNAL 1 Federated Self-Supervised Learning of Multi-Sensor Representations for Embedded Intelligence\n\n\nAaqib Saeed \nMember, IEEE, Tanir Ozcelebi Member, IEEEFlora D Salim \nSenior Member, IEEEJohan Lukkien \nIEEE INTERNET OF THINGS JOURNAL 1 Federated Self-Supervised Learning of Multi-Sensor Representations for Embedded Intelligence\nIndex Terms-self-supervised learningdeep learningfeder- ated learningembedded intelligencelow-data regimesensor analyticslearning representations\nSmartphones, wearables, and Internet of Things (IoT) devices produce a wealth of data that cannot be accumulated in a centralized repository for learning supervised models due to privacy, bandwidth limitations, and the prohibitive cost of annotations. Federated learning provides a compelling framework for learning models from decentralized data, but conventionally, it assumes the availability of labeled samples, whereas on-device data are generally either unlabeled or cannot be annotated readily through user interaction. To address these issues, we propose a self-supervised approach termed scalogramsignal correspondence learning based on wavelet transform to learn useful representations from unlabeled sensor inputs, such as electroencephalography, blood volume pulse, accelerometer, and WiFi channel state information. Our auxiliary task requires a deep temporal neural network to determine if a given pair of a signal and its complementary viewpoint (i.e., a scalogram generated with a wavelet transform) align with each other or not through optimizing a contrastive objective. We extensively assess the quality of learned features with our multi-view strategy on diverse public datasets, achieving strong performance in all domains. We demonstrate the effectiveness of representations learned from an unlabeled input collection on downstream tasks with training a linear classifier over pretrained network, usefulness in low-data regime, transfer learning, and cross-validation. Our methodology achieves competitive performance with fullysupervised networks, and it outperforms pre-training with autoencoders in both central and federated contexts. Notably, it improves the generalization in a semi-supervised setting as it reduces the volume of labeled data required through leveraging self-supervised learning.\n\nI. INTRODUCTION\n\nL EARNING representations with deep neural networks have made tremendous improvements in the last few years on challenging real-world tasks [1]- [4], thanks to the emergence of massive datasets. In particular, the wealth of sensory data from the Internet of Things (IoT) devices are Aaqib  only recently being leveraged for tackling important problems in understanding context, user monitoring, health, and other predictive analytics tasks, e.g., for emotional well-being [5], [6], sleep tracking [7], and physical activity detection [8]. The success is mainly attributed to the supervised methods that utilize labeled datasets for training models in a central environment. In contrast, learning models from unlabeled decentralized data still presents a major challenge. Obtaining large, well-curated sensory data from edge devices is especially difficult owing to issues like user privacy, the prohibitive cost of labeling, bandwidth limitations, network connectivity, and the diversity of device types [9]. These factors make it significantly challenging to harness abundant data on remote devices for learning semantic features with standard supervised approaches.\n\nTo highlight the challenges associated with learning a generalizable model for a particular use case, consider this illustrative example. Let us assume that we aim to develop a robust sleep stage classification model that can be used for a larger population of users. The standard methodology is to learn a supervised model and requires example-label pairs for providing supervision so that a model can differentiate among instances of multiple classes through learning underlying patterns in the input. The procedure begins with the data collection to monitor hundreds of users for electroencephalography (EEG) or other signals as they progress through various stages of sleep and accumulate the multi-sensor data in a centralized (data center) repository for further analysis. The next step is then to get the aggregated inputs annotated by the sleep expert (i.e., generally a professional trained in analyzing physiological signals) for the sleep classes, such as wake, N1, N2, N3, and rapid eye movement. Then, the learning and evaluation phase involves several iterations of improving the model performance. Lastly, the model is deployed in the wild for user monitoring. The process of model development, from data collection to annotation, could be extremely costly owing to the difficulty in setting up an experimental (data collection) protocol. Furthermore, the domain expertise required for the labeling could be severely limited. This problem is exacerbated by the need of supervised deep neural network models for a massive amount of labeled data to learn discriminative features. It becomes painstakingly difficult to inspect and annotate hundreds of thousands of hours of multi-sensor data. Therefore, in practice, limited-sized sensor data are collected and labeled for learning the model, which could further affect its generalization. The important point to note here is that the explained strategy is only applicable when the users arXiv:2007.13018v1 [cs.LG] 25 Jul 2020 agree on sharing their data for learning, which is not ideal in several real-world contexts due to raising privacy issues and misaligned incentives for the user. Likewise, IoT devices produce an astonishing amount of data on a daily basis, and even if the data sharing takes place, its rapidly increasing size limits exploiting for learning models. Therefore, there is a need to develop unsupervised (or self-supervised) methods that can be used to learn general-purpose models from unlabeled data. It is particularly pertinent to on-device learning (such as a smartphone), without the need for data aggregation in a centralized server, and minimal to no human involvement in terms of the annotation process. Consequently, the unsupervised model can be used as a semantic feature extractor or initialization for efficiently adapting to an end-task of interest through fine-tuning with few-labeled instances.\n\nSpecifically, the aforementioned challenges motivate the following research questions: Can we train a deep network to extract useful sensory representations in an unsupervised manner without utilizing strong labels for a specific problem, such as activity recognition? Could it also be achieved without aggregating the local data samples from remote devices in a centralized repository, i.e., employing decentralized or ondevice learning? Can we improve the network generalization in a low-data regime through fine-tuning it with few-labeled examples that potentially could be easily pooled from a group of users?\n\nPrevious approaches to learning representations from timeseries of sensory modalities with deep networks can be mainly categorized into three areas: end-to-end training of supervised models with labeled data [4], [6], [7], [10], reconstruction of the actual input for pre-training [11]- [13], and utilizing self-learning with domain-specific transformations or crossmodal learning [14]. Primarily, the focus of these methods is to conduct training of predictive models on a central server in a data center. However, as mentioned earlier, the aggregation of continuously increasing data from distributed devices is practically infeasible, aside from privacy issues. Initially, geodistributed analytics [15], [16] and distributed learning [17]- [19] in a data center environment is studied to exploit data locality and reducing network costs through pushing code to the data on the edge which generally is a node in the data center which could be across the globe. Nevertheless, these methods do not address the fundamental problem of learning representations with deep networks from unlabeled and highly distributed data that resides on user devices, which can not be aggregated in a central environment for learning.\n\nTo address the aforementioned concerns, federated learning [20] is emerging as an effective way of collaboratively training shared models from distributed private data. However, existing exploration in this area is solely focused on learning supervised models for tasks where annotations can be easily acquired based on the user interaction, e.g., mobile keyword prediction [21]. The curation of strongly labeled data becomes infeasible as annotations can not be acquired easily for solving several important problems involving sensory inputs. Because apart from wearables, other sensors could be installed in remote locations, and expert-level domain knowledge could be required to annotate samples. Hence, in such cases, unsu-((* pervised approaches provide a compelling substitute to learn from unlabeled data available in huge quantities as they do not require semantic labels.\n\nOne of the most rudimentary forms of unsupervised feature discovery has been hand-crafted feature engineering, which turns out to be largely redundant due to its limited discriminative power for building high-performance models [22]. Another area of research that is considerably explored focuses on reconstruction based approaches for extracting lowdimensional embedding through learning from data with deep autoencoders [11]. The main drawback of these methods is that they may waste the network's capacity to model low-level input details through predicting every bit of the signal. This is not needed if the aim is to learn discriminatory features that generalize well to the downstream (or end) tasks, e.g., sleep stage classification with electrical brain activity signals.\n\nA promising substitute is the emerging area of selfsupervised learning [23], which enables the learning of representations through solving an auxiliary task for which labels can be acquired from the data without any human intervention. In this case, several techniques are proposed mainly for audio, visual and textual data including estimation of missing input [24], prediction of contextually relevant information [25], recognizing degree of rotation applied on an image [26], contrastive predictive coding [1], synchronization of audio-visual inputs [2], and robotic imitation using multi-view videos [27]. Moreover, cross-modal learning is also utilized by specifying an appropriate loss term between different input modalities for training multimodal networks. However, to the best of our knowledge, previous work did not study self-supervised learning for other sensing modalities (e.g., electroencephalography, accelerometer, blood volume pulse, and others) as produced by a variety of IoT devices.\n\nIn this work, we hypothesize that the fusion of selfsupervision with federated learning could result in an effective method for learning from unlabeled, private, and diverse types of sensory data, which is crucial for several embedded (personalized) machine learning tasks. To achieve this objective, we develop a novel auxiliary task based on a wavelet transform, which we call scalogram-signal correspondence learning (SSCL). A deep temporal convolution network is trained to solve the specified task so as to learn representations from a variety of sensory inputs (e.g., electroencephalography, inertial measurement unit's sensors (IMUs), and WiFi channel state information). We name it a scalogram contrastive network (SCN). Specifically, the self-supervised scheme is designed to contrast between a raw signal (time-series) and its complementary view, which, in our case, is a scalogram, extracted with continuous wavelet transform [28]. We note that other views, such as a spectrogram derived with a fast Fourier transform can also be used for this purpose (or in combination). In this work, we opt for wavelet transformation because it is better at localizing time-frequency properties [29] of the signal.\n\nThe core idea behind our pretext task is to determine if a given pair of scalogram-signal inputs are aligned or misaligned, i.e., whether a scalogram is the transformation of a given signal. The presented auxiliary task can formally be seen as a binary classification problem, and we employ a contrastive objective inspired by [30] for optimizing it (see Figure 2 for an overview) in both central and federated settings without involving a human in the data labeling process. Importantly, we would like to highlight that for the model to solve the defined task successfully, it should learn the core semantics in shared input views through possibly relating frequency, scale, and other information present in the signal. The network captures meaningful latent relationships through correlating scalogram-signal inputs in the embedding space. Mainly, the representations that could emerge from the learning process are forms of invariances (such as sensor noise, subject-specific variations), which are essential in several tasks involving sensory data, e.g., stress detection with physiological signals.\n\nThe key contributions of this work are three-fold: First, we propose a scalogram-signal correspondence learning framework for self-supervised learning from diverse sensory data. Second, to the best of our knowledge, we, for the first time, propose to unify federated learning with self-supervision to learn from unlabeled and private data on edge devices. Third, we extensively assess the proposed method on several publicly available datasets from different domains with linear classification protocol in central and federated contexts, lowdata regime (i.e., semi-supervised setting), and transfer learning including cross-validation. The SCN achieves competitive performance compared with fully-supervised networks that are trained entirely on labeled data and perform significantly better than other approaches. Particularly, SCN fine-tuning with fewlabeled instances, e.g., five or ten instances per class, improves the F-score by as much as 5%-6% in comparison to training from scratch. Our approach also works better than transferring supervised features, learned from the source data, between the related tasks.\n\n\nII. BACKGROUND AND RELATED WORK\n\nWe consider learning sensory features from raw unlabeled data with a deep neural network F \u03b8 (parameterized by \u03b8), which transforms input from X into output in Z. Here, we refer to a vector obtained through applying a mapping function F : X \u2192 Z from an arbitrary intermediate or penultimate layer of the network as 'representation' or 'feature.' Our objective is to learn general-purpose representations that can make subsequent tasks of interests easier to solve. To this end, numerous unsupervised methods are developed to leverage a large amount of unlabeled data for achieving better generalization. Moreover, the data required for model development could not only be unannotated but also distributed, without the option to accumulate it in a centralized repository due to privacy concerns and its ever-increasing size. To tackle the issue of learning models from decentralized user data, the field of federated learning [20] is rapidly gaining momentum. Our work is intended to unify self-supervision with federated learning to realize the vision of on-device learning, with a focus on multi-sensor inputs. We describe the details of the essential building blocks of our approach and related work in the following subsections.\n\n\nA. Self-supervised Learning\n\nThe field of unsupervised learning deals with extracting disentangled representations that could be used for solving a wide variety of end-tasks. The most prominent approaches include principal component analysis, Boltzmann machine [31], autoencoders [11], generative adversarial networks [32], and autoregressive models [33]. Another emerging area of research for extracting unsupervised representations is 'selfsupervision.' It provides a general and powerful framework for learning with unlabeled inputs through solving pretext tasks. Here, a surrogate objective is specified in such a way that optimizing it would force the network to learn meaningful and usable features for the end-task. Specifically, given an unlabeled dataset D = {x 1 , x 2 , . . . , x M } with M instances. A surrogate task is designed that provides pseudolabels {y 1 , y 2 , . . . , y M } to learn F \u03b8 (without the need of any strong class annotations) through minimizing a classification, regression or metric loss L given by:\nmin \u03b8 1 M M m=1 L(F \u03b8 (x m ), y m )(1)\nIn the past few years, several self-supervised methods have been developed for vision, audio, language modeling, and other domains. However, little to no attention is paid towards exploring other sensing modalities, such as electroencephalography, IMUs, and blood volume pulse. The prominent approaches for learning from traditional input modalities include, colorization of grayscale images [34], predicting relative location of an image patch [25], audio-visual synchronization [2], temporal alignment in videos through cycle-consistency [35], word2vec (and other variants) [3], signal transformation prediction [8], contrastive predictive coding [1], and robotic imitation learning via time-contrastive networks [27]. These are some of the many strategies proposed for learning from an unlimited amount of unlabeled audio, visual, and textual data.\n\nIn this work, we seek to learn representations from data produced by sensors (time-series) on edge as obtaining a large amount of such labeled data is time-consuming and extremely costly. To solve this problem, we utilize a contrastive objective between a raw and complementary view of the data acquired via wavelet transform. A detailed explanation of the approach is provided in Section III.\n\n\nB. Wavelet Transform\n\nWhile the Fourier Transform (FT) sheds light on the frequency properties of the transformed signal, the input signal's time properties are not directly accessible from the Fourier representation. An alternative to this, which provides information about the time properties of the input signal (time locality of signal variations), is the Wavelet Transform (WT) [29]. Like the Short-term Fourier Transform (STFT), the WT divides the input signal into time windows of a certain size and operates on each time window separately. Choosing a larger time window of WT gives better frequency resolution of the WT output signal, while this reduces the time resolution. Precisely, the Wavelet series gives individual coefficients of a set of orthonormal functions (wavelets, e.g., Morlet, Haar, Daubechies). Like its counterparts, this representation effectively decomposes the input signal into combinations of wavelets. Due to these compelling properties, WT has been widely used in a myriad of domains [28]. In particular, continuous WT gained significant popularity compared to discrete counterpart since it is better at localizing time-frequency properties. A wavelet transform of a signal x(t) is defined as follows:\nT (a, b) = 1 \u221a a +\u221e \u2212\u221e x(t) \u00b7 \u03c8 t \u2212 b a dt(2)\nwhere \u03c8 represents a wavelet function, a and b denote scaling and translation factors, respectively. It is important to note that although we utilize WT in this work, other approaches like STFT could also be used in conjunction to possibly improve the performance along with segmentation [36].\n\n\nC. Federated Learning\n\nAutonomous vehicles, wearables, smartphones, and IoT sensors are examples of modern distributed devices producing a wealth of data every second. This massive amount of data offers an excellent opportunity for learning models to solve a diverse range of tasks. The applications of interest include customized fitness plans, personalized language models, and contextual awareness for driving automation. The growing computational power of edge devices allows us to leave the data decentralized and push the network computation to the client, which is also ideal from a privacy aspect. The expanding area of federated learning [20], [37], [38] explores developing methods to achieve the goal of learning from highly distributed and heterogeneous data through aggregating locally trained models on remote devices, such as smartphones and wearables. In this case, the intention is to minimize the following objective [37]:\nmin \u03b8 F \u03b8 , where F \u03b8 := C c m c m F \u03b8c .(3)\nHere, C represents the number of participating client devices in a training round, m c is the total number of instances available for client c with m = c m c , and lastly \u03b8 c denotes the weights of a local model. To produce a global model, Federated Averaging algorithm [20] is typically used to accumulate client updates after every round of local training t as with Equation 3. The research interest in federated learning revolves around improving communication efficiency [39], personalization [40], fault tolerance [41], privacy preservation [42] as well as looking into the theoretical underpinning of the federated optimization [37]. Specifically, the recent work deals with learning a unified model to solve a single as well as multiple tasks [43]. In addition to improving communication costs, the computational efficiency of federated learning on resourceconstrained devices has also been studied [44]. Similarly, the development of frameworks and productionizing of applications built around the idea of decentralized learning are also surging to address various practical problems, such as nextword and emoji prediction [21], wake word recognition [45], query suggestion [46], and traffic flow forecasting [47]. Deep reinforcement learning has also been investigated in a federated setting for edge caching in IoT to improve the quality of services and dealing with traffic off-loading [48].\n\nNevertheless, the existing techniques make a strong assumption that labeled training data are always accessible, or annotations can be extracted reliably, e.g., via user interaction with smartphone applications. However, for various problems involving sensory data (such as sleep stage scoring and context recognition), obtaining a large number of annotated examples in a real-world setting to train supervised models is prohibitively expensive and not feasible. This limits the applicability of current methods in learning from unlabeled data available from distributed IoT devices. The approach presented here is a step towards exploring self-supervised representation learning in a federated setting from unannotated multi-sensor data at the edge.\n\n\nIII. APPROACH\n\nLearning multi-sensor representations with deep networks requires a large amount of well-curated data, which is made difficult by the diversity of device types, environmental factors, inter-personal differences, privacy issues, and annotation cost. We propose a self-supervised auxiliary task whose objective at a high level is to contrast or compare raw signals and their corresponding scalograms (which are a visual representation of the wavelet transform) so that a network learns to discriminate between aligned and unaligned scalogram-signal pairs. The rationale of the proposed approach is similar in spirit to crossview learning in the audio-visual domain [2]. However, it differs in a core way that we obtain aligned and unaligned views 1 from the same modality with wavelet transform. In the absence of the semantic labels, our methodology can be leveraged to generate an endless stream of labeled data. Therefore, it can train the network without any human involvement, which is particularly attractive for on-device learning. In subsequent sections, we describe details of the correspondence learning, sample generation, preference of a loss function, and key network architectural properties.\n\n\nA. Scalogram-signal Correspondence Learning\n\nThe idea behind SSCL is to learn network parameters with a self-supervised objective that determines whether a raw signal and a scalogram correspond (or align) with each other or not. Given a multi-sensor dataset with fixed-length input segments of multiple modalities D = {x 1 , x 2 , . . . , x M } of M instances, we train a multimodal contrastive network to achieve the objective of synchronizing representations of the raw input with their corresponding scalogram. Specifically, a time-series is segmented into a fixed-sized input with a sliding window having a certain overlap between samples. Afterward, the scalogram s m of a signal x m can be generated with a specified wavelet transformation \u03a8 [29]. This procedure results 1 or in-sync and out-of-sync samples in synchronized pairs for each x m and s m of m-th instance. These co-occurring pairs of inputs are assigned a class label y m = 1, i.e., representing in-sync examples. Likewise, for generating negative samples y m = 0, for a particular x m , a randomly selected s m is assigned, which in principle represents that these scalogram-signal pairs do not align with each other. Here, we sample a negative scalogram from the same input modality. However, it can also be selected from a different modality, e.g., for accelerometer, the scalogram of the gyroscope can also be utilized. Importantly, we utilize an equal number of positive and negative instances for training the network. As described earlier, a wavelet transform provides a better multi-resolution analysis of non-stationary signals than Short-Time Fourier Transform (STFT) [28]. Hence, we extract a scalogram, which is an absolute and squared value of a WT operation. It is achieved using a continuous Morlet WT function which is expressed as follows:\n\u03c8(t) = exp \u2212t 2 2 \u00b7 exp \u2212jw0t(4)\nwhere w 0 denotes a central frequency of the mother wavelet.\n\nIn the broadest sense, the SSCL task requires a semantic understanding of how time-frequency information presented in a scalogram relates to a raw input signal, thus enabling the model to learn general-purpose embedding with a complementary view on the original input. We give a highlevel overview of our approach in Figure 2. The aim here is to learn a classifier H(.) that can minimize an empirical loss, so H(x m , s m ) = y m . A natural choice is to cast the specified problem as a binary classification task p(y|x, s) and hence, optimize a cross-entropy loss. Nevertheless, we achieve slightly better convergence through employing a contrastive loss that pulls together embedding of positive pairs and pushes different pairs apart, as it is also shown to be improving generalization in earlier work [30]:\nL = 1 M M m=1 (y m )||F X (x m ) \u2212 F S (s m )|| 2 2 + (1 \u2212 y m ) max(\u03b1 \u2212 ||F X (x m ) \u2212 F S (s m )|| 2 , 0) 2(5)\nwhere \u03b1 is a margin hyperparameter, which is enforced between positive and negative samples, F X , and F S are signal and scalogram networks, respectively. The contrastive loss optimization solves the proposed self-supervised task through the integration of not just different views of the same underlying signal, but it also aligns samples across multiple sensory modalities. This label-free correspondence learning approach results in rich representations that may be invariant to sensor noise, amplitude (or scale) variations, user-specific differences, and other factors.\n\n\nB. Network Architecture\n\nTo tackle the SSCL task, we design a dual-stream architecture named scalogram contrastive network, which is inspired from [2] and it is illustrated in Figure 2. It is composed of two distinct parts: the scalogram network and the signal network, each extracting features from their respective inputs. As the aim here is to learn representations from multiple sensors, each The models are aggregated to produce a unified model that is used for the end-task.\n\nnetwork consists of modality-specific and fusion layers to learn specialized and joint embedding, respectively. In particular, we utilize the same network architecture for learning on different datasets unless mentioned otherwise. Likewise, only the features from the signal network are used for evaluation, discarding the scalogram network after pre-training. The scalogram network consists of three 2D convolution layers with kernel sizes of 5, 4, 3, and 32, 64, 96 feature maps, respectively. Dropout is applied after every layer and maxpooling after the initial two convolutional layers with a pooling size of 2. We use the same design for each input modality, followed by the fusion layer consisting of 128 feature maps with a kernel size of 3. To learn from raw signals, we use a 1D convolutional network with the same structure as the scalogram network but with crucial differences in kernel sizes which are 10, 8, and 6 for sensor-specific layers and 4 in the case of a shared layer with a dropout layer at the end. Moreover, we use additional pre-training related layers for both networks, comprising a convolutional layer with 128 feature maps and a dense layer with 256 hidden units. These layers are discarded after the self-supervised learning phase as we hypothesize that they might learn features relevant to the auxiliary task (i.e., SSCL). We use the Mish [49] activation function in all the layers except the last, which has either linear or softmax activation. Finally, the input to our scalogram network are coefficients of the wavelet transform with a size (h\u00d7w\u00d7c), each representing height, width, and the number of channels, respectively. The signal network directly processes Stress Detection WESAD 15 3 raw input of size (w \u00d7 c).\n\n\nC. Implementation Details\n\nFor pre-training, we sample the non-corresponding scalogram-signal examples through randomly selecting scalograms from outside the current input batch while keeping the raw input fixed for positives and negatives. We preprocess the signals before computing scalogram or initiating network training as done in the previous works for each considered dataset; further details are provided in Section IV-A. We calculate summary statistics for znormalization from the training set. We use an Adam optimizer with a fixed learning rate of 0.0001 for pre-training and 0.01 or 0.02 in case of learning a linear classifier, which could also be decayed based on performance on the validation set. The network is trained with a batch size of 24, a dropout rate of 0.1, and L2 regularization rate of 0.0001. For federated learning simulation, we use the Tensorflow federated learning framework 2 . In this case, the networks are trained with a batch size of 12 for 5 local epochs using data of n randomly selected users (typically 10) at each training round with 30 \u2212 50 rounds in total, depending on the dataset size. Specifically, in our experiments, we randomly divide the training set into multiple subsets (representing each client) that are used to train the models in a federated setting. We opt for this strategy due to fewer users in existing datasets. The availability of bigger datasets with a larger pool of users could be useful to evaluate self-supervised methods in the future. A high-level overview of federated learning is illustrated in Figure 3.\n\n\nIV. EXPERIMENTS\n\nWe evaluate the effectiveness of our approach in multiple ways with several publicly available datasets from different domains. First, we probe the quality of representations with a linear classifier trained on-top of a frozen feature extractor in both central and federated learning settings. Second, we examine whether scalogram-signal correspondence learning could be used to improve the recognition rate in the low-data regime. Finally, we determine the transferability of features on related datasets, followed by an evaluation with crossvalidation to determine robustness against subject variations.\n\n\nA. Datasets and Preprocessing\n\nWe experimented with learning models on 5 datasets from the following application areas: sleep stage scoring, human activity recognition, WiFi sensing, and physiological stress detection.\n\nThe electroencephalogram (EEG) and electrooculography (EOG) signals are used from the PhysioNet Sleep-EDF dataset [50], [51] for classifying sleep into five stages (i.e., Wake, N1, N2, N3, and Rapid Eye Movement). We preprocess these signals, which are recorded at 100Hz, as done in earlier work [7] and utilize 30-second epochs (segments). For activity classification with smartphones, accelerometer, and gyroscope signals from HHAR [52] and MobiAct [53] datasets are used, which have 6 and 11 output classes, respectively. We segment the raw signals through a sliding window into a segment size of 400 samples with a 50% overlap. For device-free sensing of daily activities, we use the WiFi channel state information data [10] and follow identical preprocessing steps with [10]. Notably, the signals are resampled from 1kHz to 500Hz through uniform temporal downsampling with a rate of 2 for each of the 90 channels (i.e., 30 sub-carriers per antenna) to classify them into 7 classes. The WESAD dataset [5] is used for the detection of stress, normal, and amusement physiological states. Here, we use blood volume pulse, electrodermal activity, and temperature signals collected from a wrist wearable device at 64Hz, 4Hz, and 4Hz, respectively. Following [5], we extract 30-seconds segments and independently normalize each subject's data before the model development phase.\n\nIn all the cases, we use a random 70% \u2212 30% split of the dataset (based on users such that there is no overlap in terms of users' data) for training and evaluation, respectively. We also pick a 20% subset from training split as a validation set for hyperparameter tuning and model selection. Moreover, we also evaluate the performance of our approach with crossvalidation based on user split, i.e., leave-one-user-out. Table I summarizes the key characteristics of the datasets used in our evaluation.\n\n\nB. Quality assessment of the learned features with separability analysis\n\nIn Table II and Table III, we provide our key evaluation results in central and federated learning settings. First, we compare the performance of our approach with a) a supervised network trained end-to-end, b) an autoencoder, and c) a randomly initialized network in a central setting, i.e. when the entire data are available for learning on a server. We measure the quality of learned representations through a linear classifier trained on-top of the frozen feature extractor, which is a standard evaluation protocol used in earlier work. In the federated setting (Table III, the supervised network is learned for each user, and the weights are aggregated to create a unified model. For an autoencoder and SCN, the pre-training is performed in a federated setting to learn representations, and a classifier is trained in a standard way i.e., as if the data of end-task are available on the server. In addition, we also assess the performance when unsupervised networks are kept frozen, and classifier is also learned in a federated setting. In Table II these entries are represented with FC, which is an abbreviation of a federated classifier.\n\nIn particular, we highlight that for federated learning, we utilize random partitioning of the training sets as in [20] to tackle the low number of users in the considered (existing) datasets. This choice might result in a decentralized IID (i.e., independent and identically distributed) dataset that could be unbalanced but does not suffer from extreme heterogeneity in terms of training instances per client as generally, the case is for non-IID data that typically varies heavily based on the users' demographics, device usage, and other factors. However, we would again emphasize that our self-supervised technique does not depend on the user-generated labels for learning representations and could be easily applied to largescale datasets. However, as the end-task labels are required to evaluate the quality of learned features, the unavailability of massive multi-sensor labeled data is a critical limiting factor towards realizing the goal of assessment in the non-IID setting. We leave the evaluation of self-supervised features on a large pool of users with a greater variety of devices as future work.   On the evaluated datasets, we observe that the classifiers learned on-top of a fixed randomly initialized network achieve F-score above 60% in most cases. It highlights the representational capacity of our architecture design that, without seeing any samples, the encoder can provide reasonable embedding for a linear classifier. Notably, the SCN surpasses pre-training results with the autoencoder and on HHAR achieves better Fscore (82.7) than a supervised baseline (73.0). Particularly, we notice that the results obtained in a federated setting are close to those achieved with learning end-to-end models in a central setting, which hints towards the robustness of our approach in a federated environment. Similarly, when a linear classifier is also trained in a federated setting, the performance of SCN is mainly consistent with the centralized classifier, which is not the case for an autoencoder. Moreover, in Figure 4, we provide the t-SNE embedding of SCN on 1000 randomly selected instances from a test set of Sleep-EDF, WiFi-CSI, and HHAR. The distinct clusters of data points can be seen that are discovered entirely in an unsupervised manner. This further highlights the ability of SCN to learn meaningful representations.\n\nIn Figure 5, we compare the performance of downstream task classifiers trained on embedding from two different parts of the network. The representations from the encoder e and the features from the penultimate layer of SCN h(e) are used for this purpose. It can be seen that the classifier trained on the output of e performs significantly better than the one learned using the last layer's features. We think it could be because that layers at the end might learn auxiliary task-specific features that are not useful enough for the end-task.\n\nC. Improving generalization in low-data regime and transfer as evaluation\n\nWe explore the effectiveness of the proposed technique for improving performance with few-labeled examples. We pre-train a scalogram-contrastive network with the entire unlabeled data and use the model as initialization for learning a downstream task. We compare the performance with a standard supervised network trained only with certain labeled instances. Specifically, we use 5, 10, 20, and 40 labeled instances per class to learn the end-task model. Figure 6 and Table IV show an average F-score of 100 independent repetitions where different examples are sampled to train the network at each run. In all the cases, the results obtained with utilizing a selfsupervised network are better than the baseline, even when limited labeled data are available. This highlights that the SCN efficiently harnesses unlabeled data to learn generalized features.\n\nSimilarly, the self-supervised networks are also evaluated in terms of their usefulness in a transfer learning setting. Generally, this is achieved by treating a pre-trained model as a fixed feature extractor, and a linear model is trained on top of it using a different dataset. Here, we assess the performance on activity recognition tasks with HHAR and MobitAct datasets. Table V provides these results and compares with the supervised network, transfer from supervised (Sup.), and SCN trained on the same source instances. In both cases, we see that the recognition improves relatively if the transferred embedding is from SCN compared to a supervised network. Finally, we also assess the performance of SCN when fewlabeled instances are available for fine-tuning, but different unlabeled data are available for pre-training, as shown in Table VI. Similar to earlier semi-supervised evaluation, we finetune a pre-trained network end-to-end with 5, 10, 20, and 40 examples of each class from the target dataset. We notice a 2%\u22123% improvement in F-score over the supervised network when an SCN encoder is utilized.   \n\n\nD. Network robustness against subject variation with crossvalidation\n\nTo determine the robustness of network pre-training with the proposed approach against subject variation, we perform cross-validation (CV) based on user split. For Sleep-EDF, HHAR, and WESAD leave-one-subject-out CV is employed, whereas for MobiAct and WiFi-CSI, a 10-fold stratified CV is used due to a large number of users in the former and unavailability of subject ID's in the latter. We follow the same evaluation strategy as earlier, i.e., training a linear classifier to assess the quality of representations compared to the fullysupervised model and an autoencoder.  of subject data in a training set and achieves significantly better results than an autoencoder. Notably, on Sleep-EDF, our methods achieve a mean kappa score of 0.83 as compared to 0.77 of a supervised network and 0.76 as reported in [7]. Likewise, our self-supervised technique performs better than the hand-designed features from wrist physiological signals on WESAD by achieving an F-score of 75.7 \u00b1 0.13 as compared to 66.33 \u00b1 0.36 [5]. Furthermore, we would like to highlight that a direct comparison of existing approaches on other datasets used in our study is not feasible due to the differences in reported metrics and used sensing modalities. Nevertheless, our results with cross-validation further indicate that self-supervised learning can be effectively utilized for sensor modeling tasks on a large scale and can be combined with active learning methods [54].\n\n\nV. CONCLUSIONS\n\nIn this paper, we propose a self-supervised method for learning representations from unlabeled multi-sensor input data, which is typical in the IoT setting. Our method utilizes wavelet transform to generate a complementary view of the input (i.e., a scalogram) to define an auxiliary task of scalogram-signal correspondence. This procedure is specifically designed to work in federated learning setting to allow training networks with widely distributed and unannotated data as the labels can be readily extracted from the data without human-in-the-loop. We show the efficacy of the developed technique on several publicly available datasets involving diverse sensory streams, such as electroencephalogram, blood volume pulse, and IMUs. Particularly, we evaluate the quality of learned features with a linear classifier on an end-task and compare the performance with a fully-supervised network and pre-training with an autoencoder in both federated and central settings. Furthermore, we demonstrate an improved generalization in the low-data regime with self-supervision, i.e., when few labeled instances are used for fine-tuning network on the desired end-task. Our generic self-supervised approach can be used efficiently to learn general-purpose deep feature extractors entirely ondevice without the need to transmit the actual data to the server. In future work, we plan to combine self-supervision with architecture search on larger datasets and evaluate our method in a non-IID setting for federated learning. Another avenue of future research is to explore the effectiveness of self-supervised pre-training for adversarial robustness in a federated setting.\n\nFig. 1 :\n1Illustration of a 30-seconds long electroencephalogram (EEG) signal and a corresponding scalogram extracted with a Morlet wavelet transform.\n\nFig. 2 :\n2Scalogram contrastive network. We design a dual-stream architecture to learn from the raw input signal and its complementary view i.e. a scalogram. We map the original signal fragments into another domain and train the network to recognize which pairs belong together. Within this work, we use a wavelet transform. The highlevel overview of the method is illustrated in (a) where signal and scalogram networks are also multi-stream networks with a distinct stream for each input modality. The architecture of these modalityspecific signal and scalogram networks is shown in (b) and (c), respectively.\n\nFig. 3 :\n3Overview of federated learning framework. A central server dispatches a randomly initialized model and other training configuration details to the selected clients' devices, as depicted by dashed gray lines. The clients train local models on their private data and send the models back to the server illustrated with solid black lines.\n\nFig\n. 4: t-SNE embedding learned with scalogram contrastive network on a random subset of test subjects. Note, t-SNE does not utilize class labels, the colors are added during post-hoc analysis for better interpretability.\n\nFig. 5 :\n5Performance comparison of linear classifiers trained on-top of representations from encoder (e) and penultimate layer's projection h(e) of SCN denoted with fc 256 in Figure 2.\n\nFig. 6 :\n6Effectiveness of self-supervised learning in a low-data regime. The SCN is pre-trained on unlabeled data and fine-tuned end-to-end with few-labeled data points (i.e, 5, 10, 20, and 40 instances per class). On all the evaluated datasets, we notice a significant performance improvement over a supervised baseline network, which is trained only with labeled inputs.\n\n\nSaeed, Tanir Ozcelebi and Johan Lukkien are with the Department of Mathematics and Computer Science, Eindhoven University of Technology, The Netherlands. E-mail: {a.saeed, t.ozcelebi, j.j.lukkien}@tue.nl. Correspondence should be addressed to Aaqib Saeed. Flora D. Salim is with the School of Science, RMIT University, Melbourne Australia. She co-directs the RMIT Centre for Information Discovery and Data Analytics (CIDDA). E-mail: flora.salim@rmit.edu.au. This work is funded by SCOTT (www.scott-project.eu) project. It has received funding from the Electronic Component Systems for European Leadership Joint Undertaking under grant agreement No 737422. This Joint Undertaking receives support from the European Union's Horizon 2020 research and innovation programme and Austria, Spain, Finland, Ireland, Sweden, Germany, Poland, Portugal, Netherlands, Belgium, Norway.\n\nTABLE I :\nISummary of datasets.Task \nDataset \n#Users #Outputs \n\nSleep Stage Scoring Sleep-EDF \n20 \n5 \n\nActivity Recognition \nHHAR \n9 \n6 \n\nMobiAct \n61 \n11 \n\nDevice-Free Sensing \nWiFi-CSI \n6 \n7 \n\n\n\nTABLE II :\nIIPerformance evaluation of self-supervised representations learned in a standard central setting with a linear classifier. \n\nSleep-EDF \nHHAR \nMobiAct \nWiFi-CSI \nWESAD \n\nF-score Kappa F-score Kappa F-score Kappa F-score Kappa F-score Kappa \n\nRandom Init. \n0.67 \n0.54 \n0.64 \n0.58 \n0.65 \n0.63 \n0.36 \n0.24 \n0.73 \n0.58 \n\nSupervised \n0.82 \n0.76 \n0.73 \n0.69 \n0.95 \n0.93 \n0.96 \n0.95 \n0.85 \n0.75 \n\nAutoencoder \n0.75 \n0.66 \n0.69 \n0.63 \n0.80 \n0.78 \n0.84 \n0.81 \n0.83 \n0.72 \n\nSCN \n0.78 \n0.70 \n0.82 \n0.79 \n0.91 \n0.88 \n0.84 \n0.81 \n0.84 \n0.73 \n\n\n\nTABLE III :\nIIIAssessing performance in a federated learning setting to determine SCN's ability to learn representations from distributed data.The entries marked with FC (federated classifier) denotes metrics when both representations and classifier are learned in a federated context. \n\nSleep-EDF \nHHAR \nMobiAct \nWiFi-CSI \nWESAD \n\nF-score Kappa F-score Kappa F-score Kappa F-score Kappa F-score Kappa \n\nSupervised \n0.82 \n0.76 \n0.77 \n0.73 \n0.94 \n0.92 \n0.92 \n0.90 \n0.85 \n0.75 \n\nAutoencoder \n0.76 \n0.68 \n0.71 \n0.66 \n0.86 \n0.83 \n0.85 \n0.81 \n0.82 \n0.70 \n\nSCN \n0.78 \n0.70 \n0.80 \n0.77 \n0.90 \n0.88 \n0.85 \n0.82 \n0.83 \n0.73 \n\nAutoencoder (FC) \n0.68 \n0.56 \n0.51 \n0.44 \n0.54 \n0.47 \n0.67 \n0.60 \n0.80 \n0.67 \n\nSCN (FC) \n0.77 \n0.69 \n0.80 \n0.76 \n0.82 \n0.79 \n0.69 \n0.63 \n0.82 \n0.70 \n\n\n\nTABLE IV :\nIVGeneralization improvement in semi-supervised setting with self-supervised pre-training.Sleep-EDF \nHHAR \nMobiAct \nWiFi-CSI \nWESAD \n\nSupervised \nSCN \nSupervised \nSCN \nSupervised \nSCN \nSupervised \nSCN \nSupervised \nSCN \n\n5 \n0.58\u00b10.05 \n0.62\u00b10.05 \n0.50\u00b10.07 \n0.55\u00b10.06 \n0.56\u00b10.06 \n0.61\u00b10.07 \n0.48\u00b10.03 \n0.52\u00b10.03 \n0.71\u00b10.06 \n0.73\u00b10.06 \n\n10 \n0.64\u00b10.03 \n0.67\u00b10.04 \n0.57\u00b10.06 \n0.62\u00b10.05 \n0.65\u00b10.05 \n0.70\u00b10.05 \n0.57\u00b10.02 \n0.61\u00b10.02 \n0.74\u00b10.03 \n0.77\u00b10.03 \n\n20 \n0.68\u00b10.05 \n0.71\u00b10.02 \n0.62\u00b10.05 \n0.69\u00b10.04 \n0.74\u00b10.04 \n0.78\u00b10.04 \n0.67\u00b10.02 \n0.70\u00b10.02 \n0.77\u00b10.03 \n0.80\u00b10.03 \n\n40 \n0.72\u00b10.03 \n0.74\u00b10.02 \n0.68\u00b10.04 \n0.75\u00b10.04 \n0.82\u00b10.02 \n0.84\u00b10.02 \n0.77\u00b10.02 \n0.79\u00b10.02 \n0.81\u00b10.02 \n0.83\u00b10.02 \n\n\n\nTABLE V :\nVEvaluation of self-supervised representation in a standard transfer learning setting. HHAR \u2192 MobiAct MobiAct \u2192 HHARF-score \nKappa \nF-score \nKappa \n\nSupervised \n0.95 \n0.93 \n0.73 \n0.69 \n\nSource (SCN) \n0.91 \n0.88 \n0.82 \n0.79 \n\nTransfer (Sup.) \n0.86 \n0.83 \n0.62 \n0.54 \n\nTransfer (SCN) \n0.87 \n0.84 \n0.75 \n0.71 \n\n\n\nTABLE VI :\nVIFine-tuning transferred model with few-labeled data to improve recognition rate. We report weighted F-score averaged over 100 independent runs. T denotes a transfer learning.HHAR \u2192 MobiAct \nMobiAct \u2192 HHAR \n\nSupervised SCN (T) Supervised SCN (T) \n\n5 \n0.50 \n0.51 \n0.56 \n0.59 \n\n10 \n0.56 \n0.59 \n0.65 \n0.69 \n\n20 \n0.62 \n0.65 \n0.74 \n0.75 \n\n40 \n0.68 \n0.70 \n0.82 \n0.82 \n\n\n\n\nTable VII summarizes mean and standard deviation of metrics averaged over folds. Overall, we notice that SCN is stable despite the changes\n\nTABLE VII :\nVIIComparison of self-supervised representations to a fully-supervised network and pre-training with autoencoder using crossvalidation. Sleep-EDF 0.83\u00b10.05 0.77\u00b10.06 0.73\u00b10.08 0.65\u00b10.10 0.82\u00b10.03 0.83\u00b10.03 HHAR 0.82\u00b10.12 0.80\u00b10.13 0.62\u00b10.13 0.59\u00b10.15 0.78\u00b10.11 0.76\u00b10.12 76\u00b10.11 0.63\u00b10.17 0.71\u00b10.14 0.56\u00b10.25 0.75\u00b10.13 0.63\u00b10.19Supervised \nAutoencoder \nSCN \n\nF-score \nKappa \nF-score \nKappa \nF-score \nKappa \n\nMobiAct \n0.94\u00b10.02 0.92\u00b10.03 0.79\u00b10.04 0.75\u00b10.06 0.90\u00b10.02 0.87\u00b10.03 \n\nWiFi-CSI \n0.97\u00b10.0 \n0.97\u00b10.0 \n0.85\u00b10.01 0.82\u00b10.02 0.85\u00b10.01 0.82\u00b10.01 \n\nWESAD \n0.\nhttps://www.tensorflow.org/federated\n\nRepresentation learning with contrastive predictive coding. A V Oord, Y Li, O Vinyals, arXiv:1807.03748arXiv preprintA. v. d. Oord, Y. Li, and O. Vinyals, \"Representation learning with contrastive predictive coding,\" arXiv preprint arXiv:1807.03748, 2018.\n\nAudio-visual scene analysis with selfsupervised multisensory features. A Owens, A A Efros, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)A. Owens and A. A. Efros, \"Audio-visual scene analysis with self- supervised multisensory features,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 631-648.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \"Distributed representations of words and phrases and their composi- tionality,\" in Advances in neural information processing systems, 2013, pp. 3111-3119.\n\nMultimodal deep learning for activity and context recognition. V Radu, C Tong, S Bhattacharya, N D Lane, C Mascolo, M K Marina, F Kawsar, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies1V. Radu, C. Tong, S. Bhattacharya, N. D. Lane, C. Mascolo, M. K. Marina, and F. Kawsar, \"Multimodal deep learning for activity and context recognition,\" Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 1, no. 4, pp. 1-27, 2018.\n\nIntroducing wesad, a multimodal dataset for wearable stress and affect detection. P Schmidt, A Reiss, R Duerichen, C Marberger, K Van Laerhoven, Proceedings of the 2018 on International Conference on Multimodal Interaction. the 2018 on International Conference on Multimodal InteractionACMP. Schmidt, A. Reiss, R. Duerichen, C. Marberger, and K. Van Laer- hoven, \"Introducing wesad, a multimodal dataset for wearable stress and affect detection,\" in Proceedings of the 2018 on International Conference on Multimodal Interaction. ACM, 2018, pp. 400-408.\n\nDeepheart: semi-supervised sequence learning for cardiovascular risk prediction. B Ballinger, J Hsieh, A Singh, N Sohoni, J Wang, G H Tison, G M Marcus, J M Sanchez, C Maguire, J E Olgin, Thirty-Second AAAI Conference on Artificial Intelligence. B. Ballinger, J. Hsieh, A. Singh, N. Sohoni, J. Wang, G. H. Tison, G. M. Marcus, J. M. Sanchez, C. Maguire, J. E. Olgin et al., \"Deepheart: semi-supervised sequence learning for cardiovascular risk prediction,\" in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nDeepsleepnet: A model for automatic sleep stage scoring based on raw single-channel eeg. A Supratak, H Dong, C Wu, Y Guo, IEEE Transactions on Neural Systems and Rehabilitation Engineering. 2511A. Supratak, H. Dong, C. Wu, and Y. Guo, \"Deepsleepnet: A model for automatic sleep stage scoring based on raw single-channel eeg,\" IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 25, no. 11, pp. 1998-2008, 2017.\n\nMulti-task self-supervised learning for human activity detection. A Saeed, T Ozcelebi, J Lukkien, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies361A. Saeed, T. Ozcelebi, and J. Lukkien, \"Multi-task self-supervised learning for human activity detection,\" Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 3, no. 2, p. 61, 2019.\n\nDeepx: A software accelerator for low-power deep learning inference on mobile devices. N D Lane, S Bhattacharya, P Georgiev, C Forlivesi, L Jiao, L Qendro, F Kawsar, 2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN). IEEEN. D. Lane, S. Bhattacharya, P. Georgiev, C. Forlivesi, L. Jiao, L. Qen- dro, and F. Kawsar, \"Deepx: A software accelerator for low-power deep learning inference on mobile devices,\" in 2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN). IEEE, 2016, pp. 1-12.\n\nA survey on behavior recognition using wifi channel state information. S Yousefi, H Narui, S Dayal, S Ermon, S Valaee, IEEE Communications Magazine. 5510S. Yousefi, H. Narui, S. Dayal, S. Ermon, and S. Valaee, \"A survey on behavior recognition using wifi channel state information,\" IEEE Communications Magazine, vol. 55, no. 10, pp. 98-104, 2017.\n\nExtracting and composing robust features with denoising autoencoders. P Vincent, H Larochelle, Y Bengio, P.-A Manzagol, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learningP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, \"Extract- ing and composing robust features with denoising autoencoders,\" in Proceedings of the 25th international conference on Machine learning, 2008, pp. 1096-1103.\n\nLearning deep physiological models of affect. H P Martinez, Y Bengio, G N Yannakakis, IEEE Computational intelligence magazine. 82H. P. Martinez, Y. Bengio, and G. N. Yannakakis, \"Learning deep phys- iological models of affect,\" IEEE Computational intelligence magazine, vol. 8, no. 2, pp. 20-33, 2013.\n\nDeep learning for sensorbased activity recognition: A survey. J Wang, Y Chen, S Hao, X Peng, L Hu, Pattern Recognition Letters. 119J. Wang, Y. Chen, S. Hao, X. Peng, and L. Hu, \"Deep learning for sensor- based activity recognition: A survey,\" Pattern Recognition Letters, vol. 119, pp. 3-11, 2019.\n\nMultimodal machine learning: A survey and taxonomy. T Baltru\u0161aitis, C Ahuja, L.-P Morency, IEEE transactions on pattern analysis and machine intelligence. 41T. Baltru\u0161aitis, C. Ahuja, and L.-P. Morency, \"Multimodal machine learning: A survey and taxonomy,\" IEEE transactions on pattern anal- ysis and machine intelligence, vol. 41, no. 2, pp. 423-443, 2018.\n\nWANalytics: Analytics for a geo-distributed data-intensive world. A Vulimiri, C Curino, B Godfrey, K Karanasos, G Varghese, CIDR. A. Vulimiri, C. Curino, B. Godfrey, K. Karanasos, and G. Varghese, \"WANalytics: Analytics for a geo-distributed data-intensive world.\" in CIDR, 2015.\n\nMapreduce: simplified data processing on large clusters. J Dean, S Ghemawat, Communications of the ACM. 511J. Dean and S. Ghemawat, \"Mapreduce: simplified data processing on large clusters,\" Communications of the ACM, vol. 51, no. 1, pp. 107-113, 2008.\n\nDeep learning with elastic averaging sgd. S Zhang, A E Choromanska, Y Lecun, Advances in neural information processing systems. S. Zhang, A. E. Choromanska, and Y. LeCun, \"Deep learning with elastic averaging sgd,\" in Advances in neural information processing systems, 2015, pp. 685-693.\n\nDistributed learning, communication complexity and privacy. M F Balcan, A Blum, S Fine, Y Mansour, Conference on Learning Theory. M. F. Balcan, A. Blum, S. Fine, and Y. Mansour, \"Distributed learning, communication complexity and privacy,\" in Conference on Learning Theory, 2012, pp. 26-1.\n\nDistributed stochastic optimization and learning. O Shamir, N Srebro, 2014 52nd Annual Allerton Conference on Communication, Control, and Computing. AllertonIEEEO. Shamir and N. Srebro, \"Distributed stochastic optimization and learning,\" in 2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton). IEEE, 2014, pp. 850-857.\n\nCommunication-efficient learning of deep networks from decentralized data. H B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, Proceedings of the 20th International Conference on Artificial Intelligence and Statistics. the 20th International Conference on Artificial Intelligence and StatisticsAISTATSH. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \"Communication-efficient learning of deep networks from decentralized data,\" in Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), 2017. [Online].\n\nFederated learning for mobile keyboard prediction. A Hard, K Rao, R Mathews, S Ramaswamy, F Beaufays, S Augenstein, H Eichner, C Kiddon, D Ramage, arXiv:1811.03604arXiv preprintA. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augen- stein, H. Eichner, C. Kiddon, and D. Ramage, \"Federated learning for mobile keyboard prediction,\" arXiv preprint arXiv:1811.03604, 2018.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, nature. 5217553Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" nature, vol. 521, no. 7553, pp. 436-444, 2015.\n\nLearning classification with unlabeled data. V R De Sa, Advances in neural information processing systems. V. R. de Sa, \"Learning classification with unlabeled data,\" in Advances in neural information processing systems, 1994, pp. 112-119.\n\nSplit-brain autoencoders: Unsupervised learning by cross-channel prediction. R Zhang, P Isola, A A Efros, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionR. Zhang, P. Isola, and A. A. Efros, \"Split-brain autoencoders: Unsu- pervised learning by cross-channel prediction,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 1058-1067.\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. M Noroozi, P Favaro, European Conference on Computer Vision. SpringerM. Noroozi and P. Favaro, \"Unsupervised learning of visual representa- tions by solving jigsaw puzzles,\" in European Conference on Computer Vision. Springer, 2016, pp. 69-84.\n\nUnsupervised representation learning by predicting image rotations. S Gidaris, P Singh, N Komodakis, arXiv:1803.07728arXiv preprintS. Gidaris, P. Singh, and N. Komodakis, \"Unsupervised repre- sentation learning by predicting image rotations,\" arXiv preprint arXiv:1803.07728, 2018.\n\nTime-contrastive networks: Self-supervised learning from video. P Sermanet, C Lynch, Y Chebotar, J Hsu, E Jang, S Schaal, S Levine, G Brain, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEEP. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine, and G. Brain, \"Time-contrastive networks: Self-supervised learning from video,\" in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 1134-1141.\n\nWavelet theory and applications: a literature study. R Merry, DCT rapporten. R. Merry, \"Wavelet theory and applications: a literature study,\" DCT rapporten, vol. 2005, 2005.\n\nThe wavelet transform, time-frequency localization and signal analysis. I Daubechies, IEEE transactions on information theory. 365I. Daubechies, \"The wavelet transform, time-frequency localization and signal analysis,\" IEEE transactions on information theory, vol. 36, no. 5, pp. 961-1005, 1990.\n\nLearning a similarity metric discriminatively, with application to face verification. S Chopra, R Hadsell, Y Lecun, CVPR. S. Chopra, R. Hadsell, Y. LeCun et al., \"Learning a similarity metric discriminatively, with application to face verification,\" in CVPR (1), 2005, pp. 539-546.\n\nDeep boltzmann machines. R Salakhutdinov, G Hinton, Artificial intelligence and statistics. R. Salakhutdinov and G. Hinton, \"Deep boltzmann machines,\" in Artificial intelligence and statistics, 2009, pp. 448-455.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \"Generative adversarial nets,\" in Advances in neural information processing systems, 2014, pp. 2672- 2680.\n\nWavenet: A generative model for raw audio. A V Oord, S Dieleman, H Zen, K Simonyan, O Vinyals, A Graves, N Kalchbrenner, A Senior, K Kavukcuoglu, arXiv:1609.03499arXiv preprintA. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \"Wavenet: A gener- ative model for raw audio,\" arXiv preprint arXiv:1609.03499, 2016.\n\nColorization as a proxy task for visual understanding. G Larsson, M Maire, G Shakhnarovich, CVPR. 27G. Larsson, M. Maire, and G. Shakhnarovich, \"Colorization as a proxy task for visual understanding,\" in CVPR, vol. 2, 2017, p. 7.\n\nTemporal cycle-consistency learning. D Dwibedi, Y Aytar, J Tompson, P Sermanet, A Zisserman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionD. Dwibedi, Y. Aytar, J. Tompson, P. Sermanet, and A. Zisserman, \"Temporal cycle-consistency learning,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 1801-1810.\n\nInformation gain-based metric for recognizing transitions in human activities. A Sadri, Y Ren, F D Salim, Pervasive and Mobile Computing. 38A. Sadri, Y. Ren, and F. D. Salim, \"Information gain-based metric for recognizing transitions in human activities,\" Pervasive and Mobile Computing, vol. 38, pp. 92-109, 2017.\n\nFederated learning: Challenges, methods, and future directions. T Li, A K Sahu, A Talwalkar, V Smith, arXiv:1908.07873arXiv preprintT. Li, A. K. Sahu, A. Talwalkar, and V. Smith, \"Federated learn- ing: Challenges, methods, and future directions,\" arXiv preprint arXiv:1908.07873, 2019.\n\nAdvances and open problems in federated learning. P Kairouz, H B Mcmahan, B Avent, A Bellet, M Bennis, A N Bhagoji, K Bonawitz, Z Charles, G Cormode, R Cummings, arXiv:1912.04977arXiv preprintP. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings et al., \"Advances and open problems in federated learning,\" arXiv preprint arXiv:1912.04977, 2019.\n\nFederated learning: Strategies for improving communication efficiency. J Kone\u010dn\u1ef3, H B Mcmahan, F X Yu, P Richt\u00e1rik, A T Suresh, D Bacon, arXiv:1610.05492arXiv preprintJ. Kone\u010dn\u1ef3, H. B. McMahan, F. X. Yu, P. Richt\u00e1rik, A. T. Suresh, and D. Bacon, \"Federated learning: Strategies for improving communication efficiency,\" arXiv preprint arXiv:1610.05492, 2016.\n\nFederated evaluation of on-device personalization. K Wang, R Mathews, C Kiddon, H Eichner, F Beaufays, D Ramage, arXiv:1910.10252arXiv preprintK. Wang, R. Mathews, C. Kiddon, H. Eichner, F. Beaufays, and D. Ramage, \"Federated evaluation of on-device personalization,\" arXiv preprint arXiv:1910.10252, 2019.\n\nTowards federated learning at scale: System design. K Bonawitz, H Eichner, W Grieskamp, D Huba, A Ingerman, V Ivanov, C Kiddon, J Konecny, S Mazzocchi, H B Mcmahan, arXiv:1902.01046arXiv preprintK. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konecny, S. Mazzocchi, H. B. McMahan et al., \"Towards federated learning at scale: System design,\" arXiv preprint arXiv:1902.01046, 2019.\n\nPractical secure aggregation for privacy-preserving machine learning. K Bonawitz, V Ivanov, B Kreuter, A Marcedone, H B Mcmahan, S Patel, D Ramage, A Segal, K Seth, Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. the 2017 ACM SIGSAC Conference on Computer and Communications SecurityK. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel, D. Ramage, A. Segal, and K. Seth, \"Practical secure aggregation for privacy-preserving machine learning,\" in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. 1175-1191.\n\nFederated multi-task learning. V Smith, C.-K Chiang, M Sanjabi, A S Talwalkar, Advances in Neural Information Processing Systems. V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, \"Federated multi-task learning,\" in Advances in Neural Information Processing Systems, 2017, pp. 4424-4434.\n\nCefl: Online admission control, data scheduling and accuracy tuning for cost-efficient federated learning across edge nodes. Z Zhou, S Yang, L J Pu, S Yu, IEEE Internet of Things Journal. Z. Zhou, S. Yang, L. J. Pu, and S. Yu, \"Cefl: Online admission control, data scheduling and accuracy tuning for cost-efficient federated learning across edge nodes,\" IEEE Internet of Things Journal, pp. 1-1, 2020.\n\nFederated learning for keyword spotting. D Leroy, A Coucke, T Lavril, T Gisselbrecht, J Dureau, ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEED. Leroy, A. Coucke, T. Lavril, T. Gisselbrecht, and J. Dureau, \"Fed- erated learning for keyword spotting,\" in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6341-6345.\n\nApplied federated learning: Improving google keyboard query suggestions. T Yang, G Andrew, H Eichner, H Sun, W Li, N Kong, D Ramage, F Beaufays, arXiv:1812.02903arXiv preprintT. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ram- age, and F. Beaufays, \"Applied federated learning: Improving google keyboard query suggestions,\" arXiv preprint arXiv:1812.02903, 2018.\n\nPrivacy-preserving traffic flow prediction: A federated learning approach. Y Liu, J J Q Yu, J Kang, D Niyato, S Zhang, IEEE Internet of Things Journal. Y. Liu, J. J. Q. Yu, J. Kang, D. Niyato, and S. Zhang, \"Privacy-preserving traffic flow prediction: A federated learning approach,\" IEEE Internet of Things Journal, pp. 1-1, 2020.\n\nFederated deep reinforcement learning for internet of things with decentralized cooperative edge caching. X Wang, C Wang, X Li, V C M Leung, T Taleb, IEEE Internet of Things Journal. X. Wang, C. Wang, X. Li, V. C. M. Leung, and T. Taleb, \"Federated deep reinforcement learning for internet of things with decentralized cooperative edge caching,\" IEEE Internet of Things Journal, pp. 1-1, 2020.\n\nMish: A self regularized non-monotonic neural activation function. D Misra, arXiv:1908.08681arXiv preprintD. Misra, \"Mish: A self regularized non-monotonic neural activation function,\" arXiv preprint arXiv:1908.08681, 2019.\n\nAnalysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the eeg. B Kemp, A H Zwinderman, B Tuk, H A Kamphuisen, J J Oberye, IEEE Transactions on Biomedical Engineering. 479B. Kemp, A. H. Zwinderman, B. Tuk, H. A. Kamphuisen, and J. J. Oberye, \"Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the eeg,\" IEEE Transactions on Biomed- ical Engineering, vol. 47, no. 9, pp. 1185-1194, 2000.\n\nA L Goldberger, L A Amaral, L Glass, J M Hausdorff, P C Ivanov, R G Mark, J E Mietus, G B Moody, C.-K Peng, H E Stanley, Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals. 101A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley, \"Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals,\" Circulation, vol. 101, no. 23, pp. e215-e220, 2000.\n\nSmart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition. A Stisen, H Blunck, S Bhattacharya, T S Prentow, M B Kjaergaard, A Dey, T Sonne, M M Jensen, Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems. the 13th ACM Conference on Embedded Networked Sensor SystemsACMA. Stisen, H. Blunck, S. Bhattacharya, T. S. Prentow, M. B. Kjaergaard, A. Dey, T. Sonne, and M. M. Jensen, \"Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities for activity recognition,\" in Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems. ACM, 2015, pp. 127-140.\n\nThe mobiact dataset: Recognition of activities of daily living using smartphones. G Vavoulas, C Chatzaki, T Malliotakis, M Pediaditis, M Tsiknakis, ICT4AgeingWell. G. Vavoulas, C. Chatzaki, T. Malliotakis, M. Pediaditis, and M. Tsik- nakis, \"The mobiact dataset: Recognition of activities of daily living using smartphones.\" in ICT4AgeingWell, 2016, pp. 143-151.\n\nImproving experience sampling with multi-view user-driven annotation prediction. J Liono, F D Salim, N Van Berkel, V Kostakos, A K Qin, 2019 IEEE International Conference on Pervasive Computing and Communications. PerCom. IEEEJ. Liono, F. D. Salim, N. van Berkel, V. Kostakos, and A. K. Qin, \"Improving experience sampling with multi-view user-driven annota- tion prediction,\" in 2019 IEEE International Conference on Pervasive Computing and Communications (PerCom. IEEE, 2019, pp. 1-11.\n", "annotations": {"author": "[{\"end\":142,\"start\":130},{\"end\":198,\"start\":143},{\"end\":232,\"start\":199}]", "publisher": null, "author_last_name": "[{\"end\":141,\"start\":136},{\"end\":197,\"start\":192},{\"end\":231,\"start\":224}]", "author_first_name": "[{\"end\":135,\"start\":130},{\"end\":189,\"start\":184},{\"end\":191,\"start\":190},{\"end\":223,\"start\":218}]", "author_affiliation": null, "title": "[{\"end\":127,\"start\":1},{\"end\":359,\"start\":233}]", "venue": null, "abstract": "[{\"end\":2330,\"start\":506}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2492,\"start\":2489},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2497,\"start\":2494},{\"end\":2637,\"start\":2632},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2824,\"start\":2821},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2829,\"start\":2826},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2849,\"start\":2846},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2886,\"start\":2883},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3356,\"start\":3353},{\"end\":5491,\"start\":5487},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7242,\"start\":7239},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7247,\"start\":7244},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7252,\"start\":7249},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7258,\"start\":7254},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7316,\"start\":7312},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7322,\"start\":7318},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7416,\"start\":7412},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7736,\"start\":7732},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7742,\"start\":7738},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7772,\"start\":7768},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7778,\"start\":7774},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8312,\"start\":8308},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8627,\"start\":8623},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9364,\"start\":9360},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9558,\"start\":9554},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9988,\"start\":9984},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10279,\"start\":10275},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10333,\"start\":10329},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10390,\"start\":10386},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10425,\"start\":10422},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10469,\"start\":10466},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10521,\"start\":10517},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11861,\"start\":11857},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12117,\"start\":12113},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12465,\"start\":12461},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15322,\"start\":15318},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15892,\"start\":15888},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15911,\"start\":15907},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15949,\"start\":15945},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15981,\"start\":15977},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17097,\"start\":17093},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17150,\"start\":17146},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17184,\"start\":17181},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17245,\"start\":17241},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17280,\"start\":17277},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17318,\"start\":17315},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17353,\"start\":17350},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17420,\"start\":17416},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18337,\"start\":18333},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18972,\"start\":18968},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19524,\"start\":19520},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20179,\"start\":20175},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20185,\"start\":20181},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20191,\"start\":20187},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20467,\"start\":20463},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20788,\"start\":20784},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20892,\"start\":20891},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20993,\"start\":20989},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21015,\"start\":21011},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21037,\"start\":21033},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21064,\"start\":21060},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21152,\"start\":21148},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21268,\"start\":21264},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21424,\"start\":21420},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21649,\"start\":21645},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21677,\"start\":21673},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21700,\"start\":21696},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21735,\"start\":21731},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":21915,\"start\":21911},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23352,\"start\":23349},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24645,\"start\":24641},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25544,\"start\":25540},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26623,\"start\":26619},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27466,\"start\":27463},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29175,\"start\":29171},{\"end\":29525,\"start\":29521},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":32099,\"start\":32095},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":32105,\"start\":32101},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32280,\"start\":32277},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":32419,\"start\":32415},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":32436,\"start\":32432},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32709,\"start\":32705},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":32760,\"start\":32756},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32989,\"start\":32986},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33241,\"start\":33238},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35203,\"start\":35199},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":40919,\"start\":40916},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":41121,\"start\":41118},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":41554,\"start\":41550}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43391,\"start\":43240},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44003,\"start\":43392},{\"attributes\":{\"id\":\"fig_2\"},\"end\":44350,\"start\":44004},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44574,\"start\":44351},{\"attributes\":{\"id\":\"fig_4\"},\"end\":44761,\"start\":44575},{\"attributes\":{\"id\":\"fig_5\"},\"end\":45136,\"start\":44762},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46010,\"start\":45137},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46206,\"start\":46011},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":46749,\"start\":46207},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":47518,\"start\":46750},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":48210,\"start\":47519},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":48530,\"start\":48211},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":48907,\"start\":48531},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":49048,\"start\":48908},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":49622,\"start\":49049}]", "paragraph": "[{\"end\":3516,\"start\":2349},{\"end\":6414,\"start\":3518},{\"end\":7029,\"start\":6416},{\"end\":8247,\"start\":7031},{\"end\":9130,\"start\":8249},{\"end\":9911,\"start\":9132},{\"end\":10918,\"start\":9913},{\"end\":12132,\"start\":10920},{\"end\":13237,\"start\":12134},{\"end\":14357,\"start\":13239},{\"end\":15624,\"start\":14393},{\"end\":16661,\"start\":15656},{\"end\":17552,\"start\":16701},{\"end\":17947,\"start\":17554},{\"end\":19185,\"start\":17972},{\"end\":19525,\"start\":19232},{\"end\":20468,\"start\":19551},{\"end\":21916,\"start\":20514},{\"end\":22668,\"start\":21918},{\"end\":23890,\"start\":22686},{\"end\":25718,\"start\":23938},{\"end\":25812,\"start\":25752},{\"end\":26624,\"start\":25814},{\"end\":27313,\"start\":26738},{\"end\":27796,\"start\":27341},{\"end\":29552,\"start\":27798},{\"end\":31133,\"start\":29582},{\"end\":31758,\"start\":31153},{\"end\":31979,\"start\":31792},{\"end\":33357,\"start\":31981},{\"end\":33860,\"start\":33359},{\"end\":35082,\"start\":33937},{\"end\":37436,\"start\":35084},{\"end\":37980,\"start\":37438},{\"end\":38055,\"start\":37982},{\"end\":38911,\"start\":38057},{\"end\":40032,\"start\":38913},{\"end\":41555,\"start\":40105},{\"end\":43239,\"start\":41574}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16700,\"start\":16662},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19231,\"start\":19186},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20513,\"start\":20469},{\"attributes\":{\"id\":\"formula_3\"},\"end\":25751,\"start\":25719},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26737,\"start\":26625}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33785,\"start\":33778},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33962,\"start\":33940},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34513,\"start\":34503},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34991,\"start\":34983},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":38533,\"start\":38525},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":39295,\"start\":39288}]", "section_header": "[{\"end\":2347,\"start\":2332},{\"end\":14391,\"start\":14360},{\"end\":15654,\"start\":15627},{\"end\":17970,\"start\":17950},{\"end\":19549,\"start\":19528},{\"end\":22684,\"start\":22671},{\"end\":23936,\"start\":23893},{\"end\":27339,\"start\":27316},{\"end\":29580,\"start\":29555},{\"end\":31151,\"start\":31136},{\"end\":31790,\"start\":31761},{\"end\":33935,\"start\":33863},{\"end\":40103,\"start\":40035},{\"end\":41572,\"start\":41558},{\"end\":43249,\"start\":43241},{\"end\":43401,\"start\":43393},{\"end\":44013,\"start\":44005},{\"end\":44355,\"start\":44352},{\"end\":44584,\"start\":44576},{\"end\":44771,\"start\":44763},{\"end\":46021,\"start\":46012},{\"end\":46218,\"start\":46208},{\"end\":46762,\"start\":46751},{\"end\":47530,\"start\":47520},{\"end\":48221,\"start\":48212},{\"end\":48542,\"start\":48532},{\"end\":49061,\"start\":49050}]", "table": "[{\"end\":46206,\"start\":46043},{\"end\":46749,\"start\":46221},{\"end\":47518,\"start\":46894},{\"end\":48210,\"start\":47621},{\"end\":48530,\"start\":48338},{\"end\":48907,\"start\":48719},{\"end\":49622,\"start\":49390}]", "figure_caption": "[{\"end\":43391,\"start\":43251},{\"end\":44003,\"start\":43403},{\"end\":44350,\"start\":44015},{\"end\":44574,\"start\":44356},{\"end\":44761,\"start\":44586},{\"end\":45136,\"start\":44773},{\"end\":46010,\"start\":45139},{\"end\":46043,\"start\":46023},{\"end\":46894,\"start\":46766},{\"end\":47621,\"start\":47533},{\"end\":48338,\"start\":48223},{\"end\":48719,\"start\":48545},{\"end\":49048,\"start\":48910},{\"end\":49390,\"start\":49065}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12497,\"start\":12489},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26139,\"start\":26131},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27500,\"start\":27492},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31132,\"start\":31124},{\"end\":37126,\"start\":37118},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":37449,\"start\":37441},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":38520,\"start\":38512}]", "bib_author_first_name": "[{\"end\":49722,\"start\":49721},{\"end\":49724,\"start\":49723},{\"end\":49732,\"start\":49731},{\"end\":49738,\"start\":49737},{\"end\":49990,\"start\":49989},{\"end\":49999,\"start\":49998},{\"end\":50001,\"start\":50000},{\"end\":50392,\"start\":50391},{\"end\":50403,\"start\":50402},{\"end\":50416,\"start\":50415},{\"end\":50424,\"start\":50423},{\"end\":50426,\"start\":50425},{\"end\":50437,\"start\":50436},{\"end\":50779,\"start\":50778},{\"end\":50787,\"start\":50786},{\"end\":50795,\"start\":50794},{\"end\":50811,\"start\":50810},{\"end\":50813,\"start\":50812},{\"end\":50821,\"start\":50820},{\"end\":50832,\"start\":50831},{\"end\":50834,\"start\":50833},{\"end\":50844,\"start\":50843},{\"end\":51360,\"start\":51359},{\"end\":51371,\"start\":51370},{\"end\":51380,\"start\":51379},{\"end\":51393,\"start\":51392},{\"end\":51406,\"start\":51405},{\"end\":51913,\"start\":51912},{\"end\":51926,\"start\":51925},{\"end\":51935,\"start\":51934},{\"end\":51944,\"start\":51943},{\"end\":51954,\"start\":51953},{\"end\":51962,\"start\":51961},{\"end\":51964,\"start\":51963},{\"end\":51973,\"start\":51972},{\"end\":51975,\"start\":51974},{\"end\":51985,\"start\":51984},{\"end\":51987,\"start\":51986},{\"end\":51998,\"start\":51997},{\"end\":52009,\"start\":52008},{\"end\":52011,\"start\":52010},{\"end\":52446,\"start\":52445},{\"end\":52458,\"start\":52457},{\"end\":52466,\"start\":52465},{\"end\":52472,\"start\":52471},{\"end\":52856,\"start\":52855},{\"end\":52865,\"start\":52864},{\"end\":52877,\"start\":52876},{\"end\":53352,\"start\":53351},{\"end\":53354,\"start\":53353},{\"end\":53362,\"start\":53361},{\"end\":53378,\"start\":53377},{\"end\":53390,\"start\":53389},{\"end\":53403,\"start\":53402},{\"end\":53411,\"start\":53410},{\"end\":53421,\"start\":53420},{\"end\":53908,\"start\":53907},{\"end\":53919,\"start\":53918},{\"end\":53928,\"start\":53927},{\"end\":53937,\"start\":53936},{\"end\":53946,\"start\":53945},{\"end\":54256,\"start\":54255},{\"end\":54267,\"start\":54266},{\"end\":54281,\"start\":54280},{\"end\":54294,\"start\":54290},{\"end\":54702,\"start\":54701},{\"end\":54704,\"start\":54703},{\"end\":54716,\"start\":54715},{\"end\":54726,\"start\":54725},{\"end\":54728,\"start\":54727},{\"end\":55022,\"start\":55021},{\"end\":55030,\"start\":55029},{\"end\":55038,\"start\":55037},{\"end\":55045,\"start\":55044},{\"end\":55053,\"start\":55052},{\"end\":55311,\"start\":55310},{\"end\":55327,\"start\":55326},{\"end\":55339,\"start\":55335},{\"end\":55684,\"start\":55683},{\"end\":55696,\"start\":55695},{\"end\":55706,\"start\":55705},{\"end\":55717,\"start\":55716},{\"end\":55730,\"start\":55729},{\"end\":55956,\"start\":55955},{\"end\":55964,\"start\":55963},{\"end\":56195,\"start\":56194},{\"end\":56204,\"start\":56203},{\"end\":56206,\"start\":56205},{\"end\":56221,\"start\":56220},{\"end\":56502,\"start\":56501},{\"end\":56504,\"start\":56503},{\"end\":56514,\"start\":56513},{\"end\":56522,\"start\":56521},{\"end\":56530,\"start\":56529},{\"end\":56783,\"start\":56782},{\"end\":56793,\"start\":56792},{\"end\":57165,\"start\":57164},{\"end\":57167,\"start\":57166},{\"end\":57178,\"start\":57177},{\"end\":57187,\"start\":57186},{\"end\":57197,\"start\":57196},{\"end\":57208,\"start\":57207},{\"end\":57210,\"start\":57209},{\"end\":57710,\"start\":57709},{\"end\":57718,\"start\":57717},{\"end\":57725,\"start\":57724},{\"end\":57736,\"start\":57735},{\"end\":57749,\"start\":57748},{\"end\":57761,\"start\":57760},{\"end\":57775,\"start\":57774},{\"end\":57786,\"start\":57785},{\"end\":57796,\"start\":57795},{\"end\":58055,\"start\":58054},{\"end\":58064,\"start\":58063},{\"end\":58074,\"start\":58073},{\"end\":58245,\"start\":58244},{\"end\":58247,\"start\":58246},{\"end\":58518,\"start\":58517},{\"end\":58527,\"start\":58526},{\"end\":58536,\"start\":58535},{\"end\":58538,\"start\":58537},{\"end\":58985,\"start\":58984},{\"end\":58996,\"start\":58995},{\"end\":59298,\"start\":59297},{\"end\":59309,\"start\":59308},{\"end\":59318,\"start\":59317},{\"end\":59577,\"start\":59576},{\"end\":59589,\"start\":59588},{\"end\":59598,\"start\":59597},{\"end\":59610,\"start\":59609},{\"end\":59617,\"start\":59616},{\"end\":59625,\"start\":59624},{\"end\":59635,\"start\":59634},{\"end\":59645,\"start\":59644},{\"end\":60037,\"start\":60036},{\"end\":60231,\"start\":60230},{\"end\":60542,\"start\":60541},{\"end\":60552,\"start\":60551},{\"end\":60563,\"start\":60562},{\"end\":60764,\"start\":60763},{\"end\":60781,\"start\":60780},{\"end\":60982,\"start\":60981},{\"end\":60996,\"start\":60995},{\"end\":61013,\"start\":61012},{\"end\":61022,\"start\":61021},{\"end\":61028,\"start\":61027},{\"end\":61044,\"start\":61043},{\"end\":61053,\"start\":61052},{\"end\":61066,\"start\":61065},{\"end\":61384,\"start\":61383},{\"end\":61386,\"start\":61385},{\"end\":61394,\"start\":61393},{\"end\":61406,\"start\":61405},{\"end\":61413,\"start\":61412},{\"end\":61425,\"start\":61424},{\"end\":61436,\"start\":61435},{\"end\":61446,\"start\":61445},{\"end\":61462,\"start\":61461},{\"end\":61472,\"start\":61471},{\"end\":61779,\"start\":61778},{\"end\":61790,\"start\":61789},{\"end\":61799,\"start\":61798},{\"end\":61992,\"start\":61991},{\"end\":62003,\"start\":62002},{\"end\":62012,\"start\":62011},{\"end\":62023,\"start\":62022},{\"end\":62035,\"start\":62034},{\"end\":62476,\"start\":62475},{\"end\":62485,\"start\":62484},{\"end\":62492,\"start\":62491},{\"end\":62494,\"start\":62493},{\"end\":62777,\"start\":62776},{\"end\":62783,\"start\":62782},{\"end\":62785,\"start\":62784},{\"end\":62793,\"start\":62792},{\"end\":62806,\"start\":62805},{\"end\":63050,\"start\":63049},{\"end\":63061,\"start\":63060},{\"end\":63063,\"start\":63062},{\"end\":63074,\"start\":63073},{\"end\":63083,\"start\":63082},{\"end\":63093,\"start\":63092},{\"end\":63103,\"start\":63102},{\"end\":63105,\"start\":63104},{\"end\":63116,\"start\":63115},{\"end\":63128,\"start\":63127},{\"end\":63139,\"start\":63138},{\"end\":63150,\"start\":63149},{\"end\":63486,\"start\":63485},{\"end\":63497,\"start\":63496},{\"end\":63499,\"start\":63498},{\"end\":63510,\"start\":63509},{\"end\":63512,\"start\":63511},{\"end\":63518,\"start\":63517},{\"end\":63531,\"start\":63530},{\"end\":63533,\"start\":63532},{\"end\":63543,\"start\":63542},{\"end\":63825,\"start\":63824},{\"end\":63833,\"start\":63832},{\"end\":63844,\"start\":63843},{\"end\":63854,\"start\":63853},{\"end\":63865,\"start\":63864},{\"end\":63877,\"start\":63876},{\"end\":64134,\"start\":64133},{\"end\":64146,\"start\":64145},{\"end\":64157,\"start\":64156},{\"end\":64170,\"start\":64169},{\"end\":64178,\"start\":64177},{\"end\":64190,\"start\":64189},{\"end\":64200,\"start\":64199},{\"end\":64210,\"start\":64209},{\"end\":64221,\"start\":64220},{\"end\":64234,\"start\":64233},{\"end\":64236,\"start\":64235},{\"end\":64572,\"start\":64571},{\"end\":64584,\"start\":64583},{\"end\":64594,\"start\":64593},{\"end\":64605,\"start\":64604},{\"end\":64618,\"start\":64617},{\"end\":64620,\"start\":64619},{\"end\":64631,\"start\":64630},{\"end\":64640,\"start\":64639},{\"end\":64650,\"start\":64649},{\"end\":64659,\"start\":64658},{\"end\":65148,\"start\":65147},{\"end\":65160,\"start\":65156},{\"end\":65170,\"start\":65169},{\"end\":65181,\"start\":65180},{\"end\":65183,\"start\":65182},{\"end\":65538,\"start\":65537},{\"end\":65546,\"start\":65545},{\"end\":65554,\"start\":65553},{\"end\":65556,\"start\":65555},{\"end\":65562,\"start\":65561},{\"end\":65857,\"start\":65856},{\"end\":65866,\"start\":65865},{\"end\":65876,\"start\":65875},{\"end\":65886,\"start\":65885},{\"end\":65902,\"start\":65901},{\"end\":66329,\"start\":66328},{\"end\":66337,\"start\":66336},{\"end\":66347,\"start\":66346},{\"end\":66358,\"start\":66357},{\"end\":66365,\"start\":66364},{\"end\":66371,\"start\":66370},{\"end\":66379,\"start\":66378},{\"end\":66389,\"start\":66388},{\"end\":66707,\"start\":66706},{\"end\":66714,\"start\":66713},{\"end\":66718,\"start\":66715},{\"end\":66724,\"start\":66723},{\"end\":66732,\"start\":66731},{\"end\":66742,\"start\":66741},{\"end\":67071,\"start\":67070},{\"end\":67079,\"start\":67078},{\"end\":67087,\"start\":67086},{\"end\":67093,\"start\":67092},{\"end\":67097,\"start\":67094},{\"end\":67106,\"start\":67105},{\"end\":67427,\"start\":67426},{\"end\":67681,\"start\":67680},{\"end\":67689,\"start\":67688},{\"end\":67691,\"start\":67690},{\"end\":67705,\"start\":67704},{\"end\":67712,\"start\":67711},{\"end\":67714,\"start\":67713},{\"end\":67728,\"start\":67727},{\"end\":67730,\"start\":67729},{\"end\":68042,\"start\":68041},{\"end\":68044,\"start\":68043},{\"end\":68058,\"start\":68057},{\"end\":68060,\"start\":68059},{\"end\":68070,\"start\":68069},{\"end\":68079,\"start\":68078},{\"end\":68081,\"start\":68080},{\"end\":68094,\"start\":68093},{\"end\":68096,\"start\":68095},{\"end\":68106,\"start\":68105},{\"end\":68108,\"start\":68107},{\"end\":68116,\"start\":68115},{\"end\":68118,\"start\":68117},{\"end\":68128,\"start\":68127},{\"end\":68130,\"start\":68129},{\"end\":68142,\"start\":68138},{\"end\":68150,\"start\":68149},{\"end\":68152,\"start\":68151},{\"end\":68700,\"start\":68699},{\"end\":68710,\"start\":68709},{\"end\":68720,\"start\":68719},{\"end\":68736,\"start\":68735},{\"end\":68738,\"start\":68737},{\"end\":68749,\"start\":68748},{\"end\":68751,\"start\":68750},{\"end\":68765,\"start\":68764},{\"end\":68772,\"start\":68771},{\"end\":68781,\"start\":68780},{\"end\":68783,\"start\":68782},{\"end\":69340,\"start\":69339},{\"end\":69352,\"start\":69351},{\"end\":69364,\"start\":69363},{\"end\":69379,\"start\":69378},{\"end\":69393,\"start\":69392},{\"end\":69703,\"start\":69702},{\"end\":69712,\"start\":69711},{\"end\":69714,\"start\":69713},{\"end\":69723,\"start\":69722},{\"end\":69737,\"start\":69736},{\"end\":69749,\"start\":69748},{\"end\":69751,\"start\":69750}]", "bib_author_last_name": "[{\"end\":49729,\"start\":49725},{\"end\":49735,\"start\":49733},{\"end\":49746,\"start\":49739},{\"end\":49996,\"start\":49991},{\"end\":50007,\"start\":50002},{\"end\":50400,\"start\":50393},{\"end\":50413,\"start\":50404},{\"end\":50421,\"start\":50417},{\"end\":50434,\"start\":50427},{\"end\":50442,\"start\":50438},{\"end\":50784,\"start\":50780},{\"end\":50792,\"start\":50788},{\"end\":50808,\"start\":50796},{\"end\":50818,\"start\":50814},{\"end\":50829,\"start\":50822},{\"end\":50841,\"start\":50835},{\"end\":50851,\"start\":50845},{\"end\":51368,\"start\":51361},{\"end\":51377,\"start\":51372},{\"end\":51390,\"start\":51381},{\"end\":51403,\"start\":51394},{\"end\":51420,\"start\":51407},{\"end\":51923,\"start\":51914},{\"end\":51932,\"start\":51927},{\"end\":51941,\"start\":51936},{\"end\":51951,\"start\":51945},{\"end\":51959,\"start\":51955},{\"end\":51970,\"start\":51965},{\"end\":51982,\"start\":51976},{\"end\":51995,\"start\":51988},{\"end\":52006,\"start\":51999},{\"end\":52017,\"start\":52012},{\"end\":52455,\"start\":52447},{\"end\":52463,\"start\":52459},{\"end\":52469,\"start\":52467},{\"end\":52476,\"start\":52473},{\"end\":52862,\"start\":52857},{\"end\":52874,\"start\":52866},{\"end\":52885,\"start\":52878},{\"end\":53359,\"start\":53355},{\"end\":53375,\"start\":53363},{\"end\":53387,\"start\":53379},{\"end\":53400,\"start\":53391},{\"end\":53408,\"start\":53404},{\"end\":53418,\"start\":53412},{\"end\":53428,\"start\":53422},{\"end\":53916,\"start\":53909},{\"end\":53925,\"start\":53920},{\"end\":53934,\"start\":53929},{\"end\":53943,\"start\":53938},{\"end\":53953,\"start\":53947},{\"end\":54264,\"start\":54257},{\"end\":54278,\"start\":54268},{\"end\":54288,\"start\":54282},{\"end\":54303,\"start\":54295},{\"end\":54713,\"start\":54705},{\"end\":54723,\"start\":54717},{\"end\":54739,\"start\":54729},{\"end\":55027,\"start\":55023},{\"end\":55035,\"start\":55031},{\"end\":55042,\"start\":55039},{\"end\":55050,\"start\":55046},{\"end\":55056,\"start\":55054},{\"end\":55324,\"start\":55312},{\"end\":55333,\"start\":55328},{\"end\":55347,\"start\":55340},{\"end\":55693,\"start\":55685},{\"end\":55703,\"start\":55697},{\"end\":55714,\"start\":55707},{\"end\":55727,\"start\":55718},{\"end\":55739,\"start\":55731},{\"end\":55961,\"start\":55957},{\"end\":55973,\"start\":55965},{\"end\":56201,\"start\":56196},{\"end\":56218,\"start\":56207},{\"end\":56227,\"start\":56222},{\"end\":56511,\"start\":56505},{\"end\":56519,\"start\":56515},{\"end\":56527,\"start\":56523},{\"end\":56538,\"start\":56531},{\"end\":56790,\"start\":56784},{\"end\":56800,\"start\":56794},{\"end\":57175,\"start\":57168},{\"end\":57184,\"start\":57179},{\"end\":57194,\"start\":57188},{\"end\":57205,\"start\":57198},{\"end\":57216,\"start\":57211},{\"end\":57715,\"start\":57711},{\"end\":57722,\"start\":57719},{\"end\":57733,\"start\":57726},{\"end\":57746,\"start\":57737},{\"end\":57758,\"start\":57750},{\"end\":57772,\"start\":57762},{\"end\":57783,\"start\":57776},{\"end\":57793,\"start\":57787},{\"end\":57803,\"start\":57797},{\"end\":58061,\"start\":58056},{\"end\":58071,\"start\":58065},{\"end\":58081,\"start\":58075},{\"end\":58253,\"start\":58248},{\"end\":58524,\"start\":58519},{\"end\":58533,\"start\":58528},{\"end\":58544,\"start\":58539},{\"end\":58993,\"start\":58986},{\"end\":59003,\"start\":58997},{\"end\":59306,\"start\":59299},{\"end\":59315,\"start\":59310},{\"end\":59328,\"start\":59319},{\"end\":59586,\"start\":59578},{\"end\":59595,\"start\":59590},{\"end\":59607,\"start\":59599},{\"end\":59614,\"start\":59611},{\"end\":59622,\"start\":59618},{\"end\":59632,\"start\":59626},{\"end\":59642,\"start\":59636},{\"end\":59651,\"start\":59646},{\"end\":60043,\"start\":60038},{\"end\":60242,\"start\":60232},{\"end\":60549,\"start\":60543},{\"end\":60560,\"start\":60553},{\"end\":60569,\"start\":60564},{\"end\":60778,\"start\":60765},{\"end\":60788,\"start\":60782},{\"end\":60993,\"start\":60983},{\"end\":61010,\"start\":60997},{\"end\":61019,\"start\":61014},{\"end\":61025,\"start\":61023},{\"end\":61041,\"start\":61029},{\"end\":61050,\"start\":61045},{\"end\":61063,\"start\":61054},{\"end\":61073,\"start\":61067},{\"end\":61391,\"start\":61387},{\"end\":61403,\"start\":61395},{\"end\":61410,\"start\":61407},{\"end\":61422,\"start\":61414},{\"end\":61433,\"start\":61426},{\"end\":61443,\"start\":61437},{\"end\":61459,\"start\":61447},{\"end\":61469,\"start\":61463},{\"end\":61484,\"start\":61473},{\"end\":61787,\"start\":61780},{\"end\":61796,\"start\":61791},{\"end\":61813,\"start\":61800},{\"end\":62000,\"start\":61993},{\"end\":62009,\"start\":62004},{\"end\":62020,\"start\":62013},{\"end\":62032,\"start\":62024},{\"end\":62045,\"start\":62036},{\"end\":62482,\"start\":62477},{\"end\":62489,\"start\":62486},{\"end\":62500,\"start\":62495},{\"end\":62780,\"start\":62778},{\"end\":62790,\"start\":62786},{\"end\":62803,\"start\":62794},{\"end\":62812,\"start\":62807},{\"end\":63058,\"start\":63051},{\"end\":63071,\"start\":63064},{\"end\":63080,\"start\":63075},{\"end\":63090,\"start\":63084},{\"end\":63100,\"start\":63094},{\"end\":63113,\"start\":63106},{\"end\":63125,\"start\":63117},{\"end\":63136,\"start\":63129},{\"end\":63147,\"start\":63140},{\"end\":63159,\"start\":63151},{\"end\":63494,\"start\":63487},{\"end\":63507,\"start\":63500},{\"end\":63515,\"start\":63513},{\"end\":63528,\"start\":63519},{\"end\":63540,\"start\":63534},{\"end\":63549,\"start\":63544},{\"end\":63830,\"start\":63826},{\"end\":63841,\"start\":63834},{\"end\":63851,\"start\":63845},{\"end\":63862,\"start\":63855},{\"end\":63874,\"start\":63866},{\"end\":63884,\"start\":63878},{\"end\":64143,\"start\":64135},{\"end\":64154,\"start\":64147},{\"end\":64167,\"start\":64158},{\"end\":64175,\"start\":64171},{\"end\":64187,\"start\":64179},{\"end\":64197,\"start\":64191},{\"end\":64207,\"start\":64201},{\"end\":64218,\"start\":64211},{\"end\":64231,\"start\":64222},{\"end\":64244,\"start\":64237},{\"end\":64581,\"start\":64573},{\"end\":64591,\"start\":64585},{\"end\":64602,\"start\":64595},{\"end\":64615,\"start\":64606},{\"end\":64628,\"start\":64621},{\"end\":64637,\"start\":64632},{\"end\":64647,\"start\":64641},{\"end\":64656,\"start\":64651},{\"end\":64664,\"start\":64660},{\"end\":65154,\"start\":65149},{\"end\":65167,\"start\":65161},{\"end\":65178,\"start\":65171},{\"end\":65193,\"start\":65184},{\"end\":65543,\"start\":65539},{\"end\":65551,\"start\":65547},{\"end\":65559,\"start\":65557},{\"end\":65565,\"start\":65563},{\"end\":65863,\"start\":65858},{\"end\":65873,\"start\":65867},{\"end\":65883,\"start\":65877},{\"end\":65899,\"start\":65887},{\"end\":65909,\"start\":65903},{\"end\":66334,\"start\":66330},{\"end\":66344,\"start\":66338},{\"end\":66355,\"start\":66348},{\"end\":66362,\"start\":66359},{\"end\":66368,\"start\":66366},{\"end\":66376,\"start\":66372},{\"end\":66386,\"start\":66380},{\"end\":66398,\"start\":66390},{\"end\":66711,\"start\":66708},{\"end\":66721,\"start\":66719},{\"end\":66729,\"start\":66725},{\"end\":66739,\"start\":66733},{\"end\":66748,\"start\":66743},{\"end\":67076,\"start\":67072},{\"end\":67084,\"start\":67080},{\"end\":67090,\"start\":67088},{\"end\":67103,\"start\":67098},{\"end\":67112,\"start\":67107},{\"end\":67433,\"start\":67428},{\"end\":67686,\"start\":67682},{\"end\":67702,\"start\":67692},{\"end\":67709,\"start\":67706},{\"end\":67725,\"start\":67715},{\"end\":67737,\"start\":67731},{\"end\":68055,\"start\":68045},{\"end\":68067,\"start\":68061},{\"end\":68076,\"start\":68071},{\"end\":68091,\"start\":68082},{\"end\":68103,\"start\":68097},{\"end\":68113,\"start\":68109},{\"end\":68125,\"start\":68119},{\"end\":68136,\"start\":68131},{\"end\":68147,\"start\":68143},{\"end\":68160,\"start\":68153},{\"end\":68707,\"start\":68701},{\"end\":68717,\"start\":68711},{\"end\":68733,\"start\":68721},{\"end\":68746,\"start\":68739},{\"end\":68762,\"start\":68752},{\"end\":68769,\"start\":68766},{\"end\":68778,\"start\":68773},{\"end\":68790,\"start\":68784},{\"end\":69349,\"start\":69341},{\"end\":69361,\"start\":69353},{\"end\":69376,\"start\":69365},{\"end\":69390,\"start\":69380},{\"end\":69403,\"start\":69394},{\"end\":69709,\"start\":69704},{\"end\":69720,\"start\":69715},{\"end\":69734,\"start\":69724},{\"end\":69746,\"start\":69738},{\"end\":69755,\"start\":69752}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b0\"},\"end\":49916,\"start\":49661},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4724792},\"end\":50312,\"start\":49918},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16447573},\"end\":50713,\"start\":50314},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3337633},\"end\":51275,\"start\":50715},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52900092},\"end\":51829,\"start\":51277},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3610017},\"end\":52354,\"start\":51831},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206761818},\"end\":52787,\"start\":52356},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":195357258},\"end\":53262,\"start\":52789},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15599732},\"end\":53834,\"start\":53264},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":21329383},\"end\":54183,\"start\":53836},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207168299},\"end\":54653,\"start\":54185},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8088093},\"end\":54957,\"start\":54655},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207330385},\"end\":55256,\"start\":54959},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10137425},\"end\":55615,\"start\":55258},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15124992},\"end\":55896,\"start\":55617},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":214797870},\"end\":56150,\"start\":55898},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1275282},\"end\":56439,\"start\":56152},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7496536},\"end\":56730,\"start\":56441},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7248464},\"end\":57087,\"start\":56732},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14955348},\"end\":57656,\"start\":57089},{\"attributes\":{\"doi\":\"arXiv:1811.03604\",\"id\":\"b20\"},\"end\":58037,\"start\":57658},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1779661},\"end\":58197,\"start\":58039},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9890353},\"end\":58438,\"start\":58199},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":9658690},\"end\":58907,\"start\":58440},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":187547},\"end\":59227,\"start\":58909},{\"attributes\":{\"doi\":\"arXiv:1803.07728\",\"id\":\"b25\"},\"end\":59510,\"start\":59229},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3997350},\"end\":59981,\"start\":59512},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5690108},\"end\":60156,\"start\":59983},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":15757500},\"end\":60453,\"start\":60158},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5555257},\"end\":60736,\"start\":60455},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":877639},\"end\":60950,\"start\":60738},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1033682},\"end\":61338,\"start\":60952},{\"attributes\":{\"doi\":\"arXiv:1609.03499\",\"id\":\"b32\"},\"end\":61721,\"start\":61340},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12369376},\"end\":61952,\"start\":61723},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":118686970},\"end\":62394,\"start\":61954},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1241495},\"end\":62710,\"start\":62396},{\"attributes\":{\"doi\":\"arXiv:1908.07873\",\"id\":\"b36\"},\"end\":62997,\"start\":62712},{\"attributes\":{\"doi\":\"arXiv:1912.04977\",\"id\":\"b37\"},\"end\":63412,\"start\":62999},{\"attributes\":{\"doi\":\"arXiv:1610.05492\",\"id\":\"b38\"},\"end\":63771,\"start\":63414},{\"attributes\":{\"doi\":\"arXiv:1910.10252\",\"id\":\"b39\"},\"end\":64079,\"start\":63773},{\"attributes\":{\"doi\":\"arXiv:1902.01046\",\"id\":\"b40\"},\"end\":64499,\"start\":64081},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3833774},\"end\":65114,\"start\":64501},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":3586416},\"end\":65410,\"start\":65116},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":216284503},\"end\":65813,\"start\":65412},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":52983148},\"end\":66253,\"start\":65815},{\"attributes\":{\"doi\":\"arXiv:1812.02903\",\"id\":\"b45\"},\"end\":66629,\"start\":66255},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":213175498},\"end\":66962,\"start\":66631},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":216339535},\"end\":67357,\"start\":66964},{\"attributes\":{\"doi\":\"arXiv:1908.08681\",\"id\":\"b48\"},\"end\":67582,\"start\":67359},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":837298},\"end\":68039,\"start\":67584},{\"attributes\":{\"id\":\"b50\"},\"end\":68587,\"start\":68041},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":5687667},\"end\":69255,\"start\":68589},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":36911378},\"end\":69619,\"start\":69257},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":150380277},\"end\":70108,\"start\":69621}]", "bib_title": "[{\"end\":49987,\"start\":49918},{\"end\":50389,\"start\":50314},{\"end\":50776,\"start\":50715},{\"end\":51357,\"start\":51277},{\"end\":51910,\"start\":51831},{\"end\":52443,\"start\":52356},{\"end\":52853,\"start\":52789},{\"end\":53349,\"start\":53264},{\"end\":53905,\"start\":53836},{\"end\":54253,\"start\":54185},{\"end\":54699,\"start\":54655},{\"end\":55019,\"start\":54959},{\"end\":55308,\"start\":55258},{\"end\":55681,\"start\":55617},{\"end\":55953,\"start\":55898},{\"end\":56192,\"start\":56152},{\"end\":56499,\"start\":56441},{\"end\":56780,\"start\":56732},{\"end\":57162,\"start\":57089},{\"end\":58052,\"start\":58039},{\"end\":58242,\"start\":58199},{\"end\":58515,\"start\":58440},{\"end\":58982,\"start\":58909},{\"end\":59574,\"start\":59512},{\"end\":60034,\"start\":59983},{\"end\":60228,\"start\":60158},{\"end\":60539,\"start\":60455},{\"end\":60761,\"start\":60738},{\"end\":60979,\"start\":60952},{\"end\":61776,\"start\":61723},{\"end\":61989,\"start\":61954},{\"end\":62473,\"start\":62396},{\"end\":64569,\"start\":64501},{\"end\":65145,\"start\":65116},{\"end\":65535,\"start\":65412},{\"end\":65854,\"start\":65815},{\"end\":66704,\"start\":66631},{\"end\":67068,\"start\":66964},{\"end\":67678,\"start\":67584},{\"end\":68697,\"start\":68589},{\"end\":69337,\"start\":69257},{\"end\":69700,\"start\":69621}]", "bib_author": "[{\"end\":49731,\"start\":49721},{\"end\":49737,\"start\":49731},{\"end\":49748,\"start\":49737},{\"end\":49998,\"start\":49989},{\"end\":50009,\"start\":49998},{\"end\":50402,\"start\":50391},{\"end\":50415,\"start\":50402},{\"end\":50423,\"start\":50415},{\"end\":50436,\"start\":50423},{\"end\":50444,\"start\":50436},{\"end\":50786,\"start\":50778},{\"end\":50794,\"start\":50786},{\"end\":50810,\"start\":50794},{\"end\":50820,\"start\":50810},{\"end\":50831,\"start\":50820},{\"end\":50843,\"start\":50831},{\"end\":50853,\"start\":50843},{\"end\":51370,\"start\":51359},{\"end\":51379,\"start\":51370},{\"end\":51392,\"start\":51379},{\"end\":51405,\"start\":51392},{\"end\":51422,\"start\":51405},{\"end\":51925,\"start\":51912},{\"end\":51934,\"start\":51925},{\"end\":51943,\"start\":51934},{\"end\":51953,\"start\":51943},{\"end\":51961,\"start\":51953},{\"end\":51972,\"start\":51961},{\"end\":51984,\"start\":51972},{\"end\":51997,\"start\":51984},{\"end\":52008,\"start\":51997},{\"end\":52019,\"start\":52008},{\"end\":52457,\"start\":52445},{\"end\":52465,\"start\":52457},{\"end\":52471,\"start\":52465},{\"end\":52478,\"start\":52471},{\"end\":52864,\"start\":52855},{\"end\":52876,\"start\":52864},{\"end\":52887,\"start\":52876},{\"end\":53361,\"start\":53351},{\"end\":53377,\"start\":53361},{\"end\":53389,\"start\":53377},{\"end\":53402,\"start\":53389},{\"end\":53410,\"start\":53402},{\"end\":53420,\"start\":53410},{\"end\":53430,\"start\":53420},{\"end\":53918,\"start\":53907},{\"end\":53927,\"start\":53918},{\"end\":53936,\"start\":53927},{\"end\":53945,\"start\":53936},{\"end\":53955,\"start\":53945},{\"end\":54266,\"start\":54255},{\"end\":54280,\"start\":54266},{\"end\":54290,\"start\":54280},{\"end\":54305,\"start\":54290},{\"end\":54715,\"start\":54701},{\"end\":54725,\"start\":54715},{\"end\":54741,\"start\":54725},{\"end\":55029,\"start\":55021},{\"end\":55037,\"start\":55029},{\"end\":55044,\"start\":55037},{\"end\":55052,\"start\":55044},{\"end\":55058,\"start\":55052},{\"end\":55326,\"start\":55310},{\"end\":55335,\"start\":55326},{\"end\":55349,\"start\":55335},{\"end\":55695,\"start\":55683},{\"end\":55705,\"start\":55695},{\"end\":55716,\"start\":55705},{\"end\":55729,\"start\":55716},{\"end\":55741,\"start\":55729},{\"end\":55963,\"start\":55955},{\"end\":55975,\"start\":55963},{\"end\":56203,\"start\":56194},{\"end\":56220,\"start\":56203},{\"end\":56229,\"start\":56220},{\"end\":56513,\"start\":56501},{\"end\":56521,\"start\":56513},{\"end\":56529,\"start\":56521},{\"end\":56540,\"start\":56529},{\"end\":56792,\"start\":56782},{\"end\":56802,\"start\":56792},{\"end\":57177,\"start\":57164},{\"end\":57186,\"start\":57177},{\"end\":57196,\"start\":57186},{\"end\":57207,\"start\":57196},{\"end\":57218,\"start\":57207},{\"end\":57717,\"start\":57709},{\"end\":57724,\"start\":57717},{\"end\":57735,\"start\":57724},{\"end\":57748,\"start\":57735},{\"end\":57760,\"start\":57748},{\"end\":57774,\"start\":57760},{\"end\":57785,\"start\":57774},{\"end\":57795,\"start\":57785},{\"end\":57805,\"start\":57795},{\"end\":58063,\"start\":58054},{\"end\":58073,\"start\":58063},{\"end\":58083,\"start\":58073},{\"end\":58255,\"start\":58244},{\"end\":58526,\"start\":58517},{\"end\":58535,\"start\":58526},{\"end\":58546,\"start\":58535},{\"end\":58995,\"start\":58984},{\"end\":59005,\"start\":58995},{\"end\":59308,\"start\":59297},{\"end\":59317,\"start\":59308},{\"end\":59330,\"start\":59317},{\"end\":59588,\"start\":59576},{\"end\":59597,\"start\":59588},{\"end\":59609,\"start\":59597},{\"end\":59616,\"start\":59609},{\"end\":59624,\"start\":59616},{\"end\":59634,\"start\":59624},{\"end\":59644,\"start\":59634},{\"end\":59653,\"start\":59644},{\"end\":60045,\"start\":60036},{\"end\":60244,\"start\":60230},{\"end\":60551,\"start\":60541},{\"end\":60562,\"start\":60551},{\"end\":60571,\"start\":60562},{\"end\":60780,\"start\":60763},{\"end\":60790,\"start\":60780},{\"end\":60995,\"start\":60981},{\"end\":61012,\"start\":60995},{\"end\":61021,\"start\":61012},{\"end\":61027,\"start\":61021},{\"end\":61043,\"start\":61027},{\"end\":61052,\"start\":61043},{\"end\":61065,\"start\":61052},{\"end\":61075,\"start\":61065},{\"end\":61393,\"start\":61383},{\"end\":61405,\"start\":61393},{\"end\":61412,\"start\":61405},{\"end\":61424,\"start\":61412},{\"end\":61435,\"start\":61424},{\"end\":61445,\"start\":61435},{\"end\":61461,\"start\":61445},{\"end\":61471,\"start\":61461},{\"end\":61486,\"start\":61471},{\"end\":61789,\"start\":61778},{\"end\":61798,\"start\":61789},{\"end\":61815,\"start\":61798},{\"end\":62002,\"start\":61991},{\"end\":62011,\"start\":62002},{\"end\":62022,\"start\":62011},{\"end\":62034,\"start\":62022},{\"end\":62047,\"start\":62034},{\"end\":62484,\"start\":62475},{\"end\":62491,\"start\":62484},{\"end\":62502,\"start\":62491},{\"end\":62782,\"start\":62776},{\"end\":62792,\"start\":62782},{\"end\":62805,\"start\":62792},{\"end\":62814,\"start\":62805},{\"end\":63060,\"start\":63049},{\"end\":63073,\"start\":63060},{\"end\":63082,\"start\":63073},{\"end\":63092,\"start\":63082},{\"end\":63102,\"start\":63092},{\"end\":63115,\"start\":63102},{\"end\":63127,\"start\":63115},{\"end\":63138,\"start\":63127},{\"end\":63149,\"start\":63138},{\"end\":63161,\"start\":63149},{\"end\":63496,\"start\":63485},{\"end\":63509,\"start\":63496},{\"end\":63517,\"start\":63509},{\"end\":63530,\"start\":63517},{\"end\":63542,\"start\":63530},{\"end\":63551,\"start\":63542},{\"end\":63832,\"start\":63824},{\"end\":63843,\"start\":63832},{\"end\":63853,\"start\":63843},{\"end\":63864,\"start\":63853},{\"end\":63876,\"start\":63864},{\"end\":63886,\"start\":63876},{\"end\":64145,\"start\":64133},{\"end\":64156,\"start\":64145},{\"end\":64169,\"start\":64156},{\"end\":64177,\"start\":64169},{\"end\":64189,\"start\":64177},{\"end\":64199,\"start\":64189},{\"end\":64209,\"start\":64199},{\"end\":64220,\"start\":64209},{\"end\":64233,\"start\":64220},{\"end\":64246,\"start\":64233},{\"end\":64583,\"start\":64571},{\"end\":64593,\"start\":64583},{\"end\":64604,\"start\":64593},{\"end\":64617,\"start\":64604},{\"end\":64630,\"start\":64617},{\"end\":64639,\"start\":64630},{\"end\":64649,\"start\":64639},{\"end\":64658,\"start\":64649},{\"end\":64666,\"start\":64658},{\"end\":65156,\"start\":65147},{\"end\":65169,\"start\":65156},{\"end\":65180,\"start\":65169},{\"end\":65195,\"start\":65180},{\"end\":65545,\"start\":65537},{\"end\":65553,\"start\":65545},{\"end\":65561,\"start\":65553},{\"end\":65567,\"start\":65561},{\"end\":65865,\"start\":65856},{\"end\":65875,\"start\":65865},{\"end\":65885,\"start\":65875},{\"end\":65901,\"start\":65885},{\"end\":65911,\"start\":65901},{\"end\":66336,\"start\":66328},{\"end\":66346,\"start\":66336},{\"end\":66357,\"start\":66346},{\"end\":66364,\"start\":66357},{\"end\":66370,\"start\":66364},{\"end\":66378,\"start\":66370},{\"end\":66388,\"start\":66378},{\"end\":66400,\"start\":66388},{\"end\":66713,\"start\":66706},{\"end\":66723,\"start\":66713},{\"end\":66731,\"start\":66723},{\"end\":66741,\"start\":66731},{\"end\":66750,\"start\":66741},{\"end\":67078,\"start\":67070},{\"end\":67086,\"start\":67078},{\"end\":67092,\"start\":67086},{\"end\":67105,\"start\":67092},{\"end\":67114,\"start\":67105},{\"end\":67435,\"start\":67426},{\"end\":67688,\"start\":67680},{\"end\":67704,\"start\":67688},{\"end\":67711,\"start\":67704},{\"end\":67727,\"start\":67711},{\"end\":67739,\"start\":67727},{\"end\":68057,\"start\":68041},{\"end\":68069,\"start\":68057},{\"end\":68078,\"start\":68069},{\"end\":68093,\"start\":68078},{\"end\":68105,\"start\":68093},{\"end\":68115,\"start\":68105},{\"end\":68127,\"start\":68115},{\"end\":68138,\"start\":68127},{\"end\":68149,\"start\":68138},{\"end\":68162,\"start\":68149},{\"end\":68709,\"start\":68699},{\"end\":68719,\"start\":68709},{\"end\":68735,\"start\":68719},{\"end\":68748,\"start\":68735},{\"end\":68764,\"start\":68748},{\"end\":68771,\"start\":68764},{\"end\":68780,\"start\":68771},{\"end\":68792,\"start\":68780},{\"end\":69351,\"start\":69339},{\"end\":69363,\"start\":69351},{\"end\":69378,\"start\":69363},{\"end\":69392,\"start\":69378},{\"end\":69405,\"start\":69392},{\"end\":69711,\"start\":69702},{\"end\":69722,\"start\":69711},{\"end\":69736,\"start\":69722},{\"end\":69748,\"start\":69736},{\"end\":69757,\"start\":69748}]", "bib_venue": "[{\"end\":49719,\"start\":49661},{\"end\":50073,\"start\":50009},{\"end\":50493,\"start\":50444},{\"end\":50936,\"start\":50853},{\"end\":51499,\"start\":51422},{\"end\":52075,\"start\":52019},{\"end\":52544,\"start\":52478},{\"end\":52970,\"start\":52887},{\"end\":53525,\"start\":53430},{\"end\":53983,\"start\":53955},{\"end\":54373,\"start\":54305},{\"end\":54781,\"start\":54741},{\"end\":55085,\"start\":55058},{\"end\":55411,\"start\":55349},{\"end\":55745,\"start\":55741},{\"end\":56000,\"start\":55975},{\"end\":56278,\"start\":56229},{\"end\":56569,\"start\":56540},{\"end\":56879,\"start\":56802},{\"end\":57308,\"start\":57218},{\"end\":57707,\"start\":57658},{\"end\":58089,\"start\":58083},{\"end\":58304,\"start\":58255},{\"end\":58623,\"start\":58546},{\"end\":59043,\"start\":59005},{\"end\":59295,\"start\":59229},{\"end\":59721,\"start\":59653},{\"end\":60058,\"start\":60045},{\"end\":60283,\"start\":60244},{\"end\":60575,\"start\":60571},{\"end\":60828,\"start\":60790},{\"end\":61124,\"start\":61075},{\"end\":61381,\"start\":61340},{\"end\":61819,\"start\":61815},{\"end\":62124,\"start\":62047},{\"end\":62532,\"start\":62502},{\"end\":62774,\"start\":62712},{\"end\":63047,\"start\":62999},{\"end\":63483,\"start\":63414},{\"end\":63822,\"start\":63773},{\"end\":64131,\"start\":64081},{\"end\":64751,\"start\":64666},{\"end\":65244,\"start\":65195},{\"end\":65598,\"start\":65567},{\"end\":66009,\"start\":65911},{\"end\":66326,\"start\":66255},{\"end\":66781,\"start\":66750},{\"end\":67145,\"start\":67114},{\"end\":67424,\"start\":67359},{\"end\":67782,\"start\":67739},{\"end\":68273,\"start\":68162},{\"end\":68867,\"start\":68792},{\"end\":69419,\"start\":69405},{\"end\":69833,\"start\":69757},{\"end\":50124,\"start\":50075},{\"end\":51006,\"start\":50938},{\"end\":51563,\"start\":51501},{\"end\":53040,\"start\":52972},{\"end\":54428,\"start\":54375},{\"end\":56889,\"start\":56881},{\"end\":57385,\"start\":57310},{\"end\":58687,\"start\":58625},{\"end\":62188,\"start\":62126},{\"end\":64823,\"start\":64753},{\"end\":68929,\"start\":68869}]"}}}, "year": 2023, "month": 12, "day": 17}