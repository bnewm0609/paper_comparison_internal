{"id": 174788791, "updated": "2023-09-22 01:02:44.494", "metadata": {"title": "Second-Order Attention Network for Single Image Super-Resolution", "authors": "[{\"first\":\"Tao\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Jianrui\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Yongbing\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Shu-Tao\",\"last\":\"Xia\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 6, "day": 1}, "abstract": "Recently, deep convolutional neural networks (CNNs) have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SISR methods mainly focus on wider or deeper architecture design, neglecting to explore the feature correlations of intermediate layers, hence hindering the representational power of CNNs. To address this issue, in this paper, we propose a second-order attention network (SAN) for more powerful feature expression and feature correlation learning. Specifically, a novel train- able second-order channel attention (SOCA) module is developed to adaptively rescale the channel-wise features by using second-order feature statistics for more discriminative representations. Furthermore, we present a non-locally enhanced residual group (NLRG) structure, which not only incorporates non-local operations to capture long-distance spatial contextual information, but also contains repeated local-source residual attention groups (LSRAG) to learn increasingly abstract feature representations. Experimental results demonstrate the superiority of our SAN network over state-of-the-art SISR methods in terms of both quantitative metrics and visual quality.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2954930822", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/DaiCZXZ19", "doi": "10.1109/cvpr.2019.01132"}}, "content": {"source": {"pdf_hash": "a5bb321017f6be6ea3dc44b5c3cf72788c7334cb", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "919bb9e8c9f57a55d1c0f7659a4448cedb6f991e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a5bb321017f6be6ea3dc44b5c3cf72788c7334cb.txt", "contents": "\nSecond-order Attention Network for Single Image Super-Resolution\n\n\nTao Dai \nGraduate School at Shenzhen\nTsinghua University\nShenzhenChina\n\nPCL Research Center of Networks and Communications\nPeng Cheng Laboratory\nShenzhenChina\n\nJianrui Cai \nDepartment of Computing\nThe Hong Kong Polytechnic University\nHong KongChina\n\nYongbing Zhang cslzhang@comp.polyu.edu.hk \nGraduate School at Shenzhen\nTsinghua University\nShenzhenChina\n\nShu-Tao Xia \nGraduate School at Shenzhen\nTsinghua University\nShenzhenChina\n\nPCL Research Center of Networks and Communications\nPeng Cheng Laboratory\nShenzhenChina\n\nLei Zhang \nDepartment of Computing\nThe Hong Kong Polytechnic University\nHong KongChina\n\nDAMO Academy\nAlibaba Group\n\nSecond-order Attention Network for Single Image Super-Resolution\n10.1109/CVPR.2019.01132\nRecently, deep convolutional neural networks (CNNs)   have been widely explored in single image super-resolution (SISR) and obtained remarkable performance. However, most of the existing CNN-based SISR methods mainly focus on wider or deeper architecture design, neglecting to explore the feature correlations of intermediate layers, hence hindering the representational power of CNNs. To address this issue, in this paper, we propose a second-order attention network (SAN) for more powerful feature expression and feature correlation learning. Specifically, a novel trainable second-order channel attention (SOCA) module is developed to adaptively rescale the channel-wise features by using second-order feature statistics for more discriminative representations. Furthermore, we present a non-locally enhanced residual group (NLRG) structure, which not only incorporates non-local operations to capture long-distance spatial contextual information, but also contains repeated local-source residual attention groups (LSRAG) to learn increasingly abstract feature representations. Experimental results demonstrate the superiority of our SAN network over state-of-the-art SISR methods in terms of both quantitative metrics and visual quality. * The first two authors contribute equally to this work.\n\nIntroduction\n\nSingle image super-resolution (SISR) [5] has recently received much attention. In general, the purpose of SISR is to produce a visually high-resolution (HR) output from its low-resolution (LR) input. However, this inverse problem is ill-posed since multiple HR solutions can map to any LR input. Therefore, a great number of SR methods have been proposed, ranging from early interpolation-based [37] and model-based [4], to recent learning-based methods [32,39].\n\nThe early developed interpolated-based methods (e.g., bilinear and bicubic methods) are simple and efficient but limited in applications. For more flexible SR methods, more advanced model-based methods are proposed by exploiting powerful image priors, such as non-local similarity prior [34] and sparsity prior [4]. Although such model-based methods are flexible to produce relative high-quality HR images, they still suffer from some drawbacks: (1) such methods often involve a time-consuming optimization process; (2) the performance may degrade quickly when image statistics are biased from the image prior.\n\nDeep convolution neural networks (CNNs) have recently achieved unprecedented success in various problems [7,25]. The powerful feature representation and end-to-end training paradigm of CNN makes it a promising approach to SISR. In the last several years, a flurry of CNN-based SISR methods have been proposed to learn a mapping function from an interpolated or LR input to its corresponding HR output. By fully exploiting the image statics inherent in training datasets, CNNs have achieved state-of-the-art results in SISR [2,12,14,36,39,38]. Although considerable progress has been achieved in image SR, existing CNNbased SR models are still faced with some limitations: (1) most of CNN-based SR methods do not make full use of the information from the original LR images, thereby resulting in relatively-low performance; (2) most existing CNNbased SR models focus mainly on designing a deeper or wider network to learn more discriminative high-level features, while rarely exploiting the inherent feature correlations in intermediate layers, thus hindering the representational ability of CNNs.\n\nTo address these problems, we propose a deep secondorder attention network (SAN) for more powerful feature expression and feature correlation learning. Specifically, we (a) HR (b) FSRCNN (c) LapSRN [14] (d) SRMD [36] (e) EDSR [36] (f) DBPN [20] (g) RDN [6] (h) Ours Figure 1. Zoom visual results for 4\u00d7 SR on \"img 092\" from Urban100. Our method obtains better visual quality and recovers more image details compared with other state-of-the-art SR methods propose a second-order channel attention (SOCA) mechanism for better feature correlation learning. Our SOCA adaptively learns feature inter-dependencies by exploiting second-order feature statistics instead of first-order ones. Such SOCA mechnism makes our network focus on more informative feature and improve discriminative learning ability. Moreover, a non-locally enhanced residual group (NLRG) structure is presented to further incorporates nonlocal operations to capture long-distance spatial contextual information. By stacking the local-source residual attention groups (LSRAG) structure, we can exploit the information from the LR images and allow the abundant low-frequency information to be bypassed. As shown in Fig. 1, our method obtains better visual quality and recovers more image details compared with other state-of-the-art SR methods. In summary, the main contributions of this paper are listed as follows:\n\n\u2022 We propose a deep second-order attention network (SAN) for accurate image SR. Extensive experiments on public datasets demonstrate the superiority of our SAN over state-of-the-art methods in terms of both quantitive and visual quality.\n\n\u2022 We propose second-order channel attention (SOCA) mechanism to adaptively rescale features by considering feature statistics higher than first-order. Such SOCA mechanism allows our network to focus on more informative features and enhance discriminative learning ability. Besides, we also utilize an iterative method for covariance normalization to speed up the training of our network.\n\n\u2022 We propose non-locally enhanced residual group (NLRG) structure to build a deep network, which further incorporates non-local operations to capture spatial contextual information, and share-source residual group structure to learn deep features. Besides, the share-source residual group structure through sharesource skip connections could allow more abundant information from the LR input to be bypassed and ease the training of the deep network.\n\n\nRelated Work\n\nDuring the past decade, a plenty of image SISR methods have been proposed in the computer vision community, including interpolation-based [37], model-based [34], and CNN-based methods [2,29,14,13,29,17,30,39,38]. Due to space limitation, we here briefly review works related to CNN-based SR methods and attention mechanism, which is close to our method. CNN-based SR models. Recently, CNN-based methods have been extensively studied in image SR, due to their strong nonlinear representational power. Generally, such methods cast SR as an image-to-image regression problem, and learn an end-to-end mapping from LR to HR directly. Most existing CNN-based methods mainly focus on designing a deeper or wider network structure [2,12,13,6,39,38]. For example, Dong et al. [2] first introduced a shallow three-layer convolutional network (SRCNN) for image SR, which achieves impressive performance. Later, Kim et al. designed deeper VDSR [12] and DRCN [13] with more than 16 layers based on residual learning. To further improve the performance, Lim et al. [20] proposed a very deep and wide network EDSR by stacking modified residual blocks. The significant performance gain indicates the depth of representation plays a key role in image SR. Other recent works like MemNet [30] and RDN [39], are based on dense blocks [10] to form deep networks and focus on utilizing all the hierarchical features from all the convolutional layers. In addition to focusing on increasing the depth of the network, some other networks, such as NLRN [22] and RCAN [38], improve the performance by considering feature correlations in spatial or channel dimension. Attention mechanism. Attention in human perception generally means that human visual systems adaptively process visual information and focus on salient areas [16]. In recent years, several trials have embeded attention processing to improve the performance of CNNs for various tasks, such as image and video classification tasks [9,33]. Wang et al. [33] proposed non-local neural network to incorporate non-local operations for spatial attention in video classification. On the contrary, Hu et al. [9] proposed SENet to exploit channel-wise relationships to achieve significant performance gain for image classification.\n\nRecently, SENet was introduced to deep CNNs to further improve SR performance [38]. However, SENet only explores first-order statistics (e.g., global average pooling), while ignoring the statistics higher than first-order, thus hindering the discriminative ability of the network. In im-age SR, features with more high-frequency information are more informative for HR reconstruction. To this end, we propose a deep second-order attention network (SAN) by exploring second-order statistics of features.\n\n\nSecond-order Attention Network (SAN)\n\n\nNetwork Framework\n\nAs shown in Fig. 2, our SAN mainly consists of four parts: shallow feature extraction, non-locally enhanced residual group (NLRG) based deep feature extraction, upscale module, and reconstruction part. Given I LR and I SR as the input and output of SAN. As explored in [20,39], we apply only one convolutional layer to extract the shallow feature F 0 from the LR input\nF 0 = H SF (I LR ),(1)\nwhere H SF (\u00b7) stands for convolution operation. Then the extracted shallow feature F 0 is used for NLRG based deep feature extraction, which thus produces the deep feature as\nF DF = H NLRG (F 0 ),(2)\nwhere H NLRG represents the NLRG based deep feature extraction module, which consists of several non-local modules to enlarge receptive field and G local-source residual attention group (LSRAG) modules (see Fig. 2). So our proposed NLRG obtains very deep depth and thus provides very large receptive field size. Then the extracted deep feature F DF is upscaled via the upscale module via\nF \u2191 = H \u2191 (F DF ),(3)\nwhere H \u2191 (\u00b7) and F \u2191 are a upscale module and upscaled feature respectively. There are some choices to act as upscale part, such as transposed convolution [3], ESPCN [28]. The way of embedding upscaling feature in the last few layers obtains a good trade off between computational burden and performance, and thus is preferable to be used in recent CNN-based SR models [3,6,39]. The upscaled feature is then mapped into SR image via one convolution layer\nI SR = H R (F \u2191 ) = H SAN (I LR ),(4)\nwhere H R (\u00b7), H \u2191 (\u00b7) and H SAN are the reconstruction layer, upscale layer and the function of SAN, respectively. Then SAN will be optimized with a certain loss function. Some loss functions have been widely used, such as L 2 [2,12,29,30], L 1 [14,15,20,39], perceptual losses [11,26]. To verify the effectiveness of our SAN, we adopt the same loss functions as previous works (e.g., L 1 loss function). Given a training set with N LR images and their HR counterparts denoted by\n{I i LR , I i HR } N i=1\n, the goal of training SAN is to optimize the L 1 loss function:\nL(\u0398) = 1 N N i=1 ||H SAN (I i LR ) \u2212 I i HR || 1 ,(5)\nwhere \u0398 denotes the parameter set of SAN. The loss function is optimized by stochastic gradient descent algorithm.\n\n\nNon-locally Enhanced Residual Group (NLRG)\n\nWe now show our non-locally enhanced residual group (NLRG) (see Fig. 2), which consists of several region-level non-local (RL-NL) modules and one share-source residual group (SSRG) structure. The RL-NL exploits the abundant structure cues in LR features and the self-similarities in HR nature scenes. The SSRG is composed of G local-source residual attention groups (LSRAG) with share-source skip connections (SSC). Each LSRAG further contains M simplified residual blocks with local-source skip connection, followed by a second-order channel attention (SOCA) module to exploit feature interdependencies.\n\nIt has been verified that stacking residual blocks is helpful to form a deep CNN in [20,39]. However, very deep network built in such way would suffer from training difficulty and performance bottleneck due to the problem of gradient vanishing and exploding in deep network. Inspired by the work in [15], we propose local-source residual attention group (LSRAG) as the fundamental unit. It is known that simply stacking repeated LSRAGs would fail to obtain better performance. To address this issue, the share-source skip connection (SSC) is introduced in NLRG to not only facilitate the training of our deep network, but also to bypass abundant low-frequency information from LR images. Then a LSRAG in the g-th group is represented as:\nF g = W SSC F 0 + H g (F g\u22121 ),(6)\nwhere W SSC denotes the weight to the convolution layer, and is initialized as 0, and then gradually learns to assign more weight to the shallow feature. The bias term is omitted for simplicity. H g (\u00b7) is the function of the g-th LSRAG. F g , F g\u22121 denote the input and output of the g-th LSRAG. The deep feature is then obtained as:\nF DF = W SSC F 0 + F G .(7)\nSuch SSRG structure can not only ease the flow of information across LSRAGs, but also make it possible to train very deep CNN for image SR with high performance. Region-level non-local module (RL-NL). The proposed NLRG also exploits the abundant structure cues in LR features and the self-similarities in HR nature scenes by RL-NL modules plugged before and after the SSRG. The nonlocal neural network [33] is proposed to capture the computation of long-range dependencies throughout the entire image for high-level tasks. However, traditional globallevel non-local operations may be limited for some reasons: 1) global-level non-local operations require unacceptable computational burden, especially when the size of feature is large; 2) it is empirically shown that non-local operations at a proper neighborhood size are preferable for low-level tasks (e.g., image super-resolution) [22]. Thus for feature with higher spatial resolution or degradation, it is natural to perform region-level non-local operations. For such reasons, we divide the feature map into a grid of regions (see Fig. 2, the k \u00d7 k RL-NL indicates the input feature is first divided into a grid of k 2 blocks with the same size.), each of which is then processed by the subsequent layers. After non-local operations, the feature representation is non-locally enhanced before fedding into the subsequent layers via exploiting the spatial correlations of features. Local-source residual attention group (LSRAG). Due to our share-source skip connections, the abundant lowfrequency information can be bypassed. To go a further step to residual learning, we stack M simplified residual blocks to form a basic LSRAG. The m-th residual block (see Fig. 2) in the g-th LSRAG can be represented as\nF g,m = H g,m (F g,m\u22121 ),(8)\nwhere H g,m (\u00b7) denotes the function of m-th residual block in g-th LSRAG, and F g,m\u22121 , F g,m are the corresponding input and output. To make our network focus on more informative features, a local-source skip connection is used to produce the block output via\nF g = W g F g\u22121 + F g,M ,(9)\nwhere W g is the corresponding weight. Such local-source and share-source skip connections allow more abundant low-frequency information to be bypassed during training. For more discriminative representations, we propose SOCA mechnism embedded at the tail of each LSRAG. Our SOCA mechnism learns to adaptively rescale channel-wise features by considering second-order statistics of features.\n\n\nSecond-order Channel Attention (SOCA)\n\nMost previous CNN-based SR models do not consider the feature interdependencies. To utilize such information, SENet [9] was introduced in CNNs to rescale the channelwise features for image SR. However, SENet only exploits first-order statistics of features by global average pooling, while ignoring statistics higher than first-order, thus hindering the discriminative ability of the network. On the other hand, recent works [19,21] have shown that second-order statistics in deep CNNs are more helpful for more discriminative representations than first-order ones.\n\nInspired by the above observations, we propose a second-order channel attention (SOCA) module to learn feature interdependencies by considiering second-order statistics of features. Now we will describe how to exploit such second-order information next. Covariance normalization. Given a H \u00d7 W \u00d7 C feature map F = [f 1 , \u00b7 \u00b7 \u00b7 , f C ] with C feature maps with size of H \u00d7 W . We reshape the feature map to a feature matrix X with s = W H features of C-dimension. Then the sample covariance matrix can be computed as\n\u03a3 = X\u012aX T ,(10)\nwhere\u012a = 1 s (I \u2212 1 s 1), I and 1 are the s \u00d7 s identity matrix and matrix of all ones, respectively.\n\nIt is shown in [27,19] that covariance normalization plays a critical role for more discriminative representations. For this reason, we first perform covariance normalization for the obtained covariance matrix \u03a3, which is symmetric positive semi-definite and thus has eigenvalue decomposition (EIG) as follows\n\u03a3 = U\u039bU T ,(11)\nwhere U is an orthogonal matrix and \u039b = diag(\u03bb 1 , \u00b7 \u00b7 \u00b7 , \u03bb C ) is diagonal matrix with eigenvalues in non-increasing order. Then convariance normalization can be converted to the power of eigenvalues:\nY = \u03a3 \u03b1 = U\u039b \u03b1 U T ,(12)\nwhere \u03b1 is a positive real number, and \u039b \u03b1 = diag(\u03bb \u03b1 1 , \u00b7 \u00b7 \u00b7 , \u03bb \u03b1 C ). When \u03b1 = 1, there is no normalization; when \u03b1 < 1, it nonlinearly shrinks the eigenvalues larger than 1.0 and streches those less than 1.0. As explored in [19], \u03b1 = 1/2 works well for more discriminative representations. Thus, we set \u03b1 = 1/2 in the following. Channel attention. The normalized covariance matrix characterizes the correlations of channel-wise features. We then take such normalized covariance matrix as a channel descriptor by global covariance pooling. As illustrated in Fig. 2, let\u0176 = [y 1 , \u00b7 \u00b7 \u00b7 , y C ], the channel-wise statistics z \u2208 R C\u00d71 can be obtained by shrinking\u0176. Then the c-th dimension of z is computed as\nz c = H GCP (y c ) = 1 C C i y c (i),(13)\nwhere H GCP (\u00b7) denotes the global covariance pooling function. Compared with the commonly used first-order pooling (e.g., global average pooling), our global covariance pooling explores the feature distribution and captures the feature statistics higher than first-order for more discriminative representations.\n\nTo fully exploit feature interdependencies from the aggregated information by global covariance pooling, we apply a gating mechanism. As explored in [9], the simple sigmoid function can serve as a proper gating function\nw = f (W U \u03b4(W D z)),(14)\nwhere W D and W U are the weight set of convolution layer, which set channel dimension of features to C/r and C, respectively. f (\u00b7) and \u03b4(\u00b7) are the function of sigmoid and RELU. Finally, we obtain the channel attention map w to rescale the inputf\nc = w c \u00b7 f c ,(15)\nwhere w c and f c denote the scaling factor and feature map in the c-th channel. With such channel attention, the residual component in the LSRAG is rescaled adaptively. As is shown above, covariance normalization plays a vital role in our SOCA. However, such covariance normalization relies heavily on eigenvalue decoomposition, which is not well supported on GPU platform, thus leading to inefficient training. To solve this issue, as explored in [18], we also apply a fast matrix normalization method based on Newton-Schulz iteration [8]. In the next section, we briefly describe the covariance normalization.\n\n\nCovariance Normalization Acceleration\n\nTo date, fast implementation of EIG on GPU is still an open problem. Inspired by [18], we utilize Newton-Schulz iteration to speed up the computation of covariance normalization. Specifically, from Equ. (11), the \u03a3 has square root as \u03a3 1/2 = Y = Udiag(\u03bb 1/2 i )U T . Given Y 0 = \u03a3, Z 0 = I, for n = 1, \u00b7 \u00b7 \u00b7 , N, as shown in [18], the Newton-Schulz iteration is then updated alternately as follows:\nY n = 1 2 Y n\u22121 (3I \u2212 Z n\u22121 Y n\u22121 ), Z n = 1 2 (3I \u2212 Z n\u22121 Y n\u22121 )Z n\u22121 .(16)\nAfter enough iterations, Y n and Z n quadratically converges to Y and Y \u22121 . Such iterative operation is suitable for parallel implementation on GPU. In practice, one can achieve approximate solution with few iterations, e.g., no more than 5 iterations in our method. Since Newton-Schulz iteration only converges locally, to guarantee the convergence, we pre-normalize \u03a3 first vi\u00e2\n\u03a3 = 1 tr(\u03a3) \u03a3,(17)\nwhere tr(\u03a3) = C i \u03bb i denotes the trace of \u03a3. In such case, it can be inferred that the ||\u03a3 \u2212 I|| 2 equals to the largest singular value of (\u03a3 \u2212 I), i.e., 1 \u2212 \u03bbi i \u03bbi) less than 1, which thus satisfies the convergence condition.\n\nAfter Newton-Schulz iteration, we apply a postcompensation procedure to compensate the data magnitude caused by pre-normalization, thus producing the final normalized covariance matrix\nY = tr(\u03a3)Y N .(18)\n\nImplementations\n\nWe set LSRAG number as G = 20 in the SSRG structure, and embed RL-NL modules (k = 2) at the head and tail of SSRG. In each LSRAG, we use m = 10 residual blocks plus single SOCA module at the tail. In SOCA module, we use 1 \u00d7 1 convolution filter with reduction ratio r = 16. For other convolution filter outside SOCA, the size and number of filter are set as 3 \u00d7 3 and C = 64, respectively. For upscale part H \u2191 (\u00b7), we follow the works in [20,39] and apply ESPCNN [28] to upscale the deep features, followed by one final convolution layer with three filters to produce color images (RGB channels).\n\n\nDiscussions\n\nDifference to Non-local RNN (NLRN). NLRN [22] introduces non-local operations to capture long-distance spatial contextual information in image restoration. There are some differences between NLRN and our SAN. First, NLRN embeds non-local operations in a recurrent neural network (RNN) for image restoration, while our SAN incorporates non-local operations in deep convolutional neural network (CNN) framework for image SR. Second, NLRN only considers spatial feature correlations between each location and its neighborhood, but ignores the channelwise feature correlations. While our SAN mainly focuses on learning such channel-wise feature correlations with second-order statistics of features for more powerful representational ability. Difference to Residual Dense Network (RDN). We summarize the main differences between RDN [39] and our SAN. The first one is the design of basic block. RDN mainly combines dense blocks with local feature fusion by using local residual learning, while our SAN is built on the basis of residual blocks. The second one is the way of enhancing discriminative ability of the network. Channel attention [9,38] has been shown to be effective for better discriminative representations. However, RDN does not consider such information, but pays attention to exploiting the hierarchical features from all the convolutional layers. On the contrary, our SAN heavily relies on channel attention for better discriminative representations. Thus, we propose second-order channel attention (SOCA) mechanism to effectively learn channel-wise feature interdependencies. Difference to Residual Channel Attention Network (RCAN). Zhang et al. [38] proposed a residual in residual structure to form a very deep network. RCAN is close to our SAN, and the main differences lie in the following aspects. First, RCAN consists of several residual groups with long skip connections. While, SAN stacks repeated residual groups through share-source skip connections, which allows more abundant low-frequency information to be bypassed. Second, RCAN can only exploit the contextual information in a local receptive field, but is unable to exploit the information outside of the local region. While SAN can alleviate this problem by incorporating non-local operations to not only capture long-distance spatial contextual information, but enlarge the receptive field. Third, to enhance the discriminative ability of the network, RCAN only considers channel attention based first-order feature statistics by global average pooling. While our SAN learns channel attention based on second-order feature statistics.\n\nTo the best of our knowledge, it is the first attempt to investigate the effect of such attention based on second-order feature statistics for image SR. More analysis about the effect of such attention mechanism are shown next.\n\n\nExperiments\n\n\nSetup\n\nFollowing [20,6,39,38], we use 800 high-resolution images from DIV2K dataset [31] as training set. For testing, we adopt 5 standard benchmark datasets: Set5, Set14, BSD100, Urban100 and Manga109, each of which has different characteristics. We carry out experiments with Bicubic (BI) and Blur-downscale (BD) degradation models [36]. All the SR results are evaluated by PSNR and SSIM metrics on Y channel of transformed YCbCr space.\n\nDuring training, we augment the training images by randomly rotating 90 \u2022 , 180 \u2022 , 270 \u2022 and horizontally flipping. In each min-batch, 8 LR color patches with size 48 \u00d7 48 are provided as inputs. Our model is trained by ADAM optimizor with \u03b2 1 = 0.9, \u03b2 2 = 0.99, and \u03b5 = 10 \u22128 . The learning rate is initialized as 10 \u22124 and then reduced to half every 200 epochs. Our proposed SAN has been implemented on the Pytorch framework [23] on an Nvidia 1080Ti GPU.\n\n\nAblation Study\n\nAs discussed in Section 3, our SAN contains two main components including non-locally enhanced residual group (NLRG) and second-order channel attention (SOCA). Non-locally Enhanced Residual Group (NLRG). To verify the effectiveness of different modules, we compare NLRG with its variants trained and tested on Set5 dataset. The specific performance is listed in Table 1.\n\nBase refers to a very basic baseline which only contains the convolution layers with 20 LSRAGs and 10 residual blocks in each LSRAG, thus resulting in deep network with over 400 convolution layers. As in [38], we also add long and short skip connections in Base model. From Table 1 we can see that Base reaches PSNR=32.00 dB on Set5 (\u00d74). Results from R a to R e verify the effectiveness of individual module, since the module used alone improves the performance over Base model. Specifically, R a and R b that add a single RL-NL in shallow (before SSRG) or deep layers (after SSRG) obtain similar SR results and outperform Base, which verifies the effectiveness of RL-NL. When share-source skip connection (SSC) is added alone (R c ), the performance can be improved from 32.00 dB to 32.07 dB. The main reason lies in that share-source skip connections allows more abundant low-frequency information from the LR images to be bypassed. When both of R a and R b are used (leading to R f ), the performance can be further improved. It is found more RL-NL modules cannot obtain much better performance than R f in our method, and thus we apply R f in our method to balance the performance and efficiency. Second-order channel attention (SOCA). We also show the effect of our SOCA from the results of R d , R e , R h and R i . Specifically, R d means that channel attention is based on first-order feature statistics by global average pooling, thus leading to first-order channel attention (FOCA). R e means that channel attention is based on second-order feature statistics, thus leading to our second-order channel attention (SOCA). It can be found that both of R d and R e obtain better performance than methods of R a to R c with-  \n\n\nResults with Bicubic Degradation (BI)\n\nTo test the effectiveness of our SAN, we compare our SAN with 11 state-of-the-art CNN-based SR methods: SR-CNN [1], FSRCNN [3], VDSR [12], LapSRN [14], Mem-Net [30], EDSR [20], SRMD [36], NLRN [22], DBPN [6], RDN [39] and RCAN [38]. As in [20,39,38], we also adopt self-ensemble method to further improve our SAN denoted as SAN+. All the quantitative results for various scaling factors are reported in Table 2. Compared with other methods, our SAN+ performs the best results on all the datasets on various scaling factors. Without self-ensemble, SAN and RCAN obtain very similar results and outperform other methods. This is mainly because both of them adopt channel attention to learn feature interdependencies, thus making the network focus on more informative features.\n\nCompared with RCAN, our SAN obtains better results for datasets (e.g., such as Set5, Set14 and BSD100) with rich texture information, while obtaining a little worse results for datasets(e.g., Urban100 and Manga109) with rich repeated edge information. It is known that textures are highorder patterns and have more complex statistic characteristics, while edges are first-order patterns that can be extracted by first-order gradient operators. Thus our SOCA based on second-order feature statistics works better on images with more high-order information (e.g., textures).\n\nVisual quality. We also show the zoomed results of various methods in Fig. 3, from which we can see that most compared SR models cannot reconstruct the lattices accurately and suffer from serious blurring artifact. In contrast, our SAN obtains sharper results and recovers more highfrequency details, such as high contrast and sharp edges. Take \"img 076\" for example, most compared methods output heavy blurring artifacts. The early developed bicubic, SRCNN, FSRCNN and LapSRN even lose the main structure. More recent methods (e.g., EDSR, DBPN and RDN) can recover the main outlines but fail to recover more image details. Compared with the ground-truth, RCAN and SAN obtain more faithful results and recover more image details, but SAN has sharper results. These observations verify the superiority of SAN with more powerful representational ability. Although the recovery of high-frequency information is difficult due to limited information available in LR input (scaling factor > 4\u00d7), our SAN can still make full use of the limited LR information through share-source skip connections, and simultaneously utilize both spatial and channel feature correlations for more powerful feature expressions, thus producing more finer results.\n\n\nResults with Blur-downscale Degradation (BD)\n\nFollowing [36,39], we also compare various SR methods on image with blur-down degradation (BD) model. We compare our method with 8 state-of-the-art SR methods: SPMSR [24], SRCNN [2], FSRCNN [3], VDSR [12], IR-CNN [35], SRMD [36], RDN [39], and RCAN [38]. All the results on 3\u00d7 are shown in Table 3, from which we can observe that our SAN achieves consistently better performance than other methods even without self-ensemble. Specifically, the PSNR gain of SAN over RDN is up to 0.4 dB on Urban100 and Manga109 datasets.\n\n\nModel Size Analyses\n\nThe Table 4 shows the performance and model size of recent deep CNN-based SR methods. Among these methods, MemNet and NLRG contain much less parameters at the cost of performance degradation. Instead, our SAN has lless parameters than RDN and RCAN, but obtains higher performance, which implies that our SAN can have a good trade-off between performance and model complexity.\n\n\nConclusions\n\nWe propose a deep second-order attention network (SAN) for accurate image SR. Specifically, the non-locally enhanced residual group (NLRG) structure allows SAN to capture the long-distance dependencies and structural information by embedding non-local operations in the network. Meanwhile, NLRG allows abundant low-frequency information from the LR images to be bypassed through sharesource skip connections. In addition to exploiting the spatial feature correlations, we propose second-order channel attention (SOCA) module to learn feature interdependencies by global covariance pooling for more discriminative representations. Extensive experiments on SR with BI and BD degradation models show the effectiveness of our SAN in terms of quantitative and visual results.\n\nFigure 2 .\n2Framework of the proposed second-order attention network (SAN) and its sub-modules.\n\nFigure 3 .\n3Visual comparison for 4\u00d7 SR with BI model on Urban100 dataset. The best results are highlighted out channel attention. This indicates that channel attention plays a more important role in determining the performance. Furthermore, compared with FOCA, our SOCA achieves consistently better results, no matter if combined with other modules (e.g., RL-NL and SSC). These observations demonstrate the superiority of our SOCA.\n\nTable 1 .\n1Effects of different modules. We report the best PSNR (dB) values on Set5 (4\u00d7) in 5.6 \u00d7 10 5 iterations.Base \nRa \nR b \nRc \nR d \nRe \nR f \nRg \nR h \nRi \nRL-NL(before SSRG) \nRL-NL(after SSRG) \nshare-source skip connection (SSC) \nFirst-order channel attention (FOCA) \nSecond-order channel attention (SOCA) \n32.00 \n32.04 32.06 32.07 32.12 32.16 32.08 32.10 32.14 32.20 \n\nUrban100 (4\u00d7): \nimg 067 \n\nHR \nBicubic \nSRCNN [2] \nFSRCNN [3] \nLapSRN [14] \nPSNR/SSIM \n17.02/0.7101 \n18.39/0.8023 \n18.21/0.7994 \n18.66/0.8406 \n\nEDSR [20] \nDBPN [6] \nRDN [39] \nRCAN [38] \nSAN \n21.17/0.9052 \n20.31/0.8910 \n20.87/0.9023 \n21.29/0.9127 \n21.34/0.9081 \n\nUrban100 (4\u00d7): \nimg 076 \n\nHR \nBicubic \nSRCNN [2] \nFSRCNN [3] \nLapSRN [14] \nPSNR/SSIM \n21.59/0.6325 \n22.5619/0.7316 \n22.0382/0.6807 \n22.03/0.6948 \n\nEDSR [20] \nDBPN [6] \nRDN [39] \nRCAN [38] \nSAN \n23.95/0.7750 \n23.21/0.7455 \n24.08/0.7801 \n24.30/0.7896 \n24.53/0.7925 \n\n\n\nTable 2 .\n2Quantitative results with BI degradation model.33.82/ .9227 29.87/ .8320 28.82/ .7980 27.07/ .8280 32.21/ .9350 MemNet 3 34.09/ .9248 30.01/ .8350 28.96/ .8001 27.56/ .8376 32.51/ .30.48/ .8628 27.50/ .7513 26.90/ .7101 24.52/ .7221 27.58/ .8555 FSRCNN 4 30.72/ .8660 27.61/ .7550 26.98/ .7150 24.62/ .7280 27.90/ .8610 VDSR 4 31.35/ .8830 28.02/ .7680 27.29/ .0726 25.18/ .7540 28.83/ .8870 LapSRN 4 31.54/ .8850 28.19/ .7720 27.32/ .7270 25.21/ .7560 29.09/ .8900 MemNet 4 31.74/ .8893 28.26/ .7723 27.40/ .7281 25.50/ .7630 29.42/ .8942 EDSR 4 32.46/ .8968 28.80/ .7876 27.71/ .7420 26.64/ .8033 31.02/ .9148 SRMD 4 31.96/ .8925 28.35/ .7787 27.49/ .7337 25.68/ .7731 30.09/ .9024 DBPN 4 32.47/ .8980 28.82/ .7860 27.72/ .7400 26.38/ .7946 30.91/ .9137 NLRG 4 31.92/ .8916 28.36/ .7745 27.48/ .7346 25.79/ .7729 \u2212\u2212/ \u2212\u2212 RDN 4 32.47/ .8990 28.81/ .7871 27.72/ .7419 26.61/ .8028 31.00/ .9151 RCAN 4 32.62/ .9001 28.86/ .7888 27.76/ .7435 26.82/ .8087 31.21/ .25.33/ .6900 23.76/ .5910 24.13/ .5660 21.29/ .5440 22.46/ .6950 FSRCNN 8 20.13/ .5520 19.75/ .4820 24.21/ .5680 21.32/ .5380 22.39/ .6730 SCN 8 25.59/ .7071 24.02/ .6028 24.30/ .5698 21.52/ .5571 22.68/ .6963 VDSR 8 25.93/ .7240 24.26/ .6140 24.49/ .5830 21.70/ .5710 23.16/ .7250 LapSRN 8 26.15/ .7380 24.35/ .6200 24.54/ .5860 21.81/ .5810 23.39/ .7350 MemNet 8 26.16/ .7414 24.38/ .6199 24.58/ .5842 21.89/ .5825 23.56/ .7387 MSLap 8 26.34/ .7558 24.57/ .6273 24.65/ .5895 22.06/ .5963 23.90/ .7564 EDSR 8 26.96/ .7762 24.91/ .6420 24.81/ .5985 22.51/ .6221 24.69/ .7841 DBPN 8 27.21/ .7840 25.13/ .6480 24.88/ .6010 22.73/ .6312 25.14/ .7987 SAN 8 27.22/ .7829 25.14/ .6476 24.88/ .6011 22.70/ .6314 24.85/ .7906 SAN+ 8 27.30/ .7849 25.23/ .6493 24.97/ .6031 22.91/ .6369 25.17/ .7964Method \nSet5 \nSet14 \nBSD100 \nUrban100 \nManga109 \n\nPSNR/ SSIM \nPSNR/ SSIM \nPSNR/ SSIM \nPSNR/ SSIM \nPSNR/ SSIM \n\nBicubic \n2 33.66/ .9299 30.24/ .8688 29.56/ .8431 26.88/ .8403 30.80/ .9339 \nSRCNN 2 36.66/ .9542 32.45/ .9067 31.36/ .8879 29.50/ .8946 35.60/ .9663 \nFSRCNN 2 37.05/ .9560 32.66/ .9090 31.53/ .8920 29.88/ .9020 36.67/ .9710 \nVDSR \n2 37.53/ .9590 33.05/ .9130 31.90/ .8960 30.77/ .9140 37.22/ .9750 \nLapSRN 2 37.52/ .9591 33.08/ .9130 31.08/ .8950 30.41/ .9101 37.27/ .9740 \nMemNet 2 37.78/ .9597 33.28/ .9142 32.08/ .8978 31.31/ .9195 37.72/ .9740 \nEDSR \n2 38.11/ .9602 33.92/ .9195 32.32/ .9013 32.93/ .9351 39.10/ .9773 \nSRMD \n2 37.79/ .9601 33.32/ .9159 32.05/ .8985 31.33/ .9204 38.07/ .9761 \nNLRN \n2 38.00/ .9603 33.46/ .9159 32.19/ .8992 31.81/ .9246 \u2212\u2212/ \u2212\u2212 \nDBPN \n2 38.09/ .9600 33.85/ .9190 32.27/ .9000 32.55/ .9324 38.89/ .9775 \nRDN \n2 38.24/ .9614 34.01/ .9212 32.34/ .9017 32.89/ .9353 39.18/ .9780 \nRCAN \n2 38.27/ .9614 34.11/ .9216 32.41/ .9026 33.34/ .9384 39.43/ .9786 \nSAN \n2 38.31/ .9620 34.07/ .9213 32.42/ .9028 33.10/ .9370 39.32/ .9792 \nSAN+ \n2 38.35/ .9619 34.44/ .9244 32.50/ .9038 33.73/ .9416 39.72/ .9797 \nBicubic \n3 30.39/ .8682 27.55/ .7742 27.21/ .7385 24.46/ .7349 26.95/ .8556 \nSRCNN 3 32.75/ .9090 29.30/ .8215 28.41/ .7863 26.24/ .7989 30.48/ .9117 \nFSRCNN 3 33.18/ .9140 29.37/ .8240 28.53/ .7910 26.43/ .8080 31.10/ .9210 \nVDSR \n3 33.67/ .9210 29.78/ .8320 28.83/ .7990 27.14/ .8290 32.01/ .9340 \nLapSRN 3 9369 \nEDSR \n3 34.65/ .9280 3.52/ .8462 29.25/ .8093 28.80/ .8653 34.17/ .9476 \nSRMD \n3 34.12/ .9254 30.04/ .8382 28.97/ .8025 27.57/ .8398 33.00/ .9403 \nNLRG \n3 34.27/ .9266 30.16/ .8374 29.06/ .8026 27.93/ .8453 \u2212\u2212/ \u2212\u2212 \nRDN \n3 34.71/ .9296 30.57/ .8468 29.26/ .8093 28.80/ .8653 34.13/ .9484 \nRCAN \n3 34.74/ .9299 30.64/ .8481 29.32/ .8111 29.08/ .8702 34.43/ .9498 \nSAN \n3 34.75/ .9300 30.59/ .8476 29.33/ .8112 28.93/ .8671 34.30/ .9494 \nSAN+ \n3 34.89/ .9306 30.77/ .8498 29.38/ .8121 29.29/ .8730 34.74/ .9512 \nBicubic \n4 28.42/ .8104 26.00/ .7027 25.96/ .6675 23.14/ .6577 24.89/ .7866 \nSRCNN 4 9172 \nSAN \n4 32.64/ .9003 28.92/ .7888 27.78/ .7436 26.79/ .8068 31.18/ .9169 \nSAN+ \n4 32.70/ .9013 29.05/ .7921 27.86/ .7457 27.23/ .8169 31.66/ .9222 \nBicubic \n8 24.40/ .6580 23.10/ .5660 23.67/ .5480 20.74/ .5160 21.47/ .6500 \nSRCNN 8 \n\nTable 3 .Table 4 .\n34Quantitative results with BD degradation model. Best and second best results are highlighted and underlined 28.78/ .8308 26.38/ .7271 26.33/ .6918 23.52/ .6862 25.46/ .8149 SPMSR 3 32.21/ .9001 28.89/ .8105 28.13/ .7740 25.84/ .7856 29.64/ .9003 SRCNN 3 32.05/ .8944 28.80/ .8074 28.13/ .7736 25.70/ .7770 29.47/ .8924 FSRCNN 3 26.23/ .8124 24.44/ .7106 24.86/ .6832 22.04/ .6745 23.04/ .7927 VDSR 3 33.25/ .9150 29.46/ .8244 28.57/ .7893 26.61/ .8136 31.06/ .9234 IRCNN 3 33.38/ .9182 29.63/ .8281 28.65/ .7922 26.77/ .8154 31.15/ .9245 SRMD 3 34.01/ .9242 30.11/ .8364 28.98/ .8009 27.50/ .8370 32.97/ .9391 RDN 3 34.58/ .9280 30.53/ .8447 29.23/ .8079 28.46/ .8582 33.97/ .9465 RCAN 3 34.70/ .9288 30.63/ .8462 29.32/ .8093 28.81/ .8645 34.38/ .9483 SAN 3 34.75/ .9290 30.68/ .8466 29.33/ .8101 28.83/ .8646 34.46/ .9487 SAN+ 3 34.86/ .9297 30.77/ .8481 29.39/ .8112 29.03/ .8674 34.76/ .9501 Computational and parameter comparison (2\u00d7 Set5).Method \nSet5 \nSet14 \nBSD100 \nUrban100 \nManga109 \n\nPSNR/ SSIM \nPSNR/ SSIM \nPSNR/ SSIM \nPSNR/ SSIM \nPSNR/ SSIM \n\nBicubic \n3 EDSR \nMemNet \nNLRG \nDBPN \nRDN \nRCAN \nSAN \nPara. \n43M \n677k \n330k \n10M \n22.3M 16M \n15.7M \nPSNR \n38.11 \n37.78 \n38.00 \n38.09 \n38.24 \n38.27 \n38.31 \n\n\n\nLearning a deep convolutional network for image super-resolution. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, ECCV. Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV, 2014. 7\n\nImage super-resolution using deep convolutional networks. TPAMI. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional net- works. TPAMI, 2016. 1, 2, 3, 7, 8\n\nAccelerating the super-resolution convolutional neural network. Chao Dong, Chen Change Loy, Xiaoou Tang, ECCV. Springer7Chao Dong, Chen Change Loy, and Xiaoou Tang. Acceler- ating the super-resolution convolutional neural network. In ECCV. Springer, 2016. 3, 7, 8\n\nImage deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization. Weisheng Dong, Lei Zhang, Guangming Shi, Xiaolin Wu, TIP. 1Weisheng Dong, Lei Zhang, Guangming Shi, and Xiaolin Wu. Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization. TIP, 2011. 1\n\nLearning low-level vision. William T Freeman, C Egon, Owen T Pasztor, Carmichael, IJCV. 401William T Freeman, Egon C Pasztor, and Owen T Carmichael. Learning low-level vision. IJCV, 40(1):25-47, 2000. 1\n\nDeep backprojection networks for super-resolution. Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita, CVPR. 67Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita. Deep backprojection networks for super-resolution. In CVPR, 2018. 2, 3, 6, 7\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1\n\nFunctions of matrices: theory and computation. SIAM. J Nicholas, Higham, Nicholas J Higham. Functions of matrices: theory and com- putation. SIAM, 2008. 5\n\nSqueeze-and-excitation networks. Jie Hu, Li Shen, Gang Sun, CVPR. 56Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net- works. In CVPR, 2018. 2, 4, 5, 6\n\nLaurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. Gao Huang, Zhuang Liu, CVPR. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil- ian Q Weinberger. Densely connected convolutional net- works. In CVPR, 2017. 2\n\nPerceptual losses for real-time style transfer and super-resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, ECCV. Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 3\n\nAccurate image super-resolution using very deep convolutional networks. Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, CVPR. Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional net- works. In CVPR, 2016. 1, 2, 3, 7, 8\n\nDeeplyrecursive convolutional network for image super-resolution. Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, CVPR. Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply- recursive convolutional network for image super-resolution. In CVPR, 2016. 2\n\nDeep laplacian pyramid networks for fast and accurate superresolution. Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang, CVPR. Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming- Hsuan Yang. Deep laplacian pyramid networks for fast and accurate superresolution. In CVPR, 2017. 1, 2, 3, 7\n\nFast and accurate image super-resolution with deep laplacian pyramid networks. Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang, arXiv:1710.01992arXiv preprintWei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming- Hsuan Yang. Fast and accurate image super-resolution with deep laplacian pyramid networks. arXiv preprint arXiv:1710.01992, 2017. 3\n\nA model of saliency-based visual attention for rapid scene analysis. Christof Koch, Laurent Itti, Ernst Niebur, PAMI. 2Christof Koch Laurent Itti and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. PAMI, 1998. 2\n\nPhotorealistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Alykhan Andrew P Aitken, Johannes Tejani, Zehan Totz, Wang, CVPR. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew P Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo- realistic single image super-resolution using a generative ad- versarial network. In CVPR, 2017. 2\n\nTowards faster training of global covariance pooling networks by iterative matrix square root normalization. Peihua Li, Jiangtao Xie, Qilong Wang, Zilin Gao, CVPR. Peihua Li, Jiangtao Xie, Qilong Wang, and Zilin Gao. To- wards faster training of global covariance pooling networks by iterative matrix square root normalization. In CVPR, 2018. 5\n\nIs second-order information helpful for large-scale visual recognition. Peihua Li, Jiangtao Xie, Qilong Wang, Wangmeng Zuo, ICCV. 45Peihua Li, Jiangtao Xie, Qilong Wang, and Wangmeng Zuo. Is second-order information helpful for large-scale visual recognition. In ICCV, 2017. 4, 5\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, CVPRW. 67Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, 2017. 2, 3, 5, 6, 7\n\nBilinear cnn models for fine-grained visual recognition. Tsung-Yu Lin, Aruni Roychowdhury, Subhransu Maji, ICCV. Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear cnn models for fine-grained visual recognition. In ICCV, 2015. 4\n\nNon-local recurrent network for image restoration. Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S Huang, NIPS. 57Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and Thomas S Huang. Non-local recurrent network for image restoration. In NIPS, 2018. 2, 4, 5, 7\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. 6\n\nA statistical prediction model based on sparse representations for single image super-resolution. Tomer Peleg, Michael Elad, TIP. 8Tomer Peleg and Michael Elad. A statistical prediction model based on sparse representations for single image super-resolution. TIP, 2014. 8\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, NIPS. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NIPS, 2015. 1\n\nEnhancenet: Single image super-resolution through automated texture synthesis. S M Mehdi, Bernhard Sajjadi, Michael Sch\u00f6lkopf, Hirsch, ICCV. Mehdi SM Sajjadi, Bernhard Sch\u00f6lkopf, and Michael Hirsch. Enhancenet: Single image super-resolution through automated texture synthesis. In ICCV, 2017. 3\n\nImage classification with the fisher vector: Theory and practice. Jorge S\u00e1nchez, Florent Perronnin, Thomas Mensink, Jakob Verbeek, IJCV. 4Jorge S\u00e1nchez, Florent Perronnin, Thomas Mensink, and Jakob Verbeek. Image classification with the fisher vector: Theory and practice. IJCV. 4\n\nReal-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, P Andrew, Rob Aitken, Daniel Bishop, Zehan Rueckert, Wang, CVPR. 35Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, 2016. 3, 5\n\nImage superresolution via deep recursive residual network. Ying Tai, Jian Yang, Xiaoming Liu, CVPR. 23Ying Tai, Jian Yang, and Xiaoming Liu. Image super- resolution via deep recursive residual network. In CVPR, 2017. 2, 3\n\nMemnet: A persistent memory network for image restoration. Ying Tai, Jian Yang, Xiaoming Liu, Chunyan Xu, CVPR. 27Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem- net: A persistent memory network for image restoration. In CVPR, pages 4539-4547, 2017. 2, 3, 7\n\nNtire 2017 challenge on single image super-resolution: Methods and results. Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, CVPRW. Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming- Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, et al. Ntire 2017 chal- lenge on single image super-resolution: Methods and results. In CVPRW, 2017. 6\n\nSemi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis. Shenlong Wang, Lei Zhang, Yan Liang, Quan Pan, CVPR. Shenlong Wang, Lei Zhang, Yan Liang, and Quan Pan. Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch synthesis. In CVPR, 2012. 1\n\nAbhinav Gupta, and Kaiming He. Non-local neural networks. Xiaolong Wang, Ross Girshick, In CVPR. 23Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim- ing He. Non-local neural networks. In CVPR, 2018. 2, 3\n\nSingle image superresolution with non-local means and steering kernel regression. K Zhang, X Gao, D Tao, X Li, TIP. 2111K. Zhang, X. Gao, D. Tao, and X. Li. Single image super- resolution with non-local means and steering kernel regres- sion. TIP, 21(11):4544-4556, 2012. 1, 2\n\nLearning deep cnn denoiser prior for image restoration. Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang, CVPR. Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image restoration. In CVPR, 2017. 8\n\nLearning a single convolutional super-resolution network for multiple degradations. Kai Zhang, Wangmeng Zuo, Lei Zhang, CVPR. 7Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a single convolutional super-resolution network for multiple degradations. In CVPR, 2018. 1, 2, 6, 7, 8\n\nAn edge-guided image interpolation algorithm via directional filtering and data fusion. Lei Zhang, Xiaolin Wu, TIP. 12Lei Zhang and Xiaolin Wu. An edge-guided image interpo- lation algorithm via directional filtering and data fusion. TIP, 2006. 1, 2\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, ECCV. 7Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In ECCV, 2018. 1, 2, 6, 7, 8\n\nResidual dense network for image super-resolution. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu, CVPR. 7Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In CVPR, 2018. 1, 2, 3, 5, 6, 7, 8\n", "annotations": {"author": "[{\"end\":227,\"start\":68},{\"end\":317,\"start\":228},{\"end\":423,\"start\":318},{\"end\":587,\"start\":424},{\"end\":703,\"start\":588}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":72},{\"end\":239,\"start\":236},{\"end\":332,\"start\":327},{\"end\":435,\"start\":432},{\"end\":597,\"start\":592}]", "author_first_name": "[{\"end\":71,\"start\":68},{\"end\":235,\"start\":228},{\"end\":326,\"start\":318},{\"end\":431,\"start\":424},{\"end\":591,\"start\":588}]", "author_affiliation": "[{\"end\":138,\"start\":77},{\"end\":226,\"start\":140},{\"end\":316,\"start\":241},{\"end\":422,\"start\":361},{\"end\":498,\"start\":437},{\"end\":586,\"start\":500},{\"end\":674,\"start\":599},{\"end\":702,\"start\":676}]", "title": "[{\"end\":65,\"start\":1},{\"end\":768,\"start\":704}]", "venue": null, "abstract": "[{\"end\":2091,\"start\":793}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2147,\"start\":2144},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2506,\"start\":2502},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2526,\"start\":2523},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2565,\"start\":2561},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2568,\"start\":2565},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2862,\"start\":2858},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2885,\"start\":2882},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3291,\"start\":3288},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3294,\"start\":3291},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3709,\"start\":3706},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3712,\"start\":3709},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3715,\"start\":3712},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3718,\"start\":3715},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3721,\"start\":3718},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3724,\"start\":3721},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4483,\"start\":4479},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4497,\"start\":4493},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4511,\"start\":4507},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4525,\"start\":4521},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4537,\"start\":4534},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6899,\"start\":6895},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6917,\"start\":6913},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6944,\"start\":6941},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6947,\"start\":6944},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6950,\"start\":6947},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6953,\"start\":6950},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6956,\"start\":6953},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6959,\"start\":6956},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6962,\"start\":6959},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6965,\"start\":6962},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6968,\"start\":6965},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7483,\"start\":7480},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7486,\"start\":7483},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7489,\"start\":7486},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7491,\"start\":7489},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7494,\"start\":7491},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7497,\"start\":7494},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7527,\"start\":7524},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7693,\"start\":7689},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7707,\"start\":7703},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7812,\"start\":7808},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8030,\"start\":8026},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8043,\"start\":8039},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8075,\"start\":8071},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8288,\"start\":8284},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8302,\"start\":8298},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8559,\"start\":8555},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8729,\"start\":8726},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8732,\"start\":8729},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8750,\"start\":8746},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8898,\"start\":8895},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9101,\"start\":9097},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9855,\"start\":9851},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9858,\"start\":9855},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10744,\"start\":10741},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10756,\"start\":10752},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10958,\"start\":10955},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10960,\"start\":10958},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10963,\"start\":10960},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11310,\"start\":11307},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11313,\"start\":11310},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11316,\"start\":11313},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11319,\"start\":11316},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11329,\"start\":11325},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11332,\"start\":11329},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11335,\"start\":11332},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11338,\"start\":11335},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11362,\"start\":11358},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11365,\"start\":11362},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12559,\"start\":12555},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12562,\"start\":12559},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12774,\"start\":12770},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14013,\"start\":14009},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14496,\"start\":14492},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16240,\"start\":16237},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16550,\"start\":16546},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16553,\"start\":16550},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17342,\"start\":17338},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17345,\"start\":17342},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18111,\"start\":18107},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19098,\"start\":19095},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19914,\"start\":19910},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20001,\"start\":19998},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20200,\"start\":20196},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20322,\"start\":20318},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20444,\"start\":20440},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21887,\"start\":21883},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21890,\"start\":21887},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21912,\"start\":21908},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22102,\"start\":22098},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22890,\"start\":22886},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23196,\"start\":23193},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23199,\"start\":23196},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23721,\"start\":23717},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24940,\"start\":24936},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24942,\"start\":24940},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24945,\"start\":24942},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24948,\"start\":24945},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25007,\"start\":25003},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25257,\"start\":25253},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25791,\"start\":25787},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":26415,\"start\":26411},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28095,\"start\":28092},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28107,\"start\":28104},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28118,\"start\":28114},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28131,\"start\":28127},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28145,\"start\":28141},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28156,\"start\":28152},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28167,\"start\":28163},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28178,\"start\":28174},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28188,\"start\":28185},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28198,\"start\":28194},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28212,\"start\":28208},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28224,\"start\":28220},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28227,\"start\":28224},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":28230,\"start\":28227},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30630,\"start\":30626},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30633,\"start\":30630},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":30786,\"start\":30782},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30797,\"start\":30794},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30809,\"start\":30806},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30820,\"start\":30816},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30833,\"start\":30829},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30844,\"start\":30840},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":30854,\"start\":30850},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":30869,\"start\":30865},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":33818,\"start\":33816}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32418,\"start\":32322},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32852,\"start\":32419},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33756,\"start\":32853},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37810,\"start\":33757},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39045,\"start\":37811}]", "paragraph": "[{\"end\":2569,\"start\":2107},{\"end\":3181,\"start\":2571},{\"end\":4279,\"start\":3183},{\"end\":5661,\"start\":4281},{\"end\":5900,\"start\":5663},{\"end\":6289,\"start\":5902},{\"end\":6740,\"start\":6291},{\"end\":9017,\"start\":6757},{\"end\":9521,\"start\":9019},{\"end\":9950,\"start\":9582},{\"end\":10149,\"start\":9974},{\"end\":10562,\"start\":10175},{\"end\":11040,\"start\":10585},{\"end\":11559,\"start\":11079},{\"end\":11649,\"start\":11585},{\"end\":11818,\"start\":11704},{\"end\":12469,\"start\":11865},{\"end\":13208,\"start\":12471},{\"end\":13578,\"start\":13244},{\"end\":15367,\"start\":13607},{\"end\":15658,\"start\":15397},{\"end\":16079,\"start\":15688},{\"end\":16686,\"start\":16121},{\"end\":17203,\"start\":16688},{\"end\":17321,\"start\":17220},{\"end\":17632,\"start\":17323},{\"end\":17851,\"start\":17649},{\"end\":18589,\"start\":17877},{\"end\":18944,\"start\":18632},{\"end\":19165,\"start\":18946},{\"end\":19440,\"start\":19192},{\"end\":20073,\"start\":19461},{\"end\":20513,\"start\":20115},{\"end\":20972,\"start\":20592},{\"end\":21220,\"start\":20992},{\"end\":21406,\"start\":21222},{\"end\":22041,\"start\":21444},{\"end\":24673,\"start\":22057},{\"end\":24902,\"start\":24675},{\"end\":25357,\"start\":24926},{\"end\":25816,\"start\":25359},{\"end\":26205,\"start\":25835},{\"end\":27939,\"start\":26207},{\"end\":28754,\"start\":27981},{\"end\":29328,\"start\":28756},{\"end\":30567,\"start\":29330},{\"end\":31136,\"start\":30616},{\"end\":31535,\"start\":31160},{\"end\":32321,\"start\":31551}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9973,\"start\":9951},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10174,\"start\":10150},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10584,\"start\":10563},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11078,\"start\":11041},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11584,\"start\":11560},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11703,\"start\":11650},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13243,\"start\":13209},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13606,\"start\":13579},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15396,\"start\":15368},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15687,\"start\":15659},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17219,\"start\":17204},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17648,\"start\":17633},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17876,\"start\":17852},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18631,\"start\":18590},{\"attributes\":{\"id\":\"formula_14\"},\"end\":19191,\"start\":19166},{\"attributes\":{\"id\":\"formula_15\"},\"end\":19460,\"start\":19441},{\"attributes\":{\"id\":\"formula_16\"},\"end\":20591,\"start\":20514},{\"attributes\":{\"id\":\"formula_17\"},\"end\":20991,\"start\":20973},{\"attributes\":{\"id\":\"formula_18\"},\"end\":21425,\"start\":21407}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26204,\"start\":26197},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28391,\"start\":28384},{\"end\":30913,\"start\":30906},{\"end\":31171,\"start\":31164}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2105,\"start\":2093},{\"attributes\":{\"n\":\"2.\"},\"end\":6755,\"start\":6743},{\"attributes\":{\"n\":\"3.\"},\"end\":9560,\"start\":9524},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9580,\"start\":9563},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11863,\"start\":11821},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16119,\"start\":16082},{\"attributes\":{\"n\":\"3.4.\"},\"end\":20113,\"start\":20076},{\"attributes\":{\"n\":\"3.5.\"},\"end\":21442,\"start\":21427},{\"attributes\":{\"n\":\"3.6.\"},\"end\":22055,\"start\":22044},{\"attributes\":{\"n\":\"4.\"},\"end\":24916,\"start\":24905},{\"attributes\":{\"n\":\"4.1.\"},\"end\":24924,\"start\":24919},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25833,\"start\":25819},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27979,\"start\":27942},{\"attributes\":{\"n\":\"4.4.\"},\"end\":30614,\"start\":30570},{\"attributes\":{\"n\":\"4.5.\"},\"end\":31158,\"start\":31139},{\"attributes\":{\"n\":\"5.\"},\"end\":31549,\"start\":31538},{\"end\":32333,\"start\":32323},{\"end\":32430,\"start\":32420},{\"end\":32863,\"start\":32854},{\"end\":33767,\"start\":33758},{\"end\":37830,\"start\":37812}]", "table": "[{\"end\":33756,\"start\":32969},{\"end\":37810,\"start\":35518},{\"end\":39045,\"start\":38778}]", "figure_caption": "[{\"end\":32418,\"start\":32335},{\"end\":32852,\"start\":32432},{\"end\":32969,\"start\":32865},{\"end\":35518,\"start\":33769},{\"end\":38778,\"start\":37833}]", "figure_ref": "[{\"end\":4555,\"start\":4547},{\"end\":5466,\"start\":5460},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9600,\"start\":9594},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10388,\"start\":10382},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11935,\"start\":11929},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14700,\"start\":14694},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15326,\"start\":15320},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18446,\"start\":18440},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29406,\"start\":29400}]", "bib_author_first_name": "[{\"end\":39117,\"start\":39113},{\"end\":39128,\"start\":39124},{\"end\":39135,\"start\":39129},{\"end\":39148,\"start\":39141},{\"end\":39159,\"start\":39153},{\"end\":39382,\"start\":39378},{\"end\":39393,\"start\":39389},{\"end\":39400,\"start\":39394},{\"end\":39413,\"start\":39406},{\"end\":39424,\"start\":39418},{\"end\":39644,\"start\":39640},{\"end\":39655,\"start\":39651},{\"end\":39662,\"start\":39656},{\"end\":39674,\"start\":39668},{\"end\":39952,\"start\":39944},{\"end\":39962,\"start\":39959},{\"end\":39979,\"start\":39970},{\"end\":39992,\"start\":39985},{\"end\":40224,\"start\":40223},{\"end\":40237,\"start\":40231},{\"end\":40440,\"start\":40432},{\"end\":40452,\"start\":40448},{\"end\":40477,\"start\":40468},{\"end\":40681,\"start\":40674},{\"end\":40693,\"start\":40686},{\"end\":40709,\"start\":40701},{\"end\":40719,\"start\":40715},{\"end\":40904,\"start\":40903},{\"end\":41042,\"start\":41039},{\"end\":41049,\"start\":41047},{\"end\":41060,\"start\":41056},{\"end\":41261,\"start\":41258},{\"end\":41275,\"start\":41269},{\"end\":41498,\"start\":41492},{\"end\":41517,\"start\":41508},{\"end\":41527,\"start\":41525},{\"end\":41756,\"start\":41751},{\"end\":41766,\"start\":41762},{\"end\":41771,\"start\":41767},{\"end\":41786,\"start\":41777},{\"end\":42018,\"start\":42013},{\"end\":42028,\"start\":42024},{\"end\":42033,\"start\":42029},{\"end\":42048,\"start\":42039},{\"end\":42271,\"start\":42262},{\"end\":42284,\"start\":42277},{\"end\":42300,\"start\":42292},{\"end\":42318,\"start\":42308},{\"end\":42585,\"start\":42576},{\"end\":42598,\"start\":42591},{\"end\":42614,\"start\":42606},{\"end\":42632,\"start\":42622},{\"end\":42935,\"start\":42927},{\"end\":42949,\"start\":42942},{\"end\":42961,\"start\":42956},{\"end\":43200,\"start\":43191},{\"end\":43213,\"start\":43208},{\"end\":43227,\"start\":43221},{\"end\":43240,\"start\":43236},{\"end\":43258,\"start\":43252},{\"end\":43280,\"start\":43271},{\"end\":43296,\"start\":43289},{\"end\":43322,\"start\":43314},{\"end\":43336,\"start\":43331},{\"end\":43742,\"start\":43736},{\"end\":43755,\"start\":43747},{\"end\":43767,\"start\":43761},{\"end\":43779,\"start\":43774},{\"end\":44051,\"start\":44045},{\"end\":44064,\"start\":44056},{\"end\":44076,\"start\":44070},{\"end\":44091,\"start\":44083},{\"end\":44324,\"start\":44321},{\"end\":44338,\"start\":44330},{\"end\":44350,\"start\":44344},{\"end\":44364,\"start\":44356},{\"end\":44379,\"start\":44370},{\"end\":44625,\"start\":44617},{\"end\":44636,\"start\":44631},{\"end\":44660,\"start\":44651},{\"end\":44857,\"start\":44853},{\"end\":44868,\"start\":44863},{\"end\":44880,\"start\":44874},{\"end\":44890,\"start\":44886},{\"end\":44897,\"start\":44891},{\"end\":44911,\"start\":44903},{\"end\":45117,\"start\":45113},{\"end\":45129,\"start\":45126},{\"end\":45144,\"start\":45137},{\"end\":45162,\"start\":45155},{\"end\":45177,\"start\":45171},{\"end\":45191,\"start\":45184},{\"end\":45206,\"start\":45200},{\"end\":45217,\"start\":45212},{\"end\":45233,\"start\":45229},{\"end\":45246,\"start\":45242},{\"end\":45551,\"start\":45546},{\"end\":45566,\"start\":45559},{\"end\":45808,\"start\":45801},{\"end\":45827,\"start\":45823},{\"end\":45836,\"start\":45832},{\"end\":46091,\"start\":46090},{\"end\":46093,\"start\":46092},{\"end\":46109,\"start\":46101},{\"end\":46126,\"start\":46119},{\"end\":46378,\"start\":46373},{\"end\":46395,\"start\":46388},{\"end\":46413,\"start\":46407},{\"end\":46428,\"start\":46423},{\"end\":46704,\"start\":46698},{\"end\":46714,\"start\":46710},{\"end\":46732,\"start\":46726},{\"end\":46749,\"start\":46741},{\"end\":46757,\"start\":46756},{\"end\":46769,\"start\":46766},{\"end\":46784,\"start\":46778},{\"end\":46798,\"start\":46793},{\"end\":47136,\"start\":47132},{\"end\":47146,\"start\":47142},{\"end\":47161,\"start\":47153},{\"end\":47359,\"start\":47355},{\"end\":47369,\"start\":47365},{\"end\":47384,\"start\":47376},{\"end\":47397,\"start\":47390},{\"end\":47643,\"start\":47639},{\"end\":47660,\"start\":47653},{\"end\":47675,\"start\":47672},{\"end\":47696,\"start\":47686},{\"end\":47706,\"start\":47703},{\"end\":47717,\"start\":47714},{\"end\":47731,\"start\":47723},{\"end\":47743,\"start\":47737},{\"end\":47757,\"start\":47749},{\"end\":48142,\"start\":48134},{\"end\":48152,\"start\":48149},{\"end\":48163,\"start\":48160},{\"end\":48175,\"start\":48171},{\"end\":48427,\"start\":48419},{\"end\":48438,\"start\":48434},{\"end\":48654,\"start\":48653},{\"end\":48663,\"start\":48662},{\"end\":48670,\"start\":48669},{\"end\":48677,\"start\":48676},{\"end\":48908,\"start\":48905},{\"end\":48924,\"start\":48916},{\"end\":48937,\"start\":48930},{\"end\":48945,\"start\":48942},{\"end\":49172,\"start\":49169},{\"end\":49188,\"start\":49180},{\"end\":49197,\"start\":49194},{\"end\":49457,\"start\":49454},{\"end\":49472,\"start\":49465},{\"end\":49698,\"start\":49693},{\"end\":49713,\"start\":49706},{\"end\":49721,\"start\":49718},{\"end\":49732,\"start\":49726},{\"end\":49745,\"start\":49739},{\"end\":49756,\"start\":49753},{\"end\":50002,\"start\":49997},{\"end\":50016,\"start\":50010},{\"end\":50025,\"start\":50023},{\"end\":50038,\"start\":50032},{\"end\":50049,\"start\":50046}]", "bib_author_last_name": "[{\"end\":39122,\"start\":39118},{\"end\":39139,\"start\":39136},{\"end\":39151,\"start\":39149},{\"end\":39164,\"start\":39160},{\"end\":39387,\"start\":39383},{\"end\":39404,\"start\":39401},{\"end\":39416,\"start\":39414},{\"end\":39429,\"start\":39425},{\"end\":39649,\"start\":39645},{\"end\":39666,\"start\":39663},{\"end\":39679,\"start\":39675},{\"end\":39957,\"start\":39953},{\"end\":39968,\"start\":39963},{\"end\":39983,\"start\":39980},{\"end\":39995,\"start\":39993},{\"end\":40221,\"start\":40204},{\"end\":40229,\"start\":40225},{\"end\":40245,\"start\":40238},{\"end\":40257,\"start\":40247},{\"end\":40446,\"start\":40441},{\"end\":40466,\"start\":40453},{\"end\":40483,\"start\":40478},{\"end\":40684,\"start\":40682},{\"end\":40699,\"start\":40694},{\"end\":40713,\"start\":40710},{\"end\":40723,\"start\":40720},{\"end\":40913,\"start\":40905},{\"end\":40921,\"start\":40915},{\"end\":41045,\"start\":41043},{\"end\":41054,\"start\":41050},{\"end\":41064,\"start\":41061},{\"end\":41267,\"start\":41262},{\"end\":41279,\"start\":41276},{\"end\":41506,\"start\":41499},{\"end\":41523,\"start\":41518},{\"end\":41535,\"start\":41528},{\"end\":41760,\"start\":41757},{\"end\":41775,\"start\":41772},{\"end\":41790,\"start\":41787},{\"end\":42022,\"start\":42019},{\"end\":42037,\"start\":42034},{\"end\":42052,\"start\":42049},{\"end\":42275,\"start\":42272},{\"end\":42290,\"start\":42285},{\"end\":42306,\"start\":42301},{\"end\":42323,\"start\":42319},{\"end\":42589,\"start\":42586},{\"end\":42604,\"start\":42599},{\"end\":42620,\"start\":42615},{\"end\":42637,\"start\":42633},{\"end\":42940,\"start\":42936},{\"end\":42954,\"start\":42950},{\"end\":42968,\"start\":42962},{\"end\":43206,\"start\":43201},{\"end\":43219,\"start\":43214},{\"end\":43234,\"start\":43228},{\"end\":43250,\"start\":43241},{\"end\":43269,\"start\":43259},{\"end\":43287,\"start\":43281},{\"end\":43312,\"start\":43297},{\"end\":43329,\"start\":43323},{\"end\":43341,\"start\":43337},{\"end\":43347,\"start\":43343},{\"end\":43745,\"start\":43743},{\"end\":43759,\"start\":43756},{\"end\":43772,\"start\":43768},{\"end\":43783,\"start\":43780},{\"end\":44054,\"start\":44052},{\"end\":44068,\"start\":44065},{\"end\":44081,\"start\":44077},{\"end\":44095,\"start\":44092},{\"end\":44328,\"start\":44325},{\"end\":44342,\"start\":44339},{\"end\":44354,\"start\":44351},{\"end\":44368,\"start\":44365},{\"end\":44383,\"start\":44380},{\"end\":44629,\"start\":44626},{\"end\":44649,\"start\":44637},{\"end\":44665,\"start\":44661},{\"end\":44861,\"start\":44858},{\"end\":44872,\"start\":44869},{\"end\":44884,\"start\":44881},{\"end\":44901,\"start\":44898},{\"end\":44917,\"start\":44912},{\"end\":45124,\"start\":45118},{\"end\":45135,\"start\":45130},{\"end\":45153,\"start\":45145},{\"end\":45169,\"start\":45163},{\"end\":45182,\"start\":45178},{\"end\":45198,\"start\":45192},{\"end\":45210,\"start\":45207},{\"end\":45227,\"start\":45218},{\"end\":45240,\"start\":45234},{\"end\":45252,\"start\":45247},{\"end\":45557,\"start\":45552},{\"end\":45571,\"start\":45567},{\"end\":45821,\"start\":45809},{\"end\":45830,\"start\":45828},{\"end\":45845,\"start\":45837},{\"end\":45850,\"start\":45847},{\"end\":46099,\"start\":46094},{\"end\":46117,\"start\":46110},{\"end\":46136,\"start\":46127},{\"end\":46144,\"start\":46138},{\"end\":46386,\"start\":46379},{\"end\":46405,\"start\":46396},{\"end\":46421,\"start\":46414},{\"end\":46436,\"start\":46429},{\"end\":46708,\"start\":46705},{\"end\":46724,\"start\":46715},{\"end\":46739,\"start\":46733},{\"end\":46754,\"start\":46750},{\"end\":46764,\"start\":46758},{\"end\":46776,\"start\":46770},{\"end\":46791,\"start\":46785},{\"end\":46807,\"start\":46799},{\"end\":46813,\"start\":46809},{\"end\":47140,\"start\":47137},{\"end\":47151,\"start\":47147},{\"end\":47165,\"start\":47162},{\"end\":47363,\"start\":47360},{\"end\":47374,\"start\":47370},{\"end\":47388,\"start\":47385},{\"end\":47400,\"start\":47398},{\"end\":47651,\"start\":47644},{\"end\":47670,\"start\":47661},{\"end\":47684,\"start\":47676},{\"end\":47701,\"start\":47697},{\"end\":47712,\"start\":47707},{\"end\":47721,\"start\":47718},{\"end\":47735,\"start\":47732},{\"end\":47747,\"start\":47744},{\"end\":47761,\"start\":47758},{\"end\":47776,\"start\":47763},{\"end\":48147,\"start\":48143},{\"end\":48158,\"start\":48153},{\"end\":48169,\"start\":48164},{\"end\":48179,\"start\":48176},{\"end\":48432,\"start\":48428},{\"end\":48447,\"start\":48439},{\"end\":48660,\"start\":48655},{\"end\":48667,\"start\":48664},{\"end\":48674,\"start\":48671},{\"end\":48680,\"start\":48678},{\"end\":48914,\"start\":48909},{\"end\":48928,\"start\":48925},{\"end\":48940,\"start\":48938},{\"end\":48951,\"start\":48946},{\"end\":49178,\"start\":49173},{\"end\":49192,\"start\":49189},{\"end\":49203,\"start\":49198},{\"end\":49463,\"start\":49458},{\"end\":49475,\"start\":49473},{\"end\":49704,\"start\":49699},{\"end\":49716,\"start\":49714},{\"end\":49724,\"start\":49722},{\"end\":49737,\"start\":49733},{\"end\":49751,\"start\":49746},{\"end\":49759,\"start\":49757},{\"end\":50008,\"start\":50003},{\"end\":50021,\"start\":50017},{\"end\":50030,\"start\":50026},{\"end\":50044,\"start\":50039},{\"end\":50052,\"start\":50050}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":18874645},\"end\":39311,\"start\":39047},{\"attributes\":{\"id\":\"b1\"},\"end\":39574,\"start\":39313},{\"attributes\":{\"id\":\"b2\"},\"end\":39839,\"start\":39576},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14877173},\"end\":40175,\"start\":39841},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1414109},\"end\":40379,\"start\":40177},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3739626},\"end\":40626,\"start\":40381},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206594692},\"end\":40848,\"start\":40628},{\"attributes\":{\"id\":\"b7\"},\"end\":41004,\"start\":40850},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":140309863},\"end\":41165,\"start\":41006},{\"attributes\":{\"id\":\"b9\"},\"end\":41421,\"start\":41167},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":980236},\"end\":41677,\"start\":41423},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9971732},\"end\":41945,\"start\":41679},{\"attributes\":{\"id\":\"b12\"},\"end\":42189,\"start\":41947},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1543021},\"end\":42495,\"start\":42191},{\"attributes\":{\"doi\":\"arXiv:1710.01992\",\"id\":\"b14\"},\"end\":42856,\"start\":42497},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3108956},\"end\":43104,\"start\":42858},{\"attributes\":{\"id\":\"b16\"},\"end\":43625,\"start\":43106},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4533146},\"end\":43971,\"start\":43627},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4524639},\"end\":44252,\"start\":43973},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6540453},\"end\":44558,\"start\":44254},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1331231},\"end\":44800,\"start\":44560},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":47007607},\"end\":45073,\"start\":44802},{\"attributes\":{\"id\":\"b22\"},\"end\":45446,\"start\":45075},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14607544},\"end\":45719,\"start\":45448},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10328909},\"end\":46009,\"start\":45721},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206771333},\"end\":46305,\"start\":46011},{\"attributes\":{\"doi\":\"IJCV. 4\",\"id\":\"b26\"},\"end\":46587,\"start\":46307},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7037846},\"end\":47071,\"start\":46589},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":21618854},\"end\":47294,\"start\":47073},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8550762},\"end\":47561,\"start\":47296},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":484327},\"end\":48027,\"start\":47563},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14306093},\"end\":48359,\"start\":48029},{\"attributes\":{\"id\":\"b32\"},\"end\":48569,\"start\":48361},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12822252},\"end\":48847,\"start\":48571},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1900475},\"end\":49083,\"start\":48849},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2141622},\"end\":49364,\"start\":49085},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9760560},\"end\":49615,\"start\":49366},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":49657846},\"end\":49944,\"start\":49617},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3619954},\"end\":50207,\"start\":49946}]", "bib_title": "[{\"end\":39111,\"start\":39047},{\"end\":39942,\"start\":39841},{\"end\":40202,\"start\":40177},{\"end\":40430,\"start\":40381},{\"end\":40672,\"start\":40628},{\"end\":41037,\"start\":41006},{\"end\":41256,\"start\":41167},{\"end\":41490,\"start\":41423},{\"end\":41749,\"start\":41679},{\"end\":42011,\"start\":41947},{\"end\":42260,\"start\":42191},{\"end\":42925,\"start\":42858},{\"end\":43189,\"start\":43106},{\"end\":43734,\"start\":43627},{\"end\":44043,\"start\":43973},{\"end\":44319,\"start\":44254},{\"end\":44615,\"start\":44560},{\"end\":44851,\"start\":44802},{\"end\":45544,\"start\":45448},{\"end\":45799,\"start\":45721},{\"end\":46088,\"start\":46011},{\"end\":46696,\"start\":46589},{\"end\":47130,\"start\":47073},{\"end\":47353,\"start\":47296},{\"end\":47637,\"start\":47563},{\"end\":48132,\"start\":48029},{\"end\":48417,\"start\":48361},{\"end\":48651,\"start\":48571},{\"end\":48903,\"start\":48849},{\"end\":49167,\"start\":49085},{\"end\":49452,\"start\":49366},{\"end\":49691,\"start\":49617},{\"end\":49995,\"start\":49946}]", "bib_author": "[{\"end\":39124,\"start\":39113},{\"end\":39141,\"start\":39124},{\"end\":39153,\"start\":39141},{\"end\":39166,\"start\":39153},{\"end\":39389,\"start\":39378},{\"end\":39406,\"start\":39389},{\"end\":39418,\"start\":39406},{\"end\":39431,\"start\":39418},{\"end\":39651,\"start\":39640},{\"end\":39668,\"start\":39651},{\"end\":39681,\"start\":39668},{\"end\":39959,\"start\":39944},{\"end\":39970,\"start\":39959},{\"end\":39985,\"start\":39970},{\"end\":39997,\"start\":39985},{\"end\":40223,\"start\":40204},{\"end\":40231,\"start\":40223},{\"end\":40247,\"start\":40231},{\"end\":40259,\"start\":40247},{\"end\":40448,\"start\":40432},{\"end\":40468,\"start\":40448},{\"end\":40485,\"start\":40468},{\"end\":40686,\"start\":40674},{\"end\":40701,\"start\":40686},{\"end\":40715,\"start\":40701},{\"end\":40725,\"start\":40715},{\"end\":40915,\"start\":40903},{\"end\":40923,\"start\":40915},{\"end\":41047,\"start\":41039},{\"end\":41056,\"start\":41047},{\"end\":41066,\"start\":41056},{\"end\":41269,\"start\":41258},{\"end\":41281,\"start\":41269},{\"end\":41508,\"start\":41492},{\"end\":41525,\"start\":41508},{\"end\":41537,\"start\":41525},{\"end\":41762,\"start\":41751},{\"end\":41777,\"start\":41762},{\"end\":41792,\"start\":41777},{\"end\":42024,\"start\":42013},{\"end\":42039,\"start\":42024},{\"end\":42054,\"start\":42039},{\"end\":42277,\"start\":42262},{\"end\":42292,\"start\":42277},{\"end\":42308,\"start\":42292},{\"end\":42325,\"start\":42308},{\"end\":42591,\"start\":42576},{\"end\":42606,\"start\":42591},{\"end\":42622,\"start\":42606},{\"end\":42639,\"start\":42622},{\"end\":42942,\"start\":42927},{\"end\":42956,\"start\":42942},{\"end\":42970,\"start\":42956},{\"end\":43208,\"start\":43191},{\"end\":43221,\"start\":43208},{\"end\":43236,\"start\":43221},{\"end\":43252,\"start\":43236},{\"end\":43271,\"start\":43252},{\"end\":43289,\"start\":43271},{\"end\":43314,\"start\":43289},{\"end\":43331,\"start\":43314},{\"end\":43343,\"start\":43331},{\"end\":43349,\"start\":43343},{\"end\":43747,\"start\":43736},{\"end\":43761,\"start\":43747},{\"end\":43774,\"start\":43761},{\"end\":43785,\"start\":43774},{\"end\":44056,\"start\":44045},{\"end\":44070,\"start\":44056},{\"end\":44083,\"start\":44070},{\"end\":44097,\"start\":44083},{\"end\":44330,\"start\":44321},{\"end\":44344,\"start\":44330},{\"end\":44356,\"start\":44344},{\"end\":44370,\"start\":44356},{\"end\":44385,\"start\":44370},{\"end\":44631,\"start\":44617},{\"end\":44651,\"start\":44631},{\"end\":44667,\"start\":44651},{\"end\":44863,\"start\":44853},{\"end\":44874,\"start\":44863},{\"end\":44886,\"start\":44874},{\"end\":44903,\"start\":44886},{\"end\":44919,\"start\":44903},{\"end\":45126,\"start\":45113},{\"end\":45137,\"start\":45126},{\"end\":45155,\"start\":45137},{\"end\":45171,\"start\":45155},{\"end\":45184,\"start\":45171},{\"end\":45200,\"start\":45184},{\"end\":45212,\"start\":45200},{\"end\":45229,\"start\":45212},{\"end\":45242,\"start\":45229},{\"end\":45254,\"start\":45242},{\"end\":45559,\"start\":45546},{\"end\":45573,\"start\":45559},{\"end\":45823,\"start\":45801},{\"end\":45832,\"start\":45823},{\"end\":45847,\"start\":45832},{\"end\":45852,\"start\":45847},{\"end\":46101,\"start\":46090},{\"end\":46119,\"start\":46101},{\"end\":46138,\"start\":46119},{\"end\":46146,\"start\":46138},{\"end\":46388,\"start\":46373},{\"end\":46407,\"start\":46388},{\"end\":46423,\"start\":46407},{\"end\":46438,\"start\":46423},{\"end\":46710,\"start\":46698},{\"end\":46726,\"start\":46710},{\"end\":46741,\"start\":46726},{\"end\":46756,\"start\":46741},{\"end\":46766,\"start\":46756},{\"end\":46778,\"start\":46766},{\"end\":46793,\"start\":46778},{\"end\":46809,\"start\":46793},{\"end\":46815,\"start\":46809},{\"end\":47142,\"start\":47132},{\"end\":47153,\"start\":47142},{\"end\":47167,\"start\":47153},{\"end\":47365,\"start\":47355},{\"end\":47376,\"start\":47365},{\"end\":47390,\"start\":47376},{\"end\":47402,\"start\":47390},{\"end\":47653,\"start\":47639},{\"end\":47672,\"start\":47653},{\"end\":47686,\"start\":47672},{\"end\":47703,\"start\":47686},{\"end\":47714,\"start\":47703},{\"end\":47723,\"start\":47714},{\"end\":47737,\"start\":47723},{\"end\":47749,\"start\":47737},{\"end\":47763,\"start\":47749},{\"end\":47778,\"start\":47763},{\"end\":48149,\"start\":48134},{\"end\":48160,\"start\":48149},{\"end\":48171,\"start\":48160},{\"end\":48181,\"start\":48171},{\"end\":48434,\"start\":48419},{\"end\":48449,\"start\":48434},{\"end\":48662,\"start\":48653},{\"end\":48669,\"start\":48662},{\"end\":48676,\"start\":48669},{\"end\":48682,\"start\":48676},{\"end\":48916,\"start\":48905},{\"end\":48930,\"start\":48916},{\"end\":48942,\"start\":48930},{\"end\":48953,\"start\":48942},{\"end\":49180,\"start\":49169},{\"end\":49194,\"start\":49180},{\"end\":49205,\"start\":49194},{\"end\":49465,\"start\":49454},{\"end\":49477,\"start\":49465},{\"end\":49706,\"start\":49693},{\"end\":49718,\"start\":49706},{\"end\":49726,\"start\":49718},{\"end\":49739,\"start\":49726},{\"end\":49753,\"start\":49739},{\"end\":49761,\"start\":49753},{\"end\":50010,\"start\":49997},{\"end\":50023,\"start\":50010},{\"end\":50032,\"start\":50023},{\"end\":50046,\"start\":50032},{\"end\":50054,\"start\":50046}]", "bib_venue": "[{\"end\":39170,\"start\":39166},{\"end\":39376,\"start\":39313},{\"end\":39638,\"start\":39576},{\"end\":40000,\"start\":39997},{\"end\":40263,\"start\":40259},{\"end\":40489,\"start\":40485},{\"end\":40729,\"start\":40725},{\"end\":40901,\"start\":40850},{\"end\":41070,\"start\":41066},{\"end\":41285,\"start\":41281},{\"end\":41541,\"start\":41537},{\"end\":41796,\"start\":41792},{\"end\":42058,\"start\":42054},{\"end\":42329,\"start\":42325},{\"end\":42574,\"start\":42497},{\"end\":42974,\"start\":42970},{\"end\":43353,\"start\":43349},{\"end\":43789,\"start\":43785},{\"end\":44101,\"start\":44097},{\"end\":44390,\"start\":44385},{\"end\":44671,\"start\":44667},{\"end\":44923,\"start\":44919},{\"end\":45111,\"start\":45075},{\"end\":45576,\"start\":45573},{\"end\":45856,\"start\":45852},{\"end\":46150,\"start\":46146},{\"end\":46371,\"start\":46307},{\"end\":46819,\"start\":46815},{\"end\":47171,\"start\":47167},{\"end\":47406,\"start\":47402},{\"end\":47783,\"start\":47778},{\"end\":48185,\"start\":48181},{\"end\":48456,\"start\":48449},{\"end\":48685,\"start\":48682},{\"end\":48957,\"start\":48953},{\"end\":49209,\"start\":49205},{\"end\":49480,\"start\":49477},{\"end\":49765,\"start\":49761},{\"end\":50058,\"start\":50054}]"}}}, "year": 2023, "month": 12, "day": 17}