{"id": 249747747, "updated": "2023-10-20 17:18:05.543", "metadata": {"title": "Robust decoding of the speech envelope from EEG recordings through deep neural networks", "authors": "[{\"first\":\"Mike\",\"last\":\"Thornton\",\"middle\":[]},{\"first\":\"Danilo\",\"last\":\"Mandic\",\"middle\":[]},{\"first\":\"Tobias\",\"last\":\"Reichenbach\",\"middle\":[]}]", "venue": null, "journal": "Journal of Neural Engineering", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Objective. Smart hearing aids which can decode the focus of a user\u2019s attention could considerably improve comprehension levels in noisy environments. Methods for decoding auditory attention from electroencapholography (EEG) have attracted considerable interest for this reason. Recent studies suggest that the integration of deep neural networks (DNNs) into existing auditory attention decoding (AAD) algorithms is highly beneficial, although it remains unclear whether these enhanced algorithms can perform robustly in different real-world scenarios. Therefore, we sought to characterise the performance of DNNs at reconstructing the envelope of an attended speech stream from EEG recordings in different listening conditions. In addition, given the relatively sparse availability of EEG data, we investigate possibility of applying subject-independent algorithms to EEG recorded from unseen individuals. Approach. Both linear models and nonlinear DNNs were employed to decode the envelope of clean speech from EEG recordings, with and without subject-specific information. The mean behaviour, as well as the variability of the reconstruction, was characterised for each model. We then trained subject-specific linear models and DNNs to reconstruct the envelope of speech in clean and noisy conditions, and investigated how well they performed in different listening scenarios. We also established that these models can be used to decode auditory attention in competing-speaker scenarios. Main results. The DNNs offered a considerable advantage over their linear analogue at reconstructing the envelope of clean speech. This advantage persisted even when subject-specific information was unavailable at the time of training. The same DNN architectures generalised to a distinct dataset, which contained EEG recorded under a variety of listening conditions. In competing-speakers and speech-in-noise conditions, the DNNs significantly outperformed the linear models. Finally, the DNNs offered a considerable improvement over the linear approach at decoding auditory attention in competing-speakers scenarios. Significance. We present the first detailed study into the extent to which DNNs can be employed for reconstructing the envelope of an attended speech stream. We conclusively demonstrate that DNNs improve the reconstruction of the attended speech envelope. The variance of the reconstruction error is shown to be similar for both DNNs and the linear model. DNNs therefore show promise for real-world AAD, since they perform well in multiple listening conditions and generalise to data recorded from unseen participants.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "35709698", "pubmedcentral": null, "dblp": null, "doi": "10.1088/1741-2552/ac7976"}}, "content": {"source": {"pdf_hash": "35588085df9cc4ebd90a45ac7cab0f574dd1c2b2", "pdf_src": "IOP", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1dda38230b2d04ce4307ff25c31e3fea58613a19", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/35588085df9cc4ebd90a45ac7cab0f574dd1c2b2.txt", "contents": "\nRobust decoding of the speech envelope from EEG recordings through deep neural networks\n2022\n\nMike Thornton \nDepartment of Computing\nImperial College London\nSW7 2RHLondonUnited Kingdom\n\nDanilo Mandic \nDepartment of Electrical and Electronic Engineering\nImperial College London\nSW7 2RHLondonUnited Kingdom\n\n\ue9d9 \nTobias Reichenbach tobias.j.reichenbach@fau.de \nDepartment Artificial Intelligence in Biomedical Engineering\nFriedrich-Alexander-University Erlangen-N\u00fcrnberg\n91052ErlangenGermany\n\nRobust decoding of the speech envelope from EEG recordings through deep neural networks\n\nJ. Neural Eng\n1946007202210.1088/1741-2552/ac7976Journal of Neural Engineering REVISED ACCEPTED FOR PUBLICATION PUBLISHED Original Content from this work may be used under the terms of the Creative Commons Attribution 4.0 licence. Any further distribution of this work must maintain attribution to the author(s) and the title of the work, journal citation and DOI. PAPER * Author to whom any correspondence should be addressed.natural speech processingEEG recordingsauditory attention decodingdeep neural networks\nObjective. Smart hearing aids which can decode the focus of a user's attention could considerably improve comprehension levels in noisy environments. Methods for decoding auditory attention from electroencapholography (EEG) have attracted considerable interest for this reason. Recent studies suggest that the integration of deep neural networks (DNNs) into existing auditory attention decoding (AAD) algorithms is highly beneficial, although it remains unclear whether these enhanced algorithms can perform robustly in different real-world scenarios. Therefore, we sought to characterise the performance of DNNs at reconstructing the envelope of an attended speech stream from EEG recordings in different listening conditions. In addition, given the relatively sparse availability of EEG data, we investigate possibility of applying subject-independent algorithms to EEG recorded from unseen individuals. Approach. Both linear models and nonlinear DNNs were employed to decode the envelope of clean speech from EEG recordings, with and without subject-specific information. The mean behaviour, as well as the variability of the reconstruction, was characterised for each model. We then trained subject-specific linear models and DNNs to reconstruct the envelope of speech in clean and noisy conditions, and investigated how well they performed in different listening scenarios. We also established that these models can be used to decode auditory attention in competing-speaker scenarios. Main results. The DNNs offered a considerable advantage over their linear analogue at reconstructing the envelope of clean speech. This advantage persisted even when subject-specific information was unavailable at the time of training. The same DNN architectures generalised to a distinct dataset, which contained EEG recorded under a variety of listening conditions. In competing-speakers and speech-in-noise conditions, the DNNs significantly outperformed the linear models. Finally, the DNNs offered a considerable improvement over the linear approach at decoding auditory attention in competing-speakers scenarios. Significance. We present the first detailed study into the extent to which DNNs can be employed for reconstructing the envelope of an attended speech stream. We conclusively demonstrate that DNNs improve the reconstruction of the attended speech envelope. The variance of the reconstruction error is shown to be similar for both DNNs and the linear model. DNNs therefore show promise for real-world AAD, since they perform well in multiple listening conditions and generalise to data recorded from unseen participants.\n\nIntroduction\n\nConventional hearing aids are known to provide only a limited benefit to their users, especially when operating in noisy conditions [1]. The ability to determine the focus of a user's attention could enable the development of smart hearing aids with improved outcomes for those who suffer with hearing loss. Recent studies have demonstrated that auditory attention in multi-speaker ('cocktail party') scenarios can be decoded noninvasively from electrophysiological recordings such as the electroencephalogram (EEG) [2][3][4][5][6]. One common paradigm for auditory attention decoding (AAD) is the method of backward linear modelling, whereby a set of coefficients are estimated in order to linearly reconstruct a speech feature from EEG recordings. In AAD applications, the speech feature is typically chosen to be the speech envelope, but other features can also be used, such as a waveform related to the fundamental frequency of speech [7,8]. Both the speech envelope and the fundamental waveform of the attended speech stream are more strongly represented in a listener's EEG, and can be more accurately reconstructed from EEG recordings than corresponding features of the unattended speech streams. Therefore, in the backward modelling approach, a reconstruction score (typically Pearson's correlation coefficient between the reconstructed and the actual speech feature) for each speech stream serves as a marker of selective attention.\n\nSince the processing in the auditory system is inherently nonlinear, it is natural to ask whether nonlinear methods for AAD can offer superior performance over linear methods. Nonlinear methods for backward modelling and AAD based on artificial neural networks have been introduced recently [9,10]. Here, we set out to undertake a comprehensive account of the use of deep neural networks (DNNs) for backward nonlinear modelling of the speech envelope from EEG recordings. Artificial neural networks are heavily-parameterised, nonlinear models which are capable of representing a broad class of functions. In fact, they are universal function approximators [11]. DNNs are artificial neural networks which contain many layers of processing units (neurons). The correlation-based AAD technique described above can be also be used in conjunction with a DNN, by exchanging the linear backward model with a nonlinear DNN. Alternatively, auditory attention can be decoded directly without first reconstructing features from the EEG recordings, by utilising a DNN-based classifier which accepts EEG recordings as well as the candidate speech envelopes as inputs [10].\n\nNonlinear forward modelling based on DNNs has recently been employed to quantify the level of nonlinear processing that contributes to neural activity evoked by continuous speech [12]. The authors of that study found that as much as 25% of the evoked response arises due to nonlinear processes which can be captured by DNNs, thus justifying the use of DNNs for AAD. However, DNNs are known to suffer from issues surrounding generalisability. This has been highlighted by some recent investigations which did not achieve a competitive AAD performance across multiple datasets, when using DNNs which have elsewhere been reported to be effective [10,13].\n\nIn this work, we compared the performance of two nonlinear DNNs as well as one linear model for predicting the speech envelope from EEG recordings. Following a recent study, we examined a fully-connected (FC) feed-forward neural network (FCNN) [9]. We also considered a more lightweight convolutional neural network (CNN) based on the EEGNet architecture, which has been proposed for a range of brain-computer-interface applications [14].\n\n\nMaterials and methods\n\n\nDatasets and preprocessing\n\nTwo datasets from our research group were used in this work. The first dataset (termed Dataset 1 hereafter) was collected by Weissbart et al [15]. A total of 13 native English-speaking participants were instructed to attend to a single speaker narrating an audiobook in English, in noiseless and anechoic listening conditions. The EEG was recorded from all 13 participants, and each participant listened to 15 audiobook chapters in one recording session. The duration of each chapter was approximately 2.5 min, and each participant took breaks between chapters. This dataset therefore consists of 40 minutes of EEG responses to clean speech per participant. During the breaks, the participants were asked to answer a comprehension question in order to ensure attendance to the audiobook.\n\nThe second dataset (termed Dataset 2 hereafter) was collected by Etard and Reichenbach [16]. A total of 18 native English-speaking participants attended to a speaker narrating an audiobook chapter in several listening conditions: clean speech, speech in noisy conditions, and speech in competing-speaker scenarios. For the noisy speech, background babble noise was synthesised and combined with the speech at three different signal-to-noise ratios (SNRs) of 0.4 dB, \u22121.4 dB, and \u22123.2 dB. The comprehension levels for each SNR condition were 81%, 60%, and 34%, respectively, as measured through behavioural experiments. For the competing-speaker scenarios, two audiobooks were narrated simultaneously by a male and female speaker. There were two competingspeaker scenarios; in the first, the listener was instructed to attend to the male speaker whilst ignoring the female speaker, and in the second they were instructed to attend to the female speaker whilst ignoring the male speaker. Additionally, 12 of the participants listened to a speaker narrate an audiobook in a foreign language, Dutch. In this listening condition, the comprehension level was 0%. All stimuli were delivered binaurally. As with Dataset 1, the participants were asked comprehension questions in order to ensure attendance to the target audiobook. For each listening condition, the EEG was recorded in four trials of approximately 2.5 min in duration. This dataset therefore consists of 10 min of EEG recorded for each listening condition per participant.\n\nIn both datasets, 64-channel scalp EEG was recorded with the same equipment inside the same anechoic chamber. The EEG was sampled at a rate of 1 kHz, with electrodes positioned according to the standard 10-20 system via the actiCAP electrode cap (BrainProducts, Germany). The EEG signals were then amplified and digitised with the actiCHamp amplifier (BrainProducts, Germany). In Dataset 1, the left earlobe was used as the physical EEG reference, whereas the right earlobe was used for this purpose in Dataset 2. In order to align the stimulus with the recorded EEG, the acoustic adapter for StimTrack (BrainProducts, Germany) was used to record the audio at 1 kHz whilst simultaneously presenting it to the participant (at 44.1 kHz). The resulting sound channel was used to align the original audio tracks with the EEG recordings during post-processing.\n\nPreprocessing was performed using default routines available in MNE-Python version 0.24.1 [17]. To obtain the speech envelopes, we computed the absolute value of the Hilbert transform of each speech stream. The speech envelopes were low-pass filtered below 50 Hz (linear phase type 1 FIR antialiasing filter, Hamming window, 12.5 Hz transition bandwidth, \u22126 dB attenuation at 56.25 Hz, \u221253 dB stopband attenuation) and resampled to 125 Hz. To preprocess the EEG recordings, all channels were low-pass filtered below one of several upper passband edges (linear phase type 1 FIR anti-aliasing filters, Hamming windows, \u221253 dB stopband attenuation). The considered upper passband edges were: 8 Hz (order 1651, 2 Hz transition bandwidth, \u22126 dB attenuation at 9 Hz), 12 Hz (order 1101, 3 Hz transition bandwidth, \u22126 dB attenuation at 13.5 Hz), 16 Hz (order 825, 4 Hz transition bandwidth, \u22126 dB attenuation at 18 Hz) and 32 Hz (order 413, 8 Hz transition bandwidth, \u22126 dB attenuation at 36 Hz). The EEG recordings were subsequently resampled to 125 Hz and high-pass filtered above one of two lower passband edges in order to remove slow drifts (linear phase type 1 FIR filters, Hamming windows, \u221253 dB stopband attenuation): 0.5 Hz (order 825, 0.5 Hz transition bandwidth, \u22126 dB attenuation at 0.25 Hz), or 2 Hz (order 207, 1 Hz transition bandwidth, \u22126 dB attenuation at 1.5 Hz). Finally, for every trial, each EEG channel was standardised to have zero mean and unit variance.\n\n\nLinear models\n\nA linear backward model can be specified in the time domain by a matrix of parameters \u03b8 i,j . These are convolved with the EEG recordings to produce an estimate of the speech envelope:\ny t = C \u2211 i=1 L\u22121 \u2211 j=0 x t\u2212j,i \u03b8 i,j .(1)\nIn this expression,\u0177 t denotes an estimate of the speech envelope sampled at time t, x t,i designates the EEG sampled at time t from electrode i, C represents the number of EEG channels being considered, and L is the filter length which describes how many temporal EEG samples are employed to estimate the speech envelope. In other words, the speech envelope is represented as a linear combination of the EEG recordings x t\u2212j,i , which are weighted by the parameters \u03b8 i,j . The parameters of the linear model are obtained by minimising the sum of squared errors \u2211 T t=1 (y t \u2212\u0177 t ) 2 , where T is the total number of time samples available in the training dataset. We used ridge regression, which employs an L2 regularisation term \u03bb \u2211 C i=1 \u2211 j=0 \u03b8 2 i,j within the objective function. This results in a better-posed regression problem which is less susceptible to overfitting [18,19]. The L2 penalty penalises large weights, and the strength of the penalty is controlled by the hyperparameter \u03bb.\n\n\nDNNs\n\nThe linear models in (1) depend on L \u00d7 C parameters. Nonlinear models implemented as DNNs typically employ a much larger set of parameters and possess a more complicated functional form. The DNNs in this work can therefore be considered as more general functions relating the EEG recordings and the speech envelope.\n\nThe fundamental unit of any neural network is the 'neuron' . A neuron receives a pre-determined number of inputs, which it linearly combines according to its set of parameters (or weights). A nonlinear activation function is then applied to the resulting scalar quantity. Common choices for the activation function are the sigmoid and the hyperbolic tangent, as well as the rectified linear unit (ReLU) [20]. The latter is defined as the function f(x) = x if x > 0 and f(x) = 0 otherwise.\n\nA feed-forward neural network consists of layers of neurons, with neurons in a particular layer receiving as inputs the outputs of neurons in preceding layers (neurons within a single layer do not connect with one another). If each neuron in a particular layer is connected with each neuron in the preceding layer, the neural network is described as 'fully connected' (FC). The fully-connected feed-forward neural network (FCNN) used in this work is depicted in figure 1. If each neuron in a particular layer is instead only connected to a neighbourhood of input neurons, and all neurons in the same layer share the same parameters (weight sharing), then the neural network is described as a convolutional neural network (CNN). The CNN that was used in this work is shown in figure 1. Other types of connectivities exist, including skip connections (e.g. residual neural networks [21]) and feedback connections (recurrent neural networks [11]).\n\nThe FCNN used in this work was inspired by the architecture used by de Taillez et al [9]. A spatiotemporal segment of EEG recordings is passed through several FC feed-forward layers, with each layer containing fewer neurons than the preceding layer. The activation function is the hyperbolic tangent. The A nonlinear activation function is applied to the output of each hidden layer, followed by a dropout layer. Bottom: the CNN architecture employs convolutional layers, which make use of local connectivity and weight-sharing to reduce the number of parameters in the network. The 1st convolutional layer consists of F1 convolutional filters, each sharing a common input but comprising distinct parameterisations. The 2nd convolution flattens the channel dimension, and consists of D \u00d7 F1 convolutional filters. The 3rd convolutional layer implements the so-called depthwise separable convolution, which is similar to an ordinary convolutional layer consisting of F2 low-rank convolutional filters. Following [14], we set F2 = D \u00d7 F1 to reduce the dimensionality of the hyperparameter search. Several operations may be applied to the activations of each convolutional layer, including a non-linear activation function, batch normalisation, and spatial dropout. Average pooling is a form of downsampling whereby the activations of a neighbourhood of neurons are replaced by the average activation. This form of downsampling is only applied along the temporal dimension. number of inputs is equal to C \u00d7 T, with C as the number of EEG channels used, and T as the number of temporal samples in the segment. The scalar output represents a point estimate of the speech envelope at the onset of the segment. Following de Taillez et al, the number of neurons in each hidden layer decreases linearly from C \u00d7 T to 1. The number of hidden layers is a tunable parameter.\n\nIn this work, two types of regularisation are used to help control overfitting. The first type, dropout, randomly sets the activation of some neurons to zero according to some probability [22]. This regulates how strongly the neural network can depend on the activation of any particular neuron. The second type, L2 regularisation or 'weight decay' , uses a term \u03bb \u2211 N k=1 |w k | 2 within the objective function, where w k denotes the kth parameter of the neural network. This term penalises neural networks for which some weights are much larger than others, and promotes neural networks which do not rely too much on any particular neuron. The tunable hyperparameter \u03bb controls the strength of the regularisation penalty.\n\nThe number of weights required to fully connect two adjacent layers consisting respectively of N 1 and\nN 2 neurons is N 1 \u00d7 N 2 or N 1 \u00d7 N 2 + N 2 , depending\non whether a bias term is used. The number of parameters in an FCNN therefore grows quickly with the number hidden layers, and the FCNN can become over-parameterised. Due to local connectivity, CNNs can often represent similar functions to FCNNs with far fewer parameters, thus helping to prevent overfitting. We therefore investigated the performance of a CNN at reconstructing the speech envelope from EEG recordings.\n\nOur choice of CNN was inspired by the EEGNet architecture of Lawhern et al [14], which employs the exponential linear unit (ELU) as a nonlinear activation function, as well as batch normalisation and average pooling [23]. Batch normalisation improves convergence during training by making the optimisation problem smoother [24,25]. Average pooling is a form of downsampling, in which the average activation of a neighbourhood of neurons is taken and used as the input to the next layer. To regularise the CNN, we used L2 regularisation and a variant of dropout known as spatial dropout [26], whereby entire weight-sharing layers are dropped from the training process according to some probability. This technique can be more effective than ordinary dropout for training CNNs, since weight sharing dilutes the effect of dropping the activation of individual neurons. The CNN architecture includes a 'depthwise separable convolution' layer, which utilises low-rank approximations to ordinary convolutional filters. The scalar output of the CNN is formed by taking a linear combination of all of the activations in the final convolutional layer.\n\n\nTraining procedure\n\nFor both of the DNNs as well as the linear model, the spatio-temporal input segment consisted of 63 EEG channels and 50 time samples (C=63; T=50). The temporal length of the segment was therefore 400 ms, since a sampling rate of 125 Hz was used. For the CNN, average pooling was performed across the temporal axis using a neighbourhood of two neurons after the spatial convolution layer, and a neighbourhood of five neurons after the depthwise convolution layer. Therefore, the values of the temporal dimensions T \u2032 and T \u2032\u2032 in figure 1 were 25 and 5, respectively.\n\nThe coefficients of the linear model were fitted through ridge regression, as discussed in section 2.2. Ridge regression permits a simple closed-form expression for the optimal (least-squared-error) coefficients, given a training dataset and a regularisation parameter. In contrast, DNNs can rarely be solved analytically, and gradient-descent methods are commonly used to train them (that is, to fit their parameters). Following [9], in this work we optimised the DNN parameters by minimising the negative correlation coefficient between the reconstructed speech envelope and the target speech envelope. The NAdam optimiser was used, which employs adaptive step sizing and accelerated gradient descent through a Nesterov-like momentum term [27].\n\nDataset 1 consisted of 15 trials per participant, each of approximately 2.5 min in duration. We reserved nine of these trials for model training, three for validation, and three for evaluation. Dataset 2 consisted for four trials per listening condition, per participant. Each trial had a duration of approximately 2.5 min. We used eight trials for model training (four clean-speech trials and four high-SNR speech-in-noise trials), and four trials for validation (from the low-SNR speech-in-noise condition). The remaining trials were used for evaluation.\n\nDuring training, batches of EEG data were presented to the DNNs, and a corresponding batch of predicted speech envelope values was produced. These were correlated against the actual speech envelope values, and the DNN parameters were updated via a NAdam gradient descent step in order to maximise the correlation coefficient. After iterating through all batches of data (one epoch), the correlation score was evaluated on the validation dataset. An early-stopping procedure was used with a patience factor of P: if the validation correlation score did not increase within P consecutive epochs, training was terminated. Otherwise, the process was repeated for another epoch. The model parameters which produced the highest validation correlation score were saved. During hyperparameter tuning, P was set to 3. Once the hyperparameters were fixed, the DNNs were trained with an increased patience factor of 5.\n\nFor each analysis, we trained 15 linear models with different regularisation parameters spaced evenly on a logarithmic scale (ranging from 10 \u22127 to 10 7 inclusive). The model that achieved the highest correlation score on the validation dataset was selected for testing.\n\nThe DNN hyperparameters were tuned by randomly sampling 80 hyperparameter configurations (random search), and the configuration that led to the highest validation score (correlation coefficient) was selected for testing. The DNN hyperparameters included an L2 regularisation (weight decay) parameter, the initial optimiser step size (learning rate), the number of hidden layers or convolutional filters, and the number of filters belonging to each convolutional layer. We only tuned these hyperparameters once per DNN, for the population models trained using Dataset 1.\n\nFor the FCNN, the parameters of the random search were as follows: batch size = (64, 128, or 256); weight decay = (10 \u22128 , 10 \u22127 , ..., or 10 \u22122 ); number of hidden layers = (1, 2, 3 or 4); weight decay = (10 \u22128 , 10 \u22127 , ..., or 10 \u22122 ); initial learning rate = (10 \u22126 , 10 \u22125 , ..., or 10 \u22122 ). The dropout rate was a real number sampled uniformly between 0 and 0.5.\n\nFor the CNN, the parameters of the random search are as follows: batch size = (64, 128, or 256); weight decay = (10 \u22128 , 10 \u22127 , ..., or 10 \u22122 ); initial learning rate = (10 \u22126 , 10 \u22125 , ..., or 10 \u22122 ). The spatial dropout rate was a real number sampled uniformly between 0 and 0.4. In order to reduce the dimensionality of the random search we used the condition F 2 = F 1 \u00d7 D, with D = (2, 4, or 8) and F 1 = (2, 4, or 8).\n\nSince the task of fitting subject-specific models is quite different to fitting a population model, the optimal hyperparameters for each subject-specific DNN might vary. In this work, we re-tuned the initial learning for each subject-specific DNN using a similar random search procedure, whilst holding the other hyperparameters fixed. For the FCNN population model, we found that the following hyperparameters were suitable: three hidden layers; a dropout rate of 0.45; a batch size of 256; and a weight decay value of 1 \u00d7 10 \u22124 . Therefore, the number of neurons in each hidden layer were, in order, 2363, 1576, and 788. For the CNN population model, we found the following parameters to be suitable: F1 = 8; D = 8; a spatial dropout rate of 0.20; a batch size of 256; and a weight decay value of 1 \u00d7 10 \u22128 . Recall from figure 1 that F1 is the number of convolutional filters in the first convolutional layer, and F1 \u00d7 D is the number of convolutional filters in each of the second and third convolutional layers.\n\n\nAnalysis procedure\n\nTo evaluate the models, the EEG data were split into contiguous windows without overlap. Window sizes of 250 samples (two seconds) were considered unless otherwise stated. As in the training step, the predicted speech envelope values in each window were correlated against the actual speech envelope values. The mean and variance of the correlation score over all windows were then calculated, since these quantities are of interest in AAD applications. To construct a null distribution, the predictions for each window were also correlated against the true speech envelope in unrelated windows.\n\nIn section 3.4 we applied the linear model as well as both DNNs to EEG recorded in competingspeakers scenarios. The correlation-based method was used to decode auditory attention using each DNN or the linear model. The performance of each backward model was quantified through the attention decoding accuracy for a given window size W. The information transfer rate (bit rate) B was also calculated according to [28,29]:\nWB = log 2 N + P log 2 P + (1 \u2212 P) log 2 1 \u2212 P N \u2212 1 ,(2)\nwhere N is the number classes in the classification problem (two in this case), and P is the attention decoding accuracy. The bitrate in (2) is scaled by the latency of the decoder, W, to obtain an effective bitrate that takes into account the temporal resolution of the decoder. In this work, W was calculated by adding the duration of the temporal receptive field (T, which corresponds to 0.4 s in this study) to the duration of the window which was used to calculate the correlation coefficients. The neural networks were implemented in PyTorch version 1.10.0 [30]. Statistical analyses were conducted using Scipy version 1.7.1 and Statsmodels version 0.11.1 [31,32].\n\n\nResults\n\n\nSubject-specific models\n\nIn Dataset 1, thirteen participants listened to a single speaker who narrated an audiobook in English without background noise. The participants' EEG was recorded from 63 scalp channels whilst they listened. For our first analysis, we fitted linear and nonlinear models to each participant's EEG in order to reconstruct the envelope of the speech stream. We tested the performance of the models by dividing the test data into windows of a duration of two seconds. The reconstructed speech stream was subsequently correlated against the actual speech stream in each window. We performed this procedure using several different EEG frequency bands, and we found that using the 0.5-8 Hz band yielded linear decoders with the greatest reconstruction scores (Pearson correlation coefficients) ( figure 2(a)). For this frequency band, the spread of reconstruction scores for all participants is reported in figure 2(b). We used this frequency band for all subsequent analyses.\n\nNull distributions for the reconstruction scores were obtained by correlating each reconstructed speech envelope with the true speech envelope from an unrelated window. The median values of the null distributions are shown in green on figure 2(b). Since the reconstruction scores and null distributions were approximately normally distributed, we tested the reconstruction scores for significance with a t-test (single-tailed unpaired t-test, FDR-corrected). In addition, we compared the reconstruction scores of each pair of models for every participant (paired t-tests, FDR corrected). The corrected p-values are reported in table 1. All of the models achieved significant reconstruction scores for every participant. There were somewhat significant differences between the performances of the two DNNs for 5 participants. The CNN (FCNN) outperformed the linear model with significance for 11 (9) of the participants.\n\nTo analyze the performance on the population level, we calculated the mean reconstruction score for every participant and model. We then compared the 13 mean reconstruction scores achieved by each model. We found no significant difference between the two DNNs (p = 0.91, two-tailed paired t-test). However, both DNNs significantly outperformed the linear model (CNN: p = 1.1 \u00d7 10 \u221204 ; FCNN: p = 2.1 \u00d7 10 \u221204 ; single-tailed paired t-tests, Bonferroni corrected.)\n\nThe mean and standard deviation of the reconstruction score varied with window duration. We determined the dependence of the mean and standard deviation of the reconstruction scores on the window duration by performing the analysis procedure with windows of various sizes (ranging from 0.1 s and 10 s). The mean and standard deviation of the reconstruction scores were averaged over all participants (figure 3). The mean reconstruction scores of both linear and nonlinear models are strongly degraded for window sizes less than 2 s. For window sizes greater than 2 s, the mean reconstruction score for each DNN was around 30% above that of the linear model. The mean of the set of 13 standard deviations was very similar for all three methods across all window sizes.\n\n\nSubject-independent models\n\nTo test whether the models generalise between participants, we left one participant's data out of the training procedure, and instead trained each of the models on the data from the 12 remaining participants. We then repeated this process 13 times, leaving out a different participant each time. In this way, we trained 13 subject-independent models and applied them to data from the unseen participant. For comparison, we also trained population models using training data from all of the participants, and applied these to distinct test data (recorded from the same 13 participants). Our results are summarised in figure 4. On the subject level, the use of the linear subjectindependent models resulted in significant mean reconstruction scores for 9 of the 13 participants (single-tailed unpaired t-test, FDR corrected). Both the subject-independent CNN and FCNN yielded significant reconstruction scores for 12 participants. For each participant, we compared the use of each pair of subject-independent models using paired ttests (FDR corrected). The CNN and FCNN did not perform significantly differently for any of the 13 participants. The CNN (FCNN) outperformed the linear method with significance for 1 (3) participants. On the population level, both subjectindependent DNNs significantly outperformed the subject-independent linear models (CNN: p = 9.2 \u00d7 10 \u22125 ; FCNN: p = 0.01; single-tailed t-tests, Bonferroni corrected). There was no significant difference between the subject-independent DNNs on the population level.\n\nThe subject-independent decoders yielded scores which were approximately 50% below those of the subject-specific decoders. The population decoders performed better than the subject-independent decoders, but worse than the subject-specific decoders. This is to be expected, since the subjectspecific and subject-independent decoders respectively represent the two extremes in which either there is only subject-specific information available, or there is no subject-specific information available.\n\n\nPerformance of subject-specific models in different listening conditions\n\nFor real-world applications, a decoder needs to perform well across a range of listening conditions. We therefore trained subject-specific decoders using Dataset 2, which consisted of EEG recorded under a number of different listening conditions. We used the clean speech in native English, as well as speech in the highest SNR condition (0.4 dB) to train the decoders. We used the lowest SNR condition (\u22123.2 dB) to validate the training procedure, and we evaluated the trained decoders on the medium SNR condition (\u22121.4 dB), as well as on competing-speaker scenarios, and the clean speech in foreign Dutch condition (for which the comprehension level was 0%). For each method, we calculated the mean reconstruction score for each participant. The spread of mean reconstruction scores in each condition are shown in figure 5. To compare the performance of the trained models in each listening condition, we compared the sets of mean reconstruction scores achieved by each pair of models within each listening condition using two-tailed paired t-tests (FDR corrected). There was no significant difference between the DNNs in any listening condition. However, both DNNs significantly outperformed the linear model at reconstructing the attended speech stream in the competingspeakers conditions, as well as in the background babble noise condition. The DNNs performed similarly to the linear models at reconstructing the envelope of clean speech in a foreign language, as well as at reconstructing the unattended speech envelope in the competing-speakers conditions.\n\n\nAttention decoding performance\n\nFor our final case study, we investigated whether the subject-specific decoders described in section 3.3 could actually be used for AAD in the competingspeaker scenarios. We compared the reconstruction score (correlation coefficient) for the attended and unattended speakers in each window, and counted how many times the reconstruction of the attended envelope was greater than that of the unattended envelope. This number was taken to be the number of correct classifications, from which the binary classification accuracy could be directly calculated Table 1. Statistical tests on the data shown in figure 2(b). The first three rows show the p-values obtained when testing for difference between the reconstruction scores (\u03c1) and null distributions (\u03c10) for each method. These were FDR-corrected independently of the p-values in the next three rows. The next three rows show the p-values obtained by testing for differences between the reconstruction scores of the different techniques. P-values smaller than 0.05 are presented in boldface.  Figure 3. The mean (a) and standard deviation (b) of the reconstruction score (Pearson's correlation coefficient), averaged over all participants from Dataset 1, is plotted against the size of the correlation window used during evaluation. For window sizes less than 2 s in duration, the mean reconstruction score was considerably degraded. The variability of the reconstruction score increases sharply with decreasing window size, but is similar for all three methods. The dotted lines show the mean and standard deviation of the null reconstruction scores for the three methods.\n\n\nFigure 4.\n\nComparison between subject-specific and subject-independent decoders applied to clean speech (Dataset 1). The boxplots represent the spread of the mean reconstruction scores achieved for each participant. The first group (subject-specific models) shows the mean reconstruction scores achieved by subject-specific decoders applied to Dataset 1. The second group (leave-one-subject-out experiment) shows the mean reconstruction scores achieved by the subject-independent decoders described in section 3.2. For the third group (population models), population models were trained with Dataset 1 and subsequently applied them to data recorded from individual subjects in Dataset 2. Statistical significance is denoted by asterisks (none, p >=0.5; * , 0.5 > p >= 0.1; * * , 0.1 > p >=0.01; * * * , 0.01 > p >= 0.001; * * * * , 0.001 > p >=0.0001; * * * * * , p < 0.0001).\n\nas a percentage of the total number of windows in the trial. The decoding accuracies for three different window durations (2 s, 5 s, 10 s) are shown in figure 6. Both DNNs offered clear accuracy improvements over the linear model across all three window durations.\n\nFollowing [9], we also calculated effective bitrates for attention decoding using different window sizes. The bitrate is related to the time-rate of correct classifications, and was calculated according to equation (2). We found that a window size of 2 s maximises the decoding performance of the CNN as well as that of the linear model ( figure 6(b)). A window size of 5 s was marginally more suitable for the FCNN. Both DNNs achieved much higher bitrates than the linear model.\n\n\nDiscussion\n\nWe have investigated the performance of two types of DNNs at estimating the speech envelope from EEG recordings. The performance of each DNN was compared to that of a standard linear model. A comprehensive evaluation has shown that the two DNNs can achieve very similar performances when reconstructing the envelope of clean speech, whilst exceeding that of the linear model by about 30%. The advantage Figure 6. Comparison of the attention decoding accuracies of the three methods, across three different window sizes (a). The information transfer rate for the three methods, which quantifies the tradeoff between decoding accuracy and window duration (b). of using the DNNs over the linear models persisted even when the models were applied to subjects whose EEG data had not been seen during training. Importantly, the DNN architectures and hyperparameters have been shown to generalise to a distinct data set, and subject-specific models have been applied effectively to data in which speech was presented in different types of noise. Our results have demonstrated that DNNs have the ability to robustly enhance the decoding of speech features from EEG recordings.\n\n\nDeep learning methodology\n\nTo use the DNNs effectively, some special steps were taken. Firstly, rather than presenting batches of consecutive EEG windows to the DNNs during training, we shuffled the order of the windows in the training dataset. This is a departure from the approach suggested in de Taillez et al [9]. We also used much smaller batch sizes to train the DNNs. Small batch sizes are often desirable in deep learning, since they can help to avoid overfitting via noise injection [33,34]. We note that a much larger batch size of 1024 samples was used in the study by Cicarelli et al [10], and no weight decay was employed. In comparison to other studies, we performed a more complete hyperparameter search, which included a search over L2 regularisation parameters, training batch sizes, initial learning rates, dropout rates, and the number of hidden layers/ convolutional filters. Furthermore, we re-tuned the initial learning rate for each subject-specific DNN and participant, whilst keeping the other hyperparameters fixed. Future work will explore further individualisation of additional hyperparameters, for example the weight decay parameter. In our study, we found that it was most effective to use three hidden layers within the FCNN, whereas previous studies have made use of just one hidden layer [9,10].\n\nThe power of EEG signals vary between participants, and may vary over time and between recording sessions for a single participant. This is of little consequence for trained linear models, the outputs of which are equivariant with respect to scalings of the inputs (thus leaving correlation-based reconstruction scores invariant). However, the scores achieved by the DNNs are sensitive to changes in the power of the input channels, due to the nonlinear activation functions employed by the DNNs. In order to fix the power of the inputs, we standardised each EEG channel to have zero mean and unit variance, for all subjects in all trials. This re-scaling is non-causal, and for real-world applications it may be preferable to standardise the EEG recordings by first removing the mean via high-pass filtering, and then normalising the power of each EEG channel by dividing the recordings by a rolling estimate of the standard deviation. Full-cap EEG signals may alternatively be standardised across the channel axis, i.e. by removing the mean and dividing by the standard deviation of all EEG signals at each sample. However, this method would not be appropriate for low-density wearable montages such as concealed EEG or ear-EEG [5,6]. We note that the calculation of the speech envelope via the Hilbert transform is also a noncausal operation which must be adapted for real-time applications.\n\nThe scores achieved by the two DNNs investigated in this work were remarkably close. In fact, the outputs of the two neural networks were themselves highly correlated, which suggests that the two DNNs had learned to represent very similar functions. Owing to local neuron connectivity, the CNN required far fewer parameters than the FCNN: about nine thousand versus twelve million, respectively. The linear model required about three thousand parameters, which is comparable to the number of parameters in the CNN. The CNN may therefore be a preferable, lightweight alternative to the FCNN for practical applications. Future investigation may reveal effective architectures which are even more lightweight, for example by removing or 'pruning' neurons which are of low importance [35,36]. Additionally, prior information about this signal processing problem could be exploited by imposing inductive biases on a DNN [37]. For example, the neural response to the speech envelope has been well characterised in the literature, and the spatial arrangement of the EEG sensors is known by the experimentalist.\n\n\nSubject-specific decoders\n\nThe existing literature surrounding DNNs for AAD focuses almost exclusively on the attention decoding accuracy of the correlation-based algorithm, with DNNs being used to reconstruct the attended speech stream. It is natural to also investigate how well the DNNs perform at the fundamental task of reconstructing the attended speech stream. In order to do this, we began by training subject-specific models to predict the envelope of clean speech from EEG recordings. The effect of broadening the EEG frequency band from 0.5-8 Hz to 0.5-32 Hz had no discernible impact on the performance of the DNNs, as measured via Pearson's correlation coefficient between the actual and reconstructed speech envelopes. However, the use of the frequency band 0.5-8 Hz in place of the frequency band 2-8 Hz led to considerably improved results when decoding the envelope of clean speech. De Taillez et al found it beneficial to use broadband EEG signals in the range 1-32 Hz instead of signals in the range 2-8 Hz, since this resulted in a higher information transfer rate when using DNNs to decode auditory attention [9]. It is likely that much of this benefit can be attributed to the incorporation of lower-frequency components of the EEG signals. The effect of incorporating higherfrequency EEG components on the attention decoding accuracy in competing-speakers conditions cannot be directly inferred from our analysis, since we only considered the effect of the spectral content of the EEG signals in the context of reconstructing the envelope of clean speech in quiet conditions. We used the 0.5-8 Hz EEG frequency band for subsequent analyses. Both of the DNNs as well as the linear modelling method achieved significant reconstruction scores for all participants. On the population level, the improvement offered by the DNNs was statistically significant. The overall performance of the DNNs was around 30% greater than that of the linear model. Even on the subject level, the CNN (FCNN) offered a statistically significant performance increase compared against the linear model for 11 (9) participants.\n\nWe found that for windows smaller than around 2 s in duration, the reconstruction accuracy of all three methods was severely degraded. The latency of a real-world decoder which is based on the correlation method may therefore be limited to this timescale, unless techniques such as state-space models are employed [3]. Indeed, we found that a window size of about 2 s maximises the information transfer rate of the correlation-based AAD algorithm. The variability in the reconstruction score followed similar power-law dependencies on window size for all three methods. This finding contrasts with a previous study which found that the reconstruction score of a DNN similar to the FCNN used in this work was much more variable than that of a linear model, when applied in a competing-speaker scenario [38].\n\n\nSubject-independent decoders\n\nUsing Dataset 1, we trained 13 versions of each DNN and linear model to reconstruct the envelope of clean speech from EEG recordings. Each version was obtained by leaving out one of the thirteen participants during training. The DNNs and the linear model were then applied to the data of the 'unseen' participant. This allowed us to compare the performance of subject-independent methods with subjectspecific methods whilst holding constant certain factors such as the experimentalist, the experimental protocol, the stimuli, and the duration of the experiment.\n\nAll three subject-independent decoders yielded reconstruction scores that were significantly different from the null distribution for the majority of the participants (9 for the linear model; 12 for both the CNN and the FCNN). We found that all three subject-independent decoders performed very similarly on the subject level. However, on the population level, the subject-independent DNNs both significantly outperformed the subject-independent linear model.\n\nThe subject-independent decoders performed significantly worse than their subject-specific counterparts (the performance decrease was around 50% for all three methods). Whilst a performance penalty is to be expected when subject-independent information is unavailable, a penalty of this magnitude may imply that some subject-specific information is required for real-world applications.\n\n\nApplication to EEG recorded under different listening conditions\n\nFor real-world decoding applications, the decoder must perform well across a variety of listening conditions. It was recently found that linear models can assess neural speech tracking in two-speaker scenarios in a manner that is robust against distortions to the two speech streams [39]. Our investigation furthers this research by studying how well linear and nonlinear models can assess neural speech tracking in clean and noisy listening conditions with varying levels of speech clarity and comprehension.\n\nDecoders for auditory attention to one of two competing speakers are usually trained on the EEG data obtained when the participants listen to two competing talkers [9,10]. Here, due to the limited amount of competing-speakers data available in Dataset 2, we used a somewhat different approach in which we trained linear and nonlinear subjectspecific decoders using a mixture of clean speech and speech-in-babble-noise conditions. The singlespeaker conditions used to train the decoder provide a more stable teaching signal than the competingspeaker conditions, in which participants may sometimes direct their attention to the speaker labelled unattended' . However, the clean-and speech-innoise-single-speaker conditions may not elicit the same attention dynamics that are exhibited in the competing-speakers conditions. It was therefore important to find that our DNNs were able to reconstruct the attended speech envelope in the competingspeakers conditions as well. It is likely that even greater reconstruction scores could be achieved if competing-speakers conditions were represented in the training dataset.\n\nWe found that the DNNs outperformed the linear model by a considerable margin when reconstructing the envelope of an attended speaker in competingspeaker scenarios, as well as in background babble noise. All three methods performed very similarly at the task of reconstructing the unattended speaker in the competing-speaker scenarios.\n\nThe three methods also performed very similarly at reconstructing the envelope of clean speech in foreign Dutch. The comprehension score in this listening condition was 0%, and it is has been shown that cortical speech tracking in the delta band is modulated by the speech comprehension level [16]. Since very low comprehension levels were not represented in the training data, this may explain why the DNNs did not perform as well in this listening condition.\n\n\nAttention decoding performance\n\nFinally, we decoded auditory attention in competingspeaker scenarios using the subject-specific decoders that were trained with Dataset 2. It was found that the use of DNNs was advantageous for this purpose, as was shown in [9]. We also replicated the finding that a short window length of about 2 s was optimal for real-time applications, in the sense that the information transfer rate (ITR) defined in equation (2) is maximised. We note that this ITR does not account for the total delay required by the proposed decoding algorithm: EEG filtering operations and audio processing operations have been neglected. The number of samples used to calculate the correlation coefficients as well as the number of temporal input samples T have been included. The bitrates that were achieved by the DNNs in this work were somewhat lower than those reported in [9], and the differences cannot be fully explained by the fact that we accounted for the temporal receptive field duration T in our calculation. The differences might be explained by the fact the authors trained their DNN using EEG recorded in a competing-speaker scenario, which was the same listening condition as was used for evaluation. Despite these differences, our study provides conclusive evidence that DNNs can be used for enhanced and robust decoding of selective attention in competing-speaker scenarios.\n\nFigure 1 .\n1The two neural network architectures used in this work. A spatiotemporal segment of T EEG samples and C channels is presented to both of the neural networks. Top: the FCNN architecture consists of several fully-connected hidden layers. The Lth hidden layer consists of ND L neurons.\n\nFigure 2 .\n2Two distinct DNN architectures, as well as a linear model, were used to relate EEG recordings to the envelope of clean speech. The decoding performance for all three methods when different EEG frequency bands are used is shown in (a). Each boxplot comprises the mean envelope reconstruction score (correlation coefficient) for each participant. The subject-level results for the EEG frequency band 0.5-8 Hz are depicted in (b). Each boxplot represents the median and range of the reconstruction score when a 2 s correlation window is employed. The median reconstruction scores of the null distributions are shown in green.\n\nFigure 5 .\n5The spread of mean reconstruction scores for the participants in Dataset 2 when subject-specific decoders were applied in different listening conditions. Statistical significance is denoted through asterisks as detailed in the caption of figure 4.\n\u00a9 2022 The Author(s). Published by IOP Publishing Ltd\nAcknowledgmentMichael Thornton was supported by the UKRI CDT in AI for Healthcare https://ai4health.io (Grant No. P/S023283/1).Code availabilitySupporting Python code is available at https://github. com/Mike-boop/mldecoders. This package contains all the functions used for data preprocessing, model training, and analysis.ORCID iDsMike Thornton \ue9d9 https://orcid.org/0000-0002-2235-5879 Danilo Mandic \ue9d9 https://orcid.org/0000-0001-8432-3963 Tobias Reichenbach \ue9d9 https://orcid.org/0000-0003-3367-3511\nWhy do hearing aids fail to restore normal auditory perception?. N Lesica, 10.1016/j.tins.2018.01.008Trends Neurosci. 41Lesica N A 2018 Why do hearing aids fail to restore normal auditory perception? Trends Neurosci. 41 174-85\n\nAttentional selection in a cocktail party environment can be decoded from single-trial EEG. J A O&apos;sullivan, A J Power, N Mesgarani, S Rajaram, J J Foxe, B G Shinn-Cunningham, M Slaney, S Shamma, E C Lalor, 10.1093/cercor/bht355Cereb. Cortex. 25O'Sullivan J A, Power A J, Mesgarani N, Rajaram S, Foxe J J, Shinn-Cunningham B G, Slaney M, Shamma S A and Lalor E C 2014 Attentional selection in a cocktail party environment can be decoded from single-trial EEG Cereb. Cortex 25 1697-706\n\nReal-time tracking of selective auditory attention from M/EEG: a Bayesian filtering approach. S Miran, S Akram, A Sheikhattar, J Z Simon, T Zhang, B Babadi, 10.3389/fnins.2018.00262Front. Neurosci. 12262Miran S, Akram S, Sheikhattar A, Simon J Z, Zhang T and Babadi B 2018 Real-time tracking of selective auditory attention from M/EEG: a Bayesian filtering approach Front. Neurosci. 12 262\n\nTowards estimating selective auditory attention from EEG using a novel time-frequencysynchronisation framework. D Looney, C Park, Y Xia, P Kidmose, M Ungstrup, D P Mandic, 10.1109/IJCNN.2010.5596618.Proc. 2010 Int. Joint Conf. on Neural Networks (IJCNN). 2010 Int. Joint Conf. on Neural Networks (IJCNN)Looney D, Park C, Xia Y, Kidmose P, Ungstrup M and Mandic D P 2010 Towards estimating selective auditory attention from EEG using a novel time-frequency- synchronisation framework Proc. 2010 Int. Joint Conf. on Neural Networks (IJCNN) pp 1-5\n\nIdentifying auditory attention with ear-EEG: cEEGrid versus high-density cap-EEG comparison. M G Bleichner, Mirkovic B Debener, S , 10.1088/1741-2560/13/6/066004J. Neural Eng. 1366004Bleichner M G, Mirkovic B and Debener S 2016 Identifying auditory attention with ear-EEG: cEEGrid versus high-density cap-EEG comparison J. Neural Eng. 13 066004\n\nSingle-channel in-ear-EEG detects the focus of auditory attention to concurrent tone streams and mixed speech. L Fiedler, M W\u00f6stmann, C Graversen, A Brandmeyer, T Lunner, J Obleser, 10.1088/1741-2552/aa66ddJ. Neural Eng. 1436020Fiedler L, W\u00f6stmann M, Graversen C, Brandmeyer A, Lunner T and Obleser J 2017 Single-channel in-ear-EEG detects the focus of auditory attention to concurrent tone streams and mixed speech J. Neural Eng. 14 036020\n\nThe human auditory brainstem response to running speech reveals a subcortical mechanism for selective attention eLife. A E Forte, Etard O Reichenbach, T , 10.7554/eLife.27203627203Forte A E, Etard O and Reichenbach T 2017 The human auditory brainstem response to running speech reveals a subcortical mechanism for selective attention eLife 6 e27203\n\nDecoding of selective attention to continuous speech from the human auditory brainstem response. O Etard, M Kegler, C Braiman, A Forte, T Reichenbach, 10.1016/j.neuroimage.2019.06.029NeuroImage. 200Etard O, Kegler M, Braiman C, Forte A E and Reichenbach T 2019 Decoding of selective attention to continuous speech from the human auditory brainstem response NeuroImage 200 1-11\n\nMachine learning for decoding listeners' attention from electroencephalography evoked by continuous speech. T De Taillez, Kollmeier B Meyer, B T , 10.1111/ejn.13790Eur. J. Neurosci. 51de Taillez T, Kollmeier B and Meyer B T 2020 Machine learning for decoding listeners' attention from electroencephalography evoked by continuous speech Eur. J. Neurosci. 51 1234-41\n\nComparison of two-talker attention decoding from EEG with nonlinear neural networks and linear methods Sci. G Ciccarelli, M Nolan, J Perricone, P T Calamia, S Haro, J O&apos;sullivan, N Mesgarani, T Quatieri, C J Smalt, 10.1038/s41598-019-47795-011538Ciccarelli G, Nolan M, Perricone J, Calamia P T, Haro S, O'Sullivan J, Mesgarani N, Quatieri T F and Smalt C J 2019 Comparison of two-talker attention decoding from EEG with nonlinear neural networks and linear methods Sci. Rep. 9 11538\n\nD P Mandic, J A Chambers, Recurrent Neural Networks for Prediction. New YorkWileyMandic D P and Chambers J A 2001 Recurrent Neural Networks for Prediction (New York: Wiley)\n\nModeling nonlinear transfer functions from speech envelopes to encephalography with neural networks. T De Taillez, F Denk, B Mirkovic, B Kollmeier, B T Meyer, 10.5539/ijps.v11n4p1Int. J. Psychol. Stud. 111de Taillez T, Denk F, Mirkovic B, Kollmeier B and Meyer B T 2019 Modeling nonlinear transfer functions from speech envelopes to encephalography with neural networks Int. J. Psychol. Stud. 11 1\n\nElectroencephalography-based auditory attention decoding: toward neurosteered hearing devices. S Geirnaert, S Vandecappelle, E Alickovic, A De Cheveigne, E Lalor, B T Meyer, Miran S Francart, T Bertrand, A , 10.1109/MSP.2021.3075932IEEE Signal Process. Mag. 38Geirnaert S, Vandecappelle S, Alickovic E, de Cheveigne A, Lalor E, Meyer B T, Miran S, Francart T and Bertrand A 2021 Electroencephalography-based auditory attention decoding: toward neurosteered hearing devices IEEE Signal Process. Mag. 38 89-102\n\nEEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces. V J Lawhern, A J Solon, N R Waytowich, S M Gordon, C Hung, B J Lance, 10.1088/1741-2552/aace8cJ. Neural Eng. 1556013Lawhern V J, Solon A J, Waytowich N R, Gordon S M, Hung C P and Lance B J 2018 EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces J. Neural Eng. 15 056013\n\nCortical tracking of surprisal during continuous speech comprehension. H Weissbart, K Kandylaki, T Reichenbach, 10.1162/jocn_a_01467J. Cogn. Neurosci. 32Weissbart H, Kandylaki K D and Reichenbach T 2020 Cortical tracking of surprisal during continuous speech comprehension J. Cogn. Neurosci. 32 155-66\n\nNeural speech tracking in the theta and in the delta frequency band differentially encode clarity and comprehension of speech in noise. O Etard, T Reichenbach, 10.1523/JNEUROSCI.1828-18.2019J. Neurosci. 39Etard O and Reichenbach T 2019 Neural speech tracking in the theta and in the delta frequency band differentially encode clarity and comprehension of speech in noise J. Neurosci. 39 5750-9\n\n. A Gramfort, 10.3389/fnins.2013.00267MEG and EEG data analysis with MNE-Python Front. Neurosci. 7Gramfort A et al 2013 MEG and EEG data analysis with MNE-Python Front. Neurosci. 7 1-13\n\nT Hastie, R Tibshirani, J Friedman, The Elements of Statistical Learning. BerlinSpringerHastie T, Tibshirani R and Friedman J 2001 The Elements of Statistical Learning (Springer Series in Statistics) (Berlin: Springer)\n\nC Bishop, Pattern Recognition and Machine Learning (Information Science and Statistics. BerlinSpringerBishop C M 2006 Pattern Recognition and Machine Learning (Information Science and Statistics) (Berlin: Springer)\n\nDeep learning in neural networks: an overview Neural Netw. J Schmidhuber, 10.1016/j.neunet.2014.09.00361Schmidhuber J 2015 Deep learning in neural networks: an overview Neural Netw. 61 85-117\n\nK He, X Zhang, Ren S Sun, J , 10.1109/CVPR.2016.90Deep residual learning for image recognition Proc. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). He K, Zhang X, Ren S and Sun J 2016 Deep residual learning for image recognition Proc. 2016 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) pp 770-8\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, J. Mach. Learn. Res. 15Srivastava N, Hinton G, Krizhevsky A, Sutskever I and Salakhutdinov R 2014 Dropout: a simple way to prevent neural networks from overfitting J. Mach. Learn. Res. 15 1929-58 (available at: http://jmlr.org/papers/ v15/srivastava14a.html)\n\nFast and accurate deep network learning by exponential linear 4th Int. D-A Clevert, T Unterthiner, S Hochreiter, Clevert D-A, Unterthiner T and Hochreiter S 2016 Fast and accurate deep network learning by exponential linear 4th Int.\n\narxiv:1511.07289Conf. on Learning Representations. San Juan, Puerto Rico, 2-4Conf. on Learning Representations (San Juan, Puerto Rico, 2-4 May 2016) (arxiv:1511.07289)\n\nS Ioffe, C Szegedy, Batch normalization: accelerating deep network training by reducing internal covariate shift Proc. 32nd Int. Conf. on Machine Learning (ICML'15). 37Ioffe S and Szegedy C 2015 Batch normalization: accelerating deep network training by reducing internal covariate shift Proc. 32nd Int. Conf. on Machine Learning (ICML'15) vol 37 pp 448-56\n\nHow does batch normalization help optimization?. S Santurkar, D Tsipras, A Ilyas, A Ma\u02dbdry, Proc. 32nd Int. Conf. on Neural Information Processing Systems pp. 32nd Int. Conf. on Neural Information essing Systems ppSanturkar S, Tsipras D, Ilyas A and Ma\u02dbdry A 2018 How does batch normalization help optimization? Proc. 32nd Int. Conf. on Neural Information Processing Systems pp 2488-98\n\nJ Tompson, R Goroshin, A Jain, Y Lecun, C Bregler, Efficient object localization using convolutional networks 2015 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). Tompson J, Goroshin R, Jain A, LeCun Y and Bregler C 2015 Efficient object localization using convolutional networks 2015 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) pp 648-56\n\nT Dozat, Incorporating Nesterov momentum into Adam Proc. Int. Conf. on Learning Representations (ICLR) Workshop. Dozat T 2016 Incorporating Nesterov momentum into Adam Proc. Int. Conf. on Learning Representations (ICLR) Workshop\n\nBrain-computer interface (BCI) operation: optimizing information transfer rates. D J Mcfarland, W Sarnacki, J R Wolpaw, 10.1016/S0301-0511(03)00073-5Biol. Psychol. 63McFarland D J, Sarnacki W A and Wolpaw J R 2003 Brain-computer interface (BCI) operation: optimizing information transfer rates Biol. Psychol. 63 237-51\n\nEEG-based communication: improved accuracy by response verification. J Wolpaw, H Ramoser, D Mcfarland, G Pfurtscheller, 10.1109/86.712231IEEE Trans. Rehabil. Eng. 6Wolpaw J, Ramoser H, McFarland D and Pfurtscheller G 1998 EEG-based communication: improved accuracy by response verification IEEE Trans. Rehabil. Eng. 6 326-33\n\nPyTorch: an imperative style, high-performance deep learning library. A Paszke, Advances in Neural Information Processing Systems. 32Curran Associates, IncPaszke A et al 2019 PyTorch: an imperative style, high-performance deep learning library Advances in Neural Information Processing Systems vol 32 (Curran Associates, Inc.) pp 8024-35\n\nSciPy 10 Contributors) 2020 SciPy 1.0: fundamental algorithms for scientific computing in python. P Virtanen, 10.1038/s41592-019-0686-2Nat. Methods. 17Virtanen P et al (SciPy 10 Contributors) 2020 SciPy 1.0: fundamental algorithms for scientific computing in python Nat. Methods 17 261-72\n\nS Seabold, J Perktold, Statsmodels: econometric and statistical modeling with python Proc. 9th Python in Science Conf. Seabold S and Perktold J 2010 Statsmodels: econometric and statistical modeling with python Proc. 9th Python in Science Conf. pp 92-96\n\nD Masters, C Luschi, arXiv:1804.07612Revisiting small batch training for deep neural networks. Masters D and Luschi C 2018 Revisiting small batch training for deep neural networks (arXiv:1804.07612)\n\nOn the generalization benefit of noise in stochastic gradient descent. S Smith, Elsen E De, S , Proc. of the 37th Int. Conf. on Machine Learning (PMLR). of the 37th Int. Conf. on Machine Learning (PMLR)Smith S, Elsen E and De S 2020 On the generalization benefit of noise in stochastic gradient descent Proc. of the 37th Int. Conf. on Machine Learning (PMLR) pp 9058-67\n\nTo prune, or not to prune: exploring the efficacy of pruning for model compression. M Zhu, S Gupta, arXiv:1710.01878Zhu M and Gupta S 2017 To prune, or not to prune: exploring the efficacy of pruning for model compression (arXiv:1710.01878)\n\nJ Frankle, M Carbin, The lottery ticket hypothesis: finding sparse, trainable neural networks Proc. 7th Int. Conf. on Learning Representations. ICLR 2019Frankle J and Carbin M 2019 The lottery ticket hypothesis: finding sparse, trainable neural networks Proc. 7th Int. Conf. on Learning Representations (ICLR 2019)\n\nM M Bronstein, J Bruna, T Cohen, P Veli\u010dkovi\u0107, arXiv:2104.13478Geometric deep learning: grids, groups, graphs, geodesics, and gauges. Bronstein M M, Bruna J, Cohen T and Veli\u010dkovi\u0107 P 2021 Geometric deep learning: grids, groups, graphs, geodesics, and gauges (arXiv:2104.13478)\n\nImproving auditory attention decoding performance of linear and non-linear methods using state-space model. A Aroudi, T De Taillez, S Doclo, 10.1109/ICASSP40776.2020.9053149IEEE Int. Conf. on Acoustics, Speech and Signal Processing. ICASSPAroudi A, de Taillez T and Doclo S 2020 Improving auditory attention decoding performance of linear and non-linear methods using state-space model 2020 IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP) pp 8703-7\n\nNoise-robust cortical tracking of attended speech in real-world acoustic scenes. S A Fuglsang, T Dau, J Hjortkjaer, 10.1016/j.neuroimage.2017.04.026NeuroImage. 156Fuglsang S A, Dau T and Hjortkjaer J 2017 Noise-robust cortical tracking of attended speech in real-world acoustic scenes NeuroImage 156 435-44\n", "annotations": {"author": "[{\"end\":186,\"start\":95},{\"end\":306,\"start\":187},{\"end\":309,\"start\":307},{\"end\":489,\"start\":310}]", "publisher": null, "author_last_name": "[{\"end\":108,\"start\":100},{\"end\":200,\"start\":194},{\"end\":328,\"start\":317}]", "author_first_name": "[{\"end\":99,\"start\":95},{\"end\":193,\"start\":187},{\"end\":308,\"start\":307},{\"end\":316,\"start\":310}]", "author_affiliation": "[{\"end\":185,\"start\":110},{\"end\":305,\"start\":202},{\"end\":488,\"start\":358}]", "title": "[{\"end\":88,\"start\":1},{\"end\":577,\"start\":490}]", "venue": "[{\"end\":592,\"start\":579}]", "abstract": "[{\"end\":3720,\"start\":1093}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3871,\"start\":3868},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4255,\"start\":4252},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4258,\"start\":4255},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4261,\"start\":4258},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4264,\"start\":4261},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4267,\"start\":4264},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4679,\"start\":4676},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4681,\"start\":4679},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5474,\"start\":5471},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5477,\"start\":5474},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5840,\"start\":5836},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6338,\"start\":6334},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6524,\"start\":6520},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6988,\"start\":6984},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6991,\"start\":6988},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7241,\"start\":7238},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7431,\"start\":7427},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7632,\"start\":7628},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8367,\"start\":8363},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10758,\"start\":10754},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13264,\"start\":13260},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13267,\"start\":13264},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14112,\"start\":14108},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15079,\"start\":15075},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15137,\"start\":15133},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15229,\"start\":15226},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16156,\"start\":16152},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17197,\"start\":17193},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18389,\"start\":18385},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18530,\"start\":18526},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18637,\"start\":18633},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18640,\"start\":18637},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18900,\"start\":18896},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20475,\"start\":20472},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20787,\"start\":20783},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25949,\"start\":25945},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25952,\"start\":25949},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26579,\"start\":26575},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26678,\"start\":26674},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26681,\"start\":26678},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28589,\"start\":28586},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36366,\"start\":36363},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36571,\"start\":36568},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38334,\"start\":38331},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":38514,\"start\":38510},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":38517,\"start\":38514},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38618,\"start\":38614},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39343,\"start\":39340},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39346,\"start\":39343},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":40582,\"start\":40579},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":40584,\"start\":40582},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":41529,\"start\":41525},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":41532,\"start\":41529},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41664,\"start\":41660},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":42984,\"start\":42981},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43961,\"start\":43958},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":44294,\"start\":44291},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":44782,\"start\":44778},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":46582,\"start\":46578},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":46973,\"start\":46970},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":46976,\"start\":46973},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":48557,\"start\":48553},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":48982,\"start\":48979},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":49611,\"start\":49608}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50420,\"start\":50125},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51056,\"start\":50421},{\"attributes\":{\"id\":\"fig_2\"},\"end\":51317,\"start\":51057}]", "paragraph": "[{\"end\":5178,\"start\":3736},{\"end\":6339,\"start\":5180},{\"end\":6992,\"start\":6341},{\"end\":7432,\"start\":6994},{\"end\":8274,\"start\":7487},{\"end\":9805,\"start\":8276},{\"end\":10662,\"start\":9807},{\"end\":12136,\"start\":10664},{\"end\":12338,\"start\":12154},{\"end\":13379,\"start\":12382},{\"end\":13703,\"start\":13388},{\"end\":14193,\"start\":13705},{\"end\":15139,\"start\":14195},{\"end\":17003,\"start\":15141},{\"end\":17728,\"start\":17005},{\"end\":17832,\"start\":17730},{\"end\":18308,\"start\":17889},{\"end\":19452,\"start\":18310},{\"end\":20040,\"start\":19475},{\"end\":20788,\"start\":20042},{\"end\":21346,\"start\":20790},{\"end\":22255,\"start\":21348},{\"end\":22527,\"start\":22257},{\"end\":23098,\"start\":22529},{\"end\":23468,\"start\":23100},{\"end\":23895,\"start\":23470},{\"end\":24913,\"start\":23897},{\"end\":25531,\"start\":24936},{\"end\":25953,\"start\":25533},{\"end\":26682,\"start\":26012},{\"end\":27689,\"start\":26720},{\"end\":28610,\"start\":27691},{\"end\":29075,\"start\":28612},{\"end\":29844,\"start\":29077},{\"end\":31407,\"start\":29875},{\"end\":31905,\"start\":31409},{\"end\":33546,\"start\":31982},{\"end\":35206,\"start\":33581},{\"end\":36085,\"start\":35220},{\"end\":36351,\"start\":36087},{\"end\":36832,\"start\":36353},{\"end\":38015,\"start\":36847},{\"end\":39347,\"start\":38045},{\"end\":40743,\"start\":39349},{\"end\":41848,\"start\":40745},{\"end\":43975,\"start\":41878},{\"end\":44783,\"start\":43977},{\"end\":45377,\"start\":44816},{\"end\":45838,\"start\":45379},{\"end\":46226,\"start\":45840},{\"end\":46804,\"start\":46295},{\"end\":47921,\"start\":46806},{\"end\":48258,\"start\":47923},{\"end\":48720,\"start\":48260},{\"end\":50124,\"start\":48755}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12381,\"start\":12339},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17888,\"start\":17833},{\"attributes\":{\"id\":\"formula_2\"},\"end\":26011,\"start\":25954}]", "table_ref": "[{\"end\":34142,\"start\":34135}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":3734,\"start\":3722},{\"attributes\":{\"n\":\"2.\"},\"end\":7456,\"start\":7435},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7485,\"start\":7459},{\"attributes\":{\"n\":\"2.2.\"},\"end\":12152,\"start\":12139},{\"attributes\":{\"n\":\"2.3.\"},\"end\":13386,\"start\":13382},{\"attributes\":{\"n\":\"2.4.\"},\"end\":19473,\"start\":19455},{\"attributes\":{\"n\":\"2.5.\"},\"end\":24934,\"start\":24916},{\"attributes\":{\"n\":\"3.\"},\"end\":26692,\"start\":26685},{\"attributes\":{\"n\":\"3.1.\"},\"end\":26718,\"start\":26695},{\"attributes\":{\"n\":\"3.2.\"},\"end\":29873,\"start\":29847},{\"attributes\":{\"n\":\"3.3.\"},\"end\":31980,\"start\":31908},{\"attributes\":{\"n\":\"3.4.\"},\"end\":33579,\"start\":33549},{\"end\":35218,\"start\":35209},{\"attributes\":{\"n\":\"4.\"},\"end\":36845,\"start\":36835},{\"attributes\":{\"n\":\"4.1.\"},\"end\":38043,\"start\":38018},{\"attributes\":{\"n\":\"4.2.\"},\"end\":41876,\"start\":41851},{\"attributes\":{\"n\":\"4.3.\"},\"end\":44814,\"start\":44786},{\"attributes\":{\"n\":\"4.4.\"},\"end\":46293,\"start\":46229},{\"attributes\":{\"n\":\"4.5.\"},\"end\":48753,\"start\":48723},{\"end\":50136,\"start\":50126},{\"end\":50432,\"start\":50422},{\"end\":51068,\"start\":51058}]", "table": null, "figure_caption": "[{\"end\":50420,\"start\":50138},{\"end\":51056,\"start\":50434},{\"end\":51317,\"start\":51070}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14978,\"start\":14970},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27520,\"start\":27509},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27631,\"start\":27620},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27937,\"start\":27926},{\"end\":30499,\"start\":30491},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32806,\"start\":32798},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34194,\"start\":34183},{\"end\":34634,\"start\":34626},{\"end\":36247,\"start\":36239},{\"end\":36703,\"start\":36692},{\"end\":37258,\"start\":37250}]", "bib_author_first_name": "[{\"end\":51937,\"start\":51936},{\"end\":52194,\"start\":52191},{\"end\":52215,\"start\":52212},{\"end\":52224,\"start\":52223},{\"end\":52237,\"start\":52236},{\"end\":52250,\"start\":52247},{\"end\":52260,\"start\":52257},{\"end\":52280,\"start\":52279},{\"end\":52290,\"start\":52289},{\"end\":52302,\"start\":52299},{\"end\":52684,\"start\":52683},{\"end\":52693,\"start\":52692},{\"end\":52702,\"start\":52701},{\"end\":52719,\"start\":52716},{\"end\":52728,\"start\":52727},{\"end\":52737,\"start\":52736},{\"end\":53093,\"start\":53092},{\"end\":53103,\"start\":53102},{\"end\":53111,\"start\":53110},{\"end\":53118,\"start\":53117},{\"end\":53129,\"start\":53128},{\"end\":53143,\"start\":53140},{\"end\":53622,\"start\":53619},{\"end\":53642,\"start\":53634},{\"end\":53644,\"start\":53643},{\"end\":53655,\"start\":53654},{\"end\":53984,\"start\":53983},{\"end\":53995,\"start\":53994},{\"end\":54007,\"start\":54006},{\"end\":54020,\"start\":54019},{\"end\":54034,\"start\":54033},{\"end\":54044,\"start\":54043},{\"end\":54436,\"start\":54433},{\"end\":54449,\"start\":54444},{\"end\":54451,\"start\":54450},{\"end\":54466,\"start\":54465},{\"end\":54762,\"start\":54761},{\"end\":54771,\"start\":54770},{\"end\":54781,\"start\":54780},{\"end\":54792,\"start\":54791},{\"end\":54801,\"start\":54800},{\"end\":55151,\"start\":55150},{\"end\":55173,\"start\":55164},{\"end\":55175,\"start\":55174},{\"end\":55186,\"start\":55183},{\"end\":55517,\"start\":55516},{\"end\":55531,\"start\":55530},{\"end\":55540,\"start\":55539},{\"end\":55555,\"start\":55552},{\"end\":55566,\"start\":55565},{\"end\":55574,\"start\":55573},{\"end\":55593,\"start\":55592},{\"end\":55606,\"start\":55605},{\"end\":55620,\"start\":55617},{\"end\":55900,\"start\":55897},{\"end\":55912,\"start\":55909},{\"end\":56173,\"start\":56172},{\"end\":56187,\"start\":56186},{\"end\":56195,\"start\":56194},{\"end\":56207,\"start\":56206},{\"end\":56222,\"start\":56219},{\"end\":56566,\"start\":56565},{\"end\":56579,\"start\":56578},{\"end\":56596,\"start\":56595},{\"end\":56609,\"start\":56608},{\"end\":56625,\"start\":56624},{\"end\":56636,\"start\":56633},{\"end\":56649,\"start\":56644},{\"end\":56651,\"start\":56650},{\"end\":56663,\"start\":56662},{\"end\":56675,\"start\":56674},{\"end\":57071,\"start\":57068},{\"end\":57084,\"start\":57081},{\"end\":57095,\"start\":57092},{\"end\":57110,\"start\":57107},{\"end\":57120,\"start\":57119},{\"end\":57130,\"start\":57127},{\"end\":57448,\"start\":57447},{\"end\":57461,\"start\":57460},{\"end\":57474,\"start\":57473},{\"end\":57816,\"start\":57815},{\"end\":57825,\"start\":57824},{\"end\":58077,\"start\":58076},{\"end\":58262,\"start\":58261},{\"end\":58272,\"start\":58271},{\"end\":58286,\"start\":58285},{\"end\":58482,\"start\":58481},{\"end\":58757,\"start\":58756},{\"end\":58891,\"start\":58890},{\"end\":58897,\"start\":58896},{\"end\":58908,\"start\":58905},{\"end\":58910,\"start\":58909},{\"end\":58917,\"start\":58916},{\"end\":59289,\"start\":59288},{\"end\":59303,\"start\":59302},{\"end\":59313,\"start\":59312},{\"end\":59327,\"start\":59326},{\"end\":59340,\"start\":59339},{\"end\":59690,\"start\":59687},{\"end\":59701,\"start\":59700},{\"end\":59716,\"start\":59715},{\"end\":60020,\"start\":60019},{\"end\":60029,\"start\":60028},{\"end\":60427,\"start\":60426},{\"end\":60440,\"start\":60439},{\"end\":60451,\"start\":60450},{\"end\":60460,\"start\":60459},{\"end\":60765,\"start\":60764},{\"end\":60776,\"start\":60775},{\"end\":60788,\"start\":60787},{\"end\":60796,\"start\":60795},{\"end\":60805,\"start\":60804},{\"end\":61136,\"start\":61135},{\"end\":61449,\"start\":61446},{\"end\":61462,\"start\":61461},{\"end\":61476,\"start\":61473},{\"end\":61755,\"start\":61754},{\"end\":61765,\"start\":61764},{\"end\":61776,\"start\":61775},{\"end\":61789,\"start\":61788},{\"end\":62082,\"start\":62081},{\"end\":62449,\"start\":62448},{\"end\":62641,\"start\":62640},{\"end\":62652,\"start\":62651},{\"end\":62896,\"start\":62895},{\"end\":62907,\"start\":62906},{\"end\":63167,\"start\":63166},{\"end\":63180,\"start\":63175},{\"end\":63182,\"start\":63181},{\"end\":63188,\"start\":63187},{\"end\":63551,\"start\":63550},{\"end\":63558,\"start\":63557},{\"end\":63709,\"start\":63708},{\"end\":63720,\"start\":63719},{\"end\":64027,\"start\":64024},{\"end\":64040,\"start\":64039},{\"end\":64049,\"start\":64048},{\"end\":64058,\"start\":64057},{\"end\":64411,\"start\":64410},{\"end\":64421,\"start\":64420},{\"end\":64435,\"start\":64434},{\"end\":64856,\"start\":64853},{\"end\":64868,\"start\":64867},{\"end\":64875,\"start\":64874}]", "bib_author_last_name": "[{\"end\":51944,\"start\":51938},{\"end\":52210,\"start\":52195},{\"end\":52221,\"start\":52216},{\"end\":52234,\"start\":52225},{\"end\":52245,\"start\":52238},{\"end\":52255,\"start\":52251},{\"end\":52277,\"start\":52261},{\"end\":52287,\"start\":52281},{\"end\":52297,\"start\":52291},{\"end\":52308,\"start\":52303},{\"end\":52690,\"start\":52685},{\"end\":52699,\"start\":52694},{\"end\":52714,\"start\":52703},{\"end\":52725,\"start\":52720},{\"end\":52734,\"start\":52729},{\"end\":52744,\"start\":52738},{\"end\":53100,\"start\":53094},{\"end\":53108,\"start\":53104},{\"end\":53115,\"start\":53112},{\"end\":53126,\"start\":53119},{\"end\":53138,\"start\":53130},{\"end\":53150,\"start\":53144},{\"end\":53632,\"start\":53623},{\"end\":53652,\"start\":53645},{\"end\":53992,\"start\":53985},{\"end\":54004,\"start\":53996},{\"end\":54017,\"start\":54008},{\"end\":54031,\"start\":54021},{\"end\":54041,\"start\":54035},{\"end\":54052,\"start\":54045},{\"end\":54442,\"start\":54437},{\"end\":54463,\"start\":54452},{\"end\":54768,\"start\":54763},{\"end\":54778,\"start\":54772},{\"end\":54789,\"start\":54782},{\"end\":54798,\"start\":54793},{\"end\":54813,\"start\":54802},{\"end\":55162,\"start\":55152},{\"end\":55181,\"start\":55176},{\"end\":55528,\"start\":55518},{\"end\":55537,\"start\":55532},{\"end\":55550,\"start\":55541},{\"end\":55563,\"start\":55556},{\"end\":55571,\"start\":55567},{\"end\":55590,\"start\":55575},{\"end\":55603,\"start\":55594},{\"end\":55615,\"start\":55607},{\"end\":55626,\"start\":55621},{\"end\":55907,\"start\":55901},{\"end\":55921,\"start\":55913},{\"end\":56184,\"start\":56174},{\"end\":56192,\"start\":56188},{\"end\":56204,\"start\":56196},{\"end\":56217,\"start\":56208},{\"end\":56228,\"start\":56223},{\"end\":56576,\"start\":56567},{\"end\":56593,\"start\":56580},{\"end\":56606,\"start\":56597},{\"end\":56622,\"start\":56610},{\"end\":56631,\"start\":56626},{\"end\":56642,\"start\":56637},{\"end\":56660,\"start\":56652},{\"end\":56672,\"start\":56664},{\"end\":57079,\"start\":57072},{\"end\":57090,\"start\":57085},{\"end\":57105,\"start\":57096},{\"end\":57117,\"start\":57111},{\"end\":57125,\"start\":57121},{\"end\":57136,\"start\":57131},{\"end\":57458,\"start\":57449},{\"end\":57471,\"start\":57462},{\"end\":57486,\"start\":57475},{\"end\":57822,\"start\":57817},{\"end\":57837,\"start\":57826},{\"end\":58086,\"start\":58078},{\"end\":58269,\"start\":58263},{\"end\":58283,\"start\":58273},{\"end\":58295,\"start\":58287},{\"end\":58489,\"start\":58483},{\"end\":58769,\"start\":58758},{\"end\":58894,\"start\":58892},{\"end\":58903,\"start\":58898},{\"end\":58914,\"start\":58911},{\"end\":59300,\"start\":59290},{\"end\":59310,\"start\":59304},{\"end\":59324,\"start\":59314},{\"end\":59337,\"start\":59328},{\"end\":59354,\"start\":59341},{\"end\":59698,\"start\":59691},{\"end\":59713,\"start\":59702},{\"end\":59727,\"start\":59717},{\"end\":60026,\"start\":60021},{\"end\":60037,\"start\":60030},{\"end\":60437,\"start\":60428},{\"end\":60448,\"start\":60441},{\"end\":60457,\"start\":60452},{\"end\":60467,\"start\":60461},{\"end\":60773,\"start\":60766},{\"end\":60785,\"start\":60777},{\"end\":60793,\"start\":60789},{\"end\":60802,\"start\":60797},{\"end\":60813,\"start\":60806},{\"end\":61142,\"start\":61137},{\"end\":61459,\"start\":61450},{\"end\":61471,\"start\":61463},{\"end\":61483,\"start\":61477},{\"end\":61762,\"start\":61756},{\"end\":61773,\"start\":61766},{\"end\":61786,\"start\":61777},{\"end\":61803,\"start\":61790},{\"end\":62089,\"start\":62083},{\"end\":62458,\"start\":62450},{\"end\":62649,\"start\":62642},{\"end\":62661,\"start\":62653},{\"end\":62904,\"start\":62897},{\"end\":62914,\"start\":62908},{\"end\":63173,\"start\":63168},{\"end\":63185,\"start\":63183},{\"end\":63555,\"start\":63552},{\"end\":63564,\"start\":63559},{\"end\":63717,\"start\":63710},{\"end\":63727,\"start\":63721},{\"end\":64037,\"start\":64028},{\"end\":64046,\"start\":64041},{\"end\":64055,\"start\":64050},{\"end\":64069,\"start\":64059},{\"end\":64418,\"start\":64412},{\"end\":64432,\"start\":64422},{\"end\":64441,\"start\":64436},{\"end\":64865,\"start\":64857},{\"end\":64872,\"start\":64869},{\"end\":64886,\"start\":64876}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1016/j.tins.2018.01.008\",\"id\":\"b0\",\"matched_paper_id\":4479962},\"end\":52097,\"start\":51871},{\"attributes\":{\"doi\":\"10.1093/cercor/bht355\",\"id\":\"b1\",\"matched_paper_id\":2934551},\"end\":52587,\"start\":52099},{\"attributes\":{\"doi\":\"10.3389/fnins.2018.00262\",\"id\":\"b2\",\"matched_paper_id\":14051440},\"end\":52978,\"start\":52589},{\"attributes\":{\"doi\":\"10.1109/IJCNN.2010.5596618.\",\"id\":\"b3\",\"matched_paper_id\":12112966},\"end\":53524,\"start\":52980},{\"attributes\":{\"doi\":\"10.1088/1741-2560/13/6/066004\",\"id\":\"b4\",\"matched_paper_id\":43003051},\"end\":53870,\"start\":53526},{\"attributes\":{\"doi\":\"10.1088/1741-2552/aa66dd\",\"id\":\"b5\",\"matched_paper_id\":3872341},\"end\":54312,\"start\":53872},{\"attributes\":{\"doi\":\"10.7554/eLife.27203\",\"id\":\"b6\"},\"end\":54662,\"start\":54314},{\"attributes\":{\"doi\":\"10.1016/j.neuroimage.2019.06.029\",\"id\":\"b7\",\"matched_paper_id\":189820126},\"end\":55040,\"start\":54664},{\"attributes\":{\"doi\":\"10.1111/ejn.13790\",\"id\":\"b8\",\"matched_paper_id\":5275290},\"end\":55406,\"start\":55042},{\"attributes\":{\"doi\":\"10.1038/s41598-019-47795-0\",\"id\":\"b9\"},\"end\":55895,\"start\":55408},{\"attributes\":{\"id\":\"b10\"},\"end\":56069,\"start\":55897},{\"attributes\":{\"doi\":\"10.5539/ijps.v11n4p1\",\"id\":\"b11\",\"matched_paper_id\":201904274},\"end\":56468,\"start\":56071},{\"attributes\":{\"doi\":\"10.1109/MSP.2021.3075932\",\"id\":\"b12\",\"matched_paper_id\":235718402},\"end\":56978,\"start\":56470},{\"attributes\":{\"doi\":\"10.1088/1741-2552/aace8c\",\"id\":\"b13\",\"matched_paper_id\":3849381},\"end\":57374,\"start\":56980},{\"attributes\":{\"doi\":\"10.1162/jocn_a_01467\",\"id\":\"b14\",\"matched_paper_id\":201803120},\"end\":57677,\"start\":57376},{\"attributes\":{\"doi\":\"10.1523/JNEUROSCI.1828-18.2019\",\"id\":\"b15\",\"matched_paper_id\":160013312},\"end\":58072,\"start\":57679},{\"attributes\":{\"doi\":\"10.3389/fnins.2013.00267\",\"id\":\"b16\"},\"end\":58259,\"start\":58074},{\"attributes\":{\"id\":\"b17\"},\"end\":58479,\"start\":58261},{\"attributes\":{\"id\":\"b18\"},\"end\":58695,\"start\":58481},{\"attributes\":{\"doi\":\"10.1016/j.neunet.2014.09.003\",\"id\":\"b19\"},\"end\":58888,\"start\":58697},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.90\",\"id\":\"b20\"},\"end\":59219,\"start\":58890},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6844431},\"end\":59614,\"start\":59221},{\"attributes\":{\"id\":\"b22\"},\"end\":59848,\"start\":59616},{\"attributes\":{\"doi\":\"arxiv:1511.07289\",\"id\":\"b23\"},\"end\":60017,\"start\":59850},{\"attributes\":{\"id\":\"b24\"},\"end\":60375,\"start\":60019},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":53104146},\"end\":60762,\"start\":60377},{\"attributes\":{\"id\":\"b26\"},\"end\":61133,\"start\":60764},{\"attributes\":{\"id\":\"b27\"},\"end\":61363,\"start\":61135},{\"attributes\":{\"doi\":\"10.1016/S0301-0511(03)00073-5\",\"id\":\"b28\",\"matched_paper_id\":4697086},\"end\":61683,\"start\":61365},{\"attributes\":{\"doi\":\"10.1109/86.712231\",\"id\":\"b29\",\"matched_paper_id\":30017646},\"end\":62009,\"start\":61685},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202786778},\"end\":62348,\"start\":62011},{\"attributes\":{\"doi\":\"10.1038/s41592-019-0686-2\",\"id\":\"b31\",\"matched_paper_id\":198229805},\"end\":62638,\"start\":62350},{\"attributes\":{\"id\":\"b32\"},\"end\":62893,\"start\":62640},{\"attributes\":{\"doi\":\"arXiv:1804.07612\",\"id\":\"b33\"},\"end\":63093,\"start\":62895},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":220128124},\"end\":63464,\"start\":63095},{\"attributes\":{\"doi\":\"arXiv:1710.01878\",\"id\":\"b35\"},\"end\":63706,\"start\":63466},{\"attributes\":{\"id\":\"b36\"},\"end\":64022,\"start\":63708},{\"attributes\":{\"doi\":\"arXiv:2104.13478\",\"id\":\"b37\"},\"end\":64300,\"start\":64024},{\"attributes\":{\"doi\":\"10.1109/ICASSP40776.2020.9053149\",\"id\":\"b38\",\"matched_paper_id\":214774997},\"end\":64770,\"start\":64302},{\"attributes\":{\"doi\":\"10.1016/j.neuroimage.2017.04.026\",\"id\":\"b39\",\"matched_paper_id\":5319132},\"end\":65078,\"start\":64772}]", "bib_title": "[{\"end\":51934,\"start\":51871},{\"end\":52189,\"start\":52099},{\"end\":52681,\"start\":52589},{\"end\":53090,\"start\":52980},{\"end\":53617,\"start\":53526},{\"end\":53981,\"start\":53872},{\"end\":54759,\"start\":54664},{\"end\":55148,\"start\":55042},{\"end\":56170,\"start\":56071},{\"end\":56563,\"start\":56470},{\"end\":57066,\"start\":56980},{\"end\":57445,\"start\":57376},{\"end\":57813,\"start\":57679},{\"end\":59286,\"start\":59221},{\"end\":60424,\"start\":60377},{\"end\":61444,\"start\":61365},{\"end\":61752,\"start\":61685},{\"end\":62079,\"start\":62011},{\"end\":62446,\"start\":62350},{\"end\":63164,\"start\":63095},{\"end\":64408,\"start\":64302},{\"end\":64851,\"start\":64772}]", "bib_author": "[{\"end\":51946,\"start\":51936},{\"end\":52212,\"start\":52191},{\"end\":52223,\"start\":52212},{\"end\":52236,\"start\":52223},{\"end\":52247,\"start\":52236},{\"end\":52257,\"start\":52247},{\"end\":52279,\"start\":52257},{\"end\":52289,\"start\":52279},{\"end\":52299,\"start\":52289},{\"end\":52310,\"start\":52299},{\"end\":52692,\"start\":52683},{\"end\":52701,\"start\":52692},{\"end\":52716,\"start\":52701},{\"end\":52727,\"start\":52716},{\"end\":52736,\"start\":52727},{\"end\":52746,\"start\":52736},{\"end\":53102,\"start\":53092},{\"end\":53110,\"start\":53102},{\"end\":53117,\"start\":53110},{\"end\":53128,\"start\":53117},{\"end\":53140,\"start\":53128},{\"end\":53152,\"start\":53140},{\"end\":53634,\"start\":53619},{\"end\":53654,\"start\":53634},{\"end\":53658,\"start\":53654},{\"end\":53994,\"start\":53983},{\"end\":54006,\"start\":53994},{\"end\":54019,\"start\":54006},{\"end\":54033,\"start\":54019},{\"end\":54043,\"start\":54033},{\"end\":54054,\"start\":54043},{\"end\":54444,\"start\":54433},{\"end\":54465,\"start\":54444},{\"end\":54469,\"start\":54465},{\"end\":54770,\"start\":54761},{\"end\":54780,\"start\":54770},{\"end\":54791,\"start\":54780},{\"end\":54800,\"start\":54791},{\"end\":54815,\"start\":54800},{\"end\":55164,\"start\":55150},{\"end\":55183,\"start\":55164},{\"end\":55189,\"start\":55183},{\"end\":55530,\"start\":55516},{\"end\":55539,\"start\":55530},{\"end\":55552,\"start\":55539},{\"end\":55565,\"start\":55552},{\"end\":55573,\"start\":55565},{\"end\":55592,\"start\":55573},{\"end\":55605,\"start\":55592},{\"end\":55617,\"start\":55605},{\"end\":55628,\"start\":55617},{\"end\":55909,\"start\":55897},{\"end\":55923,\"start\":55909},{\"end\":56186,\"start\":56172},{\"end\":56194,\"start\":56186},{\"end\":56206,\"start\":56194},{\"end\":56219,\"start\":56206},{\"end\":56230,\"start\":56219},{\"end\":56578,\"start\":56565},{\"end\":56595,\"start\":56578},{\"end\":56608,\"start\":56595},{\"end\":56624,\"start\":56608},{\"end\":56633,\"start\":56624},{\"end\":56644,\"start\":56633},{\"end\":56662,\"start\":56644},{\"end\":56674,\"start\":56662},{\"end\":56678,\"start\":56674},{\"end\":57081,\"start\":57068},{\"end\":57092,\"start\":57081},{\"end\":57107,\"start\":57092},{\"end\":57119,\"start\":57107},{\"end\":57127,\"start\":57119},{\"end\":57138,\"start\":57127},{\"end\":57460,\"start\":57447},{\"end\":57473,\"start\":57460},{\"end\":57488,\"start\":57473},{\"end\":57824,\"start\":57815},{\"end\":57839,\"start\":57824},{\"end\":58088,\"start\":58076},{\"end\":58271,\"start\":58261},{\"end\":58285,\"start\":58271},{\"end\":58297,\"start\":58285},{\"end\":58491,\"start\":58481},{\"end\":58771,\"start\":58756},{\"end\":58896,\"start\":58890},{\"end\":58905,\"start\":58896},{\"end\":58916,\"start\":58905},{\"end\":58920,\"start\":58916},{\"end\":59302,\"start\":59288},{\"end\":59312,\"start\":59302},{\"end\":59326,\"start\":59312},{\"end\":59339,\"start\":59326},{\"end\":59356,\"start\":59339},{\"end\":59700,\"start\":59687},{\"end\":59715,\"start\":59700},{\"end\":59729,\"start\":59715},{\"end\":60028,\"start\":60019},{\"end\":60039,\"start\":60028},{\"end\":60439,\"start\":60426},{\"end\":60450,\"start\":60439},{\"end\":60459,\"start\":60450},{\"end\":60469,\"start\":60459},{\"end\":60775,\"start\":60764},{\"end\":60787,\"start\":60775},{\"end\":60795,\"start\":60787},{\"end\":60804,\"start\":60795},{\"end\":60815,\"start\":60804},{\"end\":61144,\"start\":61135},{\"end\":61461,\"start\":61446},{\"end\":61473,\"start\":61461},{\"end\":61485,\"start\":61473},{\"end\":61764,\"start\":61754},{\"end\":61775,\"start\":61764},{\"end\":61788,\"start\":61775},{\"end\":61805,\"start\":61788},{\"end\":62091,\"start\":62081},{\"end\":62460,\"start\":62448},{\"end\":62651,\"start\":62640},{\"end\":62663,\"start\":62651},{\"end\":62906,\"start\":62895},{\"end\":62916,\"start\":62906},{\"end\":63175,\"start\":63166},{\"end\":63187,\"start\":63175},{\"end\":63191,\"start\":63187},{\"end\":63557,\"start\":63550},{\"end\":63566,\"start\":63557},{\"end\":63719,\"start\":63708},{\"end\":63729,\"start\":63719},{\"end\":64039,\"start\":64024},{\"end\":64048,\"start\":64039},{\"end\":64057,\"start\":64048},{\"end\":64071,\"start\":64057},{\"end\":64420,\"start\":64410},{\"end\":64434,\"start\":64420},{\"end\":64443,\"start\":64434},{\"end\":64867,\"start\":64853},{\"end\":64874,\"start\":64867},{\"end\":64888,\"start\":64874}]", "bib_venue": "[{\"end\":51987,\"start\":51972},{\"end\":52344,\"start\":52331},{\"end\":52785,\"start\":52770},{\"end\":53233,\"start\":53179},{\"end\":53700,\"start\":53687},{\"end\":54091,\"start\":54078},{\"end\":54431,\"start\":54314},{\"end\":54857,\"start\":54847},{\"end\":55222,\"start\":55206},{\"end\":55514,\"start\":55408},{\"end\":55963,\"start\":55923},{\"end\":56271,\"start\":56250},{\"end\":56726,\"start\":56702},{\"end\":57175,\"start\":57162},{\"end\":57525,\"start\":57508},{\"end\":57880,\"start\":57869},{\"end\":58169,\"start\":58112},{\"end\":58333,\"start\":58297},{\"end\":58567,\"start\":58491},{\"end\":58754,\"start\":58697},{\"end\":59056,\"start\":58940},{\"end\":59375,\"start\":59356},{\"end\":59685,\"start\":59616},{\"end\":59899,\"start\":59866},{\"end\":60183,\"start\":60039},{\"end\":60534,\"start\":60469},{\"end\":60939,\"start\":60815},{\"end\":61246,\"start\":61144},{\"end\":61527,\"start\":61514},{\"end\":61846,\"start\":61822},{\"end\":62140,\"start\":62091},{\"end\":62497,\"start\":62485},{\"end\":62757,\"start\":62663},{\"end\":62988,\"start\":62932},{\"end\":63246,\"start\":63191},{\"end\":63548,\"start\":63466},{\"end\":63850,\"start\":63729},{\"end\":64156,\"start\":64087},{\"end\":64533,\"start\":64475},{\"end\":64930,\"start\":64920},{\"end\":53283,\"start\":53235},{\"end\":55973,\"start\":55965},{\"end\":58341,\"start\":58335},{\"end\":58575,\"start\":58569},{\"end\":59927,\"start\":59901},{\"end\":60591,\"start\":60536},{\"end\":63297,\"start\":63248}]"}}}, "year": 2023, "month": 12, "day": 17}