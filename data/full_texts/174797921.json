{"id": 174797921, "updated": "2023-09-28 03:10:27.786", "metadata": {"title": "In-memory hyperdimensional computing", "authors": "[{\"first\":\"Geethan\",\"last\":\"Karunaratne\",\"middle\":[]},{\"first\":\"Manuel\",\"last\":\"Gallo\",\"middle\":[\"Le\"]},{\"first\":\"Giovanni\",\"last\":\"Cherubini\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Benini\",\"middle\":[]},{\"first\":\"Abbas\",\"last\":\"Rahimi\",\"middle\":[]},{\"first\":\"Abu\",\"last\":\"Sebastian\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Nature Electronics", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "in which all the operations of HDC are implemented on two planar memristive crossbar engines together with peripheral digital CMOS circuits. We devise a way of performing hypervector binding entirely within a first memristive crossbar using an in-memory read logic operation and hypervector bundling near the crossbar with CMOS logic. These key operations of HDC cooperatively encode hypervectors with high precision, while eliminating the need to", "fields_of_study": "[\"Computer Science\",\"Physics\"]", "external_ids": {"arxiv": "1906.01548", "mag": "3032819016", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1906-01548", "doi": "10.1038/s41928-020-0410-3"}}, "content": {"source": {"pdf_hash": "511b485177cada54b850da43dd301f7bc4820a6a", "pdf_src": "ScienceParsePlus", "pdf_uri": "[\"https://arxiv.org/pdf/1906.01548v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1906.01548", "status": "GREEN"}}, "grobid": {"id": "929bc46c2582c8eef653bf6f93f86c545f92af06", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/511b485177cada54b850da43dd301f7bc4820a6a.txt", "contents": "\nIn-memory hyperdimensional computing\nPublished online: 1 June 2020\n\nGeethan Karunaratne \nIBM Research -Zurich\nR\u00fcschlikonSwitzerland\n\nDepartment of Information Technology and Electrical Engineering\nETH Z\u00fcrich\nZ\u00fcrichSwitzerland\n\nManuel Le Gallo \nIBM Research -Zurich\nR\u00fcschlikonSwitzerland\n\nGiovanni Cherubini \nIBM Research -Zurich\nR\u00fcschlikonSwitzerland\n\nLuca Benini \nDepartment of Information Technology and Electrical Engineering\nETH Z\u00fcrich\nZ\u00fcrichSwitzerland\n\nAbbas Rahimi \nDepartment of Information Technology and Electrical Engineering\nETH Z\u00fcrich\nZ\u00fcrichSwitzerland\n\n\u2709 \nAbu Sebastian \nIBM Research -Zurich\nR\u00fcschlikonSwitzerland\n\nIn-memory hyperdimensional computing\nPublished online: 1 June 202010.1038/s41928-020-0410-3Received: 13 November 2019; Accepted: 7 April 2020;Articles\nHyperdimensional computing is an emerging computational framework that takes inspiration from attributes of neuronal circuits including hyperdimensionality, fully distributed holographic representation and (pseudo)randomness. When employed for machine learning tasks, such as learning and classification, the framework involves manipulation and comparison of large patterns within memory. A key attribute of hyperdimensional computing is its robustness to the imperfections associated with the computational substrates on which it is implemented. It is therefore particularly amenable to emerging non-von Neumann approaches such as in-memory computing, where the physical attributes of nanoscale memristive devices are exploited to perform computation. Here, we report a complete in-memory hyperdimensional computing system in which all operations are implemented on two memristive crossbar engines together with peripheral digital complementary metal-oxide-semiconductor (CMOS) circuits. Our approach can achieve a near-optimum trade-off between design complexity and classification accuracy based on three prototypical hyperdimensional computing-related learning tasks: language classification, news classification and hand gesture recognition from electromyography signals. Experiments using 760,000 phase-change memory devices performing analog in-memory computing achieve comparable accuracies to software implementations.\n\nB iological computing systems trade accuracy for efficiency. Thus, one solution to reduce energy consumption in artificial systems is to adopt computational approaches that are inherently robust to uncertainty. Hyperdimensional computing (HDC) is one such framework and is based on the observation that key aspects of human memory, perception and cognition can be explained by the mathematical properties of hyperdimensional spaces comprising high-dimensional binary vectors known as hypervectors. Hypervectors are defined as d-dimensional (where d \u2265 1,000) (pseudo)random vectors with independent and identically distributed (i.i.d.) components 1 . When the dimensionality is in the thousands, a large number of quasi-orthogonal hypervectors exist. This allows HDC to combine such hypervectors into new hypervectors using well-defined vector space operations, defined such that the resulting hypervector is unique, and with the same dimension. A powerful system of computing can be built on the rich algebra of hypervectors 2 . Groups, rings and fields over hypervectors become the underlying computing structures with permutations, mappings and inverses as primitive computing operations.\n\nIn recent years, HDC has been employed in a range of applications, including machine learning, cognitive computing, robotics and traditional computing. It has shown significant promise in machine learning applications that involve temporal patterns, such as text classification 3 , biomedical signal processing 4,5 , multimodal sensor fusion 6 and distributed sensors 7,8 . A key advantage is that the training algorithm in HDC works in one or few shots: that is, object categories are learned from one or few examples, and in a single pass over the training data as opposed to many iterations. In the highlighted machine learning applications, HDC has achieved similar or higher accuracy with fewer training examples compared to support vector machines (SVMs) 4 , extreme gradient boosting 9 and convolutional neural networks (CNNs) 10 , and lower execution energy on embedded CPU/GPUs compared to SVMs 11 , CNNs and long short-term memory 5 . Applications of HDC in cognitive computing include solving Raven's progressive matrices 12 , functional imitation of concept learning in honey bees 13 and analogical reasoning 14 . In the field of robotics, HDC has been employed for learning sensorimotor control for active perception in robots 10 . In traditional forms of computing, HDC has been proposed for efficient representation of structured information 15 as well as the synthesis and execution of finite state automata 16 and variants of recurrent neural networks 17 .\n\nHDC begins by representing symbols with i.i.d. hypervectors that are combined by nearly i.i.d.-preserving operations, namely binding, bundling and permutation, and then stored in associative memories (AMs) to be recalled, matched, decomposed or reasoned about. This chain implies that failure in a component of a hypervector is not contagious and forms a computational framework that is intrinsically robust to defects, variations and noise 18 . The manipulation of large patterns stored in memory and its inherent robustness make HDC particularly well suited to emerging computing paradigms such as in-memory computing or computational memory based on emerging nanoscale resistive memory or memristive devices [19][20][21][22][23] . In one such work, a 3D vertical resistive random access memory (ReRAM) device was used to perform individual operations for HDC 24,25 . In another work, a carbon-nanotube field-effect transistor-based logic layer was integrated with ReRAMs, improving efficiency further 26 . However, these architectures offered only limited applications such as a single language recognition task 24,26 or a restricted binary classification version of the same task 26 , and their evaluation is based on simulations and compact models derived from small prototypes with only 256 ReRAM cells 24 or a small 32 bit datapath for hypervector manipulations that results in three orders of magnitude higher latency overhead 26 .\n\nIn this Article, we report a complete integrated in-memory HDC system in which all the operations of HDC are implemented on two planar memristive crossbar engines together with peripheral digital CMOS circuits. We devise a way of performing hypervector binding entirely within a first memristive crossbar using an in-memory read logic operation and hypervector bundling near the crossbar with CMOS logic. These key operations of HDC cooperatively encode hypervectors with high precision, while eliminating the need to repeatedly program (write) the memristive devices. In contrast, previous work on HDC using memristive devices did not employ in-memory logic operations for binding; instead, a ReRAM-based XOR lookup table 24 or digital logic 26 was used. Moreover, the previous in-memory compute primitives for permutation 24 and bundling 26 resulted in repeated programming of the memristive devices, which is prohibitive given the limited cycling endurance.\n\nIn our architecture, an AM search is performed using a second memristive crossbar for in-memory dot-product operations on the encoded output hypervectors from the first crossbar, realizing the full HDC system functionality. Our combination of analog in-memory computing with CMOS logic allows continual functioning of the memristive crossbars with desired accuracy for a wide range of multiclass classification tasks. We verify the integrated inference functionality of the system through large-scale mixed hardware/ software experiments, in which up to 49 d = 10,000-dimensional hypervectors are encoded in 760,000 hardware phase-change memory (PCM) devices performing analog in-memory computing. Our experiments achieve comparable accuracies to the software baselines and surpass those reported in previous work on an emulated small ReRAM crossbar 24 . Furthermore, a complete system-level design of the in-memory HDC architecture synthesized using 65 nm CMOS technology demonstrates >6\u00d7 end-to-end reductions in energy compared with a dedicated digital CMOS implementation. With our approach, we map all operations of HDC either in-memory or near-memory and demonstrate their integrated functionality for three specific machine learning related tasks.\n\n\nthe concept of in-memory HDC\n\nWhen HDC is used for learning and classification, a set of i.i.d., hence quasi-orthogonal hypervectors, referred to as basis hypervectors, are first selected to represent each symbol associated with a dataset. For example, if the task is to classify an unknown text into the corresponding language, the symbols could be the letters of the alphabet. The basis hypervectors stay fixed throughout the computation. Assuming that there are h symbols, fs i g h 1 I , the set of the h, d-dimensional basis hypervectors fB i g h 1 I is referred to as the item memory (IM) (Fig. 1). Basis hypervectors serve as the basis from which further representations are made by applying a well-defined set of component-wise operations: addition of binary hypervectors [+] is defined as the component-wise majority, multiplication (\u2295) is defined as the component-wise exclusive-OR (XOR) (or equivalently as the component-wise exclusive-NOR (XNOR)) and permutation (\u03c1) is defined as a pseudo-random shuffling of the coordinates. Applied on dense binary hypervectors where each component has equal probability of being zero or one 27 , all these operations produce a d-bit hypervector resulting in a closed system. Subsequently, during the learning phase, the basis hypervectors in the IM are combined with the component-wise operations inside an encoder to compute, for example, a quasi-orthogonal n-gram hypervector representing an object of interest 28 , and to add n-gram hypervectors from the same category of objects to produce a prototype hypervector representing the entire class of category. In the language example, the encoder would receive input text associated with a known language and would generate a prototype hypervector corresponding to that language. In this case, n determines the smallest number of symbols (letters in the example) that are combined while performing an n-gram encoding operation. When the encoder receives n consecutive symbols, {s [1], s [2], \u2026, s[n]}, it produces an n-gram hypervector through a binding operation given by\nG\u00f0s\u00bd1\ue08a; s\u00bd2\ue08a; \ue001 \ue001 \ue001 ; s\u00bdn\ue08a\u00de \u00bc B\u00bd1\ue08a\ue008\u03c1\u00f0B\u00bd2\ue08a\u00de\ue008 \ue001 \ue001 \ue001 \ue008\u03c1 n\ufffd1 \u00f0B\u00bdn\ue08a\u00de\u00f01\u00de\nwhere B[k] corresponds to the associated basis hypervector for symbol s [k]. The operator \u00c8 I denotes the XNOR and \u03c1 denotes a pseudo-random permutation operation, for example, a circular shift by 1 bit. The encoder then bundles several such n-gram hypervectors from the training data using component-wise addition followed by a binarization (majority function) to produce a prototype hypervector for the given class. The overall encoding operation results in c, d-dimensional prototype hypervectors (referred to as associative memory (AM)) assuming there are c classes.\n\nWhen inference or classification is performed, a query hypervector (for example, from a text of unknown language) is generated identically to the way the prototype hypervectors are generated. Subsequently, the query hypervector is compared with the prototype hypervectors inside the AM to make the appropriate classification. Equation (2) defines how a query hypervector Q is compared against each of the prototype hypervector P i out of c classes to find the predicted class with maximum similarity. This AM search operation can, for example, be performed by calculating the inverse Hamming distance:\nClass Pred \u00bc arg max i2f1;:::;cg X d j\u00bc1 Q\u00f0j\u00de\ue008P i \u00f0j\u00de\u00f02\u00de\nOne key observation is that the two main operations presented above, namely the encoding and AM search, are about manipulating and comparing large patterns within the memory. Both IM and AM (after learning) represent permanent hypervectors stored in the memory. As a lookup operation, different input symbols activate the corresponding stored patterns in the IM that are then combined inside or around memory with simple local operations to produce another pattern for comparison in AM. These component-wise arithmetic operations on patterns allow a high degree of parallelism as each hypervector component needs to communicate with only a local component or its immediate neighbours. This highly memory-centric aspect of HDC is the key motivation for the in-memory computing implementation proposed in this work.\n\nThe essential idea of in-memory HDC is to store the components of both the IM and the AM as the conductance values of nanoscale memristive devices 29,30 organized in crossbar arrays and enable HDC operations in or near to those devices (Fig. 1). The IM of h rows and d columns is stored in the first crossbar, where each basis hypervector is stored on a single row. To perform \u00c8 I operations between the basis hypervectors for the n-gram encoding, an in-memory read logic primitive is employed. Unlike the majority of reported in-memory logic operations [31][32][33] , the proposed in-memory read logic is non-stateful and this obviates the need for high write endurance of the memristive devices. Additional peripheral circuitry is used to implement the remaining permutations and component-wise additions needed in the encoder. The AM of c rows and d columns is implemented in the second crossbar, where each prototype hypervector is stored on a single row. During supervised learning, each prototype hypervector output from the first crossbar is programmed into a certain row of the AM based on the provided label. During inference, the query hypervector output from the first crossbar is input as voltages on the wordline driver, to perform the AM search using an in-memory dot product primitive. Because every memristive device in the AM and IM is reprogrammable, the representation of hypervectors is not hardcoded, unlike refs. [24][25][26] , which used device variability for projection.\n\nThis design ideally fits the memory-centric architecture of HDC, because it allows us to perform the main computations on the IM and AM within the memory units with a high degree of parallelism. Furthermore, the IM and AM are only programmed once while training on a specific dataset, and the two types of in-memory computation that are employed involve just read operations. Therefore, non-volatile memristive devices are very well suited for implementing the IM and AM, and only binary conductance states are required.\n\nIn this work, we have used PCM technology 34,35 , which operates by switching a phase-change material between amorphous (high resistivity) and crystalline (low resistivity) phases to implement binary data storage (see Methods). PCM has also been successfully employed in novel computing paradigms such as neuromorphic computing [36][37][38][39][40] and computational memory 20,22,41,42 , which makes it a good candidate for realizing the in-memory HDC system.\n\nIn the remaining part of this Article, we will elaborate the detailed designs of the AM, the encoder and finally propose a complete in-memory HDC system that achieves a near-optimum trade-off between design complexity and output accuracy. The functionality of the in-memory HDC system will be validated through experiments using a prototype PCM chip fabricated in 90 nm CMOS technology (see Methods), and a complete system-level design implemented using 65 nm CMOS technology will be presented.\n\n\nthe AM search module\n\nClassification involves an AM search between the prototype hypervectors and the query hypervector using a suitable similarity metric, such as the inverse Hamming distance (invHamm) computed from equation (2). Using associativity of addition operations, the expression in equation (2) can be decomposed into the addition of two dot-product terms as shown in equation (3): \nClass Pred \u00bc arg maxQ \ue001 P i\u00f03\u00de\nwhere Q I denotes the logical complement of Q. Because the operations associated with HDC ensure that both the query and prototype hypervectors have an almost equal number of zeros and ones,  \ns 1 \u2192 B 1 s h \u2192 B h Class 1 \u2192 P 1 Class c \u2192 P c s[k] s[k-1] s[k+1] s[k+2] P 1 P 2 P c B 1 B 2 B h s[k] k k s[k+1] k k s[k+2] k k (1) Row decoder (2) (d ) (1)(2)\n\nFig. 1 | the concept of in-memory HDC.\n\nA schematic of the concept of in-memory HDC showing the essential steps associated with HDC (left) and how they are realized using in-memory computing (right). An item memory (IM) stores h, d-dimensional basis hypervectors that correspond to the symbols associated with a classification problem. During learning, based on a labelled training dataset, an encoder performs dimensionality, preserving mathematical manipulations on the basis hypervectors to produce c, d-dimensional prototype hypervectors that are stored in an AM. During classification, the same encoder generates a query hypervector based on a test example. Subsequently, an AM search is performed between the query hypervector and the hypervectors stored in the AM to determine the class to which the test example belongs. In in-memory HDC, both the IM and AM are mapped onto crossbar arrays of memristive devices. The mathematical operations associated with encoding and AM search are performed in place by exploiting in-memory read logic and dot-product operations, respectively. A dimensionality of d = 10,000 is used. SA, sense amplifier; AD converters, analog-to-digital converters.\n\nthe dot product (dotp) arg max i2f1;:::;cg Q \ue001 P i I can also serve as a viable similarity metric.\n\nTo compute the invHamm similarity metric, two memristive crossbar arrays of c rows and d columns are required, as shown in Fig. 2a. The prototype hypervectors, P i , are programmed into one of the crossbar arrays as conductance states. Binary '1' components are programmed as crystalline states and binary '0' components are programmed as amorphous states. The complementary hypervectors P i I are programmed in a similar manner into the second crossbar array. The query hypervector Q and its complement Q I are applied as voltage values along the wordlines of the respective crossbars. In accordance with Kirchoff 's current law, the total current on the ith bitline will be equal to the dot product between the query hypervector and the ith prototype hypervector. The results of these in-memory dot-product operations from the two arrays are added in a pairwise manner using a digital adder circuitry in the periphery and are subsequently input to a winner-take-all (WTA) circuit that outputs a '1' only on the bitline corresponding to the class of maximum similarity value. When the dotp similarity metric is considered, only the crossbar encoding P i is used and the array of adders in the periphery is eliminated, resulting in reduced hardware complexity.\n\nExperiments were performed using a prototype PCM chip to evaluate the effectiveness of the proposed implementation on three common HDC benchmarks: language classification, news classification and hand gesture recognition from electromyography (EMG) signals (see Methods). These tasks demand a generic programmable architecture to support different numbers of inputs, classes and data types (see Methods). In the experiments, the prototype hypervectors (and their complements) are learned beforehand in software and are then programmed into the PCM devices on the chip. Inference is then performed with a software encoder and using equation (3) for the AM search, in which all multiplication operations are performed in the analog domain (by exploiting Ohm's law) on chip and the remaining operations are implemented in software (see Methods and Supplementary Note 1). The software encoder was employed to precisely assess the performance and accuracy of the AM search alone when implemented in hardware. The in-memory encoding scheme and its experimental validation are presented in sections 'The n-gram encoding module' and 'The complete in-memory HDC system' .\n\nAlthough HDC is remarkably robust to random variability and device failures, deterministic spatial variations in the conductance values could pose a challenge. Unfortunately, in our prototype PCM chip, the conductance values associated with the crystalline state do exhibit a deterministic spatial variation (Supplementary Note 2). However, given the holographic nature of the hypervectors, this can be easily addressed by a random partitioning approach. We employed a coarse-grained randomization strategy, where the   idea is to segment the prototype hypervector and to place the resulting segments spatially distributed across the crossbar array (Fig.  2b). This helps all the components of prototype hypervectors to uniformly mitigate long-range variations. The proposed strategy involves dividing the crossbar array into f equal sized partitions (r 1 , r 2 , \u2026 r f ) and storing a 1/f segment of each of the prototype hypervectors (P 1 , P 2 , ..., P c ) per partition. Here, f is called the 'partition factor' and it controls the granularity associated with the randomization. To match the segments of prototype hypervectors, the query vector is also split into equally sized subvectors Q 1 , Q 2 ,..., Q f , which are input sequentially to the wordline drivers of the crossbar. A statistical model that captures the spatiotemporal conductivity variations was used to evaluate the effectiveness of the coarse-grained randomized partitioning method (Supplementary Note 2). Simulations were carried out for different partition factors 1, 2 and 10 for the two similarity metrics dotp and invHamm, as shown in Fig. 2c. These results indicate that the classification accuracy increases with the number of partitions. For example, for language classification, the accuracy improves from 82.5% to 96% with dotp by randomizing with a partition factor of 10 instead of 1. The experimental on-chip accuracy (performed with a partition factor of 10) is close to the 10-partition simulation result and the software baseline for both similarity metrics on all three datasets. When the two similarity metrics are compared, invHamm provides slightly better accuracy for the same partition size, at the expense of almost doubled area and energy consumption. Therefore, for low-power applications, a good trade-off is the use of the dotp similarity metric with a partition factor of 10.\nr 1 r r r 3 r r P 2 1 P 1 1 P 3 1 P 2 2 P 1 2 P 3 2 P 2 3 P 1 3 P 3 3 Q Q 2 Q 1 Q 3 + b c Gate enable Gate enable dotp only paths p P 2 P 1 P c P 2 P 1 P c (1) (2) (d) (1) (2) (d ) Q (1) (2) (c) (1)(2)\n\nthe n-gram encoding module\n\nIn this section, we will focus on the design of the n-gram encoding module. As described in the section 'The concept of in-memory HDC' , one of the key operations associated with the encoder is calculation of the n-gram hypervector G given by equation (1). To find in-memory hardware-friendly operations, equation (1) is rewritten as the component-wise summation of 2 n \u2212 1 minterms given by equation (4):\nG \u00bc 2 n\ufffd1 \ufffd 1 _ j \u00bc 0 L 1;j \u00f0B\u00bd1\ue08a\u00de^\u03c1\u00f0L 2;j \u00f0B\u00bd2\ue08a\u00de\u00de^\u00bc^\u03c1 n\ufffd1 \u00f0L n;j \u00f0B\u00bdn\ue08a\u00de\u00de\u00f04\u00de\nThe operator L k,j is given by\nL k;j \u00f0B\u00bdk\ue08a\u00de \u00bc B\u00bdk\ue08a if \u00f0\ufffd1\u00de Z\u00f0k;j\u00de \u00bc 1 \u00bc B\u00bdk\ue08a otherwise where Z\u00f0k; j\u00de \u00bc 1 2 k \u00f02j \u00fe 2 k\ufffd1 \u00de \u00c4 \u00c5 I , k \u2208 {1, 2, \u2026,\nn} is the item hypervector index within an n-gram and j \u2208 {0, 1, \u2026, 2 n \u2212 1 \u2212 1} is used to index minterms. The representation given by equation (4) can be mapped into memristive crossbar arrays where the bitwise AND (\u2227) function can be realized using an in-memory read logic operation. However, the number of minterms (2 n \u2212 1 \u2212 1) rises exponentially with the size n of the n-gram, making the hardware computations costly. It is thus desirable to reduce the number of minterms and to use a fixed number of minterms independent of n.\n\nBased on equation (4), we empirically obtained a 2-minterm encoding function for calculating the n-gram hypervector given b\u0177\nG \u00bc \u00f0B\u00bd1\ue08a^\u03c1\u00f0B\u00bd2\ue08a\u00de^\u00bc \u03c1 n\ufffd1 \u00f0B\u00bdn\ue08a\u00de\u00de _\u00f0B\u00bd1\ue08a^\u03c1\u00f0B\u00bd2\ue08a\u00de^\u00bc \u03c1 n\ufffd1 \u00f0B\u00bdn\ue08a\u00de\u00de\u00f05\u00de\nEncoding based on \u011c I shows mostly functional equivalence with the ideal XNOR-based encoding scheme in certain key attributes such as similarity between the basis and prototype hypervectors (Supplementary Note 3). A schematic illustration of the corresponding n-gram encoding system is presented in Fig. 3a. The basis hypervectors are programmed on one of the crossbars and their complement vectors are programmed on the second. The component-wise logical AND operation between two hypervectors in equation (5) is realized in-memory by applying one of the hypervectors as the gate control lines of the crossbar, while selecting the wordline of the second hypervector. The result of the AND function from the crossbar is passed through an array of sense amplifiers to convert the analog values to binary values. The binary result is then stored in the minterm buffer, whose output is fed back as the gate controls by a single component shift to the right (left in the complementary crossbar). This operation approximates the permutation operation in equation (5) as a 1 bit right shift instead of a circular 1 bit shift. By performing these operations n times, it is possible to generate the n-gram. After n-gram encoding, the generated n-grams are accumulated and binarized with a threshold that depends on n (for details see Methods).\n\nTo test the effectiveness of the encoding scheme with in-memory computing, simulations were carried out using the PCM statistical model. The training was performed in software with the same encoding technique used thereafter for inference, and both the encoder and AM were implemented with modelled PCM crossbars for inference. The simulations were performed only on the language and news classification datasets, because for the EMG dataset the hypervectors used for the n-gram encoding are generated by a spatial encoding process and cannot be mapped entirely into a fixed IM of reasonable size. From the results presented in Fig.  3b, it is clear that the all-minterm approach to encoding provides the best classification accuracy in most configurations of AM, as expected. However, the 2-minterm-based encoding method yields a stable and, in some cases, particularly in the language dataset, a similar accuracy level to that of the all-minterm approach, while significantly reducing the hardware complexity. One of the perceived drawbacks of the 2-minterm approach is the increasing sparsity of the n-gram hypervectors with n. However, it can be shown that the dot-product similarity between the prototype hypervectors and hence the classification accuracy remain relatively unchanged due to the thresholding operation that depends on n (Supplementary Note 4).\n\n\nthe complete in-memory HDC system\n\nIn this section, the complete HDC system and the associated experimental results are presented. The proposed architecture comprises the 2-minterm encoder and dotp similarity metric with a partition factor of 10, as this provides the best trade-off between classification accuracy and hardware complexity (Supplementary Note 3). As shown in Fig. 4a, the proposed architecture has three PCM crossbar arrays-two with h rows and d columns and one with c \u00d7 f rows and d/f columns, with f = 10.\n\nThe system includes several peripheral circuits-an index buffer, a minterm buffer and a bundler that reside inside the encoderwhile the AM search module contains a sum buffer and a comparator circuit. The index buffer is located at the input of the IM to keep the indices of the symbols in the sequence and to feed them into the crossbar rows. The bundler accumulates the n-gram hypervectors to produce a sum hypervector. Once the threshold is applied on the sum hypervector, the result is a prototype hypervector during training or a query hypervector during inference. The controller inside the encoder module generates control signals according to the n-gram size and the length of the query sequence to allow different configurations of the encoder. During inference, one segment of the query hypervector at the output buffer of the encoder is fed at a time to the AM through an array of multiplexers so that only the corresponding partition is activated in the AM. Depending on the partition that is selected, the relevant gates are activated through a controller sitting inside the AM search module. Finally, the results in the sum buffer are sent through WTA circuitry to find the maximum index that provides the prediction.\n\nTo experimentally validate the functionality of the complete in-memory HDC architecture, we chose to implement the inference operation, which comprises both encoding (to generate the query hypervectors) and AM search (Supplementary Video 1). For faster experiments, we trained our HDC model in software using the 2-minterm approximate encoding method described in the section 'The n-gram encoding module' , which could be performed as well with our proposed in-memory HDC architecture. This software generates the hypervectors for AM from a given dataset. Subsequently, the components of all hypervectors of both IM and AM were programmed on individual hardware PCM devices, and the inference operation was implemented leveraging the two in-memory computing primitives (for both 2-minterm encoding and the AM search) using the prototype PCM chip (see Methods and Supplementary Note 1). Figure 4b summarizes the accuracy results with software, the PCM statistical model and the on-chip experiment for the language and news classification benchmarks. Compared with the previous experiment, where only AM was contained on-chip, the full chip experiment results show a similar accuracy level, indicating the minimal effect on accuracy when porting the IM into PCM devices with in-memory n-gram encoding. Furthermore, the accuracy level reported in this experiment is close to the accuracy reported with the software for the same parametric configuration of the HD inference model. Finally, to benchmark the performance of the system in terms of energy consumption, the digital submodules in the system-level architecture (marked with dotted boundaries in Fig. 4a) that fall outside the PCM crossbars arrays were synthesized using 65 nm CMOS technology. The synthesis results for these modules were combined with the performance characteristics of PCM crossbar arrays to evaluate the energy, area and throughput of the full system (see Methods). Furthermore, PCM crossbar sections were implemented in CMOS distributed standard cell registers with associated multiplier-adder tree logic and binding logic for AM and IM, respectively, to construct a complete CMOS HD processor to compare with the proposed PCM crossbar-based architecture.\n\nA comparison of the performance between the all-CMOS approach and the PCM crossbar-based approach is presented in Table 1. A 6.01\u00d7 improvement in total energy efficiency and 3.74\u00d7 reduction in area is obtained with the introduction of the PCM crossbar modules. The encoder's energy expense for processing a query reduces by a factor of 3.50 with the PCM crossbar implementation, whereas that of the AM search module reduces by a factor of 117.5. However, these efficiency factors are partially masked by the CMOS peripheral circuitry that is common to both implementations, specifically that in the encoder module, which accounts for the majority of its energy consumption. When peripheral circuits are ignored and only the parts of the design that are exclusive to each approach are directly compared to each other, 14 (1) Sim., f = 10 Software Through a sequence of in-memory logical operations, the approximated n-gram G, as in equation (5), is generated. b, Classification results on the language (using n = 4) and news (using n = 5) datasets, showing the performance of the 2-minterm approximation compared with the all-minterm approach.\n(2) (d ) (2) (1) (d ) (1) (2) ( ) (d) G(1) G(2) G(d)(2)\nare designed more energy efficiently so that the overall system efficiency can be improved further.\n\n\nConclusions\n\nHDC is a brain-inspired computational framework that is particularly well suited for the emerging computational paradigm of in-memory computing. We have reported a complete in-memory HDC system whose two main components are an encoder and an AM search engine. The main computations are performed in-memory with logical and dot-product operations on memristive devices. Due to the inherent robustness of HDC to errors, it was possible to approximate the mathematical operations associated with HDC to make it suitable for hardware implementation, and to use analog in-memory computing without significantly degrading the output accuracy. Our architecture is programmable to support different hypervector representations, dimensionality and number of input symbols and output classes to accommodate a variety of applications. \n(1) (2) (d) (d) (2) (1) (1) (2) (3) (1) (2) (3) (d )\nPartition a t r f r r  Hardware/software experiments using a prototype PCM chip delivered accuracies comparable to software baselines on language and news classification benchmarks with 10,000-dimensional hypervectors. These experiments used hardware PCM devices to implement both in-memory encoding and AM search, thus demonstrating the hardware functionality of all the operations involved in a generic HDC processor for learning and inference. A comparative study performed against a system-level design implemented using 65 nm CMOS technology showed that the in-memory HDC approach could result in more than 6\u00d7 end-to-end savings in energy. By designing more energy-efficient peripheral circuits and with the potential of scaling PCM devices to nanoscale dimensions 43 , these gains could increase several-fold. The in-memory HDC concept is also applicable to other types of memristive device based on ionic drift 44 and magnetoresistance 45 . Future work will focus on taking in-memory HDC beyond learning and classification to perform advanced cognitive tasks alongside data compression and retrieval on dense storage devices, as well as building more power-efficient peripheral hardware to harness the best of in-memory computing.\nPartition r i r 1 r r (1) (2) (c) c (1) (2) (c) c (1) (2) (c) c (1) (2) (3) (d/ d d f) f f\n\nMethods\n\nPCM-based hardware platform. The experimental hardware platform is built around a prototype PCM chip that contains PCM cells based on doped-Ge 2 Sb 2 Te 5 (d-GST) that are integrated into the prototype chip in 90 nm CMOS baseline technology. In addition to the PCM cells, the prototype chip integrates the circuitry for cell addressing, on-chip ADCs for cell readout and voltage-or current-mode cell programming. The experimental platform comprises the following main units:\n\n\u2022 a high-performance analog-front-end (AFE) board that contains digital-to-analog converters (DACs) along with discrete electronics, such as power supplies, voltage and current reference sources \u2022 a field-programmable gate array (FPGA) board that implements the data acquisition and digital logic to interface with the PCM device under test and with all the electronics of the AFE board \u2022 a second FPGA board with an embedded processor and Ethernet connection that implements the overall system control and data management as well as the interface with the host computer\n\nThe prototype chip 46 contains three million PCM cells, as well as the CMOS circuitry to address, program and read out any of these three million cells. In the PCM devices used for experimentation, two 240-nm-wide access transistors were used in parallel per PCM element (cell size of 50 F 2 ). The PCM array is organized as a matrix of 512 wordlines and 2,048 bitlines. The PCM cells were integrated into the chip in 90 nm CMOS technology using the keyhole process 47 . The bottom electrode had a radius of ~20 nm and length of ~65 nm. The phase-change material was ~100 nm thick and extended to the top electrode, whose radius was ~100 nm. The selection of one PCM cell was performed by serially addressing a wordline and a bitline. The addresses were decoded and drove the wordline driver and the bitline multiplexer. The single selected cell could be programmed by forcing a current through the bitline with a voltage-controlled current source. It could also be read by an 8 bit on-chip ADC. To read a PCM cell, the selected bitline was biased to a constant voltage of 300 mV by a voltage regulator via a voltage V read generated via an off-chip DAC. The sensed current, I read , was integrated by a capacitor, and the resulting voltage was then digitized by the on-chip 8 bit cyclic ADC. The total time of one read was 1 \u03bcs. To program a PCM cell, a voltage V prog generated off chip was converted on chip into a programming current, I prog . This current was then mirrored into the selected bitline for the desired duration of the programming pulse. The pulse used to program the PCM to the amorphous state (reset) was a box-type rectangular pulse with duration of 400 ns and amplitude of 450 \u03bcA. The pulse used to program the PCM to the crystalline state (set) was a ramp-down pulse with total duration of ~12 \u03bcs. The access-device gate voltage (wordline voltage) was kept high at 2.75 V during the programming pulses. These programming conditions were optimized to achieve the highest on/off ratio and to minimize device-to-device variability for binary storage.\n\nDatasets to evaluate in-memory HDC. We targeted three highly relevant learning and classification tasks to evaluate the proposed in-memory HDC architecture. These tasks demand a generic programmable architecture to support different numbers of inputs, classes and data types, as shown in Extended Data Table 1. In the following, we describe these tasks that are used to benchmark the performance of in-memory HDC in terms of classification accuracy.\n\n1. Language classification: in this task, HDC is applied to classify raw text composed of Latin characters into their respective language 48 . The training texts are taken from the Wortschatz Corpora 49 , where large numbers of sentences (about a million bytes of text) are available for 22 European languages. Another independent dataset, Europarl Parallel Corpus 50 , with 1,000 sentences per language, is used as the test dataset for the classification. The former database is used for training 22 prototype hypervectors for each of the languages while the latter is used to run inference on the trained HDC model. For subsequent simulations and experiments with the language dataset, we use dimensionality d = 10,000 and n-gram size n = 4. We use an IM of 27 symbols, representing the 26 letters of the Latin alphabet plus a whitespace character. Training is performed using the entire training dataset, containing a labelled text of 120,000-240,000 words per language. For inference, a query is composed of a single sentence of the test dataset, so, in total, 1,000 queries per language are used. 2. News classification: the news dataset comprises a database of Reuters news articles, subjected to a lightweight pre-processing step, covering eight different news genres 51 . The pre-processing step removes frequent 'stop' words and words with fewer than three letters. The training set has 5,400+ documents, while the testing set contains 2,100+ documents. For subsequent simulations and experiments with the news dataset, we use dimensionality d = 10,000 and n-gram size n = 5, as suggested in ref. 18 . Similar to the language task, we use an IM of 27 symbols, representing the 26 letters of the Latin alphabet plus a whitespace character. Training is performed using the entire training dataset, where all labelled documents pertaining to the same class are merged into a single text. This merged text contains 8,000-200,000 words per class. For inference, a query is composed of a single document of the test dataset. 3. Hand gesture recognition from EMG signals: in this task, we focus on use of HDC in a smart prosthetic application, namely hand gesture recognition from a stream of EMG signals. A database 52 that provides EMG samples recorded from four channels covering the forearm muscles is used for this benchmark. Each channel data is quantized into 22 intensity levels of electric potential. The sampling frequency of the EMG signal is 500 Hz. A label is provided for each time sample. The label varies from one to five corresponding to five classes of performed gestures. This dataset is used to train an HDC model to detect hand gestures of a single subject. For training on the EMG dataset, a spatial encoding scheme is first employed to fuse data from the four channels so the IM has four discrete symbols, and it is paired with a continuous IM to jointly map the 22 intensity levels per channel (details about the encoding procedure for the EMG dataset are provided in Supplementary Note 5). The pairing of IM and continuous IM allows a combination of orthogonal mapping with distance proportionality mapping. The spatial encoding creates one hypervector per time sample. A temporal encoding step is then performed, whereby n consecutive spatially encoded hypervectors are combined into an n-gram. For the subsequent simulations and experiments with the EMG dataset we use dimensionality d = 10,000 and n-gram size n = 5. Training and inference are performed using the same EMG channel signals from the same subject, but on non-overlapping sections of recording. The recording used for training contains 1,280 time samples after downsampling by a factor of 175. For inference, 780 queries are generated from the rest of the recording, where each query contains five time samples captured with the same downsampling factor.\n\nFor the different tasks, Extended Data Table 1 provides details on the desired hypervector representations and different hyperparameters including the dimension of hypervectors, the alphabet size, the n-gram size and the number of classes. For the EMG dataset, the hypervectors for the encoding operation are drawn by binding items from a pair of IM and continuous IM (Supplementary Note 5). In hardware implementation of the in-memory HDC, the IM and AM may be distributed into multiple narrower crossbars in case electrical/physical limitations arise.\n\nCoarse-grained randomization. The programming methodology followed to achieve the coarse-grained randomized partitioning in the memristive crossbar for the AM search is explained in the following steps. First, we split all prototype hypervectors (P 1 , P 2 ,..., P c ) into f subvectors of equal length, where f is the partition factor. For example, subvectors from the prototype hypervector of the first class are denoted as (P ) is programmed into the first partition r 1 such that each subvector fits to a column in the crossbar partition. The order of programming of subvectors into the columns in the partition is determined by the previously selected random permutation e. The above steps must be repeated to program all the remaining partitions (r 2 , r 3 , ..., r f ).\n\nThe methodology followed in feeding query vectors during inference is detailed in the following steps. First, we split query hypervector Q into f subvectors (Q 1 , Q 2 ,...,Q f ) of equal length. We then translate Q i component values into voltage levels and apply them onto the wordline drivers in the crossbar array. Bitlines corresponding to the partition r i are enabled. Depending on the belonging class, the partial dot products are then collected onto the respective destination in a sum buffer through AD converters at the end of the r i partition of the array. This procedure is repeated for each partition r i . Classwise partial dot products are accumulated together in each iteration and updated in the sum buffer. After the fth iteration, full dot-product values are ready in the sum buffer. The results are then compared against each other using a WTA circuit to find the maximum value to assign its index as the predicted class.\n\nExperiments on AM search. To obtain the prototype hypervectors used for the AM search, training with HDC is first performed in software on the three datasets described in the section 'Datasets to evaluate in-memory HDC' . For the language and news datasets, XOR-based encoding (see section 'The concept of in-memory HDC') is used with an n-gram size of n = 4 and n = 5, respectively. For the EMG dataset, an initial spatial encoding step creates one hypervector per time sample. A temporal encoding step is then performed, whereby n consecutive spatially encoded hypervectors are combined into an n-gram with XOR-based encoding and n = 5. The detailed encoding procedure for the EMG dataset is explained in Supplementary Note 5.\n\nOnce training is performed, the prototype hypervectors are programmed on the prototype PCM chip. In the experiment conducted with invHamm as the similarity metric, d \u00d7 c \u00d7 2 devices on the PCM prototype chip are allocated. Each device in the first half of the address range (from 1 to d \u00d7 c) is programmed with a component of a prototype hypervector P i , where i = 1, \u2026, c. Devices in the second half of the array are programmed with components of the complementary prototype hypervectors. The exact programming order is determined by the partition factor (f) employed in the coarse-grained randomized partitioning scheme. For f = 10 used in the experiment, devices from the first address up to the 1,000 \u00d7 cth address are programmed with the content of the first partition, that is, the first segment of each prototype hypervector. The second set of 1,000 \u00d7 c addresses is programmed with content of the second partition, and so on. As the hypervector components are binary, devices mapped to the logical 1 components and devices mapped to logical 0 components are programmed to the maximum (~20 \u03bcS) and minimum conductance (~0 \u03bcS) levels, respectively. The devices are programmed in a single shot (no iterative program-and-verify algorithm is used) with a single reset/set pulse for minimum/maximum conductance devices.\n\nOnce the programming phase is completed, the queries from the testing set of a given task are encoded. Only for the experiments in section '3The AM search module' , the query hypervectors are generated using the same software HD encoder used for training. In the experiments of section 'The complete in-memory HDC system' , the query hypervectors are generated with in-memory encoding using the prototype PCM chip as described in the section 'Experiments on the complete in-memory HDC system' .\n\nThe AM search on a given query hypervector is performed using the prototype PCM chip as follows. The components of the query hypervector carrying a value 1 trigger a read (300 mV applied voltage) on the devices storing the corresponding components of prototype hypervectors, thus realizing the analog multiplications through Ohm's law of the in-memory dot-product operation. The same procedure is performed with the complementary query hypervector on the devices storing complementary prototype hypervectors. The resulting current values are digitized via the on-chip ADC, transferred to the host computer and classwise summed up in software according to the predetermined partition order to obtain classwise similarity values (Supplementary Note 1). The class with the highest similarity is assigned as the predicted class for the given query. For experiments with dotp as the similarity metric, the devices attributed to complementary prototype hypervectors are not read when forming the classwise aggregate.\n\nMore details on the 2-minterm encoder. To generate an n-gram hypervector in n cycles, the crossbar is operated using the following procedure. During the first cycle, n-gram encoding is initiated by asserting the 'start' signal while choosing the index of the nth symbol s [n]. This enables all the gate lines in both crossbar arrays and the wordline corresponding to s[n] to be activated. The current released onto the bitlines passed through the sense amplifiers should ideally match the logic levels of B[n] in first array and B\u00bdn\ue08a I in the second array. The two 'minterm buffers' downstream of the sense amplifier arrays register the two hypervectors by the end of the first cycle. During subsequent jth (1 < j \u2264 n) cycles, the gate lines are driven by the right-shifted version of the incumbent values on the minterm buffers-effectively implementing permutation-while row decoders are fed with symbol s[n \u2212 j + 1] (the left shift is used for the second crossbar). This ensures that the output currents on the bitlines correspond to the component-wise logical AND between the permuted minterm buffer values and the next basis hypervector B[n \u2212 j] (complement for the second array). The expression for the value stored on the left-side minterm buffers at the end of jth cycle is given by Q j k\u00bc1 \u03c1 j\ufffdk B\u00bdn \ufffd k \u00fe 1\ue08a I . The product of the complementary hypervectors Q j k\u00bc1 \u03c1 j\ufffdk B\u00bdn \ufffd k \u00fe 1\ue08a I is stored in the right-side minterm buffers. At the end of the nth cycle, the two minterms are available in the minterm buffers. The elements in the minterm buffers are passed onto the OR gate array following the minterm buffers (shown in Fig. 3a), such that inputs to the array have matching indices from the two minterm vectors. At this point, the output of the OR gate array reflects the desired n-gram hypervector from 2-minterm n-gram encoding. After n-gram encoding, the generated n-grams are accumulated and binarized. In the hardware implementation, this step is realized inside the bundler module shown in Fig. 4a. The threshold applied to binarize the sum hypervector components is given by\nl\u00b41 2 n\ufffdlog\u00f0m\u00de\nwhere l is the length of the sequence, n is the n-gram size and m is the number of minterms used for the binding operation in the encoder (for example, m = 2 for 2-minterm encoder).\n\nExperiments on the complete in-memory HDC system. For the experiments concerning the complete in-memory HDC system, training with HDC is first performed in software on the language and news datasets. 2-minterm encoding (equation (5)) is used with n-gram sizes of n = 4 and n = 5, respectively. After training is performed, h \u00d7 d \u00d7 2 devices are allocated on the PCM chip for storing IM and complementary IM in addition to d \u00d7 c devices allocated for AM. The IM and complementary IM hypervectors are programmed on PCM devices in a single shot with reset/set pulses for logical 0/1 components. The prototype hypervectors of the AM are programmed as described in the section 'Experiments on AM search' , with the exception that the complementary prototype hypervectors are not programmed because dotp is used as the similarity metric.\n\nDuring inference, for every query to be encoded, the IM and complementary IM are read from the prototype PCM chip. In-memory read logic (AND) is performed by thresholding the read current values from the on-chip ADC in software to emulate the sense amplifiers of the eventual proposed hardware at each step of the 2-minterm n-gram encoding process (Supplementary Note 1). The other operations involved in the encoder that are not supported by the prototype PCM chip, such as the 1 bit right-shift permutation, storing of the intermediate results in the minterm buffers, ORing the results of the original and complementary minterm buffers, and the bundling of n-gram hypervectors are implemented in software. Once the encoding of the query hypervector is completed, the AM search is carried out on that query hypervector as specified in the section 'Experiments on AM search' with dotp as the similarity metric.\n\n\nPerformance, energy estimation and comparison.\n\nTo evaluate and benchmark the energy efficiency of the proposed architecture, a cycle-accurate register transfer level (RTL) model of a complete CMOS design that has equivalent throughput to that of the proposed in-memory HDC system architecture has been developed (Supplementary Note 6). A testbench infrastructure is then built to verify the correct behaviour of the model. Once the behaviour is verified, the RTL model is synthesized in a UMC 65 nm technology node using a Synopsys Design Compiler. Owing to the limitations in the electronic design automation (EDA) tools used for synthesizing the CMOS-based HDC, dimensionality d had to be limited to 2,000. The post-synthesis netlist is then verified using the same stimulus vectors applied during behavioural simulation. During post-synthesis netlist simulation, the design is clocked at a frequency of 440 MHz to create a switching activity file in value change dump (VCD) format for inference of 100 language classification queries. Then, the energy estimation for the CMOS modules is performed by converting average power values reported by Synopsys Primetime, which takes the netlist and the activity file from the previous steps as the inputs. A typical operating condition with voltage 1.2 V and temperature 25 \u00b0C is set as the corner for energy estimation of the CMOS system. Further energy and area results were obtained for d values of 100, 500, 1,000 in addition to 2,000. The results were then extrapolated to derive the energy and area estimates for dimensionality d = 10,000 to obtain a fair comparison with the in-memory HDC system.\n\nThe energy/area of the proposed in-memory HDC system architecture is obtained by adding the energy/area of the modules that are common with the full CMOS design described above, together with the energy of the PCM crossbars and the analog/digital peripheral circuits exclusive to the in-memory HDC architecture. Parameters based on the prototype PCM chip in the 90 nm technology used in the experiments are taken as the basis for the PCM-exclusive energy/ area estimation. The parameters of the sense amplifiers that are not present in the PCM hardware platform but present in the proposed in-memory HD encoder are taken from the 65 nm current latched sense amplifier presented by Chandoke and others 53 . The area of the current latched sense amplifier was estimated by scaling the area of the six-transistor SRAM cell in IBM 65 nm technology (0.54 \u03bcm 2 ) according to the number of transistors present in the sense amplifier (19). The parameters used for the PCM crossbars energy estimation are shown in Extended Data Table 2.\n\n\nData availability\n\nThe data that support the plots within this paper and other findings of this study are available from the corresponding author upon reasonable request.\n\n\ni2f1;:::;cg Q \ue001 P i \u00fe Q \ue001 P i ' arg max i2f1;:::;cg\n\nFig. 2 |\n2AM search. a, Schematic of the AM search architecture to compute the invHamm similarity metric. Two PCM crossbar arrays of c rows and d columns are employed. b, Schematic of the coarse-grained randomization strategy employed to counter the variations associated with the crystalline PCM state. c, Results of the classification task show that the experimental on-chip accuracy results compare favourably with the 10-partition simulation results and software baseline for both similarity metrics on the three datasets.\n\nFig. 3 |\n3In-memory n-gram encoding based on 2-minterm. a, The basis hypervectors and their complements are mapped onto two crossbar arrays.\n\nFig. 4 |\n4the complete in-memory HDC system. a, Schematic of the architecture, showing the 2-minterm encoder and AM search engine employing the dotp metric. b, The classification accuracy results on the news and language datasets, where both the encoding and AM search are performed in software, simulated using the PCM model and are experimentally realized on the chip.\n\n\nThe crossbar array is then divided into f equally sized partitions (r 1 , r 2 , ..., r f ). Each partition must contain d/f rows and c columns. A random permutation e of numbers 1 to c is then selected. Next, the first subvector from each class (\n\n\n.4\u00d7 and 334\u00d7 energy savings and 24.5\u00d7 and 31.9\u00d7 area savings are obtained for the encoder and AM search module, respectively. It remains part of future work to investigate methods in which peripheral modulesa \n\nb \n\nSA \nSA \nSA \nSA \nSA \nSA \n\nn-gram hypervector \n\nOriginal IM crossbar \n\ns[k] k k \n\nB 1 \n\nB 2 \n\nB h \n\nB 1 \n\nB 2 \n\nB h \n\nComplementary IM crossbar \n\ns[k] \n\n\n\nTable 1 |\n1Performance comparison between a dedicated all-CMoS implementation and in-memory HDC with PCM crossbars \n\nAll-CMoS \nPCM crossbar based \n\nencoder \nAM search \ntotal \nencoder \nAM search \ntotal \n\nenergy \n\nAverage energy per query (nJ) \n1,474 \n1,110 \n2,584 \n420.8 \n9.44 \n430.3 \n\nImprovement \n3.50\u00d7 \n117.5\u00d7 \n6.01\u00d7 \n\nExclusive modules avg. energy per query (nJ) 1,132 \n1,104 \n2,236 \n78.60 \n3.30 \n81.90 \n\nImprovement \n14.40\u00d7 \n334.6\u00d7 \n27.30\u00d7 \n\nArea \n\nTotal area (mm 2 ) \n4.77 \n2.99 \n7.76 \n1.39 \n0.68 \n2.07 \n\nImprovement \n3.43\u00d7 \n4.38\u00d7 \n3.74\u00d7 \n\nExclusive modules area (mm 2 ) \n3.53 \n2.38 \n5.91 \n0.14 \n0.075 \n0.22 \n\nImprovement \n24.57\u00d7 \n31.94\u00d7 \n27.09\u00d7 \n\nNAtuRe eLeCtRoNICS | VOL 3 | JunE 2020 | 327-337 | www.nature.com/natureelectronics\n\u00a9 The Author(s), under exclusive licence to Springer Nature Limited 2020 NAtuRe eLeCtRoNICS | VOL 3 | JunE 2020 | 327-337 | www.nature.com/natureelectronics\nNATurE ElECTrONICSExtended Data Table 1 | Architecture configurations and hyperparameters used for the tree different tasks NAtuRe eLeCtRoNICS | www.nature.com/natureelectronics\nNAtuRe eLeCtRoNICS | www.nature.com/natureelectronics\nAcknowledgementsAuthor contributionsAll authors collectively conceived the idea of in-memory hyperdimensional computing. G.K. performed the experiments and analysed the results under the supervision of M.L.G., A.R. and A.S. G.K., M.L.G., A.R. and A.S. wrote the manuscript with input from all authors.Competing interestsThe authors declare no competing interests.Additional informationExtended data is available for this paper at https://doi.org/10.1038/s41928-020-0410-3.Supplementary information is available for this paper at https://doi.org/10.1038/ s41928-020-0410-3.Correspondence and requests for materials should be addressed to A.R. or A.S.Reprints and permissions information is available at www.nature.com/reprints.Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nP Kanerva, Sparse Distributed Memory. MIT PressKanerva, P. Sparse Distributed Memory (MIT Press, 1988).\n\nHyperdimensional computing: an introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cogn. Comput. 1Kanerva, P. Hyperdimensional computing: an introduction to computing in distributed representation with high-dimensional random vectors. Cogn. Comput. 1, 139-159 (2009).\n\nRandom indexing of text samples for latent semantic analysis. P Kanerva, J Kristoferson, A Holst, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science SocietyCognitive Science Society22Kanerva, P., Kristoferson, J. & Holst, A. Random indexing of text samples for latent semantic analysis. In Proceedings of the Annual Meeting of the Cognitive Science Society Vol. 22 (Cognitive Science Society, 2000).\n\nEfficient biosignal processing using hyperdimensional computing: network templates for combined learning and classification of ExG signals. A Rahimi, P Kanerva, L Benini, J M Rabaey, Proc. IEEE. 107Rahimi, A., Kanerva, P., Benini, L. & Rabaey, J. M. Efficient biosignal processing using hyperdimensional computing: network templates for combined learning and classification of ExG signals. Proc. IEEE 107, 123-143 (2019).\n\nLaelaps: an energy-efficient seizure detection algorithm from long-term human iEEG recordings without false alarms. A Burrello, L Cavigelli, K Schindler, L Benini, A Rahimi, Proceedings of the Design, Automation & Test in Europe Conference & Exhibition (DATE). the Design, Automation & Test in Europe Conference & Exhibition (DATE)IEEEBurrello, A., Cavigelli, L., Schindler, K., Benini, L. & Rahimi, A. Laelaps: an energy-efficient seizure detection algorithm from long-term human iEEG recordings without false alarms. In Proceedings of the Design, Automation & Test in Europe Conference & Exhibition (DATE) 752-757 (IEEE, 2019).\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O J R\u00e4s\u00e4nen, J P Saarinen, IEEE Trans. Neural Netw. Learn. Syst. 27R\u00e4s\u00e4nen, O. J. & Saarinen, J. P. Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. IEEE Trans. Neural Netw. Learn. Syst. 27, 1878-1889 (2015).\n\nBrain-like classifier of temporal patterns. D Kleyko, E Osipov, Proceedings of the International Conference on Computer and Information Sciences (ICCOINS). the International Conference on Computer and Information Sciences (ICCOINS)IEEEKleyko, D. & Osipov, E. Brain-like classifier of temporal patterns. In Proceedings of the International Conference on Computer and Information Sciences (ICCOINS) 1-6 (IEEE, 2014).\n\nHyperdimensional computing in industrial systems: the use-case of distributed fault isolation in a power plant. D Kleyko, E Osipov, N Papakonstantinou, V Vyatkin, IEEE Access. 6Kleyko, D., Osipov, E., Papakonstantinou, N. & Vyatkin, V. Hyperdimensional computing in industrial systems: the use-case of distributed fault isolation in a power plant. IEEE Access 6, 30766-30777 (2018).\n\nHyperdimensional computing-based multimodality emotion recognition with physiological signals. E Chang, A Rahimi, L Benini, A A Wu, Proceedings of the IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS). the IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)IEEEChang, E., Rahimi, A., Benini, L. & Wu, A. A. Hyperdimensional computing-based multimodality emotion recognition with physiological signals. In Proceedings of the IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) 137-141 (IEEE, 2019).\n\nLearning sensorimotor control with neuromorphic sensors: toward hyperdimensional active perception. A Mitrokhin, P Sutor, C Ferm\u00fcller, Y Aloimonos, Sci. Robot. 46736Mitrokhin, A., Sutor, P., Ferm\u00fcller, C. & Aloimonos, Y. Learning sensorimotor control with neuromorphic sensors: toward hyperdimensional active perception. Sci. Robot. 4, eaaw6736 (2019).\n\nPULP-HD: accelerating brain-inspired high-dimensional computing on a parallel ultra-low power platform. F Montagna, A Rahimi, S Benatti, D Rossi, L Benini, Proceedings of the 55th Annual Design Automation Conference DAC 2018. the 55th Annual Design Automation Conference DAC 2018ACM111Montagna, F., Rahimi, A., Benatti, S., Rossi, D. & Benini, L. PULP-HD: accelerating brain-inspired high-dimensional computing on a parallel ultra-low power platform. In Proceedings of the 55th Annual Design Automation Conference DAC 2018, 111:1-111:6 (ACM, 2018).\n\nAnalogical mapping and inference with binary spatter codes and sparse distributed memory. B Emruli, R W Gayler, F Sandin, Proceedings of the International Joint Conference on Neural Networks (IJCNN). the International Joint Conference on Neural Networks (IJCNN)IEEEEmruli, B., Gayler, R. W. & Sandin, F. Analogical mapping and inference with binary spatter codes and sparse distributed memory. In Proceedings of the International Joint Conference on Neural Networks (IJCNN) 1-8 (IEEE, 2013).\n\nImitation of honey bees' concept learning processes using vector symbolic architectures. D Kleyko, E Osipov, R W Gayler, A I Khan, A G Dyer, Biol. Inspired Cogn. Architectures. 14Kleyko, D., Osipov, E., Gayler, R. W., Khan, A. I. & Dyer, A. G. Imitation of honey bees' concept learning processes using vector symbolic architectures. Biol. Inspired Cogn. Architectures 14, 57-72 (2015).\n\nAnalogical mapping using similarity of binary distributed representations. S V Slipchenko, D A Rachkovskij, Inf. Theories Appl. 16Slipchenko, S. V. & Rachkovskij, D. A. Analogical mapping using similarity of binary distributed representations. Inf. Theories Appl. 16, 269-290 (2009).\n\nTrajectory clustering of road traffic in urban environments using incremental machine learning in combination with hyperdimensional computing. T Bandaragoda, Proceedings of the IEEE Intelligent Transportation Systems Conference (ITSC). the IEEE Intelligent Transportation Systems Conference (ITSC)IEEEBandaragoda, T. et al. Trajectory clustering of road traffic in urban environments using incremental machine learning in combination with hyperdimensional computing. In Proceedings of the IEEE Intelligent Transportation Systems Conference (ITSC) 1664-1670 (IEEE, 2019).\n\nAssociative synthesis of finite state automata model of a controlled object with hyperdimensional computing. E Osipov, D Kleyko, A Legalov, Proceedings of the Annual Conference of the IEEE Industrial Electronics Society. the Annual Conference of the IEEE Industrial Electronics SocietyIEEEOsipov, E., Kleyko, D. & Legalov, A. Associative synthesis of finite state automata model of a controlled object with hyperdimensional computing. In Proceedings of the Annual Conference of the IEEE Industrial Electronics Society 3276-3281 (IEEE, 2017).\n\nInteger echo state networks: hyperdimensional reservoir computing. D Kleyko, E P Frady, E Osipov, Preprint atKleyko, D., Frady, E. P. & Osipov, E. Integer echo state networks: hyperdimensional reservoir computing. Preprint at https://arxiv.org/ pdf/1706.00280.pdf (2017).\n\nHigh-dimensional computing as a nanoscalable paradigm. A Rahimi, IEEE Trans. Circuits Syst. I Regular Papers. 64Rahimi, A. et al. High-dimensional computing as a nanoscalable paradigm. IEEE Trans. Circuits Syst. I Regular Papers 64, 2508-2521 (2017).\n\nMemristive devices for computing. J J Yang, D B Strukov, D R Stewart, Nat. Nanotechnol. 8Yang, J. J., Strukov, D. B. & Stewart, D. R. Memristive devices for computing. Nat. Nanotechnol. 8, 13-24 (2013).\n\nTemporal correlation detection using computational phase-change memory. A Sebastian, Nat. Commun. 81115Sebastian, A. et al. Temporal correlation detection using computational phase-change memory. Nat. Commun. 8, 1115 (2017).\n\nThe future of electronics based on memristive systems. M A Zidan, J P Strachan, W D Lu, Nat. Electron. 1Zidan, M. A., Strachan, J. P. & Lu, W. D. The future of electronics based on memristive systems. Nat. Electron. 1, 22-29 (2018).\n\nIn-memory computing with resistive switching devices. D Ielmini, H.-S P Wong, Nat. Electron. 1Ielmini, D. & Wong, H.-S. P. In-memory computing with resistive switching devices. Nat. Electron. 1, 333-343 (2018).\n\nMemory devices and applications for in-memory computing. A Sebastian, M Le Gallo, R Khaddam-Aljameh, E Eleftheriou, 10.1038/s41565-020-0655-zNat. Nanotechnol. Sebastian, A., Le Gallo, M., Khaddam-Aljameh, R. & Eleftheriou, E. Memory devices and applications for in-memory computing. Nat. Nanotechnol. https:// doi.org/10.1038/s41565-020-0655-z (2020).\n\nHyperdimensional computing with 3D VRRAM in-memory kernels: device-architecture co-design for energy-efficient, error-resilient language recognition. H Li, Proceedings of the IEEE International Electron Devices Meeting. the IEEE International Electron Devices MeetingIEDM) 16.1.1-16.1.4 (IEEELi, H. et al. Hyperdimensional computing with 3D VRRAM in-memory kernels: device-architecture co-design for energy-efficient, error-resilient language recognition. In Proceedings of the IEEE International Electron Devices Meeting (IEDM) 16.1.1-16.1.4 (IEEE, 2016).\n\nDevice-architecture co-design for hyperdimensional computing with 3D vertical resistive switching random access memory (3D VRRAM). H Li, T F Wu, S Mitra, H S P Wong, VLSI-TSA) 1-2Proceedings of the International Symposium on VLSI Technology, Systems and Application. the International Symposium on VLSI Technology, Systems and ApplicationIEEELi, H., Wu, T. F., Mitra, S. & Wong, H. S. P. Device-architecture co-design for hyperdimensional computing with 3D vertical resistive switching random access memory (3D VRRAM). In Proceedings of the International Symposium on VLSI Technology, Systems and Application (VLSI-TSA) 1-2 (IEEE, 2017).\n\nBrain-inspired computing exploiting carbon nanotube FETs and resistive RAM: hyperdimensional computing case study. T F Wu, Proceedings of the International Solid State Circuits Conference (ISSCC). the International Solid State Circuits Conference (ISSCC)IEEEWu, T. F. et al. Brain-inspired computing exploiting carbon nanotube FETs and resistive RAM: hyperdimensional computing case study. In Proceedings of the International Solid State Circuits Conference (ISSCC) 492-494 (IEEE, 2018).\n\nBinary spatter-coding of ordered k-tuples. P Kanerva, Proceedings of the International Conference on Artificial Neural Networks (ICANN). the International Conference on Artificial Neural Networks (ICANN)Springer1112Kanerva, P. Binary spatter-coding of ordered k-tuples. In Proceedings of the International Conference on Artificial Neural Networks (ICANN), Vol. 1112, 869-873 (Lecture Notes in Computer Science, Springer, 1996).\n\nLanguage geometry using random indexing. A Joshi, J T Halseth, P Kanerva, Proceedings of the International Symposium on Quantum Interaction. the International Symposium on Quantum InteractionSpringerJoshi, A., Halseth, J. T. & Kanerva, P. Language geometry using random indexing. In Proceedings of the International Symposium on Quantum Interaction 265-274 (Springer, 2016).\n\nResistance switching memories are memristors. L Chua, Appl. Phys. A. 102Chua, L. Resistance switching memories are memristors. Appl. Phys. A 102, 765-783 (2011).\n\nMemory leads the way to better computing. H.-S P Wong, S Salahuddin, Nat. Nanotechnol. 10Wong, H.-S. P. & Salahuddin, S. Memory leads the way to better computing. Nat. Nanotechnol. 10, 191-194 (2015).\n\nMemristive' switches enable 'stateful' logic operations via material implication. J Borghetti, Nature. 464Borghetti, J. et al. 'Memristive' switches enable 'stateful' logic operations via material implication. Nature 464, 873-876 (2010).\n\nMagic-memristor-aided logic. S Kvatinsky, IEEE Trans. Circuits Syst II Express Briefs. 61Kvatinsky, S. et al. Magic-memristor-aided logic. IEEE Trans. Circuits Syst II Express Briefs 61, 895-899 (2014).\n\nStateful logic operations in one-transistor-one-resistor resistive random access memory array. W Shen, Electron Device Lett. 40Shen, W. et al. Stateful logic operations in one-transistor-one-resistor resistive random access memory array. Electron Device Lett. 40, 1538-1541 (2019).\n\nPhase change memory. H.-S P Wong, Proc. IEEE 98. IEEE 98Wong, H.-S. P. et al. Phase change memory. Proc. IEEE 98, 2201-2227 (2010).\n\nRecent progress in phase-change memory technology. G W Burr, IEEE J. Emerging Selected Topics Circuits Syst. 6Burr, G. W. et al. Recent progress in phase-change memory technology. IEEE J. Emerging Selected Topics Circuits Syst. 6, 146-162 (2016).\n\nNanoelectronic programmable synapses based on phase change materials for brain-inspired computing. D Kuzum, R G Jeyasingh, B Lee, H.-S P Wong, Nano Lett. 12Kuzum, D., Jeyasingh, R. G., Lee, B. & Wong, H.-S. P. Nanoelectronic programmable synapses based on phase change materials for brain-inspired computing. Nano Lett. 12, 2179-2186 (2011).\n\nStochastic phase-change neurons. T Tuma, A Pantazi, M Le Gallo, A Sebastian, E Eleftheriou, Nat. Nanotechnol. 11Tuma, T., Pantazi, A., Le Gallo, M., Sebastian, A. & Eleftheriou, E. Stochastic phase-change neurons. Nat. Nanotechnol. 11, 693-699 (2016).\n\nNeuromorphic computing with multi-memristive synapses. I Boybat, Nat. Commun. 92514Boybat, I. et al. Neuromorphic computing with multi-memristive synapses. Nat. Commun. 9, 2514 (2018).\n\nTutorial: brain-inspired computing using phase-change memory devices. A Sebastian, J. Appl. Phys. 124111101Sebastian, A. et al. Tutorial: brain-inspired computing using phase-change memory devices. J. Appl. Phys. 124, 111101 (2018).\n\nAccurate deep neural network inference using computational phase-change memory. V Joshi, 10.1038/s41467-020-16108-9Nat. Commun. Joshi, V. et al. Accurate deep neural network inference using computational phase-change memory. Nat. Commun. https://doi.org/10.1038/s41467-020- 16108-9 (2020).\n\nAccumulation-based computing using phase-change memories with FET access devices. P Hosseini, A Sebastian, N Papandreou, C D Wright, H Bhaskaran, Electron Device Lett. 36Hosseini, P., Sebastian, A., Papandreou, N., Wright, C. D. & Bhaskaran, H. Accumulation-based computing using phase-change memories with FET access devices. Electron Device Lett. 36, 975-977 (2015).\n\nMixed-precision in-memory computing. Le Gallo, M , Nat. Electron. 1Le Gallo, M. et al. Mixed-precision in-memory computing. Nat. Electron. 1, 246-253 (2018).\n\nLow-power switching of phase-change materials with carbon nanotube electrodes. F Xiong, A D Liao, D Estrada, E Pop, Science. 332Xiong, F., Liao, A. D., Estrada, D. & Pop, E. Low-power switching of phase-change materials with carbon nanotube electrodes. Science 332, 568-570 (2011).\n\nNanoscience and Technology: a Collection of Reviews from. R Waser, M Aono, Nature Journals. World ScientificWaser, R. & Aono, M. in Nanoscience and Technology: a Collection of Reviews from Nature Journals 158-165 (World Scientific, 2010).\n\nA new spin on magnetic memories. A D Kent, D C Worledge, Nat. Nanotechnol. 10Kent, A. D. & Worledge, D. C. A new spin on magnetic memories. Nat. Nanotechnol. 10, 187-191 (2015).\n\nDevice, circuit and system-level analysis of noise in multi-bit phase-change memory. G Close, Proceedings of the International Electron Devices Meeting. the International Electron Devices MeetingIEDM) 29.5.1-29.5.4 (IEEEClose, G. et al. Device, circuit and system-level analysis of noise in multi-bit phase-change memory. In Proceedings of the International Electron Devices Meeting (IEDM) 29.5.1-29.5.4 (IEEE, 2010).\n\nNovel lithography-independent pore phase change memory. M Breitwisch, Proceedings of the Symposium on VLSI Technology. the Symposium on VLSI TechnologyIEEEBreitwisch, M. et al. Novel lithography-independent pore phase change memory. In Proceedings of the Symposium on VLSI Technology 100-101 (IEEE, 2007).\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, P Kanerva, J M Rabaey, Proceedings of the 2016 International Symposium on Low Power Electronics and Design ISLPED 2016. the 2016 International Symposium on Low Power Electronics and Design ISLPED 2016ACMRahimi, A., Kanerva, P. & Rabaey, J. M. A robust and energy-efficient classifier using brain-inspired hyperdimensional computing. In Proceedings of the 2016 International Symposium on Low Power Electronics and Design ISLPED 2016, 64-69 (ACM, 2016).\n\nCorpus portal for search in monolingual corpora. U Quasthoff, M Richter, C Biemann, Proceedings of the International Conference on Language Resources and Evaluation (LREC). the International Conference on Language Resources and Evaluation (LREC)ELRAQuasthoff, U., Richter, M. & Biemann, C. Corpus portal for search in monolingual corpora. In Proceedings of the International Conference on Language Resources and Evaluation (LREC) 1799-1802 (ELRA, 2006).\n\nEuroparl: a parallel corpus for statistical machine translation. P Koehn, Proceedings of the MT Summit. the MT SummitAAMT5Koehn, P. Europarl: a parallel corpus for statistical machine translation. In Proceedings of the MT Summit Vol. 5, 79-86 (AAMT, 2005).\n\nSome Text Datasets. D S Mimaroglu, Univ. MassachusettsMimaroglu, D. S. Some Text Datasets (Univ. Massachusetts, accessed 9 March 2018);\n\nHyperdimensional biosignal processing: a case study for EMG-based hand gesture recognition. A Rahimi, S Benatti, P Kanerva, L Benini, J M Rabaey, Proceedings of the 2016 IEEE International Conference on Rebooting Computing (ICRC). the 2016 IEEE International Conference on Rebooting Computing (ICRC)IEEERahimi, A., Benatti, S., Kanerva, P., Benini, L. & Rabaey, J. M. Hyperdimensional biosignal processing: a case study for EMG-based hand gesture recognition. In Proceedings of the 2016 IEEE International Conference on Rebooting Computing (ICRC) 1-8 (IEEE, 2016).\n\nComparative analysis of sense amplifiers for SRAM in 65 nm CMOS technology. N Chandoke, N Chitkara, A Grover, Proceedings of the International Conference on Electrical, Computer and Communication Technologies (ICECCT). the International Conference on Electrical, Computer and Communication Technologies (ICECCT)IEEEChandoke, N., Chitkara, N. & Grover, A. Comparative analysis of sense amplifiers for SRAM in 65 nm CMOS technology. In Proceedings of the International Conference on Electrical, Computer and Communication Technologies (ICECCT), 1-7 (IEEE, 2015).\n", "annotations": {"author": "[{\"end\":227,\"start\":69},{\"end\":288,\"start\":228},{\"end\":352,\"start\":289},{\"end\":459,\"start\":353},{\"end\":567,\"start\":460},{\"end\":570,\"start\":568},{\"end\":629,\"start\":571},{\"end\":227,\"start\":69},{\"end\":288,\"start\":228},{\"end\":352,\"start\":289},{\"end\":459,\"start\":353},{\"end\":567,\"start\":460},{\"end\":570,\"start\":568},{\"end\":629,\"start\":571}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":77},{\"end\":243,\"start\":235},{\"end\":307,\"start\":298},{\"end\":364,\"start\":358},{\"end\":472,\"start\":466},{\"end\":584,\"start\":575},{\"end\":88,\"start\":77},{\"end\":243,\"start\":235},{\"end\":307,\"start\":298},{\"end\":364,\"start\":358},{\"end\":472,\"start\":466},{\"end\":584,\"start\":575}]", "author_first_name": "[{\"end\":76,\"start\":69},{\"end\":234,\"start\":228},{\"end\":297,\"start\":289},{\"end\":357,\"start\":353},{\"end\":465,\"start\":460},{\"end\":569,\"start\":568},{\"end\":574,\"start\":571},{\"end\":76,\"start\":69},{\"end\":234,\"start\":228},{\"end\":297,\"start\":289},{\"end\":357,\"start\":353},{\"end\":465,\"start\":460},{\"end\":569,\"start\":568},{\"end\":574,\"start\":571}]", "author_affiliation": "[{\"end\":132,\"start\":90},{\"end\":226,\"start\":134},{\"end\":287,\"start\":245},{\"end\":351,\"start\":309},{\"end\":458,\"start\":366},{\"end\":566,\"start\":474},{\"end\":628,\"start\":586},{\"end\":132,\"start\":90},{\"end\":226,\"start\":134},{\"end\":287,\"start\":245},{\"end\":351,\"start\":309},{\"end\":458,\"start\":366},{\"end\":566,\"start\":474},{\"end\":628,\"start\":586}]", "title": "[{\"end\":37,\"start\":1},{\"end\":666,\"start\":630},{\"end\":37,\"start\":1},{\"end\":666,\"start\":630}]", "venue": null, "abstract": "[{\"end\":2208,\"start\":781},{\"end\":2208,\"start\":781}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2857,\"start\":2856},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3681,\"start\":3680},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3745,\"start\":3744},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3772,\"start\":3770},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3773,\"start\":3772},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4164,\"start\":4163},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4194,\"start\":4193},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4238,\"start\":4236},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4308,\"start\":4306},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4344,\"start\":4343},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4437,\"start\":4435},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4497,\"start\":4495},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4525,\"start\":4523},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4644,\"start\":4642},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4761,\"start\":4759},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4828,\"start\":4826},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4873,\"start\":4871},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5320,\"start\":5318},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5592,\"start\":5588},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5596,\"start\":5592},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5600,\"start\":5596},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5604,\"start\":5600},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5608,\"start\":5604},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5742,\"start\":5739},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5744,\"start\":5742},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5883,\"start\":5881},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5995,\"start\":5992},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5997,\"start\":5995},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6063,\"start\":6061},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6188,\"start\":6186},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6314,\"start\":6312},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7043,\"start\":7041},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7144,\"start\":7142},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7160,\"start\":7158},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8132,\"start\":8130},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10000,\"start\":9998},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10519,\"start\":10516},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10526,\"start\":10523},{\"end\":10751,\"start\":10748},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13280,\"start\":13276},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13284,\"start\":13280},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13288,\"start\":13284},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14161,\"start\":14157},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14165,\"start\":14161},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14169,\"start\":14165},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14786,\"start\":14783},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14788,\"start\":14786},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15073,\"start\":15069},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15077,\"start\":15073},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15081,\"start\":15077},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15085,\"start\":15081},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15089,\"start\":15085},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15118,\"start\":15115},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15121,\"start\":15118},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15124,\"start\":15121},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15126,\"start\":15124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15928,\"start\":15925},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16004,\"start\":16001},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23062,\"start\":23059},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31678,\"start\":31676},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":33996,\"start\":33994},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35906,\"start\":35904},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":38101,\"start\":38099},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":39238,\"start\":39236},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":39569,\"start\":39567},{\"end\":47925,\"start\":47922},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":54046,\"start\":54044},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":54274,\"start\":54270},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2857,\"start\":2856},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3681,\"start\":3680},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3745,\"start\":3744},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3772,\"start\":3770},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3773,\"start\":3772},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4164,\"start\":4163},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4194,\"start\":4193},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4238,\"start\":4236},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4308,\"start\":4306},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4344,\"start\":4343},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4437,\"start\":4435},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4497,\"start\":4495},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4525,\"start\":4523},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4644,\"start\":4642},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4761,\"start\":4759},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4828,\"start\":4826},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4873,\"start\":4871},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5320,\"start\":5318},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5592,\"start\":5588},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5596,\"start\":5592},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5600,\"start\":5596},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5604,\"start\":5600},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5608,\"start\":5604},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5742,\"start\":5739},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5744,\"start\":5742},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5883,\"start\":5881},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5995,\"start\":5992},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5997,\"start\":5995},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6063,\"start\":6061},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6188,\"start\":6186},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6314,\"start\":6312},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7043,\"start\":7041},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7144,\"start\":7142},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7160,\"start\":7158},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8132,\"start\":8130},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10000,\"start\":9998},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10519,\"start\":10516},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10526,\"start\":10523},{\"end\":10751,\"start\":10748},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13280,\"start\":13276},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13284,\"start\":13280},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13288,\"start\":13284},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14161,\"start\":14157},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14165,\"start\":14161},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14169,\"start\":14165},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14786,\"start\":14783},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14788,\"start\":14786},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15073,\"start\":15069},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15077,\"start\":15073},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15081,\"start\":15077},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15085,\"start\":15081},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15089,\"start\":15085},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15118,\"start\":15115},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15121,\"start\":15118},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15124,\"start\":15121},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15126,\"start\":15124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15928,\"start\":15925},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16004,\"start\":16001},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23062,\"start\":23059},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31678,\"start\":31676},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":33996,\"start\":33994},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35906,\"start\":35904},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":38101,\"start\":38099},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":39238,\"start\":39236},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":39569,\"start\":39567},{\"end\":47925,\"start\":47922},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":54046,\"start\":54044},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":54274,\"start\":54270}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54598,\"start\":54545},{\"attributes\":{\"id\":\"fig_2\"},\"end\":55126,\"start\":54599},{\"attributes\":{\"id\":\"fig_3\"},\"end\":55268,\"start\":55127},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55640,\"start\":55269},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55889,\"start\":55641},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56258,\"start\":55890},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":56912,\"start\":56259},{\"attributes\":{\"id\":\"fig_0\"},\"end\":54598,\"start\":54545},{\"attributes\":{\"id\":\"fig_2\"},\"end\":55126,\"start\":54599},{\"attributes\":{\"id\":\"fig_3\"},\"end\":55268,\"start\":55127},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55640,\"start\":55269},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55889,\"start\":55641},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56258,\"start\":55890},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":56912,\"start\":56259}]", "paragraph": "[{\"end\":3400,\"start\":2210},{\"end\":4875,\"start\":3402},{\"end\":6316,\"start\":4877},{\"end\":7278,\"start\":6318},{\"end\":8534,\"start\":7280},{\"end\":10608,\"start\":8567},{\"end\":11246,\"start\":10676},{\"end\":11849,\"start\":11248},{\"end\":12720,\"start\":11907},{\"end\":14217,\"start\":12722},{\"end\":14739,\"start\":14219},{\"end\":15200,\"start\":14741},{\"end\":15696,\"start\":15202},{\"end\":16092,\"start\":15721},{\"end\":16316,\"start\":16124},{\"end\":17672,\"start\":16519},{\"end\":17772,\"start\":17674},{\"end\":19034,\"start\":17774},{\"end\":20198,\"start\":19036},{\"end\":22575,\"start\":20200},{\"end\":23212,\"start\":22807},{\"end\":23320,\"start\":23290},{\"end\":23969,\"start\":23435},{\"end\":24095,\"start\":23971},{\"end\":25499,\"start\":24164},{\"end\":26865,\"start\":25501},{\"end\":27391,\"start\":26903},{\"end\":28624,\"start\":27393},{\"end\":30857,\"start\":28626},{\"end\":32001,\"start\":30859},{\"end\":32157,\"start\":32058},{\"end\":32997,\"start\":32173},{\"end\":34288,\"start\":33051},{\"end\":34864,\"start\":34390},{\"end\":35436,\"start\":34866},{\"end\":37508,\"start\":35438},{\"end\":37959,\"start\":37510},{\"end\":41808,\"start\":37961},{\"end\":42363,\"start\":41810},{\"end\":43141,\"start\":42365},{\"end\":44086,\"start\":43143},{\"end\":44816,\"start\":44088},{\"end\":46140,\"start\":44818},{\"end\":46636,\"start\":46142},{\"end\":47648,\"start\":46638},{\"end\":49746,\"start\":47650},{\"end\":49943,\"start\":49762},{\"end\":50776,\"start\":49945},{\"end\":51688,\"start\":50778},{\"end\":53341,\"start\":51739},{\"end\":54371,\"start\":53343},{\"end\":54544,\"start\":54393},{\"end\":3400,\"start\":2210},{\"end\":4875,\"start\":3402},{\"end\":6316,\"start\":4877},{\"end\":7278,\"start\":6318},{\"end\":8534,\"start\":7280},{\"end\":10608,\"start\":8567},{\"end\":11246,\"start\":10676},{\"end\":11849,\"start\":11248},{\"end\":12720,\"start\":11907},{\"end\":14217,\"start\":12722},{\"end\":14739,\"start\":14219},{\"end\":15200,\"start\":14741},{\"end\":15696,\"start\":15202},{\"end\":16092,\"start\":15721},{\"end\":16316,\"start\":16124},{\"end\":17672,\"start\":16519},{\"end\":17772,\"start\":17674},{\"end\":19034,\"start\":17774},{\"end\":20198,\"start\":19036},{\"end\":22575,\"start\":20200},{\"end\":23212,\"start\":22807},{\"end\":23320,\"start\":23290},{\"end\":23969,\"start\":23435},{\"end\":24095,\"start\":23971},{\"end\":25499,\"start\":24164},{\"end\":26865,\"start\":25501},{\"end\":27391,\"start\":26903},{\"end\":28624,\"start\":27393},{\"end\":30857,\"start\":28626},{\"end\":32001,\"start\":30859},{\"end\":32157,\"start\":32058},{\"end\":32997,\"start\":32173},{\"end\":34288,\"start\":33051},{\"end\":34864,\"start\":34390},{\"end\":35436,\"start\":34866},{\"end\":37508,\"start\":35438},{\"end\":37959,\"start\":37510},{\"end\":41808,\"start\":37961},{\"end\":42363,\"start\":41810},{\"end\":43141,\"start\":42365},{\"end\":44086,\"start\":43143},{\"end\":44816,\"start\":44088},{\"end\":46140,\"start\":44818},{\"end\":46636,\"start\":46142},{\"end\":47648,\"start\":46638},{\"end\":49746,\"start\":47650},{\"end\":49943,\"start\":49762},{\"end\":50776,\"start\":49945},{\"end\":51688,\"start\":50778},{\"end\":53341,\"start\":51739},{\"end\":54371,\"start\":53343},{\"end\":54544,\"start\":54393}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10675,\"start\":10609},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11906,\"start\":11850},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16113,\"start\":16093},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16123,\"start\":16113},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16477,\"start\":16317},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22777,\"start\":22576},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23289,\"start\":23213},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23434,\"start\":23321},{\"attributes\":{\"id\":\"formula_8\"},\"end\":24163,\"start\":24096},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32057,\"start\":32002},{\"attributes\":{\"id\":\"formula_10\"},\"end\":33050,\"start\":32998},{\"attributes\":{\"id\":\"formula_11\"},\"end\":34379,\"start\":34289},{\"attributes\":{\"id\":\"formula_12\"},\"end\":49761,\"start\":49747},{\"attributes\":{\"id\":\"formula_0\"},\"end\":10675,\"start\":10609},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11906,\"start\":11850},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16113,\"start\":16093},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16123,\"start\":16113},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16477,\"start\":16317},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22777,\"start\":22576},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23289,\"start\":23213},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23434,\"start\":23321},{\"attributes\":{\"id\":\"formula_8\"},\"end\":24163,\"start\":24096},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32057,\"start\":32002},{\"attributes\":{\"id\":\"formula_10\"},\"end\":33050,\"start\":32998},{\"attributes\":{\"id\":\"formula_11\"},\"end\":34379,\"start\":34289},{\"attributes\":{\"id\":\"formula_12\"},\"end\":49761,\"start\":49747}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":30980,\"start\":30973},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":37819,\"start\":37812},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":41856,\"start\":41849},{\"end\":54370,\"start\":54363},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":30980,\"start\":30973},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":37819,\"start\":37812},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":41856,\"start\":41849},{\"end\":54370,\"start\":54363}]", "section_header": "[{\"end\":8565,\"start\":8537},{\"end\":15719,\"start\":15699},{\"end\":16517,\"start\":16479},{\"end\":22805,\"start\":22779},{\"end\":26901,\"start\":26868},{\"end\":32171,\"start\":32160},{\"end\":34388,\"start\":34381},{\"end\":51737,\"start\":51691},{\"end\":54391,\"start\":54374},{\"end\":54608,\"start\":54600},{\"end\":55136,\"start\":55128},{\"end\":55278,\"start\":55270},{\"end\":56269,\"start\":56260},{\"end\":8565,\"start\":8537},{\"end\":15719,\"start\":15699},{\"end\":16517,\"start\":16479},{\"end\":22805,\"start\":22779},{\"end\":26901,\"start\":26868},{\"end\":32171,\"start\":32160},{\"end\":34388,\"start\":34381},{\"end\":51737,\"start\":51691},{\"end\":54391,\"start\":54374},{\"end\":54608,\"start\":54600},{\"end\":55136,\"start\":55128},{\"end\":55278,\"start\":55270},{\"end\":56269,\"start\":56260}]", "table": "[{\"end\":56258,\"start\":56099},{\"end\":56912,\"start\":56271},{\"end\":56258,\"start\":56099},{\"end\":56912,\"start\":56271}]", "figure_caption": "[{\"end\":54598,\"start\":54547},{\"end\":55126,\"start\":54610},{\"end\":55268,\"start\":55138},{\"end\":55640,\"start\":55280},{\"end\":55889,\"start\":55643},{\"end\":56099,\"start\":55892},{\"end\":54598,\"start\":54547},{\"end\":55126,\"start\":54610},{\"end\":55268,\"start\":55138},{\"end\":55640,\"start\":55280},{\"end\":55889,\"start\":55643},{\"end\":56099,\"start\":55892}]", "figure_ref": "[{\"end\":9138,\"start\":9131},{\"end\":12966,\"start\":12958},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17904,\"start\":17897},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20859,\"start\":20849},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21819,\"start\":21812},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24376,\"start\":24354},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24470,\"start\":24463},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26137,\"start\":26129},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27250,\"start\":27243},{\"end\":28866,\"start\":28843},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29521,\"start\":29512},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30284,\"start\":30277},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":49293,\"start\":49285},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":49668,\"start\":49661},{\"end\":9138,\"start\":9131},{\"end\":12966,\"start\":12958},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17904,\"start\":17897},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20859,\"start\":20849},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21819,\"start\":21812},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24376,\"start\":24354},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24470,\"start\":24463},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26137,\"start\":26129},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27250,\"start\":27243},{\"end\":28866,\"start\":28843},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29521,\"start\":29512},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30284,\"start\":30277},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":49293,\"start\":49285},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":49668,\"start\":49661}]", "bib_author_first_name": "[{\"end\":58249,\"start\":58248},{\"end\":58479,\"start\":58478},{\"end\":58738,\"start\":58737},{\"end\":58749,\"start\":58748},{\"end\":58765,\"start\":58764},{\"end\":59278,\"start\":59277},{\"end\":59288,\"start\":59287},{\"end\":59299,\"start\":59298},{\"end\":59309,\"start\":59308},{\"end\":59311,\"start\":59310},{\"end\":59677,\"start\":59676},{\"end\":59689,\"start\":59688},{\"end\":59702,\"start\":59701},{\"end\":59715,\"start\":59714},{\"end\":59725,\"start\":59724},{\"end\":60314,\"start\":60313},{\"end\":60316,\"start\":60315},{\"end\":60327,\"start\":60326},{\"end\":60329,\"start\":60328},{\"end\":60641,\"start\":60640},{\"end\":60651,\"start\":60650},{\"end\":61125,\"start\":61124},{\"end\":61135,\"start\":61134},{\"end\":61145,\"start\":61144},{\"end\":61165,\"start\":61164},{\"end\":61492,\"start\":61491},{\"end\":61501,\"start\":61500},{\"end\":61511,\"start\":61510},{\"end\":61521,\"start\":61520},{\"end\":61523,\"start\":61522},{\"end\":62100,\"start\":62099},{\"end\":62113,\"start\":62112},{\"end\":62122,\"start\":62121},{\"end\":62135,\"start\":62134},{\"end\":62458,\"start\":62457},{\"end\":62470,\"start\":62469},{\"end\":62480,\"start\":62479},{\"end\":62491,\"start\":62490},{\"end\":62500,\"start\":62499},{\"end\":62994,\"start\":62993},{\"end\":63004,\"start\":63003},{\"end\":63006,\"start\":63005},{\"end\":63016,\"start\":63015},{\"end\":63486,\"start\":63485},{\"end\":63496,\"start\":63495},{\"end\":63506,\"start\":63505},{\"end\":63508,\"start\":63507},{\"end\":63518,\"start\":63517},{\"end\":63520,\"start\":63519},{\"end\":63528,\"start\":63527},{\"end\":63530,\"start\":63529},{\"end\":63859,\"start\":63858},{\"end\":63861,\"start\":63860},{\"end\":63875,\"start\":63874},{\"end\":63877,\"start\":63876},{\"end\":64212,\"start\":64211},{\"end\":64750,\"start\":64749},{\"end\":64760,\"start\":64759},{\"end\":64770,\"start\":64769},{\"end\":65251,\"start\":65250},{\"end\":65261,\"start\":65260},{\"end\":65263,\"start\":65262},{\"end\":65272,\"start\":65271},{\"end\":65512,\"start\":65511},{\"end\":65743,\"start\":65742},{\"end\":65745,\"start\":65744},{\"end\":65753,\"start\":65752},{\"end\":65755,\"start\":65754},{\"end\":65766,\"start\":65765},{\"end\":65768,\"start\":65767},{\"end\":65985,\"start\":65984},{\"end\":66194,\"start\":66193},{\"end\":66196,\"start\":66195},{\"end\":66205,\"start\":66204},{\"end\":66207,\"start\":66206},{\"end\":66219,\"start\":66218},{\"end\":66221,\"start\":66220},{\"end\":66427,\"start\":66426},{\"end\":66441,\"start\":66437},{\"end\":66443,\"start\":66442},{\"end\":66642,\"start\":66641},{\"end\":66655,\"start\":66654},{\"end\":66667,\"start\":66666},{\"end\":66686,\"start\":66685},{\"end\":67088,\"start\":67087},{\"end\":67627,\"start\":67626},{\"end\":67633,\"start\":67632},{\"end\":67635,\"start\":67634},{\"end\":67641,\"start\":67640},{\"end\":67650,\"start\":67649},{\"end\":67654,\"start\":67651},{\"end\":68250,\"start\":68249},{\"end\":68252,\"start\":68251},{\"end\":68667,\"start\":68666},{\"end\":69094,\"start\":69093},{\"end\":69103,\"start\":69102},{\"end\":69105,\"start\":69104},{\"end\":69116,\"start\":69115},{\"end\":69475,\"start\":69474},{\"end\":69637,\"start\":69633},{\"end\":69639,\"start\":69638},{\"end\":69647,\"start\":69646},{\"end\":69876,\"start\":69875},{\"end\":70062,\"start\":70061},{\"end\":70332,\"start\":70331},{\"end\":70544,\"start\":70540},{\"end\":70546,\"start\":70545},{\"end\":70704,\"start\":70703},{\"end\":70706,\"start\":70705},{\"end\":71000,\"start\":70999},{\"end\":71009,\"start\":71008},{\"end\":71011,\"start\":71010},{\"end\":71024,\"start\":71023},{\"end\":71034,\"start\":71030},{\"end\":71036,\"start\":71035},{\"end\":71277,\"start\":71276},{\"end\":71285,\"start\":71284},{\"end\":71296,\"start\":71295},{\"end\":71308,\"start\":71307},{\"end\":71321,\"start\":71320},{\"end\":71552,\"start\":71551},{\"end\":71753,\"start\":71752},{\"end\":71997,\"start\":71996},{\"end\":72290,\"start\":72289},{\"end\":72302,\"start\":72301},{\"end\":72315,\"start\":72314},{\"end\":72329,\"start\":72328},{\"end\":72331,\"start\":72330},{\"end\":72341,\"start\":72340},{\"end\":72616,\"start\":72614},{\"end\":72625,\"start\":72624},{\"end\":72816,\"start\":72815},{\"end\":72825,\"start\":72824},{\"end\":72827,\"start\":72826},{\"end\":72835,\"start\":72834},{\"end\":72846,\"start\":72845},{\"end\":73078,\"start\":73077},{\"end\":73087,\"start\":73086},{\"end\":73293,\"start\":73292},{\"end\":73295,\"start\":73294},{\"end\":73303,\"start\":73302},{\"end\":73305,\"start\":73304},{\"end\":73524,\"start\":73523},{\"end\":73914,\"start\":73913},{\"end\":74255,\"start\":74254},{\"end\":74265,\"start\":74264},{\"end\":74276,\"start\":74275},{\"end\":74278,\"start\":74277},{\"end\":74767,\"start\":74766},{\"end\":74780,\"start\":74779},{\"end\":74791,\"start\":74790},{\"end\":75238,\"start\":75237},{\"end\":75451,\"start\":75450},{\"end\":75453,\"start\":75452},{\"end\":75660,\"start\":75659},{\"end\":75670,\"start\":75669},{\"end\":75681,\"start\":75680},{\"end\":75692,\"start\":75691},{\"end\":75702,\"start\":75701},{\"end\":75704,\"start\":75703},{\"end\":76210,\"start\":76209},{\"end\":76222,\"start\":76221},{\"end\":76234,\"start\":76233},{\"end\":58249,\"start\":58248},{\"end\":58479,\"start\":58478},{\"end\":58738,\"start\":58737},{\"end\":58749,\"start\":58748},{\"end\":58765,\"start\":58764},{\"end\":59278,\"start\":59277},{\"end\":59288,\"start\":59287},{\"end\":59299,\"start\":59298},{\"end\":59309,\"start\":59308},{\"end\":59311,\"start\":59310},{\"end\":59677,\"start\":59676},{\"end\":59689,\"start\":59688},{\"end\":59702,\"start\":59701},{\"end\":59715,\"start\":59714},{\"end\":59725,\"start\":59724},{\"end\":60314,\"start\":60313},{\"end\":60316,\"start\":60315},{\"end\":60327,\"start\":60326},{\"end\":60329,\"start\":60328},{\"end\":60641,\"start\":60640},{\"end\":60651,\"start\":60650},{\"end\":61125,\"start\":61124},{\"end\":61135,\"start\":61134},{\"end\":61145,\"start\":61144},{\"end\":61165,\"start\":61164},{\"end\":61492,\"start\":61491},{\"end\":61501,\"start\":61500},{\"end\":61511,\"start\":61510},{\"end\":61521,\"start\":61520},{\"end\":61523,\"start\":61522},{\"end\":62100,\"start\":62099},{\"end\":62113,\"start\":62112},{\"end\":62122,\"start\":62121},{\"end\":62135,\"start\":62134},{\"end\":62458,\"start\":62457},{\"end\":62470,\"start\":62469},{\"end\":62480,\"start\":62479},{\"end\":62491,\"start\":62490},{\"end\":62500,\"start\":62499},{\"end\":62994,\"start\":62993},{\"end\":63004,\"start\":63003},{\"end\":63006,\"start\":63005},{\"end\":63016,\"start\":63015},{\"end\":63486,\"start\":63485},{\"end\":63496,\"start\":63495},{\"end\":63506,\"start\":63505},{\"end\":63508,\"start\":63507},{\"end\":63518,\"start\":63517},{\"end\":63520,\"start\":63519},{\"end\":63528,\"start\":63527},{\"end\":63530,\"start\":63529},{\"end\":63859,\"start\":63858},{\"end\":63861,\"start\":63860},{\"end\":63875,\"start\":63874},{\"end\":63877,\"start\":63876},{\"end\":64212,\"start\":64211},{\"end\":64750,\"start\":64749},{\"end\":64760,\"start\":64759},{\"end\":64770,\"start\":64769},{\"end\":65251,\"start\":65250},{\"end\":65261,\"start\":65260},{\"end\":65263,\"start\":65262},{\"end\":65272,\"start\":65271},{\"end\":65512,\"start\":65511},{\"end\":65743,\"start\":65742},{\"end\":65745,\"start\":65744},{\"end\":65753,\"start\":65752},{\"end\":65755,\"start\":65754},{\"end\":65766,\"start\":65765},{\"end\":65768,\"start\":65767},{\"end\":65985,\"start\":65984},{\"end\":66194,\"start\":66193},{\"end\":66196,\"start\":66195},{\"end\":66205,\"start\":66204},{\"end\":66207,\"start\":66206},{\"end\":66219,\"start\":66218},{\"end\":66221,\"start\":66220},{\"end\":66427,\"start\":66426},{\"end\":66441,\"start\":66437},{\"end\":66443,\"start\":66442},{\"end\":66642,\"start\":66641},{\"end\":66655,\"start\":66654},{\"end\":66667,\"start\":66666},{\"end\":66686,\"start\":66685},{\"end\":67088,\"start\":67087},{\"end\":67627,\"start\":67626},{\"end\":67633,\"start\":67632},{\"end\":67635,\"start\":67634},{\"end\":67641,\"start\":67640},{\"end\":67650,\"start\":67649},{\"end\":67654,\"start\":67651},{\"end\":68250,\"start\":68249},{\"end\":68252,\"start\":68251},{\"end\":68667,\"start\":68666},{\"end\":69094,\"start\":69093},{\"end\":69103,\"start\":69102},{\"end\":69105,\"start\":69104},{\"end\":69116,\"start\":69115},{\"end\":69475,\"start\":69474},{\"end\":69637,\"start\":69633},{\"end\":69639,\"start\":69638},{\"end\":69647,\"start\":69646},{\"end\":69876,\"start\":69875},{\"end\":70062,\"start\":70061},{\"end\":70332,\"start\":70331},{\"end\":70544,\"start\":70540},{\"end\":70546,\"start\":70545},{\"end\":70704,\"start\":70703},{\"end\":70706,\"start\":70705},{\"end\":71000,\"start\":70999},{\"end\":71009,\"start\":71008},{\"end\":71011,\"start\":71010},{\"end\":71024,\"start\":71023},{\"end\":71034,\"start\":71030},{\"end\":71036,\"start\":71035},{\"end\":71277,\"start\":71276},{\"end\":71285,\"start\":71284},{\"end\":71296,\"start\":71295},{\"end\":71308,\"start\":71307},{\"end\":71321,\"start\":71320},{\"end\":71552,\"start\":71551},{\"end\":71753,\"start\":71752},{\"end\":71997,\"start\":71996},{\"end\":72290,\"start\":72289},{\"end\":72302,\"start\":72301},{\"end\":72315,\"start\":72314},{\"end\":72329,\"start\":72328},{\"end\":72331,\"start\":72330},{\"end\":72341,\"start\":72340},{\"end\":72616,\"start\":72614},{\"end\":72625,\"start\":72624},{\"end\":72816,\"start\":72815},{\"end\":72825,\"start\":72824},{\"end\":72827,\"start\":72826},{\"end\":72835,\"start\":72834},{\"end\":72846,\"start\":72845},{\"end\":73078,\"start\":73077},{\"end\":73087,\"start\":73086},{\"end\":73293,\"start\":73292},{\"end\":73295,\"start\":73294},{\"end\":73303,\"start\":73302},{\"end\":73305,\"start\":73304},{\"end\":73524,\"start\":73523},{\"end\":73914,\"start\":73913},{\"end\":74255,\"start\":74254},{\"end\":74265,\"start\":74264},{\"end\":74276,\"start\":74275},{\"end\":74278,\"start\":74277},{\"end\":74767,\"start\":74766},{\"end\":74780,\"start\":74779},{\"end\":74791,\"start\":74790},{\"end\":75238,\"start\":75237},{\"end\":75451,\"start\":75450},{\"end\":75453,\"start\":75452},{\"end\":75660,\"start\":75659},{\"end\":75670,\"start\":75669},{\"end\":75681,\"start\":75680},{\"end\":75692,\"start\":75691},{\"end\":75702,\"start\":75701},{\"end\":75704,\"start\":75703},{\"end\":76210,\"start\":76209},{\"end\":76222,\"start\":76221},{\"end\":76234,\"start\":76233}]", "bib_author_last_name": "[{\"end\":58257,\"start\":58250},{\"end\":58487,\"start\":58480},{\"end\":58746,\"start\":58739},{\"end\":58762,\"start\":58750},{\"end\":58771,\"start\":58766},{\"end\":59285,\"start\":59279},{\"end\":59296,\"start\":59289},{\"end\":59306,\"start\":59300},{\"end\":59318,\"start\":59312},{\"end\":59686,\"start\":59678},{\"end\":59699,\"start\":59690},{\"end\":59712,\"start\":59703},{\"end\":59722,\"start\":59716},{\"end\":59732,\"start\":59726},{\"end\":60324,\"start\":60317},{\"end\":60338,\"start\":60330},{\"end\":60648,\"start\":60642},{\"end\":60658,\"start\":60652},{\"end\":61132,\"start\":61126},{\"end\":61142,\"start\":61136},{\"end\":61162,\"start\":61146},{\"end\":61173,\"start\":61166},{\"end\":61498,\"start\":61493},{\"end\":61508,\"start\":61502},{\"end\":61518,\"start\":61512},{\"end\":61526,\"start\":61524},{\"end\":62110,\"start\":62101},{\"end\":62119,\"start\":62114},{\"end\":62132,\"start\":62123},{\"end\":62145,\"start\":62136},{\"end\":62467,\"start\":62459},{\"end\":62477,\"start\":62471},{\"end\":62488,\"start\":62481},{\"end\":62497,\"start\":62492},{\"end\":62507,\"start\":62501},{\"end\":63001,\"start\":62995},{\"end\":63013,\"start\":63007},{\"end\":63023,\"start\":63017},{\"end\":63493,\"start\":63487},{\"end\":63503,\"start\":63497},{\"end\":63515,\"start\":63509},{\"end\":63525,\"start\":63521},{\"end\":63535,\"start\":63531},{\"end\":63872,\"start\":63862},{\"end\":63889,\"start\":63878},{\"end\":64224,\"start\":64213},{\"end\":64757,\"start\":64751},{\"end\":64767,\"start\":64761},{\"end\":64778,\"start\":64771},{\"end\":65258,\"start\":65252},{\"end\":65269,\"start\":65264},{\"end\":65279,\"start\":65273},{\"end\":65519,\"start\":65513},{\"end\":65750,\"start\":65746},{\"end\":65763,\"start\":65756},{\"end\":65776,\"start\":65769},{\"end\":65995,\"start\":65986},{\"end\":66202,\"start\":66197},{\"end\":66216,\"start\":66208},{\"end\":66224,\"start\":66222},{\"end\":66435,\"start\":66428},{\"end\":66448,\"start\":66444},{\"end\":66652,\"start\":66643},{\"end\":66664,\"start\":66656},{\"end\":66683,\"start\":66668},{\"end\":66698,\"start\":66687},{\"end\":67091,\"start\":67089},{\"end\":67630,\"start\":67628},{\"end\":67638,\"start\":67636},{\"end\":67647,\"start\":67642},{\"end\":67659,\"start\":67655},{\"end\":68255,\"start\":68253},{\"end\":68675,\"start\":68668},{\"end\":69100,\"start\":69095},{\"end\":69113,\"start\":69106},{\"end\":69124,\"start\":69117},{\"end\":69480,\"start\":69476},{\"end\":69644,\"start\":69640},{\"end\":69658,\"start\":69648},{\"end\":69886,\"start\":69877},{\"end\":70072,\"start\":70063},{\"end\":70337,\"start\":70333},{\"end\":70551,\"start\":70547},{\"end\":70711,\"start\":70707},{\"end\":71006,\"start\":71001},{\"end\":71021,\"start\":71012},{\"end\":71028,\"start\":71025},{\"end\":71041,\"start\":71037},{\"end\":71282,\"start\":71278},{\"end\":71293,\"start\":71286},{\"end\":71305,\"start\":71297},{\"end\":71318,\"start\":71309},{\"end\":71333,\"start\":71322},{\"end\":71559,\"start\":71553},{\"end\":71763,\"start\":71754},{\"end\":72003,\"start\":71998},{\"end\":72299,\"start\":72291},{\"end\":72312,\"start\":72303},{\"end\":72326,\"start\":72316},{\"end\":72338,\"start\":72332},{\"end\":72351,\"start\":72342},{\"end\":72622,\"start\":72617},{\"end\":72822,\"start\":72817},{\"end\":72832,\"start\":72828},{\"end\":72843,\"start\":72836},{\"end\":72850,\"start\":72847},{\"end\":73084,\"start\":73079},{\"end\":73092,\"start\":73088},{\"end\":73300,\"start\":73296},{\"end\":73314,\"start\":73306},{\"end\":73530,\"start\":73525},{\"end\":73925,\"start\":73915},{\"end\":74262,\"start\":74256},{\"end\":74273,\"start\":74266},{\"end\":74285,\"start\":74279},{\"end\":74777,\"start\":74768},{\"end\":74788,\"start\":74781},{\"end\":74799,\"start\":74792},{\"end\":75244,\"start\":75239},{\"end\":75463,\"start\":75454},{\"end\":75667,\"start\":75661},{\"end\":75678,\"start\":75671},{\"end\":75689,\"start\":75682},{\"end\":75699,\"start\":75693},{\"end\":75711,\"start\":75705},{\"end\":76219,\"start\":76211},{\"end\":76231,\"start\":76223},{\"end\":76241,\"start\":76235},{\"end\":58257,\"start\":58250},{\"end\":58487,\"start\":58480},{\"end\":58746,\"start\":58739},{\"end\":58762,\"start\":58750},{\"end\":58771,\"start\":58766},{\"end\":59285,\"start\":59279},{\"end\":59296,\"start\":59289},{\"end\":59306,\"start\":59300},{\"end\":59318,\"start\":59312},{\"end\":59686,\"start\":59678},{\"end\":59699,\"start\":59690},{\"end\":59712,\"start\":59703},{\"end\":59722,\"start\":59716},{\"end\":59732,\"start\":59726},{\"end\":60324,\"start\":60317},{\"end\":60338,\"start\":60330},{\"end\":60648,\"start\":60642},{\"end\":60658,\"start\":60652},{\"end\":61132,\"start\":61126},{\"end\":61142,\"start\":61136},{\"end\":61162,\"start\":61146},{\"end\":61173,\"start\":61166},{\"end\":61498,\"start\":61493},{\"end\":61508,\"start\":61502},{\"end\":61518,\"start\":61512},{\"end\":61526,\"start\":61524},{\"end\":62110,\"start\":62101},{\"end\":62119,\"start\":62114},{\"end\":62132,\"start\":62123},{\"end\":62145,\"start\":62136},{\"end\":62467,\"start\":62459},{\"end\":62477,\"start\":62471},{\"end\":62488,\"start\":62481},{\"end\":62497,\"start\":62492},{\"end\":62507,\"start\":62501},{\"end\":63001,\"start\":62995},{\"end\":63013,\"start\":63007},{\"end\":63023,\"start\":63017},{\"end\":63493,\"start\":63487},{\"end\":63503,\"start\":63497},{\"end\":63515,\"start\":63509},{\"end\":63525,\"start\":63521},{\"end\":63535,\"start\":63531},{\"end\":63872,\"start\":63862},{\"end\":63889,\"start\":63878},{\"end\":64224,\"start\":64213},{\"end\":64757,\"start\":64751},{\"end\":64767,\"start\":64761},{\"end\":64778,\"start\":64771},{\"end\":65258,\"start\":65252},{\"end\":65269,\"start\":65264},{\"end\":65279,\"start\":65273},{\"end\":65519,\"start\":65513},{\"end\":65750,\"start\":65746},{\"end\":65763,\"start\":65756},{\"end\":65776,\"start\":65769},{\"end\":65995,\"start\":65986},{\"end\":66202,\"start\":66197},{\"end\":66216,\"start\":66208},{\"end\":66224,\"start\":66222},{\"end\":66435,\"start\":66428},{\"end\":66448,\"start\":66444},{\"end\":66652,\"start\":66643},{\"end\":66664,\"start\":66656},{\"end\":66683,\"start\":66668},{\"end\":66698,\"start\":66687},{\"end\":67091,\"start\":67089},{\"end\":67630,\"start\":67628},{\"end\":67638,\"start\":67636},{\"end\":67647,\"start\":67642},{\"end\":67659,\"start\":67655},{\"end\":68255,\"start\":68253},{\"end\":68675,\"start\":68668},{\"end\":69100,\"start\":69095},{\"end\":69113,\"start\":69106},{\"end\":69124,\"start\":69117},{\"end\":69480,\"start\":69476},{\"end\":69644,\"start\":69640},{\"end\":69658,\"start\":69648},{\"end\":69886,\"start\":69877},{\"end\":70072,\"start\":70063},{\"end\":70337,\"start\":70333},{\"end\":70551,\"start\":70547},{\"end\":70711,\"start\":70707},{\"end\":71006,\"start\":71001},{\"end\":71021,\"start\":71012},{\"end\":71028,\"start\":71025},{\"end\":71041,\"start\":71037},{\"end\":71282,\"start\":71278},{\"end\":71293,\"start\":71286},{\"end\":71305,\"start\":71297},{\"end\":71318,\"start\":71309},{\"end\":71333,\"start\":71322},{\"end\":71559,\"start\":71553},{\"end\":71763,\"start\":71754},{\"end\":72003,\"start\":71998},{\"end\":72299,\"start\":72291},{\"end\":72312,\"start\":72303},{\"end\":72326,\"start\":72316},{\"end\":72338,\"start\":72332},{\"end\":72351,\"start\":72342},{\"end\":72622,\"start\":72617},{\"end\":72822,\"start\":72817},{\"end\":72832,\"start\":72828},{\"end\":72843,\"start\":72836},{\"end\":72850,\"start\":72847},{\"end\":73084,\"start\":73079},{\"end\":73092,\"start\":73088},{\"end\":73300,\"start\":73296},{\"end\":73314,\"start\":73306},{\"end\":73530,\"start\":73525},{\"end\":73925,\"start\":73915},{\"end\":74262,\"start\":74256},{\"end\":74273,\"start\":74266},{\"end\":74285,\"start\":74279},{\"end\":74777,\"start\":74768},{\"end\":74788,\"start\":74781},{\"end\":74799,\"start\":74792},{\"end\":75244,\"start\":75239},{\"end\":75463,\"start\":75454},{\"end\":75667,\"start\":75661},{\"end\":75678,\"start\":75671},{\"end\":75689,\"start\":75682},{\"end\":75699,\"start\":75693},{\"end\":75711,\"start\":75705},{\"end\":76219,\"start\":76211},{\"end\":76231,\"start\":76223},{\"end\":76241,\"start\":76235}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":58351,\"start\":58248},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":733980},\"end\":58673,\"start\":58353},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":60571601},\"end\":59135,\"start\":58675},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57365377},\"end\":59558,\"start\":59137},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":68046116},\"end\":60189,\"start\":59560},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15258913},\"end\":60594,\"start\":60191},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":18032844},\"end\":61010,\"start\":60596},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49416738},\"end\":61394,\"start\":61012},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":155774532},\"end\":61997,\"start\":61396},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":182118830},\"end\":62351,\"start\":61999},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49291228},\"end\":62901,\"start\":62353},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15654769},\"end\":63394,\"start\":62903},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":60493868},\"end\":63781,\"start\":63396},{\"attributes\":{\"id\":\"b13\"},\"end\":64066,\"start\":63783},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":208633942},\"end\":64638,\"start\":64068},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":31150062},\"end\":65181,\"start\":64640},{\"attributes\":{\"id\":\"b16\"},\"end\":65454,\"start\":65183},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10569020},\"end\":65706,\"start\":65456},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":38314993},\"end\":65910,\"start\":65708},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2640460},\"end\":66136,\"start\":65912},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":187510377},\"end\":66370,\"start\":66138},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":57248729},\"end\":66582,\"start\":66372},{\"attributes\":{\"doi\":\"10.1038/s41565-020-0655-z\",\"id\":\"b22\",\"matched_paper_id\":214704544},\"end\":66935,\"start\":66584},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":25209638},\"end\":67493,\"start\":66937},{\"attributes\":{\"doi\":\"VLSI-TSA) 1-2\",\"id\":\"b24\",\"matched_paper_id\":10674378},\"end\":68132,\"start\":67495},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3869844},\"end\":68621,\"start\":68134},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12261165},\"end\":69050,\"start\":68623},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":39020350},\"end\":69426,\"start\":69052},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":121499827},\"end\":69589,\"start\":69428},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":205453577},\"end\":69791,\"start\":69591},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4430199},\"end\":70030,\"start\":69793},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206655274},\"end\":70234,\"start\":70032},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":201222112},\"end\":70517,\"start\":70236},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2534408},\"end\":70650,\"start\":70519},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":26729693},\"end\":70898,\"start\":70652},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":15414036},\"end\":71241,\"start\":70900},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":8980558},\"end\":71494,\"start\":71243},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":29972868},\"end\":71680,\"start\":71496},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":125727597},\"end\":71914,\"start\":71682},{\"attributes\":{\"doi\":\"10.1038/s41467-020-16108-9\",\"id\":\"b39\",\"matched_paper_id\":174801354},\"end\":72205,\"start\":71916},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":26746674},\"end\":72575,\"start\":72207},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":4925786},\"end\":72734,\"start\":72577},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":21787748},\"end\":73017,\"start\":72736},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":106685952},\"end\":73257,\"start\":73019},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":10486206},\"end\":73436,\"start\":73259},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":23900754},\"end\":73855,\"start\":73438},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":46672991},\"end\":74162,\"start\":73857},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":9812826},\"end\":74715,\"start\":74164},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":17089484},\"end\":75170,\"start\":74717},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":38407095},\"end\":75428,\"start\":75172},{\"attributes\":{\"id\":\"b50\"},\"end\":75565,\"start\":75430},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":12008695},\"end\":76131,\"start\":75567},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":15757682},\"end\":76693,\"start\":76133},{\"attributes\":{\"id\":\"b0\"},\"end\":58351,\"start\":58248},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":733980},\"end\":58673,\"start\":58353},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":60571601},\"end\":59135,\"start\":58675},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57365377},\"end\":59558,\"start\":59137},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":68046116},\"end\":60189,\"start\":59560},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":15258913},\"end\":60594,\"start\":60191},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":18032844},\"end\":61010,\"start\":60596},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49416738},\"end\":61394,\"start\":61012},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":155774532},\"end\":61997,\"start\":61396},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":182118830},\"end\":62351,\"start\":61999},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49291228},\"end\":62901,\"start\":62353},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":15654769},\"end\":63394,\"start\":62903},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":60493868},\"end\":63781,\"start\":63396},{\"attributes\":{\"id\":\"b13\"},\"end\":64066,\"start\":63783},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":208633942},\"end\":64638,\"start\":64068},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":31150062},\"end\":65181,\"start\":64640},{\"attributes\":{\"id\":\"b16\"},\"end\":65454,\"start\":65183},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":10569020},\"end\":65706,\"start\":65456},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":38314993},\"end\":65910,\"start\":65708},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2640460},\"end\":66136,\"start\":65912},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":187510377},\"end\":66370,\"start\":66138},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":57248729},\"end\":66582,\"start\":66372},{\"attributes\":{\"doi\":\"10.1038/s41565-020-0655-z\",\"id\":\"b22\",\"matched_paper_id\":214704544},\"end\":66935,\"start\":66584},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":25209638},\"end\":67493,\"start\":66937},{\"attributes\":{\"doi\":\"VLSI-TSA) 1-2\",\"id\":\"b24\",\"matched_paper_id\":10674378},\"end\":68132,\"start\":67495},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3869844},\"end\":68621,\"start\":68134},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12261165},\"end\":69050,\"start\":68623},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":39020350},\"end\":69426,\"start\":69052},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":121499827},\"end\":69589,\"start\":69428},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":205453577},\"end\":69791,\"start\":69591},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4430199},\"end\":70030,\"start\":69793},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206655274},\"end\":70234,\"start\":70032},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":201222112},\"end\":70517,\"start\":70236},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2534408},\"end\":70650,\"start\":70519},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":26729693},\"end\":70898,\"start\":70652},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":15414036},\"end\":71241,\"start\":70900},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":8980558},\"end\":71494,\"start\":71243},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":29972868},\"end\":71680,\"start\":71496},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":125727597},\"end\":71914,\"start\":71682},{\"attributes\":{\"doi\":\"10.1038/s41467-020-16108-9\",\"id\":\"b39\",\"matched_paper_id\":174801354},\"end\":72205,\"start\":71916},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":26746674},\"end\":72575,\"start\":72207},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":4925786},\"end\":72734,\"start\":72577},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":21787748},\"end\":73017,\"start\":72736},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":106685952},\"end\":73257,\"start\":73019},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":10486206},\"end\":73436,\"start\":73259},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":23900754},\"end\":73855,\"start\":73438},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":46672991},\"end\":74162,\"start\":73857},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":9812826},\"end\":74715,\"start\":74164},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":17089484},\"end\":75170,\"start\":74717},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":38407095},\"end\":75428,\"start\":75172},{\"attributes\":{\"id\":\"b50\"},\"end\":75565,\"start\":75430},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":12008695},\"end\":76131,\"start\":75567},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":15757682},\"end\":76693,\"start\":76133}]", "bib_title": "[{\"end\":58476,\"start\":58353},{\"end\":58735,\"start\":58675},{\"end\":59275,\"start\":59137},{\"end\":59674,\"start\":59560},{\"end\":60311,\"start\":60191},{\"end\":60638,\"start\":60596},{\"end\":61122,\"start\":61012},{\"end\":61489,\"start\":61396},{\"end\":62097,\"start\":61999},{\"end\":62455,\"start\":62353},{\"end\":62991,\"start\":62903},{\"end\":63483,\"start\":63396},{\"end\":63856,\"start\":63783},{\"end\":64209,\"start\":64068},{\"end\":64747,\"start\":64640},{\"end\":65509,\"start\":65456},{\"end\":65740,\"start\":65708},{\"end\":65982,\"start\":65912},{\"end\":66191,\"start\":66138},{\"end\":66424,\"start\":66372},{\"end\":66639,\"start\":66584},{\"end\":67085,\"start\":66937},{\"end\":67624,\"start\":67495},{\"end\":68247,\"start\":68134},{\"end\":68664,\"start\":68623},{\"end\":69091,\"start\":69052},{\"end\":69472,\"start\":69428},{\"end\":69631,\"start\":69591},{\"end\":69873,\"start\":69793},{\"end\":70059,\"start\":70032},{\"end\":70329,\"start\":70236},{\"end\":70538,\"start\":70519},{\"end\":70701,\"start\":70652},{\"end\":70997,\"start\":70900},{\"end\":71274,\"start\":71243},{\"end\":71549,\"start\":71496},{\"end\":71750,\"start\":71682},{\"end\":71994,\"start\":71916},{\"end\":72287,\"start\":72207},{\"end\":72612,\"start\":72577},{\"end\":72813,\"start\":72736},{\"end\":73075,\"start\":73019},{\"end\":73290,\"start\":73259},{\"end\":73521,\"start\":73438},{\"end\":73911,\"start\":73857},{\"end\":74252,\"start\":74164},{\"end\":74764,\"start\":74717},{\"end\":75235,\"start\":75172},{\"end\":75657,\"start\":75567},{\"end\":76207,\"start\":76133},{\"end\":58476,\"start\":58353},{\"end\":58735,\"start\":58675},{\"end\":59275,\"start\":59137},{\"end\":59674,\"start\":59560},{\"end\":60311,\"start\":60191},{\"end\":60638,\"start\":60596},{\"end\":61122,\"start\":61012},{\"end\":61489,\"start\":61396},{\"end\":62097,\"start\":61999},{\"end\":62455,\"start\":62353},{\"end\":62991,\"start\":62903},{\"end\":63483,\"start\":63396},{\"end\":63856,\"start\":63783},{\"end\":64209,\"start\":64068},{\"end\":64747,\"start\":64640},{\"end\":65509,\"start\":65456},{\"end\":65740,\"start\":65708},{\"end\":65982,\"start\":65912},{\"end\":66191,\"start\":66138},{\"end\":66424,\"start\":66372},{\"end\":66639,\"start\":66584},{\"end\":67085,\"start\":66937},{\"end\":67624,\"start\":67495},{\"end\":68247,\"start\":68134},{\"end\":68664,\"start\":68623},{\"end\":69091,\"start\":69052},{\"end\":69472,\"start\":69428},{\"end\":69631,\"start\":69591},{\"end\":69873,\"start\":69793},{\"end\":70059,\"start\":70032},{\"end\":70329,\"start\":70236},{\"end\":70538,\"start\":70519},{\"end\":70701,\"start\":70652},{\"end\":70997,\"start\":70900},{\"end\":71274,\"start\":71243},{\"end\":71549,\"start\":71496},{\"end\":71750,\"start\":71682},{\"end\":71994,\"start\":71916},{\"end\":72287,\"start\":72207},{\"end\":72612,\"start\":72577},{\"end\":72813,\"start\":72736},{\"end\":73075,\"start\":73019},{\"end\":73290,\"start\":73259},{\"end\":73521,\"start\":73438},{\"end\":73911,\"start\":73857},{\"end\":74252,\"start\":74164},{\"end\":74764,\"start\":74717},{\"end\":75235,\"start\":75172},{\"end\":75657,\"start\":75567},{\"end\":76207,\"start\":76133}]", "bib_author": "[{\"end\":58259,\"start\":58248},{\"end\":58489,\"start\":58478},{\"end\":58748,\"start\":58737},{\"end\":58764,\"start\":58748},{\"end\":58773,\"start\":58764},{\"end\":59287,\"start\":59277},{\"end\":59298,\"start\":59287},{\"end\":59308,\"start\":59298},{\"end\":59320,\"start\":59308},{\"end\":59688,\"start\":59676},{\"end\":59701,\"start\":59688},{\"end\":59714,\"start\":59701},{\"end\":59724,\"start\":59714},{\"end\":59734,\"start\":59724},{\"end\":60326,\"start\":60313},{\"end\":60340,\"start\":60326},{\"end\":60650,\"start\":60640},{\"end\":60660,\"start\":60650},{\"end\":61134,\"start\":61124},{\"end\":61144,\"start\":61134},{\"end\":61164,\"start\":61144},{\"end\":61175,\"start\":61164},{\"end\":61500,\"start\":61491},{\"end\":61510,\"start\":61500},{\"end\":61520,\"start\":61510},{\"end\":61528,\"start\":61520},{\"end\":62112,\"start\":62099},{\"end\":62121,\"start\":62112},{\"end\":62134,\"start\":62121},{\"end\":62147,\"start\":62134},{\"end\":62469,\"start\":62457},{\"end\":62479,\"start\":62469},{\"end\":62490,\"start\":62479},{\"end\":62499,\"start\":62490},{\"end\":62509,\"start\":62499},{\"end\":63003,\"start\":62993},{\"end\":63015,\"start\":63003},{\"end\":63025,\"start\":63015},{\"end\":63495,\"start\":63485},{\"end\":63505,\"start\":63495},{\"end\":63517,\"start\":63505},{\"end\":63527,\"start\":63517},{\"end\":63537,\"start\":63527},{\"end\":63874,\"start\":63858},{\"end\":63891,\"start\":63874},{\"end\":64226,\"start\":64211},{\"end\":64759,\"start\":64749},{\"end\":64769,\"start\":64759},{\"end\":64780,\"start\":64769},{\"end\":65260,\"start\":65250},{\"end\":65271,\"start\":65260},{\"end\":65281,\"start\":65271},{\"end\":65521,\"start\":65511},{\"end\":65752,\"start\":65742},{\"end\":65765,\"start\":65752},{\"end\":65778,\"start\":65765},{\"end\":65997,\"start\":65984},{\"end\":66204,\"start\":66193},{\"end\":66218,\"start\":66204},{\"end\":66226,\"start\":66218},{\"end\":66437,\"start\":66426},{\"end\":66450,\"start\":66437},{\"end\":66654,\"start\":66641},{\"end\":66666,\"start\":66654},{\"end\":66685,\"start\":66666},{\"end\":66700,\"start\":66685},{\"end\":67093,\"start\":67087},{\"end\":67632,\"start\":67626},{\"end\":67640,\"start\":67632},{\"end\":67649,\"start\":67640},{\"end\":67661,\"start\":67649},{\"end\":68257,\"start\":68249},{\"end\":68677,\"start\":68666},{\"end\":69102,\"start\":69093},{\"end\":69115,\"start\":69102},{\"end\":69126,\"start\":69115},{\"end\":69482,\"start\":69474},{\"end\":69646,\"start\":69633},{\"end\":69660,\"start\":69646},{\"end\":69888,\"start\":69875},{\"end\":70074,\"start\":70061},{\"end\":70339,\"start\":70331},{\"end\":70553,\"start\":70540},{\"end\":70713,\"start\":70703},{\"end\":71008,\"start\":70999},{\"end\":71023,\"start\":71008},{\"end\":71030,\"start\":71023},{\"end\":71043,\"start\":71030},{\"end\":71284,\"start\":71276},{\"end\":71295,\"start\":71284},{\"end\":71307,\"start\":71295},{\"end\":71320,\"start\":71307},{\"end\":71335,\"start\":71320},{\"end\":71561,\"start\":71551},{\"end\":71765,\"start\":71752},{\"end\":72005,\"start\":71996},{\"end\":72301,\"start\":72289},{\"end\":72314,\"start\":72301},{\"end\":72328,\"start\":72314},{\"end\":72340,\"start\":72328},{\"end\":72353,\"start\":72340},{\"end\":72624,\"start\":72614},{\"end\":72628,\"start\":72624},{\"end\":72824,\"start\":72815},{\"end\":72834,\"start\":72824},{\"end\":72845,\"start\":72834},{\"end\":72852,\"start\":72845},{\"end\":73086,\"start\":73077},{\"end\":73094,\"start\":73086},{\"end\":73302,\"start\":73292},{\"end\":73316,\"start\":73302},{\"end\":73532,\"start\":73523},{\"end\":73927,\"start\":73913},{\"end\":74264,\"start\":74254},{\"end\":74275,\"start\":74264},{\"end\":74287,\"start\":74275},{\"end\":74779,\"start\":74766},{\"end\":74790,\"start\":74779},{\"end\":74801,\"start\":74790},{\"end\":75246,\"start\":75237},{\"end\":75465,\"start\":75450},{\"end\":75669,\"start\":75659},{\"end\":75680,\"start\":75669},{\"end\":75691,\"start\":75680},{\"end\":75701,\"start\":75691},{\"end\":75713,\"start\":75701},{\"end\":76221,\"start\":76209},{\"end\":76233,\"start\":76221},{\"end\":76243,\"start\":76233},{\"end\":58259,\"start\":58248},{\"end\":58489,\"start\":58478},{\"end\":58748,\"start\":58737},{\"end\":58764,\"start\":58748},{\"end\":58773,\"start\":58764},{\"end\":59287,\"start\":59277},{\"end\":59298,\"start\":59287},{\"end\":59308,\"start\":59298},{\"end\":59320,\"start\":59308},{\"end\":59688,\"start\":59676},{\"end\":59701,\"start\":59688},{\"end\":59714,\"start\":59701},{\"end\":59724,\"start\":59714},{\"end\":59734,\"start\":59724},{\"end\":60326,\"start\":60313},{\"end\":60340,\"start\":60326},{\"end\":60650,\"start\":60640},{\"end\":60660,\"start\":60650},{\"end\":61134,\"start\":61124},{\"end\":61144,\"start\":61134},{\"end\":61164,\"start\":61144},{\"end\":61175,\"start\":61164},{\"end\":61500,\"start\":61491},{\"end\":61510,\"start\":61500},{\"end\":61520,\"start\":61510},{\"end\":61528,\"start\":61520},{\"end\":62112,\"start\":62099},{\"end\":62121,\"start\":62112},{\"end\":62134,\"start\":62121},{\"end\":62147,\"start\":62134},{\"end\":62469,\"start\":62457},{\"end\":62479,\"start\":62469},{\"end\":62490,\"start\":62479},{\"end\":62499,\"start\":62490},{\"end\":62509,\"start\":62499},{\"end\":63003,\"start\":62993},{\"end\":63015,\"start\":63003},{\"end\":63025,\"start\":63015},{\"end\":63495,\"start\":63485},{\"end\":63505,\"start\":63495},{\"end\":63517,\"start\":63505},{\"end\":63527,\"start\":63517},{\"end\":63537,\"start\":63527},{\"end\":63874,\"start\":63858},{\"end\":63891,\"start\":63874},{\"end\":64226,\"start\":64211},{\"end\":64759,\"start\":64749},{\"end\":64769,\"start\":64759},{\"end\":64780,\"start\":64769},{\"end\":65260,\"start\":65250},{\"end\":65271,\"start\":65260},{\"end\":65281,\"start\":65271},{\"end\":65521,\"start\":65511},{\"end\":65752,\"start\":65742},{\"end\":65765,\"start\":65752},{\"end\":65778,\"start\":65765},{\"end\":65997,\"start\":65984},{\"end\":66204,\"start\":66193},{\"end\":66218,\"start\":66204},{\"end\":66226,\"start\":66218},{\"end\":66437,\"start\":66426},{\"end\":66450,\"start\":66437},{\"end\":66654,\"start\":66641},{\"end\":66666,\"start\":66654},{\"end\":66685,\"start\":66666},{\"end\":66700,\"start\":66685},{\"end\":67093,\"start\":67087},{\"end\":67632,\"start\":67626},{\"end\":67640,\"start\":67632},{\"end\":67649,\"start\":67640},{\"end\":67661,\"start\":67649},{\"end\":68257,\"start\":68249},{\"end\":68677,\"start\":68666},{\"end\":69102,\"start\":69093},{\"end\":69115,\"start\":69102},{\"end\":69126,\"start\":69115},{\"end\":69482,\"start\":69474},{\"end\":69646,\"start\":69633},{\"end\":69660,\"start\":69646},{\"end\":69888,\"start\":69875},{\"end\":70074,\"start\":70061},{\"end\":70339,\"start\":70331},{\"end\":70553,\"start\":70540},{\"end\":70713,\"start\":70703},{\"end\":71008,\"start\":70999},{\"end\":71023,\"start\":71008},{\"end\":71030,\"start\":71023},{\"end\":71043,\"start\":71030},{\"end\":71284,\"start\":71276},{\"end\":71295,\"start\":71284},{\"end\":71307,\"start\":71295},{\"end\":71320,\"start\":71307},{\"end\":71335,\"start\":71320},{\"end\":71561,\"start\":71551},{\"end\":71765,\"start\":71752},{\"end\":72005,\"start\":71996},{\"end\":72301,\"start\":72289},{\"end\":72314,\"start\":72301},{\"end\":72328,\"start\":72314},{\"end\":72340,\"start\":72328},{\"end\":72353,\"start\":72340},{\"end\":72624,\"start\":72614},{\"end\":72628,\"start\":72624},{\"end\":72824,\"start\":72815},{\"end\":72834,\"start\":72824},{\"end\":72845,\"start\":72834},{\"end\":72852,\"start\":72845},{\"end\":73086,\"start\":73077},{\"end\":73094,\"start\":73086},{\"end\":73302,\"start\":73292},{\"end\":73316,\"start\":73302},{\"end\":73532,\"start\":73523},{\"end\":73927,\"start\":73913},{\"end\":74264,\"start\":74254},{\"end\":74275,\"start\":74264},{\"end\":74287,\"start\":74275},{\"end\":74779,\"start\":74766},{\"end\":74790,\"start\":74779},{\"end\":74801,\"start\":74790},{\"end\":75246,\"start\":75237},{\"end\":75465,\"start\":75450},{\"end\":75669,\"start\":75659},{\"end\":75680,\"start\":75669},{\"end\":75691,\"start\":75680},{\"end\":75701,\"start\":75691},{\"end\":75713,\"start\":75701},{\"end\":76221,\"start\":76209},{\"end\":76233,\"start\":76221},{\"end\":76243,\"start\":76233}]", "bib_venue": "[{\"end\":58892,\"start\":58841},{\"end\":59891,\"start\":59821},{\"end\":60827,\"start\":60752},{\"end\":61723,\"start\":61634},{\"end\":62632,\"start\":62579},{\"end\":63164,\"start\":63103},{\"end\":64365,\"start\":64304},{\"end\":64925,\"start\":64861},{\"end\":67204,\"start\":67157},{\"end\":67833,\"start\":67762},{\"end\":68388,\"start\":68331},{\"end\":68826,\"start\":68760},{\"end\":69243,\"start\":69193},{\"end\":70575,\"start\":70568},{\"end\":73633,\"start\":73591},{\"end\":74008,\"start\":73976},{\"end\":74464,\"start\":74384},{\"end\":74962,\"start\":74890},{\"end\":75289,\"start\":75276},{\"end\":75866,\"start\":75798},{\"end\":76444,\"start\":76352},{\"end\":58892,\"start\":58841},{\"end\":59891,\"start\":59821},{\"end\":60827,\"start\":60752},{\"end\":61723,\"start\":61634},{\"end\":62632,\"start\":62579},{\"end\":63164,\"start\":63103},{\"end\":64365,\"start\":64304},{\"end\":64925,\"start\":64861},{\"end\":67204,\"start\":67157},{\"end\":67833,\"start\":67762},{\"end\":68388,\"start\":68331},{\"end\":68826,\"start\":68760},{\"end\":69243,\"start\":69193},{\"end\":70575,\"start\":70568},{\"end\":73633,\"start\":73591},{\"end\":74008,\"start\":73976},{\"end\":74464,\"start\":74384},{\"end\":74962,\"start\":74890},{\"end\":75289,\"start\":75276},{\"end\":75866,\"start\":75798},{\"end\":76444,\"start\":76352},{\"end\":58284,\"start\":58259},{\"end\":58501,\"start\":58489},{\"end\":58839,\"start\":58773},{\"end\":59330,\"start\":59320},{\"end\":59819,\"start\":59734},{\"end\":60376,\"start\":60340},{\"end\":60750,\"start\":60660},{\"end\":61186,\"start\":61175},{\"end\":61632,\"start\":61528},{\"end\":62157,\"start\":62147},{\"end\":62577,\"start\":62509},{\"end\":63101,\"start\":63025},{\"end\":63571,\"start\":63537},{\"end\":63909,\"start\":63891},{\"end\":64302,\"start\":64226},{\"end\":64859,\"start\":64780},{\"end\":65248,\"start\":65183},{\"end\":65564,\"start\":65521},{\"end\":65794,\"start\":65778},{\"end\":66008,\"start\":65997},{\"end\":66239,\"start\":66226},{\"end\":66463,\"start\":66450},{\"end\":66741,\"start\":66725},{\"end\":67155,\"start\":67093},{\"end\":67760,\"start\":67674},{\"end\":68329,\"start\":68257},{\"end\":68758,\"start\":68677},{\"end\":69191,\"start\":69126},{\"end\":69495,\"start\":69482},{\"end\":69676,\"start\":69660},{\"end\":69894,\"start\":69888},{\"end\":70117,\"start\":70074},{\"end\":70359,\"start\":70339},{\"end\":70566,\"start\":70553},{\"end\":70759,\"start\":70713},{\"end\":71052,\"start\":71043},{\"end\":71351,\"start\":71335},{\"end\":71572,\"start\":71561},{\"end\":71778,\"start\":71765},{\"end\":72042,\"start\":72031},{\"end\":72373,\"start\":72353},{\"end\":72641,\"start\":72628},{\"end\":72859,\"start\":72852},{\"end\":73109,\"start\":73094},{\"end\":73332,\"start\":73316},{\"end\":73589,\"start\":73532},{\"end\":73974,\"start\":73927},{\"end\":74382,\"start\":74287},{\"end\":74888,\"start\":74801},{\"end\":75274,\"start\":75246},{\"end\":75448,\"start\":75430},{\"end\":75796,\"start\":75713},{\"end\":76350,\"start\":76243},{\"end\":58284,\"start\":58259},{\"end\":58501,\"start\":58489},{\"end\":58839,\"start\":58773},{\"end\":59330,\"start\":59320},{\"end\":59819,\"start\":59734},{\"end\":60376,\"start\":60340},{\"end\":60750,\"start\":60660},{\"end\":61186,\"start\":61175},{\"end\":61632,\"start\":61528},{\"end\":62157,\"start\":62147},{\"end\":62577,\"start\":62509},{\"end\":63101,\"start\":63025},{\"end\":63571,\"start\":63537},{\"end\":63909,\"start\":63891},{\"end\":64302,\"start\":64226},{\"end\":64859,\"start\":64780},{\"end\":65248,\"start\":65183},{\"end\":65564,\"start\":65521},{\"end\":65794,\"start\":65778},{\"end\":66008,\"start\":65997},{\"end\":66239,\"start\":66226},{\"end\":66463,\"start\":66450},{\"end\":66741,\"start\":66725},{\"end\":67155,\"start\":67093},{\"end\":67760,\"start\":67674},{\"end\":68329,\"start\":68257},{\"end\":68758,\"start\":68677},{\"end\":69191,\"start\":69126},{\"end\":69495,\"start\":69482},{\"end\":69676,\"start\":69660},{\"end\":69894,\"start\":69888},{\"end\":70117,\"start\":70074},{\"end\":70359,\"start\":70339},{\"end\":70566,\"start\":70553},{\"end\":70759,\"start\":70713},{\"end\":71052,\"start\":71043},{\"end\":71351,\"start\":71335},{\"end\":71572,\"start\":71561},{\"end\":71778,\"start\":71765},{\"end\":72042,\"start\":72031},{\"end\":72373,\"start\":72353},{\"end\":72641,\"start\":72628},{\"end\":72859,\"start\":72852},{\"end\":73109,\"start\":73094},{\"end\":73332,\"start\":73316},{\"end\":73589,\"start\":73532},{\"end\":73974,\"start\":73927},{\"end\":74382,\"start\":74287},{\"end\":74888,\"start\":74801},{\"end\":75274,\"start\":75246},{\"end\":75448,\"start\":75430},{\"end\":75796,\"start\":75713},{\"end\":76350,\"start\":76243}]"}}}, "year": 2023, "month": 12, "day": 17}