{"id": 232352874, "updated": "2023-10-06 05:22:35.101", "metadata": {"title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", "authors": "[{\"first\":\"Ze\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yutong\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Yue\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Han\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Yixuan\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Zheng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Stephen\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Baining\",\"last\":\"Guo\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 3, "day": 25}, "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.14030", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/LiuL00W0LG21", "doi": "10.1109/iccv48922.2021.00986"}}, "content": {"source": {"pdf_hash": "c8b25fab5608c3e033d34b4483ec47e68ba109b7", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.14030v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "353c905031189ceb0b3b1f0ccf83dafa828aca50", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c8b25fab5608c3e033d34b4483ec47e68ba109b7.txt", "contents": "\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows\n\n\nZe Liu \nMicrosoft Research Asia\n\n\nYutong Lin \nMicrosoft Research Asia\n\n\nYue Cao yuecao@microsoft.com \nMicrosoft Research Asia\n\n\nHan Hu hanhu@microsoft.com \nMicrosoft Research Asia\n\n\nYixuan Wei \nMicrosoft Research Asia\n\n\nZheng Zhang \nMicrosoft Research Asia\n\n\nStephen Lin \nMicrosoft Research Asia\n\n\nBaining Guo \nMicrosoft Research Asia\n\n\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows\n\nThis paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO testdev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-theart by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github. com/microsoft/Swin-Transformer.\n\nIntroduction\n\nModeling in computer vision has long been dominated by convolutional neural networks (CNNs). Beginning with AlexNet [39] and its revolutionary performance on the ImageNet image classification challenge, CNN architectures have evolved to become increasingly powerful through * Equal contribution. \u2020 Interns at MSRA. \u2021 Contact person. Figure 1. (a) The proposed Swin Transformer builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. (b) In contrast, previous vision Transformers [20] produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of selfattention globally. greater scale [30,76], more extensive connections [34], and more sophisticated forms of convolution [70,18,84]. With CNNs serving as backbone networks for a variety of vision tasks, these architectural advances have led to performance improvements that have broadly lifted the entire field.\n\nOn the other hand, the evolution of network architectures in natural language processing (NLP) has taken a different path, where the prevalent architecture today is instead the Transformer [64]. Designed for sequence modeling and transduction tasks, the Transformer is notable for its use of attention to model long-range dependencies in the data. Its tremendous success in the language domain has led researchers to investigate its adaptation to computer vision, where it has recently demonstrated promising results on certain tasks, specifically image classification [20] and joint vision-language modeling [47].\n\nIn this paper, we seek to expand the applicability of Transformer such that it can serve as a general-purpose backbone for computer vision, as it does for NLP and as CNNs do in vision. We observe that significant challenges in transferring its high performance in the language domain to the visual domain can be explained by differences between the two modalities. One of these differences involves scale. Unlike the word tokens that serve as the basic elements of processing in language Transformers, visual elements can vary substantially in scale, a problem that receives attention in tasks such as object detection [42,53,54]. In existing Transformer-based models [64,20], tokens are all of a fixed scale, a property unsuitable for these vision applications. Another difference is the much higher resolution of pixels in images compared to words in passages of text. There exist many vision tasks such as semantic segmentation that require dense prediction at the pixel level, and this would be intractable for Transformer on high-resolution images, as the computational complexity of its self-attention is quadratic to image size. To overcome these issues, we propose a generalpurpose Transformer backbone, called Swin Transformer, which constructs hierarchical feature maps and has linear computational complexity to image size. As illustrated in Figure 1(a), Swin Transformer constructs a hierarchical representation by starting from small-sized patches (outlined in gray) and gradually merging neighboring patches in deeper Transformer layers. With these hierarchical feature maps, the Swin Transformer model can conveniently leverage advanced techniques for dense prediction such as feature pyramid networks (FPN) [42] or U-Net [51]. The linear computational complexity is achieved by computing self-attention locally within non-overlapping windows that partition an image (outlined in red). The number of patches in each window is fixed, and thus the complexity becomes linear to image size. These merits make Swin Transformer suitable as a general-purpose backbone for various vision tasks, in contrast to previous Transformer based architectures [20] which produce feature maps of a single resolution and have quadratic complexity.\n\nA key design element of Swin Transformer is its shift of the window partition between consecutive self-attention layers, as illustrated in Figure 2. The shifted windows bridge the windows of the preceding layer, providing connections among them that significantly enhance modeling power (see Table 4). This strategy is also efficient in regards to real-world latency: all query patches within a window share the same key set 1 , which facilitates memory access in hardware. In contrast, earlier sliding window based self-attention approaches [33,50] suffer from low latency on general hardware due to different key sets for different query pixels 2 . Our experiments show that the proposed Figure 2. An illustration of the shifted window approach for computing self-attention in the proposed Swin Transformer architecture. In layer l (left), a regular window partitioning scheme is adopted, and self-attention is computed within each window. In the next layer l + 1 (right), the window partitioning is shifted, resulting in new windows. The self-attention computation in the new windows crosses the boundaries of the previous windows in layer l, providing connections among them.\n\nshifted window approach has much lower latency than the sliding window method, yet is similar in modeling power (see Tables 5 and 6). The shifted window approach also proves beneficial for all-MLP architectures [61].\n\nThe proposed Swin Transformer achieves strong performance on the recognition tasks of image classification, object detection and semantic segmentation. It outperforms the ViT / DeiT [20,63] and ResNe(X)t models [30,70] significantly with similar latency on the three tasks. Its 58.7 box AP and 51.1 mask AP on the COCO test-dev set surpass the previous state-of-the-art results by +2.7 box AP (Copy-paste [26] without external data) and +2.6 mask AP (DetectoRS [46]). On ADE20K semantic segmentation, it obtains 53.5 mIoU on the val set, an improvement of +3.2 mIoU over the previous state-of-the-art (SETR [81]). It also achieves a top-1 accuracy of 87.3% on ImageNet-1K image classification.\n\nIt is our belief that a unified architecture across computer vision and natural language processing could benefit both fields, since it would facilitate joint modeling of visual and textual signals and the modeling knowledge from both domains can be more deeply shared. We hope that Swin Transformer's strong performance on various vision problems can drive this belief deeper in the community and encourage unified modeling of vision and language signals.\n\n\nRelated Work\n\nCNN and variants CNNs serve as the standard network model throughout computer vision. While the CNN has existed for several decades [40], it was not until the introduction of AlexNet [39] that the CNN took off and became mainstream. Since then, deeper and more effective convolutional neural architectures have been proposed to further propel the deep learning wave in computer vision, e.g., VGG [52], GoogleNet [57], ResNet [30], DenseNet [34], weights across a feature map, it is difficult for a sliding-window based self-attention layer to have efficient memory access in practice.\n\nHRNet [65], and EfficientNet [58]. In addition to these architectural advances, there has also been much work on improving individual convolution layers, such as depthwise convolution [70] and deformable convolution [18,84]. While the CNN and its variants are still the primary backbone architectures for computer vision applications, we highlight the strong potential of Transformer-like architectures for unified modeling between vision and language. Our work achieves strong performance on several basic visual recognition tasks, and we hope it will contribute to a modeling shift.\n\nSelf-attention based backbone architectures Also inspired by the success of self-attention layers and Transformer architectures in the NLP field, some works employ self-attention layers to replace some or all of the spatial convolution layers in the popular ResNet [33,50,80]. In these works, the self-attention is computed within a local window of each pixel to expedite optimization [33], and they achieve slightly better accuracy/FLOPs trade-offs than the counterpart ResNet architecture. However, their costly memory access causes their actual latency to be significantly larger than that of the convolutional networks [33]. Instead of using sliding windows, we propose to shift windows between consecutive layers, which allows for a more efficient implementation in general hardware.\n\nSelf-attention/Transformers to complement CNNs Another line of work is to augment a standard CNN architecture with self-attention layers or Transformers. The selfattention layers can complement backbones [67,7,3,71,23,74,55] or head networks [32,27] by providing the capability to encode distant dependencies or heterogeneous interactions. More recently, the encoder-decoder design in Transformer has been applied for the object detection and instance segmentation tasks [8,13,85,56]. Our work explores the adaptation of Transformers for basic visual feature extraction and is complementary to these works.\n\nTransformer based vision backbones Most related to our work is the Vision Transformer (ViT) [20] and its follow-ups [63,72,15,28,66]. The pioneering work of ViT directly applies a Transformer architecture on nonoverlapping medium-sized image patches for image classification. It achieves an impressive speed-accuracy tradeoff on image classification compared to convolutional networks. While ViT requires large-scale training datasets (i.e., JFT-300M) to perform well, DeiT [63] introduces several training strategies that allow ViT to also be effective using the smaller ImageNet-1K dataset. The results of ViT on image classification are encouraging, but its architecture is unsuitable for use as a general-purpose backbone network on dense vision tasks or when the input image resolution is high, due to its low-resolution feature maps and the quadratic increase in complexity with image size. There are a few works applying ViT models to the dense vision tasks of object detection and semantic segmentation by direct upsampling or deconvolution but with relatively lower performance [2,81]. Concurrent to our work are some that modify the ViT architecture [72,15,28] for better image classification. Empirically, we find our Swin Transformer architecture to achieve the best speedaccuracy trade-off among these methods on image classification, even though our work focuses on general-purpose performance rather than specifically on classification. Another concurrent work [66] explores a similar line of thinking to build multi-resolution feature maps on Transformers. Its complexity is still quadratic to image size, while ours is linear and also operates locally which has proven beneficial in modeling the high correlation in visual signals [36,25,41]. Our approach is both efficient and effective, achieving state-of-the-art accuracy on both COCO object detection and ADE20K semantic segmentation.\n\n\nMethod\n\n\nOverall Architecture\n\nAn overview of the Swin Transformer architecture is presented in Figure 3, which illustrates the tiny version (Swin-T). It first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT. Each patch is treated as a \"token\" and its feature is set as a concatenation of the raw pixel RGB values. In our implementation, we use a patch size of 4 \u00d7 4 and thus the feature dimension of each patch is 4 \u00d7 4 \u00d7 3 = 48. A linear embedding layer is applied on this raw-valued feature to project it to an arbitrary dimension (denoted as C). Several Transformer blocks with modified self-attention computation (Swin Transformer blocks) are applied on these patch tokens. The Transformer blocks maintain the number of tokens ( H 4 \u00d7 W 4 ), and together with the linear embedding are referred to as \"Stage 1\".\n\nTo produce a hierarchical representation, the number of tokens is reduced by patch merging layers as the network gets deeper. The first patch merging layer concatenates the features of each group of 2 \u00d7 2 neighboring patches, and applies a linear layer on the 4C-dimensional concatenated features. This reduces the number of tokens by a multiple of 2 \u00d7 2 = 4 (2\u00d7 downsampling of resolution), and the output dimension is set to 2C. Swin Transformer blocks are applied afterwards for feature transformation, with the resolution kept at H 8 \u00d7 W 8 . This first block of patch merging and feature transformation is denoted as \"Stage 2\". The procedure is repeated twice, as \"Stage 3\" and \"Stage 4\", with output resolutions of H 16 \u00d7 W 16 and H 32 \u00d7 W 32 , respectively. These stages jointly produce a hierarchical representation,   with the same feature map resolutions as those of typical convolutional networks, e.g., VGG [52] and ResNet [30]. As a result, the proposed architecture can conveniently replace the backbone networks in existing methods for various vision tasks.\n\nSwin Transformer block Swin Transformer is built by replacing the standard multi-head self attention (MSA) module in a Transformer block by a module based on shifted windows (described in Section 3.2), with other layers kept the same. As illustrated in Figure 3(b), a Swin Transformer block consists of a shifted window based MSA module, followed by a 2-layer MLP with GELU nonlinearity in between. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module.\n\n\nShifted Window based Self-Attention\n\nThe standard Transformer architecture [64] and its adaptation for image classification [20] both conduct global selfattention, where the relationships between a token and all other tokens are computed. The global computation leads to quadratic complexity with respect to the number of tokens, making it unsuitable for many vision problems requiring an immense set of tokens for dense prediction or to represent a high-resolution image.\n\nSelf-attention in non-overlapped windows For efficient modeling, we propose to compute self-attention within local windows. The windows are arranged to evenly partition the image in a non-overlapping manner. Supposing each window contains M \u00d7 M patches, the computational complexity of a global MSA module and a window based one on an image of h \u00d7 w patches are 3 :\n\u2126(MSA) = 4hwC 2 + 2(hw) 2 C,(1)\u2126(W-MSA) = 4hwC 2 + 2M 2 hwC,(2)\nwhere the former is quadratic to patch number hw, and the latter is linear when M is fixed (set to 7 by default). Global self-attention computation is generally unaffordable for a large hw, while the window based self-attention is scalable.\n\n\nShifted window partitioning in successive blocks\n\nThe window-based self-attention module lacks connections across windows, which limits its modeling power. To introduce cross-window connections while maintaining the efficient computation of non-overlapping windows, we propose a shifted window partitioning approach which alternates between two partitioning configurations in consecutive Swin Transformer blocks. As illustrated in Figure 2, the first module uses a regular window partitioning strategy which starts from the top-left pixel, and the 8 \u00d7 8 feature map is evenly partitioned into 2 \u00d7 2 windows of size 4 \u00d7 4 (M = 4). Then, the next module adopts a windowing configuration that is shifted from that of the preceding layer, by displacing the windows by ( M 2 , M 2 ) pixels from the regularly partitioned windows. With the shifted window partitioning approach, consecutive Swin Transformer blocks are computed a\u015d\nz l = W-MSA LN z l\u22121 + z l\u22121 , z l = MLP LN \u1e91 l +\u1e91 l , z l+1 = SW-MSA LN z l + z l , z l+1 = MLP LN \u1e91 l+1 +\u1e91 l+1 ,(3)\nwhere\u1e91 l and z l denote the output features of the (S)W-MSA module and the MLP module for block l, respectively; 3 We omit SoftMax computation in determining complexity. W-MSA and SW-MSA denote window based multi-head self-attention using regular and shifted window partitioning configurations, respectively. The shifted window partitioning approach introduces connections between neighboring non-overlapping windows in the previous layer and is found to be effective in image classification, object detection, and semantic segmentation, as shown in Table 4.\n\n\nEfficient batch computation for shifted configuration\n\nAn issue with shifted window partitioning is that it will result in more windows,\nfrom h M \u00d7 w M to ( h M + 1) \u00d7 ( w M +1)\nin the shifted configuration, and some of the windows will be smaller than M \u00d7 M 4 . A naive solution is to pad the smaller windows to a size of M \u00d7 M and mask out the padded values when computing attention. When the number of windows in regular partitioning is small, e.g. 2 \u00d7 2, the increased computation with this naive solution is considerable (2 \u00d7 2 \u2192 3 \u00d7 3, which is 2.25 times greater). Here, we propose a more efficient batch computation approach by cyclic-shifting toward the top-left direction, as illustrated in Figure 4. After this shift, a batched window may be composed of several sub-windows that are not adjacent in the feature map, so a masking mechanism is employed to limit self-attention computation to within each sub-window. With the cyclic-shift, the number of batched windows remains the same as that of regular window partitioning, and thus is also efficient. The low latency of this approach is shown in Table 5.\n\n\nRelative position bias\n\nIn computing self-attention, we follow [49,1,32,33] by including a relative position bias B \u2208 R M 2 \u00d7M 2 to each head in computing similarity:\nAttention(Q, K, V ) = SoftMax(QK T / \u221a d + B)V,(4)\nwhere Q, K, V \u2208 R M 2 \u00d7d are the query, key and value matrices; d is the query/key dimension, and M 2 is the number of patches in a window. Since the relative position along each axis lies in the range [\u2212M + 1, M \u2212 1], we parameterize a smaller-sized bias matrixB \u2208 R (2M \u22121)\u00d7(2M \u22121) , and values in B are taken fromB.\n\nWe observe significant improvements over counterparts without this bias term or that use absolute position embedding, as shown in Table 4. Further adding absolute position embedding to the input as in [20] drops performance slightly, thus it is not adopted in our implementation.\n\nThe learnt relative position bias in pre-training can be also used to initialize a model for fine-tuning with a different window size through bi-cubic interpolation [20,63].\n\n\nArchitecture Variants\n\nWe build our base model, where C is the channel number of the hidden layers in the first stage. The model size, theoretical computational complexity (FLOPs), and throughput of the model variants for ImageNet image classification are listed in Table 1.\n\n\nExperiments\n\nWe conduct experiments on ImageNet-1K image classification [19], COCO object detection [43], and ADE20K semantic segmentation [83]. In the following, we first compare the proposed Swin Transformer architecture with the previous state-of-the-arts on the three tasks. Then, we ablate the important design elements of Swin Transformer.\n\n\nImage Classification on ImageNet-1K\n\nSettings For image classification, we benchmark the proposed Swin Transformer on ImageNet-1K [19], which contains 1.28M training images and 50K validation images from 1,000 classes. The top-1 accuracy on a single crop is reported. We consider two training settings:\n\n\u2022 Regular ImageNet-1K training. This setting mostly follows [63]. We employ an AdamW [37] optimizer for 300 epochs using a cosine decay learning rate scheduler and 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, and a weight decay of 0.05 are used. We include most of the augmentation and regularization strategies of [63] in training, except for repeated augmentation [31] and EMA [45], which do not enhance performance. Note that this is contrary to [63] where repeated augmentation is crucial to stabilize the training of ViT.\n\n\u2022 Pre-training on ImageNet-22K and fine-tuning on ImageNet-1K. We also pre-train on the larger ImageNet-22K dataset, which contains 14.2 million images and 22K classes. We employ an AdamW optimizer for 90 epochs using a linear decay learning rate scheduler with a 5-epoch linear warm-up. A batch size of 4096, an initial learning rate of 0.001, and a weight decay of 0.01 are used. In ImageNet-1K fine-tuning, we train the models for 30 epochs with a batch size of 1024, a constant learning rate of 10 \u22125 , and a weight decay of 10 \u22128 .\n\nResults with regular ImageNet-1K training Compared with the state-of-the-art ConvNets, i.e. Reg-Net [48] and EfficientNet [58], the Swin Transformer achieves a slightly better speed-accuracy trade-off. Noting that while RegNet [48] and EfficientNet [58] are obtained via a thorough architecture search, the proposed Swin Transformer is adapted from the standard Transformer and has strong potential for further improvement.\n\nResults with ImageNet-22K pre-training We also pretrain the larger-capacity Swin-B and Swin-L on ImageNet-22K. Results fine-tuned on ImageNet-1K image classification are shown in Table 1(b). For Swin-B, the ImageNet-22K pre-training brings 1.8%\u223c1.9% gains over training on ImageNet-1K from scratch. Compared with the previous best results for ImageNet-22K pre-training, our models achieve significantly better speed-accuracy trade-offs: Swin-B obtains 86.4% top-1 accuracy, which is 2.4% higher than that of ViT with similar inference throughput (84.7 vs. 85.9 images/sec) and slightly lower FLOPs (47.0G vs. 55.4G). The larger Swin-L model achieves 87.3% top-1 accuracy, +0.9% better than that of the Swin-B model. \n\n\nObject Detection on COCO\n\nSettings Object detection and instance segmentation experiments are conducted on COCO 2017, which contains 118K training, 5K validation and 20K test-dev images. An ablation study is performed using the validation set, and a system-level comparison is reported on test-dev. For the ablation study, we consider four typical object detection frameworks: Cascade Mask R-CNN [29,6], ATSS [79], RepPoints v2 [12], and Sparse RCNN [56] in mmdetection [10]. For these four frameworks, we utilize the same settings: multi-scale training [8,56] (resizing the input such that the shorter side is between 480 and 800 while the longer side is at most 1333), AdamW [44] optimizer (initial learning rate of 0.0001, weight decay of 0.05, and batch size of 16), and 3x schedule (36 epochs). For system-level comparison, we adopt an improved HTC [9] (denoted as HTC++) with instaboost [22], stronger multi-scale training [7], 6x schedule (72 epochs), soft-NMS [5], and ImageNet-22K pre-trained model as initialization.\n\nWe compare our Swin Transformer to standard Con-   Table 2. Results on COCO object detection and instance segmentation. \u2020 denotes that additional decovolution layers are used to produce hierarchical feature maps. * indicates multi-scale testing. vNets, i.e. ResNe(X)t, and previous Transformer networks, e.g. DeiT. The comparisons are conducted by changing only the backbones with other settings unchanged. Note that while Swin Transformer and ResNe(X)t are directly applicable to all the above frameworks because of their hierarchical feature maps, DeiT only produces a single resolution of feature maps and cannot be directly applied. For fair comparison, we follow [81] to construct hierarchical feature maps for DeiT using deconvolution layers.\n\nComparison to ResNe(X)t Table 2(a) lists the results of Swin-T and ResNet-50 on the four object detection frameworks. Our Swin-T architecture brings consistent +3.4\u223c4.2 box AP gains over ResNet-50, with slightly larger model size, FLOPs and latency. under different model capacity using Cascade Mask R-CNN. Swin Transformer achieves a high detection accuracy of 51.9 box AP and 45.0 mask AP, which are significant gains of +3.6 box AP and +3.3 mask AP over ResNeXt101-64x4d, which has similar model size, FLOPs and latency. On a higher baseline of 52.3 box AP and 46.0 mask AP using an improved HTC framework, the gains by Swin Transformer are also high, at +4.1 box AP and +3.1 mask AP (see Table 2(c)). Regarding inference speed, while ResNe(X)t is built by highly optimized Cudnn functions, our architecture is implemented with built-in PyTorch functions that are not all well-optimized. A thorough kernel optimization is beyond the scope of this paper.\n\n\nComparison to DeiT\n\nThe performance of DeiT-S using the Cascade Mask R-CNN framework is shown in Table 2(b). The results of Swin-T are +2.5 box AP and +2.3 mask AP higher than DeiT-S with similar model size (86M vs. 80M) and significantly higher inference speed (15.3 FPS vs. 10.4 FPS). The lower inference speed of DeiT is mainly due to its quadratic complexity to input image size.\n\nComparison to previous state-of-the-art Table 2(c) compares our best results with those of previous state-ofthe-art models. Our best model achieves 58.7 box AP and 51.1 mask AP on COCO test-dev, surpassing the previous best results by +2.7 box AP (Copy-paste [26] without external data) and +2.6 mask AP (DetectoRS [46]).\n\n\nSemantic Segmentation on ADE20K\n\nSettings ADE20K [83] is a widely-used semantic segmentation dataset, covering a broad range of 150 semantic  Table 4. Ablation study on the shifted windows approach and different position embedding methods on three benchmarks, using the Swin-T architecture. w/o shifting: all self-attention modules adopt regular window partitioning, without shifting; abs. pos.: absolute position embedding term of ViT; rel. pos.: the default settings with an additional relative position bias term (see Eq. (4)); app.: the first scaled dot-product term in Eq. (4).\n\ncategories. It has 25K images in total, with 20K for training, 2K for validation, and another 3K for testing. We utilize UperNet [69] in mmseg [16] as our base framework for its high efficiency. More details are presented in the Appendix.\n\nResults Table 3 lists the mIoU, model size (#param), FLOPs and FPS for different method/backbone pairs. From these results, it can be seen that Swin-S is +5.3 mIoU higher (49.3 vs. 44.0) than DeiT-S with similar computation cost. It is also +4.4 mIoU higher than ResNet-101, and +2.4 mIoU higher than ResNeSt-101 [78]. Our Swin-L model with ImageNet-22K pre-training achieves 53.5 mIoU on the val set, surpassing the previous best model by +3.2 mIoU (50.3 mIoU by SETR [81] which has a larger model size).\n\n\nAblation Study\n\nIn this section, we ablate important design elements in the proposed Swin Transformer, using ImageNet-1K image classification, Cascade Mask R-CNN on COCO object detection, and UperNet on ADE20K semantic segmentation.\n\nShifted windows Ablations of the shifted window approach on the three tasks are reported in Table 4. Swin-T with the shifted window partitioning outperforms the counterpart built on a single window partitioning at each stage by +1.1% top-1 accuracy on ImageNet-1K, +2.8 box AP/+2.2 mask AP on COCO, and +2.8 mIoU on ADE20K. The results indicate the effectiveness of using shifted windows to build connections among windows in the preceding layers. The latency overhead by shifted window is also small, as shown in Table 5.\n\nRelative position bias Table 4 shows comparisons of different position embedding approaches. Swin-T with relative position bias yields +1.2%/+0.8% top-1 accuracy on ImageNet-1K, +1.3/+1.5 box AP and +1.1/+1.3 mask AP method MSA in a stage (ms) Arch. (FPS) S1 S2 S3 S4 T S B sliding window (naive) 122. 5  on COCO, and +2.3/+2.9 mIoU on ADE20K in relation to those without position encoding and with absolute position embedding, respectively, indicating the effectiveness of the relative position bias. Also note that while the inclusion of absolute position embedding improves image classification accuracy (+0.4%), it harms object detection and semantic segmentation (-0.2 box/mask AP on COCO and -0.6 mIoU on ADE20K).\n\nWhile the recent ViT/DeiT models abandon translation invariance in image classification even though it has long been shown to be crucial for visual modeling, we find that inductive bias that encourages certain translation invariance is still preferable for general-purpose visual modeling, particularly for the dense prediction tasks of object detection and semantic segmentation.\n\nDifferent self-attention methods The real speed of different self-attention computation methods and implementations are compared in Table 5. Our cyclic implementation is more hardware efficient than naive padding, particularly for deeper stages. Overall, it brings a 13%, 18% and 18% speed-up on Swin-T, Swin-S and Swin-B, respectively.\n\nThe self-attention modules built on the proposed shifted window approach are 40.8\u00d7/2.5\u00d7, 20.2\u00d7/2.5\u00d7, 9.3\u00d7/2.1\u00d7, and 7.6\u00d7/1.8\u00d7 more efficient than those of sliding windows in naive/kernel implementations on four network stages, respectively. Overall, the Swin Transformer architectures built on shifted windows are 4.1/1.5, 4.0/1.5, 3.6/1.5 times faster than variants built on sliding windows for Swin-T, Swin-S, and Swin-B, respectively. Table 6 compares their accuracy on the three tasks, showing that they are similarly accurate in visual modeling.\n\nCompared to Performer [14], which is one of the fastest Transformer architectures (see [60]), the proposed shifted window based self-attention computation and the overall Swin Transformer architectures are slightly faster (see Table 5), while achieving +2.3% top-1 accuracy compared to Performer on ImageNet-1K using Swin-T (see Table 6).\n\n\nConclusion\n\nThis paper presents Swin Transformer, a new vision Transformer which produces a hierarchical feature repre-  Table 6. Accuracy of Swin Transformer using different methods for self-attention computation on three benchmarks. sentation and has linear computational complexity with respect to input image size. Swin Transformer achieves the state-of-the-art performance on COCO object detection and ADE20K semantic segmentation, significantly surpassing previous best methods. We hope that Swin Transformer's strong performance on various vision problems will encourage unified modeling of vision and language signals.\n\nAs a key element of Swin Transformer, the shifted window based self-attention is shown to be effective and efficient on vision problems, and we look forward to investigating its use in natural language processing as well.\n\n\nAcknowledgement\n\nWe thank many colleagues at Microsoft for their help, in particular, Li Dong and Furu Wei for useful discussions; Bin Xiao, Lu Yuan and Lei Zhang for help on datasets.\n\n\nA1. Detailed Architectures\n\nThe detailed architecture specifications are shown in Table 7, where an input image size of 224\u00d7224 is assumed for all architectures. \"Concat n \u00d7 n\" indicates a concatenation of n \u00d7 n neighboring features in a patch. This operation results in a downsampling of the feature map by a rate of n. \"96-d\" denotes a linear layer with an output dimension of 96. \"win. sz. 7 \u00d7 7\" indicates a multi-head self-attention module with window size of 7 \u00d7 7.\n\n\nA2. Detailed Experimental Settings\n\n\nA2.1. Image classification on ImageNet-1K\n\nThe image classification is performed by applying a global average pooling layer on the output feature map of the last stage, followed by a linear classifier. We find this strategy to be as accurate as using an additional class token as in ViT [20] and DeiT [63]. In evaluation, the top-1 accuracy using a single crop is reported.\n\nRegular ImageNet-1K training The training settings mostly follow [63]. For all model variants, we adopt a default input image resolution of 224 2 . For other resolutions such as 384 2 , we fine-tune the models trained at 224 2 resolution, instead of training from scratch, to reduce GPU consumption.\n\nWhen training from scratch with a 224 2 input, we employ an AdamW [37] optimizer for 300 epochs using a cosine decay learning rate scheduler with 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of 0.001, a weight decay of 0.05, and gradient clipping with a max norm of 1 are used. We include most of the augmentation and regularization strategies of [63] in training, including RandAugment [17], Mixup [77], Cutmix [75], random erasing [82] and stochastic depth [35], but not repeated augmentation [31] and Exponential Moving Average (EMA) [45] which do not enhance performance. Note that this is contrary to [63] where repeated augmentation is crucial to stabilize the training of ViT. An increasing degree of stochastic depth augmentation is employed for larger models, i.e. 0.2, 0.3, 0.5 for Swin-T, Swin-S, and Swin-B, respectively.\n\nFor fine-tuning on input with larger resolution, we employ an adamW [37] optimizer for 30 epochs with a constant learning rate of 10 \u22125 , weight decay of 10 \u22128 , and the same data augmentation and regularizations as the first stage except for setting the stochastic depth ratio to 0.1.\n\nImageNet-22K pre-training We also pre-train on the larger ImageNet-22K dataset, which contains 14.2 million images and 22K classes. The training is done in two stages. For the first stage with 224 2 input, we employ an AdamW optimizer for 90 epochs using a linear decay learning rate scheduler with a 5-epoch linear warm-up. A batch size of 4096, an initial learning rate of 0.001, and a weight decay of 0.01 are used. In the second stage of ImageNet-1K finetuning with 224 2 /384 2 input, we train the models for 30 epochs with a batch size of 1024, a constant learning rate of 10 \u22125 , and a weight decay of 10 \u22128 .\n\n\nA2.2. Object detection on COCO\n\nFor an ablation study, we consider four typical object detection frameworks: Cascade Mask R-CNN [29,6], ATSS [79], RepPoints v2 [12], and Sparse RCNN [56] in mmdetection [10]. For these four frameworks, we utilize the same settings: multi-scale training [8,56] (resizing the input such that the shorter side is between 480 and 800 while the longer side is at most 1333), AdamW [44] optimizer (initial learning rate of 0.0001, weight decay of 0.05, and batch size of 16), and 3x schedule (36 epochs with the learning rate decayed by 10\u00d7 at epochs 27 and 33).\n\nFor system-level comparison, we adopt an improved HTC [9] (denoted as HTC++) with instaboost [22], stronger multi-scale training [7] (resizing the input such that the shorter side is between 400 and 1400 while the longer side is at most 1600), 6x schedule (72 epochs with the learning rate decayed at epochs 63 and 69 by a factor of 0.1), soft-NMS [5], and an extra global self-attention layer appended at the output of last stage and ImageNet-22K pre-trained \n\n\nA2.3. Semantic segmentation on ADE20K\n\nADE20K [83] is a widely-used semantic segmentation dataset, covering a broad range of 150 semantic categories. It has 25K images in total, with 20K for training, 2K for validation, and another 3K for testing. We utilize UperNet [69] in mmsegmentation [16] as our base framework for its high efficiency.\n\nIn training, we employ the AdamW [44] optimizer with an initial learning rate of 6 \u00d7 10 \u22125 , a weight decay of 0.01, a scheduler that uses linear learning rate decay, and a linear warmup of 1,500 iterations. Models are trained on 8 GPUs with 2 images per GPU for 160K iterations. For augmentations, we adopt the default setting in mmsegmentation of random horizontal flipping, random re-scaling within ratio range [0.5, 2.0] and random photometric distortion. Stochastic depth with ratio of 0.2 is applied for all Swin Transformer models. Swin-T, Swin-S are trained on the standard setting as the previous approaches with an input of 512\u00d7512. Swin-B and Swin-L with \u2021 indicate that these two models are pre-trained on ImageNet-22K, and trained with the input of 640\u00d7640.\n\nIn inference, a multi-scale test using resolutions that are [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]\u00d7 of that in training is employed. When reporting test scores, both the training images and validation images are used for training, following common practice [71].\n\nA3. More Experiments A3.1. Image classification with different input size Table 8 lists the performance of Swin Transformers with different input image sizes from 224 2 to 384 2 . In general, a larger input resolution leads to better top-1 accuracy but with slower inference speed.  Table 9. Comparison of the SGD and AdamW optimizers for ResNe(X)t backbones on COCO object detection using the Cascade Mask R-CNN framework. Table 9 compares the AdamW and SGD optimizers of the ResNe(X)t backbones on COCO object detection. The Cascade Mask R-CNN framework is used in this comparison. While SGD is used as a default optimizer for Cascade Mask R-CNN framework, we generally observe improved accuracy by replacing it with an AdamW optimizer, particularly for smaller backbones. We thus use AdamW for ResNe(X)t backbones when compared to the proposed Swin Transformer architectures.\n\n\nA3.2. Different Optimizers for ResNe(X)t on COCO\n\n\nA3.3. Swin MLP-Mixer\n\nWe apply the proposed hierarchical design and the shifted window approach to the MLP-Mixer architectures [61], referred to as Swin-Mixer.  Table 10. Performance of Swin MLP-Mixer on ImageNet-1K classification. D indictes the number of channels per head. Throughput is measured using the GitHub repository of [68] and a V100 GPU, following [63]. proach, ResMLP [61]. Swin-Mixer performs significantly better than MLP-Mixer (81.3% vs. 76.4%) using slightly smaller computation budget (10.4G vs. 12.7G). It also has better speed accuracy trade-off compared to ResMLP [62]. These results indicate the proposed hierarchical design and the shifted window approach are generalizable.\n\nFigure 3 .\n3(a) The architecture of a Swin Transformer (Swin-T); (b) two successive Swin Transformer Blocks (notation presented with Eq.(3)). W-MSA and SW-MSA are multi-head self attention modules with regular and shifted windowing configurations, respectively.\n\nFigure 4 .\n4Illustration of an efficient batch computation approach for self-attention in shifted window partitioning.\n\n\ncalled Swin-B, to have of model size and computation complexity similar to ViT-B/DeiT-B. We also introduce Swin-T, Swin-S and Swin-L, which are versions of about 0.25\u00d7, 0.5\u00d7 and 2\u00d7 the model size and computational complexity, respectively. Note that the complexity of Swin-T and Swin-S are similar to those of ResNet-50 (DeiT-S) and ResNet-101, respectively. The window size is set to M = 7 by default. The query dimension of each head is d = 32, and the expansion layer of each MLP is \u03b1 = 4, for all experiments. The architecture hyper-parameters of these model variants are: \u2022 Swin-T: C = 96, layer numbers = {2, 2, 6, 2} \u2022 Swin-S: C = 96, layer numbers ={2, 2, 18, 2} \u2022 Swin-B: C = 128, layer numbers ={2, 2, 18, 2} \u2022 Swin-L: C = 192, layer numbers ={2, 2, 18, 2}\n\nTable 1 (\n1Compared to the previous state-of-the-art Transformerbased architecture, i.e. DeiT[63], Swin Transformers noticeably surpass the counterpart DeiT architectures with similar complexities: +1.5% for Swin-T (81.3%) over DeiT-S (79.8%) using 224 2 input, and +1.5%/1.4% for Swin-B (83.3%/84.5%) over DeiT-B (81.8%/83.1%) using 224 2 /384 2 input, respectively.a) \n\n\n( a )\naRegular ImageNet-1K trained models Comparison of different backbones on ImageNet-1K classification. Throughput is measured using the GitHub repository of[68] and a V100 GPU, following[63].method \nimage \nsize \n#param. FLOPs \nthroughput \n(image / s) \n\nImageNet \ntop-1 acc. \nRegNetY-4G [48] 224 2 21M 4.0G \n1156.7 \n80.0 \nRegNetY-8G [48] 224 2 39M 8.0G \n591.6 \n81.7 \nRegNetY-16G [48] 224 2 84M 16.0G \n334.7 \n82.9 \nEffNet-B3 [58] 300 2 12M 1.8G \n732.1 \n81.6 \nEffNet-B4 [58] 380 2 19M 4.2G \n349.4 \n82.9 \nEffNet-B5 [58] 456 2 30M 9.9G \n169.1 \n83.6 \nEffNet-B6 [58] 528 2 43M 19.0G \n96.9 \n84.0 \nEffNet-B7 [58] 600 2 66M 37.0G \n55.1 \n84.3 \nViT-B/16 [20] 384 2 86M 55.4G \n85.9 \n77.9 \nViT-L/16 [20] \n384 2 307M 190.7G \n27.3 \n76.5 \nDeiT-S [63] \n224 2 22M 4.6G \n940.4 \n79.8 \nDeiT-B [63] \n224 2 86M 17.5G \n292.3 \n81.8 \nDeiT-B [63] \n384 2 86M 55.4G \n85.9 \n83.1 \nSwin-T \n224 2 29M 4.5G \n755.2 \n81.3 \nSwin-S \n224 2 50M 8.7G \n436.9 \n83.0 \nSwin-B \n224 2 88M 15.4G \n278.1 \n83.5 \nSwin-B \n384 2 88M 47.0G \n84.7 \n84.5 \n(b) ImageNet-22K pre-trained models \n\nmethod \nimage \nsize \n#param. FLOPs \nthroughput \n(image / s) \n\nImageNet \ntop-1 acc. \nR-101x3 [38] \n384 2 388M 204.6G \n-\n84.4 \nR-152x4 [38] \n480 2 937M 840.5G \n-\n85.4 \nViT-B/16 [20] 384 2 86M 55.4G \n85.9 \n84.0 \nViT-L/16 [20] \n384 2 307M 190.7G \n27.3 \n85.2 \nSwin-B \n224 2 88M 15.4G \n278.1 \n85.2 \nSwin-B \n384 2 88M 47.0G \n84.7 \n86.4 \nSwin-L \n384 2 197M 103.9G \n42.1 \n87.3 \nTable 1. \n\n\nResults of semantic segmentation on the ADE20K val and test set. \u2020 indicates additional deconvolution layers are used to produce hierarchical feature maps. \u2021 indicates that the model is pre-trained on ImageNet-22K.Table 2(b) compares Swin Transformer and ResNe(X)t \n\nADE20K \nval test #param. FLOPs FPS \nMethod \nBackbone mIoU score \nDANet [23] ResNet-101 45.2 \n-\n69M 1119G 15.2 \nDLab.v3+ [11] ResNet-101 44.1 \n-\n63M 1021G 16.0 \nACNet [24] ResNet-101 45.9 38.5 \n-\nDNL [71] \nResNet-101 46.0 56.2 69M 1249G 14.8 \nOCRNet [73] ResNet-101 45.3 56.0 56M 923G 19.3 \nUperNet [69] ResNet-101 44.9 \n-\n86M 1029G 20.1 \nOCRNet [73] HRNet-w48 45.7 \n-\n71M 664G 12.5 \nDLab.v3+ [11] ResNeSt-101 46.9 55.1 66M 1051G 11.9 \nDLab.v3+ [11] ResNeSt-200 48.4 \n-\n88M 1381G 8.1 \nSETR [81] \nT-Large  \u2021 \n50.3 61.7 308M \n-\n-\nUperNet \nDeiT-S  \u2020 \n44.0 \n-\n52M 1099G 16.2 \nUperNet \nSwin-T \n46.1 \n-\n60M 945G 18.5 \nUperNet \nSwin-S \n49.3 \n-\n81M 1038G 15.2 \nUperNet \nSwin-B  \u2021 \n51.6 \n-\n121M 1841G 8.7 \nUperNet \nSwin-L  \u2021 \n53.5 62.8 234M 3230G 6.2 \nTable 3. \n\n\nconcat 2\u00d72, 192-d , LN concat 2\u00d72, 192-d , LN concat 2\u00d72, 256-d , LN concat 2\u00d72, 384-d , LN win. sz. 7\u00d77, Table 7. Detailed architecture specifications.model as initialization. We adopt stochastic depth with ratio of 0.2 for all Swin Transformer models.downsp. rate \n(output size) \nSwin-T \nSwin-S \nSwin-B \nSwin-L \n\nstage 1 \n4\u00d7 \n(56\u00d756) \n\nconcat 4\u00d74, 96-d, LN \nconcat 4\u00d74, 96-d, LN \nconcat 4\u00d74, 128-d, LN \nconcat 4\u00d74, 192-d, LN \nwin. sz. 7\u00d77, \ndim 96, head 3 \n\u00d7 2 \nwin. sz. 7\u00d77, \ndim 96, head 3 \n\u00d7 2 \nwin. sz. 7\u00d77, \ndim 128, head 4 \n\u00d7 2 \nwin. sz. 7\u00d77, \ndim 192, head 6 \n\u00d7 2 \n\nstage 2 \n8\u00d7 \n(28\u00d728) \n\ndim 192, head 6 \n\u00d7 2 \nwin. sz. 7\u00d77, \ndim 192, head 6 \n\u00d7 2 \nwin. sz. 7\u00d77, \ndim 256, head 8 \n\u00d7 2 \nwin. sz. 7\u00d77, \ndim 384, head 12 \n\u00d7 2 \n\nstage 3 \n16\u00d7 \n(14\u00d714) \n\nconcat 2\u00d72, 384-d , LN concat 2\u00d72, 384-d , LN \nconcat 2\u00d72, 512-d , LN \nconcat 2\u00d72, 768-d , LN \nwin. sz. 7\u00d77, \ndim 384, head 12 \n\u00d7 6 \nwin. sz. 7\u00d77, \ndim 384, head 12 \n\u00d7 18 \nwin. sz. 7\u00d77, \ndim 512, head 16 \n\u00d7 18 \nwin. sz. 7\u00d77, \ndim 768, head 24 \n\u00d7 18 \n\nstage 4 \n32\u00d7 \n(7\u00d77) \n\nconcat 2\u00d72, 768-d , LN concat 2\u00d72, 768-d , LN concat 2\u00d72, 1024-d , LN concat 2\u00d72, 1536-d , LN \nwin. sz. 7\u00d77, \ndim 768, head 24 \n\u00d7 2 \nwin. sz. 7\u00d77, \ndim 768, head 24 \n\u00d7 2 \nwin. sz. 7\u00d77, \ndim 1024, head 32 \n\u00d7 2 \nwin. sz. 7\u00d77, \ndim 1536, head 48 \n\u00d7 2 \n\n\n\n\nTable 8. Swin Transformers with different input image size on ImageNet-1K classification. Backbone Optimizer AP box AP box 50 AP box 75 AP mask AP mask AdamW 46.3 64.3 50.5 40.1 61.7 43.4Swin-T \nSwin-S \nSwin-B \ninput \nsize \n\ntop-1 \nacc \n\nthroughput \n(image / s) \n\ntop-1 \nacc \n\nthroughput \n(image / s) \n\ntop-1 \nacc \n\nthroughput \n(image / s) \n224 2 81.3 \n755.2 \n83.0 \n436.9 \n83.3 \n278.1 \n256 2 81.6 \n580.9 \n83.4 \n336.7 \n83.7 \n208.1 \n320 2 82.1 \n342.0 \n83.7 \n198.2 \n84.0 \n132.0 \n384 2 82.2 \n219.5 \n83.9 \n127.6 \n84.5 \n84.7 \n50 \n\nAP mask \n\n75 \n\nR50 \nSGD \n45.0 62.9 48.8 38.5 59.9 41.4 \nX101-32x4d \nSGD \n47.8 65.9 51.9 40.4 62.9 43.5 \nAdamW 48.1 66.5 52.4 41.6 63.9 45.2 \n\nX101-64x4d \nSGD \n48.8 66.9 53.0 41.4 63.9 44.7 \nAdamW 48.3 66.4 52.3 41.7 64.0 45.1 \n\n\n\nTable 10 shows the performance of Swin-Mixer compared to the original MLP-Mixer architectures MLP-Mixer [61] and a follow-up ap-ImageNet top-1 acc. MLP-Mixer-B/16 [61] 224 2 59M 12.7Gmethod \n\nimage \nsize \n#param. FLOPs \nthroughput \n(image / s) \n\n-\n76.4 \nResMLP-S24 [62] 224 2 30M 6.0G \n715 \n79.4 \nResMLP-B24 [62] 224 2 116M 23.0G \n231 \n81.0 \nSwin-T/D24 \n(Transformer) \n256 2 28M 5.9G \n563 \n81.6 \n\nSwin-Mixer-T/D24 256 2 20M 4.0G \n807 \n79.4 \nSwin-Mixer-T/D12 256 2 21M 4.0G \n792 \n79.6 \nSwin-Mixer-T/D6 256 2 23M 4.0G \n766 \n79.7 \nSwin-Mixer-B/D24 \n(no shift) \n224 2 61M 10.4G \n409 \n80.3 \n\nSwin-Mixer-B/D24 224 2 61M 10.4G \n409 \n81.3 \n\nThe query and key are projection vectors in a self-attention layer.2 While there are efficient methods to implement a sliding-window based convolution layer on general hardware, thanks to its shared kernel\nTo make the window size (M, M ) divisible by the feature map size of (h, w), bottom-right padding is employed on the feature map if needed.\n\nUnilmv2: Pseudo-masked language models for unified language model pre-training. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, PMLR, 2020. 5International Conference on Machine Learning. Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Songhao Piao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for unified language model pre-training. In International Con- ference on Machine Learning, pages 642-652. PMLR, 2020. 5\n\n. Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew Zhai, Dmitry Kislyuk, arXiv:2012.09958Toward transformer-based object detection. arXiv preprintJosh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew Zhai, and Dmitry Kislyuk. Toward transformer-based object detection. arXiv preprint arXiv:2012.09958, 2020. 3\n\nAttention augmented convolutional networks. Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc V Le, 2020Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. Attention augmented convolutional net- works, 2020. 3\n\nYolov4: Optimal speed and accuracy of object detection. Alexey Bochkovskiy, Chien-Yao Wang, Hong-Yuan Mark Liao, arXiv:2004.10934arXiv preprintAlexey Bochkovskiy, Chien-Yao Wang, and Hong- Yuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 7\n\nSoft-nms -improving object detection with one line of code. Navaneeth Bodla, Bharat Singh, Rama Chellappa, Larry S Davis, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)69Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S. Davis. Soft-nms -improving object detection with one line of code. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017. 6, 9\n\nCascade r-cnn: Delving into high quality object detection. Zhaowei Cai, Nuno Vasconcelos, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition69Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv- ing into high quality object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 6154-6162, 2018. 6, 9\n\nGcnet: Non-local networks meet squeeze-excitation networks and beyond. Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, Han Hu, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops. the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops79Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu. Gcnet: Non-local networks meet squeeze-excitation net- works and beyond. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision (ICCV) Workshops, Oct 2019. 3, 6, 7, 9\n\nEnd-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision. Springer69Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In European Confer- ence on Computer Vision, pages 213-229. Springer, 2020. 3, 6, 9\n\nHybrid task cascade for instance segmentation. Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition69Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox- iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974- 4983, 2019. 6, 9\n\nKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, arXiv:1906.07155Open mmlab detection toolbox and benchmark. 69arXiv preprintKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection tool- box and benchmark. arXiv preprint arXiv:1906.07155, 2019. 6, 9\n\nEncoder-decoder with atrous separable convolution for semantic image segmentation. Yukun Liang-Chieh Chen, George Zhu, Florian Papandreou, Hartwig Schroff, Adam, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801-818, 2018. 7\n\nReppoints v2: Verification meets regression for object detection. Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen Lin, Han Hu, NeurIPS, 2020. 6. 79Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen Lin, and Han Hu. Reppoints v2: Verification meets regres- sion for object detection. In NeurIPS, 2020. 6, 7, 9\n\nRelationnet++: Bridging visual representations for object detection via transformer decoder. Cheng Chi, Fangyun Wei, Han Hu, NeurIPS. 37Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++: Bridging visual representations for object detection via trans- former decoder. In NeurIPS, 2020. 3, 7\n\nRethinking attention with performers. Valerii Krzysztof Marcin Choromanski, David Likhosherstov, Xingyou Dohan, Andreea Song, Tamas Gane, Peter Sarlos, Jared Quincy Hawkins, Afroz Davis, Lukasz Mohiuddin, David Benjamin Kaiser, Lucy J Belanger, Adrian Colwell, Weller, International Conference on Learning Representations. 89Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sar- los, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. 8, 9\n\nDo we really need explicit position encodings for vision transformers?. Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, Huaxia Xia, arXiv:2102.10882arXiv preprintXiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit position encodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021. 3\n\nMMSegmentation: Openmmlab semantic segmentation toolbox and benchmark. 810MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox and bench- mark. https://github.com/open-mmlab/ mmsegmentation, 2020. 8, 10\n\nRandaugment: Practical automated data augmentation with a reduced search space. Barret Ekin D Cubuk, Jonathon Zoph, Quoc V Shlens, Le, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkshopsEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702-703, 2020. 9\n\nDeformable convolutional networks. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision13Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 764-773, 2017. 1, 3\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 5\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, International Conference on Learning Representations. 69Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representa- tions, 2021. 1, 2, 3, 4, 5, 6, 9\n\nSpinenet: Learning scale-permuted backbone for recognition and localization. Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, V Quoc, Xiaodan Le, Song, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi, Mingxing Tan, Yin Cui, Quoc V Le, and Xiaodan Song. Spinenet: Learning scale-permuted backbone for recogni- tion and localization. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 11592-11601, 2020. 7\n\nInstaboost: Boosting instance segmentation via probability map guided copypasting. Jianhua Hao-Shu Fang, Runzhong Sun, Minghao Wang, Yong-Lu Gou, Cewu Li, Lu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision69Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting instance segmentation via probability map guided copy- pasting. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pages 682-691, 2019. 6, 9\n\nDual attention network for scene segmentation. Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition37Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhi- wei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3146- 3154, 2019. 3, 7\n\nAdaptive context network for scene parsing. Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, Hanqing Lu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionJun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jin- hui Tang, and Hanqing Lu. Adaptive context network for scene parsing. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, pages 6748-6757, 2019. 7\n\nCognitron: A self-organizing multilayered neural network. Kunihiko Fukushima, Biological cybernetics. 203Kunihiko Fukushima. Cognitron: A self-organizing multi- layered neural network. Biological cybernetics, 20(3):121- 136, 1975. 3\n\nSimple copy-paste is a strong data augmentation method for instance segmentation. Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, D Ekin, Cubuk, V Quoc, Barret Le, Zoph, arXiv:2012.0717727arXiv preprintGolnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung- Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. arXiv preprint arXiv:2012.07177, 2020. 2, 7\n\nLearning region features for object detection. Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, Jifeng Dai, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, and Jifeng Dai. Learning region features for object detection. In Pro- ceedings of the European Conference on Computer Vision (ECCV), 2018. 3\n\nTransformer in transformer. Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang, arXiv:2103.00112arXiv preprintKai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. arXiv preprint arXiv:2103.00112, 2021. 3\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision69Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017. 6, 9\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 1, 2, 4\n\nAugment your batch: Improving generalization through instance repetition. Elad Hoffer, Itay Ben-Nun, Niv Hubara, Torsten Giladi, Daniel Hoefler, Soudry, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition69Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your batch: Improving generalization through instance repetition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8129-8138, 2020. 6, 9\n\nRelation networks for object detection. Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition35Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3588-3597, 2018. 3, 5\n\nLocal relation networks for image recognition. Han Hu, Zheng Zhang, Zhenda Xie, Stephen Lin, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)5Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3464-3473, October 2019. 2, 3, 5\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition1Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil- ian Q Weinberger. Densely connected convolutional net- works. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700-4708, 2017. 1, 2\n\nDeep networks with stochastic depth. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q Weinberger, European conference on computer vision. SpringerGao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil- ian Q Weinberger. Deep networks with stochastic depth. In European conference on computer vision, pages 646-661. Springer, 2016. 9\n\nReceptive fields, binocular interaction and functional architecture in the cat's visual cortex. H David, Hubel, N Torsten, Wiesel, The Journal of physiology. 1601David H Hubel and Torsten N Wiesel. Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. The Journal of physiology, 160(1):106-154, 1962. 3\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.698059arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5, 9\n\nBig transfer (bit): General visual representation learning. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby, arXiv:1912.113706arXiv preprintAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370, 6(2):8, 2019. 6\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. 1Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In Advances in neural information processing sys- tems, pages 1097-1105, 2012. 1, 2\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proceedings of the IEEE. 8611Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recog- nition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 2\n\nObject recognition with gradient-based learning. Yann Lecun, Patrick Haffner, L\u00e9on Bottou, Yoshua Bengio, Shape, contour and grouping in computer vision. SpringerYann LeCun, Patrick Haffner, L\u00e9on Bottou, and Yoshua Ben- gio. Object recognition with gradient-based learning. In Shape, contour and grouping in computer vision, pages 319- 345. Springer, 1999. 3\n\nFeature pyramid networks for object detection. Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 5\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 610Ilya Loshchilov and Frank Hutter. Decoupled weight de- cay regularization. In International Conference on Learning Representations, 2019. 6, 9, 10\n\nAcceleration of stochastic approximation by averaging. T Boris, Anatoli B Juditsky Polyak, SIAM journal on control and optimization. 3049Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838-855, 1992. 6, 9\n\nDetectors: Detecting objects with recursive feature pyramid and switchable atrous convolution. Siyuan Qiao, Liang-Chieh Chen, Alan Yuille, arXiv:2006.0233427arXiv preprintSiyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects with recursive feature pyramid and switch- able atrous convolution. arXiv preprint arXiv:2006.02334, 2020. 2, 7\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 1\n\nDesigning network design spaces. Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKaiming He, and Piotr Doll\u00e1rIlija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10428- 10436, 2020. 6\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learn- ing Research, 21(140):1-67, 2020. 5\n\nStand-alone selfattention in vision models. Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jon Shlens, Advances in Neural Information Processing Systems. Curran Associates, Inc323Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self- attention in vision models. In Advances in Neural Informa- tion Processing Systems, volume 32. Curran Associates, Inc., 2019. 2, 3\n\nUnet: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical image com- puting and computer-assisted intervention, pages 234-241. Springer, 2015. 2\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations. 24K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, May 2015. 2, 4\n\nAn analysis of scale invariance in object detection snip. Bharat Singh, S Larry, Davis, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionBharat Singh and Larry S Davis. An analysis of scale in- variance in object detection snip. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 3578-3587, 2018. 2\n\nSniper: Efficient multi-scale training. Bharat Singh, Mahyar Najibi, Larry S Davis, Advances in Neural Information Processing Systems. Curran Associates, Inc31Bharat Singh, Mahyar Najibi, and Larry S Davis. Sniper: Efficient multi-scale training. In Advances in Neural Infor- mation Processing Systems, volume 31. Curran Associates, Inc., 2018. 2\n\nAravind Srinivas, Tsung-Yi Lin, Niki Parmar, arXiv:2101.11605Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition. arXiv preprintAravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottle- neck transformers for visual recognition. arXiv preprint arXiv:2101.11605, 2021. 3\n\nSparse r-cnn: End-to-end object detection with learnable proposals. Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, arXiv:2011.12450,2020.369arXiv preprintPeize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen- feng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. arXiv preprint arXiv:2011.12450, 2020. 3, 6, 9\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1-9, 2015. 2\n\nEfficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, PMLRInternational Conference on Machine Learning. 36Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pages 6105-6114. PMLR, 2019. 3, 6\n\nEfficientdet: Scalable and efficient object detection. Mingxing Tan, Ruoming Pang, Quoc V Le, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionMingxing Tan, Ruoming Pang, and Quoc V Le. Efficientdet: Scalable and efficient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10781-10790, 2020. 7\n\nLong range arena : A benchmark for efficient transformers. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler, International Conference on Learning Representations. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A bench- mark for efficient transformers. In International Conference on Learning Representations, 2021. 8\n\nMlp-mixer: An all-mlp architecture for vision. Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy, 1011Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu- cas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp ar- chitecture for vision, 2021. 2, 10, 11\n\nResmlp: Feedforward networks for image classification with data-efficient training. Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, Herv\u00e9 J\u00e9gou, 11Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izac- ard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Herv\u00e9 J\u00e9gou. Resmlp: Feedforward networks for image clas- sification with data-efficient training, 2021. 11\n\nTraining data-efficient image transformers & distillation through attention. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv\u00e9 J\u00e9gou, arXiv:2012.12877911arXiv preprintHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through at- tention. arXiv preprint arXiv:2012.12877, 2020. 2, 3, 5, 6, 9, 11\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008, 2017. 1, 2, 4\n\nDeep high-resolution representation learning for visual recognition. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, 2020. 3IEEE transactions. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution represen- tation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 2020. 3\n\nPyramid vision transformer: A versatile backbone for dense prediction without convolutions. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao, arXiv:2102.12122arXiv preprintWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021. 3\n\nNon-local neural networks. Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He, IEEE Conference on Computer Vision and Pattern Recognition. Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim- ing He. Non-local neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, 2018. 3\n\nPytorch image models. Ross Wightman, 611Ross Wightman. Pytorch image mod- els. https://github.com/rwightman/ pytorch-image-models, 2019. 6, 11\n\nUnified perceptual parsing for scene understanding. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)810Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understand- ing. In Proceedings of the European Conference on Com- puter Vision (ECCV), pages 418-434, 2018. 7, 8, 10\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition13Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1492- 1500, 2017. 1, 2, 3\n\nDisentangled non-local neural networks. Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, Han Hu, Proceedings of the European conference on computer vision (ECCV), 2020. 3. the European conference on computer vision (ECCV), 2020. 3710Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled non-local neural networks. In Proceedings of the European conference on computer vision (ECCV), 2020. 3, 7, 10\n\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, E H Francis, Jiashi Tay, Shuicheng Feng, Yan, arXiv:2101.11986Tokensto-token vit: Training vision transformers from scratch on imagenet. arXiv preprintLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens- to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021. 3\n\nObjectcontextual representations for semantic segmentation. Yuhui Yuan, Xilin Chen, Jingdong Wang, 16th European Conference Computer Vision (ECCV 2020). Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object- contextual representations for semantic segmentation. In 16th European Conference Computer Vision (ECCV 2020), August 2020. 7\n\nOcnet: Object context network for scene parsing. Yuhui Yuan, Jingdong Wang, arXiv:1809.00916arXiv preprintYuhui Yuan and Jingdong Wang. Ocnet: Object context net- work for scene parsing. arXiv preprint arXiv:1809.00916, 2018. 3\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. Sangdoo Yun, Dongyoon Han, Sanghyuk Seong Joon Oh, Junsuk Chun, Youngjoon Choe, Yoo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular- ization strategy to train strong classifiers with localizable fea- tures. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision, pages 6023-6032, 2019. 9\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, BMVC. Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In BMVC, 2016. 1\n\nHongyi Zhang, Moustapha Cisse, David Yann N Dauphin, Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. arXiv preprintHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 9\n\nHang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, Manmatha, arXiv:2004.08955Split-attention networks. 7arXiv preprintHang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020. 7, 8\n\nBridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, Stan Z Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition69Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9759-9768, 2020. 6, 9\n\nExploring self-attention for image recognition. Hengshuang Zhao, Jiaya Jia, Vladlen Koltun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor- ing self-attention for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10076-10085, 2020. 3\n\nRethinking semantic segmentation from a sequence-to-sequence perspective with transformers. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, H S Philip, Torr, arXiv:2012.158407arXiv preprintSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmen- tation from a sequence-to-sequence perspective with trans- formers. arXiv preprint arXiv:2012.15840, 2020. 2, 3, 7, 8\n\nRandom erasing data augmentation. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13001-13008, 2020. 9\n\nSemantic understanding of scenes through the ade20k dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba, International Journal on Computer Vision. 7510Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi- dler, Adela Barriuso, and Antonio Torralba. Semantic under- standing of scenes through the ade20k dataset. International Journal on Computer Vision, 2018. 5, 7, 10\n\nDeformable convnets v2: More deformable, better results. Xizhou Zhu, Han Hu, Stephen Lin, Jifeng Dai, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition13Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De- formable convnets v2: More deformable, better results. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9308-9316, 2019. 1, 3\n\nDeformable {detr}: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, International Conference on Learning Representations. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable {detr}: Deformable transform- ers for end-to-end object detection. In International Confer- ence on Learning Representations, 2021. 3\n", "annotations": {"author": "[{\"end\":108,\"start\":75},{\"end\":146,\"start\":109},{\"end\":202,\"start\":147},{\"end\":256,\"start\":203},{\"end\":294,\"start\":257},{\"end\":333,\"start\":295},{\"end\":372,\"start\":334},{\"end\":411,\"start\":373}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":78},{\"end\":119,\"start\":116},{\"end\":154,\"start\":151},{\"end\":209,\"start\":207},{\"end\":267,\"start\":264},{\"end\":306,\"start\":301},{\"end\":345,\"start\":342},{\"end\":384,\"start\":381}]", "author_first_name": "[{\"end\":77,\"start\":75},{\"end\":115,\"start\":109},{\"end\":150,\"start\":147},{\"end\":206,\"start\":203},{\"end\":263,\"start\":257},{\"end\":300,\"start\":295},{\"end\":341,\"start\":334},{\"end\":380,\"start\":373}]", "author_affiliation": "[{\"end\":107,\"start\":83},{\"end\":145,\"start\":121},{\"end\":201,\"start\":177},{\"end\":255,\"start\":231},{\"end\":293,\"start\":269},{\"end\":332,\"start\":308},{\"end\":371,\"start\":347},{\"end\":410,\"start\":386}]", "title": "[{\"end\":72,\"start\":1},{\"end\":483,\"start\":412}]", "venue": null, "abstract": "[{\"end\":2032,\"start\":485}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2168,\"start\":2164},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2814,\"start\":2810},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2989,\"start\":2985},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":2992,\"start\":2989},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3025,\"start\":3021},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":3075,\"start\":3071},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3078,\"start\":3075},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":3081,\"start\":3078},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":3456,\"start\":3452},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3836,\"start\":3832},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3876,\"start\":3872},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4502,\"start\":4498},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4505,\"start\":4502},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":4508,\"start\":4505},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":4551,\"start\":4547},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4554,\"start\":4551},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5606,\"start\":5602},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5620,\"start\":5616},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6041,\"start\":6037},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6670,\"start\":6666},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6673,\"start\":6670},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7520,\"start\":7516},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7709,\"start\":7705},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7712,\"start\":7709},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7738,\"start\":7734},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":7741,\"start\":7738},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7932,\"start\":7928},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7988,\"start\":7984},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":8134,\"start\":8130},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8827,\"start\":8823},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8878,\"start\":8874},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9091,\"start\":9087},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9107,\"start\":9103},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9120,\"start\":9116},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9135,\"start\":9131},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":9287,\"start\":9283},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9310,\"start\":9306},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":9465,\"start\":9461},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9497,\"start\":9493},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":9500,\"start\":9497},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10132,\"start\":10128},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10135,\"start\":10132},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":10138,\"start\":10135},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10252,\"start\":10248},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10490,\"start\":10486},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10861,\"start\":10857},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10863,\"start\":10861},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10865,\"start\":10863},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":10868,\"start\":10865},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10871,\"start\":10868},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":10874,\"start\":10871},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10877,\"start\":10874},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10899,\"start\":10895},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10902,\"start\":10899},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11127,\"start\":11124},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11130,\"start\":11127},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":11133,\"start\":11130},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11136,\"start\":11133},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11357,\"start\":11353},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11381,\"start\":11377},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":11384,\"start\":11381},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11387,\"start\":11384},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11390,\"start\":11387},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":11393,\"start\":11390},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11739,\"start\":11735},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12351,\"start\":12348},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":12354,\"start\":12351},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":12425,\"start\":12421},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12428,\"start\":12425},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12431,\"start\":12428},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":12741,\"start\":12737},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13013,\"start\":13009},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13016,\"start\":13013},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13019,\"start\":13016},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":14953,\"start\":14949},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14969,\"start\":14965},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":15711,\"start\":15707},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15760,\"start\":15756},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16469,\"start\":16468},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17935,\"start\":17934},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19568,\"start\":19564},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19570,\"start\":19568},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19573,\"start\":19570},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19576,\"start\":19573},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20244,\"start\":20240},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20489,\"start\":20485},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":20492,\"start\":20489},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20849,\"start\":20845},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20877,\"start\":20873},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":20916,\"start\":20912},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21255,\"start\":21251},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":21489,\"start\":21485},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21514,\"start\":21510},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":21785,\"start\":21781},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21836,\"start\":21832},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21849,\"start\":21845},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":21919,\"start\":21915},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22636,\"start\":22632},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":22658,\"start\":22654},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22763,\"start\":22759},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":22785,\"start\":22781},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24076,\"start\":24072},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24078,\"start\":24076},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":24089,\"start\":24085},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24108,\"start\":24104},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":24130,\"start\":24126},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24150,\"start\":24146},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24233,\"start\":24230},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":24236,\"start\":24233},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24357,\"start\":24353},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24445,\"start\":24442},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24533,\"start\":24530},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24573,\"start\":24569},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24608,\"start\":24605},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24647,\"start\":24644},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":25376,\"start\":25372},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27061,\"start\":27057},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27117,\"start\":27113},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":27175,\"start\":27171},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":27839,\"start\":27835},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27853,\"start\":27849},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":28263,\"start\":28259},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":28419,\"start\":28415},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29515,\"start\":29514},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31231,\"start\":31227},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":31296,\"start\":31292},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33387,\"start\":33383},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":33401,\"start\":33397},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":33540,\"start\":33536},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33842,\"start\":33838},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":34154,\"start\":34150},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34194,\"start\":34190},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":34206,\"start\":34202},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":34219,\"start\":34215},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":34240,\"start\":34236},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34266,\"start\":34262},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":34302,\"start\":34298},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":34344,\"start\":34340},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":34413,\"start\":34409},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34710,\"start\":34706},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":35676,\"start\":35672},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35678,\"start\":35676},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":35689,\"start\":35685},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35708,\"start\":35704},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":35730,\"start\":35726},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35750,\"start\":35746},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35833,\"start\":35830},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":35836,\"start\":35833},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":35957,\"start\":35953},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36192,\"start\":36189},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36232,\"start\":36228},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36267,\"start\":36264},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36486,\"start\":36483},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":36648,\"start\":36644},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":36869,\"start\":36865},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36892,\"start\":36888},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36978,\"start\":36974},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":37969,\"start\":37965},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":39035,\"start\":39031},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":39238,\"start\":39234},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":39269,\"start\":39265},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":39290,\"start\":39286},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":39494,\"start\":39490},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":40853,\"start\":40849},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":41293,\"start\":41289},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":41323,\"start\":41319},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":46310,\"start\":46309}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":39865,\"start\":39603},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39985,\"start\":39866},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40754,\"start\":39986},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":41127,\"start\":40755},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":42547,\"start\":41128},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43568,\"start\":42548},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44851,\"start\":43569},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":45606,\"start\":44852},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":46241,\"start\":45607}]", "paragraph": "[{\"end\":3261,\"start\":2048},{\"end\":3877,\"start\":3263},{\"end\":6122,\"start\":3879},{\"end\":7303,\"start\":6124},{\"end\":7521,\"start\":7305},{\"end\":8216,\"start\":7523},{\"end\":8674,\"start\":8218},{\"end\":9275,\"start\":8691},{\"end\":9861,\"start\":9277},{\"end\":10651,\"start\":9863},{\"end\":11259,\"start\":10653},{\"end\":13166,\"start\":11261},{\"end\":14029,\"start\":13200},{\"end\":15102,\"start\":14031},{\"end\":15629,\"start\":15104},{\"end\":16104,\"start\":15669},{\"end\":16471,\"start\":16106},{\"end\":16776,\"start\":16536},{\"end\":17702,\"start\":16829},{\"end\":18379,\"start\":17821},{\"end\":18518,\"start\":18437},{\"end\":19498,\"start\":18560},{\"end\":19667,\"start\":19525},{\"end\":20037,\"start\":19719},{\"end\":20318,\"start\":20039},{\"end\":20493,\"start\":20320},{\"end\":20770,\"start\":20519},{\"end\":21118,\"start\":20786},{\"end\":21423,\"start\":21158},{\"end\":21992,\"start\":21425},{\"end\":22530,\"start\":21994},{\"end\":22955,\"start\":22532},{\"end\":23673,\"start\":22957},{\"end\":24702,\"start\":23702},{\"end\":25452,\"start\":24704},{\"end\":26410,\"start\":25454},{\"end\":26796,\"start\":26433},{\"end\":27119,\"start\":26798},{\"end\":27704,\"start\":27155},{\"end\":27944,\"start\":27706},{\"end\":28451,\"start\":27946},{\"end\":28686,\"start\":28470},{\"end\":29210,\"start\":28688},{\"end\":29931,\"start\":29212},{\"end\":30313,\"start\":29933},{\"end\":30651,\"start\":30315},{\"end\":31203,\"start\":30653},{\"end\":31543,\"start\":31205},{\"end\":32172,\"start\":31558},{\"end\":32395,\"start\":32174},{\"end\":32582,\"start\":32415},{\"end\":33056,\"start\":32613},{\"end\":33469,\"start\":33139},{\"end\":33770,\"start\":33471},{\"end\":34636,\"start\":33772},{\"end\":34923,\"start\":34638},{\"end\":35541,\"start\":34925},{\"end\":36133,\"start\":35576},{\"end\":36595,\"start\":36135},{\"end\":36939,\"start\":36637},{\"end\":37711,\"start\":36941},{\"end\":37970,\"start\":37713},{\"end\":38850,\"start\":37972},{\"end\":39602,\"start\":38926}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16503,\"start\":16472},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16535,\"start\":16503},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17820,\"start\":17703},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18559,\"start\":18519},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19718,\"start\":19668}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":6423,\"start\":6416},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":7436,\"start\":7422},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18378,\"start\":18371},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19497,\"start\":19490},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20176,\"start\":20169},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20769,\"start\":20762},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23143,\"start\":23136},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24762,\"start\":24755},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25485,\"start\":25478},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26153,\"start\":26146},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26845,\"start\":26838},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27271,\"start\":27264},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27961,\"start\":27954},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28787,\"start\":28780},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29209,\"start\":29202},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29242,\"start\":29235},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30454,\"start\":30447},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31098,\"start\":31091},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31439,\"start\":31432},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31541,\"start\":31534},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31674,\"start\":31667},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":32674,\"start\":32667},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":38053,\"start\":38046},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":38262,\"start\":38255},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":38403,\"start\":38396},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39073,\"start\":39065}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2046,\"start\":2034},{\"attributes\":{\"n\":\"2.\"},\"end\":8689,\"start\":8677},{\"attributes\":{\"n\":\"3.\"},\"end\":13175,\"start\":13169},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13198,\"start\":13178},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15667,\"start\":15632},{\"end\":16827,\"start\":16779},{\"end\":18435,\"start\":18382},{\"end\":19523,\"start\":19501},{\"attributes\":{\"n\":\"3.3.\"},\"end\":20517,\"start\":20496},{\"attributes\":{\"n\":\"4.\"},\"end\":20784,\"start\":20773},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21156,\"start\":21121},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23700,\"start\":23676},{\"end\":26431,\"start\":26413},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27153,\"start\":27122},{\"attributes\":{\"n\":\"4.4.\"},\"end\":28468,\"start\":28454},{\"attributes\":{\"n\":\"5.\"},\"end\":31556,\"start\":31546},{\"end\":32413,\"start\":32398},{\"end\":32611,\"start\":32585},{\"end\":33093,\"start\":33059},{\"end\":33137,\"start\":33096},{\"end\":35574,\"start\":35544},{\"end\":36635,\"start\":36598},{\"end\":38901,\"start\":38853},{\"end\":38924,\"start\":38904},{\"end\":39614,\"start\":39604},{\"end\":39877,\"start\":39867},{\"end\":40765,\"start\":40756},{\"end\":41134,\"start\":41129}]", "table": "[{\"end\":41127,\"start\":41123},{\"end\":42547,\"start\":41324},{\"end\":43568,\"start\":42764},{\"end\":44851,\"start\":43824},{\"end\":45606,\"start\":45041},{\"end\":46241,\"start\":45792}]", "figure_caption": "[{\"end\":39865,\"start\":39616},{\"end\":39985,\"start\":39879},{\"end\":40754,\"start\":39988},{\"end\":41123,\"start\":40767},{\"end\":41324,\"start\":41136},{\"end\":42764,\"start\":42550},{\"end\":43824,\"start\":43571},{\"end\":45041,\"start\":44854},{\"end\":45792,\"start\":45609}]", "figure_ref": "[{\"end\":2389,\"start\":2381},{\"end\":5240,\"start\":5232},{\"end\":6271,\"start\":6263},{\"end\":6822,\"start\":6814},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13273,\"start\":13265},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15365,\"start\":15357},{\"end\":17218,\"start\":17210},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19091,\"start\":19083}]", "bib_author_first_name": "[{\"end\":46675,\"start\":46669},{\"end\":46683,\"start\":46681},{\"end\":46694,\"start\":46690},{\"end\":46706,\"start\":46700},{\"end\":46716,\"start\":46713},{\"end\":46731,\"start\":46723},{\"end\":46739,\"start\":46737},{\"end\":46754,\"start\":46746},{\"end\":46767,\"start\":46760},{\"end\":46778,\"start\":46774},{\"end\":47134,\"start\":47130},{\"end\":47145,\"start\":47141},{\"end\":47155,\"start\":47151},{\"end\":47167,\"start\":47163},{\"end\":47171,\"start\":47168},{\"end\":47184,\"start\":47178},{\"end\":47197,\"start\":47191},{\"end\":47495,\"start\":47490},{\"end\":47509,\"start\":47503},{\"end\":47522,\"start\":47516},{\"end\":47540,\"start\":47532},{\"end\":47553,\"start\":47549},{\"end\":47555,\"start\":47554},{\"end\":47756,\"start\":47750},{\"end\":47779,\"start\":47770},{\"end\":47800,\"start\":47786},{\"end\":48066,\"start\":48057},{\"end\":48080,\"start\":48074},{\"end\":48092,\"start\":48088},{\"end\":48109,\"start\":48104},{\"end\":48111,\"start\":48110},{\"end\":48544,\"start\":48537},{\"end\":48554,\"start\":48550},{\"end\":48993,\"start\":48990},{\"end\":49005,\"start\":48999},{\"end\":49017,\"start\":49010},{\"end\":49030,\"start\":49023},{\"end\":49039,\"start\":49036},{\"end\":49510,\"start\":49503},{\"end\":49528,\"start\":49519},{\"end\":49543,\"start\":49536},{\"end\":49561,\"start\":49554},{\"end\":49580,\"start\":49571},{\"end\":49597,\"start\":49591},{\"end\":49952,\"start\":49949},{\"end\":49968,\"start\":49959},{\"end\":49980,\"start\":49975},{\"end\":49989,\"start\":49987},{\"end\":50005,\"start\":49997},{\"end\":50017,\"start\":50010},{\"end\":50029,\"start\":50023},{\"end\":50041,\"start\":50036},{\"end\":50055,\"start\":50047},{\"end\":50066,\"start\":50061},{\"end\":50527,\"start\":50524},{\"end\":50539,\"start\":50534},{\"end\":50555,\"start\":50546},{\"end\":50568,\"start\":50562},{\"end\":50576,\"start\":50574},{\"end\":50592,\"start\":50584},{\"end\":50604,\"start\":50597},{\"end\":50616,\"start\":50610},{\"end\":50628,\"start\":50623},{\"end\":50640,\"start\":50634},{\"end\":51041,\"start\":51036},{\"end\":51066,\"start\":51060},{\"end\":51079,\"start\":51072},{\"end\":51099,\"start\":51092},{\"end\":51561,\"start\":51555},{\"end\":51573,\"start\":51568},{\"end\":51584,\"start\":51581},{\"end\":51595,\"start\":51590},{\"end\":51609,\"start\":51602},{\"end\":51618,\"start\":51615},{\"end\":51908,\"start\":51903},{\"end\":51921,\"start\":51914},{\"end\":51930,\"start\":51927},{\"end\":52146,\"start\":52139},{\"end\":52182,\"start\":52177},{\"end\":52205,\"start\":52198},{\"end\":52220,\"start\":52213},{\"end\":52232,\"start\":52227},{\"end\":52244,\"start\":52239},{\"end\":52258,\"start\":52253},{\"end\":52265,\"start\":52259},{\"end\":52280,\"start\":52275},{\"end\":52294,\"start\":52288},{\"end\":52311,\"start\":52306},{\"end\":52320,\"start\":52312},{\"end\":52333,\"start\":52329},{\"end\":52335,\"start\":52334},{\"end\":52352,\"start\":52346},{\"end\":52852,\"start\":52842},{\"end\":52860,\"start\":52858},{\"end\":52871,\"start\":52868},{\"end\":52885,\"start\":52878},{\"end\":52897,\"start\":52891},{\"end\":53433,\"start\":53427},{\"end\":53456,\"start\":53448},{\"end\":53469,\"start\":53463},{\"end\":53953,\"start\":53947},{\"end\":53965,\"start\":53959},{\"end\":53975,\"start\":53970},{\"end\":53985,\"start\":53983},{\"end\":53997,\"start\":53990},{\"end\":54008,\"start\":54005},{\"end\":54019,\"start\":54013},{\"end\":54422,\"start\":54419},{\"end\":54432,\"start\":54429},{\"end\":54446,\"start\":54439},{\"end\":54461,\"start\":54455},{\"end\":54469,\"start\":54466},{\"end\":54476,\"start\":54474},{\"end\":54859,\"start\":54853},{\"end\":54878,\"start\":54873},{\"end\":54895,\"start\":54886},{\"end\":54912,\"start\":54908},{\"end\":54933,\"start\":54926},{\"end\":54946,\"start\":54940},{\"end\":54967,\"start\":54960},{\"end\":54986,\"start\":54978},{\"end\":55002,\"start\":54997},{\"end\":55019,\"start\":55012},{\"end\":55032,\"start\":55027},{\"end\":55048,\"start\":55044},{\"end\":55571,\"start\":55564},{\"end\":55584,\"start\":55576},{\"end\":55599,\"start\":55590},{\"end\":55611,\"start\":55605},{\"end\":55628,\"start\":55620},{\"end\":55637,\"start\":55634},{\"end\":55644,\"start\":55643},{\"end\":55658,\"start\":55651},{\"end\":56211,\"start\":56204},{\"end\":56234,\"start\":56226},{\"end\":56247,\"start\":56240},{\"end\":56261,\"start\":56254},{\"end\":56271,\"start\":56267},{\"end\":56731,\"start\":56728},{\"end\":56740,\"start\":56736},{\"end\":56752,\"start\":56746},{\"end\":56763,\"start\":56759},{\"end\":56775,\"start\":56768},{\"end\":56787,\"start\":56781},{\"end\":56801,\"start\":56794},{\"end\":57239,\"start\":57236},{\"end\":57248,\"start\":57244},{\"end\":57260,\"start\":57254},{\"end\":57271,\"start\":57267},{\"end\":57283,\"start\":57276},{\"end\":57295,\"start\":57289},{\"end\":57309,\"start\":57302},{\"end\":57741,\"start\":57733},{\"end\":57997,\"start\":57991},{\"end\":58009,\"start\":58006},{\"end\":58022,\"start\":58015},{\"end\":58036,\"start\":58033},{\"end\":58051,\"start\":58043},{\"end\":58058,\"start\":58057},{\"end\":58073,\"start\":58072},{\"end\":58086,\"start\":58080},{\"end\":58419,\"start\":58412},{\"end\":58427,\"start\":58424},{\"end\":58437,\"start\":58432},{\"end\":58450,\"start\":58444},{\"end\":58462,\"start\":58456},{\"end\":58801,\"start\":58798},{\"end\":58809,\"start\":58807},{\"end\":58821,\"start\":58816},{\"end\":58834,\"start\":58826},{\"end\":58848,\"start\":58840},{\"end\":58858,\"start\":58853},{\"end\":59088,\"start\":59081},{\"end\":59100,\"start\":59093},{\"end\":59465,\"start\":59458},{\"end\":59477,\"start\":59470},{\"end\":59493,\"start\":59485},{\"end\":59503,\"start\":59499},{\"end\":59943,\"start\":59939},{\"end\":59956,\"start\":59952},{\"end\":59969,\"start\":59966},{\"end\":59985,\"start\":59978},{\"end\":60000,\"start\":59994},{\"end\":60488,\"start\":60485},{\"end\":60500,\"start\":60493},{\"end\":60510,\"start\":60505},{\"end\":60524,\"start\":60518},{\"end\":60536,\"start\":60530},{\"end\":60949,\"start\":60946},{\"end\":60959,\"start\":60954},{\"end\":60973,\"start\":60967},{\"end\":60986,\"start\":60979},{\"end\":61401,\"start\":61398},{\"end\":61415,\"start\":61409},{\"end\":61428,\"start\":61421},{\"end\":61453,\"start\":61445},{\"end\":61877,\"start\":61874},{\"end\":61887,\"start\":61885},{\"end\":61899,\"start\":61893},{\"end\":61911,\"start\":61905},{\"end\":61927,\"start\":61919},{\"end\":62271,\"start\":62270},{\"end\":62287,\"start\":62286},{\"end\":62565,\"start\":62564},{\"end\":62581,\"start\":62576},{\"end\":62814,\"start\":62805},{\"end\":62832,\"start\":62827},{\"end\":62847,\"start\":62840},{\"end\":62858,\"start\":62854},{\"end\":62878,\"start\":62871},{\"end\":62892,\"start\":62885},{\"end\":62904,\"start\":62900},{\"end\":63237,\"start\":63233},{\"end\":63254,\"start\":63250},{\"end\":63274,\"start\":63266},{\"end\":63276,\"start\":63275},{\"end\":63606,\"start\":63602},{\"end\":63618,\"start\":63614},{\"end\":63633,\"start\":63627},{\"end\":63649,\"start\":63642},{\"end\":63916,\"start\":63912},{\"end\":63931,\"start\":63924},{\"end\":63945,\"start\":63941},{\"end\":63960,\"start\":63954},{\"end\":64278,\"start\":64270},{\"end\":64289,\"start\":64284},{\"end\":64302,\"start\":64298},{\"end\":64320,\"start\":64313},{\"end\":64332,\"start\":64325},{\"end\":64349,\"start\":64344},{\"end\":64711,\"start\":64703},{\"end\":64724,\"start\":64717},{\"end\":64737,\"start\":64732},{\"end\":64753,\"start\":64748},{\"end\":64766,\"start\":64760},{\"end\":64779,\"start\":64775},{\"end\":64794,\"start\":64789},{\"end\":64813,\"start\":64803},{\"end\":65158,\"start\":65154},{\"end\":65176,\"start\":65171},{\"end\":65446,\"start\":65445},{\"end\":65472,\"start\":65454},{\"end\":65791,\"start\":65785},{\"end\":65809,\"start\":65798},{\"end\":65820,\"start\":65816},{\"end\":66126,\"start\":66122},{\"end\":66140,\"start\":66136},{\"end\":66145,\"start\":66141},{\"end\":66156,\"start\":66151},{\"end\":66172,\"start\":66166},{\"end\":66188,\"start\":66181},{\"end\":66202,\"start\":66194},{\"end\":66218,\"start\":66212},{\"end\":66233,\"start\":66227},{\"end\":66248,\"start\":66242},{\"end\":66262,\"start\":66258},{\"end\":66278,\"start\":66270},{\"end\":66292,\"start\":66288},{\"end\":66608,\"start\":66603},{\"end\":66625,\"start\":66622},{\"end\":66633,\"start\":66626},{\"end\":66648,\"start\":66644},{\"end\":67158,\"start\":67153},{\"end\":67171,\"start\":67167},{\"end\":67185,\"start\":67181},{\"end\":67204,\"start\":67195},{\"end\":67216,\"start\":67210},{\"end\":67232,\"start\":67225},{\"end\":67246,\"start\":67241},{\"end\":67256,\"start\":67253},{\"end\":67266,\"start\":67261},{\"end\":67268,\"start\":67267},{\"end\":67639,\"start\":67633},{\"end\":67658,\"start\":67654},{\"end\":67673,\"start\":67667},{\"end\":67688,\"start\":67683},{\"end\":67702,\"start\":67696},{\"end\":67716,\"start\":67713},{\"end\":68115,\"start\":68111},{\"end\":68136,\"start\":68129},{\"end\":68152,\"start\":68146},{\"end\":68571,\"start\":68570},{\"end\":68583,\"start\":68582},{\"end\":68886,\"start\":68880},{\"end\":68895,\"start\":68894},{\"end\":69299,\"start\":69293},{\"end\":69313,\"start\":69307},{\"end\":69329,\"start\":69322},{\"end\":69608,\"start\":69601},{\"end\":69627,\"start\":69619},{\"end\":69637,\"start\":69633},{\"end\":70038,\"start\":70033},{\"end\":70050,\"start\":70044},{\"end\":70060,\"start\":70058},{\"end\":70071,\"start\":70068},{\"end\":70086,\"start\":70078},{\"end\":70094,\"start\":70091},{\"end\":70110,\"start\":70101},{\"end\":70124,\"start\":70121},{\"end\":70135,\"start\":70129},{\"end\":70149,\"start\":70142},{\"end\":70484,\"start\":70475},{\"end\":70497,\"start\":70494},{\"end\":70511,\"start\":70503},{\"end\":70523,\"start\":70517},{\"end\":70539,\"start\":70534},{\"end\":70554,\"start\":70546},{\"end\":70572,\"start\":70565},{\"end\":70587,\"start\":70580},{\"end\":70605,\"start\":70599},{\"end\":71122,\"start\":71114},{\"end\":71132,\"start\":71128},{\"end\":71436,\"start\":71428},{\"end\":71449,\"start\":71442},{\"end\":71462,\"start\":71456},{\"end\":71889,\"start\":71887},{\"end\":71902,\"start\":71895},{\"end\":71919,\"start\":71913},{\"end\":71933,\"start\":71927},{\"end\":71944,\"start\":71940},{\"end\":71958,\"start\":71952},{\"end\":71972,\"start\":71965},{\"end\":71981,\"start\":71978},{\"end\":71997,\"start\":71988},{\"end\":72011,\"start\":72005},{\"end\":72391,\"start\":72387},{\"end\":72408,\"start\":72404},{\"end\":72427,\"start\":72418},{\"end\":72445,\"start\":72440},{\"end\":72460,\"start\":72453},{\"end\":72473,\"start\":72467},{\"end\":72494,\"start\":72487},{\"end\":72508,\"start\":72501},{\"end\":72524,\"start\":72518},{\"end\":72539,\"start\":72534},{\"end\":72556,\"start\":72551},{\"end\":72570,\"start\":72564},{\"end\":72945,\"start\":72941},{\"end\":72960,\"start\":72955},{\"end\":72981,\"start\":72973},{\"end\":72997,\"start\":72989},{\"end\":73013,\"start\":73004},{\"end\":73031,\"start\":73024},{\"end\":73046,\"start\":73039},{\"end\":73062,\"start\":73056},{\"end\":73078,\"start\":73071},{\"end\":73094,\"start\":73089},{\"end\":73109,\"start\":73104},{\"end\":73478,\"start\":73474},{\"end\":73496,\"start\":73488},{\"end\":73511,\"start\":73503},{\"end\":73528,\"start\":73519},{\"end\":73545,\"start\":73536},{\"end\":73565,\"start\":73560},{\"end\":73879,\"start\":73873},{\"end\":73893,\"start\":73889},{\"end\":73907,\"start\":73903},{\"end\":73921,\"start\":73916},{\"end\":73938,\"start\":73933},{\"end\":73951,\"start\":73946},{\"end\":73953,\"start\":73952},{\"end\":73967,\"start\":73961},{\"end\":73981,\"start\":73976},{\"end\":74362,\"start\":74354},{\"end\":74371,\"start\":74369},{\"end\":74385,\"start\":74377},{\"end\":74398,\"start\":74393},{\"end\":74413,\"start\":74406},{\"end\":74424,\"start\":74420},{\"end\":74435,\"start\":74431},{\"end\":74447,\"start\":74441},{\"end\":74459,\"start\":74452},{\"end\":74473,\"start\":74465},{\"end\":74881,\"start\":74875},{\"end\":74892,\"start\":74888},{\"end\":74903,\"start\":74898},{\"end\":74917,\"start\":74908},{\"end\":74929,\"start\":74923},{\"end\":74940,\"start\":74936},{\"end\":74952,\"start\":74948},{\"end\":74961,\"start\":74957},{\"end\":74971,\"start\":74967},{\"end\":75284,\"start\":75276},{\"end\":75295,\"start\":75291},{\"end\":75313,\"start\":75306},{\"end\":75328,\"start\":75321},{\"end\":75592,\"start\":75588},{\"end\":75766,\"start\":75762},{\"end\":75782,\"start\":75773},{\"end\":75793,\"start\":75788},{\"end\":75806,\"start\":75800},{\"end\":75818,\"start\":75814},{\"end\":76233,\"start\":76226},{\"end\":76243,\"start\":76239},{\"end\":76259,\"start\":76254},{\"end\":76275,\"start\":76268},{\"end\":76287,\"start\":76280},{\"end\":76729,\"start\":76722},{\"end\":76743,\"start\":76735},{\"end\":76752,\"start\":76749},{\"end\":76761,\"start\":76758},{\"end\":76771,\"start\":76766},{\"end\":76786,\"start\":76779},{\"end\":76795,\"start\":76792},{\"end\":77145,\"start\":77143},{\"end\":77159,\"start\":77152},{\"end\":77169,\"start\":77166},{\"end\":77182,\"start\":77176},{\"end\":77192,\"start\":77187},{\"end\":77199,\"start\":77198},{\"end\":77201,\"start\":77200},{\"end\":77217,\"start\":77211},{\"end\":77232,\"start\":77223},{\"end\":77636,\"start\":77631},{\"end\":77648,\"start\":77643},{\"end\":77663,\"start\":77655},{\"end\":77956,\"start\":77951},{\"end\":77971,\"start\":77963},{\"end\":78225,\"start\":78218},{\"end\":78239,\"start\":78231},{\"end\":78253,\"start\":78245},{\"end\":78275,\"start\":78269},{\"end\":78291,\"start\":78282},{\"end\":78746,\"start\":78740},{\"end\":78763,\"start\":78758},{\"end\":78869,\"start\":78863},{\"end\":78886,\"start\":78877},{\"end\":78899,\"start\":78894},{\"end\":79159,\"start\":79155},{\"end\":79175,\"start\":79167},{\"end\":79188,\"start\":79180},{\"end\":79198,\"start\":79196},{\"end\":79207,\"start\":79204},{\"end\":79221,\"start\":79215},{\"end\":79230,\"start\":79227},{\"end\":79240,\"start\":79236},{\"end\":79250,\"start\":79245},{\"end\":79642,\"start\":79635},{\"end\":79655,\"start\":79650},{\"end\":79670,\"start\":79661},{\"end\":79680,\"start\":79676},{\"end\":79690,\"start\":79686},{\"end\":79692,\"start\":79691},{\"end\":80191,\"start\":80181},{\"end\":80203,\"start\":80198},{\"end\":80216,\"start\":80209},{\"end\":80684,\"start\":80678},{\"end\":80699,\"start\":80692},{\"end\":80714,\"start\":80704},{\"end\":80728,\"start\":80721},{\"end\":80739,\"start\":80734},{\"end\":80751,\"start\":80745},{\"end\":80764,\"start\":80758},{\"end\":80777,\"start\":80769},{\"end\":80787,\"start\":80784},{\"end\":80796,\"start\":80795},{\"end\":80798,\"start\":80797},{\"end\":81169,\"start\":81165},{\"end\":81182,\"start\":81177},{\"end\":81198,\"start\":81190},{\"end\":81211,\"start\":81205},{\"end\":81218,\"start\":81216},{\"end\":81605,\"start\":81600},{\"end\":81616,\"start\":81612},{\"end\":81629,\"start\":81623},{\"end\":81640,\"start\":81636},{\"end\":81652,\"start\":81647},{\"end\":81666,\"start\":81661},{\"end\":81684,\"start\":81677},{\"end\":82026,\"start\":82020},{\"end\":82035,\"start\":82032},{\"end\":82047,\"start\":82040},{\"end\":82059,\"start\":82053},{\"end\":82509,\"start\":82503},{\"end\":82521,\"start\":82515},{\"end\":82531,\"start\":82526},{\"end\":82539,\"start\":82536},{\"end\":82552,\"start\":82544},{\"end\":82565,\"start\":82559}]", "bib_author_last_name": "[{\"end\":46679,\"start\":46676},{\"end\":46688,\"start\":46684},{\"end\":46698,\"start\":46695},{\"end\":46711,\"start\":46707},{\"end\":46721,\"start\":46717},{\"end\":46735,\"start\":46732},{\"end\":46744,\"start\":46740},{\"end\":46758,\"start\":46755},{\"end\":46772,\"start\":46768},{\"end\":46783,\"start\":46779},{\"end\":47139,\"start\":47135},{\"end\":47149,\"start\":47146},{\"end\":47161,\"start\":47156},{\"end\":47176,\"start\":47172},{\"end\":47189,\"start\":47185},{\"end\":47205,\"start\":47198},{\"end\":47501,\"start\":47496},{\"end\":47514,\"start\":47510},{\"end\":47530,\"start\":47523},{\"end\":47547,\"start\":47541},{\"end\":47558,\"start\":47556},{\"end\":47768,\"start\":47757},{\"end\":47784,\"start\":47780},{\"end\":47805,\"start\":47801},{\"end\":48072,\"start\":48067},{\"end\":48086,\"start\":48081},{\"end\":48102,\"start\":48093},{\"end\":48117,\"start\":48112},{\"end\":48548,\"start\":48545},{\"end\":48566,\"start\":48555},{\"end\":48997,\"start\":48994},{\"end\":49008,\"start\":49006},{\"end\":49021,\"start\":49018},{\"end\":49034,\"start\":49031},{\"end\":49042,\"start\":49040},{\"end\":49517,\"start\":49511},{\"end\":49534,\"start\":49529},{\"end\":49552,\"start\":49544},{\"end\":49569,\"start\":49562},{\"end\":49589,\"start\":49581},{\"end\":49607,\"start\":49598},{\"end\":49957,\"start\":49953},{\"end\":49973,\"start\":49969},{\"end\":49985,\"start\":49981},{\"end\":49995,\"start\":49990},{\"end\":50008,\"start\":50006},{\"end\":50021,\"start\":50018},{\"end\":50034,\"start\":50030},{\"end\":50045,\"start\":50042},{\"end\":50059,\"start\":50056},{\"end\":50073,\"start\":50067},{\"end\":50532,\"start\":50528},{\"end\":50544,\"start\":50540},{\"end\":50560,\"start\":50556},{\"end\":50572,\"start\":50569},{\"end\":50582,\"start\":50577},{\"end\":50595,\"start\":50593},{\"end\":50608,\"start\":50605},{\"end\":50621,\"start\":50617},{\"end\":50632,\"start\":50629},{\"end\":50643,\"start\":50641},{\"end\":51058,\"start\":51042},{\"end\":51070,\"start\":51067},{\"end\":51090,\"start\":51080},{\"end\":51107,\"start\":51100},{\"end\":51113,\"start\":51109},{\"end\":51566,\"start\":51562},{\"end\":51579,\"start\":51574},{\"end\":51588,\"start\":51585},{\"end\":51600,\"start\":51596},{\"end\":51613,\"start\":51610},{\"end\":51621,\"start\":51619},{\"end\":51912,\"start\":51909},{\"end\":51925,\"start\":51922},{\"end\":51933,\"start\":51931},{\"end\":52175,\"start\":52147},{\"end\":52196,\"start\":52183},{\"end\":52211,\"start\":52206},{\"end\":52225,\"start\":52221},{\"end\":52237,\"start\":52233},{\"end\":52251,\"start\":52245},{\"end\":52273,\"start\":52266},{\"end\":52286,\"start\":52281},{\"end\":52304,\"start\":52295},{\"end\":52327,\"start\":52321},{\"end\":52344,\"start\":52336},{\"end\":52360,\"start\":52353},{\"end\":52368,\"start\":52362},{\"end\":52856,\"start\":52853},{\"end\":52866,\"start\":52861},{\"end\":52876,\"start\":52872},{\"end\":52889,\"start\":52886},{\"end\":52901,\"start\":52898},{\"end\":53446,\"start\":53434},{\"end\":53461,\"start\":53457},{\"end\":53476,\"start\":53470},{\"end\":53480,\"start\":53478},{\"end\":53957,\"start\":53954},{\"end\":53968,\"start\":53966},{\"end\":53981,\"start\":53976},{\"end\":53988,\"start\":53986},{\"end\":54003,\"start\":53998},{\"end\":54011,\"start\":54009},{\"end\":54023,\"start\":54020},{\"end\":54427,\"start\":54423},{\"end\":54437,\"start\":54433},{\"end\":54453,\"start\":54447},{\"end\":54464,\"start\":54462},{\"end\":54472,\"start\":54470},{\"end\":54484,\"start\":54477},{\"end\":54871,\"start\":54860},{\"end\":54884,\"start\":54879},{\"end\":54906,\"start\":54896},{\"end\":54924,\"start\":54913},{\"end\":54938,\"start\":54934},{\"end\":54958,\"start\":54947},{\"end\":54976,\"start\":54968},{\"end\":54995,\"start\":54987},{\"end\":55010,\"start\":55003},{\"end\":55025,\"start\":55020},{\"end\":55042,\"start\":55033},{\"end\":55056,\"start\":55049},{\"end\":55574,\"start\":55572},{\"end\":55588,\"start\":55585},{\"end\":55603,\"start\":55600},{\"end\":55618,\"start\":55612},{\"end\":55632,\"start\":55629},{\"end\":55641,\"start\":55638},{\"end\":55649,\"start\":55645},{\"end\":55661,\"start\":55659},{\"end\":55667,\"start\":55663},{\"end\":56224,\"start\":56212},{\"end\":56238,\"start\":56235},{\"end\":56252,\"start\":56248},{\"end\":56265,\"start\":56262},{\"end\":56274,\"start\":56272},{\"end\":56278,\"start\":56276},{\"end\":56734,\"start\":56732},{\"end\":56744,\"start\":56741},{\"end\":56757,\"start\":56753},{\"end\":56766,\"start\":56764},{\"end\":56779,\"start\":56776},{\"end\":56792,\"start\":56788},{\"end\":56804,\"start\":56802},{\"end\":57242,\"start\":57240},{\"end\":57252,\"start\":57249},{\"end\":57265,\"start\":57261},{\"end\":57274,\"start\":57272},{\"end\":57287,\"start\":57284},{\"end\":57300,\"start\":57296},{\"end\":57312,\"start\":57310},{\"end\":57751,\"start\":57742},{\"end\":58004,\"start\":57998},{\"end\":58013,\"start\":58010},{\"end\":58031,\"start\":58023},{\"end\":58041,\"start\":58037},{\"end\":58055,\"start\":58052},{\"end\":58063,\"start\":58059},{\"end\":58070,\"start\":58065},{\"end\":58078,\"start\":58074},{\"end\":58089,\"start\":58087},{\"end\":58095,\"start\":58091},{\"end\":58422,\"start\":58420},{\"end\":58430,\"start\":58428},{\"end\":58442,\"start\":58438},{\"end\":58454,\"start\":58451},{\"end\":58466,\"start\":58463},{\"end\":58805,\"start\":58802},{\"end\":58814,\"start\":58810},{\"end\":58824,\"start\":58822},{\"end\":58838,\"start\":58835},{\"end\":58851,\"start\":58849},{\"end\":58863,\"start\":58859},{\"end\":59091,\"start\":59089},{\"end\":59109,\"start\":59101},{\"end\":59468,\"start\":59466},{\"end\":59483,\"start\":59478},{\"end\":59497,\"start\":59494},{\"end\":59507,\"start\":59504},{\"end\":59950,\"start\":59944},{\"end\":59964,\"start\":59957},{\"end\":59976,\"start\":59970},{\"end\":59992,\"start\":59986},{\"end\":60008,\"start\":60001},{\"end\":60016,\"start\":60010},{\"end\":60491,\"start\":60489},{\"end\":60503,\"start\":60501},{\"end\":60516,\"start\":60511},{\"end\":60528,\"start\":60525},{\"end\":60540,\"start\":60537},{\"end\":60952,\"start\":60950},{\"end\":60965,\"start\":60960},{\"end\":60977,\"start\":60974},{\"end\":60990,\"start\":60987},{\"end\":61407,\"start\":61402},{\"end\":61419,\"start\":61416},{\"end\":61443,\"start\":61429},{\"end\":61464,\"start\":61454},{\"end\":61883,\"start\":61878},{\"end\":61891,\"start\":61888},{\"end\":61903,\"start\":61900},{\"end\":61917,\"start\":61912},{\"end\":61938,\"start\":61928},{\"end\":62277,\"start\":62272},{\"end\":62284,\"start\":62279},{\"end\":62295,\"start\":62288},{\"end\":62303,\"start\":62297},{\"end\":62574,\"start\":62566},{\"end\":62588,\"start\":62582},{\"end\":62592,\"start\":62590},{\"end\":62825,\"start\":62815},{\"end\":62838,\"start\":62833},{\"end\":62852,\"start\":62848},{\"end\":62869,\"start\":62859},{\"end\":62883,\"start\":62879},{\"end\":62898,\"start\":62893},{\"end\":62912,\"start\":62905},{\"end\":63248,\"start\":63238},{\"end\":63264,\"start\":63255},{\"end\":63283,\"start\":63277},{\"end\":63612,\"start\":63607},{\"end\":63625,\"start\":63619},{\"end\":63640,\"start\":63634},{\"end\":63657,\"start\":63650},{\"end\":63922,\"start\":63917},{\"end\":63939,\"start\":63932},{\"end\":63952,\"start\":63946},{\"end\":63967,\"start\":63961},{\"end\":64282,\"start\":64279},{\"end\":64296,\"start\":64290},{\"end\":64311,\"start\":64303},{\"end\":64323,\"start\":64321},{\"end\":64342,\"start\":64333},{\"end\":64358,\"start\":64350},{\"end\":64715,\"start\":64712},{\"end\":64730,\"start\":64725},{\"end\":64746,\"start\":64738},{\"end\":64758,\"start\":64754},{\"end\":64773,\"start\":64767},{\"end\":64787,\"start\":64780},{\"end\":64801,\"start\":64795},{\"end\":64821,\"start\":64814},{\"end\":65169,\"start\":65159},{\"end\":65183,\"start\":65177},{\"end\":65452,\"start\":65447},{\"end\":65479,\"start\":65473},{\"end\":65796,\"start\":65792},{\"end\":65814,\"start\":65810},{\"end\":65827,\"start\":65821},{\"end\":66134,\"start\":66127},{\"end\":66149,\"start\":66146},{\"end\":66164,\"start\":66157},{\"end\":66179,\"start\":66173},{\"end\":66192,\"start\":66189},{\"end\":66210,\"start\":66203},{\"end\":66225,\"start\":66219},{\"end\":66240,\"start\":66234},{\"end\":66256,\"start\":66249},{\"end\":66268,\"start\":66263},{\"end\":66286,\"start\":66279},{\"end\":66302,\"start\":66293},{\"end\":66620,\"start\":66609},{\"end\":66642,\"start\":66634},{\"end\":66657,\"start\":66649},{\"end\":67165,\"start\":67159},{\"end\":67179,\"start\":67172},{\"end\":67193,\"start\":67186},{\"end\":67208,\"start\":67205},{\"end\":67223,\"start\":67217},{\"end\":67239,\"start\":67233},{\"end\":67251,\"start\":67247},{\"end\":67259,\"start\":67257},{\"end\":67272,\"start\":67269},{\"end\":67652,\"start\":67640},{\"end\":67665,\"start\":67659},{\"end\":67681,\"start\":67674},{\"end\":67694,\"start\":67689},{\"end\":67711,\"start\":67703},{\"end\":67723,\"start\":67717},{\"end\":68127,\"start\":68116},{\"end\":68144,\"start\":68137},{\"end\":68157,\"start\":68153},{\"end\":68580,\"start\":68572},{\"end\":68593,\"start\":68584},{\"end\":68892,\"start\":68887},{\"end\":68901,\"start\":68896},{\"end\":68908,\"start\":68903},{\"end\":69305,\"start\":69300},{\"end\":69320,\"start\":69314},{\"end\":69335,\"start\":69330},{\"end\":69617,\"start\":69609},{\"end\":69631,\"start\":69628},{\"end\":69644,\"start\":69638},{\"end\":70042,\"start\":70039},{\"end\":70056,\"start\":70051},{\"end\":70066,\"start\":70061},{\"end\":70076,\"start\":70072},{\"end\":70089,\"start\":70087},{\"end\":70099,\"start\":70095},{\"end\":70119,\"start\":70111},{\"end\":70127,\"start\":70125},{\"end\":70140,\"start\":70136},{\"end\":70154,\"start\":70150},{\"end\":70492,\"start\":70485},{\"end\":70501,\"start\":70498},{\"end\":70515,\"start\":70512},{\"end\":70532,\"start\":70524},{\"end\":70544,\"start\":70540},{\"end\":70563,\"start\":70555},{\"end\":70578,\"start\":70573},{\"end\":70597,\"start\":70588},{\"end\":70616,\"start\":70606},{\"end\":71126,\"start\":71123},{\"end\":71135,\"start\":71133},{\"end\":71440,\"start\":71437},{\"end\":71454,\"start\":71450},{\"end\":71465,\"start\":71463},{\"end\":71893,\"start\":71890},{\"end\":71911,\"start\":71903},{\"end\":71925,\"start\":71920},{\"end\":71938,\"start\":71934},{\"end\":71950,\"start\":71945},{\"end\":71963,\"start\":71959},{\"end\":71976,\"start\":71973},{\"end\":71986,\"start\":71982},{\"end\":72003,\"start\":71998},{\"end\":72019,\"start\":72012},{\"end\":72402,\"start\":72392},{\"end\":72416,\"start\":72409},{\"end\":72438,\"start\":72428},{\"end\":72451,\"start\":72446},{\"end\":72465,\"start\":72461},{\"end\":72485,\"start\":72474},{\"end\":72499,\"start\":72495},{\"end\":72516,\"start\":72509},{\"end\":72532,\"start\":72525},{\"end\":72549,\"start\":72540},{\"end\":72562,\"start\":72557},{\"end\":72582,\"start\":72571},{\"end\":72953,\"start\":72946},{\"end\":72971,\"start\":72961},{\"end\":72987,\"start\":72982},{\"end\":73002,\"start\":72998},{\"end\":73022,\"start\":73014},{\"end\":73037,\"start\":73032},{\"end\":73054,\"start\":73047},{\"end\":73069,\"start\":73063},{\"end\":73087,\"start\":73079},{\"end\":73102,\"start\":73095},{\"end\":73115,\"start\":73110},{\"end\":73486,\"start\":73479},{\"end\":73501,\"start\":73497},{\"end\":73517,\"start\":73512},{\"end\":73534,\"start\":73529},{\"end\":73558,\"start\":73546},{\"end\":73571,\"start\":73566},{\"end\":73887,\"start\":73880},{\"end\":73901,\"start\":73894},{\"end\":73914,\"start\":73908},{\"end\":73931,\"start\":73922},{\"end\":73944,\"start\":73939},{\"end\":73959,\"start\":73954},{\"end\":73974,\"start\":73968},{\"end\":73992,\"start\":73982},{\"end\":74367,\"start\":74363},{\"end\":74375,\"start\":74372},{\"end\":74391,\"start\":74386},{\"end\":74404,\"start\":74399},{\"end\":74418,\"start\":74414},{\"end\":74429,\"start\":74425},{\"end\":74439,\"start\":74436},{\"end\":74450,\"start\":74448},{\"end\":74463,\"start\":74460},{\"end\":74478,\"start\":74474},{\"end\":74886,\"start\":74882},{\"end\":74896,\"start\":74893},{\"end\":74906,\"start\":74904},{\"end\":74921,\"start\":74918},{\"end\":74934,\"start\":74930},{\"end\":74946,\"start\":74941},{\"end\":74955,\"start\":74953},{\"end\":74965,\"start\":74962},{\"end\":74976,\"start\":74972},{\"end\":75289,\"start\":75285},{\"end\":75304,\"start\":75296},{\"end\":75319,\"start\":75314},{\"end\":75331,\"start\":75329},{\"end\":75601,\"start\":75593},{\"end\":75771,\"start\":75767},{\"end\":75786,\"start\":75783},{\"end\":75798,\"start\":75794},{\"end\":75812,\"start\":75807},{\"end\":75822,\"start\":75819},{\"end\":76237,\"start\":76234},{\"end\":76252,\"start\":76244},{\"end\":76266,\"start\":76260},{\"end\":76278,\"start\":76276},{\"end\":76290,\"start\":76288},{\"end\":76733,\"start\":76730},{\"end\":76747,\"start\":76744},{\"end\":76756,\"start\":76753},{\"end\":76764,\"start\":76762},{\"end\":76777,\"start\":76772},{\"end\":76790,\"start\":76787},{\"end\":76798,\"start\":76796},{\"end\":77150,\"start\":77146},{\"end\":77164,\"start\":77160},{\"end\":77174,\"start\":77170},{\"end\":77185,\"start\":77183},{\"end\":77196,\"start\":77193},{\"end\":77209,\"start\":77202},{\"end\":77221,\"start\":77218},{\"end\":77237,\"start\":77233},{\"end\":77242,\"start\":77239},{\"end\":77641,\"start\":77637},{\"end\":77653,\"start\":77649},{\"end\":77668,\"start\":77664},{\"end\":77961,\"start\":77957},{\"end\":77976,\"start\":77972},{\"end\":78229,\"start\":78226},{\"end\":78243,\"start\":78240},{\"end\":78267,\"start\":78254},{\"end\":78280,\"start\":78276},{\"end\":78296,\"start\":78292},{\"end\":78301,\"start\":78298},{\"end\":78756,\"start\":78747},{\"end\":78773,\"start\":78764},{\"end\":78875,\"start\":78870},{\"end\":78892,\"start\":78887},{\"end\":78914,\"start\":78900},{\"end\":78925,\"start\":78916},{\"end\":79165,\"start\":79160},{\"end\":79178,\"start\":79176},{\"end\":79194,\"start\":79189},{\"end\":79202,\"start\":79199},{\"end\":79213,\"start\":79208},{\"end\":79225,\"start\":79222},{\"end\":79234,\"start\":79231},{\"end\":79243,\"start\":79241},{\"end\":79258,\"start\":79251},{\"end\":79268,\"start\":79260},{\"end\":79648,\"start\":79643},{\"end\":79659,\"start\":79656},{\"end\":79674,\"start\":79671},{\"end\":79684,\"start\":79681},{\"end\":79695,\"start\":79693},{\"end\":80196,\"start\":80192},{\"end\":80207,\"start\":80204},{\"end\":80223,\"start\":80217},{\"end\":80690,\"start\":80685},{\"end\":80702,\"start\":80700},{\"end\":80719,\"start\":80715},{\"end\":80732,\"start\":80729},{\"end\":80743,\"start\":80740},{\"end\":80756,\"start\":80752},{\"end\":80767,\"start\":80765},{\"end\":80782,\"start\":80778},{\"end\":80793,\"start\":80788},{\"end\":80805,\"start\":80799},{\"end\":80811,\"start\":80807},{\"end\":81175,\"start\":81170},{\"end\":81188,\"start\":81183},{\"end\":81203,\"start\":81199},{\"end\":81214,\"start\":81212},{\"end\":81223,\"start\":81219},{\"end\":81610,\"start\":81606},{\"end\":81621,\"start\":81617},{\"end\":81634,\"start\":81630},{\"end\":81645,\"start\":81641},{\"end\":81659,\"start\":81653},{\"end\":81675,\"start\":81667},{\"end\":81693,\"start\":81685},{\"end\":82030,\"start\":82027},{\"end\":82038,\"start\":82036},{\"end\":82051,\"start\":82048},{\"end\":82063,\"start\":82060},{\"end\":82513,\"start\":82510},{\"end\":82524,\"start\":82522},{\"end\":82534,\"start\":82532},{\"end\":82542,\"start\":82540},{\"end\":82557,\"start\":82553},{\"end\":82569,\"start\":82566}]", "bib_entry": "[{\"attributes\":{\"doi\":\"PMLR, 2020. 5\",\"id\":\"b0\",\"matched_paper_id\":211572655},\"end\":47126,\"start\":46589},{\"attributes\":{\"doi\":\"arXiv:2012.09958\",\"id\":\"b1\"},\"end\":47444,\"start\":47128},{\"attributes\":{\"id\":\"b2\"},\"end\":47692,\"start\":47446},{\"attributes\":{\"doi\":\"arXiv:2004.10934\",\"id\":\"b3\"},\"end\":47995,\"start\":47694},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":15155826},\"end\":48476,\"start\":47997},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":206596979},\"end\":48917,\"start\":48478},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":131775495},\"end\":49455,\"start\":48919},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":218889832},\"end\":49900,\"start\":49457},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":58981777},\"end\":50522,\"start\":49902},{\"attributes\":{\"doi\":\"arXiv:1906.07155\",\"id\":\"b9\"},\"end\":50951,\"start\":50524},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3638670},\"end\":51487,\"start\":50953},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":220546238},\"end\":51808,\"start\":51489},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":225103043},\"end\":52099,\"start\":51810},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":222067132},\"end\":52768,\"start\":52101},{\"attributes\":{\"doi\":\"arXiv:2102.10882\",\"id\":\"b14\"},\"end\":53109,\"start\":52770},{\"attributes\":{\"id\":\"b15\"},\"end\":53345,\"start\":53111},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":208006202},\"end\":53910,\"start\":53347},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4028864},\"end\":54364,\"start\":53912},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":57246310},\"end\":54775,\"start\":54366},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":225039882},\"end\":55485,\"start\":54777},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":209202765},\"end\":56119,\"start\":55487},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":201126039},\"end\":56679,\"start\":56121},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":52180375},\"end\":57190,\"start\":56681},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":204970740},\"end\":57673,\"start\":57192},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":28586460},\"end\":57907,\"start\":57675},{\"attributes\":{\"doi\":\"arXiv:2012.07177\",\"id\":\"b25\"},\"end\":58363,\"start\":57909},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3984220},\"end\":58768,\"start\":58365},{\"attributes\":{\"doi\":\"arXiv:2103.00112\",\"id\":\"b27\"},\"end\":59034,\"start\":58770},{\"attributes\":{\"id\":\"b28\"},\"end\":59410,\"start\":59036},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206594692},\"end\":59863,\"start\":59412},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":219963404},\"end\":60443,\"start\":59865},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":37158713},\"end\":60897,\"start\":60445},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":131776850},\"end\":61354,\"start\":60899},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":9433631},\"end\":61835,\"start\":61356},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6773885},\"end\":62172,\"start\":61837},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":17055992},\"end\":62518,\"start\":62174},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b36\"},\"end\":62743,\"start\":62520},{\"attributes\":{\"doi\":\"arXiv:1912.11370\",\"id\":\"b37\"},\"end\":63166,\"start\":62745},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":195908774},\"end\":63543,\"start\":63168},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":14542261},\"end\":63861,\"start\":63545},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":37392765},\"end\":64221,\"start\":63863},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":10716717},\"end\":64658,\"start\":64223},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":14113767},\"end\":65113,\"start\":64660},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":53592270},\"end\":65388,\"start\":65115},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3548228},\"end\":65688,\"start\":65390},{\"attributes\":{\"doi\":\"arXiv:2006.02334\",\"id\":\"b45\"},\"end\":66049,\"start\":65690},{\"attributes\":{\"id\":\"b46\"},\"end\":66568,\"start\":66051},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":214714446},\"end\":67068,\"start\":66570},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":204838007},\"end\":67587,\"start\":67070},{\"attributes\":{\"id\":\"b49\"},\"end\":68045,\"start\":67589},{\"attributes\":{\"id\":\"b50\"},\"end\":68500,\"start\":68047},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":14124313},\"end\":68820,\"start\":68502},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4615054},\"end\":69251,\"start\":68822},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":43939258},\"end\":69599,\"start\":69253},{\"attributes\":{\"doi\":\"arXiv:2101.11605\",\"id\":\"b54\"},\"end\":69963,\"start\":69601},{\"attributes\":{\"doi\":\"arXiv:2011.12450,2020.3\",\"id\":\"b55\"},\"end\":70441,\"start\":69965},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":206592484},\"end\":71038,\"start\":70443},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b57\",\"matched_paper_id\":167217261},\"end\":71371,\"start\":71040},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":208175544},\"end\":71826,\"start\":71373},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":226281978},\"end\":72338,\"start\":71828},{\"attributes\":{\"id\":\"b60\"},\"end\":72855,\"start\":72340},{\"attributes\":{\"id\":\"b61\"},\"end\":73395,\"start\":72857},{\"attributes\":{\"doi\":\"arXiv:2012.12877\",\"id\":\"b62\"},\"end\":73844,\"start\":73397},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":13756489},\"end\":74283,\"start\":73846},{\"attributes\":{\"doi\":\"2020. 3\",\"id\":\"b64\",\"matched_paper_id\":201124533},\"end\":74781,\"start\":74285},{\"attributes\":{\"doi\":\"arXiv:2102.12122\",\"id\":\"b65\"},\"end\":75247,\"start\":74783},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":4852647},\"end\":75564,\"start\":75249},{\"attributes\":{\"id\":\"b67\"},\"end\":75708,\"start\":75566},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":50781105},\"end\":76162,\"start\":75710},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":8485068},\"end\":76680,\"start\":76164},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":219573794},\"end\":77141,\"start\":76682},{\"attributes\":{\"doi\":\"arXiv:2101.11986\",\"id\":\"b71\"},\"end\":77569,\"start\":77143},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":202734362},\"end\":77900,\"start\":77571},{\"attributes\":{\"doi\":\"arXiv:1809.00916\",\"id\":\"b73\"},\"end\":78129,\"start\":77902},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":152282661},\"end\":78714,\"start\":78131},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":15276198},\"end\":78861,\"start\":78716},{\"attributes\":{\"doi\":\"arXiv:1710.09412\",\"id\":\"b76\"},\"end\":79153,\"start\":78863},{\"attributes\":{\"doi\":\"arXiv:2004.08955\",\"id\":\"b77\"},\"end\":79529,\"start\":79155},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":208637257},\"end\":80131,\"start\":79531},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":215542547},\"end\":80584,\"start\":80133},{\"attributes\":{\"doi\":\"arXiv:2012.15840\",\"id\":\"b80\"},\"end\":81129,\"start\":80586},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":2035600},\"end\":81537,\"start\":81131},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":11371972},\"end\":81961,\"start\":81539},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":53745820},\"end\":82425,\"start\":81963},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":222208633},\"end\":82841,\"start\":82427}]", "bib_title": "[{\"end\":46667,\"start\":46589},{\"end\":48055,\"start\":47997},{\"end\":48535,\"start\":48478},{\"end\":48988,\"start\":48919},{\"end\":49501,\"start\":49457},{\"end\":49947,\"start\":49902},{\"end\":51034,\"start\":50953},{\"end\":51553,\"start\":51489},{\"end\":51901,\"start\":51810},{\"end\":52137,\"start\":52101},{\"end\":53425,\"start\":53347},{\"end\":53945,\"start\":53912},{\"end\":54417,\"start\":54366},{\"end\":54851,\"start\":54777},{\"end\":55562,\"start\":55487},{\"end\":56202,\"start\":56121},{\"end\":56726,\"start\":56681},{\"end\":57234,\"start\":57192},{\"end\":57731,\"start\":57675},{\"end\":58410,\"start\":58365},{\"end\":59079,\"start\":59036},{\"end\":59456,\"start\":59412},{\"end\":59937,\"start\":59865},{\"end\":60483,\"start\":60445},{\"end\":60944,\"start\":60899},{\"end\":61396,\"start\":61356},{\"end\":61872,\"start\":61837},{\"end\":62268,\"start\":62174},{\"end\":63231,\"start\":63168},{\"end\":63600,\"start\":63545},{\"end\":63910,\"start\":63863},{\"end\":64268,\"start\":64223},{\"end\":64701,\"start\":64660},{\"end\":65152,\"start\":65115},{\"end\":65443,\"start\":65390},{\"end\":66601,\"start\":66570},{\"end\":67151,\"start\":67070},{\"end\":67631,\"start\":67589},{\"end\":68109,\"start\":68047},{\"end\":68568,\"start\":68502},{\"end\":68878,\"start\":68822},{\"end\":69291,\"start\":69253},{\"end\":70473,\"start\":70443},{\"end\":71112,\"start\":71040},{\"end\":71426,\"start\":71373},{\"end\":71885,\"start\":71828},{\"end\":73871,\"start\":73846},{\"end\":74352,\"start\":74285},{\"end\":75274,\"start\":75249},{\"end\":75760,\"start\":75710},{\"end\":76224,\"start\":76164},{\"end\":76720,\"start\":76682},{\"end\":77629,\"start\":77571},{\"end\":78216,\"start\":78131},{\"end\":78738,\"start\":78716},{\"end\":79633,\"start\":79531},{\"end\":80179,\"start\":80133},{\"end\":81163,\"start\":81131},{\"end\":81598,\"start\":81539},{\"end\":82018,\"start\":81963},{\"end\":82501,\"start\":82427}]", "bib_author": "[{\"end\":46681,\"start\":46669},{\"end\":46690,\"start\":46681},{\"end\":46700,\"start\":46690},{\"end\":46713,\"start\":46700},{\"end\":46723,\"start\":46713},{\"end\":46737,\"start\":46723},{\"end\":46746,\"start\":46737},{\"end\":46760,\"start\":46746},{\"end\":46774,\"start\":46760},{\"end\":46785,\"start\":46774},{\"end\":47141,\"start\":47130},{\"end\":47151,\"start\":47141},{\"end\":47163,\"start\":47151},{\"end\":47178,\"start\":47163},{\"end\":47191,\"start\":47178},{\"end\":47207,\"start\":47191},{\"end\":47503,\"start\":47490},{\"end\":47516,\"start\":47503},{\"end\":47532,\"start\":47516},{\"end\":47549,\"start\":47532},{\"end\":47560,\"start\":47549},{\"end\":47770,\"start\":47750},{\"end\":47786,\"start\":47770},{\"end\":47807,\"start\":47786},{\"end\":48074,\"start\":48057},{\"end\":48088,\"start\":48074},{\"end\":48104,\"start\":48088},{\"end\":48119,\"start\":48104},{\"end\":48550,\"start\":48537},{\"end\":48568,\"start\":48550},{\"end\":48999,\"start\":48990},{\"end\":49010,\"start\":48999},{\"end\":49023,\"start\":49010},{\"end\":49036,\"start\":49023},{\"end\":49044,\"start\":49036},{\"end\":49519,\"start\":49503},{\"end\":49536,\"start\":49519},{\"end\":49554,\"start\":49536},{\"end\":49571,\"start\":49554},{\"end\":49591,\"start\":49571},{\"end\":49609,\"start\":49591},{\"end\":49959,\"start\":49949},{\"end\":49975,\"start\":49959},{\"end\":49987,\"start\":49975},{\"end\":49997,\"start\":49987},{\"end\":50010,\"start\":49997},{\"end\":50023,\"start\":50010},{\"end\":50036,\"start\":50023},{\"end\":50047,\"start\":50036},{\"end\":50061,\"start\":50047},{\"end\":50075,\"start\":50061},{\"end\":50534,\"start\":50524},{\"end\":50546,\"start\":50534},{\"end\":50562,\"start\":50546},{\"end\":50574,\"start\":50562},{\"end\":50584,\"start\":50574},{\"end\":50597,\"start\":50584},{\"end\":50610,\"start\":50597},{\"end\":50623,\"start\":50610},{\"end\":50634,\"start\":50623},{\"end\":50645,\"start\":50634},{\"end\":51060,\"start\":51036},{\"end\":51072,\"start\":51060},{\"end\":51092,\"start\":51072},{\"end\":51109,\"start\":51092},{\"end\":51115,\"start\":51109},{\"end\":51568,\"start\":51555},{\"end\":51581,\"start\":51568},{\"end\":51590,\"start\":51581},{\"end\":51602,\"start\":51590},{\"end\":51615,\"start\":51602},{\"end\":51623,\"start\":51615},{\"end\":51914,\"start\":51903},{\"end\":51927,\"start\":51914},{\"end\":51935,\"start\":51927},{\"end\":52177,\"start\":52139},{\"end\":52198,\"start\":52177},{\"end\":52213,\"start\":52198},{\"end\":52227,\"start\":52213},{\"end\":52239,\"start\":52227},{\"end\":52253,\"start\":52239},{\"end\":52275,\"start\":52253},{\"end\":52288,\"start\":52275},{\"end\":52306,\"start\":52288},{\"end\":52329,\"start\":52306},{\"end\":52346,\"start\":52329},{\"end\":52362,\"start\":52346},{\"end\":52370,\"start\":52362},{\"end\":52858,\"start\":52842},{\"end\":52868,\"start\":52858},{\"end\":52878,\"start\":52868},{\"end\":52891,\"start\":52878},{\"end\":52903,\"start\":52891},{\"end\":53448,\"start\":53427},{\"end\":53463,\"start\":53448},{\"end\":53478,\"start\":53463},{\"end\":53482,\"start\":53478},{\"end\":53959,\"start\":53947},{\"end\":53970,\"start\":53959},{\"end\":53983,\"start\":53970},{\"end\":53990,\"start\":53983},{\"end\":54005,\"start\":53990},{\"end\":54013,\"start\":54005},{\"end\":54025,\"start\":54013},{\"end\":54429,\"start\":54419},{\"end\":54439,\"start\":54429},{\"end\":54455,\"start\":54439},{\"end\":54466,\"start\":54455},{\"end\":54474,\"start\":54466},{\"end\":54486,\"start\":54474},{\"end\":54873,\"start\":54853},{\"end\":54886,\"start\":54873},{\"end\":54908,\"start\":54886},{\"end\":54926,\"start\":54908},{\"end\":54940,\"start\":54926},{\"end\":54960,\"start\":54940},{\"end\":54978,\"start\":54960},{\"end\":54997,\"start\":54978},{\"end\":55012,\"start\":54997},{\"end\":55027,\"start\":55012},{\"end\":55044,\"start\":55027},{\"end\":55058,\"start\":55044},{\"end\":55576,\"start\":55564},{\"end\":55590,\"start\":55576},{\"end\":55605,\"start\":55590},{\"end\":55620,\"start\":55605},{\"end\":55634,\"start\":55620},{\"end\":55643,\"start\":55634},{\"end\":55651,\"start\":55643},{\"end\":55663,\"start\":55651},{\"end\":55669,\"start\":55663},{\"end\":56226,\"start\":56204},{\"end\":56240,\"start\":56226},{\"end\":56254,\"start\":56240},{\"end\":56267,\"start\":56254},{\"end\":56276,\"start\":56267},{\"end\":56280,\"start\":56276},{\"end\":56736,\"start\":56728},{\"end\":56746,\"start\":56736},{\"end\":56759,\"start\":56746},{\"end\":56768,\"start\":56759},{\"end\":56781,\"start\":56768},{\"end\":56794,\"start\":56781},{\"end\":56806,\"start\":56794},{\"end\":57244,\"start\":57236},{\"end\":57254,\"start\":57244},{\"end\":57267,\"start\":57254},{\"end\":57276,\"start\":57267},{\"end\":57289,\"start\":57276},{\"end\":57302,\"start\":57289},{\"end\":57314,\"start\":57302},{\"end\":57753,\"start\":57733},{\"end\":58006,\"start\":57991},{\"end\":58015,\"start\":58006},{\"end\":58033,\"start\":58015},{\"end\":58043,\"start\":58033},{\"end\":58057,\"start\":58043},{\"end\":58065,\"start\":58057},{\"end\":58072,\"start\":58065},{\"end\":58080,\"start\":58072},{\"end\":58091,\"start\":58080},{\"end\":58097,\"start\":58091},{\"end\":58424,\"start\":58412},{\"end\":58432,\"start\":58424},{\"end\":58444,\"start\":58432},{\"end\":58456,\"start\":58444},{\"end\":58468,\"start\":58456},{\"end\":58807,\"start\":58798},{\"end\":58816,\"start\":58807},{\"end\":58826,\"start\":58816},{\"end\":58840,\"start\":58826},{\"end\":58853,\"start\":58840},{\"end\":58865,\"start\":58853},{\"end\":59093,\"start\":59081},{\"end\":59111,\"start\":59093},{\"end\":59470,\"start\":59458},{\"end\":59485,\"start\":59470},{\"end\":59499,\"start\":59485},{\"end\":59509,\"start\":59499},{\"end\":59952,\"start\":59939},{\"end\":59966,\"start\":59952},{\"end\":59978,\"start\":59966},{\"end\":59994,\"start\":59978},{\"end\":60010,\"start\":59994},{\"end\":60018,\"start\":60010},{\"end\":60493,\"start\":60485},{\"end\":60505,\"start\":60493},{\"end\":60518,\"start\":60505},{\"end\":60530,\"start\":60518},{\"end\":60542,\"start\":60530},{\"end\":60954,\"start\":60946},{\"end\":60967,\"start\":60954},{\"end\":60979,\"start\":60967},{\"end\":60992,\"start\":60979},{\"end\":61409,\"start\":61398},{\"end\":61421,\"start\":61409},{\"end\":61445,\"start\":61421},{\"end\":61466,\"start\":61445},{\"end\":61885,\"start\":61874},{\"end\":61893,\"start\":61885},{\"end\":61905,\"start\":61893},{\"end\":61919,\"start\":61905},{\"end\":61940,\"start\":61919},{\"end\":62279,\"start\":62270},{\"end\":62286,\"start\":62279},{\"end\":62297,\"start\":62286},{\"end\":62305,\"start\":62297},{\"end\":62576,\"start\":62564},{\"end\":62590,\"start\":62576},{\"end\":62594,\"start\":62590},{\"end\":62827,\"start\":62805},{\"end\":62840,\"start\":62827},{\"end\":62854,\"start\":62840},{\"end\":62871,\"start\":62854},{\"end\":62885,\"start\":62871},{\"end\":62900,\"start\":62885},{\"end\":62914,\"start\":62900},{\"end\":63250,\"start\":63233},{\"end\":63266,\"start\":63250},{\"end\":63285,\"start\":63266},{\"end\":63614,\"start\":63602},{\"end\":63627,\"start\":63614},{\"end\":63642,\"start\":63627},{\"end\":63659,\"start\":63642},{\"end\":63924,\"start\":63912},{\"end\":63941,\"start\":63924},{\"end\":63954,\"start\":63941},{\"end\":63969,\"start\":63954},{\"end\":64284,\"start\":64270},{\"end\":64298,\"start\":64284},{\"end\":64313,\"start\":64298},{\"end\":64325,\"start\":64313},{\"end\":64344,\"start\":64325},{\"end\":64360,\"start\":64344},{\"end\":64717,\"start\":64703},{\"end\":64732,\"start\":64717},{\"end\":64748,\"start\":64732},{\"end\":64760,\"start\":64748},{\"end\":64775,\"start\":64760},{\"end\":64789,\"start\":64775},{\"end\":64803,\"start\":64789},{\"end\":64823,\"start\":64803},{\"end\":65171,\"start\":65154},{\"end\":65185,\"start\":65171},{\"end\":65454,\"start\":65445},{\"end\":65481,\"start\":65454},{\"end\":65798,\"start\":65785},{\"end\":65816,\"start\":65798},{\"end\":65829,\"start\":65816},{\"end\":66136,\"start\":66122},{\"end\":66151,\"start\":66136},{\"end\":66166,\"start\":66151},{\"end\":66181,\"start\":66166},{\"end\":66194,\"start\":66181},{\"end\":66212,\"start\":66194},{\"end\":66227,\"start\":66212},{\"end\":66242,\"start\":66227},{\"end\":66258,\"start\":66242},{\"end\":66270,\"start\":66258},{\"end\":66288,\"start\":66270},{\"end\":66304,\"start\":66288},{\"end\":66622,\"start\":66603},{\"end\":66644,\"start\":66622},{\"end\":66659,\"start\":66644},{\"end\":67167,\"start\":67153},{\"end\":67181,\"start\":67167},{\"end\":67195,\"start\":67181},{\"end\":67210,\"start\":67195},{\"end\":67225,\"start\":67210},{\"end\":67241,\"start\":67225},{\"end\":67253,\"start\":67241},{\"end\":67261,\"start\":67253},{\"end\":67274,\"start\":67261},{\"end\":67654,\"start\":67633},{\"end\":67667,\"start\":67654},{\"end\":67683,\"start\":67667},{\"end\":67696,\"start\":67683},{\"end\":67713,\"start\":67696},{\"end\":67725,\"start\":67713},{\"end\":68129,\"start\":68111},{\"end\":68146,\"start\":68129},{\"end\":68159,\"start\":68146},{\"end\":68582,\"start\":68570},{\"end\":68595,\"start\":68582},{\"end\":68894,\"start\":68880},{\"end\":68903,\"start\":68894},{\"end\":68910,\"start\":68903},{\"end\":69307,\"start\":69293},{\"end\":69322,\"start\":69307},{\"end\":69337,\"start\":69322},{\"end\":69619,\"start\":69601},{\"end\":69633,\"start\":69619},{\"end\":69646,\"start\":69633},{\"end\":70044,\"start\":70033},{\"end\":70058,\"start\":70044},{\"end\":70068,\"start\":70058},{\"end\":70078,\"start\":70068},{\"end\":70091,\"start\":70078},{\"end\":70101,\"start\":70091},{\"end\":70121,\"start\":70101},{\"end\":70129,\"start\":70121},{\"end\":70142,\"start\":70129},{\"end\":70156,\"start\":70142},{\"end\":70494,\"start\":70475},{\"end\":70503,\"start\":70494},{\"end\":70517,\"start\":70503},{\"end\":70534,\"start\":70517},{\"end\":70546,\"start\":70534},{\"end\":70565,\"start\":70546},{\"end\":70580,\"start\":70565},{\"end\":70599,\"start\":70580},{\"end\":70618,\"start\":70599},{\"end\":71128,\"start\":71114},{\"end\":71137,\"start\":71128},{\"end\":71442,\"start\":71428},{\"end\":71456,\"start\":71442},{\"end\":71467,\"start\":71456},{\"end\":71895,\"start\":71887},{\"end\":71913,\"start\":71895},{\"end\":71927,\"start\":71913},{\"end\":71940,\"start\":71927},{\"end\":71952,\"start\":71940},{\"end\":71965,\"start\":71952},{\"end\":71978,\"start\":71965},{\"end\":71988,\"start\":71978},{\"end\":72005,\"start\":71988},{\"end\":72021,\"start\":72005},{\"end\":72404,\"start\":72387},{\"end\":72418,\"start\":72404},{\"end\":72440,\"start\":72418},{\"end\":72453,\"start\":72440},{\"end\":72467,\"start\":72453},{\"end\":72487,\"start\":72467},{\"end\":72501,\"start\":72487},{\"end\":72518,\"start\":72501},{\"end\":72534,\"start\":72518},{\"end\":72551,\"start\":72534},{\"end\":72564,\"start\":72551},{\"end\":72584,\"start\":72564},{\"end\":72955,\"start\":72941},{\"end\":72973,\"start\":72955},{\"end\":72989,\"start\":72973},{\"end\":73004,\"start\":72989},{\"end\":73024,\"start\":73004},{\"end\":73039,\"start\":73024},{\"end\":73056,\"start\":73039},{\"end\":73071,\"start\":73056},{\"end\":73089,\"start\":73071},{\"end\":73104,\"start\":73089},{\"end\":73117,\"start\":73104},{\"end\":73488,\"start\":73474},{\"end\":73503,\"start\":73488},{\"end\":73519,\"start\":73503},{\"end\":73536,\"start\":73519},{\"end\":73560,\"start\":73536},{\"end\":73573,\"start\":73560},{\"end\":73889,\"start\":73873},{\"end\":73903,\"start\":73889},{\"end\":73916,\"start\":73903},{\"end\":73933,\"start\":73916},{\"end\":73946,\"start\":73933},{\"end\":73961,\"start\":73946},{\"end\":73976,\"start\":73961},{\"end\":73994,\"start\":73976},{\"end\":74369,\"start\":74354},{\"end\":74377,\"start\":74369},{\"end\":74393,\"start\":74377},{\"end\":74406,\"start\":74393},{\"end\":74420,\"start\":74406},{\"end\":74431,\"start\":74420},{\"end\":74441,\"start\":74431},{\"end\":74452,\"start\":74441},{\"end\":74465,\"start\":74452},{\"end\":74480,\"start\":74465},{\"end\":74888,\"start\":74875},{\"end\":74898,\"start\":74888},{\"end\":74908,\"start\":74898},{\"end\":74923,\"start\":74908},{\"end\":74936,\"start\":74923},{\"end\":74948,\"start\":74936},{\"end\":74957,\"start\":74948},{\"end\":74967,\"start\":74957},{\"end\":74978,\"start\":74967},{\"end\":75291,\"start\":75276},{\"end\":75306,\"start\":75291},{\"end\":75321,\"start\":75306},{\"end\":75333,\"start\":75321},{\"end\":75603,\"start\":75588},{\"end\":75773,\"start\":75762},{\"end\":75788,\"start\":75773},{\"end\":75800,\"start\":75788},{\"end\":75814,\"start\":75800},{\"end\":75824,\"start\":75814},{\"end\":76239,\"start\":76226},{\"end\":76254,\"start\":76239},{\"end\":76268,\"start\":76254},{\"end\":76280,\"start\":76268},{\"end\":76292,\"start\":76280},{\"end\":76735,\"start\":76722},{\"end\":76749,\"start\":76735},{\"end\":76758,\"start\":76749},{\"end\":76766,\"start\":76758},{\"end\":76779,\"start\":76766},{\"end\":76792,\"start\":76779},{\"end\":76800,\"start\":76792},{\"end\":77152,\"start\":77143},{\"end\":77166,\"start\":77152},{\"end\":77176,\"start\":77166},{\"end\":77187,\"start\":77176},{\"end\":77198,\"start\":77187},{\"end\":77211,\"start\":77198},{\"end\":77223,\"start\":77211},{\"end\":77239,\"start\":77223},{\"end\":77244,\"start\":77239},{\"end\":77643,\"start\":77631},{\"end\":77655,\"start\":77643},{\"end\":77670,\"start\":77655},{\"end\":77963,\"start\":77951},{\"end\":77978,\"start\":77963},{\"end\":78231,\"start\":78218},{\"end\":78245,\"start\":78231},{\"end\":78269,\"start\":78245},{\"end\":78282,\"start\":78269},{\"end\":78298,\"start\":78282},{\"end\":78303,\"start\":78298},{\"end\":78758,\"start\":78740},{\"end\":78775,\"start\":78758},{\"end\":78877,\"start\":78863},{\"end\":78894,\"start\":78877},{\"end\":78916,\"start\":78894},{\"end\":78927,\"start\":78916},{\"end\":79167,\"start\":79155},{\"end\":79180,\"start\":79167},{\"end\":79196,\"start\":79180},{\"end\":79204,\"start\":79196},{\"end\":79215,\"start\":79204},{\"end\":79227,\"start\":79215},{\"end\":79236,\"start\":79227},{\"end\":79245,\"start\":79236},{\"end\":79260,\"start\":79245},{\"end\":79270,\"start\":79260},{\"end\":79650,\"start\":79635},{\"end\":79661,\"start\":79650},{\"end\":79676,\"start\":79661},{\"end\":79686,\"start\":79676},{\"end\":79697,\"start\":79686},{\"end\":80198,\"start\":80181},{\"end\":80209,\"start\":80198},{\"end\":80225,\"start\":80209},{\"end\":80692,\"start\":80678},{\"end\":80704,\"start\":80692},{\"end\":80721,\"start\":80704},{\"end\":80734,\"start\":80721},{\"end\":80745,\"start\":80734},{\"end\":80758,\"start\":80745},{\"end\":80769,\"start\":80758},{\"end\":80784,\"start\":80769},{\"end\":80795,\"start\":80784},{\"end\":80807,\"start\":80795},{\"end\":80813,\"start\":80807},{\"end\":81177,\"start\":81165},{\"end\":81190,\"start\":81177},{\"end\":81205,\"start\":81190},{\"end\":81216,\"start\":81205},{\"end\":81225,\"start\":81216},{\"end\":81612,\"start\":81600},{\"end\":81623,\"start\":81612},{\"end\":81636,\"start\":81623},{\"end\":81647,\"start\":81636},{\"end\":81661,\"start\":81647},{\"end\":81677,\"start\":81661},{\"end\":81695,\"start\":81677},{\"end\":82032,\"start\":82020},{\"end\":82040,\"start\":82032},{\"end\":82053,\"start\":82040},{\"end\":82065,\"start\":82053},{\"end\":82515,\"start\":82503},{\"end\":82526,\"start\":82515},{\"end\":82536,\"start\":82526},{\"end\":82544,\"start\":82536},{\"end\":82559,\"start\":82544},{\"end\":82571,\"start\":82559}]", "bib_venue": "[{\"end\":46842,\"start\":46798},{\"end\":47488,\"start\":47446},{\"end\":47748,\"start\":47694},{\"end\":48193,\"start\":48119},{\"end\":48645,\"start\":48568},{\"end\":49132,\"start\":49044},{\"end\":49647,\"start\":49609},{\"end\":50156,\"start\":50075},{\"end\":50703,\"start\":50661},{\"end\":51179,\"start\":51115},{\"end\":51639,\"start\":51623},{\"end\":51942,\"start\":51935},{\"end\":52422,\"start\":52370},{\"end\":52840,\"start\":52770},{\"end\":53180,\"start\":53111},{\"end\":53573,\"start\":53482},{\"end\":54092,\"start\":54025},{\"end\":54549,\"start\":54486},{\"end\":55110,\"start\":55058},{\"end\":55750,\"start\":55669},{\"end\":56351,\"start\":56280},{\"end\":56883,\"start\":56806},{\"end\":57385,\"start\":57314},{\"end\":57775,\"start\":57753},{\"end\":57989,\"start\":57909},{\"end\":58532,\"start\":58468},{\"end\":58796,\"start\":58770},{\"end\":59178,\"start\":59111},{\"end\":59586,\"start\":59509},{\"end\":60099,\"start\":60018},{\"end\":60619,\"start\":60542},{\"end\":61070,\"start\":60992},{\"end\":61543,\"start\":61466},{\"end\":61978,\"start\":61940},{\"end\":62330,\"start\":62305},{\"end\":62562,\"start\":62520},{\"end\":62803,\"start\":62745},{\"end\":63334,\"start\":63285},{\"end\":63682,\"start\":63659},{\"end\":64015,\"start\":63969},{\"end\":64429,\"start\":64360},{\"end\":64861,\"start\":64823},{\"end\":65237,\"start\":65185},{\"end\":65521,\"start\":65481},{\"end\":65783,\"start\":65690},{\"end\":66120,\"start\":66051},{\"end\":66740,\"start\":66659},{\"end\":67310,\"start\":67274},{\"end\":67774,\"start\":67725},{\"end\":68245,\"start\":68159},{\"end\":68647,\"start\":68595},{\"end\":68987,\"start\":68910},{\"end\":69386,\"start\":69337},{\"end\":69760,\"start\":69662},{\"end\":70031,\"start\":69965},{\"end\":70695,\"start\":70618},{\"end\":71185,\"start\":71141},{\"end\":71548,\"start\":71467},{\"end\":72073,\"start\":72021},{\"end\":72385,\"start\":72340},{\"end\":72939,\"start\":72857},{\"end\":73472,\"start\":73397},{\"end\":74043,\"start\":73994},{\"end\":74504,\"start\":74487},{\"end\":74873,\"start\":74783},{\"end\":75391,\"start\":75333},{\"end\":75586,\"start\":75566},{\"end\":75888,\"start\":75824},{\"end\":76369,\"start\":76292},{\"end\":76873,\"start\":76800},{\"end\":77333,\"start\":77260},{\"end\":77722,\"start\":77670},{\"end\":77949,\"start\":77902},{\"end\":78374,\"start\":78303},{\"end\":78779,\"start\":78775},{\"end\":78984,\"start\":78943},{\"end\":79310,\"start\":79286},{\"end\":79778,\"start\":79697},{\"end\":80306,\"start\":80225},{\"end\":80676,\"start\":80586},{\"end\":81286,\"start\":81225},{\"end\":81735,\"start\":81695},{\"end\":82142,\"start\":82065},{\"end\":82623,\"start\":82571},{\"end\":48254,\"start\":48195},{\"end\":48709,\"start\":48647},{\"end\":49207,\"start\":49134},{\"end\":50224,\"start\":50158},{\"end\":51230,\"start\":51181},{\"end\":53651,\"start\":53575},{\"end\":54146,\"start\":54094},{\"end\":55818,\"start\":55752},{\"end\":56409,\"start\":56353},{\"end\":56947,\"start\":56885},{\"end\":57443,\"start\":57387},{\"end\":58583,\"start\":58534},{\"end\":59232,\"start\":59180},{\"end\":59650,\"start\":59588},{\"end\":60167,\"start\":60101},{\"end\":60683,\"start\":60621},{\"end\":61135,\"start\":61072},{\"end\":61607,\"start\":61545},{\"end\":66808,\"start\":66742},{\"end\":69051,\"start\":68989},{\"end\":70759,\"start\":70697},{\"end\":71616,\"start\":71550},{\"end\":75939,\"start\":75890},{\"end\":76433,\"start\":76371},{\"end\":76933,\"start\":76875},{\"end\":78432,\"start\":78376},{\"end\":79846,\"start\":79780},{\"end\":80374,\"start\":80308},{\"end\":81334,\"start\":81288},{\"end\":82206,\"start\":82144}]"}}}, "year": 2023, "month": 12, "day": 17}