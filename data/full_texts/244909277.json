{"id": 244909277, "updated": "2023-10-05 19:20:27.429", "metadata": {"title": "Generalized Binary Search Network for Highly-Efficient Multi-View Stereo", "authors": "[{\"first\":\"Zhenxing\",\"last\":\"Mi\",\"middle\":[]},{\"first\":\"Di\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Multi-view Stereo (MVS) with known camera parameters is essentially a 1D search problem within a valid depth range. Recent deep learning-based MVS methods typically densely sample depth hypotheses in the depth range, and then construct prohibitively memory-consuming 3D cost volumes for depth prediction. Although coarse-to-fine sampling strategies alleviate this overhead issue to a certain extent, the efficiency of MVS is still an open challenge. In this work, we propose a novel method for highly efficient MVS that remarkably decreases the memory footprint, meanwhile clearly advancing state-of-the-art depth prediction performance. We investigate what a search strategy can be reasonably optimal for MVS taking into account of both efficiency and effectiveness. We first formulate MVS as a binary search problem, and accordingly propose a generalized binary search network for MVS. Specifically, in each step, the depth range is split into 2 bins with extra 1 error tolerance bin on both sides. A classification is performed to identify which bin contains the true depth. We also design three mechanisms to respectively handle classification errors, deal with out-of-range samples and decrease the training memory. The new formulation makes our method only sample a very small number of depth hypotheses in each step, which is highly memory efficient, and also greatly facilitates quick training convergence. Experiments on competitive benchmarks show that our method achieves state-of-the-art accuracy with much less memory. Particularly, our method obtains an overall score of 0.289 on DTU dataset and tops the first place on challenging Tanks and Temples advanced dataset among all the learning-based methods. The trained models and code will be released at https://github.com/MiZhenxing/GBi-Net.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2112.02338", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/MiCX22", "doi": "10.1109/cvpr52688.2022.01265"}}, "content": {"source": {"pdf_hash": "c88fc58d7e0f58008738d8844f6452faa4a5bbca", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.02338v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a82d6cc14074e04b589b00a916d0224ce1909517", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c88fc58d7e0f58008738d8844f6452faa4a5bbca.txt", "contents": "\nGeneralized Binary Search Network for Highly-Efficient Multi-View Stereo\n\n\nZhenxing Mi \nThe Department of Computer Science and Engineering\nHKUST\n\n\nDi Chang dchangac@connect.ust.hk \nThe Department of Computer Science and Engineering\nHKUST\n\n\nDan Xu danxu@cse.ust.hk \nThe Department of Computer Science and Engineering\nHKUST\n\n\nGeneralized Binary Search Network for Highly-Efficient Multi-View Stereo\n\nMulti-view Stereo (MVS) with known camera parameters is essentially a 1D search problem within a valid depth range. Recent deep learning-based MVS methods typically densely sample depth hypotheses in the depth range, and then construct prohibitively memory-consuming 3D cost volumes for depth prediction. Although coarse-to-fine sampling strategies alleviate this overhead issue to a certain extent, the efficiency of MVS is still an open challenge. In this work, we propose a novel method for highly efficient MVS that remarkably decreases the memory footprint, meanwhile clearly advancing state-of-the-art depth prediction performance. We investigate what a search strategy can be reasonably optimal for MVS taking into account of both efficiency and effectiveness. We first formulate MVS as a binary search problem, and accordingly propose a generalized binary search network for MVS. Specifically, in each step, the depth range is split into 2 bins with extra 1 error tolerance bin on both sides. A classification is performed to identify which bin contains the true depth. We also design three mechanisms to respectively handle classification errors, deal with out-of-range samples and decrease the training memory. The new formulation makes our method only sample a very small number of depth hypotheses in each step, which is highly memory efficient, and also greatly facilitates quick training convergence. Experiments on competitive benchmarks show that our method achieves stateof-the-art accuracy with much less memory. Particularly, our method obtains an overall score of 0.289 on DTU dataset and tops the first place on challenging Tanks and Temples advanced dataset among all the learning-based methods. The trained models and code will be released at https://github.com/MiZhenxing/GBi-Net.\n\nIntroduction\n\nMulti-view Stereo (MVS) is a long-standing and fundamental topic in computer vision, which aims to reconstruct 3D geometry of a scene from a set of overlapping images [9,10,26,32,35]. With known camera pa-  [5,11,33,34,41,44,45] on DTU [2]. The relationship between the overall error and the GPU memory overhead with image size 1152\u00d71600 and image number 5. (b) Comparison of the previous dense search and the proposed binary search.\n\nrameters, MVS matches pixels across images to compute dense correspondences and recover 3D points, which is essentially a 1D search problem [8]. A depth map is widely used as 3D representation due to its regular format. To overcome the issue of coarse matching in previous purely geometry-based methods, recent learning-based MVS methods [14,21,41,42] designed deep networks for dense depth prediction to significantly advance traditional pipelines. For instance, MVSNet [41] and RMVSNet [42] propose to construct 3D cost volumes from 2D image features with dense depth hypotheses. A 3D cost volume is a 5D tensor and is typically regularized by a 3D Convolutional Neural Network (CNN) or a Recurrent Neural Network (RNN) for depth prediction. The importance of 3D cost volume regularization for accurate depth prediction has been confirmed by other works [4,5,11]. However, a severe problem is that 3D cost volumes are highly memory-consuming. Existing works made significant efforts to address this issue via decreasing the resolution of feature maps [41], using a coarse-to-fine strategy that gradually increases resolution of feature maps while decreasing the depth hypothesis number [4,5,11], and removing expensive 3D CNN or RNN [33,40]. Although the memory can be alleviated to some extent, relatively lower accuracy is commonly observed. The size of 3D cost volume, specifically the depth hypothesis number, plays a dominant role in causing a large memory footprint.\n\nDue to the significance of 3D cost volumes in both model efficiency and effectiveness, a critical question naturally arises: what is a minimum volume size to secure satisfactory accuracy while maintaining as small as possible the memory overhead? In this work, we investigate this question by exploring from a perspective of discrete search strategies, to identify a minimal depth hypotheses number, a key factor in 3D cost volumes. As shown in Fig. 1b, the vanilla MVSNet [41] can be seen as a dense search method that checks all depth hypotheses similar to a linear search in a parallel manner. The coarse-to-fine methods [5,11] perform a multi-granularity search, which starts from a coarse level and gradually refines the prediction. However, these two types of methods both consider dense search in each stage. We argue that the dense search does not necessarily guarantee better accuracy due to a much larger prediction space and significantly increases model complexity, leading to higher optimization difficulty in model training.\n\nTo explore a reasonably optimal search strategy, we first formulate MVS as a binary search problem, which can remarkably reduce the cost volume size to an extremely low bound. It performs comparisons and eliminates half of the search space in each stage (see Fig. 1b), and can convergence quickly to a fine granularity within logarithmic stages. In contrast to regression-based methods, which directly sample depth values from the depth range, we first divide the depth range into 2 bins. In our design, the 'comparisons' by the network is to determine which bin contains the true depth value via performing a binary discrete classification. We use center points of the two bins to represent them and construct 3D cost volumes. The binary search offers superior efficiency, while it also brings an issue of the network classification error (i.e. prediction out of bins). The error can be accumulated from each search stage causing unstable optimization gradients and relatively low accuracy.\n\nTo tackle this issue, we further design three effective mechanisms, and accordingly propose a generalized binary search deep network, termed as GBi-Net, for highly efficient MVS. The first mechanism is that we pad two error tolerance bins on the two sides to reduce the prediction error of out of bins. The second mechanism is for training. If the network generates an error of prediction out of bins for a pixel at a search stage, we stop the forward pass of this pixel in the next stage, and the gradients at this stage are not used to update the network. Extensive experiments show that the proposed GBi-Net can largely decrease the size of 3D cost volumes for a significantly efficient network, and more importantly, without any trade-off on the depth prediction performance. The third is efficient gradient updating. It updates the network parameters immediately at each search stage without accumulating across different stages as most works do. It can largely reduce the training memory while maintaining the performance. Our method achieves state-of-the-art performance on different competitive datasets including DTU [2] and Tanks & Temples [17]. Notably, on DTU, we achieve an overall score of 0.289 (lower is better), remarkably improving the previous best performing method, and also obtain a memory efficiency improvement of 48.0% compared to UCSNet [5] and 54.1% compared to CasMVSNet [11] (see Fig. 1a).\n\nIn summary, our contribution is three-fold:\n\n\u2022 We investigate efficient MVS from a perspective of search strategies, and propose a discrete binary search method for MVS (Bi-Net) which can vastly decrease the memory usage of 3D cost volumes.\n\n\u2022 We design a highly-efficient generalized binary search network (GBi-Net) via further designing three mechanisms (i.e. padding error tolerance bins, gradients masking, and efficient gradient updating) with the binary search to avoid error accumulation of false network predictions and improve efficiency.\n\n\u2022 We evaluate our method on several challenging MVS datasets and significantly advance existing state-ofthe-art methods in terms of both the depth prediction accuracy and the memory efficiency.\n\n\nRelated Work\n\nWe review the most related works in the literature from two aspects, i.e. traditional MVS and learning-based MVS. Traditional Multi-View Stereo. In 3D reconstruction, various 3D representations are used, such as volumetric representation [18,27], point cloud [9,19], mesh [6,16,29,30] and depth map [3,10,26]. In MVS [23,26,35], depth maps have shown advantages in robustness, efficiency. They estimate a depth map of each reference image and fuse them into one 3D point cloud. For instance, COLMAP [26] simultaneously estimates pixel-wise view selection, depth, and surface normal. ACMM [35] leverages a multi-hypothesis joint voting scheme for view selection from different candidates. These existing methods perform modeling of occlusion, illumination across neighboring views for depth estimation. Although stable results can be achieved, high matching noise and poor correspondence localization in complex scenes are still severe limitations. Thus, our method mainly focuses on developing a deep learning-based MVS pipeline to advance the estimation performance. Learning-based Multi-View Stereo. Deep learning-based MVS methods [13,15,34,41] recently have achieved remarkable performance. Deep learning-based methods usually utilize Deep CNNs to estimate a dense depth map. Recently, 3D cost volumes have been widely used in MVS [33,37,41,45]. As a pioneering method, MVSNet [41] constructs the 3D cost volume from feature warping and regularizes the cost volume with 3D CNNs for depth regression.   Figure 2. The multi-stage framework of our GBi-Net. The 2D CNNs first extract 2D feature maps. Then a 3D cost volume is constructed and fused by differentiable warping and only 4 depth hypotheses. The cost volume is regularized by 3D CNNs and gets a probability volume for loss calculation with one-hot labels and training masks. We select the depth bin at current stage using argmax operation. Then we update the depth hypotheses for the next stage with our proposed Generalized Binary Search.\n\nThe main problem of vanilla MVSNet is the large memory consumption of the 3D cost volume. Recurrent MVSNet architectures [34,38,42] leverage recurrent networks to regularize cost volumes, which can decrease the memory usage to some extent in the testing phase. However, the major overhead from the 3D cost volumes is not specifically addressed by these existing methods.\n\nTo reconstruct hig-resolution depth maps meanwhile obtaining a memory-efficient cost volume, cascade-based pipelines are proposed [5,11,39], considering a coarse-tofine dense search strategy to gradually refine the depths. For instance, CasMVSNet [11] utilizes coarse feature maps and depth hypotheses in the first stage for coarse depth prediction, and then upsamples depth maps and narrows the depth range for fine-grained prediction in the next stage. Patchmatchnet [33] learns adaptive propagation and evaluation for depth hypotheses. It removes heavy regularization of 3D cost volumes to achieve an efficient model while it makes a significant trade-off between efficiency and accuracy.\n\nHowever, these existing works still consider a dense search in each regression stage. The memory overhead on the expensive 3D cost volume is clearly not optimized, while the proposed method targets highly efficient MVS with the designed binary search network, which largely advances the model efficiency, and more importantly, without sacrificing any depth prediction performance.\n\n\nThe Proposed Approach\n\nIn this section, we introduce the detailed structure of the proposed Generalized Binary Search Network (GBi-Net) for highly-efficient MVS. The overall framework is depicted in Fig. 2. It mainly consists of two parts, i.e. a 2D CNN network for learning visual image representations, and the generalized binary search network for iterative depth estimation. The GBi-Net contains K search stages. In each search stage, we first compute 3D cost volumes by differentiable warping between the reference and source feature map in a specific corresponding scale. Then 3D cost volumes are regularized by 3D CNNs for depth label prediction. The Generalized Binary Search is responsible for initializing and updating depth hypotheses according to the predicted labels iteratively. In every two stages, the networks deal with the same scale of feature maps, and the network parameters are shared. Finally, one-hot labels for training the whole network are computed from ground-truth depth maps. In the next, we first introduce the 2D image encoder in Sec. 3 [20] as an image encoder to learn generic representation for the images with shared network parameters. From FPN, we obtain a pyramid of feature maps with 4 different scales. To have more powerful representations of the images, one deformable convolutional network (DCN) [7] layer is used as output layer for each scale to more effectively capture scene contexts that are very beneficial for the MVS task.\n\n\nImage Encoding\nThe input consists of N images {I i } N \u22121 i=0 . I 0 is a ref- erence image and {I i } N \u22121 i=1 is a set of N \u2212 1 source im- ages. We use Feature Pyramid Network (FPN)\n\nCost Volume Regularization\n\nThe construction of 3D cost volumes is a critical step for deep learning-based MVS [41]. We present details about the cost volume construction and regularization for the proposed generalized binary search network. Given D depth hypotheses at the k-th search stage, i.e. {d k,j |j = 1, ..., D}, a pixel-wise dense 3D cost volume can be built by differentiable warping on the learned image feature maps [33,41]. To simplify the description, we ignore the stage index k in the following formulation.\n\nThe input of MVS consists of relative camera rotation R F0\u2192Fi and translation t F0\u2192Fi from a reference feature map F 0 to a source feature map F i . Their corresponding camera intrinsics K 0 , K i are also known. We first construct a set of 2-view cost volumes {V i } N \u22121 i=1 from the N \u2212 1 source image feature maps by differentiable warping and groupwise correlation [12,33,37]. Let p be a pixel in I 0 , p be the warped pixel of p in the source image I i by the j-th depth hypothesis, i.e. d j . Then p can be computed by:\np = K i \u00b7 (R F0\u2192Fi \u00b7 K \u22121 0 \u00b7 p \u00b7 d j + t F0\u2192Fi ),(1)\nwhere the feature maps F 0 and F i all have a channel dimension of N c . Following [12], we divide the channels of the feature maps into N g groups along the channel dimension, and each feature group therefore has N c /N g channels. Let F g i be the g-th feature group of F i . Then we can compute the i-th cost volume V i from F i as follows:\nV i (j, p, g) = N g N c F g 0 (p), F g i (p ) .(2)\nWhere \u00b7, \u00b7 denotes a correlation calculation by an inner product operation. The group-wise correlation allows us to more efficiently construct a full cost volume. After the construction of each 2-view cost volume, we apply several 3D CNN layers to predict a set of pixel-wise weight matrices\n{W i } N \u22121 i=1\n. Then we fuse these cost volumes into one cost volume V via weighted fusion [33,37] \nwith {W i } N \u22121 i=1 as: V(j, p, g) = N \u22121 i=1 W i (p) \u00b7 V i (j, p, g) N \u22121 i=1 W i (p) .(3)\nThe fused 3D cost volume V is then regularized by a 3D UNet [24,41], which gradually reduces the channel size of V to 1 and output a volume of size D, H, W , i.e. spatial size of the volume. Finally, a Softmax(\u00b7) function is performed along the D dimension to produce a probability volume P for computing the training loss and labels.\n\n\nBinary Search for MVS\n\nMulti-view stereo networks [11,41,42] typically densely sample depth hypotheses for each pixel to construct 3D cost volumes, resulting in remarkably high memory footprint. To alleviate this issue, recent cascade-based MVS methods [5,11] propose to construct cost volumes in a coarse-to-fine manner which reduces the memory usage to some extent. However, in each iterative stage, the sampling is still much dense, and thus the model efficiency is far less than optimal.\n\nIn this work, we explore a reasonably optimal sampling strategy from a perspective of discrete search for highlyefficent MVS, and propose a binary search method (Bi-Net). Specifically, instead of directly sampling depth values in the given depth range R, we divide the current depth range into bins. For the k-th search stage, we divide the depth range into 2 equal bins, i.e. {B k,j |j = 1, 2} with B k,j denoting a bin. The bin width of B 1,j in the first stage is R/2. As we cannot directly use discrete bins for warping feature maps, we sample center points of the 2 bins to represent the depth hypotheses of bins, and then construct the cost volume and perform label prediction for the 2 bins. Let the three edges from left to right of the 2 bins be {e k,m |m = 1, 2, 3}. Then, the two edges of bin B k,j are e k,j and e k,j+1 . For instance, e k,1 and e k,2 are edges of B k,1 . Then the depth hypothesis d k,j for the 2 bins can be computed as follows:\nd k,j = e k,j + e k,j+1 2 , j = 1, 2.(4)\nThe predicted label of a depth hypothesis indicates whether the true depth value is in the corresponding bin. In the kth search stage, after the network outputs the probability volume P, we apply an argmax(\u00b7) operation along the D dimension of P, which returns the label j indicates that the true depth value is in the bin B k,j . The new 2 bins at the (k + 1)-th search stage can be further generated by dividing B k,j into two equal-width bins B k+1,1 and B k+1,2 , and the corresponding three edges at this stage can be defined as:\n\ne k+1,1 = e k,j ; e k+1,2 = e k,j + e k,j+1 2 ; e k+1,3 = e k,j+1 . (5) Then new depth hypotheses are sampled from the center points of bins B k+1,1 and B k+1,2 for the (k + 1)-th stage. The initial bin width in the proposed binary search is R/2 and in the k-th stage, the bin width is R/2 k .\n\nWith our proposed binary search strategy, the depth dimension of the 3D cost volume can be decreased to 2, which pushes the cost volume size to an extremely low bound, and the memory footprint is dramatically decreased. In our experiments, the Binary Search for MVS achieves satisfactory results, outperforming several existing competitive methods, and the memory overhead of the whole MVS network becomes dominated by the 2D image encoder, no longer by the 3D cost volumes. However, as discussed in the Introduction, the issue of the network classification error can cause unstable optimization and degraded accuracy.\n\n\nGeneralized Binary Search for MVS\n\nTo handle the error accumulation and the training issue in the proposed Binary Search for MVS, we extend it to a Generalized Binary Search for MVS. Specifically, we further design three effective mechanisms which are error tolerance bins, gradient-masked optimization and efficient gradient updating mechanism, making substantial improvement over the Binary Search method. Error Tolerance Bins. After obtaining the selected bin B k,j in the k-th search stage, we first divide it into two new bins B k+1,1 and B k+1,2 for the next (k + 1)-th stage, as shown in Fig. 3. To make the network have a certain capability of tolerating prediction errors, we propose to respectively add one small bins on the left side of B k+1,1 and on the right side of B k+1,2 . This process is termed as Error Tolerance Bins (ETB). More formally, given D (D is an even number and small enough) as the final number of bins, we pad (D \u2212 2)/2 more bins to the two sides of the original two bins. After the padding, D new bins, i.e. {B k+1,j |j = 1, ..., D}, are obtained, as well as their corresponding bin edges, i.e. {e k+1,m |m = 1, ..., D + 1}. We still sample the center points as the depth hypotheses {d k+1,j |j = 1, ..., D} from these bins with Eq. 4. The error tolerance bins extend the sampling of depth hypotheses to a range out of the two original bins in the binary search, thus enabling the network to correct the predictions and to reduce error accumulation to some extent. Since the depth hypotheses number is now D, we also change the initialization of depth hypotheses in the first stage. As the initial depth range R in split into D bins, the initial bin width is R/D and in the k-th stage, the bin width is R/(D \u00d7 2 k\u22121 ). In our network implementation, we pad only 1 ETB on both sides. This leads to a depth hypothesis number of 4, i.e. D = 4. In the experiments, we observe dramatically improved depth prediction accuracy while notably, the memory consumption can be the same level as the original binary search, as the memory is still dominated by the 2D image encoder. Fig. 3 shows a real example of our GBi-Net. The hypothesis number D is set to 4. With the error tolerance bins, the network can predict a correct label of 4 when the true depth is in B 3,4 at the 3-th search stage, while the original binary search fails. Gradient-Masked Optimization. The proposed GBi-Net is trained in a supervised manner. The ground-truth labels are generated from the ground-truth depth map. In the k-th search stage, after we obtain the bins, we calculate which bin is occupied by the ground-truth depth value. Then we can convert the ground truth depth map into a ground truth occupancy volume G with one-hot encoding, which is further used for loss calculation. One problem in the iterative search is that the ground-truth depth values for some pixels may be out of the D bins. In this situation, no valid labels exist and the losses cannot be computed. This is a critical problem in network optimization. The coarseto-fine methods typically leverage a continuous regression loss, while existing MVS methods with a discrete classification loss [42] widely employ dense space discretization.\n\nIn our GBiNet, a designed mechanism to this problem is computing a mask map for each stage, based on the bins and ground-truth depth maps. If the ground-truth depth of a pixel is in the current bins, the pixel is considered as valid. Let the ground-truth depth for a pixel be d gt and the current bin edges be {e m |m = 1, ..., D + 1}, omitting the stage index for simplicity. Then the pixel is valid only if:\ne 1 \u2264 d gt < e D+1 .(6)\nOnly the loss gradients from the valid pixels are used to update the parameters in the network. The gradients from all the invalid pixels are not accumulated. With this process, we can train both Bi-Net and GBi-net successfully, as clearly confirmed by our experimental results. The gradient-masked optimization is similar to the popular selfpaced learning [25], in which at the very beginning, the network only involves easy samples (i.e. easy pixels) in training, while with the optimization proceeds, the network can predict more accurate labels for hard pixels, and most pixels will eventually participate in the learning process. As can be observed in Fig. 6c in the experiments, a large portion of pixels falls into the the current bins in our GBi-Net.\n\n\nNetwork Optimization\n\nLoss Function. Our loss function is a standard crossentropy loss that applies on the probability volume P and a ground truth occupancy volume G. A set of the valid pixels \u2126 q is first obtained by the valid mask map and then a mean loss of all valid pixels is computed as follows:\nLoss = q\u2208\u2126q D j=1\n\u2212G(j, q) log P(j, q)\n\nMemory-efficient Training. MVS methods with multiple stages [5,11] typically average the losses from all the stages   and back-propagate the gradients together. Nevertheless, this training strategy consumes significant memory because of the gradients accumulation across different stages. In our GBi-Net, we train our network in a more memory-efficient way. Specifically, we compute the loss and back-propagate the gradients immediately after each stage. The gradients are not accumulated across stages, and thus the maximum memory overhead does not exceed the stage with the largest scale. To make the training with multiple stages more stable, we first set the maximum number of search stages as 2, and gradually increase it as the epoch number increases.\n\n\nExperiments\n\n\nDatasets\n\nThe DTU dataset [2] is an indoor dataset with multiview images and camera poses. We Follow MVSNet [41] for dividing training and testing set. There are 27097 training samples in total. The BlendedMVS dataset [43] is a large-scale dataset with indoor and outdoor scenes. Following [22,34,45], we only use this dataset for training. There are 16904 training samples in total. Tanks and Temples [17] is a large-scale dataset with various outdoor scenes. It contains Intermediate subset and Advanced subset. The evaluation on this benchmark is conducted online by submitting generated point clouds to the official website.\n\n\nImplementation Details\n\nTraining Details. The proposed GBi-Net is trained on the DTU dataset for DTU benchmarking and trained on Blend-edMVS dataset for Tanks and Temples benchmarking, following [22,34,45]. We use the high-resolution DTU data provided by the open source code of MVSNet [41]. The original image size is 1200 \u00d7 1600. We first crop the input images into 1024 \u00d7 1280 following MVSNet [41]. Different from MVSNet [41] that directly downscale the image to 512 \u00d7 640, we propose an online random cropping data Table 1. Point cloud evaluation results on DTU [2]. The lower is better for Accuracy (Acc.), Completeness (Comp.), and Overall. The best result is highlighted in bold and the second in italic bold. * denotes ours without using random cropping data augmentation.\n\n\nMethod\n\nAcc  in the supplemental file. Fig. 4 and Fig. 5 are visualizations of depth maps and point clouds of our method.\n\n\nBenchmark Performance\n\nOverall Evaluation on DTU Dataset. We evaluate the results on the DTU testing set by two types of metrics. The first type of metric evaluates point clouds using official evaluation scripts of DTU [2]. It compares the distance between ground-truth point clouds and the produced point clouds.\n\nThe state-of-the-art comparison results are shown in Table 1. Our two models, GBi-Net and GBi-Net* both significantly improved the best performance on the completeness and the most important overall score (lower is better for both metrics). Our best model improves the overall score from 0.344 of UCSNet [5] to 0.289, while the memory is reduced by 48%. Note that our Bi-Net, i.e. the proposed Binary Search Network can also achieve comparable results to other dense search methods, clearly showing its effectiveness. The second type of metric directly evaluates the accuracy of the predicted depth maps. The depth ranges of DTU dataset are all 510 millimeters. Thus, we compute the depth accuracy, which counts the percentage of pixels whose absolute depth errors are less than a threshold, and 4 thresholds are considered in the evaluation (i.e. 0.125, 0.25, 0.5, 1, with millimeters as a unit). Compared to the depth range of 510 mm, these thresholds are extremely tight and challenging. The results of this type of metric are shown in Table 2. Our GBi-Net also obtains the best results on all the thresholds. The quality of depth maps also explains our best performance on point cloud evaluation.\n\nOverall Evaluation on Tanks and Temples. We train the proposed Bi-Net and GBi-Net on BlendedMVS [43], and testing on Tanks and Temples dataset. We compare our method to state-of-the-art methods. Table 3 shows results on both the Advanced subset and the Intermediate subset.\n\nOur GBi-Net achieves the best mean score of 37.32 (higher is better) on Advanced subset compared to all the competitors, and it performs the best on 4 out of the overall 6 scenes. Note that the Advanced subset contains differ-ent large-scale outdoor scenes. The results can fully confirm the effectiveness of our method. Table 3 also shows the evaluation results on the Intermediate subset. Our GBi-Net obtains highly comparable results to the state-of-the-art. Notably, with significantly less memory, our mean score is only 0.09 lower than AA-RMVSNet [34] and 0.26 lower than EPP-MVSNet [22]. Moreover, we also obtain state-ofthe-art scores on the Family and Francis scenes. Our binary search model Bi-Net also achieves satisfactory performance on both subsets. Our anonymous evaluation results on the leaderboard [1] are named as Bi-Net and GBi-Net.\n\n\nMemory Efficiency Comparison.\n\nWe compare the memory overhead with several previous best-performing learning-based MVS methods [5,11,33,34,41,44,45] on the DTU testing set. The memory usage evaluation is conducted with an image size of 1152 \u00d7 1600. We use pytorch functions 1 to measure the peak allocated memory usage of all the methods. Fig. 1a shows a comparison of the methods regarding memory usage and reconstruction error. Our GBi-Net shows a great improvement in the reconstruction quality while using much less memory. More specifically, the memory footprint is reduced by 77.5% compared to MVSNet [41], by 54.1% compared to CasMVSNet [11], by 82.4% to AA-RMVSNet [34], and by 55.9% to Vis-MVSNet [45]. Although the memory of our method is slightly 479MB larger than Patchmatchnet [33], shown in Table 1 \n\n\nModel Analysis\n\nEffect of Different Search Strategies. We first conduct a direct comparison on different search strategies as shown in Table 4, including dense linear search by regression (i.e. Dense LS), dense coarse-to-fine search by regression (i.e. Dense C2F), and our Bi-Net and GBi-Net search by discrete classification. In this comparison, Bi-Net and GBi-Net are trained without using the random cropping data aug-  Fig. 6a and Fig. 6b, the reconstruction results improve quickly from Stage 6 to 8 and then convergence, which indicates that our model can converge with a reasonably small stage number.  Effect of ETB Number. We evaluate our generalized bi- nary search with different numbers of error tolerance bins on DTU. We train 3 networks with 0, 1 and 2 ETBs on both sides, i.e. 2, 4 and 6 depth hypotheses respectively. All of the experiments run with the same setting. The evaluation results and the memory consumption are shown in Table 5. Fig. 6c and Fig. 6d also shows the valid pixel percentage and <1mm pixel percentage of different models in different stages. From 0 ETB to 1 ETB, we can observe a significant improvement in accuracy. This reveals the importance of ETBs. Note that the memory consumption of 1 ETB remains the same level as 0 ETB because the GPU memory is still dominated by the 2D image encoder. In Table 5, 2 ETBs model is slightly better than 1 ETB model on the depth map metric while consuming more memory. It also does not show better performance on the point cloud metric. Besides, in Fig. 6d, the valid and <1mm pixel percentages of 1 ETB and 2 ETBs models are very close. All these results reveal that 1 ETB in our GBi-Net is already sufficient to achieve a good balance between accuracy and memory usage. More importantly, further increasing ETBs may make the model more complex and harder to optimize. Effect of memory-efficient Training. We train a model without our Memory-efficient Training strategy (see Sec. 3.5) on the DTU dataset. This model averages the gradients accumulated from all the different stages and backpropagates them, which is a widely performed gradient updating scheme in existing MVS methods. As shown in Table 6, these two comparison models obtain very similar results on the depth performance, while with the proposed Memory-efficient Training, the memory consumption is largely reduced by 57.1%.\n\n\nConclusion\n\nIn this paper, we first presented a binary search network (Bi-Net) design for MVS to significantly reduce the memory footprint of 3D cost volumes. Based on this design, we further proposed a generalized binary search network (GBi-Net) containing three effective mechanisms, i.e. error tolerance bins, gradients masking, and efficient gradient updating. The GBi-Net can greatly improve the accuracy while maintaining the same memory usage as the Bi-Net. Experiments on challenging datasets also showed state-ofthe-art depth prediction accuracy, and remarkable memory efficiency of the proposed methods.\n\n\nDepth map Fusion\n\nAs indicated in the main paper, after obtaining the final depth maps of a scene, we filter and fuse depth maps into one point cloud. The final depth maps are generated from center points of the selected bins of the final stage. We consider both the photometric and the geometric consistency for depth map filtering. The geometric consistency is similar to MVSNet [41] measuring the depth consistency among multiple views. The photometric consistency, however, is different. The probability volume P is considered to construct the photometric consistency, following R-MVSNet [42]. As the probability volume is the classification probabilities for the depth hypotheses, it measures the matching quality of these hypotheses. Since the proposed method consists of K stages, we can obtain K probability volumes, i.e. {P k |k = 1, ..., K}. For each pixel p, its photometric consistency from its K probabilities can be calculated as follows:\nP h(p) = 1 K K k=1\nmax{P k (j, p)|j = 1, ..., D}. (8) Where P h(p) is the photometric consistency of pixel p; D is depth hypothesis number; The max operation obtains the classification probability of a selected hypothesis; K is the maximum stage considered in photometric consistency and 1 \u2264 K \u2264 K. Equation 8 actually computes an average of the probabilities of the K stages. In practice, when the maximum stage number K = 8, we set K = 6. It means that we take the average probability of the first 6 stages as the score of the photometric consistency. In our multi-stage search pipeline, as the resolutions of probability volumes are different, we upsample them to the maximum resolution of stage K before the computation. After producing the photometric consistency score for each pixel, the depths of pixels are discarded if their consistency scores are below a threshold. Figure 7a in the supplementary shows the results of each stage of a sample in the DTU dataset [2]. The depth map in each stage consists of the center-point depth values of selected bins. The quality of these depth maps can be improved quickly, demonstrating a fast search convergence of our method. The valid mask maps represent valid pixels in each search stage. Note that these mask maps are combined with the ground-truth mask maps from the dataset, and thus the background pixels are not considered. The photometric consistency (Photo. Consi.) map in stage k is computed using Equation 8 by setting K = k. As shown in the Figure 7a, the photometric consistency maps is an effective measurement of depth map quality. As shown in Figure 7b, the photometric consistency (Photo. Consi.) maps from Stage 6 are used to filter the final depth maps produced from Stage 8. The filtered depth maps are further refined by geometric consistency map, and finally fused into one point cloud. Figure 8 also shows qualitative results of a sample from the Tanks and Temples [17] dataset. The background of this image is far away from the foreground and is out of the depth range, so the MVS methods predict outlier values for the background pixels. Using the photometric consistency maps, we can effectively filter out these outliers.\n\n\nMore Visualization Results\n\nWe show more qualitative results of the proposed model in this section. Figure 9 and Figure 11 show several images and their corresponding depth maps in DTU dataset [2] and Tanks and Temples dataset [17] respectively. The depth maps are filtered by photometric consistency. Figure 10 and Figure 12 shows several point clouds of our method in DTU dataset [2] and Tanks and Temples dataset [17] respectively.    Figure 8. (a) The predicted depth maps and photometric consistency (Photo. Consi.) maps in all the stages of a sample in Tanks and Temples [17]. (b) The input image, final predicted depth map of Stage 8, photometric consistency (Photo. Consi.) map of Stage 6 and filtered depth map by photometric consistency. The background of this image is far away from the foreground and is out of the depth range so MVS methods will predicted outlier values for background pixels. With the photometric consistency, we can effectively filter out these outliers. Figure 9. Examples of images and their corresponding depth maps in DTU dataset [2]. The depth maps are filtered by photometric consistency. Figure 10. Point clouds of our method on DTU dataset [2]. Figure 11. Examples of images and their corresponding depth maps in Tanks and Temples dataset [17]. The depth maps are filtered by photometric consistency. Figure 12. Point clouds of our method on Tanks and Temples dataset [17].\n\nFigure 1 .\n1(a) Comparison with previous state-of-the-art learningbased MVS methods\n\nW\n\n\nFigure 4 .\n4Visualization of the final and filtered depth maps.\n\nFigure 5 .\n5Point clouds examples of our method on DTU[2] and Tanks and Temples[17].\n\n\nand 2, our method significantly outperforms it in both the point cloud (0.289 vs. 0.352) and depth map (12.77 vs. 8.113) evaluation by a large margin.\n\nFigure 6 .\n6(a) and (b) are evaluation results of different stage numbers. (c) and (d) are valid pixel percentage and <1mm percentage of different stage numbers of models with different ETBs.\n\nFigure 7 .\n7(a) The predicted depth maps, valid mask maps and photometric consistency (Photo. Consi.) maps in all the stages of a sample in DTU [2]. (b) The input image, final predicted depth map of Stage 8, photometric consistency (Photo. Consi.) map of Stage 6, filtered depth map by photometric consistency, ground truth depth map and fused point cloud.\n\n\n.1 and the 3D cost volume regularization in Sec. 3.2. Then, we elaborate on details about our proposed Binary Search for MVS and Generalized Binary Search for MVS in Sec. 3.3 and Sec. 3.4, respectively. Finally, we present the overall network optimization in Sec. 3.5.\n\n\nFigure 3. Illustration of Generalized Binary Search. We subdivide the selected bin into two bins according to the label. Then we pad Error Tolerance Bins (ETB) on both sides. We check the gradient mask of this pixel. Only if it is valid, the loss could participate in back-propagation.2 2 \n\n2 \n3 \n\n4 \n4 \n\n/4 \n\n/8 \n\n/16 \n\nStage 1 \n\nDepth hypothese \nPredicted labels GT depth \nDepth bins \nETBs \n\nUpdating & ETB \n\nUpdating & ETB \n\nUpdating & ETB \n\nStage 2 \n\nStage 3 \n\n\n\n\n.\u2193 Comp.\u2193 Overall\u2193 Mem. (MB)Tola [31] \n0.342 \n1.190 \n0.766 \n-\nGipuma [10] \n0.283 \n0.873 \n0.578 \n-\nMVSNet [41] \n0.396 \n0.527 \n0.462 \n9384 \nR-MVSNet [42] \n0.383 \n0.452 \n0.417 \n-\nCIDER [36] \n0.417 \n0.437 \n0.427 \n-\nPoint-MVSNet [4] \n0.342 \n0.411 \n0.376 \n-\nCasMVSNet [11] \n0.325 \n0.385 \n0.355 \n4591 \nUCS-Net [5] \n0.338 \n0.349 \n0.344 \n4057 \nCVP-MVSNet [39] 0.296 \n0.406 \n0.351 \n-\nVis-MVSNet [45] \n0.369 \n0.361 \n0.365 \n4775 \nPatchmatchNet [33] 0.427 \n0.277 \n0.352 \n1629 \nAA-RMVSNet [34] 0.376 \n0.339 \n0.357 \n11973 \nEPP-MVSNet [22] \n0.413 \n0.296 \n0.355 \n-\n\nBi-Net (ours) \n0.360 \n0.360 \n0.360 \n2108 \nGBi-Net* (ours) \n0.327 \n0.268 \n0.298 \n2108 \nGBi-Net (ours) \n0.315 \n0.262 \n0.289 \n2108 \n\nTable 2. Depth map evaluation results in terms of accuracy, and the \nmemory consumption on DTU [2]. The unit of these thresholds are \nall in millimeters. The higher is better. \n\nMethod \n<0.125\u2191 <0.25\u2191 <0.5\u2191 <1\u2191 Mem. (MB) \n\nMVSNet [41] \n8.539 \n16.85 \n32.02 53.61 \n9384 \nCasMVSNet [11] \n10.13 \n19.88 \n37.04 \n59.4 \n4591 \nPatchmatchnet [33] \n8.113 \n16.05 \n30.77 52.69 \n1629 \n\nGBi-Net (ours) \n12.77 \n24.89 \n45.1 \n65.94 \n2108 \n\naugmentation. We randomly crop images of 512\u00d7640 from \nimages of 1024 \u00d7 1280. The motivation is that cropping \nsmaller images from larger images could help to learn bet-\nter features for larger image scales without increasing the \ntraining overhead. When training on BlendedMVS dataset, \nwe use the original resolution of 576\u00d7768. For all the train-\ning, N = 5 input images are used, i.e. 1 reference image \nand 4 source images. We adopt the robust training strategy \nproposed in Patchmatchnet [33] for better learning of pixel-\nwise weights. The maximum stage number is set to 8. For \nevery 2 stages, we share the same feature map scale and the \n3D-CNN network parameters. The whole network is opti-\nmized by Adam optimizer in Pytorch for 16 epochs with an \ninitial learning rate of 0.0001, which is down-scaled by a \nfactor of 2 after 10, 12, and 14 epochs. The total training \nbatch size is 4 on two NVIDIA RTX 3090 GPUs. \nTesting Details. The model trained on DTU training set \nis used for testing on DTU testing set. The input image \nnumber N is set to 5, each with a resolution of 1152\u00d71600. \nIt takes 0.61 seconds for each testing sample. The model \ntrained on BlendedMVS dataset is used for testing on Tanks \nand Temples dataset. The image sizes are set to 1024 \u00d7 \n1920 or 1024 \u00d7 2048 to make the images divisible by 64. \nThe input image number N is set to 7. All the testings are \nconducted on an NVIDIA RTX 3090 GPU. We then filter \nand fuse depth maps of a scene into one point cloud, details \n\n\n\nTable 3 .\n3Point cloud evaluation results on the Advanced and Intermediate subsets of Tanks and Temples dataset[17]. Higher scores are better. The Mean is the average score of all scenes.Advanced \nIntermediate \n\nMethod \nMean Aud. \nBal. \nCou. Mus. \nPal. \nTem. Mean Fam. \nFra. \nHor. \nLig. \nM60 \nPan. \nPla. \nTra. \n\nMVSNet [41] \n-\n-\n-\n-\n-\n-\n-\n43.48 55.99 28.55 25.07 50.79 53.96 50.86 47.90 34.69 \nPoint-MVSNet [4] \n-\n-\n-\n-\n-\n-\n-\n48.27 61.79 41.15 34.20 50.79 51.97 50.85 52.38 43.06 \nUCSNet [5] \n-\n-\n-\n-\n-\n-\n-\n54.83 76.09 53.16 43.03 54.00 55.60 51.49 57.38 47.89 \nCasMVSNet [11] \n31.12 19.81 38.46 29.10 43.87 27.36 28.11 56.42 76.36 58.45 46.20 55.53 56.11 54.02 58.17 46.56 \nPatchmatchNet [33] 32.31 23.69 37.73 30.04 41.80 28.31 32.29 53.15 66.99 52.64 43.24 54.87 52.87 49.54 54.21 50.81 \nBP-MVSNet [28] \n31.35 20.44 35.87 29.63 43.33 27.93 30.91 57.60 77.31 60.90 47.89 58.26 56.00 51.54 58.47 50.41 \nVis-MVSNet [45] \n33.78 20.79 38.77 32.45 44.20 28.73 37.70 60.03 77.40 60.23 47.07 63.44 62.21 57.28 60.54 52.07 \nAA-RMVSNet [34] 33.53 20.96 40.15 32.05 46.01 29.28 32.71 61.51 77.77 59.53 51.53 64.02 64.05 59.47 60.85 54.90 \nEPP-MVSNet [22] \n35.72 21.28 39.74 35.34 49.21 30.00 38.75 61.68 77.86 60.54 52.96 62.33 61.69 60.34 62.44 55.30 \n\nBi-Net (ours) \n32.03 21.97 37.59 31.63 44.81 27.92 28.30 53.41 74.93 54.37 45.09 51.86 49.09 49.56 55.76 46.67 \nGBi-Net (ours) \n37.32 29.77 42.12 36.30 47.69 31.11 36.93 61.42 79.77 67.69 51.81 61.25 60.37 55.87 60.67 53.89 \n\n\n\nTable 4 .\n4Performance comparison of different search strategies for MVS on DTU[2]. LS indicates linear search, and C2F indicates coarse-to-fine search. Both of them perform search via regression in a dense manner.Method \nAcc. \u2193 Comp. \u2193 Overall \u2193 Mem (MB) \u2193 \n\nDense LS (Regression) \n0.396 \n0.527 \n0.462 \n9384 \nDense C2F (Regression) 0.325 \n0.385 \n0.355 \n4591 \nBi-Net (Classification) \n0.360 \n0.360 \n0.360 \n2108 \nGBi-Net (Classification) 0.327 \n0.268 \n0.298 \n2108 \n\nmentation described in Sec. 4.2. As we can observe from \nTable 4, our binary search networks achieve significantly \nbetter results than Dense LS and Dense C2F on both the \ndepth performance and the memory footprint, fully confirm-\ning the effectiveness of the proposed methods. \nEffect of Stage Number. We analyze the influence of the \nnumber of search stages in our method. We test our GBi-\nNet model on DTU dataset with a maximum stage number \nof 9. We compare the reconstruction results of Stage 6, 7, 8, \n9 with both the point cloud evaluation metrics and the depth \nmap evaluation metrics. As shown in \n\nTable 5 .\n5Evaluation on the number of Error Tolerance Bin (ETB), \nconsidering the performance of point cloud, depth map, and the \nmemory usage on DTU. 0, 1 and 2 indicate respectively adding 0, \n1, 2 ETBs on both sides of the search bins. \n\n# of ETBs. Overall \u2193 <0.125\u2191 <0.25\u2191 <0.5\u2191 <1\u2191 Mem.(MB) \n\n0 \n0.360 \n5.622 \n11.19 \n21.91 39.14 \n2108 \n1 \n0.298 \n11.75 \n22.92 \n41.62 60.99 \n2108 \n2 \n0.300 \n13.58 \n26.18 \n45.78 63.92 \n2438 \n\n\n\nTable 6 .\n6Point cloud evaluation and training memory overhead \ncomparison on DTU [2] for different gradient updating strategies. \nGBi-Net w/GC and GBi-Net w/EGU indicate the proposed GBi-\nNet with commonly used gradient accumulation and the proposed \nefficient gradient updating, respectively. \n\nMethod \nAcc. Comp. Overall Memory (MB) \n\nGBi-Net w/ GC \n0.320 0.277 \n0.299 \n12137 \nGBi-Net w/ EGU 0.326 0.269 \n0.298 \n5208 \n\n\nmax memory allocated and reset peak memory stats\nSupplementary materialIn this supplementary, we introduce details about the depth map fusion procedure and provide more qualitative results regarding the ablation study and the overall performance of the proposed model.\n. Temples Tanks, Leaderboard, Tanks and temples leaderboard. https : / / www . tanksandtemples.org/leaderboard. 7\n\nLarge-scale data for multiple-view stereopsis. Henrik Aanaes, George Rasmus Ramsb\u00f8l Jensen, Engin Vogiatzis, Anders Bjorholm Tola, Dahl, International Journal of Computer Vision. 120213Henrik Aanaes, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. International Journal of Computer Vision, 120(2):153-168, 2016. 1, 2, 6, 7, 8, 9, 10, 12, 13\n\nUsing multiple hypotheses to improve depth-maps for multi-view stereo. D F Neill, George Campbell, Carlos Vogiatzis, Roberto Hern\u00e1ndez, Cipolla, ECCV. SpringerNeill DF Campbell, George Vogiatzis, Carlos Hern\u00e1ndez, and Roberto Cipolla. Using multiple hypotheses to improve depth-maps for multi-view stereo. In ECCV. Springer, 2008. 2\n\nPoint-based multi-view stereo network. Rui Chen, Songfang Han, Jing Xu, Hao Su, ICCV. 7Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In ICCV, 2019. 1, 6, 7\n\nDeep stereo using adaptive thin volume representation with uncertainty awareness. Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, Hao Su, CVPR. 67Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adap- tive thin volume representation with uncertainty awareness. In CVPR, June 2020. 1, 2, 3, 4, 5, 6, 7\n\nA volumetric method for building complex models from range images. Brian Curless, Marc Levoy, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. the 23rd annual conference on Computer graphics and interactive techniquesBrian Curless and Marc Levoy. A volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 303-312, 1996. 2\n\nDeformable convolutional networks. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei, ICCV. Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, 2017. 3\n\nMulti-view stereo: A tutorial. Yasutaka Furukawa, Carlos Hern\u00e1ndez, Found. Trends. Comput. Graph. Vis. 91-2Yasutaka Furukawa and Carlos Hern\u00e1ndez. Multi-view stereo: A tutorial. Found. Trends. Comput. Graph. Vis., 9(1-2):1-148, June 2015. 1\n\nAccurate, dense, and robust multiview stereopsis. Yasutaka Furukawa, Jean Ponce, TPAMI. 328Yasutaka Furukawa and Jean Ponce. Accurate, dense, and ro- bust multiview stereopsis. TPAMI, 32(8):1362-1376, 2009. 1, 2\n\nMassively parallel multiview stereopsis by surface normal diffusion. Silvano Galliani, Katrin Lasinger, Konrad Schindler, ICCV. 6Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In ICCV, 2015. 1, 2, 6\n\nFeitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, CVPR. 67Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In CVPR, 2020. 1, 2, 3, 4, 5, 6, 7\n\nGroup-wise correlation stereo network. Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, Hongsheng Li, CVPR. Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and Hongsheng Li. Group-wise correlation stereo network. In CVPR, 2019. 4\n\nDeepmvs: Learning multi-view stereopsis. Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, Jia-Bin Huang, CVPR. Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In CVPR, June 2018. 2\n\nDpsnet: End-to-end deep plane sweep stereo. Sunghoon Im, Hae-Gon Jeon, Stephen Lin, In-So Kweon, ICLR. Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In-So Kweon. Dpsnet: End-to-end deep plane sweep stereo. In ICLR, 2019. 1\n\nSurfacenet: An end-to-end 3d neural network for multiview stereopsis. Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, Lu Fang, ICCV. Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d neural network for mul- tiview stereopsis. In ICCV, 2017. 2\n\nPoisson surface reconstruction. Michael Kazhdan, Matthew Bolitho, Hugues Hoppe, Proceedings of the fourth Eurographics symposium on Geometry processing. the fourth Eurographics symposium on Geometry processing7Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics symposium on Geometry processing, volume 7, 2006. 2\n\nTanks and temples: Benchmarking large-scale scene reconstruction. Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, Vladlen Koltun, ACM ToG. 36414Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM ToG, 36(4):1-13, 2017. 2, 6, 7, 9, 11, 14\n\nA theory of shape by space carving. N Kiriakos, Kutulakos, M Steven, Seitz, IJCV. 383Kiriakos N Kutulakos and Steven M Seitz. A theory of shape by space carving. IJCV, 38(3):199-218, 2000. 2\n\nA quasi-dense approach to surface reconstruction from uncalibrated images. Maxime Lhuillier, Long Quan, TPAMI27Maxime Lhuillier and Long Quan. A quasi-dense approach to surface reconstruction from uncalibrated images. TPAMI, 27(3):418-433, 2005. 2\n\nKaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, CVPR. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 3\n\nP-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, Yawei Luo, ICCV. Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In ICCV, 2019. 1\n\nEpp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo. Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, Fan Yu, ICCV. 67Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo. In ICCV, 2021. 6, 7\n\nReal-time visibility-based fusion of depth maps. Paul Merrell, Amir Akbarzadeh, Liang Wang, Philippos Mordohai, Jan-Michael Frahm, Ruigang Yang, David Nist\u00e9r, Marc Pollefeys, ICCV. Paul Merrell, Amir Akbarzadeh, Liang Wang, Philippos Mordohai, Jan-Michael Frahm, Ruigang Yang, David Nist\u00e9r, and Marc Pollefeys. Real-time visibility-based fusion of depth maps. In ICCV, 2007. 2\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, MICCAI. SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI. Springer, 2015. 4\n\nSelf paced deep learning for weakly supervised object detection. Enver Sangineto, Moin Nabi, Dubravko Culibrk, Nicu Sebe, TPAMI. 413Enver Sangineto, Moin Nabi, Dubravko Culibrk, and Nicu Sebe. Self paced deep learning for weakly supervised object detection. TPAMI, 41(3):712-725, 2018. 5\n\nPixelwise view selection for unstructured multi-view stereo. L Johannes, Enliang Sch\u00f6nberger, Jan-Michael Zheng, Marc Frahm, Pollefeys, ECCV. Springer1Johannes L Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In ECCV. Springer, 2016. 1, 2\n\nPhotorealistic scene reconstruction by voxel coloring. M Steven, Charles R Seitz, Dyer, IJCV. 352Steven M Seitz and Charles R Dyer. Photorealistic scene re- construction by voxel coloring. IJCV, 35(2):151-173, 1999. 2\n\nBpmvsnet: Belief-propagation-layers for multi-view-stereo. Christian Sormann, Patrick Kn\u00f6belreiter, Andreas Kuhn, Mattia Rossi, Thomas Pock, Friedrich Fraundorfer, 3DV. IEEE, 2020. 7Christian Sormann, Patrick Kn\u00f6belreiter, Andreas Kuhn, Mattia Rossi, Thomas Pock, and Friedrich Fraundorfer. Bp- mvsnet: Belief-propagation-layers for multi-view-stereo. In 3DV. IEEE, 2020. 7\n\nA skeleton-bridged deep learning approach for generating meshes of complex topologies from single rgb images. Jiapeng Tang, Xiaoguang Han, Junyi Pan, Kui Jia, Xin Tong, CVPR. Jiapeng Tang, Xiaoguang Han, Junyi Pan, Kui Jia, and Xin Tong. A skeleton-bridged deep learning approach for gener- ating meshes of complex topologies from single rgb images. In CVPR, 2019. 2\n\nSkeletonnet: A topology-preserving solution for learning mesh reconstruction of object surfaces from rgb images. Jiapeng Tang, Xiaoguang Han, Mingkui Tan, Xin Tong, Kui Jia, TPAMI. 2Jiapeng Tang, Xiaoguang Han, Mingkui Tan, Xin Tong, and Kui Jia. Skeletonnet: A topology-preserving solution for learning mesh reconstruction of object surfaces from rgb im- ages. TPAMI, 2021. 2\n\nEfficient large-scale multi-view stereo for ultra high-resolution image sets. Machine Vision and Applications. Engin Tola, Christoph Strecha, Pascal Fua, 23Engin Tola, Christoph Strecha, and Pascal Fua. Efficient large-scale multi-view stereo for ultra high-resolution im- age sets. Machine Vision and Applications, 23(5):903-920, 2012. 6\n\nHigh accuracy and visibility-consistent dense multiview stereo. Hoang-Hiep, Patrick Vu, Jean-Philippe Labatut, Renaud Pons, Keriven, TPAMI. 345Hoang-Hiep Vu, Patrick Labatut, Jean-Philippe Pons, and Renaud Keriven. High accuracy and visibility-consistent dense multiview stereo. TPAMI, 34(5):889-901, 2011. 1\n\nPatchmatchnet: Learned multi-view patchmatch stereo. Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, Marc Pollefeys, CVPR. 67Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch stereo. In CVPR, 2021. 1, 2, 3, 4, 6, 7\n\nAa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network. Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, Guoping Wang, ICCV. Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network. In ICCV, October 2021. 1, 2, 3, 6, 7\n\nMulti-scale geometric consistency guided multi-view stereo. Qingshan Xu, Wenbing Tao, CVPR. 1Qingshan Xu and Wenbing Tao. Multi-scale geometric con- sistency guided multi-view stereo. In CVPR, 2019. 1, 2\n\nLearning inverse depth regression for multi-view stereo with correlation cost volume. Qingshan Xu, Wenbing Tao, AAAI. 34Qingshan Xu and Wenbing Tao. Learning inverse depth re- gression for multi-view stereo with correlation cost volume. In AAAI, volume 34, 2020. 6\n\nPvsnet: Pixelwise visibilityaware multi-view stereo network. Qingshan Xu, Wenbing Tao, 24Qingshan Xu and Wenbing Tao. Pvsnet: Pixelwise visibility- aware multi-view stereo network, 2020. 2, 4\n\nDense hybrid recurrent multi-view stereo net with dynamic consistency checking. Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, Yu-Wing Tai, ECCV. Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-view stereo net with dy- namic consistency checking. In ECCV, 2020. 3\n\nCost volume pyramid based depth inference for multi-view stereo. Jiayu Yang, Wei Mao, M Jose, Miaomiao Alvarez, Liu, CVPR. 36Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In CVPR, 2020. 3, 6\n\nMvs2d: Efficient multi-view stereo via attention-driven 2d convolutions. Zhenpei Yang, Zhile Ren, Qi Shan, Qixing Huang, arXiv:2104.13325arXiv preprintZhenpei Yang, Zhile Ren, Qi Shan, and Qixing Huang. Mvs2d: Efficient multi-view stereo via attention-driven 2d convolutions. arXiv preprint arXiv:2104.13325, 2021. 1\n\nMvsnet: Depth inference for unstructured multi-view stereo. Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, Long Quan, ECCV. 79Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In ECCV, 2018. 1, 2, 4, 6, 7, 9\n\nRecurrent mvsnet for high-resolution multiview stereo depth inference. Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, Long Quan, CVPR. 69Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi- view stereo depth inference. In CVPR, 2019. 1, 3, 4, 5, 6, 9\n\nBlendedmvs: A largescale dataset for generalized multi-view stereo networks. Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, Long Quan, CVPR. 67Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large- scale dataset for generalized multi-view stereo networks. In CVPR, 2020. 6, 7\n\nFast-mvsnet: Sparse-todense multi-view stereo with learned propagation and gaussnewton refinement. Zehao Yu, Shenghua Gao, CVPR. 17Zehao Yu and Shenghua Gao. Fast-mvsnet: Sparse-to- dense multi-view stereo with learned propagation and gauss- newton refinement. In CVPR, June 2020. 1, 7\n\nVisibility-aware multi-view stereo network. Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, Tian Fang, 7Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network, 2020. 1, 2, 6, 7\n", "annotations": {"author": "[{\"end\":147,\"start\":76},{\"end\":240,\"start\":148},{\"end\":324,\"start\":241}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":85},{\"end\":156,\"start\":151},{\"end\":247,\"start\":245}]", "author_first_name": "[{\"end\":84,\"start\":76},{\"end\":150,\"start\":148},{\"end\":244,\"start\":241}]", "author_affiliation": "[{\"end\":146,\"start\":89},{\"end\":239,\"start\":182},{\"end\":323,\"start\":266}]", "title": "[{\"end\":73,\"start\":1},{\"end\":397,\"start\":325}]", "venue": null, "abstract": "[{\"end\":2203,\"start\":399}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2389,\"start\":2386},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2392,\"start\":2389},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2395,\"start\":2392},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2398,\"start\":2395},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2401,\"start\":2398},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2429,\"start\":2426},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2432,\"start\":2429},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2435,\"start\":2432},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2438,\"start\":2435},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2441,\"start\":2438},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2444,\"start\":2441},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2447,\"start\":2444},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2458,\"start\":2455},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2797,\"start\":2794},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2996,\"start\":2992},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2999,\"start\":2996},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3002,\"start\":2999},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3005,\"start\":3002},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3129,\"start\":3125},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3146,\"start\":3142},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3513,\"start\":3510},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3515,\"start\":3513},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3518,\"start\":3515},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3711,\"start\":3707},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3845,\"start\":3842},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3847,\"start\":3845},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3850,\"start\":3847},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3893,\"start\":3889},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3896,\"start\":3893},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4607,\"start\":4603},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4757,\"start\":4754},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4760,\"start\":4757},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7292,\"start\":7289},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7317,\"start\":7313},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7529,\"start\":7526},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7566,\"start\":7562},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8584,\"start\":8580},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8587,\"start\":8584},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8604,\"start\":8601},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8607,\"start\":8604},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8617,\"start\":8614},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8620,\"start\":8617},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8623,\"start\":8620},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8625,\"start\":8623},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8644,\"start\":8641},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8647,\"start\":8644},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8650,\"start\":8647},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8663,\"start\":8659},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8666,\"start\":8663},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8669,\"start\":8666},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8845,\"start\":8841},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8934,\"start\":8930},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9480,\"start\":9476},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9483,\"start\":9480},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9486,\"start\":9483},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9489,\"start\":9486},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9681,\"start\":9677},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9684,\"start\":9681},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9687,\"start\":9684},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9690,\"start\":9687},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9727,\"start\":9723},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10469,\"start\":10465},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10472,\"start\":10469},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10475,\"start\":10472},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10849,\"start\":10846},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10852,\"start\":10849},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10855,\"start\":10852},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10967,\"start\":10963},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11189,\"start\":11185},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12860,\"start\":12859},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12865,\"start\":12861},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13135,\"start\":13132},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13568,\"start\":13564},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13886,\"start\":13882},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13889,\"start\":13886},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14353,\"start\":14349},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14356,\"start\":14353},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14359,\"start\":14356},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14647,\"start\":14643},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15344,\"start\":15340},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15347,\"start\":15344},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15506,\"start\":15502},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15509,\"start\":15506},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15833,\"start\":15829},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15836,\"start\":15833},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15839,\"start\":15836},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16035,\"start\":16032},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16038,\"start\":16035},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21899,\"start\":21895},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22738,\"start\":22734},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23543,\"start\":23540},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23546,\"start\":23543},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24283,\"start\":24280},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24366,\"start\":24362},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24476,\"start\":24472},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24548,\"start\":24544},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24551,\"start\":24548},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24554,\"start\":24551},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24660,\"start\":24656},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25084,\"start\":25080},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25087,\"start\":25084},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25090,\"start\":25087},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25175,\"start\":25171},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25286,\"start\":25282},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25314,\"start\":25310},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25455,\"start\":25452},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26015,\"start\":26012},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26415,\"start\":26412},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27410,\"start\":27406},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28142,\"start\":28138},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28178,\"start\":28174},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28404,\"start\":28401},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28570,\"start\":28567},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28573,\"start\":28570},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28576,\"start\":28573},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28579,\"start\":28576},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28582,\"start\":28579},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28585,\"start\":28582},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28588,\"start\":28585},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29051,\"start\":29047},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29088,\"start\":29084},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29117,\"start\":29113},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":29150,\"start\":29146},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29234,\"start\":29230},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32629,\"start\":32625},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32840,\"start\":32836},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34171,\"start\":34168},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35139,\"start\":35135},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35594,\"start\":35591},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35629,\"start\":35625},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35783,\"start\":35780},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35818,\"start\":35814},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35979,\"start\":35975},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36467,\"start\":36464},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36581,\"start\":36578},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":36681,\"start\":36677},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":36810,\"start\":36806},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37024,\"start\":37021},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":37050,\"start\":37046},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":41221,\"start\":41217},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":42662,\"start\":42659}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36896,\"start\":36812},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36900,\"start\":36897},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36965,\"start\":36901},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37051,\"start\":36966},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37204,\"start\":37052},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37397,\"start\":37205},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37755,\"start\":37398},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38026,\"start\":37756},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38494,\"start\":38027},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":41104,\"start\":38495},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42578,\"start\":41105},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43652,\"start\":42579},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44083,\"start\":43653},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":44507,\"start\":44084}]", "paragraph": "[{\"end\":2652,\"start\":2219},{\"end\":4128,\"start\":2654},{\"end\":5168,\"start\":4130},{\"end\":6161,\"start\":5170},{\"end\":7581,\"start\":6163},{\"end\":7626,\"start\":7583},{\"end\":7823,\"start\":7628},{\"end\":8130,\"start\":7825},{\"end\":8325,\"start\":8132},{\"end\":10342,\"start\":8342},{\"end\":10714,\"start\":10344},{\"end\":11407,\"start\":10716},{\"end\":11789,\"start\":11409},{\"end\":13266,\"start\":11815},{\"end\":13977,\"start\":13481},{\"end\":14505,\"start\":13979},{\"end\":14903,\"start\":14560},{\"end\":15246,\"start\":14955},{\"end\":15348,\"start\":15263},{\"end\":15776,\"start\":15442},{\"end\":16270,\"start\":15802},{\"end\":17231,\"start\":16272},{\"end\":17807,\"start\":17273},{\"end\":18102,\"start\":17809},{\"end\":18722,\"start\":18104},{\"end\":21941,\"start\":18760},{\"end\":22352,\"start\":21943},{\"end\":23135,\"start\":22377},{\"end\":23439,\"start\":23160},{\"end\":23478,\"start\":23458},{\"end\":24237,\"start\":23480},{\"end\":24882,\"start\":24264},{\"end\":25666,\"start\":24909},{\"end\":25790,\"start\":25677},{\"end\":26106,\"start\":25816},{\"end\":27308,\"start\":26108},{\"end\":27583,\"start\":27310},{\"end\":28437,\"start\":27585},{\"end\":29253,\"start\":28471},{\"end\":31625,\"start\":29272},{\"end\":32241,\"start\":31640},{\"end\":33196,\"start\":32262},{\"end\":35395,\"start\":33216},{\"end\":36811,\"start\":35426}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13451,\"start\":13284},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14559,\"start\":14506},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14954,\"start\":14904},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15262,\"start\":15247},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15441,\"start\":15349},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17272,\"start\":17232},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22376,\"start\":22353},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23457,\"start\":23440},{\"attributes\":{\"id\":\"formula_9\"},\"end\":33215,\"start\":33197}]", "table_ref": "[{\"end\":25412,\"start\":25405},{\"end\":27154,\"start\":27147},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27512,\"start\":27505},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27913,\"start\":27906},{\"end\":29252,\"start\":29245},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":29398,\"start\":29391},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30210,\"start\":30203},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31439,\"start\":31432}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2217,\"start\":2205},{\"attributes\":{\"n\":\"2.\"},\"end\":8340,\"start\":8328},{\"attributes\":{\"n\":\"3.\"},\"end\":11813,\"start\":11792},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13283,\"start\":13269},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13479,\"start\":13453},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15800,\"start\":15779},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18758,\"start\":18725},{\"attributes\":{\"n\":\"3.5.\"},\"end\":23158,\"start\":23138},{\"attributes\":{\"n\":\"4.\"},\"end\":24251,\"start\":24240},{\"attributes\":{\"n\":\"4.1.\"},\"end\":24262,\"start\":24254},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24907,\"start\":24885},{\"end\":25675,\"start\":25669},{\"attributes\":{\"n\":\"4.3.\"},\"end\":25814,\"start\":25793},{\"end\":28469,\"start\":28440},{\"attributes\":{\"n\":\"4.4.\"},\"end\":29270,\"start\":29256},{\"attributes\":{\"n\":\"5.\"},\"end\":31638,\"start\":31628},{\"attributes\":{\"n\":\"1.\"},\"end\":32260,\"start\":32244},{\"attributes\":{\"n\":\"2.\"},\"end\":35424,\"start\":35398},{\"end\":36823,\"start\":36813},{\"end\":36899,\"start\":36898},{\"end\":36912,\"start\":36902},{\"end\":36977,\"start\":36967},{\"end\":37216,\"start\":37206},{\"end\":37409,\"start\":37399},{\"end\":41115,\"start\":41106},{\"end\":42589,\"start\":42580},{\"end\":43663,\"start\":43654},{\"end\":44094,\"start\":44085}]", "table": "[{\"end\":38494,\"start\":38314},{\"end\":41104,\"start\":38525},{\"end\":42578,\"start\":41293},{\"end\":43652,\"start\":42794},{\"end\":44083,\"start\":43665},{\"end\":44507,\"start\":44096}]", "figure_caption": "[{\"end\":36896,\"start\":36825},{\"end\":36965,\"start\":36914},{\"end\":37051,\"start\":36979},{\"end\":37204,\"start\":37054},{\"end\":37397,\"start\":37218},{\"end\":37755,\"start\":37411},{\"end\":38026,\"start\":37758},{\"end\":38314,\"start\":38029},{\"end\":38525,\"start\":38497},{\"end\":41293,\"start\":41117},{\"end\":42794,\"start\":42591}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4582,\"start\":4575},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5436,\"start\":5429},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7579,\"start\":7572},{\"end\":9856,\"start\":9848},{\"end\":11997,\"start\":11991},{\"end\":19326,\"start\":19320},{\"end\":20834,\"start\":20828},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":23041,\"start\":23034},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25714,\"start\":25708},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":25725,\"start\":25719},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28786,\"start\":28779},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29686,\"start\":29679},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29698,\"start\":29691},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30219,\"start\":30212},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30231,\"start\":30224},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":30791,\"start\":30784},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":34083,\"start\":34074},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":34709,\"start\":34700},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":34815,\"start\":34806},{\"end\":35064,\"start\":35056},{\"end\":35506,\"start\":35498},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35520,\"start\":35511},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35709,\"start\":35700},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35723,\"start\":35714},{\"end\":35844,\"start\":35836},{\"end\":36393,\"start\":36385},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36534,\"start\":36525},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36592,\"start\":36583},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36748,\"start\":36739}]", "bib_author_first_name": "[{\"end\":44786,\"start\":44779},{\"end\":44945,\"start\":44939},{\"end\":44960,\"start\":44954},{\"end\":44989,\"start\":44984},{\"end\":45016,\"start\":45001},{\"end\":45384,\"start\":45383},{\"end\":45386,\"start\":45385},{\"end\":45400,\"start\":45394},{\"end\":45417,\"start\":45411},{\"end\":45436,\"start\":45429},{\"end\":45688,\"start\":45685},{\"end\":45703,\"start\":45695},{\"end\":45713,\"start\":45709},{\"end\":45721,\"start\":45718},{\"end\":45927,\"start\":45923},{\"end\":45942,\"start\":45935},{\"end\":45953,\"start\":45947},{\"end\":45965,\"start\":45959},{\"end\":45972,\"start\":45970},{\"end\":45978,\"start\":45973},{\"end\":45987,\"start\":45983},{\"end\":46004,\"start\":46001},{\"end\":46304,\"start\":46299},{\"end\":46318,\"start\":46314},{\"end\":46747,\"start\":46741},{\"end\":46759,\"start\":46753},{\"end\":46769,\"start\":46764},{\"end\":46779,\"start\":46777},{\"end\":46791,\"start\":46784},{\"end\":46802,\"start\":46799},{\"end\":46813,\"start\":46807},{\"end\":46999,\"start\":46991},{\"end\":47016,\"start\":47010},{\"end\":47260,\"start\":47252},{\"end\":47275,\"start\":47271},{\"end\":47491,\"start\":47484},{\"end\":47508,\"start\":47502},{\"end\":47525,\"start\":47519},{\"end\":47808,\"start\":47800},{\"end\":47819,\"start\":47813},{\"end\":47829,\"start\":47825},{\"end\":47842,\"start\":47835},{\"end\":48093,\"start\":48085},{\"end\":48102,\"start\":48099},{\"end\":48114,\"start\":48109},{\"end\":48129,\"start\":48121},{\"end\":48145,\"start\":48136},{\"end\":48329,\"start\":48323},{\"end\":48342,\"start\":48337},{\"end\":48359,\"start\":48351},{\"end\":48374,\"start\":48366},{\"end\":48389,\"start\":48382},{\"end\":48597,\"start\":48589},{\"end\":48609,\"start\":48602},{\"end\":48623,\"start\":48616},{\"end\":48634,\"start\":48629},{\"end\":48843,\"start\":48837},{\"end\":48855,\"start\":48848},{\"end\":48869,\"start\":48862},{\"end\":48882,\"start\":48877},{\"end\":48890,\"start\":48888},{\"end\":49096,\"start\":49089},{\"end\":49113,\"start\":49106},{\"end\":49129,\"start\":49123},{\"end\":49516,\"start\":49512},{\"end\":49534,\"start\":49528},{\"end\":49548,\"start\":49541},{\"end\":49562,\"start\":49555},{\"end\":49798,\"start\":49797},{\"end\":49821,\"start\":49820},{\"end\":50034,\"start\":50028},{\"end\":50050,\"start\":50046},{\"end\":50308,\"start\":50300},{\"end\":50319,\"start\":50314},{\"end\":50332,\"start\":50328},{\"end\":50599,\"start\":50593},{\"end\":50608,\"start\":50605},{\"end\":50619,\"start\":50615},{\"end\":50631,\"start\":50624},{\"end\":50644,\"start\":50639},{\"end\":50904,\"start\":50898},{\"end\":50912,\"start\":50909},{\"end\":50924,\"start\":50919},{\"end\":50938,\"start\":50931},{\"end\":50949,\"start\":50946},{\"end\":50959,\"start\":50956},{\"end\":51194,\"start\":51190},{\"end\":51208,\"start\":51204},{\"end\":51226,\"start\":51221},{\"end\":51242,\"start\":51233},{\"end\":51264,\"start\":51253},{\"end\":51279,\"start\":51272},{\"end\":51291,\"start\":51286},{\"end\":51304,\"start\":51300},{\"end\":51588,\"start\":51584},{\"end\":51609,\"start\":51602},{\"end\":51625,\"start\":51619},{\"end\":51865,\"start\":51860},{\"end\":51881,\"start\":51877},{\"end\":51896,\"start\":51888},{\"end\":51910,\"start\":51906},{\"end\":52146,\"start\":52145},{\"end\":52164,\"start\":52157},{\"end\":52189,\"start\":52178},{\"end\":52201,\"start\":52197},{\"end\":52461,\"start\":52460},{\"end\":52479,\"start\":52470},{\"end\":52692,\"start\":52683},{\"end\":52709,\"start\":52702},{\"end\":52731,\"start\":52724},{\"end\":52744,\"start\":52738},{\"end\":52758,\"start\":52752},{\"end\":52774,\"start\":52765},{\"end\":53116,\"start\":53109},{\"end\":53132,\"start\":53123},{\"end\":53143,\"start\":53138},{\"end\":53152,\"start\":53149},{\"end\":53161,\"start\":53158},{\"end\":53487,\"start\":53480},{\"end\":53503,\"start\":53494},{\"end\":53516,\"start\":53509},{\"end\":53525,\"start\":53522},{\"end\":53535,\"start\":53532},{\"end\":53861,\"start\":53856},{\"end\":53877,\"start\":53868},{\"end\":53893,\"start\":53887},{\"end\":54168,\"start\":54161},{\"end\":54186,\"start\":54173},{\"end\":54202,\"start\":54196},{\"end\":54458,\"start\":54448},{\"end\":54472,\"start\":54465},{\"end\":54492,\"start\":54483},{\"end\":54505,\"start\":54500},{\"end\":54520,\"start\":54516},{\"end\":54792,\"start\":54784},{\"end\":54806,\"start\":54798},{\"end\":54816,\"start\":54812},{\"end\":54828,\"start\":54822},{\"end\":54842,\"start\":54835},{\"end\":55100,\"start\":55092},{\"end\":55112,\"start\":55105},{\"end\":55331,\"start\":55323},{\"end\":55343,\"start\":55336},{\"end\":55572,\"start\":55564},{\"end\":55584,\"start\":55577},{\"end\":55784,\"start\":55776},{\"end\":55798,\"start\":55790},{\"end\":55811,\"start\":55804},{\"end\":55822,\"start\":55816},{\"end\":55834,\"start\":55829},{\"end\":55848,\"start\":55842},{\"end\":55862,\"start\":55855},{\"end\":55876,\"start\":55869},{\"end\":56168,\"start\":56163},{\"end\":56178,\"start\":56175},{\"end\":56185,\"start\":56184},{\"end\":56200,\"start\":56192},{\"end\":56444,\"start\":56437},{\"end\":56456,\"start\":56451},{\"end\":56464,\"start\":56462},{\"end\":56477,\"start\":56471},{\"end\":56745,\"start\":56742},{\"end\":56756,\"start\":56751},{\"end\":56768,\"start\":56762},{\"end\":56777,\"start\":56773},{\"end\":56788,\"start\":56784},{\"end\":57027,\"start\":57024},{\"end\":57038,\"start\":57033},{\"end\":57050,\"start\":57044},{\"end\":57062,\"start\":57055},{\"end\":57073,\"start\":57069},{\"end\":57084,\"start\":57080},{\"end\":57356,\"start\":57353},{\"end\":57367,\"start\":57362},{\"end\":57379,\"start\":57373},{\"end\":57392,\"start\":57384},{\"end\":57405,\"start\":57400},{\"end\":57414,\"start\":57411},{\"end\":57425,\"start\":57421},{\"end\":57436,\"start\":57432},{\"end\":57749,\"start\":57744},{\"end\":57762,\"start\":57754},{\"end\":57984,\"start\":57976},{\"end\":57995,\"start\":57992},{\"end\":58007,\"start\":58001},{\"end\":58017,\"start\":58012},{\"end\":58027,\"start\":58023}]", "bib_author_last_name": "[{\"end\":44792,\"start\":44787},{\"end\":44805,\"start\":44794},{\"end\":44952,\"start\":44946},{\"end\":44982,\"start\":44961},{\"end\":44999,\"start\":44990},{\"end\":45021,\"start\":45017},{\"end\":45027,\"start\":45023},{\"end\":45392,\"start\":45387},{\"end\":45409,\"start\":45401},{\"end\":45427,\"start\":45418},{\"end\":45446,\"start\":45437},{\"end\":45455,\"start\":45448},{\"end\":45693,\"start\":45689},{\"end\":45707,\"start\":45704},{\"end\":45716,\"start\":45714},{\"end\":45724,\"start\":45722},{\"end\":45933,\"start\":45928},{\"end\":45945,\"start\":45943},{\"end\":45957,\"start\":45954},{\"end\":45968,\"start\":45966},{\"end\":45981,\"start\":45979},{\"end\":45999,\"start\":45988},{\"end\":46007,\"start\":46005},{\"end\":46312,\"start\":46305},{\"end\":46324,\"start\":46319},{\"end\":46751,\"start\":46748},{\"end\":46762,\"start\":46760},{\"end\":46775,\"start\":46770},{\"end\":46782,\"start\":46780},{\"end\":46797,\"start\":46792},{\"end\":46805,\"start\":46803},{\"end\":46817,\"start\":46814},{\"end\":47008,\"start\":47000},{\"end\":47026,\"start\":47017},{\"end\":47269,\"start\":47261},{\"end\":47281,\"start\":47276},{\"end\":47500,\"start\":47492},{\"end\":47517,\"start\":47509},{\"end\":47535,\"start\":47526},{\"end\":47811,\"start\":47809},{\"end\":47823,\"start\":47820},{\"end\":47833,\"start\":47830},{\"end\":47846,\"start\":47843},{\"end\":48097,\"start\":48094},{\"end\":48107,\"start\":48103},{\"end\":48119,\"start\":48115},{\"end\":48134,\"start\":48130},{\"end\":48148,\"start\":48146},{\"end\":48335,\"start\":48330},{\"end\":48349,\"start\":48343},{\"end\":48364,\"start\":48360},{\"end\":48380,\"start\":48375},{\"end\":48395,\"start\":48390},{\"end\":48600,\"start\":48598},{\"end\":48614,\"start\":48610},{\"end\":48627,\"start\":48624},{\"end\":48640,\"start\":48635},{\"end\":48846,\"start\":48844},{\"end\":48860,\"start\":48856},{\"end\":48875,\"start\":48870},{\"end\":48886,\"start\":48883},{\"end\":48895,\"start\":48891},{\"end\":49104,\"start\":49097},{\"end\":49121,\"start\":49114},{\"end\":49135,\"start\":49130},{\"end\":49526,\"start\":49517},{\"end\":49539,\"start\":49535},{\"end\":49553,\"start\":49549},{\"end\":49569,\"start\":49563},{\"end\":49807,\"start\":49799},{\"end\":49818,\"start\":49809},{\"end\":49828,\"start\":49822},{\"end\":49835,\"start\":49830},{\"end\":50044,\"start\":50035},{\"end\":50055,\"start\":50051},{\"end\":50312,\"start\":50309},{\"end\":50326,\"start\":50320},{\"end\":50341,\"start\":50333},{\"end\":50603,\"start\":50600},{\"end\":50613,\"start\":50609},{\"end\":50622,\"start\":50620},{\"end\":50637,\"start\":50632},{\"end\":50648,\"start\":50645},{\"end\":50907,\"start\":50905},{\"end\":50917,\"start\":50913},{\"end\":50929,\"start\":50925},{\"end\":50944,\"start\":50939},{\"end\":50954,\"start\":50950},{\"end\":50962,\"start\":50960},{\"end\":51202,\"start\":51195},{\"end\":51219,\"start\":51209},{\"end\":51231,\"start\":51227},{\"end\":51251,\"start\":51243},{\"end\":51270,\"start\":51265},{\"end\":51284,\"start\":51280},{\"end\":51298,\"start\":51292},{\"end\":51314,\"start\":51305},{\"end\":51600,\"start\":51589},{\"end\":51617,\"start\":51610},{\"end\":51630,\"start\":51626},{\"end\":51875,\"start\":51866},{\"end\":51886,\"start\":51882},{\"end\":51904,\"start\":51897},{\"end\":51915,\"start\":51911},{\"end\":52155,\"start\":52147},{\"end\":52176,\"start\":52165},{\"end\":52195,\"start\":52190},{\"end\":52207,\"start\":52202},{\"end\":52218,\"start\":52209},{\"end\":52468,\"start\":52462},{\"end\":52485,\"start\":52480},{\"end\":52491,\"start\":52487},{\"end\":52700,\"start\":52693},{\"end\":52722,\"start\":52710},{\"end\":52736,\"start\":52732},{\"end\":52750,\"start\":52745},{\"end\":52763,\"start\":52759},{\"end\":52786,\"start\":52775},{\"end\":53121,\"start\":53117},{\"end\":53136,\"start\":53133},{\"end\":53147,\"start\":53144},{\"end\":53156,\"start\":53153},{\"end\":53166,\"start\":53162},{\"end\":53492,\"start\":53488},{\"end\":53507,\"start\":53504},{\"end\":53520,\"start\":53517},{\"end\":53530,\"start\":53526},{\"end\":53539,\"start\":53536},{\"end\":53866,\"start\":53862},{\"end\":53885,\"start\":53878},{\"end\":53897,\"start\":53894},{\"end\":54159,\"start\":54149},{\"end\":54171,\"start\":54169},{\"end\":54194,\"start\":54187},{\"end\":54207,\"start\":54203},{\"end\":54216,\"start\":54209},{\"end\":54463,\"start\":54459},{\"end\":54481,\"start\":54473},{\"end\":54498,\"start\":54493},{\"end\":54514,\"start\":54506},{\"end\":54530,\"start\":54521},{\"end\":54796,\"start\":54793},{\"end\":54810,\"start\":54807},{\"end\":54820,\"start\":54817},{\"end\":54833,\"start\":54829},{\"end\":54847,\"start\":54843},{\"end\":55103,\"start\":55101},{\"end\":55116,\"start\":55113},{\"end\":55334,\"start\":55332},{\"end\":55347,\"start\":55344},{\"end\":55575,\"start\":55573},{\"end\":55588,\"start\":55585},{\"end\":55788,\"start\":55785},{\"end\":55802,\"start\":55799},{\"end\":55814,\"start\":55812},{\"end\":55827,\"start\":55823},{\"end\":55840,\"start\":55835},{\"end\":55853,\"start\":55849},{\"end\":55867,\"start\":55863},{\"end\":55880,\"start\":55877},{\"end\":56173,\"start\":56169},{\"end\":56182,\"start\":56179},{\"end\":56190,\"start\":56186},{\"end\":56208,\"start\":56201},{\"end\":56213,\"start\":56210},{\"end\":56449,\"start\":56445},{\"end\":56460,\"start\":56457},{\"end\":56469,\"start\":56465},{\"end\":56483,\"start\":56478},{\"end\":56749,\"start\":56746},{\"end\":56760,\"start\":56757},{\"end\":56771,\"start\":56769},{\"end\":56782,\"start\":56778},{\"end\":56793,\"start\":56789},{\"end\":57031,\"start\":57028},{\"end\":57042,\"start\":57039},{\"end\":57053,\"start\":57051},{\"end\":57067,\"start\":57063},{\"end\":57078,\"start\":57074},{\"end\":57089,\"start\":57085},{\"end\":57360,\"start\":57357},{\"end\":57371,\"start\":57368},{\"end\":57382,\"start\":57380},{\"end\":57398,\"start\":57393},{\"end\":57409,\"start\":57406},{\"end\":57419,\"start\":57415},{\"end\":57430,\"start\":57426},{\"end\":57441,\"start\":57437},{\"end\":57752,\"start\":57750},{\"end\":57766,\"start\":57763},{\"end\":57990,\"start\":57985},{\"end\":57999,\"start\":57996},{\"end\":58010,\"start\":58008},{\"end\":58021,\"start\":58018},{\"end\":58032,\"start\":58028}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":44890,\"start\":44777},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14450404},\"end\":45310,\"start\":44892},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6771025},\"end\":45644,\"start\":45312},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":199552074},\"end\":45839,\"start\":45646},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":208310189},\"end\":46230,\"start\":45841},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12358833},\"end\":46704,\"start\":46232},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4028864},\"end\":46958,\"start\":46706},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":61831046},\"end\":47200,\"start\":46960},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2845053},\"end\":47413,\"start\":47202},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9067666},\"end\":47692,\"start\":47415},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":209370711},\"end\":48044,\"start\":47694},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":73729084},\"end\":48280,\"start\":48046},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4559916},\"end\":48543,\"start\":48282},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53073405},\"end\":48765,\"start\":48545},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9035204},\"end\":49055,\"start\":48767},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14224},\"end\":49444,\"start\":49057},{\"attributes\":{\"id\":\"b16\"},\"end\":49759,\"start\":49446},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1538200},\"end\":49951,\"start\":49761},{\"attributes\":{\"id\":\"b18\"},\"end\":50200,\"start\":49953},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10716717},\"end\":50506,\"start\":50202},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":204960425},\"end\":50818,\"start\":50508},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":244169344},\"end\":51139,\"start\":50820},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10091321},\"end\":51517,\"start\":51141},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3719281},\"end\":51793,\"start\":51519},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3405508},\"end\":52082,\"start\":51795},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":977535},\"end\":52403,\"start\":52084},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":687379},\"end\":52622,\"start\":52405},{\"attributes\":{\"doi\":\"3DV. IEEE, 2020. 7\",\"id\":\"b27\"},\"end\":52997,\"start\":52624},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":75136723},\"end\":53365,\"start\":52999},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":221112260},\"end\":53743,\"start\":53367},{\"attributes\":{\"id\":\"b30\"},\"end\":54083,\"start\":53745},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":9124253},\"end\":54393,\"start\":54085},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":227247619},\"end\":54712,\"start\":54395},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":236956903},\"end\":55030,\"start\":54714},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":118679778},\"end\":55235,\"start\":55032},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":209500632},\"end\":55501,\"start\":55237},{\"attributes\":{\"id\":\"b36\"},\"end\":55694,\"start\":55503},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":220666071},\"end\":56096,\"start\":55696},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":209405014},\"end\":56362,\"start\":56098},{\"attributes\":{\"doi\":\"arXiv:2104.13325\",\"id\":\"b39\"},\"end\":56680,\"start\":56364},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4712004},\"end\":56951,\"start\":56682},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":67855970},\"end\":57274,\"start\":56953},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":208248003},\"end\":57643,\"start\":57276},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":214713824},\"end\":57930,\"start\":57645},{\"attributes\":{\"id\":\"b44\"},\"end\":58157,\"start\":57932}]", "bib_title": "[{\"end\":44937,\"start\":44892},{\"end\":45381,\"start\":45312},{\"end\":45683,\"start\":45646},{\"end\":45921,\"start\":45841},{\"end\":46297,\"start\":46232},{\"end\":46739,\"start\":46706},{\"end\":46989,\"start\":46960},{\"end\":47250,\"start\":47202},{\"end\":47482,\"start\":47415},{\"end\":47798,\"start\":47694},{\"end\":48083,\"start\":48046},{\"end\":48321,\"start\":48282},{\"end\":48587,\"start\":48545},{\"end\":48835,\"start\":48767},{\"end\":49087,\"start\":49057},{\"end\":49510,\"start\":49446},{\"end\":49795,\"start\":49761},{\"end\":50298,\"start\":50202},{\"end\":50591,\"start\":50508},{\"end\":50896,\"start\":50820},{\"end\":51188,\"start\":51141},{\"end\":51582,\"start\":51519},{\"end\":51858,\"start\":51795},{\"end\":52143,\"start\":52084},{\"end\":52458,\"start\":52405},{\"end\":53107,\"start\":52999},{\"end\":53478,\"start\":53367},{\"end\":54147,\"start\":54085},{\"end\":54446,\"start\":54395},{\"end\":54782,\"start\":54714},{\"end\":55090,\"start\":55032},{\"end\":55321,\"start\":55237},{\"end\":55774,\"start\":55696},{\"end\":56161,\"start\":56098},{\"end\":56740,\"start\":56682},{\"end\":57022,\"start\":56953},{\"end\":57351,\"start\":57276},{\"end\":57742,\"start\":57645}]", "bib_author": "[{\"end\":44794,\"start\":44779},{\"end\":44807,\"start\":44794},{\"end\":44954,\"start\":44939},{\"end\":44984,\"start\":44954},{\"end\":45001,\"start\":44984},{\"end\":45023,\"start\":45001},{\"end\":45029,\"start\":45023},{\"end\":45394,\"start\":45383},{\"end\":45411,\"start\":45394},{\"end\":45429,\"start\":45411},{\"end\":45448,\"start\":45429},{\"end\":45457,\"start\":45448},{\"end\":45695,\"start\":45685},{\"end\":45709,\"start\":45695},{\"end\":45718,\"start\":45709},{\"end\":45726,\"start\":45718},{\"end\":45935,\"start\":45923},{\"end\":45947,\"start\":45935},{\"end\":45959,\"start\":45947},{\"end\":45970,\"start\":45959},{\"end\":45983,\"start\":45970},{\"end\":46001,\"start\":45983},{\"end\":46009,\"start\":46001},{\"end\":46314,\"start\":46299},{\"end\":46326,\"start\":46314},{\"end\":46753,\"start\":46741},{\"end\":46764,\"start\":46753},{\"end\":46777,\"start\":46764},{\"end\":46784,\"start\":46777},{\"end\":46799,\"start\":46784},{\"end\":46807,\"start\":46799},{\"end\":46819,\"start\":46807},{\"end\":47010,\"start\":46991},{\"end\":47028,\"start\":47010},{\"end\":47271,\"start\":47252},{\"end\":47283,\"start\":47271},{\"end\":47502,\"start\":47484},{\"end\":47519,\"start\":47502},{\"end\":47537,\"start\":47519},{\"end\":47813,\"start\":47800},{\"end\":47825,\"start\":47813},{\"end\":47835,\"start\":47825},{\"end\":47848,\"start\":47835},{\"end\":48099,\"start\":48085},{\"end\":48109,\"start\":48099},{\"end\":48121,\"start\":48109},{\"end\":48136,\"start\":48121},{\"end\":48150,\"start\":48136},{\"end\":48337,\"start\":48323},{\"end\":48351,\"start\":48337},{\"end\":48366,\"start\":48351},{\"end\":48382,\"start\":48366},{\"end\":48397,\"start\":48382},{\"end\":48602,\"start\":48589},{\"end\":48616,\"start\":48602},{\"end\":48629,\"start\":48616},{\"end\":48642,\"start\":48629},{\"end\":48848,\"start\":48837},{\"end\":48862,\"start\":48848},{\"end\":48877,\"start\":48862},{\"end\":48888,\"start\":48877},{\"end\":48897,\"start\":48888},{\"end\":49106,\"start\":49089},{\"end\":49123,\"start\":49106},{\"end\":49137,\"start\":49123},{\"end\":49528,\"start\":49512},{\"end\":49541,\"start\":49528},{\"end\":49555,\"start\":49541},{\"end\":49571,\"start\":49555},{\"end\":49809,\"start\":49797},{\"end\":49820,\"start\":49809},{\"end\":49830,\"start\":49820},{\"end\":49837,\"start\":49830},{\"end\":50046,\"start\":50028},{\"end\":50057,\"start\":50046},{\"end\":50314,\"start\":50300},{\"end\":50328,\"start\":50314},{\"end\":50343,\"start\":50328},{\"end\":50605,\"start\":50593},{\"end\":50615,\"start\":50605},{\"end\":50624,\"start\":50615},{\"end\":50639,\"start\":50624},{\"end\":50650,\"start\":50639},{\"end\":50909,\"start\":50898},{\"end\":50919,\"start\":50909},{\"end\":50931,\"start\":50919},{\"end\":50946,\"start\":50931},{\"end\":50956,\"start\":50946},{\"end\":50964,\"start\":50956},{\"end\":51204,\"start\":51190},{\"end\":51221,\"start\":51204},{\"end\":51233,\"start\":51221},{\"end\":51253,\"start\":51233},{\"end\":51272,\"start\":51253},{\"end\":51286,\"start\":51272},{\"end\":51300,\"start\":51286},{\"end\":51316,\"start\":51300},{\"end\":51602,\"start\":51584},{\"end\":51619,\"start\":51602},{\"end\":51632,\"start\":51619},{\"end\":51877,\"start\":51860},{\"end\":51888,\"start\":51877},{\"end\":51906,\"start\":51888},{\"end\":51917,\"start\":51906},{\"end\":52157,\"start\":52145},{\"end\":52178,\"start\":52157},{\"end\":52197,\"start\":52178},{\"end\":52209,\"start\":52197},{\"end\":52220,\"start\":52209},{\"end\":52470,\"start\":52460},{\"end\":52487,\"start\":52470},{\"end\":52493,\"start\":52487},{\"end\":52702,\"start\":52683},{\"end\":52724,\"start\":52702},{\"end\":52738,\"start\":52724},{\"end\":52752,\"start\":52738},{\"end\":52765,\"start\":52752},{\"end\":52788,\"start\":52765},{\"end\":53123,\"start\":53109},{\"end\":53138,\"start\":53123},{\"end\":53149,\"start\":53138},{\"end\":53158,\"start\":53149},{\"end\":53168,\"start\":53158},{\"end\":53494,\"start\":53480},{\"end\":53509,\"start\":53494},{\"end\":53522,\"start\":53509},{\"end\":53532,\"start\":53522},{\"end\":53541,\"start\":53532},{\"end\":53868,\"start\":53856},{\"end\":53887,\"start\":53868},{\"end\":53899,\"start\":53887},{\"end\":54161,\"start\":54149},{\"end\":54173,\"start\":54161},{\"end\":54196,\"start\":54173},{\"end\":54209,\"start\":54196},{\"end\":54218,\"start\":54209},{\"end\":54465,\"start\":54448},{\"end\":54483,\"start\":54465},{\"end\":54500,\"start\":54483},{\"end\":54516,\"start\":54500},{\"end\":54532,\"start\":54516},{\"end\":54798,\"start\":54784},{\"end\":54812,\"start\":54798},{\"end\":54822,\"start\":54812},{\"end\":54835,\"start\":54822},{\"end\":54849,\"start\":54835},{\"end\":55105,\"start\":55092},{\"end\":55118,\"start\":55105},{\"end\":55336,\"start\":55323},{\"end\":55349,\"start\":55336},{\"end\":55577,\"start\":55564},{\"end\":55590,\"start\":55577},{\"end\":55790,\"start\":55776},{\"end\":55804,\"start\":55790},{\"end\":55816,\"start\":55804},{\"end\":55829,\"start\":55816},{\"end\":55842,\"start\":55829},{\"end\":55855,\"start\":55842},{\"end\":55869,\"start\":55855},{\"end\":55882,\"start\":55869},{\"end\":56175,\"start\":56163},{\"end\":56184,\"start\":56175},{\"end\":56192,\"start\":56184},{\"end\":56210,\"start\":56192},{\"end\":56215,\"start\":56210},{\"end\":56451,\"start\":56437},{\"end\":56462,\"start\":56451},{\"end\":56471,\"start\":56462},{\"end\":56485,\"start\":56471},{\"end\":56751,\"start\":56742},{\"end\":56762,\"start\":56751},{\"end\":56773,\"start\":56762},{\"end\":56784,\"start\":56773},{\"end\":56795,\"start\":56784},{\"end\":57033,\"start\":57024},{\"end\":57044,\"start\":57033},{\"end\":57055,\"start\":57044},{\"end\":57069,\"start\":57055},{\"end\":57080,\"start\":57069},{\"end\":57091,\"start\":57080},{\"end\":57362,\"start\":57353},{\"end\":57373,\"start\":57362},{\"end\":57384,\"start\":57373},{\"end\":57400,\"start\":57384},{\"end\":57411,\"start\":57400},{\"end\":57421,\"start\":57411},{\"end\":57432,\"start\":57421},{\"end\":57443,\"start\":57432},{\"end\":57754,\"start\":57744},{\"end\":57768,\"start\":57754},{\"end\":57992,\"start\":57976},{\"end\":58001,\"start\":57992},{\"end\":58012,\"start\":58001},{\"end\":58023,\"start\":58012},{\"end\":58034,\"start\":58023}]", "bib_venue": "[{\"end\":45069,\"start\":45029},{\"end\":45461,\"start\":45457},{\"end\":45730,\"start\":45726},{\"end\":46013,\"start\":46009},{\"end\":46415,\"start\":46326},{\"end\":46823,\"start\":46819},{\"end\":47061,\"start\":47028},{\"end\":47288,\"start\":47283},{\"end\":47541,\"start\":47537},{\"end\":47852,\"start\":47848},{\"end\":48154,\"start\":48150},{\"end\":48401,\"start\":48397},{\"end\":48646,\"start\":48642},{\"end\":48901,\"start\":48897},{\"end\":49208,\"start\":49137},{\"end\":49578,\"start\":49571},{\"end\":49841,\"start\":49837},{\"end\":50026,\"start\":49953},{\"end\":50347,\"start\":50343},{\"end\":50654,\"start\":50650},{\"end\":50968,\"start\":50964},{\"end\":51320,\"start\":51316},{\"end\":51638,\"start\":51632},{\"end\":51922,\"start\":51917},{\"end\":52224,\"start\":52220},{\"end\":52497,\"start\":52493},{\"end\":52681,\"start\":52624},{\"end\":53172,\"start\":53168},{\"end\":53546,\"start\":53541},{\"end\":53854,\"start\":53745},{\"end\":54223,\"start\":54218},{\"end\":54536,\"start\":54532},{\"end\":54853,\"start\":54849},{\"end\":55122,\"start\":55118},{\"end\":55353,\"start\":55349},{\"end\":55562,\"start\":55503},{\"end\":55886,\"start\":55882},{\"end\":56219,\"start\":56215},{\"end\":56435,\"start\":56364},{\"end\":56799,\"start\":56795},{\"end\":57095,\"start\":57091},{\"end\":57447,\"start\":57443},{\"end\":57772,\"start\":57768},{\"end\":57974,\"start\":57932},{\"end\":46491,\"start\":46417},{\"end\":49266,\"start\":49210}]"}}}, "year": 2023, "month": 12, "day": 17}