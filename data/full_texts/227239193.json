{"id": 227239193, "updated": "2023-10-06 08:56:37.414", "metadata": {"title": "Decentralized Multi-Agent Linear Bandits with Safety Constraints", "authors": "[{\"first\":\"Sanae\",\"last\":\"Amani\",\"middle\":[]},{\"first\":\"Christos\",\"last\":\"Thrampoulidis\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 12, "day": 1}, "abstract": "We study decentralized stochastic linear bandits, where a network of $N$ agents acts cooperatively to efficiently solve a linear bandit-optimization problem over a $d$-dimensional space. For this problem, we propose DLUCB: a fully decentralized algorithm that minimizes the cumulative regret over the entire network. At each round of the algorithm each agent chooses its actions following an upper confidence bound (UCB) strategy and agents share information with their immediate neighbors through a carefully designed consensus procedure that repeats over cycles. Our analysis adjusts the duration of these communication cycles ensuring near-optimal regret performance $\\mathcal{O}(d\\log{NT}\\sqrt{NT})$ at a communication rate of $\\mathcal{O}(dN^2)$ per round. The structure of the network affects the regret performance via a small additive term - coined the regret of delay - that depends on the spectral gap of the underlying graph. Notably, our results apply to arbitrary network topologies without a requirement for a dedicated agent acting as a server. In consideration of situations with high communication cost, we propose RC-DLUCB: a modification of DLUCB with rare communication among agents. The new algorithm trades off regret performance for a significantly reduced total communication cost of $\\mathcal{O}(d^3N^{2.5})$ over all $T$ rounds. Finally, we show that our ideas extend naturally to the emerging, albeit more challenging, setting of safe bandits. For the recently studied problem of linear bandits with unknown linear safety constraints, we propose the first safe decentralized algorithm. Our study contributes towards applying bandit techniques in safety-critical distributed systems that repeatedly deal with unknown stochastic environments. We present numerical simulations for various network topologies that corroborate our theoretical findings.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2012.00314", "mag": "3110181803", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/AmaniT21", "doi": "10.1609/aaai.v35i8.16820"}}, "content": {"source": {"pdf_hash": "17e65ef9d81727671cc75280ee30a5e219c2956d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.00314v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1e27b09cfed5cb0b60044287e0b60da97192141e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/17e65ef9d81727671cc75280ee30a5e219c2956d.txt", "contents": "\nDecentralized Multi-Agent Linear Bandits with Safety Constraints\n1 Dec 2020\n\nSanae Amani \nUniversity of California\nSanta Barbara\n\nChristos Thrampoulidis \nUniversity of California\nSanta Barbara\n\nDecentralized Multi-Agent Linear Bandits with Safety Constraints\n1 Dec 2020\nWe study decentralized stochastic linear bandits, where a network of N agents acts cooperatively to efficiently solve a linear bandit-optimization problem over a d-dimensional space. For this problem, we propose DLUCB: a fully decentralized algorithm that minimizes the cumulative regret over the entire network. At each round of the algorithm each agent chooses its actions following an upper confidence bound (UCB) strategy and agents share information with their immediate neighbors through a carefully designed consensus procedure that repeats over cycles. Our analysis adjusts the duration of these communication cycles ensuring near-optimal regret performance O(d log N T \u221a N T ) at a communication rate of O(dN 2 ) per round. The structure of the network affects the regret performance via a small additive term -coined the regret of delay -that depends on the spectral gap of the underlying graph. Notably, our results apply to arbitrary network topologies without a requirement for a dedicated agent acting as a server. In consideration of situations with high communication cost, we propose RC-DLUCB: a modification of DLUCB with rare communication among agents. The new algorithm trades off regret performance for a significantly reduced total communication cost of O(d 3 N 2.5 ) over all T rounds. Finally, we show that our ideas extend naturally to the emerging, albeit more challenging, setting of safe bandits. For the recently studied problem of linear bandits with unknown linear safety constraints, we propose the first safe decentralized algorithm. Our study contributes towards applying bandit techniques in safety-critical distributed systems that repeatedly deal with unknown stochastic environments. We present numerical simulations for various network topologies that corroborate our theoretical findings.This paper contributes to the intersection of these two emerging lines of work. Concretely, we consider the problem of decentralized multi-agent linear bandits for a general (connected) network structure of N agents, who can only communicate messages with their immediate neighbors. For this, we propose and analyze the first fully-decentralized algorithm. We also present a communication-efficient version and discuss key tradeoffs between regret, communication cost and graph structure. Finally, we present the first simultaneously distributed and safe bandit algorithm for a setting with unknown linear constraints.Notation. We use lower-case letters for scalars, lower-case bold letters for vectors, and upper-case bold letters for matrices. The Euclidean-norm of x is denoted by x 2 . We denote the transpose of any column vector x by x T . For any vectors x and y, we use x, y to denote their inner product. Let A be a positive definite d\u00d7d matrix and \u03bd \u2208 R d . The weighted 2-norm of \u03bd with respect to A is defined by \u03bd A = \u221a \u03bd T A\u03bd. For positive integers n and m \u2264 n, [n] and [m : n] denote the sets {1, 2, . . . , n} and {m, . . . , n}, respectively. We use 1 and e i to denote the vector of all 1's and the i-th standard basis vector, respectively.Problem formulationDecentralized Linear Bandit. We consider a network of N agents and known convex compact decision set D \u2282 R d (our results can be easily extended to settings with time varying decision sets). Agents play actions synchronously. At each round t, each agent i chooses an action x i,t \u2208 D and observes reward y i,t = \u03b8 * , x i,t + \u03b7 i,t , where \u03b8 * \u2208 R d is an unknown vector and \u03b7 i,t is random additive noise.Communication Model. The agents are represented by the nodes of an undirected and connected graph G. Each agent can send and receive messages only to and from its immediate neighbors. The topology of G is known to all agents via a communication matrix P (see Assumption 1).Safety. The learning environment might be subject to unknown constraints that restrict the choice of actions. In this paper, we model the safety constraint by a linear function depending on an unknown vector \u00b5 * \u2208 R d and a known constant c \u2208 R. Specifically, the chosen action x i,t must satisfy \u00b5 * , x i,t \u2264 c, for all i and t, with high probability. We define the unknown safe set as D s (\u00b5 * ) := {x \u2208 D : \u00b5 * , x \u2264 c}.After playing x i,t , agent i observes bandit-feedback measurements z i,t = \u00b5 * , x i,t + \u03b6 i,t . This type of safety constraint, but for single-agent settings, has been recently introduced and studied in [AAT19, PGBJ20, SGBK15, SZBY18, MAAT19]. See also [KGYVR17, KB19] for related notions of safety studied recently in the context of single-agent linear bandits.Goal. Let T be the total number of rounds. We define the cumulative regret of the entire network as:The optimal action x * is defined with respect to D and D s (\u00b5 * ) as arg max x\u2208D \u03b8 * , x and arg max x\u2208D s (\u00b5 * ) \u03b8 * , x in the original and safe settings, respectively. The goal is to minimize the cumulative regret, while each agent is allowed to share poly(N d) values per round to its neighbors. Specifically, we wish to achieve a regret close to that incurred by an optimal centralized algorithm for N T rounds (the total number of plays). In the presence of safety constraint, in addition to the aforementioned goals, agents' actions must also satisfy the safety constraint at each round.ContributionsDLUCB. We propose a fully decentralized linear bandit algorithm (DLUCB), at each round of which, the agents simultaneously share information among each other and pick their next actions. We prove a regret bound that captures both the degree of selected actions' optimality and the inevitable delay in informationsharing due to the network structure. See Section 2.1 and 2.2. Compared to existing distributed LB algorithms, ours can be implemented (and remains valid) for any arbitrary (connected) network without requiring a peer-to-peer network structure or a master node. See Section 2.4.RC-DLUCB.We propose a fully decentralized algorithm with rare communication (RC-DLUCB) to reduce the communication cost (total number of values communicated during the run of algorithm) for applications that are sensitive to high communication cost. See Section 2.3Safe-DLUCB. We present and analyze the first fully decentralized algorithm for safe LBs with linear\n\nIntroduction\n\nLinear stochastic bandits (LB) provide simple, yet commonly encountered, models for a variety of sequential decision-making problems under uncertainty. Specifically, LB generalizes the classical multi-armed bandit (MAB) problem of K arms that each yields reward sampled independently from an underlying distribution with unknown parameters, to a setting where the expected reward of each arm is a linear function that depends on the same unknown parameter vector [DHK08, AYPS11, RT10]. LBs have been successfully applied over the years in online advertising, recommendation services, resource allocation, etc. [LS18]. More recently, researchers have explored the potentials of such algorithms in more complex systems, such as in robotics, wireless networks, the power grid, medical trials, e.g., [LHLK13, AM19, BKS16, SZBY18]. A distinguishing feature of many of these -perhaps less conventional-bandit applications, is their distributive nature. For example, in sensor/wireless networks [AM19], a collaborative behavior is required for decisionmakers/agents to select better actions as individuals, but each of them is only able to share information about the unknown environment with a subset of neighboring agents. While a distributed nature is inherent in certain systems, distributed solutions might also be preferred in broader settings, as they can lead to speed-ups of the learning process. This calls for extensions of the traditional bandit setting to networked systems. At the same time, in many of these applications the unknown system might be safety-critical, i.e., the algorithm's chosen actions need to satisfy certain constraints that, importantly, are often unknown. This leads to the challenge of balancing the goal of reward maximization with the restriction of playing safe actions. The past few years have seen a surge of research activity in these two areas: (i) distributed [WHCW19, MRKR19, SBFH + 13, LSL16a]; and (ii) safe bandits [SGBK15, SZBY18, KGYVR17, AAT19, AAT20b, AAT20a, MAAT19, KB19, PGBJ20].\n\nconstraints. Our algorithm provably achieves regret of the same order (wrt. N T ) as if no constraints were present. See Section 3 We complement our theoretical results with numerical simulations under various settings in Section 4.\n\n\nRelated works\n\nDecentralized Bandits. There are several recent works on decentralized/distributed stochastic MAB problems. In the context of the classical K-armed MAB, [MRKR19, LSL16a, LSL16b] proposed decentralized algorithms for a network of N agents that can share information only with their immediate neighbors, while [SBFH + 13] studies the MAB problem on peer-to-peer networks. More recently, [WHCW19] focuses on communication efficiency and presented K-armed MAB algorithms with significantly lower communication overhead. In contrast to these, here, we study a LB model. The most closely related works on distributed/decentralized LB are [WHCW19] and [KSS16]. In [WHCW19], the authors present a communicationefficient algorithm that operates under the coordination of a central server, such that every agent has instantaneous access to the full network information through the server. This model differs from the fully decentralized one considered here. In another closely related work, [KSS16] studies distributed LBs in peerto-peer networks, where each agent can only send information to one other randomly chosen agent, not necessarily its neighbor, per round. A feature, in common with our algorithm, is the delayed use of bandit feedback, but the order of the delay differs between the two, owing to the different model. Please also see Section 2.4 for a more elaborate comparison. To recap, even-though motivated by the aforementioned works, our paper presents the first fully decentralized algorithm for the multi-agent LB problem on a general network topology, with communication between any two neighbors in the network. Furthermore, non of the above has studied the presence of safety constraints.\n\nSafe Bandits. In a more general context, the notion of safe learning has many diverse definitions in the literature. Specifically, safety in bandit problems has itself received significant attention in recent years, e.g. [SGBK15, SZBY18, KGYVR17, AAT19, AAT20b, AAT20a, MAAT19, KB19, PGBJ20]. To the best of our knowledge, all existing works on MAB/LB problems with safety constraints study a single-agent. As mentioned in Section 1.1, the multi-agent safe LB studied here is a canonical extension of the single-agent setting studied in [AAT19, MAAT19, PGBJ20]. Accordingly, our algorithm and analysis builds on ideas introduced in this prior work and extends them to multi-agent collaborative learning.\n\n\nDecentralized Linear Algorithms\n\nIn this section, we present Decentralized Linear Upper Confidence Bound (DLUCB). Starting with a highlevel description of the gossip communication protocol and of the benefits and challenges it brings to the problem in Section 2.1, we then explain DLUCB Algorithm 1 in Section 2.2. In Section 2.3 we present a communication-efficient version of DLUCB. Finally, in Section 2.4 we compare our algorithms to prior art. Throughout this section, we do not assume any safety constraints. Below, we introduce some necessary assumptions.\n\nAssumption 1 (Communication Matrix). For an undirected connected graph G with N nodes, P \u2208 R N \u00d7N is a symmetric communication matrix if it satisfies the following three conditions: (i) P i,j = 0 if there is no connection between nodes i and j; (ii) the sum of each row and column of P is 1; (iii) the eigenvalues are real and their magnitude is less than 1, i.e., 1 = |\u03bb 1 |> |\u03bb 2 |\u2265 . . . |\u03bb N |\u2265 0. We assume that agents have knowledge of communication matrix P.\n\nWe remark that P can be constructed with little global information about the graph, such as its adjacency matrix and the graph's maximal degree; see Section 4 for an explicit construction. Once P is known, the total number of agents N and the graph's spectral gap 1 \u2212 |\u03bb 2 | are also known. We show in Section 2.2 that the latter two parameters fully capture how the network structure affects the algorithm's regret.\n\nAssumption 2 (Subgaussian Noise). For i \u2208 [N ] and t > 0, \u03b7 i,t , \u03b6 i,t are zero-mean \u03c3-subGaussian random variables.\n\nAssumption 3 (Boundedness). Without loss of generality, x 2 \u2264 1 for all x \u2208 D, \u03b8 * 2 \u2264 1, and \u00b5 * 2 \u2264 1.\n\n\nInformation-sharing protocol\n\nDLCUB implements a UCB strategy. At the core of single-agent UCB algorithms, is the construction of a proper confidence set around the true parameter \u03b8 * using past actions and their observed rewards. In multi-agent settings, each agent i \u2208 [N ] maintains their own confidence set C i,t at every round t. To exploit the network structure and enjoy the benefits of collaborative learning, it is important that C i,t is built using information about past actions of not only agent i itself, but also of agents j = i \u2208 [N ]. For simplicity, we consider first a centralized setting of perfect information-sharing among agents. Specifically, assume that at every round t, agent i knows the past chosen actions and their observed rewards by all other agents in the graph. Having gathered all this information, each agent i maintains knowledge of the following sufficient statistics during all rounds t:\nA * ,t = \u03bbI + t\u22121 \u03c4 =1 N i=1 x i,\u03c4 x T i,\u03c4 , b * ,t = t\u22121 \u03c4 =1 N i=1 y i,\u03c4 x i,\u03c4 .(1)\nHere, \u03bb \u2265 1 is a regularization parameter. Of course, in this idealized scenario, the confidence set constructed based on (1) is the same for every agent. In fact, it is the same as the confidence set that would be constructed by a single-agent that is allowed to choose N actions at every round. Here, we study a decentralized setting with imperfect information sharing. In particular, each agent i can only communicate with its immediate neighbors j \u2208 N (i) at any time t. As such, it does not have direct access to the \"perfect statistics\" A * ,t and b * ,t in (1). Instead, it is confined to approximations of them, which we denote by A i,t and b i,t . At worst-case, where no communication is used,\nA i,t = \u03bbI + t\u22121 \u03c4 =1 x i,\u03c4 x T i,\u03c4 (similarly for b i,t ).\nBut, this is a very poor approximation of A * ,t (correspondingly, b * ,t ). Our goal is to construct a communication scheme that exploits exchange of information among agents to allow for drastically better approximations of (1). Towards this goal, our algorithm implements an appropriate gossip protocol to communicate each agent's past actions and observed rewards to the rest of the network (even beyond immediate neighbors). We describe the details of this protocol next.\n\nRunning Consensus. In order to share information about agents' past actions among the network, we rely on running consensus, e.g., [Lyn96, XB04]. The goal of running consensus is that after enough rounds of communication, each agent has an accurate estimate of the average (over all agents) of the initial values of each agent. Precisely, let \u03bd 0 \u2208 R N be a vector, where each entry \u03bd 0,i , i \u2208 [N ] represents agent's i information at some initial round. Then, running consensus aims at providing an accurate estimate of the average 1 N i\u2208[N ] \u03bd 0,i at each agent. Note that encoding \u03bd 0 = Xe j , allows all agents to eventually get an estimate of the value X = i\u2208[N ] \u03bd 0,i that was initially known only to agent j. To see how this is relevant to our setting recall (1) and focus at t = 2 for simplicity. At round t = 2, each agent j only knows x j,1 and estimation of A * ,2 = N i=1 x i,1 x T i,1 by agent j boils down to estimating each x i,1 , i = j. In our previous example, let X be k-th entry of x i,1 for some i = j. By running consensus on \u03bd 0 = [x j,1 ] k e j for k \u2208 [d], every agent eventually builds an accurate estimate of [x j,1 ] k , the k-th entry of x j,1 that would otherwise only be known to j. It turns out that the communication matrix P defined in Assumption 1 plays a key role in reaching consensus. The details are standard in the rich related literature [XB04, Lyn96]. Here, we only give a brief explanation of the high-level principles. Roughly speaking, a consensus algorithm updates \u03bd 0 by \u03bd 1 = P\u03bd 0 and so on. Note that this operation respects the network structure since the updated value \u03bd 1,j is a weighted average of only \u03bd 0,j itself and neighbor-only values \u03bd 0,i , i \u2208 N (j). Thus, after S rounds, agent j has access to entry j of \u03bd S = P S \u03bd 0 . This is useful because P is well-known to satisfy the following mixing property:\nlim S\u2192\u221e P S = 11 T /N [XB04]. Thus, lim S\u2192\u221e [\u03bd S ] j = 1 N N i=1 \u03bd 0,i , \u2200j \u2208 [N ], as desired.\nOf course, in practice, the number S of communication rounds is finite, leading to an \u01eb-approximation of the average. Accelerated-Consensus. In this paper, we adapt polynomial filtering introduced in [MRKR19, SBB + 17] to speed up the mixing of information by following an approach whose convergence rate is faster than the standard multiplication method above. Specifically, after S communication rounds, instead of P S , agents compute and apply to the initial vector \u03bd 0 an appropriate re-scaled Chebyshev polynomial q S (P) of degree S of the communication matrix. Recall that Chebyshev polynomials are defined recursively. It turns out that the Chebyshev polynomial of degree \u2113 for a communication matrix P is also given by a recursive formula as follows: q \u2113+1 (P) = 2w \u2113 |\u03bb2|w \u2113+1 Pq \u2113 (P) \u2212 w \u2113\u22121 w \u2113+1 q \u2113\u22121 (P), where w 0 = 0, w 1 = 1/|\u03bb 2 |, w \u2113+1 = 2w \u2113 /|\u03bb 2 |\u2212w \u2113\u22121 , q 0 (P) = I and q 1 (P) = P. Specifically, in a Chebyshev-accelerated gossip protocol [MRKR19], the agents update their estimates of the average of the initial vector's \u03bd 0 entries as follows:\n\u03bd \u2113+1 = (2w \u2113 )/(|\u03bb 2 |w \u2113+1 )P\u03bd \u2113 \u2212 (w \u2113\u22121 /w \u2113+1 )\u03bd \u2113\u22121 .\n(2)\n\nOur algorithm DLUCB, which we present later in Section 2.2, implements the Checyshev-accelerated gossip protocol outlined above; see [MRKR19] for a similar implementation only for the classical K-Armed MAB. Specifically, we summarize the accelerated communication step described in (2) with a function Comm(x now , x prev , \u2113) with three inputs as follows: (1) x now is the quantity of interest that the agent wants to update at the current round, (2) x prev is the estimated value for the same quantity of interest that the agent updated in the previous round (cf. \u03bd \u2113\u22121 in (2)); (3) \u2113 is the current communication round. Note that inputs here are scalars, however, matrices and vectors can also be passed as inputs, in which case Comm runs entrywise. For a detailed description of Comm please refer to Algorithm 3 in Appendix B.\n\nThe accelerated consensus algorithm implemented in Comm guarantees fast mixing of information thanks to the following key property [MRKR19, Lem. 3]: for \u01eb \u2208 (0, 1) and any vector \u03bd 0 in the N -dimensional simplex, it holds that\nN q S (P)\u03bd 0 \u2212 1 2 \u2264 \u01eb, provided S = log(2N/\u01eb) 2 log(1/|\u03bb 2 |) .(3)\nIn view of this, our algorithm properly calls Comm (see Algorithm 1) such that for every i \u2208 [N ] and t \u2208 [T ], the action x i,t and corresponding reward y i,t are communicated within the network for S rounds. At round t + S, agent i has access to a i,j x j,t and a i,j y j,t where a i,j = N [q S (P)] i,j . Thanks to (3), a i,j is \u01eb close to 1, thus, these are good approximations of the true x j,t and y j,t . Accordingly, at the beginning of round t > S, each agent i computes\nA i,t := \u03bbI + t\u2212S \u03c4 =1 N j=1 a 2 i,j x j,\u03c4 x T j,\u03c4 , b i,t := t\u2212S \u03c4 =1 N j=1 a 2 i,j y j,\u03c4 x j,\u03c4 ,(4)\nwhich are agent i's approximations of the sufficient statistics A * ,t\u2212S+1 and b * ,t\u2212S+1 defined in (1). On the other hand, for rounds 1 \u2264 t \u2264 S (before any mixing has been completed), let\nA i,t = \u03bbI + t\u22121 \u03c4 =1 x i,\u03c4 x T i,\u03c4 and b i,t = t\u22121 \u03c4 =1 y i,\u03c4 x i,\u03c4 for i \u2208 [N ].\nWith these, at the beginning of each round t \u2208 [T ], agent i constructs the confidence set\nC i,t := {\u03bd \u2208 R d : \u03bd \u2212\u03b8 i,t Ai,t \u2264 \u03b2 t },(5)where\u03b8 i,t = A \u22121 i,t b i,t\nand \u03b2 t is chosen as in Thm. 1 below to guarantee \u03b8 * \u2208 C i,t with high probability. Theorem 1 (Confidence sets). Let Assumptions 1, 2 and 3 hold. Fix \u01eb \u2208 (0, 1) and S as in (3). For \u03b4 \u2208 (0, 1), let\n\u03b2 t := (1 + \u01eb)\u03c3 d log 2\u03bbdN + 2N 2 t \u03bbd\u03b4 + \u03bb 1/2 .\nThen with probability at least 1 \u2212 \u03b4, for all i \u2208 [N ] and t \u2208 [T ] it holds that \u03b8 * \u2208 C i,t .\n\nThe proof is mostly adapted from [AYPS11, Thm. 2] with necessary modifications to account for the imperfect information; see Appendix A.1.\n\n\nDecentralized Linear UCB\n\nWe now describe DLUCB Algorithm 1 (see Appendix A.3 for a more detailed version). Each agent runs DLUCB in a parallel/synchronized way. For concreteness, let us focus on agent i \u2208 [N ]. At every round t,\n\nAlgorithm 1: DLUCB for Agent i Input: D, N , d, |\u03bb 2 |, \u01eb, \u03bb, \u03b4, T 1 S = log(2N/\u01eb)/ 2 log(1/|\u03bb 2 |)\n2 A i,1 = \u03bbI, b i,1 = 0, A i,0 = A i,1 = B i,0 = B i,1 = \u2205 3 for t = 1, . . . , S do 4\nPlay x i,t = arg max x\u2208D max \u03bd\u2208Ci,t \u03bd, x and observe y i,t .\n5 A i,t .append(X i,t ) and B i,t .append(y i,t ) 6 A i,t+1 = Comm(A i,t , A i,t\u22121 , {t, t \u2212 1, . . . , 1}) 7 B i,t+1 = Comm(B i,t , B i,t\u22121 , {t, t \u2212 1, . . . , 1}) // Comm runs for each member of A i,t and B i,t 8 A i,t+1 = A i,t + x i,t x T i,t , b i,t+1 = b i,t + y i,t x i,t 9 A i,S = \u03bbI, b i,S = 0 10 for t = S + 1, . . . , T do 11 A i,t = A i,t\u22121 + N 2 A i,t (1) T A i,t (1), b i,t = b i,t\u22121 + N 2 A i,t (1) T B i,t (1) 12\nPlay x i,t = arg max x\u2208D max \u03bd\u2208Ci,t \u03bd, x and observe y i,t\n13 A i,t .remove A i,t (1) .append X i,t 14 B i,t .remove B i,t (1) .append y i,t 15 A i,t+1 = Comm(A i,t , A i,t\u22121 (2 : S), {S, S \u2212 1, . . . , 1}) 16 B i,t+1 = Comm(B i,t , B i,t\u22121 (2 : S), {S, S \u2212 1, . . . , 1})\nthe agent maintains the following first-in first-out (FIFO) queues of size at most S: A i,t , B i,t , A i,t\u22121 , and B i,t\u22121 . The queue A i,t contains agent i's estimates of all actions played at rounds [t \u2212 S : t \u2212 1]. Concretely, its j-th member, denoted by A i,t (j) \u2208 R N \u00d7d , is a matrix whose k-th row is agent i's estimate of agent k's action played at round t + j \u2212 S \u2212 1. Similarly, we define B i,t as the queue containing agent i's estimates of rewards observed at rounds [t \u2212 S : t \u2212 1]. At each round t, agent i sends every member of A i,t and B i,t (each entry of them) to its neighbors and at the same time it receives the corresponding values from them. The received values are used to update the information stored in A i,t and B i,t . The update is implemented by the sub-routine Comm outlined in Section 2.1 and presented in detail in Appendix B.\n\nAt the beginning of rounds t > S when, the information of rounds [t \u2212 S] is mixed enough, agent i updates its estimates A i,t and b i,t of A * ,t\u2212S and b * ,t\u2212S , respectively. Using these, it creates the confidence set C i,t and runs the UCB decision rule of Line 12 to select an action. Next, agent i updates A i,t and B i,t in Lines 13 and 14, by eliminating the first elements (dequeuing) A i,t (1) and B i,t (1) of the queues A i,t and B i,t and adding the following elements at their end (enqueuing). At A i,t it appends X i,t \u2208 R N \u00d7d , whose rows are all zero but its i-th row which is set to x T i,t . Concurrently, at B i,t , it appends y i,t \u2208 R N , whose elements are all zero but its i-th element which is set to y i,t . Note that X i,t (similarly, y i,t ) contains agent i's estimates of actions at round t, and the zero rows will be updated with agent i's estimates of other agents' information at round t in future rounds. This is achieved via calling the consensus algorithm Comm in Lines 15 and 16, with which agent i communicates all the members of A i,t and B i,t with its neighbors.\n\nRegret analysis. There are two key challenges in the analysis of DLUCB compared to that of singleagent LUCB. First, information sharing is imperfect: the consensus algorithm mixes information for a finite number S of communication rounds resulting in \u01eb-approximations of the desired quantities (cf. (3)). Second, agents can use this (imperfect) information to improve their actions only after an inevitable delay. To see what changes in the analysis of regret, consider the standard decomposition of agent i's instantaneous regret at round t:\nr i,t = \u03b8 * , x * \u2212 \u03b8 * , x i,t \u2264 2\u03b2 t x i,t A \u22121 i,t\n. Using Cauchy-Schwartz inequality, an upper bound on the cumulative regret T t=1 N i=1 r i,t can be obtained by bounding the following key term:\nT t=1 N i=1 x i,t 2 A \u22121 i,t .(6)\nWe do this in two steps, each addressing one of the above challenges. First, in Lemma 1, we address the influence of imperfect information by relating the A \u22121 i,t -norms in (6), with those in terms of their perfect information counterparts A \u22121 * ,t\u2212S+1 . Hereafter, let A * ,t = \u03bbI for t = \u2212S, . . . , 0, 1.\n\nLemma 1 (Influence of imperfect information). Fix any \u01eb \u2208 (0, 1/(4d + 1)) and choose S as in (3). Then,\nfor all i \u2208 [N ], t \u2208 [T ] it holds that x i,t 2 A \u22121 i,t \u2264 e x i,t 2 A \u22121 * ,t\u2212S+1 .\nThe intuition behind the lemma comes from the discussion on the accelerated protocol in Section 2.1.\nSpecifically, with sufficiently small communication-error \u01eb (cf. (3)), A i,t (cf. (4)) is a good approxi- mation of A * ,t\u2212S+1 (cf.\n(1)). The lemma replaces the task of bounding (6) with that of bounding\nT t=1 N i=1 x i,t 2 A \u22121 * ,t\u2212S+1\n. Unfortunately, this remains challenging. Intuitively, the reason for this is the mismatch of information about past actions in the gram matrix A * ,t\u2212S+1 at time t, compared to the inclusion of all terms x i,\u03c4 up to time t in (6). Our idea is to relate\nx i,t A \u22121 * ,t\u2212S+1 to x i,t B \u22121 i,t ,\nwhere\nB i,t = A * ,t + i\u22121 j=1 x j,t x T j,t\n. This is possible thanks to the following lemma.\nLemma 2 (Influence of delays). Let S as in (3). Then, x i,t 2 A \u22121 * ,t\u2212S+1 \u2264 e x i,t 2 B \u22121 i,t ,\nis true for all pairs\n(i, t) \u2208 [N ] \u00d7 [T ] except for at most \u03c8(\u03bb, |\u03bb 2 |, \u01eb, d, N, T ) := Sd log (1 + N T d\u03bb ) of them.\nUsing Lemmas 1 and 2 allows controlling the regret of all actions, but at most \u03c8 of them, using standard machinery in the analysis of UCB-type algorithms. The proofs of Lemmas 1 and 2 and technical details relating the results to a desired regret bound are deferred to Appendix A.2. The theorem below is our first main result and bounds the regret of DLUCB.\n\nTheorem 2 (Regret of DLUCB). Fix \u01eb \u2208 (0, 1/(4d + 1)) and \u03b4 \u2208 (0, 1). Let Assumptions 1, 2, 3 hold, and S be chosen as in (3). Then, with probability at least 1 \u2212 \u03b4, it holds that:\nR T \u2264 2Sd log 1 + N T d\u03bb + 2e\u03b2 T 2dN T log \u03bb + N T d .\nThe regret bound has two additive terms: a small term 2\u03c8(\u03bb, |\u03bb 2 |, \u01eb, d, N, T ) (cf. Lemma 2), which we call regret of delay, and, a second main term that (notably) is of the same order as the regret of a centralized problem where communication is possible between any two nodes (see Table 1). Thm. 2 holds for small \u01eb \u2264 1/(4d + 1). In Appendix A.2, we also provide a general regret bound for arbitrary \u01eb \u2208 (0, 1).\n\n\nDLUCB with Rare Communication\n\nAs discussed in more detail in Section 2.4, DLUCB achieves order-wise optimal regret, but its communication cost scales as O(dN 2 T ), i.e., linearly with the horizon duration T (see Table 1). In this section, we present a modification tailored to communication settings that are sensitive to communication cost. The new algorithm -termed RC-DLUCB -is also a fully decentralized algorithm that trade-offs a slight increase in the regret performance, while guaranteeing a significantly reduced communication cost of O d 3 N 2.5 log(N d) log 1/2 (1/|\u03bb2|) over the entire horizon [T ]. We defer a detailed description (see Algorithm 4) and analysis (see Thms. 4 and 5) of RC-DLUCB in Appendix C. At a high-level, we design RC-DLUCB inspired by the Rarely Switching OFUL algorithm by [AYPS11]. In contrast to the Rarely Switching OFUL algorithm that is designed to save on computations in single-agent systems, RC-DLUCB incorporates a similar idea in our previous DLUCB to save on communication rounds. Specifically, compared to DLUCB where communication happens at each round, in RC-DLUCB agents continue selecting actions individually (i.e., with no communication), unless a certain condition is triggered by any one of them. Then, they all switch to a communication phase, in which they communicate the unmixed information they have gathered for a duration of S rounds. Roughly speaking, an agent triggers the communication phase only once it has gathered enough new information compared to the last update by the rest of the network. This can be measured by keeping track of the variations in the corresponding gram matrix.\n\n\nAlgorithm\n\n\nRegret\n\nCommunication Baselines. In the absence of communication, each agent independently implements a single-agent LUCB [AYPS11]. This trivial 'No Communication' algorithm has zero communication cost and applies to any graph, but its regret scales linearly with the number of agents. At another extreme, a fully 'Centralized' algorithm assumes communication is possible between any two agents at every round. This achieves optimal regret\u00d5( \u221a N T ), which is a lower bound to the regret of any decentralized algorithm. However, it is only applicable in very limited network topologies, such as a star graph where the central node acts as a master node, or, a complete graph. Notably, DLUCB achieves order-wise optimal regret that is same as that of the 'Centralized' algorithm modulo a small additive regret-of-delay term.\nDLUCB O(d log(N d) log 0.5 (1/|\u03bb2|) log(N T ) + d log(N T ) \u221a N T ) O(dN 2 T log(N d) log 0.5 (1/|\u03bb2|) ) RC-DLUCB O(N d 1.5 log(N d) log 0.5 (1/|\u03bb2|) log 1.5 (N T ) + d log 2 (N T ) \u221a N T ) O(d 3 N 2.5 log(N d) log 0.5 (1/|\u03bb2|) ) No Communication O(dN log(T ) \u221a T ) 0 Centralized O(d log(N T ) \u221a N T ) O(dN 2 T ) DCB O((dN log(N T )) 3 + log(N T ) \u221a N T ) O(d 2 N T log(N T )) DisLinUCB O(log 2 (N T ) \u221a N T ) O(d 3 N 1.5 )\nDisLinUCB. In a motivating recent paper [WHCW19], the authors presented 'DisLinUCB' a communication algorithm that applies to multi-agent settings, in which agents can communicate with a masternode/server, by sending or receiving information to/from it with zero latency. Notably, DisLinUCB is shown to achieve order-optimal regret performance same as the 'Centralized' algorithm, but at a significantly lower communication cost that does not scale with T (see Table 1). In this paper, we do not assume presence of a master-node. In our setting, this can only be assumed in very limited cases: a star or a complete graph. Thus, compared to DisLinUCB, our DLUCB can be used for arbitrary network topologies with similar regret guarantees. However, DLUCB requires that communication be performed at each round. This discrepancy motivated us to introduce RC-LUCB, which has communication cost (slightly larger, but) comparable to that of DisLinUCB (see Table 1), while being applicable to general graphs. As a final note, as in RC-DLUCB, the reduced communication cost in DisLinUCB relies on the idea of the Rarely Switching OFUL algorithm of [AYPS11].\n\nDCB. In another closely related work [KSS16] presented DCB for decentralized linear bandits in peer-to-peer networks. Specifically, it is assumed in [KSS16] that at every round each agent communicates with only one other randomly chosen agent per round. Instead, we consider fixed network topologies where each agent can only communicate with its immediate neighbors at every round. Thus, the two algorithms are not directly comparable. Nevertheless, we remark that, similar to our setting, DCB also faces the challenge of controlling a delayed use of information, caused by requiring enough mixing of the communicated information among agents. A key difference is that the duration of delay is typically O(log t) in DCB, while in DLUCB it is fixed to S, i.e., independent of the round t. This explains the significantly smaller first-term in the regret of DLUCB as compared to the first-term in the regret of DCB in Table 1.\n\n\nSafe Decentralized Linear Bandits\n\nFor the safe decentralized LB problem, we propose Safe-DLUCB, an extension of DLUCB to the safe setting and an extension of single-agent safe algorithms [AAT19, MAAT19, PGBJ20] to multi-agent systems. We defer a detailed description of Safe-DLCUB to Algorithm 5 in Appendix D. Here, we give a high-level description of its main steps and present its regret guarantees. First, we need the following assumption and notation.\n\nAssumption 4 (Non-empty safe set). A safe action x 0 \u2208 D and c 0 := \u00b5 * , x 0 < c are known to all agents. Also, \u03b8 * , x 0 \u2265 0.\n\nDefine the normalized safe actionx 0 := x0 x0 . For any x \u2208 R d , denote by x o := x,x 0 x 0 its projection on x 0 , and, by x \u22a5 := x \u2212 x o its projection onto the orthogonal subspace.\n\nIn the presence of safety, the agents must act conservatively to ensure that the chosen actions x i,t do not violate the safety constraint \u00b5 * , x i,t \u2264 c. To this end, agent i communicates, not only x i,t and y i,t , but also the bandit-feedback measurements z i,t , following the communication protocol implemented by Comm (cf. Section 2.1). Once information is sufficiently mixed, it builds an additional confidence set E i,t that includes \u00b5 \u22a5 * with high probability (note that \u00b5 o * is already known by Assumption 4). Please refer to Appendix D.1 for the details on constructing E i,t . Once E i,t is constructed, agent i creates the following safe inner approximation of the true D s (\u00b5 * ):\nD s i,t := {x \u2208 D : x o ,x 0 x 0 c 0 + \u03bc \u22a5 i,t , x \u22a5 + \u03b2 t x \u22a5 A \u22a5,\u22121 i,t \u2264 c}.\nSpecifically, Proposition 1 in Appendix D.1 guarantees for any \u03b4 \u2208 (0, 1) that for all\ni \u2208 [N ], t \u2208 [T ], all actions in D s i,t are safe with probability 1 \u2212 \u03b4. After constructing D s i,t , agent i selects safe action x i,t \u2208 D s i,t\nfollowing a UCB decision rule:\n\u03b8 i,t , x i,t = max x\u2208D s i,t max \u03bd\u2208\u03bar Ci,t \u03bd, x .(7)\nA subtle, but critical, point in (7) is that the inner maximization is over an appropriately enlarged confidence set \u03ba r C i,t . Specifically, compared to Lines 4 and 12 in Algorithm 1, we need here that \u03ba r > 1. Intuitively, this is required because the outer maximization in (7) is not over the entire D s (\u00b5 * ), but only a subset of it. Thus, larger values of \u03ba r are needed to provide enough exploration to the algorithm so that the selected actions in D s i,t are -often enoughoptimistic, i.e., \u03b8 i,t , x i,t \u2265 \u03b8 * , x * ; see Lemma 5 in Appendix D.2 for the exact statement. We attribute the above idea that more aggressive exploration of that form is needed in the safe setting to [MAAT19], only they considered a Thompson-sampling scheme and a single agent.\n\n[PGBJ20] extended this idea to UCB algorithms, again in the single-agent setting (and for a slightly relaxed notion of safety). Here, we show that the idea extends to multi-agent systems and when incorporated to the framework of DLUCB leads to a safe decentralized algorithm with provable regret guarantees stated in the theorem below. See Appendix D for the proof.\n\nTheorem 3 (Regret of Safe-DLUCB). Fix \u03b4 \u2208 (0, 0.5), \u03ba r = 2 c\u2212c0 + 1, \u01eb \u2208 (0, 1/(4d + 1)). Let Assumptions 1, 2, 3, 4 hold, and S be chosen as in (3). Then, with probability at least 1 \u2212 2\u03b4, it holds that:\nR T \u2264 2Sd log(1 + N T d\u03bb ) + 2e\u03ba r \u03b2 T 2dN T log(\u03bb + N T d ).\nThe regret bound is of the same order as DLUCB regret bound, with only an additional factor \u03ba r in its second term.\n\n\nExperiments\n\nIn this section, we evaluate our algorithms' performance on synthetic data. Since the UCB decision rule at line 12 of Algorithm 1 involves a generally non-convex optimization problem, we use a standard computationally tractable modification that replaces \u2113 2 with \u2113 1 norms in the definition of confidence set (5) (unless the decision set is finite); see [DHK08]. All results directly apply to this modified algorithm after only changing the radius \u03b2 t with \u03b2 t \u221a d [DHK08, Section 3.4]. All the results shown depict averages over 20 realizations, for which we have chosen d = 5, D = [\u22121, 1] 5 , \u03bb = 1, and \u03c3 = 0.1. Moreover, \u03b8 * is drawn from N (0, I 5 ) and then normalized to unit norm. We compute the communication matrix as P = I \u2212 1 \u03b4max+1 D \u22121/2 LD \u22121/2 , where \u03b4 max is the maximum degree of the graph and L is the graph Laplacian (see [DAW11] for details). In Figures  1a and 1b, fixing N = 20, we evaluate the performance of DLUCB and RC-DLUCB on 4 different topologies: Ring, Star, Complete, and a Random Erd\u0151s-R\u00e9nyi graph with parameter p = 0.5; see Figure 2 in Appendix E for graphical illustrations of the graphs. We also compare them to the performance of No Communication (see Section 2.4). The plot verifies the sublinear growth for all graphs, the superiority over the setting of No Communication and the fact that smaller |\u03bb 2 | leads to a smaller regret (regret of delay term in Thm. 2). A comparison between Figures 1a and 1b, further confirms the slightly better regret performance of DLUCB compared to RC-DLUCB (but the latter has superior communication cost). Figure 1c emphasizes the value of collaboration in speeding up the learning process. It depicts the per-agent regret of DLUCB on random graphs with N = 5, 10 and 15 nodes and compares their performance with the single-agent LUCB. Clearly, as the number of agents increases, each agent learns the environment faster as an individual.\n\n\nConclusion\n\nIn this paper, we proposed two fully decentralized LB algorithms: 1) DLUCB and 2) RC-DLUCB with small communication cost. We also proposed Safe-DLUCB to address the problem of safe LB in multi-agent settings. We derived near-optimal regret bounds for all the aforementioned algorithms that are applicable to arbitrary, but fixed networks. An interesting open problem is to design decentralized algorithms with provable guarantees for settings with time-varying networks. Also, extensions to nonlinear settings and other types of safety-constraints are important future directions.\n\n[AAT20b] Sanae Amani, Mahnoosh Alizadeh, and Christos Thrampoulidis. Regret bounds for safe gaussian process bandit optimization. arXiv preprint arXiv:2005.01936, 2020.\n\n[AG13] Shipra Agrawal and Navin Goyal. \n\n\nA Regret analysis of DLUCB\n\nIn this section, we provide complete proofs of Theorem 1 and Lemmas 1 and 2. We also show how this completes the regret bound analysis and leads to Theorem 2.\n\n\nA.1 Proof of Theorem 1\n\nThe proof is mostly adapted from [AYPS11, Thm. 2]. We present the details for the reader's convenience. First, we give the following lemma.\n\nLemma 3. Let\ns i,t := t\u22121 \u03c4 =1 \u03b7 i,\u03c4 x i,\u03c4 , if t \u2208 [S] t\u2212S \u03c4 =1 N j=1 a 2 i,j \u03b7 j,\u03c4 x j,\u03c4 , if t \u2208 [S + 1 : T ].(8)\nThen, with probability at least 1 \u2212 \u03b4, for all i \u2208 [N ] and t \u2208 [T ]:\ns i,t 2 A \u22121 i,t \u2264 2(1 + \u01eb) 2 \u03c3 2 log N det(A i,t ) 1/2 \u03b4\u03bb d/2 .(9)\nProof. Proof of Theorem 1 in [AYPS11] with slight changes can be applied here. The difference comes from the fact that the noise variance in this setting is bounded by:\nmax(Var[\u03b7 j,\u03c4 ], Var[a i,j \u03b7 j,\u03c4 ]) \u2264 \u03c3 2 max i,j a 2 i,j \u2264 (1 + \u01eb) 2 \u03c3 2 , \u2200i, j \u2208 [N ], \u03c4 \u2208 [S + 1 : T ],(10)\nwhere the last inequality follows from the choice of S according to (3) which guarantees a i,j \u2264 1 + \u01eb for all i, j \u2208 [N ] (this can be seen by plugging \u03bd 0 = e i for all i \u2208 [N ] in (3)).\n\nWe are now ready to complete the proof of Theorem 1. For t \u2208 [S + 1 : T ] and i \u2208 [N ], we have:\n\u03b8 i,t = A \u22121 i,t b i,t = A \u22121 i,t \uf8ee \uf8f0 \u2212\u03bb\u03b8 * + \u03bb\u03b8 * + t\u2212S \u03c4 =1 N j=1 a 2 i,j x j,\u03c4 (x T j,\u03c4 \u03b8 * + \u03b7 j,\u03c4 ) \uf8f9 \uf8fb = A \u22121 i,t \uf8ee \uf8f0 A i,t \u03b8 * \u2212 \u03bb\u03b8 * + t\u2212S \u03c4 =1 N j=1 a 2 i,j \u03b7 j,\u03c4 x j,\u03c4 \uf8f9 \uf8fb = \u03b8 * \u2212 \u03bbA \u22121 i,t \u03b8 * + A \u22121 i,t s i,t .\nHence, for any x \u2208 R d , we get:\n| x,\u03b8 i,t \u2212 \u03b8 * | = | x, A \u22121 i,t s i,t \u2212 \u03bb x, A \u22121 i,t \u03b8 * | \u2264 x A \u22121 i,t s i,t A \u22121 i,t + \u03bb 1/2 \u03b8 * 2 .\nLemma 3 and plugging x = A \u22121 i,t (\u03b8 i,t \u2212 \u03b8 * ) imply that with probability at least 1 \u2212 \u03b4, for all i \u2208 [N ] and t \u2208 [S + 1 : T ]:\n\u03b8 i,t \u2212 \u03b8 * Ai,t \u2264 (1 + \u01eb)\u03c3 2 log N det(A i,t ) 1/2 \u03b4\u03bb d/2 + \u03bb 1/2 \u2264 (1 + \u01eb)\u03c3 dS log 2\u03bbdN + 2N 2 t \u03bbd\u03b4 + \u03bb 1/2 ,\nwhere the last inequality follows from the fact that det\n(A) = d i=1 \u03bb i (A) \u2264 (trace(A)/d) d , trace(A i,t ) \u2264\n(1 + \u01eb)trace(A * ,t\u2212S ) and \u01eb \u2264 1. We proved the theorem for t \u2208 [S + 1 : T ]. However, the result hold for rounds t \u2208 [S] by following the exact same argument.\n\n\nA.2 Proof of Theorem 2\n\nHere we prove the regret bound of DLUCB.\n\nConditioning on \u03b8 * \u2208 C i,t , \u2200i \u2208 [N ], t \u2208 [T ]. Consider the event\nE 1 := {\u03b8 * \u2208 C i,t , \u2200i \u2208 [N ], t \u2208 [T ]},(11)\nthat \u03b8 * is inside the confidence sets for all agents i \u2208 [N ] and rounds t \u2208 [T ]. By Theorem 1 the event holds with probability at least 1 \u2212 \u03b4. Onwards, we condition on this event, and make repeated use of the fact that \u03b8 * \u2208 C i,t for all i \u2208 [N ], t \u2208 [T ], without further explicit reference.\n\nDecomposing the regret. As is standard in regret analysis, we start with instantaneous regret decomposition. Let \u03b8 i,t , x i,t = max \u03bd\u2208Ci,t,x\u2208D \u03bd, x . We decompose the instantaneous regret of agent i at round t \u2208 [T ] as follows:\nr i,t = \u03b8 * , x * \u2212 \u03b8 * , x i,t \u2264 \u03b8 i,t , x i,t \u2212 \u03b8 * , x i,t \u2264 \u03b8 * \u2212\u03b8 i,t Ai,t x i,t A \u22121 i,t \u2264 2 min(\u03b2 t x i,t A \u22121 i,t , 1),(12)\nwhere the last inequality follows by conditioning on\u0112 1 and Assumption 3.\n\nIf information sharing was perfect at each round and also agents ran DLUCB in an asynchronous manner with no delay in using information, the current gram matrix of agent i round t could be computed using all the information gathered by all the agents in rounds 1, . . . , t \u2212 1, and also that gathered by agents j = 1, . . . , i \u2212 1 at round t:\nB i,t = A * ,t + i\u22121 j=1 x j,t x T j,t .(13)\nIn what follows, we prove Lemmas 1 and 2 to establish a connection between norms in regret decomposition (12) and those with respect to B i,t .\n\nProof of Lemma 1. \nSince A * ,t\u2212S+1 = \u03bbI A i,t for all t \u2208 [S] and i \u2208 [N ], we observe that x i,t 2 A \u22121 i,t \u2264 x i,t 2 A \u22121 * ,t\u2212S+1 \u2264 e x i,t 2 A \u22121 * ,t\u2212S+1(1 \u2212 \u01eb) 2 A * ,t\u2212S+1 \u2264 A i,t \u2264 (1 + \u01eb) 2 A * ,t\u2212S+1 ,(14)\nwhere the inequalities hold element-wise. Furthermore, for any positive semi-definite matrices A, B, and C such that A = B + C, we have:\ndet(A) \u2265 det(B), det(A) \u2265 det(C),(15)\nand for any x = 0 ([AYPS11, Lemm. 12]):\nx 2 A x 2 B \u2264 det(A) det(B) and x 2 B \u22121 x 2 A \u22121 \u2264 det(A) det(B) .(16)\nCombining (14), (15) and (16) yield that:\nx i,t 2 A \u22121 i,t (16) \u2264 x i,t 2 [(1+\u01eb) 2 A * ,t\u2212S+1] \u22121 det[(1 + \u01eb) 2 A * ,t\u2212S+1 ] det A i,t (15) \u2264 1 (1 + \u01eb) 2 x i,t 2 A \u22121 * ,t\u2212S+1 det[(1 + \u01eb) 2 A * ,t\u2212S+1 ] det[(1 \u2212 \u01eb) 2 A * ,t\u2212S+1 ] \u2264 1 + \u01eb 1 \u2212 \u01eb 2d x i,t 2 A \u22121 * ,t\u2212S+1 (17) = 1 + 2\u01eb 1 \u2212 \u01eb 2d x i,t 2 A \u22121 * ,t\u2212S+1 \u2264 e 4\u01ebd 1\u2212\u01eb x i,t 2 A \u22121 * ,t\u2212S+1 (18) \u2264 e x i,t 2 A \u22121 * ,t\u2212S+1 ,(19)\nas desired. Inequality (17) follows from the fact that det(\u03b1A) = \u03b1 d det(A) for any \u03b1 \u2208 R. In inequalities (18) and (19), we use 1 + a \u2264 e a for all real a and \u01eb \u2264 1/(4d + 1), respectively. Also note that inequality (17) gives a general upper bound when there is no assumption on how small \u01eb is.\n\n\nProof of Lemma 2.\n\nBy the definition of B i,t in (13), we have:\nx i,t 2 A \u22121 * ,t\u2212S+1 (16) \u2264 x i,t 2 B \u22121 i,t det B i,t det A * ,t\u2212S+1 (15) \u2264 x i,t 2 B \u22121 i,t det A * ,t+1 det A * ,t\u2212S+1 .(20)\nNote that for any t \u2264 1, det A * ,t = \u03bb d and det A * ,T \u2264 (trace(\nA * ,T )/d) d \u2264 \u03bb + N T d d\n, and consequently:\ndet A * ,T det A * ,2\u2212S = det A * ,T det A * ,3\u2212S = . . . = det A * ,T det A * ,0 = det A * ,T det A * ,1 \u2264 1 + N T d\u03bb d .(21)\nWithout loss of generality, we assume A * ,T \u2032 = A * ,T for any T \u2032 \u2265 T . Let m T = \u2308T /S\u2309. Then for any\n\u22121 \u2264 n \u2264 S \u2212 2, det A * ,T det A * ,\u2212n = mT +1 k=1 det A * ,kS\u2212n det A * ,(k\u22121)S\u2212n .(22)\nSince 1 \u2264 det A * ,kS\u2212n det A * ,(k\u22121)S\u2212n for all k \u2208 [m T + 1] and \u22121 \u2264 n \u2264 S \u2212 2, we can deduce from (21) and (22) that\ne \u2264 det A * ,t+1 det A * ,t\u2212S+1(23)\nfor at most Sd log 1 + N T d\u03bb number of rounds t \u2208 [T ]. This result coupled with (20) implies that:\nx i,t 2 A \u22121 * ,t\u2212S+1 \u2264 e x i,t 2 B \u22121 i,t(24)\nis true for all but at most \u03c8(\u03bb, |\u03bb 2 |, \u01eb, d, N, T ) = Sd log 1 + N T d\u03bb pairs of (i, t)\n\u2208 [N ] \u00d7 [T ].\nCompleting the proof of Theorem 2. We are now ready to complete the proof of Theorem 2. We call the pairs (i, t) that satisfy:\nx i,t 2 A \u22121 * ,t\u2212S+1 \u2264 e x i,t 2 B \u22121 i,t(25)\n\"good pairs\" and the rest \"bad pairs\", and define the set of good pairs by B good :\n= (i, t) \u2208 [N ] \u00d7 [T ], x i,t 2 A \u22121 * ,t\u2212S+1 \u2264 e x i,t 2 B\nWe first decompose the cumulative regret R T into components:\nR T (DLUCB) = R T,bad (DLUCB) + R T,good (DLUCB).(26)\nThe first term is the regret of bad pairs that is also bounded by its maximum possible number of them multiplied by 2, which is 2\u03c8(\u03bb, |\u03bb 2 |, \u01eb, d, N, T ). The second term is the regret of good pairs. We bound R T,good (DLUCB) as follows:\nR T,good (DLUCB) = (i,t)\u2208B good r i,t \u2264 2\u03b2 T (i,t)\u2208B good min(\u03b2 t x i,t A \u22121 i,t , 1) \u2264 2\u03b2 T N T (i,t)\u2208B good min x i,t 2 A \u22121 i,t , 1 Lemma 1 \u2264 2\u03b2 T eN T (i,t)\u2208B good min x i,t 2 A \u22121 * ,t\u2212S+1 , 1 Lemma 2 \u2264 2e\u03b2 T N T T t=S N i=1 min x i,t 2 B \u22121 i,t , 1 \u2264 2e\u03b2 T 2dN T log \u03bb + N T d .(27)\nIn the last inequality, we used the standard argument in regret analysis of linear bandit algorithm stated in the following [AYPS11, Lemma. 11]:\nn t=1 min y t 2 V \u22121 t , 1 \u2264 2 log det V n+1 det V 1 where V n = V 1 + n\u22121 t=1 y t y T t .(28)\nPutting things together, we conclude that, with probability at least 1 \u2212 \u03b4:\nR T (DLUCB) \u2264 2Sd log 1 + N T d\u03bb + 2e\u03b2 T 2dN T log \u03bb + N T d ,(29)\nas desired. Note that (29) holds for \u01eb \u2208 (0, 1/(4d + 1)). However, for an arbitrary \u01eb \u2208 (0, 1), we get a general regret bound following the inequality (17) and same argument above as follows:\nR T (DLUCB) \u2264 2Sd log 1 + N T dS + 2\u03b2 T 1 + \u01eb 1 \u2212 \u01eb d 2edN T log \u03bb + N T d .(30)\n\nA.3 Detailed version of DLUCB\n\nIn this section, we present a detailed pseudo code for DLUCB where we explicitly clarify how the decision rule and the communication steps are conducted.\n\nRemark 1 (Alternative communication scheme). In Section 2.1, we described how each agent communicates their information to the rest of the network with the goal of consensus. Here, we remark on the following alternative scheme. It is possible that each agent communicates the estimates of the following two quantities at rounds t + 1, . . . , t + S: (i) N j=1 x j,t x T j,t and (ii) N j=1 y j,t x j,t . After obtaining accurate estimates of the aforementioned quantities at the end of round t + S, each agent adds its estimate to its previous mixed information (its estimates of N j=1 x j,\u03c4 x T j,\u03c4 and N j=1 y j,\u03c4 x j,\u03c4 for all \u03c4 < t) and construct its confidence set. On the one hand, following this protocol, each agent sends d(d + 1) values per round to each of its neighbors. In comparison, the information sharing protocol described in Section 2.1 requires that each agent sends N (d + 1) values per round. On the other hand, the alternative information sharing protocol described here, requires memory of size Sd(d + 1) for each agent to keep their estimates at each round. This is in contrast to the storage requirement of size SN (d + 1) for the information sharing protocol of Section 2.1. Either of the two protocols lead to the same regret guarantees for DLUCB. The alternative presented here is to be preferred in terms of communication overhead (number of values that are communicated per round) and storage (number of values needed to be maintained by every agent at each round) when N \u226b d.\n\nAlgorithm 2: Detailed DLUCB for Agent i Input: D, N , d, |\u03bb 2 |, \u01eb, \u03bb, \u03b4, T 1 S = log(2N/\u01eb)/ 2 log(1/|\u03bb 2 |)\n2 A i,1 = \u03bbI, b i,1 = 0, A i,0 = A i,1 = B i,0 = B i,1 = \u2205 3 for t = 1, . . . , S do 4 Construct C i,t := {\u03bd \u2208 R d : \u03bd \u2212\u03b8 i,t Ai,t \u2264 \u03b2 t } where\u03b8 i,t = A \u22121 i,t b i,t\nand \u03b2 t is chosen as in Thm. 1.\n\n5 Play x i,t = arg max x\u2208D max \u03bd\u2208Ci,t \u03bd, x and observe y i,t .\n\n\n6\n\nA i,t .append(X i,t ) and B i,t .append(y i,t ) 7 for j = 1 : . . . , t do\n8 [A i,t+1 (j)] m,n = Comm([A i,t (j)] m,n , [A i,t\u22121 (j + 1)] m,n , S \u2212 j), \u2200m \u2208 [N ], n \u2208 [d] 9 [B i,t+1 (j)] m = Comm([B i,t (j)] m , [B i,t\u22121 (j + 1)] m , S \u2212 j), \u2200m \u2208 [N ] 10 A i,t+1 = A i,t + x i,t x T i,t , b i,t+1 = b i,t + y i,t x i,t 11 A i,S = \u03bbI, b i,S = 0 12 for t = S + 1, . . . , T do 13 A i,t = A i,t\u22121 + N 2 A i,t (1) T A i,t (1), b i,t = b i,t\u22121 + N 2 A i,t (1) T B i,t (1) 14 Construct C i,t := {\u03bd \u2208 R d : \u03bd \u2212\u03b8 i,t Ai,t \u2264 \u03b2 t } where\u03b8 i,t = A \u22121 i,t b i,t\nand \u03b2 t is chosen as in Thm. 1.\n\n15 Play x i,t = arg max x\u2208D max \u03bd\u2208Ci,t \u03bd, x and observe y i,t\n16 A i,t .remove A i,t (1) .append X i,t 17 B i,t .remove B i,t (1) .append y i,t 18 for j = 1 : . . . , S do 19 [A i,t+1 (j)] m,n = Comm([A i,t (j)] m,n , [A i,t\u22121 (j + 1)] m,n , S \u2212 j), \u2200m \u2208 [N ], n \u2208 [d] 20 [B i,t+1 (j)] m = Comm([B i,t (j)] m , [B i,t\u22121 (j + 1)] m , S \u2212 j), for all m \u2208 [N ]\nRemark 2 (Decentralized Linear TS). Thompson Sampling (TS) is another strategy to tackle the decentralized LB problem. A Decentralized Linear TS (DLTS) algorithm follows the same communication procedure as DLUCB. However, the decision rule is different, such that lines 12 of DLUCB changes to: \nx i,t = arg max x\u2208D \u03b8 i,t , x ,(31)where\u03b8 i,t =\u03b8 i,t + \u03b2 t A \u2212 1 2 i,t \u03c1 i,t with \u03c1 i,\n\nB Communication step\n\nIn this section, we summarize the accelerated Chebyshev communication step discussed in Section 2.1 in Algorithm 3, which follows the same steps as those of the communication algorithm presented in [MRKR19].\n\nChebyshev polynomials [You14] are defined as T 0 (x) = 1, T 1 (x) = x and T k+1 (x) = 2xT k (x) \u2212 T k\u22121 (x). Define:\nq \u2113 (P) = T \u2113 (P/|\u03bb 2 |) T \u2113 (1/|\u03bb 2 |) .(32)\nBy the properties of Chebyshev polynomial [AS14], it can be shown that:\nq \u2113+1 (P) = 2w \u2113 |\u03bb 2 |w \u2113+1 Pq \u2113 (P) \u2212 w \u2113\u22121 w \u2113+1 q \u2113\u22121 (P),(33)\nwhere w 0 = 1, w 1 = 1/|\u03bb 2 |, w \u2113+1 = 2w \u2113 /|\u03bb 2 |\u2212w \u2113\u22121 , q 0 (P) = I and q 1 (P) = P. This implies that, when agents share an specific quantity, whose initial values given by agents are denoted by vector \u03bd 0 \u2208 R N , by using the recursive Chebyshev-accelerated updating rule, they have:\n\u03bd \u2113+1 = 2w \u2113 |\u03bb 2 |w \u2113+1 P\u03bd \u2113 \u2212 w \u2113\u22121 w \u2113+1 \u03bd \u2113\u22121 .(34)\nIn light of the above mentioned recursive procedure, the accelerated communication step is summarized in Algorithm 3 below for agent i. We denote the inputs by: 1) x now , which is the quantity of interest that agent i wants to update at the current round, 2) x prev , which is the estimated value for a quantity of interest that agent i updated at the previous round, and 3) \u2113 which is the current round of communication. Note that inputs are scalars, however matrices and vectors also can be passed as inputs with Comm running for each of their entries.\n\nAlgorithm 3: Comm for Agent i Input: x now , x prev , \u2113 1 w 0 = 0, w 1 = 1/|\u03bb 2 |, w r = 2w r\u22121 /|\u03bb 2 |\u2212w r\u22122 \u22002 \u2264 r \u2264 S 2 x i,now = x now , x i,prev = x prev 3 Send x i,now and receive the corresponding x j,now to and from j \u2208 N (i) // Recall that all agents run Comm in parallel\n4 if \u2113 = 1 then 5 x i,next = P i,i x i,now + j\u2208N (i) P i,j x j,now 6 else 7 x i,next = 2w \u2113\u22121 |\u03bb2|w \u2113 P i,i x i,now + 2w \u2113\u22121 |\u03bb2|w \u2113 j\u2208N (i) P i,j x j,now \u2212 w \u2113\u22122 w \u2113 x i,prev 8 Return x i,next\n\nC DLUCB with Rare Communication\n\nIn this section, we describe RC-DLUCB summarized in Algorithm 4. Technical details provided in this section are mostly adapted from the tools we used in the previous sections for the analysis of DLUCB.\n\nIn RC-DLUCB, at round t, agent i uses all its available information to first compute A i,t and b i,t with new definitions which we will elaborate on later in this section, and then it creates confidence set C i,t as in (5). The volume of the confidence ellipsoid C i,t depends on det A i,t . If for any agent i, the corresponding det A i,t varies greatly, it needs extra information to shrink its confidence set. As such, agents keep running UCB decision rule until at some round t, one of the agents, say agent i, faces a relatively large increase in det A i,t . Under such a condition (Line 7 of Algorithm 4), a communication phase begins. When a communication phase begins, all the agents share their unmixed information with their neighbors for S rounds. During these S rounds, all agents keep playing the action that was selected based on the last time they updated their confidence sets. We define CP k to be the event that the k-th communication phase has started.\n\nWe denote the round at which the k-th communication phase ends by t k . The set of rounds between t k\u22121 and t k are referred to as epoch k and is denoted by the ordered set EP k := [t k\u22121 + 1 : t k ]. In particular, at rounds t k\u22121 + 1, . . . , t k \u2212 S, agents run UCB decision rule independently with no communication and at the last S rounds of each epoch EP k (the k-th communication phase), they communicate their unmixed information and play the actions that were selected based on the last time they updated their confidence sets, x i,t k \u2212S .\n\nNote that, the quantities that agents communicate with each other during the communication phase are matrices and vectors (rather than vectors and scalars in DLUCB). To make this concrete, suppose that no communication has occurred between rounds s and t. The goal of a communication phase starting at round t + 1 is to provide each agent with accurate estimates of: 1) \nA i,1 = \u03bbI, b i,1 = 0, W i,new = W i,syn = 0, v i,new = v i,syn = 0, t = 1, k = 1, t 0 = 0 3 while t \u2264 T do 4 Construct C i,t := {\u03bd \u2208 R d : \u03bd \u2212\u03b8 i,t Ai,t \u2264 \u03b2 t }, where\u03b8 i,t = A \u22121 i,t b i,t\nand \u03b2 t is chosen as in Thm. 1.\n\n5 Play x i,t = arg max x\u2208D max \u03bd\u2208Ci,t \u03bd, x and observe y i,t .\n6 W i,new = W i,new + x i,t x T i,t , v i,new = v i,new + y i,t x i,t 7 A i,t+1 = \u03bbI + W i,syn + W i,new , b i,t+1 = v i,syn + v i,new 8 if log det A i,t+1 /det A i,t k\u22121 +1 (t \u2212 t k\u22121 ) > M then 9\nStart k-th Communication phase. \nW j,syn = W j,syn + N M j,S+1 , v j,syn = v j,syn + N n j,S+1 20 W j,new = Sx j,t x T j,t , v j,new = S \u2113=1 y j,t+\u2113 21 A j,t+S+1 = \u03bbI + W j,syn + W j,new , b j,t+S+1 = v j,syn + v j,new 22 t k = t + S, t = t k + 1, k = k + 1 23 else 24 t = t + 1\nat round t + S. To this end, at round t + 1, each agent i sends t \u03c4 =s x i,\u03c4 x T i,\u03c4 and t \u03c4 =s y i,\u03c4 x i,\u03c4 to its neighbors and receives the corresponding quantities from them. The agents keep updating their estimates of t \u03c4 =s N j=1 x j,\u03c4 x T j,\u03c4 and t \u03c4 =s N j=1 y j,\u03c4 x j,\u03c4 by running the accelerated consensus (Algorithm 3) for S rounds. Now, we define the sufficient statistics available for agent i at round t to construct C i,t as follows:\nA i,t = \u03bbI + t k\u22121 \u03c4 =1 N j=1 a i,j x j,\u03c4 x T j,\u03c4 + t\u22121 \u03c4 =t k\u22121 +1 x i,\u03c4 x T i,\u03c4 , if t \u2208 [t k\u22121 + 1 : t k \u2212 S], \u03bbI + t k\u22121 \u03c4 =1 N j=1 a i,j x j,\u03c4 x T j,\u03c4 + t k \u2212S\u22121 \u03c4 =t k\u22121 +1 x i,\u03c4 x T i,\u03c4 , if t \u2208 [t k \u2212 S + 1 : t k ],(35)b i,t = t k\u22121 \u03c4 =1 N j=1 a i,j y j,\u03c4 x j,\u03c4 + t\u22121 \u03c4 =t k\u22121 +1 y i,\u03c4 x i,\u03c4 , if t \u2208 [t k\u22121 + 1 : t k \u2212 S], t k\u22121 \u03c4 =1 N j=1 a i,j y j,\u03c4 x j,\u03c4 + t k \u2212S\u22121 \u03c4 =t k +1 y i,\u03c4 x i,\u03c4 , if t \u2208 [t k \u2212 S + 1 : t k ].(36)\n\nC.1 Regret Analysis\n\nTheorem 4 (Regret of RC-DLUCB). Fix \u01eb \u2208 (0, 1/(2d + 1)) and \u03b4 \u2208 (0, 1). Let Assumptions 1, 2, 3 hold, and S be chosen as in (3). Then, with probability at least 1 \u2212 \u03b4, it holds that:\nR T (RC-DLUCB) \u2264 4\u03b2 T SN d log \u03bb + N T d / \u221a \u03bb + 4 log 1.5 \u03bb + N T d \u221a dN T = O N d 1.5 log(N d) log(1/|\u03bb 2 |) log 1.5 (N T ) + d log 2 (N T ) \u221a N T .(37)\nProof. By the instantaneous regret decomposition of agent i at round t, we have\nr i,t \u2264 2 min(\u03b2 t x i,t A \u22121 i,t , 1).(38)\nDefine:\u00c3\ni,t = \u03bbI + t k\u22121 \u03c4 =1 N j=1 x j,\u03c4 x T j,\u03c4 + t\u22121 \u03c4 =t k\u22121 +1 x i,\u03c4 x T i,\u03c4 , if t \u2208 [t k\u22121 + 1 : t k \u2212 S] \u03bbI + t k\u22121 \u03c4 =1 N j=1 x j,\u03c4 x T j,\u03c4 + t k \u2212S\u22121 \u03c4 =t k\u22121 +1 x i,\u03c4 x T i,\u03c4 , if t \u2208 [t k \u2212 S + 1 : t k ](39)\nLet K be the total number of epochs (communication phases). Recall the definition of A * ,t in (1). Following the same argument in the proof of Lemma 1, for any \u01eb \u2208 (0, 1/(2d + 1)), i \u2208 [N ], k \u2208 [K] and t \u2208 EP k :\nx i,t 2 A \u22121 i,t \u2264 e x i,t 2 A \u22121 i,t \u2264 e x i,t 2 A \u22121 * ,t k\u22121 +1 \u2264 e x i,t 2 A \u22121 * ,t k +1 det A * ,t k +1 det A * ,t k\u22121 +1 . (40) Note that 1 \u2264 det A * ,t k +1\ndet A * ,t k\u22121 +1 for any k \u2208 [K], and also\ndet A * ,t K+1 +1 det A * ,t 0 +1 \u2264 1 + N T d\u03bb d .\nThus, by the definition of det A * ,t k\u22121 +1 \u2264 e}. We decompose the cumulative regret R T (RC-DLUCB) into two components as follows:\nB i,t in (13), we conclude that det A * ,t k +1 det A * ,t k\u22121 +1 \u2264 e and x i,t 2 A \u22121 i,t \u2264 e 2 x i,t 2 A \u22121 * ,t k +1 \u2264 e 2 x i,t 2 B \u22121 i,t(41)R T (RC-DLUCB) = R T,bad (RC-DLUCB) + R T,good (RC-DLUCB).(43)\nThe first term is the regret of bad epochs and the second term is the network's regret of rounds in good epochs. We first establish the upper bound on R T,good (RC-DLUCB) as follows:\nR T,good (RC-DLUCB) = 2\u03b2 T k\u2208P good t\u2208EP k N i=1 min x i,t A \u22121 i,t , 1 (41) \u2264 2e\u03b2 T k\u2208P good t\u2208EP k N i=1 min x i,t B \u22121 i,t , 1 \u2264 2e\u03b2 T T t=1 N i=1 min x i,t B \u22121 i,t , 1 (28) \u2264 2e\u03b2 T 2dN T log \u03bb + N T d .(44)\nNow suppose EP k is a bad epoch andR k (RC-DLUCB) is the regret incurred by the actions of this epoch.\nR k (RC-DLUCB) = 2\u03b2 T t k t=t k\u22121 +1 N i=1 min x i,t A \u22121 i,t , 1 = 2\u03b2 T N i=1 \uf8eb \uf8ed S x i,t k \u2212S A \u22121 i,t k \u2212S + t k \u2212S t=t k\u22121 +1 min x i,t A \u22121 i,t , 1 \uf8f6 \uf8f8 \u2264 2\u03b2 T \uf8eb \uf8ec \uf8edSN/ \u221a \u03bb + N i=1 (t k \u2212 t k\u22121 \u2212 S) t k \u2212S t=t k\u22121 +1 min x i,t 2 A \u22121 i,t , 1 \uf8f6 \uf8f7 \uf8f8 (28) \u2264 2\u03b2 T \uf8eb \uf8ec \uf8edSN/ \u221a \u03bb + N i=1 2 (t k \u2212 t k\u22121 \u2212 S) log det A i,t k \u2212S det A i,t k\u22121 +1 \uf8f6 \uf8f7 \uf8f8 Line 7 of Alg. 4 \u2264 4N \u03b2 T S/ \u221a \u03bb + \u221a M .(45)\nSince the number of bad epochs is at most \u03c6(\u03bb, d, N, T ), we deduce:\nR T,bad (RC-DLUCB) \u2264 4N \u03b2 T \u03c6(\u03bb, d, N, T ) S/ \u221a \u03bb + \u221a M ,(46)\nand:\nR T (RC-DLUCB) \u2264 4\u03b2 T \uf8eb \uf8ed N \u03c6(\u03bb, d, N, T ) S/ \u221a \u03bb + \u221a M + e dN T log \u03bb + N T d \uf8f6 \uf8f8(47)\nBy the choice of M = T log(1+N T /d\u03bb) dN , we have: \nR T (RC-DLUCB) \u2264 4\u03b2 T SN d log \u03bb + N T d / \u221a \u03bb + 4 log 1.5 \u03bb + N T d \u221a dN T = O N d 1.5 log(N d) log(1/|\u03bb 2 |) log 1.5 (N T ) + d log 2 (N T ) \u221a N T .(48)\n\nC.2 Communication Cost\n(1 \u2212 \u01eb)A * ,t k +1 \u2264 A i,t k +1 \u2264 (1 + \u01eb)A * ,t k +1 .(49)\nNow, let the duration of k-th epoch be less than a fixed constant C > 0 and assume agent i has triggered the k-th communication phase. Following the same arguments in concluding (19) from (17), we conclude from (15), (49) and \u01eb \u2208 (0, 1/(2d + 1)) that\n1 + log det A * ,t k +1 /det A * ,t k\u22121 +1 C \u2265 log det A i,t k +1 /det A i,t k\u22121 +1 (t k \u2212 t k\u22121 ) > M,(50)\nHence:\nlog det A * ,t k +1 det A * ,t k\u22121 +1 > M/C \u2212 1.(51)\nCombining (51) and the fact that\nK k=1 log det A * ,t k +1 det A * ,t k\u22121 +1 = log det A * ,t K +1 det A * ,t 0 +1 = log det A * ,T +1 det A * ,1 \u2264 \u03c6(\u03bb, d, N, T ),\nwe conclude that there are at most \u2308 \u03c6(\u03bb,d,N,T ) M/C\u22121 \u2309 number of epochs with length less than C. Clearly, the number of epochs with length larger than C is at most \u2308T /C\u2309. Thus, plugging C = M \u221a \u03c6(\u03bb,d,N,T )M/T +1 , we deduce that the total number of epochs is at most: \n\u2308T /C\u2309 + \u03c6(\u03bb, d, N, T ) M/C \u2212 1 = O T \u03c6(\u03bb, d, N, T ) M = O(d \u221a N ).(52)\n\nD Safe-DLUCB\n\nIn this section, we provide a more elaborate discussion on the content of Section 3. We begin with a simple example from ad placement that serves as a motivation for the setting of safe distributed LBs that we consider here. On the distributed side, advertisement companies (aka agents) may decide to cooperate with a common goal of speeding up the learning of their common users' preferences. On the safety side, some users may be sensitive on certain types of ads (e.g. political, religious), which if they encounter, they will immediately terminate their subscription. Thus, the agents can classify such ads as unsafe and the remaining as safe, and try to avoid offering the unsafe ones with high probability. We present our Safe-DLUCB in Algorithm 5.\n\nIn what follows, we introduce the missing technical details of Section 3 that eventually lead to the proof of Theorem 3.\n\n\nD.1 Confidence Sets\n\nWe start the technical details of this section by discussion on how the confidence sets E i,t are constructed. At round t, agent i computes the following:\nA \u22a5 i,t = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03bb I \u2212x 0x T 0 + t\u22121 \u03c4 =1 x \u22a5 i,\u03c4 x \u22a5 i,\u03c4 T , if t \u2208 [S], \u03bb I \u2212x 0x T 0 + t\u2212S \u03c4 =1 N j=1 a 2 i,j x \u22a5 j,\u03c4 x \u22a5 j,\u03c4 T , if t \u2208 [S + 1 : T ],(53)r \u22a5 i,t = t\u22121 \u03c4 =1 z \u22a5 i,\u03c4 x \u22a5 i,\u03c4 , if t \u2208 [S], t\u2212S \u03c4 =1 N j=1 a 2 i,j z \u22a5 j,\u03c4 x \u22a5 j,\u03c4 , if t \u2208 [S + 1 : T ],(54)\nwhere,\nz \u22a5 j,t := \u00b5 \u22a5 * , x \u22a5 j,t + \u03b6 j,t = \u00b5 * , x j,t \u2212 \u00b5 o * , x o j,t + \u03b6 j,t = z j,t \u2212 \u00b5 o * , x o j,t = z j,t \u2212 x j,t ,x 0 x 0 c 0 .(55)\nNext, agent i constructs the confidence set\nE i,t := {\u03bd \u2208 R d : \u03bd \u2212\u03bc \u22a5 i,t A \u22a5 i,t \u2264 \u03b2 t },(56)where\u03bc \u22a5 i,t = A \u22a5 i,t \u22121 r \u22a5 i,t .\nSimilar arguments to those in the proof of Thm. 1 guarantees that if \u03b2 t is chosen according to Thm. 1, then \u00b5 \u22a5 * \u2208 E i,t for all i \u2208 [N ] and t \u2208 [T ] with probability at least 1 \u2212 \u03b4.\nConditioning on \u00b5 \u22a5 * \u2208 E i,t , \u2200i \u2208 [N ], t \u2208 [T ]\n. Consider the event\nE 2 := {\u00b5 \u22a5 * \u2208 E i,t , \u2200i \u2208 [N ], t \u2208 [T ]},(57)\nthat \u00b5 \u22a5 * is inside the confidence sets for all agents i \u2208 [N ] and rounds t \u2208 [T ]. Following the proof of Theorem 1, we can prove that the event holds with probability at least 1 \u2212 \u03b4. Onwards, we condition on this event, and make repeated use of the fact that \u00b5 \u22a5 * \u2208 E i,t for all i \u2208 [N ], t \u2208 [T ], without further explicit reference. Once E i,t is constructed, agent i creates the following safe inner approximation of the true D s (\u00b5 * ):\nD s i,t := {x \u2208 D : x o ,x 0 x 0 c 0 + \u03bc \u22a5 i,t , x \u22a5 + \u03b2 t x \u22a5 A \u22a5 i,t \u22121 \u2264 c}.(58)\nThanks to the following proposition, it is guaranteed that D s i,t is a set of safe actions with high probability. Proposition 1. For all i \u2208 [N ], t \u2208 [T ], and \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 all actions in D s i,t are safe, i.e., \u00b5 * , x \u2264 c, \u2200x \u2208 D s i,t .\n\nProof. Let x \u2208 D s i,t . By the definition of D s i,t in (58), we have\nx o ,x 0 x 0 c 0 + \u03bc \u22a5 i,t , x \u22a5 + \u03b2 t x \u22a5 A \u22a5 i,t \u22121 \u2264 c.(59)\nMoreover, using Cauchy-Schwarz inequality and conditioned on the event\u0112 2 in (57), we get\n| \u03bc \u22a5 i,t \u2212 \u00b5 \u22a5 * , x \u22a5 |\u2264 \u03b2 t x \u22a5 A \u22a5 i,t \u22121 \u21d2 \u00b5 \u22a5 * , x \u22a5 \u2264 \u03bc \u22a5 i,t , x \u22a5 + \u03b2 t x \u22a5 A \u22a5 i,t \u22121 .(60)\nNote that \u00b5 \u22a5 * ,\nx \u22a5 = \u00b5 * , x \u2212 \u00b5 o * , x o = \u00b5 * , x \u2212 x,x0 x0 c 0 .\nCombining this fact with (59) and (60) concludes that\n\u00b5 * , x = x,x 0 x 0 c 0 + \u00b5 \u22a5 * , x \u22a5 \u2264 x,x 0 x 0 c 0 + \u03bc \u22a5 i,t , x \u22a5 + \u03b2 t x \u22a5 A \u22a5 i,t \u22121 \u2264 c,(61)\nwhich implies that x is safe, as desired.\n\n\nD.2 Optimism in the face of Safety Constraint\n\nIn this section, we show how large the value of \u03ba r must be to provide enough exploration to the algorithm so that the selected actions in D s i,t are -often enoughoptimistic, i.e., \u03b8 i,t , x i,t \u2265 \u03b8 * , x * [AL + 17]. First, we state the following lemma borrowed from [PGBJ20] used in the proof of Lemma 5.\n\nLemma 4 ([PGBJ20]). For any vector x \u2208 R d , the following inequality holds:\nx \u22a5 A \u22a5 i,t \u22121 \u2264 x A \u22121 i,t .(62)\nProof. For a proof see [PGBJ20, Lemma 3]. Now, we state the main Lemma 5.\n\nLemma 5 (Optimism in the face of safety constraint). Let \u03ba r \u2265 2 c\u2212c0 + 1. Then for all i \u2208 [N ] and t \u2208 [T ], with probability at least 1 \u2212 \u03b4, \u03b8 i,t , x i,t \u2265 \u03b8 * , x * .\n\nProof. The proof of the lemma uses ideas introduced recently for single-agent LBs with linear constraints in [AAT19, MAAT19, PGBJ20]. We consider the following two cases:\n1) First, if x * \u2208 D s\ni,t , then by the definition of \u03b8 i,t , x i,t in (7), one can easily observe that\n\u03b8 i,t , x i,t \u2265 \u03b8 * , x * ,(63)\nas desired.\n\n2) Now, we focus on the other case when x * / \u2208 D s i,t , which means\nx o * ,x 0 x 0 c 0 + \u03bc \u22a5 i,t , x \u22a5 * + \u03b2 t x \u22a5 * A \u22a5 i,t \u22121 > c.(64)\nRecall thatx 0 = x0 x0 2 and note that x o 0 = x 0 and x \u22a5 0 = 0. Thus,\nx o 0 ,x0 x0 c 0 + \u03bc \u22a5 i,t , x \u22a5 0 + \u03b2 t x \u22a5 0 A \u22a5 i,t \u22121 = c 0 < c, which implies that x 0 \u2208 D s i,t . Now, for each i \u2208 [N ] and t \u2208 [T ], let w i,t := \u03b1 i,t x * + (1 \u2212 \u03b1 i,t )\nx 0 be a point on the line connecting x * and x i,t such that w i,t is on the boundary of D s i,t . Formally:\n\u03b1 i,t := max{\u03b1 \u2208 [0, 1] : \u03b1x * + (1 \u2212 \u03b1)x 0 \u2208 D s i,t } (65) Note that \u03b1x * + (1 \u2212 \u03b1)x 0 o = \u03b1x o * + (1 \u2212 \u03b1)x 0 and \u03b1x * + (1 \u2212 \u03b1)x 0 \u22a5 = \u03b1x \u22a5 * . By the definition of D s i,t\nin (58) and Proposition 1 we have\n\u03b1 i,t := max \uf8f1 \uf8f2 \uf8f3 \u03b1 \u2208 [0, 1] : \u03b1 x o * ,x 0 + (1 \u2212 \u03b1) x 0 ,x 0 x 0 c 0 + \u03b1 \u03bc \u22a5 i,t , x \u22a5 * + \u03b2 t x \u22a5 * A \u22a5 i,t \u22121 \u2264 c \uf8fc \uf8fd \uf8fe .(66)\nThe definition of \u03b1 i,t in (66) implies that\n(1 \u2212 \u03b1 i,t ) x 0 ,x 0 x 0 c 0 + \u03b1 i,t x o * ,x 0 x 0 c 0 + \u03bc \u22a5 i,t , x \u22a5 * + \u03b2 t x \u22a5 * A \u22a5 i,t \u22121 = c.(67)Let M = x o * ,x0 x0 c 0 + \u03bc \u22a5 i,t , x \u22a5 * + \u03b2 t x \u22a5 * A \u22a5 i,t \u22121 .\nNote that due to (64), M > c, and recall thatx 0 = x0\nx0 2 .\nThen, we have\n0 < \u03b1 i,t = c \u2212 x0,x0 x0 c 0 M \u2212 x0,x0 x0 c 0 = c \u2212 c 0 M \u2212 c 0 < 1.(68)Now, we show that \u03b1 i,t \u2265 c\u2212c0 c\u2212c0+2\u03b2t x \u22a5 * ( A \u22a5 i,t ) \u22121\n, which eventually leads to a proper value for \u03ba r > 1 that guarantees that the selected safe action x i,t from the conservative safe set D s i,t is always optimistic, i.e., \u03b8 i,t , x i,t \u2265 \u03b8 * , x * , with high probability.\n\nIn order to lower bound \u03b1 i,t (upper bound M ), we first rewrite M as the following:\nM = x o * ,x 0 x 0 c 0 + \u00b5 * , x \u22a5 * + \u03bc \u22a5 i,t \u2212 \u00b5 * , x \u22a5 * + \u03b2 t x \u22a5 * A \u22a5 i,t \u22121 ,(69)M 1 = (c \u2212 c 0 ) \u03b8 * , x * + (\u03ba r \u2212 1)M 2 (c \u2212 c 0 ) + 2M 2 \u2265 \u03b8 * , x * \u21d0\u21d2 (c \u2212 c 0 ) \u03b8 * , x * + (\u03ba r \u2212 1)M 2 \u2265 (c \u2212 c 0 + 2M 2 ) \u03b8 * , x * \u21d0\u21d2 (c \u2212 c 0 )(\u03ba r \u2212 1) \u2265 2 \u03b8 * , x * \u21d0\u21d2 (c \u2212 c 0 )(\u03ba r \u2212 1) \u2265 2 \u21d0\u21d2 \u03ba r \u2265 2 c \u2212 c 0 + 1,(74)\nas desired.\n\n\nD.3 Completing the proof of Theorem 3\n\nWe use the following decomposition for bounding the regret:\nr i,t = \u03b8 * , x * \u2212 \u03b8 * , x i,t = \u03b8 * , x * \u2212 \u03b8 i,t , x i,t Term I + \u03b8 i,t , x i,t \u2212 \u03b8 * , x i,t Term II(75)\nLemma 5 implies that for any i \u2208 [N ] and t \u2208 [T ] with probability at least 1 \u2212 \u03b4, Term II \u2264 0. Hence all that remains to complete the proof of Theorem 3 is to bound Term II which follows the same procedure as proof of Theorem 2. Putting things together, we conclude that for any \u01eb \u2208 (0, 1/(4d + 1)), \u03b4 \u2208 (0, 0.5), and \u03ba r \u2265 2 c\u2212c0 + 1, with probability at least 1 \u2212 2\u03b4, we have:\n\nR T (Safe-DLUCB) \u2264 4Sd log 1 + N T d\u03bb + 2e\u03ba r \u03b2 T 2dN T log \u03bb + N T d .\n\nRemark 3 (On Assumption 4). Before closing, we remark that Assumption 4 introduced in Section 3 is a slightly relaxed version of a corresponding assumption in [MAAT19]. Concretely, [MAAT19] assumes the known safe action to be the origin, x 0 = 0 and c 0 = 0. The relaxed Assumption 4 is borrowed by [PGBJ20] relying on the same assumption, which was introduced therein under a slightly relaxed notion of safety. All the experiments are implemented in Matlab on a 2020 MacBook Pro with 32GB of RAM. This section complements our discussions in Section 4 with extra details and experiments. In all our experiments, we have set \u01eb = 1/(4d + 1). In our simulations, we have implemented a slightly modified version of DLUCB such that at each round t > S, each agent further exploits its own information that was gathered in rounds 1, . . . , S. Concretely, we did not execute Line 11 of Algorithm 2. Our numerical results show that this minor modification performs marginally better than the original Algorithm 2. It is worth noting that our theoretical results (which hold for a more conservative algorithm where agents do not use their own information of rounds 1, . . . , S) remain true for the slightly improved version that we have implemented in this section.\n\n\nE Additional Experiments\n\nIn Figure 2, we present the graph topologies that were used in our experiments. For instance, recall that, in Figures 1a and 1b in Section 4, we ran DLUCB and RC-DLUCB on the graphs in Figures 2b, 2a, 2c, and 2d to show the effect of various graph topologies and connectivity level on the regret performance of DLUCB.\n\nFigures 3a, 3b, 3c, and 3d show the standard deviation around the regret averages that were depicted in Figures 1a and 1b. Averages are over 20 problem realizations. Specifically, we compare the regret of DLUCB and RC-DLUCB on different graph topologies. There are several observations worth emphasizing here. First, the plots confirm our theoretical findings that DLUCB slightly outperforms RC-DLUCB in all network topologies. However, recall that RC-DLUCB has significantly lower communication cost. Second, the simulations confirm that regret performance of either of the two algorithms is better the larger the spectral gap 1 \u2212 |\u03bb 2 |. Third, note that in the first S rounds, before any communication has yet happened, the regret grows almost linearly. The performance improves drastically, confirming the sub-linear trend of our bounds, once using mixed information begins. Further note that the value of S (depicted at each figure) increases as the |\u03bb 2 | increases; see Eqn.\n\n(3).\n\nNext, in Figure 3e, we compare the average regret of (i) Safe-DLUCB and (ii) DLUCB with oracle access to the unknown safe set for the reference. Here, we considered a Random Erd\u0151s-R\u00e9nyi graph with parameters p = 0.5 and N = 20. Moreover, \u00b5 * is drawn from N (0, I 5 ) and then normalized to unit norm and the constraint boundary c is drawn uniformly from [0,1] (x 0 = 0, c 0 = 0). As expected, DLUCB that assumes knowledge of \u00b5 * outperforms Safe-DLUCB. However, despite choosing actions conservatively from inner approximation safe sets, the performance of Safe-DLUCB appears acceptable. In particular it shows sublinear growth of similar order as that of DLUCB.\n\nAlso, Figure 3f shows the standard deviation around the regret averages that were depicted in Figure 1c over 20 problem realizations.\n\nFinally, in Figure 4 we have numerically confirmed the result of Theorem 5, according to which the number of communication phases of RC-DLUCB is O(d \u221a N ), i.e., independent on the time horizon T . Specifically, in Figure 4a, we have fixed d = 5 and random graphs with N = 10, 50, and 100 nodes and in Figure 4b \n\n\nfor t \u2208 [S] and i \u2208 [N ]. Thus, we focus on t \u2208 [S + 1 : T ]. Due to the choice of S in (3), we know |a i,j \u2212 1|\u2264 \u01eb for all i, j \u2208 [N ]. Hence, for all i \u2208 [N ] and t \u2208 [S + 1 : T ], we get:\n\n\nt \u223c N (0, I). The multivariate Gaussian distribution can be generalized to other appropriate distributions; see [AL + 17]. Following similar arguments in the analysis of the single agent TS algorithm [AG13, AL + 17] combined with our techniques in the previous sections of this paper, we can easily show that the regret of the DLTS is O(d log(N d) log 0.5 (1/|\u03bb2|) log(N T ) + d log(N T ) \u221a N T ).\n\n\ny j,\u03c4 x j,\u03c4 , Algorithm 4: RC-DLUCB for Agent i Input: D, N , d, |\u03bb 2 |,\n\n10 if\n10\u00bd{CP k } // if k-th communication phase has started. 11 then 12 M j,0 = 0, M j,1 = W j,new , n j,0 = 0, n j,1 = v j,new for all j \u2208 [N ] 13 for s=1,. . . ,S do 14 for Agent j = 1, . . . , N do 15 Play x j,t+s = x j,t and observe y j,t+s . 16 M j,s+1 = Comm(M j,s , M j,s\u22121 , s) 17 n j,s+1 = Comm(n j,s , n j,s\u22121 , s) 18 for Agent j = 1, . . . , N do 19\n\nFigure 2 :\n2Graph topologies\n\nFigure 3 :\n3, we have used a fixed random graph with N = 10 and settings with d = 2, 5, and 10. We ran RC-DLUCB for different values of the time-horizon T , and, for each of them, we have recorded the number of communication phases. The result confirms our theoretical findings in Section C.2 and (52) that the total number of communication phases does not depend on T . The shaded regions show standard deviation around the mean. The results are averages over 20 problem realizations. See text for details.\n\nFigure 4 :\n4Number of communication phases for different values of the time-horizon T for the RC-DLUCB algorithm. In agreement with Theorem 5 the number of communication phases is independent of T .\n\nTable 1 :\n1Comparison of DLUCB and RC-DLUCB to baseline, as well as, to state-of-the-art.See Section 2.4 \n\n\n\nThompson sampling for contextual bandits with linear payoffs. In International Conference on Machine Learning, pages 127-135, 2013. Vincent Zhuang, Joel W Burdick, and Yisong Yue. Stagewise safe bayesian optimization with gaussian processes. arXiv preprint arXiv:1806.07555, 2018.[WHCW19] Yuanhao Wang, Jiachen Hu, Xiaoyu Chen, and Liwei Wang. Distributed bandit learning: How much communication is needed to achieve (near) optimal regret. arXiv preprint arXiv:1904.06309, 2019.[AL + 17] \nMarc Abeille, Alessandro Lazaric, et al. Linear thompson sampling revisited. Electronic Journal \nof Statistics, 11(2):5165-5197, 2017. \n\n[AM19] \nOrly Avner and Shie Mannor. Multi-user communication networks: A coordinated multi-armed \nbandit approach. IEEE/ACM Transactions on Networking, 27(6):2192-2207, 2019. \n\n[AS14] \nMario Arioli and Jennifer Scott. Chebyshev acceleration of iterative refinement. Numerical \nAlgorithms, 66(3):591-608, 2014. \n\n[AYPS11] \nYasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, and Csaba Szepesv\u00e1ri. Improved algorithms for linear \nstochastic bandits. In Advances in Neural Information Processing Systems, pages 2312-2320, \n2011. \n\n[BKS16] \nFelix Berkenkamp, Andreas Krause, and Angela P Schoellig. Bayesian optimization with safety \nconstraints: safe and automatic parameter tuning in robotics. arXiv preprint arXiv:1602.04450, \n2016. \n\n[DAW11] \nJohn C Duchi, Alekh Agarwal, and Martin J Wainwright. Dual averaging for distributed \noptimization: Convergence analysis and network scaling. IEEE Transactions on Automatic \ncontrol, 57(3):592-606, 2011. \n\n[DHK08] \nVarsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under \nbandit feedback. 2008. \n\n[KB19] \nKia Khezeli and Eilyan Bitar. Safe linear stochastic bandits. arXiv preprint arXiv:1911.09501, \n2019. \n\n[KGYVR17] Abbas Kazerouni, Mohammad Ghavamzadeh, Yasin Abbasi Yadkori, and Benjamin Van Roy. \nConservative contextual linear bandits. In Advances in Neural Information Processing Systems, \npages 3910-3919, 2017. \n\n[KSS16] \nNathan Korda, Bal\u00e1zs Sz\u00f6r\u00e9nyi, and Li Shuai. Distributed clustering of linear bandits in peer \nto peer networks. In Journal of machine learning research workshop and conference proceedings, \nvolume 48, pages 1301-1309. International Machine Learning Societ, 2016. \n\n[LHLK13] Shuai Li, Fei Hao, Mei Li, and Hee-Cheol Kim. Medicine rating prediction and recommendation \nin mobile social networks. In International conference on grid and pervasive computing, pages \n216-223. Springer, 2013. \n\n[LS18] \nTor Lattimore and Csaba Szepesv\u00e1ri. Bandit algorithms. preprint, page 28, 2018. \n\n[LSL16a] \nPeter Landgren, Vaibhav Srivastava, and Naomi Ehrich Leonard. Distributed cooperative \ndecision-making in multiarmed bandits: Frequentist and bayesian algorithms. In 2016 IEEE \n55th Conference on Decision and Control (CDC), pages 167-172. IEEE, 2016. \n\n[LSL16b] \nPeter Landgren, Vaibhav Srivastava, and Naomi Ehrich Leonard. On distributed cooperative \ndecision-making in multiarmed bandits. In 2016 European Control Conference (ECC), pages \n243-248. IEEE, 2016. \n\n[Lyn96] \nNancy A Lynch. Distributed algorithms. Elsevier, 1996. \n\n[MAAT19] Ahmadreza Moradipari, Sanae Amani, Mahnoosh Alizadeh, and Christos Thrampoulidis. Safe \nlinear thompson sampling. arXiv preprint arXiv:1911.02156, 2019. \n[MRKR19] David Mart\u00ednez-Rubio, Varun Kanade, and Patrick Rebeschini. Decentralized cooperative \nstochastic bandits. In Advances in Neural Information Processing Systems, pages 4531-4542, \n2019. \n\n[PGBJ20] \nAldo Pacchiano, Mohammad Ghavamzadeh, Peter Bartlett, and Heinrich Jiang. Stochastic \nbandits with linear constraints. arXiv preprint arXiv:2006.10185, 2020. \n\n[RT10] \nPaat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics \nof Operations Research, 35(2):395-411, 2010. \n\n[SBB + 17] \nKevin Seaman, Francis Bach, S\u00e9bastien Bubeck, Yin Tat Lee, and Laurent Massouli\u00e9. Optimal \nalgorithms for smooth and strongly convex distributed optimization in networks. In Proceedings \nof the 34th International Conference on Machine Learning-Volume 70, pages 3027-3036. JMLR. \norg, 2017. \n\n[SBFH + 13] Bal\u00e1zs Sz\u00f6r\u00e9nyi, R\u00f3bert Busa-Fekete, Istv\u00e1n Heged\u0171s, R\u00f3bert Orm\u00e1ndi, M\u00e1rk Jelasity, and \nBal\u00e1zs K\u00e9gl. Gossip-based distributed stochastic bandit algorithms. In Journal of Machine \nLearning Research Workshop and Conference Proceedings, volume 2, pages 1056-1064. Inter-\nnational Machine Learning Societ, 2013. \n\n[SGBK15] Yanan Sui, Alkis Gotovos, Joel W Burdick, and Andreas Krause. Safe exploration for op-\ntimization with gaussian processes. Proceedings of Machine Learning Research, 37:997-1005, \n2015. \n\n[SZBY18] \nYanan Sui, [XB04] \nLin Xiao and Stephen Boyd. Fast linear iterations for distributed averaging. Systems & Control \nLetters, 53(1):65-78, 2004. \n\n[You14] \nDavid M Young. Iterative solution of large linear systems. Elsevier, 2014. \n\n\nAcknowledgmentsThis work is supported by the National Science Foundation under Grant Number (1934641).Algorithm 5: Safe-DLUCB for Agent iInput: x 0 , c 0 , c, D, N , d, |\u03bb 2 |, \u01eb, \u03bb, \u03b4, T 1 S = log(2N/\u01eb)/ 2 log(1/|\u03bb 2 |),r \u22a5 i,t and \u03b2 t is chosen as in Thm. 1.r \u22a5 i,t and \u03b2 t is chosen as in Thm. 1.Play x i,t = arg max x\u2208D s i,t max \u03bd\u2208\u03bar Ci,t \u03bd, x and observe y i,t and z i,t .\nLinear stochastic bandits under safety constraints. Sanae Amani, Mahnoosh Alizadeh, Christos Thrampoulidis, Advances in Neural Information Processing Systems. Sanae Amani, Mahnoosh Alizadeh, and Christos Thrampoulidis. Linear stochastic bandits under safety constraints. In Advances in Neural Information Processing Systems, pages 9252- 9262, 2019.\n\nGeneralized linear bandits with safety constraints. Sanae Amani, Mahnoosh Alizadeh, Christos Thrampoulidis, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEESanae Amani, Mahnoosh Alizadeh, and Christos Thrampoulidis. Generalized linear bandits with safety constraints. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3562-3566. IEEE, 2020.\n", "annotations": {"author": "[{\"end\":130,\"start\":78},{\"end\":194,\"start\":131}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":84},{\"end\":153,\"start\":140}]", "author_first_name": "[{\"end\":83,\"start\":78},{\"end\":139,\"start\":131}]", "author_affiliation": "[{\"end\":129,\"start\":91},{\"end\":193,\"start\":155}]", "title": "[{\"end\":65,\"start\":1},{\"end\":259,\"start\":195}]", "venue": null, "abstract": "[{\"end\":6506,\"start\":271}]", "bib_ref": "[{\"end\":7138,\"start\":7132},{\"end\":9454,\"start\":9447},{\"end\":28089,\"start\":28081},{\"end\":36524,\"start\":36517}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":69539,\"start\":69347},{\"attributes\":{\"id\":\"fig_2\"},\"end\":69939,\"start\":69540},{\"attributes\":{\"id\":\"fig_3\"},\"end\":70014,\"start\":69940},{\"attributes\":{\"id\":\"fig_4\"},\"end\":70376,\"start\":70015},{\"attributes\":{\"id\":\"fig_5\"},\"end\":70406,\"start\":70377},{\"attributes\":{\"id\":\"fig_6\"},\"end\":70915,\"start\":70407},{\"attributes\":{\"id\":\"fig_7\"},\"end\":71115,\"start\":70916},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":71223,\"start\":71116},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":76096,\"start\":71224}]", "paragraph": "[{\"end\":8550,\"start\":6522},{\"end\":8784,\"start\":8552},{\"end\":10503,\"start\":8802},{\"end\":11208,\"start\":10505},{\"end\":11773,\"start\":11244},{\"end\":12240,\"start\":11775},{\"end\":12658,\"start\":12242},{\"end\":12777,\"start\":12660},{\"end\":12883,\"start\":12779},{\"end\":13812,\"start\":12916},{\"end\":14602,\"start\":13899},{\"end\":15139,\"start\":14663},{\"end\":17007,\"start\":15141},{\"end\":18178,\"start\":17104},{\"end\":18242,\"start\":18239},{\"end\":19074,\"start\":18244},{\"end\":19303,\"start\":19076},{\"end\":19851,\"start\":19372},{\"end\":20143,\"start\":19954},{\"end\":20317,\"start\":20227},{\"end\":20589,\"start\":20391},{\"end\":20735,\"start\":20640},{\"end\":20875,\"start\":20737},{\"end\":21107,\"start\":20904},{\"end\":21208,\"start\":21109},{\"end\":21356,\"start\":21296},{\"end\":21845,\"start\":21787},{\"end\":22924,\"start\":22060},{\"end\":24029,\"start\":22926},{\"end\":24573,\"start\":24031},{\"end\":24773,\"start\":24628},{\"end\":25117,\"start\":24808},{\"end\":25222,\"start\":25119},{\"end\":25409,\"start\":25309},{\"end\":25613,\"start\":25542},{\"end\":25902,\"start\":25648},{\"end\":25948,\"start\":25943},{\"end\":26037,\"start\":25988},{\"end\":26158,\"start\":26137},{\"end\":26615,\"start\":26258},{\"end\":26796,\"start\":26617},{\"end\":27267,\"start\":26852},{\"end\":28924,\"start\":27301},{\"end\":29762,\"start\":28947},{\"end\":31336,\"start\":30187},{\"end\":32263,\"start\":31338},{\"end\":32723,\"start\":32301},{\"end\":32852,\"start\":32725},{\"end\":33038,\"start\":32854},{\"end\":33737,\"start\":33040},{\"end\":33904,\"start\":33818},{\"end\":34084,\"start\":34054},{\"end\":34905,\"start\":34139},{\"end\":35272,\"start\":34907},{\"end\":35479,\"start\":35274},{\"end\":35657,\"start\":35542},{\"end\":37589,\"start\":35673},{\"end\":38184,\"start\":37604},{\"end\":38354,\"start\":38186},{\"end\":38395,\"start\":38356},{\"end\":38584,\"start\":38426},{\"end\":38750,\"start\":38611},{\"end\":38764,\"start\":38752},{\"end\":38938,\"start\":38869},{\"end\":39175,\"start\":39007},{\"end\":39476,\"start\":39288},{\"end\":39574,\"start\":39478},{\"end\":39830,\"start\":39798},{\"end\":40068,\"start\":39937},{\"end\":40238,\"start\":40182},{\"end\":40454,\"start\":40294},{\"end\":40521,\"start\":40481},{\"end\":40592,\"start\":40523},{\"end\":40938,\"start\":40641},{\"end\":41169,\"start\":40940},{\"end\":41375,\"start\":41302},{\"end\":41721,\"start\":41377},{\"end\":41910,\"start\":41767},{\"end\":41930,\"start\":41912},{\"end\":42265,\"start\":42129},{\"end\":42343,\"start\":42304},{\"end\":42457,\"start\":42416},{\"end\":43096,\"start\":42801},{\"end\":43162,\"start\":43118},{\"end\":43358,\"start\":43292},{\"end\":43406,\"start\":43387},{\"end\":43638,\"start\":43534},{\"end\":43849,\"start\":43728},{\"end\":43986,\"start\":43886},{\"end\":44123,\"start\":44034},{\"end\":44265,\"start\":44139},{\"end\":44396,\"start\":44313},{\"end\":44518,\"start\":44457},{\"end\":44811,\"start\":44573},{\"end\":45245,\"start\":45101},{\"end\":45416,\"start\":45341},{\"end\":45675,\"start\":45484},{\"end\":45942,\"start\":45789},{\"end\":47449,\"start\":45944},{\"end\":47559,\"start\":47451},{\"end\":47758,\"start\":47727},{\"end\":47822,\"start\":47760},{\"end\":47902,\"start\":47828},{\"end\":48409,\"start\":48378},{\"end\":48472,\"start\":48411},{\"end\":49063,\"start\":48769},{\"end\":49381,\"start\":49174},{\"end\":49499,\"start\":49383},{\"end\":49617,\"start\":49546},{\"end\":49974,\"start\":49685},{\"end\":50586,\"start\":50031},{\"end\":50868,\"start\":50588},{\"end\":51298,\"start\":51097},{\"end\":52271,\"start\":51300},{\"end\":52822,\"start\":52273},{\"end\":53194,\"start\":52824},{\"end\":53417,\"start\":53386},{\"end\":53481,\"start\":53419},{\"end\":53712,\"start\":53680},{\"end\":54406,\"start\":53959},{\"end\":55046,\"start\":54864},{\"end\":55281,\"start\":55202},{\"end\":55333,\"start\":55325},{\"end\":55760,\"start\":55546},{\"end\":55969,\"start\":55926},{\"end\":56153,\"start\":56021},{\"end\":56545,\"start\":56363},{\"end\":56860,\"start\":56758},{\"end\":57321,\"start\":57253},{\"end\":57388,\"start\":57384},{\"end\":57528,\"start\":57476},{\"end\":58017,\"start\":57767},{\"end\":58132,\"start\":58126},{\"end\":58218,\"start\":58186},{\"end\":58621,\"start\":58350},{\"end\":59463,\"start\":58709},{\"end\":59585,\"start\":59465},{\"end\":59763,\"start\":59609},{\"end\":60045,\"start\":60039},{\"end\":60225,\"start\":60182},{\"end\":60498,\"start\":60313},{\"end\":60571,\"start\":60551},{\"end\":61068,\"start\":60622},{\"end\":61425,\"start\":61153},{\"end\":61497,\"start\":61427},{\"end\":61650,\"start\":61561},{\"end\":61771,\"start\":61754},{\"end\":61879,\"start\":61826},{\"end\":62021,\"start\":61980},{\"end\":62378,\"start\":62071},{\"end\":62456,\"start\":62380},{\"end\":62564,\"start\":62491},{\"end\":62737,\"start\":62566},{\"end\":62909,\"start\":62739},{\"end\":63014,\"start\":62933},{\"end\":63058,\"start\":63047},{\"end\":63129,\"start\":63060},{\"end\":63270,\"start\":63199},{\"end\":63559,\"start\":63450},{\"end\":63770,\"start\":63737},{\"end\":63946,\"start\":63902},{\"end\":64174,\"start\":64121},{\"end\":64195,\"start\":64182},{\"end\":64553,\"start\":64329},{\"end\":64639,\"start\":64555},{\"end\":64973,\"start\":64962},{\"end\":65074,\"start\":65015},{\"end\":65564,\"start\":65184},{\"end\":65637,\"start\":65566},{\"end\":66897,\"start\":65639},{\"end\":67243,\"start\":66926},{\"end\":68226,\"start\":67245},{\"end\":68232,\"start\":68228},{\"end\":68897,\"start\":68234},{\"end\":69032,\"start\":68899},{\"end\":69346,\"start\":69034}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13898,\"start\":13813},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14662,\"start\":14603},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17103,\"start\":17008},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18238,\"start\":18179},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19371,\"start\":19304},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19953,\"start\":19852},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20226,\"start\":20144},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20363,\"start\":20318},{\"attributes\":{\"id\":\"formula_8\"},\"end\":20390,\"start\":20363},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20639,\"start\":20590},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21295,\"start\":21209},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21786,\"start\":21357},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22059,\"start\":21846},{\"attributes\":{\"id\":\"formula_13\"},\"end\":24627,\"start\":24574},{\"attributes\":{\"id\":\"formula_14\"},\"end\":24807,\"start\":24774},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25308,\"start\":25223},{\"attributes\":{\"id\":\"formula_16\"},\"end\":25541,\"start\":25410},{\"attributes\":{\"id\":\"formula_17\"},\"end\":25647,\"start\":25614},{\"attributes\":{\"id\":\"formula_18\"},\"end\":25942,\"start\":25903},{\"attributes\":{\"id\":\"formula_19\"},\"end\":25987,\"start\":25949},{\"attributes\":{\"id\":\"formula_20\"},\"end\":26136,\"start\":26038},{\"attributes\":{\"id\":\"formula_21\"},\"end\":26257,\"start\":26159},{\"attributes\":{\"id\":\"formula_22\"},\"end\":26851,\"start\":26797},{\"attributes\":{\"id\":\"formula_23\"},\"end\":30186,\"start\":29763},{\"attributes\":{\"id\":\"formula_24\"},\"end\":33817,\"start\":33738},{\"attributes\":{\"id\":\"formula_25\"},\"end\":34053,\"start\":33905},{\"attributes\":{\"id\":\"formula_26\"},\"end\":34138,\"start\":34085},{\"attributes\":{\"id\":\"formula_27\"},\"end\":35541,\"start\":35480},{\"attributes\":{\"id\":\"formula_28\"},\"end\":38868,\"start\":38765},{\"attributes\":{\"id\":\"formula_29\"},\"end\":39006,\"start\":38939},{\"attributes\":{\"id\":\"formula_30\"},\"end\":39287,\"start\":39176},{\"attributes\":{\"id\":\"formula_31\"},\"end\":39797,\"start\":39575},{\"attributes\":{\"id\":\"formula_32\"},\"end\":39936,\"start\":39831},{\"attributes\":{\"id\":\"formula_33\"},\"end\":40181,\"start\":40069},{\"attributes\":{\"id\":\"formula_34\"},\"end\":40293,\"start\":40239},{\"attributes\":{\"id\":\"formula_35\"},\"end\":40640,\"start\":40593},{\"attributes\":{\"id\":\"formula_36\"},\"end\":41301,\"start\":41170},{\"attributes\":{\"id\":\"formula_37\"},\"end\":41766,\"start\":41722},{\"attributes\":{\"id\":\"formula_38\"},\"end\":42071,\"start\":41931},{\"attributes\":{\"id\":\"formula_39\"},\"end\":42128,\"start\":42071},{\"attributes\":{\"id\":\"formula_40\"},\"end\":42303,\"start\":42266},{\"attributes\":{\"id\":\"formula_41\"},\"end\":42415,\"start\":42344},{\"attributes\":{\"id\":\"formula_42\"},\"end\":42800,\"start\":42458},{\"attributes\":{\"id\":\"formula_43\"},\"end\":43291,\"start\":43163},{\"attributes\":{\"id\":\"formula_44\"},\"end\":43386,\"start\":43359},{\"attributes\":{\"id\":\"formula_45\"},\"end\":43533,\"start\":43407},{\"attributes\":{\"id\":\"formula_46\"},\"end\":43727,\"start\":43639},{\"attributes\":{\"id\":\"formula_47\"},\"end\":43885,\"start\":43850},{\"attributes\":{\"id\":\"formula_48\"},\"end\":44033,\"start\":43987},{\"attributes\":{\"id\":\"formula_49\"},\"end\":44138,\"start\":44124},{\"attributes\":{\"id\":\"formula_50\"},\"end\":44312,\"start\":44266},{\"attributes\":{\"id\":\"formula_51\"},\"end\":44456,\"start\":44397},{\"attributes\":{\"id\":\"formula_52\"},\"end\":44572,\"start\":44519},{\"attributes\":{\"id\":\"formula_53\"},\"end\":45100,\"start\":44812},{\"attributes\":{\"id\":\"formula_54\"},\"end\":45340,\"start\":45246},{\"attributes\":{\"id\":\"formula_55\"},\"end\":45483,\"start\":45417},{\"attributes\":{\"id\":\"formula_56\"},\"end\":45756,\"start\":45676},{\"attributes\":{\"id\":\"formula_57\"},\"end\":47726,\"start\":47560},{\"attributes\":{\"id\":\"formula_58\"},\"end\":48377,\"start\":47903},{\"attributes\":{\"id\":\"formula_59\"},\"end\":48768,\"start\":48473},{\"attributes\":{\"id\":\"formula_60\"},\"end\":49099,\"start\":49064},{\"attributes\":{\"id\":\"formula_61\"},\"end\":49150,\"start\":49099},{\"attributes\":{\"id\":\"formula_62\"},\"end\":49545,\"start\":49500},{\"attributes\":{\"id\":\"formula_63\"},\"end\":49684,\"start\":49618},{\"attributes\":{\"id\":\"formula_64\"},\"end\":50030,\"start\":49975},{\"attributes\":{\"id\":\"formula_65\"},\"end\":51062,\"start\":50869},{\"attributes\":{\"id\":\"formula_66\"},\"end\":53385,\"start\":53195},{\"attributes\":{\"id\":\"formula_67\"},\"end\":53679,\"start\":53482},{\"attributes\":{\"id\":\"formula_68\"},\"end\":53958,\"start\":53713},{\"attributes\":{\"id\":\"formula_69\"},\"end\":54634,\"start\":54407},{\"attributes\":{\"id\":\"formula_70\"},\"end\":54841,\"start\":54634},{\"attributes\":{\"id\":\"formula_71\"},\"end\":55201,\"start\":55047},{\"attributes\":{\"id\":\"formula_72\"},\"end\":55324,\"start\":55282},{\"attributes\":{\"id\":\"formula_73\"},\"end\":55545,\"start\":55334},{\"attributes\":{\"id\":\"formula_74\"},\"end\":55925,\"start\":55761},{\"attributes\":{\"id\":\"formula_75\"},\"end\":56020,\"start\":55970},{\"attributes\":{\"id\":\"formula_76\"},\"end\":56300,\"start\":56154},{\"attributes\":{\"id\":\"formula_77\"},\"end\":56362,\"start\":56300},{\"attributes\":{\"id\":\"formula_78\"},\"end\":56757,\"start\":56546},{\"attributes\":{\"id\":\"formula_79\"},\"end\":57252,\"start\":56861},{\"attributes\":{\"id\":\"formula_80\"},\"end\":57383,\"start\":57322},{\"attributes\":{\"id\":\"formula_81\"},\"end\":57475,\"start\":57389},{\"attributes\":{\"id\":\"formula_82\"},\"end\":57683,\"start\":57529},{\"attributes\":{\"id\":\"formula_83\"},\"end\":57766,\"start\":57708},{\"attributes\":{\"id\":\"formula_84\"},\"end\":58125,\"start\":58018},{\"attributes\":{\"id\":\"formula_85\"},\"end\":58185,\"start\":58133},{\"attributes\":{\"id\":\"formula_86\"},\"end\":58349,\"start\":58219},{\"attributes\":{\"id\":\"formula_87\"},\"end\":58693,\"start\":58622},{\"attributes\":{\"id\":\"formula_88\"},\"end\":59924,\"start\":59764},{\"attributes\":{\"id\":\"formula_89\"},\"end\":60038,\"start\":59924},{\"attributes\":{\"id\":\"formula_90\"},\"end\":60181,\"start\":60046},{\"attributes\":{\"id\":\"formula_91\"},\"end\":60277,\"start\":60226},{\"attributes\":{\"id\":\"formula_92\"},\"end\":60312,\"start\":60277},{\"attributes\":{\"id\":\"formula_93\"},\"end\":60550,\"start\":60499},{\"attributes\":{\"id\":\"formula_94\"},\"end\":60621,\"start\":60572},{\"attributes\":{\"id\":\"formula_95\"},\"end\":61152,\"start\":61069},{\"attributes\":{\"id\":\"formula_96\"},\"end\":61560,\"start\":61498},{\"attributes\":{\"id\":\"formula_97\"},\"end\":61753,\"start\":61651},{\"attributes\":{\"id\":\"formula_98\"},\"end\":61825,\"start\":61772},{\"attributes\":{\"id\":\"formula_99\"},\"end\":61979,\"start\":61880},{\"attributes\":{\"id\":\"formula_100\"},\"end\":62490,\"start\":62457},{\"attributes\":{\"id\":\"formula_101\"},\"end\":62932,\"start\":62910},{\"attributes\":{\"id\":\"formula_102\"},\"end\":63046,\"start\":63015},{\"attributes\":{\"id\":\"formula_103\"},\"end\":63198,\"start\":63130},{\"attributes\":{\"id\":\"formula_104\"},\"end\":63449,\"start\":63271},{\"attributes\":{\"id\":\"formula_105\"},\"end\":63736,\"start\":63560},{\"attributes\":{\"id\":\"formula_106\"},\"end\":63901,\"start\":63771},{\"attributes\":{\"id\":\"formula_107\"},\"end\":64053,\"start\":63947},{\"attributes\":{\"id\":\"formula_108\"},\"end\":64120,\"start\":64053},{\"attributes\":{\"id\":\"formula_109\"},\"end\":64181,\"start\":64175},{\"attributes\":{\"id\":\"formula_110\"},\"end\":64268,\"start\":64196},{\"attributes\":{\"id\":\"formula_111\"},\"end\":64328,\"start\":64268},{\"attributes\":{\"id\":\"formula_112\"},\"end\":64729,\"start\":64640},{\"attributes\":{\"id\":\"formula_113\"},\"end\":64961,\"start\":64729},{\"attributes\":{\"id\":\"formula_114\"},\"end\":65183,\"start\":65075}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27144,\"start\":27137},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27491,\"start\":27484},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30655,\"start\":30648},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31144,\"start\":31137},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32262,\"start\":32255}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":6520,\"start\":6508},{\"attributes\":{\"n\":\"1.3\"},\"end\":8800,\"start\":8787},{\"attributes\":{\"n\":\"2\"},\"end\":11242,\"start\":11211},{\"attributes\":{\"n\":\"2.1\"},\"end\":12914,\"start\":12886},{\"attributes\":{\"n\":\"2.2\"},\"end\":20902,\"start\":20878},{\"attributes\":{\"n\":\"2.3\"},\"end\":27299,\"start\":27270},{\"end\":28936,\"start\":28927},{\"end\":28945,\"start\":28939},{\"attributes\":{\"n\":\"3\"},\"end\":32299,\"start\":32266},{\"attributes\":{\"n\":\"4\"},\"end\":35671,\"start\":35660},{\"attributes\":{\"n\":\"5\"},\"end\":37602,\"start\":37592},{\"end\":38424,\"start\":38398},{\"end\":38609,\"start\":38587},{\"end\":40479,\"start\":40457},{\"end\":43116,\"start\":43099},{\"end\":45787,\"start\":45758},{\"end\":47826,\"start\":47825},{\"end\":49172,\"start\":49152},{\"end\":51095,\"start\":51064},{\"end\":54862,\"start\":54843},{\"end\":57707,\"start\":57685},{\"end\":58707,\"start\":58695},{\"end\":59607,\"start\":59588},{\"end\":62069,\"start\":62024},{\"end\":65013,\"start\":64976},{\"end\":66924,\"start\":66900},{\"end\":70021,\"start\":70016},{\"end\":70388,\"start\":70378},{\"end\":70418,\"start\":70408},{\"end\":70927,\"start\":70917},{\"end\":71126,\"start\":71117}]", "table": "[{\"end\":71223,\"start\":71206},{\"end\":76096,\"start\":71704}]", "figure_caption": "[{\"end\":69539,\"start\":69349},{\"end\":69939,\"start\":69542},{\"end\":70014,\"start\":69942},{\"end\":70376,\"start\":70024},{\"end\":70406,\"start\":70390},{\"end\":70915,\"start\":70420},{\"end\":71115,\"start\":70929},{\"end\":71206,\"start\":71128},{\"end\":71704,\"start\":71226}]", "figure_ref": "[{\"end\":36560,\"start\":36542},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36743,\"start\":36735},{\"end\":37119,\"start\":37102},{\"end\":37266,\"start\":37257},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":66937,\"start\":66929},{\"end\":67053,\"start\":67036},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":67129,\"start\":67111},{\"end\":67366,\"start\":67349},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":68252,\"start\":68243},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":68914,\"start\":68905},{\"end\":69002,\"start\":68993},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":69054,\"start\":69046},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":69258,\"start\":69249},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":69345,\"start\":69336}]", "bib_author_first_name": "[{\"end\":76533,\"start\":76528},{\"end\":76549,\"start\":76541},{\"end\":76568,\"start\":76560},{\"end\":76883,\"start\":76878},{\"end\":76899,\"start\":76891},{\"end\":76918,\"start\":76910}]", "bib_author_last_name": "[{\"end\":76539,\"start\":76534},{\"end\":76558,\"start\":76550},{\"end\":76582,\"start\":76569},{\"end\":76889,\"start\":76884},{\"end\":76908,\"start\":76900},{\"end\":76932,\"start\":76919}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":201058395},\"end\":76824,\"start\":76476},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":216339010},\"end\":77281,\"start\":76826}]", "bib_title": "[{\"end\":76526,\"start\":76476},{\"end\":76876,\"start\":76826}]", "bib_author": "[{\"end\":76541,\"start\":76528},{\"end\":76560,\"start\":76541},{\"end\":76584,\"start\":76560},{\"end\":76891,\"start\":76878},{\"end\":76910,\"start\":76891},{\"end\":76934,\"start\":76910}]", "bib_venue": "[{\"end\":76633,\"start\":76584},{\"end\":77032,\"start\":76934}]"}}}, "year": 2023, "month": 12, "day": 17}