{"id": 260055723, "updated": "2023-09-07 13:46:08.185", "metadata": {"title": "mmMIC: Multi-modal Speech Recognition based on mmWave Radar", "authors": "[{\"first\":\"Long\",\"last\":\"Fan\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Xinran\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Yi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Chuyu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Sanglu\",\"last\":\"Lu\",\"middle\":[]}]", "venue": "IEEE INFOCOM 2023 - IEEE Conference on Computer Communications", "journal": "IEEE INFOCOM 2023 - IEEE Conference on Computer Communications", "publication_date": {"year": 2023, "month": 5, "day": 17}, "abstract": "With the proliferation of voice assistants, microphone-based speech recognition technology usually cannot achieve good performance in the situation of multiple sound sources and ambient noises. In this paper, we propose a novel mmWave-based solution to perform speech recognition to tackle the issues of multiple sound sources and ambient noises, by precisely extracting the multi-modal features from lip motion and vocal-cords vibration from the single channel of mmWave. We propose a difference-based method for feature extraction of lip motion to suppress the dynamic interference from body motion and head motion. We propose a speech detection method based on cross-validation of lip motion and vocal-cords vibration so as to avoid wasting computing resources on nonspeaking activities. We propose a multi-modal fusion framework for speech recognition by fusing the signal features from lip motion and vocal-cords vibration with the attention mechanism. We implemented a prototype system and evaluated the performance in real test-beds. Experiment results show that the average speech recognition accuracy is 92.8% in realistic environments.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/infocom/FanXLLWL23", "doi": "10.1109/infocom53939.2023.10229085"}}, "content": {"source": {"pdf_hash": "dfb184e214b7ee6de6c6a36dd5ecc0384a9a2380", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "131e7f421c323ac4356f8b6ec919541f4ad9629f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/dfb184e214b7ee6de6c6a36dd5ecc0384a9a2380.txt", "contents": "\nmmMIC: Multi-modal Speech Recognition based on mmWave Radar\n\n\nLong Fan fanl@smail.nju.edu.cn \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nLei Xie lxie@nju.edu.cn \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nXinran Lu \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nYi Li yili@smail.nju.edu.cn \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nChuyu Wang \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nSanglu Lu sanglu@nju.edu.cn \nState Key Laboratory for Novel Software Technology\nNanjing University\nChina\n\nmmMIC: Multi-modal Speech Recognition based on mmWave Radar\n10.1109/INFOCOM53939.2023.10229085\nWith the proliferation of voice assistants, microphone-based speech recognition technology usually cannot achieve good performance in the situation of multiple sound sources and ambient noises. In this paper, we propose a novel mmWave-based solution to perform speech recognition to tackle the issues of multiple sound sources and ambient noises, by precisely extracting the multi-modal features from lip motion and vocal-cords vibration from the single channel of mmWave. We propose a difference-based method for feature extraction of lip motion to suppress the dynamic interference from body motion and head motion. We propose a speech detection method based on cross-validation of lip motion and vocal-cords vibration so as to avoid wasting computing resources on nonspeaking activities. We propose a multi-modal fusion framework for speech recognition by fusing the signal features from lip motion and vocal-cords vibration with the attention mechanism. We implemented a prototype system and evaluated the performance in real test-beds. Experiment results show that the average speech recognition accuracy is 92.8% in realistic environments.Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\n\nI. INTRODUCTION\n\nNowadays, with the practical use of speech recognition technology, voice assistants have been widely used in various application scenarios to bring more convenience to our lives. In particular, speech recognition in voice assistants is used to improve the efficiency of human-computer interaction in smart driving [1] [2], smart home [3], and smart medical care [4]. For example, for intelligent meeting minutes, voice assistants can be deployed in the meeting room to convert human voice to text. Moreover, for safe driving, voice assistants can also be used in intelligent driving to recognize the instructions from the drivers without manual touch. Current speech recognition technology in voice assistants is mainly based on a microphone to collect voice signals from human subjects. The microphonebased speech recognition [5] [6] works well in situations without other voice interference and environmental noises. However, in the situation of multiple sound sources and ambient noises, the speech recognition performance based on a microphone decreases dramatically. For example, when multiple passengers in the car are speaking simultaneously, multiple voices mix together in the voice assistant, which prevents the driver from effectively interacting with voice assistants. Therefore, new approaches are essentially required to collect voice-related signals from the human subject so as to ensure the performance of speech recognition.\n\nThere are two main approaches for speech recognition technology to improve speech recognition performance in the situation of multiple sound sources and ambient noises. The first approach is based on single-sensor speech recognition. It uses a single sensor, such as a wireless antenna or camera, to collect voice-related signals. Considering the wide coverage of wireless signals, researchers leverage the wireless signals such as WiFi [7]- [10], RFID [11]- [14], and mmWave [15]- [18] to perform speech recognition. Either the lip motion or the sound vibration is collected via a wireless channel for speech recognition. Besides, the camera-based solutions [19] are also investigated to collect the sound vibrations according to ultra-high-frame-rate video streams. However, the single-sensor-based approach usually fails to achieve good performance in speech recognition since they usually cannot recover the voice-related signals from the single channel well.\n\nThe second approach is based on multi-modal fusion from multiple sensors for speech enhancement and recognition. This approach uses multiple sensors with different modalities, such as audio, video, and wireless signals to collect the voicerelated signals simultaneously. The complementarity among different modalities is investigated and leveraged to enhance speech recognition performance. However, the existing multimodal fusion-based approaches either have privacy issues, e.g., the audio-visual fusion [20] [21], or have limitations in sensing range, e.g., the Ultrasound [22] [23]. Moreover, the multiple sensor-based approaches incur additional hardware costs and require essential synchronization in both time and space. In this paper, we propose a novel speech recognition solution called mmMIC based on mmWave, by extracting voicerelated signal features from multiple modalities, i.e., lip motion and vocal-cords vibration, as shown in Fig.1. Different from the previous approaches, mmMIC extracts the features of the above two modalities from a single channel, i.e., mmWave. We use mmWave radar to transmit frequency-modulated continuous waves (FMCW) to focus on the region of the lip and vocal cords. To extract the features from lip motion, we propose a feature extraction scheme for macro-motion based on Doppler velocity to represent the lip motion. Moreover, since the reflected signals from lip motion usually include other dynamic interference, e.g., the signals from body motion and head motion, we propose a difference-based method to suppress the dynamic interference from body motion and head motion. To extract the features from vocal-cords vibration, we propose a feature extraction scheme for micro-vibration based on frequency-time spectrogram to represent the vocalcords vibration. Moreover, to efficiently detect the speaking activity of the human subject, we propose a speech detection method based on cross-validation of lip motion and vocalcords vibration so as to avoid wasting computing resources on nonspeaking activities. To effectively fuse the features from the two modalities for speech recognition, we propose TransFuser, a multi-modal fusion transformer that leverages the attention mechanism to fuse the spectrograms of lip motion and vocal-cords vibration. In this way, the mutually complementary features can be leveraged to further improve speech recognition performance.\n\nThere are three challenges to be addressed in this paper. The first challenge is to extract the signal features from lip motion with the dynamic interference from the body motion and head motion. Due to the dynamic interference from body and head motion, the reflected signals from lip motion are severely suppressed by the interference signals, since the amplitudes of body motion and head motion are usually much greater than the lip motion. To address this challenge, we propose a difference-based method to remove the dynamic interference. Specifically, according to the range bins of mmWave, we extract the signals from the adjacent bins next to the lip motion-related bin, and then average the signals in the adjacent bins to estimate the dynamic interference to the lip motion. After that, we obtain the lip motion-related spectrogram by canceling the averaged signals in the original spectrogram.\n\nThe second challenge is to effectively distinguish the speaking activities from nonspeaking activities so as to avoid unnecessary wastes of computing resources on the nonspeaking activities. Considering that there always exist nonspeaking activities in real scenarios, e.g., the human subject is eating or chewing with obvious lip motion and minor vocal-cords vibration. If these signals of nonspeaking activities cannot be effectively distinguished, much computing resource can be wasted in feature extraction/fusion and speech recognition. To address this challenge, we propose a cross-validationbased speech detection method. We first use lip motionrelated signals for pre-detection to determine whether there exists a possibility of the speaking activity. Then, we further verify the vocal-cords vibration-related signals from the predetection results. Considering that when the human subject is actually speaking, the vowels usually lead to obvious vocal-cords vibration, and the vowels and consonants usually appear alternately, thus we can distinguish the speaking and nonspeaking activities by further verifying the amplitudes of vocal-cords vibration, especially from pronouncing the vowels.\n\nThe third challenge is to fuse multi-modal features to improve speech recognition performance. While we use mmWave as a single channel to perceive the lip motion and vocalcords vibration signals simultaneously, there exist inherent correlation and complementarity in the features of the two modalities. For example, both features are temporally and spatially synchronized, and lip motion features can effectively compensate for the vocal-cords vibration features when the latter is not so obvious. To effectively fuse the corresponding features to improve speech recognition performance, it is challenging to explore the inherent correlation and complementarity between the two modalities. To address this challenge, we propose Transfuser, a multi-modal fusion framework that uses the attention mechanism, i.e., cross-attention and mergedattention, to fuse the Doppler-velocity spectrogram and vocalcords vibration spectrogram. In this way, the inherent correlation and complementarity can be effectively quantified by leveraging the attention mechanism.\n\nThis paper makes the following contributions. First, we propose a novel mmWave-based solution to perform speech recognition in the situation of multiple sound sources and ambient noises, by precisely extracting the multi-modal features from lip motion and vocal-cords vibration from the single channel of mmWave. Second, we propose a multi-modal fusion framework for speech recognition by fusing the signal features from lip motion and vocal-cords vibration with the attention mechanism. Third, we implemented a prototype system and evaluated the performance in real test-beds, including multiple sound sources and ambient noises. Experiment results show that the average speech recognition accuracy is 92.8% in realistic environments.\n\nII. RELATED WORK Speech recognition is widely used in real-world scenarios. Speech recognition based on one single sensor is called singlechannel speech recognition, while speech recognition based on multiple sensors is called multi-model speech recognition.\n\nSingle-channel Speech Recognition. Microphone-based automatic speech recognition(ASR) system is widely used in our lives [24] [25]. The microphone is the most commonly used but not suitable for all scenarios, for example, throughwall perception. To solve the problem, some researchers propose other channels for ASR. The wireless perception technology obtains the characteristics of the signals channel by analyzing the changes of the wireless signals during the propagation process to realize the scene's perception. Wang et al. [7] propose a Wi-Fi signal-aware-based lip reading by detecting and analyzing fine-grained radio reflections from mouth motion. Li et al. [15] proposed a flexible mmWave interrogation system that directly captures and analyzes sound vibrations for user authentication. Wang et al. [11] proposed Tag-Bug, which focuses on human voices with complex frequency bands and eavesdrops on speakers through walls by capturing sub-millimeter vibrations. However, the singlesensor-based approach usually fails to achieve good performance in speech recognition since they usually cannot recover voice-related signals from the single channel well.\n\nMulti-Channel Speech Recognition. Multi-channel wireless sensing technology is widely used because of its comple-mentarity between channels. Therefore, vision and wireless signal perceptions are introduced to improve speech recognition performance [26] [21]. Ding et al. [26] proposed a selfsupervised audio-video synchronous learning method to solve the speaker classification problem without extensive labeling work. With the widespread of deep learning in speech signals processing, Yu et al. [21] proposed an attention mechanism to realize multi-modal fusion speech recognition. However, the audio-visual method requires an additional camera in addition to the microphone. In addition, cameras are inconvenient in many typical scenarios due to privacy concerns. Therefore, UltraSE [22] used ultrasound Doppler shifts caused by facial motion to enhance noisy speech. Nevertheless, this algorithm limits the distance from the mouth to the microphone to 20cm. Therefore, speech recognition work based on radio frequency sensing, which has a broader sensing area, has also attracted much attention. Liu et al. [27]integrated the speech perception signals of mmWave and microphone modalities and proposed an anti-noise multi-modal speech recognition system. Therefore, multi-modal fusion methods can effectively improve speech recognition performance by utilizing the complementarity between modalities. Nevertheless, the multiple sensor-based approaches incur additional hardware costs and require essential synchronization in both time and space.\n\nWe are considering the significant shortcomings of singlechannel speech recognition and the complementarity of multimodal features. Therefore, we propose a single-channel multimodal speech recognition algorithm. Specifically, we utilize a single-channel mmWave radar that can reduce costs and effectively avoid synchronization problems between multiple sensors. In addition, the complementarity of lip motion and vocal-cords vibration can effectively improve speech recognition performance.\n\nIII. EMPIRICAL STUDY This section briefly introduces the feasibility of simultaneously sensing human subjects' micro-vibration and macromotion using FMCW mmWave radar. A. Principle of Vibration Perception mmWave radar senses the motion of a target by transmitting the FMCW signal and receiving the reflected signal. Assuming that the transmitted and received signals are defined as:\nS tx (t) = exp(j2\u21e1f c t + j\u21e1Bt 2 /T ),(1)S rx (t) = A exp[j2\u21e1f c (t \u2327 ) + j\u21e1B(t \u2327 ) 2 /T ].\n(2) Here, f c represents the start frequency, B represents the bandwidth of the chirp, T represents the period of the chirp, and \u2327 represents the delay of the received signal. Assuming that the distance between the vibrating target and the mmWave radar is d. We use a mixer and filter to eliminate the carrier wave in the received signal S rx (t) and obtain the intermediatefrequency (IF) signal S IF (t) as:\nS IF (t) = A exp[j2\u21e1( B2d T c t + 2\u21e1 d)].(3)\nHere, A is the path loss, c is the speed of light, and is the wavelength. Suppose the displacement d of the moving Since this change is below the frequency resolution T , it will not be captured effectively in the spectrum. Moreover, the phase of IF signal changes is given by:\n= 4\u21e1 d.(4)\nIf the displacement of the moving target is 1mm (for 60GHz radar =5mm, B=4GHz), the phase of the IF signal changes is 0.8\u21e1, i.e., 144 , this change is much larger than the phase angle resolution. Therefore, according to the range resolution d res = c 2B of mmWave radar, it is difficult to distinguish the position of lips and vocal-cords even with a minimum range resolution. Fortunately, the phase of the IF signal is sensitive to small changes. Therefore, we can simultaneously sense lip motion and vocal-cords vibration using the phase change within the same range bin.\n\n\nB. Sensing Lip Motion\n\nLip language is composed of a series of lip motions. Therefore, we can obtain lip language information by sensing lip motion signals. Here, we explore whether we can use mmWave radar to perceive lip motion and obtain lip language information. Notably, we only moved the lips without real pronunciation to avoid the disturbance of the vocal-cords vibration. To confirm the location of the lip motion, we use Range-FFT and Doppler-FFT [28] on the received signal to obtain the range bin where the lip motion is located, as shown in Fig.2(a). Although the frequency of lip motion is minor, its motion amplitude is large. Moreover, the velocity changes obviously, and the lip motion information can be perceived with the phase velocity feature, as shown in Fig.2(b). According to Eq.(4), the angular frequency w of the phaseamplitude changing with time is given by:\nw = t = 4\u21e1v r = 2\u21e1f d .(5)\nHere, v r is the target velocity, and f d is the Doppler shift. Therefore, we can obtain the lip motion information by calculating the Doppler frequency of the reflected signals. Specifically, the frequency of lip motion is usually below 20Hz. We consider performing down-sampling (sampling rate is 76.9Hz) of the original phase signals while retaining more phase information. According to Eq.(5), the velocity information of the lip motion can be obtained. Fig.3 shows the velocity-time spectrograms of lip motion for different phonetic symbols which are perceived by mmWave radar. Note that there are significant differences in the velocity-time spectrograms among different lip motions, no matter whether it is a vowel phoneme or consonant phoneme. Thus, mmWave radar can obtain lip language information by sensing lip motion.\n\n\nC. Sensing Vocal-cords Vibration\n\nThe human voice mainly depends on the vibration of the vocal-cords. Therefore, we designed feasibility experiments to explore the correlation between vocal-cords vibration and speech signals. Specifically, we placed a mmWave radar in front of the person to sense vocal-cords vibration signals and a microphone to collect speech signals. The mmWave radar measures distance with FMCW chirp signals.\n\nWhen the human subjects speak, they are located in the radial direction of the mmWave radar. Therefore, according to Eq.(4), we can obtain the vocal-cord displacement by calculating the phase change of the mmWave radar signals. Since lip motion is low-frequency and has a large amplitude, we can easily remove the interference of lip motion by using a high-pass filter. We can observe that the phase change of mmWave radar can depict the vibration of the vocal-cords caused by a human voice.\n\nThe distance between the subject and the mmWave radar is estimated by calculating the frequency difference between the transmitted signal and the received signal. We use Range-FFT and Doppler-FFT on the reflected signals of mmWave radar to estimate the location of vocal-cords vibration. Fig.4(a) and Fig.4(b) respectively show the speech signals collected by the microphone and the vocal-cords vibration signals collected by the millimeter wave radar. Comparing Fig.4(a) with Fig.4(b), the mmWave signal perceived vocal-cords vibration signal has a significant correlation with the speech signal collected by the microphone. For example, two voiceprints can be seen at 200Hz and 300Hz in the time-frequency spectrogram.\n\nThus, mmWave radar can obtain speech by sensing the vocal-cords vibration and lip language by sensing the lip motion. We can draw the following conclusions: Lip motion has the characteristics of low frequency, large amplitude, and high velocity. We can use the doppler velocity spectrogram to extract lip motion features. The vocal-cords vibration has the characteristics of high frequency and small vibration amplitude. We can extract the vocal-cords vibration signal frequency-time spectrogram by sensing the phase change.\n\n\nIV. SYSTEM DESIGN\n\nWe design mmMIC, a mmWave-based multi-modal fusion lip-vibration speech recognition approach, which simultaneously senses the speaker's lip macro-motion signal and vocalcords micro-vibration signal using mmWave radar. As shown in Fig.5, we propose the following three modules:\n\nExtracting Macro-motion Features. To extract the Macromotion spectrogram of lip motion, we utilize mmWave radar to transmit beamforming chirp signals, then extract the doppler velocity spectrogram from the reflected signal. Consider that body and head motions are inevitable when humans speak. We propose a difference-based motion disturbance removal algorithm to reduce the impact of body and head motion. Extracting Micro-vibration Features. We obtain the vocal-cords vibration signal from the phase change of the reflected signal. However, many resources will be wasted on processing meaningless noise without proper detection mechanisms. Therefore, we propose a speech detection method based on lip motion and vocal-cords vibration cross-verification to solve this problem.\n\nAttention-based Multi-modal Fusion and Recognition. Vowels produce significant vocal-cords vibration when humans speak, and all phonetic symbols produce lip motion. We propose TransFuser, a novel multi-modal fusion transformer that leverages attention to integrate lip motion spectrogram and vibration spectrogram representations.To realize speech recognition and verify the effectiveness of feature fusion, we connect a recognition module to the fusion network output.\n\n\nA. Extracting Macro-motion Features\n\nSignal Preprocessing. To extract lip motion features, we first utilize mmWave radar to transmit FMCW signals and employ beamforming to focus the signal energy on the lip region, as shown in Fig.2. Then, to estimate the lip motion, we perform Range-FFT operation at the fast-time windows and Doppler-FFT operation at the slow-time windows on the received signal. For example, as shown in Fig.2(a), we can see that the Doppler velocity changes significantly at the position where the range-FFT bin is 8. Therefore, the range bin corresponding to the target is found by finding the max value within the user-specified range limit in the range profile. After that, we can obtain the phase signal containing the lip motion from the range bin. As shown in Fig.6, we use the vector of the signals to provide an intuitive representation of the signal superimposition:\nS = S v + S l + S d + S b .(6)\nHere, S represents all reflected signals at the receiver, S b represents the background static interference signals, S d represents the dynamic interference signals generated by the speaker's body motion and head motion, S l represents the lip motion signals, S v represents the vocal-cords vibration signals we expect to obtain. Since S b is a static background component, we directly cancel it according to the reflected signal variability. Due to the low frequency and significant  6. A multi-model speech recognization in mmMIC amplitude variation of lip motion, we apply a low-pass filter and resample the phase signal in the range bin to remove vocalcord interference. So the receiver signals can be expressed as:\nS l+d = S l + S d .(7)\nHere, S l+d represents all reflected signals, S d represents the dynamic interference signals generated by the speaker's body and head motion, S l represents the lip motion signals.\n\nDynamic Interference Removal. At present, we have obtained the lip motion signal S l+d , i.e., Eq(7) without vocalcords vibration. Considering that during the process of lip motion, the human's body and head motion, which is called dynamic interference, also exists simultaneously. Therefore, we need to consider the effects of dynamic interference when extracting lip motion features. Even if we use beamforming to reduce the beam width of mmWave radar, it cannot altogether avoid dynamic interference. As shown in Fig.2(a), we find that the dynamic interference in the Range-doppler spectrogram is contained in the range bin where the lip motion is located and has apparent changes in its adjacent bins.\n\nTo solve this problem, we propose a difference-based method to remove the dynamic interference. Specifically, we can estimate dynamic interference signals S d from the adjacent range bins where the lip motion is located, expressed as:\nS d = 1 2N N X n= N\n\u21b5 n \u00b7 S l+d (R + n) n, N 2 Z, n 6 = 0. (8) Here,\u015c d is the estimated signal of dynamic interference, Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\n\nS l+d (R) is the lip motion signal, including dynamic interference at range bin R, 2N is the number of adjacent bins. \u21b5 n is the weight of the signal in the n-th bin and P N n= N \u21b5 n = 1. After that, we use Short-Time Fourier Transform (STFT) [29] to calculate the Doppler spectrogram of the lip motion-related signals and the dynamic interference signals, respectively:\nST F T (S l ) = ST F T (S l+d ) ST F T (\u015c d ).\n(9) Finally, we obtain the Doppler velocity spectrogram of lip motion-related without dynamic interference.\n\n\nB. Extracting Micro-vibration Features\n\nSignal Preprocessing. Based on the empirical study, we can obtain the phase change signal caused by the vibration of the vocal-cords. According to Eq.(4), the displacement d of the measured object is related to the phase of the reflected signal. Since phase is directly related to vocal-cords vibration, we combine the phase variation over time within each frame into a waveform. In addition, the phase of the reflected signal also changes when the sensed human subject has other motions, i.e., body and head motions, as shown in Fig.6. Fortunately, since the lip motion component S l , the body motion component S d , and the background component S b are low-frequency signals. Therefore, we can use a high-pass filter to suppress these low-frequency components effectively. According to Eq.(6), the vocal-cords vibration signal can be expressed as S = S v . We have obtained the vocal-cords vibration without lip motion signal and dynamic signal.\n\nSpeech Activity Detection based on Cross-validation. Considering that there always exist nonspeaking activities in real scenarios, e.g., the human subject is eating or chewing with obvious lip motion and minor vocal-cords vibration. If these signals of nonspeaking activities cannot be effectively distinguished, much computing resource can be wasted in feature extraction/fusion and speech recognition. Fortunately, mmWave radar record both vocal-cords vibration signals and lip motion signals. During the process of speech activity, lip motion and vocal-cords vibration usually coincide. Therefore, we can exploit the multi-modal correlation to distinguish the speech and non-speech activity. To solve this problem, we propose a method based on cross-validation of lip motion and vocal-cords vibration using support vector machines (SVMs), as shown in Fig.7. This method mainly contains three steps:\n\nLip Motion Pre-detection. Considering that human speech activities must contain lip motion, we use a threshold-based energy detection algorithm to estimate the energy intensity of lip motion signals within the window. Specifically, we split the lip motion signals into segments of \u2327 window length and then calculate the energy intensity E l (t, t + \u2327 ) within the window. If E l (t, t + \u2327 ) is greater than the threshold of lip motion, we obtain the energy E l (t, t + \u2327 ) and further verify it with vocal-cords vibration features.\n\nVerification of Vocal-cords Vibration. We use the sliding window dt to perform sliding detection with an overlap is dt/2 on the time window of \u2327 in the signal of vocal-cords vibration. Note that not all phonetic symbols vibrate the vocal cords, for example, unvoiced consonants. Fortunately, since  Fig. 7. Speech activity detection based on cross-validation consonants rarely occur alone, we can perceive speech activity based on the consonant and vowel alternation principle in human speech. We compute the signal energy E v (t + \u2327 ) = P E i v (t + dt) if the energy of each window dt is greater than the threshold in the vocal-cords vibration signal.\n= \u2212 ( ) l E t ( ) v E t ( ) v E t ( ) l E t ( ) 0 W E b t \uf0d7 + =\nDecision based on SVM. We can obtain the energy E l (\u2327 ) of lip motion spectrogram and the energy E v (\u2327 ) of vocalcords vibration signal in the \u2327 window. To comprehensively consider the energy of lip motion and vocal-cords vibration, we can obtain the final energy E( \u2327 ) using Concat operation to concatenate the lip motion's energy E l ( \u2327 ) and the vocalcords vibration's energy E v ( \u2327 ), express as:\nE( \u2327 ) = Concat(E l ( \u2327 ), E v ( \u2327 )).\n(10) Finally, we utilize SVM to discriminatively classify the energy E( \u2327 ) of speech detection. Specifically, the positive samples are signals with a speech activity window t, and the negative samples are signals without a speech activity signal.\n\nCGAN-based Enhancement. The vocal-cord only generates the fundamental frequency of speech signals, and the highfrequency harmonics are mainly generated by the resonance of the oral and nasal cavities [30]. Therefore, the high-frequency harmonic components are lacking in the speech signal collected by mmWave radar, as shown in Fig.4. Fig.4(a) compares the frequency band between the original sound from the microphone and the extracted sound from mmWave radar in the spectrogram. The low-frequency band is the fundamental frequency for the human voice, while the high-frequency band is the harmonic frequency. Therefore, we need to recover the harmonic band from the fundamental frequency.\n\nTo solve the problem, we use the cycle-consistent adversarial networks(Cycle-GAN) [31] to generate all frequency coefficients from the fundamental frequency signal. We use the STFT to convert the vocal-cords vibration signal and the speech signal to the spectrogram, respectively. For the generator network, we use nine blocks for 128 \u21e5 128 images in the generator, and each residual block consists of two convolutional layers with kernel size 3 \u21e5 3. The discriminator network contains two convolutional blocks, six residual blocks, and a fully connected network to predict true or false.\n\nIn order to make the generated spectrograms consistent with the natural speech spectrogram features, we utilize a similarity loss, i.e., L1 loss, to constrain the correspondence between the vibration and speech spectrum. Fig.9 shows that\n\n\nCycle-consistency Loss\n\n\nReal\n\n\nFake or\n\n\nSimilarity Loss\n\nGenerator Discriminator\n\n\nConv2d\n\nRes-block Deconv2d FC Inverse Generator Real Spectrogram Fake Spectrogram Fig. 8. CGAN framework.\n\n\nOriginal\n\nAfter CGAN Ground Truth Fig. 9. Spectrogram comparison the spectrogram of the original vocal-cord signal collected by mmWave radar only contains the fundamental frequency signal, while the vocal-cords vibration signal we generated contains rich high-frequency harmonics. Note that they are consistent by comparing the spectrogram after CGAN and the ground truth. Finally, we use inverse STFT [32] to transform the spectrogram to achieve a better human voice.\n\n\nC. Attention-based Multi-modal Fusion and Recognition\n\nLip motion and vocal-cords vibration represent human speech signals from macro and micro perspectives, respectively. Since mmWave radar can sense lip motion and vocalcords vibration simultaneously, we fuse these two modalities to improve speech recognition accuracy. We propose TransFuser , which improves the complementarity of multimodal features by fusing vocal-cord micro-vibration signals and lip macro-motion features. As shown in Fig.11, to fuse the vocal-cords vibration feature and lip motion feature, our key idea is to leverage the self-attention mechanism [33] of transformers.\n\nVocal-cords Vibration Encoder. The feature extraction of the speech signal is the first step of automatic speech recognition(ASR) [34], so the speech encoder based on the attention mechanism is widely used. Fig.10(a) is the feature encoding module of vocal-cords vibration. We use learned embeddings to transform input segments x = (x 1 , ..., x N ) into continuous representation output segments y = (y 1 , ..., y N ), with x i , y i 2 R d . As shown in Fig.10(a), for the embedded segmentation vector, we compute the attention function\nAttention(Q, K, V ) = softmax( QK T p d k )V.(11)\nHere, Q represents a set of query matrices, the keys and values are packed together into matrices K and matrices V , will be concatenated into a vector and passed through a feedforward network to a predefined output dimension. Lip Motion Encoder. Unlike the speech encoder, the lip encoder inputs a lip motion spectrogram. Therefore, we focus on patch features and study the use of vision transformers (ViTs) [35] for vision encoder. In addition, Fig.10(b) is the feature encoding module of lip motion. Considering that the lip motion spectrograms that we collected contain important temporal information. As a result, we segment the spectrograms L 2 R H\u21e5W \u21e5C into a sequence of flattened 2D patches L p 2 R M \u21e5(H\u21e5P \u21e5C) according to the time dimension, where (H, W ) is the resolution of an original spectrogram, (H, P ) is the resolution of each patch, C is the number of channels, and M = W/P is the resulting number of patches. Finally, to preserve the temporal logic in the lip motion spectrogram, we add standard learnable 1D position embeddings.\n\nMulti-modal Fusion. Although lip motion and vocal-cords vibration represent speech signals from different perspectives, their speech information has a significant correlation and complementarity. For example, vocal-cords vibration is weak for consonants, lip motion is significant, while vocal-cords vibration and lip movement are significant for vowels. Since the human voice is completed by vocal-cords vibration and lip motion, they describe speech information from two aspects and have a significant correlation. Therefore, we can exploit the correlation and complementarity between the two modalities to improve speech recognition performance. To reasonably and effectively fuse lip motion and vocal-cords vibration features, we propose an attention-based multi-modal fusion approach, TransFuser, to improve the recognition performance. As shown in Fig.11, TransFuser is mainly connected by cross-attention [36] and merged-attention [37] alternately. Specifically, the essence of cross-attention, i.e.,A S =  \n= att((K F , V F ), Q S )+ att((K F , V F ), Q L )\n, is to achieve features complementarity between lip motion and vocal-cords vibration by merging the attention information. Speech Recognition. To realize speech recognition and verify the effectiveness of feature fusion, we connect a recognition module to the fusion network output. Inspired by the existing ASR methods [5], we design a decoder to recognize the fused features. Specifically, similar to the encoder structure in Fig.9, the decoder is mainly connected by a multi-head attention sublayer and a feed-forward sublayer alternately. In addition, a normalization layer is added before each block, and a residual connection is applied after each block.\n\n\nV. PERFORMANCE EVALUATION\n\n\nA. Experiment Setup\n\nHardware. The platform to implement our algorithm is based on TI's IWR6843BOOST radar, high-speed data collection board DCA1000EVM, and laptop, as shown in Fig.12. The IWR6843BOOST is a mmWave radar that continuously transmits FMCW with a 60GHz carrier to measure distance and angle. Specifically, the chirp transmits period of the radar configuration is 50\u00b5s. The received channel has a 4000k ADC sampling rate, and each received chirp contains 64 samples.\n\nSoftware. We use a laptop to run a python script to connect and control the radar. We write python scripts to control the microphone and mmWave radar to capture both mmWave radar signals and audio signals. The vocal-cords vibration encoder, the lip encoder, and the decoder for recognition consist of N = 6 identical layers, the hidden layer size is 384, and the number of attention heads is 6. We apply dropout to the output of each sub-layer with a dropout rate of 0.2. We use the Adam optimizer with = 0.9, l rate = 1e 4. Weight decay is 100, and batch size is set to 32. Dataset. We choose 48 phonetic symbols as recognition objects. We invited eight male and seven female volunteers to speak all the phonetic symbols. We collected 21,600 sets of data, including speech signals, mmWave vocal-cords vibration signals, and lip motion signals. We choose 1/5 of them as the test, which covers all the phonetic symbols of each volunteer.\n\nMetrics. To comprehensively evaluate the overall performance of our algorithm, we use two metrics, accuracy and recall. The accuracy rate represents the proportion of correctly identified samples in all samples. We use the recall rate to evaluate the speech recognition performance to reduce the missed recognition rate. The recall rate is the proportion of correctly identified samples to the total samples for each class.\n\n\nB. Performance\n\nOverall Performance. Our solution achieves the best performance in speech recognition in the presence of speech interference. Fig.13(a) illustrates the accurate measurement of different phonetic symbols for five volunteers. Our algorithm achieves over 90% accuracy for all phonetic symbols. Especially for vowels, the accuracy rate reaches 95%. Even for consonants, we can exploit the complementarity of multi-modality to improve accuracy in speech recognition. In addition, as shown in Fig.13(b), our recall rate is also higher than 85% in speech recognition.\n\nImpact of Distance and Orientation. Our system can still perform speech recognition efficiently when the distance is within 2m, and the orientation is less than 30 degrees. In our experiments, we set the angle to change from 0 degrees to 60 degrees and the distance to change from 0.5m to 3m. Fig.13(c), Fig.13(d) and Fig.13(e) indicate that when the direction angle is 30 degrees and the distance is within 2m, and the speech recognition accuracy is more than 90%.\n\nImpact of Ambient Noises. Our system can perform speech recognition efficiently in music noise. We evaluated the speech recognition performance of five volunteers in different decibel music noise scenarios. Fig.13(f) shows that speech recognition accuracy under different decibels remains above 90% without apparent attenuation. Therefore, our system is hardly disturbed by ambient noise.\n\nImpact of Multi-human. Our system can perform speech recognition efficiently in multi-human speech noise. We separately evaluated the speech recognition performance of volunteers in the presence of different amounts of human speech noise. Fig.13(g) shows that as the number of humans producing speech noise increases, speech recognition performance does not significantly impact.   Impact of Body Motion. Our system can effectively perform speech recognition when the human subject's speaking body motion displacement is less than 10cm. We individually assessed five volunteers with a range of body motion within 40 cm. Fig.13(h) shows that the error rate increases as the vibration amplitude of the body motion increases.\n\n\nC. Ablation Study\n\nIn this section, we conduct an ablation study to quantify the fusion of the two modal signals and our proposed fusion method. In contrast, we comprehensively validate our method by ablating specific components:\n\nLip Motion, where no vocal-cord micro-vibration signal is fused in our fusion recognition network, we replace the TransFuser sub-network with a transformer of the same depth to extract features. As shown in Table I, we find that whether it is vowels or consonants, the recognition accuracy of lip movements is gathering more than 73.35%.\n\nVocal-cords Vibration, where no lip macro-motion signal is fused in our fusion recognition network, we replace the TransFuser sub-network with a transformer of the same depth to extract features. As shown in Table I, we find that the recognition accuracy of vowels with noticeable vocal cord vibration was 90.8%, but the recognition rate of consonants with slight vocal-cord vibration was 75.3%.\n\nFusion (Lip Motion and Vocal-cords Vibration), which uses TransFuser sub-network to fuse lip macro-motion and vocal-cord micro-vibration to perform speech recognition on the fused features. As shown in table I, we find that after fuse lip motion and vocal-cords vibration features, our recognition accuracy for both vowels and consonants is higher than 90%.\n\nIn Table I, we find that even though lip motion and vocal-cords vibration have lower recognition performance in vowels and consonants, after TransFuser sub-network fusion, its recognition performance is 20.97% higher than lip motion and 3.9% higher than vocal-cords vibration.\n\nVI. CONCLUSION In this paper, we propose a novel mmWave-based solution to perform speech recognition in the situation of multiple sound sources and ambient noises by precisely extracting the multi-modal features from lip motion and vocal-cords vibration from the single channel of mmWave. To extract the signal features from lip motion with the dynamic interference from the body and head motions, we propose a difference-based method to remove the dynamic interference. We propose a cross-validation-based speech detection method to distinguish speaking activities from nonspeaking activities so as to avoid wasting computing resources on nonspeaking activities. We implemented a prototype system and evaluated the performance in real test-beds. Experiment results show that the average speech recognition accuracy is 92.8% in realistic environments.\n\nFig. 1 .\n1Speech recognition based on single-channel multi-modal fusion\n\n\n1mm (for 60GHz radar =5mm, B=4GHz), and the frequency of the IF signal changes in the observation window. This corresponds to only an additional 0.026T , i.e., fT = B2 d c .\n\n\nvocal-cords vibration captured by mmWave Fig. 4. Comparison of spectrogram of vocal-cords vibration signal and microphone speech signal\n\n\nFig. 5. System overview\n\nFig. 10 .\n10Feature encoding module\n\nFig. 11 .\n11Attention-based multi-modal fusion and recognition.att((K L , V L ), Q S ) and A L = att((K S , V S ), Q L ), is to achieve features correlation between lip motion and vocalcords vibration by exchanging the attention information. The essence of merged-attention, i.e., A F\n\nFig. 12 .\n12The evaluation setup.\n\nTABLE I PERFORMANCE\nICOMPARISON OF DIFFERENT FEATURES.Dataset \nMethods \nLip \nVibration Fusion \nMonophthongs \n73.2 \n91.3 \n95.89 \nDiphthongs \n80.64 \n93.2 \n97.61 \nAll Vowels \n75.21 \n90.8 \n95.5 \nUnvoiced Consonants \n75.51 \n73.1 \n90.56 \nVoiced Consonants \n77.1 \n76.2 \n93.31 \nAll Consonants \n73.35 \n75.3 \n91.3 \nAll Phonetics \n71.83 \n88.9 \n92.8 \n\n\nACKNOWLEDGMENTS\nMulti-modal fusion transformer for end-to-end autonomous driving. A Prakash, K Chitta, A Geiger, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). A. Prakash, K. Chitta, and A. Geiger, \"Multi-modal fusion transformer for end-to-end autonomous driving,\" in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 7073- 7083.\n\nMillimeter wave integrated sensing and communication with hybrid architecture in vehicle to vehicle network. S Wang, R Chen, L Zhao, C Liu, 2021 IEEE 94th Vehicular Technology Conference (VTC2021-Fall). S. Wang, R. Chen, L. Zhao, and C. Liu, \"Millimeter wave integrated sensing and communication with hybrid architecture in vehicle to vehicle network,\" in 2021 IEEE 94th Vehicular Technology Conference (VTC2021-Fall), 2021, pp. 01-06.\n\nComparing social robot, screen and voice interfaces for smart-home control. M Luria, G Hoffman, O Zuckerman, Proceedings of the 2017 CHI conference on human factors in computing systems. the 2017 CHI conference on human factors in computing systemsM. Luria, G. Hoffman, and O. Zuckerman, \"Comparing social robot, screen and voice interfaces for smart-home control,\" in Proceedings of the 2017 CHI conference on human factors in computing systems, 2017, pp. 580-628.\n\nSmart healthcare: making medical care more intelligent. S Tian, W Yang, J M Le Grange, P Wang, W Huang, Z Ye, Global Health Journal. 33S. Tian, W. Yang, J. M. Le Grange, P. Wang, W. Huang, and Z. Ye, \"Smart healthcare: making medical care more intelligent,\" Global Health Journal, vol. 3, no. 3, pp. 62-65, 2019.\n\nReal-time, universal, and robust adversarial attacks against speaker recognition systems. Y Xie, C Shi, Z Li, J Liu, Y Chen, B Yuan, ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing. Y. Xie, C. Shi, Z. Li, J. Liu, Y. Chen, and B. Yuan, \"Real-time, universal, and robust adversarial attacks against speaker recognition systems,\" in ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 1738-1742.\n\nEnd-to-end multi-speaker speech recognition with transformer. X Chang, W Zhang, Y Qian, J L Roux, S Watanabe, ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing. X. Chang, W. Zhang, Y. Qian, J. L. Roux, and S. Watanabe, \"End-to-end multi-speaker speech recognition with transformer,\" in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6134-6138.\n\nWe can hear you with wi-fi. G Wang, Y Zou, Z Zhou, K Wu, L M , -S Ni, Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM. the Annual International Conference on Mobile Computing and Networking, MOBICOM593G. Wang, Y. Zou, Z. Zhou, K. Wu, and L. M.-S. Ni, \"We can hear you with wi-fi!\" in Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM, 2014, p. 593.\n\nWira: Enabling crosstechnology communication from wifi to lora with ieee 802.11ax. D Xia, X Zheng, F Yu, L Liu, H Ma, IEEE INFOCOM 2022 -IEEE Conference on Computer Communications. D. Xia, X. Zheng, F. Yu, L. Liu, and H. Ma, \"Wira: Enabling cross- technology communication from wifi to lora with ieee 802.11ax,\" in IEEE INFOCOM 2022 -IEEE Conference on Computer Communica- tions, 2022, pp. 430-439.\n\nPrecise power delay profiling with commodity wi-fi. Y Xie, Z Li, M Li, IEEE Transactions on Mobile Computing. 186Y. Xie, Z. Li, and M. Li, \"Precise power delay profiling with commodity wi-fi,\" IEEE Transactions on Mobile Computing, vol. 18, no. 6, pp. 1342-1355, 2019.\n\nm 3 m3: Multipath assisted wi-fi localization with a single access point. Z Chen, G Zhu, S Wang, Y Xu, J Xiong, J Zhao, J Luo, X Wang, IEEE Transactions on Mobile Computing. 202Z. Chen, G. Zhu, S. Wang, Y. Xu, J. Xiong, J. Zhao, J. Luo, and X. Wang, \"m 3 m3: Multipath assisted wi-fi localization with a single access point,\" IEEE Transactions on Mobile Computing, vol. 20, no. 2, pp. 588-602, 2021.\n\nThru-the-wall eavesdropping on loudspeakers via rfid by capturing submm level vibration. C Wang, L Xie, Y Lin, W Wang, Y Chen, Y Bu, K Zhang, S Lu, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies5C. Wang, L. Xie, Y. Lin, W. Wang, Y. Chen, Y. Bu, K. Zhang, and S. Lu, \"Thru-the-wall eavesdropping on loudspeakers via rfid by capturing sub- mm level vibration,\" Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 5, no. 4, pp. 1-25, 2021.\n\nRed: Rfid-based eccentricity detection for high-speed rotating machinery. Y Zheng, Y He, M Jin, X Zheng, Y Liu, IEEE INFOCOM 2018 -IEEE Conference on Computer Communications. Y. Zheng, Y. He, M. Jin, X. Zheng, and Y. Liu, \"Red: Rfid-based eccen- tricity detection for high-speed rotating machinery,\" in IEEE INFOCOM 2018 -IEEE Conference on Computer Communications, 2018, pp. 1565- 1573.\n\nRf-wise: Pushing the limit of rfid-based sensing. C Zhao, Z Li, H Ding, G Wang, W Xi, J Zhao, IEEE INFOCOM 2022 -IEEE Conference on Computer Communications. C. Zhao, Z. Li, H. Ding, G. Wang, W. Xi, and J. Zhao, \"Rf-wise: Pushing the limit of rfid-based sensing,\" in IEEE INFOCOM 2022 - IEEE Conference on Computer Communications, 2022, pp. 1779-1788.\n\nRfmehndi: A fingertip profiled rf identifier. C Zhao, Z Li, T Liu, H Ding, J Han, W Xi, R Gui, IEEE INFOCOM 2019 -IEEE Conference on Computer Communications. C. Zhao, Z. Li, T. Liu, H. Ding, J. Han, W. Xi, and R. Gui, \"Rf- mehndi: A fingertip profiled rf identifier,\" in IEEE INFOCOM 2019 - IEEE Conference on Computer Communications, 2019, pp. 1513-1521.\n\nVocalprint: exploring a resilient and secure voice authentication via mmwave biometric interrogation. H Li, C Xu, A S Rathore, Z Li, H Zhang, C Song, K Wang, L Su, F Lin, K Ren, Proceedings of the 18th Conference on Embedded Networked Sensor Systems. the 18th Conference on Embedded Networked Sensor SystemsH. Li, C. Xu, A. S. Rathore, Z. Li, H. Zhang, C. Song, K. Wang, L. Su, F. Lin, K. Ren et al., \"Vocalprint: exploring a resilient and secure voice authentication via mmwave biometric interrogation,\" in Proceedings of the 18th Conference on Embedded Networked Sensor Systems, 2020, pp. 312-325.\n\nVocalprint: A mmwave-based unmediated vocal sensing system for secure authentication. H Li, C Xu, A S Rathore, Z Li, H Zhang, C Song, K Wang, L Su, F Lin, K Ren, W Xu, IEEE Transactions on Mobile Computing. H. Li, C. Xu, A. S. Rathore, Z. Li, H. Zhang, C. Song, K. Wang, L. Su, F. Lin, K. Ren, and W. Xu, \"Vocalprint: A mmwave-based unmediated vocal sensing system for secure authentication,\" IEEE Transactions on Mobile Computing, pp. 1-1, 2021.\n\nmmphone: Acoustic eavesdropping on loudspeakers via mmwave-characterized piezoelectric effect. C Wang, F Lin, T Liu, Z Liu, Y Shen, Z Ba, L Lu, W Xu, K Ren, IEEE INFOCOM 2022 -IEEE Conference on Computer Communications. C. Wang, F. Lin, T. Liu, Z. Liu, Y. Shen, Z. Ba, L. Lu, W. Xu, and K. Ren, \"mmphone: Acoustic eavesdropping on loudspeakers via mmwave-characterized piezoelectric effect,\" in IEEE INFOCOM 2022 -IEEE Conference on Computer Communications, 2022, pp. 820-829.\n\nSpiralspy: Exploring a stealthy and practical covert channel to attack air-gapped computing devices via mmwave sensing. Z Li, B Chen, X Chen, H Li, C Xu, F Lin, C X Lu, K Ren, W Xu, The 29th Network and Distributed System Security (NDSS) Symposium 2022. The Internet Society. Z. Li, B. Chen, X. Chen, H. Li, C. Xu, F. Lin, C. X. Lu, K. Ren, and W. Xu, \"Spiralspy: Exploring a stealthy and practical covert channel to attack air-gapped computing devices via mmwave sensing,\" in The 29th Network and Distributed System Security (NDSS) Symposium 2022. The Internet Society, 2022.\n\nThe visual microphone: Passive recovery of sound from video. A Davis, M Rubinstein, N Wadhwa, G J Mysore, F Durand, W T Freeman, 10.1145/2601097.2601119ACM Transactions on Graphics. 334A. Davis, M. Rubinstein, N. Wadhwa, G. J. Mysore, F. Durand, and W. T. Freeman, \"The visual microphone: Passive recovery of sound from video,\" ACM Transactions on Graphics, vol. 33, no. 4, July 2014. [Online]. Available: https://doi.org/10.1145/2601097.2601119\n\nAudio-visual recognition of overlapped speech for the lrs2 dataset. J Yu, S.-X Zhang, J Wu, S Ghorbani, B Wu, S Kang, S Liu, X Liu, H Meng, D Yu, ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing. J. Yu, S.-X. Zhang, J. Wu, S. Ghorbani, B. Wu, S. Kang, S. Liu, X. Liu, H. Meng, and D. Yu, \"Audio-visual recognition of overlapped speech for the lrs2 dataset,\" in ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6984- 6988.\n\nFusing information streams in endto-end audio-visual speech recognition. W Yu, S Zeiler, D Kolossa, ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEW. Yu, S. Zeiler, and D. Kolossa, \"Fusing information streams in end- to-end audio-visual speech recognition,\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 3430-3434.\n\nUltrase: single-channel speech enhancement using ultrasound. K Sun, X Zhang, Proceedings of the 27th Annual International Conference on Mobile Computing and Networking. the 27th Annual International Conference on Mobile Computing and NetworkingK. Sun and X. Zhang, \"Ultrase: single-channel speech enhancement using ultrasound,\" in Proceedings of the 27th Annual International Conference on Mobile Computing and Networking, 2021, pp. 160-173.\n\nBig brother is listening: An evaluation framework on ultrasonic microphone jammers. Y Chen, M Gao, Y Li, L Zhang, L Lu, F Lin, J Han, K Ren, IEEE INFOCOM 2022 -IEEE Conference on Computer Communications. Y. Chen, M. Gao, Y. Li, L. Zhang, L. Lu, F. Lin, J. Han, and K. Ren, \"Big brother is listening: An evaluation framework on ultrasonic microphone jammers,\" in IEEE INFOCOM 2022 -IEEE Conference on Computer Communications, 2022, pp. 1119-1128.\n\nSemi-black-box attacks against speech recognition systems using adversarial samples. Y Wu, J Liu, Y Chen, J Cheng, 2019 IEEE International Symposium on Dynamic Spectrum Access Networks. DyS-PANY. Wu, J. Liu, Y. Chen, and J. Cheng, \"Semi-black-box attacks against speech recognition systems using adversarial samples,\" in 2019 IEEE International Symposium on Dynamic Spectrum Access Networks (DyS- PAN), 2019, pp. 1-5.\n\nRobust detection of machine-induced audio attacks in intelligent audio systems with microphone array. Z Li, C Shi, T Zhang, Y Xie, J Liu, B Yuan, Y Chen, 10.1145/3460120.3484755Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS '21. the 2021 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS '21New York, NY, USAAssociation for Computing MachineryZ. Li, C. Shi, T. Zhang, Y. Xie, J. Liu, B. Yuan, and Y. Chen, \"Robust detection of machine-induced audio attacks in intelligent audio systems with microphone array,\" in Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS '21. New York, NY, USA: Association for Computing Machinery, 2021, p. 1884-1899. [Online]. Available: https://doi.org/10.1145/3460120.3484755\n\nSelf-supervised learning for audio-visual speaker diarization. Y Ding, Y Xu, S.-X Zhang, Y Cong, L Wang, ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Y. Ding, Y. Xu, S.-X. Zhang, Y. Cong, and L. Wang, \"Self-supervised learning for audio-visual speaker diarization,\" in ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP), 2020, pp. 4367-4371.\n\nWavoice: A noise-resistant multi-modal speech recognition system fusing mmwave and audio signals. T Liu, M Gao, F Lin, C Wang, Z Ba, J Han, W Xu, K Ren, Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems. the 19th ACM Conference on Embedded Networked Sensor SystemsT. Liu, M. Gao, F. Lin, C. Wang, Z. Ba, J. Han, W. Xu, and K. Ren, \"Wavoice: A noise-resistant multi-modal speech recognition system fusing mmwave and audio signals,\" in Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems, 2021, pp. 97-110.\n\nmmvib: micrometerlevel vibration measurement with mmwave radar. C Jiang, J Guo, Y He, M Jin, S Li, Y Liu, Proceedings of the 26th Annual International Conference on Mobile Computing and Networking. the 26th Annual International Conference on Mobile Computing and NetworkingC. Jiang, J. Guo, Y. He, M. Jin, S. Li, and Y. Liu, \"mmvib: micrometer- level vibration measurement with mmwave radar,\" in Proceedings of the 26th Annual International Conference on Mobile Computing and Networking, 2020, pp. 1-13.\n\nFrequency domain processing. N Kehtarnavaz, Digital Signal Processing System Design. BurlingtonAcademic PressSecond Edition. second edition edN. Kehtarnavaz, \"Frequency domain processing,\" in Digital Signal Pro- cessing System Design (Second Edition), second edition ed. Burlington: Academic Press, 2008, pp. 175-196.\n\nThe human voice. R T Sataloff, Scientific American. 2676R. T. Sataloff, \"The human voice,\" Scientific American, vol. 267, no. 6, pp. 108-115, 1992.\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, 2017 IEEE International Conference on Computer Vision (ICCV. J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, \"Unpaired image-to-image translation using cycle-consistent adversarial networks,\" in 2017 IEEE International Conference on Computer Vision (ICCV), 2017, pp. 2242- 2251.\n\nSignal estimation from modified short-time fourier transform. D Griffin, J Lim, IEEE Transactions on acoustics, speech, and signal processing. 322D. Griffin and J. Lim, \"Signal estimation from modified short-time fourier transform,\" IEEE Transactions on acoustics, speech, and signal processing, vol. 32, no. 2, pp. 236-243, 1984.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Proceedings of the 31st International Conference on Neural Information Processing Systems, 2017, p. 6000-6010.\n\nThe speechtransformer for large-scale mandarin chinese speech recognition. Y Zhao, J Li, X Wang, Y Li, ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing. Y. zhao, J. Li, X. Wang, and Y. Li, \"The speechtransformer for large-scale mandarin chinese speech recognition,\" in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 7095-7099.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.11929arXiv preprintA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \"An image is worth 16x16 words: Transformers for image recognition at scale,\" arXiv preprint arXiv:2010.11929, 2020.\n\nAn empirical study of training end-to-end vision-and-language transformers. Z.-Y Dou, Y Xu, Z Gan, J Wang, S Wang, L Wang, C Zhu, P Zhang, L Yuan, N Peng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition18Z.-Y. Dou, Y. Xu, Z. Gan, J. Wang, S. Wang, L. Wang, C. Zhu, P. Zhang, L. Yuan, N. Peng et al., \"An empirical study of training end-to-end vision-and-language transformers,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 166-18 176.\n\nDecoupling the role of data, attention, and losses in multimodal transformers. L A Hendricks, J Mellor, R Schneider, J.-B Alayrac, A Nematzadeh, Transactions of the Association for Computational Linguistics. 9L. A. Hendricks, J. Mellor, R. Schneider, J.-B. Alayrac, and A. Ne- matzadeh, \"Decoupling the role of data, attention, and losses in multi- modal transformers,\" Transactions of the Association for Computational Linguistics, vol. 9, pp. 570-585, 2021.\n", "annotations": {"author": "[{\"end\":171,\"start\":63},{\"end\":273,\"start\":172},{\"end\":361,\"start\":274},{\"end\":467,\"start\":362},{\"end\":556,\"start\":468},{\"end\":662,\"start\":557}]", "publisher": null, "author_last_name": "[{\"end\":71,\"start\":68},{\"end\":179,\"start\":176},{\"end\":283,\"start\":281},{\"end\":367,\"start\":365},{\"end\":478,\"start\":474},{\"end\":566,\"start\":564}]", "author_first_name": "[{\"end\":67,\"start\":63},{\"end\":175,\"start\":172},{\"end\":280,\"start\":274},{\"end\":364,\"start\":362},{\"end\":473,\"start\":468},{\"end\":563,\"start\":557}]", "author_affiliation": "[{\"end\":170,\"start\":95},{\"end\":272,\"start\":197},{\"end\":360,\"start\":285},{\"end\":466,\"start\":391},{\"end\":555,\"start\":480},{\"end\":661,\"start\":586}]", "title": "[{\"end\":60,\"start\":1},{\"end\":722,\"start\":663}]", "venue": null, "abstract": "[{\"end\":2125,\"start\":758}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2461,\"start\":2458},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2465,\"start\":2462},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2481,\"start\":2478},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2509,\"start\":2506},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2974,\"start\":2971},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2978,\"start\":2975},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4028,\"start\":4025},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4034,\"start\":4030},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4045,\"start\":4041},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4051,\"start\":4047},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4068,\"start\":4064},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4074,\"start\":4070},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4251,\"start\":4247},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5063,\"start\":5059},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5068,\"start\":5064},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5133,\"start\":5129},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5138,\"start\":5134},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11255,\"start\":11251},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11260,\"start\":11256},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11663,\"start\":11660},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11802,\"start\":11798},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11945,\"start\":11941},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12548,\"start\":12544},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12553,\"start\":12549},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12571,\"start\":12567},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12796,\"start\":12792},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13085,\"start\":13081},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13410,\"start\":13406},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16591,\"start\":16587},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24874,\"start\":24870},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29197,\"start\":29193},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29771,\"start\":29767},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31114,\"start\":31110},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31806,\"start\":31802},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31959,\"start\":31955},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32826,\"start\":32822},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34382,\"start\":34378},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34408,\"start\":34404},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34856,\"start\":34853}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41756,\"start\":41684},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41932,\"start\":41757},{\"attributes\":{\"id\":\"fig_3\"},\"end\":42070,\"start\":41933},{\"attributes\":{\"id\":\"fig_4\"},\"end\":42096,\"start\":42071},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42133,\"start\":42097},{\"attributes\":{\"id\":\"fig_7\"},\"end\":42419,\"start\":42134},{\"attributes\":{\"id\":\"fig_8\"},\"end\":42454,\"start\":42420},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42796,\"start\":42455}]", "paragraph": "[{\"end\":3586,\"start\":2144},{\"end\":4551,\"start\":3588},{\"end\":6967,\"start\":4553},{\"end\":7873,\"start\":6969},{\"end\":9075,\"start\":7875},{\"end\":10131,\"start\":9077},{\"end\":10868,\"start\":10133},{\"end\":11128,\"start\":10870},{\"end\":12294,\"start\":11130},{\"end\":13843,\"start\":12296},{\"end\":14335,\"start\":13845},{\"end\":14719,\"start\":14337},{\"end\":15220,\"start\":14812},{\"end\":15543,\"start\":15266},{\"end\":16128,\"start\":15555},{\"end\":17015,\"start\":16154},{\"end\":17872,\"start\":17043},{\"end\":18305,\"start\":17909},{\"end\":18798,\"start\":18307},{\"end\":19520,\"start\":18800},{\"end\":20046,\"start\":19522},{\"end\":20344,\"start\":20068},{\"end\":21123,\"start\":20346},{\"end\":21594,\"start\":21125},{\"end\":22493,\"start\":21634},{\"end\":23244,\"start\":22525},{\"end\":23449,\"start\":23268},{\"end\":24156,\"start\":23451},{\"end\":24392,\"start\":24158},{\"end\":24625,\"start\":24413},{\"end\":24997,\"start\":24627},{\"end\":25152,\"start\":25045},{\"end\":26143,\"start\":25195},{\"end\":27046,\"start\":26145},{\"end\":27579,\"start\":27048},{\"end\":28234,\"start\":27581},{\"end\":28704,\"start\":28299},{\"end\":28991,\"start\":28744},{\"end\":29683,\"start\":28993},{\"end\":30273,\"start\":29685},{\"end\":30512,\"start\":30275},{\"end\":30597,\"start\":30574},{\"end\":30705,\"start\":30608},{\"end\":31176,\"start\":30718},{\"end\":31823,\"start\":31234},{\"end\":32362,\"start\":31825},{\"end\":33464,\"start\":32413},{\"end\":34480,\"start\":33466},{\"end\":35193,\"start\":34532},{\"end\":35702,\"start\":35245},{\"end\":36640,\"start\":35704},{\"end\":37065,\"start\":36642},{\"end\":37644,\"start\":37084},{\"end\":38111,\"start\":37646},{\"end\":38501,\"start\":38113},{\"end\":39225,\"start\":38503},{\"end\":39457,\"start\":39247},{\"end\":39796,\"start\":39459},{\"end\":40193,\"start\":39798},{\"end\":40552,\"start\":40195},{\"end\":40830,\"start\":40554},{\"end\":41683,\"start\":40832}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14761,\"start\":14720},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14811,\"start\":14761},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15265,\"start\":15221},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15554,\"start\":15544},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17042,\"start\":17016},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22524,\"start\":22494},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23267,\"start\":23245},{\"attributes\":{\"id\":\"formula_7\"},\"end\":24412,\"start\":24393},{\"attributes\":{\"id\":\"formula_8\"},\"end\":25044,\"start\":24998},{\"attributes\":{\"id\":\"formula_9\"},\"end\":28298,\"start\":28235},{\"attributes\":{\"id\":\"formula_10\"},\"end\":28743,\"start\":28705},{\"attributes\":{\"id\":\"formula_11\"},\"end\":32412,\"start\":32363},{\"attributes\":{\"id\":\"formula_12\"},\"end\":34531,\"start\":34481}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":39673,\"start\":39666},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40013,\"start\":40006},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":40564,\"start\":40557}]", "section_header": "[{\"end\":2142,\"start\":2127},{\"end\":16152,\"start\":16131},{\"end\":17907,\"start\":17875},{\"end\":20066,\"start\":20049},{\"end\":21632,\"start\":21597},{\"end\":25193,\"start\":25155},{\"end\":30537,\"start\":30515},{\"end\":30544,\"start\":30540},{\"end\":30554,\"start\":30547},{\"end\":30572,\"start\":30557},{\"end\":30606,\"start\":30600},{\"end\":30716,\"start\":30708},{\"end\":31232,\"start\":31179},{\"end\":35221,\"start\":35196},{\"end\":35243,\"start\":35224},{\"end\":37082,\"start\":37068},{\"end\":39245,\"start\":39228},{\"end\":41693,\"start\":41685},{\"end\":42107,\"start\":42098},{\"end\":42144,\"start\":42135},{\"end\":42430,\"start\":42421},{\"end\":42475,\"start\":42456}]", "table": "[{\"end\":42796,\"start\":42510}]", "figure_caption": "[{\"end\":41756,\"start\":41695},{\"end\":41932,\"start\":41759},{\"end\":42070,\"start\":41935},{\"end\":42096,\"start\":42073},{\"end\":42133,\"start\":42110},{\"end\":42419,\"start\":42147},{\"end\":42454,\"start\":42433},{\"end\":42510,\"start\":42477}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5503,\"start\":5498},{\"end\":16692,\"start\":16684},{\"end\":16915,\"start\":16907},{\"end\":17506,\"start\":17501},{\"end\":19093,\"start\":19088},{\"end\":19106,\"start\":19101},{\"end\":19271,\"start\":19263},{\"end\":19285,\"start\":19277},{\"end\":20303,\"start\":20298},{\"end\":21829,\"start\":21824},{\"end\":22029,\"start\":22021},{\"end\":22389,\"start\":22384},{\"end\":23011,\"start\":23010},{\"end\":23975,\"start\":23967},{\"end\":25730,\"start\":25725},{\"end\":27004,\"start\":26999},{\"end\":27886,\"start\":27880},{\"end\":29336,\"start\":29321},{\"end\":30501,\"start\":30496},{\"end\":30688,\"start\":30682},{\"end\":30748,\"start\":30742},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31677,\"start\":31671},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32041,\"start\":32032},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32289,\"start\":32280},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32869,\"start\":32860},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34326,\"start\":34320},{\"end\":34966,\"start\":34961},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35407,\"start\":35401},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37216,\"start\":37210},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37580,\"start\":37571},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37945,\"start\":37939},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37956,\"start\":37950},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37970,\"start\":37964},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38326,\"start\":38320},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38751,\"start\":38742},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39129,\"start\":39123}]", "bib_author_first_name": "[{\"end\":42880,\"start\":42879},{\"end\":42891,\"start\":42890},{\"end\":42901,\"start\":42900},{\"end\":43304,\"start\":43303},{\"end\":43312,\"start\":43311},{\"end\":43320,\"start\":43319},{\"end\":43328,\"start\":43327},{\"end\":43708,\"start\":43707},{\"end\":43717,\"start\":43716},{\"end\":43728,\"start\":43727},{\"end\":44155,\"start\":44154},{\"end\":44163,\"start\":44162},{\"end\":44171,\"start\":44170},{\"end\":44173,\"start\":44172},{\"end\":44186,\"start\":44185},{\"end\":44194,\"start\":44193},{\"end\":44203,\"start\":44202},{\"end\":44503,\"start\":44502},{\"end\":44510,\"start\":44509},{\"end\":44517,\"start\":44516},{\"end\":44523,\"start\":44522},{\"end\":44530,\"start\":44529},{\"end\":44538,\"start\":44537},{\"end\":44971,\"start\":44970},{\"end\":44980,\"start\":44979},{\"end\":44989,\"start\":44988},{\"end\":44997,\"start\":44996},{\"end\":44999,\"start\":44998},{\"end\":45007,\"start\":45006},{\"end\":45388,\"start\":45387},{\"end\":45396,\"start\":45395},{\"end\":45403,\"start\":45402},{\"end\":45411,\"start\":45410},{\"end\":45417,\"start\":45416},{\"end\":45419,\"start\":45418},{\"end\":45424,\"start\":45422},{\"end\":45885,\"start\":45884},{\"end\":45892,\"start\":45891},{\"end\":45901,\"start\":45900},{\"end\":45907,\"start\":45906},{\"end\":45914,\"start\":45913},{\"end\":46254,\"start\":46253},{\"end\":46261,\"start\":46260},{\"end\":46267,\"start\":46266},{\"end\":46546,\"start\":46545},{\"end\":46554,\"start\":46553},{\"end\":46561,\"start\":46560},{\"end\":46569,\"start\":46568},{\"end\":46575,\"start\":46574},{\"end\":46584,\"start\":46583},{\"end\":46592,\"start\":46591},{\"end\":46599,\"start\":46598},{\"end\":46962,\"start\":46961},{\"end\":46970,\"start\":46969},{\"end\":46977,\"start\":46976},{\"end\":46984,\"start\":46983},{\"end\":46992,\"start\":46991},{\"end\":47000,\"start\":46999},{\"end\":47006,\"start\":47005},{\"end\":47015,\"start\":47014},{\"end\":47530,\"start\":47529},{\"end\":47539,\"start\":47538},{\"end\":47545,\"start\":47544},{\"end\":47552,\"start\":47551},{\"end\":47561,\"start\":47560},{\"end\":47895,\"start\":47894},{\"end\":47903,\"start\":47902},{\"end\":47909,\"start\":47908},{\"end\":47917,\"start\":47916},{\"end\":47925,\"start\":47924},{\"end\":47931,\"start\":47930},{\"end\":48243,\"start\":48242},{\"end\":48251,\"start\":48250},{\"end\":48257,\"start\":48256},{\"end\":48264,\"start\":48263},{\"end\":48272,\"start\":48271},{\"end\":48279,\"start\":48278},{\"end\":48285,\"start\":48284},{\"end\":48656,\"start\":48655},{\"end\":48662,\"start\":48661},{\"end\":48668,\"start\":48667},{\"end\":48670,\"start\":48669},{\"end\":48681,\"start\":48680},{\"end\":48687,\"start\":48686},{\"end\":48696,\"start\":48695},{\"end\":48704,\"start\":48703},{\"end\":48712,\"start\":48711},{\"end\":48718,\"start\":48717},{\"end\":48725,\"start\":48724},{\"end\":49241,\"start\":49240},{\"end\":49247,\"start\":49246},{\"end\":49253,\"start\":49252},{\"end\":49255,\"start\":49254},{\"end\":49266,\"start\":49265},{\"end\":49272,\"start\":49271},{\"end\":49281,\"start\":49280},{\"end\":49289,\"start\":49288},{\"end\":49297,\"start\":49296},{\"end\":49303,\"start\":49302},{\"end\":49310,\"start\":49309},{\"end\":49317,\"start\":49316},{\"end\":49698,\"start\":49697},{\"end\":49706,\"start\":49705},{\"end\":49713,\"start\":49712},{\"end\":49720,\"start\":49719},{\"end\":49727,\"start\":49726},{\"end\":49735,\"start\":49734},{\"end\":49741,\"start\":49740},{\"end\":49747,\"start\":49746},{\"end\":49753,\"start\":49752},{\"end\":50201,\"start\":50200},{\"end\":50207,\"start\":50206},{\"end\":50215,\"start\":50214},{\"end\":50223,\"start\":50222},{\"end\":50229,\"start\":50228},{\"end\":50235,\"start\":50234},{\"end\":50242,\"start\":50241},{\"end\":50244,\"start\":50243},{\"end\":50250,\"start\":50249},{\"end\":50257,\"start\":50256},{\"end\":50720,\"start\":50719},{\"end\":50729,\"start\":50728},{\"end\":50743,\"start\":50742},{\"end\":50753,\"start\":50752},{\"end\":50755,\"start\":50754},{\"end\":50765,\"start\":50764},{\"end\":50775,\"start\":50774},{\"end\":50777,\"start\":50776},{\"end\":51174,\"start\":51173},{\"end\":51183,\"start\":51179},{\"end\":51192,\"start\":51191},{\"end\":51198,\"start\":51197},{\"end\":51210,\"start\":51209},{\"end\":51216,\"start\":51215},{\"end\":51224,\"start\":51223},{\"end\":51231,\"start\":51230},{\"end\":51238,\"start\":51237},{\"end\":51246,\"start\":51245},{\"end\":51706,\"start\":51705},{\"end\":51712,\"start\":51711},{\"end\":51722,\"start\":51721},{\"end\":52140,\"start\":52139},{\"end\":52147,\"start\":52146},{\"end\":52606,\"start\":52605},{\"end\":52614,\"start\":52613},{\"end\":52621,\"start\":52620},{\"end\":52627,\"start\":52626},{\"end\":52636,\"start\":52635},{\"end\":52642,\"start\":52641},{\"end\":52649,\"start\":52648},{\"end\":52656,\"start\":52655},{\"end\":53054,\"start\":53053},{\"end\":53060,\"start\":53059},{\"end\":53067,\"start\":53066},{\"end\":53075,\"start\":53074},{\"end\":53490,\"start\":53489},{\"end\":53496,\"start\":53495},{\"end\":53503,\"start\":53502},{\"end\":53512,\"start\":53511},{\"end\":53519,\"start\":53518},{\"end\":53526,\"start\":53525},{\"end\":53534,\"start\":53533},{\"end\":54274,\"start\":54273},{\"end\":54282,\"start\":54281},{\"end\":54291,\"start\":54287},{\"end\":54300,\"start\":54299},{\"end\":54308,\"start\":54307},{\"end\":54759,\"start\":54758},{\"end\":54766,\"start\":54765},{\"end\":54773,\"start\":54772},{\"end\":54780,\"start\":54779},{\"end\":54788,\"start\":54787},{\"end\":54794,\"start\":54793},{\"end\":54801,\"start\":54800},{\"end\":54807,\"start\":54806},{\"end\":55281,\"start\":55280},{\"end\":55290,\"start\":55289},{\"end\":55297,\"start\":55296},{\"end\":55303,\"start\":55302},{\"end\":55310,\"start\":55309},{\"end\":55316,\"start\":55315},{\"end\":55751,\"start\":55750},{\"end\":56058,\"start\":56057},{\"end\":56060,\"start\":56059},{\"end\":56274,\"start\":56270},{\"end\":56281,\"start\":56280},{\"end\":56289,\"start\":56288},{\"end\":56298,\"start\":56297},{\"end\":56300,\"start\":56299},{\"end\":56650,\"start\":56649},{\"end\":56661,\"start\":56660},{\"end\":56947,\"start\":56946},{\"end\":56958,\"start\":56957},{\"end\":56969,\"start\":56968},{\"end\":56979,\"start\":56978},{\"end\":56992,\"start\":56991},{\"end\":57001,\"start\":57000},{\"end\":57003,\"start\":57002},{\"end\":57012,\"start\":57011},{\"end\":57022,\"start\":57021},{\"end\":57522,\"start\":57521},{\"end\":57530,\"start\":57529},{\"end\":57536,\"start\":57535},{\"end\":57544,\"start\":57543},{\"end\":57958,\"start\":57957},{\"end\":57973,\"start\":57972},{\"end\":57982,\"start\":57981},{\"end\":57996,\"start\":57995},{\"end\":58011,\"start\":58010},{\"end\":58019,\"start\":58018},{\"end\":58034,\"start\":58033},{\"end\":58046,\"start\":58045},{\"end\":58058,\"start\":58057},{\"end\":58069,\"start\":58068},{\"end\":58442,\"start\":58438},{\"end\":58449,\"start\":58448},{\"end\":58455,\"start\":58454},{\"end\":58462,\"start\":58461},{\"end\":58470,\"start\":58469},{\"end\":58478,\"start\":58477},{\"end\":58486,\"start\":58485},{\"end\":58493,\"start\":58492},{\"end\":58502,\"start\":58501},{\"end\":58510,\"start\":58509},{\"end\":59034,\"start\":59033},{\"end\":59036,\"start\":59035},{\"end\":59049,\"start\":59048},{\"end\":59059,\"start\":59058},{\"end\":59075,\"start\":59071},{\"end\":59086,\"start\":59085}]", "bib_author_last_name": "[{\"end\":42888,\"start\":42881},{\"end\":42898,\"start\":42892},{\"end\":42908,\"start\":42902},{\"end\":43309,\"start\":43305},{\"end\":43317,\"start\":43313},{\"end\":43325,\"start\":43321},{\"end\":43332,\"start\":43329},{\"end\":43714,\"start\":43709},{\"end\":43725,\"start\":43718},{\"end\":43738,\"start\":43729},{\"end\":44160,\"start\":44156},{\"end\":44168,\"start\":44164},{\"end\":44183,\"start\":44174},{\"end\":44191,\"start\":44187},{\"end\":44200,\"start\":44195},{\"end\":44206,\"start\":44204},{\"end\":44507,\"start\":44504},{\"end\":44514,\"start\":44511},{\"end\":44520,\"start\":44518},{\"end\":44527,\"start\":44524},{\"end\":44535,\"start\":44531},{\"end\":44543,\"start\":44539},{\"end\":44977,\"start\":44972},{\"end\":44986,\"start\":44981},{\"end\":44994,\"start\":44990},{\"end\":45004,\"start\":45000},{\"end\":45016,\"start\":45008},{\"end\":45393,\"start\":45389},{\"end\":45400,\"start\":45397},{\"end\":45408,\"start\":45404},{\"end\":45414,\"start\":45412},{\"end\":45427,\"start\":45425},{\"end\":45889,\"start\":45886},{\"end\":45898,\"start\":45893},{\"end\":45904,\"start\":45902},{\"end\":45911,\"start\":45908},{\"end\":45917,\"start\":45915},{\"end\":46258,\"start\":46255},{\"end\":46264,\"start\":46262},{\"end\":46270,\"start\":46268},{\"end\":46551,\"start\":46547},{\"end\":46558,\"start\":46555},{\"end\":46566,\"start\":46562},{\"end\":46572,\"start\":46570},{\"end\":46581,\"start\":46576},{\"end\":46589,\"start\":46585},{\"end\":46596,\"start\":46593},{\"end\":46604,\"start\":46600},{\"end\":46967,\"start\":46963},{\"end\":46974,\"start\":46971},{\"end\":46981,\"start\":46978},{\"end\":46989,\"start\":46985},{\"end\":46997,\"start\":46993},{\"end\":47003,\"start\":47001},{\"end\":47012,\"start\":47007},{\"end\":47018,\"start\":47016},{\"end\":47536,\"start\":47531},{\"end\":47542,\"start\":47540},{\"end\":47549,\"start\":47546},{\"end\":47558,\"start\":47553},{\"end\":47565,\"start\":47562},{\"end\":47900,\"start\":47896},{\"end\":47906,\"start\":47904},{\"end\":47914,\"start\":47910},{\"end\":47922,\"start\":47918},{\"end\":47928,\"start\":47926},{\"end\":47936,\"start\":47932},{\"end\":48248,\"start\":48244},{\"end\":48254,\"start\":48252},{\"end\":48261,\"start\":48258},{\"end\":48269,\"start\":48265},{\"end\":48276,\"start\":48273},{\"end\":48282,\"start\":48280},{\"end\":48289,\"start\":48286},{\"end\":48659,\"start\":48657},{\"end\":48665,\"start\":48663},{\"end\":48678,\"start\":48671},{\"end\":48684,\"start\":48682},{\"end\":48693,\"start\":48688},{\"end\":48701,\"start\":48697},{\"end\":48709,\"start\":48705},{\"end\":48715,\"start\":48713},{\"end\":48722,\"start\":48719},{\"end\":48729,\"start\":48726},{\"end\":49244,\"start\":49242},{\"end\":49250,\"start\":49248},{\"end\":49263,\"start\":49256},{\"end\":49269,\"start\":49267},{\"end\":49278,\"start\":49273},{\"end\":49286,\"start\":49282},{\"end\":49294,\"start\":49290},{\"end\":49300,\"start\":49298},{\"end\":49307,\"start\":49304},{\"end\":49314,\"start\":49311},{\"end\":49320,\"start\":49318},{\"end\":49703,\"start\":49699},{\"end\":49710,\"start\":49707},{\"end\":49717,\"start\":49714},{\"end\":49724,\"start\":49721},{\"end\":49732,\"start\":49728},{\"end\":49738,\"start\":49736},{\"end\":49744,\"start\":49742},{\"end\":49750,\"start\":49748},{\"end\":49757,\"start\":49754},{\"end\":50204,\"start\":50202},{\"end\":50212,\"start\":50208},{\"end\":50220,\"start\":50216},{\"end\":50226,\"start\":50224},{\"end\":50232,\"start\":50230},{\"end\":50239,\"start\":50236},{\"end\":50247,\"start\":50245},{\"end\":50254,\"start\":50251},{\"end\":50260,\"start\":50258},{\"end\":50726,\"start\":50721},{\"end\":50740,\"start\":50730},{\"end\":50750,\"start\":50744},{\"end\":50762,\"start\":50756},{\"end\":50772,\"start\":50766},{\"end\":50785,\"start\":50778},{\"end\":51177,\"start\":51175},{\"end\":51189,\"start\":51184},{\"end\":51195,\"start\":51193},{\"end\":51207,\"start\":51199},{\"end\":51213,\"start\":51211},{\"end\":51221,\"start\":51217},{\"end\":51228,\"start\":51225},{\"end\":51235,\"start\":51232},{\"end\":51243,\"start\":51239},{\"end\":51249,\"start\":51247},{\"end\":51709,\"start\":51707},{\"end\":51719,\"start\":51713},{\"end\":51730,\"start\":51723},{\"end\":52144,\"start\":52141},{\"end\":52153,\"start\":52148},{\"end\":52611,\"start\":52607},{\"end\":52618,\"start\":52615},{\"end\":52624,\"start\":52622},{\"end\":52633,\"start\":52628},{\"end\":52639,\"start\":52637},{\"end\":52646,\"start\":52643},{\"end\":52653,\"start\":52650},{\"end\":52660,\"start\":52657},{\"end\":53057,\"start\":53055},{\"end\":53064,\"start\":53061},{\"end\":53072,\"start\":53068},{\"end\":53081,\"start\":53076},{\"end\":53493,\"start\":53491},{\"end\":53500,\"start\":53497},{\"end\":53509,\"start\":53504},{\"end\":53516,\"start\":53513},{\"end\":53523,\"start\":53520},{\"end\":53531,\"start\":53527},{\"end\":53539,\"start\":53535},{\"end\":54279,\"start\":54275},{\"end\":54285,\"start\":54283},{\"end\":54297,\"start\":54292},{\"end\":54305,\"start\":54301},{\"end\":54313,\"start\":54309},{\"end\":54763,\"start\":54760},{\"end\":54770,\"start\":54767},{\"end\":54777,\"start\":54774},{\"end\":54785,\"start\":54781},{\"end\":54791,\"start\":54789},{\"end\":54798,\"start\":54795},{\"end\":54804,\"start\":54802},{\"end\":54811,\"start\":54808},{\"end\":55287,\"start\":55282},{\"end\":55294,\"start\":55291},{\"end\":55300,\"start\":55298},{\"end\":55307,\"start\":55304},{\"end\":55313,\"start\":55311},{\"end\":55320,\"start\":55317},{\"end\":55763,\"start\":55752},{\"end\":56069,\"start\":56061},{\"end\":56278,\"start\":56275},{\"end\":56286,\"start\":56282},{\"end\":56295,\"start\":56290},{\"end\":56306,\"start\":56301},{\"end\":56658,\"start\":56651},{\"end\":56665,\"start\":56662},{\"end\":56955,\"start\":56948},{\"end\":56966,\"start\":56959},{\"end\":56976,\"start\":56970},{\"end\":56989,\"start\":56980},{\"end\":56998,\"start\":56993},{\"end\":57009,\"start\":57004},{\"end\":57019,\"start\":57013},{\"end\":57033,\"start\":57023},{\"end\":57527,\"start\":57523},{\"end\":57533,\"start\":57531},{\"end\":57541,\"start\":57537},{\"end\":57547,\"start\":57545},{\"end\":57970,\"start\":57959},{\"end\":57979,\"start\":57974},{\"end\":57993,\"start\":57983},{\"end\":58008,\"start\":57997},{\"end\":58016,\"start\":58012},{\"end\":58031,\"start\":58020},{\"end\":58043,\"start\":58035},{\"end\":58055,\"start\":58047},{\"end\":58066,\"start\":58059},{\"end\":58075,\"start\":58070},{\"end\":58446,\"start\":58443},{\"end\":58452,\"start\":58450},{\"end\":58459,\"start\":58456},{\"end\":58467,\"start\":58463},{\"end\":58475,\"start\":58471},{\"end\":58483,\"start\":58479},{\"end\":58490,\"start\":58487},{\"end\":58499,\"start\":58494},{\"end\":58507,\"start\":58503},{\"end\":58515,\"start\":58511},{\"end\":59046,\"start\":59037},{\"end\":59056,\"start\":59050},{\"end\":59069,\"start\":59060},{\"end\":59083,\"start\":59076},{\"end\":59097,\"start\":59087}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":233148602},\"end\":43192,\"start\":42813},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":245015722},\"end\":43629,\"start\":43194},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6893869},\"end\":44096,\"start\":43631},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":208108613},\"end\":44410,\"start\":44098},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":212414744},\"end\":44906,\"start\":44412},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":211068692},\"end\":45357,\"start\":44908},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":10323062},\"end\":45799,\"start\":45359},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":249912494},\"end\":46199,\"start\":45801},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9786871},\"end\":46469,\"start\":46201},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":261292787},\"end\":46870,\"start\":46471},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":243760215},\"end\":47453,\"start\":46872},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4507152},\"end\":47842,\"start\":47455},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":249363889},\"end\":48194,\"start\":47844},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":86606918},\"end\":48551,\"start\":48196},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":227154551},\"end\":49152,\"start\":48553},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":236409543},\"end\":49600,\"start\":49154},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":249901105},\"end\":50078,\"start\":49602},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":248223926},\"end\":50656,\"start\":50080},{\"attributes\":{\"doi\":\"10.1145/2601097.2601119\",\"id\":\"b18\",\"matched_paper_id\":250074908},\"end\":51103,\"start\":50658},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":209862704},\"end\":51630,\"start\":51105},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":233296658},\"end\":52076,\"start\":51632},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":231740266},\"end\":52519,\"start\":52078},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":249907587},\"end\":52966,\"start\":52521},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":209459245},\"end\":53385,\"start\":52968},{\"attributes\":{\"doi\":\"10.1145/3460120.3484755\",\"id\":\"b24\",\"matched_paper_id\":243092171},\"end\":54208,\"start\":53387},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":211096559},\"end\":54658,\"start\":54210},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":243942547},\"end\":55214,\"start\":54660},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":221785132},\"end\":55719,\"start\":55216},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":59853680},\"end\":56038,\"start\":55721},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4734886},\"end\":56187,\"start\":56040},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":195944196},\"end\":56585,\"start\":56189},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":53067},\"end\":56917,\"start\":56587},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13756489},\"end\":57444,\"start\":56919},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":146108083},\"end\":57879,\"start\":57446},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b34\"},\"end\":58360,\"start\":57881},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":241033425},\"end\":58952,\"start\":58362},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":231740629},\"end\":59413,\"start\":58954}]", "bib_title": "[{\"end\":42877,\"start\":42813},{\"end\":43301,\"start\":43194},{\"end\":43705,\"start\":43631},{\"end\":44152,\"start\":44098},{\"end\":44500,\"start\":44412},{\"end\":44968,\"start\":44908},{\"end\":45385,\"start\":45359},{\"end\":45882,\"start\":45801},{\"end\":46251,\"start\":46201},{\"end\":46543,\"start\":46471},{\"end\":46959,\"start\":46872},{\"end\":47527,\"start\":47455},{\"end\":47892,\"start\":47844},{\"end\":48240,\"start\":48196},{\"end\":48653,\"start\":48553},{\"end\":49238,\"start\":49154},{\"end\":49695,\"start\":49602},{\"end\":50198,\"start\":50080},{\"end\":50717,\"start\":50658},{\"end\":51171,\"start\":51105},{\"end\":51703,\"start\":51632},{\"end\":52137,\"start\":52078},{\"end\":52603,\"start\":52521},{\"end\":53051,\"start\":52968},{\"end\":53487,\"start\":53387},{\"end\":54271,\"start\":54210},{\"end\":54756,\"start\":54660},{\"end\":55278,\"start\":55216},{\"end\":55748,\"start\":55721},{\"end\":56055,\"start\":56040},{\"end\":56268,\"start\":56189},{\"end\":56647,\"start\":56587},{\"end\":56944,\"start\":56919},{\"end\":57519,\"start\":57446},{\"end\":58436,\"start\":58362},{\"end\":59031,\"start\":58954}]", "bib_author": "[{\"end\":42890,\"start\":42879},{\"end\":42900,\"start\":42890},{\"end\":42910,\"start\":42900},{\"end\":43311,\"start\":43303},{\"end\":43319,\"start\":43311},{\"end\":43327,\"start\":43319},{\"end\":43334,\"start\":43327},{\"end\":43716,\"start\":43707},{\"end\":43727,\"start\":43716},{\"end\":43740,\"start\":43727},{\"end\":44162,\"start\":44154},{\"end\":44170,\"start\":44162},{\"end\":44185,\"start\":44170},{\"end\":44193,\"start\":44185},{\"end\":44202,\"start\":44193},{\"end\":44208,\"start\":44202},{\"end\":44509,\"start\":44502},{\"end\":44516,\"start\":44509},{\"end\":44522,\"start\":44516},{\"end\":44529,\"start\":44522},{\"end\":44537,\"start\":44529},{\"end\":44545,\"start\":44537},{\"end\":44979,\"start\":44970},{\"end\":44988,\"start\":44979},{\"end\":44996,\"start\":44988},{\"end\":45006,\"start\":44996},{\"end\":45018,\"start\":45006},{\"end\":45395,\"start\":45387},{\"end\":45402,\"start\":45395},{\"end\":45410,\"start\":45402},{\"end\":45416,\"start\":45410},{\"end\":45422,\"start\":45416},{\"end\":45429,\"start\":45422},{\"end\":45891,\"start\":45884},{\"end\":45900,\"start\":45891},{\"end\":45906,\"start\":45900},{\"end\":45913,\"start\":45906},{\"end\":45919,\"start\":45913},{\"end\":46260,\"start\":46253},{\"end\":46266,\"start\":46260},{\"end\":46272,\"start\":46266},{\"end\":46553,\"start\":46545},{\"end\":46560,\"start\":46553},{\"end\":46568,\"start\":46560},{\"end\":46574,\"start\":46568},{\"end\":46583,\"start\":46574},{\"end\":46591,\"start\":46583},{\"end\":46598,\"start\":46591},{\"end\":46606,\"start\":46598},{\"end\":46969,\"start\":46961},{\"end\":46976,\"start\":46969},{\"end\":46983,\"start\":46976},{\"end\":46991,\"start\":46983},{\"end\":46999,\"start\":46991},{\"end\":47005,\"start\":46999},{\"end\":47014,\"start\":47005},{\"end\":47020,\"start\":47014},{\"end\":47538,\"start\":47529},{\"end\":47544,\"start\":47538},{\"end\":47551,\"start\":47544},{\"end\":47560,\"start\":47551},{\"end\":47567,\"start\":47560},{\"end\":47902,\"start\":47894},{\"end\":47908,\"start\":47902},{\"end\":47916,\"start\":47908},{\"end\":47924,\"start\":47916},{\"end\":47930,\"start\":47924},{\"end\":47938,\"start\":47930},{\"end\":48250,\"start\":48242},{\"end\":48256,\"start\":48250},{\"end\":48263,\"start\":48256},{\"end\":48271,\"start\":48263},{\"end\":48278,\"start\":48271},{\"end\":48284,\"start\":48278},{\"end\":48291,\"start\":48284},{\"end\":48661,\"start\":48655},{\"end\":48667,\"start\":48661},{\"end\":48680,\"start\":48667},{\"end\":48686,\"start\":48680},{\"end\":48695,\"start\":48686},{\"end\":48703,\"start\":48695},{\"end\":48711,\"start\":48703},{\"end\":48717,\"start\":48711},{\"end\":48724,\"start\":48717},{\"end\":48731,\"start\":48724},{\"end\":49246,\"start\":49240},{\"end\":49252,\"start\":49246},{\"end\":49265,\"start\":49252},{\"end\":49271,\"start\":49265},{\"end\":49280,\"start\":49271},{\"end\":49288,\"start\":49280},{\"end\":49296,\"start\":49288},{\"end\":49302,\"start\":49296},{\"end\":49309,\"start\":49302},{\"end\":49316,\"start\":49309},{\"end\":49322,\"start\":49316},{\"end\":49705,\"start\":49697},{\"end\":49712,\"start\":49705},{\"end\":49719,\"start\":49712},{\"end\":49726,\"start\":49719},{\"end\":49734,\"start\":49726},{\"end\":49740,\"start\":49734},{\"end\":49746,\"start\":49740},{\"end\":49752,\"start\":49746},{\"end\":49759,\"start\":49752},{\"end\":50206,\"start\":50200},{\"end\":50214,\"start\":50206},{\"end\":50222,\"start\":50214},{\"end\":50228,\"start\":50222},{\"end\":50234,\"start\":50228},{\"end\":50241,\"start\":50234},{\"end\":50249,\"start\":50241},{\"end\":50256,\"start\":50249},{\"end\":50262,\"start\":50256},{\"end\":50728,\"start\":50719},{\"end\":50742,\"start\":50728},{\"end\":50752,\"start\":50742},{\"end\":50764,\"start\":50752},{\"end\":50774,\"start\":50764},{\"end\":50787,\"start\":50774},{\"end\":51179,\"start\":51173},{\"end\":51191,\"start\":51179},{\"end\":51197,\"start\":51191},{\"end\":51209,\"start\":51197},{\"end\":51215,\"start\":51209},{\"end\":51223,\"start\":51215},{\"end\":51230,\"start\":51223},{\"end\":51237,\"start\":51230},{\"end\":51245,\"start\":51237},{\"end\":51251,\"start\":51245},{\"end\":51711,\"start\":51705},{\"end\":51721,\"start\":51711},{\"end\":51732,\"start\":51721},{\"end\":52146,\"start\":52139},{\"end\":52155,\"start\":52146},{\"end\":52613,\"start\":52605},{\"end\":52620,\"start\":52613},{\"end\":52626,\"start\":52620},{\"end\":52635,\"start\":52626},{\"end\":52641,\"start\":52635},{\"end\":52648,\"start\":52641},{\"end\":52655,\"start\":52648},{\"end\":52662,\"start\":52655},{\"end\":53059,\"start\":53053},{\"end\":53066,\"start\":53059},{\"end\":53074,\"start\":53066},{\"end\":53083,\"start\":53074},{\"end\":53495,\"start\":53489},{\"end\":53502,\"start\":53495},{\"end\":53511,\"start\":53502},{\"end\":53518,\"start\":53511},{\"end\":53525,\"start\":53518},{\"end\":53533,\"start\":53525},{\"end\":53541,\"start\":53533},{\"end\":54281,\"start\":54273},{\"end\":54287,\"start\":54281},{\"end\":54299,\"start\":54287},{\"end\":54307,\"start\":54299},{\"end\":54315,\"start\":54307},{\"end\":54765,\"start\":54758},{\"end\":54772,\"start\":54765},{\"end\":54779,\"start\":54772},{\"end\":54787,\"start\":54779},{\"end\":54793,\"start\":54787},{\"end\":54800,\"start\":54793},{\"end\":54806,\"start\":54800},{\"end\":54813,\"start\":54806},{\"end\":55289,\"start\":55280},{\"end\":55296,\"start\":55289},{\"end\":55302,\"start\":55296},{\"end\":55309,\"start\":55302},{\"end\":55315,\"start\":55309},{\"end\":55322,\"start\":55315},{\"end\":55765,\"start\":55750},{\"end\":56071,\"start\":56057},{\"end\":56280,\"start\":56270},{\"end\":56288,\"start\":56280},{\"end\":56297,\"start\":56288},{\"end\":56308,\"start\":56297},{\"end\":56660,\"start\":56649},{\"end\":56667,\"start\":56660},{\"end\":56957,\"start\":56946},{\"end\":56968,\"start\":56957},{\"end\":56978,\"start\":56968},{\"end\":56991,\"start\":56978},{\"end\":57000,\"start\":56991},{\"end\":57011,\"start\":57000},{\"end\":57021,\"start\":57011},{\"end\":57035,\"start\":57021},{\"end\":57529,\"start\":57521},{\"end\":57535,\"start\":57529},{\"end\":57543,\"start\":57535},{\"end\":57549,\"start\":57543},{\"end\":57972,\"start\":57957},{\"end\":57981,\"start\":57972},{\"end\":57995,\"start\":57981},{\"end\":58010,\"start\":57995},{\"end\":58018,\"start\":58010},{\"end\":58033,\"start\":58018},{\"end\":58045,\"start\":58033},{\"end\":58057,\"start\":58045},{\"end\":58068,\"start\":58057},{\"end\":58077,\"start\":58068},{\"end\":58448,\"start\":58438},{\"end\":58454,\"start\":58448},{\"end\":58461,\"start\":58454},{\"end\":58469,\"start\":58461},{\"end\":58477,\"start\":58469},{\"end\":58485,\"start\":58477},{\"end\":58492,\"start\":58485},{\"end\":58501,\"start\":58492},{\"end\":58509,\"start\":58501},{\"end\":58517,\"start\":58509},{\"end\":59048,\"start\":59033},{\"end\":59058,\"start\":59048},{\"end\":59071,\"start\":59058},{\"end\":59085,\"start\":59071},{\"end\":59099,\"start\":59085}]", "bib_venue": "[{\"end\":42984,\"start\":42910},{\"end\":43395,\"start\":43334},{\"end\":43816,\"start\":43740},{\"end\":44229,\"start\":44208},{\"end\":44635,\"start\":44545},{\"end\":45108,\"start\":45018},{\"end\":45523,\"start\":45429},{\"end\":45980,\"start\":45919},{\"end\":46309,\"start\":46272},{\"end\":46643,\"start\":46606},{\"end\":47103,\"start\":47020},{\"end\":47628,\"start\":47567},{\"end\":47999,\"start\":47938},{\"end\":48352,\"start\":48291},{\"end\":48802,\"start\":48731},{\"end\":49359,\"start\":49322},{\"end\":49820,\"start\":49759},{\"end\":50354,\"start\":50262},{\"end\":50838,\"start\":50810},{\"end\":51341,\"start\":51251},{\"end\":51830,\"start\":51732},{\"end\":52245,\"start\":52155},{\"end\":52723,\"start\":52662},{\"end\":53152,\"start\":53083},{\"end\":53663,\"start\":53564},{\"end\":54414,\"start\":54315},{\"end\":54888,\"start\":54813},{\"end\":55412,\"start\":55322},{\"end\":55804,\"start\":55765},{\"end\":56090,\"start\":56071},{\"end\":56367,\"start\":56308},{\"end\":56728,\"start\":56667},{\"end\":57124,\"start\":57035},{\"end\":57639,\"start\":57549},{\"end\":57955,\"start\":57881},{\"end\":58598,\"start\":58517},{\"end\":59160,\"start\":59099},{\"end\":43879,\"start\":43818},{\"end\":45604,\"start\":45525},{\"end\":47173,\"start\":47105},{\"end\":48860,\"start\":48804},{\"end\":52322,\"start\":52247},{\"end\":53161,\"start\":53154},{\"end\":53766,\"start\":53665},{\"end\":54950,\"start\":54890},{\"end\":55489,\"start\":55414},{\"end\":55816,\"start\":55806},{\"end\":57200,\"start\":57126},{\"end\":58666,\"start\":58600}]"}}}, "year": 2023, "month": 12, "day": 17}