{"id": 247957775, "updated": "2023-10-05 15:35:54.19", "metadata": {"title": "Multi-View Transformer for 3D Visual Grounding", "authors": "[{\"first\":\"Shijia\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Yilun\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jiaya\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Liwei\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The 3D visual grounding task aims to ground a natural language description to the targeted object in a 3D scene, which is usually represented in 3D point clouds. Previous works studied visual grounding under specific views. The vision-language correspondence learned by this way can easily fail once the view changes. In this paper, we propose a Multi-View Transformer (MVT) for 3D visual grounding. We project the 3D scene to a multi-view space, in which the position information of the 3D scene under different views are modeled simultaneously and aggregated together. The multi-view space enables the network to learn a more robust multi-modal representation for 3D visual grounding and eliminates the dependence on specific views. Extensive experiments show that our approach significantly outperforms all state-of-the-art methods. Specifically, on Nr3D and Sr3D datasets, our method outperforms the best competitor by 11.2% and 7.1% and even surpasses recent work with extra 2D assistance by 5.9% and 6.6%. Our code is available at https://github.com/sega-hsj/MVT-3DVG.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.02174", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/HuangCJ022", "doi": "10.1109/cvpr52688.2022.01508"}}, "content": {"source": {"pdf_hash": "703416b6e3474c7faf1a634d06e9fdd61c509b85", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.02174v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5673c649308eccb4b51438a329295ab9ca3bd30f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/703416b6e3474c7faf1a634d06e9fdd61c509b85.txt", "contents": "\nMulti-View Transformer for 3D Visual Grounding\n\n\nShijia Huang sjhuang@cse.cuhk.edu.hk \nThe Chinese University of Hong Kong\n\n\nYilun Chen ylchen@cse.cuhk.edu.hk \nThe Chinese University of Hong Kong\n\n\nJiaya Jia \nThe Chinese University of Hong Kong\n\n\nLiwei Wang lwwang@cse.cuhk.edu.hk \nThe Chinese University of Hong Kong\n\n\nMulti-View Transformer for 3D Visual Grounding\n\nThe 3D visual grounding task aims to ground a natural language description to the targeted object in a 3D scene, which is usually represented in 3D point clouds. Previous works studied visual grounding under specific views. The vision-language correspondence learned by this way can easily fail once the view changes. In this paper, we propose a Multi-View Transformer (MVT) for 3D visual grounding. We project the 3D scene to a multi-view space, in which the position information of the 3D scene under different views are modeled simultaneously and aggregated together. The multi-view space enables the network to learn a more robust multi-modal representation for 3D visual grounding and eliminates the dependence on specific views. Extensive experiments show that our approach significantly outperforms all state-of-the-art methods. Specifically, on Nr3D and Sr3D datasets, our method outperforms the best competitor by 11.2% and 7.1% and even surpasses recent work with extra 2D assistance by 5.9% and 6.6%. Our code is available at https://github.com/sega-hsj/MVT-3DVG.\n\nIntroduction\n\nVisual Grounding (VG) aims to ground a natural language description to the target object. The field has made tremendous progress in 2D images [18,22,30,35]. With the rapid development of 3D sensors and 3D vision technology, 3D visual grounding has recently attracted more attention, with wide applications including visionlanguage navigation [31,39], intelligent agents [26,32], and autonomous vehicles [11,21]. Compared to 2D visual grounding, the 3D task has more complex input data (e.g., sparse point clouds) and more variant spatial relationships, switching the output from grounding to 2D regions to 3D objects.\n\nRecent works [1,3,14,25,33,36] mainly follow a twostage scheme, i.e., first generating all candidate objects in 3D scene and then selecting the most matched one. These two-stage approaches in 3D visual grounding are tailored from 2D visual grounding methods [20,34], which have Query: \"Facing the bed, the pillow on the far right.\"\n\n\nFront View\n\nBack View Figure 1. An example from Nr3D [1]: when the view is changed, the position information in 3D scene can be quite different. The mentioned \"pillow\" exhibits changed positions, i.e., 3D coordinates, when the view is changed from the front to the back.\n\nnot considered the unique property of 3D data. Therefore, in this paper, we bring the community's attention back to studying the intrinsic property of 3D data to break through the 3D visual grounding research.\n\nPosition information is essential to locate an object. The visual grounding aligns visual position information in the scene (e.g., coordinates of objects) with the location described in natural language (e.g., \"left\" or \"right\"). Unlike 2D images with static views, 3D scenes can freely rotate to different views. These dynamic view changes make 3D visual grounding more challenging. The view changes will directly affect the position encoding, e.g., coordinates of objects, thus bringing more difficulty in representing 3D objects. For example, as shown in Fig. 1, the rightmost pillow in the front view becomes the leftmost one in the back view. This discrepancy commonly exists in real applications: when a robot or an intelligent agent is navigating in the 3D scene, its view is usually different from the speaker's or commander's view. Such view discrepancy can bring more ambiguity in identifying the location of targets. To alleviate this view issue, when Referit3D [1] were collecting data, they split the data into view-dependent and view-independent categories according to whether the description was constrained to the speaker's view. They have provided additional view guidance, e.g., \"Facing the bed\" in the example of Fig. 1, to those view-dependent data. In practice, the view constraint is usually implicit, and we cannot always rely on speakers to provide augmented descriptions to solve the problem. Especially, the robot or the intelligent agent should be able to ground to the target object no matter how its view is different from the speaker's. Thus, developing a robust approach to view changes becomes imperative in 3D visual grounding.\n\nExisting methods [1,10,14,25,33,36,38] have paid little attention to the issue as mentioned above and mainly learned the grounding task under specific views. The vision-language correspondence learned in this way can easily fail once the view changes. An intuitive solution to alleviating the problem is to add augmented rotation data under different views during training. However, as we tried and tested (see Tab. 4 in our experimental section), such a straightforward method does not bring the improvement. This result can be understood, i.e., although novel views of the 3D scene can be augmented in the training stage, visual features under specific views are not accurately aligned with the position information embedded in the language description. An alternative is to ask the model to predict the view attached to the language query, say, the speaker's view, and then align the query with the visual features under the predicted view. However, this will incur a lot more costs for view prediction, and if the model predicts the wrong view, the alignment between the two modalities will inevitably be damaged. Experiments provided by LanguageRefer [25] confirmed this point, i.e., even if they manually correct the view of all training data to the speaker's view, the performance of the model on unseen test data with view discrepancy still cannot be improved. Therefore, in this paper, we turn to the exploration of learning better multimodal representations for 3D visual grounding, which are robust to view changes, instead of augmenting view data or predicting views.\n\nIn this paper, we propose a Multi-View Transformer (MVT) for 3D visual grounding to learn such a view-robust representation. Given the 3D scene under a specific view, our MVT projects it to a multi-view space by rotating the original view with equal angles. Our approach simultaneously models the position information within the 3D scene under different views and aggregates all information to learn a view-robust representation by eliminating the dependence on the starting view. Furthermore, to reduce the computational cost of modeling multi-view space, we decouple the 3D object representation computation into computing point clouds features and object coordinates separately to share the point clouds features across views. Besides, with the rich and fine-grained category labels provided by Nr3D [1] and Sr3D [1], we make full use of these language priors and propose a language-guided object classification task to enhance the object encoder further. Extensive experiments have been conducted on three mainstream benchmark datasets Nr3D [1], Sr3D [1], and ScanRefer [3]. Our MVT outperforms all state-ofthe-art methods by a large margin. Specifically, on Nr3D and Sr3D, MVT not only outperforms the best competitor by 11.2% and 7.1% respectively but also surpasses the method [33] with extra 2D assistance by 5.9% and 6.6%.\n\n\nRelated Work\n\n3D Visual Grounding. The task of 3D visual grounding aims to ground objects described by natural language in 3D scenes. Referit3D [1] and ScanRefer [3] have built benchmarks and baselines for the 3D visual grounding task with language query annotations on ScanNet [7] dataset. Recent works [1,3,14,25,33,36] solve the 3D grounding tasks following a two-stage scheme. In the first stage, 3D object proposals are generated either with ground truth [1] or by a 3D object detector [3,16]. In the second stage, the task is modeled as a matching problem where 3D visual features are fused with the text features of the query language description to predict the matching scores. Referit3D [1] uses GRU [6] to extract text features, and makes use of GNN [27] to model the relationship between objects. With the widespread of transformers [29], recent works [14,25,33] have explored transformers for feature extraction and feature fusion. Among them, InstanceRefer [36] conducts multi-level contextual referring by considering attributes and local relations. LanguageRefer [25] replaces point cloud features with predicted labels of objects, transforming the multi-modal task into a language modeling problem. SAT [33] has conducted joint training of 2d grounding and 3d grounding, improving the performance significantly by cross-modal knowledge transfer. 3DVG-Transformer [38] proposes a coordinate-guided contextual aggregation module and a multiplex attention module to fully utilize the contextual clues in the 3D scene. FFL-3DOG [10] uses language and visual scene graphs to guide the feature matching between language and point cloud. TransRefer3D [14] designs an entity-aware attention module and a relationaware attention module to conduct fine-grained cross-modal feature matching. Different from previous works, we start by studying the intrinsic problem of 3D visual data and build a view-robust multi-modal representation to solve the problem. Learning with Natural Language Supervision. In parallel, there are also a series of works [8,13,17,24,37] to learn the visual model guided by natural language features, with the target of learning transferrable models from language. Among them, VirTex [8] pre-trains the network using dense captions (e.g., COCO Captions [4]) to learn visual representations and transfer them to downstream visual recognition tasks. CLIP [24] performs the natural language supervision on a sufficiently large dataset collected from the Internet and demonstrates its powerful zero-shot transfer capability. In this paper, we leverage fine-grained object labels to enhance object encoders to improve the alignment between object features and language queries. Multi-View Learning. In 3D computer vision, recent works have focused on learning better representation by rendering objects from different views. MV3D [5] encodes the sparse 3D point cloud with a compact multi-view (e.g., bird's eye view and front view) representation for the 3D detection task [12]. MVCNN [28] completes the 3d object classification by rendering a large number of 2d pictures from 3d objects. These works project 3D point clouds onto different 2D planes to enhance visual features. Unlike them, we project the positional information of the 3D scene into a multi-view space, thereby learning a view-robust representation for better vision-language alignment.\n\n\nMethod\n\nIn this section, we introduce the proposed Multi-View Transformer (MVT) for 3D visual grounding. Fig. 3 shows the whole framework, which contains object encoding, language encoding, multi-modal feature fusion, and multiview aggregation modules. In this section, we first illustrate our multi-view 3D visual grounding framework and then provide details of each component.\n\n\nMulti-View 3D Visual Grounding\n\nThe key idea of our model is to learn a multi-modal representation independent from its specific starting view. Given the 3D scene S under an arbitrary view and a natural language query Q, we extend S to N different views {S 1 , . . . , S N } through rotations, given as:\nS j = R(\u03b8 j v ) \u00d7 S,(1)R(\u03b8) = \uf8eb \uf8ed cos \u03b8 \u2212 sin \u03b8 0 sin \u03b8 cos \u03b8 0 0 0 1 \uf8f6 \uf8f8 T ,(2)\nwhere R(\u03b8) is the rotation matrix, and T is the transpose. R(\u03b8) \u00d7 S means rotating the 3D scene S clockwise by \u03b8 degrees along the Z-axis. \u03b8 j v is the view-rotation angle of the j-th view. We adopt the equal angle rotation, which is:\n\u03b8 j v = 2\u03c0(j \u2212 1) N , j \u2208 {1, . . . , N },(3)\nwhere N is the number of total views, usually set to be {1, 2, 4, 8}. Fig. 2 shows \u03b8 v with different view settings. \u03b8 1 v equals to 0 hence S 1 is the same as S. Therefore, the original 3D visual grounding task VG under certain view S is now\nV 1 V 1 V 1 V 1 V 2 V 3 V 4 V 5 V 8 V 7 V 6 V 2 V 2 V 3 V 4 = = = = Figure 2.\nThe illustration of multi-view generation. N is the view number, V1 is the initial view of scene S. We generate multiple views by the equal angle rotation, which divides the 2\u03c0 view space equally into N parts.\n\nextended to learn a more robust multi-view representation, which is modeled as:\nVG(S, Q) = g(f (S 1 , Q), . . . , f (S N , Q)),(4)\nwhere f (\u00b7) is a shared network for feature extractions under different views. f (\u00b7) takes into account the position information of different views in the multi-view space. g(\u00b7) is an order-independent function for information aggregation. Therefore, Eq.(4) has the intriguing property, i.e., when we change the starting view from S to any other in {S 1 , . . . , S N }, the final multi-view representation of VG(S, Q) keeps invariant. This property meets our goal of building a robust representation that is independent of its initial view by projecting S to the multi-view space of {S 1 , . . . , S N }.\n\n\nObject Feature Encoding\n\nFollowing previous settings [1,3], we assume to have access to M candidate objects in the scene S under a specific view. Objects are either generated from the ground truth as in Referit3D [1] or predicted by a 3D object detector as in ScanRefer [3]. 3D object encoding extracts features from 3D point cloud data, which is the main computing bottleneck, and it is more costly to encode features for multiple views. Therefore, we decoupled the 3D object feature extraction process into two steps, i.e., computing point clouds features and multi-view positional encoding.  Figure 3. The framework of the proposed multi-view transformer (MVT) for 3D visual grounding. We project the 3D scene to a multi-view space by the equal angle rotation. For each view, object features and language features are fused by the transformer decoder [29]. We finally aggregate information from multi-view space to form a view-robust representation and predict the grounding results.\n\nfor all objects are extracted by the object encoder (e.g. PointNet++ [23]) and a linear mapping layer, given as:\nx i = LN(W x \u00b7 PointNet++(pc i )),(5)\nwhere x i is the point cloud feature of object i, W x is a projection matrix. LN(\u00b7) is layer normalization [2].\n\nMulti-View Positional Encoding. The object coordinates are represented by box center coordinates {b 1 , . . . , b M }, in the form of XYZ. We rotate them to generate the object coordinates under multiple views, formulated as:\nb j i = R(\u03b8 j v ) \u00d7 b i ,(6)\nwhere b j i is the coordinate of object i under view j. R(\u03b8) and \u03b8 j v are defined in Eq. (2) and Eq. (3). Then we obtain the d-dimensional positional encoding PE(b j i ) for object i under view j, given as:\nPE(b j i ) = LN(W b [b j i , r i ]),(7)\nwhere r i \u2208 R 1 is the box size of object i, W b is a projection matrix mapping the box information to high dimension embedding, LN(\u00b7) is layer normalization [2], and [\u00b7, \u00b7] is the concatenate operation. \no j i = x i + PE(b j i ),(8)\nwhere x i is shared by multiple views, providing visual information, e.g., categories, colors, etc. PE(\u00b7) provides the size and position information for each view.\n\n\nMulti-Modal Feature Fusion\n\nThe multi-modal features are fused by the language feature L and the object features O under multiple views. Following [14,25,33], we adopt a BERT model [9] as the language encoder. Given the query expression Q with k 1 words, we embed them into d-dimensional feature vectors L, given as:\n{l s , l 1 , . . . , l k1 } = BERT(Q),(9)\nwhere l s is the sentence-level feature, l i is the feature of i-th word. We fine-tune the BERT during training. To enhance the BERT, a text classifier with two FC layers is applied on l s , which predicts the object category described by Q, and supervised by a cross-entropy loss L text . We adopt a standard transformer decoder [29] for the multi-modal feature fusion, which is shared by different views. We fuse the multi-modal feature under view j as:\nF j = Decoder(O j , L),(10)\nwhere F j = {f j 1 , . . . , f j M } is multi-modal feature of object i under view j. As shown at the bottom of Fig. 3, in each layer of the decoder, object features O j first pass through a self-attention module [29], where interactions between objects are modeled. Then object features O j and language features L are fused through a cross-attention module [29].\n\n\nMulti-View Aggregation\n\nAfter the multi-modal feature fusion, we aggregate the multi-view information from {f 1 i , . . . , f N i }. The aggregation function should be order-independent as in Eq. (4), here we choose the average aggregation by default, given as:\ng i = N j=1 f j i N ,(11)\nwhere G = {g 1 , . . . , g M } is the final multi-modal feature of each object. Finally we obtain the grounding score by applying two FC layers on G.\n\nWe aggregate the multi-view information after the multimodal fusion by default and then make the final prediction. We can also aggregate the multi-view information at earlier stages (e.g., after object feature encoding). There are also other alternatives to the aggregation function (e.g. max, avg + max). We explore the performance of different aggregation stages and different aggregation functions in Tab. 6.\n\n\nImproving Object Encoder\n\nTo enhance the object encoder, we introduce an auxiliary task to learn the classification of 3D objects like previous works [1,25]. However, these works mainly apply a few FC layers on the object encoder to classify objects, using only the visual data but neglecting any multi-modal information in this visual grounding task. We propose the auxiliary task of language-guided classification by leveraging object category labels as the text supervision.\n\nAs shown in the top of Fig. 3, given the set of category label texts (e.g. \"armchair\", \"rocking chair\", and \"sofa chair\"), we first extract text features from category labels following Eq.(9). The d-dimensional category text features are denoted as C = {c 1 , . . . , c k2 } \u2208 R k2\u00d7d , where c i is the sentence-level feature of category i, and k 2 is the category number (e.g. 524 in Nr3D [1] and 607 in Sr3D [1]). Then classification logits p i \u2208 R k2 of object i are calculated by the inner product of category text features C and the object feature x i . We adopt a cross-entropy loss to supervise p i , denotes as L obj .\n\n\nOverall Loss Functions\n\nWe use a cross-entropy loss L ref to supervise the final grounding score, and two auxiliary losses L text and L obj mentioned above to enhance the object encoder and language encoder. The total loss is defined as:\nL total = L ref + \u03b1(L text + L obj ),(12)\nwhere \u03b1 controls the ratio of each loss term. We set \u03b1 = 0.5 by default.\n\n\nExperiments\n\n\nDatasets\n\nNr3D. The Nr3D dataset [1] annotates the indoor 3D scene dataset ScanNet [7] with 45, 503 human utterances. There are a total of 707 unique indoor scenes with target objects from 76 fine-grained classes. Each scene contains no more than six distractors (objects in the same class as the target). Two kinds of data splits are used in Referit3D. The \"easy\" and \"hard\" splits depend on whether the scene contains more than two distractors. The \"view-dep.\" and \"viewindep.\" splits depend on whether the referring expression is dependent on the speaker's view or not. Sr3D/Sr3D+. The Sr3D dataset [1] contains 83, 572 utterances that are automatically generated based on a \"targetspatial relationship-anchor object\" template. The Sr3D+ dataset further expands Sr3D by adding samples without multiple distractors, resulting in a 114, 532 utterances.\n\nScanRefer. The ScanRef dataset [3] annotates 800 indoor scenes in ScanNet [7] with 51, 583 human utterances, containing 36, 665, 9, 508, and 5, 410 samples in train/val/test sets, respectively. The data can be divided into \"Unique\" and \"Multiple\", depending on whether there are objects of the same class as the target in the scene.\n\n\nExperimental Setting\n\nEvaluation metric. For datasets in Referit3D [1] (e.g. Nr3D, Sr3D, and Sr3D+), the proposals are generated from the ground truth. The models are evaluated by the accuracy, which measures the percentage of successful matches between the predicted proposal and the ground truth proposal. For ScanRefer [3], the proposals are generated from a pretrained detector. We adopt PointGroup [16] as the detector, following InstanceRefer [36]. The Acc@mIoU is adopted as the evaluation metric, where mIoU \u2208 {0.25, 0.5}. Acc@mIoU measures the fraction of language queries whose predicted box overlaps the ground truth box with 3D intersection over the union (IoU) higher than m. Implementation details. We set the dimension d in all transformer layers as 768. We adopt the first three layers of BERT [9] as the text encoder and a four layers transformer decoder [29] for multi-modal feature fusion, which makes the number of parameters comparable to SAT [33]. The text encoder is initialized from the first three layers of BERT BASE [29]. The fusion decoder is trained from scratch. The number of views N is set to 4. Model optimization is conducted using Adam [19] optimizer with a batch size of 24. We set an initial learning rate of 0.0005 for the model, and the learning rate of the transformer layer is further multiplied by 0.1. We reduce the learning rate by a multiplicative factor of 0.65 every 10 epochs after 40 epochs for a total of 100 epochs.  Table 2. Performance on Sr3D. The improvement is calculated in comparison with the best competitor without extra assistance.\n\n\n3D Visual Grounding Results\n\nNr3D. Tab. 1 reports the grounding accuracy on Nr3D [1] dataset. Our MVT greatly surpasses all previous methods. Compared with the best competitor [25] using the same training setting, MVT outperforms the state-of-theart method by +11.2%, from 43.9% to 55.1%. Even when compared with SAT [33], which utilizes extra 2D semantics assisted its training, our MVT still outperforms it by 5.9% absolute value. Specifically, the high accuracy on the \"Easy\" split (11.3% better than LanguageRefer [25]) reflects the better visual-language alignment of our MVT. At the same time, the high accuracy on the \"View-dep\" split (12.6% better than LanguageRefer [25]) reflects the effectiveness and robustness of the multi-view representation learned by our MVT. Tab. 1 also reports the performance of training on Nr3D together with Sr3D and Sr3D+ datasets [1]. Both synthetic datasets expand the scale of the training set, therefore further improving the performance on Nr3D. As the results show, our MVT on \"Nr3D w/ Sr3D\" and \"Nr3D w/ Sr3D+\" achieves the overall grounding accuracy of 58.5% and 59.5% respectively, which significantly surpasses the state-of-the-art method [14] by 11.3% and 11.5%.\n\nSr3D. Tab. 2 shows the grounding accuracy on Sr3D [1]. The MVT achieves great improvements and is significantly better than state-of-the-art methods [14,33]. Though our approach does not use any additional 2D data, ours is still much better than SAT [33] which is with 2D semantics assisting its training. Compared with TranRefer3D [14] and SAT [33], our MVT exceeds them +7.1% and +6.6% respectively.\n\nScanRefer. Tab. 3 shows the performance on ScanRefer [3]. We follow the same settings with InstanceRefer [36]. Since the detector has completed the 3D object classification and feature extraction steps, we only test the performance of the multi-view representation. As the results show, the overall Acc@0.25 and Acc@0.5 of our   Table 4. Ablation studies of the MVT components on Nr3D.\n\nMVT with the view number of 1 is 38.33% and 31.12%, respectively. After increasing the view number to be 4, the performance is improved to 40.80% and 33.26% respectively. Inference Time. On a TITAN X (Pascal), for one data pair of a 3D scene and a language description, the average inference time of MVT with view number N = 4 is 264ms, and the inference time with N = 1 is 261ms. The increase in inference time by our multi-view modeling is slight.\n\n\nAblation studies 4.4.1 Effectiveness of each Component\n\nTo investigate the effectiveness of each component in MVT, we conduct detailed ablation studies on Nr3D [1], shown in Tab. 4. The baseline model in row (a) simply passes object features through several linear layers and then fuses object features with language features through multiplication directly. Thereby the baseline model does not model the relationship between objects. After we introduce the transformer decoder [29] to model object relationship and vision-language fusion in row (b), the performance is improved from 36.9% to 40.4%. We add random rotation augmentation R(\u03b8 aug ) to the input scene in row (c), where \u03b8 aug is randomly sampled from \u03b8 v in Eq. (3) with N = 4. The improvement in the \"Easy\" split shows that the augmentation improves the performance of the object encoder. However, the rotation augmentation cannot eliminate the dependence on the specific views, and the performance in the \"View-dep\" split degraded from 38.4% to 35.2%. Row (d) and (e) bear out the effectiveness of the proposed language-guided supervision and multi-view modeling, respectively. They improve the performance to 46.2% and 52.3% respectively. The improvement of language-guided supervision comes from better vision-language alignments. The giant jump of the \"View-dep.\" split from 35.2% to 49.1% shows that our proposed multiview space modeling effectively learns a view-robust representation, eliminating the dependence on specific views. Adopting both language-guided supervision and multi-view encoding in (g) can improve the overall accuracy to 55.1%, surpassing all previous methods. We also compare the performance w/ and w/o rotation augmentation after adding the multi-view modeling. Comparing the performance in row (f) and row (g), we find that after adding the multi-view modeling, the rotation augmentation can enhance the object encoder and cause no degradation in the \"View-dep.\" split.  testing, the grounding accuracy can still be improved. This observation tells that the information aggregation from the multi-view space can be helpful. We also compare the results of training with other view numbers. When training with only 2 views, the model's performance can achieve 51.9%. When two views are relatively close, the position information provided by them for visual grounding is similar and redundant, thus we found 4-view can already provide a robust multi-view representation. When training with more views (e.g., N = 8 views), redundant views cannot improve performance continually and may cause a little performance degradation due to low training efficiency.  Table 6. Ablation of multi-view aggregation on Nr3D.\n\nMulti-view aggregation. Tab. 6 shows performances of different multi-view aggregation settings. We first adopt average aggregation and test different aggregation stages. The positional encoding is just a linear mapping with LayerNorm [2]. If we aggregate after the positional encoding generation (PE), it is equivalent to averaging the coordinates of multiple views. It performs badly since the position information is destroyed. To aggregate after object feature generation (+x), we first pass the object features into a 2-layers transformer encoder [29] and then fuse object features from different views. It keeps the position information but still can not achieve the best performance.\n\nAggregation after multi-modal feature fusion ( * L) achieves the best performance. We also test different aggregation functions. \"Avg\" is more effective than \"Max\", and \"Avg + Max\" achieves similar performance as \"Avg\". Visualization. We show the visualization results in Fig. 4. \"No MVT\" is the baseline model same as in Tab. 4 (b), \"MVT w/ 1-view test\" is the MVT model but test under 1-view, \"MVT\" is our proposed model. From (a, d, f), we can see that the baseline model's object recognition ability is relatively poor, and it is easy to predict objects of the wrong categories. By learning a view-robust representation, MVT can effectively learn the position correspondence between 3D scenes and language queries, thus predicting correctly in (b, c). We can see from (d, e) that information aggregation from the multi-view space can achieve better performance. We also show the failure cases of MVT in (f). When faced with complicated language queries and spatial relationships, MVT may also predict objects of the wrong categories or in the wrong positions.\n\n\nConclusion\n\nIn this paper, we propose a Multi-View Transformer (MVT) for 3D visual grounding. In MVT, the given 3D scene is projected to a multi-view space, in which the position information of the 3D scene under different views are modeled simultaneously. Based on it, MVT learns a more robust multi-modal representation for 3D visual grounding, which eliminates the dependence on specific views. Extensive experiments show that our MVT outperforms all state-of-the-art methods by a large margin. Detailed ablation studies show the effectiveness of all components in our model.\n\n\nObject Feature Generation. We obtain final object features O j = {o j 1 , . . . , o j M } under view j by adding the positional encoding PE with the point cloud features x, given as:\n\nFigure 4 .\n4The visualization results. The green/red/blue colors indicate the correct/incorrect/ground truth boxes. Best viewed in color.\n\n\nThis decoupling can significantly save the computational cost since different views can share the same point cloud features outputted from the first step. Finally, the object features can be calculated by simply combining both. Point Cloud Features. We sample 1024 points of point clouds for each object i in the form of RGB-XYZ, denoted as pc i \u2208 R 1024\u00d76 . Then point cloud features {x 1 , . . . , x M }Point \nClouds \n\nObject \nCoordinates \n\n\u2026 \n\u2026 \n\n: \"Facing the bed, \n\nthe pillow on the far right.\" \n\n: [chair, \n\narmchair, sofa chair, \u2026] \n\n\u2026 \n\nMulti-View Space \n\nObject Feature Encoding \n\nView 1 \nView 2 \n\nView N \n\n\u2026 \n\nMulti-Modal Feature Fusion \n\nLanguage Feature Encoding \n\nTransformer Decoder \n\n\u00d7 \n\n\u2026 \n\u2026 \n\u2026 \n\u2026 \n\n\u2112 \n\n\u2112 \n\n\u2112 \n\n[ , . . . , \n] \n\n[ , , . . . , ] \n[ , . . . , ] \n\n\n\n\nTable 1. Performance on Nr3D trained with or without Sr3D/Sr3D+. The improvement is calculated in comparison with the best competitor without extra assistance.Arch. \n\nExtra \nOverall \nEasy \nHard \nView-dep. \nView-indep. \n\nNr3D \nReferIt3D [1] \nNone \n35.6%\u00b10.7% \n43.6%\u00b10.8% \n27.9%\u00b10.7% \n32.5%\u00b10.7% \n37.1%\u00b10.8% \nInstanceRefer [36] \nNone \n38.8%\u00b10.4% \n46.0%\u00b10.5% \n31.8%\u00b10.4% \n34.5%\u00b10.6% \n41.9%\u00b10.4% \n3DVG-Transformer [38] \nNone \n40.8% \u00b1 0.2% 48.5% \u00b1 0.2% 34.8% \u00b1 0.4% 34.8% \u00b1 0.7% 43.7% \u00b1 0.5% \nFFL-3DOG [10] \nNone \n41.7% \n48.2% \n35.0% \n37.1% \n44.7% \nTransRefer3D [14] \nNone \n42.1%\u00b10.2% \n48.5%\u00b10.2% \n36.0%\u00b10.4% \n36.5%\u00b10.6% \n44.9%\u00b10.3% \nLanguageRefer [25] \nNone \n43.9% \n51.0% \n36.6% \n41.7% \n45.0% \nSAT [33] \n2D assist. \n49.2%\u00b10.3% \n56.3%\u00b10.5% \n42.4%\u00b10.4% \n46.9%\u00b10.3% \n50.4%\u00b10.3% \n\nOurs \nNone \n55.1%\u00b10.3% \n(+11.2%) \n\n61.3%\u00b10.4% \n(+11.3%) \n\n49.1%\u00b10.4% \n(+12.5%) \n\n54.3%\u00b10.5% \n(+12.6%) \n\n55.4%\u00b10.3% \n(+10.4%) \n\nNr3D w/ Sr3D \nReferIt3D [1] \nNone \n37.2%\u00b10.3% \n44.0%\u00b10.6% \n30.6%\u00b10.3% \n33.3%\u00b10.6% \n39.1%\u00b10.2% \nnon-SAT [33] \nNone \n43.9%\u00b10.3% \n-\n-\n-\n-\nTransRefer3D [14] \nNone \n47.2%\u00b10.3% \n55.4%\u00b10.5% \n39.3%\u00b10.5% \n40.3%\u00b10.4% \n50.6%\u00b10.2% \nSAT [33] \n2D assist. \n53.9%\u00b10.2% \n61.5%\u00b10.1% \n46.7%\u00b10.3% \n52.7%\u00b10.7% \n54.5%\u00b10.3% \n\nOurs \nNone \n58.5%\u00b10.2% \n(+11.3%) \n\n65.6%\u00b10.2% \n(+10.2%) \n\n51.6%\u00b10.3% \n(+12.3%) \n\n56.6%\u00b10.3% \n(+16.3%) \n\n59.4%\u00b10.2% \n(+8.8%) \n\nNr3D w/ Sr3D+ \nReferIt3D [1] \nNone \n37.6%\u00b10.4% \n45.4%\u00b10.6% \n30.0%\u00b10.4% \n33.1%\u00b10.5% \n39.8%\u00b10.4% \nnon-SAT [33] \nNone \n45.9%\u00b10.2% \n-\n-\n-\n-\nTransRefer3D [14] \nNone \n48.0%\u00b10.2% \n56.7%\u00b10.4% \n39.6%\u00b10.2% \n42.5%\u00b10.2% \n50.7%\u00b10.4% \nSAT [33] \n2D assist. \n56.5%\u00b10.1% \n64.9%\u00b10.2% \n48.4%\u00b10.1% \n54.4%\u00b10.3% \n57.6%\u00b10.1% \n\nOurs \nNone \n59.5%\u00b10.2% \n(+11.5%) \n\n67.4%\u00b10.4% \n(+10.7%) \n\n52.7%\u00b10.4% \n(+13.1%) \n\n59.1%\u00b10.5% \n(+16.6%) \n\n60.3%\u00b10.1% \n(+9.6%) \n\nArch. \nExtra \nOverall \nEasy \nHard \nView-dep. \nView-indep. \n\nReferIt3D [1] \nNone \n40.8%\u00b10.2% \n44.7%\u00b10.1% \n31.5%\u00b10.4% \n39.2%\u00b11.0% \n40.8%\u00b10.1% \nInstanceRefer [36] \nNone \n48.0%\u00b10.3% \n51.1%\u00b10.2% \n40.5%\u00b10.3% \n45.4%\u00b10.9% \n48.1%\u00b10.3% \n3DVG-Transformer [38] \nNone \n51.4% \u00b1 0.1% 54.2% \u00b1 0.1% 44.9% \u00b1 0.5% 44.6% \u00b1 0.3% 51.7% \u00b1 0.1% \nLanguageRefer [25] \nNone \n56.0% \n58.9% \n49.3% \n49.2% \n56.3% \nTransRefer3D [14] \nNone \n57.4%\u00b10.2% \n60.5%\u00b10.2% \n50.2%\u00b10.2% \n49.9%\u00b10.6% \n57.7%\u00b10.2% \nSAT  \u2020 [33] \n2D assist. \n57.9% \n61.2% \n50.0% \n49.2% \n58.3% \n\nOurs \nNone \n64.5%\u00b10.1% \n(+7.1%) \n\n66.9%\u00b10.1% \n(+6.4%) \n\n58.8%\u00b10.1% \n(+8.6%) \n\n58.4%\u00b10.8% \n(+8.5%) \n\n64.7%\u00b10.1% \n(+7.0%) \n\n\n\nTable 3 .\n3Performance on ScanRefer compared with previous works.Decoder Aug. Lang-Sup. Multi-View \nOverall \nEasy \nHard \nView-dep. View-indep. \n\n(a) \n36.9% \n43.5% 30.5% \n32.9% \n38.8% \n(b) \n\u221a \n40.4% \n47.5% 33.7% \n38.4% \n41.4% \n(c) \n\u221a \n\u221a \n40.8% \n48.4% 33.5% \n35.2%\u2193 \n43.6% \n(d) \n\u221a \n\u221a \n\u221a \n46.2% \n53.8% 38.9% \n42.6% \n48.0% \n(e) \n\u221a \n\u221a \n\u221a \n52.3% \n59.1% 45.7% \n50.3%\u2191 \n53.3% \n(f) \n\u221a \n\u221a \n\u221a \n53.3% \n59.8% 47.0% \n54.4% \n52.7% \n(g) \n\u221a \n\u221a \n\u221a \n\u221a \n55.1% \n61.3% 49.1% \n54.3%\u2248 \n55.4% \n\n\n\nTable 5 .\n5Ablation of view numbers on Nr3D.4.4.2 Effectiveness of Multi-View Modeling.Number of views. Tab. 5 shows the performance with different numbers of view on Nr3D[1]. We first compare the performance of 4-view training but test in different numbers of views, which shows that the improvement of MVT comes from two parts. Firstly, when training with 4 views but testing with 1 view, the overall accuracy 51.6% is much higher than training in 1 view only, i.e., 46.2%, and the \"View-dep.\" accuracy also increases by 8.1%. This result shows that multi-view modeling can learn an effective representation that can benefit different views. Secondly, when increasing the view number duringView Number Overall \nEasy \nHard \nView-dep. View-indep. \nTrain \nTest \n\n4 \n1 \n51.6% \n58.4% 45.0% \n50.7% \n52.0% \n4 \n2 \n54.3% \n60.6% 48.3% \n53.7% \n54.7% \n4 \n4 \n55.1% \n61.3% 49.1% \n54.3% \n55.4% \n4 \n8 \n54.8% \n61.1% 48.8% \n54.2% \n55.1% \n1 \n1 \n46.2% \n53.8% 38.9% \n42.6% \n48.0% \n2 \n2 \n51.9% \n60.2% 43.9% \n50.7% \n52.5% \n8 \n8 \n53.4% \n60.9% 46.2% \n53.5% \n53.4% \n\n\n\n\nAggregation OverallEasy Hard View-dep. View-indep.After PE \n42.5% \n49.5% 35.8% \n37.4% \n45.0% \nAfter +x \n47.4% \n54.2% 41.0% \n44.3% \n49.0% \nAfter  * L \n55.1% \n61.3% 49.1% \n54.3% \n55.4% \n\nAvg \n55.1% \n61.3% 49.1% \n54.3% \n55.4% \nMax \n52.7% \n60.0% 45.7% \n51.7% \n53.2% \nAvg + Max \n55.2% \n62.1% 48.5% \n54.4% \n55.6% \n\n\n\nReferit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas Guibas, ECCV. 67Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In ECCV, 2020. 1, 2, 3, 5, 6, 7\n\n. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, 4Layer normalization. arXivJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv, 2016. 4, 8\n\nScanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu Chen, X Angel, Matthias Chang, Nie\u00dfner, ECCV. 67Dave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. Scanrefer: 3d object localization in rgb-d scans using natural language. In ECCV, 2020. 1, 2, 3, 5, 6, 7\n\nMicrosoft coco captions: Data collection and evaluation server. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, C Lawrence Zitnick, arXivXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan- tam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv, 2015. 2\n\nMulti-view 3d object detection network for autonomous driving. Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia, CVPR. Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In CVPR, 2017. 3\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, NIPS. 2Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. NIPS, 2014. 2\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Nie\u00dfner, CVPR. 25Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 2, 5\n\nVirtex: Learning visual representations from textual annotations. Karan Desai, Justin Johnson, CVPR. Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In CVPR, 2021. 2\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL. 47Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In NAACL, 2019. 4, 5, 7\n\nFree-form description guided 3d visual graph network for object grounding in point cloud. Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, Xiangdong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, Ajmal Mian, 26arXivMingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Ajmal Mian. Free-form description guided 3d visual graph network for object grounding in point cloud. arXiv, 2021. 2, 6\n\nCityflow-nl: Tracking and retrieval of vehicles at city scale by natural language descriptions. arXiv. Qi Feng, Vitaly Ablavsky, Stan Sclaroff, Qi Feng, Vitaly Ablavsky, and Stan Sclaroff. Cityflow-nl: Tracking and retrieval of vehicles at city scale by natural language descriptions. arXiv, 2021. 1\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. Andreas Geiger, Philip Lenz, Raquel Urtasun, CVPR. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012. 3\n\nSelf-supervised learning of visual features through embedding images into text topic spaces. Lluis Gomez, Yash Patel, Mar\u00e7al Rusi\u00f1ol, Dimosthenis Karatzas, C V Jawahar, CVPR. Lluis Gomez, Yash Patel, Mar\u00e7al Rusi\u00f1ol, Dimosthenis Karatzas, and CV Jawahar. Self-supervised learning of visual features through embedding images into text topic spaces. In CVPR, 2017. 2\n\nTransrefer3d: Entityand-relation aware transformer for fine-grained 3d visual grounding. Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, Si Liu, Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on Multimedia6Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei Huang, Aixi Zhang, and Si Liu. Transrefer3d: Entity- and-relation aware transformer for fine-grained 3d visual grounding. In Proceedings of the 29th ACM International Conference on Multimedia, 2021. 1, 2, 4, 6\n\nText-guided graph neural networks for referring 3d instance segmentation. Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, Tyng-Luh Liu, AAAI. Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neural networks for referring 3d instance segmentation. In AAAI, 2021. 7\n\nPointgroup: Dual-set point grouping for 3d instance segmentation. Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia, CVPR, 2020. 25Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi- Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3d instance segmentation. In CVPR, 2020. 2, 5\n\nLearning visual features from large weakly supervised data. Armand Joulin, Laurens Van Der Maaten, Allan Jabri, Nicolas Vasilache, ECCV. Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from large weakly supervised data. In ECCV, 2016. 2\n\nReferitgame: Referring to objects in photographs of natural scenes. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, EMNLP. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in pho- tographs of natural scenes. In EMNLP, 2014. 1\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5\n\nLearning to assemble neural module tree networks for visual grounding. Daqing Liu, Hanwang Zhang, Feng Wu, Zheng-Jun Zha, ICCV. Daqing Liu, Hanwang Zhang, Feng Wu, and Zheng-Jun Zha. Learning to assemble neural module tree networks for visual grounding. In ICCV, 2019. 1\n\nAttngrounder: Talking to cars with attention. Vivek Mittal, ECCV. Vivek Mittal. Attngrounder: Talking to cars with attention. In ECCV, 2020. 1\n\nFlickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. A Bryan, Liwei Plummer, Chris M Wang, Juan C Cervantes, Julia Caicedo, Svetlana Hockenmaier, Lazebnik, ICCV. Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspon- dences for richer image-to-sentence models. In ICCV, 2015. 1\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. Li Charles R Qi, Hao Yi, Leonidas J Su, Guibas, 2017. 4Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv, 2017. 4\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, arXivAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. arXiv, 2021. 2\n\nLanguagerefer: Spatial-language model for 3d visual grounding. Junha Roh, Karthik Desingh, Ali Farhadi, Dieter Fox, CoRL. 56Junha Roh, Karthik Desingh, Ali Farhadi, and Dieter Fox. Languagerefer: Spatial-language model for 3d visual grounding. In CoRL, 2021. 1, 2, 4, 5, 6\n\nHabitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, ICCV. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied ai research. In ICCV, 2019. 1\n\nThe graph neural network model. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, Gabriele Monfardini, IEEE transactions on neural networks. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 2008. 2\n\nEvangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. Hang Su, Subhransu Maji, ICCV. Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In ICCV, 2015. 3\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, NIPS. 7Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. 2, 4, 5, 7, 8\n\nImproving weakly supervised visual grounding by contrastive knowledge distillation. Liwei Wang, Jing Huang, Yin Li, Kun Xu, Zhengyuan Yang, Dong Yu, CVPR. Liwei Wang, Jing Huang, Yin Li, Kun Xu, Zhengyuan Yang, and Dong Yu. Improving weakly supervised visual grounding by contrastive knowledge distillation. In CVPR, 2021. 1\n\nReinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang, CVPR. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self- supervised imitation learning for vision-language navigation. In CVPR, 2019. 1\n\nGibson env: Real-world perception for embodied agents. Fei Xia, Zhiyang Amir R Zamir, Alexander He, Jitendra Sax, Silvio Malik, Savarese, CVPR. Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In CVPR, 2018. 1\n\nSat: 2d semantics assisted training for 3d visual grounding. Zhengyuan Yang, Songyang Zhang, Liwei Wang, Jiebo Luo, ICCV. 56Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Luo. Sat: 2d semantics assisted training for 3d visual grounding. In ICCV, 2021. 1, 2, 4, 5, 6\n\nMattnet: Modular attention network for referring expression comprehension. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L Berg, CVPR. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In CVPR, 2018. 1\n\nModeling context in referring expressions. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, Tamara L Berg, ECCV. Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In ECCV, 2016. 1\n\nInstancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring. Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, Shuguang Cui, ICCV. 67Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, and Shuguang Cui. Instancerefer: Cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual refer- ring. In ICCV, 2021. 1, 2, 5, 6, 7\n\nYuhao Zhang, Hang Jiang, Yasuhide Miura, D Christopher, Curtis P Manning, Langlotz, Contrastive learning of medical visual representations from paired images and text. arXiv. Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. Contrastive learning of medical visual representations from paired images and text. arXiv, 2020. 2\n\n3dvg-transformer: Relation modeling for visual grounding on point clouds. Lichen Zhao, Daigang Cai, Lu Sheng, Dong Xu, ICCV, 2021. 26Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg- transformer: Relation modeling for visual grounding on point clouds. In ICCV, 2021. 2, 6\n\nVision-language navigation with self-supervised auxiliary reasoning tasks. Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang, CVPR. 2020Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. In CVPR, 2020. 1\n", "annotations": {"author": "[{\"end\":125,\"start\":50},{\"end\":198,\"start\":126},{\"end\":247,\"start\":199},{\"end\":320,\"start\":248}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":57},{\"end\":136,\"start\":132},{\"end\":208,\"start\":205},{\"end\":258,\"start\":254}]", "author_first_name": "[{\"end\":56,\"start\":50},{\"end\":131,\"start\":126},{\"end\":204,\"start\":199},{\"end\":253,\"start\":248}]", "author_affiliation": "[{\"end\":124,\"start\":88},{\"end\":197,\"start\":161},{\"end\":246,\"start\":210},{\"end\":319,\"start\":283}]", "title": "[{\"end\":47,\"start\":1},{\"end\":367,\"start\":321}]", "venue": null, "abstract": "[{\"end\":1443,\"start\":369}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1605,\"start\":1601},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1608,\"start\":1605},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1611,\"start\":1608},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1614,\"start\":1611},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1805,\"start\":1801},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1808,\"start\":1805},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1833,\"start\":1829},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1836,\"start\":1833},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1866,\"start\":1862},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1869,\"start\":1866},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2094,\"start\":2091},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2096,\"start\":2094},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2099,\"start\":2096},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2102,\"start\":2099},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2105,\"start\":2102},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2108,\"start\":2105},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2340,\"start\":2336},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2343,\"start\":2340},{\"end\":2442,\"start\":2434},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2468,\"start\":2465},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3871,\"start\":3868},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4578,\"start\":4575},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4581,\"start\":4578},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4584,\"start\":4581},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4587,\"start\":4584},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4590,\"start\":4587},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4593,\"start\":4590},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4596,\"start\":4593},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5718,\"start\":5714},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6945,\"start\":6942},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6958,\"start\":6955},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7187,\"start\":7184},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7197,\"start\":7194},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7216,\"start\":7213},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7427,\"start\":7423},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7620,\"start\":7617},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7638,\"start\":7635},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7754,\"start\":7751},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7780,\"start\":7777},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7782,\"start\":7780},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7785,\"start\":7782},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7788,\"start\":7785},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7791,\"start\":7788},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7794,\"start\":7791},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7936,\"start\":7933},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7967,\"start\":7964},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7970,\"start\":7967},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8172,\"start\":8169},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8185,\"start\":8182},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8237,\"start\":8233},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8321,\"start\":8317},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8340,\"start\":8336},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8343,\"start\":8340},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8346,\"start\":8343},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8447,\"start\":8443},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8555,\"start\":8551},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8696,\"start\":8692},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8856,\"start\":8852},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9017,\"start\":9013},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9137,\"start\":9133},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9528,\"start\":9525},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9531,\"start\":9528},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9534,\"start\":9531},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9537,\"start\":9534},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9540,\"start\":9537},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9690,\"start\":9687},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9759,\"start\":9756},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9860,\"start\":9856},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10331,\"start\":10328},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10476,\"start\":10472},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10488,\"start\":10484},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13229,\"start\":13226},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13231,\"start\":13229},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13389,\"start\":13386},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13446,\"start\":13443},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14031,\"start\":14027},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14234,\"start\":14230},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14422,\"start\":14419},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15089,\"start\":15086},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15479,\"start\":15475},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15482,\"start\":15479},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15485,\"start\":15482},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15512,\"start\":15509},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16021,\"start\":16017},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16388,\"start\":16384},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16534,\"start\":16530},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17544,\"start\":17541},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17547,\"start\":17544},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18263,\"start\":18260},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18283,\"start\":18280},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18904,\"start\":18901},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18954,\"start\":18951},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19473,\"start\":19470},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19757,\"start\":19754},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19800,\"start\":19797},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20383,\"start\":20380},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20465,\"start\":20461},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20511,\"start\":20507},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20871,\"start\":20868},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20934,\"start\":20930},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21026,\"start\":21022},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21105,\"start\":21101},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21233,\"start\":21229},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21737,\"start\":21734},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21833,\"start\":21829},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21974,\"start\":21970},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22175,\"start\":22171},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22332,\"start\":22328},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22526,\"start\":22523},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22845,\"start\":22841},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22920,\"start\":22917},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23020,\"start\":23016},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23023,\"start\":23020},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23121,\"start\":23117},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23203,\"start\":23199},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23216,\"start\":23212},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23326,\"start\":23323},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23379,\"start\":23375},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24272,\"start\":24269},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24591,\"start\":24587},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24837,\"start\":24834},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27047,\"start\":27044},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27365,\"start\":27361},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33312,\"start\":33309}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29330,\"start\":29146},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29469,\"start\":29331},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30251,\"start\":29470},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32664,\"start\":30252},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33136,\"start\":32665},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34181,\"start\":33137},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34493,\"start\":34182}]", "paragraph": "[{\"end\":2076,\"start\":1459},{\"end\":2409,\"start\":2078},{\"end\":2682,\"start\":2424},{\"end\":2893,\"start\":2684},{\"end\":4556,\"start\":2895},{\"end\":6137,\"start\":4558},{\"end\":7470,\"start\":6139},{\"end\":10852,\"start\":7487},{\"end\":11233,\"start\":10863},{\"end\":11539,\"start\":11268},{\"end\":11855,\"start\":11621},{\"end\":12144,\"start\":11902},{\"end\":12432,\"start\":12223},{\"end\":12513,\"start\":12434},{\"end\":13170,\"start\":12565},{\"end\":14159,\"start\":13198},{\"end\":14273,\"start\":14161},{\"end\":14423,\"start\":14312},{\"end\":14650,\"start\":14425},{\"end\":14887,\"start\":14680},{\"end\":15132,\"start\":14928},{\"end\":15325,\"start\":15162},{\"end\":15644,\"start\":15356},{\"end\":16142,\"start\":15687},{\"end\":16535,\"start\":16171},{\"end\":16799,\"start\":16562},{\"end\":16975,\"start\":16826},{\"end\":17388,\"start\":16977},{\"end\":17868,\"start\":17417},{\"end\":18496,\"start\":17870},{\"end\":18736,\"start\":18523},{\"end\":18851,\"start\":18779},{\"end\":19721,\"start\":18878},{\"end\":20055,\"start\":19723},{\"end\":21650,\"start\":20080},{\"end\":22865,\"start\":21682},{\"end\":23268,\"start\":22867},{\"end\":23655,\"start\":23270},{\"end\":24106,\"start\":23657},{\"end\":26808,\"start\":24165},{\"end\":27499,\"start\":26810},{\"end\":28564,\"start\":27501},{\"end\":29145,\"start\":28579}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11563,\"start\":11540},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11620,\"start\":11563},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11901,\"start\":11856},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12222,\"start\":12145},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12564,\"start\":12514},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14311,\"start\":14274},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14679,\"start\":14651},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14927,\"start\":14888},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15161,\"start\":15133},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15686,\"start\":15645},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16170,\"start\":16143},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16825,\"start\":16800},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18778,\"start\":18737}]", "table_ref": "[{\"end\":21533,\"start\":21526},{\"end\":23606,\"start\":23599},{\"end\":26763,\"start\":26756}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1457,\"start\":1445},{\"end\":2422,\"start\":2412},{\"attributes\":{\"n\":\"2.\"},\"end\":7485,\"start\":7473},{\"attributes\":{\"n\":\"3.\"},\"end\":10861,\"start\":10855},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11266,\"start\":11236},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13196,\"start\":13173},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15354,\"start\":15328},{\"attributes\":{\"n\":\"3.4.\"},\"end\":16560,\"start\":16538},{\"attributes\":{\"n\":\"3.5.\"},\"end\":17415,\"start\":17391},{\"attributes\":{\"n\":\"3.6.\"},\"end\":18521,\"start\":18499},{\"attributes\":{\"n\":\"4.\"},\"end\":18865,\"start\":18854},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18876,\"start\":18868},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20078,\"start\":20058},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21680,\"start\":21653},{\"attributes\":{\"n\":\"4.4.\"},\"end\":24163,\"start\":24109},{\"attributes\":{\"n\":\"5.\"},\"end\":28577,\"start\":28567},{\"end\":29342,\"start\":29332},{\"end\":32675,\"start\":32666},{\"end\":33147,\"start\":33138}]", "table": "[{\"end\":30251,\"start\":29877},{\"end\":32664,\"start\":30413},{\"end\":33136,\"start\":32731},{\"end\":34181,\"start\":33830},{\"end\":34493,\"start\":34234}]", "figure_caption": "[{\"end\":29330,\"start\":29148},{\"end\":29469,\"start\":29344},{\"end\":29877,\"start\":29472},{\"end\":30413,\"start\":30254},{\"end\":32731,\"start\":32677},{\"end\":33830,\"start\":33149},{\"end\":34234,\"start\":34184}]", "figure_ref": "[{\"end\":3459,\"start\":3453},{\"end\":4134,\"start\":4128},{\"end\":10966,\"start\":10960},{\"end\":11978,\"start\":11972},{\"end\":13776,\"start\":13768},{\"end\":16289,\"start\":16283},{\"end\":17899,\"start\":17893},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27779,\"start\":27773}]", "bib_author_first_name": "[{\"end\":34592,\"start\":34587},{\"end\":34610,\"start\":34605},{\"end\":34627,\"start\":34624},{\"end\":34640,\"start\":34633},{\"end\":34660,\"start\":34652},{\"end\":34895,\"start\":34890},{\"end\":34899,\"start\":34896},{\"end\":34909,\"start\":34904},{\"end\":34914,\"start\":34910},{\"end\":34930,\"start\":34922},{\"end\":34932,\"start\":34931},{\"end\":35147,\"start\":35136},{\"end\":35155,\"start\":35154},{\"end\":35171,\"start\":35163},{\"end\":35427,\"start\":35421},{\"end\":35437,\"start\":35434},{\"end\":35452,\"start\":35444},{\"end\":35469,\"start\":35458},{\"end\":35487,\"start\":35480},{\"end\":35500,\"start\":35495},{\"end\":35519,\"start\":35509},{\"end\":35798,\"start\":35791},{\"end\":35811,\"start\":35805},{\"end\":35818,\"start\":35816},{\"end\":35826,\"start\":35824},{\"end\":35835,\"start\":35831},{\"end\":36068,\"start\":36060},{\"end\":36082,\"start\":36076},{\"end\":36102,\"start\":36093},{\"end\":36114,\"start\":36108},{\"end\":36359,\"start\":36353},{\"end\":36366,\"start\":36365},{\"end\":36381,\"start\":36374},{\"end\":36395,\"start\":36389},{\"end\":36409,\"start\":36403},{\"end\":36426,\"start\":36418},{\"end\":36711,\"start\":36706},{\"end\":36725,\"start\":36719},{\"end\":36944,\"start\":36939},{\"end\":36961,\"start\":36953},{\"end\":36975,\"start\":36969},{\"end\":36989,\"start\":36981},{\"end\":37282,\"start\":37275},{\"end\":37293,\"start\":37289},{\"end\":37300,\"start\":37298},{\"end\":37310,\"start\":37305},{\"end\":37327,\"start\":37318},{\"end\":37344,\"start\":37335},{\"end\":37353,\"start\":37350},{\"end\":37367,\"start\":37361},{\"end\":37379,\"start\":37374},{\"end\":37722,\"start\":37720},{\"end\":37735,\"start\":37729},{\"end\":37750,\"start\":37746},{\"end\":37996,\"start\":37989},{\"end\":38011,\"start\":38005},{\"end\":38024,\"start\":38018},{\"end\":38276,\"start\":38271},{\"end\":38288,\"start\":38284},{\"end\":38302,\"start\":38296},{\"end\":38323,\"start\":38312},{\"end\":38335,\"start\":38334},{\"end\":38337,\"start\":38336},{\"end\":38638,\"start\":38632},{\"end\":38650,\"start\":38643},{\"end\":38662,\"start\":38657},{\"end\":38675,\"start\":38668},{\"end\":38688,\"start\":38681},{\"end\":38700,\"start\":38696},{\"end\":38710,\"start\":38708},{\"end\":39185,\"start\":39178},{\"end\":39201,\"start\":39193},{\"end\":39218,\"start\":39207},{\"end\":39233,\"start\":39225},{\"end\":39470,\"start\":39468},{\"end\":39488,\"start\":39478},{\"end\":39504,\"start\":39495},{\"end\":39513,\"start\":39510},{\"end\":39527,\"start\":39519},{\"end\":39537,\"start\":39532},{\"end\":39790,\"start\":39784},{\"end\":39806,\"start\":39799},{\"end\":39828,\"start\":39823},{\"end\":39843,\"start\":39836},{\"end\":40087,\"start\":40082},{\"end\":40107,\"start\":40100},{\"end\":40121,\"start\":40117},{\"end\":40136,\"start\":40130},{\"end\":40349,\"start\":40348},{\"end\":40365,\"start\":40360},{\"end\":40555,\"start\":40549},{\"end\":40568,\"start\":40561},{\"end\":40580,\"start\":40576},{\"end\":40594,\"start\":40585},{\"end\":40801,\"start\":40796},{\"end\":40996,\"start\":40995},{\"end\":41009,\"start\":41004},{\"end\":41024,\"start\":41019},{\"end\":41026,\"start\":41025},{\"end\":41037,\"start\":41033},{\"end\":41039,\"start\":41038},{\"end\":41056,\"start\":41051},{\"end\":41074,\"start\":41066},{\"end\":41413,\"start\":41411},{\"end\":41431,\"start\":41428},{\"end\":41444,\"start\":41436},{\"end\":41446,\"start\":41445},{\"end\":41689,\"start\":41685},{\"end\":41703,\"start\":41699},{\"end\":41708,\"start\":41704},{\"end\":41719,\"start\":41714},{\"end\":41735,\"start\":41729},{\"end\":41751,\"start\":41744},{\"end\":41765,\"start\":41757},{\"end\":41781,\"start\":41775},{\"end\":41796,\"start\":41790},{\"end\":41811,\"start\":41805},{\"end\":41825,\"start\":41821},{\"end\":42150,\"start\":42145},{\"end\":42163,\"start\":42156},{\"end\":42176,\"start\":42173},{\"end\":42192,\"start\":42186},{\"end\":42409,\"start\":42402},{\"end\":42425,\"start\":42417},{\"end\":42443,\"start\":42434},{\"end\":42459,\"start\":42455},{\"end\":42470,\"start\":42466},{\"end\":42487,\"start\":42480},{\"end\":42500,\"start\":42494},{\"end\":42512,\"start\":42509},{\"end\":42525,\"start\":42518},{\"end\":42542,\"start\":42534},{\"end\":42813,\"start\":42807},{\"end\":42830,\"start\":42825},{\"end\":42839,\"start\":42837},{\"end\":42858,\"start\":42852},{\"end\":42881,\"start\":42873},{\"end\":43221,\"start\":43217},{\"end\":43235,\"start\":43226},{\"end\":43439,\"start\":43433},{\"end\":43453,\"start\":43449},{\"end\":43467,\"start\":43463},{\"end\":43481,\"start\":43476},{\"end\":43498,\"start\":43493},{\"end\":43511,\"start\":43506},{\"end\":43513,\"start\":43512},{\"end\":43527,\"start\":43521},{\"end\":43541,\"start\":43536},{\"end\":43834,\"start\":43829},{\"end\":43845,\"start\":43841},{\"end\":43856,\"start\":43853},{\"end\":43864,\"start\":43861},{\"end\":43878,\"start\":43869},{\"end\":43889,\"start\":43885},{\"end\":44176,\"start\":44173},{\"end\":44190,\"start\":44183},{\"end\":44202,\"start\":44198},{\"end\":44224,\"start\":44216},{\"end\":44237,\"start\":44230},{\"end\":44253,\"start\":44244},{\"end\":44267,\"start\":44260},{\"end\":44272,\"start\":44268},{\"end\":44282,\"start\":44279},{\"end\":44597,\"start\":44594},{\"end\":44610,\"start\":44603},{\"end\":44634,\"start\":44625},{\"end\":44647,\"start\":44639},{\"end\":44659,\"start\":44653},{\"end\":44913,\"start\":44904},{\"end\":44928,\"start\":44920},{\"end\":44941,\"start\":44936},{\"end\":44953,\"start\":44948},{\"end\":45199,\"start\":45192},{\"end\":45207,\"start\":45204},{\"end\":45220,\"start\":45213},{\"end\":45232,\"start\":45227},{\"end\":45242,\"start\":45239},{\"end\":45252,\"start\":45247},{\"end\":45267,\"start\":45261},{\"end\":45269,\"start\":45268},{\"end\":45513,\"start\":45506},{\"end\":45525,\"start\":45518},{\"end\":45539,\"start\":45535},{\"end\":45555,\"start\":45546},{\"end\":45557,\"start\":45556},{\"end\":45570,\"start\":45564},{\"end\":45572,\"start\":45571},{\"end\":45867,\"start\":45861},{\"end\":45876,\"start\":45874},{\"end\":45890,\"start\":45882},{\"end\":45903,\"start\":45897},{\"end\":45916,\"start\":45911},{\"end\":45927,\"start\":45923},{\"end\":45940,\"start\":45932},{\"end\":46218,\"start\":46213},{\"end\":46230,\"start\":46226},{\"end\":46246,\"start\":46238},{\"end\":46255,\"start\":46254},{\"end\":46275,\"start\":46269},{\"end\":46277,\"start\":46276},{\"end\":46655,\"start\":46649},{\"end\":46669,\"start\":46662},{\"end\":46677,\"start\":46675},{\"end\":46689,\"start\":46685},{\"end\":46934,\"start\":46928},{\"end\":46942,\"start\":46940},{\"end\":46955,\"start\":46948},{\"end\":46970,\"start\":46963}]", "bib_author_last_name": "[{\"end\":34603,\"start\":34593},{\"end\":34622,\"start\":34611},{\"end\":34631,\"start\":34628},{\"end\":34650,\"start\":34641},{\"end\":34667,\"start\":34661},{\"end\":34902,\"start\":34900},{\"end\":34920,\"start\":34915},{\"end\":34939,\"start\":34933},{\"end\":35152,\"start\":35148},{\"end\":35161,\"start\":35156},{\"end\":35177,\"start\":35172},{\"end\":35186,\"start\":35179},{\"end\":35432,\"start\":35428},{\"end\":35442,\"start\":35438},{\"end\":35456,\"start\":35453},{\"end\":35478,\"start\":35470},{\"end\":35493,\"start\":35488},{\"end\":35507,\"start\":35501},{\"end\":35527,\"start\":35520},{\"end\":35803,\"start\":35799},{\"end\":35814,\"start\":35812},{\"end\":35822,\"start\":35819},{\"end\":35829,\"start\":35827},{\"end\":35839,\"start\":35836},{\"end\":36074,\"start\":36069},{\"end\":36091,\"start\":36083},{\"end\":36106,\"start\":36103},{\"end\":36121,\"start\":36115},{\"end\":36363,\"start\":36360},{\"end\":36372,\"start\":36367},{\"end\":36387,\"start\":36382},{\"end\":36401,\"start\":36396},{\"end\":36416,\"start\":36410},{\"end\":36437,\"start\":36427},{\"end\":36446,\"start\":36439},{\"end\":36717,\"start\":36712},{\"end\":36733,\"start\":36726},{\"end\":36951,\"start\":36945},{\"end\":36967,\"start\":36962},{\"end\":36979,\"start\":36976},{\"end\":36999,\"start\":36990},{\"end\":37287,\"start\":37283},{\"end\":37296,\"start\":37294},{\"end\":37303,\"start\":37301},{\"end\":37316,\"start\":37311},{\"end\":37333,\"start\":37328},{\"end\":37348,\"start\":37345},{\"end\":37359,\"start\":37354},{\"end\":37372,\"start\":37368},{\"end\":37384,\"start\":37380},{\"end\":37727,\"start\":37723},{\"end\":37744,\"start\":37736},{\"end\":37759,\"start\":37751},{\"end\":38003,\"start\":37997},{\"end\":38016,\"start\":38012},{\"end\":38032,\"start\":38025},{\"end\":38282,\"start\":38277},{\"end\":38294,\"start\":38289},{\"end\":38310,\"start\":38303},{\"end\":38332,\"start\":38324},{\"end\":38345,\"start\":38338},{\"end\":38641,\"start\":38639},{\"end\":38655,\"start\":38651},{\"end\":38666,\"start\":38663},{\"end\":38679,\"start\":38676},{\"end\":38694,\"start\":38689},{\"end\":38706,\"start\":38701},{\"end\":38714,\"start\":38711},{\"end\":39191,\"start\":39186},{\"end\":39205,\"start\":39202},{\"end\":39223,\"start\":39219},{\"end\":39237,\"start\":39234},{\"end\":39476,\"start\":39471},{\"end\":39493,\"start\":39489},{\"end\":39508,\"start\":39505},{\"end\":39517,\"start\":39514},{\"end\":39530,\"start\":39528},{\"end\":39541,\"start\":39538},{\"end\":39797,\"start\":39791},{\"end\":39821,\"start\":39807},{\"end\":39834,\"start\":39829},{\"end\":39853,\"start\":39844},{\"end\":40098,\"start\":40088},{\"end\":40115,\"start\":40108},{\"end\":40128,\"start\":40122},{\"end\":40141,\"start\":40137},{\"end\":40358,\"start\":40350},{\"end\":40372,\"start\":40366},{\"end\":40376,\"start\":40374},{\"end\":40559,\"start\":40556},{\"end\":40574,\"start\":40569},{\"end\":40583,\"start\":40581},{\"end\":40598,\"start\":40595},{\"end\":40808,\"start\":40802},{\"end\":41002,\"start\":40997},{\"end\":41017,\"start\":41010},{\"end\":41031,\"start\":41027},{\"end\":41049,\"start\":41040},{\"end\":41064,\"start\":41057},{\"end\":41086,\"start\":41075},{\"end\":41096,\"start\":41088},{\"end\":41426,\"start\":41414},{\"end\":41434,\"start\":41432},{\"end\":41449,\"start\":41447},{\"end\":41457,\"start\":41451},{\"end\":41697,\"start\":41690},{\"end\":41712,\"start\":41709},{\"end\":41727,\"start\":41720},{\"end\":41742,\"start\":41736},{\"end\":41755,\"start\":41752},{\"end\":41773,\"start\":41766},{\"end\":41788,\"start\":41782},{\"end\":41803,\"start\":41797},{\"end\":41819,\"start\":41812},{\"end\":41831,\"start\":41826},{\"end\":42154,\"start\":42151},{\"end\":42171,\"start\":42164},{\"end\":42184,\"start\":42177},{\"end\":42196,\"start\":42193},{\"end\":42415,\"start\":42410},{\"end\":42432,\"start\":42426},{\"end\":42453,\"start\":42444},{\"end\":42464,\"start\":42460},{\"end\":42478,\"start\":42471},{\"end\":42492,\"start\":42488},{\"end\":42507,\"start\":42501},{\"end\":42516,\"start\":42513},{\"end\":42532,\"start\":42526},{\"end\":42548,\"start\":42543},{\"end\":42823,\"start\":42814},{\"end\":42835,\"start\":42831},{\"end\":42850,\"start\":42840},{\"end\":42871,\"start\":42859},{\"end\":42892,\"start\":42882},{\"end\":43224,\"start\":43222},{\"end\":43240,\"start\":43236},{\"end\":43447,\"start\":43440},{\"end\":43461,\"start\":43454},{\"end\":43474,\"start\":43468},{\"end\":43491,\"start\":43482},{\"end\":43504,\"start\":43499},{\"end\":43519,\"start\":43514},{\"end\":43534,\"start\":43528},{\"end\":43552,\"start\":43542},{\"end\":43839,\"start\":43835},{\"end\":43851,\"start\":43846},{\"end\":43859,\"start\":43857},{\"end\":43867,\"start\":43865},{\"end\":43883,\"start\":43879},{\"end\":43892,\"start\":43890},{\"end\":44181,\"start\":44177},{\"end\":44196,\"start\":44191},{\"end\":44214,\"start\":44203},{\"end\":44228,\"start\":44225},{\"end\":44242,\"start\":44238},{\"end\":44258,\"start\":44254},{\"end\":44277,\"start\":44273},{\"end\":44288,\"start\":44283},{\"end\":44601,\"start\":44598},{\"end\":44623,\"start\":44611},{\"end\":44637,\"start\":44635},{\"end\":44651,\"start\":44648},{\"end\":44665,\"start\":44660},{\"end\":44675,\"start\":44667},{\"end\":44918,\"start\":44914},{\"end\":44934,\"start\":44929},{\"end\":44946,\"start\":44942},{\"end\":44957,\"start\":44954},{\"end\":45202,\"start\":45200},{\"end\":45211,\"start\":45208},{\"end\":45225,\"start\":45221},{\"end\":45237,\"start\":45233},{\"end\":45245,\"start\":45243},{\"end\":45259,\"start\":45253},{\"end\":45274,\"start\":45270},{\"end\":45516,\"start\":45514},{\"end\":45533,\"start\":45526},{\"end\":45544,\"start\":45540},{\"end\":45562,\"start\":45558},{\"end\":45577,\"start\":45573},{\"end\":45872,\"start\":45868},{\"end\":45880,\"start\":45877},{\"end\":45895,\"start\":45891},{\"end\":45909,\"start\":45904},{\"end\":45921,\"start\":45917},{\"end\":45930,\"start\":45928},{\"end\":45944,\"start\":45941},{\"end\":46224,\"start\":46219},{\"end\":46236,\"start\":46231},{\"end\":46252,\"start\":46247},{\"end\":46267,\"start\":46256},{\"end\":46285,\"start\":46278},{\"end\":46295,\"start\":46287},{\"end\":46660,\"start\":46656},{\"end\":46673,\"start\":46670},{\"end\":46683,\"start\":46678},{\"end\":46692,\"start\":46690},{\"end\":46938,\"start\":46935},{\"end\":46946,\"start\":46943},{\"end\":46961,\"start\":46956},{\"end\":46976,\"start\":46971}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":221378802},\"end\":34886,\"start\":34495},{\"attributes\":{\"id\":\"b1\"},\"end\":35061,\"start\":34888},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":209414687},\"end\":35355,\"start\":35063},{\"attributes\":{\"id\":\"b3\"},\"end\":35726,\"start\":35357},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":707161},\"end\":35980,\"start\":35728},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5201925},\"end\":36288,\"start\":35982},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7684883},\"end\":36638,\"start\":36290},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":219573658},\"end\":36855,\"start\":36640},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52967399},\"end\":37183,\"start\":36857},{\"attributes\":{\"id\":\"b9\"},\"end\":37615,\"start\":37185},{\"attributes\":{\"id\":\"b10\"},\"end\":37916,\"start\":37617},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6724907},\"end\":38176,\"start\":37918},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11863056},\"end\":38541,\"start\":38178},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":236924276},\"end\":39102,\"start\":38543},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":235306096},\"end\":39400,\"start\":39104},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":214795000},\"end\":39722,\"start\":39402},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5776303},\"end\":40012,\"start\":39724},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6308361},\"end\":40302,\"start\":40014},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6628106},\"end\":40476,\"start\":40304},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":91184480},\"end\":40748,\"start\":40478},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":221655446},\"end\":40892,\"start\":40750},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6941275},\"end\":41329,\"start\":40894},{\"attributes\":{\"doi\":\"2017. 4\",\"id\":\"b22\"},\"end\":41612,\"start\":41331},{\"attributes\":{\"id\":\"b23\"},\"end\":42080,\"start\":41614},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":235765540},\"end\":42354,\"start\":42082},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":91184540},\"end\":42773,\"start\":42356},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206756462},\"end\":43100,\"start\":42775},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2407217},\"end\":43404,\"start\":43102},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13756489},\"end\":43743,\"start\":43406},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":220363571},\"end\":44069,\"start\":43745},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":53735892},\"end\":44537,\"start\":44071},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":49358881},\"end\":44841,\"start\":44539},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":235166799},\"end\":45115,\"start\":44843},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3441497},\"end\":45461,\"start\":45117},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1688357},\"end\":45721,\"start\":45463},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":232092539},\"end\":46211,\"start\":45723},{\"attributes\":{\"id\":\"b36\"},\"end\":46573,\"start\":46213},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":244127479},\"end\":46851,\"start\":46575},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":208158040},\"end\":47133,\"start\":46853}]", "bib_title": "[{\"end\":34585,\"start\":34495},{\"end\":35134,\"start\":35063},{\"end\":35789,\"start\":35728},{\"end\":36058,\"start\":35982},{\"end\":36351,\"start\":36290},{\"end\":36704,\"start\":36640},{\"end\":36937,\"start\":36857},{\"end\":37987,\"start\":37918},{\"end\":38269,\"start\":38178},{\"end\":38630,\"start\":38543},{\"end\":39176,\"start\":39104},{\"end\":39466,\"start\":39402},{\"end\":39782,\"start\":39724},{\"end\":40080,\"start\":40014},{\"end\":40346,\"start\":40304},{\"end\":40547,\"start\":40478},{\"end\":40794,\"start\":40750},{\"end\":40993,\"start\":40894},{\"end\":42143,\"start\":42082},{\"end\":42400,\"start\":42356},{\"end\":42805,\"start\":42775},{\"end\":43215,\"start\":43102},{\"end\":43431,\"start\":43406},{\"end\":43827,\"start\":43745},{\"end\":44171,\"start\":44071},{\"end\":44592,\"start\":44539},{\"end\":44902,\"start\":44843},{\"end\":45190,\"start\":45117},{\"end\":45504,\"start\":45463},{\"end\":45859,\"start\":45723},{\"end\":46647,\"start\":46575},{\"end\":46926,\"start\":46853}]", "bib_author": "[{\"end\":34605,\"start\":34587},{\"end\":34624,\"start\":34605},{\"end\":34633,\"start\":34624},{\"end\":34652,\"start\":34633},{\"end\":34669,\"start\":34652},{\"end\":34904,\"start\":34890},{\"end\":34922,\"start\":34904},{\"end\":34941,\"start\":34922},{\"end\":35154,\"start\":35136},{\"end\":35163,\"start\":35154},{\"end\":35179,\"start\":35163},{\"end\":35188,\"start\":35179},{\"end\":35434,\"start\":35421},{\"end\":35444,\"start\":35434},{\"end\":35458,\"start\":35444},{\"end\":35480,\"start\":35458},{\"end\":35495,\"start\":35480},{\"end\":35509,\"start\":35495},{\"end\":35529,\"start\":35509},{\"end\":35805,\"start\":35791},{\"end\":35816,\"start\":35805},{\"end\":35824,\"start\":35816},{\"end\":35831,\"start\":35824},{\"end\":35841,\"start\":35831},{\"end\":36076,\"start\":36060},{\"end\":36093,\"start\":36076},{\"end\":36108,\"start\":36093},{\"end\":36123,\"start\":36108},{\"end\":36365,\"start\":36353},{\"end\":36374,\"start\":36365},{\"end\":36389,\"start\":36374},{\"end\":36403,\"start\":36389},{\"end\":36418,\"start\":36403},{\"end\":36439,\"start\":36418},{\"end\":36448,\"start\":36439},{\"end\":36719,\"start\":36706},{\"end\":36735,\"start\":36719},{\"end\":36953,\"start\":36939},{\"end\":36969,\"start\":36953},{\"end\":36981,\"start\":36969},{\"end\":37001,\"start\":36981},{\"end\":37289,\"start\":37275},{\"end\":37298,\"start\":37289},{\"end\":37305,\"start\":37298},{\"end\":37318,\"start\":37305},{\"end\":37335,\"start\":37318},{\"end\":37350,\"start\":37335},{\"end\":37361,\"start\":37350},{\"end\":37374,\"start\":37361},{\"end\":37386,\"start\":37374},{\"end\":37729,\"start\":37720},{\"end\":37746,\"start\":37729},{\"end\":37761,\"start\":37746},{\"end\":38005,\"start\":37989},{\"end\":38018,\"start\":38005},{\"end\":38034,\"start\":38018},{\"end\":38284,\"start\":38271},{\"end\":38296,\"start\":38284},{\"end\":38312,\"start\":38296},{\"end\":38334,\"start\":38312},{\"end\":38347,\"start\":38334},{\"end\":38643,\"start\":38632},{\"end\":38657,\"start\":38643},{\"end\":38668,\"start\":38657},{\"end\":38681,\"start\":38668},{\"end\":38696,\"start\":38681},{\"end\":38708,\"start\":38696},{\"end\":38716,\"start\":38708},{\"end\":39193,\"start\":39178},{\"end\":39207,\"start\":39193},{\"end\":39225,\"start\":39207},{\"end\":39239,\"start\":39225},{\"end\":39478,\"start\":39468},{\"end\":39495,\"start\":39478},{\"end\":39510,\"start\":39495},{\"end\":39519,\"start\":39510},{\"end\":39532,\"start\":39519},{\"end\":39543,\"start\":39532},{\"end\":39799,\"start\":39784},{\"end\":39823,\"start\":39799},{\"end\":39836,\"start\":39823},{\"end\":39855,\"start\":39836},{\"end\":40100,\"start\":40082},{\"end\":40117,\"start\":40100},{\"end\":40130,\"start\":40117},{\"end\":40143,\"start\":40130},{\"end\":40360,\"start\":40348},{\"end\":40374,\"start\":40360},{\"end\":40378,\"start\":40374},{\"end\":40561,\"start\":40549},{\"end\":40576,\"start\":40561},{\"end\":40585,\"start\":40576},{\"end\":40600,\"start\":40585},{\"end\":40810,\"start\":40796},{\"end\":41004,\"start\":40995},{\"end\":41019,\"start\":41004},{\"end\":41033,\"start\":41019},{\"end\":41051,\"start\":41033},{\"end\":41066,\"start\":41051},{\"end\":41088,\"start\":41066},{\"end\":41098,\"start\":41088},{\"end\":41428,\"start\":41411},{\"end\":41436,\"start\":41428},{\"end\":41451,\"start\":41436},{\"end\":41459,\"start\":41451},{\"end\":41699,\"start\":41685},{\"end\":41714,\"start\":41699},{\"end\":41729,\"start\":41714},{\"end\":41744,\"start\":41729},{\"end\":41757,\"start\":41744},{\"end\":41775,\"start\":41757},{\"end\":41790,\"start\":41775},{\"end\":41805,\"start\":41790},{\"end\":41821,\"start\":41805},{\"end\":41833,\"start\":41821},{\"end\":42156,\"start\":42145},{\"end\":42173,\"start\":42156},{\"end\":42186,\"start\":42173},{\"end\":42198,\"start\":42186},{\"end\":42417,\"start\":42402},{\"end\":42434,\"start\":42417},{\"end\":42455,\"start\":42434},{\"end\":42466,\"start\":42455},{\"end\":42480,\"start\":42466},{\"end\":42494,\"start\":42480},{\"end\":42509,\"start\":42494},{\"end\":42518,\"start\":42509},{\"end\":42534,\"start\":42518},{\"end\":42550,\"start\":42534},{\"end\":42825,\"start\":42807},{\"end\":42837,\"start\":42825},{\"end\":42852,\"start\":42837},{\"end\":42873,\"start\":42852},{\"end\":42894,\"start\":42873},{\"end\":43226,\"start\":43217},{\"end\":43242,\"start\":43226},{\"end\":43449,\"start\":43433},{\"end\":43463,\"start\":43449},{\"end\":43476,\"start\":43463},{\"end\":43493,\"start\":43476},{\"end\":43506,\"start\":43493},{\"end\":43521,\"start\":43506},{\"end\":43536,\"start\":43521},{\"end\":43554,\"start\":43536},{\"end\":43841,\"start\":43829},{\"end\":43853,\"start\":43841},{\"end\":43861,\"start\":43853},{\"end\":43869,\"start\":43861},{\"end\":43885,\"start\":43869},{\"end\":43894,\"start\":43885},{\"end\":44183,\"start\":44173},{\"end\":44198,\"start\":44183},{\"end\":44216,\"start\":44198},{\"end\":44230,\"start\":44216},{\"end\":44244,\"start\":44230},{\"end\":44260,\"start\":44244},{\"end\":44279,\"start\":44260},{\"end\":44290,\"start\":44279},{\"end\":44603,\"start\":44594},{\"end\":44625,\"start\":44603},{\"end\":44639,\"start\":44625},{\"end\":44653,\"start\":44639},{\"end\":44667,\"start\":44653},{\"end\":44677,\"start\":44667},{\"end\":44920,\"start\":44904},{\"end\":44936,\"start\":44920},{\"end\":44948,\"start\":44936},{\"end\":44959,\"start\":44948},{\"end\":45204,\"start\":45192},{\"end\":45213,\"start\":45204},{\"end\":45227,\"start\":45213},{\"end\":45239,\"start\":45227},{\"end\":45247,\"start\":45239},{\"end\":45261,\"start\":45247},{\"end\":45276,\"start\":45261},{\"end\":45518,\"start\":45506},{\"end\":45535,\"start\":45518},{\"end\":45546,\"start\":45535},{\"end\":45564,\"start\":45546},{\"end\":45579,\"start\":45564},{\"end\":45874,\"start\":45861},{\"end\":45882,\"start\":45874},{\"end\":45897,\"start\":45882},{\"end\":45911,\"start\":45897},{\"end\":45923,\"start\":45911},{\"end\":45932,\"start\":45923},{\"end\":45946,\"start\":45932},{\"end\":46226,\"start\":46213},{\"end\":46238,\"start\":46226},{\"end\":46254,\"start\":46238},{\"end\":46269,\"start\":46254},{\"end\":46287,\"start\":46269},{\"end\":46297,\"start\":46287},{\"end\":46662,\"start\":46649},{\"end\":46675,\"start\":46662},{\"end\":46685,\"start\":46675},{\"end\":46694,\"start\":46685},{\"end\":46940,\"start\":46928},{\"end\":46948,\"start\":46940},{\"end\":46963,\"start\":46948},{\"end\":46978,\"start\":46963}]", "bib_venue": "[{\"end\":34673,\"start\":34669},{\"end\":35192,\"start\":35188},{\"end\":35419,\"start\":35357},{\"end\":35845,\"start\":35841},{\"end\":36127,\"start\":36123},{\"end\":36452,\"start\":36448},{\"end\":36739,\"start\":36735},{\"end\":37006,\"start\":37001},{\"end\":37273,\"start\":37185},{\"end\":37718,\"start\":37617},{\"end\":38038,\"start\":38034},{\"end\":38351,\"start\":38347},{\"end\":38782,\"start\":38716},{\"end\":39243,\"start\":39239},{\"end\":39553,\"start\":39543},{\"end\":39859,\"start\":39855},{\"end\":40148,\"start\":40143},{\"end\":40382,\"start\":40378},{\"end\":40604,\"start\":40600},{\"end\":40814,\"start\":40810},{\"end\":41102,\"start\":41098},{\"end\":41409,\"start\":41331},{\"end\":41683,\"start\":41614},{\"end\":42202,\"start\":42198},{\"end\":42554,\"start\":42550},{\"end\":42930,\"start\":42894},{\"end\":43246,\"start\":43242},{\"end\":43558,\"start\":43554},{\"end\":43898,\"start\":43894},{\"end\":44294,\"start\":44290},{\"end\":44681,\"start\":44677},{\"end\":44963,\"start\":44959},{\"end\":45280,\"start\":45276},{\"end\":45583,\"start\":45579},{\"end\":45950,\"start\":45946},{\"end\":46386,\"start\":46297},{\"end\":46704,\"start\":46694},{\"end\":46982,\"start\":46978},{\"end\":38835,\"start\":38784}]"}}}, "year": 2023, "month": 12, "day": 17}