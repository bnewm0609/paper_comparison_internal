{"id": 220055765, "updated": "2023-10-06 13:54:30.776", "metadata": {"title": "The Convex Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network Verification", "authors": "[{\"first\":\"Christian\",\"last\":\"Tjandraatmadja\",\"middle\":[]},{\"first\":\"Ross\",\"last\":\"Anderson\",\"middle\":[]},{\"first\":\"Joey\",\"last\":\"Huchette\",\"middle\":[]},{\"first\":\"Will\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Krunal\",\"last\":\"Patel\",\"middle\":[]},{\"first\":\"Juan\",\"last\":\"Vielma\",\"middle\":[\"Pablo\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 24}, "abstract": "We improve the effectiveness of propagation- and linear-optimization-based neural network verification algorithms with a new tightened convex relaxation for ReLU neurons. Unlike previous single-neuron relaxations which focus only on the univariate input space of the ReLU, our method considers the multivariate input space of the affine pre-activation function preceding the ReLU. Using results from submodularity and convex geometry, we derive an explicit description of the tightest possible convex relaxation when this multivariate input is over a box domain. We show that our convex relaxation is significantly stronger than the commonly used univariate-input relaxation which has been proposed as a natural convex relaxation barrier for verification. While our description of the relaxation may require an exponential number of inequalities, we show that they can be separated in linear time and hence can be efficiently incorporated into optimization algorithms on an as-needed basis. Based on this novel relaxation, we design two polynomial-time algorithms for neural network verification: a linear-programming-based algorithm that leverages the full power of our relaxation, and a fast propagation algorithm that generalizes existing approaches. In both cases, we show that for a modest increase in computational effort, our strengthened relaxation enables us to verify a significantly larger number of instances compared to similar algorithms.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2006.14076", "mag": "3104862198", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/TjandraatmadjaA20", "doi": null}}, "content": {"source": {"pdf_hash": "0eef1268e103bc5edd33282134060ede324e5c5c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.14076v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "25a48fa1e8567e24beca203f7764957119b10b67", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0eef1268e103bc5edd33282134060ede324e5c5c.txt", "contents": "\nThe Convex Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network Verification\n24 Jun 2020\n\nChristian Tjandraatmadja \nRice University\nColumbia University\n\n\nGoogle Research \nRice University\nColumbia University\n\n\nRoss Anderson \nRice University\nColumbia University\n\n\nGoogle Research \nRice University\nColumbia University\n\n\nJoey Huchette joehuchette@rice.edu \nRice University\nColumbia University\n\n\nWill Ma \nRice University\nColumbia University\n\n\nKrunal Patel krunalp@google.com \nRice University\nColumbia University\n\n\nGoogle Research \nRice University\nColumbia University\n\n\nJuan Pablo \nRice University\nColumbia University\n\n\nVielma Google Research \nRice University\nColumbia University\n\n\nMit \nRice University\nColumbia University\n\n\nThe Convex Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network Verification\n24 Jun 2020\nWe improve the effectiveness of propagation-and linear-optimization-based neural network verification algorithms with a new tightened convex relaxation for ReLU neurons. Unlike previous single-neuron relaxations which focus only on the univariate input space of the ReLU, our method considers the multivariate input space of the affine pre-activation function preceding the ReLU. Using results from submodularity and convex geometry, we derive an explicit description of the tightest possible convex relaxation when this multivariate input is over a box domain. We show that our convex relaxation is significantly stronger than the commonly used univariate-input relaxation which has been proposed as a natural convex relaxation barrier for verification. While our description of the relaxation may require an exponential number of inequalities, we show that they can be separated in linear time and hence can be efficiently incorporated into optimization algorithms on an as-needed basis. Based on this novel relaxation, we design two polynomial-time algorithms for neural network verification: a linearprogramming-based algorithm that leverages the full power of our relaxation, and a fast propagation algorithm that generalizes existing approaches. In both cases, we show that for a modest increase in computational effort, our strengthened relaxation enables us to verify a significantly larger number of instances compared to similar algorithms.Preprint. Under review.\n\nIntroduction\n\nA fundamental problem in deep neural networks is to verify or certify that a trained network is robust, i.e. not susceptible to adversarial attacks [10,27,37]. Current approaches for neural network verification can be divided into exact (complete) methods and relaxed (incomplete) methods. Exact verifiers are often based on mixed integer programming (MIP) [3,4,8,9,11,13,16,23,29,39,46] or satisfiability modulo theories (SMT) [15,18,19,25,31] and, per their name, exactly solve the problem, with no false negatives or false positives. However, exact verifiers are typically based on solving NP-hard optimization problems [19] which can significantly limit their scalability. In contrast, relaxed verifiers are often based on polynomially-solvable optimization problems such as convex optimization or linear programming (LP) [2,14,22,24,28,30,32,45,48], which in turn lend themselves to faster propagation-based methods where bounds are computed by a series of variable substitutions in a backwards pass through the network [34,42,43,44,47]. Unfortunately, relaxed verifiers achieve this speed and scalability by trading off effectiveness (i.e. increased false negative rates), possibly failing to certify robustness when robustness is, in fact, present. As might be expected, the success of relaxed methods hinges on their tightness, or how closely they approximate the object which they are relaxing.\n\nAs producing the tightest possible relaxation for an entire neural network is no easier than the original verification problem, most relaxation approaches turn their attention instead to simpler substructures, such as individual neurons. For example, the commonly used \u2206-relaxation 1 [15] is simple and offers the tightest possible relaxation for the univariate ReLU function, and as a result is the foundation for many relaxed verification methods. Recently, Salman et al. [30] characterized the convex relaxation barrier, showing that the effectiveness of all existing propagation-based fast verifiers is fundamentally limited by the tightness of this \u2206-relaxation. Unfortunately, they show computationally that this convex barrier can be a severe limitation on the effectiveness of relaxed verifiers based upon it. While the convex relaxation barrier can be bypassed in various ways (e.g. considering relaxations for multiple neurons [32]), as noted in [30, Appendix A] all existing approaches that achieve this do so by trading off clarity and speed.\n\nIn this paper we improve the effectiveness of propagation-and LP-based relaxed verifiers with a new tightened convex relaxation for ReLU neurons. Unlike the \u2206-relaxation which focuses only on the univariate input space of the ReLU, our relaxation considers the multivariate input space of the affine pre-activation function preceding the ReLU. By doing this, we are able to bypass the convex barrier from [30] while remaining in the realm of single-neuron relaxations that can be utilized by fast propagation-and LP-based verifiers.\n\nMore specifically, our contributions are as follows.\n\n1. Using results from submodularity and convex geometry, we derive an explicit linear inequality description for the tightest possible convex relaxation of a single neuron, where, in the spirit of [3,4], we take this to encompass the ReLU activation function, the affine preactivation function preceding it, and known bounds on each input to this affine function. We show that this new convex relaxation is significantly stronger than the \u2206-relaxation, and hence bypasses the convex barrier from [30] without the need to consider multi-neuron interactions as in, e.g.\n\n[32].\n\n2. We show that this description, while requiring an exponential number of inequalities in the worst case, admits an efficient separation routine. In particular, we present a linear time algorithm that, given a point, either asserts that this point lies within the relaxation, or returns an inequality that is not satisfied by this point. Using this routine, we develop two verification algorithms that incorporate our tighter inequalities into the relaxation. 3. Computational experiments on verification problems using networks from the ERAN dataset [36] demonstrate that leveraging these inequalities yields a substantial improvement in verification capability. In particular, our fast propagation-based algorithm surpasses the strongest possible algorithm restricted by the convex barrier (i.e. optimizing over the \u2206relaxation at every neuron). We also show that our methods are competitive with more expensive state-of-the-art methods such as RefineZono [35] and kPoly [32], certifying more images than them in several cases.\n\n\nVerification via mathematical optimization\n\nConsider a neural network f : R m \u00d1 R r described in terms of N neurons in a linear order. 2 The first m neurons are the input neurons, while the remaining intermediate neurons are indexed by i \" m`1, . . . , N . Given some input x P R m , the relationship f pxq \" y can be described as\n\nx i \" z i @i \" 1, . . . , m (the inputs) (1a)\nz i \" \u00ff i\u00b41\nj\"1 w i,j z j`bi @i \" m`1, . . . , N (the pre-activation value) (1b) z i \" \u03c3p\u1e91 i q @i \" m`1, . . . , N (the post-activation value) (1c) y i \" \u00ff N j\"1 w i,j z j`bi @i \" N`1, . . . , N`r (the outputs).\n\nHere the constants w and b are the weights and biases, respectively, learned during training, while \u03c3pvq def \" maxt0, vu is the ReLU activation function. Appropriately, for each neuron i we dub the variable\u1e91 i the pre-activation variable and z i the post-activation variable.\n\nGiven a trained network (i.e. fixed architecture, weights, and biases), we study a verification problem of the following form: given constant c P R r , polyhedron X \u010e R m , \u03b2 P R, and \u03b3pc, Xq\ndef \" max xPX c\u00a8f pxq \" max x,y,\u1e91,z t c\u00a8y | x P X, (1) u ,(2)\ndoes \u03b3pc, Xq \u010f \u03b2? Unfortunately, this problem is NP-hard [19]. Moreover, one is typically not content with solving just one problem of this form, but would like to query for many reasonable choices of c and X to be convinced that the network is robust to adversarial perturbations.\n\nA promising approach to approximately solving the verification problem is to replace the intractable optimization problem defining \u03b3 in (2) with a tractable relaxation. In particular, we aim to identify a tractable optimization problem whose optimal objective value \u03b3 R pc, Xq satisfies \u03b3pc, Xq \u010f \u03b3 R pc, Xq, for all parameters c and X of interest. Then, if \u03b3 R pc, Xq \u010f \u03b2, we have answered the verification problem in the affirmative. However, note that it may well be the case that, by relaxing the problem, we may fail to verify a network that is, in fact, verifiable (i.e. \u03b3pc, Xq \u010f \u03b2 \u0103 \u03b3 R pc, Xq). Therefore, the strength of our relaxation is crucial for reducing the false negative rate of our verification method.\n\n\nThe \u2206-relaxation and its convex relaxation barrier\n\nSalman et al.\n\n[30] note that many relaxation approaches for ReLU networks are based on the singleactivation-function set A i def \" tp\u1e91 i , z i q P R 2 |L i \u010f\u1e91 i \u010f\u00db i , z i \" \u03c3 j p\u1e91 i qu, where the pre-activation boundsL i ,\u00db i P R are taken so thatL i \u010f\u1e91 i \u010f\u00db i for any point that satisfies x P X and (1). The \u2206-relaxation C i \u2206 def \" ConvpA i q is optimal in the sense that it describes the convex hull of A i , with three simple linear inequalities: z i \u011b 0, z i \u011b\u1e91 i , and z i \u010f\u00db \u00ee Ui\u00b4Li p\u1e91 i\u00b4Li q. The simplicity and small size of the \u2206-relaxation is appealing, as it leads to the relaxation\n\u03b3 \u2206 pc, Xq def \" max x,y,\u1e91,z c\u00a8y\u02c7\u02c7x P X, (1a), (1b), (1d), p\u1e91 i , z i q P C i \u2206 @i \" m`1, . . . , N ( .(3)\nThis is a small 3 Linear Programming (LP) problem than is theoretically tractable and relatively easy to solve in practice. Moreover, a plethora of fast propagation-based algorithms [33, 34, 41, 42, 43, 47] center on an approach that can be interpreted as further relaxing \u03b3 \u2206 , where inequalities describing the sets C i \u2206 are judiciously dropped from the description in such a way that this LP becomes much easier to solve. Unfortunately, Salman et al. [30] observe that the quality of the verification bounds obtained through the \u2206-relaxation are intrinsically limited; a phenomenon they call the convex relaxation barrier. Nonetheless, this LP, along with faster propagation algorithms that utilize the inequalities defining C i \u2206 , have been frequently applied to the verification task, often with substantial success.\n\n\nOur approach: Eliding pre-activation variables\n\nIn this paper, we show that we can significantly improve over the accuracy of \u2206-relaxation verifiers with only a minimal trade-off in simplicity and speed. The key for this result is the observation that pre-activation variables are a \"devil in disguise\" in the context of convex relaxations. For a neuron i, the pre-activation variable\u1e91 i and the post-activation variable z i form the minimal set of variables needed to capture (and relax) the nonlinearity introduced by the ReLU. However, this approach ignores the inputs to the pre-activation variable\u1e91 i , i.e. the preceding post-activation variables z 1:i\u00b41 def \" pz 1 , . . . , z i\u00b41 q.\n\nOur approach captures these relationships by instead turning our attention to the i-dimensional\nset 4 S i def \" ! z P R i\u02c7L \u010f z 1:i\u00b41 \u010f U, z i \" \u03c3\u00b4\u0159 i\u00b41 j\"1 w i,j z j`bi\u00af) ,\nwhere the post-activation bounds L, U P R i\u00b41 are such that L j \u010f z j \u010f U j for each point satisfying x P X and (1). Note that no pre-activation variables appear in this description; we elide them completely, substituting the affine function describing them inside of the activation function. This immediately gives a single-neuron relaxation of the form\n\u03b3 Elide pc, Xq def \" max x,y,z c\u00a8y\u02c7\u02c7x P X, (1a), (1d), z 1:i P C i Elide @i \" m`1, . . . , N ( ,(4)\nwhere C i Elide def \" ConvpS i q is the convex hull of S i , as shown in Figure 1 (adapted from [3]), which contrasts it with the convex barrier and \u2206-relaxation. We will show that, unsurprisingly, C i Elide will require exponentially many inequalities to describe in the worst case. However, a contribution of this paper is to show that this need not be a barrier to incorporating this tighter relaxation into verification algorithms.\n\n\nAn exact convex relaxation for a single ReLU neuron\n\nLet w P R n , b P R, f pxq def \" w\u00a8x`b, and L, U P R n be such that L \u0103 U . For ease of exposition, we rewrite the single-neuron set S i in the generic form S def \" t px, yq P rL, U s\u02c6R | y \" \u03c3pf pxqq u .\n\nNotationally, take n def \" t1, . . . , nu,\nL i def \" \" L i w i \u011b 0 U i o.w. and\u0216 i def \" \" U i w i \u011b 0 L i o.w. for each i P n , \u2113pIq def \" \u0159 iPI w iLi`\u0159 iRI w i\u0216i`b , and J def \" ! pI, hq P 2 n \u02c6 n \u02c7\u02c7\u02c7\u2113 pIq \u011b 0, \u2113pI Y thuq \u0103 0, w i \u2030 0 @i P I ) .\nOur main technical result uses results from submodularity and convex geometry [1,6,26,38] to give the following closed-form characterization of ConvpSq. For a proof of Theorem 1, see Appendix A. 5 Theorem 1. If \u2113p n q \u011b 0, then ConvpSq \" S \" t px, yq P rL, U s\u02c6R | y \" f pxq u. Alternatively, if \u2113pHq \u0103 0, then ConvpSq \" S \" rL, U s\u02c6t 0 u. Otherwise, ConvpSq is equal to the set of all px, yq P R n\u02c6R satisfying 4 The effective dimension of this set can be much smaller if wi,\u00a8is sparse. This is the case with a feedforward network, where the number of nonzeros is (at most) the number of neurons in the preceding layer. 5 We also show in Appendix A that ConvpSq is the projection of the LP relaxation of the MIP in [3,4].\ny \u011b w\u00a8x`b, y \u011b 0, L \u010f x \u010f U (6a) y \u010f \u00ff iPI w i px i\u00b4Li q`\u2113 pIq U h\u00b4Lh px h\u00b4Lh q @pI, hq P J .(6b)\nFurthermore, if d def \" |t i P n | w i \u2030 0 u|, then d \u010f |J | \u010f r 1 2 ds`d r 1 2 ds\u02d8a nd for each of these inequalities (and each d P n ) there exist data that makes it hold at equality.\n\nNote that this is the tightest possible relaxation when x P rL, U s. Moreover, we observe that the relaxation offered by ConvpSq can be arbitrarily tighter than that derived from the \u2206-relaxation. Proposition 1. For any input dimension n, there exists a pointx P R n , and a problem instance given by the affine function f , the \u2206-relaxation C \u2206 , and the single neuron set S such that max y:pf pxq,yqPC\u2206 y\u02d8\u00b4`max y:px,yqPConvpSq y\u02d8\" \u2126pnq.\n\nAlthough the family of upper-bounding constraints (6b) may be exponentially large, the structure of the inequalities is remarkably simple. As a result, the separation problem can be solved efficiently: given px, yq, either verify that px, yq P ConvpSq, or produce an inequality from the description (6) which is violated at px, yq. For instance, we can solve in Opn log nq time the optimization problem\n\u03c5pxq def \" min \" \u00ff iPI w i px i\u00b4Li q`\u2113 pIq U h\u00b4Lh px h\u00b4Lh q\u02c7\u02c7\u02c7\u02c7pI, hq P J * ,(7)\nby sorting the indices with w i \u2030 0 in nondecreasing order of values px i\u00b4Li q{p\u0216 i\u00b4Li q, then adding them to I in this order so long as \u2113pIq \u011b 0 (note that adding to I can only decrease \u2113pIq), and then letting h be the index that triggered the stopping condition \u2113pI Y thuq \u0103 0. For more details, see the proof of Proposition 2 in Appendix B.\n\nThen, to check if px, yq P ConvpSq, we first check if the point satisfies (6a), which can be accomplished in Opnq time. If so, we compute \u03c5pxq in Opn log nq time. If y \u010f \u03c5pxq, then px, yq P ConvpSq. Otherwise, an optimal solution to (7) yields an inequality from (6b) that is most violated at px, yq. In addition, we can also solve (7) slightly faster. Proposition 2. Optimization problem (7) can be solved in Opnq time.\n\nTogether with the ellipsoid algorithm [17], Proposition 2 shows that the single-neuron relaxation \u03b3 Elide can be efficiently solved (at least in a theoretical sense). Corollary 1. If the weights w and biases b describing the neural network are rational, then the single-neuron relaxation (4) can be solved in polynomial time on the encoding sizes of w and b.\n\nFor proofs of Proposition 1, Proposition 2 and Corollary 1, see Appendix B.\n\n\nA propagation-based algorithm\n\nWe now present a technique to use the new family of strong inequalities (6b) to generate strong post-activation bounds for a trained neural network. To properly define the algorithm, we begin by restating a generic propagation-based bound generation framework under which various algorithms from the literature are special cases (partially or completely) [33, 34, 41, 42, 43, 47].\n\n\nA generic framework for computing post-activation bounds\n\nConsider a bounded input domain X \u010e R m , along with a single output (i.e. r \" 1) to be maximized, which we name Cpzq \" \u0159 \u03b7 i\"1 c i z i`b for some \u03b7 \u010f N . In this section, our goal is produce efficient algorithms for producing valid upper bounds for C. First, let z i pxq denote the unique value of z i (post-activation variable i) implied by the equalities (1b-1c) when we set z 1:m \" x for some x P X. Next, assume that for each intermediate neuron i \" m`1, . . . , \u03b7 we have affine functions of the form L i pz 1:i\u00b41 q \" \u0159 i\u00b41 j\"1 w l ij z j`b l i and U i pz 1:i\u00b41 q \" \u0159 i\u00b41 j\"1 w u ij z j`b u i , such that L i pz 1:i\u00b41 pxqq \u010f z i pxq \u010f U i pz 1:i\u00b41 pxqq @x P X, i \" 1, . . . , \u03b7.\n\n(8) We consider how to construct these functions in the next subsection. Then, given these functions we can compute a bound on C pz 1:\u03b7 pxqq through the following optimization problem:\nB pC, \u03b7q def \" max z Cpzq \" \u00ff \u03b7 i\"1 c i z i`b (9a)\ns.t. z 1:m P X (9b) L i pz 1:i\u00b41 q \u010f z i \u010f U i pz 1:i\u00b41 q @i \" m`1, . . . , \u03b7.\n\n(9c) Proposition 3. The optimal value of (9) is no less than max xPX C pz 1:\u03b7 pxqq.\n\nThe optimal value B pC, \u03b7q can be quickly computed through propagation methods without explicitly computing an optimal solution to (9) [30,47]. Such methods perform a backward pass to sequentially eliminate (project out) the intermediate variables z N , . . . , z m`1 , which can be interpreted as applying Fourier-Motzkin elimination [7, Chapter 2.8]. In a nutshell, for i \" \u03b7, . . . , m`1, the elimination step for variable z i uses its objective coefficient (which may be changing throughout the algorithm) to determine which one of the bounds from (9c) will be binding at the optimal solution and replaces z i by the expression L i pz 1:i\u00b41 q or U i pz 1:i\u00b41 q accordingly. The procedure ends with a smaller LP that only involves the input variables z 1:m and can be quickly solved with an appropriate method. For instance, when X is a box, as is common in verification problems, this final LP can be trivially solved by considering each variable individually. For more details, see Algorithm 1 in Appendix C.\n\n\nSelecting the bounding functions\n\nThe framework described in the previous section required as input the family of bounding functions tL i , U i u \u03b7 i\"m`1 . A typical approach to generate these will proceed sequentially, deriving the i-th pair of functions using scalar boundsL i ,\u00db i P R on the i-th pre-activation variables\u1e91 i , which by (1b) is equal to \u0159 i\u00b41 j\"1 w i,j z j`bi . Hence, these scalar bounds must satisf\u0177\nL i \u010f \u00ff i\u00b41 j\"1 w i,j z j pxq`b i \u010f\u00db i @x P X.(10)\nThese bounds can then be used as a basis to linearize the nonlinear equation\nz i \" \u03c3\u00b4\u00ff i\u00b41 j\"1 w i,j z j`bi\u00af( 11)\nimplied by (1b-1c). If\u00db i \u010f 0 orL i \u011b 0, then (11) behaves linearly when (10) holds, and so we can let L i pz 1:i\u00b41 q \" U i pz 1:i\u00b41 q \" \u0159 i\u00b41 j\"1 w i,j z j`bi or L i pz 1:i\u00b41 q \" U i pz 1:i\u00b41 q \" 0, respectively. Otherwise, we can construct non-trivial bounds such as\nL i pz 1:i\u00b41 q \"\u00db \u00ee U i\u00b4Li\u02dci\u00b41 \u00ff j\"1 w i,j z j`bi\u00b8a nd U i pz 1:i\u00b41 q \"\u00db \u00ee U i\u00b4Li\u02dci\u00b41 \u00ff j\"1 w i,j z j`bi\u00b4Li\u00b8,\nwhich can be derived from the \u2206-relaxation: U i pz 1:i\u00b41 q is the single upper-bounding inequality present on the left side of Figure 1, and L i pz 1:i\u00b41 q is a shifted down version of this inequality. 6  can be derived by selecting the same U i pz 1:i\u00b41 q as above and L i pz 1:i\u00b41 q \" 0 if |L i | \u011b |\u00db i | or L i pz 1:i\u00b41 q \" \u0159 i\u00b41 j\"1 w i,j z j`bi otherwise (i.e. whichever yields the smallest area of the relaxation). In the next subsection, we propose using (6b) for U i pz 1:i\u00b41 q.\n\nScalar bounds satisfying (10) for the i-th pre-activation variable can be computed by letting\nC U,i`z 1:pi\u00b41q\u02d8\" \u0159 i\u00b41 j\"1 w i,j z j`bi and then settingL i \"\u00b4B`C L,i , i\u00b41\u02d8and\u00db i \" B`C U,i , i\u00b41\u02d8\n. Therefore, to reach a final bound for \u03b7 \" N , we can iteratively computeL i and\u00db i for i \" m`1, . . . , N by solving (9) each time, since each of these problems requires only affine bounding functions up to intermediate neuron i\u00b41. See Algorithm 4 in Appendix C for details.\n\n\nOur contribution: Tighter bounds by dynamically updating bounding functions\n\nIn Theorem 1 we have derived a family of inequalities, (6b), which can be applied to yield valid upper bounding affine functions for each intermediate neuron in a network. As there may be exponentially many such inequalities, it is not clear a priori which to select as input to the algorithm from Section 4.1. Therefore, we present a simple iterative scheme in which we apply a small number of solves of (9), incrementally updating the set of affine bounding functions used at each iteration.\n\nOur goal is to update the upper bounding function U i with one of the inequalities from (6b) via the separation procedure of Proposition 2, which requires an optimal solution z 1:N for (9). However, the backward pass of the propagation algorithm described in Section 4.1 only computes the optimal value B pC, \u03b7q and a partial solution z 1:m . For this reason, we first extend the propagation algorithm with a forward pass that completes the partial solution z 1:m by propagating the values for z m`1 , . . . , z N through the network. This propagation uses the same affine bounding functions from (9c) that were used to eliminate variables in the backward pass. For more details, see Algorithm 2 in Appendix C.\n\nIn essence, our complete dynamic algorithm initializes with a set of bounding functions (e.g. from Fast-Lin or DeepPoly), applies a backward pass to solve the bounding problem, and then a forward pass to reconstruct the full solution. It then takes that full solution, and at each intermediate neuron i applies the separation procedure of Proposition 2 to produce an inequality from the family (6b). If this inequality is violated, it replaces the upper bounding function U i with this inequality from (6b). We then repeat for as many iterations as desired and take the best bound produced across all iterations. In this way, we use separation to help us select from a large family just one inequality that will (hopefully) be most beneficial for improving the bound. For more details, see Algorithm 3 in Appendix C.\n\n\nComputational experiments\n\n\nComputational setup\n\nWe evaluate two methods: the propagation-based algorithm from Section 4.3 and a method based on partially solving the LP from Theorem 1 by treating the inequalities (6b) as cutting planes, i.e. inequalities that are dynamically added to tighten a relaxation. To focus on the benefit of incorporating the inequalities (6b) into verification algorithms, we implement simple versions of the algorithms, devoid of extraneous features and fine-tuning. We name this framework \"Cut-to-Verify\" (C2V), and the propagation-based and LP-based algorithms FastC2V and OptC2V, respectively.\n\nThe overall framework in both methods is the same: we compute scalar bounds for the pre-activation variables of all neurons as we move forward in the network, using those bounds to produce the subsequent affine bounding functions and LP formulations as discussed in Section 4.2. Below, we describe the bounds computation for each individual neuron.\n\nPropagation-based algorithm (FastC2V). We implement the algorithm described in Section 4.3, using as initial affine bounding functions tL i , U i u N i\"m`1 the ones from DeepPoly [34] and CROWN-Ada [47], as described in Section 4.1. 7 In this implementation, we run a single iteration of the algorithm.\n\nLP-based algorithm (OptC2V). Each bound is generated by solving a series of LPs where our upper bounding inequalities are dynamically generated and added as cutting planes. We start with the standard \u2206-relaxation LP, solve it to optimality, and then for every neuron preceding the one we are bounding, we add the most violated inequality with respect to the LP optimum by solving (7). This can be repeated multiple times. In this implementation, we perform three rounds of separation. We generate new cuts from scratch for each bound that we compute.\n\nIn both methods, at each neuron we take the best between the bound produced by the method and the trivial interval arithmetic bound. Appendix D contains other implementation details.\n\nWe compare each of our novel algorithms against their natural baselines: DeepPoly for our propagation-based method, and the standard \u2206-relaxation LP for our cutting plane method. Our implementation of DeepPoly is slightly different from the one in [34] in that we take the best of interval arithmetic and the result of DeepPoly at each neuron. Moreover, our implementation is sequential, even though operations in the same layer could be parallelized (for each of the algorithms implemented in this work). The LP method simply solves the \u2206-relaxation LP to generate Verification problem. We consider the following verification problem: given a correctly labeled target image, certify that the neural network returns the same label for each input within L 8 -distance at most \u01eb of that target image. More precisely, given an imagex P r0, 1s m correctly labeled as t, a neural network where f k pxq returns its logit for class k P K, and a distance \u01eb \u0105 0, the imagex is verified to be robust if max xPrL,\u00dbs max kPK tf k pxq\u00b4f t pxqu \u0103 0, whereL i \" maxt0,x i\u00b4\u01eb u and\nU i \" mint1,x i`\u01eb u for all i \" 1, . . . , m.\nHandling the inner max term can be done by computing bounds for f k pxq\u00b4f t pxq for every class k \u2030 t and checking if the maximum bound is negative.\n\nTo facilitate the comparison with existing algorithms, our experimental setup closely follows that of Singh et al. [32]. We experiment on a subset of trained neural networks from the publicly available ERAN dataset [36]. We examine the following networks: the fully connected ReLU networks 6x100 (\u01eb \" 0.026), 9x100 (\u01eb \" 0.026), 6x200 (\u01eb \" 0.015), 9x200 (\u01eb \" 0.015), all trained on MNIST without adversarial training; the ReLU convolutional networks ConvSmall for MNIST (\u01eb \" 0.12), with 3 layers and trained without adversarial training; the ReLU network ConvBig for MNIST (\u01eb \" 0.3), with 6 layers and trained with DiffAI; and the ReLU network ConvSmall for CIFAR-10 (\u01eb \" 2{255), with 3 layers and trained with PGD. These \u01eb values are the ones used in [32] and they are cited as being challenging. For more details on these networks, see [36]. For each network, we verify the first 1000 images from their respective test sets except those that are incorrectly classified.\n\nDue to numerical issues with LPs, we zero out small values in the convolutional networks for the LP-based algorithms (see Appendix D). Other than this, we do not perform any tuning according to instance. Our implementation is in C++ and we perform our experiments in an Intel Xeon E5-2699 2.3Ghz machine with 128GB of RAM. We use Gurobi 8.1 as the LP solver, take advantage of incremental solves, and set the LP algorithm to dual simplex, as we find it to be faster for these LPs in practice. This means that our LP implementation does not run in polynomial time, even though it could in theory by using a different LP algorithm (see Corollary 1).\n\n\nComputational results\n\nThe computational results in Table 1 demonstrate that adding the upper bounding inequalities proposed in this paper significantly improves the number of images verified compared to their base counterparts. While on average FastC2V spends an order of magnitude more time than DeepPoly to achieve this, it still takes below one minute on average for all instances examined. OptC2V takes approximately 1.2 to 2.7 times of a pure LP method to generate bounds in the problems examined.\n\nSince we start from the LP basis of the previous solve, subsequent LPs after adding cuts are generally faster.\n\nInterestingly, we observe that FastC2V verifies more images than LP in almost all cases in much less time. This indicates that, in practice, a two-inequality relaxation with a single (carefully chosen) tighter inequality from (6b) can often be stronger than the three-inequality \u2206-relaxation.\n\nWhen compared to other state-of-the-art incomplete verifiers, we observe that for the larger networks, improving DeepPoly with our inequalities enables it to verify more images than RefineZono [35], a highly fine-tuned method that combines MIP, LP, and DeepPoly, but without the expensive computation and the parameter tuning needs from RefineZono. In addition, we find that adding our inequalities to LPs is competitive with kPoly, surpassing it for some of the networks. While the timings in [32] may not be comparable to our timings, the authors report average times for RefineZono and kPoly within the range of 4 to 15 minutes and 40 seconds to 8 minutes, respectively.\n\nOutlook: Our methods as subroutines The scope of our computational experiments is to demonstrate the practicality and strength of our full-neuron relaxation applied to simple methods, rather than to engineer full-blown state-of-the-art verification methods. Towards such a goal, we remark that both RefineZono and kPoly rely on LP and other faster verification methods as building blocks to a stronger method, and either of our methods could be plugged into them. For example, we could consider a hybrid approach similar to RefineZono that uses the stronger, but slower OptC2V in the earlier layers (where it can have the most impact) and then switches to FastC2V, which could result in verification times closer to FastC2V with an effectiveness closer to OptC2V. In addition, kPoly exploits the correlation between multiple neurons in the same layer, whereas our approach does not, suggesting that there is room to combine approaches. Finally, we note that solving time can be controlled with a more careful management of the inequalities to be added and parallelizing bound computation of neurons in the same layer.\n\n\nBroader Impact\n\nIn a world where deep learning is impacting our lives in ever more tangible ways, verification is an essential task to ensure that these black box systems behave as we expect them to. Our fast, simple algorithms have the potential to make a positive impact by verifying a larger number of inputs to be robust within a short time-frame, often required in several applications. Of course, we should be cautious that although our algorithms provide a mathematical certificate of an instance being robust, failure to use the system correctly, such as modeling the verification problem in a way that does not reflect real-world concerns, can still lead to unreliable neural networks. We also highlight that our version of the verification problem, while accurately capturing a reasonable formal specification of robustness, clearly does not perfectly coincide with \"robustness\" as may be used in a colloquial sense. Therefore, we highlight the importance of understanding the strengths and limitations of the mathematical model of verification used, so that a false sense of complacency does not set in. \n\n\nA Proof of Theorem 1\n\nWe provide two different proofs for Theorem 1. The first proof is based on classical machinery from submodular and convex optimization. The alternative proof is based on projecting down an extended MIP formulation built using disjunctive programming. We include them both since each proof provides unique insights on our new relaxation.\n\nWe first state a lemma that is used by both proofs for bounding the number of inequalities. Notationally, we will take 0 d and 1 d as the length d vectors of all zeros and all ones, respectively, and epiq P R n for i P n as the i-th canonical unit vector, where the length will be implicitly determined by the context of its use. In some cases it will be convenient to refer to the 0-th canonical vector ep0q \" 0 n . Lemma 1. If u, v P t0, 1u d are such that \u0159 d i\"1 |u i\u00b4vi | \" 1, then we say that uv is an edge of r0, 1s d . For w P R d and b P R, we say the hyperplane w\u00a8x`b \" 0 cuts edge uv of r0, 1s d if w\u00a8u`b \u0103 0 and w\u00a8v`b \u011b 0. If b \u0103 0 and \u0159 d i\"1 w i`b \u011b 0, then the number of edges cut by one such hyperplane is lower-bounded by d and upper-bounded by r 1 2 ds`d r 1 2 ds\u02d8. For each bound there exists a hyperplane with w P R d such that the bounds holds at equality.\n\nProof. Consider the graph G \" pV, Eq with V \" t0, 1u d and E equal to the edges of r0, 1s d . Let s \" 0 d and t \" 1 d . Then w\u00a8s`b \u0103 0 and w\u00a8t`b \u011b 0, so the edges of r0, 1s d cut by the hyperplane form a s\u00b4t graph-cut in G (note that this does not have the same meaning as the definition of cut for an edge given in the Lemma statement). Hence, the number of edges cut by the hyperplane are lower bounded by d (e.g. follows by Menger's theorem by noting that there are d disjoint paths in G from s to t). An example of a hyperplane that achieves this lower bound is w \" 1 d and b \"\u00b41{2.\n\nThe tight upper bound follows from a simple adaptation of the proof of a result from [26]. 8 An example of a hyperplane that achieves this upper bound is w \" 1 d and b \"\u00b4r 1 2 ds.\n\n\nA.1 A proof using submodularity\n\nWe start with an example.\n\n\nA.1.1 Illustrative example and definitions\n\nExample 1. Consider the set from (5) for n \" 2, w \" p1, 1q, b \" p\u00b41.5q, L \" p0, 0q and U \" p0, 0q, which corresponds to S \" px, yq P r0, 1s 2\u02c6R\u02c7y \" gpxq ( for gpxq def \" max t 0, x 1`x2\u00b41 .5 u. Set S is depicted in Figure 2 and we can check that ConvpSq is described by\n\nx P r0, 1s 2 (12a) y \u011b gpxq (12b) y \u010f r 1 pxq, y \u010f r 2 pxq (12c) for r 1 pxq def \" 0.5x 2 and r 2 pxq def \" 0.5x 1 . Inequality (12b) is obtained by relaxing the equation describing S to an inequality and using the fact that gpxq is convex. Functions r 1 and r 2 from inequality (12c) are depicted in Figures 2a and 2b, respectively. These functions can be obtained through the following interpolation procedure.\n\nFirst, consider the subdivision of r0, 1s 2 into the triangles T 1 and T 2 depicted in Figures 2a and  2b, respectively. As depicted Figure 2a, the vertices of T 1 are obtained by incrementally adding the canonical vectors to p0, 0q, in order, until we obtain p1, 1q. That is, the vertices of T 1 are ep0q \" p0, 0q, ep0q`ep1q \" ep1q \" p0, 1q and ep0q`ep1q`ep2q \" p1, 1q. In contrast, as depicted in Figure 2b, the vertices of T 2 are obtained by incrementally adding the canonical vectors in reverse order (i.e. the vertices of T 2 are ep0q \" p0, 0q, ep0q`ep2q \" p1, 0q and ep0q`ep2q`ep1q \" p1, 1q).\n\nSecond, we obtain r 1 and r 2 by constructing the unique affine interpolation of g on T 1 and T 2 , respectively. That is, as depicted in Figure 2a, r 1 pxq \" \u03b1 1\u00a8x`\u03b2 1 , where \u03b1 1 P R 2 and \u03b2 1 P R are such that r 1 is equal to g for the three vertices p0, 0q, p1, 0q and p1, 1q of T 1 :\n0 0 1 0 1 1\u00b8\u03b1 1`\u03b2 1 \"\u02dcg p0, 0q gp1, 0q gp1, 1q\u00b8\"\u02dc0 0 0.5\u00b8.\nThe unique solution of this system is \u03b1 1 \" p0, 0.5q and \u03b2 1 \" 0, which yields r 1 pxq \" 0.5x 2 . Function r 2 is obtained by a similar procedure using the vertices of T 2 as illustrated Figure 2b.    The subdivision of r0, 1s 2 into T 1 and T 2 can be extended to r0, 1s n by considering all n! possible orders in which we can obtain 1 n from 0 n by incrementally adding the canonical vectors. We represent these orders using the set of all permutations of n . In Example 1, this set is given by S 2 def \" t \u03c0 1 , \u03c0 2 u, where \u03c0 i : 2 \u00d1 2 for each i P 2 , \u03c0 1 p1q \" 1, \u03c0 1 p2q \" 2, \u03c0 2 p1q \" 2, and \u03c0 2 p2q \" 1. Then, under the notation of Definition 1 below, we have T 1 \" T \u03c01 and T 2 \" T \u03c02 . Definition 1. Let S n be the set of all permutations of n . Then for every \u03c0 P S n , we define\nV \u03c0 \" ! \u0159 j i\"0 e p\u03c0 piqq ) n j\"0 and T \u03c0 \" conv pV \u03c0 q \" x P R n\u02c71 \u011b x \u03c0p1q \u011b x \u03c0p2q \u011b . . . \u011b x \u03c0pnq \u011b 0 ( .(13)\nThe collection of simplices t T \u03c0 u \u03c0PSn , whose union is r0, 1s n , is known as the Kuhn triangulation of r0, 1s n [40].\n\nThe number of simplices in the Kuhn triangulation is exponential, so an n-dimensional generalization of Example 1 could contain an exponential number of inequalities in (12c). Fortunately, as illustrated in the following example, the characterization of T \u03c0 in the right hand side of (13) allow us to easily filter for relevant inequalities.\n\nExample 1 continued. Consider the point px\u02da, y\u02daq \" p0.6, 0.3, 0.5q depicted as a red star in Figure 2c. To check if px\u02da, y\u02daq P ConvpSq we can first verify that y\u02da\u011b g px\u02daq and x\u02daP r0, 1s 2 . It then only remains to check that px\u02da, y\u02daq satisfies all inequalities in (12c). However, we can instead exploit the fact that if x P T 1 , then r 1 pxq \" min t r 1 pxq, r 2 pxq u. As illustrated in Figure 2c we can use the fact that x1 \u011b x2 to conclude that x\u02da(depicted as a red circle in Figure 2c) belongs to T 1 . Finally, we can check that r 1 px\u02daq \" 0.3 \u0103 0.5 to conclude that px\u02da, y\u02daq R ConvpSq (Point px\u02da, 0.3q is depicted as a red diamond in Figure 2c).\n\nTo show that the ideas in Example 1 can be generalized, we will exploit properties of submodular functions. For that we connect functions from r0, 1s n with set-functions. We pick one specific connection that simplifies the statement and proof of Theorem 1.\n\n\nDefinition 2. A set-function H : 2 n \u00d1 R is submodular if\n\nHpSq`HpT q \u011b HpS Y T q`HpS X T q @S, T \u010e n .\n\nFor any h : r0, 1s n \u00d1 R we define the set-function H : 2 n \u00d1 R given by HpIq \" h\u00b4\u0159 iRI epiqf or each I \u010e n . In particular, H p n q \" h p0 n q and H pHq \" h p1 n q. In general, for any function from r0, 1s n to R defined as a lower case letter (e.g. h), we let the associated set-function be defined by the upper case version of this letter (e.g. H).\n\n\nA.1.2 Proof of Theorem 1\n\nOur proof has three steps. First, we formalize the idea in Example 1 for arbitrary dimensions (Theorem 2). Then, we reduce the number of inequalities by characterizing which of the simplices T \u03c0 lead to identical inequalities (Lemma 2). Finally, to complete the proof of Theorem 1, we describe the explicit form of these inequalities.\n\nCorollary 3.14 in [38] gives us a precise description of Conv pQq where Q is a normalized version of S from (5) that also considers any convex activation function. We include a submodularity-based proof of the corollary for completeness, adapted to our context. Theorem 2. Let w P R \u01f9 and b P R, f pxq \" w\u00a8x`b, \u03c1 : R \u00d1 R be any convex function, gpxq \" \u03c1pf pxqq and Q \" t px, yq P r0, 1s n\u02c6R | y \" \u03c1pf pxqq u.\n\nFor each \u03c0 P S n let r \u03c0 : r0, 1s n \u00d1 R be the unique affine interpolation 9 of g on T \u03c0 such that r \u03c0 pvq \" gpvq for all v P V \u03c0 . Then ConvpQq equals the set of all px, yq P R n\u02c6R satisfying\ny \u011b gpxq (14a) y \u010f r \u03c0 pxq @\u03c0 P S n (14b) 0 \u010f x i \u010f 1 @i P n (14c)\nProof. Let h : r0, 1s n \u00d1 R be such that hpxq \"\u00b4gpxq \"\u00b4\u03c1pf pxqq for all x P r0, 1s n . In addition, let h and h respectively be the convex and concave envelopes of h (i.e. the largest convex underestimator of h, which is well-defined because the pointwise maximum of convex functions lying below h is a convex function, and the smallest concave overestimator of h, which is similarly well-defined). Then Q \" t px, yq P r0, 1s n\u02c6R |\u00b4y \" hpxq u and Conv pQq \" px, yq P r0, 1s n\u02c6R\u02c7h pxq \u010f\u00b4y \u010f hpxq ( (e.g. [30, Proposition B.1]). Function h is concave and hence h \" h, so it only remains to describe h.\n\nTo describe h, we define a set function H based on h (see Definition 2), which is submodular because\u00b4\u03c1 is concave and w is non-negative (e.g. see [1,Section 3.1]). Submodularity allows us to describe the lower convex envelope of the continuous function h through the Lov\u00e1sz extension of the set function H. This extension is the piecewise affine function from r0, 1s n to R defined over the pieces tT \u03c0 : \u03c0 P S n u, which equals max \u03c0PSn p\u00b4r \u03c0 q by convexity (e.g. see [6] for further details). Therefore the constraint required for convpQq is hpxq \u010f\u00b4y \u00f0\u00f1 y \u010f min \u03c0PSn r \u03c0 pxq which completes the derivation of inequalities (14b) in the theorem statement.\n\nNote that even though there are exponentially many inequalities in (14b), the tightest constraint on y at any given point x P r0, 1s n can be efficiently found, by sorting the coordinates of x to find the simplex T \u03c0 to which x belongs. Moreover, going from r0, 1s n to rL, U s and eliminating the sign restriction on w can be achieved with standard variable transformations (e.g. see the comments before [38, Corollary 3.14]).\n\nBefore demonstrating the variable transformations, we first further refine Theorem 2 for the case when \u03c1 is equal to the ReLU activation function \u03c3. In particular, we generally have that each one of the n! inequalities in (14b) is facet-defining because they hold at equality over the n`1 affinely independent points t pv, gpvqq u vPV\u03c0 . Hence, they are all needed to describe ConvpRq. However, because it may happen that r \u03c0 \" r \u03c0 1 for \u03c0 \u2030 \u03c0 1 , the number of inequalities in (14b) after removing duplicates may be much smaller. The following lemma shows that this is indeed the case when \u03c1 is equal to the ReLU activation function \u03c3. The lemma also gives a closed form expression for the interpolating functions r \u03c0 in this case. Lemma 2. Let w P R \u01f9 and\u00b4\u0159 n i\"1 w i \u010f b \u0103 0, f pxq \" w\u00a8x`b, and gpxq \" \u03c3pf pxqq. If t r \u03c0 u \u03c0PSn are the affine interpolation functions from Theorem 2, then px, yq P R n`1\u02c7y \u010f r \u03c0 pxq @\u03c0 P S n ( \" px, yq P R n`1\u02c7y \u010f r I,h pxq @ pI, hq P I ( where I def \" pI, hq P 2 n \u02c6 n \u02c7\u02c7F pIq \u011b 0, F pI Y thuq \u0103 0 ( , r I,h pxq def \" F pIqx h`\u0159 iPI w i x i , and F : 2 n \u00d1 R is the set-function associated to f as defined in Definition 2.\n\nProof. Fix \u03c0 P S n and for each j P n let Ipjq def \" t \u03c0piq u n i\"j`1 . Then the interpolation condition for r \u03c0 given by r \u03c0 pvq \" gpvq for all v P V \u03c0 is equivalent to R \u03c0 pIpjqq \" G pIpjqq @j \" 0, 1, . . . , n (15) where R \u03c0 and G are the set-functions associated to r \u03c0 and g as defined in Definition 2. For j \" 0, condition (15) implies r \u03c0 p0 n q \" R \u03c0 p n q \" G p n q \" gp0 n q \" 0 and hence there exists \u03b1 P R n such that r \u03c0 pxq \" \u03b1\u00a8x (i.e. r \u03c0 is a linear function). For j P n , condition (15) further implies that \u03b1 \u03c0pjq \" g\u00b4\u00ff Now, because F pHq \" f p1 n q \u011b 0, w P R \u01f9 , and b \u0103 0, there exists a unique k P n such that pIpkq, \u03c0pkqq P I. Furthermore, w P R \u01f9 , F pI pkq Y t \u03c0 pkq uq \" F pI pk\u00b41qq \u0103 0, and F pI pkqq \u011b 0 imply F pIpjqq \u0103 0 and G pIpjqq \" 0 @j \" 0, . . . , k\u00b41; (17a) F pIpjqq \u011b 0 and G pIpjqq \" F pIpjqq @j \" k, . . . , n.\n\n(17b) Equations (16) and (17a) imply \u03b1 \u03c0pjq \" 0 for all j P k or equivalently \u03b1 i \" 0 for all i R I pkq Y t \u03c0 pkq u. Equations (16) and (17) imply \u03b1 \u03c0pkq \" G pI pkqq \" F pI pkqq. Finally, equations (16) and (17b) imply that \u03b1 \u03c0pjq \" w \u03c0pjq for all j \" k`1, . . . , n or equivalently \u03b1 i \" w i for all i P I. Hence, r \u03c0 \" r Ipkq,\u03c0pkq . The lemma follows by noting that for any pI, hq P I there exists at least one \u03c0 P S n such that pI pkq , \u03c0 pkqq \" pI, hq.\n\nFinally, we obtain the proof of Theorem 1 recalling that f pxq \" w\u00a8x`b for w P R n and b P R, and S \" t px, yq P rL, U s\u02c6R | y \" \u03c3pf pxqq u for L, U P R n such that L \u0103 U . Theorem 1. If \u2113p n q \u011b 0, then ConvpSq \" S \" t px, yq P rL, U s\u02c6R | y \" f pxq u. Alternatively, if \u2113pHq \u0103 0, then ConvpSq \" S \" rL, U s\u02c6t 0 u. Otherwise, ConvpSq is equal to the set of all px, yq P R n\u02c6R satisfying\ny \u011b w\u00a8x`b, y \u011b 0, L \u010f x \u010f U (6a) y \u010f \u00ff iPI w i px i\u00b4Li q`\u2113 pIq U h\u00b4Lh px h\u00b4Lh q @pI, hq P J . (6b)\nFurthermore, if d def \" |t i P n | w i \u2030 0 u|, then d \u010f |J | \u010f r 1 2 ds`d r 1 2 ds\u02d8a nd for each of these inequalities (and each d P n ) there exist data that makes it hold at equality.\n\nProof. Recalling that J def \" pI, hq P 2 n \u02c6 n \u02c7\u02c7\u2113 pIq \u011b 0, \u2113pI Y thuq \u0103 0, w i \u2030 0 @i P I ( we can assume without loss of generality that w i \u2030 0 for all i P n and hence d \" n (Indices i with w i \" 0 do not affect (6b) or the definition of J and the only inequalities for S or ConvpSq in which a given x i appears are L i \u010f x i \u010f U i ).\n\nFor the first case, the result follows because f pxq \u0103 0 for all x P rL, U s and hence gpxq \" 0 for all x P rL, U s.\n\nFor the second case, the result follows because f pxq \u011b 0 for all x P rL, U s and hence gpxq \" f pxq for all x P rL, U s.\n\nFor the third case, recall thatL i \"\n\" L i w i \u011b 0 U i o.w. and\u0216 i \" \" U i w i \u011b 0 L i o.w.\n, and consider the affine variable transformation given by\nx i def \"\nx i\u00b4L\u020b U i\u00b4Li and x i \" p\u0216 i\u00b4Li qx i`Li @i P n .\nLetw i def \" w i p\u0216 i\u00b4Li q for each i P n ,b def \" b`\u0159 n i\"1 w iLi \" \u2113p n q \u0103 0, andf pxq def \"w\u00a8x`b (recall that \u2113pIq def \" \u0159 iPI w iLi`\u0159 iRI w i\u0216i`b )(18)\n. Then we may infer that w ixi \" w i px i\u00b4Li q @i P n ,\n\nthat f pxq \"f pxq, and finally that px, yq P S if and only if px, yq PS def \" ! px, yq P r0, 1s n\u02c6R\u02c7y \" \u03c3pf pxqq\n\n) .\n\nIn addition, we concludew P R \u01f9 , using the definition ofL and\u0216 and the fact that L \u0103 U . Hence, Theorem 2 and Lemma 2 are applicable forS andgpxq \" \u03c3pf pxqq. Then\n\nConvpSq \"\n\n! px, yq P r0, 1s n\u02c6R`\u02c7f pxq \u010f y \u010f r I,h pxq @ pI, hq P I\n) where I \" ! pI, hq P 2 n \u02c6 n \u02c7\u02c7\u02c7F pIq \u011b 0,F pI Y thuq \u0103 0 )\nand r I,h pxq \"F pIqx h\u0159 iPIw ixi . Using the definitions ofb andw i we get (20) and hence I \" J \" pI, hq P 2 n \u02c6 n \u02c7\u02c7\u2113 pIq \u011b 0, \u2113pI Y thuq \u0103 0 ( . Combining (18-20), we get\u0213\nF pIq \" \u00ff iRIw i`b \" \u00ff iRI w i\u00b4\u0216i\u00b4Li\u00af`\u02dcb`n \u00ff i\"1 w iLi\u00b8\" \u00ff iRI w i\u0216i`\u00ff iPI w iLi`b \" \u2113 pIqI,h pxq \"F pIqx h`\u00ff iPIw ixi \" \u2113 pIq x h\u00b4Lh U h\u00b4Lh`\u00ff iPI w i\u00b4xi\u00b4Li\u00af.\nHence, Conv pSq is described by (6).\n\nFinally, pI, hq P J if and only if the hyperplane \u0159 n i\"1 w i x i`b \" 0 cuts the edge uv of r0, 1s n given by u def \" \u0159 iRpIYt h uq epiq and v def \" \u0159 iRI epiq (with the convention that an empty sum is equal to zero). The result on |J | then follows by Lemma 1 recalling that without loss of generality we have assumed n \" d.\n\n\nA.2 An alternative proof using mixed-integer programming and projection\n\nWe can alternatively prove Theorem 1 by connecting it to the MIP formulation from [3] for S defined in (5). For this, first recall that that f pxq \" w\u00a8x`b for w P R n and b P R, and S \" t px, yq P rL, U s\u02c6R | y \" \u03c3pf pxqq u for L, U P R n such that L \u0103 U .\nCorollary 2. Let R sharp def \" $ ' ' ' & ' ' ' % px, y, zq P rL, U s\u02c6R\u02c6r0, 1s 2\u02c7y \u011b 0, y \u011b w\u00a8x`b, y \u010ff px, zq, z 1`z2 \" 1 , / / / . / / / - , wheref px, zq def \" max x 1 ,x 2 $ ' & ' % w\u00a8x 2`b z 2\u02c7x \"x 1`x2 , Lz k \u010fx k \u010f U z k @k P 2 x 1 ,x 2 P R n , / . / - .\nThen Conv pSq \" Proj x,y pR sharp q def \" px, yq P R n`1\u02c7D z P R 2 s.t. px, y, xq P R sharp ( .\n\nProof. Follows from [3,Proposition 5] for the case d \" 2, w 1 \" 0, b 1 \" 0, w 2 \" w, b 2 \" b.\nLemma 3. Let R def \" $ ' & ' % px, yq P rL, U s\u02c6R\u02c7\u02c7\u02c7\u02c7\u02c7\u02c7y \u011b 0, y \u011b w\u00a8x`b, y \u010ff pxq , / . / - wheref pxq def \" max x 1 ,x 2 ,z $ ' ' ' ' ' ' & ' ' ' ' ' ' % w\u00a8x 2`b z 2\u02c7x \"x 1`x2 , Lz k \u010fx k \u010f U z k @k P 2 x 1 ,x 2 P R n z 1`z2 \" 1 z P r0, 1s 2 , / / / / / / . / / / / / / - .(21)\nThen Conv pSq \" R.\n\nProof. By Corollary 2 it suffices to show R \" Proj x,y pR sharp q.\n\nInclusion Proj x,y pR sharp q \u010e R follows by noting thatf px,\u1e91q \u010ff pxq for any px,\u0177,\u1e91q P R sharp .\n\nFor inclusion R \u010e Proj x,y pR sharp q, let px,\u0177q P R, and let`x 1 ,x 2 , z\u02d8P R 2n`2 be an optimal solution to the optimization problem in the right hand side of (21) for x \"x. Such solution exists because forx P rL, U s this optimization problem is the maximization of a linear function over a non-empty bounded polyhedron. Then,f pxq \"f px, zq, and hence px,\u0177, zq P R sharp .\n\nTheorem 1. If \u2113p n q \u011b 0, then ConvpSq \" S \" t px, yq P rL, U s\u02c6R | y \" f pxq u. Alternatively, if \u2113pHq \u0103 0, then ConvpSq \" S \" rL, U s\u02c6t 0 u. Otherwise, ConvpSq is equal to the set of all px, yq P R n\u02c6R satisfying\ny \u011b w\u00a8x`b, y \u011b 0, L \u010f x \u010f U (6a) y \u010f \u00ff iPI w i px i\u00b4Li q`\u2113 pIq U h\u00b4Lh px h\u00b4Lh q @pI, hq P J . (6b) Furthermore, if d def\n\" |t i P n | w i \u2030 0 u|, then d \u010f |J | \u010f r 1 2 ds`d r 1 2 ds\u02d8a nd for each of these inequalities (and each d P n ) there exist data that makes it hold at equality.\n\nProof. Recalling that J def \" pI, hq P 2 n \u02c6 n \u02c7\u02c7\u2113 pIq \u011b 0, \u2113pI Y thuq \u0103 0, w i \u2030 0 @i P I ( we can assume without loss of generality that w i \u2030 0 for all i P n and hence d \" n (Indices i with w i \" 0 do not affect (6b) or the definition of J and the only inequalities for S or ConvpSq in which x i appear for such index are L i \u010f x i \u010f U i ).\n\nFor the first case, the result follows because f pxq \u0103 0 for all x P rL, U s and hence gpxq \" 0 for all x P rL, U s.\n\nFor the second case, the result follows because f pxq \u011b 0 for all x P rL, U s and hence gpxq \" f pxq for all x P rL, U s.\n\nFor the third case, it suffices to show that\nf pxq \" min pI,hqPJ # \u00ff iPI w i px i\u00b4Li q`\u2113 pIq U h\u00b4Lh px h\u00b4Lh q + ,(22)\nin which case, set R from Lemma 3 is exactly the set described by (6). To show (22) we first simplify the optimization problem definingf pxq by applying the simple substitutionsx def \"x 2 \" x\u00b4x 1 and z def \" z 2 \" 1\u00b4z 1 :f\npxq \" max x,z $ & % w\u00a8x`bz\u02c7\u02c7\u02c7\u02c7\u02c7\u02c7L p1\u00b4zq \u010f x\u00b4x \u010f U p1\u00b4zq,\nLz \u010fx \u010f U z, z P r0, 1s\n\n, .\n\n-.\n\nThis optimization problem is feasible and bounded when L \u010f x \u010f U , and thus we may assume an optimal solution exists.\n\nConsider some i P n . If w i \u0105 0, thenx i \u011b L i z and x i\u00b4xi \u010f U i p1\u00b4zq hold at any optimal solution, since we are maximizing the problem and each constraint involves only a single x i and z. Analogously, if w i \u0103 0, thenx i \u010f U i z and x i\u00b4xi \u011b L i p1\u00b4zq are implied as well. To unify these two cases into one as a simplification, observe that these constraints can be expressed as w ixi \u011b w iLi z and w i px i\u00b4xi q \u010f w i\u0216i p1\u00b4zq respectively (recall that w i \u2030 0 by assumption, and thatL i \" L i if w i \u011b 0, or U i otherwise, and\u0216 i \" U i if w i \u011b 0, or L i otherwise). Therefore, we can drop these constraints and keep the remaining ones:\nf pxq \" max x,z $ ' & ' %\nw\u00a8x`bz\u02c7\u02c7\u02c7\u02c7\u02c7\u02c7w i px i\u00b4xi q \u011b w iLi p1\u00b4zq @i P n , w ixi \u010f w i\u0216i z @i P n z P r0, 1s\n, / . / - . Define \u03b3 i def \" w i p\u0216 i z\u00b4x i q for all i P n .\nWe can then rewrite the problem as:\nf pxq \" max \u03b3,z $ ' & ' % pw\u00a8\u0216`bqz\u00b4n \u00ff i\"1 \u03b3 i\u02c7w i p\u0216 i\u00b4Li qz\u00b4\u03b3 i \u010f w i px i\u00b4Li q @i P n \u03b3 \u011b 0, z P r0, 1s , / . / - .\nWe next take the dual of this problem. By strong duality, the following holds:\nf pxq \" min \u03b1,\u03b2 $ ' ' ' & ' ' ' % n \u00ff i\"1 w i px i\u00b4Li q\u03b1 i`\u03b2\u02c7n \u00ff i\"1 w i p\u0216 i\u00b4Li q\u03b1 i`\u03b2 \u011b n \u00ff i\"1 w i\u0216i`b , \u03b1 P r0, 1s n , \u03b2 \u011b 0 , / / / . / / / - .\nTo conclude the proof, we describe the optimal solutions of the optimization problem above. Note that it is a minimization variant of a fractional knapsack problem and it can be solved by a greedy algorithm, in which we order the indices of \u03b1 by xi\u00b4L\u020b Ui\u00b4Li and maximally select those with the smallest ratios, until the knapsack constraint is satisfied at equality. We also need to consider \u03b2 in the knapsack, but since the ratios for \u03b1 i are in r0, 1s and the ratio for \u03b2 is 1, \u03b2 would only be picked last. Moreover, under the assumptions of our current third case, we have \u2113p n q \" \u0159 n i\"1 w iLi`b \u0103 0, and thus that we can satisfy the knapsack constraint by choosing from \u03b1's (recall that \u2113pIq \" \u0159 iPI w iLi`\u0159 iRI w i\u0216i`b ). Therefore we may set \u03b2 \" 0. Let I be the set of indices in which \u03b1 i \" 1 for the optimal solution and h be the next index to be considered by the greedy procedure after the elements in I. Then\n\u03b1 h \"\u00b4\u0159 n i\"1 w i\u0216i`b\u00af\u00b4\u00b4\u0159 iPI w i p\u0216 i\u00b4Li q\u016a h\u00b4Lh \" \u2113pIq U h\u00b4Lh P r0, 1q.\nObserve that \u2113pIq \u011b 0 is equivalent to stating that the items in I are below the knapsack capacity, since \u2113pIq equals the capacity of the knapsack minus the total weight of the items in I. Therefore, \u2113pIq \u011b 0 and \u2113pI Y thuq \u0103 0 (i.e. the items in I fit but we can only add h partially). Hence, we can write the optimization problem definingf pxq as finding the optimal I and h:\nf pxq \" min I,hRI # \u00ff iPI w i px i\u00b4Li q`\u2113 pIq U h\u00b4Lh px h\u00b4Lh q | \u2113pIq \u011b 0, \u2113pI Y thuq \u0103 0 + .\nWe obtain (22) by recalling that J \" pI, hq P 2 n \u02c6 n \u02c7\u02c7\u2113 pIq \u011b 0, \u2113pI Y thuq \u0103 0 ( . Finally, pI, hq P J if and only if the hyperplane \u0159 n i\"1 w i x i`b \" 0 cuts the edge uv of r0, 1s n given by u def \" \u0159 iRpIYt h uq epiq and v def \" \u0159 iRI epiq (with the convention that an empty sum is equal to zero). The result on |J | then follows by Lemma 1 recalling that without loss of generality we have assumed n \" d.\n\nB Proofs of other results from Section 3 Proposition 1. For any input dimension n, there exists a pointx P R n , and a problem instance given by the affine function f , the \u2206-relaxation C \u2206 , and the single neuron set S such that max y:pf pxq,yqPC\u2206 y\u02d8\u00b4`max y:px,yqPConvpSq y\u02d8\" \u2126pnq.\n\nProof. This follows as a straightforward extension of [3, Example 2], as the \u2206-relaxation is equal to the projection of the big-M formulation presented in that work.\n\nThe following proposition shows how the additional structure in Lemma 2 allows increasing the speed of checking for violated inequalities from Opn logpnqq, achievable by sorting the input components, to Opnq. Proposition 2. Optimization problem (7) can be solved in Opnq time.\nProof. Recall that J def \"\npI, hq P 2 n \u02c6 n \u02c7\u02c7\u2113 pIq \u011b 0, \u2113pI Y thuq \u0103 0, w i \u2030 0 @i P I ( ,\n\u2113pIq def \" \u0159 iPI w iLi`\u0159 iRI w i\u0216i`b ,L i def \" \" L i w i \u011b 0 U i o.w. and\u0216 i def \" \" U i w i \u011b 0 L i o.w.\nfor each i P n , and (7) is the optimization problem given by\n\u03c5pxq def \" min \" \u00ff iPI w i px i\u00b4Li q`\u2113 pIq U h\u00b4Lh px h\u00b4Lh q\u02c7\u02c7\u02c7\u02c7pI, hq P J * .\nFirst, we can check in Opnq time if \u2113p n q \u011b 0 or \u2113pHq \u0103 0, in which case J \" H and (7) is infeasible. Otherwise, \u2113p n q \u0103 0, \u2113pHq \u011b 0, and J \u2030 H.\n\nWe can also remove in Opnq time all i P n such that w i \" 0. Then without loss of generality we may assume that w i \u2030 0 for all i P n and hence L \u0103 U implies that w i p\u0216 i\u00b4Li q \u0105 0 @i P n .\n\nWe will show that (7) is equivalent to the linear programming problem\n\u03c9pxq def \" min v n \u00ff i\"1 w i px i\u00b4Li qv i (24a) s.t. n \u00ff i\"1 w i p\u0216 i\u00b4Li qv i \" n \u00ff i\"1 w i\u0216i`b ,(24b)0 \u010f v \u010f 1.(24c)\nNote that the set of basic feasible solutions for the linear programming problem is exactly the set of all feasible points with at most one fractional component (see, e.g., [7, Chapter 3]). That is, all basic feasible solutions of (24) are elements of V def \" t v P r0, 1s n | |t i P n | v i P p0, 1q u| \u010f 1 u.\n\nTo prove that \u03c9pxq \u010f \u03c5pxq, consider the mapping \u03a6 : J \u00d1 V given by\n\u03a6 ppI, hqq i \" $ ' & ' % 1 i P I \u2113pIq w h p\u0216 h\u00b4Lh q i \" h 0 o.w.\n@i P n .\n\nLet p\u012a,hq P J be an optimal solution for (7) and letv \" \u03a6``\u012a,h\u02d8\u02d8. Then\nn \u00ff i\"1 w i p\u0216 i\u00b4Li qv i \" \u00ff iP\u012a w i p\u0216 i\u00b4Li q`whp\u0216h\u00b4Lhq \u2113p\u012aq whp\u0216h\u00b4Lhq \" n \u00ff i\"1 w i\u0216i`b ,\nand hencev satisfies (24b). Algebraic manipulation shows that w h p\u0216 h\u00b4Lh q \" \u2113pIq\u00b4\u2113pI Y thuq @I \u010e n , h P n zI.\n\nIn addition, p\u012a,hq P J implies \u2113p\u012aq \u011b 0 and \u2113p\u012a Y thuq \u0103 0. Combining this with (25) gives the inequality \u2113p\u012aq \u0103 whp\u0216h\u00b4Lhq. Therefore,v h P r0, 1q, and hencev h is feasible for (24). In addition, for any pI, hq P J we have that n \u00ff i\"1 w i px i\u00b4Li qv i \" \u00ff iPI w i px i\u00b4Li q`w h p\u0216 h\u00b4Lh q \u2113pIq w h px h\u00b4Lh q and hence the objective value ofv h for (24) is the same as the objective value of pI, hq for (7).\n\nTo prove \u03c9pxq \u011b \u03c5pxq we will show that, through \u03a6, the greedy procedure to solve (7) described in the main text just before the statement of Proposition 2, becomes the standard greedy procedure for (24) and hence also yields an optimal basic feasible solution to (24). For simplicity, assume without loss of generality that we have re-ordered the indices in n so that\nx 1\u00b4L1 U 1\u00b4L1 \u010f x 2\u00b4L2 U 2\u00b4L2 \u010f\u00a8\u00a8\u00a8\u010f x n\u00b4Ln U n\u00b4Ln .(26)\nThen the greedy procedure that incrementally grows I terminates with some pI, hq P J where I \" h\u00b41 . Then v \" \u03a6 pp h\u00b41 , hqq is a basic feasible solution for (24) with the same objective value as the objective value of pI, hq for (7). To conclude that \u03c9pxq \u011b \u03c5pxq, we claim that v is an optimal solution for (24) since the standard greedy procedure for (24) is known to generate the optimal solution for this problem. For completeness, we give the following self contained proof of the claim. Assume for a contradiction that \u03c9 pxq \u0103 \u0159 n i\"1 w i px i\u00b4Li qv i and let v 1 be an optimal solution to (24). Because v 1 \u2030 v and both v and v 1 satisfy (24b), (23) implies there must exists j 1 , j 2 P n such that j 1 \u0103 j 2 , j 1 \u010f h, v 1 j1 \u0103 v j1 , j 2 \u011b h and v 1 j2 \u0105 v j2 . Let \u01eb \u0105 0 be the largest value such that v 1 j1`\u01eb wj 1 p\u0216j 1\u00b4L j 1 q \u010f v j1 and v 1 j2\u00b4\u01eb wj 2 p\u0216j 2\u00b4L j 2 q \u011b v j2 , and let\nv 2 def \" v 1`\u01eb w j1 p\u0216 j1\u00b4Lj1 q epj 1 q\u00b4\u01eb w j2 p\u0216 j2\u00b4Lj2 q epj 2 q.\nBy (26) we either have\nx j1\u00b4Lj1 U j1\u00b4Lj1 \" x j2\u00b4Lj2 U j2\u00b4Lj2 or x j1\u00b4Lj1 U j1\u00b4Lj1 \u0103 x j2\u00b4Lj2 U j2\u00b4Lj2 .(27)\nIn the first case v 2 is a feasible solution to (24) that has fewer different components with v and has the same objective value as v 1 . Hence, by repeating this procedure we will eventually have the second case in which v 2 is a feasible solution to (24) that has an objective value strictly smaller than that of v 1 , which contradicts the optimality of v 1 .\n\nThe greedy procedure to solve (7) and (24) can be executed in Opn logpnqq time through the sorting required to get (26). However, an optimal basic feasible solution\u03b1 to (24) can also be obtained in Opnq time by solving a weighted median problem (e.g. [20, Chapter 17.1]). This solution can be converted to an optimal solution to (7) in Opnq time as follows. Because\u03b1 is a basic feasible solution to (24), it has at most one fractional component (see, e.g., [7, Chapter 3]). Take\u00ce \" t i P n |\u03b1 i \" 1 u. Ifv has one fractional component, take\u0125 to be this component. Then, becaus\u00ea \u03b1 satisfies (24b) we have w\u0125p\u0216\u0125\u00b4L\u0125q\u03b1\u0125 \"\nn \u00ff i\"1 w i\u0216i`b\u00b4\u00ff iP\u00ce w i p\u0216 i\u00b4Li q \" \u2113p\u00ceq(28)\nTogether with\u03b1\u0125 P p0, 1q, (25) for I \"\u00ce and h \"\u0125, and (23) for i \"\u0125, we have \u2113p\u00ceq \u0105 0 and \u2113p\u00ceq\u00b4\u2113p\u00ce Y t\u0125uq \u0105 \u2113p\u00ceq.\n\nThen \u2113p\u00ce Y t\u0125uq \u0103 0 and p\u00ce,\u0125q P J . Finally, (28) implies that the objective value of\u03b1 for (24) is the same as the objective value of p\u00ce,\u0125q for (7).\n\nIf, on the other hand,v has no fractional component, then\u03b1 satisfying (24b) implies 0 \" n \u00ff i\"1 w i\u0216i`b\u00b4\u00ff iP\u00ce w i p\u0216 i\u00b4Li q \" \u2113p\u00ceq.\n\nThen, \u2113p n q \u0103 0 implies that there exists\u0125 P n z\u00ce such that \u2113p\u00ce Y t\u0125uq \u0103 0 and p\u00ce,\u0125q P J . Finally, (29) implies that the objective value of\u03b1 for (24) is the same as the objective value of p\u00ce,\u0125q for (7). This conversion of an optimal basic feasible solution for (24) to a solution to (7) also gives an alternate proof to \u03c9pxq \u011b \u03c5pxq. Proof. If w and b are rational, then the coefficients of the inequalities in (6b) are also rational numbers with sizes that are polynomial in the sizes of w and b. Then the result follows from Proposition 2 and [12, Theorem 7.26].\n\nC Propagation algorithms\n\n\nC.1 Description and analysis of algorithms\n\nIn this section, we provide pseudocode for the propagation-based algorithms described in Section 4.\n\nIn the scope of a single neuron, Algorithm 1 specifies the framework outlined in Section 4. C.2 Proofs of other results from section 4 Proposition 3. The optimal value of (9) is no less than max xPX C pz 1:\u03b7 pxqq.\n\nProof. For any x P X, by definition of validity in (8), setting z i \u00d0 z i pxq for all i \" 1, . . . , N yields a feasible solution to (9) with objective value c pz 1:N pxqq, completing the proof.\n\n\nD Implementation details\n\nIn this section, we add to the implementation details provided in Section 5.\n\nThe implementation of the propagation-based algorithm involves the following details:\n\n\u2022 It may occur that the result of Algorithm 1 has zero coefficients for some variables x i , in which case any feasible value for x i produces an optimal solution. For those variables, we select the midpoint between the lower bound and upper bound to proceed with Algorithm 2. \u2022 We find that running more than one iteration of the propagation-based algorithm does not yield improving results. A possible reason for this is that while these inequalities are stronger in some portions of the input space, they are looser by themselves in others, and balancing this can be difficult. Improving this trade-off however is outside the scope of this paper. \u2022 We use no tolerance on violation. That is, every violated inequality is swapped in.\n\nThe implementation of the LP-based algorithm involves the following details:\n\n\u2022 We find that the Conv networks examined are very numerically unstable for LPs due to the presence of very small weights in the networks. Taking no action results in imprecise solutions, sometimes resulting in infeasible LPs being constructed. To improve on this instability, we consider as zero any weight or generated bound below 10\u00b45. In addition, we run DeepPoly before the LP to quickly check if the neuron can be linearized. This is applied only to the LP-based methods. Note that the default feasibility and optimality tolerances in Gurobi are 10\u00b46. With this, we end up solving an approximate problem rather than the exact problem, though arguably it is too difficult to solve these numerically unstable LPs with high precision and reasonable time in practice. \u2022 For separation, we implement the Opn log nq version of the algorithm based on sorting instead of the Opnq version. \u2022 For each bound computed, we generate new cuts from scratch. More specifically, when solving for each bound, we make a copy of the model and its LP basis from the previous solve, run the LP solve and cut loop, retrieve the bound, and then discard this copy of the model. \u2022 We add cuts whose violation exceeds a tolerance of 10\u00b45.\n\n\u2022 In the context of mixed-integer programming, it is well known that selecting a smaller subset of cuts to add can be very beneficial to reduce solving time, but for simplicity, we perform no cut selection in this method. \u2022 An alternative to the LP-based method is to solve a MIP with analogous cutting planes with binary variables [3], but we find that this method, free of binary variables, is more lightweight and effective even without cut selection and all the presolve functionalities of modern MIP solvers. The ability to solve these LPs very quickly is important since we solve them at every neuron. In addition, this gives us more fine-grained control on the cuts, providing a better opportunity to evaluate our inequalities.\n\nThe implementation of all algorithms involve the following details:\n\n\u2022 We attempt to linearize each neuron with simple interval arithmetic before running a more expensive procedure. This makes a particularly large difference in solving time for the Conv networks, in which many neurons are linearizable. \u2022 As done in other algorithms in the literature, we elide the last affine layer, a step that is naturally incorporated in the framework from Section 4.1. In other words, we do not consider the last affine layer to be a neuron but to be the objective function. \u2022 We fully compute the bounds of all neurons in the network, including differences of logits.\n\nWe make no attempt to stop early even if we have the opportunity to infer robustness earlier.\n\n\u2022 When solving the verification problem, scalar bounds on the intermediate neurons only need to be computed once per input image (i.e. once per set X), and can be reused for each target class (i.e. reused for different objectives c).\n\n(a) OptC2V :\nOptC2VWe develop a polynomial-time LP-based algorithm that harnesses the full power of our new relaxation. (b) FastC2V: We develop a fast propagation-based algorithm that generalizes existing approaches (e.g. Fast-Lin [42] and DeepPoly [34]) by dynamically adapting the relaxation using our new inequalities.\n\nFigure 1 :\n1A simple neural network with m \" 2 dimensional input and one intermediate neuron (N \" 3). (Left) The feasible region for \u03b3 \u2206 , and (Right) The feasible region for \u03b3 Elide . The x, y, and\u1e91 variables, which depend affinely on the others, are projected out.\n\n\nr1 using T1.\n\n\nr2 using T2.\n\nFigure 2 :\n2Using interpolation on triangles to construct ConvpSq for Example 1.\n\n\npiqq\u00af\" G pIpjqq\u00b4G pIpj\u00b41qq @j P n . (16)\n\nCorollary 1 .\n1If the weights w and biases b describing the neural network are rational, then the single-neuron relaxation (4) can be solved in polynomial time on the encoding sizes of w and b.\n\n\n1 and Algorithm 3 (which requires Algorithm 2) details our new algorithm proposed in Section 4.3. Finally, Algorithm 4 establishes how to compute bounds for the entire network, considering DeepPoly [34] and Fast-Lin [42] as possible initial methods.Observation 1. Algorithm 1 runs in T`OpC`Aq time. Algorithm 2 runs in OpAq time. Algorithm 3 runs in pK`1qT`OpKpC`Aqq time. Observation 2. Algorithm 4 takes OpN KpT`Aqq time if K \u011b 1. If K \" 0, then Algorithm 4 takes OpN pT`Aqq time.\n\n\nThis pair is used by algorithms such as Fast-Lin [42], DeepZ [33], Neurify [41], and that of Wong and Kolter [43]. Algorithms such as DeepPoly [34] and CROWN-Ada [47]\n\nTable 1 :\n1Number of images verified and average verification times per image for a set of networks from the ERAN dataset [36]. ConvS and ConvB denote ConvSmall and ConvBig respectively. Results for RefineZono and kPoly are taken from [32]. bounds at each neuron. In addition, we compare them with RefineZono [35] and kPoly [32], two state-of-the-art incomplete verification methods.MNIST \nCIFAR-10 \n\n\n\n\nKrishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy A Mann, and Pushmeet Kohli. A dual approach to scalable verification of deep networks. In UAI, volume 1, page 2, 2018. [15] Ruediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In International Symposium on Automated Technology for Verification and Analysis, pages 269-286. Springer, 2017. Nina Narodytska, Shiva Kasiviswanathan, Leonid Ryzhyk, Mooly Sagiv, and Toby Walsh. Verifying properties of binarized deep neural networks. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In IEEE European Symposium on Security and Privacy, pages 372-387, March 2016. [28] Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In Advances in Neural Information Processing Systems, pages 10877-10887, 2018. [29] Ansgar R\u00f6ssig. Verification of neural networks. Technical Report 19-40, ZIB, Takustr. 7, 14195 Berlin, 2019. [30] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to tight robustness verification of neural networks. In Advances in Neural Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. Boosting robustness certification of neural networks. In International Conference on Learning Representations, 2019. [36] Gagandeep Singh, Jonathan Maurer, Christoph M\u00fcller, Matthew Mirman, Timon Gehr, Adrian Hoffmann, Petar Tsankov, Dana Drachsler Cohen, Markus P\u00fcschel, and Martin Vechev. ERAN verification dataset. https://github.com/eth-sri/eran. [37] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations, 2014. [38] Mohit Tawarmalani, Jean-Philippe P Richard, and Chuanhui Xiong. Explicit convex and concave envelopes through polyhedral subdivisions. Vincent Tjeng, Kai Xiao, and Russ Tedrake. Verifying neural networks with mixed integer programming. In International Conference on Learning Representations, 2019. [40] M.J. Todd. The Computation of Fixed Points and Applications. Lecture Notes in Mathematics; 513. Springer-Verlag, 1976. [41] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety analysis of neural networks. In Advances in Neural Information Processing Systems, pages 6367-6377, 2018. [42] Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 5276-5285, Stockholmsm\u00e4ssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. [43] Eric Wong and J. Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning, pages 5286-5295, 2018. [44] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial defenses. In 32nd Conference on Neural Information Processing Systems, 2018. [45] Weiming Xiang, Hoang-Dung Tran, and Taylor T Johnson. Output reachable set estimation and verification for multilayer neural networks. IEEE Transactions on Neural Networks and Kai Y. Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, and Aleksander Madry. Training for faster adversarial robustness verification via inducing ReLU stability. In International Conference on Learning Representations, 2019. [47] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In Advances in neural information processing systems, pages 4939-4948, 2018. [48] Chen Zhu, Renkun Ni, Ping-yeh Chiang, Hengduo Li, Furong Huang, and Tom Goldstein.Improving the tightness of convex relaxation bounds for training certifiably robust classifiers.[6] Francis Bach. Learning with submodular functions: A convex optimization perspective. Foun-\ndations and Trends R in Machine Learning, 6(2-3):145-373, 2013. \n\n[7] Dimitris Bertsimas and John Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, \n1997. \n\n[8] Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth Misener. Ef-\nficient verification of ReLU-based neural networks via dependency analysis. In Thirty-Fourth \nAAAI Conference on Artificial Intelligence, 2020. \n\n[9] Rudy Bunel, Jingyue Lu, Ilker Turkaslan, P Kohli, P Torr, and P Mudigonda. Branch and \nbound for piecewise linear neural network verification. Journal of Machine Learning Research, \n21(2020), 2020. \n\n[10] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In \n2017 IEEE Symposium on Security and Privacy (SP), pages 39-57, 2017. \n\n[11] Chih-Hong Cheng, Georg N\u00fchrenberg, and Harald Ruess. Maximum resilience of artificial \nneural networks. In International Symposium on Automated Technology for Verification and \nAnalysis, pages 251-268. Springer, 2017. \n\n[12] Michele Conforti, G\u00e9rard Cornu\u00e9jols, and Giacomo Zambelli. Integer programming, volume \n271. Springer, 2014. \n\n[13] Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. Output range \nanalysis for deep feedforward neural networks. In NASA Formal Methods Symposium, pages \n121-138. Springer, 2018. \n\n[14] [16] Matteo Fischetti and Jason Jo. Deep neural networks as 0-1 mixed integer linear programs: A \nfeasibility study. Constraints, 23:296-309, 2018. \n\n[17] Martin Gr\u00f6tschel, L\u00e1szl\u00f3 Lov\u00e1sz, and Alexander Schrijver. Geometric algorithms and combi-\nnatorial optimization, volume 2. Springer Science & Business Media, 2012. \n\n[18] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep \nneural networks. In International Conference on Computer Aided Verification, pages 3-29. \nSpringer, 2017. \n\n[19] Guy Katz, Clark Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. Reluplex: \nAn efficient SMT solver for verifying deep neural networks. In International Conference on \nComputer Aided Verification, pages 97-117, 2017. \n\n[20] Bernhard Korte and Jens Vygen. Combinatorial Optimization: Theory and Algorithms. \nSpringer, 2000. \n\n[21] Changliu Liu, Tomer Arnon, Christopher Lazarus, Clark Barrett, and Mykel J Kochenderfer. \nAlgorithms for verifying deep neural networks. arXiv preprint arXiv:1903.06758, 2019. \n\n[22] Chen Liu, Mathieu Salzmann, and Sabine S\u00fcsstrunk. Training provably robust models by \npolyhedral envelope regularization. arXiv, pages arXiv-1912, 2019. \n\n[23] Alessio Lomuscio and Lalit Maganti. An approach to reachability analysis for feed-forward \nReLU neural networks. arXiv preprint arXiv:1706.07351, 2017. \n\n[24] Zhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, and Luca Daniel. \nFastened CROWN: Tightened neural network robustness certificates. \narXiv preprint \narXiv:1912.00574, 2019. \n\n[25] [26] Patrick E O'Neil. Hyperplane cuts of an n-cube. Discrete Mathematics, 1(2):193-195, 1971. \n[27] Information Processing Systems, pages 9832-9842, 2019. \n[31] Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker. Towards verification \nof artificial neural networks. In MBMV, pages 30-40, 2015. \n[32] Gagandeep Singh, Rupanshu Ganvir, Markus P\u00fcschel, and Martin Vechev. Beyond the single \nneuron convex barrier for neural network certification. In Advances in Neural Information \nProcessing Systems, pages 15072-15083, 2019. \n[33] Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, and Martin Vechev. Fast \nand effective robustness certification. In Advances in Neural Information Processing Systems, \npages 10802-10813, 2018. \n[34] Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. An abstract domain for \ncertifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):1-\n30, 2019. \n[35] Mathematical Programming, 138(1-2):531-\n577, 2013. \n[39] Learning Systems, 29(11):5777-5783, 2018. \n[46] arXiv preprint arXiv:2002.09766, 2020. \n\nSometimes also called the triangle relaxation [21, 32].2 This allows us to consider feedforward networks, including those that skip layers (e.g. see[30, 48]).\nHere, \"small\" means the number of variables and constraints is Op# of neuronsq.\nNote that these functions satisfy (8) only when\u00dbi \u0105 0 andLi \u0103 0.\nOur framework supports initializing from the Fast-Lin inequalities as well, but it has been observed that the inequalities from DeepPoly perform better computationally.\nSee also[5, Theorem 7.9]: the proof of [26,Lemma 2]  can be readily adapted to accommodate non-strict, rather than strict, linear inequalities.\nSuch an affine interpolation exists and is unique because V\u03c0 is a set of n`1 affinely independent points.\nAlgorithm 1 The Backwards Pass for Upper Bounds 1: Inputs:Input domain X \u010e R m , affine functions L i pz 1:i\u00b41 q \" \u0159 i\u00b41 j\"1 w l ij z j`b l i , U i pz 1:i\u00b41 q \" \u0159 i\u00b41 j\"1 w u ij z j`b u i for each i \" m`1, . . . , \u03b7, and affine function Cpzq \" \u0159 \u03b7 i\"1 c i z i`b 2: Outputs:Upper bound on Cpzq, optimal point x\u02daP X, and boolean vector pub_used m`1 , . . . , ub_used \u03b7 q 3: function PROPAGATIONBOUND(X, L, U, C)4:ub_used i \u00d0 false for all i \" m`1, . . . , \u03b7 5:An upper bound on max xPX Cpxq 3: function TIGHTENEDPROPAGATIONBOUND(X, L, U, C, k) 4:for iter \" 1, . . . , k do 7:for i \" m`1, . . . , \u03b7 do 9:end for 12:end for 15:return B 16: end function Proposition 4. The solution z\u02dareturned by Algorithm 2 is optimal for the relaxed problem (9).Proof. Denote by expr k def \" \u0159 jPJ k w k j z j`b k the expression expr at the end of iteration k \" 1, . . . , K of the while loop in Algorithm 1, for some subsets J 1 , . . . , J K \u010e \u03b7 , and let expr 0 be the initial expr as defined in line 5, i.e. Cpzq. For each k \" 0, . . . , K\u00b41, we obtain expr k`1 by replacing, for some i, z i by U i pz 1:i\u00b41 q if w k i \u0105 0, or by L i pz 1:i\u00b41 q if if w k i \u0103 0. Note that if w k i \" 0, we can safely ignore any substitution because it will not affect the expression. Due to the constraints (9c), this substitution implies that expr k \u010f expr k`1 for any z 1:m P X. This inductively establishes that, restricting to z 1:m P X,Note that J K \u010e tz 1 , . . . , z m u since we have made all the substitutions possible for i \u0105 m. Therefore, the optimal value of (9) is upper-bounded by the bound corresponding to the solution returned Algorithm 4 FastC2V Algorithm 1: Inputs: A feedforward neural network as defined in (1) (with input domain X, ReLU neurons i \" m`1, . . . , N , and a single affine output neuron indexed by N`1), initial_method P tDeepPoly, Fast-Linu, and number of iterations per neuron k \u011b 0 (note that if k \" 0, we recover DeepPoly or Fast-Lin) 2: Outputs:Lower and upper bounds tL i ,\u00db i u N`1 i\"1 on the pre-activation function (if ReLU) or output (if affine) of neuron i 3: function FASTC2V(X, W, b, initial_method, k)4:for i \" m`1, . . . , N`1 do 5:if i \" N`1 then break end if 9:\u22b2 Build bounding functions L i and U i for subsequent iterations 10:ifL i \u011b 0 then \u22b2 ReLU i is always active for any z 1:m P X 11:To see that this upper bound is achieved, observe that each inequality in (30) holds as equality if we substitute z j \" zj for all j, by construction of Algorithm 2 and boolean vector pub_used m`1 , . . . , ub_used \u03b7 q. Note also that z\u02dasatisfies (9c) by construction. That is, we have a feasible z\u02dasuch that Cpz\u02daq is no less than the optimal value of (9), and thus z\u02damust be an optimal solution.We would like to highlight to the interested reader that this result can also be derived from an argument using Fourier-Motzkin elimination [7, Chapter 2.8] to project out the intermediate variables z m`1:\u03b7 . Notably, as each inequality neuron has exactly one inequality upper bounding and one inequality lower bounding its post-activation value, this projection does not produce an \"explosion\" of new inequalities as is typically observed when applying Fourier-Motzkin to an arbitrary polyhedron.Define C def \" | t i P \u03b7 | c i \u2030 0 u | and suppose that we use the affine bounding inequalities from Fast-Lin or DeepPoly. Let K be the number of iterations in Algorithm 3, T be the time required to maximize an arbitrary affine function over X, and A be the number of arcs in the network (i.e. nonzero weights).\nMaximizing a class of submodular utility functions. Mathematical programming. Shabbir Ahmed, Alper Atamt\u00fcrk, 128Shabbir Ahmed and Alper Atamt\u00fcrk. Maximizing a class of submodular utility functions. Mathematical programming, 128(1-2):149-169, 2011.\n\nTightened convex relaxations for neural network robustness certification. Ziye Brendon G Anderson, Jingqi Ma, Somayeh Li, Sojoudi, arXiv:2004.00570arXiv preprintBrendon G Anderson, Ziye Ma, Jingqi Li, and Somayeh Sojoudi. Tightened convex relaxations for neural network robustness certification. arXiv preprint arXiv:2004.00570, 2020.\n\nStrong mixed-integer programming formulations for trained neural networks. Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, Juan Pablo Vielma, Mathematical Programming. Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong mixed-integer programming formulations for trained neural networks. Mathematical Programming, pages 1-37, 2020.\n\nStrong mixed-integer programming formulations for trained neural networks. Ross Anderson, Joey Huchette, Christian Tjandraatmadja, Juan Pablo Vielma, Proceedings of the 20th Conference on Integer Programming and Combinatorial Optimization (IPCO 2019). A. Lodi and V. Nagarajanthe 20th Conference on Integer Programming and Combinatorial Optimization (IPCO 2019)11480Ross Anderson, Joey Huchette, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong mixed-integer programming formulations for trained neural networks. In A. Lodi and V. Na- garajan, editors, Proceedings of the 20th Conference on Integer Programming and Combina- torial Optimization (IPCO 2019), volume 11480 of Lecture Notes in Computer Science, pages 27-42, 2019.\n\nDiscrete mathematics of neural networks: selected topics. Martin Anthony, SIAM8Martin Anthony. Discrete mathematics of neural networks: selected topics, volume 8. SIAM, 2001.\n", "annotations": {"author": "[{\"end\":187,\"start\":124},{\"end\":242,\"start\":188},{\"end\":295,\"start\":243},{\"end\":350,\"start\":296},{\"end\":424,\"start\":351},{\"end\":471,\"start\":425},{\"end\":542,\"start\":472},{\"end\":597,\"start\":543},{\"end\":647,\"start\":598},{\"end\":709,\"start\":648},{\"end\":752,\"start\":710}]", "publisher": null, "author_last_name": "[{\"end\":148,\"start\":134},{\"end\":203,\"start\":195},{\"end\":256,\"start\":248},{\"end\":311,\"start\":303},{\"end\":364,\"start\":356},{\"end\":432,\"start\":430},{\"end\":484,\"start\":479},{\"end\":558,\"start\":550},{\"end\":608,\"start\":603},{\"end\":670,\"start\":662}]", "author_first_name": "[{\"end\":133,\"start\":124},{\"end\":194,\"start\":188},{\"end\":247,\"start\":243},{\"end\":302,\"start\":296},{\"end\":355,\"start\":351},{\"end\":429,\"start\":425},{\"end\":478,\"start\":472},{\"end\":549,\"start\":543},{\"end\":602,\"start\":598},{\"end\":654,\"start\":648},{\"end\":661,\"start\":655},{\"end\":713,\"start\":710}]", "author_affiliation": "[{\"end\":186,\"start\":150},{\"end\":241,\"start\":205},{\"end\":294,\"start\":258},{\"end\":349,\"start\":313},{\"end\":423,\"start\":387},{\"end\":470,\"start\":434},{\"end\":541,\"start\":505},{\"end\":596,\"start\":560},{\"end\":646,\"start\":610},{\"end\":708,\"start\":672},{\"end\":751,\"start\":715}]", "title": "[{\"end\":110,\"start\":1},{\"end\":862,\"start\":753}]", "venue": null, "abstract": "[{\"end\":2348,\"start\":875}]", "bib_ref": "[{\"end\":2516,\"start\":2512},{\"end\":2519,\"start\":2516},{\"end\":2522,\"start\":2519},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2724,\"start\":2721},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2726,\"start\":2724},{\"end\":2728,\"start\":2726},{\"end\":2730,\"start\":2728},{\"end\":2733,\"start\":2730},{\"end\":2736,\"start\":2733},{\"end\":2739,\"start\":2736},{\"end\":2742,\"start\":2739},{\"end\":2745,\"start\":2742},{\"end\":2748,\"start\":2745},{\"end\":2751,\"start\":2748},{\"end\":2796,\"start\":2792},{\"end\":2799,\"start\":2796},{\"end\":2802,\"start\":2799},{\"end\":2805,\"start\":2802},{\"end\":2807,\"start\":2805},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3193,\"start\":3190},{\"end\":3196,\"start\":3193},{\"end\":3199,\"start\":3196},{\"end\":3202,\"start\":3199},{\"end\":3205,\"start\":3202},{\"end\":3208,\"start\":3205},{\"end\":3211,\"start\":3208},{\"end\":3214,\"start\":3211},{\"end\":3217,\"start\":3214},{\"end\":3393,\"start\":3389},{\"end\":3396,\"start\":3393},{\"end\":3399,\"start\":3396},{\"end\":3402,\"start\":3399},{\"end\":3405,\"start\":3402},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4052,\"start\":4051},{\"end\":4247,\"start\":4243},{\"end\":4710,\"start\":4706},{\"end\":5234,\"start\":5230},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5613,\"start\":5610},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5615,\"start\":5613},{\"end\":5913,\"start\":5909},{\"end\":6545,\"start\":6541},{\"end\":6952,\"start\":6948},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7158,\"start\":7157},{\"end\":8205,\"start\":8201},{\"end\":10366,\"start\":10362},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12153,\"start\":12150},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13080,\"start\":13077},{\"end\":13082,\"start\":13080},{\"end\":13085,\"start\":13082},{\"end\":13087,\"start\":13085},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13195,\"start\":13194},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13412,\"start\":13411},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13621,\"start\":13620},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13718,\"start\":13715},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13720,\"start\":13718},{\"end\":17834,\"start\":17830},{\"end\":17837,\"start\":17834},{\"end\":19879,\"start\":19878},{\"end\":25011,\"start\":25007},{\"end\":26139,\"start\":26135},{\"end\":26239,\"start\":26235},{\"end\":26861,\"start\":26857},{\"end\":38344,\"start\":38340},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39742,\"start\":39739},{\"end\":39754,\"start\":39742},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":46101,\"start\":46098},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":46122,\"start\":46119},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":46654,\"start\":46651},{\"end\":46668,\"start\":46654},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":61655,\"start\":61652},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":73446,\"start\":73445},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":73874,\"start\":73871}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":63366,\"start\":63044},{\"attributes\":{\"id\":\"fig_1\"},\"end\":63634,\"start\":63367},{\"attributes\":{\"id\":\"fig_2\"},\"end\":63649,\"start\":63635},{\"attributes\":{\"id\":\"fig_3\"},\"end\":63664,\"start\":63650},{\"attributes\":{\"id\":\"fig_5\"},\"end\":63746,\"start\":63665},{\"attributes\":{\"id\":\"fig_6\"},\"end\":63789,\"start\":63747},{\"attributes\":{\"id\":\"fig_7\"},\"end\":63984,\"start\":63790},{\"attributes\":{\"id\":\"fig_8\"},\"end\":64469,\"start\":63985},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":64638,\"start\":64470},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":65041,\"start\":64639},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":73389,\"start\":65042}]", "paragraph": "[{\"end\":3767,\"start\":2364},{\"end\":4823,\"start\":3769},{\"end\":5357,\"start\":4825},{\"end\":5411,\"start\":5359},{\"end\":5980,\"start\":5413},{\"end\":5987,\"start\":5982},{\"end\":7019,\"start\":5989},{\"end\":7352,\"start\":7066},{\"end\":7399,\"start\":7354},{\"end\":7611,\"start\":7412},{\"end\":7888,\"start\":7613},{\"end\":8081,\"start\":7890},{\"end\":8425,\"start\":8144},{\"end\":9148,\"start\":8427},{\"end\":9216,\"start\":9203},{\"end\":9799,\"start\":9218},{\"end\":10730,\"start\":9907},{\"end\":11423,\"start\":10781},{\"end\":11520,\"start\":11425},{\"end\":11953,\"start\":11599},{\"end\":12489,\"start\":12054},{\"end\":12749,\"start\":12545},{\"end\":12793,\"start\":12751},{\"end\":13721,\"start\":12999},{\"end\":14005,\"start\":13820},{\"end\":14445,\"start\":14007},{\"end\":14849,\"start\":14447},{\"end\":15274,\"start\":14931},{\"end\":15696,\"start\":15276},{\"end\":16056,\"start\":15698},{\"end\":16133,\"start\":16058},{\"end\":16547,\"start\":16167},{\"end\":17292,\"start\":16608},{\"end\":17478,\"start\":17294},{\"end\":17608,\"start\":17530},{\"end\":17693,\"start\":17610},{\"end\":18708,\"start\":17695},{\"end\":19131,\"start\":18745},{\"end\":19259,\"start\":19183},{\"end\":19565,\"start\":19297},{\"end\":20163,\"start\":19676},{\"end\":20258,\"start\":20165},{\"end\":20636,\"start\":20360},{\"end\":21209,\"start\":20716},{\"end\":21921,\"start\":21211},{\"end\":22739,\"start\":21923},{\"end\":23367,\"start\":22791},{\"end\":23717,\"start\":23369},{\"end\":24021,\"start\":23719},{\"end\":24573,\"start\":24023},{\"end\":24757,\"start\":24575},{\"end\":25823,\"start\":24759},{\"end\":26018,\"start\":25870},{\"end\":26990,\"start\":26020},{\"end\":27639,\"start\":26992},{\"end\":28145,\"start\":27665},{\"end\":28257,\"start\":28147},{\"end\":28551,\"start\":28259},{\"end\":29226,\"start\":28553},{\"end\":30345,\"start\":29228},{\"end\":31463,\"start\":30364},{\"end\":31824,\"start\":31488},{\"end\":32703,\"start\":31826},{\"end\":33291,\"start\":32705},{\"end\":33472,\"start\":33293},{\"end\":33533,\"start\":33508},{\"end\":33849,\"start\":33580},{\"end\":34263,\"start\":33851},{\"end\":34864,\"start\":34265},{\"end\":35154,\"start\":34866},{\"end\":36005,\"start\":35214},{\"end\":36242,\"start\":36121},{\"end\":36585,\"start\":36244},{\"end\":37239,\"start\":36587},{\"end\":37498,\"start\":37241},{\"end\":37604,\"start\":37560},{\"end\":37957,\"start\":37606},{\"end\":38320,\"start\":37986},{\"end\":38730,\"start\":38322},{\"end\":38924,\"start\":38732},{\"end\":39591,\"start\":38992},{\"end\":40248,\"start\":39593},{\"end\":40677,\"start\":40250},{\"end\":41838,\"start\":40679},{\"end\":42690,\"start\":41840},{\"end\":43148,\"start\":42692},{\"end\":43537,\"start\":43150},{\"end\":43822,\"start\":43637},{\"end\":44161,\"start\":43824},{\"end\":44279,\"start\":44163},{\"end\":44402,\"start\":44281},{\"end\":44440,\"start\":44404},{\"end\":44554,\"start\":44496},{\"end\":44613,\"start\":44565},{\"end\":44826,\"start\":44771},{\"end\":44940,\"start\":44828},{\"end\":44945,\"start\":44942},{\"end\":45110,\"start\":44947},{\"end\":45121,\"start\":45112},{\"end\":45180,\"start\":45123},{\"end\":45417,\"start\":45243},{\"end\":45613,\"start\":45577},{\"end\":45940,\"start\":45615},{\"end\":46272,\"start\":46016},{\"end\":46629,\"start\":46534},{\"end\":46724,\"start\":46631},{\"end\":47022,\"start\":47004},{\"end\":47090,\"start\":47024},{\"end\":47190,\"start\":47092},{\"end\":47568,\"start\":47192},{\"end\":47784,\"start\":47570},{\"end\":48069,\"start\":47906},{\"end\":48414,\"start\":48071},{\"end\":48532,\"start\":48416},{\"end\":48655,\"start\":48534},{\"end\":48701,\"start\":48657},{\"end\":48997,\"start\":48775},{\"end\":49078,\"start\":49055},{\"end\":49083,\"start\":49080},{\"end\":49087,\"start\":49085},{\"end\":49206,\"start\":49089},{\"end\":49850,\"start\":49208},{\"end\":49959,\"start\":49877},{\"end\":50057,\"start\":50022},{\"end\":50255,\"start\":50177},{\"end\":51326,\"start\":50405},{\"end\":51778,\"start\":51401},{\"end\":52284,\"start\":51873},{\"end\":52568,\"start\":52286},{\"end\":52735,\"start\":52570},{\"end\":53013,\"start\":52737},{\"end\":53105,\"start\":53041},{\"end\":53274,\"start\":53213},{\"end\":53499,\"start\":53353},{\"end\":53690,\"start\":53501},{\"end\":53761,\"start\":53692},{\"end\":54190,\"start\":53880},{\"end\":54258,\"start\":54192},{\"end\":54332,\"start\":54324},{\"end\":54404,\"start\":54334},{\"end\":54609,\"start\":54497},{\"end\":55017,\"start\":54611},{\"end\":55386,\"start\":55019},{\"end\":56339,\"start\":55443},{\"end\":56431,\"start\":56409},{\"end\":56879,\"start\":56517},{\"end\":57498,\"start\":56881},{\"end\":57659,\"start\":57546},{\"end\":57809,\"start\":57661},{\"end\":57942,\"start\":57811},{\"end\":58509,\"start\":57944},{\"end\":58535,\"start\":58511},{\"end\":58681,\"start\":58582},{\"end\":58896,\"start\":58683},{\"end\":59092,\"start\":58898},{\"end\":59197,\"start\":59121},{\"end\":59284,\"start\":59199},{\"end\":60021,\"start\":59286},{\"end\":60099,\"start\":60023},{\"end\":61318,\"start\":60101},{\"end\":62054,\"start\":61320},{\"end\":62123,\"start\":62056},{\"end\":62713,\"start\":62125},{\"end\":62808,\"start\":62715},{\"end\":63043,\"start\":62810}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7411,\"start\":7400},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8143,\"start\":8082},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9906,\"start\":9800},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11598,\"start\":11521},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12053,\"start\":11954},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12998,\"start\":12794},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13819,\"start\":13722},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14930,\"start\":14850},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17529,\"start\":17479},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19182,\"start\":19132},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19296,\"start\":19260},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19675,\"start\":19566},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20359,\"start\":20259},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25869,\"start\":25824},{\"attributes\":{\"id\":\"formula_16\"},\"end\":35213,\"start\":35155},{\"attributes\":{\"id\":\"formula_17\"},\"end\":36120,\"start\":36006},{\"attributes\":{\"id\":\"formula_18\"},\"end\":38991,\"start\":38925},{\"attributes\":{\"id\":\"formula_19\"},\"end\":43636,\"start\":43538},{\"attributes\":{\"id\":\"formula_20\"},\"end\":44495,\"start\":44441},{\"attributes\":{\"id\":\"formula_21\"},\"end\":44564,\"start\":44555},{\"attributes\":{\"id\":\"formula_22\"},\"end\":44770,\"start\":44614},{\"attributes\":{\"id\":\"formula_24\"},\"end\":45242,\"start\":45181},{\"attributes\":{\"id\":\"formula_25\"},\"end\":45508,\"start\":45418},{\"attributes\":{\"id\":\"formula_26\"},\"end\":45576,\"start\":45508},{\"attributes\":{\"id\":\"formula_27\"},\"end\":46533,\"start\":46273},{\"attributes\":{\"id\":\"formula_28\"},\"end\":47003,\"start\":46725},{\"attributes\":{\"id\":\"formula_29\"},\"end\":47905,\"start\":47785},{\"attributes\":{\"id\":\"formula_30\"},\"end\":48774,\"start\":48702},{\"attributes\":{\"id\":\"formula_31\"},\"end\":49054,\"start\":48998},{\"attributes\":{\"id\":\"formula_32\"},\"end\":49876,\"start\":49851},{\"attributes\":{\"id\":\"formula_33\"},\"end\":50021,\"start\":49960},{\"attributes\":{\"id\":\"formula_34\"},\"end\":50176,\"start\":50058},{\"attributes\":{\"id\":\"formula_35\"},\"end\":50404,\"start\":50256},{\"attributes\":{\"id\":\"formula_36\"},\"end\":51400,\"start\":51327},{\"attributes\":{\"id\":\"formula_37\"},\"end\":51872,\"start\":51779},{\"attributes\":{\"id\":\"formula_38\"},\"end\":53040,\"start\":53014},{\"attributes\":{\"id\":\"formula_39\"},\"end\":53212,\"start\":53106},{\"attributes\":{\"id\":\"formula_40\"},\"end\":53352,\"start\":53275},{\"attributes\":{\"id\":\"formula_42\"},\"end\":53864,\"start\":53762},{\"attributes\":{\"id\":\"formula_43\"},\"end\":53879,\"start\":53864},{\"attributes\":{\"id\":\"formula_44\"},\"end\":54323,\"start\":54259},{\"attributes\":{\"id\":\"formula_45\"},\"end\":54496,\"start\":54405},{\"attributes\":{\"id\":\"formula_47\"},\"end\":55442,\"start\":55387},{\"attributes\":{\"id\":\"formula_48\"},\"end\":56408,\"start\":56340},{\"attributes\":{\"id\":\"formula_49\"},\"end\":56516,\"start\":56432},{\"attributes\":{\"id\":\"formula_50\"},\"end\":57545,\"start\":57499}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27701,\"start\":27694}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2362,\"start\":2350},{\"attributes\":{\"n\":\"2\"},\"end\":7064,\"start\":7022},{\"attributes\":{\"n\":\"2.1\"},\"end\":9201,\"start\":9151},{\"attributes\":{\"n\":\"2.2\"},\"end\":10779,\"start\":10733},{\"attributes\":{\"n\":\"3\"},\"end\":12543,\"start\":12492},{\"attributes\":{\"n\":\"4\"},\"end\":16165,\"start\":16136},{\"attributes\":{\"n\":\"4.1\"},\"end\":16606,\"start\":16550},{\"attributes\":{\"n\":\"4.2\"},\"end\":18743,\"start\":18711},{\"attributes\":{\"n\":\"4.3\"},\"end\":20714,\"start\":20639},{\"attributes\":{\"n\":\"5\"},\"end\":22767,\"start\":22742},{\"attributes\":{\"n\":\"5.1\"},\"end\":22789,\"start\":22770},{\"attributes\":{\"n\":\"5.2\"},\"end\":27663,\"start\":27642},{\"end\":30362,\"start\":30348},{\"end\":31486,\"start\":31466},{\"end\":33506,\"start\":33475},{\"end\":33578,\"start\":33536},{\"end\":37558,\"start\":37501},{\"end\":37984,\"start\":37960},{\"end\":46014,\"start\":45943},{\"end\":58580,\"start\":58538},{\"end\":59119,\"start\":59095},{\"end\":63057,\"start\":63045},{\"end\":63378,\"start\":63368},{\"end\":63676,\"start\":63666},{\"end\":63804,\"start\":63791},{\"end\":64649,\"start\":64640}]", "table": "[{\"end\":65041,\"start\":65023},{\"end\":73389,\"start\":69289}]", "figure_caption": "[{\"end\":63366,\"start\":63064},{\"end\":63634,\"start\":63380},{\"end\":63649,\"start\":63637},{\"end\":63664,\"start\":63652},{\"end\":63746,\"start\":63678},{\"end\":63789,\"start\":63749},{\"end\":63984,\"start\":63806},{\"end\":64469,\"start\":63987},{\"end\":64638,\"start\":64472},{\"end\":65023,\"start\":64651},{\"end\":69289,\"start\":65044}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12135,\"start\":12127},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19811,\"start\":19803},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33803,\"start\":33795},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34169,\"start\":34152},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34370,\"start\":34352},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34407,\"start\":34398},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34673,\"start\":34664},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35013,\"start\":35004},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35410,\"start\":35401},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36689,\"start\":36680},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36985,\"start\":36976},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37076,\"start\":37067},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37237,\"start\":37228}]", "bib_author_first_name": "[{\"end\":77713,\"start\":77706},{\"end\":77726,\"start\":77721},{\"end\":77955,\"start\":77951},{\"end\":77982,\"start\":77976},{\"end\":77994,\"start\":77987},{\"end\":78292,\"start\":78288},{\"end\":78307,\"start\":78303},{\"end\":78322,\"start\":78318},{\"end\":78336,\"start\":78327},{\"end\":78357,\"start\":78353},{\"end\":78363,\"start\":78358},{\"end\":78685,\"start\":78681},{\"end\":78700,\"start\":78696},{\"end\":78720,\"start\":78711},{\"end\":78741,\"start\":78737},{\"end\":78747,\"start\":78742},{\"end\":79405,\"start\":79399}]", "bib_author_last_name": "[{\"end\":77719,\"start\":77714},{\"end\":77735,\"start\":77727},{\"end\":77974,\"start\":77956},{\"end\":77985,\"start\":77983},{\"end\":77997,\"start\":77995},{\"end\":78006,\"start\":77999},{\"end\":78301,\"start\":78293},{\"end\":78316,\"start\":78308},{\"end\":78325,\"start\":78323},{\"end\":78351,\"start\":78337},{\"end\":78370,\"start\":78364},{\"end\":78694,\"start\":78686},{\"end\":78709,\"start\":78701},{\"end\":78735,\"start\":78721},{\"end\":78754,\"start\":78748},{\"end\":79413,\"start\":79406}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":77875,\"start\":77628},{\"attributes\":{\"doi\":\"arXiv:2004.00570\",\"id\":\"b1\"},\"end\":78211,\"start\":77877},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":53760231},\"end\":78604,\"start\":78213},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":53760231},\"end\":79339,\"start\":78606},{\"attributes\":{\"id\":\"b4\"},\"end\":79515,\"start\":79341}]", "bib_title": "[{\"end\":78286,\"start\":78213},{\"end\":78679,\"start\":78606}]", "bib_author": "[{\"end\":77721,\"start\":77706},{\"end\":77737,\"start\":77721},{\"end\":77976,\"start\":77951},{\"end\":77987,\"start\":77976},{\"end\":77999,\"start\":77987},{\"end\":78008,\"start\":77999},{\"end\":78303,\"start\":78288},{\"end\":78318,\"start\":78303},{\"end\":78327,\"start\":78318},{\"end\":78353,\"start\":78327},{\"end\":78372,\"start\":78353},{\"end\":78696,\"start\":78681},{\"end\":78711,\"start\":78696},{\"end\":78737,\"start\":78711},{\"end\":78756,\"start\":78737},{\"end\":79415,\"start\":79399}]", "bib_venue": "[{\"end\":77704,\"start\":77628},{\"end\":77949,\"start\":77877},{\"end\":78396,\"start\":78372},{\"end\":78856,\"start\":78756},{\"end\":79397,\"start\":79341},{\"end\":78967,\"start\":78882}]"}}}, "year": 2023, "month": 12, "day": 17}