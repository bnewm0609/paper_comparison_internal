{"id": 224828219, "updated": "2023-10-06 10:11:10.407", "metadata": {"title": "Learning Quadrupedal Locomotion over Challenging Terrain", "authors": "[{\"first\":\"Joonho\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Jemin\",\"last\":\"Hwangbo\",\"middle\":[]},{\"first\":\"Lorenz\",\"last\":\"Wellhausen\",\"middle\":[]},{\"first\":\"Vladlen\",\"last\":\"Koltun\",\"middle\":[]},{\"first\":\"Marco\",\"last\":\"Hutter\",\"middle\":[]}]", "venue": "Science Robotics 2020 Vol. 5, Issue 47, eabc5986", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Some of the most challenging environments on our planet are accessible to quadrupedal animals but remain out of reach for autonomous machines. Legged locomotion can dramatically expand the operational domains of robotics. However, conventional controllers for legged locomotion are based on elaborate state machines that explicitly trigger the execution of motion primitives and reflexes. These designs have escalated in complexity while falling short of the generality and robustness of animal locomotion. Here we present a radically robust controller for legged locomotion in challenging natural environments. We present a novel solution to incorporating proprioceptive feedback in locomotion control and demonstrate remarkable zero-shot generalization from simulation to natural environments. The controller is trained by reinforcement learning in simulation. It is based on a neural network that acts on a stream of proprioceptive signals. The trained controller has taken two generations of quadrupedal ANYmal robots to a variety of natural environments that are beyond the reach of prior published work in legged locomotion. The controller retains its robustness under conditions that have never been encountered during training: deformable terrain such as mud and snow, dynamic footholds such as rubble, and overground impediments such as thick vegetation and gushing water. The presented work opens new frontiers for robotics and indicates that radical robustness in natural environments can be achieved by training in much simpler domains.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2010.11251", "mag": "3104876774", "acl": null, "pubmed": "33087482", "pubmedcentral": null, "dblp": "journals/scirobotics/HwangboWK020", "doi": "10.1126/scirobotics.abc5986"}}, "content": {"source": {"pdf_hash": "ad47de9f8e7ad20a81daa821b92925185fafba74", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2010.11251v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://www.research-collection.ethz.ch/bitstream/20.500.11850/448343/1/2020_science_robotics_lee_locomotion.pdf", "status": "GREEN"}}, "grobid": {"id": "d1ffdb9f0186866330bd526f284b74f8e2597d63", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ad47de9f8e7ad20a81daa821b92925185fafba74.txt", "contents": "\nLearning Quadrupedal Locomotion over Challenging Terrain\n2020\n\nJoonho Lee \nRobotic Systems Lab\nETH Zurich\nZurichSwitzerland\n\nJemin Hwangbo \nRobotic Systems Lab\nETH Zurich\nZurichSwitzerland\n\nRobotics and Artificial Intelligence Lab\nKAIST\nDaejeonKorea\n\nLorenz Wellhausen \nRobotic Systems Lab\nETH Zurich\nZurichSwitzerland\n\nVladlen Koltun \nIntelligent Systems Lab\nIntel\nSanta ClaraCAUSA\n\nMarco Hutter \nRobotic Systems Lab\nETH Zurich\nZurichSwitzerland\n\nLearning Quadrupedal Locomotion over Challenging Terrain\n\nThis is the accepted version of Science Robotics\n55986202010.1126/scirobotics.abc5986Research Article ETH Zurich and Intel 1 \u2020 Substantial part of the work was carried out during his stay at 1\nSome of the most challenging environments on our planet are accessible to quadrupedal animals but remain out of reach for autonomous machines. Legged locomotion can dramatically expand the operational domains of robotics. However, conventional controllers for legged locomotion are based on elaborate state machines that explicitly trigger the execution of motion primitives and reflexes. These designs have escalated in complexity while falling short of the generality and robustness of animal locomotion. Here we present a radically robust controller for legged locomotion in challenging natural environments. We present a novel solution to incorporating proprioceptive feedback in locomotion control and demonstrate remarkable zero-shot generalization from simulation to natural environments. The controller is trained by reinforcement learning in simulation. It is based on a neural network that acts on a stream of proprioceptive signals. The trained controller has taken two generations of quadrupedal ANYmal robots to a variety of natural environments that are beyond the reach of prior published work in legged locomotion. The controller retains its robustness under conditions that have never been encountered during training: deformable terrain such as mud and snow, dynamic footholds such as rubble, and overground impediments such as thick vegetation and gushing water. The presented work opens new frontiers for robotics and indicates that radical robustness in natural environments can be achieved by training in much simpler domains.\n\nINTRODUCTION\n\nLegged locomotion can dramatically expand the reach of robotics. Much of the dry landmass on Earth remains impassible to wheeled and tracked machines, the stability of which can be severely compromised on challenging terrain. Quadrupedal animals, on the other hand, can access some of the most remote parts of our planet. They can choose safe footholds within their kinematic reach and rapidly change their kinematic state in response to the environment. Legged robots have the potential to traverse any terrain that their animal counterparts can.\n\nTo date, no published work has demonstrated dynamic locomotion in diverse, challenging natural environments as shown in Fig. 1. These environments have highly irregular profiles, deformable terrain, slippery surfaces, and overground obstructions. Under such conditions, existing published controllers manifest frequent foot slippage, loss of balance, and ultimately catastrophic failure. The challenge is exacerbated by the inaccessibility of veridical information about the physical properties of the terrain. Exteroceptive sensors such as cameras and LiDAR cannot reliably measure physical characteristics such as friction and compliance, are impeded by obstructions such as vegetation, snow, and water, and may not have the coverage and temporal resolution to capture changes induced by the robot itself, such as the crumbling of loose ground under the robot's feet. Under these conditions, the robot must rely crucially on proprioception -the sensing of its own bodily configuration at high temporal resolution. In response to unforeseen events such as unexpected ground contact, terrain deformation, and foot slippage, the controller must rapidly produce whole-body trajectories subject to multiple objectives: balancing, avoiding self-collision, counteracting external disturbances, and locomotion. While animals instinctively solve this complex control problem, it is an open challenge in robotics.\n\nConventional approaches to legged locomotion on uneven terrain have yielded increasingly complex control architectures. Many rely on elaborate state machines that coordinate the execution of motion primitives and reflex controllers [1][2][3][4][5]. To trigger transitions between states or the execution of a reflex, many systems explicitly estimate states such as ground contact and slippage [6][7][8]. Such estimation is commonly based on empirically tuned thresholds and can become erratic in the presence of unmodeled factors such as mud, snow, or vegetation. Other systems employ contact sensors at the feet, which can become unreliable in field conditions [9][10][11]. Overall, conventional systems for legged locomotion on rough terrain escalate in complexity as more scenarios are taken into account, have become extremely laborious to develop and maintain, and remain vulnerable to corner cases.\n\nModel-free reinforcement learning (RL) has recently emerged as an alternative approach in the development of legged locomotion skills [12][13][14]. The idea of RL is to tune a controller to optimize a given reward function. The optimization is performed on data acquired by executing the controller itself, which improves with experience. RL has been used to simplify the design of locomotion controllers, automate parts of the design process, and learn behaviors that could not be engineered with prior approaches [12][13][14][15].\n\nHowever, application of RL to legged locomotion has largely been confined to laboratory environments and conditions. Our prior work demonstrated end-to-end learning of locomotion and recovery behaviors -but only on flat ground, in the lab [12]. Other work also developed RL techniques for legged locomotion, but likewise focused largely on flat or moderately textured surfaces in laboratory settings [13,14,[16][17][18][19].\n\nHere we present a radically robust controller for blind quadrupedal locomotion on challenging terrain. The controller uses only proprioceptive measurements from joint encoders and an inertial measurement unit (IMU), which are the most durable and reliable sensors on legged machines. The operation of the controller is shown in Fig. 1 and Movie 1. The controller was used to drive two generations of ANYmal quadrupeds [20] in a variety of conditions that are beyond the reach of prior published work in legged robotics. The controller reliably trots through mud, sand, rubble, thick vegetation, snow, running water, and a variety of other off-road terrain. The same controller was also used in our entry in the DARPA Subterranean Challenge Urban Circuit. In all deployments, robots of the same generation were driven by exactly the same controller under all conditions. No tuning was required to adapt to different environments.\n\nLike a number of prior applications of model-free RL to legged locomotion, we train the controller in simulation [12,14,16]. Prior efforts have established a number of practices for successful transfer of legged locomotion controllers from simulation to physical machines. One is realistic modeling of the physical system, including the actuators [12]. Another is randomization of physical parameters that vary between simulation and reality, such that the controller becomes robust to a range of conditions that cover those that arise in physical deployment, without the necessity to precisely model these conditions a priori [21].\n\nWe use these ideas as well, but have found that they were not sufficient to achieve robust locomotion on rough terrain. We therefore introduce and validate a number of additional ingredients that are crucial to realizing the presented skills. The first is a different policy architecture. Rather than using a multi-layer perceptron (MLP) that operates on a snapshot of the robot's current state, as was common in prior work, we use a sequence model, specifically a temporal convolutional network (TCN) [22] that produces actuation based on an extended history of proprioceptive states. We do not employ explicit contact and slip estimation modules, which are known to be brittle in challenging situations; rather, the TCN learns to implicitly reason about contact and slippage events from proprioceptive history as needed.\n\nThe second important idea that enables the demonstrated results is privileged learning [23]. We have found that training a rough-terrain locomotion policy directly via reinforcement learning was not successful: the supervisory signal was sparse and the presented network failed to learn locomotion within reasonable time budgets. Instead, we decompose the training process into two stages. First, we train a teacher policy that has access to privileged information, namely ground-truth knowledge of the terrain and the robot's contact with it. The privileged information enables the policy to quickly achieve high performance. We then use this privileged teacher to guide the learning of a purely proprioceptive student controller that only uses sensors that are available on the real robot. This privileged learning protocol is enabled by simulation, but the resulting proprioceptive policy is not confined to simulation and is deployed on physical machines.\n\nThe third idea that has proven important in achieving the presented levels of robustness is an automated curriculum that synthesizes terrains adaptively, based on the controller's performance at different stages of the training process. In essence, terrains are synthesized such that the controller is capable of traversing them while becoming more robust. We evaluate the traversability of parameterized terrains and use particle filtering to maintain a distribution of terrain parameters of medium difficulty [24,25] that adapt as the neural network learns. The training conditions grow increasingly more challenging, yielding an omnidirectional controller that combines agility with unprecedented resilience. The result is a legged locomotion controller that is far more robust than any counterparts in existing literature. Remarkably, the controller is consistently effective in zero-shot generalization settings. That is, it remains robust when tested in conditions that have never been encountered during training. Our training in simulation only uses rigid terrains and a small set of procedurally generated terrain profiles, such as hills and steps. Yet when deployed on physical quadrupeds, the controller successfully handles deformable terrain (mud, moss, snow), dynamic footholds (stepping on a rolling board in a cluttered indoor environment, or debris in the field), and overground impediments such as thick vegetation, rubble, and gushing water. Our methodology and results open new frontiers for legged robotics and suggest that the extraordinary complexity of the physical world can be tamed without brittle and painstaking modeling or dangerous and expensive trial-anderror in field conditions.\nE Forest (mud) D Forest (wet moss) F Forest (vegetation) A\n\nRESULTS\n\nMovie 1 summarizes the results of the presented work. We have deployed the trained locomotion controller on two generations of ANYmal robots: ANYmal-B (Fig. 2D-G) and ANYmal-C ( Fig. 2A-C and  Fig. 3). The robots have different kinematics, inertia, and actuators.\n\n\nNatural environments\n\nThe presented controller has been deployed in diverse natural environments, as shown in Fig. 1 and Movies 1 and S1. These include steep Step height (cm)\n\nStep height (cm)\n\nStep up   Existing terrain estimation pipelines that use cameras or LiDAR [27] fail in environments with snow ( Fig. 2A), water (Fig. 2C), or dense vegetation (Fig. 2F). Our controller does not rely on exteroception and is immune to such failure. The controller learns omnidirectional locomotion based on a history of proprioceptive observations and is robust in zero-shot deployment on terrains with characteristics that were never experienced during training.\n\nWe have compared the presented controller to a state-of-the-art baseline [1,26] in the forest environment. The baseline could traverse flat and unobstructed patches, but failed frequently upon encountering loose branches, thick vegetation, and mud, as shown in Movie S1. Our controller never failed in these experiments.\n\nWe have quantitatively evaluated the presented controller and the baseline in three conditions: moss, mud, and vegetation ( Fig. 2D-F). We have measured locomotion speed and energy efficiency. The results are reported in Table 1. The presented controller achieves higher locomotion speed in all conditions. We computed the dimensionless cost of transport (COT) to compare the efficiency of the controllers at different speed ranges. We define mechanical COT as \u2211 12 actuators [\u03c4\u03b8] + /(mgv). \u03c4 denotes joint torque,\u03b8 is joint speed, mg is the total weight, and v is the locomotion speed. This quantity represents positive mechanical power exerted by the actuator per unit weight and unit locomotion speed [28]. As shown in Table 1, the presented controller is more energy-efficient, with a lower COT than the baseline.\n\nThe quantitative evaluation reported in Table 1 understates the difference between the two controllers because it only measures speed and energetic efficiency of the baseline when it successfully locomotes. The baseline's catastrophic failures are not factored into these measurements: when the baseline fails, it is reset by a human operator in a more stable configuration. Catastrophic failures of the baseline controller due to thick vegetation and other factors are shown in Movie S1. Our controller exhibited no such failures.\n\n\nDARPA Subterranean Challenge\n\nOur controller was used by the CERBERUS team for the DARPA Subterranean Challenge Urban Circuit (Fig. 2G). It replaced a modelbased controller that had been employed by the team in the past [1,26]. The objective of the competition is to develop robotic systems that rapidly map, navigate, and search complex underground environments, including tunnels, urban underground, and cave networks. The human operators are not allowed to assist the robots during the competition physically; only teleoperation is allowed. Accordingly, the locomotion controller needs to perform without failure over extended mission durations. To our knowledge, this is the first use of a legged locomotion controller trained via model-free RL in such competitive field deployment.\n\nThe presented controller drove two ANYmal-B robots in four missions of 60 minutes. The controller exhibited a zero failure rate throughout the competition. A steep staircase that was traversed by one of the robots during the competition is shown in Fig. 2G.\n\n\nIndoor experiments\n\nWe further evaluated the robustness of the presented controller in an indoor environment populated by loose debris, as shown in Fig. 3A. Support surfaces are unstable and the robot's feet frequently slip. Such conditions can be found at disaster sites and construction zones, where legged robots are expected to operate in the future.\n\nResults are shown in Fig. 3A and Movie S2. The robot moves omnidirectionally over the area. The presented controller can stably locomote over shifting support surfaces. This level of robustness is beyond the reach of prior controllers for ANYmal robots [1,26] and is comparable to the state of the art [2,29].\n\nThe learned controller manifests a foot-trapping reflex, as shown in Fig. 3B and Movie S3. The policy identifies the trapping of the foot purely from proprioceptive observations and lifts the foot over the obstacle. Such reflexes were not specified in any way during training: they developed adaptively. This distinguishes the presented approach from conventional controller design methods, which explicitly build in such reflexes and orchestrate their execution by a higher-level state machine [1, 3]. The step shown in Fig. 3B is 16.8 cm high, which is higher than the foot clearance of the legs during normal walking on flat terrain. The maximum foot clearance on flat terrain is 12.9 cm and 13.6 cm for the LF and RF legs, respectively 1 , and increases up to 22.5 cm and 18.5 cm in the case of foot-trapping. Our controller also learns to adapt the hind leg trajectories when stepping up. The maximum foot clearance on flat terrains is 13.5 cm and 9.06 cm for the LH and RH legs, and increases up to 16.6 cm and 15.9 cm when the front legs are above the step. Further analysis is provided in the Materials and Methods section. Note also that the reflexes learned by our controller are more general and are not tied to particular contact events. Fig. 3C shows the controller responding to a mid-shin collision during the swing phase. Here, the trapping event was not signalled by foot contact, and scripted controllers that use foot contact events as triggers would not appropriately handle this situation. Our controller, on the other hand, analyzes the proprioceptive stream as a whole and is trained without making assumptions about possible contact locations. Hence, it can learn to react to any obstructions and disturbances that impact the robot's bodily configuration.\n\nWe now focus on comparing the presented approach with the baseline [1,26] in controlled settings. We first compare the robustness of the controllers in the diagnostic setting of a single step, as shown in Fig. 3D. In each trial, the robot is driven straight to a step for 10 s. A trial is a success if the robot traverses the step with both front and hind legs. We conducted 10 trials for each step height and computed the success rate. Since the baseline controller takes a desired linear velocity of the base as input, we commanded a forward velocity of 0.2 m/s and 0.6 m/s. 0.6 m/s is the maximum speed of the baseline. The success rates are given in Fig. 3E. The presented controller outperforms the baseline in both stepping up and down. The baseline showed high sensitivity to foot-trapping, which often led to a fall, as shown in Movie S3.\n\nWe also tested the controllers in the presence of substantial model mismatch. We attached a 10 kg payload, as shown in Fig. 3D and Movie S4. This payload is 22.7 % of the total weight of the robot, and was never simulated during training. As shown in Fig. 3E, the presented controller can still traverse steps up to 13.4 cm despite the model mismatch. The baseline is incapable of traversing any steps under any command speed with the payload.\n\nWe then evaluate the tracking performance of the controllers on flat ground with the payload. We commanded each controller in 8 directions and measured the locomotion speed and the tracking error. Target speed is fixed to 0.4 m/s for the baseline controller, which is similar to the operating speed of the presented controller. In Fig. 3F, we show the velocity profiles of the controllers. Our controller locomotes at around 0.4 m/s in all directions and performs similarly with the payload. On the other hand, the locomotion speed of the baseline varies with direction, which can be seen by the anisotropic velocity profile, and the velocity profile shifts significantly off center with the payload. Fig. 3G shows the heading error of the controllers in each commanded direction. The heading error is the angle between the command velocity and the base velocity of the robot. The heading error of the presented controller is consistently smaller than the baseline, both with and without the payload. The baseline's error in the lateral direction reaches \u223c30 \u2022 and the baseline fails when a speed of (0.6 m/s) is commanded, as shown in Movie S4. In contrast, the average heading error of the presented controller stays within 10 \u2022 with or without the payload. We conclude that the presented controller is much more robust to model mismatch.\n\nNext we test robustness to foot slippage. To introduce slippage, we used a moistened whiteboard [1]. The results are shown in Movie S5. The baseline quickly loses balance, aggressively swings the legs, and falls. In contrast, the presented controller adapts to the slippery terrain and successfully locomotes in the commanded direction.\n\n\nDISCUSSION\n\nThe presented results substantially advance the published state of the art in legged robotics. Beyond the results themselves, the methodology presented in this work can have broad applications. Prior to our work, a hypothesis could be held that training in simulation is fundamentally constrained by the limitations of simulation environments in representing the complexity of the physical world. Present-day technology is severely limited in its ability to simulate compliant contact, slippage, and deformable and crumbling terrain. As a result, phenomena such as mud, snow, thick vegetation, gushing water, and many others are beyond the capabilities of robotics simulation frameworks [30][31][32].\n\nThe sample complexity of model-free RL algorithms, which commonly require millions of time steps for training, further exacerbates the challenge by precluding reliance on frameworks that may require seconds of computation per time step.\n\nOur work demonstrates that simulating the astonishing variety of the physical world may not be necessary. Our training environment features only rigid terrain, with no compliance or overground obstructions such as vegetation. Nevertheless, controllers trained in this environment successfully meet the diversity of field conditions encountered at deployment.\n\nWe see a number of limitations and opportunities for future work. First, the presented controller only exhibits the trot gait. This is narrower than the range of gate patterns discovered by quadrupeds in nature [33]. The gait pattern is constrained in part by the kinematics and dynamics of the robot, but the ANYmal machines are physically capable of multiple gates [26]. We hypothesize that training protocols and objectives that emphasize diversity can elicit these.\n\nSecond, the presented controller relies solely on proprioception.\n\nThis is a significant advantage in that the controller makes few assumptions on the sensor suite and is not susceptible to failure when exteroception breaks down. Indeed, existing work has argued that a blind (proprioceptive) controller should form the basis of a legged locomotion stack [3]. Nevertheless, blind locomotion is inherently limited. If the machine is commanded to walk off a cliff, it will. Even in less extreme conditions, the robot's gait is fairly conservative since it must by necessity feel out the environment with its body as it locomotes. A major opportunity for future work is to use the presented methodology as a starting point in the development of a hybrid proprioceptive-exteroceptive controller that, like many animals, will be able to locomote even when vision and other external senses are disrupted, but will use exteroceptive data when it is provided. This will enable legged machines to autonomously traverse environments that may have fatal elements such as cliffs, and to raise speed and energetic efficiency in safer conditions. More broadly, the presented results expedite the deployment of legged machines in environments that are beyond the reach of wheeled and tracked robots and are dangerous or inaccessible to humans, while the presented methodology opens new frontiers for training complex robotic systems in simulation and deploying them in the full richness and complexity of the physical world.\n\n\nMATERIALS AND METHODS\n\n\nA. Overview\n\nThe main objective of the presented controller is to locomote over rough terrain following a command. The command is given either by a human operator or by a higher-level navigation controller. In our formulation, unlike many existing works [12,14,16] that focus on tracking the target velocity of the base ( B IB v T ), only the direction ( B IBv T ) is given to the controller. The reason is that the feasible range of target speeds is often unclear on challenging terrain. For example, the robot can walk faster downhill than uphill.\n\nThe command vector is defined as ( B IBv T ) xy , (\u03c9 T ) z . The first part is the target horizontal direction in base frame ( B IBv T ) xy := cos(\u03c8 T ), sin(\u03c8 T ) , where \u03c8 T is the yaw angle to command direction in the base frame. The stop command is defined as 0.0, 0.0 . The second part is the turning direction (\u03c9 T ) z \u2208 {\u22121, 0, 1}. 1 refers to counter-clockwise rotation along the base z-axis.\n\nAn overview of our method is given in Fig. 4. We use a privileged learning strategy inspired by \"learning by cheating\" [23] (Fig. 4A). We first train a teacher policy that has access to privileged information concerning the terrain. This teacher policy is then distilled into a proprioceptive student policy that does not rely on privileged information. The privileged teacher policy is confined to simulation, but the student policy is deployed on physical machines. One difference of our methodology from that of Chen et al. [23] is that we do not rely on expert demonstrations to train the privileged policy; rather, the teacher policy is trained via reinforcement learning. The privileged teacher model is based on multi-layer perceptrons (MLPs) that receive information on the current state of the robot, properties of the terrain, and the robot's contact with the terrain. The model computes a latent embeddingl t that represents the current state, and an action\u0101 t . The training objective rewards locomotion in prescribed directions.\n\nAfter the teacher policy is trained, it is used to supervise a proprioceptive student policy. The student model is a temporal convolutional network (TCN) [22] that receives a sequence of N proprioceptive observations as input. The student policy is trained by imitation. The vectorsl t and\u0101 t computed by the teacher policy are used to supervise the student. This is illustrated in Fig. 4A.   Fig. 4. Overview of the presented approach. (A) Two-stage training process. First, a teacher policy is trained using reinforcement learning in simulation. It has access to privileged information that is not available in the real world. Next, a proprioceptive student policy learns by imitating the teacher. The student policy acts on a stream of proprioceptive sensory input and does not use privileged information. (B) An adaptive terrain curriculum synthesizes terrains at an appropriate level of difficulty during the course of training. Particle filtering is used to maintain a distribution of terrain parameters that are challenging but traversable by the policy. (C) Architecture of the locomotion controller. The learned proprioceptive policy modulates motion primitives via kinematic residuals. An empirical model of the joint PD controller facilitates deployment on physical machines.\n\nTraining is conducted on procedurally generated terrains in simulation. The terrains are synthesized adaptively, to facilitate learning according to the skill level of the trained policies at any given time. We define a traversability measure for terrain and develop a samplingbased method to select terrains with the appropriate difficulty during the course of training. We use particle filtering to maintain an appropriate distribution of terrain parameters. This is illustrated in Fig. 4B. The terrain curriculum is applied during both teacher and student training.\n\nOur control architecture is shown in Fig. 4C. We employ the Policies Modulating Trajectory Generators (PMTG) architecture [34] to provide priors on motion generation. The neural network policy modulates leg phases and motion primitives by synthesizing residual position commands.\n\nThe simulation uses a learned dynamics model of the robot's joint PD controller [12]. This facilitates the transfer of policies from simulation to reality. After training in simulation, the proprioceptive controller is deployed directly on physical legged machines, with no fine-tuning.\n\n\nMotion synthesis\n\nWe now elaborate on the control architecture that is illustrated in Fig. 4C. It is divided into motion generation and tracking. The input to our controller consists of the command vector and a sequence of proprioceptive measurements including base velocity, orientation and joint states. The controller does not use any exteroceptive input (e.g., no haptic sensors, cameras, or depth sensors). The input also does not contain any handcrafted features such as foot contact states or estimated terrain geometry. The controller outputs joint position targets.\n\nOur motion generation strategy is based on the periodic leg phase. Previous works commonly leveraged predefined foot contact schedules [2,26,35]. We define a periodic phase variable\n\u03c6 i \u2208 [0.0, 2\u03c0) for each leg, which represents contact phase if \u03c6 \u2208 [0.0, \u03c0) and swing phase if \u03c6 \u2208 [\u03c0, 2\u03c0). At every time step t, \u03c6 i = (\u03c6 i,0 + ( f 0 + f i )t) (mod 2\u03c0)\nwhere \u03c6 i,0 is the initial phase, f 0 is a common base frequency, and f i is the frequency offset for the i-th leg. We want the legs to manifest periodic motions when f 0 + f i = 0 and engage ground contact in contact phase. We set f 0 as 1. 25 Hz, which is the value used by a previously developed conventional controller for a trot gait [26].\n\nThe target foot positions, which are the output of the motion generation block, are defined in the horizontal frames [35] of the feet (H i , i \u2208 {1, 2, 3, 4}). H i is a reference frame that is attached below the hip joint of the i-th leg. The distance equals the nominal reach of the leg. The z-axis of the frame ( H i z) is parallel to e g and H i x is the projection of the base x-axis ( B x) onto the horizontal plane, i.e., the frame has the same yaw angle with the robot. The roll and pitch angles of H i are decoupled from the base. This kinematic trick reduces the effect of base attitude on the foot motions [35] and consequently stabilizes training. Defining the output in H i results in less premature termination at the beginning of the policy training, when the base motion is unstable due to random actions. Another benefit is that we can decompose the action distribution of the stochastic policy in the lateral and vertical directions during policy training. We applied larger noise in the lateral direction to promote exploration along the ground surface.\n\nWe use the PMTG architecture [34] to integrate a neural network to regulate the controller. Our implementation consists of four identical foot trajectory generators (FTGs) and a neural network policy. The FTG is a function F(\u03c6) : [0.0, 2\u03c0) \u2192 R 3 that outputs foot position targets for each leg. The FTG drives vertical stepping motion when f i is non-zero. The definition of F(\u03c6) is given in supplementary section S3.\n\nThe policy outputs f i s and target foot position residuals (\u2206r f i ,T ), and the target foot position for the i-th foot is r f i ,T := F(\u03c6 i ) + \u2206r f i ,T .\n\nThe tracking control is done using analytic inverse kinematics (IK) and joint position control. Each foot position target defined in H i is first expressed in the robot base frame, and the joint position targets are computed using analytic IK. The joint position targets are then tracked by joint position PD controllers. The main reason for using analytic IK is to maximize computational efficiency and to reuse existing position control actuator models [6,15] for the sim-to-real transfer.\n\n\nTeacher policy\n\nWe formulate the control problem as a Markov Decision Process (MDP). MDP is a mathematical framework for modeling discrete-time control processes in which the evolution of the state and the outcomes are partly stochastic. An MDP is defined by a state space S, action space A, a scalar reward function R(s t , s t+1 ), and the transition probability P(s t+1 |s t , a t ). A learning agent selects an action a t from its policy \u03c0(a t |s t ) and receives a reward r t from the environment. The objective of the RL framework is to find an optimal policy \u03c0 * that maximizes the discounted sum of rewards over an infinite time horizon.\n\nAssuming the environment is fully observable to the teacher, we formulate locomotion control as an MDP and use an off-the-shelf RL method [36] to solve it. In this section, we provide the MDP for teacher training, which is defined by a tuple of state space, action space, transition probability, and reward function.\n\nThe state is defined as s t := o t , x t , where o t is the measurement vector obtainable from the robot and x t is the privileged information that is usually not available in the real world. The detailed definitions are given in Table S4. o t contains command, orientation, base twist, joint positions and velocities, \u03c6 i s, f i s, and previous foot position targets. Joint position errors and velocities measured at -0.01 s and -0.02 s are contained in o t , which is the same as the input to the learned model of the joint-level PD controller. This information allows the policy to exploit the actuator dynamics [12]. To encode the leg phase, we use cos(\u03c6), sin(\u03c6) instead of \u03c6, which is a smooth and unique representation for the angle. Previous foot position targets are also fed back to the policy and are used to compute the target smoothness reward that is explained in the following paragraph. When the student controller is deployed, the quantities in o t are replaced with readings from the proprioceptive sensors and the base velocity and orientation are provided by a state estimator [37]. x t contains noiseless information that we receive directly from a physics engine. x t mainly consists of information related to foot-ground interactions such as terrain profile, foot contact states and forces, friction coefficients, and external disturbance forces applied during training. Specifically, we represent the terrain profile with the elevation of 9 scan points around each foot, which are symmetrically placed along a circle with a 10 cm radius (visualized in Fig. 4).\n\nThe action (\u0101 t ) is a 16-dimensional vector consisting of leg frequencies and foot position residuals.\n\nThe reward function is defined such that an RL agent receives a higher reward if it advances faster towards the goal. The reward function is specified in detail in supplementary section S4.\n\nThe policy network is constructed by two MLP blocks as shown in Fig. 4A. The MLP encoder embeds x t into a latent vectorl t . The command and robot states are not included in x t , sol t contains only the terrain-and contact-related features. We hypothesize thatl t drives adaptive behaviors such as changing foot clearance depending on the terrain profile. Thenl t and o t are provided to the subsequent MLP layers to compute action.\n\nThe Trust Region Policy Optimization (TRPO) [36] algorithm is used for training. The hyperparameters we used are given in Table S7.\n\n\nStudent policy\n\nThe proprioceptive student policy only has access to o t . A key hypothesis here is that the latent featuresl t can be (partially) recovered from a time series of proprioceptive observations, h t , which is defined as h t := o t \\ { f o , joint history, previous foot position targets}.\n\nThe student policy uses a temporal convolutional network (TCN) [22] encoder. The input to the TCN encoder is H = {h t\u22121 , ..., h t\u2212N\u22121 }, where N is the history length. The encoder is fully convolutional and consists of three dilated causal convolutional layers, interleaved with strided convolutional layers that reduce dimensionality. The architecture is specified in Tables S5 and S6. We use the TCN architecture because it affords transparent control over the input history length, can accommodate long histories, and is known to be robust to hyperparameter settings [22]. A comparison with a recurrent neural network architecture is provided in supplementary section S8.\n\nThe student policy is trained via supervised learning. The loss function is defined as\nL := (\u0101 t (o t , x t ) \u2212 a t (o t , H)) 2 + (l t (o t , x t ) \u2212 l t (H)) 2 . (1)\nQuantities marked by a bar (\u00b7) denote target values generated by the teacher. We employ the dataset aggregation strategy (DAgger) [38]. Specifically, training data is generated by rolling out trajectories by the student policy. For each visited state, the teacher policy computes its embedding and action vectors (\u00b7). These outputs of the teacher policy are used as supervisory signals associated with the corresponding states. The hyperparameters we used are given in Table S8.\n\n\nAdaptive terrain curriculum\n\nOur method is inspired by automatic curriculum learning (ACL) for RL agents [25,39]. The paired open-ended trailblazer (POET) approach [25] generates diverse parameterized terrains for a 2D bipedal agent. The method employs minimal criteria (MC) [24,40] and aims to choose environmental parameters that are neither too challenging nor trivial for the agents: this is realized by selecting task parameters that yield mid-range rewards. Florensa et al. [39] similarly choose achievable yet difficult goals for RL agents.\n\nOur method likewise realizes a training curriculum that gradually modifies a distribution over environmental parameters such that the policy can continuously improve locomotion skills and generalize to new environments. Our work differs from POET as POET aims for open-ended search in the space of possible problems and evolves a population of specialized agents while we seek to obtain a single generalist agent. Fig. 4B shows the types of terrains used in our training environment. Each terrain is generated by a parameter vector c T \u2208 C. The terrains are described in detail in supplementary section S5. Our ACL method approximates a distribution of desirable c T s using a particle filter.\n\nWe first describe how a given c T is evaluated in simulation. Instead of directly using the reward function to evaluate the learning progress [25,[41][42][43], we evaluate c T s by the traversability of generated terrains, which is defined as the success rate of traversing a terrain. We found traversability to be more intuitive than the reward function, which consists of multiple objectives that are often unbounded. We first define a labeling function \u03bd as\n\u03bd(s t , a t , s t+1 ) = 1 if v pr (s t+1 ) > 0.2 0 if v pr (s t+1 ) < 0.2 \u2228 termination (2)\nfor a state transition from s t to s t+1 . v pr (s t+1 ) stands for the inner product of the base velocity and commanded direction at time step t + 1.\n\nIf \u03c0 can locomote in the commanded direction faster than 0.2 m/s, we consider the terrain traversable in this direction. The threshold is a hyperparameter; 0.2 m/s is about one third of the maximum speed of our robot. Traversability is defined as\nTr(c T , \u03c0) = E \u03be\u223c\u03c0 {\u03bd(s t , a t , s t+1 | c T )} \u2208 [0.0, 1.0],(3)\nwhere \u03be refers to trajectories generated by \u03c0. This follows a definition of empirical traversability in prior work [44]. The objective of our terrain generation method is to find c T s with mid-range traversability (Tr(c T , \u03c0) \u2208 [0.5, 0.9]). The rationale is to synthesize terrains that are neither too easy nor too difficult. We define terrain desirability as follows:\n\nTd(c T , \u03c0) := Pr(Tr(c T , \u03c0) \u2208 [0.5, 0.9]) (4)\n= E \u03be\u223c\u03c0 {Tr(c T , \u03c0) \u2208 [0.5, 0.9]},(5)\nwhere 0.5 and 0.9 are fixed thresholds for minimum/maximum traversability.\n\nWe use a particle filter to keep track of a distribution of highdesirability c T s during training. We formulate a particle filtering problem where we approximate the distribution of terrain parameters that satisfies Tr(c T , \u03c0) \u2208 [0.5, 0.9] with a finite set of sampling points (c k T \u2208 C, k \u2208 1, \u00b7 \u00b7 \u00b7 , N particle ). Our algorithm is modeled on the Sequential Importance Resampling (SIR) particle filter. It is based on the following assumptions.\n\n1. Terrain parameters with similar Tr(\u00b7, \u03c0) are close in Euclidean distance in parameter space.\n\n\n2.\n\nA policy trained over the terrains generated by c T s in some area of C will learn to interpolate to nearby terrain parameters.\n\n3. c T,0 , c T,1 , ... forms a Markov process, where c T,j = {c 1 T,j , c 2 T,j , ...c N particle T,j } at iteration j.\n\nThe first assumption comes from the insight that terrain parameters can be interpolated, e.g., the difficulty of a staircase increases as we increase the step height. The second assumption justifies the use of discrete samples from C to train a policy that generalizes over a certain region of C. The last assumption is necessary for formulating a particle filter. The importance weight w k is defined for each c k T , and the set of tuples c k T , w k approximates the target distribution (c T s with Tr(c T , \u03c0) \u2208 [0.5, 0.9]). We define the measurement variable y k j such that y k j = 1 if Tr(c k T,j , \u03c0) \u2208 [0.5, 0.9]. Then the terrain desirability defined above becomes the measurement probability Pr(y k j |c k T,j ) = Pr(Tr(c k T,i , \u03c0) \u2208 [0.5, 0.9]) = Td(c k T,j , \u03c0).\n\nFor practical implementation, the measurement probability is computed by the empirical expectation from the samples collected during policy training:\nPr(y k j |c k T,j ) \u2248 N traj \u2211 1(Tr(c k T,j , \u03c0) \u2208 [0.5, 0.9]) N traj ,(7)\nwhere N traj denotes the number of trajectories generated using c k T,j . The trajectories are also used for policy training. Our method therefore does not require additional evaluation steps to advance the curriculum of the terrain parameters. Resampling is done such that the probability of choosing the kth sample equals the normalized importance weight w k / \u2211\nN particle i w i \u2208 [0, 1].\nThe transition model is a random walk in C. Each parameter of a sampling point is shifted to its adjacent value by a fixed probability p transition . It satisfies the third assumption (Markov process) because the evolution of each parameter only relies on the current value and randomly sampled noise. To improve exploration, we bounded and discretized C to reduce the search space. The initial samples (c k T,0 ) are either drawn uniformly from C or concentrated on almost flat terrains.\n\nImplementation details and an overview of the training process are provided in supplementary section S2 and Algorithm S1 in the supplement.  Fig. 5. Ablation studies. We trained each model 5 times using different random seeds. Error bars denote 95 % confidence intervals. (A) Test setups. The robot is commanded to advance for 10 s in the specified direction (black arrow). We conducted 100 trials for each test. On the step test, a trial is considered successful if the robot traverses the step with both front and hind legs. Robots are initialized with random joint configurations. Initial yaw angle is sampled from U(\u2212\u03c0, \u03c0) for the slope test and from U(\u2212\u03c0/6, \u03c0/6) for the other tests.  \n\n\nValidation of the method\n\nWe present ablation studies to justify each component of our approach:\n\n(1) using a sequence model for the student policy, (2) privileged training, and (3) adaptive terrain curriculum.\n\n\nMemory in proprioceptive control\n\nWe evaluate the importance of incorporating proprioceptive memory in the controller via the TCN architecture [22]. Let TCN-N denote a TCN with a receptive field of N time steps. The network architectures we use are specified in detail in Table S5. We test controllers in diagnostic settings designed to focus on specific capabilities. Specifically, we test omnidirectional locomotion on sloped ground, traversal of a discrete step, and robustness to external disturbances (Fig. 5A). Fig. 5B-D summarizes the importance of the memory length N. In these experiments, N is varied from 1 (corresponding to 20 ms of memory) to 100 (2 s of proprioceptive memory). The latter is the default setting used in our deployed controller.\n\nAs shown in Fig. 5B, memory length doesn't have a strong effect in the uniform slope setting. Memory length does have a strong effect on the controller's ability to traverse a step (Fig. 5B-C). Controllers with longer memory are able to handle higher steps. As shown in Fig. 5C, the failure rate of limited-memory controllers is particularly high when the hind legs encounter the step. Controllers with longer memory also adapt hind leg trajectories to ensure higher foot clearances. Fig. 5D shows that controllers with longer memory are more robust to external disturbances. We applied an external 50 N force laterally to the base for 5 s during a straight walk and evaluated the resulting deviation from the intended locomotion direction. The deviation of the TCN-100 controller was 35.5 % lower than that of TCN-1.\n\n\nPrivileged training\n\nWe now assess the importance of privileged training. As a baseline, we train a TCN-20 policy directly, without the two-stage privileged training protocol. The policy is trained by TRPO [36] with the same reward and hyperparameters that we use for teacher training. This baseline is compared to the same TCN-20 architecture trained via privileged learning.\n\nThe results are summarized in Fig. 5E-G. Fig. 5E shows that the baseline fails the diagnostic tests: it is incapable of locomoting on a slope or traversing a step. Fig. 5F shows that the baseline does not reach comparable reward during training as the teacher MLP architecture with privileged information or the proprioceptive TCN-20 architecture (same as the baseline, no privileged information) trained via privileged learning. Fig. 5G shows the mean episode length during training, which indicates that the baseline fails to learn to balance and locomote.\n\n\nAdaptive terrain curriculum\n\nWe now evaluate the effect of the adaptive terrain curriculum on teacher training. Terrains used for training (specifically, hills, steps, and stairs) are shown in Fig. 4B. As a baseline, we trained a teacher using randomly generated terrains that are uniformly sampled from C as specified in Table S2. The success rates on the testing terrains are significantly lower when trained without the adaptive curriculum, as shown in Fig. 5H. Fig. 5I shows that a teacher trained without adaptive curriculum plateaus at a lower reward level. Throughout the training process, the mean episode length is shorter for the model being trained without adaptive curriculum (Fig. 5J). This is because uniform sampling is more likely to draw terrains that cannot be successfully traversed by the policy being trained. On these terrains, the policy fails early and receives less training signal as a result. The adaptive curriculum modulates the difficulty of sampled terrains so as to maximize the didactic benefit of each episode. We provide an additional evaluation of the adaptive curriculum in supplementary section S6.\n\n\nFurther analysis of emergent behavior\n\nHere we provide further analysis on how the proprioceptive policy adapts to different situations.\n\nTo investigate how the proprioceptive policy perceives the environment, we trained a decoder network which reconstructs the privileged information x t \u2208 X from the output of an intermediate layer of a trained TCN policy. x t consists of information that is not directly observable by the student policy such as contact states, terrain shape, and external disturbances. For classification of foot contact states, we employ a standard cross-entropy loss function. For regression of other states, we predict both mean m i and standard deviation \u03c3 i for each component and use a negative Gaussian log-likelihood loss to quantify the uncertainty encoded in the TCN representation [45]:\nL = \u2211 i\u2208dim(X\\contact states) (m i \u2212 m gt i ) 2 2\u03c3 2 i + log(\u03c3 i )(8)\nwith added weight decay. The superscript gt refers to the ground truth generated in simulation. Note that the parameters of the policy network are fixed during decoder training. Therefore, the decoder network is not used for policy training. It only provides insight into the information encoded by the TCN policy after training. In Fig. 6, we provide snapshots of the foot-trapping reflex motion (Fig. 6A) and the reconstructed privileged information. In Fig. 6B we show the reconstructed terrain geometry and foot contact state. When the LF foot collides with the step, the estimated elevation in front of the front legs increases and its uncertainty grows (i+ii). The estimated elevations and normal vectors adapt to the step during the foot-trapping reflex (iii+iv). After the successful step-up, the terrain uncertainty remains elevated (v), indicating an anticipation of generally rough terrain. Additionally, the decoder network can detect foot contacts with horizontal and vertical surfaces while successfully identifying frontal collision as such, as indicated by the estimated terrain normal vector (i + iii). The ability to reconstruct explicit environmental information from the encoding of the proprioceptive history is a strong indicator that the TCN policy learns to build an internal representation of the environment and uses it for decision making. We provide more examples of the reconstructed privileged information in supplementary section S7.\n\nWe then analyse how the proprioceptive policy leverages past observations. We compute the saliency map of the input H \u2208 R 60\u00d7N , and visualize the sensitivity of the policy to each element of the input while overcoming the step [46]. Each column of H is a proprioceptive measurement h \u2208 R 60 , and we stack N measurements (history length = 0.02 s \u00d7 N). We define the saliency value for the i-th measurement (i \u2208 [0, N]) as\nM i = \u2211 j\u2208channels (|d((r f ,T ) z )/dH i,j |) \u2208 R,(9)\nwhere (r f ,T ) z refers to the height command for the foot f . We computed the value for (r f ,T ) z because we are interested in the change in foot clearance. M i can be interpreted as the sensitivity of the output to the ith measurement. As we use 1D convolution over time, the output is in R N , i.e., each row of H is regarded as a channel.\n\nIn Fig. 6C we can see that the saliency value at the foot-trapping event (FT) is kept high while stepping up. The policy has direct access to the measurements at the moment of foot-trapping, and leverages this in the following swing phase. This is highlighted by the red boxes in Fig. 6D. The policy attends to the LF leg joint states measured at the foot-trapping event. \n\n\nACKNOWLEDGMENTS\n\n\nSUPPLEMENTARY MATERIALS\n\nSection S1. Nomenclature Section S2. Implementation details Section S3. Foot trajectory generator Section S4. Reward function for teacher policy training Section S5. Parameterized terrains Section S6. Qualitative evaluation of the adaptive terrain curriculum Section S7. Reconstruction of the privileged information in different situations Section S8. Recurrent neural network student policy Section S9. Ablation of the latent representation loss for student training Algorithm S1. Teacher training with automatic terrain curriculum Figure S1.\n\nIllustration of the adaptive curriculum. Figure S2.\n\nReconstructed privileged information in different situations. Figure S3.\n\nComparison of neural network architectures for the proprioceptive controller Table S1.\n\nComputation time for training Table S2.\n\nParameter spaces C for simulated terrains Table S3.\n\nHyperparameters for automatic terrain curriculum Table S4.\n\nState representation for proprioceptive controller and the privileged information Table S5.\n\nNeural network architectures Table S6.\n\nNetwork parameter settings and the training time for student policies Table S7.\n\nHyperparameters for teacher policy training Table S8.\n\nHyperparameters for student policy training Table S9.\n\nHyperparameters for decoder training Movie S1.\n\nDeployment in a forest Movie S2.\n\nLocomotion over unstable debris Movie S3.\n\nStep experiment Movie S4.\n\nPayload experiment Movie S5.\n\nFoot slippage experiment SUPPLEMENTARY MATERIALS S1. Nomenclatur\u00ea\n(\u00b7) normalized vecto\u1e59 (\u00b7) first derivativ\u0113 (\u00b7)\nteacher's quantity (\u00b7) T target quantity C AB v linear velocity of B frame with respect to A fram\u0117 expressed in C frame c T terrain parameter vector \u03c9 angular velocity \u03c4 joint torque \u03b8 joint angle \u03c8 yaw angle \u03c6 leg phase f leg frequency r f linear position of a foot e g gravity vector H horizontal frame g i gap function of the i-th possible contact pair I c index set of all contacts I c,body index set of body contacts I c, f oot index set of foot contacts I swing index set of swing legs |\u00b7| cardinality of a set or l 1 norm ||\u00b7|| l 2 norm\n\n\nS2. Implementation details\n\nThe RaiSim simulator [30] is used for rigid-body and contact dynamics simulation. The actuator networks [12] are trained for each robot to simulate Series Elastic Actuators (SEA) [47] at the joint. The input to the actuator model is a 6-dimensional real-valued vector consisting of the joint position error and velocity at current time step t and two past states corresponding to t \u2212 0.01 s and t \u2212 0.02 s. The feature selection is done as in [12].\n\nAs several studies have shown that randomization of dynamic properties improves the robustness of the policy [12,16], we also randomized several physical quantities, and the teacher policy has access to these values during training. We applied disturbances, randomized friction coefficients between the feet and the terrain, and additive noise to the observations during training.\n\nThe training process for the teacher policy is depicted in Algorithm S1. Hyperparameters are given in Table S3. In our implementation of the terrain curriculum, we update the curriculum every N evaluate policy iterations to reduce variance. We assume that within N evaluate iterations, the performance of the policy is similar. With the slower update rate, the measurement probability of Eq.6 becomes\nPr(y k j |c k T,j ) \u2248 N evaluate \u2211 N traj \u2211 1(Tr(c k T,j , \u03c0) \u2208 [0.5, 0.9]) N traj N evaluate .(10)\nAdditionally, we leverage replay memory to prevent degeneration of the particle filter and to avoid catastrophic forgetting. The controller is implemented with a state machine to switch between the \"standing still\" state and the locomotion state. We set the base frequency f 0 to zero when the zero command is given for 0.5 s, which stops FTGs, and the robot stands still on the terrain. f 0 is set to 1.25 Hz when the direction command is given or the linear velocity of the base exceeds 0.3 m/s for the disturbance rejection. The state machine is included in the training environment.\n\nDuring the deployment, the base velocity and orientation are estimated by the state estimator that relies on inertial measurements and leg kinematics [37].\n\nThe neural network policy runs at 400 Hz on an onboard CPU (Intel i7-5600U, 2.6 -3.2GHz, dual-core 64-bit) integrated into the robot. The Tensorflow C++ API is used for onboard inference.\n\nAlgorithm S1. Teacher training with automatic terrain curriculum 1: Initialize a replay memory, Sample N particle c T,0 s uniformly from C (Table S2), i, j = 0. 2: repeat 3: for 0 \u2264 k \u2264 N evaluate do 4: for 0 \u2264 l \u2264 N particle do 5: for 0 \u2264 m \u2264 N traj do 6: Generate terrain using c l T,j 7:\n\nInitialize robot at random position 8: Run policy \u03c0 i 9:\n\nCompute traverability label for each state transition (Eq. 2) 10:\n\nSave the scores and the trajectory 11: Update policy using TRPO [36] 12:\ni = i + 1 13:\nfor 0 \u2264 l \u2264 N particle do 14: Compute measurement probability for each parameter c l T,j s (Eq. 9) 15: for 0 \u2264 l \u2264 N particle do 16: Update weights w j = P(y l i |c l T,j ) \u2211 m P(y m i |c m T,j ) 17:\n\nResample N particle parameters 18: Append c T,j s to the replay memory 19: for 0 \u2264 l \u2264 N particle do 20: by p replay probability, sample from replay memory 21: by p transition probability, move c l T,j to an adjacent value in C.\n\n\n22:\nj = j + 1 23: until Convergence\n\nS3. Foot trajectory generator\n\nThe foot trajectory is defined as\nF(\u03c6 i ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 (h(\u22122k 3 + 3k 2 ) \u2212 0.5) H i z k \u2208 [0, 1] (h(2k 3 \u2212 9k 2 + 12k \u2212 4) \u2212 0.5) H i z k \u2208 [1, 2] \u22120.5 H i z otherwise,(11)\nwhere k = 2(\u03c6 i \u2212 \u03c0)/\u03c0 and h is a parameter for the maximum foot height. Each segment during the swing phase (k \u2208 [0, 2)) is a cubic Hermite spline connecting the highest and lowest points with a zero first derivative at the connecting points. Other periodic functions such as h i sin(\u03c6 i ) can be used for the FTG. With a set of reasonably tuned f 0 , h and \u03c6 i,0 , a quadruped can stably step in place. In our setting, f 0 = 1.25, h = 0.2 m, and \u03c6 i,0 are sampled from U(0, 2\u03c0).\n\n\nS4. Reward function for teacher policy training\n\nThe reward function is defined as 0.05r lv + 0.05r av + 0.04r b + 0.01r f c + 0.02r bc + 0.025r s + 2 \u00b7 10 \u22125 r \u03c4 . The individual terms are defined as follows.\n\n\u2022 Linear Velocity Reward (r lv ): This term maximizes the v pr = ( B IB v) xy \u00b7 ( B IBv T ) xy , which is the base linear velocity projected onto the command direction.\nr lv := \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 exp (\u22122.0(v pr \u2212 0.6) 2 ) v pr < 0.6 1.0 v pr \u2265 0.6 0.0 zero command .(12)\nThe velocity threshold is defined as 0.6 m/s which is the maximum speed reachable on the flat terrain with the existing controller [26].\n\n\u2022 Angular Velocity Reward (r av ): We motivate the agent to turn as fast as possible along the base z-axis when ( B IB\u03c9 T ) z is nonzero. It is defined as\nr av := exp (\u22121.5(\u03c9 pr \u2212 0.6) 2 ) \u03c9 pr < 0.6 1.0 \u03c9 pr \u2265 0.6 ,(13)\nwhere \u03c9 pr = ( B IB \u03c9) z \u00b7 ( B IB\u03c9 T ) z .\n\n\u2022 Base Motion Reward (r b ): This term penalizes the velocity orthogonal to the target direction and the roll and pitch rates such that the base is stable during the locomotion.\nr b := exp(\u22121.5v 2 o ) + exp(\u22121.5||( B IB \u03c9) xy || 2 )(14)\nwhere\nv o = ||( B IB v) xy \u2212 v pr \u00b7 ( B IBv T ) xy ||.\nWhen the stop command is given, v o is replaced by || B IB v||.\n\n\u2022 Foot Clearance Reward (r f c ): When a leg is in swing phase, i.e., \u03c6 i \u2208 [\u03c0, 2\u03c0), the robot should lift the corresponding foot higher than the surroundings to avoid collision. We first define the set of such collision-free feet as F clear = {i : r f ,i > max(H scan,i ), i \u2208 I swing }, where H scan,i is the set of scanned heights around the i-th foot. Then the clearance cost is defined as\nr f c := \u2211 i\u2208I swing (1 F clear (i)/|I swing |) \u2208 [0.0, 1.0].(15)\n\u2022 Body Collision Reward (r bc ): We want to penalize undesirable contact between the robot's body parts and the terrain to avoid hardware damage.\n\nr bc := \u2212|I c,body \\I c, f oot |. (16) \u2022 Target Smoothness Reward (r s ): The magnitude of the second order finite difference derivatives of the target foot positions are penalized such that the generated foot trajectories become smoother.\nr s := \u2212||(r f ,d ) t \u2212 2(r f ,d ) t\u22121 + (r f ,d ) t\u22122 ||.(17)\n\u2022 Torque Reward (r \u03c4 ): We penalize the joint torques to prevent damaging joint actuators during the deployment and to reduce energy consumption (\u03c4 \u221d electric current).\nr \u03c4 := \u2212 \u2211 i\u2208joints |\u03c4 i |.(18)\n\nS5. Parameterized terrains\n\nIt is important to generate training environments that can pose representative challenges such as foot slippage and foot-trapping. To efficiently synthesize random terrains, we use procedural generation techniques [48]. This method allows us to generate a large number of different terrains by changing a set of terrain parameters c T \u2208 C.\n\nIn the following, we describe the three terrain generators used in this work. See Fig. 4B for a visualization of the terrains and Table S2 for the definition of parameter spaces C.\n\n\u2022 The Hills terrain is based on Perlin noise [49]. The terrain is generated via three parameters: roughness, frequency of the Perlin noise, and amplitude of the Perlin noise. The height of each element of the output height map hm is defined as hm[i, j] := Perlin(c T,2 , c T,3 )[i, j] + U(\u2212c T,1 , c T,1 ). A policy experiences smooth slopes and foot slippage on this terrain during training.\n\n\u2022 The Steps terrain consists of square steps of random height. For every c T,1 by c T,1 blocks, the height is sampled from U(0, c T,2 ). A policy experiences discrete elevation changes and foot-trapping on this terrain.\n\n\u2022 The Stairs terrain is a staircase with fixed width and height. The robot is initialized at the flat segment in the middle of the staircase (see Fig. 4B).\n\nThe ranges are defined considering the kinematics of the robot, e.g., a step height should be lower than leg length. During training, the terrain is regenerated every episode with a different random seed.\n\n\nS6. Qualitative evaluation of the adaptive terrain curriculum\n\nThe behavior of adaptive curriculum is illustrated in Fig. S1. Fig. S1A-C focuses on the Hills terrain type. There are three parameters for this terrain: roughness, frequency, and amplitude. The relationship between traversability (Eq. 3) and desirability (Eq. 4) is illustrated in Fig. S1A-B. Undesirable terrains are either too easy or too difficult, as shown in the leftmost and rightmost panels of Fig. S1A. Fig. S1B-C shows that the particle filter fits the latent distribution of desirable terrains, which has a bow shape in the frequency-amplitude marginal (middle). Fig. S1D focuses on the Stairs terrain type and shows the evolution of terrain parameters during training. The particle filter rejects parameters that represent short and steep steps (upper-left area). The curriculum initially focuses on wide and shallow steps (middle panels, particularly Iter. 50-60), and then broadens the distribution to include narrower steps (rightmost panels).\n\n\nS7. Reconstruction of the privileged information in different situations\n\nIn Fig. S2, we provide the decoded privileged information in different situations. Fig. S2A shows the estimated friction coefficient between the feet and the terrain when traversing a wet, slippery whiteboard, as shown in Movie S5. The estimate decreases as soon as the first foot starts slipping (i), remains low throughout the traversal (ii) and increases about 2 s after the robot returns to normal ground (iii). The external disturbance and terrain information can also be reconstructed from the TCN embedding. As shown in Fig. S2B, the decoder detects downward external force when an unknown 10 kg payload is applied. While traversing dense vegetation as shown in Fig. S2C, it detects a force opposite the motion direction, which makes the policy to counteract and push through the vegetation. The uncertainty of the elevation estimates are notably high in the natural terrains shown in Fig. S2C and Fig. S2D, which indicates that the TCN policy encodes the roughness of the terrain.\n\n\nS8. Recurrent neural network student policy\n\nWe use the TCN architecture for the proprioceptive policy [22]. For comparison, we also evaluated a recurrent network with gated recurrent units (GRU) [50]. The architectures are specified in Tables S5 and S6. The loss function for training a GRU student policy is defined as\nL := (\u0101 t (o t , x t ) \u2212 a t (o t )) 2 + (l t (o t , x t ) \u2212 l t (o t )) 2 .(19)\nTo improve the performance and computational efficiency of the training, we have implemented Truncated Backpropagation Though Time (Truncated BPTT) [51]. Performance on the diagnostic settings presented in Fig. 5A is given in Fig. S3. Overall, the performance of the GRU-based controller is between that of TCN-20 and TCN-100. The performance is comparable to TCN-100 in the slope setting, but the GRU-based controller fails to achieve the performance of TCN-100 in step experiments.\n\nThe chief advantage of the TCN is in training efficiency. The training time for the TCN is much faster in comparison to the GRU. The computation times are reported in Table S1.\n\n\nS9. Ablation of the latent representation loss for student training\n\nWe examine the effect of the second term in the loss function for student policy training presented in Eq. 1, which is a squared error loss for the latent vector l t . As a baseline, we train a student policy using the following loss function:\nL := (\u0101 t (o t , x t ) \u2212 a t (o t , H)) 2 ,(20)\nwhich simply imitates the output of the teacher. The result is reported in Fig. S3 as 'TCN-100 naive IL'. The performance is comparable in the uniform slope setting and under external disturbances. On the other hand, the ablated version has lower success rates on steps.    Table S2. Parameter spaces C for simulated terrains. N (m, d) denotes that the value is sampled from the Gaussian distribution of mean m and stardard deviation d. The friction coefficient is clipped to be above 0.1.\n\n\nParameter value\n\nNumber of particles (N particle ) 10 per terrain type Transition probability (p transition ) 0.8\n\nTrajectories per particle (N traj ) 6\n\nUpdate rate of the terrain parameters (N evaluate ) 10\n\nProbability of sampling from replay memory (P replay ) 0.05 Foot-ground friction coefficients 4\n\nExternal force applied to the base 3  Table S5. Neural network architectures. Unless specified otherwise, the dilation and stride are 1 for convolutional layers. The filter size is fixed to 5. The layers marked with * are copied from the teacher to learners after the teacher training. id refers to the identity map. The TCN-N architecture uses dilated causal convolution [22]. Each convolutional layer is followed by a relu activation function.  Table S6. Network parameter settings and the training time for student policies. SGD time refers to the computation time required for one stochastic gradient descent update with the batch size given in Table S8. The computation times are presented as empirical means with standard deviations. *The sequence length for the GRU network stands for the sequence length used for Truncated BPTT [51].  Table S9. Hyperparameters for decoder training. exp(a,b) denotes exponential decay, which is defined as lr 0 * a updates/b . The Adam [52] optimizer is used.\n\nFig. 1 .\n1Deployment of the presented locomotion controller in a variety of challenging environments.\n\nFig. 3 .\n3Evaluation in an indoor environment. (A) Locomotion over unstable debris. The robot steps onto loose boards (highlighted in red and blue) that dislodge under the robot's feet. (B) The policy exhibits a foot-trapping reflex and overcomes a 16.8 cm step. (C) The policy learns to appropriately handle obstructions irrespective of the contact location. Here it is shown reacting to an obstacle that is encountered mid-shin during the swing phase. (D) Controlled experiments with steps and payload. Our controller and a baseline [1, 26] are commanded to walk over a step with and without the 10 kg payload. (E) Success rates for different step heights. The success rate is evaluated over 10 trials for each condition. (F) Mean linear speed for different command directions on flat terrain. 0 \u2022 refers to the front of the robot. Shaded area denotes 95 % confidence interval. (G) Mean heading errors for different command directions on flat terrain. Shaded area denotes 95 % confidence interval.\n\n\nThe friction coefficients between the feet and the ground are sampled from U(0.4, 1.0). The external force is applied for 5 s in the lateral direction. (B-D) Importance of memory length N in the TCN-N encoder. (E-G) Importance of privileged training. (F) Learning curves for the teacher (grey) and a TCN-20 student trained directly, without privileged training (red). For comparison, the blue line indicates the mean reward of a TCN-20 student trained with privileged training. The reward is computed by running each policy on uniformly sampled terrains. (H-J) Importance of the adaptive curriculum.\n\nFig. 6 .\n6Analysis of the emergent foot-trapping reflex. FT denotes the first contact of the LF foot with the step (foot-trapping event). (A) The LF foot hits the step and then manifests higher foot clearance to overcome the step (ii-iv) in the following swing phase. (B) Reconstructed terrain information from TCN embeddings. Red ellipsoid: estimated terrain shape around the foot. The center of the ellipsoid refers to the estimated terrain elevation and the vertical length represents uncertainty (standard deviation). Black arrow: terrain normal at the in-contact foot. Red cone: uncertainty of normal estimation. Blue spheres: estimated in-contact feet. (C) Input saliency at different moments. The peaks show that the TCN policy attends to the foot-trapping (FT) that happened around 2.1 s. The orange curve (flat terrain) shows the saliency value computed on a flat terrain at similar gait phases. (D) Saliency map unrolled across input channels at 3.4 s. Red boxes refer to joint measurements from the LF leg at the moment it collides with the step.\n\n\n780883. The work has been conducted as part of ANYmal Research, a community to advance legged robotics. Author contribution J.L formulated the main idea of the training and control methods, implemented the controller, set up the simulation, and trained control policies. J.L performed the indoor experiments. J.H contributed in setting up the simulation. J.L and L.W performed outdoor experiments together. J.L, J.H, L.W, M.H, and V.K refined ideas, contributed in the experiment design and analyzed the data. Conflict of interest The authors declare that they have no competing interests. Data and materials availability All (other) data needed to evaluate the conclusions in the paper are present in the paper or the Supplementary Materials. Other materials can be found at https://github.com/leggedrobotics/learning_quadrupedal_ locomotion_over_challenging_terrain_supplementary.\n\nFig. S1 .Fig. S2 .\nS1S2Illustration of the adaptive curriculum. (A) Examples of Hills terrains. The color bar indicates desirability; dark blue represents low desirability. (B) Terrain desirability estimated from 1000 trajectories generated by a fully trained teacher policy. The red crosses correspond to the examples presented in A. (C) The distribution of terrain profiles sampled by the particle filter during the last 100 iterations of teacher training. (D) Evolution of Stairs terrain parameters during training. Reconstructed privileged information in different situations. (A) Estimated friction coefficient between the feet and the terrain while traversing a wet whiteboard. The shaded area denotes 95 % confidence interval. (B-D) Reconstruction of the external disturbance and terrain information in different scenarios. Blue arrow: estimated external force applied to the torso. Red ellipsoid: estimated terrain shape around the foot. The center of the ellipsoid refers to the estimated terrain elevation and the vertical length represents uncertainty (1 standard deviation). For each foot, 8 ellipsoids are symmetrically placed along a circle with 10 cm radius. Black arrow: terrain normal at the in-contact foot.\n\n\nNumber of samplesStair parmeters generated by particle fiter during trainingD \n\n0.6 \n\n0.4 \n\n0.2 \n\nB \n\nTerrain desirability \n\ni \n\nii \n\niii \n\niv \n\nRoughness=0.02 \nFrequency=0.1 \nAmplitude=2.0 \nTraversability= 0.91 \n\nRoughtness=0.15 \nFrequency=0.3 \nAmplitude=2.0 \nTraversability= 0.56 \n\nRoughness=0.15 \nFrequency=1.2 \nAmplitude=0.5 \nTraversability= 0.67 \n\nRoughness=0.15 \nFrequency=1.2 \nAmplitude=2.0 \nTraversability=0.15 \n\nA \n\ni \nii \niii \niv \n\nC \n\n6 \n\n18 \n\n30 \n\n42 \n\n54 \n\n66 \nNumber of samples \n\n\n\n\nFig. S3. Comparison of neural network architectures for the proprioceptive controller. We trained each model 5 times using different random seeds. The error bars denote 95% confidence intervals. 'TCN-100 naive IL' denotes the TCN-100 network trained using a naive imitation learning method without the latent representation loss (Eq. 19).Table S1. Computation time for training. The TCN-100 architecture is used for the student policy. The training is conducted on a desktop machine with i7-8700K CPU and a Geforce RTX 2080 GPU.TCN-1 \nTCN-20 \nTCN-100 \nTCN-100 \nNaive IL \n\nGRU \nTeacher \n\nName \nTime \n\nTeacher policy training \n\u2248 12 hrs \n\nStudent policy training \n\u2248 4 hrs \n\nAdaptive terrain curriculum \n2.9 s \n\nTerrain \ngrid size friction coefficient parameters (c T ) \nrange \n\nHills \n0.2 m \nN (0.7, 0.2) \n\nroughness (m) \n[0.0, 0.05] \n\nfrequency \n[0.2, 1.0] \n\namplitude (m) \n[0.2, 3.0] \n\nSlippery Hills \n0.2 m \nN (0.3, 0.1) \n\nroughness (m) \n[0.0, 0.05] \n\nfrequency \n[0.2, 1.0] \n\namplitude (m) \n[0.2, 3.0] \n\nSteps \n0.02 m \nN (0.7, 0.2) \nstep width (m) \n[0.1, 0.5] \n\nstep height (m) \n[0.05, 0.3] \n\nStairs \n0.02 m \nN (0.7, 0.2) \nstep width (m) \n[0.1, 0.5] \n\nstep height (m) \n[0.02, 0.2] \n\n\n\nTable S3 .\nS3Hyperparameters for automatic terrain curriculum.Data \ndimension x t o t h t \n\nDesired direction (( B \nIBv d ) xy ) \n2 \n\nDesired turning direction (( B \nIB\u03c9 d ) z ) \n1 \n\nGravity vector (e g ) \n3 \n\nBase angular velocity ( B \nIB \u03c9) \n3 \n\nBase linear velocity ( B \nIB v) \n3 \n\nJoint position/velocity (\u03b8 i ,\u03b8 i ) \n24 \n\nFTG phases (sin(\u03c6 i ), cos(\u03c6 i )) \n8 \n\nFTG frequencies (\u03c6 i ) \n4 \n\nBase frequency ( f o ) \n1 \n\nJoint position error history \n24 \n\nJoint velocity history \n24 \n\nFoot target history ((r f ,d ) t\u22121,t\u22122 ) \n24 \n\nTerrain normal at each foot \n12 \n\nHeight scan around each foot \n36 \n\nFoot contact forces \n4 \n\nFoot contact states \n4 \n\nThigh contact states \n4 \n\nShank contact states \n4 \n\n\n\nTable S4 .\nS4State representation for proprioceptive controller (top) and the privileged information (bottom).Layer \nTeacher \nTCN-N Student \nGRU Student \nDecoder \n\ninput \no t \nx t \no t \nh (60\u00d7N) \no t \no t \no t , l t \n\n1 \nid tanh(72) id 1D conv dilation 1 id GRU(68) relu(196) \n\n2 \nid tanh(64) id \n1D conv stride 2 \nconcatenate \nOutput \n\n3 \nconcatenate \nid 1D conv dilation 2 \ntanh(256)* \n-\n\n4 \ntanh(256)* \nid \n1D conv stride 2 \ntanh(128)* \n-\n\n5 \ntanh(128)* \nid 1D conv dilation 4 \ntanh(64)* \n-\n\n6 \ntanh(64)* \nid \n1D conv stride 2 \nOutput* \n-\n\n7 \nOutput* \nid \ntanh(64) \n-\n-\n\n8 \n-\nconcatenate \n-\n-\n\n9 \n-\ntanh(256)* \n-\n-\n\n10 \n-\ntanh(128)* \n-\n-\n\n11 \n-\ntanh(64)* \n-\n-\n\n12 \n-\nOutput* \n-\n-\n\n\n\n\nTable S7. Hyperparameters for teacher policy training.Table S8. Hyperparameters for student policy training. exp(a,b) denotes exponential decay, which is defined as lr 0 * a updates/b . The Adam[52] optimizer is used.Parameter \nValue \n\ndiscount factor \n0.995 \n\nKL-d threshold \n0.01 \n\nmax. episode length \n400 \n\nCG damping \n1e-1 \n\nCG iteration \n50 \n\ndiscount factor \n0.995 \n\nbatch size \n80000 \n\ntotal iterations \n10000 \n\nParameter \nTCN-N \nGRU \n\ninitial learning rate \n5e-4 \n2e-4 \n\nlearning rate decay \nexp(0.995, 100) \n\nmax. episode length \n400 \n\nbatch size \n20000 \n10000 \n\nminibatches \n5 \n\nepochs \n4 \n\ntotal iteration \n4000 \n\nParameter \nvalues \n\ninitial learning rate \n1e-4 \n\nlearning rate decay exp(0.99, 100) \n\nbatch size \n20000 \n\nminibatches \n2 \n\nepochs \n10 \n\ntotal iteration \n1000 \n\nweight decay \nl2-norm, 1e-4 \n\n\nWe denote left, right, fore, and hind as L, R, F, H, respectively, to compactly refer to a leg. For example, 'LF leg' refers to the left fore leg.\n\nDynamic locomotion on slippery ground. F Jenelten, J Hwangbo, F Tresoldi, C D Bellicoso, M Hutter, IEEE Robotics and Automation Letters. F. Jenelten, J. Hwangbo, F. Tresoldi, C. D. Bellicoso, M. Hutter, Dynamic locomotion on slippery ground, IEEE Robotics and Automation Letters 4170-4176 (2019).\n\nContact model fusion for event-based locomotion in unstructured terrains. G Bledt, P M Wensing, S Ingersoll, S Kim, IEEE International Conference on Robotics and Automation (ICRA. IEEEG. Bledt, P. M. Wensing, S. Ingersoll, S. Kim, Contact model fusion for event-based locomotion in unstructured terrains, 2018 IEEE Interna- tional Conference on Robotics and Automation (ICRA) (IEEE, 2018).\n\nHeuristic planning for rough terrain locomotion in presence of external disturbances and variable perception quality. M Focchi, R Orsolino, M Camurri, V Barasuol, C Mastalli, D G Caldwell, C Semini, Advances in Robotics Research: From Lab to Market. SpringerM. Focchi, R. Orsolino, M. Camurri, V. Barasuol, C. Mastalli, D. G. Caldwell, C. Semini. Heuristic planning for rough terrain locomotion in presence of external disturbances and variable perception quality. Advances in Robotics Research: From Lab to Market (Springer, 2020), 165-209.\n\nDynamic walking with compliance on a Cassie bipedal robot. J Reher, W Ma, A D Ames, IEEEJ. Reher, W. Ma, A. D. Ames, Dynamic walking with compliance on a Cassie bipedal robot, European Control Conference, 2589-2595 (IEEE, 2019).\n\nY Gong, R Hartley, X Da, A Hereid, O Harib, J Huang, J W Grizzle, Feedback control of a Cassie bipedal robot: Walking, standing, and riding a Segway, American Control Conference. IEEEY. Gong, R. Hartley, X. Da, A. Hereid, O. Harib, J. Huang, J. W. Grizzle, Feedback control of a Cassie bipedal robot: Walking, standing, and riding a Segway, American Control Conference, 4559-4566 (IEEE, 2019).\n\nProbabilistic foot contact estimation by fusing information from dynamics and differential/forward kinematics. J Hwangbo, C D Bellicoso, P Fankhauser, M Huttery, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEJ. Hwangbo, C. D. Bellicoso, P. Fankhauser, M. Huttery, Probabilistic foot contact estimation by fusing information from dynamics and differ- ential/forward kinematics, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 3872-3878 (IEEE, 2016).\n\nProbabilistic contact estimation and impact detection for state estimation of quadruped robots. M Camurri, M Fallon, S Bazeille, A Radulescu, V Barasuol, D G Caldwell, C Semini, IEEE Robotics and Automation Letters. M. Camurri, M. Fallon, S. Bazeille, A. Radulescu, V. Barasuol, D. G. Caldwell, C. Semini, Probabilistic contact estimation and impact de- tection for state estimation of quadruped robots, IEEE Robotics and Automation Letters 1023-1030 (2017).\n\nSlip detection and recovery for quadruped robots. M Focchi, V Barasuol, M Frigerio, D G Caldwell, C Semini, Robotics Research. SpringerM. Focchi, V. Barasuol, M. Frigerio, D. G. Caldwell, C. Semini. Slip de- tection and recovery for quadruped robots. Robotics Research (Springer, 2018), 185-199.\n\nState estimation for legged robots on unstable and slippery terrain. M Bl\u00f6sch, C Gehring, P Fankhauser, M Hutter, M A Hoepflinger, R Siegwart, IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEM. Bl\u00f6sch, C. Gehring, P. Fankhauser, M. Hutter, M. A. Hoepflinger, R. Siegwart, State estimation for legged robots on unstable and slippery terrain, 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, 6058-6064 (IEEE, 2013).\n\nDynamic trotting on slopes for quadrupedal robots. C Gehring, C D Bellicoso, S Coros, M Bloesch, P Fankhauser, M Hutter, R Siegwart, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEC. Gehring, C. D. Bellicoso, S. Coros, M. Bloesch, P. Fankhauser, M. Hutter, R. Siegwart, Dynamic trotting on slopes for quadrupedal robots, 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 5129-5135 (IEEE, 2015).\n\nLegged robot state-estimation through combined forward kinematic and preintegrated contact factors. R Hartley, J Mangelson, L Gan, M G Jadidi, J M Walls, R M Eustice, J W Grizzle, IEEE International Conference on Robotics and Automation (ICRA). IEEER. Hartley, J. Mangelson, L. Gan, M. G. Jadidi, J. M. Walls, R. M. Eu- stice, J. W. Grizzle, Legged robot state-estimation through combined forward kinematic and preintegrated contact factors, 2018 IEEE Inter- national Conference on Robotics and Automation (ICRA), 1-8 (IEEE, 2018).\n\nLearning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics p. 5872J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, M. Hutter, Learning agile and dynamic motor skills for legged robots, Science Robotics p. eaau5872 (2019).\n\nLearning to walk via deep reinforcement learning. T Haarnoja, S Ha, A Zhou, J Tan, G Tucker, S Levine, Robotics: Science and Systems. T. Haarnoja, S. Ha, A. Zhou, J. Tan, G. Tucker, S. Levine, Learning to walk via deep reinforcement learning, Robotics: Science and Systems (2019).\n\nLearning locomotion skills for Cassie: Iterative design and sim-to-real. Z Xie, P Clary, J Dao, P Morais, J Hurst, M Van De Panne, Conference on Robot Learning. Z. Xie, P. Clary, J. Dao, P. Morais, J. Hurst, M. van de Panne, Learning locomotion skills for Cassie: Iterative design and sim-to-real, Confer- ence on Robot Learning (2019).\n\nJ Lee, J Hwangbo, M Hutter, arXiv:1901.07517Robust recovery controller for a quadrupedal robot using deep reinforcement learning. J. Lee, J. Hwangbo, M. Hutter, Robust recovery controller for a quadrupedal robot using deep reinforcement learning, arXiv:1901.07517 (2019).\n\nSim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, Robotics: Science and Systems. J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, V. Vanhoucke, Sim-to-real: Learning agile locomotion for quadruped robots, Robotics: Science and Systems (2018).\n\nData efficient reinforcement learning for legged robots. Y Yang, K Caluwaerts, A Iscen, T Zhang, J Tan, V Sindhwani, Conference on Robot Learning. Y. Yang, K. Caluwaerts, A. Iscen, T. Zhang, J. Tan, V. Sindhwani, Data efficient reinforcement learning for legged robots, Conference on Robot Learning (2019).\n\nS Ha, P Xu, Z Tan, S Levine, J Tan, arXiv:2002.08550Learning to walk in the real world with minimal human effort. S. Ha, P. Xu, Z. Tan, S. Levine, J. Tan, Learning to walk in the real world with minimal human effort, arXiv:2002.08550 (2020).\n\nX B Peng, E Coumans, T Zhang, T.-W Lee, J Tan, S Levine, arXiv:2004.00784Learning agile robotic locomotion skills by imitating animals. X. B. Peng, E. Coumans, T. Zhang, T.-W. Lee, J. Tan, S. Levine, Learning agile robotic locomotion skills by imitating animals, arXiv:2004.00784 (2020).\n\nANYmal -a highly mobile and dynamic quadrupedal robot. M Hutter, C Gehring, D Jud, A Lauber, C D Bellicoso, V Tsounis, J Hwangbo, K Bodie, P Fankhauser, M Bloesch, R Diethelm, S Bachmann, A Melzer, M A H\u00f6pflinger, IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEM. Hutter, C. Gehring, D. Jud, A. Lauber, C. D. Bellicoso, V. Tsou- nis, J. Hwangbo, K. Bodie, P. Fankhauser, M. Bloesch, R. Diethelm, S. Bachmann, A. Melzer, M. A. H\u00f6pflinger, ANYmal -a highly mobile and dynamic quadrupedal robot, IEEE/RSJ International Conference on Intelligent Robots and Systems, 38-44 (IEEE, 2016).\n\nSim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P , IEEE International Conference on Robotics and Automation (ICRA. IEEEX. B. Peng, M. Andrychowicz, W. Zaremba, P. Abbeel, Sim-to-real trans- fer of robotic control with dynamics randomization, IEEE International Conference on Robotics and Automation (ICRA) (IEEE, 2018).\n\nAn empirical evaluation of generic convolutional and recurrent networks for sequence modeling. S Bai, J Z Kolter, V Koltun, arXiv:1803.01271S. Bai, J. Z. Kolter, V. Koltun, An empirical evaluation of generic convolu- tional and recurrent networks for sequence modeling, arXiv:1803.01271 (2018).\n\nLearning by cheating. D Chen, B Zhou, V Koltun, P Kr\u00e4henb\u00fchl, Conference on Robot Learning. D. Chen, B. Zhou, V. Koltun, P. Kr\u00e4henb\u00fchl, Learning by cheating, Conference on Robot Learning (2019).\n\nMinimal criterion coevolution: a new approach to open-ended search. J C Brant, K O Stanley, Genetic and Evolutionary Computation Conference. J. C. Brant, K. O. Stanley, Minimal criterion coevolution: a new ap- proach to open-ended search, Genetic and Evolutionary Computation Conference, 67-74 (2017).\n\nPaired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions. R Wang, J Lehman, J Clune, K O Stanley, arXiv:1901.01753R. Wang, J. Lehman, J. Clune, K. O. Stanley, Paired open-ended trail- blazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions, arXiv:1901.01753 (2019).\n\nDynamic locomotion through online nonlinear motion optimization for quadrupedal robots. C D Bellicoso, F Jenelten, C Gehring, M Hutter, IEEE Robotics and Automation Letters. C. D. Bellicoso, F. Jenelten, C. Gehring, M. Hutter, Dynamic locomotion through online nonlinear motion optimization for quadrupedal robots, IEEE Robotics and Automation Letters 2261-2268 (2018).\n\nRobotcentric elevation mapping with uncertainty estimates. P Fankhauser, M Bloesch, C Gehring, M Hutter, R Siegwart, Mobile Service Robotics. World ScientificP. Fankhauser, M. Bloesch, C. Gehring, M. Hutter, R. Siegwart. Robot- centric elevation mapping with uncertainty estimates. Mobile Service Robotics (World Scientific, 2014), 433-440.\n\nEfficient bipedal robots based on passive-dynamic walkers. S Collins, A Ruina, R Tedrake, M Wisse, Science. S. Collins, A. Ruina, R. Tedrake, M. Wisse, Efficient bipedal robots based on passive-dynamic walkers, Science 1082-1085 (2005).\n\nVision 60: Latest blind-mode stress testing of V60 legged robot. Ghost Robotics. Ghost Robotics, Vision 60: Latest blind-mode stress testing of V60 legged robot, www.youtube.com/watch?v=tQsLauQWp8M (2019).\n\nPer-contact iteration method for solving contact dynamics. J Hwangbo, J Lee, M Hutter, IEEE Robotics and Automation Letters. J. Hwangbo, J. Lee, M. Hutter, Per-contact iteration method for solv- ing contact dynamics, IEEE Robotics and Automation Letters 895-902 (2018).\n\nE Coumans, others, Bullet physics library, Open source: bulletphysics. org. E. Coumans, others, Bullet physics library, Open source: bulletphysics. org (2013).\n\nR Smith, others, Open dynamics engine, Open source: ode.org. R. Smith, others, Open dynamics engine, Open source: ode.org (2005).\n\nR M Alexander, Principles of Animal Locomotion. Princeton University PressR. M. Alexander, Principles of Animal Locomotion (Princeton University Press, 2003).\n\nA Iscen, K Caluwaerts, J Tan, T Zhang, E Coumans, V Sindhwani, V Vanhoucke, Policies modulating trajectory generators, Conference on Robot Learning. A. Iscen, K. Caluwaerts, J. Tan, T. Zhang, E. Coumans, V. Sindhwani, V. Vanhoucke, Policies modulating trajectory generators, Conference on Robot Learning, 916-926 (2018).\n\nA reactive controller framework for quadrupedal locomotion on challenging terrain. V Barasuol, J Buchli, C Semini, M Frigerio, E R De Pieri, D G Caldwell, IEEE International Conference on Robotics and Automation. IEEEV. Barasuol, J. Buchli, C. Semini, M. Frigerio, E. R. De Pieri, D. G. Caldwell, A reactive controller framework for quadrupedal locomotion on challenging terrain, 2013 IEEE International Conference on Robotics and Automation, 2554-2561 (IEEE, 2013).\n\nJ Schulman, S Levine, P Abbeel, M Jordan, P Moritz, Trust region policy optimization, International Conference on Machine Learning. J. Schulman, S. Levine, P. Abbeel, M. Jordan, P. Moritz, Trust re- gion policy optimization, International Conference on Machine Learning, 1889-1897 (2015).\n\nState estimation for legged robots-consistent fusion of leg kinematics and imu. M Bloesch, M Hutter, M A Hoepflinger, S Leutenegger, C Gehring, C D Remy, R Siegwart, M. Bloesch, M. Hutter, M. A. Hoepflinger, S. Leutenegger, C. Gehring, C. D. Remy, R. Siegwart, State estimation for legged robots-consistent fusion of leg kinematics and imu, Robotics 17-24 (2013).\n\nA reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, International Conference on Artificial Intelligence and Statistics. S. Ross, G. Gordon, D. Bagnell, A reduction of imitation learning and structured prediction to no-regret online learning, International Conference on Artificial Intelligence and Statistics, 627-635 (2011).\n\nAutomatic goal generation for reinforcement learning agents. C Florensa, D Held, X Geng, P , International Conference on Machine Learning. C. Florensa, D. Held, X. Geng, P. Abbeel, Automatic goal generation for reinforcement learning agents, International Conference on Machine Learning, 1514-1523 (2018).\n\nRevising the evolutionary computation abstraction: minimal criteria novelty search, Genetic and Evolutionary Computation Conference. J Lehman, K O Stanley, J. Lehman, K. O. Stanley, Revising the evolutionary computation ab- straction: minimal criteria novelty search, Genetic and Evolutionary Computation Conference, 103-110 (2010).\n\nTeacher-student curriculum learning. T Matiisen, A Oliver, T Cohen, J Schulman, IEEE transactions on neural networks and learning systems. T. Matiisen, A. Oliver, T. Cohen, J. Schulman, Teacher-student curricu- lum learning, IEEE transactions on neural networks and learning systems (2019).\n\nLearning symmetric and low-energy locomotion. W Yu, G Turk, C K Liu, ACM Transactions on Graphics. 144W. Yu, G. Turk, C. K. Liu, Learning symmetric and low-energy locomo- tion, ACM Transactions on Graphics (TOG) p. 144 (2018).\n\nRibas, others, Solving rubik's cube with a robot hand. I Akkaya, M Andrychowicz, M Chociej, M Litwin, B Mcgrew, A Petron, A Paino, M Plappert, G Powell, R , arXiv:1910.07113I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, others, Solving rubik's cube with a robot hand, arXiv:1910.07113 (2019).\n\nLearning ground traversability from simulations. R O Chavez-Garcia, J Guzzi, L M Gambardella, A Giusti, IEEE Robotics and Automation Letters. R. O. Chavez-Garcia, J. Guzzi, L. M. Gambardella, A. Giusti, Learning ground traversability from simulations, IEEE Robotics and Automation Letters 1695-1702 (2018).\n\nWhat uncertainties do we need in bayesian deep learning for computer vision?. A Kendall, Y Gal, Advances in neural information processing systems. A. Kendall, Y. Gal, What uncertainties do we need in bayesian deep learning for computer vision?, Advances in neural information processing systems, 5574-5584 (2017).\n\nK Simonyan, A Vedaldi, A Zisserman, arXiv:1312.6034Deep inside convolutional networks: Visualising image classification models and saliency maps. K. Simonyan, A. Vedaldi, A. Zisserman, Deep inside convolutional networks: Visualising image classification models and saliency maps, arXiv:1312.6034 (2013).\n\nSeries elastic actuators. G A Pratt, M M Williamson, IEEE/RSJ International Conference on Intelligent Robots and Systems. G. A. Pratt, M. M. Williamson, Series elastic actuators, IEEE/RSJ International Conference on Intelligent Robots and Systems, 399-406 (1995).\n\nGroenewegen, A survey of procedural methods for terrain modelling. R M Smelik, K J De Kraker, T Tutenel, R Bidarra, S A , Proceedings of the CASA Workshop on 3D Advanced Media In Gaming And Simulation (3AMIGAS). the CASA Workshop on 3D Advanced Media In Gaming And Simulation (3AMIGAS)R. M. Smelik, K. J. De Kraker, T. Tutenel, R. Bidarra, S. A. Groenewe- gen, A survey of procedural methods for terrain modelling, Proceedings of the CASA Workshop on 3D Advanced Media In Gaming And Simulation (3AMIGAS), 25-34 (2009).\n\nA survey of procedural noise functions. A Lagae, S Lefebvre, R Cook, T Derose, G Drettakis, D S Ebert, J P Lewis, K Perlin, M Zwicker, Computer Graphics Forum. Wiley Online LibraryA. Lagae, S. Lefebvre, R. Cook, T. DeRose, G. Drettakis, D. S. Ebert, J. P. Lewis, K. Perlin, M. Zwicker, A survey of procedural noise functions, Computer Graphics Forum, 2579-2600 (Wiley Online Library, 2010).\n\nJ Chung, C Gulcehre, K Cho, Y Bengio, arXiv:1412.3555Empirical evaluation of gated recurrent neural networks on sequence modeling. J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical evaluation of gated recurrent neural networks on sequence modeling, arXiv:1412.3555 (2014).\n\nAn efficient gradient-based algorithm for on-line training of recurrent network trajectories. R J Williams, J Peng, Neural Computation. 490501R. J. Williams, J. Peng, An efficient gradient-based algorithm for on-line training of recurrent network trajectories, Neural Computation 490-501 (1990).\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, International Conference on Learning Representations. D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, International Conference on Learning Representations (2015).\n", "annotations": {"author": "[{\"end\":125,\"start\":64},{\"end\":251,\"start\":126},{\"end\":320,\"start\":252},{\"end\":384,\"start\":321},{\"end\":448,\"start\":385}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":71},{\"end\":139,\"start\":132},{\"end\":269,\"start\":259},{\"end\":335,\"start\":329},{\"end\":397,\"start\":391}]", "author_first_name": "[{\"end\":70,\"start\":64},{\"end\":131,\"start\":126},{\"end\":258,\"start\":252},{\"end\":328,\"start\":321},{\"end\":390,\"start\":385}]", "author_affiliation": "[{\"end\":124,\"start\":76},{\"end\":189,\"start\":141},{\"end\":250,\"start\":191},{\"end\":319,\"start\":271},{\"end\":383,\"start\":337},{\"end\":447,\"start\":399}]", "title": "[{\"end\":57,\"start\":1},{\"end\":505,\"start\":449}]", "venue": "[{\"end\":555,\"start\":507}]", "abstract": "[{\"end\":2248,\"start\":700}]", "bib_ref": "[{\"end\":4455,\"start\":4452},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4458,\"start\":4455},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4461,\"start\":4458},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4464,\"start\":4461},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4467,\"start\":4464},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4616,\"start\":4613},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4619,\"start\":4616},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4622,\"start\":4619},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4885,\"start\":4882},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4889,\"start\":4885},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4893,\"start\":4889},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5264,\"start\":5260},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5268,\"start\":5264},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5272,\"start\":5268},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5645,\"start\":5641},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5649,\"start\":5645},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5653,\"start\":5649},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5657,\"start\":5653},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5903,\"start\":5899},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6064,\"start\":6060},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6067,\"start\":6064},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6071,\"start\":6067},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6075,\"start\":6071},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6079,\"start\":6075},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6083,\"start\":6079},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6508,\"start\":6504},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7133,\"start\":7129},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7136,\"start\":7133},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7139,\"start\":7136},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7367,\"start\":7363},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7647,\"start\":7643},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8156,\"start\":8152},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8565,\"start\":8561},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9950,\"start\":9946},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9953,\"start\":9950},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11755,\"start\":11751},{\"end\":12216,\"start\":12213},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12219,\"start\":12216},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13170,\"start\":13166},{\"end\":14038,\"start\":14035},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14041,\"start\":14038},{\"end\":15475,\"start\":15472},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15478,\"start\":15475},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15524,\"start\":15521},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15527,\"start\":15524},{\"end\":17381,\"start\":17378},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17384,\"start\":17381},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20988,\"start\":20984},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20992,\"start\":20988},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20996,\"start\":20992},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21812,\"start\":21808},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21968,\"start\":21964},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22426,\"start\":22423},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23862,\"start\":23858},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23865,\"start\":23862},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23868,\"start\":23865},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24680,\"start\":24676},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25088,\"start\":25084},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25758,\"start\":25754},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27584,\"start\":27580},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27823,\"start\":27819},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28742,\"start\":28739},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28745,\"start\":28742},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28748,\"start\":28745},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29201,\"start\":29199},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29300,\"start\":29296},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29424,\"start\":29420},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29923,\"start\":29919},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30409,\"start\":30405},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31412,\"start\":31409},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31415,\"start\":31412},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":32237,\"start\":32233},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33032,\"start\":33028},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":33514,\"start\":33510},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":34779,\"start\":34775},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35236,\"start\":35232},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35744,\"start\":35740},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":36148,\"start\":36144},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36604,\"start\":36600},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":36607,\"start\":36604},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36663,\"start\":36659},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":36774,\"start\":36770},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36777,\"start\":36774},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":36979,\"start\":36975},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37885,\"start\":37881},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":37889,\"start\":37885},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":37893,\"start\":37889},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":37897,\"start\":37893},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":38877,\"start\":38873},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":43034,\"start\":43030},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":44677,\"start\":44673},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":47362,\"start\":47358},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":49132,\"start\":49128},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":52275,\"start\":52271},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":52358,\"start\":52354},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":52433,\"start\":52429},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":52697,\"start\":52693},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":52813,\"start\":52809},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":52816,\"start\":52813},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":54325,\"start\":54321},{\"end\":54690,\"start\":54688},{\"end\":54719,\"start\":54717},{\"end\":54748,\"start\":54746},{\"end\":54773,\"start\":54771},{\"end\":54847,\"start\":54845},{\"end\":54972,\"start\":54969},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":55002,\"start\":54998},{\"end\":55050,\"start\":55047},{\"end\":55123,\"start\":55120},{\"end\":55153,\"start\":55150},{\"end\":55256,\"start\":55253},{\"end\":55296,\"start\":55293},{\"end\":55326,\"start\":55323},{\"end\":55381,\"start\":55378},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":56784,\"start\":56780},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":58054,\"start\":58050},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":58767,\"start\":58763},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":59121,\"start\":59117},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":62247,\"start\":62243},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":62340,\"start\":62336},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":62694,\"start\":62690},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":64742,\"start\":64738},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":65206,\"start\":65202},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":65347,\"start\":65343},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":73519,\"start\":73515}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":65469,\"start\":65367},{\"attributes\":{\"id\":\"fig_2\"},\"end\":66470,\"start\":65470},{\"attributes\":{\"id\":\"fig_5\"},\"end\":67072,\"start\":66471},{\"attributes\":{\"id\":\"fig_6\"},\"end\":68131,\"start\":67073},{\"attributes\":{\"id\":\"fig_7\"},\"end\":69016,\"start\":68132},{\"attributes\":{\"id\":\"fig_8\"},\"end\":70243,\"start\":69017},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":70740,\"start\":70244},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":71926,\"start\":70741},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":72632,\"start\":71927},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":73318,\"start\":72633},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":74138,\"start\":73319}]", "paragraph": "[{\"end\":2811,\"start\":2264},{\"end\":4218,\"start\":2813},{\"end\":5124,\"start\":4220},{\"end\":5658,\"start\":5126},{\"end\":6084,\"start\":5660},{\"end\":7014,\"start\":6086},{\"end\":7648,\"start\":7016},{\"end\":8472,\"start\":7650},{\"end\":9433,\"start\":8474},{\"end\":11147,\"start\":9435},{\"end\":11480,\"start\":11217},{\"end\":11657,\"start\":11505},{\"end\":11675,\"start\":11659},{\"end\":12138,\"start\":11677},{\"end\":12460,\"start\":12140},{\"end\":13279,\"start\":12462},{\"end\":13812,\"start\":13281},{\"end\":14601,\"start\":13845},{\"end\":14860,\"start\":14603},{\"end\":15217,\"start\":14883},{\"end\":15528,\"start\":15219},{\"end\":17309,\"start\":15530},{\"end\":18157,\"start\":17311},{\"end\":18602,\"start\":18159},{\"end\":19944,\"start\":18604},{\"end\":20282,\"start\":19946},{\"end\":20997,\"start\":20297},{\"end\":21235,\"start\":20999},{\"end\":21595,\"start\":21237},{\"end\":22066,\"start\":21597},{\"end\":22133,\"start\":22068},{\"end\":23577,\"start\":22135},{\"end\":24153,\"start\":23617},{\"end\":24555,\"start\":24155},{\"end\":25598,\"start\":24557},{\"end\":26886,\"start\":25600},{\"end\":27456,\"start\":26888},{\"end\":27737,\"start\":27458},{\"end\":28025,\"start\":27739},{\"end\":28602,\"start\":28046},{\"end\":28785,\"start\":28604},{\"end\":29301,\"start\":28957},{\"end\":30374,\"start\":29303},{\"end\":30793,\"start\":30376},{\"end\":30952,\"start\":30795},{\"end\":31445,\"start\":30954},{\"end\":32093,\"start\":31464},{\"end\":32411,\"start\":32095},{\"end\":33997,\"start\":32413},{\"end\":34102,\"start\":33999},{\"end\":34293,\"start\":34104},{\"end\":34729,\"start\":34295},{\"end\":34862,\"start\":34731},{\"end\":35167,\"start\":34881},{\"end\":35844,\"start\":35169},{\"end\":35932,\"start\":35846},{\"end\":36492,\"start\":36014},{\"end\":37042,\"start\":36524},{\"end\":37737,\"start\":37044},{\"end\":38199,\"start\":37739},{\"end\":38442,\"start\":38292},{\"end\":38690,\"start\":38444},{\"end\":39128,\"start\":38758},{\"end\":39177,\"start\":39130},{\"end\":39291,\"start\":39217},{\"end\":39742,\"start\":39293},{\"end\":39839,\"start\":39744},{\"end\":39973,\"start\":39846},{\"end\":40094,\"start\":39975},{\"end\":40872,\"start\":40096},{\"end\":41023,\"start\":40874},{\"end\":41463,\"start\":41099},{\"end\":41979,\"start\":41491},{\"end\":42671,\"start\":41981},{\"end\":42770,\"start\":42700},{\"end\":42884,\"start\":42772},{\"end\":43645,\"start\":42921},{\"end\":44464,\"start\":43647},{\"end\":44843,\"start\":44488},{\"end\":45403,\"start\":44845},{\"end\":46542,\"start\":45435},{\"end\":46681,\"start\":46584},{\"end\":47363,\"start\":46683},{\"end\":48898,\"start\":47434},{\"end\":49322,\"start\":48900},{\"end\":49723,\"start\":49378},{\"end\":50097,\"start\":49725},{\"end\":50686,\"start\":50143},{\"end\":50739,\"start\":50688},{\"end\":50813,\"start\":50741},{\"end\":50901,\"start\":50815},{\"end\":50942,\"start\":50903},{\"end\":50995,\"start\":50944},{\"end\":51055,\"start\":50997},{\"end\":51148,\"start\":51057},{\"end\":51188,\"start\":51150},{\"end\":51269,\"start\":51190},{\"end\":51324,\"start\":51271},{\"end\":51379,\"start\":51326},{\"end\":51427,\"start\":51381},{\"end\":51461,\"start\":51429},{\"end\":51504,\"start\":51463},{\"end\":51531,\"start\":51506},{\"end\":51561,\"start\":51533},{\"end\":51628,\"start\":51563},{\"end\":52219,\"start\":51676},{\"end\":52698,\"start\":52250},{\"end\":53080,\"start\":52700},{\"end\":53482,\"start\":53082},{\"end\":54169,\"start\":53583},{\"end\":54326,\"start\":54171},{\"end\":54515,\"start\":54328},{\"end\":54807,\"start\":54517},{\"end\":54865,\"start\":54809},{\"end\":54932,\"start\":54867},{\"end\":55006,\"start\":54934},{\"end\":55220,\"start\":55021},{\"end\":55450,\"start\":55222},{\"end\":55554,\"start\":55521},{\"end\":56173,\"start\":55693},{\"end\":56385,\"start\":56225},{\"end\":56555,\"start\":56387},{\"end\":56785,\"start\":56649},{\"end\":56941,\"start\":56787},{\"end\":57050,\"start\":57008},{\"end\":57229,\"start\":57052},{\"end\":57294,\"start\":57289},{\"end\":57407,\"start\":57344},{\"end\":57802,\"start\":57409},{\"end\":58014,\"start\":57869},{\"end\":58255,\"start\":58016},{\"end\":58487,\"start\":58319},{\"end\":58888,\"start\":58549},{\"end\":59070,\"start\":58890},{\"end\":59464,\"start\":59072},{\"end\":59685,\"start\":59466},{\"end\":59842,\"start\":59687},{\"end\":60048,\"start\":59844},{\"end\":61072,\"start\":60114},{\"end\":62137,\"start\":61149},{\"end\":62460,\"start\":62185},{\"end\":63025,\"start\":62542},{\"end\":63203,\"start\":63027},{\"end\":63518,\"start\":63275},{\"end\":64056,\"start\":63567},{\"end\":64172,\"start\":64076},{\"end\":64211,\"start\":64174},{\"end\":64267,\"start\":64213},{\"end\":64364,\"start\":64269},{\"end\":65366,\"start\":64366}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11206,\"start\":11148},{\"attributes\":{\"id\":\"formula_1\"},\"end\":28956,\"start\":28786},{\"attributes\":{\"id\":\"formula_2\"},\"end\":36013,\"start\":35933},{\"attributes\":{\"id\":\"formula_3\"},\"end\":38291,\"start\":38200},{\"attributes\":{\"id\":\"formula_4\"},\"end\":38757,\"start\":38691},{\"attributes\":{\"id\":\"formula_5\"},\"end\":39216,\"start\":39178},{\"attributes\":{\"id\":\"formula_7\"},\"end\":41098,\"start\":41024},{\"attributes\":{\"id\":\"formula_8\"},\"end\":41490,\"start\":41464},{\"attributes\":{\"id\":\"formula_9\"},\"end\":47433,\"start\":47364},{\"attributes\":{\"id\":\"formula_10\"},\"end\":49377,\"start\":49323},{\"attributes\":{\"id\":\"formula_11\"},\"end\":51675,\"start\":51629},{\"attributes\":{\"id\":\"formula_12\"},\"end\":53582,\"start\":53483},{\"attributes\":{\"id\":\"formula_13\"},\"end\":55020,\"start\":55007},{\"attributes\":{\"id\":\"formula_14\"},\"end\":55488,\"start\":55457},{\"attributes\":{\"id\":\"formula_15\"},\"end\":55692,\"start\":55555},{\"attributes\":{\"id\":\"formula_16\"},\"end\":56648,\"start\":56556},{\"attributes\":{\"id\":\"formula_17\"},\"end\":57007,\"start\":56942},{\"attributes\":{\"id\":\"formula_18\"},\"end\":57288,\"start\":57230},{\"attributes\":{\"id\":\"formula_19\"},\"end\":57343,\"start\":57295},{\"attributes\":{\"id\":\"formula_20\"},\"end\":57868,\"start\":57803},{\"attributes\":{\"id\":\"formula_21\"},\"end\":58318,\"start\":58256},{\"attributes\":{\"id\":\"formula_22\"},\"end\":58519,\"start\":58488},{\"attributes\":{\"id\":\"formula_23\"},\"end\":62541,\"start\":62461},{\"attributes\":{\"id\":\"formula_24\"},\"end\":63566,\"start\":63519}]", "table_ref": "[{\"end\":12690,\"start\":12683},{\"end\":13191,\"start\":13184},{\"end\":13328,\"start\":13321},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":32651,\"start\":32643},{\"end\":34861,\"start\":34853},{\"end\":35556,\"start\":35539},{\"end\":36491,\"start\":36483},{\"end\":43167,\"start\":43159},{\"end\":45736,\"start\":45728},{\"end\":50900,\"start\":50892},{\"end\":50941,\"start\":50933},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":50994,\"start\":50986},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":51054,\"start\":51046},{\"end\":51147,\"start\":51139},{\"end\":51187,\"start\":51179},{\"end\":51268,\"start\":51260},{\"end\":51323,\"start\":51315},{\"end\":51378,\"start\":51370},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":53192,\"start\":53184},{\"end\":54666,\"start\":54656},{\"end\":59028,\"start\":59020},{\"end\":62393,\"start\":62377},{\"end\":63202,\"start\":63194},{\"end\":63849,\"start\":63841},{\"end\":64412,\"start\":64404},{\"end\":64821,\"start\":64813},{\"end\":65023,\"start\":65015},{\"end\":65217,\"start\":65209}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2262,\"start\":2250},{\"attributes\":{\"n\":\"2.\"},\"end\":11215,\"start\":11208},{\"end\":11503,\"start\":11483},{\"end\":13843,\"start\":13815},{\"end\":14881,\"start\":14863},{\"attributes\":{\"n\":\"3.\"},\"end\":20295,\"start\":20285},{\"attributes\":{\"n\":\"4.\"},\"end\":23601,\"start\":23580},{\"end\":23615,\"start\":23604},{\"end\":28044,\"start\":28028},{\"end\":31462,\"start\":31448},{\"end\":34879,\"start\":34865},{\"end\":36522,\"start\":36495},{\"end\":39844,\"start\":39842},{\"end\":42698,\"start\":42674},{\"end\":42919,\"start\":42887},{\"end\":44486,\"start\":44467},{\"end\":45433,\"start\":45406},{\"end\":46582,\"start\":46545},{\"attributes\":{\"n\":\"5.\"},\"end\":50115,\"start\":50100},{\"end\":50141,\"start\":50118},{\"end\":52248,\"start\":52222},{\"end\":55456,\"start\":55453},{\"end\":55519,\"start\":55490},{\"end\":56223,\"start\":56176},{\"end\":58547,\"start\":58521},{\"end\":60112,\"start\":60051},{\"end\":61147,\"start\":61075},{\"end\":62183,\"start\":62140},{\"end\":63273,\"start\":63206},{\"end\":64074,\"start\":64059},{\"end\":65376,\"start\":65368},{\"end\":65479,\"start\":65471},{\"end\":67082,\"start\":67074},{\"end\":69036,\"start\":69018},{\"end\":71938,\"start\":71928},{\"end\":72644,\"start\":72634}]", "table": "[{\"end\":70740,\"start\":70322},{\"end\":71926,\"start\":71271},{\"end\":72632,\"start\":71990},{\"end\":73318,\"start\":72744},{\"end\":74138,\"start\":73538}]", "figure_caption": "[{\"end\":65469,\"start\":65378},{\"end\":66470,\"start\":65481},{\"end\":67072,\"start\":66473},{\"end\":68131,\"start\":67084},{\"end\":69016,\"start\":68134},{\"end\":70243,\"start\":69041},{\"end\":70322,\"start\":70246},{\"end\":71271,\"start\":70743},{\"end\":71990,\"start\":71941},{\"end\":72744,\"start\":72647},{\"end\":73538,\"start\":73321}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2939,\"start\":2933},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6420,\"start\":6414},{\"end\":11379,\"start\":11368},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11417,\"start\":11395},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11599,\"start\":11593},{\"end\":11797,\"start\":11789},{\"end\":11813,\"start\":11805},{\"end\":11844,\"start\":11836},{\"end\":12596,\"start\":12586},{\"end\":13950,\"start\":13941},{\"end\":14859,\"start\":14852},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15018,\"start\":15011},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15247,\"start\":15240},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15606,\"start\":15599},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16058,\"start\":16051},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16787,\"start\":16780},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17523,\"start\":17516},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17972,\"start\":17965},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18285,\"start\":18278},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18417,\"start\":18410},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18942,\"start\":18935},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19312,\"start\":19305},{\"end\":24601,\"start\":24595},{\"end\":24690,\"start\":24681},{\"end\":25989,\"start\":25982},{\"end\":25999,\"start\":25993},{\"end\":27379,\"start\":27372},{\"end\":27502,\"start\":27495},{\"end\":28121,\"start\":28114},{\"end\":33996,\"start\":33989},{\"end\":34366,\"start\":34359},{\"end\":37465,\"start\":37458},{\"end\":42128,\"start\":42122},{\"end\":43402,\"start\":43393},{\"end\":43411,\"start\":43404},{\"end\":43666,\"start\":43659},{\"end\":43839,\"start\":43828},{\"end\":43924,\"start\":43917},{\"end\":44138,\"start\":44131},{\"end\":44882,\"start\":44875},{\"end\":44893,\"start\":44886},{\"end\":45016,\"start\":45009},{\"end\":45282,\"start\":45275},{\"end\":45606,\"start\":45599},{\"end\":45878,\"start\":45862},{\"end\":46103,\"start\":46094},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":47773,\"start\":47767},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":47839,\"start\":47831},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":47897,\"start\":47890},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":49735,\"start\":49728},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":50012,\"start\":50005},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50685,\"start\":50676},{\"end\":50738,\"start\":50729},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":50812,\"start\":50803},{\"end\":58979,\"start\":58972},{\"end\":59840,\"start\":59833},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60175,\"start\":60168},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60185,\"start\":60177},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60404,\"start\":60396},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60524,\"start\":60516},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60534,\"start\":60526},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60696,\"start\":60688},{\"end\":61159,\"start\":61152},{\"end\":61240,\"start\":61232},{\"end\":61684,\"start\":61676},{\"end\":61826,\"start\":61818},{\"end\":62049,\"start\":62041},{\"end\":62062,\"start\":62054},{\"end\":62755,\"start\":62748},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":62775,\"start\":62768},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":63649,\"start\":63642}]", "bib_author_first_name": "[{\"end\":74327,\"start\":74326},{\"end\":74339,\"start\":74338},{\"end\":74350,\"start\":74349},{\"end\":74362,\"start\":74361},{\"end\":74364,\"start\":74363},{\"end\":74377,\"start\":74376},{\"end\":74660,\"start\":74659},{\"end\":74669,\"start\":74668},{\"end\":74671,\"start\":74670},{\"end\":74682,\"start\":74681},{\"end\":74695,\"start\":74694},{\"end\":75095,\"start\":75094},{\"end\":75105,\"start\":75104},{\"end\":75117,\"start\":75116},{\"end\":75128,\"start\":75127},{\"end\":75140,\"start\":75139},{\"end\":75152,\"start\":75151},{\"end\":75154,\"start\":75153},{\"end\":75166,\"start\":75165},{\"end\":75579,\"start\":75578},{\"end\":75588,\"start\":75587},{\"end\":75594,\"start\":75593},{\"end\":75596,\"start\":75595},{\"end\":75750,\"start\":75749},{\"end\":75758,\"start\":75757},{\"end\":75769,\"start\":75768},{\"end\":75775,\"start\":75774},{\"end\":75785,\"start\":75784},{\"end\":75794,\"start\":75793},{\"end\":75803,\"start\":75802},{\"end\":75805,\"start\":75804},{\"end\":76256,\"start\":76255},{\"end\":76267,\"start\":76266},{\"end\":76269,\"start\":76268},{\"end\":76282,\"start\":76281},{\"end\":76296,\"start\":76295},{\"end\":76758,\"start\":76757},{\"end\":76769,\"start\":76768},{\"end\":76779,\"start\":76778},{\"end\":76791,\"start\":76790},{\"end\":76804,\"start\":76803},{\"end\":76816,\"start\":76815},{\"end\":76818,\"start\":76817},{\"end\":76830,\"start\":76829},{\"end\":77172,\"start\":77171},{\"end\":77182,\"start\":77181},{\"end\":77194,\"start\":77193},{\"end\":77206,\"start\":77205},{\"end\":77208,\"start\":77207},{\"end\":77220,\"start\":77219},{\"end\":77488,\"start\":77487},{\"end\":77498,\"start\":77497},{\"end\":77509,\"start\":77508},{\"end\":77523,\"start\":77522},{\"end\":77533,\"start\":77532},{\"end\":77535,\"start\":77534},{\"end\":77550,\"start\":77549},{\"end\":77935,\"start\":77934},{\"end\":77946,\"start\":77945},{\"end\":77948,\"start\":77947},{\"end\":77961,\"start\":77960},{\"end\":77970,\"start\":77969},{\"end\":77981,\"start\":77980},{\"end\":77995,\"start\":77994},{\"end\":78005,\"start\":78004},{\"end\":78444,\"start\":78443},{\"end\":78455,\"start\":78454},{\"end\":78468,\"start\":78467},{\"end\":78475,\"start\":78474},{\"end\":78477,\"start\":78476},{\"end\":78487,\"start\":78486},{\"end\":78489,\"start\":78488},{\"end\":78498,\"start\":78497},{\"end\":78500,\"start\":78499},{\"end\":78511,\"start\":78510},{\"end\":78513,\"start\":78512},{\"end\":78936,\"start\":78935},{\"end\":78947,\"start\":78946},{\"end\":78954,\"start\":78953},{\"end\":78969,\"start\":78968},{\"end\":78982,\"start\":78981},{\"end\":78993,\"start\":78992},{\"end\":79003,\"start\":79002},{\"end\":79268,\"start\":79267},{\"end\":79280,\"start\":79279},{\"end\":79286,\"start\":79285},{\"end\":79294,\"start\":79293},{\"end\":79301,\"start\":79300},{\"end\":79311,\"start\":79310},{\"end\":79573,\"start\":79572},{\"end\":79580,\"start\":79579},{\"end\":79589,\"start\":79588},{\"end\":79596,\"start\":79595},{\"end\":79606,\"start\":79605},{\"end\":79615,\"start\":79614},{\"end\":79838,\"start\":79837},{\"end\":79845,\"start\":79844},{\"end\":79856,\"start\":79855},{\"end\":80172,\"start\":80171},{\"end\":80179,\"start\":80178},{\"end\":80188,\"start\":80187},{\"end\":80199,\"start\":80198},{\"end\":80208,\"start\":80207},{\"end\":80215,\"start\":80214},{\"end\":80225,\"start\":80224},{\"end\":80234,\"start\":80233},{\"end\":80518,\"start\":80517},{\"end\":80526,\"start\":80525},{\"end\":80540,\"start\":80539},{\"end\":80549,\"start\":80548},{\"end\":80558,\"start\":80557},{\"end\":80565,\"start\":80564},{\"end\":80769,\"start\":80768},{\"end\":80775,\"start\":80774},{\"end\":80781,\"start\":80780},{\"end\":80788,\"start\":80787},{\"end\":80798,\"start\":80797},{\"end\":81012,\"start\":81011},{\"end\":81014,\"start\":81013},{\"end\":81022,\"start\":81021},{\"end\":81033,\"start\":81032},{\"end\":81045,\"start\":81041},{\"end\":81052,\"start\":81051},{\"end\":81059,\"start\":81058},{\"end\":81356,\"start\":81355},{\"end\":81366,\"start\":81365},{\"end\":81377,\"start\":81376},{\"end\":81384,\"start\":81383},{\"end\":81394,\"start\":81393},{\"end\":81396,\"start\":81395},{\"end\":81409,\"start\":81408},{\"end\":81420,\"start\":81419},{\"end\":81431,\"start\":81430},{\"end\":81440,\"start\":81439},{\"end\":81454,\"start\":81453},{\"end\":81465,\"start\":81464},{\"end\":81477,\"start\":81476},{\"end\":81489,\"start\":81488},{\"end\":81499,\"start\":81498},{\"end\":81501,\"start\":81500},{\"end\":81979,\"start\":81978},{\"end\":81981,\"start\":81980},{\"end\":81989,\"start\":81988},{\"end\":82005,\"start\":82004},{\"end\":82016,\"start\":82015},{\"end\":82385,\"start\":82384},{\"end\":82392,\"start\":82391},{\"end\":82394,\"start\":82393},{\"end\":82404,\"start\":82403},{\"end\":82608,\"start\":82607},{\"end\":82616,\"start\":82615},{\"end\":82624,\"start\":82623},{\"end\":82634,\"start\":82633},{\"end\":82850,\"start\":82849},{\"end\":82852,\"start\":82851},{\"end\":82861,\"start\":82860},{\"end\":82863,\"start\":82862},{\"end\":83220,\"start\":83219},{\"end\":83228,\"start\":83227},{\"end\":83238,\"start\":83237},{\"end\":83247,\"start\":83246},{\"end\":83249,\"start\":83248},{\"end\":83572,\"start\":83571},{\"end\":83574,\"start\":83573},{\"end\":83587,\"start\":83586},{\"end\":83599,\"start\":83598},{\"end\":83610,\"start\":83609},{\"end\":83914,\"start\":83913},{\"end\":83928,\"start\":83927},{\"end\":83939,\"start\":83938},{\"end\":83950,\"start\":83949},{\"end\":83960,\"start\":83959},{\"end\":84256,\"start\":84255},{\"end\":84267,\"start\":84266},{\"end\":84276,\"start\":84275},{\"end\":84287,\"start\":84286},{\"end\":84701,\"start\":84700},{\"end\":84712,\"start\":84711},{\"end\":84719,\"start\":84718},{\"end\":84913,\"start\":84912},{\"end\":85074,\"start\":85073},{\"end\":85205,\"start\":85204},{\"end\":85207,\"start\":85206},{\"end\":85365,\"start\":85364},{\"end\":85374,\"start\":85373},{\"end\":85388,\"start\":85387},{\"end\":85395,\"start\":85394},{\"end\":85404,\"start\":85403},{\"end\":85415,\"start\":85414},{\"end\":85428,\"start\":85427},{\"end\":85770,\"start\":85769},{\"end\":85782,\"start\":85781},{\"end\":85792,\"start\":85791},{\"end\":85802,\"start\":85801},{\"end\":85814,\"start\":85813},{\"end\":85816,\"start\":85815},{\"end\":85828,\"start\":85827},{\"end\":85830,\"start\":85829},{\"end\":86155,\"start\":86154},{\"end\":86167,\"start\":86166},{\"end\":86177,\"start\":86176},{\"end\":86187,\"start\":86186},{\"end\":86197,\"start\":86196},{\"end\":86525,\"start\":86524},{\"end\":86536,\"start\":86535},{\"end\":86546,\"start\":86545},{\"end\":86548,\"start\":86547},{\"end\":86563,\"start\":86562},{\"end\":86578,\"start\":86577},{\"end\":86589,\"start\":86588},{\"end\":86591,\"start\":86590},{\"end\":86599,\"start\":86598},{\"end\":86900,\"start\":86899},{\"end\":86908,\"start\":86907},{\"end\":86918,\"start\":86917},{\"end\":87265,\"start\":87264},{\"end\":87277,\"start\":87276},{\"end\":87285,\"start\":87284},{\"end\":87293,\"start\":87292},{\"end\":87644,\"start\":87643},{\"end\":87654,\"start\":87653},{\"end\":87656,\"start\":87655},{\"end\":87882,\"start\":87881},{\"end\":87894,\"start\":87893},{\"end\":87904,\"start\":87903},{\"end\":87913,\"start\":87912},{\"end\":88183,\"start\":88182},{\"end\":88189,\"start\":88188},{\"end\":88197,\"start\":88196},{\"end\":88199,\"start\":88198},{\"end\":88420,\"start\":88419},{\"end\":88430,\"start\":88429},{\"end\":88446,\"start\":88445},{\"end\":88457,\"start\":88456},{\"end\":88467,\"start\":88466},{\"end\":88477,\"start\":88476},{\"end\":88487,\"start\":88486},{\"end\":88496,\"start\":88495},{\"end\":88508,\"start\":88507},{\"end\":88518,\"start\":88517},{\"end\":88778,\"start\":88777},{\"end\":88780,\"start\":88779},{\"end\":88797,\"start\":88796},{\"end\":88806,\"start\":88805},{\"end\":88808,\"start\":88807},{\"end\":88823,\"start\":88822},{\"end\":89115,\"start\":89114},{\"end\":89126,\"start\":89125},{\"end\":89352,\"start\":89351},{\"end\":89364,\"start\":89363},{\"end\":89375,\"start\":89374},{\"end\":89683,\"start\":89682},{\"end\":89685,\"start\":89684},{\"end\":89694,\"start\":89693},{\"end\":89696,\"start\":89695},{\"end\":89989,\"start\":89988},{\"end\":89991,\"start\":89990},{\"end\":90001,\"start\":90000},{\"end\":90003,\"start\":90002},{\"end\":90016,\"start\":90015},{\"end\":90027,\"start\":90026},{\"end\":90038,\"start\":90037},{\"end\":90040,\"start\":90039},{\"end\":90482,\"start\":90481},{\"end\":90491,\"start\":90490},{\"end\":90503,\"start\":90502},{\"end\":90511,\"start\":90510},{\"end\":90521,\"start\":90520},{\"end\":90534,\"start\":90533},{\"end\":90536,\"start\":90535},{\"end\":90545,\"start\":90544},{\"end\":90547,\"start\":90546},{\"end\":90556,\"start\":90555},{\"end\":90566,\"start\":90565},{\"end\":90834,\"start\":90833},{\"end\":90843,\"start\":90842},{\"end\":90855,\"start\":90854},{\"end\":90862,\"start\":90861},{\"end\":91204,\"start\":91203},{\"end\":91206,\"start\":91205},{\"end\":91218,\"start\":91217},{\"end\":91451,\"start\":91450},{\"end\":91453,\"start\":91452},{\"end\":91463,\"start\":91462}]", "bib_author_last_name": "[{\"end\":74336,\"start\":74328},{\"end\":74347,\"start\":74340},{\"end\":74359,\"start\":74351},{\"end\":74374,\"start\":74365},{\"end\":74384,\"start\":74378},{\"end\":74666,\"start\":74661},{\"end\":74679,\"start\":74672},{\"end\":74692,\"start\":74683},{\"end\":74699,\"start\":74696},{\"end\":75102,\"start\":75096},{\"end\":75114,\"start\":75106},{\"end\":75125,\"start\":75118},{\"end\":75137,\"start\":75129},{\"end\":75149,\"start\":75141},{\"end\":75163,\"start\":75155},{\"end\":75173,\"start\":75167},{\"end\":75585,\"start\":75580},{\"end\":75591,\"start\":75589},{\"end\":75601,\"start\":75597},{\"end\":75755,\"start\":75751},{\"end\":75766,\"start\":75759},{\"end\":75772,\"start\":75770},{\"end\":75782,\"start\":75776},{\"end\":75791,\"start\":75786},{\"end\":75800,\"start\":75795},{\"end\":75813,\"start\":75806},{\"end\":76264,\"start\":76257},{\"end\":76279,\"start\":76270},{\"end\":76293,\"start\":76283},{\"end\":76304,\"start\":76297},{\"end\":76766,\"start\":76759},{\"end\":76776,\"start\":76770},{\"end\":76788,\"start\":76780},{\"end\":76801,\"start\":76792},{\"end\":76813,\"start\":76805},{\"end\":76827,\"start\":76819},{\"end\":76837,\"start\":76831},{\"end\":77179,\"start\":77173},{\"end\":77191,\"start\":77183},{\"end\":77203,\"start\":77195},{\"end\":77217,\"start\":77209},{\"end\":77227,\"start\":77221},{\"end\":77495,\"start\":77489},{\"end\":77506,\"start\":77499},{\"end\":77520,\"start\":77510},{\"end\":77530,\"start\":77524},{\"end\":77547,\"start\":77536},{\"end\":77559,\"start\":77551},{\"end\":77943,\"start\":77936},{\"end\":77958,\"start\":77949},{\"end\":77967,\"start\":77962},{\"end\":77978,\"start\":77971},{\"end\":77992,\"start\":77982},{\"end\":78002,\"start\":77996},{\"end\":78014,\"start\":78006},{\"end\":78452,\"start\":78445},{\"end\":78465,\"start\":78456},{\"end\":78472,\"start\":78469},{\"end\":78484,\"start\":78478},{\"end\":78495,\"start\":78490},{\"end\":78508,\"start\":78501},{\"end\":78521,\"start\":78514},{\"end\":78944,\"start\":78937},{\"end\":78951,\"start\":78948},{\"end\":78966,\"start\":78955},{\"end\":78979,\"start\":78970},{\"end\":78990,\"start\":78983},{\"end\":79000,\"start\":78994},{\"end\":79010,\"start\":79004},{\"end\":79277,\"start\":79269},{\"end\":79283,\"start\":79281},{\"end\":79291,\"start\":79287},{\"end\":79298,\"start\":79295},{\"end\":79308,\"start\":79302},{\"end\":79318,\"start\":79312},{\"end\":79577,\"start\":79574},{\"end\":79586,\"start\":79581},{\"end\":79593,\"start\":79590},{\"end\":79603,\"start\":79597},{\"end\":79612,\"start\":79607},{\"end\":79628,\"start\":79616},{\"end\":79842,\"start\":79839},{\"end\":79853,\"start\":79846},{\"end\":79863,\"start\":79857},{\"end\":80176,\"start\":80173},{\"end\":80185,\"start\":80180},{\"end\":80196,\"start\":80189},{\"end\":80205,\"start\":80200},{\"end\":80212,\"start\":80209},{\"end\":80222,\"start\":80216},{\"end\":80231,\"start\":80226},{\"end\":80244,\"start\":80235},{\"end\":80523,\"start\":80519},{\"end\":80537,\"start\":80527},{\"end\":80546,\"start\":80541},{\"end\":80555,\"start\":80550},{\"end\":80562,\"start\":80559},{\"end\":80575,\"start\":80566},{\"end\":80772,\"start\":80770},{\"end\":80778,\"start\":80776},{\"end\":80785,\"start\":80782},{\"end\":80795,\"start\":80789},{\"end\":80802,\"start\":80799},{\"end\":81019,\"start\":81015},{\"end\":81030,\"start\":81023},{\"end\":81039,\"start\":81034},{\"end\":81049,\"start\":81046},{\"end\":81056,\"start\":81053},{\"end\":81066,\"start\":81060},{\"end\":81363,\"start\":81357},{\"end\":81374,\"start\":81367},{\"end\":81381,\"start\":81378},{\"end\":81391,\"start\":81385},{\"end\":81406,\"start\":81397},{\"end\":81417,\"start\":81410},{\"end\":81428,\"start\":81421},{\"end\":81437,\"start\":81432},{\"end\":81451,\"start\":81441},{\"end\":81462,\"start\":81455},{\"end\":81474,\"start\":81466},{\"end\":81486,\"start\":81478},{\"end\":81496,\"start\":81490},{\"end\":81512,\"start\":81502},{\"end\":81986,\"start\":81982},{\"end\":82002,\"start\":81990},{\"end\":82013,\"start\":82006},{\"end\":82389,\"start\":82386},{\"end\":82401,\"start\":82395},{\"end\":82411,\"start\":82405},{\"end\":82613,\"start\":82609},{\"end\":82621,\"start\":82617},{\"end\":82631,\"start\":82625},{\"end\":82645,\"start\":82635},{\"end\":82858,\"start\":82853},{\"end\":82871,\"start\":82864},{\"end\":83225,\"start\":83221},{\"end\":83235,\"start\":83229},{\"end\":83244,\"start\":83239},{\"end\":83257,\"start\":83250},{\"end\":83584,\"start\":83575},{\"end\":83596,\"start\":83588},{\"end\":83607,\"start\":83600},{\"end\":83617,\"start\":83611},{\"end\":83925,\"start\":83915},{\"end\":83936,\"start\":83929},{\"end\":83947,\"start\":83940},{\"end\":83957,\"start\":83951},{\"end\":83969,\"start\":83961},{\"end\":84264,\"start\":84257},{\"end\":84273,\"start\":84268},{\"end\":84284,\"start\":84277},{\"end\":84293,\"start\":84288},{\"end\":84709,\"start\":84702},{\"end\":84716,\"start\":84713},{\"end\":84726,\"start\":84720},{\"end\":84921,\"start\":84914},{\"end\":85080,\"start\":85075},{\"end\":85217,\"start\":85208},{\"end\":85371,\"start\":85366},{\"end\":85385,\"start\":85375},{\"end\":85392,\"start\":85389},{\"end\":85401,\"start\":85396},{\"end\":85412,\"start\":85405},{\"end\":85425,\"start\":85416},{\"end\":85438,\"start\":85429},{\"end\":85779,\"start\":85771},{\"end\":85789,\"start\":85783},{\"end\":85799,\"start\":85793},{\"end\":85811,\"start\":85803},{\"end\":85825,\"start\":85817},{\"end\":85839,\"start\":85831},{\"end\":86164,\"start\":86156},{\"end\":86174,\"start\":86168},{\"end\":86184,\"start\":86178},{\"end\":86194,\"start\":86188},{\"end\":86204,\"start\":86198},{\"end\":86533,\"start\":86526},{\"end\":86543,\"start\":86537},{\"end\":86560,\"start\":86549},{\"end\":86575,\"start\":86564},{\"end\":86586,\"start\":86579},{\"end\":86596,\"start\":86592},{\"end\":86608,\"start\":86600},{\"end\":86905,\"start\":86901},{\"end\":86915,\"start\":86909},{\"end\":86926,\"start\":86919},{\"end\":87274,\"start\":87266},{\"end\":87282,\"start\":87278},{\"end\":87290,\"start\":87286},{\"end\":87651,\"start\":87645},{\"end\":87664,\"start\":87657},{\"end\":87891,\"start\":87883},{\"end\":87901,\"start\":87895},{\"end\":87910,\"start\":87905},{\"end\":87922,\"start\":87914},{\"end\":88186,\"start\":88184},{\"end\":88194,\"start\":88190},{\"end\":88203,\"start\":88200},{\"end\":88427,\"start\":88421},{\"end\":88443,\"start\":88431},{\"end\":88454,\"start\":88447},{\"end\":88464,\"start\":88458},{\"end\":88474,\"start\":88468},{\"end\":88484,\"start\":88478},{\"end\":88493,\"start\":88488},{\"end\":88505,\"start\":88497},{\"end\":88515,\"start\":88509},{\"end\":88794,\"start\":88781},{\"end\":88803,\"start\":88798},{\"end\":88820,\"start\":88809},{\"end\":88830,\"start\":88824},{\"end\":89123,\"start\":89116},{\"end\":89130,\"start\":89127},{\"end\":89361,\"start\":89353},{\"end\":89372,\"start\":89365},{\"end\":89385,\"start\":89376},{\"end\":89691,\"start\":89686},{\"end\":89707,\"start\":89697},{\"end\":89998,\"start\":89992},{\"end\":90013,\"start\":90004},{\"end\":90024,\"start\":90017},{\"end\":90035,\"start\":90028},{\"end\":90488,\"start\":90483},{\"end\":90500,\"start\":90492},{\"end\":90508,\"start\":90504},{\"end\":90518,\"start\":90512},{\"end\":90531,\"start\":90522},{\"end\":90542,\"start\":90537},{\"end\":90553,\"start\":90548},{\"end\":90563,\"start\":90557},{\"end\":90574,\"start\":90567},{\"end\":90840,\"start\":90835},{\"end\":90852,\"start\":90844},{\"end\":90859,\"start\":90856},{\"end\":90869,\"start\":90863},{\"end\":91215,\"start\":91207},{\"end\":91223,\"start\":91219},{\"end\":91460,\"start\":91454},{\"end\":91466,\"start\":91464}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":199541368},\"end\":74583,\"start\":74287},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52291968},\"end\":74974,\"start\":74585},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":44061745},\"end\":75517,\"start\":74976},{\"attributes\":{\"id\":\"b3\"},\"end\":75747,\"start\":75519},{\"attributes\":{\"id\":\"b4\"},\"end\":76142,\"start\":75749},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16203754},\"end\":76659,\"start\":76144},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":775191},\"end\":77119,\"start\":76661},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":46318912},\"end\":77416,\"start\":77121},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7395885},\"end\":77881,\"start\":77418},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15896090},\"end\":78341,\"start\":77883},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3504071},\"end\":78874,\"start\":78343},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":58031572},\"end\":79215,\"start\":78876},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":57189150},\"end\":79497,\"start\":79217},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":218875900},\"end\":79835,\"start\":79499},{\"attributes\":{\"doi\":\"arXiv:1901.07517\",\"id\":\"b14\"},\"end\":80108,\"start\":79837},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13750177},\"end\":80458,\"start\":80110},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":195833719},\"end\":80766,\"start\":80460},{\"attributes\":{\"doi\":\"arXiv:2002.08550\",\"id\":\"b17\"},\"end\":81009,\"start\":80768},{\"attributes\":{\"doi\":\"arXiv:2004.00784\",\"id\":\"b18\"},\"end\":81298,\"start\":81011},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":19007565},\"end\":81907,\"start\":81300},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3707478},\"end\":82287,\"start\":81909},{\"attributes\":{\"doi\":\"arXiv:1803.01271\",\"id\":\"b21\"},\"end\":82583,\"start\":82289},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":204780964},\"end\":82779,\"start\":82585},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":9494471},\"end\":83082,\"start\":82781},{\"attributes\":{\"doi\":\"arXiv:1901.01753\",\"id\":\"b24\"},\"end\":83481,\"start\":83084},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4569339},\"end\":83852,\"start\":83483},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12745373},\"end\":84194,\"start\":83854},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1315227},\"end\":84432,\"start\":84196},{\"attributes\":{\"id\":\"b28\"},\"end\":84639,\"start\":84434},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3742121},\"end\":84910,\"start\":84641},{\"attributes\":{\"id\":\"b30\"},\"end\":85071,\"start\":84912},{\"attributes\":{\"id\":\"b31\"},\"end\":85202,\"start\":85073},{\"attributes\":{\"id\":\"b32\"},\"end\":85362,\"start\":85204},{\"attributes\":{\"id\":\"b33\"},\"end\":85684,\"start\":85364},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6021870},\"end\":86152,\"start\":85686},{\"attributes\":{\"id\":\"b35\"},\"end\":86442,\"start\":86154},{\"attributes\":{\"id\":\"b36\"},\"end\":86807,\"start\":86444},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":103456},\"end\":87201,\"start\":86809},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":22729745},\"end\":87508,\"start\":87203},{\"attributes\":{\"id\":\"b39\"},\"end\":87842,\"start\":87510},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":8432394},\"end\":88134,\"start\":87844},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":21747305},\"end\":88362,\"start\":88136},{\"attributes\":{\"doi\":\"arXiv:1910.07113\",\"id\":\"b42\"},\"end\":88726,\"start\":88364},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":3666270},\"end\":89034,\"start\":88728},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":71134},\"end\":89349,\"start\":89036},{\"attributes\":{\"doi\":\"arXiv:1312.6034\",\"id\":\"b45\"},\"end\":89654,\"start\":89351},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":17120394},\"end\":89919,\"start\":89656},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":9981404},\"end\":90439,\"start\":89921},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":762012},\"end\":90831,\"start\":90441},{\"attributes\":{\"doi\":\"arXiv:1412.3555\",\"id\":\"b49\"},\"end\":91107,\"start\":90833},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":12979634},\"end\":91404,\"start\":91109},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":6628106},\"end\":91647,\"start\":91406}]", "bib_title": "[{\"end\":74324,\"start\":74287},{\"end\":74657,\"start\":74585},{\"end\":75092,\"start\":74976},{\"end\":76253,\"start\":76144},{\"end\":76755,\"start\":76661},{\"end\":77169,\"start\":77121},{\"end\":77485,\"start\":77418},{\"end\":77932,\"start\":77883},{\"end\":78441,\"start\":78343},{\"end\":78933,\"start\":78876},{\"end\":79265,\"start\":79217},{\"end\":79570,\"start\":79499},{\"end\":80169,\"start\":80110},{\"end\":80515,\"start\":80460},{\"end\":81353,\"start\":81300},{\"end\":81976,\"start\":81909},{\"end\":82605,\"start\":82585},{\"end\":82847,\"start\":82781},{\"end\":83569,\"start\":83483},{\"end\":83911,\"start\":83854},{\"end\":84253,\"start\":84196},{\"end\":84497,\"start\":84434},{\"end\":84698,\"start\":84641},{\"end\":85767,\"start\":85686},{\"end\":86897,\"start\":86809},{\"end\":87262,\"start\":87203},{\"end\":87879,\"start\":87844},{\"end\":88180,\"start\":88136},{\"end\":88775,\"start\":88728},{\"end\":89112,\"start\":89036},{\"end\":89680,\"start\":89656},{\"end\":89986,\"start\":89921},{\"end\":90479,\"start\":90441},{\"end\":91201,\"start\":91109},{\"end\":91448,\"start\":91406}]", "bib_author": "[{\"end\":74338,\"start\":74326},{\"end\":74349,\"start\":74338},{\"end\":74361,\"start\":74349},{\"end\":74376,\"start\":74361},{\"end\":74386,\"start\":74376},{\"end\":74668,\"start\":74659},{\"end\":74681,\"start\":74668},{\"end\":74694,\"start\":74681},{\"end\":74701,\"start\":74694},{\"end\":75104,\"start\":75094},{\"end\":75116,\"start\":75104},{\"end\":75127,\"start\":75116},{\"end\":75139,\"start\":75127},{\"end\":75151,\"start\":75139},{\"end\":75165,\"start\":75151},{\"end\":75175,\"start\":75165},{\"end\":75587,\"start\":75578},{\"end\":75593,\"start\":75587},{\"end\":75603,\"start\":75593},{\"end\":75757,\"start\":75749},{\"end\":75768,\"start\":75757},{\"end\":75774,\"start\":75768},{\"end\":75784,\"start\":75774},{\"end\":75793,\"start\":75784},{\"end\":75802,\"start\":75793},{\"end\":75815,\"start\":75802},{\"end\":76266,\"start\":76255},{\"end\":76281,\"start\":76266},{\"end\":76295,\"start\":76281},{\"end\":76306,\"start\":76295},{\"end\":76768,\"start\":76757},{\"end\":76778,\"start\":76768},{\"end\":76790,\"start\":76778},{\"end\":76803,\"start\":76790},{\"end\":76815,\"start\":76803},{\"end\":76829,\"start\":76815},{\"end\":76839,\"start\":76829},{\"end\":77181,\"start\":77171},{\"end\":77193,\"start\":77181},{\"end\":77205,\"start\":77193},{\"end\":77219,\"start\":77205},{\"end\":77229,\"start\":77219},{\"end\":77497,\"start\":77487},{\"end\":77508,\"start\":77497},{\"end\":77522,\"start\":77508},{\"end\":77532,\"start\":77522},{\"end\":77549,\"start\":77532},{\"end\":77561,\"start\":77549},{\"end\":77945,\"start\":77934},{\"end\":77960,\"start\":77945},{\"end\":77969,\"start\":77960},{\"end\":77980,\"start\":77969},{\"end\":77994,\"start\":77980},{\"end\":78004,\"start\":77994},{\"end\":78016,\"start\":78004},{\"end\":78454,\"start\":78443},{\"end\":78467,\"start\":78454},{\"end\":78474,\"start\":78467},{\"end\":78486,\"start\":78474},{\"end\":78497,\"start\":78486},{\"end\":78510,\"start\":78497},{\"end\":78523,\"start\":78510},{\"end\":78946,\"start\":78935},{\"end\":78953,\"start\":78946},{\"end\":78968,\"start\":78953},{\"end\":78981,\"start\":78968},{\"end\":78992,\"start\":78981},{\"end\":79002,\"start\":78992},{\"end\":79012,\"start\":79002},{\"end\":79279,\"start\":79267},{\"end\":79285,\"start\":79279},{\"end\":79293,\"start\":79285},{\"end\":79300,\"start\":79293},{\"end\":79310,\"start\":79300},{\"end\":79320,\"start\":79310},{\"end\":79579,\"start\":79572},{\"end\":79588,\"start\":79579},{\"end\":79595,\"start\":79588},{\"end\":79605,\"start\":79595},{\"end\":79614,\"start\":79605},{\"end\":79630,\"start\":79614},{\"end\":79844,\"start\":79837},{\"end\":79855,\"start\":79844},{\"end\":79865,\"start\":79855},{\"end\":80178,\"start\":80171},{\"end\":80187,\"start\":80178},{\"end\":80198,\"start\":80187},{\"end\":80207,\"start\":80198},{\"end\":80214,\"start\":80207},{\"end\":80224,\"start\":80214},{\"end\":80233,\"start\":80224},{\"end\":80246,\"start\":80233},{\"end\":80525,\"start\":80517},{\"end\":80539,\"start\":80525},{\"end\":80548,\"start\":80539},{\"end\":80557,\"start\":80548},{\"end\":80564,\"start\":80557},{\"end\":80577,\"start\":80564},{\"end\":80774,\"start\":80768},{\"end\":80780,\"start\":80774},{\"end\":80787,\"start\":80780},{\"end\":80797,\"start\":80787},{\"end\":80804,\"start\":80797},{\"end\":81021,\"start\":81011},{\"end\":81032,\"start\":81021},{\"end\":81041,\"start\":81032},{\"end\":81051,\"start\":81041},{\"end\":81058,\"start\":81051},{\"end\":81068,\"start\":81058},{\"end\":81365,\"start\":81355},{\"end\":81376,\"start\":81365},{\"end\":81383,\"start\":81376},{\"end\":81393,\"start\":81383},{\"end\":81408,\"start\":81393},{\"end\":81419,\"start\":81408},{\"end\":81430,\"start\":81419},{\"end\":81439,\"start\":81430},{\"end\":81453,\"start\":81439},{\"end\":81464,\"start\":81453},{\"end\":81476,\"start\":81464},{\"end\":81488,\"start\":81476},{\"end\":81498,\"start\":81488},{\"end\":81514,\"start\":81498},{\"end\":81988,\"start\":81978},{\"end\":82004,\"start\":81988},{\"end\":82015,\"start\":82004},{\"end\":82019,\"start\":82015},{\"end\":82391,\"start\":82384},{\"end\":82403,\"start\":82391},{\"end\":82413,\"start\":82403},{\"end\":82615,\"start\":82607},{\"end\":82623,\"start\":82615},{\"end\":82633,\"start\":82623},{\"end\":82647,\"start\":82633},{\"end\":82860,\"start\":82849},{\"end\":82873,\"start\":82860},{\"end\":83227,\"start\":83219},{\"end\":83237,\"start\":83227},{\"end\":83246,\"start\":83237},{\"end\":83259,\"start\":83246},{\"end\":83586,\"start\":83571},{\"end\":83598,\"start\":83586},{\"end\":83609,\"start\":83598},{\"end\":83619,\"start\":83609},{\"end\":83927,\"start\":83913},{\"end\":83938,\"start\":83927},{\"end\":83949,\"start\":83938},{\"end\":83959,\"start\":83949},{\"end\":83971,\"start\":83959},{\"end\":84266,\"start\":84255},{\"end\":84275,\"start\":84266},{\"end\":84286,\"start\":84275},{\"end\":84295,\"start\":84286},{\"end\":84711,\"start\":84700},{\"end\":84718,\"start\":84711},{\"end\":84728,\"start\":84718},{\"end\":84923,\"start\":84912},{\"end\":85082,\"start\":85073},{\"end\":85219,\"start\":85204},{\"end\":85373,\"start\":85364},{\"end\":85387,\"start\":85373},{\"end\":85394,\"start\":85387},{\"end\":85403,\"start\":85394},{\"end\":85414,\"start\":85403},{\"end\":85427,\"start\":85414},{\"end\":85440,\"start\":85427},{\"end\":85781,\"start\":85769},{\"end\":85791,\"start\":85781},{\"end\":85801,\"start\":85791},{\"end\":85813,\"start\":85801},{\"end\":85827,\"start\":85813},{\"end\":85841,\"start\":85827},{\"end\":86166,\"start\":86154},{\"end\":86176,\"start\":86166},{\"end\":86186,\"start\":86176},{\"end\":86196,\"start\":86186},{\"end\":86206,\"start\":86196},{\"end\":86535,\"start\":86524},{\"end\":86545,\"start\":86535},{\"end\":86562,\"start\":86545},{\"end\":86577,\"start\":86562},{\"end\":86588,\"start\":86577},{\"end\":86598,\"start\":86588},{\"end\":86610,\"start\":86598},{\"end\":86907,\"start\":86899},{\"end\":86917,\"start\":86907},{\"end\":86928,\"start\":86917},{\"end\":87276,\"start\":87264},{\"end\":87284,\"start\":87276},{\"end\":87292,\"start\":87284},{\"end\":87296,\"start\":87292},{\"end\":87653,\"start\":87643},{\"end\":87666,\"start\":87653},{\"end\":87893,\"start\":87881},{\"end\":87903,\"start\":87893},{\"end\":87912,\"start\":87903},{\"end\":87924,\"start\":87912},{\"end\":88188,\"start\":88182},{\"end\":88196,\"start\":88188},{\"end\":88205,\"start\":88196},{\"end\":88429,\"start\":88419},{\"end\":88445,\"start\":88429},{\"end\":88456,\"start\":88445},{\"end\":88466,\"start\":88456},{\"end\":88476,\"start\":88466},{\"end\":88486,\"start\":88476},{\"end\":88495,\"start\":88486},{\"end\":88507,\"start\":88495},{\"end\":88517,\"start\":88507},{\"end\":88521,\"start\":88517},{\"end\":88796,\"start\":88777},{\"end\":88805,\"start\":88796},{\"end\":88822,\"start\":88805},{\"end\":88832,\"start\":88822},{\"end\":89125,\"start\":89114},{\"end\":89132,\"start\":89125},{\"end\":89363,\"start\":89351},{\"end\":89374,\"start\":89363},{\"end\":89387,\"start\":89374},{\"end\":89693,\"start\":89682},{\"end\":89709,\"start\":89693},{\"end\":90000,\"start\":89988},{\"end\":90015,\"start\":90000},{\"end\":90026,\"start\":90015},{\"end\":90037,\"start\":90026},{\"end\":90043,\"start\":90037},{\"end\":90490,\"start\":90481},{\"end\":90502,\"start\":90490},{\"end\":90510,\"start\":90502},{\"end\":90520,\"start\":90510},{\"end\":90533,\"start\":90520},{\"end\":90544,\"start\":90533},{\"end\":90555,\"start\":90544},{\"end\":90565,\"start\":90555},{\"end\":90576,\"start\":90565},{\"end\":90842,\"start\":90833},{\"end\":90854,\"start\":90842},{\"end\":90861,\"start\":90854},{\"end\":90871,\"start\":90861},{\"end\":91217,\"start\":91203},{\"end\":91225,\"start\":91217},{\"end\":91462,\"start\":91450},{\"end\":91468,\"start\":91462}]", "bib_venue": "[{\"end\":74422,\"start\":74386},{\"end\":74763,\"start\":74701},{\"end\":75224,\"start\":75175},{\"end\":75576,\"start\":75519},{\"end\":75926,\"start\":75815},{\"end\":76380,\"start\":76306},{\"end\":76875,\"start\":76839},{\"end\":77246,\"start\":77229},{\"end\":77628,\"start\":77561},{\"end\":78090,\"start\":78016},{\"end\":78586,\"start\":78523},{\"end\":79030,\"start\":79012},{\"end\":79349,\"start\":79320},{\"end\":79658,\"start\":79630},{\"end\":79965,\"start\":79881},{\"end\":80275,\"start\":80246},{\"end\":80605,\"start\":80577},{\"end\":80880,\"start\":80820},{\"end\":81145,\"start\":81084},{\"end\":81581,\"start\":81514},{\"end\":82081,\"start\":82019},{\"end\":82382,\"start\":82289},{\"end\":82675,\"start\":82647},{\"end\":82920,\"start\":82873},{\"end\":83217,\"start\":83084},{\"end\":83655,\"start\":83619},{\"end\":83994,\"start\":83971},{\"end\":84302,\"start\":84295},{\"end\":84513,\"start\":84499},{\"end\":84764,\"start\":84728},{\"end\":84986,\"start\":84923},{\"end\":85132,\"start\":85082},{\"end\":85250,\"start\":85219},{\"end\":85511,\"start\":85440},{\"end\":85897,\"start\":85841},{\"end\":86284,\"start\":86206},{\"end\":86522,\"start\":86444},{\"end\":86994,\"start\":86928},{\"end\":87340,\"start\":87296},{\"end\":87641,\"start\":87510},{\"end\":87981,\"start\":87924},{\"end\":88233,\"start\":88205},{\"end\":88417,\"start\":88364},{\"end\":88868,\"start\":88832},{\"end\":89181,\"start\":89132},{\"end\":89495,\"start\":89402},{\"end\":89776,\"start\":89709},{\"end\":90131,\"start\":90043},{\"end\":90599,\"start\":90576},{\"end\":90962,\"start\":90886},{\"end\":91243,\"start\":91225},{\"end\":91520,\"start\":91468},{\"end\":90206,\"start\":90133}]"}}}, "year": 2023, "month": 12, "day": 17}