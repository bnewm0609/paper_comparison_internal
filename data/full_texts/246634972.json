{"id": 246634972, "updated": "2023-12-17 18:33:22.638", "metadata": {"title": "Textured Mesh Quality Assessment: Large-scale Dataset and Deep Learning-based Quality Metric", "authors": "[{\"first\":\"YANA\",\"last\":\"NEHM\u00c9\",\"middle\":[]}]", "venue": "ACM Transactions on Graphics", "journal": "ACM Transactions on Graphics", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Over the past decade, 3D graphics have become highly detailed to mimic the real world, exploding their size and complexity. Certain applications and device constraints necessitate their simplification and/or lossy compression, which can degrade their visual quality. Thus, to ensure the best Quality of Experience (QoE), it is important to evaluate the visual quality to accurately drive the compression and find the right compromise between visual quality and data size. In this work, we focus on subjective and objective quality assessment of textured 3D meshes. We first establish a large-scale dataset, which includes 55 source models quantitatively characterized in terms of geometric, color, and semantic complexity, and corrupted by combinations of 5 types of compression-based distortions applied on the geometry, texture mapping and texture image of the meshes. This dataset contains over 343k distorted stimuli. We propose an approach to select a challenging subset of 3000 stimuli for which we collected 148929 quality judgments from over 4500 participantsinalarge-scalecrowdsourcedsubjectiveexperiment.Leveragingoursubject-rateddataset,alearning-basedqualitymetricfor3Dgraphicswas proposed.Ourmetricdemonstratesstate-of-the-artresultsonourdatasetoftexturedmeshesandonadatasetofdistortedmesheswithvertexcolors. Finally,wepresentanapplicationofourmetricanddatasettoexploretheinfluenceofdistortioninteractionsandcontentcharacteristicsonthe perceivedqualityofcompressedtexturedmeshes.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tog/NehmeDDFCL23", "doi": "10.1145/3592786"}}, "content": {"source": {"pdf_hash": "d1ad378d143279e014276cdfb442595cedbd3686", "pdf_src": "ScienceParsePlus", "pdf_uri": "[\"https://export.arxiv.org/pdf/2202.02397v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2202.02397", "status": "GREEN"}}, "grobid": {"id": "a0cb185088f25f58289d7b28d78032ed9b28ca58", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/d1ad378d143279e014276cdfb442595cedbd3686.txt", "contents": "\nTextured Mesh Quality Assessment: Large-Scale Dataset and Deep Learning-based Quality Metric\n\n\nYana Nehm\u00e9 \nUMR5205\nUniv Lyon\nINSA Lyon\nCNRS\nUCBL, LIRIS\nFrance\n\nUMR5205\nJOHANNA DELANOY\nUniv Lyon\nINSA Lyon\nCNRS\nUCBL, LIRIS\nFrance\n\nFLORENT DUPONT\nUMR5205\nUniv Lyon\nUCBL\nCNRS\nINSA Lyon, LIRIS\nFrance\n\nUMR5205\nJEAN-PHILIPPE FARRUGIA\nUniv Lyon\nUCBL\nCNRS\nINSA Lyon, LIRIS\nFrance\n\nUMR 6004\nPATRICK LE CALLET\nNantes Universit\u00e9\n\u00c9cole Centrale Nantes\nCNRS\nLS2NFrance\n\nGUILLAUME LAVOU\u00c9\nUniv Lyon\nCentrale Lyon\n\nUMR5205\nCNRS\nINSA Lyon\nUCBL, LIRIS\nENISE\nFrance\n\nTextured Mesh Quality Assessment: Large-Scale Dataset and Deep Learning-based Quality Metric\n1BBC47EE8F6337A74EED9DC6B75B737510.1145/3592786CCS Concepts:Computing methodologies \u2192 PerceptionMesh modelsTexturingImage-based renderingNeural networks Computer Graphics, Perception, 3D Mesh, Texture, Visual Quality Assessment, Subjective Quality Evaluation, Objective Quality Evaluation, Dataset, Perceptual Metric, Deep Learning, Crowdsourcing\nOver the past decade, 3D graphics have become highly detailed to mimic the real world, exploding their size and complexity.Certain applications and device constraints necessitate their simplification and/or lossy compression, which can degrade their visual quality.Thus, to ensure the best Quality of Experience (QoE), it is important to evaluate the visual quality to accurately drive the compression and find the right compromise between visual quality and data size.In this work, we focus on subjective and objective quality assessment of textured 3D meshes.We first establish a large-scale dataset, which includes 55 source models quantitatively characterized in terms of geometric, color, and semantic complexity, and corrupted by combinations of 5 types of compression-based distortions applied on the geometry, texture mapping and texture image of the meshes.This dataset contains over 343k distorted stimuli.We propose an approach to select a challenging subset of 3000 stimuli for which we collected 148929 quality judgments from over 4500 participants in a large-scale crowdsourced subjective experiment.Leveraging our subject-rated dataset, a learning-based quality metric for 3D graphics was proposed.Our metric demonstrates state-of-the-art results on our dataset of textured meshes and on a dataset of distorted meshes with vertex colors.Finally, we present an application of our metric and dataset to explore the influence of distortion interactions and content characteristics on the perceived quality of compressed textured meshes.\n\nINTRODUCTION\n\nThe use of 3D graphical data is growing for the general public with the proliferation of acquisition technologies (3D scanners, 360 \u00b0cameras, MRI, etc.), intuitive 3D modeling tools, 3D printers, and affordable virtual and mixed reality Head-Mounted Displays -HMD-(Oculus Rift, HTC Vive, Microsoft HoloLens, etc.).All of these technologies make the size and complexity of 3D data explode.The Authors' addresses: Yana Nehm\u00e9, yana.nehme@gmail.com,Univ Lyon, INSA Lyon, CNRS, UCBL, LIRIS, UMR5205, F-69621 Villeurbanne, France; Johanna Delanoy, johanna.delanoy@insa-lyon.fr,Univ Lyon, INSA Lyon, CNRS, UCBL, LIRIS, UMR5205, F-69621 Villeurbanne, France; Florent Dupont, Florent.Dupont@liris.cnrs.fr,Univ Lyon, UCBL, CNRS, INSA Lyon, LIRIS, UMR5205, F-69622 Villeurbanne, France; Jean-Philippe Farrugia, jean-philippe.farrugia@univ-lyon1.fr, Univ Lyon, UCBL, CNRS, INSA Lyon, LIRIS, UMR5205, F-69622 Villeurbanne, France; Patrick Le Callet, patrick.lecallet@univnantes.fr, Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France; Guillaume Lavou\u00e9, glavoue@liris.cnrs.fr,Univ Lyon, Centrale Lyon, CNRS, INSA Lyon, UCBL, LIRIS, UMR5205, ENISE, F-42023 Saint Etienne, France.resulting 3D scenes are huge and extremely detailed: they may contain several million geometric primitives, associated with a wide range of appearance attributes, intended to reproduce a realistic material appearance.\n\nExtended reality -XR-(i.e.Augmented and Virtual Reality AR/VR) is seen as the next potential computing platform.The great advantage of XR technologies is that they provide 6 Degrees of Freedom (6DoF) allowing realistic interactions and a high level of immersion.However, the visualization and interaction in XR of large and complex 3D scenes remains an unsolved issue to date due to two major challenges: (1) the complexity of a 3D scene that can be displayed on a HMD is substantially smaller than that on a standard screen, (2) for networked applications, latency problems may occur when streaming the 3D scene on the client device.This problem is growing as more online VR/AR applications consider 3D data stored on remote servers.\n\nTo adapt the complexity of 3D content for HMDs (notably for autonomous devices, such as the Occulus Quest 2) and to avoid latency due to transmission, simplification and compression are inevitable.These operations reduce the amount of data (reduce the Level of Details (LoD) and the size of 3D data) and by extension the costs in processing, storage, and transmission.However, such operations are lossy and result in visual degradations that may affect the perceived quality of the 3D scene and, in turn, the user's Quality Of Experience (QoE).It is therefore essential to define measures to accurately assess the impact of these distortions in order to find the right compromise between visual quality and data size/LoD.For this purpose, quality assessment methodologies are required.\n\nThe perceptual quality can be assessed using subjective studies and objective metrics.Objective metrics consist in algorithms designed to automatically predict the visual quality loss (i.e. the level of annoyance of visual artifacts).On the other hand, subjective studies, aka.user studies, involve inviting a group of participants to assess the visual quality of test data.These subjective experiments provide the most reliable way to create ground-truth datasets useful for understanding human psychological behavior (when perceiving multimedia content) as well as for benchmarking and tuning objective quality metrics.\n\nPublic quality assessment metrics and datasets for 3D graphics are lacking, especially for meshes with color and/or texture attributes.Indeed, existing datasets are rather small and have limited generalization ability, making them not challenging enough to test and benchmark quality metrics and insufficient to train and drive learning-based ones.\n\nIn this work, we produce a valuable large-scale quality assessment dataset of textured meshes, with more than 343k distorted meshes generated from the compression of 55 source models.To ensure the generalization ability of our dataset, (1) we devised a set of measures to quantitatively characterize the source models in order to avoid biases related to the selection of these models.(2) We involved mixed compression-based distortions of different natures, such as texture compression, geometry quantization, UV map quantization, LoD and texture sub-sampling.(3) We proposed an approach for the selection of a challenging subset of 3000 stimuli that was subject-rated in a large-scale crowdsourced experiment.\n\nLeveraging our annotated dataset, we propose an image-based deep learning quality metric for 3D graphics, called Graphics-LPIPS (Learned Perceptual Image Patch Similarity).It operates on patches of rendered stimuli that are fed to a convolutional neural network with learning weights on top to extract features.Features are then fused and pooled to predict the quality of the patch.The global quality score of the stimulus is obtained by averaging the quality of local patches.Our metric outperforms other image quality metrics in terms of correlation and classification ability on our dataset of textured meshes and on an existing dataset of meshes with vertex colors.\n\nFinally, our metric allowed us to annotate the remaining stimuli in our dataset to subsequently explore the influence of distortion interactions and content characteristics on the perceived quality of 3D graphics.\n\nThe main contributions of our work are as follows:\n\n\u2022 We provide the largest dataset of textured meshes with over 343k stimuli generated from 55 source models quantitatively characterized in terms of geometric, color, and semantic complexity to ensure their diversity.The dataset covers a wide range of compression-based distortions applied with different strengths.The database can be used to train noreference quality metrics and develop rate-distortion models for meshes.\u2022 From the established dataset, we carefully selected a challenging subset of 3000 stimuli that we annotated in a largescale subjective experiment in crowdsourcing involving over 4500 participants.To the best of our knowledge, it is the largest quality assessment dataset of textured meshes associated with subjective scores to date.This database is valuable for training and benchmarking quality metrics.\u2022 We propose an image-based metric, named Graphics-LPIPS, for assessing the quality of rendered 3D graphics.It employs convolutional neural networks.Our metric demonstrates state-of-the-art results on two different datasets.\u2022 Leveraging our whole dataset and metric, we provide an in-depth analysis on the effect of each distortion and their combinations on the perceived quality of textured meshes.We also evaluate the influence of the geometric and color complexity of the model on the perception of distortions.\n\nThe datasets and the source code of the metric are publicly available1 .\n\nThe rest of this paper is organized as follows: Section 2 reviews previous work on subjective and objective quality assessment of 3D graphics.Section 3 describes our dataset and the set of measures we propose for 3D content characterization.Section 4 details the subjective experiment and the process adopted to select the subset of test stimuli.We describe in section 5 our learning-based quality metric and evaluate its performance.Section 6 presents an application of the metric and the dataset.Concluding remarks and perspectives are provided in section 8.\n\n\nRELATED WORK\n\nIn this section, we review previous work on subjective and objective quality assessment of graphical 3D content.We provide an overview of existing datasets and metrics for predicting the visual impact of distortions applied on such data.Note that, 3D data can be represented in different ways (3D meshes, point clouds, voxels), with and without appearance attributes (color, texture, material, etc.).We are specifically interested in 3D meshes and point clouds with color/texture attributes.\n\n\nSubjective quality assessment\n\nSubjective quality tests involving 3D models were initially introduced on meshes, more precisely on geometry-only models, to assess the artifacts induced by the simplification, smoothing, watermarking and compression [Christaki et al. 2018;Corsini et al. 2007;Lavou\u00e9 2009;Torkhani et al. 2015;Vanhoey et al. 2017;V\u00e1\u0161a and Rus 2012;Watson 2001].Little work considered meshes with color attributes (vertex color or texture) [Guo et al. 2016;Guti\u00e9rrez et al. 2020;Nehm\u00e9 et al. 2020;Nehm\u00e9 et al. 2021b;Pan et al. 2005;Zerman et al. 2020].\n\nSubjective quality experiments involving point clouds have become prevalent over the last six years.The pioneering studies were conducted on colorless point cloud content [Alexiou and Ebrahimi 2017;Alexiou et al. 2017;Javaheri et al. 2017;Javaheri et al. 2019;Su et al. 2019].Subsequently, the majority of the studies focused on evaluating the quality of colored point clouds [Alexiou et al. 2019;Liu et al. 2021a;Perry et al. 2020;Subramanyam et al. 2020;Torlig et al. 2018;Wu et al. 2021;Yang et al. 2020;Zerman et al. 2019Zerman et al. , 2020]].\n\nThe aforementioned works considered different subjective methodologies, different ways to display the 3D models to participants (still images, animations, interactive scenes) and different test equipment (2D screen, augmented reality and virtual reality headsets).\n\nThe experimental methodologies used were inspired by existing image/video subjective methodologies.They are mainly derived from single stimulus methods, in which participants see only one stimulus and rate its quality [Corsini et al. 2007;Guti\u00e9rrez et al. 2020;Subramanyam et al. 2020;Torkhani et al. 2015;Viola et al. 2022;Zerman et al. 2020], double stimulus methods in which participants rate the visual degradation after seeing the reference and distorted stimuli [Lavou\u00e9 2009;Nehm\u00e9 et al. 2021b;Perry et al. 2020;Su et al. 2019;Torlig et al. 2018;Watson 2001], and pairwise comparison methods in which participants choose the better quality stimulus from two stimuli presented to them [Alexiou et al. 2019;Christaki et al. 2018;Guo et al. 2016;Vanhoey et al. 2017;V\u00e1\u0161a and Rus 2012].Recently, a couple of comprehensive/comparative studies [Alexiou and Ebrahimi 2017;Nehm\u00e9 et al. 2020] evaluated the impact of the subjective methodologies on the obtained quality scores.They found that the double stimulus method is the most suitable to assess the quality of 3D graphics.\n\nSome researchers have implemented real-time interactive scenes, allowing participants to freely interact (rotate and zoom) with the 3D models being rated (real-time interactive inspection) [Alexiou and Ebrahimi 2017;Alexiou et al. 2017Alexiou et al. , 2020;;Guti\u00e9rrez et al. 2020;Mekuria et al. 2017;Subramanyam et al. 2020;Torlig et al. 2018;Wu et al. 2021;Yang et al. 2020].These experiments are conducted on desktop devices as well as in immersive environments with varying Degrees of Freedom (DoF).Other researchers have controlled the viewpoints visualized by the participants to ensure the same user experience (passive inspection).They used 2D still images or predefined camera paths to generate videos of the models [Guo et al. 2016;Javaheri et al. 2017;Nehm\u00e9 et al. 2021b;Pan et al. 2005;Rogowitz and Rushmeier 2001;Su et al. 2019;Zerman et al. 2019Zerman et al. , 2020]].This approach avoids cognitive overload that can alter human judgments.The effect of adopting different modes of inspection for subjective quality assessment is still unclear/an open question as very few comparisons have been made to date [Torkhani et al. 2015;Viola et al. 2022].\n\nMost of the reported experiments for both meshes and point clouds were conducted on desktop settings.Only the studies presented in [Alexiou et al. 2020;Christaki et al. 2018;Nehm\u00e9 et al. 2020;Nehm\u00e9 et al. 2021b;Subramanyam et al. 2020;Viola et al. 2022;Wu et al. 2021] considered a VR environment, and those presented in [Alexiou et al. 2017;Guti\u00e9rrez et al. 2020] a AR environment.An early attempt of 3D tele-immersion was reported in [Mekuria et al. 2017].So far, no work has been done to understand the impact of display devices on the perceived quality of 3D content.\n\nThe experiments presented above were conducted in laboratories.In recent years, CrowdSourcing (CS) experiments have gained popularity, as they are relatively fast and are therefore more practical for evaluating large-scale datasets.A recent study investigated whether a crowdsourcing test can achieve the accuracy of a laboratory test for the quality assessment of 3D graphics [Nehm\u00e9 et al. 2021a].The results showed that under controlled conditions and with a proper participant screening approach, a crowdsourcing experiment based on the double stimulus method can be as accurate as a laboratory experiment (based on the same methodology).Another crowdsourcing study evaluated the perception of compression distortions on point clouds [Lazzarotto et al. 2021].\n\nSeveral works presented above have publicly released their datasets.Table 1 lists the publicly available quality assessment datasets for 3D content with color/texture attributes.There is a lack of largescale 3D content datasets, especially those for meshes with color attributes, either in the form of vertex colors or texture maps.Existing datasets are rather small: they contain only few hundreds distorted stimuli, which is not sufficient to drive deep learning metrics that rely on the richness and generality of datasets.In this work, we produce the largest quality assessment dataset of textured meshes to date.In total, more than 343k distorted meshes were generated,  [Javaheri et al. 2019] Point cloud \u2022 2\u00d7Colorless\n\n\u2022 Colored\n\n\u2022 54\n\n\u2022 54 WPC [Su et al. 2019] Point cloud Colored 740 VsenseVVDB [Zerman et al. 2019] Point cloud Colored 32\n\nVsenseVVDB2 [Zerman et al. 2020]\n\u2022 Point cloud \u2022 Mesh \u2022 Colored \u2022 Texture maps\n\u2022 136\n\n\u2022 28 ICIP2020 [Perry et al. 2020] Point cloud Colored 96 PointXR [Alexiou et al. 2020] Point cloud Colored 40 SJTU-PCQA [Yang et al. 2020]  of which 3000 are associated with subjective Mean Opinion Scores (MOS) derived from a large-scale subjective experiment conducted in crowdsourcing.Quality scores of the remaining stimuli were predicted (Pseudo-MOS) using a proposed quality metric based on deep learning, trained and tested on the subset of annotated stimuli.Our large dataset allowed us to analyze the impact of the distortions and model characteristics on the perceived quality of textured meshes.\n\n\nObjective quality assessment\n\nSimple geometric measures, such as Hausdorff distance [Aspert et al. 2002], Root Mean Squared (RMS) error, and Peak Signal-to-Noise Ratio (PSNR), are only weakly correlated with the human vision since they are based on pure geometric distances and ignore perceptual information [Lavou\u00e9 et al. 2016;Zerman et al. 2019].Therefore, many perceptually-driven visual quality metrics have been proposed for meshes and point clouds.\n\nThe most popular of these metrics are based on top-down approaches.They treat the Human Visual System (HVS) as a black box and identify changes in content features induced by distortions to estimate perceived quality.They are mostly full-reference and work in a similar way by first establishing the correspondence between the reference and degraded models, after which a set of local feature errors are computed locally (over a neighborhood around each point/vertex) and then pooled into a global quality score.Early metrics developed evaluate only geometric distortions such as MSDM2 [Lavou\u00e9 2011], DAME [V\u00e1\u0161a and Rus 2012], FMPD [Wang et al. 2012], and TPDM [Torkhani et al. 2014] for meshes, and the pointto-point, point-to-plane and plane-to plane distances [Alexiou and Ebrahimi 2018;Tian et al. 2017] and PC-MSDM [Meynet et al. 2019] for point clouds.The following works pioneered the development of metrics for 3D content with color attributes, some of which were designed for meshes [Guo et al. 2016;Nehm\u00e9 et al. 2021b;Tian and AlRegib 2008] and the majority for point clouds such as PCQM [Meynet et al. 2020], Hist_Y [Viola et al. 2020], GraphSIM [Yang et al. 2020;Zhang et al. 2021], Point-to-distribution [Javaheri et al. 2020].\n\nWith the rise of machine learning, a new category of quality metrics has emerged.These metrics are based on a purely data-driven approach, and do not rely on any explicit model.They learn/optimize the weights of geometric and color descriptors using mainly regression [Chetouani 2018;Nehm\u00e9 et al. 2021b;Yildiz et al. 2020].More recently, deep learning approaches are gaining in popularity.They allow, among other benefits, the emergence of no-reference methods [Nouri et al. 2017].Convolutional Neural Networks (CNNs) were investigated and adjusted to assess the quality of both meshes and point clouds.In [Abouelaziz et al. 2017], the CNN was fed with perceptual hand-crafted geometric features extracted from the 3D mesh and presented as 2D patches.An end-to-end sparse CNN was designed to develop a no-reference quality metric for colored point clouds [Liu et al. 2021a].A recent work computed the perceptual loss for point clouds using an auto-encoder architecture based on convolution layers [Quach et al. 2021].\n\nThe field of quality assessment of 3D content, especially those with color attributes (either in the form of texture maps or vertex/point colors), can still be considered to be in its early stages compared to that of images.Thus, many image-based approaches have been proposed for the quality assessment of 3D data, whether meshes [Caillaud et al. 2016;Zhu et al. 2010] or point clouds [He et al. 2021;Wu et al. 2021;Yang et al. 2020], using existing well-known Image Quality Metrics (IQMs), such as SSIM (and its derivatives) [Wang et al. 2004], iCID [Preiss et al. 2014], HDR-VDP2 [Mantiuk et al. 2011], VIF [Sheikh and Bovik 2006], etc.That is, IQMs are applied to projected views (rendered snapshots) of 3D models allowing a complete capture of geometric and color distortions as reflected in the final rendering as well as environmental and lighting conditions.Lately, several authors have exploited CNNs to assess the quality of 3D content using image-based approaches.For instance, PQA-Net [Liu et al. 2021c] is a deep neural network for no-reference quality evaluation of point clouds that extracts features from multiple views using CNNs.The features are then fused and fed to a distortion identifier and a quality predictor.Another network, proposed in [Tao et al. 2021] for colored point clouds, uses a feature extractor composed of sequential CNNs to extract multiscale features from geometry and color patches separately.The final quality score is then obtained as a weighted average across all patches.For meshes, a recent metric was devised by extracting feature vectors from 3 different CNN models and combining them [Abouelaziz et al. 2020].It uses a patch-selection strategy based on mesh saliency to give more importance to perceptual relevant/attractive regions.The existing works considered geometry-only meshes (without color/texture attributes).In this work, we propose a learning-based quality metric for textured meshes.\n\nOver the last years, convolutional neural networks have successfully rivaled traditional image quality metrics [Bosse et al. 2018;Gao et al. 2017;Talebi and Milanfar 2018;Zhang et al. 2018].Readers can refer to [Tariq et al. 2020] for a comprehensive study determining why deep features are good predictors of image quality.For 3D content, it is still difficult/challenging to develop quality metrics based on deep learning, mainly due to the lack of large and rich datasets of 3D objects, especially those with color attributes, as mentioned previously in subsection 2.1.Given our large-scale dataset of 3000 textured mesh, we propose a learning-based metric, called Graphics-LPIPS, for assessing the quality of rendered 3D graphics.The metric is an extension of the LPIPS metric [Zhang et al. 2018] originally designed for images and perceptual similarity tasks, which we adapted for 3D graphics and quality assessment tasks.Our metric employs pre-trained CNN with learning linear weights on top that we fed with patches of rendered images of 3D models.Our metric provides a good stability and excellent results in terms of correlation and classification ability on two different datasets.\n\n\nDATASET GENERATION\n\nWe produced a large-scale textured meshes quality assessment dataset composed of 343750 distorted meshes derived from 55 source models each associated with 6250 distorted versions.Distortions represent combination of level of details simplification, and texture and geometry compression.The dataset covers a wide range of geometric, color and semantic characteristics.Indeed, each source model has been carefully selected and characterized as will be shown in this section.\n\n\n3D source model selection\n\nWe collected 55 textured 3D models from SketchFab2 .The selection was done manually and carefully to get high quality textured meshes with creative commons licenses.Table 2 lists the models, their number of vertices and semantic category, while Figure 1 illustrates them.\n\nSome models were cleaned up to repair topological and geometrical defects (zero-area triangles, non-manifold geometry, holes, etc.).In addition, all models were converted to a unique format: the meshes are provided as OBJ (+ the material file), and the textures as JPEG images of 2K resolution (normalized texture size: 2048 \u00d7 2048).The textures encode surface colors (i.e.diffuse map); other information such as surface normals, roughness, and ambient occlusion are ignored.For models with multiple texture images, these were baked into one single image.More details about the data preparation can be found in supplemental material.\n\n\nContent characterization\n\nOur goal is to create a high diversity/generality dataset and avoid biases related to the selection of source models.To this end, we proposed an approach to quantitatively characterize the geometric, color and semantic complexity of 3D models.\n\nThe characterization of 3D models is not as straightforward as it seems due to the multimodal nature of these data (geometry, color/texture and material information).In the field of images and videos, the content characterization and classification is usually based on the Spatial perceptual Information (SI) [Engelke et al. 2009;ITU-T P.910 2008;Yu and Winkler 2013].It is an indicator of edge energy, i.e. it emphasizes regions of high spatial frequency that correspond to edges: an image  is converted to gray-scale, then filtered with horizontal and vertical Sobel kernels.The standard deviation () over the pixels of the Sobel-filtered image is finally computed (Eq.1).\n\n\n\ud835\udc46\ud835\udc3c = \ud835\udc60\ud835\udc61\ud835\udc51 \ud835\udc60\ud835\udc5d\ud835\udc4e\ud835\udc50\ud835\udc52 [\ud835\udc46\ud835\udc5c\ud835\udc4f\ud835\udc52\ud835\udc59 (\ud835\udc3c )]\n\n(1)\n\nSince there is no standard on how to characterize 3D content and inspired by the SI of images, we proposed two new measures to characterize the color and the geometry of textured 3D models.We also introduced a third measure, based on the visual attention complexity [Abid et al. 2020] (presented later in this subsection), to characterize the models regarding their semantic aspect.Our measures are based on rendered images, which means they are sensitive to the chosen viewpoint.However, for most objects, our human judgment will be mainly focused on the most informative viewpoint i.e. the one containing the most color, geometry, and semantic information.One way to reflect this tendency would be to compute our measures on a variety of viewpoints and take their maximum value.For the sake of simplicity, and because we will further use this main viewpoint to train our network, we manually chose the main viewpoint for each model (shown in Figure 1) and compute our measures on renderings from this viewpoint.We conducted experiments (detailed in the supplemental material) that show that this manual viewpoint selection leads to very similar results to maxpooling measures taken from 4 viewpoints around the objects.The proposed characterization strategy can thus be applied in both cases (automatic viewpoint sampling + max-pooling or manual viewpoint selection) with similar results.\n\nA. Geometric characterization To enhance the geometry of the model, we compute a rendering that only takes into account the shading of the model without any of its texture information.Then, we calculate the SI of the rendered snapshot.We thus obtain   an objective indicator/measure that assesses the geometric complexity of the 3D model.See Figure 2.a.\n\n\nB. Color characterization\n\nTo assess the textural/color characteristics, we consider this time a rendering without any shading; just the colors from the texture are displayed.We apply the Sobel filter on the rendered snapshot and remove the silhouette detected by the filter because it rather reflects geometric information than color information, and is already taken into account by   .The color characterization is thus defined by the   measure computed on the obtained filtered image.See C. Semantic characterization Semantic information cannot be characterized by the SI.We therefore employed the Visual Attention Complexity (VAC) measure, recently proposed in [Abid et al. 2020], which is perfectly adapted for this task.The VAC measure consists in evaluating the dispersion of salient regions of the rendered 3D model.Rendered images of 3D models associated with low VAC scores indicate that there are highly salient regions that attract human visual attention (focused gaze behavior), while images with high VAC scores indicate that the saliency is diffused and not focused on one region (overall gazing behavior).The VAC is computed as follows and illustrated in Figure 2.c: First, we render the 3D model with shading and texture attributes under its main viewpoint.Once the snapshot of the final rendered object is generated, we compute the saliency map (which represents the probability of gazing at each pixel) using the \"Salicon\" computational model as recommended in [Abid et al. 2020].Finally, we compute a conditional entropy on the saliency map.Thus, we obtain the    which characterizes the visual attention complexity of the 3D model and is closely linked to a semantic value.\n\nWe applied these measures on our 55 selected models.The measures were computed on snapshots of the models having the same resolution.The obtained measures, normalized between 0 and 1, are shown in Figure 3.As it can be seen, the source models of our dataset have various geometric, color and semantic characteristics.Figure 3.a shows a good distribution on the   /  plane: models located at the right-bottom corner of the plane, such as the model #42, present a rich texture with very low geometric complexity.On the contrary, the model #35 in the top-left corner of the plot is monochrome but has sharp edges and many small geometric details to depict its hair and face.Figure 3.b shows that our models cover different semantic aspects.For instance, model #1 and #35 have a low    indicating that the visual attention of the participants will probably be focused on the salient regions of these models that are the epigraph (the writing more generally) and the face, respectively.On the other hand, there are no particularly salient regions on models #14 and #49.These models exhibit low visual attention complexity.\n\nThe set of measures we proposed reveals the characteristics of 3D models in terms of geometry, color and semantics.These measures are extremely fast to compute and work consistently for both coarse and dense 3D data (regardless of the 3D data representation and the number of vertices).\n\n\nDistortions\n\nFrom the 55 source models, we created 343750 distorted versions generated from combinations of 5 real-world distortions.The distortions represent lossy compression and simplification algorithms applied on the geometry, texture mapping and texture image.\u2022 Texture compression: we used the JPEG compression which is the most commonly used algorithm for lossy 2D image compression.We selected 5 texture qualities (  ) obtained by varying the compression level: 90 (the best quality considered but the least effective compression), 75, 50, 25, 10 (the lowest texture quality and the highest compression).We note that for each distortion type, the degradation levels were selected to cover a range of high, medium and low quality distorted meshes.\n\nBy combining/mixing all geometry and texture distortions, we obtained 10   \u00d7 5  \u00d7 5  \u00d7 5   \u00d7 5   = 6250 distortions per model, a.k.a.Hypothetical Reference Circuits (HRCs) according to [VQEG 2010].HRCs denote the processing operations applied to the source models to obtain the distorted versions.Each HRC is associated with a size (in Bytes), resulting from the compression of the source model with the corresponding distortion parameters (using JPEG for the texture and Draco encoder for the connectivity, geometry and UV maps).Thus, the size of a stimulus (in Bytes) is equal to the sum of the size of its compressed texture and its compressed 3D model.\n\nThe distortions were applied systematically with the same levels to all models.Our dataset thus contains 343 750 distorted stimuli (55 source models \u00d7 6250 HRCs) that span a great diversity in terms of visual content and quality.Figure 4 shows some visual examples of distorted stimuli along with their distortion parameters.More examples are provided in the supplementary material.\n\n\nSUBJECTIVE EXPERIMENT\n\nWe conducted a large-scale subjective quality assessment experiment in CrowdSourcing (CS), wherein 4513 participants were involved to rate the perceived quality of a generalized and challenging subset of 3000 stimuli carefully selected from the dataset presented above.Over 148k quality scores were collected.This section describes our extensive online subjective study.\n\n\nTest stimuli selection\n\nOur dataset contains more than 343k stimuli.Participants cannot be asked to rate the quality of such a large amount of data.We therefore had to select a subset of stimuli to rate in the subjective experiment.The selection of this subset is a crucial step since we aim to use it later to train a learning-based metric.Thus, we seek to obtain an unbiased, generalized and challenging subset, which leads to several selection criteria: the selected subset had to contain all the source models, as well as an equal distribution of HRCs (combinations of distortions created).In addition, the subset of stimuli must equitably cover the entire range of quality (from imperceptible to very annoying distortions) to have a balanced representation of the visual quality.Last but not least, we want this subset to be challenging for objective quality metrics.\n\nWe selected 3000 stimuli from 343750 (which represents about 0.9% of the total dataset).To do so, we developed the following approach.\n\n\u2022 First, we predicted the Mean Opinion Score (MOS) of all the 343750 stimuli, using existing objective quality metrics that we calibrated on an existing subjectively-rated quality assessment dataset: we used the dataset of meshes with vertex colors [Nehm\u00e9 et al. 2021b].We fitted two logistic regression models (mapping functions) between the MOSs of this dataset and the following two objective quality metrics: (1) HDR-VDP2 [Mantiuk et al. 2011] since it provided the best performance among the Image Quality Metrics (IQMs) tested on this dataset [Nehm\u00e9 et al. 2021b] and\n\n(2) LPIPS (Learned Perceptual Image Patch Similarity) [Zhang et al. 2018] since it is a commonly used IQM, based on pre-trained CNNs, with many successful applications [Huang et al. 2018;Yang et al. 2018].\n\nWe computed HDR-VDP2 and LPIPS on snapshots of the stimuli in our dataset rendered from their main viewpoints (defined in  subsection 3.2), and then predicted their MOS using the obtained regression models.As in [Liu et al. 2021a;Wu et al. 2020], we refer to the predicted MOSs as Pseudo-MOSs.Thus, we obtained 2 pseudo-MOS values per stimulus (pseudo-MOS    and pseudo-MOS   ).We used two metrics (instead of one) to be able to sample stimuli for which the metrics do not agree on their quality (as shown later in this subsection and in Figure 5), resulting in a more challenging subset of stimuli.\u2022 Second, to ensure a good and equitable coverage of the whole visual quality range and to get a subset of challenging stimuli, we regularly sampled the plane formed by the 2 pseudo-MOSs, shown in Figure 5, while respecting 2 constraints: considering HDR-VDP2 as the pivot metric, we uniformly sampled the area of each quality range.For each sampled point, we selected the \"closest\" set of stimuli (in terms of distance), and then chose the one that ensures an equal/equitable distribution of (1) all source models (e.g., as many degraded stimuli for model    as for model    ) and (2) all levels of each distortion(e.g,almost as many stimuli are selected with a  = 7 as those with a  = 10).\n\n\nRendering\n\nTo adequately assess the visual quality of 3D content, it is important that the object moves so that the observer can see the whole object and not focus on one single viewpoint [Rogowitz and Rushmeier 2001].Thus to avoid manual selection of multiple relevant viewpoints for each model, we animated all models in our dataset with a full rotation (360 \u00b0) around their vertical axis.We rendered the dynamic test stimuli in a neutral room (light gray walls), without shadows and under a directional light coming from the top right of the room.All models were approximately the same size and rendered with a lambertian material; mipmapping was activated.\n\nWe designed the experiment based on the Double Stimulus Impairment Scale (DSIS) method, in which observers see the source model and the same model impaired side by side and rate the impairment of the second stimulus in relation to the source model using a five-level impairment scale, displayed after the presentation of each pair of stimuli.This method has proven to be the most accurate and stable for evaluating the quality of 3D graphics [Nehm\u00e9 et al. 2020].Since the experiment is conducted in crowdsourcing, we generated videos of the rendered stimuli in order to limit the participant's interactions with the 3D objects, since we have no full control over the participant's test environment.The only interaction required by the participant is the selection of the score when rating.The videos were all in 650 \u00d7 550 resolution (so that the videos of the source and degraded models fit simultaneously on a screen with a minimum resolution of 1920 \u00d7 1080) with a frame rate of 30 fps and encoded using H.264 encoder (mp4 container) at a bitrate of 5 Mbps to ensure imperceptibility of compression artifacts.Videos are 8 seconds long, which is the time it takes for models to complete the full rotation.\n\n\nExperimental environment\n\nTo obtain reliable and controlled results, we used our own web platform to present the subjective experiment to participants.This platform has proven its effectiveness in achieving the accuracy of a laboratory test and producing reliable results [Nehm\u00e9 et al. 2021a].The crowdsourcing service was used only to recruit participants.Our platform, illustrated in Figure 6, is suitable for presenting videos according to the DSIS method.Only a web browser with an MPEG-4 decoder is required to run the experiment; no other software needs to be installed.The platform first checks the compatibility of the participant's device: minimum screen resolution of 1920\u00d71080, page zoom level, maintain full screen mode throughout the experiment.The test instructions are then displayed to the participant with a progress bar, at the bottom of this page, showing the status of the loading process of all the video pairs that will be used in the test.This way, the videos of the source and distorted models are played simultaneously during the test, without any latency or unintended interruptions.When the loading is completed a start button appears leading to the test.These steps/windows are illustrated in the supplementary material.The pairs of videos are displayed in a random order to each participant.Participants cannot replay the videos or provide their score until the videos have been played completely.There is no time limit for voting and videos of the stimuli are not shown during that time.At the end of the experiment, participants will receive unique codes allowing them to get their remuneration.\n\n\nTest sessions & participants\n\nCreation of test sessions.The number of stimuli and the duration of the experiment should be limited as much as possible in crowdsourcing to keep participants motivated and to avoid unreliable results [Jim\u00e9nez et al. 2018;Redi et al. 2015;Reimann et al. 2021].Therefore, we divided our subset of 3000 test stimuli into 100 playlists.Each participant rates one playlist, i.e., only 30 stimuli.This way, we stay within the duration of the experiment in [Nehm\u00e9 et al. 2021a] and within that of a pilot experiment we conducted to evaluate the number of scores needed per stimulus to have the same confidence intervals as for a lab study (see supplementary material).The test stimuli were fairly distributed among the playlists so that each playlist contains a maximum diversity of source models (a source model is repeated a maximum of 2 times in a playlist).Additionally, each playlist spans the full range of distortions and all playlists have nearly the same pseudo-MOS distribution.\n\nAs in [Ghadiyaram and Bovik 2016], we injected 3 Golden Units (GU, a.k.a trapping stimuli) into each playlist to facilitate the detection of unreliable participants later.The golden units (GUs) included (1) a very poor quality stimulus (  ) , (2) a very high quality stimulus ( \u210e\u210e ) and (3) a stimulus displayed twice ( 1 and ( 2 ) ) to assess the participant's consistency (coherence of his/her scores).Participants who fail to answer correctly the golden units are considered outliers and their scores are discarded.\n\nTraining.The experiment started with training.In order to familiarize participants with the task and the rating scale, we selected 5 stimuli not included in the 3000 stimuli test and all referring to the same source model.Each stimulus represented one level of the five-level scale of the DSIS method.After displaying each pair of training videos for 8 sec, the rating interface is displayed for 5 sec and the proposed score assigned to this distortion is highlighted.Once the training is completed the actual test began.\n\nDuration.The test session of our experiment consisted of 33 pairs of videos to rate (1 playlist of 30 stimuli and 3 golden units) and lasted about 10-12 minutes: informed consent + loading videos + instructions + 5 training stimuli \u00d7 (8s video length + 5s Rating) + 33 test stimuli \u00d7 (8s video length + \u223c 4s Rating).\n\nParticipants.We ran our experiment until each playlist was fully rated by 45 participants.To set the number of participants per playlist, we referred to the CS experiment in [Nehm\u00e9 et al. 2021a], but we also conducted another pilot experiment with 30 stimuli (selected from this dataset) that were rated by 60 participants.We assessed the evolution of the confidence intervals according to the number of participants (see supplementary material).\n\nIt took us about 5 days to collect all the data: 148929 quality judgments were collected.A total of 4513 participants completed the experiment: 2501 males and 2012 females.They were from 67 different countries and aged between 18 and 80.All participants were naive about the purpose of the experiment.The recruiting process of the participants was conducted using Prolific4 , as the results of the CS experiment in [Nehm\u00e9 et al. 2021a] highlight the reliability and seriousness of the Prolific participants.\n\nIn the remainder of the paper, subjective scores are mapped on a discrete numerical scale from 1 to 5, following the ITU recommendation [ITU-R BT.500-13 2012] as follows: Imperceptible: 5, Perceptible but not annoying: 4, Slightly annoying: 3, Annoying: 2, Very annoying: 1.\n\nParticipants screening.As in [Ho\u00dffeld et al. 2014], participants were filtered by combining the following two screening strategies:\n\n(1) the ITU-R BT.500-13 screening procedure [ITU-R BT.500-13 2012], which detected 159 outliers.This procedure is summarized in [Mantiuk et al. 2012] as follows: \"The procedure involves counting the number of trials in which the result of the observer lies outside \u00b12 \u00d7 standard deviation range and rejecting those observers for which (i) more than 5% of the trials are outside that range; and (ii) the trials outside that range are evenly distributed so that the absolute difference between the counts of trials exceeding the lower and the upper bound of that range is not more than 30%.\"\n\n(2) the golden units (GU) analysis, which revealed 110 outliers distributed as follows:\n\n\u2022 24 participants rated the distortion of the very poor quality stimulus as imperceptible or perceptible but not annoying (\n\ud835\udc3a\ud835\udc48 \ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc5f \ud835\udc56 \u2265 4, where \ud835\udc60 \ud835\udc3a\ud835\udc48 \ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc5f \ud835\udc56\ndenotes the score assigned by participant  to   ).\n\n\u2022 39 participants rated the very good quality GU as annoying or very annoying (\n\ud835\udc3a\ud835\udc48 \u210e\ud835\udc56\ud835\udc54\u210e \ud835\udc56 \u2264 2).\n\u2022 32 participant gave inconsistent scores to the third GU showed twice ( \n\ud835\udc3a\ud835\udc48 \ud835\udc5f\ud835\udc52\ud835\udc5d1 \ud835\udc56 \u2212 \ud835\udc60 \ud835\udc3a\ud835\udc48 \ud835\udc5f\ud835\udc52\ud835\udc5d2 \ud835\udc56\n\u2265 3).\n\n\u2022 7 participants rated \n\ud835\udc3a\ud835\udc48 \ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc5f \ud835\udc56 = 3 & \ud835\udc60 \ud835\udc3a\ud835\udc48 \u210e\ud835\udc56\ud835\udc54\u210e \ud835\udc56 = 3. \u2022 8 participants scored (\ud835\udc60 \ud835\udc3a\ud835\udc48 \u210e\ud835\udc56\ud835\udc54\u210e \ud835\udc56 = 3 | \ud835\udc60 \ud835\udc3a\ud835\udc48 \ud835\udc5d\ud835\udc5c\ud835\udc5c\ud835\udc5f \ud835\udc56 = 3) & \ud835\udc60 \ud835\udc3a\ud835\udc48 \ud835\udc5f\ud835\udc52\ud835\udc5d1 \ud835\udc56 \u2212 \ud835\udc60 \ud835\udc3a\ud835\udc48 \ud835\udc5f\ud835\udc52\ud835\udc5d2 \ud835\udc56 = 2.\nOf the participants who failed to evaluate the golden units, 14 were also detected by the ITU-R BT.500-13 screening procedure.As a result, 255 out of 4513 participants were rejected (5.6%).Only the scores of the remaining participants will be used in the following sections.We compute the Mean Opinion Score (MOS) from these scores by averaging the scores given by different participants on each stimuli.\n\nWe present in Figure 7 the distribution of raw scores and MOSs obtained for the subset of 3000 stimuli evaluated by the participants.It can be seen that the subjective scores distributed across the whole quality range.Figure 8 shows the distribution of Confidence Intervals (CIs) of the MOSs for each source model.No loose confidence intervals were found (most confidence intervals are below 0.3), demonstrating good agreement between participants' ratings across the stimuli of the different models.\n\n\nGRAPHICS-LPIPS: A PERCEPTUAL QUALITY METRIC FOR 3D GRAPHICS BASED ON CNN\n\nAs discussed in the related work section, there is a lack of quality metrics for 3D graphics with color attributes, especially those based on deep learning approaches.Given our large-scale dataset of textured mesh of which 3000 stimuli are associated with subjective\n.4 #1 #2 #3 #4 #5 #6 #7 #8 #9 #1 0 #1 1 #1 2 #1 3 #1 4 #1 5 #1 6 #1 7 #1 8 #1 9 #2 0 #2 1 #2 2 #2 3 #2 4 #2 5 #2 6 #2 7 #2 8 #2 9 #3 0 #3 1 #3 2 #3 3 #3 4 #3 5 #3 6 #3 7 #3 8 #3 9 #4 0 #4 1 #4 2 #4 3 #4 4 #4 5 #4 6 #4 7 #4 8 #4 9 #5 0 #5 1 #5 2 #5 3 #5 4 #5 5\nModel ID CI Fig. 8. Boxplots of CIs obtained for the source models.Mean values are displayed as circles.\n\nscores, we were able to create a deep-learning quality metric for such data.We considered the Learned Perceptual Image Patch Similarity (LPIPS) metric [Zhang et al. 2018] which employs a deep neural network to evaluate the perceptual similarity between 2 image patches, and adapted it to 3D data, then trained it using our dataset.The choice of LPIPS was motivated by its many successful applications [Huang et al. 2018;Yang et al. 2018], the simplicity of the approach and the fact that it is based on an in-depth study across different architectures.\n\n\nDescription of Graphis-LPIPS\n\nThe base principle of LPIPS is to use pre-trained neural networks to extract deep features from two image patches,  (the reference) and  0 (the distorted patch).The two inputs are treated in parallel by two siamese CNNs that share weights and that we denote by  .The feature difference between the two patches ( () \u2212  ( 0 ) 2 ) can then be mapped to a perceptual difference by going through a 1 \u00d7 1 convolution layer that learns the appropriate weights  for each channel.Compared to the original LPIPS, we added a 1 \u00d7 1 convolution layer with weights  0 which allows to further calibrate the model to our perceptual data.Finally, the result is spatially averaged over all pixels from the patch.The obtained score  (,  0 ) represents a perceptual difference between the original patch  and the distorted one.The architecture of our network is shown in Figure 9.In accordance to the recommendation of the authors, we use the pre-trained AlexNet with fixed weights as feature extractor and only learn the weights  and  0 of the convolution layers on top.\n\n-\n\n\nF F w w\u2080\n\n\nSpatial average\n\nFig. 9. Graphics-LPIPS architecture: to compute a distance  (,  0 ) between two patches  and  0 , features are first extracted from the layers of the network  and normalized in the channel dimension.The feature difference then goes through two layers of 1 \u00d7 1 convolution in order to reach a 1channel measure.Finally, we average across spatial dimension and sum across all layers.\n\nThe LPIPS metric was originally trained and tested for perceptual similarity tasks (a.k.aTwo Alternative Force Choice 2AFC), in which participants were asked to choose which of two distorted patches ( 0 ,  1 ) is more similar to the reference .A small network was thus added by the authors at training time to map the obtained perceptual distances to the collected preference score.Differently from the original LPIPS, our dataset is composed of a MOS scores per image and we want our metric to predict this overall quality score.We thus modify the remaining of LPIPS pipeline in the following ways: (1) we divide our stimuli into patches that are suitable for distance computation, (2) we pool the perceptual distances obtained on all these patches to get an image score that can be compared to the MOS, (3) we devise a training strategy to ensure that the loss can be computed per image (and not only per-patch).\n\nGiven a distorted image  (along with its collected quality score   ) and a reference image   , we sample corresponding patches   and    .We use the previously described pipeline to obtain the perceptual score  (   ,   ) of each pair of patches.Similarly to [Bosse et al. 2018;Kang et al. 2014], we opted for the \"pooling by simple averaging\" to get the score of each image, the estimated overall image quality Q of  is thus computed as:\nQ\ud835\udc3c = 1 \ud835\udc41 \ud835\udc5d \ud835\udc41 \ud835\udc5d \u2211\ufe01 \ud835\udc56=1 \ud835\udc51 (\ud835\udc65 \ud835\udc5f \ud835\udc56 , \ud835\udc65 \ud835\udc56 ) (2)\nwhere   denotes the number of patches sampled from  ,   refers to a patch from the distorted image  , and    is its corresponding patch on the reference image   .Note that since our stimuli are renderings of distorted meshes, the perceived visual alterations are expected to be relatively well distributed on the image, without strong local distortions.For this reason, averaging the local scores over the image is a reasonable strategy and gives the best result compared to other pooling functions.Results with other pooling strategies (L2, L3 or max pooling) are reported in the supplementary material.\n\nThe Mean Square Error (MSE) is used as the minimization criterion.The loss function to train our network is then:\n\ud835\udc38 \ud835\udc3c = ( Q\ud835\udc3c \u2212 \ud835\udc40\ud835\udc42\ud835\udc46 \ud835\udc3c ) 2 (3)\nwhere   is the loss over an image.\n\nTraining data.As training data, we use for each stimulus of our dataset a snapshot taken from its main viewpoint to which we assigned the obtained MOS.Thus, we have 3000 annotated images representing our 3000 degraded stimuli.The image size, 650 \u00d7 550, is the video resolution of the stimuli seen by the participants in the subjective experiment.We divided (patchified) the images into small overlapping patches of size 64 \u00d7 64 sampled every 32 pixels.We removed patches containing less than 65% stimulus information (i.e., the percentage of background pixels in the patch is greater than 35%).We got an average of 60 patches per stimulus.\n\n80 % of the stimuli in the dataset (about 2400) are used for training and 20 % for testing.The dataset is randomly split by source model ensuring that no 3D model is used for both training and testing.As a result 44 source models out of 55 were included in the training while the rest were used for testing.We used k-fold cross-validation and generated 5 different splits.We report the performances over the 5 folds in the next subsection and chose a representative fold with average performances for all the subsequent evaluations.\n\nTraining.As the distances computed for patches of the same image are combined for the optimization of the network (Eq. 2 and Eq.3), we cannot treat each patch as a separate sample (in other words, the patches of the same image can not be distributed over different batches).Thus, each batch was made to contain   images, each represented by   patches, resulting in a batch size of   \u00d7   patches.The backpropagated error is the average loss over the images in a batch (mean(  ) computed in Eq. 3).During training, patches are randomly sampled every epoch to ensure that as many different image patches as possible are used in training.At inference time, we use all the patches from the image to make the prediction.\n\nOur final model was trained for 10 epochs (5 epochs at initial learning rate 10 \u22124 and 5 epochs with linear decay).Each batch contained   = 4 images (stimuli), each represented by   = 150 patches.The maximum number of patches for one image in our dataset being 149, all the patches from each image are thus seen at each iteration.For images that are represented by less than 150 patches, we repeat the patches until reaching this number.\n\nWe refer to the version of LPIPS adapted for 3D Graphics as Graphics-LPIPS and compare its performances to those of the original LPIPS in the following subsection.\n\n\nResults and Evaluation\n\nFigure 10 summarizes the performance of Graphics-LPIPS and compares it to state-of-the-art Image Quality Metrics (IQMs), including the original LPIPS, on the test set of each of our five folds (each fold containing around 600 stimuli obtained from 11 source models).We report the mean and standard deviation over the five folds, the results on each individual fold can be found in supplemental material.Plots illustrating the subjective scores versus the metric values on a representative fold are shown in Figure 11.As for our metric, the IQMs were computed on the snapshots taken from the main viewpoint of the stimuli.The parameters of the IQMs are provided in the supplementary material.\n\nWe evaluated the performance of the metrics in terms of correlations and classification abilities.For correlation measures, the Pearson Linear Correlation () and the Spearman Rank Order Correlation () were chosen.To account for saturation effects associated with human senses, we computed PLCC after a logistic regression that establishes a nonlinear mapping between subjective and metric scores.For the classification ability analysis, we considered that proposed in [Krasula et al. 2016] which consists of assessing (1) the ability of the metric to distinguish between significantly different and similar pairs of stimuli and (2) the ability of the metric to predict which stimulus is of better quality in a pair of stimuli.This analysis takes into account the uncertainty of the subjective scores and considers the Area Under the Curve (AUC) as the performance measure.We denote   and   respectively for the 2 analysis scenarios.The proposed metric shows a much better classification ability and correlation with MOSs than IQMs.Statistical tests on the logistic regression residuals yield p-values << 0.0001.Additionally, our metric exhibits more consistent results over the different folds (smaller standard deviation), showing that it generalizes better to a wide variety of images.The poor performance of the IQMs reflects the challenging aspect of our dataset.We believe that this is related to (1) the process of selecting the 3000 stimuli, which samples a lot of stimuli for which two quality metrics did not agree (see section 4.1) and to (2) the large variability of source models and distortion combinations (mixed distortions) present in this dataset.\n\n\nValidation on a dataset of 3D meshes with vertex colors\n\nWe evaluated the performance of our metric on the dataset of meshes with vertex colors reported in [Nehm\u00e9 et al. 2021b], to assess its robustness.This dataset is composed of 480 animated stimuli, generated from 5 source models corrupted by simplifications and compressions applied on geometry and color: uniform quantizations applied on either (1) geometry or (2) color, simplifications that take into account either the (3) geometry only or (4) both geometry and color.Each distortion was applied with 4 different strengths adjusted manually in order to cover the whole range of visual quality from imperceptible to high levels of impairment.Each stimulus was displayed in 3 viewpoints and animated with 2 short rigid motions (rotations and zooms).The dataset was obtained through a subjective study in VR based on the DSIS method.\n\nAs each stimulus in this dataset was rated in 3 different viewpoints, we computed Graphics-LPIPS on snapshots taken from each viewpoint.We did not consider the influence of animations.Thus, for a given stimulus, we averaged its mean opinion scores (MOSs) over the two animations.The database used is therefore composed of 240 stimuli.We included the results of IQMs computed on these snapshots as well as the results of the Color Mesh Distortion Measure (CMDM), which is a model-based quality metric for 3D meshes with colors attributes (i.e.works entirely on the mesh domain).It incorporates perceptually-relevant geometry and color features and is based on a data-driven approach [Nehm\u00e9 et al. 2021b].CMDM was computed only over the visible parts of the 3D model (visible vertices) in each viewpoint.\n\nTable 3. Performance comparison of different metrics on a dataset of meshes with vertex colors.For metrics marked with a *, the values are reprinted from the supplementary material of [Nehm\u00e9 et al. 2021b].Although our metric was trained on a different dataset with different models and different distortions and even different color representation (textures and not vertex colors), its performance is comparable4to that of CMDM which was learned on this dataset.This shows the good robustness of our metric and validates (1) that it did not just learn the distortions that are specific to our dataset and (2) its ability to differentiate and rank stimuli from different source models and different distortions.Moreover, Table 3 also shows that our metric can be computed on different viewpoints of the 3D object (even if it is not necessarily the main viewpoint) and still provide good results in correlation with MOSs.\n\n\nGraphics\n\nFurthermore, we noticed that IQMs exhibit poorer performance on our dataset of textured meshes than on the dataset of meshes with vertex colors (see Figure 10 and Table 3).This difference in IQMs performance between the two datasets illustrates once again the challenging aspect of our dataset.\n\n\nRobustness to changes in lighting and material\n\nLighting.One of the potential drawbacks of an image-based metric like ours is its potential dependency on the rendering conditions.Our metric is supposed to be able to evaluate the quality of a 3D model, regardless of how it is illuminated.To test whether our metric is robust to changes in the lighting conditions, we conducted the two following experiments:\n\n(1) We moved the directional light toward the left side of the object (Figure 12.left) and toward the bottom (Figure 12.right).We then rendered the images with these new conditions and used the same network as before (without any retraining) to compute results on the test set of our representative fold.Figure 12 provides the correlation results according to the angle from the canonical axis.For horizontal variations, the performance remains almost identical and even increases slightly for grazing angles.For vertical variations, the performance decreases slightly when the illumination becomes close to front lighting (\u2248 0.01 decrease in Pearson correlation).Given the fact that front lighting tends to completely mask geometric details, the observed robustness of our metric remains excellent, although the network never saw those lighting conditions before.\n\n(2) We also tested two completely different lightings: one dimmed light coming from the left (Figure 13.a) and a spot light coming from the front of the object (Figure 13.b).In line with previous results, the first configuration leads to very good scores ( = 0.87), showing even better performances than with the original illumination.The second condition leads to more degraded results ( = 0.81).Still, although this configuration is completely different from the original one, its performances are still outperforming other IQMs.Material.Additionally, we evaluated how our metric behaves when changing the material property of an object.Note that, contrary to lighting conditions, our metric is not supposed to be robust against material change since the material intrinsically defines the appearance of the object itself.We rendered the objects with a glossy material (glossiness = 0.8, metallic = 0 in Unity PBR model, see Figure 13.c) as well as with a metallic one (glossiness = 0.6, metallic = 0.8, see Figure 13.d).In that case, the performances of our metric decrease significantly with Pearson correlations of respectively 0.72 and 0.74.As mentioned above, this is expected since materials play a key role in the perception of an object, more specular material can lead to amplifying the visual effects of geometric deformation while minoring the texture ones.These results open interesting perspectives on the influence of the material in the perception of compression artifacts, that would require new subjective studies.\n\n\nView-independent approach\n\nIn order to have an automatic version of our metric that does not require manual selection of a main viewpoint for each 3D model, we considered another training scenario, using a set of snapshots of the model taken from different viewpoints.It may be especially relevant in our case since all the stimuli in our dataset were animated with a full rotation (360 \u00b0) during the subjective test (see subsection 4.2).For each stimuli, we thus generated 4 snapshots taken from 4 camera positions regularly sampled on its bounding box and prepared the data as for the above network.We used the same training and testing set as our representative fold and used the same training parameters, randomly sampling   = 300 patches from all possible viewpoints.The results on the test set are reported in Table 4.For the IQMs, the global quality score of a stimulus is the average of the IQM values computed on its 4 snapshots.Comparing the results of Figure 10 and Table 4, we observe that the performance of our metric decreases slightly when considering a view-independent approach.This indicates that our manual selection of the main viewpoint for the source models is indeed relevant and helps the network.It also indicates that the perceptual pooling is not uniform: some parts or viewpoints of the objects have a stronger influence on the overall quality perceived by the observer.This effect depends on the metrics, as the performance of SSIM and iCID improves when considering multiple viewpoints per stimulus, while that of HDR-VDP remains stable.\n\n\nAPPLICATION\n\nThis section presents an application of the proposed metric and dataset.We used Graphics-LPIPS to annotate our whole dataset of textured meshes and study the influence of factors on the visual quality.\n\nIndeed, our subjective experiment involved only 3000 stimuli out of 343750 (i.e.only 3000 stimuli have a MOS value).To annotate the remaining stimuli of the dataset, we applied Graphics-LPIPS to predict their MOS, referred to as pseudo-MOS.The pseudo-MOSs distribution of all stimuli of the dataset is provided in the supplementary material.\n\nAnnotating the entire dataset allowed us to explore the influence of the different compression parameters and their interactions on the subjective scores and thus on the perceived quality.We were also able to evaluate the impact of model characteristics on the perception of distortions.The subsequent subsections detail these analyzes.\n\n\nInfluence of each distortion on perceived quality\n\nThe perceptual quality of textured 3D content depends on both the geometry and color distortions.Since the distortions in our dataset are of different natures (quantization, sub-sampling, simplification) and affect different aspects of the 3D model (geometry and texture), we believe that their impact on the perceived quality is therefore very different.In this subsection, we provide an in-depth analysis of the effect of each distortion on the perceived quality.We also determine which distortions affect the quality scores the most.To do so, we ran a Multivariate Analysis of Variance (ANOVA:   \u00d7  \u00d7 \u00d7  \u00d7  ) on the quality score of the entire dataset.All of the five distortions affect significantly the perceived quality (p-value << 0.0001), the full ANOVA table can be found in supplemental. .6.1.1Influence of the geometry and texture coordinate quantization.Figures 14a and 14b show the impact of the quantization parameters on the visual quality of 3D models.As expected, quantizing the vertex position or texture coordinates with too few bits can seriously degrade model quality.The advantage of using fewer quantization bits is the size reduction of the compressed files, however the resulting visual quality is vastly different from that of the original source model.Therefore, choosing the optimal/correct quantization parameters for an application depends on the intended quality as well as the size constraint.This is known as Rate-Distortion (RD) optimization.6.1.2Influence of the LoD simplification.When looking at Figure 15a, it appears that the most simplified stimuli (7, 8, 9) rated slightly better than the less simplified stimuli (1, 2, 3).This is counter-intuitive and could led us to think that simplifying the models with high strength did not introduce markedly visible impairments.This is not strictly true: it is actually highly dependent on the geometry quantization level.In fact, if we consider only the subset of the least geometry quantized stimuli ( = 11 &  = 10), we see that the MOS logically decreases as the simplification level increases (see Figure 15b).There is thus a significant interaction between the geometry quantization of the model and its levels of detail (p-value << 0.0001).Subsection 6.2.1 details this point.Note that for the most simplified level 10, it is a bit peculiar: for 10, the models are brutally/roughly simplified to have about 2000 faces.This is very degrading (regardless of the  and  values), especially for dense models with the highest number of vertices.6.1.3Influence of the texture compression and sub-sampling.Figures 16a and 16b show the impact of the two distortions applied to the texture map (JPEG compression   and sub-sampling   ) on the perceived quality.While their effect is statistically significant, the impact of these distortions on the MOS is not as obvious as that of texture coordinates quantization (shown in Figure 14b).Figure 16a shows that for   \u2265 50, the increase of the texture quality does not seem to affect the overall perceived quality.For texture sub-sampling, Figure 16b shows that increasing the texture resolution   more than 712 \u00d7 712 did not overall influence the perceived quality.This may seem logical since the resolution of the stimulus videos shown in the experiment was 650 \u00d7 650.However, we recall that texel density depends on the surface area and even though render resolution is smaller than the texture resolution it does not mean that distortions will always be imperceptible (particularly when the UV mapping is non-uniform).The impact of texture sub-sampling   is emphasized when considering its interaction with texture compression   (see subsection 6.2.2).Thus for our visualization conditions, it seems that we can push the JPEG compression level and sub-sample the texture heavily without impacting the overall quality of the stimulus.This allows to drastically reduce the size of the compressed data.\n\n\nInfluence of distortion interactions on perceived quality\n\nBased on the results of the previous subsection, we believe that the impact of the combinations of the different types of distortions differ from the cumulative impact of each distortion applied alone.This subsection presents the distortion interactions that have the most impact on the perceived quality of textured meshes.Other interesting interactions are provided in the supplementary material.\n\n6.2.1 Interaction of LoD simplification and position quantization.\n\nThe perception of geometry quantization artifacts depends strongly on the level of details of the stimulus (significant interaction with a p-value << 0.0001).Figure 17 illustrates this interaction.When quantizing stimulus's positions with too few bits ( \u2208 {7, 8, 9}), the MOS increases as the simplification level   increases (i.e., the number of vertices decreases).The reason is that, the local geometry alteration (local contrast alteration) caused by a strong quantization is more visible on dense meshes (less simplified,   = 1) than on coarse meshes (  = 9).Figure 18 illustrates a visual example in which we can see how the effect of the quantization of the vertex positions is much more visible on the dense model (  = 1).This is due to the fact that the frequency of artifacts created by geometry quantization is higher on a dense mesh than on a simplified mesh; this is what makes the artifacts more visible.\n\n\nInteraction of the texture compression and sub-sampling.\n\nThere is a significant interaction (p-value << 0.0001) between the compression and sub-sampling applied on texture images.Figure 19 shows its impact on the perceived quality.For the lowest texture quality (  = 10), the MOS increases as the texture size   increases.Overall, compression artifacts are less visible on larger textures.The reason is that the blocking artifacts caused by the JPEG compression are bigger/larger on screen for smaller textures.We can also notice that stimuli with medium or low compressed textures (  \u2265 50) obtained almost the same MOSs regardless the texture size.This is coherent with what we observed in Figure 16a.\n\nThose results show that, for the viewing conditions of our experiment, the perception of texture compression artifacts is subject to significant masking effects, probably due to the texture mapping, shading, and rasterization processes.Those masking effects makes the JPEG artifacts significantly less visible than for a natural 2D image directly displayed on the screen.\n\n\nInfluence of content characteristics on perceived quality\n\nThe content has a concealing effect on the perception of the distortions, which is consistent with the characteristics of the human visual system [Karunasekera and Kingsbury 1995].Indeed, for the same distortion parameters, the perceived quality may not be the same depending on the models and their characteristics.\n\nIn this section, we evaluate the influence of content characteristics on the perception of distortions and thus on quality.To do so, we use the content characterization measures we developed in Section 3.2 (  and   ).We group our 55 models into 5 clusters based on their geometric and color complexity.Thus, the first cluster \"  1\" contains the first 11 models with the least complex geometry (lowest   values), while \"  5\" designates the 11 models with the most geometric details (highest   values).Similarly, \"  1\" denotes the first 11 source models with the least color details while \"  5\" refers to the models with the richest texture.To assess the impact of geometry complexity, we eliminated the stimuli with rich textures (  4 and   5) in order to dissociate the influence of geometry and color and to avoid a possible masking effect of one on the other.According to ANOVA, a significant interaction exists between the geometric complexity of the model and the visual impact of the position quantization (p-value << 0.0001).Figure 20a shows that the geometric information can masks the geometry alteration caused by the quantization of the vertices position: For the same quantization level , meshes with complex geometry (\u2208 {  4,   5}) obtained higher MOSs than those with less complex geometry (\u2208 {  1,   2}).\n\nRegarding the impact of color complexity, the results presented in Figure 20b show that for the same level of quantization, models with rich texture (\u2208 {  4,   5}) were judged to be of higher quality than those having simpler texture (\u2208 {  1,   2}) .These results corroborate those observed for point clouds, reported in [Liu et al. 2021b].\n\nThus, we can say that both geometry and color mask the geometric degradations of a quantized 3D model.6.3.2Influence of geometric and color complexity on the perception of texture coordinates quantization.Following the same approach described in 6.3.1, we varied  \u2208 {6, 7, 8} and set the levels of all\n\n\nother distortions at their best levels (\ud835\udc3f\ud835\udc5c\ud835\udc37\n\ud835\udc60\ud835\udc56\ud835\udc5a\ud835\udc5d\ud835\udc3f \u2208 {\ud835\udc3f1, \ud835\udc3f2, \ud835\udc3f3} & \ud835\udc5e\ud835\udc5d \u2208 {10, 11} & \ud835\udc47 \ud835\udc44 \u2208 {75, 90} & \ud835\udc47 \ud835\udc46 \u2208 {1440 \u00d7 1440, 2048 \u00d7 2048})\nin order to evaluate whether the model's geometric and color characteristics can mask the impairments caused by quantizing its UV map.\n\nFigure 21 clearly shows that models with close-to-uniform textures (i.e., low contrast, with no high frequencies or noticeable structure, \u2208 {  1,   2}) are less sensitive to the UV map quantization than those with colorful and detail-rich textures (\u2208 {  4,   5}).\n\nWhen analyzing the interaction between the geometric complexity   of the models and the quantization of their UV map , we realized that the influence of this interaction is more complex to evaluate, yet it is significant (p-value << 0.0001).The boxplots of the MOSs illustrating this interaction, as well as some visual examples are provided in the supplementary material.We noticed that the impact of the UV map quantization on the visual quality depends not only on the geometric and color complexity of the model but also on the amount of texture seams (the level of fragmentation of its texture atlas): quantization artifacts are more visible on models exhibiting a large number of texture seams (texture atlas highly fragmented and/or not efficiently packed).Further work is still needed to study the effect of texture seams.We speculate that the set of quality measures, reported in [Maggiordomo et al. 2020], to characterize the quality of the surface parametrization, notably the \"UV Occupancy\u017e measure which assesses the quality of the atlas packing and the \"Atlas Crumbliness and Solidity\u017e measure which captures the severity of texture seams in a given UV map, could be a good starting point.\n\n\nLIMITATIONS\n\nIn the present work, the material information of each object is limited to one single diffuse texture, which is then mapped onto a Lambertian material for rendering.This pipeline thus does not integrate other texture maps representing physically-based material information like metalness and roughness, nor microgeometry information like normals or bumps.We made this technical choice for the sake of simplicity, given that this simple material representation remains reasonably realistic for many use cases and spans an already huge space of distortion parameters.As illustrated in Section 5.4, the nature of the material (e.g., its specularity) obviously influences the visual impact of compression artifacts.In that respect, new user studies are needed to understand and model this influence.\n\nOur metric implementation makes the hypothesis that the perceived distortion of a global model can be modeled as an average of the local distortions from patches sampled either on a principal view or on multiple views.Even if we showed that this pooling provided better results than some other choices (L2, L3, or max pooling), it is highly probable that the reality of our visual system is quite different.Complex attention mechanisms must be at work, as raised by the results of our view-independent approach.We believe that one solution to capture this complexity would be to learn this pooling function, by letting the network learn where to focus its attention.Similarly, the view-independent version of our metric is based on the automatic computation of 4 views sampled around the object.The integration of a data-driven view selection model (such as proposed in [Secord et al. 2011]) could certainly improve the results.\n\nThe proposed deep-learning metric has been shown to outperform standard state-of-the-art image quality metrics.More extensive comparisons could be conducted by including metrics dedicated to meshes and point clouds (e.g., [Meynet et al. 2020;Nehm\u00e9 et al. 2021b]) as well as no-reference ones (e.g.[Liu et al. 2021c]).\n\nThe proposed dataset is large but based on only 55 source models.An extension could be considered by increasing the variety of source models (e.g., adding 300 sources) and limiting the number of distortions per source to keep subjective testing feasible.\n\n\nCONCLUSION AND FUTURE WORK\n\nWe produced a valuable large-scale textured meshes quality assessment dataset, with more than 343k distorted meshes derived from 55 source models corrupted by combinations of 5 real-world distortions, related to compression and simplification, applied on the geometry and texture.The source models cover a good diversity in visual contents.Indeed, we proposed three measures, based on spatial information and visual attention complexity, to quantitatively characterize the geometric, color and semantic complexity of the model.A subset of 3000 stimuli were rated in a crowdsourcing subjective experiment.This subset was selected to equitably cover the entire quality range, and to be challenging for objective quality metrics.\n\nOur dataset served us to develop a new image-based quality assessment metric for 3D graphics based on CNN.The metric, called Graphics-LPIPS, can be seen as an extension of LPIPS [Zhang et al. 2018].It is computed on rendered snapshots of the 3D models.It employs a Siamese network fed with reference patches and distorted patches.We employed the AlexNet architecture with learning linear weights on top.The overall quality of the model is derived by averaging local patch qualities.The metric outperformed other image quality metrics in terms of correlations with subjective scores and classification abiltity on our textured mesh dataset.Our metric also demonstrated a good robustness as it provided the best results on a dataset of meshes with vertex colors.\n\nAfter validating the performance of Graphics-LPIPS, we used it to predict the quality scores of stimuli in our dataset not included in the subjective experiment.Annotating the entire dataset allowed us to analyze the influence of each distortion as well as that of their combinations on the perceived quality.We also determine which distortions affect the quality scores the most.We found a strong perceptual interaction between the geometry quantization of the mesh and its level of details.Indeed, quantization artifacts are less visible on coarse meshes.Regarding the texture compression, we showed that the quality level of the JPEG compression algorithm applied to the texture can be reduced to very low values while maintaining the quality of the final rendered stimulus.\n\nFurthermore, we evaluated the influence of the geometry and color complexity on the perception of distortions.We observed that both color and geometry can mask the geometric degradations of a quantized 3D model.Models with close-to-uniform textures are less sensitive to UV map quantization; however, the impact of this distortion on the visual quality depends also on the amount of texture seams.\n\nFurther potential applications.Our dataset of 340K stimuli associated with pseudo-MOSs can be used to train no-reference quality metrics, but not only.Another real-world use case/application of this dataset can be in the rate-distortion control and optimization.This is possible because each of our stimuli is associated with a quality score and a file size, resulting from the compression methods we used for the source model and the texture.Thus, our dataset could allow to propose an analytical perceptual rate-distortion model capable of maximizing the visual quality of the reconstructed textured meshes subjected to a target bitrate.\n\nFuture works.As mentioned in Section 7, some parts of the 3D objects probably have a stronger impact on the overall perceived quality than others.Therefore, we believe that an important improvement to our metric Graphics-LPIPS is to integrate and learn an attention model that would estimate the perceptual weight of each patch (from each view) on the global perceived quality.Still as mentioned in Section 7, a natural follow-up of our work is to replicate this study for 3D objects associated with more complex appearance models, e.g.represented by GGX parametrizations including normal, diffuse, roughness, and specular maps.\n\nThe source code of our metric and the datasets of textured meshes along with the subjective scores (individual quality scores, MOSs and Pseudo-MOSs) is publicly available online5 .\n\nFig. 1 .\n1\nFig. 1.The 3D graphic source models constituting our database.\n\n\n\n\nFigure 2.b.\n\n\nFig. 2 .\n2\nFig. 2. Characterization of the geometry, color, and semantic of textured 3D models.\n\n\n\u2022\n\nLevel of Detail (LoD) simplification: we applied a surface simplification algorithm based on iterative edge collapse and quadric error metric.This algorithm takes into account both geometry and texture mapping[Cignoni et al. 2008;Garland and Heckbert 1998].We generated 10 levels of simplification (  \u2208 [1, 10]) by uniformly reducing the number of mesh faces so that the mesh of the most degraded level (  = 10) has around 2000 faces (regardless of the source model).Thus,\u0394  = ( 0 \u2212   )/10 where \u0394  is the number of faces removed at each   level,  0 is the number of faces of the source model, and   = 2000.\u2022 Quantization: we uniformly quantized the position of the vertices as well as the coordinates of the texture using Draco 3 , an open-source library for compressing and decompressing 3D geometric meshes and point clouds.To generate the compressed meshes, we considered 5 levels for each attribute: \u015b The quantization bits for the position attribute  range from 7 to 11 bits ( \u2208 [7, 11]).\u015b The quantization bits for the texture coordinates attribute (a.k.a UV map)  range from 6 and 10 bits ( \u2208 [6, 10]).The UV map represents the parametrization defined to map the texture image onto the model surface.\u2022 Texture map sub-sampling: we reduced the size of the texture maps by resampling them using the Lanczos low pass filter.We generated 5 texture sizes (  ): 2048 \u00d7 2048 (the original size), 1440 \u00d7 1440, 1024 \u00d7 1024, 712 \u00d7 712, 512 \u00d7 512.\n\n\n\n\nFig. 3. (a) Geometry and color spatial information and (b) visual attention complexity for the selected source models.\n\n\nFig. 4 .\n4\nFig. 4. Some examples of distorted models associated with their size (in KB), from barely visibles distortions (Distortion 1) up to very annoying ones (Distortion 3).Acronyms refer to   |  |  |   |   .\n\n\nFig. 5 .\n5\nFig. 5. Selection of the test stimuli by constrained sampling of the plane formed by 2 pseudo-MOSs.The black dots refer to the pseudo-MOS values of all stimuli in the dataset, while the blue dots refer to those selected for the subjective experiment.\n\n\nFig. 6 .\n6\nFig. 6.The graphical interface of the subjective experiment platform.\n\n\nFig. 7 .\n7\nFig. 7. Distribution of (a) raw scores and (b) MOSs for the subset of 3000 stimuli rated in the subjective experiment.\n\n\nFig. 10 .\n10\nFig. 10.Performance of our metric (Graphics-LPIPS) compared to stateof-the-art image metrics.The reported numbers are averages over our five folds while the error bars show the standard deviation over the folds.\n\n\n\n\nFig. 11.MOS vs. quality metric values for the test set of textured meshes.Each point represents a distorted stimulus identified by its source model.The curve shows the logistic regression.\n\n\nFig. 12 .\n12\nFig.12.Performance of our metric when the light direction changes horizontally (left) or vertically (right).The angles in abscissa refer to the angles from the canonical axis (0 corresponds an horizontal line pointing straight to the front).The reference lighting lies on the left, its performances are depicted with a dash line.Samples of the rendered images are shown at the bottom, the reference lighting is circled in green.\n\n\n\n\nFig. 13.Different conditions of lighting (dimmed light and spotlight) or material (glossy and metallic) along with their prediction scores (correlations).\n\n\n\n\n. Boxplots of MOSs obtained for the quantization of the (a) vertices' positions  and (b) texture coordinates  .Mean values are displayed as circles.\n\n\n\n\nBoxplots of MOSs obtained for the LoD simplification   (a) for all the stimuli and (b) for the least geometry quantized stimuli ( = 11 &  = 10).Mean values are displayed as circles.\n\n\n\n\nBoxplots of MOSs obtained for the texture (a) compression   and (b) sub-sampling   .Mean values are displayed as circles.\n\n\nFig. 17 .\n17\nFig. 17.Boxplots of MOSs illustrating the interaction between the LoD simplification   and the quantization of the model's positions .\n\n\n\n\nFig. 18.Visual example illustrating the interaction between the LoD simplification   and the position quantization  regarding the perceived quality.Acronyms refer to the following combination of distortion parameters:   | | |  |  .The geometric quantization artifacts ( = 7) are more visible on the dense mesh (b) than on the simplified mesh (c).\n\n\nFig. 19 .\n19\nFig. 19.Boxplots of MOSs illustrating the interaction between the texture compression   and sub-sampling   .\n\n\nFig. 20 .\n20\nFig. 20.Boxplots of the MOSs illustrating the influence of (a) geometric complexity   and (b) color complexity   of the models on the perceived degradation of geometric quantization .\n\n\nFig. 21 .\n21\nFig. 21.Boxplots of the MOSs illustrating the influence of the color complexity   of the models on the perceived degradation of texture coordinates quantization  .\n\n\nTable 1 .\n1\nPublicly available quality assessment datasets for meshes and point clouds.\nDataset3D Repre-sentationAttributes# Stimuli ratedLIRIS Textured Mesh [Guo et al. 2016]MeshTexture maps\u2022 100\u00d72 renderings \u2022 36\u00d72 renderings3D Meshes with Vertex Colors [Nehm\u00e9 et al. 2021b]MeshVertex colors480M-PCCD [Alexiou et al. 2019]Point cloudColored\u2022 240 \u2022 40 & 30IRPC\n\nTable 2 .\n2\nList of source models, their number of vertices and semantic category.\nModel ID#VerticesSemantic categoryModel ID#VerticesSemantic category#1357364Animal statue#29119038Food#2123189Human statue#30114145Decoration#3395490Furniture#3116803Musical instrument#499984Machine#32358684Sculpture#5198683Decoration#33155931Animal statue#6258490Human statue#34486850Building#7483746Human statue#35150006Bust#8250723Sculpture#36249439Mean of transport#9189633Animal#37130016Sculpture#10109929Machine#38304435Bust#11134472Electronic device#39100890Sculpture#1217560Musical instrument#40124936Plant#1375950Animal skeleton#4174819Animal#14209609Animal#42150498Decoration#1577924Animal statue#4362989Decoration#16669346Animal#4436204Animal#17393652Animal#45650778Sculpture#1827611Food#464999Food#19308944Sculpture#47110819Book#20185416Sculpture#4856902Decoration#21149906Apparel#4992223Mean of transport#2274662Animal#50260670Decoration#2320144Plant#5160710Mean of transport#24297988Decoration#52635206Animal statue#25151002Sculpture#53150000Decoration#2698763Machine#54299976Animal#2777027Mean of transport#55125002Animal#28306933Animal skeleton\n\n\n\n-LPIPS CMDM vis * SSIM* HDR-VDP2* iCID*\n\ud835\udc43\ud835\udc3f\ud835\udc36\ud835\udc360.890.890.790.810.86\ud835\udc46\ud835\udc45\ud835\udc42\ud835\udc36\ud835\udc360.880.870.80.830.87\n\nTable 4 .\n4\nPerformance comparison of different metrics on the test set of our textured mesh dataset, when several viewpoints are considered per stimulus.\nGraphics-LPIPS SSIM HDR-VDP2 iCID\ud835\udc43\ud835\udc3f\ud835\udc36\ud835\udc360.840.690.680.68\ud835\udc46\ud835\udc45\ud835\udc42\ud835\udc36\ud835\udc360.830.670.690.67\ud835\udc34\ud835\udc48\ud835\udc36 \ud835\udc37\ud835\udc460.740.640.640.63\ud835\udc34\ud835\udc48\ud835\udc36 \ud835\udc35\ud835\udc4a0.960.90.880.88\nhttps://github.com/MEPP-team/Graphics-LPIPS\nhttps://sketchfab.com/features/free-3d-models\nhttps://github.com/google/draco\nhttps://www.prolific.co/\nhttps://github.com/MEPP-team/Graphics-LPIPS\nACKNOWLEDGMENTSThis work was supported by French National Research Agency as part of ANR-PISCo project (ANR-17-CE33-0005).\nPerceptual Characterization of 3D Graphical Contents Based on Attention Complexity Measures. M Abid, M Perreira Da Silva, P Le Callet, 10.1145/3423328.3423498Proceedings of the 1st Workshop on Quality of Experience (QoE) in Visual Multimedia Applications. the 1st Workshop on Quality of Experience (QoE) in Visual Multimedia Applications2020. 202020\n\nNo-reference mesh visual quality assessment via ensemble of convolutional neural networks and compact multi-linear pooling. Ilyass Abouelaziz, Aladine Chetouani, Mohammed El Hassouni, Longin , Jan Latecki, Hocine Cherifi, Pattern Recognition. 1002020. 2020\n\nA convolutional neural network framework for blind mesh visual quality assessment. Ilyass Abouelaziz, Mohammed El Hassouni, Hocine Cherifi, 10.1109/ICIP.2017.8296382IEEE International Conference on Image Processing (ICIP. 2017. 2017. 2017\n\nOn the performance of metrics to predict quality in point cloud representations. E Alexiou, T Ebrahimi, 10.1109/ICME.2018.8486512International Society for Optics and Photonics, SPIE, 282 \u015b 297. E. Alexiou and T. Ebrahimi. Andrew G Tescher, 2017. 2018103962018 IEEE International Conference on Multimedia and Expo (ICME). 1\u015b6\n\nTowards subjective quality assessment of point cloud imaging in augmented reality. E Alexiou, E Upenik, T Ebrahimi, 10.1109/MMSP.2017.81222372017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP). 1\u015b6. 2017\n\nPointXR: A Toolbox for Visualization and Subjective Evaluation of Point Clouds in Virtual Reality. E Alexiou, I Viola, T M Borges, T A Fonseca, R L De Queiroz, T Ebrahimi, 10.1109/QoMEX48832.2020.91231212020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX). 1\u015b6. E Alexiou, N Yang, T Ebrahimi, 2019. 2019. 20208A comprehensive study of the rate-distortion performance in MPEG point cloud compression\n\nMESH: measuring errors between surfaces using the Hausdorff distance. N Aspert, D Santa-Cruz, T Ebrahimi, 10.1109/ICME.2002.1035879Proceedings. IEEE International Conference on Multimedia and Expo. IEEE International Conference on Multimedia and Expo2002\n\nDeep Neural Networks for No-Reference and Full-Reference Image Quality Assessment. Sebastian Bosse, Dominique Maniry, Klaus-Robert M\u00fcller, Thomas Wiegand, Wojciech Samek, 10.1109/TIP.2017.2760518IEEE Transactions on Image Processing. 272018. 2018\n\nProgressive Compression of Arbitrary Textured Meshes. F Caillaud, V Vidal, F Dupont, G Lavou\u00e9, Computer Graphics Forum. 352016. Oct. 2016\n\nThree-dimensional mesh quality metric with reference based on a support vector regression model. A Chetouani, 10.1117/1.JEI.27.4.043048Journal of Electronic Imaging. 272018. 2018\n\nMeshLab: an Open-Source Mesh Processing Tool. Kyriaki Christaki, Emmanouil Christakis, Petros Drakoulis, ; 1\u015b12 Paolo Cignoni, Marco Callieri, Massimiliano Corsini, Matteo Dellepiane, Fabio Ganovelli, Guido Ranzuglia, 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136Subjective Visual Quality Assessment of Immersive 3D Media Compressed by Open-Source Static 3D Mesh Codecs. 25th International Conference on MultiMedia Modeling (MMM). 2018. 2018. 2008Eurographics Italian Chapter Conference. The Eurographics Association\n\nWatermarked 3-D mesh quality assessment. Massimiliano Corsini, Elisa Drelie Gelasca, Touradj Ebrahimi, Mauro Barni, IEEE Transactions on Multimedia. 92007. 2007\n\nReduced-reference metric design for objective perceptual quality assessment in wireless imaging. Ulrich Engelke, Maulana Kusuma, Hans-J\u00fcrgen Zepernick, Manora Caldera, 10.1016/j.image.2009.06.005Signal Processing: Image Communication. 242009. 2009\n\nDeepSim: Deep similarity for image quality assessment. Fei Gao, Yi Wang, Panpeng Li, Min Tan, Jun Yu, Yani Zhu, 10.1016/j.neucom.2017.01.054Machine Learning and Signal Processing for Big Multimedia Analysis. 2017. 2017257\n\nSimplifying Surfaces with Color and Texture Using Quadric Error Metrics. Michael Garland, Paul S Heckbert, Proceedings of the Conference on Visualization. the Conference on Visualization1998. 199898\n\nMassive Online Crowdsourced Study of Subjective and Objective Picture Quality. Deepti Ghadiyaram, Alan C Bovik, 10.1109/TIP.2015.2500021IEEE Transactions on Image Processing. 252016. 2016\n\nJinjiang Guo, Vincent Vidal, Irene Cheng, Anup Basu, Atilla Baskurt, Guillaume Lavou\u00e9, 10.1145/2996296Subjective and Objective Visual Quality Assessment of Textured 3D Meshes. 2016. 10 201614\n\nQuality Evaluation of 3D Objects in Mixed Reality For Different Lighting Conditions. Electronic Imaging. Jes\u00fas Guti\u00e9rrez, Toinon Vigier, Patrick Le, Callet , 10.2352/ISSN.2470-1173.2020.11.HVEI-1282020. 2020. 2020\n\nTowards A Colored Point Cloud Quality Assessment Method Using Colored Texture And Curvature Projection. Zhouyan He, Gangyi Jiang, Zhidi Jiang, Mei Yu, 10.1109/ICIP42928.2021.95067622021 IEEE International Conference on Image Processing (ICIP). 1444\u015b1448. 2021\n\nTobias Ho\u00dffeld, Christian Keimel, Matthias Hirth, Bruno Gardlo, Julian Habigt, Klaus Diepold, Phuoc Tran-Gia, 10.1109/TMM.2013.2291663Best Practices for QoE Crowdtesting: QoE Assessment With Crowdsourcing. 2014. 201416\n\nMultimodal Unsupervised Image-to-Image Translation. Xun Huang, Ming-Yu Liu, Serge Belongi, Jan Kautz, 10.1007/978-3-030-01219-9_112018. 2018. 2018\n\nMethodology for the subjective assessment of the quality of television pictures BT Series Broadcasting service. ITU-R BT.500-13. 20122012International Telecommunication Union\n\nSubjective video quality assessment methods for multimedia applications. ITU-T P.9102008. 2008International Telecommunication Union\n\nSubjective and objective quality evaluation of compressed point clouds. A Javaheri, C Brites, F Pereira, J Ascenso, 10.1109/MMSP.2017.81222392017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP). 1\u015b6. 2017\n\nJavaheri, Brites, Pereira, Ascenso, arXiv:1912.09137Point Cloud Rendering after Coding : Impacts on Subjective and Objective Quality. 2019. 2019\n\nMahalanobis Based Point to Distribution Metric for Point Cloud Geometry Quality Evaluation. A Javaheri, C Brites, F Pereira, J Ascenso, 10.1109/LSP.2020.3010128IEEE Signal Processing Letters. 272020. 2020\n\nRafael Zequeira, Jim\u00e9nez , Laura Fern\u00e1ndez Gallardo, Sebastian M\u00f6ller, 10.1109/QoMEX.2018.8463298fluence of Number of Stimuli for Subjective Speech Quality Assessment in Crowdsourcing. 2018. 2018. 2018Tenth International Conference on Quality of Multimedia Experience (QoMEX)\n\nConvolutional Neural Networks for No-Reference Image Quality Assessment. Le Kang, Peng Ye, Yi Li, David Doermann, 10.1109/CVPR.2014.224IEEE Conference on Computer Vision and Pattern Recognition. 2014. 2014. 2014\n\nA distortion measure for blocking artifacts in images based on human visual sensitivity. S A Karunasekera, N G Kingsbury, 10.1109/83.388074IEEE Transactions on Image Processing. 41995. 1995\n\nOn the accuracy of objective image and video quality models: New methodology for performance evaluation. L Krasula, K Fliegel, P Le Callet, M Kl\u00edma, Eighth International Conference on Quality of Multimedia Experience. 2016. 2016. 2016\n\nA Local Roughness Measure for 3D Meshes and Its Application to Visual Masking. Guillaume Lavou\u00e9, 10.1145/1462048.1462052ACM Trans. Appl. Percept. 52009. 2009Article\n\nA Multiscale Metric for 3D Mesh Visual Quality Assessment. Guillaume Lavou\u00e9, Computer Graphics Forum. 302011. 2011\n\nOn the Efficiency of Image Metrics for Evaluating the Visual Quality of 3D Models. Guillaume Lavou\u00e9, Mohamed Chaker Larabi, Libor Vasa, IEEE Transactions on Visualization and Computer Graphics. 222016. 2016. 1987\u015b1999\n\nBenchmarking of objective quality metrics for point cloud compression. Davi Lazzarotto, Evangelos Alexiou, Touradj Ebrahimi, 2021 IEEE 23rd International Workshop on Multimedia Signal Processing (MMSP). 2021\n\nReduced Reference Perceptual Quality Model With Application to Rate Control for Video-Based Point Cloud Compression. Qi Liu, Hui Yuan, Raouf Hamzaoui, Honglei Su, Junhui Hou, Huan Yang, 10.1109/TIP.2021.3096060IEEE Transactions on Image Processing. 302021b. 2021\n\nPQA-Net: Deep No Reference Point Cloud Quality Assessment via Multi-view Projection. Qi Liu, Hui Yuan, Honglei Su, Hao Liu, Yu Wang, Huan Yang, Junhui Hou, 10.1109/TCSVT.2021.3100282IEEE Transactions on Circuits and Systems for Video Technology. 2021c. 2021\n\nPoint Cloud Quality Assessment: Dataset Construction and Learning-based No-Reference Approach. Yipeng Liu, Qi Yang, Yiling Xu, Le Yang, arXiv:2012.118952021a. 2021preprint\n\nReal-World Textured Things: a Repository of Textured Models Generated with Modern Photo-Reconstruction Tools. Andrea Maggiordomo, Federico Ponchio, Paolo Cignoni, Marco Tarini, ArXiv:2004.147532020. 2020\n\nHDR-VDP-2: A Calibrated Visual Metric for Visibility and Quality Predictions in All Luminance Conditions. Rafa\u0142 Mantiuk, Joong Kil, Allan G Kim, Wolfgang Rempel, Heidrich, 10.1145/2010324.1964935ACM Trans. Graph. 3040142011. July 2011\n\nDesign, Implementation, and Evaluation of a Point Cloud Codec for Tele-Immersive Video. K Rafa\u0142, Anna Mantiuk, Rados\u0142aw Tomaszewska, ; R Mantiuk, K Mekuria, P Blom, Cesar, 10.1111/j.1467-8659.2012.03188.xComparison of Four Subjective Methods for Image Quality Assessment. 2012. dec 2012. 2017. Apr. 201731IEEE Transactions on Circuits and Systems for Video Technology\n\nPC-MSDM: A quality metric for 3D point clouds. Gabriel Meynet, Julie Digne, Guillaume Lavou\u00e9, 10.1109/QoMEX.2019.87433132019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX). 1\u015b3. 2019\n\nPCQM: A Full-Reference Quality Metric for Colored 3D Point Clouds. Gabriel Meynet, Yana Nehm\u00e9, Julie Digne, Guillaume Lavou\u00e9, 10.1109/QoMEX48832.2020.91231472020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX). 1\u015b6. 2020\n\nComparison of Subjective Methods for Quality Assessment of 3D Graphics in Virtual Reality. Yana Nehm\u00e9, Jean-Philippe Farrugia, Florent Dupont, Patrick Le Callet, Guillaume Lavou\u00e9, 10.1145/3427931ACM Transactions on Applied Perception. 1812020. Dec. 2020\n\nExploring Crowdsourcing for Subjective Quality Assessment of 3D Graphics. Yana Nehm\u00e9, Patrick Le Callet, Florent Dupont, Jean-Philippe Farrugia, Guillaume Lavou\u00e9, IEEE International Workshop on Multimedia Signal Processing. 2021a. 2021MMSP\n\nVisual Quality of 3D Meshes With Diffuse Colors in Virtual Reality: Subjective and Objective Evaluation. Yana Nehm\u00e9, Florent Dupont, Jean-Philippe Farrugia, Patrick Le Callet, Guillaume Lavou\u00e9, 10.1109/TVCG.2020.3036153IEEE Transactions on Visualization and Computer Graphics. 272021b. 2021\n\nAnass Nouri, Christophe Charrier, Olivier L\u00e9zoray, 3D Blind Mesh Quality Assessment Index. IS&T International Symposium on Electronic Imaging. 2017. Jan. 2017\n\nQuality metric for approximating subjective evaluation of 3-D objects. Yixin Pan, Cheng, Basu, 10.1109/TMM.2005.843364IEEE Transactions on Multimedia. 722005. apr 2005\n\nQuality Evaluation Of Static Point Clouds Encoded Using MPEG Codecs. S Perry, H P Cong, L A Da Silva Cruz, J Prazeres, M Pereira, A Pinheiro, E Dumic, E Alexiou, T Ebrahimi, 10.1109/ICIP40778.2020.91913082020 IEEE International Conference on Image Processing (ICIP). 3428\u015b3432. 2020\n\nColor-image quality assessment: From prediction to optimization. Jens Preiss, Felipe Fernandes, Philipp Urban, 10.1109/TIP.2014.2302684IEEE Transactions on Image Processing. 232014. 2014\n\nA deep perceptual metric for 3D point clouds. Maurice Quach, Aladine Chetouani, Giuseppe Valenzise, Frederic Dufaux, 10.2352/ISSN.2470-1173.2021.9.IQSP-257Electronic Imaging. 20212021. 2021\n\nJudith Redi, Ernestasia Siahaan, Pavel Korshunov, Julian Habigt, Tobias Hossfeld, 10.1145/2810188.2810194When the Crowd Challenges the Lab: Lessons Learnt from Subjective Studies on Image Aesthetic Appeal. Proceedings of the Fourth International Workshop on Crowdsourcing for Multimedia. 2015. 2015\n\nTeaching Data-driven Video Processing via Crowdsourced Data Collection. Max Reimann, Ole Wegen, Sebastian Pasewaldt, Amir Semmo, J\u00fcrgen D\u00f6llner, Matthias Trapp, 10.2312/eged.20211000Eurographics 2021 -Education Papers. 2021. 2021\n\nAre image quality metrics adequate to evaluate the quality of geometric objects?. Bernice E Rogowitz, Holly E Rushmeier, 10.1117/12.429504Human Vision and Electronic Imaging VI. 2001. 20014299\n\nPerceptual models of viewpoint preference. Adrian Secord, Jingwan Lu, Adam Finkelstein, Manish Singh, Andrew Nealen, ACM Transactions on Graphics. 302011. Oct. 2011\n\nImage information and visual quality. H R Sheikh, A C Bovik, IEEE Transactions on Image Processing. 1522006. Feb. 2006\n\nPerceptual Quality Assessment of 3D Point Clouds. H Su, Z Duanmu, W Liu, Q Liu, Z Wang, 2019 IEEE International Conference on Image Processing (ICIP). 2019\n\nComparing the Quality of Highly Realistic Digital Humans in 3DoF and 6DoF: A Volumetric Video Case Study. Shishir Subramanyam, Jie Li, Irene Viola, Pablo Cesar, 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). IEEE2020\n\nNIMA: Neural Image Assessment. Hossein Talebi, Peyman Milanfar, 10.1109/TIP.2018.2831899IEEE Transactions on Image Processing. 2728318992018. 2018. 2018\n\nWen-Xu Tao, Gang-Yi Jiang, Zhi-Di Jiang, Mei Yu, Point Cloud Projection and Multi-Scale Feature Fusion Network Based Blind Quality Assessment for Colored Point Clouds. New York, NY, USA2021\n\nTaimoor Tariq, Okan Tarhan Tursun, Munchurl Kim, Piotr Didyk, Why Are Deep Representations Good Perceptual Quality Features? Computer Vision \u015b ECCV 2020. 2020. 2020\n\nBatex3: Bit allocation for progressive transmission of textured 3-D models. Dihong Tian, G Alregib, IEEE Transactions on Circuits and Systems for Video Technology. 182008. 2008\n\nGeometric distortion metrics for point cloud compression. Dong Tian, Hideaki Ochimizu, Chen Feng, Robert Cohen, Anthony Vetro, 10.1109/ICIP.2017.82969252017 IEEE International Conference on Image Processing (ICIP). 3460\u015b3464. 2017\n\nA curvature-tensor-based perceptual quality metric for 3D triangular meshes. Machine Graphics & Vision. Fakhri Torkhani, Kai Wang, Jean-Marc Chassery, 2014. 201423\n\nPerceptual quality assessment of 3D dynamic meshes: Subjective and objective studies. Fakhri Torkhani, Kai Wang, Jean-Marc Chassery, 10.1016/j.image.2014.12.008Image Communication. 3122015. Feb. 2015Signal Processing\n\nA novel methodology for quality assessment of voxelized point clouds. E M Torlig, E Alexiou, T A Fonseca, R L De Queiroz, T Ebrahimi, Applications of Digital Image Processing. Andrew G Tescher, International Society for Optics and Photonics, SPIE2018. 174 \u015b 19010752\n\nVisual quality assessment of 3D models: On the influence of light-material interaction. K Vanhoey, B Sauvage, P Kraemer, G Lavou\u00e9, 10.1145/3129505ACM Transactions on Applied Perception. 1512017. 2017\n\nDihedral Angle Mesh Error: a fast perception correlated distortion measure for fixed connectivity triangle meshes. Libor V\u00e1\u0161a, Computer Graphics Forum. 315Jan Rus. 2012. 2012\n\nA Color-Based Objective Quality Metric for Point Cloud Contents. Irene Viola, Shishir Subramanyam, Pablo Cesar, 10.1109/QoMEX48832.2020.91230892020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX). 1\u015b6. 2020\n\nIrene Viola, Shishir Subramanyam, Jie Li, Pablo Cesar, arXiv:2201.07701[cs.MM]On the impact of VR assessment on the Quality of Experience of Highly Realistic Digital Humans. 2022\n\nReport on the Validation of Video Quality Models for High Definition Video Content. 2010. June 2010VQEG\n\nA Fast Roughness-Based Approach to the Assessment of 3D Mesh Visual Quality. Kai Wang, Fakhri Torkhani, Annick Montanvert, Computers & Graphics. 2012. 2012\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Transactions on Image Processing. 132004. 2004\n\nMeasuring and predicting visual fidelity. Benjamin Watson, 2001. 2001ACM Siggraph\n\nEnd-to-End Blind Image Quality Prediction With Cascaded Deep Neural Network. Jinjian Wu, Jupo Ma, Fuhu Liang, Weisheng Dong, Guangming Shi, Weisi Lin, 10.1109/TIP.2020.3002478IEEE Transactions on Image Processing. 292020. 2020\n\nSubjective Quality Database and Objective Study of Compressed Point Clouds With 6DoF Head-Mounted Display. Xinju Wu, Yun Zhang, Chunling Fan, Junhui Hou, Sam Kwong, IEEE Transactions on Circuits and Systems for Video Technology. 312021. 2021\n\nPose Guided Human Video Generation. Computer Vision \u015b ECCV. Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping Shi, Dahua Lin, 10.1007/978-3-030-01249-6_132018. 2018. 2018\n\nPredicting the Perceptual Quality of Point Cloud: A 3D-to-2D Projection-Based Exploration. Q Yang, H Chen, Z Ma, Y Xu, R Tang, J Sun, IEEE Transactions on Multimedia. 2020. 2020\n\nInferring Point Cloud Quality via Graph Similarity. Qi Yang, Zhan Ma, Yiling Xu, Zhu Li, Jun Sun, 10.1109/TPAMI.2020.3047083IEEE Transactions on Pattern Analysis and Machine Intelligence. 2020. 2020\n\nA machine learning framework for full-reference 3D shape quality assessment. Zeynep Cipiloglu Yildiz, A Cengiz Oztireli, Tolga Capin, Visual Computer. 362020. 2020\n\nImage complexity and spatial information. Honghai Yu, Stefan Winkler, 10.1109/QoMEX.2013.6603194Fifth International Workshop on Quality of Multimedia Experience (QoMEX). 2013. 2013. 2013\n\nSubjective and objective quality assessment for volumetric video compression. Emin Zerman, Pan Gao, Cagri Ozcinar, Aljosa Smolic, Electronic Imaging. 102019. 2019. 2019\n\nTextured mesh vs coloured point cloud: A subjective study for volumetric video compression. Emin Zerman, Cagri Ozcinar, Pan Gao, Aljosa Smolic, 10.1109/CVPR.2018.000682020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX). Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, IEEE2020. 2018. 2018. 2018IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n\nMS-GraphSIM: Inferring Point Cloud Quality via Multiscale Graph Similarity. Yujie Zhang, Qi Yang, Yiling Xu, 10.1145/3474085.34752942021. 1230\u015b1238Association for Computing MachineryNew York, NY, USA\n\nQuantitative analysis of discrete 3D geometrical detail levels based on perceptual metric. Qing Zhu, Junqiao Zhao, Zhiqiang Du, Yeting Zhang, 10.1016/j.cag.2009.10.004Computers & Graphics. 342010. 2010\n", "annotations": {"author": "[{\"end\":548,\"start\":96}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":101}]", "author_first_name": "[{\"end\":100,\"start\":96}]", "author_affiliation": "[{\"end\":159,\"start\":108},{\"end\":228,\"start\":161},{\"end\":296,\"start\":230},{\"end\":372,\"start\":298},{\"end\":456,\"start\":374},{\"end\":498,\"start\":458},{\"end\":547,\"start\":500}]", "title": "[{\"end\":93,\"start\":1},{\"end\":641,\"start\":549}]", "venue": null, "abstract": "[{\"end\":2537,\"start\":989}]", "bib_ref": "[{\"end\":10510,\"start\":10478},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10876,\"start\":10853},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10896,\"start\":10876},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10908,\"start\":10896},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10929,\"start\":10908},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":10949,\"start\":10929},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10967,\"start\":10949},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":10978,\"start\":10967},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11075,\"start\":11058},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11097,\"start\":11075},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11115,\"start\":11097},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11134,\"start\":11115},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11150,\"start\":11134},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":11169,\"start\":11150},{\"end\":11370,\"start\":11343},{\"end\":11390,\"start\":11370},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11411,\"start\":11390},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11432,\"start\":11411},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11447,\"start\":11432},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11569,\"start\":11548},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11586,\"start\":11569},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11604,\"start\":11586},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11628,\"start\":11604},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11647,\"start\":11628},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":11662,\"start\":11647},{\"end\":11679,\"start\":11662},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":11697,\"start\":11679},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":11719,\"start\":11697},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12227,\"start\":12206},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12249,\"start\":12227},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12273,\"start\":12249},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":12294,\"start\":12273},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":12312,\"start\":12294},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":12330,\"start\":12312},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12469,\"start\":12456},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12488,\"start\":12469},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12506,\"start\":12488},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":12521,\"start\":12506},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":12540,\"start\":12521},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":12552,\"start\":12540},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12699,\"start\":12678},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12721,\"start\":12699},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12737,\"start\":12721},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":12757,\"start\":12737},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":12775,\"start\":12757},{\"end\":12859,\"start\":12832},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12876,\"start\":12859},{\"end\":13281,\"start\":13254},{\"end\":13300,\"start\":13281},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13323,\"start\":13300},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13345,\"start\":13323},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13365,\"start\":13345},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13389,\"start\":13365},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":13408,\"start\":13389},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":13423,\"start\":13408},{\"end\":13440,\"start\":13423},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13806,\"start\":13789},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13827,\"start\":13806},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13846,\"start\":13827},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13862,\"start\":13846},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13890,\"start\":13862},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":13905,\"start\":13890},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":13923,\"start\":13905},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":13945,\"start\":13923},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":14207,\"start\":14185},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":14225,\"start\":14207},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14380,\"start\":14359},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14402,\"start\":14380},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14420,\"start\":14402},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14439,\"start\":14420},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":14463,\"start\":14439},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":14481,\"start\":14463},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":14495,\"start\":14481},{\"end\":14570,\"start\":14549},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14591,\"start\":14570},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14685,\"start\":14664},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15197,\"start\":15178},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15562,\"start\":15538},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16263,\"start\":16241},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16333,\"start\":16317},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":16389,\"start\":16369},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":16445,\"start\":16426},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":16533,\"start\":16514},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16586,\"start\":16565},{\"end\":16638,\"start\":16620},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17212,\"start\":17192},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17436,\"start\":17416},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":17455,\"start\":17436},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18162,\"start\":18150},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":18189,\"start\":18170},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":18214,\"start\":18196},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":18247,\"start\":18225},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18354,\"start\":18327},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":18370,\"start\":18354},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18404,\"start\":18384},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18573,\"start\":18556},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18592,\"start\":18573},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":18614,\"start\":18592},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18682,\"start\":18662},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":18710,\"start\":18691},{\"end\":18739,\"start\":18721},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":18756,\"start\":18739},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18803,\"start\":18781},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19090,\"start\":19074},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19109,\"start\":19090},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":19127,\"start\":19109},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":19286,\"start\":19267},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19436,\"start\":19412},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19679,\"start\":19661},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":19822,\"start\":19803},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20178,\"start\":20156},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":20194,\"start\":20178},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20227,\"start\":20211},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":20242,\"start\":20227},{\"end\":20258,\"start\":20242},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":20370,\"start\":20352},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20397,\"start\":20377},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20429,\"start\":20408},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":20458,\"start\":20435},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20839,\"start\":20822},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":21105,\"start\":21088},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21482,\"start\":21458},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21902,\"start\":21883},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21918,\"start\":21902},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":21943,\"start\":21918},{\"end\":21961,\"start\":21943},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22001,\"start\":21983},{\"end\":22571,\"start\":22553},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24999,\"start\":24978},{\"end\":25016,\"start\":24999},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":25036,\"start\":25016},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25664,\"start\":25646},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27812,\"start\":27794},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28626,\"start\":28609},{\"end\":31186,\"start\":31175},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":33707,\"start\":33688},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33886,\"start\":33865},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":34007,\"start\":33988},{\"end\":34087,\"start\":34068},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":34201,\"start\":34182},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":34217,\"start\":34201},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34451,\"start\":34433},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":34466,\"start\":34451},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":35731,\"start\":35702},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36636,\"start\":36618},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37676,\"start\":37657},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39266,\"start\":39245},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":39283,\"start\":39266},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":39303,\"start\":39283},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":39514,\"start\":39495},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40061,\"start\":40034},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":41582,\"start\":41563},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":42271,\"start\":42252},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":42672,\"start\":42651},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":42903,\"start\":42883},{\"end\":45780,\"start\":45762},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":46031,\"start\":46012},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":46048,\"start\":46031},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":48855,\"start\":48836},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":48871,\"start\":48855},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":53543,\"start\":53523},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":54898,\"start\":54878},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":56314,\"start\":56295},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":56620,\"start\":56601},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":69672,\"start\":69639},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":71469,\"start\":71452},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":73226,\"start\":73201},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":75218,\"start\":75198},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":75500,\"start\":75480},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":75519,\"start\":75500},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":75573,\"start\":75555},{\"end\":76787,\"start\":76768},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":80406,\"start\":80385},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":80431,\"start\":80406}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":80057,\"start\":79982},{\"attributes\":{\"id\":\"fig_1\"},\"end\":80073,\"start\":80058},{\"attributes\":{\"id\":\"fig_2\"},\"end\":80171,\"start\":80074},{\"attributes\":{\"id\":\"fig_3\"},\"end\":81621,\"start\":80172},{\"attributes\":{\"id\":\"fig_4\"},\"end\":81744,\"start\":81622},{\"attributes\":{\"id\":\"fig_5\"},\"end\":81959,\"start\":81745},{\"attributes\":{\"id\":\"fig_6\"},\"end\":82223,\"start\":81960},{\"attributes\":{\"id\":\"fig_7\"},\"end\":82306,\"start\":82224},{\"attributes\":{\"id\":\"fig_8\"},\"end\":82438,\"start\":82307},{\"attributes\":{\"id\":\"fig_10\"},\"end\":82665,\"start\":82439},{\"attributes\":{\"id\":\"fig_11\"},\"end\":82858,\"start\":82666},{\"attributes\":{\"id\":\"fig_12\"},\"end\":83302,\"start\":82859},{\"attributes\":{\"id\":\"fig_13\"},\"end\":83461,\"start\":83303},{\"attributes\":{\"id\":\"fig_14\"},\"end\":83614,\"start\":83462},{\"attributes\":{\"id\":\"fig_15\"},\"end\":83800,\"start\":83615},{\"attributes\":{\"id\":\"fig_16\"},\"end\":83926,\"start\":83801},{\"attributes\":{\"id\":\"fig_17\"},\"end\":84076,\"start\":83927},{\"attributes\":{\"id\":\"fig_18\"},\"end\":84427,\"start\":84077},{\"attributes\":{\"id\":\"fig_19\"},\"end\":84551,\"start\":84428},{\"attributes\":{\"id\":\"fig_20\"},\"end\":84750,\"start\":84552},{\"attributes\":{\"id\":\"fig_21\"},\"end\":84929,\"start\":84751},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":85292,\"start\":84930},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":86437,\"start\":85293},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":86529,\"start\":86438},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":86803,\"start\":86530}]", "paragraph": "[{\"end\":3969,\"start\":2553},{\"end\":4705,\"start\":3971},{\"end\":5492,\"start\":4707},{\"end\":6115,\"start\":5494},{\"end\":6465,\"start\":6117},{\"end\":7177,\"start\":6467},{\"end\":7848,\"start\":7179},{\"end\":8063,\"start\":7850},{\"end\":8115,\"start\":8065},{\"end\":9458,\"start\":8117},{\"end\":9532,\"start\":9460},{\"end\":10094,\"start\":9534},{\"end\":10602,\"start\":10111},{\"end\":11170,\"start\":10636},{\"end\":11720,\"start\":11172},{\"end\":11986,\"start\":11722},{\"end\":13063,\"start\":11988},{\"end\":14226,\"start\":13065},{\"end\":14799,\"start\":14228},{\"end\":15563,\"start\":14801},{\"end\":16289,\"start\":15565},{\"end\":16300,\"start\":16291},{\"end\":16306,\"start\":16302},{\"end\":16412,\"start\":16308},{\"end\":16446,\"start\":16414},{\"end\":16498,\"start\":16493},{\"end\":17105,\"start\":16500},{\"end\":17562,\"start\":17138},{\"end\":18804,\"start\":17564},{\"end\":19823,\"start\":18806},{\"end\":21770,\"start\":19825},{\"end\":22963,\"start\":21772},{\"end\":23459,\"start\":22986},{\"end\":23760,\"start\":23489},{\"end\":24395,\"start\":23762},{\"end\":24667,\"start\":24424},{\"end\":25343,\"start\":24669},{\"end\":25378,\"start\":25375},{\"end\":26770,\"start\":25380},{\"end\":27125,\"start\":26772},{\"end\":28823,\"start\":27155},{\"end\":29942,\"start\":28825},{\"end\":30230,\"start\":29944},{\"end\":30988,\"start\":30246},{\"end\":31646,\"start\":30990},{\"end\":32030,\"start\":31648},{\"end\":32426,\"start\":32056},{\"end\":33301,\"start\":32453},{\"end\":33437,\"start\":33303},{\"end\":34012,\"start\":33439},{\"end\":34219,\"start\":34014},{\"end\":35511,\"start\":34221},{\"end\":36174,\"start\":35525},{\"end\":37382,\"start\":36176},{\"end\":39011,\"start\":37411},{\"end\":40026,\"start\":39044},{\"end\":40546,\"start\":40028},{\"end\":41069,\"start\":40548},{\"end\":41387,\"start\":41071},{\"end\":41835,\"start\":41389},{\"end\":42344,\"start\":41837},{\"end\":42620,\"start\":42346},{\"end\":42753,\"start\":42622},{\"end\":43344,\"start\":42755},{\"end\":43433,\"start\":43346},{\"end\":43558,\"start\":43435},{\"end\":43642,\"start\":43592},{\"end\":43723,\"start\":43644},{\"end\":43813,\"start\":43740},{\"end\":43843,\"start\":43838},{\"end\":43868,\"start\":43845},{\"end\":44399,\"start\":43995},{\"end\":44901,\"start\":44401},{\"end\":45244,\"start\":44978},{\"end\":45609,\"start\":45505},{\"end\":46163,\"start\":45611},{\"end\":47247,\"start\":46196},{\"end\":47250,\"start\":47249},{\"end\":47661,\"start\":47281},{\"end\":48577,\"start\":47663},{\"end\":49015,\"start\":48579},{\"end\":49663,\"start\":49059},{\"end\":49778,\"start\":49665},{\"end\":49840,\"start\":49806},{\"end\":50481,\"start\":49842},{\"end\":51015,\"start\":50483},{\"end\":51731,\"start\":51017},{\"end\":52170,\"start\":51733},{\"end\":52335,\"start\":52172},{\"end\":53053,\"start\":52362},{\"end\":54719,\"start\":53055},{\"end\":55611,\"start\":54779},{\"end\":56415,\"start\":55613},{\"end\":57336,\"start\":56417},{\"end\":57643,\"start\":57349},{\"end\":58053,\"start\":57694},{\"end\":58919,\"start\":58055},{\"end\":60454,\"start\":58921},{\"end\":62025,\"start\":60484},{\"end\":62242,\"start\":62041},{\"end\":62585,\"start\":62244},{\"end\":62923,\"start\":62587},{\"end\":66904,\"start\":62977},{\"end\":67364,\"start\":66966},{\"end\":67432,\"start\":67366},{\"end\":68352,\"start\":67434},{\"end\":69058,\"start\":68413},{\"end\":69431,\"start\":69060},{\"end\":69809,\"start\":69493},{\"end\":71129,\"start\":69811},{\"end\":71471,\"start\":71131},{\"end\":71774,\"start\":71473},{\"end\":72045,\"start\":71911},{\"end\":72310,\"start\":72047},{\"end\":73515,\"start\":72312},{\"end\":74326,\"start\":73531},{\"end\":75256,\"start\":74328},{\"end\":75575,\"start\":75258},{\"end\":75831,\"start\":75577},{\"end\":76588,\"start\":75862},{\"end\":77350,\"start\":76590},{\"end\":78129,\"start\":77352},{\"end\":78528,\"start\":78131},{\"end\":79169,\"start\":78530},{\"end\":79799,\"start\":79171},{\"end\":79981,\"start\":79801},{\"end\":80056,\"start\":79994},{\"end\":80072,\"start\":80061},{\"end\":80170,\"start\":80086},{\"end\":81620,\"start\":80176},{\"end\":81743,\"start\":81625},{\"end\":81958,\"start\":81757},{\"end\":82222,\"start\":81972},{\"end\":82305,\"start\":82236},{\"end\":82437,\"start\":82319},{\"end\":82664,\"start\":82453},{\"end\":82857,\"start\":82669},{\"end\":83301,\"start\":82873},{\"end\":83460,\"start\":83306},{\"end\":83613,\"start\":83465},{\"end\":83799,\"start\":83618},{\"end\":83925,\"start\":83804},{\"end\":84075,\"start\":83941},{\"end\":84426,\"start\":84080},{\"end\":84550,\"start\":84442},{\"end\":84749,\"start\":84566},{\"end\":84928,\"start\":84765},{\"end\":85018,\"start\":84943},{\"end\":85376,\"start\":85306},{\"end\":86480,\"start\":86441},{\"end\":86685,\"start\":86543}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16492,\"start\":16447},{\"attributes\":{\"id\":\"formula_1\"},\"end\":43591,\"start\":43559},{\"attributes\":{\"id\":\"formula_2\"},\"end\":43739,\"start\":43724},{\"attributes\":{\"id\":\"formula_3\"},\"end\":43837,\"start\":43814},{\"attributes\":{\"id\":\"formula_4\"},\"end\":43994,\"start\":43869},{\"attributes\":{\"id\":\"formula_5\"},\"end\":45504,\"start\":45245},{\"attributes\":{\"id\":\"formula_6\"},\"end\":49058,\"start\":49016},{\"attributes\":{\"id\":\"formula_7\"},\"end\":49805,\"start\":49779},{\"attributes\":{\"id\":\"formula_8\"},\"end\":71910,\"start\":71821}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15640,\"start\":15639},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23661,\"start\":23660},{\"end\":56424,\"start\":56423},{\"end\":57144,\"start\":57143},{\"end\":57519,\"start\":57518},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":61280,\"start\":61279},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":61441,\"start\":61440}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2551,\"start\":2539},{\"attributes\":{\"n\":\"2\"},\"end\":10109,\"start\":10097},{\"attributes\":{\"n\":\"2.1\"},\"end\":10634,\"start\":10605},{\"attributes\":{\"n\":\"2.2\"},\"end\":17136,\"start\":17108},{\"attributes\":{\"n\":\"3\"},\"end\":22984,\"start\":22966},{\"attributes\":{\"n\":\"3.1\"},\"end\":23487,\"start\":23462},{\"attributes\":{\"n\":\"3.2\"},\"end\":24422,\"start\":24398},{\"end\":25373,\"start\":25346},{\"end\":27153,\"start\":27128},{\"attributes\":{\"n\":\"3.3\"},\"end\":30244,\"start\":30233},{\"attributes\":{\"n\":\"4\"},\"end\":32054,\"start\":32033},{\"attributes\":{\"n\":\"4.1\"},\"end\":32451,\"start\":32429},{\"attributes\":{\"n\":\"4.2\"},\"end\":35523,\"start\":35514},{\"attributes\":{\"n\":\"4.3\"},\"end\":37409,\"start\":37385},{\"attributes\":{\"n\":\"4.4\"},\"end\":39042,\"start\":39014},{\"attributes\":{\"n\":\"5\"},\"end\":44976,\"start\":44904},{\"attributes\":{\"n\":\"5.1\"},\"end\":46194,\"start\":46166},{\"end\":47261,\"start\":47253},{\"end\":47279,\"start\":47264},{\"attributes\":{\"n\":\"5.2\"},\"end\":52360,\"start\":52338},{\"attributes\":{\"n\":\"5.3\"},\"end\":54777,\"start\":54722},{\"end\":57347,\"start\":57339},{\"attributes\":{\"n\":\"5.4\"},\"end\":57692,\"start\":57646},{\"attributes\":{\"n\":\"5.5\"},\"end\":60482,\"start\":60457},{\"attributes\":{\"n\":\"6\"},\"end\":62039,\"start\":62028},{\"attributes\":{\"n\":\"6.1\"},\"end\":62975,\"start\":62926},{\"attributes\":{\"n\":\"6.2\"},\"end\":66964,\"start\":66907},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":68411,\"start\":68355},{\"attributes\":{\"n\":\"6.3\"},\"end\":69491,\"start\":69434},{\"end\":71820,\"start\":71777},{\"attributes\":{\"n\":\"7\"},\"end\":73529,\"start\":73518},{\"attributes\":{\"n\":\"8\"},\"end\":75860,\"start\":75834},{\"end\":79991,\"start\":79983},{\"end\":80083,\"start\":80075},{\"end\":80174,\"start\":80173},{\"end\":81754,\"start\":81746},{\"end\":81969,\"start\":81961},{\"end\":82233,\"start\":82225},{\"end\":82316,\"start\":82308},{\"end\":82449,\"start\":82440},{\"end\":82869,\"start\":82860},{\"end\":83937,\"start\":83928},{\"end\":84438,\"start\":84429},{\"end\":84562,\"start\":84553},{\"end\":84761,\"start\":84752},{\"end\":84940,\"start\":84931},{\"end\":85303,\"start\":85294},{\"end\":86540,\"start\":86531}]", "table": "[{\"end\":85292,\"start\":85019},{\"end\":86437,\"start\":85377},{\"end\":86529,\"start\":86481},{\"end\":86803,\"start\":86686}]", "figure_caption": "[{\"end\":80057,\"start\":79993},{\"end\":80073,\"start\":80060},{\"end\":80171,\"start\":80085},{\"end\":81621,\"start\":80175},{\"end\":81744,\"start\":81624},{\"end\":81959,\"start\":81756},{\"end\":82223,\"start\":81971},{\"end\":82306,\"start\":82235},{\"end\":82438,\"start\":82318},{\"end\":82665,\"start\":82452},{\"end\":82858,\"start\":82668},{\"end\":83302,\"start\":82872},{\"end\":83461,\"start\":83305},{\"end\":83614,\"start\":83464},{\"end\":83800,\"start\":83617},{\"end\":83926,\"start\":83803},{\"end\":84076,\"start\":83940},{\"end\":84427,\"start\":84079},{\"end\":84551,\"start\":84441},{\"end\":84750,\"start\":84565},{\"end\":84929,\"start\":84764},{\"end\":85019,\"start\":84942},{\"end\":85377,\"start\":85305},{\"end\":86481,\"start\":86440},{\"end\":86686,\"start\":86542}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23742,\"start\":23741},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26332,\"start\":26331},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27122,\"start\":27121},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28308,\"start\":28307},{\"end\":29030,\"start\":29029},{\"end\":29150,\"start\":29149},{\"end\":29504,\"start\":29503},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31885,\"start\":31884},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":34767,\"start\":34766},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35025,\"start\":35024},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37779,\"start\":37778},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":44423,\"start\":44422},{\"end\":44627,\"start\":44626},{\"end\":45523,\"start\":45522},{\"end\":47055,\"start\":47054},{\"end\":47287,\"start\":47286},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":52371,\"start\":52369},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":52878,\"start\":52876},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":57507,\"start\":57505},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":58135,\"start\":58133},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":58174,\"start\":58172},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":58368,\"start\":58366},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59024,\"start\":59022},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59091,\"start\":59089},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59857,\"start\":59855},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59940,\"start\":59938},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":61429,\"start\":61427},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":63862,\"start\":63851},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":64520,\"start\":64517},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":65071,\"start\":65068},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":65889,\"start\":65886},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":65901,\"start\":65898},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":66051,\"start\":66048},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":67601,\"start\":67599},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":68007,\"start\":68005},{\"attributes\":{\"ref_id\":\"fig_19\"},\"end\":68544,\"start\":68542},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":69057,\"start\":69054},{\"attributes\":{\"ref_id\":\"fig_20\"},\"end\":70852,\"start\":70849},{\"attributes\":{\"ref_id\":\"fig_20\"},\"end\":71208,\"start\":71205},{\"attributes\":{\"ref_id\":\"fig_21\"},\"end\":72056,\"start\":72054}]", "bib_author_first_name": "[{\"end\":87212,\"start\":87211},{\"end\":87220,\"start\":87219},{\"end\":87229,\"start\":87221},{\"end\":87241,\"start\":87240},{\"end\":87244,\"start\":87242},{\"end\":87599,\"start\":87593},{\"end\":87619,\"start\":87612},{\"end\":87639,\"start\":87631},{\"end\":87642,\"start\":87640},{\"end\":87659,\"start\":87653},{\"end\":87665,\"start\":87662},{\"end\":87681,\"start\":87675},{\"end\":87816,\"start\":87810},{\"end\":87837,\"start\":87829},{\"end\":87840,\"start\":87838},{\"end\":87857,\"start\":87851},{\"end\":88049,\"start\":88048},{\"end\":88060,\"start\":88059},{\"end\":88195,\"start\":88189},{\"end\":88197,\"start\":88196},{\"end\":88377,\"start\":88376},{\"end\":88388,\"start\":88387},{\"end\":88398,\"start\":88397},{\"end\":88623,\"start\":88622},{\"end\":88634,\"start\":88633},{\"end\":88643,\"start\":88642},{\"end\":88645,\"start\":88644},{\"end\":88655,\"start\":88654},{\"end\":88657,\"start\":88656},{\"end\":88668,\"start\":88667},{\"end\":88670,\"start\":88669},{\"end\":88684,\"start\":88683},{\"end\":88815,\"start\":88814},{\"end\":88826,\"start\":88825},{\"end\":88834,\"start\":88833},{\"end\":89023,\"start\":89022},{\"end\":89033,\"start\":89032},{\"end\":89047,\"start\":89046},{\"end\":89300,\"start\":89291},{\"end\":89317,\"start\":89308},{\"end\":89338,\"start\":89326},{\"end\":89353,\"start\":89347},{\"end\":89371,\"start\":89363},{\"end\":89511,\"start\":89510},{\"end\":89523,\"start\":89522},{\"end\":89532,\"start\":89531},{\"end\":89542,\"start\":89541},{\"end\":89693,\"start\":89692},{\"end\":89828,\"start\":89821},{\"end\":89849,\"start\":89840},{\"end\":89868,\"start\":89862},{\"end\":89881,\"start\":89880},{\"end\":89892,\"start\":89882},{\"end\":89907,\"start\":89902},{\"end\":89930,\"start\":89918},{\"end\":89946,\"start\":89940},{\"end\":89964,\"start\":89959},{\"end\":89981,\"start\":89976},{\"end\":90364,\"start\":90352},{\"end\":90379,\"start\":90374},{\"end\":90386,\"start\":90380},{\"end\":90403,\"start\":90396},{\"end\":90419,\"start\":90414},{\"end\":90576,\"start\":90570},{\"end\":90593,\"start\":90586},{\"end\":90613,\"start\":90602},{\"end\":90631,\"start\":90625},{\"end\":90780,\"start\":90777},{\"end\":90788,\"start\":90786},{\"end\":90802,\"start\":90795},{\"end\":90810,\"start\":90807},{\"end\":90819,\"start\":90816},{\"end\":90828,\"start\":90824},{\"end\":91025,\"start\":91018},{\"end\":91039,\"start\":91035},{\"end\":91041,\"start\":91040},{\"end\":91230,\"start\":91224},{\"end\":91247,\"start\":91243},{\"end\":91249,\"start\":91248},{\"end\":91342,\"start\":91334},{\"end\":91355,\"start\":91348},{\"end\":91368,\"start\":91363},{\"end\":91380,\"start\":91376},{\"end\":91393,\"start\":91387},{\"end\":91412,\"start\":91403},{\"end\":91637,\"start\":91632},{\"end\":91655,\"start\":91649},{\"end\":91671,\"start\":91664},{\"end\":91682,\"start\":91676},{\"end\":91853,\"start\":91846},{\"end\":91864,\"start\":91858},{\"end\":91877,\"start\":91872},{\"end\":91888,\"start\":91885},{\"end\":92009,\"start\":92003},{\"end\":92028,\"start\":92019},{\"end\":92045,\"start\":92037},{\"end\":92058,\"start\":92053},{\"end\":92073,\"start\":92067},{\"end\":92087,\"start\":92082},{\"end\":92102,\"start\":92097},{\"end\":92278,\"start\":92275},{\"end\":92293,\"start\":92286},{\"end\":92304,\"start\":92299},{\"end\":92317,\"start\":92314},{\"end\":92753,\"start\":92752},{\"end\":92765,\"start\":92764},{\"end\":92775,\"start\":92774},{\"end\":92786,\"start\":92785},{\"end\":93149,\"start\":93148},{\"end\":93161,\"start\":93160},{\"end\":93171,\"start\":93170},{\"end\":93182,\"start\":93181},{\"end\":93268,\"start\":93262},{\"end\":93286,\"start\":93279},{\"end\":93294,\"start\":93289},{\"end\":93304,\"start\":93295},{\"end\":93324,\"start\":93315},{\"end\":93614,\"start\":93612},{\"end\":93625,\"start\":93621},{\"end\":93632,\"start\":93630},{\"end\":93642,\"start\":93637},{\"end\":93842,\"start\":93841},{\"end\":93844,\"start\":93843},{\"end\":93860,\"start\":93859},{\"end\":93862,\"start\":93861},{\"end\":94049,\"start\":94048},{\"end\":94060,\"start\":94059},{\"end\":94071,\"start\":94070},{\"end\":94074,\"start\":94072},{\"end\":94084,\"start\":94083},{\"end\":94267,\"start\":94258},{\"end\":94413,\"start\":94404},{\"end\":94553,\"start\":94544},{\"end\":94569,\"start\":94562},{\"end\":94576,\"start\":94570},{\"end\":94590,\"start\":94585},{\"end\":94755,\"start\":94751},{\"end\":94777,\"start\":94768},{\"end\":94794,\"start\":94787},{\"end\":95008,\"start\":95006},{\"end\":95017,\"start\":95014},{\"end\":95029,\"start\":95024},{\"end\":95047,\"start\":95040},{\"end\":95058,\"start\":95052},{\"end\":95068,\"start\":95064},{\"end\":95240,\"start\":95238},{\"end\":95249,\"start\":95246},{\"end\":95263,\"start\":95256},{\"end\":95271,\"start\":95268},{\"end\":95279,\"start\":95277},{\"end\":95290,\"start\":95286},{\"end\":95303,\"start\":95297},{\"end\":95513,\"start\":95507},{\"end\":95521,\"start\":95519},{\"end\":95534,\"start\":95528},{\"end\":95541,\"start\":95539},{\"end\":95701,\"start\":95695},{\"end\":95723,\"start\":95715},{\"end\":95738,\"start\":95733},{\"end\":95753,\"start\":95748},{\"end\":95901,\"start\":95896},{\"end\":95916,\"start\":95911},{\"end\":95927,\"start\":95922},{\"end\":95929,\"start\":95928},{\"end\":95943,\"start\":95935},{\"end\":96115,\"start\":96114},{\"end\":96127,\"start\":96123},{\"end\":96145,\"start\":96137},{\"end\":96160,\"start\":96159},{\"end\":96162,\"start\":96161},{\"end\":96173,\"start\":96172},{\"end\":96184,\"start\":96183},{\"end\":96449,\"start\":96442},{\"end\":96463,\"start\":96458},{\"end\":96480,\"start\":96471},{\"end\":96684,\"start\":96677},{\"end\":96697,\"start\":96693},{\"end\":96710,\"start\":96705},{\"end\":96727,\"start\":96718},{\"end\":96956,\"start\":96952},{\"end\":96977,\"start\":96964},{\"end\":96995,\"start\":96988},{\"end\":97011,\"start\":97004},{\"end\":97014,\"start\":97012},{\"end\":97032,\"start\":97023},{\"end\":97194,\"start\":97190},{\"end\":97209,\"start\":97202},{\"end\":97212,\"start\":97210},{\"end\":97228,\"start\":97221},{\"end\":97250,\"start\":97237},{\"end\":97270,\"start\":97261},{\"end\":97466,\"start\":97462},{\"end\":97481,\"start\":97474},{\"end\":97503,\"start\":97490},{\"end\":97521,\"start\":97514},{\"end\":97524,\"start\":97522},{\"end\":97542,\"start\":97533},{\"end\":97654,\"start\":97649},{\"end\":97672,\"start\":97662},{\"end\":97690,\"start\":97683},{\"end\":97885,\"start\":97880},{\"end\":98048,\"start\":98047},{\"end\":98057,\"start\":98056},{\"end\":98059,\"start\":98058},{\"end\":98067,\"start\":98066},{\"end\":98069,\"start\":98068},{\"end\":98086,\"start\":98085},{\"end\":98098,\"start\":98097},{\"end\":98109,\"start\":98108},{\"end\":98121,\"start\":98120},{\"end\":98130,\"start\":98129},{\"end\":98141,\"start\":98140},{\"end\":98331,\"start\":98327},{\"end\":98346,\"start\":98340},{\"end\":98365,\"start\":98358},{\"end\":98503,\"start\":98496},{\"end\":98518,\"start\":98511},{\"end\":98538,\"start\":98530},{\"end\":98558,\"start\":98550},{\"end\":98647,\"start\":98641},{\"end\":98664,\"start\":98654},{\"end\":98679,\"start\":98674},{\"end\":98697,\"start\":98691},{\"end\":98712,\"start\":98706},{\"end\":99016,\"start\":99013},{\"end\":99029,\"start\":99026},{\"end\":99046,\"start\":99037},{\"end\":99062,\"start\":99058},{\"end\":99076,\"start\":99070},{\"end\":99094,\"start\":99086},{\"end\":99261,\"start\":99254},{\"end\":99263,\"start\":99262},{\"end\":99279,\"start\":99274},{\"end\":99281,\"start\":99280},{\"end\":99415,\"start\":99409},{\"end\":99431,\"start\":99424},{\"end\":99440,\"start\":99436},{\"end\":99460,\"start\":99454},{\"end\":99474,\"start\":99468},{\"end\":99571,\"start\":99570},{\"end\":99573,\"start\":99572},{\"end\":99583,\"start\":99582},{\"end\":99585,\"start\":99584},{\"end\":99703,\"start\":99702},{\"end\":99709,\"start\":99708},{\"end\":99719,\"start\":99718},{\"end\":99726,\"start\":99725},{\"end\":99733,\"start\":99732},{\"end\":99922,\"start\":99915},{\"end\":99939,\"start\":99936},{\"end\":99949,\"start\":99944},{\"end\":99962,\"start\":99957},{\"end\":100087,\"start\":100080},{\"end\":100102,\"start\":100096},{\"end\":100209,\"start\":100203},{\"end\":100222,\"start\":100215},{\"end\":100236,\"start\":100230},{\"end\":100247,\"start\":100244},{\"end\":100401,\"start\":100394},{\"end\":100413,\"start\":100409},{\"end\":100437,\"start\":100429},{\"end\":100448,\"start\":100443},{\"end\":100642,\"start\":100636},{\"end\":100650,\"start\":100649},{\"end\":100800,\"start\":100796},{\"end\":100814,\"start\":100807},{\"end\":100829,\"start\":100825},{\"end\":100842,\"start\":100836},{\"end\":100857,\"start\":100850},{\"end\":101080,\"start\":101074},{\"end\":101094,\"start\":101091},{\"end\":101110,\"start\":101101},{\"end\":101227,\"start\":101221},{\"end\":101241,\"start\":101238},{\"end\":101257,\"start\":101248},{\"end\":101424,\"start\":101423},{\"end\":101426,\"start\":101425},{\"end\":101436,\"start\":101435},{\"end\":101447,\"start\":101446},{\"end\":101449,\"start\":101448},{\"end\":101460,\"start\":101459},{\"end\":101462,\"start\":101461},{\"end\":101476,\"start\":101475},{\"end\":101535,\"start\":101529},{\"end\":101537,\"start\":101536},{\"end\":101710,\"start\":101709},{\"end\":101721,\"start\":101720},{\"end\":101732,\"start\":101731},{\"end\":101743,\"start\":101742},{\"end\":101942,\"start\":101937},{\"end\":102068,\"start\":102063},{\"end\":102083,\"start\":102076},{\"end\":102102,\"start\":102097},{\"end\":102240,\"start\":102235},{\"end\":102255,\"start\":102248},{\"end\":102272,\"start\":102269},{\"end\":102282,\"start\":102277},{\"end\":102600,\"start\":102597},{\"end\":102613,\"start\":102607},{\"end\":102630,\"start\":102624},{\"end\":102752,\"start\":102751},{\"end\":102760,\"start\":102759},{\"end\":102762,\"start\":102761},{\"end\":102771,\"start\":102770},{\"end\":102773,\"start\":102772},{\"end\":102783,\"start\":102782},{\"end\":102785,\"start\":102784},{\"end\":102901,\"start\":102893},{\"end\":103018,\"start\":103011},{\"end\":103027,\"start\":103023},{\"end\":103036,\"start\":103032},{\"end\":103052,\"start\":103044},{\"end\":103068,\"start\":103059},{\"end\":103079,\"start\":103074},{\"end\":103274,\"start\":103269},{\"end\":103282,\"start\":103279},{\"end\":103298,\"start\":103290},{\"end\":103310,\"start\":103304},{\"end\":103319,\"start\":103316},{\"end\":103471,\"start\":103465},{\"end\":103481,\"start\":103478},{\"end\":103493,\"start\":103488},{\"end\":103503,\"start\":103499},{\"end\":103519,\"start\":103511},{\"end\":103530,\"start\":103525},{\"end\":103674,\"start\":103673},{\"end\":103682,\"start\":103681},{\"end\":103690,\"start\":103689},{\"end\":103696,\"start\":103695},{\"end\":103702,\"start\":103701},{\"end\":103710,\"start\":103709},{\"end\":103815,\"start\":103813},{\"end\":103826,\"start\":103822},{\"end\":103837,\"start\":103831},{\"end\":103845,\"start\":103842},{\"end\":103853,\"start\":103850},{\"end\":104044,\"start\":104038},{\"end\":104064,\"start\":104063},{\"end\":104071,\"start\":104065},{\"end\":104087,\"start\":104082},{\"end\":104175,\"start\":104168},{\"end\":104186,\"start\":104180},{\"end\":104396,\"start\":104392},{\"end\":104408,\"start\":104405},{\"end\":104419,\"start\":104414},{\"end\":104435,\"start\":104429},{\"end\":104580,\"start\":104576},{\"end\":104594,\"start\":104589},{\"end\":104607,\"start\":104604},{\"end\":104619,\"start\":104613},{\"end\":104741,\"start\":104734},{\"end\":104756,\"start\":104749},{\"end\":104770,\"start\":104764},{\"end\":104772,\"start\":104771},{\"end\":104783,\"start\":104780},{\"end\":104801,\"start\":104795},{\"end\":104986,\"start\":104981},{\"end\":104996,\"start\":104994},{\"end\":105009,\"start\":105003},{\"end\":105201,\"start\":105197},{\"end\":105214,\"start\":105207},{\"end\":105229,\"start\":105221},{\"end\":105240,\"start\":105234}]", "bib_author_last_name": "[{\"end\":87217,\"start\":87213},{\"end\":87238,\"start\":87230},{\"end\":87251,\"start\":87245},{\"end\":87610,\"start\":87600},{\"end\":87629,\"start\":87620},{\"end\":87651,\"start\":87643},{\"end\":87673,\"start\":87666},{\"end\":87689,\"start\":87682},{\"end\":87827,\"start\":87817},{\"end\":87849,\"start\":87841},{\"end\":87865,\"start\":87858},{\"end\":88057,\"start\":88050},{\"end\":88069,\"start\":88061},{\"end\":88205,\"start\":88198},{\"end\":88385,\"start\":88378},{\"end\":88395,\"start\":88389},{\"end\":88407,\"start\":88399},{\"end\":88631,\"start\":88624},{\"end\":88640,\"start\":88635},{\"end\":88652,\"start\":88646},{\"end\":88665,\"start\":88658},{\"end\":88681,\"start\":88671},{\"end\":88693,\"start\":88685},{\"end\":88823,\"start\":88816},{\"end\":88831,\"start\":88827},{\"end\":88843,\"start\":88835},{\"end\":89030,\"start\":89024},{\"end\":89044,\"start\":89034},{\"end\":89056,\"start\":89048},{\"end\":89306,\"start\":89301},{\"end\":89324,\"start\":89318},{\"end\":89345,\"start\":89339},{\"end\":89361,\"start\":89354},{\"end\":89377,\"start\":89372},{\"end\":89520,\"start\":89512},{\"end\":89529,\"start\":89524},{\"end\":89539,\"start\":89533},{\"end\":89549,\"start\":89543},{\"end\":89703,\"start\":89694},{\"end\":89838,\"start\":89829},{\"end\":89860,\"start\":89850},{\"end\":89878,\"start\":89869},{\"end\":89900,\"start\":89893},{\"end\":89916,\"start\":89908},{\"end\":89938,\"start\":89931},{\"end\":89957,\"start\":89947},{\"end\":89974,\"start\":89965},{\"end\":89991,\"start\":89982},{\"end\":90372,\"start\":90365},{\"end\":90394,\"start\":90387},{\"end\":90412,\"start\":90404},{\"end\":90425,\"start\":90420},{\"end\":90584,\"start\":90577},{\"end\":90600,\"start\":90594},{\"end\":90623,\"start\":90614},{\"end\":90639,\"start\":90632},{\"end\":90784,\"start\":90781},{\"end\":90793,\"start\":90789},{\"end\":90805,\"start\":90803},{\"end\":90814,\"start\":90811},{\"end\":90822,\"start\":90820},{\"end\":90832,\"start\":90829},{\"end\":91033,\"start\":91026},{\"end\":91050,\"start\":91042},{\"end\":91241,\"start\":91231},{\"end\":91255,\"start\":91250},{\"end\":91346,\"start\":91343},{\"end\":91361,\"start\":91356},{\"end\":91374,\"start\":91369},{\"end\":91385,\"start\":91381},{\"end\":91401,\"start\":91394},{\"end\":91419,\"start\":91413},{\"end\":91647,\"start\":91638},{\"end\":91662,\"start\":91656},{\"end\":91674,\"start\":91672},{\"end\":91856,\"start\":91854},{\"end\":91870,\"start\":91865},{\"end\":91883,\"start\":91878},{\"end\":91891,\"start\":91889},{\"end\":92017,\"start\":92010},{\"end\":92035,\"start\":92029},{\"end\":92051,\"start\":92046},{\"end\":92065,\"start\":92059},{\"end\":92080,\"start\":92074},{\"end\":92095,\"start\":92088},{\"end\":92111,\"start\":92103},{\"end\":92284,\"start\":92279},{\"end\":92297,\"start\":92294},{\"end\":92312,\"start\":92305},{\"end\":92323,\"start\":92318},{\"end\":92762,\"start\":92754},{\"end\":92772,\"start\":92766},{\"end\":92783,\"start\":92776},{\"end\":92794,\"start\":92787},{\"end\":92918,\"start\":92910},{\"end\":92926,\"start\":92920},{\"end\":92935,\"start\":92928},{\"end\":92944,\"start\":92937},{\"end\":93158,\"start\":93150},{\"end\":93168,\"start\":93162},{\"end\":93179,\"start\":93172},{\"end\":93190,\"start\":93183},{\"end\":93277,\"start\":93269},{\"end\":93313,\"start\":93305},{\"end\":93331,\"start\":93325},{\"end\":93619,\"start\":93615},{\"end\":93628,\"start\":93626},{\"end\":93635,\"start\":93633},{\"end\":93651,\"start\":93643},{\"end\":93857,\"start\":93845},{\"end\":93872,\"start\":93863},{\"end\":94057,\"start\":94050},{\"end\":94068,\"start\":94061},{\"end\":94081,\"start\":94075},{\"end\":94090,\"start\":94085},{\"end\":94274,\"start\":94268},{\"end\":94420,\"start\":94414},{\"end\":94560,\"start\":94554},{\"end\":94583,\"start\":94577},{\"end\":94595,\"start\":94591},{\"end\":94766,\"start\":94756},{\"end\":94785,\"start\":94778},{\"end\":94803,\"start\":94795},{\"end\":95012,\"start\":95009},{\"end\":95022,\"start\":95018},{\"end\":95038,\"start\":95030},{\"end\":95050,\"start\":95048},{\"end\":95062,\"start\":95059},{\"end\":95073,\"start\":95069},{\"end\":95244,\"start\":95241},{\"end\":95254,\"start\":95250},{\"end\":95266,\"start\":95264},{\"end\":95275,\"start\":95272},{\"end\":95284,\"start\":95280},{\"end\":95295,\"start\":95291},{\"end\":95307,\"start\":95304},{\"end\":95517,\"start\":95514},{\"end\":95526,\"start\":95522},{\"end\":95537,\"start\":95535},{\"end\":95546,\"start\":95542},{\"end\":95713,\"start\":95702},{\"end\":95731,\"start\":95724},{\"end\":95746,\"start\":95739},{\"end\":95760,\"start\":95754},{\"end\":95909,\"start\":95902},{\"end\":95920,\"start\":95917},{\"end\":95933,\"start\":95930},{\"end\":95950,\"start\":95944},{\"end\":95960,\"start\":95952},{\"end\":96121,\"start\":96116},{\"end\":96135,\"start\":96128},{\"end\":96157,\"start\":96146},{\"end\":96170,\"start\":96163},{\"end\":96181,\"start\":96174},{\"end\":96189,\"start\":96185},{\"end\":96196,\"start\":96191},{\"end\":96456,\"start\":96450},{\"end\":96469,\"start\":96464},{\"end\":96487,\"start\":96481},{\"end\":96691,\"start\":96685},{\"end\":96703,\"start\":96698},{\"end\":96716,\"start\":96711},{\"end\":96734,\"start\":96728},{\"end\":96962,\"start\":96957},{\"end\":96986,\"start\":96978},{\"end\":97002,\"start\":96996},{\"end\":97021,\"start\":97015},{\"end\":97039,\"start\":97033},{\"end\":97200,\"start\":97195},{\"end\":97219,\"start\":97213},{\"end\":97235,\"start\":97229},{\"end\":97259,\"start\":97251},{\"end\":97277,\"start\":97271},{\"end\":97472,\"start\":97467},{\"end\":97488,\"start\":97482},{\"end\":97512,\"start\":97504},{\"end\":97531,\"start\":97525},{\"end\":97549,\"start\":97543},{\"end\":97660,\"start\":97655},{\"end\":97681,\"start\":97673},{\"end\":97698,\"start\":97691},{\"end\":97889,\"start\":97886},{\"end\":97896,\"start\":97891},{\"end\":97902,\"start\":97898},{\"end\":98054,\"start\":98049},{\"end\":98064,\"start\":98060},{\"end\":98083,\"start\":98070},{\"end\":98095,\"start\":98087},{\"end\":98106,\"start\":98099},{\"end\":98118,\"start\":98110},{\"end\":98127,\"start\":98122},{\"end\":98138,\"start\":98131},{\"end\":98150,\"start\":98142},{\"end\":98338,\"start\":98332},{\"end\":98356,\"start\":98347},{\"end\":98371,\"start\":98366},{\"end\":98509,\"start\":98504},{\"end\":98528,\"start\":98519},{\"end\":98548,\"start\":98539},{\"end\":98565,\"start\":98559},{\"end\":98652,\"start\":98648},{\"end\":98672,\"start\":98665},{\"end\":98689,\"start\":98680},{\"end\":98704,\"start\":98698},{\"end\":98721,\"start\":98713},{\"end\":99024,\"start\":99017},{\"end\":99035,\"start\":99030},{\"end\":99056,\"start\":99047},{\"end\":99068,\"start\":99063},{\"end\":99084,\"start\":99077},{\"end\":99100,\"start\":99095},{\"end\":99272,\"start\":99264},{\"end\":99291,\"start\":99282},{\"end\":99422,\"start\":99416},{\"end\":99434,\"start\":99432},{\"end\":99452,\"start\":99441},{\"end\":99466,\"start\":99461},{\"end\":99481,\"start\":99475},{\"end\":99580,\"start\":99574},{\"end\":99591,\"start\":99586},{\"end\":99706,\"start\":99704},{\"end\":99716,\"start\":99710},{\"end\":99723,\"start\":99720},{\"end\":99730,\"start\":99727},{\"end\":99738,\"start\":99734},{\"end\":99934,\"start\":99923},{\"end\":99942,\"start\":99940},{\"end\":99955,\"start\":99950},{\"end\":99968,\"start\":99963},{\"end\":100094,\"start\":100088},{\"end\":100111,\"start\":100103},{\"end\":100213,\"start\":100210},{\"end\":100228,\"start\":100223},{\"end\":100242,\"start\":100237},{\"end\":100250,\"start\":100248},{\"end\":100407,\"start\":100402},{\"end\":100427,\"start\":100414},{\"end\":100441,\"start\":100438},{\"end\":100454,\"start\":100449},{\"end\":100647,\"start\":100643},{\"end\":100658,\"start\":100651},{\"end\":100805,\"start\":100801},{\"end\":100823,\"start\":100815},{\"end\":100834,\"start\":100830},{\"end\":100848,\"start\":100843},{\"end\":100863,\"start\":100858},{\"end\":101089,\"start\":101081},{\"end\":101099,\"start\":101095},{\"end\":101119,\"start\":101111},{\"end\":101236,\"start\":101228},{\"end\":101246,\"start\":101242},{\"end\":101266,\"start\":101258},{\"end\":101433,\"start\":101427},{\"end\":101444,\"start\":101437},{\"end\":101457,\"start\":101450},{\"end\":101473,\"start\":101463},{\"end\":101485,\"start\":101477},{\"end\":101545,\"start\":101538},{\"end\":101718,\"start\":101711},{\"end\":101729,\"start\":101722},{\"end\":101740,\"start\":101733},{\"end\":101750,\"start\":101744},{\"end\":101947,\"start\":101943},{\"end\":102074,\"start\":102069},{\"end\":102095,\"start\":102084},{\"end\":102108,\"start\":102103},{\"end\":102246,\"start\":102241},{\"end\":102267,\"start\":102256},{\"end\":102275,\"start\":102273},{\"end\":102288,\"start\":102283},{\"end\":102605,\"start\":102601},{\"end\":102622,\"start\":102614},{\"end\":102641,\"start\":102631},{\"end\":102757,\"start\":102753},{\"end\":102768,\"start\":102763},{\"end\":102780,\"start\":102774},{\"end\":102796,\"start\":102786},{\"end\":102908,\"start\":102902},{\"end\":103021,\"start\":103019},{\"end\":103030,\"start\":103028},{\"end\":103042,\"start\":103037},{\"end\":103057,\"start\":103053},{\"end\":103072,\"start\":103069},{\"end\":103083,\"start\":103080},{\"end\":103277,\"start\":103275},{\"end\":103288,\"start\":103283},{\"end\":103302,\"start\":103299},{\"end\":103314,\"start\":103311},{\"end\":103325,\"start\":103320},{\"end\":103476,\"start\":103472},{\"end\":103486,\"start\":103482},{\"end\":103497,\"start\":103494},{\"end\":103509,\"start\":103504},{\"end\":103523,\"start\":103520},{\"end\":103534,\"start\":103531},{\"end\":103679,\"start\":103675},{\"end\":103687,\"start\":103683},{\"end\":103693,\"start\":103691},{\"end\":103699,\"start\":103697},{\"end\":103707,\"start\":103703},{\"end\":103714,\"start\":103711},{\"end\":103820,\"start\":103816},{\"end\":103829,\"start\":103827},{\"end\":103840,\"start\":103838},{\"end\":103848,\"start\":103846},{\"end\":103857,\"start\":103854},{\"end\":104061,\"start\":104045},{\"end\":104080,\"start\":104072},{\"end\":104093,\"start\":104088},{\"end\":104178,\"start\":104176},{\"end\":104194,\"start\":104187},{\"end\":104403,\"start\":104397},{\"end\":104412,\"start\":104409},{\"end\":104427,\"start\":104420},{\"end\":104442,\"start\":104436},{\"end\":104587,\"start\":104581},{\"end\":104602,\"start\":104595},{\"end\":104611,\"start\":104608},{\"end\":104626,\"start\":104620},{\"end\":104747,\"start\":104742},{\"end\":104762,\"start\":104757},{\"end\":104778,\"start\":104773},{\"end\":104793,\"start\":104784},{\"end\":104806,\"start\":104802},{\"end\":104992,\"start\":104987},{\"end\":105001,\"start\":104997},{\"end\":105012,\"start\":105010},{\"end\":105205,\"start\":105202},{\"end\":105219,\"start\":105215},{\"end\":105232,\"start\":105230},{\"end\":105246,\"start\":105241}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1145/3423328.3423498\",\"id\":\"b0\",\"matched_paper_id\":222413596},\"end\":87467,\"start\":87118},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":214035605},\"end\":87725,\"start\":87469},{\"attributes\":{\"doi\":\"10.1109/ICIP.2017.8296382\",\"id\":\"b2\",\"matched_paper_id\":1349264},\"end\":87965,\"start\":87727},{\"attributes\":{\"doi\":\"10.1109/ICME.2018.8486512\",\"id\":\"b3\",\"matched_paper_id\":26938964},\"end\":88291,\"start\":87967},{\"attributes\":{\"doi\":\"10.1109/MMSP.2017.8122237\",\"id\":\"b4\",\"matched_paper_id\":19274400},\"end\":88521,\"start\":88293},{\"attributes\":{\"doi\":\"10.1109/QoMEX48832.2020.9123121\",\"id\":\"b5\",\"matched_paper_id\":218550199},\"end\":88950,\"start\":88523},{\"attributes\":{\"doi\":\"10.1109/ICME.2002.1035879\",\"id\":\"b6\",\"matched_paper_id\":3132146},\"end\":89206,\"start\":88952},{\"attributes\":{\"doi\":\"10.1109/TIP.2017.2760518\",\"id\":\"b7\",\"matched_paper_id\":7885416},\"end\":89454,\"start\":89208},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":21565866},\"end\":89593,\"start\":89456},{\"attributes\":{\"doi\":\"10.1117/1.JEI.27.4.043048\",\"id\":\"b9\",\"matched_paper_id\":52166370},\"end\":89773,\"start\":89595},{\"attributes\":{\"doi\":\"10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136\",\"id\":\"b10\",\"matched_paper_id\":1866174},\"end\":90309,\"start\":89775},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9630004},\"end\":90471,\"start\":90311},{\"attributes\":{\"doi\":\"10.1016/j.image.2009.06.005\",\"id\":\"b12\",\"matched_paper_id\":35283477},\"end\":90720,\"start\":90473},{\"attributes\":{\"doi\":\"10.1016/j.neucom.2017.01.054\",\"id\":\"b13\",\"matched_paper_id\":828879},\"end\":90943,\"start\":90722},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":12319407},\"end\":91143,\"start\":90945},{\"attributes\":{\"doi\":\"10.1109/TIP.2015.2500021\",\"id\":\"b15\",\"matched_paper_id\":8531217},\"end\":91332,\"start\":91145},{\"attributes\":{\"doi\":\"10.1145/2996296\",\"id\":\"b16\"},\"end\":91525,\"start\":91334},{\"attributes\":{\"doi\":\"10.2352/ISSN.2470-1173.2020.11.HVEI-128\",\"id\":\"b17\"},\"end\":91740,\"start\":91527},{\"attributes\":{\"doi\":\"10.1109/ICIP42928.2021.9506762\",\"id\":\"b18\",\"matched_paper_id\":238686125},\"end\":92001,\"start\":91742},{\"attributes\":{\"doi\":\"10.1109/TMM.2013.2291663\",\"id\":\"b19\"},\"end\":92221,\"start\":92003},{\"attributes\":{\"doi\":\"10.1007/978-3-030-01219-9_11\",\"id\":\"b20\"},\"end\":92369,\"start\":92223},{\"attributes\":{\"doi\":\"ITU-R BT.500-13. 2012\",\"id\":\"b21\"},\"end\":92545,\"start\":92371},{\"attributes\":{\"doi\":\"ITU-T P.910\",\"id\":\"b22\"},\"end\":92678,\"start\":92547},{\"attributes\":{\"doi\":\"10.1109/MMSP.2017.8122239\",\"id\":\"b23\",\"matched_paper_id\":7405964},\"end\":92908,\"start\":92680},{\"attributes\":{\"doi\":\"arXiv:1912.09137\",\"id\":\"b24\"},\"end\":93054,\"start\":92910},{\"attributes\":{\"doi\":\"10.1109/LSP.2020.3010128\",\"id\":\"b25\",\"matched_paper_id\":221159216},\"end\":93260,\"start\":93056},{\"attributes\":{\"doi\":\"10.1109/QoMEX.2018.8463298\",\"id\":\"b26\"},\"end\":93537,\"start\":93262},{\"attributes\":{\"doi\":\"10.1109/CVPR.2014.224\",\"id\":\"b27\",\"matched_paper_id\":11654786},\"end\":93750,\"start\":93539},{\"attributes\":{\"doi\":\"10.1109/83.388074\",\"id\":\"b28\",\"matched_paper_id\":20194163},\"end\":93941,\"start\":93752},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":19411829},\"end\":94177,\"start\":93943},{\"attributes\":{\"doi\":\"10.1145/1462048.1462052\",\"id\":\"b30\",\"matched_paper_id\":16577037},\"end\":94343,\"start\":94179},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":16657222},\"end\":94459,\"start\":94345},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9490200},\"end\":94678,\"start\":94461},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":247476810},\"end\":94887,\"start\":94680},{\"attributes\":{\"doi\":\"10.1109/TIP.2021.3096060\",\"id\":\"b34\",\"matched_paper_id\":227162585},\"end\":95151,\"start\":94889},{\"attributes\":{\"doi\":\"10.1109/TCSVT.2021.3100282\",\"id\":\"b35\",\"matched_paper_id\":237736233},\"end\":95410,\"start\":95153},{\"attributes\":{\"doi\":\"arXiv:2012.11895\",\"id\":\"b36\"},\"end\":95583,\"start\":95412},{\"attributes\":{\"doi\":\"ArXiv:2004.14753\",\"id\":\"b37\"},\"end\":95788,\"start\":95585},{\"attributes\":{\"doi\":\"10.1145/2010324.1964935\",\"id\":\"b38\",\"matched_paper_id\":756729},\"end\":96024,\"start\":95790},{\"attributes\":{\"doi\":\"10.1111/j.1467-8659.2012.03188.x\",\"id\":\"b39\",\"matched_paper_id\":6272402},\"end\":96393,\"start\":96026},{\"attributes\":{\"doi\":\"10.1109/QoMEX.2019.8743313\",\"id\":\"b40\",\"matched_paper_id\":195698263},\"end\":96608,\"start\":96395},{\"attributes\":{\"doi\":\"10.1109/QoMEX48832.2020.9123147\",\"id\":\"b41\",\"matched_paper_id\":215535683},\"end\":96859,\"start\":96610},{\"attributes\":{\"doi\":\"10.1145/3427931\",\"id\":\"b42\",\"matched_paper_id\":231653865},\"end\":97114,\"start\":96861},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":247246611},\"end\":97355,\"start\":97116},{\"attributes\":{\"doi\":\"10.1109/TVCG.2020.3036153\",\"id\":\"b44\",\"matched_paper_id\":226295171},\"end\":97647,\"start\":97357},{\"attributes\":{\"id\":\"b45\"},\"end\":97807,\"start\":97649},{\"attributes\":{\"doi\":\"10.1109/TMM.2005.843364\",\"id\":\"b46\",\"matched_paper_id\":6591149},\"end\":97976,\"start\":97809},{\"attributes\":{\"doi\":\"10.1109/ICIP40778.2020.9191308\",\"id\":\"b47\",\"matched_paper_id\":224921484},\"end\":98260,\"start\":97978},{\"attributes\":{\"doi\":\"10.1109/TIP.2014.2302684\",\"id\":\"b48\",\"matched_paper_id\":17037200},\"end\":98448,\"start\":98262},{\"attributes\":{\"doi\":\"10.2352/ISSN.2470-1173.2021.9.IQSP-257\",\"id\":\"b49\",\"matched_paper_id\":232046279},\"end\":98639,\"start\":98450},{\"attributes\":{\"doi\":\"10.1145/2810188.2810194\",\"id\":\"b50\"},\"end\":98939,\"start\":98641},{\"attributes\":{\"doi\":\"10.2312/eged.20211000\",\"id\":\"b51\",\"matched_paper_id\":244986172},\"end\":99170,\"start\":98941},{\"attributes\":{\"doi\":\"10.1117/12.429504\",\"id\":\"b52\",\"matched_paper_id\":16399426},\"end\":99364,\"start\":99172},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":6656545},\"end\":99530,\"start\":99366},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":3716103},\"end\":99650,\"start\":99532},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":202788024},\"end\":99807,\"start\":99652},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":218597250},\"end\":100047,\"start\":99809},{\"attributes\":{\"doi\":\"10.1109/TIP.2018.2831899\",\"id\":\"b57\",\"matched_paper_id\":13911460},\"end\":100201,\"start\":100049},{\"attributes\":{\"id\":\"b58\"},\"end\":100392,\"start\":100203},{\"attributes\":{\"id\":\"b59\"},\"end\":100558,\"start\":100394},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":15480121},\"end\":100736,\"start\":100560},{\"attributes\":{\"doi\":\"10.1109/ICIP.2017.8296925\",\"id\":\"b61\",\"matched_paper_id\":3471304},\"end\":100968,\"start\":100738},{\"attributes\":{\"id\":\"b62\"},\"end\":101133,\"start\":100970},{\"attributes\":{\"doi\":\"10.1016/j.image.2014.12.008\",\"id\":\"b63\",\"matched_paper_id\":14589990},\"end\":101351,\"start\":101135},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":53399730},\"end\":101619,\"start\":101353},{\"attributes\":{\"doi\":\"10.1145/3129505\",\"id\":\"b65\",\"matched_paper_id\":1640517},\"end\":101820,\"start\":101621},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":3286512},\"end\":101996,\"start\":101822},{\"attributes\":{\"doi\":\"10.1109/QoMEX48832.2020.9123089\",\"id\":\"b67\",\"matched_paper_id\":216033693},\"end\":102233,\"start\":101998},{\"attributes\":{\"doi\":\"arXiv:2201.07701[cs.MM]\",\"id\":\"b68\"},\"end\":102413,\"start\":102235},{\"attributes\":{\"id\":\"b69\"},\"end\":102518,\"start\":102415},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":15637365},\"end\":102675,\"start\":102520},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":207761262},\"end\":102849,\"start\":102677},{\"attributes\":{\"id\":\"b72\"},\"end\":102932,\"start\":102851},{\"attributes\":{\"doi\":\"10.1109/TIP.2020.3002478\",\"id\":\"b73\",\"matched_paper_id\":220568805},\"end\":103160,\"start\":102934},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":236905980},\"end\":103403,\"start\":103162},{\"attributes\":{\"doi\":\"10.1007/978-3-030-01249-6_13\",\"id\":\"b75\"},\"end\":103580,\"start\":103405},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":226631211},\"end\":103759,\"start\":103582},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2020.3047083\",\"id\":\"b77\",\"matched_paper_id\":219177378},\"end\":103959,\"start\":103761},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":52109770},\"end\":104124,\"start\":103961},{\"attributes\":{\"doi\":\"10.1109/QoMEX.2013.6603194\",\"id\":\"b79\",\"matched_paper_id\":2677022},\"end\":104312,\"start\":104126},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":53684071},\"end\":104482,\"start\":104314},{\"attributes\":{\"doi\":\"10.1109/CVPR.2018.00068\",\"id\":\"b81\",\"matched_paper_id\":215752296},\"end\":104903,\"start\":104484},{\"attributes\":{\"doi\":\"10.1145/3474085.3475294\",\"id\":\"b82\"},\"end\":105104,\"start\":104905},{\"attributes\":{\"doi\":\"10.1016/j.cag.2009.10.004\",\"id\":\"b83\",\"matched_paper_id\":774457},\"end\":105307,\"start\":105106}]", "bib_title": "[{\"end\":87209,\"start\":87118},{\"end\":87591,\"start\":87469},{\"end\":87808,\"start\":87727},{\"end\":88046,\"start\":87967},{\"end\":88374,\"start\":88293},{\"end\":88620,\"start\":88523},{\"end\":89020,\"start\":88952},{\"end\":89289,\"start\":89208},{\"end\":89508,\"start\":89456},{\"end\":89690,\"start\":89595},{\"end\":89819,\"start\":89775},{\"end\":90350,\"start\":90311},{\"end\":90568,\"start\":90473},{\"end\":90775,\"start\":90722},{\"end\":91016,\"start\":90945},{\"end\":91222,\"start\":91145},{\"end\":91844,\"start\":91742},{\"end\":92750,\"start\":92680},{\"end\":93146,\"start\":93056},{\"end\":93610,\"start\":93539},{\"end\":93839,\"start\":93752},{\"end\":94046,\"start\":93943},{\"end\":94256,\"start\":94179},{\"end\":94402,\"start\":94345},{\"end\":94542,\"start\":94461},{\"end\":94749,\"start\":94680},{\"end\":95004,\"start\":94889},{\"end\":95236,\"start\":95153},{\"end\":95894,\"start\":95790},{\"end\":96112,\"start\":96026},{\"end\":96440,\"start\":96395},{\"end\":96675,\"start\":96610},{\"end\":96950,\"start\":96861},{\"end\":97188,\"start\":97116},{\"end\":97460,\"start\":97357},{\"end\":97878,\"start\":97809},{\"end\":98045,\"start\":97978},{\"end\":98325,\"start\":98262},{\"end\":98494,\"start\":98450},{\"end\":99011,\"start\":98941},{\"end\":99252,\"start\":99172},{\"end\":99407,\"start\":99366},{\"end\":99568,\"start\":99532},{\"end\":99700,\"start\":99652},{\"end\":99913,\"start\":99809},{\"end\":100078,\"start\":100049},{\"end\":100634,\"start\":100560},{\"end\":100794,\"start\":100738},{\"end\":101219,\"start\":101135},{\"end\":101421,\"start\":101353},{\"end\":101707,\"start\":101621},{\"end\":101935,\"start\":101822},{\"end\":102061,\"start\":101998},{\"end\":102595,\"start\":102520},{\"end\":102749,\"start\":102677},{\"end\":103009,\"start\":102934},{\"end\":103267,\"start\":103162},{\"end\":103671,\"start\":103582},{\"end\":103811,\"start\":103761},{\"end\":104036,\"start\":103961},{\"end\":104166,\"start\":104126},{\"end\":104390,\"start\":104314},{\"end\":104574,\"start\":104484},{\"end\":105195,\"start\":105106}]", "bib_author": "[{\"end\":87219,\"start\":87211},{\"end\":87240,\"start\":87219},{\"end\":87253,\"start\":87240},{\"end\":87612,\"start\":87593},{\"end\":87631,\"start\":87612},{\"end\":87653,\"start\":87631},{\"end\":87662,\"start\":87653},{\"end\":87675,\"start\":87662},{\"end\":87691,\"start\":87675},{\"end\":87829,\"start\":87810},{\"end\":87851,\"start\":87829},{\"end\":87867,\"start\":87851},{\"end\":88059,\"start\":88048},{\"end\":88071,\"start\":88059},{\"end\":88387,\"start\":88376},{\"end\":88397,\"start\":88387},{\"end\":88409,\"start\":88397},{\"end\":88633,\"start\":88622},{\"end\":88642,\"start\":88633},{\"end\":88654,\"start\":88642},{\"end\":88667,\"start\":88654},{\"end\":88683,\"start\":88667},{\"end\":88695,\"start\":88683},{\"end\":89032,\"start\":89022},{\"end\":89046,\"start\":89032},{\"end\":89058,\"start\":89046},{\"end\":89308,\"start\":89291},{\"end\":89326,\"start\":89308},{\"end\":89347,\"start\":89326},{\"end\":89363,\"start\":89347},{\"end\":89379,\"start\":89363},{\"end\":89522,\"start\":89510},{\"end\":89531,\"start\":89522},{\"end\":89541,\"start\":89531},{\"end\":89551,\"start\":89541},{\"end\":89705,\"start\":89692},{\"end\":89840,\"start\":89821},{\"end\":89862,\"start\":89840},{\"end\":89880,\"start\":89862},{\"end\":89902,\"start\":89880},{\"end\":89918,\"start\":89902},{\"end\":89940,\"start\":89918},{\"end\":89959,\"start\":89940},{\"end\":89976,\"start\":89959},{\"end\":89993,\"start\":89976},{\"end\":90374,\"start\":90352},{\"end\":90396,\"start\":90374},{\"end\":90414,\"start\":90396},{\"end\":90427,\"start\":90414},{\"end\":90586,\"start\":90570},{\"end\":90602,\"start\":90586},{\"end\":90625,\"start\":90602},{\"end\":90641,\"start\":90625},{\"end\":90786,\"start\":90777},{\"end\":90795,\"start\":90786},{\"end\":90807,\"start\":90795},{\"end\":90816,\"start\":90807},{\"end\":90824,\"start\":90816},{\"end\":90834,\"start\":90824},{\"end\":91035,\"start\":91018},{\"end\":91052,\"start\":91035},{\"end\":91243,\"start\":91224},{\"end\":91257,\"start\":91243},{\"end\":91348,\"start\":91334},{\"end\":91363,\"start\":91348},{\"end\":91376,\"start\":91363},{\"end\":91387,\"start\":91376},{\"end\":91403,\"start\":91387},{\"end\":91421,\"start\":91403},{\"end\":91649,\"start\":91632},{\"end\":91664,\"start\":91649},{\"end\":91676,\"start\":91664},{\"end\":91685,\"start\":91676},{\"end\":91858,\"start\":91846},{\"end\":91872,\"start\":91858},{\"end\":91885,\"start\":91872},{\"end\":91893,\"start\":91885},{\"end\":92019,\"start\":92003},{\"end\":92037,\"start\":92019},{\"end\":92053,\"start\":92037},{\"end\":92067,\"start\":92053},{\"end\":92082,\"start\":92067},{\"end\":92097,\"start\":92082},{\"end\":92113,\"start\":92097},{\"end\":92286,\"start\":92275},{\"end\":92299,\"start\":92286},{\"end\":92314,\"start\":92299},{\"end\":92325,\"start\":92314},{\"end\":92764,\"start\":92752},{\"end\":92774,\"start\":92764},{\"end\":92785,\"start\":92774},{\"end\":92796,\"start\":92785},{\"end\":92920,\"start\":92910},{\"end\":92928,\"start\":92920},{\"end\":92937,\"start\":92928},{\"end\":92946,\"start\":92937},{\"end\":93160,\"start\":93148},{\"end\":93170,\"start\":93160},{\"end\":93181,\"start\":93170},{\"end\":93192,\"start\":93181},{\"end\":93279,\"start\":93262},{\"end\":93289,\"start\":93279},{\"end\":93315,\"start\":93289},{\"end\":93333,\"start\":93315},{\"end\":93621,\"start\":93612},{\"end\":93630,\"start\":93621},{\"end\":93637,\"start\":93630},{\"end\":93653,\"start\":93637},{\"end\":93859,\"start\":93841},{\"end\":93874,\"start\":93859},{\"end\":94059,\"start\":94048},{\"end\":94070,\"start\":94059},{\"end\":94083,\"start\":94070},{\"end\":94092,\"start\":94083},{\"end\":94276,\"start\":94258},{\"end\":94422,\"start\":94404},{\"end\":94562,\"start\":94544},{\"end\":94585,\"start\":94562},{\"end\":94597,\"start\":94585},{\"end\":94768,\"start\":94751},{\"end\":94787,\"start\":94768},{\"end\":94805,\"start\":94787},{\"end\":95014,\"start\":95006},{\"end\":95024,\"start\":95014},{\"end\":95040,\"start\":95024},{\"end\":95052,\"start\":95040},{\"end\":95064,\"start\":95052},{\"end\":95075,\"start\":95064},{\"end\":95246,\"start\":95238},{\"end\":95256,\"start\":95246},{\"end\":95268,\"start\":95256},{\"end\":95277,\"start\":95268},{\"end\":95286,\"start\":95277},{\"end\":95297,\"start\":95286},{\"end\":95309,\"start\":95297},{\"end\":95519,\"start\":95507},{\"end\":95528,\"start\":95519},{\"end\":95539,\"start\":95528},{\"end\":95548,\"start\":95539},{\"end\":95715,\"start\":95695},{\"end\":95733,\"start\":95715},{\"end\":95748,\"start\":95733},{\"end\":95762,\"start\":95748},{\"end\":95911,\"start\":95896},{\"end\":95922,\"start\":95911},{\"end\":95935,\"start\":95922},{\"end\":95952,\"start\":95935},{\"end\":95962,\"start\":95952},{\"end\":96123,\"start\":96114},{\"end\":96137,\"start\":96123},{\"end\":96159,\"start\":96137},{\"end\":96172,\"start\":96159},{\"end\":96183,\"start\":96172},{\"end\":96191,\"start\":96183},{\"end\":96198,\"start\":96191},{\"end\":96458,\"start\":96442},{\"end\":96471,\"start\":96458},{\"end\":96489,\"start\":96471},{\"end\":96693,\"start\":96677},{\"end\":96705,\"start\":96693},{\"end\":96718,\"start\":96705},{\"end\":96736,\"start\":96718},{\"end\":96964,\"start\":96952},{\"end\":96988,\"start\":96964},{\"end\":97004,\"start\":96988},{\"end\":97023,\"start\":97004},{\"end\":97041,\"start\":97023},{\"end\":97202,\"start\":97190},{\"end\":97221,\"start\":97202},{\"end\":97237,\"start\":97221},{\"end\":97261,\"start\":97237},{\"end\":97279,\"start\":97261},{\"end\":97474,\"start\":97462},{\"end\":97490,\"start\":97474},{\"end\":97514,\"start\":97490},{\"end\":97533,\"start\":97514},{\"end\":97551,\"start\":97533},{\"end\":97662,\"start\":97649},{\"end\":97683,\"start\":97662},{\"end\":97700,\"start\":97683},{\"end\":97891,\"start\":97880},{\"end\":97898,\"start\":97891},{\"end\":97904,\"start\":97898},{\"end\":98056,\"start\":98047},{\"end\":98066,\"start\":98056},{\"end\":98085,\"start\":98066},{\"end\":98097,\"start\":98085},{\"end\":98108,\"start\":98097},{\"end\":98120,\"start\":98108},{\"end\":98129,\"start\":98120},{\"end\":98140,\"start\":98129},{\"end\":98152,\"start\":98140},{\"end\":98340,\"start\":98327},{\"end\":98358,\"start\":98340},{\"end\":98373,\"start\":98358},{\"end\":98511,\"start\":98496},{\"end\":98530,\"start\":98511},{\"end\":98550,\"start\":98530},{\"end\":98567,\"start\":98550},{\"end\":98654,\"start\":98641},{\"end\":98674,\"start\":98654},{\"end\":98691,\"start\":98674},{\"end\":98706,\"start\":98691},{\"end\":98723,\"start\":98706},{\"end\":99026,\"start\":99013},{\"end\":99037,\"start\":99026},{\"end\":99058,\"start\":99037},{\"end\":99070,\"start\":99058},{\"end\":99086,\"start\":99070},{\"end\":99102,\"start\":99086},{\"end\":99274,\"start\":99254},{\"end\":99293,\"start\":99274},{\"end\":99424,\"start\":99409},{\"end\":99436,\"start\":99424},{\"end\":99454,\"start\":99436},{\"end\":99468,\"start\":99454},{\"end\":99483,\"start\":99468},{\"end\":99582,\"start\":99570},{\"end\":99593,\"start\":99582},{\"end\":99708,\"start\":99702},{\"end\":99718,\"start\":99708},{\"end\":99725,\"start\":99718},{\"end\":99732,\"start\":99725},{\"end\":99740,\"start\":99732},{\"end\":99936,\"start\":99915},{\"end\":99944,\"start\":99936},{\"end\":99957,\"start\":99944},{\"end\":99970,\"start\":99957},{\"end\":100096,\"start\":100080},{\"end\":100113,\"start\":100096},{\"end\":100215,\"start\":100203},{\"end\":100230,\"start\":100215},{\"end\":100244,\"start\":100230},{\"end\":100252,\"start\":100244},{\"end\":100409,\"start\":100394},{\"end\":100429,\"start\":100409},{\"end\":100443,\"start\":100429},{\"end\":100456,\"start\":100443},{\"end\":100649,\"start\":100636},{\"end\":100660,\"start\":100649},{\"end\":100807,\"start\":100796},{\"end\":100825,\"start\":100807},{\"end\":100836,\"start\":100825},{\"end\":100850,\"start\":100836},{\"end\":100865,\"start\":100850},{\"end\":101091,\"start\":101074},{\"end\":101101,\"start\":101091},{\"end\":101121,\"start\":101101},{\"end\":101238,\"start\":101221},{\"end\":101248,\"start\":101238},{\"end\":101268,\"start\":101248},{\"end\":101435,\"start\":101423},{\"end\":101446,\"start\":101435},{\"end\":101459,\"start\":101446},{\"end\":101475,\"start\":101459},{\"end\":101487,\"start\":101475},{\"end\":101720,\"start\":101709},{\"end\":101731,\"start\":101720},{\"end\":101742,\"start\":101731},{\"end\":101752,\"start\":101742},{\"end\":101949,\"start\":101937},{\"end\":102076,\"start\":102063},{\"end\":102097,\"start\":102076},{\"end\":102110,\"start\":102097},{\"end\":102248,\"start\":102235},{\"end\":102269,\"start\":102248},{\"end\":102277,\"start\":102269},{\"end\":102290,\"start\":102277},{\"end\":102607,\"start\":102597},{\"end\":102624,\"start\":102607},{\"end\":102643,\"start\":102624},{\"end\":102759,\"start\":102751},{\"end\":102770,\"start\":102759},{\"end\":102782,\"start\":102770},{\"end\":102798,\"start\":102782},{\"end\":102910,\"start\":102893},{\"end\":103023,\"start\":103011},{\"end\":103032,\"start\":103023},{\"end\":103044,\"start\":103032},{\"end\":103059,\"start\":103044},{\"end\":103074,\"start\":103059},{\"end\":103085,\"start\":103074},{\"end\":103279,\"start\":103269},{\"end\":103290,\"start\":103279},{\"end\":103304,\"start\":103290},{\"end\":103316,\"start\":103304},{\"end\":103327,\"start\":103316},{\"end\":103478,\"start\":103465},{\"end\":103488,\"start\":103478},{\"end\":103499,\"start\":103488},{\"end\":103511,\"start\":103499},{\"end\":103525,\"start\":103511},{\"end\":103536,\"start\":103525},{\"end\":103681,\"start\":103673},{\"end\":103689,\"start\":103681},{\"end\":103695,\"start\":103689},{\"end\":103701,\"start\":103695},{\"end\":103709,\"start\":103701},{\"end\":103716,\"start\":103709},{\"end\":103822,\"start\":103813},{\"end\":103831,\"start\":103822},{\"end\":103842,\"start\":103831},{\"end\":103850,\"start\":103842},{\"end\":103859,\"start\":103850},{\"end\":104063,\"start\":104038},{\"end\":104082,\"start\":104063},{\"end\":104095,\"start\":104082},{\"end\":104180,\"start\":104168},{\"end\":104196,\"start\":104180},{\"end\":104405,\"start\":104392},{\"end\":104414,\"start\":104405},{\"end\":104429,\"start\":104414},{\"end\":104444,\"start\":104429},{\"end\":104589,\"start\":104576},{\"end\":104604,\"start\":104589},{\"end\":104613,\"start\":104604},{\"end\":104628,\"start\":104613},{\"end\":104994,\"start\":104981},{\"end\":105003,\"start\":104994},{\"end\":105014,\"start\":105003},{\"end\":105207,\"start\":105197},{\"end\":105221,\"start\":105207},{\"end\":105234,\"start\":105221},{\"end\":105248,\"start\":105234}]", "bib_venue": "[{\"end\":87372,\"start\":87276},{\"end\":87710,\"start\":87691},{\"end\":87947,\"start\":87892},{\"end\":88187,\"start\":88096},{\"end\":88515,\"start\":88434},{\"end\":88812,\"start\":88726},{\"end\":89148,\"start\":89083},{\"end\":89440,\"start\":89403},{\"end\":89574,\"start\":89551},{\"end\":89759,\"start\":89730},{\"end\":90222,\"start\":90056},{\"end\":90458,\"start\":90427},{\"end\":90706,\"start\":90668},{\"end\":90928,\"start\":90862},{\"end\":91098,\"start\":91052},{\"end\":91318,\"start\":91281},{\"end\":91508,\"start\":91436},{\"end\":91630,\"start\":91527},{\"end\":91995,\"start\":91923},{\"end\":92207,\"start\":92137},{\"end\":92273,\"start\":92223},{\"end\":92481,\"start\":92371},{\"end\":92618,\"start\":92547},{\"end\":92902,\"start\":92821},{\"end\":93042,\"start\":92962},{\"end\":93246,\"start\":93216},{\"end\":93445,\"start\":93359},{\"end\":93732,\"start\":93674},{\"end\":93928,\"start\":93891},{\"end\":94159,\"start\":94092},{\"end\":94323,\"start\":94299},{\"end\":94445,\"start\":94422},{\"end\":94653,\"start\":94597},{\"end\":94881,\"start\":94805},{\"end\":95136,\"start\":95099},{\"end\":95397,\"start\":95335},{\"end\":95505,\"start\":95412},{\"end\":95693,\"start\":95585},{\"end\":96001,\"start\":95985},{\"end\":96296,\"start\":96230},{\"end\":96602,\"start\":96515},{\"end\":96853,\"start\":96767},{\"end\":97094,\"start\":97056},{\"end\":97338,\"start\":97279},{\"end\":97632,\"start\":97576},{\"end\":97790,\"start\":97700},{\"end\":97958,\"start\":97927},{\"end\":98254,\"start\":98182},{\"end\":98434,\"start\":98397},{\"end\":98623,\"start\":98605},{\"end\":98927,\"start\":98746},{\"end\":99158,\"start\":99123},{\"end\":99348,\"start\":99310},{\"end\":99511,\"start\":99483},{\"end\":99630,\"start\":99593},{\"end\":99801,\"start\":99740},{\"end\":100037,\"start\":99970},{\"end\":100174,\"start\":100137},{\"end\":100369,\"start\":100252},{\"end\":100546,\"start\":100456},{\"end\":100722,\"start\":100660},{\"end\":100962,\"start\":100890},{\"end\":101072,\"start\":100970},{\"end\":101314,\"start\":101295},{\"end\":101527,\"start\":101487},{\"end\":101805,\"start\":101767},{\"end\":101972,\"start\":101949},{\"end\":102227,\"start\":102141},{\"end\":102407,\"start\":102313},{\"end\":102497,\"start\":102415},{\"end\":102663,\"start\":102643},{\"end\":102835,\"start\":102798},{\"end\":102891,\"start\":102851},{\"end\":103146,\"start\":103109},{\"end\":103389,\"start\":103327},{\"end\":103463,\"start\":103405},{\"end\":103747,\"start\":103716},{\"end\":103947,\"start\":103885},{\"end\":104110,\"start\":104095},{\"end\":104294,\"start\":104222},{\"end\":104462,\"start\":104444},{\"end\":104732,\"start\":104651},{\"end\":104979,\"start\":104905},{\"end\":105293,\"start\":105273},{\"end\":87455,\"start\":87374},{\"end\":89202,\"start\":89150},{\"end\":91131,\"start\":91100},{\"end\":100388,\"start\":100371}]"}}}, "year": 2023, "month": 12, "day": 17}