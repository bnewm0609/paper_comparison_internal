{"id": 247519181, "updated": "2023-10-05 16:04:55.098", "metadata": {"title": "PanoFormer: Panorama Transformer for Indoor 360 Depth Estimation", "authors": "[{\"first\":\"Zhijie\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Chunyu\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Kang\",\"last\":\"Liao\",\"middle\":[]},{\"first\":\"Lang\",\"last\":\"Nie\",\"middle\":[]},{\"first\":\"Zishuo\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Yao\",\"last\":\"Zhao\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Existing panoramic depth estimation methods based on convolutional neural networks (CNNs) focus on removing panoramic distortions, failing to perceive panoramic structures efficiently due to the fixed receptive field in CNNs. This paper proposes the panorama transformer (named PanoFormer) to estimate the depth in panorama images, with tangent patches from spherical domain, learnable token flows, and panorama specific metrics. In particular, we divide patches on the spherical tangent domain into tokens to reduce the negative effect of panoramic distortions. Since the geometric structures are essential for depth estimation, a self-attention module is redesigned with an additional learnable token flow. In addition, considering the characteristic of the spherical domain, we present two panorama-specific metrics to comprehensively evaluate the panoramic depth estimation models' performance. Extensive experiments demonstrate that our approach significantly outperforms the state-of-the-art (SOTA) methods. Furthermore, the proposed method can be effectively extended to solve semantic panorama segmentation, a similar pixel2pixel task. Code will be available.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2203.09283", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ShenLLNZZ22", "doi": "10.48550/arxiv.2203.09283"}}, "content": {"source": {"pdf_hash": "68472545de36e20f63b48c229bc92a69fce872a2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2203.09283v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "89732c786f263f4aedfbb436f7615fef02373df2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/68472545de36e20f63b48c229bc92a69fce872a2.txt", "contents": "\nPanoFormer: Panorama Transformer for Indoor 360\u00b0Depth Estimation\n\n\nZhijie Shen zhjshen@bjtu.edu.cn \nInstitute of Information Science\nBeijing Jiaotong University\nChina\n\nBeijing Key Laboratory of Advanced Information Science and Network Technology\n\n\nChunyu Lin cylin@bjtu.edu.cn \nInstitute of Information Science\nBeijing Jiaotong University\nChina\n\nBeijing Key Laboratory of Advanced Information Science and Network Technology\n\n\nKang Liao kangliao@bjtu.edu.cn \nInstitute of Information Science\nBeijing Jiaotong University\nChina\n\nBeijing Key Laboratory of Advanced Information Science and Network Technology\n\n\nMax Planck Institute for Informatics\nGermany\n\nLang Nie nielang@bjtu.edu.cn \nInstitute of Information Science\nBeijing Jiaotong University\nChina\n\nBeijing Key Laboratory of Advanced Information Science and Network Technology\n\n\nZishuo Zheng zszheng@bjtu.edu.cn \nInstitute of Information Science\nBeijing Jiaotong University\nChina\n\nBeijing Key Laboratory of Advanced Information Science and Network Technology\n\n\nYao Zhao yzhao@bjtu.edu.cn \nInstitute of Information Science\nBeijing Jiaotong University\nChina\n\nBeijing Key Laboratory of Advanced Information Science and Network Technology\n\n\nChunyu Lin \nPanoFormer: Panorama Transformer for Indoor 360\u00b0Depth Estimation\n\nExisting panoramic depth estimation methods based on convolutional neural networks (CNNs) focus on removing panoramic distortions, failing to perceive panoramic structures efficiently due to the fixed receptive field in CNNs. This paper proposes the panorama transformer (named PanoFormer ) to estimate the depth in panorama images, with tangent patches from spherical domain, learnable token flows, and panorama specific metrics. In particular, we divide patches on the spherical tangent domain into tokens to reduce the negative effect of panoramic distortions. Since the geometric structures are essential for depth estimation, a self-attention module is redesigned with an additional learnable token flow. In addition, considering the characteristic of the spherical domain, we present two panorama-specific metrics to comprehensively evaluate the panoramic depth estimation models' performance. Extensive experiments demonstrate that our approach significantly outperforms the state-of-the-art (SOTA) methods. Furthermore, the proposed method can be effectively extended to solve semantic panorama segmentation, a similar pixel2pixel task.\n\nIntroduction\n\nDepth information is important for computer systems to understand the real 3D world. Monocular depth estimation has attracted researchers' [6,7,8,15,40,39] attention with its convenience and low cost, especially for panoramic depth estimation [41,43,34], where the depth of the whole scene can be obtained from a single 360\u00b0image.\n\nSince estimating depth from a single image is an ill-posed and inherently ambiguous problem, current solutions almost use powerful CNNs to extract explicitly or implicitly prior geometric to realize it [3,5]. However, when applied to panoramic tasks, these SOTA depth estimation solutions for perspective imagery [21] show a dramatic degradation because the 360\u00b0field-of-view (FoV) arXiv:2203.09283v2 [cs.CV] 12 Jul 2022 PanoFormer Fig. 1. We present PanoFomer to establish panoramic perception capability. The tangent-patch is proposed to remove panoramic distortions, and the token flows force the token positions to fit the structure of the sofa better. More details refer to Sec. 3 from panorama brings geometric distortions that challenge the structure perception. Specifically, distortions in panoramas (usually represented in equirectangular projection-ERP) increase from the center to both sides along the latitude direction, severely deforming objects' shapes. Due to the fixed receptive field, CNNs are inferior for dealing with distortions and perceiving geometric structures in panoramas [5]. To deal with the distortions in panoramas, some researchers [17,28,29,34] adopt the projection-fusion strategy. But this strategy needs to cover the domain gap between different projections, and the extra cross-projection fusion module increases computational burdens. Other researchers [9,10,13,30,44,30,37,38,16,19,22] employ various distortion-aware convolution filters to make CNN-based depth estimation solutions adapt to 360\u00b0i mages. However, the fixed sampling positions still limit their performance. Pintore et al. [26] focuses on the full geometric context of an indoor scene, proposing SliceNet but losing detailed information when reconstructing the depth map. We note that all the existing methods cannot perceive the distorted geometric structures with the fixed receptive field.\n\nTo address the above limitations, we propose the first panorama Transformer (PanoFormer) to enable the network's panoramic perception capability by removing distortions and perceiving geometric structures simultaneously (shown in Fig. 1). To make the Transformer suitable for panoramic dense prediction tasks (e.g., depth estimation and semantic segmentation), we redesign its structure. First, we propose a dense patches dividing method and handcrafted tokens to catch detailed features. Then, we design a relative position embedding method to reduce the negative effect of distortions, which utilizes a central token to locate the eight most relevant tokens to form a tangent patch (it differs from directly dividing patches on the ERP domain in traditional vision Transformers). To achieve this goal, we propose an efficient spherical token locating model (STLM) to guide the 'non-distortion' token sampling process on the ERP domain directly by building the Transformations among the three domains (shown in Fig. 3). Subsequently, we design a Panoramic Structure-guided Transformer (PST) block to replace the traditional block in a hierarchical architecture. Specifically, we redesign the self-attention module with additional learnable weight to push token flow, so as to flexibly capture various objects' structures. This module encourages the PanoFormer to further perceive geometric structures effec-tively. In this way, we establish our network's perception capability to achieve panoramic depth estimation. Moreover, the proposed PST block can be applied to other learning frameworks as well.\n\nFurthermore, current evaluation metrics for depth estimation are suitable for perspective imagery. However, these metrics did not consider distortions and the seamless boundary property in panoramas. To comprehensively evaluate the depth estimation for panoramic images, we design a Pole Root Mean Square Error (P-RMSE) and Left-Right Consistency Error (LRCE) to measure the accuracy on polar regions and depth consistency around the boundaries, respectively.\n\nExtensive experiments demonstrate that our solution significantly outperforms SOTA algorithms in panoramic depth estimation. Besides, our solution achieves the best performance when applied to semantic segmentation, which is also a pixel2pixel panoramic task. The contributions of this paper are summarized as follows:\n\n-We present PanoFormer , the first panorama Transformer, to establish the panoramic perception capability by reducing distortions and perceiving geometric structures for the panoramic depth estimation task. -We propose a PST block that divides patches on the spherical tangent domain and reshapes the self-attention module with the learnable token flow. Moreover, the proposed block can be applied in other learning frameworks. -Considering the difference between Panorama and normal images, we design two new panorama-specific metrics to evaluate the panoramic depth estimation. -Experiments demonstrate that our method significantly outperforms the current state-of-the-art approaches on all metrics. The excellent panorama semantic segmentation results also prove the extension ability of our model.\n\n\nRelated Work\n\n\nPanoramic Depth Estimation\n\nThere are two main fusion methods to reduce distortions while estimating depth on ERP maps. One is the equirectangular-cube fusion method represented by Bifuse [34], and the other is the dual-cube fusion approach described by Shen [28]. Specifically, Bifuse [34] propose a two-branch method of fusing equirectangular projection and cube projection, which improves the tolerance of the model to distortions. Moreover, UniFuse [17] also uses a dual projection fusion scheme only at the encoding stage to reduce computation cost. Noting that the single-cube projection method produces significant discontinuities at the cube boundary, Shen et al [28] proposed a dual-cube approach based on a 45\u00b0rotation to reduce distortions. This class of methods can attenuate the negative effect of distortions, but they need to repeatedly change the projection for fusion, increasing the model's complexity.\n\n\nC\u00d7H\u00d7W\n\n...  To apply depth estimation models of normal images to panoramas, Tateno et al. [33] obtained exciting results by designing distortion-aware convolution filters to expand the perceptual field. Zioulis et al. [45] demonstrated that monocular depth estimation models trained on conventional 2D images produce lowquality results, highlighting the necessity of learning directly on the 360\u00b0domain. Jin et al. [18] demonstrated the effectiveness of geometric prior for panoramic depth estimation. Chen et al. [5] used strip pooling and deformable convolution to design a new encoding structure for accommodating different degrees of distortions. Moreover, Pintore et al. [26] proposed SliceNet, a network similar to HorizonNet [31], which uses a bidirectional Long Short-Term Memory (LSTM) to model long-range dependencies. However, the slicing method ignores the latitudinal distortion property and thus cannot accurately predict the depth near the poles. Besides, [2,11] proved that on large-scale datasets, Transformer-based depth estimation for normal images are superior to CNN.\n\n\nC\u00d7H\u00d7W\n\n\nVision Transformer\n\nUnlike CNN-based networks, the Transformer has the nature to model longrange dependencies by global self-attention [27]. Inspired by ViT [11], researchers have designed many efficient networks that have the advantages of both CNNs and Transformers. To enhance local features extraction, convolutional layers are added into muti-head self-attention (CvT [36]) and feed-forward network (FFN) (CeiT [42], LocalViT [23]) is replaced by locally-enhanced feed-forward network (LeFF) (Uformer [35]). Besides, CvT [36] demonstrates that the padding operation in CNNs implicitly encodes position, and CeiT [42] proposes the imageto-tokens embedding method. Inspired by SwinT [24], Uformer [35] proposes a shifted windows-based multi-head attention mechanism to improve the efficiency of the model. But all these solutions are developed based on normal FoV images, which cannot be applied to panoramic images directly. Based on these previous works, we further explore suitable Transformer structure for panoramic images and adapt it to the dense prediction task.\n\n\nPanoFomer\n\n\nArchitecture Overview\n\nOur primary motivation is to make the Transformer suitable for pixel-level omnidirectional vision tasks by redesigning the standard components in conventional Transformers. Specifically, we propose a pixel-level patch division strategy, a relative position embedding method, and a panoramic self-attention mechanism. The proposed pixel-level patch division strategy is to enhance local features and improve the ability of Transformers to capture detailed features. For position embedding, we renounce the conventional absolute position embedding method and get the position of other related tokens on the same patch by the central token (described in 3.4). This method not only eliminates distortions, but also provides position embedding. Furthermore, we establish a learnable flow in the panorama self-attention module to perceive panoramic structures that are essential for depth estimation.\n\nAs shown in Fig. 2, the PanoFomer is a hierarchical structure with five major parts: input stem, output stem, encoder, decoder and bottleneck. For the input stem, a 3\u00d73 convolution layer is adopted with size H\u00d7W to form the features with dimension C. Then the features are fed into the encoder. There are four hierarchical stages in encoder and decoder, and each of them contains a position embedding, two PST blocks (sharing the same settings), and a convolution layer. Specifically, a 4\u00d74 convolution layer is adopted for increasing dimension and down-sampling in the encoder, while a 2\u00d72 transposed convolution layer is used in the decoder for decreasing dimension and up-sampling. Finally, the output features from the decoder share the same resolution and dimension as the features from the input stem. Furthermore, the output stem, implemented by a 3\u00d73 convolution, is employed to recover the depth map from features. More specifically, the number of heads is sequentially set as [Encoder:1, 2, 4, 8; Bottleneck: 16; Decoder: 16,8,4,2]. As for all padding operations in convolution layers, we utilize circular padding for both horizontal sides of the features.\n\n\nTransformer-customized Spherical Token\n\nIn vision Transformers, the input image is first divided into patches of the same size. For example, ViT [11] divides the input image into patches with size of 16\u00d716 to reduce the computational burden. Then, these patches are embedded as tokens in a learning-based way via a linear layer. However, this strategy loses much detailed information, which is a fatal drawback for dense prediction tasks, such as depth estimation. To overcome this issue, we propose a pixel-level patches dividing method.\n\nFirst, the input features are divided into pixel-level patches, which means each sampling position in the features corresponds to a patch centered on it. Such a dense division strategy allows the network to learn more detailed features, which is beneficial for dense prediction tasks. Furthermore, we make each \n\n\nRelative Position Embedding\n\nInspired by the cube projection, we note that the spherical tangent projection can effectively remove the distortion (see Supplementary Materials for proof). Therefore, we propose STLM to initialize the position of related tokens. Unlike the conventional Transformers (e.g., ViT [11]), which directly adds absolute position encoding to the features, we \"embed\" the position information via the central token. Firstly, the central token is projected from the ERP domain to the spherical domain; then, we use the central token to look up the position of eight nearest neighbors on the tangent plane; finally, these positions are all projected back to the ERP domain (the three steps are represented by yellow arrows in Fig. 3). We call patches formed in this way as tangent patches. To facilitate locating the related tokens in the ERP domain, we further establish the relationship among the three domains (illustrated in Fig. 3). Tangent domain to spherical domain: Let the unit sphere be S 2 , and S(0, 0) = (\u03b8 0 , \u03d5 0 ) \u2208 S 2 is the spherical coordinate origin. \u2200S(x, y) = (\u03b8, \u03d5) \u2208 S 2 , we can obtain other 8 points (related tokens) around it (current token) on the spherical domain.\nS(\u00b11, 0) =(\u03b8 \u00b1 \u2206\u03b8, \u03d5) S(0, \u00b11) =(\u03b8, \u03d5 \u00b1 \u2206\u03d5) S(\u00b11, \u00b11) =(\u03b8 \u00b1 \u2206\u03b8, \u03d5 \u00b1 \u2206\u03d5)(1)\nwhere (\u03b8, \u03d5) denotes the unit spherical coordinates, and \u03b8 \u2208 (\u2212\u03c0, \u03c0), \u03d5 \u2208 (\u2212 \u03c0 2 , \u03c0 2 ); \u2206\u03b8,\u2206\u03d5 is the sampling step size. By the geocentric projection [25], we can calculate the local coordinates (T (x, y))of the sampling point in tangent domain [9] (the current token in tangent domain is represented as T (0, 0) = T (\u03b8, \u03d5) = (0, 0)):\nT (\u03b8 \u00b1 \u2206\u03b8, \u03d5) =(\u00b1 tan \u2206\u03d5, 0) T (\u03b8, \u03d5 \u00b1 \u2206\u03d5) =(0, \u03d5 \u00b1 tan \u2206\u03b8)\nT (\u03b8 \u00b1 \u2206\u03b8, \u03d5 \u00b1 \u2206\u03d5) =(\u00b1 tan \u2206\u03d5, \u00b1 sec \u2206\u03d5 tan \u2206\u03b8)\n\nBy applying the inverse projection described in [9], we can get the position of all tokens of a tangent patch in the spherical domain. Spherical domain to ERP domain: Furthermore, by utilizing the projection equation [28], we can get the position of each tangent patch in the ERP domain. This whole process is named Spherical Token Locating Model (STLM).\n\n\nPanorama Self-Attention with Token Flow\n\nBased on the traditional vision Transformer block, we replace the original attention mechanism with panorama self-attention. To further enhance local features interaction, we replace FFN with LeFF [42] for our pixel-level depth estimation task. Specifically, as illustrated in Fig. 4, when the features f \u2208 R C\u00d7H\u00d7W with a height of H and a width of W are fed into PST block, they are flattened and reshaped as f \u2032 \u2208 R N \u00d7C , where N = H \u00d7W . Then a fully connected layer is applied to obtain query Q \u2208 R N \u00d7d and value V \u2208 R N \u00d7d , where d = C/M , and M is the head number. The Q and V will pass through three parallel branches for computing attention score (A \u2208 R N \u00d79 ), token flows (\u2206s \u2208 R N \u00d718 ), and re-sampling features. In the top branch, a full connection layer is adopted to get attention weights W A \u2208 R N \u00d79 from Q, and then softmax is employed to calculate the attention score A. In the middle branch, another fully connection layer is used to learn a token flow \u2206s and it is further reshaped to \u2206s \u2032 \u2208 R d\u00d7H\u00d7W \u00d79\u00d72 , sharing the same dimension with\u015d (the initialed position from the STLM). Moreover, \u2206s \u2032 and\u015d are added together to calculate the final token positions. In the bottom branch, the value V is reshaped to V \u2032 \u2208 R C\u00d7H\u00d7W and are sampled to form \nPSA(f,\u015d) = M m=1 W m * H\u00d7W q=1 9 k=1 A mqk \u00b7 W \u2032 m f (\u015d mqk + \u2206s mqk ) ,(3)\nwhere\u015d = ST LM (f ), and ST LM (\u00b7) denotes the spherical token locating model; m indexes the head of self-attention, M is the whole heads, q index the current point (token), k indexes the tokens in a tangent patch, \u2206s mqk is the learned flow of each token, A mqk represents the attention weight of each token, and W m and W \u2032 m are normal learnable weights of each head. From the above process, we can see that the final positions of the tokens are determined by two steps: position initialization from STLM and additional learnable flow. Actually, the initialized position realizes the division of the tangent patch (described in 3.4) and removes the panoramic distortion. Furthermore, the learnable flow exhibits a panoramic geometry by adjusting the spatial distribution of tokens. To verify the effectiveness of the token flow, we visualize all tokens from the first PST block in Fig. 5. It can be observed that this additional flow provides the network with clear scene geometric information, which helps the network to estimate the panorama depth with the structure as a clue.\n\n\nObjective Function\n\nFor better supervision, we combine reverse Huber [14] (or Berhu [21]) loss and gradient loss [28] to design our objective function as commonly used in previous works [26,28]. In our objective function, the Berhu loss \u03b2 \u03b4 can be written as:\n\u03b2 \u03b4 (g, p) = |g \u2212 p| for |g \u2212 p| \u2264 \u03b4 |(g\u2212p) 2 |+\u03b4 2 2\u03b4 otherwise (4)\nwhere g, p denote the ground truth and predicted values, respectively. Similar to SliceNet [26], we apply gradient loss to Berhu loss. To obtain depth edges, we use two convolution kernels to obtain gradients in horizontal and vertical directions, respectively. They are represented as K h and K v , where K h = [-1 0 1, -2 0 2, -1 0 1], and K v = (K h ) T . Denote the gradient function as G, the horizontal gradient I h and vertical gradient I v of the input image I can be expressed as I h = G(K h , I) and I v = G(K v , I), respectively. In this paper, \u03b4 = 0.2 and the final objective function can be written as\n\u2113 f inal = \u03b2 0.2 (g, p) + \u03b2 0.2 (G(K h , g), G(K h , p)) + \u03b2 0.2 (G(K v , g), G(K v , p)), (5)\n\nPanorama-specific Metrics\n\nRethinking the spherical domain, we note that two significant properties cannot be neglected: the spherical domain is continuous and seamless everywhere; the distortion in the spherical domain is equal everywhere. For the first issue, we propose LRCE to measure the depth consistency of left-right boundaries. For the second issue, since distortions on ERP maps vary in longitude, RMSE cannot visually reflect the model's ability to adapt to distortions. Therefore, we provide P-RMSE to focus on the regions with massive distortions to verify the model's panoramic perception capability. Pole Root Mean Square Error. Cube projection is a special spherical tangent projection format that projects the sphere onto the cube's six faces. The top and bottom faces correspond to the polar regions of the spherical domain, so we select the two parts to design P-RMSE (illustrated in Fig. 6). Define the function of converting ERP to Cube as E2C(\u00b7), the converted polar regions of the ERP image E can be expressed as Select(E2C(E), T, B), where T, B represent the top and bottom parts, respectively. The error C e between the ground truth GT and the predicted depth map P at the polar regions can be expressed as\nC e = Select(E2C(GT ), T, B) \u2212 Select(E2C(P ), T, B)(6)\nThe final P-RMSE can be written as\nP-RMSE = 1 N Ce N Ce i=1 |C i e |(7)\nwhere N Ce is the number of values in C e .\n\nLeft-Right Consistency Error. We can evaluate the depth consistency of the left-right boundaries by calculating the horizontal gradient between the both sides of the depth map. Define that the horizontal gradient G H E of the image E can be written as\nG H E = E col f irst \u2212E col last ,\nwhere E col f irst / E col last represent the values in the first/last columns of the image E. But consider an extreme case where if the edge of an object in the scene happens to be on the edge of the depth map, then there is ambiguity in reflecting continuity only by G H E . We cannot tell whether this discontinuity is real or caused by the model. Therefore, we add ground truth to our design. The horizontal gradient of ground truth and the predicted depth map are denoted as G H GT and G H P (where G H GT = GT col f irst \u2212 GT col last , G H P = P col f irst \u2212 P col last ), respectively. The final expression can be as follows:\nLRCE = 1 N error Nerror i=1 |error i |(8)\nwhere error = G H GT \u2212 G H P and N error is the number of values in error.\n\n\nExperiments\n\nIn the experimental part, we compare the state-of-the-art approaches on four popular datasets and validate the effectiveness of our model.\n\n\nDatasets and Implementations\n\nFour datasets are used for our experimental validation, they are Stanford2D-3D [1], Matterport3D [4], PanoSUNCG [29] and 3D60 [45]. Stanford2D3D and Matterport3D are two real-world datasets. They were rendered from a common viewpoint. Previous work used a dataset that was rendered only on the equator and its surroundings, ignoring the area near the poles, which undermined the integrity of the panorama. We strictly follow the previous works and employ the official datasets (Notice that the Stanford2D3D and Matterport3D that are contained in 3D60 have a problem that the light in the scenarios will leak the depth information). PanoSUNCG is a virtual panoramic dataset. And 3D60 is an updated version of 360D (360D is no longer available now). It consists of data from the above three datasets. There is a gap between the distributions of these three datasets, which makes the dataset more responsive to the model's generalizability. Note that we divide the dataset as the previous work and eliminate the samples that failed to render [5,34].\n\nIn the implementation, we conduct our experiments on two GTX 3090 GPUs, and the batch size is set to 4. We choose Adam [20] as the optimizer and keep the default settings. The initialized learning rate is 1 \u00d7 10 \u22124 . The number of parameters of our model is 20.37 M. \n\n\nComparison Results\n\nWe selected the metrics used in previous work and the two proposed metrics for the quantitative comparison, including RMSE, \u03b4(1.25, 1.25 2 , 1.25 3 ) and panorama-specific metrics, LRCE and P-RMSE (We cannot calculate the proposed new metrics due to limitation of the two real-world datasets). RMSE reflects the overall variability. \u03b4 exhibits the difference between ground truth and the predicted depth.\n\nQuantitative Analysis.  PanoSUNCG, there is a 42% improvement on RMSE. But there is just a 16% improvement on 3D60 dataset with RMSE. The improvement is not particularly significant compared to the other three datasets because 3D60 dataset is more extensive, the difference between the models is not obvious. The improvement on \u03b4 performance further demonstrates that our model can obtain more accurate prediction results. On the new metric P-RMSE, we achieved an average gain of about 40% on the other two virtual datasets. It indicates that our model is more resilient to the distortion in panoramas. In addition, on LRCE, our model outperforms 40% on PanoSUNCG and 12% on 3D60, showing that our model can better constrain the depth consistency of the left-right boundaries in panoramas, because our network fully considers the seamless property of the sphere. Qualitative Analysis. Fig. 7 shows the qualitative comparison with the current SOTA approaches. From the figures, we can observe that SliceNet is relatively accurate in predicting regions without distortion. However, the model performance degrades dramatically in regions with distortions or large object deformations.\n\nAlthough SliceNet can efficiently focus on the global panoramic structures, the depth reconstruction process cannot accurately recover the details, which affects the model's performance. UniFuse can deal with deformation effectively, but it still suffers from incorrect estimating and tends to lose detailed information. From Fig. 8, we can observe that our results are very competitive at boundary and pole areas. \n\n\nAblation study\n\nWith the same conditions, we validated the key components of our model by ablation study on Stanford2D3D (real-world dataset, small-scale, challenging). As illustrated in Table 2, a presents the baseline structure that we use convolutional layers to replace PST blocks; Our network with the traditional attention mechanism is expressed with b; c indicates our attention module without token flow; Our entire network is shown as d.\n\nTransformer vs. CNN. From Table 2, we can observe that the Transformer gains 35% improvements over CNNs in terms of RMSE. Furthermore, qualitative results in b are more precise than the CNNs. Essentially, CNNs are a special kind of self-attention. Since the convolutional kernel is fixed, it requires various components or structures or even deeper networks to help the model learn the data patterns. On the other hand, the attention in Transformer is more flexible, and it is relatively easier to learn the patterns. Effectiveness of Tangent-patches for Transformer. To illustrate the effectiveness of the tangent-patch dividing method, we compared an alternative attention structure that currently performs SOTA in vision Transformers. From Table 2, our network with tangent-patches (c) outperforms the attention mechanism (b) with 21% on RMSE, 10% on P-RMSE and 12% on LRCE. It proves that tangent-patch can help networks deal with panoramic distortions. Effectiveness of Token Flow. Since the geometric structures are essential for depth estimation, we add the additional token flows to perceive geometric structures. The results in Table 2 show that our model with the token flow can make P-RMSE more competitive. In Fig. 9, we can observe that the token flow allows the model to estimate the depth details more accurately.\n\nRGB GT a b c d Fig. 9. Qualitative comparison of ablation study. a, b, c, d are the same as Table 2 5\n\n\n.4 Extensibility\n\nWe also validate the extensibility of our model by the panoramic segmentation that is also a pixel2pixel task. We did not change any structure of our network and strictly followed the experimental protocol in [32]. As listed in Table 3, the experimental results show that our model outperforms the current SOTA approaches. Due to page limitations, more qualitative comparisons and the results with a high resolution can be found in the supplementary material. \n\n\nConclusion\n\nIn this paper, we propose the first panorama Transformer (PanoFormer) for indoor panoramic depth estimation. Unlike current approaches, we remove the negative effect of distortions and further model geometric structures by using learnable token flow to establish the network's panoramic perceptions. Concretely, we design a PST block, which can be effectively extended to other learning frameworks. To comprehensively measure the performance of the panoramic depth estimation models, we propose two panorama-specific metrics based on the priors of equirectangular images. Experiments demonstrate that our algorithm significantly outperforms current SOTA methods on depth estimation and other pixel2pixel panoramic tasks, such as semantic segmentation.\n\nFig. 2 .\n2Our PanoFormer takes a monocular RGB panoramic image as the input and outputs the high-quality depth map\n\nFig. 3 .\n3Spherical Token Locating Model (STLM): locate related tokens on ERP domain. 1: tangential domain of unit sphere to spherical domain; 2: spherical domain to ERP domain surrounding positions, illustrated in Fig. 3 left) to balance the computational burden. Unlike standard Transformers that embed patches as tokens by a linear layer, our tokens are handcrafted. We define the features at the central position as the central token and those from the other 8 surrounding positions as the related tokens. The central token can determine the position of related tokens by looking up the eight most relevant tokens among the features. To remove distortion and embed position information for the handcrafted tokens, we propose a distortion-based relative position embedding method in Sec. 3.3.\n\nFig. 4 .\n4The proposed PST Block can remove the negative effect of distortions and perceive geometric structures\n\nFig. 5 .\n5Visualization of the token flows from the first PST block, which suggest the panoramic structures the divided patches (described in 3.1) by looking up the related tokens in the final token positions. Afterward, the PSA can be represented as follows:\n\nFig. 6 .\n6P-RMSE: calculate the RMSE of the polar regions\n\nFig. 7 .Fig. 8 .\n78Qualitative results on Matterport3D, Stanford2D3D, PanoSUNCG, and 3D60. More results can be found in Visualization of the new metrics' comparison between our method and Unifuse[17]. (a) We stitch the ERP results to observe the depth consistency. (b) We project the areas with massive distortions to cube face to compare the models' performance\n\nTable 1 .\n1Quantitative comparisons on Matterport3D, Stanford2D3D, PanoSUNCG and 3D60 Datasets.Classic metrics \n\n\nTable 1\n1shows the quantitative comparison results \n\n\nTable 2 .\n2Ablation study. We trained on Stanford2D3D for 70 epochs. a is the baseline structure developed with CNNsIndex Transformer STLM Token Flow RMSE P-RMSE LRCE \na \n0.6704 \n0.2258 \n0.2733 \nb \n0.4349 \n0.2068 \n0.2155 \nc \n0.3739 \n0.1825 \n0.1916 \nd \n0.3366 0.1793 0.1784 \n\n\n\nTable 3 .\n3Quantitative comparison for semantic segmentation on Stanford2D3D. Results are averaged over the official 3 folds[32] Dataset \nMethod \nmIoU\u2191 mAcc\u2191 \nTangentImg [12] \n41.8 \n54.9 \nStanford2D3D \nHoHoNet [32] \n43.3 \n53.9 \nOurs \n48.9 \n64.5 \n\n\nAcknowledgement. This work was supported by the National Key R&D Program of China (No.2021ZD0112100), and the National Natural Science Foundation of China (Nos. 62172032, U1936212, 62120106009).\nJoint 2d-3d-semantic data for indoor scene understanding. I Armeni, S Sax, A R Zamir, S Savarese, arXiv:1702.01105arXiv preprintArmeni, I., Sax, S., Zamir, A.R., Savarese, S.: Joint 2d-3d-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105 (2017)\n\nAdabins: Depth estimation using adaptive bins. S F Bhat, I Alhashim, P Wonka, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionBhat, S.F., Alhashim, I., Wonka, P.: Adabins: Depth estimation using adaptive bins. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition. pp. 4009-4018 (2021)\n\nA Bhoi, arXiv:1901.09402Monocular depth estimation: A survey. arXiv preprintBhoi, A.: Monocular depth estimation: A survey. arXiv preprint arXiv:1901.09402 (2019)\n\nMatterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niebner, M Savva, S Song, A Zeng, Y Zhang, 2017 International Conference on 3D Vision (3DV). IEEE Computer SocietyChang, A., Dai, A., Funkhouser, T., Halber, M., Niebner, M., Savva, M., Song, S., Zeng, A., Zhang, Y.: Matterport3d: Learning from rgb-d data in indoor environ- ments. In: 2017 International Conference on 3D Vision (3DV). pp. 667-676. IEEE Computer Society (2017)\n\nDistortion-aware monocular depth estimation for omnidirectional images. H X Chen, K Li, Z Fu, M Liu, Z Chen, Y Guo, IEEE Signal Processing Letters. 28Chen, H.X., Li, K., Fu, Z., Liu, M., Chen, Z., Guo, Y.: Distortion-aware monocular depth estimation for omnidirectional images. IEEE Signal Processing Letters 28, 334-338 (2021)\n\nCube padding for weakly-supervised saliency prediction in 360 videos. H T Cheng, C H Chao, J D Dong, H K Wen, T L Liu, M Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCheng, H.T., Chao, C.H., Dong, J.D., Wen, H.K., Liu, T.L., Sun, M.: Cube padding for weakly-supervised saliency prediction in 360 videos. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1420-1429 (2018)\n\nOmnidirectional depth extension networks. X Cheng, P Wang, Y Zhou, C Guan, R Yang, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEECheng, X., Wang, P., Zhou, Y., Guan, C., Yang, R.: Omnidirectional depth exten- sion networks. In: 2020 IEEE International Conference on Robotics and Automa- tion (ICRA). pp. 589-595. IEEE (2020)\n\nSpherical cnns. T S Cohen, M Geiger, J K\u00f6hler, M Welling, International Conference on Learning Representations. Cohen, T.S., Geiger, M., K\u00f6hler, J., Welling, M.: Spherical cnns. In: International Conference on Learning Representations (2018)\n\nSpherenet: Learning spherical representations for detection and classification in omnidirectional images. B Coors, A P Condurache, A Geiger, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Coors, B., Condurache, A.P., Geiger, A.: Spherenet: Learning spherical represen- tations for detection and classification in omnidirectional images. In: Proceedings of the European conference on computer vision (ECCV). pp. 518-533 (2018)\n\nDeformable convolutional networks. J Dai, H Qi, Y Xiong, Y Li, G Zhang, H Hu, Y Wei, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionDai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., Wei, Y.: Deformable convolu- tional networks. In: Proceedings of the IEEE international conference on computer vision. pp. 764-773 (2017)\n\nA Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.11929An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)\n\nTangent images for mitigating spherical distortion. M Eder, M Shvets, J Lim, J M Frahm, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionEder, M., Shvets, M., Lim, J., Frahm, J.M.: Tangent images for mitigating spherical distortion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12426-12434 (2020)\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Advances in neural information processing systems. 27Eigen, D., Puhrsch, C., Fergus, R.: Depth map prediction from a single image using a multi-scale deep network. Advances in neural information processing systems 27 (2014)\n\nA novel approach to quantized matrix completion using huber loss measure. A Esmaeili, F Marvasti, IEEE Signal Processing Letters. 262Esmaeili, A., Marvasti, F.: A novel approach to quantized matrix completion using huber loss measure. IEEE Signal Processing Letters 26(2), 337-341 (2019)\n\nLearning so (3) equivariant representations with spherical cnns. C Esteves, C Allen-Blanchette, A Makadia, K Daniilidis, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Esteves, C., Allen-Blanchette, C., Makadia, A., Daniilidis, K.: Learning so (3) equivariant representations with spherical cnns. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 52-68 (2018)\n\nC Jiang, J Huang, K Kashinath, P Marcus, M Niessner, arXiv:1901.02039Spherical cnns on unstructured grids. arXiv preprintJiang, C., Huang, J., Kashinath, K., Marcus, P., Niessner, M., et al.: Spherical cnns on unstructured grids. arXiv preprint arXiv:1901.02039 (2019)\n\nUnifuse: Unidirectional fusion for 360 panorama depth estimation. H Jiang, Z Sheng, S Zhu, Z Dong, R Huang, IEEE Robotics and Automation Letters. 62Jiang, H., Sheng, Z., Zhu, S., Dong, Z., Huang, R.: Unifuse: Unidirectional fusion for 360 panorama depth estimation. IEEE Robotics and Automation Letters 6(2), 1519-1526 (2021)\n\nGeometric structure based and regularized depth estimation from 360 indoor imagery. L Jin, Y Xu, J Zheng, J Zhang, R Tang, S Xu, J Yu, S Gao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJin, L., Xu, Y., Zheng, J., Zhang, J., Tang, R., Xu, S., Yu, J., Gao, S.: Geomet- ric structure based and regularized depth estimation from 360 indoor imagery. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 889-898 (2020)\n\nGeometry aware convolutional filters for omnidirectional images representation. R Khasanova, P Frossard, International Conference on Machine Learning. PMLRKhasanova, R., Frossard, P.: Geometry aware convolutional filters for omnidirec- tional images representation. In: International Conference on Machine Learning. pp. 3351-3359. PMLR (2019)\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintKingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)\n\nDeeper depth prediction with fully convolutional residual networks. I Laina, C Rupprecht, V Belagiannis, F Tombari, N Navab, 2016 Fourth international conference on 3D vision (3DV). IEEELaina, I., Rupprecht, C., Belagiannis, V., Tombari, F., Navab, N.: Deeper depth prediction with fully convolutional residual networks. In: 2016 Fourth international conference on 3D vision (3DV). pp. 239-248. IEEE (2016)\n\nSpherephd: Applying cnns on a spherical polyhedron representation of 360deg images. Y Lee, J Jeong, J Yun, W Cho, K J Yoon, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionLee, Y., Jeong, J., Yun, J., Cho, W., Yoon, K.J.: Spherephd: Applying cnns on a spherical polyhedron representation of 360deg images. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9181- 9189 (2019)\n\nY Li, K Zhang, J Cao, R Timofte, L Van Gool, arXiv:2104.05707Localvit: Bringing locality to vision transformers. arXiv preprintLi, Y., Zhang, K., Cao, J., Timofte, R., Van Gool, L.: Localvit: Bringing locality to vision transformers. arXiv preprint arXiv:2104.05707 (2021)\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10012-10022 (2021)\n\nI F Pearson, Map Projections: Theory and Applications. Pearson, I.F.: Map Projections: Theory and Applications (1990)\n\nSlicenet: deep dense depth estimation from a single indoor panorama using a slice-based representation. G Pintore, M Agus, E Almansa, J Schneider, E Gobbetti, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPintore, G., Agus, M., Almansa, E., Schneider, J., Gobbetti, E.: Slicenet: deep dense depth estimation from a single indoor panorama using a slice-based repre- sentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11536-11545 (2021)\n\nVision transformers for dense prediction. R Ranftl, A Bochkovskiy, V Koltun, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionRanftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12179-12188 (2021)\n\nDistortion-tolerant monocular depth estimation on omnidirectional images using dual-cubemap. Z Shen, C Lin, L Nie, K Liao, Y Zhao, 2021 IEEE International Conference on Multimedia and Expo (ICME). IEEEShen, Z., Lin, C., Nie, L., Liao, K., Zhao, Y.: Distortion-tolerant monocular depth estimation on omnidirectional images using dual-cubemap. In: 2021 IEEE Inter- national Conference on Multimedia and Expo (ICME). pp. 1-6. IEEE (2021)\n\nSemantic scene completion from a single depth image. S Song, F Yu, A Zeng, A X Chang, M Savva, T Funkhouser, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSong, S., Yu, F., Zeng, A., Chang, A.X., Savva, M., Funkhouser, T.: Semantic scene completion from a single depth image. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1746-1754 (2017)\n\nKernel transformer networks for compact spherical convolution. Y C Su, K Grauman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSu, Y.C., Grauman, K.: Kernel transformer networks for compact spherical con- volution. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9442-9451 (2019)\n\nHorizonnet: Learning room layout with 1d representation and pano stretch data augmentation. C Sun, C W Hsiao, M Sun, H T Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSun, C., Hsiao, C.W., Sun, M., Chen, H.T.: Horizonnet: Learning room layout with 1d representation and pano stretch data augmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1047- 1056 (2019)\n\nHohonet: 360 indoor holistic understanding with latent horizontal features. C Sun, M Sun, H T Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSun, C., Sun, M., Chen, H.T.: Hohonet: 360 indoor holistic understanding with latent horizontal features. In: Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition. pp. 2573-2582 (2021)\n\nDistortion-aware convolutional filters for dense prediction in panoramic images. K Tateno, N Navab, F Tombari, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Tateno, K., Navab, N., Tombari, F.: Distortion-aware convolutional filters for dense prediction in panoramic images. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 707-722 (2018)\n\nBifuse: Monocular 360 depth estimation via bi-projection fusion. F E Wang, Y H Yeh, M Sun, W C Chiu, Y H Tsai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionWang, F.E., Yeh, Y.H., Sun, M., Chiu, W.C., Tsai, Y.H.: Bifuse: Monocular 360 depth estimation via bi-projection fusion. In: Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition. pp. 462-471 (2020)\n\nUformer: A general u-shaped transformer for image restoration. Z Wang, X Cun, J Bao, J Liu, arXiv:2106.03106arXiv preprintWang, Z., Cun, X., Bao, J., Liu, J.: Uformer: A general u-shaped transformer for image restoration. arXiv preprint arXiv:2106.03106 (2021)\n\nCVT: Introducing convolutions to vision transformers. H Wu, B Xiao, N Codella, M Liu, X Dai, L Yuan, L Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionWu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.: CVT: In- troducing convolutions to vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 22-31 (2021)\n\nSnap angle prediction for 360 panoramas. B Xiong, K Grauman, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Xiong, B., Grauman, K.: Snap angle prediction for 360 panoramas. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 3-18 (2018)\n\nY Xu, Z Zhang, S Gao, Spherical dnns and their applications in 360\u00b0images and videos. IEEE Transactions on Pattern Analysis and Machine Intelligence. Xu, Y., Zhang, Z., Gao, S.: Spherical dnns and their applications in 360\u00b0images and videos. IEEE Transactions on Pattern Analysis and Machine Intelligence (2021)\n\nMulti-modal masked pre-training for monocular panoramic depth completion. Z Yan, X Li, K Wang, Z Zhang, J Li, J Yang, arXiv:2203.09855arXiv preprintYan, Z., Li, X., Wang, K., Zhang, Z., Li, J., Yang, J.: Multi-modal masked pre-training for monocular panoramic depth completion. arXiv preprint arXiv:2203.09855 (2022)\n\nZ Yan, K Wang, X Li, Z Zhang, B Xu, J Li, J Yang, arXiv:2107.13802Rignet: Repetitive image guided network for depth completion. arXiv preprintYan, Z., Wang, K., Li, X., Zhang, Z., Xu, B., Li, J., Yang, J.: Rignet: Repeti- tive image guided network for depth completion. arXiv preprint arXiv:2107.13802 (2021)\n\nFlat2sphere: Learning spherical convolution for fast features from 360 imagery. S Yu-Chuan, G Kristen, Proceedings of International Conference on Neural Information Processing Systems (NIPS. International Conference on Neural Information Processing Systems (NIPSYu-Chuan, S., Kristen, G.: Flat2sphere: Learning spherical convolution for fast features from 360 imagery. In: Proceedings of International Conference on Neural Information Processing Systems (NIPS) (2017)\n\nIncorporating convolution designs into visual transformers. K Yuan, S Guo, Z Liu, A Zhou, F Yu, W Wu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionYuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F., Wu, W.: Incorporating convolution designs into visual transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 579-588 (2021)\n\nImproving 360 monocular depth estimation via non-local dense prediction transformer and joint supervised and self-supervised learning. I Yun, H J Lee, C E Rhee, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence36Yun, I., Lee, H.J., Rhee, C.E.: Improving 360 monocular depth estimation via non-local dense prediction transformer and joint supervised and self-supervised learning. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 3224-3233 (2022)\n\nDeformable DETR: Deformable transformers for end-to-end object detection. X Zhu, W Su, L Lu, B Li, X Wang, J Dai, International Conference on Learning Representations. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable DETR: De- formable transformers for end-to-end object detection. In: International Conference on Learning Representations (2020)\n\nOmnidepth: Dense depth estimation for indoors spherical panoramas. N Zioulis, A Karakottas, D Zarpalas, P Daras, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Zioulis, N., Karakottas, A., Zarpalas, D., Daras, P.: Omnidepth: Dense depth esti- mation for indoors spherical panoramas. In: Proceedings of the European Confer- ence on Computer Vision (ECCV). pp. 448-465 (2018)\n", "annotations": {"author": "[{\"end\":248,\"start\":68},{\"end\":426,\"start\":249},{\"end\":652,\"start\":427},{\"end\":830,\"start\":653},{\"end\":1012,\"start\":831},{\"end\":1188,\"start\":1013},{\"end\":1200,\"start\":1189}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":75},{\"end\":259,\"start\":256},{\"end\":436,\"start\":432},{\"end\":661,\"start\":658},{\"end\":843,\"start\":838},{\"end\":1021,\"start\":1017},{\"end\":1199,\"start\":1196}]", "author_first_name": "[{\"end\":74,\"start\":68},{\"end\":255,\"start\":249},{\"end\":431,\"start\":427},{\"end\":657,\"start\":653},{\"end\":837,\"start\":831},{\"end\":1016,\"start\":1013},{\"end\":1195,\"start\":1189}]", "author_affiliation": "[{\"end\":167,\"start\":101},{\"end\":247,\"start\":169},{\"end\":345,\"start\":279},{\"end\":425,\"start\":347},{\"end\":525,\"start\":459},{\"end\":605,\"start\":527},{\"end\":651,\"start\":607},{\"end\":749,\"start\":683},{\"end\":829,\"start\":751},{\"end\":931,\"start\":865},{\"end\":1011,\"start\":933},{\"end\":1107,\"start\":1041},{\"end\":1187,\"start\":1109}]", "title": "[{\"end\":65,\"start\":1},{\"end\":1265,\"start\":1201}]", "venue": null, "abstract": "[{\"end\":2411,\"start\":1267}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2569,\"start\":2566},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2571,\"start\":2569},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2573,\"start\":2571},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2576,\"start\":2573},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2579,\"start\":2576},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2582,\"start\":2579},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2674,\"start\":2670},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2677,\"start\":2674},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2680,\"start\":2677},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2964,\"start\":2961},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2966,\"start\":2964},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3076,\"start\":3072},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3862,\"start\":3859},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3928,\"start\":3924},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3931,\"start\":3928},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3934,\"start\":3931},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3937,\"start\":3934},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4154,\"start\":4151},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4157,\"start\":4154},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4160,\"start\":4157},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4163,\"start\":4160},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4166,\"start\":4163},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4169,\"start\":4166},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4172,\"start\":4169},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4175,\"start\":4172},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4178,\"start\":4175},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4181,\"start\":4178},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4184,\"start\":4181},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4392,\"start\":4388},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8056,\"start\":8052},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8127,\"start\":8123},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8154,\"start\":8150},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8321,\"start\":8317},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8539,\"start\":8535},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8881,\"start\":8877},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9009,\"start\":9005},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9206,\"start\":9202},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9304,\"start\":9301},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9467,\"start\":9463},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9523,\"start\":9519},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9761,\"start\":9758},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9764,\"start\":9761},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10025,\"start\":10021},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10047,\"start\":10043},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10263,\"start\":10259},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10306,\"start\":10302},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10321,\"start\":10317},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10396,\"start\":10392},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10416,\"start\":10412},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10507,\"start\":10503},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10576,\"start\":10572},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10590,\"start\":10586},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12928,\"start\":12925},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12930,\"start\":12928},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12932,\"start\":12930},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12934,\"start\":12932},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13211,\"start\":13207},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14228,\"start\":14224},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15362,\"start\":15358},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15456,\"start\":15453},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15703,\"start\":15700},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15873,\"start\":15869},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16251,\"start\":16247},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18555,\"start\":18551},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18570,\"start\":18566},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18599,\"start\":18595},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18672,\"start\":18668},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18675,\"start\":18672},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18906,\"start\":18902},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22234,\"start\":22231},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22252,\"start\":22249},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22268,\"start\":22264},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22282,\"start\":22278},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23194,\"start\":23191},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23197,\"start\":23194},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23323,\"start\":23319},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27610,\"start\":27606},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30171,\"start\":30167},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30909,\"start\":30905}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28739,\"start\":28624},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29536,\"start\":28740},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29650,\"start\":29537},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29911,\"start\":29651},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29970,\"start\":29912},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30334,\"start\":29971},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30448,\"start\":30335},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30502,\"start\":30449},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30779,\"start\":30503},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31028,\"start\":30780}]", "paragraph": "[{\"end\":2757,\"start\":2427},{\"end\":4657,\"start\":2759},{\"end\":6261,\"start\":4659},{\"end\":6722,\"start\":6263},{\"end\":7042,\"start\":6724},{\"end\":7846,\"start\":7044},{\"end\":8784,\"start\":7892},{\"end\":9875,\"start\":8794},{\"end\":10959,\"start\":9906},{\"end\":11891,\"start\":10997},{\"end\":13059,\"start\":11893},{\"end\":13600,\"start\":13102},{\"end\":13913,\"start\":13602},{\"end\":15130,\"start\":13945},{\"end\":15542,\"start\":15206},{\"end\":15650,\"start\":15603},{\"end\":16006,\"start\":15652},{\"end\":17320,\"start\":16050},{\"end\":18479,\"start\":17397},{\"end\":18741,\"start\":18502},{\"end\":19426,\"start\":18811},{\"end\":20754,\"start\":19550},{\"end\":20845,\"start\":20811},{\"end\":20926,\"start\":20883},{\"end\":21179,\"start\":20928},{\"end\":21848,\"start\":21215},{\"end\":21965,\"start\":21891},{\"end\":22119,\"start\":21981},{\"end\":23198,\"start\":22152},{\"end\":23467,\"start\":23200},{\"end\":23894,\"start\":23490},{\"end\":25077,\"start\":23896},{\"end\":25494,\"start\":25079},{\"end\":25943,\"start\":25513},{\"end\":27273,\"start\":25945},{\"end\":27376,\"start\":27275},{\"end\":27857,\"start\":27397},{\"end\":28623,\"start\":27872}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15205,\"start\":15131},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15602,\"start\":15543},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17396,\"start\":17321},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18810,\"start\":18742},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19521,\"start\":19427},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20810,\"start\":20755},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20882,\"start\":20846},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21214,\"start\":21180},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21890,\"start\":21849}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25691,\"start\":25684},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25978,\"start\":25971},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26695,\"start\":26688},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27089,\"start\":27082},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":27374,\"start\":27367},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27632,\"start\":27625}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2425,\"start\":2413},{\"attributes\":{\"n\":\"2\"},\"end\":7861,\"start\":7849},{\"attributes\":{\"n\":\"2.1\"},\"end\":7890,\"start\":7864},{\"end\":8792,\"start\":8787},{\"end\":9883,\"start\":9878},{\"attributes\":{\"n\":\"2.2\"},\"end\":9904,\"start\":9886},{\"attributes\":{\"n\":\"3\"},\"end\":10971,\"start\":10962},{\"attributes\":{\"n\":\"3.1\"},\"end\":10995,\"start\":10974},{\"attributes\":{\"n\":\"3.2\"},\"end\":13100,\"start\":13062},{\"attributes\":{\"n\":\"3.3\"},\"end\":13943,\"start\":13916},{\"attributes\":{\"n\":\"3.4\"},\"end\":16048,\"start\":16009},{\"attributes\":{\"n\":\"3.5\"},\"end\":18500,\"start\":18482},{\"attributes\":{\"n\":\"4\"},\"end\":19548,\"start\":19523},{\"attributes\":{\"n\":\"5\"},\"end\":21979,\"start\":21968},{\"attributes\":{\"n\":\"5.1\"},\"end\":22150,\"start\":22122},{\"attributes\":{\"n\":\"5.2\"},\"end\":23488,\"start\":23470},{\"attributes\":{\"n\":\"5.3\"},\"end\":25511,\"start\":25497},{\"end\":27395,\"start\":27379},{\"attributes\":{\"n\":\"6\"},\"end\":27870,\"start\":27860},{\"end\":28633,\"start\":28625},{\"end\":28749,\"start\":28741},{\"end\":29546,\"start\":29538},{\"end\":29660,\"start\":29652},{\"end\":29921,\"start\":29913},{\"end\":29988,\"start\":29972},{\"end\":30345,\"start\":30336},{\"end\":30457,\"start\":30450},{\"end\":30513,\"start\":30504},{\"end\":30790,\"start\":30781}]", "table": "[{\"end\":30448,\"start\":30431},{\"end\":30502,\"start\":30459},{\"end\":30779,\"start\":30620},{\"end\":31028,\"start\":30910}]", "figure_caption": "[{\"end\":28739,\"start\":28635},{\"end\":29536,\"start\":28751},{\"end\":29650,\"start\":29548},{\"end\":29911,\"start\":29662},{\"end\":29970,\"start\":29923},{\"end\":30334,\"start\":29991},{\"end\":30431,\"start\":30347},{\"end\":30620,\"start\":30515},{\"end\":30910,\"start\":30792}]", "figure_ref": "[{\"end\":3197,\"start\":3191},{\"end\":4895,\"start\":4889},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5677,\"start\":5671},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11911,\"start\":11905},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14669,\"start\":14662},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14872,\"start\":14865},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16333,\"start\":16327},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18287,\"start\":18281},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20432,\"start\":20426},{\"end\":24787,\"start\":24781},{\"end\":25411,\"start\":25405},{\"end\":27173,\"start\":27167},{\"end\":27296,\"start\":27290}]", "bib_author_first_name": "[{\"end\":31283,\"start\":31282},{\"end\":31293,\"start\":31292},{\"end\":31300,\"start\":31299},{\"end\":31302,\"start\":31301},{\"end\":31311,\"start\":31310},{\"end\":31546,\"start\":31545},{\"end\":31548,\"start\":31547},{\"end\":31556,\"start\":31555},{\"end\":31568,\"start\":31567},{\"end\":31921,\"start\":31920},{\"end\":32148,\"start\":32147},{\"end\":32157,\"start\":32156},{\"end\":32164,\"start\":32163},{\"end\":32178,\"start\":32177},{\"end\":32188,\"start\":32187},{\"end\":32199,\"start\":32198},{\"end\":32208,\"start\":32207},{\"end\":32216,\"start\":32215},{\"end\":32224,\"start\":32223},{\"end\":32641,\"start\":32640},{\"end\":32643,\"start\":32642},{\"end\":32651,\"start\":32650},{\"end\":32657,\"start\":32656},{\"end\":32663,\"start\":32662},{\"end\":32670,\"start\":32669},{\"end\":32678,\"start\":32677},{\"end\":32968,\"start\":32967},{\"end\":32970,\"start\":32969},{\"end\":32979,\"start\":32978},{\"end\":32981,\"start\":32980},{\"end\":32989,\"start\":32988},{\"end\":32991,\"start\":32990},{\"end\":32999,\"start\":32998},{\"end\":33001,\"start\":33000},{\"end\":33008,\"start\":33007},{\"end\":33010,\"start\":33009},{\"end\":33017,\"start\":33016},{\"end\":33450,\"start\":33449},{\"end\":33459,\"start\":33458},{\"end\":33467,\"start\":33466},{\"end\":33475,\"start\":33474},{\"end\":33483,\"start\":33482},{\"end\":33778,\"start\":33777},{\"end\":33780,\"start\":33779},{\"end\":33789,\"start\":33788},{\"end\":33799,\"start\":33798},{\"end\":33809,\"start\":33808},{\"end\":34111,\"start\":34110},{\"end\":34120,\"start\":34119},{\"end\":34122,\"start\":34121},{\"end\":34136,\"start\":34135},{\"end\":34535,\"start\":34534},{\"end\":34542,\"start\":34541},{\"end\":34548,\"start\":34547},{\"end\":34557,\"start\":34556},{\"end\":34563,\"start\":34562},{\"end\":34572,\"start\":34571},{\"end\":34578,\"start\":34577},{\"end\":34900,\"start\":34899},{\"end\":34915,\"start\":34914},{\"end\":34924,\"start\":34923},{\"end\":34938,\"start\":34937},{\"end\":34953,\"start\":34952},{\"end\":34961,\"start\":34960},{\"end\":34976,\"start\":34975},{\"end\":34988,\"start\":34987},{\"end\":35000,\"start\":34999},{\"end\":35011,\"start\":35010},{\"end\":35442,\"start\":35441},{\"end\":35450,\"start\":35449},{\"end\":35460,\"start\":35459},{\"end\":35467,\"start\":35466},{\"end\":35469,\"start\":35468},{\"end\":35909,\"start\":35908},{\"end\":35918,\"start\":35917},{\"end\":35929,\"start\":35928},{\"end\":36238,\"start\":36237},{\"end\":36250,\"start\":36249},{\"end\":36518,\"start\":36517},{\"end\":36529,\"start\":36528},{\"end\":36549,\"start\":36548},{\"end\":36560,\"start\":36559},{\"end\":36906,\"start\":36905},{\"end\":36915,\"start\":36914},{\"end\":36924,\"start\":36923},{\"end\":36937,\"start\":36936},{\"end\":36947,\"start\":36946},{\"end\":37242,\"start\":37241},{\"end\":37251,\"start\":37250},{\"end\":37260,\"start\":37259},{\"end\":37267,\"start\":37266},{\"end\":37275,\"start\":37274},{\"end\":37587,\"start\":37586},{\"end\":37594,\"start\":37593},{\"end\":37600,\"start\":37599},{\"end\":37609,\"start\":37608},{\"end\":37618,\"start\":37617},{\"end\":37626,\"start\":37625},{\"end\":37632,\"start\":37631},{\"end\":37638,\"start\":37637},{\"end\":38141,\"start\":38140},{\"end\":38154,\"start\":38153},{\"end\":38449,\"start\":38448},{\"end\":38451,\"start\":38450},{\"end\":38461,\"start\":38460},{\"end\":38669,\"start\":38668},{\"end\":38678,\"start\":38677},{\"end\":38691,\"start\":38690},{\"end\":38706,\"start\":38705},{\"end\":38717,\"start\":38716},{\"end\":39093,\"start\":39092},{\"end\":39100,\"start\":39099},{\"end\":39109,\"start\":39108},{\"end\":39116,\"start\":39115},{\"end\":39123,\"start\":39122},{\"end\":39125,\"start\":39124},{\"end\":39526,\"start\":39525},{\"end\":39532,\"start\":39531},{\"end\":39541,\"start\":39540},{\"end\":39548,\"start\":39547},{\"end\":39559,\"start\":39558},{\"end\":39873,\"start\":39872},{\"end\":39880,\"start\":39879},{\"end\":39887,\"start\":39886},{\"end\":39894,\"start\":39893},{\"end\":39900,\"start\":39899},{\"end\":39907,\"start\":39906},{\"end\":39916,\"start\":39915},{\"end\":39923,\"start\":39922},{\"end\":40306,\"start\":40305},{\"end\":40308,\"start\":40307},{\"end\":40529,\"start\":40528},{\"end\":40540,\"start\":40539},{\"end\":40548,\"start\":40547},{\"end\":40559,\"start\":40558},{\"end\":40572,\"start\":40571},{\"end\":41057,\"start\":41056},{\"end\":41067,\"start\":41066},{\"end\":41082,\"start\":41081},{\"end\":41498,\"start\":41497},{\"end\":41506,\"start\":41505},{\"end\":41513,\"start\":41512},{\"end\":41520,\"start\":41519},{\"end\":41528,\"start\":41527},{\"end\":41894,\"start\":41893},{\"end\":41902,\"start\":41901},{\"end\":41908,\"start\":41907},{\"end\":41916,\"start\":41915},{\"end\":41918,\"start\":41917},{\"end\":41927,\"start\":41926},{\"end\":41936,\"start\":41935},{\"end\":42380,\"start\":42379},{\"end\":42382,\"start\":42381},{\"end\":42388,\"start\":42387},{\"end\":42837,\"start\":42836},{\"end\":42844,\"start\":42843},{\"end\":42846,\"start\":42845},{\"end\":42855,\"start\":42854},{\"end\":42862,\"start\":42861},{\"end\":42864,\"start\":42863},{\"end\":43342,\"start\":43341},{\"end\":43349,\"start\":43348},{\"end\":43356,\"start\":43355},{\"end\":43358,\"start\":43357},{\"end\":43813,\"start\":43812},{\"end\":43823,\"start\":43822},{\"end\":43832,\"start\":43831},{\"end\":44230,\"start\":44229},{\"end\":44232,\"start\":44231},{\"end\":44240,\"start\":44239},{\"end\":44242,\"start\":44241},{\"end\":44249,\"start\":44248},{\"end\":44256,\"start\":44255},{\"end\":44258,\"start\":44257},{\"end\":44266,\"start\":44265},{\"end\":44268,\"start\":44267},{\"end\":44718,\"start\":44717},{\"end\":44726,\"start\":44725},{\"end\":44733,\"start\":44732},{\"end\":44740,\"start\":44739},{\"end\":44971,\"start\":44970},{\"end\":44977,\"start\":44976},{\"end\":44985,\"start\":44984},{\"end\":44996,\"start\":44995},{\"end\":45003,\"start\":45002},{\"end\":45010,\"start\":45009},{\"end\":45018,\"start\":45017},{\"end\":45418,\"start\":45417},{\"end\":45427,\"start\":45426},{\"end\":45705,\"start\":45704},{\"end\":45711,\"start\":45710},{\"end\":45720,\"start\":45719},{\"end\":46092,\"start\":46091},{\"end\":46099,\"start\":46098},{\"end\":46105,\"start\":46104},{\"end\":46113,\"start\":46112},{\"end\":46122,\"start\":46121},{\"end\":46128,\"start\":46127},{\"end\":46336,\"start\":46335},{\"end\":46343,\"start\":46342},{\"end\":46351,\"start\":46350},{\"end\":46357,\"start\":46356},{\"end\":46366,\"start\":46365},{\"end\":46372,\"start\":46371},{\"end\":46378,\"start\":46377},{\"end\":46726,\"start\":46725},{\"end\":46738,\"start\":46737},{\"end\":47175,\"start\":47174},{\"end\":47183,\"start\":47182},{\"end\":47190,\"start\":47189},{\"end\":47197,\"start\":47196},{\"end\":47205,\"start\":47204},{\"end\":47211,\"start\":47210},{\"end\":47692,\"start\":47691},{\"end\":47699,\"start\":47698},{\"end\":47701,\"start\":47700},{\"end\":47708,\"start\":47707},{\"end\":47710,\"start\":47709},{\"end\":48168,\"start\":48167},{\"end\":48175,\"start\":48174},{\"end\":48181,\"start\":48180},{\"end\":48187,\"start\":48186},{\"end\":48193,\"start\":48192},{\"end\":48201,\"start\":48200},{\"end\":48522,\"start\":48521},{\"end\":48533,\"start\":48532},{\"end\":48547,\"start\":48546},{\"end\":48559,\"start\":48558}]", "bib_author_last_name": "[{\"end\":31290,\"start\":31284},{\"end\":31297,\"start\":31294},{\"end\":31308,\"start\":31303},{\"end\":31320,\"start\":31312},{\"end\":31553,\"start\":31549},{\"end\":31565,\"start\":31557},{\"end\":31574,\"start\":31569},{\"end\":31926,\"start\":31922},{\"end\":32154,\"start\":32149},{\"end\":32161,\"start\":32158},{\"end\":32175,\"start\":32165},{\"end\":32185,\"start\":32179},{\"end\":32196,\"start\":32189},{\"end\":32205,\"start\":32200},{\"end\":32213,\"start\":32209},{\"end\":32221,\"start\":32217},{\"end\":32230,\"start\":32225},{\"end\":32648,\"start\":32644},{\"end\":32654,\"start\":32652},{\"end\":32660,\"start\":32658},{\"end\":32667,\"start\":32664},{\"end\":32675,\"start\":32671},{\"end\":32682,\"start\":32679},{\"end\":32976,\"start\":32971},{\"end\":32986,\"start\":32982},{\"end\":32996,\"start\":32992},{\"end\":33005,\"start\":33002},{\"end\":33014,\"start\":33011},{\"end\":33021,\"start\":33018},{\"end\":33456,\"start\":33451},{\"end\":33464,\"start\":33460},{\"end\":33472,\"start\":33468},{\"end\":33480,\"start\":33476},{\"end\":33488,\"start\":33484},{\"end\":33786,\"start\":33781},{\"end\":33796,\"start\":33790},{\"end\":33806,\"start\":33800},{\"end\":33817,\"start\":33810},{\"end\":34117,\"start\":34112},{\"end\":34133,\"start\":34123},{\"end\":34143,\"start\":34137},{\"end\":34539,\"start\":34536},{\"end\":34545,\"start\":34543},{\"end\":34554,\"start\":34549},{\"end\":34560,\"start\":34558},{\"end\":34569,\"start\":34564},{\"end\":34575,\"start\":34573},{\"end\":34582,\"start\":34579},{\"end\":34912,\"start\":34901},{\"end\":34921,\"start\":34916},{\"end\":34935,\"start\":34925},{\"end\":34950,\"start\":34939},{\"end\":34958,\"start\":34954},{\"end\":34973,\"start\":34962},{\"end\":34985,\"start\":34977},{\"end\":34997,\"start\":34989},{\"end\":35008,\"start\":35001},{\"end\":35017,\"start\":35012},{\"end\":35447,\"start\":35443},{\"end\":35457,\"start\":35451},{\"end\":35464,\"start\":35461},{\"end\":35475,\"start\":35470},{\"end\":35915,\"start\":35910},{\"end\":35926,\"start\":35919},{\"end\":35936,\"start\":35930},{\"end\":36247,\"start\":36239},{\"end\":36259,\"start\":36251},{\"end\":36526,\"start\":36519},{\"end\":36546,\"start\":36530},{\"end\":36557,\"start\":36550},{\"end\":36571,\"start\":36561},{\"end\":36912,\"start\":36907},{\"end\":36921,\"start\":36916},{\"end\":36934,\"start\":36925},{\"end\":36944,\"start\":36938},{\"end\":36956,\"start\":36948},{\"end\":37248,\"start\":37243},{\"end\":37257,\"start\":37252},{\"end\":37264,\"start\":37261},{\"end\":37272,\"start\":37268},{\"end\":37281,\"start\":37276},{\"end\":37591,\"start\":37588},{\"end\":37597,\"start\":37595},{\"end\":37606,\"start\":37601},{\"end\":37615,\"start\":37610},{\"end\":37623,\"start\":37619},{\"end\":37629,\"start\":37627},{\"end\":37635,\"start\":37633},{\"end\":37642,\"start\":37639},{\"end\":38151,\"start\":38142},{\"end\":38163,\"start\":38155},{\"end\":38458,\"start\":38452},{\"end\":38464,\"start\":38462},{\"end\":38675,\"start\":38670},{\"end\":38688,\"start\":38679},{\"end\":38703,\"start\":38692},{\"end\":38714,\"start\":38707},{\"end\":38723,\"start\":38718},{\"end\":39097,\"start\":39094},{\"end\":39106,\"start\":39101},{\"end\":39113,\"start\":39110},{\"end\":39120,\"start\":39117},{\"end\":39130,\"start\":39126},{\"end\":39529,\"start\":39527},{\"end\":39538,\"start\":39533},{\"end\":39545,\"start\":39542},{\"end\":39556,\"start\":39549},{\"end\":39568,\"start\":39560},{\"end\":39877,\"start\":39874},{\"end\":39884,\"start\":39881},{\"end\":39891,\"start\":39888},{\"end\":39897,\"start\":39895},{\"end\":39904,\"start\":39901},{\"end\":39913,\"start\":39908},{\"end\":39920,\"start\":39917},{\"end\":39927,\"start\":39924},{\"end\":40316,\"start\":40309},{\"end\":40537,\"start\":40530},{\"end\":40545,\"start\":40541},{\"end\":40556,\"start\":40549},{\"end\":40569,\"start\":40560},{\"end\":40581,\"start\":40573},{\"end\":41064,\"start\":41058},{\"end\":41079,\"start\":41068},{\"end\":41089,\"start\":41083},{\"end\":41503,\"start\":41499},{\"end\":41510,\"start\":41507},{\"end\":41517,\"start\":41514},{\"end\":41525,\"start\":41521},{\"end\":41533,\"start\":41529},{\"end\":41899,\"start\":41895},{\"end\":41905,\"start\":41903},{\"end\":41913,\"start\":41909},{\"end\":41924,\"start\":41919},{\"end\":41933,\"start\":41928},{\"end\":41947,\"start\":41937},{\"end\":42385,\"start\":42383},{\"end\":42396,\"start\":42389},{\"end\":42841,\"start\":42838},{\"end\":42852,\"start\":42847},{\"end\":42859,\"start\":42856},{\"end\":42869,\"start\":42865},{\"end\":43346,\"start\":43343},{\"end\":43353,\"start\":43350},{\"end\":43363,\"start\":43359},{\"end\":43820,\"start\":43814},{\"end\":43829,\"start\":43824},{\"end\":43840,\"start\":43833},{\"end\":44237,\"start\":44233},{\"end\":44246,\"start\":44243},{\"end\":44253,\"start\":44250},{\"end\":44263,\"start\":44259},{\"end\":44273,\"start\":44269},{\"end\":44723,\"start\":44719},{\"end\":44730,\"start\":44727},{\"end\":44737,\"start\":44734},{\"end\":44744,\"start\":44741},{\"end\":44974,\"start\":44972},{\"end\":44982,\"start\":44978},{\"end\":44993,\"start\":44986},{\"end\":45000,\"start\":44997},{\"end\":45007,\"start\":45004},{\"end\":45015,\"start\":45011},{\"end\":45024,\"start\":45019},{\"end\":45424,\"start\":45419},{\"end\":45435,\"start\":45428},{\"end\":45708,\"start\":45706},{\"end\":45717,\"start\":45712},{\"end\":45724,\"start\":45721},{\"end\":46096,\"start\":46093},{\"end\":46102,\"start\":46100},{\"end\":46110,\"start\":46106},{\"end\":46119,\"start\":46114},{\"end\":46125,\"start\":46123},{\"end\":46133,\"start\":46129},{\"end\":46340,\"start\":46337},{\"end\":46348,\"start\":46344},{\"end\":46354,\"start\":46352},{\"end\":46363,\"start\":46358},{\"end\":46369,\"start\":46367},{\"end\":46375,\"start\":46373},{\"end\":46383,\"start\":46379},{\"end\":46735,\"start\":46727},{\"end\":46746,\"start\":46739},{\"end\":47180,\"start\":47176},{\"end\":47187,\"start\":47184},{\"end\":47194,\"start\":47191},{\"end\":47202,\"start\":47198},{\"end\":47208,\"start\":47206},{\"end\":47214,\"start\":47212},{\"end\":47696,\"start\":47693},{\"end\":47705,\"start\":47702},{\"end\":47715,\"start\":47711},{\"end\":48172,\"start\":48169},{\"end\":48178,\"start\":48176},{\"end\":48184,\"start\":48182},{\"end\":48190,\"start\":48188},{\"end\":48198,\"start\":48194},{\"end\":48205,\"start\":48202},{\"end\":48530,\"start\":48523},{\"end\":48544,\"start\":48534},{\"end\":48556,\"start\":48548},{\"end\":48565,\"start\":48560}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1702.01105\",\"id\":\"b0\"},\"end\":31496,\"start\":31224},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":227227779},\"end\":31918,\"start\":31498},{\"attributes\":{\"doi\":\"arXiv:1901.09402\",\"id\":\"b2\"},\"end\":32082,\"start\":31920},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":21435690},\"end\":32566,\"start\":32084},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":224704488},\"end\":32895,\"start\":32568},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":46937879},\"end\":33405,\"start\":32897},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":221848568},\"end\":33759,\"start\":33407},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3525710},\"end\":34002,\"start\":33761},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":51883314},\"end\":34497,\"start\":34004},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":4028864},\"end\":34897,\"start\":34499},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b10\"},\"end\":35387,\"start\":34899},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":209414882},\"end\":35831,\"start\":35389},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2255738},\"end\":36161,\"start\":35833},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53111438},\"end\":36450,\"start\":36163},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4065882},\"end\":36903,\"start\":36452},{\"attributes\":{\"doi\":\"arXiv:1901.02039\",\"id\":\"b15\"},\"end\":37173,\"start\":36905},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":231846497},\"end\":37500,\"start\":37175},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":219964527},\"end\":38058,\"start\":37502},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53550083},\"end\":38402,\"start\":38060},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b19\"},\"end\":38598,\"start\":38404},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11091110},\"end\":39006,\"start\":38600},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":198165883},\"end\":39523,\"start\":39008},{\"attributes\":{\"doi\":\"arXiv:2104.05707\",\"id\":\"b22\"},\"end\":39797,\"start\":39525},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":232352874},\"end\":40303,\"start\":39799},{\"attributes\":{\"id\":\"b24\"},\"end\":40422,\"start\":40305},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":235679277},\"end\":41012,\"start\":40424},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":232352612},\"end\":41402,\"start\":41014},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":236265764},\"end\":41838,\"start\":41404},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":20416090},\"end\":42314,\"start\":41840},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":54457562},\"end\":42742,\"start\":42316},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":58004623},\"end\":43263,\"start\":42744},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":227126786},\"end\":43729,\"start\":43265},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":52086927},\"end\":44162,\"start\":43731},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":219963198},\"end\":44652,\"start\":44164},{\"attributes\":{\"doi\":\"arXiv:2106.03106\",\"id\":\"b34\"},\"end\":44914,\"start\":44654},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":232417787},\"end\":45374,\"start\":44916},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4560593},\"end\":45702,\"start\":45376},{\"attributes\":{\"id\":\"b37\"},\"end\":46015,\"start\":45704},{\"attributes\":{\"doi\":\"arXiv:2203.09855\",\"id\":\"b38\"},\"end\":46333,\"start\":46017},{\"attributes\":{\"doi\":\"arXiv:2107.13802\",\"id\":\"b39\"},\"end\":46643,\"start\":46335},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1505637},\"end\":47112,\"start\":46645},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":232307700},\"end\":47554,\"start\":47114},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":237592947},\"end\":48091,\"start\":47556},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":222208633},\"end\":48452,\"start\":48093},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":50787222},\"end\":48895,\"start\":48454}]", "bib_title": "[{\"end\":31543,\"start\":31498},{\"end\":32145,\"start\":32084},{\"end\":32638,\"start\":32568},{\"end\":32965,\"start\":32897},{\"end\":33447,\"start\":33407},{\"end\":33775,\"start\":33761},{\"end\":34108,\"start\":34004},{\"end\":34532,\"start\":34499},{\"end\":35439,\"start\":35389},{\"end\":35906,\"start\":35833},{\"end\":36235,\"start\":36163},{\"end\":36515,\"start\":36452},{\"end\":37239,\"start\":37175},{\"end\":37584,\"start\":37502},{\"end\":38138,\"start\":38060},{\"end\":38666,\"start\":38600},{\"end\":39090,\"start\":39008},{\"end\":39870,\"start\":39799},{\"end\":40526,\"start\":40424},{\"end\":41054,\"start\":41014},{\"end\":41495,\"start\":41404},{\"end\":41891,\"start\":41840},{\"end\":42377,\"start\":42316},{\"end\":42834,\"start\":42744},{\"end\":43339,\"start\":43265},{\"end\":43810,\"start\":43731},{\"end\":44227,\"start\":44164},{\"end\":44968,\"start\":44916},{\"end\":45415,\"start\":45376},{\"end\":46723,\"start\":46645},{\"end\":47172,\"start\":47114},{\"end\":47689,\"start\":47556},{\"end\":48165,\"start\":48093},{\"end\":48519,\"start\":48454}]", "bib_author": "[{\"end\":31292,\"start\":31282},{\"end\":31299,\"start\":31292},{\"end\":31310,\"start\":31299},{\"end\":31322,\"start\":31310},{\"end\":31555,\"start\":31545},{\"end\":31567,\"start\":31555},{\"end\":31576,\"start\":31567},{\"end\":31928,\"start\":31920},{\"end\":32156,\"start\":32147},{\"end\":32163,\"start\":32156},{\"end\":32177,\"start\":32163},{\"end\":32187,\"start\":32177},{\"end\":32198,\"start\":32187},{\"end\":32207,\"start\":32198},{\"end\":32215,\"start\":32207},{\"end\":32223,\"start\":32215},{\"end\":32232,\"start\":32223},{\"end\":32650,\"start\":32640},{\"end\":32656,\"start\":32650},{\"end\":32662,\"start\":32656},{\"end\":32669,\"start\":32662},{\"end\":32677,\"start\":32669},{\"end\":32684,\"start\":32677},{\"end\":32978,\"start\":32967},{\"end\":32988,\"start\":32978},{\"end\":32998,\"start\":32988},{\"end\":33007,\"start\":32998},{\"end\":33016,\"start\":33007},{\"end\":33023,\"start\":33016},{\"end\":33458,\"start\":33449},{\"end\":33466,\"start\":33458},{\"end\":33474,\"start\":33466},{\"end\":33482,\"start\":33474},{\"end\":33490,\"start\":33482},{\"end\":33788,\"start\":33777},{\"end\":33798,\"start\":33788},{\"end\":33808,\"start\":33798},{\"end\":33819,\"start\":33808},{\"end\":34119,\"start\":34110},{\"end\":34135,\"start\":34119},{\"end\":34145,\"start\":34135},{\"end\":34541,\"start\":34534},{\"end\":34547,\"start\":34541},{\"end\":34556,\"start\":34547},{\"end\":34562,\"start\":34556},{\"end\":34571,\"start\":34562},{\"end\":34577,\"start\":34571},{\"end\":34584,\"start\":34577},{\"end\":34914,\"start\":34899},{\"end\":34923,\"start\":34914},{\"end\":34937,\"start\":34923},{\"end\":34952,\"start\":34937},{\"end\":34960,\"start\":34952},{\"end\":34975,\"start\":34960},{\"end\":34987,\"start\":34975},{\"end\":34999,\"start\":34987},{\"end\":35010,\"start\":34999},{\"end\":35019,\"start\":35010},{\"end\":35449,\"start\":35441},{\"end\":35459,\"start\":35449},{\"end\":35466,\"start\":35459},{\"end\":35477,\"start\":35466},{\"end\":35917,\"start\":35908},{\"end\":35928,\"start\":35917},{\"end\":35938,\"start\":35928},{\"end\":36249,\"start\":36237},{\"end\":36261,\"start\":36249},{\"end\":36528,\"start\":36517},{\"end\":36548,\"start\":36528},{\"end\":36559,\"start\":36548},{\"end\":36573,\"start\":36559},{\"end\":36914,\"start\":36905},{\"end\":36923,\"start\":36914},{\"end\":36936,\"start\":36923},{\"end\":36946,\"start\":36936},{\"end\":36958,\"start\":36946},{\"end\":37250,\"start\":37241},{\"end\":37259,\"start\":37250},{\"end\":37266,\"start\":37259},{\"end\":37274,\"start\":37266},{\"end\":37283,\"start\":37274},{\"end\":37593,\"start\":37586},{\"end\":37599,\"start\":37593},{\"end\":37608,\"start\":37599},{\"end\":37617,\"start\":37608},{\"end\":37625,\"start\":37617},{\"end\":37631,\"start\":37625},{\"end\":37637,\"start\":37631},{\"end\":37644,\"start\":37637},{\"end\":38153,\"start\":38140},{\"end\":38165,\"start\":38153},{\"end\":38460,\"start\":38448},{\"end\":38466,\"start\":38460},{\"end\":38677,\"start\":38668},{\"end\":38690,\"start\":38677},{\"end\":38705,\"start\":38690},{\"end\":38716,\"start\":38705},{\"end\":38725,\"start\":38716},{\"end\":39099,\"start\":39092},{\"end\":39108,\"start\":39099},{\"end\":39115,\"start\":39108},{\"end\":39122,\"start\":39115},{\"end\":39132,\"start\":39122},{\"end\":39531,\"start\":39525},{\"end\":39540,\"start\":39531},{\"end\":39547,\"start\":39540},{\"end\":39558,\"start\":39547},{\"end\":39570,\"start\":39558},{\"end\":39879,\"start\":39872},{\"end\":39886,\"start\":39879},{\"end\":39893,\"start\":39886},{\"end\":39899,\"start\":39893},{\"end\":39906,\"start\":39899},{\"end\":39915,\"start\":39906},{\"end\":39922,\"start\":39915},{\"end\":39929,\"start\":39922},{\"end\":40318,\"start\":40305},{\"end\":40539,\"start\":40528},{\"end\":40547,\"start\":40539},{\"end\":40558,\"start\":40547},{\"end\":40571,\"start\":40558},{\"end\":40583,\"start\":40571},{\"end\":41066,\"start\":41056},{\"end\":41081,\"start\":41066},{\"end\":41091,\"start\":41081},{\"end\":41505,\"start\":41497},{\"end\":41512,\"start\":41505},{\"end\":41519,\"start\":41512},{\"end\":41527,\"start\":41519},{\"end\":41535,\"start\":41527},{\"end\":41901,\"start\":41893},{\"end\":41907,\"start\":41901},{\"end\":41915,\"start\":41907},{\"end\":41926,\"start\":41915},{\"end\":41935,\"start\":41926},{\"end\":41949,\"start\":41935},{\"end\":42387,\"start\":42379},{\"end\":42398,\"start\":42387},{\"end\":42843,\"start\":42836},{\"end\":42854,\"start\":42843},{\"end\":42861,\"start\":42854},{\"end\":42871,\"start\":42861},{\"end\":43348,\"start\":43341},{\"end\":43355,\"start\":43348},{\"end\":43365,\"start\":43355},{\"end\":43822,\"start\":43812},{\"end\":43831,\"start\":43822},{\"end\":43842,\"start\":43831},{\"end\":44239,\"start\":44229},{\"end\":44248,\"start\":44239},{\"end\":44255,\"start\":44248},{\"end\":44265,\"start\":44255},{\"end\":44275,\"start\":44265},{\"end\":44725,\"start\":44717},{\"end\":44732,\"start\":44725},{\"end\":44739,\"start\":44732},{\"end\":44746,\"start\":44739},{\"end\":44976,\"start\":44970},{\"end\":44984,\"start\":44976},{\"end\":44995,\"start\":44984},{\"end\":45002,\"start\":44995},{\"end\":45009,\"start\":45002},{\"end\":45017,\"start\":45009},{\"end\":45026,\"start\":45017},{\"end\":45426,\"start\":45417},{\"end\":45437,\"start\":45426},{\"end\":45710,\"start\":45704},{\"end\":45719,\"start\":45710},{\"end\":45726,\"start\":45719},{\"end\":46098,\"start\":46091},{\"end\":46104,\"start\":46098},{\"end\":46112,\"start\":46104},{\"end\":46121,\"start\":46112},{\"end\":46127,\"start\":46121},{\"end\":46135,\"start\":46127},{\"end\":46342,\"start\":46335},{\"end\":46350,\"start\":46342},{\"end\":46356,\"start\":46350},{\"end\":46365,\"start\":46356},{\"end\":46371,\"start\":46365},{\"end\":46377,\"start\":46371},{\"end\":46385,\"start\":46377},{\"end\":46737,\"start\":46725},{\"end\":46748,\"start\":46737},{\"end\":47182,\"start\":47174},{\"end\":47189,\"start\":47182},{\"end\":47196,\"start\":47189},{\"end\":47204,\"start\":47196},{\"end\":47210,\"start\":47204},{\"end\":47216,\"start\":47210},{\"end\":47698,\"start\":47691},{\"end\":47707,\"start\":47698},{\"end\":47717,\"start\":47707},{\"end\":48174,\"start\":48167},{\"end\":48180,\"start\":48174},{\"end\":48186,\"start\":48180},{\"end\":48192,\"start\":48186},{\"end\":48200,\"start\":48192},{\"end\":48207,\"start\":48200},{\"end\":48532,\"start\":48521},{\"end\":48546,\"start\":48532},{\"end\":48558,\"start\":48546},{\"end\":48567,\"start\":48558}]", "bib_venue": "[{\"end\":31280,\"start\":31224},{\"end\":31657,\"start\":31576},{\"end\":31980,\"start\":31944},{\"end\":32280,\"start\":32232},{\"end\":32714,\"start\":32684},{\"end\":33100,\"start\":33023},{\"end\":33558,\"start\":33490},{\"end\":33871,\"start\":33819},{\"end\":34209,\"start\":34145},{\"end\":34651,\"start\":34584},{\"end\":35109,\"start\":35035},{\"end\":35558,\"start\":35477},{\"end\":35987,\"start\":35938},{\"end\":36291,\"start\":36261},{\"end\":36637,\"start\":36573},{\"end\":37010,\"start\":36974},{\"end\":37319,\"start\":37283},{\"end\":37725,\"start\":37644},{\"end\":38209,\"start\":38165},{\"end\":38446,\"start\":38404},{\"end\":38780,\"start\":38725},{\"end\":39213,\"start\":39132},{\"end\":39636,\"start\":39586},{\"end\":40000,\"start\":39929},{\"end\":40358,\"start\":40318},{\"end\":40664,\"start\":40583},{\"end\":41162,\"start\":41091},{\"end\":41599,\"start\":41535},{\"end\":42026,\"start\":41949},{\"end\":42479,\"start\":42398},{\"end\":42952,\"start\":42871},{\"end\":43446,\"start\":43365},{\"end\":43906,\"start\":43842},{\"end\":44356,\"start\":44275},{\"end\":44715,\"start\":44654},{\"end\":45097,\"start\":45026},{\"end\":45501,\"start\":45437},{\"end\":45852,\"start\":45726},{\"end\":46089,\"start\":46017},{\"end\":46461,\"start\":46401},{\"end\":46834,\"start\":46748},{\"end\":47287,\"start\":47216},{\"end\":47778,\"start\":47717},{\"end\":48259,\"start\":48207},{\"end\":48631,\"start\":48567},{\"end\":31725,\"start\":31659},{\"end\":33164,\"start\":33102},{\"end\":34260,\"start\":34211},{\"end\":34705,\"start\":34653},{\"end\":35626,\"start\":35560},{\"end\":36688,\"start\":36639},{\"end\":37793,\"start\":37727},{\"end\":39281,\"start\":39215},{\"end\":40058,\"start\":40002},{\"end\":40732,\"start\":40666},{\"end\":41220,\"start\":41164},{\"end\":42090,\"start\":42028},{\"end\":42547,\"start\":42481},{\"end\":43020,\"start\":42954},{\"end\":43514,\"start\":43448},{\"end\":43957,\"start\":43908},{\"end\":44424,\"start\":44358},{\"end\":45155,\"start\":45099},{\"end\":45552,\"start\":45503},{\"end\":46907,\"start\":46836},{\"end\":47345,\"start\":47289},{\"end\":47826,\"start\":47780},{\"end\":48682,\"start\":48633}]"}}}, "year": 2023, "month": 12, "day": 17}