{"id": 14501274, "updated": "2023-07-05 13:31:06.714", "metadata": {"title": "On the automatic classification of app reviews", "authors": "[{\"first\":\"Walid\",\"last\":\"Maalej\",\"middle\":[]},{\"first\":\"Zijad\",\"last\":\"Kurtanovi\u0107\",\"middle\":[]},{\"first\":\"Hadeer\",\"last\":\"Nabil\",\"middle\":[]},{\"first\":\"Christoph\",\"last\":\"Stanik\",\"middle\":[]}]", "venue": "Requirements Engineering", "journal": "Requirements Engineering", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "App stores like Google Play and Apple AppStore have over 3\u00a0million apps covering nearly every kind of software and service. Billions of users regularly download, use, and review these apps. Recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. The majority of the reviews, however, is rather non-informative just praising the app and repeating to the star ratings in words. This paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and text ratings. For this, we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques. We conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching. We found that metadata alone results in a poor classification accuracy. When combined with simple text classification and natural language preprocessing of the text\u2014particularly with bigrams and lemmatization\u2014the classification precision for all review types got up to 88\u201392\u00a0% and the recall up to 90\u201399\u00a0%. Multiple binary classifiers outperformed single multiclass classifiers. Our results inspired the design of a review analytics tool, which should help app vendors and developers deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders. We describe the tool main features and summarize nine interviews with practitioners on how review analytics tools including ours could be used in practice.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2605032615", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/re/MaalejKNS16", "doi": "10.1007/s00766-016-0251-9"}}, "content": {"source": {"pdf_hash": "2d1426579d84ba5076f2faf1b687bea8c112fac3", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5d25f14567750fa0cac42e03404781dbdec822ae", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2d1426579d84ba5076f2faf1b687bea8c112fac3.txt", "contents": "\nOn the Automatic Classification of App Reviews\n\n\nWalid Maalej \n\u00b7 Zijad \nKurtanovi\u0107 \u00b7 Hadeer \nChristoph Stanik \nWalid Maalej \nZijad Kurtanovi\u0107 \nChristoph Stanik \nHadeer Nabil nabil.hadeer@gmail.com \n\nDepartment of Informatics\nUniversity of Hamburg\nHamburgGermany\n\n\nGerman University of Cairo\n\n\nOn the Automatic Classification of App Reviews\n10.1007/s00766-016-0251-9Received: 14. November 2016 / Accepted: 26. April 2016The final publication is available at Springer via 2 Walid Maalej et al.User Feedback \u00b7 Review Analytics \u00b7 Software Analytics \u00b7 Machine Learning \u00b7 Natural Language Processing \u00b7 Data Driven Requirements Engineering\nApp stores like Google Play and Apple AppStore have over 3 Million apps covering nearly every kind of software and service. Billions of users regularly download, use, and review these apps. Recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. The majority of the reviews, however, is rather non-informative just praising the app and repeating to the star ratings in words. This paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and text ratings. For this, we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques. We conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching. We found that metadata alone results in a poor classification accuracy. When combined with simple text classification and natural language preprocessing of the text -particularly with Bigrams and lemmatization -the classification precision for all review types got up to 88-92% and the recall up to 90-99%. Multiple binary classifiers outperformed single multiclass classifiers. Our results inspired the design of a review analytics tool, which should help app vendors and developers deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders. We describe the tool main features and summarize 9 interviews with practitioners on how review analytics tools including ours could be used in practice.Nowadays it is hard to imagine a business or a service that does not have any app support. In July 2014, leading app stores such as Google Play, Apple AppStore, and Windows Phone Store had over 3 million apps 1 . The app download numbers are astronomic with hundreds of billions of downloads over the last 5 years[9]. Smartphone, tablet, and more recently also desktop users can search the store for the apps, download and install them with a few clicks. Users can also review the app by giving a star rating and a text feedback.Studies highlighted the importance of the reviews for the app success[22]. Apps with better reviews get a better ranking in the store and with it a better visibility and higher sales and download numbers[6]. The reviews seem to help users navigate the jungle of apps and decide which one to use. Using free-text and star rating, the users are able to express their satisfaction, dissatisfaction or ask for missing features. Moreover, recent research has pointed the potential importance of the reviews for the app developers and vendors as well. A significant amount of the reviews include requirements-related information such as bugs or issues[29], summary of the user experience with certain features [12], requests for enhancements[19], and even ideas for new features[8,29].Unfortunately, there are also a bunch of useless, low quality reviews, which include senseless information, insulting comments, spam, or just repetition of the star rating in words. With hundreds of reviews submitted per day for popular apps[17,29], it becomes difficult for developers and analysts to filter and process useful information from the reviews.As a first step towards a tool support for analyzing app reviews, we suggest automatically classifying them according to the type of information they include. We design, evaluate, and compare different classifiers for categorizing reviews into four basic types. Bug reports describe problems with the app which should be corrected, such as a crash, an erroneous behavior, or a performance issue. In feature requests, users ask for missing functionality (e.g., provided by other apps) or missing content (e.g., in catalogues and games), share ideas on how to improve the app in future releases by adding or changing features. User experiences combine \"helpfulness\" and \"feature information\" content reported by Pagano and Maalej[29]. These reviews document the experience of users with the app and how it helped in certain situations. They can be seen as documentation of the app, its requirements, and features. Finally, ratings are simple text reflections of the numeric star rating. Ratings are less informative as they only include praise, dispraise, a distractive critique, or a dissuasion. The classification of the reviews would help with directing the On the Automatic Classification of App Reviews 3 different types to suitable software project members; for instance a bug report can be delivered to developers and testers, a feature requests can be delivered to requirements analysts and user experience reports to documentation team or usability experts. This paper extends our previous work published at the International IEEE Requirements Engineering conference[23]. The main contribution of this work is threefold. First, it introduces a landscape of well known probabilistic techniques and heuristics that can be used for classifying the reviews based on their metadata (e.g., the star rating and text length), keyword frequencies, linguistic rules, and sentiment analysis. Second, we report on an extensive study to compare the accuracy of the review classification techniques. The study data and its results serve as a benchmark for the classification of user app reviews and can be used and extended by researchers and tool vendors. Third, we derive from the findings insights into how to design a tool for review analytics for different stakeholders in software projects. We also present a research prototype and report on a preliminary evaluation with practitioners.The main extension of this paper compared to our previous RE 2015 paper[23]can be summarized as follows:On the Automatic Classification of App Reviews 5 Table 1: Keywords indicating a review type (basic classifier). Review type\n\n1. We extended the classification techniques, in particular adding bigrams and its combinations. We improved the data preparation and classification scripts. This significantly improved the classification accuracy compared to the results reported previously. 2. We added a differentiated analysis between the subsamples consisting of iOS reviews and Android reviews. 3. We added a discussion of the most informative classification features. 4. We added the results of the statistical tests to check if the differences between the various combination of techniques against a baseline technique are statistically significant. We used the Holm's step-down method [16] to mitigate the problem of multiple comparisons. This improves the overall reliability of the reported results. 5. We developed a prototype for a review analytics tool and added a section in this paper to share insights about the tool. 6. We also conducted a qualitative study with 9 practitioners from 8 organizations to (a) evaluate the review analytics tool and (b) capture the current practices and challenges of how practitioners deal with app reviews and how this should change in future.\n\nThe remainder of the paper is structured as follows. Section 2 introduces the classification techniques. Section 3 describes the study design including the questions, method, and data used. Section 4 reports on the results comparing the accuracy and the performance of the various classification techniques. Then, in Section 5 discusses the overall findings, how they should be interpreted and which threats to validity should be considered when using the results. Section 6 describe a prototype review analytics tool and the results of the interviews, which we conducted with practitioners to evaluate the analytics tool and its underlying assumptions. Finally, Section 7 reviews the related work and Section 8 concludes the paper.\n\n\nReview Classification Techniques\n\nA user review consists of a main text part and additional metadata. The text includes the title and the body of the review. The metadata can be considered as a set of additional numerical properties which can be collected with the reviews such as the star rating or the submission time.\n\nOne review can belong to one or more types. For instance, the review \"Doesn't work at the moment. Was quite satisfied before the last update. Will change the rating once it's functional again\" should be classified as a bug report and a rating but neither as a user experience nor as a feature request. The review \"Wish it had live audio, like Voxer does. It would be nice if you could press the play button and have it play all messages, instead of having to hit play for each individual message. Be nice to have a speed button, like Voxer does, to speed up the playing of messages. Be nice to have changeable backgrounds, for each chat, like We Chat does. Other than those feature requests, it's a great app\" should be classified as a feature request and a rating but neither as a bug report nor as a user experience.\n\nHere we introduce various classification techniques, which can be applied on the text and the metadata and combined to automatically predict the review types.\n\n\nBasic Classifier: String Matching\n\nThe most trivial technique to automatically categorize a user review is to check whether it contains a certain keyword. We can manually define (and possibly maintain) a list of keywords that we expect to find in a bug report, a feature request, a user experience, or a rating. We then check whether one of the keywords is included in the text. For this, we can use regular expressions in tools like grep, string matching libraries, or SQL queries, while ignoring letter cases and wrapping around the keywords (e.g., using \"LIKE\" in SQL or \\p in grep). Table 1 shows a collection of possible keywords for each review type, which we compiled from the literature [1,19,29] and used for the string matching classifier. It is hard to get a complete list for each of the rows in the table. This lists here were compiled by the authors based on experience and literature rather as indicative lists and it is not meant to be complete neither exhaustive.\n\n\nDocument Classification: Bag of Words\n\nDocument classification is a popular technique in information science, where a document is assigned to a certain class. A popular example is the email classification as \"spam\" or \"no spam\". In our case, a document is a single review including the title and the text. There is a fundament difference between document classification and the string matching, as the latter is a static approach (based on the manual identification of keywords) while the first is a dynamic approach and the keywords are automatically identified and weighted by a supervised machine-learning algorithm.\n\nThe basic form of document classification is called bag of words (BOW). The classifier creates a dictionary of all terms in the corpus of all reviews and calculates whether the term is present in the review of a certain type and how often. Supervised machine learning algorithms can then be trained with a set of reviews (training set) to learn the review type based on the terms existence and frequency. Some terms like \"app\", \"version\", or \"use\" might appear more frequently in general. An advantage of bag of words is that it does not require manually maintaining a list of keywords for each review type. In addition, the classifier can use patterns of keywords co-occurrences to predict the review type. Finally, in addition to the review text, this technique can be extended with other machine learning features based on the review metadata.\n\nA common alternative to terms count is to use tf-idf (term frequency-inverse document frequency), which increases proportionally to the number of times a term appears in a review, but is offset by the frequency of the term in the corpus. Tf-idf combines word (term) frequencies with the inverse document frequency in order to understand the importance (weight) of a word in the given document. It gives words a greater weight proportionally to the number of times the word occurs but reduces the importance of a word that occurs generally in many or each documents, like stop words.\n\n\nNatural Language Processing: Text Preprocessing\n\nPreprocessing the review text with common natural language processing (NLP) techniques such as stopword removal, stemming, lemmatization, tense detection, and bigrams, can help increasing the classification accuracy.\n\nStopwords are common English words such as \"the\", \"am\", and \"their\", which typically have a grammatical function and do not influence the semantic of a sentence [4]. Removing them from the review text can reduce noise. This allows informative terms like \"bug\" or \"add\" to become more influential (with more weight), which might improve the accuracy of document classifiers. However, some keywords that are commonly defined as stopwords can be relevant for the review classification. For instance, the terms \"should\" and \"must\" might indicate a feature request (e.g. the app should offer a function to share the images on twitter and Facebook). The terms \"did\", \"when\", \"while\" and \"because\" might indicate a feature description and an experience (e.g. I love this app because it always helped when I had time conflicts organizing meetings); the terms \"but\", \"before\", and \"now\" a bug report (e.g. before the update I was able to switch to another open app and copy the text e.g. from my SMS. Now, if I do this I cannot paste the copied text in the search field ), and \"very\", \"too\", \"up\", and \"down\" a rating (e.g. Thumbs up! Very good up).\n\nLemmatization [4] is the process of reducing different inflected forms of a word to their basic lemma to be analyzed as a single item. For instance, \"fixing\", \"fixed\", and \"fixes\" become \"fix\". Similarly, stemming reduces each term to its basic form by removing its postfix. While lemmatization takes the linguistic context of the term into consideration and uses dictionaries, stemmers just operate on single words and therefore cannot distinguish between words which have different meanings depending on part of speech. For instance, lemmatization recognizes \"good\" as the lemma of \"better\" and stemmer will reduce \"goods\" and \"good\" to the same term. Both lemmatization and stemming can help the classifier to unify keywords with same meaning but different language forms, which will increase their count [12]. For instance, the classifier will better learn from the reviews \"crashed when I opened the pdf' and \"the new version crashes all time\" as the term \"crash\" is an indication for the bug report.\n\nFinally, we can also use bigrams of the n-gram family [4]. Bigrams are all combinations of two contiguous words in a sentence. If we have the sentence: \"The app crashes often\", the bigrams are the following: \"The, app\",\"app, crashes\",\"crashes, often\". If used by the document classifier instead of single terms, bigrams allow to additionally capture the context of the word in the review [13]. For instance the bigrams \"crashes always\" and \"never crashes\" have two different meanings and might also reveal two different types of reviews, while the three single terms \"crashes\", \"never\" and \"always\" might all only reveal bug report (i.e. praise or user experience vs. bug reports).\n\n\nReview Metadata: Rating, Length, Tense, and Sentiments\n\nCommon metadata that can be collected with the reviews include the star rating, the length, and the submission time. The star rating is a numeric value between 1 and 5 given by the user. For instance, bug reports are more likely to be found in negative ratings. Previous studies have shown that user experience (i.e. helpfulness and feature description) are very likely to be found in positive reviews typically with 4 or 5 stars [17,29]. The length of the review text can also be used as a classifier feature. Lengthy reviews might be more informative indicating a report on an experience or a bug [29].\n\nFinally, the tense of the verbs in the review can be used as an indication of the review type. For instance, a past tense is rather used for reporting and might reveal a description of a feature, while a future tense is used for a promise or a hypothetical scenario and might rather reveal an enhancement or a feature request. We distinguish between past, present, and future verbs and use all of them as indication for the review types. Since one review can include several tenses, we calculate the ratio of each tense (e.g., 50% of a review verbs are in past and 50% are in present) as metadata. Tense extraction can be seen as NLP technique since it is identified with part-of-speech tagging, commonly provided in NLP libraries. It can also be stored and used as metadata in addition to the text.\n\nReviews in the app stores usually reflect users' positive and negative emotions [12]. For example, a bug report will probably include a negative sentiment, while a user experience would probably be combined with a positive sentiment [29].\n\nMore fine-grained sentiments than the star rating can be extracted from the reviews and used as a feature for training the classifier. For this we used the tool SentiStrength [35], which assigns for each review one negative sentiment score in a scale of -5 to -1 and one positive score in a scale of 1 to 5. The review \"This app is useless !!! I hate it\" will get the positive score +1 and the negative score -5. SentiStrength is designed for short informal texts such as social media posts, comments, or reviews [36].\n\nThere are two options for using the sentiments in the classification. We can either combine the negative and positive scores in an absolute signed score as one single classification feature (e.g., -4 and +2 are simplified to -4). Alternatively, we can use two features: one for the negative and one for the positive score. This enables the classifier to learn from a more fine grained sentiment value. For instance, a feature request might include a positive sentiment as the user is already using the app and a negative as the user is missing a feature.\n\n\nSupervised Learning: Binary vs. Multiclass Classifiers\n\nMachine learning approaches have been used to build individual classification systems. They can be distinguished in supervised and unsupervised learning approaches. The goal of both learning techniques is to classify text (e.g., document, paragraph, or sentence) or numbers (e.g., temperature, noise, or tree height) by assigning a category from a pre-specified set or a real number [30].\n\nSupervised learning approaches need to be trained using a labeled truth set before they can be applied. The training set contains already classified instances that supervised learning algorithm use to infer a classification model, which is then used to classify unseen instances.\n\nIn our case, a review can belong to more than one type, e.g., including a negative rating (dispraise) and a bug report or a positive rating (praise) and a user experience. For example the following review \"The app crashes every time I upload a picture. Please fix it and please add the feature of adding video clip as well \" should be classified as a bug report and feature request. That is, the output of the classifiers consists of four probabilities for each category, where it is indicated with a certain probability how much does the review belong to a certain category if particular properties are observed.\n\nSupervised machine learning algorithms can be used to classify the reviews. The idea is to first calculate a vector of properties (called features) for each review. Then, in a training phase, the classifier calculates the probability for each property to be observed in the reviews of a certain type. Finally, in the test phase, the classifier uses the previous observations to decide whether this review is of a type or not. This is called a binary classification. In our case, each review can be binary-classified four times: as (1) a bug report or not, (2) a feature request or not, (3) a user experience or not, and finally (4) a rating or not. This requires creating four different classification models and training each of them with true positives and true negatives (e.g. true bug reports and not bug reports).\n\nAlternatively, it is possible to assign the review to several classes at once. This is called multiclass classification. In this case one single classification model is created based on one training set. That is the classifier is able to return for each review four different probability values, each for the review to be in type 1-4.\n\nNaive Bayes is a very popular algorithm for binary classifiers [4], which is based on the Bayes' theorem with strong independence assumptions between the features. It is simple, efficient, and does not require a large training set like most other classifiers. Decision Tree learning is another popular classification algorithm [37], which assumes that all features have finite discrete domains and that there is a single target feature representing the classification (i.e., the tree leaves). Finally, the multinomial logistic regression (also known as maximum entropy or MaxEnt) [37] is a popular algorithm for multiclass classification. Instead of assuming a statistical independence of the classification features (i.e. all features independently influence the overall probably of the review to be of a certain type), MaxEnt assumes a linear combination of the features and that some review-specific parameters can be used to determine the probability of each particular review type.\n\n\nResearch Design\n\nWe summarize the research questions, data, and method.\n\n\nResearch Questions\n\nOur goal is to study how accurately the classification techniques from Section 2 can predict the four review types: bug report, feature request, user experience, and rating. This includes answering the following questions:\n\n-Classification techniques: How should the review metadata, text classification, NLP, and sentiment analysis be combined for the classification of app reviews? -Review types: Can the four review types be automatically predicted and which type can be predicted more accurately? -Classification algorithms: Which classification algorithm leads to better results (Naive Bayes vs. Decision Tree, vs. Maximum Entropy)? -Performance & data: How much time and training data are needed for an accurate classification and is there a difference when using various review data?\n\n\nResearch Method and Data\n\nTo answer the research questions we conducted a series of experiments involving four phases. First, we collected real reviews from app stores and extracted their metadata. Second, we created a truth set by selecting a representative sample of these reviews, manually analyzing their content, and labeling them as bug report, feature request, user experience, or rating. Third, we implemented different classifiers and used one part of the truth set to train them (i.e. as training set). We then ran the classifiers on the other part to test whether the classification is correct (i.e. test or evaluation set). Finally, we evaluated the classifiers' accuracy and compared the results. The following elaborates on each phase. We crawled the Apple AppStore [29] and the Google Play stores to collect the experiment data. We iterated over app categories in the stores and selected the top apps in each category. Low ranked apps typically do not have reviews and are thus irrelevant for our study [17]. From the Apple store we collected \u223c1.1 million reviews for 1100 apps, half of them paid and half free. Google store was restrictive for collecting the reviews and we were able to only gather 146,057 reviews for 80 apps: also half were paid and half free. We created a uniform dataset including the review text, title, app name, category, store, submission date, username, and star rating.\n\nFrom the collected data, we randomly sampled a subset for the manual labeling as shown in Table 2. We selected 1000 random reviews from the Apple store data and 1000 from the Google store data. To ensure that enough reviews with 1, 2, 3, 4, and 5 stars are sampled, we split the two 1000-reviews samples into 5 corresponding subsamples each of size 200. Moreover, we selected 3 random Android apps and 3 iOS apps from the top 100 and fetched their reviews between 2012-2014. From all reviews of each app, we randomly sampled 400. This led to additional 1200 iOS and 1200 Android app-specific reviews. In total, we had 4400 reviews in our sample.\n\nFor the truth set creation, we conducted a peer, manual content analysis for all the 4400 reviews. Every review in the sample was assigned randomly to 2 coders from a total of 10 people. The coders were computer science master students, who were paid for this task. Every coder read each review carefully and indicated its types: bug report, feature request, user experience, or rating. We briefed the coders in a meeting, introduced the task, the review types, and discussed several examples. We also developed a coding guide, which describes the coding task, defines precisely what each type is, and lists examples to reduce disagreements and increase the quality of the manual labeling. Finally, the coders were able to use a coding tool (shown on Figure 1) that helps to concentrate on one review at once and to reduce coding errors. If both coders agreed on a review type, we used that label in our golden standard. A third coder checked each label and solved the disagreements for a review type by either accepting the proposed label for this type or rejecting it. This ensured that the golden set contained only peer-agreed labels.\n\nIn the third phase, we used the manually labeled reviews to train and to test the classifiers. A summary of the experiment data is shown in Table 3. We only used reviews, for which both coders agreed that they are of a certain type or not. This helped that a review in the corresponding evaluation sample (e.g., bug reports) is labeled correctly. Otherwise training and testing the classifiers on unclear data will lead to unreliable results. We evaluated the different  techniques introduced in Section 2, while varying the classification features and the machine learning algorithms. We evaluated the classification accuracy using the standard metrics precision and recall. P recision i is the fraction of reviews that are classified correctly to belong to type i. Recall i is the fraction of reviews of type i which are classified correctly. They were calculated as follows:\nP recision i = T P i T P i + F P i Recall i = T P i T P i + F N i (1)\nT P i is the number of reviews that are classified as type i and actually are of type i. F P i is the number of reviews that are classified as type i but actually belong to another type j where j = i. F N i is the number of reviews that are classified to other type j where j = i but actually belong to type i. We also calculated the F-Measure (F1), which is the harmonic mean of precision and recall providing a single accuracy measure. We randomly split the truth set at a ratio of 70:30. That is, we randomly used 70% of the data for the training set and 30% for the test set. Based on the size of our truth set, we felt this ratio is a good tradeoff for having large-enough training and test sets. Moreover, we experimented with other ratios and with the cross-validation method. We also calculated how informative the classification features are and ran paired t-tests to check whether the differences of F1-scores are statistically significant. The results reported in section 4 are obtained using the Monte Carlo crossvalidation [38] method with 10 runs and random 70:30 split ratio. That is, for each run, 70% of the truth set (e.g. for true positive bug reports) is randomly selected and used as a training set and the remaining 30% is used as a test set. Additional experiments data, scripts, and results are available on the project website: http://mast.informatik.uni-hamburg.de/app-review-analysis/.\n\n\nResearch Results\n\nWe report on the results of our experiments and compare the accuracy (i.e. precision, recall and F-measures) as well as the performance of the various techniques.  Table 4 summarizes the results of the classification techniques using Naive Bayes classifier on the whole data of the truth set (from the Apple AppStore and the Google Play Store). The results in Table 4 indicate the mean values obtained by the cross-validation for each single combination of classification techniques and a review type. The numbers in bold represent the highest scores for each column, which means the highest accuracy metric (precision, recall and F-measure) for each classifier. Table 5 shows the p-values of paired t-tests on whether the differences between the mean F1-scores of the baseline classifier and the various classification techniques are statistically significant. For Example: if one classifier result is 80% for a specific combination of techniques and another result is 81% for another combination, those two results could be statistically different or it could be by chance. If the p-value calculated by the paired t-test is very small, this means that the difference between the two values is statistically significant. We used Holm's step-down method [16] to control the family-wise error rate. Overall, the precisions and recalls of all probabilistic techniques were clearly higher than 50% except for three cases: the precision and recall of feature request classifiers based on rating only as well as the recall of the same technique (rating only) to predict ratings. Almost all probabilistic approaches outperformed the basic classifiers that use string matching with at least 10% higher precisions and recalls.\n\n\nClassification Techniques\n\nThe combination of text classifiers, metadata, NLP, and the sentiments extraction generally resulted in high precision and recall values (in most cases above 70%). However, the combination of the techniques did not always rank best. Classifiers only using metadata generally had a rather low precision but a surprisingly high recall except for predicting ratings where we observed the opposite.\n\nConcerning NLP techniques, there was no clear trend like \"more language processing leads to better results\". Overall, removing stopwords significantly increased the precision to predict bug reports, feature request and user experience, while it decreased the precision for ratings. We observed the same when adding lemmatization. On the other hand, combing stop word removal and lemmatization didn't had any significant effect on precision and recall.\n\nWe did not observe any significant difference between using one or two sentiment scores.\n\n\nReview Types\n\nWe achieved the highest precision for predicting user experience and ratings (92%), the highest recall and F-measure for user experience (respectively 99% and 92%).\n\nFor bug reports we found that the highest precision (89%) was achieved with the bag of words, rating, and one sentiment, while the highest recall (98%) with using Bigrams, rating, and one score sentiment. For predicting bug reports the recall might be more important than precision. Bug reports are critical reviews and app vendors would probably need to make sure that a review analytics tool does not miss any of them, with the compromise that a few of the reviews predicted as bug reports are actually not (false positives). For a balance between precision and recall combining bag of words, lemmatization, bigram, rating, and tense seems to work best.\n\nConcerning feature requests, using the bag of words, rating, and one sentiment resulted in the highest precision with 89%. The best F-measure was 85% with bag of words, lemmatization, bigram, rating, and tense as the classification features.\n\nThe results for predicting user experiences were surprisingly high. We expect those to be hard to predict as the basic technique for user experiences shows. The best option that balances precision and recall was to combine bag of words with bigrams, lemmatization, the rating, and the tense. This option achieved a balanced precision and recall with a F-measure of 92%.\n\nFor predicting ratings with the bigram, rating, and one sentiment score lead to the top precision of 92%. This result means that stakeholders can precisely select rating among many reviews. Even if not all ratings are selected (false negatives) due to average recall, those that are selected will be very likely ratings. A common use case would be to filter out reviews that only include ratings or to select another type of reviews with or without ratings. Table 6 shows the ten most informative features of a combined classification technique for each review type.  Table 7 shows the results of comparing the different machine learning algorithms Naive Bayes, Decision Trees, and MaxEnt. We report on two classification techniques (bag of words and bag of words + metadata) since the other results are consistent and can be downloaded from the project website 2 . In all experiments, we found that binary classifiers are more accurate for predicting the review types than multiclass classifiers. One possible reason is that each binary classifier uses two training sets: one set where the corresponding type is observed (e.g., bug report) and one set where it is not (e.g., not bug report). Concerning the binary classifiers Naive Bayes outperformed the other algorithms.\n\n\nClassification Algorithms\n\n\nPerformance and Data\n\nThe more data is used to train a classifier the more time the classifier would need to create its prediction model. This is depicted in Figure 2 where we normalized the mean time needed for the four classifiers depending on the size of the training set.\n\nIn this case, we used a consistent size for the test set of 50 randomly selected reviews to allow a comparison of the results.\n\nWe found that, when using more than 200 reviews to train the classifiers the time curve gets much more steep with a rather exponential than a linear shape. For instance, the time needed for training almost doubles when the training size grows from 200 to 300 reviews. We also found that MaxEnt needed much more time to build its model compared to all other algorithms for binary classification. Using the classification technique BoW and Metadata, MaxEnt took on average \u223c40 times more than Naive Bayes and \u223c1,36 times more than Decision Tree learning. These numbers exclude the overhead introduced by the sentiment analysis, the lemmatization, and the tense detection (part-of-speech tagging). The performance of these techniques is studied well in the literature [4] and their overhead is rather exponential to the text length. However, the preprocessing can be conducted once on each review and stored separately for later usages by the classifiers. Finally, stopword removal introduces a minimal overhead that is linear to the text length.  Figure 3 and 2 it seems that 100-150 reviews is a good size of the training sets for each review type, allowing for a high accuracy while saving resources. With an equal ratio of candidate and noncandidate reviews the expected size of the training set doubles leading to 200-300 reviews per classifier recommended for training.\n\nFinally, we also compared the accuracy of predicting the Apple AppStore reviews with the Google Play Store reviews. We found that there are differences in predicting the review types between both app stores as shown in Table 8 and  Table 9. The highest values of a metric are emphasized as bold for each review type. The biggest difference in both stores is in predicting bug reports. While the top value for F-measure for predicting bugs in the Apple AppStore is 90%, the F-measure for the Google Play Store is 80%. A reason for this difference might be, that we had less labeled reviews for bug reports in the Google Play Store. On the other hand, feature requests in the Google Play Store have a promising precision of 96% with a recall of 88%, while the precision in the Apple AppStore is 88% with a respective recall of 84%, by comparing the top F-measure values for feature requests. Furthermore, we found that the  F-measure of the Google Play Store reviews significantly decreases for user experience compared to the overall results from Table 4. One possible reason for this result is that AppStore reviews are more homogeneous in term of vocabulary and follow similar patterns. This might be also caused by a homogeneous group of iOS users compared to Android users.  \n\n\nDiscussion\n\nWe discuss the main findings of our study and the limitations of the results.\n\n\nWhat is the Best Classifier?\n\nOur results show that no single classifier works best for all review types and data sources. However, several findings can be insightful for designing a review analytics tool:\n\n-The numerical metadata including the star rating, sentiment score, and length had an overall low precision for predicting bug reports, feature requests, and user experience. A high recall for bug reports is important (to not miss critical reports). On the other hand, metadata increased the classification accuracy of text classification (in particular for feature requests and ratings). -On average, the pure text classification (bag of words) achieved a satisfactory precision and recall around 70%. However, fine-tuning the classifiers with the review metadata, the sentiment scores, NLP, and the tense of the review text increased the precision up to 92% for user experiences and ratings, and the recall up to 99% for user experience and bug reports.\n\n-Lemmatization and stopword removal should be used with care. Removing the default list of stopwords in common corpora (e.g., as defined in the Natural Language ToolKit [4]) might decrease the classification accuracy as they include informative keywords like \"want\", \"please\", or \"can\" while removing non-informative stopwords such as \"I\", \"and\", and \"their\" increased the precision and recall in particular for bug reports and ratings. -While the sentiment scores from the automated sentiment analysis increased the accuracy of the classifiers, we did not observe a major difference between one sentiment score vs. two (one negative and one positive score). -Four multiple binary classifiers, one for each review type, performed significantly better than a single multiclass classifier in all cases. -Naive Bayes seems to be an appropriate classification algorithm as it can achieve high accuracy with a small training set and less training time than other studied classifiers (more than 30 times faster).\n\nWe were able to achieve encouraging results with F-measures above 85% for the whole dataset and up to 90% for the Apple store reviews. However, we think that there is still room for improvement in future work. Other potential metadata such as user names, helpfulness score, and submission date might further improve the accuracy. First, some users tend to submit informative reviews, other users submit more bugs or wishes, while others tend to only express complains and ratings. For using the user name as classification feature, we need to track users behaviors over all apps, which brings restrictions as only app vendors can simply access this information. Second, some stores allow users to rate the reviews of others and vote for their helpfulness (as in Stack Overflow). The helpfulness score can improve the classification since user experiences and bug reports are particularly considered helpful by app users [29]. Finally, the relative submission time of the review can also indicate its type. After major releases, reviews become frequent and might be reflective including ratings or bug reports [29]. After the release storm a thoughtful review might rather indicate a user experience or a feature request.\n\n\nBetween-Apps vs. Within-App Analysis\n\nOne goal of this study is to create a tool that takes a review as an input, analyzes it, and correctly classifies its information. This tool needs a training set including example reviews, their metadata, and their correct types. In this paper, we used random training sets including reviews from different apps -an approach that we call between-apps analysis. Such a training set can e.g., be provided by the research community as an annotated corpora or a benchmark. It can also be provided and maintained as a service by the app store providers as Apple, Google, or Microsoft for all apps available on their stores.\n\nOne alternative approach is to train the classifiers separately for each single app -that is to conduct within-app analyses. The customization of the classifier to learn and categorize the reviews written for the same app might improve its accuracy. In addition to general keywords that characterize a review type (e.g., \"fix\" for bug reports or \"add\" for feature requests) the classifier can also learn specific app features (e.g., \"share picture\" or \"create calendar\"). This app specific approach can add a context to the classification. For instance, a specific feature is probably buggy as noted by the developers in the release note. Or, if a feature name is new in the reviews, it probably refers to a new or missing feature. Moreover, additional app specific data such as current bug fixes, release notes, or app description pages could also be used for training the classifiers. In practice, we think that a combination of between-apps analysis and within-app analysis is most reasonable. In particular, grouping the reviews according to their types and according the app features might be useful for the stakeholders (see Section 7.2).\n\nFinally, it might be useful to fine-tune the classifiers to specific app categories. For instance, we observed that some reviews for gaming apps are particularly hard to classify and it is difficult to judge whether the review refers to a bug or to a feature. Since requirements for this category might be handled differently, it might also make sense to use a different classification scheme, e.g., focusing on scenarios or situations.\n\n\nLimitations and Threats to Validity\n\nAs for any empirical research, our work has limitations to its internal and external validity. Concerning the internal validity, one common risk for conducting experiments with manual data analysis is that human coders can make mistakes when coding (i.e. labeling) the data, resulting in unreliable classifications.\n\nWe took several measures to mitigate this risk. First, we created a coding guide [27] that precisely defines the review types with detailed examples. The guide was used by the human coders during the labeling task. Moreover, each review in the sample was coded at least by two people. In the experiment, we only used the reviews, where at least two coders agreed on their types. Finally, we hired the coders to reduce volunteer bias, briefed them together for a shared understanding of the task, and used a tool to reduce concentration mistakes. The guide, data, and all results are available on the project website 3 .\n\nHowever, we cannot completely exclude the possibility of mistakes as reviews often have low quality text and contain multiple overlapping types of information. We think that this potential risk might marginally influence the accuracy evaluation, but does not bias the overall results. An additional careful, qualitative analysis of the disagreements will lead to clarifying \"unclear reviews\" and give insights into improving the classification.\n\nMoreover, there are other possibilities to classify the content of the review. Previously, we have reported on 17 types of information that can be found in reviews [29]. This study focuses on four which we think are the most relevant for requirements engineering community. Other studies that use different types and taxonomies might lead to different results.\n\nWe are aware that we might have missed some keywords for the basic classifiers. Including more keywords will, of course, increase the accuracy of the string matching. However, we think the difference will remain significant as the recall shows for this case. Similarly, we might have missed other machine learning features, algorithms, or metrics. Also, improving classification scripts and random runs will lead to slightly different results. We thus refrain from claiming the completeness of the results. Our study is a first step towards a standard benchmark that requires additional replication and extension studies. We did not report on an automated selection of the machine learning features, which might have resulted into statistically more significant results. Automated feature selection can only be applied in part for this experiment, since some features are exclusive (e.g. using lemmatization / removing the stopwords or not, and using one or two sentiments). Finally, manually combining and comparing the classifiers help interpret the findings. We think, this is a reasonable compromise for a first study of its kind.\n\nFinally, while the results indicate differences between the single combinations of the classification techniques to predict a certain review type, an important question is how significant are the differences. To mitigate this threats we conducted multiple paired t-test on the mean values of the single k-iteration runs. The p-values of these tests should be used carefully when interpreting how strong is the difference between the techniques for a specific scenario. Nevertheless, the large number of paired t-tests might imply that we accumulate the type I error. We applied the Holm's step-down method [16] to mitigate this threat.\n\nConcerning the external validity, we are confident that our results have a high level of generalizability for app reviews. In particular, the results are widely applicable to Apple and Google app stores due to our large random sample. Together, these stores cover over 75% of the app market. However, the results of this study might not apply to other stores (e.g., Amazon) which have a different \"reviewing culture\", to other languages that have different patterns and heuristics than English, and to other types of reviews, e.g., for hotels or movies.\n\n\nA Review Analytics Tool\n\nOur results show that app reviews can be classified as bug reports, feature requests, user experiences, and ratings (praise or dispraise) with a high accuracy between 85-92%. This is encouraging for building analytics tools for the reviews.\n\nSimple statistics about the types (e.g. \"How much bug reports, feature requests, etc. do we have?\") can give an overall idea about the app usage, in particular if compared against other apps (e.g. \"Is my app getting more feature requests or more bug reports than similar apps?\"). Reviews that are only ratings can be filtered out from the manual analysis. Ratings are very persuasive in app reviews (up to 70%) while rather uninformative for developers. Filtering them out would save time. Feature request reviews can be assigned to analysts and architects to discuss and schedule them for future releases or even to get inspirations for new requirements.\n\nUser experience reviews can serve as ad-hoc documentation of the app and its features. These reviews can, e.g., be posted as Frequently Asked Questions to help other users. They can also help app vendors to better understand the users, in particular those, analysts have never thought about. Some user experience reviews describe workarounds and unusual situations that could inspire analysts and architects as well.\n\nThese use cases and the results of our quantitative evaluation of the single classification techniques inspired us to design a prototype tool for review analytics. In the following we discuss how the data collection and processing of the tool works and how the classification can be visualized to the tool user in a user interface. Finally, we report on a qualitative evaluation of the tool with practitioners.\n\n\nData Collection and Processing\n\nIn the data collection phase the app reviews are collected and stored in a canonical dataset. Only data needed for further processing is stored in the data set. If a review dataset already exists, it gets updated with new submitted reviews. The tool collects both the text data as well as the metadata available from the stores.\n\nThe usage of the tool includes two steps. In the first step the classifiers of the tool are trained. In the second step the classifiers are used to perform the classification. The research dataset collected and annotated in this study can be used as input for the first step which enables to immediately start using the tool by stakeholders. The classification can be done either continuously (when a review is submitted, it gets immediately classified), or periodically, e.g. every day or a week a bunch of review data is fetched and classified. The tool offer an interface to the user to define the period of time for the collection and processing of the data. If integrated into the app stores, the user reviews can be fetched and classified immediately.\n\nThere is bunch of preprocessing operation that need to be conducted before the classification can be done. This includes the NLP, the extraction of the sentiments, the extraction of the tense, etc. This kind of preprocessing is related to the single reviews. Therefore, it can be done independently from the classification. Every time the review is submitted (or fetched), the preprocessing is conducted and additional data is stored in the database. Figure 4, 5, and 6 show the prototype of the review analytics tool. On the top of each figure, in the navigation bar, the tool users can log in, get support and submit feedback, see notifications like reviews from watched app users, or go to the home screen to see the dashboard. The start page of the prototype (see Figure 4) shows the functionality for importing the reviews, where developers can e.g. import locally stored reviews in form of a CSV file. Alternatively, a crawler could retrieve app reviews automatically, based on a link to the app page in the corresponding store. After importing reviews, the tool user is able to get an overview of the current trends. Figure 4 shows the number and ratios of classified reviews for each category (bug report, feature request, user experience, and rating) and for each version of the app. This grouped bar chart gives an overview about what app users reported within the given time frames. The chart helps to understand if, for example, app users reported more bug reports and feature requests after the release of a new version. The trend can also be used as overall indicator to understand how the project is going. If the number of bug reports is usually high, development teams might increase their focus on quality management and testing. If feature requests are increasing over time, the focus might shift more to analyzing requirements and the development roadmap might be changed to better address the needs of users.\n\n\nUser Interface\n\nIf an app is distributed in multiple app stores like the Google Play Store or the Apple App Store, the tool user can switch to the App Store Comparison tab on the top of the page. Figure 5 illustrates pie charts, that represent the overall review type distribution in each store. This view provides an overview and allows for comparing the app performance across the stores. Project managers could use this information in order set the development priorities. For instance, if there are much more bugs in the Google Play Store than in other stores, the development team might focus on fixing bugs first. Or, the Windows Phone app has few bugs, the development team might focus on implementing frequently asked features first. Details about each review types, like the feature requests, can be seen in the third tab, called Review Details (see Figure 6). As developers might need more detailed information about the current version or specific review types, they can retrieve in-depth, automatically extracted information in this view. The selection of displayed reviews can be narrowed down by using the first three drop down menus. In the first drop-down menu, tool users can choose if they want to know more about a specific review type. The second drop down menu filters the data by the source of the reviews (app stores). The third can help to filter for reviews with a specific language (e.g. German vs. English). The fourth drop down menu helps to sort the reviews by the given star rating. The filtered data is then visualized below the menus. On the left side, a pie chart can also be used to filter specific types by clicking on the chart. On the right side of the pie chart, tool users can see a scrollable list of actual reviews that apply to the filters.\n\nFor each review, tool users are able to perform two actions. First, a drop down menu shows the review type as it was classified by the system. If tool users think, that a review was misclassified, they are able to correct this by choosing the proper type in the drop down menu. This input can help improve the classification as it extends the training set. The second action allows tool users to \"watch\" the reviews of app users. This is useful when an app user, for example, gives a lower star rating because of some difficulties with the app, but also promises to increase the rating as soon as the issue will be resolved [29]. By using this feature, developers can track if an update helped the user and if it had any effect on the review. Some app stores, as the Google Play Store allows developers to directly answers reviews in their store. This functionality can be used to communicate with users by e.g. writing them that a fix for their problem will be implemented in the next version or to proactively ask if the last update improved their experience. Watched users are bookmarked. This helps when reviews are meaningful but of a low priority. To better understand the needs of developers and analysts for a review analytics tool support, we have conducted interviews with multiple practitioners. The interviews focused on answering the following research questions:\n\n1. Review usefulness: How do practitioners perceive app reviews, their usefulness, and do they currently use these reviews? In January 2016, we interviewed nine practitioners from 7 European companies and one research organization. The interviewees are summarized on Table  10. The subjects had different job positions, such as developers, requirements engineers, researcher, project manager, and software architects. Each interview lasted for 30-45 minutes and was conducted by two of the authors. Our findings are summarized below.\n\nReview usefulness The interviewees think that reviews are useful, but most interviewees do not look at the reviews regularly. Feedback about apps come from multiple channels like the app stores, email or the customer care department, as well as from internal employees. Participants do not use tools to extract information from reviews but do this manually by looking at individual reviews, by inviting test groups or by directly asking users. One of the difficulty mentioned is that many reviews contain non-informative comments like \"I like this app\", \"it crashed, fix it!\", which must be filtered as it takes too much time otherwise to look at all of them. The subjects 2, 3, 4, 5, and 6 state that feedback influences the development process of an app if an issue is reported by multiple users. If e.g. a bug occurs on a very specific hardware device that just little costumers own, developers and project managers are more likely to ignore or delay that issue in their planning.\n\nAnalytics use cases The interviewees state that they desire to understand users as they want to provide a better experience by reducing bugs and adding requested features. As subject 3 stated, the company reacts to reviews because the image of the company is important. Therefore, most of the interviewees wish an automatic tool support for extracting information from reviews. Subject 2, 3, 6, and 7 state that a tool should be able to filter non-informative reviews. Furthermore, subject 2, 4, 5, and 6 suggest to quantify and group feature requests and bugs to better prioritize them in the development phase.\n\nFeedback on tool Overall, our tool suggestion was described by the interviewees as helpful, clearly designed and well packed with functionality. Nevertheless, subjects also suggested making the classification more \"powerful\", by aggregating frequently mentioned issues. Moreover, subjects suggested adding hardware and interaction data from the app itself so that developers can better understand the undertaken steps of the user before a bug occurred (stated by subject 1 and 2). Furthermore, some interviewees stated that it is important to have more options to sort reviews, e.g. by showing the most recent or the most frequent issues. Subjects wanted to have the most important information directly visible on the screen and look into minor issues afterwards. Depending on the job position interviewees liked different parts of the tool more. Project managers especially found the view of Figure 4 and Figure 5 useful. They gave reason that they do not have time to check individual reviews and just want to know the current situation of the app. Subject 2 explained that he wants to have such views on separate screens in their office, so that his team can check the current status every morning. If a lot of bug reports occurred after publishing a new version of their app, they could take immediate action.\n\nIntegration into workflows We found that there are three common ideas for integrating the review analytics tool into the app development. One group of interviewees said that they would like to have this analytics support as a standalone web-based tool with responsive design, so that they could still monitor the status of their app outside the office. The second group of interviewees asked for an integration into issue trackers or at least to have an export functionality for these systems. The best case for this would be the automatic creation of issues out of the reviews. Some also said that it would be good if these issues could be proactively suggested by the tool but eventually edited by a team member in order to formulate it more clearly. One subject stated, having some actual reviews attached to the issue would be useful. The third group of interviewees stated that they would like to see an integration into crash trackers like crashlytics 4 , that reports the stack trace of occurred crashes. Since there are also bugs that do not produce any crash, combing these two perspectives, would provide a more complete view on current problems in once place.\n\n\nRelated Work\n\nWe focus the related work discussion on three areas: user feedback and crowdsourcing requirements, app store analysis, as well as classification of issue reports.\n\n\nUser Feedback and Crowdsourcing Requirements\n\nResearch has shown that the involvement of users and their continuous feedback are major success factors for software projects [34]. Bano and Zowghi identified 290 publications that highlight the positive impact of user involvement [2]. Pagano and Bruegge [28] interviewed developers and found that user feedback contains important information to improve software quality and to identify missing features. Recently, researchers also started discussing the potentials of a crowd-based requirements engineering [11,21]. These works stressed the scalability issues when involving many users and the importance of a tool support for analyzing user feedback.\n\nBug repositories are perhaps the most studied tools for collecting user feedback in software projects [3], mainly from the development and maintenance perspective. We previously discussed how user feedback could be considered in software lifecycles in general [24] and requirements engineering in particular, distinguishing between implicit and explicit feedback [25]. Seyff et al. [33] and Schneider et al. [32] proposed to continuously elicit user requirements with feedback from mobile devices, including implicit information on the application context. In this paper we propose an approach to systematically analyze explicit, informal user feedback. Our discussion of review analytics contribute to the vision of crowd-based requirements by helping app analysts and developers to identify and organize useful feedback in the app reviews.\n\n\nApp Store Analysis\n\nIn recent year, app store analysis has become a popular topic amongst researchers [14] and practitioners 5 . We can observe three directions: (a) general exploratory studies, (b) app features extraction, and (c) reviews filtering and summarization.\n\nGeneral Studies Finkelstein et al. [6] studied the relationships between customer, business, and technical characteristics of the apps in the BlackBerry Store, using data mining and NLP techniques to explore trends and correlations. They found a mild correlation between price and the number of features claimed for an app and a strong correlation between customer rating and the app popularity. This finding motivates the importance of reviews for both developers and users.\n\nHoon et al. [17] and Pagano and Maalej [29] conducted broad exploratory studies of app reviews in the Apple Store, identifying trends for the ratings, review quantity, quality, and the topics discussed in the reviews. Their findings motivated this work. Our reviews types [29] and the evidence of their importance [17,29] are derived from these studies. One part of our experiment data (iOS random) is derived from Pagano and Maalej's dataset. We extended this dataset with Android reviews and took additional recent reviews from 2013 and 2014. Finally this work is rather evaluative than exploratory. We studied how to automatically classify the reviews using machine learning and NLP techniques.\n\nFeature Extraction and Opinion Mining Other researchers mined the app features and the user opinions about them from the app stores. Harman et al. [13] extracted app features from the official app description pages using a collocation and a greedy algorithm. Guzman and Maalej also applied collocations and sentiment analysis to extract app features from the user reviews together with an opinion summary about the features [12]. Similarly, Li et al. [22] studied user satisfaction in the app stores. The authors extracted quality indicators from reviews by matching words or phrases in the user comments with a predefined dictionary. Opinion mining is popular in other domains to analyze opinions about movies, cameras, or blogs [18,26,31]. Mining reviews in app stores exhibits different challenges. The text in app reviews tends to be 3 to 4 times shorter [20], having a length that is comparable to that of a Twitter message [29], but posing an additional challenge in comparison to feature extraction in Twitter messages due to the absence of hash tags. While we also use natural language processing and sentiment analysis, we focus on a complementary problem. Our goal is to classify the reviews and assign them to appropriate stakeholders rather then to extract the features and get an overall average opinion.\n\nFiltering and Summarizing Reviews Recently, researchers also suggested probabilistic approaches to summarize the reviews content and filter informative reviews. Galvis Carre\u00f1o and Winbladh [8] extracted word-based topics from reviews and assigned sentiments to them through an approach that combines topic modeling and sentiment analysis. Similarly, Chen et al. [5] proposed AR-miner, a review analytics framework for mining informative app reviews. AR-miner first filters \"noisy and irrelevant\" reviews. Then, it summarizes and ranks the informative reviews also using topic modeling and heuristics from the review metadata. The main use case of these works is to summarize and visualize discussion topics in the reviews. This visualization could inspire analysts and managers for planning and prioritizing future releases. Our goal is similar but our approach and use case are different. Instead of topic modeling, we use supervised machine learning based on a variety of features. While the overhead is bigger to train the algorithm, the accuracy is typically higher. Our use case is to automatically categorize the reviews into bug reports, feature requests, feature experiences, and ratings. This classification helps to split the reviews over the stakeholders rather than to summarize them. Finally, our approach is app-independent, while review summarization and opinion mining approaches are applied on separate apps with separate vocabularies and topics.\n\nFinally, perhaps the most related work to ours is of Iacob and Harrison [19]. In a first step, the authors extracted feature requests from app store reviews by means of linguistic rules. Then they used LDA to group the feature requests. While Iacob and Harrison focused on one type of reviews, i.e. feature requests, we also studied bug reports, user experiences, and ratings, as we think all are relevant for project stakeholders. Iacob and Harrison fine-tuned an NLP approach and developed several linguistic rules to extract feature requests. We tried to combine different information and evaluated different techniques including NLP, sentiment analysis, and text classification, which are not specific to the English language. Applying LDA to summarize the classified bug reports, user experiences, and ratings is a promising future work -as Iacob and Harrison showed that this works well for feature requests.\n\n\nIssue Classification and Prediction\n\nBug tracking and bug prediction are well studied fields in software engineering. One alternative to our approach is to ask users to manually classify their reviews. Herzig et al. [15] conducted a study and found that about a third of all manually-classified reports in the issue trackers of five large open source projects are misclassified, e.g. as a bug instead of documentation or a feature request. This finding shows that the manual classification of reports is errorprone, which is one important motivation for our study. We think that manually classifying reviews is misleading for users and most popular app stores do not provide a field to enter the type of a review.\n\nAntoniol et al. [1] reported on a study about classifying entries of an issue tracker as bugs or enhancements. Our study is inspired by theirs but targets different type of data (app reviews) that are created by other stakeholders (end users). We also target user experiences and ratings that are very common in app reviews. Finally, in addition to pure text classification, we evaluated other heuristics and features such as tense, sentiment, rating, and review length.\n\nFitzgerald et al. [7] reported on an approach for the early prediction of failures in feature request management systems. Unlike our work, the authors focused on feature requests, but defined various subtypes such as abandoned development, stalled development, and rejection reversal. The authors also used data from the issue trackers of open source projects. Instead of issue trackers, we mine app stores and combine metadata and heuristics with text classification.\n\nFinally, there is a large body of knowledge on predicting defects by mining software repositories [10]. We think that the manual description of issues by users (i.e. the crowd) is complementary to the analysis of code and other technical artifacts. Moreover, our work also includes predicting new ideas (innovations) as well as feature descriptions and documentation.\n\n\nConclusion\n\nApp stores provide a rich source of information for software projects, as they combine technical, business, and user-related information in one place. Analytics tools can help stakeholders to deal with the large amount, the variety, and quality of the app stores data and to take the right decisions about the requirements and future releases. In this paper, we proposed and studied one functionality of app store analytics that enables the automatic classification of user reviews into bug reports, feature requests, user experiences, and ratings (i.e. simple praise or dispraise repeating the star rating). In a series of experiments, we compared the accuracy of simple string matching, text classification, natural language processing, sentiment analysis, and review metadata to classify the reviews. We reported on several findings which can inspire the design of review analytics tools. In particular, metadata-only based classifiers have a poor performance. The performance of text-based classification can be enhanced with metadata such as the tense of the text, the star rating, the sentiment score, and the length. Moreover, stopword removal and lemmatization, two popular NLP techniques used for preprocessing the text in document classification, should be used carefully, since every word in a short informal review can be informative. For instance stop word removal can decrease the classification accuracy. We also found that using bigrams instead of single words for text-based classification (i.e. \"bag of bigrams\") lead to higher F1-score values. Overall, the best precision and recall for all four review types are encouraging -ranging from 89% up to 99%. Our work helps to filter reviews of interest for certain stakeholders as developers, analysts, and other users, as our 9 qualitative interviewees confirmed. Complementary within-app analytics such as the feature extraction, opinion mining, and the summarization of the reviews, will make app store data more useful for software and requirements engineering decisions.\n\nFig. 1 :\n1Tool for manual labeling of the reviews.\n\nFigure 3\n3shows how the accuracy changes when the classifiers use larger training sets. The precision curves are represented with continuous lines while the recall curves are dotted. From\n\nFig. 2 :\n2How the size of the training set influences the time to build the classification model (Naive Bayes using BoW + rating + lemmatize).\n\nFig. 3 :\n3How the size of the training set influences the classifier accuracy (Naive Bayes using BoW + rating + lemmatization).\n\nFig. 4 :\n4Analytics Tool: Review Types Over Time\n\nFig. 5 :\n5Analytics Tool: App Store Comparison\n\nFig. 6 :\n6Analytics\n\nTable 2 :\n2Overview of the evaluation data.App(s) \nCategory \nPlatform \n#Reviews Sample \n\n1100 apps \nall iOS \nApple \n1,126,453 \n1000 \nDropbox \nProductivity \nApple \n2009 \n400 \nEvernote \nProductivity \nApple \n8878 \n400 \nTripAdvisor Travel \nApple \n3165 \n400 \n\n80 apps \nTop four \nGoogle \n146,057 \n1000 \nPicsArt \nPhotography \nGoogle \n4438 \n400 \nPinterest \nSocial \nGoogle \n4486 \n400 \nWhatsapp \nCommunication Google \n7696 \n400 \n\nTotal \n1,303,182 \n4400 \n\n\n\nTable 3 :\n3Number of manually analyzed and labeled reviews.Sample \nManually \nanalyzed \n\nBug \nReports \n\nFeature \nRequests \n\nUser \nExperiences \n\nRatings \n\nRandom apps Apple \n1000 \n109 \n83 \n370 \n856 \nSelected apps Apple \n1200 \n192 \n63 \n274 \n373 \nRandom apps Google 1000 \n27 \n135 \n16 \n569 \nSelected apps Google 1200 \n50 \n18 \n77 \n923 \n\nTotal \n4400 \n378 \n299 \n737 \n2721 \n\n\n\nTable 4 :\n4Accuracy of the classification techniques using Naive Bayes on app reviews from Apple and Google stores (mean values of the 10 runs, random 70:30 splits for training:evaluation sets). Bold values represent the highest score for the corresponding accuracy metric per review type.Classification techniques \nBug Reports \nFeature Requests \nUser Experiences \nRatings \nPrecision Recall \nF1 \nPrecision Recall \nF1 \nPrecision \nRecall \nF1 \nPrecision Recall \nF1 \n\nBasic (string matching) \n0.58 \n0.24 \n0.33 \n0.39 \n0.55 \n0.46 \n0.27 \n0.12 \n0.17 \n0.74 \n0.56 \n0.64 \n\nDocument Classification (& NLP) \nBag of words (BOW) \n0.79 \n0.65 \n0.71 \n0.76 \n0.54 \n0.63 \n0.82 \n0.59 \n0.68 \n0.67 \n0.85 \n0.75 \nBigram \n0.68 \n0.98 \n0.80 \n0.68 \n0.97 \n0.80 \n0.70 \n0.99 \n0.82 \n0.91 \n0.62 \n0.73 \nBOW + Bigram \n0.85 \n0.90 \n0.87 \n0.86 \n0.85 \n0.85 \n0.87 \n0.91 \n0.89 \n0.85 \n0.89 \n0.87 \nBOW + lemmatization \n0.88 \n0.74 \n0.80 \n0.86 \n0.65 \n0.74 \n0.90 \n0.67 \n0.77 \n0.73 \n0.91 \n0.81 \nBOW -stopwords \n0.86 \n0.69 \n0.76 \n0.86 \n0.65 \n0.74 \n0.91 \n0.67 \n0.77 \n0.74 \n0.91 \n0.81 \nBOW + lemmatization -stopwords \n0.85 \n0.71 \n0.77 \n0.87 \n0.67 \n0.76 \n0.91 \n0.67 \n0.77 \n0.75 \n0.90 \n0.82 \nBOW + Bigrams -stopwords + lemmatization \n0.85 \n0.91 \n0.88 \n0.86 \n0.83 \n0.85 \n0.89 \n0.94 \n0.91 \n0.85 \n0.90 \n0.87 \n\nMetadata \nRating \n0.64 \n0.82 \n0.72 \n0.31 \n0.35 \n0.31 \n0.74 \n0.89 \n0.81 \n0.72 \n0.34 \n0.46 \nRating + length \n0.76 \n0.75 \n0.75 \n0.68 \n0.67 \n0.67 \n0.72 \n0.82 \n0.77 \n0.70 \n0.68 \n0.69 \nRating + length + tense \n0.74 \n0.73 \n0.74 \n0.64 \n0.71 \n0.67 \n0.74 \n0.80 \n0.77 \n0.70 \n0.68 \n0.69 \nRating + length + tense + 1x sentiment \n0.69 \n0.76 \n0.72 \n0.66 \n0.66 \n0.66 \n0.71 \n0.85 \n0.77 \n0.71 \n0.66 \n0.68 \nRating + length + tense + 2x sentiments \n0.66 \n0.78 \n0.71 \n0.65 \n0.72 \n0.68 \n0.67 \n0.88 \n0.76 \n0.69 \n0.67 \n0.68 \n\nCombined (text and metadata) \nBOW + rating + lemmatize \n0.85 \n0.73 \n0.78 \n0.89 \n0.64 \n0.74 \n0.90 \n0.67 \n0.77 \n0.73 \n0.89 \n0.80 \nBOW + rating + 1x sentiment \n0.89 \n0.72 \n0.79 \n0.89 \n0.60 \n0.71 \n0.92 \n0.73 \n0.81 \n0.75 \n0.93 \n0.83 \nBOW + rating + tense + 1sentiment \n0.87 \n0.71 \n0.78 \n0.87 \n0.60 \n0.70 \n0.92 \n0.69 \n0.79 \n0.74 \n0.90 \n0.81 \nBigram + rating + 1x sentiment \n0.73 \n0.98 \n0.83 \n0.71 \n0.96 \n0.81 \n0.75 \n0.99 \n0.85 \n0.92 \n0.69 \n0.79 \nBigram -stopwords + lemmatization + rating + tense + 2x sentiment \n0.72 \n0.97 \n0.82 \n0.70 \n0.94 \n0.80 \n0.75 \n0.98 \n0.85 \n0.92 \n0.72 \n0.81 \nBOW + bigram + tense + 1x sentiment \n0.87 \n0.88 \n0.87 \n0.85 \n0.83 \n0.83 \n0.88 \n0.94 \n0.91 \n0.83 \n0.87 \n0.85 \nBOW + lemmatize + bigram + rating + tense \n0.88 \n0.88 \n0.88 \n0.87 \n0.84 0.85 \n0.89 \n0.94 \n0.92 \n0.84 \n0.90 0.87 \nBOW -stopwords + bigram + rating + tense + 1xsentiment \n0.88 \n0.89 \n0.88 \n0.86 \n0.84 \n0.85 \n0.87 \n0.93 \n0.90 \n0.83 \n0.89 \n0.86 \nBOW -stopwords + lemmatization + rating + 1x sentiment + tense \n0.88 \n0.71 \n0.79 \n0.87 \n0.64 \n0.74 \n0.91 \n0.72 \n0.80 \n0.73 \n0.90 \n0.80 \nBOW -stopwords + lemmatization + rating + 2x sentiments + tense \n0.87 \n0.71 \n0.78 \n0.86 \n0.68 \n0.76 \n0.91 \n0.73 \n0.81 \n0.75 \n0.90 \n0.82 \n\n\n\nTable 5 :\n5Results of the paired t-test between the different techniques (one in each row) and the baseline BoW (using Naive Bayes on app reviews from Apple and Google stores).Classification techniques \nBug Reports \nFeature Requests \nUser Experiences \nRatings \nF1-Score \np-Value \nF1-Score \np-Value \nF1-Score \np-Value \nF1-Score \np-Value \n\nDocument Classification (& NLP) \nBag of words (BOW) \n0.71 \nBaseline \n0.63 \nBaseline \n0.68 \nBaseline \n0.75 \nBaseline \nBigram \n0.80 \n0.043 \n0.80 \n2.5e \u2212 06 \n0.82 \n0.00026 \n0.73 \n0.55 \nBOW + bigram \n0.87 \n6.9e \u2212 05 \n0.85 \n2.6e \u2212 07 \n0.89 \n4.7e \u2212 06 \n0.87 \n2.9e \u2212 05 \nBOW + lemmatization \n0.80 \n0.031 \n0.74 \n0.0022 \n0.77 \n0.0028 \n0.81 \n0.029 \nBOW -stopwords \n0.76 \n0.09 \n0.74 \n0.0023 \n0.77 \n0.0017 \n0.81 \n0.0019 \nBOW -stopwords + lemmatization \n0.77 \n0.051 \n0.76 \n0.0008 \n0.77 \n0.0021 \n0.82 \n0.0005 \nBOW -stopwords + lemmatization + bigram \n0.88 \n6.6e \u2212 05 \n0.85 \n2.9e \u2212 07 \n0.91 \n4.3e \u2212 08 \n0.87 \n0.0009 \n\nMetadata \nRating \n0.72 \n1.0 \n0.31 \n0.04 \n0.81 \n7.1e \u2212 05 \n0.46 \n6.9e \u2212 06 \nRating + length \n0.75 \n0.09 \n0.67 \n0.04 \n0.77 \n0.0005 \n0.69 \n0.0098 \nRating + length + tense \n0.74 \n0.63 \n0.67 \n0.083 \n0.77 \n0.0029 \n0.69 \n0.029 \nRating + length + tense + 1x sentiment \n0.73 \n1.0 \n0.66 \n0.16 \n0.77 \n0.004 \n0.68 \n8.9e \u2212 05 \nRating + length + tense + 2x sentiments \n0.71 \n1.0 \n0.68 \n0.0002 \n0.76 \n0.028 \n0.68 \n0.029 \n\nCombined (text and metadata) \nBOW + rating + lemmatize \n0.78 \n0.064 \n0.74 \n0.0005 \n0.77 \n0.0023 \n0.80 \n0.0044 \nBOW + rating + 1x sentiment \n0.79 \n0.0027 \n0.71 \n0.039 \n0.81 \n0.0002 \n0.83 \n0.001 \nBOW + rating + 1sentiment + tense \n0.78 \n0.0097 \n0.70 \n0.039 \n0.79 \n0.0002 \n0.81 \n0.0012 \nBigram + rating + 1sentiment \n0.83 \n0.0039 \n0.81 \n9.5e \u2212 06 \n0.85 \n2e \u2212 05 \n0.79 \n0.042 \nBigram -stopwords + lemmatization + rating + tense + 2x sentiment \n0.82 \n0.0019 \n0.80 \n1.7e \u2212 06 \n0.85 \n2.5e \u2212 05 \n0.81 \n0.029 \nBOW + bigram + tense + 1x sentiment \n0.87 \n0.0001 \n0.83 \n1.2e \u2212 05 \n0.91 \n1.9e \u2212 07 \n0.85 \n0.0002 \nBOW + lemmatize + bigram + rating + tense \n0.88 \n7.6e \u2212 06 \n0.85 \n7.6e \u2212 07 \n0.92 \n1.2e \u2212 07 \n0.87 \n1.6e \u2212 05 \nBOW -stopwords + bigram + rating + tense + 1xsentiment \n0.88 \n1.6e \u2212 06 \n0.85 \n7.6e \u2212 07 \n0.90 \n4.8e \u2212 06 \n0.86 \n0.0002 \nBOW -stopwords + lemmatization + rating + tense + 1x sentiment \n0.79 \n0.064 \n0.74 \n0.0008 \n0.80 \n0.0014 \n0.80 \n0.029 \nBOW -stopwords + lemmatization + rating + tense + 2x sentiments \n0.78 \n0.051 \n0.76 \n0.0012 \n0.81 \n0.0003 \n0.82 \n0.0002 \n\n\n\nTable 6 :\n6Most informative features for the classification technique bigram -stop words + lemmatization + rating + 2x sentiment scores + tenseBug Report \nFeature Request \nUser Experience \nRating \n\nrating (1) \nbigram (way to) \nrating (3) \nbigram (will not) \nrating (2) \nbigram (try to) \nrating (1) \nbigram (to download) \nbigram (every time) \nbigram (would like) \nbigram (use to) \nbigram (use to) \nbigram (last update) \nbigram (5 star) \nbigram (to find) \nbigram (new update) \nbigram (please fix) \nrating (1) \nbigram (easy to) \nbigram (fix this) \nsentiment (-4) \nbigram (new update) \nbigram (go to) \nbigram (can get) \nbigram (new update) \nbigram (back) \nbigram (great to) \nbigram (to go) \nbigram (to load) \nrating (2) \nbigram (app to) \nrating (1) \nbigram (it can) \npresent cont. (1) \nbigram (this great) \nbigram (great app) \nbigram (can and) \nbigram (please fix) \nsentiment (-3) \npresent simple (1) \n\n\n\nTable 7 :\n7F-measures of the evaluated machine learning algorithms (B=Binary classifier, MC=Multiclass classifiers) on app reviews from Apple and Google stores.Type Technique \nBug R. \nF Req. \nU Exp. \nRat. \nAvg. \n\nNaive Bayes \nB \nBag of Words (BOW) \n0.71 \n0.63 \n0.68 \n0.75 \n0.70 \nMC \nBag of Words \n0.66 \n0.31 \n0.43 \n0.59 \n0.50 \nB \nBOW+metadata \n0.79 \n0.71 \n0.81 \n0.83 \n0.79 \nMC \nBOW+metadata \n0.62 \n0.42 \n0.50 \n0.58 \n0.53 \n\nDecision Tree \nB \nBag of Words \n0.81 \n0.77 \n0.82 \n0.79 \n0.79 \nMC \nBag of Words \n0.49 \n0.32 \n0.44 \n0.52 \n0.44 \nB \nBOW+metadata \n0.73 \n0.68 \n0.78 \n0.78 \n0.72 \nMC \nBOW+metadata \n0.62 \n0.47 \n0.53 \n0.54 \n0.54 \n\nMaxEnt \nB \nBag of words \n0.66 \n0.65 \n0.58 \n0.67 \n0.65 \nMC \nBag of words \n0.26 \n0.00 \n0.12 \n0.22 \n0.15 \nB \nBOW+metadata \n0.66 \n0.65 \n0.60 \n0.69 \n0.65 \nMC \nBOW+metadata \n0.14 \n0.00 \n0.29 \n0.04 \n0.12 \n\n\n\nTable 8 :\n8Accuracy of the classification techniques using Naive Bayes on reviews only from the Apple App Store.Classification techniques \nBug Reports \nFeature Requests \nUser Experiences \nRatings \n\nPrecision \nRecall \nF1 \nPrecision \nRecall \nF1 \nPrecision \nRecall \nF1 \nPrecision \nRecall \nF1 \n\nCombined (text and metadata) \nBOW + rating + lemmatize \n0.82 \n0.69 \n0.74 \n0.88 \n0.48 \n0.61 \n0.71 \n0.39 \n0.50 \n0.63 \n0.89 \n0.73 \nBOW + rating + 1x sentiment \n0.89 \n0.70 \n0.78 \n0.90 \n0.55 \n0.67 \n0.75 \n0.55 \n0.62 \n0.67 \n0.94 \n0.78 \nBOW + rating + 1x sentiment + tense \n0.89 \n0.69 \n0.78 \n0.92 \n0.60 \n0.72 \n0.81 \n0.55 \n0.65 \n0.71 \n0.89 \n0.78 \nBigram + rating + 1x sentiment \n0.76 \n0.96 \n0.85 \n0.70 \n0.98 \n0.82 \n0.72 \n0.84 \n0.76 \n0.81 \n0.85 \n0.83 \nBigram -stopwords + lemmatize + \nrating + 2x sentiment + tense \n0.75 \n0.97 \n0.84 \n0.73 \n0.94 \n0.82 \n0.69 \n0.88 \n0.77 \n0.83 \n0.87 \n0.85 \n\nBOW + bigram + tense + 1x sentiment \n0.84 \n0.92 \n0.88 \n0.88 \n0.77 \n0.82 \n0.79 \n0.78 \n0.78 \n0.77 \n0.91 \n0.83 \nBOW + bigram + lemmatize + \nrating + tense \n0.90 \n0.91 0.90 \n0.88 \n0.75 \n0.80 \n0.85 \n0.74 0.79 \n0.77 \n0.91 \n0.83 \n\nBOW + bigram -stopwords + rating + \ntense + 1x sentiment \n0.88 \n0.88 \n0.88 \n0.88 \n0.84 \n0.85 \n0.89 \n0.69 \n0.77 \n0.79 \n0.90 \n0.84 \n\nBOW -stopwords + lemmatization + \nrating + 1x sentiment + tense \n0.94 \n0.77 \n0.84 \n0.92 \n0.57 \n0.69 \n0.90 \n0.58 \n0.70 \n0.69 \n0.91 \n0.78 \n\nBOW -stopwords + lemmatization + \nrating + 2x sentiments + tense \n0.92 \n0.74 \n0.81 \n0.96 \n0.45 \n0.60 \n0.90 \n0.59 \n0.70 \n0.67 \n0.91 \n0.76 \n\n\n\nTable 9 :\n9Accuracy of the classification techniques using Naive Bayes on reviews only from the Google Play Store.Classification techniques \nBug Reports \nFeature Requests \nUser Experiences \nRatings \n\nPrecision \nRecall \nF1 \nPrecision \nRecall \nF1 \nPrecision \nRecall \nF1 \nPrecision \nRecall \nF1 \n\nCombined (text and metadata) \nBOW + rating + lemmatize \n0.80 \n0.50 \n0.61 \n0.92 \n0.72 \n0.81 \n0.73 \n0.51 \n0.59 \n0.69 \n0.90 \n0.77 \nBOW + rating + 1x sentiment \n0.94 \n0.55 \n0.68 \n0.90 \n0.75 \n0.82 \n0.82 \n0.54 \n0.64 \n0.66 \n0.90 \n0.76 \nBOW + rating + 1x sentiment + tense \n0.82 \n0.60 \n0.67 \n0.93 \n0.78 \n0.85 \n0.92 \n0.61 \n0.72 \n0.71 \n0.92 \n0.79 \nBigram + rating + 1x sentiment \n0.70 \n0.85 \n0.76 \n0.81 \n0.98 \n0.88 \n0.70 \n0.85 \n0.76 \n0.91 \n0.77 \n0.83 \nBigram -stopwords + lemmatize + \nrating + 2x sentiment + tense \n0.80 \n0.81 0.80 \n0.76 \n0.96 \n0.85 \n0.75 \n0.85 \n0.79 \n0.91 \n0.65 \n0.75 \n\nBOW + bigram + tense + 1x sentiment \n0.82 \n0.61 \n0.68 \n0.93 \n0.84 \n0.88 \n0.84 \n0.68 \n0.75 \n0.79 \n0.92 \n0.85 \nBOW + bigram + lemmatize + \nrating + tense \n0.85 \n0.70 \n0.76 \n0.94 \n0.83 \n0.88 \n0.84 \n0.75 0.78 \n0.84 \n0.92 0.88 \n\nBOW + bigram -stopwords + rating + \ntense + 1x sentiment \n0.92 \n0.67 \n0.76 \n0.96 \n0.84 \n0.89 \n0.87 \n0.64 \n0.73 \n0.76 \n0.94 \n0.84 \n\nBOW -stopwords + lemmatization + \nrating + 1x sentiment + tense \n0.93 \n0.54 \n0.67 \n0.94 \n0.65 \n0.77 \n0.76 \n0.62 \n0.65 \n0.78 \n0.93 \n0.85 \n\nBOW -stopwords + lemmatization + \nrating + 2x sentiments + tense \n0.93 \n0.57 \n0.69 \n0.93 \n0.72 \n0.81 \n0.88 \n0.64 \n0.72 \n0.72 \n0.92 \n0.80 \n\n\n\nTable 10 :\n10Overview of the interviewed subjects. Integration into workflows: How should our analytics tool be integrated in a professional work environment?Subject Role \nCompany \n\n1 \nSenior app developer \nGerman SME, app development \n\n2 \nSenior tester, quality manager \nLarge european social media com-\npany \n\n3 \nLead engineer \nEuropean telecommunication com-\npany \n\n4 \nProject manager for apps \nGlobal market research company \n\n5 \nLead architect, project manager \nSoftware development company for \nMac solutions \n\n6 \nRE Researcher with practice experience \nUniversity \n\n7 \nUsability/Requirements engineer \nLarge software development com-\npany \n\n8 \nProject manager, requirements analyst \nDanish SME, app development \n\n9 \nProject manager, requirements analyst \nDanish SME, app development \n\n2. Analytics use cases: What are the main use cases for a review analytics \ntool for developers, analysts, and managers? \n3. Feedback on tool: How do practitioners perceive our analytics tool? Which \nfeatures are they missing and what would they change? \n4. \nhttp://www.statista.com/statistics/276623/number-of-apps-available-in-leading-appstores/\nWalid Maalej et al.\nhttp://mast.informatik.uni-hamburg.de/app-review-analysis/\nhttp://mast.informatik.uni-hamburg.de/app-review-analysis/\nWalid Maalej et al.\nhttps://crashlytics.com\nhttp://www.appannie.com/\nAcknowledgements We thank D. Pagano for his support with the data collection, M. H\u00e4ring for contributing to the development of the coding tool, as well as the RE15 reviewers, M. Nagappan, and T. Johann for the comments on the paper. We are also very grateful to the participants in the evaluation interviews. This work was partly supported by Microsoft Research (SEIF Award 2014).\nIs it a bug or an enhancement?: A text-based approach to classify change requests. Giuliano Antoniol, Kamel Ayari, Massimiliano Di Penta, Foutse Khomh, Yann-Ga\u00ebl Gu\u00e9h\u00e9neuc, Proceedings of the 2008 Conference of the Center for Advanced Studies on Collaborative Research: Meeting of Minds, CASCON '08. the 2008 Conference of the Center for Advanced Studies on Collaborative Research: Meeting of Minds, CASCON '08ACM23318Giuliano Antoniol, Kamel Ayari, Massimiliano Di Penta, Foutse Khomh, and Yann-Ga\u00ebl Gu\u00e9h\u00e9neuc. Is it a bug or an enhancement?: A text-based approach to classify change requests. In Proceedings of the 2008 Conference of the Center for Advanced Studies on Collaborative Research: Meeting of Minds, CASCON '08, pages 23:304-23:318. ACM, 2008.\n\nA systematic review on the relationship between user involvement and system success. Muneera Bano, Didar Zowghi, Information & Software Technology. 58Muneera Bano and Didar Zowghi. A systematic review on the relationship between user involvement and system success. Information & Software Technology, 58:148-169, 2015.\n\nWhat makes a good bug report?. Nicolas Bettenburg, Sascha Just, Adrian Schr\u00f6ter, Cathrin Weiss, Rahul Premraj, Thomas Zimmermann, Proceedings of the 16th ACM. the 16th ACM32Nicolas Bettenburg, Sascha Just, Adrian Schr\u00f6ter, Cathrin Weiss, Rahul Premraj, and Thomas Zimmermann. What makes a good bug report? In Proceedings of the 16th ACM 32\n\n. Walid Maalej, Walid Maalej et al.\n\nSIGSOFT International Symposium on Foundations of software engineering. ACM Press308SIGSOFT International Symposium on Foundations of software engineering, page 308. ACM Press, November 2008.\n\nNatural language processing with Python. Steven Bird, Ewan Klein, Edward Loper, O'reillySteven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python. O'reilly, 2009.\n\nAR-miner: Mining informative reviews for developers from mobile app marketplace. Ning Chen, Jialiu Lin, C H Steven, Xiaokui Hoi, Boshen Xiao, Zhang, Proceedings of the 36th International Conference on Software Engineering, ICSE 2014. the 36th International Conference on Software Engineering, ICSE 2014ACMNing Chen, Jialiu Lin, Steven C. H. Hoi, Xiaokui Xiao, and Boshen Zhang. AR-miner: Mining informative reviews for developers from mobile app marketplace. In Proceedings of the 36th International Conference on Software Engineering, ICSE 2014, pages 767-778. ACM, 2014.\n\nApp store analysis: Mining app stores for relationships between customer, business and technical characteristics. Anthony Finkelstein, Mark Harman, Yue Jia, William Martin, Federica Sarro, Yuanyuan Zhang, UCL Department of Computer ScienceReseach Note RN/14/10Anthony Finkelstein, Mark Harman, Yue Jia, William Martin, Federica Sarro, and Yuanyuan Zhang. App store analysis: Mining app stores for relationships between cus- tomer, business and technical characteristics. Reseach Note RN/14/10, UCL Department of Computer Science, 2014.\n\nEarly failure prediction in feature request management systems. C Fitzgerald, E Letier, A Finkelstein, Proceedings of the 2011 IEEE 19th International Requirements Engineering Conference, RE '11. the 2011 IEEE 19th International Requirements Engineering Conference, RE '11IEEE Computer SocietyC. Fitzgerald, E. Letier, and A. Finkelstein. Early failure prediction in feature request management systems. In Proceedings of the 2011 IEEE 19th International Requirements Engineering Conference, RE '11, pages 229-238. IEEE Computer Society, 2011.\n\nAnalysis of user comments: an approach for software requirements evolution. Laura V Galvis Carre\u00f1o, Kristina Winbladh, ICSE '13 Proceedings of the 2013 International Conference on Software Engineering. IEEE PressLaura V. Galvis Carre\u00f1o and Kristina Winbladh. Analysis of user comments: an approach for software requirements evolution. In ICSE '13 Proceedings of the 2013 International Conference on Software Engineering, pages 582-591. IEEE Press, 2013.\n\nNumber of mobile app downloads worldwide from. Gartner, Gartner IncTechnical reportin millionsGartner. Number of mobile app downloads worldwide from 2009 to 2017 (in millions). Technical report, Gartner Inc., March 2015.\n\nChecking app behavior against app descriptions. Alessandra Gorla, Ilaria Tavecchia, Florian Gross, Andreas Zeller, Proceedings of the 36th International Conference on Software Engineering. the 36th International Conference on Software EngineeringACMAlessandra Gorla, Ilaria Tavecchia, Florian Gross, and Andreas Zeller. Checking app behavior against app descriptions. In Proceedings of the 36th International Conference on Software Engineering, pages 1025-1035. ACM, 2014.\n\nTowards crowd-based requirements engineering: A research preview. Eduard C Groen, Joerg Doerr, Sebastian Adam, REFSQ 2015, number 9013 in LNCS. Springer InternationalEduard C. Groen, Joerg Doerr, and Sebastian Adam. Towards crowd-based requirements engineering: A research preview. In REFSQ 2015, number 9013 in LNCS, pages 247-253. Springer International, 2015.\n\nHow do users like this feature? a fine grained sentiment analysis of app reviews. E Guzman, W Maalej, Requirements Engineering Conference (RE). E. Guzman and W. Maalej. How do users like this feature? a fine grained sentiment analysis of app reviews. In Requirements Engineering Conference (RE), 2014 IEEE 22nd International, pages 153-162, 2014.\n\nApp store mining and analysis: MSR for app stores. M Harman, Yue Jia, Yuanyuan Zhang, Proc. of Working Conference on Mining Software Repositories -MSR '12. of Working Conference on Mining Software Repositories -MSR '12M. Harman, Yue Jia, and Yuanyuan Zhang. App store mining and analysis: MSR for app stores. In Proc. of Working Conference on Mining Software Repositories -MSR '12, pages 108-111, June 2012.\n\nThe 36th CREST Open Workshop. Mark Harman, University College LondonMark Harman et al., editors. The 36th CREST Open Workshop. University College London, October 2014.\n\nIt's Not a Bug, It's a Feature: How Misclassification Impacts Bug Prediction. Kim Herzig, Sascha Just, Andreas Zeller, Proceedings of the 2013 International Conference on Software Engineering. the 2013 International Conference on Software EngineeringIEEE PressKim Herzig, Sascha Just, and Andreas Zeller. It's Not a Bug, It's a Feature: How Misclassification Impacts Bug Prediction. In Proceedings of the 2013 International Conference on Software Engineering, pages 392-401. IEEE Press, 2013.\n\nA simple sequentially rejective multiple test procedure. Sture Holm, Scandinavian journal of statistics. Sture Holm. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics, pages 65-70, 1979.\n\nAn analysis of the mobile app review landscape: trends and implications. Leonard Hoon, Rajesh Vasa, Jean-Guy Schneider, John Grundy, Swinburne University of TechnologyTechnical reportLeonard Hoon, Rajesh Vasa, Jean-Guy Schneider, and John Grundy. An analysis of the mobile app review landscape: trends and implications. Technical report, Swinburne University of Technology, 2013.\n\nMining opinion features in customer reviews. Minqing Hu, Bing Liu, Proceedings of the International Conference on Knowledge Discovery and Data Mining -KDD '04. the International Conference on Knowledge Discovery and Data Mining -KDD '04AAAI PressMinqing Hu and Bing Liu. Mining opinion features in customer reviews. In Proceedings of the International Conference on Knowledge Discovery and Data Mining -KDD '04, pages 755-760. AAAI Press, July 2004.\n\nRetrieving and analyzing mobile apps feature requests from online reviews. Claudia Iacob, Rachel Harrison, MSR '13 Proceedings of the 10th Working Conference on Mining Software Repositories. IEEE PressClaudia Iacob and Rachel Harrison. Retrieving and analyzing mobile apps feature requests from online reviews. In MSR '13 Proceedings of the 10th Working Conference on Mining Software Repositories, pages 41-44. IEEE Press, 2013.\n\nBeyond the stars: exploiting free-text user reviews to improve the accuracy of movie recommendations. Niklas Jakob, Stefan Hagen Weber, Mark Christoph M\u00fcller, Iryna Gurevych, Proceeding of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion. eeding of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinionACM PressNiklas Jakob, Stefan Hagen Weber, Mark Christoph M\u00fcller, and Iryna Gurevych. Beyond the stars: exploiting free-text user reviews to improve the accuracy of movie recommen- dations. In Proceeding of the 1st international CIKM workshop on Topic-sentiment analysis for mass opinion. ACM Press, 2009.\n\nDemocratic mass participation of users in requirements engineering?. Timo Johann, Walid Maalej, Requirements Engineering Conference (RE). Timo Johann and Walid Maalej. Democratic mass participation of users in require- ments engineering? In Requirements Engineering Conference (RE), 2015 IEEE 23rd International, 2015.\n\nA user satisfaction analysis approach for software evolution. Huiying Li, Li Zhang, Lin Zhang, Jufang Shen, Progress in Informatics and Computing (PIC), 2010 IEEE International Conference on. IEEE2Huiying Li, Li Zhang, Lin Zhang, and Jufang Shen. A user satisfaction analysis approach for software evolution. In Progress in Informatics and Computing (PIC), 2010 IEEE International Conference on, volume 2, pages 1093-1097. IEEE, 2010.\n\nBug report, feature request, or simply praise? on automatically classifying app reviews. W Maalej, H Nabil, Requirements Engineering Conference (RE), 2015 IEEE 23rd International. W. Maalej and H. Nabil. Bug report, feature request, or simply praise? on automatically classifying app reviews. In Requirements Engineering Conference (RE), 2015 IEEE 23rd International, pages 116-125, Aug 2015.\n\nWhen users become collaborators. Walid Maalej, Hans-J\u00f6rg Happel, Asarnusch Rashid, Proceeding of the 24th ACM SIGPLAN conference companion on Object oriented programming systems languages and applications -OOPSLA '09. eeding of the 24th ACM SIGPLAN conference companion on Object oriented programming systems languages and applications -OOPSLA '09ACM Press981Walid Maalej, Hans-J\u00f6rg Happel, and Asarnusch Rashid. When users become collab- orators. In Proceeding of the 24th ACM SIGPLAN conference companion on Object oriented programming systems languages and applications -OOPSLA '09, page 981. ACM Press, 2009.\n\nOn the Socialness of Software. Walid Maalej, Dennis Pagano, 2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing. IEEEWalid Maalej and Dennis Pagano. On the Socialness of Software. In 2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing, pages 864-871. IEEE, 2011.\n\nTopic sentiment mixture: modeling facets and opinions in Weblogs. Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, Chengxiang Zhai, Proc. of the 16th international conference on World Wide Web. of the 16th international conference on World Wide WebQiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. Topic sentiment mixture: modeling facets and opinions in Weblogs. In Proc. of the 16th international conference on World Wide Web, pages 171-180, 2007.\n\nThe Content Analysis Guidebook. A Kimberly, Neuendorf, Sage PublicationsKimberly A Neuendorf. The Content Analysis Guidebook. Sage Publications, 2002.\n\nUser involvement in software evolution practice: A case study. Dennis Pagano, Bernd Br\u00fcgge, Proceedings of the 2013 International Conference on Software Engineering. the 2013 International Conference on Software EngineeringIEEE PressDennis Pagano and Bernd Br\u00fcgge. User involvement in software evolution practice: A case study. In Proceedings of the 2013 International Conference on Software Engineering, pages 953-962. IEEE Press, 2013.\n\nUser feedback in the appstore : an empirical study. Dennis Pagano, Walid Maalej, Proc. of the International Conference on Requirements Engineering -RE '13. of the International Conference on Requirements Engineering -RE '13Dennis Pagano and Walid Maalej. User feedback in the appstore : an empirical study. In Proc. of the International Conference on Requirements Engineering -RE '13, pages 125-134, 2013.\n\nOpinion Mining and Sentiment Analysis. Bo Pang, Lillian Lee, Found. Trends Inf. Retr. 21-2Bo Pang and Lillian Lee. Opinion Mining and Sentiment Analysis. Found. Trends Inf. Retr., 2(1-2):1-135, January 2008.\n\nExtracting product features and opinions from reviews. Ana-Maria Popescu, Oren Etzioni, Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing. the Conference on Human Language Technology and Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsAna-Maria Popescu and Oren Etzioni. Extracting product features and opinions from reviews. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 339-346. Association for Computational Linguistics, 2005.\n\nKurt Schneider, Sebastian Meyer, Maximilian Peters, Felix Schliephacke, Jonas M\u00f6rschbach, Lukas Aguirre, Product-Focused Software Process Improvement. Berlin Heidelberg; Berlin, HeidelbergSpringerKurt Schneider, Sebastian Meyer, Maximilian Peters, Felix Schliephacke, Jonas M\u00f6rschbach, and Lukas Aguirre. Product-Focused Software Process Improvement, vol- ume 6156 of Lecture Notes in Computer Science. Springer Berlin Heidelberg, Berlin, Heidelberg, 2010.\n\nUsing Mobile RE Tools to Give End-Users Their Own Voice. Norbert Seyff, Florian Graf, Neil Maiden, 18th IEEE International Requirements Engineering Conference. IEEENorbert Seyff, Florian Graf, and Neil Maiden. Using Mobile RE Tools to Give End-Users Their Own Voice. In 18th IEEE International Requirements Engineering Conference, pages 37-46. IEEE, 2010.\n\nStandish Group, Chaos report. Technical reportStandish Group. Chaos report. Technical report, 2014.\n\nSentiment strength detection for the social web. Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Journal of the American Society for Information Science and Technology. 631Mike Thelwall, Kevan Buckley, and Georgios Paltoglou. Sentiment strength detection for the social web. Journal of the American Society for Information Science and Technology, 63(1):163-173, 2012.\n\nSentiment strength detection in short informal text. Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, Arvid Kappas, Journal of the American Society for Information Science and Technology. 6112Mike Thelwall, Kevan Buckley, Georgios Paltoglou, Di Cai, and Arvid Kappas. Senti- ment strength detection in short informal text. Journal of the American Society for Information Science and Technology, 61(12):2544-2558, 2010.\n\nData Mining with R: Learning with Case Studies. Luis Torgo, Data Mining and Knowledge Discovery Series. Chapman & Hall/CRCLuis Torgo. Data Mining with R: Learning with Case Studies. Data Mining and Knowledge Discovery Series. Chapman & Hall/CRC, 2010.\n\nMonte carlo cross validation. Chemometrics and Intelligent Laboratory Systems. Qing-Song Xu, Yi-Zeng Liang, 56Qing-Song Xu and Yi-Zeng Liang. Monte carlo cross validation. Chemometrics and Intelligent Laboratory Systems, 56(1):1-11, 2001.\n", "annotations": {"author": "[{\"end\":63,\"start\":50},{\"end\":72,\"start\":64},{\"end\":93,\"start\":73},{\"end\":111,\"start\":94},{\"end\":125,\"start\":112},{\"end\":143,\"start\":126},{\"end\":161,\"start\":144},{\"end\":198,\"start\":162},{\"end\":263,\"start\":199},{\"end\":293,\"start\":264}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":56},{\"end\":71,\"start\":66},{\"end\":92,\"start\":86},{\"end\":110,\"start\":104},{\"end\":124,\"start\":118},{\"end\":142,\"start\":132},{\"end\":160,\"start\":154},{\"end\":174,\"start\":169}]", "author_first_name": "[{\"end\":55,\"start\":50},{\"end\":65,\"start\":64},{\"end\":83,\"start\":73},{\"end\":85,\"start\":84},{\"end\":103,\"start\":94},{\"end\":117,\"start\":112},{\"end\":131,\"start\":126},{\"end\":153,\"start\":144},{\"end\":168,\"start\":162}]", "author_affiliation": "[{\"end\":262,\"start\":200},{\"end\":292,\"start\":265}]", "title": "[{\"end\":47,\"start\":1},{\"end\":340,\"start\":294}]", "venue": null, "abstract": "[{\"end\":6680,\"start\":634}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7346,\"start\":7342},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10579,\"start\":10576},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10582,\"start\":10579},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10585,\"start\":10582},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13349,\"start\":13346},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14344,\"start\":14341},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15139,\"start\":15135},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15391,\"start\":15388},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15726,\"start\":15722},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16508,\"start\":16504},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16511,\"start\":16508},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16677,\"start\":16673},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17565,\"start\":17561},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17718,\"start\":17714},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17900,\"start\":17896},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18238,\"start\":18234},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19241,\"start\":19237},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21362,\"start\":21359},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21627,\"start\":21623},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21880,\"start\":21876},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23956,\"start\":23952},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24194,\"start\":24190},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28361,\"start\":28357},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30012,\"start\":30008},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35370,\"start\":35367},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38485,\"start\":38482},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40245,\"start\":40241},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40434,\"start\":40430},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":43226,\"start\":43222},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":44376,\"start\":44372},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46316,\"start\":46312},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":54119,\"start\":54115},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":59847,\"start\":59843},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":59951,\"start\":59948},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":59976,\"start\":59972},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":60229,\"start\":60225},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":60232,\"start\":60229},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":60476,\"start\":60473},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":60635,\"start\":60631},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":60738,\"start\":60734},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":60757,\"start\":60753},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":60783,\"start\":60779},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":61321,\"start\":61317},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":61523,\"start\":61520},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":61978,\"start\":61974},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":62005,\"start\":62001},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":62238,\"start\":62234},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":62280,\"start\":62276},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":62283,\"start\":62280},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":62812,\"start\":62808},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":63089,\"start\":63085},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":63116,\"start\":63112},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":63395,\"start\":63391},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":63398,\"start\":63395},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":63401,\"start\":63398},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":63524,\"start\":63520},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":63594,\"start\":63590},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":64172,\"start\":64169},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":64345,\"start\":64342},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":65521,\"start\":65517},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":66582,\"start\":66578},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":67096,\"start\":67093},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":67570,\"start\":67567},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":68121,\"start\":68117}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":70493,\"start\":70442},{\"attributes\":{\"id\":\"fig_1\"},\"end\":70682,\"start\":70494},{\"attributes\":{\"id\":\"fig_3\"},\"end\":70826,\"start\":70683},{\"attributes\":{\"id\":\"fig_4\"},\"end\":70955,\"start\":70827},{\"attributes\":{\"id\":\"fig_5\"},\"end\":71005,\"start\":70956},{\"attributes\":{\"id\":\"fig_6\"},\"end\":71053,\"start\":71006},{\"attributes\":{\"id\":\"fig_7\"},\"end\":71074,\"start\":71054},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":71521,\"start\":71075},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":71889,\"start\":71522},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":74849,\"start\":71890},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":77270,\"start\":74850},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":78171,\"start\":77271},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":79001,\"start\":78172},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":80505,\"start\":79002},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":82010,\"start\":80506},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":83062,\"start\":82011}]", "paragraph": "[{\"end\":7841,\"start\":6682},{\"end\":8575,\"start\":7843},{\"end\":8898,\"start\":8612},{\"end\":9718,\"start\":8900},{\"end\":9878,\"start\":9720},{\"end\":10861,\"start\":9916},{\"end\":11483,\"start\":10903},{\"end\":12331,\"start\":11485},{\"end\":12915,\"start\":12333},{\"end\":13183,\"start\":12967},{\"end\":14325,\"start\":13185},{\"end\":15332,\"start\":14327},{\"end\":16015,\"start\":15334},{\"end\":16678,\"start\":16074},{\"end\":17479,\"start\":16680},{\"end\":17719,\"start\":17481},{\"end\":18239,\"start\":17721},{\"end\":18795,\"start\":18241},{\"end\":19242,\"start\":18854},{\"end\":19523,\"start\":19244},{\"end\":20138,\"start\":19525},{\"end\":20958,\"start\":20140},{\"end\":21294,\"start\":20960},{\"end\":22282,\"start\":21296},{\"end\":22356,\"start\":22302},{\"end\":22601,\"start\":22379},{\"end\":23169,\"start\":22603},{\"end\":24584,\"start\":23198},{\"end\":25231,\"start\":24586},{\"end\":26371,\"start\":25233},{\"end\":27250,\"start\":26373},{\"end\":28733,\"start\":27321},{\"end\":30472,\"start\":28754},{\"end\":30896,\"start\":30502},{\"end\":31349,\"start\":30898},{\"end\":31439,\"start\":31351},{\"end\":31620,\"start\":31456},{\"end\":32277,\"start\":31622},{\"end\":32520,\"start\":32279},{\"end\":32891,\"start\":32522},{\"end\":34166,\"start\":32893},{\"end\":34472,\"start\":34219},{\"end\":34600,\"start\":34474},{\"end\":35974,\"start\":34602},{\"end\":37254,\"start\":35976},{\"end\":37346,\"start\":37269},{\"end\":37554,\"start\":37379},{\"end\":38311,\"start\":37556},{\"end\":39319,\"start\":38313},{\"end\":40541,\"start\":39321},{\"end\":41200,\"start\":40582},{\"end\":42346,\"start\":41202},{\"end\":42784,\"start\":42348},{\"end\":43139,\"start\":42824},{\"end\":43760,\"start\":43141},{\"end\":44206,\"start\":43762},{\"end\":44568,\"start\":44208},{\"end\":45704,\"start\":44570},{\"end\":46341,\"start\":45706},{\"end\":46896,\"start\":46343},{\"end\":47164,\"start\":46924},{\"end\":47821,\"start\":47166},{\"end\":48239,\"start\":47823},{\"end\":48651,\"start\":48241},{\"end\":49014,\"start\":48686},{\"end\":49773,\"start\":49016},{\"end\":51704,\"start\":49775},{\"end\":53489,\"start\":51723},{\"end\":54867,\"start\":53491},{\"end\":55402,\"start\":54869},{\"end\":56387,\"start\":55404},{\"end\":57001,\"start\":56389},{\"end\":58316,\"start\":57003},{\"end\":59488,\"start\":58318},{\"end\":59667,\"start\":59505},{\"end\":60369,\"start\":59716},{\"end\":61212,\"start\":60371},{\"end\":61483,\"start\":61235},{\"end\":61960,\"start\":61485},{\"end\":62659,\"start\":61962},{\"end\":63978,\"start\":62661},{\"end\":65443,\"start\":63980},{\"end\":66359,\"start\":65445},{\"end\":67075,\"start\":66399},{\"end\":67547,\"start\":67077},{\"end\":68017,\"start\":67549},{\"end\":68386,\"start\":68019},{\"end\":70441,\"start\":68401}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":27320,\"start\":27251}]", "table_ref": "[{\"end\":10475,\"start\":10468},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24683,\"start\":24676},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26520,\"start\":26513},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28925,\"start\":28918},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29121,\"start\":29114},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29424,\"start\":29417},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33358,\"start\":33351},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33468,\"start\":33461},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36215,\"start\":36195},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":37029,\"start\":37022},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":55145,\"start\":55136}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":8610,\"start\":8578},{\"attributes\":{\"n\":\"2.1\"},\"end\":9914,\"start\":9881},{\"attributes\":{\"n\":\"2.2\"},\"end\":10901,\"start\":10864},{\"attributes\":{\"n\":\"2.3\"},\"end\":12965,\"start\":12918},{\"attributes\":{\"n\":\"2.4\"},\"end\":16072,\"start\":16018},{\"attributes\":{\"n\":\"2.5\"},\"end\":18852,\"start\":18798},{\"attributes\":{\"n\":\"3\"},\"end\":22300,\"start\":22285},{\"attributes\":{\"n\":\"3.1\"},\"end\":22377,\"start\":22359},{\"attributes\":{\"n\":\"3.2\"},\"end\":23196,\"start\":23172},{\"attributes\":{\"n\":\"4\"},\"end\":28752,\"start\":28736},{\"attributes\":{\"n\":\"4.1\"},\"end\":30500,\"start\":30475},{\"attributes\":{\"n\":\"4.2\"},\"end\":31454,\"start\":31442},{\"attributes\":{\"n\":\"4.3\"},\"end\":34194,\"start\":34169},{\"attributes\":{\"n\":\"4.4\"},\"end\":34217,\"start\":34197},{\"attributes\":{\"n\":\"5\"},\"end\":37267,\"start\":37257},{\"attributes\":{\"n\":\"5.1\"},\"end\":37377,\"start\":37349},{\"attributes\":{\"n\":\"5.2\"},\"end\":40580,\"start\":40544},{\"attributes\":{\"n\":\"5.3\"},\"end\":42822,\"start\":42787},{\"attributes\":{\"n\":\"6\"},\"end\":46922,\"start\":46899},{\"attributes\":{\"n\":\"6.1\"},\"end\":48684,\"start\":48654},{\"attributes\":{\"n\":\"6.2\"},\"end\":51721,\"start\":51707},{\"attributes\":{\"n\":\"7\"},\"end\":59503,\"start\":59491},{\"attributes\":{\"n\":\"7.1\"},\"end\":59714,\"start\":59670},{\"attributes\":{\"n\":\"7.2\"},\"end\":61233,\"start\":61215},{\"attributes\":{\"n\":\"7.3\"},\"end\":66397,\"start\":66362},{\"attributes\":{\"n\":\"8\"},\"end\":68399,\"start\":68389},{\"end\":70451,\"start\":70443},{\"end\":70503,\"start\":70495},{\"end\":70692,\"start\":70684},{\"end\":70836,\"start\":70828},{\"end\":70965,\"start\":70957},{\"end\":71015,\"start\":71007},{\"end\":71063,\"start\":71055},{\"end\":71085,\"start\":71076},{\"end\":71532,\"start\":71523},{\"end\":71900,\"start\":71891},{\"end\":74860,\"start\":74851},{\"end\":77281,\"start\":77272},{\"end\":78182,\"start\":78173},{\"end\":79012,\"start\":79003},{\"end\":80516,\"start\":80507},{\"end\":82022,\"start\":82012}]", "table": "[{\"end\":71521,\"start\":71119},{\"end\":71889,\"start\":71582},{\"end\":74849,\"start\":72180},{\"end\":77270,\"start\":75027},{\"end\":78171,\"start\":77415},{\"end\":79001,\"start\":78333},{\"end\":80505,\"start\":79115},{\"end\":82010,\"start\":80621},{\"end\":83062,\"start\":82170}]", "figure_caption": "[{\"end\":70493,\"start\":70453},{\"end\":70682,\"start\":70505},{\"end\":70826,\"start\":70694},{\"end\":70955,\"start\":70838},{\"end\":71005,\"start\":70967},{\"end\":71053,\"start\":71017},{\"end\":71074,\"start\":71065},{\"end\":71119,\"start\":71087},{\"end\":71582,\"start\":71534},{\"end\":72180,\"start\":71902},{\"end\":75027,\"start\":74862},{\"end\":77415,\"start\":77283},{\"end\":78333,\"start\":78184},{\"end\":79115,\"start\":79014},{\"end\":80621,\"start\":80518},{\"end\":82170,\"start\":82025}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25992,\"start\":25984},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34363,\"start\":34355},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":35655,\"start\":35647},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":50234,\"start\":50226},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":50551,\"start\":50543},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":50907,\"start\":50899},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":51911,\"start\":51903},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":52574,\"start\":52566},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":57904,\"start\":57896},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":57917,\"start\":57909}]", "bib_author_first_name": "[{\"end\":83831,\"start\":83823},{\"end\":83847,\"start\":83842},{\"end\":83867,\"start\":83855},{\"end\":83884,\"start\":83878},{\"end\":83901,\"start\":83892},{\"end\":84590,\"start\":84583},{\"end\":84602,\"start\":84597},{\"end\":84856,\"start\":84849},{\"end\":84875,\"start\":84869},{\"end\":84888,\"start\":84882},{\"end\":84906,\"start\":84899},{\"end\":84919,\"start\":84914},{\"end\":84935,\"start\":84929},{\"end\":85166,\"start\":85161},{\"end\":85436,\"start\":85430},{\"end\":85447,\"start\":85443},{\"end\":85461,\"start\":85455},{\"end\":85663,\"start\":85659},{\"end\":85676,\"start\":85670},{\"end\":85683,\"start\":85682},{\"end\":85685,\"start\":85684},{\"end\":85701,\"start\":85694},{\"end\":85713,\"start\":85707},{\"end\":86273,\"start\":86266},{\"end\":86291,\"start\":86287},{\"end\":86303,\"start\":86300},{\"end\":86316,\"start\":86309},{\"end\":86333,\"start\":86325},{\"end\":86349,\"start\":86341},{\"end\":86754,\"start\":86753},{\"end\":86768,\"start\":86767},{\"end\":86778,\"start\":86777},{\"end\":87314,\"start\":87309},{\"end\":87316,\"start\":87315},{\"end\":87341,\"start\":87333},{\"end\":87968,\"start\":87958},{\"end\":87982,\"start\":87976},{\"end\":88001,\"start\":87994},{\"end\":88016,\"start\":88009},{\"end\":88456,\"start\":88450},{\"end\":88458,\"start\":88457},{\"end\":88471,\"start\":88466},{\"end\":88488,\"start\":88479},{\"end\":88831,\"start\":88830},{\"end\":88841,\"start\":88840},{\"end\":89148,\"start\":89147},{\"end\":89160,\"start\":89157},{\"end\":89174,\"start\":89166},{\"end\":89539,\"start\":89535},{\"end\":89755,\"start\":89752},{\"end\":89770,\"start\":89764},{\"end\":89784,\"start\":89777},{\"end\":90230,\"start\":90225},{\"end\":90478,\"start\":90471},{\"end\":90491,\"start\":90485},{\"end\":90506,\"start\":90498},{\"end\":90522,\"start\":90518},{\"end\":90831,\"start\":90824},{\"end\":90840,\"start\":90836},{\"end\":91312,\"start\":91305},{\"end\":91326,\"start\":91320},{\"end\":91768,\"start\":91762},{\"end\":91782,\"start\":91776},{\"end\":91788,\"start\":91783},{\"end\":91800,\"start\":91796},{\"end\":91810,\"start\":91801},{\"end\":91824,\"start\":91819},{\"end\":92401,\"start\":92397},{\"end\":92415,\"start\":92410},{\"end\":92717,\"start\":92710},{\"end\":92724,\"start\":92722},{\"end\":92735,\"start\":92732},{\"end\":92749,\"start\":92743},{\"end\":93174,\"start\":93173},{\"end\":93184,\"start\":93183},{\"end\":93516,\"start\":93511},{\"end\":93534,\"start\":93525},{\"end\":93552,\"start\":93543},{\"end\":94128,\"start\":94123},{\"end\":94143,\"start\":94137},{\"end\":94499,\"start\":94492},{\"end\":94507,\"start\":94505},{\"end\":94521,\"start\":94514},{\"end\":94534,\"start\":94530},{\"end\":94549,\"start\":94539},{\"end\":94926,\"start\":94925},{\"end\":95114,\"start\":95108},{\"end\":95128,\"start\":95123},{\"end\":95542,\"start\":95536},{\"end\":95556,\"start\":95551},{\"end\":95932,\"start\":95930},{\"end\":95946,\"start\":95939},{\"end\":96164,\"start\":96155},{\"end\":96178,\"start\":96174},{\"end\":96714,\"start\":96710},{\"end\":96735,\"start\":96726},{\"end\":96753,\"start\":96743},{\"end\":96767,\"start\":96762},{\"end\":96787,\"start\":96782},{\"end\":96805,\"start\":96800},{\"end\":97232,\"start\":97225},{\"end\":97247,\"start\":97240},{\"end\":97258,\"start\":97254},{\"end\":97533,\"start\":97525},{\"end\":97679,\"start\":97675},{\"end\":97695,\"start\":97690},{\"end\":97713,\"start\":97705},{\"end\":98054,\"start\":98050},{\"end\":98070,\"start\":98065},{\"end\":98088,\"start\":98080},{\"end\":98102,\"start\":98100},{\"end\":98113,\"start\":98108},{\"end\":98478,\"start\":98474},{\"end\":98767,\"start\":98758},{\"end\":98779,\"start\":98772}]", "bib_author_last_name": "[{\"end\":83840,\"start\":83832},{\"end\":83853,\"start\":83848},{\"end\":83876,\"start\":83868},{\"end\":83890,\"start\":83885},{\"end\":83911,\"start\":83902},{\"end\":84595,\"start\":84591},{\"end\":84609,\"start\":84603},{\"end\":84867,\"start\":84857},{\"end\":84880,\"start\":84876},{\"end\":84897,\"start\":84889},{\"end\":84912,\"start\":84907},{\"end\":84927,\"start\":84920},{\"end\":84946,\"start\":84936},{\"end\":85173,\"start\":85167},{\"end\":85441,\"start\":85437},{\"end\":85453,\"start\":85448},{\"end\":85467,\"start\":85462},{\"end\":85668,\"start\":85664},{\"end\":85680,\"start\":85677},{\"end\":85692,\"start\":85686},{\"end\":85705,\"start\":85702},{\"end\":85718,\"start\":85714},{\"end\":85725,\"start\":85720},{\"end\":86285,\"start\":86274},{\"end\":86298,\"start\":86292},{\"end\":86307,\"start\":86304},{\"end\":86323,\"start\":86317},{\"end\":86339,\"start\":86334},{\"end\":86355,\"start\":86350},{\"end\":86765,\"start\":86755},{\"end\":86775,\"start\":86769},{\"end\":86790,\"start\":86779},{\"end\":87331,\"start\":87317},{\"end\":87350,\"start\":87342},{\"end\":87742,\"start\":87735},{\"end\":87974,\"start\":87969},{\"end\":87992,\"start\":87983},{\"end\":88007,\"start\":88002},{\"end\":88023,\"start\":88017},{\"end\":88464,\"start\":88459},{\"end\":88477,\"start\":88472},{\"end\":88493,\"start\":88489},{\"end\":88838,\"start\":88832},{\"end\":88848,\"start\":88842},{\"end\":89155,\"start\":89149},{\"end\":89164,\"start\":89161},{\"end\":89180,\"start\":89175},{\"end\":89546,\"start\":89540},{\"end\":89762,\"start\":89756},{\"end\":89775,\"start\":89771},{\"end\":89791,\"start\":89785},{\"end\":90235,\"start\":90231},{\"end\":90483,\"start\":90479},{\"end\":90496,\"start\":90492},{\"end\":90516,\"start\":90507},{\"end\":90529,\"start\":90523},{\"end\":90834,\"start\":90832},{\"end\":90844,\"start\":90841},{\"end\":91318,\"start\":91313},{\"end\":91335,\"start\":91327},{\"end\":91774,\"start\":91769},{\"end\":91794,\"start\":91789},{\"end\":91817,\"start\":91811},{\"end\":91833,\"start\":91825},{\"end\":92408,\"start\":92402},{\"end\":92422,\"start\":92416},{\"end\":92720,\"start\":92718},{\"end\":92730,\"start\":92725},{\"end\":92741,\"start\":92736},{\"end\":92754,\"start\":92750},{\"end\":93181,\"start\":93175},{\"end\":93190,\"start\":93185},{\"end\":93523,\"start\":93517},{\"end\":93541,\"start\":93535},{\"end\":93559,\"start\":93553},{\"end\":94135,\"start\":94129},{\"end\":94150,\"start\":94144},{\"end\":94503,\"start\":94500},{\"end\":94512,\"start\":94508},{\"end\":94528,\"start\":94522},{\"end\":94537,\"start\":94535},{\"end\":94554,\"start\":94550},{\"end\":94935,\"start\":94927},{\"end\":94946,\"start\":94937},{\"end\":95121,\"start\":95115},{\"end\":95135,\"start\":95129},{\"end\":95549,\"start\":95543},{\"end\":95563,\"start\":95557},{\"end\":95937,\"start\":95933},{\"end\":95950,\"start\":95947},{\"end\":96172,\"start\":96165},{\"end\":96186,\"start\":96179},{\"end\":96724,\"start\":96715},{\"end\":96741,\"start\":96736},{\"end\":96760,\"start\":96754},{\"end\":96780,\"start\":96768},{\"end\":96798,\"start\":96788},{\"end\":96813,\"start\":96806},{\"end\":97238,\"start\":97233},{\"end\":97252,\"start\":97248},{\"end\":97265,\"start\":97259},{\"end\":97539,\"start\":97534},{\"end\":97688,\"start\":97680},{\"end\":97703,\"start\":97696},{\"end\":97723,\"start\":97714},{\"end\":98063,\"start\":98055},{\"end\":98078,\"start\":98071},{\"end\":98098,\"start\":98089},{\"end\":98106,\"start\":98103},{\"end\":98120,\"start\":98114},{\"end\":98484,\"start\":98479},{\"end\":98770,\"start\":98768},{\"end\":98785,\"start\":98780}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":214266},\"end\":84496,\"start\":83740},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10542149},\"end\":84816,\"start\":84498},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5913612},\"end\":85157,\"start\":84818},{\"attributes\":{\"id\":\"b3\"},\"end\":85194,\"start\":85159},{\"attributes\":{\"id\":\"b4\"},\"end\":85387,\"start\":85196},{\"attributes\":{\"id\":\"b5\"},\"end\":85576,\"start\":85389},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3078225},\"end\":86150,\"start\":85578},{\"attributes\":{\"id\":\"b7\"},\"end\":86687,\"start\":86152},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":39435085},\"end\":87231,\"start\":86689},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10199190},\"end\":87686,\"start\":87233},{\"attributes\":{\"id\":\"b10\"},\"end\":87908,\"start\":87688},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":8233204},\"end\":88382,\"start\":87910},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":536375},\"end\":88746,\"start\":88384},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4860496},\"end\":89094,\"start\":88748},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1469807},\"end\":89503,\"start\":89096},{\"attributes\":{\"id\":\"b15\"},\"end\":89672,\"start\":89505},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":713631},\"end\":90166,\"start\":89674},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":122415379},\"end\":90396,\"start\":90168},{\"attributes\":{\"id\":\"b18\"},\"end\":90777,\"start\":90398},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5724860},\"end\":91228,\"start\":90779},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7829630},\"end\":91658,\"start\":91230},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12360833},\"end\":92326,\"start\":91660},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":26741563},\"end\":92646,\"start\":92328},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":18623619},\"end\":93082,\"start\":92648},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":16177318},\"end\":93476,\"start\":93084},{\"attributes\":{\"id\":\"b25\"},\"end\":94090,\"start\":93478},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":15091989},\"end\":94424,\"start\":94092},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5965756},\"end\":94891,\"start\":94426},{\"attributes\":{\"id\":\"b28\"},\"end\":95043,\"start\":94893},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":277264},\"end\":95482,\"start\":95045},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1893408},\"end\":95889,\"start\":95484},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":43902914},\"end\":96098,\"start\":95891},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":631855},\"end\":96708,\"start\":96100},{\"attributes\":{\"id\":\"b33\"},\"end\":97166,\"start\":96710},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2824623},\"end\":97523,\"start\":97168},{\"attributes\":{\"id\":\"b35\"},\"end\":97624,\"start\":97525},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":42429082},\"end\":97995,\"start\":97626},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":215834710},\"end\":98424,\"start\":97997},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":134419},\"end\":98677,\"start\":98426},{\"attributes\":{\"id\":\"b39\"},\"end\":98917,\"start\":98679}]", "bib_title": "[{\"end\":83821,\"start\":83740},{\"end\":84581,\"start\":84498},{\"end\":84847,\"start\":84818},{\"end\":85657,\"start\":85578},{\"end\":86751,\"start\":86689},{\"end\":87307,\"start\":87233},{\"end\":87956,\"start\":87910},{\"end\":88448,\"start\":88384},{\"end\":88828,\"start\":88748},{\"end\":89145,\"start\":89096},{\"end\":89750,\"start\":89674},{\"end\":90223,\"start\":90168},{\"end\":90822,\"start\":90779},{\"end\":91303,\"start\":91230},{\"end\":91760,\"start\":91660},{\"end\":92395,\"start\":92328},{\"end\":92708,\"start\":92648},{\"end\":93171,\"start\":93084},{\"end\":93509,\"start\":93478},{\"end\":94121,\"start\":94092},{\"end\":94490,\"start\":94426},{\"end\":95106,\"start\":95045},{\"end\":95534,\"start\":95484},{\"end\":95928,\"start\":95891},{\"end\":96153,\"start\":96100},{\"end\":97223,\"start\":97168},{\"end\":97673,\"start\":97626},{\"end\":98048,\"start\":97997},{\"end\":98472,\"start\":98426}]", "bib_author": "[{\"end\":83842,\"start\":83823},{\"end\":83855,\"start\":83842},{\"end\":83878,\"start\":83855},{\"end\":83892,\"start\":83878},{\"end\":83913,\"start\":83892},{\"end\":84597,\"start\":84583},{\"end\":84611,\"start\":84597},{\"end\":84869,\"start\":84849},{\"end\":84882,\"start\":84869},{\"end\":84899,\"start\":84882},{\"end\":84914,\"start\":84899},{\"end\":84929,\"start\":84914},{\"end\":84948,\"start\":84929},{\"end\":85175,\"start\":85161},{\"end\":85443,\"start\":85430},{\"end\":85455,\"start\":85443},{\"end\":85469,\"start\":85455},{\"end\":85670,\"start\":85659},{\"end\":85682,\"start\":85670},{\"end\":85694,\"start\":85682},{\"end\":85707,\"start\":85694},{\"end\":85720,\"start\":85707},{\"end\":85727,\"start\":85720},{\"end\":86287,\"start\":86266},{\"end\":86300,\"start\":86287},{\"end\":86309,\"start\":86300},{\"end\":86325,\"start\":86309},{\"end\":86341,\"start\":86325},{\"end\":86357,\"start\":86341},{\"end\":86767,\"start\":86753},{\"end\":86777,\"start\":86767},{\"end\":86792,\"start\":86777},{\"end\":87333,\"start\":87309},{\"end\":87352,\"start\":87333},{\"end\":87744,\"start\":87735},{\"end\":87976,\"start\":87958},{\"end\":87994,\"start\":87976},{\"end\":88009,\"start\":87994},{\"end\":88025,\"start\":88009},{\"end\":88466,\"start\":88450},{\"end\":88479,\"start\":88466},{\"end\":88495,\"start\":88479},{\"end\":88840,\"start\":88830},{\"end\":88850,\"start\":88840},{\"end\":89157,\"start\":89147},{\"end\":89166,\"start\":89157},{\"end\":89182,\"start\":89166},{\"end\":89548,\"start\":89535},{\"end\":89764,\"start\":89752},{\"end\":89777,\"start\":89764},{\"end\":89793,\"start\":89777},{\"end\":90237,\"start\":90225},{\"end\":90485,\"start\":90471},{\"end\":90498,\"start\":90485},{\"end\":90518,\"start\":90498},{\"end\":90531,\"start\":90518},{\"end\":90836,\"start\":90824},{\"end\":90846,\"start\":90836},{\"end\":91320,\"start\":91305},{\"end\":91337,\"start\":91320},{\"end\":91776,\"start\":91762},{\"end\":91796,\"start\":91776},{\"end\":91819,\"start\":91796},{\"end\":91835,\"start\":91819},{\"end\":92410,\"start\":92397},{\"end\":92424,\"start\":92410},{\"end\":92722,\"start\":92710},{\"end\":92732,\"start\":92722},{\"end\":92743,\"start\":92732},{\"end\":92756,\"start\":92743},{\"end\":93183,\"start\":93173},{\"end\":93192,\"start\":93183},{\"end\":93525,\"start\":93511},{\"end\":93543,\"start\":93525},{\"end\":93561,\"start\":93543},{\"end\":94137,\"start\":94123},{\"end\":94152,\"start\":94137},{\"end\":94505,\"start\":94492},{\"end\":94514,\"start\":94505},{\"end\":94530,\"start\":94514},{\"end\":94539,\"start\":94530},{\"end\":94556,\"start\":94539},{\"end\":94937,\"start\":94925},{\"end\":94948,\"start\":94937},{\"end\":95123,\"start\":95108},{\"end\":95137,\"start\":95123},{\"end\":95551,\"start\":95536},{\"end\":95565,\"start\":95551},{\"end\":95939,\"start\":95930},{\"end\":95952,\"start\":95939},{\"end\":96174,\"start\":96155},{\"end\":96188,\"start\":96174},{\"end\":96726,\"start\":96710},{\"end\":96743,\"start\":96726},{\"end\":96762,\"start\":96743},{\"end\":96782,\"start\":96762},{\"end\":96800,\"start\":96782},{\"end\":96815,\"start\":96800},{\"end\":97240,\"start\":97225},{\"end\":97254,\"start\":97240},{\"end\":97267,\"start\":97254},{\"end\":97541,\"start\":97525},{\"end\":97690,\"start\":97675},{\"end\":97705,\"start\":97690},{\"end\":97725,\"start\":97705},{\"end\":98065,\"start\":98050},{\"end\":98080,\"start\":98065},{\"end\":98100,\"start\":98080},{\"end\":98108,\"start\":98100},{\"end\":98122,\"start\":98108},{\"end\":98486,\"start\":98474},{\"end\":98772,\"start\":98758},{\"end\":98787,\"start\":98772}]", "bib_venue": "[{\"end\":84150,\"start\":84040},{\"end\":84989,\"start\":84977},{\"end\":85880,\"start\":85812},{\"end\":86961,\"start\":86885},{\"end\":88156,\"start\":88099},{\"end\":89314,\"start\":89252},{\"end\":89924,\"start\":89867},{\"end\":91015,\"start\":90939},{\"end\":92021,\"start\":91931},{\"end\":93825,\"start\":93696},{\"end\":94672,\"start\":94618},{\"end\":95268,\"start\":95211},{\"end\":95707,\"start\":95640},{\"end\":96397,\"start\":96301},{\"end\":96898,\"start\":96861},{\"end\":84038,\"start\":83913},{\"end\":84644,\"start\":84611},{\"end\":84975,\"start\":84948},{\"end\":85266,\"start\":85196},{\"end\":85428,\"start\":85389},{\"end\":85810,\"start\":85727},{\"end\":86264,\"start\":86152},{\"end\":86883,\"start\":86792},{\"end\":87433,\"start\":87352},{\"end\":87733,\"start\":87688},{\"end\":88097,\"start\":88025},{\"end\":88526,\"start\":88495},{\"end\":88890,\"start\":88850},{\"end\":89250,\"start\":89182},{\"end\":89533,\"start\":89505},{\"end\":89865,\"start\":89793},{\"end\":90271,\"start\":90237},{\"end\":90469,\"start\":90398},{\"end\":90937,\"start\":90846},{\"end\":91419,\"start\":91337},{\"end\":91929,\"start\":91835},{\"end\":92464,\"start\":92424},{\"end\":92838,\"start\":92756},{\"end\":93262,\"start\":93192},{\"end\":93694,\"start\":93561},{\"end\":94238,\"start\":94152},{\"end\":94616,\"start\":94556},{\"end\":94923,\"start\":94893},{\"end\":95209,\"start\":95137},{\"end\":95638,\"start\":95565},{\"end\":95975,\"start\":95952},{\"end\":96299,\"start\":96188},{\"end\":96859,\"start\":96815},{\"end\":97326,\"start\":97267},{\"end\":97553,\"start\":97541},{\"end\":97795,\"start\":97725},{\"end\":98192,\"start\":98122},{\"end\":98528,\"start\":98486},{\"end\":98756,\"start\":98679}]"}}}, "year": 2023, "month": 12, "day": 17}