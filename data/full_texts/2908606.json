{"id": 2908606, "updated": "2023-09-29 14:40:12.412", "metadata": {"title": "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition", "authors": "[{\"first\":\"Yandong\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yuxiao\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Xiaodong\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Jianfeng\",\"last\":\"Gao\",\"middle\":[]}]", "venue": "ECCV", "journal": "87-102", "publication_date": {"year": 2016, "month": 7, "day": 27}, "abstract": "In this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1607.08221", "mag": "2952419167", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/GuoZHHG16", "doi": "10.1007/978-3-319-46487-9_6"}}, "content": {"source": {"pdf_hash": "4603cb8e05258bb0572ae912ad20903b8f99f4b1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1607.08221v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1607.08221", "status": "GREEN"}}, "grobid": {"id": "11145b0202ac4b6a66cec670da25cc5f0bf62e7a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4603cb8e05258bb0572ae912ad20903b8f99f4b1.txt", "contents": "\nMS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition\n\n\nYandong Guo yandong.guo@microsoft.com \nMicrosoft Research\n\n\nLei Zhang leizhang@microsoft.com \nMicrosoft Research\n\n\nYuxiao Hu yuxiao.hu@microsoft.com \nMicrosoft Research\n\n\nXiaodong He \nMicrosoft Research\n\n\nJianfeng Gao jfgao@microsoft.com \nMicrosoft Research\n\n\nMS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition\nFace recognitionlarge scalebenchmarktraining datacelebrity recognitionknowledge base\nIn this paper, we design a benchmark task and provide the associated datasets for recognizing face images and link them to corresponding entity keys in a knowledge base. More specifically, we propose a benchmark task to recognize one million celebrities from their face images, by using all the possibly collected face images of this individual on the web as training data. The rich information provided by the knowledge base helps to conduct disambiguation and improve the recognition accuracy, and contributes to various real-world applications, such as image captioning and news video analysis. Associated with this task, we design and provide concrete measurement set, evaluation protocol, as well as training data. We also present in details our experiment setup and report promising baseline results. Our benchmark task could lead to one of the largest classification problems in computer vision. To the best of our knowledge, our training dataset, which contains 10M images in version 1, is the largest publicly available one in the world.\n\nIntroduction\n\nIn this paper, we design a benchmark task as to recognize one million celebrities from their face images and identify them by linking to the unique entity keys in a knowledge base. We also construct associated datasets to train and test for this benchmark task. Our paper is mainly to close the following two gaps in current face recognition, as reported in [1]. First, there has not been enough effort in determining the identity of a person from a face image with disambiguation, especially at the web scale. The current face identification task mainly focuses on finding similar images (in terms of certain types of distance metric) for the input image, rather than answering questions such as \"who is in the image?\" and \"if it is Anne in the image, which Anne?\". This lacks an important step of \"recognizing\". The second gap is about the scale. The publicly available datasets are much smaller than that being used privately in industry, such as Facebook [2,3] and Google [4], as summarized in Table 1. Though the research in face This paper is published at ECCV 2016. arXiv:1607.08221v1 [cs.CV] 27 Jul 2016 recognition highly desires large datasets consisting of many distinct people, such large dataset is not easily or publicly accessible to most researchers. This greatly limits the contributions from research groups, especially in academia.\n\nOur benchmark task has the following properties. First, we define our face recognition as to determine the identity of a person from his/her face images. More specifically, we introduce a knowledge base into face recognition, since the recent advance in knowledge bases has demonstrated incredible capability of providing accurate identifiers and rich properties for celebrities. Examples include Satori knowledge graph in Microsoft and \"freebase\" in [5]. Our face recognition task is demonstrated in Fig. 1. An example of our face recognition task. Our task is to recognize the face in the image and then link this face with the corresponding entity key in the knowledge base. By recognizing the left image to be \"Anne Hathaway\" and linking to the entity key, we know she is an American actress born in 1982, who has played Mia Thermopolis in The Princess Diaries, not the other Anne Hathaway who was the wife of William Shakespeare. Input image is from the web. 2 Linking the image with an entity key in the knowledge base, rather than an isolated string for a person's name naturally solves the disambiguation issue in the traditional face recognition task. Moreover, the linked entity key is associated with rich and comprehensive property information in the knowledge base, which makes our task more similar to human behavior compared with traditional face identification, since retrieving the individual's name as well as the associated information naturally takes place when humans are viewing a face image. The rich information makes our face recognition task practical and beneficial to many real applications, including image search, ranking, caption generation, image deep understanding, etc. Second, our benchmark task targets at recognizing celebrities. Recognizing celebrities, rather than a pre-selected private group of people, represents public interest and could be directly applied to a wide range of real scenarios. More-over, only with popular celebrities, we can leverage the existing information (e.g. name, profession) in the knowledge base and the information on the web to build a large-scale dataset which is publicly available for training, measurement, and re-distributing under certain licenses. The security department may have many labeled face images for criminal identification, but the data can not be publicly shared.\n\nThird, we select one million celebrities from freebase and provide their associated entity keys, and encourage researchers to build recognizers to identify each people entity. Considering each entity as one class may lead to, to the best of our knowledge, the largest classification problem in computer vision. The clear definition and mutually exclusiveness of these classes are supported by the unique entity keys and their associated properties provided by the knowledge base, since in our dataset, there are a significant amount of celebrities having same/similar names. This is different from generic image classification, where to obtain a large number of exclusive classes with clear definition itself is a challenging and open problem [6].\n\nThe large scale of our problem naturally introduces the following attractive challenges. With the increased number of classes, the inter-class variance tends to decrease. There are celebrities look very similar to each other (or even twins) in our one-million list. Moreover, large intra-class variance is introduced by popular celebrities with millions of images available, as well as celebrities with very large appearance variation (e.g., due to age, makeups, or even sex reassignment surgery).\n\nIn order to evaluate the performance of our benchmark task, we provide concrete measurement set and evaluation protocol. Our measurement set consists of images for a subset of celebrities in our one-million celebrity list. The celebrities are selected in a way that, our measurement set mainly focuses on popular celebrities to represent the interest of real application and users, while the measurement set still maintains enough (about 25%) tail celebrities to encourage the performance on celebrity coverage. We manually label images for these celebrities carefully. The correctness of our labeling is ensured by deep research on the web content, consensus verification, and multiple iterations of carefully review. In order to make our measurement more challenging, we blend a set of distractor images with this set of carefully labeled images. The distractor images are images of other celebrities or ordinary people on the web, which are mainly used to hide the celebrities we select in the measurement.\n\nAlong with this challenging yet attractive large scale benchmark task proposed, we also provide a very large training dataset to facilitate the task. The training dataset contains about 10M images for 100K top celebrities selected from our one-million celebrity list in terms of their web appearance frequency. Our training data is, to the best of our knowledge, the largest publicly available one in the world, as shown in Table 1. We plan to further extend the size in the near future. For each of the image in our training data, we provide the thumbnail of the original image and cropped face region from the original image (with/without alignment). This is to maximize the convenience for the researchers to investigate using this data.\n\nWith this training data, we trained a convolutional deep neural network with the classification setup (by considering each entity as one class). The experimental results show that without extra effort in fine-tuning the model structure, we recognize 44.2% of the images in the measurement set with the precision 95% (hard case, details provided in section 4). We provide the details of our experiment setup and experimental results to serve as a very promising baseline in section 4.\n\nContribution Summary Our contribution in this paper is summarized as follows.\n\n-We design a benchmark task: to recognize one million celebrities from their face images, and link to their corresponding entity keys in freebase [5]. -We provide the following datasets, 2 -One million celebrities selected from freebase with corresponding entity keys , and a snapshot for freebase data dumps; -Manually labeled measurement set with carefully designed evaluation protocol; -A large scale training dataset, with face region cropped and aligned (to the best of our knowledge, the largest publicly available one). -We provide promising baseline performance with our training data to inspire more research effort on this task.\n\nOur benchmark task could lead to a very large scale classification problem in computer vision with meaningful real applications. This benefits people in experimenting different recognition models (especially fine-grained neural network) with the given training/testing data. Moreover, we encourage people to bring in more outside data and evaluate experimental results in a separate track.\n\n\nRelated works\n\nTypically, there are two types of tasks for face recognition. One is very wellstudied, called face verification, which is to determine whether two given face images belong to the same person. Face verification has been heavily investigated. One of the most widely used measurement sets for verification is Labeled Faces in the Wild (LFW) in [7,8], which provides 3000 matched face image pairs and 3000 mismatched face image pairs, and allows researchers to report verification accuracy with different settings. The best performance on LFW datasets has been frequently updated in the past several years. Especially, with the \"unrestricted, labeled outside data\" setting, multiple research groups have claimed higher accuracy than human performance for verification task on LFW [4,9].\n\nRecently, the interest in the other type of face recognition task, face identification, has greatly increased [9,10,11,3]. For typical face identification problems, two sets of face images are given, called gallery set and query set. Then the task is, for a given face image in the query set, to find the most similar faces in the gallery image set. When the gallery image set only has a very limited number (say, less than five) of face images for each individual, the most effective solution is still to learn a generic feature which can tell whether or not two face images are the same person, which is essentially still the problem of face verification. Currently, the MegaFace in [11] might be one of the most difficult face identification benchmarks. The difficulty of MegaFace mainly comes from the up-to one million distractors blended in the gallery image set. Note that the query set in MegaFace are selected from images from FaceScrub [12] and FG-NET [13], which contains 530 and 82 persons respectively.\n\nSeveral datasets have been published to facilitate the training for the face verification and identification tasks. Examples include LFW [7,8], Youtube Face Database (YFD) [14], CelebFaces+ [15], and CASIA-WebFace [16]. In LFW, 13000 images of faces were collected from the web, and then carefully labeled with celebrities' names. The YFD contains 3425 videos of 1595 different people. The CelebFace+ dataset contains 202, 599 face images of 10, 177 celebrities. People in CelebFaces+ and LFW are claimed to be mutually exclusive. The CASIA-WebFace [16] is currently the largest dataset which is publicly available, with about 10K celebrities, and 500K images. A quick summary is listed in Table 1. Available people images IJB-A [17] public 500 5712 LFW [7,8] public 5K 13K YFD [14] public 1595 3425 videos CelebFaces [15] public 10K 202K CASIA-WebFace [16]  As shown in Table 1, our training dataset is considerably larger than the publicly available datasets. Another uniqueness of our training dataset is that our dataset focuses on facilitating our celebrity recognition task, so our dataset needs to cover as many popular celebrities as possible, and have to solve the data disambiguation problem to collect right images for each celebrity. On the other hand, the existing datasets are mainly used to train a generalizable face feature, and celebrity coverage is not a major concern for these datasets. Therefore, for the typical existing dataset, if a name string corresponds to multiple celebrities (e.g., Mike Smith) and would lead to ambiguous image search result, these celebrities are usually removed from the datasets to help the precision of the collected training data [18].\n\n\nBenchmark construction\n\nOur benchmark task is to recognize one million celebrities from their face images, and link to their corresponding entity keys in the knowledge base. Here we describe how we construct this task in details.\n\n\nOne million celebrity list\n\nWe select one million celebrities to recognize from a knowledge graph called freebase [5], where each entity is identified by a unique key (called machine identifier, MID in freebase) and associated with rich properties. We require that the entities we select are human beings in the real world and have/had public attentions.\n\nThe first step is to select a subset of entities (from freebase [5]) which correspond to real people using the criteria in [1]. In freebase, there are more than 50 million topics capsulated in about 2 billion triplets. Note that we don't include any person if his/her facial appearance is unknown or not clearly defined.\n\nThe second step is to rank all the entities in the above subset according to the frequency of their occurrence on the web [1]. We select the top one million entities to form our celebrity list and provide their entity keys (MID) in freebase. We concern the public attention (popularity on the web) for two reasons. First, we want to align our benchmark task with the interest of real applications. For applications like image search, image annotations and deep understanding, and image caption generation, the recognition of popular celebrities would be more attractive to most of the users than ordinary people. Second, we include popular celebrities so that we have better chance to obtain multiple authority images for each of them to enable our training, testing, and re-distributing under certain licenses.\n\nWe present the distribution of the one million celebrities in different aspects including profession, nationality, age, and gender. In our one million celebrity list, we include persons with more than 2000 different professions ( Fig. 2 (a)), and come from more than 200 distinct countries/regions ( Fig. 2 (b)), which introduces a great diversity to our data. We cover all the major races in the world (Caucasian, Mongoloid, and Negroid). Moreover, as shown in Fig. 2 (c), we cover a large range of ages in our list. Though we do not manually select celebrities to make the profession (or gender, nationality, age) distribution uniform, the diversity (gender, age, profession, race, nationality) of our celebrity list is guaranteed by the large scale of our dataset. This is different from [17], in which there are about 500 subjects so the manual balancing over gender distribution is inevitable.  demonstrates that we don't include celebrities who were born before 1846 (long time before the first rollfilm specialized camera \"Kodak\" was invented [19]) and covers celebrities of a large variance of age. In (d), we notice that we have more females than males in our onemillion celebrity list. This might be correlated with the profession distribution in our list.\n\nNote that our property statistics are limited to the availability of freebase information. Some celebrities in our one million list do not have complete properties. If a certain celebrity does not have property A available in freebase, we do not include this celebrity for the statistic calculation of the property A.\n\n\nCelebrity selection for measurement\n\nIn order to evaluate the recognition performance on the one million celebrities obtained in the last subsection, we build up a measurement set which includes a set of carefully labeled images blended with another set of randomly selected face images as distractors. The measurement set construction is described in details in the following subsections, while the evaluation protocol is described in Section 4.\n\nFor the labeled images, we sample a subset of celebrities 3 from the onemillion celebrity list due to limited labeling resource. The sampling weight is designed in a way that, our measurement set mainly focuses on top celebrities (rank among the top in the occurrence frequency list) to represent the interest of real applications and users, yet maintain a certain amount of tail celebrities (celebrities not mentioned frequently on the web, e.g., from 1 to 10 times in total) to guarantee the measurement coverage over the one-million list.\n\nMore specifically, let f i denote the number of documents mentioned the i th celebrity on the web. Following the method in [1], we set the probability for the i th celebrity to get selected to be proportional to f i , defined as,\nf i = f 1 \u221a 5 i ,(1)\nwhere the exponent 1/ \u221a 5 is obtained empirically to include more celebrities with small f .\n\nThough it seems to be a natural solution, we do not set the sampling weights to be proportional to f i , since this option will make our measurement set barely contain any celebrities from the bottom 90% in our one-million list (ordered by f i ). The reason is that the distribution of f is very long-tailed. More than 90% of the celebrities have f smaller than 30, while the top celebrities have f larger than one million. We need to include sufficient number of tail celebrities to encourage researchers to work on the hard cases to improve the performance from the perspective of recognition coverage. This is the reason that we applied the adjustment in (1).\n\nWith the sampling weight f in (1) applied, our measurement set still mainly focuses on the most popular celebrities, while about 25% of the celebrities in our measurement set come from the bottom 90% in our one-million celebrity list (ordered by f ). If we do not apply the adjustment in (1), but just use f as the sampling weight, less than 10% of the celebrities in the measurement set come from the bottom 90% in our one-million celebrity list.\n\nSince the list of the celebrities in our measurement set is not exposed 4 , and our measurement set contains 25% of the celebrities in our measurement set come from the bottom 90%, researchers need to include as many celebrities as possible (not only the popular ones) from our one-million list to improve the performance of coverage. This pushes the scale of our task to be very large.\n\n\nLabeling for measurement\n\nAfter we have the set of celebrities for measurement, we provide two images for each of the celebrity. The correctness of our image labeling is ensured by deep research on the web content, multiple iterations of carefully review, and very rigorous consensus verification. Details are listed as follows.\n\nScraping Scraping provides image candidates for each of the celebrities selected for the measurement set. Though in the end we provide only two images per celebrity for evaluation, we scraped about 30 images per celebrities. During the scraping procedure, we applied different search queries, including the celebrity's name, name plus profession, and names in other languages (if available). The advantages of introducing multiple variations of the query used for each celebrity is that with multiple queries, we have better chance to capture the images which are truly about the given celebrity. Moreover, the variation of the query and scraping multiple images also brings in the diversity to the images for the given celebrity. Especially for the famous celebrities, the top one image returned by search engine is typically his/her representative image (frontal facial image with high quality), which is relatively easier to recognize, compared with the other images returned by the search engine. We increase the scraping depth so that we have more diverse images to be recognized for each of the celebrity. Fig. 3. Labeling GUI for \"Chuck Palhniuk\". (partial view) As shown in the figure, in the upper right corner, a representative image and a short description is provided. For a given image candidate, judge can label as \"not for this celebrity\" (red), \"yes for this celebrity\" (green), or \"broken image\" (dark gray).\n\nLabel Labeling picks up the images which are truly about the given celebrity. As shown in Fig.3, for each given celebrity, we (all the authors) manually label all the scraped image candidates to be truly about this celebrity or not. Extreme cautious was applied. We have access to the page which contains the scraped image to be labeled. Whenever needed, the judge (the authors) is asked to visit the original page with the scraped image and read the page content to guide his/her labeling. The rich information on the original page benefits the quality of the labeling, especially for a lot of the hard cases. Each of the image-celebrity entity pair was judged by at least two persons. Whenever there is a conflict, the two judges review together and provide the final decision based on verbal discussion. In total, we have about 30K images labeled, spent hundreds of hours.\n\nIn our measurement set, we select two images for each of the celebrity to keep the evaluation cost low. We have two subset (each of them have the same celebrity list), described as follows.\n\n\n-Random set\n\nThe image in this subset is randomly selected from the labeled images. One image per celebrity. This set reveals how many celebrities are truly covered by the models to be tested.\n\n\n-Hard set\n\nThe image in this subset is the one (from the labeled images) which is the most different from any images in the training dataset. One image per celebrity. This set is to evaluate the generalization ability of the model.\n\nThen, we blend the labeled images with images from other celebrities or ordinary people. The evaluation protocol is introduced in details in the next section.\n\n\nCelebrity recognition\n\nIn this section, we set up the evaluation protocol for our benchmark task. Moreover, in order to facilitate the researchers to work on this problem, we provide a training dataset which is encouraged (optional ) to use. We also present the baseline performance obtained by using our provided training data. We also encourage researchers to train with outside data and evaluate in a separate track.\n\n\nEvaluation Protocol\n\nWe evaluate the performance of our proposed recognition task in terms of precision and coverage (defined in the following subsection) using the settings described as follows.\n\nSetup We setup our evaluation protocol as follows. For a model to be tested, we collect the model prediction for both the labeled image and distractors in the measurement set. Note that we don't expose which images in the measurement are labeled ones or which are distractors. This setup avoids human labeling to the measurement set, and encourages researchers to build a recognizer which could robustly distinguish one million (as many as possible) people faces, rather than focusing merely on a small group of people. Moreover, during the training procedure, if the researcher leverages outside data for training, we do not require participants to exclude celebrities in our measurement from the training data set. Our measurement still evaluate the generalization ability of our recognition model, due to the following reasons. There are one million celebrities to be recognized in our task, and there are millions of images for some popular celebrities on the web. It is practically impossible to include all the images for every celebrity in the list. On the other hand, according to section 4.2, the images in our measurement set is typically not the representative images for the given celebrity (e.g., the top one searching result). Therefore the chance to include the measurement images in the training set is relatively low, as long as the celebrity list in the measurement set is hidden. This is different from most of the existing face recognition benchmark tasks, in which the measurement set is published and targeted on a small group of people. For these traditional benchmark tasks, the evaluation generalization ability relies on manually excluding the images (from the training set) of all the persons in the measurement set (This is mainly based on the integrity of the participants).\n\nEvaluation metric In the measurement set, we have n images, denoted by {x i } n i=1 . The first m images {x i |i = 1, 2, 3, ..., m} are the labeled images for our selected celebrities, while the rest {x i |i = m + 1, ..., n} are distractors. Note that we hide the order of the images in the measurement set.\n\nFor the i th image, let g(x i ) denote the ground truth label (entity key obtained by labeling). For any model to be tested, we assume the model to output {\u011d(x i ), c(x i )} as the predicted entity key of the i th image, and its corresponding prediction confidence. We allow the model to perform rejection. That is, if c(x i ) < t, where t is a preset threshold, the recognition result for image x i will be ignored. We define the precision with the threshold t as,\nP (t) = |{x i |\u011d(x i ) = g(x i ) \u2227 c(x i ) \u2265 t, i = 1, 2, ..., m}| |{x i |c(x i ) \u2265 t, i = 1, 2, ..., m}| ,(2)\nwhere the nominator is the number of the images of which the prediction is correct (and confidence score is larger than the threshold). The denominator is the number of images (within the set {x i } m i=1 ) which the model does have prediction (not reject to recognize).\n\nThe coverage in our protocol is defined as\nC(t) = |{x i |c(x i ) \u2265 t, i = 1, 2, ..., m}| m(3)\nFor each given t, a pair of precision P (t) and coverage C(t) can be obtained for the model to be tested. The precision P (t) is a function of C(t). Our major evaluation metric is the maximum of the coverage satisfying the condition of precision, P (t) \u2265 P min . The value of P min is 0.95 in our current setup. Other metrics and analysis/discussions are also welcomed to report. The reason that we prefer a fixed high precision and measure the corresponding coverage is because in many real applications high precision is usually more desirable and of greater value.\n\n\nTraining dataset\n\nIn order to facilitate the above face recognition task we provide a large training dataset. This training dataset is prepared by the following two steps. First, we select the top 100K entities from our one-million celebrity list in terms of their web appearance frequency. Then, we retrieve approximately 100 images per celebrity from popular search engines.\n\nWe do not provide training images for the entire one-million celebrity list for the following considerations. First, limited by time and resource, we can only manage to prepare a dataset of top 100K celebrities as a v1 dataset to facilitate the participants to quickly get started. We will continuously extend the dataset to cover more celebrities in the future. Moreover, as shown in the experimental results in the next subsection, this dataset is already very promising to use. Our training dataset covers about 75% of celebrities in our measurement set, which implies that the upper bound of recognition recall rate based on the provided  training data cannot exceed 75%. Therefore, we also encourage the participants, especially who are passionate to break this 75% upper bound to treat the dataset development as one of the key problems in this challenge, and bring in outside data to get higher recognition recall rate and compare experimental results in a separate track. Especially, we encourage people label their data with entity keys in the freebase snapshot we provided and publish, so that different dataset could be easily united to facilitate collaboration.\n\nOn example in our training dataset is shown in Figure 4. As shown in the figures, same celebrity may look very differently in different images. In Figure  4, we see images for Steve Jobs (m.06y3r) when he was about 20/30 years old, as well as images when he was about 50 years old. The image at row 2, column 8 (in green rectangle) in Figure 4 is claimed to be Steve Jobs when he was in high school. Notice that the image at row 2, column 3 in Figure 4, marked with red rectangle is considered as a noise sample in our dataset, since this image was synthesized by combining one image of Steve Jobs and one image of Ashton Kutcher, who is the actor in the movie \"Jobs\".\n\nAs we have mentioned, we do not manually remove the noise in this training data set. This is partially because to prepare training data of this size is beyond the scale of manually labeling. In addition, we have observed that the state-ofthe-art deep neural network learning algorithm can tolerate a certain level of noise in the training data. Though for a small percentage of celebrities their image search result is far from perfect, more data especially more individuals covered by the training data could still be of great value to the face recognition research, which is also reported in [18]. Moreover, we believe that data cleaning, noisy label removal, and learning with noisy data are all good and real problems that are worth of dedicated research efforts. Therefore, we leave this problem open and do not limit the use of outside training data.\n\n\nBaseline\n\nThere are typically two categories of methods to recognize people from face images. One is template-based. For methods in this category, a gallery set which contains multiple images for the targeted group of people is pre-built. Then, for the given image in the query set, the most similar image(s) in the gallery set (according to some certain metrics or in pre-learned feature space) is retrieved, and the annotation of this/these similar images are used to estimate the identity of the given query image. When the gallery is not very large, this category of methods is very convenient for adding/removing entities in the gallery since the face feature representation could be learned in advance. However, when the gallery is large, a complicated index needs to be built to shorten the retrieval time. In this case, the flexibility of adding/removing entities for the methods in this category vanishes. Moreover, the accuracy of the template-based methods highly relies on the annotation accuracy in the gallery set. When there are many people in the targeted group, accurate annotation is beyond human effort and could be a very challenging problem itself.\n\nWe choose the second category, which is a model-based method. More specifically, we model our problem as a classification problem and consider each celebrity as a class.\n\nIn our experiment, we trained a deep neural network following the network structure in [20]. Training a deep neural network for 100K celebrities is not a trivial task. If we directly train the model from scratch, it is hard to see the model starts to converge even after a long run due to the large number of categories. To address this problem, we started from training a small model for 500 celebrities, which have the largest numbers of images for each celebrity. In addition, we used the pre-trained model from [20] to initialize this small model. This step is optional, but we observed that it helps the training process converge faster. After 50, 000 iterations, we stopped to train this model, and used it as a pre-trained model to initialize the full model of 100K celebrities. After 250, 000 iterations, with learning rate decreased from the initial value 0.01 to 0.001 and 0.0001 after 100, 000 and 200, 000 iterations, the training loss decrease becomes very slow and indiscernible. Then we stopped the training and used the last model snapshot to evaluate the performance of celebrity recognition on our measurement set. The experimental results (on the published 500 celebrities) are shown in Fig. 5 and Table 2.  The promising results can be attributed to the deep neural network capability and the high quality of image search results thanks for years of improvement in image search engines. However, the curves also shows that the task is indeed very challenge. To achieve both high precision and high recall, a great amount of research efforts need to be spent on data collection, cleaning, learning algorithm, and model generalization, which are valuable problems to computer vision researchers.\n\n\nDiscussion and Future work\n\nIn this paper, we have defined a benchmark task which is to recognize one million celebrities in the world from their face images, and link the face to a corresponding entity key in a knowledge base. Our face recognition has the property of disambiguation, and close to the human behavior in recognizing images. We also provide concrete measurement set for people to evaluate the model performance easily, and provide, to the best of our knowledge, the largest training dataset to facilitate research in the area.\n\nBeyond face recognition, our datasets could inspire other research topics. For example, people could adopt one of the cutting-edge unsupervised/semisupervised clustering algorithms [21] [22] [23] [24] on our training dataset, and/or develop new algorithms which can accurately locate and remove outliers in a large, real dataset. Another interesting topic is the to build estimators to predict a person's properties from his/her face images. For example, the images in our training dataset are associated with entity keys in knowledge base, of which the gender information (or other properties) could be easily retrieved. People could train a robust gender classifier for the face images in the wild based on this large scale training data. We look forward to exciting research inspired by our training dataset and benchmark task.\n\n\nInput: Face image from the web Task: To Recognize the face image and Link to the corresponding entity in knowledge base Knowledge base (partial view) \u2026 Fig. 1.\n\nFig. 2 .\n2Distribution of the properties of the celebrities in our one-million list in different aspects. The large scale of our dataset naturally introduces great diversity. As shown in (a) and (b), we include persons with more than 2000 different professions, and come from more than 200 distinct countries/regions. The figure (c)\n\nFig. 4 .\n4Examples (subset) of the training images for the celebrity with entity key m.06y3r (Steve Jobs). The image marked with a green rectangle is claimed to be Steve Jobs when he was in high school. The image marked with a red rectangle is considered as a noise sample in our dataset, since it is synthesized by combining one image of Steve Jobs and one image of Ashton Kutcher, who is the actor in the movie \"Jobs\".\n\nFig. 5 .\n5Precision-coverage curve with our baseline model\n\nTable 1 .\n1Face recognition datasetsDataset \n\n\nTable 2 .\n2Experimental results on the 500 published celebrities Develop-Hard: 100K model Develop-Random: 100K modelCoverage@Precision 99% Coverage@Precision 95% \nHard Set \n0.052 \n0.442 \nRandom Set \n0.606 \n0.728 \n\n0.0 \n0.2 \n0.4 \n0.6 \n0.8 \n1.0 \n\ncoverage \n\n0.0 \n\n0.2 \n\n0.4 \n\n0.6 \n\n0.8 \n\n1.0 \n\nprecision \n\n\nImage resource: http://www.hdwallpapers.in/anne hathaway 2-wallpapers.html, retrieved by image.bing.com.\nInstructions and download links: http://msceleb.org\nCurrently there are 1500. We will increase the number of celebrities in our measurement set in the future.\nWe publish the images for 500 celebrities, called development set, while hold the rest\n\nMS-Celeb-1M: Challenge of recognizing one million celebrities in the real world. Y Guo, L Zhang, Y Hu, X He, J Gao, IS&T International Symposium on Electronic Imaging. Guo, Y., Zhang, L., Hu, Y., He, X., Gao, J.: MS-Celeb-1M: Challenge of recognizing one million celebrities in the real world. In: IS&T International Symposium on Electronic Imaging. (2016)\n\nDeepface: Closing the gap to human-level performance in face verification. Y Taigman, M Yang, M Ranzato, L Wolf, Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPRTaigman, Y., Yang, M., Ranzato, M., Wolf, L.: Deepface: Closing the gap to human-level performance in face verification. In: Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR). (June 2014)\n\nWeb-scale training for face identification. Y Taigman, M Yang, M Ranzato, L Wolf, Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR). of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR)Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Web-scale training for face identi- fication. In: Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR), IEEE (2015) 2746-2754\n\nFacenet: A unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR). of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR)Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face recognition and clustering. In: Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR). (June 2015)\n\nGoogle: Freebase data dumps. Google: Freebase data dumps. https://developers.google.com/freebase/data (2015)\n\nImageNet Large Scale Visual Recognition Challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, International Journal of Computer Vision (IJCV). 1153Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115(3) (2015) 211-252\n\nLabeled faces in the wild: A database for studying face recognition in unconstrained environments. G B Huang, M Ramesh, T Berg, E Learned-Miller, 07-49AmherstUniversity of MassachusettsTechnical ReportHuang, G.B., Ramesh, M., Berg, T., Learned-Miller, E.: Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst (October 2007)\n\nLabeled faces in the wild: Updates and new reporting procedures. G B Huang, E Learned-Miller, UM-CS-2014-003AmherstUniversity of MassachusettsTechnical ReportHuang, G.B., Learned-Miller, E.: Labeled faces in the wild: Updates and new reporting procedures. Technical Report UM-CS-2014-003, University of Mas- sachusetts, Amherst (May 2014)\n\nY Sun, X Wang, X Tang, arXiv:1502.00873DeepID3: Face recognition with very deep neural networks. arXiv preprintSun, Y., Wang, X., Tang, X.: DeepID3: Face recognition with very deep neural networks. arXiv preprint arXiv:1502.00873 (2014)\n\nLearning compact face representation: Packing a face into an int32. H Fan, M Yang, Z Cao, Y Jiang, Q Yin, Proc. of ACM Int'l Conf. on Multimedia. of ACM Int'l Conf. on MultimediaACMFan, H., Yang, M., Cao, Z., Jiang, Y., Yin, Q.: Learning compact face represen- tation: Packing a face into an int32. In: Proc. of ACM Int'l Conf. on Multimedia, ACM (2014) 933-936\n\nThe MegaFace benchmark: 1 million faces for recognition at scale. I Kemelmacher-Shlizerman, S Seitz, D Miller, E Brossard, ArXiv e-printsKemelmacher-Shlizerman, I., Seitz, S., Miller, D., Brossard, E.: The MegaFace benchmark: 1 million faces for recognition at scale. ArXiv e-prints (2015)\n\nA data-driven approach to cleaning large face datasets. H W Ng, S Winkler, Proc. of IEEE Int'l Conf. on Image Proc. (ICIP). of IEEE Int'l Conf. on Image . (ICIP)Ng, H.W., Winkler, S.: A data-driven approach to cleaning large face datasets. In: Proc. of IEEE Int'l Conf. on Image Proc. (ICIP). (Oct 2014)\n\nAn overview of research activities in facial age estimation using the FG-NET aging database. G Panis, A Lanitis, Proc. of the European Conf. on Computer Vision (ECCV) Workshops. of the European Conf. on Computer Vision (ECCV) WorkshopsPanis, G., Lanitis, A.: An overview of research activities in facial age estimation using the FG-NET aging database. In: Proc. of the European Conf. on Computer Vision (ECCV) Workshops. (2014)\n\nFace recognition in unconstrained videos with matched background similarity. L Wolf, T Hassner, I Maoz, Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR). of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR)Wolf, L., Hassner, T., Maoz, I.: Face recognition in unconstrained videos with matched background similarity. In: Proc. of IEEE Computer Soc. Conf. on Com- puter Vision and Pattern Recognition (CVPR). (2011)\n\nDeep learning face representation from predicting 10,000 classes. Y Sun, X Wang, X Tang, Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPRSun, Y., Wang, X., Tang, X.: Deep learning face representation from predicting 10,000 classes. In: Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR). (June 2014)\n\nD Yi, Z Lei, S Liao, S Z Li, arXiv:1411.7923Learning face representation from scratch. arXiv preprintYi, D., Lei, Z., Liao, S., Li, S.Z.: Learning face representation from scratch. arXiv preprint arXiv:1411.7923 (2014)\n\nPushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a. B F Klare, B Klein, E Taborsky, A Blanton, J Cheney, K Allen, P Grother, A Mah, A K Jain, Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR). of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR)Klare, B.F., Klein, B., Taborsky, E., Blanton, A., Cheney, J., Allen, K., Grother, P., Mah, A., Jain, A.K.: Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a. In: Proc. of IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition (CVPR). (June 2015)\n\nDeep face recognition. O M Parkhi, A Vedaldi, A Zisserman, Proceedings of the British Machine Vision Conference (BMVC. the British Machine Vision Conference (BMVCParkhi, O.M., Vedaldi, A., Zisserman, A.: Deep face recognition. In: Proceedings of the British Machine Vision Conference (BMVC). (2015)\n\nCamera. US Patent 388850 A (1888). G Eastman, Eastman, G.: Camera. US Patent 388850 A (1888)\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in Neural Information Processing Systems (NIPS). MIT PressKrizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems (NIPS), MIT Press (2012) 1097-1105\n\nOn spectral clustering: Analysis and an algorithm. A Y Ng, M I Jordan, Y Weiss, Advances in Neural Information Processing Systems (NIPS). MIT PressNg, A.Y., Jordan, M.I., Weiss, Y.: On spectral clustering: Analysis and an algo- rithm. In: Advances in Neural Information Processing Systems (NIPS), MIT Press (2001) 849-856\n\nSemi-supervised learning on riemannian manifolds. M Belkin, P Niyogi, Journal of Machine Learning. 561-3Belkin, M., Niyogi, P.: Semi-supervised learning on riemannian manifolds. Journal of Machine Learning 56(1-3) (June 2004) 209-239\n\nSemi-supervised learning using gaussian fields and harmonic functions. X Zhu, Z Ghahramani, J Lafferty, Proc. of Int'l Conf. on Machine Learning. of Int'l Conf. on Machine LearningZhu, X., Ghahramani, Z., Lafferty, J.: Semi-supervised learning using gaussian fields and harmonic functions. In: Proc. of Int'l Conf. on Machine Learning. (2003) 912-919\n\nLearning with local and global consistency. D Zhou, O Bousquet, T N Lal, J Weston, B Schlkopf, Advances in Neural Information Processing Systems (NIPS). MIT PressZhou, D., Bousquet, O., Lal, T.N., Weston, J., Schlkopf, B.: Learning with local and global consistency. In: Advances in Neural Information Processing Systems (NIPS), MIT Press (2004) 321-328\n", "annotations": {"author": "[{\"end\":132,\"start\":73},{\"end\":187,\"start\":133},{\"end\":243,\"start\":188},{\"end\":277,\"start\":244},{\"end\":332,\"start\":278}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":81},{\"end\":142,\"start\":137},{\"end\":197,\"start\":195},{\"end\":255,\"start\":253},{\"end\":290,\"start\":287}]", "author_first_name": "[{\"end\":80,\"start\":73},{\"end\":136,\"start\":133},{\"end\":194,\"start\":188},{\"end\":252,\"start\":244},{\"end\":286,\"start\":278}]", "author_affiliation": "[{\"end\":131,\"start\":112},{\"end\":186,\"start\":167},{\"end\":242,\"start\":223},{\"end\":276,\"start\":257},{\"end\":331,\"start\":312}]", "title": "[{\"end\":70,\"start\":1},{\"end\":402,\"start\":333}]", "venue": null, "abstract": "[{\"end\":1534,\"start\":488}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1911,\"start\":1908},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2512,\"start\":2509},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2514,\"start\":2512},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2529,\"start\":2526},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3356,\"start\":3353},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3867,\"start\":3866},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6003,\"start\":6000},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8971,\"start\":8968},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10213,\"start\":10210},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10215,\"start\":10213},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10648,\"start\":10645},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10650,\"start\":10648},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10766,\"start\":10763},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10769,\"start\":10766},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10772,\"start\":10769},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10774,\"start\":10772},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11342,\"start\":11338},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11603,\"start\":11599},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11619,\"start\":11615},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11810,\"start\":11807},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11812,\"start\":11810},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11846,\"start\":11842},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11864,\"start\":11860},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11888,\"start\":11884},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12223,\"start\":12219},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12403,\"start\":12399},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12427,\"start\":12424},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12429,\"start\":12427},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12452,\"start\":12448},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12492,\"start\":12488},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12527,\"start\":12523},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13357,\"start\":13353},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13710,\"start\":13707},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14016,\"start\":14013},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14075,\"start\":14072},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14396,\"start\":14393},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15879,\"start\":15875},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16138,\"start\":16134},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17789,\"start\":17786},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18669,\"start\":18666},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29992,\"start\":29988},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31686,\"start\":31682},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32114,\"start\":32110},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":34039,\"start\":34035},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34049,\"start\":34045}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34846,\"start\":34685},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35180,\"start\":34847},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35602,\"start\":35181},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35662,\"start\":35603},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35709,\"start\":35663},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36015,\"start\":35710}]", "paragraph": "[{\"end\":2900,\"start\":1550},{\"end\":5255,\"start\":2902},{\"end\":6004,\"start\":5257},{\"end\":6503,\"start\":6006},{\"end\":7514,\"start\":6505},{\"end\":8256,\"start\":7516},{\"end\":8741,\"start\":8258},{\"end\":8820,\"start\":8743},{\"end\":9460,\"start\":8822},{\"end\":9851,\"start\":9462},{\"end\":10651,\"start\":9869},{\"end\":11668,\"start\":10653},{\"end\":13358,\"start\":11670},{\"end\":13590,\"start\":13385},{\"end\":13947,\"start\":13621},{\"end\":14269,\"start\":13949},{\"end\":15082,\"start\":14271},{\"end\":16350,\"start\":15084},{\"end\":16669,\"start\":16352},{\"end\":17118,\"start\":16709},{\"end\":17661,\"start\":17120},{\"end\":17892,\"start\":17663},{\"end\":18006,\"start\":17914},{\"end\":18670,\"start\":18008},{\"end\":19119,\"start\":18672},{\"end\":19507,\"start\":19121},{\"end\":19838,\"start\":19536},{\"end\":21265,\"start\":19840},{\"end\":22142,\"start\":21267},{\"end\":22333,\"start\":22144},{\"end\":22528,\"start\":22349},{\"end\":22762,\"start\":22542},{\"end\":22922,\"start\":22764},{\"end\":23344,\"start\":22948},{\"end\":23542,\"start\":23368},{\"end\":25347,\"start\":23544},{\"end\":25656,\"start\":25349},{\"end\":26123,\"start\":25658},{\"end\":26505,\"start\":26235},{\"end\":26549,\"start\":26507},{\"end\":27168,\"start\":26601},{\"end\":27547,\"start\":27189},{\"end\":28722,\"start\":27549},{\"end\":29392,\"start\":28724},{\"end\":30250,\"start\":29394},{\"end\":31422,\"start\":30263},{\"end\":31593,\"start\":31424},{\"end\":33308,\"start\":31595},{\"end\":33852,\"start\":33339},{\"end\":34684,\"start\":33854}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17913,\"start\":17893},{\"attributes\":{\"id\":\"formula_1\"},\"end\":26234,\"start\":26124},{\"attributes\":{\"id\":\"formula_2\"},\"end\":26600,\"start\":26550}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":2555,\"start\":2548},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7947,\"start\":7940},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12367,\"start\":12360},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12548,\"start\":12541},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":32819,\"start\":32812}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1548,\"start\":1536},{\"attributes\":{\"n\":\"2\"},\"end\":9867,\"start\":9854},{\"attributes\":{\"n\":\"3\"},\"end\":13383,\"start\":13361},{\"attributes\":{\"n\":\"3.1\"},\"end\":13619,\"start\":13593},{\"attributes\":{\"n\":\"3.2\"},\"end\":16707,\"start\":16672},{\"attributes\":{\"n\":\"3.3\"},\"end\":19534,\"start\":19510},{\"end\":22347,\"start\":22336},{\"end\":22540,\"start\":22531},{\"attributes\":{\"n\":\"4\"},\"end\":22946,\"start\":22925},{\"attributes\":{\"n\":\"4.1\"},\"end\":23366,\"start\":23347},{\"attributes\":{\"n\":\"4.2\"},\"end\":27187,\"start\":27171},{\"attributes\":{\"n\":\"4.3\"},\"end\":30261,\"start\":30253},{\"attributes\":{\"n\":\"5\"},\"end\":33337,\"start\":33311},{\"end\":34856,\"start\":34848},{\"end\":35190,\"start\":35182},{\"end\":35612,\"start\":35604},{\"end\":35673,\"start\":35664},{\"end\":35720,\"start\":35711}]", "table": "[{\"end\":35709,\"start\":35700},{\"end\":36015,\"start\":35827}]", "figure_caption": "[{\"end\":34846,\"start\":34687},{\"end\":35180,\"start\":34858},{\"end\":35602,\"start\":35192},{\"end\":35662,\"start\":35614},{\"end\":35700,\"start\":35675},{\"end\":35827,\"start\":35722}]", "figure_ref": "[{\"end\":3409,\"start\":3403},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15324,\"start\":15314},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15394,\"start\":15384},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15556,\"start\":15546},{\"end\":20958,\"start\":20952},{\"end\":21362,\"start\":21357},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28779,\"start\":28771},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28880,\"start\":28871},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29067,\"start\":29059},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29176,\"start\":29168},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32807,\"start\":32801}]", "bib_author_first_name": "[{\"end\":36450,\"start\":36449},{\"end\":36457,\"start\":36456},{\"end\":36466,\"start\":36465},{\"end\":36472,\"start\":36471},{\"end\":36478,\"start\":36477},{\"end\":36802,\"start\":36801},{\"end\":36813,\"start\":36812},{\"end\":36821,\"start\":36820},{\"end\":36832,\"start\":36831},{\"end\":37267,\"start\":37266},{\"end\":37278,\"start\":37277},{\"end\":37286,\"start\":37285},{\"end\":37297,\"start\":37296},{\"end\":37737,\"start\":37736},{\"end\":37748,\"start\":37747},{\"end\":37764,\"start\":37763},{\"end\":38310,\"start\":38309},{\"end\":38325,\"start\":38324},{\"end\":38333,\"start\":38332},{\"end\":38339,\"start\":38338},{\"end\":38349,\"start\":38348},{\"end\":38361,\"start\":38360},{\"end\":38367,\"start\":38366},{\"end\":38376,\"start\":38375},{\"end\":38388,\"start\":38387},{\"end\":38398,\"start\":38397},{\"end\":38411,\"start\":38410},{\"end\":38413,\"start\":38412},{\"end\":38421,\"start\":38420},{\"end\":38852,\"start\":38851},{\"end\":38854,\"start\":38853},{\"end\":38863,\"start\":38862},{\"end\":38873,\"start\":38872},{\"end\":38881,\"start\":38880},{\"end\":39250,\"start\":39249},{\"end\":39252,\"start\":39251},{\"end\":39261,\"start\":39260},{\"end\":39525,\"start\":39524},{\"end\":39532,\"start\":39531},{\"end\":39540,\"start\":39539},{\"end\":39831,\"start\":39830},{\"end\":39838,\"start\":39837},{\"end\":39846,\"start\":39845},{\"end\":39853,\"start\":39852},{\"end\":39862,\"start\":39861},{\"end\":40192,\"start\":40191},{\"end\":40218,\"start\":40217},{\"end\":40227,\"start\":40226},{\"end\":40237,\"start\":40236},{\"end\":40473,\"start\":40472},{\"end\":40475,\"start\":40474},{\"end\":40481,\"start\":40480},{\"end\":40815,\"start\":40814},{\"end\":40824,\"start\":40823},{\"end\":41228,\"start\":41227},{\"end\":41236,\"start\":41235},{\"end\":41247,\"start\":41246},{\"end\":41692,\"start\":41691},{\"end\":41699,\"start\":41698},{\"end\":41707,\"start\":41706},{\"end\":42072,\"start\":42071},{\"end\":42078,\"start\":42077},{\"end\":42085,\"start\":42084},{\"end\":42093,\"start\":42092},{\"end\":42095,\"start\":42094},{\"end\":42388,\"start\":42387},{\"end\":42390,\"start\":42389},{\"end\":42399,\"start\":42398},{\"end\":42408,\"start\":42407},{\"end\":42420,\"start\":42419},{\"end\":42431,\"start\":42430},{\"end\":42441,\"start\":42440},{\"end\":42450,\"start\":42449},{\"end\":42461,\"start\":42460},{\"end\":42468,\"start\":42467},{\"end\":42470,\"start\":42469},{\"end\":42969,\"start\":42968},{\"end\":42971,\"start\":42970},{\"end\":42981,\"start\":42980},{\"end\":42992,\"start\":42991},{\"end\":43281,\"start\":43280},{\"end\":43405,\"start\":43404},{\"end\":43419,\"start\":43418},{\"end\":43432,\"start\":43431},{\"end\":43434,\"start\":43433},{\"end\":43762,\"start\":43761},{\"end\":43764,\"start\":43763},{\"end\":43770,\"start\":43769},{\"end\":43772,\"start\":43771},{\"end\":43782,\"start\":43781},{\"end\":44084,\"start\":44083},{\"end\":44094,\"start\":44093},{\"end\":44340,\"start\":44339},{\"end\":44347,\"start\":44346},{\"end\":44361,\"start\":44360},{\"end\":44665,\"start\":44664},{\"end\":44673,\"start\":44672},{\"end\":44685,\"start\":44684},{\"end\":44687,\"start\":44686},{\"end\":44694,\"start\":44693},{\"end\":44704,\"start\":44703}]", "bib_author_last_name": "[{\"end\":36454,\"start\":36451},{\"end\":36463,\"start\":36458},{\"end\":36469,\"start\":36467},{\"end\":36475,\"start\":36473},{\"end\":36482,\"start\":36479},{\"end\":36810,\"start\":36803},{\"end\":36818,\"start\":36814},{\"end\":36829,\"start\":36822},{\"end\":36837,\"start\":36833},{\"end\":37275,\"start\":37268},{\"end\":37283,\"start\":37279},{\"end\":37294,\"start\":37287},{\"end\":37302,\"start\":37298},{\"end\":37745,\"start\":37738},{\"end\":37761,\"start\":37749},{\"end\":37772,\"start\":37765},{\"end\":38322,\"start\":38311},{\"end\":38330,\"start\":38326},{\"end\":38336,\"start\":38334},{\"end\":38346,\"start\":38340},{\"end\":38358,\"start\":38350},{\"end\":38364,\"start\":38362},{\"end\":38373,\"start\":38368},{\"end\":38385,\"start\":38377},{\"end\":38395,\"start\":38389},{\"end\":38408,\"start\":38399},{\"end\":38418,\"start\":38414},{\"end\":38429,\"start\":38422},{\"end\":38860,\"start\":38855},{\"end\":38870,\"start\":38864},{\"end\":38878,\"start\":38874},{\"end\":38896,\"start\":38882},{\"end\":39258,\"start\":39253},{\"end\":39276,\"start\":39262},{\"end\":39529,\"start\":39526},{\"end\":39537,\"start\":39533},{\"end\":39545,\"start\":39541},{\"end\":39835,\"start\":39832},{\"end\":39843,\"start\":39839},{\"end\":39850,\"start\":39847},{\"end\":39859,\"start\":39854},{\"end\":39866,\"start\":39863},{\"end\":40215,\"start\":40193},{\"end\":40224,\"start\":40219},{\"end\":40234,\"start\":40228},{\"end\":40246,\"start\":40238},{\"end\":40478,\"start\":40476},{\"end\":40489,\"start\":40482},{\"end\":40821,\"start\":40816},{\"end\":40832,\"start\":40825},{\"end\":41233,\"start\":41229},{\"end\":41244,\"start\":41237},{\"end\":41252,\"start\":41248},{\"end\":41696,\"start\":41693},{\"end\":41704,\"start\":41700},{\"end\":41712,\"start\":41708},{\"end\":42075,\"start\":42073},{\"end\":42082,\"start\":42079},{\"end\":42090,\"start\":42086},{\"end\":42098,\"start\":42096},{\"end\":42396,\"start\":42391},{\"end\":42405,\"start\":42400},{\"end\":42417,\"start\":42409},{\"end\":42428,\"start\":42421},{\"end\":42438,\"start\":42432},{\"end\":42447,\"start\":42442},{\"end\":42458,\"start\":42451},{\"end\":42465,\"start\":42462},{\"end\":42475,\"start\":42471},{\"end\":42978,\"start\":42972},{\"end\":42989,\"start\":42982},{\"end\":43002,\"start\":42993},{\"end\":43289,\"start\":43282},{\"end\":43416,\"start\":43406},{\"end\":43429,\"start\":43420},{\"end\":43441,\"start\":43435},{\"end\":43767,\"start\":43765},{\"end\":43779,\"start\":43773},{\"end\":43788,\"start\":43783},{\"end\":44091,\"start\":44085},{\"end\":44101,\"start\":44095},{\"end\":44344,\"start\":44341},{\"end\":44358,\"start\":44348},{\"end\":44370,\"start\":44362},{\"end\":44670,\"start\":44666},{\"end\":44682,\"start\":44674},{\"end\":44691,\"start\":44688},{\"end\":44701,\"start\":44695},{\"end\":44713,\"start\":44705}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":64161158},\"end\":36724,\"start\":36368},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2814088},\"end\":37220,\"start\":36726},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9380519},\"end\":37668,\"start\":37222},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206592766},\"end\":38146,\"start\":37670},{\"attributes\":{\"id\":\"b4\"},\"end\":38256,\"start\":38148},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2930547},\"end\":38750,\"start\":38258},{\"attributes\":{\"doi\":\"07-49\",\"id\":\"b6\"},\"end\":39182,\"start\":38752},{\"attributes\":{\"doi\":\"UM-CS-2014-003\",\"id\":\"b7\"},\"end\":39522,\"start\":39184},{\"attributes\":{\"doi\":\"arXiv:1502.00873\",\"id\":\"b8\"},\"end\":39760,\"start\":39524},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1356132},\"end\":40123,\"start\":39762},{\"attributes\":{\"id\":\"b10\"},\"end\":40414,\"start\":40125},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14599182},\"end\":40719,\"start\":40416},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11896595},\"end\":41148,\"start\":40721},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9898157},\"end\":41623,\"start\":41150},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206592295},\"end\":42069,\"start\":41625},{\"attributes\":{\"doi\":\"arXiv:1411.7923\",\"id\":\"b15\"},\"end\":42289,\"start\":42071},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3176168},\"end\":42943,\"start\":42291},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4637184},\"end\":43243,\"start\":42945},{\"attributes\":{\"id\":\"b18\"},\"end\":43337,\"start\":43245},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":195908774},\"end\":43708,\"start\":43339},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":18764978},\"end\":44031,\"start\":43710},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":17133491},\"end\":44266,\"start\":44033},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1052837},\"end\":44618,\"start\":44268},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":508435},\"end\":44973,\"start\":44620}]", "bib_title": "[{\"end\":36447,\"start\":36368},{\"end\":36799,\"start\":36726},{\"end\":37264,\"start\":37222},{\"end\":37734,\"start\":37670},{\"end\":38307,\"start\":38258},{\"end\":39828,\"start\":39762},{\"end\":40470,\"start\":40416},{\"end\":40812,\"start\":40721},{\"end\":41225,\"start\":41150},{\"end\":41689,\"start\":41625},{\"end\":42385,\"start\":42291},{\"end\":42966,\"start\":42945},{\"end\":43402,\"start\":43339},{\"end\":43759,\"start\":43710},{\"end\":44081,\"start\":44033},{\"end\":44337,\"start\":44268},{\"end\":44662,\"start\":44620}]", "bib_author": "[{\"end\":36456,\"start\":36449},{\"end\":36465,\"start\":36456},{\"end\":36471,\"start\":36465},{\"end\":36477,\"start\":36471},{\"end\":36484,\"start\":36477},{\"end\":36812,\"start\":36801},{\"end\":36820,\"start\":36812},{\"end\":36831,\"start\":36820},{\"end\":36839,\"start\":36831},{\"end\":37277,\"start\":37266},{\"end\":37285,\"start\":37277},{\"end\":37296,\"start\":37285},{\"end\":37304,\"start\":37296},{\"end\":37747,\"start\":37736},{\"end\":37763,\"start\":37747},{\"end\":37774,\"start\":37763},{\"end\":38324,\"start\":38309},{\"end\":38332,\"start\":38324},{\"end\":38338,\"start\":38332},{\"end\":38348,\"start\":38338},{\"end\":38360,\"start\":38348},{\"end\":38366,\"start\":38360},{\"end\":38375,\"start\":38366},{\"end\":38387,\"start\":38375},{\"end\":38397,\"start\":38387},{\"end\":38410,\"start\":38397},{\"end\":38420,\"start\":38410},{\"end\":38431,\"start\":38420},{\"end\":38862,\"start\":38851},{\"end\":38872,\"start\":38862},{\"end\":38880,\"start\":38872},{\"end\":38898,\"start\":38880},{\"end\":39260,\"start\":39249},{\"end\":39278,\"start\":39260},{\"end\":39531,\"start\":39524},{\"end\":39539,\"start\":39531},{\"end\":39547,\"start\":39539},{\"end\":39837,\"start\":39830},{\"end\":39845,\"start\":39837},{\"end\":39852,\"start\":39845},{\"end\":39861,\"start\":39852},{\"end\":39868,\"start\":39861},{\"end\":40217,\"start\":40191},{\"end\":40226,\"start\":40217},{\"end\":40236,\"start\":40226},{\"end\":40248,\"start\":40236},{\"end\":40480,\"start\":40472},{\"end\":40491,\"start\":40480},{\"end\":40823,\"start\":40814},{\"end\":40834,\"start\":40823},{\"end\":41235,\"start\":41227},{\"end\":41246,\"start\":41235},{\"end\":41254,\"start\":41246},{\"end\":41698,\"start\":41691},{\"end\":41706,\"start\":41698},{\"end\":41714,\"start\":41706},{\"end\":42077,\"start\":42071},{\"end\":42084,\"start\":42077},{\"end\":42092,\"start\":42084},{\"end\":42100,\"start\":42092},{\"end\":42398,\"start\":42387},{\"end\":42407,\"start\":42398},{\"end\":42419,\"start\":42407},{\"end\":42430,\"start\":42419},{\"end\":42440,\"start\":42430},{\"end\":42449,\"start\":42440},{\"end\":42460,\"start\":42449},{\"end\":42467,\"start\":42460},{\"end\":42477,\"start\":42467},{\"end\":42980,\"start\":42968},{\"end\":42991,\"start\":42980},{\"end\":43004,\"start\":42991},{\"end\":43291,\"start\":43280},{\"end\":43418,\"start\":43404},{\"end\":43431,\"start\":43418},{\"end\":43443,\"start\":43431},{\"end\":43769,\"start\":43761},{\"end\":43781,\"start\":43769},{\"end\":43790,\"start\":43781},{\"end\":44093,\"start\":44083},{\"end\":44103,\"start\":44093},{\"end\":44346,\"start\":44339},{\"end\":44360,\"start\":44346},{\"end\":44372,\"start\":44360},{\"end\":44672,\"start\":44664},{\"end\":44684,\"start\":44672},{\"end\":44693,\"start\":44684},{\"end\":44703,\"start\":44693},{\"end\":44715,\"start\":44703}]", "bib_venue": "[{\"end\":36534,\"start\":36484},{\"end\":36921,\"start\":36839},{\"end\":37387,\"start\":37304},{\"end\":37857,\"start\":37774},{\"end\":38175,\"start\":38148},{\"end\":38478,\"start\":38431},{\"end\":38849,\"start\":38752},{\"end\":39247,\"start\":39184},{\"end\":39619,\"start\":39563},{\"end\":39906,\"start\":39868},{\"end\":40189,\"start\":40125},{\"end\":40538,\"start\":40491},{\"end\":40897,\"start\":40834},{\"end\":41337,\"start\":41254},{\"end\":41796,\"start\":41714},{\"end\":42156,\"start\":42115},{\"end\":42560,\"start\":42477},{\"end\":43062,\"start\":43004},{\"end\":43278,\"start\":43245},{\"end\":43499,\"start\":43443},{\"end\":43846,\"start\":43790},{\"end\":44130,\"start\":44103},{\"end\":44412,\"start\":44372},{\"end\":44771,\"start\":44715},{\"end\":36999,\"start\":36923},{\"end\":37466,\"start\":37389},{\"end\":37936,\"start\":37859},{\"end\":39940,\"start\":39908},{\"end\":40577,\"start\":40540},{\"end\":40956,\"start\":40899},{\"end\":41416,\"start\":41339},{\"end\":41874,\"start\":41798},{\"end\":42639,\"start\":42562},{\"end\":43107,\"start\":43064},{\"end\":44448,\"start\":44414}]"}}}, "year": 2023, "month": 12, "day": 17}