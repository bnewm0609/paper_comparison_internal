{"id": 9634599, "updated": "2023-09-28 15:48:41.191", "metadata": {"title": "Joint Distribution Optimal Transportation for Domain Adaptation", "authors": "[{\"first\":\"Nicolas\",\"last\":\"Courty\",\"middle\":[]},{\"first\":\"R'emi\",\"last\":\"Flamary\",\"middle\":[]},{\"first\":\"Amaury\",\"last\":\"Habrard\",\"middle\":[]},{\"first\":\"Alain\",\"last\":\"Rakotomamonjy\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 5, "day": 24}, "abstract": "This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function $f$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain $\\mathcal{P}_s$ and $\\mathcal{P}_t$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target $\\mathcal{P}^f_t=(X,f(X))$ by optimizing simultaneously the optimal coupling and $f$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1705.08848", "mag": "2962997028", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/CourtyFHR17", "doi": "10.14288/1.0357417"}}, "content": {"source": {"pdf_hash": "91db604c8ec5c209f7e47a4ef5310adaf2c17ec4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1705.08848v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "da821945687b54b27653fc8875a5fff2e37c6d84", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/91db604c8ec5c209f7e47a4ef5310adaf2c17ec4.txt", "contents": "\nJoint distribution optimal transportation for domain adaptation\n\n\nNicolas Courty courty@univ-ubs.fr \nR\u00e9mi Flamary remi.flamary@unice.fr \nAmaury Habrard amaury.habrard@univ-st-etienne.fr \nAlain Rakotomamonjy \n\nUMR 6074\nUniversit\u00e9 de Bretagne Sud\nIRISA\nCNRS\nUniversit\u00e9 C\u00f4te d'Azur\n7293LagrangeUMR\n\n\nCNRS\nUniv Lyon\nUJM-Saint-EtienneOCA\n\n\nLab. Hubert Curien UMR 5516\nCNRS\nNormandie Universite Universit\u00e9 de Rouen\nLITIS\nF-42023, 4108EA\n\nJoint distribution optimal transportation for domain adaptation\n\nThis paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function f in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a nonlinear transformation between the joint feature/label space distributions of the two domain P s and P t that can be estimated with optimal transport. We propose a solution of this problem that allows to recover an estimated target P f t = (X, f (X)) by optimizing simultaneously the optimal coupling and f . We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.\n\nIntroduction\n\nIn the context of supervised learning, one generally assumes that the test data is a realization of the same process that generated the learning set. Yet, in many practical applications it is often not the case, since several factors can slightly alter this process. The particular case of visual adaptation [1] in computer vision is a good example: given a new dataset of images without any label, one may want to exploit a different annotated dataset, provided that they share sufficient common information and labels. However, the generating process can be different in several aspects, such as the conditions and devices used for acquisition, different pre-processing, different compressions, etc. Domain adaptation techniques aim at alleviating this issue by transferring knowledge between domains [2]. We propose in this paper a principled and theoretically founded way of tackling this problem.\n\nThe domain adaptation (DA) problem is not new and has received a lot of attention during the past ten years. State-of-the-art methods are mainly differing by the assumptions made over the change in data distributions. In the covariate shift assumption, the differences between the domains are characterized by a change in the feature distributions P(X), while the conditional distributions P(Y |X) remain unchanged (X and Y being respectively the instance and label spaces). Importance re-weighting can be used to learn a new classifier (e.g. [3]), provided that the overlapping of the distributions is large enough. Kernel alignment [4] has also been considered for the same purpose. Other types of method, denoted as Invariant Components by Gong and co-authors [5], are looking for a transformation T such that the new representations of input data are matching, i.e. P s (T (X)) = P t (T (X)). Methods are then differing by: i) The considered class of transformation, that are generally defined as projections (e.g. [6,7,8,9,5]), affine transform [4] or non-linear transformation as expressed by neural networks [10,11] ii) The types of divergences used to compare P s (T (X)) and P t (T (X)), such as Kullback Leibler [12] or Maximum Mean Discrepancy [9,5]. Those divergences usually require that the distributions share a common support to be defined. A particular case is found in the use of optimal transport, introduced for domain adaptation by [13,14]. T is then defined to be a push-forward operator such that P s (X) = P t (T (X)) and that minimizes a global transportation effort or cost between distributions. The associated divergence is the so-called Wasserstein metric, that has a natural Lagrangian formulation and avoids the estimation of continuous distribution by means of kernel. As such, it also alleviates the need for a shared support.\n\nThe methods discussed above implicitly assume that the conditional distributions are unchanged by T , i.e. P s (Y |T (X)) \u2248 P t (Y |T (X)) but there is no clear reason for this assumption to hold. A more general approach is to adapt both marginal feature and conditional distributions by minimizing a global divergence between them. However, this task is usually hard since no label is available in the target domain and therefore no empirical version P t (Y |X) can be used. This was achieved by restricting to specific class of transformation such as projection [9,5].\n\nContributions and outline. In this work we propose a novel framework for unsupervised domain adaptation between joint distributions. We propose to find a function f that predicts an output value given an input x \u2208 X , and that minimizes the optimal transport loss between the joint source distribution P s and an estimated target joint distribution P f t = (X, f (X)) depending on f (detailed in Section 2). The method is denoted as JDOT for \"Joint Distribution Optimal Transport\" in the remainder. We show that the resulting optimization problem stands for a minimization of a bound on the target error of f (Section 3) and propose an efficient algorithm to solve it (Section 4). Our approach is very general and does not require to learn explicitly a transformation, as it directly solves for the best function. We show that it can handle both regression and classification problems with a large class of functions f including kernel machines and neural networks. We finally provide several numerical experiments on real regression and classification problems that show the performances of JDOT over the state-of-the-art (Section 5).\n\n\nJoint distribution Optimal Transport\n\nLet \u2126 \u2208 R d be a compact input measurable space of dimension d and C the set of labels. P(\u2126) denotes the set of all the probability measures over \u2126. The standard learning paradigm assumes classically the existence of a set of data X s = {x s i } Ns i=1 associated with a set of class label information Y s = {y s i } Ns i=1 , y s i \u2208 C (the learning set), and a data set with unknown labels X t = {x t i } Nt i=1 (the testing set). In order to determine the set of labels Y t associated with X t , one usually relies on an empirical estimate of the joint probability distribution P(X, Y ) \u2208 P(\u2126 \u00d7 C) from (X s , Y s ), and the assumption that X s and X t are drawn from the same distribution \u00b5 \u2208 P(\u2126). In the considered adaptation problem, one assumes the existence of two distinct joint probability distributions P s (X, Y ) and P t (X, Y ) which correspond respectively to two different source and target domains. We will write \u00b5 s and \u00b5 t their respective marginal distributions over X.\n\n\nOptimal transport in domain adaptation\n\nThe Monge problem is seeking for a map T 0 : \u2126 \u2192 \u2126 that pushes \u00b5 s toward \u00b5 t defined as:\nT 0 = argmin T \u2126 d(x, T (x))d\u00b5 s (x), s.t. T #\u00b5 s = \u00b5 t ,\nwhere T #\u00b5 s the image measure of \u00b5 s by T , verifying:\nT #\u00b5 s (A) = \u00b5 t (T \u22121 (A)), \u2200 Borel subset A \u2282 \u2126,(1)\nand d : \u2126 \u00d7 \u2126 \u2192 R + is a metric. In the remainder, we will always consider without further notification the case where d is the squared Euclidean metric. When T 0 exists, it is called an optimal transport map, but it is not always the case (e.g. assume that \u00b5 s is defined by one Dirac measure and \u00b5 t by two). A relaxed version of this problem has been proposed by Kantorovitch [15], who rather seeks for a transport plan (or equivalently a joint probability distribution) \u03b3 \u2208 P(\u2126 \u00d7 \u2126) such that:\n\u03b3 0 = argmin \u03b3\u2208\u03a0(\u00b5s,\u00b5t) \u2126\u00d7\u2126 d(x 1 , x 2 )d\u03b3(x 1 , x 2 ),(2)\nwhere \u03a0(\u00b5 s , \u00b5 t ) = {\u03b3 \u2208 P(\u2126 \u00d7 \u2126)|p + #\u03b3 = \u00b5 s , p \u2212 #\u03b3 = \u00b5 t } and p + and p \u2212 denotes the two marginal projections of \u2126 \u00d7 \u2126 to \u2126. Minimizers of this problem are called optimal transport plans. Should \u03b3 0 be of the form (id \u00d7 T )#\u00b5 s , then the solution to Kantorovich and Monge problems coincide. As such the Kantorovich relaxation can be seen as a generalization of the Monge problem, with less constraints on the existence and uniqueness of solutions [16].\n\nOptimal transport has been used in DA as a principled way to bring the source and target distribution closer [13,14,17], by seeking for a transport plan between the empirical distributions of X s and X t and interpolating X s thanks to a barycentric mapping [14], or by estimating a mapping which is not the solution of Monge problem but allows to map unseen samples [17]. Moreover, they show that better constraining the structure of \u03b3 through entropic or classwise regularization terms helps in achieving better empirical results.\n\n\nJoint distribution optimal transport loss\n\nThe main idea of this work is is to handle a change in both marginal and conditional distributions. As such, we are looking for a transformation T that will align directly the joint distributions P s and P t . Following the Kantovorich formulation of (2), T will be implicitly expressed through a coupling between both joint distributions as:\n\u03b3 0 = argmin \u03b3\u2208\u03a0(Ps,Pt) (\u2126\u00d7C) 2 D(x 1 , y 1 ; x 2 , y 2 )d\u03b3(x 1 , y 1 ; x 2 , y 2 ),(3)\nwhere D(x 1 , y 1 ; x 2 , y 2 ) = \u03b1d(x 1 , x 2 ) + L(y 1 , y 2 ) is a joint cost measure combining both the distances between the samples and a loss function L measuring the discrepancy between y 1 and y 2 . While this joint cost is specific (separable), we leave for future work the analysis of generic joint cost function. Putting it in words, matching close source and target samples with similar labels costs few. \u03b1 is a positive parameter which balances the metric in the feature space and the loss. As such, when \u03b1 \u2192 +\u221e, this cost is dominated by the metric in the input feature space, and the solution of the coupling problem is the same as in [14]. It can be shown that a minimizer to (3) always exists and is unique provided that D(\u00b7) is lower semi-continuous (see [18], Theorem 4.1), which is the case when d(\u00b7) is a norm and for every usual loss functions [19].\n\nIn the unsupervised DA problem, one does not have access to labels in the target domain, and as such it is not possible to find the optimal coupling. Since our goal is to find a function on the target domain f : \u2126 \u2192 C, we suggest to replace y 2 by a proxy f (x 2 ). This leads to the definition of the following joint distribution that uses a given function f as a proxy for y:\nP f t = (x, f (x)) x\u223c\u00b5t(4)\nIn practice we consider empirical versions of P s and P f t , i.e.P s = 1\nNs Ns i=1 \u03b4 x s i ,y s i andP f t = 1 Nt Nt i=1 \u03b4 x t i ,f (x t i )\n. \u03b3 is then a matrix which belongs to \u2206 , i.e.the transportation polytope of nonnegative matrices between uniform distributions. Since our goal is to estimate a prediction f on the target domain, we propose to find the one that produces predictions that match optimally source labels to the aligned target instances in the transport plan. For this purpose, we propose to solve the following problem for JDOT:\nmin f,\u03b3\u2208\u2206 ij D(x s i , y s i ; x t j , f (x t j ))\u03b3 ij \u2261 min f W 1 (P s ,P f t )(5)\nwhere W 1 is the 1-Wasserstein distance for the loss D(x 1 , y 1 ; x 2 , y 2 ) = \u03b1d(x 1 , x 2 ) + L(y 1 , y 2 ). We will make clear in the next section that the function f we retrieve is theoretically sound with respect to the target error. Note that in practice we add a regularization term for function f in order to avoid overfitting as discussed in Section 4. An illustration of JDOT for a regression problem is given in Figure 1. In this figure, we have very different joint and marginal distributions but we want to illustrate that the OT matrix \u03b3 obtained using the true empirical distribution P t is very similar to the one obtained with the proxy P f t which leads to a very good model for JDOT. Choice of \u03b1. This is an important parameter balancing the alignment of feature space and labels. A natural choice of the \u03b1 parameter is obtained by normalizing the range of values of d(\nx s i , x t j ) with \u03b1 = 1/ max i,j d(x s i , x t j ).\nIn the numerical experiment section, we show that this setting is very good in two out of three experiments. However, in some cases, better performances are obtained with a cross-validation of this parameter. Also note that \u03b1 is strongly linked to the smoothness of the loss L and of the optimal labelling functions and can be seen as a Lipschitz constant in the bound of Theorem D.1.\n\nRelation to other optimal transport based DA methods. Previous DA methods based on optimal transport [14,17] do not not only differ by the nature of the considered distributions, but also in the way the optimal plan is used to find f . They learn a complex mapping between the source and target distributions when the objective is only to estimate a prediction function f on target. To do so, they rely on a barycentric mapping that minimizes only approximately the Wasserstein distance between the distributions. As discussed in Section 4, JDOT uses the optimal plan to propagate and fuse the labels from the source to target. Not only are the performances enhanced, but we also show how this approach is more theoretically well grounded in next section 3.\n\nRelation to Transport L p distances. Recently, Thorpe and co-authors introduced the Transportation L p distance [20]. Their objective is to compute a meaningful distance between multi-dimensional signals. Interestingly their distance can be seen as optimal transport between two distributions of the form (4) where the functions are known and the label loss L is chosen as a L p distance. While their approach is inspirational, JDOT is different both in its formulation, where we introduce a more general class of loss L, and in its objective, as our goal is to estimate the target function f which is not known a priori. Finally we show theoretically and empirically that our formulation addresses successfully the problem of domain adaptation.\n\n\nA Bound on the Target Error\n\nLet f be an hypothesis function from a given class of hypothesis H. We define the expected loss in the target domain err T (f ) as err T (f ) def = E (x,y)\u223cPt L(y, f (x)). We define similarly err S (f ) for the source domain. We assume the loss function L to be bounded, symmetric, k-lipschitz and satisfying the triangle inequality.\n\nTo provide some guarantees on our method, we consider an adaptation of the notion probabilistic Lipschitzness introduced in [21,22] which assumes that two close instances must have the same labels with high probability. It corresponds to a relaxation of the classic Lipschitzness allowing one to model the marginal-label relatedness such as in Nearest-Neighbor classification, linear classification or cluster assumption. We propose an extension of this notion in a domain adaptation context by assuming that a labeling function must comply with two close instances of each domain w.r.t. a coupling \u03a0.\n\nDefinition (Probabilistic Transfer Lipschitzness) Let \u00b5 s and \u00b5 t be respectively the source and target distributions. Let \u03c6 : R \u2192 [0, 1]. A labeling function f : \u2126 \u2192 R and a joint distribution \u03a0(\u00b5 s , \u00b5 t ) over \u00b5 s and \u00b5 t are \u03c6-Lipschitz transferable if for all \u03bb > 0:\nP r (x1,x2)\u223c\u03a0(\u00b5s,\u00b5t) [|f (x 1 ) \u2212 f (x 2 )| > \u03bbd(x 1 , x 2 )] \u2264 \u03c6(\u03bb).\nIntuitively, given a deterministic labeling functions f and a coupling \u03a0, it bounds the probability of finding pairs of source-target instances labelled differently in a (1/\u03bb)-ball with respect to \u03a0.\n\nWe can now give our main result (simplified version):\nTheorem 3.1 Let f be any labeling function of \u2208 H. Let \u03a0 * = argmin \u03a0\u2208\u03a0(Ps,P f t ) (\u2126\u00d7C) 2 \u03b1d(x s , x t ) + L(y s , y t )d\u03a0(x s , y s ; x t , y t )\nand W 1 (P s ,P f t ) the associated 1-Wasserstein distance. Let f * \u2208 H be a Lipschitz labeling function that verifies the \u03c6-probabilistic transfer Lipschitzness (PTL) assumption w.r.t. \u03a0 * and that minimizes the joint error\nerr S (f * ) + err T (f * ) w.r.t all PTL functions compatible with \u03a0 * . We assume the input instances are bounded s.t. |f * (x 1 ) \u2212 f * (x 2 )| \u2264 M for all x 1 , x 2 .\nLet L be any symmetric loss function, k-Lipschitz and satisfying the triangle inequality. Consider a sample of N s labeled source instances drawn from P s and N t unlabeled instances drawn from \u00b5 t , and then for all \u03bb > 0, with \u03b1 = k\u03bb, we have with probability at least 1 \u2212 \u03b4 that:\nerr T (f ) \u2264 W 1 (P s ,P f t ) + 2 c log( 2 \u03b4 ) 1 \u221a N S + 1 \u221a N T + err S (f * ) + err T (f * ) + kM \u03c6(\u03bb).\nThe detailed proof of Theorem D.1 is given in the supplementary material. The previous bound on the target error above is interesting to interpret. The first two terms correspond to the objective function (5) we propose to minimize accompanied with a sampling bound. The last term \u03c6(\u03bb) assesses the probability under which the probabilistic Lipschitzness does not hold. The remaining two terms involving f * correspond to the joint error minimizer illustrating that domain adaptation can work only if we can predict well in both domains, similarly to existing results in the literature [23,24]. If the last terms are small enough, adaptation is possible if we are able to align well P s and P f t , provided that f * and \u03a0 * verify the PTL. Finally, note that \u03b1 = k\u03bb and tuning this parameter is thus actually related to finding the Lipschitz constants of the problem.\n\n\nLearning with Joint Distribution OT\n\nIn this section, we provide some details about the JDOT's optimization problem given in Equation (5) and discuss algorithms for its resolution. We will assume that the function space H to which f belongs is either a RKHS or a function space parametrized by some parameters w \u2208 R p . This framework encompasses linear models, neural networks, and kernel methods. Accordingly, we are going to define a regularization term \u2126(f ) on f . Depending on how H is defined, \u2126(f ) is either a non-decreasing function of the squared-norm induced by the RKHS (so that the representer theorem is applicable) or a squared-norm on the vector parameter. We will further assume that \u2126(f ) is continuously differentiable. As discussed above, f is to be learned according to the following optimization problem\nmin f \u2208H,\u03b3\u2208\u2206 i,j \u03b3 i,j \u03b1d(x s i , x t j ) + L(y s i , f (x t j )) + \u03bb\u2126(f )(6)\nwhere the loss function L is continuous and differentiable with respects to its second variable. Note that while the above problem does not involve any regularization term on the coupling matrix \u03b3, it is essentially for the sake of simplicity and readability. Regularizers like entropic regularization [25], which is relevant when the number of samples is very large, can still be used without significant change to the algorithmic framework.\n\nOptimization procedure. According to the above hypotheses on f and L, Problem (6) is smooth and the constraints are separable according to f and \u03b3. Hence, a natural way to solve the problem (6) is to rely on alternate optimization w.r.t. both parameters \u03b3 and f . This algorithm well-known as Block Coordinate Descent (BCD) or Gauss-Seidel method (the pseudo code of the algorithm is given in appendix). Block optimization steps are discussed with further details in the following.\n\nSolving with fixed f boils down to a classical OT problem with a loss matrix C such that C i,j = \u03b1d(x s i , x t j ) + L(y s i , f (x t j )). We can use classical OT solvers such as the network simplex algorithm, but other strategies can be considered, such as regularized OT [25] or stochastic versions [26].\n\nThe optimization problem with fixed \u03b3 leads to a new learning problem expressed as\nmin f \u2208H i,j \u03b3 i,j L(y s i , f (x t j )) + \u03bb\u2126(f )(7)\nNote how the data fitting term elegantly and naturally encodes the transfer of source labels y s i through estimated labels of test samples with a weighting depending on the optimal transport matrix. However, this comes at the price of having a quadratic number N s N t of terms, which can be considered as computationally expensive. We will see in the sequel that we can benefit from the structure of the chosen loss to greatly reduce its complexity. In addition, we emphasize that when H is a RKHS, owing to kernel trick and the representer theorem, problem (7) can be re-expressed as an optimization problem with N t number of parameters all belonging to R.\n\nLet us now discuss briefly the convergence of the proposed algorithm. Owing to the 2-block coordinate descent structure, to the differentiability of the objective function in Problem (6) and constraints on f (or its kernel trick parameters) and \u03b3 are closed, non-empty and convex, convergence result of Grippo et al. [27] on 2-block Gauss-Seidel methods directly applies. It states that if the sequence {\u03b3 k , f k } produced by the algorithm has limit points then every limit point of the sequence is a critical point of Problem (6).\n\nEstimating f for least square regression problems. We detail the use of JDOT for transfer leastsquare regression problem i.e when L is the squared-loss. In this context, when the optimal transport matrix \u03b3 is fixed the learning problem boils down to\nmin f \u2208H j 1 n t \u0177 j \u2212 f (x t j ) 2 + \u03bb f 2(8)\nwhere the\u0177 j = n t j \u03b3 i,j y s i is a weighted average of the source target values. Note that this simplification results from the properties of the quadratic loss and that it may not occur for more complex regression loss.\n\nEstimating f for hinge loss classification problems. We now aim at estimating a multiclass classifier with a one-against-all strategy. We suppose that the data fitting is the binary squared hinge loss of the form L(y, f (x)) = max(0, 1 \u2212 yf (x)) 2 . In a One-Against-All strategy we often use the binary matrices P such that P s i,k = 1 if sample i is of class k else P s i,k = 0. Denote as f k \u2208 H the decision function related to the k-vs-all problem. The learning problem (7) can now be expressed as\nmin f k \u2208H j,kP j,k L(1, f k (x t j )) + (1 \u2212P j,k )L(\u22121, f k (x t j )) + \u03bb k f k 2(9)\nwhereP is the transported class proportion matrixP = 1 Nt \u03b3 P s . Interestingly this formulation illustrates that for each target sample, the data fitting term is a convex sum of hinge loss for a negative and positive label with weights in \u03b3.\n\n\nNumerical experiments\n\nIn this section we evaluate the performance of our method (JDOT) on two different transfer tasks of classification and regression on real datasets 2 .\n\nCaltech-Office classification dataset. This dataset [28] is dedicated to visual adaptation. It contains images from four different domains: Amazon, the Caltech-256 image collection, Webcam and DSLR. Several features, such as presence/absence of background, lightning conditions, image quality, etc.) induce a distribution shift between the domains, and it is therefore relevant to consider a domain adaptation task to perform the classification. Following [14], we choose deep learning features to represent the images, extracted as the weights of the fully connected 6th layer of the DECAF convolutional neural network [29], pre-trained on ImageNet. The final feature vector is a sparse 4096 dimensional vector.   [30], ARTL), and the two variants of regularized optimal transport [14]: entropy-regularized OT-IT and classwise regularization implemented with the Majoration-Minimization algorithm OT-MM, that showed to give better results in practice than its group-lasso counterpart. The classification is conducted with a SVM together with a linear kernel for every method. Its results when learned on the source domain and tested on the target domain are also reported to serve as baseline (Base). All the methods have hyper-parameters, that are selected using the reverse cross-validation of Zhong and colleagues [31]. The classification accuracy for all the methods is reported in Table 1. We can see that JDOT is consistently outperforming the baseline (5 points in average), indicating that the adaptation is successful in every cases. Its mean accuracy is the best as well as its average ranking. We conducted a Wilcoxon signed-rank test to test if JDOT was statistically better than the other methods, and report the p-value in the tables. This test shows that JDOT is statistically better than the considered methods, except for OT based ones that where state of the art on this dataset [14].\n\nAmazon review classification dataset We now consider the Amazon review dataset [32] which contains online reviews of different products collected on the Amazon website. Reviews are encoded with bag-of-word unigram and bigram features as input. The problem is to predict positive (higher than 3 stars) or negative (3 stars or less) notation of reviews (binary classification). Since different words are employed to qualify the different categories of products, a domain adaptation task can be formulated if one wants to predict positive reviews of a product from labelled reviews of a different product. Following [33,11], we consider only a subset of four different types of product: books, DVDs, electronics and kitchens. This yields 12 possible adaptation tasks. Each domain contains 2000 labelled samples and approximately 4000 unlabelled ones. We therefore use these unlabelled samples to perform the transfer, and test on the 2000 labelled data.\n\nThe goal of this experiment is to compare to the state-of-the-art method on this subset, namely Domain adversarial neural network ( [11], denoted DANN), and to show the versatility of our method that can adapt to any type of classifier. The neural network used for all methods in this experiment is a simple 2-layer model with sigmoid activation function in the hidden layer to promote non-linearity. 50 neurons are used in this hidden layer. For DANN, hyper-parameters are set through the reverse cross-validation proposed in [11], and following the recommendation of authors the learning rate is set to 10 \u22123 . In the case of JDOT, we used the heuristic setting of \u03b1 = 1/ max i,j d(x s i , x t j ), and as such we do not need any cross-validation. The squared Euclidean norm is used for both metric in feature space and we test as loss functions both mean squared errors (mse) and Hinge losses. 10 iterations of the block coordinate descent are realized. For each method, we stop the learning process of the network after 5 epochs. Classification accuracies are presented in table 2. The neural network (NN), trained on source and tested on target, is also presented as a baseline. JDOT surpasses DANN in 11 out of 12 tasks (except on books\u2192dvd). The Hinge loss is better in than mse in 10 out of 12 cases, which is expected given the superiority of the Hinge loss on classification tasks [19].\n\nWifi localization regression dataset For the regression task, we use the cross-domain indoor Wifi localization dataset that was proposed by Zhang and co-authors [4], and recently studied in [5]. From a multi-dimensional signal (collection of signal strength perceived from several access points), the goal is to locate the device in a hallway, discretized into a grid of 119 squares, by learning a mapping from the signal to the grid element. This translates as a regression problem. As the signals were acquired at different time periods by different devices, a shift can be encountered and calls for an adaptation. In the remaining, we follow the exact same experimental protocol as in [4,5] for ease of comparison. Two cases of adaptation are considered: transfer across periods, for which three time periods t1, t2 and t3 are considered, and transfer across devices, where three different devices are used to collect the signals in the same straight-line hallways (hallway1-3), leading to three different adaptation tasks in both cases.\n\nWe compare the result of our method with several state-of-the-art methods: kernel ridge regression with RBF kernel (KRR), surrogate kernel ( [4], denoted SurK), domain-invariant projection and its cluster regularized version ( [7], denoted respectively DIP and DIP-CC), generalized target shift ( [34], denoted GeTarS), and conditional transferable components, with its target information preservation regularization ( [5], denoted respectively CTC and CTC-TIP). As in [4,5], the hyper-parameters of the competing methods are cross-validated on a small subset of the target domain. In the case of JDOT, we simply set the \u03b1 to the heuristic value of \u03b1 = 1/ max i,j d(x s i , x t j ) as discussed previously, and f is estimated with kernel ridge regression.\n\nFollowing [4], the accuracy is measured in the following way: the prediction is said to be correct if it falls within a range of three meters in the transfer across periods, and six meters in the transfer across devices. For each experiment, we randomly sample sixty percent of the source and target domain, and report the mean and standard deviation of ten repetitions accuracies in Table 3. For transfer across periods, JDOT performs best in one out of three tasks. For transfer across devices, the superiority of JDOT is clearly assessed, for it reaches an average score > 98%, which is at least ten points ahead of the best competing method for every task. Those extremely good results could be explained by the fact that using optimal transport allows to consider large shifts of distribution, for which divergences (such as maximum mean discrepancy used in CTC) or reweighting strategies can not cope with.\n\n\nDiscussion and conclusion\n\nWe have presented in this paper the Joint Distribution Optimal Transport for domain adaptation, which is a principled way of performing domain adaptation with optimal transport. JDOT assumes the existence of a transfer map that transforms a source domain joint distribution P s (X, Y ) into a target domain equivalent version P t (X, Y ). Through this transformation, the alignment of both feature space and conditional distributions is operated, allowing to devise an efficient algorithm that simultaneously optimizes for a coupling between P s and P t and a prediction function that solves the transfer problem. We also proved that learning with JDOT is equivalent to minimizing a bound on the target distribution. We have demonstrated through experiments on classical real-world benchmark datasets the superiority of our approach w.r.t. several state-of-the-art methods, including previous work on optimal transport based domain adaptation, domain adversarial neural networks or transfer components, on a variety of task including classification and regression. We have also showed the versatility of our method, that can accommodate with several types of loss functions (mse, hinge) or class of hypothesis (including kernel machines or neural networks). Potential follow-ups of this work include a semi-supervised extension (using unlabelled examples in source domain) and investigating stochastic techniques for solving efficiently the adaptation. From a theoretical standpoint, future works include a deeper study of probabilistic transfer lipschitzness and the development of guarantees able to take into the complexity of the hypothesis class and the space of possible transport plans.\n\n\nAppendix Section\n\n\nA Illustration on a simple example\n\nWe illustrate the behavior of our method on a 3-class toy example (Figure 2). We consider a classification problem using the hinge loss and H is a Reproducing Kernel Hilbert Space. Source domain samples are drawn from three different 2D Gaussian distributions with with different centers and standard deviations. The target domain is obtained rotating the source distribution by \u03c0/4 radian. Two types of kernel are considered: linear and RBF. In Figure 2.a, one can observe on the first column of images that using directly a classifier learned on the source domain leads to bad performances because of the rotation. We then show the iterations of the block coordinate descent which allows one to recover the true labels of the target domain. It is also interesting to examine the impact of the \u03b1 parameter on the success of the method. In Figure 2.b, we show the evolution of classification accuracy for six different \u03b1 in the case of RBF kernel. Relying mostly on the label cost (\u03b1 = {0.1}) leads to a deterioration of the final accuracy. Using only the input space distance (\u03b1 = {50, 100}), which is equivalent to [14], allows a performance gain. But it is clear that using both losses with \u03b1 = {0.5, 1, 10} leads to the best performance. Also note the small number of iterations required (< 10) for achieving a steady state. B Block coordinate descent algorithm for solving JDOT We give in algorithm 1 an overview of the block coordinate descent algorithm used for solving JDOT.\n\n\nAlgorithm 1 Optimization with Block Coordinate Descent\n\nInitialize function f 0 and set k = 1 Set \u03b1 and \u03bb while not converged do \u03b3 k \u2190 Solve OT problem (3 in paper) with fixed f k\u22121 f k \u2190 Solve learning problem (7 in paper) with fixed \u03b3 k k \u2190 k + 1 end while C BCD iterations on real data\n\nWe report in Table 4, for a fixed set of parameter (no CV), the evolution of the empirical error along the iterations of the 15 first iterations of the BCD on a real dataset. We can see that generally the result stabilizes at around 10 iterations. We can also observe that the increase in performance is not monotonic, contrary to the toy example. We first recall some hypothesis used for this theorem.\n\nH \u2282 C \u2126 is the hypothesis class. L : C \u00d7 C \u2192 R + is the loss function measuring the discrepancy between two labels. This loss is assumed to be symmetric, bounded and k-lipschitz in its second argument, i.e. there exists k such that for any y 1 , y 2 , y 3 \u2208 C:\n\n|L(y 1 , y 2 ) \u2212 L(y 1 , y 3 )| \u2264 k|y 2 \u2212 y 3 |. We can similarly define err S (f ) in the source domain and the expected inter function loss err T (f, g) = E (x,y)\u223cPt L(g(x), f (x)).\n\nThe proxy P f t over \u2126 \u00d7 C of P t w.r.t. to \u00b5 t and f is defined as: P f t = (x, f (x)) x\u223c\u00b5t .\n\nWe consider the following transport loss function:\nW 1 (P s , P f t ) = inf \u03a0\u2208\u03a0(Ps,P f t ) (\u2126\u00d7C) 2\n\u03b1d(x s , x t ) + L(y s , y t )d\u03a0((x s , y s ), (x t , y t )).\n\nWe now recall the definition of the theorem with all the assumptions.\n\nTheorem D.1 Let H \u2282 C \u2126 be the hypothesis class where \u2126 is a compact mesurable space of finite dimension accompanied with a metric d, and C is the output space. Let f be any labeling function of \u2208 H. Let P s , P t , P f t be three probability distributions over \u2126 \u00d7 C with bounded support, with P f t defined w.r.t. the marginal \u00b5 t of P t and f , accompanied with a sample of N s labeled source instances drawn from P s and N t unlabeled instances drawn from \u00b5 t and labeled by f , such that P s and P f t and the associated samples follow the assumptions of Theorem E.1. Let \u03a0 * = argmin \u03a0\u2208\u03a0(Ps,P f t ) (\u2126\u00d7C) 2 \u03b1d(x s , x t ) + L(y s , y t )d\u03a0((x s , y s ), (x t , y t )). Let f * be a Lipschitz labeling function of H, that verifies the \u03c6-probabilistic transfer Lipschitzness (PTL) assumption with respect to \u03a0 * and that minimizes the joint error err S (f * ) + err T (f * ) w.r.t all compatible PTL functions with \u03a0 * . We assume the instance space X \u2286 \u2126 is bounded 3 such that\n\nFigure 1 :\n1Illustration of JDOT on a 1D regression problem. (left) Source and target empirical distributions and marginals (middle left) Source and target models (middle right) OT matrix on empirical joint distributions and with JDOT proxy joint distribution (right) estimated prediction function f .\n\n\nThe dimension d for SA is chosen from {1, 4, 7, . . . , 31}. The entropy regularization for OT-IT and OT-MM is taken from {10 2 , . . . , 10 5 }, 10 2 being the minimum value for the Sinkhorn algorithm to prevent numerical errors. Finally the \u03b7 parameter of OT-MM is selected from {1, . . . , 10 5 } and the \u03b1 in JDOT from {10 \u22125 , 10 \u22124 , . . . , 1}.\n\nFigure 2 :\n2Illustration on a toy example. (a): Decision boundaries for linear and RBF kernels on selected iterations. The source domain is depicted with crosses, while the target domain samples are class-colored circles. (b): Evolution of the accuracy along 15 iterations of the method for different values of the \u03b1 parameter;\n\nP\nt and P s are respectively the target and source distributions over \u2126\u00d7C, with \u00b5 t and \u00b5 s the respective marginals over \u2126. The expected loss in the target domain err T (f ) is defined for any f \u2208 H as err T (f )\n\nTable 1 :\n1Accuracy on the Caltech-Office Dataset. Best value in bold.Domains \nBase \nSurK \nSA \nARTL \nOT-IT OT-MM JDOT \n\ncaltech\u2192amazon \n92.07 \n91.65 \n90.50 \n92.17 \n89.98 \n92.59 \n91.54 \ncaltech\u2192webcam \n76.27 \n77.97 \n81.02 \n80.00 \n80.34 \n78.98 \n88.81 \ncaltech\u2192dslr \n84.08 \n82.80 \n85.99 \n88.54 \n78.34 \n76.43 \n89.81 \namazon\u2192caltech \n84.77 \n84.95 \n85.13 \n85.04 \n85.93 \n87.36 \n85.22 \namazon\u2192webcam \n79.32 \n81.36 \n85.42 \n79.32 \n74.24 \n85.08 \n84.75 \namazon\u2192dslr \n86.62 \n87.26 \n89.17 \n85.99 \n77.71 \n79.62 \n87.90 \nwebcam\u2192caltech \n71.77 \n71.86 \n75.78 \n72.75 \n84.06 \n82.99 \n82.64 \nwebcam\u2192amazon \n79.44 \n78.18 \n81.42 \n79.85 \n89.56 \n90.50 \n90.71 \nwebcam\u2192dslr \n96.18 \n95.54 \n94.90 100.00 \n99.36 \n99.36 \n98.09 \ndslr\u2192caltech \n77.03 \n76.94 \n81.75 \n78.45 \n85.57 \n83.35 \n84.33 \ndslr\u2192amazon \n83.19 \n82.15 \n83.19 \n83.82 \n90.50 \n90.50 \n88.10 \ndslr\u2192webcam \n96.27 \n92.88 \n88.47 \n98.98 \n96.61 \n96.61 \n96.61 \nMean \n83.92 \n83.63 \n85.23 \n85.41 \n86.02 \n86.95 \n89.04 \nMean rank \n5.33 \n5.58 \n4.00 \n3.75 \n3.50 \n2.83 \n2.50 \np-value \n< 0.01 < 0.01 \n0.01 \n0.04 \n0.25 \n0.86 \n\u2212 \n\n\n\nTable 2 :\n2Accuracy on the Amazon review experiment. Maximum value in bold font.Domains \nNN \nDANN JDOT (mse) JDOT (Hinge) \n\nbooks\u2192dvd \n0.805 \n0.806 \n0.794 \n0.795 \nbooks\u2192kitchen \n0.768 \n0.767 \n0.791 \n0.794 \nbooks\u2192electronics \n0.746 \n0.747 \n0.778 \n0.781 \ndvd\u2192books \n0.725 \n0.747 \n0.761 \n0.763 \ndvd\u2192kitchen \n0.760 \n0.765 \n0.811 \n0.821 \ndvd\u2192electronics \n0.732 \n0.738 \n0.778 \n0.788 \nkitchen\u2192books \n0.704 \n0.718 \n0.732 \n0.728 \nkitchen\u2192dvd \n0.723 \n0.730 \n0.764 \n0.765 \nkitchen\u2192electronics 0.847 \n0.846 \n0.844 \n0.845 \nelectronics\u2192books \n0.713 \n0.718 \n0.740 \n0.749 \nelectronics\u2192dvd \n0.726 \n0.726 \n0.738 \n0.737 \nelectronics\u2192kitchen 0.855 \n0.850 \n0.868 \n0.872 \n\nMean \n0.759 \n0.763 \n0.783 \n0.787 \np-value \n0.004 \n0.006 \n0.025 \n\u2212 \n\nWe compare our method with four other methods: the surrogate kernel approach ([4], denoted \nSurK), subspace adaptation for its simplicity and good performances on visual adaptation ([8], SA), \nAdaptation Regularization based Transfer Learning (\n\nTable 3 :\n3Comparison of different methods on the Wifi localization dataset. Maximum value in bold.Domains \nKRR \nSurK \nDIP \nDIP-CC \nGeTarS \nCTC \nCTC-TIP \nJDOT \n\nt1 \u2192 t2 \n80.84\u00b11.14 \n90.36\u00b11.22 \n87.98\u00b12.33 \n91.30\u00b13.24 86.76 \u00b1 1.91 89.36\u00b11.78 \n89.22\u00b11.66 93.03 \u00b1 1.24 \nt1 \u2192 t3 \n76.44\u00b12.66 \n94.97\u00b11.29 \n84.20\u00b14.29 \n84.32\u00b14.57 \n90.62\u00b12.25 \n94.80\u00b10.87 92.60 \u00b1 4.50 90.06 \u00b1 2.01 \nt2 \u2192 t3 \n67.12\u00b11.28 85.83 \u00b1 1.31 80.58 \u00b1 2.10 81.22 \u00b1 4.31 82.68 \u00b1 3.71 87.92 \u00b1 1.87 89.52 \u00b1 1.14 86.76 \u00b1 1.72 \n\nhallway1 60.02 \u00b12.60 76.36 \u00b1 2.44 77.48 \u00b1 2.68 76.24\u00b1 5.14 84.38 \u00b1 1.98 86.98 \u00b1 2.02 86.78 \u00b1 2.31 98.83\u00b10.58 \nhallway2 49.38 \u00b1 2.30 64.69 \u00b10.77 78.54 \u00b1 1.66 \n77.8\u00b1 2.70 \n77.38 \u00b1 2.09 87.74 \u00b1 1.89 87.94 \u00b1 2.07 98.45\u00b10.67 \nhallway3 48.42 \u00b11.32 65.73 \u00b1 1.57 75.10\u00b1 3.39 73.40\u00b1 4.06 80.64 \u00b1 1.76 82.02\u00b1 2.34 81.72 \u00b1 2.25 99.27\u00b10.41 \n\n\n\n\nIter caltech \u2192 amazon dslr \u2192 amazon webcam \u2192 caltech : Accuracy of the estimated model along BCD iterations on Caltech-office dataset D Proof of Theorem 3.10 \n89.14 \n80.9 \n75.6 \n1 \n91.75 \n86.22 \n80.23 \n2 \n91.44 \n86.95 \n81.75 \n3 \n91.54 \n87.68 \n82.64 \n4 \n91.44 \n87.68 \n83.26 \n5 \n91.65 \n88.0 \n83.35 \n6 \n91.86 \n88.1 \n83.17 \n7 \n92.17 \n87.89 \n83.08 \n8 \n92.28 \n87.58 \n83.26 \n9 \n92.28 \n87.58 \n83.26 \n10 \n92.28 \n87.68 \n83.35 \n11 \n92.28 \n87.79 \n83.44 \n12 \n92.28 \n87.79 \n83.44 \n13 \n92.28 \n87.79 \n83.44 \n14 \n92.28 \n87.79 \n83.44 \nTable 4\nOpen Source Python implementation of JDOT: https://github.com/rflamary/JDOT\nSince the input space is bounded by say a constant K: x \u2264 K, since f * is supposed l-Lipschitz, then we have for any x1, x2: |f (x1) \u2212 f (x2)| \u2264 l x1 \u2212 x2 \u2264 2lK = M .\nAcknowledgementsThis work benefited from the support of the project OATMIL ANR-17-CE23-0012 of the French National Research Agency (ANR), the Normandie Projet GRR-DAISI, European funding FEDER DAISI and CNRS funding from the D\u00e9fi Imag'In. The authors also wish to thank Kai Zhang and Qiaojun Wang for providing the Wifi localization dataset.(12)Line(10)is due to the symmetry of the loss. Line(11)comes from the fact that:Now, we haveLine(13)is a consequence of the duality form of the Kantorovitch-Rubinstein theorem saying that for any coupling \u03a0 \u2208 \u03a0(P s , P f t ), we have:Since the inequality is true for any coupling, it is then also true for \u03a0 * . Inequality(14)is due to the k-lipschitzness of the loss L in its second argument. Inequality(15)uses the fact that f * and \u03a0 * verify the probabilistic transfer Lipschitzness property with probability 1 \u2212 \u03c6(\u03bb), additionally, taking into account that the deviation between 2 instances with respect to f * is bounded by M we have the additional term kM \u03c6(\u03bb) that covers the regions where the PTL does not hold.(16)is obtained by the symmetry of d, the use of triangle inequality on L and by replacing k\u03bb by \u03b1. Other inequalities above are due the use of triangle inequality or properties of the absolute value. The last line(18)is due to the definition of \u03a0 * . Now, note that by the use of triangle inequality:Indeed, the cost function D((x s , y s ), (x t , y t )) = \u03b1d(x 1 , x 2 ) + L(y 1 , y 2 ) defines a distance over (\u2126 \u00d7 L) 2 , assuming that P s and P f t have bounded support and the fact that our loss function is bounded, we can apply Theorem E.1 (presented below) on W 1 (P s ,P s ) and W 1 (P f t , P t ) above (with probability \u03b4/2 each). The two settings may have different constants N and c and and we consider the maximum N and the minimum c that comply with both cases.Combining inequalities(12),(18), inequality(20)and the use of the union bound, the theorem holds with probability at least 1 \u2212 \u03b4 for any f \u2208 H.Note that, additionally to the analysis in the paper, a link can be made with classic generalization bounds when the two distributions are equal, i.e. P s = P t . Indeed, if we can choose f * as the true labeling function on source/target domains such that f * is strongly \u03c6-lipschitz w.r.t. \u03a0 * (i.e. \u03c6(\u03bb) is almost 0), then the bound is similar to a classic generalization bound: terms involving f * are null and using the same sample for source and target d(x 1 , x 2 ) = 0 w.r.t the best alignment. Thus, it remains only the label loss which corresponds to a classic supervised learning loss.E Empirical concentration result for Wasserstein distanceWe give now the result from Bolley and co-authors used in the previous section.Theorem E.1 (from[35], Theorem 1.1.) Let \u00b5 be a probability measure in Z so that for some \u03b1 > 0 we have for any z R d e \u03b1dist(z,z ) 2 d\u00b5 < \u221e and\u03bc = 1 N N i=1 \u03b4 zi be the associated empirical measure defined on a sample of independent variables {z i } N i=1 drawn from \u00b5. Then, for any d > dim(Z) and c < c, there exists some constant N 0 depending on d and some square exponential moments of \u00b5 such that for any > 0 and N \u2265 N 0 max( \u2212(d +2) , 1),where c can be calculated explicitly.\nVisual domain adaptation: an overview of recent advances. V M Patel, R Gopalan, R Li, R Chellappa, IEEE Signal Processing Magazine. 323V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual domain adaptation: an overview of recent advances. IEEE Signal Processing Magazine, 32(3), 2015.\n\nA survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on Knowledge and Data Engineering. 2210S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345-1359, 2010.\n\nDirect importance estimation with model selection and its application to covariate shift adaptation. M Sugiyama, S Nakajima, H Kashima, P V Buenau, M Kawanabe, NIPS. M. Sugiyama, S. Nakajima, H. Kashima, P.V. Buenau, and M. Kawanabe. Direct importance estimation with model selection and its application to covariate shift adaptation. In NIPS, 2008.\n\nCovariate shift in Hilbert space: A solution via surrogate kernels. K Zhang, V W Zheng, Q Wang, J T Kwok, Q Yang, I Marsic, ICML. K. Zhang, V. W. Zheng, Q. Wang, J. T. Kwok, Q. Yang, and I. Marsic. Covariate shift in Hilbert space: A solution via surrogate kernels. In ICML, 2013.\n\nDomain adaptation with conditional transferable components. M Gong, K Zhang, T Liu, D Tao, C Glymour, B Sch\u00f6lkopf, ICML. 48M. Gong, K. Zhang, T. Liu, D. Tao, C. Glymour, and B. Sch\u00f6lkopf. Domain adaptation with conditional transferable components. In ICML, volume 48, pages 2839-2848, 2016.\n\nGeodesic flow kernel for unsupervised domain adaptation. B Gong, Y Shi, F Sha, K Grauman, CVPR. B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In CVPR, 2012.\n\nUnsupervised domain adaptation by domain invariant projection. M Baktashmotlagh, M Harandi, B Lovell, M Salzmann, ICCV. M. Baktashmotlagh, M. Harandi, B. Lovell, and M. Salzmann. Unsupervised domain adaptation by domain invariant projection. In ICCV, pages 769-776, 2013.\n\nUnsupervised visual domain adaptation using subspace alignment. B Fernando, A Habrard, M Sebban, T Tuytelaars, ICCV. B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Unsupervised visual domain adaptation using subspace alignment. In ICCV, 2013.\n\nTransfer joint matching for unsupervised domain adaptation. M Long, J Wang, G Ding, J Sun, P Yu, CVPR. M. Long, J. Wang, G. Ding, J. Sun, and P. Yu. Transfer joint matching for unsupervised domain adaptation. In CVPR, pages 1410-1417, 2014.\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, ICML. Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML, pages 1180-1189, 2015.\n\nDomain-adversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, Journal of Machine Learning Research. 1759Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1-35, 2016.\n\nBregman divergence-based regularization for transfer subspace learning. S Si, D Tao, B Geng, IEEE Transactions on Knowledge and Data Engineering. 227S. Si, D. Tao, and B. Geng. Bregman divergence-based regularization for transfer subspace learning. IEEE Transactions on Knowledge and Data Engineering, 22(7):929-942, July 2010.\n\nDomain adaptation with regularized optimal transport. N Courty, R Flamary, D Tuia, ECML/PKDD. N. Courty, R. Flamary, and D. Tuia. Domain adaptation with regularized optimal transport. In ECML/PKDD, 2014.\n\nOptimal transport for domain adaptation. N Courty, R Flamary, D Tuia, A Rakotomamonjy, IEEE Transactions on Pattern Analysis and Machine Intelligence. N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016.\n\nOn the translocation of masses. L Kantorovich, C.R. (Doklady) Acad. Sci. URSS (N.S.). 37L. Kantorovich. On the translocation of masses. C.R. (Doklady) Acad. Sci. URSS (N.S.), 37:199-201, 1942.\n\nOptimal transport for applied mathematicians. F Santambrogio, Birk\u00e4user. F. Santambrogio. Optimal transport for applied mathematicians. Birk\u00e4user, NY, 2015.\n\nMapping estimation for discrete optimal transport. M Perrot, N Courty, R Flamary, A Habrard, NIPS. M. Perrot, N. Courty, R. Flamary, and A. Habrard. Mapping estimation for discrete optimal transport. In NIPS, pages 4197-4205, 2016.\n\nOptimal transport: old and new. Grund. der mathematischen Wissenschaften. C Villani, SpringerC. Villani. Optimal transport: old and new. Grund. der mathematischen Wissenschaften. Springer, 2009.\n\nAre loss functions all the same?. Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, Alessandro Verri, Neural Computation. 165Lorenzo Rosasco, Ernesto De Vito, Andrea Caponnetto, Michele Piana, and Alessandro Verri. Are loss functions all the same? Neural Computation, 16(5):1063-1076, 2004.\n\nA transportation l p distance for signal analysis. M Thorpe, S Park, S Kolouri, G Rohde, D Slepcev, abs/1609.08669CoRRM. Thorpe, S. Park, S. Kolouri, G. Rohde, and D. Slepcev. A transportation l p distance for signal analysis. CoRR, abs/1609.08669, 2016.\n\nAccess to unlabeled data can speed up prediction time. R Urner, S Shalev-Shwartz, S Ben-David, Proceedings of ICML. ICMLR. Urner, S. Shalev-Shwartz, and S. Ben-David. Access to unlabeled data can speed up prediction time. In Proceedings of ICML, pages 641-648, 2011.\n\nDomain adaptation-can quantity compensate for quality. S Ben-David, S Shalev-Shwartz, R Urner, Proc of ISAIM. of ISAIMS. Ben-David, S. Shalev-Shwartz, and R. Urner. Domain adaptation-can quantity compensate for quality? In Proc of ISAIM, 2012.\n\nDomain adaptation: Learning bounds and algorithms. Y Mansour, M Mohri, A Rostamizadeh, Proc. of COLT. of COLTY. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In Proc. of COLT, 2009.\n\nA theory of learning from different domains. S Ben-David, J Blitzer, K Crammer, A Kulesza, F Pereira, J. Wortman Vaughan, Machine Learning. 79S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman Vaughan. A theory of learning from different domains. Machine Learning, 79(1-2):151-175, 2010.\n\nSinkhorn distances: Lightspeed computation of optimal transport. M Cuturi, NIPS. M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS, 2013.\n\nStochastic optimization for large-scale optimal transport. A Genevay, M Cuturi, G Peyr\u00e9, F Bach, NIPS. A. Genevay, M. Cuturi, G. Peyr\u00e9, and F. Bach. Stochastic optimization for large-scale optimal transport. In NIPS, pages 3432-3440, 2016.\n\nOn the convergence of the block nonlinear gauss-seidel method under convex constraints. Luigi Grippo, Marco Sciandrone, Operations research letters. 263Luigi Grippo and Marco Sciandrone. On the convergence of the block nonlinear gauss-seidel method under convex constraints. Operations research letters, 26(3):127-136, 2000.\n\nAdapting visual category models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, ECCV. K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In ECCV, LNCS, pages 213-226, 2010.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, ICML. J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014.\n\nAdaptation regularization: A general framework for transfer learning. M Long, J Wang, G Ding, S Jialin Pan, P S Yu, IEEE TKDE. 267M. Long, J. Wang, G. Ding, S. Jialin Pan, and P.S. Yu. Adaptation regularization: A general framework for transfer learning. IEEE TKDE, 26(7):1076-1089, 2014.\n\nCross validation framework to choose amongst models and datasets for transfer learning. E Zhong, W Fan, Q Yang, O Verscheure, J Ren, ECML/PKDD. E. Zhong, W. Fan, Q. Yang, O. Verscheure, and J. Ren. Cross validation framework to choose amongst models and datasets for transfer learning. In ECML/PKDD, 2010.\n\nDomain adaptation with structural correspondence learning. J Blitzer, R Mcdonald, F Pereira, Proc. of the 2006 conference on empirical methods in natural language processing. of the 2006 conference on empirical methods in natural language processingJ. Blitzer, R. McDonald, and F. Pereira. Domain adaptation with structural correspondence learning. In Proc. of the 2006 conference on empirical methods in natural language processing, pages 120-128, 2006.\n\nMarginalized denoising autoencoders for domain adaptation. M Chen, Z Xu, K Weinberger, F Sha, ICML. M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized denoising autoencoders for domain adaptation. In ICML, 2012.\n\nMulti-source domain adaptation: A causal view. K Zhang, M Gong, B Sch\u00f6lkopf, AAAI Conference on Artificial Intelligence. K. Zhang, M. Gong, and B. Sch\u00f6lkopf. Multi-source domain adaptation: A causal view. In AAAI Conference on Artificial Intelligence, pages 3150-3157, 2015.\n\nQuantitative concentration inequalities for empirical measures on non-compact spaces. F Bolley, A Guillin, C Villani, Probability Theory and Related Fields. F. Bolley, A. Guillin, and C Villani. Quantitative concentration inequalities for empirical measures on non-compact spaces. Probability Theory and Related Fields, pages 541-593, 2007.\n\nLet L be any loss function symmetric, k-lipschitz and that satisfies the triangle inequality. Then, there exists, c and N , such that for N s > N and N t > N , for all \u03bb > 0, with \u03b1 = k\u03bb, we have with probability at least 1 \u2212 \u03b4: Note that c is such that \u00b5 verifies for any measure \u03bd the Talagrand (transport) inequality T 1 (c) : W 1 (\u00b5, \u03bd) \u2264 2 c H(\u03bd|\u00b5) with H is the relative entropy. T 1 (c) holds when for some \u03b1 > 0 and for any z. |f * (x 1 ) \u2212 f * (x 2 )| \u2264 M for all x 1 , x 2 \u2208 X 2. R d e \u03b1dist(z,z ) 2 d\u00b5(z) < \u221e, and c can be found explicitly [35|f * (x 1 ) \u2212 f * (x 2 )| \u2264 M for all x 1 , x 2 \u2208 X 2 . Let L be any loss function symmetric, k-lipschitz and that satisfies the triangle inequality. Then, there exists, c and N , such that for N s > N and N t > N , for all \u03bb > 0, with \u03b1 = k\u03bb, we have with probability at least 1 \u2212 \u03b4: Note that c is such that \u00b5 verifies for any measure \u03bd the Talagrand (transport) inequality T 1 (c) : W 1 (\u00b5, \u03bd) \u2264 2 c H(\u03bd|\u00b5) with H is the relative entropy. T 1 (c) holds when for some \u03b1 > 0 and for any z : R d e \u03b1dist(z,z ) 2 d\u00b5(z) < \u221e, and c can be found explicitly [35].\n", "annotations": {"author": "[{\"end\":101,\"start\":67},{\"end\":137,\"start\":102},{\"end\":187,\"start\":138},{\"end\":208,\"start\":188},{\"end\":296,\"start\":209},{\"end\":334,\"start\":297},{\"end\":432,\"start\":335}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":75},{\"end\":114,\"start\":107},{\"end\":152,\"start\":145},{\"end\":207,\"start\":194}]", "author_first_name": "[{\"end\":74,\"start\":67},{\"end\":106,\"start\":102},{\"end\":144,\"start\":138},{\"end\":193,\"start\":188}]", "author_affiliation": "[{\"end\":295,\"start\":210},{\"end\":333,\"start\":298},{\"end\":431,\"start\":336}]", "title": "[{\"end\":64,\"start\":1},{\"end\":496,\"start\":433}]", "venue": null, "abstract": "[{\"end\":1501,\"start\":498}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1828,\"start\":1825},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2323,\"start\":2320},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2966,\"start\":2963},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3057,\"start\":3054},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3186,\"start\":3183},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3442,\"start\":3439},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3444,\"start\":3442},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3446,\"start\":3444},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3448,\"start\":3446},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3450,\"start\":3448},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3473,\"start\":3470},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3539,\"start\":3535},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3542,\"start\":3539},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3646,\"start\":3642},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3678,\"start\":3675},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3680,\"start\":3678},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3877,\"start\":3873},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3880,\"start\":3877},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4848,\"start\":4845},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4850,\"start\":4848},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7702,\"start\":7698},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8338,\"start\":8334},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8454,\"start\":8450},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8457,\"start\":8454},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8460,\"start\":8457},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8603,\"start\":8599},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8712,\"start\":8708},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10005,\"start\":10001},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10128,\"start\":10124},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10221,\"start\":10217},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12701,\"start\":12697},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12704,\"start\":12701},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13471,\"start\":13467},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14595,\"start\":14591},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14598,\"start\":14595},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17192,\"start\":17188},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17195,\"start\":17192},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17610,\"start\":17607},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18684,\"start\":18680},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19584,\"start\":19580},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19612,\"start\":19608},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20734,\"start\":20730},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20945,\"start\":20942},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22536,\"start\":22532},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22940,\"start\":22936},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23104,\"start\":23100},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23199,\"start\":23195},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23266,\"start\":23262},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23802,\"start\":23798},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24382,\"start\":24378},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24468,\"start\":24464},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25002,\"start\":24998},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25005,\"start\":25002},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25473,\"start\":25469},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25868,\"start\":25864},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26732,\"start\":26728},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26899,\"start\":26896},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26928,\"start\":26925},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27426,\"start\":27423},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27428,\"start\":27426},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27921,\"start\":27918},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28007,\"start\":28004},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28078,\"start\":28074},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28199,\"start\":28196},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28249,\"start\":28246},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28251,\"start\":28249},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28547,\"start\":28544},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32348,\"start\":32344}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35467,\"start\":35165},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35821,\"start\":35468},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36150,\"start\":35822},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36365,\"start\":36151},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37409,\"start\":36366},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38374,\"start\":37410},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":39193,\"start\":38375},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39720,\"start\":39194}]", "paragraph": "[{\"end\":2418,\"start\":1517},{\"end\":4279,\"start\":2420},{\"end\":4851,\"start\":4281},{\"end\":5988,\"start\":4853},{\"end\":7018,\"start\":6029},{\"end\":7150,\"start\":7061},{\"end\":7264,\"start\":7209},{\"end\":7816,\"start\":7319},{\"end\":8339,\"start\":7877},{\"end\":8873,\"start\":8341},{\"end\":9261,\"start\":8919},{\"end\":10222,\"start\":9350},{\"end\":10601,\"start\":10224},{\"end\":10702,\"start\":10629},{\"end\":11179,\"start\":10771},{\"end\":12154,\"start\":11264},{\"end\":12594,\"start\":12210},{\"end\":13353,\"start\":12596},{\"end\":14100,\"start\":13355},{\"end\":14465,\"start\":14132},{\"end\":15068,\"start\":14467},{\"end\":15341,\"start\":15070},{\"end\":15611,\"start\":15412},{\"end\":15666,\"start\":15613},{\"end\":16040,\"start\":15815},{\"end\":16494,\"start\":16212},{\"end\":17470,\"start\":16602},{\"end\":18299,\"start\":17510},{\"end\":18820,\"start\":18378},{\"end\":19303,\"start\":18822},{\"end\":19613,\"start\":19305},{\"end\":19697,\"start\":19615},{\"end\":20411,\"start\":19751},{\"end\":20946,\"start\":20413},{\"end\":21197,\"start\":20948},{\"end\":21468,\"start\":21245},{\"end\":21972,\"start\":21470},{\"end\":22302,\"start\":22060},{\"end\":22478,\"start\":22328},{\"end\":24383,\"start\":22480},{\"end\":25335,\"start\":24385},{\"end\":26733,\"start\":25337},{\"end\":27775,\"start\":26735},{\"end\":28532,\"start\":27777},{\"end\":29446,\"start\":28534},{\"end\":31169,\"start\":29476},{\"end\":32709,\"start\":31227},{\"end\":33000,\"start\":32768},{\"end\":33404,\"start\":33002},{\"end\":33666,\"start\":33406},{\"end\":33851,\"start\":33668},{\"end\":33947,\"start\":33853},{\"end\":33999,\"start\":33949},{\"end\":34109,\"start\":34048},{\"end\":34180,\"start\":34111},{\"end\":35164,\"start\":34182}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7208,\"start\":7151},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7318,\"start\":7265},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7876,\"start\":7817},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9349,\"start\":9262},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10628,\"start\":10602},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10770,\"start\":10703},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11263,\"start\":11180},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12209,\"start\":12155},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15411,\"start\":15342},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15814,\"start\":15667},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16211,\"start\":16041},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16601,\"start\":16495},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18377,\"start\":18300},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19750,\"start\":19698},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21244,\"start\":21198},{\"attributes\":{\"id\":\"formula_15\"},\"end\":22059,\"start\":21973},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34047,\"start\":34000}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23874,\"start\":23867},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28925,\"start\":28918},{\"end\":33022,\"start\":33015}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1515,\"start\":1503},{\"attributes\":{\"n\":\"2\"},\"end\":6027,\"start\":5991},{\"attributes\":{\"n\":\"2.1\"},\"end\":7059,\"start\":7021},{\"attributes\":{\"n\":\"2.2\"},\"end\":8917,\"start\":8876},{\"attributes\":{\"n\":\"3\"},\"end\":14130,\"start\":14103},{\"attributes\":{\"n\":\"4\"},\"end\":17508,\"start\":17473},{\"attributes\":{\"n\":\"5\"},\"end\":22326,\"start\":22305},{\"attributes\":{\"n\":\"6\"},\"end\":29474,\"start\":29449},{\"end\":31188,\"start\":31172},{\"end\":31225,\"start\":31191},{\"end\":32766,\"start\":32712},{\"end\":35176,\"start\":35166},{\"end\":35833,\"start\":35823},{\"end\":36153,\"start\":36152},{\"end\":36376,\"start\":36367},{\"end\":37420,\"start\":37411},{\"end\":38385,\"start\":38376}]", "table": "[{\"end\":37409,\"start\":36437},{\"end\":38374,\"start\":37491},{\"end\":39193,\"start\":38475},{\"end\":39720,\"start\":39352}]", "figure_caption": "[{\"end\":35467,\"start\":35178},{\"end\":35821,\"start\":35470},{\"end\":36150,\"start\":35835},{\"end\":36365,\"start\":36154},{\"end\":36437,\"start\":36378},{\"end\":37491,\"start\":37422},{\"end\":38475,\"start\":38387},{\"end\":39352,\"start\":39196}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11697,\"start\":11689},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31302,\"start\":31293},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31681,\"start\":31673},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32075,\"start\":32067}]", "bib_author_first_name": "[{\"end\":43221,\"start\":43220},{\"end\":43223,\"start\":43222},{\"end\":43232,\"start\":43231},{\"end\":43243,\"start\":43242},{\"end\":43249,\"start\":43248},{\"end\":43484,\"start\":43483},{\"end\":43486,\"start\":43485},{\"end\":43493,\"start\":43492},{\"end\":43791,\"start\":43790},{\"end\":43803,\"start\":43802},{\"end\":43815,\"start\":43814},{\"end\":43826,\"start\":43825},{\"end\":43828,\"start\":43827},{\"end\":43838,\"start\":43837},{\"end\":44109,\"start\":44108},{\"end\":44118,\"start\":44117},{\"end\":44120,\"start\":44119},{\"end\":44129,\"start\":44128},{\"end\":44137,\"start\":44136},{\"end\":44139,\"start\":44138},{\"end\":44147,\"start\":44146},{\"end\":44155,\"start\":44154},{\"end\":44383,\"start\":44382},{\"end\":44391,\"start\":44390},{\"end\":44400,\"start\":44399},{\"end\":44407,\"start\":44406},{\"end\":44414,\"start\":44413},{\"end\":44425,\"start\":44424},{\"end\":44672,\"start\":44671},{\"end\":44680,\"start\":44679},{\"end\":44687,\"start\":44686},{\"end\":44694,\"start\":44693},{\"end\":44888,\"start\":44887},{\"end\":44906,\"start\":44905},{\"end\":44917,\"start\":44916},{\"end\":44927,\"start\":44926},{\"end\":45162,\"start\":45161},{\"end\":45174,\"start\":45173},{\"end\":45185,\"start\":45184},{\"end\":45195,\"start\":45194},{\"end\":45410,\"start\":45409},{\"end\":45418,\"start\":45417},{\"end\":45426,\"start\":45425},{\"end\":45434,\"start\":45433},{\"end\":45441,\"start\":45440},{\"end\":45643,\"start\":45642},{\"end\":45652,\"start\":45651},{\"end\":45830,\"start\":45829},{\"end\":45839,\"start\":45838},{\"end\":45851,\"start\":45850},{\"end\":45861,\"start\":45860},{\"end\":45872,\"start\":45871},{\"end\":45886,\"start\":45885},{\"end\":45900,\"start\":45899},{\"end\":45912,\"start\":45911},{\"end\":46252,\"start\":46251},{\"end\":46258,\"start\":46257},{\"end\":46265,\"start\":46264},{\"end\":46563,\"start\":46562},{\"end\":46573,\"start\":46572},{\"end\":46584,\"start\":46583},{\"end\":46755,\"start\":46754},{\"end\":46765,\"start\":46764},{\"end\":46776,\"start\":46775},{\"end\":46784,\"start\":46783},{\"end\":47063,\"start\":47062},{\"end\":47271,\"start\":47270},{\"end\":47434,\"start\":47433},{\"end\":47444,\"start\":47443},{\"end\":47454,\"start\":47453},{\"end\":47465,\"start\":47464},{\"end\":47690,\"start\":47689},{\"end\":47852,\"start\":47845},{\"end\":47869,\"start\":47862},{\"end\":47872,\"start\":47870},{\"end\":47885,\"start\":47879},{\"end\":47905,\"start\":47898},{\"end\":47923,\"start\":47913},{\"end\":48173,\"start\":48172},{\"end\":48183,\"start\":48182},{\"end\":48191,\"start\":48190},{\"end\":48202,\"start\":48201},{\"end\":48211,\"start\":48210},{\"end\":48433,\"start\":48432},{\"end\":48442,\"start\":48441},{\"end\":48460,\"start\":48459},{\"end\":48701,\"start\":48700},{\"end\":48714,\"start\":48713},{\"end\":48732,\"start\":48731},{\"end\":48942,\"start\":48941},{\"end\":48953,\"start\":48952},{\"end\":48962,\"start\":48961},{\"end\":49164,\"start\":49163},{\"end\":49177,\"start\":49176},{\"end\":49188,\"start\":49187},{\"end\":49199,\"start\":49198},{\"end\":49210,\"start\":49209},{\"end\":49230,\"start\":49220},{\"end\":49499,\"start\":49498},{\"end\":49666,\"start\":49665},{\"end\":49677,\"start\":49676},{\"end\":49687,\"start\":49686},{\"end\":49696,\"start\":49695},{\"end\":49940,\"start\":49935},{\"end\":49954,\"start\":49949},{\"end\":50222,\"start\":50221},{\"end\":50232,\"start\":50231},{\"end\":50241,\"start\":50240},{\"end\":50250,\"start\":50249},{\"end\":50478,\"start\":50477},{\"end\":50489,\"start\":50488},{\"end\":50496,\"start\":50495},{\"end\":50507,\"start\":50506},{\"end\":50518,\"start\":50517},{\"end\":50527,\"start\":50526},{\"end\":50536,\"start\":50535},{\"end\":50798,\"start\":50797},{\"end\":50806,\"start\":50805},{\"end\":50814,\"start\":50813},{\"end\":50822,\"start\":50821},{\"end\":50836,\"start\":50835},{\"end\":50838,\"start\":50837},{\"end\":51106,\"start\":51105},{\"end\":51115,\"start\":51114},{\"end\":51122,\"start\":51121},{\"end\":51130,\"start\":51129},{\"end\":51144,\"start\":51143},{\"end\":51384,\"start\":51383},{\"end\":51395,\"start\":51394},{\"end\":51407,\"start\":51406},{\"end\":51840,\"start\":51839},{\"end\":51848,\"start\":51847},{\"end\":51854,\"start\":51853},{\"end\":51868,\"start\":51867},{\"end\":52046,\"start\":52045},{\"end\":52055,\"start\":52054},{\"end\":52063,\"start\":52062},{\"end\":52361,\"start\":52360},{\"end\":52371,\"start\":52370},{\"end\":52382,\"start\":52381}]", "bib_author_last_name": "[{\"end\":43229,\"start\":43224},{\"end\":43240,\"start\":43233},{\"end\":43246,\"start\":43244},{\"end\":43259,\"start\":43250},{\"end\":43490,\"start\":43487},{\"end\":43498,\"start\":43494},{\"end\":43800,\"start\":43792},{\"end\":43812,\"start\":43804},{\"end\":43823,\"start\":43816},{\"end\":43835,\"start\":43829},{\"end\":43847,\"start\":43839},{\"end\":44115,\"start\":44110},{\"end\":44126,\"start\":44121},{\"end\":44134,\"start\":44130},{\"end\":44144,\"start\":44140},{\"end\":44152,\"start\":44148},{\"end\":44162,\"start\":44156},{\"end\":44388,\"start\":44384},{\"end\":44397,\"start\":44392},{\"end\":44404,\"start\":44401},{\"end\":44411,\"start\":44408},{\"end\":44422,\"start\":44415},{\"end\":44435,\"start\":44426},{\"end\":44677,\"start\":44673},{\"end\":44684,\"start\":44681},{\"end\":44691,\"start\":44688},{\"end\":44702,\"start\":44695},{\"end\":44903,\"start\":44889},{\"end\":44914,\"start\":44907},{\"end\":44924,\"start\":44918},{\"end\":44936,\"start\":44928},{\"end\":45171,\"start\":45163},{\"end\":45182,\"start\":45175},{\"end\":45192,\"start\":45186},{\"end\":45206,\"start\":45196},{\"end\":45415,\"start\":45411},{\"end\":45423,\"start\":45419},{\"end\":45431,\"start\":45427},{\"end\":45438,\"start\":45435},{\"end\":45444,\"start\":45442},{\"end\":45649,\"start\":45644},{\"end\":45662,\"start\":45653},{\"end\":45836,\"start\":45831},{\"end\":45848,\"start\":45840},{\"end\":45858,\"start\":45852},{\"end\":45869,\"start\":45862},{\"end\":45883,\"start\":45873},{\"end\":45897,\"start\":45887},{\"end\":45909,\"start\":45901},{\"end\":45922,\"start\":45913},{\"end\":46255,\"start\":46253},{\"end\":46262,\"start\":46259},{\"end\":46270,\"start\":46266},{\"end\":46570,\"start\":46564},{\"end\":46581,\"start\":46574},{\"end\":46589,\"start\":46585},{\"end\":46762,\"start\":46756},{\"end\":46773,\"start\":46766},{\"end\":46781,\"start\":46777},{\"end\":46798,\"start\":46785},{\"end\":47075,\"start\":47064},{\"end\":47284,\"start\":47272},{\"end\":47441,\"start\":47435},{\"end\":47451,\"start\":47445},{\"end\":47462,\"start\":47455},{\"end\":47473,\"start\":47466},{\"end\":47698,\"start\":47691},{\"end\":47860,\"start\":47853},{\"end\":47877,\"start\":47873},{\"end\":47896,\"start\":47886},{\"end\":47911,\"start\":47906},{\"end\":47929,\"start\":47924},{\"end\":48180,\"start\":48174},{\"end\":48188,\"start\":48184},{\"end\":48199,\"start\":48192},{\"end\":48208,\"start\":48203},{\"end\":48219,\"start\":48212},{\"end\":48439,\"start\":48434},{\"end\":48457,\"start\":48443},{\"end\":48470,\"start\":48461},{\"end\":48711,\"start\":48702},{\"end\":48729,\"start\":48715},{\"end\":48738,\"start\":48733},{\"end\":48950,\"start\":48943},{\"end\":48959,\"start\":48954},{\"end\":48975,\"start\":48963},{\"end\":49174,\"start\":49165},{\"end\":49185,\"start\":49178},{\"end\":49196,\"start\":49189},{\"end\":49207,\"start\":49200},{\"end\":49218,\"start\":49211},{\"end\":49238,\"start\":49231},{\"end\":49506,\"start\":49500},{\"end\":49674,\"start\":49667},{\"end\":49684,\"start\":49678},{\"end\":49693,\"start\":49688},{\"end\":49701,\"start\":49697},{\"end\":49947,\"start\":49941},{\"end\":49965,\"start\":49955},{\"end\":50229,\"start\":50223},{\"end\":50238,\"start\":50233},{\"end\":50247,\"start\":50242},{\"end\":50258,\"start\":50251},{\"end\":50486,\"start\":50479},{\"end\":50493,\"start\":50490},{\"end\":50504,\"start\":50497},{\"end\":50515,\"start\":50508},{\"end\":50524,\"start\":50519},{\"end\":50533,\"start\":50528},{\"end\":50544,\"start\":50537},{\"end\":50803,\"start\":50799},{\"end\":50811,\"start\":50807},{\"end\":50819,\"start\":50815},{\"end\":50833,\"start\":50823},{\"end\":50841,\"start\":50839},{\"end\":51112,\"start\":51107},{\"end\":51119,\"start\":51116},{\"end\":51127,\"start\":51123},{\"end\":51141,\"start\":51131},{\"end\":51148,\"start\":51145},{\"end\":51392,\"start\":51385},{\"end\":51404,\"start\":51396},{\"end\":51415,\"start\":51408},{\"end\":51845,\"start\":51841},{\"end\":51851,\"start\":51849},{\"end\":51865,\"start\":51855},{\"end\":51872,\"start\":51869},{\"end\":52052,\"start\":52047},{\"end\":52060,\"start\":52056},{\"end\":52073,\"start\":52064},{\"end\":52368,\"start\":52362},{\"end\":52379,\"start\":52372},{\"end\":52390,\"start\":52383}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":43450,\"start\":43162},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":740063},\"end\":43687,\"start\":43452},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9133542},\"end\":44038,\"start\":43689},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1592117},\"end\":44320,\"start\":44040},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5807252},\"end\":44612,\"start\":44322},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6742009},\"end\":44822,\"start\":44614},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":12271421},\"end\":45095,\"start\":44824},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9440223},\"end\":45347,\"start\":45097},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":407822},\"end\":45589,\"start\":45349},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6755881},\"end\":45779,\"start\":45591},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2871880},\"end\":46177,\"start\":45781},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206742596},\"end\":46506,\"start\":46179},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":18945224},\"end\":46711,\"start\":46508},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13347901},\"end\":47028,\"start\":46713},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":122853046},\"end\":47222,\"start\":47030},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":123894440},\"end\":47380,\"start\":47224},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11198605},\"end\":47613,\"start\":47382},{\"attributes\":{\"id\":\"b17\"},\"end\":47809,\"start\":47615},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":11845688},\"end\":48119,\"start\":47811},{\"attributes\":{\"doi\":\"abs/1609.08669\",\"id\":\"b19\"},\"end\":48375,\"start\":48121},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6166696},\"end\":48643,\"start\":48377},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2431408},\"end\":48888,\"start\":48645},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6178817},\"end\":49116,\"start\":48890},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8577357},\"end\":49431,\"start\":49118},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15966283},\"end\":49604,\"start\":49433},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6849824},\"end\":49845,\"start\":49606},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":12714392},\"end\":50171,\"start\":49847},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7534823},\"end\":50396,\"start\":50173},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6161478},\"end\":50725,\"start\":50398},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1990988},\"end\":51015,\"start\":50727},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13322062},\"end\":51322,\"start\":51017},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15978939},\"end\":51778,\"start\":51324},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10686834},\"end\":51996,\"start\":51780},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":10115836},\"end\":52272,\"start\":51998},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":17451427},\"end\":52614,\"start\":52274},{\"attributes\":{\"id\":\"b35\"},\"end\":53728,\"start\":52616}]", "bib_title": "[{\"end\":43218,\"start\":43162},{\"end\":43481,\"start\":43452},{\"end\":43788,\"start\":43689},{\"end\":44106,\"start\":44040},{\"end\":44380,\"start\":44322},{\"end\":44669,\"start\":44614},{\"end\":44885,\"start\":44824},{\"end\":45159,\"start\":45097},{\"end\":45407,\"start\":45349},{\"end\":45640,\"start\":45591},{\"end\":45827,\"start\":45781},{\"end\":46249,\"start\":46179},{\"end\":46560,\"start\":46508},{\"end\":46752,\"start\":46713},{\"end\":47060,\"start\":47030},{\"end\":47268,\"start\":47224},{\"end\":47431,\"start\":47382},{\"end\":47843,\"start\":47811},{\"end\":48430,\"start\":48377},{\"end\":48698,\"start\":48645},{\"end\":48939,\"start\":48890},{\"end\":49161,\"start\":49118},{\"end\":49496,\"start\":49433},{\"end\":49663,\"start\":49606},{\"end\":49933,\"start\":49847},{\"end\":50219,\"start\":50173},{\"end\":50475,\"start\":50398},{\"end\":50795,\"start\":50727},{\"end\":51103,\"start\":51017},{\"end\":51381,\"start\":51324},{\"end\":51837,\"start\":51780},{\"end\":52043,\"start\":51998},{\"end\":52358,\"start\":52274},{\"end\":53049,\"start\":52616}]", "bib_author": "[{\"end\":43231,\"start\":43220},{\"end\":43242,\"start\":43231},{\"end\":43248,\"start\":43242},{\"end\":43261,\"start\":43248},{\"end\":43492,\"start\":43483},{\"end\":43500,\"start\":43492},{\"end\":43802,\"start\":43790},{\"end\":43814,\"start\":43802},{\"end\":43825,\"start\":43814},{\"end\":43837,\"start\":43825},{\"end\":43849,\"start\":43837},{\"end\":44117,\"start\":44108},{\"end\":44128,\"start\":44117},{\"end\":44136,\"start\":44128},{\"end\":44146,\"start\":44136},{\"end\":44154,\"start\":44146},{\"end\":44164,\"start\":44154},{\"end\":44390,\"start\":44382},{\"end\":44399,\"start\":44390},{\"end\":44406,\"start\":44399},{\"end\":44413,\"start\":44406},{\"end\":44424,\"start\":44413},{\"end\":44437,\"start\":44424},{\"end\":44679,\"start\":44671},{\"end\":44686,\"start\":44679},{\"end\":44693,\"start\":44686},{\"end\":44704,\"start\":44693},{\"end\":44905,\"start\":44887},{\"end\":44916,\"start\":44905},{\"end\":44926,\"start\":44916},{\"end\":44938,\"start\":44926},{\"end\":45173,\"start\":45161},{\"end\":45184,\"start\":45173},{\"end\":45194,\"start\":45184},{\"end\":45208,\"start\":45194},{\"end\":45417,\"start\":45409},{\"end\":45425,\"start\":45417},{\"end\":45433,\"start\":45425},{\"end\":45440,\"start\":45433},{\"end\":45446,\"start\":45440},{\"end\":45651,\"start\":45642},{\"end\":45664,\"start\":45651},{\"end\":45838,\"start\":45829},{\"end\":45850,\"start\":45838},{\"end\":45860,\"start\":45850},{\"end\":45871,\"start\":45860},{\"end\":45885,\"start\":45871},{\"end\":45899,\"start\":45885},{\"end\":45911,\"start\":45899},{\"end\":45924,\"start\":45911},{\"end\":46257,\"start\":46251},{\"end\":46264,\"start\":46257},{\"end\":46272,\"start\":46264},{\"end\":46572,\"start\":46562},{\"end\":46583,\"start\":46572},{\"end\":46591,\"start\":46583},{\"end\":46764,\"start\":46754},{\"end\":46775,\"start\":46764},{\"end\":46783,\"start\":46775},{\"end\":46800,\"start\":46783},{\"end\":47077,\"start\":47062},{\"end\":47286,\"start\":47270},{\"end\":47443,\"start\":47433},{\"end\":47453,\"start\":47443},{\"end\":47464,\"start\":47453},{\"end\":47475,\"start\":47464},{\"end\":47700,\"start\":47689},{\"end\":47862,\"start\":47845},{\"end\":47879,\"start\":47862},{\"end\":47898,\"start\":47879},{\"end\":47913,\"start\":47898},{\"end\":47931,\"start\":47913},{\"end\":48182,\"start\":48172},{\"end\":48190,\"start\":48182},{\"end\":48201,\"start\":48190},{\"end\":48210,\"start\":48201},{\"end\":48221,\"start\":48210},{\"end\":48441,\"start\":48432},{\"end\":48459,\"start\":48441},{\"end\":48472,\"start\":48459},{\"end\":48713,\"start\":48700},{\"end\":48731,\"start\":48713},{\"end\":48740,\"start\":48731},{\"end\":48952,\"start\":48941},{\"end\":48961,\"start\":48952},{\"end\":48977,\"start\":48961},{\"end\":49176,\"start\":49163},{\"end\":49187,\"start\":49176},{\"end\":49198,\"start\":49187},{\"end\":49209,\"start\":49198},{\"end\":49220,\"start\":49209},{\"end\":49240,\"start\":49220},{\"end\":49508,\"start\":49498},{\"end\":49676,\"start\":49665},{\"end\":49686,\"start\":49676},{\"end\":49695,\"start\":49686},{\"end\":49703,\"start\":49695},{\"end\":49949,\"start\":49935},{\"end\":49967,\"start\":49949},{\"end\":50231,\"start\":50221},{\"end\":50240,\"start\":50231},{\"end\":50249,\"start\":50240},{\"end\":50260,\"start\":50249},{\"end\":50488,\"start\":50477},{\"end\":50495,\"start\":50488},{\"end\":50506,\"start\":50495},{\"end\":50517,\"start\":50506},{\"end\":50526,\"start\":50517},{\"end\":50535,\"start\":50526},{\"end\":50546,\"start\":50535},{\"end\":50805,\"start\":50797},{\"end\":50813,\"start\":50805},{\"end\":50821,\"start\":50813},{\"end\":50835,\"start\":50821},{\"end\":50843,\"start\":50835},{\"end\":51114,\"start\":51105},{\"end\":51121,\"start\":51114},{\"end\":51129,\"start\":51121},{\"end\":51143,\"start\":51129},{\"end\":51150,\"start\":51143},{\"end\":51394,\"start\":51383},{\"end\":51406,\"start\":51394},{\"end\":51417,\"start\":51406},{\"end\":51847,\"start\":51839},{\"end\":51853,\"start\":51847},{\"end\":51867,\"start\":51853},{\"end\":51874,\"start\":51867},{\"end\":52054,\"start\":52045},{\"end\":52062,\"start\":52054},{\"end\":52075,\"start\":52062},{\"end\":52370,\"start\":52360},{\"end\":52381,\"start\":52370},{\"end\":52392,\"start\":52381}]", "bib_venue": "[{\"end\":48497,\"start\":48493},{\"end\":48763,\"start\":48755},{\"end\":48999,\"start\":48992},{\"end\":51573,\"start\":51499},{\"end\":43292,\"start\":43261},{\"end\":43551,\"start\":43500},{\"end\":43853,\"start\":43849},{\"end\":44168,\"start\":44164},{\"end\":44441,\"start\":44437},{\"end\":44708,\"start\":44704},{\"end\":44942,\"start\":44938},{\"end\":45212,\"start\":45208},{\"end\":45450,\"start\":45446},{\"end\":45668,\"start\":45664},{\"end\":45960,\"start\":45924},{\"end\":46323,\"start\":46272},{\"end\":46600,\"start\":46591},{\"end\":46862,\"start\":46800},{\"end\":47114,\"start\":47077},{\"end\":47295,\"start\":47286},{\"end\":47479,\"start\":47475},{\"end\":47687,\"start\":47615},{\"end\":47949,\"start\":47931},{\"end\":48170,\"start\":48121},{\"end\":48491,\"start\":48472},{\"end\":48753,\"start\":48740},{\"end\":48990,\"start\":48977},{\"end\":49256,\"start\":49240},{\"end\":49512,\"start\":49508},{\"end\":49707,\"start\":49703},{\"end\":49994,\"start\":49967},{\"end\":50264,\"start\":50260},{\"end\":50550,\"start\":50546},{\"end\":50852,\"start\":50843},{\"end\":51159,\"start\":51150},{\"end\":51497,\"start\":51417},{\"end\":51878,\"start\":51874},{\"end\":52117,\"start\":52075},{\"end\":52429,\"start\":52392},{\"end\":53104,\"start\":53051}]"}}}, "year": 2023, "month": 12, "day": 17}