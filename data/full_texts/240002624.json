{"id": 240002624, "updated": "2022-09-29 23:08:09.445", "metadata": {"title": "YOLO-FIRI: Improved YOLOv5 for Infrared Image Object Detection", "authors": "[{\"first\":\"Shasha\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yongjun\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Yao\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Mengjun\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xiaorong\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "IEEE Access", "journal": "IEEE Access", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "To solve object detection issues in infrared images, such as a low recognition rate and a high false alarm rate caused by long-distance, weak energy, and low resolution, we propose a region-free object detector named YOLO-FIR for the infrared (IR) image with the core of YOLOv5 by compressing channels, optimizing parameters, etc. And an improved infrared image object detection network, YOLO-FIRI, is further developed. Specifically, while designing the feature extraction network, the cross-stage-partial- connections (CSP) module in the shallow layer is expanded and iterated to maximize the use of shallow features. In addition, an improved attention module is introduced in residual blocks to focus on objects and suppress background. Moreover, multiscale detection is added to improve the detection accuracy of small objects. Experimental results on the KAIST and FLIR datasets show that YOLO-FIRI demonstrates a qualitative improvement compared with the state-of-the-art detectors. Compared with YOLOv4, the mean average precision (mAP50) of YOLO-FIRI is increased by 21% on the KAIST dataset, the speed is reduced by 62%, the parameters are decreased by 89%, the weight size is reduced by more than 94%, and the computational costs are reduced by 84%. Compared with YOLO-FIR, YOLO-FIRI has an approximately 5% to 20% improvement in AP, AR (average recall), mAP50, F1, mAP50:75, etc. Furthermore, due to the shortcomings of high noise and weak features, image fusion can be applied to the image preprocessing as a data enhancement method by fusing visible and infrared images based on a convolutional neural network.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/access/LiLLLX21", "doi": "10.1109/access.2021.3120870"}}, "content": {"source": {"pdf_hash": "571ac4abdc1f16e985a7b7d2de490d609fd09699", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://doi.org/10.1109/access.2021.3120870", "status": "GOLD"}}, "grobid": {"id": "0f8e99d8fb4d8aae819e337c1d3e9cb5f37d60bb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/571ac4abdc1f16e985a7b7d2de490d609fd09699.txt", "contents": "\n\n\n\n\nSchool of Physics and Electronics\nHenan University\n475004KaifengChina\n\n\nSchool of Computer and Electrical Engineering\nHunan University of Arts and Science\n415000ChangdeChina\n\n\n\nI. INTRODUCTION\n\nObject detection in infrared images has received extensive attention within the field of computer vision due to its important research value for applications. It also occupies an irreplaceable position in many fields, such as in diagnosing diseased cells [1], video surveillance [2], drone cruise [3], infrared warning [4], infrared night vision [5], infrared guidance [6], and other civilian and military fields. Although object detection models have achieved promising results in various tasks, it is still a challenging task in infrared images, as most models only focus on visible images.\n\nObtained through thermal radiation, infrared images have outstanding characteristics, such as object detection from long distances, high concealment, and availability both in the daytime and nighttime. With the expansion of distance imaging, the ever-growing demands for intelligent object detection in infrared images have become more urgent. However, constructing models for infrared images to achieve the desired results has been restricted by the longer imaging wavelength, larger noise, poorer spatial resolution, and more sensitivity to temperature changes in the environment compared with those characteristics of visible images. In recent years, several studies have investigated the possibility of object detection in infrared images using methods, such as spatial filtering [7], frequency domain filtering [8], and sparse representation [9]. However, these traditional object detection methods in infrared images are restricted by a single application scenario. They also have a slow recognition speed and a weak generalizability, making them difficult to use for fully extracting important features when applied to multi-scene and real-time detection applications.\n\nConvolutional neural networks (CNN) models have the ability to learn the deep features of the input. The high-level data features from the original pixels of the training data can be learned to obtain a better feature expression capability for complex context information. Some networks have greatly improved their accuracy and generalize ability [10], and have solved the problems of image classification [11], image segmentation [12], superresolution [13], etc. Examples include region-based two-stage object detection algorithms, such as R-CNN [14], Fast R-CNN [15], and Faster R-CNN [16], as well as region-free one-stage object detection algorithms in the SSD (Single Shot Multi-box Detector) series [17], [18] and the YOLO series [19]- [22]. In comparison to visible images, the lower signal-to-noise ratio of infrared images makes the objects easier to submerge and interfere; the complex background of infrared images makes the object areas dark and uneven; and the distance between objects and infrared sensors is relatively long, making the objects occupy small areas of the whole image. This shows that the CNN models are robust when applied to visible images. Despite the effectiveness of these studies, using a convolutional neural network to detect weak and small objects in infrared images has become a difficult and hot research topic.\n\nBased on the characteristics of infrared objects, we propose an object detection algorithm named YOLO-FIR for infrared images based on the region-free detector YOLOv5 [23]. As a state-of-the-art detector, YOLOv5 has the advantages of fast convergence, high precision, and strong customization. It also has strong real-time processing capabilities and low hardware computing requirements, meaning that it can be easily transplanted to mobile devices. These advantages are very helpful for ensuring the detection accuracy of the object in infrared images. Then, we make further improvements and propose a novel detection model, YOLO-FIRI, for small and weak objects in infrared images. The model proves to be reliable and efficient for object detection in infrared images. In the design of the YOLO-FIRI network, the CSP [24] module in the backbone network is extended to focus on shallow information and extract features to the maximum, while the feature extraction module is iterated to extract the detailed information and the deep features more thoroughly. Meanwhile, the SK (Select Kernel) [25] attention module is introduced and improved in residual blocks, and the features are re-weighted and fused from the channel dimension. In the detection stage, to better detect small and weak objects, multiscale feature detection is improved. Fourscale feature maps are used to detect multiscale objects, especially to enhance the detection of small objects. Additionally, to evaluate the contribution of each part of the network model designed in this paper, we conduct an ablation experiment on the KAIST [26] infrared pedestrian dataset. Finally, in the experimental analysis, the image fusion method is adopted to realize data enhancement by using the Densefuse [27] network to fuse visible and infrared images. We also use another dataset, FLIR [28], to further evaluate the performance. Experimental results demonstrate that the presented model improves the detection accuracy and ensures real-time detection speed in infrared images.\n\nThe main contributions of the research can be summarized as follows: 1. Based on studying the unique features of infrared images, this paper proposes the YOLO-FIR method for infrared objects with the core of YOLOv5 by analyzing the network structure, compressing channels, optimizing parameters, etc. Further improvements are made to design a novel network, YOLO-FIRI, where the feature extraction network is designed for complete use of the shallow features, and the detection head network has four layers to focus on small and weak objects. 2. Our designed feature extraction network extends and iterates the shallow CSP module, which uses an improved attention module, forcing the network to pay more attention to the shallow and detailed features in infrared images, as well as make the model more robust by learning more distinguishable features. 3. In image preprocessing, we use convolutional neural network, Densefuse network, to fuse visible and infrared images, which can realize data enhancement. Thus, image fusion can be used as a data enhancement method to enhance the features of infrared images.\n\n\nII. RELATED WORK\n\nInfrared image object detection has the advantage of not being disturbed by the environment, and it is a research hotspot in the field of object detection [29]. Currently, infrared image object detection approaches have been divided into two types: traditional algorithms and CNN models.\n\n\nA. TRADITIONAL INFRARED OBJECT DETECTION\n\nTraditional infrared object detection methods consider infrared images as three parts: object, background, and noise in the images. The idea is to suppress background and noise, thus, strengthening the object to achieve object detection by using various methods. K. Zhao et al. [7] first used the detection method based on spatial filtering for infrared object detection. In terms of different gray values of object and background, the background is selected and suppressed, and thus the object is detected. However, this method allowed all isolated noise points of small objects to pass, leading to a low detection rate. To address this problem, T.S. Anju [8] used the frequency difference between the object and background to separate the high-frequency part and the low-frequency part to achieve the detection task. Compared with that of spatial filtering methods, the detection effect of frequency domain filtering is a substantial promotion, but incurs high computational complexity. P. Jiao et al. [9] adopted sparse representation to cast the principle of infrared object detection in the form of a low-rank matrix and sparse matrix recovery, thus, achieving object segmentation and detection. The performance on the signal-clutter ratio (BCR) and background suppression factor (BSF) is much better than that of the filtering methods, but the nonlocal autocorrelation of the infrared image background cannot be used well, which leads to the lack of a background suppression effect. As the BCR decreases, the image background becomes increasingly complex. Enormous computational costs do not justify applicating the above models in real-time detection.\n\nTraditional infrared pedestrian detection methods use artificially designed feature extractors, such as Haar [30], histogram of oriented gradients (HOG) [31], or aggregate channel features (ACF) [32] to extract the features of objects. Next, it takes advantage of the sliding window to extract local features, and then use support vector machine (SVM) [33] or AdaBoost [34] to determine whether there is an object in the region. Unfortunately, these infrared object detection algorithms have strong pertinence, a high time complexity, and window redundancy. They are also not robust to the changes in object diversity.\n\n\nB. NEURAL NETWORK OBJECT DETECTION ALGORITHMS\n\nDue to the improvement in computability and the widespread use of infrared imaging system equipment, many datasets are released to the public, such as KAIST [26], FLIR [28], and OTCBVS [35], which prompts deep learning to be gradually applied in the field of infrared image object detection. Thanks to the strong capablity of feature expression, the CNN models open new horizons and create a large amount of excitement in object detection. They preserve the neighborhood relations and spatial locality of the input in their latent higher-level feature representations. Additionally, the number of free parameters describing their shared weights does not depend on the input dimensionality, meaning that the CNN can scale well to realistic-sized high-dimensional images in terms of computational complexity. The object detection networks mainly include two-stage detectors and one-stage detectors [36].\n\nIn 2014, R. Girshick et al. presented a pioneering twostage object detector, R-CNN [14], and the main idea was to divide object detection into two steps: to generate proposals and predict objects. However, the computation was not shared and was extremely time-consuming. To accelerate inference speed and achieve better detection accuracy, Fast R-CNN [15] and Faster R-CNN [16] were developed by using ROI (Region of Interest) pooling and a novel proposal generator, RPN (Region Proposal Network), respectively. Currently, there are many network model variants based on Faster R-CNN for solving different problems, such as R-FCN [37], Mask R-CNN [12], and Sparse R-CNN [38]. In the field of infrared image object detection, D. Ghose et al. [39] proposed a method with a few modifications based on classical two-stage detector Faster R-CNN, which used the corresponding saliency maps to enhance the infrared images. However, since the process of training the saliency network was not added to the Faster R-CNN, the non end-to-end multitask training was very time-consuming. C. Devaguptapu et al. [40] developed a multimodel Faster R-CNN to obtain high-level infrared features through RGB channels, but the multimodel undoubtedly increased the inference speed of training. J. Park et al. [4] developed a CNN-based human detection method for infrared images. The performance was improved by performing pixelwise segmentation and making fine-grained predictions, whereas the proposed method lacked generality for the different datasets. Practically speaking, two-stage detectors have difficulty achieving real-time inference.\n\nTo address the issue of two-stage detectors, J. Redmon et al. [19] proposed a region-free one-stage object detection algorithm, YOLO, in 2016 that divided the image into grid cells and considered each cell as a proposal to detect the object. Compared with Faster R-CNN, YOLO omitted proposal generation and achieved end-to-end detection, which could realize real-time detection. Subsequently, the greatly improved detection speed gives rise to extensive research, such as SSD [17], YOLOv3 [21], and YOLOv4 [22]. To detect infrared objects, M. Kristo et al. [41] used the one-stage detector YOLOv3 to detect persons at night in different weather conditions. YOLOv3 is faster than the two-stage detector Faster R-CNN. However, YOLOv3 easily misses small objects, so its detection accuracy is very low. M. Li et al. proposed SE-YOLO [42], a real-time pedestrian object detection algorithm for small objects in infrared images, which improved the feature expression ability of the network combined with the SE block [43].To further improve the speed and accuracy of object detection, especially when objects are small and occluded, Y. Li et al. [44] developed a detector, YOLO-ACN, by introducing an attention module, CIoU (Complete Intersection over Union) [45], [46] loss, improved Soft-NMS (Non-Maximum Suppression) [47], and depthwise separable convolution. The detector, YOLO-ACN, can focus on small objects and avoid the deletion of occluded objects. However, there were still a large number of parameters to save that the weight file was too large, which made it difficult to apply on mobile devices. In addition to the above methods based on the classic YOLOv3, there were some other onestage network models for infrared image object detection. Y. Cao et al. [48] presented a DNN-based one-stage detector, ThermalDet, which included a dual-pass fusion block (DFB) and a channelwise enhancement module (CEM). The mAP of ThermalDet is 74.6% in the FLIR dataset, and thus, cannot achieve the desired results. X. Dai et al. [49] presented an SSD-like object detection method TIRNet. In this method, VGG was adopted to extract features, and the residual branch was introduced to robust features. Although TIRNet only cost a little additional time, its detection performance still could not meet the actual application requirements. X. Song et al. [50] harnessed the features of infrared images and visible images to achieve fused features. Then, a multispectral feature fusion network (MSFFN) was proposed based on YOLOv3 to detect pedestrian objects, but the excellence of the MSFFN was only obvious when the input images were of a small size.\n\nThese methods achieved a better performance for nighttime object detection in different fields, such as pedestrian detection [4], [8], [41], [44] and autonomous driving [48], [49]. Despite the recent progress, it was difficult to transplant these models to mobile devices after training, especially for drone equipment, satellite equipment, infrared cameras, etc.  To solve the problems in the existing models, this paper studies the state-of-the-art detector YOLOv5, which was first released on June 25, 2020. Based on studying the unique features of infrared images, this paper proposes the YOLO-FIR method for infrared objects with the core of YOLOv5 by analyzing the network structure, compressing channels, optimizing parameters, etc. Furthermore, by extending the thickness of the shallow CSP module that contains rich feature information in the backbone network, incorporating an improved SK attention module in the residual blocks to boost the feature extract ability and adding the detection layer to detect smaller objects, YOLO-FIRI, an improved infrared image object detection framework, is further proposed. In addition, the Densefuse network is used to fuse infrared images and visible images to generate more informative fused infrared images. Experimental results show that, compared with the latest infrared image object models, whether on the KAIST dataset or the FLIR dataset, the proposed object detection model YOLOv5-FIRI for infrared images brings about notable improvements in detection accuracy, detection speed, and model size. In addition, the detection accuracy of the proposed models on the fused dataset is improved to a certain degree.\n\n\nIII. PROPOSED METHOD A. NETWORK ARCHITECTURE\n\nOne-stage deep convolutional neural networks YOLOv3 and YOLOv4 have achieved a good performance in object detec-tion. YOLOv5 uses a variety of network structures and two types of CSP modules to improve YOLOv4, so that YOLOv5 is very conducive to object detection and recognition in terms of detection accuracy and computational complexity. Therefore, this paper proposes a method, YOLO-FIR, for infrared objects with the core of YOLOv5 by analyzing network structure, dividing data, compressing channels, optimizing parameters, training, and testing the model, etc. As a result, a novel network model, YOLO-FIRI, is designed and implemented to detect small and weak objects quickly in infrared images. The structure of YOLO-FIRI is shown in Figure 1, which mainly includes three parts: a backbone network with a lightweight feature extraction network, a neck network to realize cross-stage feature fusion, and a multiscale detection head.\n\nIn Figure 1, the input infrared image changes from 512\u00d7512\u00d71 (infrared images have a single channel) to 256\u00d7256\u00d74 after the focus operation. Then, in the backbone network, the extended CSP module is used to extract rich information from shallow and deep feature maps after the focus operation, and the attention mechanism introduced in the CSP module guides the assignment of different weights to realize and notice the extraction of weak and small features. In addition, the SPP (Spatial Pyramid Pooling) [51] layer can concatenate the results obtained in the channel dimension through four pooling windows to solve the alignment problem of anchors and feature maps. We use the SK attention module to enhance the extracted features. Second, in the neck network, PANet (Path Aggregation Network) [52] is used to generate feature pyramids, and the top-down and bottom-up fusion structures are both used to effectively fuse the multiscale features extracted from the backbone network and enhance the detection of objects with different scales. Finally, in the detection head, the four sets of output feature maps are detected, and the anchor boxes are applied on the output feature maps to generate the final output vectors with a class probability score, a confidence score, and a bounding box. Then, according to the NMS [53] postprocessing, the results detected by the four detection layers are screened to obtain the final detection results. The additional set of feature maps can solve the problem of missed and false detections caused by long-distance shooting. The proposed network models demostrate a qualitative improvement compared with the latest infrared image object detection in terms of detection accuracy, reasoning speed, and network parameters.\n\n\nB. EXTENDED CSP MODULE\n\nWith network layers deepening gradually, the convolutional neural network can better extract the semantic information of high-level features, but the resolution of the high-level feature maps is low. In contrast, the resolution of the feature map in the shallow layer is high, whereas the feature semantic information extracted by the shallow network is weak. For small and weak objects with a few features in infrared images, deep convolution may cause object features to be difficult to extract or even lost. To maximize the extraction of features that are conducive to the detection of weak and small objects in infrared images, it is necessary to make full use of the high-level resolution features of the convolutional neural network in the shallow layer. Thus, in the feature extraction stage, we extend the thickness of the CSP module in the shallow feature extraction process. Through feedback and iteration step-by-step, the object features in the feature maps can be fully extracted to achieve multifeature extraction from shallow layers to deep layers. Moreover, when deepening the CSP modules in the overall feature extraction network by controlling the width and depth factors, we only extend the thickness of the CSP module to extract shallow features. The backbone structure of the entire feature extraction network is shown in Figure 2. In this way, without substantially increasing the size of the network model and the complexity of the algorithm, the ability to extract shallow feature information is enhanced, which is conducive to the detection of weak and small objects in infrared images. In addition, the CSP structure divides the feature maps into two branches to extract features and then merge them, which can achieve a richer gradient combination while reducing the amount of calculations.\n\nIn Figure 2, after the focus slicing operation, the Conv and CSP modules are stacked three times, the shallow layer is extended to the same number of CSP module feedback iterations as the deep layer, and feature maps of different sizes are obtained step-by-step. Then, the full extraction of fine-grained features of shallow information and deep highlevel semantic information are obtained, and the specific CSP module structure is shown in Figure 3. Conv represents the three operations of standard convolution, normalization, and the activation function, while Conv2d represents standard convolution. Through the concatenate operation, the feature maps containing the two branches of the Conv and the attention module (SK Layer) are merged, and the 128\u00d7128 features are fully extracted in the shallow layer. Compared with the YOLOv5m model that adds 108 layers, we only add 18 layers to the network by extending the shallow CSP module. We ensure that the detection speed of the model is not reduced when improving the detection accuracy of the model. \n\n\nC. IMPROVED SK ATTENTION MODULE\n\nThe visual system tends to pay attention to a part of the information that assists with judging the image and ignores the unimportant information [54]. In object detection, an attention mechanism can be introduced in the residual blocks of the shallow feature extraction stage to effectively select object information, and more weights can be assigned to small and weak objects to improve the feature expression ability of small objects for accurate detection. The SKNet [25] network can adaptively adjust the size of the receptive field according to multiple scales of the input information, and better extract objects with different sizes and distances. Therefore, we introduce an improved SK attention module in each CSP module and use two convolutional operations with different convolution kernel sizes to learn the channel weights. The output vector continues to perform 1\u00d71 convolution operations. The corresponding introduction position in the CSP module is the SK layer in Figure 3, and the specific improved SK attention mechanism structure is shown in Figure 4. As shown in Figure 4, after two Conv modules, which include standard convolution, normalization, and activation functions, the improved SK attention module is directly embedded into the residual blocks, and it is mainly divided into three parts: split, fuse, and scale. The split operation separates the input vector by performing the Conv operation with two different sizes of kernels, 3\u00d73 and 5\u00d75, to obtain the output vectors U1 and U2, and to obtain the vector U after the addition operation. The fuse stage uses global average pooling (F gp ) to compress the matrix to 1\u00d71\u00d7C and uses a channel descriptor to represent the information of each channel. Therefore, the dependency between the channels is established, which can be expressed as (1), and the fully connected layer (F f c ) makes the relationship between the channels flexible and nonlinear. Here, two fully connected layers are also used to add more nonlinearity, fit the complex correlation between the channels, reduce the number of parameters and calculations as much as possible, and obtain the weight value, which is given by the Eq.:\nF gp (U ) = 1 W \u00d7 H H i=1 W j=1 U (i, j) (1) F f c (F gp , \u03c9) = \u03c3 (B (F gp , \u03c9))(2)\nIn (1), W and H are the width and height, respectively, and i and j are the i-th row and j-th column of the image, respectively. In (2), \u03c9 is the weight, \u03c3 is the ReLU activation function [55], and B represents the batch normalization operation.\n\nScaling is a simple weighting operation. The weight values calculated in the fuse stage are multiplied back to the original matrix to obtain the final output of the SK blocks, which can strengthen the useful information of weak and small objects for different channelwise scenarios. The matrix vectors are added again and merged to make full use of the shallow and deep layer information. By using a simple and effective fully connected layer, the output obtained after the sigmoid activation weight value is directly multiplied by the vector U to obtain the vector V instead of generating vectors a and b (two weight matrices to multiply). Thus, the computational complexity is reduced, and the reduction of the inference speed caused by the increased network layer is avoided.\nF scale (U, F f c ) = V 1 + V 2 = U 1 \u00b7 F f c + U 2 \u00b7 F f c (3)\nIn (3), F scale (U, F f c ) is channelwise multiplication, multiplying the feature maps U with the weight value obtained in the F f c stage, and outputting the weighted feature maps.\n\nSK is a lightweight module that can be directly embedded in the network. It has a strong generalize ability by acquiring different receptive field information and an adaptive adjustment structure which is beneficial to the detection of pedestrians in infrared images. Moreover, systemic improvements can be achieved with a minimal computational burden. \n\n\nD. MULTISCALE FEATURE DETECTION\n\nThe YOLOv5 network uses three types of output feature maps to detect objects with different sizes and uses 8 downsampling output feature maps to detect small objects. The objects in the KAIST [26] dataset are small and weak; therefore, we add a feature scale to focus on smaller objects. When feature maps are upsampled to the size of 64\u00d764, we continue to upsample the feature maps to obtain 4 downsampling feature maps. At the same time, the expanded 128\u00d7128 feature maps are fused with the same size feature map of the second layer in the backbone network to make full use of the shallow and deep features. After multiscale fusion, the four feature scales are 128\u00d7128, 64\u00d764, 32\u00d732, and 18\u00d718, as shown in Figure 5. The 32\u00d732 marked in the grid division represents the size of each grid. Nine anchors with three detection scales are increased to twelve anchors with four detection scales. YOLOv5 can adaptively calculate suitable anchors according to different datasets, making it easier for the model to learn the converge and predict objects with different scales. In Figure 5, the left part is the prediction of the model, and the four detection layers (P2-P5) predict the values, i.e., the central point tx and ty, the width tw and height th, and the confidence score. The right part is the ground truth of the objects, and the network obtains the label information of the input images. Then, the loss between the prediction value and the ground truth is established to calculate the loss of each detection layer. Through the feedback of the loss, the model gradually optimizes the performance and completes the training. The loss calculation method for each detection layer is the same, which is obtained by calculating the sum of the bounding box regression loss, class loss, and confidence loss, e.g., the calculation of the P2 detection layer is shown as Eq.: loss p2 = loss ciou + loss cls + loss obj (4) Here, the bounding box loss (loss ciou ) uses CIoU, the class loss is calculated through BCE (Binary Cross Entropy) loss, and the confidence loss is realized by BCE with logits loss to get numerical stability.\n\n\nIV. EXPERIMENT ANALYSIS\n\nTo test the performance of the infrared image object detection models YOLO-FIR and YOLO-FIRI which are proposed in this paper, we use the public KAIST and FLIR infrared pedestrian dataset. First, the latest detection algorithms and the detection models proposed in this paper are compared in terms of detection accuracy, speed, computational complexity, parameters, etc. Second, an ablation experiment is carried out on the improved YOLO-FIRI model to test the performance brought by the different improved methods. Third, the KAIST dataset provides well-aligned visible and infrared image pairs, so the Densefuse deep neural network is used to fuse the visible and infrared images, enhance the characteristics of the objects, and generate a fusion infrared image dataset. Then, YOLO-FIR and YOLO-FIRI experi-VOLUME XX, 2021  ment on the fused dataset. Finally, the FLIR dataset is also used to further test the performance of the proposed models.\n\n\nA. COMPARISON OF THE DETECTION PERFORMANCE\n\nThe KAIST infrared pedestrian dataset is a classic public object detection dataset evaluated by most infrared image object detection algorithms. It contains large-scale, accurate manual annotations, well-aligned visible and infrared image pairs, and has a total of 95,328 pairs of images (640\u00d7512 resolution), including various conventional traffic scenes on campus, street, and countryside. The dataset labels include two pedestrian object categories: person and people. Those who are better to distinguish are labeled as person, and multiple individuals who are not easy to distinguish are labeled as people. C. Li  show that these data is sufficient to achieve a successful model training and performance evaluation, while also achieving a high detection accuracy. Table 1 is the comparison of the state-of-the-art infrared image object detection algorithms and the proposed YOLO-FIR and YOLO-FIRI in this paper in terms of the various evaluation indicators. Here, the size of the input image is 512\u00d7512, the training epochs are set to 300, the batch size is 16, the initial learning rate is 0.001 and learning rate decay of 0.01 every 5 epochs, the IoU threshold is set to 0.20, and the momentum and weight decay are 0.937 and 0.0005, respectively. We keep mosaic as 1 to use the mosaic data enhancement algorithm to expand the diversity of object samples. The training of all experiments is based on the PyTorch and carried out on the GeForce GTX 1660 GPU.\n\nIn Table 1, YOLO-FIR and YOLO-FIRI are the proposed infrared image object detection models based on YOLOv5. Compared with YOLOv3, YOLOv4, and YOLO-ACN, YOLO-FIR and YOLO-FIRI have greatly improved in various indicators. In particular, YOLO-FIRI is 24.8%, 26.9%, and 26.0% higher than the latest classic one-stage object detection algorithm YOLOv4 in detection accuracy indicators AP, AR, and F1, and the mAP50 has also been improved by approximately 21.4%. Compared with YOLO-FIR, the AP, AR, and mAP50 improved by 3.9%, 8.1%, and 13.2%, respectively. The object features in the infrared images are weak and small. If the IoU threshold is set too large, it will be more unfavorable for detecting objects in infrared images. In contrast, setting the highest IoU value to 0.75 is more common and suitable for actual applications. We use mAP50:75 to calculate the average detection accuracy under six different IoU thresholds. Table 1 shows that the mAP50:75 of YOLO-FIRI is approximately 62.3% higher than that of YOLOv4, and the detection time and   calculation amount decrease by approximately 62.2% and 84.1%, respectively, which greatly improves the real-time processing capability and reduces the hardware calculation requirements. In terms of the number of parameters, YOLO-FIRI directly reduces from tens of millions to millions. The corresponding weight file size also reduces from more than 200 MB of YOLOv3 and YOLOv4 to 15 MB of YOLO-FIRI, which is more conducive to model transplantation for mobile devices. This is mainly because the YOLO-FIR and YOLO-FIRI models use CSPDarknet as the backbone, and perform channel compression, as well as parameter optimization. As a result, the models are able to solve problems, such as repeating gradient information in network optimization in the backbone of other large-scale convolutional neural network frameworks. The gradient changes are integrated into the feature map from beginning to end, thus reducing the number of parameters and flop value of the model, which not only ensures the inference speed and accuracy, but also reduces the model size.\n\nYOLO-FIRI is an improved infrared image object detection framework for weak and small objects in infrared images. Compared with YOLO-FIR, under the condition that the detection time, computational complexity, parameters, etc. are basically unchanged, the detection accuracy indicators AP, AR, mAP and F1 have been improved by approximately 4.2%, 9.1%, 5.6%, and 6.8%, respectively, especially mAP50:75 by approximately 13.2%. This indicates that the detection accuracy has significantly progressed after improving the detection of weak and small objects in infrared images. To further compare the accuracy differences between the two classes, we test each accuracy evaluation index on the KAIST dataset with YOLO-FIR and YOLO-FIRI. The results are shown in Table 2. In the detection of the two classes, AP and AR increased by 4.6%-8.1%. Moreover, the mAP50 for person and people reached 98.8% and 99.0%, respectively, and the mAP50:75 for the class of people increased by 13.6%.\n\nTo study the changes within the different detection models in the training process, Figure 6 shows the training accuracy of YOLOv3, YOLOv4, YOLO-FIR, and the different improved methods based on YOLO-FIR. The left image is the mean average precision of the two classes when the IoU is 0.5 (mAP50), and the right image is the mean average precision of the six different thresholds when the IoU is 0.5 to 0.75 (mAP50: 75). Because YOLOv4 improves the feature extraction network and data enhancement technology of YOLOv3, its detection accuracy is slightly better than that of YOLOv3. However, the YOLO-FIR and YOLO-FIRI detection methods proposed in this paper have a much higher performance than YOLOv4 whether in the mAP50 on the left or the mAP50:75 on the right, and the number of trainings required to enter the stable state is also relatively low. Compared with YOLOv3 and YOLOv4, the accuracy of mAP50 approaches 80%, whereas the improved YOLO-FIRI approaches 98%; the mAP50:75 approaches 60%, whereas the improved YOLO-FIRI approaches 95%, which greatly VOLUME XX, 2021 improves the average detection accuracy. In addition, as the epochs increase, the detection performance of YOLOv3 and YOLOv4 does not always exhibit an upward trend. If the training is continued, overfitting will occur [58], which will cause a slight decrease in the detection performance. However, YOLO-FIR and YOLO-FIRI adapt a variety of CSP structure extraction and fusion features. As the number of trainings increases, their detection performance maintains a steady trend. Loss plays an important role in the training process, which reflects the relationship between the true value and the predicted value. The smaller the loss is, the closer the prediction value is to the true value, and the better the performance of the model. Figure 7 shows the loss convergence rates of YOLOv3, YOLOv4, YOLO-FIR, and different improved methods based on YOLO-FIR. From Figure 7, we can see that the bounding box loss, class loss, and object loss in the training set and the validation set present a falling trend and eventually stabilize. Whether in the training or the validation, the improved YOLO-FIRI has been considerably reduced compared with YOLOv4. Compared with the proposed YOLO-FIR, although the curve is relatively close, there is still a slight decrease for the bounding box regression loss in the validation set. YOLOv4 is 0.037 at 300 epochs, whereas the bounding box regression loss of YOLO-FIRI is 0.012, which means that the proposed method can significantly accelerate the network's training process and converge to a lower loss when optimizing the neural network.\n(a1) (b1) (c1) (d1) (a2) (b2) (c2) (d2)\nExamples of the test results on the YOLOv4 and the YOLO-FIRI models are shown in Figure 8. In terms of the number of detected objects, YOLO-FIRI detects 1 or 2 more pedestrian objects than the YOLOv4 in each image. As shown in (a2) and (b2), YOLO-FIRI can solve the missed detection problem of YOLOv4 by effectively extracting features for the possible occlusion of parallel pedestrians; for the long-distance pedestrians detected in (c2) and (d2), YOLO-FIRI realizes the detection of small objects through multiscale detection. Although the object in the infrared image is difficult to distinguish from the background, the improved model can still achieve the detection of the pedestrian in the infrared images with different distances by enhancing shallow features, fusing multiple features, and improving multiscale detection.\n\n\nB. ABLATION STUDY\n\nTo see the effect of the different improved technologies more intuitively on the performance of the model, we conduct an ablation experiment. Specifically, keeping the structure of YOLOv5s unchanged and only improving the extended CSP module, we can observe the impact of the performance. Then, we improve the SK attention module, and multiscale feature detection to observe the experimental results and analyze the influence.\n\nOur ablation experiment also keeps training for 300 epochs. When the training result is stabilized, the training is completed and then tested on the KAIST test set. The indicators are shown in Table 3. By introducing the extended CSP, the improved SK attention module, and the added detection head, the accuracy indicators of object detection have been improved accordingly. When the integration of these four improvements is tested as the final network model, the tested indicators show better detection accuracy than the three methods introduced separately. The corresponding mAP50 increased by 2.8%, and the mAP50:75 achieved a maximum increase of 10.2%.\n\n\nC. INFRARED OBJECT DETECTION ON THE FUSED KAIST\n\nIn the field of image analysis, the quality of the image directly affects the design of the algorithm and the accuracy of the detection. Compared with visible images, infrared images have lower resolution and blurred visual effects. At present, the performance of the object detection model based on deep learning proves to be high quality and the image performance is better as well. However, when it is applied to infrared images to detect weak and small objects, the performance of the model is greatly reduced. Therefore, to improve the detection performance of weak and small objects, data enhancement as an effective method has been proven to solve the challenges brought by object detection tasks, such as Cutout [59], CutMix [60], Keep Augment [61], and other data enhancement methods by improving the image resolution [17], [62]. Image fusion can be a data enhancement method that performs fusion processing for a variety of different types of source images. Compared with a single image, image fusion has a substantial enhancement in image quality and clarity. The KAIST dataset provides infrared and visible image pairs of the same scene. Given the lack of a publicly available infrared dataset, the generated fusion dataset also achieves data diversity. Visible images can reflect the spectral information properties of objects, containing more detailed information, and are more in line with visual characteristics of the human eyes. And the thermal radiation characteristics of infrared images are more sensitive to objects and areas, which can avoid interference caused by scene changes. Therefore, infrared image and visible image are complementary, and the image fusion method can be used to fuse infrared and visible images into a more informative infrared image by using the fusion method to achieve the purpose of data enhancement. Figure 9 shows the Densefuse [27] framework for fusing infrared and visible images, which mainly includes three parts: encoder, fusion layer, and decoder. The encoder mainly includes two kinds of layers, C1 and a dense layer. C1 contains a 3\u00d73 convolution kernel to extract rough features, and the dense layer contains three convolutional layers to extract deep high-level features. The encoder convolution kernel size and convolution stride are 3\u00d73 and 1, respectively, which can receive images of any size, while the dense layer can retain depth features as much as possible in the encoding network. The fusion layer uses an additive strategy to fuse the infrared and visible image features extracted by the encoder, which can be written as Eq.:\nF m (x, y) = \u03bbV is m (x, y) + (1 \u2212 \u03bb) Ir m (x, y) (5)\nIn (5), V is m (x, y) represents the visible image of the mth channel, Ir m (x, y) represents the infrared image of the m-th channel, F m (x, y) indicates the fusion result of the mth channel, and \u03bb is the weighting coefficient. The output of the fusion layer enters the decoder, which contains four 3\u00d73 convolutional layers to reconstruct the fused image. The loss function of the network H (x, y) is weighted by the structural similarity loss function H SSIM (x, y) and the pixel loss function H P (x, y).\n\nH (x, y) = \u03b3H SSIM (x, y) + H P (x, y) = \u03b3 (1 \u2212 SSIM (Out (x, y) , In (x, y))) + Out (x, y) \u2212 In (x, y) 2 (6) VOLUME XX, 2021  In (6), Out (x, y) and In (x, y) represent the output image and the input image, respectively. H P (x, y) is the Euclidean distance between Out (x, y) and In (x, y). SSIM (\u00b7) is the structural similarity. Considering that there are three orders of magnitude differences between the pixel loss and SSIM loss, we take 100 here.\n(b1) (c1) (a2) (b2) (c2) (a3) (b3) (c3) (a1)\nIn this experiment, we use the Densefuse network, which is shown in Figure 9, to obtain the fused KAIST dataset. The image comparison of the infrared images, visible images, and fused images of the KAIST are shown in Figure 10. Among them, the detailed information of some objects is marked with red boxes for highlighting. Compared with the infrared image (a1), the fused image (c1) has a clearer body posture and contour, which expands the data features, so that the object features are more obvious and easier to extract. Compared with the visible image (b2), the fused image (c2) avoids the influence of the red bus occlusion in the background, making the object easier to recognize instead of being filtered out as the background. Compared with the\n(a) (b) (c) (d) FIGURE 11\n. Some detection results on the infrared and fused KAIST dataset. We use the proposed YOLO-FIRI to test the two kinds of datasets. The first row is the images in the infrared KAIST dataset and the second row is the images in the fused KAIST dataset. Although the YOLO-FIRI achieves better detection results on the infrared KAIST dataset, the fused dataset obtained through data enhancement can further improve the performance with the proposed models. infrared image (a3) and the visible image (b3), the number of objects and human characteristics in the fused image (c3) are clearer.\n\nFor the visible, infrared, and fused infrared datasets, the proposed network models are trained separately. Table 4 compares the test results of visible, infrared and fused infrared images on YOLO-FIR and the improved network model YOLO-FIRI. On the detection model of YOLO-FIR, when fused dataset is used as input, both AP and mAP50 are improved, and the detection accuracy of mAP50 is improved by 0.7% compared to a single infrared image. On the YOLO-FIRI, the fused infrared dataset achieved the best results, and the mAP increased to 98.5%. The KAIST dataset has serious occlusion problems in visible images, but it can effectively improve this problem on the infrared dataset. Therefore, on the YOLO-FIRI models, the performance of the fused infrared dataset is better than that of the visible dataset and the infrared dataset.\n\nAfter training, the models are further tested on two dif-ferent types of datasets. Figure 11 shows some randomly selected images of infrared and fused datasets. In Column (a), the fused image can extract pedestrian objects that are occluded due to the shooting angle; in Column (b), pedestrian objects at the edge can also be detected more accurately after fusion. In the densely crowded images of the two Columns (c) and (d), the number of pedestrian objects detected has increased from two and six to three and seven, respectively. We can see that the fused test set can detect more pedestrian objects than the infrared test set, and the overall accuracy rate has been improved.\n\n\nD. INFRARED OBJECT DETECTION ON THE FLIR DATASET\n\nOn the KAIST dataset, we compare different models of the YOLO series. To better test the detection performance of the proposed models on infrared images, we test YOLO-FIRI and compare the different detection approaches on the FLIR dataset [28]. In the FLIR dataset, it provides both visible and infrared images, whereas the visible and infrared image pairs are not well aligned. Some infrared images do not have corresponding visible images, and only the infrared images are labeled. Therefore, we simply train on the infrared images and do not need to adapt a pretrained detector with RGB images. The training set and the test set contain 8862 and 1366 images with 640\u00d7512 resolution, and a total of three classes are included. In the experiments, we use the train set and test set as provided in the dataset benchmark. The experimental setting is still the same as that of the KAIST dataset. As shown in Table 5, we compare the value of AP for each class and the mAP of the different detectors, which are presented with percentage. In [40], when only Faster R-CNN was used as the detector, the mAP was 53.9%. However, the MMTOD-CG used the Faster R-CNN as a baseline and combined a pretrained detector, which increased the mAP by 7%. TermalDet [48] was proposed based on RefineDet [63], which took into account the features of each layer as the final detection, and the accuracy increased to 74.6%. The proposed methods YOLO-FIR and YOLO-FIRI are based on the stateof-the-art detector YOLOv5 in this paper. Under the premise of ensuring speed, the accuracy is further improved with the full use of features in the shallow layers, the attention of the important feature, and the improved detection head. We can observe that the values of AP and mAP all outperformed those in previous work, and our YOLO-FIRI further reaches 83.5%. In other words, the region-free framework YOLO-FIRI can learn more features from infrared images and the situation of false detection and long-distance missed detection has improved.\n\n\nV. CONCLUSION\n\nTo overcome the drawbacks of infrared images object detection, in this paper, we propose a one-stage region-free object detector YOLO-FIR for infrared images, which is based on the YOLOv5 and the application for infrared images. Combining the features of infrared images, we further propose an improved YOLO-FIRI based on YOLO-FIR. Precisely, we extend and iterate the shallow CSP module of the feature extraction network and combine an improved attention module to the residual blocks to maximize the use of shallow features, forcing the network to learn the robust and distinguishable features. Additionally, the network detection head is improved, multiscale object detection layers are added, and the detection accuracy of infrared small objects is improved. Compared with YOLOv4, YOLO-FIRI has made a qualitative leap in various indicators. The mAP of YOLO-FIRI is increased by approximately 37% on the infrared images of KAIST, the detection time is reduced by approximately 62%, the network parameters are reduced by more than 89%, and the weight size is reduced by more than 93%. Compared with YOLO-FIR, the mAP of YOLO-FIRI reaches 98.3% and increases approximately 13% on KAIST. The AP for the bicycle class of YOLO-FIRI on FLIR also reaches 85%, which is an increase of 15%. Our proposed model's stateof-the-art performance can be attributed to the combination of learned shallower features and attention features, which allows our model to detect infrared objects based on their low resolution and unclear features. Because the KAIST dataset provides well-aligned visible and infrared images, we prove that data enhancement can be realized to further improve the detection accuracy of infrared images through the use of the convolutional neural network in image preprocessing to fuse visible and infrared images.\n\nIn this paper, we mainly focus on the single infrared images which are still and chaotic. It would be interesting to use the infrared video to realize the object detection because the video sequences have a strongly correlation between the front and rear frames. Therefore, the detection performance in infrared video object detection will be better.\n\nFIGURE 1 .\n1YOLO-FIRI overall network architecture. It mainly includes three parts: the Backbone network, the Neck, and the Detection head. The input single-channel infrared image is extracted through the backbone network with the extended CSP module. Multiscale features are further fused in the neck. Finally, the four-scale feature maps obtained are used to achieve multiscale object detection in the detection head.\n\nFIGURE 2 .\n2The Backbone structure with the extended shallow CSP module. We extend the shallow CSP to achieve the same number of iterations as the deep CSP (which is iterated 3 times). Then, the network can realize the focus on shallow features.\n\nFIGURE 3 .\n3The extended CSP module with the improved SK attention module. The attention module is added between the two convolutional layers as an SK layer.\n\nFIGURE 4 .\n4is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4The improved SK attention module. We implement the improved SK convolution via three operators: split, fuse, and scale. In the split operation, two kernels with different sizes are split; in the fuse operation, global pooling and a full connection are used; in the scale operation, the results generated by the above two-stage are weighted.\n\nFIGURE 5 .\n5Multiscale detection structure. The input image through the network extracts features, and then four sets of feature maps are finally detected to predict the six values of each object. According to the label information, the loss between the prediction and ground truth is calculated, and then the training is completed.\n\nFIGURE 6 .\n6Performance comparison of the mean average precision using the training set of the KAIST. The left curves are mAP50 (IoU=0.5). The right curves are mAP50:75 (the AP by 6 IoU thresholds of 0.5:0.05:0.75 is averaged). These curves separately represent YOLOv3, YOLOv4, and the proposed models YOLO-FIR and YOLO-FIRI. YOLO-FIR w ECSP and YOLO-FIR w DH are the proposed YOLO-FIR with the extended CSP module and the detection head.\n\nFIGURE 7 .\n7Performance comparison of three kinds of loss using the KAIST dataset. The first row is the loss of the training set of KAIST, and the three figures in the first row from left to right are the box regression loss (box_loss), the class loss (cls_loss), and the object loss (obj_loss). The second row is the loss of the test set of KAIST, and the three figures in the first row from the left are the same as those in first row.\n\nFIGURE 8 .\n8Some examples of the detection result on the test set of the KAIST dataset. The first row is the result of YOLOv4, and the second row is the result of YOLO-FIRI. We used the same four images to compare the performance of the detection models.\n\nFIGURE 9 .\n9The architecture of the Densefuse network. The input images are the well-aligned visible and infrared images. The encoder network uses a convolutional and dense block to extract more useful features from the source images. The fusion layer is used to fuse the extracted features. The decoder network can rebuild the fused infrared images to realize the fused KAIST dataset.\n\nFIGURE 10 .\n10Some image examples of infrared images, visible images, and fusion images on the KAIST dataset. The first column is the infrared images, the second column is the visible images, and the third column is the fused images.\n\n\nShasha Li et al.: YOLO-FIRI: Improved YOLOv5 for Infrared Image Object DetectionVOLUME XX, 2021 \nUpsample \n\nUpsample \n\nUpsample \n\nConcatenate \n\nConcatenate \n\nConcatenate \n\nSPP \nSK \n\n16\u00d716 \n\n64\u00d764 \n\n128\u00d7128 \n\ninput \n512\u00d7512 \n\nFocus \n256\u00d7256 \nCSP\u00d73 \n128\u00d7128 \nCSP\u00d73 \n64\u00d764 CSP\u00d73 \n\n32\u00d732 CSP\u00d71 \n\n16\u00d716 \nConv16\u00d716 \n\nConv32\u00d732 \n\nConv64\u00d764 \n\nConv128\u00d7128 \n\n32\u00d732 \n32\u00d732 \n\n64\u00d764 \n\n128\u00d7128 \n\nDetection head \n\noutput \n\nBackbone network \n\nNeck \n\n\n\nTABLE 1 .\n1Quantitative comparison of YOLO-FIRI and the other state-of-the-art object detectors. The results are reported in terms of mAP percentage and times on the KAIST test set.Method \nAP/% \nAR/% \nmAP/% \nF1/% \nmAP50:75/% \nTime/ms \nParams/M \nWeight/MB \nFLOPs/B \n\nYOLOv3 \n73.5 \n76.9 \n79.6 \n74.8 \n57.0 \n25 \n61.5 \n246.4 \n155.1 \nYOLOv4 \n76.9 \n75.8 \n81.0 \n76.3 \n58.9 \n37 \n63.9 \n256.3 \n128.4 \nYOLO-ACN \n76.2 \n87.9 \n82.3 \n81.6 \n59.3 \n20 \n47.4 \n177.6 \n111.8 \nYOLO-FIR \n92.1 \n88.1 \n93.1 \n90.0 \n82.4 \n12 \n7.1 \n16.2 \n16.4 \nYOLO-FIRI \n96.0 \n96.2 \n98.3 \n96.1 \n95.6 \n14 \n7.2 \n15.0 \n20.4 \n\nmAP_50 \nmAP_50:75 \n\nYOLOv5-FIR w DH \n\nYOLOv3 \n\nYOLOv4 \n\nYOLOv5-FIR \n\nYOLOv5-FIR w ECSP \n\nYOLOv5-FIRI \n\n\n\nTABLE 2 .\n2Performance of precisions about two classes on the test set of the KAIST. This is mainly due to YOLO-FIRI's improvements in extending and iterating CSP modules, introducing attention mechanisms, and improving multiscale detection heads.Method \nclases AP/% AR/% mAP50/% mAP50:75/% \n\nYOLO-FIR person \n92.0 \n92.4 \n95.9 \n87.8 \nYOLO-FIRI person \n97.4 \n97.0 \n98.8 \n96.9 \nYOLO-FIR people \n93.2 \n88.5 \n91.6 \n83.9 \nYOLO-FIRI people \n97.2 \n96.6 \n99.0 \n97.5 \n\n\n\nTABLE 3 .\n3Ablation study of detection precision on the test set of KAIST.CSP SK HEAD AP/% AR/% mAP50/% mAP50:75/% \n\u221a \n92.0 \n93.9 \n97.5 \n94.3 \n\u221a \n92.4 \n87.4 \n95.5 \n85.5 \n\u221a \n93.2 \n93.1 \n97.0 \n94.1 \n\u221a \n\u221a \n\u221a \n96.0 \n96.3 \n98.3 \n95.6 \n\n\n\nTABLE 4 .\n4Performance comparisons on the visible (VIS) dataset, infrared dataset, and fused dataset of the KAIST.Type \nAP/% \nAR/% \nmAP/% \nF1/% \n\nYOLO-FIR: Accuracy of Object Detection \nVIS \n92.4 \n87.3 \n95.5 \n89.7 \nIR \n92.1 \n88.1 \n93.1 \n90.0 \nFused \n92.4 \n84.9 \n93.8 \n88.1 \n\nYOLO-FIRI: Accuracy of Object Detection \nVIS \n92.0 \n93.9 \n97.4 \n92.9 \nIR \n96.0 \n96.2 \n98.3 \n96.1 \nFused \n97.4 \n96.4 \n98.5 \n96.8 \n\n\n\nTABLE 5 .\n5Quantitative comparison of FLIR measured by the common metric AP and mAP in percentage with IoU=0.5. We evaluate the three classes labeled by FLIR and take reported numbers from[48] for most compared methods. Our YOLO-FLIR outperforms the previous works. Bolded numbers mark the best results.Method \nPerson/% \nBicycle/% \nCar/% \nmAP/% \n\nFaster R-CNN [40] \n39.6 \n54.7 \n67.6 \n53.9 \nMMTOD-CG [40] \n50.3 \n63.3 \n70.6 \n61.4 \nRefineDet [63] \n77.2 \n57.2 \n84.5 \n72.9 \nTermalDet [48] \n78.2 \n60.0 \n85.5 \n74.6 \nYOLO-FIR \n85.2 \n70.7 \n84.3 \n80.1 \nYOLO-FIRI \n85.8 \n85.3 \n90.6 \n83.5 \n\n\nVOLUME XX, 2021\nVOLUME XX, 2021\nVOLUME XX, 2021\nVOLUME XX, 2021\nVOLUME XX, 2021\nShasha Li et al.: YOLO-FIRI: Improved YOLOv5 for Infrared Image Object Detection\n\nInfrared imaging technology for breast cancer detection -current status, protocols and new directions. S G Kandlikar, I Perez-Raya, P A Raghupathi, J.-L Gonzalez-Hernandez, D Dabydeen, L Medeiros, P Phatak, International Journal of Heat and Mass Transfer. 108S. G. Kandlikar, I. Perez-Raya, P. A. Raghupathi, J.-L. Gonzalez- Hernandez, D. Dabydeen, L. Medeiros, and P. Phatak, \"Infrared imaging technology for breast cancer detection -current status, protocols and new directions,\" International Journal of Heat and Mass Transfer, vol. 108, pp. 2303-2320, 2017. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0017931016336031\n\nA novel infrared video surveillance system using deep learning based techniques. H Zhang, C Luo, Q Wang, M Kitchin, A Parmley, J Monge-Alvarez, P Casaseca-De-La Higuera, Multimedia Tools and Applications. 7720H. Zhang, C. Luo, Q. Wang, M. Kitchin, A. Parmley, J. Monge-Alvarez, and P. Casaseca-de-la Higuera, \"A novel infrared video surveillance system using deep learning based techniques,\" Multimedia Tools and Applications, vol. 77, no. 20, pp. 26 657-26 676, 2018. [Online].\n\n. 10.1007/s11042-018-5883-yAvailable: https://doi.org/10.1007/s11042-018-5883-y\n\nA survey of computer vision methods for 2d object detection from unmanned aerial vehicles. D Cazzato, C Cimarelli, J L Sanchez-Lopez, H Voos, M Leo, Journal of Imaging. 68D. Cazzato, C. Cimarelli, J. L. Sanchez-Lopez, H. Voos, and M. Leo, \"A survey of computer vision methods for 2d object detection from unmanned aerial vehicles,\" Journal of Imaging, vol. 6, no. 8, 2020. [Online]. Available: https://www.mdpi.com/2313-433X/6/8/78\n\nCnnbased person detection using infrared images for night-time intrusion warning systems. J Park, J Chen, Y K Cho, D Y Kang, B J Son, Sensors. 201J. Park, J. Chen, Y. K. Cho, D. Y. Kang, and B. J. Son, \"Cnn- based person detection using infrared images for night-time intrusion warning systems,\" Sensors, vol. 20, no. 1, 2020. [Online]. Available: https://www.mdpi.com/1424-8220/20/1/34\n\nEfficient pedestrian detection with enhanced object segmentation in far ir night vision. K Piniarski, P Paw\u0142owski, 2017 Signal Processing: Algorithms, Architectures, Arrangements, and Applications. K. Piniarski and P. Paw\u0142owski, \"Efficient pedestrian detection with en- hanced object segmentation in far ir night vision,\" in 2017 Signal Process- ing: Algorithms, Architectures, Arrangements, and Applications (SPA), 2017, pp. 160-165.\n\nInfrared imaging guidance missile's target recognition simulation based on air-to-air combat. S Li, C Wang, H Huang, 10.1117/12.2505607Optical Sensing and Imaging Technologies and Applications. M. Guina, H. Gong, J. Lu, and D. Liu10846S. Li, C. Wang, and H. Huang, \"Infrared imaging guidance missile's target recognition simulation based on air-to-air combat,\" in Optical Sensing and Imaging Technologies and Applications, M. Guina, H. Gong, J. Lu, and D. Liu, Eds., vol. 10846, International Society for Optics and Photonics. SPIE, 2018, pp. 768 -780. [Online]. Available: https://doi.org/10.1117/12.2505607\n\nBackground noise suppression in small targets infrared images and its method discussion. K Zhao, X Kong, Optics and Optoelectronic Technology. 02K. Zhao and X. Kong, \"Background noise suppression in small targets infrared images and its method discussion,\" Optics and Optoelectronic Technology, no. 02, pp. 9-12, 2004.\n\nShearlet transform based image denoising using histogram thresholding. T S Anju, N R N Raj, 2016 International Conference on Communication Systems and Networks (ComNet). T. S. Anju and N. R. N. Raj, \"Shearlet transform based image denoising using histogram thresholding,\" in 2016 International Conference on Com- munication Systems and Networks (ComNet), 2016, pp. 162-166.\n\nResearch on image classification and retrieval method based on deep learning and sparse representation. P Jiao, MasterP. Jiao, \"Research on image classification and retrieval method based on deep learning and sparse representation,\" Master, 2019.\n\nRecent advances in deep learning for object detection. X Wu, D Sahoo, S C Hoi, Neurocomputing. 396X. Wu, D. Sahoo, and S. C. Hoi, \"Recent advances in deep learning for object detection,\" Neurocomputing, vol. 396, pp. 39-64, 2020. [Online].\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n\nMask r-cnn. K He, G Gkioxari, P Dollar, R Girshick, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)K. He, G. Gkioxari, P. Dollar, and R. Girshick, \"Mask r-cnn,\" in Proceed- ings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n\nImage super-resolution using deep convolutional networks. C Dong, C C Loy, K He, X Tang, IEEE Transactions on Pattern Analysis and Machine Intelligence. 382C. Dong, C. C. Loy, K. He, and X. Tang, \"Image super-resolution using deep convolutional networks,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 38, no. 2, pp. 295-307, 2016.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)R. Girshick, J. Donahue, T. Darrell, and J. Malik, \"Rich feature hierarchies for accurate object detection and semantic segmentation,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014.\n\nFast r-cnn. R Girshick, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)R. Girshick, \"Fast r-cnn,\" in Proceedings of the IEEE International Con- ference on Computer Vision (ICCV), December 2015.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, IEEE Transactions on Pattern Analysis and Machine Intelligence. 396S. Ren, K. He, R. Girshick, and J. Sun, \"Faster r-cnn: Towards real-time object detection with region proposal networks,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 6, pp. 1137-1149, 2017.\n\nSsd: Single shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C.-Y Fu, A C Berg, Computer Vision -ECCV 2016. B. Leibe, J. Matas, N. Sebe, and M. WellingSpringer International PublishingW. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg, \"Ssd: Single shot multibox detector,\" in Computer Vision -ECCV 2016, B. Leibe, J. Matas, N. Sebe, and M. Welling, Eds. Cham: Springer International Publishing, 2016, pp. 21-37.\n\nDSSD : Deconvolutional single shot detector. C Fu, W Liu, A Ranga, A Tyagi, A C Berg, abs/1701.06659CoRR. C. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, \"DSSD : Deconvolutional single shot detector,\" CoRR, vol. abs/1701.06659, 2017. [Online]. Available: http://arxiv.org/abs/1701.06659\n\nYou only look once: Unified, real-time object detection. J Redmon, S Divvala, R Girshick, A Farhadi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \"You only look once: Unified, real-time object detection,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n\nYolo9000: Better, faster, stronger. J Redmon, A Farhadi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)J. Redmon and A. Farhadi, \"Yolo9000: Better, faster, stronger,\" in Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), July 2017.\n\nYOLOv3: An Incremental Improvement. J Redmon, A Farhadi, abs/1804.02767CoRR. J. Redmon and A. Farhadi, \"YOLOv3: An Incremental Improvement,\" CoRR, vol. abs/1804.02767, 2018. [Online]. Available: http://arxiv.org/ab s/1804.02767\n\nYolov4: Optimal speed and accuracy of object detection. A Bochkovskiy, C Wang, H M Liao, CoRR. A. Bochkovskiy, C. Wang, and H. M. Liao, \"Yolov4: Optimal speed and accuracy of object detection,\" CoRR, vol. abs/2004.10934, 2020. [Online]. Available: https://arxiv.org/abs/2004.10934\n\nG Jocher, A Stoken, J Borovec, Nanocode012, L Christopherstan, Changyu, Laughing, Hogan, A Alexwang1900, L Chaurasia, Diaconu, Marc, Doug, F Durgesh, Ingham, Frederik, A Guilhen, H Colmagro, Ye, J Jacobsolawetz, J Poznanski, J Fang, K Kim, L Doan, Yu, 10.5281/zenodo.4418161ultralytics/yolov5: v4.0 -nn.SiLU() activations, Weights & Biases logging, PyTorch Hub integration. G. Jocher, A. Stoken, J. Borovec, NanoCode012, ChristopherSTAN, L. Changyu, Laughing, tkianai, yxNONG, A. Hogan, lorenzomammana, AlexWang1900, A. Chaurasia, L. Diaconu, Marc, wanghaoyang0106, ml5ah, Doug, Durgesh, F. Ingham, Frederik, Guilhen, A. Colmagro, H. Ye, Jacobsolawetz, J. Poznanski, J. Fang, J. Kim, K. Doan, and L. Yu, \"ultralytics/yolov5: v4.0 -nn.SiLU() activations, Weights & Biases logging, PyTorch Hub integration,\" Jan. 2021. [Online]. Available: https://doi.org/10.5281/zenodo.4418161\n\nCspnet: A new backbone that can enhance learning capability of cnn. C.-Y Wang, H.-Y M Liao, Y.-H Wu, P.-Y Chen, J.-W Hsieh, I.-H Yeh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) WorkshopsC.-Y. Wang, H.-Y. M. Liao, Y.-H. Wu, P.-Y. Chen, J.-W. Hsieh, and I.- H. Yeh, \"Cspnet: A new backbone that can enhance learning capability of cnn,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2020.\n\nSelective kernel networks. X Li, W Wang, X Hu, J Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)X. Li, W. Wang, X. Hu, and J. Yang, \"Selective kernel networks,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nKaist multi-spectral day/night data set for autonomous and assisted driving. Y Choi, N Kim, S Hwang, K Park, J S Yoon, K An, I S Kweon, IEEE Transactions on Intelligent Transportation Systems. 193Y. Choi, N. Kim, S. Hwang, K. Park, J. S. Yoon, K. An, and I. S. Kweon, \"Kaist multi-spectral day/night data set for autonomous and assisted driv- ing,\" IEEE Transactions on Intelligent Transportation Systems, vol. 19, no. 3, pp. 934-948, 2018.\n\nDensefuse: A fusion approach to infrared and visible images. H Li, X.-J Wu, IEEE Transactions on Image Processing. 285H. Li and X.-J. Wu, \"Densefuse: A fusion approach to infrared and visible images,\" IEEE Transactions on Image Processing, vol. 28, no. 5, pp. 2614- 2623, 2019.\n\nFlir releases machine learning thermal dataset for advanced driver assistance systems. Anonymous, Vision Systems Design. 23Anonymous, \"Flir releases machine learning thermal dataset for advanced driver assistance systems,\" Vision Systems Design, vol. 23, no. 9, 2018.\n\nAdaptable active contour model with applications to infrared ship target segmentation. L Fang, X Wang, Y Wan, 10.1117/1.JEI.25.4.041010Journal of Electronic Imaging. 254L. Fang, X. Wang, and Y. Wan, \"Adaptable active contour model with applications to infrared ship target segmentation,\" Journal of Electronic Imaging, vol. 25, no. 4, pp. 1 -10, 2016. [Online]. Available: https://doi.org/10.1117/1.JEI.25.4.041010\n\nRobust real-time face detection. P Viola, M J Jones, International Journal of Computer Vision. 572P. Viola and M. J. Jones, \"Robust real-time face detection,\" International Journal of Computer Vision, vol. 57, no. 2, pp. 137-154, 2004. [Online].\n\n. 10.1023/B:VISI.0000013087.49260.fbAvailable: https://doi.org/10.1023/B:VISI.0000013087.49260.fb\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). 1N. Dalal and B. Triggs, \"Histograms of oriented gradients for human detection,\" in 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), vol. 1, 2005, pp. 886-893 vol. 1.\n\nPedestrian detection: An evaluation of the state of the art. P Dollar, C Wojek, B Schiele, P Perona, IEEE Transactions on Pattern Analysis and Machine Intelligence. 344P. Dollar, C. Wojek, B. Schiele, and P. Perona, \"Pedestrian detection: An evaluation of the state of the art,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 4, pp. 743-761, 2012.\n\nA tutorial on \u03c5-support vector machines. P.-H Chen, C.-J Lin, B Sch\u00f6lkopf, 10.1002/asmb.537Applied Stochastic Models in Business and Industry. 212P.-H. Chen, C.-J. Lin, and B. Sch\u00f6lkopf, \"A tutorial on \u03c5-support vector machines,\" Applied Stochastic Models in Business and Industry, vol. 21, no. 2, pp. 111-136, 2005. [Online]. Available: https: //onlinelibrary.wiley.com/doi/abs/10.1002/asmb.537\n\nExperiments with a new boosting algorithm. Y Freund, R E Schapire, ICML. Y. Freund and R. E. Schapire, \"Experiments with a new boosting algo- rithm,\" in ICML, 1996, pp. 148-156.\n\nDiast variability illuminated thermal and visible ear images datasets. S M Z S Z Ariffin, N Jamil, P N M A Rahman, 2016 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA). S. M. Z. S. Z. Ariffin, N. Jamil, and P. N. M. A. Rahman, \"Diast variability illuminated thermal and visible ear images datasets,\" in 2016 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA), 2016, pp. 191-195.\n\nStacked convolutional auto-encoders for hierarchical feature extraction. J Masci, U Meier, D Cire\u015fan, J Schmidhuber, Artificial Neural Networks and Machine Learning -ICANN 2011. Duch, M. Girolami, and S. KaskiBerlin, Heidelberg; Berlin HeidelbergSpringerJ. Masci, U. Meier, D. Cire\u015fan, and J. Schmidhuber, \"Stacked convo- lutional auto-encoders for hierarchical feature extraction,\" in Artificial Neural Networks and Machine Learning -ICANN 2011, T. Honkela, W. Duch, M. Girolami, and S. Kaski, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 52-59.\n\nLearning deep embeddings with histogram loss. E Ustinova, V Lempitsky, Advances in Neural Information Processing. Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. GarnettCurran Associates, Inc29E. Ustinova and V. Lempitsky, \"Learning deep embeddings with histogram loss,\" in Advances in Neural Information Processing Systems, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29. Curran Associates, Inc., 2016. [Online]. Available: https://proceedings.ne urips.cc/paper/2016/file/325995af77a0e8b06d1204a171010b3a-Paper.pdf\n\nSparse r-cnn: End-to-end object detection with learnable proposals. P Sun, R Zhang, Y Jiang, T Kong, C Xu, W Zhan, M Tomizuka, L Li, Z Yuan, C Wang, P Luo, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)14P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka, L. Li, Z. Yuan, C. Wang, and P. Luo, \"Sparse r-cnn: End-to-end object detection with learnable proposals,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021, pp. 14 454- 14 463.\n\nPedestrian detection in thermal images using saliency maps. D Ghose, S M Desai, S Bhattacharya, D Chakraborty, M Fiterau, T Rahman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) WorkshopsD. Ghose, S. M. Desai, S. Bhattacharya, D. Chakraborty, M. Fiterau, and T. Rahman, \"Pedestrian detection in thermal images using saliency maps,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.\n\nBorrow from anywhere: Pseudo multi-modal object detection in thermal imagery. C Devaguptapu, N Akolekar, M Sharma, V Balasubramanian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) WorkshopsC. Devaguptapu, N. Akolekar, M. M Sharma, and V. N Balasubramanian, \"Borrow from anywhere: Pseudo multi-modal object detection in thermal imagery,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.\n\nThermal object detection in difficult weather conditions using yolo. M Kri\u0161to, M Ivasic-Kos, M Pobar, IEEE Access. 8M. Kri\u0161to, M. Ivasic-Kos, and M. Pobar, \"Thermal object detection in difficult weather conditions using yolo,\" IEEE Access, vol. 8, pp. 125 459- 125 476, 2020.\n\nResearch on infrared pedestrian small target detection technology based on yolov3. M Li, T Zhang, W Cui, Infrared Technology. 4202M. Li, T. Zhang, and W. Cui, \"Research on infrared pedestrian small target detection technology based on yolov3,\" Infrared Technology, vol. 42, no. 02, pp. 176-181, 2020.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)J. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), June 2018.\n\nYolo-acn: Focusing on small target and occluded object detection. Y Li, S Li, H Du, L Chen, D Zhang, Y Li, IEEE Access. 8Y. Li, S. Li, H. Du, L. Chen, D. Zhang, and Y. Li, \"Yolo-acn: Focusing on small target and occluded object detection,\" IEEE Access, vol. 8, pp. 227 288-227 303, 2020.\n\nDistanceiou loss: Faster and better learning for bounding box regression. Z Zheng, P Wang, W Liu, J Li, R Ye, D Ren, pp. 12 993-13 000Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Z. Zheng, P. Wang, W. Liu, J. Li, R. Ye, and D. Ren, \"Distance- iou loss: Faster and better learning for bounding box regression,\" Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, pp. 12 993-13 000, Apr. 2020. [Online]. Available: https: //ojs.aaai.org/index.php/AAAI/article/view/6999\n\nEnhancing geometric factors in model learning and inference for object detection and instance segmentation. Z Zheng, P Wang, D Ren, W Liu, R Ye, Q Hu, W Zuo, abs/2005.03572CoRR. Z. Zheng, P. Wang, D. Ren, W. Liu, R. Ye, Q. Hu, and W. Zuo, \"Enhancing geometric factors in model learning and inference for object detection and instance segmentation,\" CoRR, vol. abs/2005.03572, 2020. [Online]. Available: https://arxiv.org/abs/2005.03572\n\nSoft-nms -improving object detection with one line of code. N Bodla, B Singh, R Chellappa, L S Davis, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)N. Bodla, B. Singh, R. Chellappa, and L. S. Davis, \"Soft-nms -improving object detection with one line of code,\" in Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n\nEvery feature counts: An improved one-stage detector in thermal imagery. Y Cao, T Zhou, X Zhu, Y Su, 2019 IEEE 5th International Conference on Computer and Communications (ICCC). Y. Cao, T. Zhou, X. Zhu, and Y. Su, \"Every feature counts: An improved one-stage detector in thermal imagery,\" in 2019 IEEE 5th International Conference on Computer and Communications (ICCC), 2019, pp. 1965- 1969.\n\nTirnet: Object detection in thermal infrared images for autonomous driving. X Dai, X Yuan, X Wei, 10.1007/s10489-020-01882-2Applied Intelligence. 513X. Dai, X. Yuan, and X. Wei, \"Tirnet: Object detection in thermal infrared images for autonomous driving,\" Applied Intelligence, vol. 51, no. 3, pp. 1244-1261, 2021. [Online]. Available: https://doi.org/10.1007/s10489-0 20-01882-2\n\nA multispectral feature fusion network for robust pedestrian detection. X Song, S Gao, C Chen, Alexandria Engineering Journal. 601X. Song, S. Gao, and C. Chen, \"A multispectral feature fusion network for robust pedestrian detection,\" Alexandria Engineering Journal, vol. 60, no. 1, pp. 73-85, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1110016820302507\n\nSpatial pyramid pooling in deep convolutional networks for visual recognition. K He, X Zhang, S Ren, J Sun, IEEE Transactions on Pattern Analysis and Machine Intelligence. 379K. He, X. Zhang, S. Ren, and J. Sun, \"Spatial pyramid pooling in deep convolutional networks for visual recognition,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, no. 9, pp. 1904-1916, 2015.\n\nPannet: A deep network architecture for pan-sharpening. J Yang, X Fu, Y Hu, Y Huang, X Ding, J Paisley, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)J. Yang, X. Fu, Y. Hu, Y. Huang, X. Ding, and J. Paisley, \"Pannet: A deep network architecture for pan-sharpening,\" in Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.\n\nEfficient non-maximum suppression. A Neubeck, L Van Gool, 18th International Conference on Pattern Recognition (ICPR'06). A. Neubeck and L. Van Gool, \"Efficient non-maximum suppression,\" in 18th International Conference on Pattern Recognition (ICPR'06), vol. 3, 2006, pp. 850-855.\n\nShow, attend and tell: Neural image caption generation with visual attention. K Xu, J Ba, R Kiros, K Cho, A C Courville, R Salakhutdinov, R S Zemel, Y Bengio, abs/1502.03044CoRR. K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio, \"Show, attend and tell: Neural image caption generation with visual attention,\" CoRR, vol. abs/1502.03044, 2015. [Online]. Available: http://arxiv.org/abs/1502.03044\n\nRectifier nonlinearities improve neural network acoustic models. A L Maas, A Y Hannun, A Y Ng, Proc. icml. icmlCiteseer303A. L. Maas, A. Y. Hannun, A. Y. Ng et al., \"Rectifier nonlinearities improve neural network acoustic models,\" in Proc. icml, vol. 30, no. 1. Citeseer, 2013, p. 3.\n\nMultispectral pedestrian detection via simultaneous detection and segmentation. C Li, D Song, R Tong, M Tang, abs/1808.04818CoRR. C. Li, D. Song, R. Tong, and M. Tang, \"Multispectral pedestrian detection via simultaneous detection and segmentation,\" CoRR, vol. abs/1808.04818, 2018. [Online]. Available: http://arxiv.org/abs/1808.048 18\n\nMultispectral deep neural networks for pedestrian detection. J Liu, S Zhang, S Wang, D N Metaxas, abs/1611.02644CoRR. J. Liu, S. Zhang, S. Wang, and D. N. Metaxas, \"Multispectral deep neural networks for pedestrian detection,\" CoRR, vol. abs/1611.02644, 2016. [Online]. Available: http://arxiv.org/abs/1611.02644\n\nThe problem of overfitting. D M Hawkins, 10.1021/ci0342472Journal of Chemical Information and Computer Sciences. 441D. M. Hawkins, \"The problem of overfitting,\" Journal of Chemical Information and Computer Sciences, vol. 44, no. 1, pp. 1-12, 2004. [Online]. Available: https://doi.org/10.1021/ci0342472\n\nImproved regularization of convolutional neural networks with cutout. T Devries, G W Taylor, abs/1708.04552CoRR. T. Devries and G. W. Taylor, \"Improved regularization of convolutional neural networks with cutout,\" CoRR, vol. abs/1708.04552, 2017. [Online].\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. S Yun, D Han, S J Oh, S Chun, J Choe, Y Yoo, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, \"Cutmix: Regularization strategy to train strong classifiers with localizable features,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\n\nKeepaugment: A simple information-preserving data augmentation approach. C Gong, D Wang, M Li, V Chandra, Q Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)C. Gong, D. Wang, M. Li, V. Chandra, and Q. Liu, \"Keepaugment: A sim- ple information-preserving data augmentation approach,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021, pp. 1055-1064.\n\n3d object proposals for accurate object class detection. X Chen, K Kundu, Y Zhu, A G Berneshawi, H Ma, S Fidler, R Urtasun, Advances in Neural Information Processing Systems. CiteseerX. Chen, K. Kundu, Y. Zhu, A. G. Berneshawi, H. Ma, S. Fidler, and R. Urtasun, \"3d object proposals for accurate object class detection,\" in Advances in Neural Information Processing Systems. Citeseer, 2015, pp. 424-432.\n\nSingle-shot refinement neural network for object detection. S Zhang, L Wen, X Bian, Z Lei, S Z Li, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, \"Single-shot refinement neural network for object detection,\" in Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), June 2018.\n\nShe received her B.S. degree in electronic science and technology from Henan Institute of Engineering. SHASHA LI was born in Luoyang City. Henan Province, China; Zhengzhou, China; Kaifeng, ChinaShe is currently pursuing the M.S. degree in Optical Engineering at Henan university. Her research interests include computer vision, image processing, and object detectionSHASHA LI was born in Luoyang City, Henan Province, China, in 1996. She received her B.S. degree in electronic science and technology from Henan Institute of Engineering, Zhengzhou, China, in 2019. She is currently pursuing the M.S. degree in Optical Engineering at Henan univer- sity, Kaifeng, China. Her research interests include computer vision, image processing, and object detection.\n\nhe was a lecturer at School of Physics and Electronics Henan University. Since 2012, he has been an assistant professor. He presided over and participated in a number of National Natural Science and Technology Funds, and took part in Shaanxi Province Key Science and Technology Innovation Team Project. He has applied for 6 national invention patents, and published more than 30 papers in Multimedia Tools & Applications. He received his MS degrees in circuit and system from Guangxi Normal University in 2005 and Ph.D. degree in communication and information system from Xidian University in 2017. Henan Province, ChinaJournal of Zhengzhou University, Journal of Henan UniversityOptical Engineering. His research interests are image processing and artificial intelligence. His email is lyj@henu.edu.cnYONGJUN LI was born in Kaifeng City, Henan Province, China in 1977. He received his MS degrees in circuit and system from Guangxi Nor- mal University in 2005 and Ph.D. degree in com- munication and information system from Xidian University in 2017. From 2005 to 2008, he was a lecturer at School of Physics and Electronics Henan University. Since 2012, he has been an assistant professor. He presided over and participated in a number of National Natural Science and Technology Funds, and took part in Shaanxi Province Key Science and Technology Innovation Team Project. He has applied for 6 national invention patents, and published more than 30 pa- pers in Multimedia Tools & Applications, Optical Engineering, Journal of Electronic Imaging, Journal of Zhengzhou University, Journal of Henan University, etc. His research interests are image processing and artificial intelligence. His email is lyj@henu.edu.cn.\n\nHe received the B.S. degree from Hainan University, China. Yao Li Was, Henan, Chinain 2020. He is currently pursuing the M.S. degree with the Henan University. His current research interest includes computer vision, image processing, and deep learningYAO LI was born in Henan, China, in 1998. He received the B.S. degree from Hainan University, China, in 2020. He is currently pursuing the M.S. degree with the Henan University. His current research interest includes computer vision, image processing, and deep learning.\n\nHe is currently pursuing the M.S. degree in optical engineering with Henan University. Menjun Li Was Born In, Shangqiu, Henan, China; Kaifeng, ChinaHis research interests include computer vision and image super-resolutionMENJUN LI was born in Shangqiu, Henan, China, in 1995. He is currently pursuing the M.S. degree in optical engineering with Henan Univer- sity, Kaifeng, China. His research interests include computer vision and image super-resolution.\n\nSince 2005, she has been a teacher in Hunan University of Arts and Sciences, and she has published many academic papers. XIAORONG XU was born in Weinan City. Shanxi Province, China; Guilin, ChinaShe received her M.S. degree in computer software and theory from Guangxi Normal University. Her research interests are image processing and pattern recognitionXIAORONG XU was born in Weinan City, Shanxi Province, China, in 1979. She received her M.S. degree in computer software and theory from Guangxi Normal University, Guilin, China, in 2005. Since 2005, she has been a teacher in Hunan University of Arts and Sciences, and she has pub- lished many academic papers. Her research inter- ests are image processing and pattern recognition.\n", "annotations": {"author": "[{\"end\":75,\"start\":4},{\"end\":179,\"start\":76}]", "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": "[{\"end\":74,\"start\":5},{\"end\":178,\"start\":77}]", "title": null, "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":457,\"start\":454},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":481,\"start\":478},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":499,\"start\":496},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":521,\"start\":518},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":548,\"start\":545},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":571,\"start\":568},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1580,\"start\":1577},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1612,\"start\":1609},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1643,\"start\":1640},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2321,\"start\":2317},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2380,\"start\":2376},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2405,\"start\":2401},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2427,\"start\":2423},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2521,\"start\":2517},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2538,\"start\":2534},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2561,\"start\":2557},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2679,\"start\":2675},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2685,\"start\":2681},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2710,\"start\":2706},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2716,\"start\":2712},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3494,\"start\":3490},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4146,\"start\":4142},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4420,\"start\":4416},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4931,\"start\":4927},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5090,\"start\":5086},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5174,\"start\":5170},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6653,\"start\":6649},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7107,\"start\":7104},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7486,\"start\":7483},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7833,\"start\":7830},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8599,\"start\":8595},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8643,\"start\":8639},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8685,\"start\":8681},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8842,\"start\":8838},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8859,\"start\":8855},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9315,\"start\":9311},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9326,\"start\":9322},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9343,\"start\":9339},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10054,\"start\":10050},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10144,\"start\":10140},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10412,\"start\":10408},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10434,\"start\":10430},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10690,\"start\":10686},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10707,\"start\":10703},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10730,\"start\":10726},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10801,\"start\":10797},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11156,\"start\":11152},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11346,\"start\":11343},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11746,\"start\":11742},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12160,\"start\":12156},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12173,\"start\":12169},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12190,\"start\":12186},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12241,\"start\":12237},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12514,\"start\":12510},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12696,\"start\":12692},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12825,\"start\":12821},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12938,\"start\":12934},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12944,\"start\":12940},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12999,\"start\":12995},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13447,\"start\":13443},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13708,\"start\":13704},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":14030,\"start\":14026},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14453,\"start\":14450},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14458,\"start\":14455},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14464,\"start\":14460},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14470,\"start\":14466},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14498,\"start\":14494},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14504,\"start\":14500},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":17490,\"start\":17486},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":17780,\"start\":17776},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":18305,\"start\":18301},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":21825,\"start\":21821},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22150,\"start\":22146},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23496,\"start\":23493},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24073,\"start\":24070},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":24130,\"start\":24126},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25797,\"start\":25793},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27517,\"start\":27514},{\"end\":29365,\"start\":29363},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":34597,\"start\":34593},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":38704,\"start\":38700},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":38717,\"start\":38713},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":38736,\"start\":38732},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":38811,\"start\":38807},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":38817,\"start\":38813},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39865,\"start\":39861},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":44817,\"start\":44813},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":45615,\"start\":45611},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":45824,\"start\":45820},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":45861,\"start\":45857},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":54599,\"start\":54595}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":49203,\"start\":48783},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49450,\"start\":49204},{\"attributes\":{\"id\":\"fig_2\"},\"end\":49609,\"start\":49451},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50092,\"start\":49610},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50426,\"start\":50093},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50866,\"start\":50427},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51305,\"start\":50867},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51561,\"start\":51306},{\"attributes\":{\"id\":\"fig_9\"},\"end\":51948,\"start\":51562},{\"attributes\":{\"id\":\"fig_10\"},\"end\":52183,\"start\":51949},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":52620,\"start\":52184},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":53303,\"start\":52621},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":53765,\"start\":53304},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":53998,\"start\":53766},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":54405,\"start\":53999},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":54986,\"start\":54406}]", "paragraph": "[{\"end\":791,\"start\":199},{\"end\":1968,\"start\":793},{\"end\":3321,\"start\":1970},{\"end\":5360,\"start\":3323},{\"end\":6473,\"start\":5362},{\"end\":6781,\"start\":6494},{\"end\":8484,\"start\":6826},{\"end\":9104,\"start\":8486},{\"end\":10055,\"start\":9154},{\"end\":11678,\"start\":10057},{\"end\":14323,\"start\":11680},{\"end\":15991,\"start\":14325},{\"end\":16978,\"start\":16040},{\"end\":18740,\"start\":16980},{\"end\":20584,\"start\":18767},{\"end\":21639,\"start\":20586},{\"end\":23853,\"start\":21675},{\"end\":24183,\"start\":23938},{\"end\":24963,\"start\":24185},{\"end\":25210,\"start\":25028},{\"end\":25565,\"start\":25212},{\"end\":27727,\"start\":25601},{\"end\":28702,\"start\":27755},{\"end\":30210,\"start\":28749},{\"end\":32317,\"start\":30212},{\"end\":33297,\"start\":32319},{\"end\":35951,\"start\":33299},{\"end\":36821,\"start\":35992},{\"end\":37269,\"start\":36843},{\"end\":37928,\"start\":37271},{\"end\":40579,\"start\":37980},{\"end\":41141,\"start\":40634},{\"end\":41595,\"start\":41143},{\"end\":42394,\"start\":41641},{\"end\":43005,\"start\":42421},{\"end\":43839,\"start\":43007},{\"end\":44521,\"start\":43841},{\"end\":46588,\"start\":44574},{\"end\":48430,\"start\":46606},{\"end\":48782,\"start\":48432}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":23937,\"start\":23854},{\"attributes\":{\"id\":\"formula_1\"},\"end\":25027,\"start\":24964},{\"attributes\":{\"id\":\"formula_2\"},\"end\":35991,\"start\":35952},{\"attributes\":{\"id\":\"formula_3\"},\"end\":40633,\"start\":40580},{\"attributes\":{\"id\":\"formula_4\"},\"end\":41640,\"start\":41596},{\"attributes\":{\"id\":\"formula_5\"},\"end\":42420,\"start\":42395}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29524,\"start\":29517},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30222,\"start\":30215},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31143,\"start\":31136},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":33083,\"start\":33076},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":37471,\"start\":37464},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":43122,\"start\":43115},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":45487,\"start\":45480}]", "section_header": "[{\"end\":197,\"start\":182},{\"end\":6492,\"start\":6476},{\"end\":6824,\"start\":6784},{\"end\":9152,\"start\":9107},{\"end\":16038,\"start\":15994},{\"end\":18765,\"start\":18743},{\"end\":21673,\"start\":21642},{\"end\":25599,\"start\":25568},{\"end\":27753,\"start\":27730},{\"end\":28747,\"start\":28705},{\"end\":36841,\"start\":36824},{\"end\":37978,\"start\":37931},{\"end\":44572,\"start\":44524},{\"end\":46604,\"start\":46591},{\"end\":48794,\"start\":48784},{\"end\":49215,\"start\":49205},{\"end\":49462,\"start\":49452},{\"end\":49621,\"start\":49611},{\"end\":50104,\"start\":50094},{\"end\":50438,\"start\":50428},{\"end\":50878,\"start\":50868},{\"end\":51317,\"start\":51307},{\"end\":51573,\"start\":51563},{\"end\":51961,\"start\":51950},{\"end\":52631,\"start\":52622},{\"end\":53314,\"start\":53305},{\"end\":53776,\"start\":53767},{\"end\":54009,\"start\":54000},{\"end\":54416,\"start\":54407}]", "table": "[{\"end\":52620,\"start\":52266},{\"end\":53303,\"start\":52803},{\"end\":53765,\"start\":53552},{\"end\":53998,\"start\":53841},{\"end\":54405,\"start\":54114},{\"end\":54986,\"start\":54710}]", "figure_caption": "[{\"end\":49203,\"start\":48796},{\"end\":49450,\"start\":49217},{\"end\":49609,\"start\":49464},{\"end\":50092,\"start\":49623},{\"end\":50426,\"start\":50106},{\"end\":50866,\"start\":50440},{\"end\":51305,\"start\":50880},{\"end\":51561,\"start\":51319},{\"end\":51948,\"start\":51575},{\"end\":52183,\"start\":51964},{\"end\":52266,\"start\":52186},{\"end\":52803,\"start\":52633},{\"end\":53552,\"start\":53316},{\"end\":53841,\"start\":53778},{\"end\":54114,\"start\":54011},{\"end\":54710,\"start\":54418}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16789,\"start\":16781},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16991,\"start\":16983},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20118,\"start\":20110},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20597,\"start\":20589},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21035,\"start\":21027},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22665,\"start\":22657},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22746,\"start\":22738},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22768,\"start\":22760},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26318,\"start\":26310},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26682,\"start\":26674},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33391,\"start\":33383},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":35119,\"start\":35111},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":35245,\"start\":35237},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":36081,\"start\":36073},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":39840,\"start\":39832},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":41717,\"start\":41709},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41867,\"start\":41858},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43933,\"start\":43924}]", "bib_author_first_name": "[{\"end\":55253,\"start\":55252},{\"end\":55255,\"start\":55254},{\"end\":55268,\"start\":55267},{\"end\":55282,\"start\":55281},{\"end\":55284,\"start\":55283},{\"end\":55301,\"start\":55297},{\"end\":55323,\"start\":55322},{\"end\":55335,\"start\":55334},{\"end\":55347,\"start\":55346},{\"end\":55883,\"start\":55882},{\"end\":55892,\"start\":55891},{\"end\":55899,\"start\":55898},{\"end\":55907,\"start\":55906},{\"end\":55918,\"start\":55917},{\"end\":55929,\"start\":55928},{\"end\":55946,\"start\":55945},{\"end\":56454,\"start\":56453},{\"end\":56465,\"start\":56464},{\"end\":56478,\"start\":56477},{\"end\":56480,\"start\":56479},{\"end\":56497,\"start\":56496},{\"end\":56505,\"start\":56504},{\"end\":56886,\"start\":56885},{\"end\":56894,\"start\":56893},{\"end\":56902,\"start\":56901},{\"end\":56904,\"start\":56903},{\"end\":56911,\"start\":56910},{\"end\":56913,\"start\":56912},{\"end\":56921,\"start\":56920},{\"end\":56923,\"start\":56922},{\"end\":57273,\"start\":57272},{\"end\":57286,\"start\":57285},{\"end\":57714,\"start\":57713},{\"end\":57720,\"start\":57719},{\"end\":57728,\"start\":57727},{\"end\":58319,\"start\":58318},{\"end\":58327,\"start\":58326},{\"end\":58621,\"start\":58620},{\"end\":58623,\"start\":58622},{\"end\":58631,\"start\":58630},{\"end\":58635,\"start\":58632},{\"end\":59029,\"start\":59028},{\"end\":59228,\"start\":59227},{\"end\":59234,\"start\":59233},{\"end\":59243,\"start\":59242},{\"end\":59245,\"start\":59244},{\"end\":59460,\"start\":59459},{\"end\":59466,\"start\":59465},{\"end\":59475,\"start\":59474},{\"end\":59482,\"start\":59481},{\"end\":59842,\"start\":59841},{\"end\":59848,\"start\":59847},{\"end\":59860,\"start\":59859},{\"end\":59870,\"start\":59869},{\"end\":60229,\"start\":60228},{\"end\":60237,\"start\":60236},{\"end\":60239,\"start\":60238},{\"end\":60246,\"start\":60245},{\"end\":60252,\"start\":60251},{\"end\":60609,\"start\":60608},{\"end\":60621,\"start\":60620},{\"end\":60632,\"start\":60631},{\"end\":60643,\"start\":60642},{\"end\":61055,\"start\":61054},{\"end\":61406,\"start\":61405},{\"end\":61413,\"start\":61412},{\"end\":61419,\"start\":61418},{\"end\":61431,\"start\":61430},{\"end\":61765,\"start\":61764},{\"end\":61772,\"start\":61771},{\"end\":61784,\"start\":61783},{\"end\":61793,\"start\":61792},{\"end\":61804,\"start\":61803},{\"end\":61815,\"start\":61811},{\"end\":61821,\"start\":61820},{\"end\":61823,\"start\":61822},{\"end\":62236,\"start\":62235},{\"end\":62242,\"start\":62241},{\"end\":62249,\"start\":62248},{\"end\":62258,\"start\":62257},{\"end\":62267,\"start\":62266},{\"end\":62269,\"start\":62268},{\"end\":62539,\"start\":62538},{\"end\":62549,\"start\":62548},{\"end\":62560,\"start\":62559},{\"end\":62572,\"start\":62571},{\"end\":62986,\"start\":62985},{\"end\":62996,\"start\":62995},{\"end\":63367,\"start\":63366},{\"end\":63377,\"start\":63376},{\"end\":63616,\"start\":63615},{\"end\":63631,\"start\":63630},{\"end\":63639,\"start\":63638},{\"end\":63641,\"start\":63640},{\"end\":63842,\"start\":63841},{\"end\":63852,\"start\":63851},{\"end\":63862,\"start\":63861},{\"end\":63886,\"start\":63885},{\"end\":63931,\"start\":63930},{\"end\":63947,\"start\":63946},{\"end\":63981,\"start\":63980},{\"end\":64010,\"start\":64009},{\"end\":64021,\"start\":64020},{\"end\":64037,\"start\":64036},{\"end\":64054,\"start\":64053},{\"end\":64067,\"start\":64066},{\"end\":64075,\"start\":64074},{\"end\":64082,\"start\":64081},{\"end\":64791,\"start\":64787},{\"end\":64802,\"start\":64798},{\"end\":64804,\"start\":64803},{\"end\":64815,\"start\":64811},{\"end\":64824,\"start\":64820},{\"end\":64835,\"start\":64831},{\"end\":64847,\"start\":64843},{\"end\":65327,\"start\":65326},{\"end\":65333,\"start\":65332},{\"end\":65341,\"start\":65340},{\"end\":65347,\"start\":65346},{\"end\":65765,\"start\":65764},{\"end\":65773,\"start\":65772},{\"end\":65780,\"start\":65779},{\"end\":65789,\"start\":65788},{\"end\":65797,\"start\":65796},{\"end\":65799,\"start\":65798},{\"end\":65807,\"start\":65806},{\"end\":65813,\"start\":65812},{\"end\":65815,\"start\":65814},{\"end\":66191,\"start\":66190},{\"end\":66200,\"start\":66196},{\"end\":66765,\"start\":66764},{\"end\":66773,\"start\":66772},{\"end\":66781,\"start\":66780},{\"end\":67127,\"start\":67126},{\"end\":67136,\"start\":67135},{\"end\":67138,\"start\":67137},{\"end\":67494,\"start\":67493},{\"end\":67503,\"start\":67502},{\"end\":67877,\"start\":67876},{\"end\":67887,\"start\":67886},{\"end\":67896,\"start\":67895},{\"end\":67907,\"start\":67906},{\"end\":68239,\"start\":68235},{\"end\":68250,\"start\":68246},{\"end\":68257,\"start\":68256},{\"end\":68635,\"start\":68634},{\"end\":68645,\"start\":68644},{\"end\":68647,\"start\":68646},{\"end\":68842,\"start\":68841},{\"end\":68850,\"start\":68843},{\"end\":68861,\"start\":68860},{\"end\":68870,\"start\":68869},{\"end\":68876,\"start\":68871},{\"end\":69291,\"start\":69290},{\"end\":69300,\"start\":69299},{\"end\":69309,\"start\":69308},{\"end\":69320,\"start\":69319},{\"end\":69830,\"start\":69829},{\"end\":69842,\"start\":69841},{\"end\":70406,\"start\":70405},{\"end\":70413,\"start\":70412},{\"end\":70422,\"start\":70421},{\"end\":70431,\"start\":70430},{\"end\":70439,\"start\":70438},{\"end\":70445,\"start\":70444},{\"end\":70453,\"start\":70452},{\"end\":70465,\"start\":70464},{\"end\":70471,\"start\":70470},{\"end\":70479,\"start\":70478},{\"end\":70487,\"start\":70486},{\"end\":71017,\"start\":71016},{\"end\":71026,\"start\":71025},{\"end\":71028,\"start\":71027},{\"end\":71037,\"start\":71036},{\"end\":71053,\"start\":71052},{\"end\":71068,\"start\":71067},{\"end\":71079,\"start\":71078},{\"end\":71610,\"start\":71609},{\"end\":71625,\"start\":71624},{\"end\":71637,\"start\":71636},{\"end\":71647,\"start\":71646},{\"end\":72181,\"start\":72180},{\"end\":72191,\"start\":72190},{\"end\":72205,\"start\":72204},{\"end\":72472,\"start\":72471},{\"end\":72478,\"start\":72477},{\"end\":72487,\"start\":72486},{\"end\":72724,\"start\":72723},{\"end\":72730,\"start\":72729},{\"end\":72738,\"start\":72737},{\"end\":73134,\"start\":73133},{\"end\":73140,\"start\":73139},{\"end\":73146,\"start\":73145},{\"end\":73152,\"start\":73151},{\"end\":73160,\"start\":73159},{\"end\":73169,\"start\":73168},{\"end\":73431,\"start\":73430},{\"end\":73440,\"start\":73439},{\"end\":73448,\"start\":73447},{\"end\":73455,\"start\":73454},{\"end\":73461,\"start\":73460},{\"end\":73467,\"start\":73466},{\"end\":74028,\"start\":74027},{\"end\":74037,\"start\":74036},{\"end\":74045,\"start\":74044},{\"end\":74052,\"start\":74051},{\"end\":74059,\"start\":74058},{\"end\":74065,\"start\":74064},{\"end\":74071,\"start\":74070},{\"end\":74417,\"start\":74416},{\"end\":74426,\"start\":74425},{\"end\":74435,\"start\":74434},{\"end\":74448,\"start\":74447},{\"end\":74450,\"start\":74449},{\"end\":74870,\"start\":74869},{\"end\":74877,\"start\":74876},{\"end\":74885,\"start\":74884},{\"end\":74892,\"start\":74891},{\"end\":75267,\"start\":75266},{\"end\":75274,\"start\":75273},{\"end\":75282,\"start\":75281},{\"end\":75644,\"start\":75643},{\"end\":75652,\"start\":75651},{\"end\":75659,\"start\":75658},{\"end\":76040,\"start\":76039},{\"end\":76046,\"start\":76045},{\"end\":76055,\"start\":76054},{\"end\":76062,\"start\":76061},{\"end\":76412,\"start\":76411},{\"end\":76420,\"start\":76419},{\"end\":76426,\"start\":76425},{\"end\":76432,\"start\":76431},{\"end\":76441,\"start\":76440},{\"end\":76449,\"start\":76448},{\"end\":76836,\"start\":76835},{\"end\":76847,\"start\":76846},{\"end\":77161,\"start\":77160},{\"end\":77167,\"start\":77166},{\"end\":77173,\"start\":77172},{\"end\":77182,\"start\":77181},{\"end\":77189,\"start\":77188},{\"end\":77191,\"start\":77190},{\"end\":77204,\"start\":77203},{\"end\":77221,\"start\":77220},{\"end\":77223,\"start\":77222},{\"end\":77232,\"start\":77231},{\"end\":77589,\"start\":77588},{\"end\":77591,\"start\":77590},{\"end\":77599,\"start\":77598},{\"end\":77601,\"start\":77600},{\"end\":77611,\"start\":77610},{\"end\":77613,\"start\":77612},{\"end\":77890,\"start\":77889},{\"end\":77896,\"start\":77895},{\"end\":77904,\"start\":77903},{\"end\":77912,\"start\":77911},{\"end\":78209,\"start\":78208},{\"end\":78216,\"start\":78215},{\"end\":78225,\"start\":78224},{\"end\":78233,\"start\":78232},{\"end\":78235,\"start\":78234},{\"end\":78490,\"start\":78489},{\"end\":78492,\"start\":78491},{\"end\":78836,\"start\":78835},{\"end\":78847,\"start\":78846},{\"end\":78849,\"start\":78848},{\"end\":79111,\"start\":79110},{\"end\":79118,\"start\":79117},{\"end\":79125,\"start\":79124},{\"end\":79127,\"start\":79126},{\"end\":79133,\"start\":79132},{\"end\":79141,\"start\":79140},{\"end\":79149,\"start\":79148},{\"end\":79615,\"start\":79614},{\"end\":79623,\"start\":79622},{\"end\":79631,\"start\":79630},{\"end\":79637,\"start\":79636},{\"end\":79648,\"start\":79647},{\"end\":80121,\"start\":80120},{\"end\":80129,\"start\":80128},{\"end\":80138,\"start\":80137},{\"end\":80145,\"start\":80144},{\"end\":80147,\"start\":80146},{\"end\":80161,\"start\":80160},{\"end\":80167,\"start\":80166},{\"end\":80177,\"start\":80176},{\"end\":80529,\"start\":80528},{\"end\":80538,\"start\":80537},{\"end\":80545,\"start\":80544},{\"end\":80553,\"start\":80552},{\"end\":80560,\"start\":80559},{\"end\":80562,\"start\":80561}]", "bib_author_last_name": "[{\"end\":55265,\"start\":55256},{\"end\":55279,\"start\":55269},{\"end\":55295,\"start\":55285},{\"end\":55320,\"start\":55302},{\"end\":55332,\"start\":55324},{\"end\":55344,\"start\":55336},{\"end\":55354,\"start\":55348},{\"end\":55889,\"start\":55884},{\"end\":55896,\"start\":55893},{\"end\":55904,\"start\":55900},{\"end\":55915,\"start\":55908},{\"end\":55926,\"start\":55919},{\"end\":55943,\"start\":55930},{\"end\":55969,\"start\":55947},{\"end\":56462,\"start\":56455},{\"end\":56475,\"start\":56466},{\"end\":56494,\"start\":56481},{\"end\":56502,\"start\":56498},{\"end\":56509,\"start\":56506},{\"end\":56891,\"start\":56887},{\"end\":56899,\"start\":56895},{\"end\":56908,\"start\":56905},{\"end\":56918,\"start\":56914},{\"end\":56927,\"start\":56924},{\"end\":57283,\"start\":57274},{\"end\":57296,\"start\":57287},{\"end\":57717,\"start\":57715},{\"end\":57725,\"start\":57721},{\"end\":57734,\"start\":57729},{\"end\":58324,\"start\":58320},{\"end\":58332,\"start\":58328},{\"end\":58628,\"start\":58624},{\"end\":58639,\"start\":58636},{\"end\":59034,\"start\":59030},{\"end\":59231,\"start\":59229},{\"end\":59240,\"start\":59235},{\"end\":59249,\"start\":59246},{\"end\":59463,\"start\":59461},{\"end\":59472,\"start\":59467},{\"end\":59479,\"start\":59476},{\"end\":59486,\"start\":59483},{\"end\":59845,\"start\":59843},{\"end\":59857,\"start\":59849},{\"end\":59867,\"start\":59861},{\"end\":59879,\"start\":59871},{\"end\":60234,\"start\":60230},{\"end\":60243,\"start\":60240},{\"end\":60249,\"start\":60247},{\"end\":60257,\"start\":60253},{\"end\":60618,\"start\":60610},{\"end\":60629,\"start\":60622},{\"end\":60640,\"start\":60633},{\"end\":60649,\"start\":60644},{\"end\":61064,\"start\":61056},{\"end\":61410,\"start\":61407},{\"end\":61416,\"start\":61414},{\"end\":61428,\"start\":61420},{\"end\":61435,\"start\":61432},{\"end\":61769,\"start\":61766},{\"end\":61781,\"start\":61773},{\"end\":61790,\"start\":61785},{\"end\":61801,\"start\":61794},{\"end\":61809,\"start\":61805},{\"end\":61818,\"start\":61816},{\"end\":61828,\"start\":61824},{\"end\":62239,\"start\":62237},{\"end\":62246,\"start\":62243},{\"end\":62255,\"start\":62250},{\"end\":62264,\"start\":62259},{\"end\":62274,\"start\":62270},{\"end\":62546,\"start\":62540},{\"end\":62557,\"start\":62550},{\"end\":62569,\"start\":62561},{\"end\":62580,\"start\":62573},{\"end\":62993,\"start\":62987},{\"end\":63004,\"start\":62997},{\"end\":63374,\"start\":63368},{\"end\":63385,\"start\":63378},{\"end\":63628,\"start\":63617},{\"end\":63636,\"start\":63632},{\"end\":63646,\"start\":63642},{\"end\":63849,\"start\":63843},{\"end\":63859,\"start\":63853},{\"end\":63870,\"start\":63863},{\"end\":63883,\"start\":63872},{\"end\":63902,\"start\":63887},{\"end\":63911,\"start\":63904},{\"end\":63921,\"start\":63913},{\"end\":63928,\"start\":63923},{\"end\":63944,\"start\":63932},{\"end\":63957,\"start\":63948},{\"end\":63966,\"start\":63959},{\"end\":63972,\"start\":63968},{\"end\":63978,\"start\":63974},{\"end\":63989,\"start\":63982},{\"end\":63997,\"start\":63991},{\"end\":64007,\"start\":63999},{\"end\":64018,\"start\":64011},{\"end\":64030,\"start\":64022},{\"end\":64034,\"start\":64032},{\"end\":64051,\"start\":64038},{\"end\":64064,\"start\":64055},{\"end\":64072,\"start\":64068},{\"end\":64079,\"start\":64076},{\"end\":64087,\"start\":64083},{\"end\":64091,\"start\":64089},{\"end\":64796,\"start\":64792},{\"end\":64809,\"start\":64805},{\"end\":64818,\"start\":64816},{\"end\":64829,\"start\":64825},{\"end\":64841,\"start\":64836},{\"end\":64851,\"start\":64848},{\"end\":65330,\"start\":65328},{\"end\":65338,\"start\":65334},{\"end\":65344,\"start\":65342},{\"end\":65352,\"start\":65348},{\"end\":65770,\"start\":65766},{\"end\":65777,\"start\":65774},{\"end\":65786,\"start\":65781},{\"end\":65794,\"start\":65790},{\"end\":65804,\"start\":65800},{\"end\":65810,\"start\":65808},{\"end\":65821,\"start\":65816},{\"end\":66194,\"start\":66192},{\"end\":66203,\"start\":66201},{\"end\":66504,\"start\":66495},{\"end\":66770,\"start\":66766},{\"end\":66778,\"start\":66774},{\"end\":66785,\"start\":66782},{\"end\":67133,\"start\":67128},{\"end\":67144,\"start\":67139},{\"end\":67500,\"start\":67495},{\"end\":67510,\"start\":67504},{\"end\":67884,\"start\":67878},{\"end\":67893,\"start\":67888},{\"end\":67904,\"start\":67897},{\"end\":67914,\"start\":67908},{\"end\":68244,\"start\":68240},{\"end\":68254,\"start\":68251},{\"end\":68267,\"start\":68258},{\"end\":68642,\"start\":68636},{\"end\":68656,\"start\":68648},{\"end\":68858,\"start\":68851},{\"end\":68867,\"start\":68862},{\"end\":68883,\"start\":68877},{\"end\":69297,\"start\":69292},{\"end\":69306,\"start\":69301},{\"end\":69317,\"start\":69310},{\"end\":69332,\"start\":69321},{\"end\":69839,\"start\":69831},{\"end\":69852,\"start\":69843},{\"end\":70410,\"start\":70407},{\"end\":70419,\"start\":70414},{\"end\":70428,\"start\":70423},{\"end\":70436,\"start\":70432},{\"end\":70442,\"start\":70440},{\"end\":70450,\"start\":70446},{\"end\":70462,\"start\":70454},{\"end\":70468,\"start\":70466},{\"end\":70476,\"start\":70472},{\"end\":70484,\"start\":70480},{\"end\":70491,\"start\":70488},{\"end\":71023,\"start\":71018},{\"end\":71034,\"start\":71029},{\"end\":71050,\"start\":71038},{\"end\":71065,\"start\":71054},{\"end\":71076,\"start\":71069},{\"end\":71086,\"start\":71080},{\"end\":71622,\"start\":71611},{\"end\":71634,\"start\":71626},{\"end\":71644,\"start\":71638},{\"end\":71663,\"start\":71648},{\"end\":72188,\"start\":72182},{\"end\":72202,\"start\":72192},{\"end\":72211,\"start\":72206},{\"end\":72475,\"start\":72473},{\"end\":72484,\"start\":72479},{\"end\":72491,\"start\":72488},{\"end\":72727,\"start\":72725},{\"end\":72735,\"start\":72731},{\"end\":72742,\"start\":72739},{\"end\":73137,\"start\":73135},{\"end\":73143,\"start\":73141},{\"end\":73149,\"start\":73147},{\"end\":73157,\"start\":73153},{\"end\":73166,\"start\":73161},{\"end\":73172,\"start\":73170},{\"end\":73437,\"start\":73432},{\"end\":73445,\"start\":73441},{\"end\":73452,\"start\":73449},{\"end\":73458,\"start\":73456},{\"end\":73464,\"start\":73462},{\"end\":73471,\"start\":73468},{\"end\":74034,\"start\":74029},{\"end\":74042,\"start\":74038},{\"end\":74049,\"start\":74046},{\"end\":74056,\"start\":74053},{\"end\":74062,\"start\":74060},{\"end\":74068,\"start\":74066},{\"end\":74075,\"start\":74072},{\"end\":74423,\"start\":74418},{\"end\":74432,\"start\":74427},{\"end\":74445,\"start\":74436},{\"end\":74456,\"start\":74451},{\"end\":74874,\"start\":74871},{\"end\":74882,\"start\":74878},{\"end\":74889,\"start\":74886},{\"end\":74895,\"start\":74893},{\"end\":75271,\"start\":75268},{\"end\":75279,\"start\":75275},{\"end\":75286,\"start\":75283},{\"end\":75649,\"start\":75645},{\"end\":75656,\"start\":75653},{\"end\":75664,\"start\":75660},{\"end\":76043,\"start\":76041},{\"end\":76052,\"start\":76047},{\"end\":76059,\"start\":76056},{\"end\":76066,\"start\":76063},{\"end\":76417,\"start\":76413},{\"end\":76423,\"start\":76421},{\"end\":76429,\"start\":76427},{\"end\":76438,\"start\":76433},{\"end\":76446,\"start\":76442},{\"end\":76457,\"start\":76450},{\"end\":76844,\"start\":76837},{\"end\":76856,\"start\":76848},{\"end\":77164,\"start\":77162},{\"end\":77170,\"start\":77168},{\"end\":77179,\"start\":77174},{\"end\":77186,\"start\":77183},{\"end\":77201,\"start\":77192},{\"end\":77218,\"start\":77205},{\"end\":77229,\"start\":77224},{\"end\":77239,\"start\":77233},{\"end\":77596,\"start\":77592},{\"end\":77608,\"start\":77602},{\"end\":77616,\"start\":77614},{\"end\":77893,\"start\":77891},{\"end\":77901,\"start\":77897},{\"end\":77909,\"start\":77905},{\"end\":77917,\"start\":77913},{\"end\":78213,\"start\":78210},{\"end\":78222,\"start\":78217},{\"end\":78230,\"start\":78226},{\"end\":78243,\"start\":78236},{\"end\":78500,\"start\":78493},{\"end\":78844,\"start\":78837},{\"end\":78856,\"start\":78850},{\"end\":79115,\"start\":79112},{\"end\":79122,\"start\":79119},{\"end\":79130,\"start\":79128},{\"end\":79138,\"start\":79134},{\"end\":79146,\"start\":79142},{\"end\":79153,\"start\":79150},{\"end\":79620,\"start\":79616},{\"end\":79628,\"start\":79624},{\"end\":79634,\"start\":79632},{\"end\":79645,\"start\":79638},{\"end\":79652,\"start\":79649},{\"end\":80126,\"start\":80122},{\"end\":80135,\"start\":80130},{\"end\":80142,\"start\":80139},{\"end\":80158,\"start\":80148},{\"end\":80164,\"start\":80162},{\"end\":80174,\"start\":80168},{\"end\":80185,\"start\":80178},{\"end\":80535,\"start\":80530},{\"end\":80542,\"start\":80539},{\"end\":80550,\"start\":80546},{\"end\":80557,\"start\":80554},{\"end\":80565,\"start\":80563},{\"end\":83479,\"start\":83469},{\"end\":84041,\"start\":84020},{\"end\":84051,\"start\":84043}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":126331039},\"end\":55799,\"start\":55149},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4783630},\"end\":56279,\"start\":55801},{\"attributes\":{\"doi\":\"10.1007/s11042-018-5883-y\",\"id\":\"b2\"},\"end\":56360,\"start\":56281},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":221356192},\"end\":56793,\"start\":56362},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":209433989},\"end\":57181,\"start\":56795},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12175574},\"end\":57617,\"start\":57183},{\"attributes\":{\"doi\":\"10.1117/12.2505607\",\"id\":\"b6\",\"matched_paper_id\":115849140},\"end\":58227,\"start\":57619},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":123788345},\"end\":58547,\"start\":58229},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14611769},\"end\":58922,\"start\":58549},{\"attributes\":{\"id\":\"b9\"},\"end\":59170,\"start\":58924},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":199543948},\"end\":59411,\"start\":59172},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":59827,\"start\":59413},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":54465873},\"end\":60168,\"start\":59829},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6593498},\"end\":60524,\"start\":60170},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":215827080},\"end\":61040,\"start\":60526},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206770307},\"end\":61323,\"start\":61042},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10328909},\"end\":61726,\"start\":61325},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2141740},\"end\":62188,\"start\":61728},{\"attributes\":{\"doi\":\"abs/1701.06659\",\"id\":\"b18\",\"matched_paper_id\":7691159},\"end\":62479,\"start\":62190},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206594738},\"end\":62947,\"start\":62481},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":786357},\"end\":63328,\"start\":62949},{\"attributes\":{\"doi\":\"abs/1804.02767\",\"id\":\"b21\",\"matched_paper_id\":4714433},\"end\":63557,\"start\":63330},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":216080778},\"end\":63839,\"start\":63559},{\"attributes\":{\"doi\":\"10.5281/zenodo.4418161\",\"id\":\"b23\"},\"end\":64717,\"start\":63841},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":208310312},\"end\":65297,\"start\":64719},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":80628366},\"end\":65685,\"start\":65299},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3819249},\"end\":66127,\"start\":65687},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5060429},\"end\":66406,\"start\":66129},{\"attributes\":{\"id\":\"b28\"},\"end\":66675,\"start\":66408},{\"attributes\":{\"doi\":\"10.1117/1.JEI.25.4.041010\",\"id\":\"b29\",\"matched_paper_id\":64054421},\"end\":67091,\"start\":66677},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2796017},\"end\":67338,\"start\":67093},{\"attributes\":{\"doi\":\"10.1023/B:VISI.0000013087.49260.fb\",\"id\":\"b31\"},\"end\":67437,\"start\":67340},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206590483},\"end\":67813,\"start\":67439},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206764948},\"end\":68192,\"start\":67815},{\"attributes\":{\"doi\":\"10.1002/asmb.537\",\"id\":\"b34\",\"matched_paper_id\":60673449},\"end\":68589,\"start\":68194},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1836349},\"end\":68768,\"start\":68591},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12431653},\"end\":69215,\"start\":68770},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":12640199},\"end\":69781,\"start\":69217},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":15402687},\"end\":70335,\"start\":69783},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":227162412},\"end\":70954,\"start\":70337},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":118645139},\"end\":71529,\"start\":70956},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":162168555},\"end\":72109,\"start\":71531},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":220668719},\"end\":72386,\"start\":72111},{\"attributes\":{\"id\":\"b43\"},\"end\":72688,\"start\":72388},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":140309863},\"end\":73065,\"start\":72690},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":230513289},\"end\":73354,\"start\":73067},{\"attributes\":{\"doi\":\"pp. 12 993-13 000\",\"id\":\"b46\",\"matched_paper_id\":208158250},\"end\":73917,\"start\":73356},{\"attributes\":{\"doi\":\"abs/2005.03572\",\"id\":\"b47\",\"matched_paper_id\":218538057},\"end\":74354,\"start\":73919},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":15155826},\"end\":74794,\"start\":74356},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":215800156},\"end\":75188,\"start\":74796},{\"attributes\":{\"doi\":\"10.1007/s10489-020-01882-2\",\"id\":\"b50\",\"matched_paper_id\":224977719},\"end\":75569,\"start\":75190},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":225684123},\"end\":75958,\"start\":75571},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":436933},\"end\":76353,\"start\":75960},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":12259703},\"end\":76798,\"start\":76355},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":5057778},\"end\":77080,\"start\":76800},{\"attributes\":{\"doi\":\"abs/1502.03044\",\"id\":\"b55\",\"matched_paper_id\":1055111},\"end\":77521,\"start\":77082},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":16489696},\"end\":77807,\"start\":77523},{\"attributes\":{\"doi\":\"abs/1808.04818\",\"id\":\"b57\",\"matched_paper_id\":52002655},\"end\":78145,\"start\":77809},{\"attributes\":{\"doi\":\"abs/1611.02644\",\"id\":\"b58\",\"matched_paper_id\":5311183},\"end\":78459,\"start\":78147},{\"attributes\":{\"doi\":\"10.1021/ci0342472\",\"id\":\"b59\",\"matched_paper_id\":12440383},\"end\":78763,\"start\":78461},{\"attributes\":{\"doi\":\"abs/1708.04552\",\"id\":\"b60\",\"matched_paper_id\":23714201},\"end\":79021,\"start\":78765},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":152282661},\"end\":79539,\"start\":79023},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":227151236},\"end\":80061,\"start\":79541},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":10236420},\"end\":80466,\"start\":80063},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":7271452},\"end\":80934,\"start\":80468},{\"attributes\":{\"id\":\"b65\"},\"end\":81691,\"start\":80936},{\"attributes\":{\"id\":\"b66\"},\"end\":83408,\"start\":81693},{\"attributes\":{\"id\":\"b67\"},\"end\":83931,\"start\":83410},{\"attributes\":{\"id\":\"b68\"},\"end\":84388,\"start\":83933},{\"attributes\":{\"id\":\"b69\"},\"end\":85125,\"start\":84390}]", "bib_title": "[{\"end\":55250,\"start\":55149},{\"end\":55880,\"start\":55801},{\"end\":56451,\"start\":56362},{\"end\":56883,\"start\":56795},{\"end\":57270,\"start\":57183},{\"end\":57711,\"start\":57619},{\"end\":58316,\"start\":58229},{\"end\":58618,\"start\":58549},{\"end\":59225,\"start\":59172},{\"end\":59457,\"start\":59413},{\"end\":59839,\"start\":59829},{\"end\":60226,\"start\":60170},{\"end\":60606,\"start\":60526},{\"end\":61052,\"start\":61042},{\"end\":61403,\"start\":61325},{\"end\":61762,\"start\":61728},{\"end\":62233,\"start\":62190},{\"end\":62536,\"start\":62481},{\"end\":62983,\"start\":62949},{\"end\":63364,\"start\":63330},{\"end\":63613,\"start\":63559},{\"end\":64785,\"start\":64719},{\"end\":65324,\"start\":65299},{\"end\":65762,\"start\":65687},{\"end\":66188,\"start\":66129},{\"end\":66493,\"start\":66408},{\"end\":66762,\"start\":66677},{\"end\":67124,\"start\":67093},{\"end\":67491,\"start\":67439},{\"end\":67874,\"start\":67815},{\"end\":68233,\"start\":68194},{\"end\":68632,\"start\":68591},{\"end\":68839,\"start\":68770},{\"end\":69288,\"start\":69217},{\"end\":69827,\"start\":69783},{\"end\":70403,\"start\":70337},{\"end\":71014,\"start\":70956},{\"end\":71607,\"start\":71531},{\"end\":72178,\"start\":72111},{\"end\":72469,\"start\":72388},{\"end\":72721,\"start\":72690},{\"end\":73131,\"start\":73067},{\"end\":73428,\"start\":73356},{\"end\":74025,\"start\":73919},{\"end\":74414,\"start\":74356},{\"end\":74867,\"start\":74796},{\"end\":75264,\"start\":75190},{\"end\":75641,\"start\":75571},{\"end\":76037,\"start\":75960},{\"end\":76409,\"start\":76355},{\"end\":76833,\"start\":76800},{\"end\":77158,\"start\":77082},{\"end\":77586,\"start\":77523},{\"end\":77887,\"start\":77809},{\"end\":78206,\"start\":78147},{\"end\":78487,\"start\":78461},{\"end\":78833,\"start\":78765},{\"end\":79108,\"start\":79023},{\"end\":79612,\"start\":79541},{\"end\":80118,\"start\":80063},{\"end\":80526,\"start\":80468},{\"end\":81037,\"start\":80936},{\"end\":82113,\"start\":81693},{\"end\":84509,\"start\":84390}]", "bib_author": "[{\"end\":55267,\"start\":55252},{\"end\":55281,\"start\":55267},{\"end\":55297,\"start\":55281},{\"end\":55322,\"start\":55297},{\"end\":55334,\"start\":55322},{\"end\":55346,\"start\":55334},{\"end\":55356,\"start\":55346},{\"end\":55891,\"start\":55882},{\"end\":55898,\"start\":55891},{\"end\":55906,\"start\":55898},{\"end\":55917,\"start\":55906},{\"end\":55928,\"start\":55917},{\"end\":55945,\"start\":55928},{\"end\":55971,\"start\":55945},{\"end\":56464,\"start\":56453},{\"end\":56477,\"start\":56464},{\"end\":56496,\"start\":56477},{\"end\":56504,\"start\":56496},{\"end\":56511,\"start\":56504},{\"end\":56893,\"start\":56885},{\"end\":56901,\"start\":56893},{\"end\":56910,\"start\":56901},{\"end\":56920,\"start\":56910},{\"end\":56929,\"start\":56920},{\"end\":57285,\"start\":57272},{\"end\":57298,\"start\":57285},{\"end\":57719,\"start\":57713},{\"end\":57727,\"start\":57719},{\"end\":57736,\"start\":57727},{\"end\":58326,\"start\":58318},{\"end\":58334,\"start\":58326},{\"end\":58630,\"start\":58620},{\"end\":58641,\"start\":58630},{\"end\":59036,\"start\":59028},{\"end\":59233,\"start\":59227},{\"end\":59242,\"start\":59233},{\"end\":59251,\"start\":59242},{\"end\":59465,\"start\":59459},{\"end\":59474,\"start\":59465},{\"end\":59481,\"start\":59474},{\"end\":59488,\"start\":59481},{\"end\":59847,\"start\":59841},{\"end\":59859,\"start\":59847},{\"end\":59869,\"start\":59859},{\"end\":59881,\"start\":59869},{\"end\":60236,\"start\":60228},{\"end\":60245,\"start\":60236},{\"end\":60251,\"start\":60245},{\"end\":60259,\"start\":60251},{\"end\":60620,\"start\":60608},{\"end\":60631,\"start\":60620},{\"end\":60642,\"start\":60631},{\"end\":60651,\"start\":60642},{\"end\":61066,\"start\":61054},{\"end\":61412,\"start\":61405},{\"end\":61418,\"start\":61412},{\"end\":61430,\"start\":61418},{\"end\":61437,\"start\":61430},{\"end\":61771,\"start\":61764},{\"end\":61783,\"start\":61771},{\"end\":61792,\"start\":61783},{\"end\":61803,\"start\":61792},{\"end\":61811,\"start\":61803},{\"end\":61820,\"start\":61811},{\"end\":61830,\"start\":61820},{\"end\":62241,\"start\":62235},{\"end\":62248,\"start\":62241},{\"end\":62257,\"start\":62248},{\"end\":62266,\"start\":62257},{\"end\":62276,\"start\":62266},{\"end\":62548,\"start\":62538},{\"end\":62559,\"start\":62548},{\"end\":62571,\"start\":62559},{\"end\":62582,\"start\":62571},{\"end\":62995,\"start\":62985},{\"end\":63006,\"start\":62995},{\"end\":63376,\"start\":63366},{\"end\":63387,\"start\":63376},{\"end\":63630,\"start\":63615},{\"end\":63638,\"start\":63630},{\"end\":63648,\"start\":63638},{\"end\":63851,\"start\":63841},{\"end\":63861,\"start\":63851},{\"end\":63872,\"start\":63861},{\"end\":63885,\"start\":63872},{\"end\":63904,\"start\":63885},{\"end\":63913,\"start\":63904},{\"end\":63923,\"start\":63913},{\"end\":63930,\"start\":63923},{\"end\":63946,\"start\":63930},{\"end\":63959,\"start\":63946},{\"end\":63968,\"start\":63959},{\"end\":63974,\"start\":63968},{\"end\":63980,\"start\":63974},{\"end\":63991,\"start\":63980},{\"end\":63999,\"start\":63991},{\"end\":64009,\"start\":63999},{\"end\":64020,\"start\":64009},{\"end\":64032,\"start\":64020},{\"end\":64036,\"start\":64032},{\"end\":64053,\"start\":64036},{\"end\":64066,\"start\":64053},{\"end\":64074,\"start\":64066},{\"end\":64081,\"start\":64074},{\"end\":64089,\"start\":64081},{\"end\":64093,\"start\":64089},{\"end\":64798,\"start\":64787},{\"end\":64811,\"start\":64798},{\"end\":64820,\"start\":64811},{\"end\":64831,\"start\":64820},{\"end\":64843,\"start\":64831},{\"end\":64853,\"start\":64843},{\"end\":65332,\"start\":65326},{\"end\":65340,\"start\":65332},{\"end\":65346,\"start\":65340},{\"end\":65354,\"start\":65346},{\"end\":65772,\"start\":65764},{\"end\":65779,\"start\":65772},{\"end\":65788,\"start\":65779},{\"end\":65796,\"start\":65788},{\"end\":65806,\"start\":65796},{\"end\":65812,\"start\":65806},{\"end\":65823,\"start\":65812},{\"end\":66196,\"start\":66190},{\"end\":66205,\"start\":66196},{\"end\":66506,\"start\":66495},{\"end\":66772,\"start\":66764},{\"end\":66780,\"start\":66772},{\"end\":66787,\"start\":66780},{\"end\":67135,\"start\":67126},{\"end\":67146,\"start\":67135},{\"end\":67502,\"start\":67493},{\"end\":67512,\"start\":67502},{\"end\":67886,\"start\":67876},{\"end\":67895,\"start\":67886},{\"end\":67906,\"start\":67895},{\"end\":67916,\"start\":67906},{\"end\":68246,\"start\":68235},{\"end\":68256,\"start\":68246},{\"end\":68269,\"start\":68256},{\"end\":68644,\"start\":68634},{\"end\":68658,\"start\":68644},{\"end\":68860,\"start\":68841},{\"end\":68869,\"start\":68860},{\"end\":68885,\"start\":68869},{\"end\":69299,\"start\":69290},{\"end\":69308,\"start\":69299},{\"end\":69319,\"start\":69308},{\"end\":69334,\"start\":69319},{\"end\":69841,\"start\":69829},{\"end\":69854,\"start\":69841},{\"end\":70412,\"start\":70405},{\"end\":70421,\"start\":70412},{\"end\":70430,\"start\":70421},{\"end\":70438,\"start\":70430},{\"end\":70444,\"start\":70438},{\"end\":70452,\"start\":70444},{\"end\":70464,\"start\":70452},{\"end\":70470,\"start\":70464},{\"end\":70478,\"start\":70470},{\"end\":70486,\"start\":70478},{\"end\":70493,\"start\":70486},{\"end\":71025,\"start\":71016},{\"end\":71036,\"start\":71025},{\"end\":71052,\"start\":71036},{\"end\":71067,\"start\":71052},{\"end\":71078,\"start\":71067},{\"end\":71088,\"start\":71078},{\"end\":71624,\"start\":71609},{\"end\":71636,\"start\":71624},{\"end\":71646,\"start\":71636},{\"end\":71665,\"start\":71646},{\"end\":72190,\"start\":72180},{\"end\":72204,\"start\":72190},{\"end\":72213,\"start\":72204},{\"end\":72477,\"start\":72471},{\"end\":72486,\"start\":72477},{\"end\":72493,\"start\":72486},{\"end\":72729,\"start\":72723},{\"end\":72737,\"start\":72729},{\"end\":72744,\"start\":72737},{\"end\":73139,\"start\":73133},{\"end\":73145,\"start\":73139},{\"end\":73151,\"start\":73145},{\"end\":73159,\"start\":73151},{\"end\":73168,\"start\":73159},{\"end\":73174,\"start\":73168},{\"end\":73439,\"start\":73430},{\"end\":73447,\"start\":73439},{\"end\":73454,\"start\":73447},{\"end\":73460,\"start\":73454},{\"end\":73466,\"start\":73460},{\"end\":73473,\"start\":73466},{\"end\":74036,\"start\":74027},{\"end\":74044,\"start\":74036},{\"end\":74051,\"start\":74044},{\"end\":74058,\"start\":74051},{\"end\":74064,\"start\":74058},{\"end\":74070,\"start\":74064},{\"end\":74077,\"start\":74070},{\"end\":74425,\"start\":74416},{\"end\":74434,\"start\":74425},{\"end\":74447,\"start\":74434},{\"end\":74458,\"start\":74447},{\"end\":74876,\"start\":74869},{\"end\":74884,\"start\":74876},{\"end\":74891,\"start\":74884},{\"end\":74897,\"start\":74891},{\"end\":75273,\"start\":75266},{\"end\":75281,\"start\":75273},{\"end\":75288,\"start\":75281},{\"end\":75651,\"start\":75643},{\"end\":75658,\"start\":75651},{\"end\":75666,\"start\":75658},{\"end\":76045,\"start\":76039},{\"end\":76054,\"start\":76045},{\"end\":76061,\"start\":76054},{\"end\":76068,\"start\":76061},{\"end\":76419,\"start\":76411},{\"end\":76425,\"start\":76419},{\"end\":76431,\"start\":76425},{\"end\":76440,\"start\":76431},{\"end\":76448,\"start\":76440},{\"end\":76459,\"start\":76448},{\"end\":76846,\"start\":76835},{\"end\":76858,\"start\":76846},{\"end\":77166,\"start\":77160},{\"end\":77172,\"start\":77166},{\"end\":77181,\"start\":77172},{\"end\":77188,\"start\":77181},{\"end\":77203,\"start\":77188},{\"end\":77220,\"start\":77203},{\"end\":77231,\"start\":77220},{\"end\":77241,\"start\":77231},{\"end\":77598,\"start\":77588},{\"end\":77610,\"start\":77598},{\"end\":77618,\"start\":77610},{\"end\":77895,\"start\":77889},{\"end\":77903,\"start\":77895},{\"end\":77911,\"start\":77903},{\"end\":77919,\"start\":77911},{\"end\":78215,\"start\":78208},{\"end\":78224,\"start\":78215},{\"end\":78232,\"start\":78224},{\"end\":78245,\"start\":78232},{\"end\":78502,\"start\":78489},{\"end\":78846,\"start\":78835},{\"end\":78858,\"start\":78846},{\"end\":79117,\"start\":79110},{\"end\":79124,\"start\":79117},{\"end\":79132,\"start\":79124},{\"end\":79140,\"start\":79132},{\"end\":79148,\"start\":79140},{\"end\":79155,\"start\":79148},{\"end\":79622,\"start\":79614},{\"end\":79630,\"start\":79622},{\"end\":79636,\"start\":79630},{\"end\":79647,\"start\":79636},{\"end\":79654,\"start\":79647},{\"end\":80128,\"start\":80120},{\"end\":80137,\"start\":80128},{\"end\":80144,\"start\":80137},{\"end\":80160,\"start\":80144},{\"end\":80166,\"start\":80160},{\"end\":80176,\"start\":80166},{\"end\":80187,\"start\":80176},{\"end\":80537,\"start\":80528},{\"end\":80544,\"start\":80537},{\"end\":80552,\"start\":80544},{\"end\":80559,\"start\":80552},{\"end\":80567,\"start\":80559},{\"end\":83481,\"start\":83469},{\"end\":84043,\"start\":84020},{\"end\":84053,\"start\":84043}]", "bib_venue": "[{\"end\":59643,\"start\":59574},{\"end\":60016,\"start\":59957},{\"end\":60806,\"start\":60737},{\"end\":61201,\"start\":61142},{\"end\":62737,\"start\":62668},{\"end\":63161,\"start\":63092},{\"end\":65036,\"start\":64953},{\"end\":65517,\"start\":65444},{\"end\":69463,\"start\":69426},{\"end\":70656,\"start\":70583},{\"end\":71271,\"start\":71188},{\"end\":71848,\"start\":71765},{\"end\":72899,\"start\":72830},{\"end\":73599,\"start\":73553},{\"end\":74593,\"start\":74534},{\"end\":76594,\"start\":76535},{\"end\":77634,\"start\":77630},{\"end\":79298,\"start\":79235},{\"end\":79817,\"start\":79744},{\"end\":80722,\"start\":80653},{\"end\":81130,\"start\":81075},{\"end\":82313,\"start\":82292},{\"end\":84585,\"start\":84548},{\"end\":55403,\"start\":55356},{\"end\":56004,\"start\":55971},{\"end\":56529,\"start\":56511},{\"end\":56936,\"start\":56929},{\"end\":57379,\"start\":57298},{\"end\":57811,\"start\":57754},{\"end\":58370,\"start\":58334},{\"end\":58717,\"start\":58641},{\"end\":59026,\"start\":58924},{\"end\":59265,\"start\":59251},{\"end\":59572,\"start\":59488},{\"end\":59955,\"start\":59881},{\"end\":60321,\"start\":60259},{\"end\":60735,\"start\":60651},{\"end\":61140,\"start\":61066},{\"end\":61499,\"start\":61437},{\"end\":61856,\"start\":61830},{\"end\":62294,\"start\":62290},{\"end\":62666,\"start\":62582},{\"end\":63090,\"start\":63006},{\"end\":63405,\"start\":63401},{\"end\":63652,\"start\":63648},{\"end\":64213,\"start\":64115},{\"end\":64951,\"start\":64853},{\"end\":65442,\"start\":65354},{\"end\":65878,\"start\":65823},{\"end\":66242,\"start\":66205},{\"end\":66527,\"start\":66506},{\"end\":66841,\"start\":66812},{\"end\":67186,\"start\":67146},{\"end\":67602,\"start\":67512},{\"end\":67978,\"start\":67916},{\"end\":68335,\"start\":68285},{\"end\":68662,\"start\":68658},{\"end\":68972,\"start\":68885},{\"end\":69393,\"start\":69334},{\"end\":69895,\"start\":69854},{\"end\":70581,\"start\":70493},{\"end\":71186,\"start\":71088},{\"end\":71763,\"start\":71665},{\"end\":72224,\"start\":72213},{\"end\":72512,\"start\":72493},{\"end\":72828,\"start\":72744},{\"end\":73185,\"start\":73174},{\"end\":73551,\"start\":73490},{\"end\":74095,\"start\":74091},{\"end\":74532,\"start\":74458},{\"end\":74973,\"start\":74897},{\"end\":75334,\"start\":75314},{\"end\":75696,\"start\":75666},{\"end\":76130,\"start\":76068},{\"end\":76533,\"start\":76459},{\"end\":76920,\"start\":76858},{\"end\":77259,\"start\":77255},{\"end\":77628,\"start\":77618},{\"end\":77937,\"start\":77933},{\"end\":78263,\"start\":78259},{\"end\":78572,\"start\":78519},{\"end\":78876,\"start\":78872},{\"end\":79233,\"start\":79155},{\"end\":79742,\"start\":79654},{\"end\":80236,\"start\":80187},{\"end\":80651,\"start\":80567},{\"end\":81073,\"start\":81039},{\"end\":82290,\"start\":82115},{\"end\":83467,\"start\":83410},{\"end\":84018,\"start\":83933},{\"end\":84546,\"start\":84511}]"}}}, "year": 2023, "month": 12, "day": 17}