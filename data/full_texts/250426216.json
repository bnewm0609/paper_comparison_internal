{"id": 250426216, "updated": "2023-10-05 12:37:28.839", "metadata": {"title": "Progressively-connected Light Field Network for Efficient View Synthesis", "authors": "[{\"first\":\"Peng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yuan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Guying\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Jiatao\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Lingjie\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Taku\",\"last\":\"Komura\",\"middle\":[]},{\"first\":\"Wenping\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This paper presents a Progressively-connected Light Field network (ProLiF), for the novel view synthesis of complex forward-facing scenes. ProLiF encodes a 4D light field, which allows rendering a large batch of rays in one training step for image- or patch-level losses. Directly learning a neural light field from images has difficulty in rendering multi-view consistent images due to its unawareness of the underlying 3D geometry. To address this problem, we propose a progressive training scheme and regularization losses to infer the underlying geometry during training, both of which enforce the multi-view consistency and thus greatly improves the rendering quality. Experiments demonstrate that our method is able to achieve significantly better rendering quality than the vanilla neural light fields and comparable results to NeRF-like rendering methods on the challenging LLFF dataset and Shiny Object dataset. Moreover, we demonstrate better compatibility with LPIPS loss to achieve robustness to varying light conditions and CLIP loss to control the rendering style of the scene. Project page: https://totoro97.github.io/projects/prolif.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.04465", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2207-04465", "doi": "10.48550/arxiv.2207.04465"}}, "content": {"source": {"pdf_hash": "e645946a7506385fcc9e2e88f2e0f0cf11dd6f16", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2207.04465v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4a7e8d8364685c5ed262440d933a88f1aadeaf09", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e645946a7506385fcc9e2e88f2e0f0cf11dd6f16.txt", "contents": "\nProgressively-connected Light Field Network for Efficient View Synthesis\n\n\nPeng Wang pwang3@cs.hku.hk \nYuan Liu yliu@cs.hku.hk \nGuying Lin \nJiatao Gu \nLingjie Liu lliu@mpi-inf.mpg.de \nTaku Komura \nWenping Wang wenping@tamu.edu \n\nThe University of Hong Kong Hong Kong\nChina\n\n\nThe University of Hong Kong Hong Kong\nChina\n\n\nMax Planck Institute for Informatics\nZhejiang University\nChina, United States, Germany\n\n\nThe University of Hong Kong Hong Kong\nChina\n\n\nTexas A&M University United States\n\n\nProgressively-connected Light Field Network for Efficient View Synthesis\n\n\n\nINTRODUCTION\n\nRendering free-viewpoint photo-realistic images from camera captures is a long-standing problem in computer vision, multimedia and graphics. This task, which we usually call image based rendering (IBR) or novel view synthesis (NVS), has many practical applications such as AR/VR, human-computer interaction, etc.\n\nRecently, the rendering quality of IBR has been improved drastically with the developments of Neural Radiance Fields (NeRF) [36], which represents a scene as a radiance field encoded by the Multilayer Perceptrons (MLPs) and renders images of the scene by the differentiable volume rendering on the field. Training of NeRF is mainly based on a pixel-level loss that compares the rendered color from the NeRF with the ground-truth color pixel-by-pixel. Recent works have shown that training a NeRF model with advanced patch-or image-level losses like GAN loss [13,39,50] or CLIP loss [18,19,44,60] would greatly expand the scope of NeRF applications in image generation or image editing. However, computing patch-or image-level losses requires rendering a large batch of pixels in one training step. Since NeRF requires hundreds of network evaluations to synthesize a pixel color, these recent works either only synthesize small patches by NeRF or need multiple high-end GPUs with rather large device memories to compute such high-level losses in training. When rendering a large batch, these methods cost excessively-long rendering time and huge GPU memory.\n\nTo improve the efficiency, recent works [8,9,17,28,38,43,45,67] take advantage of voxels [9,17,28,45,67] or networks [8,38,43] to memorize the surface distributions and only sample few points near the surfaces for rendering. However, these speed-up methods are only applicable in the inference stage or in the late training stage after convergence to valid surfaces, but not at the beginning of the training. It is because they require a good estimation of the surfaces to guide the sampling of points for rendering acceleration.\n\nInstead of learning a radiance field, some recent works manage to learn a neural light field, that directly maps a ray representation to a color [2,26,52]. This direct prediction avoids exhaustive evaluations of the network like NeRF, and only need one forward pass to synthesize a pixel color in both training and inference stages, which greatly improves the rendering efficiency and reduces the memory consumption. Hence, neural light fields are more suitable to be trained with image-or patch-level supervision. The main limitation of neural light fields is the lack of multi-view consistency in generating different views of the same scene, because such neural light fields are 3D geometry-agnostic. For example, for Rendered image from a training view Rendered image from a novel view Figure 2: A vanilla neural light field network is able to fit the training images (left) but cannot generalize to novel views (right). Here we use the LFN [52] with SIREN activations [51] to fit the 4D light field.\n\ntwo different rays, it is difficult for the light field network to know whether these two rays intersect to the same 3D surface point so that the rendered colors for these two rays should be consistent. In this case, neural light fields easily overfit to input images rather than representing the correct scene geometry, leading to severe artifacts in the novel rendered views as shown in Fig. 2.\n\nTo address these problems, we propose a Progressively-connected Light Field network, called ProLiF, that preserves multi-view consistency in the rendering and enables fast rendering in both training and inference stages without extensive memory consumption. Instead of directly learning a neural network to map a ray to a color, we design a point-based neural light field. In our method, we evenly sample points on the ray and use the concatenation of all point coordinates as the parameterization of the ray. We then feed these coordinates into a network to predict the density values and the colors of all these sample points in a single forward pass. Finally, we accumulate the colors and density values in a volume rendering fashion to compute the final pixel color.\n\nSimply applying the point-based neural light field still cannot perfectly keep multi-view consistency, i.e., the generalization ability to novel views, because the prediction of the color and the density on a sample point will be entangled with coordinates of other points on the same ray. Therefore, we additionally propose a progressive training scheme and multi-view consistency losses, to further improve the generalization ability. In the progressive training scheme, the points along the ray are sampled, and the network with sparse neuron connections processes every point separately at the beginning of the training, which enables a coarse but geometry-consistent initialization of the scene representation. Then, in the subsequent training steps, the neuron connections inside the network are gradually increased to improve the capacity of our neural representation for high-quality rendering. Meanwhile, during training, the novel multi-view consistency losses are used, which force rays intersecting on a surface point to have consistent colors.\n\nWe have conducted experiments on the LLFF dataset [35] and the Shiny Object dataset [63] to demonstrate the effectiveness of our method. Results show that the proposed method achieves better quality than the baseline neural light fields and comparable rendering quality to NeRF [36]. We also demonstrate that our method is well-compatible with LPIPS loss [69] to achieve robustness to different lighting conditions, and with CLIP loss to manipulate rendering style using texts. Moreover, our neural light field can efficiently render images at the resolution of 400 \u00d7 400 at 5 framesper-second (FPS) using a model size of 11MB.\n\nOur contributions can be summarized as:\n\n\u2022 A novel and compact neural light field representation for fast view synthesis in the both training and inference time without extensive memory consumption for a complex scene. The fast rendering also enables the use of the high-level losses on the full image during training. \u2022 A progressive training scheme that provides good trade-off between the network capacity and multi-view consistency. \u2022 Novel regularization losses for preserving multi-view consistency in the rendering of the neural light field.\n\n\nRELATED WORKS\n\nTraditional IBR. Novel view synthesis is a long-standing problem. Previously, novel view synthesis is mainly solved by blending input images [5,6,15,16,21,24,25,46,47,59,65] or fitting a plenoptic function [7,12]. The key problem to these methods is to select the correct combination of input colors to blend the image or form the plenoptic color. To address this problem, these methods either require very dense input views [12,25] or rely on an external geometry reconstruction [5,42]. Our method can be regarded as a neural plenoptic function which uses a neural network to map input rays to colors by fitting the input images. We do not resort to external geometry reconstruction but apply progressive training scheme and regularization to help the network learn the geometry. Neural Radiance field. Recent works [1, 14, 22, 24, 28-34, 36, 40, 41, 48, 53, 54, 58, 61, 64] show that we are able to learn neural scene representations for the NVS task. Among these works, NeRF [36] demonstrates significant improvements on this task by learning a neural radiance field from input images. However, NeRF still suffers from insufficient rendering in both training or inference stages. Many follow-up works proposes voxels [17,28,67] or other network designs [27,38] to speed up the rendering in inference stage. However, these methods do not allow efficiently rendering a large batch of rays in the training stage while our method can do this for better compatibility with image-or patch-level losses like LPIPS or CLIP losses. Neural Light field. Light Field Network (LFN) [52,55] shows the possibility to learn a neural light field to directly map rays to colors for the NVS task. However, training a neural light field easily overfits on input images due to the absence of an explicit 3D geometry. Concurrent works [2,56] solves this problem by adding features of sample points along rays [56] or learning subdivided neural light fields [2], which achieve better rendering quality at a cost of more computation in training and thus does not allow rendering a large batch of rays in one training step. The most similar concurrent work is NeuLF [26] which applies a depthbased regularization to prevent overfitting. Our method uses a progressive training scheme and different regularization losses, which demonstrates better rendering quality than NeuLF.\n\nHigh-level losses for NeRF. Some recent works managed to apply high-level losses on NeRF, like GAN loss [4,13,39,50] or CLIP loss [18][19][20]60]. These works use CLIP loss for sparse view reconstruction [20], generation [18,19] and style editing [60]. Among them, CLIP-NeRF [60] also intends for style editing on NeRF but uses an advanced conditional NeRF pre-trained on large training set. In contrast, our method does not require pre-training and enables rendering a large batch rays in on training step. Figure 3: (a) The visualization of a 4D two-plane light field. Each ray is defined by the coordinates of the intersections with the near plane and far plane. (b) We evenly sample planes between the near and the far plane and use the coordinates of intersections on these sample planes as the ray representation.\n\u2026 (a) (b) ( , ) ( , ) ( 0 , 0 ) ( \u22121 , \u22121 )\n\nMETHOD\n\nGiven a set of input images with known camera poses, our goal is to learn a neural light field from these images. Assuming all cameras capturing the input images are forward-facing to the scene, rays emitted from these images can be parameterized by = ( , , , ) as shown in Fig. 3 (a), where ( , ) is the point coordinate of the near plane and ( , ) is that of the far plane. The targeted neural light field is a function : R 4 \u2192 R 3 that maps the ray to a color \u2208 R 3 . A straightforward way to model this function is to train an MLP that maps ( , , , ) to the color on input images. However, such a design is 3D geometry-agnostic, which easily overfits to input images without the ability to generalize to novel views as shown in Fig. 2. To enable the neural light field to learn a multiview consistent scene representation, we propose a point-based neural light field (Sec. 3.1), a progressive training scheme (Sec. 3.2), and a multi-view consistency regularization scheme (Sec. 3.3) to improve its generalization ability to novel views.\n\n\nPoint-based neural light field\n\nOn the given ray, we evenly sample points at predefined depth values { 0 , 1 , ..., \u22121 } between the near and the far plane, as shown in Fig. 3 (b). Then, the ray is paramterized by the concatenation of all point coordinates ( 0 , 0, 1 , 1 , ..., \u22121 , \u22121 ) and is fed into a Multi-layer Perceptron : R 2 \u2192 R \u00d7 R 3 to predict densities and colors at all these points {( , )| = 0, ..., \u2212 1} in one forward pass. Finally, similar to NeRF [36], the colors and densities at these points are accumulated by a volume rendering scheme to compute the rendered color for this ray as follows:  Figure 4: Progressive training scheme. In this training scheme, we first separately predict densities and colors of points with different subnetworks, and then we progressively densify the connections between subnetworks to merge them. At the last training stage, we obtain a single fully-connected MLP to predict all the densities and colors of point samples.\n= \u22121 \u2211\ufe01 =0 ,(1)\nwhere , are the alpha value and the transmittance on the -th point respectively, which are computed by\n= 1 \u2212 exp(\u2212 ) = \u22121 =0 (1 \u2212 ).(2)\nHere denotes the distance between -th and ( + 1)-th point.\n\nDiscussion. In contrast to the straightforward design that directly maps a ray to a color, our neural light field predicts densities and colors at the sampled points and the final pixel color is then computed by volume rendering. This scheme provides the network an opportunity to infer the occlusion relationships among points and to learn the underlying scene geometry. Meanwhile, in comparison with NeRF [36] which needs to forward-pass the network multiple times with all sample points, our network predicts all densities and colors in a single forward-pass of the MLP. Thus, our method is much more efficient and allows rendering a large batch of rays in one training step.\n\nHowever, our efficient computation comes at a cost that the color and the density of a 3D point are not only dependent on its own coordinate but also entangled with coordinates of other point samples on the same ray. In this case, when two rays intersect on a 3D point, the neural light field possibly assigns totally different colors and densities for this 3D point on two rays, which leads to inconsistency in scene geometry and bad generalization to novel views. To tackle this issue, in the following, we introduce the progressive training scheme and multi-view consistency regularizations in the following.\n\n\nProgressive training scheme\n\nConsidering that the inconsistency mainly comes from the entanglement between point coordinates on a ray, we propose a progressive training scheme to process every point independently at the beginning of training to learn consistent scene geometry and then gradually improve the capacity of our network for better rendering quality by adding connections in the network.\n\nOur key idea is that at the beginning of training, we discard parts of the neuron connections in our MLP network to ensure that the color or density of a point only depends on the coordinate of this point without being entangled with coordinates of other points. In this case, a point will have the same color and density on different rays, which provides a strong inductive bias for the network to learn a consistent scene geometry. However, to ensure small memory consumption and efficient computation, the MLP network at the beginning is small with sparse connections, whose capacity is not enough to build a complex scene representation for high-quality rendering. Thus, in the subsequent training, we gradually increase the connections in our MLP to improve the capacity.\n\nSpecifically, as shown in Fig. 4, we split the training into -stages. At the beginning stage 0, the coordinate of a point at a specific depth only connects to its own feature vector thus it does not affect the features of points at other depth samples. In stage 0, given depth samples, this architecture can be regarded as separated small subnetworks which process the points at different depth independently to predict the colors and densities. Note that points at the same depth on different rays share the same small subnetwork. After training on stage 0 for steps, we switch the training to the next stage. During switching, two neighboring subnetworks will be merged into one single fully connected network by concatenating features of the two subnetworks and adding connections between adjacent layers of the merged network. In stage 1, we will have /2 subnetworks, each of which processes coordinates of two neighboring point samples and produces the colors and densities for these two points. The new connections are constructed by merging weight parameters of the subnetworks and adding zero values to the new weight parameters. Note in the merging, the total channel number in the hidden layers of all points stays unchanged. After switching, we will continue training with the architecture of the stage 1 for steps. Then, we repeat the merging and training until we get a single full-connected network to process all input point coordinates. More details can be found in the supplementary materials. Progressive depth sampling. Besides increasing network capacity by adding more neuron connections, we also begin with a very sparse depth samples between the near plane and far plane (Fig.  3), and progressively add more sampling depth values in the training. In order to process points at newly-added depth samples, we interpolate weights in subnetworks of neighboring depth values. For more details, please refer to supplementary materials.\n\n\nMulti-view consistency regularization\n\nIn the whole training process, we apply multi-view consistency regularizations to force the network to learn a consistent underlying scene geometry. Density consistency loss. The density consistency loss is based on the fact that the density of a 3D point is a constant value regardless which the input ray the point locates. However, in the later training stages, the density of a point would also be affected by other points on the ray. Thus, the predicted densities of the same point on different rays would be different. This motivates us to design a loss to constrain the predicted densities of the same point on different rays to be consistent. In the following, we derive a loss to implement this density constraint.\n\nTo derive the loss, we denote the whole process of computing a density on a point at depth on the ray ( , , , ) as = ( , , , ). Note consists of (1) sampling points on the ray ( , , , ) with predefined depth values and (2) predicting the density value at all the sample points. Meanwhile, is differentiable with ( , , , ) because the both suboperations are differentiable. The density consistency loss for this point at depth is\nL density = E , , , , \u2225\u2207 ( , , , ) \u00b7 p\u2225 2 + \u2225\u2207 ( , , , ) \u00b7 q\u2225 2 , (3) where p = ( , 0, \u2212(1 \u2212 ), 0) \u22a4 q = (0, , 0, \u2212(1 \u2212 )) \u22a4 .(4)\nThe intuition behind this loss is that (1) all rays intersecting the point form a 2D manifold in ( , , , ); (2) (p, q) is a basis spanning the tangent space of this manifold on this point; (3) we minimize the gradient \u2207 's projection in this tangent space. Ideally, the densities of this point will be the same for all rays intersecting on this point so such a projection will be zero. We provide detailed derivation in the supplementary material.\n\nColor consistency loss. Similarly, we expect the rendered colors for rays intersecting the same surface point to have consistent colors. We follow the similar process to represent the whole rendering procedure of our neural radiance field by = ( , , , ) and compute a color consistency loss by\nL color = E , , , (R (\u2207 ( , , , ) \u00b7 p) + R (\u2207 ( , , , ) \u00b7 q)) ,(5)where p = (\u02c6, 0, \u2212(1 \u2212\u02c6), 0) \u22a4 q = (0,\u02c6, 0, \u2212(1 \u2212\u02c6)) \u22a4 = \u2211\ufe01 .(6)\nHere and are the transmittance and the alpha values in Eqn. 2, is the predicted depth of the visible surface point for this ray, and R is chosen as smooth L1 loss [11] to reduce sensitivity to outliers on occlusion boundaries. The intuition behind this color consistency loss is similar to the density consistency loss, which computes the tangent space basis (p, q) at the depth\u02c6and minimizes the projection of \u2207 's gradient in this tangent space.\n\nIn summary, the total loss to train our neural light field is\nL = L render + density L densty + color L color ,(7)\nwhere the render loss L render is given by L render = E , , , \u2225 , , , \u2212 \u2225 2 2 . density = 10 \u22123 , and color = 10 \u22122 .\n\n\nEXPERIMENTS 4.1 Experimental Settings\n\nDatasets. We evaluate ProLiF on two public forward-facing datasets: the LLFF dataset [35] and Shiny dataset [63]. Both the datasets contain eight sets of images capturing the real scenes. Baselines. We compare our method with the baseline 4D light field networks with different activations of SIREN [51] or ReLU. We also report the comparisons to (1) the state-of-the-art neural volume rendering methods NeRF [36] and MipNeRF [3], (2) the state-ofthe-art MPI based method Nex [63], and (3) the fast rendering method AutoInt [27]. And we additionally report comparisons to two concurrent works (4) ray space embedding network (RSEN) [2] and (5) NeuLF [26] in the supplementary material for reference. Implementation details. The whole training is divided into 5 stages. For k-th training stage (indexed from zero), the number of subnetworks is 16\u00d72 \u2212 , and the hidden width of each sub-network is 32 \u00d7 2 respectively. Each network is with hidden depth of 8 (for the setting of ProLiF) or 12 (for the setting of ProLiF-Deep), and skip connections are used. We use SIREN activation [51] with its default parameter 0 = 30 to fit high frequency details. Weight normalization [49] is used to stabilize the training. The input coordinates are normalized to NDC space [36], and at the transition of each training stage, we also subdivide the depth samples. That is, the number of sampled depth planes is 16 \u00d7 2 at k-th training stage. We train our network on LLFF and Shiny dataset for 1M iterations. In each iteration, we sample 16384 rays for pixel fitting and extra randomly sample 4096 rays for regularization. Adam [23] optimizer is used with initial learning rate of 1 \u00d7 10 \u22124 . The learning rate is gradually decayed to 2.5 \u00d7 10 \u22126 at the end of training using cosine scheduling. The total time of training on a scene is around  \n\n\nComparison\n\nThe quantitative comparisons on the LLFF dataset [35] are presented in Table. 1. The results show that ProLiF performs better on image quality than the baseline light field networks using ProLiF NeRF Nex Ground truth Seasoning CD Giants Figure 6: Qualitative comparison on Shiny Object dataset. Our method succeeds in recovering speculars, and shows fewer image noises than NeRF. SIREN [51] or ReLU activations. Besides, we achieve comparable rendering quality to the state-of-the-art volume rendering method NeRF [36] as well as MipNeRF [3], while ProLiF costs much less time to render an image. We also report the results of the recent fast neural rendering method AutoInt [27] and ProLiF shows better rendering quality and computational efficiency. It is to be noted that to obtain satisfactory results on LLFF dataset, both AutoInt and NeRF need multiple evaluations of the network on sample points on the ray, while our method only needs a single evaluation. Fig 5  provides the qualitative results. Baseline light field network with SIREN activation fails to synthesize novel images due to overfitting. Switching to ReLU activation leads to better generalization ability but loses high-frequency details. In comparison, our method and NeRF synthesize novel view images in high quality.\n\nWe also report the quantitative comparisons on the Shiny Object dataset [63]. This dataset contains eight challenging scenes with complex specular effects and reflections. As shown in Table. 2, our method has better rendering quality than NeRF, and is comparable to the MPI based method Nex [63]. Qualitative results in Fig. 6 show that our method has fewer image noises, e.g., noises on the CD and liquid container, than NeRF's render result.\n\nBesides, we report the GPU memory consumption of ProLiF and the SOTA methods when rendering 1024 pixels. As shown in Table. 3, ProLiF requires much less GPU memory than the other methods, which indicates its excellent compatibility in rendering large patches of pixels.\n\n\nAblation Studies\n\nWe validate our method designs on the four scenes of the LLFF dataset in Table 4. Our full model achieves best quantitative results. As shown in Fig 7,   network, the results still suffer from overfitting and the underlying scene geometry cannot be correctly learned. After adding the progressive training scheme, the scene geometry can be correctly reconstructed in general but the rendered images are very noisy. Using regularization losses improves the generalization ability but there are observable artifacts if we do not use the progressive training scheme. In contrast, the full model achieves the best quality. Besides, we present the qualitative results of our training progression in Fig 8. By increasing the neuron connections in our progressive training scheme, the light field network is able to generate more details on both color images and depth maps.\n\n\nApplications\n\nRendering under varying light conditions. To demonstrate the compatibility of our method with image-or patch-level losses, we collect images of eight scenes, where each scene contains images under different light conditions. We change the lighting conditions by controlling the exposure of cameras or switching on/off the external light sources. To handle different light conditions, we train our network with LPIPS [69] Figure 9: Novel view synthesis using input images with varying light conditions. The first column shows a subset of input images while the other columns show the rendered images of different methods. Our method with LPIPS succeeds in recovering the details while achieving robustness to light changes.\n\ncan evaluate the \"perceptual\" similarity between two images, and is robust to noises, intensity changes, distortions, etc. Specifically, in each training step, we randomly render a 300 \u00d7 300 image patch and compare it with the ground-truth image patch to compute the LPIPS loss. Thus, the total training loss is\nL = L reg + LPIPS L LPIPS ,(8)\nwhere L reg = color L color + density L density is the aforementioned regularization loss, and LPIPS is set 0.025. We compare our method with baselines: (1) Table 5: Quantitative results on the light change dataset. We report the metrics of SSIM\u2191/LPIPS\u2193 for each scene. 'NeRF-P' denotes NeRF with 32 \u00d7 32 patch supervision, and 'NeRF-U' denotes NeRF with 256 \u00d7 256 up-sampled patch supervision.\n\npatch and up-samples the patch to 256 \u00d7 256 by a CNN to compute the LPIPS loss. Note that NeRF is not able to efficiently render a large patch in one training step due to the limited computational resources while our method is able to render a large patch of 300 \u00d7 300. As shown in Table. 5, our method with large-patch LPIPS loss achieves better quality than the two other baselines in terms of SSIM and LPIPS. We do not compare the PSNR since this pixel-wise metric is not compatible with varying light conditions. Qualitative results in Fig 9 show that NeRF with MSE loss suffers from severe \"foggy\" effects. NeRF with upsampling cannot reconstruct the correct appearance details. NeRF with small-patch LPIPS loss performs better than the original NeRF and that with upsampling but there are still visible artifacts. Text-guided scene style editing. We also show that our method can be supervised with CLIP loss [44]. CLIP (Contrastive Language-Image Pre-training) is a large model that is trained on the large scale images-text pairs. Here we apply CLIP to control the style of the scene with given texts. To achieve this, we introduce appearance embeddings (feature codes), and concatenate the embeddings to the intermediate features of ProLiF to generate images of different appearances. To train it, we propose a loss\nL = L render + L reg + CLIP L CLIP ,(9)\nwhere L render is the MSE loss between the rendered pixels and reference image pixels; L CLIP = 1 \u2212 f image \u00b7f text \u2225f image \u2225 \u00b7 \u2225f text \u2225 denotes the semantic distance between the given text and our rendered patches, and f image and f text are the extracted features of the rendered image and the given text respectively by the CLIP model. CLIP is set \"colorful leaves\" \"colorful flowers\" \"golden room\" \"wooden room\" \"autumn\" \"snows are on the flowers\"\n\n\nRoom\n\nFlower Leaves Room Figure 10: Examples of text-guided style control using CLIP loss. First column shows the original image while the other columns show the used texts and the rendered images. Note we manipulate the style of the whole scene so that we are able to render multi-view consistent images of the same style.\n\nto 0.05. It is noted that L CLIP and L render are calculated from two different appearance embeddings. In each training step we randomly sample 16384 pixels to compute L , a patch of size 224 \u00d7 224 for L , and extra 4096 pixels for L . Fig 10 shows the qualitative results and we provide videos in the supplementary material. Note our style editing is conducted for the whole scene so that our model is able to produce multi-view consistent images of the same style in the scene. In comparison, previous style editing [10] is only targeted for a single image.\n\n\nLimitations\n\nOur method is built on the near-far-plane setting and can not be used for 360 \u2022 scenes. Using the Plucker coordinate [52] for a more flexible ray parameterization is an interesting future work. Though our method achieves fast rendering in the inference stage, training such a network for each scene to achieve comparable results to NeRF is still time-consuming. In the future, we may try to decrease the training time by introducing extra data structures [37,57,66] or explore the possibility of generalizable light fields [33,62,68] without the requirement of scene-specific training.\n\n\nCONCLUSION\n\nWe have presented ProLiF, a compact neural representation for efficient view synthesis with around 40\u00d7 speeding up than NeRF. ProLiF, as a light field network, is able to efficiently render a large patch of pixels in a single training step and therefore has better compatibility with image-or patch-level losses such as LPIPS and CLIP loss. Directly training a light field network often suffers from overfitting, which cannot generalize to novel views. To address this, we propose a novel progressive training scheme and regularization losses to preserve multi-view consistency of rendered images. In the future, we may speed up the training of ProLiF and incorporate it in more applications, such as 3D consistent image generation. \n= \u00d72 \u2212 subnetworks \u03a6 that \u03a6 = { 0 , 1 , ..., \u22121 }.\nEach subnetwork is with hidden width \u00d7 2 . Here and are preset parameters. Note that the sum of hidden widths for each layer is constant during the whole training progress. For each subnetwork, its input is a concatenation of the sampled x-y coordinates, and the output is the corresponding concatenated color and density values, as shown in Fig. 4 of the main paper. When = log 2 ( ) at the last training stage, ProLiF is a single fully-connected MLP that \u03a6 = { 0 }.\n\nAdd connections. We now consider to increase the network capacity by merging the subnetworks at the end of k-th training stage. Specifically, we merge the neighboring sub-networks 2 and 2 +1 to +1 for \u2208 {0, ..., \u00d7 2 \u2212( +1) \u2212 1}. After merging, we expect the output values of the whole network \u03a6 are kept unchanged. To achieve this, we consider constructing the weight matrix +1 and bias vector +1 of each layer function +1 = ( +1 +1 + +1 ) of the MLP, where and is the input and output of the layer respectively, and is either an activation function (for the first and hidden layers) or the identity function (for the last layer).\n\nGiven the concatenation of the layer input +1 = 2 , 2 +1 \u22a4 , we expect the output after connection equals the corresponding concatenated outputs, i.e., we expect +1 = 2 , 2 +1 \u22a4 . To this end, we construct the weight matrix and bias vector as  return weight , bias Subdivide depth samples. When switching between the training stages, we also subdivide the depth planes for better rendering quality. Therefore, the number of depth samples at training stage is = \u00d7 2 . Similar to progressive connection, the rendered color of each input ray is kept unchanged after we perform the subdivision. We achieve this by reparameterizing the weight matrices and bias vectors of the first and the last layer functions of the networks. \n+1 = 2 0 0 2 +1 , +1 = 2 2 +1 .(10)\nWe make the concatenation of x-y coordinates as the network input. At the end of k-th training stage, the network input is changed from\n= 0 , 1 , ..., \u22121 , \u22121 \u22a4 to +1 = 0 +1 , 1 +1 , ..., +1 \u22121 +1 , +1 \u22121 +1 \u22a4 .\nThe sampled depth values at stage are defined by { = + 1 2 | = 0, ..., \u22121}. It is to be noted that we assume the near plane and far plane are placed at depth value of 0 and 1. Then it is not difficult to find that = ( 2 +1 + 2 +1 +1 )/2 and = ( 2 +1 + 2 +1 +1 )/2. With this observation we can construct the new weight and bias parameters of the first layer, which is described by the code snippet that 1 def subdiv_input_layers ( org_weight , org_bias ) : It can be validated that this construction of new weight and bias parameters will not affect the values of hidden features after depth subdivision.\n\n(b) Subdivide output predictions. We also subdivide the output color and density samples by reparameterizing the last layer of each subnetwork. To obey the principle that the final rendered colors will not be affected after the reparameterization, we double-copy the original parameters of the last layers, which is described by the code that 1 \n\n\nA.2 Derivation of consistency losses\n\nIn this section we derive the formula of consistency losses for regularization. Density consistency loss. As described in the main paper, we denote the whole process of computing a density on a point = ( , ) at depth on the ray ( , , , ) as = ( , , , ). The derivation consists of two parts: 1) we derive that all rays intersecting the same point form an affine space, and 2) we perform locality analysis on the derived affine space.\n\n1) For an arbitrary ray ( \u2032 , \u2032 , \u2032 , \u2032 ) that intersects , we have\n= \u2032 + ( \u2032 \u2212 \u2032 ) = \u2032 + ( \u2032 \u2212 \u2032 ) .(12)\nWe already know that ( , , , ) intersects on so we have\n= + ( \u2212 ) = + ( \u2212 ) .(13)\nThen, we have\n\u2032 + ( \u2032 \u2212 \u2032 ) = + ( \u2212 ) \u2032 + ( \u2032 \u2212 \u2032 ) = + ( \u2212 ) \u21d2 \u2212( \u2032 \u2212 ) * (1 \u2212 ) = ( \u2032 \u2212 ) * \u2212( \u2032 \u2212 ) * (1 \u2212 ) = ( \u2032 \u2212 ) * .(14)\nEqn 14 denotes that all intersected ray ( \u2032 , \u2032 , \u2032 , \u2032 ) can be represented by two parameters , that\n( \u2032 , \u2032 , \u2032 , \u2032 ) \u22a4 = ( , , , ) \u22a4 + p + q,(15)\nwhere\np = ( , 0, \u2212(1 \u2212 ), 0) \u22a4 , q = (0, , 0, \u2212(1 \u2212 )) \u22a4 .(16)\nThis is actually an affine space with a fixed point ( , , , ) and spanning basis p and q.\n\n2) We now perform local analysis of the density value around ( , , , ). For a first-order approximation of the density value at the neighboring ray ( \u2032 , \u2032 , \u2032 , \u2032 ), we have ( \u2032 , \u2032 , \u2032 , \u2032 ) \u2248 ( , , , ) + \u2207 \u00b7 ( p + q) \u2248 ( , , , ) + \u2207 \u00b7 p + \u2207 \u00b7 q.\n\nTo regularize the density value, we expect\n( \u2032 , \u2032 , \u2032 , \u2032 ) \u2212 ( , , , ) = 0 \u21d2 \u2207 \u00b7 p + \u2207 \u00b7 q \u2248 0,(18)\nwhich is a linear combination of two projected values. Note that and can be arbitrary combinations, therefore we directly minimize the individual projected terms. This derives the density consistency loss in Eqn 2 of the main paper.\n\nColor consistency loss. The derivation of color consistency loss is the same as the density consistency loss by replacing the density function with the color function . Alternatively, we may constraint the sample color instead of the output ray color . In this case, we expect the sampled colors to be consistent on different ray parameters and apply the same loss to the sampled colors as to the densities. However, in our early experiments, we found that imposing such a loss on the sample colors cannot help achieve view-consistent results. We think it is due to the accumulated error -though we regularize every sampled colors, the final rendered pixel color, which is synthesized by volume accumulation from all the sampled colors, still suffers from multi-view inconsistency. Therefore, we instead directly regularize the rendered colors. That is, the final rendered colors of rays intersecting the same surface point are expected to be consistent. A.3 Text-guided scene style editing Network architecture. To edit the style of the scenes by texts using CLIP loss, we choose to use a slightly different architecture to that used for the original NVS task, and in this architecture we introduce the appearance embeddings. The architecture for this task is shown in Fig. 11. For simplicity of description, we only visualize the architecture at the last training stage. As already described in the main paper, the overall training loss is L = L render + L reg + CLIP L CLIP . And L CLIP and L render are calculated from two different appearance embeddings.\n\nReference images w/o stop gradient w/ stop gradient Figure 12: Effect of stop-gradient operators. The top and bottom rows show images from two different views. The text for L CLIP here is \"colorful flowers\". Adding stop-gradient operators helps the rendered images preserve the original instrinsic structures.  Table 7: Comparison to RSEN on LLFF dataset. We report the results in terms of PSNR. FPS numbers are computed on the resolution of 504 \u00d7 378. We additionally provide the results of RSEN that uses a NeRF model as teacher for extra supervison, for reference.\n\nEffect of stop-gradient operators. In the implementation, we add the stop-gradient operators to the output densities and a hidden feature of the appearance network (green dotted arrows in Figure 11) when calculating L CLIP . We use this strategy to regularize the rendered results of style-editing to a similar intrinsic structure of the original scene. The qualitative comparisons between w/ stop-gradient and w/o stop-gradient are presented in Figure 12.\n\n\nB ADDITIONAL RESULTS\n\nHere we additionally report the quantitative comparisons to the concurrent works NeuLF [26] and Ray Space Embedding Networks (RSEN) [2]. Since the codes of these works have not been made public yet, we compare the quantitative results with those reported in their papers. Table 6 provides the comparisons to NeuLF on the Shiny Object dataset [63], where our method performs better rendering quality than NeuLF. The results of NeuLF on the LLFF dataset are not reported in their paper. Table 7 shows the comparisons to RSEN. The experiments of RSEN on LLFF dataset are conducted on the resolution of 504 \u00d7 378, and we compare to it using the same resolution. Our method achieves comparable results as RSEN while is much more efficient in rendering.\n\nFigure 5 :\n5Qualitative comparison on LLFF dataset. LFN-SIREN suffers from overtting and LFN-ReLU cannot render fine details. Our method and MipNeRF show better image quality than the two baseline neural light fields.\n\nFigure 7 :Figure 8 :\n78Ablation study. We present the visual comparison with different method designs. Training progression. We show the rendered novel views and depth maps of ProLiF at the end of the first stage and the last stage.\n\nFigure 11 :\n11Architecture for text-guided scene editing. The yellow block denotes the appearance embedding, and the gray blocks denote the output RGB and density samples, as well as the final rendered colors. The green dotted arrows denote stop-gradient operators when calculating L CLIP .\n\n\nThe LLFF dataset contains scenes with complex geometries, and the Shiny dataset contains challenging view-dependent effects, like specular highlights and reflections. The original image resolution of LLFF dataset is 4032 \u00d7 3024, we follow NeRF[36] and down-sample the images to 1008 \u00d7 756 to fairly compare with different methods. For the images of Shiny dataset, we follow Nex[63] and use the resolution of either 1008 \u00d7 756 or 1008 \u00d7 567, depending on the original aspect ratios. For both dataset we choose one out of eight images as test set and the rest as training set.Besides, to demonstrate that our \nmethod can be supervised with patch-level loss, like LPIPS [69], \nwe additionally self-collect a dataset of eight scenes, where each \nscene is captured under different lighting conditions. \n\n\n\n: Quantitative results on LLFF dataset. The top block shows the quantitative results among the neural light field methods, and the bottom block additionally reports the quantitative results of NeRF[36], MipNeRF[3] and AutoInt[27] for reference. ProLiF is much faster than (Mip)NeRF while achieves comparable rendering quality. The numbers with bold and underline denote the best and the second best respectively. FPS numbers are computed on the resolution of 1008 \u00d7 756. The FPS and model sizes are all computed with the model in the training stage. Quantitative results on Shiny Object dataset. We report the results in terms of PSNR. ProLiF outperforms NeRF[36] on speed and image quality, and costs much less memory and time than Nex[63]. The rendering time and model sizes are all computed with the model in the training stage.28 hours (ProLiF) or 35 hours (ProLiF-Deep) for 1M iterations on a single Nvidia 2080Ti GPU.Method \nRoom Fern Leaves Fortress Orchids Flower T-Rex Horns Mean FPS Model size \nLFN-SIREN \n18.30 17.22 \n13.27 \n20.32 \n13.02 \n21.43 \n16.87 16.03 17.06 \n2.35 \n8.0MB \nLFN-ReLU \n27.10 20.84 \n17.40 \n28.01 \n15.47 \n24.64 \n22.84 23.98 22.54 2.90 \n7.1MB \nProLiF \n31.49 22.91 \n19.94 \n30.23 \n18.62 \n28.08 \n25.84 26.86 25.50 \n1.40 \n11.0MB \nProLiF-Deep 31.76 23.03 20.04 \n30.38 \n18.82 \n28.26 26.32 27.34 25.74 1.15 \n16.1MB \nMipNeRF \n32.52 25.12 \n20.97 \n31.42 \n20.28 \n27.79 27.16 27.55 26.60 0.02 \n2.3MB \nNeRF \n32.70 25.17 20.92 \n31.16 \n20.36 \n27.40 \n26.80 27.45 26.50 \n0.04 \n4.4MB \nAutoInt \n27.98 21.59 \n19.78 \n28.99 \n16.35 \n26.86 \n23.20 21.42 23.27 0.09 \n5.4MB \nTable 1Method \nCD \nTools Crest Seasoning Food Giants \nLab \nPasta Mean FPS Model size \nNeRF \n30.14 27.54 20.30 \n27.79 \n23.32 \n24.86 \n29.60 21.23 25.60 \n0.04 \n4.4MB \nNex \n31.43 28.16 21.23 \n28.60 \n23.68 26.00 30.43 22.07 26.45 0.02 \n401.6MB \nProLiF \n31.10 26.98 19.69 \n28.24 \n23.02 \n25.63 \n30.97 21.16 25.85 1.40 \n11.0MB \nProLiF-Deep 31.56 27.21 19.83 \n28.43 \n23.14 \n25.96 31.26 21.23 26.07 \n1.15 \n16.1MB \nTable 2: \n\n\nif we directly train the point-based light field We provide the GPU memory consumption of the methods to forward 1024 pixels.ProLiF ProLiF-Deep NeRF \nNex \nAutoInt \nMem. 105MB \n148MB \n1.5GB 5.1GB \n9.0GB \nTable 3: w/o Prog. Connection \nGround Truth \nw/o Regularization \nFull Method \nDirect training \n\n\n\n\nloss, which is a high-level metric that Ablation study. We provide quantitative results in terms of PSNR on four scenes of the LLFF dataset.Leaves Fortress Trex Horns Mean \nLFN-SIREN \n13.27 \n20.32 \n16.87 16.03 \n16.62 \nDirect training \n16.89 \n25.03 \n22.84 22.47 \n21.81 \nw/o Reg. \n16.37 \n25.11 \n22.41 22.73 \n21.65 \nw/o Prog. \n19.33 \n29.56 \n25.35 25.37 \n24.91 \nFull method \n19.94 \n30.23 \n25.84 26.86 25.72 \nTable 4: NeRF \n\nBench \n\nProLiF \nGround truth \n\nStation \n\nDoll \n\nNeRF-patch \nNeRF-up-sample \n\nCorner \n\n\n\n\ndef subdiv_output_layers ( org_weight , org_bias ): # org_weight -shape of [ n_subnetworks , d_out , d_in ]2 \n\n3 \n\n# org_bias \n-shape of [ n_subnetworks , d_out ] \n\n\n\n\nProLiF-Deep 31.56 27.21 19.83 28.43 23.14 25.96 31.26 21.23 26.07 1.15 16.1MB Table 6: Comparison to NeuLF on Shiny Object dataset. We report the results in terms of PSNR. FPS numbers are computed on the resolution of 1008 \u00d7 756. Room Fern Leaves Fortress Orchids Flower T-Rex Horns Mean FPS Model sizeMethod \nCD \nTools Crest Seasoning Food Giants \nLab \nPasta Mean FPS Model size \nNeuLF \n32.11 26.73 20.11 \n27.12 \n22.61 \n24.95 31.95 20.64 25.78 2.65 \n5.0MB \nProLiF \n31.10 26.98 19.69 \n28.24 \n23.02 \n25.63 \n30.97 21.16 25.85 \n1.40 \n11.0MB \nMethod \nRSEN \n33.57 24.25 21.82 \n31.46 \n20.29 \n28.71 \n29.41 30.12 27.45 0.43 \n4.8MB \nProLiF \n32.77 24.66 \n21.50 \n31.56 \n19.40 \n29.07 \n27.42 28.91 26.91 5.22 \n11.0MB \nProLiF-Deep \n32.98 24.81 21.65 \n31.63 \n19.62 \n29.26 \n27.92 29.32 27.15 \n4.58 \n16.1MB \nRSEN (with teacher) 34.04 26.06 \n22.27 \n32.60 \n21.10 \n28.90 \n28.80 29.76 27.94 \n0.43 \n4.8MB \n\nProgressively-connected Light Field Network for Efficient View Synthesis Preprint, Arxiv, 2022\nACKNOWLEDGMENTSWe thank Jiepeng and Xiaoxiao for the proofreading. Lingjie Liu was supported by Lise Meitner Postdoctoral Fellowship. Computational resources are mainly provided by HKU GPU Farm.\nDmitry Ulyanov, and Victor Lempitsky. 2020. Neural point-based graphics. Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, ECCV. Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. 2020. Neural point-based graphics. In ECCV.\n\nBenjamin Attal, Jia-Bin Huang, Michael Zollhoefer, Johannes Kopf, Changil Kim, arXiv:2112.01523Learning Neural Light Fields with Ray-Space Embedding Networks. arXiv preprintBenjamin Attal, Jia-Bin Huang, Michael Zollhoefer, Johannes Kopf, and Changil Kim. 2021. Learning Neural Light Fields with Ray-Space Embedding Networks. arXiv preprint arXiv:2112.01523 (2021).\n\nMip-nerf: A multiscale representation for anti-aliasing neural radiance fields. T Jonathan, Ben Barron, Matthew Mildenhall, Peter Tancik, Ricardo Hedman, Martin-Brualla, Srinivasan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. 2021. Mip-nerf: A multiscale represen- tation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5855-5864.\n\n2021. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. Marco Eric R Chan, Petr Monteiro, Jiajun Kellnhofer, Gordon Wu, Wetzstein, CVPR. Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. 2021. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In CVPR.\n\nDepth synthesis and local warps for plausible image-based navigation. Gaurav Chaurasia, Olga Sylvain Duchene, George Sorkine-Hornung, Drettakis, ACM TOGGaurav Chaurasia, Sylvain Duchene, Olga Sorkine-Hornung, and George Dret- takis. 2013. Depth synthesis and local warps for plausible image-based navigation. ACM TOG (2013).\n\nExtreme view synthesis. Inchang Choi, Orazio Gallo, Alejandro Troccoli, H Min, Kim, ICCV. Inchang Choi, Orazio Gallo, Alejandro Troccoli, Min H Kim, and Jan Kautz. 2019. Extreme view synthesis. In ICCV.\n\nUnstructured light fields. Abe Davis, Marc Levoy, Fredo Durand, In EurographicsAbe Davis, Marc Levoy, and Fredo Durand. 2012. Unstructured light fields. In Eurographics.\n\nJiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, Qi Tian, arXiv:2111.15552NeuSample: Neural Sample Field for Efficient View Synthesis. arXiv preprintJiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang, Wenyu Liu, and Qi Tian. 2021. NeuSample: Neural Sample Field for Efficient View Synthesis. arXiv preprint arXiv:2111.15552 (2021).\n\nFastnerf: High-fidelity neural rendering at 200fps. J Stephan, Marek Garbin, Matthew Kowalski, Jamie Johnson, Julien Shotton, Valentin, CVPR. Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. 2021. Fastnerf: High-fidelity neural rendering at 200fps. In CVPR.\n\nImage style transfer using convolutional neural networks. A Leon, Alexander S Gatys, Matthias Ecker, Bethge, In CVPRLeon A Gatys, Alexander S Ecker, and Matthias Bethge. 2016. Image style transfer using convolutional neural networks. In CVPR.\n\nFast r-cnn. Ross Girshick, CVPR. Ross Girshick. 2015. Fast r-cnn. In CVPR.\n\nJ Steven, Radek Gortler, Richard Grzeszczuk, Michael F Szeliski, Cohen, The lumigraph. In SIGGRAPH. Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. 1996. The lumigraph. In SIGGRAPH.\n\nStylenerf: A style-based 3d-aware generator for high-resolution image synthesis. Jiatao Gu, Lingjie Liu, Peng Wang, Christian Theobalt, ICLR. Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. 2022. Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis. In ICLR.\n\nGerard Pons-Moll, and Christian Theobalt. 2021. Real-time Deep Dynamic Characters. Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, ACM TOGMarc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard Pons- Moll, and Christian Theobalt. 2021. Real-time Deep Dynamic Characters. ACM TOG (2021).\n\nDeep blending for free-viewpoint image-based rendering. Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, Gabriel Brostow, ACM TOG. Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. 2018. Deep blending for free-viewpoint image-based rendering. ACM TOG (2018).\n\nScalable inside-out image-based rendering. Peter Hedman, Tobias Ritschel, George Drettakis, Gabriel Brostow, ACM TOGPeter Hedman, Tobias Ritschel, George Drettakis, and Gabriel Brostow. 2016. Scalable inside-out image-based rendering. ACM TOG (2016).\n\nBaking neural radiance fields for real-time view synthesis. Peter Hedman, P Pratul, Ben Srinivasan, Jonathan T Mildenhall, Paul Barron, Debevec, CVPR. Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. 2021. Baking neural radiance fields for real-time view synthesis. In CVPR.\n\nAvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars. Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, Ziwei Liu, ACM Transactions on Graphics (TOG). 41Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. 2022. AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1-19.\n\n. Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, Ben Poole, arXiv:2112.01455Zero-Shot Text-Guided Object Generation with Dream Fields. arXiv preprintAjay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. 2021. Zero-Shot Text-Guided Object Generation with Dream Fields. arXiv preprint arXiv:2112.01455 (2021).\n\nPutting nerf on a diet: Semantically consistent few-shot view synthesis. Ajay Jain, Matthew Tancik, Pieter Abbeel, ICCV. Ajay Jain, Matthew Tancik, and Pieter Abbeel. 2021. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In ICCV.\n\nLearning-based view synthesis for light field cameras. Ting-Chun Nima Khademi Kalantari, Ravi Wang, Ramamoorthi, ACM TOGNima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi. 2016. Learning-based view synthesis for light field cameras. ACM TOG (2016).\n\nPetr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli, Gordon Wetzstein, Neural Lumigraph Rendering. In CVPR. Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli, and Gordon Wetzstein. 2021. Neural Lumigraph Rendering. In CVPR.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).\n\nPoint-Based Neural Rendering with Per-View Optimization. Georgios Kopanas, Julien Philip, Thomas Leimk\u00fchler, George Drettakis, Computer Graphics Forum. 40Georgios Kopanas, Julien Philip, Thomas Leimk\u00fchler, and George Drettakis. 2021. Point-Based Neural Rendering with Per-View Optimization. Computer Graphics Forum 40, 4 (2021), 29-43.\n\nLight field rendering. Marc Levoy, Pat Hanrahan, Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. the 23rd annual conference on Computer graphics and interactive techniquesMarc Levoy and Pat Hanrahan. 1996. Light field rendering. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques. 31-42.\n\nZhong Li, Song Liangchen, Celong Liu, Junsong Yuan, Yi Xu, arXiv:2105.07112NeuLF: Practical Novel View Synthesis with Neural Light Field. arXiv preprintZhong Li, Song Liangchen, Celong Liu, Junsong Yuan, and Yi Xu. 2021. NeuLF: Practical Novel View Synthesis with Neural Light Field. arXiv preprint arXiv:2105.07112 (2021).\n\nAutoint: Automatic integration for fast neural volume rendering. B David, Lindell, N P Julien, Gordon Martel, Wetzstein, CVPR. David B Lindell, Julien NP Martel, and Gordon Wetzstein. 2021. Autoint: Auto- matic integration for fast neural volume rendering. In CVPR.\n\nKyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020. Neural sparse voxel fields. Lingjie Liu, Jiatao Gu, NeurIPS. Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020. Neural sparse voxel fields. In NeurIPS.\n\nNeural Actor: Neural Free-view Synthesis of Human Actors with Pose Control. Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt, SIGGRAPH Asia. Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt. 2021. Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control. In SIGGRAPH Asia.\n\nNeural Human Video Rendering by Learning Dynamic Textures and Rendering-to-Video Translation. Lingjie Liu, Weipeng Xu, Marc Habermann, Michael Zollh\u00f6fer, Florian Bernard, Hyeongwoo Kim, Wenping Wang, Christian Theobalt, IEEE TVCG. Lingjie Liu, Weipeng Xu, Marc Habermann, Michael Zollh\u00f6fer, Florian Bernard, Hyeongwoo Kim, Wenping Wang, and Christian Theobalt. 2020. Neural Hu- man Video Rendering by Learning Dynamic Textures and Rendering-to-Video Translation. IEEE TVCG (2020).\n\nNeural rendering and reenactment of human actor videos. Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Hyeongwoo Kim, Florian Bernard, Marc Habermann, Wenping Wang, Christian Theobalt, ACM TOGLingjie Liu, Weipeng Xu, Michael Zollhoefer, Hyeongwoo Kim, Florian Bernard, Marc Habermann, Wenping Wang, and Christian Theobalt. 2019. Neural render- ing and reenactment of human actor videos. ACM TOG (2019).\n\nDist: Rendering deep implicit signed distance function with differentiable sphere tracing. Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, Zhaopeng Cui, CVPR. Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys, and Zhaopeng Cui. 2020. Dist: Rendering deep implicit signed distance function with differentiable sphere tracing. In CVPR.\n\nYuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei Zhou, Wenping Wang, arXiv:2107.13421Neural Rays for Occlusion-aware Image-based Rendering. arXiv preprintYuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei Zhou, and Wenping Wang. 2021. Neural Rays for Occlusion-aware Image-based Rendering. arXiv preprint arXiv:2107.13421 (2021).\n\nNeural Volumes: Learning dynamic renderable volumes from images. Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh, SIGGRAPH. Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. 2019. Neural Volumes: Learning dynamic render- able volumes from images. In SIGGRAPH.\n\nBen Mildenhall, P Pratul, Rodrigo Srinivasan, Nima Khademi Ortiz-Cayon, Ravi Kalantari, Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. ACM TOG. Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalan- tari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines. ACM TOG (2019).\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, ECCV. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV.\n\nThomas M\u00fcller, Alex Evans, Christoph Schied, Alexander Keller, arXiv:2201.05989stant Neural Graphics Primitives with a Multiresolution Hash Encoding. arXiv preprintThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. In- stant Neural Graphics Primitives with a Multiresolution Hash Encoding. arXiv preprint arXiv:2201.05989 (2022).\n\nDONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, H Joerg, Chakravarty R Alla Mueller, Anton Chaitanya, Markus Kaplanyan, Steinberger, Computer Graphics Forum. Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H Mueller, Chakravarty R Alla Chaitanya, Anton Kaplanyan, and Markus Steinberger. 2021. DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. In Computer Graphics Forum.\n\nGiraffe: Representing scenes as compositional generative neural feature fields. Michael Niemeyer, Andreas Geiger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMichael Niemeyer and Andreas Geiger. 2021. Giraffe: Representing scenes as compositional generative neural feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11453-11464.\n\nDifferentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger, CVPR. Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2020. Differentiable volumetric rendering: Learning implicit 3d representations with- out 3d supervision. In CVPR.\n\nNeural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, Xiaowei Zhou, CVPR. Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR.\n\nSoft 3D reconstruction for view synthesis. Eric Penner, Li Zhang, ACM TOGEric Penner and Li Zhang. 2017. Soft 3D reconstruction for view synthesis. ACM TOG (2017).\n\nTerminerf: Ray termination prediction for efficient neural rendering. Martin Piala, Ronald Clark, 3Martin Piala and Ronald Clark. 2021. Terminerf: Ray termination prediction for efficient neural rendering. In 3DV.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, PMLRInternational Conference on Machine Learning. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sand- hini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning. PMLR, 8748-8763.\n\nKilonerf: Speeding up neural radiance fields with thousands of tiny mlps. Christian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger, CVPR. Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In CVPR.\n\nFree View Synthesis. Gernot Riegler, Vladlen Koltun, ECCV. Gernot Riegler and Vladlen Koltun. 2020. Free View Synthesis. In ECCV.\n\nStable View Synthesis. Gernot Riegler, Vladlen Koltun, CVPR. Gernot Riegler and Vladlen Koltun. 2021. Stable View Synthesis. In CVPR.\n\nDarius R\u00fcckert, Linus Franke, Marc Stamminger, arXiv:2110.06635ADOP: Approximate Differentiable One-Pixel Point Rendering. arXiv preprintDarius R\u00fcckert, Linus Franke, and Marc Stamminger. 2021. ADOP: Approximate Differentiable One-Pixel Point Rendering. arXiv preprint arXiv:2110.06635 (2021).\n\nWeight normalization: A simple reparameterization to accelerate training of deep neural networks. Tim Salimans, P Durk, Kingma, In NeurIPSTim Salimans and Durk P Kingma. 2016. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NeurIPS.\n\nGraf: Generative radiance fields for 3d-aware image synthesis. Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger, NeurIPS. Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020. Graf: Generative radiance fields for 3d-aware image synthesis. In NeurIPS.\n\nImplicit neural representations with periodic activation functions. Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein, NeurIPS. Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gor- don Wetzstein. 2020. Implicit neural representations with periodic activation functions. In NeurIPS.\n\nLight field networks: Neural scene representations with singleevaluation rendering. Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, Fredo Durand, NeurIPS. Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. 2021. Light field networks: Neural scene representations with single- evaluation rendering. In NeurIPS.\n\nDeepVoxels: Learning persistent 3d feature embeddings. Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wetzstein, Michael Zollhofer, CVPR. Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wet- zstein, and Michael Zollhofer. 2019. DeepVoxels: Learning persistent 3d feature embeddings. In CVPR.\n\nScene representation networks: Continuous 3d-structure-aware neural scene representations. Vincent Sitzmann, Michael Zollh\u00f6fer, Gordon Wetzstein, NeurIPS. Vincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wetzstein. 2019. Scene repre- sentation networks: Continuous 3d-structure-aware neural scene representa- tions. In NeurIPS.\n\nCameron Smith, Hong-Xing Yu, Sergey Zakharov, Fredo Durand, Joshua B Tenenbaum, Jiajun Wu, Vincent Sitzmann, arXiv:2205.03923Unsupervised Discovery and Composition of Object Light Fields. arXiv preprintCameron Smith, Hong-Xing Yu, Sergey Zakharov, Fredo Durand, Joshua B Tenenbaum, Jiajun Wu, and Vincent Sitzmann. 2022. Unsupervised Discovery and Composition of Object Light Fields. arXiv preprint arXiv:2205.03923 (2022).\n\nMohammed Suhail, Carlos Esteves, arXiv:2112.09687Leonid Sigal, and Ameesh Makadia. 2021. Light Field Neural Rendering. arXiv preprintMohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. 2021. Light Field Neural Rendering. arXiv preprint arXiv:2112.09687 (2021).\n\nCheng Sun, Min Sun, Hwann-Tzong Chen, arXiv:2111.11215Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction. arXiv preprintCheng Sun, Min Sun, and Hwann-Tzong Chen. 2021. Direct Voxel Grid Opti- mization: Super-fast Convergence for Radiance Fields Reconstruction. arXiv preprint arXiv:2111.11215 (2021).\n\nDeferred neural rendering: Image synthesis using neural textures. Justus Thies, Michael Zollh\u00f6fer, Matthias Nie\u00dfner, ACM TOGJustus Thies, Michael Zollh\u00f6fer, and Matthias Nie\u00dfner. 2019. Deferred neural rendering: Image synthesis using neural textures. ACM TOG (2019).\n\nIGNOR: image-guided neural object rendering. Justus Thies, Michael Zollh\u00f6fer, Christian Theobalt, Marc Stamminger, Matthias Nie\u00dfner, ICLR. Justus Thies, Michael Zollh\u00f6fer, Christian Theobalt, Marc Stamminger, and Matthias Nie\u00dfner. 2020. IGNOR: image-guided neural object rendering. In ICLR.\n\nCan Wang, Menglei Chai, arXiv:2112.05139Mingming He, Dongdong Chen, and Jing Liao. 2021. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. arXiv preprintCan Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. 2021. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. arXiv preprint arXiv:2112.05139 (2021).\n\nNeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang, NeurIPS. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wen- ping Wang. 2021. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction. In NeurIPS.\n\nIbrnet: Learning multi-view image-based rendering. Qianqian Wang, Zhicheng Wang, Kyle Genova, P Pratul, Howard Srinivasan, Jonathan T Zhou, Ricardo Barron, Noah Martin-Brualla, Thomas Snavely, Funkhouser, CVPR. Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. 2021. Ibrnet: Learning multi-view image-based rendering. In CVPR.\n\nNex: Real-time view synthesis with neural basis expansion. Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn, CVPR. Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. 2021. Nex: Real-time view synthesis with neural basis expansion. In CVPR.\n\nMulti-View Neural Human Rendering. Minye Wu, Yuehao Wang, Qiang Hu, Jingyi Yu, CVPR. Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu. 2020. Multi-View Neural Human Rendering. In CVPR.\n\nDeep view synthesis from sparse photometric images. Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, Ravi Ramamoorthi, ACM TOGZexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, and Ravi Ramamoor- thi. 2019. Deep view synthesis from sparse photometric images. ACM TOG (2019).\n\nAlex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, Angjoo Kanazawa, arXiv:2112.05131Plenoxels: Radiance Fields without Neural Networks. arXiv preprintAlex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2021. Plenoxels: Radiance Fields without Neural Net- works. arXiv preprint arXiv:2112.05131 (2021).\n\nPlenoctrees for real-time rendering of neural radiance fields. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa, CVPR. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021. Plenoctrees for real-time rendering of neural radiance fields. In CVPR.\n\n2021. pixelnerf: Neural radiance fields from one or few images. Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa, CVPR. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021. pixelnerf: Neural radiance fields from one or few images. In CVPR.\n\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, CVPR. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In CVPR.\n", "annotations": {"author": "[{\"end\":103,\"start\":76},{\"end\":128,\"start\":104},{\"end\":140,\"start\":129},{\"end\":151,\"start\":141},{\"end\":184,\"start\":152},{\"end\":197,\"start\":185},{\"end\":228,\"start\":198},{\"end\":274,\"start\":229},{\"end\":320,\"start\":275},{\"end\":409,\"start\":321},{\"end\":455,\"start\":410},{\"end\":493,\"start\":456}]", "publisher": null, "author_last_name": "[{\"end\":85,\"start\":81},{\"end\":112,\"start\":109},{\"end\":139,\"start\":136},{\"end\":150,\"start\":148},{\"end\":163,\"start\":160},{\"end\":196,\"start\":190},{\"end\":210,\"start\":206}]", "author_first_name": "[{\"end\":80,\"start\":76},{\"end\":108,\"start\":104},{\"end\":135,\"start\":129},{\"end\":147,\"start\":141},{\"end\":159,\"start\":152},{\"end\":189,\"start\":185},{\"end\":205,\"start\":198}]", "author_affiliation": "[{\"end\":273,\"start\":230},{\"end\":319,\"start\":276},{\"end\":408,\"start\":322},{\"end\":454,\"start\":411},{\"end\":492,\"start\":457}]", "title": "[{\"end\":73,\"start\":1},{\"end\":566,\"start\":494}]", "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1026,\"start\":1022},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1460,\"start\":1456},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1463,\"start\":1460},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":1466,\"start\":1463},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1484,\"start\":1480},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1487,\"start\":1484},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":1490,\"start\":1487},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":1493,\"start\":1490},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2099,\"start\":2096},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2101,\"start\":2099},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2104,\"start\":2101},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2107,\"start\":2104},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2110,\"start\":2107},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2113,\"start\":2110},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2116,\"start\":2113},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":2119,\"start\":2116},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2148,\"start\":2145},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2151,\"start\":2148},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2154,\"start\":2151},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2157,\"start\":2154},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":2160,\"start\":2157},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2176,\"start\":2173},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2179,\"start\":2176},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2182,\"start\":2179},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2735,\"start\":2732},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2738,\"start\":2735},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2741,\"start\":2738},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3536,\"start\":3532},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3564,\"start\":3560},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5875,\"start\":5871},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":5909,\"start\":5905},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6103,\"start\":6099},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":6180,\"start\":6176},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7160,\"start\":7157},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7162,\"start\":7160},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7165,\"start\":7162},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7168,\"start\":7165},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7171,\"start\":7168},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7174,\"start\":7171},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7177,\"start\":7174},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7180,\"start\":7177},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7183,\"start\":7180},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7186,\"start\":7183},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7189,\"start\":7186},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7225,\"start\":7222},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7228,\"start\":7225},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7445,\"start\":7441},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7448,\"start\":7445},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7499,\"start\":7496},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7502,\"start\":7499},{\"end\":7891,\"start\":7833},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7998,\"start\":7994},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8240,\"start\":8236},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8243,\"start\":8240},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":8246,\"start\":8243},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8276,\"start\":8272},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8279,\"start\":8276},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8592,\"start\":8588},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8595,\"start\":8592},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8835,\"start\":8832},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8838,\"start\":8835},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8910,\"start\":8906},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8957,\"start\":8954},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9164,\"start\":9160},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9478,\"start\":9475},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9481,\"start\":9478},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9484,\"start\":9481},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9487,\"start\":9484},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9505,\"start\":9501},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9509,\"start\":9505},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9513,\"start\":9509},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9516,\"start\":9513},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9579,\"start\":9575},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9596,\"start\":9592},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9599,\"start\":9596},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9622,\"start\":9618},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9650,\"start\":9646},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11758,\"start\":11754},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12886,\"start\":12882},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19267,\"start\":19263},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19912,\"start\":19908},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":19935,\"start\":19931},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20126,\"start\":20122},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20236,\"start\":20232},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20252,\"start\":20249},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":20303,\"start\":20299},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20351,\"start\":20347},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20458,\"start\":20455},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20477,\"start\":20473},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20906,\"start\":20902},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20997,\"start\":20993},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21087,\"start\":21083},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21439,\"start\":21435},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21719,\"start\":21715},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22056,\"start\":22052},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22184,\"start\":22180},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22207,\"start\":22204},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22345,\"start\":22341},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":23035,\"start\":23031},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":23254,\"start\":23250},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":24998,\"start\":24994},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26960,\"start\":26956},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28709,\"start\":28705},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":28883,\"start\":28879},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29221,\"start\":29217},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":29224,\"start\":29221},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":29227,\"start\":29224},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29289,\"start\":29285},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":29292,\"start\":29289},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":29295,\"start\":29292},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33170,\"start\":33169},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":37556,\"start\":37552},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37600,\"start\":37597},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":37811,\"start\":37807},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":39207,\"start\":39203},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":39341,\"start\":39337},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":39962,\"start\":39958},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":39974,\"start\":39971},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":39990,\"start\":39986},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":40424,\"start\":40420},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":40501,\"start\":40497}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38431,\"start\":38213},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38665,\"start\":38432},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38957,\"start\":38666},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":39758,\"start\":38958},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41749,\"start\":39759},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42051,\"start\":41750},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42560,\"start\":42052},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":42728,\"start\":42561},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":43615,\"start\":42729}]", "paragraph": "[{\"end\":896,\"start\":584},{\"end\":2054,\"start\":898},{\"end\":2585,\"start\":2056},{\"end\":3591,\"start\":2587},{\"end\":3989,\"start\":3593},{\"end\":4761,\"start\":3991},{\"end\":5819,\"start\":4763},{\"end\":6448,\"start\":5821},{\"end\":6489,\"start\":6450},{\"end\":6998,\"start\":6491},{\"end\":9369,\"start\":7016},{\"end\":10190,\"start\":9371},{\"end\":11284,\"start\":10244},{\"end\":12262,\"start\":11319},{\"end\":12381,\"start\":12279},{\"end\":12473,\"start\":12415},{\"end\":13153,\"start\":12475},{\"end\":13766,\"start\":13155},{\"end\":14167,\"start\":13798},{\"end\":14945,\"start\":14169},{\"end\":16900,\"start\":14947},{\"end\":17665,\"start\":16942},{\"end\":18095,\"start\":17667},{\"end\":18673,\"start\":18226},{\"end\":18968,\"start\":18675},{\"end\":19547,\"start\":19100},{\"end\":19610,\"start\":19549},{\"end\":19781,\"start\":19664},{\"end\":21651,\"start\":19823},{\"end\":22957,\"start\":21666},{\"end\":23402,\"start\":22959},{\"end\":23673,\"start\":23404},{\"end\":24561,\"start\":23694},{\"end\":25300,\"start\":24578},{\"end\":25613,\"start\":25302},{\"end\":26039,\"start\":25645},{\"end\":27365,\"start\":26041},{\"end\":27859,\"start\":27406},{\"end\":28185,\"start\":27868},{\"end\":28746,\"start\":28187},{\"end\":29347,\"start\":28762},{\"end\":30095,\"start\":29362},{\"end\":30614,\"start\":30147},{\"end\":31246,\"start\":30616},{\"end\":31971,\"start\":31248},{\"end\":32143,\"start\":32008},{\"end\":32824,\"start\":32220},{\"end\":33171,\"start\":32826},{\"end\":33645,\"start\":33212},{\"end\":33714,\"start\":33647},{\"end\":33808,\"start\":33753},{\"end\":33848,\"start\":33835},{\"end\":34066,\"start\":33965},{\"end\":34119,\"start\":34114},{\"end\":34266,\"start\":34177},{\"end\":34516,\"start\":34268},{\"end\":34560,\"start\":34518},{\"end\":34852,\"start\":34620},{\"end\":36413,\"start\":34854},{\"end\":36982,\"start\":36415},{\"end\":37440,\"start\":36984},{\"end\":38212,\"start\":37465}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10234,\"start\":10191},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12278,\"start\":12263},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12414,\"start\":12382},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18225,\"start\":18096},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19035,\"start\":18969},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19099,\"start\":19035},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19663,\"start\":19611},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25644,\"start\":25614},{\"attributes\":{\"id\":\"formula_8\"},\"end\":27405,\"start\":27366},{\"attributes\":{\"id\":\"formula_9\"},\"end\":30146,\"start\":30096},{\"attributes\":{\"id\":\"formula_10\"},\"end\":32007,\"start\":31972},{\"attributes\":{\"id\":\"formula_12\"},\"end\":32219,\"start\":32144},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33752,\"start\":33715},{\"attributes\":{\"id\":\"formula_14\"},\"end\":33834,\"start\":33809},{\"attributes\":{\"id\":\"formula_15\"},\"end\":33964,\"start\":33849},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34113,\"start\":34067},{\"attributes\":{\"id\":\"formula_17\"},\"end\":34176,\"start\":34120},{\"attributes\":{\"id\":\"formula_19\"},\"end\":34619,\"start\":34561}]", "table_ref": "[{\"end\":21743,\"start\":21737},{\"end\":23149,\"start\":23143},{\"end\":23527,\"start\":23521},{\"end\":23774,\"start\":23767},{\"end\":25809,\"start\":25802},{\"end\":26329,\"start\":26323},{\"end\":36733,\"start\":36726},{\"end\":37744,\"start\":37737},{\"end\":37957,\"start\":37950}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":582,\"start\":570},{\"attributes\":{\"n\":\"2\"},\"end\":7014,\"start\":7001},{\"attributes\":{\"n\":\"3\"},\"end\":10242,\"start\":10236},{\"attributes\":{\"n\":\"3.1\"},\"end\":11317,\"start\":11287},{\"attributes\":{\"n\":\"3.2\"},\"end\":13796,\"start\":13769},{\"attributes\":{\"n\":\"3.3\"},\"end\":16940,\"start\":16903},{\"attributes\":{\"n\":\"4\"},\"end\":19821,\"start\":19784},{\"attributes\":{\"n\":\"4.2\"},\"end\":21664,\"start\":21654},{\"attributes\":{\"n\":\"4.3\"},\"end\":23692,\"start\":23676},{\"attributes\":{\"n\":\"4.4\"},\"end\":24576,\"start\":24564},{\"end\":27866,\"start\":27862},{\"attributes\":{\"n\":\"4.5\"},\"end\":28760,\"start\":28749},{\"attributes\":{\"n\":\"5\"},\"end\":29360,\"start\":29350},{\"end\":33210,\"start\":33174},{\"end\":37463,\"start\":37443},{\"end\":38224,\"start\":38214},{\"end\":38453,\"start\":38433},{\"end\":38678,\"start\":38667}]", "table": "[{\"end\":39758,\"start\":39534},{\"end\":41749,\"start\":40684},{\"end\":42051,\"start\":41877},{\"end\":42560,\"start\":42194},{\"end\":42728,\"start\":42670},{\"end\":43615,\"start\":43033}]", "figure_caption": "[{\"end\":38431,\"start\":38226},{\"end\":38665,\"start\":38456},{\"end\":38957,\"start\":38681},{\"end\":39534,\"start\":38960},{\"end\":40684,\"start\":39761},{\"end\":41877,\"start\":41752},{\"end\":42194,\"start\":42054},{\"end\":42670,\"start\":42563},{\"end\":43033,\"start\":42731}]", "figure_ref": "[{\"end\":3385,\"start\":3377},{\"end\":3988,\"start\":3982},{\"end\":9887,\"start\":9879},{\"end\":10524,\"start\":10518},{\"end\":10982,\"start\":10976},{\"end\":11466,\"start\":11456},{\"end\":11910,\"start\":11902},{\"end\":14979,\"start\":14973},{\"end\":16650,\"start\":16641},{\"end\":21911,\"start\":21903},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22645,\"start\":22630},{\"end\":23285,\"start\":23279},{\"end\":23845,\"start\":23839},{\"end\":24394,\"start\":24388},{\"end\":25007,\"start\":24999},{\"end\":26591,\"start\":26581},{\"end\":27896,\"start\":27887},{\"end\":28435,\"start\":28423},{\"end\":30495,\"start\":30489},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":36131,\"start\":36124},{\"end\":36476,\"start\":36467},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37181,\"start\":37172},{\"end\":37439,\"start\":37430}]", "bib_author_first_name": "[{\"end\":43987,\"start\":43979},{\"end\":44000,\"start\":43995},{\"end\":44021,\"start\":44016},{\"end\":44176,\"start\":44168},{\"end\":44191,\"start\":44184},{\"end\":44206,\"start\":44199},{\"end\":44227,\"start\":44219},{\"end\":44241,\"start\":44234},{\"end\":44616,\"start\":44615},{\"end\":44630,\"start\":44627},{\"end\":44646,\"start\":44639},{\"end\":44664,\"start\":44659},{\"end\":44680,\"start\":44673},{\"end\":45235,\"start\":45230},{\"end\":45253,\"start\":45249},{\"end\":45270,\"start\":45264},{\"end\":45289,\"start\":45283},{\"end\":45570,\"start\":45564},{\"end\":45586,\"start\":45582},{\"end\":45610,\"start\":45604},{\"end\":45851,\"start\":45844},{\"end\":45864,\"start\":45858},{\"end\":45881,\"start\":45872},{\"end\":45893,\"start\":45892},{\"end\":46054,\"start\":46051},{\"end\":46066,\"start\":46062},{\"end\":46079,\"start\":46074},{\"end\":46201,\"start\":46195},{\"end\":46214,\"start\":46208},{\"end\":46228,\"start\":46220},{\"end\":46243,\"start\":46235},{\"end\":46256,\"start\":46251},{\"end\":46264,\"start\":46262},{\"end\":46603,\"start\":46602},{\"end\":46618,\"start\":46613},{\"end\":46634,\"start\":46627},{\"end\":46650,\"start\":46645},{\"end\":46666,\"start\":46660},{\"end\":46906,\"start\":46905},{\"end\":46922,\"start\":46913},{\"end\":46924,\"start\":46923},{\"end\":46940,\"start\":46932},{\"end\":47107,\"start\":47103},{\"end\":47168,\"start\":47167},{\"end\":47182,\"start\":47177},{\"end\":47199,\"start\":47192},{\"end\":47221,\"start\":47212},{\"end\":47464,\"start\":47458},{\"end\":47476,\"start\":47469},{\"end\":47486,\"start\":47482},{\"end\":47502,\"start\":47493},{\"end\":47762,\"start\":47758},{\"end\":47781,\"start\":47774},{\"end\":47794,\"start\":47787},{\"end\":47806,\"start\":47799},{\"end\":48049,\"start\":48044},{\"end\":48064,\"start\":48058},{\"end\":48077,\"start\":48073},{\"end\":48096,\"start\":48085},{\"end\":48110,\"start\":48104},{\"end\":48129,\"start\":48122},{\"end\":48374,\"start\":48369},{\"end\":48389,\"start\":48383},{\"end\":48406,\"start\":48400},{\"end\":48425,\"start\":48418},{\"end\":48643,\"start\":48638},{\"end\":48653,\"start\":48652},{\"end\":48665,\"start\":48662},{\"end\":48686,\"start\":48678},{\"end\":48688,\"start\":48687},{\"end\":48705,\"start\":48701},{\"end\":48975,\"start\":48967},{\"end\":48990,\"start\":48982},{\"end\":49003,\"start\":48998},{\"end\":49017,\"start\":49009},{\"end\":49026,\"start\":49023},{\"end\":49038,\"start\":49033},{\"end\":49305,\"start\":49301},{\"end\":49315,\"start\":49312},{\"end\":49336,\"start\":49328},{\"end\":49338,\"start\":49337},{\"end\":49353,\"start\":49347},{\"end\":49365,\"start\":49362},{\"end\":49721,\"start\":49717},{\"end\":49735,\"start\":49728},{\"end\":49750,\"start\":49744},{\"end\":49964,\"start\":49955},{\"end\":49993,\"start\":49989},{\"end\":50164,\"start\":50160},{\"end\":50181,\"start\":50177},{\"end\":50194,\"start\":50188},{\"end\":50206,\"start\":50202},{\"end\":50219,\"start\":50215},{\"end\":50233,\"start\":50227},{\"end\":50460,\"start\":50459},{\"end\":50476,\"start\":50471},{\"end\":50707,\"start\":50699},{\"end\":50723,\"start\":50717},{\"end\":50738,\"start\":50732},{\"end\":50757,\"start\":50751},{\"end\":51006,\"start\":51002},{\"end\":51017,\"start\":51014},{\"end\":51358,\"start\":51353},{\"end\":51367,\"start\":51363},{\"end\":51385,\"start\":51379},{\"end\":51398,\"start\":51391},{\"end\":51407,\"start\":51405},{\"end\":51744,\"start\":51743},{\"end\":51762,\"start\":51761},{\"end\":51764,\"start\":51763},{\"end\":51779,\"start\":51773},{\"end\":52039,\"start\":52032},{\"end\":52051,\"start\":52045},{\"end\":52272,\"start\":52265},{\"end\":52282,\"start\":52278},{\"end\":52300,\"start\":52294},{\"end\":52320,\"start\":52309},{\"end\":52335,\"start\":52329},{\"end\":52349,\"start\":52340},{\"end\":52676,\"start\":52669},{\"end\":52689,\"start\":52682},{\"end\":52698,\"start\":52694},{\"end\":52717,\"start\":52710},{\"end\":52736,\"start\":52729},{\"end\":52755,\"start\":52746},{\"end\":52768,\"start\":52761},{\"end\":52784,\"start\":52775},{\"end\":53120,\"start\":53113},{\"end\":53133,\"start\":53126},{\"end\":53145,\"start\":53138},{\"end\":53167,\"start\":53158},{\"end\":53180,\"start\":53173},{\"end\":53194,\"start\":53190},{\"end\":53213,\"start\":53206},{\"end\":53229,\"start\":53220},{\"end\":53557,\"start\":53550},{\"end\":53568,\"start\":53563},{\"end\":53583,\"start\":53576},{\"end\":53595,\"start\":53590},{\"end\":53605,\"start\":53601},{\"end\":53625,\"start\":53617},{\"end\":53833,\"start\":53829},{\"end\":53843,\"start\":53839},{\"end\":53857,\"start\":53850},{\"end\":53871,\"start\":53863},{\"end\":53882,\"start\":53878},{\"end\":53898,\"start\":53889},{\"end\":53916,\"start\":53909},{\"end\":53930,\"start\":53923},{\"end\":54308,\"start\":54301},{\"end\":54324,\"start\":54319},{\"end\":54337,\"start\":54332},{\"end\":54354,\"start\":54347},{\"end\":54372,\"start\":54365},{\"end\":54388,\"start\":54383},{\"end\":54597,\"start\":54594},{\"end\":54611,\"start\":54610},{\"end\":54627,\"start\":54620},{\"end\":54644,\"start\":54640},{\"end\":54652,\"start\":54645},{\"end\":54670,\"start\":54666},{\"end\":55143,\"start\":55140},{\"end\":55157,\"start\":55156},{\"end\":55173,\"start\":55166},{\"end\":55194,\"start\":55186},{\"end\":55196,\"start\":55195},{\"end\":55209,\"start\":55205},{\"end\":55221,\"start\":55218},{\"end\":55441,\"start\":55435},{\"end\":55454,\"start\":55450},{\"end\":55471,\"start\":55462},{\"end\":55489,\"start\":55480},{\"end\":55893,\"start\":55887},{\"end\":55906,\"start\":55900},{\"end\":55926,\"start\":55919},{\"end\":55942,\"start\":55935},{\"end\":55950,\"start\":55949},{\"end\":55976,\"start\":55958},{\"end\":55991,\"start\":55986},{\"end\":56009,\"start\":56003},{\"end\":56430,\"start\":56423},{\"end\":56448,\"start\":56441},{\"end\":56934,\"start\":56927},{\"end\":56949,\"start\":56945},{\"end\":56968,\"start\":56961},{\"end\":56985,\"start\":56978},{\"end\":57309,\"start\":57305},{\"end\":57324,\"start\":57316},{\"end\":57339,\"start\":57332},{\"end\":57352,\"start\":57344},{\"end\":57363,\"start\":57359},{\"end\":57376,\"start\":57371},{\"end\":57389,\"start\":57382},{\"end\":57678,\"start\":57674},{\"end\":57689,\"start\":57687},{\"end\":57872,\"start\":57866},{\"end\":57886,\"start\":57880},{\"end\":58086,\"start\":58082},{\"end\":58100,\"start\":58096},{\"end\":58105,\"start\":58101},{\"end\":58116,\"start\":58111},{\"end\":58132,\"start\":58126},{\"end\":58148,\"start\":58141},{\"end\":58162,\"start\":58154},{\"end\":58178,\"start\":58172},{\"end\":58193,\"start\":58187},{\"end\":58208,\"start\":58202},{\"end\":58222,\"start\":58218},{\"end\":58664,\"start\":58655},{\"end\":58680,\"start\":58673},{\"end\":58691,\"start\":58687},{\"end\":58705,\"start\":58698},{\"end\":58900,\"start\":58894},{\"end\":58917,\"start\":58910},{\"end\":59033,\"start\":59027},{\"end\":59050,\"start\":59043},{\"end\":59145,\"start\":59139},{\"end\":59160,\"start\":59155},{\"end\":59173,\"start\":59169},{\"end\":59535,\"start\":59532},{\"end\":59547,\"start\":59546},{\"end\":59789,\"start\":59784},{\"end\":59803,\"start\":59799},{\"end\":59817,\"start\":59810},{\"end\":59835,\"start\":59828},{\"end\":60074,\"start\":60067},{\"end\":60091,\"start\":60085},{\"end\":60109,\"start\":60100},{\"end\":60124,\"start\":60119},{\"end\":60140,\"start\":60134},{\"end\":60430,\"start\":60423},{\"end\":60446,\"start\":60441},{\"end\":60462,\"start\":60458},{\"end\":60476,\"start\":60472},{\"end\":60493,\"start\":60488},{\"end\":60761,\"start\":60754},{\"end\":60778,\"start\":60772},{\"end\":60791,\"start\":60786},{\"end\":60807,\"start\":60799},{\"end\":60823,\"start\":60817},{\"end\":60842,\"start\":60835},{\"end\":61135,\"start\":61128},{\"end\":61153,\"start\":61146},{\"end\":61171,\"start\":61165},{\"end\":61372,\"start\":61365},{\"end\":61389,\"start\":61380},{\"end\":61400,\"start\":61394},{\"end\":61416,\"start\":61411},{\"end\":61431,\"start\":61425},{\"end\":61433,\"start\":61432},{\"end\":61451,\"start\":61445},{\"end\":61463,\"start\":61456},{\"end\":61798,\"start\":61790},{\"end\":61813,\"start\":61807},{\"end\":62072,\"start\":62067},{\"end\":62081,\"start\":62078},{\"end\":62098,\"start\":62087},{\"end\":62480,\"start\":62474},{\"end\":62495,\"start\":62488},{\"end\":62515,\"start\":62507},{\"end\":62727,\"start\":62721},{\"end\":62742,\"start\":62735},{\"end\":62763,\"start\":62754},{\"end\":62778,\"start\":62774},{\"end\":62799,\"start\":62791},{\"end\":62971,\"start\":62968},{\"end\":62985,\"start\":62978},{\"end\":63426,\"start\":63422},{\"end\":63440,\"start\":63433},{\"end\":63450,\"start\":63446},{\"end\":63465,\"start\":63456},{\"end\":63480,\"start\":63476},{\"end\":63496,\"start\":63489},{\"end\":63768,\"start\":63760},{\"end\":63783,\"start\":63775},{\"end\":63794,\"start\":63790},{\"end\":63804,\"start\":63803},{\"end\":63819,\"start\":63813},{\"end\":63840,\"start\":63832},{\"end\":63842,\"start\":63841},{\"end\":63856,\"start\":63849},{\"end\":63869,\"start\":63865},{\"end\":63892,\"start\":63886},{\"end\":64211,\"start\":64203},{\"end\":64233,\"start\":64225},{\"end\":64255,\"start\":64247},{\"end\":64277,\"start\":64269},{\"end\":64506,\"start\":64501},{\"end\":64517,\"start\":64511},{\"end\":64529,\"start\":64524},{\"end\":64540,\"start\":64534},{\"end\":64709,\"start\":64702},{\"end\":64717,\"start\":64714},{\"end\":64728,\"start\":64722},{\"end\":64746,\"start\":64741},{\"end\":64757,\"start\":64754},{\"end\":64766,\"start\":64762},{\"end\":64950,\"start\":64946},{\"end\":64959,\"start\":64955},{\"end\":64983,\"start\":64976},{\"end\":64999,\"start\":64992},{\"end\":65014,\"start\":65006},{\"end\":65028,\"start\":65022},{\"end\":65386,\"start\":65382},{\"end\":65398,\"start\":65391},{\"end\":65410,\"start\":65403},{\"end\":65422,\"start\":65419},{\"end\":65430,\"start\":65427},{\"end\":65441,\"start\":65435},{\"end\":65679,\"start\":65675},{\"end\":65690,\"start\":65684},{\"end\":65702,\"start\":65695},{\"end\":65717,\"start\":65711},{\"end\":65944,\"start\":65937},{\"end\":65959,\"start\":65952},{\"end\":65973,\"start\":65967},{\"end\":65975,\"start\":65974},{\"end\":65986,\"start\":65983},{\"end\":66004,\"start\":65998}]", "bib_author_last_name": "[{\"end\":43993,\"start\":43988},{\"end\":44014,\"start\":44001},{\"end\":44027,\"start\":44022},{\"end\":44182,\"start\":44177},{\"end\":44197,\"start\":44192},{\"end\":44217,\"start\":44207},{\"end\":44232,\"start\":44228},{\"end\":44245,\"start\":44242},{\"end\":44625,\"start\":44617},{\"end\":44637,\"start\":44631},{\"end\":44657,\"start\":44647},{\"end\":44671,\"start\":44665},{\"end\":44687,\"start\":44681},{\"end\":44703,\"start\":44689},{\"end\":44715,\"start\":44705},{\"end\":45247,\"start\":45236},{\"end\":45262,\"start\":45254},{\"end\":45281,\"start\":45271},{\"end\":45292,\"start\":45290},{\"end\":45303,\"start\":45294},{\"end\":45580,\"start\":45571},{\"end\":45602,\"start\":45587},{\"end\":45626,\"start\":45611},{\"end\":45637,\"start\":45628},{\"end\":45856,\"start\":45852},{\"end\":45870,\"start\":45865},{\"end\":45890,\"start\":45882},{\"end\":45897,\"start\":45894},{\"end\":45902,\"start\":45899},{\"end\":46060,\"start\":46055},{\"end\":46072,\"start\":46067},{\"end\":46086,\"start\":46080},{\"end\":46206,\"start\":46202},{\"end\":46218,\"start\":46215},{\"end\":46233,\"start\":46229},{\"end\":46249,\"start\":46244},{\"end\":46260,\"start\":46257},{\"end\":46269,\"start\":46265},{\"end\":46611,\"start\":46604},{\"end\":46625,\"start\":46619},{\"end\":46643,\"start\":46635},{\"end\":46658,\"start\":46651},{\"end\":46674,\"start\":46667},{\"end\":46684,\"start\":46676},{\"end\":46911,\"start\":46907},{\"end\":46930,\"start\":46925},{\"end\":46946,\"start\":46941},{\"end\":46954,\"start\":46948},{\"end\":47116,\"start\":47108},{\"end\":47175,\"start\":47169},{\"end\":47190,\"start\":47183},{\"end\":47210,\"start\":47200},{\"end\":47230,\"start\":47222},{\"end\":47237,\"start\":47232},{\"end\":47467,\"start\":47465},{\"end\":47480,\"start\":47477},{\"end\":47491,\"start\":47487},{\"end\":47511,\"start\":47503},{\"end\":47772,\"start\":47763},{\"end\":47785,\"start\":47782},{\"end\":47797,\"start\":47795},{\"end\":47817,\"start\":47807},{\"end\":48056,\"start\":48050},{\"end\":48071,\"start\":48065},{\"end\":48083,\"start\":48078},{\"end\":48102,\"start\":48097},{\"end\":48120,\"start\":48111},{\"end\":48137,\"start\":48130},{\"end\":48381,\"start\":48375},{\"end\":48398,\"start\":48390},{\"end\":48416,\"start\":48407},{\"end\":48433,\"start\":48426},{\"end\":48650,\"start\":48644},{\"end\":48660,\"start\":48654},{\"end\":48676,\"start\":48666},{\"end\":48699,\"start\":48689},{\"end\":48712,\"start\":48706},{\"end\":48721,\"start\":48714},{\"end\":48980,\"start\":48976},{\"end\":48996,\"start\":48991},{\"end\":49007,\"start\":49004},{\"end\":49021,\"start\":49018},{\"end\":49031,\"start\":49027},{\"end\":49042,\"start\":49039},{\"end\":49310,\"start\":49306},{\"end\":49326,\"start\":49316},{\"end\":49345,\"start\":49339},{\"end\":49360,\"start\":49354},{\"end\":49371,\"start\":49366},{\"end\":49726,\"start\":49722},{\"end\":49742,\"start\":49736},{\"end\":49757,\"start\":49751},{\"end\":49987,\"start\":49965},{\"end\":49998,\"start\":49994},{\"end\":50011,\"start\":50000},{\"end\":50175,\"start\":50165},{\"end\":50186,\"start\":50182},{\"end\":50200,\"start\":50195},{\"end\":50213,\"start\":50207},{\"end\":50225,\"start\":50220},{\"end\":50243,\"start\":50234},{\"end\":50469,\"start\":50461},{\"end\":50483,\"start\":50477},{\"end\":50487,\"start\":50485},{\"end\":50715,\"start\":50708},{\"end\":50730,\"start\":50724},{\"end\":50749,\"start\":50739},{\"end\":50767,\"start\":50758},{\"end\":51012,\"start\":51007},{\"end\":51026,\"start\":51018},{\"end\":51361,\"start\":51359},{\"end\":51377,\"start\":51368},{\"end\":51389,\"start\":51386},{\"end\":51403,\"start\":51399},{\"end\":51410,\"start\":51408},{\"end\":51750,\"start\":51745},{\"end\":51759,\"start\":51752},{\"end\":51771,\"start\":51765},{\"end\":51786,\"start\":51780},{\"end\":51797,\"start\":51788},{\"end\":52043,\"start\":52040},{\"end\":52054,\"start\":52052},{\"end\":52276,\"start\":52273},{\"end\":52292,\"start\":52283},{\"end\":52307,\"start\":52301},{\"end\":52327,\"start\":52321},{\"end\":52338,\"start\":52336},{\"end\":52358,\"start\":52350},{\"end\":52680,\"start\":52677},{\"end\":52692,\"start\":52690},{\"end\":52708,\"start\":52699},{\"end\":52727,\"start\":52718},{\"end\":52744,\"start\":52737},{\"end\":52759,\"start\":52756},{\"end\":52773,\"start\":52769},{\"end\":52793,\"start\":52785},{\"end\":53124,\"start\":53121},{\"end\":53136,\"start\":53134},{\"end\":53156,\"start\":53146},{\"end\":53171,\"start\":53168},{\"end\":53188,\"start\":53181},{\"end\":53204,\"start\":53195},{\"end\":53218,\"start\":53214},{\"end\":53238,\"start\":53230},{\"end\":53561,\"start\":53558},{\"end\":53574,\"start\":53569},{\"end\":53588,\"start\":53584},{\"end\":53599,\"start\":53596},{\"end\":53615,\"start\":53606},{\"end\":53629,\"start\":53626},{\"end\":53837,\"start\":53834},{\"end\":53848,\"start\":53844},{\"end\":53861,\"start\":53858},{\"end\":53876,\"start\":53872},{\"end\":53887,\"start\":53883},{\"end\":53907,\"start\":53899},{\"end\":53921,\"start\":53917},{\"end\":53935,\"start\":53931},{\"end\":54317,\"start\":54309},{\"end\":54330,\"start\":54325},{\"end\":54345,\"start\":54338},{\"end\":54363,\"start\":54355},{\"end\":54381,\"start\":54373},{\"end\":54395,\"start\":54389},{\"end\":54608,\"start\":54598},{\"end\":54618,\"start\":54612},{\"end\":54638,\"start\":54628},{\"end\":54664,\"start\":54653},{\"end\":54680,\"start\":54671},{\"end\":54693,\"start\":54682},{\"end\":55154,\"start\":55144},{\"end\":55164,\"start\":55158},{\"end\":55184,\"start\":55174},{\"end\":55203,\"start\":55197},{\"end\":55216,\"start\":55210},{\"end\":55233,\"start\":55222},{\"end\":55237,\"start\":55235},{\"end\":55448,\"start\":55442},{\"end\":55460,\"start\":55455},{\"end\":55478,\"start\":55472},{\"end\":55496,\"start\":55490},{\"end\":55898,\"start\":55894},{\"end\":55917,\"start\":55907},{\"end\":55933,\"start\":55927},{\"end\":55947,\"start\":55943},{\"end\":55956,\"start\":55951},{\"end\":55984,\"start\":55977},{\"end\":56001,\"start\":55992},{\"end\":56019,\"start\":56010},{\"end\":56032,\"start\":56021},{\"end\":56439,\"start\":56431},{\"end\":56455,\"start\":56449},{\"end\":56943,\"start\":56935},{\"end\":56959,\"start\":56950},{\"end\":56976,\"start\":56969},{\"end\":56992,\"start\":56986},{\"end\":57314,\"start\":57310},{\"end\":57330,\"start\":57325},{\"end\":57342,\"start\":57340},{\"end\":57357,\"start\":57353},{\"end\":57369,\"start\":57364},{\"end\":57380,\"start\":57377},{\"end\":57394,\"start\":57390},{\"end\":57685,\"start\":57679},{\"end\":57695,\"start\":57690},{\"end\":57878,\"start\":57873},{\"end\":57892,\"start\":57887},{\"end\":58094,\"start\":58087},{\"end\":58109,\"start\":58106},{\"end\":58124,\"start\":58117},{\"end\":58139,\"start\":58133},{\"end\":58152,\"start\":58149},{\"end\":58170,\"start\":58163},{\"end\":58185,\"start\":58179},{\"end\":58200,\"start\":58194},{\"end\":58216,\"start\":58209},{\"end\":58228,\"start\":58223},{\"end\":58671,\"start\":58665},{\"end\":58685,\"start\":58681},{\"end\":58696,\"start\":58692},{\"end\":58712,\"start\":58706},{\"end\":58908,\"start\":58901},{\"end\":58924,\"start\":58918},{\"end\":59041,\"start\":59034},{\"end\":59057,\"start\":59051},{\"end\":59153,\"start\":59146},{\"end\":59167,\"start\":59161},{\"end\":59184,\"start\":59174},{\"end\":59544,\"start\":59536},{\"end\":59552,\"start\":59548},{\"end\":59560,\"start\":59554},{\"end\":59797,\"start\":59790},{\"end\":59808,\"start\":59804},{\"end\":59826,\"start\":59818},{\"end\":59842,\"start\":59836},{\"end\":60083,\"start\":60075},{\"end\":60098,\"start\":60092},{\"end\":60117,\"start\":60110},{\"end\":60132,\"start\":60125},{\"end\":60150,\"start\":60141},{\"end\":60439,\"start\":60431},{\"end\":60456,\"start\":60447},{\"end\":60470,\"start\":60463},{\"end\":60486,\"start\":60477},{\"end\":60500,\"start\":60494},{\"end\":60770,\"start\":60762},{\"end\":60784,\"start\":60779},{\"end\":60797,\"start\":60792},{\"end\":60815,\"start\":60808},{\"end\":60833,\"start\":60824},{\"end\":60852,\"start\":60843},{\"end\":61144,\"start\":61136},{\"end\":61163,\"start\":61154},{\"end\":61181,\"start\":61172},{\"end\":61378,\"start\":61373},{\"end\":61392,\"start\":61390},{\"end\":61409,\"start\":61401},{\"end\":61423,\"start\":61417},{\"end\":61443,\"start\":61434},{\"end\":61454,\"start\":61452},{\"end\":61472,\"start\":61464},{\"end\":61805,\"start\":61799},{\"end\":61821,\"start\":61814},{\"end\":62076,\"start\":62073},{\"end\":62085,\"start\":62082},{\"end\":62103,\"start\":62099},{\"end\":62486,\"start\":62481},{\"end\":62505,\"start\":62496},{\"end\":62523,\"start\":62516},{\"end\":62733,\"start\":62728},{\"end\":62752,\"start\":62743},{\"end\":62772,\"start\":62764},{\"end\":62789,\"start\":62779},{\"end\":62807,\"start\":62800},{\"end\":62976,\"start\":62972},{\"end\":62990,\"start\":62986},{\"end\":63431,\"start\":63427},{\"end\":63444,\"start\":63441},{\"end\":63454,\"start\":63451},{\"end\":63474,\"start\":63466},{\"end\":63487,\"start\":63481},{\"end\":63501,\"start\":63497},{\"end\":63773,\"start\":63769},{\"end\":63788,\"start\":63784},{\"end\":63801,\"start\":63795},{\"end\":63811,\"start\":63805},{\"end\":63830,\"start\":63820},{\"end\":63847,\"start\":63843},{\"end\":63863,\"start\":63857},{\"end\":63884,\"start\":63870},{\"end\":63900,\"start\":63893},{\"end\":63912,\"start\":63902},{\"end\":64223,\"start\":64212},{\"end\":64245,\"start\":64234},{\"end\":64267,\"start\":64256},{\"end\":64290,\"start\":64278},{\"end\":64509,\"start\":64507},{\"end\":64522,\"start\":64518},{\"end\":64532,\"start\":64530},{\"end\":64543,\"start\":64541},{\"end\":64712,\"start\":64710},{\"end\":64720,\"start\":64718},{\"end\":64739,\"start\":64729},{\"end\":64752,\"start\":64747},{\"end\":64760,\"start\":64758},{\"end\":64778,\"start\":64767},{\"end\":64953,\"start\":64951},{\"end\":64974,\"start\":64960},{\"end\":64990,\"start\":64984},{\"end\":65004,\"start\":65000},{\"end\":65020,\"start\":65015},{\"end\":65037,\"start\":65029},{\"end\":65389,\"start\":65387},{\"end\":65401,\"start\":65399},{\"end\":65417,\"start\":65411},{\"end\":65425,\"start\":65423},{\"end\":65433,\"start\":65431},{\"end\":65450,\"start\":65442},{\"end\":65682,\"start\":65680},{\"end\":65693,\"start\":65691},{\"end\":65709,\"start\":65703},{\"end\":65726,\"start\":65718},{\"end\":65950,\"start\":65945},{\"end\":65965,\"start\":65960},{\"end\":65981,\"start\":65976},{\"end\":65996,\"start\":65987},{\"end\":66009,\"start\":66005}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":44166,\"start\":43906},{\"attributes\":{\"doi\":\"arXiv:2112.01523\",\"id\":\"b1\"},\"end\":44533,\"start\":44168},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":232352655},\"end\":45134,\"start\":44535},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":227247980},\"end\":45492,\"start\":45136},{\"attributes\":{\"id\":\"b4\"},\"end\":45818,\"start\":45494},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":54474649},\"end\":46022,\"start\":45820},{\"attributes\":{\"id\":\"b6\"},\"end\":46193,\"start\":46024},{\"attributes\":{\"doi\":\"arXiv:2111.15552\",\"id\":\"b7\"},\"end\":46548,\"start\":46195},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":232270138},\"end\":46845,\"start\":46550},{\"attributes\":{\"id\":\"b9\"},\"end\":47089,\"start\":46847},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206770307},\"end\":47165,\"start\":47091},{\"attributes\":{\"id\":\"b11\"},\"end\":47375,\"start\":47167},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":239016913},\"end\":47673,\"start\":47377},{\"attributes\":{\"id\":\"b13\"},\"end\":47986,\"start\":47675},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":54151018},\"end\":48324,\"start\":47988},{\"attributes\":{\"id\":\"b15\"},\"end\":48576,\"start\":48326},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":232379923},\"end\":48891,\"start\":48578},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":248834037},\"end\":49297,\"start\":48893},{\"attributes\":{\"doi\":\"arXiv:2112.01455\",\"id\":\"b18\"},\"end\":49642,\"start\":49299},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":232478424},\"end\":49898,\"start\":49644},{\"attributes\":{\"id\":\"b20\"},\"end\":50158,\"start\":49900},{\"attributes\":{\"id\":\"b21\"},\"end\":50413,\"start\":50160},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b22\"},\"end\":50640,\"start\":50415},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":235619435},\"end\":50977,\"start\":50642},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1363510},\"end\":51351,\"start\":50979},{\"attributes\":{\"doi\":\"arXiv:2105.07112\",\"id\":\"b25\"},\"end\":51676,\"start\":51353},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":227255123},\"end\":51943,\"start\":51678},{\"attributes\":{\"id\":\"b27\"},\"end\":52187,\"start\":51945},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":235313531},\"end\":52573,\"start\":52189},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219601848},\"end\":53055,\"start\":52575},{\"attributes\":{\"id\":\"b30\"},\"end\":53457,\"start\":53057},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":208512845},\"end\":53827,\"start\":53459},{\"attributes\":{\"doi\":\"arXiv:2107.13421\",\"id\":\"b32\"},\"end\":54234,\"start\":53829},{\"attributes\":{\"id\":\"b33\"},\"end\":54592,\"start\":54236},{\"attributes\":{\"id\":\"b34\"},\"end\":55066,\"start\":54594},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":213175590},\"end\":55433,\"start\":55068},{\"attributes\":{\"doi\":\"arXiv:2201.05989\",\"id\":\"b36\"},\"end\":55786,\"start\":55435},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":234365467},\"end\":56341,\"start\":55788},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":227151657},\"end\":56827,\"start\":56343},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":209376368},\"end\":57185,\"start\":56829},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":229924396},\"end\":57629,\"start\":57187},{\"attributes\":{\"id\":\"b41\"},\"end\":57794,\"start\":57631},{\"attributes\":{\"id\":\"b42\"},\"end\":58009,\"start\":57796},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b43\",\"matched_paper_id\":231591445},\"end\":58579,\"start\":58011},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":232352619},\"end\":58871,\"start\":58581},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":221112229},\"end\":59002,\"start\":58873},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":226964601},\"end\":59137,\"start\":59004},{\"attributes\":{\"doi\":\"arXiv:2110.06635\",\"id\":\"b47\"},\"end\":59432,\"start\":59139},{\"attributes\":{\"id\":\"b48\"},\"end\":59719,\"start\":59434},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":220364071},\"end\":59997,\"start\":59721},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":219720931},\"end\":60337,\"start\":59999},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":235352518},\"end\":60697,\"start\":60339},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":54444417},\"end\":61035,\"start\":60699},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":174798113},\"end\":61363,\"start\":61037},{\"attributes\":{\"doi\":\"arXiv:2205.03923\",\"id\":\"b54\"},\"end\":61788,\"start\":61365},{\"attributes\":{\"doi\":\"arXiv:2112.09687\",\"id\":\"b55\"},\"end\":62065,\"start\":61790},{\"attributes\":{\"doi\":\"arXiv:2111.11215\",\"id\":\"b56\"},\"end\":62406,\"start\":62067},{\"attributes\":{\"id\":\"b57\"},\"end\":62674,\"start\":62408},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":53783711},\"end\":62966,\"start\":62676},{\"attributes\":{\"doi\":\"arXiv:2112.05139\",\"id\":\"b59\"},\"end\":63329,\"start\":62968},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":235490453},\"end\":63707,\"start\":63331},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":232045969},\"end\":64142,\"start\":63709},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":232168851},\"end\":64464,\"start\":64144},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":219631039},\"end\":64648,\"start\":64466},{\"attributes\":{\"id\":\"b64\"},\"end\":64944,\"start\":64650},{\"attributes\":{\"doi\":\"arXiv:2112.05131\",\"id\":\"b65\"},\"end\":65317,\"start\":64946},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":232352425},\"end\":65609,\"start\":65319},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":227254854},\"end\":65863,\"start\":65611},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":4766599},\"end\":66181,\"start\":65865}]", "bib_title": "[{\"end\":43977,\"start\":43906},{\"end\":44613,\"start\":44535},{\"end\":45228,\"start\":45136},{\"end\":45842,\"start\":45820},{\"end\":46600,\"start\":46550},{\"end\":47101,\"start\":47091},{\"end\":47456,\"start\":47377},{\"end\":48042,\"start\":47988},{\"end\":48636,\"start\":48578},{\"end\":48965,\"start\":48893},{\"end\":49715,\"start\":49644},{\"end\":50697,\"start\":50642},{\"end\":51000,\"start\":50979},{\"end\":51741,\"start\":51678},{\"end\":52030,\"start\":51945},{\"end\":52263,\"start\":52189},{\"end\":52667,\"start\":52575},{\"end\":53548,\"start\":53459},{\"end\":54299,\"start\":54236},{\"end\":55138,\"start\":55068},{\"end\":55885,\"start\":55788},{\"end\":56421,\"start\":56343},{\"end\":56925,\"start\":56829},{\"end\":57303,\"start\":57187},{\"end\":58080,\"start\":58011},{\"end\":58653,\"start\":58581},{\"end\":58892,\"start\":58873},{\"end\":59025,\"start\":59004},{\"end\":59782,\"start\":59721},{\"end\":60065,\"start\":59999},{\"end\":60421,\"start\":60339},{\"end\":60752,\"start\":60699},{\"end\":61126,\"start\":61037},{\"end\":62719,\"start\":62676},{\"end\":63420,\"start\":63331},{\"end\":63758,\"start\":63709},{\"end\":64201,\"start\":64144},{\"end\":64499,\"start\":64466},{\"end\":65380,\"start\":65319},{\"end\":65673,\"start\":65611},{\"end\":65935,\"start\":65865}]", "bib_author": "[{\"end\":43995,\"start\":43979},{\"end\":44016,\"start\":43995},{\"end\":44029,\"start\":44016},{\"end\":44184,\"start\":44168},{\"end\":44199,\"start\":44184},{\"end\":44219,\"start\":44199},{\"end\":44234,\"start\":44219},{\"end\":44247,\"start\":44234},{\"end\":44627,\"start\":44615},{\"end\":44639,\"start\":44627},{\"end\":44659,\"start\":44639},{\"end\":44673,\"start\":44659},{\"end\":44689,\"start\":44673},{\"end\":44705,\"start\":44689},{\"end\":44717,\"start\":44705},{\"end\":45249,\"start\":45230},{\"end\":45264,\"start\":45249},{\"end\":45283,\"start\":45264},{\"end\":45294,\"start\":45283},{\"end\":45305,\"start\":45294},{\"end\":45582,\"start\":45564},{\"end\":45604,\"start\":45582},{\"end\":45628,\"start\":45604},{\"end\":45639,\"start\":45628},{\"end\":45858,\"start\":45844},{\"end\":45872,\"start\":45858},{\"end\":45892,\"start\":45872},{\"end\":45899,\"start\":45892},{\"end\":45904,\"start\":45899},{\"end\":46062,\"start\":46051},{\"end\":46074,\"start\":46062},{\"end\":46088,\"start\":46074},{\"end\":46208,\"start\":46195},{\"end\":46220,\"start\":46208},{\"end\":46235,\"start\":46220},{\"end\":46251,\"start\":46235},{\"end\":46262,\"start\":46251},{\"end\":46271,\"start\":46262},{\"end\":46613,\"start\":46602},{\"end\":46627,\"start\":46613},{\"end\":46645,\"start\":46627},{\"end\":46660,\"start\":46645},{\"end\":46676,\"start\":46660},{\"end\":46686,\"start\":46676},{\"end\":46913,\"start\":46905},{\"end\":46932,\"start\":46913},{\"end\":46948,\"start\":46932},{\"end\":46956,\"start\":46948},{\"end\":47118,\"start\":47103},{\"end\":47177,\"start\":47167},{\"end\":47192,\"start\":47177},{\"end\":47212,\"start\":47192},{\"end\":47232,\"start\":47212},{\"end\":47239,\"start\":47232},{\"end\":47469,\"start\":47458},{\"end\":47482,\"start\":47469},{\"end\":47493,\"start\":47482},{\"end\":47513,\"start\":47493},{\"end\":47774,\"start\":47758},{\"end\":47787,\"start\":47774},{\"end\":47799,\"start\":47787},{\"end\":47819,\"start\":47799},{\"end\":48058,\"start\":48044},{\"end\":48073,\"start\":48058},{\"end\":48085,\"start\":48073},{\"end\":48104,\"start\":48085},{\"end\":48122,\"start\":48104},{\"end\":48139,\"start\":48122},{\"end\":48383,\"start\":48369},{\"end\":48400,\"start\":48383},{\"end\":48418,\"start\":48400},{\"end\":48435,\"start\":48418},{\"end\":48652,\"start\":48638},{\"end\":48662,\"start\":48652},{\"end\":48678,\"start\":48662},{\"end\":48701,\"start\":48678},{\"end\":48714,\"start\":48701},{\"end\":48723,\"start\":48714},{\"end\":48982,\"start\":48967},{\"end\":48998,\"start\":48982},{\"end\":49009,\"start\":48998},{\"end\":49023,\"start\":49009},{\"end\":49033,\"start\":49023},{\"end\":49044,\"start\":49033},{\"end\":49312,\"start\":49301},{\"end\":49328,\"start\":49312},{\"end\":49347,\"start\":49328},{\"end\":49362,\"start\":49347},{\"end\":49373,\"start\":49362},{\"end\":49728,\"start\":49717},{\"end\":49744,\"start\":49728},{\"end\":49759,\"start\":49744},{\"end\":49989,\"start\":49955},{\"end\":50000,\"start\":49989},{\"end\":50013,\"start\":50000},{\"end\":50177,\"start\":50160},{\"end\":50188,\"start\":50177},{\"end\":50202,\"start\":50188},{\"end\":50215,\"start\":50202},{\"end\":50227,\"start\":50215},{\"end\":50245,\"start\":50227},{\"end\":50471,\"start\":50459},{\"end\":50485,\"start\":50471},{\"end\":50489,\"start\":50485},{\"end\":50717,\"start\":50699},{\"end\":50732,\"start\":50717},{\"end\":50751,\"start\":50732},{\"end\":50769,\"start\":50751},{\"end\":51014,\"start\":51002},{\"end\":51028,\"start\":51014},{\"end\":51363,\"start\":51353},{\"end\":51379,\"start\":51363},{\"end\":51391,\"start\":51379},{\"end\":51405,\"start\":51391},{\"end\":51412,\"start\":51405},{\"end\":51752,\"start\":51743},{\"end\":51761,\"start\":51752},{\"end\":51773,\"start\":51761},{\"end\":51788,\"start\":51773},{\"end\":51799,\"start\":51788},{\"end\":52045,\"start\":52032},{\"end\":52056,\"start\":52045},{\"end\":52278,\"start\":52265},{\"end\":52294,\"start\":52278},{\"end\":52309,\"start\":52294},{\"end\":52329,\"start\":52309},{\"end\":52340,\"start\":52329},{\"end\":52360,\"start\":52340},{\"end\":52682,\"start\":52669},{\"end\":52694,\"start\":52682},{\"end\":52710,\"start\":52694},{\"end\":52729,\"start\":52710},{\"end\":52746,\"start\":52729},{\"end\":52761,\"start\":52746},{\"end\":52775,\"start\":52761},{\"end\":52795,\"start\":52775},{\"end\":53126,\"start\":53113},{\"end\":53138,\"start\":53126},{\"end\":53158,\"start\":53138},{\"end\":53173,\"start\":53158},{\"end\":53190,\"start\":53173},{\"end\":53206,\"start\":53190},{\"end\":53220,\"start\":53206},{\"end\":53240,\"start\":53220},{\"end\":53563,\"start\":53550},{\"end\":53576,\"start\":53563},{\"end\":53590,\"start\":53576},{\"end\":53601,\"start\":53590},{\"end\":53617,\"start\":53601},{\"end\":53631,\"start\":53617},{\"end\":53839,\"start\":53829},{\"end\":53850,\"start\":53839},{\"end\":53863,\"start\":53850},{\"end\":53878,\"start\":53863},{\"end\":53889,\"start\":53878},{\"end\":53909,\"start\":53889},{\"end\":53923,\"start\":53909},{\"end\":53937,\"start\":53923},{\"end\":54319,\"start\":54301},{\"end\":54332,\"start\":54319},{\"end\":54347,\"start\":54332},{\"end\":54365,\"start\":54347},{\"end\":54383,\"start\":54365},{\"end\":54397,\"start\":54383},{\"end\":54610,\"start\":54594},{\"end\":54620,\"start\":54610},{\"end\":54640,\"start\":54620},{\"end\":54666,\"start\":54640},{\"end\":54682,\"start\":54666},{\"end\":54695,\"start\":54682},{\"end\":55156,\"start\":55140},{\"end\":55166,\"start\":55156},{\"end\":55186,\"start\":55166},{\"end\":55205,\"start\":55186},{\"end\":55218,\"start\":55205},{\"end\":55235,\"start\":55218},{\"end\":55239,\"start\":55235},{\"end\":55450,\"start\":55435},{\"end\":55462,\"start\":55450},{\"end\":55480,\"start\":55462},{\"end\":55498,\"start\":55480},{\"end\":55900,\"start\":55887},{\"end\":55919,\"start\":55900},{\"end\":55935,\"start\":55919},{\"end\":55949,\"start\":55935},{\"end\":55958,\"start\":55949},{\"end\":55986,\"start\":55958},{\"end\":56003,\"start\":55986},{\"end\":56021,\"start\":56003},{\"end\":56034,\"start\":56021},{\"end\":56441,\"start\":56423},{\"end\":56457,\"start\":56441},{\"end\":56945,\"start\":56927},{\"end\":56961,\"start\":56945},{\"end\":56978,\"start\":56961},{\"end\":56994,\"start\":56978},{\"end\":57316,\"start\":57305},{\"end\":57332,\"start\":57316},{\"end\":57344,\"start\":57332},{\"end\":57359,\"start\":57344},{\"end\":57371,\"start\":57359},{\"end\":57382,\"start\":57371},{\"end\":57396,\"start\":57382},{\"end\":57687,\"start\":57674},{\"end\":57697,\"start\":57687},{\"end\":57880,\"start\":57866},{\"end\":57894,\"start\":57880},{\"end\":58096,\"start\":58082},{\"end\":58111,\"start\":58096},{\"end\":58126,\"start\":58111},{\"end\":58141,\"start\":58126},{\"end\":58154,\"start\":58141},{\"end\":58172,\"start\":58154},{\"end\":58187,\"start\":58172},{\"end\":58202,\"start\":58187},{\"end\":58218,\"start\":58202},{\"end\":58230,\"start\":58218},{\"end\":58673,\"start\":58655},{\"end\":58687,\"start\":58673},{\"end\":58698,\"start\":58687},{\"end\":58714,\"start\":58698},{\"end\":58910,\"start\":58894},{\"end\":58926,\"start\":58910},{\"end\":59043,\"start\":59027},{\"end\":59059,\"start\":59043},{\"end\":59155,\"start\":59139},{\"end\":59169,\"start\":59155},{\"end\":59186,\"start\":59169},{\"end\":59546,\"start\":59532},{\"end\":59554,\"start\":59546},{\"end\":59562,\"start\":59554},{\"end\":59799,\"start\":59784},{\"end\":59810,\"start\":59799},{\"end\":59828,\"start\":59810},{\"end\":59844,\"start\":59828},{\"end\":60085,\"start\":60067},{\"end\":60100,\"start\":60085},{\"end\":60119,\"start\":60100},{\"end\":60134,\"start\":60119},{\"end\":60152,\"start\":60134},{\"end\":60441,\"start\":60423},{\"end\":60458,\"start\":60441},{\"end\":60472,\"start\":60458},{\"end\":60488,\"start\":60472},{\"end\":60502,\"start\":60488},{\"end\":60772,\"start\":60754},{\"end\":60786,\"start\":60772},{\"end\":60799,\"start\":60786},{\"end\":60817,\"start\":60799},{\"end\":60835,\"start\":60817},{\"end\":60854,\"start\":60835},{\"end\":61146,\"start\":61128},{\"end\":61165,\"start\":61146},{\"end\":61183,\"start\":61165},{\"end\":61380,\"start\":61365},{\"end\":61394,\"start\":61380},{\"end\":61411,\"start\":61394},{\"end\":61425,\"start\":61411},{\"end\":61445,\"start\":61425},{\"end\":61456,\"start\":61445},{\"end\":61474,\"start\":61456},{\"end\":61807,\"start\":61790},{\"end\":61823,\"start\":61807},{\"end\":62078,\"start\":62067},{\"end\":62087,\"start\":62078},{\"end\":62105,\"start\":62087},{\"end\":62488,\"start\":62474},{\"end\":62507,\"start\":62488},{\"end\":62525,\"start\":62507},{\"end\":62735,\"start\":62721},{\"end\":62754,\"start\":62735},{\"end\":62774,\"start\":62754},{\"end\":62791,\"start\":62774},{\"end\":62809,\"start\":62791},{\"end\":62978,\"start\":62968},{\"end\":62992,\"start\":62978},{\"end\":63433,\"start\":63422},{\"end\":63446,\"start\":63433},{\"end\":63456,\"start\":63446},{\"end\":63476,\"start\":63456},{\"end\":63489,\"start\":63476},{\"end\":63503,\"start\":63489},{\"end\":63775,\"start\":63760},{\"end\":63790,\"start\":63775},{\"end\":63803,\"start\":63790},{\"end\":63813,\"start\":63803},{\"end\":63832,\"start\":63813},{\"end\":63849,\"start\":63832},{\"end\":63865,\"start\":63849},{\"end\":63886,\"start\":63865},{\"end\":63902,\"start\":63886},{\"end\":63914,\"start\":63902},{\"end\":64225,\"start\":64203},{\"end\":64247,\"start\":64225},{\"end\":64269,\"start\":64247},{\"end\":64292,\"start\":64269},{\"end\":64511,\"start\":64501},{\"end\":64524,\"start\":64511},{\"end\":64534,\"start\":64524},{\"end\":64545,\"start\":64534},{\"end\":64714,\"start\":64702},{\"end\":64722,\"start\":64714},{\"end\":64741,\"start\":64722},{\"end\":64754,\"start\":64741},{\"end\":64762,\"start\":64754},{\"end\":64780,\"start\":64762},{\"end\":64955,\"start\":64946},{\"end\":64976,\"start\":64955},{\"end\":64992,\"start\":64976},{\"end\":65006,\"start\":64992},{\"end\":65022,\"start\":65006},{\"end\":65039,\"start\":65022},{\"end\":65391,\"start\":65382},{\"end\":65403,\"start\":65391},{\"end\":65419,\"start\":65403},{\"end\":65427,\"start\":65419},{\"end\":65435,\"start\":65427},{\"end\":65452,\"start\":65435},{\"end\":65684,\"start\":65675},{\"end\":65695,\"start\":65684},{\"end\":65711,\"start\":65695},{\"end\":65728,\"start\":65711},{\"end\":65952,\"start\":65937},{\"end\":65967,\"start\":65952},{\"end\":65983,\"start\":65967},{\"end\":65998,\"start\":65983},{\"end\":66011,\"start\":65998}]", "bib_venue": "[{\"end\":44033,\"start\":44029},{\"end\":44325,\"start\":44263},{\"end\":44788,\"start\":44717},{\"end\":45309,\"start\":45305},{\"end\":45562,\"start\":45494},{\"end\":45908,\"start\":45904},{\"end\":46049,\"start\":46024},{\"end\":46346,\"start\":46287},{\"end\":46690,\"start\":46686},{\"end\":46903,\"start\":46847},{\"end\":47122,\"start\":47118},{\"end\":47265,\"start\":47239},{\"end\":47517,\"start\":47513},{\"end\":47756,\"start\":47675},{\"end\":48146,\"start\":48139},{\"end\":48367,\"start\":48326},{\"end\":48727,\"start\":48723},{\"end\":49078,\"start\":49044},{\"end\":49763,\"start\":49759},{\"end\":49953,\"start\":49900},{\"end\":50280,\"start\":50245},{\"end\":50457,\"start\":50415},{\"end\":50792,\"start\":50769},{\"end\":51117,\"start\":51028},{\"end\":51489,\"start\":51428},{\"end\":51803,\"start\":51799},{\"end\":52063,\"start\":52056},{\"end\":52373,\"start\":52360},{\"end\":52804,\"start\":52795},{\"end\":53111,\"start\":53057},{\"end\":53635,\"start\":53631},{\"end\":54006,\"start\":53953},{\"end\":54405,\"start\":54397},{\"end\":54824,\"start\":54695},{\"end\":55243,\"start\":55239},{\"end\":55583,\"start\":55514},{\"end\":56057,\"start\":56034},{\"end\":56538,\"start\":56457},{\"end\":56998,\"start\":56994},{\"end\":57400,\"start\":57396},{\"end\":57672,\"start\":57631},{\"end\":57864,\"start\":57796},{\"end\":58278,\"start\":58234},{\"end\":58718,\"start\":58714},{\"end\":58930,\"start\":58926},{\"end\":59063,\"start\":59059},{\"end\":59260,\"start\":59202},{\"end\":59530,\"start\":59434},{\"end\":59851,\"start\":59844},{\"end\":60159,\"start\":60152},{\"end\":60509,\"start\":60502},{\"end\":60858,\"start\":60854},{\"end\":61190,\"start\":61183},{\"end\":61551,\"start\":61490},{\"end\":61907,\"start\":61839},{\"end\":62210,\"start\":62121},{\"end\":62472,\"start\":62408},{\"end\":62813,\"start\":62809},{\"end\":63128,\"start\":63008},{\"end\":63510,\"start\":63503},{\"end\":63918,\"start\":63914},{\"end\":64296,\"start\":64292},{\"end\":64549,\"start\":64545},{\"end\":64700,\"start\":64650},{\"end\":65105,\"start\":65055},{\"end\":65456,\"start\":65452},{\"end\":65732,\"start\":65728},{\"end\":66015,\"start\":66011},{\"end\":44846,\"start\":44790},{\"end\":51193,\"start\":51119},{\"end\":56606,\"start\":56540}]"}}}, "year": 2023, "month": 12, "day": 17}