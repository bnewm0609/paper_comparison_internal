{"id": 252783094, "updated": "2023-06-13 14:41:13.587", "metadata": {"title": "Single Image Shadow Detection via Complementary Mechanism", "authors": "[{\"first\":\"Yurui\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Xueyang\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Chengzhi\",\"last\":\"Cao\",\"middle\":[]},{\"first\":\"Xi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Qibin\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Zheng-Jun\",\"last\":\"Zha\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 30th ACM International Conference on Multimedia", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In this paper, we present a novel shadow detection framework by investigating the mutual complementary mechanisms contained in this specific task. Our method is based on a key observation: in a single shadow image, shadow regions and non-shadow counterparts are complementary to each other in nature, thus a better estimation on one side leads to an improved estimation on the other, and vice versa. Motivated by this observation, we first leverage two parallel interactive branches to jointly produce shadow and non-shadow masks. The interaction between two parallel branches is to retain the deactivated intermediate features of one branch by introducing the negative activation technique, which could serve as complementary features to the other branch. Besides, we also apply identity reconstruction loss as complementary training guidance at the image level. Finally, we design two discriminative losses to satisfy the complementary requirements of shadow detection, i.e., neither missing any shadow regions nor falsely detecting non-shadow regions. By fully exploring and exploiting the complementary mechanism of shadow detection, our method can confidently predict more accurate shadow detection results. Extensive experiments on the three widely-used benchmarks demonstrate our proposed method achieves superior shadow detection performance against state-of-the-art methods with a relatively low computational cost.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/ZhuFCWSZ22", "doi": "10.1145/3503161.3547904"}}, "content": {"source": {"pdf_hash": "30cc6906f90dfa716b53c9662eadc335aee5714e", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4cc520d5f72ccb0b00b4682139d79f309804b4b5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/30cc6906f90dfa716b53c9662eadc335aee5714e.txt", "contents": "\nSingle Image Shadow Detection via Complementary Mechanism\nACMCopyright ACMOctober 10-14, 2022. 2022. October 10-14, 2022\n\nYurui Zhu \nXueyang Fu xyfu@ustc.edu.cn \nXi Wang wangxxi@mail.ustc.edu.cn \nYurui Zhu \nXueyang Fu xyfu@ustc.edu.cn. \nChengzhi Cao chengzhicao@mail.ustc.edu.cn \nXi Wang \nQibin Sun qibinsun@ustc.edu.cn \nZheng-Jun \n\nUniversity of Science and Technology of China Hefei\nAnhuiChina\n\n\nChengzhi Cao\nUniversity of Science and Technology of China Hefei\nAnhuiChina\n\n\nUniversity of Science and Technology of China Hefei\nAnhuiChina\n\n\nQibin Sun\nUniversity of Science and Technology of China Hefei\nAnhuiChina\n\n\nZheng-Jun Zha\nUniversity of Science and Technology of China Hefei\nAnhuiChina\n\n\nUniversity of Science and Technology of China Hefei\nAnhuiChina\n\nSingle Image Shadow Detection via Complementary Mechanism\n\nProceedings of the 30th ACM International Conference on Multimedia (MM '22)\nthe 30th ACM International Conference on Multimedia (MM '22)Lisboa, Portugal Zha; Lisboa, Portugal; Lisbon, PortugalACM10October 10-14, 2022. 2022. October 10-14, 202210.1145/3503161.3547904KEYWORDS Shadow Detection, Neural Networks, Complementary Mechanisms * Xueyang Fu is the corresponding author (CCS CONCEPTS \u2022 Computing methodologies \u2192 Scene understanding\nIn this paper, we present a novel shadow detection framework by investigating the mutual complementary mechanisms contained in this specific task. Our method is based on a key observation: in a single shadow image, shadow regions and non-shadow counterparts are complementary to each other in nature, thus a better estimation on one side leads to an improved estimation on the other, and vice versa. Motivated by this observation, we first leverage two parallel interactive branches to jointly produce shadow and non-shadow masks. The interaction between two parallel branches is to retain the deactivated intermediate features of one branch by introducing the negative activation technique, which could serve as complementary features to the other branch. Besides, we also apply identity reconstruction loss as complementary training guidance at the image level. Finally, we design two discriminative losses to satisfy the complementary requirements of shadow detection, i.e., neither missing any shadow regions nor falsely detecting non-shadow regions. By fully exploring and exploiting the complementary mechanism of shadow detection, our method can confidently predict more accurate shadow detection results. Extensive experiments on the three widely-used benchmarks demonstrate our proposed method achieves superior shadow detection performance against state-of-the-art methods with a relatively low computational cost. Our source code is available at this repository.\n\nINTRODUCTION\n\nAs a common natural phenomenon, shadows are usually cast when associated objects completely or partially block the light sources from a specific direction. Hence, aware of the shadow location could offer valuable visual hints for perceiving the scene geometry [18,19,29], the light sources position [23,30], camera parameters [43], etc. This will largely facilitate various scene understanding related works, e.g., image segmentation [8], 3D scene reconstruction [42] and rough geometry estimation [31]. Therefore, detecting shadows from a single image is crucial for computer vision tasks.\n\nCurrently, a series of methods have been proposed for this specific task, which could be roughly divided into two flavors: traditional methods and convolutional neural networks (CNNs) methods. The former mainly focuses on designing hand-crafted priors and assumptions, e.g., color chromaticity [4,5], textures and illumination cues [16,34,50]. Since the hand-crafted priors are designed for specific shadow scenes, when the shadow scenarios deviate from the predefined assumptions, the detection results of these methods in complex conditions are often unsatisfactory.\n\nRecently, deep CNNs-based approaches merge as a promising solution for shadow detection and dominate this field. However, how to accurately detect shadows still remains challenging, e.g., existing methods are easily mistaking the ambiguous areas. To address the above drawbacks, researchers attempt to [2,13,13,15,40,51] explore effective contextual information or multi-scale techniques to boost the detection performance. There also exist methods to explore the potential hints to resolve these ambiguous areas. For example, FDRNet [52] suggests the intensity-bias cue and devises a novel decomposition and re-weighting strategy to mitigate the intensity-bias for shadow detection. However, such external cues also bring the side effect, e.g., due to the gradual changes of the shadow effects near the shadow boundary [24,25], the intensity bias will affect the shadow intensity across the shadow boundary, resulting in coarse boundary results. However, there is potential complementary information internally within this specific task that\n\nhas not yet been utilized. For example, the above methods usually employ a single-branch framework for shadow detection, which fails to utilize the mutual complementary contextual information from their non-shadow counterparts, limiting their further shadow detection performance.\n\nInstead, in this paper, we explore and exploit the mutual complementary mechanisms contained in this specific task for shadow detection. We first notice the essential complementarity between the shadow regions and their non-shadow counterparts. Therefore, we propose a novel shadow detection framework, which consists of two interactive branches to concurrently estimate the shadow and non-shadow masks. In order to collaborate and boost these two complementary parts, we introduce the negative activation technique and the identity reconstruction constraint to fully mine the complementarity at the feature and image level. For the features interaction, we retain the deactivated intermediate features of one branch and offer these features as additional supplementary information to the other branch, and vice versa. At the image level, the results of two branches should always sum to a constant at each corresponding position, and we impose this identity constraint as the guidance to optimize our framework. With the complementary auxiliary information from each other's branches, our network could embrace interesting merits: while improving the performance of shadow detection, it also obviously improves the confidence of predicted results.\n\nIn addition, we further explore the complementary requirements of shadow detection: not to miss all shadow areas, as well as not to mistake any non-shadow counterparts. Moreover, we transfer these complementary requirements as two complementary discriminative losses for our framework. To be specific, we introduce the inner and outer discriminators to encourage the shadow detection branch to predict the accurate shadow locations in an adversarial manner. Moreover, we further employ the dilation and erosion operation on the ground truth masks to generate pseudo masks for benefiting the discriminating capability of two discriminators. Finally, our framework could confidently predict more accurate shadow detection results by fully exploring and exploiting the complementary mechanism of shadow detection.\n\nIn summary, the contributions of this paper are as follows:\n\n\u2022 We propose a novel shadow detection framework, in which the mutual complementary information contained in this specific task is explored and exploited, e.g., the complementarity between the shadow regions and non-shadow counterparts, and the complementary requirements of shadow detection. \u2022 We observe that the shadow and surrounding non-shadow regions are interrelated and complementary. Hence, we elaborately design dual interactive branches to cooperatively offer complementary auxiliary information and supervision for each other at the feature and image-level via introducing the negative activation technique and identical reconstruction loss. \u2022 We investigate the complementary requirements of shadow detection: neither missing any shadow regions nor falsely detecting non-shadow regions. Moreover, we further devise the complementary discriminative constraints derived from the above complementary requirements to boost the performance of shadow detection. \u2022 Extensive experiments indicate that our proposed method could achieve superior shadow detection performance both quantitatively and qualitatively with relatively smaller parameters and faster inference speed.\n\n\nRELATED WORK\n\nIn this paper, we mainly focus on shadow detection from a single shadow image. The corresponding studies can be roughly divided into two groups: traditional and Convolutional Neural Networks (CNNs)-based methods.\n\n\nTraditional Methods\n\nEarly methods focus on exploring various hints for shadow detection, e.g., illumination models or color priors, hand-crafted image features, edges, and textures. Zhu et al. [50] attempt to detect shadows in real-world scenes where color information is unavailable based on the usual characteristics of shadows. Vicente et al. [37] identify shadows via incorporating the learned appearance and contextual cues of shadows. However, These strategies could produce relatively accurate detection results, but their performances will drop significantly when dealing with complex shadow scenes where the handcrafted features are far from enough to discern the shadows regions.\n\n\nCNNs-based Methods\n\nThanks to the remarkable success of CNNs in various computer vision tasks, CNNs-based shadow detection methods are capable of easily identifying shadow context from the public shadow datasets. Therefore, CNNs-based shadow detection methods have been far exceeding the performance of previous traditional methods. Khan et al. [20] firstly propose a CNNs-based framework to automatically detect shadows by building a 7-layer network structure. Shen et al. [32] constructs CNNs to exploit the detected shadow edges and efficient least-square optimization for detecting shadow regions.\n\nRecently, many works tend to design more efficient and effective feature extraction modules to improve the network's ability to understand shadow scenes, thereby further enhancing shadow detection performance. For example, Zhu et al. [51] formulate the recurrent attention residual module and construct the bidirectional feature pyramid network to explore the global and local shadow contexts. Hu et al. [15] present a direction-aware spatial module to aggregate the contextual features for better detecting shadows. DSD [47] proposes a distraction-aware module to explicitly consider various ambiguous cases for improving the performance of shadow detection. Fang et al. [2] explore an effective context augmentation with the parallel multi-scale convolution operations for robust shadow detections.\n\nThere also exist studies that attempt to address shadow detection from other perspectives. RCMPNet [27] develops a designed ensemble model to predict corresponding confidence maps of previous methods' results, causing their performance to be greatly dependent on the performance of the previous methods. ADNet [26] employs the shadow attenuation network to produce more adversarial training examples for their shadow detection network. Hu et al. [14] tend to resolve the shadow detection for the general real-world shadow scenes.\n\nIn the era of deep learning, CNNs-based methods also explore the potential cues for shadow detection, e.g., combining with multitask learning [1,38], intensity bias [52], ensemble techniques [27]. However, few methods notice the complementarity between the shadow regions and their non-shadow counterparts for this specific task. To the best of our knowledge, we are the first to explore and exploit this complementary mechanism to boost the performance of shadow detection. Moreover, previous methods also make use of Generative Adversarial Networks (GANs) to address the shadow detection problem. However, the purpose of using GANs is different from ours. scGAN [28] introduces a tunable sensitivity parameter to overcome the inflexibility of GANs in the shadow detection task. ST-CGAN [38] takes advantage of GANs to obtain the high-level semantics and global scene characteristics for shadow detection. Unlike theirs, we utilize the tailored GANs to implement the complementary requirements of shadow detection, e.g., the specific inputs after erosion or dilation operation for discriminators.\n\n\nMETHOD\n\nWe present our proposed shadow detection framework in Figure 1. In this section, we would like to further describe the motivation for designing the above framework and the details of our framework.\n\n\nMotivation\n\nHere we illustrate the motivation behind the effective complementary mechanisms utilized in our framework. The first complementarity is that we observe that the shadow regions and non-shadow counterparts are mutually interrelated and complementary in nature. The shadow maskM and non-shadow counterpartM themselves should satisfy the identity constraint relationship: 1=M + M . The design of dual detection branches could collaborate these two estimations by reducing the output of one branch from the original input as complementary auxiliary information for the other branch. However, previous methods only predict shadow masks, which naturally ignores such auxiliary information. Furthermore, Figure 2: Visual results of DSD [47] and FDRNet [52] with the different inputs. The inputs are the original images and the cropped patches that contain only the shadow regions. When the cropped patches are used as inputs, the shadow detection performance of the previous methods drops drastically, indicating the surrounding non-shadow information is crucial for shadow detection.\n\ninspired by the fact that the information of the surrounding nonshadow regions as references is essential for existing methods to realize the judgment of the shadow location. As shown in Figure 2, although the existing methods could predict the shadow masks well in the images with shadows and non-shadow regions. However, the cropped patches from original inputs only contain shadow regions, and the performance of these methods drops significantly. This also indicates that the assistance of the surrounding non-shadow regions is also required and important for shadow detection.\n\nTherefore, we explicitly explore and exploit the complementary relationship between the non-shadow and shadow regions to achieve better performance. We develop a novel shadow detection framework, which jointly generates the shadow maskM and their non-shadow counterpartM via dual interactive branches. Then, we utilize the complementary information through the interaction of intermediate features between two branches and the identity supervision for the final outputs.\n\nExcept for the complementary mechanism between the shadow and non-shadow regions, we also explore and exploit the complementary requirements of shadow detection: neither missing any shadow regions nor falsely detecting non-shadow regions. These two requirements are obviously complementary, and only by meeting these two requirements, accurate shadow detection results could be obtained. Hence, motivated by the above complementary requirements, we devise two complementary discriminative losses imposed on the estimated results of the shadow detection branch and implement them with two complementary discriminators ( & ). Utilizing imposes the inner discriminative constraint to reduce the detection network to miss any ambiguous shadow regions, e.g., shadow regions like non-shadow patterns. Meanwhile, utilizing imposes the outer discriminative constraint to avoid falsely detecting non-shadow regions like shadow patterns. Note that similar requirements could also be applied to the non-shadow detection branch, but due to the limited memory of our device, we only utilize the complementary requirements for the shadow detection branch. Extensive experiments indicate that this complementarity also brings obvious improvement.\n\n\nThe Generator\n\nOur Generator is in line with the classical encoder-decoder structure. However, unlike the previous methods, our generator includes one parameters-shared encoder for extracting the backbone features, two interactive decoders as detection heads for predictin\u011d M andM given the input shadow images I \u2208 [0, 1] \u00d7 \u00d7 . , and represent the channel, height and width of the original images, respectively. Next, we provide detailed descriptions of the implementation architecture of our framework.\n\nBackbone Encoder. Following [52], we also employ the lightweight EfficientNet-B3 [33] as our backbone network to extract the hierarchical features, namely F ( = 1, 2, \u00b7 \u00b7 \u00b7 , ). After that, these features with different scales are aggregated as encoder features with the up-sampling and concatenation operations. Finally, in order to reduce the dimensions of encoder features F and the computation costs, we apply the 1\u00d71 point-wise convolutional layers to reduce their channels to 32. The aforementioned operations can be defined as\nF = ( [F 0 , F \u2191 1 , ..., F \u2191 ]), F = 1\u00d71 (F ),(1)\nwhere \u2191 denotes the bilinear up-sampling operation. Dual Interactive Decoders. Dual decoders share the same architecture, consisting of several Residual Blocks [10] as our basic blocks. Each basic block adopts ReLU [6] as the activation function after the convolution, which is one of the widely-used activation functions in current network architectures. Intuitively, in the decoder for predicting shadow masks, the activation function is devised to highlight (activate) the desired shadow regions under supervised learning, and vice versa. Because of the complementarity between the shadow and non-shadow counterparts, the deactivated features of the shadow detection decoder could be delivered to the non-shadow detection decoder. On the contrary, the deactivated features of the non-shadow detection decoder could be delivered to assist the shadow detection. Hence, we introduce the Negative Activation Technique (NAT) [12] to retain the deactivated features to achieve the interaction between two decoders at the feature level. Assuming that F is the intermediate feature, then the NAT operation of the ReLU function can be defined as\nF \u2212 = \u2212 (F) = F \u2212 (F) = {F, 0}.(2)\nAs shown in Figure 1, we conduct the interaction of the two branches after the basic block of each branch. Here, we utilize F ( \u2208 1, 2 for two decoders ) to represent outputs of the -th basic block, and F +1 to represent the inputs of the ( + 1)-th basic block to illustrate the feature interaction process between two decoders as follows:\nF +1 1 = ([ (F 1 ), \u2212 (F 2 )]), F +1 2 = ([ (F 2 ), \u2212 (F 1 )]).(3)\nFinally, we optimize our generator with various objective functions. Following [47,52], we adopt the weighted binary crossentropy loss for the results of the shadow detection branch, which is defined as\nL 1 (M , M ) = \u2212 \u2211\ufe01 ( ( ) log(M ( ) )+ (1 \u2212 ( ) log(1 \u2212M ( ) ))),(4)\nwhere M denotes the ground truth shadow masks; denotes the pixel index along the spatial dimension. , , and refer to the number of pixels in the shadow regions, the number of pixels in the non-shadow regions, and the number of pixels in the entire image, respectively. Similarly, the loss of the non-shadow detection branch, which is defined as\nL 2 (M , M ) = \u2212 \u2211\ufe01 [ ( ) log(M ( ) )+ (1 \u2212 ( ) log(1 \u2212M ( ) ))],(5)\nwhere M denotes the ground truth non-shadow masks; denotes the pixel index along the spatial dimension. Moreover, the estimated resultsM andM should satisfy the identity constraint as follows:\nL . (M ,M , 1) = \u2225M +M \u2212 1\u2225 1 ,(6)\nwhere 1 denotes an all-ones matrix of the same size asM andM . Besides, we also impose adversarial losses, which are defined as \n\nwhere I is the input shadow image, acting as the conditional input in the discriminators. Therefore, the total loss of our generator is a weighted sum of the predefined losses:\nL = L 1 + 2 L 2 + . L . + L ,(8)\nwhere 2 , . , and indicate the weight factors.\n\n\nDual Complementary Discriminators\n\nThese two complementary discriminators have the same network architecture, but different functions. The architectures for and refer to [38,46]. During the training phase, we refer to the Generative Adversarial Networks (GANs) [7] to update the shadow detection network and discriminators in an alternately iterative manner. Inner Discriminator. The inner discriminator is trained to distinguish whether the detection shadow results miss any shadow regions. Meanwhile, in order to fool , our generator has to detect the shadow masksM covering possible shadow regions. Therefore, we achieve the first requirement in an adversarial fashion. Moreover, we employ the image erosion operation [3,17] on the ground truth masks to obtain pseudo masks M E for enhancing the discriminating capability of . Note that dilation and erosion are basic image morphological processing operations. The dilation operation often utilizes a structuring element for probing and expanding the shapes contained in the input image. And the erosion operation often utilizes a structuring element for probing and reducing the shapes contained in the input image. Therefore, the eroded masks naturally miss partial shadow regions, and the dilated masks naturally include some extra non-shadow regions, which could help the inner discriminator to distinguish whether the detection shadow results miss any shadow regions. Therefore, the corresponding adversarial constraint to optimize can be defined as \n\nOuter Discriminator. The outer discriminator is trained to distinguish whether the detection shadow branch falsely detects any non-shadow regions as shadows. Meanwhile, in order to fool , our generator has to detect the shadow masksM and avoid detecting possible non-shadow regions. Similarly, we employ the image dilation operation on the ground truth masks to obtain pseudo masks M D . The dilated masks naturally include some extra non-shadow regions, which could help the outer discriminator to \n\nFinally, with the help of and , we could naturally achieve the complementary requirements of shadow detection. Furthermore, the generator has an incentive to produce a more accurate shadow mask under the designed discriminative constraints.\n\n\nExperiments\n\nImplementation Details. We implement our proposed shadow detection framework via the PyTorch platform on the PC with the RTX 1080Ti GPU. For training, the training images are resized to the fixed resolution of 416 \u00d7 416 and applied random flipping as the augmentation strategy. The proposed framework is optimized for 30 epochs by the Adamax [21] optimizer with a fixed learning rate of 1 \u2212 3. The minimum training batch size is 4. The whole framework takes about 2 hours on the SBU and 1 hour on the ISTD dataset. For the hyperparameters, the weight factors ( 2 , . , and ) in Equation (8) are empirically set as 1, 1 \u2212 4 and 1 \u2212 2. Following [1,26,27,27,52], we also apply the fully connected CRF operation [22] to refine the estimated results in the inference phase.\n\nDataset. We conduct shadow detection experiments on the three representative benchmark datasets, i.e., SBU [36], UCF [50], and ISTD [38] dataset. SBU dataset includes 4089 and 638 pairs of images for training and testing. ISTD dataset includes 1870 image triples (shadow images, shadow-free images, and shadow masks). This dataset has been separated into 1330 and 540 triplets for the training and testing. Following the same experiment setting with previous methods [1,26,27,27,52], we train our framework on the SBU training dataset, and test on the SBU and UCF testing dataset. The testing for the ISTD dataset is utilizing the model trained on the corresponding training dataset.\n\nEvaluation Metric. Following previous methods [1,27,52], we adopt the widely-used metric, balance error rate (BER) [35], to evaluate the quantitative performance. BER is defined as\n= (1 \u2212 1 2 ( + ) + + )) \u00d7 100,(11)\nwhere , , , and stand for the number of True Positives, True Negatives, False Positives, and False Negatives, respectively. Lower BER values denote better results.\n\n\nDetection Evaluation on Benchmarks\n\nIn Table 1, we report the comparison results with recent stateof-the-art (SOTA) methods on the three benchmarks, including one traditional method: Unary-Pariwise [9] and 13 CNNs-based methods: FDRNet [52], RCMPNet [27], ECA [2], MTMT [1], DSD [47], DC-DSPF [43], ADNet [26], DSC [15], BDRAR [51], ST-CGAN [38], patched-CNN [11], scGAN [28], and stacked-CNN [36]. For fair comparisons with these SOTA methods, we utilize the provided pre-trained model or results from authors to obtain the quantitative  results. For example, we employ the publicly available detection results provided by authors of ECA [2], not the metric values of their paper. Besides, we also compare our results with four SOTA saliency object detection methods referring to FDRNet [52]. They were retrained and tested on the shadow datasets, and the corresponding evaluation results of these methods can be found in [52]. Obviously, our method performs the best BER scores over SOTA methods on the three benchmark datasets. Compared to RCMPNet, which is the second best-performing method, our method successfully reduces the BER score by 1.3% on the SBU testing dataset. Note that RCMPNet is an ensemble model to predict shadow masks based on the previous three SOTA methods, which largely demonstrates the effectiveness of our proposed shadow detection framework.\n\nIn addition, we also provide the visual comparison results with different SOTA methods in Figure 4. Obviously, our method performs better than the existing shadow detection results. The shadows in the image in the fifth and last row are very similar to the appearance of the black object, only our method successfully detects the shadows without misjudging. FDRNet still misses partial shadow regions (e.g., first three rows), and there also exist cases where the non-shadow regions are mistaken. In contrast, our method could successfully detect more accurate shadow masks with more fine-grained boundaries. This also indicates the effectiveness of complementary mechanisms adopted in our method.\n\n\nAblation Study\n\nAnalysis of the effectiveness of the complementarity between the shadow and non-shadow regions. We compare five models with the different configurations: (1) Model-1: only utilizing a single decoder to predict shadow masks; (2) Model-2: expanding the number of channels in the decoder of Model-1 to twice the original setting; (3) Model-3: utilizing two non-interactive decoders to predict both shadow and non-shadow masks; (4) Model-5: utilizing two interactive decoders with the negative activation technique to predict both shadow and non-shadow masks; (5) Model-6: L . is added based on the Model-3. In the absence of branch interactions, the performance of Model-1 and Model-3 is comparable. Moreover, with the complementary interaction between two decoders, the detection performance obviously successfully improves 5.1% on the SBU testing dataset, as reported in Table 2. The detection performance obviously successfully improves by 0.11 on the SBU testing dataset, which indicates that only increasing the network parameters is not as effective as interactive strategies. Compared with Model-3 and Model-4, L . also brings a certain improvement in shadow detection performance. In Figure 5, we also provide the visual results of different models to verify the effectiveness of the proposed complementarity.\n\nAnalysis of the Confidence of Predictions. We employ entropy to measure the confidence of predicted results. A lower entropy value denotes higher confidence. However, utilizing the entropy is to measure the confidence of the prediction and not to measure the accuracy of the prediction. We only compared the entropy performance with FDRNet [52], which has the closest comparable performance to our method. Note that codes of RCMPNet [27] are not publicly available. As shown in Figure 6, our method could confidently predict more accurate results. Moreover, we conduct the statistics of predictions on the SBU testing dataset among three models: FDRNet [52], our framework with only a decoder for detecting shadow regions, and our default framework. Obviously, Analysis of the Interaction Strategy. In our default framework, we introduce the negative activation technique (NAT) as the interaction strategy between two decoders. Here, we further investigate another straightforward manner to achieve the interaction between features of two decoders, which is to concatenate features along the channel dimension. From the results reported in Table 2, we find that adopting the NAT performs better than the straightforward concatenation manner. In our framework, NAT retains the deactivated features of one branch and delivers them to another complementary branch, and vice versa. This interaction strategy allows one branch to utilize the deactivated features of the other branch, thereby achieving complementarity at the feature level. On the contrary, these deactivated features are abandoned in the direct concatenation strategy, leading to suboptimal performance.\n\nAnalysis of the effectiveness of the complementary requirements of shadow detection. The complementary requirements of shadow detection are implemented by two complementary discriminators. Therefore, we conduct experiments to verify the effects of these two discriminators and . In Table 2, we provide the shadow detection performance when we remove the and and corresponding discriminative losses. With the help of the complementary discriminators, the BER value is almost reduced by 0.16 on the SBU dataset.\n\nFurthermore, we take the inner discriminator to investigate its corresponding discriminative capability.\n\nis designed to distinguish whether the detection shadow results miss any shadow regions to meet the requirement of shadow detection. As shown in Figure 7, we feed three types of shadow masks: M , M D , and M E . We find could distinguish the result misses partial shadow regions based on the visualizations of CAM [48]. Regardless of whether the input masks miss the shadow area or not, the feature responses of appear to be quite different.  : Feature map visualization of the discriminator with different masks as inputs using class activation mapping (CAM) [48]. The different inputs are: the GT shadow mask M , the dilated GT shadow mask M D , and the eroded GT shadow mask M E . Note that M D never participated in the training of , but the visualization result is consistent with the result of M , which means that could better distinguish whether missing shadow regions.\n\nAnalysis of the Number of Features Interactions, As reported in Table 3, we conduct experiments to verify the effects of the number of interactions between the decoders. Because we conduct the interactions of the two decoders after the basic block of each branch, the number of interactions is also the same as the number of Resblocks [10] adopted in our framework. It reaches the lowest BER value when the number of interactions is 4. Therefore, we empirically set the number of interactions to 4 as the default setting.\n\nAnalysis of the Parameters and Inference Time. To verify the effectiveness and efficiency of our method, we also compare the  parameters and average inference times. For the inference time, we repeatedly ran different SOTA models on images with a resolution of 416 \u00d7416 100 times to obtain the average inference time. Since the ensemble strategy of RCMPNet needs to be implemented based on the results of the previous shadow detection methods (MTMT, DSDNet, and BARAR), it often requires a large computational cost. However, the source codes of RCMPNet are not publicly available, we provide comparisons of network parameters and the average inference time with these previous methods in Table 4. Obviously, our method is almost 8 times faster in terms of inference time compared with RCMPNet.\n\n\nCONCLUSION\n\nIn our paper, we develop a novel framework for shadow detection, which investigates and exploits the complementary mechanisms contained in this specific task. Therefore, our framework consists of two key components that correspond to the two investigated complementarities, including the mutual complementarity between the shadow regions and their non-shadow counterparts, and the complementary requirements for shadow detection. Furthermore, we conduct comprehensive experiments and visualizations to demonstrate the effectiveness of two explored complementary mechanisms. Moreover, our method achieves superior performance against the existing state-of-the-art shadow detection methods with faster inference speed and smaller network parameters.\n\nFigure 1 :\n1Illustration of generator of our proposed shadow detection framework, which consists of one parameters-shared encoder for extracting backbone features, two interactive decoders as detection heads for predictingM andM . Moreover, we provide visualizations of interactive features in the above pipeline.\n\nL\n= E (I ,M )\u223c (I ,M ) [log( (I ,M ))]+ E (I ,M )\u223c (I ,M ) [log( (I ,M ))],\n\nFigure 3 :\n3Illustration of our proposed Dual Discriminators, which are designed to achieve the complementary requirements of shadow detection.\n\nL\n= E (I ,M )\u223c (I ,M ) [log( (I , M ))]+ E (I ,M E )\u223c (I ,M E ) [log(1 \u2212 (I , M E ))]+ E (I ,M )\u223c (I ,M ) [log(1 \u2212 (I ,M ))].\n\n\ndistinguish whether the shadow results falsely contain any nonshadow regions. The adversarial constraint of can be defined as L = E (I ,M )\u223c (I ,M ) [log( (I , M ))]+ E (I ,M D )\u223c (I ,M D ) [log(1 \u2212 (I , M D ))]+ E (I ,M )\u223c (I ,M ) [log(1 \u2212 (I ,M ))].\n\nFigure 4 :\n4The visual comparison results of different methods on the real shadow scenarios. (a) to (f) are the detection results from state-of-the-art methods: BDRAR[51], DSC[13], DSD[47], MTMT[1], ECA[2], and FDRNet[52], respectively.\n\nFigure 5 :\n5Visual comparison results of models with different configurations. (a) the result of Model-1; (b) the result of Model-5.\n\nFigure 6 :\n6Shadow detection results with the corresponding entropy maps, which demonstrate our method could confidently predict more accurate shadow locations. (a) and (b) are the estimated results from FDRNet[52] and ours; (c) and (d) are the corresponding entropy map of (a) and (b); (e) is the mean entropy map statistics on SBU dataset[36] (from left to right are: mean entropy values of FDRNet, our framework with only single decoder for shadow detection, and our default framework).\n\nFigure 7\n7Figure 7: Feature map visualization of the discriminator with different masks as inputs using class activation mapping (CAM) [48]. The different inputs are: the GT shadow mask M , the dilated GT shadow mask M D , and the eroded GT shadow mask M E . Note that M D never participated in the training of , but the visualization result is consistent with the result of M , which means that could better distinguish whether missing shadow regions.\n\nTable 1 :\n1Quantitative comparison of our method with the SOTA methods on the three public benchmark datasets for shadow detection. For each testing dataset, we provide the BER metric values. The best results are in bold, and \u2193 indicates lower is better.SBU \nUCF ISTD \nMethod \nYear BER\u2193 BER\u2193 BER\u2193 \nOurs \n-\n2.94 \n6.73 \n1.44 \nOurs-w/o-CRF \n-\n3.02 \n6.69 \n1.41 \n\nFDRNet [52] \n2021 3.04 \n7.28 \n1.55 \nRCMPNet [27] \n2021 2.98 \n6.75 \n1.61 \nECA [2] \n2021 5.93 10.71 2.03 \nMTMT [1] \n2020 3.15 \n7.47 \n1.72 \nDSD [47] \n2019 3.45 \n7.59 \n2.17 \nDC-DSPF [41] \n2018 4.90 \n7.90 \n-\nADNet [26] \n2018 5.37 \n9.25 \n-\nDSC [15] \n2018 5.59 10.54 3.42 \nBDRAR [51] \n2018 3.64 \n7.81 \n2.69 \nST-CGAN [38] \n2018 8.14 11.23 3.85 \npatched-CNN [11] 2018 11.56 \n-\n-\nscGAN [28] \n2017 9.10 11.50 4.70 \nstacked-CNN [36] 2016 11.00 13.00 8.60 \nUnary-Pariwise [9] 2011 25.03 \n-\n-\n\nITSD [49] \n2020 5.00 10.16 2.73 \nEGNet [45] \n2019 4.49 \n9.20 \n1.85 \nSRM [39] \n2017 6.57 12.51 7.92 \nAmulet [44] \n2017 6.57 12.51 7.92 \n\n\n\nTable 2 :\n2Ablation study of the components used in our framework on the SBU and ISTD testing dataset. SD: Single Decoder; DD: Dual Decoder; DC: Direct Concatenation interaction strategy; NAT: Negative Activation Technique interaction strategy. Meanwhile, the statistical results also demonstrate that the confidence of the prediction results could be improved under the interaction of two decoders.Models \nSetting \nMetric (BER \u2193) \n\nSBU \nISTD \nModel-1 SD \n3.33 \n1.72 \nModel-2 SD w double channels \n3.25 \n1.64 \nModel-3 DD w/o interaction \n3.31 \n1.69 \nModel-4 DD + DC \n3.19 \n1.65 \nModel-5 DD + NAT \n3.14 \n1.61 \nModel-6 DD + NAT+ L \n\n. \n\n3.10 \n1.56 \nModel-7 DD + NAT+ L \n. + L \n2.99 \n1.47 \nModel-8 DD + NAT+ L \n. + L \n3.01 \n1.48 \nOurs \nDD + NAT+ L \n. + L \n+ L \n2.94 \n1.44 \n\nthe mean entropy values of our method are less than half that of \nFDRNet. \n\nTable 3 :\n3Ablation study of numbers of features interaction on the SBU dataset.Interaction Numbers \n1 \n2 \n3 \n4 \n5 \n6 \nBER \u2193 \n3.06 3.03 2.98 2.94 2.96 2.99 \n\n\n\nTable 4 :\n4The network parameters and average inference time of different shadow detection methods. The average inference times are obtained with the resolution of 416 \u00d7 416 on the 1080Ti GPU device.Methods \nParameters(M: 10 6 ) Average Inference Time (ms) \nBADRA [51] \n42.46 \n96.73 \nDSD [47] \n58.16 \n71.32 \nMTMT [1] \n44.13 \n57.08 \nRCMPNet [27] \n-\n>225.13 (96.73 + 71.32 + 57.08) \nECA [2] \n157.76 \n92.82 \nFDRNet [52] \n10.77 \n29.74 \nOurs \n10.95 \n28.31 \n\n\nACKNOWLEDGMENTS\nA multi-task mean teacher for semi-supervised shadow detection. Zhihao Chen, Lei Zhu, Liang Wan, Song Wang, Wei Feng, Pheng-Ann Heng, Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. the IEEE/CVF Conference on computer vision and pattern recognitionZhihao Chen, Lei Zhu, Liang Wan, Song Wang, Wei Feng, and Pheng-Ann Heng. 2020. A multi-task mean teacher for semi-supervised shadow detection. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. 5611-5620.\n\nRobust Shadow Detection by Exploring Effective Shadow Contexts. Xianyong Fang, Xiaohao He, Linbo Wang, Jianbing Shen, Proceedings of the 29th ACM International Conference on Multimedia -ACM Multimedia. the 29th ACM International Conference on Multimedia -ACM MultimediaXianyong Fang, Xiaohao He, Linbo Wang, and Jianbing Shen. 2021. Robust Shadow Detection by Exploring Effective Shadow Contexts. In Proceedings of the 29th ACM International Conference on Multimedia -ACM Multimedia 2021.\n\nVisual Boundary Knowledge Translation for Foreground Segmentation. Zunlei Feng, Lechao Cheng, Xinchao Wang, Xiang Wang, Ya Jie Liu, Xiangtong Du, Mingli Song, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Zunlei Feng, Lechao Cheng, Xinchao Wang, Xiang Wang, Ya Jie Liu, Xiang- tong Du, and Mingli Song. 2021. Visual Boundary Knowledge Translation for Foreground Segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 1334-1342.\n\nEntropy minimization for shadow removal. Graham D Finlayson, S Mark, Cheng Drew, Lu, International Journal of Computer Vision. 85Graham D Finlayson, Mark S Drew, and Cheng Lu. 2009. Entropy minimization for shadow removal. International Journal of Computer Vision 85, 1 (2009), 35-57.\n\nOn the removal of shadows from images. Graham D Finlayson, D Steven, Cheng Hordley, Mark S Lu, Drew, 28Graham D Finlayson, Steven D Hordley, Cheng Lu, and Mark S Drew. 2005. On the removal of shadows from images. IEEE transactions on pattern analysis and machine intelligence 28, 1 (2005), 59-68.\n\nDeep sparse rectifier neural networks. Xavier Glorot, Antoine Bordes, Yoshua Bengio, Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings. the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference ProceedingsXavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectifier neural networks. In Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 315-323.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. Advances in neural information processing systems (2014).\n\nWavelet multi-scale transform based foreground segmentation and shadow elimination. Ye-Peng Guan, The Open Signal Processing Journal. 11Ye-Peng Guan. 2008. Wavelet multi-scale transform based foreground segmenta- tion and shadow elimination. The Open Signal Processing Journal 1, 1 (2008).\n\nSingle-image shadow detection and removal using paired regions. Ruiqi Guo, Qieyun Dai, Derek Hoiem, CVPR 2011. IEEE. Ruiqi Guo, Qieyun Dai, and Derek Hoiem. 2011. Single-image shadow detection and removal using paired regions. In CVPR 2011. IEEE, 2033-2040.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition. the IEEE/CVF Conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition.\n\nFast shadow detection from a single image using a patched convolutional neural network. Sepideh Hosseinzadeh, Moein Shakeri, Hong Zhang, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEESepideh Hosseinzadeh, Moein Shakeri, and Hong Zhang. 2018. Fast shadow detection from a single image using a patched convolutional neural network. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 3124-3129.\n\nTrash or Treasure? An Interactive Dual-Stream Strategy for Single Image Reflection Separation. Qiming Hu, Xiaojie Guo, Advances in Neural Information Processing Systems. 34Qiming Hu and Xiaojie Guo. 2021. Trash or Treasure? An Interactive Dual-Stream Strategy for Single Image Reflection Separation. Advances in Neural Information Processing Systems 34 (2021).\n\nDirection-aware spatial context features for shadow detection and removal. Xiaowei Hu, Chi-Wing Fu, Lei Zhu, IEEE transactions. 42Jing Qin, and Pheng-Ann HengXiaowei Hu, Chi-Wing Fu, Lei Zhu, Jing Qin, and Pheng-Ann Heng. 2019. Direction-aware spatial context features for shadow detection and removal. IEEE transactions on pattern analysis and machine intelligence 42, 11 (2019), 2795-2808.\n\nRevisiting shadow detection: A new benchmark dataset for complex world. Xiaowei Hu, Tianyu Wang, Chi-Wing Fu, Yitong Jiang, Qiong Wang, Pheng-Ann Heng, IEEE Transactions on Image Processing. 30Xiaowei Hu, Tianyu Wang, Chi-Wing Fu, Yitong Jiang, Qiong Wang, and Pheng- Ann Heng. 2021. Revisiting shadow detection: A new benchmark dataset for complex world. IEEE Transactions on Image Processing 30 (2021), 1925-1934.\n\nDirection-aware spatial context features for shadow detection. Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, Pheng-Ann Heng, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and Pheng-Ann Heng. 2018. Direction-aware spatial context features for shadow detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7454-7462.\n\nWhat characterizes a shadow boundary under the sun and sky. Xiang Huang, Gang Hua, Jack Tumblin, Lance Williams, 2011 international conference on computer vision. IEEEXiang Huang, Gang Hua, Jack Tumblin, and Lance Williams. 2011. What charac- terizes a shadow boundary under the sun and sky?. In 2011 international conference on computer vision. IEEE, 898-905.\n\nScale-space properties of the multiscale morphological dilation-erosion. T Paul, Mohamed Jackway, Deriche, IEEE transactions. 18Paul T. Jackway and Mohamed Deriche. 1996. Scale-space properties of the multiscale morphological dilation-erosion. IEEE transactions on pattern analysis and machine intelligence 18, 1 (1996), 38-51.\n\nEstimating geo-temporal location of stationary cameras using shadow trajectories. N Imran, Hassan Junejo, Foroosh, European conference on computer vision. SpringerImran N Junejo and Hassan Foroosh. 2008. Estimating geo-temporal location of stationary cameras using shadow trajectories. In European conference on computer vision. Springer, 318-331.\n\nRendering synthetic objects into legacy photographs. Kevin Karsch, Varsha Hedau, David Forsyth, Derek Hoiem, ACM Transactions on Graphics (TOG). 30Kevin Karsch, Varsha Hedau, David Forsyth, and Derek Hoiem. 2011. Rendering synthetic objects into legacy photographs. ACM Transactions on Graphics (TOG) 30, 6 (2011), 1-12.\n\nAutomatic feature learning for robust shadow detection. Salman Hameed Khan, Mohammed Bennamoun, Ferdous Sohel, Roberto Togneri, 2014 IEEE Conference on Computer Vision and Pattern Recognition. IEEESalman Hameed Khan, Mohammed Bennamoun, Ferdous Sohel, and Roberto Togneri. 2014. Automatic feature learning for robust shadow detection. In 2014 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 1939-1946.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).\n\nEfficient inference in fully connected crfs with gaussian edge potentials. Philipp Kr\u00e4henb\u00fchl, Vladlen Koltun, Advances in neural information processing systems. 24Philipp Kr\u00e4henb\u00fchl and Vladlen Koltun. 2011. Efficient inference in fully con- nected crfs with gaussian edge potentials. Advances in neural information pro- cessing systems 24 (2011).\n\nEstimating the natural illumination conditions from a single outdoor image. Jean-Fran\u00e7ois Lalonde, Alexei A Efros, G Srinivasa, Narasimhan, International Journal of Computer Vision. 98Jean-Fran\u00e7ois Lalonde, Alexei A Efros, and Srinivasa G Narasimhan. 2012. Estimat- ing the natural illumination conditions from a single outdoor image. International Journal of Computer Vision 98, 2 (2012), 123-145.\n\nFrom Shadow Segmentation to Shadow Removal. Hieu Le, Dimitris Samaras, The IEEE European Conference on Computer Vision (ECCV). Hieu Le and Dimitris Samaras. 2020. From Shadow Segmentation to Shadow Removal. In The IEEE European Conference on Computer Vision (ECCV).\n\nPhysics-based shadow image decomposition for shadow removal. Hieu Le, Dimitris Samaras, IEEE Transactions on Pattern Analysis & Machine Intelligence. 01Hieu Le and Dimitris Samaras. 2021. Physics-based shadow image decomposition for shadow removal. IEEE Transactions on Pattern Analysis & Machine Intelligence 01 (2021), 1-1.\n\nA+ d net: Training a shadow detector with adversarial shadow attenuation. Hieu Le, Tomas F Yago Vicente, Vu Nguyen, Minh Hoai, Dimitris Samaras, Hieu Le, Tomas F Yago Vicente, Vu Nguyen, Minh Hoai, and Dimitris Samaras. 2018. A+ d net: Training a shadow detector with adversarial shadow attenuation.\n\nProceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)In Proceedings of the European Conference on Computer Vision (ECCV). 662-678.\n\nShadow Detection via Predicting the Confidence Maps of Shadow Detection Methods. Jingwei Liao, Yanli Liu, Guanyu Xing, Housheng Wei, Jueyu Chen, Songhua Xu, Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on MultimediaJingwei Liao, Yanli Liu, Guanyu Xing, Housheng Wei, Jueyu Chen, and Songhua Xu. 2021. Shadow Detection via Predicting the Confidence Maps of Shadow Detection Methods. In Proceedings of the 29th ACM International Conference on Multimedia. 704-712.\n\nShadow detection with conditional generative adversarial networks. Tomas F Yago Vu Nguyen, Maozheng Vicente, Minh Zhao, Dimitris Hoai, Samaras, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionVu Nguyen, Tomas F Yago Vicente, Maozheng Zhao, Minh Hoai, and Dimitris Samaras. 2017. Shadow detection with conditional generative adversarial net- works. In Proceedings of the IEEE International Conference on Computer Vision. 4510-4518.\n\nAttached shadow coding: Estimating surface normals from shadows under unknown reflectance and lighting conditions. Takahiro Okabe, Imari Sato, Yoichi Sato, IEEE 12th International Conference on Computer Vision. IEEETakahiro Okabe, Imari Sato, and Yoichi Sato. 2009. Attached shadow coding: Es- timating surface normals from shadows under unknown reflectance and lighting conditions. In 2009 IEEE 12th International Conference on Computer Vision. IEEE, 1693-1700.\n\nRobust shadow and illumination estimation using a mixture model. Alexandros Panagopoulos, Dimitris Samaras, Nikos Paragios, 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEEAlexandros Panagopoulos, Dimitris Samaras, and Nikos Paragios. 2009. Ro- bust shadow and illumination estimation using a mixture model. In 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 651-658.\n\nSimultaneous cast shadows, illumination and geometry inference using hypergraphs. Alexandros Panagopoulos, Chaohui Wang, 35Dimitris Samaras, and Nikos ParagiosAlexandros Panagopoulos, Chaohui Wang, Dimitris Samaras, and Nikos Paragios. 2012. Simultaneous cast shadows, illumination and geometry inference using hypergraphs. IEEE transactions on pattern analysis and machine intelligence 35, 2 (2012), 437-449.\n\nShadow optimization from structured deep edge detection. Li Shen, Karianto Teck Wee Chua, Leman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLi Shen, Teck Wee Chua, and Karianto Leman. 2015. Shadow optimization from structured deep edge detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2067-2074.\n\nEfficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, PMLRInternational conference on machine learning. Mingxing Tan and Quoc Le. 2019. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning. PMLR, 6105-6114.\n\nNew spectrum ratio properties and features for shadow detection. Jiandong Tian, Xiaojun Qi, Liangqiong Qu, Yandong Tang, Pattern Recognition. 51Jiandong Tian, Xiaojun Qi, Liangqiong Qu, and Yandong Tang. 2016. New spectrum ratio properties and features for shadow detection. Pattern Recognition 51 (2016), 85-96.\n\nLeave-One-Out Kernel Optimization for Shadow Detection and Removal. F Yago Tom\u00e1s, Minh Vicente, Dimitris Hoai, Samaras, 10.1109/TPAMI.2017.2691703IEEE Transactions on Pattern Analysis and Machine Intelligence. 40Tom\u00e1s F. Yago Vicente, Minh Hoai, and Dimitris Samaras. 2018. Leave-One-Out Kernel Optimization for Shadow Detection and Removal. IEEE Transactions on Pattern Analysis and Machine Intelligence 40, 3 (2018), 682-695. https://doi.org/ 10.1109/TPAMI.2017.2691703\n\nLarge-scale training of shadow detectors with noisily-annotated shadow examples. Tom\u00e1s F Yago, Le Vicente, Chen-Ping Hou, Minh Yu, Dimitris Hoai, Samaras, European Conference on Computer Vision. SpringerTom\u00e1s F Yago Vicente, Le Hou, Chen-Ping Yu, Minh Hoai, and Dimitris Samaras. 2016. Large-scale training of shadow detectors with noisily-annotated shadow examples. In European Conference on Computer Vision. Springer, 816-832.\n\nSingle Image Shadow Detection Using Multiple Cues in a Supermodular MRF. Tom\u00e1s F Yago, Chen-Ping Vicente, Dimitris Yu, Samaras, BMVC. Tom\u00e1s F Yago Vicente, Chen-Ping Yu, and Dimitris Samaras. 2013. Single Image Shadow Detection Using Multiple Cues in a Supermodular MRF.. In BMVC.\n\nStacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. Jifeng Wang, Xiang Li, Jian Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJifeng Wang, Xiang Li, and Jian Yang. 2018. Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1788-1797.\n\nA stagewise refinement model for detecting salient objects in images. Tiantian Wang, Ali Borji, Lihe Zhang, Pingping Zhang, Huchuan Lu, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionTiantian Wang, Ali Borji, Lihe Zhang, Pingping Zhang, and Huchuan Lu. 2017. A stagewise refinement model for detecting salient objects in images. In Proceedings of the IEEE international conference on computer vision. 4019-4028.\n\nSingle-Stage Instance Shadow Detection with Bidirectional Relation Learning. Tianyu Wang, Xiaowei Hu, Chi-Wing Fu, Pheng-Ann Heng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionTianyu Wang, Xiaowei Hu, Chi-Wing Fu, and Pheng-Ann Heng. 2021. Single- Stage Instance Shadow Detection with Bidirectional Relation Learning. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1-11.\n\nDensely Cascaded Shadow Detection Network via Deeply Supervised Parallel Fusion. Yupei Wang, Xin Zhao, Yin Li, Xuecai Hu, Kaiqi Huang, Nlpr Cripac, IJCAI. 5Yupei Wang, Xin Zhao, Yin Li, Xuecai Hu, Kaiqi Huang, and NLPR CRIPAC. 2018. Densely Cascaded Shadow Detection Network via Deeply Supervised Parallel Fusion.. In IJCAI, Vol. 5. 6.\n\nShadow detection and sun direction in photo collections. Scott Wehrwein, Kavita Bala, Noah Snavely, 2015 International Conference on 3D Vision. IEEE. Scott Wehrwein, Kavita Bala, and Noah Snavely. 2015. Shadow detection and sun direction in photo collections. In 2015 International Conference on 3D Vision. IEEE, 460-468.\n\nCamera calibration and geolocation estimation from two shadow trajectories. Lin Wu, Xiaochun Cao, Hassan Foroosh, Computer Vision and Image Understanding. 114Lin Wu, Xiaochun Cao, and Hassan Foroosh. 2010. Camera calibration and geo- location estimation from two shadow trajectories. Computer Vision and Image Understanding 114, 8 (2010), 915-927.\n\nAmulet: Aggregating multi-level convolutional features for salient object detection. Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, Xiang Ruan, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionPingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, and Xiang Ruan. 2017. Amulet: Aggregating multi-level convolutional features for salient object detection. In Proceedings of the IEEE international conference on computer vision. 202-211.\n\nEGNet: Edge guidance network for salient object detection. Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, Ming-Ming Cheng, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionJia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming- Ming Cheng. 2019. EGNet: Edge guidance network for salient object detection. In Proceedings of the IEEE/CVF international conference on computer vision. 8779- 8788.\n\nSelf-generated Defocus Blur Detection via Dual Adversarial Discriminators. Wenda Zhao, Cai Shang, Huchuan Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionWenda Zhao, Cai Shang, and Huchuan Lu. 2021. Self-generated Defocus Blur Detection via Dual Adversarial Discriminators. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6933-6942.\n\nDistraction-aware shadow detection. Quanlong Zheng, Xiaotian Qiao, Ying Cao, Rynson Wh Lau, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionQuanlong Zheng, Xiaotian Qiao, Ying Cao, and Rynson WH Lau. 2019. Distraction-aware shadow detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5167-5176.\n\nLearning deep features for discriminative localization. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2921-2929.\n\nInteractive two-stream decoder for accurate and fast saliency detection. Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, Lingxiao Yang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHuajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, and Lingxiao Yang. 2020. Interactive two-stream decoder for accurate and fast saliency detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9141-9150.\n\nLearning to recognize shadows in monochromatic natural images. Jiejie Zhu, G G Kegan, Samuel, Z Syed, Marshall F Masood, Tappen, 2010 IEEE Computer Society conference on computer vision and pattern recognition. IEEEJiejie Zhu, Kegan GG Samuel, Syed Z Masood, and Marshall F Tappen. 2010. Learning to recognize shadows in monochromatic natural images. In 2010 IEEE Computer Society conference on computer vision and pattern recognition. IEEE, 223-230.\n\nBidirectional feature pyramid network with recurrent attention residual modules for shadow detection. Lei Zhu, Zijun Deng, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu, Jing Qin, Pheng-Ann Heng, Proceedings of the European Conference on Computer Vision (ECCV. the European Conference on Computer Vision (ECCVLei Zhu, Zijun Deng, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu, Jing Qin, and Pheng-Ann Heng. 2018. Bidirectional feature pyramid network with recurrent attention residual modules for shadow detection. In Proceedings of the European Conference on Computer Vision (ECCV). 121-136.\n\nMitigating Intensity Bias in Shadow Detection via Feature Decomposition and Reweighting. Lei Zhu, Ke Xu, Zhanghan Ke, Rynson Wh Lau, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLei Zhu, Ke Xu, Zhanghan Ke, and Rynson WH Lau. 2021. Mitigating Intensity Bias in Shadow Detection via Feature Decomposition and Reweighting. In Pro- ceedings of the IEEE/CVF International Conference on Computer Vision. 4702-4711.\n", "annotations": {"author": "[{\"end\":133,\"start\":123},{\"end\":162,\"start\":134},{\"end\":196,\"start\":163},{\"end\":207,\"start\":197},{\"end\":237,\"start\":208},{\"end\":280,\"start\":238},{\"end\":289,\"start\":281},{\"end\":321,\"start\":290},{\"end\":332,\"start\":322},{\"end\":397,\"start\":333},{\"end\":475,\"start\":398},{\"end\":540,\"start\":476},{\"end\":615,\"start\":541},{\"end\":694,\"start\":616},{\"end\":759,\"start\":695}]", "publisher": "[{\"end\":62,\"start\":59},{\"end\":1014,\"start\":1011}]", "author_last_name": "[{\"end\":132,\"start\":129},{\"end\":144,\"start\":142},{\"end\":170,\"start\":166},{\"end\":206,\"start\":203},{\"end\":218,\"start\":216},{\"end\":250,\"start\":247},{\"end\":288,\"start\":284},{\"end\":299,\"start\":296}]", "author_first_name": "[{\"end\":128,\"start\":123},{\"end\":141,\"start\":134},{\"end\":165,\"start\":163},{\"end\":202,\"start\":197},{\"end\":215,\"start\":208},{\"end\":246,\"start\":238},{\"end\":283,\"start\":281},{\"end\":295,\"start\":290},{\"end\":331,\"start\":322}]", "author_affiliation": "[{\"end\":396,\"start\":334},{\"end\":474,\"start\":399},{\"end\":539,\"start\":477},{\"end\":614,\"start\":542},{\"end\":693,\"start\":617},{\"end\":758,\"start\":696}]", "title": "[{\"end\":58,\"start\":1},{\"end\":817,\"start\":760}]", "venue": "[{\"end\":894,\"start\":819}]", "abstract": "[{\"end\":2730,\"start\":1257}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3010,\"start\":3006},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3013,\"start\":3010},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3016,\"start\":3013},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3049,\"start\":3045},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3052,\"start\":3049},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3076,\"start\":3072},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3183,\"start\":3180},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3213,\"start\":3209},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3248,\"start\":3244},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3635,\"start\":3632},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3637,\"start\":3635},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3674,\"start\":3670},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3677,\"start\":3674},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3680,\"start\":3677},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4213,\"start\":4210},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4216,\"start\":4213},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4219,\"start\":4216},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4222,\"start\":4219},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4225,\"start\":4222},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4228,\"start\":4225},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":4446,\"start\":4442},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4732,\"start\":4728},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4735,\"start\":4732},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8965,\"start\":8961},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9118,\"start\":9114},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9809,\"start\":9805},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9938,\"start\":9934},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10301,\"start\":10297},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10471,\"start\":10467},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10588,\"start\":10584},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10738,\"start\":10735},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10968,\"start\":10964},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11179,\"start\":11175},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11315,\"start\":11311},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11541,\"start\":11538},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11544,\"start\":11541},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":11565,\"start\":11561},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11591,\"start\":11587},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12064,\"start\":12060},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12188,\"start\":12184},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":13448,\"start\":13444},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13464,\"start\":13460},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16620,\"start\":16616},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16673,\"start\":16669},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17337,\"start\":17333},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17391,\"start\":17388},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18100,\"start\":18096},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18838,\"start\":18834},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":18841,\"start\":18838},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20232,\"start\":20228},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20235,\"start\":20232},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20322,\"start\":20319},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20782,\"start\":20779},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20785,\"start\":20782},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22671,\"start\":22667},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22972,\"start\":22969},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22975,\"start\":22972},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22978,\"start\":22975},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22981,\"start\":22978},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22984,\"start\":22981},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23038,\"start\":23034},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23207,\"start\":23203},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23217,\"start\":23213},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":23232,\"start\":23228},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23566,\"start\":23563},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23569,\"start\":23566},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23572,\"start\":23569},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23575,\"start\":23572},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23578,\"start\":23575},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23830,\"start\":23827},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23833,\"start\":23830},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23836,\"start\":23833},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23900,\"start\":23896},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24364,\"start\":24361},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24403,\"start\":24399},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24417,\"start\":24413},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24426,\"start\":24423},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24436,\"start\":24433},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24446,\"start\":24442},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":24460,\"start\":24456},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24472,\"start\":24468},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24482,\"start\":24478},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24494,\"start\":24490},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24508,\"start\":24504},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24526,\"start\":24522},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24538,\"start\":24534},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24560,\"start\":24556},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24805,\"start\":24802},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24955,\"start\":24951},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25090,\"start\":25086},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":27912,\"start\":27908},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":28005,\"start\":28001},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":28225,\"start\":28221},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":30170,\"start\":30166},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":30416,\"start\":30412},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":33899,\"start\":33895},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33908,\"start\":33904},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":33917,\"start\":33913},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33926,\"start\":33923},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33934,\"start\":33931},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":33950,\"start\":33946},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":34315,\"start\":34311},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":34445,\"start\":34441}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33124,\"start\":32810},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33201,\"start\":33125},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33346,\"start\":33202},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33473,\"start\":33347},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33727,\"start\":33474},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33965,\"start\":33728},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34099,\"start\":33966},{\"attributes\":{\"id\":\"fig_7\"},\"end\":34590,\"start\":34100},{\"attributes\":{\"id\":\"fig_8\"},\"end\":35044,\"start\":34591},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36021,\"start\":35045},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36868,\"start\":36022},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37028,\"start\":36869},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37483,\"start\":37029}]", "paragraph": "[{\"end\":3336,\"start\":2746},{\"end\":3906,\"start\":3338},{\"end\":4950,\"start\":3908},{\"end\":5232,\"start\":4952},{\"end\":6482,\"start\":5234},{\"end\":7294,\"start\":6484},{\"end\":7355,\"start\":7296},{\"end\":8535,\"start\":7357},{\"end\":8764,\"start\":8552},{\"end\":9457,\"start\":8788},{\"end\":10061,\"start\":9480},{\"end\":10863,\"start\":10063},{\"end\":11394,\"start\":10865},{\"end\":12493,\"start\":11396},{\"end\":12701,\"start\":12504},{\"end\":13792,\"start\":12716},{\"end\":14375,\"start\":13794},{\"end\":14847,\"start\":14377},{\"end\":16080,\"start\":14849},{\"end\":16586,\"start\":16098},{\"end\":17121,\"start\":16588},{\"end\":18312,\"start\":17173},{\"end\":18687,\"start\":18348},{\"end\":18957,\"start\":18755},{\"end\":19371,\"start\":19027},{\"end\":19633,\"start\":19441},{\"end\":19797,\"start\":19669},{\"end\":19975,\"start\":19799},{\"end\":20055,\"start\":20009},{\"end\":21566,\"start\":20093},{\"end\":22067,\"start\":21568},{\"end\":22309,\"start\":22069},{\"end\":23094,\"start\":22325},{\"end\":23779,\"start\":23096},{\"end\":23961,\"start\":23781},{\"end\":24160,\"start\":23997},{\"end\":25534,\"start\":24199},{\"end\":26233,\"start\":25536},{\"end\":27566,\"start\":26252},{\"end\":29233,\"start\":27568},{\"end\":29744,\"start\":29235},{\"end\":29850,\"start\":29746},{\"end\":30729,\"start\":29852},{\"end\":31252,\"start\":30731},{\"end\":32047,\"start\":31254},{\"end\":32809,\"start\":32062}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17172,\"start\":17122},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18347,\"start\":18313},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18754,\"start\":18688},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19026,\"start\":18958},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19440,\"start\":19372},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19668,\"start\":19634},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20008,\"start\":19976},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23996,\"start\":23962}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24209,\"start\":24202},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27129,\"start\":27122},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28715,\"start\":28708},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29524,\"start\":29517},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30802,\"start\":30795},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31949,\"start\":31942}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2744,\"start\":2732},{\"attributes\":{\"n\":\"2\"},\"end\":8550,\"start\":8538},{\"attributes\":{\"n\":\"2.1\"},\"end\":8786,\"start\":8767},{\"attributes\":{\"n\":\"2.2\"},\"end\":9478,\"start\":9460},{\"attributes\":{\"n\":\"3\"},\"end\":12502,\"start\":12496},{\"attributes\":{\"n\":\"3.1\"},\"end\":12714,\"start\":12704},{\"attributes\":{\"n\":\"3.2\"},\"end\":16096,\"start\":16083},{\"attributes\":{\"n\":\"3.3\"},\"end\":20091,\"start\":20058},{\"attributes\":{\"n\":\"3.4\"},\"end\":22323,\"start\":22312},{\"attributes\":{\"n\":\"3.5\"},\"end\":24197,\"start\":24163},{\"attributes\":{\"n\":\"3.6\"},\"end\":26250,\"start\":26236},{\"attributes\":{\"n\":\"4\"},\"end\":32060,\"start\":32050},{\"end\":32821,\"start\":32811},{\"end\":33127,\"start\":33126},{\"end\":33213,\"start\":33203},{\"end\":33349,\"start\":33348},{\"end\":33739,\"start\":33729},{\"end\":33977,\"start\":33967},{\"end\":34111,\"start\":34101},{\"end\":34600,\"start\":34592},{\"end\":35055,\"start\":35046},{\"end\":36032,\"start\":36023},{\"end\":36879,\"start\":36870},{\"end\":37039,\"start\":37030}]", "table": "[{\"end\":36021,\"start\":35300},{\"end\":36868,\"start\":36422},{\"end\":37028,\"start\":36950},{\"end\":37483,\"start\":37229}]", "figure_caption": "[{\"end\":33124,\"start\":32823},{\"end\":33201,\"start\":33128},{\"end\":33346,\"start\":33215},{\"end\":33473,\"start\":33350},{\"end\":33727,\"start\":33476},{\"end\":33965,\"start\":33741},{\"end\":34099,\"start\":33979},{\"end\":34590,\"start\":34113},{\"end\":35044,\"start\":34602},{\"end\":35300,\"start\":35057},{\"end\":36422,\"start\":36034},{\"end\":36950,\"start\":36881},{\"end\":37229,\"start\":37041}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12566,\"start\":12558},{\"end\":13420,\"start\":13412},{\"end\":13989,\"start\":13981},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18368,\"start\":18360},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25634,\"start\":25626},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27449,\"start\":27441},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28054,\"start\":28046},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":30005,\"start\":29997}]", "bib_author_first_name": "[{\"end\":37570,\"start\":37564},{\"end\":37580,\"start\":37577},{\"end\":37591,\"start\":37586},{\"end\":37601,\"start\":37597},{\"end\":37611,\"start\":37608},{\"end\":37627,\"start\":37618},{\"end\":38097,\"start\":38089},{\"end\":38111,\"start\":38104},{\"end\":38121,\"start\":38116},{\"end\":38136,\"start\":38128},{\"end\":38588,\"start\":38582},{\"end\":38601,\"start\":38595},{\"end\":38616,\"start\":38609},{\"end\":38628,\"start\":38623},{\"end\":38641,\"start\":38635},{\"end\":38656,\"start\":38647},{\"end\":38667,\"start\":38661},{\"end\":39105,\"start\":39104},{\"end\":39117,\"start\":39112},{\"end\":39389,\"start\":39388},{\"end\":39403,\"start\":39398},{\"end\":39419,\"start\":39413},{\"end\":39672,\"start\":39666},{\"end\":39688,\"start\":39681},{\"end\":39703,\"start\":39697},{\"end\":40255,\"start\":40252},{\"end\":40272,\"start\":40268},{\"end\":40293,\"start\":40288},{\"end\":40305,\"start\":40301},{\"end\":40315,\"start\":40310},{\"end\":40337,\"start\":40330},{\"end\":40350,\"start\":40345},{\"end\":40368,\"start\":40362},{\"end\":40742,\"start\":40735},{\"end\":41011,\"start\":41006},{\"end\":41023,\"start\":41017},{\"end\":41034,\"start\":41029},{\"end\":41254,\"start\":41247},{\"end\":41266,\"start\":41259},{\"end\":41282,\"start\":41274},{\"end\":41292,\"start\":41288},{\"end\":41736,\"start\":41729},{\"end\":41756,\"start\":41751},{\"end\":41770,\"start\":41766},{\"end\":42208,\"start\":42202},{\"end\":42220,\"start\":42213},{\"end\":42551,\"start\":42544},{\"end\":42564,\"start\":42556},{\"end\":42572,\"start\":42569},{\"end\":42941,\"start\":42934},{\"end\":42952,\"start\":42946},{\"end\":42967,\"start\":42959},{\"end\":42978,\"start\":42972},{\"end\":42991,\"start\":42986},{\"end\":43007,\"start\":42998},{\"end\":43349,\"start\":43342},{\"end\":43357,\"start\":43354},{\"end\":43371,\"start\":43363},{\"end\":43380,\"start\":43376},{\"end\":43395,\"start\":43386},{\"end\":43835,\"start\":43830},{\"end\":43847,\"start\":43843},{\"end\":43857,\"start\":43853},{\"end\":43872,\"start\":43867},{\"end\":44206,\"start\":44205},{\"end\":44220,\"start\":44213},{\"end\":44544,\"start\":44543},{\"end\":44558,\"start\":44552},{\"end\":44868,\"start\":44863},{\"end\":44883,\"start\":44877},{\"end\":44896,\"start\":44891},{\"end\":44911,\"start\":44906},{\"end\":45194,\"start\":45188},{\"end\":45216,\"start\":45208},{\"end\":45235,\"start\":45228},{\"end\":45250,\"start\":45243},{\"end\":45598,\"start\":45597},{\"end\":45614,\"start\":45609},{\"end\":45862,\"start\":45855},{\"end\":45882,\"start\":45875},{\"end\":46219,\"start\":46206},{\"end\":46235,\"start\":46229},{\"end\":46237,\"start\":46236},{\"end\":46246,\"start\":46245},{\"end\":46578,\"start\":46574},{\"end\":46591,\"start\":46583},{\"end\":46862,\"start\":46858},{\"end\":46875,\"start\":46867},{\"end\":47202,\"start\":47198},{\"end\":47219,\"start\":47207},{\"end\":47231,\"start\":47229},{\"end\":47244,\"start\":47240},{\"end\":47259,\"start\":47251},{\"end\":47707,\"start\":47700},{\"end\":47719,\"start\":47714},{\"end\":47731,\"start\":47725},{\"end\":47746,\"start\":47738},{\"end\":47757,\"start\":47752},{\"end\":47771,\"start\":47764},{\"end\":48222,\"start\":48210},{\"end\":48242,\"start\":48234},{\"end\":48256,\"start\":48252},{\"end\":48271,\"start\":48263},{\"end\":48771,\"start\":48763},{\"end\":48784,\"start\":48779},{\"end\":48797,\"start\":48791},{\"end\":49187,\"start\":49177},{\"end\":49210,\"start\":49202},{\"end\":49225,\"start\":49220},{\"end\":49617,\"start\":49607},{\"end\":49639,\"start\":49632},{\"end\":49995,\"start\":49993},{\"end\":50010,\"start\":50002},{\"end\":50457,\"start\":50449},{\"end\":50467,\"start\":50463},{\"end\":50768,\"start\":50760},{\"end\":50782,\"start\":50775},{\"end\":50797,\"start\":50787},{\"end\":50809,\"start\":50802},{\"end\":51078,\"start\":51077},{\"end\":51083,\"start\":51079},{\"end\":51095,\"start\":51091},{\"end\":51113,\"start\":51105},{\"end\":51579,\"start\":51577},{\"end\":51598,\"start\":51589},{\"end\":51608,\"start\":51604},{\"end\":51621,\"start\":51613},{\"end\":52008,\"start\":51999},{\"end\":52026,\"start\":52018},{\"end\":52310,\"start\":52304},{\"end\":52322,\"start\":52317},{\"end\":52331,\"start\":52327},{\"end\":52805,\"start\":52797},{\"end\":52815,\"start\":52812},{\"end\":52827,\"start\":52823},{\"end\":52843,\"start\":52835},{\"end\":52858,\"start\":52851},{\"end\":53297,\"start\":53291},{\"end\":53311,\"start\":53304},{\"end\":53324,\"start\":53316},{\"end\":53338,\"start\":53329},{\"end\":53817,\"start\":53812},{\"end\":53827,\"start\":53824},{\"end\":53837,\"start\":53834},{\"end\":53848,\"start\":53842},{\"end\":53858,\"start\":53853},{\"end\":53870,\"start\":53866},{\"end\":54130,\"start\":54125},{\"end\":54147,\"start\":54141},{\"end\":54158,\"start\":54154},{\"end\":54470,\"start\":54467},{\"end\":54483,\"start\":54475},{\"end\":54495,\"start\":54489},{\"end\":54833,\"start\":54825},{\"end\":54845,\"start\":54841},{\"end\":54859,\"start\":54852},{\"end\":54870,\"start\":54864},{\"end\":54882,\"start\":54877},{\"end\":55318,\"start\":55310},{\"end\":55336,\"start\":55325},{\"end\":55351,\"start\":55342},{\"end\":55361,\"start\":55357},{\"end\":55373,\"start\":55367},{\"end\":55389,\"start\":55380},{\"end\":55852,\"start\":55847},{\"end\":55862,\"start\":55859},{\"end\":55877,\"start\":55870},{\"end\":56293,\"start\":56285},{\"end\":56309,\"start\":56301},{\"end\":56320,\"start\":56316},{\"end\":56335,\"start\":56326},{\"end\":56751,\"start\":56746},{\"end\":56764,\"start\":56758},{\"end\":56778,\"start\":56773},{\"end\":56794,\"start\":56790},{\"end\":56809,\"start\":56802},{\"end\":57274,\"start\":57268},{\"end\":57288,\"start\":57281},{\"end\":57304,\"start\":57294},{\"end\":57316,\"start\":57310},{\"end\":57331,\"start\":57323},{\"end\":57807,\"start\":57801},{\"end\":57814,\"start\":57813},{\"end\":57816,\"start\":57815},{\"end\":57833,\"start\":57832},{\"end\":57848,\"start\":57840},{\"end\":57850,\"start\":57849},{\"end\":58295,\"start\":58292},{\"end\":58306,\"start\":58301},{\"end\":58320,\"start\":58313},{\"end\":58333,\"start\":58325},{\"end\":58345,\"start\":58338},{\"end\":58354,\"start\":58350},{\"end\":58369,\"start\":58360},{\"end\":58856,\"start\":58853},{\"end\":58864,\"start\":58862},{\"end\":58877,\"start\":58869},{\"end\":58891,\"start\":58882}]", "bib_author_last_name": "[{\"end\":37575,\"start\":37571},{\"end\":37584,\"start\":37581},{\"end\":37595,\"start\":37592},{\"end\":37606,\"start\":37602},{\"end\":37616,\"start\":37612},{\"end\":37632,\"start\":37628},{\"end\":38102,\"start\":38098},{\"end\":38114,\"start\":38112},{\"end\":38126,\"start\":38122},{\"end\":38141,\"start\":38137},{\"end\":38593,\"start\":38589},{\"end\":38607,\"start\":38602},{\"end\":38621,\"start\":38617},{\"end\":38633,\"start\":38629},{\"end\":38645,\"start\":38642},{\"end\":38659,\"start\":38657},{\"end\":38672,\"start\":38668},{\"end\":39102,\"start\":39084},{\"end\":39110,\"start\":39106},{\"end\":39122,\"start\":39118},{\"end\":39126,\"start\":39124},{\"end\":39386,\"start\":39368},{\"end\":39396,\"start\":39390},{\"end\":39411,\"start\":39404},{\"end\":39422,\"start\":39420},{\"end\":39428,\"start\":39424},{\"end\":39679,\"start\":39673},{\"end\":39695,\"start\":39689},{\"end\":39710,\"start\":39704},{\"end\":40266,\"start\":40256},{\"end\":40286,\"start\":40273},{\"end\":40299,\"start\":40294},{\"end\":40308,\"start\":40306},{\"end\":40328,\"start\":40316},{\"end\":40343,\"start\":40338},{\"end\":40360,\"start\":40351},{\"end\":40375,\"start\":40369},{\"end\":40747,\"start\":40743},{\"end\":41015,\"start\":41012},{\"end\":41027,\"start\":41024},{\"end\":41040,\"start\":41035},{\"end\":41257,\"start\":41255},{\"end\":41272,\"start\":41267},{\"end\":41286,\"start\":41283},{\"end\":41296,\"start\":41293},{\"end\":41749,\"start\":41737},{\"end\":41764,\"start\":41757},{\"end\":41776,\"start\":41771},{\"end\":42211,\"start\":42209},{\"end\":42224,\"start\":42221},{\"end\":42554,\"start\":42552},{\"end\":42567,\"start\":42565},{\"end\":42576,\"start\":42573},{\"end\":42944,\"start\":42942},{\"end\":42957,\"start\":42953},{\"end\":42970,\"start\":42968},{\"end\":42984,\"start\":42979},{\"end\":42996,\"start\":42992},{\"end\":43012,\"start\":43008},{\"end\":43352,\"start\":43350},{\"end\":43361,\"start\":43358},{\"end\":43374,\"start\":43372},{\"end\":43384,\"start\":43381},{\"end\":43400,\"start\":43396},{\"end\":43841,\"start\":43836},{\"end\":43851,\"start\":43848},{\"end\":43865,\"start\":43858},{\"end\":43881,\"start\":43873},{\"end\":44211,\"start\":44207},{\"end\":44228,\"start\":44221},{\"end\":44237,\"start\":44230},{\"end\":44550,\"start\":44545},{\"end\":44565,\"start\":44559},{\"end\":44574,\"start\":44567},{\"end\":44875,\"start\":44869},{\"end\":44889,\"start\":44884},{\"end\":44904,\"start\":44897},{\"end\":44917,\"start\":44912},{\"end\":45206,\"start\":45195},{\"end\":45226,\"start\":45217},{\"end\":45241,\"start\":45236},{\"end\":45258,\"start\":45251},{\"end\":45607,\"start\":45599},{\"end\":45621,\"start\":45615},{\"end\":45625,\"start\":45623},{\"end\":45873,\"start\":45863},{\"end\":45889,\"start\":45883},{\"end\":46227,\"start\":46220},{\"end\":46243,\"start\":46238},{\"end\":46256,\"start\":46247},{\"end\":46268,\"start\":46258},{\"end\":46581,\"start\":46579},{\"end\":46599,\"start\":46592},{\"end\":46865,\"start\":46863},{\"end\":46883,\"start\":46876},{\"end\":47205,\"start\":47203},{\"end\":47227,\"start\":47220},{\"end\":47238,\"start\":47232},{\"end\":47249,\"start\":47245},{\"end\":47267,\"start\":47260},{\"end\":47712,\"start\":47708},{\"end\":47723,\"start\":47720},{\"end\":47736,\"start\":47732},{\"end\":47750,\"start\":47747},{\"end\":47762,\"start\":47758},{\"end\":47774,\"start\":47772},{\"end\":48232,\"start\":48223},{\"end\":48250,\"start\":48243},{\"end\":48261,\"start\":48257},{\"end\":48276,\"start\":48272},{\"end\":48285,\"start\":48278},{\"end\":48777,\"start\":48772},{\"end\":48789,\"start\":48785},{\"end\":48802,\"start\":48798},{\"end\":49200,\"start\":49188},{\"end\":49218,\"start\":49211},{\"end\":49234,\"start\":49226},{\"end\":49630,\"start\":49618},{\"end\":49644,\"start\":49640},{\"end\":50000,\"start\":49996},{\"end\":50024,\"start\":50011},{\"end\":50031,\"start\":50026},{\"end\":50461,\"start\":50458},{\"end\":50470,\"start\":50468},{\"end\":50773,\"start\":50769},{\"end\":50785,\"start\":50783},{\"end\":50800,\"start\":50798},{\"end\":50814,\"start\":50810},{\"end\":51089,\"start\":51084},{\"end\":51103,\"start\":51096},{\"end\":51118,\"start\":51114},{\"end\":51127,\"start\":51120},{\"end\":51575,\"start\":51563},{\"end\":51587,\"start\":51580},{\"end\":51602,\"start\":51599},{\"end\":51611,\"start\":51609},{\"end\":51626,\"start\":51622},{\"end\":51635,\"start\":51628},{\"end\":51997,\"start\":51985},{\"end\":52016,\"start\":52009},{\"end\":52029,\"start\":52027},{\"end\":52038,\"start\":52031},{\"end\":52315,\"start\":52311},{\"end\":52325,\"start\":52323},{\"end\":52336,\"start\":52332},{\"end\":52810,\"start\":52806},{\"end\":52821,\"start\":52816},{\"end\":52833,\"start\":52828},{\"end\":52849,\"start\":52844},{\"end\":52861,\"start\":52859},{\"end\":53302,\"start\":53298},{\"end\":53314,\"start\":53312},{\"end\":53327,\"start\":53325},{\"end\":53343,\"start\":53339},{\"end\":53822,\"start\":53818},{\"end\":53832,\"start\":53828},{\"end\":53840,\"start\":53838},{\"end\":53851,\"start\":53849},{\"end\":53864,\"start\":53859},{\"end\":53877,\"start\":53871},{\"end\":54139,\"start\":54131},{\"end\":54152,\"start\":54148},{\"end\":54166,\"start\":54159},{\"end\":54473,\"start\":54471},{\"end\":54487,\"start\":54484},{\"end\":54503,\"start\":54496},{\"end\":54839,\"start\":54834},{\"end\":54850,\"start\":54846},{\"end\":54862,\"start\":54860},{\"end\":54875,\"start\":54871},{\"end\":54887,\"start\":54883},{\"end\":55323,\"start\":55319},{\"end\":55340,\"start\":55337},{\"end\":55355,\"start\":55352},{\"end\":55365,\"start\":55362},{\"end\":55378,\"start\":55374},{\"end\":55395,\"start\":55390},{\"end\":55857,\"start\":55853},{\"end\":55868,\"start\":55863},{\"end\":55880,\"start\":55878},{\"end\":56299,\"start\":56294},{\"end\":56314,\"start\":56310},{\"end\":56324,\"start\":56321},{\"end\":56339,\"start\":56336},{\"end\":56756,\"start\":56752},{\"end\":56771,\"start\":56765},{\"end\":56788,\"start\":56779},{\"end\":56800,\"start\":56795},{\"end\":56818,\"start\":56810},{\"end\":57279,\"start\":57275},{\"end\":57292,\"start\":57289},{\"end\":57308,\"start\":57305},{\"end\":57321,\"start\":57317},{\"end\":57336,\"start\":57332},{\"end\":57811,\"start\":57808},{\"end\":57822,\"start\":57817},{\"end\":57830,\"start\":57824},{\"end\":57838,\"start\":57834},{\"end\":57857,\"start\":57851},{\"end\":57865,\"start\":57859},{\"end\":58299,\"start\":58296},{\"end\":58311,\"start\":58307},{\"end\":58323,\"start\":58321},{\"end\":58336,\"start\":58334},{\"end\":58348,\"start\":58346},{\"end\":58358,\"start\":58355},{\"end\":58374,\"start\":58370},{\"end\":58860,\"start\":58857},{\"end\":58867,\"start\":58865},{\"end\":58880,\"start\":58878},{\"end\":58895,\"start\":58892}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":219615222},\"end\":38023,\"start\":37500},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":239012056},\"end\":38513,\"start\":38025},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":235306284},\"end\":39041,\"start\":38515},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3353774},\"end\":39327,\"start\":39043},{\"attributes\":{\"id\":\"b4\"},\"end\":39625,\"start\":39329},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2239473},\"end\":40221,\"start\":39627},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1033682},\"end\":40649,\"start\":40223},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14983192},\"end\":40940,\"start\":40651},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6137921},\"end\":41199,\"start\":40942},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206594692},\"end\":41639,\"start\":41201},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3988220},\"end\":42105,\"start\":41641},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":239049654},\"end\":42467,\"start\":42107},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":218674010},\"end\":42860,\"start\":42469},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208138485},\"end\":43277,\"start\":42862},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8182535},\"end\":43768,\"start\":43279},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14882677},\"end\":44130,\"start\":43770},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16182777},\"end\":44459,\"start\":44132},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":16453537},\"end\":44808,\"start\":44461},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":12447228},\"end\":45130,\"start\":44810},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":659803},\"end\":45551,\"start\":45132},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b20\"},\"end\":45778,\"start\":45553},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5574079},\"end\":46128,\"start\":45780},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14819664},\"end\":46528,\"start\":46130},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":220935474},\"end\":46795,\"start\":46530},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":229371498},\"end\":47122,\"start\":46797},{\"attributes\":{\"id\":\"b25\"},\"end\":47423,\"start\":47124},{\"attributes\":{\"id\":\"b26\"},\"end\":47617,\"start\":47425},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":239011825},\"end\":48141,\"start\":47619},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":25120102},\"end\":48646,\"start\":48143},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9652770},\"end\":49110,\"start\":48648},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11959057},\"end\":49523,\"start\":49112},{\"attributes\":{\"id\":\"b31\"},\"end\":49934,\"start\":49525},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2642044},\"end\":50373,\"start\":49936},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b33\",\"matched_paper_id\":167217261},\"end\":50693,\"start\":50375},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":27750744},\"end\":51007,\"start\":50695},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2017.2691703\",\"id\":\"b35\",\"matched_paper_id\":35488672},\"end\":51480,\"start\":51009},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":17623309},\"end\":51910,\"start\":51482},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":10784756},\"end\":52192,\"start\":51912},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":413731},\"end\":52725,\"start\":52194},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":28544003},\"end\":53212,\"start\":52727},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235718970},\"end\":53729,\"start\":53214},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":51604870},\"end\":54066,\"start\":53731},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":14882893},\"end\":54389,\"start\":54068},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6412560},\"end\":54738,\"start\":54391},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":10212545},\"end\":55249,\"start\":54740},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":201319385},\"end\":55770,\"start\":55251},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":235702592},\"end\":56247,\"start\":55772},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":195513288},\"end\":56688,\"start\":56249},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":6789015},\"end\":57193,\"start\":56690},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":219633658},\"end\":57736,\"start\":57195},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":15765187},\"end\":58188,\"start\":57738},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":52953674},\"end\":58762,\"start\":58190},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":244398896},\"end\":59257,\"start\":58764}]", "bib_title": "[{\"end\":37562,\"start\":37500},{\"end\":38087,\"start\":38025},{\"end\":38580,\"start\":38515},{\"end\":39082,\"start\":39043},{\"end\":39664,\"start\":39627},{\"end\":40250,\"start\":40223},{\"end\":40733,\"start\":40651},{\"end\":41004,\"start\":40942},{\"end\":41245,\"start\":41201},{\"end\":41727,\"start\":41641},{\"end\":42200,\"start\":42107},{\"end\":42542,\"start\":42469},{\"end\":42932,\"start\":42862},{\"end\":43340,\"start\":43279},{\"end\":43828,\"start\":43770},{\"end\":44203,\"start\":44132},{\"end\":44541,\"start\":44461},{\"end\":44861,\"start\":44810},{\"end\":45186,\"start\":45132},{\"end\":45853,\"start\":45780},{\"end\":46204,\"start\":46130},{\"end\":46572,\"start\":46530},{\"end\":46856,\"start\":46797},{\"end\":47698,\"start\":47619},{\"end\":48208,\"start\":48143},{\"end\":48761,\"start\":48648},{\"end\":49175,\"start\":49112},{\"end\":49991,\"start\":49936},{\"end\":50447,\"start\":50375},{\"end\":50758,\"start\":50695},{\"end\":51075,\"start\":51009},{\"end\":51561,\"start\":51482},{\"end\":51983,\"start\":51912},{\"end\":52302,\"start\":52194},{\"end\":52795,\"start\":52727},{\"end\":53289,\"start\":53214},{\"end\":53810,\"start\":53731},{\"end\":54123,\"start\":54068},{\"end\":54465,\"start\":54391},{\"end\":54823,\"start\":54740},{\"end\":55308,\"start\":55251},{\"end\":55845,\"start\":55772},{\"end\":56283,\"start\":56249},{\"end\":56744,\"start\":56690},{\"end\":57266,\"start\":57195},{\"end\":57799,\"start\":57738},{\"end\":58290,\"start\":58190},{\"end\":58851,\"start\":58764}]", "bib_author": "[{\"end\":37577,\"start\":37564},{\"end\":37586,\"start\":37577},{\"end\":37597,\"start\":37586},{\"end\":37608,\"start\":37597},{\"end\":37618,\"start\":37608},{\"end\":37634,\"start\":37618},{\"end\":38104,\"start\":38089},{\"end\":38116,\"start\":38104},{\"end\":38128,\"start\":38116},{\"end\":38143,\"start\":38128},{\"end\":38595,\"start\":38582},{\"end\":38609,\"start\":38595},{\"end\":38623,\"start\":38609},{\"end\":38635,\"start\":38623},{\"end\":38647,\"start\":38635},{\"end\":38661,\"start\":38647},{\"end\":38674,\"start\":38661},{\"end\":39104,\"start\":39084},{\"end\":39112,\"start\":39104},{\"end\":39124,\"start\":39112},{\"end\":39128,\"start\":39124},{\"end\":39388,\"start\":39368},{\"end\":39398,\"start\":39388},{\"end\":39413,\"start\":39398},{\"end\":39424,\"start\":39413},{\"end\":39430,\"start\":39424},{\"end\":39681,\"start\":39666},{\"end\":39697,\"start\":39681},{\"end\":39712,\"start\":39697},{\"end\":40268,\"start\":40252},{\"end\":40288,\"start\":40268},{\"end\":40301,\"start\":40288},{\"end\":40310,\"start\":40301},{\"end\":40330,\"start\":40310},{\"end\":40345,\"start\":40330},{\"end\":40362,\"start\":40345},{\"end\":40377,\"start\":40362},{\"end\":40749,\"start\":40735},{\"end\":41017,\"start\":41006},{\"end\":41029,\"start\":41017},{\"end\":41042,\"start\":41029},{\"end\":41259,\"start\":41247},{\"end\":41274,\"start\":41259},{\"end\":41288,\"start\":41274},{\"end\":41298,\"start\":41288},{\"end\":41751,\"start\":41729},{\"end\":41766,\"start\":41751},{\"end\":41778,\"start\":41766},{\"end\":42213,\"start\":42202},{\"end\":42226,\"start\":42213},{\"end\":42556,\"start\":42544},{\"end\":42569,\"start\":42556},{\"end\":42578,\"start\":42569},{\"end\":42946,\"start\":42934},{\"end\":42959,\"start\":42946},{\"end\":42972,\"start\":42959},{\"end\":42986,\"start\":42972},{\"end\":42998,\"start\":42986},{\"end\":43014,\"start\":42998},{\"end\":43354,\"start\":43342},{\"end\":43363,\"start\":43354},{\"end\":43376,\"start\":43363},{\"end\":43386,\"start\":43376},{\"end\":43402,\"start\":43386},{\"end\":43843,\"start\":43830},{\"end\":43853,\"start\":43843},{\"end\":43867,\"start\":43853},{\"end\":43883,\"start\":43867},{\"end\":44213,\"start\":44205},{\"end\":44230,\"start\":44213},{\"end\":44239,\"start\":44230},{\"end\":44552,\"start\":44543},{\"end\":44567,\"start\":44552},{\"end\":44576,\"start\":44567},{\"end\":44877,\"start\":44863},{\"end\":44891,\"start\":44877},{\"end\":44906,\"start\":44891},{\"end\":44919,\"start\":44906},{\"end\":45208,\"start\":45188},{\"end\":45228,\"start\":45208},{\"end\":45243,\"start\":45228},{\"end\":45260,\"start\":45243},{\"end\":45609,\"start\":45597},{\"end\":45623,\"start\":45609},{\"end\":45627,\"start\":45623},{\"end\":45875,\"start\":45855},{\"end\":45891,\"start\":45875},{\"end\":46229,\"start\":46206},{\"end\":46245,\"start\":46229},{\"end\":46258,\"start\":46245},{\"end\":46270,\"start\":46258},{\"end\":46583,\"start\":46574},{\"end\":46601,\"start\":46583},{\"end\":46867,\"start\":46858},{\"end\":46885,\"start\":46867},{\"end\":47207,\"start\":47198},{\"end\":47229,\"start\":47207},{\"end\":47240,\"start\":47229},{\"end\":47251,\"start\":47240},{\"end\":47269,\"start\":47251},{\"end\":47714,\"start\":47700},{\"end\":47725,\"start\":47714},{\"end\":47738,\"start\":47725},{\"end\":47752,\"start\":47738},{\"end\":47764,\"start\":47752},{\"end\":47776,\"start\":47764},{\"end\":48234,\"start\":48210},{\"end\":48252,\"start\":48234},{\"end\":48263,\"start\":48252},{\"end\":48278,\"start\":48263},{\"end\":48287,\"start\":48278},{\"end\":48779,\"start\":48763},{\"end\":48791,\"start\":48779},{\"end\":48804,\"start\":48791},{\"end\":49202,\"start\":49177},{\"end\":49220,\"start\":49202},{\"end\":49236,\"start\":49220},{\"end\":49632,\"start\":49607},{\"end\":49646,\"start\":49632},{\"end\":50002,\"start\":49993},{\"end\":50026,\"start\":50002},{\"end\":50033,\"start\":50026},{\"end\":50463,\"start\":50449},{\"end\":50472,\"start\":50463},{\"end\":50775,\"start\":50760},{\"end\":50787,\"start\":50775},{\"end\":50802,\"start\":50787},{\"end\":50816,\"start\":50802},{\"end\":51091,\"start\":51077},{\"end\":51105,\"start\":51091},{\"end\":51120,\"start\":51105},{\"end\":51129,\"start\":51120},{\"end\":51577,\"start\":51563},{\"end\":51589,\"start\":51577},{\"end\":51604,\"start\":51589},{\"end\":51613,\"start\":51604},{\"end\":51628,\"start\":51613},{\"end\":51637,\"start\":51628},{\"end\":51999,\"start\":51985},{\"end\":52018,\"start\":51999},{\"end\":52031,\"start\":52018},{\"end\":52040,\"start\":52031},{\"end\":52317,\"start\":52304},{\"end\":52327,\"start\":52317},{\"end\":52338,\"start\":52327},{\"end\":52812,\"start\":52797},{\"end\":52823,\"start\":52812},{\"end\":52835,\"start\":52823},{\"end\":52851,\"start\":52835},{\"end\":52863,\"start\":52851},{\"end\":53304,\"start\":53291},{\"end\":53316,\"start\":53304},{\"end\":53329,\"start\":53316},{\"end\":53345,\"start\":53329},{\"end\":53824,\"start\":53812},{\"end\":53834,\"start\":53824},{\"end\":53842,\"start\":53834},{\"end\":53853,\"start\":53842},{\"end\":53866,\"start\":53853},{\"end\":53879,\"start\":53866},{\"end\":54141,\"start\":54125},{\"end\":54154,\"start\":54141},{\"end\":54168,\"start\":54154},{\"end\":54475,\"start\":54467},{\"end\":54489,\"start\":54475},{\"end\":54505,\"start\":54489},{\"end\":54841,\"start\":54825},{\"end\":54852,\"start\":54841},{\"end\":54864,\"start\":54852},{\"end\":54877,\"start\":54864},{\"end\":54889,\"start\":54877},{\"end\":55325,\"start\":55310},{\"end\":55342,\"start\":55325},{\"end\":55357,\"start\":55342},{\"end\":55367,\"start\":55357},{\"end\":55380,\"start\":55367},{\"end\":55397,\"start\":55380},{\"end\":55859,\"start\":55847},{\"end\":55870,\"start\":55859},{\"end\":55882,\"start\":55870},{\"end\":56301,\"start\":56285},{\"end\":56316,\"start\":56301},{\"end\":56326,\"start\":56316},{\"end\":56341,\"start\":56326},{\"end\":56758,\"start\":56746},{\"end\":56773,\"start\":56758},{\"end\":56790,\"start\":56773},{\"end\":56802,\"start\":56790},{\"end\":56820,\"start\":56802},{\"end\":57281,\"start\":57268},{\"end\":57294,\"start\":57281},{\"end\":57310,\"start\":57294},{\"end\":57323,\"start\":57310},{\"end\":57338,\"start\":57323},{\"end\":57813,\"start\":57801},{\"end\":57824,\"start\":57813},{\"end\":57832,\"start\":57824},{\"end\":57840,\"start\":57832},{\"end\":57859,\"start\":57840},{\"end\":57867,\"start\":57859},{\"end\":58301,\"start\":58292},{\"end\":58313,\"start\":58301},{\"end\":58325,\"start\":58313},{\"end\":58338,\"start\":58325},{\"end\":58350,\"start\":58338},{\"end\":58360,\"start\":58350},{\"end\":58376,\"start\":58360},{\"end\":58862,\"start\":58853},{\"end\":58869,\"start\":58862},{\"end\":58882,\"start\":58869},{\"end\":58897,\"start\":58882}]", "bib_venue": "[{\"end\":37715,\"start\":37634},{\"end\":38225,\"start\":38143},{\"end\":38735,\"start\":38674},{\"end\":39168,\"start\":39128},{\"end\":39366,\"start\":39329},{\"end\":39850,\"start\":39712},{\"end\":40426,\"start\":40377},{\"end\":40783,\"start\":40749},{\"end\":41057,\"start\":41042},{\"end\":41379,\"start\":41298},{\"end\":41852,\"start\":41778},{\"end\":42275,\"start\":42226},{\"end\":42595,\"start\":42578},{\"end\":43051,\"start\":43014},{\"end\":43479,\"start\":43402},{\"end\":43931,\"start\":43883},{\"end\":44256,\"start\":44239},{\"end\":44614,\"start\":44576},{\"end\":44953,\"start\":44919},{\"end\":45323,\"start\":45260},{\"end\":45595,\"start\":45553},{\"end\":45940,\"start\":45891},{\"end\":46310,\"start\":46270},{\"end\":46655,\"start\":46601},{\"end\":46945,\"start\":46885},{\"end\":47196,\"start\":47124},{\"end\":47489,\"start\":47425},{\"end\":47842,\"start\":47776},{\"end\":48354,\"start\":48287},{\"end\":48857,\"start\":48804},{\"end\":49299,\"start\":49236},{\"end\":49605,\"start\":49525},{\"end\":50110,\"start\":50033},{\"end\":50520,\"start\":50476},{\"end\":50835,\"start\":50816},{\"end\":51217,\"start\":51155},{\"end\":51675,\"start\":51637},{\"end\":52044,\"start\":52040},{\"end\":52415,\"start\":52338},{\"end\":52930,\"start\":52863},{\"end\":53426,\"start\":53345},{\"end\":53884,\"start\":53879},{\"end\":54216,\"start\":54168},{\"end\":54544,\"start\":54505},{\"end\":54956,\"start\":54889},{\"end\":55468,\"start\":55397},{\"end\":55963,\"start\":55882},{\"end\":56422,\"start\":56341},{\"end\":56897,\"start\":56820},{\"end\":57419,\"start\":57338},{\"end\":57947,\"start\":57867},{\"end\":58439,\"start\":58376},{\"end\":58968,\"start\":58897},{\"end\":37783,\"start\":37717},{\"end\":38294,\"start\":38227},{\"end\":38783,\"start\":38737},{\"end\":39975,\"start\":39852},{\"end\":41447,\"start\":41381},{\"end\":43543,\"start\":43481},{\"end\":47540,\"start\":47491},{\"end\":47895,\"start\":47844},{\"end\":48408,\"start\":48356},{\"end\":50174,\"start\":50112},{\"end\":52479,\"start\":52417},{\"end\":52984,\"start\":52932},{\"end\":53494,\"start\":53428},{\"end\":55010,\"start\":54958},{\"end\":55526,\"start\":55470},{\"end\":56031,\"start\":55965},{\"end\":56490,\"start\":56424},{\"end\":56961,\"start\":56899},{\"end\":57487,\"start\":57421},{\"end\":58489,\"start\":58441},{\"end\":59026,\"start\":58970}]"}}}, "year": 2023, "month": 12, "day": 17}