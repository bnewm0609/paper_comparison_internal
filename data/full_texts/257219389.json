{"id": 257219389, "updated": "2023-10-05 03:30:44.819", "metadata": {"title": "LAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints", "authors": "[{\"first\":\"Mengmeng\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Lin\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Hellward\",\"last\":\"Broszio\",\"middle\":[]},{\"first\":\"Jiangtao\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Runjiang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Monika\",\"last\":\"Sester\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Yang\",\"middle\":[\"Ying\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Trajectory prediction for autonomous driving must continuously reason the motion stochasticity of road agents and comply with scene constraints. Existing methods typically rely on one-stage trajectory prediction models, which condition future trajectories on observed trajectories combined with fused scene information. However, they often struggle with complex scene constraints, such as those encountered at intersections. To this end, we present a novel method, called LAformer. It uses a temporally dense lane-aware estimation module to select only the top highly potential lane segments in an HD map, which effectively and continuously aligns motion dynamics with scene information, reducing the representation requirements for the subsequent attention-based decoder by filtering out irrelevant lane segments. Additionally, unlike one-stage prediction models, LAformer utilizes predictions from the first stage as anchor trajectories and adds a second-stage motion refinement module to further explore temporal consistency across the complete time horizon. Extensive experiments on Argoverse 1 and nuScenes demonstrate that LAformer achieves excellent performance for multimodal trajectory prediction.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2302.13933", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2302-13933", "doi": "10.48550/arxiv.2302.13933"}}, "content": {"source": {"pdf_hash": "5bb0907de748de3b9b827e9f1ab4f9944249d459", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.13933v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e47252b9a5790d9b0777eb76c5c7af703c1701e0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5bb0907de748de3b9b827e9f1ab4f9944249d459.txt", "contents": "\nLAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints\n\n\nMengmeng Liu \nLeibniz University Hannover\n\n\nPhiGent Robotics\n\n\nHao Cheng h.cheng-2@utwente.nl \nUniversity of Twente\n\n\nLin Chen \nVISCODA GmbH\n\n\nHellward Broszio \nVISCODA GmbH\n\n\nJiangtao Li \nPhiGent Robotics\n\n\nRunjiang Zhao \nPhiGent Robotics\n\n\nMonika Sester \nLeibniz University Hannover\n\n\nMichael Ying Yang \nUniversity of Twente\n\n\nLAformer: Trajectory Prediction for Autonomous Driving with Lane-Aware Scene Constraints\n\nTrajectory prediction for autonomous driving must continuously reason the motion stochasticity of road agents and comply with scene constraints. Existing methods typically rely on one-stage trajectory prediction models, which condition future trajectories on observed trajectories combined with fused scene information. However, they often struggle with complex scene constraints, such as those encountered at intersections. To this end, we present a novel method, called LAformer. It uses a temporally dense laneaware estimation module to select only the top highly potential lane segments in an HD map, which effectively and continuously aligns motion dynamics with scene information, reducing the representation requirements for the subsequent attention-based decoder by filtering out irrelevant lane segments. Additionally, unlike one-stage prediction models, LAformer utilizes predictions from the first stage as anchor trajectories and adds a second-stage motion refinement module to further explore temporal consistency across the complete time horizon. Extensive experiments on Argoverse 1 and nuScenes demonstrate that LAformer achieves excellent performance for multimodal trajectory prediction.\n\nIntroduction\n\nAccurate trajectory prediction is paramount for enabling autonomous driving in diverse traffic scenarios involving interactions with various road agents. Due to the stochastic behaviors of agents and their mutual influences, in addition to the varying environmental scene contexts, trajectory prediction remains an exceedingly challenging task. Therefore, this task necessitates effective learning of an agent's motion dynamics and interactions with other agents, as well as careful consideration of scene constraints.\n\nNumerous data-driven approaches have been developed to tackle trajectory prediction by extracting motion dynam- ics from sequential trajectories and scene contexts from rasterized map data, and then fusing them in a latent space as the input to a multimodal decoder, as demonstrated in works such as Trajectron++ [39], CoverNet [34], and AgentFormer [47]. However, these approaches fail to utilize spatial and temporal information at an early stage for the subsequent decoding module. Moreover, rasterized map requires large receptive filters and computational cost to perceive the scene context, which may not provide accurate road structure features at complex intersections, especially for vehicle trajectory prediction. Consequently, the decoder may generate trajectory predictions that are non-compliant with the scene. To mitigate this problem, VectorNet [10] proposes to unify trajectory and high-definition (HD) map data into a consistent vectorized form. This vectorization enables trajectories and lane segments based on HD maps to be easily processed and fused using the same encoder.\n\nThere are already quite a lot attempts to explore lane segments, including deep feature fusion, e.g. [20,48] and heuristic searching e.g. [9]. We further categorize the mainstream methods into spatially and temporally dense methods. Most of the current methods belong to the former one, which estimate dense probabilistic goal candidates [50,15], segment proposals for endpoints [43,11,12] or for the whole sequence encoding [9] projected on the given scene. We argue that these methods are suboptimal because compound prediction errors could occur if the prediction is inaccurate at initial steps. In contrast, the temporally dense methods seek to estimate the likelihood of motion states aligning with lane positions at each time step. Hence, the decoder has a better chance to adjust its predictions if the motion states and lane segments deviate as the time unfolds. However, this is not trivial because the estimation module needs to account for the variability of lane segments and uncertainty of motion states. Also, the alignment simply based on distance metrics [12] is insufficient when the ego vehicle is at an intersection with multiple parallel lanes, or when the ego vehicle makes a lane change or a turn. Nevertheless, not much research has been done in exploring the temporally dense methods and not much attention has been paid to selectively feed the map information to the decoder in order to facilitate the decoding process.\n\nTo this end, we propose a temporally dense method, called LAformer. The essence of LAformer is illustrated in Fig. 1. It utilizes a lane-aware estimation module to select only the top-k highly potential lane segments at each time step, which effectively and continuously aligns motion dynamics with scene information. Specifically, we employ an attention-based encoder, termed Global Interaction Graph (GIG), to extract spatial-temporal features from the unified vectorized trajectories and HD map. Different from the spatially dense methods such as [43,11,12,9], we train a binary classifier using the lane information extracted from the GIG module and the target agent's motion including speed and orientation information for step-wise lane selection throughout the prediction time horizon. Then, we introduce a Laplacian mixture density network (MDN) to generate scene-compliant multimodal trajectory predictions aligned with only the selected lane segments. In this way, irrelevant lane segments are filtered out to lessen the representation requirements for the decoding process.\n\nAdditionally, to further exploit the temporal consistency over the complete time horizon, we introduce a motion refinement module. LAformer utilizes predictions from the first stage as anchor trajectories, which distinguishes itself from anchor-points-based trajectory prediction methods using predefined anchor points [3,41]. Then the second-stage motion refinement module takes as input both the observed and predicted trajectories to further reduce prediction offsets, which is different from that of the first stage. Although, the spirit of this refinement module is not particularly new in computer vision tasks, to our best knowledge, we are the first to effectively apply it to improve trajectory prediction.\n\nOur key contributions are summarized as follows:\n\n\u2022 We propose a novel temporally dense lane-aware se-lection method to identify the top-k highly potential lane segments at each predicted time step, which is different from previous spatially dense approaches. This selection method facilitates the lane-conditioned decoder for trajectory prediction.\n\n\u2022 We leverage the predicted trajectories from the first stage as anchor trajectories and introduce a secondstage motion refinement module that considers both observed and predicted trajectories. The refinement module further explores the temporal consistency across the past and future time horizons.\n\n\u2022 We demonstrate the effectiveness of LAformer on two benchmark datasets, i.e., Argoverse 1 [4] and nuScenes [2]. It achieves excellent performances on both benchmarks and shows superior generalized performance for the multimodal motion prediction task.\n\n\nRelated Work\n\nModeling interactions between agents. Agents are interconnected for social connections and collision avoidance [33,17]. Most deep learning models, e.g. [1,24,16,38,39,47], use agents' hidden states to aggregate the interaction information. The most popular aggregation strategies include pooling [1,7,10], message passing [49,46,48] using graph convolutional networks (GCNs) [44], and attention mechanisms [47,26,5]. To differentiate the impacts of surrounding agents based on their relative positions and attributes, we propose the use of attention mechanisms for interaction modeling in this work.\n\nPredicting multimodal trajectories. In the context of trajectory prediction for autonomous driving, predicting diverse multimodal trajectories is more favorable than singlemodal trajectories to cope with agents' uncertain behaviors and scene constraints. Generative models, e.g., Generative Adversarial Nets (GANs) [14], Variational Auto-Encoder (VAE) [23] and conditional-VAE [22], and Flows [35], use sampling-based approaches to generate multiple predictions [16,24,36,6]. However, they do not provide a straightforward estimation of the likelihood of each mode. Although Gaussian Mixture Density Networks (MDNs) can provide a probability density function to learn the mode distribution, similar to the generative models, they often suffer from the so-called mode-collapse problem [37] when only a single ground truth trajectory is used for supervised learning. To mitigate the mode-collapse problem, this paper explores the use of a Laplacian MDN with a winner-takes-all strategy [28,52,9]. Additional, in order to increase modality diversity, some approaches generate a plethora of predictions and employ ensembling techniques such as clustering or Non Maximum Suppression to reduce the predictions into a limited number of modalities [41,43]. Nevertheless, this ensembling process is time-consuming and impractical for real-time autonomous vehicles [52]. Therefore, this paper refrains from adopting this technique.\n\nExtracting scene contextual information. To predict scene-compliant trajectories, scene contexts must be considered. Convolutional neural networks (CNNs) are commonly used to extract scene contexts from bird's-eye view images, such as RGB images with general contexts [24,38] and semantic maps with different scene categories [34,39,47]. However, CNNs struggle to capture fine-grained scene information like lane geometry and traffic regulations. Furthermore, the sparse information on rasterized data leads to less computational efficiency, requiring a powerful fusion module to align heterogeneous motion and scene information for the prediction module. To address these challenges, a unified vectorization scheme [10] can be used to align trajectories and lanes from an HD map. Both trajectories and scene contexts, denoted by points, polylines, and polygons, are coded in a unified vector with coordinate information and various agent or lane attributes [25,15,52,5,31,9,43]. This data representation is adopted in our scene-aware trajectory prediction approach. Several approaches have been proposed to leverage lanebased scene information to guide the prediction process. Proposal-based models [7,40] classify an agent's maneuvers and then predict subsequent trajectories accordingly. Goal-based models predict feasible goals [36,50,15,11,12] that lie in plausible lanes, and then generate complete trajectories. Other methods use a fixed set of anchors corresponding to trajectory distribution modes to regress predicted multimodal trajectories [3,41]. Alternatively, [51] proposes a method that treats the collection of historical trajectories at an agent's current location as prior information to narrow down the search space for potential future tra-jectories. We categorize these methods as spatially dense lane-based methods, as they focus on generating a probabilistic distribution of candidate goals or full trajectories over the space. However, these methods do not fully explore temporal information to account for motion uncertainty and scene variability as time progresses. Additionally, the prediction module must implicitly filter out irrelevant scene information, which can be challenging in complex scene constraints, such as those present at intersections.\n\nIn contrast to these methods, we propose a temporally dense lane-aware module to learn the alignment between an agent's motion dynamics and potential lane segments. Instead of simply combining motion encoding and lane encoding and allowing the decoder to implicitly learn their relationship [32], we explicitly estimate the likelihood of the lane that an agent will take at each time step. Then, we select only the top highly potential lane information to balance the variability of lane segments and uncertainty of motion dynamics.\n\n\nMethod\n\n\nProblem formulation\n\nFollowing the mainstream works, e.g. [10,15,43,52,9], we assume that detecting and tracking road agents, as well as perceiving the environment, provides high-quality trajectory and HD map data in a 2D coordinate system. Namely, for agent i, the xand y-positions within a given time horizon {\u2212t h + 1, \u00b7 \u00b7 \u00b7 , 0, 1, \u00b7 \u00b7 \u00b7 , t f } are obtained, along with the HD map C of the scene. The downstream task is to predict the subsequent trajectories Y i 1:t f by leveraging the HD map and observed trajectories of all agents in the given scenario, including the target agent's trajectory X i \u2212t h +1:0 . Both agents' past trajectories and lane centerlines are represented as vectors. To be more specific, for agent i, its history trajectory X i is represented as an ordered sequence of sparse trajectory vectors\nA i \u2212t h +1:0 = {v i \u2212t h +2 , v i \u2212t h +3 , ..., v i 0 } over the past t h time steps. Each trajectory vector v i t is defined as v i t = [d i t,s , d i t,e , a i ],\nwhere d i t,s and d i t,e denote the start and end points, respectively, and a i corresponds to agent i's attribute features, such as timestamp and object type (i.e., autonomous vehicles, target agent, and others). In addition, lane centerlines are further sliced into predefined segments to capture fine-grained lane information in order to model an agent's intention precisely. Similar to the trajectory vector, a lane centerline segment is represented as C i 1:\nN = {v i 1 , v i 2 , ..., v i N }, where N denotes the total vector length. Each lane vector v i n = [d i n,s , d i n,e , a i , d i n,pre ] adds d i n,\npre to indicate the predecessor of the start point. The lane vectors are connected end-to-end to obtain the HD map's structural features.\n\nMoreover, to ensure input feature invariance with respect to an agent's location, the coordinates of all vectors are normalized to be centered around the target agent's last observed position. Fig. 2 presents the overall framework of LAformer, which takes vectorized trajectories and HD map lane segments as input and outputs multimodal trajectories for the target agent. Each module of LAformer is explained in detail below.\n\n\nAgent motion and scene encoding\n\nWe design an attention-based Global Interaction Graph (GIG) to encode agent motion and scene information. Concretely, we process trajectory vectors A i and lane vectors C j 1:N using a Multi-Layer Perceptron (MLP) and a Gated Recurrent Unit (GRU) layer in a sequential manner. The output encodings of these layers are represented as h i for \u2200i \u2208 {1, . . . , N traj } and c j for \u2200j \u2208 {1, . . . , N lane } in a given scenario. To fuse these encodings, we design a symmetric cross attention mechanism that operates on h i and c j as follows:\nh i = h i + CrossAtt{h i , c j } for j \u2208 {1, . . . , N lane }, (1) c j = c j + CrossAtt{c j , h i } for i \u2208 {1, . . . , N traj }. (2)\nAfterwards, the GIG further explores the self-attention and skip-connection to learn the interactions among agents.\nh i = ConCat[h i , c j ] for j \u2208 {1, . . . , N lane },(3)h i = h i + SelfAtt{h i } for i \u2208 {1, . . . , N traj }. (4)\n\nTemporally dense lane-aware estimation\n\nWe propose a temporally dense lane-aware probability estimation module that uses attention to guide a target agent towards the most influential lane segments for its future trajectories. Specifically, we align the target agent's motion and lane information at each future time step t \u2208 {1, . . . , t f }. To achieve this, we predict lane probabilities using a lane scoring header and an attention mechanism.\n\nThe key (K) and value (V ) vectors are linear projections of the agent motion encoding h i , while the query (Q) vector is a linear projection of the lane encoding c j . These vectors are then fed into a scaled dot-product attention block\nA i,j = softmax( QK T \u221a d k\n)V , resulting in the predicted score of the j-th lane segment at t given b\u0177\ns j,t = exp(\u03c6{h i , c j , A i,j }) Nlane n=1 exp(\u03c6{h i , c n , A i,n }) ,(5)\nwhere \u03c6 denotes a two-layer MLP.\n\nTo balance the variability of lane segments and uncertainty of motion dynamics, we select the top-k lane segments {c 1 , c 2 , . . . , c k } with the k highest scores {\u015d 1 ,\u015d 2 , . . . ,\u015d k } as the candidate lane segments. We then concatenate the candidate lane segments and associated scores over the future time steps to obtain C = ConCat{c 1:k ,\u015d 1:k } t f t=1 . Next, we perform cross attention to project the target agent's past trajectory encoding h i as the query vector, and the candidate lane encodings C as the key and value vectors. The output is updated motion information aligned with the lane information, denoted as h i,att . This cross attention further explores scene information in spatial and temporal dimensions.\n\nThe lane scoring module uses a binary cross-entropy loss L lane to optimize the probability estimation. The ground truth value s t is set to 1 for the lane segment that is closest to the trajectory' truth position, and 0 for all other lanes. It is worth mentioning that the ground truth lane segment s t does not need additional labeling and can be identified easily using a distance metric, such as the Euclidean distance.\nL lane = tf t=1 L CE (s t ,\u015d t ).(6)\n\nMultimodal conditional decoder\n\nThis section introduces a Laplacian mixture density network (MDN) decoder that is conditioned on the encodings of the target agent's past trajectory h i and the updated motion information aligned with the candidate lane information h i,att . To further preserve the diversity of multimodalities, we sample a latent vector z from a multivariate normal distribution, which serves as an additional condition added to the encodings for the predictions. The decoder predicts a set of trajectories We train the Laplacian MDN decoder by minimizing a regression loss and a classification loss. The regression loss is computed using the Winner-Takes-All strategy [28,52,9] and is defined as:\nL reg = 1 t f t f t=1 \u2212 log P (Y t |\u00b5 m * t , b m * t ),(7)\nwhere Y represents the ground truth position and m * represents the mode with the minimum L 2 error among the M predictions. The cross-entropy loss is used to optimize the mode classification and is defined as:\nL cls = M m=1 \u2212\u03c0 m log(\u03c0 m ).(8)\nWe adopt the soft displacement error, following [52], as our target probability \u03c0 m . The total loss for the motion prediction in the first stage is given by:\nL S1 = \u03bb 1 L lane + L reg + L cls ,(9)\nwhere \u03bb 1 controls the relative importance of L lane .\n\n\nMotion refinement\n\nA second-stage motion refinement is introduced to further explore the temporal consistency for predicting more accurate future trajectories. The goal is to reduce the offset between ground truth trajectory Y 1:t f and predicted trajec-tory\u0176 1:t f . In this stage, we leverage the complete trajectory\n{{X} 0 \u2212t h +1 , {\u0176 } t f\n1 } as the input to extract the motion encoding\u1e23 i using a similar temporal encoder as in the first stage. Then, a regression header constructed by a two-layer MLP takes as input all the motion encodings [h i , h i,att ,\u1e23 i ] in both stages and predicts the offset \u2206Y = Y \u2212\u0176 m between the ground truth and predicted trajectories. We use L 2 loss to optimize the offset.\nL off = 1 t f t f t=1 ||\u2206\u0176 t \u2212 \u2206Y t || 2 .(10)\nFurthermore, we use a cosine function, denoted by Eq. (11), to explicitly aid the model in learning the turning angle from the last observed position. It measures the difference between the ground truth angle \u03b8 t = arctan2(Y t \u2212 X 0 ) and the predicted angle\u03b8 t = arctan2(\u0176 t \u2212 X 0 ).\nL angle = 1 t f t f t=1 \u2212cos(\u03b8 t \u2212 \u03b8 t ).(11)\nHere, we employ a Winner-Takes-All strategy to optimize the offset and angle losses, similar to the first stage. The total loss in the second stage can be expressed as:\nL S2 = L S1 + \u03bb 2 L off + \u03bb 3 L angle ,(12)\nwhere \u03bb 2 and \u03bb 3 control the relative importance of the corresponding loss terms.\n\n\nExperiments\n\n\nExperimental setup\n\nDatasets. The proposed approach is developed and evaluated on two challenging and widely used benchmarks for autonomous driving: nuScenes [2] and Argoverse 1 [4]. These benchmarks provide trajectories of various types of road agents with an HD map of the given scene. In nuScenes, the target agent's subsequent six-second trajectory is predicted based on its and neighboring agents' trajectories up to two seconds, with trajectory sampling at 2 Hz. In Argoverse 1, the target agent's subsequent three-second trajectory is predicted based on its and neighboring agents' trajectories in the initial two seconds, with trajectory sampling at 10 Hz. To ensure a fair comparison, the official data partitioning and online test server of both benchmarks are used for the training and test setting, respectively.\n\nEvaluation metrics: We adopt the standard evaluation metrics to measure prediction performance, including FDE K and ADE K for measuring L 2 errors at the final step and averaged at each step, respectively, for predicting K modes.\n\nHere, the minimum error of the K modes is reported. Both ADE and FDE are measured in meters. In addition, the miss rate MR K measures the percentage of scenarios for which the final-step error is larger than 2.0 m. K is set to 5 and 10 in nuScenes and 6 in Argoverse 1 for the multimodal trajectory prediction. For all the evaluation metrics, the lower the better.\n\nImplementation details. The hidden dimension of all the feature vectors in LAformer is set to 128. Only the lane segments that are within 50 m (Manhattan distance) of the target agent are sampled as the scene contexts. \u03bb 1 in Eq. (9) is set to 10. \u03bb 2 , \u03bb 3 in Eq. (12) are set to 5 and 2, respectively. We use a two-stage training scheme. In the first stage, all the modules except for the motion refinement module are trained using the Adam optimizer [21]. In the second stage, all the modules are trained together. LAFormer was trained on 8xRTX3090 cards with each stage for about 8 hours 1 .\n\n\nQuantitative results and comparison\n\nTables 1 and 2 present the results obtained on the Argoverse 1 validation and online test sets, and the nuScenes online test set, respectively. The leaderboard results (online tests) are updated up to 2023-02-20 according to the officially published papers.\n\nIn the Argoverse 1 benchmark, LAformer achieves the state-of-the-art performance on the validation set by a clear margin in ADE and FDE. It also achieves excellent results on the test set, on par with the runner-up method HiVT.\n\nIn the nuScenes benchmark, LAformer achieves compet-  [51]. \u2021: the models tested on both benchmarks. The best/second best values are highlighted in boldface/underlined.\n\nitive performance, only slightly inferior to the newly released FRM [32] in terms of ADE. FRM introduces relationship reasoning to help understand future interactions between the ego and other agents, while LAformer relies on attention mechanisms to learn interactions between agents and focuses more on scene constraints. This difference in approach may contribute to the performance difference. However, LAformer outperforms other lane-based models, e.g., LaneGCN [25,51] and PGP [9], with a clear margin, indicating that our lane-aware estimation is more effective than the other distance-based or heuristic lane searching. Moreover, when compared to the models (marked by \u2021) tested on both benchmarks, namely THOMAS [11], GO-HOME [12], LaneGCN [25]\u2666 enhanced by local behavior data LBA [51], as well as FRM, LAformer shows evidently more generalized performance across the benchmarks. This suggests that the proposed temporally dense lane-aware estimation module effectively aligns scene constraints with motion dynamics, even though the trajectories provided in Argoverse 1 and nuScenes include locations in different cities and driving directions. Further evidence supporting the efficacy of this module can be found in the following ablation study presented in Table 3.\n\n\nAblation study\n\nConsidering data scale and availability of ground truth, we carry out the ablation study on the Argoverse  From Table 3, the performance of the Baseline is much inferior to the other models. The comparison of Baseline vs. Baseline+S2 and LAformer (Tem.) vs. LAformer (Full) demonstrates the performance gain of S2, e.g., ca. 3% in FDE. The comparison of LAformer (Spa.) vs. LAformer (Tem.) shows that our temporally dense method is more effective than the spatially dense method, reducing ADE by about 4% and FDE by about 8%. Furthermore, we carry out an ablation study to analyze the effectiveness of the angle L angle and offset L off losses in the second stage. As shown in Table 4, by including both losses helps to improve the prediction accuracy by about 2% measured in ADE and FDE. Table 4. Ablation study on the loss functions to penalize the angle Langle and offset Loff errors in the second stage.\nL angle L off ADE 6 FDE 6 \u221a - 0.65 0.94 - \u221a 0.65 0.92 \u221a \u221a 0.64 0.92\nWe also ablate the latent variable z added to the input of the multimodal conditional decoder. It is sampled from a multivariate normal distribution with its dimension setting to 2. However, as shown in Table 5, we find that inserting z only leads to a marginal performance improvement (less than 0.5 cm). \n\n\nSensitivity analysis of the hyper-parameters\n\nThe number of top-k lane segments and the weights of losses are crucial hyper-parameters for LAformer. To examine their impact, we conduct an empirical study by varying their values around the experimental settings indicated by an underline in Tables 6 and 7.\n\nAs shown on the left side of Table 6, increasing the number of lane segments from 1 to 4 initially results in a performance gain up to k = 3, but after that, it starts to decline. In the second stage, k = 2 provides better results than 3. Using a larger k increases the chances of including irrelevant lane segments, while a relatively small k enables the decoder to focus on the most relevant lane segments. We also vary the loss weights \u03bb 1 in Eq. (9) and \u03bb 2 , \u03bb 3 in Eq. (12). As shown in Table 7, we only observe marginal performance differences, e.g., ADE 6 fluctuates within 1 cm. \n\n\nComputational performance\n\nAs reported in Table 8, LAformer has 2,645K parameters, similar to HiVT-128 but larger than LTP and DenseTNT. Its inference time for a scenario with an average of 12 agents is ca. 115 ms, which is not a main strength compared to HiVT. But this inference speed is comparable to LTP and faster than DenseTNT and PGP, making LAformer close to real-time use cases at 10 Hz.  Figure 3 presents qualitative results of LAFormer compared to runner-up models on the Argoverse 1 validation set 2 . To ensure a fair comparison, we use the publicly trained HiVT [52] model to replicate the results reported in their paper. As DenseTNT [15] does not provide a trained model, we retrain the offline model with optimization (DenseTNT w/100ms opt.) from scratch to achieve similar performance to that reported in their paper. All the models produce reasonable multimodal predictions for the target agent in various traffic scenarios at intersections, such as turning right 1 and left 2 , 4 , or driving straight with acceleration 3 . However, LAformer generates more accurate predictions in the right-turn scenario 1 and the acceleration scenario 3 , while other models tend to predict decelerating or turning modes. Furthermore, when the temporally dense lane-aware module is deactivated (w/o D vs. w/o S2), LAformer generates less diverse predictions  in the lateral directions. However, the complete model with the second-stage refinement module shown in the rightmost column maintains good prediction diversity and accuracy. Figure 4 presents more qualitative results of LAformer on nuScenes. It not only generates accurate prediction in straightforward driving but also at complicated intersections, for example, continuing driving forward, turning left or right. The multimodal predictions aligned with lane segments implies the agent's motion uncertain at intersections.\n\n\nQualitative results\n\n\nConclusion\n\nThe paper presents LAformer, an end-to-end attentionbased trajectory prediction model that takes observed trajectories and an HD map as input and outputs a set of multimodal predicted trajectories. A Transformer-based temporally dense lane-aware module and a second-stage motion refinement module are used to improve prediction accuracy. LAformer outperforms other models on both Argoverse 1 and nuScenes motion forecasting benchmarks, demonstrating a superior generalized performance. Moreover, extensive ablation and sensitivity studies verify the efficacy of the lane-aware and motion refinement modules.\n\nFigure 1 .\n1A temporally dense lane-aware estimation module estimates the step-wise likelihood of lane segments aligning with motion dynamics, e.g., selecting the most scene-compliant lane segments at each future time step to facilitate the decoding process.\n\nFigure 2 .\n2The LAformer framework takes vectorized trajectories and HD map lane segments as input. The agent motion and scene encodings are represented as hi and cj, respectively, and are later fused by the attention-based global interaction graph. The decoder then takes the target agent's past trajectory hi, the updated motion information aligned with the candidate lane information hi,att from the laneaware estimation module, and a random latent variable z as inputs, and predicts the future trajectory\u0176i. The refinement module further reduces the predicted offset \u2206Y to improve prediction accuracy.\n\n\nLaplace(\u00b5, b), where\u03c0 m denotes the probability of each mode indexed by m among the M predicted modes and M m=1\u03c0 m = 1. Here, \u00b5 and b represent the location and scale parameters of each Laplace component. We use an MLP to predict\u03c0 m , a GRU to recover the time dimension t f of the predictions, and two side-by-side MLPs to predict \u00b5 and b.\n\n\n1 validation set with 39,472 sequences. The Baseline model predicts future trajectories only conditioned on the observed trajectories of the target and its neighboring agents, with the second refinement module (S2) and lane-aware estimation module removed. LAformer (Spa.) only estimates the likelihood of goal position aligning with the lane information, similar to the spatially dense models. In contrast, LAformer (Tem.) estimates the likelihood of the position at each time step aligning with the temporally dense lane information. LAformer (Full) is the complete proposed model.\n\nFigure 3 .\n3Qualitative comparison of models in complex scenarios, with each row representing a unique intersection scenario and each column representing results predicted by the same model. Models include: LAformer w/o D (proposed model without temporally dense lane-aware module), LAformer w/o S2 (proposed model with temporally dense lane-aware module but without second-stage motion refinement), and LAformer (complete model) in the rightmost column.\n\nFigure 4 .\n4The qualitative results of LAformer on nuScenes. Predicted trajectories are presented in red color and the corresponding ground truth trajectories are presented in green color.\n\n\nTable 5. Ablation study on the latent variable z.z \nADE 6 FDE 6 \n\n-\n0.692 1.035 \n\u221a \n0.690 1.032 \n\n\n\n\nTable 6. The number of top lane segments that impacts the prediction performance. Left: first stage, Right: second stage.k ADE 6 FDE 6 \n\n1 \n0.68 \n1.00 \n2 \n0.66 \n0.95 \n3 \n0.65 \n0.95 \n4 \n0.66 \n0.96 \n\nk ADE 6 FDE 6 \n\n2 \n0.64 \n0.92 \n3 \n0.64 \n0.93 \n\n\n\n\n\u03bb 1 ADE 6 FDE 6Table 7. The number of top lane segments that impacts the prediction performance. Left: first stage, Right: second stage.8 \n0.70 \n1.05 \n9 \n0.70 \n1.02 \n10 \n0.69 \n1.01 \n11 \n0.70 \n1.05 \n12 \n0.70 \n1.05 \n\n\u03bb 2 ADE 6 FDE 6 \n\n1 \n0.68 \n1.01 \n5 \n0.68 \n1.00 \n10 \n0.69 \n1.02 \n\n\u03bb 3 ADE 6 FDE 6 \n\n1 \n0.68 \n1.00 \n2 \n0.68 \n1.00 \n10 \n0.68 \n1.01 \n\n\nMore details about the implementation and training scheme can be found in the supplementary material.\nMore qualitative results are presented in the supplementary material.\n\nSocial lstm: Human trajectory prediction in crowded spaces. Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, Silvio Savarese, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAlexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So- cial lstm: Human trajectory prediction in crowded spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 961-971, 2016. 2\n\nnuscenes: A multimodal dataset for autonomous driving. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition56Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi- ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi- modal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621-11631, 2020. 2, 5, 6\n\nMultipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. Yuning Chai, Benjamin Sapp, Mayank Bansal, Dragomir Anguelov, PMLRProceedings of the Conference on Robot Learning. the Conference on Robot Learning1006Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajec- tory hypotheses for behavior prediction. In Proceedings of the Conference on Robot Learning, volume 100 of Proceed- ings of Machine Learning Research, pages 86-99. PMLR, 30 Oct-01 Nov 2020. 2, 3, 6\n\nArgoverse: 3d tracking and forecasting with rich maps. Ming-Fang Chang, John W Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, James Hays, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition56Ming-Fang Chang, John W Lambert, Patsorn Sangkloy, Jag- jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James Hays. Argo- verse: 3d tracking and forecasting with rich maps. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 2, 5, 6\n\nScept: Scene-consistent, policy-based trajectory predictions for planning. Yuxiao Chen, Boris Ivanovic, Marco Pavone, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition23Yuxiao Chen, Boris Ivanovic, and Marco Pavone. Scept: Scene-consistent, policy-based trajectory predictions for planning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17103- 17112, 2022. 2, 3\n\nHierarchical latent structure for multi-modal vehicle trajectory forecasting. Dooseop Choi, Kyoungwook Min, European Conference on Computer Vision. Springer2022Dooseop Choi and KyoungWook Min. Hierarchical latent structure for multi-modal vehicle trajectory forecasting. In European Conference on Computer Vision, pages 129-145. Springer, 2022. 2\n\nConvolutional social pooling for vehicle trajectory prediction. Nachiket Deo, M Mohan, Trivedi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition Workshops23Nachiket Deo and Mohan M Trivedi. Convolutional social pooling for vehicle trajectory prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1468-1476, 2018. 2, 3\n\nNachiket Deo, M Mohan, Trivedi, arXiv:2001.00735Trajectory forecasts in unknown environments conditioned on grid-based plans. arXiv preprintNachiket Deo and Mohan M Trivedi. Trajectory forecasts in unknown environments conditioned on grid-based plans. arXiv preprint arXiv:2001.00735, 2020. 6\n\nMultimodal trajectory prediction conditioned on lane-graph traversals. Nachiket Deo, Eric Wolff, Oscar Beijbom, PMLRConference on Robot Learning. 67Nachiket Deo, Eric Wolff, and Oscar Beijbom. Multimodal trajectory prediction conditioned on lane-graph traversals. In Conference on Robot Learning, pages 203-212. PMLR, 2022. 1, 2, 3, 5, 6, 7\n\nVectornet: Encoding hd maps and agent dynamics from vectorized representation. Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, Cordelia Schmid, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition13Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, and Cordelia Schmid. Vectornet: Encoding hd maps and agent dynamics from vectorized rep- resentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11525- 11533, 2020. 1, 2, 3\n\nThomas: Trajectory heatmap output with learned multi-agent sampling. Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, Fabien Moutarde, International Conference on Learning Representations. 26Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bog- dan Stanciulescu, and Fabien Moutarde. Thomas: Trajectory heatmap output with learned multi-agent sampling. In Inter- national Conference on Learning Representations, 2021. 2, 3, 6\n\nGohome: Graphoriented heatmap output for future motion estimation. Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, Fabien Moutarde, 2022 International Conference on Robotics and Automation. 36Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bog- dan Stanciulescu, and Fabien Moutarde. Gohome: Graph- oriented heatmap output for future motion estimation. In 2022 International Conference on Robotics and Automation, pages 9107-9114. IEEE, 2022. 2, 3, 6\n\nLatent variable sequential set transformers for joint multi-agent motion prediction. Roger Girgis, Florian Golemo, Felipe Codevilla, Martin Weiss, Jim Aldon, D&apos; Souza, Samira Ebrahimi Kahou, Felix Heide, Christopher Pal, International Conference on Learning Representations. Roger Girgis, Florian Golemo, Felipe Codevilla, Martin Weiss, Jim Aldon D'Souza, Samira Ebrahimi Kahou, Felix Heide, and Christopher Pal. Latent variable sequential set transformers for joint multi-agent motion prediction. In In- ternational Conference on Learning Representations, 2021. 6\n\n. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Generative adversarial networks. Communications of the ACM. 6311Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Commu- nications of the ACM, 63(11):139-144, 2020. 2\n\nDensetnt: End-to-end trajectory prediction from dense goal sets. Junru Gu, Chen Sun, Hang Zhao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision67Junru Gu, Chen Sun, and Hang Zhao. Densetnt: End-to-end trajectory prediction from dense goal sets. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 15303-15312, 2021. 2, 3, 6, 7\n\nSocial gan: Socially acceptable trajectories with generative adversarial networks. Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, Alexandre Alahi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionAgrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajec- tories with generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 2255-2264, 2018. 2\n\nSocial force model for pedestrian dynamics. Dirk Helbing, Peter Molnar, Physical review E. 5154282Dirk Helbing and Peter Molnar. Social force model for pedestrian dynamics. Physical review E, 51(5):4282, 1995. 2\n\nMulti-modal motion prediction with transformer-based neural network for autonomous driving. Zhiyu Huang, Xiaoyu Mo, Chen Lv, 2022 International Conference on Robotics and Automation. IEEEZhiyu Huang, Xiaoyu Mo, and Chen Lv. Multi-modal mo- tion prediction with transformer-based neural network for autonomous driving. In 2022 International Conference on Robotics and Automation, pages 2605-2611. IEEE, 2022. 6\n\nWhat-if motion prediction for autonomous driving. Siddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew Hartnett, Deva Ramanan, arXiv:2008.10587arXiv preprintSiddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew Hartnett, and Deva Ramanan. What-if motion prediction for autonomous driving. arXiv preprint arXiv:2008.10587, 2020. 6\n\nLapred: Lane-aware prediction of multimodal future trajectories of dynamic agents. Byeoungdo Kim, Seokhwan Seong Hyeon Park, Elbek Lee, Dongsuk Khoshimjonov, Junsoo Kum, Jeong Soo Kim, Jun Won Kim, Choi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition16ByeoungDo Kim, Seong Hyeon Park, Seokhwan Lee, Elbek Khoshimjonov, Dongsuk Kum, Junsoo Kim, Jeong Soo Kim, and Jun Won Choi. Lapred: Lane-aware prediction of multi- modal future trajectories of dynamic agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 14636-14645, 2021. 1, 6\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, International Conference on Learning Representations. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015. 5\n\nSemi-supervised learning with deep generative models. Shakir Durk P Kingma, Danilo Mohamed, Max Jimenez Rezende, Welling, Advances in Neural Information Processing Systems. 27Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep gen- erative models. Advances in Neural Information Processing Systems, 27, 2014. 2\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, Inernational Conference on Learning Representations. Diederik P Kingma and Max Welling. Auto-encoding vari- ational bayes. In Inernational Conference on Learning Rep- resentations, 2014. 2\n\nDesire: Distant future prediction in dynamic scenes with interacting agents. Namhoon Lee, Wongun Choi, Paul Vernaza, B Christopher, Choy, H S Philip, Manmohan Torr, Chandraker, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition23Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr, and Manmohan Chandraker. Desire: Distant future prediction in dynamic scenes with interact- ing agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 336-345, 2017. 2, 3\n\nLearning lane graph representations for motion forecasting. Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, Raquel Urtasun, European Conference on Computer Vision. Springer36Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel Urtasun. Learning lane graph representa- tions for motion forecasting. In European Conference on Computer Vision, pages 541-556. Springer, 2020. 3, 6\n\nMultimodal motion prediction with stacked transformers. Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, Bolei Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition26Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, and Bolei Zhou. Multimodal motion prediction with stacked transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7577- 7586, 2021. 2, 6\n\nProbabilistic multi-modal trajectory prediction with lane attention for autonomous vehicles. Chenxu Luo, Lin Sun, Dariush Dabiri, Alan Yuille, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEChenxu Luo, Lin Sun, Dariush Dabiri, and Alan Yuille. Probabilistic multi-modal trajectory prediction with lane at- tention for autonomous vehicles. In 2020 IEEE/RSJ Interna- tional Conference on Intelligent Robots and Systems (IROS), pages 2370-2376. IEEE, 2020. 6\n\nOvercoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction. Osama Makansi, Eddy Ilg, Ozgun Cicek, Thomas Brox, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition25Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming limitations of mixture density networks: A sam- pling and fitting framework for multimodal future prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7144-7153, 2019. 2, 5\n\nMulti-head attention with joint agent-map representation for trajectory prediction in autonomous driving. Kaouther Messaoud, Nachiket Deo, M Mohan, Fawzi Trivedi, Nashashibi, arXiv:2005.02545arXiv preprintKaouther Messaoud, Nachiket Deo, Mohan M Trivedi, and Fawzi Nashashibi. Multi-head attention with joint agent-map representation for trajectory prediction in autonomous driv- ing. arXiv preprint arXiv:2005.02545, 2020. 6\n\nDivide-and-conquer for lane-aware diverse trajectory prediction. Sriram Narayanan, Ramin Moslemi, Francesco Pittaluga, Buyu Liu, Manmohan Chandraker, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSriram Narayanan, Ramin Moslemi, Francesco Pittaluga, Buyu Liu, and Manmohan Chandraker. Divide-and-conquer for lane-aware diverse trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 15799-15808, 2021. 6\n\nScene transformer: A unified architecture for predicting future trajectories of multiple agents. Jiquan Ngiam, Vijay Vasudevan, Benjamin Caine, Zhengdong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, J David, Ben Weiss, Zhifeng Sapp, Jonathon Chen, Shlens, International Conference on Learning Representations. 36Jiquan Ngiam, Vijay Vasudevan, Benjamin Caine, Zheng- dong Zhang, Hao-Tien Lewis Chiang, Jeffrey Ling, Re- becca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venu- gopal, David J Weiss, Ben Sapp, Zhifeng Chen, and Jonathon Shlens. Scene transformer: A unified architecture for pre- dicting future trajectories of multiple agents. In International Conference on Learning Representations, 2022. 3, 6\n\nLeveraging future relationship reasoning for vehicle trajectory prediction. Daehee Park, Hobin Ryu, Yunseo Yang, Jegyeong Cho, Jiwon Kim, Kuk-Jin Yoon, International Conference on Learning Representations. 36Daehee Park, Hobin Ryu, Yunseo Yang, Jegyeong Cho, Ji- won Kim, and Kuk-Jin Yoon. Leveraging future relationship reasoning for vehicle trajectory prediction. In International Conference on Learning Representations, 2023. 3, 6\n\nYou'll never walk alone: Modeling social behavior for multi-target tracking. Stefano Pellegrini, Andreas Ess, Konrad Schindler, Luc Van Gool, 2009 IEEE 12th International Conference on Computer Vision. Stefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc Van Gool. You'll never walk alone: Modeling social be- havior for multi-target tracking. In 2009 IEEE 12th Inter- national Conference on Computer Vision, pages 261-268. IEEE, 2009. 2\n\nCovernet: Multimodal behavior prediction using trajectory sets. Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton, Oscar Beijbom, Eric M Wolff, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition16Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton, Oscar Beijbom, and Eric M Wolff. Covernet: Multimodal behavior prediction using trajectory sets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14074-14083, 2020. 1, 3, 6\n\nVariational inference with normalizing flows. Danilo Rezende, Shakir Mohamed, PMLRInternational Conference on Machine Learning. Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Ma- chine Learning, pages 1530-1538. PMLR, 2015. 2\n\nPrecog: Prediction conditioned on goals in visual multi-agent settings. Nicholas Rhinehart, Rowan Mcallister, Kris Kitani, Sergey Levine, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision23Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction conditioned on goals in visual multi-agent settings. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2821- 2830, 2019. 2, 3\n\nOn gans and gmms. Eitan Richardson, Yair Weiss, Advances in Neural Information Processing Systems. 31Eitan Richardson and Yair Weiss. On gans and gmms. In Advances in Neural Information Processing Systems, vol- ume 31, 2018. 2\n\nSophie: An attentive gan for predicting paths compliant to social and physical constraints. Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid Rezatofighi, Silvio Savarese, Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition. the IEEE/CVF conference on Computer Vision and Pattern Recognition23Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid Rezatofighi, and Silvio Savarese. Sophie: An attentive gan for predicting paths compliant to social and physical constraints. In Proceedings of the IEEE/CVF con- ference on Computer Vision and Pattern Recognition, pages 1349-1358, 2019. 2, 3\n\nTrajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data. Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, Marco Pavone, European Conference on Computer Vision. Springer6Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-feasible trajec- tory forecasting with heterogeneous data. In European Con- ference on Computer Vision, pages 683-700. Springer, 2020. 1, 2, 3, 6\n\nPip: Planninginformed trajectory prediction for autonomous driving. Haoran Song, Wenchao Ding, Yuxuan Chen, Shaojie Shen, Michael Yu Wang, Qifeng Chen, European Conference on Computer Vision. SpringerHaoran Song, Wenchao Ding, Yuxuan Chen, Shaojie Shen, Michael Yu Wang, and Qifeng Chen. Pip: Planning- informed trajectory prediction for autonomous driving. In European Conference on Computer Vision, pages 598-614. Springer, 2020. 3\n\nEfficient information fusion and trajectory aggregation for behavior prediction. Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, S Khaled, Nigamaa Refaat, Andre Nayakanti, Kan Cornman, Bertrand Chen, Chi Pang Douillard, Dragomir Lam, Anguelov, 2022 International Conference on Robotics and Automation. 36Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivas- tava, Khaled S Refaat, Nigamaa Nayakanti, Andre Cornman, Kan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir Anguelov, et al. Multipath++: Efficient information fu- sion and trajectory aggregation for behavior prediction. In 2022 International Conference on Robotics and Automation, pages 7814-7821. IEEE, 2022. 2, 3, 6\n\nStepwise goal-driven networks for trajectory prediction. Chuhua Wang, Yuchen Wang, Mingze Xu, David J Crandall, IEEE Robotics and Automation Letters. 72Chuhua Wang, Yuchen Wang, Mingze Xu, and David J Cran- dall. Stepwise goal-driven networks for trajectory predic- tion. IEEE Robotics and Automation Letters, 7(2):2716- 2723, 2022. 6\n\nLtp: Lane-based trajectory prediction for autonomous driving. Jingke Wang, Tengju Ye, Ziqing Gu, Junbo Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition67Jingke Wang, Tengju Ye, Ziqing Gu, and Junbo Chen. Ltp: Lane-based trajectory prediction for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 17134-17142, 2022. 2, 3, 6, 7\n\nSemi-supervised classification with graph convolutional networks. Max Welling, N Thomas, Kipf, International Conference on Learning Representations. Max Welling and Thomas N Kipf. Semi-supervised classi- fication with graph convolutional networks. In International Conference on Learning Representations, 2017. 2\n\nTpcn: Temporal point cloud networks for motion forecasting. Maosheng Ye, Tongyi Cao, Qifeng Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionMaosheng Ye, Tongyi Cao, and Qifeng Chen. Tpcn: Tempo- ral point cloud networks for motion forecasting. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11318-11327, 2021. 6\n\nSpatio-temporal graph transformer networks for pedestrian trajectory prediction. Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, Shuai Yi, European Conference on Computer Vision. SpringerCunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi. Spatio-temporal graph transformer networks for pedestrian trajectory prediction. In European Conference on Computer Vision, pages 507-523. Springer, 2020. 2\n\nAgentformer: Agent-aware transformers for socio-temporal multi-agent forecasting. Ye Yuan, Xinshuo Weng, Yanglan Ou, Kris M Kitani, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision6Ye Yuan, Xinshuo Weng, Yanglan Ou, and Kris M Kitani. Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9813- 9823, 2021. 1, 2, 3, 6\n\nDistributed representations for graph-centric motion forecasting. Wenyuan Zeng, Ming Liang, Renjie Liao, Raquel Urtasun Lanercnn, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE26Wenyuan Zeng, Ming Liang, Renjie Liao, and Raquel Urta- sun. Lanercnn: Distributed representations for graph-centric motion forecasting. In 2021 IEEE/RSJ International Con- ference on Intelligent Robots and Systems, pages 532-539. IEEE, 2021. 1, 2, 6\n\nSr-lstm: State refinement for lstm towards pedestrian trajectory prediction. Pu Zhang, Wanli Ouyang, Pengfei Zhang, Jianru Xue, Nanning Zheng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPu Zhang, Wanli Ouyang, Pengfei Zhang, Jianru Xue, and Nanning Zheng. Sr-lstm: State refinement for lstm to- wards pedestrian trajectory prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12085-12094, 2019. 2\n\nTnt: Target-driven trajectory prediction. Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, Conference on Robot Learning. 6PMLR, 2021. 2, 3Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning Chai, Cordelia Schmid, et al. Tnt: Target-driven trajectory pre- diction. In Conference on Robot Learning, pages 895-904. PMLR, 2021. 2, 3, 6\n\nAware of the history: Trajectory forecasting with the local behavior data. Yiqi Zhong, Zhenyang Ni, Siheng Chen, Ulrich Neumann, European Conference on Computer Vision. Springer36Yiqi Zhong, Zhenyang Ni, Siheng Chen, and Ulrich Neu- mann. Aware of the history: Trajectory forecasting with the local behavior data. In European Conference on Computer Vision, pages 393-409. Springer, 2022. 3, 6\n\nHivt: Hierarchical vector transformer for multi-agent motion prediction. Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, Kejie Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition67Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, and Ke- jie Lu. Hivt: Hierarchical vector transformer for multi-agent motion prediction. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 8823-8833, 2022. 2, 3, 5, 6, 7\n", "annotations": {"author": "[{\"end\":154,\"start\":92},{\"end\":209,\"start\":155},{\"end\":234,\"start\":210},{\"end\":267,\"start\":235},{\"end\":299,\"start\":268},{\"end\":333,\"start\":300},{\"end\":378,\"start\":334},{\"end\":420,\"start\":379}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":101},{\"end\":164,\"start\":159},{\"end\":218,\"start\":214},{\"end\":251,\"start\":244},{\"end\":279,\"start\":277},{\"end\":313,\"start\":309},{\"end\":347,\"start\":341},{\"end\":396,\"start\":392}]", "author_first_name": "[{\"end\":100,\"start\":92},{\"end\":158,\"start\":155},{\"end\":213,\"start\":210},{\"end\":243,\"start\":235},{\"end\":276,\"start\":268},{\"end\":308,\"start\":300},{\"end\":340,\"start\":334},{\"end\":386,\"start\":379},{\"end\":391,\"start\":387}]", "author_affiliation": "[{\"end\":134,\"start\":106},{\"end\":153,\"start\":136},{\"end\":208,\"start\":187},{\"end\":233,\"start\":220},{\"end\":266,\"start\":253},{\"end\":298,\"start\":281},{\"end\":332,\"start\":315},{\"end\":377,\"start\":349},{\"end\":419,\"start\":398}]", "title": "[{\"end\":89,\"start\":1},{\"end\":509,\"start\":421}]", "venue": null, "abstract": "[{\"end\":1716,\"start\":511}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2569,\"start\":2565},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2584,\"start\":2580},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2606,\"start\":2602},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3117,\"start\":3113},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3454,\"start\":3450},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3457,\"start\":3454},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3490,\"start\":3487},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3691,\"start\":3687},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3694,\"start\":3691},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3732,\"start\":3728},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3735,\"start\":3732},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3738,\"start\":3735},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3777,\"start\":3774},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4424,\"start\":4420},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5349,\"start\":5345},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5352,\"start\":5349},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5355,\"start\":5352},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5357,\"start\":5355},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6203,\"start\":6200},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6206,\"start\":6203},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7346,\"start\":7343},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7363,\"start\":7360},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7636,\"start\":7632},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7639,\"start\":7636},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7676,\"start\":7673},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7679,\"start\":7676},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7682,\"start\":7679},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7685,\"start\":7682},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7688,\"start\":7685},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7691,\"start\":7688},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7820,\"start\":7817},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7822,\"start\":7820},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7825,\"start\":7822},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7847,\"start\":7843},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7850,\"start\":7847},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7853,\"start\":7850},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7900,\"start\":7896},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7931,\"start\":7927},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7934,\"start\":7931},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7936,\"start\":7934},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8441,\"start\":8437},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8478,\"start\":8474},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8503,\"start\":8499},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8519,\"start\":8515},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8588,\"start\":8584},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8591,\"start\":8588},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8594,\"start\":8591},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8596,\"start\":8594},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8910,\"start\":8906},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9110,\"start\":9106},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9113,\"start\":9110},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9115,\"start\":9113},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9366,\"start\":9362},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9369,\"start\":9366},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9481,\"start\":9477},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9817,\"start\":9813},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9820,\"start\":9817},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9875,\"start\":9871},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9878,\"start\":9875},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9881,\"start\":9878},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10265,\"start\":10261},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10507,\"start\":10503},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10510,\"start\":10507},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10513,\"start\":10510},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10515,\"start\":10513},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10518,\"start\":10515},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10520,\"start\":10518},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10523,\"start\":10520},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10748,\"start\":10745},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10751,\"start\":10748},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10881,\"start\":10877},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10884,\"start\":10881},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10887,\"start\":10884},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10890,\"start\":10887},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10893,\"start\":10890},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11100,\"start\":11097},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11103,\"start\":11100},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11124,\"start\":11120},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12122,\"start\":12118},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12433,\"start\":12429},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12436,\"start\":12433},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12439,\"start\":12436},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12442,\"start\":12439},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12444,\"start\":12442},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18280,\"start\":18276},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":18283,\"start\":18280},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18285,\"start\":18283},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":18661,\"start\":18657},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19684,\"start\":19680},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20430,\"start\":20427},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20450,\"start\":20447},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22149,\"start\":22145},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":22873,\"start\":22869},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23057,\"start\":23053},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23455,\"start\":23451},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23458,\"start\":23455},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23470,\"start\":23467},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23709,\"start\":23705},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23723,\"start\":23719},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23737,\"start\":23733},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23779,\"start\":23775},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26351,\"start\":26347},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":27044,\"start\":27040},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27117,\"start\":27113}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29255,\"start\":28996},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29862,\"start\":29256},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30205,\"start\":29863},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30791,\"start\":30206},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31247,\"start\":30792},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31437,\"start\":31248},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31538,\"start\":31438},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31786,\"start\":31539},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32134,\"start\":31787}]", "paragraph": "[{\"end\":2250,\"start\":1732},{\"end\":3347,\"start\":2252},{\"end\":4793,\"start\":3349},{\"end\":5879,\"start\":4795},{\"end\":6596,\"start\":5881},{\"end\":6646,\"start\":6598},{\"end\":6947,\"start\":6648},{\"end\":7249,\"start\":6949},{\"end\":7504,\"start\":7251},{\"end\":8120,\"start\":7521},{\"end\":9543,\"start\":8122},{\"end\":11825,\"start\":9545},{\"end\":12359,\"start\":11827},{\"end\":13196,\"start\":12392},{\"end\":13828,\"start\":13364},{\"end\":14118,\"start\":13981},{\"end\":14545,\"start\":14120},{\"end\":15120,\"start\":14581},{\"end\":15370,\"start\":15255},{\"end\":15936,\"start\":15529},{\"end\":16176,\"start\":15938},{\"end\":16281,\"start\":16205},{\"end\":16391,\"start\":16359},{\"end\":17126,\"start\":16393},{\"end\":17551,\"start\":17128},{\"end\":18304,\"start\":17622},{\"end\":18575,\"start\":18365},{\"end\":18767,\"start\":18609},{\"end\":18861,\"start\":18807},{\"end\":19182,\"start\":18883},{\"end\":19578,\"start\":19209},{\"end\":19910,\"start\":19626},{\"end\":20125,\"start\":19957},{\"end\":20252,\"start\":20170},{\"end\":21093,\"start\":20289},{\"end\":21324,\"start\":21095},{\"end\":21690,\"start\":21326},{\"end\":22287,\"start\":21692},{\"end\":22584,\"start\":22327},{\"end\":22813,\"start\":22586},{\"end\":22983,\"start\":22815},{\"end\":24261,\"start\":22985},{\"end\":25187,\"start\":24280},{\"end\":25562,\"start\":25256},{\"end\":25870,\"start\":25611},{\"end\":26460,\"start\":25872},{\"end\":28351,\"start\":26490},{\"end\":28995,\"start\":28388}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13363,\"start\":13197},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13980,\"start\":13829},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15254,\"start\":15121},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15428,\"start\":15371},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15487,\"start\":15428},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16204,\"start\":16177},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16358,\"start\":16282},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17588,\"start\":17552},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18364,\"start\":18305},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18608,\"start\":18576},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18806,\"start\":18768},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19208,\"start\":19183},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19625,\"start\":19579},{\"attributes\":{\"id\":\"formula_13\"},\"end\":19956,\"start\":19911},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20169,\"start\":20126},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25255,\"start\":25188}]", "table_ref": "[{\"end\":24260,\"start\":24253},{\"end\":24399,\"start\":24392},{\"end\":24964,\"start\":24957},{\"end\":25076,\"start\":25069},{\"end\":25466,\"start\":25459},{\"end\":25869,\"start\":25855},{\"end\":25908,\"start\":25901},{\"end\":26372,\"start\":26365},{\"end\":26512,\"start\":26505}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1730,\"start\":1718},{\"attributes\":{\"n\":\"2.\"},\"end\":7519,\"start\":7507},{\"attributes\":{\"n\":\"3.\"},\"end\":12368,\"start\":12362},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12390,\"start\":12371},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14579,\"start\":14548},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15527,\"start\":15489},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17620,\"start\":17590},{\"attributes\":{\"n\":\"3.5.\"},\"end\":18881,\"start\":18864},{\"attributes\":{\"n\":\"4.\"},\"end\":20266,\"start\":20255},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20287,\"start\":20269},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22325,\"start\":22290},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24278,\"start\":24264},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25609,\"start\":25565},{\"attributes\":{\"n\":\"4.5.\"},\"end\":26488,\"start\":26463},{\"attributes\":{\"n\":\"4.6.\"},\"end\":28373,\"start\":28354},{\"attributes\":{\"n\":\"5.\"},\"end\":28386,\"start\":28376},{\"end\":29007,\"start\":28997},{\"end\":29267,\"start\":29257},{\"end\":30803,\"start\":30793},{\"end\":31259,\"start\":31249}]", "table": "[{\"end\":31538,\"start\":31489},{\"end\":31786,\"start\":31662},{\"end\":32134,\"start\":31925}]", "figure_caption": "[{\"end\":29255,\"start\":29009},{\"end\":29862,\"start\":29269},{\"end\":30205,\"start\":29865},{\"end\":30791,\"start\":30208},{\"end\":31247,\"start\":30805},{\"end\":31437,\"start\":31261},{\"end\":31489,\"start\":31440},{\"end\":31662,\"start\":31541},{\"end\":31925,\"start\":31789}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4911,\"start\":4905},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14319,\"start\":14313},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26869,\"start\":26861},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28011,\"start\":28003}]", "bib_author_first_name": "[{\"end\":32377,\"start\":32368},{\"end\":32393,\"start\":32385},{\"end\":32407,\"start\":32400},{\"end\":32429,\"start\":32420},{\"end\":32443,\"start\":32441},{\"end\":32459,\"start\":32453},{\"end\":32958,\"start\":32952},{\"end\":32972,\"start\":32967},{\"end\":32986,\"start\":32982},{\"end\":32988,\"start\":32987},{\"end\":33002,\"start\":32995},{\"end\":33015,\"start\":33009},{\"end\":33020,\"start\":33016},{\"end\":33033,\"start\":33028},{\"end\":33043,\"start\":33038},{\"end\":33056,\"start\":33054},{\"end\":33071,\"start\":33062},{\"end\":33085,\"start\":33080},{\"end\":33666,\"start\":33660},{\"end\":33681,\"start\":33673},{\"end\":33694,\"start\":33688},{\"end\":33711,\"start\":33703},{\"end\":34189,\"start\":34180},{\"end\":34201,\"start\":34197},{\"end\":34203,\"start\":34202},{\"end\":34220,\"start\":34213},{\"end\":34238,\"start\":34231},{\"end\":34254,\"start\":34246},{\"end\":34266,\"start\":34260},{\"end\":34279,\"start\":34277},{\"end\":34291,\"start\":34286},{\"end\":34303,\"start\":34298},{\"end\":34315,\"start\":34311},{\"end\":34330,\"start\":34325},{\"end\":34892,\"start\":34886},{\"end\":34904,\"start\":34899},{\"end\":34920,\"start\":34915},{\"end\":35405,\"start\":35398},{\"end\":35422,\"start\":35412},{\"end\":35740,\"start\":35732},{\"end\":35747,\"start\":35746},{\"end\":36154,\"start\":36146},{\"end\":36161,\"start\":36160},{\"end\":36519,\"start\":36511},{\"end\":36529,\"start\":36525},{\"end\":36542,\"start\":36537},{\"end\":36867,\"start\":36861},{\"end\":36877,\"start\":36873},{\"end\":36887,\"start\":36883},{\"end\":36896,\"start\":36894},{\"end\":36911,\"start\":36903},{\"end\":36930,\"start\":36922},{\"end\":36943,\"start\":36935},{\"end\":37475,\"start\":37469},{\"end\":37491,\"start\":37484},{\"end\":37509,\"start\":37502},{\"end\":37526,\"start\":37520},{\"end\":37547,\"start\":37541},{\"end\":37925,\"start\":37919},{\"end\":37941,\"start\":37934},{\"end\":37959,\"start\":37952},{\"end\":37976,\"start\":37970},{\"end\":37997,\"start\":37991},{\"end\":38421,\"start\":38416},{\"end\":38437,\"start\":38430},{\"end\":38452,\"start\":38446},{\"end\":38470,\"start\":38464},{\"end\":38481,\"start\":38478},{\"end\":38496,\"start\":38489},{\"end\":38510,\"start\":38504},{\"end\":38519,\"start\":38511},{\"end\":38532,\"start\":38527},{\"end\":38551,\"start\":38540},{\"end\":38907,\"start\":38904},{\"end\":38924,\"start\":38920},{\"end\":38945,\"start\":38940},{\"end\":38957,\"start\":38953},{\"end\":38967,\"start\":38962},{\"end\":38989,\"start\":38982},{\"end\":39002,\"start\":38997},{\"end\":39020,\"start\":39014},{\"end\":39379,\"start\":39374},{\"end\":39388,\"start\":39384},{\"end\":39398,\"start\":39394},{\"end\":39839,\"start\":39834},{\"end\":39853,\"start\":39847},{\"end\":39865,\"start\":39863},{\"end\":39881,\"start\":39875},{\"end\":39901,\"start\":39892},{\"end\":40384,\"start\":40380},{\"end\":40399,\"start\":40394},{\"end\":40646,\"start\":40641},{\"end\":40660,\"start\":40654},{\"end\":40669,\"start\":40665},{\"end\":41018,\"start\":41010},{\"end\":41038,\"start\":41031},{\"end\":41050,\"start\":41043},{\"end\":41064,\"start\":41058},{\"end\":41079,\"start\":41075},{\"end\":41386,\"start\":41377},{\"end\":41400,\"start\":41392},{\"end\":41424,\"start\":41419},{\"end\":41437,\"start\":41430},{\"end\":41458,\"start\":41452},{\"end\":41469,\"start\":41464},{\"end\":41473,\"start\":41470},{\"end\":41486,\"start\":41479},{\"end\":42023,\"start\":42022},{\"end\":42039,\"start\":42034},{\"end\":42308,\"start\":42302},{\"end\":42330,\"start\":42324},{\"end\":42343,\"start\":42340},{\"end\":42649,\"start\":42648},{\"end\":42663,\"start\":42660},{\"end\":42955,\"start\":42948},{\"end\":42967,\"start\":42961},{\"end\":42978,\"start\":42974},{\"end\":42989,\"start\":42988},{\"end\":43010,\"start\":43009},{\"end\":43012,\"start\":43011},{\"end\":43029,\"start\":43021},{\"end\":43556,\"start\":43552},{\"end\":43567,\"start\":43564},{\"end\":43577,\"start\":43574},{\"end\":43585,\"start\":43582},{\"end\":43598,\"start\":43592},{\"end\":43609,\"start\":43605},{\"end\":43622,\"start\":43616},{\"end\":43971,\"start\":43964},{\"end\":43985,\"start\":43977},{\"end\":44000,\"start\":43993},{\"end\":44014,\"start\":44007},{\"end\":44027,\"start\":44022},{\"end\":44530,\"start\":44524},{\"end\":44539,\"start\":44536},{\"end\":44552,\"start\":44545},{\"end\":44565,\"start\":44561},{\"end\":45050,\"start\":45045},{\"end\":45064,\"start\":45060},{\"end\":45075,\"start\":45070},{\"end\":45089,\"start\":45083},{\"end\":45652,\"start\":45644},{\"end\":45671,\"start\":45663},{\"end\":45678,\"start\":45677},{\"end\":45691,\"start\":45686},{\"end\":46036,\"start\":46030},{\"end\":46053,\"start\":46048},{\"end\":46072,\"start\":46063},{\"end\":46088,\"start\":46084},{\"end\":46102,\"start\":46094},{\"end\":46637,\"start\":46631},{\"end\":46650,\"start\":46645},{\"end\":46670,\"start\":46662},{\"end\":46687,\"start\":46678},{\"end\":46709,\"start\":46695},{\"end\":46725,\"start\":46718},{\"end\":46739,\"start\":46732},{\"end\":46753,\"start\":46749},{\"end\":46768,\"start\":46762},{\"end\":46780,\"start\":46774},{\"end\":46793,\"start\":46792},{\"end\":46804,\"start\":46801},{\"end\":46819,\"start\":46812},{\"end\":46834,\"start\":46826},{\"end\":47382,\"start\":47376},{\"end\":47394,\"start\":47389},{\"end\":47406,\"start\":47400},{\"end\":47421,\"start\":47413},{\"end\":47432,\"start\":47427},{\"end\":47445,\"start\":47438},{\"end\":47819,\"start\":47812},{\"end\":47839,\"start\":47832},{\"end\":47851,\"start\":47845},{\"end\":47866,\"start\":47863},{\"end\":48248,\"start\":48244},{\"end\":48265,\"start\":48260},{\"end\":48272,\"start\":48266},{\"end\":48288,\"start\":48282},{\"end\":48290,\"start\":48289},{\"end\":48305,\"start\":48300},{\"end\":48319,\"start\":48315},{\"end\":48321,\"start\":48320},{\"end\":48805,\"start\":48799},{\"end\":48821,\"start\":48815},{\"end\":49125,\"start\":49117},{\"end\":49142,\"start\":49137},{\"end\":49159,\"start\":49155},{\"end\":49174,\"start\":49168},{\"end\":49585,\"start\":49580},{\"end\":49602,\"start\":49598},{\"end\":49886,\"start\":49882},{\"end\":49904,\"start\":49898},{\"end\":49918,\"start\":49915},{\"end\":49937,\"start\":49930},{\"end\":49951,\"start\":49946},{\"end\":49971,\"start\":49965},{\"end\":50532,\"start\":50529},{\"end\":50548,\"start\":50543},{\"end\":50567,\"start\":50559},{\"end\":50586,\"start\":50581},{\"end\":50961,\"start\":50955},{\"end\":50975,\"start\":50968},{\"end\":50988,\"start\":50982},{\"end\":51002,\"start\":50995},{\"end\":51016,\"start\":51009},{\"end\":51019,\"start\":51017},{\"end\":51032,\"start\":51026},{\"end\":51415,\"start\":51403},{\"end\":51434,\"start\":51429},{\"end\":51449,\"start\":51442},{\"end\":51463,\"start\":51462},{\"end\":51479,\"start\":51472},{\"end\":51493,\"start\":51488},{\"end\":51508,\"start\":51505},{\"end\":51526,\"start\":51518},{\"end\":51536,\"start\":51533},{\"end\":51541,\"start\":51537},{\"end\":51561,\"start\":51553},{\"end\":52077,\"start\":52071},{\"end\":52090,\"start\":52084},{\"end\":52103,\"start\":52097},{\"end\":52115,\"start\":52108},{\"end\":52418,\"start\":52412},{\"end\":52431,\"start\":52425},{\"end\":52442,\"start\":52436},{\"end\":52452,\"start\":52447},{\"end\":52917,\"start\":52914},{\"end\":52928,\"start\":52927},{\"end\":53230,\"start\":53222},{\"end\":53241,\"start\":53235},{\"end\":53253,\"start\":53247},{\"end\":53716,\"start\":53710},{\"end\":53725,\"start\":53721},{\"end\":53736,\"start\":53730},{\"end\":53747,\"start\":53742},{\"end\":53759,\"start\":53754},{\"end\":54112,\"start\":54110},{\"end\":54126,\"start\":54119},{\"end\":54140,\"start\":54133},{\"end\":54149,\"start\":54145},{\"end\":54151,\"start\":54150},{\"end\":54611,\"start\":54604},{\"end\":54622,\"start\":54618},{\"end\":54636,\"start\":54630},{\"end\":54649,\"start\":54643},{\"end\":54657,\"start\":54650},{\"end\":55079,\"start\":55077},{\"end\":55092,\"start\":55087},{\"end\":55108,\"start\":55101},{\"end\":55122,\"start\":55116},{\"end\":55135,\"start\":55128},{\"end\":55601,\"start\":55597},{\"end\":55614,\"start\":55608},{\"end\":55624,\"start\":55620},{\"end\":55634,\"start\":55630},{\"end\":55643,\"start\":55640},{\"end\":55662,\"start\":55650},{\"end\":55679,\"start\":55676},{\"end\":55688,\"start\":55686},{\"end\":55701,\"start\":55695},{\"end\":55716,\"start\":55708},{\"end\":56099,\"start\":56095},{\"end\":56115,\"start\":56107},{\"end\":56126,\"start\":56120},{\"end\":56139,\"start\":56133},{\"end\":56493,\"start\":56487},{\"end\":56505,\"start\":56500},{\"end\":56518,\"start\":56510},{\"end\":56528,\"start\":56525},{\"end\":56538,\"start\":56533}]", "bib_author_last_name": "[{\"end\":32383,\"start\":32378},{\"end\":32398,\"start\":32394},{\"end\":32418,\"start\":32408},{\"end\":32439,\"start\":32430},{\"end\":32451,\"start\":32444},{\"end\":32468,\"start\":32460},{\"end\":32965,\"start\":32959},{\"end\":32980,\"start\":32973},{\"end\":32993,\"start\":32989},{\"end\":33007,\"start\":33003},{\"end\":33026,\"start\":33021},{\"end\":33036,\"start\":33034},{\"end\":33052,\"start\":33044},{\"end\":33060,\"start\":33057},{\"end\":33078,\"start\":33072},{\"end\":33093,\"start\":33086},{\"end\":33671,\"start\":33667},{\"end\":33686,\"start\":33682},{\"end\":33701,\"start\":33695},{\"end\":33720,\"start\":33712},{\"end\":34195,\"start\":34190},{\"end\":34211,\"start\":34204},{\"end\":34229,\"start\":34221},{\"end\":34244,\"start\":34239},{\"end\":34258,\"start\":34255},{\"end\":34275,\"start\":34267},{\"end\":34284,\"start\":34280},{\"end\":34296,\"start\":34292},{\"end\":34309,\"start\":34304},{\"end\":34323,\"start\":34316},{\"end\":34335,\"start\":34331},{\"end\":34897,\"start\":34893},{\"end\":34913,\"start\":34905},{\"end\":34927,\"start\":34921},{\"end\":35410,\"start\":35406},{\"end\":35426,\"start\":35423},{\"end\":35744,\"start\":35741},{\"end\":35753,\"start\":35748},{\"end\":35762,\"start\":35755},{\"end\":36158,\"start\":36155},{\"end\":36167,\"start\":36162},{\"end\":36176,\"start\":36169},{\"end\":36523,\"start\":36520},{\"end\":36535,\"start\":36530},{\"end\":36550,\"start\":36543},{\"end\":36871,\"start\":36868},{\"end\":36881,\"start\":36878},{\"end\":36892,\"start\":36888},{\"end\":36901,\"start\":36897},{\"end\":36920,\"start\":36912},{\"end\":36933,\"start\":36931},{\"end\":36950,\"start\":36944},{\"end\":37482,\"start\":37476},{\"end\":37500,\"start\":37492},{\"end\":37518,\"start\":37510},{\"end\":37539,\"start\":37527},{\"end\":37556,\"start\":37548},{\"end\":37932,\"start\":37926},{\"end\":37950,\"start\":37942},{\"end\":37968,\"start\":37960},{\"end\":37989,\"start\":37977},{\"end\":38006,\"start\":37998},{\"end\":38428,\"start\":38422},{\"end\":38444,\"start\":38438},{\"end\":38462,\"start\":38453},{\"end\":38476,\"start\":38471},{\"end\":38487,\"start\":38482},{\"end\":38502,\"start\":38497},{\"end\":38525,\"start\":38520},{\"end\":38538,\"start\":38533},{\"end\":38555,\"start\":38552},{\"end\":38918,\"start\":38908},{\"end\":38938,\"start\":38925},{\"end\":38951,\"start\":38946},{\"end\":38960,\"start\":38958},{\"end\":38980,\"start\":38968},{\"end\":38995,\"start\":38990},{\"end\":39012,\"start\":39003},{\"end\":39027,\"start\":39021},{\"end\":39382,\"start\":39380},{\"end\":39392,\"start\":39389},{\"end\":39403,\"start\":39399},{\"end\":39845,\"start\":39840},{\"end\":39861,\"start\":39854},{\"end\":39873,\"start\":39866},{\"end\":39890,\"start\":39882},{\"end\":39907,\"start\":39902},{\"end\":40392,\"start\":40385},{\"end\":40406,\"start\":40400},{\"end\":40652,\"start\":40647},{\"end\":40663,\"start\":40661},{\"end\":40672,\"start\":40670},{\"end\":41029,\"start\":41019},{\"end\":41041,\"start\":41039},{\"end\":41056,\"start\":41051},{\"end\":41073,\"start\":41065},{\"end\":41087,\"start\":41080},{\"end\":41390,\"start\":41387},{\"end\":41417,\"start\":41401},{\"end\":41428,\"start\":41425},{\"end\":41450,\"start\":41438},{\"end\":41462,\"start\":41459},{\"end\":41477,\"start\":41474},{\"end\":41490,\"start\":41487},{\"end\":41496,\"start\":41492},{\"end\":42032,\"start\":42024},{\"end\":42046,\"start\":42040},{\"end\":42050,\"start\":42048},{\"end\":42322,\"start\":42309},{\"end\":42338,\"start\":42331},{\"end\":42359,\"start\":42344},{\"end\":42368,\"start\":42361},{\"end\":42658,\"start\":42650},{\"end\":42670,\"start\":42664},{\"end\":42679,\"start\":42672},{\"end\":42959,\"start\":42956},{\"end\":42972,\"start\":42968},{\"end\":42986,\"start\":42979},{\"end\":43001,\"start\":42990},{\"end\":43007,\"start\":43003},{\"end\":43019,\"start\":43013},{\"end\":43034,\"start\":43030},{\"end\":43046,\"start\":43036},{\"end\":43562,\"start\":43557},{\"end\":43572,\"start\":43568},{\"end\":43580,\"start\":43578},{\"end\":43590,\"start\":43586},{\"end\":43603,\"start\":43599},{\"end\":43614,\"start\":43610},{\"end\":43630,\"start\":43623},{\"end\":43975,\"start\":43972},{\"end\":43991,\"start\":43986},{\"end\":44005,\"start\":44001},{\"end\":44020,\"start\":44015},{\"end\":44032,\"start\":44028},{\"end\":44534,\"start\":44531},{\"end\":44543,\"start\":44540},{\"end\":44559,\"start\":44553},{\"end\":44572,\"start\":44566},{\"end\":45058,\"start\":45051},{\"end\":45068,\"start\":45065},{\"end\":45081,\"start\":45076},{\"end\":45094,\"start\":45090},{\"end\":45661,\"start\":45653},{\"end\":45675,\"start\":45672},{\"end\":45684,\"start\":45679},{\"end\":45699,\"start\":45692},{\"end\":45711,\"start\":45701},{\"end\":46046,\"start\":46037},{\"end\":46061,\"start\":46054},{\"end\":46082,\"start\":46073},{\"end\":46092,\"start\":46089},{\"end\":46113,\"start\":46103},{\"end\":46643,\"start\":46638},{\"end\":46660,\"start\":46651},{\"end\":46676,\"start\":46671},{\"end\":46693,\"start\":46688},{\"end\":46716,\"start\":46710},{\"end\":46730,\"start\":46726},{\"end\":46747,\"start\":46740},{\"end\":46760,\"start\":46754},{\"end\":46772,\"start\":46769},{\"end\":46790,\"start\":46781},{\"end\":46799,\"start\":46794},{\"end\":46810,\"start\":46805},{\"end\":46824,\"start\":46820},{\"end\":46839,\"start\":46835},{\"end\":46847,\"start\":46841},{\"end\":47387,\"start\":47383},{\"end\":47398,\"start\":47395},{\"end\":47411,\"start\":47407},{\"end\":47425,\"start\":47422},{\"end\":47436,\"start\":47433},{\"end\":47450,\"start\":47446},{\"end\":47830,\"start\":47820},{\"end\":47843,\"start\":47840},{\"end\":47861,\"start\":47852},{\"end\":47875,\"start\":47867},{\"end\":48258,\"start\":48249},{\"end\":48280,\"start\":48273},{\"end\":48298,\"start\":48291},{\"end\":48313,\"start\":48306},{\"end\":48327,\"start\":48322},{\"end\":48813,\"start\":48806},{\"end\":48829,\"start\":48822},{\"end\":49135,\"start\":49126},{\"end\":49153,\"start\":49143},{\"end\":49166,\"start\":49160},{\"end\":49181,\"start\":49175},{\"end\":49596,\"start\":49586},{\"end\":49608,\"start\":49603},{\"end\":49896,\"start\":49887},{\"end\":49913,\"start\":49905},{\"end\":49928,\"start\":49919},{\"end\":49944,\"start\":49938},{\"end\":49963,\"start\":49952},{\"end\":49980,\"start\":49972},{\"end\":50541,\"start\":50533},{\"end\":50557,\"start\":50549},{\"end\":50579,\"start\":50568},{\"end\":50593,\"start\":50587},{\"end\":50966,\"start\":50962},{\"end\":50980,\"start\":50976},{\"end\":50993,\"start\":50989},{\"end\":51007,\"start\":51003},{\"end\":51024,\"start\":51020},{\"end\":51037,\"start\":51033},{\"end\":51427,\"start\":51416},{\"end\":51440,\"start\":51435},{\"end\":51460,\"start\":51450},{\"end\":51470,\"start\":51464},{\"end\":51486,\"start\":51480},{\"end\":51503,\"start\":51494},{\"end\":51516,\"start\":51509},{\"end\":51531,\"start\":51527},{\"end\":51551,\"start\":51542},{\"end\":51565,\"start\":51562},{\"end\":51575,\"start\":51567},{\"end\":52082,\"start\":52078},{\"end\":52095,\"start\":52091},{\"end\":52106,\"start\":52104},{\"end\":52124,\"start\":52116},{\"end\":52423,\"start\":52419},{\"end\":52434,\"start\":52432},{\"end\":52445,\"start\":52443},{\"end\":52457,\"start\":52453},{\"end\":52925,\"start\":52918},{\"end\":52935,\"start\":52929},{\"end\":52941,\"start\":52937},{\"end\":53233,\"start\":53231},{\"end\":53245,\"start\":53242},{\"end\":53258,\"start\":53254},{\"end\":53719,\"start\":53717},{\"end\":53728,\"start\":53726},{\"end\":53740,\"start\":53737},{\"end\":53752,\"start\":53748},{\"end\":53762,\"start\":53760},{\"end\":54117,\"start\":54113},{\"end\":54131,\"start\":54127},{\"end\":54143,\"start\":54141},{\"end\":54158,\"start\":54152},{\"end\":54616,\"start\":54612},{\"end\":54628,\"start\":54623},{\"end\":54641,\"start\":54637},{\"end\":54666,\"start\":54658},{\"end\":55085,\"start\":55080},{\"end\":55099,\"start\":55093},{\"end\":55114,\"start\":55109},{\"end\":55126,\"start\":55123},{\"end\":55141,\"start\":55136},{\"end\":55606,\"start\":55602},{\"end\":55618,\"start\":55615},{\"end\":55628,\"start\":55625},{\"end\":55638,\"start\":55635},{\"end\":55648,\"start\":55644},{\"end\":55674,\"start\":55663},{\"end\":55684,\"start\":55680},{\"end\":55693,\"start\":55689},{\"end\":55706,\"start\":55702},{\"end\":55723,\"start\":55717},{\"end\":56105,\"start\":56100},{\"end\":56118,\"start\":56116},{\"end\":56131,\"start\":56127},{\"end\":56147,\"start\":56140},{\"end\":56498,\"start\":56494},{\"end\":56508,\"start\":56506},{\"end\":56523,\"start\":56519},{\"end\":56531,\"start\":56529},{\"end\":56541,\"start\":56539}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":9854676},\"end\":32895,\"start\":32308},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":85517967},\"end\":33570,\"start\":32897},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b2\",\"matched_paper_id\":204509212},\"end\":34123,\"start\":33572},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":198162846},\"end\":34809,\"start\":34125},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":250073075},\"end\":35318,\"start\":34811},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":250425801},\"end\":35666,\"start\":35320},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":21712887},\"end\":36144,\"start\":35668},{\"attributes\":{\"doi\":\"arXiv:2001.00735\",\"id\":\"b7\"},\"end\":36438,\"start\":36146},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b8\",\"matched_paper_id\":235669892},\"end\":36780,\"start\":36440},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":218581144},\"end\":37398,\"start\":36782},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":238744427},\"end\":37850,\"start\":37400},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":237263205},\"end\":38329,\"start\":37852},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":246824069},\"end\":38900,\"start\":38331},{\"attributes\":{\"id\":\"b13\"},\"end\":39307,\"start\":38902},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":237267284},\"end\":39749,\"start\":39309},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4461350},\"end\":40334,\"start\":39751},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5771125},\"end\":40547,\"start\":40336},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":237503713},\"end\":40958,\"start\":40549},{\"attributes\":{\"doi\":\"arXiv:2008.10587\",\"id\":\"b18\"},\"end\":41292,\"start\":40960},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":232478351},\"end\":41976,\"start\":41294},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6628106},\"end\":42246,\"start\":41978},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6377199},\"end\":42613,\"start\":42248},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":216078090},\"end\":42869,\"start\":42615},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8394584},\"end\":43490,\"start\":42871},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":220848238},\"end\":43906,\"start\":43492},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":232307829},\"end\":44429,\"start\":43908},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":220363790},\"end\":44924,\"start\":44431},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":182952939},\"end\":45536,\"start\":44926},{\"attributes\":{\"doi\":\"arXiv:2005.02545\",\"id\":\"b28\"},\"end\":45963,\"start\":45538},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":233289806},\"end\":46532,\"start\":45965},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":251648671},\"end\":47298,\"start\":46534},{\"attributes\":{\"id\":\"b31\"},\"end\":47733,\"start\":47300},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7065547},\"end\":48178,\"start\":47735},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":208267649},\"end\":48751,\"start\":48180},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b34\",\"matched_paper_id\":12554042},\"end\":49043,\"start\":48753},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":145047911},\"end\":49560,\"start\":49045},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":44110630},\"end\":49788,\"start\":49562},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":46938830},\"end\":50444,\"start\":49790},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":214802528},\"end\":50885,\"start\":50446},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":214641168},\"end\":51320,\"start\":50887},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":244730940},\"end\":52012,\"start\":51322},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":232380341},\"end\":52348,\"start\":52014},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":250616038},\"end\":52846,\"start\":52350},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":3144218},\"end\":53160,\"start\":52848},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":232110870},\"end\":53627,\"start\":53162},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":218673505},\"end\":54026,\"start\":53629},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":232352504},\"end\":54536,\"start\":54028},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":231632882},\"end\":54998,\"start\":54538},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":71148538},\"end\":55553,\"start\":55000},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":221172860},\"end\":56018,\"start\":55555},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":250698890},\"end\":56412,\"start\":56020},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":250602571},\"end\":56953,\"start\":56414}]", "bib_title": "[{\"end\":32366,\"start\":32308},{\"end\":32950,\"start\":32897},{\"end\":33658,\"start\":33572},{\"end\":34178,\"start\":34125},{\"end\":34884,\"start\":34811},{\"end\":35396,\"start\":35320},{\"end\":35730,\"start\":35668},{\"end\":36509,\"start\":36440},{\"end\":36859,\"start\":36782},{\"end\":37467,\"start\":37400},{\"end\":37917,\"start\":37852},{\"end\":38414,\"start\":38331},{\"end\":39372,\"start\":39309},{\"end\":39832,\"start\":39751},{\"end\":40378,\"start\":40336},{\"end\":40639,\"start\":40549},{\"end\":41375,\"start\":41294},{\"end\":42020,\"start\":41978},{\"end\":42300,\"start\":42248},{\"end\":42646,\"start\":42615},{\"end\":42946,\"start\":42871},{\"end\":43550,\"start\":43492},{\"end\":43962,\"start\":43908},{\"end\":44522,\"start\":44431},{\"end\":45043,\"start\":44926},{\"end\":46028,\"start\":45965},{\"end\":46629,\"start\":46534},{\"end\":47374,\"start\":47300},{\"end\":47810,\"start\":47735},{\"end\":48242,\"start\":48180},{\"end\":48797,\"start\":48753},{\"end\":49115,\"start\":49045},{\"end\":49578,\"start\":49562},{\"end\":49880,\"start\":49790},{\"end\":50527,\"start\":50446},{\"end\":50953,\"start\":50887},{\"end\":51401,\"start\":51322},{\"end\":52069,\"start\":52014},{\"end\":52410,\"start\":52350},{\"end\":52912,\"start\":52848},{\"end\":53220,\"start\":53162},{\"end\":53708,\"start\":53629},{\"end\":54108,\"start\":54028},{\"end\":54602,\"start\":54538},{\"end\":55075,\"start\":55000},{\"end\":55595,\"start\":55555},{\"end\":56093,\"start\":56020},{\"end\":56485,\"start\":56414}]", "bib_author": "[{\"end\":32385,\"start\":32368},{\"end\":32400,\"start\":32385},{\"end\":32420,\"start\":32400},{\"end\":32441,\"start\":32420},{\"end\":32453,\"start\":32441},{\"end\":32470,\"start\":32453},{\"end\":32967,\"start\":32952},{\"end\":32982,\"start\":32967},{\"end\":32995,\"start\":32982},{\"end\":33009,\"start\":32995},{\"end\":33028,\"start\":33009},{\"end\":33038,\"start\":33028},{\"end\":33054,\"start\":33038},{\"end\":33062,\"start\":33054},{\"end\":33080,\"start\":33062},{\"end\":33095,\"start\":33080},{\"end\":33673,\"start\":33660},{\"end\":33688,\"start\":33673},{\"end\":33703,\"start\":33688},{\"end\":33722,\"start\":33703},{\"end\":34197,\"start\":34180},{\"end\":34213,\"start\":34197},{\"end\":34231,\"start\":34213},{\"end\":34246,\"start\":34231},{\"end\":34260,\"start\":34246},{\"end\":34277,\"start\":34260},{\"end\":34286,\"start\":34277},{\"end\":34298,\"start\":34286},{\"end\":34311,\"start\":34298},{\"end\":34325,\"start\":34311},{\"end\":34337,\"start\":34325},{\"end\":34899,\"start\":34886},{\"end\":34915,\"start\":34899},{\"end\":34929,\"start\":34915},{\"end\":35412,\"start\":35398},{\"end\":35428,\"start\":35412},{\"end\":35746,\"start\":35732},{\"end\":35755,\"start\":35746},{\"end\":35764,\"start\":35755},{\"end\":36160,\"start\":36146},{\"end\":36169,\"start\":36160},{\"end\":36178,\"start\":36169},{\"end\":36525,\"start\":36511},{\"end\":36537,\"start\":36525},{\"end\":36552,\"start\":36537},{\"end\":36873,\"start\":36861},{\"end\":36883,\"start\":36873},{\"end\":36894,\"start\":36883},{\"end\":36903,\"start\":36894},{\"end\":36922,\"start\":36903},{\"end\":36935,\"start\":36922},{\"end\":36952,\"start\":36935},{\"end\":37484,\"start\":37469},{\"end\":37502,\"start\":37484},{\"end\":37520,\"start\":37502},{\"end\":37541,\"start\":37520},{\"end\":37558,\"start\":37541},{\"end\":37934,\"start\":37919},{\"end\":37952,\"start\":37934},{\"end\":37970,\"start\":37952},{\"end\":37991,\"start\":37970},{\"end\":38008,\"start\":37991},{\"end\":38430,\"start\":38416},{\"end\":38446,\"start\":38430},{\"end\":38464,\"start\":38446},{\"end\":38478,\"start\":38464},{\"end\":38489,\"start\":38478},{\"end\":38504,\"start\":38489},{\"end\":38527,\"start\":38504},{\"end\":38540,\"start\":38527},{\"end\":38557,\"start\":38540},{\"end\":38920,\"start\":38904},{\"end\":38940,\"start\":38920},{\"end\":38953,\"start\":38940},{\"end\":38962,\"start\":38953},{\"end\":38982,\"start\":38962},{\"end\":38997,\"start\":38982},{\"end\":39014,\"start\":38997},{\"end\":39029,\"start\":39014},{\"end\":39384,\"start\":39374},{\"end\":39394,\"start\":39384},{\"end\":39405,\"start\":39394},{\"end\":39847,\"start\":39834},{\"end\":39863,\"start\":39847},{\"end\":39875,\"start\":39863},{\"end\":39892,\"start\":39875},{\"end\":39909,\"start\":39892},{\"end\":40394,\"start\":40380},{\"end\":40408,\"start\":40394},{\"end\":40654,\"start\":40641},{\"end\":40665,\"start\":40654},{\"end\":40674,\"start\":40665},{\"end\":41031,\"start\":41010},{\"end\":41043,\"start\":41031},{\"end\":41058,\"start\":41043},{\"end\":41075,\"start\":41058},{\"end\":41089,\"start\":41075},{\"end\":41392,\"start\":41377},{\"end\":41419,\"start\":41392},{\"end\":41430,\"start\":41419},{\"end\":41452,\"start\":41430},{\"end\":41464,\"start\":41452},{\"end\":41479,\"start\":41464},{\"end\":41492,\"start\":41479},{\"end\":41498,\"start\":41492},{\"end\":42034,\"start\":42022},{\"end\":42048,\"start\":42034},{\"end\":42052,\"start\":42048},{\"end\":42324,\"start\":42302},{\"end\":42340,\"start\":42324},{\"end\":42361,\"start\":42340},{\"end\":42370,\"start\":42361},{\"end\":42660,\"start\":42648},{\"end\":42672,\"start\":42660},{\"end\":42681,\"start\":42672},{\"end\":42961,\"start\":42948},{\"end\":42974,\"start\":42961},{\"end\":42988,\"start\":42974},{\"end\":43003,\"start\":42988},{\"end\":43009,\"start\":43003},{\"end\":43021,\"start\":43009},{\"end\":43036,\"start\":43021},{\"end\":43048,\"start\":43036},{\"end\":43564,\"start\":43552},{\"end\":43574,\"start\":43564},{\"end\":43582,\"start\":43574},{\"end\":43592,\"start\":43582},{\"end\":43605,\"start\":43592},{\"end\":43616,\"start\":43605},{\"end\":43632,\"start\":43616},{\"end\":43977,\"start\":43964},{\"end\":43993,\"start\":43977},{\"end\":44007,\"start\":43993},{\"end\":44022,\"start\":44007},{\"end\":44034,\"start\":44022},{\"end\":44536,\"start\":44524},{\"end\":44545,\"start\":44536},{\"end\":44561,\"start\":44545},{\"end\":44574,\"start\":44561},{\"end\":45060,\"start\":45045},{\"end\":45070,\"start\":45060},{\"end\":45083,\"start\":45070},{\"end\":45096,\"start\":45083},{\"end\":45663,\"start\":45644},{\"end\":45677,\"start\":45663},{\"end\":45686,\"start\":45677},{\"end\":45701,\"start\":45686},{\"end\":45713,\"start\":45701},{\"end\":46048,\"start\":46030},{\"end\":46063,\"start\":46048},{\"end\":46084,\"start\":46063},{\"end\":46094,\"start\":46084},{\"end\":46115,\"start\":46094},{\"end\":46645,\"start\":46631},{\"end\":46662,\"start\":46645},{\"end\":46678,\"start\":46662},{\"end\":46695,\"start\":46678},{\"end\":46718,\"start\":46695},{\"end\":46732,\"start\":46718},{\"end\":46749,\"start\":46732},{\"end\":46762,\"start\":46749},{\"end\":46774,\"start\":46762},{\"end\":46792,\"start\":46774},{\"end\":46801,\"start\":46792},{\"end\":46812,\"start\":46801},{\"end\":46826,\"start\":46812},{\"end\":46841,\"start\":46826},{\"end\":46849,\"start\":46841},{\"end\":47389,\"start\":47376},{\"end\":47400,\"start\":47389},{\"end\":47413,\"start\":47400},{\"end\":47427,\"start\":47413},{\"end\":47438,\"start\":47427},{\"end\":47452,\"start\":47438},{\"end\":47832,\"start\":47812},{\"end\":47845,\"start\":47832},{\"end\":47863,\"start\":47845},{\"end\":47877,\"start\":47863},{\"end\":48260,\"start\":48244},{\"end\":48282,\"start\":48260},{\"end\":48300,\"start\":48282},{\"end\":48315,\"start\":48300},{\"end\":48329,\"start\":48315},{\"end\":48815,\"start\":48799},{\"end\":48831,\"start\":48815},{\"end\":49137,\"start\":49117},{\"end\":49155,\"start\":49137},{\"end\":49168,\"start\":49155},{\"end\":49183,\"start\":49168},{\"end\":49598,\"start\":49580},{\"end\":49610,\"start\":49598},{\"end\":49898,\"start\":49882},{\"end\":49915,\"start\":49898},{\"end\":49930,\"start\":49915},{\"end\":49946,\"start\":49930},{\"end\":49965,\"start\":49946},{\"end\":49982,\"start\":49965},{\"end\":50543,\"start\":50529},{\"end\":50559,\"start\":50543},{\"end\":50581,\"start\":50559},{\"end\":50595,\"start\":50581},{\"end\":50968,\"start\":50955},{\"end\":50982,\"start\":50968},{\"end\":50995,\"start\":50982},{\"end\":51009,\"start\":50995},{\"end\":51026,\"start\":51009},{\"end\":51039,\"start\":51026},{\"end\":51429,\"start\":51403},{\"end\":51442,\"start\":51429},{\"end\":51462,\"start\":51442},{\"end\":51472,\"start\":51462},{\"end\":51488,\"start\":51472},{\"end\":51505,\"start\":51488},{\"end\":51518,\"start\":51505},{\"end\":51533,\"start\":51518},{\"end\":51553,\"start\":51533},{\"end\":51567,\"start\":51553},{\"end\":51577,\"start\":51567},{\"end\":52084,\"start\":52071},{\"end\":52097,\"start\":52084},{\"end\":52108,\"start\":52097},{\"end\":52126,\"start\":52108},{\"end\":52425,\"start\":52412},{\"end\":52436,\"start\":52425},{\"end\":52447,\"start\":52436},{\"end\":52459,\"start\":52447},{\"end\":52927,\"start\":52914},{\"end\":52937,\"start\":52927},{\"end\":52943,\"start\":52937},{\"end\":53235,\"start\":53222},{\"end\":53247,\"start\":53235},{\"end\":53260,\"start\":53247},{\"end\":53721,\"start\":53710},{\"end\":53730,\"start\":53721},{\"end\":53742,\"start\":53730},{\"end\":53754,\"start\":53742},{\"end\":53764,\"start\":53754},{\"end\":54119,\"start\":54110},{\"end\":54133,\"start\":54119},{\"end\":54145,\"start\":54133},{\"end\":54160,\"start\":54145},{\"end\":54618,\"start\":54604},{\"end\":54630,\"start\":54618},{\"end\":54643,\"start\":54630},{\"end\":54668,\"start\":54643},{\"end\":55087,\"start\":55077},{\"end\":55101,\"start\":55087},{\"end\":55116,\"start\":55101},{\"end\":55128,\"start\":55116},{\"end\":55143,\"start\":55128},{\"end\":55608,\"start\":55597},{\"end\":55620,\"start\":55608},{\"end\":55630,\"start\":55620},{\"end\":55640,\"start\":55630},{\"end\":55650,\"start\":55640},{\"end\":55676,\"start\":55650},{\"end\":55686,\"start\":55676},{\"end\":55695,\"start\":55686},{\"end\":55708,\"start\":55695},{\"end\":55725,\"start\":55708},{\"end\":56107,\"start\":56095},{\"end\":56120,\"start\":56107},{\"end\":56133,\"start\":56120},{\"end\":56149,\"start\":56133},{\"end\":56500,\"start\":56487},{\"end\":56510,\"start\":56500},{\"end\":56525,\"start\":56510},{\"end\":56533,\"start\":56525},{\"end\":56543,\"start\":56533}]", "bib_venue": "[{\"end\":32619,\"start\":32553},{\"end\":33244,\"start\":33178},{\"end\":33807,\"start\":33775},{\"end\":34486,\"start\":34420},{\"end\":35078,\"start\":35012},{\"end\":35925,\"start\":35853},{\"end\":37101,\"start\":37035},{\"end\":39534,\"start\":39478},{\"end\":40058,\"start\":39992},{\"end\":41647,\"start\":41581},{\"end\":43197,\"start\":43131},{\"end\":44183,\"start\":44117},{\"end\":45245,\"start\":45179},{\"end\":46264,\"start\":46198},{\"end\":48478,\"start\":48412},{\"end\":49312,\"start\":49256},{\"end\":50131,\"start\":50065},{\"end\":52608,\"start\":52542},{\"end\":53409,\"start\":53343},{\"end\":54289,\"start\":54233},{\"end\":55292,\"start\":55226},{\"end\":56692,\"start\":56626},{\"end\":32551,\"start\":32470},{\"end\":33176,\"start\":33095},{\"end\":33773,\"start\":33726},{\"end\":34418,\"start\":34337},{\"end\":35010,\"start\":34929},{\"end\":35466,\"start\":35428},{\"end\":35851,\"start\":35764},{\"end\":36270,\"start\":36194},{\"end\":36584,\"start\":36556},{\"end\":37033,\"start\":36952},{\"end\":37610,\"start\":37558},{\"end\":38064,\"start\":38008},{\"end\":38609,\"start\":38557},{\"end\":39087,\"start\":39029},{\"end\":39476,\"start\":39405},{\"end\":39990,\"start\":39909},{\"end\":40425,\"start\":40408},{\"end\":40730,\"start\":40674},{\"end\":41008,\"start\":40960},{\"end\":41579,\"start\":41498},{\"end\":42104,\"start\":42052},{\"end\":42419,\"start\":42370},{\"end\":42732,\"start\":42681},{\"end\":43129,\"start\":43048},{\"end\":43670,\"start\":43632},{\"end\":44115,\"start\":44034},{\"end\":44653,\"start\":44574},{\"end\":45177,\"start\":45096},{\"end\":45642,\"start\":45538},{\"end\":46196,\"start\":46115},{\"end\":46901,\"start\":46849},{\"end\":47504,\"start\":47452},{\"end\":47935,\"start\":47877},{\"end\":48410,\"start\":48329},{\"end\":48879,\"start\":48835},{\"end\":49254,\"start\":49183},{\"end\":49659,\"start\":49610},{\"end\":50063,\"start\":49982},{\"end\":50633,\"start\":50595},{\"end\":51077,\"start\":51039},{\"end\":51633,\"start\":51577},{\"end\":52162,\"start\":52126},{\"end\":52540,\"start\":52459},{\"end\":52995,\"start\":52943},{\"end\":53341,\"start\":53260},{\"end\":53802,\"start\":53764},{\"end\":54231,\"start\":54160},{\"end\":54740,\"start\":54668},{\"end\":55224,\"start\":55143},{\"end\":55753,\"start\":55725},{\"end\":56187,\"start\":56149},{\"end\":56624,\"start\":56543}]"}}}, "year": 2023, "month": 12, "day": 17}