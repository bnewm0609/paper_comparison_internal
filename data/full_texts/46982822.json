{"id": 46982822, "updated": "2023-10-01 19:37:57.128", "metadata": {"title": "Exact Low Tubal Rank Tensor Recovery from Gaussian Measurements", "authors": "[{\"first\":\"Canyi\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Jiashi\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Zhouchen\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Shuicheng\",\"last\":\"Yan\",\"middle\":[]}]", "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence", "journal": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "The recent proposed Tensor Nuclear Norm (TNN) [Lu et al., 2016; 2018a] is an interesting convex penalty induced by the tensor SVD [Kilmer and Martin, 2011]. It plays a similar role as the matrix nuclear norm which is the convex surrogate of the matrix rank. Considering that the TNN based Tensor Robust PCA [Lu et al., 2018a] is an elegant extension of Robust PCA with a similar tight recovery bound, it is natural to solve other low rank tensor recovery problems extended from the matrix cases. However, the extensions and proofs are generally tedious. The general atomic norm provides a unified view of low-complexity structures induced norms, e.g., the $\\ell_1$-norm and nuclear norm. The sharp estimates of the required number of generic measurements for exact recovery based on the atomic norm are known in the literature. In this work, with a careful choice of the atomic set, we prove that TNN is a special atomic norm. Then by computing the Gaussian width of certain cone which is necessary for the sharp estimate, we achieve a simple bound for guaranteed low tubal rank tensor recovery from Gaussian measurements. Specifically, we show that by solving a TNN minimization problem, the underlying tensor of size $n_1\\times n_2\\times n_3$ with tubal rank $r$ can be exactly recovered when the given number of Gaussian measurements is $O(r(n_1+n_2-r)n_3)$. It is order optimal when comparing with the degrees of freedom $r(n_1+n_2-r)n_3$. Beyond the Gaussian mapping, we also give the recovery guarantee of tensor completion based on the uniform random mapping by TNN minimization. Numerical experiments verify our theoretical results.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1806.02511", "mag": "2963107235", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ijcai/LuFLY18", "doi": "10.24963/ijcai.2018/347"}}, "content": {"source": {"pdf_hash": "ef2f3d7041c1ec4caca8e598fc8df5b6cf9a63fc", "pdf_src": "Anansi", "pdf_uri": "[\"https://www.ijcai.org/proceedings/2018/0347.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://www.ijcai.org/proceedings/2018/0347.pdf", "status": "BRONZE"}}, "grobid": {"id": "d9224af806a8fa2ebe500f289e5654b801647b2d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ef2f3d7041c1ec4caca8e598fc8df5b6cf9a63fc.txt", "contents": "\nExact Low Tubal Rank Tensor Recovery from Gaussian Measurements\n\n\nCanyi Lu canyilu@gmail.com \nDepartment of Electrical and Computer Engineering\nCarnegie Mellon University\n\n\nJiashi Feng \nDepartment of Electrical and Computer Engineering\nNational University of Singapore\n\n\nZhouchen Lin zlin@pku.edu.cn \nSchool of EECS\nKey Laboratory of Machine Perception (MOE)\nPeking University\n\n\nCooperative Medianet Innovation Center\nShanghai Jiao Tong University\n\n\nShuicheng Yan \nDepartment of Electrical and Computer Engineering\nNational University of Singapore\n\n\n360 AI Institute\n\n\nExact Low Tubal Rank Tensor Recovery from Gaussian Measurements\n\nThe recent proposed Tensor Nuclear Norm (TNN)[Lu et al., 2016;2018a]is an interesting convex penalty induced by the tensor SVD [Kilmer and  Martin, 2011]. It plays a similar role as the matrix nuclear norm which is the convex surrogate of the matrix rank. Considering that the TNN based Tensor Robust PCA[Lu et al., 2018a]is an elegant extension of Robust PCA with a similar tight recovery bound, it is natural to solve other low rank tensor recovery problems extended from the matrix cases. However, the extensions and proofs are generally tedious. The general atomic norm provides a unified view of low-complexity structures induced norms, e.g., the 1 -norm and nuclear norm. The sharp estimates of the required number of generic measurements for exact recovery based on the atomic norm are known in the literature. In this work, with a careful choice of the atomic set, we prove that TNN is a special atomic norm. Then by computing the Gaussian width of certain cone which is necessary for the sharp estimate, we achieve a simple bound for guaranteed low tubal rank tensor recovery from Gaussian measurements. Specifically, we show that by solving a TNN minimization problem, the underlying tensor of size n 1 \u00d7 n 2 \u00d7 n 3 with tubal rank r can be exactly recovered when the given number of Gaussian measurements is O(r(n 1 + n 2 \u2212 r)n 3 ). It is order optimal when comparing with the degrees of freedom r(n 1 + n 2 \u2212 r)n 3 . Beyond the Gaussian mapping, we also give the recovery guarantee of tensor completion based on the uniform random mapping by TNN minimization. Numerical experiments verify our theoretical results.\n\nIntroduction\n\nMany engineering problems look for solutions to underdetermined systems of linear equations: a system is considered underdetermined if there are fewer equations than unknowns. * Corresponding author.\n\nSuppose we are given information about an object x 0 \u2208 R d of the form \u03a6x 0 \u2208 R m where \u03a6 is an m \u00d7 d matrix. We want the bound on the number of rows m to ensure that x 0 is the unique minimizer to the problem min x x A , s.t. \u03a6x 0 = \u03a6x.\n\n(1)\n\nHere \u00b7 A is a norm with some suitable properties which encourage solutions to conform to some notion of simplicity. For example, the compressed sensing problem aims to recover a sparse vector x 0 from (1) by taking \u00b7 A as the 1 -norm x 1 . We would like to know that how many measurements are required to recover an s-sparse x 0 . This of course depends on the kind of measurements. For instance, it is shown in [Cand\u00e8s et al., 2006] that 20s log d randomly selected Fourier coefficients are sufficient. If the Gaussian measurement map (\u03a6 has entries i.i.d. sampled from a Gaussian distribution with mean 0 and variance 1 m ) is used, 2s log d s + 5 4 s measurements are needed [Donoho and Tanner, 2009;Chandrasekaran et al., 2012]. Another interesting structured object is the low-rank matrix X 0 \u2208 R n1\u00d7n2 . In this case, the ith component of a linear operator is given by\n[\u03a6(X 0 )] i = \u03a6 i , X 0 , where \u03a6 i \u2208 R n1\u00d7n2\n. This includes the matrix completion problem [Cand\u00e8s and Recht, 2009] as a special case based on a proper choice of \u03a6 i . By taking \u00b7 A as the matrix nuclear norm X * , the convex program (1) recovers X 0 provided that the number of measurements is of the order \u00b5(X 0 )r(n 1 + n 2 \u2212 r) log 2 (n 1 + n 2 ), where r is the rank of X 0 and \u00b5(X 0 ) is the incoherence parameter [Cand\u00e8s and Recht, 2009;Chen, 2015]. Compared with the degrees of freedom r(n 1 + n 2 \u2212 r) of the rank-r matrix, such a rate is optimal (up to a logarithmic factor). If the Gaussian measurement map is used, about 3r(n 1 + n 2 \u2212 r) samples are sufficient for exact recovery [Recht et al., 2010]. Beyond the sparse vector and low-rank matrix, there have some other structured signals which can be recovered by (1). The work [Chandrasekaran et al., 2012] gives some more examples, presents a unified view of the convex programming to inverse problems and provides a simple framework to derive exact recovery bounds for a variety of simple models. Their considered models are formed as the sum of a few atoms from some elementary atomic sets. The convex programming formulation is based on minimizing the norm induced by the convex hull of the atomic set; this norm is referred to as the atomic norm (the 1 -norm and nuclear norm are special cases). By using the properties of the atomic norm, an analysis of the underlying convex geometry provides sharp estimates of the number of generic measurements required for exact recovery of models from partial information. A key step to estimate the required number of measurements is to compute the Gaussian width of the tangent cone associated with the atomic norm ball.\n\nThis work focuses the study on the low-rank tensor which is an interesting object structured that has many applications in signal processing. Recovering low-rank tensor is not easy since the tensor rank is not well defined. There have several tensor rank definitions, but each has its limitation. For example, the CP rank, defined as the smallest number of rank one tensor decomposition, is generally NP hard to compute. Also, its convex envelope is in general intractable. The tractable Tucker rank is more widely used. However, considering the low Tucker rank tensor recovery problem, the required number of measurements of existing convex model is much higher than the degrees of freedom [Mu et al., 2014]. This is different from the nuclear norm minimization for low-rank matrix recovery which has order optimal rate [Chen, 2015].\n\nIn this work, we first study the low tubal rank tensor recovery from Gaussian measurements. Tensor RPCA [Lu et al., 2016;2018a] studies the low tubal rank tensor recovery from sparse corruptions by Tensor Nuclear Norm (TNN) minimization. We show that TNN is a new instance of the atomic norm based on a proper choice of the atomic set. From the perspective of atomic norm minimization, we give the low tubal rank recovery guarantee from Gaussian measurements. Specifically, to recover a tensor of size n 1 \u00d7n 2 \u00d7n 3 with tubal rank r from Gaussian measurement by TNN minimization, the required number of measurements is O(r(n 1 + n 2 \u2212 r)n 3 ). It is order optimal when comparing with the degrees of freedom r(n 1 + n 2 \u2212 r)n 3 . Second, we study the tensor completion problem from uniform random sampling. We show that, to recover a tensor of tubal rank r, the sampling complexity is O(r min(n 1 , n 2 )n 3 log 2 (min(n 1 , n 2 )n 3 )), which is order optimal (up to a log factor). The same problem has been studied in [Zhang and Aeron, 2017] but its proofs have several errors.\n\n\nNotations and Preliminaries\n\nWe introduce some notations used in this paper. We denote tensors by boldface Euler script letters, e.g., A, matrices by boldface capital letters, e.g., A, vectors by boldface lowercase letters, e.g., a, and scalars by lowercase letters, e.g., a. We denote I n as the n \u00d7 n sized identity matrix. The field of real number and complex number are denoted as R and C, respectively. For a 3-way tensor A \u2208 C n1\u00d7n2\u00d7n3 , we denote its (i, j, k)-th entry as A ijk or a ijk and use the Matlab notation A(i, :, :), A(:, i, :) and A(:, :, i) to respectively denote the i-th horizontal, lateral and frontal slice. More often, the frontal slice A(:, :, i) is denoted compactly as A (i) . The tube is denoted as A(i, j, :). The inner product of A and B in C n1\u00d7n2 is defined as A, B = Tr(A * B), where A * denotes the conjugate transpose of A and Tr(\u00b7) denotes the matrix trace. The inner product of A and B in C n1\u00d7n2\u00d7n3 is defined as A, B = n3 i=1 A (i) , B (i) . For any A \u2208 C n1\u00d7n2\u00d7n3 , the complex conjugate of A is denoted as conj(A), which takes the complex conjugate of all entries of A. We denote t as the nearest integer less than or equal to t and t as the one greater than or equal to t. We denote the 1 -norm as A 1 = ijk |a ijk |, the infinity norm as A \u221e = max ijk |a ijk | and the Frobenius norm as A F = ijk |a ijk | 2 . The same norms are used for matrices and vectors. The spectral norm of a matrix A is denoted\nas A = max i \u03c3 i (A), where \u03c3 i (A)'s are the singular values of A. The matrix nuclear norm is A * = i \u03c3 i (A).\nFor A \u2208 R n1\u00d7n2\u00d7n3 , by using the Matlab command fft, we denote\u0100 \u2208 C n1\u00d7n2\u00d7n3 as the result of Fast Fourier Transformation (FFT) of A along the 3-rd dimension, i.e., A = fft(A, [ ], 3). In the same fashion, we can compute A from\u0100 using the inverse FFT, i.e., A = ifft(\u0100, [ ], 3) . In particular, we denote\u0100 as a block diagonal matrix with i-th block on the diagonal as the frontal slice\u0100 (i) of\u0100, i.e.,\nA = bdiag(\u0100) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0\u0100 (1)\u0100 (2) . . .\u0100 (n3) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb . The block circulant matrix of A is defined as bcirc(A) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 A (1) A (n3) \u00b7 \u00b7 \u00b7 A (2) A (2) A (1) \u00b7 \u00b7 \u00b7 A (3) . . . . . . . . . . . . A (n3) A (n3\u22121) \u00b7 \u00b7 \u00b7 A (1) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb .\nThe block circulant matrix can be block diagonalized, i.e.,\n(F n3 \u2297 I n1 ) \u00b7 bcirc(A) \u00b7 (F \u22121 n3 \u2297 I n2 ) =\u0100,\nwhere F n3 \u2208 C n3\u00d7n3 is the discrete Fourier transformation matrix, \u2297 denotes the Kronecker product. Note that (F n3 \u2297 I n1 )/ \u221a n 3 is orthogonal. We define the following operators\nunfold(A) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 A (1) A (2) . . . A (n3) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb , fold(unfold(A)) = A.\n\nDefinition 1. (t-product) [Kilmer and Martin, 2011] Let\nA \u2208 R n1\u00d7n2\u00d7n3 and B \u2208 R n2\u00d7l\u00d7n3 . Then the t-product A * B is defined to be a tensor C \u2208 R n1\u00d7l\u00d7n3 , C = A * B = fold(bcirc(A) \u00b7 unfold(B)).\nThe frontal slices of\u0100 has the following property\n\u0100 (1) \u2208 R n1\u00d7n2 , conj(\u0100 (i) ) =\u0100 (n3\u2212i+2) , i = 2, \u00b7 \u00b7 \u00b7 , n3+1 2 .\n(2)\n\nUsing the above property, the work [Lu et al., 2018a] proposes a more efficient way for computing t-product than the method in [Kilmer and Martin, 2011]. The conjugate transpose of a tensor A of size n 1 \u00d7 n 2 \u00d7 n 3 is the n 2 \u00d7 n 1 \u00d7 n 3 tensor A * obtained by conjugate transposing each of the frontal slice and then reversing the order of transposed frontal slices 2 through n 3 .\n\n\nDefinition 3. (Identity tensor) [Kilmer and Martin, 2011]\n\nThe identity tensor I \u2208 R n\u00d7n\u00d7n3 is the tensor whose first frontal slice is the n \u00d7 n identity matrix, and other frontal slices are all zeros. \nA = U * S * V * , where U \u2208 R n1\u00d7n1\u00d7n3 , V \u2208 R n2\u00d7n2\u00d7n3 are orthogonal, and S \u2208 R n1\u00d7n2\u00d7n3 is a f-diagonal tensor.\nTheorem 1 gives the t-SVD based on t-product. See Figure  1 for an illustration. Theorem 1 appears first in [Kilmer and Martin, 2011] but their proof is not rigorous since it cannot guarantee that U and V are real tensors. The work [Lu et al., 2018a] fixes this issue by using property (2), and further gives a more efficient way for computing t-SVD (see Algorithm 1). Algorithm 1 only needs to compute n3+1 2 matrix SVDs, while this number is n 3 by the method in [Kilmer and Martin, 2011]. The entries of the first frontal slice S(:, :, 1) are called as the singular values of the tensor A. The number of nonzero singular values is equivalent to the tensor tubal rank.\nDefinition 6. (Tensor tubal rank) [Lu et al., 2018a] For A \u2208 R n1\u00d7n2\u00d7n3 , the tensor tubal rank, denoted as rank t (A), is defined as the number of nonzero singular values of cS, where S is from the t-SVD of A = U * S * V * . We can write rank t (A) = #{i, S(i, i, 1) = 0} = #{i, S(i, i, :) = 0}.\nFor A \u2208 R n1\u00d7n2\u00d7n3 with tubal rank r, it has the skinny t-SVD, i.e., A = U * S * V * , where U \u2208 R n1\u00d7r\u00d7n3 , S \u2208 R r\u00d7r\u00d7n3 , and V \u2208 R n2\u00d7r\u00d7n3 , in which U * * U = I and V * * V = I. We use the skinny t-SVD throughout this paper.\n\nDefinition 7. (Tensor nuclear norm) [Lu et al., 2018a] Let A = U * S * V * be the t-SVD of A \u2208 R n1\u00d7n2\u00d7n3 . The tensor nuclear norm of A is defined as the sum of the tensor singular values, i.e.,\nA * = r i=1 S(i, i, 1), where r = rank t (A). Algorithm 1 T-SVD Input: A \u2208 R n1\u00d7n2\u00d7n3 . Output: T-SVD components U , S and V of A. 1. Compute\u0100 = fft(A, [ ], 3).\n\nCompute each frontal slice of\u016a ,S andV from\u0100 by\n\nfor\ni = 1, \u00b7 \u00b7 \u00b7 , n3+1 2 do [\u016a (i) ,S (i) ,V (i) ] = SVD(\u0100 (i) ); end for for i = n3+1 2 + 1, \u00b7 \u00b7 \u00b7 , n 3 d\u014d U (i) = conj(\u016a (n3\u2212i+2) ); S (i) =S (n3\u2212i+2) ; V (i) = conj(V (n3\u2212i+2) ); end for 3. Compute U = ifft(\u016a , [ ], 3), S = ifft(S, [ ], 3), and V = ifft(V, [ ], 3).\nThe above definition of TNN is defined based on t-SVD. It is equivalent to 1 n3 \u0100 * as given in [Lu et al., 2016]. Indeed,\nA * = r i=1 S(i, i, 1) = S, I = 1 n 3 S ,\u012a = 1 n 3 S ,\u012a = 1 n 3 n3 i=1 \u0100 (i) * = 1 n 3 \u0100 * .\nAbove the factor 1 n3 is from the property F n3 2 F = n 3 , where F n3 is the discrete Fourier transformation matrix. TNN is the dual norm of the tensor spectral norm, and vice versa. Definite the tensor average rank as rank a (A) = 1 n3 bcirc(A). Then the convex envelope of the tensor average rank is the tensor nuclear within the set {A| A \u2264 1}. It is worth mentioning that the above definition of tensor nuclear norm is different from the one in [Zhang and Aeron, 2017] due to the factor 1 n3 . This factor is crucial in theoretical analysis. Intuitively, it makes the model, theoretical proof and the way for optimization consistent with the matrix cases.\n\n\nTensor Nuclear Norm Is an Atomic Norm\n\nBased on the above tensor tubal rank, this work considers the following problem. Suppose that we have a linear map \u03a6 : R n1\u00d7n2\u00d7n3 \u2192 R m and the observations y = \u03a6(M) for M \u2208 R n1\u00d7n2\u00d7n3 which has tubal rank r. Our goal is to recover the underlying M from the observations y. This can be achieved by solving the following convex program\nX = arg min X X * , s.t. y = \u03a6(X ).(3)\nNow, how many measurements are required to guarantee the exact recovery (i.e.,X = M)? This problem is an extension of the low-rank matrix recovery problem [Recht et al., 2010].\n\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence  To answer the above question, we will use the unified theory in [Chandrasekaran et al., 2012] which provides sharp estimates of the number of measurements required for exact and robust recovery of models from Gaussian measurements. The key challenge is to reformulate TNN as a special case of the atomic norm and compute the Gaussian width. In this section, we will show that TNN is a special case of the atomic norm. Let A be a collection of atoms that is a compact subset of R p and conv(A) be its convex hull. The atomic norm induced by A is defined as [Chandrasekaran et al., 2012] x\nA = inf a\u2208A c a : x = a\u2208A c a a, c a \u2265 0, \u2200a \u2208 A .\nWe also need some other notations which will be used in the analysis. The support function of A is given as\nx * A = sup{ x, a : a \u2208 A}.\nIf \u00b7 A is a norm, the support function \u00b7 * A is the dual norm of this atomic norm.\n\nA convex set C is a cone if it is closed under positive linear combinations. The polar C * of a cone C is the cone\nC * = {x \u2208 R p : x, z \u2264 0, \u2200z \u2208 C}.\nThe tangent cone at nonzero x is defined as\nT A (x) = cone{z \u2212 x : z A \u2264 x A }. The normal cone N A (x) at x is defined as N A (x) = {s : s, z \u2212 s \u2264 0, \u2200z s.t. z A \u2264 x A }.\nNote that the normal cone N A (x) is the conic hull of the subdifferential of the atomic norm at x.\n\nBy a proper choice of the set A, the atomic norm reduces to several well-known norms. For example, let A \u2282 R p be the set of unit-norm one-sparse vectors {\u00b1e i } p i=1 . Then k-sparse vectors in R p can be constructed using a linear combination of k elements of the atomic set and the atomic norm x A reduces to the 1 -norm. Let A be the set of rank-one matrices of unit-Euclidean-norm. Then the rank-k matrices can be constructed using a linear combination of k elements of the atomic set and the atomic norm reduces to the matrix nuclear norm. Some other examples of atomic norms can be found in [Chandrasekaran et al., 2012]. At the following, we define a new atomic set A, and show that TNN is also an atomic norm induced by such an atomic set.\n\nLet D be a set of the following matrices, i.e., D \u2208 D where\nD = \uf8ee \uf8ef \uf8ef \uf8f0 D 1 D 2 . . . D n3 \uf8f9 \uf8fa \uf8fa \uf8fb \u2208 C n1n3\u00d7n2n3 ,\nwhere D i \u2208 C n1\u00d7n2 and there exists k such that D k = 0, rank(D k ) = 1, D k F = 1, and D j = 0, for all j = k. Then, for any A \u2208 R n1\u00d7n2\u00d7n3 , we have\n\u0100 * = inf \uf8f1 \uf8f2 \uf8f3 D \u2208D cD :\u0100 = D \u2208D cDD, cD \u2265 0, \u2200D \u2208 D \uf8fc \uf8fd \uf8fe .\nAbove we use the property of the rank one matrix decomposition of a matrix. This is equivalent to\n\u0100 * = inf \uf8f1 \uf8f2 \uf8f3 D \u2208D cD :\u0100 = D \u2208D cDD, cD \u2265 0, \u2200D \u2208 D \uf8fc \uf8fd \uf8fe = inf \uf8f1 \uf8f2 \uf8f3 D \u2208D c D : A = D \u2208D c D D, c D \u2265 0, \u2200D \u2208 D \uf8fc \uf8fd \uf8fe ,(4)\nwhere (4) uses the linear property of the inverse discrete Fourier transformation along the third dimension of a three way tensor. Motivated by (4), we define the atomic set A as\nA = {W \u2208 C n1\u00d7n2\u00d7n3 : W = n 3 D,D \u2208 D}. (5) By A * = 1 n3\n\u0100 * , we have the following result.\n\nTheorem 2. Let A be the set defined as in (5). The atomic norm A A is TNN, i.e.,\nA * = A A = inf W\u2208A c W : A = W\u2208A c W W, c W \u2265 0, \u2200W \u2208 A .\nFor any W \u2208 A, we have W * = n 3 D * = D * = 1. So the convex hull conv(A) is the TNN ball in which TNN is less than or equal to one. Interpreting TNN as a special atomic norm by choosing a proper atomic set is crucial for the low-rank tensor recovery guarantee.\n\n\nLow-rank Tensor Recovery from Gaussian Measurements\n\nThe Corollary 3.3 in [Chandrasekaran et al., 2012] shows that x 0 is the unique solution to problem (1) with high probability provided m \u2265 \u03c9 2 (T A (x 0 ) \u2229 S p\u22121 ) + 1. Here, T A (x 0 ) is the tangent cone at x 0 \u2208 R p , S p\u22121 is the unit sphere, and \u03c9(S) is the Gaussian width of a set S, defined as\n\u03c9(S) = E g sup z\u2208S g z ,\nwhere g is a vector of independent zero-mean unit-variance Gaussians. To apply such a result for our low tubal rank recovery, we need to estimate the Gaussian width of our atomic set A defined in (5). Theorem 3. Let M \u2208 R n1\u00d7n2\u00d7n3 be a tubal rank r tensor and A in (5). We have that\n\u03c9(T A (M) \u2229 S n1n2n3\u22121 ) \u2264 3r(n 1 + n 2 \u2212 r)n 3 . (6)\nNow, by using (6) and the Corollary 3.3 in [Chandrasekaran et al., 2012], we have the following main result. Theorem 4. Let \u03a6 : R n1\u00d7n2\u00d7n3 \u2192 R n be a random map with i.i.d. zero-mean Gaussian entries having variance 1 m and M \u2208 R n1\u00d7n2\u00d7n3 be a tensor of tubal rank r. Then, with high probability, we have:\n\n(1) exact recovery:X = M, whereX is the unique optimum of (3), provided that m \u2265 3r(n 1 + n 2 \u2212 r)n 3 + 1;\n\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence  (2) robust recovery: X \u2212 M F \u2264 2\u03b4 , whereX is optimal toX\n= arg min X X * , s.t. y \u2212 \u03a6(X ) 2 \u2264 \u03b4,(7)\nprovided that m \u2265 3r(n1+n2\u2212r)n3+3/2\n\n(1\u2212 ) 2\n\n.\n\nThe above theorem shows that the tensor with tubal rank r can be recovered exactly by solving the convex program (3) or approximately by (7) when the required number of measurements is of the order O(r(n 1 + n 2 \u2212 r)n 3 ). Note that such a rate is optimal compared with the degrees of freedom of a tensor with tubal rank r. Theorem 5. A n 1 \u00d7 n 2 \u00d7 n 3 sized tensor with tubal rank r has at most r(n 1 + n 2 \u2212 r)n 3 degrees of freedom.\n\nIt is worth mentioning that the guarantee for low tubal rank tehsor recovery in Theorem 4 is an extension of the low matrix guarantee in [Recht et al., 2010;Chandrasekaran et al., 2012]. If n 3 = 1, the tensor X reduces to a matrix, the tensor tubal rank reduces to the matrix rank, and TNN reduces to the matrix nuclear norm. Thus the convex program (3) and the theoretical guarantee in Theorem 4 include the low rank matrix recovery model and guarantee as special cases, respectively. Compared with the existing low rank tensor recovery guarantees (based on different tensor ranks, e.g., [Mu et al., 2014]) which are not order optimal, our guarantee enjoys the same optimal rate as the matrix case and our model (3) is computable.\n\n\nExact Tensor Completion Guarantee\n\nTheorem 4 gives the recovery guarantee of program (3) based on the Gaussian measurements. In this section, we consider the tensor completion problem which is a special case of (3) but based on the uniform random mapping. Suppose that M \u2208 R n1\u00d7n2\u00d7n3 and rank t (M) = r. We consider the Bernoulli model in this work: the entries of M are independently observed with probability p. We denote the set of the indices of the observed entries as \u2126. We simply denote \u2126 \u223c Ber(p). Then, the tensor completion problem asks for recovering M from the observations {M ij , (i, j, k) \u2208 \u2126}. We can solve this problem by solving the following program min X X * , s.t. P \u2126 (X ) = P \u2126 (M),\n\nwhere P \u2126 (X ) denotes the projection of X on the observed set \u2126. \nU * * e i F \u2264 \u00b5r n 1 n 3 ,(9)max j=1,\u00b7\u00b7\u00b7 ,n2 V * * e j F \u2264 \u00b5r n 2 n 3 ,(10)\nwheree i denotes the tensor column basis, which is a tensor of size n \u00d7 1 \u00d7 n 3 with its (i, 1, 1)-th entry equaling 1 and the rest equaling 0. We also define the tensor tube basis\u0117 k , which is a tensor of size 1 \u00d7 1 \u00d7 n 3 with its (1, 1, k)-th entry equaling 1 and the rest equaling 0. Denote n (1) = max(n 1 , n 2 ) and n (2) = min(n 1 , n 2 ). r = rankt(X 0 ) = 0.2n n rankt(X 0 ) m rankt(X )\n\nX \u2212X 0 F X 0 F 10 2 541 2 1.2e\u22129 20 4 2161 4 1.6e\u22129 30 6 4861 6 1.5e\u22129 r = rankt(X 0 ) = 0.3n\nn rankt(X 0 ) m rankt(X ) X \u2212X 0 F X 0 F 10 3 766 3\n1.6e\u22129 20 6 3061 6 1.2e\u22129 30 9 6886 9 1.2e\u22129 Table 1: Exact low tubal rank tensor recovery from Gaussian measurements with sufficient number of measurements. Theorem 6. Let M \u2208 R n1\u00d7n2\u00d7n3 with rank t (M) = r and the skinny t-SVD be M = U * S * V * . Suppose that the indices \u2126 \u223c Ber(p) and the tensor incoherence conditions (9)-(10) hold. There exist universal constants c 0 , c 1 , c 2 > 0 such that if p \u2265 c 0 \u00b5r log 2 (n (1) n 3 ) n (2) n 3 , then M is the unique solution to (8) with probability at least 1 \u2212 c 1 (n 1 + n 2 ) \u2212c2 . Theorem 6 shows that, to recover a n 1 \u00d7 n 2 \u00d7 n 3 sized tensor with tubal rank r, the sampling complexity is O(rn (1) n 3 log 2 (n (1) n 3 )). Such a bound is tight compared with the degrees of freedom 1 .\n\n\nExperiments\n\nIn this section, we conducts experiments to first verify the exact recovery guarantee in Theorem 4 for (3) from Gaussian measurements, then to verify the exact recovery guarantee in Theorem 6 for tensor completion (8). Both (3) and (8) \n\n\nExact Recovery from Gaussian Measurements\n\nTo verify Theorem 4, we can reformulate (3) a\u015d\nX = arg min X X * , s.t. y = Avec(X ),(11)\nwhere X \u2208 R n1\u00d7n2\u00d7n3 , A \u2208 R m\u00d7(n1n2n3) , y \u2208 R m and vec(X ) denotes the vectorization of X . The elements of A are with i.i.d. zero-mean Gaussian entries having variance 1/m. Thus, Avec(X ) gives the linear map \u03a6(X ). First, we test on random tensors, provided sufficient number of measurements as suggested in Theorem 4. We generate X 0 \u2208 R n\u00d7n\u00d7n3 of tubal rank r by X 0 = P * Q, where P \u2208 R n\u00d7r\u00d7n3 and Q \u2208 R r\u00d7n\u00d7n3 are with i.i.d. standard Gaussian random variables. We generate A \u2208 R m\u00d7(n 2 n3) with its entries being i.i.d., zero-mean, 1 m -variance Gaussian variables. Then, let y = Avec(X 0 ). We choose n = 10, 20, 30, n 3 = 5, r = 0.2n and r = 0.3n. We set the number of measurements m = 3r(2n \u2212 r)n 3 + 1 as in Theorem 4. The results are given in Table 1, in whichX is the solution to (11). It can be seen that the relative errors X \u2212 X 0 F / X 0 F are very small and the tubal ranks ofX are correct. Thus, this experiment verifies Theorem 4 for low tubal rank tensor recovery from Gaussian measurements.\n\nSecond, we exam the phase transition phenomenon in tubal rank r and the number of measurements m. We set n 1 = n 2 = 30 and n 3 = 5. We vary m between 1 and n 1 n 2 n 3 where the tensor is completely discovered. For a fixed m, we generate all possible tubal ranks such that r(n 1 + n 2 \u2212 r)n 3 \u2264 m. For each (m, r) pair, we repeat the following procedure 10 times. We generate X 0 , A, y in the same way as the first experiment above. We declare X 0 to be recovered if Figure 2 plots the fraction of correct recovery for each pair. The color of the cell in the figure reflects the empirical recovery rate of the 10 runs (scaled between 0 and 1). In all experiments, white denotes perfect recovery, while black denotes failure. It can be seen that there is a large region in which the recovery is correct. When the underlying tubal rank r of X 0 is relatively larger, the required number of measurements for correct recovery is also larger. Such a result is consistent with our theoretical result. Similar phenomenon can be found in low-rank matrix recovery [Chandrasekaran et al., 2012].\nX \u2212 X 0 F / X 0 F \u2264 10 \u22123 .\n\nExact Tensor Completion\n\nFirst, we verify the exact tensor completion guarantee in Theorem 6 on random data. We generate M \u2208 R n\u00d7n\u00d7n with tubal rank r by M = P * Q, where the entries of P \u2208 R n\u00d7r\u00d7n and Q \u2208 R r\u00d7n\u00d7n are independently sampled from an N (0, 1/n) distribution. Then we sample m = pn 3 elements uniformly from M to form the known samples. A useful quantity for reference is the number of degrees of freedom d r = r(2n \u2212 r)n.\n\nThe results in Table 1 shows that program (8) gives the correct recovery in the sense that the relative errors are small, less than 10 \u22125 and the tubal ranks of the obtained solution are correct. These results well verify the recovery guarantee in Theorem 6.\n\nSecond, we examine the recovery phenomenon with varying tubal rank of M and varying sampling rate p. We consider two sizes of M \u2208 R n\u00d7n\u00d7n : (1) n = 40; (2) n = 50. We generate M = P * Q, where the entries of P \u2208 R n\u00d7r\u00d7n and Q \u2208 R r\u00d7n\u00d7n are independently sampled from an N (0, 1/n) distribution. We set m = pn 3 . We choose p in [0.01 : 0.01 : 0.99] and r = 1, 2, . . . , 30 in the case n = 40, and r = 1, 2, . . . , 35 in the case n = 50. X 0 \u2208 R n\u00d7n\u00d7n , r = rankt(X 0 ), m = pn 3 , dr = r(2n \u2212 r)n n r m dr p rankt(X )   For each (r, p) triple, we simulate 10 test instances and declare a trial to be successful if the recoveredX satisfies X \u2212 M F / M F \u2264 10 \u22123 . Figure 3 plots the fraction of correct recovery for each triple (black = 0% and white = 100%). It can be seen that there is a large region in which the recovery is correct. Interestingly, the experiments reveal very similar plots for different n, suggesting that our asymptotic conditions for recovery may be conservative. Such a phenomenon is also consistent with the result in Theorem 6 which shows that the recovery is correct when the sampling rate p is not small and the tubal rank r is relatively low.\n\n\nConclusion\n\nThis paper first considers the exact guarantee of TNN minimization for low tubal rank tensor recovery from Gaussian measurements. We prove that TNN is a new instance of the atomic norm associated with certain atomic set. From the perspective of atomic norm minimization, we give the optimal estimation of the required measurements for the exact low tubal rank tensor recovery. Second, we give the exact recovery guarantee of TNN minimization for tensor completion. This result fixes the errors in the proofs of [Zhang and Aeron, 2017]. Numerical experiments verify our theoretical results.\n\nBy treating TNN as an instance of the atomic norm, we can get more results of low tubal rank recovery by using existing results, e.g., [Foygel and Mackey, 2014;Amelunxen et al., 2014]. Beyond the study on the convex TNN, it is also interesting to study the noncnovex models [Lu et al., 2015].\n\nFigure 1 :\n1Illustration of the t-SVD of an n1 \u00d7 n2 \u00d7 n3 tensor.Definition 2. (Conjugate transpose)[Lu et al., 2016;2018a] \n\nDefinition 4 .\n4(Orthogonal tensor) [Kilmer and Martin, 2011] A tensor Q \u2208 R n\u00d7n\u00d7n3 is orthogonal if it satisfies Q * * Q = Q * Q * = I. Definition 5. (F-diagonal Tensor) [Kilmer and Martin, 2011] A tensor is called f-diagonal if each of its frontal slices is a diagonal matrix. Theorem 1. (T-SVD) [Lu et al., 2018a; Kilmer and Martin, 2011] Let A \u2208 R n1\u00d7n2\u00d7n3 . Then it can be factored as\n\nDefinition 8 .\n8(Tensor spectral norm) [Lu et al., 2016] The tensor spectral norm of A \u2208 R n1\u00d7n2\u00d7n3 , denoted as A , is defined as A = bcirc(A) .\n\nFigure 2 :\n2Phase transitions for low tubal rank tensor recovery from Gaussian measurements. Fraction of correct recoveries is across 10 trials, as a function of r(n 1 +n 2 \u2212r)n 3 m (y-axis) and sampling rate m n 1 n 2 n 3 . In this test, n1 = n2 = 30, n3 = 5.\n\n\ncan be solved by the standard ADMM [Lu et al., 2018b] 2 .\n\nFigure 3 :\n3Phase transitions for tensor completion. Fraction of correct recoveries is across 10 trials, as a function of tubal rank r (y-axis) and sampling rate p (x-axis). The results are shown for different sizes of M \u2208 R n\u00d7n\u00d7n : (a) n = 40; (b) n = 50.\n\nTable 2 :\n2Exact tensor completion on random data.\nProceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence \nThe proofs in[Zhang and Aeron, 2017]  for tensor completion have several errors. Their used TNN definition is different from ours.2  The codes of our methods can be found at https://sites.google.com/site/canyilu/ Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence \nAcknowledgements J. Feng is partially supported by National University of Singapore startup grant R-263-000-C08-133 and Ministry of Education of Singapore AcRF Tier One grant R-263-000-C21-112. Z. Lin was supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), National Natural Science Foundation (NSF) of China (grant nos. 61625301 and 61731018), Qualcomm, and Microsoft Research Asia.\nTensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization. Amelunxen, arXiv:1804.03728Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. Lu et al., 2018b] Canyi Lu, Jiashi Feng, Shuicheng Yan, and Zhouchen Linthe Twenty-Seventh International Joint Conference on Artificial IntelligenceTanner9arXiv preprintPhase transitions in convex programs with random data. Information and Inference. IJCAI-18References [Amelunxen et al., 2014] Dennis Amelunxen, Martin Lotz, Michael B McCoy, and Joel A Tropp. Living on the edge: Phase transitions in convex programs with random data. Information and Inference, pages 224-294, 2014. [Cand\u00e8s and Recht, 2009] E.J. Cand\u00e8s and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717-772, 2009. [Cand\u00e8s et al., 2006] Emmanuel J Cand\u00e8s, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. TIT, 52(2):489-509, 2006. [Chandrasekaran et al., 2012] Venkat Chandrasekaran, Ben- jamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex geometry of linear inverse problems. Foundations of Computational mathematics, 12(6):805-849, 2012. [Chen, 2015] Yudong Chen. Incoherence-optimal matrix completion. TIT, 61(5):2909-2923, May 2015. [Donoho and Tanner, 2009] David Donoho and Jared Tanner. Counting faces of randomly projected polytopes when the projection radically lowers dimension. Journal of the Amer- ican Mathematical Society, 22(1):1-53, 2009. [Foygel and Mackey, 2014] Rina Foygel and Lester Mackey. Corrupted sensing: Novel guarantees for separating struc- tured signals. TIT, 60(2):1223-1247, 2014. [Kilmer and Martin, 2011] Misha E Kilmer and Carla D Mar- tin. Factorization strategies for third-order tensors. Linear Algebra and its Applications, 435(3):641-658, 2011. [Lu et al., 2015] Canyi Lu, Changbo Zhu, Chunyan Xu, Shuicheng Yan, and Zhouchen Lin. Generalized singu- lar value thresholding. In AAAI, 2015. [Lu et al., 2016] Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, and Shuicheng Yan Yan. Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization. In CVPR. IEEE, 2016. [Lu et al., 2018a] Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, and Shuicheng Yan Yan. Tensor robust principal component analysis with a new tensor nuclear norm. arXiv preprint arXiv:1804.03728, 2018. [Lu et al., 2018b] Canyi Lu, Jiashi Feng, Shuicheng Yan, and Zhouchen Lin. A unified alternating direction method of multipliers by majorization minimization. TPAMI, 40(3):527-541, 2018. [Mu et al., 2014] Cun Mu, Bo Huang, John Wright, and Don- ald Goldfarb. Square deal: Lower bounds and improved relaxations for tensor recovery. In ICML, pages 73-81, 2014. [Recht et al., 2010] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of lin- ear matrix equations via nuclear norm minimization. SIAM review, 52(3):471-501, 2010. [Zhang and Aeron, 2017] Zemin Zhang and Shuchin Aeron. Exact tensor completion using t-SVD. TSP, 65(6):1511- 1526, 2017. Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)\n", "annotations": {"author": "[{\"end\":173,\"start\":67},{\"end\":271,\"start\":174},{\"end\":450,\"start\":272},{\"end\":569,\"start\":451}]", "publisher": null, "author_last_name": "[{\"end\":75,\"start\":73},{\"end\":185,\"start\":181},{\"end\":284,\"start\":281},{\"end\":464,\"start\":461}]", "author_first_name": "[{\"end\":72,\"start\":67},{\"end\":180,\"start\":174},{\"end\":280,\"start\":272},{\"end\":460,\"start\":451}]", "author_affiliation": "[{\"end\":172,\"start\":95},{\"end\":270,\"start\":187},{\"end\":378,\"start\":302},{\"end\":449,\"start\":380},{\"end\":549,\"start\":466},{\"end\":568,\"start\":551}]", "title": "[{\"end\":64,\"start\":1},{\"end\":633,\"start\":570}]", "venue": null, "abstract": "[{\"end\":2259,\"start\":635}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3152,\"start\":3132},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3423,\"start\":3398},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3451,\"start\":3423},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3710,\"start\":3687},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4040,\"start\":4016},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4051,\"start\":4040},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4309,\"start\":4289},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4467,\"start\":4438},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6038,\"start\":6021},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6163,\"start\":6151},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6287,\"start\":6270},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6293,\"start\":6287},{\"end\":10296,\"start\":10271},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11098,\"start\":11080},{\"end\":11338,\"start\":11313},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12837,\"start\":12820},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14191,\"start\":14171},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14380,\"start\":14351},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14872,\"start\":14843},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16198,\"start\":16169},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17656,\"start\":17627},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18342,\"start\":18313},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19522,\"start\":19502},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19550,\"start\":19522},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19972,\"start\":19955},{\"end\":27176,\"start\":27153},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27393,\"start\":27368},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27416,\"start\":27393},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27524,\"start\":27507},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27643,\"start\":27626},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27649,\"start\":27643}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27650,\"start\":27526},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28041,\"start\":27651},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28188,\"start\":28042},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28450,\"start\":28189},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28510,\"start\":28451},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28768,\"start\":28511},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28820,\"start\":28769}]", "paragraph": "[{\"end\":2474,\"start\":2275},{\"end\":2713,\"start\":2476},{\"end\":2718,\"start\":2715},{\"end\":3594,\"start\":2720},{\"end\":5328,\"start\":3641},{\"end\":6164,\"start\":5330},{\"end\":7245,\"start\":6166},{\"end\":8694,\"start\":7277},{\"end\":9209,\"start\":8807},{\"end\":9508,\"start\":9449},{\"end\":9740,\"start\":9559},{\"end\":10069,\"start\":10020},{\"end\":10142,\"start\":10139},{\"end\":10527,\"start\":10144},{\"end\":10732,\"start\":10589},{\"end\":11518,\"start\":10848},{\"end\":12044,\"start\":11816},{\"end\":12241,\"start\":12046},{\"end\":12456,\"start\":12453},{\"end\":12846,\"start\":12724},{\"end\":13600,\"start\":12940},{\"end\":13976,\"start\":13642},{\"end\":14192,\"start\":14016},{\"end\":14874,\"start\":14194},{\"end\":15033,\"start\":14926},{\"end\":15144,\"start\":15062},{\"end\":15260,\"start\":15146},{\"end\":15340,\"start\":15297},{\"end\":15569,\"start\":15470},{\"end\":16319,\"start\":15571},{\"end\":16380,\"start\":16321},{\"end\":16587,\"start\":16436},{\"end\":16747,\"start\":16650},{\"end\":17052,\"start\":16874},{\"end\":17146,\"start\":17111},{\"end\":17228,\"start\":17148},{\"end\":17550,\"start\":17288},{\"end\":17907,\"start\":17606},{\"end\":18215,\"start\":17933},{\"end\":18575,\"start\":18270},{\"end\":18683,\"start\":18577},{\"end\":18835,\"start\":18685},{\"end\":18914,\"start\":18879},{\"end\":18923,\"start\":18916},{\"end\":18926,\"start\":18925},{\"end\":19363,\"start\":18928},{\"end\":20097,\"start\":19365},{\"end\":20805,\"start\":20135},{\"end\":20873,\"start\":20807},{\"end\":21346,\"start\":20950},{\"end\":21441,\"start\":21348},{\"end\":22236,\"start\":21494},{\"end\":22488,\"start\":22252},{\"end\":22580,\"start\":22534},{\"end\":23639,\"start\":22624},{\"end\":24728,\"start\":23641},{\"end\":25193,\"start\":24783},{\"end\":25453,\"start\":25195},{\"end\":26627,\"start\":25455},{\"end\":27231,\"start\":26642},{\"end\":27525,\"start\":27233}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3640,\"start\":3595},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8806,\"start\":8695},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9448,\"start\":9210},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9558,\"start\":9509},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9820,\"start\":9741},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10019,\"start\":9878},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10138,\"start\":10070},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10847,\"start\":10733},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11815,\"start\":11519},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12402,\"start\":12242},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12723,\"start\":12457},{\"attributes\":{\"id\":\"formula_11\"},\"end\":12939,\"start\":12847},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14015,\"start\":13977},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14925,\"start\":14875},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15061,\"start\":15034},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15296,\"start\":15261},{\"attributes\":{\"id\":\"formula_16\"},\"end\":15469,\"start\":15341},{\"attributes\":{\"id\":\"formula_17\"},\"end\":16435,\"start\":16381},{\"attributes\":{\"id\":\"formula_18\"},\"end\":16649,\"start\":16588},{\"attributes\":{\"id\":\"formula_19\"},\"end\":16873,\"start\":16748},{\"attributes\":{\"id\":\"formula_20\"},\"end\":17110,\"start\":17053},{\"attributes\":{\"id\":\"formula_21\"},\"end\":17287,\"start\":17229},{\"attributes\":{\"id\":\"formula_22\"},\"end\":17932,\"start\":17908},{\"attributes\":{\"id\":\"formula_23\"},\"end\":18269,\"start\":18216},{\"attributes\":{\"id\":\"formula_24\"},\"end\":18878,\"start\":18836},{\"attributes\":{\"id\":\"formula_26\"},\"end\":20903,\"start\":20874},{\"attributes\":{\"id\":\"formula_27\"},\"end\":20949,\"start\":20903},{\"attributes\":{\"id\":\"formula_28\"},\"end\":21493,\"start\":21442},{\"attributes\":{\"id\":\"formula_29\"},\"end\":22623,\"start\":22581},{\"attributes\":{\"id\":\"formula_30\"},\"end\":24756,\"start\":24729}]", "table_ref": "[{\"end\":21546,\"start\":21539},{\"end\":23389,\"start\":23382},{\"end\":25217,\"start\":25210}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2273,\"start\":2261},{\"attributes\":{\"n\":\"2\"},\"end\":7275,\"start\":7248},{\"end\":9877,\"start\":9822},{\"end\":10587,\"start\":10530},{\"attributes\":{\"n\":\"2.\"},\"end\":12451,\"start\":12404},{\"attributes\":{\"n\":\"3\"},\"end\":13640,\"start\":13603},{\"attributes\":{\"n\":\"4\"},\"end\":17604,\"start\":17553},{\"attributes\":{\"n\":\"5\"},\"end\":20133,\"start\":20100},{\"attributes\":{\"n\":\"6\"},\"end\":22250,\"start\":22239},{\"attributes\":{\"n\":\"6.1\"},\"end\":22532,\"start\":22491},{\"attributes\":{\"n\":\"6.2\"},\"end\":24781,\"start\":24758},{\"attributes\":{\"n\":\"7\"},\"end\":26640,\"start\":26630},{\"end\":27537,\"start\":27527},{\"end\":27666,\"start\":27652},{\"end\":28057,\"start\":28043},{\"end\":28200,\"start\":28190},{\"end\":28522,\"start\":28512},{\"end\":28779,\"start\":28770}]", "table": null, "figure_caption": "[{\"end\":27650,\"start\":27539},{\"end\":28041,\"start\":27668},{\"end\":28188,\"start\":28059},{\"end\":28450,\"start\":28202},{\"end\":28510,\"start\":28453},{\"end\":28768,\"start\":28524},{\"end\":28820,\"start\":28781}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10907,\"start\":10898},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24118,\"start\":24110},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26128,\"start\":26120}]", "bib_author_first_name": null, "bib_author_last_name": "[{\"end\":29768,\"start\":29759}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1804.03728\",\"id\":\"b0\",\"matched_paper_id\":7469586},\"end\":32970,\"start\":29645}]", "bib_title": "[{\"end\":29757,\"start\":29645}]", "bib_author": "[{\"end\":29770,\"start\":29759}]", "bib_venue": "[{\"end\":30033,\"start\":29951},{\"end\":29877,\"start\":29786}]"}}}, "year": 2023, "month": 12, "day": 17}