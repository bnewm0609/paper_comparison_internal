{"id": 245424895, "updated": "2023-10-05 18:43:34.056", "metadata": {"title": "Graph attentive feature aggregation for text-independent speaker verification", "authors": "[{\"first\":\"Hye-jin\",\"last\":\"Shim\",\"middle\":[]},{\"first\":\"Jungwoo\",\"last\":\"Heo\",\"middle\":[]},{\"first\":\"Jae-han\",\"last\":\"Park\",\"middle\":[]},{\"first\":\"Ga-hui\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Ha-Jin\",\"last\":\"Yu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The objective of this paper is to combine multiple frame-level features into a single utterance-level representation considering pairwise relationship. For this purpose, we propose a novel graph attentive feature aggregation module by interpreting each frame-level feature as a node of a graph. The inter-relationship between all possible pairs of features, typically exploited indirectly, can be directly modeled using a graph. The module comprises a graph attention layer and a graph pooling layer followed by a readout operation. The graph attention layer first models the non-Euclidean data manifold between different nodes. Then, the graph pooling layer discards less informative nodes considering the significance of the nodes. Finally, the readout operation combines the remaining nodes into a single representation. We employ two recent systems, SE-ResNet and RawNet2, with different input features and architectures and demonstrate that the proposed feature aggregation module consistently shows a relative improvement over 10%, compared to the baseline.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2112.12343", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icassp/ShimHPLY22", "doi": "10.1109/icassp43922.2022.9746257"}}, "content": {"source": {"pdf_hash": "5a19c24c3491ff5f56b49f883d46f7425937634a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.12343v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e3c0a631b50bd36b15a9b1483f224d967434df4f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5a19c24c3491ff5f56b49f883d46f7425937634a.txt", "contents": "\nGRAPH ATTENTIVE FEATURE AGGREGATION FOR TEXT-INDEPENDENT SPEAKER VERIFICATION\n\n\nHye-Jin Shim \nSchool of Computer Science\nUniversity of Seoul\n\n\n\u2020 \nJungwoo Heo \nSchool of Computer Science\nUniversity of Seoul\n\n\n\u2020 \nJae-Han Park \nKT Corporation\n\n\nGa-Hui Lee \nKT Corporation\n\n\nHa-Jin Yu \nSchool of Computer Science\nUniversity of Seoul\n\n\nGRAPH ATTENTIVE FEATURE AGGREGATION FOR TEXT-INDEPENDENT SPEAKER VERIFICATION\nIndex Terms-speaker verificationfeature aggregationatten- tiongraph attention networksdeep learning\nThe objective of this paper is to combine multiple frame-level features into a single utterance-level representation considering pairwise relationship. For this purpose, we propose a novel graph attentive feature aggregation module by interpreting each frame-level feature as a node of a graph. The inter-relationship between all possible pairs of features, typically exploited indirectly, can be directly modeled using a graph. The module comprises a graph attention layer and a graph pooling layer followed by a readout operation. The graph attention layer first models the non-Euclidean data manifold between different nodes. Then, the graph pooling layer discards less informative nodes considering the significance of the nodes. Finally, the readout operation combines the remaining nodes into a single representation. We employ two recent systems, SE-ResNet and RawNet2, with different input features and architectures and demonstrate that the proposed feature aggregation module consistently shows a relative improvement over 10%, compared to the baseline.\n\nINTRODUCTION\n\nSpeaker verification (SV) can be used for various authentication scenarios where the identity of a given speech is compared to that of the claimed speaker. In SV, speaker representations are derived by first extracting the frame-level features and then aggregating them. The network extracting the frame-level features is referred to as trunk network (e.g., convolutional neural network (CNN) and x-vector [1][2][3][4][5][6]). After extracting the frame-level features, various techniques including gated recurrent network (GRU), learnable dictionary encoding (LDE) are used on top of the trunk network to aggregate framelevel features into a single utterance-level feature [7][8][9][10][11][12][13][14][15][16][17][18]. The condensed utterance-level representation ultimately represents the entire character of an utterance, unlike speech recognition where each frame-level representation is of worth. Hence, feature aggregation plays an essential role and a number of studies in the recent literature have focused on developing feature aggregation methods.\n\nA group of studies employed recurrent layers for feature aggregation. These studies modeled the frame-level representations in sequential order [7][8][9][10][11]. Another group of research integrated framelevel features using LDE [16][17][18]. Similar to the recurrent layers, the LDE layer aggregates features over time whereas it further utilizes statistics. Sequential modeling, a foundation of the above two approaches, has been used in SV based on its success in language processing and speech recognition, where the sequential order could \u2020 Equally contributed. * Corresponding author. be a prior knowledge. However, recent studies found that sequential information may not be the key, especially in text-independent SV (TI-SV) [14,19].\n\nOther groups of researches exploited attention-based approaches- [12][13][14][15], which exclusively emphasize important features regardless of their sequence and have recently become the mainstream in TI-SV. These approaches assign an attention weight in the form of a scalar to each frame-level feature and then perform weighted summation (also with statistics as in [14]). Each attention weight for a feature is derived via a dot product between a feature and a projection vector. A softmax non-linearity is applied to attention weights to exclusively emphasize which frame is more important, however, they are compared indirectly without modeling their relationships.\n\nThe field of graph neural networks is recently getting attention by incorporating the advantages of graph structures and deep neural networks [20,21]. Interpreting high-dimensional representations as nodes of a graph, graph neural networks can model the non-Euclidean data manifold within nodes, including inter-node relationship. Particularly, recent architectures such as graph convolutional network [22] and graph attention network (GAT) [23] combined with various graph pooling layers [24][25][26] have demonstrated competitive performance even in image and audio domains [27][28][29]. Inspired by the recent success of graph neural networks, we argue that the feature aggregation could be further improved by explicitly comparing and modeling the inter-relationship between framelevel features. The feature aggregation is a task to express characteristics relative to other features and condense them into a single representation. Hence, the inter-relationship between features extends the ability of its expressiveness and could be the vital information itself. To this end, we propose a novel feature aggregation module to model inter-relationship by interpreting each frame-level feature as a node of a graph. The proposed module leverages the correlation of all possible pairs of nodes to obtain an utterance-level representation. In other words, entire frame-level features are aggregated considering their inter-relationships in addition to their intrinsic characteristics by utilizing a graph. Specifically, the aggregation involves a GAT, a graph pooling layer, and a readout operation [23,24]. The GAT assigns different weights to each node pairs (edges) and models their relationships. Then, the graph pooling layer discards less informative nodes. Finally, a single utterance-level representation is derived using the readout operation.\n\nThe effectiveness of the proposed feature aggregation framework is validated using two strong baselines, SE-ResNet [30] and RawNet2 [11], comprising different input features and architectures. Experimental results on the VoxCeleb datasets [31,32] demonstrate more than 10% consistent improvement over the corresponding baseline. Using the proposed graph aggregation module, we could compose more lightweight, yet better performing models compared to self-attentive pooling and GRU-based aggregations. Proposed graph attentive feature aggregation module. After extracting frame-level features using a trunk network, a graph is constructed using each sequence element as a node. Each node can have a dimensionality equal to either the number of filters (raw waveform, 1D-CNN) or the number of filters \u00d7 the number of frequency bins (mel-filterbank, 2D-CNN). Graph attention network first aggregates nodes features, and then the graph pooling reduces the number of nodes. The readout function integrates nodes into a single representation (best viewed in color).\n\n\nGRAPH ATTENTIVE FEATURE AGGREGATION\n\nIn this section, we introduce the graph attentive feature aggregation module. The proposed module is located after the extraction of frame-level features using a trunk network (RawNet2 or SE-ResNet). The proposed approach consists of three components: i) graph attention layer, ii) graph pooling layer, and iii) readout. We first describe how we formulate a graph from a feature map and perform GAT to integrate the overall features. Then, we explain how the graph pooling layer discards less informative nodes and the readout combines the results into a single representative feature. The overall scheme is illustrated in Figure 1 using RawNet2 (the scheme is identical for SE-ResNet except the dimensionality of each node).\n\n\nGraph attention layer\n\nWe first formulate a graph using frame-level features extracted from a trunk network. Specifically, we interpret each sequence element as a node of a graph; for RawNet2, each node has a dimensionality equal to the number of filters and for SE-ResNet, the dimensionality equals to the number of filters \u00d7 the number of frequency bins. Note that we design a fully-connected (complete) graph where all possible edges exist between pairs of nodes. Nodes can be directly compared considering all combinations/relationships because all nodes have bi-directional edges. Hence, entire frame-level features are aggregated considering their inter-relationships as well as intrinsic characteristics of their own by utilizing the graph attention layer.\n\nLet G be a complete graph comprising N nodes with the F dimensional features of each node. A set of nodes in G is defined as x \u2208 R N \u00d7F and each nodes are represented as row vectors x1, x2, \u00b7 \u00b7 \u00b7 , xN . In the GAT layer, x is first projected into F dimensional space using a matrix multiplication with W \u2208 R F \u00d7F resulting in n \u2208 R N \u00d7F (Equation (1)). Then, we calculate attention scores where an attention score aij between i-th node n i and j-th node n j (n i , n j \u2208 R F , i = j) is calculated through Equation (2) and (3). We derive eij, by first concatenating two nodes n i and n j and then projecting to a scalar value through a dot product with \u03b3 \u2208 R 2F \u00d71 , a learnable parameter, followed by a Leaky ReLU non-linearity [33]. Using the calculated attention scores, the GAT performs self-attention on node n i as in Equation (4). The GAT is Table 1. SE-ResNet architecture applying the proposed graph aggregation method. ReLU and batch normalization layers are omitted. Squeeze and excitation [34] is performed after all convolutional blocks. The numbers denoted in brackets refer to [size\u00d7 size, filters] and those in the output refer to (filters, frequency, time). The output of the last fully-connected layer is used as the speaker embedding. \n\napplied to every node x transforming n to n and then the output n is fed to a graph pooling layer. The GAT process can be described as follows:\nn = xW,(1)eij = LeakyReLU (\u03b3 \u00b7 concat(n i, n j )),(2)aij = softmax(eij) = exp(eij) N t=1 exp(eit) ,(3)ni = N j=1 aijn j .(4)\n\nGraph pooling layer and readout\n\nWe utilize a graph pooling layer followed by the readout operation to obtain an utterance-level feature. Similar to pooling layers in convolutional neural networks, graph pooling can reduce the size of features to enable high-level feature encoding and receptive field enlargement. Specifically, the graph pooling layer reduces the original graph into a sub-graph by removing less informative nodes as depicted in Figure 1. For the graph pooling, we exploit gPool introduced in the Graph U-Net architecture [24]. There are various other graph pooling methods, but many of them assume the hierarchical architecture of the graph. As it is difficult to define a hierarchical relationship between frame-level features in audio data, we adopt gPool, which does not require the graph to have a hierarchical structure. gPool consists of the following processes: projection, top-K node selection, and a gating mechanism. A set of nodes in the graph pooling layer l is represented as n l . Note that the adjacency matrix is omitted because we utilize a complete graph which is different from the original paper of [24]. In the projection stage, gPool obtains score vector y by estimating the scalar projection values of each node, using a trainable projection vector p \u2208 R F . In the top-K node selection, the operation of node ranking (rank) outputs indices of the K-largest values (index) based on calculated score vector y. The score vector of selected nodes,\u1ef9, can be obtained by applying sigmoid to each element in the extracted scalar projection vector. In the gate stage, element-wise product is conducted between the pooled nodes\u00f1 l and the selected node scores y.\n\nFormally, gPool can be denoted as follows: y = n l p l / p l , index = rank(y, K), y = sigmoid(y(index)), n l = n l (index, :)\nn l+1 =\u00f1 l \u1ef9(5)\nAfter the graph pooling, we conduct the readout operation, which combines the processed nodes into a single node. We explore various types of readout operation where the best result is obtained using the summation of nodes: Here, U is the aggregated utterance-level feature and K nodes n k are integrated to n. The results of various readout mechanisms can be found in Section 4.\nU = K k=1 n l+1 k .(6)\n\nEXPERIMENTS\n\n\nDatasets\n\nAll experiments are performed using the VoxCeleb1&2 datasets [31,32]. We train the model using the development subset of VoxCeleb2 that includes the utterances from 5,994 speakers. Then, the evaluation is performed using the original trial that uses VoxCeleb1's evaluation subset.\n\n\nImplementation details\n\nBoth baselines using a mel-filterbank and a raw waveform are implemented based on the PyTorch framework [38]. Graphs are implemented using Deep Graph Library [39]. We adjust the number of attention heads and use 16 heads in RawNet2 and 32 heads in SE-ResNet based on empirical results.\n\n\nSE-ResNet.\n\nOur mel-filterbank baseline is most similar to the architecture of [30], but overall details are adjusted. We use 40dimensional mel-filterbank features extracted with 1,024 point FFT and a hamming window of width 25ms and step 10ms. The melfilterbank baseline is optimized using Adam optimizer which uses a learning rate of 0.001, and the learning rate is decreased by 5% in each epoch. We use two types of loss functions, additive angular margin softmax (AAM-softmax) [40] and angular prototypical loss (AP) [41] to consider inter-class as well as intra-class covariance. We employ a margin of 0.3 and a scale of 30 for AAM-softmax. For AP, we use a mini-batch size of 200, where each mini-batch contains two utterances per speaker. The system is trained for 100 epochs. Our overall architecture applying the proposed method is presented in Table 1.\n\nRawNet2. RawNet2 [11] is an end-to-end system that is fed by raw waveforms directly without preprocessing techniques. For minibatch construction, utterances are either cropped or duplicated (concatenated) into 59,049 samples (\u2248 3.69s) in the training phase, following [11]. In the evaluation phase, no adjustments are made to the length. We modify several details from RawNet2 [11] in the process of adjusting the GAT as follows: i) exclude sinc-conv layer, as it slows down training time in spite of showing similar performance, ii) replace softmax with additive margin softmax (AM-softmax) [42], iii) reduce the dimensions by half for the last fully-connected layer before speaker embedding (1024 dimensions to 512 dimensions). \n\n\nRESULTS\n\nApplication of GAT. In Table 2, we first address the effectiveness of the graph attention layer on our two baselines: SE-ResNet and RawNet2. Here, the graph pooling is temporarily excluded and summation readout operation is applied. Adoption of the graph attention layer reduced the EER relatively by 6% and 10%, respectively. In addition, we confirm that the number of parameters also decreases (by almost 25% in the case of RawNet2 (13.2M to 9.9M)).\n\nGraph pooling. In Table 3, we explore the effect of applying the graph pooling layer between the graph attention layer and the readout operation with diverse pooling ratios. The denoted percentage of pooling refers to the remaining ratios of nodes; higher ratios mean that fewer nodes have been removed. Top 80% pooling removing 20% of irrelevant nodes shows the best results where the relative improvement compared to the baseline becomes 11.6% and 13.3%. Through a comparison of the three different ratios of pooling, we find that discarding too many nodes worsens the performance (11% pool) but removing a few of the most irrelevant nodes improves the performance (80% pool). We thus conclude that the discriminative information is well combined into the majority of nodes (80% in our case) via the GAT layer.\n\nComparison of readout operations. In Table 4, we further compare several readout operations using the RawNet2 baseline. We find that the proposed graph-based aggregation shows consistent effectiveness, regardless of the adopted readout approach because all node aggregation methods performed better than GRU. Among various methods, summation achieved the best result.\n\nComparison with state-of-the-art systems. In Table 5, we compare the proposed system with other recent state-of-the-art systems in the literature that adopt various feature aggregation methods. None of the systems apply data augmentation techniques. In both input fea-tures, the proposed models using the proposed graph aggregation module demonstrated the best performance, showing the effectiveness of the proposed approach. Given that we did not use data augmentation methods, there is room to improve the system using various data augmentation.\n\nAdditional experiments. Table 6 addresses two additional experiments using the RawNet2 baseline. First, we explore whether adopting a GRU after the GAT is beneficial and show the result in the first row. To maintain the overall complexity of the model, we omitted the last residual block and then placed the GAT and the GRU in sequence. However, this worsens the performance, as presented in the first row. In our analysis, this shows that applying GRU directly to the GAT's output is not effective, because the GAT's output is not sequential. Second, we explore using two GAT layers with different pooling architectures proposed in [25] and denote the result in second and third rows. Both architectures adopt two GAT layers in sequence. The global architecture concatenates two GAT layers' output and feeds it to the graph pooling layer followed by the readout, whereas hierarchical readout performs graph pooling and readout after each GAT layer and then adds them element-wisely. Through experiments, we found that both modified pooling architectures did not bring further improvements.\n\n\nCONCLUSIONS\n\nIn this paper, we proposed a graph feature aggregation method for TI-SV. Utilizing a GAT, graph pooling layer, and readout operation, we directly modeled the inter-relationship between entire framelevel features, which is partially or indirectly utilized in the existing methods. As this is the first work employing graph neural networks for feature aggregation, we also explored various configurations to optimize the system. Consistent improvements over the baselines with different aggregation modules demonstrate the effectiveness of the proposed approach.\n\nFig. 1 .\n1Fig. 1. Proposed graph attentive feature aggregation module. After extracting frame-level features using a trunk network, a graph is constructed using each sequence element as a node. Each node can have a dimensionality equal to either the number of filters (raw waveform, 1D-CNN) or the number of filters \u00d7 the number of frequency bins (mel-filterbank, 2D-CNN). Graph attention network first aggregates nodes features, and then the graph pooling reduces the number of nodes. The readout function integrates nodes into a single representation (best viewed in color).\n\nTable 2 .\n2Application of proposed graph module to SE-ResNet and RawNet2. SAP in SE-ResNet and GRU in RawNet2 are both replaced by GAT.Feature extractor Aggregation # Params EER (%) \n\nSE-ResNet \nSAP \n6.0M \n1.98 \nSE-ResNet \nGAT \n5.4M \n1.86 \n\nRawNet2 \nGRU \n13.2M \n2.48 \nRawNet2 \nGAT \n9.9M \n2.23 \n\nTable 3. Comparison of applying different pooling ratios in graph \npooling layer. A higher pooling ratio means more nodes remain. \n\nPooling method \nEER (%) \nSE-ResNet RawNet2 \n\nw/o gPool \n1.86 \n2.23 \nTop 11% gPool \n2.00 \n2.39 \nTop 33% gPool \n1.91 \n2.17 \nTop 80% gPool \n1.75 \n2.15 \n\n\n\nTable 4 .\n4Comparison of various readout methods. \"Combine by concat\" indicates that the results of summation, standard deviation (std), minimum, and maximum are concatenated by reducing the output dimensions in one-quarter.Operation \nEER (%) \n\nMean \n2.38 \nSum \n2.23 \nMax \n2.3 \nCombine by concat \n2.38 \n\n\n\nTable 5 .\n5Results of comparison with recent state-of-the-art systems. The two baselines of this study are each compared with various architectures that use the same input feature.Input Feature \nFront-end \nAggregation \nLoss \nEER (%) \n\nChung et al. [2] \nSpec-257 \nThin ResNet-34 \nSAP \nAP \n2.21 \nYu et al. [35] \nSpec-512 \nResNet-50 \nTAP \nEAM-Softmax \n2.94 \nLiu et al. [36] \nMFB-40 \nDense-Residual \nABP \nSoftmax \n2.54 \nJung et al. [1] \nMFB-40 \nFast ResNet-34 \nLDE \nNP + Softmax \n1.98 \nKye et al. [15] \nMFB-40 \nFast ResNet-34 \nCAP \nNP + Softmax \n1.88 \nOurs \nMFB-40 \nSE-ResNet \nSAP \nAP + AAM-Softmax \n1.98 \nOurs(Proposed) \nMFB-40 \nSE-ResNet \nGAT \nAP + AAM-Softmax \n1.75 \nLin et al. [3] \n\nRaw waveform \n\nwav2spk \nGating + SP \nAM-Softmax \n3.00 \nZhu et al. [37] \nY-vector \nSP \nAM-Softmax \n2.60 \nJung et al. [11] \nRawNet2 \nGRU \nSoftmax \n2.48 \nOurs(Proposed) \nRawNet2 \nGAT \nAM-Softmax \n2.15 \n\nTable 6. Results of additional experiments. The first row shows the \nresult of applying GAT and GRU sequentially. The lower two rows \nshow the result of applying two GAT layers with different pooling \narchitectures. \nSystem \nEER (%) \n\nGAT -GRU \n4.16 \n\n2GAT -Global architecture \n2.66 \n2GAT -Hierarchical architecture \n2.55 \n\n\nAcknowledgement.We would like to thank Jee-weon Jung at Naver Coporation for his help with the conceptualization and editing.\nImproving multi-scale aggregation using feature pyramid module for robust speaker verification of variable-duration utterances. Y Jung, S M Kye, Y Choi, Proc. Interspeech. InterspeechY. Jung, S. M. Kye, Y. Choi et al., \"Improving multi-scale ag- gregation using feature pyramid module for robust speaker ver- ification of variable-duration utterances,\" in Proc. Interspeech, 2020.\n\nIn defence of metric learning for speaker recognition. J S Chung, J Huh, S Mun, Proc. Interspeech. InterspeechJ. S. Chung, J. Huh, S. Mun et al., \"In defence of metric learn- ing for speaker recognition,\" in in Proc. Interspeech, 2020.\n\nWav2spk: A simple dnn architecture for learning speaker embeddings from waveforms. W.-W Lin, M.-W Mak, Proc. Interspeech. InterspeechW.-W. Lin and M.-W. Mak, \"Wav2spk: A simple dnn architec- ture for learning speaker embeddings from waveforms.,\" in in Proc. Interspeech, 2020.\n\nX-vectors: Robust dnn embeddings for speaker recognition. D Snyder, D Garcia-Romero, G Sell, Proc. ICASSP. IEEE. ICASSP. IEEED. Snyder, D. Garcia-Romero, G. Sell et al., \"X-vectors: Robust dnn embeddings for speaker recognition,\" in Proc. ICASSP. IEEE, 2018.\n\nDeep speaker embedding learning with multi-level pooling for text-independent speaker verification. Y Tang, G Ding, J Huang, Proc. ICASSP. IEEE. ICASSP. IEEEY. Tang, G. Ding, J. Huang et al., \"Deep speaker embedding learning with multi-level pooling for text-independent speaker verification,\" in Proc. ICASSP. IEEE, 2019.\n\nVector-based attentive pooling for text-independent speaker verification. Y Wu, C Guo, H Gao, Proc. Interspeech. InterspeechY. Wu, C. Guo, H. Gao et al., \"Vector-based attentive pooling for text-independent speaker verification.,\" in in Proc. Inter- speech, 2020.\n\nDeep speaker: an endto-end neural speaker embedding system. C Li, X Ma, B Jiang, arXiv:1705.02304arXiv preprintC. Li, X. Ma, B. Jiang et al., \"Deep speaker: an end- to-end neural speaker embedding system,\" arXiv preprint arXiv:1705.02304, 2017.\n\nShort utterance compensation in speaker verification via cosine-based teacher-student learning of speaker embeddings. J Jung, H.-S Heo, H Shim, H.-J Yu, 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEEJ.-w. Jung, H.-S. Heo, H.-j. Shim and H.-J. Yu, \"Short ut- terance compensation in speaker verification via cosine-based teacher-student learning of speaker embeddings,\" in 2019 IEEE Automatic Speech Recognition and Understanding Work- shop (ASRU). IEEE, 2019.\n\nSeq2seq attentional siamese neural networks for text-dependent speaker verification. Y Zhang, M Yu, N Li, Proc. ICASSP. IEEE. ICASSP. IEEEY. Zhang, M. Yu, N. Li et al., \"Seq2seq attentional siamese neural networks for text-dependent speaker verification,\" in Proc. ICASSP. IEEE, 2019.\n\nRawnet: Advanced end-to-end deep neural network using raw waveforms for textindependent speaker verification. J Jung, H.-S Heo, J Kim, Proc. Interspeech. InterspeechJ.-w. Jung, H.-S. Heo, J.-h. Kim et al., \"Rawnet: Advanced end-to-end deep neural network using raw waveforms for text- independent speaker verification,\" in Proc. Interspeech, 2019.\n\nImproved rawnet with feature map scaling for text-independent speaker verification using raw waveforms. J Jung, S Kim, H.-J Shim, Proc. Interspeech. InterspeechJ.-w. Jung, S.-b. Kim, H.-j. Shim et al., \"Improved rawnet with feature map scaling for text-independent speaker verification using raw waveforms,\" in Proc. Interspeech, 2020.\n\nDeep neural network embeddings for text-independent speaker verification. D Snyder, D Garcia-Romero, D Povey, S Khudanpur, Proc. Interspeech. InterspeechD. Snyder, D. Garcia-Romero, D. Povey and S. Khudan- pur, \"Deep neural network embeddings for text-independent speaker verification.,\" in in Proc. Interspeech, 2017.\n\nSelf-attentive speaker embeddings for text-independent speaker verification. Y Zhu, T Ko, D Snyder, Proc. Interspeech. InterspeechY. Zhu, T. Ko, D. Snyder et al., \"Self-attentive speaker embed- dings for text-independent speaker verification.,\" in in Proc. Interspeech, 2018.\n\nAttentive statistics pooling for deep speaker embedding. K Okabe, T Koshinaka, K Shinoda, Proc. Interspeech. InterspeechK. Okabe, T. Koshinaka and K. Shinoda, \"Attentive statistics pooling for deep speaker embedding,\" in Proc. Interspeech, 2018.\n\nCross attentive pooling for speaker verification. S M Kye, Y Kwon, J S Chung, 2021 IEEE Spoken Language Technology Workshop (SLT). IEEES. M. Kye, Y. Kwon and J. S. Chung, \"Cross attentive pool- ing for speaker verification,\" in 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021.\n\nExploring the encoding layer and loss function in end-to-end speaker and language recognition system. W Cai, J Chen, M Li, Proc. Interspeech. InterspeechW. Cai, J. Chen and M. Li, \"Exploring the encoding layer and loss function in end-to-end speaker and language recognition system,\" in Proc. Interspeech, 2018.\n\nUtterancelevel aggregation for speaker recognition in the wild. W Xie, A Nagrani, J S Chung, A Zisserman, Proc. ICASSP. IEEE. ICASSP. IEEEW. Xie, A. Nagrani, J. S. Chung and A. Zisserman, \"Utterance- level aggregation for speaker recognition in the wild,\" in Proc. ICASSP. IEEE, 2019.\n\nSpatial pyramid encoding with convex length normalization for text-independent speaker verification. Y Jung, Y Kim, H Lim, Proc. Interspeech. InterspeechY. Jung, Y. Kim, H. Lim et al., \"Spatial pyramid encoding with convex length normalization for text-independent speaker verification,\" in Proc. Interspeech, 2019.\n\nEcapatdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification. B Desplanques, J Thienpondt, K Demuynck, Proc. Interspeech. InterspeechB. Desplanques, J. Thienpondt and K. Demuynck, \"Ecapa- tdnn: Emphasized channel attention, propagation and aggrega- tion in tdnn based speaker verification,\" in Proc. Interspeech, 2020.\n\nGraph embedding techniques, applications, and performance: A survey. P Goyal, E Ferrara, Knowledge-Based SystemsP. Goyal and E. Ferrara, \"Graph embedding techniques, appli- cations, and performance: A survey,\" Knowledge-Based Sys- tems, 2018.\n\nA comprehensive survey on graph neural networks. Z Wu, S Pan, F Chen, IEEE transactions on neural networks and learning systems. Z. Wu, S. Pan, F. Chen et al., \"A comprehensive survey on graph neural networks,\" IEEE transactions on neural networks and learning systems, 2020.\n\nSemi-supervised classification with graph convolutional networks. T N Kipf, M Welling, arXiv:1609.02907arXiv preprintT. N. Kipf and M. Welling, \"Semi-supervised classifica- tion with graph convolutional networks,\" arXiv preprint arXiv:1609.02907, 2016.\n\nGraph attention networks. P Veli\u010dkovi\u0107, G Cucurull, A Casanova, arXiv:1710.10903arXiv preprintP. Veli\u010dkovi\u0107, G. Cucurull, A. Casanova et al., \"Graph attention networks,\" arXiv preprint arXiv:1710.10903, 2017.\n\nGraph u-nets. H Gao, S Ji, Proc. ICML. ICMLH. Gao and S. Ji, \"Graph u-nets,\" in Proc. ICML, 2019.\n\nSelf-attention graph pooling. J Lee, I Lee, J Kang, Proc. ICML. ICMLJ. Lee, I. Lee and J. Kang, \"Self-attention graph pooling,\" in Proc. ICML, 2019.\n\nStructpool: Structured graph pooling via conditional random fields. H Yuan, S Ji, Proc. ICLR. ICLRH. Yuan and S. Ji, \"Structpool: Structured graph pooling via conditional random fields,\" in Proc. ICLR, 2020.\n\nGraph attention networks for speaker verification. J Jung, H.-S Heo, H.-J Yu, J S Chung, Proc. ICASSP. IEEE. ICASSP. IEEEJ.-w. Jung, H.-S. Heo, H.-J. Yu and J. S. Chung, \"Graph at- tention networks for speaker verification,\" in Proc. ICASSP. IEEE, 2021.\n\nGraph attention networks for anti-spoofing. H Tak, J Jung, J Patino, Proc. Interspeech. InterspeechH. Tak, J.-w. Jung, J. Patino et al., \"Graph attention networks for anti-spoofing,\" in Proc. Interspeech, 2021.\n\nEnd-to-end spectrotemporal graph attention networks for speaker verification anti-spoofing and speech deepfake detection. H Tak, J Jung, J Patino, arXiv:2107.12710arXiv preprintH. Tak, J.-w. Jung, J. Patino et al., \"End-to-end spectro- temporal graph attention networks for speaker verification anti-spoofing and speech deepfake detection,\" arXiv preprint arXiv:2107.12710, 2021.\n\nClova baseline system for the VoxCeleb speaker recognition challenge 2020. H S Heo, B.-J Lee, J Huh, J S Chung, arXiv:2009.14153arXiv preprintH. S. Heo, B.-J. Lee, J. Huh and J. S. Chung, \"Clova baseline system for the VoxCeleb speaker recognition challenge 2020,\" arXiv preprint arXiv:2009.14153, 2020.\n\nVoxceleb: a largescale speaker identification dataset. A Nagrani, J S Chung, A Zisserman, Proc. Interspeech. InterspeechA. Nagrani, J. S. Chung and A. Zisserman, \"Voxceleb: a large- scale speaker identification dataset,\" in Proc. Interspeech, 2017.\n\nVoxceleb2: Deep speaker recognition. J S Chung, A Nagrani, A Zisserman, Proc. Interspeech. InterspeechJ. S. Chung, A. Nagrani and A. Zisserman, \"Voxceleb2: Deep speaker recognition,\" in Proc. Interspeech, 2018.\n\nRectifier nonlinearities improve neural network acoustic models. A L Maas, A Y Hannun, A Y Ng, Proc. ICML. ICMLA. L. Maas, A. Y. Hannun, A. Y. Ng et al., \"Rectifier non- linearities improve neural network acoustic models,\" in Proc. ICML, 2013.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, Proc. CVPR. CVPRJ. Hu, L. Shen and G. Sun, \"Squeeze-and-excitation networks,\" in Proc. CVPR, 2018.\n\nEnsemble additive margin softmax for speaker verification. Y.-Q Yu, L Fan, W.-J Li, Proc. ICASSP. IEEE. ICASSP. IEEEY.-Q. Yu, L. Fan and W.-J. Li, \"Ensemble additive margin soft- max for speaker verification,\" in Proc. ICASSP. IEEE, 2019, pp. 6046-6050.\n\nAn effective deep embedding learning method based on dense-residual networks for speaker verification. Y Liu, Y Song, I Mcloughlin, Proc. ICASSP. IEEE. ICASSP. IEEEY. Liu, Y. Song, I. McLoughlin et al., \"An effective deep em- bedding learning method based on dense-residual networks for speaker verification,\" in Proc. ICASSP. IEEE, 2021.\n\nY-vector: Multiscale waveform encoder for speaker embedding. G Zhu, F Jiang, Z Duan, Proc. Interspeech. InterspeechG. Zhu, F. Jiang and Z. Duan, \"Y-vector: Multiscale waveform encoder for speaker embedding,\" in in Proc. Interspeech, 2021.\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, NIPS. A. Paszke, S. Gross, F. Massa et al., \"Pytorch: An imperative style, high-performance deep learning library,\" NIPS, 2019.\n\nDeep graph library: Towards efficient and scalable deep learning on graphs. M Wang, L Yu, D Zheng, M. Wang, L. Yu, D. Zheng et al., \"Deep graph library: Towards efficient and scalable deep learning on graphs.,\" 2019.\n\nArcface: Additive angular margin loss for deep face recognition. J Deng, J Guo, N Xue, S Zafeiriou, Proc. CVPR. CVPRJ. Deng, J. Guo, N. Xue and S. Zafeiriou, \"Arcface: Additive angular margin loss for deep face recognition,\" in Proc. CVPR, 2019.\n\nDeep metric learning with angular loss. J Wang, F Zhou, S Wen, Proc. ICCV. ICCVJ. Wang, F. Zhou, S. Wen et al., \"Deep metric learning with angular loss,\" in Proc. ICCV, 2017.\n\nCosface: Large margin cosine loss for deep face recognition. H Wang, Y Wang, Z Zhou, Proc. CVPR. CVPRH. Wang, Y. Wang, Z. Zhou et al., \"Cosface: Large margin cosine loss for deep face recognition,\" in Proc. CVPR, 2018.\n", "annotations": {"author": "[{\"end\":143,\"start\":81},{\"end\":146,\"start\":144},{\"end\":208,\"start\":147},{\"end\":211,\"start\":209},{\"end\":242,\"start\":212},{\"end\":271,\"start\":243},{\"end\":331,\"start\":272}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":89},{\"end\":158,\"start\":155},{\"end\":224,\"start\":220},{\"end\":253,\"start\":250},{\"end\":281,\"start\":279}]", "author_first_name": "[{\"end\":88,\"start\":81},{\"end\":145,\"start\":144},{\"end\":154,\"start\":147},{\"end\":210,\"start\":209},{\"end\":219,\"start\":212},{\"end\":249,\"start\":243},{\"end\":278,\"start\":272}]", "author_affiliation": "[{\"end\":142,\"start\":95},{\"end\":207,\"start\":160},{\"end\":241,\"start\":226},{\"end\":270,\"start\":255},{\"end\":330,\"start\":283}]", "title": "[{\"end\":78,\"start\":1},{\"end\":409,\"start\":332}]", "venue": null, "abstract": "[{\"end\":1573,\"start\":510}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1998,\"start\":1995},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2001,\"start\":1998},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2004,\"start\":2001},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2007,\"start\":2004},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2010,\"start\":2007},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2013,\"start\":2010},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2266,\"start\":2263},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2269,\"start\":2266},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2272,\"start\":2269},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2276,\"start\":2272},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2280,\"start\":2276},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2284,\"start\":2280},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2288,\"start\":2284},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2292,\"start\":2288},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2296,\"start\":2292},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2300,\"start\":2296},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2304,\"start\":2300},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2308,\"start\":2304},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2796,\"start\":2793},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2799,\"start\":2796},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2802,\"start\":2799},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2806,\"start\":2802},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2810,\"start\":2806},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2883,\"start\":2879},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2887,\"start\":2883},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2891,\"start\":2887},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3387,\"start\":3383},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3390,\"start\":3387},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3462,\"start\":3458},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3466,\"start\":3462},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3470,\"start\":3466},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3474,\"start\":3470},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3766,\"start\":3762},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4212,\"start\":4208},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4215,\"start\":4212},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4472,\"start\":4468},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4511,\"start\":4507},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4559,\"start\":4555},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4563,\"start\":4559},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4567,\"start\":4563},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4646,\"start\":4642},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4650,\"start\":4646},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4654,\"start\":4650},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5669,\"start\":5665},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5672,\"start\":5669},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6039,\"start\":6035},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6056,\"start\":6052},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6163,\"start\":6159},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6166,\"start\":6163},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9245,\"start\":9241},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9517,\"start\":9513},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10582,\"start\":10578},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11180,\"start\":11176},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12372,\"start\":12368},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12375,\"start\":12372},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12722,\"start\":12718},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12776,\"start\":12772},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12985,\"start\":12981},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13387,\"start\":13383},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13427,\"start\":13423},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13787,\"start\":13783},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14038,\"start\":14034},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14147,\"start\":14143},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14362,\"start\":14358},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17330,\"start\":17326}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":18937,\"start\":18360},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":19516,\"start\":18938},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":19822,\"start\":19517},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":21032,\"start\":19823}]", "paragraph": "[{\"end\":2647,\"start\":1589},{\"end\":3391,\"start\":2649},{\"end\":4064,\"start\":3393},{\"end\":5918,\"start\":4066},{\"end\":6979,\"start\":5920},{\"end\":7744,\"start\":7019},{\"end\":8510,\"start\":7770},{\"end\":9766,\"start\":8512},{\"end\":9911,\"start\":9768},{\"end\":11734,\"start\":10071},{\"end\":11862,\"start\":11736},{\"end\":12258,\"start\":11879},{\"end\":12587,\"start\":12307},{\"end\":12899,\"start\":12614},{\"end\":13764,\"start\":12914},{\"end\":14496,\"start\":13766},{\"end\":14959,\"start\":14508},{\"end\":15773,\"start\":14961},{\"end\":16142,\"start\":15775},{\"end\":16691,\"start\":16144},{\"end\":17783,\"start\":16693},{\"end\":18359,\"start\":17799}]", "formula": "[{\"attributes\":{\"id\":\"formula_1\"},\"end\":9922,\"start\":9912},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9965,\"start\":9922},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10014,\"start\":9965},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10036,\"start\":10014},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11878,\"start\":11863},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12281,\"start\":12259}]", "table_ref": "[{\"end\":9368,\"start\":9361},{\"end\":13763,\"start\":13756},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14538,\"start\":14531},{\"end\":14986,\"start\":14979},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15819,\"start\":15812},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16196,\"start\":16189},{\"end\":16724,\"start\":16717}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1587,\"start\":1575},{\"attributes\":{\"n\":\"2.\"},\"end\":7017,\"start\":6982},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7768,\"start\":7747},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10069,\"start\":10038},{\"attributes\":{\"n\":\"3.\"},\"end\":12294,\"start\":12283},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12305,\"start\":12297},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12612,\"start\":12590},{\"end\":12912,\"start\":12902},{\"attributes\":{\"n\":\"4.\"},\"end\":14506,\"start\":14499},{\"attributes\":{\"n\":\"5.\"},\"end\":17797,\"start\":17786},{\"end\":18369,\"start\":18361},{\"end\":18948,\"start\":18939},{\"end\":19527,\"start\":19518},{\"end\":19833,\"start\":19824}]", "table": "[{\"end\":19516,\"start\":19074},{\"end\":19822,\"start\":19742},{\"end\":21032,\"start\":20004}]", "figure_caption": "[{\"end\":18937,\"start\":18371},{\"end\":19074,\"start\":18950},{\"end\":19742,\"start\":19529},{\"end\":20004,\"start\":19835}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7650,\"start\":7642},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10493,\"start\":10485}]", "bib_author_first_name": "[{\"end\":21288,\"start\":21287},{\"end\":21296,\"start\":21295},{\"end\":21298,\"start\":21297},{\"end\":21305,\"start\":21304},{\"end\":21597,\"start\":21596},{\"end\":21599,\"start\":21598},{\"end\":21608,\"start\":21607},{\"end\":21615,\"start\":21614},{\"end\":21865,\"start\":21861},{\"end\":21875,\"start\":21871},{\"end\":22115,\"start\":22114},{\"end\":22125,\"start\":22124},{\"end\":22142,\"start\":22141},{\"end\":22417,\"start\":22416},{\"end\":22425,\"start\":22424},{\"end\":22433,\"start\":22432},{\"end\":22715,\"start\":22714},{\"end\":22721,\"start\":22720},{\"end\":22728,\"start\":22727},{\"end\":22966,\"start\":22965},{\"end\":22972,\"start\":22971},{\"end\":22978,\"start\":22977},{\"end\":23270,\"start\":23269},{\"end\":23281,\"start\":23277},{\"end\":23288,\"start\":23287},{\"end\":23299,\"start\":23295},{\"end\":23730,\"start\":23729},{\"end\":23739,\"start\":23738},{\"end\":23745,\"start\":23744},{\"end\":24041,\"start\":24040},{\"end\":24052,\"start\":24048},{\"end\":24059,\"start\":24058},{\"end\":24384,\"start\":24383},{\"end\":24392,\"start\":24391},{\"end\":24402,\"start\":24398},{\"end\":24691,\"start\":24690},{\"end\":24701,\"start\":24700},{\"end\":24718,\"start\":24717},{\"end\":24727,\"start\":24726},{\"end\":25014,\"start\":25013},{\"end\":25021,\"start\":25020},{\"end\":25027,\"start\":25026},{\"end\":25271,\"start\":25270},{\"end\":25280,\"start\":25279},{\"end\":25293,\"start\":25292},{\"end\":25511,\"start\":25510},{\"end\":25513,\"start\":25512},{\"end\":25520,\"start\":25519},{\"end\":25528,\"start\":25527},{\"end\":25530,\"start\":25529},{\"end\":25857,\"start\":25856},{\"end\":25864,\"start\":25863},{\"end\":25872,\"start\":25871},{\"end\":26132,\"start\":26131},{\"end\":26139,\"start\":26138},{\"end\":26150,\"start\":26149},{\"end\":26152,\"start\":26151},{\"end\":26161,\"start\":26160},{\"end\":26455,\"start\":26454},{\"end\":26463,\"start\":26462},{\"end\":26470,\"start\":26469},{\"end\":26776,\"start\":26775},{\"end\":26791,\"start\":26790},{\"end\":26805,\"start\":26804},{\"end\":27103,\"start\":27102},{\"end\":27112,\"start\":27111},{\"end\":27327,\"start\":27326},{\"end\":27333,\"start\":27332},{\"end\":27340,\"start\":27339},{\"end\":27621,\"start\":27620},{\"end\":27623,\"start\":27622},{\"end\":27631,\"start\":27630},{\"end\":27835,\"start\":27834},{\"end\":27849,\"start\":27848},{\"end\":27861,\"start\":27860},{\"end\":28033,\"start\":28032},{\"end\":28040,\"start\":28039},{\"end\":28148,\"start\":28147},{\"end\":28155,\"start\":28154},{\"end\":28162,\"start\":28161},{\"end\":28336,\"start\":28335},{\"end\":28344,\"start\":28343},{\"end\":28528,\"start\":28527},{\"end\":28539,\"start\":28535},{\"end\":28549,\"start\":28545},{\"end\":28555,\"start\":28554},{\"end\":28557,\"start\":28556},{\"end\":28776,\"start\":28775},{\"end\":28783,\"start\":28782},{\"end\":28791,\"start\":28790},{\"end\":29066,\"start\":29065},{\"end\":29073,\"start\":29072},{\"end\":29081,\"start\":29080},{\"end\":29400,\"start\":29399},{\"end\":29402,\"start\":29401},{\"end\":29412,\"start\":29408},{\"end\":29419,\"start\":29418},{\"end\":29426,\"start\":29425},{\"end\":29428,\"start\":29427},{\"end\":29685,\"start\":29684},{\"end\":29696,\"start\":29695},{\"end\":29698,\"start\":29697},{\"end\":29707,\"start\":29706},{\"end\":29917,\"start\":29916},{\"end\":29919,\"start\":29918},{\"end\":29928,\"start\":29927},{\"end\":29939,\"start\":29938},{\"end\":30157,\"start\":30156},{\"end\":30159,\"start\":30158},{\"end\":30167,\"start\":30166},{\"end\":30169,\"start\":30168},{\"end\":30179,\"start\":30178},{\"end\":30181,\"start\":30180},{\"end\":30370,\"start\":30369},{\"end\":30376,\"start\":30375},{\"end\":30384,\"start\":30383},{\"end\":30553,\"start\":30549},{\"end\":30559,\"start\":30558},{\"end\":30569,\"start\":30565},{\"end\":30849,\"start\":30848},{\"end\":30856,\"start\":30855},{\"end\":30864,\"start\":30863},{\"end\":31147,\"start\":31146},{\"end\":31154,\"start\":31153},{\"end\":31163,\"start\":31162},{\"end\":31396,\"start\":31395},{\"end\":31406,\"start\":31405},{\"end\":31415,\"start\":31414},{\"end\":31629,\"start\":31628},{\"end\":31637,\"start\":31636},{\"end\":31643,\"start\":31642},{\"end\":31836,\"start\":31835},{\"end\":31844,\"start\":31843},{\"end\":31851,\"start\":31850},{\"end\":31858,\"start\":31857},{\"end\":32058,\"start\":32057},{\"end\":32066,\"start\":32065},{\"end\":32074,\"start\":32073},{\"end\":32255,\"start\":32254},{\"end\":32263,\"start\":32262},{\"end\":32271,\"start\":32270}]", "bib_author_last_name": "[{\"end\":21293,\"start\":21289},{\"end\":21302,\"start\":21299},{\"end\":21310,\"start\":21306},{\"end\":21605,\"start\":21600},{\"end\":21612,\"start\":21609},{\"end\":21619,\"start\":21616},{\"end\":21869,\"start\":21866},{\"end\":21879,\"start\":21876},{\"end\":22122,\"start\":22116},{\"end\":22139,\"start\":22126},{\"end\":22147,\"start\":22143},{\"end\":22422,\"start\":22418},{\"end\":22430,\"start\":22426},{\"end\":22439,\"start\":22434},{\"end\":22718,\"start\":22716},{\"end\":22725,\"start\":22722},{\"end\":22732,\"start\":22729},{\"end\":22969,\"start\":22967},{\"end\":22975,\"start\":22973},{\"end\":22984,\"start\":22979},{\"end\":23275,\"start\":23271},{\"end\":23285,\"start\":23282},{\"end\":23293,\"start\":23289},{\"end\":23302,\"start\":23300},{\"end\":23736,\"start\":23731},{\"end\":23742,\"start\":23740},{\"end\":23748,\"start\":23746},{\"end\":24046,\"start\":24042},{\"end\":24056,\"start\":24053},{\"end\":24063,\"start\":24060},{\"end\":24389,\"start\":24385},{\"end\":24396,\"start\":24393},{\"end\":24407,\"start\":24403},{\"end\":24698,\"start\":24692},{\"end\":24715,\"start\":24702},{\"end\":24724,\"start\":24719},{\"end\":24737,\"start\":24728},{\"end\":25018,\"start\":25015},{\"end\":25024,\"start\":25022},{\"end\":25034,\"start\":25028},{\"end\":25277,\"start\":25272},{\"end\":25290,\"start\":25281},{\"end\":25301,\"start\":25294},{\"end\":25517,\"start\":25514},{\"end\":25525,\"start\":25521},{\"end\":25536,\"start\":25531},{\"end\":25861,\"start\":25858},{\"end\":25869,\"start\":25865},{\"end\":25875,\"start\":25873},{\"end\":26136,\"start\":26133},{\"end\":26147,\"start\":26140},{\"end\":26158,\"start\":26153},{\"end\":26171,\"start\":26162},{\"end\":26460,\"start\":26456},{\"end\":26467,\"start\":26464},{\"end\":26474,\"start\":26471},{\"end\":26788,\"start\":26777},{\"end\":26802,\"start\":26792},{\"end\":26814,\"start\":26806},{\"end\":27109,\"start\":27104},{\"end\":27120,\"start\":27113},{\"end\":27330,\"start\":27328},{\"end\":27337,\"start\":27334},{\"end\":27345,\"start\":27341},{\"end\":27628,\"start\":27624},{\"end\":27639,\"start\":27632},{\"end\":27846,\"start\":27836},{\"end\":27858,\"start\":27850},{\"end\":27870,\"start\":27862},{\"end\":28037,\"start\":28034},{\"end\":28043,\"start\":28041},{\"end\":28152,\"start\":28149},{\"end\":28159,\"start\":28156},{\"end\":28167,\"start\":28163},{\"end\":28341,\"start\":28337},{\"end\":28347,\"start\":28345},{\"end\":28533,\"start\":28529},{\"end\":28543,\"start\":28540},{\"end\":28552,\"start\":28550},{\"end\":28563,\"start\":28558},{\"end\":28780,\"start\":28777},{\"end\":28788,\"start\":28784},{\"end\":28798,\"start\":28792},{\"end\":29070,\"start\":29067},{\"end\":29078,\"start\":29074},{\"end\":29088,\"start\":29082},{\"end\":29406,\"start\":29403},{\"end\":29416,\"start\":29413},{\"end\":29423,\"start\":29420},{\"end\":29434,\"start\":29429},{\"end\":29693,\"start\":29686},{\"end\":29704,\"start\":29699},{\"end\":29717,\"start\":29708},{\"end\":29925,\"start\":29920},{\"end\":29936,\"start\":29929},{\"end\":29949,\"start\":29940},{\"end\":30164,\"start\":30160},{\"end\":30176,\"start\":30170},{\"end\":30184,\"start\":30182},{\"end\":30373,\"start\":30371},{\"end\":30381,\"start\":30377},{\"end\":30388,\"start\":30385},{\"end\":30556,\"start\":30554},{\"end\":30563,\"start\":30560},{\"end\":30572,\"start\":30570},{\"end\":30853,\"start\":30850},{\"end\":30861,\"start\":30857},{\"end\":30875,\"start\":30865},{\"end\":31151,\"start\":31148},{\"end\":31160,\"start\":31155},{\"end\":31168,\"start\":31164},{\"end\":31403,\"start\":31397},{\"end\":31412,\"start\":31407},{\"end\":31421,\"start\":31416},{\"end\":31634,\"start\":31630},{\"end\":31640,\"start\":31638},{\"end\":31649,\"start\":31644},{\"end\":31841,\"start\":31837},{\"end\":31848,\"start\":31845},{\"end\":31855,\"start\":31852},{\"end\":31868,\"start\":31859},{\"end\":32063,\"start\":32059},{\"end\":32071,\"start\":32067},{\"end\":32078,\"start\":32075},{\"end\":32260,\"start\":32256},{\"end\":32268,\"start\":32264},{\"end\":32276,\"start\":32272}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":218487178},\"end\":21539,\"start\":21159},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":214667019},\"end\":21776,\"start\":21541},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":221093963},\"end\":22054,\"start\":21778},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":46954166},\"end\":22314,\"start\":22056},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":67788135},\"end\":22638,\"start\":22316},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":226202997},\"end\":22903,\"start\":22640},{\"attributes\":{\"doi\":\"arXiv:1705.02304\",\"id\":\"b6\"},\"end\":23149,\"start\":22905},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":53027466},\"end\":23642,\"start\":23151},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":145873753},\"end\":23928,\"start\":23644},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":119114892},\"end\":24277,\"start\":23930},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":226202021},\"end\":24614,\"start\":24279},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6206708},\"end\":24934,\"start\":24616},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":52190317},\"end\":25211,\"start\":24936},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4407761},\"end\":25458,\"start\":25213},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":221112108},\"end\":25752,\"start\":25460},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4881455},\"end\":26065,\"start\":25754},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":67856245},\"end\":26351,\"start\":26067},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":195218675},\"end\":26668,\"start\":26353},{\"attributes\":{\"id\":\"b18\"},\"end\":27031,\"start\":26670},{\"attributes\":{\"id\":\"b19\"},\"end\":27275,\"start\":27033},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":57375753},\"end\":27552,\"start\":27277},{\"attributes\":{\"doi\":\"arXiv:1609.02907\",\"id\":\"b21\"},\"end\":27806,\"start\":27554},{\"attributes\":{\"doi\":\"arXiv:1710.10903\",\"id\":\"b22\"},\"end\":28016,\"start\":27808},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":153311899},\"end\":28115,\"start\":28018},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":119314157},\"end\":28265,\"start\":28117},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":211031192},\"end\":28474,\"start\":28267},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":225040053},\"end\":28729,\"start\":28476},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":233181886},\"end\":28941,\"start\":28731},{\"attributes\":{\"doi\":\"arXiv:2107.12710\",\"id\":\"b28\"},\"end\":29322,\"start\":28943},{\"attributes\":{\"doi\":\"arXiv:2009.14153\",\"id\":\"b29\"},\"end\":29627,\"start\":29324},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":10475843},\"end\":29877,\"start\":29629},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":49211906},\"end\":30089,\"start\":29879},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":16489696},\"end\":30334,\"start\":30091},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":140309863},\"end\":30488,\"start\":30336},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":145978146},\"end\":30743,\"start\":30490},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":235780809},\"end\":31083,\"start\":30745},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":235377451},\"end\":31323,\"start\":31085},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":202786778},\"end\":31550,\"start\":31325},{\"attributes\":{\"id\":\"b38\"},\"end\":31768,\"start\":31552},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":8923541},\"end\":32015,\"start\":31770},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":23083969},\"end\":32191,\"start\":32017},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":68589},\"end\":32411,\"start\":32193}]", "bib_title": "[{\"end\":21285,\"start\":21159},{\"end\":21594,\"start\":21541},{\"end\":21859,\"start\":21778},{\"end\":22112,\"start\":22056},{\"end\":22414,\"start\":22316},{\"end\":22712,\"start\":22640},{\"end\":23267,\"start\":23151},{\"end\":23727,\"start\":23644},{\"end\":24038,\"start\":23930},{\"end\":24381,\"start\":24279},{\"end\":24688,\"start\":24616},{\"end\":25011,\"start\":24936},{\"end\":25268,\"start\":25213},{\"end\":25508,\"start\":25460},{\"end\":25854,\"start\":25754},{\"end\":26129,\"start\":26067},{\"end\":26452,\"start\":26353},{\"end\":26773,\"start\":26670},{\"end\":27324,\"start\":27277},{\"end\":28030,\"start\":28018},{\"end\":28145,\"start\":28117},{\"end\":28333,\"start\":28267},{\"end\":28525,\"start\":28476},{\"end\":28773,\"start\":28731},{\"end\":29682,\"start\":29629},{\"end\":29914,\"start\":29879},{\"end\":30154,\"start\":30091},{\"end\":30367,\"start\":30336},{\"end\":30547,\"start\":30490},{\"end\":30846,\"start\":30745},{\"end\":31144,\"start\":31085},{\"end\":31393,\"start\":31325},{\"end\":31833,\"start\":31770},{\"end\":32055,\"start\":32017},{\"end\":32252,\"start\":32193}]", "bib_author": "[{\"end\":21295,\"start\":21287},{\"end\":21304,\"start\":21295},{\"end\":21312,\"start\":21304},{\"end\":21607,\"start\":21596},{\"end\":21614,\"start\":21607},{\"end\":21621,\"start\":21614},{\"end\":21871,\"start\":21861},{\"end\":21881,\"start\":21871},{\"end\":22124,\"start\":22114},{\"end\":22141,\"start\":22124},{\"end\":22149,\"start\":22141},{\"end\":22424,\"start\":22416},{\"end\":22432,\"start\":22424},{\"end\":22441,\"start\":22432},{\"end\":22720,\"start\":22714},{\"end\":22727,\"start\":22720},{\"end\":22734,\"start\":22727},{\"end\":22971,\"start\":22965},{\"end\":22977,\"start\":22971},{\"end\":22986,\"start\":22977},{\"end\":23277,\"start\":23269},{\"end\":23287,\"start\":23277},{\"end\":23295,\"start\":23287},{\"end\":23304,\"start\":23295},{\"end\":23738,\"start\":23729},{\"end\":23744,\"start\":23738},{\"end\":23750,\"start\":23744},{\"end\":24048,\"start\":24040},{\"end\":24058,\"start\":24048},{\"end\":24065,\"start\":24058},{\"end\":24391,\"start\":24383},{\"end\":24398,\"start\":24391},{\"end\":24409,\"start\":24398},{\"end\":24700,\"start\":24690},{\"end\":24717,\"start\":24700},{\"end\":24726,\"start\":24717},{\"end\":24739,\"start\":24726},{\"end\":25020,\"start\":25013},{\"end\":25026,\"start\":25020},{\"end\":25036,\"start\":25026},{\"end\":25279,\"start\":25270},{\"end\":25292,\"start\":25279},{\"end\":25303,\"start\":25292},{\"end\":25519,\"start\":25510},{\"end\":25527,\"start\":25519},{\"end\":25538,\"start\":25527},{\"end\":25863,\"start\":25856},{\"end\":25871,\"start\":25863},{\"end\":25877,\"start\":25871},{\"end\":26138,\"start\":26131},{\"end\":26149,\"start\":26138},{\"end\":26160,\"start\":26149},{\"end\":26173,\"start\":26160},{\"end\":26462,\"start\":26454},{\"end\":26469,\"start\":26462},{\"end\":26476,\"start\":26469},{\"end\":26790,\"start\":26775},{\"end\":26804,\"start\":26790},{\"end\":26816,\"start\":26804},{\"end\":27111,\"start\":27102},{\"end\":27122,\"start\":27111},{\"end\":27332,\"start\":27326},{\"end\":27339,\"start\":27332},{\"end\":27347,\"start\":27339},{\"end\":27630,\"start\":27620},{\"end\":27641,\"start\":27630},{\"end\":27848,\"start\":27834},{\"end\":27860,\"start\":27848},{\"end\":27872,\"start\":27860},{\"end\":28039,\"start\":28032},{\"end\":28045,\"start\":28039},{\"end\":28154,\"start\":28147},{\"end\":28161,\"start\":28154},{\"end\":28169,\"start\":28161},{\"end\":28343,\"start\":28335},{\"end\":28349,\"start\":28343},{\"end\":28535,\"start\":28527},{\"end\":28545,\"start\":28535},{\"end\":28554,\"start\":28545},{\"end\":28565,\"start\":28554},{\"end\":28782,\"start\":28775},{\"end\":28790,\"start\":28782},{\"end\":28800,\"start\":28790},{\"end\":29072,\"start\":29065},{\"end\":29080,\"start\":29072},{\"end\":29090,\"start\":29080},{\"end\":29408,\"start\":29399},{\"end\":29418,\"start\":29408},{\"end\":29425,\"start\":29418},{\"end\":29436,\"start\":29425},{\"end\":29695,\"start\":29684},{\"end\":29706,\"start\":29695},{\"end\":29719,\"start\":29706},{\"end\":29927,\"start\":29916},{\"end\":29938,\"start\":29927},{\"end\":29951,\"start\":29938},{\"end\":30166,\"start\":30156},{\"end\":30178,\"start\":30166},{\"end\":30186,\"start\":30178},{\"end\":30375,\"start\":30369},{\"end\":30383,\"start\":30375},{\"end\":30390,\"start\":30383},{\"end\":30558,\"start\":30549},{\"end\":30565,\"start\":30558},{\"end\":30574,\"start\":30565},{\"end\":30855,\"start\":30848},{\"end\":30863,\"start\":30855},{\"end\":30877,\"start\":30863},{\"end\":31153,\"start\":31146},{\"end\":31162,\"start\":31153},{\"end\":31170,\"start\":31162},{\"end\":31405,\"start\":31395},{\"end\":31414,\"start\":31405},{\"end\":31423,\"start\":31414},{\"end\":31636,\"start\":31628},{\"end\":31642,\"start\":31636},{\"end\":31651,\"start\":31642},{\"end\":31843,\"start\":31835},{\"end\":31850,\"start\":31843},{\"end\":31857,\"start\":31850},{\"end\":31870,\"start\":31857},{\"end\":32065,\"start\":32057},{\"end\":32073,\"start\":32065},{\"end\":32080,\"start\":32073},{\"end\":32262,\"start\":32254},{\"end\":32270,\"start\":32262},{\"end\":32278,\"start\":32270}]", "bib_venue": "[{\"end\":21329,\"start\":21312},{\"end\":21638,\"start\":21621},{\"end\":21898,\"start\":21881},{\"end\":22167,\"start\":22149},{\"end\":22459,\"start\":22441},{\"end\":22751,\"start\":22734},{\"end\":22963,\"start\":22905},{\"end\":23376,\"start\":23304},{\"end\":23768,\"start\":23750},{\"end\":24082,\"start\":24065},{\"end\":24426,\"start\":24409},{\"end\":24756,\"start\":24739},{\"end\":25053,\"start\":25036},{\"end\":25320,\"start\":25303},{\"end\":25589,\"start\":25538},{\"end\":25894,\"start\":25877},{\"end\":26191,\"start\":26173},{\"end\":26493,\"start\":26476},{\"end\":26833,\"start\":26816},{\"end\":27100,\"start\":27033},{\"end\":27404,\"start\":27347},{\"end\":27618,\"start\":27554},{\"end\":27832,\"start\":27808},{\"end\":28055,\"start\":28045},{\"end\":28179,\"start\":28169},{\"end\":28359,\"start\":28349},{\"end\":28583,\"start\":28565},{\"end\":28817,\"start\":28800},{\"end\":29063,\"start\":28943},{\"end\":29397,\"start\":29324},{\"end\":29736,\"start\":29719},{\"end\":29968,\"start\":29951},{\"end\":30196,\"start\":30186},{\"end\":30400,\"start\":30390},{\"end\":30592,\"start\":30574},{\"end\":30895,\"start\":30877},{\"end\":31187,\"start\":31170},{\"end\":31427,\"start\":31423},{\"end\":31626,\"start\":31552},{\"end\":31880,\"start\":31870},{\"end\":32090,\"start\":32080},{\"end\":32288,\"start\":32278},{\"end\":21342,\"start\":21331},{\"end\":21651,\"start\":21640},{\"end\":21911,\"start\":21900},{\"end\":22181,\"start\":22169},{\"end\":22473,\"start\":22461},{\"end\":22764,\"start\":22753},{\"end\":23782,\"start\":23770},{\"end\":24095,\"start\":24084},{\"end\":24439,\"start\":24428},{\"end\":24769,\"start\":24758},{\"end\":25066,\"start\":25055},{\"end\":25333,\"start\":25322},{\"end\":25907,\"start\":25896},{\"end\":26205,\"start\":26193},{\"end\":26506,\"start\":26495},{\"end\":26846,\"start\":26835},{\"end\":28061,\"start\":28057},{\"end\":28185,\"start\":28181},{\"end\":28365,\"start\":28361},{\"end\":28597,\"start\":28585},{\"end\":28830,\"start\":28819},{\"end\":29749,\"start\":29738},{\"end\":29981,\"start\":29970},{\"end\":30202,\"start\":30198},{\"end\":30406,\"start\":30402},{\"end\":30606,\"start\":30594},{\"end\":30909,\"start\":30897},{\"end\":31200,\"start\":31189},{\"end\":31886,\"start\":31882},{\"end\":32096,\"start\":32092},{\"end\":32294,\"start\":32290}]"}}}, "year": 2023, "month": 12, "day": 17}