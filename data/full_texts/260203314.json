{"id": 260203314, "updated": "2023-12-11 09:54:38.729", "metadata": {"title": "Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space", "authors": "[{\"first\":\"Eduardo\",\"last\":\"Montesuma\",\"middle\":[\"Fernandes\"]},{\"first\":\"Fred\",\"last\":\"Mboula\",\"middle\":[\"Ngole\"]},{\"first\":\"Antoine\",\"last\":\"Souloumiac\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification performance. Finally, we show that interpolations in the Wasserstein hull of learned atoms provide data that can generalize to the target domain.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ecai/MontesumaMS23", "doi": "10.3233/faia230459"}}, "content": {"source": {"pdf_hash": "46bb49d5f9a53571796333146d2edf57b821f0ac", "pdf_src": "ArXiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2307.14953v3.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBYNC", "open_access_url": "https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230459", "status": "HYBRID"}}, "grobid": {"id": "9a7432fa54559e31b420db1c0cd8e4621cb8e7eb", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/46bb49d5f9a53571796333146d2edf57b821f0ac.txt", "contents": "\nMULTI-SOURCE DOMAIN ADAPTATION THROUGH DATASET DICTIONARY LEARNING IN WASSERSTEIN SPACE\n8 Nov 2023\n\nEduardo Fernandes Montesuma \nCEA\nList Universit\u00e9 Paris-Saclay\nF-91120PalaiseauFrance\n\nFred Ngol\u00e8 Mboula \nCEA\nList Universit\u00e9 Paris-Saclay\nF-91120PalaiseauFrance\n\nAntoine Souloumiac \nCEA\nList Universit\u00e9 Paris-Saclay\nF-91120PalaiseauFrance\n\nMULTI-SOURCE DOMAIN ADAPTATION THROUGH DATASET DICTIONARY LEARNING IN WASSERSTEIN SPACE\n8 Nov 202337E712FF95430DAEB9F2283E37A8067DarXiv:2307.14953v3[cs.LG]Optimal TransportTransfer LearningDomain AdaptationDictionary Learning\nThis paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain.We propose a novel MSDA framework based on dictionary learning and optimal transport.We interpret each domain in MSDA as an empirical distribution.As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions.We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates.Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions.We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification performance.Finally, we show that interpolations in the Wasserstein hull of learned atoms provide data that can generalize to the target domain 1 .\n\nIntroduction\n\nTraditional Machine Learning (ML) works under the assumption that training and test data follow a single probability distribution.Indeed, the Empirical Risk Minimization (ERM) framework of [37] measures generalization regarding an unknown probability distribution from which training and test data are sampled.Nonetheless, as [25] remarks, this is seldom the case in realistic applications due to changes in how the data is acquired.This results in a change in the data distribution, or distributional shift that motivates the field of Domain Adaptation (DA).\n\nDA is an important framework where one assumes labeled data from a source domain and seeks to adapt models to an unlabeled target domain.When multiple source domains are available, one has a Multi-Source DA (MSDA) setting.This problem is more challenging as one has multiple distributional shifts co-occurring, that is, between sources and between sources and the target.In this work, we assume that the domain shifts have regularities that can be learned and leveraged for MSDA.In this context, Optimal Transport (OT) is a mathematical theory useful for DA, as it allows for the comparison and matching probability distributions.Previous works employed OT for the single-source case, as in [1,2,3], and MSDA as in [4,5,6].\n\nIn parallel, Dictionary Learning (DiL) expresses a set of vectors as weighted combinations of dictionary elements, named atoms.Previous works proposed OT for DiL over histogram data, such as [7] and [8].Nonetheless, when data is high-dimensional, modeling distributions as histograms is intractable due the curse of dimensionality, which limits the use of previous DiL works for MSDA.\n\n\nContributions.\n\nIn this paper we propose a novel DiL framework (section 4), for distributions represented as point clouds.We further explore (section 4.2) two ways of using DiL for MSDA, by reconstructing labeled samples in the target domain, and by ensembling classifiers learned with labeled data from atoms.In addition, we justify these methods theoretically through results in the literature [9, Theorem 2], and through novel theoretical results (i.e., theorem 2).To the best of our knowledge this is the first work to propose a DiL of point clouds, and to explore the connections between DiL of distributions and MSDA.\n\nPaper Organization.Section 2 covers the related literature.Section 3 covers the necessary background, i.e., DA, OT and DiL concepts.Section 4 presents our framework.Section 5 explores our experiments in MSDA.Section 6 discusses our results.Finally, section 7 concludes our paper.\n\n\nRelated Work\n\nThere are mainly two methodologies in DA.The first, shallow DA, leverages pre-existing feature extractors and performs adaptation either by re-weighting or transforming source domain data to resemble target domain data.The second, deep DA, uses source and target domain data during the training of a Deep Neural Net (DNN), so that learned features are independent of distributional shift.\n\nThere are at least 3 classes of shallow DA methods: (i) importance re-weighting strategies [10], which give importance to source samples similar to the target domain, (ii) projection-based methods [11], which seek a sub-space where distributions share common characteristics, and (iii) OT-based methods [1], which use OT for matching, or calculating distances between distributions.\n\nFor deep DA, methods penalize encoder parameters that map source and target distributions to different locations in the latent space.As a consequence, deep DA is more complex than shallow DA, since encoder parameters are free.Examples of deep DA methods include [12], who uses an adversarial loss, and [3,13], who use OT as a loss function between distributions of latent representations.\n\nFor MSDA, some works generalize previous single-source baselines.For instance, [14] proposes a moment-matching strategy across the different domains.[6] proposes weighting source domains linearly, then applying the Joint Distribution Optimal Transport (JDOT) strategy of [2].This approach combines notions of importance weighting and OT-based DA. [4,5] generalize the approach of [1], by first calculating a Wasserstein barycenter of the different source domains, then transporting the barycenter to the target domain.\n\nIn parallel, DiL is a representation learning technique, that was previously used in DA by [15].However, classic DiL lacks a probabilistic interpretation.In this context, OT offers a probabilistic foreground for DiL, when data is represented through histograms.This is done by either using the Sinkhorn divergence [16] as the objective function [7], or by aggregating atoms in a Wasserstein space [8].Nonetheless, in the context of DA it is computationally intractable to bin the feature space, which is commonly high-dimensional.This issue hinders the applicability of previous DiL approaches for DA.In contrast, we propose a new OT-inspired DiL framework for point clouds, which makes it suitable for MSDA.\n\n\nBackground\n\n\nDomain Adaptation\n\nIn ML, learning a classifier consists on estimating h : X \u2192 Y, among a set of functions H, where X (e.g., R d ) is the feature space and Y (e.g., {1, \u2022 \u2022 \u2022 , n c }) is the label space.This estimation is done via risk minimization [17],\nh \u22c6 = argmin h\u2208H R Q (h) = E x\u223cQ [L(h(x), h 0 (x))],(1)\nfor a loss function L, a distribution Q, and a ground-truth labeling function h 0 .R Q is known as true risk.Since Q and h 0 are seldom known a priori, it is unfeasible to directly minimize equation 1.In practice, one acquires a dataset, {(x\n(Q) i , y (Q) i )} n i=1 , with x (Q) i i.i.d. \u223c Q and y (Q) i = h 0 (x (Q) i )\nand minimizes the empirical risk,\n\u0125 = argmin h\u2208H RQ (h) = 1 n n i=1 L(h(x (Q) i ), y (Q) i\n).\n\n\nHenceforth x\n\n(Q) i denotes a feature vector sampled from the marginal Q(X).Likewise, y\n(Q) i\ndenotes its corresponding label.We denote its corresponding one-hot encoding (hard-labels) or probability vector (soft-labels) by y\n(Q) i .\nAs discussed in [18], if training and test data are i.i.d.from Q, R Q \u2192 RQ as n \u2192 \u221e.Nonetheless this assumption is restrictive, as it disregards the distributional heterogeneity within training data, and between train and test data, which motivates DA [11].Following [11], a domain D = (X , Q(X)) is a pair of a feature space, and a feature distribution.In DA, one has different domains, i.e., a labeled source D S with samples {(x\n(Q S ) i , y (Q S ) i )} n Q S i=1 and a target D T with samples {x (Q T ) j } n Q T j=1\n. In practice, one assumes a shared feature space (e.g., R d ), so that domains differ in their distribution, Q S (X) \u0338 = Q T (X).This is known in the literature as distributional shift.The goal of DA is improving performance on the target, given knowledge from the source domain.We investigate MSDA, that is, DA between labeled sources {D S \u2113 } N S \u2113=1 and an unlabeled target D T .\n\n\nOptimal Transport\n\nOT is a field of mathematics widely used in DA and ML.Henceforth we focus on computational OT.We refer readers to [19] and [20] for further background on this topic.Let x\n(P ) i i.i.d. \u223c P (resp. x (Q) j i.i.d.\n\u223c Q).We P and Q empirically using mixtures of Dirac deltas,\nP (x) = 1 n P n P i=1 \u03b4(x \u2212 x (P ) i ).(2)\nWe refer to P as a point cloud, and\nX (P ) = [x (P ) 1 , \u2022 \u2022 \u2022 , x(P )\nn P ] \u2208 R n P \u00d7d to its support.The Kantorovich formulation of OT seeks an OT plan, \u03c0 \u2208 R n P \u00d7n Q that preserves mass,\n\u03a0( P , Q) := {\u03c0 : i \u03c0 i,j = 1 /n Q ; j \u03c0 i,j = 1 /n P }.\nwhere \u03c0 i,j denotes how much mass x\n(P ) i sends to x (Q) j . In this sense, the OT problem between P and Q is, \u03c0 \u22c6 = OT(X (P ) , X (Q) ) = argmin \u03c0\u2208\u03a0( P , Q) \u27e8C, \u03c0\u27e9 F ,(3)\nwhere \u27e8\u2022, \u2022\u27e9 F denotes the Frobenius inner product and C i,j = c(x\n(P ) i , x (Q) j\n) is called ground-cost matrix.This is a linear program on the variables \u03c0 i,j , which has computational complexity O(n 3 log n).Given \u03c0, one often wants to map samples from P into Q, which can be done through the barycentric projection [1],\nT \u03c0 (x (P ) i ) = argmin x\u2208R d n Q j=1 \u03c0 i,j c(x, x (Q) j ).\nWhen c is the Euclidean distance, the barycentric projection has closed form,\nT \u03c0 (x (P ) i ) = n P n Q j=1 \u03c0 i,j x (Q) j ,(4)\nor T \u03c0 (X (P ) ) = n P \u03c0X (Q) in short.\n\nOptimal Transport for Domain Adaptation.In the semminal works of [1], the authors proposed using OT for DA, under the assumption that there is T : R d \u2192 R d such that,\nT \u266f Q S = Q T and Q S (Y |X) = Q T (Y |T (X)),(5)\nwhere T \u266f is the push-forward operator (see e.g., [21]).[1] propose estimating T through T \u03c0 in equation 4, which allows mapping samples from Q S to Q T .\n\nWasserstein Barycenters.When the ground-cost is a distance, OT defines a distance between distributions, W c ( P , Q) = \u27e8C, \u03c0 \u22c6 \u27e9 F , called Wasserstein distance.As such, OT defines barycenters of probability distributions [22].Henceforth we denote the K\u2212simplex as\n\u2206 K = {a \u2208 R K + : k a k = 1}. Definition 1. For distributions P = {P k } K\nk=1 and weights \u03b1 \u2208 \u2206 K , the Wasserstein barycenter is a solution to,\nB \u22c6 = B(\u03b1; P) = inf B K k=1 \u03b1 k W c (P k , B).(6)\nHenceforth we call B(\u2022; P) barycentric operator.In this context, the Wasserstein hull of distributions P is,\nM(P) = {B(\u03b1; P) : \u03b1 \u2208 \u2206 K }(7)\nWhen the distributions in P are empirical, solving equation 6 corresponds to estimating the support X (B) of B.\n\nIn this context, [23] proposed an algorithm known as free-support Wasserstein barycenter for calculating B. Let x (B) i\n\n\u223c N (0, I d ) be an initialization for the barycenter's support.One updates the support of B with,\n\u03c0 (k,it) = OT(X (P k ) , X (B) it ) X (B) it+1 \u2190 \u03b8X (B) it + (1 \u2212 \u03b8) K k=1 \u03b1 k T \u03c0 (k,it) (X (B) it )(8)\nwhere \u03b8 is found at each iteration via line search.In the context of MSDA, [4] previously defined a barycenter of labeled distributions by penalizing transport plans \u03c0 (k,it) that mix classes.\n\nMini-batch OT.For large scale datasets, computing OT is likely unfeasible, due its cubic complexity.A workaround, coming from ML, consists on using mini-batches [24].For M batches of size n b , this approach decreases the time complexity to O(M n 3 b log n b ).Remark on Notation.While W 2 ( P , Q) is defined between empirical distributions, in practice it is a function of (X (P ) , X (Q) ).With an abuse of notation, the mini-batch Wasserstein distance between given random samples of size n b from P and Q is still noted as W 2 ( P , Q), with the support matrices restricted to a mini-batch.\n\n\nDictionary Learning\n\n\nDiL is a representation learning technique that expresses a collection of vectors {x\n\u2113 } N \u2113=1 , x \u2113 \u2208 R d through a set of atoms P = {p k } K k=1 , p k \u2208 R d and weights A = {\u03b1 \u2113 } N \u2113=1 , \u03b1 \u2113 \u2208 R K . Mathematically, argmin P,A 1 N N i=1 L(x \u2113 , P T \u03b1 \u2113 ) + \u03bb A \u2126 A (A) + \u03bb P \u2126 P (P),\nwhere L is a suitable loss, \u2126 A and \u2126 P are regularizing terms on A and P respectively.In this sense, OT has previously contributed to DiL either by defining a meaningful loss function, or novel ways to aggregating atoms.For instance, [7] proposed using the Sinkhorn divergence of [16] as a loss function, while [8] proposed using Wasserstein barycenters for aggregating atoms.These works assume data in the form of histograms, i.e., x \u2113 \u2208 \u2206 d .As consequence, p k \u2208 \u2206 d and \u03b1 \u2113 \u2208 \u2206 K .\n\n\nProposed Framework\n\nIn this section, we present our novel framework for MSDA, called Dataset Dictionary Learning (DaDiL).As our discussion relies on analogies with DiL theory, we provide in Table 1  We propose a novel algorithm for calculating differentiable Wasserstein barycenters of labeled empirical distributions.This algorithm is at the core of DaDiL (section 4), since we later represent datasets as barycenters of learned atoms.\n\nIn OT, there are at least 2 ways of integrating labels, either by penalizing OT plans that transport mass between different classes [1,4], or by defining a metric in the label space [25].We choose to integrate labels in the ground-cost,\nC i,j = \u2225x (P ) i \u2212 x (Q) j \u2225 2 2 + \u03b2\u2225y (P ) i \u2212 y (Q) j \u2225 2 2 ,(9)\nAlgorithm 1 Labeled Wasserstein Barycenter\nInput: {X (P k ) , Y (P k ) } K k=1 , \u03b1 \u2208 \u2206 K , \u03c4 > 0, N itb . 1: for i = 1, \u2022 \u2022 \u2022 , n B do 2: x (B) i \u223c N (0, I d ), y (B) i = randint(n c ) 3: end for 4: while |J it \u2212 J it\u22121 | \u2265 \u03c4 and it \u2264 N itb do 5: for k = 1, \u2022 \u2022 \u2022 K do 6: \u03c0 (k,it) = OT (X (P k ) , Y (P k ) ); (X (B) it , Y (B) it ) 7:\nend for 8:\nJ it = K k=1 \u03b1 k \u27e8\u03c0 (k,it) , C (k) \u27e9 F 9: X (B) it+1 = K k=1 \u03b1 k T \u03c0 (k,it) (X (B) it ) 10: Y (B) it+1 = K k=1 \u03b1 k T \u03c0 (k,it) (Y (B) it ) 11: end while Output: Labeled barycenter support (X (B) , Y (B) ).\nwhere y denotes labels one-hot encoding, and \u03b2 > 0 controls the importance of label discrepancy.While simple, this choice allows us to motivate the barycentric projection of [1], and the label propagation of [26] as first-order optimality conditions of W c ( P , Q),\nx(P ) i = T \u03c0 (x (P ) i ) = n P n Q j=1 \u03c0 i,j x (Q) j , \u0177(P ) i = T \u03c0 (y (P ) i ) = n P n Q j=1 \u03c0 i,j y (Q) j .(10)\nHenceforth we denote,\n\u03c0 = OT (X (P ) , Y (P ) ); (X (Q) , Y (Q) ) .\nAs a consequence, we can interpolate between two point clouds, since \u0177(P ) i corresponds to a soft-label (i.e., probabilities).We use equations 9 and 10 for proposing a new barycenter strategy between labeled point clouds, shown in algorithm 1.\n\nDifferentiation.For calculating derivatives of x (B) i and y\n(B) i w.r.t. x (P k ) l , y (P k ) l\n, and \u03b1, we use the Envelope theorem of [27].In other words, we do not propagate derivatives through the iterations of algorithm 1.We provide further details in our appendix.\n\nComputational Complexity.Let Pk have n points in its support, for k = 1, \u2022 \u2022 \u2022 , K. The complexity of algorithm 1 is dominated by line 6, which has complexity O(n 3 log n).Hence, the overall computational complexity is O(N itb Kn 3 log n).\n\n\nDataset Dictionary Learning for MSDA\n\nIn this section, we introduce our novel framework, called DaDiL, and explore how to use it for MSDA.Let Q = { QS \u2113 } N S \u2113=1 \u222a { QT } correspond to N S labeled sources and an unlabeled target.\nLet A = [\u03b1 1 , \u2022 \u2022 \u2022 , \u03b1 N S , \u03b1 N S +1\n], and P = { Pk } K k=1 .The Pk 's are an empirical approximation of the point clouds that interpolate distributional shift.Following our notation, \u03b1 T := \u03b1 N S +1 .For N = N S + 1, DaDiL consists on minimizing,\n(P \u22c6 , A \u22c6 ) = argmin P,A\u2208(\u2206 K ) N 1 N N \u2113=1 L( Q\u2113 , B(\u03b1 \u2113 ; P)),(11)\nwhere L is a loss between distributions.Since the target domain is not labeled, we define,\nL( Q\u2113 , B\u2113 ) = W c ( Q\u2113 , B\u2113 ), if Q\u2113 is labeled, W 2 ( Q\u2113 , B\u2113 ), otherwise,\ni.e., when no labels in Q\u2113 are available, we minimize the standard 2-Wasserstein distance.Optimizing 11 over entire datasets might be intractable due the complexity of OT.We thus employ mini-batch OT [24].In addition, we need to enforce the constraints y\n(P k ) l \u2208 \u2206 nc and \u03b1 \u2113 \u2208 \u2206 K .\nIn the first case we do a change of variables, and optimize the logits p \u2208 R nc s.t.y = softmax(p).In the second case, we project \u03b1 \u2113 into the simplex orthogonally,\nproj \u2206 K (\u03b1 \u2113 ) = argmin \u03b1\u2208\u2206 K \u2225\u03b1 \u2212 \u03b1 \u2113 \u2225 2 .\nThe overall optimization algorithm is shown in algorithm 2.\n\nAlgorithm 2 DaDiL learning loop.\nInput: Q = { Q\u2113 } N \u2113=1\n, number of iterations Niter, of atoms K, of batches M , batch size n b , learning rate \u03b7.1: Initialize x\n(P k ) j \u223c N (0, I d ), a \u2113 \u223c N (0, IK ). 2: for it = 1 \u2022 \u2022 \u2022 , Niter do 3: for batch = 1, \u2022 \u2022 \u2022 , M do 4: for \u2113 = 1, \u2022 \u2022 \u2022 , (NS + 1) do 5:\nSample {x\n(Q \u2113 ) 1 , \u2022 \u2022 \u2022 , x (Q \u2113 ) n b }. 6:\nif Q\u2113 is labeled then 7:\n\nSample {y\n(Q \u2113 ) 1 , \u2022 \u2022 \u2022 , y (Q \u2113 ) n b }. 8: end if 9: for k = 1, \u2022 \u2022 \u2022 , K do 10:\nsample {(x\n(P k ) 1\n, p\n(P k ) 1\n), \u2022 \u2022 \u2022 , (x\n(P k ) n b , p (P k ) n b )}, 11:\nchange variables y\n(P k ) j = softmax(p (P k ) j ) 12:\nend for 13:\n\ncalculate\nX (B \u2113 ) , Y (B \u2113 ) = B(\u03b1 \u2113 ; P) 14:\nend for 15:\nL = ( 1 /N) N \u2113=1 L( Q\u2113 , B\u2113 ) 16:\nx\n(P k ) j \u2190 x (P k ) j \u2212 \u03b7 \u2202L /\u2202x (P k ) j 17: p (P k ) j \u2190 p (P k ) j \u2212 \u03b7 \u2202L /\u2202p (P k ) j 18: \u03b1 \u2113 \u2190 proj \u2206 K (\u03b1 \u2113 \u2212 \u03b7 \u2202L /\u2202\u03b1 \u2113 ). 19:\nend for 20: end for Output: Dictionary P \u22c6 and weights A \u22c6 .Intuition.We learn how to express each distribution Q\u2113 \u2208 Q as a barycenter of free distributions P = { Pk } K k=1 , parametrized by their support i.e., (X (P k ) , Y (P k ) ).In other words, we learn P s.t.Q is contained in the Wasserstein hull of atoms, M(P).\n\nImplementation.We implement algorithms 1 and 2 using Pytorch [28] and Python Optimal Transport (POT) [29], for automatic differentiation and OT details respectively.As previous works [4,6], DaDiL is applied to the latent space of an encoder, pre-trained on source domain data, as shown in figure 1.\n\nComputational Complexity.In algorithm 2, the complexity of line 13 dominates over other lines.As we discussed in section 4.1, the complexity of calculating B(\u03b1 \u2113 ; P) depends on the size of distributions support.Since we do computations using mini-batches, this corresponds to O(N itb n 3 b log n b ).This is repeated for\nN iter \u00d7 M \u00d7 (N S + 1), which implies a complexity of O(N iter M N S N itb n 3 b log n b ).\nMulti-Source Domain Adaptation.We recast the hypothesis in eq. 5 for MSDA.We assume the existence of K > 1 unknown distributions, P 1 , \u2022 \u2022 \u2022 , P K for which Q \u2113 can be approximated as their interpolation in Wasserstein space, i.e.\nQ \u2113 = T \u266f B \u2113 , and Q \u2113 (Y |X) = B \u2113 (Y |T (X)),\nfor B \u2113 = B(\u03b1 \u2113 ; P) and a possibly non-linear transformation\nT . If W c (Q \u2113 , B \u2113 ) \u2248 0 we can assume T (x) = x.\nWe start by learning (P, A), as illustrated in figure 2.Then, we propose 2 ways of using our dictionary for MSDA.Our first strategy, called DaDiL-R, consists on computing BT = B(\u03b1 T ; P), i.e., the distribution in M(P) closest to QT .Since each Pk has a labeled support, algorithm 1 yields  matrices X (B T ) and Y (B T ) corresponding to the support of BT .Then,\n\u0125R = argmin h\u2208H RB T (h) = 1 n n i=1 L(h(x (B T ) i ), y (B T ) i )\nWe theoretically justify it using Theorem 2 of [9],\n\nTheorem 1. (Due to [9]) Let X (P ) \u2208 R n P \u00d7d and X (Q) \u2208 R n Q \u00d7d be i.i.d.samples from P and Q.Then, for any d \u2032 > d and \u03be \u2032 < \u221a 2 there exists some constant n 0 depending on d \u2032 s.t. for \u03b4 \u2208 (0, 1) and min(n\nP , n Q ) \u2265 n 0 max(\u03b4 \u2212(d+2) , 1) with probability at least 1 \u2212 \u03b4 for all h, R Q (h) \u2264 R P (h) + W 2 ( P , Q) + \u03b6 + \u03bb,\nwhere,\n\u03b6 = 2 (log 1 /\u03b4) /\u03be \u2032 1 /n P + 1 /n Q , \u03bb = min h\u2208H R Q (h) + R P (h).\nAdditional discussion on this result is provided in our appendix.We apply this result for the residual shift W 2 ( QT , BT ),\nR Q T (h) \u2264 R B T (h) + W 2 ( QT , BT ) + \u03b6 + \u03bb.(12)\nAs discussed in [9], 3 factors play a role in the success of DA, namely, W 2 ( P , Q), R B T (h), and \u03bb.The first term is the reconstruction error, and is directly minimized in algorithm 2. The second term is the risk of h in B T , which is minimized when learning the classifier \u0125R = argmin RB T (h).This term depends on the separability of classes in BT , which is enforced by considering labels in the ground-cost (eqn.9).The last term is the joint risk \u03bb of a classifier learned with data from Q T and B T .This term is difficult to bound, as no labels in QT are available, but, under the hypothesis\nQ T (Y |X) = B T (Y |T (X)\n), this term is low.This was similarly assumed by [1,9].DaDiL-R is illustrated in figure 3a.\n\nOur second strategy, called DaDiL-E, is based on ensembling.Since each of our atoms is labeled, i.e., each x\n(P k ) i has an associated y (P k ) i\n, we may learn a set of K classifiers, \u0125k = argmin h\u2208H RP k (h), one for each atom.Naturally, one may use \u03b1 \u2208 \u2206 K for weighting predictions of atom classifiers.We weight the \u0125k 's using \u03b1 T , which is theoretically justified in theorem 2, \u0125E (x\n(Q T ) j ) = K k=1 \u03b1 T,k \u0125k (x (Q T ) j ), Theorem 2. Let {X (P k ) } K k=1 , X (P k ) \u2208 R n k \u00d7d and X (Q T ) \u2208 R n T \u00d7d be i.i.d. samples from P k and Q T . Let \u0125k be the minimizer of R P k and R \u03b1 (h) = K k=1 \u03b1 k R P k (h).\nUnder the same conditions of theorem 1, and for \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4, the following holds,\nR Q T ( \u0125\u03b1 ) \u2264 R \u03b1 ( \u0125\u03b1 ) + W 2 (B(\u03b1; P), QT ) + \u03b3 + \u03bb + \u03b6, \u03b3 = K k=1 \u03b1 k W 2 ( Pk , B(\u03b1; P)), \u03b6 = K k=1 \u03b1 k 2 log 1 /\u03b4 /\u03be \u2032 1 /n k + 1 /n T , \u03bb = K k=1 \u03b1 k min h\u2208H R P k (h) + R Q T (h) .\nWe provide the proof of this result and additional discussion in our appendix.This bound depends on different terms.First, \u03b3 is, for a given \u03b1, minimal, as B(\u03b1; P) is the minimizer of B \u2192 k \u03b1 k W 2 ( Pk , B). \u03bb corresponds to the complexity of domain adaptation, and in general cannot be directly controlled due the unavailability of labels in QT .Finally, \u03be corresponds to the sample complexity of estimating W 2 (P k , Q T ) via finite samples.Note that \u03b1 T minimizes the terms in the r.h.s., as, by design, it minimizes the term \u03b1 \u2192 W 2 (B(\u03b1; P), QT ).DaDiL-E is illustrated in figure 3b.The complexity of our model is controlled through n and K.We provide further analysis on the robustness w.r.t.hyper-parameter choice, as well as the full set of chosen hyper-parameters in our appendix.For other algorithms from the State-of-the-Art (SOTA), we use the best hyper-parameter settings reported by their respective authors.\n\nCaltech-Office 10 is a benchmark consisting on the intersection of the Caltech 256 dataset of [30] and the Office 31 dataset of [31].It has 4 domains: Amazon (A), dSLR (D), Webcam (W) and Caltech (C).In this benchmark we compare DaDiL with other shallow DA algorithms, such as: (i) Subspace Alignment (SA) of [32]; (ii) Transfer Component Analysis (TCA) of [33]; (iii) Optimal Transport Domain Adaptation (OTDA) of [1]; (iv) Wasserstein Barycenter Transport (WBT) of [4,5]; (v) Weighted JDOT (WJDOT) of [6].(i) and (ii) are standard algorithms in DA, (iii) is Ablation Study.We investigate the effectiveness of DiL in comparison with other barycenter-based approaches.As follows, we compare the performance on the Caltech-Office 10 benchmark of 4 methods: (i) Wasserstein Barycenter (WB); (ii) WBT; (iii) Wasserstein Barycentric Coordinates Regression (WBR)-R and WBR-E, which can be understood as the adaptation of the framework of [34] for point clouds.The R and E methods are analogous to DaDiL when the atoms are initialized and fixed as the source domains.We provide further details of this adaptation in our appendix.\n\nWe report our findings in table 3. Overall, WB and WBR have sub-optimal performance.On the one hand, this implies that QT \u0338 \u2208 M(Q S ).On the other hand, this implies that DiL is key for MSDA.Indeed, since Pk \u2208 P are free, DaDiL learns P s.t.QT \u2208 M(P).WBT reg compensates this fact by transporting the B towards QT , thus minimizing the residual shift W 2 ( B, PT ).\n\nRefurbished Office 31.In this experiment, we use the Office 31 benchmark of [31], with the improvements proposed by [35].This benchmark has 3 domains: Amazon (A), dSLR (D) and Webcam (W).Our goal is to establish a comparison with deep DA methods.As follows, we consider: (i) Domain Adversarial Neural Network (DANN) of [12], (ii) Wasserstein Distance Guided Representation Learning (WDGRL) of [13], (iii) Deep-JDOT of [3], (iv) Moment Matching for MSDA (M3SDA) of [14], (v) WJDOT and (vi) WBT.While (i) -(iii) are single source baselines, (iv) is a standard method for MSDA.We use a ResNet-50 [36] as backbone.\n\nA summary of our results is shown in table 4. Overall, DaDiL-R and E are especially better than previous algorithms in the Amazon domain.As a consequence, in terms of average domain performance, DaDiL-R and E improve over the second-best method (WBT reg ) by a margin of 2.29% and 1.37% respectively.\n\n\nCWRU.\n\nIn this benchmark, we explore DaDiL for cross-domain fault diagnosis.The goal is to classify which type of fault has occurred, based on sensor readings.Hence, we extract 2048 Fourier coefficients from a sub-set of 4096 time-steps extracted from the raw signals (see [37], or our appendix for more details).As feature extractor, we use a 3-layer fully connected encoder.We compare 3 single, and 5 multi-source DA algorithms to DaDiL, namely, DANN, OTDA, TCA, M3SDA, LTC-MSDA of [38], JCPOT of [26], WBT reg and WJDOT.We present a summary of our results in table 5. Overall, WBT reg and DaDiL are the best performing methods, demonstrating the power of Wasserstein barycenters for DA.Our method outperforms WBT reg by 7.71%, in terms of average domain performance.Furthermore, our methods surpass other deep learning baselines, such as M3SDA [14] and LTC-MSDA [38], by a margin of 19.90%.\n\n\nDomain Adaptation using Atom Interpolations\n\nBesides performing MSDA with optimal barycentric coordinates \u03b1 T \u2208 \u2206 K , in this section we explore the question how well do \u03b1 \u2208 \u2206 K perform?We explore these questions in terms of Wasserstein distance W 2 (B(\u03b1; P), QT ), and classification accuracy of using \u03b1 in DaDiL-R and E, as shown in figure 4.\n\nIn figure 4 we construct an uniform grid over \u2206 3 .For each \u03b1 in such grid, we reconstruct B(\u03b1; P), then we evaluate: (i) the reconstruction loss W 2 (B(\u03b1; P), QT ); (ii) the classification accuracy of DaDiL-E, with \u03b1, on QT ; (iii) the classification accuracy of DaDiL-R with \u03b1, on QT .These correspond to the 3 rows in figure 4. As shown, the weights found by DaDiL are optimal w.r.t.other choices \u03b1 \u2208 \u2206 3 .Nonetheless, a wide region of the simplex yield equally good reconstructions, either w.r.t.reconstruction loss, or w.r.t.DA performance.We conclude that DaDiL is able to learn distribution whose interpolations generalize well to the target domain.\n\nFurthermore, in figure 5 we analyze the correlation between DA performance and reconstruction loss, for \u03b1 \u2208 \u2206 3 .Our analysis shows that these 2 terms are negatively correlated, for both DaDiL-R and E. Indeed, based on our theoretical analysis (theorems 1 and 2), classification risk is bounded by the reconstruction loss.Since DA performance is inversely proportional to the classifier risk in a given domain, our analysis agrees with both theorems.Finally, we analyze the performance of DaDiL-R and E for \u03b1 taken uniformly from \u2206 K , for K \u2208 {3, \u2022 \u2022 \u2022 , 8}.We report our findings in figure 6, and compare the performance w.r.t.DaDiL performance in table 2, for \u03b1 := \u03b1 T .As shown in Figure 6 \u03b1 T is above average for most domains and number of atoms K.\n\nOverall, figures 4, 5 and 6 show that DaDiL learns an optimal set of barycentric coordinates for the target domain.Nonetheless, interpolations in the Wasserstein hull M(P) of atom distributions can be equally interesting for MSDA.These remarks indicate that DaDiL is able to (i) learn common discriminant information about the source domains; (ii) interpolate the distributional shift between the various distributions in Q = { QS \u2113 } N S \u2113=1 \u222a { QT } through the atoms P.  2 for DaDiL.\n\n\nDiscussion\n\nBenefits of Dictionary Learning.Our proposed framework allows for the learning of new, virtual distributions, which can then be used to reconstruct distributions seen during DiL by generating new samples.As such, our algorithm is able to improve over past SOTA, and, as shown in section 5.2, we are able to generate new domains by interpolating the atom distributions in Wasserstein space.Especially, we improve previous SOTA and barycenter-based algorithms by 3.15% in the Caltech-Office 10 benchmark.\n\nBenefits of Wasserstein Barycenters.In our experiments, we established a comparison between DaDiL, WBT [4] and WJDOT [6].The first two methods rely on Wasserstein barycenters for reconstructing the target domain, while WJDOT aggregates the source domains linearly.Overall we show that Wasserstein barycenters are an important component of MSDA, as they allow to average probability distributions non-linearly.On the other hand, the linear average of distributions can be understood as importance weighting on samples.Under the covariate shift hypothesis, re-weighting samples is enough, but under more complicated shifts (i.e.non-linear data transformations), Wasserstein barycenters are more flexible.\n\nShallow vs. Deep Domain Adaptation.As remarked by [6], the assumption of having a meaningful feature extractor \u03d5 before performing DA is realistic, as in modern practice pre-trained models are widely available.It is noteworthy that a fine-tuning step with source-domain data may be necessary in order to achieve better performance.In addition, doing so allows for the comparison with deep DA methods.In this context, we remark that our method improves over previous deep DA SOTA in the context of the Refurbished Office 31 and CWRU benchmarks.Overall, shallow DA is computationally simpler than deep DA, as one needs to learn a smaller set of parameters (i.e., the classification layer).\n\n\nConclusion\n\nIn this work, we tackle the problem of MSDA through OT-based DiL of probability distributions.We view elements in DiL as empirical distributions.As such we learn a dictionary that is able to interpolate the distributional shift of distributions in DiL.We make 2 methodological contributions to MSDA, through methods called DaDiL-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on ensembling of atom classifiers.\n\nOur methods are theoretically grounded on previous theorems from the literature [9, Theorem 2] and a novel result (theorem 2).\n\nOur proposed methods are compared to 11 methods from the SOTA in MSDA in 3 benchmarks, namely, Caltech-Office 10 [31,30], Refurbished Office 31 [31,35] and CWRU.We improve previous performance by 3.15%, 2.29% and 7.71%\n\nrespectively.Moreover, we show that general interpolations in the Wasserstein hull of our learned dictionary can be equally interesting for MSDA.\n\nOur framework opens an interesting line of research, for learning empirical distributions, generating synthetic through Wasserstein barycenters and interpolating distributional shift in Wasserstein space.It is flexible so as to accommodate other notions of barycenters of distributions, and loss functions between reconstructions and real datasets.In practical terms, future works will focus on parametric formulations of DaDiL.In theoretical terms, we seek to understand the statistical challenges posed by DaDiL.\n\n\nA Introduction\n\nWe divide our supplementary material into 3 sections.In appendix B we provide further details about our theoretical results.Especially, appendix B.2 we provide a proof for our theorem 2. In appendix C we provide a detailed description of our adaptation of the WBR algorithm of [34].This algorithm allows us to establish an illustration on why perform dictionary learning, as learning atom distributions improves over simply regressing barycentric coordinates of the Wasserstein hull of source distributions Q S .Finally, appendix E provides further experimentation on the hyper-parameters of our algorithm.We provide in table 13 the complete setting of hyper-parameters used in our experiments.\n\n\nB Proofs of Theorems B.1 Preliminaries and Theorem 1\n\nIn the following discussion, we need to extend the definition of risk of a classifier h, for pairs (h, h \u2032 ) \u2208 H, as is done in [9],\nh \u22c6 = argmin h\u2208H R Q (h, h \u2032 ) = E x\u223cQ [L(h(x), h \u2032 (x))].\nThis is different to what was presented in section 3.1, but is needed to state the results that follow.Note that, under the assumption that L suffices the triangle inequality, so does R Q for h 1 , h 2 , h 3 \u2208 H.We start by bounding risks under P and Q by their Wasserstein distance.Lemma 1. (Due to [9]) Let P and Q be two probability distributions over R d .Assume that the cost function c(x (P ) , x (Q) ) = \u2225\u03d5(x (P ) ) \u2212 \u03d5(x (Q) )\u2225 F , where F is a reproducing kernel Hilbert space equipped with kernel \u03a6 : R d \u00d7 R d \u2192 R induced by \u03d5 : R d \u2192 H k and \u03a6(x (P ) , x (Q) ) = \u27e8\u03d5(x (P ) ), \u03d5(x (Q) )\u27e9 F .Assume that the kernel \u03a6 \u2208 F is square-root integrable w.r.t.both P and Q and 0 \u2264 \u03a6(x (P ) ,\nx (Q) ) \u2264 M , \u2200x (P ) , x (Q) \u2208 R d . Then the following holds, R Q (h, h \u2032 ) \u2264 R P (h, h \u2032 ) + W 1 (P, Q).(13)\nNote.Following [39], H\u00f6lder's inequality implies that p \u2264 q =\u21d2 W p \u2264 W q , where W p corresponds to the Wasserstein distance with ground-cost c(x\n(P ) , x (Q) ) = \u2225x (P ) \u2212 x (Q) \u2225 p p .\nAs consequence, bound 13 is also valid for the Euclidean distance.\n\nThe consequence of Lemma 1 relates the risk of a pair (h, h \u2032 ) under distributions P and Q by the Wasserstein distance.This is in line with other theoretical results, such as [40].Before relating the risks of h with the empirical Wasserstein distance, one needs to establish the convergence of the empirical P to the true P .This is done in the next lemma.Lemma 2. (Due to [41]) Let P be a probability distribution over R d , so that for some \u03b1 > 0 we have that R d e \u03b1||x|| 2 dP < \u221e and P be its associated empirical approximation with support {x (P ) i } n i=1 drawn independently from P .Then, for any d \u2032 > d and \u03be \u2032 < \u221a 2 there is a constant n 0 depending on d \u2032 and some square exponential moment of P such that for any \u03f5 > 0 and n \u2265 n 0 max(\u03f5 \u2212(d \u2032 +2) , 1),\nP[W 1 ( P , P ) > \u03f5] \u2264 exp \u2212 \u03be \u2032 2 n\u03f5 2 ,\nwhere d \u2032 and \u03be \u2032 can be calculated explicitly.\n\nLemma 2 states the conditions for which P and P are close in the sense of Wasserstein.This last bound is on the form P[quantity > \u03f5] < \u03b4, that is, with high probability quantity \u2264 \u03f5.These types of bounds are ubiquitous in the theoretical analysis of learning algorithms.We can express \u03f5 explicitly in terms of \u03b4,\n\u03f5 = 2 n\u03be \u2032 log( 1 \u03b4 ),(14)\nwhich will be useful in the following discussion.We recall that, as stated in [9, Theorem 2], and Theorem 1, under suitable conditions,\nR Q (h) \u2264 R P (h) + W 2 ( P , Q) + \u03b6 + \u03bb, \u03b6 = 2 (log 1 /\u03b4) /\u03be \u2032 1 /n P + 1 /n Q , \u03bb = min h\u2208H R Q (h) + R P (h),\nThese results allowed [9] to provide theoretical guarantees for the OTDA framework of [1].As discussed in [9], the minimization of W 2 ( P , Q) alone does not guarantees DA success.As illustrated by the authors, during transportation one can minimize the domain distance while mixing classes.This leads to a situation where one has a low W 2 , but a risk R P high over all h \u2208 H.The same reasoning may be applied to the joint error \u03bb.A few assumptions need to be done at this point, H1 The distribution P \u2032 which reduces W 2 (\u2022, Q) has distinguishable classes, i.e., R P \u2032 (h) is low.\n\nH2 The distribution P \u2032 which reduces W 2 (\u2022, Q) has the same class-structure as Q (i.e.classes lie on the same side of the boundary).In other words, \u03bb is low.\n\nWe now recast this theorem and hypothesis in terms of DaDiL-R.First, one has Q = Q T , i.e., the target domain distribution.Second, one uses P = B T , i.e., the barycentric reconstruction of the target, through our dictionary.Our algorithm regulates R B T by integrating the labels in the ground-cost.As a consequence, giving mass to samples with different classes is too costly, and the OT transport plans are class sparse, i.e. \u03c0 ij > 0 \u21d0\u21d2 y\n(P ) i = y (Q) j .\nThe second hypothesis is difficult to ensure in unsupervised DA, as one does not have any information about the class structure of Q.As [1] and [9], we assume a degree of regularity in the distributional shift of distributions in Q.This allows us to predict classes in the target, through label propagation.\n\n\nB.2 Proof of Theorem 2\n\nBefore proving our results, we define the following risks,\nR Q T (h) = E x\u223cQ T [L(h(x), h Q T ,0 (x))], R P k (h) = E x\u223cP k [L(h(x), h P k ,0 (x))], R \u03b1 (h) = K k=1 \u03b1 k R P k (h).\nwhich are the risk under the target, under each of the atom distributions, and the combined risk weighted by \u03b1, respectively.h Q T ,0 and h P k ,0 are the ground-truth labeling functions of the target distribution, and atom k.Likewise, we define the following classifiers,\nh \u22c6 T,k = argmin h\u2208H R Q T (h) + R P k (h), \u0125k = argmin h\u2208H R P k (h), \u0125\u03b1 (x) = k \u03b1 k \u0125k (x).\nOur proof relies on the triangle inequality for the risk.With our previous definitions,\nR Q T ( \u0125\u03b1 ) \u2264 R Q T (h \u22c6 T,k ) + R Q T (h \u22c6 T,k , \u0125\u03b1 ).\nNow, we add and subtract R P k (h \u22c6 T,k , \u0125\u03b1 ),\nR Q T ( \u0125\u03b1 ) \u2264 (R Q T (h \u22c6 T,k , \u0125\u03b1 ) \u2212 R P k (h \u22c6 T,k , \u0125\u03b1 )) + (R Q T (h \u22c6 T,k ) + R P k (h \u22c6 T,k , \u0125\u03b1 ) \u2264R P k (h \u22c6 T ,k )+R P k ( \u0125\u03b1)\n),\n\u2264 (R Q T (h \u22c6 T,k , \u0125\u03b1 ) \u2212 R P k (h \u22c6 T,k , \u0125\u03b1 )) \u2264W2(P k ,Q T ) by Lemma 1. + (R Q T (h \u22c6 T,k ) + R P k (h \u22c6 T,k ))\n=\u03bb k by Def.\n+R P k ( \u0125\u03b1 )\nwhere, in the first line, we used the triangle inequality again.This result leads to,\nR Q T ( \u0125\u03b1 ) \u2264 R P k ( \u0125\u03b1 ) + W 2 (P k , Q T ) + \u03bb k .\nNow, summing over k, weighted by \u03b1, one has\nR Q T ( \u0125\u03b1 ) = k \u03b1 k R Q T ( \u0125\u03b1 ).\nWe can bound this latter term as follows,\nk \u03b1 k R Q T ( \u0125\u03b1 ) \u2264 k \u03b1 k R P k ( \u0125\u03b1 ) + k \u03b1 k (W 2 (P k , Q T ) + \u03bb k ), = R \u03b1 ( \u0125\u03b1 ) + k \u03b1 k (W 2 (P k , Q T ) + \u03bb k ), \u2264 R \u03b1 ( \u0125\u03b1 ) + k \u03b1 k (W 2 ( Pk , QT ) + \u03bb k + \u03b6 k ),(15)\nFrom this last inequality, we use the triangle inequality between Pk , B(\u03b1; P) and QT , W 2 ( Pk , QT ) \u2264 W 2 ( Pk , B(\u03b1; P)) + W 2 (B(\u03b1; P), QT ).\n\nSumming over k, and noting that, This theorem is similar to previous theoretical guarantees for MSDA, such as those of [40], [9], and [4].A few remarks may be made about the terms \u03b3, \u03bb and \u03b6,\n\n\u2022 \u03b6 corresponds to the sample complexity of approximating W 2 (P k , Q T ) by W 2 ( Pk , QT ).It decreases with the number of samples in the atoms, and in the target domain.\n\n\u2022 \u03bb corresponds to the domain adaptation complexity between the atoms and the target.As in DaDiL-R, this term cannot be bounded, but it is in general assumed to remain bounded during DA.\n\n\u2022 \u03b3 is a new term, corresponding to k \u03b1 k W 2 ( Pk , B(\u03b1; P)).For a given \u03b1, this term is minimal since B(\u03b1; P) is, by definition, the minimizer of B \u2192 k \u03b1 k W 2 ( Pk , B).\n\n\nC Wasserstein Barycentric Coordinate Regression\n\nIn this section we describe how to adapt the WBR algorithm [34] for empirical distributions.Let Q S = { Q\u2113 } N S \u2113=1 be the set of labeled source distributions, and \u03b1 \u2208 \u2206 N S be a set of barycentric coordinates, i.e., they allow to interpolate between distributions in Q through B(\u03b1; Q S ).Let QT be an unlabeled target distirbution.[34] proposed regressing \u03b1 by the following minimization problem,\n\u03b1 \u22c6 = argmin \u03b1\u2208\u2206 N S W 2 ( QT , B(\u03b1; Q S )).(16)\nIn [34], the minimization in equation 16 was done when QS \u2113 and QT are histograms, i.e., vectors in \u2206 d .In our case, these distributions are empirical.As a consequence, we can employ a similar method to DaDiL for regressing \u03b1 \u2208 \u2206 N S , as shown in algorithm 3.\n\nAlgorithm 3 Wasserstein Barycentric Regression of Point Clouds.\nInput: QS = { QS \u2113 } N S \u2113=1\n, QT , batch size n b , learning rate \u03b7, number of iterations Niter, number of batches M .1: Initialize\n\u03b1 \u2113 = N \u22121 S , \u2113 = 1, \u2022 \u2022 \u2022 , NS. 2: for it = 1 \u2022 \u2022 \u2022 , Niter do 3: for batch = 1, \u2022 \u2022 \u2022 , M do 4: for \u2113 = 1, \u2022 \u2022 \u2022 , NS do 5: Sample X (Q S \u2113 ) = {x (Q S \u2113 ) i } n b i=1 6:\nend for 7:\n\nsample\nX (Q T ) = {x (Q T ) j } n b j=1 8: calculate X (B) = B(\u03b1 \u2113 ; QS) 9: L \u2190 W2( QT , B) 10: \u03b1 \u2190 proj \u2206 N S (\u03b1 \u2212 \u03b7 \u2202L /\u2202\u03b1 \u2113 ). 11:\nend for 12: end for Output: Barycentric coordinates \u03b1 \u22c6 .\n\nAs noted by [34], the barycentric regression algorithm leads to poor reconstructions, when QT is far from the distributions in Q S .This remark explains its sub-optimal performance in our ablation study (i.e., table 3 of our main paper).These results also support why DiL is important in MSDA, as learning atom distributions leads to a Wasserstein hull P for which QT is close.\n\n\nD Experiments Details D.1 Dataset Description\n\nCaltech-Office 10: We use the experimental setting of [4], namely, the 5 fold cross-validation partitions and the features (DeCAF 7th layer activations).\n\nRefurbished-Office 31: This dataset consists on the Office 31 dataset of [31].We follow the discussion of [35] and adopt their modifications, namely, the replacements of images in the Amazon domain that contained label noise.The authors modifications are publicly available in a Gitlab respository 2 .Since our method relies on features, we adopt the following methodology, 1.A backbone deep Neural Net (NN) is fitted using all source data.For instance if adaptation is done between A, D \u2192 W , then all data from A and D is used for training the NN.\n\n2. Features are extracted using the convolutional layers.This generates a new dataset of shape (n samples , d), where d is the dimensionality of features.\n\nWe split each domain in a train and test set, corresponding to 80% and 20% of samples, respectively.This is shown in Table 7.Here, we highlight that during fine-tuning in step 1, all data from sources is used (i.e., training and test partitions), as evaluation is performed solely on target data.DaDiL is then run using the features extracted at step 2. For the Refurbished-Office 31, we chose to use the ResNet-50 of [36] as backbone.\n\nSince this benchmark has a significant performance saturation (i.e., fine-tuning a complete ResNet yields 100% classification accuracy when adapting towards dSLR and Webcam), we use a slightly more challenging setting, where we keep all but the last convolutional block frozen during fine-tuning.\n\n\nCWRU:\n\nThe third benchmark we experiment on is the Case Western Reserve University bearing fault diagnosis benchmark, which is publicly available3 .We use the 12k drive end bearing fault data, where we consider the file ids reported in table 6.This is similar to other studies in bearing fault diagnosis, as reported in [37].\n\n\nD.2 DaDiL Implementation Details\n\nIn this section we highlight a few details in our implementation.Concerning algorithm 1, we remark that we differentiate X (B) and Y (B) at termination of algorithm 1 (main paper).This means that derivatives are not back-propagated through the while loop in algorithm 1 (main paper).In practical terms, this corresponds to stating with torch.no_grad()before the loop, then using the transport plans \u03c0 (k) at termination for calculating the support X (B) , Y (B) .This is similar to what was proposed by [42].In practice, \u03c0\n\ni,l is not differentiated w.r.t.x\n(P k ) l , y (P k ) l or \u03b1.\nWe also implement algorithm 2 using Pytorch, thus calculating the derivatives of L(P, A) automatically.The projection into the simplex is done using POT's function ot.utils.proj_simplex,which does so by sorting (see e.g., [43] and [44] for more details).\n\n\nE Additional Experiments\n\nIn this section, we provide additional experiments.In section E.1, we explore the reasoning for tuning the hyperparameters of DaDiL.In addition, section E.2 we explore the stability of algorithm 2 w.r.t.different random initializations of atoms, as well as the evolution of weights \u03b1 w.r.t.iterations.\n\n\nE.1 Hyper-parameter Analysis\n\nIn our experiments, we tune hyper-parameters so as to maximize performance on target domain training data.Our evaluation is done in an independent partition of the test set, not seen during training.In what follows, we highlight that previous best-performing method in the SOTA, i.e.WBT reg , has 92.55% of average domain performance.\n\nNumber of Atoms.We test K \u2208 {3, \u2022 \u2022 \u2022 , 8}.We divide our discussion into two experiments: (i) DA performance, and (ii) weight sparsity as a function of the number of atoms K.We fix n = 1000 and n b = 200.First, we measure classification performance on the target domain, which is shown in Table 8.Performance fluctuates over choices of K, being higher with increasing number of components.For all choices, we improve over previous SOTA.As the number of atoms grows, \u03b1 T becomes more sparse.This implies that DaDiL selects a sub-set of atoms to reconstruct the target domain whenever K is overestimated.\n\nNumber of Samples.We divide our discussion into two experiments: (i) DA performance and (ii) support density, as a function of the number of samples n in the support of Pk .We fix K = 8 and n b = 200.First, as shown in table 10, DaDiL is more sensible to the number of samples in the support of atoms.While the best choice is n = 1000, for other values of n we remain SOTA. .We evaluate the density of atoms support as a function of n by measuring the average distance of each point in X (P k ) to its 5 nearest neighbors, i.e., density score = 1 5nK K k=1 n i=1 x (P k ) j \u2208N5(x\n(P k ) i ) \u2225x (P k ) i \u2212 x (P k ) j \u2225 2 2 .\n1.The stability of the loss curve w.r.t.different and independent initializations, 2. The stability of parameter updates w.r.t.iterations, For the first criterion, we calculate the mean and standard deviation of L it over 5 independent runs.Overall, the loss is stable w.r.t.random initializations, and it converges to different local minima, as shown the small variation in the loss at the beginning and end of training.These experiments raise the question on whether minimizers to eqn.11 are unique.It is generally not the case.For instance, if one wants to express Q = {Q 0 }, Q 0 = N (\u00b5, I) as the barycenter of P = {P 1 , P 2 , P 3 }, where P i = N (\u00b5 i , I), any set of means with ( 1 /3) 3 i=1 \u00b5 i = \u00b5 is a minimizer of eqn.11.Thus theoretical analysis of minimizers of eqn.11 is challenging, and we will consider it in future works.\n\nConcerning the second criterion, we evaluate the magnitude of updates in X\n(P k ) it\u22121 \u2225 2 F , \u2206 A (it) = \u2225A it \u2212 A it\u22121 \u2225 2 F .(17)\nWe show our results in figure 8.Note that, towards the end of learning, the updates on DaDiL's variables fall close to 0, indicating that the algorithm converged to a local minimum.\n\nFigure 1 :\n1\nFigure 1: Conceptual illustration of DaDiL.Each domain is denoted by a blue or orange circle, corresponding to whether it is labeled or not.DaDiL reconstructs domains as Wasserstein barycenters, denoted by squares, of atoms, denoted by triangles.The target domain (orange circle) is unlabeled, but we are able to represent it through a labeled distribution through a Wasserstein barycenter.\n\n\nFigure 2 :\n2\nFigure 2: From left to right: set of datasets Q = { QS1 , QS2 , QS3 , QT }, where QT is the unlabeled target domain; atoms P = { P1 , P2 , P3 }; barycentric weights A.\n\n\nFigure 3 :\n3\nFigure 3: Conceptual illustration of the 2 methods, based on DaDiL, for MSDA.\n\n\nW\n\n\n\n\nFigure 4 :=1 W 2 (\n42\nFigure 4: Analysis of DA on Caltech-Office with interpolations of dictionary atoms.The black cross represents the \u03b1 found by DaDiL.\n\n\nFigure 5 :\n5\nFigure 5: Correlation between DiL loss and the performance of DaDiL-R and E.\n\n\nFigure 6 :\n6\nFigure 6: Performance analysis of latent space interpolations on the Caltech-Office 10 benchmark.The red dotted line corresponds to the results reported in Table2for DaDiL.\n\n\nk \u03b1 k W 2 (\u03b1 k W 2 (\n22\nB(\u03b1; P), QT ) = W 2 (B(\u03b1; P), QT ) one has, k \u03b1 k W 2 ( Pk , QT ) \u2264 W 2 (B(\u03b1; P), QT ) + \u03b3.Plugging this result back into equation 15,R Q T ( \u0125\u03b1 ) \u2264 R \u03b1 ( \u0125\u03b1 ) + W 2 ( QT , B(\u03b1; P)) + \u03b3 + \u03bb + \u03b6, Pk , B(\u03b1; P)), and, \u03bb = K k=1 \u03b1 k \u03bb k , \u03be = K k=1 \u03b1 k \u03be kwhich concludes the proof.\n\n\nFigure 7 :\n7\nFigure 7: DaDiL's loss function (eqn 11) as a function of iterations.The solid line represents an average over 5 independents runs, whereas the shaded region represents a confidence interval of \u00b12\u03c3.\n\n\n(\n\nP k ) it , Y (P k ) itand A it .We use the Frobenius norm for comparing updates, i.e.,\n\n\nFigure 8 :\n8\nFigure 8: Magnitude of updates of DaDiL's algorithm as a function of iterations.\n\n\nTable 1 :\n1\nOverview of analogies between different frameworks of DiL.\na comparison of DiL concepts in different\n\nTable 2 :\n2\n[6]ssification accuracy (in %) of DA methods.Each column represents a target domain for which we report mean \u00b1 standard deviation over 5 folds.*and\u2020denoteresults from[4]and[6].\u00b1 1.36 96.83 \u00b1 1.33 88.36 \u00b1 1.33 82.95 \u00b1 1.26 89.67 SA 88.61 \u00b1 1.72 92.08 \u00b1 3.82 79.33 \u00b1 3.67 73.00 \u00b1 2.31 83.26 TCA \u22c6 86.83 \u00b1 4.71 89.32 \u00b1 1.33 97.51 \u00b1 1.18 80.79 \u00b1 2.65 88.61 OTDA 88.26 \u00b1 1.36 90.41 \u00b1 3.86 88.09 \u00b1 3.80 83.02 \u00b1 1.67 87.44 WJDOT \u2020 94.23 \u00b1 0.90 100.00 \u00b1 0.00 89.33 \u00b1 2.91 85.93 \u00b1 2.07 92.37 WBT \u22c6 reg 92.74 \u00b1 0.45 95.87 \u00b1 1.43 96.57\u00b1 1.76 85.01 \u00b1 0.84 92.55 DaDiL-R 94.06 \u00b1 1.82 98.75 \u00b1 1.71 98.98 \u00b1 1.51 88.97 \u00b1 1.06 95.19 DaDiL-E 94.16 \u00b1 1.58 100.00 \u00b1 0.00 99.32 \u00b1 0.93 89.15 \u00b1 1.68 95.66\nMethodADWCAvgBaseline90.55\n\nTable 3 :\n3\nClassification accuracy (in %) of DA methods.P and A indicate learning atom distributions and barycentric coefficients respectively.T indicates an additional transport step towards Q T .\u00b1 1.16 90.62 \u00b1 8.38 93.89 \u00b1 3.30 83.73 \u00b1 1.49 89.19 WBT reg 92.74 \u00b1 0.45 95.87 \u00b1 1.43 96.57\u00b1 1.76 85.01 \u00b1 0.84 92.55 WBR-R 91.35 \u00b1 1.19 91.87 \u00b1 9.47 81.69 \u00b1 3.26 86.31 \u00b1 1.73 86.09WBR-E 91.97 \u00b1 2.40 91.87 \u00b1 2.79 83.73 \u00b1 2.57 86.13 \u00b1 1.84 88.42 DaDiL-R 94.06 \u00b1 1.82 98.75 \u00b1 1.71 98.98 \u00b1 1.51 88.97 \u00b1 1.06 95.19 DaDiL-E 94.16 \u00b1 1.58 100.00 \u00b1 0.00 99.32 \u00b1 0.93 89.15 \u00b1 1.68 95.66\nMethodP A TADWCAvg.WB88.54\nthe single-source OT baseline, and (iv, v) are the SOTA for shallow MSDA.The baseline corresponds to training a single-layer Perceptron with the concatenation of source domain data.Our results is presented in table 2. DaDiL improve over previous OT-based MSDA baselines, i.e.WJDOT and WBT, being especially better on the Webcam and Caltech domains.Overall, we improve previous SOTA by 3.15 in terms of average DA performance.\n\n\nTable 4 :\n4\nClassification accuracy (in %) of DA methods on the Refurbished Office 31 benchmark.Each column represents a target domain for which we report mean \u00b1 standard deviation over 5 folds.\nMethodADWAvgBaseline70.57 97.00 95.47 87.68DANN78.19 97.00 93.08 89.42WDGRL76.06 97.00 93.71 88.92DeepJDOT 80.85 94.00 93.38 89.61M3SDA64.89 98.00 96.85 86.58WBT reg77.48 96.00 95.59 89.69WJDOT70.21 97.00 94.96 87.39DaDiL-R85.46 93.00 97.48 91.98DaDiL-E83.51 94.00 94.34 90.61\n\nTable 5 :\n5\nClassification accuracy (in %) of DA methods on the CWRU benchmark.Each column represents a target domain for which we report mean \u00b1 standard deviation over 5 folds.\nMethod1772rpm1750rpm1730rpmAvgBaseline DANN OTDA TCA M3SDA WJDOT M3SDA \u03b2 LTC-MSDA 82.21 \u00b1 8.03 75.33 \u00b1 5.91 81.04 \u00b1 5.45 79.52 70.90 \u00b1 0.40 79.76 \u00b1 0.11 72.26 \u00b1 0.23 74.31 67.96 \u00b1 8.52 64.38 \u00b1 5.03 57.75 \u00b1 17.06 63.37 70.48 \u00b1 2.25 79.61 \u00b1 0.25 74.98 \u00b1 1.26 75.02 87.17 \u00b1 4.25 84.11 \u00b1 4.77 92.74 \u00b1 4.12 88.01 56.86 \u00b1 7.31 69.81 \u00b1 0.36 61.06 \u00b1 6.35 62.57 65.01 \u00b1 0.27 69.81 \u00b1 0.07 57.40 \u00b1 1.18 64.07 60.15 \u00b1 8.38 70.00 \u00b1 0.00 64.00 \u00b1 5.47 64.72 JCPOT 77.48 \u00b1 0.86 96.00 \u00b1 0.10 95.59 \u00b1 0.56 91.74 WBT reg 99.28 \u00b1 0.18 79.91 \u00b1 0.04 97.71 \u00b1 0.76 92.30 DaDiL-R 99.86 \u00b1 0.21 99.85 \u00b1 0.08 100.00 \u00b1 0.00 99.90 DaDiL-E 93.71 \u00b1 6.50 83.63 \u00b1 4.98 99.97 \u00b1 0.05 92.33\n\nTable 6 :\n6\nFile ids of data used in the CWRU experiments.In the CWRU benchmark, for each file id we randomly sample a window of length 4096 from the complete signal.From this sub-sample, we extract the first 2048 Fourier coefficients, from the 4096 acquires through a Fast Fourier Transform (FFT).For each target domain, we pre-train a multi-layer Perceptron 4 from scratch, and use the activation of its penultimate layer as features for shallow methods, including DaDiL.Overall, when training DaDiL, we use all source data (i.e.train and test), and the train partition of the target domain, without its labels.The performance evaluation is done in the independent test set, which is never seen during training.\nLabel0123456789Fault Location NoneInner Race FaultBall FaultOuter Race FaultFault Diameter0.007 0.014 0.021 0.007 0.014 0.021 0.007 0.014 0.0211772 rpm981061702101191862231311982351750 rpm991071712111201872241321992361730 rpm100108172212121188225133200237\n\nTable 7 :\n7\nDetails about the datasets considered for domain adaptation.\nDataset# Classes Domain # Training Samples # Test Samples # SamplesAmazon748210958Caltech-Office 1010dSLR Webcam108 22449 71157 295Caltech9562241180Total20365542590Amazon22535642817Office 3131dSLR Webcam398 636100 159498 795Total328782341101772rpm600020008000CWRU101750rpm 1720rpm6000 60002000 20008000 8000Total18000600024000\n\nTable 8 :\n8\nClassification accuracy in %, as a function of number of components K. DaDiL-R 93.53 94.16 93.33 94.30 94.23 95.30 DaDiL-E 93.89 94.33 93.86 94.68 94.66 95.70Second, for a fixed K we measure the sparsity of \u03b1 T through the following score,sparsity score = 100%\u00d7 1 \u2212 \u2225\u03b1 T \u2225 0 K ,that is, the percentage of zero entries in the target domain weights.\nK345678\n\nTable 9 :\n9\nSparsity Score (in %) as a function of number of atoms K.\nK345678Amazon 6.66 0.008.00 20.00 8.58 15.00Caltech 6.66 20.00 16.00 30.00 31.42 27.50dSLR0.00 5.008.00 23.33 25.71 27.50Webcam 6.66 5.00 12.00 10.00 20.00 27.50Avg5.00 7.50 11.00 20.83 21.43 24.37\n\nTable 10 :\n10\nClassification accuracy in %, as a function of the number of samples n.Second, let us explore the density of samples in the atoms support.Let N 5 (x\nn5001000 2000DaDiL-R 92.48 95.30 93.57DaDiL-E 93.12 95.70 93.58(P k ) i) denote the set of 5 nearest neighborsin Pk , w.r.t. x(P K ) i\nhttps://gitlab.com/tringwald/adaptiope\nhttps://engineering.case.edu/bearingdatacenter/download-data-file\n2048 \u2192 1024 \u2192\n\u2192 256 with ReLU activations.\nAs table11shows, the samples concentrate over a region of R d , meaning that the atoms' support are stable w.r.t n.Final Hyper-parameter Settings.Overall, we select hyper-parameters that maximize classification performance on target domain samples seen during training.For evaluation, we use an independent partition of the target domain data not seen during training.We report the used hyper-parameters in table13.\nOptimal transport for domain adaptation. Nicolas Courty, R\u00e9mi Flamary, Devis Tuia, Alain Rakotomamonjy, IEEE transactions on pattern analysis and machine intelligence. 201639\n\nJoint distribution optimal transportation for domain adaptation. Nicolas Courty, R\u00e9mi Flamary, Amaury Habrard, Alain Rakotomamonjy, Advances in Neural Information Processing Systems. 302017\n\nDeepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation. Benjamin Bharath Bhushan Damodaran, R\u00e9mi Kellenberger, Flamary, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2018Devis Tuia, and Nicolas Courty\n\nWasserstein barycenter for multi-source domain adaptation. Eduardo Fernandes, Montesuma , Fred Maurice, Ngole Mboula, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2021\n\nWasserstein barycenter transport for acoustic adaptation. Eduardo Fernandes, Montesuma , Fred Maurice, Ngole Mboula, ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). May 2021\n\nMulti-source domain adaptation via weighted joint distributions optimal transport. Rosanna Turrisi, R\u00e9mi Flamary, Alain Rakotomamonjy, The 38th Conference on Uncertainty in Artificial Intelligence. 2022\n\nFast dictionary learning with a smoothed wasserstein loss. Antoine Rolet, Marco Cuturi, Gabriel Peyr\u00e9, Artificial Intelligence and Statistics. PMLR2016\n\nWasserstein dictionary learning: Optimal transport-based unsupervised nonlinear dictionary learning. Morgan A Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Ngole, David Coeurjolly, Marco Cuturi, Gabriel Peyr\u00e9, Jean-Luc Starck, SIAM Journal on Imaging Sciences. 1112018\n\nTheoretical analysis of domain adaptation with optimal transport. Ievgen Redko, Amaury Habrard, Marc Sebban, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer2017\n\nCovariate shift adaptation by importance weighted cross validation. Masashi Sugiyama, Matthias Krauledat, Klaus-Robert M\u00fcller, Journal of Machine Learning Research. 852007\n\nA survey on transfer learning. Jialin Sinno, Qiang Pan, Yang, IEEE Transactions on knowledge and data engineering. 22102009\n\nDomain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, Victor Lempitsky, The journal of machine learning research. 1712016\n\nWasserstein distance guided representation learning for domain adaptation. Jian Shen, Yanru Qu, Weinan Zhang, Yong Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832\n\nMoment matching for multisource domain adaptation. Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, Bo Wang, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019\n\nCoupled dictionary and feature space learning with applications to cross-domain image synthesis and recognition. De- , An Huang, Yu-Chiang Frank, Wang , Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2013\n\nSinkhorn distances: Lightspeed computation of optimal transport. Marco Cuturi, Advances in neural information processing systems. 201326\n\nPrinciples of risk minimization for learning theory. Vladimir Vapnik, Advances in neural information processing systems. 19914\n\nA survey on domain adaptation theory: learning bounds and theoretical guarantees. Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, Youn\u00e8s Bennani, arXiv:2004.118292020arXiv preprint\n\nRecent advances in optimal transport for machine learning. Eduardo Fernandes Montesuma, Fred Ngole Mboula, Antoine Souloumiac, arXiv:2306.161562023arXiv preprint\n\nComputational optimal transport: With applications to data science. Gabriel Peyr\u00e9, Marco Cuturi, Foundations and Trends\u00ae in Machine Learning. 201911\n\nOptimal transport for applied mathematicians. Filippo Santambrogio, 20155594Birk\u00e4user, NY\n\nBarycenters in the wasserstein space. Martial Agueh, Guillaume Carlier, SIAM Journal on Mathematical Analysis. 4322011\n\nFast computation of wasserstein barycenters. Marco Cuturi, Arnaud Doucet, International conference on machine learning. PMLR2014\n\nKilian Fatras, Younes Zine, Szymon Majewski, R\u00e9mi Flamary, R\u00e9mi Gribonval, Nicolas Courty, arXiv:2101.01792Minibatch optimal transport distances; analysis and applications. 2021arXiv preprint\n\nGeometric dataset distances via optimal transport. David Alvarez, -Melis , Nicolo Fusi, Advances in Neural Information Processing Systems. 202033\n\nAdvances in domain adaptation theory. Ievgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, Younes Bennani, 2019Elsevier\n\nTheory of maxima and the method of lagrange. Sn Afriat, SIAM Journal on Applied Mathematics. 2031971\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alch\u00e9-Buc, E Fox, R Garnett, Curran Associates, Inc201932\n\nPot: Python optimal transport. R\u00e9mi Flamary, Nicolas Courty, Alexandre Gramfort, Aur\u00e9lie Mokhtar Z Alaya, Stanislas Boisbunon, Laetitia Chambon, Adrien Chapel, Kilian Corenflos, Nemo Fatras, Fournier, J. Mach. Learn. Res. 22782021\n\nCaltech-256 object category dataset. Gregory Griffin, Alex Holub, Pietro Perona, 2007California Institute of TechnologyTechnical report\n\nAdapting visual category models to new domains. Kate Saenko, Brian Kulis, Mario Fritz, Trevor Darrell, European conference on computer vision. Springer2010\n\nUnsupervised visual domain adaptation using subspace alignment. Basura Fernando, Amaury Habrard, Marc Sebban, Tinne Tuytelaars, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2013\n\nDomain adaptation via transfer component analysis. Ivor W Sinno Jialin Pan, James T Tsang, Qiang Kwok, Yang, 201022\n\nWasserstein barycentric coordinates: histogram regression using optimal transport. Nicolas Bonneel, Gabriel Peyr\u00e9, Marco Cuturi, ACM Trans. Graph. 3542016\n\nAdaptiope: A modern benchmark for unsupervised domain adaptation. Tobias Ringwald, Rainer Stiefelhagen, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2021\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016\n\nIntelligent fault diagnosis under varying working conditions based on domain adaptive convolutional neural networks. Bo Zhang, Wei Li, Xiao-Li Li, See-Kiong Ng, Ieee Access. 62018\n\nLearning to combine: Knowledge aggregation for multi-source domain adaptation. Hang Wang, Minghao Xu, Bingbing Ni, Wenjun Zhang, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part VIII 16\n\nOptimal transport: old and new. C\u00e9dric Villani, 2009Springer338\n\nA theory of learning from different domains. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, Jennifer Wortman Vaughan, Machine learning. 7912010\n\nQuantitative concentration inequalities for empirical measures on non-compact spaces. Probability Theory and Related Fields. Fran\u00e7ois Bolley, Arnaud Guillin, C\u00e9dric Villani, 2007137\n\nInterpolating between optimal transport and mmd using sinkhorn divergences. Jean Feydy, Thibault S\u00e9journ\u00e9, Fran\u00e7ois-Xavier Vialard, Shun-Ichi Amari, Alain Trouv\u00e9, Gabriel Peyr\u00e9, The 22nd International Conference on Artificial Intelligence and Statistics. PMLR2019\n\nValidation of subgradient optimization. Michael Held, Philip Wolfe, Harlan P Crowder, Mathematical programming. 611974\n\nFast projection onto the simplex and the \u2113 1 ball. Laurent Condat, Mathematical Programming. 15812016\n\nLearning with minibatch wasserstein: asymptotic and gradient properties. Kilian Fatras, Younes Zine, R\u00e9mi Flamary, R\u00e9mi Gribonval, Nicolas Courty, AISTATS. 2020\n", "annotations": {"author": "[{\"end\":186,\"start\":101},{\"end\":262,\"start\":187},{\"end\":339,\"start\":263}]", "publisher": null, "author_last_name": "[{\"end\":128,\"start\":109},{\"end\":204,\"start\":198},{\"end\":281,\"start\":271}]", "author_first_name": "[{\"end\":108,\"start\":101},{\"end\":191,\"start\":187},{\"end\":197,\"start\":192},{\"end\":270,\"start\":263}]", "author_affiliation": "[{\"end\":185,\"start\":130},{\"end\":261,\"start\":206},{\"end\":338,\"start\":283}]", "title": "[{\"end\":88,\"start\":1},{\"end\":427,\"start\":340}]", "venue": null, "abstract": "[{\"end\":1707,\"start\":566}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1916,\"start\":1912},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2053,\"start\":2049},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2978,\"start\":2975},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2980,\"start\":2978},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2982,\"start\":2980},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3002,\"start\":2999},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3004,\"start\":3002},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3006,\"start\":3004},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3203,\"start\":3200},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3211,\"start\":3208},{\"end\":3806,\"start\":3796},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4802,\"start\":4798},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4908,\"start\":4904},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5013,\"start\":5010},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5357,\"start\":5353},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5396,\"start\":5393},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5399,\"start\":5396},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5564,\"start\":5560},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5633,\"start\":5630},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5755,\"start\":5752},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5831,\"start\":5828},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5833,\"start\":5831},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5864,\"start\":5861},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6096,\"start\":6092},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6319,\"start\":6315},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6349,\"start\":6346},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6401,\"start\":6398},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6978,\"start\":6974},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7708,\"start\":7704},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7944,\"start\":7940},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7959,\"start\":7955},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8732,\"start\":8728},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8741,\"start\":8737},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9673,\"start\":9670},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9972,\"start\":9969},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10176,\"start\":10172},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10181,\"start\":10178},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10505,\"start\":10501},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11015,\"start\":11011},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11397,\"start\":11394},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11678,\"start\":11674},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12657,\"start\":12654},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12704,\"start\":12700},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12734,\"start\":12731},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13481,\"start\":13478},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13483,\"start\":13481},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13532,\"start\":13528},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14380,\"start\":14377},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14415,\"start\":14411},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15042,\"start\":15038},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16342,\"start\":16338},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17927,\"start\":17923},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17967,\"start\":17963},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18048,\"start\":18045},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18050,\"start\":18048},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19454,\"start\":19451},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19479,\"start\":19476},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20063,\"start\":20060},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20728,\"start\":20725},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20730,\"start\":20728},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22716,\"start\":22712},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22750,\"start\":22746},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22931,\"start\":22927},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22979,\"start\":22975},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23036,\"start\":23033},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23088,\"start\":23085},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23090,\"start\":23088},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23124,\"start\":23121},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23555,\"start\":23551},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24190,\"start\":24186},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24230,\"start\":24226},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":24433,\"start\":24429},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24507,\"start\":24503},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24531,\"start\":24528},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24578,\"start\":24574},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24707,\"start\":24703},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25302,\"start\":25298},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25513,\"start\":25509},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25528,\"start\":25524},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25876,\"start\":25872},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25894,\"start\":25890},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28792,\"start\":28789},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28806,\"start\":28803},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29443,\"start\":29440},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30793,\"start\":30789},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30796,\"start\":30793},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30824,\"start\":30820},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30827,\"start\":30824},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":31857,\"start\":31853},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32458,\"start\":32455},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32822,\"start\":32819},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33345,\"start\":33341},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33761,\"start\":33757},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33959,\"start\":33955},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35053,\"start\":35050},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":35117,\"start\":35114},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35137,\"start\":35134},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":36377,\"start\":36374},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36385,\"start\":36382},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":38311,\"start\":38307},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38316,\"start\":38313},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38325,\"start\":38322},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":39031,\"start\":39027},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":39305,\"start\":39301},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":39423,\"start\":39419},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":40271,\"start\":40267},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":40739,\"start\":40736},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40914,\"start\":40910},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":40947,\"start\":40943},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":41966,\"start\":41962},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":42604,\"start\":42600},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":43149,\"start\":43145},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":43454,\"start\":43450},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":43463,\"start\":43459}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":46971,\"start\":46566},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47154,\"start\":46972},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47247,\"start\":47155},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47253,\"start\":47248},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47409,\"start\":47254},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47501,\"start\":47410},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47689,\"start\":47502},{\"attributes\":{\"id\":\"fig_9\"},\"end\":47994,\"start\":47690},{\"attributes\":{\"id\":\"fig_10\"},\"end\":48208,\"start\":47995},{\"attributes\":{\"id\":\"fig_11\"},\"end\":48300,\"start\":48209},{\"attributes\":{\"id\":\"fig_13\"},\"end\":48396,\"start\":48301},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":48510,\"start\":48397},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":49233,\"start\":48511},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":50263,\"start\":49234},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50736,\"start\":50264},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":51569,\"start\":50737},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":52540,\"start\":51570},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":52941,\"start\":52541},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":53310,\"start\":52942},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":53579,\"start\":53311},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":53878,\"start\":53580}]", "paragraph": "[{\"end\":2282,\"start\":1723},{\"end\":3007,\"start\":2284},{\"end\":3393,\"start\":3009},{\"end\":4019,\"start\":3412},{\"end\":4300,\"start\":4021},{\"end\":4705,\"start\":4317},{\"end\":5089,\"start\":4707},{\"end\":5479,\"start\":5091},{\"end\":5999,\"start\":5481},{\"end\":6709,\"start\":6001},{\"end\":6979,\"start\":6744},{\"end\":7277,\"start\":7036},{\"end\":7391,\"start\":7358},{\"end\":7451,\"start\":7449},{\"end\":7541,\"start\":7468},{\"end\":7679,\"start\":7548},{\"end\":8119,\"start\":7688},{\"end\":8592,\"start\":8209},{\"end\":8784,\"start\":8614},{\"end\":8884,\"start\":8825},{\"end\":8963,\"start\":8928},{\"end\":9118,\"start\":8999},{\"end\":9211,\"start\":9176},{\"end\":9415,\"start\":9349},{\"end\":9674,\"start\":9433},{\"end\":9813,\"start\":9736},{\"end\":9902,\"start\":9863},{\"end\":10071,\"start\":9904},{\"end\":10276,\"start\":10122},{\"end\":10543,\"start\":10278},{\"end\":10690,\"start\":10620},{\"end\":10849,\"start\":10741},{\"end\":10992,\"start\":10881},{\"end\":11113,\"start\":10994},{\"end\":11213,\"start\":11115},{\"end\":11511,\"start\":11319},{\"end\":12108,\"start\":11513},{\"end\":12905,\"start\":12419},{\"end\":13344,\"start\":12928},{\"end\":13582,\"start\":13346},{\"end\":13693,\"start\":13651},{\"end\":13997,\"start\":13987},{\"end\":14469,\"start\":14203},{\"end\":14607,\"start\":14586},{\"end\":14898,\"start\":14654},{\"end\":14960,\"start\":14900},{\"end\":15172,\"start\":14998},{\"end\":15413,\"start\":15174},{\"end\":15646,\"start\":15454},{\"end\":15898,\"start\":15687},{\"end\":16059,\"start\":15969},{\"end\":16392,\"start\":16138},{\"end\":16589,\"start\":16425},{\"end\":16695,\"start\":16636},{\"end\":16729,\"start\":16697},{\"end\":16859,\"start\":16754},{\"end\":17010,\"start\":17001},{\"end\":17073,\"start\":17049},{\"end\":17084,\"start\":17075},{\"end\":17171,\"start\":17161},{\"end\":17184,\"start\":17181},{\"end\":17207,\"start\":17194},{\"end\":17260,\"start\":17242},{\"end\":17308,\"start\":17297},{\"end\":17319,\"start\":17310},{\"end\":17368,\"start\":17357},{\"end\":17405,\"start\":17404},{\"end\":17860,\"start\":17540},{\"end\":18160,\"start\":17862},{\"end\":18483,\"start\":18162},{\"end\":18807,\"start\":18576},{\"end\":18918,\"start\":18857},{\"end\":19335,\"start\":18972},{\"end\":19455,\"start\":19404},{\"end\":19667,\"start\":19457},{\"end\":19793,\"start\":19787},{\"end\":19990,\"start\":19865},{\"end\":20647,\"start\":20044},{\"end\":20767,\"start\":20675},{\"end\":20877,\"start\":20769},{\"end\":21160,\"start\":20916},{\"end\":21501,\"start\":21388},{\"end\":22616,\"start\":21691},{\"end\":23741,\"start\":22618},{\"end\":24108,\"start\":23743},{\"end\":24720,\"start\":24110},{\"end\":25022,\"start\":24722},{\"end\":25918,\"start\":25032},{\"end\":26265,\"start\":25966},{\"end\":26923,\"start\":26267},{\"end\":27679,\"start\":26925},{\"end\":28167,\"start\":27681},{\"end\":28684,\"start\":28182},{\"end\":29388,\"start\":28686},{\"end\":30077,\"start\":29390},{\"end\":30546,\"start\":30092},{\"end\":30674,\"start\":30548},{\"end\":30894,\"start\":30676},{\"end\":31041,\"start\":30896},{\"end\":31557,\"start\":31043},{\"end\":32270,\"start\":31576},{\"end\":32459,\"start\":32327},{\"end\":33213,\"start\":32519},{\"end\":33471,\"start\":33326},{\"end\":33579,\"start\":33513},{\"end\":34347,\"start\":33581},{\"end\":34437,\"start\":34390},{\"end\":34751,\"start\":34439},{\"end\":34914,\"start\":34779},{\"end\":35612,\"start\":35028},{\"end\":35773,\"start\":35614},{\"end\":36218,\"start\":35775},{\"end\":36545,\"start\":36238},{\"end\":36630,\"start\":36572},{\"end\":37024,\"start\":36752},{\"end\":37206,\"start\":37119},{\"end\":37311,\"start\":37264},{\"end\":37452,\"start\":37450},{\"end\":37582,\"start\":37570},{\"end\":37682,\"start\":37597},{\"end\":37781,\"start\":37738},{\"end\":37858,\"start\":37817},{\"end\":38186,\"start\":38039},{\"end\":38379,\"start\":38188},{\"end\":38554,\"start\":38381},{\"end\":38742,\"start\":38556},{\"end\":38916,\"start\":38744},{\"end\":39366,\"start\":38968},{\"end\":39677,\"start\":39416},{\"end\":39742,\"start\":39679},{\"end\":39875,\"start\":39772},{\"end\":40060,\"start\":40050},{\"end\":40068,\"start\":40062},{\"end\":40253,\"start\":40196},{\"end\":40632,\"start\":40255},{\"end\":40835,\"start\":40682},{\"end\":41386,\"start\":40837},{\"end\":41542,\"start\":41388},{\"end\":41979,\"start\":41544},{\"end\":42277,\"start\":41981},{\"end\":42605,\"start\":42287},{\"end\":43164,\"start\":42642},{\"end\":43199,\"start\":43166},{\"end\":43482,\"start\":43228},{\"end\":43812,\"start\":43511},{\"end\":44179,\"start\":43845},{\"end\":44783,\"start\":44181},{\"end\":45364,\"start\":44785},{\"end\":46249,\"start\":45409},{\"end\":46325,\"start\":46251},{\"end\":46565,\"start\":46384},{\"end\":46970,\"start\":46580},{\"end\":47153,\"start\":46986},{\"end\":47246,\"start\":47169},{\"end\":47408,\"start\":47277},{\"end\":47500,\"start\":47424},{\"end\":47688,\"start\":47516},{\"end\":47993,\"start\":47715},{\"end\":48207,\"start\":48009},{\"end\":48299,\"start\":48213},{\"end\":48395,\"start\":48315},{\"end\":48468,\"start\":48410},{\"end\":49206,\"start\":48524},{\"end\":49809,\"start\":49247},{\"end\":50262,\"start\":49837},{\"end\":50459,\"start\":50277},{\"end\":50915,\"start\":50750},{\"end\":52284,\"start\":51583},{\"end\":52614,\"start\":52554},{\"end\":53302,\"start\":52955},{\"end\":53381,\"start\":53324},{\"end\":53743,\"start\":53595}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7035,\"start\":6980},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7357,\"start\":7278},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7448,\"start\":7392},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7547,\"start\":7542},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7687,\"start\":7680},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8208,\"start\":8120},{\"attributes\":{\"id\":\"formula_6\"},\"end\":8824,\"start\":8785},{\"attributes\":{\"id\":\"formula_7\"},\"end\":8927,\"start\":8885},{\"attributes\":{\"id\":\"formula_8\"},\"end\":8998,\"start\":8964},{\"attributes\":{\"id\":\"formula_9\"},\"end\":9175,\"start\":9119},{\"attributes\":{\"id\":\"formula_10\"},\"end\":9348,\"start\":9212},{\"attributes\":{\"id\":\"formula_11\"},\"end\":9432,\"start\":9416},{\"attributes\":{\"id\":\"formula_12\"},\"end\":9735,\"start\":9675},{\"attributes\":{\"id\":\"formula_13\"},\"end\":9862,\"start\":9814},{\"attributes\":{\"id\":\"formula_14\"},\"end\":10121,\"start\":10072},{\"attributes\":{\"id\":\"formula_15\"},\"end\":10619,\"start\":10544},{\"attributes\":{\"id\":\"formula_16\"},\"end\":10740,\"start\":10691},{\"attributes\":{\"id\":\"formula_17\"},\"end\":10880,\"start\":10850},{\"attributes\":{\"id\":\"formula_18\"},\"end\":11318,\"start\":11214},{\"attributes\":{\"id\":\"formula_19\"},\"end\":12418,\"start\":12218},{\"attributes\":{\"id\":\"formula_20\"},\"end\":13650,\"start\":13583},{\"attributes\":{\"id\":\"formula_21\"},\"end\":13986,\"start\":13694},{\"attributes\":{\"id\":\"formula_22\"},\"end\":14202,\"start\":13998},{\"attributes\":{\"id\":\"formula_23\"},\"end\":14585,\"start\":14470},{\"attributes\":{\"id\":\"formula_24\"},\"end\":14653,\"start\":14608},{\"attributes\":{\"id\":\"formula_25\"},\"end\":14997,\"start\":14961},{\"attributes\":{\"id\":\"formula_26\"},\"end\":15686,\"start\":15647},{\"attributes\":{\"id\":\"formula_27\"},\"end\":15968,\"start\":15899},{\"attributes\":{\"id\":\"formula_28\"},\"end\":16137,\"start\":16060},{\"attributes\":{\"id\":\"formula_29\"},\"end\":16424,\"start\":16393},{\"attributes\":{\"id\":\"formula_30\"},\"end\":16635,\"start\":16590},{\"attributes\":{\"id\":\"formula_31\"},\"end\":16753,\"start\":16730},{\"attributes\":{\"id\":\"formula_32\"},\"end\":17000,\"start\":16860},{\"attributes\":{\"id\":\"formula_33\"},\"end\":17048,\"start\":17011},{\"attributes\":{\"id\":\"formula_34\"},\"end\":17160,\"start\":17085},{\"attributes\":{\"id\":\"formula_35\"},\"end\":17180,\"start\":17172},{\"attributes\":{\"id\":\"formula_36\"},\"end\":17193,\"start\":17185},{\"attributes\":{\"id\":\"formula_37\"},\"end\":17241,\"start\":17208},{\"attributes\":{\"id\":\"formula_38\"},\"end\":17296,\"start\":17261},{\"attributes\":{\"id\":\"formula_39\"},\"end\":17356,\"start\":17320},{\"attributes\":{\"id\":\"formula_40\"},\"end\":17403,\"start\":17369},{\"attributes\":{\"id\":\"formula_41\"},\"end\":17539,\"start\":17406},{\"attributes\":{\"id\":\"formula_42\"},\"end\":18575,\"start\":18484},{\"attributes\":{\"id\":\"formula_43\"},\"end\":18856,\"start\":18808},{\"attributes\":{\"id\":\"formula_44\"},\"end\":18971,\"start\":18919},{\"attributes\":{\"id\":\"formula_45\"},\"end\":19403,\"start\":19336},{\"attributes\":{\"id\":\"formula_46\"},\"end\":19786,\"start\":19668},{\"attributes\":{\"id\":\"formula_47\"},\"end\":19864,\"start\":19794},{\"attributes\":{\"id\":\"formula_48\"},\"end\":20043,\"start\":19991},{\"attributes\":{\"id\":\"formula_49\"},\"end\":20674,\"start\":20648},{\"attributes\":{\"id\":\"formula_50\"},\"end\":20915,\"start\":20878},{\"attributes\":{\"id\":\"formula_51\"},\"end\":21387,\"start\":21161},{\"attributes\":{\"id\":\"formula_52\"},\"end\":21690,\"start\":21502},{\"attributes\":{\"id\":\"formula_53\"},\"end\":32518,\"start\":32460},{\"attributes\":{\"id\":\"formula_54\"},\"end\":33325,\"start\":33214},{\"attributes\":{\"id\":\"formula_55\"},\"end\":33512,\"start\":33472},{\"attributes\":{\"id\":\"formula_56\"},\"end\":34389,\"start\":34348},{\"attributes\":{\"id\":\"formula_57\"},\"end\":34778,\"start\":34752},{\"attributes\":{\"id\":\"formula_58\"},\"end\":35027,\"start\":34915},{\"attributes\":{\"id\":\"formula_59\"},\"end\":36237,\"start\":36219},{\"attributes\":{\"id\":\"formula_60\"},\"end\":36751,\"start\":36631},{\"attributes\":{\"id\":\"formula_61\"},\"end\":37118,\"start\":37025},{\"attributes\":{\"id\":\"formula_62\"},\"end\":37263,\"start\":37207},{\"attributes\":{\"id\":\"formula_63\"},\"end\":37449,\"start\":37312},{\"attributes\":{\"id\":\"formula_64\"},\"end\":37569,\"start\":37453},{\"attributes\":{\"id\":\"formula_65\"},\"end\":37596,\"start\":37583},{\"attributes\":{\"id\":\"formula_66\"},\"end\":37737,\"start\":37683},{\"attributes\":{\"id\":\"formula_67\"},\"end\":37816,\"start\":37782},{\"attributes\":{\"id\":\"formula_68\"},\"end\":38038,\"start\":37859},{\"attributes\":{\"id\":\"formula_69\"},\"end\":39415,\"start\":39367},{\"attributes\":{\"id\":\"formula_70\"},\"end\":39771,\"start\":39743},{\"attributes\":{\"id\":\"formula_71\"},\"end\":40049,\"start\":39876},{\"attributes\":{\"id\":\"formula_72\"},\"end\":40195,\"start\":40069},{\"attributes\":{\"id\":\"formula_74\"},\"end\":43227,\"start\":43200},{\"attributes\":{\"id\":\"formula_75\"},\"end\":45408,\"start\":45365},{\"attributes\":{\"id\":\"formula_76\"},\"end\":46383,\"start\":46326}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13105,\"start\":13104},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23776,\"start\":23775},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25594,\"start\":25593},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28156,\"start\":28155},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":32204,\"start\":32202},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":41668,\"start\":41667},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":42523,\"start\":42522},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":44477,\"start\":44476},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":45012,\"start\":45010}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1721,\"start\":1709},{\"end\":3410,\"start\":3396},{\"attributes\":{\"n\":\"2\"},\"end\":4315,\"start\":4303},{\"attributes\":{\"n\":\"3\"},\"end\":6722,\"start\":6712},{\"attributes\":{\"n\":\"3.1\"},\"end\":6742,\"start\":6725},{\"end\":7466,\"start\":7454},{\"attributes\":{\"n\":\"3.2\"},\"end\":8612,\"start\":8595},{\"attributes\":{\"n\":\"3.3\"},\"end\":12130,\"start\":12111},{\"end\":12217,\"start\":12133},{\"attributes\":{\"n\":\"4\"},\"end\":12926,\"start\":12908},{\"attributes\":{\"n\":\"4.2\"},\"end\":15452,\"start\":15416},{\"end\":25030,\"start\":25025},{\"attributes\":{\"n\":\"5.2\"},\"end\":25964,\"start\":25921},{\"attributes\":{\"n\":\"6\"},\"end\":28180,\"start\":28170},{\"attributes\":{\"n\":\"7\"},\"end\":30090,\"start\":30080},{\"end\":31574,\"start\":31560},{\"end\":32325,\"start\":32273},{\"end\":36570,\"start\":36548},{\"end\":38966,\"start\":38919},{\"end\":40680,\"start\":40635},{\"end\":42285,\"start\":42280},{\"end\":42640,\"start\":42608},{\"end\":43509,\"start\":43485},{\"end\":43843,\"start\":43815},{\"end\":46577,\"start\":46567},{\"end\":46983,\"start\":46973},{\"end\":47166,\"start\":47156},{\"end\":47250,\"start\":47249},{\"end\":47273,\"start\":47255},{\"end\":47421,\"start\":47411},{\"end\":47513,\"start\":47503},{\"end\":47711,\"start\":47691},{\"end\":48006,\"start\":47996},{\"end\":48211,\"start\":48210},{\"end\":48312,\"start\":48302},{\"end\":48407,\"start\":48398},{\"end\":48521,\"start\":48512},{\"end\":49244,\"start\":49235},{\"end\":50274,\"start\":50265},{\"end\":50747,\"start\":50738},{\"end\":51580,\"start\":51571},{\"end\":52551,\"start\":52542},{\"end\":52952,\"start\":52943},{\"end\":53321,\"start\":53312},{\"end\":53591,\"start\":53581}]", "table": "[{\"end\":48510,\"start\":48469},{\"end\":49233,\"start\":49207},{\"end\":49836,\"start\":49810},{\"end\":50736,\"start\":50460},{\"end\":51569,\"start\":50916},{\"end\":52540,\"start\":52285},{\"end\":52941,\"start\":52615},{\"end\":53310,\"start\":53303},{\"end\":53579,\"start\":53382},{\"end\":53878,\"start\":53744}]", "figure_caption": "[{\"end\":46971,\"start\":46579},{\"end\":47154,\"start\":46985},{\"end\":47247,\"start\":47168},{\"end\":47253,\"start\":47251},{\"end\":47409,\"start\":47276},{\"end\":47501,\"start\":47423},{\"end\":47689,\"start\":47515},{\"end\":47994,\"start\":47714},{\"end\":48208,\"start\":48008},{\"end\":48300,\"start\":48212},{\"end\":48396,\"start\":48314},{\"end\":48469,\"start\":48409},{\"end\":49207,\"start\":48523},{\"end\":49810,\"start\":49246},{\"end\":50460,\"start\":50276},{\"end\":50916,\"start\":50749},{\"end\":52285,\"start\":51582},{\"end\":52615,\"start\":52553},{\"end\":53303,\"start\":52954},{\"end\":53382,\"start\":53323},{\"end\":53744,\"start\":53594}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18159,\"start\":18158},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19027,\"start\":19026},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20766,\"start\":20764},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22281,\"start\":22279},{\"end\":26264,\"start\":26263},{\"end\":26278,\"start\":26277},{\"end\":26596,\"start\":26595},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26949,\"start\":26948},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27518,\"start\":27517},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27618,\"start\":27617},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":46415,\"start\":46414}]", "bib_author_first_name": "[{\"end\":54491,\"start\":54484},{\"end\":54504,\"start\":54500},{\"end\":54519,\"start\":54514},{\"end\":54531,\"start\":54526},{\"end\":54691,\"start\":54684},{\"end\":54704,\"start\":54700},{\"end\":54720,\"start\":54714},{\"end\":54735,\"start\":54730},{\"end\":54906,\"start\":54898},{\"end\":54938,\"start\":54934},{\"end\":55179,\"start\":55172},{\"end\":55200,\"start\":55191},{\"end\":55207,\"start\":55203},{\"end\":55222,\"start\":55217},{\"end\":55470,\"start\":55463},{\"end\":55491,\"start\":55482},{\"end\":55498,\"start\":55494},{\"end\":55513,\"start\":55508},{\"end\":55723,\"start\":55716},{\"end\":55737,\"start\":55733},{\"end\":55752,\"start\":55747},{\"end\":55903,\"start\":55896},{\"end\":55916,\"start\":55911},{\"end\":55932,\"start\":55925},{\"end\":56097,\"start\":56091},{\"end\":56099,\"start\":56098},{\"end\":56117,\"start\":56109},{\"end\":56132,\"start\":56125},{\"end\":56146,\"start\":56142},{\"end\":56159,\"start\":56154},{\"end\":56177,\"start\":56172},{\"end\":56193,\"start\":56186},{\"end\":56209,\"start\":56201},{\"end\":56333,\"start\":56327},{\"end\":56347,\"start\":56341},{\"end\":56361,\"start\":56357},{\"end\":56543,\"start\":56536},{\"end\":56562,\"start\":56554},{\"end\":56586,\"start\":56574},{\"end\":56678,\"start\":56672},{\"end\":56691,\"start\":56686},{\"end\":56822,\"start\":56814},{\"end\":56838,\"start\":56830},{\"end\":56853,\"start\":56849},{\"end\":56868,\"start\":56862},{\"end\":56882,\"start\":56878},{\"end\":56903,\"start\":56895},{\"end\":56921,\"start\":56916},{\"end\":56938,\"start\":56932},{\"end\":57080,\"start\":57076},{\"end\":57092,\"start\":57087},{\"end\":57103,\"start\":57097},{\"end\":57115,\"start\":57111},{\"end\":57296,\"start\":57288},{\"end\":57309,\"start\":57303},{\"end\":57319,\"start\":57315},{\"end\":57330,\"start\":57325},{\"end\":57342,\"start\":57338},{\"end\":57353,\"start\":57351},{\"end\":57611,\"start\":57608},{\"end\":57616,\"start\":57614},{\"end\":57633,\"start\":57624},{\"end\":57645,\"start\":57641},{\"end\":57845,\"start\":57840},{\"end\":57974,\"start\":57966},{\"end\":58129,\"start\":58123},{\"end\":58143,\"start\":58137},{\"end\":58159,\"start\":58153},{\"end\":58173,\"start\":58169},{\"end\":58188,\"start\":58182},{\"end\":58300,\"start\":58293},{\"end\":58326,\"start\":58322},{\"end\":58332,\"start\":58327},{\"end\":58348,\"start\":58341},{\"end\":58472,\"start\":58465},{\"end\":58485,\"start\":58480},{\"end\":58600,\"start\":58593},{\"end\":58683,\"start\":58676},{\"end\":58700,\"start\":58691},{\"end\":58808,\"start\":58803},{\"end\":58823,\"start\":58817},{\"end\":58894,\"start\":58888},{\"end\":58909,\"start\":58903},{\"end\":58922,\"start\":58916},{\"end\":58937,\"start\":58933},{\"end\":58951,\"start\":58947},{\"end\":58970,\"start\":58963},{\"end\":59137,\"start\":59132},{\"end\":59153,\"start\":59147},{\"end\":59162,\"start\":59156},{\"end\":59272,\"start\":59266},{\"end\":59286,\"start\":59280},{\"end\":59302,\"start\":59296},{\"end\":59316,\"start\":59312},{\"end\":59331,\"start\":59325},{\"end\":59531,\"start\":59527},{\"end\":59543,\"start\":59540},{\"end\":59560,\"start\":59551},{\"end\":59572,\"start\":59568},{\"end\":59585,\"start\":59580},{\"end\":59603,\"start\":59596},{\"end\":59618,\"start\":59612},{\"end\":59634,\"start\":59628},{\"end\":59647,\"start\":59640},{\"end\":59664,\"start\":59660},{\"end\":59678,\"start\":59673},{\"end\":59697,\"start\":59690},{\"end\":59710,\"start\":59704},{\"end\":59724,\"start\":59717},{\"end\":59739,\"start\":59733},{\"end\":59755,\"start\":59748},{\"end\":59770,\"start\":59764},{\"end\":59791,\"start\":59785},{\"end\":59803,\"start\":59801},{\"end\":59816,\"start\":59810},{\"end\":59829,\"start\":59822},{\"end\":59892,\"start\":59891},{\"end\":59903,\"start\":59902},{\"end\":59917,\"start\":59916},{\"end\":59932,\"start\":59931},{\"end\":59945,\"start\":59944},{\"end\":59952,\"start\":59951},{\"end\":60027,\"start\":60023},{\"end\":60044,\"start\":60037},{\"end\":60062,\"start\":60053},{\"end\":60080,\"start\":60073},{\"end\":60107,\"start\":60098},{\"end\":60127,\"start\":60119},{\"end\":60143,\"start\":60137},{\"end\":60158,\"start\":60152},{\"end\":60174,\"start\":60170},{\"end\":60268,\"start\":60261},{\"end\":60282,\"start\":60278},{\"end\":60296,\"start\":60290},{\"end\":60413,\"start\":60409},{\"end\":60427,\"start\":60422},{\"end\":60440,\"start\":60435},{\"end\":60454,\"start\":60448},{\"end\":60588,\"start\":60582},{\"end\":60605,\"start\":60599},{\"end\":60619,\"start\":60615},{\"end\":60633,\"start\":60628},{\"end\":60828,\"start\":60824},{\"end\":60830,\"start\":60829},{\"end\":60854,\"start\":60849},{\"end\":60856,\"start\":60855},{\"end\":60869,\"start\":60864},{\"end\":60980,\"start\":60973},{\"end\":60997,\"start\":60990},{\"end\":61010,\"start\":61005},{\"end\":61118,\"start\":61112},{\"end\":61135,\"start\":61129},{\"end\":61356,\"start\":61349},{\"end\":61368,\"start\":61361},{\"end\":61384,\"start\":61376},{\"end\":61394,\"start\":61390},{\"end\":61666,\"start\":61664},{\"end\":61677,\"start\":61674},{\"end\":61689,\"start\":61682},{\"end\":61703,\"start\":61694},{\"end\":61811,\"start\":61807},{\"end\":61825,\"start\":61818},{\"end\":61838,\"start\":61830},{\"end\":61849,\"start\":61843},{\"end\":62018,\"start\":62012},{\"end\":62094,\"start\":62090},{\"end\":62110,\"start\":62106},{\"end\":62124,\"start\":62120},{\"end\":62138,\"start\":62134},{\"end\":62156,\"start\":62148},{\"end\":62174,\"start\":62166},{\"end\":62182,\"start\":62175},{\"end\":62352,\"start\":62344},{\"end\":62367,\"start\":62361},{\"end\":62383,\"start\":62377},{\"end\":62482,\"start\":62478},{\"end\":62498,\"start\":62490},{\"end\":62524,\"start\":62509},{\"end\":62543,\"start\":62534},{\"end\":62556,\"start\":62551},{\"end\":62572,\"start\":62565},{\"end\":62714,\"start\":62707},{\"end\":62727,\"start\":62721},{\"end\":62741,\"start\":62735},{\"end\":62743,\"start\":62742},{\"end\":62845,\"start\":62838},{\"end\":62969,\"start\":62963},{\"end\":62984,\"start\":62978},{\"end\":62995,\"start\":62991},{\"end\":63009,\"start\":63005},{\"end\":63028,\"start\":63021}]", "bib_author_last_name": "[{\"end\":54498,\"start\":54492},{\"end\":54512,\"start\":54505},{\"end\":54524,\"start\":54520},{\"end\":54545,\"start\":54532},{\"end\":54698,\"start\":54692},{\"end\":54712,\"start\":54705},{\"end\":54728,\"start\":54721},{\"end\":54749,\"start\":54736},{\"end\":54932,\"start\":54907},{\"end\":54951,\"start\":54939},{\"end\":54960,\"start\":54953},{\"end\":55189,\"start\":55180},{\"end\":55215,\"start\":55208},{\"end\":55229,\"start\":55223},{\"end\":55480,\"start\":55471},{\"end\":55506,\"start\":55499},{\"end\":55520,\"start\":55514},{\"end\":55731,\"start\":55724},{\"end\":55745,\"start\":55738},{\"end\":55766,\"start\":55753},{\"end\":55909,\"start\":55904},{\"end\":55923,\"start\":55917},{\"end\":55938,\"start\":55933},{\"end\":56107,\"start\":56100},{\"end\":56123,\"start\":56118},{\"end\":56140,\"start\":56133},{\"end\":56152,\"start\":56147},{\"end\":56170,\"start\":56160},{\"end\":56184,\"start\":56178},{\"end\":56199,\"start\":56194},{\"end\":56216,\"start\":56210},{\"end\":56339,\"start\":56334},{\"end\":56355,\"start\":56348},{\"end\":56368,\"start\":56362},{\"end\":56552,\"start\":56544},{\"end\":56572,\"start\":56563},{\"end\":56593,\"start\":56587},{\"end\":56684,\"start\":56679},{\"end\":56695,\"start\":56692},{\"end\":56701,\"start\":56697},{\"end\":56828,\"start\":56823},{\"end\":56847,\"start\":56839},{\"end\":56860,\"start\":56854},{\"end\":56876,\"start\":56869},{\"end\":56893,\"start\":56883},{\"end\":56914,\"start\":56904},{\"end\":56930,\"start\":56922},{\"end\":56948,\"start\":56939},{\"end\":57085,\"start\":57081},{\"end\":57095,\"start\":57093},{\"end\":57109,\"start\":57104},{\"end\":57118,\"start\":57116},{\"end\":57301,\"start\":57297},{\"end\":57313,\"start\":57310},{\"end\":57323,\"start\":57320},{\"end\":57336,\"start\":57331},{\"end\":57349,\"start\":57343},{\"end\":57358,\"start\":57354},{\"end\":57622,\"start\":57617},{\"end\":57639,\"start\":57634},{\"end\":57852,\"start\":57846},{\"end\":57981,\"start\":57975},{\"end\":58135,\"start\":58130},{\"end\":58151,\"start\":58144},{\"end\":58167,\"start\":58160},{\"end\":58180,\"start\":58174},{\"end\":58196,\"start\":58189},{\"end\":58320,\"start\":58301},{\"end\":58339,\"start\":58333},{\"end\":58359,\"start\":58349},{\"end\":58478,\"start\":58473},{\"end\":58492,\"start\":58486},{\"end\":58613,\"start\":58601},{\"end\":58689,\"start\":58684},{\"end\":58708,\"start\":58701},{\"end\":58815,\"start\":58809},{\"end\":58830,\"start\":58824},{\"end\":58901,\"start\":58895},{\"end\":58914,\"start\":58910},{\"end\":58931,\"start\":58923},{\"end\":58945,\"start\":58938},{\"end\":58961,\"start\":58952},{\"end\":58977,\"start\":58971},{\"end\":59145,\"start\":59138},{\"end\":59167,\"start\":59163},{\"end\":59278,\"start\":59273},{\"end\":59294,\"start\":59287},{\"end\":59310,\"start\":59303},{\"end\":59323,\"start\":59317},{\"end\":59339,\"start\":59332},{\"end\":59409,\"start\":59400},{\"end\":59538,\"start\":59532},{\"end\":59549,\"start\":59544},{\"end\":59566,\"start\":59561},{\"end\":59578,\"start\":59573},{\"end\":59594,\"start\":59586},{\"end\":59610,\"start\":59604},{\"end\":59626,\"start\":59619},{\"end\":59638,\"start\":59635},{\"end\":59658,\"start\":59648},{\"end\":59671,\"start\":59665},{\"end\":59688,\"start\":59679},{\"end\":59702,\"start\":59698},{\"end\":59715,\"start\":59711},{\"end\":59731,\"start\":59725},{\"end\":59746,\"start\":59740},{\"end\":59762,\"start\":59756},{\"end\":59783,\"start\":59771},{\"end\":59799,\"start\":59792},{\"end\":59808,\"start\":59804},{\"end\":59820,\"start\":59817},{\"end\":59838,\"start\":59830},{\"end\":59900,\"start\":59893},{\"end\":59914,\"start\":59904},{\"end\":59929,\"start\":59918},{\"end\":59942,\"start\":59933},{\"end\":59949,\"start\":59946},{\"end\":59960,\"start\":59953},{\"end\":60035,\"start\":60028},{\"end\":60051,\"start\":60045},{\"end\":60071,\"start\":60063},{\"end\":60096,\"start\":60081},{\"end\":60117,\"start\":60108},{\"end\":60135,\"start\":60128},{\"end\":60150,\"start\":60144},{\"end\":60168,\"start\":60159},{\"end\":60181,\"start\":60175},{\"end\":60191,\"start\":60183},{\"end\":60276,\"start\":60269},{\"end\":60288,\"start\":60283},{\"end\":60303,\"start\":60297},{\"end\":60420,\"start\":60414},{\"end\":60433,\"start\":60428},{\"end\":60446,\"start\":60441},{\"end\":60462,\"start\":60455},{\"end\":60597,\"start\":60589},{\"end\":60613,\"start\":60606},{\"end\":60626,\"start\":60620},{\"end\":60644,\"start\":60634},{\"end\":60847,\"start\":60831},{\"end\":60862,\"start\":60857},{\"end\":60874,\"start\":60870},{\"end\":60880,\"start\":60876},{\"end\":60988,\"start\":60981},{\"end\":61003,\"start\":60998},{\"end\":61017,\"start\":61011},{\"end\":61127,\"start\":61119},{\"end\":61148,\"start\":61136},{\"end\":61359,\"start\":61357},{\"end\":61374,\"start\":61369},{\"end\":61388,\"start\":61385},{\"end\":61398,\"start\":61395},{\"end\":61672,\"start\":61667},{\"end\":61680,\"start\":61678},{\"end\":61692,\"start\":61690},{\"end\":61706,\"start\":61704},{\"end\":61816,\"start\":61812},{\"end\":61828,\"start\":61826},{\"end\":61841,\"start\":61839},{\"end\":61855,\"start\":61850},{\"end\":62026,\"start\":62019},{\"end\":62104,\"start\":62095},{\"end\":62118,\"start\":62111},{\"end\":62132,\"start\":62125},{\"end\":62146,\"start\":62139},{\"end\":62164,\"start\":62157},{\"end\":62190,\"start\":62183},{\"end\":62359,\"start\":62353},{\"end\":62375,\"start\":62368},{\"end\":62391,\"start\":62384},{\"end\":62488,\"start\":62483},{\"end\":62507,\"start\":62499},{\"end\":62532,\"start\":62525},{\"end\":62549,\"start\":62544},{\"end\":62563,\"start\":62557},{\"end\":62578,\"start\":62573},{\"end\":62719,\"start\":62715},{\"end\":62733,\"start\":62728},{\"end\":62751,\"start\":62744},{\"end\":62852,\"start\":62846},{\"end\":62976,\"start\":62970},{\"end\":62989,\"start\":62985},{\"end\":63003,\"start\":62996},{\"end\":63019,\"start\":63010},{\"end\":63035,\"start\":63029}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13347901},\"end\":54617,\"start\":54443},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9634599},\"end\":54808,\"start\":54619},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4331539},\"end\":55111,\"start\":54810},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":235681434},\"end\":55403,\"start\":55113},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":235771722},\"end\":55631,\"start\":55405},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":219980389},\"end\":55835,\"start\":55633},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1726037},\"end\":55988,\"start\":55837},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":215821071},\"end\":56259,\"start\":55990},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5111106},\"end\":56466,\"start\":56261},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":17547265},\"end\":56639,\"start\":56468},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":740063},\"end\":56764,\"start\":56641},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2871880},\"end\":56999,\"start\":56766},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3861990},\"end\":57235,\"start\":57001},{\"attributes\":{\"id\":\"b13\"},\"end\":57493,\"start\":57237},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7695816},\"end\":57773,\"start\":57495},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15966283},\"end\":57911,\"start\":57775},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":15348764},\"end\":58039,\"start\":57913},{\"attributes\":{\"doi\":\"arXiv:2004.11829\",\"id\":\"b17\"},\"end\":58232,\"start\":58041},{\"attributes\":{\"doi\":\"arXiv:2306.16156\",\"id\":\"b18\"},\"end\":58395,\"start\":58234},{\"attributes\":{\"id\":\"b19\"},\"end\":58545,\"start\":58397},{\"attributes\":{\"id\":\"b20\"},\"end\":58636,\"start\":58547},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8592977},\"end\":58756,\"start\":58638},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16786361},\"end\":58886,\"start\":58758},{\"attributes\":{\"doi\":\"arXiv:2101.01792\",\"id\":\"b23\"},\"end\":59079,\"start\":58888},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":211066128},\"end\":59226,\"start\":59081},{\"attributes\":{\"id\":\"b25\"},\"end\":59353,\"start\":59228},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":121647477},\"end\":59455,\"start\":59355},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":202786778},\"end\":59990,\"start\":59457},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":233238471},\"end\":60222,\"start\":59992},{\"attributes\":{\"id\":\"b29\"},\"end\":60359,\"start\":60224},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7534823},\"end\":60516,\"start\":60361},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":9440223},\"end\":60771,\"start\":60518},{\"attributes\":{\"id\":\"b32\"},\"end\":60888,\"start\":60773},{\"attributes\":{\"id\":\"b33\"},\"end\":61044,\"start\":60890},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":230338579},\"end\":61301,\"start\":61046},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206594692},\"end\":61545,\"start\":61303},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":54443549},\"end\":61726,\"start\":61547},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":220633368},\"end\":61978,\"start\":61728},{\"attributes\":{\"id\":\"b38\"},\"end\":62043,\"start\":61980},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":8577357},\"end\":62217,\"start\":62045},{\"attributes\":{\"id\":\"b40\"},\"end\":62400,\"start\":62219},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":84834062},\"end\":62665,\"start\":62402},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":206797746},\"end\":62785,\"start\":62667},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":209852063},\"end\":62888,\"start\":62787},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":203951343},\"end\":63050,\"start\":62890}]", "bib_title": "[{\"end\":54482,\"start\":54443},{\"end\":54682,\"start\":54619},{\"end\":54896,\"start\":54810},{\"end\":55170,\"start\":55113},{\"end\":55461,\"start\":55405},{\"end\":55714,\"start\":55633},{\"end\":55894,\"start\":55837},{\"end\":56089,\"start\":55990},{\"end\":56325,\"start\":56261},{\"end\":56534,\"start\":56468},{\"end\":56670,\"start\":56641},{\"end\":56812,\"start\":56766},{\"end\":57074,\"start\":57001},{\"end\":57286,\"start\":57237},{\"end\":57606,\"start\":57495},{\"end\":57838,\"start\":57775},{\"end\":57964,\"start\":57913},{\"end\":58463,\"start\":58397},{\"end\":58674,\"start\":58638},{\"end\":58801,\"start\":58758},{\"end\":59130,\"start\":59081},{\"end\":59398,\"start\":59355},{\"end\":59525,\"start\":59457},{\"end\":60021,\"start\":59992},{\"end\":60407,\"start\":60361},{\"end\":60580,\"start\":60518},{\"end\":60971,\"start\":60890},{\"end\":61110,\"start\":61046},{\"end\":61347,\"start\":61303},{\"end\":61662,\"start\":61547},{\"end\":61805,\"start\":61728},{\"end\":62088,\"start\":62045},{\"end\":62476,\"start\":62402},{\"end\":62705,\"start\":62667},{\"end\":62836,\"start\":62787},{\"end\":62961,\"start\":62890}]", "bib_author": "[{\"end\":54500,\"start\":54484},{\"end\":54514,\"start\":54500},{\"end\":54526,\"start\":54514},{\"end\":54547,\"start\":54526},{\"end\":54700,\"start\":54684},{\"end\":54714,\"start\":54700},{\"end\":54730,\"start\":54714},{\"end\":54751,\"start\":54730},{\"end\":54934,\"start\":54898},{\"end\":54953,\"start\":54934},{\"end\":54962,\"start\":54953},{\"end\":55191,\"start\":55172},{\"end\":55203,\"start\":55191},{\"end\":55217,\"start\":55203},{\"end\":55231,\"start\":55217},{\"end\":55482,\"start\":55463},{\"end\":55494,\"start\":55482},{\"end\":55508,\"start\":55494},{\"end\":55522,\"start\":55508},{\"end\":55733,\"start\":55716},{\"end\":55747,\"start\":55733},{\"end\":55768,\"start\":55747},{\"end\":55911,\"start\":55896},{\"end\":55925,\"start\":55911},{\"end\":55940,\"start\":55925},{\"end\":56109,\"start\":56091},{\"end\":56125,\"start\":56109},{\"end\":56142,\"start\":56125},{\"end\":56154,\"start\":56142},{\"end\":56172,\"start\":56154},{\"end\":56186,\"start\":56172},{\"end\":56201,\"start\":56186},{\"end\":56218,\"start\":56201},{\"end\":56341,\"start\":56327},{\"end\":56357,\"start\":56341},{\"end\":56370,\"start\":56357},{\"end\":56554,\"start\":56536},{\"end\":56574,\"start\":56554},{\"end\":56595,\"start\":56574},{\"end\":56686,\"start\":56672},{\"end\":56697,\"start\":56686},{\"end\":56703,\"start\":56697},{\"end\":56830,\"start\":56814},{\"end\":56849,\"start\":56830},{\"end\":56862,\"start\":56849},{\"end\":56878,\"start\":56862},{\"end\":56895,\"start\":56878},{\"end\":56916,\"start\":56895},{\"end\":56932,\"start\":56916},{\"end\":56950,\"start\":56932},{\"end\":57087,\"start\":57076},{\"end\":57097,\"start\":57087},{\"end\":57111,\"start\":57097},{\"end\":57120,\"start\":57111},{\"end\":57303,\"start\":57288},{\"end\":57315,\"start\":57303},{\"end\":57325,\"start\":57315},{\"end\":57338,\"start\":57325},{\"end\":57351,\"start\":57338},{\"end\":57360,\"start\":57351},{\"end\":57614,\"start\":57608},{\"end\":57624,\"start\":57614},{\"end\":57641,\"start\":57624},{\"end\":57648,\"start\":57641},{\"end\":57854,\"start\":57840},{\"end\":57983,\"start\":57966},{\"end\":58137,\"start\":58123},{\"end\":58153,\"start\":58137},{\"end\":58169,\"start\":58153},{\"end\":58182,\"start\":58169},{\"end\":58198,\"start\":58182},{\"end\":58322,\"start\":58293},{\"end\":58341,\"start\":58322},{\"end\":58361,\"start\":58341},{\"end\":58480,\"start\":58465},{\"end\":58494,\"start\":58480},{\"end\":58615,\"start\":58593},{\"end\":58691,\"start\":58676},{\"end\":58710,\"start\":58691},{\"end\":58817,\"start\":58803},{\"end\":58832,\"start\":58817},{\"end\":58903,\"start\":58888},{\"end\":58916,\"start\":58903},{\"end\":58933,\"start\":58916},{\"end\":58947,\"start\":58933},{\"end\":58963,\"start\":58947},{\"end\":58979,\"start\":58963},{\"end\":59147,\"start\":59132},{\"end\":59156,\"start\":59147},{\"end\":59169,\"start\":59156},{\"end\":59280,\"start\":59266},{\"end\":59296,\"start\":59280},{\"end\":59312,\"start\":59296},{\"end\":59325,\"start\":59312},{\"end\":59341,\"start\":59325},{\"end\":59411,\"start\":59400},{\"end\":59540,\"start\":59527},{\"end\":59551,\"start\":59540},{\"end\":59568,\"start\":59551},{\"end\":59580,\"start\":59568},{\"end\":59596,\"start\":59580},{\"end\":59612,\"start\":59596},{\"end\":59628,\"start\":59612},{\"end\":59640,\"start\":59628},{\"end\":59660,\"start\":59640},{\"end\":59673,\"start\":59660},{\"end\":59690,\"start\":59673},{\"end\":59704,\"start\":59690},{\"end\":59717,\"start\":59704},{\"end\":59733,\"start\":59717},{\"end\":59748,\"start\":59733},{\"end\":59764,\"start\":59748},{\"end\":59785,\"start\":59764},{\"end\":59801,\"start\":59785},{\"end\":59810,\"start\":59801},{\"end\":59822,\"start\":59810},{\"end\":59840,\"start\":59822},{\"end\":60037,\"start\":60023},{\"end\":60053,\"start\":60037},{\"end\":60073,\"start\":60053},{\"end\":60098,\"start\":60073},{\"end\":60119,\"start\":60098},{\"end\":60137,\"start\":60119},{\"end\":60152,\"start\":60137},{\"end\":60170,\"start\":60152},{\"end\":60183,\"start\":60170},{\"end\":60193,\"start\":60183},{\"end\":60278,\"start\":60261},{\"end\":60290,\"start\":60278},{\"end\":60305,\"start\":60290},{\"end\":60422,\"start\":60409},{\"end\":60435,\"start\":60422},{\"end\":60448,\"start\":60435},{\"end\":60464,\"start\":60448},{\"end\":60599,\"start\":60582},{\"end\":60615,\"start\":60599},{\"end\":60628,\"start\":60615},{\"end\":60646,\"start\":60628},{\"end\":60849,\"start\":60824},{\"end\":60864,\"start\":60849},{\"end\":60876,\"start\":60864},{\"end\":60882,\"start\":60876},{\"end\":60990,\"start\":60973},{\"end\":61005,\"start\":60990},{\"end\":61019,\"start\":61005},{\"end\":61129,\"start\":61112},{\"end\":61150,\"start\":61129},{\"end\":61361,\"start\":61349},{\"end\":61376,\"start\":61361},{\"end\":61390,\"start\":61376},{\"end\":61400,\"start\":61390},{\"end\":61674,\"start\":61664},{\"end\":61682,\"start\":61674},{\"end\":61694,\"start\":61682},{\"end\":61708,\"start\":61694},{\"end\":61818,\"start\":61807},{\"end\":61830,\"start\":61818},{\"end\":61843,\"start\":61830},{\"end\":61857,\"start\":61843},{\"end\":62028,\"start\":62012},{\"end\":62106,\"start\":62090},{\"end\":62120,\"start\":62106},{\"end\":62134,\"start\":62120},{\"end\":62148,\"start\":62134},{\"end\":62166,\"start\":62148},{\"end\":62192,\"start\":62166},{\"end\":62361,\"start\":62344},{\"end\":62377,\"start\":62361},{\"end\":62393,\"start\":62377},{\"end\":62490,\"start\":62478},{\"end\":62509,\"start\":62490},{\"end\":62534,\"start\":62509},{\"end\":62551,\"start\":62534},{\"end\":62565,\"start\":62551},{\"end\":62580,\"start\":62565},{\"end\":62721,\"start\":62707},{\"end\":62735,\"start\":62721},{\"end\":62753,\"start\":62735},{\"end\":62854,\"start\":62838},{\"end\":62978,\"start\":62963},{\"end\":62991,\"start\":62978},{\"end\":63005,\"start\":62991},{\"end\":63021,\"start\":63005},{\"end\":63037,\"start\":63021}]", "bib_venue": "[{\"end\":54609,\"start\":54547},{\"end\":54800,\"start\":54751},{\"end\":55026,\"start\":54962},{\"end\":55319,\"start\":55231},{\"end\":55621,\"start\":55522},{\"end\":55829,\"start\":55768},{\"end\":55978,\"start\":55940},{\"end\":56250,\"start\":56218},{\"end\":56452,\"start\":56370},{\"end\":56631,\"start\":56595},{\"end\":56754,\"start\":56703},{\"end\":56990,\"start\":56950},{\"end\":57181,\"start\":57120},{\"end\":57431,\"start\":57360},{\"end\":57715,\"start\":57648},{\"end\":57903,\"start\":57854},{\"end\":58032,\"start\":57983},{\"end\":58121,\"start\":58041},{\"end\":58291,\"start\":58234},{\"end\":58537,\"start\":58494},{\"end\":58591,\"start\":58547},{\"end\":58747,\"start\":58710},{\"end\":58876,\"start\":58832},{\"end\":59059,\"start\":58995},{\"end\":59218,\"start\":59169},{\"end\":59264,\"start\":59228},{\"end\":59446,\"start\":59411},{\"end\":59889,\"start\":59840},{\"end\":60212,\"start\":60193},{\"end\":60259,\"start\":60224},{\"end\":60502,\"start\":60464},{\"end\":60713,\"start\":60646},{\"end\":60822,\"start\":60773},{\"end\":61035,\"start\":61019},{\"end\":61230,\"start\":61150},{\"end\":61477,\"start\":61400},{\"end\":61719,\"start\":61708},{\"end\":61908,\"start\":61857},{\"end\":62010,\"start\":61980},{\"end\":62208,\"start\":62192},{\"end\":62342,\"start\":62219},{\"end\":62655,\"start\":62580},{\"end\":62777,\"start\":62753},{\"end\":62878,\"start\":62854},{\"end\":63044,\"start\":63037},{\"end\":55077,\"start\":55028},{\"end\":55394,\"start\":55321},{\"end\":57229,\"start\":57183},{\"end\":57489,\"start\":57433},{\"end\":57769,\"start\":57717},{\"end\":60767,\"start\":60715},{\"end\":61297,\"start\":61232},{\"end\":61541,\"start\":61479},{\"end\":61921,\"start\":61910}]"}}}, "year": 2023, "month": 12, "day": 17}