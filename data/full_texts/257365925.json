{"id": 257365925, "updated": "2023-10-05 03:25:00.499", "metadata": {"title": "PyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow", "authors": "[{\"first\":\"Jiarui\",\"last\":\"Lei\",\"middle\":[]},{\"first\":\"Xiaobo\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Yue\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Dong\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "During industrial processing, unforeseen defects may arise in products due to uncontrollable factors. Although unsupervised methods have been successful in defect localization, the usual use of pre-trained models results in low-resolution outputs, which damages visual performance. To address this issue, we propose PyramidFlow, the first fully normalizing flow method without pre-trained models that enables high-resolution defect localization. Specifically, we propose a latent template-based defect contrastive localization paradigm to reduce intra-class variance, as the pre-trained models do. In addition, PyramidFlow utilizes pyramid-like normalizing flows for multi-scale fusing and volume normalization to help generalization. Our comprehensive studies on MVTecAD demonstrate the proposed method outperforms the comparable algorithms that do not use external priors, even achieving state-of-the-art performance in more challenging BTAD scenarios.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2303.02595", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LeiHWL23", "doi": "10.1109/cvpr52729.2023.01359"}}, "content": {"source": {"pdf_hash": "cb958dcdf5b50dd8642492193ba8e700e7b96388", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.02595v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "db7cc6ef638061e7ee426d2491e9cc2d86106473", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cb958dcdf5b50dd8642492193ba8e700e7b96388.txt", "contents": "\nPyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow\n\n\nJiarui Lei \nState Key Laboratory of Modern Optical Instrumentation\nZhejiang University\n\n\nZJU-Hangzhou Global Scientific and Technological Innovation Center\nChina\n\nXiaobo Hu huxiaobo@zju.edu.cn \nState Key Laboratory of Modern Optical Instrumentation\nZhejiang University\n\n\nZJU-Hangzhou Global Scientific and Technological Innovation Center\nChina\n\nYue Wang \nState Key Laboratory of Modern Optical Instrumentation\nZhejiang University\n\n\nZJU-Hangzhou Global Scientific and Technological Innovation Center\nChina\n\nDong Liu liudongopt@zju.edu.cn \nState Key Laboratory of Modern Optical Instrumentation\nZhejiang University\n\n\nZJU-Hangzhou Global Scientific and Technological Innovation Center\nChina\n\nJiaxing Key Laboratory of Photonic Sensing & Intelligent Imaging\nChina\n\nIntelligent Optics & Photonics Research Center\nJiaxing Research Institute Zhejiang University\n\n\nPyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid Normalizing Flow\n\nDuring industrial processing, unforeseen defects may arise in products due to uncontrollable factors. Although unsupervised methods have been successful in defect localization, the usual use of pre-trained models results in lowresolution outputs, which damages visual performance. To address this issue, we propose PyramidFlow, the first fully normalizing flow method without pre-trained models that enables high-resolution defect localization. Specifically, we propose a latent template-based defect contrastive localization paradigm to reduce intra-class variance, as the pre-trained models do. In addition, PyramidFlow utilizes pyramid-like normalizing flows for multi-scale fusing and volume normalization to help generalization. Our comprehensive studies on MVTecAD demonstrate the proposed method outperforms the comparable algorithms that do not use external priors, even achieving state-of-the-art performance in more challenging BTAD scenarios.\n\nIntroduction\n\nDue to the uncontrollable factors in the complex industrial manufacturing process, unforeseen defects will be brought to products inevitably. As the human visual system has the inherent ability to perceive anomalies [25], quality control relies on manual inspection for a long time.\n\nHowever, large-scale images and tiny defects are challenging for manual inspection, so increasing research is focused on automated machine vision inspection. Among all the methods, supervised deep learning has achieved great success. It relies on annotated datasets to learn discriminative features, effectively overcoming the hand-crafted shortcomings. However, because of insufficient negative samples, the high demand for labels, and the absence of prior knowledge, those approaches based on supervised learning may suffer in identifying unseen defects in practices, Recently, unsupervised methods have been applied to defect detection, as shown in Fig. 1(a,b). Reconstructionbased methods [4,15,23,29] are the most famous, which take reconstructed images as templates and then apply explicit contrast in image space to achieve high-resolution localization. However, reconstructing using decoders is an ill-posed inverse problem, it is hard to reconstruct complex details. To overcome the above limitations, anomalybased methods [6,7] utilizing texture-aware pre-trained models achieves high image-level performance, which also damages pixel-level visual performance. One of the most promising methods is convolutional normalizing flows [10,22,27], which models the probability distribution further from pre-trained features, earning higher performance.\n\nIn this paper, a Pyramid Normalizing Flow (Pyramid-Flow) is proposed. It develops the idea of templates from image space into latent space by normalizing flow, then performing contrast \u2206z d for high-resolution anomaly localization, as shown in Fig. 1(c). Specifically, we propose the multi-scale Pyramid Coupling Block, which includes invertible pyramid and volume normalization, as the critical module to construct volume-preserving PyramidFlow. To the best of our knowledge, PyramidFlow is the first UNet-like fully normalizing flow specifically designed for anomaly localization, analogous to UNet [19] for biomedical image segmentation. Our main contributions can be summarized as follows.\n\n\u2022 We propose a latent template-based defect contrastive localization paradigm. Similar to the reconstructionbased methods, we perform contrast localization in latent space, which avoids the ill-posedness and reduces intra-classes variance efficiently.\n\n\u2022 We propose PyramidFlow, which includes invertible pyramids and pyramid coupling blocks for multi-scale fusing and mapping, enabling high-resolution defect localization. Additionally, we propose volume normalization for improving generalization.\n\n\u2022 We conduct comprehensive experiments to demonstrate that our advanced method outperforms comparable algorithms that do not use external priors, and even achieves state-of-the-art performance in complex scenarios.\n\n\nRelated Work\n\n\nDeep learning-based Defect Localization\n\nWith the rise of deep learning, numerous works apply generalized computer vision methods for defect detection. Some works are based on object detection [13,14,28,30,31], which relies on annotated rectangular boxes, enabling locating and classifying defects end-to-end. The other is applying semantic segmentation [5,18,24], which enables pixellevel localization, suit for complex scenarios with difficultto-locate boundaries. However, these works still rely on supervised learning, they attempt to collect sufficient defective samples to learn well-defined representations.\n\nRecently, some promising work has considered the scarcity of defects in real-world scenarios, where defectfree samples are only obtained. These methods can be classified as reconstruction-based and anomaly-based. The reconstruction-based method relies on generative models such as VAE or GAN, which encode a defective image and reconstruct it to a defect-free image, then localize the defect with the contrast of these two images. The reconstruction-based method performs well on single textural images, but they cannot generalize to non-textural images for ill-posedness and degeneracy [25]. The anomalybased method treats defects as anomalous, applying neural networks to discriminate between normality and anomalous. These methods extract pre-trained features, then estimate their probability density using Mahalanobis distances or K-NearestNeighbor, while the lower probability indicates where the image patches are abnormal. Although anomaly-based methods had achieved great success in defect detection, it locates defects with low pixel-level resolution compared with reconstruction-based methods, usually 1/16th or even lower, which greatly limits practical industrial applications.\n\nTo overcome existing shortcomings, we propose a latent template-based defect contrastive localization paradigm, which breaks the limitation of low-frequency textureaware-only models, enabling more accurate results.\n\n\nNormalizing Flow\n\nNormalizing flow is a kind of invertible neural network with bijective mappings and traceable Jacobi determinants. It was first proposed for nonlinear independent component estimation [8] and applied to anomaly detection [21] recently for its invertibility helps prevent mode collapse. The normalizing flow comprises coupling blocks, these basic modules for realizing nonlinear mappings and calculating Jacobi determinants. Originally, NICE [8] proposed the additive coupling layer with unitary Jacobi determinants, while RealNVP [9] further proposed the affine coupling layer that enables the generation of non-volume-preserving mappings. However, redundant volume degrees of freedom can lead to increased optimization complexity, creating a domain gap between maximum likelihood estimation and anomaly metrics, which may potentially compromise the generalization performance in anomaly detection.\n\nPrevious works [10,21,27] on anomaly localization usually follow the methods proposed in RealNVP, but some challenges remain. Some studies [22] have found that convolutional normalizing flow focuses on local rather than semantic correlations, which are usually addressed by image embeddings [12]. Hence, earlier studies [21] adopted pretrained backbones, while recent trends used pre-trained encoders to extract image patches [10,22,27]. However, pretrained-based methods rely on task-irrelevant external priors, which limit generalization in unforeseen scenarios.\n\nTo address the above challenges, we propose a pyramidlike normalizing flow called PyramidFlow, which utilizes volume normalization to preserve volume mappings that include task-relevant implicit priors. Additionally, our method offers the option of using pre-trained models, and we have observed that external priors from pre-trained models can improve generalization performance. We will discuss these contributions in Sec \n\n\nMethodology\n\nOur algorithm consists of two processes, training and evaluation, as shown in Fig. 2. The training process is similar to siamese networks, the model is optimized by minimizing the Frequency differences F(\u2206z d ) within the im- age pair. For the evaluation process, latent templates are obtained through inference at the total training dataset, then latent contrast and pyramid composition are applied to obtain an anomaly localization map. The details are shown in the following sections.\n( ) I i ( ) I j (a) Training Minimize } { , 0,1 ( ) , 1 , k I k B \uf02d \uf03d \uf04c k Image Template 1 f \uf02d Forward f d z (b) Evaluation I d z \uf044 com \uf04c 1x1 Conv NF NF d x d z 2 2 ( ) ( ) d d d z i z z j \uf03d \uf02d \uf044 dec \uf04c dec \uf04c dec \uf04c dec \uf04c NF NF Anomaly Localization ) ( d z \uf044 \uf046\n\nInvertible Pyramid\n\nDefect images contain various frequency components. Usually, the low-frequency components represent the slow gradient background, while the high-frequency components correspond to details or defects. To decouple the frequency components and identify each frequency component independently, we propose invertible pyramids, which enable multi-scale decomposition and composition for a single feature. To facilitate feature learning, previous work applies pre-trained encoders to extract features. Although pretrained methods with external priors help performance improvement, to fully explore the advantages of our approach in our primary study, let's consider a baseline without any pre-trained model.\n\nFor a three-channel image I, apply orthogonally initialized 1 \u00d7 1 convolution W \u2208 R C\u00d73 to the image for obtain features x = WI. Given a feature x and a positive integer L, the pyramid decomposition is a mapping from features to feature sets L dec : x \u2192 {x d |d \u2208 Z L\u22121 }, where x d is the d\u2212level pyramid can be calculated as\nx d = D d (x) \u2212 U (D d+1 (x))(1)\nwhere D(\u00b7) and U (\u00b7) are arbitrary linear upsampling and downsampling operators, while D d (\u00b7) represents repeated downsampling d times. If Eq. (1) is further satisfied D 0 (x) = x, D L (x) = 0, then the inverse operation L com : {x d |d \u2208 Z L\u22121 } of the pyramid decomposition can be described as\nx = L\u22121 d=0 U d (x d )(2)\nDiffering from Gaussian pyramid, Eq. (2) indicates that there is always an inverse operation for pyramid decomposition, which is called pyramid composition. The method based on Eqs. (1) and (2), enabling perform multi-scale feature decomposition and composition, is a critical invertible module for PyramidFlow.\n\n\nPyramid Coupling Block\n\nInvertible Modules. Invertible modules are the essential elements to implementing invertible neural networks. The invertible modules introduced in this paper are invertible convolution, invertible pyramid, and affine coupling block. The affine coupling block is the basic module that constitutes the normalizing flow. It is based on feature splitting for invertible nonlinear mappings with easily traceable Jacobian determinants and inverse operations. As shown in Fig. 3(a), the conventional affine coupling block splits a single feature along the channel dimension, where one sub-feature keeps its identity while another is performed affine transformation controlled by it. Denote the splitted features are x 0 , x 1 and its outputs are y 0 , y 1 , then the corresponding transformation can be described as\ny 0 = x 0 y 1 = exp (s(x 0 )) x 1 + t(x 0 )(3)\nwhere s(\u00b7), t(\u00b7) are affine parameters, can be estimated by zero-initialized convolutional neural networks. For formula (3), there is an explicit inverse transformation:\nx 0 = y 0 x 1 = exp (\u2212s(y 0 )) (y 1 \u2212 t(y 0 ))(4)\nDenote the element at position i, j of s(\u00b7) as s i,j (\u00b7). As the Jacobian matrix of transformation(3) is a triangular matrix, its logarithmic determinant can be estimated as\nlog \u2202(y 0 , y 1 ) \u2202(x 0 , x 1 ) = i,j s i,j (x 0 )(5)\nEqs.\n\n(3) to (5) are the basis of all affine coupling blocks. However, the coupling block shown in Fig. 3(a) remains  identical for one part. Therefore the reverse cascade architecture is proposed in NICE [8] such that both parts are transformed, as shown in Fig. 3(b). The previous works construct the holistic invertible normalizing flow by iterative applying the structure shown in Fig. 3(b). Implementation. Our method decomposes a single feature along the scale and realizes multi-scale feature fusion based on Eqs. (3) to (5). In our implementation, the multi-scale affine parameters s(\u00b7), t(\u00b7) are estimated using a convolutional neural network with two linear layers, where bilinear interpolation is applied to match the target shape.\nx 0 x 1 x 0 y 1 y z 1\u00d71 Conv Compose Pyramid 0 z 1 z Decompose Pyramid Compose Pyramid (d) 2 x 1 x 0 x x z 1\u00d71 Conv 0 z 1 z 2 z 0 y 1 y 2 y x 0 x 1 x 0 y 1 y x 0 x 1 x 0 y 1 y 0 z 1 z dec \uf04c com \uf04c PyramidFlow I x 0 x 1 x 2 x 3 x 0 z 1 z 2 z 3 z z 1 D k \uf02d \uf0ce \uf0a2 1 L d \uf02d \uf0ce \uf0a2 (e)\nIn addition, we employ invertible 1x1 convolution [11] for feature fusion within features. Specifically, denoting the full rank matrix corresponding to the invertible 1 \u00d7 1 convolution as A, which can be decomposed by PLU as\nA = PL(U + diag(exp(s i )))(6)\nwhere P is a frozen permutation matrix, L is a lower triangular matrix with unit diagonal elements, U is an upper triangular matrix with zero diagonal elements, and exp(s i ) is the i-th eigenvalue of the matrix A, which always holds nonnegativity. The matrix A is always invertible during optimization, then its logarithmic Jacobian determinant can be estimated as\nlog |A| = i s i(7)\nIn summary, Eqs. (3) to (7) describe proposed pyramidal coupling block mathematically, as shown in Fig. 3(c). First, multi-scale feature fusion (3)(4)(5) is performed, and then apply linear fusion(6-7) for shuffle channels. Furthermore, we propose a dual coupling block as shown in Fig. 3(d), which is equivalent to the reverse parallel of the coupling block in Fig. 3(c). The dual coupling block is reparameterized in our implementation, and its affine parameters s(\u00b7), t(\u00b7) are estimated from concatenated features. Volume Normalization. Suppose that the invertible transformation f : x \u2192 z maps the variable x to the latent variable z. Previous works have assumed that the latent variable follows basic probability distribution (e.g. Gaussian distribution), then estimates sample probability density based on the following equation:\nP (x) = P (z) \u2202f (x) \u2202x(8)\nHowever, this approach relies on the basic distribution assumption and ignores the effect of the implicit prior in the probability density transform on generalization. When such approaches are applied to anomaly detection, the inconsistency between the training objectives and the anomaly evaluation results in domain gaps. Similar to batch normalization or instance normalization in deep learning, the proposed volume normalization will be employed for volume-preserving mappings, as illustrated in Fig. 4. Particularly, for the affine coupling block, the parameter s(\u00b7) is subtracted from its mean value before performing Eq. (3); for the invertible convolution, the parameter s i is subtracted from its mean value before calculating the matrix A based on Eq. (6). Depending on the statistical dimension, we propose Spatial Volume Normalization (SVN) and Channel Volume Normalization (CVN). SVN performs mean statistics along the spatial dimension, while CVN is along the channel dimension. Various volume normalization methods contain different priors, then we will explore their impact in Sec. 4.2.\n\n\nPyramid Normalizing Flow\n\nArchitecture. Our PyramidFlow can be obtained by stacking the pyramid coupled blocks of Fig. 3(c,d) along the depth D \u2212 1 times and along the layer L \u2212 1 times, as shown in Fig. 3(e). Specifically, PyramidFlow boosts the image I to feature x using matrix W, then performs the pyramid decomposition based on Eq. (1). The pyramid coupling blocks described in Eqs. (3) to (7) are calculated in the order described in Fig. 3(e) to obtain potential pyramid features z d , d = 0, 1, \u00b7 \u00b7 \u00b7 , L \u2212 1, which are finally composed into latent variables according to Eq. (2). Loss Function. In the cases with volume normalization, the loss function excludes the probability density coefficients. Moreover, the logarithmic Jacobian determinant of the semi-orthogonal matrix W is sample-independent, so its effect could be ignored during training.\n\nSuppose a training batch with 2 normal samples, its latent variables are z d (i), z d (j). Previous studies train neural networks using spatial difference\n\u2206z d = z d (i) \u2212 z d (j) 2 .\nHowever, it ignored the impact of high-frequency defects. To address the above shortcoming, we propose the following Fourier loss function.\nL loss = F(L com ({\u2206z d |d \u2208 Z L\u22121 }))(9)\nwhere F is the fast Fourier transform of the image. Training the normalizing flow using Eq. (9) enables the model to focus on the high-frequency, allowing faster convergence. We will discuss this trick in Sec. 4.3.\n\nDefect Localization. Previous studies [22,27] usually localize defects with obvious differences based on categoryindependent zero templates. In our method, the defects are modeled as anomalous deviations with respect to the template. Then, the anomaly of the latent pyramid z d is defined as \u03c3(z d ) = z d \u2212z d , wherez d is mean of the latent pyramid. Finally, the total anomaly can be estimated as\n\u03c3(z) = L com ({\u03c3(z d )|d \u2208 Z L\u22121 })(10)\nThe Eq. (10) shows that the total anomaly is a composition of anomalies at various scales, which is consistent with the empirical method proposed by Rudolph, et al. [22]. Image Template Estimation. The image template is a prototype of normal samples, a visualization of the latent template. Our fully normalizing flow is based on 1 \u00d7 1 convolution instead of pre-trained encoders, maintaining end-to-end and near-invertibility, thus the flow's input x temp can be retrieved using Eq. (2) and Eq. (4) from latent meanz d , then solve the least square problem WI temp = x temp for image template I temp .\n\n\nExperiment and Discussion\n\nIn this chapter, we perform unsupervised anomaly localization experiments on MVTec Anomaly Detection Dataset [2] (MVTecAD) and BeanTech Anomaly Detection Dataset [16] (BTAD). MVTecAD contains 15 categories of industrial defect images, five of which are textural images and the other ten are object images. The object images contain three classes (grid, metal nut, screw) without rough registration and one class (hazelnut) without fine registration, and we will discuss these cases in Sec. 4.5. The BTAD contains three types of real-world and industry-oriented textural images, which is more challenging for pixel-level localization. All experiments take Area Under the Receiver Operating characteristic Curve (AUROC) and Area Under the Per Region Overlap (AUPRO) as evaluation metrics. AUROC is the most widely used anomaly evaluation metric, and higher values indicate that various thresholds have less impact on performance. However, AUROC prefers larger anomalies and may fail in small proportions of anomalies. Thus, we further evaluate AUPRO for localization metric, similar to Intersection Over Union (IoU) commonly used in semantic segmentation. Detailed definitions can be found in [2].\n\n\nComplexity Analysis\n\nThe normalizing flow based on Eqs. (3) and (4) is computationally invertible, which indicates that only one copy of the variables is necessary for all stages. This feature decreases the memory footprint during backpropagation from linear to constant complexity. We have analyzed the above characteristics based on a fixed number of pyramid layers L = 8, image resolution with 256 \u00d7 256, and channels C = 24, then changed the number of stacked layers D to explore the trends of memory usage and model parameters. The forward and memory-saving backpropagation is implemented based on the self-developed PyTorch-based [17] framework autoFlow. All indicators are recorded during steady-state training, then plotted as bar and line graphs, as shown in Fig. 5.\n\nThe memory usage based on auto-differentiation increases linearly with depth D, while the implementation based on normalizing flow achieves approximate depthindependent memory usage. The memory superiority enables the proposed method to be trained in memoryconstrained devices below 4G without powerful hardware. The line graph shows the exponential trends between model parameters and depth, while the horizontal line represents the parameters of the usual pre-trained model. We mainly adopt methods with D < 8 or even shallower, where the number of parameters is far smaller than popular pretraining-based methods. In summary, in scenarios of memory constraint, the proposed PyramidFlow enables dealing with larger images and requires fewer parameters than others.\n\n\nStudy on Volume Normalization\n\nIn this subsection, based on MVTecAD, we investigate the impact of volume normalization on generalization. The experiment without data augmentation, fixed pyramid layers L = 4, channels C = 16, and linear interpolated image to 256 \u00d7 256. During the training, the volume normalization applies sample mean normalization and updates the running mean with 0.1 momenta, while in the testing, the volume normalization is based on the running mean. We sufficiently explored the volume normalization methods proposed in Sec. 3.2. Some representative categories are shown as Tab. 1.  The result in Tab. 1 shows the performance differences between the various volume normalization methods: CVN outperforms SVN for the first three classes, while the latter behaves the opposite. We further visualize these defect distributions in Fig. 6, which shows that SVN-superior classes are commonly textural images with a larger range of defects, while CVN-superior classes are object images.\n\nSVN with larger receptive fields achieves non-local localization by aggregating an extensive range of texture features, while CVN realizes accurate localization by shuffling channels. In a word, different volume normalization techniques implicitly embody distinct task-specific priors. Furthermore, our ablation study in Sec. 4 normalization does help to improve average performance.\n\n\n.3 shows that volume\n\n\nAblation Study\n\nThis subsection discusses the impact of some proposed methods on performance. The study is conducted on full MVTecAD, and other settings are the same as Sec. 4.2. We ablate four methods from baseline individually. In particular, experiment I is based on latent Gaussian assumption and Eq. (8) without volume normalization. For experiment II , the category-independent zero templatez d = 0 is applied. Then, Experiment III does not adopt the method of Eq. (10) but composes the pyramid first and localizes its difference later. Finally, experiment IV adopts a spatial version of the loss function instead of Eq. (9). The result of the above ablation experiments is shown in Tab. 2. Table 2. The ablation study on full MVTecAD. For each cell in the table, the first row is Pixel-AUROC% and the second is AUPRO% . The number within parentheses means the change relative to baseline, the larger absolute value with larger importance.\n\n\nMethod\n\nClasses Tab. 2 demonstrates that experiments I -IV present various performance degradation. Experiment I has the largest average degradation, with the object classes being more affected. Although the non-volume-preserving enables larger outputs and higher Image-AUROC performance, the implicit prior in volume normalization discussed in Sec. 4.2 is more helpful for generalization. For experiment II , it shows that the latent template benefits the performance, and object classes are improved greatly. It is because the categoryspecific latent template reduces intra-class variance, helping convergence during training. Then, experiment III suggests that multi-scale differences had a more pronounced impact on textural classes, as higher-level operators with larger receptive fields correspond to large defects. Finally, Experiment IV reveals that Fourier loss (9) is the icing on the cake that helps performance improvement. To summarize, methods I -III are critical for the proposed model, while tricks IV help further improvements.\n\n\nAnomaly Localization\n\nMVTecAD. We performed defect localization for 12 registered classes in MVTecAD. In our comparisons, the method based on pre-trained models or using external datasets is viewed as requiring external prior, corresponding to the first column of Tab. 3. In our implementation, we augment textural classes with flips and rotations, each with a probability of 0.5, while object categories do not undergo any augmentation operation. It is worth noting that those base on complex augmentation or weak supervision is not considered in our comparisons, as our approach is capable of incorporating these techniques to improve performance. The detailed results are shown in Tab. 3. First, we take three methods based on image contrast, AnoGAN [23], Vanilla VAE [15], and AE-SSIM [4]. They are not dependent on external datasets, so it is fair to com-pare them with our FNF model. Furthermore, we also compared our method to those that utilize external priors, such as S-T, SPADE, etc. All methods were reproduced based on the official implementation or AnomaLib [1]. For fair comparisons, we adapted the 1 \u00d7 1 convolution W to the pre-trained encoder, where the pre-trained encoder is the first two layers of ResNet18 for extracting the image into features of original 1/4 size and with 64 channels.\n\nAs shown in Tab. 3, our FNF method greatly outperforms the comparable methods without external priors, even exceeding S-T, SPADE, and CS-Flow that using external priors. Most of the reconstruction-based methods in Tab. 3 suffer from ill-posedness in complex scenarios (e.g., tile and wood.), while our method achieves the best AUPRO score owing to high-resolution contrast in latent space. However, a larger resolution implies larger intra-class variance, which degrades the overall AUROC performance for hardto-determine anomaly boundaries. Furthermore, Fig. 7(a) visualizes representative examples of MVTecAD anomaly localization, which shows that our method achieves precise localization with reasonable scale.\n\nBTAD. To fully illustrate our superiority, we experimented on the more challenging BTAD dataset without any data augmentation, and other settings are the same as MVTecAD. The detailed result in Tab. 4 shows that our method also achieves state-of-the-art performance, and Fig. 7(b) visualizes representative examples of BTAD anomaly localization.  \n\n\nStudy on unregistered categories\n\nIn principle, template-based methods require pixel-level registration between images and templates, which is typically satisfied in most real-world scenarios, but may fail in some cases. This subsection explores the performance of the proposed method on unregistered (e.g. rotation, shift) categories (e.g. grid, metal nut, screw, and hazelnut), as shown in Tab. 5. The reconstruction-based AE-SSIM heavily relies on pixel-level contrast, which decreases the average localization accuracy (AUPRO%). In contrast, the anomaly-based SPADE avoids registration issues and achieves better performance. Fortunately, our ResNet18based method, which utilizes normalizing flow to reduce patch variance, remains competitive in unregistered scenes, although it falls short of state-of-the-art performance. We visualize these categories in Fig. 7(c). \n\n\nConclusion\n\nIn this paper, we propose PyramidFlow, the first fully normalizing flow method based on the latent template-based contrastive paradigm, utilizing pyramid-like normalizing flows and volume normalization, enabling high-resolution defect contrastive localization. Our method can be trained end-to-end from scratch, similar to UNet, and our comprehensive experiments demonstrate that it outperforms comparable algorithms that do not use external priors, even achieving state-of-the-art performance in complex scenarios. While experiments on unregistered categories show that our method falls short of state-of-the-art, it still exhibits competitive performance. Future research will focus on improving performance in such scenarios. \n\n\nModel Architecture\n\nIn this subsection, we provide the detailed architecture of the proposed PyramidFlow, including invertible pyramids, pyramid coupling blocks, and volume normalization. Invertible Pyramid. The invertible pyramid is inspired by the Laplacian pyramid, which is commonly used in image processing. In invertible pyramids, the pyramid decomposition and composition are performed on the per-channel features. The linear downsampling operator D(\u00b7) first applies a Gaussian filter with kernel size 5 \u00d7 5, then downsamples using nearestneighbor interpolation. In contrast, upsampling U (\u00b7) performs nearest-neighbor interpolation before applying Gaussian filtering. Pyramid Coupling Block. For the example of dual coupling blocks, denoting the feature notations as shown in Fig. 3(d), the corresponding pseudocode is described in Algorithm 1. It is mainly composed of three custom functions -AffineParamBlock, VolumeNorm2d, and InvConv. Volume Normalization. The proposed volume normalization is similar to some normalization techniques such as Batch Normalization, but without normalizing the standard deviation. Taking Channel Volume Normalization (CVN) as an example, it can be described by the Algorithm 2.\n\n\nMore Experiment Results\n\n\nDetailed Ablation Results\n\nWe present the detailed ablation results of Sec 4.3, as shown in Tables S1 and S2. Textural Image. As shown in Table S1. For most textural categories, occurring performance degradation when the proposed methods are ablated. However, the results on the carpet show abnormal performance improvement. This means that inductive bias brings positive or negative effects on various categories. Object Image. As shown in Table S2. Due to the image patch in object categories with larger variances, the influence of volume normalization and the latent template is also larger. The performance of the object categories is less influenced by pyramid difference, indicating that multi-scale is not a critical factor for object defect detection.   \n\n\nMore Visualization Results\n\nIn this subsection, we present more visualization results of Sec 4.4. Since many categories, we separated results into two charts for visualization, as shown in Figs. S1 and S2. MVTecAD. As Figs. S1 and S2 shows, AE-SSIM performs better for simple categories, such as the bottle and zipper. However, it does not work in complex scenarios, e.g., it cannot localize carpet defects with fixed patterns or pill defects with high-frequency noises. It is worth noticing that AE-SSIM is a template-based method, which maintains the resolution during processing, enabling preserve the details in defect localization.\n\nSPADE and PaDiM are pre-trained-based methods. They achieve better results in almost all categories but still maintain some shortcomings. On the one hand, their localization results are blurry and larger than ground truths. On the other hand, they cannot localize tiny defects, such as cracks in the wood. Our proposed PyramidFlow is based on latent templates, which allows for preserving details effectively, with the ability to detect tiny defects and show their scale. In all categories in MVTecAD, our method achieves the best visual performance. BTAD. BTAD is more challenging than MVTecAD, as shown in the last three columns of Fig. S2. The AE-SSIM method almost failed in BTAD without beneficial results. For categories 01 and 02, the localization areas of SPADE and PaDiM are obviously larger than ground truths. For the most challenging category 03, their results are incredibly varied from GT.\n\nOur method provides more accurate results for BTAD defect localization. For the 01 categories, the localization results preserve the original details. Categories 02 and 03 also mostly reflect the essential shape of the defect.\n\nFigure 1 .\n1Illustration of various anomaly localization methods. (a) Reconstruction-based method. (b) Anomaly-based method, where NF denotes normalizing flow. (c) Our PyramidFlow, which combines latent templates and normalizing flow, enables highresolution localization.\n\nFigure 2 .\n2Schematic of training and evaluation for PyramidFlow. (a) Given any normality pair, minimize the distance of latent variables. (b) The means of the latent variables are contrasted to the examples, then apply pyramid composition to obtain an anomaly localization map.\n\nFigure 3 .\n3The proposed pyramid coupling block and PyramidFlow, where the solid line symbolizes the transformation while the dotted line refers to identity. (a) Channel-splitting affine coupling block. (b) The reverse cascade of (a)-architecture. (c) The proposed scale-wise pyramid coupling block. (d) The reverse parallel and reparameterized of (c)-architecture. (e) The proposed PyramidFlow, is a stacking of (c,d)-architecture both in depths and layers, where 1 \u00d7 1 convolution is neglected to represent.\n\nFigure 4 .\n4Illustration of volume normalization. (a) Batch Normalization. (b) The proposed Channel Volume Normalization (CVN). (c) The proposed Spatial Volume Normalization (SVN).\n\nFigure 5 .\n5Analysis of model complexity for various depths. The bars correspond to the Training Memory Usage (GB) in the left vertical coordinate, while the line graph and horizontal lines relate to the Model Parameters (M) on the other side. For each depth D, the left bar presents the normalizing flow implemented based on autoFlow framework with memory-saving tricks, while the right is implemented by PyTorch with auto-differentiation.\n\nFigure 6 .\n6Defects in Tab. 1 are visualized as heat maps. The top row displays CVN-superior class object images, while the bottom row displays SVN-superior class texture images.\n\nFigure 7 .\n7Visualization of our results on MVTecAD and BTAD. From top to bottom are original images, estimated image templates, our localization results, and ground truths. (a) The six challenging results on MVTecAD. (b) Two representative results on BTAD. (c) Results for the four unregistered categories on MVTecAD.\n\nFigure S1 .\nS1Visualization of competitive results on MVTecAD. From top to bottom are original images, AE-SSIM results, SPADE results, PaDiM results, our results, and ground truths. The red box indicates the localization is ambiguous and non-unique, while the green indicates successful results.\n\nFigure S2 .\nS2Visualization of competitive results on MVTecAD and BTAD. From top to bottom are original images, AE-SSIM results, SPADE results, PaDiM results, our results, and ground truths. The last three columns are the results of BTAD. The red box indicates the localization is ambiguous and non-unique, while the green indicates successful results.\n\nTable 1 .\n1Quantitative results of CVN and SVN on different categories. For each case in the table, the first column is Pixel-AUROC% and the second is AUPRO% , while the values within parentheses represent the relative improvement.\n\nTable 3 .\n3Quantitative results of various challenging methods on MVTecAD. In the table, the fully normalized flow method is labeled as FNF, while the abbreviations Res18, WRes50, EffiB5, and DTD are denoted as ResNet18, Wide-ResNet50-2, EfficientNet-B5, and Describable Textures Dataset, respectively. For each case in the table, the first row is Pixel-AUROC% and the second is AUPRO% , where the best results are marked in bold.External \nPrior \nMethods \ncarpet leather \ntile \nwood bottle cable capsule hazelnut \npill \ntoothbrush transistor zipper MEAN \n\n\u00d7 \n\nAnoGAN [23] \n54.2 \n64.1 \n49.7 \n62.1 \n85.8 \n78.0 \n84.1 \n87.1 \n86.8 \n90.0 \n79.9 \n78.1 \n75.0 \n20.4 \n37.8 \n17.7 \n38.6 \n62.0 \n38.3 \n30.6 \n69.8 \n77.6 \n74.9 \n54.9 \n46.7 \n47.4 \n\nVanilla VAE [15] \n62.0 \n83.5 \n52.0 \n69.9 \n89.4 \n81.6 \n90.7 \n95.1 \n87.9 \n95.3 \n85.1 \n77.5 \n80.8 \n61.9 \n64.9 \n24.2 \n57.8 \n70.5 \n77.9 \n77.9 \n77.0 \n79.3 \n85.4 \n61.0 \n60.8 \n66.6 \n\nAE-SSIM [4] \n87.0 \n78.0 \n59.0 \n73.0 \n93.0 \n82.0 \n94.0 \n97.0 \n91.0 \n92.0 \n80.0 \n88.0 \n84.5 \n64.7 \n56.1 \n17.5 \n60.5 \n83.4 \n47.8 \n86.0 \n91.6 \n83.0 \n78.4 \n72.4 \n66.5 \n67.3 \n\nOurs \n90.8 \n99.6 \n97.9 \n93.8 \n95.9 \n92.1 \n96.1 \n98.0 \n96.2 \n98.9 \n97.4 \n95.4 \n96.0 \n(FNF) \n91.0 \n99.7 \n95.8 \n96.2 \n94.0 \n86.4 \n93.1 \n97.3 \n96.3 \n97.7 \n91.4 \n95.1 \n94.5 \n\nRes18 \nS-T [3] \n93.5 \n97.8 \n92.5 \n92.1 \n97.8 \n91.9 \n96.8 \n98.2 \n96.5 \n97.9 \n73.7 \n95.6 \n93.7 \n87.9 \n94.5 \n94.6 \n91.1 \n93.1 \n81.8 \n96.8 \n96.5 \n96.1 \n93.3 \n66.6 \n95.1 \n90.6 \n\nWRes50 \nSPADE [6] \n97.5 \n97.6 \n87.4 \n88.5 \n98.4 \n97.2 \n99.0 \n99.1 \n96.5 \n97.9 \n94.1 \n96.5 \n95.8 \n94.7 \n97.2 \n75.9 \n87.4 \n95.5 \n90.9 \n93.7 \n95.4 \n94.6 \n93.5 \n97.4 \n92.6 \n92.4 \n\nWRes50 \nPaDiM [7] \n99.1 \n99.2 \n94.1 \n94.9 \n98.3 \n96.7 \n98.5 \n98.2 \n95.7 \n98.8 \n97.5 \n98.5 \n97.5 \n96.2 \n97.8 \n86.0 \n91.1 \n94.8 \n88.8 \n93.5 \n92.6 \n92.7 \n93.1 \n84.5 \n95.9 \n92.3 \n\nEffiB5 \nCS-Flow [22] \n98.0 \n98.4 \n93.9 \n88.6 \n90.9 \n95.3 \n97.9 \n96.3 \n95.7 \n96.3 \n95.5 \n96.4 \n95.3 \n98.0 \n98.5 \n94.5 \n92.9 \n88.7 \n94.0 \n96.1 \n95.1 \n91.1 \n89.9 \n96.9 \n95.4 \n94.2 \n\nDTD \nDRAEM [29] \n94.9 \n96.6 \n99.6 \n97.3 \n97.6 \n95.4 \n94.0 \n99.2 \n95.0 \n98.1 \n90.0 \n94.4 \n96.0 \n96.1 \n97.9 \n99.7 \n97.9 \n97.2 \n90.4 \n96.5 \n98.7 \n93.7 \n97.1 \n92.9 \n94.7 \n96.1 \n\nRes18 \nOurs \n97.4 \n98.7 \n97.1 \n97.0 \n97.8 \n91.8 \n98.6 \n98.1 \n96.1 \n98.5 \n96.9 \n96.6 \n97.1 \n97.2 \n99.2 \n97.2 \n97.9 \n95.5 \n90.3 \n98.3 \n98.1 \n96.1 \n97.9 \n94.7 \n95.4 \n96.5 \n\n\n\nTable 4 .\n4Quantitative results of various challenging meth-\nods on BTAD. For each case in the table, the first row is \nImage-AUROC% and the second is Pixel-AUROC% , where \nthe best results are marked in bold. \n\nMethods \nClasses \nMEAM \n01 \n02 \n03 \n\nVT-ADL [16] \n97.6 \n71.0 \n82.6 \n83.7 \n99.0 \n94.0 \n77.0 \n90.0 \n\nP-SVDD [26] \n95.7 \n72.1 \n82.1 \n83.3 \n91.6 \n93.6 \n91.0 \n92.1 \n\nSPADE [6] \n91.4 \n71.4 \n99.9 \n87.6 \n97.3 \n94.4 \n99.1 \n96.9 \n\nPatchCore [20] \n90.9 \n79.3 \n99.8 \n90.0 \n95.5 \n94.7 \n99.3 \n96.5 \n\nPaDiM [7] \n99.8 \n82.0 \n99.4 \n93.7 \n97.0 \n96.0 \n98.8 \n97.3 \n\nOurs (Res18) \n100.0 \n88.2 \n99.3 \n95.8 \n97.4 \n97.6 \n98.1 \n97.7 \n\n\n\nTable 5 .\n5Quantitative results on unregistered categories without Rough Registration (RR) or Fine Registration (FR). For each case in the table, the first row is Pixel-AUROC% and the second is AUPRO% , where the best results are marked in bold.Methods \n\nClasses \n\nMEAN \nw/o RR \nw/o FR \n\ngrid metal nut screw hazelnut \n\nAE-SSIM [4] \n94.0 \n89.0 \n96.0 \n97.0 \n94.0 \n84.9 \n60.3 \n88.7 \n91.6 \n81.4 \n\nSPADE [6] \n93.7 \n98.1 \n98.9 \n99.1 \n97.5 \n86.7 \n94.4 \n96.0 \n95.4 \n93.1 \n\nOurs (Res18) \n95.7 \n97.2 \n94.6 \n98.1 \n96.4 \n94.3 \n91.4 \n94.7 \n98.1 \n94.6 \n\n\n\n\nThe pre-trained encoder is the first two layers of ResNet18 for extracting the features from 1024 \u00d7 1024 image to 256 \u00d7 256 features with 64 channels.Supplementary Material for \nPyramidFlow: High-Resolution Defect Contrastive Localization using Pyramid \nNormalizing Flow \n\n1. Implementation Details \n\n1.1. Experimental Settings \n\nHardware. We implemented our models in Python3.8 and Pytorch1.10. Experiments are run on NVIDIA GTX3060 GPUs. \nBaseline method. We train our baseline model on 256 \u00d7 256 image. During all experiments, the training batch size is fixed \nto 2. Model parameters are updated using Adam optimizer with a constant learning rate of 2 \u00d7 10 \u22124 , epsilon of 1 \u00d7 10 \u22124 , \nweight decay of 1\u00d710 \u22125 , and beta parameters of (0.5, 0.9). In addition, we apply gradient clipping with a maximum gradient \nof 1.0 for training stability. \nPre-trained method. For the pre-trained version of PyramidFlow, we used ImageNet-pretrained ResNet18 from torchvision. \n\n\n\nAlgorithm 1 Dual Coupling Block. (Python-like Pseudocode) Algorithm 2 Volume Normalization. (Pytorch-like Pseudocode) Input: input x, momentum \u03b2 Output: output y def VolumeNorm2d(x, \u03b2 = 0.1): if training: x = mean(x, dim=1) % CVN: zero-mean normalization along channel dimensions y = x -x xrunning = (1 \u2212 \u03b2) \u00d7xrunning + \u03b2 \u00d7x % update running mean else:y = x -xrunning return yTable S1. The ablation study on textural images in MVTecAD. For each cell in the table, the first row is Pixel-AUROC% and the second is AUPRO% .Input: x0, x1, x2 \nOutput: z0, z1, z2 \nxcat0 = Interpolate(x0, x1.shape) \nxcat2 = Interpolate(x2, x1.shape) \nxcat = Concat(xcat0, xcat2) \ns1, t1 = AffineParamBlock(xcat) \ny1 = exp (s1) x1 + t1 \nz0, z1, z2 = x0, InvConv(y1), x2 \n\ndef AffineParamBlock(x, clamp=2): \nparams = CNN2d(x) % only two convolutional layers and one \nactivation layer \ns0, t = Chunk2d(params) \ns = VolumeNorm2d(clamp * 0.636 * atan(s0/clamp)) % as shown in \nAlgorithm 2. Where 0.636 is an approximation of 2/\u03c0. \nreturn s, t \n\ndef InvConv(y): \nsi = si -mean(si) \nkernel = PL(U + diag(exp (si)) \nz = Conv2d(y, kernel) \nreturn z \n\nMethod \nTexture \nMean \ncarpet grid leather \ntile \nwood \n\nOurs (baseline) \n90.8 \n94.2 \n99.6 \n97.9 \n93.8 \n95.2 \n91.0 \n92.7 \n99.7 \n95.8 \n96.2 \n95.1 \n\nI. w/o Volume \nNormalization \n\n93.5 \n88.5 \n99.5 \n74.4 \n91.3 \n89.4 \n93.7 \n88.1 \n95.5 \n65.7 \n94.2 \n87.5 \n\nII. w/o Latent \nTemplate \n\n91.8 \n86.8 \n99.4 \n94.8 \n93.0 \n93.1 \n91.3 \n88.0 \n97.7 \n89.9 \n92.7 \n91.9 \n\nIII. w/o Pyramid \nDifference \n\n75.9 \n78.0 \n99.3 \n96.0 \n89.7 \n87.8 \n76.1 \n76.1 \n99.4 \n94.4 \n93.0 \n87.8 \n\nIV. w/o Fourier \nLoss \n\n90.5 \n84.3 \n99.4 \n96.2 \n89.7 \n92.0 \n91.4 \n86.2 \n99.6 \n92.6 \n94.0 \n92.8 \n\n\n\nTable S2 .\nS2The ablation study on object images in MVTecAD. For each cell in the table, the first row is Pixel-AUROC% and the second is AUPRO% .Method \nObject \nMean \nbottle cable capsule hazelnut metalnut \npill \nscrew toothbrush transistor zipper \n\nOurs (baseline) \n95.9 \n92.1 \n96.1 \n98.0 \n92.8 \n96.2 \n94.0 \n98.9 \n97.4 \n95.4 \n95.7 \n94.0 \n86.4 \n93.1 \n97.3 \n89.5 \n96.3 \n94.1 \n97.9 \n91.4 \n95.1 \n93.5 \n\nI. w/o Volume \nNormalization \n\n76.5 \n84.7 \n82.9 \n97.9 \n87.9 \n94.8 \n94.1 \n56.4 \n82.2 \n95.0 \n85.2 \n77.8 \n75.1 \n81.3 \n95.4 \n81.5 \n81.5 \n94.0 \n74.2 \n82.7 \n92.6 \n83.6 \n\nII. w/o Latent \nTemplate \n\n83.2 \n87.8 \n90.0 \n97.9 \n87.6 \n94.6 \n93.0 \n84.7 \n94.8 \n93.7 \n90.7 \n82.4 \n76.6 \n87.3 \n83.9 \n74.2 \n89.3 \n92.7 \n90.7 \n77.4 \n92.8 \n84.7 \n\nIII. w/o Pyramid \nDifference \n\n92.8 \n91.4 \n96.0 \n97.5 \n86.4 \n95.3 \n92.7 \n98.0 \n95.4 \n85.2 \n93.1 \n83.4 \n84.1 \n94.0 \n97.6 \n81.2 \n95.4 \n93.1 \n97.1 \n90.7 \n77.2 \n89.4 \n\nIV. w/o Fourier \nLoss \n\n88.0 \n88.6 \n95.1 \n97.3 \n88.9 \n96.2 \n94.2 \n98.3 \n95.1 \n90.9 \n93.3 \n88.0 \n81.2 \n94.0 \n98.3 \n89.0 \n96.9 \n94.4 \n97.9 \n88.0 \n90.8 \n91.9 \n\ncarpet \nleather \ntile \nwood \nbottle \ncable \ncapsule hazelnut \npill \n\nImage \n\nOurs \n\nGT \n\nPaDiM \nSPADE AE-SSIM \n\n\nAcknowledgment. We would like to extend sincere appreciation to Jiabao Lei for his valuable guidance and insightful suggestions, which greatly contributed to the success of this work.\nAnomalib: A deep learning library for anomaly detection. S Akcay, D Ameln, A Vaidya, B Lakshmanan, N Ahuja, U Genc, 2022 IEEE International Conference on Image Processing (ICIP). S. Akcay, D. Ameln, A. Vaidya, B. Lakshmanan, N. Ahuja, and U. Genc. Anomalib: A deep learning library for anomaly detection. In 2022 IEEE International Conference on Image Processing (ICIP), pages 1706-1710.\n\nMvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionPaul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9592-9600.\n\nUninformed students: Student-teacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPaul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4183-4192.\n\nImproving unsupervised defect segmentation by applying structural similarity to autoencoders. Paul Bergmann, Sindy L\u00f6we, Michael Fauser, David Sattlegger, Carsten Steger, VISIGRAPP (5: VISAPP). Paul Bergmann, Sindy L\u00f6we, Michael Fauser, David Sattleg- ger, and Carsten Steger. Improving unsupervised defect seg- mentation by applying structural similarity to autoencoders. In VISIGRAPP (5: VISAPP), 2019.\n\nMixed supervision for surface-defect detection: From weakly to fully supervised learning. Jakob Bo\u017ei\u010d, Domen Tabernik, Danijel Sko\u010daj, Computers in Industry. 129103459Jakob Bo\u017ei\u010d, Domen Tabernik, and Danijel Sko\u010daj. Mixed supervision for surface-defect detection: From weakly to fully supervised learning. Computers in Industry, 129:103459, 2021.\n\nSub-image anomaly detection with deep pyramid correspondences. CoRR, abs. Niv Cohen, Yedid Hoshen, Niv Cohen and Yedid Hoshen. Sub-image anomaly detection with deep pyramid correspondences. CoRR, abs/2005.02357, 2020.\n\nPadim: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, International Conference on Pattern Recognition. SpringerThomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. Padim: a patch distribution modeling framework for anomaly detection and localization. In Inter- national Conference on Pattern Recognition, pages 475-489. Springer.\n\nNICE: non-linear independent components estimation. Laurent Dinh, David Krueger, Yoshua Bengio, 3rd International Conference on Learning Representations. San Diego, CA, USAWorkshop Track ProceedingsLaurent Dinh, David Krueger, and Yoshua Bengio. NICE: non-linear independent components estimation. In Yoshua Bengio and Yann LeCun, editors, 3rd International Confer- ence on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings, 2015.\n\nDensity estimation using real NVP. Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. OpenReview.netLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th Interna- tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Pro- ceedings. OpenReview.net, 2017.\n\nCflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. Denis Gudovskiy, Shun Ishizaka, Kazuki Kozuka, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionDenis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka. Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. In Proceed- ings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 98-107.\n\nGlow: Generative flow with invertible 1x1 convolutions. P Durk, Prafulla Kingma, Dhariwal, Advances in neural information processing systems. 31Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018.\n\nWhy normalizing flows fail to detect out-of-distribution data. Polina Kirichenko, Pavel Izmailov, Andrew G Wilson, Advances in neural information processing systems. 33Polina Kirichenko, Pavel Izmailov, and Andrew G Wil- son. Why normalizing flows fail to detect out-of-distribution data. Advances in neural information processing systems, 33:20578-20589, 2020.\n\nDefectnet: Toward fast and effective defect detection. Feng Li, Feng Li, Qinggang Xi, IEEE Transactions on Instrumentation and Measurement. 70Feng Li, Feng Li, and QingGang Xi. Defectnet: Toward fast and effective defect detection. IEEE Transactions on Instrumentation and Measurement, 70:1-9, 2021.\n\nFpcb surface defect detection: A decoupled two-stage object detection framework. J Luo, Z Yang, S Li, Y Wu, IEEE Transactions on Instrumentation and Measurement. 70J. Luo, Z. Yang, S. Li, and Y. Wu. Fpcb surface defect detection: A decoupled two-stage object detection frame- work. IEEE Transactions on Instrumentation and Measure- ment, 70:1-11, 2021.\n\nDeep generative model using unregularized score for anomaly detection with heterogeneous complexity. Takashi Matsubara, Kazuki Sato, Kenta Hama, Ryosuke Tachibana, Kuniaki Uehara, IEEE Transactions on Cybernetics. Takashi Matsubara, Kazuki Sato, Kenta Hama, Ryosuke Tachibana, and Kuniaki Uehara. Deep generative model us- ing unregularized score for anomaly detection with heteroge- neous complexity. IEEE Transactions on Cybernetics, 2020.\n\nVt-adl: A vision transformer network for image anomaly detection and localization. P Mishra, R Verk, D Fornasier, C Piciarelli, G L Foresti, 2021 IEEE 30th International Symposium on Industrial Electronics (ISIE). P. Mishra, R. Verk, D. Fornasier, C. Piciarelli, and G. L. Foresti. Vt-adl: A vision transformer network for image anomaly detection and localization. In 2021 IEEE 30th Inter- national Symposium on Industrial Electronics (ISIE), pages 01-06.\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.\n\nA generic deep-learningbased approach for automated surface inspection. R Ren, T Hung, K C Tan, IEEE Transactions on Cybernetics. 483R. Ren, T. Hung, and K. C. Tan. A generic deep-learning- based approach for automated surface inspection. IEEE Transactions on Cybernetics, 48(3):929-940, 2018.\n\nUnet: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference. Munich, GermanySpringerProceedings, Part III 18Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234-241. Springer, 2015.\n\nTowards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKarsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, and Peter Gehler. Towards to- tal recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14318-14328.\n\nSame same but differnet: Semi-supervised defect detection with normalizing flows. Marco Rudolph, Bastian Wandt, Bodo Rosenhahn, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer visionMarco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same same but differnet: Semi-supervised defect detection with normalizing flows. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 1907- 1916.\n\nFully convolutional cross-scale-flows for imagebased defect detection. Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, Bastian Wandt, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionMarco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bas- tian Wandt. Fully convolutional cross-scale-flows for image- based defect detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1088-1097.\n\nUnsupervised anomaly detection with generative adversarial networks to guide marker discovery. Thomas Schlegl, Philipp Seeb\u00f6ck, Ursula Sebastian M Waldstein, Georg Schmidt-Erfurth, Langs, International conference on information processing in medical imaging. SpringerThomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on in- formation processing in medical imaging, pages 146-157. Springer.\n\nSegmentation-based deep-learning approach for surface-defect detection. Domen Tabernik, Jure Samo\u0161ela, Danijel Skvar\u010d, Sko\u010daj, Journal of Intelligent Manufacturing. 313Domen Tabernik, Samo\u0160ela, Jure Skvar\u010d, and Danijel Sko\u010daj. Segmentation-based deep-learning approach for surface-defect detection. Journal of Intelligent Manufactur- ing, 31(3):759-776, 2020.\n\nDeep learning for unsupervised anomaly localization in industrial images: A survey. Xian Tao, Xinyi Gong, Xin Zhang, Shaohua Yan, Chandranath Adak, IEEE Transactions on Instrumentation and Measurement. Xian Tao, Xinyi Gong, Xin Zhang, Shaohua Yan, and Chan- dranath Adak. Deep learning for unsupervised anomaly lo- calization in industrial images: A survey. IEEE Transactions on Instrumentation and Measurement, 2022.\n\nPatch svdd: Patch-level svdd for anomaly detection and segmentation. Jihun Yi, Sungroh Yoon, Proceedings of the Asian Conference on Computer Vision. the Asian Conference on Computer VisionJihun Yi and Sungroh Yoon. Patch svdd: Patch-level svdd for anomaly detection and segmentation. In Proceedings of the Asian Conference on Computer Vision.\n\nFastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, Liwei Wu, abs/2111.07677CoRRJiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, and Liwei Wu. Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. CoRR, abs/2111.07677, 2021.\n\nEs-net: Efficient scale-aware network for tiny defect detection. Xuyi Yu, Wentao Lyu, Di Zhou, Chengqun Wang, Weiqiang Xu, IEEE Transactions on Instrumentation and Measurement. 71Xuyi Yu, Wentao Lyu, Di Zhou, Chengqun Wang, and Weiqiang Xu. Es-net: Efficient scale-aware network for tiny defect detection. IEEE Transactions on Instrumentation and Measurement, 71:1-14, 2022.\n\nDraema discriminatively trained reconstruction embedding for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Sko\u010daj, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionVitjan Zavrtanik, Matej Kristan, and Danijel Sko\u010daj. Draem- a discriminatively trained reconstruction embedding for sur- face anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8330- 8339, 2021.\n\nA smallsized object detection oriented multi-scale feature fusion approach with application to defect detection. N Zeng, P Wu, Z Wang, H Li, W Liu, X Liu, IEEE Transactions on Instrumentation and Measurement. 71N. Zeng, P. Wu, Z. Wang, H. Li, W. Liu, and X. Liu. A small- sized object detection oriented multi-scale feature fusion ap- proach with application to defect detection. IEEE Transac- tions on Instrumentation and Measurement, 71:1-14, 2022.\n\nCadn: A weakly supervised learningbased category-aware object detection network for surface defect detection. Jiabin Zhang, Hu Su, Wei Zou, Xinyi Gong, Zhengtao Zhang, Fei Shen, Pattern Recognition. 109107571Jiabin Zhang, Hu Su, Wei Zou, Xinyi Gong, Zhengtao Zhang, and Fei Shen. Cadn: A weakly supervised learning- based category-aware object detection network for surface defect detection. Pattern Recognition, 109:107571, 2021.\n", "annotations": {"author": "[{\"end\":257,\"start\":95},{\"end\":439,\"start\":258},{\"end\":600,\"start\":440},{\"end\":951,\"start\":601}]", "publisher": null, "author_last_name": "[{\"end\":105,\"start\":102},{\"end\":267,\"start\":265},{\"end\":448,\"start\":444},{\"end\":609,\"start\":606}]", "author_first_name": "[{\"end\":101,\"start\":95},{\"end\":264,\"start\":258},{\"end\":443,\"start\":440},{\"end\":605,\"start\":601}]", "author_affiliation": "[{\"end\":182,\"start\":107},{\"end\":256,\"start\":184},{\"end\":364,\"start\":289},{\"end\":438,\"start\":366},{\"end\":525,\"start\":450},{\"end\":599,\"start\":527},{\"end\":708,\"start\":633},{\"end\":782,\"start\":710},{\"end\":854,\"start\":784},{\"end\":950,\"start\":856}]", "title": "[{\"end\":92,\"start\":1},{\"end\":1043,\"start\":952}]", "venue": null, "abstract": "[{\"end\":1998,\"start\":1045}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2234,\"start\":2230},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2994,\"start\":2991},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2997,\"start\":2994},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3000,\"start\":2997},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3003,\"start\":3000},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3333,\"start\":3330},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3335,\"start\":3333},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3542,\"start\":3538},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3545,\"start\":3542},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3548,\"start\":3545},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4261,\"start\":4257},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5281,\"start\":5277},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5284,\"start\":5281},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5287,\"start\":5284},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5290,\"start\":5287},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5293,\"start\":5290},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5441,\"start\":5438},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5444,\"start\":5441},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5447,\"start\":5444},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6291,\"start\":6287},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7313,\"start\":7310},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7351,\"start\":7347},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7570,\"start\":7567},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7659,\"start\":7656},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8045,\"start\":8041},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8048,\"start\":8045},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8051,\"start\":8048},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8169,\"start\":8165},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8321,\"start\":8317},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8350,\"start\":8346},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8456,\"start\":8452},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8459,\"start\":8456},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8462,\"start\":8459},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12842,\"start\":12839},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13034,\"start\":13031},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13357,\"start\":13354},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13897,\"start\":13893},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14631,\"start\":14628},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14634,\"start\":14631},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14637,\"start\":14634},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16112,\"start\":16109},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16850,\"start\":16847},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17936,\"start\":17932},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17939,\"start\":17936},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18346,\"start\":18342},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18503,\"start\":18499},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19078,\"start\":19075},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19132,\"start\":19128},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20160,\"start\":20157},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20804,\"start\":20800},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23041,\"start\":23040},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23753,\"start\":23750},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25875,\"start\":25871},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25893,\"start\":25889},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25910,\"start\":25907},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26193,\"start\":26190}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33170,\"start\":32898},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33450,\"start\":33171},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33961,\"start\":33451},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34143,\"start\":33962},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34585,\"start\":34144},{\"attributes\":{\"id\":\"fig_8\"},\"end\":34765,\"start\":34586},{\"attributes\":{\"id\":\"fig_10\"},\"end\":35085,\"start\":34766},{\"attributes\":{\"id\":\"fig_11\"},\"end\":35382,\"start\":35086},{\"attributes\":{\"id\":\"fig_12\"},\"end\":35736,\"start\":35383},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35969,\"start\":35737},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38264,\"start\":35970},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38888,\"start\":38265},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39431,\"start\":38889},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":40401,\"start\":39432},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":42076,\"start\":40402},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43235,\"start\":42077}]", "paragraph": "[{\"end\":2296,\"start\":2014},{\"end\":3654,\"start\":2298},{\"end\":4349,\"start\":3656},{\"end\":4602,\"start\":4351},{\"end\":4850,\"start\":4604},{\"end\":5066,\"start\":4852},{\"end\":5698,\"start\":5125},{\"end\":6889,\"start\":5700},{\"end\":7105,\"start\":6891},{\"end\":8024,\"start\":7126},{\"end\":8590,\"start\":8026},{\"end\":9016,\"start\":8592},{\"end\":9519,\"start\":9032},{\"end\":10499,\"start\":9799},{\"end\":10827,\"start\":10501},{\"end\":11157,\"start\":10861},{\"end\":11495,\"start\":11184},{\"end\":12330,\"start\":11522},{\"end\":12547,\"start\":12378},{\"end\":12771,\"start\":12598},{\"end\":12830,\"start\":12826},{\"end\":13568,\"start\":12832},{\"end\":14067,\"start\":13843},{\"end\":14464,\"start\":14099},{\"end\":15319,\"start\":14484},{\"end\":16449,\"start\":15347},{\"end\":17310,\"start\":16478},{\"end\":17466,\"start\":17312},{\"end\":17635,\"start\":17496},{\"end\":17892,\"start\":17678},{\"end\":18293,\"start\":17894},{\"end\":18936,\"start\":18334},{\"end\":20161,\"start\":18966},{\"end\":20939,\"start\":20185},{\"end\":21707,\"start\":20941},{\"end\":22712,\"start\":21741},{\"end\":23097,\"start\":22714},{\"end\":24068,\"start\":23139},{\"end\":25115,\"start\":24079},{\"end\":26427,\"start\":25140},{\"end\":27142,\"start\":26429},{\"end\":27491,\"start\":27144},{\"end\":28366,\"start\":27528},{\"end\":29110,\"start\":28381},{\"end\":30333,\"start\":29133},{\"end\":31125,\"start\":30389},{\"end\":31764,\"start\":31156},{\"end\":32669,\"start\":31766},{\"end\":32897,\"start\":32671}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9777,\"start\":9520},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10860,\"start\":10828},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11183,\"start\":11158},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12377,\"start\":12331},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12597,\"start\":12548},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12825,\"start\":12772},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13842,\"start\":13569},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14098,\"start\":14068},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14483,\"start\":14465},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15346,\"start\":15320},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17495,\"start\":17467},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17677,\"start\":17636},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18333,\"start\":18294}]", "table_ref": "[{\"end\":23827,\"start\":23820},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30508,\"start\":30500},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30811,\"start\":30803}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2012,\"start\":2000},{\"attributes\":{\"n\":\"2.\"},\"end\":5081,\"start\":5069},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5123,\"start\":5084},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7124,\"start\":7108},{\"attributes\":{\"n\":\"3.\"},\"end\":9030,\"start\":9019},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9797,\"start\":9779},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11520,\"start\":11498},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16476,\"start\":16452},{\"attributes\":{\"n\":\"4.\"},\"end\":18964,\"start\":18939},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20183,\"start\":20164},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21739,\"start\":21710},{\"end\":23120,\"start\":23100},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23137,\"start\":23123},{\"end\":24077,\"start\":24071},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25138,\"start\":25118},{\"attributes\":{\"n\":\"4.5.\"},\"end\":27526,\"start\":27494},{\"attributes\":{\"n\":\"5.\"},\"end\":28379,\"start\":28369},{\"attributes\":{\"n\":\"1.2.\"},\"end\":29131,\"start\":29113},{\"attributes\":{\"n\":\"2.\"},\"end\":30359,\"start\":30336},{\"attributes\":{\"n\":\"2.1.\"},\"end\":30387,\"start\":30362},{\"attributes\":{\"n\":\"2.2.\"},\"end\":31154,\"start\":31128},{\"end\":32909,\"start\":32899},{\"end\":33182,\"start\":33172},{\"end\":33462,\"start\":33452},{\"end\":33973,\"start\":33963},{\"end\":34155,\"start\":34145},{\"end\":34597,\"start\":34587},{\"end\":34777,\"start\":34767},{\"end\":35098,\"start\":35087},{\"end\":35395,\"start\":35384},{\"end\":35747,\"start\":35738},{\"end\":35980,\"start\":35971},{\"end\":38275,\"start\":38266},{\"end\":38899,\"start\":38890},{\"end\":42088,\"start\":42078}]", "table": "[{\"end\":38264,\"start\":36401},{\"end\":38888,\"start\":38277},{\"end\":39431,\"start\":39135},{\"end\":40401,\"start\":39584},{\"end\":42076,\"start\":40924},{\"end\":43235,\"start\":42223}]", "figure_caption": "[{\"end\":33170,\"start\":32911},{\"end\":33450,\"start\":33184},{\"end\":33961,\"start\":33464},{\"end\":34143,\"start\":33975},{\"end\":34585,\"start\":34157},{\"end\":34765,\"start\":34599},{\"end\":35085,\"start\":34779},{\"end\":35382,\"start\":35101},{\"end\":35736,\"start\":35398},{\"end\":35969,\"start\":35749},{\"end\":36401,\"start\":35982},{\"end\":39135,\"start\":38901},{\"end\":39584,\"start\":39434},{\"end\":40924,\"start\":40404},{\"end\":42223,\"start\":42091}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2961,\"start\":2950},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3909,\"start\":3900},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9116,\"start\":9110},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11993,\"start\":11987},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12931,\"start\":12925},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13091,\"start\":13085},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13217,\"start\":13211},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14592,\"start\":14583},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14775,\"start\":14766},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14855,\"start\":14846},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":15853,\"start\":15847},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16577,\"start\":16566},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16660,\"start\":16651},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16898,\"start\":16892},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20938,\"start\":20932},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":22566,\"start\":22560},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":26993,\"start\":26984},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":27424,\"start\":27415},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":28361,\"start\":28355},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29906,\"start\":29897},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32407,\"start\":32400}]", "bib_author_first_name": "[{\"end\":43478,\"start\":43477},{\"end\":43487,\"start\":43486},{\"end\":43496,\"start\":43495},{\"end\":43506,\"start\":43505},{\"end\":43520,\"start\":43519},{\"end\":43529,\"start\":43528},{\"end\":43893,\"start\":43889},{\"end\":43911,\"start\":43904},{\"end\":43925,\"start\":43920},{\"end\":43945,\"start\":43938},{\"end\":44454,\"start\":44450},{\"end\":44472,\"start\":44465},{\"end\":44486,\"start\":44481},{\"end\":44506,\"start\":44499},{\"end\":45031,\"start\":45027},{\"end\":45047,\"start\":45042},{\"end\":45061,\"start\":45054},{\"end\":45075,\"start\":45070},{\"end\":45095,\"start\":45088},{\"end\":45434,\"start\":45429},{\"end\":45447,\"start\":45442},{\"end\":45465,\"start\":45458},{\"end\":45764,\"start\":45761},{\"end\":45777,\"start\":45772},{\"end\":45999,\"start\":45993},{\"end\":46017,\"start\":46008},{\"end\":46035,\"start\":46026},{\"end\":46051,\"start\":46044},{\"end\":46418,\"start\":46411},{\"end\":46430,\"start\":46425},{\"end\":46446,\"start\":46440},{\"end\":46882,\"start\":46875},{\"end\":46895,\"start\":46889},{\"end\":46916,\"start\":46912},{\"end\":47403,\"start\":47398},{\"end\":47419,\"start\":47415},{\"end\":47436,\"start\":47430},{\"end\":47906,\"start\":47905},{\"end\":47921,\"start\":47913},{\"end\":48217,\"start\":48211},{\"end\":48235,\"start\":48230},{\"end\":48254,\"start\":48246},{\"end\":48570,\"start\":48566},{\"end\":48579,\"start\":48575},{\"end\":48592,\"start\":48584},{\"end\":48894,\"start\":48893},{\"end\":48901,\"start\":48900},{\"end\":48909,\"start\":48908},{\"end\":48915,\"start\":48914},{\"end\":49274,\"start\":49267},{\"end\":49292,\"start\":49286},{\"end\":49304,\"start\":49299},{\"end\":49318,\"start\":49311},{\"end\":49337,\"start\":49330},{\"end\":49693,\"start\":49692},{\"end\":49703,\"start\":49702},{\"end\":49711,\"start\":49710},{\"end\":49724,\"start\":49723},{\"end\":49738,\"start\":49737},{\"end\":49740,\"start\":49739},{\"end\":50108,\"start\":50104},{\"end\":50120,\"start\":50117},{\"end\":50135,\"start\":50128},{\"end\":50153,\"start\":50146},{\"end\":50168,\"start\":50162},{\"end\":50182,\"start\":50175},{\"end\":50197,\"start\":50191},{\"end\":50208,\"start\":50203},{\"end\":50224,\"start\":50220},{\"end\":50237,\"start\":50233},{\"end\":50510,\"start\":50509},{\"end\":50517,\"start\":50516},{\"end\":50525,\"start\":50524},{\"end\":50527,\"start\":50526},{\"end\":50800,\"start\":50796},{\"end\":50821,\"start\":50814},{\"end\":50837,\"start\":50831},{\"end\":51375,\"start\":51368},{\"end\":51387,\"start\":51382},{\"end\":51403,\"start\":51396},{\"end\":51420,\"start\":51412},{\"end\":51438,\"start\":51432},{\"end\":51450,\"start\":51445},{\"end\":51952,\"start\":51947},{\"end\":51969,\"start\":51962},{\"end\":51981,\"start\":51977},{\"end\":52452,\"start\":52447},{\"end\":52465,\"start\":52462},{\"end\":52480,\"start\":52476},{\"end\":52499,\"start\":52492},{\"end\":52997,\"start\":52991},{\"end\":53014,\"start\":53007},{\"end\":53030,\"start\":53024},{\"end\":53059,\"start\":53054},{\"end\":53534,\"start\":53529},{\"end\":53549,\"start\":53545},{\"end\":53567,\"start\":53560},{\"end\":53906,\"start\":53902},{\"end\":53917,\"start\":53912},{\"end\":53927,\"start\":53924},{\"end\":53942,\"start\":53935},{\"end\":53959,\"start\":53948},{\"end\":54311,\"start\":54306},{\"end\":54323,\"start\":54316},{\"end\":54671,\"start\":54665},{\"end\":54678,\"start\":54676},{\"end\":54691,\"start\":54686},{\"end\":54701,\"start\":54698},{\"end\":54714,\"start\":54706},{\"end\":54722,\"start\":54719},{\"end\":54734,\"start\":54729},{\"end\":55017,\"start\":55013},{\"end\":55028,\"start\":55022},{\"end\":55036,\"start\":55034},{\"end\":55051,\"start\":55043},{\"end\":55066,\"start\":55058},{\"end\":55418,\"start\":55412},{\"end\":55435,\"start\":55430},{\"end\":55452,\"start\":55445},{\"end\":55950,\"start\":55949},{\"end\":55958,\"start\":55957},{\"end\":55964,\"start\":55963},{\"end\":55972,\"start\":55971},{\"end\":55978,\"start\":55977},{\"end\":55985,\"start\":55984},{\"end\":56404,\"start\":56398},{\"end\":56414,\"start\":56412},{\"end\":56422,\"start\":56419},{\"end\":56433,\"start\":56428},{\"end\":56448,\"start\":56440},{\"end\":56459,\"start\":56456}]", "bib_author_last_name": "[{\"end\":43484,\"start\":43479},{\"end\":43493,\"start\":43488},{\"end\":43503,\"start\":43497},{\"end\":43517,\"start\":43507},{\"end\":43526,\"start\":43521},{\"end\":43534,\"start\":43530},{\"end\":43902,\"start\":43894},{\"end\":43918,\"start\":43912},{\"end\":43936,\"start\":43926},{\"end\":43952,\"start\":43946},{\"end\":44463,\"start\":44455},{\"end\":44479,\"start\":44473},{\"end\":44497,\"start\":44487},{\"end\":44513,\"start\":44507},{\"end\":45040,\"start\":45032},{\"end\":45052,\"start\":45048},{\"end\":45068,\"start\":45062},{\"end\":45086,\"start\":45076},{\"end\":45102,\"start\":45096},{\"end\":45440,\"start\":45435},{\"end\":45456,\"start\":45448},{\"end\":45472,\"start\":45466},{\"end\":45770,\"start\":45765},{\"end\":45784,\"start\":45778},{\"end\":46006,\"start\":46000},{\"end\":46024,\"start\":46018},{\"end\":46042,\"start\":46036},{\"end\":46060,\"start\":46052},{\"end\":46423,\"start\":46419},{\"end\":46438,\"start\":46431},{\"end\":46453,\"start\":46447},{\"end\":46887,\"start\":46883},{\"end\":46910,\"start\":46896},{\"end\":46923,\"start\":46917},{\"end\":47413,\"start\":47404},{\"end\":47428,\"start\":47420},{\"end\":47443,\"start\":47437},{\"end\":47911,\"start\":47907},{\"end\":47928,\"start\":47922},{\"end\":47938,\"start\":47930},{\"end\":48228,\"start\":48218},{\"end\":48244,\"start\":48236},{\"end\":48261,\"start\":48255},{\"end\":48573,\"start\":48571},{\"end\":48582,\"start\":48580},{\"end\":48595,\"start\":48593},{\"end\":48898,\"start\":48895},{\"end\":48906,\"start\":48902},{\"end\":48912,\"start\":48910},{\"end\":48918,\"start\":48916},{\"end\":49284,\"start\":49275},{\"end\":49297,\"start\":49293},{\"end\":49309,\"start\":49305},{\"end\":49328,\"start\":49319},{\"end\":49344,\"start\":49338},{\"end\":49700,\"start\":49694},{\"end\":49708,\"start\":49704},{\"end\":49721,\"start\":49712},{\"end\":49735,\"start\":49725},{\"end\":49748,\"start\":49741},{\"end\":50115,\"start\":50109},{\"end\":50126,\"start\":50121},{\"end\":50144,\"start\":50136},{\"end\":50160,\"start\":50154},{\"end\":50173,\"start\":50169},{\"end\":50189,\"start\":50183},{\"end\":50201,\"start\":50198},{\"end\":50218,\"start\":50209},{\"end\":50231,\"start\":50225},{\"end\":50243,\"start\":50238},{\"end\":50514,\"start\":50511},{\"end\":50522,\"start\":50518},{\"end\":50531,\"start\":50528},{\"end\":50812,\"start\":50801},{\"end\":50829,\"start\":50822},{\"end\":50842,\"start\":50838},{\"end\":51380,\"start\":51376},{\"end\":51394,\"start\":51388},{\"end\":51410,\"start\":51404},{\"end\":51430,\"start\":51421},{\"end\":51443,\"start\":51439},{\"end\":51457,\"start\":51451},{\"end\":51960,\"start\":51953},{\"end\":51975,\"start\":51970},{\"end\":51991,\"start\":51982},{\"end\":52460,\"start\":52453},{\"end\":52474,\"start\":52466},{\"end\":52490,\"start\":52481},{\"end\":52505,\"start\":52500},{\"end\":53005,\"start\":52998},{\"end\":53022,\"start\":53015},{\"end\":53052,\"start\":53031},{\"end\":53075,\"start\":53060},{\"end\":53082,\"start\":53077},{\"end\":53543,\"start\":53535},{\"end\":53558,\"start\":53550},{\"end\":53574,\"start\":53568},{\"end\":53582,\"start\":53576},{\"end\":53910,\"start\":53907},{\"end\":53922,\"start\":53918},{\"end\":53933,\"start\":53928},{\"end\":53946,\"start\":53943},{\"end\":53964,\"start\":53960},{\"end\":54314,\"start\":54312},{\"end\":54328,\"start\":54324},{\"end\":54674,\"start\":54672},{\"end\":54684,\"start\":54679},{\"end\":54696,\"start\":54692},{\"end\":54704,\"start\":54702},{\"end\":54717,\"start\":54715},{\"end\":54727,\"start\":54723},{\"end\":54737,\"start\":54735},{\"end\":55020,\"start\":55018},{\"end\":55032,\"start\":55029},{\"end\":55041,\"start\":55037},{\"end\":55056,\"start\":55052},{\"end\":55069,\"start\":55067},{\"end\":55428,\"start\":55419},{\"end\":55443,\"start\":55436},{\"end\":55459,\"start\":55453},{\"end\":55955,\"start\":55951},{\"end\":55961,\"start\":55959},{\"end\":55969,\"start\":55965},{\"end\":55975,\"start\":55973},{\"end\":55982,\"start\":55979},{\"end\":55989,\"start\":55986},{\"end\":56410,\"start\":56405},{\"end\":56417,\"start\":56415},{\"end\":56426,\"start\":56423},{\"end\":56438,\"start\":56434},{\"end\":56454,\"start\":56449},{\"end\":56464,\"start\":56460}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":246904715},\"end\":43807,\"start\":43420},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":189857704},\"end\":44354,\"start\":43809},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":207880670},\"end\":44931,\"start\":44356},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49567058},\"end\":45337,\"start\":44933},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":233219867},\"end\":45685,\"start\":45339},{\"attributes\":{\"id\":\"b5\"},\"end\":45904,\"start\":45687},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":226976039},\"end\":46357,\"start\":45906},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13995862},\"end\":46838,\"start\":46359},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8768364},\"end\":47292,\"start\":46840},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":236447794},\"end\":47847,\"start\":47294},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49657329},\"end\":48146,\"start\":47849},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219687356},\"end\":48509,\"start\":48148},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":233178517},\"end\":48810,\"start\":48511},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":236151246},\"end\":49164,\"start\":48812},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52154997},\"end\":49607,\"start\":49166},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":233307063},\"end\":50064,\"start\":49609},{\"attributes\":{\"id\":\"b16\"},\"end\":50435,\"start\":50066},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4376168},\"end\":50730,\"start\":50437},{\"attributes\":{\"id\":\"b18\"},\"end\":51312,\"start\":50732},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":235436036},\"end\":51863,\"start\":51314},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":221370646},\"end\":52374,\"start\":51865},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":238408276},\"end\":52894,\"start\":52376},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":17427022},\"end\":53455,\"start\":52896},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":207172033},\"end\":53816,\"start\":53457},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":250916882},\"end\":54235,\"start\":53818},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":220250825},\"end\":54579,\"start\":54237},{\"attributes\":{\"doi\":\"abs/2111.07677\",\"id\":\"b26\"},\"end\":54946,\"start\":54581},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":248313143},\"end\":55322,\"start\":54948},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":237142564},\"end\":55834,\"start\":55324},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":247116896},\"end\":56286,\"start\":55836},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":224805840},\"end\":56718,\"start\":56288}]", "bib_title": "[{\"end\":43475,\"start\":43420},{\"end\":43887,\"start\":43809},{\"end\":44448,\"start\":44356},{\"end\":45025,\"start\":44933},{\"end\":45427,\"start\":45339},{\"end\":45991,\"start\":45906},{\"end\":46409,\"start\":46359},{\"end\":46873,\"start\":46840},{\"end\":47396,\"start\":47294},{\"end\":47903,\"start\":47849},{\"end\":48209,\"start\":48148},{\"end\":48564,\"start\":48511},{\"end\":48891,\"start\":48812},{\"end\":49265,\"start\":49166},{\"end\":49690,\"start\":49609},{\"end\":50507,\"start\":50437},{\"end\":50794,\"start\":50732},{\"end\":51366,\"start\":51314},{\"end\":51945,\"start\":51865},{\"end\":52445,\"start\":52376},{\"end\":52989,\"start\":52896},{\"end\":53527,\"start\":53457},{\"end\":53900,\"start\":53818},{\"end\":54304,\"start\":54237},{\"end\":55011,\"start\":54948},{\"end\":55410,\"start\":55324},{\"end\":55947,\"start\":55836},{\"end\":56396,\"start\":56288}]", "bib_author": "[{\"end\":43486,\"start\":43477},{\"end\":43495,\"start\":43486},{\"end\":43505,\"start\":43495},{\"end\":43519,\"start\":43505},{\"end\":43528,\"start\":43519},{\"end\":43536,\"start\":43528},{\"end\":43904,\"start\":43889},{\"end\":43920,\"start\":43904},{\"end\":43938,\"start\":43920},{\"end\":43954,\"start\":43938},{\"end\":44465,\"start\":44450},{\"end\":44481,\"start\":44465},{\"end\":44499,\"start\":44481},{\"end\":44515,\"start\":44499},{\"end\":45042,\"start\":45027},{\"end\":45054,\"start\":45042},{\"end\":45070,\"start\":45054},{\"end\":45088,\"start\":45070},{\"end\":45104,\"start\":45088},{\"end\":45442,\"start\":45429},{\"end\":45458,\"start\":45442},{\"end\":45474,\"start\":45458},{\"end\":45772,\"start\":45761},{\"end\":45786,\"start\":45772},{\"end\":46008,\"start\":45993},{\"end\":46026,\"start\":46008},{\"end\":46044,\"start\":46026},{\"end\":46062,\"start\":46044},{\"end\":46425,\"start\":46411},{\"end\":46440,\"start\":46425},{\"end\":46455,\"start\":46440},{\"end\":46889,\"start\":46875},{\"end\":46912,\"start\":46889},{\"end\":46925,\"start\":46912},{\"end\":47415,\"start\":47398},{\"end\":47430,\"start\":47415},{\"end\":47445,\"start\":47430},{\"end\":47913,\"start\":47905},{\"end\":47930,\"start\":47913},{\"end\":47940,\"start\":47930},{\"end\":48230,\"start\":48211},{\"end\":48246,\"start\":48230},{\"end\":48263,\"start\":48246},{\"end\":48575,\"start\":48566},{\"end\":48584,\"start\":48575},{\"end\":48597,\"start\":48584},{\"end\":48900,\"start\":48893},{\"end\":48908,\"start\":48900},{\"end\":48914,\"start\":48908},{\"end\":48920,\"start\":48914},{\"end\":49286,\"start\":49267},{\"end\":49299,\"start\":49286},{\"end\":49311,\"start\":49299},{\"end\":49330,\"start\":49311},{\"end\":49346,\"start\":49330},{\"end\":49702,\"start\":49692},{\"end\":49710,\"start\":49702},{\"end\":49723,\"start\":49710},{\"end\":49737,\"start\":49723},{\"end\":49750,\"start\":49737},{\"end\":50117,\"start\":50104},{\"end\":50128,\"start\":50117},{\"end\":50146,\"start\":50128},{\"end\":50162,\"start\":50146},{\"end\":50175,\"start\":50162},{\"end\":50191,\"start\":50175},{\"end\":50203,\"start\":50191},{\"end\":50220,\"start\":50203},{\"end\":50233,\"start\":50220},{\"end\":50245,\"start\":50233},{\"end\":50516,\"start\":50509},{\"end\":50524,\"start\":50516},{\"end\":50533,\"start\":50524},{\"end\":50814,\"start\":50796},{\"end\":50831,\"start\":50814},{\"end\":50844,\"start\":50831},{\"end\":51382,\"start\":51368},{\"end\":51396,\"start\":51382},{\"end\":51412,\"start\":51396},{\"end\":51432,\"start\":51412},{\"end\":51445,\"start\":51432},{\"end\":51459,\"start\":51445},{\"end\":51962,\"start\":51947},{\"end\":51977,\"start\":51962},{\"end\":51993,\"start\":51977},{\"end\":52462,\"start\":52447},{\"end\":52476,\"start\":52462},{\"end\":52492,\"start\":52476},{\"end\":52507,\"start\":52492},{\"end\":53007,\"start\":52991},{\"end\":53024,\"start\":53007},{\"end\":53054,\"start\":53024},{\"end\":53077,\"start\":53054},{\"end\":53084,\"start\":53077},{\"end\":53545,\"start\":53529},{\"end\":53560,\"start\":53545},{\"end\":53576,\"start\":53560},{\"end\":53584,\"start\":53576},{\"end\":53912,\"start\":53902},{\"end\":53924,\"start\":53912},{\"end\":53935,\"start\":53924},{\"end\":53948,\"start\":53935},{\"end\":53966,\"start\":53948},{\"end\":54316,\"start\":54306},{\"end\":54330,\"start\":54316},{\"end\":54676,\"start\":54665},{\"end\":54686,\"start\":54676},{\"end\":54698,\"start\":54686},{\"end\":54706,\"start\":54698},{\"end\":54719,\"start\":54706},{\"end\":54729,\"start\":54719},{\"end\":54739,\"start\":54729},{\"end\":55022,\"start\":55013},{\"end\":55034,\"start\":55022},{\"end\":55043,\"start\":55034},{\"end\":55058,\"start\":55043},{\"end\":55071,\"start\":55058},{\"end\":55430,\"start\":55412},{\"end\":55445,\"start\":55430},{\"end\":55461,\"start\":55445},{\"end\":55957,\"start\":55949},{\"end\":55963,\"start\":55957},{\"end\":55971,\"start\":55963},{\"end\":55977,\"start\":55971},{\"end\":55984,\"start\":55977},{\"end\":55991,\"start\":55984},{\"end\":56412,\"start\":56398},{\"end\":56419,\"start\":56412},{\"end\":56428,\"start\":56419},{\"end\":56440,\"start\":56428},{\"end\":56456,\"start\":56440},{\"end\":56466,\"start\":56456}]", "bib_venue": "[{\"end\":44103,\"start\":44037},{\"end\":44664,\"start\":44598},{\"end\":46531,\"start\":46513},{\"end\":46997,\"start\":46983},{\"end\":47592,\"start\":47527},{\"end\":50962,\"start\":50947},{\"end\":51608,\"start\":51542},{\"end\":52140,\"start\":52075},{\"end\":52654,\"start\":52589},{\"end\":54425,\"start\":54386},{\"end\":55590,\"start\":55534},{\"end\":43597,\"start\":43536},{\"end\":44035,\"start\":43954},{\"end\":44596,\"start\":44515},{\"end\":45125,\"start\":45104},{\"end\":45495,\"start\":45474},{\"end\":45759,\"start\":45687},{\"end\":46109,\"start\":46062},{\"end\":46511,\"start\":46455},{\"end\":46981,\"start\":46925},{\"end\":47525,\"start\":47445},{\"end\":47989,\"start\":47940},{\"end\":48312,\"start\":48263},{\"end\":48649,\"start\":48597},{\"end\":48972,\"start\":48920},{\"end\":49378,\"start\":49346},{\"end\":49821,\"start\":49750},{\"end\":50102,\"start\":50066},{\"end\":50565,\"start\":50533},{\"end\":50945,\"start\":50844},{\"end\":51540,\"start\":51459},{\"end\":52073,\"start\":51993},{\"end\":52587,\"start\":52507},{\"end\":53153,\"start\":53084},{\"end\":53620,\"start\":53584},{\"end\":54018,\"start\":53966},{\"end\":54384,\"start\":54330},{\"end\":54663,\"start\":54581},{\"end\":55123,\"start\":55071},{\"end\":55532,\"start\":55461},{\"end\":56043,\"start\":55991},{\"end\":56485,\"start\":56466}]"}}}, "year": 2023, "month": 12, "day": 17}