{"id": 199405345, "updated": "2023-10-06 23:57:41.706", "metadata": {"title": "Distilling Knowledge From a Deep Pose Regressor Network", "authors": "[{\"first\":\"Muhamad\",\"last\":\"Saputra\",\"middle\":[\"Risqi\",\"U.\"]},{\"first\":\"Pedro\",\"last\":\"Gusmao\",\"middle\":[\"P.\",\"B.\",\"de\"]},{\"first\":\"Yasin\",\"last\":\"Almalioglu\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Markham\",\"middle\":[]},{\"first\":\"Niki\",\"last\":\"Trigoni\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 8, "day": 2}, "abstract": "This paper presents a novel method to distill knowledge from a deep pose regressor network for efficient Visual Odometry (VO). Standard distillation relies on\"dark knowledge\"for successful knowledge transfer. As this knowledge is not available in pose regression and the teacher prediction is not always accurate, we propose to emphasize the knowledge transfer only when we trust the teacher. We achieve this by using teacher loss as a confidence score which places variable relative importance on the teacher prediction. We inject this confidence score to the main training task via Attentive Imitation Loss (AIL) and when learning the intermediate representation of the teacher through Attentive Hint Training (AHT) approach. To the best of our knowledge, this is the first work which successfully distill the knowledge from a deep pose regression network. Our evaluation on the KITTI and Malaga dataset shows that we can keep the student prediction close to the teacher with up to 92.95% parameter reduction and 2.12x faster in computation time.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1908.00858", "mag": "2965100203", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/SaputraGAMT19", "doi": "10.1109/iccv.2019.00035"}}, "content": {"source": {"pdf_hash": "1fce898e203b1932a6a3055e59ccc199d1d4b47e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1908.00858v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1908.00858", "status": "GREEN"}}, "grobid": {"id": "d541762e8a6577c1f10d0f21130a6826235ae481", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1fce898e203b1932a6a3055e59ccc199d1d4b47e.txt", "contents": "\nDistilling Knowledge From a Deep Pose Regressor Network\n\n\nRisqi U Muhamad \nDepartment of Computer Science\nUniversity of Oxford\n\n\nPedro P B Saputra \nDepartment of Computer Science\nUniversity of Oxford\n\n\nYasin De Gusmao \nDepartment of Computer Science\nUniversity of Oxford\n\n\nAndrew Almalioglu \nDepartment of Computer Science\nUniversity of Oxford\n\n\nNiki Markham \nDepartment of Computer Science\nUniversity of Oxford\n\n\nTrigoni \nDepartment of Computer Science\nUniversity of Oxford\n\n\nDistilling Knowledge From a Deep Pose Regressor Network\n\nThis paper presents a novel method to distill knowledge from a deep pose regressor network for efficient Visual Odometry (VO). Standard distillation relies on \"dark knowledge\" for successful knowledge transfer. As this knowledge is not available in pose regression and the teacher prediction is not always accurate, we propose to emphasize the knowledge transfer only when we trust the teacher. We achieve this by using teacher loss as a confidence score which places variable relative importance on the teacher prediction. We inject this confidence score to the main training task via Attentive Imitation Loss (AIL) and when learning the intermediate representation of the teacher through Attentive Hint Training (AHT) approach. To the best of our knowledge, this is the first work which successfully distill the knowledge from a deep pose regression network. Our evaluation on the KITTI and Malaga dataset shows that we can keep the student prediction close to the teacher with up to 92.95% parameter reduction and 2.12\u00d7 faster in computation time.\n\nIntroduction\n\nDeep Neural Networks (DNNs) have received increased attention in the last decade due to their success in image and natural language understanding. The availability of large datasets, increased computing power, and advancement of learning algorithms play a pivotal role in their glory. Despite their successes, DNN-based approaches typically require tens or hundreds of million weights. As a consequence, the huge computational and space requirement prevents DNN models from being widely implemented in resource-constrained environment (e.g. mobile phones, quadcopter, etc.). To compound the issue, these applications typically require near real-time inference.\n\nWithin the last few years, there have been tremendous efforts towards compressing DNNs. State-of-the-art approaches for network compression such as quantization [9,6,17], pruning [12,10,35], or low-rank decomposition [30,2] can yield significant speed-ups but at the cost of ac-curacy. On the other hand, an approach called Knowledge Distillation (KD) proposed by Hinton et al. [16] offers to recover the accuracy drop by transferring the knowledge of a large teacher model to a small student model. Some recent works show that a small network trained by KD could match or even exceed the accuracy of a large network if it is trained with careful optimization [25].\n\nMost works in network compression, including KD, focus on the problem of classification. KD works very well in classification since it has the advantage of \"dark knowledge\" which refers to the softened logits output of the teacher. This provides more information than mere onehot encoding of the class label and contains hidden knowledge about the correlations of class labels [16]. By using the logits output for training, the student network can emulate the generalization capability of the teacher network. However, this advantage does not exist in regression. In the regression problem, a deep regression network predicts sequential, continuous, values which have the exact same characteristics as the ground truth, with the exception of being plagued with an unknown error distribution. Without access to any dark knowledge, it is unclear how KD could help in compressing a regression network. In recent surveys, it is even stated that the main drawback of KD is that it only works for classification problems [5].\n\nKD methods for classification [16,25,20,36,32,19,23] rely solely on the teacher prediction without considering the error made w.r.t. ground truth. In regression however, the real-valued predictions are unbounded, and hence the teacher can give highly erroneous guidance to the student network. Previous work [4] alleviated this issue by using teacher loss as an upper bound. However, it was designed for standard bounding box regression which has different characteristic to pose regression as it belongs to SE(3) (Lie Groups). Moreover, they directly transferred the knowledge from the teacher without filtering which one is good and which one is bad. To this end, our novel insight is to use the teacher loss as a confidence score to decide when we can trust the teacher. We demonstrate that this is key to successfully distilling deep pose regression networks.\n\nWe will demonstrate our work in distillation for cam- era pose regression problem which is widely known as Visual Odometry (VO). In particular, we employ DNN-based VO methods [33,34,37,38,26,1] which replaces the conventional VO pipeline based on multiple-view geometry [15,27] to DNN-based pipeline which automatically learns useful features for estimating 6 Degree-of-Freedom (DoF) camera poses. To the best of our knowledge, this work is a first attempt to distill the knowledge from a deep pose regression network. Our key contributions are:\n\n\u2022 We study different ways to blend the loss of the student both w.r.t. ground truth and w.r.t. teacher, and propose to use the teacher loss as a confidence score to attentively learn from examples that the teacher is good at predicting through Attentive Imitation Loss (AIL).\n\n\u2022 We also propose Attentive Hint Training (AHT) as a novel way to learn the intermediate representation of the teacher, based on the teacher loss.\n\n\u2022 We perform extensive experiment on KITTI and Malaga datasets which show that our proposed approach can reduce the number of student parameters by up to 92.95% (2.12\u00d7 faster) whilst keeping the prediction accuracy very close to that of the teacher.\n\n\nRelated Work\n\nMany compression methods have been recently developed. Besides KD, there are other approaches available such as quantization, pruning, and low-rank decomposition.\n\nNetwork quantization reduces the number of bits required to compress the network. The quantization could be applied by using 16-bit or 8-bit representation as proposed by [31,11]. As an extreme case, a 1-bit representation (or binary network) could also be used as seen in [6,17]. By restricting the weights into two possible values (e.g. -1 or 1), binary networks can dramatically reduce both computation time and memory consumption at the cost of significant reduction in accuracy [5].\n\nPruning, as the name implies, removes redundant and non-informative weights. Weight pruning can be done by using magnitude-based method [14,10] or dropout-based method [35,22]. The pruning can be applied in the individual neuron connection [14,13] or in convolutional filter itself [21]. This approach promises significant parameter reduction without greatly affecting accuracy, although it typically requires more training stages [13]. However, in practice, it requires additional implementation of sparse matrix multiplication which possibly needs more resource consumption and specialized hardware and software [21].\n\nLow-rank decomposition reduces DNN complexity by exploiting the low-rank constraints on the network weights. Most approaches, such as [24,18,2], factorize a fully connected or convolutional layer through matrix/tensor decomposition techniques such as SVD or CP-decomposition. By using this factorization technique, the number of matrix multiplications becomes smaller than the original network. However, the obtained compression ratio is generally lower than the pruning-based approach and the low-rank constraint imposed on the network might impact the network performance if the rank is not selected with care.\n\n\nBackground: Distillation for Classification\n\nKnowledege Distillation (KD) is an approach to transfer the knowledge of a large teacher network to a small student network. The main idea of KD is to allow the student to capture the finer structure learned by the teacher instead of learning solely from the true labels. Let T be the teacher network where O T = softmax(a T ) is the teacher output probability and a T is the teacher's logits (pre-softmax output). A student network S with O S = softmax(a S ) as the prediction and a S as the logits is trained to mimic O T . Since O T is usually very close to the one-hot representation of the class labels, a temperature \u03c4 > 1 is used to soften the output probability distribution of T . The same temperature is used for training S such that O \u03c4 T = softmax( a T \u03c4 ) and O \u03c4 S = softmax( a S \u03c4 ), but \u03c4 = 1 is then used for testing S. If H is the cross-entropy and y is the one-hot encoding of the true labels, then KD objective function is formed by minimizing both hard label (y) error and soft label error (illustrated in Fig. 1 (a)) as follows\nL KD = \u03b1H(y, O S ) + (1 \u2212 \u03b1)H(O T , O S )(1)\nwhere \u03b1 is a parameter to balance both cross-entropies. The KD formulation with softened outputs (\u03c4 > 1) in (1) gives more information for S to learn, as it provides information about the relative similarity of the incorrect predictions [16], [25]. For example, T may mistakenly predict an image of a car as a truck, but that mistake still has a much higher probability than mistaking it for a cat. These relative probabilities of incorrect prediction convey how T tends to generalize to new data [16]. However, this advantage does not exist in the regression problem. As seen in Fig. 1 (b), both teacher and ground truth label have the same characteristic. Intuitively, we would prefer to minimize S's prediction directly w.r.t. the ground truth labels since the teacher label is plagued with an unknown error distribution. However, our experiments show that training S only with the ground truth labels gives very poor results.\n\n\nBlending Teacher, Student, and Imitation Loss\n\nAs it is unclear how we can take advantage of T 's prediction in distilling regression networks, we study different ways to blend together the loss of S's prediction w.r.t. ground truth and w.r.t T 's prediction. For simplicity, we refer to the error of S w.r.t ground truth as student loss and the loss of T w.r.t ground truth as teacher loss. We refer to imitation loss (L imit ) for the errors of S w.r.t. T (since S tries to imitate T 's prediction). The following outlines different formulations and rationales of blending teacher, student, and imitation loss. Minimum of student and imitation. In the simplest formulation, we assume that T has good prediction accuracy in all conditions. In this case, as T 's prediction will be very close to the ground truth, it does not really matter whether we minimize S w.r.t. ground truth or w.r.t. T . Then, we simply minimize whichever one is smaller between the student loss and imitation loss as follows\nL reg = 1 n n i=1 min p S \u2212 p gt 2 , p S \u2212 p T 2(2)\nwhere p S , p T , and p gt are S's prediction, T 's prediction, and ground truth labels respectively. Imitation loss as an additional loss. Instead of seeking the minimum between the student and imitation loss, we can use the imitation loss as an additional loss term for the student loss. In this case, we regard the imitation loss as another way to regularize the network and prevent the network from overfitting [16]. Then, the objective function becomes\nL reg = 1 n n i=1 \u03b1 p S \u2212 p gt 2 + (1 \u2212 \u03b1) p S \u2212 p T 2 (3)\nwhere \u03b1 is a scale factor used to balance the student and imitation loss. This formulation is similar to the original formulation of KD for classification as seen in (1) except the cross-entropy loss is replaced by regression loss.\n\nTeacher loss as an upper bound. Equations (2) and (3) assume that T has very good generalization capability in most conditions. However in practice, T can give very erroneous guidance for S. There is a possibility that in adverse environments, T may predict camera poses that are contradictory to the ground truth pose. Hence, instead of directly minimizing S w.r.t. T , we can utilize T as an upper bound. This means that S's prediction should be as close as possible to the ground truth pose, but we do not add additional loss for S when its performance surpasses T [4]. In this formulation, (3) becomes the following equation\nL reg = 1 n n i=1 \u03b1 p S \u2212 p gt 2 + (1 \u2212 \u03b1)L imit (4) L imit = p S \u2212 p T 2 , if p S \u2212 p gt 2 > p T \u2212 p gt 2 0, otherwise(5)\nProbabilistic imitation loss (PIL). As stated before, T is not always accurate in practice. Since there is some degree of uncertainty in T 's prediction, we can explicitly model this uncertainty with a parametric distribution. For example, we can model the imitation loss using Laplace's distribution\nIP (p S |p T , \u03c3) = 1 2\u03c3 exp \u2212 p S \u2212 p T \u03c3 (6)\nwhere \u03c3 is an additional quantity that S should predict. In this case, the imitation loss is turned into minimizing the negative log likelihood of (6) as follows\n\u2212 log IP (p S |p T , \u03c3) = p S \u2212 p T \u03c3 + log \u03c3 + const. (7)\nThe final objective is retrieved by replacing L imit in (4) with (7). We can view (7) as a way for S to learn suitable coefficient (via \u03c3) to down-weight unreliable T 's prediction. Besides Laplacian distribution, another parametric distribution like Gaussian can be used as well.\n\nAttentive imitation loss (AIL). The main objective of modeling the uncertainty in the imitation loss is that we could then adaptively down-weight the imitation loss when a particular T 's prediction is not reliable. However, modeling T 's prediction with a parametric distribution may not accurately reflect the error distribution of T 's prediction. Hence, instead of relying on S to learn a quantity \u03c3 to down-weight unreliable T 's prediction, we can use the empirical error of T 's prediction w.r.t. ground truth (which is the teacher loss) to do the job. Then, the objective function becomes\nL reg = 1 n n i=1 \u03b1 p S \u2212 p gt 2 i + (1 \u2212 \u03b1)\u03a6 i p S \u2212 p T 2 i (8) \u03a6 i = 1 \u2212 p T \u2212 p gt 2 i \u03b7 (9) \u03b7 = max (e T ) \u2212 min (e T )(10)e T = { p T \u2212 p gt 2 j : j = 1, ..., N }(11)\nwhere \u03a6 i is the normalized teacher loss for each i sample, e T is a set of teacher loss from entire training data, and \u03b7 is a normalization parameter that we can retrieve from subtracting the maximum and the minimum of e T . Note that . i and . j are not p-norm symbol. Instead we use i and j in . i and . j as index to differentiate which loss is computed from the batch samples (i = 1, ..., n) and which loss is calculated from entire training data (j = 1, ..., N ). Fig. 1 (b) shows how each component in (8)-(11) blends together. Note that we still keep \u03b1 to govern the relationship between student and imitation loss. In this case, \u03a6 i 's role is to put different relative importance, hence it is called attentive, for each component in the imitation loss as seen in weighted sum operation. Notice that (8) can be rearranged into\nL reg = \u03b1 n n i=1 p S \u2212 p gt 2 i + 1\u2212\u03b1 n n i=1 \u03a6 i p S \u2212 p T 2 i .\nAs \u03a6 i is computed differently for each image sample and is intended to down-weight unreliable T 's prediction, we could also say that by multiplying the imitation loss with \u03a6 i , we rely more on the example data which T is good at predicting in the process of knowledge transfer between T and S.\n\n\nLearning Intermediate Representations\n\nBlending teacher, student, and imitation loss have set the objective function for the main KD task. Another important aspect in KD's transfer process is Hint Training (HT). HT is the process of training the intermediate representation of S such that it could mimic the latent representation of T . It was designed as an extension of the original KD [16] and formulated by [25] to transfer the knowledge of T to S with deeper but thinner architecture. Even if it is devised to help training S with deeper layers than T , we would argue that it is also an important step for training a regressor network with shallow architecture. HT could act as another way to regularize S such that it could better mimic the generalization capability of T [25].\n\nIn Hint Training, a hint is defined as a layer in T that is used to guide a guided layer in S. Let W guided and W hint be the parameters of S and T up to their guided and hint layers respectively. With the standard HT formulation, we can train S up to the guided layer by minimiz-\ning L hint = 1 n n i=1 \u03a8 T (I; W hint ) \u2212 \u03a8 S (I; W guided ) 2 ,\nwhere \u03a8 T and \u03a8 S are T 's and S's deep neural functions up to their respective hint or guided layers. The drawback with this formulation is that it does not take into account the fact that T is not a perfect function estimator and can give incorrect guidance to S. While in Section 4 we describe how to tackle this issue through down-weighting unreliable T prediction by multiplying it with normalized teacher loss, we argue that this step is also required for HT. Then, we propose a modification of HT termed Attentive Hint Training (AHT) as follows:\nL hint = 1 n n i=1 \u03a6 i \u03a8 T (I; W hint ) \u2212 \u03a8 S (I; W guided ) 2 i(12)\nwhere \u03a6 i is the normalized teacher loss as seen in (9). While (8) and (12) can be trained jointly, we found out that training separately yields superior performance especially in absolute pose error. Then, the knowledge transfer between T and S becomes 2 stages optimization procedures. The 1st stage trains S up to the guided layer with (12) as the objective. The 2nd stage trains the remaining layer of S (from guided until the last layer) with (8) as the objective.\n\n6. Implementation Details\n\n\nCamera Pose Regression with DNNs\n\nAs we demonstrate our distillation approach for Visual Odometry (VO) problem, we will briefly review VO approaches. Conventional VO estimates the camera poses by finding feature correspondences between multiple images and applying multiple-view geometry techniques [15,27]. On the other hand, DNNs learn the camera ego-motion directly from raw image sequences by training the network in an end-to-end manner. Let I t\u22121,t \u2208 IR 2\u00d7(w\u00d7h\u00d7c) be two concatenated images at times t \u2212 1 and t, where w, h, and c are the image width, height, and channels respectively. DNNs essentially learn a mapping function to regress the 6-DoF camera poses {(IR 2\u00d7(w\u00d7h\u00d7c) ) 1:N } \u2192 {(IR 6 ) 1:N }, where N are the total number of image pairs. In the supervised case, learning 6-DoF camera poses can be achieved by minimizing the discrepancy between the predicted poses p pr \u2208 IR 6 and the ground truth poses p gt \u2208 IR 6 as fol-lows L reg = 1 n n i=1 p pr \u2212 p gt 2 , given n sample images. However, since translation and rotation have different constraints, we usually decompose L reg into\nL reg = 1 n n i=1 \u03b2 t pr \u2212 t gt 2 + (1 \u2212 \u03b2) r pr \u2212 r gt 2(13)\nwhere t \u2208 IR 3 and r \u2208 IR 3 are the translation and rotation components in x, y, and z axes. \u03b2 \u2208 IR is used to balance t and r. Here the rotation part is represented as an Euler angle. Another representation such as quaternion or rotation matrix can be used as well [33].\n\nTo apply our distillation approach to DNN-based VO, we decompose each translation and rotation component in (13) to (8). We also decompose AHT formulation in (12) into translation and rotation representation, and apply \u03a6 i differently for each representation. As the teacher loss distribution is also different for translation and rotation (as seen in Fig. 2), \u03b7 in (9) is computed differently for each of them.  Figure 2. Empirical error distribution of the teacher network for translation and rotation on KITTI dataset Seq 00-08.\n\n\nNetwork Architecture\n\nWe employ ESP-VO [34] for the teacher network T in which the architecture is depicted in Fig. 3 (left). It consists of two main parts, namely the feature extractor network and a pose regressor network. The feature extractor is composed from a series of Convolutional Neural Networks (CNNs) to extract salient features for VO estimation. Since VO estimates the camera pose between consecutive frames, optical-flow like feature extractor network (FlowNet [7]) is used to initialize the CNNs. The pose regressor consists of Long-Short Term Memory (LSTM) Recurrent Neural Networks (RNNs) and Fully Connected (FC) Layers to regress 6-DoF camera poses. The LSTM is utilized to learn longterm motion dependencies among image frames [33]. Fig. 3 (right) depicts S with 92.95% distillation rate (d rate ). The main building blocks of S are essentially the same as T except we remove a number of layers from T to construct a smaller network. To specify the structure of S, in general, we can remove the layers from T which contribute the most to the number of weights, but S should still consist of a feature extractor (CNN) and a regressor (LSTM/FC). In the feature extractor, the largest number of weights usually corresponds to the few last layers of CNNs, while in the regressor part it corresponds to the LSTM layers. Thus, for d rate = 92.95%, we remove the last five layers of the CNN and the two RNN-LSTM layers. However, we still initialize the CNN with the FlowNet's weight as in ESP-VO. To compensate for the loss of removing the CNN and LSTM layers, we add 1 FC layer in the regressor part for d rate < 75% and 2 FC layers for d rate > 75%. This corresponds to fewer than 1% additional parameters.  Figure 3. Details of network architecture for teacher (left) and student network with 92.95% distillation rate (right).\n\n\nTraining Details\n\nAs stated in Section 5, we employ two stages of optimization. The first stage is training the intermediate representation of S through AHT. As seen in Fig. 3, we select the 1st FC layer of T as a hint and the 3rd FC layer of S (or the 2nd FC layer for d rate < 75%) as the guided layer. We used the FC layer of T as a hint not only to provide easier guidance for training S, since both FC layers in T and S have the same dimensions, but also to transfer the ability of T to learn the long-term motion dynamics of camera poses as the FC layer of T is positioned after the RNN-LSTM layers. In the second stage, we freeze weights of S trained from the first stage and train the remaining layers of S using (8) as the objective.\n\n\nExperimental Results\n\n\nExperiment Environments\n\nWe implemented T and S in Keras. We employed NVIDIA TITAN V GPU for training and NVIDIA Jetson TX2 for testing. The training for each stage goes up to 30 epochs. For both training stages, we utilize Adam Optimizer with 1e \u2212 4 learning rate. We also applied Dropout [28] with 0.25 dropout rate for regularizing the network. For the data, we used KITTI [8] and Malaga odometry dataset [3]. We utilized KITTI Seq 00-08 for training and Seq 09-10 for testing. Before training, we reduced the KITTI image dimension to 192 \u00d7 640. We only use Malaga dataset for testing the model that has been trained on KITTI. For this purpose, we cropped the Malaga images to the KITTI image size. Since there is no ground truth in Malaga dataset, we perform qualitative evaluation against GPS data.\n\n\nMetrics\n\nIn this work, we want to measure the trade-off between accuracy and parameter reduction. In VO, accuracy can be measured by several metrics. We use Root Mean Square (RMS) Relative Pose Error (RPE) for translation (t) and rotation (r) and RMS Absolute Trajectory Error (ATE) as they has been widely used in many VO or SLAM benchmarks [29]. For parameter reduction, we measure the percentage (%) of S's parameters w.r.t. T 's parameters. We also measure the associated computation time (ms) and the model size (MB) for each reduction rate.\n\n\nSensitivity Analysis\n\nThe Impact of Different Methods for Blending Teacher, Student, and Imitation Loss. In this experiment, we want to understand the impact of different approaches to blending teacher, student, and imitation loss as described in Section 4. We used S with d rate = 72.78% constructed from removing the last 3 CNNs and replacing 2 LSTMs with 1 FC layer. In order to get a fair comparison without having bias from the AHT process, we trained S with standard HT approach in the first stage. Then, we trained the remaining layer(s) with all formulations described in Section 4 in the second stage. For additional comparison, we add a baseline approach, in which we only minimize the student loss. Fig. 4 (a) and (b) depicts the RPE and the CDF of ATE of different methods in blending the losses. It can be seen that AIL has the best accuracy in both RPE and ATE. This indicates that distilling knowledge from T to S only when we trust T does not reduce the quality of knowledge transfer, but instead improve the generalization capability of S. Two approaches (minimum of student and imitation; imitation loss as additional loss) that rely on the assumption that T 's prediction is always accurate have inferior performance even if compared to the baseline. PIL, either using Lapla-    cian or Gaussian, yields good accuracy in RPE, but lacks robustness since they have larger overall drift (as seen in Fig.  4 (b)). This is probably due to the failure of the parametric distribution function to model the teacher error distribution accurately. The upper bound objective has good balance between RPE and ATE but the performance is inferior to AIL.\n\nThe Impact of Attentive Hint Training. As we want to inspect the effect of the proposed AHT approach, we trained the model with 3 different procedures: without HT, with HT, and with AHT. We also alternate between using the student loss and AIL to see the effect of applying attentive transfer mechanism in both intermediate (as AHT) and final layer (as AIL), or only in one of them. We used the same model architecture as the previous ablation to conduct this experiment. We compare RMS Reconstruction Error of S's output latent representation w.r.t. T 's representation and ATE w.r.t. ground truth. Table 1 lists the results of this study which clearly shows that as soon as HT is applied, S's reconstruction error w.r.t. T reduces dramatically (see row [1,2] or [6,7]). This shows that without having guidance in the intermediate layer, it is very difficult for S to imitate the generalization capability of T . AHT then reduces further the reconstruction error of HT by giving different relative importance to T 's representation and placing more emphasis on representations that produce accurate T predictions. Fig. 5 visualizes the output latent representation for different training procedures. It can be seen that AHT's output representation is very close to T . Slight differences with T 's representation are due to different relative importance placed on T 's predictions. However, even if AHT does not try to blindly imitate T 's representation, the average reconstruction error is still lower than the HT approach which attempts to perfectly imitate T 's representation (see Table 1 row [2,4] or [7,9]).\n\nThe last column of Table 1 shows ATE for different combinations of applying attentive knowledge transfer. As it can be seen in row [2,4] (or [7,9]) that applying attentive loss in the intermediate layer (AHT) can significantly reduce the ATE of S. However, the reduction rate is not as large as when applying it in the final task (AIL) (see Table 1 row [2,3] or [7,8]) as it can reduce the ATE up to 1.8\u00d7 smaller. This is sensible because the accuracy of a DNN model depends on the output from the final task. A better guidance in the final layer (main task) can yield stronger performance than a better guidance in the intermediate layer. Finally, applying attentive loss in both intermediate (AHT) and final layers (AIL) consistently gives the best result for 6 and 5 CNNs architecture (see Table 1 row 5 and 10). \n\n\nTrade-off between Accuracy, Model Size, and Execution Time\n\nIn this experiment, we want to understand the trade-off between the model size, execution time, and accuracy for different d rate . Fig. 6 shows that our proposed distillation approach can keep S very close to T up to d rate = 92.95%. It can even achieve better performance than the teacher for d rate = 65.77% and 79.69% as T might be overparameterized (see also the output trajectory in Fig. 7). For d rate > 92.95%, the performance starts to degrade more  rapidly as it becomes too difficult to transfer the knowledge from T to S without other constraints. It can also be seen that if S is trained directly to fit the ground truth with hard loss (supervised student), it shows very poor performance. Table 2 shows the comparison between T and S in terms of number of parameters, model size, and computation time. As we can see, with d rate = 92.95% we can reduce the model size from 286.9MB to 7.3MB (2.5%). Removing 2 LSTMs, which are responsible for 44.72% of T 's parame- This has significant practical implication. If we re-train T given subsampled images such that the frame rate is similar to S with d rate = 92.95%, T 's prediction accuracy will degrade 160% (see the trajectory in Fig. 7). This is probably due to the difficulty of estimating accurate optical flow representation in large stereo baseline. Meanwhile with the same computation budget, the distilled S yields stronger performance than the subsampled T with only 8.63% accuracy drop w.r.t supervised T as seen in Fig. 8.  \n\n\nComparison with Related Works\n\nAs there is no specific KD for pose regression, to compare our proposed KD with other related works, we used some well known KD approaches for classification and object detection. However, we modify the objective function to fit our regression problem. We used 3 baselines for this experiment: KD [16], FitNets [25], and Chen's Object Detection (OD) model [4]. For KD [16], we trained with standard training procedure (without HT or AHT) and replaced the objective function with (3). For FitNets [25], we used HT approach for the 1st stage of training and utilize (3) as the objective in the 2nd stage. For Chen's OD [4], we also used standard HT for the 1st stage and employ (4) as the objective in the 2nd stage. For all models, we used d rate = 92.95% to train S as an extreme example (see supplementary material for experiment with d rate = 65.77%).  both RPE and ATE. Even if most of the competing approaches have better RPE than the supervised T , it has huge bias in the relative pose prediction such that the integration of these relative poses yields very large ATE. T tackle this bias to some extent by using LSTM layers which are supposed to learn the long-term motion dynamic of the camera poses [33]. Since S removes the LSTM layers, most approaches fail to recover this knowledge from T but our proposed approach is able to reduce ATE by focusing to learn the good predictions from T . Fig. 7 shows the comparison of the output trajectory between the competing approaches on KITTI dataset. It can be seen that our distillation output trajectory is closer to T than the competing approaches. Fig. 9 depicts the qualitative evaluation in Malaga dataset. Note that we have not trained on this dataset, demonstrating generalization capacity. We can see that our proposed approach yields a closer trajectory to GPS, even when it is compared to T . This signifies that our distillation approach yields good generalization ability even when it is tested on a different dataset. This result also shows that T may overfit the training data as it has many redundant parameters. However, this redundancy seems necessary for the initial stage of training as a DNN requires large degree-of-freedom to find better weights and connections [16]. Meanwhile, directly training S without any supervision from T seems to be very difficult. Our results show that we can have better generalization when distilling large degree-of-freedom T to small degree-of-freedom S if we transfer the knowledge from T to S only when we trust T .\n\n\nConclusion\n\nWe have presented an approach to distill the knowledge from a deep pose regressor network to a much smaller network with a small loss of accuracy. We have shown that the teacher loss can be used as an effective attentive mechanism to transfer the knowledge between teacher and student. For future work, we will investigate whether another compression technique can be combined with our distillation approach to further reduce the computation time.\n\nFigure 1 .\n1Comparison between (a) standard Knowledge Distillation applied to classification problem and (b) our Knowledge Distillation approach applied to regression problem. Note that in regression, we are unable to use the dark knowledge provided by soft teacher labels.\n\nFigure 4 .\n4The impact of different ways in blending teacher, student, and imitation loss to (a) RPE and (b) ATE. Same legend is used for both graphs.\n\na\nTotal FC layers until intermediate layer used for HT and AHT. The number in the bracket indicates drate. b Reconstruction error of S's output intermediate representation w.r.t. T 's output intermediate representation.\n\nFigure 5 .\n5The difference of latent feature representation between T and S, trained without HT, with HT, and with AHT.\n\nFigure 6 .Figure 7 .\n67RMS absolute pose errors between Supervised and Distilled Student for different drate. Trajectory prediction from T and S trained with various distillation approaches in KITTI Seq 09. The number in the bracket indicates the percentage of S parameters w.r.t. T .\n\nFigure 8 .\n8Distribution and histogram of ATE between distilled S and supervised T with image sampling.\n\nFigure 9 .\n9Qualitative evaluation in Malaga dataset Seq 04 and Seq 09. All model are only trained on KITTI Seq 00-08.\n\nTable 1 .\n1The impact of using Attentive Imitation Loss (AIL) and \nAttentive Hint Training (AHT) algorithm \n\nArchitecture \nHT \nFinal \nRec. \nATE \nCNN-FC a \nObj. \nError b \n(m) \n1 \n6 -2 (72.88%) \n-\nstudent 0.6242 80.842 \n2 \n6 -2 (72.88%) \nHT \nstudent 0.0252 65.251 \n3 \n6 -2 (72.88%) \nHT \nAIL \n0.0252 36.459 \n4 \n6 -2 (72.88%) AHT student 0.0166 52.320 \n5 \n6 -2 (72.88%) AHT \nAIL \n0.0166 32.259 \n\n\n\nTable 2 .\n2Trade-off between the number of parameters, model size, computation time, and accuracy (ATE)Network \nParameters \nSize \nEx. Time \nATE \n(Weights %) \n(millions) \n(MB) \n(ms) \n(m) \nT (100%) \n33.64 \n286.9 \n87 \n26.74 \nS (55.28%) \n18.59 \n74.6 \n82 \n26.92 \nS (41.25%) \n13.88 \n55.7 \n71 \n29.69 \nS (34.23%) \n11.52 \n46.3 \n62 \n18.09 \nS (27.22%) \n9.16 \n36.8 \n58 \n32.26 \nS (20.30%) \n6.83 \n27.5 \n47 \n25.86 \nS (7.05%) \n2.37 \n7.3 \n41 \n29.03 \n\n\n\nTable 3 .\n3Comparison with other distillation approaches ters can already reduce T 's model size to 78MB (27%) but it has less impact in the computation time as the LSTM has been implemented efficiently for NVIDIA cuDNN. With d rate = 92.95%, we reduce the computation time from 87ms to 41ms (2.12\u00d7), effectively doubling the frame rate.Method \nRMS RPE (t) RMS RPE (r) RMS ATE \nSupervised T \n0.1197 \n0.2377 \n26.7386 \nSupervised S \n0.1367 \n0.1627 \n71.7517 \nKD [16] \n0.1875 \n0.1439 \n165.2182 \nChen's OD [4] \n0.1197 \n0.1416 \n46.2320 \nFitNets [25] \n0.1450 \n0.1409 \n31.9624 \nOurs \n0.1053 \n0.1406 \n29.0294 \n\n\n\nTable 3\n3shows the result of this experiment. It can be seen that our proposed approach have better accuracy for20 m \n\n-4.4805 \n-4.48 \n-4.4795 \n-4.479 \n\n36.7208 \n\n36.721 \n\n36.7212 \n\n36.7214 \n\n36.7216 \n\n36.7218 \n\n36.722 \n\nGPS \nSupervised T \nKD \nChen OD \nFitNets \nOurs \n100 m \n\n-4.476 \n-4.475 \n-4.474 \n-4.473 \n-4.472 \n\n36.7155 \n\n36.716 \n\n36.7165 \n\n36.717 \n\n36.7175 \n\n36.718 \n\nGPS \nSupervised T \nKD \nChen OD \nFitNets \nOurs \n\n\n\nGanvo: Unsupervised deep monocular visual odometry and depth estimation with generative adversarial networks. Y Almalioglu, M R U Saputra, P P B Gusmao, A Markham, N Trigoni, IEEE International Conference on Robotics and Automation (ICRA). Y. Almalioglu, M. R. U. Saputra, P. P. B. d. Gusmao, A. Markham, and N. Trigoni. Ganvo: Unsupervised deep monocular visual odometry and depth estimation with gen- erative adversarial networks. In IEEE International Confer- ence on Robotics and Automation (ICRA), 2019. 2\n\nSparsification and Separation of Deep Learning Layers for Constrained Resource Inference on Wearables. S Bhattacharya, N D Lane, ACM Conference on Embedded Network Sensor Systems (SenSys). 1S. Bhattacharya and N. D. Lane. Sparsification and Sep- aration of Deep Learning Layers for Constrained Resource Inference on Wearables. In ACM Conference on Embedded Network Sensor Systems (SenSys), pages 176-189, 2016. 1, 2\n\nThe m\u00e1laga urban dataset: High-rate stereo and lidar in a realistic urban scenario. The International. J.-L Blanco-Claraco, F.-\u00c1 Moreno-Due\u00f1as, J Gonz\u00e1lez-Jim\u00e9nez, Journal of Robotics Research (IJRR). 332J.-L. Blanco-Claraco, F.-\u00c1. Moreno-Due\u00f1as, and J. Gonz\u00e1lez-Jim\u00e9nez. The m\u00e1laga urban dataset: High-rate stereo and lidar in a realistic urban scenario. The Interna- tional Journal of Robotics Research (IJRR), 33(2):207-214, 2014. 6\n\nLearning efficient object detection models with knowledge distillation. G Chen, W Choi, X Yu, T Han, M Chandraker, Advances in Neural Information Processing Systems (NIPS). G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker. Learn- ing efficient object detection models with knowledge distilla- tion. In Advances in Neural Information Processing Systems (NIPS), pages 742-751, 2017. 1, 3, 8\n\nModel compression and acceleration for deep neural networks: The principles, progress, and challenges. Y Cheng, D Wang, P Zhou, T Zhang, IEEE Signal Processing Magazine. 351Y. Cheng, D. Wang, P. Zhou, and T. Zhang. Model compres- sion and acceleration for deep neural networks: The prin- ciples, progress, and challenges. IEEE Signal Processing Magazine, 35(1):126-136, 2018. 1, 2\n\nBinaryConnect: Training Deep Neural Networks with binary weights during propagations. M Courbariaux, Y Bengio, J.-P David, Advances in Neural Information Processing Systems (NIPS). 1M. Courbariaux, Y. Bengio, and J.-P. David. BinaryConnect: Training Deep Neural Networks with binary weights during propagations. In Advances in Neural Information Processing Systems (NIPS), pages 1-9, 2015. 1, 2\n\nLearning Optical Flow with Convolutional Networks. A Dosovitskiy, P Fischery, E Ilg, P Hausser, C Hazirbas, V Golkov, P V D Smagt, D Cremers, T Brox, Flownet, 18-DeceIEEE International Conference on Computer Vision (ICCV). 11A. Dosovitskiy, P. Fischery, E. Ilg, P. Hausser, C. Hazir- bas, V. Golkov, P. V. D. Smagt, D. Cremers, and T. Brox. FlowNet: Learning Optical Flow with Convolutional Net- works. In IEEE International Conference on Computer Vi- sion (ICCV), volume 11-18-Dece, pages 2758-2766, 2016. 5\n\nAre we ready for autonomous driving? The KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)A. Geiger, P. Lenz, and R. Urtasun. Are we ready for au- tonomous driving? The KITTI vision benchmark suite. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 3354-3361, 2012. 6\n\nCompressing Deep Convolutional Networks using Vector Quantization. Y Gong, L Liu, M Yang, L Bourdev, arXiv:1412.6115Y. Gong, L. Liu, M. Yang, and L. Bourdev. Compressing Deep Convolutional Networks using Vector Quantization. In arXiv:1412.6115, 2015. 1\n\nDynamic Network Surgery for Efficient DNNs. Y Guo, A Yao, Y Chen, Advances in Neural Information Processing Systems (NIPS). 1Y. Guo, A. Yao, and Y. Chen. Dynamic Network Surgery for Efficient DNNs. In Advances in Neural Information Pro- cessing Systems (NIPS), 2016. 1, 2\n\nDeep learning with limited numerical precision. S Gupta, A Agrawal, K Gopalakrishnan, P Narayanan, International Conference on Machine Learning (ICML). S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan. Deep learning with limited numerical precision. In Inter- national Conference on Machine Learning (ICML), pages 1737-1746, 2015. 2\n\nDeep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. S Han, H Mao, W J Dally, International Conference on Learning Representations (ICLR). S. Han, H. Mao, and W. J. Dally. Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. In International Confer- ence on Learning Representations (ICLR), 2016. 1\n\nDSD : Dense-Sparse-Dense Training for Deep Neural Networks. S Han, H Mao, E Gong, S Tang, J Dally, International Conference on Learning Representations (ICLR. S. Han, H. Mao, E. Gong, S. Tang, and J. Dally. DSD : Dense-Sparse-Dense Training for Deep Neural Networks. In International Conference on Learning Representations (ICLR), 2017. 2\n\nLearning both Weights and Connections for Efficient Neural Networks. S Han, J Pool, J Tran, W J Dally, Advances in Neural Information Processing Systems (NIPS). S. Han, J. Pool, J. Tran, and W. J. Dally. Learning both Weights and Connections for Efficient Neural Networks. In Advances in Neural Information Processing Systems (NIPS), pages 1-9, 2015. 2\n\nMultiple View Geometry in Computer Vision. R Hartley, A Zisserman, Cambridge University Press242 editionR. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2 edition, 2004. 2, 4\n\nDistilling the Knowledge in a Neural Network. G Hinton, O Vinyals, J Dean, NIPS Deep Learning and Representation Learning Workshop. G. Hinton, O. Vinyals, and J. Dean. Distilling the Knowledge in a Neural Network. In NIPS Deep Learning and Represen- tation Learning Workshop (2015), pages 1-9, 2015. 1, 3, 4, 8\n\nBinarized Neural Networks. I Hubara, M Courbariaux, D Soudry, R El-Yaniv, Y Bengio, Advances in Neural Information Processing Systems (NIPS). 1I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized Neural Networks. In Advances in Neu- ral Information Processing Systems (NIPS), 2016. 1, 2\n\nSpeeding up convolutional neural networks with low rank expansions. M Jaderberg, A Vedaldi, A Zisserman, Proceedings of the British Machine Vision Conference (BMVC). the British Machine Vision Conference (BMVC)M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference (BMVC), 2014. 2\n\nSelf-supervised knowledge distillation using singular value decomposition. S H Lee, D H Kim, B C Song, European Conference on Computer Vision (ECCV). SpringerS. H. Lee, D. H. Kim, and B. C. Song. Self-supervised knowledge distillation using singular value decomposition. In European Conference on Computer Vision (ECCV), pages 339-354. Springer, 2018. 1\n\nUnifying distillation and privileged information. D Lopez-Paz, L Bottou, B Sch\u00f6lkopf, V Vapnik, International Conference on Learning Representations (ICLR). D. Lopez-Paz, L. Bottou, B. Sch\u00f6lkopf, and V. Vapnik. Uni- fying distillation and privileged information. In International Conference on Learning Representations (ICLR), 2016. 1\n\nThinet: A filter level pruning method for deep neural network compression. J.-H Luo, J Wu, W Lin, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)J.-H. Luo, J. Wu, and W. Lin. Thinet: A filter level prun- ing method for deep neural network compression. In Pro- ceedings of the IEEE International Conference on Computer Vision (ICCV), pages 5058-5066, 2017. 2\n\nVariational Dropout Sparsifies Deep Neural Networks. D Molchanov, A Ashukha, D Vetrov, International Conference on Machine Learning (ICML). D. Molchanov, A. Ashukha, and D. Vetrov. Variational Dropout Sparsifies Deep Neural Networks. In International Conference on Machine Learning (ICML), 2017. 2\n\nModel compression via distillation and quantization. A Polino, R Pascanu, D Alistarh, International Conference on Learning Representations (ICLR). A. Polino, R. Pascanu, and D. Alistarh. Model compression via distillation and quantization. In International Conference on Learning Representations (ICLR), 2018. 1\n\nLearning separable filters. R Rigamonti, A Sironi, V Lepetit, P Fua, Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR). the IEEE conference on Computer Vision and Pattern Recognition (CVPR)R. Rigamonti, A. Sironi, V. Lepetit, and P. Fua. Learning separable filters. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 2754-2761, 2013. 2\n\nFitNets: Hints for Thin Deep Nets. A Romero, N Ballas, S E Kahou, A Chassang, C Gatta, Y Bengio, ternational Conference on Learning Representations (ICLR). A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. FitNets: Hints for Thin Deep Nets. In In- ternational Conference on Learning Representations (ICLR), 2015. 1, 3, 4, 8\n\nLearning monocular visual odometry through geometry-aware curriculum learning. M R U Saputra, P P B Gusmao, S Wang, A Markham, N Trigoni, IEEE International Conference on Robotics and Automation (ICRA). M. R. U. Saputra, P. P. B. d. Gusmao, S. Wang, A. Markham, and N. Trigoni. Learning monocular visual odometry through geometry-aware curriculum learning. In IEEE In- ternational Conference on Robotics and Automation (ICRA), 2019. 2\n\nVisual SLAM and Structure from Motion in Dynamic Environments : A Survey. M R U Saputra, A Markham, N Trigoni, ACM Computing Surveys. 5124M. R. U. Saputra, A. Markham, and N. Trigoni. Visual SLAM and Structure from Motion in Dynamic Environments : A Survey. ACM Computing Surveys, 51(2), 2018. 2, 4\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, The Journal of Machine Learning Research. 151N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929-1958, 2014. 6\n\nA benchmark for the evaluation of rgb-d slam systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEJ. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre- mers. A benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 573-580. IEEE, 2012. 6\n\nConvolutional neural networks with low-rank regularization. C Tai, T Xiao, Y Zhang, X Wang, W E , International Conference on Learning Representations (ICLR). C. Tai, T. Xiao, Y. Zhang, X. Wang, and W. E. Convolu- tional neural networks with low-rank regularization. In In- ternational Conference on Learning Representations (ICLR), 2016. 1\n\nImproving the speed of neural networks on cpus. V Vanhoucke, A Senior, M Z Mao, Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop. Deep Learning and Unsupervised Feature Learning NIPS Workshoppage 4. CiteseerV. Vanhoucke, A. Senior, and M. Z. Mao. Improving the speed of neural networks on cpus. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, vol- ume 1, page 4. Citeseer, 2011. 2\n\nProgressive blockwise knowledge distillation for neural network acceleration. H Wang, H Zhao, X Li, X Tan, International Joint Conference on Artificial Intelligence (IJCAI). H. Wang, H. Zhao, X. Li, and X. Tan. Progressive block- wise knowledge distillation for neural network acceleration. In International Joint Conference on Artificial Intelligence (IJCAI), pages 2769-2775, 2018. 1\n\nDeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks. S Wang, R Clark, H Wen, N Trigoni, IEEE International Conference on Robotics and Automation (ICRA. 5S. Wang, R. Clark, H. Wen, and N. Trigoni. DeepVO: To- wards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks. In IEEE International Con- ference on Robotics and Automation (ICRA), 2017. 2, 5, 8\n\nEnd-to-end, sequence-to-sequence probabilistic visual odometry through deep neural networks. S Wang, R Clark, H Wen, N Trigoni, The International Journal of Robotics Research (IJRR). 25S. Wang, R. Clark, H. Wen, and N. Trigoni. End-to-end, sequence-to-sequence probabilistic visual odometry through deep neural networks. The International Journal of Robotics Research (IJRR), pages 1-30, 2018. 2, 5\n\nDeepIoT: Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework. S Yao, Y Zhao, A Zhang, L Su, T Abdelzaher, 15th ACM Conference on Embedded Networked Sensor Systems (SenSys). 17S. Yao, Y. Zhao, A. Zhang, L. Su, and T. Abdelzaher. DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems with a Compressor-Critic Framework. In 15th ACM Conference on Embedded Networked Sensor Sys- tems (SenSys), number 17, 2017. 1, 2\n\nA gift from knowledge distillation: Fast optimization, network minimization and transfer learning. J Yim, D Joo, J Bae, J Kim, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)J. Yim, D. Joo, J. Bae, and J. Kim. A gift from knowl- edge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 4133-4141, 2017. 1\n\nUnsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction. H Zhan, R Garg, C Weerasekera, K Li, H Agarwal, I Reid, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)H. Zhan, R. Garg, C. Saroj Weerasekera, K. Li, H. Agar- wal, and I. Reid. Unsupervised learning of monocular depth estimation and visual odometry with deep feature recon- struction. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 340- 349, 2018. 2\n\nDeeptam: Deep tracking and mapping. H Zhou, B Ummenhofer, T Brox, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)H. Zhou, B. Ummenhofer, and T. Brox. Deeptam: Deep tracking and mapping. In Proceedings of the European Con- ference on Computer Vision (ECCV), pages 822-838, 2018. 2\n", "annotations": {"author": "[{\"end\":129,\"start\":59},{\"end\":202,\"start\":130},{\"end\":273,\"start\":203},{\"end\":346,\"start\":274},{\"end\":414,\"start\":347},{\"end\":477,\"start\":415}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":67},{\"end\":147,\"start\":140},{\"end\":218,\"start\":209},{\"end\":291,\"start\":281},{\"end\":359,\"start\":352},{\"end\":422,\"start\":415}]", "author_first_name": "[{\"end\":64,\"start\":59},{\"end\":66,\"start\":65},{\"end\":135,\"start\":130},{\"end\":139,\"start\":136},{\"end\":208,\"start\":203},{\"end\":280,\"start\":274},{\"end\":351,\"start\":347}]", "author_affiliation": "[{\"end\":128,\"start\":76},{\"end\":201,\"start\":149},{\"end\":272,\"start\":220},{\"end\":345,\"start\":293},{\"end\":413,\"start\":361},{\"end\":476,\"start\":424}]", "title": "[{\"end\":56,\"start\":1},{\"end\":533,\"start\":478}]", "venue": null, "abstract": "[{\"end\":1585,\"start\":535}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2427,\"start\":2424},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2429,\"start\":2427},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2432,\"start\":2429},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2446,\"start\":2442},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2449,\"start\":2446},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2452,\"start\":2449},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2484,\"start\":2480},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2486,\"start\":2484},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2645,\"start\":2641},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2927,\"start\":2923},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3311,\"start\":3307},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3948,\"start\":3945},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3985,\"start\":3981},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3988,\"start\":3985},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3991,\"start\":3988},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3994,\"start\":3991},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3997,\"start\":3994},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4000,\"start\":3997},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4003,\"start\":4000},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4262,\"start\":4259},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4995,\"start\":4991},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4998,\"start\":4995},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5001,\"start\":4998},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5004,\"start\":5001},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5007,\"start\":5004},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5009,\"start\":5007},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5090,\"start\":5086},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5093,\"start\":5090},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6393,\"start\":6389},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6396,\"start\":6393},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6494,\"start\":6491},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6497,\"start\":6494},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6704,\"start\":6701},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6847,\"start\":6843},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6850,\"start\":6847},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6879,\"start\":6875},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6882,\"start\":6879},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6951,\"start\":6947},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6954,\"start\":6951},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6993,\"start\":6989},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7142,\"start\":7138},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7325,\"start\":7321},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7466,\"start\":7462},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7469,\"start\":7466},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7471,\"start\":7469},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9324,\"start\":9320},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9330,\"start\":9326},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9584,\"start\":9580},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11487,\"start\":11483},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12389,\"start\":12386},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":13207,\"start\":13204},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15003,\"start\":15000},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15785,\"start\":15781},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":15808,\"start\":15804},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16176,\"start\":16172},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17949,\"start\":17945},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17952,\"start\":17949},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19079,\"start\":19075},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19194,\"start\":19190},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19201,\"start\":19198},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19659,\"start\":19655},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20094,\"start\":20091},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20367,\"start\":20363},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22523,\"start\":22519},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22608,\"start\":22605},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22640,\"start\":22637},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23381,\"start\":23377},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26003,\"start\":26000},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26005,\"start\":26003},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26012,\"start\":26009},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26014,\"start\":26012},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26847,\"start\":26844},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26849,\"start\":26847},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26856,\"start\":26853},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26858,\"start\":26856},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26996,\"start\":26993},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26998,\"start\":26996},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27006,\"start\":27003},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":27008,\"start\":27006},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27218,\"start\":27215},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27220,\"start\":27218},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27227,\"start\":27224},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27229,\"start\":27227},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29572,\"start\":29568},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29586,\"start\":29582},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29630,\"start\":29627},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29643,\"start\":29639},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29771,\"start\":29767},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29891,\"start\":29888},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30483,\"start\":30479},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31513,\"start\":31509}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32532,\"start\":32258},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32684,\"start\":32533},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32905,\"start\":32685},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33026,\"start\":32906},{\"attributes\":{\"id\":\"fig_6\"},\"end\":33312,\"start\":33027},{\"attributes\":{\"id\":\"fig_8\"},\"end\":33417,\"start\":33313},{\"attributes\":{\"id\":\"fig_9\"},\"end\":33537,\"start\":33418},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33931,\"start\":33538},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34367,\"start\":33932},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":34971,\"start\":34368},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35395,\"start\":34972}]", "paragraph": "[{\"end\":2261,\"start\":1601},{\"end\":2928,\"start\":2263},{\"end\":3949,\"start\":2930},{\"end\":4814,\"start\":3951},{\"end\":5361,\"start\":4816},{\"end\":5638,\"start\":5363},{\"end\":5786,\"start\":5640},{\"end\":6037,\"start\":5788},{\"end\":6216,\"start\":6054},{\"end\":6705,\"start\":6218},{\"end\":7326,\"start\":6707},{\"end\":7940,\"start\":7328},{\"end\":9037,\"start\":7988},{\"end\":10012,\"start\":9083},{\"end\":11015,\"start\":10062},{\"end\":11525,\"start\":11068},{\"end\":11816,\"start\":11585},{\"end\":12446,\"start\":11818},{\"end\":12870,\"start\":12570},{\"end\":13079,\"start\":12918},{\"end\":13419,\"start\":13139},{\"end\":14017,\"start\":13421},{\"end\":15026,\"start\":14191},{\"end\":15390,\"start\":15094},{\"end\":16177,\"start\":15432},{\"end\":16459,\"start\":16179},{\"end\":17077,\"start\":16525},{\"end\":17616,\"start\":17147},{\"end\":17643,\"start\":17618},{\"end\":18746,\"start\":17680},{\"end\":19080,\"start\":18809},{\"end\":19613,\"start\":19082},{\"end\":21458,\"start\":19638},{\"end\":22203,\"start\":21479},{\"end\":23032,\"start\":22254},{\"end\":23581,\"start\":23044},{\"end\":25243,\"start\":23606},{\"end\":26860,\"start\":25245},{\"end\":27678,\"start\":26862},{\"end\":29237,\"start\":27741},{\"end\":31795,\"start\":29271},{\"end\":32257,\"start\":31810}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9082,\"start\":9038},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11067,\"start\":11016},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11584,\"start\":11526},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12569,\"start\":12447},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12917,\"start\":12871},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13138,\"start\":13080},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14146,\"start\":14018},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14190,\"start\":14146},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15093,\"start\":15027},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16524,\"start\":16460},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17146,\"start\":17078},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18808,\"start\":18747}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25852,\"start\":25845},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26839,\"start\":26832},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26888,\"start\":26881},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27210,\"start\":27203},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27662,\"start\":27655},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":28451,\"start\":28444}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1599,\"start\":1587},{\"attributes\":{\"n\":\"2.\"},\"end\":6052,\"start\":6040},{\"attributes\":{\"n\":\"3.\"},\"end\":7986,\"start\":7943},{\"attributes\":{\"n\":\"4.\"},\"end\":10060,\"start\":10015},{\"attributes\":{\"n\":\"5.\"},\"end\":15430,\"start\":15393},{\"attributes\":{\"n\":\"6.1.\"},\"end\":17678,\"start\":17646},{\"attributes\":{\"n\":\"6.2.\"},\"end\":19636,\"start\":19616},{\"attributes\":{\"n\":\"6.3.\"},\"end\":21477,\"start\":21461},{\"attributes\":{\"n\":\"7.\"},\"end\":22226,\"start\":22206},{\"attributes\":{\"n\":\"7.1.\"},\"end\":22252,\"start\":22229},{\"attributes\":{\"n\":\"7.2.\"},\"end\":23042,\"start\":23035},{\"attributes\":{\"n\":\"7.3.\"},\"end\":23604,\"start\":23584},{\"attributes\":{\"n\":\"7.4.\"},\"end\":27739,\"start\":27681},{\"attributes\":{\"n\":\"7.5.\"},\"end\":29269,\"start\":29240},{\"attributes\":{\"n\":\"8.\"},\"end\":31808,\"start\":31798},{\"end\":32269,\"start\":32259},{\"end\":32544,\"start\":32534},{\"end\":32687,\"start\":32686},{\"end\":32917,\"start\":32907},{\"end\":33048,\"start\":33028},{\"end\":33324,\"start\":33314},{\"end\":33429,\"start\":33419},{\"end\":33548,\"start\":33539},{\"end\":33942,\"start\":33933},{\"end\":34378,\"start\":34369},{\"end\":34980,\"start\":34973}]", "table": "[{\"end\":33931,\"start\":33550},{\"end\":34367,\"start\":34036},{\"end\":34971,\"start\":34706},{\"end\":35395,\"start\":35085}]", "figure_caption": "[{\"end\":32532,\"start\":32271},{\"end\":32684,\"start\":32546},{\"end\":32905,\"start\":32688},{\"end\":33026,\"start\":32919},{\"end\":33312,\"start\":33051},{\"end\":33417,\"start\":33326},{\"end\":33537,\"start\":33431},{\"end\":34036,\"start\":33944},{\"end\":34706,\"start\":34380},{\"end\":35085,\"start\":34982}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9025,\"start\":9015},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9673,\"start\":9663},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14671,\"start\":14661},{\"end\":19440,\"start\":19434},{\"end\":19503,\"start\":19495},{\"end\":19733,\"start\":19727},{\"end\":20375,\"start\":20369},{\"end\":21347,\"start\":21339},{\"end\":21636,\"start\":21630},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24304,\"start\":24294},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25010,\"start\":24999},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26366,\"start\":26360},{\"end\":27879,\"start\":27873},{\"end\":28137,\"start\":28130},{\"end\":28939,\"start\":28933},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":29234,\"start\":29228},{\"end\":30677,\"start\":30671},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":30882,\"start\":30876}]", "bib_author_first_name": "[{\"end\":35508,\"start\":35507},{\"end\":35522,\"start\":35521},{\"end\":35526,\"start\":35523},{\"end\":35537,\"start\":35536},{\"end\":35541,\"start\":35538},{\"end\":35551,\"start\":35550},{\"end\":35562,\"start\":35561},{\"end\":36013,\"start\":36012},{\"end\":36029,\"start\":36028},{\"end\":36031,\"start\":36030},{\"end\":36433,\"start\":36429},{\"end\":36454,\"start\":36450},{\"end\":36471,\"start\":36470},{\"end\":36836,\"start\":36835},{\"end\":36844,\"start\":36843},{\"end\":36852,\"start\":36851},{\"end\":36858,\"start\":36857},{\"end\":36865,\"start\":36864},{\"end\":37259,\"start\":37258},{\"end\":37268,\"start\":37267},{\"end\":37276,\"start\":37275},{\"end\":37284,\"start\":37283},{\"end\":37624,\"start\":37623},{\"end\":37639,\"start\":37638},{\"end\":37652,\"start\":37648},{\"end\":37985,\"start\":37984},{\"end\":38000,\"start\":37999},{\"end\":38012,\"start\":38011},{\"end\":38019,\"start\":38018},{\"end\":38030,\"start\":38029},{\"end\":38042,\"start\":38041},{\"end\":38052,\"start\":38051},{\"end\":38056,\"start\":38053},{\"end\":38065,\"start\":38064},{\"end\":38076,\"start\":38075},{\"end\":38515,\"start\":38514},{\"end\":38525,\"start\":38524},{\"end\":38533,\"start\":38532},{\"end\":39041,\"start\":39040},{\"end\":39049,\"start\":39048},{\"end\":39056,\"start\":39055},{\"end\":39064,\"start\":39063},{\"end\":39272,\"start\":39271},{\"end\":39279,\"start\":39278},{\"end\":39286,\"start\":39285},{\"end\":39549,\"start\":39548},{\"end\":39558,\"start\":39557},{\"end\":39569,\"start\":39568},{\"end\":39587,\"start\":39586},{\"end\":39950,\"start\":39949},{\"end\":39957,\"start\":39956},{\"end\":39964,\"start\":39963},{\"end\":39966,\"start\":39965},{\"end\":40310,\"start\":40309},{\"end\":40317,\"start\":40316},{\"end\":40324,\"start\":40323},{\"end\":40332,\"start\":40331},{\"end\":40340,\"start\":40339},{\"end\":40659,\"start\":40658},{\"end\":40666,\"start\":40665},{\"end\":40674,\"start\":40673},{\"end\":40682,\"start\":40681},{\"end\":40684,\"start\":40683},{\"end\":40987,\"start\":40986},{\"end\":40998,\"start\":40997},{\"end\":41217,\"start\":41216},{\"end\":41227,\"start\":41226},{\"end\":41238,\"start\":41237},{\"end\":41510,\"start\":41509},{\"end\":41520,\"start\":41519},{\"end\":41535,\"start\":41534},{\"end\":41545,\"start\":41544},{\"end\":41557,\"start\":41556},{\"end\":41862,\"start\":41861},{\"end\":41875,\"start\":41874},{\"end\":41886,\"start\":41885},{\"end\":42264,\"start\":42263},{\"end\":42266,\"start\":42265},{\"end\":42273,\"start\":42272},{\"end\":42275,\"start\":42274},{\"end\":42282,\"start\":42281},{\"end\":42284,\"start\":42283},{\"end\":42594,\"start\":42593},{\"end\":42607,\"start\":42606},{\"end\":42617,\"start\":42616},{\"end\":42630,\"start\":42629},{\"end\":42958,\"start\":42954},{\"end\":42965,\"start\":42964},{\"end\":42971,\"start\":42970},{\"end\":43380,\"start\":43379},{\"end\":43393,\"start\":43392},{\"end\":43404,\"start\":43403},{\"end\":43679,\"start\":43678},{\"end\":43689,\"start\":43688},{\"end\":43700,\"start\":43699},{\"end\":43967,\"start\":43966},{\"end\":43980,\"start\":43979},{\"end\":43990,\"start\":43989},{\"end\":44001,\"start\":44000},{\"end\":44390,\"start\":44389},{\"end\":44400,\"start\":44399},{\"end\":44410,\"start\":44409},{\"end\":44412,\"start\":44411},{\"end\":44421,\"start\":44420},{\"end\":44433,\"start\":44432},{\"end\":44442,\"start\":44441},{\"end\":44782,\"start\":44781},{\"end\":44786,\"start\":44783},{\"end\":44797,\"start\":44796},{\"end\":44801,\"start\":44798},{\"end\":44811,\"start\":44810},{\"end\":44819,\"start\":44818},{\"end\":44830,\"start\":44829},{\"end\":45213,\"start\":45212},{\"end\":45217,\"start\":45214},{\"end\":45228,\"start\":45227},{\"end\":45239,\"start\":45238},{\"end\":45506,\"start\":45505},{\"end\":45520,\"start\":45519},{\"end\":45530,\"start\":45529},{\"end\":45544,\"start\":45543},{\"end\":45557,\"start\":45556},{\"end\":45885,\"start\":45884},{\"end\":45894,\"start\":45893},{\"end\":45907,\"start\":45906},{\"end\":45917,\"start\":45916},{\"end\":45928,\"start\":45927},{\"end\":46317,\"start\":46316},{\"end\":46324,\"start\":46323},{\"end\":46332,\"start\":46331},{\"end\":46341,\"start\":46340},{\"end\":46349,\"start\":46348},{\"end\":46351,\"start\":46350},{\"end\":46647,\"start\":46646},{\"end\":46660,\"start\":46659},{\"end\":46670,\"start\":46669},{\"end\":46672,\"start\":46671},{\"end\":47102,\"start\":47101},{\"end\":47110,\"start\":47109},{\"end\":47118,\"start\":47117},{\"end\":47124,\"start\":47123},{\"end\":47505,\"start\":47504},{\"end\":47513,\"start\":47512},{\"end\":47522,\"start\":47521},{\"end\":47529,\"start\":47528},{\"end\":47922,\"start\":47921},{\"end\":47930,\"start\":47929},{\"end\":47939,\"start\":47938},{\"end\":47946,\"start\":47945},{\"end\":48337,\"start\":48336},{\"end\":48344,\"start\":48343},{\"end\":48352,\"start\":48351},{\"end\":48361,\"start\":48360},{\"end\":48367,\"start\":48366},{\"end\":48805,\"start\":48804},{\"end\":48812,\"start\":48811},{\"end\":48819,\"start\":48818},{\"end\":48826,\"start\":48825},{\"end\":49348,\"start\":49347},{\"end\":49356,\"start\":49355},{\"end\":49364,\"start\":49363},{\"end\":49379,\"start\":49378},{\"end\":49385,\"start\":49384},{\"end\":49396,\"start\":49395},{\"end\":49893,\"start\":49892},{\"end\":49901,\"start\":49900},{\"end\":49915,\"start\":49914}]", "bib_author_last_name": "[{\"end\":35519,\"start\":35509},{\"end\":35534,\"start\":35527},{\"end\":35548,\"start\":35542},{\"end\":35559,\"start\":35552},{\"end\":35570,\"start\":35563},{\"end\":36026,\"start\":36014},{\"end\":36036,\"start\":36032},{\"end\":36448,\"start\":36434},{\"end\":36468,\"start\":36455},{\"end\":36488,\"start\":36472},{\"end\":36841,\"start\":36837},{\"end\":36849,\"start\":36845},{\"end\":36855,\"start\":36853},{\"end\":36862,\"start\":36859},{\"end\":36876,\"start\":36866},{\"end\":37265,\"start\":37260},{\"end\":37273,\"start\":37269},{\"end\":37281,\"start\":37277},{\"end\":37290,\"start\":37285},{\"end\":37636,\"start\":37625},{\"end\":37646,\"start\":37640},{\"end\":37658,\"start\":37653},{\"end\":37997,\"start\":37986},{\"end\":38009,\"start\":38001},{\"end\":38016,\"start\":38013},{\"end\":38027,\"start\":38020},{\"end\":38039,\"start\":38031},{\"end\":38049,\"start\":38043},{\"end\":38062,\"start\":38057},{\"end\":38073,\"start\":38066},{\"end\":38081,\"start\":38077},{\"end\":38090,\"start\":38083},{\"end\":38522,\"start\":38516},{\"end\":38530,\"start\":38526},{\"end\":38541,\"start\":38534},{\"end\":39046,\"start\":39042},{\"end\":39053,\"start\":39050},{\"end\":39061,\"start\":39057},{\"end\":39072,\"start\":39065},{\"end\":39276,\"start\":39273},{\"end\":39283,\"start\":39280},{\"end\":39291,\"start\":39287},{\"end\":39555,\"start\":39550},{\"end\":39566,\"start\":39559},{\"end\":39584,\"start\":39570},{\"end\":39597,\"start\":39588},{\"end\":39954,\"start\":39951},{\"end\":39961,\"start\":39958},{\"end\":39972,\"start\":39967},{\"end\":40314,\"start\":40311},{\"end\":40321,\"start\":40318},{\"end\":40329,\"start\":40325},{\"end\":40337,\"start\":40333},{\"end\":40346,\"start\":40341},{\"end\":40663,\"start\":40660},{\"end\":40671,\"start\":40667},{\"end\":40679,\"start\":40675},{\"end\":40690,\"start\":40685},{\"end\":40995,\"start\":40988},{\"end\":41008,\"start\":40999},{\"end\":41224,\"start\":41218},{\"end\":41235,\"start\":41228},{\"end\":41243,\"start\":41239},{\"end\":41517,\"start\":41511},{\"end\":41532,\"start\":41521},{\"end\":41542,\"start\":41536},{\"end\":41554,\"start\":41546},{\"end\":41564,\"start\":41558},{\"end\":41872,\"start\":41863},{\"end\":41883,\"start\":41876},{\"end\":41896,\"start\":41887},{\"end\":42270,\"start\":42267},{\"end\":42279,\"start\":42276},{\"end\":42289,\"start\":42285},{\"end\":42604,\"start\":42595},{\"end\":42614,\"start\":42608},{\"end\":42627,\"start\":42618},{\"end\":42637,\"start\":42631},{\"end\":42962,\"start\":42959},{\"end\":42968,\"start\":42966},{\"end\":42975,\"start\":42972},{\"end\":43390,\"start\":43381},{\"end\":43401,\"start\":43394},{\"end\":43411,\"start\":43405},{\"end\":43686,\"start\":43680},{\"end\":43697,\"start\":43690},{\"end\":43709,\"start\":43701},{\"end\":43977,\"start\":43968},{\"end\":43987,\"start\":43981},{\"end\":43998,\"start\":43991},{\"end\":44005,\"start\":44002},{\"end\":44397,\"start\":44391},{\"end\":44407,\"start\":44401},{\"end\":44418,\"start\":44413},{\"end\":44430,\"start\":44422},{\"end\":44439,\"start\":44434},{\"end\":44449,\"start\":44443},{\"end\":44794,\"start\":44787},{\"end\":44808,\"start\":44802},{\"end\":44816,\"start\":44812},{\"end\":44827,\"start\":44820},{\"end\":44838,\"start\":44831},{\"end\":45225,\"start\":45218},{\"end\":45236,\"start\":45229},{\"end\":45247,\"start\":45240},{\"end\":45517,\"start\":45507},{\"end\":45527,\"start\":45521},{\"end\":45541,\"start\":45531},{\"end\":45554,\"start\":45545},{\"end\":45571,\"start\":45558},{\"end\":45891,\"start\":45886},{\"end\":45904,\"start\":45895},{\"end\":45914,\"start\":45908},{\"end\":45925,\"start\":45918},{\"end\":45936,\"start\":45929},{\"end\":46321,\"start\":46318},{\"end\":46329,\"start\":46325},{\"end\":46338,\"start\":46333},{\"end\":46346,\"start\":46342},{\"end\":46657,\"start\":46648},{\"end\":46667,\"start\":46661},{\"end\":46676,\"start\":46673},{\"end\":47107,\"start\":47103},{\"end\":47115,\"start\":47111},{\"end\":47121,\"start\":47119},{\"end\":47128,\"start\":47125},{\"end\":47510,\"start\":47506},{\"end\":47519,\"start\":47514},{\"end\":47526,\"start\":47523},{\"end\":47537,\"start\":47530},{\"end\":47927,\"start\":47923},{\"end\":47936,\"start\":47931},{\"end\":47943,\"start\":47940},{\"end\":47954,\"start\":47947},{\"end\":48341,\"start\":48338},{\"end\":48349,\"start\":48345},{\"end\":48358,\"start\":48353},{\"end\":48364,\"start\":48362},{\"end\":48378,\"start\":48368},{\"end\":48809,\"start\":48806},{\"end\":48816,\"start\":48813},{\"end\":48823,\"start\":48820},{\"end\":48830,\"start\":48827},{\"end\":49353,\"start\":49349},{\"end\":49361,\"start\":49357},{\"end\":49376,\"start\":49365},{\"end\":49382,\"start\":49380},{\"end\":49393,\"start\":49386},{\"end\":49401,\"start\":49397},{\"end\":49898,\"start\":49894},{\"end\":49912,\"start\":49902},{\"end\":49920,\"start\":49916}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52282758},\"end\":35907,\"start\":35397},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15762813},\"end\":36324,\"start\":35909},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16276115},\"end\":36761,\"start\":36326},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":29308926},\"end\":37153,\"start\":36763},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":32588614},\"end\":37535,\"start\":37155},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1518846},\"end\":37931,\"start\":37537},{\"attributes\":{\"doi\":\"18-Dece\",\"id\":\"b6\",\"matched_paper_id\":12552176},\"end\":38441,\"start\":37933},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6724907},\"end\":38971,\"start\":38443},{\"attributes\":{\"doi\":\"arXiv:1412.6115\",\"id\":\"b8\"},\"end\":39225,\"start\":38973},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":744803},\"end\":39498,\"start\":39227},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2547043},\"end\":39841,\"start\":39500},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2134321},\"end\":40247,\"start\":39843},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16800094},\"end\":40587,\"start\":40249},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2238772},\"end\":40941,\"start\":40589},{\"attributes\":{\"id\":\"b14\"},\"end\":41168,\"start\":40943},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7200347},\"end\":41480,\"start\":41170},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6453539},\"end\":41791,\"start\":41482},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17864746},\"end\":42186,\"start\":41793},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":49869692},\"end\":42541,\"start\":42188},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":8125776},\"end\":42877,\"start\":42543},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11169209},\"end\":43324,\"start\":42879},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":18201582},\"end\":43623,\"start\":43326},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3323727},\"end\":43936,\"start\":43625},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":430659},\"end\":44352,\"start\":43938},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2723173},\"end\":44700,\"start\":44354},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":85453160},\"end\":45136,\"start\":44702},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":3400843},\"end\":45436,\"start\":45138},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6844431},\"end\":45828,\"start\":45438},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206942855},\"end\":46254,\"start\":45830},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4167933},\"end\":46596,\"start\":46256},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15196840},\"end\":47021,\"start\":46598},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":51606880},\"end\":47408,\"start\":47023},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9114952},\"end\":47826,\"start\":47410},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":21689894},\"end\":48226,\"start\":47828},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":588549},\"end\":48703,\"start\":48228},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206596723},\"end\":49239,\"start\":48705},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4578162},\"end\":49854,\"start\":49241},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":51929808},\"end\":50203,\"start\":49856}]", "bib_title": "[{\"end\":35505,\"start\":35397},{\"end\":36010,\"start\":35909},{\"end\":36427,\"start\":36326},{\"end\":36833,\"start\":36763},{\"end\":37256,\"start\":37155},{\"end\":37621,\"start\":37537},{\"end\":37982,\"start\":37933},{\"end\":38512,\"start\":38443},{\"end\":39269,\"start\":39227},{\"end\":39546,\"start\":39500},{\"end\":39947,\"start\":39843},{\"end\":40307,\"start\":40249},{\"end\":40656,\"start\":40589},{\"end\":41214,\"start\":41170},{\"end\":41507,\"start\":41482},{\"end\":41859,\"start\":41793},{\"end\":42261,\"start\":42188},{\"end\":42591,\"start\":42543},{\"end\":42952,\"start\":42879},{\"end\":43377,\"start\":43326},{\"end\":43676,\"start\":43625},{\"end\":43964,\"start\":43938},{\"end\":44387,\"start\":44354},{\"end\":44779,\"start\":44702},{\"end\":45210,\"start\":45138},{\"end\":45503,\"start\":45438},{\"end\":45882,\"start\":45830},{\"end\":46314,\"start\":46256},{\"end\":46644,\"start\":46598},{\"end\":47099,\"start\":47023},{\"end\":47502,\"start\":47410},{\"end\":47919,\"start\":47828},{\"end\":48334,\"start\":48228},{\"end\":48802,\"start\":48705},{\"end\":49345,\"start\":49241},{\"end\":49890,\"start\":49856}]", "bib_author": "[{\"end\":35521,\"start\":35507},{\"end\":35536,\"start\":35521},{\"end\":35550,\"start\":35536},{\"end\":35561,\"start\":35550},{\"end\":35572,\"start\":35561},{\"end\":36028,\"start\":36012},{\"end\":36038,\"start\":36028},{\"end\":36450,\"start\":36429},{\"end\":36470,\"start\":36450},{\"end\":36490,\"start\":36470},{\"end\":36843,\"start\":36835},{\"end\":36851,\"start\":36843},{\"end\":36857,\"start\":36851},{\"end\":36864,\"start\":36857},{\"end\":36878,\"start\":36864},{\"end\":37267,\"start\":37258},{\"end\":37275,\"start\":37267},{\"end\":37283,\"start\":37275},{\"end\":37292,\"start\":37283},{\"end\":37638,\"start\":37623},{\"end\":37648,\"start\":37638},{\"end\":37660,\"start\":37648},{\"end\":37999,\"start\":37984},{\"end\":38011,\"start\":37999},{\"end\":38018,\"start\":38011},{\"end\":38029,\"start\":38018},{\"end\":38041,\"start\":38029},{\"end\":38051,\"start\":38041},{\"end\":38064,\"start\":38051},{\"end\":38075,\"start\":38064},{\"end\":38083,\"start\":38075},{\"end\":38092,\"start\":38083},{\"end\":38524,\"start\":38514},{\"end\":38532,\"start\":38524},{\"end\":38543,\"start\":38532},{\"end\":39048,\"start\":39040},{\"end\":39055,\"start\":39048},{\"end\":39063,\"start\":39055},{\"end\":39074,\"start\":39063},{\"end\":39278,\"start\":39271},{\"end\":39285,\"start\":39278},{\"end\":39293,\"start\":39285},{\"end\":39557,\"start\":39548},{\"end\":39568,\"start\":39557},{\"end\":39586,\"start\":39568},{\"end\":39599,\"start\":39586},{\"end\":39956,\"start\":39949},{\"end\":39963,\"start\":39956},{\"end\":39974,\"start\":39963},{\"end\":40316,\"start\":40309},{\"end\":40323,\"start\":40316},{\"end\":40331,\"start\":40323},{\"end\":40339,\"start\":40331},{\"end\":40348,\"start\":40339},{\"end\":40665,\"start\":40658},{\"end\":40673,\"start\":40665},{\"end\":40681,\"start\":40673},{\"end\":40692,\"start\":40681},{\"end\":40997,\"start\":40986},{\"end\":41010,\"start\":40997},{\"end\":41226,\"start\":41216},{\"end\":41237,\"start\":41226},{\"end\":41245,\"start\":41237},{\"end\":41519,\"start\":41509},{\"end\":41534,\"start\":41519},{\"end\":41544,\"start\":41534},{\"end\":41556,\"start\":41544},{\"end\":41566,\"start\":41556},{\"end\":41874,\"start\":41861},{\"end\":41885,\"start\":41874},{\"end\":41898,\"start\":41885},{\"end\":42272,\"start\":42263},{\"end\":42281,\"start\":42272},{\"end\":42291,\"start\":42281},{\"end\":42606,\"start\":42593},{\"end\":42616,\"start\":42606},{\"end\":42629,\"start\":42616},{\"end\":42639,\"start\":42629},{\"end\":42964,\"start\":42954},{\"end\":42970,\"start\":42964},{\"end\":42977,\"start\":42970},{\"end\":43392,\"start\":43379},{\"end\":43403,\"start\":43392},{\"end\":43413,\"start\":43403},{\"end\":43688,\"start\":43678},{\"end\":43699,\"start\":43688},{\"end\":43711,\"start\":43699},{\"end\":43979,\"start\":43966},{\"end\":43989,\"start\":43979},{\"end\":44000,\"start\":43989},{\"end\":44007,\"start\":44000},{\"end\":44399,\"start\":44389},{\"end\":44409,\"start\":44399},{\"end\":44420,\"start\":44409},{\"end\":44432,\"start\":44420},{\"end\":44441,\"start\":44432},{\"end\":44451,\"start\":44441},{\"end\":44796,\"start\":44781},{\"end\":44810,\"start\":44796},{\"end\":44818,\"start\":44810},{\"end\":44829,\"start\":44818},{\"end\":44840,\"start\":44829},{\"end\":45227,\"start\":45212},{\"end\":45238,\"start\":45227},{\"end\":45249,\"start\":45238},{\"end\":45519,\"start\":45505},{\"end\":45529,\"start\":45519},{\"end\":45543,\"start\":45529},{\"end\":45556,\"start\":45543},{\"end\":45573,\"start\":45556},{\"end\":45893,\"start\":45884},{\"end\":45906,\"start\":45893},{\"end\":45916,\"start\":45906},{\"end\":45927,\"start\":45916},{\"end\":45938,\"start\":45927},{\"end\":46323,\"start\":46316},{\"end\":46331,\"start\":46323},{\"end\":46340,\"start\":46331},{\"end\":46348,\"start\":46340},{\"end\":46354,\"start\":46348},{\"end\":46659,\"start\":46646},{\"end\":46669,\"start\":46659},{\"end\":46678,\"start\":46669},{\"end\":47109,\"start\":47101},{\"end\":47117,\"start\":47109},{\"end\":47123,\"start\":47117},{\"end\":47130,\"start\":47123},{\"end\":47512,\"start\":47504},{\"end\":47521,\"start\":47512},{\"end\":47528,\"start\":47521},{\"end\":47539,\"start\":47528},{\"end\":47929,\"start\":47921},{\"end\":47938,\"start\":47929},{\"end\":47945,\"start\":47938},{\"end\":47956,\"start\":47945},{\"end\":48343,\"start\":48336},{\"end\":48351,\"start\":48343},{\"end\":48360,\"start\":48351},{\"end\":48366,\"start\":48360},{\"end\":48380,\"start\":48366},{\"end\":48811,\"start\":48804},{\"end\":48818,\"start\":48811},{\"end\":48825,\"start\":48818},{\"end\":48832,\"start\":48825},{\"end\":49355,\"start\":49347},{\"end\":49363,\"start\":49355},{\"end\":49378,\"start\":49363},{\"end\":49384,\"start\":49378},{\"end\":49395,\"start\":49384},{\"end\":49403,\"start\":49395},{\"end\":49900,\"start\":49892},{\"end\":49914,\"start\":49900},{\"end\":49922,\"start\":49914}]", "bib_venue": "[{\"end\":35635,\"start\":35572},{\"end\":36096,\"start\":36038},{\"end\":36525,\"start\":36490},{\"end\":36934,\"start\":36878},{\"end\":37323,\"start\":37292},{\"end\":37716,\"start\":37660},{\"end\":38154,\"start\":38099},{\"end\":38644,\"start\":38543},{\"end\":39038,\"start\":38973},{\"end\":39349,\"start\":39293},{\"end\":39650,\"start\":39599},{\"end\":40033,\"start\":39974},{\"end\":40406,\"start\":40348},{\"end\":40748,\"start\":40692},{\"end\":40984,\"start\":40943},{\"end\":41300,\"start\":41245},{\"end\":41622,\"start\":41566},{\"end\":41957,\"start\":41898},{\"end\":42336,\"start\":42291},{\"end\":42698,\"start\":42639},{\"end\":43051,\"start\":42977},{\"end\":43464,\"start\":43413},{\"end\":43770,\"start\":43711},{\"end\":44091,\"start\":44007},{\"end\":44508,\"start\":44451},{\"end\":44903,\"start\":44840},{\"end\":45270,\"start\":45249},{\"end\":45613,\"start\":45573},{\"end\":46017,\"start\":45938},{\"end\":46413,\"start\":46354},{\"end\":46745,\"start\":46678},{\"end\":47195,\"start\":47130},{\"end\":47601,\"start\":47539},{\"end\":48009,\"start\":47956},{\"end\":48445,\"start\":48380},{\"end\":48916,\"start\":48832},{\"end\":49487,\"start\":49403},{\"end\":49986,\"start\":49922},{\"end\":38732,\"start\":38646},{\"end\":42003,\"start\":41959},{\"end\":43112,\"start\":43053},{\"end\":44162,\"start\":44093},{\"end\":46808,\"start\":46747},{\"end\":48987,\"start\":48918},{\"end\":49558,\"start\":49489},{\"end\":50037,\"start\":49988}]"}}}, "year": 2023, "month": 12, "day": 17}