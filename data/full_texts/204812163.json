{"id": 204812163, "updated": "2023-04-05 02:25:39.83", "metadata": {"title": "Nexus: a GPU cluster engine for accelerating DNN-based video analysis", "authors": "[{\"first\":\"Haichen\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Lequn\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yuchen\",\"last\":\"Jin\",\"middle\":[]},{\"first\":\"Liangyu\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Bingyu\",\"last\":\"Kong\",\"middle\":[]},{\"first\":\"Matthai\",\"last\":\"Philipose\",\"middle\":[]},{\"first\":\"Arvind\",\"last\":\"Krishnamurthy\",\"middle\":[]},{\"first\":\"Ravi\",\"last\":\"Sundaram\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 27th ACM Symposium on Operating Systems Principles", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "We address the problem of serving Deep Neural Networks (DNNs) efficiently from a cluster of GPUs. In order to realize the promise of very low-cost processing made by accelerators such as GPUs, it is essential to run them at sustained high utilization. Doing so requires cluster-scale resource management that performs detailed scheduling of GPUs, reasoning about groups of DNN invocations that need to be co-scheduled, and moving from the conventional whole-DNN execution model to executing fragments of DNNs. Nexus is a fully implemented system that includes these innovations. In large-scale case studies on 16 GPUs, when required to stay within latency constraints at least 99% of the time, Nexus can process requests at rates 1.8-12.7X higher than state of the art systems can. A long-running multi-application deployment stays within 84% of optimal utilization and, on a 100-GPU cluster, violates latency SLOs on 0.27% of requests.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2982157693", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/sosp/ShenCJZKPKS19", "doi": "10.1145/3341301.3359658"}}, "content": {"source": {"pdf_hash": "2febafc70b47a917805faaf130c7fb2cac8cf66b", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3341301.3359658", "status": "BRONZE"}}, "grobid": {"id": "48405ecbef4a24cfec2a6c8cbed08ddb08e447c4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2febafc70b47a917805faaf130c7fb2cac8cf66b.txt", "contents": "\nNexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis\nACMCopyright ACM2019. October 27-30, 2019\n\nHaichen Shen \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nWeb Services \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nLequn Chen \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nYuchen Jin \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nLiangyu Zhao \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nBingyu Kong \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nShanghai Jiao \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nTong University \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nMatthai Philipose \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nArvind Krishnamurthy \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nRavi Sundaram \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nHaichen Shen \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nLequn Chen \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nYuchen Jin \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nLiangyu Zhao \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nBingyu Kong \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nMatthai Philipose \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nArvind Krishnamurthy \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nRavi Sun \nUniversity of Washington\nUniversity of Washington\nUniversity of Washington\nMicrosoft Research\nUniversity of Washington\nNortheastern University\n\n\nNexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis\n\nSOSP '19: Symposium on Operating Systems Principles\nHuntsville, ON, Canada; New York, NY, USAACM162019. October 27-30, 201910.1145/3341301.3359658\nWe address the problem of serving Deep Neural Networks (DNNs) efficiently from a cluster of GPUs. In order to realize the promise of very low-cost processing made by accelerators such as GPUs, it is essential to run them at sustained high utilization. Doing so requires cluster-scale resource management that performs detailed scheduling of GPUs, reasoning about groups of DNN invocations that need to be coscheduled, and moving from the conventional whole-DNN execution model to executing fragments of DNNs. Nexus is a fully implemented system that includes these innovations. In large-scale case studies on 16 GPUs, when required to stay within latency constraints at least 99% of the time, Nexus can process requests at rates 1.8-12.7\u00d7 higher than state of the art systems can. A long-running multi-application deployment stays within 84% of optimal utilization and, on a 100-GPU cluster, violates latency SLOs on 0.27% of requests.ACM Reference Format:\n\nworkload are Deep Neural Networks (DNNs), which are networks of dense linear algebra computations. Specialized hardware accelerators for DNNs, in the form of Graphic Processing Units (GPUs, which this paper focuses on) and even more specialized Tensor Processing Units (TPUs) have emerged in the recent past. GPU accelerators process DNNs orders of magnitude faster and cheaper than CPUs in many cases. However, GPUs are expensive and very-high-capacity: modern devices each provide over 100 TFLOPS. Cost-savings from using them depends critically on operating them at sustained high utilization. A fundamental problem, therefore, is to distribute the large incoming workload onto a cluster of accelerators at high accelerator utilization and acceptable latency. We address this problem in this paper.\n\nConceptually, this problem can be thought of as sharding inputs via a distributed frontend onto DNNs on backend GPUs. Several interacting factors complicate this viewpoint. First, given the size of GPUs, it is often necessary to place different types of networks on the same GPU. It is then important to select and schedule them so as to maximize their combined throughput while satisfying latency bounds. Second, many applications consist of groups of DNNs that feed into each other. It is important to be able to specify these groups and to schedule the execution of the entire group on the cluster so as to maximize performance. Third, it is well known that dense linear algebra computations such as DNNs execute much more efficiently when their inputs are batched together. Batching complicates scheduling and routing because (a) it benefits from cross-tenant and crossrequest coordination and (b) it forces the underlying binpacking-based scheduling algorithms to incorporate batch size. Fourth, the increasingly common use of transfer learning in today's workloads has led to specialization of networks, where two tasks that formerly used identical networks now use networks that are only mostly identical. Since batching works only when multiple inputs are applied to the same model in conventional DNN execution systems, the benefits of batching are lost.\n\nNexus is a GPU cluster for DNN execution that addresses these problems to attain high execution throughput under latency Service Level Objectives (SLOs). It uses three main techniques to do so. First, it relies on a novel batching-aware scheduler (Section 6.1) that performs bin packing when the balls being packed into bins have variable size, depending on the size of the batch they are in. This schedule specifies the GPUs needed, the distribution of DNNs across them, and the order of their execution so as to maximize execution throughput while staying within latency bounds. Second, it allows groups of related DNN invocations to be written as queries and provides automated query optimization to assign optimal batch sizes to the components of the query so as to maximize overall execution throughput of the query while staying within its latency bounds (Section 6.2). Finally, Nexus breaks from orthodoxy and allows batching of parts of networks with different batch sizes. This enables the batched execution of specialized networks (Section 6.3). Nexus is completely implemented as a containerized system deployable on a commercial cloud and comprises of roughly 10k lines of C++. We have deployed Nexus on a 100-GPU cluster. On focused 16-GPU experiments compared with existing DNN serving systems (Tensorflow Serving [25] and Clipper [6]), we measure the maximal request rate processed by these systems on fixed applications such that at least 99% of requests are handled within latency SLOs. By this metric, Nexus is able to handle 1.8-4.4\u00d7 more requests on a traffic monitoring application, and 9.4-12.7\u00d7 on a gamestream analysis case study. On a much larger experiment on an 100-GPU cluster, 7 applications and 12 different models, Nexus achieves a maximal request rate of over 98.7% while maintaining similar high throughputs.\n\n\nBackground\n\nA vision-based application aggregates visual information from one or more video streams using custom \"business\" logic. Each stream is processed using a pipeline similar to that in Figure 1. CPU-based code, either on the edge or in the cloud, selects frames from the stream for processing, applies business logic to identify what parts (or windows) of the image need deeper analysis, applies a DNN query to these windows, and aggregates the results in an applicationspecific way, often writing to a database. A query may represent a single DNN applied to the window, but often it may represent a sequence of dependent DNN applications, e.g., running an object detector on the window and running a car make/model detector on all sub-windows containing cars. Typically, a stream is sampled a few times a second or minute, and the DNN query should complete execution in tens to hundreds of milliseconds (for \"live\" applications) or within several hours for (\"batch\" applications). The execution of DNNs dominates the computation pipeline, and the cost of executing them dominates the cost of the vision service. Nexus provides a standalone service that implements the DNN-based analysis stage for vision pipelines.\n\n\nAccelerators and the challenge of utilizing them\n\nAs Table 1 shows, a key to minimizing the cost of executing DNNs is the use of specialized accelerators such as GPUs and TPUs, which are highly optimized to execute the dense linear algebra computations that comprise DNN models. The table shows the execution latency and the dollar cost of 1000 invocations for a few common models on CPUs and GPUs. Execution times on CPUs can be orders of magnitude slower than that on GPUs. For many applications, therefore, latency constraints alone may dictate GPU-accelerated execution.\n\nPerhaps more fundamentally, GPUs and TPUs promise much lower cost per operation than even highly accelerated CPUs: Table 1 lower-bounds the cost of executing a model by assuming that models can be executed at peak speed on each platform. Even compared to state of the art CPUs, accelerators can yield a cost advantage of up to 9\u00d7 (for TPUs) and 34\u00d7 (for GPUs). On the other hand, accelerators have extremely high computational capacities (e.g., 125 TFLOPS for the NVIDIA V100). To realize their cost savings, it is critical to sustain high utilization of this capacity. Sustaining high utilization is hard, however. For instance, the LeNet model of Table 1 consumes 20 MOPs to run, implying that a single V100 would require 125 TFLOPS \u00f7 20 MOPs = 6.25M inputs/second to run at full utilization! No single stream, or even most applications, can yield such rates. By aggregating inputs across streams and applications, Nexus is designed to funnel adequate work to each accelerator. However, as we discuss next, having \"enough\" work is 1 Per-device prices for 1000 invocations assuming peak execution rates on on-demand instances of AWS c5.large (Intel AVX 512), p2.xlarge (NVIDIA K80), p3.2xlarge (NVIDIA V100) and GCP Cloud TPU. not sufficient to achieve high utilization: it is important to group the right type of work in the right place.\n\n\nPlacing, packing and batching DNNs\n\nDNNs are networks of dense linear algebra operations (e.g., matrix multiplication and convolution), called layers or kernels. Networks are also called models. By default, the GPU simply executes the kernels presented to it in the order received. The kernels themselves are often computationally intensive, requiring MFLOPs to GFLOPs to execute, and range in size from one MB to hundreds of MBs. These facts have important implications for GPU utilization.\n\nFirst, loading models into memory can cost hundreds of milliseconds to seconds. When serving DNNs at high volume, therefore, it is usually essential to place the DNN on a particular GPU by pre-loading it on to GPU memory and then re-using it across many subsequent invocations. Placement brings with it the traditional problems of efficient packing. Which models should be co-located on each GPU, and how should they be scheduled to minimize mutual interference? Second, it is well known that the processor utilization achieved by kernels depends critically upon batching, i.e., grouping input matrices into higher-dimensional ones before applying custom \"batched\" implementations of the kernels. Intuitively, batching allows kernels to avoid stalling on memory accesses by operating on each loaded input many more times than without batching. On an NVIDIA GTX1080, batching improves the throughput of model execution by 4.7-13.3\u00d7 for batch sizes of 32 for VGG, ResNet, and Inception models relative to executing them individually. Further, our empirical measurements indicate that we can often use a linear model to fit the batched execution latency as follows:\nbatch_lat(b) = \u03b1b + \u03b2,(1)\nwhere \u03b2 is the fixed cost to invoke a model and \u03b1 is the cost of each additional task in the batch. Large batches amortize the fixed cost \u03b2 and help achieve higher throughputs.\n\nAlthough batching is critical for utilization, it complicates the resource allocation and scheduling decisions made inside of a cluster. We elaborate on these issues in Section 4. Further, batching is conventionally only feasible when the same model is invoked with different inputs. For instance, we expect many applications to use the same well-known, generally applicable, models (e.g., Resnet50 for object recognition). However, the generality of these models comes at the price of higher resource use. It has become common practice [12,24] to use smaller models specialized (using \"transfer learning\") to the few objects, faces, etc. relevant to an application by altering (\"re-training\") just the output layers of the models. Since such customization destroys the uniformity required by conventional batching, making specialized models play well with batching is often critical to efficiency.\n\n\nRelated work\n\nThe projects most closely related to Nexus are Clipper [6] and Tensorflow Serving [25]. Clipper is a \"prediction serving system\" that serves a variety of machine learning models including DNNs, on CPUs and GPUs. Given a request to serve a machine learning task, Clipper selects the type of model to serve it, batches requests, and forwards the batched requests to a backend container. By batching requests, and adapting batch sizes online under a latency SLO, Clipper takes a significant step toward Nexus's goal of maximizing serving throughput under latency constraints. Clipper also provides approximation and caching services, complementary to Nexus's focus on executing all requests exactly but efficiently. Tensorflow Serving can be viewed as a variant of Clipper that does not provide approximation and caching, but also has additional machinery for versioning models.\n\nTo the basic batched-execution architecture of Clipper, Nexus builds along the dimensions of scale, expressivity and granularity. These techniques address the challenges brought up earlier in this section and thus reflect Nexus's focus on executing DNNs on GPUs at high efficiency and scale. Scale: Nexus provides the machinery to scale serving to large, changing workloads. In particular, it automates the allocation of GPU resources and the placement and scheduling of models across allocated resources. It provides a distributed frontend that scales with requests. These functions are performed on a continuing basis to adapt to workloads. Expressivity: Nexus provides a query mechanism that (a) allows related DNN execution tasks to be specified jointly, and (b) allows the user to specify the latency SLO just at the whole-query level. Nexus then analyzes the query and allocates latency bounds and batch sizes to constituent DNN tasks so as to maximize the throughput of the whole query. Granularity: Where Clipper limits the granularity of batched execution to whole models, Nexus automatically identifies common subgraphs of models and executes them in a batch. This is critical for batching on specialized models, which often share all but the output layer, as described previously.\n\nOther work has explored system-level optimization for DNN inference. They address goals other than Nexus's focus on improving accelerator utilization via aggressive batching and are broadly complementary. MCDNN [14] is an early example of a system that exploited shared model prefixes resulting from specialization on mobile CPUs. However, like Mainstream [19] after it, which provided a more sophisticated infrastructure for managing specialization on server CPUs, it focused on shared prefixes that applied to a common input, with the goal of avoiding redundant computation across prefixes. Nexus, on the other hand, focuses on shared prefixes that operate on different inputs with the goal of utilizing the underlying GPU hardware more efficiently via batching.\n\nA common theme in these and other systems is to automatically select approximately equivalent, but more performant, models for a task. MCDNN [14] introduced the notion of selecting from a catalog of variants of models that trade off accuracy for performance. VideoStorm [42] generalized the notion of exploiting such tradeoffs to all parameters of the computer vision system, not just those related to DNNs. NoScope [20] proposed specializing models to queries to speed up query execution. More recent work [35] proposes cascading small dynamically specialized models that handle the common case with larger backup models. Focus [17] applies cascading to the video indexing scenario, backing up a small, frequent ingest-time indexing DNN with a large but infrequent query-time DNN. Nexus focuses on providing an engine for non-approximating execution of the incoming models, a facility that can potentially be used as a backend for these approximation-based systems.\n\nServing DNNs at scale is similar to other large-scale shorttask serving problems. These systems have distributed front ends that dispatch low-latency tasks to queues on the backend servers. Sparrow [27] focuses on dispatch strategies to reduce the delays associated with queuing in such systems. Slicer [3] provides a fast, fault-tolerant service for dividing the back end into shards and load balancing across them. Both systems assume that the backend server allocation and task placement is performed at a higher (application) level, using cluster resource managers such as Mesos [16] or Omega [33]. Nexus shares the philosophy of these systems of having a fast data plane that dispatches incoming messages from the frontend to backend GPUs and a slower control plane that performs more heavyweight scheduling tasks, such as resource allocation, packing and load balancing. On the other hand, compared to these generic systems, Nexus provides query processing, task allocation, and subtask scheduling functionality that is targeted to better batching over DNN-based workloads.\n\nMuch work has focused on producing faster models often at small losses in accuracy [2,31,41] Further, models of varying accuracy can be combined to maintain high accuracy and performance [6,14,17,20,35]. Nexus views the optimization, selection, and combination of models as best done by the application, and provides no special support for these functions. Our mechanisms are also orthogonal to the scheduling, placement, and time-sharing mechanisms in training systems [13,30,38] since DNN serving has to be performed within tight latency SLOs while maintaining high utilization.\n\n\nScheduling problems in batched execution\n\nBatched execution of models improve GPU utilization but also raises many challenges in determining how cluster resources are allocated to different applications and how to batch model invocations without violating latency constraints.  Fundamentally, the algorithm for packing models on GPUs needs to take into account the fact that the processing cost of an input is \"squishy\", i.e., it varies with the size of the batch within which that input is processed. Further, the latency of execution also depends on the batch size. This new version of bin packed scheduling, which we dub squishy bin packing, needs to reason explicitly about batching. Second, batching complicates query processing. If a certain latency SLO (Service Level Objective) is allocated to the query as a whole, the system needs to partition the latency across the DNN invocations that comprise the query so that each latency split allows efficient batched execution of the related DNN invocation. We call this complex query scheduling. Third, in addition to batching-aware resource allocation, the runtime dispatch engine also has to determine what requests are batched and what requests are dropped during periods of bursty arrival. We now use examples and measurements to elaborate on these underlying scheduling and resource allocation challenges.\n\n\nSquishy bin packing\n\nConsider a workload that consists of three different types of tasks that invoke different DNN models. Let the desired latency SLOs for tasks invoking models A, B, and C be 200ms, 250ms, and 250ms, respectively. Table 2 provides the batch execution latency and throughput at different batch sizes (i.e., the \"batching profile\") for each model.\n\nWe first explore the basic scenario where all three types of tasks are associated with high request rates so that multiple GPUs are required to handle each task type. To maximize GPU efficiency, we need to choose the largest possible batch size while still meeting the latency SLO. Note that the batch execution cost for a given task type cannot exceed half of the task's latency SLO; a task that missed being scheduled with a batch would be executed as part of the next batch, and thus its latency would be twice the batch execution cost. For example, the latency SLO for Model A tasks is 200 ms, so the maximum batch size we can use is 16. Therefore, the maximum throughput that Model A can achieve on a single GPU is 160 reqs/sec, and the number of GPUs to be allocated for Model A should r A /160, where r A is the observed request rate. Similarly, the number of GPUs for models B and C should be r B /128 and r C /128, where r B and r C are the request rates for models B and C respectively. Figure 2(a) depicts the desired schedules for the different models. We next consider a situation where the request rates for the models are lower, with each one requiring less than a GPU. In this case, the scheduler needs to consolidate multiple types of DNN tasks onto the same GPU to optimize resource utilization. Consider a workload where Model A receives 64 reqs/sec, and Model B and Model C receive 32 reqs/sec each. We consider schedules where multiple models are assigned to a GPU. The GPU then executes batches of different types of models in a round-robin manner, and it cycles through them over a time period that we refer to as the duty cycle. The worst-case latency for a task is no longer twice the batch execution cost but rather the sum of the duty cycle and the batch execution cost for that task type.\n\nGiven this setup, Model A tasks can be scheduled in batches of 8 as part of a duty cycle of 125ms; note that the resulting throughput is the desired rate of 64 reqs/sec, the batch execution cost for 8 tasks is 75ms, and the worst-case execution delay of 200ms matches the latency SLO (see Figure 2(b)). We then check whether the GPU has sufficient slack to accommodate tasks associated with models B or C. Within a duty cycle of 125ms, we would need to execute 4 tasks of either B or C to meet the desired rate of 32 reqs/sec. The batch execution cost of 4 model B tasks is 50ms, which can fit into the residual slack in the duty cycle. On the other hand, a batch of 4 model C tasks would incur 60ms and cannot be scheduled inside the duty cycle. Further, the worst-case latency for model B is the sum of the duty cycle and its own batch execution cost, 175ms(= 125 + 50), which is lower than its latency SLO 250ms. Thus, it is possible to co-locate Model B, but not Model C, on the same GPU as Model A.\n\nWe now make a few observations regarding the scenario discussed above and why the associated optimization problem cannot be addressed directly by known scheduling algorithms. First, unlike vanilla bin-packing that would pack fixed-size balls into bins, the tasks here incur lower costs when multiple tasks of the same type are squished together into a GPU. Second, in addition to the capacity constraints associated with the GPU's compute and/or memory capabilities, there are also latency constraints in generating a valid schedule. Third, there are many degrees of freedom in generating a valid schedule. The batch size associated with a model execution is not only a function of the request rate but also of the duty cycle in which the batch is embedded. In Section 6.1, we describe how to extend traditional algorithms to handle this setting.\n\n\nComplex query scheduling\n\nApplications often comprise of dependent computations of multiple DNN models. For example, a common pattern is a detection and recognition pipeline that first detects certain objects from the image and then recognizes each object. The developer will specify a latency SLO for the entire query, but since the system would host and execute the constituent models on different nodes, it would have to derive latency SLOs for the invoked models and the schedules that meet these latency SLOs. We discussed the latter issue in the previous example, and we now focus on the former issue.\n\nConsider a query that executes Model X and feeds its output to Model Y. Suppose we have a 100ms latency budget for processing this query, and suppose that every invocation of X yields \u03b3 outputs (on average). When \u03b3 < 1, X operates as a filter; when \u03b3 = 1, X maps an input to an output; when \u03b3 > 1, X yields multiple outputs from an input (e.g., detection of objects within a frame).\n\nAssume that Figure 3 depicts the batch execution latency and throughput of models X and Y. The system has to decide what latency SLOs it has to enforce on each model such that the overall latency is within 100ms and the GPU utilization of the query as a whole is maximized. For this example, we consider a limited set of latency split plans for models X and Y: (a) 40ms and 60ms, (b) 50ms and 50ms, (c) 60ms and 40ms. It would appear that plan (a) should work best since the sum of the throughputs is largest among the three plans, but a closer examination reveals some interesting details.\n\nFor workloads involving a large number of requests, let us assume that p and q GPUs execute X and Y, respectively. We then have \u03b3 \u00b7 p \u00b7 T X = q \u00b7 T Y , where T X and T Y are per-GPU throughputs of X and Y, such that the pipeline won't be bottlenecked by any model. We define the average throughput as the pipeline throughput divided by the total number of GPUs, which is p \u00b7 T X /(p + q). We evaluate the average throughputs for the three latency split plans with \u03b3 = 0.1, 1, 10. Figure 4 shows that each of the plans achieves the best performance for different \u03b3 values. In fact, there is no universal best split: it depends on \u03b3 , which can vary over time.\n\nWe note two observations from this example. First latency split for complex query impacts overall efficiency, and it is necessary to account both batch performance and workload statistics to make the best decision. Second, latency split should not be static but rather adapted over time in accordance with the current workload. Section 6.2 describes how Nexus automatically and continually derives latency splits.\n\n\nRate control and adaptive batching\n\nModel serving systems need to perform adaptive batching based on the number of requests received. When there is a burst of requests, the system needs to drop certain requests in order to serve the remaining requests within the latency    SLO. One approach is to perform lazy dropping, i.e., drop a request only when it has already missed its deadline, and determine the batch size based on the time budget remaining for the earliest request in the queue (as in Clipper [6]). We use simulation to evaluate this approach for different batching profiles (as modeled by Equation 1). We fix latency SLO to 100ms and optimal model throughput on a single GPU to 500 reqs/s, and vary \u03b1. Given the fixed throughput, the fixed cost of \u03b2 reduces as we increase \u03b1. The workload is generated using uniform and Poisson arrivals with the mean request rate set to 90% of the optimal throughput. We define the bad rate to be the percentage of requests that exceed the deadline or get dropped. Figure 5 shows that the lazy dropping strategy performs poorly for Poisson distributions when \u03b1 is small and \u03b2 is correspondingly high. Since the system always attempts to execute the oldest received request, it often has to resort to a small batch size in order to meet the deadline, but this causes the dispatcher to fall behind further given the high fixed cost is not amortized over sufficient requests. This experiment indicates that even the runtime needs to consider batch efficiency in determining what tasks to dispatch.\n\n\nNexus architecture\n\nThe primary goal of Nexus is to attain high execution efficiency on GPU clusters while serving video analysis requests within a specified latency SLO. Our service model assumes that the system can drop requests if it cannot execute them within their deadlines (following prior work [5]). Note that this is appropriate for video stream analysis, as the next sampled frame would typically be processed even if the earlier one is dropped. We also note that we could configure our system to simply delay the execution of requests that miss their deadlines to a later time and at a lower priority; many of our techniques would still be effective in improving efficiency in such a setting.\n\nNexus works on three planes (as depicted by Figure 6). The management plane allows developers to ingest and deploy applications and models, at a timescale of hours to weeks. The control plane, via the global scheduler, is responsible for resource allocation and scheduling at a typical timescale of seconds to minutes. The data plane, comprised of in-application Nexus library instances and backend modules (together, the Nexus runtime), dispatches and executes user requests at the timescale of milliseconds to seconds. The global scheduler interacts with the underlying cluster resource manager (e.g., Mesos [16], Azure Scale Sets [23]) to acquire CPUs/GPUs for the frontend/backend. A load balancer (not shown) from the underlying cluster spreads user requests across Nexus's distributed frontend. We sketch the three planes. Management plane: Models are stored in a model database and may be accompanied by either a sample data set or a batching profile. Nexus uses the sample dataset, if available, to derive a batching profile. A profiler measures the execution latency and memory use for different batch sizes when the models are uploaded to Nexus. Applications are containers that provide the Nexus library to client programs. Developers store these application containers in cluster-provided container repositories and may instruct Nexus to ingest a container, at which point it is loaded from the repository onto a frontend CPU. Control plane: The global scheduler is a cluster-wide resource manager that uses load statistics from the runtime. It uses this profile to add or remove frontend and backend nodes from the cluster, invokes the epoch scheduler to decide which models to execute and at what batch size, and which backend to place the models on so as to balance the load and maximize utilization. Multiple models may be mapped onto a single backend, albeit with an execution schedule that ensures they do not interfere as in Section 4.1. The mapping from models to backends is captured in a routing table that is sent to frontends. The matching execution schedule for each backend is captured in a schedule that is sent to backends. On receiving a routing table, frontends update their current  routing table. On receiving a schedule, backends load appropriate models into GPU memory and set their execution schedule.\n\nAllocation, scheduling, and routing updates happen at the granularity of an epoch, typically 30-60s, although a new epoch can also be triggered by large changes in workload. To prevent oscillation from frequent reconfiguration, we limit the minimum period between two epochs to 10 seconds. Epoch scheduling involves the following:\n\n\u2022 Produce an updated split of the latency SLO for the individual models inside a query (see Section 6.2). \u2022 Combine two or more models that share a prefix and latency SLO into a new prefix-batched model (see Section 6.3). \u2022 Perform profile-guided squishy bin packing to allocate the GPU resources for each model. (see Section 6.1).\n\nBecause epoch scheduling reacts to workload change at the granularity of an epoch at best, Nexus relies on admission control that drops excessive requests to make sure that most (targeted for 99% in the evaluation) requests can be served within their latency constraints within an epoch. Requests are dropped using an early-drop policy (see Adaptive Batching in Section 6.3). Data plane: When a user request comes into (a replica of) an application container, the application invokes DNNs via the Nexus library API. The library consults the local routing table to find a suitable backend for that model, dispatches the request to the backend, and delivers responses back to the application. The application is responsible for packaging and delivering the end-result of the query to the user. A backend module uses multiple threads to queue requests from various frontends, selects and executes models on these inputs in a batched mode according to the current schedule, and sends back the results to frontends. It can utilize one or more GPUs on a given node, with each GPU managed by a GPU scheduler that schedules tasks on it.\n\n\nBatch-aware scheduling and dispatch\n\nWe now describe the algorithms used by the global scheduler and the node dispatcher. First, we consider the case of scheduling streams of individual DNN task requests, given their expected arrival rates and latency SLOs. We next consider how to schedule streams of more complex queries/jobs that invoke multiple DNN tasks. We then describe how the node runtime cycles through DNNs and performs batching.\n\n\nScheduling streams of individual DNN tasks\n\nWe build upon the discussion presented in Section 4.1. The scheduler identifies for each cluster node the models hosted by it. As discussed earlier, the scheduling problem has the Notation Description\nS i Session i M k i DNN model k i for session S i L i Latency constraint for session S i R i Request rate for session S i \u2113 k i (b)\nExecution cost for M k i and batch size b Table 3: Notation structure of bin-packing [21], but we need to address the \"squishiness\" of tasks and the need to meet latency SLOs. Inputs: The scheduler is provided the request rate for a model at a given latency SLO. We refer to the requests for a given model and latency SLO as a session. Note that a session would correspond to classification requests from different users and possibly different applications that invoke the model with a given latency constraint. Table 3 describes the notation used below. Formally, a session S i specifies a model M k i and a latency constraint L i , and there is a request rate of R i associated with it. The scheduler is also provided with the batching profiles of different models. The latency of executing b invocations of M k i is \u2113 k i (b). We assume that throughput is non-decreasing with batch size b.\n\nScheduling overview: The scheduler allocates one or more sessions to each GPU and specifies their target batch sizes. Each GPU node n is then expected to cycle through the sessions allocated to it, execute invocations of each model in batched mode, and complete one entire cycle of batched executions within a duty cycle of d n . For sessions that have a sufficient number of user requests, one or more GPU nodes are allocated to a single session. The integer programming formulation and a greedy approximation algorithm described below computes the residual workload for such sessions (after allocating an integral number of GPUs) and then attempts to perform bin packing with the remaining smaller sessions. Scheduling Large Sessions: For session S i , we first compute the peak throughput of M k i when executed in isolation on a GPU. With a batch size b, the worst case latency for any given request is 2\u2113 k i (b), as we explained in Section 4.1. Denote batch size B i as the maximum value for b that meets the constraint 2\u2113 k i (b) \u2264 L i . Therefore, the maximal throughput, denoted by T i , for session S i on a single GPU is\nB i /\u2113 k i (B i ).\nThe number of GPU nodes we allocate to execute just S i requests is n = \u230aR i /T i \u230b. There will be a residual unallocated load for session S i after taking into account this allocated load. Note that n = 0 for sessions that don't have sufficient requests to utilize an entire GPU. (Function ScheduleSaturate in Algorithm 1 provides the pseudocode.) Optimization problem for scheduling residual workload: We next consider the problem of scheduling the residual loads, i.e., a workload where none of the models have sufficient load to need an entire GPU. The optimization problem can be expressed as an integer programming problem.\n\n\nDecision Variables Definition\n\u0434 j \u2208 {0, 1} Whether GPU j is in use s i j \u2208 {0, 1}\nWhether session i is assigned to GPU j b i j \u2208 R \u22650\n\nBatch size of session i on GPU j\n\nWe need to minimize: j \u0434 j , while subject to:\ns i j = 1 \u2192 \u0434 j = 1 \u2200j (a) j s i j = 1 \u2200i (b) s i j = 0 \u2192 b i j = 0 \u2200i, j (c) s i j = 1 \u2192 b i j \u2265 1 \u2200i, j (d) d j = i:t i j =1 \u2113 k i (b i j ) \u2200j (e) d j + \u2113 k i (b i j ) \u2264 L i \u2200i, j (s i j = 1) (f) b i j \u2265 r i d j \u2200i, j (s i j = 1) (g)\nThe constraints correspond to the following requirements. Note that some of the constraints are not linear, and we omit details on how to express them in a strictly linear way. We used the CPLEX package to solve this formulation on benchmark workloads. Even after incorporating optimizations, such as using a greedy algorithm to provide both an initial feasible solution and an upper bound for the number of GPUs needed, solving the integer program is expensive. For example, computing the minimum number of GPUs for 25 sessions takes several hours, even though the upper bound, determined via a greedy algorithm, is 8 GPUs. Further this optimization problem is proven to be strongly NP-hard (see Appendix A). We, therefore, resort to the following greedy scheduling algorithm. Greedy scheduling algorithm for residual loads: For the bin packing process, the scheduler inspects each residual session in isolation and computes the largest batch size and the corresponding duty cycle in order to meet the throughput and SLO needs. The intuition behind choosing the largest batch size is to have an initial schedule wherein the GPU operates at the highest efficiency. This initial schedule, however, is not cost-effective as it assumes that each GPU is running just one session within its duty cycle, so the algorithm then attempts to merge multiple sessions within a GPU's duty cycle. In doing so, it should not violate the latency SLOs, so we require that the merging process only reduces the duty cycle of the combined allocation. The algorithm considers sessions in decreasing order of associated work and merges \nS i = \u27e8M k i , L i , R i \u27e9 in Sessions do 6: B i \u2190 argmax b (2\u2113 k i (b) \u2264 L i ) 7: T i \u2190 B i /\u2113 k i (B i ) 8: let R i = n \u00b7 T i + r i 9:\nnodes \u2190 nodes \u2295 n GPU nodes for M k i with batch B i 10:\n\nresidue_loads \u2190 residue_loads \u2295\u27e8M k i , L i , r i \u27e9 11: return nodes, residue_loads ScheduleResidue(residue_loads) 12: for for n = \u27e8b, d, occ\u27e9 in nodes do 22: n \u2032 \u2190 MergeNodes(n, \u27e8b i , d i , occ i \u27e9) 23: if n \u2032 NULL and n \u2032 .occ > max_occ then 24: max_occ \u2190 n \u2032 .occ 25: max_node \u2190 n \u2032\n\u27e8M k i , L i , r i \u27e9 in residue_loads do 13: b i \u2190 argmax b (\u2113 k i (b) + b/r i \u2264 L i ) 14: d i \u2190 b i /r i 15: occ i \u2190 \u2113 k i (b i )/d i\n\n26:\n\nif max_node NULL then 27: replace max_node for its original node in nodes 28: else 29: nodes \u2190 nodes \u2295\u27e8b i , d i , occ i \u27e9 30: return nodes them into existing duty cycles that have the highest allocations, thus following the design principle behind the best-fit decreasing technique for traditional bin packing.\n\nWe now elaborate on this greedy scheduling algorithm (which is also depicted in function ScheduleResidue of Algorithm 1). Denote r i to be the request rate of a residual load. Suppose we execute the residual load with batch size b, the duty cycle d for gathering b inputs is b/r i . Then, the worst case latency is d + \u2113 k i (b). Therefore, we have the constraint:\nd + \u2113 k i (b) = b/r i + \u2113 k i (b) \u2264 L i(2)\nWe begin residual load scheduling (Line 12-15) by choosing for session S i the maximum batch size b i that satisfies the above constraint. The corresponding duty cycle d i is also at its maximal value. Denote occupancy (occ) as the fraction of the duty cycle d i occupied by S i 's residual load invocations:\nocc i (b i ) = \u2113 k i (b i )/d i .\nNext, we start to merge these fractional GPU nodes into fewer nodes (Line 16-30 in Algorithm 1). This part resembles the classic bin packing algorithm that first sorts sessions by decreasing occupancy and merges two nodes into a single node by best fit. The primary difference is how to determine whether two nodes can be merged such that their sessions won't violate the latency SLOs. Figure 7 depicts the process of merging two nodes. Suppose we have two sessions S 1 and S 2 on separate nodes, with request rates r 1 and r 2 , assigned batch sizes b 1 and b 2 , and duty cycles d 1 and d 2 . We use d = min(d 1 , d 2 ) as the new duty cycle of a combined node. Without loss of generality, we assume d = d 2 . We then use b \u2032 1 = d \u00b7 r 1 \u2264 b 1 as the new batch size for S 1 . Note that the worst case latency of requests in S 1 now becomes d + \u2113 k 1 (b \u2032 1 ) \u2264 d 1 + \u2113 k 1 (b 1 ) \u2264 L i , and we won't violate the latency constraint for S 1 by this adjustment. If \u2113 k 1 (b \u2032 1 ) + \u2113 k 2 (b 2 ) \u2264 d and memory capacity permits, a single node can handle the computation of both S 1 and S 2 , and we allocate these two sessions to the same node. While the above discussion considers merging two sessions, the underlying principle generalizes to nodes containing multiple sessions.\n\nNote that this algorithm does not assume the linear relationship between execution latency and batch size mentioned in Equation 1. The algorithm only assumes that the latency per input, \u2113(b)/b, is non-decreasing with batch size b.\n\nFinally, we extend the algorithm to be incremental across epochs, thus minimizing the movement of models across nodes. If the overall workload decreases, the scheduler attempts to move sessions from the least utilized backends to other backends. If a backend no longer executes any session, the scheduler releases the backend. If workload increases such that a backend becomes overloaded, we evict the cheapest sessions on this backend until it is no longer overloaded and perform bin packing again to relocate these evicted sessions.\n\n\nScheduling complex queries\n\nWe now present the query analysis algorithm that operates on dataflow representations of application queries in order to determine the latency SLO splits for the constituent models. The output of this analysis is given as input to the scheduling algorithm of Section 6.1 that works with individual models.\n\nQuery analysis extracts the dataflow dependency graph between model invocations in application code. For example, Figure 8 depicts a traffic analysis application that first uses the SSD model to detect objects and recognizes cars and faces correspondingly. We formulate the scheduling of queries as the following optimization problem. Suppose the query involves a set of models M i with request rate R i , and the end-to-end latency SLO is L. The objective is to find the best latency SLO split L i for each model M i to minimize the total number of GPUs that are required for the query. Because latency L i is determined by batch size b i , the optimization problem is equivalent to finding the best batch sizes that minimizes GPU count, while meeting the latency SLO along every path from the root model (M root ) to the leaf models.\nminimize {b v } v R v l v (b v )/b v subject to u:M root M v l u (b u ) \u2264 L \u2200v \u2208 leaf\nWe use dynamic programming to solve this optimization problem for the case of fork-join dependency graphs, but limit our exposition to the simpler case of tree-like dependency graphs. For example, Figure 8 can be treated as a tree-structured dependency graph models (we can as the output does not invoke additional DNN models. Denote f (u, t) as the minimum number of GPUs required to run models represented by u and the subtree at u within the time budget t. For a non-leaf node u, the algorithm allocates a time budget k for node u and at most t \u2212 k for the rest of the subtree, and it then enumerates all k \u2264 t to find the optimal split. More formally,\nf (u, t) = min k :k \u2264t min b:l u (b)\u2264k R u l u (b) b + min t \u2032 :t \u2032 \u2264t \u2212k v:M u \u2192M v f (v, t \u2032 )\nSince the dynamic programming cannot handle continuous state space, we approximate the state space of time budget with L/\u03b5 pieces of segments, where \u03b5 is the length of a segment. The time complexity is quadratic in L/\u03b5.\n\n\nBatch-aware dispatch\n\nWe now briefly describe the runtime mechanisms that control the execution of DNN tasks on backend nodes.\n\nOverlapping CPU and GPU computation: DNN tasks can be decomposed into three stages: pre-processing (including decoding and batching images), forwarding, and postprocessing. Pre-and post-processing are usually done on the CPU since they are not compute-intensive, whereas forwarding of a neural network model runs on the GPU. In order to achieve maximum GPU utilization, it is necessary to overlap the CPU computation and GPU computation. Therefore, the Nexus backend uses a thread pool of workers that preprocess the requests for the next batch and post-process the outputs of the previous batch on CPU, while another thread is dedicated to launching batched executions on the GPU. We observe experimentally that it usually takes 4 to 5 CPU cores to saturate GPU throughput, depending on the amount of computation in the models. Nexus uses an event-driven approach [28] to handling I/O, pre-and post-processing. GPU Multiplexing: DNN frameworks provide no specific support for the concurrent execution of multiple models. For example, if two models that share a GPU execute in two processes or containers, they will independently issue requests to execute layers/kernels to the underlying GPU. The GPU runtime will typically serve these requests in first-come-firstserved fashion, resulting in an arbitrary interleaving of the operations for the two models. The interleaving increases the execution latency of both models and makes it hard to predict the latency. Instead, the Nexus node runtime manages the execution of all models on a GPU, so it is able to pick batch sizes and execution schedules for all models in a round-robin fashion to make sure models abide by their latency SLOs. In addition, Nexus overlaps the pre-and postprocessing in CPU with the GPU execution to increase GPU utilization. Prefix Batching: Another important observation is that transfer learning [8,26,34,36,40] adapts a model from one dataset to another or from one task to another by re-training only the last few layers. DNN frameworks assume that if models differ in any layer, they cannot be executed in a batched fashion at all. However, in the common setting of model specialization, several models may differ only by their output layer. Batching the execution of all but the output layer can yield substantial batching gains. Nexus computes the hash of every sub-tree of the model schema and compares it with the existing models in the database to identify common sub-trees when a model is uploaded. At runtime, models with known common sub-trees are loaded partially in the backend and batched at the sub-tree (or prefix) granularity. The different suffix parts are then executed sequentially. Adaptive Batching: As discussed in Section 4.3, lazy dropping during dispatch could lead to small batch sizes and low efficiency. In Nexus, we use an early drop policy that skips over requests that would cause sub-optimal batching. Specifically, the dispatcher scans through the queue using a sliding window whose length is the batch size determined by the global scheduler for a given session. It stops at the first request that has enough budget for batched execution latency of the entire window and drops all earlier requests. We use simulation to compare the lazy drop and early drop policy. Similar to Figure 5, we fix latency SLO to 100ms and optimal throughput to 500 reqs/s. Figure 9 depicts the throughput achieved by lazy drop and early drop policy under different \u03b1 when 99% of requests are served within latency SLO. The results show that early drop can achieve up to 25% higher throughput than lazy drop.\n\n\nEvaluation\n\nWe implemented Nexus in roughly 10k lines of C++ code. Nexus supports the execution of models trained by various frameworks including Caffe [18], Caffe2 [9], Tensorflow (TF) [1], and Darknet [32]. Nexus can be deployed in a cluster using Docker Swarm [7] (used below) or Kubernetes [11]. In our evaluation, we use this implementation to answer the following questions. (1) Does using Nexus result in better cluster utilization while meeting SLOs with respect to existing systems? (2) Does high performance persist when Nexus is used at a large scale? (3) How do the new techniques in Nexus contribute to its performance? (4) What determines how well each of these techniques work? For a given workload and cluster, we refer to the maximum rate of queries that Nexus can process such that 99% of them are served within their latency SLOs as its throughput. We use throughput as the primary measure of cluster utilization.\n\n\nWorkload\n\nOur basic approach is to run Nexus (and its various configurations and competitors) on either a small (16-GPU) or a large (100-GPU) cluster on various mixes of the applications and input videos specified in Table 4. These applications are modeled closely on widely-known video analysis scenarios, but we implemented each of them since we are unaware of freely available, widely used versions. They encompass a wide variety of characteristics. Some (e.g., game and traffic, which implements Figure 8) are based on 24/7 live video streams, whereas others (e.g., dance and logo) apply to footage of individual performances. Some require simple queries (e.g.,  Table 4: Evaluated application and input data. Squishy scheduling, early drop, complex query analysis and prefixbatching are abbreviated as SS, ED, QA and PB. QA-k indicates that the related complex query has k stages. Models for detection, recognition and tracking are abbreviated 'det.', 'rec.' and 'trk.' game, designated \"QA-1\" has 1 stage), and others more complex ones (e.g., the 5-stage logo, designated \"QA-5\", seeks to detect people, find their torsos, look for logos, and if found, detect and recognize the player's number). Most use multiple specialized versions of models and are therefore amenable to prefix batching, designated \"PA\". For each workload, we have collected several hours and many streams (for live streams) or files (for episodes) of video, which sample and play in a loop to preserve temporal characteristics while allowing arbitrarily long simulations. Unless otherwise mentioned, we sample inter-arrival time between frames uniformly.\n\n\nUsing Clipper and Tensorflow as baselines\n\nClipper and TF Serving assume cluster scheduling and latency SLOs for DNN invocations are handled externally. Careful scheduling and latency allocation are two of Nexus's core contributions. To provide a basis for comparison, we furnish simple default versions of each. A batch-oblivious scheduler 2 greedily allocates to each model/SLO a share of the cluster proportional to its request rate and inversely proportional to its maximum single-node throughput. Further, we split the latency for a query evenly across its stages. The oblivious scheduler may map multiple models onto a Clipper GPU, in which case we launch one container per model on the GPU. We rely on Clipper's load balancer to manage 2 Note that we retain Clipper's adaptive batching, which is orthogonal to the scheduling scheme used, in all experiments. Adaptive batching groups requests into batches on a single backend node, adapting the batch size dynamically to get higher throughput. Scheduling, on the other hand, works at cluster-scale over a coarse epoch granularity and performs resource allocation. In particular, it determines how many replicas to use for each model, and which GPUs to place them on. Clipper assumes an external scheduler, so we had to provide a batch-oblivious scheduler as a reasonable baseline. model replicas. In contrast, TF does not provide a frontend load balancer, nor does it allow the specification of latency SLOs per request. We, therefore, provide a dispatcher and pick the maximum batch size for each model, so its SLO is not violated.\n\n\nSingle-application case studies\n\nTo compare our performance with those of Clipper and TF Serving, we ran the game and traffic applications separately on a 16-GPU cluster. In each case, we ran an ablation study on Nexus features to gauge their impact.\n\n\nGame analysis\n\nWhen analyzing game streams, we seek to recognize six numbers (e.g., number of kills, number of players alive) and one object icon on each frame. We use versions of LeNet [22] specialized to the game's font and the number of digits to recognize numbers, and ResNet-50 [15] with its last layer specialized to recognize the icon. We include 20 games in the case study, and consider a latency SLO of 50ms (sensitivity to SLOs is analyzed in Section 7.5). The request rates of frames from the 20 games follow the Zipf-0.9 distribution. We noticed that both Clipper and TF show extremely poor throughput on the tiny LeNet model. We conjecture, but could not confirm, that this is because of inadequate parallelism between CPU and GPU processing. To be maximally fair to them, we allow the two baselines to invoke just the ResNet model. Their resulting throughput, which we report, is better by over 4\u00d7 than including LeNet. Finally, we additively turn off prefix batching (PB), squishy scheduling (SS), early drop (ED), and overlapped processing in the CPU and GPU (OL, see Section 6.3). The game query has only 1 stage and, therefore, does not exercise query analysis (QA). Figure 10 shows the results. Nexus increases throughput significantly, by 9.4 and 12.7\u00d7 relative to Clipper and TF Serving on this application. Several of Nexus's techniques contribute, with OL the most (incrementally disabling OL results in an additional 7.4\u00d7 throughput reduction), and ED the least (disabling it results in a 3% reduction). Note that even though OL is the dominant technique for this application, the other Nexus techniques together result in a 48% fall in throughput (from 4120 to 2143 req/s) when disabled. ED is designed to address variability in requests. When we re-ran the experiments with frame inter-arrival rates sampled from a Poisson distribution as opposed to uniform, the significance of ED increases, and throughput drops by a more significant 8.5%.\n\nDisabling OL causes a dramatic reduction in throughput for this application because of a complex interaction between its tight (50ms) SLO, relatively high preprocessing times (roughly 10ms), and the low forwarding times of the models (roughly 6ms for ResNet-50 and under 0.1ms for LeNet). Serializing preprocessing with GPU execution for each batch results in roughly half the cycles of the GPU remaining idle. Further, given the tight SLO and the latency of preprocessing, batch sizes need to remain close to 1 to guarantee timely execution. These small batch sizes have many times lower throughput than the optimal large batches. Taken together, these factors result in a large throughput loss, making OL critical in the tight-SLO/small model regime. As the analysis for the traffic monitoring application ( Figure 11) below shows, with more relaxed SLOs and larger models, the importance of overlapped preprocessing (OL) is diminished.\n\n\nTraffic monitoring\n\ntraffic uses SSD [4], VGG-Face [29] and GoogleNet-car [39] for object detection, face recognition and car make/model analysis on 20 long-running traffic streams with a latency SLO of 400ms. These models are larger than those used in the game analysis case: the largest (SSD), which is invoked on every frame, runs for 47ms at batchsize 1 and the smallest (Googlenet) runs for 4.2ms, compared to 6.2ms/0.1ms for gaming.\n\nOur first experiment replicates the cumulative ablation test that we performed on gaming analysis in the previous section. Figure 11 shows the results for analyzing nonrush-hour traffic data. 3 Once again, maximum throughput comes from many Nexus techniques working together. All techniques other than OL, when disabled together, result in a signficant 39% performance drop relative to full Nexus (throughput falls from 534 to 326 req/s). Further, the contribution of ED is small.\n\nOn the other hand, the relative contribution of OL is 3 See the analysis below comparing rush-hour with non-rush-hour results. much smaller in this application: disabling OL incrementally results in throughput falling only by a further 34% (from 326 to 216 req/s), as opposed to the previous 7.4\u00d7 drop. Through more detailed analysis, we confirm that the more relaxed SLO enables large-batch execution, which together with the large model forwarding time renders the preprocessing time relatively small (so less of the GPU stays idle) and allows high throughput due to batch execution.\n\nUnlike game, traffic is a two-stage application: the first stage detects vehicles or people, and the second stage recognizes their make/model or identity. Query Analysis (QA) is thus applicable. Instead of splitting latency evenly (which is the baseline, represented by -QA the figure), QA allocates 345ms of the 400ms latency to object detection via SSD. Disabling QA results in a significant throughput loss, with throughput falling by 19% (from 534 to 433 req/s).\n\nAn interesting aspect of the traffic application is that the nature of data analysis depends strongly on whether the traffic being analyzed is rush-hour. Figure 12 summarizes throughput achieved on non-rush-hour vs rush-hour traffic. Three points are worth noting. First, throughput achieved during rush hour is significantly less than that during nonrush hour. This is because rush-hour traffic is more complex: more vehicles are detected, and require follow-on analysis, on every frame. Second, although during rush hour, the benefit of Nexus relative to the TF Serving baseline falls (from 534/227 = 2.4\u00d7 during non-rush hour to 264/146 = 1.8\u00d7 during rush hour) 4 , the benefit provided by Nexus is still significant. In particular, the relative benefit of the QA technique also falls. This seems to be because various subsystems are over-subscribed during rush hour.\n\n\nLong-running multi-application deployment\n\nTo check whether Nexus maintains its gains when run at large scale, especially in the face of significant workload variation across multiple applications, we deployed Nexus on a cluster ranging from 16 to 100 GPUs on a commercial cloud, running all applications from Table 4 simultaneously for a period of several hours. We focused on two metrics. First, how close to optimal is GPU utilization during this period? Second, how well does Nexus react to workload changes?\n\nWe first study the optimality of Nexus by using the uniform distribution to generate highly controlled workloads for the applications and perform the evaluation on a cluster of 16 GTX 1080Ti GPUs. To bound the optimal (smallest) number of GPUs needed for a session, we assumed that its model is fully (not just prefix) batchable, that its SLO allows it to run at the optimal batch size, and that it has enough requests coming in to be scheduled back-to-back on GPUs. Of course, real sessions often violate one or more of these assumptions and will have lower throughput. Nexus achieved a bad rate of less than 1% consistently and used 11.7 GPUs on average. We then computed the theoretical lower bound of the number of GPUs required based on the maximal throughput that a model can achieve on a single GPU. The lower bound for this workload is 9.8 GPUs on average. It indicates that Nexus scheduler can achieve 84% of GPU efficiency compared to the theoretical lower bound.\n\nNext, we deploy Nexus on 100 K80s and evaluate workloads with varying Poisson arrival rates. In particular, we fixed the number of model sessions, designated by a given model and its latency SLO, per application (e.g., game had 50 model sessions, traffic had 20), but varied the request rate per session by varying the rate at which each submitted frames. Figure 13 shows Nexus adapting to a change in workload during a 1000-sec window of the deployment. The top panel shows a stacked graph of requests over time, the middle one the number of GPUs allocated and the bottom one the bad rate, with instantaneous bad rates above 1% marked in red. Around 326s into the window, the number of requests increases and starts varying significantly. Nexus, which is running with 30s epochs, starts dropping requests, detects the change within 12s (this could have been as long as 30s) and allocates more GPUs. It deallocates GPUs (this time with a roughly 10s lag) at the 644s mark when demand subsides. Nexus violates the latency SLOs on 0.27% of requests on average. The sporadic high bad rate (>1%) is mainly due to scheduling reconfiguration triggered by workload changes.\n\nThese results illustrate that Nexus responds well to variable workloads at large scale and is able to allocate close to the aggressive theoretical lower bound.\n\n\nSensitivity analysis of Nexus features\n\nWe now present micro-benchmarks to analyze the main components of Nexus. Overall, we find that Nexus's core techniques are quite robust to variations in key design parameters. GPU Multiplexing. The Nexus runtime (Section 6.3) focuses on minimizing interference on GPU between executing models (by avoiding interleaving during their execution), and idling while switching between models (by overlapping pre/post-processing on CPU with model execution on the Bad rate Figure 13: A 1000 sec window from our large-scale deployment. GPU, and not waiting for fixed target batch sizes to fill up before dispatch to the GPU). Figure 14 analyzes the importance of these features by comparing the throughput of Nexus with those of Clipper, TF Serving, and a version of Nexus (\"Nexus-parallel\") that issues models in parallel and does not control interference. This experiment runs increasing numbers of copies of the Inception model with a latency SLO of 100ms. Throughputs of all four models suffer, TF Serving less than Clipper because it runs models in a round-robin fashion whereas Clipper deploys them in independent containers that interfere. Nexus achieves 1.4-2.1\u00d7 throughput compared to TF serving, and 1.9-9.8\u00d7 throughput compared to Clipper on a single GPU. Nexus-parallel fares better because it avoids idling (but still suffers from interference), and Nexus fares the best. We see similar trends across other models. Figure 14(b) compares the throughput while varying the latency SLO from 50ms to 200ms, with the number of models fixed at 3. When latency SLO becomes higher, the greater scheduling slack gives Nexus-parallel higher throughput. Prefix Batching. Figure 15 examines how the throughput and memory benefits of prefix batching scale as the number of variants of Resnet50 that differ only in the last layer increases, on a single GPU. Figure 15 batching to unbatched execution of the variants. Without prefix batching, the variants have to execute on smaller \"sub-batches\" to satisfy their SLOs, yielding worse aggregate throughput. With prefix batching, since many models can execute in one batch, the sub-batches can be aggregated into large batches that maintain up to 110% higher throughput. Similarly, when the (unshared) model suffixes are small (\"1 FC\", indicating one \"fully connected\" unshared layer, in Figure 15(b)), additional model variants use negligible extra GPU memory. As the number of unshared layers increase (\"2 FC\" and \"3 FC\" add two and three fully connected layers to the shared prefix), the memory benefits fall. Without prefix batching (black line), however, we quickly run out of GPU memory even if a model has only one unshared layer. Squishy Scheduling. We now examine the sensitivity of squishy scheduling to model types, request rates, and SLOs. We compare the throughput of Nexus with squishy scheduling to a baseline version of Nexus that uses batch-oblivious scheduling instead. Both need to allocate 16 sessions on 8 GPUs under 5 scenarios: (a) Inception or (b) ResNet models with mixed SLOs ranging from 50ms to 200ms, (c) Inception or (d) ResNet models with mixed request rates following Zipf-0.9 distribution, (e) 8 different model architectures, each associated with two SLOs, 50ms and 100ms. Figure 16 depicts the relative throughput of standard Nexus with regard to baseline. Nexus outperforms baseline across all mixes, with the highest gains (up to 64%) coming from handling varying request rates, and the lowest (11%) coming from handling varying request mixes. Complex Query Analysis. To evaluate the performance gain of the query analyzer, we compare the throughput of Nexus with and without the query analyzer. The baseline simply splits the latency SLO evenly across the various stages in the query. The query includes two stages: (a) the first stage executes SSD, and then (b) invokes Inception model for \u03b3 times. The experiment is performed on 8 GPUs. We vary the latency SLO from 300ms to 500ms and choose \u03b3 to be 0.1, 1, and 10. Figure 17 shows that Nexus with the query analyzer achieves 13-55% higher throughput than the baseline.   \n\n\nConclusion\n\nWe present a scalable and efficient system design for serving Deep Neural Network (DNN) applications. Instead of serving the entire application in an opaque CPU-based container with models embedded in it, which leads to sub-optimal GPU utilization, our system operates directly on models and GPUs. This design enables several optimizations in batching and allows more efficient resource allocation. Our system is fully implemented, in C++, and evaluation shows that Nexus can achieve 1.8-12.7\u00d7 more throughput relative to state-ofthe-art baselines while staying within latency constraints (achieving a \"good rate\") over 99% of the time.\n\nFigure 1 :\n1A typical vision processing pipeline. Nexus is designed to provide DNN-based analysis for tens of thousands of streams.\n\nFigure 2 :\n2Resource allocation example.\n\nFigure 3 :Figure 4 :\n34Batch The average throughput with three latency split plans for varying \u03b3 .\n\nFigure 6 :\n6Nexus runtime system overview.\n\n\ncan only be assigned to GPUs that are in use. (b) Each session can only be assigned to one GPU.(c) b i j is 0 if i is not assigned to GPU j. (d) b i j is at least 1 if i is assigned to GPU j.(e) Length of a duty cycle as a function of assigned sessions. (f) Latency SLO constraint. (g) Scheduled rate meets the request rate requirement.\n\n\n16:  sort residue_loads by occ i in descending order17:  nodes \u2190 [] 18: for \u27e8M k i , L i , r i , b i , d i , occ i \u27e9 in residue_loads do\n\nFigure 7 :\n7Merge two nodes into one. Use the smaller duty cycle as new duty cycle for both nodes. Update the batch size accordingly and re-estimate the batch latency. If sum of latencies doesn't exceed new duty cycle, the two nodes can be merged.\n\nFigure 8 :\n8Dataflow graph of traffic analysis application.\n\nFigure 9 :\n9Maximal throughput achieved by lazy drop and early drop policy under various \u03b1.\n\nFigure 10 :\n10Game analysis ablation study.\n\nFigure 11 :\n11Traffic analysis ablation study.\n\nFigure 12 :\n12Diurnal throughput variation for traffic analysis.\n\nFigure 14 :\n14Impact on throughput of varying numbers of models served (a) and latency SLOs (b) under GPU multiplexing.\n\nFigure 15 :\n15Impact on throughput (a) and memory use (b) of varying numbers of batched models under prefix batching.\n\nFigure 16 :\n16Impact on throughput of varying model-and SLOmixes under squishy scheduling.\n\nFigure 17 :\n17Impact on throughput of varying query latency SLO and \u03b3 (see Section 4.2) under complex query analysis.\n\nTable 2 :\n2Batching profiles for models used in the example.Lat is the latency (ms) for processing a batch, and Req/s is \nthe throughput achieved. \n\n\n\n\nAlgorithm 1 Squishy Bin Packing AlgorithmSqishyBinPacking(Sessions) \n1: nodes, residue_loads \u2190 ScheduleSaturate(Sessions) \n2: nodes \u2190 nodes \u2295 ScheduleResidule(residue_loads) \n3: return nodes \n\nScheduleSaturate(Sessions) \n4: nodes, residue_loads \u2190 [], [] \n5: for \nWe were unable to determine why the relative performance of Clipper fell so sharply on rush-hour data.\nAcknowledgementsWe thank Lenin Sivalingam, Peter Bodik, anonymous reviewers, and our shepherd, Ion Stoica, for their helpful feedback. We also thank the Watch For team at Microsoft Research for insights and suggestions on game stream analysis and large-scale video processing. This work was supported by NSF award CNS-1614717.A Hardness of Fixed-rate GPU Scheduling Problem (FGSP)We now justify the use of an approximate algorithm for GPU cluster scheduling. We define the Fixed-rate GPU Scheduling Problem (FGSP), which is a highly restricted version of the general problem, and we show that even the restricted version is intractable.FGSP:Input -models M i , 1 \u2264 i \u2264 n with corresponding latencies L i , latency bounds B i and GPU count C. (The latencies correspond to the fixed rates.) Output -Partition of the models into C sets so that in eachWe show that FGSP is strongly NP-hard by reduction from 3-PARTITION[10].Proof. We start with a given instance of 3-PARTITION which consists of a bound B and 3n B 4 \u2264 a 1 , a 2 . . . , a 3n \u2264 B 2 ; the goal of 3-PARTITION is to partition the a i s into triples such that the sum of each triple is B. Observe that wlog we may assume that 1\u2264i \u22643n a i = nB.From the given instance of 3-PARTITION we create an instance of FGSP by settingIt is clear that if there exists a solution to the 3-PARTITION instance then the same partition into n triples yields a partition of the FGSP instance into C = n sets so that D + L i \u2264 9B + a i since D = 7B and L i = 2B + a i . In the other direction suppose there exists a solution to FGSP. Observe that in any solution to FGSP every set can have at most 3 models because otherwise the duty cycle D would exceed 8B and then the constraint D + L i \u2264 B i would be violated for any i in the set, since D + L i > 10B but B i < 10B. Since there are a total of 3n models and C = n sets every set must have exactly 3 models, i.e. every set must be a triple. Since D + L i \u2264 B i for any i in the set, we have that D + 2B + a i \u2264 9B + a i or D \u2264 7B. But this implies that in every triple the sum of the L i s is at most 7B or the sum of the corresponding a i s is at most B. But since the sum of all the n triples is nB and each triple is at most B it must be that the sum of each triple is exactly B. This means that the partition of models of the FGSP instance into sets is also a solution for the partition of the corresponding a i into triples in 3-PARTITION.\u25a1\nTensorFlow: A System for Large-Scale Machine Learning. Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng, 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016. Savannah, GA, USAMart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Va- sudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for Large-Scale Machine Learning. In 12th USENIX Symposium on Operating Systems Design and Implemen- tation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016. USENIX Association, 265-283.\n\nFast Cholesky factorization on GPUs for batch and native modes in MAGMA. Ahmad Abdelfattah, Azzam Haidar, Journal of Computational Science. 20Stanimire Tomov, and Jack DongarraAhmad Abdelfattah, Azzam Haidar, Stanimire Tomov, and Jack Don- garra. 2017. Fast Cholesky factorization on GPUs for batch and native modes in MAGMA. Journal of Computational Science 20 (2017), 85-93.\n\nSlicer: Auto-Sharding for Datacenter Applications. Atul Adya, Daniel Myers, Jon Howell, Jeremy Elson, Colin Meek, Vishesh Khemani, Stefan Fulger, Pan Gu, Lakshminath Bhuvanagiri, Jason Hunter, Roberto Peon, Larry Kai, Alexander Shraer, 12th USENIX Symposium on Operating Systems Design and Implementation. Savannah, GA, USAArif Merchant, and Kfir Lev-AriAtul Adya, Daniel Myers, Jon Howell, Jeremy Elson, Colin Meek, Vishesh Khemani, Stefan Fulger, Pan Gu, Lakshminath Bhuvanagiri, Jason Hunter, Roberto Peon, Larry Kai, Alexander Shraer, Arif Mer- chant, and Kfir Lev-Ari. 2016. Slicer: Auto-Sharding for Datacenter Applications. In 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016. USENIX Association, 739-753.\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, IEEE transactions. 40Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Mur- phy, and Alan L Yuille. 2018. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence 40, 4 (2018), 834-848.\n\nClipper: A Low-Latency Online Prediction Serving System. Daniel Crankshaw, Xin Wang, Giulio Zhou, Michael J Franklin, Joseph E Gonzalez, Ion Stoica, 14th USENIX Symposium on Networked Systems Design and Implementation. Boston, MA, USADaniel Crankshaw, Xin Wang, Giulio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica. 2017. Clipper: A Low-Latency Online Prediction Serving System. In 14th USENIX Symposium on Net- worked Systems Design and Implementation, NSDI 2017, Boston, MA, USA, March 27-29, 2017. USENIX Association, 613-627.\n\nDocker Swarm. Docker, Docker. 2014. Docker Swarm. https://github.com/docker/swarm.\n\nDeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell, Proceedings of the 31th International Conference on Machine Learning. the 31th International Conference on Machine LearningBeijing, China32Workshop and Conference Proceedings)Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. 2014. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014 (JMLR Workshop and Conference Proceedings), Vol. 32. JMLR.org, 647-655.\n\nFacebook, Caffe2: A New Lightweight, Modular, and Scalable Deep Learning Framework. Facebook. 2018. Caffe2: A New Lightweight, Modular, and Scalable Deep Learning Framework. https://caffe2.ai/.\n\nComputers and Intractability: A Guide to the Theory of NP-Completeness. R Michael, David S Garey, Johnson, W. H. Freeman & CoNew York, NY, USAMichael R. Garey and David S. Johnson. 1979. Computers and In- tractability: A Guide to the Theory of NP-Completeness. W. H. Freeman & Co., New York, NY, USA.\n\nKubernetes: Production-Grade Container Orchestration. Google, Google. 2014. Kubernetes: Production-Grade Container Orchestration. https://kubernetes.io/\n\nCloud AutoML Vision. Google, Google. 2019. Cloud AutoML Vision. https://cloud.google.com/vision/ automl/docs/.\n\nTiresias: A GPU Cluster Manager for Distributed Deep Learning. Juncheng Gu, Mosharaf Chowdhury, G Kang, Yibo Shin, Myeongjae Zhu, Junjie Jeon, Hongqiang Qian, Chuanxiong Liu, Guo, 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19. Juncheng Gu, Mosharaf Chowdhury, Kang G Shin, Yibo Zhu, Myeong- jae Jeon, Junjie Qian, Hongqiang Liu, and Chuanxiong Guo. 2019. Tiresias: A GPU Cluster Manager for Distributed Deep Learning. In 16th USENIX Symposium on Networked Systems Design and Implemen- tation (NSDI 19). 485-500.\n\nMCDNN: An approximation-based execution framework for deep stream processing under resource constraints. Seungyeop Han, Haichen Shen, Matthai Philipose, Sharad Agarwal, Alec Wolman, Arvind Krishnamurthy, Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services. the 14th Annual International Conference on Mobile Systems, Applications, and ServicesACMSeungyeop Han, Haichen Shen, Matthai Philipose, Sharad Agar- wal, Alec Wolman, and Arvind Krishnamurthy. 2016. MCDNN: An approximation-based execution framework for deep stream processing under resource constraints. In Proceedings of the 14th Annual Interna- tional Conference on Mobile Systems, Applications, and Services. ACM, 123-136.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770-778.\n\nMesos: A Platform for Fine-Grained Resource Sharing in the Data Center. Benjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, Anthony D Joseph, Randy H Katz, Scott Shenker, Ion Stoica, Proceedings of the 8th USENIX Symposium on Networked Systems Design and Implementation. the 8th USENIX Symposium on Networked Systems Design and ImplementationBoston, MA, USAUSENIX AssociationBenjamin Hindman, Andy Konwinski, Matei Zaharia, Ali Ghodsi, An- thony D. Joseph, Randy H. Katz, Scott Shenker, and Ion Stoica. 2011. Mesos: A Platform for Fine-Grained Resource Sharing in the Data Cen- ter. In Proceedings of the 8th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2011, Boston, MA, USA, March 30 - April 1, 2011. USENIX Association.\n\nFocus: Querying Large Video Datasets with Low Latency and Low Cost. Kevin Hsieh, Ganesh Ananthanarayanan, Peter Bod\u00edk, Shivaram Venkataraman, Paramvir Bahl, Matthai Philipose, Phillip B Gibbons, Onur Mutlu, 13th USENIX Symposium on Operating Systems Design and Implementation. Carlsbad, CA, USAUSENIX AssociationKevin Hsieh, Ganesh Ananthanarayanan, Peter Bod\u00edk, Shivaram Venkataraman, Paramvir Bahl, Matthai Philipose, Phillip B. Gibbons, and Onur Mutlu. 2018. Focus: Querying Large Video Datasets with Low Latency and Low Cost. In 13th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2018, Carlsbad, CA, USA, October 8-10, 2018. USENIX Association, 269-286.\n\nCaffe: Convolutional architecture for fast feature embedding. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaACMYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional architecture for fast feature embedding. In Pro- ceedings of the 22nd ACM international conference on Multimedia. ACM, 675-678.\n\nMainstream: Dynamic stem-sharing for multi-tenant video processing. H Angela, Jiang, L-K Daniel, Christopher Wong, Lilia Canel, Ishan Tang, Michael Misra, Kaminsky, A Michael, Padmanabhan Kozuch, Pillai, Gregory R David G Andersen, Ganger, USENIX Annual Technical Conference (USENIX ATC 18. Angela H Jiang, Daniel L-K Wong, Christopher Canel, Lilia Tang, Is- han Misra, Michael Kaminsky, Michael A Kozuch, Padmanabhan Pillai, David G Andersen, and Gregory R Ganger. 2018. Mainstream: Dy- namic stem-sharing for multi-tenant video processing. In 2018 USENIX Annual Technical Conference (USENIX ATC 18). 29-42.\n\nNoScope: optimizing neural network queries over video at scale. Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, Matei Zaharia, Proceedings of the VLDB Endowment. the VLDB Endowment10Daniel Kang, John Emmons, Firas Abuzaid, Peter Bailis, and Matei Zaharia. 2017. NoScope: optimizing neural network queries over video at scale. Proceedings of the VLDB Endowment 10, 11 (2017), 1586-1597.\n\nBin-Packing. Bernhard Korte, Jens Vygen, 10.1007/978-3-540-71844-4_18Combinatorial Optimization: Theory and Algorithms. Berlin Heidelberg; Berlin, HeidelbergSpringerBernhard Korte and Jens Vygen. 2008. Bin-Packing. In Combinato- rial Optimization: Theory and Algorithms. Springer Berlin Heidelberg, Berlin, Heidelberg, 449-465. https://doi.org/10.1007/978-3-540-71844- 4_18\n\nGradient-based learning applied to document recognition. Yann Lecun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, Proc. IEEE. 86Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278-2324.\n\nVirtual Machine Scale Sets. Microsoft, Microsoft. 2018. Virtual Machine Scale Sets. https://docs.microsoft. com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale- sets-overview.\n\n. Microsoft, Microsoft. 2019. Custom Vision. https://azure.microsoft.com/en-us/ services/cognitive-services/custom-vision-service/.\n\nTensorflow-serving: Flexible, high-performance ml serving. Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, Jordan Soyke, arXiv:1712.06139arXiv preprintChristopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke. 2017. Tensorflow-serving: Flexible, high-performance ml serving. arXiv preprint arXiv:1712.06139 (2017).\n\nLearning and transferring mid-level image representations using convolutional neural networks. Maxime Oquab, Leon Bottou, Ivan Laptev, Josef Sivic, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMaxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. 2014. Learn- ing and transferring mid-level image representations using convo- lutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1717-1724.\n\nSparrow: distributed, low latency scheduling. Kay Ousterhout, Patrick Wendell, Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. the Twenty-Fourth ACM Symposium on Operating Systems PrinciplesACMMatei Zaharia, and Ion StoicaKay Ousterhout, Patrick Wendell, Matei Zaharia, and Ion Stoica. 2013. Sparrow: distributed, low latency scheduling. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. ACM, 69-84.\n\nFlash: An efficient and portable Web server. S Vivek, Peter Pai, Willy Druschel, Zwaenepoel, USENIX Annual Technical Conference. General TrackVivek S Pai, Peter Druschel, and Willy Zwaenepoel. 1999. Flash: An efficient and portable Web server. In USENIX Annual Technical Conference, General Track. 199-212.\n\nDeep Face Recognition. M Omkar, Andrea Parkhi, Andrew Vedaldi, Zisserman, 10.5244/C.29.41Proceedings of the British Machine Vision Conference 2015. the British Machine Vision Conference 2015Swansea, UKBMVA Press12Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman. 2015. Deep Face Recognition. In Proceedings of the British Machine Vision Confer- ence 2015, BMVC 2015, Swansea, UK, September 7-10, 2015. BMVA Press, 41.1-41.12. https://doi.org/10.5244/C.29.41\n\nOptimus: an efficient dynamic resource scheduler for deep learning clusters. Yanghua Peng, Yixin Bao, Yangrui Chen, Chuan Wu, Chuanxiong Guo, Proceedings of the Thirteenth EuroSys Conference. the Thirteenth EuroSys ConferenceACM3Yanghua Peng, Yixin Bao, Yangrui Chen, Chuan Wu, and Chuanxiong Guo. 2018. Optimus: an efficient dynamic resource scheduler for deep learning clusters. In Proceedings of the Thirteenth EuroSys Conference. ACM, 3.\n\nXnor-net: Imagenet classification using binary convolutional neural networks. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi, European Conference on Computer Vision. SpringerMohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016. Xnor-net: Imagenet classification using binary convolu- tional neural networks. In European Conference on Computer Vision. Springer, 525-542.\n\nJoseph Redmon, Darknet: Open Source Neural Networks in C. Joseph Redmon. 2013-2016. Darknet: Open Source Neural Networks in C. http://pjreddie.com/darknet/.\n\nOmega: flexible, scalable schedulers for large compute clusters. Malte Schwarzkopf, Andy Konwinski, Michael Abd-El-Malek, John Wilkes, Proceedings of the 8th ACM European Conference on Computer Systems. the 8th ACM European Conference on Computer SystemsACMMalte Schwarzkopf, Andy Konwinski, Michael Abd-El-Malek, and John Wilkes. 2013. Omega: flexible, scalable schedulers for large compute clusters. In Proceedings of the 8th ACM European Conference on Computer Systems. ACM, 351-364.\n\nCNN features off-the-shelf: an astounding baseline for recognition. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsAli Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. 2014. CNN features off-the-shelf: an astounding baseline for recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops. 806-813.\n\nFast video classification via adaptive cascading of deep models. Haichen Shen, Seungyeop Han, Matthai Philipose, Arvind Krishnamurthy, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHaichen Shen, Seungyeop Han, Matthai Philipose, and Arvind Krish- namurthy. 2017. Fast video classification via adaptive cascading of deep models. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3646-3654.\n\nTwo-stream convolutional networks for action recognition in videos. Karen Simonyan, Andrew Zisserman, Advances in neural information processing systems. Karen Simonyan and Andrew Zisserman. 2014. Two-stream convolu- tional networks for action recognition in videos. In Advances in neural information processing systems. 568-576.\n\nGandiva: Introspective Cluster Scheduling for Deep Learning. Wencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, Lidong Zhou, 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). USENIX Association. Carlsbad, CAWencong Xiao, Romil Bhardwaj, Ramachandran Ramjee, Muthian Si- vathanu, Nipun Kwatra, Zhenhua Han, Pratyush Patel, Xuan Peng, Hanyu Zhao, Quanlu Zhang, Fan Yang, and Lidong Zhou. 2018. Gan- diva: Introspective Cluster Scheduling for Deep Learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). USENIX Association, Carlsbad, CA, 595-610.\n\nA large-scale car dataset for fine-grained categorization and verification. Linjie Yang, Ping Luo, Chen Change Loy, Xiaoou Tang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLinjie Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang. 2015. A large-scale car dataset for fine-grained categorization and verification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3973-3981.\n\nHow transferable are features in deep neural networks?. Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson, Advances in neural information processing systems. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks?. In Advances in neural information processing systems. 3320-3328.\n\nExploiting Sparseness In Deep Neural Networks For Large Vocabulary Speech Recognition. Dong Yu, Frank Seide, Gang Li, Li Deng, Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing. IEEE International Conference on Acoustics, Speech, and Signal ProcessingICASSPDong Yu, Frank Seide, Gang Li, and Li Deng. 2012. Exploiting Sparse- ness In Deep Neural Networks For Large Vocabulary Speech Recog- nition. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP).\n\nLive video analytics at scale with approximation and delay-tolerance. Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Philipose, Paramvir Bahl, Michael J Freedman, 14th USENIX Symposium on Networked Systems Design and Implementation. Haoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Phili- pose, Paramvir Bahl, and Michael J Freedman. 2017. Live video analyt- ics at scale with approximation and delay-tolerance. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). 377-392.\n", "annotations": {"author": "[{\"end\":272,\"start\":114},{\"end\":431,\"start\":273},{\"end\":588,\"start\":432},{\"end\":745,\"start\":589},{\"end\":904,\"start\":746},{\"end\":1062,\"start\":905},{\"end\":1222,\"start\":1063},{\"end\":1384,\"start\":1223},{\"end\":1548,\"start\":1385},{\"end\":1715,\"start\":1549},{\"end\":1875,\"start\":1716},{\"end\":2034,\"start\":1876},{\"end\":2191,\"start\":2035},{\"end\":2348,\"start\":2192},{\"end\":2507,\"start\":2349},{\"end\":2665,\"start\":2508},{\"end\":2829,\"start\":2666},{\"end\":2996,\"start\":2830},{\"end\":3151,\"start\":2997}]", "publisher": "[{\"end\":74,\"start\":71},{\"end\":3319,\"start\":3316}]", "author_last_name": "[{\"end\":126,\"start\":122},{\"end\":285,\"start\":277},{\"end\":442,\"start\":438},{\"end\":599,\"start\":596},{\"end\":758,\"start\":754},{\"end\":916,\"start\":912},{\"end\":1076,\"start\":1072},{\"end\":1238,\"start\":1228},{\"end\":1402,\"start\":1393},{\"end\":1569,\"start\":1556},{\"end\":1729,\"start\":1721},{\"end\":1888,\"start\":1884},{\"end\":2045,\"start\":2041},{\"end\":2202,\"start\":2199},{\"end\":2361,\"start\":2357},{\"end\":2519,\"start\":2515},{\"end\":2683,\"start\":2674},{\"end\":2850,\"start\":2837},{\"end\":3005,\"start\":3002}]", "author_first_name": "[{\"end\":121,\"start\":114},{\"end\":276,\"start\":273},{\"end\":437,\"start\":432},{\"end\":595,\"start\":589},{\"end\":753,\"start\":746},{\"end\":911,\"start\":905},{\"end\":1071,\"start\":1063},{\"end\":1227,\"start\":1223},{\"end\":1392,\"start\":1385},{\"end\":1555,\"start\":1549},{\"end\":1720,\"start\":1716},{\"end\":1883,\"start\":1876},{\"end\":2040,\"start\":2035},{\"end\":2198,\"start\":2192},{\"end\":2356,\"start\":2349},{\"end\":2514,\"start\":2508},{\"end\":2673,\"start\":2666},{\"end\":2836,\"start\":2830},{\"end\":3001,\"start\":2997}]", "author_affiliation": "[{\"end\":271,\"start\":128},{\"end\":430,\"start\":287},{\"end\":587,\"start\":444},{\"end\":744,\"start\":601},{\"end\":903,\"start\":760},{\"end\":1061,\"start\":918},{\"end\":1221,\"start\":1078},{\"end\":1383,\"start\":1240},{\"end\":1547,\"start\":1404},{\"end\":1714,\"start\":1571},{\"end\":1874,\"start\":1731},{\"end\":2033,\"start\":1890},{\"end\":2190,\"start\":2047},{\"end\":2347,\"start\":2204},{\"end\":2506,\"start\":2363},{\"end\":2664,\"start\":2521},{\"end\":2828,\"start\":2685},{\"end\":2995,\"start\":2852},{\"end\":3150,\"start\":3007}]", "title": "[{\"end\":70,\"start\":1},{\"end\":3221,\"start\":3152}]", "venue": "[{\"end\":3274,\"start\":3223}]", "abstract": "[{\"end\":4326,\"start\":3370}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7828,\"start\":7824},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7844,\"start\":7841},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11175,\"start\":11174},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13884,\"start\":13880},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13887,\"start\":13884},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14316,\"start\":14313},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14344,\"start\":14340},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16643,\"start\":16639},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16788,\"start\":16784},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17339,\"start\":17335},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17468,\"start\":17464},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17614,\"start\":17610},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17705,\"start\":17701},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17827,\"start\":17823},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18364,\"start\":18360},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18468,\"start\":18465},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18749,\"start\":18745},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18763,\"start\":18759},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19329,\"start\":19326},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19332,\"start\":19329},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19335,\"start\":19332},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19433,\"start\":19430},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19436,\"start\":19433},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19439,\"start\":19436},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19442,\"start\":19439},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19445,\"start\":19442},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19717,\"start\":19713},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19720,\"start\":19717},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19723,\"start\":19720},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28398,\"start\":28395},{\"end\":29739,\"start\":29736},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30753,\"start\":30749},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30776,\"start\":30772},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35182,\"start\":35178},{\"end\":40150,\"start\":40147},{\"end\":40190,\"start\":40187},{\"end\":40236,\"start\":40233},{\"end\":40280,\"start\":40277},{\"end\":40303,\"start\":40300},{\"end\":40485,\"start\":40482},{\"end\":40537,\"start\":40534},{\"end\":40546,\"start\":40543},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":46802,\"start\":46798},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":47812,\"start\":47809},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":47815,\"start\":47812},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":47818,\"start\":47815},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":47821,\"start\":47818},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":47824,\"start\":47821},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":49693,\"start\":49689},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":49705,\"start\":49702},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":49726,\"start\":49723},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":49744,\"start\":49740},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":49835,\"start\":49831},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":54141,\"start\":54137},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":54238,\"start\":54234},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":56901,\"start\":56898},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":56916,\"start\":56912},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":56939,\"start\":56935},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":57494,\"start\":57493},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":57838,\"start\":57837},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":59504,\"start\":59503}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":67454,\"start\":67322},{\"attributes\":{\"id\":\"fig_1\"},\"end\":67496,\"start\":67455},{\"attributes\":{\"id\":\"fig_2\"},\"end\":67596,\"start\":67497},{\"attributes\":{\"id\":\"fig_3\"},\"end\":67640,\"start\":67597},{\"attributes\":{\"id\":\"fig_4\"},\"end\":67979,\"start\":67641},{\"attributes\":{\"id\":\"fig_5\"},\"end\":68118,\"start\":67980},{\"attributes\":{\"id\":\"fig_6\"},\"end\":68367,\"start\":68119},{\"attributes\":{\"id\":\"fig_7\"},\"end\":68428,\"start\":68368},{\"attributes\":{\"id\":\"fig_8\"},\"end\":68521,\"start\":68429},{\"attributes\":{\"id\":\"fig_9\"},\"end\":68566,\"start\":68522},{\"attributes\":{\"id\":\"fig_10\"},\"end\":68614,\"start\":68567},{\"attributes\":{\"id\":\"fig_11\"},\"end\":68680,\"start\":68615},{\"attributes\":{\"id\":\"fig_13\"},\"end\":68801,\"start\":68681},{\"attributes\":{\"id\":\"fig_14\"},\"end\":68920,\"start\":68802},{\"attributes\":{\"id\":\"fig_15\"},\"end\":69012,\"start\":68921},{\"attributes\":{\"id\":\"fig_16\"},\"end\":69131,\"start\":69013},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":69282,\"start\":69132},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":69547,\"start\":69283}]", "paragraph": "[{\"end\":5129,\"start\":4328},{\"end\":6494,\"start\":5131},{\"end\":8337,\"start\":6496},{\"end\":9562,\"start\":8352},{\"end\":10139,\"start\":9615},{\"end\":11480,\"start\":10141},{\"end\":11974,\"start\":11519},{\"end\":13138,\"start\":11976},{\"end\":13341,\"start\":13165},{\"end\":14241,\"start\":13343},{\"end\":15133,\"start\":14258},{\"end\":16426,\"start\":15135},{\"end\":17192,\"start\":16428},{\"end\":18160,\"start\":17194},{\"end\":19241,\"start\":18162},{\"end\":19823,\"start\":19243},{\"end\":21189,\"start\":19868},{\"end\":21555,\"start\":21213},{\"end\":23373,\"start\":21557},{\"end\":24378,\"start\":23375},{\"end\":25226,\"start\":24380},{\"end\":25836,\"start\":25255},{\"end\":26220,\"start\":25838},{\"end\":26812,\"start\":26222},{\"end\":27472,\"start\":26814},{\"end\":27887,\"start\":27474},{\"end\":29431,\"start\":27926},{\"end\":30137,\"start\":29454},{\"end\":32475,\"start\":30139},{\"end\":32807,\"start\":32477},{\"end\":33140,\"start\":32809},{\"end\":34270,\"start\":33142},{\"end\":34713,\"start\":34310},{\"end\":34960,\"start\":34760},{\"end\":35985,\"start\":35093},{\"end\":37118,\"start\":35987},{\"end\":37767,\"start\":37138},{\"end\":37903,\"start\":37852},{\"end\":37937,\"start\":37905},{\"end\":37985,\"start\":37939},{\"end\":39836,\"start\":38222},{\"end\":40030,\"start\":39974},{\"end\":40318,\"start\":40032},{\"end\":40771,\"start\":40460},{\"end\":41137,\"start\":40773},{\"end\":41489,\"start\":41181},{\"end\":42802,\"start\":41524},{\"end\":43034,\"start\":42804},{\"end\":43570,\"start\":43036},{\"end\":43906,\"start\":43601},{\"end\":44743,\"start\":43908},{\"end\":45485,\"start\":44830},{\"end\":45802,\"start\":45583},{\"end\":45931,\"start\":45827},{\"end\":49534,\"start\":45933},{\"end\":50469,\"start\":49549},{\"end\":52104,\"start\":50482},{\"end\":53695,\"start\":52150},{\"end\":53948,\"start\":53731},{\"end\":55918,\"start\":53966},{\"end\":56858,\"start\":55920},{\"end\":57299,\"start\":56881},{\"end\":57781,\"start\":57301},{\"end\":58368,\"start\":57783},{\"end\":58836,\"start\":58370},{\"end\":59708,\"start\":58838},{\"end\":60223,\"start\":59754},{\"end\":61198,\"start\":60225},{\"end\":62366,\"start\":61200},{\"end\":62527,\"start\":62368},{\"end\":66670,\"start\":62570},{\"end\":67321,\"start\":66685}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13164,\"start\":13139},{\"attributes\":{\"id\":\"formula_1\"},\"end\":35092,\"start\":34961},{\"attributes\":{\"id\":\"formula_2\"},\"end\":37137,\"start\":37119},{\"attributes\":{\"id\":\"formula_3\"},\"end\":37851,\"start\":37800},{\"attributes\":{\"id\":\"formula_4\"},\"end\":38221,\"start\":37986},{\"attributes\":{\"id\":\"formula_5\"},\"end\":39973,\"start\":39837},{\"attributes\":{\"id\":\"formula_6\"},\"end\":40453,\"start\":40319},{\"attributes\":{\"id\":\"formula_7\"},\"end\":41180,\"start\":41138},{\"attributes\":{\"id\":\"formula_8\"},\"end\":41523,\"start\":41490},{\"attributes\":{\"id\":\"formula_9\"},\"end\":44829,\"start\":44744},{\"attributes\":{\"id\":\"formula_10\"},\"end\":45582,\"start\":45486}]", "table_ref": "[{\"end\":9625,\"start\":9618},{\"end\":10263,\"start\":10256},{\"end\":10797,\"start\":10790},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21431,\"start\":21424},{\"end\":32367,\"start\":32314},{\"end\":35142,\"start\":35135},{\"end\":35612,\"start\":35605},{\"end\":50696,\"start\":50689},{\"end\":51146,\"start\":51139},{\"end\":60028,\"start\":60021}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":8350,\"start\":8340},{\"attributes\":{\"n\":\"2.1\"},\"end\":9613,\"start\":9565},{\"attributes\":{\"n\":\"2.2\"},\"end\":11517,\"start\":11483},{\"attributes\":{\"n\":\"3\"},\"end\":14256,\"start\":14244},{\"attributes\":{\"n\":\"4\"},\"end\":19866,\"start\":19826},{\"attributes\":{\"n\":\"4.1\"},\"end\":21211,\"start\":21192},{\"attributes\":{\"n\":\"4.2\"},\"end\":25253,\"start\":25229},{\"attributes\":{\"n\":\"4.3\"},\"end\":27924,\"start\":27890},{\"attributes\":{\"n\":\"5\"},\"end\":29452,\"start\":29434},{\"attributes\":{\"n\":\"6\"},\"end\":34308,\"start\":34273},{\"attributes\":{\"n\":\"6.1\"},\"end\":34758,\"start\":34716},{\"end\":37799,\"start\":37770},{\"end\":40458,\"start\":40455},{\"attributes\":{\"n\":\"6.2\"},\"end\":43599,\"start\":43573},{\"attributes\":{\"n\":\"6.3\"},\"end\":45825,\"start\":45805},{\"attributes\":{\"n\":\"7\"},\"end\":49547,\"start\":49537},{\"attributes\":{\"n\":\"7.1\"},\"end\":50480,\"start\":50472},{\"attributes\":{\"n\":\"7.2\"},\"end\":52148,\"start\":52107},{\"attributes\":{\"n\":\"7.3\"},\"end\":53729,\"start\":53698},{\"attributes\":{\"n\":\"7.3.1\"},\"end\":53964,\"start\":53951},{\"attributes\":{\"n\":\"7.3.2\"},\"end\":56879,\"start\":56861},{\"attributes\":{\"n\":\"7.4\"},\"end\":59752,\"start\":59711},{\"attributes\":{\"n\":\"7.5\"},\"end\":62568,\"start\":62530},{\"attributes\":{\"n\":\"8\"},\"end\":66683,\"start\":66673},{\"end\":67333,\"start\":67323},{\"end\":67466,\"start\":67456},{\"end\":67518,\"start\":67498},{\"end\":67608,\"start\":67598},{\"end\":68130,\"start\":68120},{\"end\":68379,\"start\":68369},{\"end\":68440,\"start\":68430},{\"end\":68534,\"start\":68523},{\"end\":68579,\"start\":68568},{\"end\":68627,\"start\":68616},{\"end\":68693,\"start\":68682},{\"end\":68814,\"start\":68803},{\"end\":68933,\"start\":68922},{\"end\":69025,\"start\":69014},{\"end\":69142,\"start\":69133}]", "table": "[{\"end\":69282,\"start\":69193},{\"end\":69547,\"start\":69326}]", "figure_caption": "[{\"end\":67454,\"start\":67335},{\"end\":67496,\"start\":67468},{\"end\":67596,\"start\":67521},{\"end\":67640,\"start\":67610},{\"end\":67979,\"start\":67643},{\"end\":68118,\"start\":67982},{\"end\":68367,\"start\":68132},{\"end\":68428,\"start\":68381},{\"end\":68521,\"start\":68442},{\"end\":68566,\"start\":68537},{\"end\":68614,\"start\":68582},{\"end\":68680,\"start\":68630},{\"end\":68801,\"start\":68696},{\"end\":68920,\"start\":68817},{\"end\":69012,\"start\":68936},{\"end\":69131,\"start\":69028},{\"end\":69193,\"start\":69144},{\"end\":69326,\"start\":69285}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8540,\"start\":8532},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22562,\"start\":22554},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23672,\"start\":23664},{\"end\":26242,\"start\":26234},{\"end\":27302,\"start\":27294},{\"end\":28910,\"start\":28902},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30191,\"start\":30183},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41918,\"start\":41910},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":44030,\"start\":44022},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":45035,\"start\":45027},{\"end\":49232,\"start\":49224},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":49308,\"start\":49300},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":50980,\"start\":50972},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":55145,\"start\":55136},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":56739,\"start\":56730},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":57433,\"start\":57424},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59001,\"start\":58992},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":61565,\"start\":61556},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":63045,\"start\":63036},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":63197,\"start\":63188},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":63999,\"start\":63990},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":64243,\"start\":64234},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":64427,\"start\":64418},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":64905,\"start\":64896},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":65824,\"start\":65815},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":66573,\"start\":66564}]", "bib_author_first_name": "[{\"end\":72149,\"start\":72143},{\"end\":72161,\"start\":72157},{\"end\":72177,\"start\":72170},{\"end\":72191,\"start\":72184},{\"end\":72202,\"start\":72198},{\"end\":72217,\"start\":72210},{\"end\":72232,\"start\":72224},{\"end\":72246,\"start\":72240},{\"end\":72265,\"start\":72257},{\"end\":72281,\"start\":72274},{\"end\":72298,\"start\":72289},{\"end\":72311,\"start\":72307},{\"end\":72328,\"start\":72323},{\"end\":72342,\"start\":72336},{\"end\":72355,\"start\":72350},{\"end\":72362,\"start\":72356},{\"end\":72377,\"start\":72371},{\"end\":72391,\"start\":72387},{\"end\":72393,\"start\":72392},{\"end\":72407,\"start\":72402},{\"end\":72423,\"start\":72419},{\"end\":72438,\"start\":72432},{\"end\":72450,\"start\":72446},{\"end\":72464,\"start\":72455},{\"end\":73200,\"start\":73195},{\"end\":73219,\"start\":73214},{\"end\":73555,\"start\":73551},{\"end\":73568,\"start\":73562},{\"end\":73579,\"start\":73576},{\"end\":73594,\"start\":73588},{\"end\":73607,\"start\":73602},{\"end\":73621,\"start\":73614},{\"end\":73637,\"start\":73631},{\"end\":73649,\"start\":73646},{\"end\":73665,\"start\":73654},{\"end\":73684,\"start\":73679},{\"end\":73700,\"start\":73693},{\"end\":73712,\"start\":73707},{\"end\":73727,\"start\":73718},{\"end\":74408,\"start\":74397},{\"end\":74421,\"start\":74415},{\"end\":74441,\"start\":74434},{\"end\":74457,\"start\":74452},{\"end\":74470,\"start\":74466},{\"end\":74472,\"start\":74471},{\"end\":74861,\"start\":74855},{\"end\":74876,\"start\":74873},{\"end\":74889,\"start\":74883},{\"end\":74903,\"start\":74896},{\"end\":74905,\"start\":74904},{\"end\":74922,\"start\":74916},{\"end\":74924,\"start\":74923},{\"end\":74938,\"start\":74935},{\"end\":75512,\"start\":75508},{\"end\":75530,\"start\":75522},{\"end\":75541,\"start\":75536},{\"end\":75555,\"start\":75551},{\"end\":75569,\"start\":75565},{\"end\":75581,\"start\":75577},{\"end\":75595,\"start\":75589},{\"end\":76423,\"start\":76422},{\"end\":76438,\"start\":76433},{\"end\":76440,\"start\":76439},{\"end\":76989,\"start\":76981},{\"end\":77002,\"start\":76994},{\"end\":77015,\"start\":77014},{\"end\":77026,\"start\":77022},{\"end\":77042,\"start\":77033},{\"end\":77054,\"start\":77048},{\"end\":77070,\"start\":77061},{\"end\":77087,\"start\":77077},{\"end\":77577,\"start\":77568},{\"end\":77590,\"start\":77583},{\"end\":77604,\"start\":77597},{\"end\":77622,\"start\":77616},{\"end\":77636,\"start\":77632},{\"end\":77651,\"start\":77645},{\"end\":78251,\"start\":78244},{\"end\":78263,\"start\":78256},{\"end\":78279,\"start\":78271},{\"end\":78289,\"start\":78285},{\"end\":78715,\"start\":78707},{\"end\":78729,\"start\":78725},{\"end\":78746,\"start\":78741},{\"end\":78759,\"start\":78756},{\"end\":78775,\"start\":78768},{\"end\":78777,\"start\":78776},{\"end\":78791,\"start\":78786},{\"end\":78793,\"start\":78792},{\"end\":78805,\"start\":78800},{\"end\":78818,\"start\":78815},{\"end\":79466,\"start\":79461},{\"end\":79480,\"start\":79474},{\"end\":79504,\"start\":79499},{\"end\":79520,\"start\":79512},{\"end\":79543,\"start\":79535},{\"end\":79557,\"start\":79550},{\"end\":79576,\"start\":79569},{\"end\":79578,\"start\":79577},{\"end\":79592,\"start\":79588},{\"end\":80146,\"start\":80138},{\"end\":80156,\"start\":80152},{\"end\":80172,\"start\":80168},{\"end\":80188,\"start\":80182},{\"end\":80206,\"start\":80198},{\"end\":80217,\"start\":80213},{\"end\":80234,\"start\":80228},{\"end\":80253,\"start\":80247},{\"end\":80739,\"start\":80738},{\"end\":80758,\"start\":80755},{\"end\":80778,\"start\":80767},{\"end\":80790,\"start\":80785},{\"end\":80803,\"start\":80798},{\"end\":80817,\"start\":80810},{\"end\":80836,\"start\":80835},{\"end\":80857,\"start\":80846},{\"end\":80881,\"start\":80874},{\"end\":80883,\"start\":80882},{\"end\":81350,\"start\":81344},{\"end\":81361,\"start\":81357},{\"end\":81375,\"start\":81370},{\"end\":81390,\"start\":81385},{\"end\":81404,\"start\":81399},{\"end\":81695,\"start\":81687},{\"end\":81707,\"start\":81703},{\"end\":82110,\"start\":82106},{\"end\":82122,\"start\":82118},{\"end\":82137,\"start\":82131},{\"end\":82153,\"start\":82146},{\"end\":82732,\"start\":82721},{\"end\":82745,\"start\":82741},{\"end\":82759,\"start\":82754},{\"end\":82777,\"start\":82769},{\"end\":82789,\"start\":82787},{\"end\":82802,\"start\":82795},{\"end\":82811,\"start\":82807},{\"end\":82832,\"start\":82825},{\"end\":82847,\"start\":82841},{\"end\":83230,\"start\":83224},{\"end\":83242,\"start\":83238},{\"end\":83255,\"start\":83251},{\"end\":83269,\"start\":83264},{\"end\":83723,\"start\":83720},{\"end\":83743,\"start\":83736},{\"end\":84186,\"start\":84185},{\"end\":84199,\"start\":84194},{\"end\":84210,\"start\":84205},{\"end\":84472,\"start\":84471},{\"end\":84486,\"start\":84480},{\"end\":84501,\"start\":84495},{\"end\":84996,\"start\":84989},{\"end\":85008,\"start\":85003},{\"end\":85021,\"start\":85014},{\"end\":85033,\"start\":85028},{\"end\":85048,\"start\":85038},{\"end\":85441,\"start\":85433},{\"end\":85460,\"start\":85453},{\"end\":85476,\"start\":85470},{\"end\":85488,\"start\":85485},{\"end\":85770,\"start\":85764},{\"end\":85992,\"start\":85987},{\"end\":86010,\"start\":86006},{\"end\":86029,\"start\":86022},{\"end\":86048,\"start\":86044},{\"end\":86481,\"start\":86478},{\"end\":86506,\"start\":86499},{\"end\":86526,\"start\":86517},{\"end\":86543,\"start\":86537},{\"end\":87043,\"start\":87036},{\"end\":87059,\"start\":87050},{\"end\":87072,\"start\":87065},{\"end\":87090,\"start\":87084},{\"end\":87561,\"start\":87556},{\"end\":87578,\"start\":87572},{\"end\":87886,\"start\":87879},{\"end\":87898,\"start\":87893},{\"end\":87921,\"start\":87909},{\"end\":87937,\"start\":87930},{\"end\":87954,\"start\":87949},{\"end\":87970,\"start\":87963},{\"end\":87984,\"start\":87976},{\"end\":87996,\"start\":87992},{\"end\":88008,\"start\":88003},{\"end\":88021,\"start\":88015},{\"end\":88032,\"start\":88029},{\"end\":88045,\"start\":88039},{\"end\":88621,\"start\":88615},{\"end\":88632,\"start\":88628},{\"end\":88642,\"start\":88638},{\"end\":88649,\"start\":88643},{\"end\":88661,\"start\":88655},{\"end\":89103,\"start\":89098},{\"end\":89118,\"start\":89114},{\"end\":89132,\"start\":89126},{\"end\":89144,\"start\":89141},{\"end\":89482,\"start\":89478},{\"end\":89492,\"start\":89487},{\"end\":89504,\"start\":89500},{\"end\":89511,\"start\":89509},{\"end\":90006,\"start\":90001},{\"end\":90020,\"start\":90014},{\"end\":90044,\"start\":90039},{\"end\":90059,\"start\":90052},{\"end\":90079,\"start\":90071},{\"end\":90095,\"start\":90086}]", "bib_author_last_name": "[{\"end\":72155,\"start\":72150},{\"end\":72168,\"start\":72162},{\"end\":72182,\"start\":72178},{\"end\":72196,\"start\":72192},{\"end\":72208,\"start\":72203},{\"end\":72222,\"start\":72218},{\"end\":72238,\"start\":72233},{\"end\":72255,\"start\":72247},{\"end\":72272,\"start\":72266},{\"end\":72287,\"start\":72282},{\"end\":72305,\"start\":72299},{\"end\":72321,\"start\":72312},{\"end\":72334,\"start\":72329},{\"end\":72348,\"start\":72343},{\"end\":72369,\"start\":72363},{\"end\":72385,\"start\":72378},{\"end\":72400,\"start\":72394},{\"end\":72417,\"start\":72408},{\"end\":72430,\"start\":72424},{\"end\":72444,\"start\":72439},{\"end\":72453,\"start\":72451},{\"end\":72470,\"start\":72465},{\"end\":73212,\"start\":73201},{\"end\":73226,\"start\":73220},{\"end\":73560,\"start\":73556},{\"end\":73574,\"start\":73569},{\"end\":73586,\"start\":73580},{\"end\":73600,\"start\":73595},{\"end\":73612,\"start\":73608},{\"end\":73629,\"start\":73622},{\"end\":73644,\"start\":73638},{\"end\":73652,\"start\":73650},{\"end\":73677,\"start\":73666},{\"end\":73691,\"start\":73685},{\"end\":73705,\"start\":73701},{\"end\":73716,\"start\":73713},{\"end\":73734,\"start\":73728},{\"end\":74413,\"start\":74409},{\"end\":74432,\"start\":74422},{\"end\":74450,\"start\":74442},{\"end\":74464,\"start\":74458},{\"end\":74479,\"start\":74473},{\"end\":74871,\"start\":74862},{\"end\":74881,\"start\":74877},{\"end\":74894,\"start\":74890},{\"end\":74914,\"start\":74906},{\"end\":74933,\"start\":74925},{\"end\":74945,\"start\":74939},{\"end\":75365,\"start\":75359},{\"end\":75520,\"start\":75513},{\"end\":75534,\"start\":75531},{\"end\":75549,\"start\":75542},{\"end\":75563,\"start\":75556},{\"end\":75575,\"start\":75570},{\"end\":75587,\"start\":75582},{\"end\":75603,\"start\":75596},{\"end\":76163,\"start\":76155},{\"end\":76431,\"start\":76424},{\"end\":76446,\"start\":76441},{\"end\":76455,\"start\":76448},{\"end\":76712,\"start\":76706},{\"end\":76833,\"start\":76827},{\"end\":76992,\"start\":76990},{\"end\":77012,\"start\":77003},{\"end\":77020,\"start\":77016},{\"end\":77031,\"start\":77027},{\"end\":77046,\"start\":77043},{\"end\":77059,\"start\":77055},{\"end\":77075,\"start\":77071},{\"end\":77091,\"start\":77088},{\"end\":77096,\"start\":77093},{\"end\":77581,\"start\":77578},{\"end\":77595,\"start\":77591},{\"end\":77614,\"start\":77605},{\"end\":77630,\"start\":77623},{\"end\":77643,\"start\":77637},{\"end\":77665,\"start\":77652},{\"end\":78254,\"start\":78252},{\"end\":78269,\"start\":78264},{\"end\":78283,\"start\":78280},{\"end\":78293,\"start\":78290},{\"end\":78723,\"start\":78716},{\"end\":78739,\"start\":78730},{\"end\":78754,\"start\":78747},{\"end\":78766,\"start\":78760},{\"end\":78784,\"start\":78778},{\"end\":78798,\"start\":78794},{\"end\":78813,\"start\":78806},{\"end\":78825,\"start\":78819},{\"end\":79472,\"start\":79467},{\"end\":79497,\"start\":79481},{\"end\":79510,\"start\":79505},{\"end\":79533,\"start\":79521},{\"end\":79548,\"start\":79544},{\"end\":79567,\"start\":79558},{\"end\":79586,\"start\":79579},{\"end\":79598,\"start\":79593},{\"end\":80150,\"start\":80147},{\"end\":80166,\"start\":80157},{\"end\":80180,\"start\":80173},{\"end\":80196,\"start\":80189},{\"end\":80211,\"start\":80207},{\"end\":80226,\"start\":80218},{\"end\":80245,\"start\":80235},{\"end\":80261,\"start\":80254},{\"end\":80746,\"start\":80740},{\"end\":80753,\"start\":80748},{\"end\":80765,\"start\":80759},{\"end\":80783,\"start\":80779},{\"end\":80796,\"start\":80791},{\"end\":80808,\"start\":80804},{\"end\":80823,\"start\":80818},{\"end\":80833,\"start\":80825},{\"end\":80844,\"start\":80837},{\"end\":80864,\"start\":80858},{\"end\":80872,\"start\":80866},{\"end\":80900,\"start\":80884},{\"end\":80908,\"start\":80902},{\"end\":81355,\"start\":81351},{\"end\":81368,\"start\":81362},{\"end\":81383,\"start\":81376},{\"end\":81397,\"start\":81391},{\"end\":81412,\"start\":81405},{\"end\":81701,\"start\":81696},{\"end\":81713,\"start\":81708},{\"end\":82116,\"start\":82111},{\"end\":82129,\"start\":82123},{\"end\":82144,\"start\":82138},{\"end\":82161,\"start\":82154},{\"end\":82376,\"start\":82367},{\"end\":82540,\"start\":82531},{\"end\":82739,\"start\":82733},{\"end\":82752,\"start\":82746},{\"end\":82767,\"start\":82760},{\"end\":82785,\"start\":82778},{\"end\":82793,\"start\":82790},{\"end\":82805,\"start\":82803},{\"end\":82823,\"start\":82812},{\"end\":82839,\"start\":82833},{\"end\":82853,\"start\":82848},{\"end\":83236,\"start\":83231},{\"end\":83249,\"start\":83243},{\"end\":83262,\"start\":83256},{\"end\":83275,\"start\":83270},{\"end\":83734,\"start\":83724},{\"end\":83751,\"start\":83744},{\"end\":84192,\"start\":84187},{\"end\":84203,\"start\":84200},{\"end\":84219,\"start\":84211},{\"end\":84231,\"start\":84221},{\"end\":84478,\"start\":84473},{\"end\":84493,\"start\":84487},{\"end\":84509,\"start\":84502},{\"end\":84520,\"start\":84511},{\"end\":85001,\"start\":84997},{\"end\":85012,\"start\":85009},{\"end\":85026,\"start\":85022},{\"end\":85036,\"start\":85034},{\"end\":85052,\"start\":85049},{\"end\":85451,\"start\":85442},{\"end\":85468,\"start\":85461},{\"end\":85483,\"start\":85477},{\"end\":85496,\"start\":85489},{\"end\":85777,\"start\":85771},{\"end\":86004,\"start\":85993},{\"end\":86020,\"start\":86011},{\"end\":86042,\"start\":86030},{\"end\":86055,\"start\":86049},{\"end\":86497,\"start\":86482},{\"end\":86515,\"start\":86507},{\"end\":86535,\"start\":86527},{\"end\":86552,\"start\":86544},{\"end\":87048,\"start\":87044},{\"end\":87063,\"start\":87060},{\"end\":87082,\"start\":87073},{\"end\":87104,\"start\":87091},{\"end\":87570,\"start\":87562},{\"end\":87588,\"start\":87579},{\"end\":87891,\"start\":87887},{\"end\":87907,\"start\":87899},{\"end\":87928,\"start\":87922},{\"end\":87947,\"start\":87938},{\"end\":87961,\"start\":87955},{\"end\":87974,\"start\":87971},{\"end\":87990,\"start\":87985},{\"end\":88001,\"start\":87997},{\"end\":88013,\"start\":88009},{\"end\":88027,\"start\":88022},{\"end\":88037,\"start\":88033},{\"end\":88050,\"start\":88046},{\"end\":88626,\"start\":88622},{\"end\":88636,\"start\":88633},{\"end\":88653,\"start\":88650},{\"end\":88666,\"start\":88662},{\"end\":89112,\"start\":89104},{\"end\":89124,\"start\":89119},{\"end\":89139,\"start\":89133},{\"end\":89151,\"start\":89145},{\"end\":89485,\"start\":89483},{\"end\":89498,\"start\":89493},{\"end\":89507,\"start\":89505},{\"end\":89516,\"start\":89512},{\"end\":90012,\"start\":90007},{\"end\":90037,\"start\":90021},{\"end\":90050,\"start\":90045},{\"end\":90069,\"start\":90060},{\"end\":90084,\"start\":90080},{\"end\":90104,\"start\":90096}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6287870},\"end\":73120,\"start\":72088},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9066813},\"end\":73498,\"start\":73122},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":9258625},\"end\":74282,\"start\":73500},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3429309},\"end\":74796,\"start\":74284},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1701442},\"end\":75343,\"start\":74798},{\"attributes\":{\"id\":\"b5\"},\"end\":75427,\"start\":75345},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6161478},\"end\":76153,\"start\":75429},{\"attributes\":{\"id\":\"b7\"},\"end\":76348,\"start\":76155},{\"attributes\":{\"id\":\"b8\"},\"end\":76650,\"start\":76350},{\"attributes\":{\"id\":\"b9\"},\"end\":76804,\"start\":76652},{\"attributes\":{\"id\":\"b10\"},\"end\":76916,\"start\":76806},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":73725044},\"end\":77461,\"start\":76918},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":207236834},\"end\":78196,\"start\":77463},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206594692},\"end\":78633,\"start\":78198},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":892222},\"end\":79391,\"start\":78635},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":31635004},\"end\":80074,\"start\":79393},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1799558},\"end\":80668,\"start\":80076},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3520440},\"end\":81278,\"start\":80670},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":20732104},\"end\":81672,\"start\":81280},{\"attributes\":{\"doi\":\"10.1007/978-3-540-71844-4_18\",\"id\":\"b19\",\"matched_paper_id\":13434122},\"end\":82047,\"start\":81674},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14542261},\"end\":82337,\"start\":82049},{\"attributes\":{\"id\":\"b21\"},\"end\":82527,\"start\":82339},{\"attributes\":{\"id\":\"b22\"},\"end\":82660,\"start\":82529},{\"attributes\":{\"doi\":\"arXiv:1712.06139\",\"id\":\"b23\"},\"end\":83127,\"start\":82662},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206592191},\"end\":83672,\"start\":83129},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10529766},\"end\":84138,\"start\":83674},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":15169084},\"end\":84446,\"start\":84140},{\"attributes\":{\"doi\":\"10.5244/C.29.41\",\"id\":\"b27\",\"matched_paper_id\":4637184},\"end\":84910,\"start\":84448},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4949076},\"end\":85353,\"start\":84912},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14925907},\"end\":85762,\"start\":85355},{\"attributes\":{\"id\":\"b30\"},\"end\":85920,\"start\":85764},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14702928},\"end\":86408,\"start\":85922},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6383532},\"end\":86969,\"start\":86410},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":259188},\"end\":87486,\"start\":86971},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11797475},\"end\":87816,\"start\":87488},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":52987896},\"end\":88537,\"start\":87818},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2510693},\"end\":89040,\"start\":88539},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":362467},\"end\":89389,\"start\":89042},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":10835319},\"end\":89929,\"start\":89391},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11894138},\"end\":90456,\"start\":89931}]", "bib_title": "[{\"end\":72141,\"start\":72088},{\"end\":73193,\"start\":73122},{\"end\":73549,\"start\":73500},{\"end\":74395,\"start\":74284},{\"end\":74853,\"start\":74798},{\"end\":75506,\"start\":75429},{\"end\":76979,\"start\":76918},{\"end\":77566,\"start\":77463},{\"end\":78242,\"start\":78198},{\"end\":78705,\"start\":78635},{\"end\":79459,\"start\":79393},{\"end\":80136,\"start\":80076},{\"end\":80736,\"start\":80670},{\"end\":81342,\"start\":81280},{\"end\":81685,\"start\":81674},{\"end\":82104,\"start\":82049},{\"end\":83222,\"start\":83129},{\"end\":83718,\"start\":83674},{\"end\":84183,\"start\":84140},{\"end\":84469,\"start\":84448},{\"end\":84987,\"start\":84912},{\"end\":85431,\"start\":85355},{\"end\":85985,\"start\":85922},{\"end\":86476,\"start\":86410},{\"end\":87034,\"start\":86971},{\"end\":87554,\"start\":87488},{\"end\":87877,\"start\":87818},{\"end\":88613,\"start\":88539},{\"end\":89096,\"start\":89042},{\"end\":89476,\"start\":89391},{\"end\":89999,\"start\":89931}]", "bib_author": "[{\"end\":72157,\"start\":72143},{\"end\":72170,\"start\":72157},{\"end\":72184,\"start\":72170},{\"end\":72198,\"start\":72184},{\"end\":72210,\"start\":72198},{\"end\":72224,\"start\":72210},{\"end\":72240,\"start\":72224},{\"end\":72257,\"start\":72240},{\"end\":72274,\"start\":72257},{\"end\":72289,\"start\":72274},{\"end\":72307,\"start\":72289},{\"end\":72323,\"start\":72307},{\"end\":72336,\"start\":72323},{\"end\":72350,\"start\":72336},{\"end\":72371,\"start\":72350},{\"end\":72387,\"start\":72371},{\"end\":72402,\"start\":72387},{\"end\":72419,\"start\":72402},{\"end\":72432,\"start\":72419},{\"end\":72446,\"start\":72432},{\"end\":72455,\"start\":72446},{\"end\":72472,\"start\":72455},{\"end\":73214,\"start\":73195},{\"end\":73228,\"start\":73214},{\"end\":73562,\"start\":73551},{\"end\":73576,\"start\":73562},{\"end\":73588,\"start\":73576},{\"end\":73602,\"start\":73588},{\"end\":73614,\"start\":73602},{\"end\":73631,\"start\":73614},{\"end\":73646,\"start\":73631},{\"end\":73654,\"start\":73646},{\"end\":73679,\"start\":73654},{\"end\":73693,\"start\":73679},{\"end\":73707,\"start\":73693},{\"end\":73718,\"start\":73707},{\"end\":73736,\"start\":73718},{\"end\":74415,\"start\":74397},{\"end\":74434,\"start\":74415},{\"end\":74452,\"start\":74434},{\"end\":74466,\"start\":74452},{\"end\":74481,\"start\":74466},{\"end\":74873,\"start\":74855},{\"end\":74883,\"start\":74873},{\"end\":74896,\"start\":74883},{\"end\":74916,\"start\":74896},{\"end\":74935,\"start\":74916},{\"end\":74947,\"start\":74935},{\"end\":75367,\"start\":75359},{\"end\":75522,\"start\":75508},{\"end\":75536,\"start\":75522},{\"end\":75551,\"start\":75536},{\"end\":75565,\"start\":75551},{\"end\":75577,\"start\":75565},{\"end\":75589,\"start\":75577},{\"end\":75605,\"start\":75589},{\"end\":76165,\"start\":76155},{\"end\":76433,\"start\":76422},{\"end\":76448,\"start\":76433},{\"end\":76457,\"start\":76448},{\"end\":76714,\"start\":76706},{\"end\":76835,\"start\":76827},{\"end\":76994,\"start\":76981},{\"end\":77014,\"start\":76994},{\"end\":77022,\"start\":77014},{\"end\":77033,\"start\":77022},{\"end\":77048,\"start\":77033},{\"end\":77061,\"start\":77048},{\"end\":77077,\"start\":77061},{\"end\":77093,\"start\":77077},{\"end\":77098,\"start\":77093},{\"end\":77583,\"start\":77568},{\"end\":77597,\"start\":77583},{\"end\":77616,\"start\":77597},{\"end\":77632,\"start\":77616},{\"end\":77645,\"start\":77632},{\"end\":77667,\"start\":77645},{\"end\":78256,\"start\":78244},{\"end\":78271,\"start\":78256},{\"end\":78285,\"start\":78271},{\"end\":78295,\"start\":78285},{\"end\":78725,\"start\":78707},{\"end\":78741,\"start\":78725},{\"end\":78756,\"start\":78741},{\"end\":78768,\"start\":78756},{\"end\":78786,\"start\":78768},{\"end\":78800,\"start\":78786},{\"end\":78815,\"start\":78800},{\"end\":78827,\"start\":78815},{\"end\":79474,\"start\":79461},{\"end\":79499,\"start\":79474},{\"end\":79512,\"start\":79499},{\"end\":79535,\"start\":79512},{\"end\":79550,\"start\":79535},{\"end\":79569,\"start\":79550},{\"end\":79588,\"start\":79569},{\"end\":79600,\"start\":79588},{\"end\":80152,\"start\":80138},{\"end\":80168,\"start\":80152},{\"end\":80182,\"start\":80168},{\"end\":80198,\"start\":80182},{\"end\":80213,\"start\":80198},{\"end\":80228,\"start\":80213},{\"end\":80247,\"start\":80228},{\"end\":80263,\"start\":80247},{\"end\":80748,\"start\":80738},{\"end\":80755,\"start\":80748},{\"end\":80767,\"start\":80755},{\"end\":80785,\"start\":80767},{\"end\":80798,\"start\":80785},{\"end\":80810,\"start\":80798},{\"end\":80825,\"start\":80810},{\"end\":80835,\"start\":80825},{\"end\":80846,\"start\":80835},{\"end\":80866,\"start\":80846},{\"end\":80874,\"start\":80866},{\"end\":80902,\"start\":80874},{\"end\":80910,\"start\":80902},{\"end\":81357,\"start\":81344},{\"end\":81370,\"start\":81357},{\"end\":81385,\"start\":81370},{\"end\":81399,\"start\":81385},{\"end\":81414,\"start\":81399},{\"end\":81703,\"start\":81687},{\"end\":81715,\"start\":81703},{\"end\":82118,\"start\":82106},{\"end\":82131,\"start\":82118},{\"end\":82146,\"start\":82131},{\"end\":82163,\"start\":82146},{\"end\":82378,\"start\":82367},{\"end\":82542,\"start\":82531},{\"end\":82741,\"start\":82721},{\"end\":82754,\"start\":82741},{\"end\":82769,\"start\":82754},{\"end\":82787,\"start\":82769},{\"end\":82795,\"start\":82787},{\"end\":82807,\"start\":82795},{\"end\":82825,\"start\":82807},{\"end\":82841,\"start\":82825},{\"end\":82855,\"start\":82841},{\"end\":83238,\"start\":83224},{\"end\":83251,\"start\":83238},{\"end\":83264,\"start\":83251},{\"end\":83277,\"start\":83264},{\"end\":83736,\"start\":83720},{\"end\":83753,\"start\":83736},{\"end\":84194,\"start\":84185},{\"end\":84205,\"start\":84194},{\"end\":84221,\"start\":84205},{\"end\":84233,\"start\":84221},{\"end\":84480,\"start\":84471},{\"end\":84495,\"start\":84480},{\"end\":84511,\"start\":84495},{\"end\":84522,\"start\":84511},{\"end\":85003,\"start\":84989},{\"end\":85014,\"start\":85003},{\"end\":85028,\"start\":85014},{\"end\":85038,\"start\":85028},{\"end\":85054,\"start\":85038},{\"end\":85453,\"start\":85433},{\"end\":85470,\"start\":85453},{\"end\":85485,\"start\":85470},{\"end\":85498,\"start\":85485},{\"end\":85779,\"start\":85764},{\"end\":86006,\"start\":85987},{\"end\":86022,\"start\":86006},{\"end\":86044,\"start\":86022},{\"end\":86057,\"start\":86044},{\"end\":86499,\"start\":86478},{\"end\":86517,\"start\":86499},{\"end\":86537,\"start\":86517},{\"end\":86554,\"start\":86537},{\"end\":87050,\"start\":87036},{\"end\":87065,\"start\":87050},{\"end\":87084,\"start\":87065},{\"end\":87106,\"start\":87084},{\"end\":87572,\"start\":87556},{\"end\":87590,\"start\":87572},{\"end\":87893,\"start\":87879},{\"end\":87909,\"start\":87893},{\"end\":87930,\"start\":87909},{\"end\":87949,\"start\":87930},{\"end\":87963,\"start\":87949},{\"end\":87976,\"start\":87963},{\"end\":87992,\"start\":87976},{\"end\":88003,\"start\":87992},{\"end\":88015,\"start\":88003},{\"end\":88029,\"start\":88015},{\"end\":88039,\"start\":88029},{\"end\":88052,\"start\":88039},{\"end\":88628,\"start\":88615},{\"end\":88638,\"start\":88628},{\"end\":88655,\"start\":88638},{\"end\":88668,\"start\":88655},{\"end\":89114,\"start\":89098},{\"end\":89126,\"start\":89114},{\"end\":89141,\"start\":89126},{\"end\":89153,\"start\":89141},{\"end\":89487,\"start\":89478},{\"end\":89500,\"start\":89487},{\"end\":89509,\"start\":89500},{\"end\":89518,\"start\":89509},{\"end\":90014,\"start\":90001},{\"end\":90039,\"start\":90014},{\"end\":90052,\"start\":90039},{\"end\":90071,\"start\":90052},{\"end\":90086,\"start\":90071},{\"end\":90106,\"start\":90086}]", "bib_venue": "[{\"end\":72570,\"start\":72553},{\"end\":73823,\"start\":73806},{\"end\":75032,\"start\":75017},{\"end\":75742,\"start\":75675},{\"end\":77856,\"start\":77770},{\"end\":78436,\"start\":78374},{\"end\":79001,\"start\":78915},{\"end\":79687,\"start\":79670},{\"end\":80382,\"start\":80331},{\"end\":81467,\"start\":81449},{\"end\":81831,\"start\":81794},{\"end\":83418,\"start\":83356},{\"end\":83896,\"start\":83833},{\"end\":84649,\"start\":84596},{\"end\":85137,\"start\":85104},{\"end\":86176,\"start\":86125},{\"end\":86715,\"start\":86643},{\"end\":87247,\"start\":87185},{\"end\":88164,\"start\":88152},{\"end\":88809,\"start\":88747},{\"end\":89681,\"start\":89608},{\"end\":72551,\"start\":72472},{\"end\":73260,\"start\":73228},{\"end\":73804,\"start\":73736},{\"end\":74498,\"start\":74481},{\"end\":75015,\"start\":74947},{\"end\":75357,\"start\":75345},{\"end\":75673,\"start\":75605},{\"end\":76237,\"start\":76165},{\"end\":76420,\"start\":76350},{\"end\":76704,\"start\":76652},{\"end\":76825,\"start\":76806},{\"end\":77175,\"start\":77098},{\"end\":77768,\"start\":77667},{\"end\":78372,\"start\":78295},{\"end\":78913,\"start\":78827},{\"end\":79668,\"start\":79600},{\"end\":80329,\"start\":80263},{\"end\":80959,\"start\":80910},{\"end\":81447,\"start\":81414},{\"end\":81792,\"start\":81743},{\"end\":82173,\"start\":82163},{\"end\":82365,\"start\":82339},{\"end\":82719,\"start\":82662},{\"end\":83354,\"start\":83277},{\"end\":83831,\"start\":83753},{\"end\":84267,\"start\":84233},{\"end\":84594,\"start\":84537},{\"end\":85102,\"start\":85054},{\"end\":85536,\"start\":85498},{\"end\":85820,\"start\":85779},{\"end\":86123,\"start\":86057},{\"end\":86641,\"start\":86554},{\"end\":87183,\"start\":87106},{\"end\":87639,\"start\":87590},{\"end\":88150,\"start\":88052},{\"end\":88745,\"start\":88668},{\"end\":89202,\"start\":89153},{\"end\":89606,\"start\":89518},{\"end\":90174,\"start\":90106}]"}}}, "year": 2023, "month": 12, "day": 17}