{"id": 252846243, "updated": "2023-10-05 09:58:51.869", "metadata": {"title": "Point Cloud Scene Completion with Joint Color and Semantic Estimation from Single RGB-D Image", "authors": "[{\"first\":\"Zhaoxuan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xiaoguang\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Tong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Baocai\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Yang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We present a deep reinforcement learning method of progressive view inpainting for colored semantic point cloud scene completion under volume guidance, achieving high-quality scene reconstruction from only a single RGB-D image with severe occlusion. Our approach is end-to-end, consisting of three modules: 3D scene volume reconstruction, 2D RGB-D and segmentation image inpainting, and multi-view selection for completion. Given a single RGB-D image, our method first predicts its semantic segmentation map and goes through the 3D volume branch to obtain a volumetric scene reconstruction as a guide to the next view inpainting step, which attempts to make up the missing information; the third step involves projecting the volume under the same view of the input, concatenating them to complete the current view RGB-D and segmentation map, and integrating all RGB-D and segmentation maps into the point cloud. Since the occluded areas are unavailable, we resort to a A3C network to glance around and pick the next best view for large hole completion progressively until a scene is adequately reconstructed while guaranteeing validity. All steps are learned jointly to achieve robust and consistent results. We perform qualitative and quantitative evaluations with extensive experiments on the 3D-FUTURE data, obtaining better results than state-of-the-arts.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2210.05891", "mag": null, "acl": null, "pubmed": "37018106", "pubmedcentral": null, "dblp": "journals/pami/ZhangHDLYY23", "doi": "10.1109/tpami.2023.3264449"}}, "content": {"source": {"pdf_hash": "f32ae39f30181cd737af7b5ced35f6b7d854529c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2210.05891v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "92b6947ebf8644365976e465cfd1b0568ebf7581", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f32ae39f30181cd737af7b5ced35f6b7d854529c.txt", "contents": "\nPoint Cloud Scene Completion with Joint Color and Semantic Estimation from Single RGB-D Image\n\n\nZhaoxuan Zhang \nXiaoguang Han \nBo Dong \nTong Li \nBaocai Yin \nXin Yang \nPoint Cloud Scene Completion with Joint Color and Semantic Estimation from Single RGB-D Image\n1\nWe present a deep reinforcement learning method of progressive view inpainting for colored semantic point cloud scene completion under volume guidance, achieving high-quality scene reconstruction from only a single RGB-D image with severe occlusion. Our approach is end-to-end, consisting of three modules: 3D scene volume reconstruction, 2D RGB-D and segmentation image inpainting, and multi-view selection for completion. Given a single RGB-D image, our method first predicts its semantic segmentation map and goes through the 3D volume branch to obtain a volumetric scene reconstruction as a guide to the next view inpainting step, which attempts to make up the missing information; the third step involves projecting the volume under the same view of the input, concatenating them to complete the current view RGB-D and segmentation map, and integrating all RGB-D and segmentation maps into the point cloud. Since the occluded areas are unavailable, we resort to a A3C network to glance around and pick the next best view for large hole completion progressively until a scene is adequately reconstructed while guaranteeing validity. All steps are learned jointly to achieve robust and consistent results. We perform qualitative and quantitative evaluations with extensive experiments on the 3D-FUTURE data, obtaining better results than state-of-the-arts.Index Terms-3D scene reconstruction, reinforcement learning, view path planning, 3D scene semantic segmentation.\n\nINTRODUCTION\n\nA 3D scene with plentiful color and semantic segmentation information can bring human an immersive virtual reality or augmented reality experience, and it also enables the robotics to perform indoor obstacle avoidance, navigation and other tasks well-founded.\n\nThus, reconstructing the 3D scenes from multiple or single images has always been a focus of community's work. Recent years, with the development of deep learning neural networks, previous works [1], [2], [3], [4], [5], [6] focus on reconstructing and completing a 3D voxel scene model from only a single depth or RGB-D image, and predict every voxels' semantic label at the same time, which is also called semantic scene completion(SSC) task. However, the output of SSC is always of volumetric expression, which is a low-resolution representation compared to point cloud. And even though the input of SSC is sometimes an RGB-D image [5], there is still very little work to predict the color information of the model. With this motivation, our goal is to implement an algorithm that completes the missing areas of a 3D point cloud scene as well as predicting its colored and semantic segmentation label from a single RGB-D image which we denote as colored semantic scene completion(CSSC) in this paper. These missing areas are always not directly captured by the sensor due to object occlusion or self-occlusion. Although this problem is mild in human vision system, it becomes severe in machine vision because of the sheer imbalance between input and output information. Due to the disorder of point clouds, there is almost no deep learning network that can generate scenelevel point clouds, let alone predict the semantic and color information while predicting point coordinates.\n\nConsider the difficulty and complexity of CSSC task, we propose to split it into a sequence of sub-tasks and solving them in turn: we first try to recover the missing area for the initial incomplete point cloud from a single depth image, denoted as geometric scene completion(GSC), which has been done in our previous CVPR work [7]; then we introduce the RGB image corresponding to the input depth map into the algorithm to restore the color information of the entire scene, denoted as colored scene completion(CSC); inspired by [8], considering that the semantic information will help both geometry and color completion task, we finally add a semantic segmentation prediction and completion module to the algorithm as an additional guided input for the other two completion tasks, which corresponds to CSSC task.\n\nFor the first sub-task, one class of popular approaches [9], [10], [11], [12] to this problem is based on classifyand-search: pixels of the depth map are classified into several semantic object regions, which are mapped to most similar 3D ones in a prepared dataset to construct a fully 3D scene. Owing to the limited capacity of the database, results from classify-and-search are often far away from the ground truth. By transforming the depth map into an incomplete point cloud, Song et al. [1] recently presented the first end-to-end deep network to map it to a fully voxelized scene, while simultaneously outputting the class labels each voxel belongs to. The availability of volumetric representations makes it possible to leverage 3D convolutional neural networks (3DCNN) to effectively capture the global contextual information, however, starting with an incomplete point cloud results in loss of input information and consequently low-resolution outputs. Several recent works [3], [4], [5], [6] attempt to compensate the lost information by extracting features from the 2D input domain in parallel and feeding them to the 3DCNN stream. For the second sub-task, less previous work directly address this issue. Wang et al. [13] propose an end-to-end method for mesh object reconstruction from a single RGB image, which can produce more accurate 3D shape and also predict per-vertex properties(e.g. color). As for scene-level color estimation, the problem become more difficult due to multiple different objects in the scene that block each other. At this time, the information provided by the input 2D image is very limited for colored scene completion task. For the third sub-task, although several recent works [1], [3], [4], [5], [6] have done the semantic information prediction task at the same time with the scene completion task, but they always regard the semantic label of voxel as a multitask output and less consider whether a pre-predicted semantic label will help scene completion task. Following [8], we add the semantic segmentation image as the local structure information to guide the RGB-D image inpainting, where we argue better local information should help to improve inpainting quality further then the global context provided by low-resolution voxels. To our best knowledge, no work has been done on addressing the low-resolution issue of improving output quality and predicting the both color and segmentation label at the same time.\n\nTaking an RGB-D image as input, in this work we advocate the approach of straightforwardly reconstructing 3D points to fill missing region and estimate both color and semantic labels at the same time to achieve high-resolution colored semantic completion (Fig. 1). To this end, we propose to carry out semantic segmentation prediction on initial RGB-D input and completion on multi-view RGB-D and segmentation images in an iterative fashion until all holes are filled, with each iteration focusing on one viewpoint. At each iteration/viewpoint, we render both an RGB-D image and a segmentation map relative to the current view and fill the produced holes using 2D inpainting. The recovered pixels are re-projected to 3D color and semantic labeled points and used for the next iteration. Our approach has two issues: First, different choices of sequences of viewpoints strongly affect the quality of final results because given a partial point cloud, different visible contexts captured from myriad perspectives present various levels of difficulties in the completion task, producing diverse prediction accuracies; moreover, selecting a larger number of views for the sake of easier inpainting to fill smaller holes in each iteration will lead to error accumulation in the end. Thus we need a policy to determine the next best view as well as the appropriate number of selected viewpoints. Second, although existing deep learning based approaches [8], [14], [15], [16] show excellent performance for image completion, directly applying them to RGB-D and segmentation maps across different viewpoints usually yields inaccurate and inconsistent reconstructions. The reason is because of lack of global and local context understanding. To address the first issue, we employ a reinforcement learning optimization strategy for view path planning. In particular, the current state is defined as the updated point cloud after the previous iteration and the action space is spanned by a set of pre-sampled viewpoints chosen to maximize 3D content recovery. The policy that maps the current state to the next action is approximated by a multi-view convolutional neural network (MVCNN) [17] for classification. The second issue is handled by a volume-guided view completion deepnet. It combines the 2D inpainting network [8], [16] and another 3D completion network [1] to form a joint learning machine. In it low-resolution volumetric results of the 3D net are projected and concatenated to inputs of the 2D net, lending better global context information to segmentation image inpainting, and the completed segmentation map will lead better local structure information to RGB-D image inpainting. At the same time, losses from the 2D net are back-propagated to the 3D stream to benefit its optimization and further help improve the quality of 2D outputs. As demonstrated in our experimental results, the proposed joint learning machine significantly outperforms existing methods quantitatively and qualitatively.\n\n\nContributions\n\nExisting methods of deep learning semantic scene completion from single (RGB-)D image are always given scene models with volumetric representation which is of low resolution. Our previous work [7] offers a preliminary attempt at outputting high resolution results, point cloud, when given a single depth map, by adopting an end-to-end volumeguided progressive view inpainting method. Initially, our goal is to reconstruct a complete 3D point cloud scene only from a single RGB-D image, and try to understand the scene as much as possible, such as recovering its color and semantic information. After completing the prediction of the geometric information, in this presented paper, we begin to estimate the color and semantic information of the generated points by inputting RGB image. Inspired by [8], considering that the semantic information will help both geometry and completion completion task, we design to use the semantic information of the point cloud as an intermediate result rather than the final output of the network. In particular, we add an RGB image inpainting branch to the volume-guided image inpainting module, and a semantic segmentation prediction and completion module to the algorithm as an additional guided input for both geometry and color completion task. Also, we replace the reinforcement learning algorithm from DQN to A3C for better storage efficiency and more robust performance. In summary, our contributions are as follows.\n\n\u2022 The first surface-generated algorithm for colored semantic point cloud scene completion from a single RGB-D image by directly predicting the coordinates of the missing points (scene geometry information) and its colored and semantic labels. Both quantitative and qualitative results show the effectiveness of color and semantic information for helping point cloud geometric scene completion task.\n\n\u2022 A novel deep reinforcement learning strategy for determining the optimal sequence of viewpoints for progressive colored semantic scene completion. Ablation studies prove the effectiveness of the reward functions and the superiority of the view path planning strategy itself compared to choosing viewpoints in an orderly manner.\n\n\u2022 A volume-guided view inpainting module that not only produces high-resolution outputs but also makes full use of the global context. The projected volumetric segmentation and depth map improve the performance of both inpainting network and volume completion network by end-to-end training procedure.\n\n\nRELATED WORKS\n\nMany prior works are related to scene completion. The literature review is conducted in the following aspects.\n\nGeometry Completion Geometry completion has a long history in 3D processing, known for cleaning up broken single objects or incomplete scenes. Small holes can be filled by primitives fitting [18], [19], smoothness minimization [20], [21], [22], or structures analysis [23], [24], [25]. These methods however seriously depend on prior knowledge. Template or part based approaches can successfully recover the underlying structures of a partial input by retrieving the most similar shape from a database, matching with the input, deforming disparate parts and assembling them [25], [26], [27], [28]. However, these methods require manually segmented data, and tend to fail when the input does not match well with the template due to the limited capacity of the database. Recently, deep learning based methods have gained much attentions for shape completion [28], [29], [30], [31], [32], [33], while scene completion from sparse observed views remains challenging due to large-scale data loss in occluded regions. Song et al. [1] first propose an end-toend network based on 3DCNNs, named SSCNet, which takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. ScanComplete [34] extends it to handle larger scenes with varying spatial extent. Wang et al. [6] combine it with an adversarial mechanism to make the results more plausible, and propose a novel architecture in 2019 [35], named ForkNet, that leverages a shared embedding encoding both geometric and semantic surface cues, as well as multiple generators designed to deal with limited paired data. Zhang et al. [2] apply a dense CRF model followed with SSCNet to further increase the accuracy. In order to exploit the information of input images, Garbade et al. [5] adopt a two stream neural network, leveraging both depth information and semantic context features extracted from the RGB images. Guo et al. [3] present a view-volume CNN which extracts detailed geometric features from the 2D depth image and projects them into a 3D volume to assist completed scene inference. Li et al. [36] propose a novel anisotropic convolution module using in 3D convolution network which can handle the object variations in the semantic scene completion from an RGB-D image. Chen et al. [37] propose a novel 3D sketch-aware feature embedding scheme which explicitly embeds geometric information with structure-preserving details. However, all these works based on the volumetric representation result in low-resolution outputs. In this paper, we directly predict point cloud to achieve high-resolution completion by conducting inpainting on multi-view 2D images.\n\nImage Inpainting Similar to geometry completion, researchers have employed various priors or optimized models to complete a depth image [38], [39], [40], [41], [42], [43], [44], [45]. The patch-based image synthesis idea is also applied [46], [47]. Recently, significant progresses have been achieved in image inpainting field with deep convolutional networks and generative adversarial networks (GANs) for regular or free-form holes [15], [16], [48], [49], [50]. Zhang et al. [51] imitate them with a deep end-to-end model for depth inpainting. Compared with inpainting task on irregular holes , recovering missing information in our task is more challenging due to the holes are always created by mutual occlusion of objects. Shih et al. [52] present a learning-based inpainting model that synthesizes new local color-and-depth content into the occluded region in a spatial context-aware manner, which is similar to our work. But they only inpaint the holes guided by 2D context features, which may lead some unreasonable results. To address it, an additional 3D global context is provided in our paper, guiding the inpainting on diverse views to reach more accurate and consistent output.\n\nDeep Learning On Point Cloud Deep learning has been introduced to various point cloud processing tasks to improve performance of algorithms, such as classification, segmentation, object generation and completion. Our CSSC task belongs to point cloud scene completion and segmentation. Achlioptas et al. [53] introduce the first deep generative model for the point cloud. Due to the model architecture is not primarily built to do geometry completion task, the completion performance is not considered ideal. Yuan et al. [54] propose the first learning-based architecture, named point The pipeline of our method. Given a single RGB-D image I 0 &D 0 , we first predict its segmentation map S 0 , then we convert I 0 , D 0 , S 0 to a colored semantic labeled point cloud P 0 , here shown in two different views. View path planning module is used to seek the next-best-view v 1 , under which the point cloud is projected to a new RGB-D image I 1 &D 1 and a new segmentation image S 1 , causing holes. In parallel, the P 0 is also completed in volumetric space by SSCNet, resulting in V c . Under the viewpoint v 1 , V c is projected to D c 1 , S c 1 using for guiding the inpainting of I 1 , D 1 , S 1 with image inpainting module. Repeating this process several times, we can achieve the final high-quality colored semantic scene completion.\n\ncompletion network (PCN), focusing on shape completion task. PCN applies the Folding operation [55] to approximate a relatively smooth surface and conduct shape completion. Recently, Sarmad et al. [56] propose a reinforcement learning agent controlled GAN based network (RL-GAN-Net) for real-time point cloud completion. The RL agent used in RL-GAN-Net avoids complex optimization and accelerates the prediction process, and it does not focus on enhancing the accuracy of the points. Huang et al. [57] propose a learningbased approach, point fractal network (PF-Net), for precise and high-fidelity point cloud completion. PF-Net preserves the spatial arrangements of the incomplete point cloud and can predict the detailed geometrical structure of the missing region(s) in the prediction. However, all these works only solve the object-level point completion task, but not for the larger scale situation. To address it, we project the large scale point clouds to multi-view 2D images and achieve the pointcloud completion by image inpainting.\n\nThe semantic segmentation on point clouds is the expansion of 2D-Image domains. Huang et al. [58] first propose 3D fully convolutional neural networks(3D-FCNN) which predicts coarse voxel-level semantic label. PointNet [59] and following works [60], [61] use multi-layer perception (MLP) to predict higher resolution point-level segmentation results. Pham et al. [62] propose a semantic-instance segmentation method that jointly performs both of the tasks via a novel multi-task pointwise network and a multi-value conditional random field model. Wang et al. [63] design a novel graph attention convolution (GAC) framework with learnable kernel shape for structured feature learning of 3D point cloud and apply it to train an end-to-end network for semantic segmentation task. Jiang et al. [64] explore semantic relation between each point and its contextual neighbors through edges, and propose a hierarchical pointedge interaction network with point branch as well as edge branch. All these works take the point cloud as the input and design different networks to treat the 3D points directly. However, our approach only takes a single RGB-D image and outputs a completed and colored semantic labeled point cloud.\n\nView Path Planing Projecting a scene or an object to the image plane will severely cause information loss because of self-occusions. A straightforward solution is utilizing dense views for making up [17], [65], [66], yet it will lead to heavy computation cost. Choy et al. [67] propose a 3D recurrent neural networks to integrate information from multi-views which decreases the number of views to five or less. Even so, how many views are sufficient for completion and which views are better to provide the most informative features, are still open questions. Optimal view path planning, as the problem to predict next best view from current state, has been studied in recent years. It plays critical roles for scene reconstruction as well as environment navigation in autonomous robotics system [68], [69], [70], [71]. Most recently, this problem is also explored in the area of objectlevel shape reconstruction [72]. A learning framework is designed in [73], by exploiting the spatial and temporal structure of the sequential observations, to predict a view sequence for groundtruth fitting. Our work explores the approaches of view path planning for scene completion. We propose to train an asynchronous advantage actor-critic (A3C) algorithm [74] to choose the best view sequence in a reinforcement learning framework.\n\n\nMETHODOLOGY\n\n\nOverview\n\nThe insight behind our approach is that viewing an incomplete 3D point cloud from different viewpoints should give us more information about the incompleteness than viewing it under a fixed angle. Therefore, an effective way to fix the incompleteness is sequentially fixing the observed holes under different viewpoints until we cannot obverse any hole. Our main idea is projecting an incomplete point cloud into multi-view 2D images(include RGB image, depth map and segmentation map) and performing 2D inpainting tasks on them. Then, the restored 2D images are converted back to the 3D point cloud to obtain a complete colored semantic point cloud. In order to ensure 2D image inpainting task have sufficient global and local information, we introduce the 3D volumetric occupancy to provide valuable global information, and we propose to leverage semantic segmentation on RGB-D images to provide local information.\n\nIn particular, taking an RGB-D image I 0 &D 0 as input, we first predict the semantic label for each pixel, which results in a semantic segmentation map S 0 , and convert I 0 , D 0 , S 0 to a colored semantic labeled (CSL) point cloud P CSL 0 , which suffers from severe data loss. Our goal is to generate 3D points to complete P CSL 0 's lack of geometry information, and predict the inferred points' color and segmentation information at the same time. To take full advantage of the context information, we execute the inpainting operations view by view in an accumulative way, with inferred points for the current viewpoint kept and used to help inpainting of the next viewpoint. Assume I 0 , D 0 , S 0 is rendered from P CSL 0 under viewpoint v 0 , we start our completion procedure with a new view v 1 and render P CSL 0 under v 1 to obtain three new images I 1 , D 1 , S 1 , which potentially have many holes. We fill these holes with 2D inpainting, turning I 1 , D 1 , S 1 to\u00ce 1 ,D 1 ,\u015c 1 . The inferred depth pixels inD 1 are then converted to 3D points, attached color information and segmentation label from\u00ce 1 ,\u015c 1 , and aggregated with P CSL 0 to output a denser point cloud P CSL 1 . This procedure is repeated for a sequence of new viewpoints v 2 , v 3 , ..., v n , yielding point clouds P CSL 2 , P CSL 3 , ..., P CSL n , with P CSL n being our final output. Fig. 2 depicts the overall pipeline of our proposed algorithm. Since P CSL n depends on the view path v 2 , v 3 , ..., v n , we describe in section 3.3 a deep reinforcement learning framework to seek the best view path. Before that, we introduce our solution to another critical problem of 2D inpainting, i.e., transforming\nI i , D i , S i to\u00ce i ,D i ,\u015c i , in section 3.2 first.\n\nVolume-guided Image Inpainting\n\nDeep Convolutional Neural Network (CNN) has been widely utilized to effectively extract context features for image inpainting tasks, achieving excellent performance. Although it can be directly applied to each viewpoint independently, this simplistic approach will lead to inconsistencies across views because of lack of global context understandings. We propose a volume-guided view inpainting framework by first conducting completion in the voxel space, converting point cloud P 's volumetric occupancy grid V to its semantic labeled completed version V c . Denote the projected depth map from V c to the view v i as D c i , segmentation map from V c as S c i . As shown in Fig. 2 and Fig. 3, this is implemented using a fivemodule neural network architecture consisting of a volume completion network, a segmentation inpainting network, an RGB inpainting network, a depth inpainting network, and a differentiate projection layer connecting them. In addition, there is a semantic segmentation network before doing image inpainting. The details of each module and our training strategy are described below.\n\n\nRGB-D Semantic Segmentation\n\nIn our implementation, DANet [75] is used as our RGB-D semantic segmentation network, which takes 640 \u00d7 480 I 0 &D 0 and outputs the same size S 0 . Volume Completion We employ SSCNet proposed in [1] to map V to V c for volume completion. SSCNet predicts not only volumetric occupancy but also the semantic labels for each voxel. Such a multi-task learning scheme helps us better capture object-aware context features and contributes to higher accuracy. The readers are referred to [1] for details on how to set up this network architecture. We retain the multi-task output as [1], where the resolution of input is 240 \u00d7 144 \u00d7 240, and the output is 60 \u00d7 36 \u00d7 60 \u00d7 12 indicating the probability of the grid corresponding to 12 labels (excluding empty label).\n\n\nSegmentation and RGB-D Inpainting\n\nAs we have predicted the volume completion V c , we need to use this global information to better inpaint the segmentation and RGB-D images rendered in different viewpoints, so as to gradually complete the point cloud scene P CSL 0 . In particular, we first complete the holes in segmentation map S i , and then using it to guide the inpainting of RGB-D images I i &D i . Among various existing approaches, PartialCNN [16] is chosen to handle our case with holes of irregular shapes for segmentation and depth inpainting, and StructureFlow [8] is chosen to better predict the color information through flow for RGB inpainting. Specifically, for segmentation inpainting, S i , I i , D i and S c i are first concatenated to form a map. The resulting map is then fed into a U-Net structure implemented with a masked and re-normalized convolution operation (also called partial convolution), followed by an automatic mask-updating step. The output\u015c i is also in 640\u00d7480. Identically, for depth inpainting, D i , I i , D c i and the completed segmentation map S i are concatenated as the input, andD i is the output in 640\u00d7480. As for RGB inpainting, I i , D c i ,\u015c i are concatenated as a map to feed into a U-Net color generator and also into a encoder to generate appearance flow. Guided by the flow information, colors are predicted from regions with similar structures. The output\u00ce i is also with size 640\u00d7480. We refer the readers to [8], [16] for details of the architecture settings and the design of loss functions. Projection Layer As validated in our experiments described in section 4.3.1, the projection of V c greatly benefits inpainting of 2D maps. We further exploit the benefit of 2D inpainting to volume completion by propagating the 2D loss back to optimize the parameters of 3D CNNs. Doing so requires a differentiable projection layer. There are two options for the implementation of this layer: the technique proposed in [76] and the homography warping method in [77]. The first one is chosen for a more accurate projection. Thus, we connect V c and D c i , S c i using this layer. For the sake of notational convenience, we use V to represent V c ,D to represent D c i and S to represent S c i . Specifically, for each pixel x in D or S, we launch a ray that starts from the viewpoint v i , passes through x, and intersects a sequence of voxels in V , noted as l 1 , l 2 , ..., l Nx . We denote the value of the k th voxel's first channel in V as V k , which represents the probability of this voxel belong to empty label, and denote the value of other channels as s k . Then, we define the depth value of this pixel x as\nD(x) = Nx k=1 P x k d k ,(1)\nwhere d k is the distance from the viewpoint to voxel l k and P x k the probability of the ray corresponding to x first meets the l k voxel\nP x k = (1 \u2212 V k ) k\u22121 j=1 V j , k = 1, 2, ..., N x .(2)\nThe segmentation value of the pixel x as\nS(x) = Nx k=1 P x k \u00b7 sof t arg max(s k ).(3)\nThe derivative of D(x) with respect to V k can be calculated as\n\u2202D(x) \u2202V k = Nx i=k (d i+1 \u2212 d i ) 1\u2264t\u2264i,t =k V t .(4)\nAnd the derivative of S(x) with respect to V k can be calculated as\n\u2202S(x) \u2202V k = Nx i=k (u i+1 \u2212 u i ) 1\u2264t\u2264i,t =k V t \u00b7 \u2202u(\u00b7) \u2202s(\u00b7) ,(5)\nwhere u i = sof t arg max(s i ). The derivative of S(x) with respect to s k can be calculated as\n\u2202S(x) \u2202s k = P x k \u00b7 \u2202u(\u00b7) \u2202s(\u00b7) .(6)\nThis guarantees back propagation of the projection layer. In order to speed up implementation, the processing of all rays are implemented in parallel via GPUs. Joint Training Training the five components together from scratch is tricky, and training convergence and stability are not guaranteed. Instead, the proposed inpainting network is trained as follows: 1) We pre-train both the RGB-D semantic segmentation network and the volume completion network independently. 2) With fixed parameters of the RGB-D semantic segmentation network and the volume completion network, we train the segmentation inpainting network. 3) We train depth inpainting network and RGB inpainting network with fixed parameters of the other three networks independently. 4) Once we have all pre-trained models for the five networks, we train the entire network jointly.\n\nThe training data are generated based on the 3D-FUTURE synthetic scene dataset provided in [78]. Please go Sec. 4 for more generation details.\n\n\nProgressive Scene Completion\n\nGiven an incomplete point cloud P CSL 0 that is converted from I 0 , D 0 , S 0 with respect to view v 0 , we describe in this subsection how to obtain the optimal next view sequence v 1 , v 2 , ..., v n . The problem is defined as a Markov decision process (MDP) consisting of state, action, reward, and an agent which takes actions during the process. The agent inputs the current state, outputs the corresponding optimal action, and receives the most reward from the environment. We train our agent using A3C [74], an algorithm of deep reinforcement learning. We adopt the A3C as our viewpoint sequence planning method for the reason that off-policy reinforcement learning methods like DQN [79] take up additional memory space caused by the experience replay, and at the same time A3C can achieve faster training speed and better viewpoint path selection because of the more diverse data brought by multi-agent parallelism. The ablation study results prove that the A3C is more suitable for our complex tasks than DQN. The definitions of the proposed MDP and the training procedure are given below. State We define the state as the updated point cloud at each iteration, with the initial state being P CSL 0 . As the iteration continues, the state for performing completion on the i th view is P CSL i\u22121 , which is accumulated from all previous iteration updates. Action Space The action at the i th iteration is to determine the next best view v i . To ease the training process and support the use of A3C, we evenly sample a set of scene-centric camera views to form a discrete action space. Specifically, we first place P CSL 0 in its bounding sphere and keep it upright. At this point, v 0 is located on z-y plane, and camera points to the coordinate system origin. Then, two circle paths are created for both the equatorial and 70-degree latitude line. In our experiments, 20 camera views are uniformly selected on these two paths, 10 per circle. All views are facing to the center of the bounding sphere. We fixed these views for all training samples. The set of 20 views is denoted as C = {c 1 , c 2 , ..., c 20 }. In particular, a viewpoint is defined as:\n\nx = a sin \u03b8 sin \u03c6, y = a cos \u03b8, z = a sin \u03b8 cos \u03c6,  Fig. 4, all viewpoints are facing to the scene center and a is set to 3m in our experiments. Reward An reward function is commonly unitized to evaluate the result for an action executed by the agent. In our work, at the i th iteration, the inputs are three incomplete maps I i , D i , S i rendered from P CSL i\u22121 under view v i chosen in the action space C. The result of the agent action are corresponding inpainted image\u00ce i ,D i ,\u015c i . Hence the accuracy of this inpainting operation can be used as the primary rewarding strategy. It can be measured by the mean error of the pixels inside the holes between\u00ce i ,D i ,\u015c i and its ground truth I gt i , D gt i , S gt i . All the ground truth maps are pre-rendered from 3D-FUTURE dataset. Thus we define the award function as\nR acc i = \u2212 1 3|\u2126| (L 1 \u2126 (D i , D gt i ) + L 1 \u2126 (\u00ce i , I gt i ) + L 1 \u2126 (\u015c i , S gt i )),(8)\nwhere L 1 denotes the L 1 loss, \u2126 the set of pixels inside the holes, and |\u2126| the number of pixels inside \u2126.\n\nIf we only use the above reward function R acc i , the agent tends to change the viewpoint slightly in each action cycle, since doing this results in small holes. However, this incurs higher computational cost while accumulating errors. We thus introduce a new reward term to encourage inferring more missing points at each step. This is implemented by measuring the percentage of filled original holes. To do so, we need to calculate the area of missing regions in an incomplete point cloud P , which is not trivial in a 3D space. Therefore, we project P under all camera views to the action space C and count the number of pixels inside the generated holes in each rendered image. The sum of these numbers is denoted as Area h (P ) for measuring the area. We thus define the new reward term as\nR hole i = Area h (P i\u22121 ) \u2212 Area h (P i ) Area h (P 0 ) \u2212 1(9)\nto avoid the agent from choosing the same action as in previous steps. We further define a termination criterion to stop view path search by Area h (P i )/Area h (P 0 ) < 7%, which means that all missing points of P 0 have been nearly recovered. We set the reward for terminal to 1. However, the functions R acc i and R hole i are focused on the quality of the inpainted maps in 2D space only. Therefore, the chosen viewpoint may not directly enforce predicting more accurate 3D points. To this end, we introduce point cloud recover reward function R pcacc i , and it is defined as: \nR pcacc i = 1 N N j=1 f (p j , P GT ), p j \u2208P CSL i ,(10)\nwhere, N is the total number of points inP CSL i ;P CSL i is a 3D point set and it contains all recovered 3D points from iteration i. In other words,\nP CSL i = P CSL i\u22121 +P CSL i\n; P GT is the ground truth point cloud; L(\u00b7) represents the segmentation label of a point. Intuitively, given a predicted 3D point p j , we calculate the closest point to p j in P GT , marked as q. If the predicted semantic label of p j (i.e., L(p j )) is the same as the semantic label of the closet point q, we then count p j as a correct prediction. Otherwise, it is counted as an incorrect prediction. R pcacc i estimates the percentage of the correct predictions among all predictions. Combining all three reward functions, the final reward function is:\nR total i = \u03b1R acc i + \u03b2(R pcacc i \u2212 1) + \u03b3R hole i ,(12)\nwhere \u03b1, \u03b2, \u03b3 are the balancing weights. A3C Training Our A3C is built upon MVCNN [17]. It consists of n local networks and a global network, which have same architecture but time difference in network parameters updating. These local networks are trained in parallel to update the parameters of the global network. It takes mutilview RGB-D and segmentation maps projected from P i\u22121 as inputs and outputs the value of input state v \u03b8v (P ) from critic branch as well as action probability distribution \u03c0 \u03b8 (P ) from actor branch. The whole network is trained to approximate the correct probability \u03c0 \u03b8 (v i |P i\u22121 ) for taking action v i and the value function v \u03b8v (P i\u22121 ), which is the expected reward that the agent receives when taking action v i at state P i\u22121 .\n\n\nThe loss function for training actor is\nLoss(\u03b8) = E[log \u03c0 \u03b8 (v i |P i\u22121 )\u00b7(R total i +\u03b4v \u03b8v (P i )\u2212v \u03b8v (P i\u22121 ))](13)\nwhere \u03b4 is a discount factor. The loss function for training critic is\nLoss(\u03b8 v ) = E[(R total i + \u03b4v \u03b8v (P i ) \u2212 v \u03b8v (P i\u22121 )) 2 ](14)\nFinally, our loss function for training A3C is\nL our = Loss(\u03b8) + Loss(\u03b8 v ).(15)\nOur network structure is shown in Fig. 4. After a view pooling layer and a fully-connected layer, we obtain a 512-D vector, which is split evenly into two parts to learn the action probability distribution \u03c0 \u03b8 (P ) and the state value function v(P ). Finally, the agent choose the action v i for current state P i\u22121 based on policy \u03c0 \u03b8 (P i\u22121 ). In the end, we reach the decision on maps I i , D i , S i for inpainting.\n\n\nEXPERIMENTAL RESULTS\n\nDataset The dataset we used to train our 2DCNN and A3C is generated from 3D-FUTURE [78]. 3D-FUTURE contains 16,563 unique detailed 3D instances of furniture with highresolution textures of 5,000 different rooms. Using the camera view generation method provided by [78], we generated a total of 2,541 camera viewpoints in 1,000 random rooms and rendered 2,541 RGB-D images from scene models under these camera viewpoints. After filtering the above images and removing the unqualified camera viewpoints as Song et al. [1], we finally obtained 1,582 valid camera viewpoints and their corresponding initial RGB-D scene images. Among them, 1,431 images are randomly selected using for training and the remaining 151 for testing. As for ground truth data, we performed frustum cutting of each room model based on those valid camera viewpoints (the distance to the camera is set to 0.1m and 6m for the near and far clipping plane, respectively). After generating these local scene mesh model, we sampled around 800,000 points for each model using Poisson Disk Sampling method [80] and then voxelized each point cloud at different resolutions by Voxel Down Sampling algorithm in Open3D library [81]. These ground truth point clouds and voxels will be used for the training of the relevant networks and the calculation of the evaluation metrics. Specifically, for 2DCNN, we first re-projected 1,582 RGB-D images into initial point clouds according to their corresponding viewpoints. Then for each initial viewpoint v 0 , we generated 20 viewpoints around it using the similar method as in Sec. 3.3 to avoid causing large holes and to ensure that sufficient contextual information is available in the learning process. After projecting the initial point clouds and ground truth point clouds under these viewpoints, we got 1, 582 \u00d7 20 = 31, 640 pairs of images for training and testing our image inpainting and view path planning networks. In addition, we added noise based on the standard normal distribution to the input depth images, for simulating the image taken by the sensor in the real world.\n\nFor the real data, we use the ScanNet V2 dataset [82], which contains more than 1,500 scanned scenes with cluttered object placement. We randomly selected 100 scenes and calculated 1-3 initial views for each scene according to the complexity of the scene following Song et al. [1], and finally obtained 153 real RGB-D scans. The ground truth point cloud and voxel of each local scan are generated in similar way as on the 3D-FUTURE dataset. These data are only used for testing our method and related methods. Implementation Details Our network architecture is implemented in PyTorch. The provided pre-trained model of SSCNet [1] is used to initialize parameters of our 3DCNN part. It takes around 60 hours to train the depth inpainting network , RGB inpainting network and segmentation inpainting network on our training dataset separately and 20 hours to fine-tune the whole network after the addition of projection layer. During A3C training process, we set the number n of local networks to 3, the weight \u03b1, \u03b2, \u03b3 for reward calculation to 0.05, 1, 0.1 and the discount factor \u03b4 to 0.9. Compared to 5 days used for training DQN, training A3C only takes 3 days and running our complete algorithm once takes about 100s which adopts 5.31 viewpoints on average. Metrics The evaluation metrics will be divided into two aspects: geometric completion and semantic segmentation. The Chamfer Distance (CD) [83] is used as one of our metrics for evaluate the accuracy of our generated point set P , compared with the ground truth point cloud P GT . Following Sung et al. [25], we also use C r (Completeness) and A r (Acuraccy) to evaluate how complete and accuracy of the generated result. We define it as:\nC r (P, P GT ) = |{d(x, P ) < r|x \u2208 P GT }| |{y|y \u2208 P GT }| (16) A r (P, P GT ) = |{d(x, P GT ) < r|x \u2208 P }| |{y|y \u2208 P }|(17)\nwhere d(x, P ) denotes the distance from a point x to a point set P , |\u00b7| denotes the number of the elements in the set, and r means the distance threshold. In our experiments, we report the C r and A r w.r.t five different r (0.02, 0.04, 0.06, 0.08, 0.10 are used). We also use the same semantic categories as the ones used in [1] for our semantic segmentation validation. The 11 classes are of varying shapes and sizes, and they are: L = {ceiling, floor, wall, window, chair, bed, sofa, table, tvs, furniture, and other objects}. One issue here is that the reconstructed 3D points are not guaranteed to be well aligned with ground truth 3D points. Therefore it is difficult to define True Positive (TP), False Positive (FP) and False Negative (FN) to calculate Intersection over Union(IoU) directly with IoU = T P/(T P + F P + F N ). For this reason, we choose to voxelize P and P GT with different resolution to achieve an one-to-one correspondence between P and P GT with minimal loss of accuracy. Specifically, followed by Song et al. [1], the size of the local scene model is defined as 4.8m \u00d7 2.44m \u00d7 4.8m. Thus, given different length of voxel grid's edge one can get voxel models of different resolution. Given P and grid's edge length e, we first divide the space into combinations of grids, and then traverse every points in P to accumulate the number of points in each grid and compute its corresponding semantic label. In particular, assume that the vertex of the grid g with minimum coordinates is p g = (x g , y g , z g ), the semantic of grid l g can be defined as:\nl g = arg max l\u2208L (num(g, l)) num(g, l) = |p|p \u2208 g, L(p) = l| ,(18)\nwhere p \u2208 g denotes the point p is inside the grid g. If there exists two or more labelsL = {l 1 , l 2 , ...} with the same number num(g, l), then the calculation of l g will be updated as:\nl g = L(arg min p (dist(p, p g )|L(p) \u2208L)),(19)\nwhere dist(p, p g ) denotes the European distance between point p and p g . For choosing an appropriate grid edge length e, we calculate the CD between point cloud and its voxelization result as well as the voxelization time w.t.r six different e (0.08, 0.06, 0.04, 0.02, 0.01, 0.005 are used, equal to the resolution of 60\u00d736\u00d760, 80\u00d748\u00d780, ..., 960\u00d7488\u00d7960), as shown in Fig. 5. After balancing accuracy and efficiency, we finally exclude the case that e = 0.005 and calculate the IoU for semantic segmentation on the other five e. Following Song et al. [1], we do not evaluate on points outside the viewing frustum or the room. Baselines This article is an extension of our previous CVPR work [7], thus one baseline is our previous work which is defined as Geometry Scene Completion(GSC). As shown in Tab. 1, the input of GSC is only a single depth map, and the reinforcement learning network is DQN [79]. Also, we propose another baseline Colored Scene Completion(CSC) which introduce the RGB image into the algorithm and output a completed scene point cloud with plentiful color. The network compositions of GSC and CSC are similar to CSSC. In particular, GSC is composed of a volume completion network(SSCNet [1]), a depth inpainting network(PartialCNN [16]), a projection layer connecting them and a MVCNN [17] in DQN for determining the best geometry scene completion viewpoint path. The definition of DQN's component are almost same with A3C as mentioned in Sec. 3.3, but only using R acc and R hole as its reward functions.The DQN takes mutil-view depth maps projected from P i\u22121 as inputs and outputs the Q-value of different actions. The whole network is trained to approximate the action-value function Q(P i\u22121 , v i ), which is the expected reward that the agent receives when taking action v i at state P i\u22121 . To ensure stability of the learning process, we introduce a target network separated from the architecture of [79], whose loss function for training DQN is (20) where r is the reward, \u03b3 a discount factor, and \u03b8 the parameters of the target network. For effective learning, we create an experience replay buffer to reduce the correlation between data. The buffer stores the tuples (P i\u22121 , v i , r, P i ) proceeded with the episode. We also employ the technique of [84] to remove upward bias caused by max vi+1 Q(P i , v i+1 ; \u03b8 ) and change the loss function to\nLoss(\u03b8) = E[(r + \u03b3 max vi+1 Q(P i , v i+1 ; \u03b8 ) \u2212 Q(P i\u22121 , v i ; \u03b8)) 2 ].L our = E[(r + \u03b3Q(P i , arg max vi+1 Q(P i , v i+1 ; \u03b8); \u03b8 ) \u2212 Q(P i\u22121 , v i ; \u03b8)) 2 ].(21)\nAs for CSC baseline, the RGB information is introduced to the geometric completion method to predict the color of generated result. Compared with GSC, the differences are as follows: 1) We add an RGB image inpainting branch in depth inpainting module; 2) We update the reward function R acc using in DQN by adding the L 1 loss between inpainted RGB image and its ground truth.\n\nThe only difference between CSSC baseline and our final approach is that: DQN is the algorithm used in View Path Planning Module of CSSC baseline while A3C is Ours.\n\n\nQuantitative Comparisons\n\nWe validate the effectiveness of the proposed approach by comparing the state-of-the-art methods, including SSCNet [1], VVNet [3] and ForkNet [35]. For fair evaluation, we first retrained the above 3 methods on the 3D-FUTURE dataset using their respective hyperparameter settings mentioned in [1], [3], [35]. In particular, since the generated training dataset of 3D-FUTURE only contains 1,431 images, training these networks on this dataset alone is insufficient. As a solution already used in [35], we initialize parameters of these 3 networks by the provided pre-trained model, and then refine them by supplementing the training data from 3D-FUTURE with 1,400 randomly selected samples from their original datasets in each epoch of training. After finishing training process, we first render the volume obtained from SSCNet, VVNet and ForkNet to several depth maps under the same viewpoints as our method. We then convert these depth maps to point cloud.\n\nThe results on the 3D-FUTURE dataset are shown in Tab. 1. As seen, our approach outperforms all the others on CD and Completeness metrics. Since the Accuracy metric measures the accuracy of the prediction points, a high A r with low C r indicates that the algorithm tends to output only the points with high confidence and ignore the completeness of the model. This also validates that the using of volumetric representation greatly reduces the quality of the outputs. Considering that it may be unfair to convert voxel predictions to point cloud, we convert our point cloud result to voxels by judging whether there is a point within the grid. However, compared to voxel results output from SSCNet, VVNet and ForkNet, our predicted point cloud only provides surface information. It means converting a point cloud to voxel and directly comparing to the corresponding voxel ground truth is also unfair. Therefore, to enforce fairness, we transform our reconstructed point cloud to voxel and compare them with the corresponding ground truth that is on the surface only.\n\nThese voxel-based results are reported in Tab. 2 evaluated by each class and average IoU for semantic segmentation task. Following Song et al. [1], we also report the scene completion performance of related methods by treating all non-empty object class as one category and evaluating IoU of the binary predictions. Except classes ceil and f loor, our approach provides more accurate results compared with voxel-based methods. It also can be observed that the IoU of voxel-based methods increase significantly when e increases from 0.02 to 0.04 or from 0.04 to 0.06, which indicates that the predictions of such methods are far from the ground truth surface of the scene and sufficient accuracy can only be obtained at the expense of high resolution. In contrast, our performance grows more smoothly, indicating that our generated points are closer to the ground truth scene surface.\n\nWe also report different tasks' completion results in Tab 1: Geometric Scene Completion(GSC) which only completes the 3D points, Colored Scene Completion(CSC) which will also output the completed points' colored labels, Colored Semantic Scene Completion(CSSC) which outputs both the colored label and semantic label of the point but uses DQN as its view planning method and Ours which output the same as CSSC with A3C as the RL method. And we can draw a conclusion: with the input of more information and the output of multi-tasking and the adjustment of the algorithm structure, the performance of scene completion is continuously improving.\n\nThe comparison of results on the ScanNet dataset are shown in Tab. 3 and Tab. 4. The first observation is that the performance of all methods on the real data degrades, especially for SSCNet and VVNet. It is more clearly observed in the visualization results in Fig. 8 that these two methods are no longer available. This also proves that ForkNet and our method have the better generalization property. Since the ground truth models in the ScanNet dataset do not contain ceilings, the IoU of class ceil. in Tab. 4 are all zeros.\n\n\nQualitative Comparisons\n\nThe visual comparisons of these methods are shown in Fig.  6 and Fig. 8. It can be seen that, the generated point clouds from voxel-based methods are of no surface details. Our proposed method not only produces more accurate results, but also predicts the reasonable colors for generating points. This can be validated in Tab 1 and shown in Fig 6. In addition, by conducting completion in multiple views, our approach also recovers more missing points, showing better completeness as validated in Tab 1. We show the colored labeled completion results of CSC in Fig. 12. We can observe how additional information and view planing method influence the completion performance from the perspective of visualization. We also show our volumetric results which are converting from point cloud in Fig. 7.  \n\n\nAblation Studies\n\nTo ensure the effectiveness of several key components of our system, we do some control experiments by removing each component.\n\n\nOn Image Inpainting\n\nWe ablate our 2D image inpainting method with different configurations as shown in Tab. 5 ,Tab. 6, Fig. 9, Fig. 10 and Fig. 11. The BN in Tab. 6 denotes Backbone, and P.C. for P artConv, S.F. for StrucF low. The P BP in Tab. 5 and Tab. 6 denotes whether the volume-guided 2D inpainting network is trained with or without projection back-propagation.   We use the metrics of L 1 \u2126 , P SN R and SSIM for the RGB-D inpainting comparisons, IoU s for semantic inpainting comparisons. The quantitative results are reported in Tab. 5 and Tab. 6, from which we observe that: Case2 v.s. Case3, Case8 v.s. Case9: A key idea in StrucFlow is to use appearance flow to sample features from regions with similar structures [8], which means that the network could easily predict the color detail if it can refer to areas with the same structure. This can explain the following situation: The RGB inpainting network using StructureFlow baseline is better than the one using PartialCNN baseline, but at the same time the depth inpainting network using PartialCNN is better then StructureFlow. Areas with similar structures in RGB images generally have similar colors, but areas with similar structures in depth images may indeed have large differences in depth values. From Case1 to Case5: With the increase of effective in-put(incomplete RGB image, predicted segmentation image and depth map rendered from voxel completion result), the effect of depth map completion gradually improves. Especially in scene one ( Fig. 9 top 2rows), the network can gradually predict the information behind the hole in the back of the chair(inside the red dashed box). From Case7 to Case10: With the increase of effective input(incomplete depth image and predicted segmentation image), the effect of RGB image completion gradually improves. From Case12 to Case16: With the increase of effective input(incomplete depth image, incomplete RGB image and and segmentation map rendered from voxel completion result), the effect of segmentation image completion gradually improves. Case6, Case11 and Case17: The projection layer used for joint training do help increase the accurate of RGB-  D image inpainting. But the improvement of segmentation inpainting is not obvious, this may be because the semantic segmentation map contains a lot of structure information, and it is not suitable to use the method of inpainting ordinary images to complete it. Combine with the previous observation, the efficacy of the volume guidance is evaluated.\n\n\nOn Action Space Setting\n\nIn order to determine the effect of different (\u03b8, \u03c6) on the completion performance, we additionally set 3 new sets of action spaces and retrain the A3C algorithm, and the final reconstruction results are shown in Tab. 7. As can be seen, \u03b8 has little effect on the reconstruction performance ( Ours v.s. Casea and Caseb v.s. Casec), while a too small \u03c6 will lead to a worse situation ( Ours v.s. Caseb and Casea v.s. Casec).\n\n\nOn View Path Planning\n\nWithout using reinforcement learning for path planning, there exists a straightforward way to do completion: we can uniformly sample views from C with different steps and directly perform image inpainting on them. In this uniform manner, three methods with three different numbers of views are evaluated. We denote them as U 5 , U 10 and U 20 (with steps 4, 2 and 1). In addition, we also train a new A3C with only the reward function R acc i , denoted as Ours w/o.hole , and Ours w/o.3D without the reward function R pcacc i . Visual comparison results on some sampled scenes are shown in Fig. 12 and Fig. 14, where our proposed model results in much better appearances than others. The selected viewpoints of the first 6 steps for some randomly chosen scenes are shown in Fig. 13, where we can find that different reinforcement learning strategies will perform different view path planning for the same scene. The quantitative results are reported in Tab 1, from which we observe that: U 5 v.s. U 10 v.s. U 20 v.s. Ours: An optimal sequence of viewpoints determined by a reinforcement learning algorithm will help scene color completion more effectively than a serial way. It is not that the more viewpoints for view inpainting, the better the reconstruction quality. A reasonable RL agent can ensure the scene color completion quality as well as complete the scene faster. From Tab. 1 we can observe that: although the CD value is getting smaller as the number of viewing angles increases, A r is also decreasing at the same time. This shows that a large number of viewpoints brings full coverage of the scene, but this also leads to an increase in noisy points which affects the generation of correct position points. Ours w/o.hole v.s. Ours: The reward function R hole i is efficient of scene color completion process, and we also observe that Ours w/o.hole chooses 6.28 viewpoints on average since it tends to pick views with small holes for higher R acc i . Ours w/o.3D v.s. Ours: With the reward function R pcacc i , the CD is decreased compared to the one without the function, and the completed points will look closer to the surface of the real scene. CSSC v.s. Ours: For complex problems like viewpoint path planning, reinforcement learning algorithms that consider both actions and policy (A3C) will be superior to action-based methods (DQN ). This is demonstrated in the following aspects: 1) Shorter training time. Training the DQN in CSSC took about 5 days and only 3 days to train the A3C. 2) Smaller selected viewpoints on average. As shown in Fig. 13(a), the average number of viewpoints chosen by CSSC is 7.83 and 5.31 of ours. 3) More diverse viewpoint options. As illustrated in Fig. 13(c,d), A3C will choose different start points for different scenes, while DQN basically focused the start point around v 17 and v 18 . 4) Better reconstruction performance. Whether for the CD, C r and A r metrics in Tab. 1 or for the Comp. and semantic IoU in Tab. 2, our approach outperforms CSSC almost across the board.\n\n\nLimitations\n\nAlthough our method have a good reconstruction performance, however, as shown in Tab. 8, the biggest problem is the long inference time. Since our method has to render 2D segmentation and RGB-D images from the point cloud multiple times, a significant chunk of the running time is spent on rendering. Therefore using a faster rendering tool such as Blender can speed up our approach. The large number of   parameter size is due to the fact that our method consists of multiple 2D image processing networks. A possible solution is to consider model lightweighting. Our method can currently only complete objects within the viewing frustum.\n\nIn the future, we will consider segmenting the objects at the boundary of the frustum and performing object-level completion to get a more complete and reasonable model.   \n\n\nCONCLUSION\n\nIn this paper, we propose the first surface-generated approach for colored semantic point cloud scene completion from a single RGB-D image. The missing 3D points are inferred by conducting completion on multi-view depth maps and its missing color and semantic labels by RGB and segmentation maps. To guarantee a more accurate and consistent output, a volume-guided view image inpianting network is proposed. In addition, a deep reinforcement learning framework is devised to seek the optimal view path to contribute the best result in accuracy. The experiments demonstrate that our model is the best choice and significantly outperforms existing methods. There is a research direction worth further exploration in the future: how to post-process the generated point cloud to guarantee the completion is watertight.\n\nFig. 1 .\n1Surface-generated Colored Semantic Point Cloud Scene Completion. (a) A single-view RGB-D image as input; (b) Visible surface from the RGB-D image, which is represented as the point cloud. In our paper, the color of depth map is for visualization only; (c) Our scene completion result with color label: directly recovering the missing points of the occluded regions; (d) Another view of our result labeled with segmentation.\n\nFig. 2 .\n2Fig. 2. The pipeline of our method. Given a single RGB-D image I 0 &D 0 , we first predict its segmentation map S 0 , then we convert I 0 , D 0 , S 0 to a colored semantic labeled point cloud P 0 , here shown in two different views. View path planning module is used to seek the next-best-view v 1 , under which the point cloud is projected to a new RGB-D image I 1 &D 1 and a new segmentation image S 1 , causing holes. In parallel, the P 0 is also completed in volumetric space by SSCNet, resulting in V c . Under the viewpoint v 1 , V c is projected to D c 1 , S c 1 using for guiding the inpainting of I 1 , D 1 , S 1 with image inpainting module. Repeating this process several times, we can achieve the final high-quality colored semantic scene completion.\n\nFig. 3 .\n3Details of our volume-guided image inpainting module. The output of segmentation map inpainting network\u015c i will be input to both depth inpainting network and RGB inpainting network, as it can provide structure information.\n\n\nwhere \u03b8 \u2208 {70 \u2022 , 90 \u2022 } and \u03c6 \u2208 {\u221250 \u2022 , \u221240 \u2022 , \u221230 \u2022 , \u221220 \u2022 ,-10 \u2022 , 10 \u2022 , 20 \u2022 , 30 \u2022 , 40 \u2022 , 50 \u2022 }. At this point, [\u03b8, \u03c6] for c 1 is [90 \u2022 , \u221250 \u2022 ], [90 \u2022 , \u221240 \u2022 ] for c 2 ,[70 \u2022 , \u221240 \u2022 ] for c 11 and [70 \u2022 , 50 \u2022 ] for c 20 . As shown in\n\nFig. 4 .\n4The architecture of a single agent of our A3C. For a point cloud state, MVCNN is used to predict the best view for the next inpainting.\n\n\nL(p) = L(arg min q\u2208P p \u2212 q 2 2 ) 0, others,\n\n\nAt state P i\u22121 , we render at all viewpoints {c 1 , c 2 , ..., c 20 } in the action space C in 224 \u00d7 224 resolution and get the corresponding multi-view depth maps {D 1 i , D 2 i , ..., D 20 i }, RGB images {I 1 i , I 2 i , ..., I 20 i } and segmentation maps {S 1 i , S 2 i , ..., S 20 i }. These maps are first concatenated according to different viewpoints and then sent to the same CN N as inputs.\n\nFig. 5 .\n5(a) Visualization of voxelization results with different resolutions determined by different grid edge lengths. (b) The relationship between different e on result accuracy (evaluated by CD) and voxelization generating time.\n\nFig. 6 .\n6Comparisons against the state-of-the-arts on the 3D-FUTURE test set. Given different inputs and the referenced groundtruth, we show the semantic completion results of four modules, with the corresponding point cloud completeness and accuracy error maps below.Fig. 7. Comparisons against the state-of-the-arts with voxel representation.\n\nFig. 8 .\n8Scene semantic completion results against the state-of-the-arts on the ScanNet test set, with the corresponding point cloud completeness and accuracy error maps below.\n\nFig. 9 .\n9Comparisons on the variants of depth inpainting network. Given incomplete depth images, we show results of different case compared with the groundtruth. Both the inpainted map and its error map are shown.\n\nFig. 10 .\n10Comparisons on the variants of RGB inpainting network. Given incomplete RGB images, we show results of different case compared with the groundtruth. Both the inpainted map and its error map are shown.\n\nFig. 11 .\n11Comparisons on the variants of segmentation inpainting network. Given incomplete segmentation images, we show results of different case compared with the groundtruth.\n\nFig. 12 .\n12Comparisons on the variants of view path planning and different baselines. Given different inputs and the referenced groundtruth, we show the colored completion results of seven different approaches, with the corresponding completeness and accuracy error maps below.\n\nFig. 13 .\n13Ablation studies on view path planning. For all charts, the horizontal coordinate denotes the number of steps and the vertical coordinate indicates the index of the selected viewpoint. Only the first 6 viewpoint selections are shown in this figure. (a) The view selection paths for the same scene by the four reinforcement learning methods (A3C, A3C w/o.hole , A3C w/o.3d and DQN ). Also shown on the legend are the average numbers of selected viewpoints on the 3D-FUTURE test set. (b) The view selection paths of U 5 , U 10 and U 2 0. (c-f) Viewpoint selection results of different methods on the same 20 random scenes.\n\nFig. 14 .\n14Visualization of different chosen viewpoint paths for several methods. The last column is the completeness error map of the corresponding reconstruction results with C 0.02 marked on the side.\n\nTABLE 1\n1Quantitative semantic segmentation results in terms of scene completion IoU (Com.) and scene semantic IoU on the 3D-FUTURE test set.Quantitative comparisons against existing methods and ablation studies on the 3D-FUTURE test set. The CD metric, Cr (Completeness) and Ar \n(Accuracy) (w.r.t different thresholds) are used. The units of Cr and Ar are percentages. \n\nM ethods \nRGB \nSeg. \nRL \nCD \nCr/Ar(r = 0.02) Cr/Ar(0.04) Cr/Ar(0.06) Cr/Ar(0.08) Cr/Ar(0.10) \nSSCN et \n\u00d7 \n-\n2.2006 \n63.08/86.27 \n67.83/91.14 \n70.30/93.17 \n71.92/94.28 \n73.16/95.07 \nV V N et \n\u00d7 \n-\n2.1832 \n66.45/93.05 \n69.73/95.02 \n71.59/95.85 \n72.90/96.35 \n73.95/96.71 \nF orkN et \n\u00d7 \n-\n2.0818 \n67.09/91.05 \n70.46/93.92 \n72.27/95.12 \n73.60/95.88 \n74.56/96.38 \nGSC \n\u00d7 \n\u00d7 \nDQN \n0.5364 \n77.25/73.55 \n80.82/79.00 \n82.86/81.96 \n84.29/83.92 \n85.39/85.37 \nCSC \n\u00d7 \nDQN \n0.4503 \n77.62/77.04 \n81.72/78.61 \n84.06/82.38 \n85.66/84.93 \n86.88/86.78 \nCSSC \nDQN \n0.4200 \n78.73/78.99 \n82.55/84.96 \n84.71/87.99 \n86.21/89.93 \n87.36/91.27 \nOurs \nA3C \n0.4095 \n79.78/79.07 \n83.65/85.03 \n85.87/88.13 \n87.39/90.11 \n88.55/91.47 \nU 5 \n-\n0.4520 \n78.61/79.31 \n82.39/85.15 \n84.49/88.22 \n85.99/90.15 \n87.14/91.49 \nU 10 \n-\n0.4476 \n79.23/77.48 \n82.89/83.79 \n84.99/87.06 \n86.47/89.12 \n87.59/90.55 \nU 20 \n-\n0.4576 \n78.54/77.90 \n82.41/83.88 \n84.62/87.07 \n86.14/89.10 \n87.30/90.54 \nOurs w/o.hole \nA3C \n0.4508 \n78.58/77.77 \n82.51/83.99 \n84.70/87.25 \n86.23/89.35 \n87.37/90.81 \nOurs w/o.3D \nA3C \n0.4423 \n78.53/78.35 \n82.34/84.47 \n84.53/87.64 \n86.06/89.64 \n87.22/91.06 \n\nTABLE 2 \nM ethods \ne \nCom. \nceil. \nfloor \nwall \nwin. \nchair \nbed \nsofa \ntable \ntvs \nfurn. \nobjs. \nAvg. \n\nSSCNet [1] \n\ne = 0.01 \n1.17 \n0.08 \n0.16 \n1.35 \n0.27 \n0.22 \n0.07 \n0.18 \n0.13 \n0.46 \n0.59 \n0.51 \n0.36 \ne = 0.02 \n2.44 \n0.22 \n0.27 \n2.68 \n0.55 \n0.44 \n0.11 \n0.41 \n0.31 \n0.63 \n1.34 \n1.00 \n0.72 \ne = 0.04 \n4.22 \n0.70 \n0.80 \n4.12 \n0.95 \n0.81 \n0.21 \n0.71 \n0.72 \n0.84 \n2.07 \n1.83 \n1.25 \ne = 0.06 \n8.51 \n5.06 \n4.30 \n7.39 \n2.13 \n1.39 \n0.36 \n1.18 \n1.56 \n1.51 \n3.36 \n3.42 \n2.88 \ne = 0.08 \n14.85 \n0.99 14.21 \n6.29 \n2.28 \n2.34 \n1.19 \n2.60 \n3.87 \n1.27 \n5.55 \n4.80 \n4.13 \n\nVVNet [3] \n\ne = 0.01 \n1.33 \n0.04 \n0.19 \n1.76 \n0.37 \n0.23 \n0.09 \n0.21 \n0.19 \n0.32 \n0.47 \n0.68 \n0.41 \ne = 0.02 \n2.90 \n0.10 \n0.32 \n3.39 \n0.91 \n0.53 \n0.15 \n0.53 \n0.46 \n0.51 \n1.31 \n1.51 \n0.88 \ne = 0.04 \n5.26 \n0.31 \n0.92 \n5.60 \n1.57 \n1.04 \n0.33 \n0.97 \n1.32 \n0.78 \n2.19 \n2.69 \n1.61 \ne = 0.06 \n10.82 \n3.17 \n6.41 \n10.47 \n3.63 \n1.70 \n0.62 \n1.85 \n2.25 \n1.65 \n3.81 \n5.43 \n3.73 \ne = 0.08 \n18.68 \n0.48 21.73 \n8.85 \n3.44 \n2.84 \n1.79 \n3.39 \n4.89 \n1.29 \n6.59 \n7.49 \n5.71 \n\nForkNet [35] \n\ne = 0.01 \n2.32 \n2.77 \n1.70 \n2.45 \n0.28 \n0.51 \n0.23 \n0.39 \n1.26 \n0.26 \n1.11 \n1.39 \n1.12 \ne = 0.02 \n4.98 \n4.92 \n2.87 \n5.08 \n0.63 \n1.31 \n0.53 \n1.07 \n1.87 \n0.53 \n2.76 \n2.94 \n2.23 \ne = 0.04 \n16.68 \n6.05 45.31 10.77 \n1.57 \n2.54 \n1.51 \n1.96 \n3.62 \n1.73 \n5.13 \n6.20 \n7.85 \ne = 0.06 \n20.63 \n5.75 47.14 10.50 \n2.14 \n3.75 \n2.36 \n3.97 \n11.81 \n2.03 \n9.23 \n8.35 \n9.73 \ne = 0.08 \n27.87 \n6.93 44.44 20.91 \n4.89 \n4.80 \n2.68 \n5.01 \n7.67 \n6.05 \n11.16 11.70 \n11.47 \n\nCSSC \n\ne = 0.01 \n20.99 \n1.42 16.80 18.35 12.23 \n7.21 \n4.17 \n5.70 \n9.48 \n8.87 \n14.04 18.54 \n10.62 \ne = 0.02 \n25.49 \n3.40 20.71 21.10 14.63 \n7.84 \n5.18 \n7.24 \n10.24 11.17 17.54 21.56 \n12.78 \ne = 0.04 \n29.65 \n5.90 25.11 22.45 15.29 \n8.04 \n6.21 \n8.00 \n11.02 11.26 19.82 23.22 \n14.21 \ne = 0.06 \n32.25 \n6.16 26.43 23.71 16.24 \n8.20 \n6.23 \n8.53 \n11.28 11.42 20.94 23.57 \n14.79 \ne = 0.08 \n34.89 \n6.59 29.10 25.06 16.08 \n8.23 \n6.54 \n8.94 \n11.85 10.69 21.38 24.24 \n15.34 \n\nOurs \n\ne = 0.01 \n21.07 \n1.43 16.88 18.34 12.30 \n7.25 \n4.12 \n5.66 \n9.34 \n8.79 \n14.14 18.92 \n10.65 \ne = 0.02 \n26.02 \n3.55 20.91 21.50 15.18 \n8.03 \n5.21 \n7.34 \n10.33 11.11 17.66 22.26 \n13.01 \ne = 0.04 \n30.85 \n6.26 25.78 23.23 16.45 \n8.50 \n5.98 \n8.26 \n11.51 11.23 20.26 24.56 \n14.73 \ne = 0.06 \n33.89 \n6.65 27.26 24.65 18.20 \n8.79 \n6.46 \n8.93 \n11.91 11.72 21.97 25.44 \n15.63 \ne = 0.08 \n36.96 \n7.27 30.37 26.16 17.71 \n8.96 \n6.86 \n9.58 \n12.61 11.37 22.26 26.47 \n16.33 \n\n\n\nTABLE 3\n3Quantitative comparisons against existing methods and ablation studies on the ScanNet test set. The CD metric, Cr (Completeness) and Ar Quantitative semantic segmentation results in terms of scene completion IoU (Com.) and scene semantic IoU on the ScanNet test set.(Accuracy) (w.r.t different thresholds) are used. The units of Cr and Ar are percentages. \n\nM ethods \nRGB \nSeg. \nRL \nCD \nCr/Ar(r = 0.02) Cr/Ar(0.04) Cr/Ar(0.06) Cr/Ar(0.08) Cr/Ar(0.10) \nSSCN et \n\u00d7 \n-\n5.0607 \n40.17/22.97 \n50.76/30.70 \n57.77/35.47 \n62.85/38.87 \n66.67/41.50 \nV V N et \n\u00d7 \n-\n4.8375 \n40.07/56.12 \n48.27/66.87 \n53.89/71.61 \n58.31/74.22 \n61.87/76.03 \nF orkN et \n\u00d7 \n-\n2.1234 \n64.96/67.95 \n69.74/71.67 \n72.08/73.24 \n73.58/74.32 \n74.52/75.01 \nOurs \nA3C \n0.8635 \n69.02/62.88 \n71.91/66.59 \n73.22/68.62 \n74.08/69.94 \n74.66/70.85 \n\nTABLE 4 \nM ethods \ne \nCom. \nceil. \nfloor \nwall \nwin. chair \nbed \nsofa \ntable \ntvs \nfurn. \nobjs. \nAvg. \n\nSSCNet [1] \n\ne = 0.01 \n0.72 \n0.00 \n0.43 \n0.01 \n0.00 \n0.00 \n0.00 \n0.02 \n0.02 \n0.00 \n0.07 \n0.10 \n0.06 \ne = 0.02 \n1.33 \n0.00 \n0.92 \n0.01 \n0.00 \n0.00 \n0.00 \n0.05 \n0.04 \n0.00 \n0.13 \n0.17 \n0.12 \ne = 0.04 \n2.42 \n0.00 \n1.90 \n0.03 \n0.00 \n0.00 \n0.00 \n0.05 \n0.10 \n0.00 \n0.27 \n0.26 \n0.24 \ne = 0.06 \n3.24 \n0.00 \n2.47 \n0.05 \n0.00 \n0.00 \n0.01 \n0.06 \n0.16 \n0.00 \n0.40 \n0.25 \n0.31 \ne = 0.08 \n4.35 \n0.00 \n4.28 \n0.09 \n0.00 \n0.00 \n0.00 \n0.04 \n0.26 \n0.00 \n0.62 \n0.39 \n0.52 \n\nVVNet [3] \n\ne = 0.01 \n1.41 \n0.00 \n1.17 \n0.01 \n0.01 \n0.12 \n0.00 \n0.01 \n0.15 \n0.00 \n0.00 \n0.18 \n0.15 \ne = 0.02 \n2.40 \n0.00 \n2.27 \n0.02 \n0.00 \n0.19 \n0.00 \n0.01 \n0.22 \n0.00 \n0.00 \n0.33 \n0.28 \ne = 0.04 \n4.19 \n0.00 \n4.43 \n0.05 \n0.00 \n0.31 \n0.00 \n0.02 \n0.32 \n0.00 \n0.00 \n0.56 \n0.52 \ne = 0.06 \n5.47 \n0.00 \n5.85 \n0.03 \n0.00 \n0.40 \n0.00 \n0.01 \n0.35 \n0.00 \n0.00 \n0.87 \n0.68 \ne = 0.08 \n7.81 \n0.00 11.58 \n0.07 \n0.00 \n0.59 \n0.00 \n0.04 \n0.66 \n0.00 \n0.00 \n0.81 \n1.25 \n\nForkNet [35] \n\ne = 0.01 \n3.88 \n0.00 \n0.99 \n1.82 \n0.00 \n0.35 \n0.04 \n0.64 \n0.13 \n0.01 \n0.62 \n0.73 \n0.49 \ne = 0.02 \n7.13 \n0.00 \n1.94 \n2.72 \n0.01 \n0.65 \n0.08 \n1.18 \n0.27 \n0.00 \n1.19 \n1.35 \n0.85 \ne = 0.04 \n12.11 \n0.00 \n3.24 \n4.24 \n0.04 \n1.20 \n0.13 \n1.94 \n0.58 \n0.00 \n1.88 \n2.13 \n1.40 \ne = 0.06 \n16.93 \n0.00 \n5.79 \n4.68 \n0.04 \n1.79 \n0.28 \n2.48 \n1.09 \n0.19 \n2.48 \n2.77 \n1.96 \ne = 0.08 \n20.09 \n0.00 \n5.33 \n5.32 \n0.00 \n1.62 \n0.54 \n3.00 \n1.50 \n0.00 \n2.65 \n3.24 \n2.11 \n\nOurs \n\ne = 0.01 \n7.73 \n0.00 \n7.95 \n4.90 \n0.77 \n3.05 \n0.84 \n5.77 \n5.12 \n0.00 \n4.36 \n2.56 \n3.21 \ne = 0.02 \n12.74 \n0.00 14.31 \n9.01 \n1.16 \n4.99 \n1.68 \n9.32 \n8.38 \n0.00 \n8.05 \n4.19 \n5.55 \ne = 0.04 \n17.39 \n0.00 19.60 12.79 \n1.85 \n5.84 \n3.02 11.74 10.58 0.17 13.08 \n5.40 \n7.64 \ne = 0.06 \n20.22 \n0.00 22.05 14.69 \n1.54 \n6.76 \n2.94 12.39 12.11 0.38 15.95 \n6.28 \n8.64 \ne = 0.08 \n22.43 \n0.00 24.07 16.31 \n1.64 \n6.43 \n3.92 13.97 13.09 0.74 17.17 \n6.73 \n9.46 \n\n\n\nTABLE 5\n5Quantitative ablation studies on RGB-D image inpainting network.T ask \nV ersion \nBackbone \nSeg. Depth RGB \nV dep \nP BP \nL 1 \n\n\u2126 \n\nP SN R \nSSIM \n\nDepth \nInpainting \n\nCase1 \nP artConv \n\u00d7 \n-\n\u00d7 \n\u00d7 \n\u00d7 \n0.0805 \n25.52 \n0.9533 \nCase2 \nP artConv \n-\n\u00d7 \n\u00d7 \n\u00d7 \n0.0653 \n27.41 \n0.9512 \nCase3 \nStrucF low \n-\n\u00d7 \n\u00d7 \n\u00d7 \n0.0631 \n26.89 \n0.9443 \nCase4 \nP artConv \n-\n\u00d7 \n\u00d7 \n0.0615 \n27.31 \n0.9528 \nCase5 \nP artConv \n-\n\u00d7 \n0.0593 \n27.98 \n0.9582 \nCase6 \nP artConv \n-\n0.0543 \n28.56 \n0.9608 \n\nRGB \nInpainting \n\nCase7 \nP artConv \n\u00d7 \n\u00d7 \n-\n-\n\u00d7 \n0.1056 \n23.93 \n0.9064 \nCase8 \nP artConv \n\u00d7 \n-\n-\n\u00d7 \n0.0986 \n24.44 \n0.9124 \nCase9 \nStrucF low \n\u00d7 \n-\n-\n\u00d7 \n0.0683 \n25.77 \n0.8971 \nCase10 \nStrucF low \n-\n-\n\u00d7 \n0.0603 \n26.38 \n0.8947 \nCase11 \nStrucF low \n-\n-\n0.0587 \n27.03 \n0.9288 \n\n\n\nTABLE 6\n6Quantitative ablation studies on segmentation image inpainting network. 60.31 81.23 54.93 24.55 31.28 23.84 43.76 21.30 50.48 77.41 43.47 Case17 P.C. 9.21 58.77 79.44 51.18 26.31 34.40 22.31 42.59 29.72 52.40 76.50 43.89V ersion \nBN \nDep. RGB \nVseg \nP BP \nceil. \nfloor \nwall \nwin. \nchair \nbed \nsofa \ntable \ntvs \nfurn. \nobjs. \nAvg. \nCase12 \nP.C. \n\u00d7 \n\u00d7 \n\u00d7 \n\u00d7 \n7.63 56.28 79.44 50.28 25.57 33.39 21.56 41.28 26.67 51.80 74.44 42.58 \nCase13 \nS.F. \n\u00d7 \n\u00d7 \n\u00d7 \n\u00d7 \n7.55 59.23 75.38 50.47 27.88 30.31 20.11 39.57 32.14 52.09 69.72 42.22 \nCase14 \nP.C. \n\u00d7 \n\u00d7 \n\u00d7 \n9.08 58.77 80.16 48.41 25.88 32.13 22.16 42.42 29.17 51.28 75.66 43.19 \nCase15 \nP.C. \n\u00d7 \n\u00d7 \n9.09 59.36 80.57 47.89 26.03 32.11 21.93 42.55 28.51 51.42 76.01 43.22 \nCase16 \nP.C. \n\u00d7 \n9.08 \n\nTABLE 7\n7Quantitative ablation studies on different action space settings.Case a: \u03b8 \u2208 {60 \u2022 , 80 \u2022 } , \u03c6 \u2208 {\u221250 \u2022 , \u221240 \u2022 , ..., 10 \u2022 , 10 \u2022 , ..., 50 \u2022 }. Case b: \u03b8 \u2208 {70 \u2022 , 90 \u2022 } , \u03c6 \u2208 {\u221225 \u2022 , \u221220 \u2022 , ..., 5 \u2022 , 5 \u2022 , ..., 25 \u2022 }. Case c: \u03b8 \u2208 {60 \u2022 , 80 \u2022 } , \u03c6 \u2208 {\u221225 \u2022 , \u221220 \u2022 , ..., 5 \u2022 , 5 \u2022 , ..., 25 \u2022 }.CD \nC r=0.02 \nC 0.04 \nC 0.06 \nC 0.09 \nC 0.10 \nOurs \n0.4095 \n79.78 \n83.65 \n85.87 \n87.39 \n88.55 \nCase a \n0.4126 \n79.82 \n82.30 \n85.96 \n87.38 \n88.12 \nCase b \n0.5231 \n73.54 \n79.21 \n84.68 \n85.43 \n87.70 \nCase c \n0.5187 \n74.63 \n78.49 \n83.62 \n86.38 \n86.61 \n\n\n\nTABLE 8\n8The parameter size, running GPU memory and the inference time against the state-of-the-arts.P aram. \n(MB) \n\nGPU M em. \n(MB) \n\nInf.time \n(s) \nSSCN et \n3.7 \n420 \n0.56 \nV V N et \n42.9 \n1,600 \n0.73 \nF orkN et \n26.2 \n583 \n0.81 \nOurs \n1,264.8 \n8,107 \n99.61 \n\nBaocai Yin is a professor and doctoral advisor at Dalian University of Technology. He received his Ph.D. degree in computational mathematics from Dalian University of Technology(1990)(1991)(1992)(1993), where he also received his M.S. degree in computational mathematics(1985)(1986)(1987)(1988)and his B.S. degree in applied mathematics(1981)(1982)(1983)(1984)(1985). His research areas include digital multimedia technology, virtual reality and graphics technology, and multi-function perception technology. Xin Yang is a professor and doctoral advisor at Dalian University of Technology. He received his Ph.D. degree in computer science from Zhejiang University (2007-2012), and his B.S. degree in computer science from Jilin University(2003)(2004)(2005)(2006)(2007). His main research interests include computer graphics and vision, intelligent robot technology, focusing on the efficient expression, understanding, perception and interaction of scenes.\nSemantic scene completion from a single depth image. S Song, F Yu, A Zeng, A X Chang, M Savva, T A Funkhouser, IEEE Conference on Computer Vision and Pattern Recognition. S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. A. Funkhouser, \"Semantic scene completion from a single depth im- age,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 190-198.\n\nSemantic scene completion with dense crf from a single depth image. L Zhang, L Wang, X Zhang, P Shen, M Bennamoun, G Zhu, S A A Shah, J Song, Neurocomputing. 318L. Zhang, L. Wang, X. Zhang, P. Shen, M. Bennamoun, G. Zhu, S. A. A. Shah, and J. Song, \"Semantic scene completion with dense crf from a single depth image,\" Neurocomputing, vol. 318, pp. 182- 195, 2018.\n\nView-volume network for semantic scene completion from a single depth image. Y.-X Guo, X Tong, International Joint Conference on Artificial Intelligence. Y.-X. Guo and X. Tong, \"View-volume network for semantic scene completion from a single depth image,\" in International Joint Conference on Artificial Intelligence, 2018, pp. 726-732.\n\nSee and think: Disentangling semantic scene completion. S Liu, Y Hu, Y Zeng, Q Tang, B Jin, Y Han, X Li, Neural Information Processing Systems. S. Liu, Y. Hu, Y. Zeng, Q. Tang, B. Jin, Y. Han, and X. Li, \"See and think: Disentangling semantic scene completion,\" in Neural Information Processing Systems, 2018, pp. 261-272.\n\nTwo stream 3d semantic scene completion. M Garbade, Y.-T Chen, J Sawatzky, J Gall, IEEE Conference on Computer Vision and Pattern Recognition Workshops. M. Garbade, Y.-T. Chen, J. Sawatzky, and J. Gall, \"Two stream 3d semantic scene completion,\" in IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2019, pp. 416-425.\n\nAdversarial semantic scene completion from a single depth image. Y Wang, D J Tan, N Navab, F Tombari, IEEE International Conference on 3D Vision. Y. Wang, D. J. Tan, N. Navab, and F. Tombari, \"Adversarial semantic scene completion from a single depth image,\" in IEEE International Conference on 3D Vision, 2018, pp. 426-434.\n\nDeep reinforcement learning of volumeguided progressive view inpainting for 3d point scene completion from a single depth image. X Han, Z Zhang, D Du, M Yang, J Yu, P Pan, X Yang, L Liu, Z Xiong, S Cui, IEEE Conference on Computer Vision and Pattern Recognition. X. Han, Z. Zhang, D. Du, M. Yang, J. Yu, P. Pan, X. Yang, L. Liu, Z. Xiong, and S. Cui, \"Deep reinforcement learning of volume- guided progressive view inpainting for 3d point scene completion from a single depth image,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 234-243.\n\nStructureflow: Image inpainting via structure-aware appearance flow. Y Ren, X Yu, R Zhang, T H Li, S Liu, G Li, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionY. Ren, X. Yu, R. Zhang, T. H. Li, S. Liu, and G. Li, \"Structure- flow: Image inpainting via structure-aware appearance flow,\" in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 181-190.\n\nAn interactive approach to semantic modeling of indoor scenes with an rgbd camera. T Shao, W Xu, K Zhou, J Wang, D Li, B Guo, ACM Transactions on Graphics. 316T. Shao, W. Xu, K. Zhou, J. Wang, D. Li, and B. Guo, \"An interactive approach to semantic modeling of indoor scenes with an rgbd camera,\" ACM Transactions on Graphics, vol. 31, no. 6, pp. 136:1-136:11, 2012.\n\nAutomatic semantic modeling of indoor scenes from low-quality rgb-d data using contextual information. K Chen, Y.-K Lai, Y.-X Wu, R Martin, S.-M Hu, ACM Transactions on Graphics. 33612K. Chen, Y.-K. Lai, Y.-X. Wu, R. Martin, and S.-M. Hu, \"Auto- matic semantic modeling of indoor scenes from low-quality rgb-d data using contextual information,\" ACM Transactions on Graphics, vol. 33, no. 6, pp. 208:1-208:12, 2014.\n\nAligning 3d models to RGB-D images of cluttered scenes. S Gupta, P A Arbel\u00e1ez, R B Girshick, J Malik, IEEE Conference on Computer Vision and Pattern Recognition. S. Gupta, P. A. Arbel\u00e1ez, R. B. Girshick, and J. Malik, \"Aligning 3d models to RGB-D images of cluttered scenes,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 4731-4740.\n\nPredicting complete 3d models of indoor scenes. R Guo, C Zou, D Hoiem, arXiv:1504.02437R. Guo, C. Zou, and D. Hoiem, \"Predicting complete 3d models of indoor scenes,\" arXiv:1504.02437, 2015.\n\nPixel2mesh: 3d mesh model generation via image guided deformation. N Wang, Y Zhang, Z Li, Y Fu, H Yu, W Liu, X Xue, Y.-G Jiang, IEEE Transactions on Pattern Analysis and Machine Intelligence. N. Wang, Y. Zhang, Z. Li, Y. Fu, H. Yu, W. Liu, X. Xue, and Y.-G. Jiang, \"Pixel2mesh: 3d mesh model generation via image guided deformation,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\n\nContext encoders: Feature learning by inpainting. D Pathak, P Krahenbuhl, J Donahue, T Darrell, A A Efros, IEEE Conference on Computer Vision and Pattern Recognition. D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, \"Context encoders: Feature learning by inpainting,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2536-2544.\n\nGlobally and locally consistent image completion. S Iizuka, E Simo-Serra, H Ishikawa, ACM Transactions on Graphics. 364107S. Iizuka, E. Simo-Serra, and H. Ishikawa, \"Globally and lo- cally consistent image completion,\" ACM Transactions on Graphics, vol. 36, no. 4, p. 107, 2017.\n\nImage inpainting for irregular holes using partial convolutions. G Liu, F A Reda, K J Shih, T.-C Wang, A Tao, B Catanzaro, arXiv:1804.07723G. Liu, F. A. Reda, K. J. Shih, T.-C. Wang, A. Tao, and B. Catanzaro, \"Image inpainting for irregular holes using partial convolutions,\" arXiv:1804.07723, 2018.\n\nMultiview convolutional neural networks for 3d shape recognition. H Su, S Maji, E Kalogerakis, E G Learned-Miller, IEEE International Conference on Computer Vision. H. Su, S. Maji, E. Kalogerakis, and E. G. Learned-Miller, \"Multi- view convolutional neural networks for 3d shape recognition,\" in IEEE International Conference on Computer Vision, 2015.\n\nCompletion and reconstruction with primitive shapes. R Schnabel, P Degener, R Klein, Wiley Computer Graphics Forum. R. Schnabel, P. Degener, and R. Klein, \"Completion and recon- struction with primitive shapes,\" in Wiley Computer Graphics Fo- rum, 2009, pp. 503-512.\n\nGlobfit: Consistently fitting primitives by discovering global relations. Y Li, X Wu, Y Chrysathou, A Sharf, D Cohen-Or, N J Mitra, ACM Transactions on Graphics. 30452Y. Li, X. Wu, Y. Chrysathou, A. Sharf, D. Cohen-Or, and N. J. Mi- tra, \"Globfit: Consistently fitting primitives by discovering global relations,\" ACM Transactions on Graphics, vol. 30, no. 4, p. 52, 2011.\n\nLeast-squares meshes. O Sorkine, D Cohen-Or, IEEE Shape Modeling Applications. O. Sorkine and D. Cohen-Or, \"Least-squares meshes,\" in IEEE Shape Modeling Applications, 2004, pp. 191-199.\n\nA robust hole-filling algorithm for triangular mesh. W Zhao, S Gao, H Lin, Springer The Visual Computer. 23W. Zhao, S. Gao, and H. Lin, \"A robust hole-filling algorithm for triangular mesh,\" Springer The Visual Computer, vol. 23, no. 12, pp. 987-997, 2007.\n\nScreened poisson surface reconstruction. M Kazhdan, H Hoppe, ACM Transactions on Graphics. 32329M. Kazhdan and H. Hoppe, \"Screened poisson surface reconstruc- tion,\" ACM Transactions on Graphics, vol. 32, no. 3, p. 29, 2013.\n\nPartial and approximate symmetry detection for 3d geometry. N J Mitra, L J Guibas, M Pauly, ACM Transactions on Graphics. 253N. J. Mitra, L. J. Guibas, and M. Pauly, \"Partial and approximate symmetry detection for 3d geometry,\" ACM Transactions on Graph- ics, vol. 25, no. 3, pp. 560-568, 2006.\n\nApproximate symmetry detection in partial 3d meshes. I Sipiran, R Gregor, T Schreck, Wiley Computer Graphics Forum. I. Sipiran, R. Gregor, and T. Schreck, \"Approximate symmetry detection in partial 3d meshes,\" in Wiley Computer Graphics Forum, 2014, pp. 131-140.\n\nData-driven structural priors for shape completion. M Sung, V G Kim, R Angst, L Guibas, ACM Transactions on Graphics (TOG). 346M. Sung, V. G. Kim, R. Angst, and L. Guibas, \"Data-driven struc- tural priors for shape completion,\" ACM Transactions on Graphics (TOG), vol. 34, no. 6, pp. 1-11, 2015.\n\nStructure recovery by part assembly. C.-H Shen, H Fu, K Chen, S.-M Hu, ACM Transactions on Graphics. 316180C.-H. Shen, H. Fu, K. Chen, and S.-M. Hu, \"Structure recovery by part assembly,\" ACM Transactions on Graphics, vol. 31, no. 6, p. 180, 2012.\n\nLearning part-based templates from large collections of 3d shapes. V G Kim, W Li, N J Mitra, S Chaudhuri, S Diverdi, T Funkhouser, ACM Transactions on Graphics. 32470V. G. Kim, W. Li, N. J. Mitra, S. Chaudhuri, S. DiVerdi, and T. Funkhouser, \"Learning part-based templates from large collec- tions of 3d shapes,\" ACM Transactions on Graphics, vol. 32, no. 4, p. 70, 2013.\n\nCompleting 3d object shape from one depth image. J Rock, T Gupta, J Thorsen, J Gwak, D Shin, D Hoiem, IEEE Conference on Computer Vision and Pattern Recognition. J. Rock, T. Gupta, J. Thorsen, J. Gwak, D. Shin, and D. Hoiem, \"Completing 3d object shape from one depth image,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2484-2493.\n\nA field model for repairing 3d shapes. D Thanh Nguyen, B.-S Hua, K Tran, Q.-H Pham, S.-K Yeung, IEEE Conference on Computer Vision and Pattern Recognition. D. Thanh Nguyen, B.-S. Hua, K. Tran, Q.-H. Pham, and S.-K. Yeung, \"A field model for repairing 3d shapes,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 5676-5684.\n\nVconv-dae: Deep volumetric shape learning without object labels. A Sharma, O Grau, M Fritz, Springer European Conference on Computer Vision. A. Sharma, O. Grau, and M. Fritz, \"Vconv-dae: Deep volumetric shape learning without object labels,\" in Springer European Confer- ence on Computer Vision, 2016, pp. 236-250.\n\nShape completion enabled robotic grasping. J Varley, C Dechant, A Richardson, J Ruales, P Allen, IEEE Intelligent Robots and Systems. J. Varley, C. DeChant, A. Richardson, J. Ruales, and P. Allen, \"Shape completion enabled robotic grasping,\" in IEEE Intelligent Robots and Systems, 2017, pp. 2442-2447.\n\nShape completion using 3d-encoder-predictor cnns and shape synthesis. A Dai, C R Qi, M Nie\u00dfner, IEEE Conference on Computer Vision and Pattern Recognition. A. Dai, C. R. Qi, and M. Nie\u00dfner, \"Shape completion using 3d- encoder-predictor cnns and shape synthesis,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 6545-6554.\n\nHighresolution shape completion using deep neural networks for global structure and local geometry inference. X Han, Z Li, H Huang, E Kalogerakis, Y Yu, IEEE International Conference on Computer Vision. X. Han, Z. Li, H. Huang, E. Kalogerakis, and Y. Yu, \"High- resolution shape completion using deep neural networks for global structure and local geometry inference,\" in IEEE Interna- tional Conference on Computer Vision, 2017, pp. 85-93.\n\nScancomplete: Large-scale scene completion and semantic segmentation for 3d scans. A Dai, D Ritchie, M Bokeloh, S Reed, J Sturm, M Nie\u00dfner, IEEE Conference on Computer Vision and Pattern Recognition. A. Dai, D. Ritchie, M. Bokeloh, S. Reed, J. Sturm, and M. Nie\u00dfner, \"Scancomplete: Large-scale scene completion and semantic seg- mentation for 3d scans,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 4578-4587.\n\nForknet: Multibranch volumetric semantic completion from a single depth image. Y Wang, D J Tan, N Navab, F Tombari, IEEE International Conference on Computer Vision. Y. Wang, D. J. Tan, N. Navab, and F. Tombari, \"Forknet: Multi- branch volumetric semantic completion from a single depth im- age,\" in IEEE International Conference on Computer Vision, 2019, pp. 8608-8617.\n\nAnisotropic convolutional networks for 3d semantic scene completion. J Li, K Han, P Wang, Y Liu, X Yuan, IEEE Conference on Computer Vision and Pattern Recognition. J. Li, K. Han, P. Wang, Y. Liu, and X. Yuan, \"Anisotropic con- volutional networks for 3d semantic scene completion,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 3351-3359.\n\n3d sketch-aware semantic scene completion via semi-supervised structure prior. X Chen, K.-Y Lin, C Qian, G Zeng, H Li, IEEE Conference on Computer Vision and Pattern Recognition. X. Chen, K.-Y. Lin, C. Qian, G. Zeng, and H. Li, \"3d sketch-aware semantic scene completion via semi-supervised structure prior,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 4193-4202.\n\nDepth map inpainting under a second-order smoothness prior. D Herrera, J Kannala, J Heikkil\u00e4, Springer Scandinavian Conference on Image Analysis. D. Herrera, J. Kannala, J. Heikkil\u00e4 et al., \"Depth map inpainting under a second-order smoothness prior,\" in Springer Scandinavian Conference on Image Analysis, 2013, pp. 555-566.\n\nGuided inpainting and filtering for kinect depth maps. J Liu, X Gong, J Liu, IEEE International Conference on Pattern Recognition. J. Liu, X. Gong, and J. Liu, \"Guided inpainting and filtering for kinect depth maps,\" in IEEE International Conference on Pattern Recognition, 2012, pp. 2055-2058.\n\nDepth-based inpainting for disocclusion filling. S M Muddala, M Sjostrom, R Olsson, IEEE 3DTV-Conference. S. M. Muddala, M. Sjostrom, and R. Olsson, \"Depth-based inpaint- ing for disocclusion filling,\" in IEEE 3DTV-Conference, 2014, pp. 1-4.\n\n3d aware correction and completion of depth maps in piecewise planar scenes. A K Thabet, J Lahoud, D Asmar, B Ghanem, Springer Asian Conference on Computer Vision. A. K. Thabet, J. Lahoud, D. Asmar, and B. Ghanem, \"3d aware correction and completion of depth maps in piecewise planar scenes,\" in Springer Asian Conference on Computer Vision, 2014, pp. 226-241.\n\nAn improved edge detection algorithm for depth map inpainting. W Chen, H Yue, J Wang, X Wu, Elsevier Optics and Lasers in Engineering. 55W. Chen, H. Yue, J. Wang, and X. Wu, \"An improved edge detection algorithm for depth map inpainting,\" Elsevier Optics and Lasers in Engineering, vol. 55, pp. 69-77, 2014.\n\nBuilding scene models by completing and hallucinating depth and semantics. M Liu, X He, M Salzmann, Springer European Conference on Computer Vision. M. Liu, X. He, and M. Salzmann, \"Building scene models by completing and hallucinating depth and semantics,\" in Springer European Conference on Computer Vision, 2016, pp. 258-274.\n\nDepth image inpainting: Improving low rank matrix completion with low gradient regularization. H Xue, S Zhang, D Cai, IEEE Transactions on Image Processing. 269H. Xue, S. Zhang, and D. Cai, \"Depth image inpainting: Improving low rank matrix completion with low gradient regularization,\" IEEE Transactions on Image Processing, vol. 26, no. 9, pp. 4311-4320, 2017.\n\nProbability contour guided depth map inpainting and superresolution using non-local total generalized variation. H.-T Zhang, J Yu, Z.-F Wang, Springer Multimedia Tools and Applications. 77H.-T. Zhang, J. Yu, and Z.-F. Wang, \"Probability contour guided depth map inpainting and superresolution using non-local total generalized variation,\" Springer Multimedia Tools and Applications, vol. 77, no. 7, pp. 9003-9020, 2018.\n\nFilling large holes in lidar data by inpainting depth gradients. D Doria, R J Radke, IEEE Conference on Computer Vision and Pattern Recognition Workshops. D. Doria and R. J. Radke, \"Filling large holes in lidar data by inpainting depth gradients,\" in IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2012, pp. 65-72.\n\nDepth-based image completion for view synthesis. J Gautier, O Le Meur, C Guillemot, IEEE 3DTV Conference. J. Gautier, O. Le Meur, and C. Guillemot, \"Depth-based image completion for view synthesis,\" in IEEE 3DTV Conference, 2011, pp. 1-4.\n\nFree-form image inpainting with gated convolution. J Yu, Z Lin, J Yang, X Shen, X Lu, T S Huang, arXiv:1806.03589J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, \"Free-form image inpainting with gated convolution,\" arXiv:1806.03589, 2018.\n\nRecurrent feature reasoning for image inpainting. J Li, N Wang, L Zhang, B Du, D Tao, IEEE Conference on Computer Vision and Pattern Recognition. J. Li, N. Wang, L. Zhang, B. Du, and D. Tao, \"Recurrent feature reasoning for image inpainting,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 7760-7768.\n\nPrior guided gan based semantic inpainting. A Lahiri, A K Jain, S Agrawal, P Mitra, P K Biswas, IEEE Conference on Computer Vision and Pattern Recognition. 705A. Lahiri, A. K. Jain, S. Agrawal, P. Mitra, and P. K. Biswas, \"Prior guided gan based semantic inpainting,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 13 696-13 705.\n\nDeep depth completion of a single rgb-d image. Y Zhang, T Funkhouser, IEEE Conference on Computer Vision and Pattern Recognition. Y. Zhang and T. Funkhouser, \"Deep depth completion of a single rgb-d image,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 175-185.\n\n3d photography using context-aware layered depth inpainting. M.-L Shih, S.-Y Su, J Kopf, J.-B Huang, IEEE Conference on Computer Vision and Pattern Recognition. M.-L. Shih, S.-Y. Su, J. Kopf, and J.-B. Huang, \"3d photography using context-aware layered depth inpainting,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 8028-8038.\n\nLearning representations and generative models for 3d point clouds. P Achlioptas, O Diamanti, I Mitliagkas, L Guibas, P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, \"Learning representations and generative models for 3d point clouds,\" 2017.\n\nPcn: Point completion network. W Yuan, T Khot, D Held, C Mertz, M Hebert, 2018 International Conference on 3D Vision. W. Yuan, T. Khot, D. Held, C. Mertz, and M. Hebert, \"Pcn: Point completion network,\" in 2018 International Conference on 3D Vision, 2018.\n\nFoldingnet: Point cloud auto-encoder via deep grid deformation. Y Yang, C Feng, Y Shen, D Tian, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionY. Yang, C. Feng, Y. Shen, and D. Tian, \"Foldingnet: Point cloud auto-encoder via deep grid deformation,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 206-215.\n\nRl-gan-net: A reinforcement learning agent controlled gan network for real-time point cloud shape completion. M Sarmad, H J Lee, Y M Kim, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionM. Sarmad, H. J. Lee, and Y. M. Kim, \"Rl-gan-net: A reinforcement learning agent controlled gan network for real-time point cloud shape completion,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.\n\nPf-net: Point fractal network for 3d point cloud completion. Z Huang, Y Yu, J Xu, F Ni, X Le, Z. Huang, Y. Yu, J. Xu, F. Ni, and X. Le, \"Pf-net: Point fractal network for 3d point cloud completion,\" 2020.\n\nPoint cloud labeling using 3d convolutional neural network. J Huang, S You, 2016 23rd International Conference on Pattern Recognition. J. Huang and S. You, \"Point cloud labeling using 3d convolutional neural network,\" in 2016 23rd International Conference on Pattern Recognition, 2016.\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. C R Qi, H Su, K Mo, L J Guibas, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionC. R. Qi, H. Su, K. Mo, and L. J. Guibas, \"Pointnet: Deep learning on point sets for 3d classification and segmentation,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion, 2017, pp. 652-660.\n\nPointnet++: Deep hierarchical feature learning on point sets in a metric space. C R Qi, L Yi, H Su, L J Guibas, Advances in neural information processing systems. C. R. Qi, L. Yi, H. Su, and L. J. Guibas, \"Pointnet++: Deep hierarchical feature learning on point sets in a metric space,\" in Advances in neural information processing systems, 2017.\n\nExploring spatial context for 3d semantic segmentation of point clouds. F Engelmann, T Kontogianni, A Hermans, B Leibe, Proceedings of the IEEE International Conference on Computer Vision Workshops. the IEEE International Conference on Computer Vision WorkshopsF. Engelmann, T. Kontogianni, A. Hermans, and B. Leibe, \"Explor- ing spatial context for 3d semantic segmentation of point clouds,\" in Proceedings of the IEEE International Conference on Computer Vision Workshops, 2017.\n\nJsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields. Q.-H Pham, T Nguyen, B.-S Hua, G Roig, S.-K Yeung, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionQ.-H. Pham, T. Nguyen, B.-S. Hua, G. Roig, and S.-K. Yeung, \"Jsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task pointwise networks and multi-value conditional random fields,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nGraph attention convolution for point cloud semantic segmentation. L Wang, Y Huang, Y Hou, S Zhang, J Shan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionL. Wang, Y. Huang, Y. Hou, S. Zhang, and J. Shan, \"Graph attention convolution for point cloud semantic segmentation,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.\n\nHierarchical point-edge interaction network for point cloud semantic segmentation. L Jiang, H Zhao, S Liu, X Shen, C.-W Fu, J Jia, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionL. Jiang, H. Zhao, S. Liu, X. Shen, C.-W. Fu, and J. Jia, \"Hier- archical point-edge interaction network for point cloud semantic segmentation,\" in Proceedings of the IEEE International Conference on Computer Vision, 2019.\n\nVolumetric and multi-view cnns for object classification on 3d data. C R Qi, H Su, M Nie\u00dfner, A Dai, M Yan, L J Guibas, IEEE Conference on Computer Vision and Pattern Recognition. C. R. Qi, H. Su, M. Nie\u00dfner, A. Dai, M. Yan, and L. J. Guibas, \"Vol- umetric and multi-view cnns for object classification on 3d data,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 5648-5656.\n\nMulti-view 3d models from single images with a convolutional network. M Tatarchenko, A Dosovitskiy, T Brox, Springer European Conference on Computer Vision. M. Tatarchenko, A. Dosovitskiy, and T. Brox, \"Multi-view 3d models from single images with a convolutional network,\" in Springer European Conference on Computer Vision, 2016, pp. 322-337.\n\n3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. C B Choy, D Xu, J Gwak, K Chen, S Savarese, Springer European Conference on Computer Vision. C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, \"3d- r2n2: A unified approach for single and multi-view 3d object reconstruction,\" in Springer European Conference on Computer Vision, 2016, pp. 628-644.\n\nAn adaptive hierarchical next-best-view algorithm for 3d reconstruction of indoor scenes. K.-L Low, A Lastra, Conference on Computer Graphics and Applications. K.-L. Low and A. Lastra, \"An adaptive hierarchical next-best-view algorithm for 3d reconstruction of indoor scenes,\" in Conference on Computer Graphics and Applications, 2006, pp. 1-8.\n\nData acquisition and view planning for 3-d modeling tasks. P S Blaer, P K Allen, IEEE Conference on Intelligent Robots and Systems. P. S. Blaer and P. K. Allen, \"Data acquisition and view planning for 3-d modeling tasks,\" in IEEE Conference on Intelligent Robots and Systems, 2007, pp. 417-422.\n\nDense scene reconstruction with points of interest. Q.-Y Zhou, V Koltun, ACM Transactions on Graphics. 324112Q.-Y. Zhou and V. Koltun, \"Dense scene reconstruction with points of interest,\" ACM Transactions on Graphics, vol. 32, no. 4, p. 112, 2013.\n\nQuality-driven poisson-guided autoscanning. S Wu, W Sun, P Long, H Huang, D Cohen-Or, M Gong, O Deussen, B Chen, ACM Transactions on Graphics. 336S. Wu, W. Sun, P. Long, H. Huang, D. Cohen-Or, M. Gong, O. Deussen, and B. Chen, \"Quality-driven poisson-guided au- toscanning,\" ACM Transactions on Graphics, vol. 33, no. 6, 2014.\n\nActive object reconstruction using a guided view planner. X Yang, Y Wang, Y Wang, B Yin, Q Zhang, X Wei, H Fu, ternational Joint Conference on Artificial Intelligence. X. Yang, Y. Wang, Y. Wang, B. Yin, Q. Zhang, X. Wei, and H. Fu, \"Active object reconstruction using a guided view planner,\" In- ternational Joint Conference on Artificial Intelligence, pp. 4965-4971, 2018.\n\n3d attention-driven depth acquisition for object identification. K Xu, Y Shi, L Zheng, J Zhang, M Liu, H Huang, H Su, D Cohen-Or, B Chen, ACM Transactions on Graphics. 356238K. Xu, Y. Shi, L. Zheng, J. Zhang, M. Liu, H. Huang, H. Su, D. Cohen-Or, and B. Chen, \"3d attention-driven depth acquisition for object identification,\" ACM Transactions on Graphics, vol. 35, no. 6, p. 238, 2016.\n\nAsynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T P Lillicrap, T Harley, D Silver, K Kavukcuoglu, ACM International Conference on Machine Learning. V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, \"Asynchronous methods for deep reinforcement learning,\" in ACM International Conference on Ma- chine Learning, 2016, pp. 1928-1937.\n\nDual attention network for scene segmentation. J Fu, J Liu, H Tian, Y Li, Y Bao, Z Fang, H Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJ. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, \"Dual attention network for scene segmentation,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3146-3154.\n\nMulti-view supervision for single-view reconstruction via differentiable ray consistency. S Tulsiani, T Zhou, A A Efros, J Malik, IEEE Conference on Computer Vision and Pattern Recognition. S. Tulsiani, T. Zhou, A. A. Efros, and J. Malik, \"Multi-view supervision for single-view reconstruction via differentiable ray consistency,\" in IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 2626-2634.\n\nMvsnet: Depth inference for unstructured multi-view stereo. Y Yao, Z Luo, S Li, T Fang, L Quan, Springer European Conference on Computer Vision. Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, \"Mvsnet: Depth inference for unstructured multi-view stereo,\" in Springer European Conference on Computer Vision, 2018, pp. 767-783.\n\n. H Fu, R Jia, L Gao, M Gong, B Zhao, S Maybank, D Tao, arXiv:2009.09633arXiv preprint3d-future: 3d furniture shape with textureH. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. Maybank, and D. Tao, \"3d-future: 3d furniture shape with texture,\" arXiv preprint arXiv:2009.09633, 2020.\n\nHuman-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540529V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., \"Human-level control through deep reinforcement learning,\" Nature, vol. 518, no. 7540, p. 529, 2015.\n\nFast poisson disk sampling in arbitrary dimensions. R Bridson, SIGGRAPH sketches. 1011R. Bridson, \"Fast poisson disk sampling in arbitrary dimensions.\" SIGGRAPH sketches, vol. 10, no. 1, p. 1, 2007.\n\nOpen3D: A modern library for 3D data processing. Q.-Y Zhou, J Park, V Koltun, arXiv:1801.09847Q.-Y. Zhou, J. Park, and V. Koltun, \"Open3D: A modern library for 3D data processing,\" arXiv:1801.09847, 2018.\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nie\u00dfner, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)IEEEA. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner, \"Scannet: Richly-annotated 3d reconstructions of indoor scenes,\" in Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017.\n\nA point set generation network for 3d object reconstruction from a single image. H Fan, H Su, L J Guibas, IEEE Conference on Computer Vision and Pattern Recognition. H. Fan, H. Su, and L. J. Guibas, \"A point set generation network for 3d object reconstruction from a single image,\" in IEEE Con- ference on Computer Vision and Pattern Recognition, 2017, pp. 2463- 2471.\n\nDeep reinforcement learning with double q-learning. H Van Hasselt, A Guez, D Silver, AAAI. H. Van Hasselt, A. Guez, and D. Silver, \"Deep reinforcement learning with double q-learning,\" in AAAI, 2016, pp. 2094-2100.\n", "annotations": {"author": "[{\"end\":112,\"start\":97},{\"end\":127,\"start\":113},{\"end\":136,\"start\":128},{\"end\":145,\"start\":137},{\"end\":157,\"start\":146},{\"end\":167,\"start\":158}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":106},{\"end\":126,\"start\":123},{\"end\":135,\"start\":131},{\"end\":144,\"start\":142},{\"end\":156,\"start\":153},{\"end\":166,\"start\":162}]", "author_first_name": "[{\"end\":105,\"start\":97},{\"end\":122,\"start\":113},{\"end\":130,\"start\":128},{\"end\":141,\"start\":137},{\"end\":152,\"start\":146},{\"end\":161,\"start\":158}]", "author_affiliation": null, "title": "[{\"end\":94,\"start\":1},{\"end\":261,\"start\":168}]", "venue": null, "abstract": "[{\"end\":1735,\"start\":264}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2210,\"start\":2207},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2215,\"start\":2212},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2220,\"start\":2217},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2225,\"start\":2222},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2230,\"start\":2227},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2235,\"start\":2232},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2649,\"start\":2646},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3826,\"start\":3823},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4027,\"start\":4024},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4369,\"start\":4366},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4375,\"start\":4371},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4381,\"start\":4377},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4387,\"start\":4383},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4806,\"start\":4803},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5297,\"start\":5294},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5302,\"start\":5299},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5307,\"start\":5304},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5312,\"start\":5309},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5543,\"start\":5539},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6032,\"start\":6029},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6037,\"start\":6034},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6042,\"start\":6039},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6047,\"start\":6044},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6052,\"start\":6049},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6329,\"start\":6326},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8225,\"start\":8222},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8231,\"start\":8227},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8237,\"start\":8233},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8243,\"start\":8239},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8955,\"start\":8951},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9089,\"start\":9086},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9095,\"start\":9091},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9133,\"start\":9130},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9990,\"start\":9987},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10594,\"start\":10591},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12611,\"start\":12607},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12617,\"start\":12613},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12647,\"start\":12643},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12653,\"start\":12649},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12659,\"start\":12655},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12688,\"start\":12684},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12694,\"start\":12690},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12700,\"start\":12696},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12994,\"start\":12990},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13000,\"start\":12996},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13006,\"start\":13002},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13012,\"start\":13008},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13276,\"start\":13272},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13282,\"start\":13278},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13288,\"start\":13284},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13294,\"start\":13290},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":13300,\"start\":13296},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13306,\"start\":13302},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13443,\"start\":13440},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13669,\"start\":13665},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13749,\"start\":13746},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13872,\"start\":13868},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14064,\"start\":14061},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14215,\"start\":14212},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14360,\"start\":14357},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14540,\"start\":14536},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14729,\"start\":14725},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15242,\"start\":15238},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15248,\"start\":15244},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15254,\"start\":15250},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15260,\"start\":15256},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15266,\"start\":15262},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15272,\"start\":15268},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15278,\"start\":15274},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15284,\"start\":15280},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":15343,\"start\":15339},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":15349,\"start\":15345},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15540,\"start\":15536},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15546,\"start\":15542},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15552,\"start\":15548},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":15558,\"start\":15554},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15564,\"start\":15560},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":15583,\"start\":15579},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15846,\"start\":15842},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16602,\"start\":16598},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16819,\"start\":16815},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":17734,\"start\":17730},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":17836,\"start\":17832},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":18136,\"start\":18132},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":18776,\"start\":18772},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":18902,\"start\":18898},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":18927,\"start\":18923},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":18933,\"start\":18929},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":19046,\"start\":19042},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":19242,\"start\":19238},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":19473,\"start\":19469},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20099,\"start\":20095},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20105,\"start\":20101},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":20111,\"start\":20107},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":20173,\"start\":20169},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":20697,\"start\":20693},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":20703,\"start\":20699},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":20709,\"start\":20705},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":20715,\"start\":20711},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":20814,\"start\":20810},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":20856,\"start\":20852},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":21147,\"start\":21143},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":25122,\"start\":25118},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25288,\"start\":25285},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25574,\"start\":25571},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25669,\"start\":25666},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26307,\"start\":26303},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26428,\"start\":26425},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27323,\"start\":27320},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27329,\"start\":27325},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":27827,\"start\":27823},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":27869,\"start\":27865},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":30172,\"start\":30168},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":30767,\"start\":30763},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":30948,\"start\":30944},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35834,\"start\":35830},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":37388,\"start\":37384},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":37569,\"start\":37565},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37820,\"start\":37817},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":38374,\"start\":38370},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":38491,\"start\":38487},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":39445,\"start\":39441},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":39672,\"start\":39669},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":40021,\"start\":40018},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":40796,\"start\":40792},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":40960,\"start\":40956},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":41549,\"start\":41546},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":42261,\"start\":42258},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":43664,\"start\":43661},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":43804,\"start\":43801},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":44012,\"start\":44008},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":44323,\"start\":44320},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":44368,\"start\":44364},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":44422,\"start\":44418},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":45045,\"start\":45041},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":45091,\"start\":45087},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":45399,\"start\":45395},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46348,\"start\":46345},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":46359,\"start\":46356},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":46376,\"start\":46372},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":46526,\"start\":46523},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":46531,\"start\":46528},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":46537,\"start\":46533},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":46729,\"start\":46725},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":48404,\"start\":48401},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":52025,\"start\":52022}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":59412,\"start\":58978},{\"attributes\":{\"id\":\"fig_1\"},\"end\":60186,\"start\":59413},{\"attributes\":{\"id\":\"fig_2\"},\"end\":60420,\"start\":60187},{\"attributes\":{\"id\":\"fig_3\"},\"end\":60673,\"start\":60421},{\"attributes\":{\"id\":\"fig_4\"},\"end\":60820,\"start\":60674},{\"attributes\":{\"id\":\"fig_5\"},\"end\":60866,\"start\":60821},{\"attributes\":{\"id\":\"fig_6\"},\"end\":61270,\"start\":60867},{\"attributes\":{\"id\":\"fig_7\"},\"end\":61505,\"start\":61271},{\"attributes\":{\"id\":\"fig_8\"},\"end\":61852,\"start\":61506},{\"attributes\":{\"id\":\"fig_9\"},\"end\":62031,\"start\":61853},{\"attributes\":{\"id\":\"fig_10\"},\"end\":62247,\"start\":62032},{\"attributes\":{\"id\":\"fig_11\"},\"end\":62461,\"start\":62248},{\"attributes\":{\"id\":\"fig_12\"},\"end\":62641,\"start\":62462},{\"attributes\":{\"id\":\"fig_13\"},\"end\":62921,\"start\":62642},{\"attributes\":{\"id\":\"fig_14\"},\"end\":63555,\"start\":62922},{\"attributes\":{\"id\":\"fig_15\"},\"end\":63761,\"start\":63556},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":67667,\"start\":63762},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":70398,\"start\":67668},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":71146,\"start\":70399},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":71894,\"start\":71147},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":72460,\"start\":71895},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":72723,\"start\":72461}]", "paragraph": "[{\"end\":2010,\"start\":1751},{\"end\":3493,\"start\":2012},{\"end\":4308,\"start\":3495},{\"end\":6773,\"start\":4310},{\"end\":9776,\"start\":6775},{\"end\":11252,\"start\":9794},{\"end\":11652,\"start\":11254},{\"end\":11983,\"start\":11654},{\"end\":12286,\"start\":11985},{\"end\":12414,\"start\":12304},{\"end\":15100,\"start\":12416},{\"end\":16293,\"start\":15102},{\"end\":17633,\"start\":16295},{\"end\":18677,\"start\":17635},{\"end\":19894,\"start\":18679},{\"end\":21219,\"start\":19896},{\"end\":22161,\"start\":21246},{\"end\":23860,\"start\":22163},{\"end\":25057,\"start\":23950},{\"end\":25847,\"start\":25089},{\"end\":28524,\"start\":25885},{\"end\":28693,\"start\":28554},{\"end\":28791,\"start\":28751},{\"end\":28901,\"start\":28838},{\"end\":29024,\"start\":28957},{\"end\":29190,\"start\":29094},{\"end\":30075,\"start\":29229},{\"end\":30219,\"start\":30077},{\"end\":32417,\"start\":30252},{\"end\":33244,\"start\":32419},{\"end\":33448,\"start\":33340},{\"end\":34245,\"start\":33450},{\"end\":34893,\"start\":34310},{\"end\":35101,\"start\":34952},{\"end\":35689,\"start\":35131},{\"end\":36517,\"start\":35748},{\"end\":36709,\"start\":36639},{\"end\":36822,\"start\":36776},{\"end\":37276,\"start\":36857},{\"end\":39390,\"start\":37301},{\"end\":41091,\"start\":39392},{\"end\":42799,\"start\":41218},{\"end\":43057,\"start\":42868},{\"end\":45492,\"start\":43106},{\"end\":46035,\"start\":45659},{\"end\":46201,\"start\":46037},{\"end\":47187,\"start\":46230},{\"end\":48256,\"start\":47189},{\"end\":49141,\"start\":48258},{\"end\":49785,\"start\":49143},{\"end\":50315,\"start\":49787},{\"end\":51141,\"start\":50343},{\"end\":51289,\"start\":51162},{\"end\":53813,\"start\":51313},{\"end\":54264,\"start\":53841},{\"end\":57320,\"start\":54290},{\"end\":57974,\"start\":57336},{\"end\":58148,\"start\":57976},{\"end\":58977,\"start\":58163}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":23916,\"start\":23861},{\"attributes\":{\"id\":\"formula_1\"},\"end\":28553,\"start\":28525},{\"attributes\":{\"id\":\"formula_2\"},\"end\":28750,\"start\":28694},{\"attributes\":{\"id\":\"formula_3\"},\"end\":28837,\"start\":28792},{\"attributes\":{\"id\":\"formula_4\"},\"end\":28956,\"start\":28902},{\"attributes\":{\"id\":\"formula_5\"},\"end\":29093,\"start\":29025},{\"attributes\":{\"id\":\"formula_6\"},\"end\":29228,\"start\":29191},{\"attributes\":{\"id\":\"formula_8\"},\"end\":33339,\"start\":33245},{\"attributes\":{\"id\":\"formula_9\"},\"end\":34309,\"start\":34246},{\"attributes\":{\"id\":\"formula_10\"},\"end\":34951,\"start\":34894},{\"attributes\":{\"id\":\"formula_12\"},\"end\":35130,\"start\":35102},{\"attributes\":{\"id\":\"formula_13\"},\"end\":35747,\"start\":35690},{\"attributes\":{\"id\":\"formula_14\"},\"end\":36638,\"start\":36560},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36775,\"start\":36710},{\"attributes\":{\"id\":\"formula_16\"},\"end\":36856,\"start\":36823},{\"attributes\":{\"id\":\"formula_17\"},\"end\":41217,\"start\":41092},{\"attributes\":{\"id\":\"formula_18\"},\"end\":42867,\"start\":42800},{\"attributes\":{\"id\":\"formula_19\"},\"end\":43105,\"start\":43058},{\"attributes\":{\"id\":\"formula_20\"},\"end\":45567,\"start\":45493},{\"attributes\":{\"id\":\"formula_21\"},\"end\":45658,\"start\":45567}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1749,\"start\":1737},{\"attributes\":{\"n\":\"1.1\"},\"end\":9792,\"start\":9779},{\"attributes\":{\"n\":\"2\"},\"end\":12302,\"start\":12289},{\"attributes\":{\"n\":\"3\"},\"end\":21233,\"start\":21222},{\"attributes\":{\"n\":\"3.1\"},\"end\":21244,\"start\":21236},{\"attributes\":{\"n\":\"3.2\"},\"end\":23948,\"start\":23918},{\"end\":25087,\"start\":25060},{\"end\":25883,\"start\":25850},{\"attributes\":{\"n\":\"3.3\"},\"end\":30250,\"start\":30222},{\"end\":36559,\"start\":36520},{\"attributes\":{\"n\":\"4\"},\"end\":37299,\"start\":37279},{\"attributes\":{\"n\":\"4.1\"},\"end\":46228,\"start\":46204},{\"attributes\":{\"n\":\"4.2\"},\"end\":50341,\"start\":50318},{\"attributes\":{\"n\":\"4.3\"},\"end\":51160,\"start\":51144},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":51311,\"start\":51292},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":53839,\"start\":53816},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":54288,\"start\":54267},{\"attributes\":{\"n\":\"4.4\"},\"end\":57334,\"start\":57323},{\"attributes\":{\"n\":\"5\"},\"end\":58161,\"start\":58151},{\"end\":58987,\"start\":58979},{\"end\":59422,\"start\":59414},{\"end\":60196,\"start\":60188},{\"end\":60683,\"start\":60675},{\"end\":61280,\"start\":61272},{\"end\":61515,\"start\":61507},{\"end\":61862,\"start\":61854},{\"end\":62041,\"start\":62033},{\"end\":62258,\"start\":62249},{\"end\":62472,\"start\":62463},{\"end\":62652,\"start\":62643},{\"end\":62932,\"start\":62923},{\"end\":63566,\"start\":63557},{\"end\":63770,\"start\":63763},{\"end\":67676,\"start\":67669},{\"end\":70407,\"start\":70400},{\"end\":71155,\"start\":71148},{\"end\":71903,\"start\":71896},{\"end\":72469,\"start\":72462}]", "table": "[{\"end\":67667,\"start\":63904},{\"end\":70398,\"start\":67944},{\"end\":71146,\"start\":70473},{\"end\":71894,\"start\":71377},{\"end\":72460,\"start\":72211},{\"end\":72723,\"start\":72563}]", "figure_caption": "[{\"end\":59412,\"start\":58989},{\"end\":60186,\"start\":59424},{\"end\":60420,\"start\":60198},{\"end\":60673,\"start\":60423},{\"end\":60820,\"start\":60685},{\"end\":60866,\"start\":60823},{\"end\":61270,\"start\":60869},{\"end\":61505,\"start\":61282},{\"end\":61852,\"start\":61517},{\"end\":62031,\"start\":61864},{\"end\":62247,\"start\":62043},{\"end\":62461,\"start\":62261},{\"end\":62641,\"start\":62475},{\"end\":62921,\"start\":62655},{\"end\":63555,\"start\":62935},{\"end\":63761,\"start\":63569},{\"end\":63904,\"start\":63772},{\"end\":67944,\"start\":67678},{\"end\":70473,\"start\":70409},{\"end\":71377,\"start\":71157},{\"end\":72211,\"start\":71905},{\"end\":72563,\"start\":72471}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7038,\"start\":7030},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23543,\"start\":23537},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24643,\"start\":24626},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32477,\"start\":32471},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":36897,\"start\":36891},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":43484,\"start\":43478},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":50055,\"start\":50049},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":50403,\"start\":50396},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":50414,\"start\":50408},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":50690,\"start\":50684},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50911,\"start\":50904},{\"end\":51138,\"start\":51132},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":51418,\"start\":51412},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51427,\"start\":51420},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51439,\"start\":51432},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":52827,\"start\":52810},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":54887,\"start\":54880},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":54899,\"start\":54892},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":55071,\"start\":55064},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":56862,\"start\":56852},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":57003,\"start\":56991}]", "bib_author_first_name": "[{\"end\":73735,\"start\":73734},{\"end\":73743,\"start\":73742},{\"end\":73749,\"start\":73748},{\"end\":73757,\"start\":73756},{\"end\":73759,\"start\":73758},{\"end\":73768,\"start\":73767},{\"end\":73777,\"start\":73776},{\"end\":73779,\"start\":73778},{\"end\":74131,\"start\":74130},{\"end\":74140,\"start\":74139},{\"end\":74148,\"start\":74147},{\"end\":74157,\"start\":74156},{\"end\":74165,\"start\":74164},{\"end\":74178,\"start\":74177},{\"end\":74185,\"start\":74184},{\"end\":74189,\"start\":74186},{\"end\":74197,\"start\":74196},{\"end\":74509,\"start\":74505},{\"end\":74516,\"start\":74515},{\"end\":74823,\"start\":74822},{\"end\":74830,\"start\":74829},{\"end\":74836,\"start\":74835},{\"end\":74844,\"start\":74843},{\"end\":74852,\"start\":74851},{\"end\":74859,\"start\":74858},{\"end\":74866,\"start\":74865},{\"end\":75132,\"start\":75131},{\"end\":75146,\"start\":75142},{\"end\":75154,\"start\":75153},{\"end\":75166,\"start\":75165},{\"end\":75495,\"start\":75494},{\"end\":75503,\"start\":75502},{\"end\":75505,\"start\":75504},{\"end\":75512,\"start\":75511},{\"end\":75521,\"start\":75520},{\"end\":75885,\"start\":75884},{\"end\":75892,\"start\":75891},{\"end\":75901,\"start\":75900},{\"end\":75907,\"start\":75906},{\"end\":75915,\"start\":75914},{\"end\":75921,\"start\":75920},{\"end\":75928,\"start\":75927},{\"end\":75936,\"start\":75935},{\"end\":75943,\"start\":75942},{\"end\":75952,\"start\":75951},{\"end\":76392,\"start\":76391},{\"end\":76399,\"start\":76398},{\"end\":76405,\"start\":76404},{\"end\":76414,\"start\":76413},{\"end\":76416,\"start\":76415},{\"end\":76422,\"start\":76421},{\"end\":76429,\"start\":76428},{\"end\":76870,\"start\":76869},{\"end\":76878,\"start\":76877},{\"end\":76884,\"start\":76883},{\"end\":76892,\"start\":76891},{\"end\":76900,\"start\":76899},{\"end\":76906,\"start\":76905},{\"end\":77258,\"start\":77257},{\"end\":77269,\"start\":77265},{\"end\":77279,\"start\":77275},{\"end\":77285,\"start\":77284},{\"end\":77298,\"start\":77294},{\"end\":77628,\"start\":77627},{\"end\":77637,\"start\":77636},{\"end\":77639,\"start\":77638},{\"end\":77651,\"start\":77650},{\"end\":77653,\"start\":77652},{\"end\":77665,\"start\":77664},{\"end\":77981,\"start\":77980},{\"end\":77988,\"start\":77987},{\"end\":77995,\"start\":77994},{\"end\":78192,\"start\":78191},{\"end\":78200,\"start\":78199},{\"end\":78209,\"start\":78208},{\"end\":78215,\"start\":78214},{\"end\":78221,\"start\":78220},{\"end\":78227,\"start\":78226},{\"end\":78234,\"start\":78233},{\"end\":78244,\"start\":78240},{\"end\":78580,\"start\":78579},{\"end\":78590,\"start\":78589},{\"end\":78604,\"start\":78603},{\"end\":78615,\"start\":78614},{\"end\":78626,\"start\":78625},{\"end\":78628,\"start\":78627},{\"end\":78951,\"start\":78950},{\"end\":78961,\"start\":78960},{\"end\":78975,\"start\":78974},{\"end\":79246,\"start\":79245},{\"end\":79253,\"start\":79252},{\"end\":79255,\"start\":79254},{\"end\":79263,\"start\":79262},{\"end\":79265,\"start\":79264},{\"end\":79276,\"start\":79272},{\"end\":79284,\"start\":79283},{\"end\":79291,\"start\":79290},{\"end\":79548,\"start\":79547},{\"end\":79554,\"start\":79553},{\"end\":79562,\"start\":79561},{\"end\":79577,\"start\":79576},{\"end\":79579,\"start\":79578},{\"end\":79888,\"start\":79887},{\"end\":79900,\"start\":79899},{\"end\":79911,\"start\":79910},{\"end\":80177,\"start\":80176},{\"end\":80183,\"start\":80182},{\"end\":80189,\"start\":80188},{\"end\":80203,\"start\":80202},{\"end\":80212,\"start\":80211},{\"end\":80224,\"start\":80223},{\"end\":80226,\"start\":80225},{\"end\":80499,\"start\":80498},{\"end\":80510,\"start\":80509},{\"end\":80718,\"start\":80717},{\"end\":80726,\"start\":80725},{\"end\":80733,\"start\":80732},{\"end\":80964,\"start\":80963},{\"end\":80975,\"start\":80974},{\"end\":81209,\"start\":81208},{\"end\":81211,\"start\":81210},{\"end\":81220,\"start\":81219},{\"end\":81222,\"start\":81221},{\"end\":81232,\"start\":81231},{\"end\":81498,\"start\":81497},{\"end\":81509,\"start\":81508},{\"end\":81519,\"start\":81518},{\"end\":81761,\"start\":81760},{\"end\":81769,\"start\":81768},{\"end\":81771,\"start\":81770},{\"end\":81778,\"start\":81777},{\"end\":81787,\"start\":81786},{\"end\":82046,\"start\":82042},{\"end\":82054,\"start\":82053},{\"end\":82060,\"start\":82059},{\"end\":82071,\"start\":82067},{\"end\":82322,\"start\":82321},{\"end\":82324,\"start\":82323},{\"end\":82331,\"start\":82330},{\"end\":82337,\"start\":82336},{\"end\":82339,\"start\":82338},{\"end\":82348,\"start\":82347},{\"end\":82361,\"start\":82360},{\"end\":82372,\"start\":82371},{\"end\":82677,\"start\":82676},{\"end\":82685,\"start\":82684},{\"end\":82694,\"start\":82693},{\"end\":82705,\"start\":82704},{\"end\":82713,\"start\":82712},{\"end\":82721,\"start\":82720},{\"end\":83028,\"start\":83027},{\"end\":83034,\"start\":83029},{\"end\":83047,\"start\":83043},{\"end\":83054,\"start\":83053},{\"end\":83065,\"start\":83061},{\"end\":83076,\"start\":83072},{\"end\":83402,\"start\":83401},{\"end\":83412,\"start\":83411},{\"end\":83420,\"start\":83419},{\"end\":83696,\"start\":83695},{\"end\":83706,\"start\":83705},{\"end\":83717,\"start\":83716},{\"end\":83731,\"start\":83730},{\"end\":83741,\"start\":83740},{\"end\":84027,\"start\":84026},{\"end\":84034,\"start\":84033},{\"end\":84036,\"start\":84035},{\"end\":84042,\"start\":84041},{\"end\":84415,\"start\":84414},{\"end\":84422,\"start\":84421},{\"end\":84428,\"start\":84427},{\"end\":84437,\"start\":84436},{\"end\":84452,\"start\":84451},{\"end\":84830,\"start\":84829},{\"end\":84837,\"start\":84836},{\"end\":84848,\"start\":84847},{\"end\":84859,\"start\":84858},{\"end\":84867,\"start\":84866},{\"end\":84876,\"start\":84875},{\"end\":85265,\"start\":85264},{\"end\":85273,\"start\":85272},{\"end\":85275,\"start\":85274},{\"end\":85282,\"start\":85281},{\"end\":85291,\"start\":85290},{\"end\":85627,\"start\":85626},{\"end\":85633,\"start\":85632},{\"end\":85640,\"start\":85639},{\"end\":85648,\"start\":85647},{\"end\":85655,\"start\":85654},{\"end\":86005,\"start\":86004},{\"end\":86016,\"start\":86012},{\"end\":86023,\"start\":86022},{\"end\":86031,\"start\":86030},{\"end\":86039,\"start\":86038},{\"end\":86380,\"start\":86379},{\"end\":86391,\"start\":86390},{\"end\":86402,\"start\":86401},{\"end\":86702,\"start\":86701},{\"end\":86709,\"start\":86708},{\"end\":86717,\"start\":86716},{\"end\":86992,\"start\":86991},{\"end\":86994,\"start\":86993},{\"end\":87005,\"start\":87004},{\"end\":87017,\"start\":87016},{\"end\":87263,\"start\":87262},{\"end\":87265,\"start\":87264},{\"end\":87275,\"start\":87274},{\"end\":87285,\"start\":87284},{\"end\":87294,\"start\":87293},{\"end\":87611,\"start\":87610},{\"end\":87619,\"start\":87618},{\"end\":87626,\"start\":87625},{\"end\":87634,\"start\":87633},{\"end\":87932,\"start\":87931},{\"end\":87939,\"start\":87938},{\"end\":87945,\"start\":87944},{\"end\":88282,\"start\":88281},{\"end\":88289,\"start\":88288},{\"end\":88298,\"start\":88297},{\"end\":88667,\"start\":88663},{\"end\":88676,\"start\":88675},{\"end\":88685,\"start\":88681},{\"end\":89037,\"start\":89036},{\"end\":89046,\"start\":89045},{\"end\":89048,\"start\":89047},{\"end\":89360,\"start\":89359},{\"end\":89371,\"start\":89370},{\"end\":89374,\"start\":89372},{\"end\":89382,\"start\":89381},{\"end\":89602,\"start\":89601},{\"end\":89608,\"start\":89607},{\"end\":89615,\"start\":89614},{\"end\":89623,\"start\":89622},{\"end\":89631,\"start\":89630},{\"end\":89637,\"start\":89636},{\"end\":89639,\"start\":89638},{\"end\":89849,\"start\":89848},{\"end\":89855,\"start\":89854},{\"end\":89863,\"start\":89862},{\"end\":89872,\"start\":89871},{\"end\":89878,\"start\":89877},{\"end\":90171,\"start\":90170},{\"end\":90181,\"start\":90180},{\"end\":90183,\"start\":90182},{\"end\":90191,\"start\":90190},{\"end\":90202,\"start\":90201},{\"end\":90211,\"start\":90210},{\"end\":90213,\"start\":90212},{\"end\":90531,\"start\":90530},{\"end\":90540,\"start\":90539},{\"end\":90838,\"start\":90834},{\"end\":90849,\"start\":90845},{\"end\":90855,\"start\":90854},{\"end\":90866,\"start\":90862},{\"end\":91199,\"start\":91198},{\"end\":91213,\"start\":91212},{\"end\":91225,\"start\":91224},{\"end\":91239,\"start\":91238},{\"end\":91415,\"start\":91414},{\"end\":91423,\"start\":91422},{\"end\":91431,\"start\":91430},{\"end\":91439,\"start\":91438},{\"end\":91448,\"start\":91447},{\"end\":91705,\"start\":91704},{\"end\":91713,\"start\":91712},{\"end\":91721,\"start\":91720},{\"end\":91729,\"start\":91728},{\"end\":92208,\"start\":92207},{\"end\":92218,\"start\":92217},{\"end\":92220,\"start\":92219},{\"end\":92227,\"start\":92226},{\"end\":92229,\"start\":92228},{\"end\":92688,\"start\":92687},{\"end\":92697,\"start\":92696},{\"end\":92703,\"start\":92702},{\"end\":92709,\"start\":92708},{\"end\":92715,\"start\":92714},{\"end\":92893,\"start\":92892},{\"end\":92902,\"start\":92901},{\"end\":93198,\"start\":93197},{\"end\":93200,\"start\":93199},{\"end\":93206,\"start\":93205},{\"end\":93212,\"start\":93211},{\"end\":93218,\"start\":93217},{\"end\":93220,\"start\":93219},{\"end\":93689,\"start\":93688},{\"end\":93691,\"start\":93690},{\"end\":93697,\"start\":93696},{\"end\":93703,\"start\":93702},{\"end\":93709,\"start\":93708},{\"end\":93711,\"start\":93710},{\"end\":94029,\"start\":94028},{\"end\":94042,\"start\":94041},{\"end\":94057,\"start\":94056},{\"end\":94068,\"start\":94067},{\"end\":94584,\"start\":94580},{\"end\":94592,\"start\":94591},{\"end\":94605,\"start\":94601},{\"end\":94612,\"start\":94611},{\"end\":94623,\"start\":94619},{\"end\":95133,\"start\":95132},{\"end\":95141,\"start\":95140},{\"end\":95150,\"start\":95149},{\"end\":95157,\"start\":95156},{\"end\":95166,\"start\":95165},{\"end\":95606,\"start\":95605},{\"end\":95615,\"start\":95614},{\"end\":95623,\"start\":95622},{\"end\":95630,\"start\":95629},{\"end\":95641,\"start\":95637},{\"end\":95647,\"start\":95646},{\"end\":96068,\"start\":96067},{\"end\":96070,\"start\":96069},{\"end\":96076,\"start\":96075},{\"end\":96082,\"start\":96081},{\"end\":96093,\"start\":96092},{\"end\":96100,\"start\":96099},{\"end\":96107,\"start\":96106},{\"end\":96109,\"start\":96108},{\"end\":96470,\"start\":96469},{\"end\":96485,\"start\":96484},{\"end\":96500,\"start\":96499},{\"end\":96826,\"start\":96825},{\"end\":96828,\"start\":96827},{\"end\":96836,\"start\":96835},{\"end\":96842,\"start\":96841},{\"end\":96850,\"start\":96849},{\"end\":96858,\"start\":96857},{\"end\":97221,\"start\":97217},{\"end\":97228,\"start\":97227},{\"end\":97533,\"start\":97532},{\"end\":97535,\"start\":97534},{\"end\":97544,\"start\":97543},{\"end\":97546,\"start\":97545},{\"end\":97825,\"start\":97821},{\"end\":97833,\"start\":97832},{\"end\":98064,\"start\":98063},{\"end\":98070,\"start\":98069},{\"end\":98077,\"start\":98076},{\"end\":98085,\"start\":98084},{\"end\":98094,\"start\":98093},{\"end\":98106,\"start\":98105},{\"end\":98114,\"start\":98113},{\"end\":98125,\"start\":98124},{\"end\":98406,\"start\":98405},{\"end\":98414,\"start\":98413},{\"end\":98422,\"start\":98421},{\"end\":98430,\"start\":98429},{\"end\":98437,\"start\":98436},{\"end\":98446,\"start\":98445},{\"end\":98453,\"start\":98452},{\"end\":98788,\"start\":98787},{\"end\":98794,\"start\":98793},{\"end\":98801,\"start\":98800},{\"end\":98810,\"start\":98809},{\"end\":98819,\"start\":98818},{\"end\":98826,\"start\":98825},{\"end\":98835,\"start\":98834},{\"end\":98841,\"start\":98840},{\"end\":98853,\"start\":98852},{\"end\":99165,\"start\":99164},{\"end\":99173,\"start\":99172},{\"end\":99175,\"start\":99174},{\"end\":99184,\"start\":99183},{\"end\":99193,\"start\":99192},{\"end\":99203,\"start\":99202},{\"end\":99205,\"start\":99204},{\"end\":99218,\"start\":99217},{\"end\":99228,\"start\":99227},{\"end\":99238,\"start\":99237},{\"end\":99585,\"start\":99584},{\"end\":99591,\"start\":99590},{\"end\":99598,\"start\":99597},{\"end\":99606,\"start\":99605},{\"end\":99612,\"start\":99611},{\"end\":99619,\"start\":99618},{\"end\":99627,\"start\":99626},{\"end\":100088,\"start\":100087},{\"end\":100100,\"start\":100099},{\"end\":100108,\"start\":100107},{\"end\":100110,\"start\":100109},{\"end\":100119,\"start\":100118},{\"end\":100474,\"start\":100473},{\"end\":100481,\"start\":100480},{\"end\":100488,\"start\":100487},{\"end\":100494,\"start\":100493},{\"end\":100502,\"start\":100501},{\"end\":100740,\"start\":100739},{\"end\":100746,\"start\":100745},{\"end\":100753,\"start\":100752},{\"end\":100760,\"start\":100759},{\"end\":100768,\"start\":100767},{\"end\":100776,\"start\":100775},{\"end\":100787,\"start\":100786},{\"end\":101074,\"start\":101073},{\"end\":101082,\"start\":101081},{\"end\":101097,\"start\":101096},{\"end\":101107,\"start\":101106},{\"end\":101109,\"start\":101108},{\"end\":101117,\"start\":101116},{\"end\":101127,\"start\":101126},{\"end\":101129,\"start\":101128},{\"end\":101142,\"start\":101141},{\"end\":101152,\"start\":101151},{\"end\":101166,\"start\":101165},{\"end\":101168,\"start\":101167},{\"end\":101181,\"start\":101180},{\"end\":101506,\"start\":101505},{\"end\":101706,\"start\":101702},{\"end\":101714,\"start\":101713},{\"end\":101722,\"start\":101721},{\"end\":101923,\"start\":101922},{\"end\":101930,\"start\":101929},{\"end\":101932,\"start\":101931},{\"end\":101941,\"start\":101940},{\"end\":101950,\"start\":101949},{\"end\":101960,\"start\":101959},{\"end\":101974,\"start\":101973},{\"end\":102378,\"start\":102377},{\"end\":102385,\"start\":102384},{\"end\":102391,\"start\":102390},{\"end\":102393,\"start\":102392},{\"end\":102719,\"start\":102718},{\"end\":102734,\"start\":102733},{\"end\":102742,\"start\":102741}]", "bib_author_last_name": "[{\"end\":73740,\"start\":73736},{\"end\":73746,\"start\":73744},{\"end\":73754,\"start\":73750},{\"end\":73765,\"start\":73760},{\"end\":73774,\"start\":73769},{\"end\":73790,\"start\":73780},{\"end\":74137,\"start\":74132},{\"end\":74145,\"start\":74141},{\"end\":74154,\"start\":74149},{\"end\":74162,\"start\":74158},{\"end\":74175,\"start\":74166},{\"end\":74182,\"start\":74179},{\"end\":74194,\"start\":74190},{\"end\":74202,\"start\":74198},{\"end\":74513,\"start\":74510},{\"end\":74521,\"start\":74517},{\"end\":74827,\"start\":74824},{\"end\":74833,\"start\":74831},{\"end\":74841,\"start\":74837},{\"end\":74849,\"start\":74845},{\"end\":74856,\"start\":74853},{\"end\":74863,\"start\":74860},{\"end\":74869,\"start\":74867},{\"end\":75140,\"start\":75133},{\"end\":75151,\"start\":75147},{\"end\":75163,\"start\":75155},{\"end\":75171,\"start\":75167},{\"end\":75500,\"start\":75496},{\"end\":75509,\"start\":75506},{\"end\":75518,\"start\":75513},{\"end\":75529,\"start\":75522},{\"end\":75889,\"start\":75886},{\"end\":75898,\"start\":75893},{\"end\":75904,\"start\":75902},{\"end\":75912,\"start\":75908},{\"end\":75918,\"start\":75916},{\"end\":75925,\"start\":75922},{\"end\":75933,\"start\":75929},{\"end\":75940,\"start\":75937},{\"end\":75949,\"start\":75944},{\"end\":75956,\"start\":75953},{\"end\":76396,\"start\":76393},{\"end\":76402,\"start\":76400},{\"end\":76411,\"start\":76406},{\"end\":76419,\"start\":76417},{\"end\":76426,\"start\":76423},{\"end\":76432,\"start\":76430},{\"end\":76875,\"start\":76871},{\"end\":76881,\"start\":76879},{\"end\":76889,\"start\":76885},{\"end\":76897,\"start\":76893},{\"end\":76903,\"start\":76901},{\"end\":76910,\"start\":76907},{\"end\":77263,\"start\":77259},{\"end\":77273,\"start\":77270},{\"end\":77282,\"start\":77280},{\"end\":77292,\"start\":77286},{\"end\":77301,\"start\":77299},{\"end\":77634,\"start\":77629},{\"end\":77648,\"start\":77640},{\"end\":77662,\"start\":77654},{\"end\":77671,\"start\":77666},{\"end\":77985,\"start\":77982},{\"end\":77992,\"start\":77989},{\"end\":78001,\"start\":77996},{\"end\":78197,\"start\":78193},{\"end\":78206,\"start\":78201},{\"end\":78212,\"start\":78210},{\"end\":78218,\"start\":78216},{\"end\":78224,\"start\":78222},{\"end\":78231,\"start\":78228},{\"end\":78238,\"start\":78235},{\"end\":78250,\"start\":78245},{\"end\":78587,\"start\":78581},{\"end\":78601,\"start\":78591},{\"end\":78612,\"start\":78605},{\"end\":78623,\"start\":78616},{\"end\":78634,\"start\":78629},{\"end\":78958,\"start\":78952},{\"end\":78972,\"start\":78962},{\"end\":78984,\"start\":78976},{\"end\":79250,\"start\":79247},{\"end\":79260,\"start\":79256},{\"end\":79270,\"start\":79266},{\"end\":79281,\"start\":79277},{\"end\":79288,\"start\":79285},{\"end\":79301,\"start\":79292},{\"end\":79551,\"start\":79549},{\"end\":79559,\"start\":79555},{\"end\":79574,\"start\":79563},{\"end\":79594,\"start\":79580},{\"end\":79897,\"start\":79889},{\"end\":79908,\"start\":79901},{\"end\":79917,\"start\":79912},{\"end\":80180,\"start\":80178},{\"end\":80186,\"start\":80184},{\"end\":80200,\"start\":80190},{\"end\":80209,\"start\":80204},{\"end\":80221,\"start\":80213},{\"end\":80232,\"start\":80227},{\"end\":80507,\"start\":80500},{\"end\":80519,\"start\":80511},{\"end\":80723,\"start\":80719},{\"end\":80730,\"start\":80727},{\"end\":80737,\"start\":80734},{\"end\":80972,\"start\":80965},{\"end\":80981,\"start\":80976},{\"end\":81217,\"start\":81212},{\"end\":81229,\"start\":81223},{\"end\":81238,\"start\":81233},{\"end\":81506,\"start\":81499},{\"end\":81516,\"start\":81510},{\"end\":81527,\"start\":81520},{\"end\":81766,\"start\":81762},{\"end\":81775,\"start\":81772},{\"end\":81784,\"start\":81779},{\"end\":81794,\"start\":81788},{\"end\":82051,\"start\":82047},{\"end\":82057,\"start\":82055},{\"end\":82065,\"start\":82061},{\"end\":82074,\"start\":82072},{\"end\":82328,\"start\":82325},{\"end\":82334,\"start\":82332},{\"end\":82345,\"start\":82340},{\"end\":82358,\"start\":82349},{\"end\":82369,\"start\":82362},{\"end\":82383,\"start\":82373},{\"end\":82682,\"start\":82678},{\"end\":82691,\"start\":82686},{\"end\":82702,\"start\":82695},{\"end\":82710,\"start\":82706},{\"end\":82718,\"start\":82714},{\"end\":82727,\"start\":82722},{\"end\":83041,\"start\":83035},{\"end\":83051,\"start\":83048},{\"end\":83059,\"start\":83055},{\"end\":83070,\"start\":83066},{\"end\":83082,\"start\":83077},{\"end\":83409,\"start\":83403},{\"end\":83417,\"start\":83413},{\"end\":83426,\"start\":83421},{\"end\":83703,\"start\":83697},{\"end\":83714,\"start\":83707},{\"end\":83728,\"start\":83718},{\"end\":83738,\"start\":83732},{\"end\":83747,\"start\":83742},{\"end\":84031,\"start\":84028},{\"end\":84039,\"start\":84037},{\"end\":84050,\"start\":84043},{\"end\":84419,\"start\":84416},{\"end\":84425,\"start\":84423},{\"end\":84434,\"start\":84429},{\"end\":84449,\"start\":84438},{\"end\":84455,\"start\":84453},{\"end\":84834,\"start\":84831},{\"end\":84845,\"start\":84838},{\"end\":84856,\"start\":84849},{\"end\":84864,\"start\":84860},{\"end\":84873,\"start\":84868},{\"end\":84884,\"start\":84877},{\"end\":85270,\"start\":85266},{\"end\":85279,\"start\":85276},{\"end\":85288,\"start\":85283},{\"end\":85299,\"start\":85292},{\"end\":85630,\"start\":85628},{\"end\":85637,\"start\":85634},{\"end\":85645,\"start\":85641},{\"end\":85652,\"start\":85649},{\"end\":85660,\"start\":85656},{\"end\":86010,\"start\":86006},{\"end\":86020,\"start\":86017},{\"end\":86028,\"start\":86024},{\"end\":86036,\"start\":86032},{\"end\":86042,\"start\":86040},{\"end\":86388,\"start\":86381},{\"end\":86399,\"start\":86392},{\"end\":86411,\"start\":86403},{\"end\":86706,\"start\":86703},{\"end\":86714,\"start\":86710},{\"end\":86721,\"start\":86718},{\"end\":87002,\"start\":86995},{\"end\":87014,\"start\":87006},{\"end\":87024,\"start\":87018},{\"end\":87272,\"start\":87266},{\"end\":87282,\"start\":87276},{\"end\":87291,\"start\":87286},{\"end\":87301,\"start\":87295},{\"end\":87616,\"start\":87612},{\"end\":87623,\"start\":87620},{\"end\":87631,\"start\":87627},{\"end\":87637,\"start\":87635},{\"end\":87936,\"start\":87933},{\"end\":87942,\"start\":87940},{\"end\":87954,\"start\":87946},{\"end\":88286,\"start\":88283},{\"end\":88295,\"start\":88290},{\"end\":88302,\"start\":88299},{\"end\":88673,\"start\":88668},{\"end\":88679,\"start\":88677},{\"end\":88690,\"start\":88686},{\"end\":89043,\"start\":89038},{\"end\":89054,\"start\":89049},{\"end\":89368,\"start\":89361},{\"end\":89379,\"start\":89375},{\"end\":89392,\"start\":89383},{\"end\":89605,\"start\":89603},{\"end\":89612,\"start\":89609},{\"end\":89620,\"start\":89616},{\"end\":89628,\"start\":89624},{\"end\":89634,\"start\":89632},{\"end\":89645,\"start\":89640},{\"end\":89852,\"start\":89850},{\"end\":89860,\"start\":89856},{\"end\":89869,\"start\":89864},{\"end\":89875,\"start\":89873},{\"end\":89882,\"start\":89879},{\"end\":90178,\"start\":90172},{\"end\":90188,\"start\":90184},{\"end\":90199,\"start\":90192},{\"end\":90208,\"start\":90203},{\"end\":90220,\"start\":90214},{\"end\":90537,\"start\":90532},{\"end\":90551,\"start\":90541},{\"end\":90843,\"start\":90839},{\"end\":90852,\"start\":90850},{\"end\":90860,\"start\":90856},{\"end\":90872,\"start\":90867},{\"end\":91210,\"start\":91200},{\"end\":91222,\"start\":91214},{\"end\":91236,\"start\":91226},{\"end\":91246,\"start\":91240},{\"end\":91420,\"start\":91416},{\"end\":91428,\"start\":91424},{\"end\":91436,\"start\":91432},{\"end\":91445,\"start\":91440},{\"end\":91455,\"start\":91449},{\"end\":91710,\"start\":91706},{\"end\":91718,\"start\":91714},{\"end\":91726,\"start\":91722},{\"end\":91734,\"start\":91730},{\"end\":92215,\"start\":92209},{\"end\":92224,\"start\":92221},{\"end\":92233,\"start\":92230},{\"end\":92694,\"start\":92689},{\"end\":92700,\"start\":92698},{\"end\":92706,\"start\":92704},{\"end\":92712,\"start\":92710},{\"end\":92718,\"start\":92716},{\"end\":92899,\"start\":92894},{\"end\":92906,\"start\":92903},{\"end\":93203,\"start\":93201},{\"end\":93209,\"start\":93207},{\"end\":93215,\"start\":93213},{\"end\":93227,\"start\":93221},{\"end\":93694,\"start\":93692},{\"end\":93700,\"start\":93698},{\"end\":93706,\"start\":93704},{\"end\":93718,\"start\":93712},{\"end\":94039,\"start\":94030},{\"end\":94054,\"start\":94043},{\"end\":94065,\"start\":94058},{\"end\":94074,\"start\":94069},{\"end\":94589,\"start\":94585},{\"end\":94599,\"start\":94593},{\"end\":94609,\"start\":94606},{\"end\":94617,\"start\":94613},{\"end\":94629,\"start\":94624},{\"end\":95138,\"start\":95134},{\"end\":95147,\"start\":95142},{\"end\":95154,\"start\":95151},{\"end\":95163,\"start\":95158},{\"end\":95171,\"start\":95167},{\"end\":95612,\"start\":95607},{\"end\":95620,\"start\":95616},{\"end\":95627,\"start\":95624},{\"end\":95635,\"start\":95631},{\"end\":95644,\"start\":95642},{\"end\":95651,\"start\":95648},{\"end\":96073,\"start\":96071},{\"end\":96079,\"start\":96077},{\"end\":96090,\"start\":96083},{\"end\":96097,\"start\":96094},{\"end\":96104,\"start\":96101},{\"end\":96116,\"start\":96110},{\"end\":96482,\"start\":96471},{\"end\":96497,\"start\":96486},{\"end\":96505,\"start\":96501},{\"end\":96833,\"start\":96829},{\"end\":96839,\"start\":96837},{\"end\":96847,\"start\":96843},{\"end\":96855,\"start\":96851},{\"end\":96867,\"start\":96859},{\"end\":97225,\"start\":97222},{\"end\":97235,\"start\":97229},{\"end\":97541,\"start\":97536},{\"end\":97552,\"start\":97547},{\"end\":97830,\"start\":97826},{\"end\":97840,\"start\":97834},{\"end\":98067,\"start\":98065},{\"end\":98074,\"start\":98071},{\"end\":98082,\"start\":98078},{\"end\":98091,\"start\":98086},{\"end\":98103,\"start\":98095},{\"end\":98111,\"start\":98107},{\"end\":98122,\"start\":98115},{\"end\":98130,\"start\":98126},{\"end\":98411,\"start\":98407},{\"end\":98419,\"start\":98415},{\"end\":98427,\"start\":98423},{\"end\":98434,\"start\":98431},{\"end\":98443,\"start\":98438},{\"end\":98450,\"start\":98447},{\"end\":98456,\"start\":98454},{\"end\":98791,\"start\":98789},{\"end\":98798,\"start\":98795},{\"end\":98807,\"start\":98802},{\"end\":98816,\"start\":98811},{\"end\":98823,\"start\":98820},{\"end\":98832,\"start\":98827},{\"end\":98838,\"start\":98836},{\"end\":98850,\"start\":98842},{\"end\":98858,\"start\":98854},{\"end\":99170,\"start\":99166},{\"end\":99181,\"start\":99176},{\"end\":99190,\"start\":99185},{\"end\":99200,\"start\":99194},{\"end\":99215,\"start\":99206},{\"end\":99225,\"start\":99219},{\"end\":99235,\"start\":99229},{\"end\":99250,\"start\":99239},{\"end\":99588,\"start\":99586},{\"end\":99595,\"start\":99592},{\"end\":99603,\"start\":99599},{\"end\":99609,\"start\":99607},{\"end\":99616,\"start\":99613},{\"end\":99624,\"start\":99620},{\"end\":99630,\"start\":99628},{\"end\":100097,\"start\":100089},{\"end\":100105,\"start\":100101},{\"end\":100116,\"start\":100111},{\"end\":100125,\"start\":100120},{\"end\":100478,\"start\":100475},{\"end\":100485,\"start\":100482},{\"end\":100491,\"start\":100489},{\"end\":100499,\"start\":100495},{\"end\":100507,\"start\":100503},{\"end\":100743,\"start\":100741},{\"end\":100750,\"start\":100747},{\"end\":100757,\"start\":100754},{\"end\":100765,\"start\":100761},{\"end\":100773,\"start\":100769},{\"end\":100784,\"start\":100777},{\"end\":100791,\"start\":100788},{\"end\":101079,\"start\":101075},{\"end\":101094,\"start\":101083},{\"end\":101104,\"start\":101098},{\"end\":101114,\"start\":101110},{\"end\":101124,\"start\":101118},{\"end\":101139,\"start\":101130},{\"end\":101149,\"start\":101143},{\"end\":101163,\"start\":101153},{\"end\":101178,\"start\":101169},{\"end\":101191,\"start\":101182},{\"end\":101514,\"start\":101507},{\"end\":101711,\"start\":101707},{\"end\":101719,\"start\":101715},{\"end\":101729,\"start\":101723},{\"end\":101927,\"start\":101924},{\"end\":101938,\"start\":101933},{\"end\":101947,\"start\":101942},{\"end\":101957,\"start\":101951},{\"end\":101971,\"start\":101961},{\"end\":101982,\"start\":101975},{\"end\":102382,\"start\":102379},{\"end\":102388,\"start\":102386},{\"end\":102400,\"start\":102394},{\"end\":102731,\"start\":102720},{\"end\":102739,\"start\":102735},{\"end\":102749,\"start\":102743}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":20416090},\"end\":74060,\"start\":73681},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52927274},\"end\":74426,\"start\":74062},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":49211080},\"end\":74764,\"start\":74428},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":54059353},\"end\":75088,\"start\":74766},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4747587},\"end\":75427,\"start\":75090},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52986362},\"end\":75753,\"start\":75429},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":73728973},\"end\":76320,\"start\":75755},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":199543694},\"end\":76784,\"start\":76322},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13918176},\"end\":77152,\"start\":76786},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":207218131},\"end\":77569,\"start\":77154},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":145508},\"end\":77930,\"start\":77571},{\"attributes\":{\"doi\":\"arXiv:1504.02437\",\"id\":\"b11\"},\"end\":78122,\"start\":77932},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":214810648},\"end\":78527,\"start\":78124},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2202933},\"end\":78898,\"start\":78529},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3707204},\"end\":79178,\"start\":78900},{\"attributes\":{\"doi\":\"arXiv:1804.07723\",\"id\":\"b15\"},\"end\":79479,\"start\":79180},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2407217},\"end\":79832,\"start\":79481},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7779419},\"end\":80100,\"start\":79834},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1925677},\"end\":80474,\"start\":80102},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6920955},\"end\":80662,\"start\":80476},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3365551},\"end\":80920,\"start\":80664},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1371704},\"end\":81146,\"start\":80922},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10145935},\"end\":81442,\"start\":81148},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14471042},\"end\":81706,\"start\":81444},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2902529},\"end\":82003,\"start\":81708},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1549630},\"end\":82252,\"start\":82005},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6407636},\"end\":82625,\"start\":82254},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6851666},\"end\":82986,\"start\":82627},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7422055},\"end\":83334,\"start\":82988},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2782113},\"end\":83650,\"start\":83336},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6796508},\"end\":83954,\"start\":83652},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1003795},\"end\":84302,\"start\":83956},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3052611},\"end\":84744,\"start\":84304},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1063815},\"end\":85183,\"start\":84746},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":202540160},\"end\":85555,\"start\":85185},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":214803127},\"end\":85923,\"start\":85557},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":214727619},\"end\":86317,\"start\":85925},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":924607},\"end\":86644,\"start\":86319},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6998495},\"end\":86940,\"start\":86646},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6205585},\"end\":87183,\"start\":86942},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":18010768},\"end\":87545,\"start\":87185},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":120657898},\"end\":87854,\"start\":87547},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":10513795},\"end\":88184,\"start\":87856},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":9068366},\"end\":88548,\"start\":88186},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4572349},\"end\":88969,\"start\":88550},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10284435},\"end\":89308,\"start\":88971},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":10747078},\"end\":89548,\"start\":89310},{\"attributes\":{\"doi\":\"arXiv:1806.03589\",\"id\":\"b47\"},\"end\":89796,\"start\":89550},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":219633936},\"end\":90124,\"start\":89798},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":219965283},\"end\":90481,\"start\":90126},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":4424244},\"end\":90771,\"start\":90483},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":215548442},\"end\":91128,\"start\":90773},{\"attributes\":{\"id\":\"b52\"},\"end\":91381,\"start\":91130},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":51908879},\"end\":91638,\"start\":91383},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":8338993},\"end\":92095,\"start\":91640},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":139103260},\"end\":92624,\"start\":92097},{\"attributes\":{\"id\":\"b56\"},\"end\":92830,\"start\":92626},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":2884555},\"end\":93117,\"start\":92832},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":5115938},\"end\":93606,\"start\":93119},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":1745976},\"end\":93954,\"start\":93608},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":3609891},\"end\":94436,\"start\":93956},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":90262893},\"end\":95063,\"start\":94438},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":198164185},\"end\":95520,\"start\":95065},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":202237019},\"end\":95996,\"start\":95522},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":1009127},\"end\":96397,\"start\":95998},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":512676},\"end\":96743,\"start\":96399},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":6325059},\"end\":97125,\"start\":96745},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":14997988},\"end\":97471,\"start\":97127},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":6597255},\"end\":97767,\"start\":97473},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":14440062},\"end\":98017,\"start\":97769},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":13917705},\"end\":98345,\"start\":98019},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":13687628},\"end\":98720,\"start\":98347},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":741188},\"end\":99108,\"start\":98722},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":6875312},\"end\":99535,\"start\":99110},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":52180375},\"end\":99995,\"start\":99537},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":73431591},\"end\":100411,\"start\":99997},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":4712004},\"end\":100735,\"start\":100413},{\"attributes\":{\"doi\":\"arXiv:2009.09633\",\"id\":\"b77\"},\"end\":101014,\"start\":100737},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":205242740},\"end\":101451,\"start\":101016},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":3129455},\"end\":101651,\"start\":101453},{\"attributes\":{\"doi\":\"arXiv:1801.09847\",\"id\":\"b80\"},\"end\":101857,\"start\":101653},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":7684883},\"end\":102294,\"start\":101859},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":6746759},\"end\":102664,\"start\":102296},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":6208256},\"end\":102880,\"start\":102666}]", "bib_title": "[{\"end\":73732,\"start\":73681},{\"end\":74128,\"start\":74062},{\"end\":74503,\"start\":74428},{\"end\":74820,\"start\":74766},{\"end\":75129,\"start\":75090},{\"end\":75492,\"start\":75429},{\"end\":75882,\"start\":75755},{\"end\":76389,\"start\":76322},{\"end\":76867,\"start\":76786},{\"end\":77255,\"start\":77154},{\"end\":77625,\"start\":77571},{\"end\":78189,\"start\":78124},{\"end\":78577,\"start\":78529},{\"end\":78948,\"start\":78900},{\"end\":79545,\"start\":79481},{\"end\":79885,\"start\":79834},{\"end\":80174,\"start\":80102},{\"end\":80496,\"start\":80476},{\"end\":80715,\"start\":80664},{\"end\":80961,\"start\":80922},{\"end\":81206,\"start\":81148},{\"end\":81495,\"start\":81444},{\"end\":81758,\"start\":81708},{\"end\":82040,\"start\":82005},{\"end\":82319,\"start\":82254},{\"end\":82674,\"start\":82627},{\"end\":83025,\"start\":82988},{\"end\":83399,\"start\":83336},{\"end\":83693,\"start\":83652},{\"end\":84024,\"start\":83956},{\"end\":84412,\"start\":84304},{\"end\":84827,\"start\":84746},{\"end\":85262,\"start\":85185},{\"end\":85624,\"start\":85557},{\"end\":86002,\"start\":85925},{\"end\":86377,\"start\":86319},{\"end\":86699,\"start\":86646},{\"end\":86989,\"start\":86942},{\"end\":87260,\"start\":87185},{\"end\":87608,\"start\":87547},{\"end\":87929,\"start\":87856},{\"end\":88279,\"start\":88186},{\"end\":88661,\"start\":88550},{\"end\":89034,\"start\":88971},{\"end\":89357,\"start\":89310},{\"end\":89846,\"start\":89798},{\"end\":90168,\"start\":90126},{\"end\":90528,\"start\":90483},{\"end\":90832,\"start\":90773},{\"end\":91412,\"start\":91383},{\"end\":91702,\"start\":91640},{\"end\":92205,\"start\":92097},{\"end\":92890,\"start\":92832},{\"end\":93195,\"start\":93119},{\"end\":93686,\"start\":93608},{\"end\":94026,\"start\":93956},{\"end\":94578,\"start\":94438},{\"end\":95130,\"start\":95065},{\"end\":95603,\"start\":95522},{\"end\":96065,\"start\":95998},{\"end\":96467,\"start\":96399},{\"end\":96823,\"start\":96745},{\"end\":97215,\"start\":97127},{\"end\":97530,\"start\":97473},{\"end\":97819,\"start\":97769},{\"end\":98061,\"start\":98019},{\"end\":98403,\"start\":98347},{\"end\":98785,\"start\":98722},{\"end\":99162,\"start\":99110},{\"end\":99582,\"start\":99537},{\"end\":100085,\"start\":99997},{\"end\":100471,\"start\":100413},{\"end\":101071,\"start\":101016},{\"end\":101503,\"start\":101453},{\"end\":101920,\"start\":101859},{\"end\":102375,\"start\":102296},{\"end\":102716,\"start\":102666}]", "bib_author": "[{\"end\":73742,\"start\":73734},{\"end\":73748,\"start\":73742},{\"end\":73756,\"start\":73748},{\"end\":73767,\"start\":73756},{\"end\":73776,\"start\":73767},{\"end\":73792,\"start\":73776},{\"end\":74139,\"start\":74130},{\"end\":74147,\"start\":74139},{\"end\":74156,\"start\":74147},{\"end\":74164,\"start\":74156},{\"end\":74177,\"start\":74164},{\"end\":74184,\"start\":74177},{\"end\":74196,\"start\":74184},{\"end\":74204,\"start\":74196},{\"end\":74515,\"start\":74505},{\"end\":74523,\"start\":74515},{\"end\":74829,\"start\":74822},{\"end\":74835,\"start\":74829},{\"end\":74843,\"start\":74835},{\"end\":74851,\"start\":74843},{\"end\":74858,\"start\":74851},{\"end\":74865,\"start\":74858},{\"end\":74871,\"start\":74865},{\"end\":75142,\"start\":75131},{\"end\":75153,\"start\":75142},{\"end\":75165,\"start\":75153},{\"end\":75173,\"start\":75165},{\"end\":75502,\"start\":75494},{\"end\":75511,\"start\":75502},{\"end\":75520,\"start\":75511},{\"end\":75531,\"start\":75520},{\"end\":75891,\"start\":75884},{\"end\":75900,\"start\":75891},{\"end\":75906,\"start\":75900},{\"end\":75914,\"start\":75906},{\"end\":75920,\"start\":75914},{\"end\":75927,\"start\":75920},{\"end\":75935,\"start\":75927},{\"end\":75942,\"start\":75935},{\"end\":75951,\"start\":75942},{\"end\":75958,\"start\":75951},{\"end\":76398,\"start\":76391},{\"end\":76404,\"start\":76398},{\"end\":76413,\"start\":76404},{\"end\":76421,\"start\":76413},{\"end\":76428,\"start\":76421},{\"end\":76434,\"start\":76428},{\"end\":76877,\"start\":76869},{\"end\":76883,\"start\":76877},{\"end\":76891,\"start\":76883},{\"end\":76899,\"start\":76891},{\"end\":76905,\"start\":76899},{\"end\":76912,\"start\":76905},{\"end\":77265,\"start\":77257},{\"end\":77275,\"start\":77265},{\"end\":77284,\"start\":77275},{\"end\":77294,\"start\":77284},{\"end\":77303,\"start\":77294},{\"end\":77636,\"start\":77627},{\"end\":77650,\"start\":77636},{\"end\":77664,\"start\":77650},{\"end\":77673,\"start\":77664},{\"end\":77987,\"start\":77980},{\"end\":77994,\"start\":77987},{\"end\":78003,\"start\":77994},{\"end\":78199,\"start\":78191},{\"end\":78208,\"start\":78199},{\"end\":78214,\"start\":78208},{\"end\":78220,\"start\":78214},{\"end\":78226,\"start\":78220},{\"end\":78233,\"start\":78226},{\"end\":78240,\"start\":78233},{\"end\":78252,\"start\":78240},{\"end\":78589,\"start\":78579},{\"end\":78603,\"start\":78589},{\"end\":78614,\"start\":78603},{\"end\":78625,\"start\":78614},{\"end\":78636,\"start\":78625},{\"end\":78960,\"start\":78950},{\"end\":78974,\"start\":78960},{\"end\":78986,\"start\":78974},{\"end\":79252,\"start\":79245},{\"end\":79262,\"start\":79252},{\"end\":79272,\"start\":79262},{\"end\":79283,\"start\":79272},{\"end\":79290,\"start\":79283},{\"end\":79303,\"start\":79290},{\"end\":79553,\"start\":79547},{\"end\":79561,\"start\":79553},{\"end\":79576,\"start\":79561},{\"end\":79596,\"start\":79576},{\"end\":79899,\"start\":79887},{\"end\":79910,\"start\":79899},{\"end\":79919,\"start\":79910},{\"end\":80182,\"start\":80176},{\"end\":80188,\"start\":80182},{\"end\":80202,\"start\":80188},{\"end\":80211,\"start\":80202},{\"end\":80223,\"start\":80211},{\"end\":80234,\"start\":80223},{\"end\":80509,\"start\":80498},{\"end\":80521,\"start\":80509},{\"end\":80725,\"start\":80717},{\"end\":80732,\"start\":80725},{\"end\":80739,\"start\":80732},{\"end\":80974,\"start\":80963},{\"end\":80983,\"start\":80974},{\"end\":81219,\"start\":81208},{\"end\":81231,\"start\":81219},{\"end\":81240,\"start\":81231},{\"end\":81508,\"start\":81497},{\"end\":81518,\"start\":81508},{\"end\":81529,\"start\":81518},{\"end\":81768,\"start\":81760},{\"end\":81777,\"start\":81768},{\"end\":81786,\"start\":81777},{\"end\":81796,\"start\":81786},{\"end\":82053,\"start\":82042},{\"end\":82059,\"start\":82053},{\"end\":82067,\"start\":82059},{\"end\":82076,\"start\":82067},{\"end\":82330,\"start\":82321},{\"end\":82336,\"start\":82330},{\"end\":82347,\"start\":82336},{\"end\":82360,\"start\":82347},{\"end\":82371,\"start\":82360},{\"end\":82385,\"start\":82371},{\"end\":82684,\"start\":82676},{\"end\":82693,\"start\":82684},{\"end\":82704,\"start\":82693},{\"end\":82712,\"start\":82704},{\"end\":82720,\"start\":82712},{\"end\":82729,\"start\":82720},{\"end\":83043,\"start\":83027},{\"end\":83053,\"start\":83043},{\"end\":83061,\"start\":83053},{\"end\":83072,\"start\":83061},{\"end\":83084,\"start\":83072},{\"end\":83411,\"start\":83401},{\"end\":83419,\"start\":83411},{\"end\":83428,\"start\":83419},{\"end\":83705,\"start\":83695},{\"end\":83716,\"start\":83705},{\"end\":83730,\"start\":83716},{\"end\":83740,\"start\":83730},{\"end\":83749,\"start\":83740},{\"end\":84033,\"start\":84026},{\"end\":84041,\"start\":84033},{\"end\":84052,\"start\":84041},{\"end\":84421,\"start\":84414},{\"end\":84427,\"start\":84421},{\"end\":84436,\"start\":84427},{\"end\":84451,\"start\":84436},{\"end\":84457,\"start\":84451},{\"end\":84836,\"start\":84829},{\"end\":84847,\"start\":84836},{\"end\":84858,\"start\":84847},{\"end\":84866,\"start\":84858},{\"end\":84875,\"start\":84866},{\"end\":84886,\"start\":84875},{\"end\":85272,\"start\":85264},{\"end\":85281,\"start\":85272},{\"end\":85290,\"start\":85281},{\"end\":85301,\"start\":85290},{\"end\":85632,\"start\":85626},{\"end\":85639,\"start\":85632},{\"end\":85647,\"start\":85639},{\"end\":85654,\"start\":85647},{\"end\":85662,\"start\":85654},{\"end\":86012,\"start\":86004},{\"end\":86022,\"start\":86012},{\"end\":86030,\"start\":86022},{\"end\":86038,\"start\":86030},{\"end\":86044,\"start\":86038},{\"end\":86390,\"start\":86379},{\"end\":86401,\"start\":86390},{\"end\":86413,\"start\":86401},{\"end\":86708,\"start\":86701},{\"end\":86716,\"start\":86708},{\"end\":86723,\"start\":86716},{\"end\":87004,\"start\":86991},{\"end\":87016,\"start\":87004},{\"end\":87026,\"start\":87016},{\"end\":87274,\"start\":87262},{\"end\":87284,\"start\":87274},{\"end\":87293,\"start\":87284},{\"end\":87303,\"start\":87293},{\"end\":87618,\"start\":87610},{\"end\":87625,\"start\":87618},{\"end\":87633,\"start\":87625},{\"end\":87639,\"start\":87633},{\"end\":87938,\"start\":87931},{\"end\":87944,\"start\":87938},{\"end\":87956,\"start\":87944},{\"end\":88288,\"start\":88281},{\"end\":88297,\"start\":88288},{\"end\":88304,\"start\":88297},{\"end\":88675,\"start\":88663},{\"end\":88681,\"start\":88675},{\"end\":88692,\"start\":88681},{\"end\":89045,\"start\":89036},{\"end\":89056,\"start\":89045},{\"end\":89370,\"start\":89359},{\"end\":89381,\"start\":89370},{\"end\":89394,\"start\":89381},{\"end\":89607,\"start\":89601},{\"end\":89614,\"start\":89607},{\"end\":89622,\"start\":89614},{\"end\":89630,\"start\":89622},{\"end\":89636,\"start\":89630},{\"end\":89647,\"start\":89636},{\"end\":89854,\"start\":89848},{\"end\":89862,\"start\":89854},{\"end\":89871,\"start\":89862},{\"end\":89877,\"start\":89871},{\"end\":89884,\"start\":89877},{\"end\":90180,\"start\":90170},{\"end\":90190,\"start\":90180},{\"end\":90201,\"start\":90190},{\"end\":90210,\"start\":90201},{\"end\":90222,\"start\":90210},{\"end\":90539,\"start\":90530},{\"end\":90553,\"start\":90539},{\"end\":90845,\"start\":90834},{\"end\":90854,\"start\":90845},{\"end\":90862,\"start\":90854},{\"end\":90874,\"start\":90862},{\"end\":91212,\"start\":91198},{\"end\":91224,\"start\":91212},{\"end\":91238,\"start\":91224},{\"end\":91248,\"start\":91238},{\"end\":91422,\"start\":91414},{\"end\":91430,\"start\":91422},{\"end\":91438,\"start\":91430},{\"end\":91447,\"start\":91438},{\"end\":91457,\"start\":91447},{\"end\":91712,\"start\":91704},{\"end\":91720,\"start\":91712},{\"end\":91728,\"start\":91720},{\"end\":91736,\"start\":91728},{\"end\":92217,\"start\":92207},{\"end\":92226,\"start\":92217},{\"end\":92235,\"start\":92226},{\"end\":92696,\"start\":92687},{\"end\":92702,\"start\":92696},{\"end\":92708,\"start\":92702},{\"end\":92714,\"start\":92708},{\"end\":92720,\"start\":92714},{\"end\":92901,\"start\":92892},{\"end\":92908,\"start\":92901},{\"end\":93205,\"start\":93197},{\"end\":93211,\"start\":93205},{\"end\":93217,\"start\":93211},{\"end\":93229,\"start\":93217},{\"end\":93696,\"start\":93688},{\"end\":93702,\"start\":93696},{\"end\":93708,\"start\":93702},{\"end\":93720,\"start\":93708},{\"end\":94041,\"start\":94028},{\"end\":94056,\"start\":94041},{\"end\":94067,\"start\":94056},{\"end\":94076,\"start\":94067},{\"end\":94591,\"start\":94580},{\"end\":94601,\"start\":94591},{\"end\":94611,\"start\":94601},{\"end\":94619,\"start\":94611},{\"end\":94631,\"start\":94619},{\"end\":95140,\"start\":95132},{\"end\":95149,\"start\":95140},{\"end\":95156,\"start\":95149},{\"end\":95165,\"start\":95156},{\"end\":95173,\"start\":95165},{\"end\":95614,\"start\":95605},{\"end\":95622,\"start\":95614},{\"end\":95629,\"start\":95622},{\"end\":95637,\"start\":95629},{\"end\":95646,\"start\":95637},{\"end\":95653,\"start\":95646},{\"end\":96075,\"start\":96067},{\"end\":96081,\"start\":96075},{\"end\":96092,\"start\":96081},{\"end\":96099,\"start\":96092},{\"end\":96106,\"start\":96099},{\"end\":96118,\"start\":96106},{\"end\":96484,\"start\":96469},{\"end\":96499,\"start\":96484},{\"end\":96507,\"start\":96499},{\"end\":96835,\"start\":96825},{\"end\":96841,\"start\":96835},{\"end\":96849,\"start\":96841},{\"end\":96857,\"start\":96849},{\"end\":96869,\"start\":96857},{\"end\":97227,\"start\":97217},{\"end\":97237,\"start\":97227},{\"end\":97543,\"start\":97532},{\"end\":97554,\"start\":97543},{\"end\":97832,\"start\":97821},{\"end\":97842,\"start\":97832},{\"end\":98069,\"start\":98063},{\"end\":98076,\"start\":98069},{\"end\":98084,\"start\":98076},{\"end\":98093,\"start\":98084},{\"end\":98105,\"start\":98093},{\"end\":98113,\"start\":98105},{\"end\":98124,\"start\":98113},{\"end\":98132,\"start\":98124},{\"end\":98413,\"start\":98405},{\"end\":98421,\"start\":98413},{\"end\":98429,\"start\":98421},{\"end\":98436,\"start\":98429},{\"end\":98445,\"start\":98436},{\"end\":98452,\"start\":98445},{\"end\":98458,\"start\":98452},{\"end\":98793,\"start\":98787},{\"end\":98800,\"start\":98793},{\"end\":98809,\"start\":98800},{\"end\":98818,\"start\":98809},{\"end\":98825,\"start\":98818},{\"end\":98834,\"start\":98825},{\"end\":98840,\"start\":98834},{\"end\":98852,\"start\":98840},{\"end\":98860,\"start\":98852},{\"end\":99172,\"start\":99164},{\"end\":99183,\"start\":99172},{\"end\":99192,\"start\":99183},{\"end\":99202,\"start\":99192},{\"end\":99217,\"start\":99202},{\"end\":99227,\"start\":99217},{\"end\":99237,\"start\":99227},{\"end\":99252,\"start\":99237},{\"end\":99590,\"start\":99584},{\"end\":99597,\"start\":99590},{\"end\":99605,\"start\":99597},{\"end\":99611,\"start\":99605},{\"end\":99618,\"start\":99611},{\"end\":99626,\"start\":99618},{\"end\":99632,\"start\":99626},{\"end\":100099,\"start\":100087},{\"end\":100107,\"start\":100099},{\"end\":100118,\"start\":100107},{\"end\":100127,\"start\":100118},{\"end\":100480,\"start\":100473},{\"end\":100487,\"start\":100480},{\"end\":100493,\"start\":100487},{\"end\":100501,\"start\":100493},{\"end\":100509,\"start\":100501},{\"end\":100745,\"start\":100739},{\"end\":100752,\"start\":100745},{\"end\":100759,\"start\":100752},{\"end\":100767,\"start\":100759},{\"end\":100775,\"start\":100767},{\"end\":100786,\"start\":100775},{\"end\":100793,\"start\":100786},{\"end\":101081,\"start\":101073},{\"end\":101096,\"start\":101081},{\"end\":101106,\"start\":101096},{\"end\":101116,\"start\":101106},{\"end\":101126,\"start\":101116},{\"end\":101141,\"start\":101126},{\"end\":101151,\"start\":101141},{\"end\":101165,\"start\":101151},{\"end\":101180,\"start\":101165},{\"end\":101193,\"start\":101180},{\"end\":101516,\"start\":101505},{\"end\":101713,\"start\":101702},{\"end\":101721,\"start\":101713},{\"end\":101731,\"start\":101721},{\"end\":101929,\"start\":101922},{\"end\":101940,\"start\":101929},{\"end\":101949,\"start\":101940},{\"end\":101959,\"start\":101949},{\"end\":101973,\"start\":101959},{\"end\":101984,\"start\":101973},{\"end\":102384,\"start\":102377},{\"end\":102390,\"start\":102384},{\"end\":102402,\"start\":102390},{\"end\":102733,\"start\":102718},{\"end\":102741,\"start\":102733},{\"end\":102751,\"start\":102741}]", "bib_venue": "[{\"end\":73850,\"start\":73792},{\"end\":74218,\"start\":74204},{\"end\":74580,\"start\":74523},{\"end\":74908,\"start\":74871},{\"end\":75241,\"start\":75173},{\"end\":75573,\"start\":75531},{\"end\":76016,\"start\":75958},{\"end\":76505,\"start\":76434},{\"end\":76940,\"start\":76912},{\"end\":77331,\"start\":77303},{\"end\":77731,\"start\":77673},{\"end\":77978,\"start\":77932},{\"end\":78314,\"start\":78252},{\"end\":78694,\"start\":78636},{\"end\":79014,\"start\":78986},{\"end\":79243,\"start\":79180},{\"end\":79644,\"start\":79596},{\"end\":79948,\"start\":79919},{\"end\":80262,\"start\":80234},{\"end\":80553,\"start\":80521},{\"end\":80767,\"start\":80739},{\"end\":81011,\"start\":80983},{\"end\":81268,\"start\":81240},{\"end\":81558,\"start\":81529},{\"end\":81830,\"start\":81796},{\"end\":82104,\"start\":82076},{\"end\":82413,\"start\":82385},{\"end\":82787,\"start\":82729},{\"end\":83142,\"start\":83084},{\"end\":83475,\"start\":83428},{\"end\":83784,\"start\":83749},{\"end\":84110,\"start\":84052},{\"end\":84505,\"start\":84457},{\"end\":84944,\"start\":84886},{\"end\":85349,\"start\":85301},{\"end\":85720,\"start\":85662},{\"end\":86102,\"start\":86044},{\"end\":86463,\"start\":86413},{\"end\":86775,\"start\":86723},{\"end\":87046,\"start\":87026},{\"end\":87347,\"start\":87303},{\"end\":87680,\"start\":87639},{\"end\":88003,\"start\":87956},{\"end\":88341,\"start\":88304},{\"end\":88734,\"start\":88692},{\"end\":89124,\"start\":89056},{\"end\":89414,\"start\":89394},{\"end\":89599,\"start\":89550},{\"end\":89942,\"start\":89884},{\"end\":90280,\"start\":90222},{\"end\":90611,\"start\":90553},{\"end\":90932,\"start\":90874},{\"end\":91196,\"start\":91130},{\"end\":91499,\"start\":91457},{\"end\":91817,\"start\":91736},{\"end\":92316,\"start\":92235},{\"end\":92685,\"start\":92626},{\"end\":92965,\"start\":92908},{\"end\":93310,\"start\":93229},{\"end\":93769,\"start\":93720},{\"end\":94153,\"start\":94076},{\"end\":94708,\"start\":94631},{\"end\":95250,\"start\":95173},{\"end\":95720,\"start\":95653},{\"end\":96176,\"start\":96118},{\"end\":96554,\"start\":96507},{\"end\":96916,\"start\":96869},{\"end\":97285,\"start\":97237},{\"end\":97603,\"start\":97554},{\"end\":97870,\"start\":97842},{\"end\":98160,\"start\":98132},{\"end\":98513,\"start\":98458},{\"end\":98888,\"start\":98860},{\"end\":99300,\"start\":99252},{\"end\":99713,\"start\":99632},{\"end\":100185,\"start\":100127},{\"end\":100556,\"start\":100509},{\"end\":101199,\"start\":101193},{\"end\":101533,\"start\":101516},{\"end\":101700,\"start\":101653},{\"end\":102036,\"start\":101984},{\"end\":102460,\"start\":102402},{\"end\":102755,\"start\":102751},{\"end\":76563,\"start\":76507},{\"end\":91885,\"start\":91819},{\"end\":92384,\"start\":92318},{\"end\":93378,\"start\":93312},{\"end\":94217,\"start\":94155},{\"end\":94772,\"start\":94710},{\"end\":95314,\"start\":95252},{\"end\":95774,\"start\":95722},{\"end\":99781,\"start\":99715},{\"end\":102084,\"start\":102038}]"}}}, "year": 2023, "month": 12, "day": 17}