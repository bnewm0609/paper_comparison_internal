{"id": 233460170, "updated": "2023-10-06 04:21:36.842", "metadata": {"title": "EEG-based detection of the locus of auditory attention with convolutional neural networks", "authors": "[{\"first\":\"Servaas\",\"last\":\"Vandecappelle\",\"middle\":[]},{\"first\":\"Lucas\",\"last\":\"Deckers\",\"middle\":[]},{\"first\":\"Neetha\",\"last\":\"Das\",\"middle\":[]},{\"first\":\"Amir\",\"last\":\"Ansari\",\"middle\":[\"Hossein\"]},{\"first\":\"Alexander\",\"last\":\"Bertrand\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Francart\",\"middle\":[]}]", "venue": "eLife", "journal": "eLife", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "In a multi-speaker scenario, the human auditory system is able to attend to one particular speaker of interest and ignore the others. It has been demonstrated that it is possible to use electroencephalography (EEG) signals to infer to which speaker someone is attending by relating the neural activity to the speech signals. However, classifying auditory attention within a short time interval remains the main challenge. We present a convolutional neural network-based approach to extract the locus of auditory attention (left/right) without knowledge of the speech envelopes. Our results show that it is possible to decode the locus of attention within 1\u20132 s, with a median accuracy of around 81%. These results are promising for neuro-steered noise suppression in hearing aids, in particular in scenarios where per-speaker envelopes are unavailable.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "33929315", "pubmedcentral": "8143791", "dblp": null, "doi": "10.7554/elife.56481"}}, "content": {"source": {"pdf_hash": "bffca5cee59304fb2a093aa36137dc9ca3e4e217", "pdf_src": "PubMedCentral", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://doi.org/10.7554/elife.56481", "status": "GOLD"}}, "grobid": {"id": "29464875f65978d65c7b4dd58b09298e8b974883", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bffca5cee59304fb2a093aa36137dc9ca3e4e217.txt", "contents": "\nEEG-based detection of the locus of auditory attention with convolutional neural networks\nPublished: 30 April 2021\n\nServaas Vandecappelle servaas.vandecappelle@gmail.comsv \nDepartment of Neurosciences\nExperimental Oto-rhino-laryngology\nLeuvenBelgium\n\nDepartment of Electrical Engineering (ESAT)\nStadius Center for Dynamical Systems\nSignal Processing and Data Analytics\nLeuvenBelgium\n\nLucas Deckers \nDepartment of Neurosciences\nExperimental Oto-rhino-laryngology\nLeuvenBelgium\n\nDepartment of Electrical Engineering (ESAT)\nStadius Center for Dynamical Systems\nSignal Processing and Data Analytics\nLeuvenBelgium\n\nNeetha Das \nDepartment of Neurosciences\nExperimental Oto-rhino-laryngology\nLeuvenBelgium\n\nDepartment of Electrical Engineering (ESAT)\nStadius Center for Dynamical Systems\nSignal Processing and Data Analytics\nLeuvenBelgium\n\nAmir Hossein Ansari \nDepartment of Electrical Engineering (ESAT)\nStadius Center for Dynamical Systems\nSignal Processing and Data Analytics\nLeuvenBelgium\n\nAlexander Bertrand \nDepartment of Electrical Engineering (ESAT)\nStadius Center for Dynamical Systems\nSignal Processing and Data Analytics\nLeuvenBelgium\n\nTom Francart tom.francart@kuleuven.betf \nDepartment of Neurosciences\nExperimental Oto-rhino-laryngology\nLeuvenBelgium\n\nEEG-based detection of the locus of auditory attention with convolutional neural networks\nPublished: 30 April 202110.7554/eLife.56481Received: 28 February 2020 Accepted: 28 April 2021*For correspondence: Competing interests: The authors declare that no competing interests exist. Funding: See page 14 Reviewing editor: Barbara G Shinn-Cunningham, Carnegie Mellon University, United States\nIn a multi-speaker scenario, the human auditory system is able to attend to one particular speaker of interest and ignore the others. It has been demonstrated that it is possible to use electroencephalography (EEG) signals to infer to which speaker someone is attending by relating the neural activity to the speech signals. However, classifying auditory attention within a short time interval remains the main challenge. We present a convolutional neural network-based approach to extract the locus of auditory attention (left/right) without knowledge of the speech envelopes. Our results show that it is possible to decode the locus of attention within 1-2 s, with a median accuracy of around 81%. These results are promising for neuro-steered noise suppression in hearing aids, in particular in scenarios where per-speaker envelopes are unavailable. Vandecappelle et al. eLife 2021;10:e56481. DOI: https://doi.org/10.7554/eLife.56481 TOOLS AND RESOURCES modeling approach: predicting EEG from the auditory stimulus(Akram et al., 2016;Alickovic et al., 2016), canonical correlation analysis (CCA)-based methods(de Cheveign\u00e9 et al., 2018), and Bayesian state-space modeling(Miran et al., 2018).All studies mentioned above are based on linear decoders. However, since the human auditory system is inherently nonlinear(Faure and Korn, 2001), nonlinear models (such as neural networks) could be beneficial for reliable and quick AAD. In Taillez et al., 2017, a feedforward neural network for EEG-based speech stimulus reconstruction was presented, showing that artificial neural networks are a feasible alternative to linear decoding methods.Recently, convolutional neural networks (CNNs) have become the preferred approach for many recognition and detection tasks, in particular in the field of image classification(LeCun et al., 2015). Recent research on CNNs has also shown promising results for EEG classification: in seizure detection (Acharya et al., 2018a; Ansari et al., 2018a), depression detection (Liu et al., 2017), and sleep stage classification (Acharya et al., 2018b; Ansari et al., 2018b). In terms of EEG-based AAD, Ciccarelli et al., 2019 recently showed that a (subject-dependent) CNN using a classification approach can outperform linear methods for decision windows of 10 s.Current state-of-the-art models are thus capable of classifying auditory attention in a two-speaker scenario with high accuracy (75-85%) over a data window with a length of 10 s, but their performance drops drastically when shorter windows are used (e.g.,de Cheveign\u00e9 et al., 2018;Ciccarelli et al., 2019). However, to achieve sufficiently fast AAD-based steering of a hearing aid, short decision windows (down to a few seconds) are required. This inherent trade-off between accuracy and decision window length was investigated by Geirnaert et al., 2020, who proposed a method to combine both properties into a single metric, by searching for the optimal trade-off point to minimize the expected switch duration in an AAD-based volume control system with robustness constraints. The robustness against AAD errors can be improved by using smaller relative volume changes for every new AAD decision, while the decision window length determines how often an AAD decision (volume step) is made. It was found that such systems favor short window lengths (<< 10 s) with mediocre accuracy over long windows (10-30 s) with high accuracy.Apart from decoding which speech envelope corresponds to the attended speaker, it may also be possible to decode the spatial locus of attention. That is, not decoding which speaker is attended to, but rather which location in space. The benefit of this approach for neuro-steered auditory prostheses is that no access to the clean speech stimuli is needed. This has been investigated based on differences in the EEG entropy features(Lu et al., 2018), but the performance was insufficient for practical use (below 70% for 60 s windows). However, recent research(Wolbers et al., 2011;Bednar and Lalor, 2018;Patel et al., 2018;O'Sullivan et al., 2019;Bednar and Lalor, 2020)has shown that the direction of auditory attention is neurally encoded, indicating that it could be possible to decode the attended sound position or trajectory from EEG. A few studies employing MEG have suggested that in particular the alpha power band could be tracked to determine the locus of auditory attention(Frey et al., 2014;W\u00f6 stmann et al., 2016). Another study, employing scalp EEG, found the beta power band related with selective attention(Gao et al., 2017).The aim of this paper is to further explore the possibilities of CNNs for EEG-based AAD. As opposed toTaillez et al., 2017 andCiccarelli et al., 2019, who aim to decode the attended speaker (for a given set of speech envelopes), we aim to decode the locus of auditory attention (left/right). When the locus of attention is known, a hearing aid can steer a beamformer in that direction to enhance the attended speaker.\n\nIntroduction\n\nIn a multi-speaker scenario, the human auditory system is able to focus on just one speaker, ignoring all other speakers and noise. This situation is called the 'cocktail party problem' (Cherry, 1953). However, elderly people and people suffering from hearing loss have particular difficulty attending to one person in such an environment. In current hearing aids, this problem is mitigated by automatic noise suppression systems. When multiple speakers are present, however, these systems have to rely on heuristics such as the speaker volume or the listener's look direction to determine the relevant speaker, which often fail in practice.\n\nThe emerging field of auditory attention decoding (AAD) tackles the challenge of directly decoding auditory attention from neural activity, which may replace such unreliable and indirect heuristics. This research finds applications in the development of neuro-steered hearing prostheses that analyze brain signals to automatically decode the direction or speaker to whom the user is attending, to subsequently amplify that specific speech stream while suppressing other speech streams and surrounding noise. The desired result is increased speech intelligibility for the listener.\n\nIn a competing two-speaker scenario, it has been shown that the neural activity (as recorded using electroencephalography [EEG] or magnetoencephalography [MEG]) consistently tracks the dynamic variation of an incoming speech envelope during auditory processing, and that the attended speech envelope is typically more pronounced than the unattended speech envelope (Ding and Simon, 2012;O'Sullivan et al., 2015). This neural tracking of the stimulus can then be used to determine auditory attention. A common approach is stimulus reconstruction, where the poststimulus brain activity is used to decode and reconstruct the attended stimulus envelope (O'Sullivan et al., 2015;Pasley et al., 2012). The reconstructed envelope is then correlated with the original stimulus envelopes, and the one yielding the highest correlation is then considered to belong to the attended speaker. Other methods for attention decoding include the forward\n\n\nMaterials and methods\n\n\nExperiment setup\n\nThe dataset used for this work was gathered previously (Das et al., 2016). EEG data was collected from 16 normal-hearing subjects while they listened to two competing speakers and were instructed to attend to one particular speaker. Every subject signed an informed consent form approved by the KU Leuven ethical committee.\n\nThe EEG data was recorded using a 64-channel BioSemi ActiveTwo system, at a sampling rate of 8196 Hz, in an electromagnetically shielded and soundproof room. The auditory stimuli were low-pass filtered with a cutoff frequency of 4 kHz and presented at 60 dBA through Etymotic ER3 insert earphones. APEX 3 was used as stimulation software (Francart et al., 2008).\n\nThe auditory stimuli were comprised of four Dutch stories, narrated by three male Flemish speakers (DeBuren, 2007). Each story was 12 min long and split into two parts of 6 min each. Silent segments longer than 500 ms were shortened to 500 ms. The stimuli were set to equal root-meansquare intensities and were perceived as equally loud.\n\nThe experiment was split into eight trials, each 6 min long. In every trial, subjects were presented with two parts of two different stories. One part was presented in the left ear, while the other was presented in the right ear. Subjects were instructed to attend to one of the two via a monitor positioned in front of them. The symbol '<' was shown on the left side of the screen when subjects had to attend to the story in the left ear, and the symbol '>' was shown on the right side of the screen when subjects had to attend to the story in the right ear. They did not receive instructions on where to focus their gaze.\n\nIn subsequent trials, subjects attended either to the second part of the same story (so they could follow the story line) or to the first part of the next story. After each trial, subjects completed a multiple-choice quiz about the attended story. In total, there was 8 \u00c2 6 min = 48 min of data per subject. For an example of how stimuli were presented, see Table 1. (The original experiment [Das et al., 2016] contained 12 additional trials of 2 min each, collected at the end of every measurement session. These trials were repetitions of earlier stimuli and were not used in this work.)\n\nThe attended ear alternated over consecutive trials to get an equal amount of data per ear (and per subject), which is important to avoid the lateralization bias described by Das et al., 2016. Stimuli were presented in the same order to each subject, and either dichotically or after head-related transfer function (HRTF) filtering (simulating sound coming from \u00b190 deg). As with the attended ear, the HRTF/dichotic condition was randomized and balanced within and over subjects. In this work, we do not distinguish between dichotic and HRTF to ensure there is as much data as possible for training the neural network.\n\n\nData preprocessing\n\nThe EEG data was filtered with an equiripple FIR bandpass filter and its group delay was compensated for. For use with linear models, the EEG was filtered between 1 and 9 Hz, which has been found to be an optimal frequency range for linear attention decoding (Pasley et al., 2012;Ding and Simon, 2012). For the CNN models, a broader bandwidth between 1 and 32 Hz was used, as Taillez et al., 2017 show that this is more optimal. In both cases, the maximal bandpass attenuation was 0.5 dB while the stopband attenuation was 20 dB (at 0-1 Hz) and 15 dB (at 32-64 Hz). After the bandpass filtering, the EEG data was downsampled to 20 Hz (linear model) and 128 Hz (CNN). Artifacts were removed with the generic MWF-based removal algorithm described in Somers et al., 2018. Table 1. First eight trials for a random subject. Trials are numbered according to the order in which they were presented to the subject. Which ear was attended to first was determined randomly. After that, the attended ear was alternated. Presentation (dichotic/HRTF) was balanced over subjects with respect to the attended ear. Adapted from Das et al., 2016. HRTF = head-related transfer function. Data of each subject was divided into a training, validation, and test set. Per set, data segments were generated with a sliding window equal in size to the chosen window length and with an overlap of 50%. Data was normalized on a subject-by-subject basis, based on statistics of the training set only, and in such a way that proportions between EEG channels were maintained. Concretely, for each subject we calculated the power per channel, based on the 10% trimmed mean of the squared samples. All channels were then divided by the square root of the median of those 64 values (one for each EEG channel). Data of each subject was thus normalized based on a single (subject-specific) value.\n\n\nConvolutional neural networks\n\nA convolutional neural network (CNN) consists of a series of convolutional layers and nonlinear activation functions, typically followed by pooling layers. In convolutional layers, one or more convolutional filters slide over the data to extract local data features. Pooling layers then aggregate the output by computing, for example, the mean. Similar to other types of neural networks, a CNN is optimized by minimizing a loss function, and the optimal parameters are estimated with an optimization algorithm such as stochastic gradient descent.\n\nOur proposed CNN for decoding the locus of auditory attention is shown in Figure 1. The input is a 64 \u00c2 T matrix, where 64 is the number of EEG channels in our dataset and T is the number of samples in the decision window. (We tested multiple decision window lengths, as discussed later.) The first step in the model is a convolutional layer, indicated in blue. Five independent 64 \u00c2 17 spatio-temporal filters are shifted over the input matrix, which, since the first dimension is equal to the number of channels, each result in a time series of dimensions 1 \u00c2 T. Note that '17' is 130 ms at 128 Hz, and 130 ms was found to be an optimal filter width -that is, longer or shorter decision window lengths gave a higher loss on a validation set. A rectifying linear unit (ReLu) activation function is used after the convolution step.\n\nIn the average pooling step, data is averaged over the time dimension, thus reducing each time series to a single number. After the pooling step, there are two fully connected (FC) layers. The first layer contains five neurons (one for each time series) and is followed by a sigmoid activation function, and the second layer contains two (output) neurons. These two neurons are connected to a cross-entropy loss function. Note that with only two directions (left/right), a single output neuron (coupled with a binary cross-entropy loss) would have sufficed as well. With this setup, it is easier to extend to more locations, however. The full CNN consists of approximately 5500 parameters.\n\nThe implementation was done in MATLAB 2016b and MatConvNet (version 1.0-beta25), a CNN toolbox for MATLAB (Vedaldi and Lenc, 2015). The source code is available at https://github.com/ exporl/locus-of-auditory-attention-cnn (copy archived at swh:1:rev:3e5e21a7e6072182e076-f9863ebc82b85e7a01b1; Vandecappelle, 2021).\n\n\nCNN training and evaluation\n\nThe model was trained on data of all subjects, including the subject it was tested on (but without using the same data for both training and testing). This means we are training a subject-specific decoder, where the data of the other subjects can be viewed as a regularization or data augmentation technique to avoid overfitting on the (limited) amount of training data of the subject under test. To prevent the model from overfitting to one particular story, we cross-validated over the four stories (resulting in four folds). That is, we held out one story and trained on the remaining three stories (illustrated in Table 2). Such overfitting is not an issue for simple linear models, but may be an issue for the CNN we propose here. Indeed, even showing only the EEG responses to a part of a story could result in the model learning certain story-specific characteristics. That could then lead to overly optimistic results when the model is presented with the EEG responses to another (albeit different) part of the same story. Similarly, since each speaker has their own 'story-telling' characteristics (e.g., speaking rate or intonation), and a different voice timbre, EEG responses to different speakers may differ. Therefore, it is possible that the model gains an advantage by having 'seen' the EEG response to a specific speaker, so we retained only the folds wherein the same speaker was never simultaneously part of both the training and the test set. In the end, only two folds remained (see Table 2). We refer to the combined cross-validation approach as leave-one-story+speaker-out.\n\nIn an additional experiment, we investigated the subject dependency of the model, where, in addition to cross-validating over story and speaker, we also cross-validated over subjects. That is, we no longer trained and tested on N subjects, but instead trained on N \u00c0 1 subjects and tested on the held-out subject. Such a paradigm has the advantage that new subjects do not have to undergo potentially expensive and time-consuming retraining, making it more suitable for real-life applications. Whether it is actually a better choice than subject-specific retraining depends on the difference in performance between the two paradigms. If the difference is sufficiently large, subjectdependent retraining might be a price one is willing to pay.\n\nWe trained the network by minimizing the cross-entropy between the network outputs and the corresponding labels (the attended ear). We used mini-batch stochastic gradient descent with an initial learning rate of 0.09 and a momentum of 0.9. We applied a step decay learning schedule that decreased the learning rate after epoch 10 and 35 to 0.045 and 0.0225, respectively, to assure convergence. The batch size was set to 20, partly because of memory constraints, and partly because we did not see much improvement with larger batch sizes. Weights and biases were initialized by drawing randomly from a normal distribution with a mean of 0 and a standard deviation of 0.5. Table 2. Cross-validating over stories and speakers. With the current dataset, there are only two folds that do not mix stories and speakers across training and test sets. Top: Story 1 as test data; story 2, 3, and 4 as training data and validation data (85/15% division, per story). Bottom: similarly, but now with a different story and speaker as test data. In both cases, the story and speaker are completely unseen by the model. The model is trained on the same training set for all subjects and tested on a unique, subject-specific, test set. Training ran for 100 epochs, as early experiments showed that the optimal decoder was usually found between epoch 70 and 95. Regularization consisted of weight decay with a value of 5 \u00c2 10 -4 , and, after training, of selecting the decoder in the iteration where the validation loss was minimal.\n\nNote that the addition of data of the other subjects can also be viewed as a regularization technique that further reduces the risk of overfitting. All hyperparameters given above were determined by running a grid search over a set of reasonable values. Performance during this grid search was measured on the validation set.\n\nNote that in this work the decoding accuracy is defined as the percentage of correctly classified decision windows on the test set, averaged over the two folds mentioned earlier (one for each story narrated by a different speaker).\n\n\nLinear baseline model (stimulus reconstruction)\n\nA linear stimulus reconstruction model (Biesmans et al., 2017) was used as baseline. In this model, a spatio-temporal filter was trained and applied on the EEG data and its time-shifted versions up to 250 ms delay, based on least-squares regression, in order to reconstruct the envelope of the attended stimulus. The reconstructed envelope was then correlated (Pearson correlation coefficient) with each of the two speaker envelopes over a data window with a predefined length, denoted as the decision window (different lengths were tested). The classification was made by selecting the position corresponding to the speaker that yielded the highest correlation in this decision window. The envelopes were calculated with the 'powerlaw subbands' method proposed by Biesmans et al., 2017; that is, a gammatone filter bank was used to split the speech into subbands, and per subband the envelope was calculated with a power law compression with exponent 0.6. The different subbands were then added again (each with a coefficient of 1) to form the broadband envelope. Envelopes were filtered and downsampled in the same vein as the EEG recordings.\n\nFor a fairer comparison with the CNN, the linear model was also trained in a leave-one-story +speaker-out way. In contrast to the CNN, however, the linear model was not trained on any other data than that of the subject under testing, since including data of other subjects harms the performance of the linear model.\n\nNote that the results of the linear model here merely serve as a representative baseline, and that a comparison between the two models should be treated with care -in part because the CNN is nonlinear, but also because the linear model is only able to relate the EEG to the envelopes of the recorded audio, while the CNN is free to extract any feature it finds optimal (though only from the EEG, as no audio is given to the CNN). Additionally, the preprossessing is slightly different for both models. However, that preprocessing was chosen such that each model would perform optimallyusing the same preprocessing would, in fact, negatively impact one of the two models.\n\n\nMinimal expected switch duration\n\nFor some of the statistical tests below, we use the minimal expected switch duration (MESD) proposed by Geirnaert et al., 2020 as a relevant metric to assess AAD performance. The goal of the MESD metric is to have a single value as measure of performance, resolving the trade-off between accuracy and the decision window length. The MESD was defined as the expected time required for an AAD-based gain control system to reach a stable volume switch between both speakers, following an attention switch of the user. The MESD is calculated by optimizing a Markov chain as a model for the volume control system, which uses the AAD decision time and decoding accuracy as parameters. As a by-product, it provides the optimal volume increment per AAD decision.\n\nOne caveat is that the MESD metric assumes that all decisions are taken independently of each other, but this may not be true when the window length is very small, for example, smaller than 1 s. In that case, the model behind the MESD metric may slightly underestimate the time needed for a stable switch to occur. However, it can still serve as a useful tool for comparing models.\n\n\nResults\n\n\nDecoding performance\n\nSeven different decision window lengths were tested: 10, 5, 2, 1, 0.5, 0.25, and 0.13 s. This defines the amount of data that is used to make a single left/right decision. In the AAD literature, decision windows range from approximately 60 to 5 s. In this work, the focus lies on shorter decision windows. This is done for practical reasons: in neuro-steered hearing aid applications, the detection time should ideally be short enough to quickly detect attention switches of the user.\n\nTo capture the general performance of the CNN, the reported accuracy for each subject is the mean accuracy of 10 different training runs of the model, each with a different (random) initialization. All MESD values in this work are based on these mean accuracies.\n\nThe linear model was not evaluated at a decision window length of 0.13 s since its kernel has a width of 0.25 s, which places a lower bound on the possible decision window length. Figure 2 shows the decoding accuracy at 1 and 10 s for the CNN and the linear model. For both decision windows, the CNN had a higher median decoding accuracy, but a larger intersubject variability. Two subjects had a decoding accuracy lower than 50% at a window length of 10 s, and were therefore not considered in the subsequent analysis, nor are they shown in the figures in this section.\n\nFor 1 s decision windows, a Wilcoxon signed-rank test yielded significant differences in detection accuracy between the linear decoder model and the CNN (W = 3, p < 0.001), with an increase in median accuracy from 58.1 to 80.8%. Similarly, for 10 s decision windows, a Wilcoxon signed-rank test showed a significant difference between the two models (W = 16, p = 0.0203), with the CNN achieving a median accuracy of 85.1% compared to 75.7% for the linear model.\n\nThe minimal expected switch duration (MESD) (Geirnaert et al., 2020) outputs a single number for each subject, given a set of window lengths and corresponding decoding accuracies. This allows for a direct comparison between the linear and the CNN model, independent of window length. As shown in Figure 3, the linear model achieves a median MESD of 22.6 s, while the CNN achieves a median MESD of only 0.819 s. A Wilcoxon signed-rank test shows this difference to be significant (W = 105, p < 0.001). The extremely low MESD for the CNN is the result of the median accuracy still being 68.7% at only 0.13 s, and the fact that the MESD typically chooses the optimal operation point at short decision window lengths (Geirnaert et al., 2020).\n\nIt is not entirely clear why the CNN fails for 2 of the 16 subjects. Our analysis shows that the results depend heavily on the story that is being tested on: for the two subjects with below 50% accuracy, the CNN performed poorly on story 1 and 2, but performed well on stories 3 and 4 (80% and higher). Our results are based on stories 1 and 2, however, since stories 3 and 4 are narrated by the same speaker and we wanted to avoid having the same speaker in both the training and test set. It is possible that the subjects did not comply with the task in these conditions. \n\n\nEffect of decision window length\n\nShorter decision windows contain less information and should therefore result in poorer performance compared to longer decision windows. Figure 4 visualizes the relation between window length and detection accuracy. A linear mixed-effects model fit for decoding accuracy, with decision window length as fixed effect and subject as random effect, shows a significant effect of window length for both the CNN model (df = 96, p < 0.001) and the linear model (df = 94, p < 0.001). The analysis was based on the decision window lengths shown in Figure 4; that is, seven window lengths for the CNN and six for the linear model. \n\n\nInterpretation of results\n\nInterpreting the mechanisms behind a neural network remains a challenge. In an attempt to understand which frequency bands of the EEG the network uses, we retested (without retraining) the model in two ways: (1) by filtering out a certain frequency range ( Figure 5, left); (2) by filtering out everything except a particular frequency range ( Figure 5, right). The frequency ranges are defined as follows: d = 1-4 Hz; q = 4-8 Hz; a = 8-14 Hz; b = 14-32 Hz. Figure 5 shows that the CNN uses mainly information from the beta band, in line with Gao et al., 2017. Note that the poor results for the other frequency bands (Figure 5, right) does not necessarily mean that the network does not use the other bands, but rather, if it does, it is in combination with other bands.\n\nWe additionally investigated the weights of the filters of the convolutional layer, as they give an indication of what channel the model finds important. We calculated the power of the filter weights per channel, and to capture the general trend, we calculated a grand-average over all models (i.e., all window lengths, stories, and runs). Moreover, we normalized the results with the per-channel power of the EEG in the training set, to account for that fact that what comes out of the convolutional layer is a function of both the filter weights and the magnitude of the input.\n\nThe results are shown in Figure 6. We see primarily activations in the frontal and temporal regions, and to a lesser extent also in the occipital lobe. Activations appear to be slightly stronger on the right side, as well. This result is in line with Ciccarelli et al., 2019, who also saw stronger activations in the frontal channels (mostly for the 'Wet 18 CH' and 'Dry 18 CH' systems). Additionally, Gao et al., 2017 also found the frontal channels to significantly differ from the other channels within the beta band ( Figure 3 and Table 1 in Gao et al., 2017). The prior (eye) artifact removal step in the EEG preprocessing and the importance of the beta band in the decision-making ( Figure 5) suggests that the focus on the frontal channel is not necessarily attributed to eye artifacts. It is noted that the filters of the network act as backward decoders, and therefore care should be taken when interpreting topoplots related to the decoder coefficients. As opposed to a forward (encoding) model, the coefficients of a backward (decoding) model are not necessarily predictive for the strength of the neural response in these channels. For example, the network may perform an implicit noise reduction transformation, thereby involving channels with low SNR as well.\n\n\nEffect of validation procedure\n\nIn all previous results, we used a leave-one-story+speaker-out scheme to prevent the CNN from gaining an advantage by already having seen EEG responses elicited by the same speaker or different parts of the same story. However, it is noted that in the majority of the AAD literature, training and test sets often do contain samples from the same speaker or story (albeit from different parts of the story).\n\nTo investigate the impact of cross-validating over speaker and story, we trained the CNN again, but this time using data of each trial (later referred to as 'Every trial'). Here, the training set consisted of the first 75% of each trial, the validation set of the next 15% and the test set of the last 15%. We performed this experiment twice -once using data preprocessed in the manner explained in the ''Data processing'' section, and once with the artefact removal filtering (MWF) stage excluded. Figure 7 shows the results of all three experiments for decision windows of 1 s. Other window lengths show similar results.\n\nFor decision windows of 1 s, using data from all trials, in addition to applying a per-trial MWF filter, results in a median decoding accuracy of 92.8% (Figure 7, right), compared to only 80.8% when leaving out both story and speaker (Figure 7, left). A Wilcoxon signed-rank test shows this difference to be significant (W = 91, p = 0.0134). There is, however, no statistically significant difference in decoding accuracy between leaving out both story and speaker and when using data of all trials, but without applying any spatial filtering for artifact removal (W = 48, p = 0.8077).\n\nIt appears that having the same speaker and story in both the training and test set is less problematic than we had anticipated, and employing a classical scheme wherein both sets draw from the same trials (though use different parts) is fine, but only on the condition that they are preprocessed in a trial-independent way. \n\n\nSubject-independent decoding\n\nIn a final experiment, we investigated how well the CNN performs on subjects that were not part of the training set. Here, the CNN is trained on N -1 subjects and tested on the held-out subject -but still in a leave-one-story+speaker out manner, as before. The results are shown in Figure 8. For windows of 1 s, a Wilcoxon signed-rank test shows that leaving out the test subject results in a significant decrease in decoding accuracy from 80.8% to 69.3% (W = 14, p = 0.0134). Surprisingly, for one subject the network performs better when its data was not included during training. Other window lengths show similar results.\n\n\nDiscussion\n\nWe proposed a novel CNN-based model for decoding the direction of attention (left/right) without access to the stimulus envelopes, and found it to significantly outperform a linear decoder that was trained to reconstruct the envelope of the attended speaker.\n\n\nDecoding accuracy\n\nThe CNN model resulted in a significant increase in decoding accuracy compared to the linear model: for decision windows as low as 1 s, the CNN's median performance is around 81%. This is also better than entropy-based direction classification presented in literature (Lu et al., 2018), in which the average decoding performance proved to be insufficient for real-life use (less than 80%\n\nLeave-one-story+speaker-out Every trial (unprocessed) Every trial (per-trial MWFs) 50 60 70 80 90 100 Accuracy (%) Figure 7. Impact of the model validation strategy on the performance of the CNN (decision windows of 1 s). In Leave-one-story+speaker-out, the training set does not contain examples of the speakers or stories that appear in the test set. In Every trial (unprocessed), the training, validation, and test sets are extracted from every trial (although always disjoint), and no spatial filtering takes places. In Every trial (per-trial MWFs), data is again extracted from every trial, but this time per-trial MWF filters are applied. CNN = convolutional neural network.\n\nfor decision windows of 60 s). Moreover, our network achieves an unprecedented median MESD of only 0.819 s, compared to 22.6 s for the linear method, allowing for robust neuro-steered volume control with a practically acceptable latency. Despite the impressive median accuracy of our CNN, there is clearly more variability between subjects in comparison to the linear model. Figure 4, for example, shows that some subjects have an accuracy of more than 90%, while others are at chance level -and two subjects even perform below chance level. While this increase in variance could be due to our dataset being too small for the large number of parameters in the CNN, we observed that the poorly performing subjects do better on stories 3 and 4, which were originally excluded as a test set in the cross-validation. Why our system performs poorly on some stories, and why this effect differs from subject to subject, is not clear, but nevertheless it does impact the per-subject results. This story-effect is not present in the linear model, probably because that model has far fewer parameters and is unable to pick up certain intricacies of stories or speakers.\n\nAs expected, we found a significant effect of decision window length on accuracy. This effect is, however, clearly different for the two models: the performance of the CNN is much less dependent on window length than is the case for the linear model. For the CNN, going from 10 s to 1 s, the median accuracy decreases by only 4.3% (from 85.1% to 80.8%), while with the linear model it decreases by 17.6% (from 75.7% to 58.1%). Moreover, even at 0.25 s the CNN still achieves a median accuracy of 74.0%, compared to only 53.4% for the linear model. We hypothesize that this difference is because the CNN does not know the stimulus and is only required to decode the locus of attention. As opposed to traditional AAD techniques, it does not have to relate the neural activity to the underlying speech envelopes. The latter requires computing correlation coefficients between\n\n\nAll subjects\n\nLeave-one-subject-out the stimulus and the neural responses, which are only sufficiently reliable and discriminative when computed over long windows. As usual with deep neural networks, it is hard to pinpoint exactly which information the system uses to achieve attention decoding. Potential information sources are spatial patterns of brain activity related to auditory attention, but also eye gaze or (ear) muscle activity which can be reflected in the EEG. While the subjects most likely focused on a screen in front of them and were instructed to sit still, and we conducted a number of control experiments such as removing the frontal EEG channels, none of these arguments or experiments was fully conclusive, so we can not exclude the possibility that information from other sources than the brain was used to decode attention.\n\nLastly, we evaluated our system using a leave-one-story+speaker-out approach, which is not commonly done in the literature. The usual approach is to leave out a single trial without consideration for speaker and/or story. This is probably fine for linear models, but we wanted to see whether the same would hold for a more complex model such as a CNN. Our results demonstrate that, when properly preprocessing the data, there is no significant difference in decoding accuracy between the leave-one-story+speaker-out approach and the classical approach. However, strong overfitting effects were observed when a per-trial (data-driven) preprocessing is performed, for example, for artifact removal. This implies that the data-driven procedure generates intertrial differences in the spatio-temporal data structure that can be exploited by the network. We conclude that one should be careful when applying data-driven preprocessing methods such as independent component analysis, principal component analysis, or MWF in combination with spatio-temporal decoders. In such cases, it is important not to run the preprocessing on a per-trial basis, but run it only once on the entire recording to avoid adding per-trial fingerprints that can be discovered by the network.\n\n\nFuture improvements\n\nWe hypothesize that much of the variation within and across subjects and stories currently observed is due to the small size of the dataset. The network probably needs more examples to learn to generalize better. However, a sufficiently large dataset, one which also allows for the strict cross-validation used in this work, is currently not available.\n\nPartly as a result of the limited amount of data available, the CNN proposed in this work is relatively simple. With more data, more complex CNN architectures would become feasible. Such complex CNN architectures may benefit more from generalization features such as dropout and batch normalization, not discussed in this work.\n\nAlso, for a practical neuro-steered hearing aid, it may be beneficial to make soft decisions. Instead of the translation of the continuous softmax outputs into binary decisions, the system could output a probability of left or right being attended, and the corresponding noise suppression system could adapt accordingly. In this way the integrated system could benefit from temporal relations or the knowledge of the current state to predict future states. The CNN could for example be extended by a long short term memory (LSTM) network.\n\n\nApplications\n\nThe main bottleneck in the implementation of neuro-steered noise suppression in hearing aids thus far has been the detection speed (state-of-the-art algorithms only achieve reasonable accuracies when using long decision windows). This can be quantified through the MESD metric, which captures both the effect of detection speed and decoding accuracy. While our linear baseline model achieves a median MESD of 22.6 s, our CNN achieves a median MESD of only 0.819 s, which is a major step forward.\n\nMoreover, our CNN-based system has an MESD of 5 s or less for 11 out of 16 subjects (eight subjects even have an MESD below 1 s), which is what we assume the minimum for an auditory attention detection system to be feasible in practice. Note that while a latency of 5 s may at first sight still seem long for practical use, it should not be confused with the time it takes to actually start steering toward the attended speaker: the user will already hear the effect of switching attention sooner. Instead, the MESD corresponds to the total time it takes to switch an AAD-steered volume control system from one speaker to the other in a reliable fashion by introducing an optimized amount of 'inertia' in the volume control system to avoid spurious switches due to false positives (Geirnaert et al., 2020). (For reference, an MESD of 5 s corresponds to a decoding accuracy of 70% at 1 s.) On the other hand, one subject does have an MESD of 33.4 s, and two subjects have an infinitely high MESD due to below 50% performance. The intersubject variability thus remains a challenge, since the goal is to create an algorithm that is both robust and able to quickly decode attention within the assumed limits for all subjects.\n\nAnother difficulty in neuro-steered hearing aids is that the clean speech envelopes are not available. This has so far been addressed using sophisticated noise suppression systems (Van Eyndhoven et al., 2017;O'Sullivan et al., 2017;Aroudi et al., 2018). If the speakers are spatially separated, our CNN might elegantly solve this problem by steering a beamformer toward the direction of attention, without requiring access to the envelopes of the speakers at all. Note that in a practical system, the system would need to be extended to more than two possible directions of attention, depending on the desired spatial resolution.\n\nFor application in hearing aids, a number of other issues need to be investigated, such as the effect of hearing loss (Holmes et al., 2017), acoustic circumstances (e.g., background noise, speaker locations and reverberation [Das et al., 2018;Das et al., 2016Fuglsang et al., 2017Aroudi et al., 2019]), mechanisms for switching attention (Akram et al., 2016), etc. The computational complexity would also need to be reduced. Especially if deeper, more complex networks are designed, CNN pruning will be necessary (Anwar et al., 2017). Then, a hardware DNN implementation or even computation on an external device such as a smartphone could be considered. Another practical obstacle is the numerous electrodes used for the EEG measurements. Similar to the work of Mirkovic et al., 2015;Mundanad and Bertrand, 2018;Fiedler et al., 2016;Montoya-Mart\u00ednez et al., 2019, it should be investigated how many and which electrodes are minimally needed for adequate performance.\n\nIn addition to potential use in future hearing devices, fast and accurate detection of the locus of attention can also be an important tool in future fundamental research. Thus far, it was not possible to measure compliance of the subjects with the instruction to direct their attention to one ear. Not only may the proposed CNN approach enable this, but it will also allow to track the locus of attention in almost real-time, which can be useful to study attention in dynamic situations, and its interplay with other elements such as eye gaze, speech intelligibility and cognition.\n\nIn conclusion, we proposed a novel EEG-based CNN for decoding the locus of auditory attention (based only on the EEG), and showed that it significantly outperforms a commonly used linear model for decoding the attended speaker. Moreover, we showed that the way the model is trained, and the way the data is preprocessed, impacts the results significantly. Although there are still some practical problems, the proposed model approaches the desired real-time detection performance. Furthermore, as it does not require the clean speech envelopes, this model has potential applications in realistic noise suppression systems for hearing aids.\n\nFigure 1 .\n1CNN architecture (windows of T samples). Input: T time samples of a 64-channel EEG signal, at a sampling rate of 128 Hz. Output: two scalars that determine the attended direction (left/right). The convolution, shown in blue, considers 130 ms of data over all channels. EEG = electroencephalography, CNN = convolutional neural network, ReLu = rectifying linear unit, FC = fully connected.\n\nFigure 2 .\n2Auditory attention detection performance of the CNN for two different window lengths. Linear decoding model shown as baseline. Blue dots: per-subject results, averaged over two test stories. Gray lines: same subjects. Red triangles: median accuracies. CNN = convolutional neural network.\n\nFigure 3 .Figure 4 .\n34Minimal expected switch durations (MESDs) for the CNN and the linear baseline. Dots: per-subject results, averaged over two test stories. Gray lines: same subjects. Vertical black bars: median MESD. As before, two poorly performing subjects were excluded from the analysis. CNN = convolutional neural network. Auditory attention detection performance as a function of the decision window length. Blue dots: persubject results, averaged over two test stories. Gray lines: same subjects. Red triangles: median accuracies. CNN = convolutional neural network.\n\nFigure 5 .\n5Auditory attention detection performance of the CNN when one particular frequency band is removed (left) and when only one band is used (right). The original results are also shown for reference. Each box plot contains results for all window lengths and for the two test stories.\n\nFigure 6 .\n6Grand-average topographic map of the normalized power of convolutional filters.\n\nFigure 8 .\n8Impact of leaving out the test subject on the accuracy of the CNN model (decision windows of 1 s). Blue dots: per-subject results, averaged over two test stories. Gray lines: same subjects. Red triangles: median accuracies. CNN = convolutional neural network.\n\n\nVandecappelle et al. eLife 2021;10:e56481. DOI: https://doi.org/10.7554/eLife.56481Story \nSpeaker \nSubject 1 \nSubject 2 \n. . . \nSubject 16 \n\n1 \n1 \ntest \ntest \n. . . \ntest \n\n2 \n2 \ntrain/val \n\n3 \n3 \ntrain/val \n\n4 \n3 \ntrain/val \n\nStory \nSpeaker \nSubject 1 \nSubject 2 \n. . . \nSubject 16 \n\n1 \n1 \ntrain/val \n\n2 \n2 \ntest \ntest \n. . . \ntest \n\n3 \n3 \ntrain/val \n\n4 \n3 \ntrain/val \n\n\nAcknowledgementsThe funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.Author contributions\nDeep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals. U R Acharya, S L Oh, Y Hagiwara, J H Tan, H Adeli, 10.1016/j.compbiomed.2017.09.01728974302Computers in Biology and Medicine. 100Acharya UR, Oh SL, Hagiwara Y, Tan JH, Adeli H. 2018a. Deep convolutional neural network for the automated detection and diagnosis of seizure using EEG signals. Computers in Biology and Medicine 100:270-278. DOI: https://doi.org/10.1016/j.compbiomed.2017.09.017, PMID: 28974302\n\nAutomated EEG-based screening of depression using deep convolutional neural network. U R Acharya, S L Oh, Y Hagiwara, J H Tan, H Adeli, D P Subha, 10.1016/j.cmpb.2018.04.01229852953Computer Methods and Programs in Biomedicine. 161Acharya UR, Oh SL, Hagiwara Y, Tan JH, Adeli H, Subha DP. 2018b. Automated EEG-based screening of depression using deep convolutional neural network. Computer Methods and Programs in Biomedicine 161: 103-113. DOI: https://doi.org/10.1016/j.cmpb.2018.04.012, PMID: 29852953\n\nRobust decoding of selective auditory attention from MEG in a competing-speaker environment via state-space modeling. S Akram, A Presacco, J Z Simon, S A Shamma, B Babadi, 10.1016/j.neuroimage.2015.09.04826436490NeuroImage. 124Akram S, Presacco A, Simon JZ, Shamma SA, Babadi B. 2016. Robust decoding of selective auditory attention from MEG in a competing-speaker environment via state-space modeling. NeuroImage 124:906-917. DOI: https://doi.org/10.1016/j.neuroimage.2015.09.048, PMID: 26436490\n\nA system identification approach to determining listening attention from EEG signals. E Alickovic, T Lunner, F Gustafsson, 24th European Signal Processing Conference (EUSIPCO). Alickovic E, Lunner T, Gustafsson F. 2016. A system identification approach to determining listening attention from EEG signals. 24th European Signal Processing Conference (EUSIPCO) 31-35.\n\nNeonatal seizure detection using deep convolutional neural networks. A H Ansari, P J Cherian, A Caicedo, G Naulaers, De Vos, M , Van Huffel, S , 10.1142/S012906571850011929747532International Journal of Neural Systems. 291850011Ansari AH, Cherian PJ, Caicedo A, Naulaers G, De Vos M, Van Huffel S. 2018a. Neonatal seizure detection using deep convolutional neural networks. International Journal of Neural Systems 29:1850011. DOI: https://doi.org/ 10.1142/S0129065718500119, PMID: 29747532\n\nQuiet sleep detection in preterm infants using deep convolutional neural networks. A H Ansari, De Wel, O Lavanga, M Caicedo, A Dereymaeker, A Jansen, K Vervisch, J , De Vos, M Naulaers, G , Van Huffel, S , 10.1088/1741-2552/aadc1f30132438Journal of Neural Engineering. 1566006Ansari AH, De Wel O, Lavanga M, Caicedo A, Dereymaeker A, Jansen K, Vervisch J, De Vos M, Naulaers G, Van Huffel S. 2018b. Quiet sleep detection in preterm infants using deep convolutional neural networks. Journal of Neural Engineering 15:066006. DOI: https://doi.org/10.1088/1741-2552/aadc1f, PMID: 30132438\n\nStructured pruning of deep convolutional neural networks. S Anwar, K Hwang, Sung W , 10.1145/3005348ACM Journal on Emerging Technologies in Computing Systems. 13Anwar S, Hwang K, Sung W. 2017. Structured pruning of deep convolutional neural networks. ACM Journal on Emerging Technologies in Computing Systems 13:1-18. DOI: https://doi.org/10.1145/3005348\n\nA Aroudi, D Marquardt, S Daclo, EEG-based auditory attention decoding using steerable binaural superdirective beamformer. International Conference on Acoustics, Speech and Signal Processing. Aroudi A, Marquardt D, Daclo S. 2018. EEG-based auditory attention decoding using steerable binaural superdirective beamformer. International Conference on Acoustics, Speech and Signal Processing (ICASSP) 851-855.\n\nImpact of different acoustic components on EEG-Based auditory attention decoding in noisy and reverberant conditions. A Aroudi, B Mirkovic, De Vos, M Doclo, S , 10.1109/TNSRE.2019.290340430843845IEEE Transactions on Neural Systems and Rehabilitation Engineering. 27Aroudi A, Mirkovic B, De Vos M, Doclo S. 2019. Impact of different acoustic components on EEG-Based auditory attention decoding in noisy and reverberant conditions. IEEE Transactions on Neural Systems and Rehabilitation Engineering 27:652-663. DOI: https://doi.org/10.1109/TNSRE.2019.2903404, PMID: 30843845\n\nNeural tracking of auditory motion is reflected by Delta phase and alpha power of EEG. A Bednar, E C Lalor, 10.1016/j.neuroimage.2018.07.05430053517NeuroImage. 181Bednar A, Lalor EC. 2018. Neural tracking of auditory motion is reflected by Delta phase and alpha power of EEG. NeuroImage 181:683-691. DOI: https://doi.org/10.1016/j.neuroimage.2018.07.054, PMID: 30053517\n\nWhere is the cocktail party? decoding locations of attended and unattended moving sound sources using EEG. A Bednar, E C Lalor, 10.1016/j.neuroimage.2019.11628331629828NeuroImage. 205116283Bednar A, Lalor EC. 2020. Where is the cocktail party? decoding locations of attended and unattended moving sound sources using EEG. NeuroImage 205:116283. DOI: https://doi.org/10.1016/j.neuroimage.2019.116283, PMID: 31629828\n\nAuditory-Inspired speech envelope extraction methods for improved EEG-Based auditory attention detection in a cocktail party scenario. W Biesmans, N Das, T Francart, A Bertrand, 10.1109/TNSRE.2016.257190027244743IEEE Transactions on Neural Systems and Rehabilitation Engineering. 25Biesmans W, Das N, Francart T, Bertrand A. 2017. Auditory-Inspired speech envelope extraction methods for improved EEG-Based auditory attention detection in a cocktail party scenario. IEEE Transactions on Neural Systems and Rehabilitation Engineering 25:402-412. DOI: https://doi.org/10.1109/TNSRE.2016.2571900, PMID: 27244743\n\nSome experiments on the recognition of speech, with one and with two ears. E C Cherry, 10.1121/1.1907229The Journal of the Acoustical Society of America. 25Cherry EC. 1953. Some experiments on the recognition of speech, with one and with two ears. The Journal of the Acoustical Society of America 25:975-979. DOI: https://doi.org/10.1121/1.1907229\n\nComparison of Two-Talker attention decoding from EEG with nonlinear neural networks and linear methods. G Ciccarelli, M Nolan, J Perricone, P T Calamia, S Haro, J O&apos;sullivan, N Mesgarani, T F Quatieri, C J Smalt, 10.1038/s41598-019-47795-031395905Scientific Reports. 911538Ciccarelli G, Nolan M, Perricone J, Calamia PT, Haro S, O'Sullivan J, Mesgarani N, Quatieri TF, Smalt CJ. 2019. Comparison of Two-Talker attention decoding from EEG with nonlinear neural networks and linear methods. Scientific Reports 9:11538. DOI: https://doi.org/10.1038/s41598-019-47795-0, PMID: 31395905\n\nThe effect of head-related filtering and ear-specific decoding Bias on auditory attention detection. N Das, W Biesmans, A Bertrand, T Francart, 10.1088/1741-2560/13/5/05601427618842Journal of Neural Engineering. 1356014Das N, Biesmans W, Bertrand A, Francart T. 2016. The effect of head-related filtering and ear-specific decoding Bias on auditory attention detection. Journal of Neural Engineering 13:056014. DOI: https://doi.org/10.1088/ 1741-2560/13/5/056014, PMID: 27618842\n\nEEG-based auditory attention detection: boundary conditions for background noise and speaker positions. N Das, A Bertrand, T Francart, 10.1088/1741-2552/aae0a630207293Journal of Neural Engineering. 1566017Das N, Bertrand A, Francart T. 2018. EEG-based auditory attention detection: boundary conditions for background noise and speaker positions. Journal of Neural Engineering 15:066017. DOI: https://doi.org/10. 1088/1741-2552/aae0a6, PMID: 30207293\n\nDecoding the auditory brain with canonical component analysis. A De Cheveign\u00e9, Dde Wong, Di Liberto, G M Hjortkjaer, J Slaney, M Lalor, E , 10.1016/j.neuroimage.2018.01.03329378317NeuroImage. 172de Cheveign\u00e9 A, Wong DDE, Di Liberto GM, Hjortkjaer J, Slaney M, Lalor E. 2018. Decoding the auditory brain with canonical component analysis. NeuroImage 172:206-216. DOI: https://doi.org/10.1016/j.neuroimage. 2018.01.033, PMID: 29378317\n\nEmergence of neural encoding of auditory objects while listening to competing speakers. Deburen, 10.1073/pnas.120538110922753470Radioboeken Voor Kinderen. 109Ding N, Simon JZDeBuren. 2007. Radioboeken Voor Kinderen. http://www.radioboeken.eu/kinderradioboeken.php?lang=NL Ding N, Simon JZ. 2012. Emergence of neural encoding of auditory objects while listening to competing speakers. PNAS 109:11854-11859. DOI: https://doi.org/10.1073/pnas.1205381109, PMID: 22753470\n\nIs there Chaos in the brain? I. concepts of nonlinear dynamics and methods of investigation. P Faure, H Korn, 10.1016/S0764-4469(01)01377-4Comptes Rendus De l'Acad\u00e9mie Des Sciences -Series III -Sciences De La Vie. 324Faure P, Korn H. 2001. Is there Chaos in the brain? I. concepts of nonlinear dynamics and methods of investigation. Comptes Rendus De l'Acad\u00e9mie Des Sciences -Series III -Sciences De La Vie 324:773-793. DOI: https://doi.org/10.1016/S0764-4469(01)01377-4\n\nEar-EEG allows extraction of neural responses in challenging listening scenarios-a future technology for hearing aids?. L Fiedler, J Obleser, T Lunner, C Graversen, 10.1109/EMBC.2016.759202038th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). Fiedler L, Obleser J, Lunner T, Graversen C. 2016. Ear-EEG allows extraction of neural responses in challenging listening scenarios-a future technology for hearing aids? 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 5697-5700. DOI: https://doi.org/10.1109/EMBC.2016. 7592020\n\nAPEX 3: a multi-purpose test platform for auditory psychophysical experiments. T Francart, A Van Wieringen, J Wouters, 10.1016/j.jneumeth.2008.04.02018538414Journal of Neuroscience Methods. 172Francart T, van Wieringen A, Wouters J. 2008. APEX 3: a multi-purpose test platform for auditory psychophysical experiments. Journal of Neuroscience Methods 172:283-293. DOI: https://doi.org/10.1016/j.jneumeth.2008. 04.020, PMID: 18538414\n\nSelective modulation of auditory cortical alpha activity in an audiovisual spatial attention task. J N Frey, N Mainy, J P Lachaux, N M\u00fc Ller, O Bertrand, N Weisz, 10.1523/JNEUROSCI.4813-13.201424806688Journal of Neuroscience. 34Frey JN, Mainy N, Lachaux JP, M\u00fc ller N, Bertrand O, Weisz N. 2014. Selective modulation of auditory cortical alpha activity in an audiovisual spatial attention task. Journal of Neuroscience 34:6634-6639. DOI: https://doi. org/10.1523/JNEUROSCI.4813-13.2014, PMID: 24806688\n\nNoise-robust cortical tracking of attended speech in real-world acoustic scenes. S A Fuglsang, T Dau, J Hjortkjaer, 10.1016/j.neuroimage.2017.04.02628412441NeuroImage. 156Fuglsang SA, Dau T, Hjortkjaer J. 2017. Noise-robust cortical tracking of attended speech in real-world acoustic scenes. NeuroImage 156:435-444. DOI: https://doi.org/10.1016/j.neuroimage.2017.04.026, PMID: 28412441\n\nSelective attention enhances Beta-Band cortical oscillation to speech under \"Cocktail-Party\" Listening Conditions. Y Gao, Q Wang, Y Ding, C Wang, H Li, X Wu, T Qu, L Li, 10.3389/fnhum.2017.0003428239344Frontiers in Human Neuroscience. 1134Gao Y, Wang Q, Ding Y, Wang C, Li H, Wu X, Qu T, Li L. 2017. Selective attention enhances Beta-Band cortical oscillation to speech under \"Cocktail-Party\" Listening Conditions. Frontiers in Human Neuroscience 11:34. DOI: https://doi.org/10.3389/fnhum.2017.00034, PMID: 28239344\n\nAn interpretable performance metric for auditory attention decoding algorithms in a context of Neuro-Steered gain control. S Geirnaert, T Francart, A Bertrand, 10.1109/TNSRE.2019.295272431715568IEEE Transactions on Neural Systems and Rehabilitation Engineering. 28Geirnaert S, Francart T, Bertrand A. 2020. An interpretable performance metric for auditory attention decoding algorithms in a context of Neuro-Steered gain control. IEEE Transactions on Neural Systems and Rehabilitation Engineering 28:307-317. DOI: https://doi.org/10.1109/TNSRE.2019.2952724, PMID: 31715568\n\nPeripheral hearing loss reduces the ability of children to direct selective attention during multi-talker listening. E Holmes, P T Kitterick, A Q Summerfield, 10.1016/j.heares.2017.05.00528505526Hearing Research. 350Holmes E, Kitterick PT, Summerfield AQ. 2017. Peripheral hearing loss reduces the ability of children to direct selective attention during multi-talker listening. Hearing Research 350:160-172. DOI: https://doi.org/10.1016/j. heares.2017.05.005, PMID: 28505526\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, 10.1038/nature1453926017442Nature. 521LeCun Y, Bengio Y, Hinton G. 2015. Deep learning. Nature 521:436-444. DOI: https://doi.org/10.1038/ nature14539, PMID: 26017442\n\nLearning a convolutional neural network for sleep stage classification. N Liu, Z Lu, B Xu, Q Liao, BioMedical Engineering and Informatics. CISP-BMEIImage and Signal Processing. 10th International CongressLiu N, Lu Z, Xu B, Liao Q. 2017. Learning a convolutional neural network for sleep stage classification. Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI), 2017 10th International Congress.\n\nIdentification of auditory Object-Specific attention from Single-Trial electroencephalogram signals via entropy measures and machine learning. Y Lu, M Wang, Q Zhang, Y Han, 10.3390/e20050386Entropy. 20386Lu Y, Wang M, Zhang Q, Han Y. 2018. Identification of auditory Object-Specific attention from Single-Trial electroencephalogram signals via entropy measures and machine learning. Entropy 20:386. DOI: https://doi. org/10.3390/e20050386\n\nReal-Time tracking of selective auditory attention from M/EEG: a bayesian filtering approach. S Miran, S Akram, A Sheikhattar, J Z Simon, T Zhang, B Babadi, 10.3389/fnins.2018.0026229765298Frontiers in Neuroscience. 12262Miran S, Akram S, Sheikhattar A, Simon JZ, Zhang T, Babadi B. 2018. Real-Time tracking of selective auditory attention from M/EEG: a bayesian filtering approach. Frontiers in Neuroscience 12:262. DOI: https://doi.org/ 10.3389/fnins.2018.00262, PMID: 29765298\n\nDecoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications. B Mirkovic, S Debener, M Jaeger, De Vos, M , 10.1088/1741-2560/12/4/04600726035345Journal of Neural Engineering. 1246007Mirkovic B, Debener S, Jaeger M, De Vos M. 2015. Decoding the attended speech stream with multi-channel EEG: implications for online, daily-life applications. Journal of Neural Engineering 12:046007. DOI: https://doi. org/10.1088/1741-2560/12/4/046007, PMID: 26035345\n\nOptimal number and placement of eeg electrodes for measurement of neural tracking of speech. J Montoya-Mart\u00ednez, A Bertrand, T Francart, 10.1101/800979Montoya-Mart\u00ednez J, Bertrand A, Francart T. 2019. Optimal number and placement of eeg electrodes for measurement of neural tracking of speech. bioRxiv. DOI: https://doi.org/10.1101/800979\n\nThe effect of miniaturization and galvanic separation of EEG sensor devices in an auditory attention detection task. A N Mundanad, A Bertrand, 10.1109/EMBC.2018.851221240th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). Mundanad AN, Bertrand A. 2018. The effect of miniaturization and galvanic separation of EEG sensor devices in an auditory attention detection task. 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) 77-80. DOI: https://doi.org/10.1109/EMBC.2018.8512212\n\nAttentional selection in a cocktail party environment can be decoded from Single-Trial EEG. J A O&apos;sullivan, A J Power, N Mesgarani, S Rajaram, J J Foxe, B G Shinn-Cunningham, M Slaney, S A Shamma, E C Lalor, 10.1093/cercor/bht35524429136Cerebral Cortex. 25O'Sullivan JA, Power AJ, Mesgarani N, Rajaram S, Foxe JJ, Shinn-Cunningham BG, Slaney M, Shamma SA, Lalor EC. 2015. Attentional selection in a cocktail party environment can be decoded from Single-Trial EEG. Cerebral Cortex 25:1697-1706. DOI: https://doi.org/10.1093/cercor/bht355, PMID: 24429136\n\nNeural decoding of attentional selection in multi-speaker environments without access to clean sources. J O&apos;sullivan, Z Chen, J Herrero, G M Mckhann, S A Sheth, A D Mehta, N Mesgarani, 10.1088/1741-2552/aa7ab428776506Journal of Neural Engineering. 1456001O'Sullivan J, Chen Z, Herrero J, McKhann GM, Sheth SA, Mehta AD, Mesgarani N. 2017. Neural decoding of attentional selection in multi-speaker environments without access to clean sources. Journal of Neural Engineering 14:056001. DOI: https://doi.org/10.1088/1741-2552/aa7ab4, PMID: 28776506\n\nLook at me when I'm talking to you: selective attention at a multisensory cocktail party can be decoded using stimulus reconstruction and alpha power modulations. A E O&apos;sullivan, C Y Lim, E C Lalor, 10.1111/ejn.14425European Journal of Neuroscience. 50O'Sullivan AE, Lim CY, Lalor EC. 2019. Look at me when I'm talking to you: selective attention at a multisensory cocktail party can be decoded using stimulus reconstruction and alpha power modulations. European Journal of Neuroscience 50:3282-3295. DOI: https://doi.org/10.1111/ejn.14425\n\nReconstructing speech from human auditory cortex. B N Pasley, S V David, N Mesgarani, A Flinker, S A Shamma, N E Crone, R T Knight, E F Chang, 10.1371/journal.pbio.100125122303281PLOS Biology. 101001251Pasley BN, David SV, Mesgarani N, Flinker A, Shamma SA, Crone NE, Knight RT, Chang EF. 2012. Reconstructing speech from human auditory cortex. PLOS Biology 10:e1001251. DOI: https://doi.org/10.1371/ journal.pbio.1001251, PMID: 22303281\n\nJoint representation of spatial and phonetic features in the human core auditory cortex. P Patel, L K Long, J L Herrero, A D Mehta, N Mesgarani, 10.1016/j.celrep.2018.07.07630134167Cell Reports. 24Patel P, Long LK, Herrero JL, Mehta AD, Mesgarani N. 2018. Joint representation of spatial and phonetic features in the human core auditory cortex. Cell Reports 24:2051-2062. DOI: https://doi.org/10.1016/j.celrep. 2018.07.076, PMID: 30134167\n\nA generic EEG artifact removal algorithm based on the multi-channel Wiener filter. B Somers, T Francart, A Bertrand, 10.1088/1741-2552/aaac9229393057Journal of Neural Engineering. 1536007Somers B, Francart T, Bertrand A. 2018. A generic EEG artifact removal algorithm based on the multi-channel Wiener filter. Journal of Neural Engineering 15:036007. DOI: https://doi.org/10.1088/1741-2552/aaac92, PMID: 29393057\n\nMachine learning for decoding listeners' attention from electroencephalography evoked by continuous speech. T Taillez, B Kollmeier, B T Meyer, 10.1111/ejn.13790European Journal of Neuroscience. 51Taillez T, Kollmeier B, Meyer BT. 2017. Machine learning for decoding listeners' attention from electroencephalography evoked by continuous speech. European Journal of Neuroscience 51:1234-1241. DOI: https://doi.org/10.1111/ejn.13790\n\nEEG-Informed attended speaker extraction from recorded speech mixtures with application in Neuro-Steered hearing prostheses. S Van Eyndhoven, T Francart, A Bertrand, 10.1109/TBME.2016.2587382IEEE Transactions on Biomedical Engineering. 64Van Eyndhoven S, Francart T, Bertrand A. 2017. EEG-Informed attended speaker extraction from recorded speech mixtures with application in Neuro-Steered hearing prostheses. IEEE Transactions on Biomedical Engineering 64:1045-1056. DOI: https://doi.org/10.1109/TBME.2016.2587382\n\nEEG-based detection of the locus of auditory attention with convolutional neural networks. Software Heritage. swh:1:rev:8c485f2e1d3a79b55b71b3195cdf0235af488d95. S Vandecappelle, Vandecappelle S. 2021. EEG-based detection of the locus of auditory attention with convolutional neural networks. Software Heritage. swh:1:rev:8c485f2e1d3a79b55b71b3195cdf0235af488d95. https://archive. softwareheritage.org/swh:1:dir:8901ca73c9ef6f86de11719af6d410a02e7eb291\n\nMatconvnet: convolutional neural networks for matlab. A Vedaldi, K Lenc, Proceedings of the 23rd ACM International Conference on Multimedia ACM 689-692. the 23rd ACM International Conference on Multimedia ACM 689-692Vedaldi A, Lenc K. 2015. Matconvnet: convolutional neural networks for matlab. Proceedings of the 23rd ACM International Conference on Multimedia ACM 689-692.\n\nDecoding the direction of auditory motion in blind humans. T Wolbers, P Zahorik, N A Giudice, 10.1016/j.neuroimage.2010.04.26620451630NeuroImage. 56Wolbers T, Zahorik P, Giudice NA. 2011. Decoding the direction of auditory motion in blind humans. NeuroImage 56:681-687. DOI: https://doi.org/10.1016/j.neuroimage.2010.04.266, PMID: 20451630\n\nSpatiotemporal dynamics of auditory attention synchronize with speech. M W\u00f6 Stmann, B Herrmann, B Maess, J Obleser, 10.1073/pnas.152335711327001PNAS. 113W\u00f6 stmann M, Herrmann B, Maess B, Obleser J. 2016. Spatiotemporal dynamics of auditory attention synchronize with speech. PNAS 113:3873-3878. DOI: https://doi.org/10.1073/pnas.1523357113, PMID: 27001 861\n", "annotations": {"author": "[{\"end\":384,\"start\":117},{\"end\":610,\"start\":385},{\"end\":833,\"start\":611},{\"end\":987,\"start\":834},{\"end\":1140,\"start\":988},{\"end\":1259,\"start\":1141}]", "publisher": null, "author_last_name": "[{\"end\":138,\"start\":125},{\"end\":398,\"start\":391},{\"end\":621,\"start\":618},{\"end\":853,\"start\":839},{\"end\":1006,\"start\":998},{\"end\":1153,\"start\":1145}]", "author_first_name": "[{\"end\":124,\"start\":117},{\"end\":390,\"start\":385},{\"end\":617,\"start\":611},{\"end\":838,\"start\":834},{\"end\":997,\"start\":988},{\"end\":1144,\"start\":1141}]", "author_affiliation": "[{\"end\":250,\"start\":174},{\"end\":383,\"start\":252},{\"end\":476,\"start\":400},{\"end\":609,\"start\":478},{\"end\":699,\"start\":623},{\"end\":832,\"start\":701},{\"end\":986,\"start\":855},{\"end\":1139,\"start\":1008},{\"end\":1258,\"start\":1182}]", "title": "[{\"end\":90,\"start\":1},{\"end\":1349,\"start\":1260}]", "venue": null, "abstract": "[{\"end\":6631,\"start\":1649}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6847,\"start\":6833},{\"end\":7999,\"start\":7994},{\"end\":8031,\"start\":8026},{\"end\":8259,\"start\":8237},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8283,\"start\":8259},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8546,\"start\":8521},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8566,\"start\":8546},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8925,\"start\":8907},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9538,\"start\":9515},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9655,\"start\":9640},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10915,\"start\":10897},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11287,\"start\":11271},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12017,\"start\":11996},{\"end\":12038,\"start\":12017},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12133,\"start\":12113},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12504,\"start\":12485},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12865,\"start\":12849},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15833,\"start\":15809},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20581,\"start\":20558},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21305,\"start\":21284},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25714,\"start\":25691},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26384,\"start\":26360},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28209,\"start\":28193},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29278,\"start\":29255},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29422,\"start\":29406},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29567,\"start\":29550},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":33494,\"start\":33477},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":40994,\"start\":40970},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":41620,\"start\":41592},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":41644,\"start\":41620},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41664,\"start\":41644},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":42182,\"start\":42161},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":42286,\"start\":42268},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":42302,\"start\":42286},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":42323,\"start\":42302},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":42343,\"start\":42323},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":42401,\"start\":42381},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":42576,\"start\":42556},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":42828,\"start\":42806},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":42856,\"start\":42828},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":42877,\"start\":42856},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":42906,\"start\":42877}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44636,\"start\":44236},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44937,\"start\":44637},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45517,\"start\":44938},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45810,\"start\":45518},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45903,\"start\":45811},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46176,\"start\":45904},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":46550,\"start\":46177}]", "paragraph": "[{\"end\":7288,\"start\":6647},{\"end\":7870,\"start\":7290},{\"end\":8807,\"start\":7872},{\"end\":9175,\"start\":8852},{\"end\":9539,\"start\":9177},{\"end\":9878,\"start\":9541},{\"end\":10503,\"start\":9880},{\"end\":11094,\"start\":10505},{\"end\":11714,\"start\":11096},{\"end\":13597,\"start\":11737},{\"end\":14177,\"start\":13631},{\"end\":15010,\"start\":14179},{\"end\":15701,\"start\":15012},{\"end\":16018,\"start\":15703},{\"end\":17646,\"start\":16050},{\"end\":18390,\"start\":17648},{\"end\":19907,\"start\":18392},{\"end\":20234,\"start\":19909},{\"end\":20467,\"start\":20236},{\"end\":21663,\"start\":20519},{\"end\":21981,\"start\":21665},{\"end\":22653,\"start\":21983},{\"end\":23444,\"start\":22690},{\"end\":23827,\"start\":23446},{\"end\":24346,\"start\":23862},{\"end\":24610,\"start\":24348},{\"end\":25182,\"start\":24612},{\"end\":25645,\"start\":25184},{\"end\":26385,\"start\":25647},{\"end\":26961,\"start\":26387},{\"end\":27620,\"start\":26998},{\"end\":28421,\"start\":27650},{\"end\":29002,\"start\":28423},{\"end\":30277,\"start\":29004},{\"end\":30718,\"start\":30312},{\"end\":31342,\"start\":30720},{\"end\":31929,\"start\":31344},{\"end\":32256,\"start\":31931},{\"end\":32914,\"start\":32289},{\"end\":33187,\"start\":32929},{\"end\":33596,\"start\":33209},{\"end\":34278,\"start\":33598},{\"end\":35440,\"start\":34280},{\"end\":36314,\"start\":35442},{\"end\":37164,\"start\":36331},{\"end\":38430,\"start\":37166},{\"end\":38806,\"start\":38454},{\"end\":39135,\"start\":38808},{\"end\":39675,\"start\":39137},{\"end\":40187,\"start\":39692},{\"end\":41410,\"start\":40189},{\"end\":42041,\"start\":41412},{\"end\":43010,\"start\":42043},{\"end\":43594,\"start\":43012},{\"end\":44235,\"start\":43596}]", "formula": null, "table_ref": "[{\"end\":10870,\"start\":10863},{\"end\":12513,\"start\":12506},{\"end\":16675,\"start\":16668},{\"end\":17561,\"start\":17554},{\"end\":19071,\"start\":19064},{\"end\":29546,\"start\":29539}]", "section_header": "[{\"end\":6645,\"start\":6633},{\"end\":8831,\"start\":8810},{\"end\":8850,\"start\":8834},{\"end\":11735,\"start\":11717},{\"end\":13629,\"start\":13600},{\"end\":16048,\"start\":16021},{\"end\":20517,\"start\":20470},{\"end\":22688,\"start\":22656},{\"end\":23837,\"start\":23830},{\"end\":23860,\"start\":23840},{\"end\":26996,\"start\":26964},{\"end\":27648,\"start\":27623},{\"end\":30310,\"start\":30280},{\"end\":32287,\"start\":32259},{\"end\":32927,\"start\":32917},{\"end\":33207,\"start\":33190},{\"end\":36329,\"start\":36317},{\"end\":38452,\"start\":38433},{\"end\":39690,\"start\":39678},{\"end\":44247,\"start\":44237},{\"end\":44648,\"start\":44638},{\"end\":44959,\"start\":44939},{\"end\":45529,\"start\":45519},{\"end\":45822,\"start\":45812},{\"end\":45915,\"start\":45905}]", "table": "[{\"end\":46550,\"start\":46262}]", "figure_caption": "[{\"end\":44636,\"start\":44249},{\"end\":44937,\"start\":44650},{\"end\":45517,\"start\":44962},{\"end\":45810,\"start\":45531},{\"end\":45903,\"start\":45824},{\"end\":46176,\"start\":45917},{\"end\":46262,\"start\":46179}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14261,\"start\":14253},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":24800,\"start\":24792},{\"end\":25951,\"start\":25943},{\"end\":27143,\"start\":27135},{\"end\":27546,\"start\":27538},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27915,\"start\":27907},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28010,\"start\":27994},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28116,\"start\":28108},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28285,\"start\":28268},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29037,\"start\":29029},{\"end\":29534,\"start\":29526},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29702,\"start\":29693},{\"end\":31227,\"start\":31219},{\"end\":31513,\"start\":31496},{\"end\":31594,\"start\":31578},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32579,\"start\":32571},{\"end\":33721,\"start\":33713},{\"end\":34663,\"start\":34655}]", "bib_author_first_name": "[{\"end\":46823,\"start\":46822},{\"end\":46825,\"start\":46824},{\"end\":46836,\"start\":46835},{\"end\":46838,\"start\":46837},{\"end\":46844,\"start\":46843},{\"end\":46856,\"start\":46855},{\"end\":46858,\"start\":46857},{\"end\":46865,\"start\":46864},{\"end\":47316,\"start\":47315},{\"end\":47318,\"start\":47317},{\"end\":47329,\"start\":47328},{\"end\":47331,\"start\":47330},{\"end\":47337,\"start\":47336},{\"end\":47349,\"start\":47348},{\"end\":47351,\"start\":47350},{\"end\":47358,\"start\":47357},{\"end\":47367,\"start\":47366},{\"end\":47369,\"start\":47368},{\"end\":47853,\"start\":47852},{\"end\":47862,\"start\":47861},{\"end\":47874,\"start\":47873},{\"end\":47876,\"start\":47875},{\"end\":47885,\"start\":47884},{\"end\":47887,\"start\":47886},{\"end\":47897,\"start\":47896},{\"end\":48319,\"start\":48318},{\"end\":48332,\"start\":48331},{\"end\":48342,\"start\":48341},{\"end\":48669,\"start\":48668},{\"end\":48671,\"start\":48670},{\"end\":48681,\"start\":48680},{\"end\":48683,\"start\":48682},{\"end\":48694,\"start\":48693},{\"end\":48705,\"start\":48704},{\"end\":48718,\"start\":48716},{\"end\":48725,\"start\":48724},{\"end\":48731,\"start\":48728},{\"end\":48741,\"start\":48740},{\"end\":49174,\"start\":49173},{\"end\":49176,\"start\":49175},{\"end\":49187,\"start\":49185},{\"end\":49194,\"start\":49193},{\"end\":49205,\"start\":49204},{\"end\":49216,\"start\":49215},{\"end\":49231,\"start\":49230},{\"end\":49241,\"start\":49240},{\"end\":49253,\"start\":49252},{\"end\":49258,\"start\":49256},{\"end\":49265,\"start\":49264},{\"end\":49277,\"start\":49276},{\"end\":49283,\"start\":49280},{\"end\":49293,\"start\":49292},{\"end\":49735,\"start\":49734},{\"end\":49744,\"start\":49743},{\"end\":49756,\"start\":49752},{\"end\":49758,\"start\":49757},{\"end\":50033,\"start\":50032},{\"end\":50043,\"start\":50042},{\"end\":50056,\"start\":50055},{\"end\":50557,\"start\":50556},{\"end\":50567,\"start\":50566},{\"end\":50580,\"start\":50578},{\"end\":50587,\"start\":50586},{\"end\":50596,\"start\":50595},{\"end\":51100,\"start\":51099},{\"end\":51110,\"start\":51109},{\"end\":51112,\"start\":51111},{\"end\":51491,\"start\":51490},{\"end\":51501,\"start\":51500},{\"end\":51503,\"start\":51502},{\"end\":51935,\"start\":51934},{\"end\":51947,\"start\":51946},{\"end\":51954,\"start\":51953},{\"end\":51966,\"start\":51965},{\"end\":52485,\"start\":52484},{\"end\":52487,\"start\":52486},{\"end\":52863,\"start\":52862},{\"end\":52877,\"start\":52876},{\"end\":52886,\"start\":52885},{\"end\":52899,\"start\":52898},{\"end\":52901,\"start\":52900},{\"end\":52912,\"start\":52911},{\"end\":52920,\"start\":52919},{\"end\":52939,\"start\":52938},{\"end\":52952,\"start\":52951},{\"end\":52954,\"start\":52953},{\"end\":52966,\"start\":52965},{\"end\":52968,\"start\":52967},{\"end\":53447,\"start\":53446},{\"end\":53454,\"start\":53453},{\"end\":53466,\"start\":53465},{\"end\":53478,\"start\":53477},{\"end\":53929,\"start\":53928},{\"end\":53936,\"start\":53935},{\"end\":53948,\"start\":53947},{\"end\":54339,\"start\":54338},{\"end\":54357,\"start\":54354},{\"end\":54366,\"start\":54364},{\"end\":54377,\"start\":54376},{\"end\":54379,\"start\":54378},{\"end\":54393,\"start\":54392},{\"end\":54403,\"start\":54402},{\"end\":54412,\"start\":54411},{\"end\":55271,\"start\":55270},{\"end\":55280,\"start\":55279},{\"end\":55770,\"start\":55769},{\"end\":55781,\"start\":55780},{\"end\":55792,\"start\":55791},{\"end\":55802,\"start\":55801},{\"end\":56350,\"start\":56349},{\"end\":56362,\"start\":56361},{\"end\":56379,\"start\":56378},{\"end\":56803,\"start\":56802},{\"end\":56805,\"start\":56804},{\"end\":56813,\"start\":56812},{\"end\":56822,\"start\":56821},{\"end\":56824,\"start\":56823},{\"end\":56835,\"start\":56834},{\"end\":56846,\"start\":56845},{\"end\":56858,\"start\":56857},{\"end\":57288,\"start\":57287},{\"end\":57290,\"start\":57289},{\"end\":57302,\"start\":57301},{\"end\":57309,\"start\":57308},{\"end\":57709,\"start\":57708},{\"end\":57716,\"start\":57715},{\"end\":57724,\"start\":57723},{\"end\":57732,\"start\":57731},{\"end\":57740,\"start\":57739},{\"end\":57746,\"start\":57745},{\"end\":57752,\"start\":57751},{\"end\":57758,\"start\":57757},{\"end\":58234,\"start\":58233},{\"end\":58247,\"start\":58246},{\"end\":58259,\"start\":58258},{\"end\":58802,\"start\":58801},{\"end\":58812,\"start\":58811},{\"end\":58814,\"start\":58813},{\"end\":58827,\"start\":58826},{\"end\":58829,\"start\":58828},{\"end\":59177,\"start\":59176},{\"end\":59186,\"start\":59185},{\"end\":59196,\"start\":59195},{\"end\":59445,\"start\":59444},{\"end\":59452,\"start\":59451},{\"end\":59458,\"start\":59457},{\"end\":59464,\"start\":59463},{\"end\":59941,\"start\":59940},{\"end\":59947,\"start\":59946},{\"end\":59955,\"start\":59954},{\"end\":59964,\"start\":59963},{\"end\":60332,\"start\":60331},{\"end\":60341,\"start\":60340},{\"end\":60350,\"start\":60349},{\"end\":60365,\"start\":60364},{\"end\":60367,\"start\":60366},{\"end\":60376,\"start\":60375},{\"end\":60385,\"start\":60384},{\"end\":60829,\"start\":60828},{\"end\":60841,\"start\":60840},{\"end\":60852,\"start\":60851},{\"end\":60863,\"start\":60861},{\"end\":60870,\"start\":60869},{\"end\":61311,\"start\":61310},{\"end\":61331,\"start\":61330},{\"end\":61343,\"start\":61342},{\"end\":61675,\"start\":61674},{\"end\":61677,\"start\":61676},{\"end\":61689,\"start\":61688},{\"end\":62222,\"start\":62221},{\"end\":62224,\"start\":62223},{\"end\":62243,\"start\":62242},{\"end\":62245,\"start\":62244},{\"end\":62254,\"start\":62253},{\"end\":62267,\"start\":62266},{\"end\":62278,\"start\":62277},{\"end\":62280,\"start\":62279},{\"end\":62288,\"start\":62287},{\"end\":62290,\"start\":62289},{\"end\":62310,\"start\":62309},{\"end\":62320,\"start\":62319},{\"end\":62322,\"start\":62321},{\"end\":62332,\"start\":62331},{\"end\":62334,\"start\":62333},{\"end\":62793,\"start\":62792},{\"end\":62812,\"start\":62811},{\"end\":62820,\"start\":62819},{\"end\":62831,\"start\":62830},{\"end\":62833,\"start\":62832},{\"end\":62844,\"start\":62843},{\"end\":62846,\"start\":62845},{\"end\":62855,\"start\":62854},{\"end\":62857,\"start\":62856},{\"end\":62866,\"start\":62865},{\"end\":63404,\"start\":63403},{\"end\":63406,\"start\":63405},{\"end\":63425,\"start\":63424},{\"end\":63427,\"start\":63426},{\"end\":63434,\"start\":63433},{\"end\":63436,\"start\":63435},{\"end\":63837,\"start\":63836},{\"end\":63839,\"start\":63838},{\"end\":63849,\"start\":63848},{\"end\":63851,\"start\":63850},{\"end\":63860,\"start\":63859},{\"end\":63873,\"start\":63872},{\"end\":63884,\"start\":63883},{\"end\":63886,\"start\":63885},{\"end\":63896,\"start\":63895},{\"end\":63898,\"start\":63897},{\"end\":63907,\"start\":63906},{\"end\":63909,\"start\":63908},{\"end\":63919,\"start\":63918},{\"end\":63921,\"start\":63920},{\"end\":64315,\"start\":64314},{\"end\":64324,\"start\":64323},{\"end\":64326,\"start\":64325},{\"end\":64334,\"start\":64333},{\"end\":64336,\"start\":64335},{\"end\":64347,\"start\":64346},{\"end\":64349,\"start\":64348},{\"end\":64358,\"start\":64357},{\"end\":64749,\"start\":64748},{\"end\":64759,\"start\":64758},{\"end\":64771,\"start\":64770},{\"end\":65188,\"start\":65187},{\"end\":65199,\"start\":65198},{\"end\":65212,\"start\":65211},{\"end\":65214,\"start\":65213},{\"end\":65636,\"start\":65635},{\"end\":65653,\"start\":65652},{\"end\":65665,\"start\":65664},{\"end\":66189,\"start\":66188},{\"end\":66535,\"start\":66534},{\"end\":66546,\"start\":66545},{\"end\":66916,\"start\":66915},{\"end\":66927,\"start\":66926},{\"end\":66938,\"start\":66937},{\"end\":66940,\"start\":66939},{\"end\":67269,\"start\":67268},{\"end\":67282,\"start\":67281},{\"end\":67294,\"start\":67293},{\"end\":67303,\"start\":67302}]", "bib_author_last_name": "[{\"end\":46833,\"start\":46826},{\"end\":46841,\"start\":46839},{\"end\":46853,\"start\":46845},{\"end\":46862,\"start\":46859},{\"end\":46871,\"start\":46866},{\"end\":47326,\"start\":47319},{\"end\":47334,\"start\":47332},{\"end\":47346,\"start\":47338},{\"end\":47355,\"start\":47352},{\"end\":47364,\"start\":47359},{\"end\":47375,\"start\":47370},{\"end\":47859,\"start\":47854},{\"end\":47871,\"start\":47863},{\"end\":47882,\"start\":47877},{\"end\":47894,\"start\":47888},{\"end\":47904,\"start\":47898},{\"end\":48329,\"start\":48320},{\"end\":48339,\"start\":48333},{\"end\":48353,\"start\":48343},{\"end\":48678,\"start\":48672},{\"end\":48691,\"start\":48684},{\"end\":48702,\"start\":48695},{\"end\":48714,\"start\":48706},{\"end\":48722,\"start\":48719},{\"end\":48738,\"start\":48732},{\"end\":49183,\"start\":49177},{\"end\":49191,\"start\":49188},{\"end\":49202,\"start\":49195},{\"end\":49213,\"start\":49206},{\"end\":49228,\"start\":49217},{\"end\":49238,\"start\":49232},{\"end\":49250,\"start\":49242},{\"end\":49262,\"start\":49259},{\"end\":49274,\"start\":49266},{\"end\":49290,\"start\":49284},{\"end\":49741,\"start\":49736},{\"end\":49750,\"start\":49745},{\"end\":50040,\"start\":50034},{\"end\":50053,\"start\":50044},{\"end\":50062,\"start\":50057},{\"end\":50564,\"start\":50558},{\"end\":50576,\"start\":50568},{\"end\":50584,\"start\":50581},{\"end\":50593,\"start\":50588},{\"end\":51107,\"start\":51101},{\"end\":51118,\"start\":51113},{\"end\":51498,\"start\":51492},{\"end\":51509,\"start\":51504},{\"end\":51944,\"start\":51936},{\"end\":51951,\"start\":51948},{\"end\":51963,\"start\":51955},{\"end\":51975,\"start\":51967},{\"end\":52494,\"start\":52488},{\"end\":52874,\"start\":52864},{\"end\":52883,\"start\":52878},{\"end\":52896,\"start\":52887},{\"end\":52909,\"start\":52902},{\"end\":52917,\"start\":52913},{\"end\":52936,\"start\":52921},{\"end\":52949,\"start\":52940},{\"end\":52963,\"start\":52955},{\"end\":52974,\"start\":52969},{\"end\":53451,\"start\":53448},{\"end\":53463,\"start\":53455},{\"end\":53475,\"start\":53467},{\"end\":53487,\"start\":53479},{\"end\":53933,\"start\":53930},{\"end\":53945,\"start\":53937},{\"end\":53957,\"start\":53949},{\"end\":54352,\"start\":54340},{\"end\":54362,\"start\":54358},{\"end\":54374,\"start\":54367},{\"end\":54390,\"start\":54380},{\"end\":54400,\"start\":54394},{\"end\":54409,\"start\":54404},{\"end\":54804,\"start\":54797},{\"end\":55277,\"start\":55272},{\"end\":55285,\"start\":55281},{\"end\":55778,\"start\":55771},{\"end\":55789,\"start\":55782},{\"end\":55799,\"start\":55793},{\"end\":55812,\"start\":55803},{\"end\":56359,\"start\":56351},{\"end\":56376,\"start\":56363},{\"end\":56387,\"start\":56380},{\"end\":56810,\"start\":56806},{\"end\":56819,\"start\":56814},{\"end\":56832,\"start\":56825},{\"end\":56843,\"start\":56836},{\"end\":56855,\"start\":56847},{\"end\":56864,\"start\":56859},{\"end\":57299,\"start\":57291},{\"end\":57306,\"start\":57303},{\"end\":57320,\"start\":57310},{\"end\":57713,\"start\":57710},{\"end\":57721,\"start\":57717},{\"end\":57729,\"start\":57725},{\"end\":57737,\"start\":57733},{\"end\":57743,\"start\":57741},{\"end\":57749,\"start\":57747},{\"end\":57755,\"start\":57753},{\"end\":57761,\"start\":57759},{\"end\":58244,\"start\":58235},{\"end\":58256,\"start\":58248},{\"end\":58268,\"start\":58260},{\"end\":58809,\"start\":58803},{\"end\":58824,\"start\":58815},{\"end\":58841,\"start\":58830},{\"end\":59183,\"start\":59178},{\"end\":59193,\"start\":59187},{\"end\":59203,\"start\":59197},{\"end\":59449,\"start\":59446},{\"end\":59455,\"start\":59453},{\"end\":59461,\"start\":59459},{\"end\":59469,\"start\":59465},{\"end\":59944,\"start\":59942},{\"end\":59952,\"start\":59948},{\"end\":59961,\"start\":59956},{\"end\":59968,\"start\":59965},{\"end\":60338,\"start\":60333},{\"end\":60347,\"start\":60342},{\"end\":60362,\"start\":60351},{\"end\":60373,\"start\":60368},{\"end\":60382,\"start\":60377},{\"end\":60392,\"start\":60386},{\"end\":60838,\"start\":60830},{\"end\":60849,\"start\":60842},{\"end\":60859,\"start\":60853},{\"end\":60867,\"start\":60864},{\"end\":61328,\"start\":61312},{\"end\":61340,\"start\":61332},{\"end\":61352,\"start\":61344},{\"end\":61686,\"start\":61678},{\"end\":61698,\"start\":61690},{\"end\":62240,\"start\":62225},{\"end\":62251,\"start\":62246},{\"end\":62264,\"start\":62255},{\"end\":62275,\"start\":62268},{\"end\":62285,\"start\":62281},{\"end\":62307,\"start\":62291},{\"end\":62317,\"start\":62311},{\"end\":62329,\"start\":62323},{\"end\":62340,\"start\":62335},{\"end\":62809,\"start\":62794},{\"end\":62817,\"start\":62813},{\"end\":62828,\"start\":62821},{\"end\":62841,\"start\":62834},{\"end\":62852,\"start\":62847},{\"end\":62863,\"start\":62858},{\"end\":62876,\"start\":62867},{\"end\":63422,\"start\":63407},{\"end\":63431,\"start\":63428},{\"end\":63442,\"start\":63437},{\"end\":63846,\"start\":63840},{\"end\":63857,\"start\":63852},{\"end\":63870,\"start\":63861},{\"end\":63881,\"start\":63874},{\"end\":63893,\"start\":63887},{\"end\":63904,\"start\":63899},{\"end\":63916,\"start\":63910},{\"end\":63927,\"start\":63922},{\"end\":64321,\"start\":64316},{\"end\":64331,\"start\":64327},{\"end\":64344,\"start\":64337},{\"end\":64355,\"start\":64350},{\"end\":64368,\"start\":64359},{\"end\":64756,\"start\":64750},{\"end\":64768,\"start\":64760},{\"end\":64780,\"start\":64772},{\"end\":65196,\"start\":65189},{\"end\":65209,\"start\":65200},{\"end\":65220,\"start\":65215},{\"end\":65650,\"start\":65637},{\"end\":65662,\"start\":65654},{\"end\":65674,\"start\":65666},{\"end\":66203,\"start\":66190},{\"end\":66543,\"start\":66536},{\"end\":66551,\"start\":66547},{\"end\":66924,\"start\":66917},{\"end\":66935,\"start\":66928},{\"end\":66948,\"start\":66941},{\"end\":67279,\"start\":67270},{\"end\":67291,\"start\":67283},{\"end\":67300,\"start\":67295},{\"end\":67311,\"start\":67304}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1016/j.compbiomed.2017.09.017\",\"id\":\"b0\",\"matched_paper_id\":9657480},\"end\":47228,\"start\":46716},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":46925833},\"end\":47732,\"start\":47230},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14106822},\"end\":48230,\"start\":47734},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16567681},\"end\":48597,\"start\":48232},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13671673},\"end\":49088,\"start\":48599},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52057832},\"end\":49674,\"start\":49090},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7333079},\"end\":50030,\"start\":49676},{\"attributes\":{\"id\":\"b7\"},\"end\":50436,\"start\":50032},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":73494492},\"end\":51010,\"start\":50438},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":51726606},\"end\":51381,\"start\":51012},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":204742059},\"end\":51797,\"start\":51383},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":27835175},\"end\":52407,\"start\":51799},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16308483},\"end\":52756,\"start\":52409},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":91320213},\"end\":53343,\"start\":52758},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":25324045},\"end\":53822,\"start\":53345},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52188708},\"end\":54273,\"start\":53824},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3531854},\"end\":54707,\"start\":54275},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15570759},\"end\":55175,\"start\":54709},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":17481305},\"end\":55647,\"start\":55177},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":54563129},\"end\":56268,\"start\":55649},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":41858699},\"end\":56701,\"start\":56270},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":20140745},\"end\":57204,\"start\":56703},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":5319132},\"end\":57591,\"start\":57206},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":23809918},\"end\":58108,\"start\":57593},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":202025584},\"end\":58682,\"start\":58110},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4904459},\"end\":59159,\"start\":58684},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1779661},\"end\":59370,\"start\":59161},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3669946},\"end\":59795,\"start\":59372},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":43982392},\"end\":60235,\"start\":59797},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14051440},\"end\":60716,\"start\":60237},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":22076362},\"end\":61215,\"start\":60718},{\"attributes\":{\"id\":\"b31\"},\"end\":61555,\"start\":61217},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53096315},\"end\":62127,\"start\":61557},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2934551},\"end\":62686,\"start\":62129},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3548096},\"end\":63238,\"start\":62688},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":129944139},\"end\":63784,\"start\":63240},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1300841},\"end\":64223,\"start\":63786},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52067765},\"end\":64663,\"start\":64225},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":10736717},\"end\":65077,\"start\":64665},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5275290},\"end\":65508,\"start\":65079},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3206129},\"end\":66024,\"start\":65510},{\"attributes\":{\"id\":\"b41\"},\"end\":66478,\"start\":66026},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":207224096},\"end\":66854,\"start\":66480},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":18255541},\"end\":67195,\"start\":66856},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":28105079},\"end\":67553,\"start\":67197}]", "bib_title": "[{\"end\":46820,\"start\":46716},{\"end\":47313,\"start\":47230},{\"end\":47850,\"start\":47734},{\"end\":48316,\"start\":48232},{\"end\":48666,\"start\":48599},{\"end\":49171,\"start\":49090},{\"end\":49732,\"start\":49676},{\"end\":50554,\"start\":50438},{\"end\":51097,\"start\":51012},{\"end\":51488,\"start\":51383},{\"end\":51932,\"start\":51799},{\"end\":52482,\"start\":52409},{\"end\":52860,\"start\":52758},{\"end\":53444,\"start\":53345},{\"end\":53926,\"start\":53824},{\"end\":54336,\"start\":54275},{\"end\":54795,\"start\":54709},{\"end\":55268,\"start\":55177},{\"end\":55767,\"start\":55649},{\"end\":56347,\"start\":56270},{\"end\":56800,\"start\":56703},{\"end\":57285,\"start\":57206},{\"end\":57706,\"start\":57593},{\"end\":58231,\"start\":58110},{\"end\":58799,\"start\":58684},{\"end\":59174,\"start\":59161},{\"end\":59442,\"start\":59372},{\"end\":59938,\"start\":59797},{\"end\":60329,\"start\":60237},{\"end\":60826,\"start\":60718},{\"end\":61672,\"start\":61557},{\"end\":62219,\"start\":62129},{\"end\":62790,\"start\":62688},{\"end\":63401,\"start\":63240},{\"end\":63834,\"start\":63786},{\"end\":64312,\"start\":64225},{\"end\":64746,\"start\":64665},{\"end\":65185,\"start\":65079},{\"end\":65633,\"start\":65510},{\"end\":66532,\"start\":66480},{\"end\":66913,\"start\":66856},{\"end\":67266,\"start\":67197}]", "bib_author": "[{\"end\":46835,\"start\":46822},{\"end\":46843,\"start\":46835},{\"end\":46855,\"start\":46843},{\"end\":46864,\"start\":46855},{\"end\":46873,\"start\":46864},{\"end\":47328,\"start\":47315},{\"end\":47336,\"start\":47328},{\"end\":47348,\"start\":47336},{\"end\":47357,\"start\":47348},{\"end\":47366,\"start\":47357},{\"end\":47377,\"start\":47366},{\"end\":47861,\"start\":47852},{\"end\":47873,\"start\":47861},{\"end\":47884,\"start\":47873},{\"end\":47896,\"start\":47884},{\"end\":47906,\"start\":47896},{\"end\":48331,\"start\":48318},{\"end\":48341,\"start\":48331},{\"end\":48355,\"start\":48341},{\"end\":48680,\"start\":48668},{\"end\":48693,\"start\":48680},{\"end\":48704,\"start\":48693},{\"end\":48716,\"start\":48704},{\"end\":48724,\"start\":48716},{\"end\":48728,\"start\":48724},{\"end\":48740,\"start\":48728},{\"end\":48744,\"start\":48740},{\"end\":49185,\"start\":49173},{\"end\":49193,\"start\":49185},{\"end\":49204,\"start\":49193},{\"end\":49215,\"start\":49204},{\"end\":49230,\"start\":49215},{\"end\":49240,\"start\":49230},{\"end\":49252,\"start\":49240},{\"end\":49256,\"start\":49252},{\"end\":49264,\"start\":49256},{\"end\":49276,\"start\":49264},{\"end\":49280,\"start\":49276},{\"end\":49292,\"start\":49280},{\"end\":49296,\"start\":49292},{\"end\":49743,\"start\":49734},{\"end\":49752,\"start\":49743},{\"end\":49761,\"start\":49752},{\"end\":50042,\"start\":50032},{\"end\":50055,\"start\":50042},{\"end\":50064,\"start\":50055},{\"end\":50566,\"start\":50556},{\"end\":50578,\"start\":50566},{\"end\":50586,\"start\":50578},{\"end\":50595,\"start\":50586},{\"end\":50599,\"start\":50595},{\"end\":51109,\"start\":51099},{\"end\":51120,\"start\":51109},{\"end\":51500,\"start\":51490},{\"end\":51511,\"start\":51500},{\"end\":51946,\"start\":51934},{\"end\":51953,\"start\":51946},{\"end\":51965,\"start\":51953},{\"end\":51977,\"start\":51965},{\"end\":52496,\"start\":52484},{\"end\":52876,\"start\":52862},{\"end\":52885,\"start\":52876},{\"end\":52898,\"start\":52885},{\"end\":52911,\"start\":52898},{\"end\":52919,\"start\":52911},{\"end\":52938,\"start\":52919},{\"end\":52951,\"start\":52938},{\"end\":52965,\"start\":52951},{\"end\":52976,\"start\":52965},{\"end\":53453,\"start\":53446},{\"end\":53465,\"start\":53453},{\"end\":53477,\"start\":53465},{\"end\":53489,\"start\":53477},{\"end\":53935,\"start\":53928},{\"end\":53947,\"start\":53935},{\"end\":53959,\"start\":53947},{\"end\":54354,\"start\":54338},{\"end\":54364,\"start\":54354},{\"end\":54376,\"start\":54364},{\"end\":54392,\"start\":54376},{\"end\":54402,\"start\":54392},{\"end\":54411,\"start\":54402},{\"end\":54415,\"start\":54411},{\"end\":54806,\"start\":54797},{\"end\":55279,\"start\":55270},{\"end\":55287,\"start\":55279},{\"end\":55780,\"start\":55769},{\"end\":55791,\"start\":55780},{\"end\":55801,\"start\":55791},{\"end\":55814,\"start\":55801},{\"end\":56361,\"start\":56349},{\"end\":56378,\"start\":56361},{\"end\":56389,\"start\":56378},{\"end\":56812,\"start\":56802},{\"end\":56821,\"start\":56812},{\"end\":56834,\"start\":56821},{\"end\":56845,\"start\":56834},{\"end\":56857,\"start\":56845},{\"end\":56866,\"start\":56857},{\"end\":57301,\"start\":57287},{\"end\":57308,\"start\":57301},{\"end\":57322,\"start\":57308},{\"end\":57715,\"start\":57708},{\"end\":57723,\"start\":57715},{\"end\":57731,\"start\":57723},{\"end\":57739,\"start\":57731},{\"end\":57745,\"start\":57739},{\"end\":57751,\"start\":57745},{\"end\":57757,\"start\":57751},{\"end\":57763,\"start\":57757},{\"end\":58246,\"start\":58233},{\"end\":58258,\"start\":58246},{\"end\":58270,\"start\":58258},{\"end\":58811,\"start\":58801},{\"end\":58826,\"start\":58811},{\"end\":58843,\"start\":58826},{\"end\":59185,\"start\":59176},{\"end\":59195,\"start\":59185},{\"end\":59205,\"start\":59195},{\"end\":59451,\"start\":59444},{\"end\":59457,\"start\":59451},{\"end\":59463,\"start\":59457},{\"end\":59471,\"start\":59463},{\"end\":59946,\"start\":59940},{\"end\":59954,\"start\":59946},{\"end\":59963,\"start\":59954},{\"end\":59970,\"start\":59963},{\"end\":60340,\"start\":60331},{\"end\":60349,\"start\":60340},{\"end\":60364,\"start\":60349},{\"end\":60375,\"start\":60364},{\"end\":60384,\"start\":60375},{\"end\":60394,\"start\":60384},{\"end\":60840,\"start\":60828},{\"end\":60851,\"start\":60840},{\"end\":60861,\"start\":60851},{\"end\":60869,\"start\":60861},{\"end\":60873,\"start\":60869},{\"end\":61330,\"start\":61310},{\"end\":61342,\"start\":61330},{\"end\":61354,\"start\":61342},{\"end\":61688,\"start\":61674},{\"end\":61700,\"start\":61688},{\"end\":62242,\"start\":62221},{\"end\":62253,\"start\":62242},{\"end\":62266,\"start\":62253},{\"end\":62277,\"start\":62266},{\"end\":62287,\"start\":62277},{\"end\":62309,\"start\":62287},{\"end\":62319,\"start\":62309},{\"end\":62331,\"start\":62319},{\"end\":62342,\"start\":62331},{\"end\":62811,\"start\":62792},{\"end\":62819,\"start\":62811},{\"end\":62830,\"start\":62819},{\"end\":62843,\"start\":62830},{\"end\":62854,\"start\":62843},{\"end\":62865,\"start\":62854},{\"end\":62878,\"start\":62865},{\"end\":63424,\"start\":63403},{\"end\":63433,\"start\":63424},{\"end\":63444,\"start\":63433},{\"end\":63848,\"start\":63836},{\"end\":63859,\"start\":63848},{\"end\":63872,\"start\":63859},{\"end\":63883,\"start\":63872},{\"end\":63895,\"start\":63883},{\"end\":63906,\"start\":63895},{\"end\":63918,\"start\":63906},{\"end\":63929,\"start\":63918},{\"end\":64323,\"start\":64314},{\"end\":64333,\"start\":64323},{\"end\":64346,\"start\":64333},{\"end\":64357,\"start\":64346},{\"end\":64370,\"start\":64357},{\"end\":64758,\"start\":64748},{\"end\":64770,\"start\":64758},{\"end\":64782,\"start\":64770},{\"end\":65198,\"start\":65187},{\"end\":65211,\"start\":65198},{\"end\":65222,\"start\":65211},{\"end\":65652,\"start\":65635},{\"end\":65664,\"start\":65652},{\"end\":65676,\"start\":65664},{\"end\":66205,\"start\":66188},{\"end\":66545,\"start\":66534},{\"end\":66553,\"start\":66545},{\"end\":66926,\"start\":66915},{\"end\":66937,\"start\":66926},{\"end\":66950,\"start\":66937},{\"end\":67281,\"start\":67268},{\"end\":67293,\"start\":67281},{\"end\":67302,\"start\":67293},{\"end\":67313,\"start\":67302}]", "bib_venue": "[{\"end\":66696,\"start\":66633},{\"end\":46946,\"start\":46913},{\"end\":47455,\"start\":47411},{\"end\":47956,\"start\":47946},{\"end\":48407,\"start\":48355},{\"end\":48816,\"start\":48777},{\"end\":49357,\"start\":49328},{\"end\":49833,\"start\":49776},{\"end\":50221,\"start\":50064},{\"end\":50699,\"start\":50633},{\"end\":51170,\"start\":51160},{\"end\":51561,\"start\":51551},{\"end\":52077,\"start\":52011},{\"end\":52561,\"start\":52513},{\"end\":53028,\"start\":53010},{\"end\":53555,\"start\":53526},{\"end\":54020,\"start\":53991},{\"end\":54465,\"start\":54455},{\"end\":54862,\"start\":54837},{\"end\":55389,\"start\":55316},{\"end\":55938,\"start\":55839},{\"end\":56458,\"start\":56427},{\"end\":56927,\"start\":56904},{\"end\":57372,\"start\":57362},{\"end\":57826,\"start\":57795},{\"end\":58370,\"start\":58304},{\"end\":58895,\"start\":58879},{\"end\":59238,\"start\":59232},{\"end\":59509,\"start\":59471},{\"end\":59994,\"start\":59987},{\"end\":60451,\"start\":60426},{\"end\":60939,\"start\":60910},{\"end\":61308,\"start\":61217},{\"end\":61824,\"start\":61725},{\"end\":62386,\"start\":62371},{\"end\":62939,\"start\":62910},{\"end\":63493,\"start\":63461},{\"end\":63977,\"start\":63965},{\"end\":64418,\"start\":64406},{\"end\":64843,\"start\":64814},{\"end\":65271,\"start\":65239},{\"end\":65744,\"start\":65701},{\"end\":66186,\"start\":66026},{\"end\":66631,\"start\":66553},{\"end\":67000,\"start\":66990},{\"end\":67345,\"start\":67341}]"}}}, "year": 2023, "month": 12, "day": 17}