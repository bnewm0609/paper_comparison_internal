{"id": 28698351, "updated": "2023-11-12 15:31:14.811", "metadata": {"title": "VisDA: The Visual Domain Adaptation Challenge", "authors": "[{\"first\":\"Xingchao\",\"last\":\"Peng\",\"middle\":[]},{\"first\":\"Ben\",\"last\":\"Usman\",\"middle\":[]},{\"first\":\"Neela\",\"last\":\"Kaushik\",\"middle\":[]},{\"first\":\"Judy\",\"last\":\"Hoffman\",\"middle\":[]},{\"first\":\"Dequan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Kate\",\"last\":\"Saenko\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 10, "day": 18}, "abstract": "We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, a large-scale testbed for unsupervised domain adaptation across visual domains. Unsupervised domain adaptation aims to solve the real-world problem of domain shift, where machine learning models trained on one domain must be transferred and adapted to a novel visual domain without additional supervision. The VisDA2017 challenge is focused on the simulation-to-reality shift and has two associated tasks: image classification and image segmentation. The goal in both tracks is to first train a model on simulated, synthetic data in the source domain and then adapt it to perform well on real image data in the unlabeled test domain. Our dataset is the largest one to date for cross-domain object classification, with over 280K images across 12 categories in the combined training, validation and testing domains. The image segmentation dataset is also large-scale with over 30K images across 18 categories in the three domains. We compare VisDA to existing cross-domain adaptation datasets and provide a baseline performance analysis using various domain adaptation models that are currently popular in the field.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2766897166", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1710-06924", "doi": null}}, "content": {"source": {"pdf_hash": "51f786c6feecf4a378bec22f807a048bb0dbbc26", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1710.06924v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4edfc695ef6850da2b22fb1d56f5c326f0991ab6", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/51f786c6feecf4a378bec22f807a048bb0dbbc26.txt", "contents": "\nVisDA: The Visual Domain Adaptation Challenge\n\n\nXingchao Peng \nDepartment of Computer Science\nBoston University\n\n\nBen Usman \nDepartment of Computer Science\nBoston University\n\n\nNeela Kaushik \nDepartment of Computer Science\nBoston University\n\n\nJudy Hoffman \nEECS\nUniversity of California Berkeley\n\n\nDequan Wang dqwang@eecs.berkeley.edu \nEECS\nUniversity of California Berkeley\n\n\nKate Saenko saenko@bu.edu \nDepartment of Computer Science\nBoston University\n\n\nVisDA: The Visual Domain Adaptation Challenge\n\nWe present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge 1 , a large-scale testbed for unsupervised domain adaptation across visual domains. Unsupervised domain adaptation aims to solve the real-world problem of domain shift, where machine learning models trained on one domain must be transferred and adapted to a novel visual domain without additional supervision. The VisDA2017 challenge is focused on the simulation-toreality shift and has two associated tasks: image classification and image segmentation. The goal in both tracks is to first train a model on simulated, synthetic data in the source domain and then adapt it to perform well on real image data in the unlabeled test domain. Our dataset is the largest one to date for cross-domain object classification, with over 280K images across 12 categories in the combined training, validation and testing domains. The image segmentation dataset is also large-scale with over 30K images across 18 categories in the three domains. We compare VisDA to existing cross-domain adaptation datasets and provide a baseline performance analysis using various domain adaptation models that are currently popular in the field.\n\nIntroduction\n\nIt is well known that the success of machine learning methods on visual recognition tasks is highly dependent on access to large labeled datasets. Unfortunately, model performance often drops significantly on data from a new deployment domain, a problem known as dataset shift, or dataset bias [27]. Changes in the visual domain can include lighting, camera pose and background variation, as well as general changes in how the image data is collected. While this problem has been studied extensively in the domain adaptation literature [11], progress has been limited by the lack of large-scale challenge benchmarks.\n\nChallenge competitions drive innovation in computer vision, however, most challenges randomly sample train and test sets from the same domain. This assumes that test data has the same distribution as train data-an assumption that rarely holds in real-life deployment scenarios. Cross-domain benchmarks [24,34,4,44] have been created for evaluating domain adaptation algorithms, but they often have low task diversity, small domain shifts and small dataset size. This paper introduces the Visual Domain Adaptation (VisDA) dataset and challenge to encourage further progress in domain transfer methods. The challenge task is to train a model on a source domain and then improve its performance on a new target domain, without any additional annotations. Our challenge includes two tracks. The first is the more traditional object classification task. The second is the relatively less-studied semantic segmentation task, which involves labeling each pixel of an input image with a semantic label.\n\nThis first VisDA challenge focuses on the domain shift from simulated to real imagery-a challenging shift that has many practical applications in computer vision. One such application is augmenting training data with simulated imagery for tasks where real image data is difficult or expensive to collect. To address the scale problem, we generate the largest cross-domain object classification dataset to date with over 280K images in the combined training, validation and testing sets. For the semantic segmentation track we augment existing datasets for a total of nearly 30k images across three domains.\n\nThe VisDA2017 challenge focuses on unsupervised domain adaptation, which assumes that the deployment do-main is not labeled. For each task we provide a labeled training (source) domain and unlabeled validation and test (target) domains. The goal of unsupervised adaptation is to utilize the unlabeled target data to improve the source model's performance on the target domain. While there may be scenarios where target labels are available for supervised adaptation, we focus on purely unsupervised adaptation as it is one of the most challenging and realistic scenarios.\n\n\nRelated Work\n\nThere has been a lot of prior work on visual domain adaptation, ranging from the earlier shallow feature methods [35,2,12] to the more recent deep adaptation approaches [14,46]. A review of existing work in this area is beyond the scope of this paper; we refer the reader to a recent survey [11].\n\nSeveral benchmark datasets have been collected and used to evaluate visual domain adaptation, summarized in Table 1. Adaptation of image classification methods has been among the most extensively studied problems of visual domain adaptation. One of the difficulties in re-using existing datasets to create multi-domain benchmarks is that the same categories must exist in all domains. For digits (ten categories, 0-9), the most popular benchmark consists of three domains: MNIST (hand-written digits) [18], USPS (handwritten digits) [16] and SVHN (street view house numbers) [25]. The original digit images are sometimes synthetically augmented (e.g. by inverting colors) to create additional domains. The Office dataset [35] is a popular benchmark for real-world objects. It contains 31 object categories captured in three domains: office environment images taken with a high quality camera (DSLR), the same environment captured with a low quality webcam (WEB-CAM), and images downloaded from the amazon.com website (AMAZON).\n\nOne problem with these popular benchmarks is the lack of task diversity: the most common cross-domain datasets focus on the image classification task, ignoring other tasks such as detection, structure prediction and sequence labeling. Another problem is the relatively small domain shifts, such as the shift between two different sensors (DSLR vs Webcam in the Office dataset [35]), or between two very similar handwritten digit datasets (MNIST vs USPS). Over time, improvements in the underlying visual representations and adaptation techniques have closed the domain gap on these benchmarks, and more challenging domain shifts are now needed to drive further progress. Another issue is the small scale. Modern computer vision methods require a lot of training data, while cross-domain datasets such as [35] only contain several hundred images.\n\nThe Cross-Dataset Testbed [44] is a more recent classification benchmark. The \"dense\" version contains 40 classes extracted from 4 datasets-Caltech256, Bing, SUN and Imagenet-with a minimum of 20 images per class in each dataset. It is significantly larger than Office, however, some domains are fairly close as they were collected in a similar way from web search engines. On the Caltech-Imagenet shift, adaptation performance has reached close to 90% ac-\n\n\nDIGIT CLASSIFICATION Dataset\n\nExamples Classes Domains USPS-subset [16] 1,800 10-digits 1 MNIST-subset [18] 2,000 10-digits 1 USPS-Full [16] 9,000 10-digits 1 MNIST-Full [18] 70,000 10-digits 1 SVHN [25] 630,420 Examples Classes Domains COIL20 [24] 1,440 20-coil 1 Office [34] 1,410 31-office 3 Caltech [4] 1,123 10-office 1 CAD-office [35] 775 20-office 1 Cross-Dataset [43] 40 VisDA-C 207,785 12 3\n\n\nSEMANTIC SEGMENTATION Dataset\n\nExamples Classes Domains SYNTHIA-subset [32] 9,400 12-City 1 CityScapes [10] 5,000 34 1 GTA5 [31] 24,966 18 1 VisDA-S 30,000 18 3 curacy [4]. Synthetic data augmentation has been extensively applied to computer vision research. More specifically, 3D models have been utilized to generate synthetic images with variable object poses, textures, and backgrounds [26]. Recent usage of 3D simulation has been extended to multiple vision tasks such as object detection [26,40], pose estimation [38], robotic simulation [45], semantic segmentation [30]. Popular 3D model databases of common objects that may be used in visual domain adaptation tasks include ObjectNet3D [51], ShapeNet and the related Mod-elNet [6]. Table 2 shows a comparison of the VisDA classification dataset to existing synthetic object datasets.\n\nThe popularity of the image classification task as a testbed could be due to the relative simplicity of the task and the lower effort required to engineer a good baseline model. Compared with other vision problems such as object detection, activity detection in video, or structure prediction, the image classification is simpler and less computationally expensive to explore. Moreover, many state-ofthe-art classification models are readily available for use as a baseline upon which adaptation can be applied. At the same time, other tasks may have characteristics that could present unique challenges for domain adaptation. In this work, we propose experimental setups for both the more common classification task, and the less studied semantic segmentation task.\n\n\nSYNTHETIC OBJECTS Dataset\n\nModels Images Classes ModelNet [50] 127,915 -662 PASCAL3D+ [52] -30,899 12 ObjectNet3D [51] 44,147 -100 ShapeNet-Core [5] 51,300 -55 ShapeNet-Sem [5] 12,000 -270 CIN 2D+3D [3] -11,664 18 Redwood [9] 10,000 -44 IKEA [19] 219 -11 VisDA-C 1,907 152,397 12 Table 2: Comparison between VisDA-C and existing synthetic object datasets. Many datasets or consist renders with non-realistic lighting conditions.\n\nThe semantic segmentation methods assign object labels to each pixel of an image based on local image features. Dataset annotation is a highly labor-intensive process, this why there are only a very few semantic segmentation datasets designed specifically for domain adaptation. Two datasets that are frequently paired together for visual segmentation tasks are SYNTHIA [32] and CityScapes [10]. SYNTHIA provides a collection of synthetically generated urban images that mimic car dash-cam footage, while CityScapes provides images of real urban street scenes. Other synthetic street-view datasets include GTA5 [31] and Virtual KITTI [13]. While urban scenes are the most commonly studied tasks for domain adaptation in semantic segmentation, other datasets exist that study, for example, segmentation in video data for visual tracking [23,49] or segmentation of clothing and fashion images for the purpose of identifying people in web-cam footage based on physical descriptions of their apparel [8].\n\n\nVisDA-C: Classification Dataset\n\nThe VisDA Classification (VisDA-C) dataset provides a large-scale testbed for unsupervised domain adaptation in image classification tasks. The dataset consists of three splits (domains), each containing the same 12 object categories:  \n\u2022\n\nDataset Acquisition\n\n\nTraining Domain: Synthetic CAD\n\nThe synthetic dataset was generated by rendering 3D models of the same object categories as in the real data from different angles and under different lighting conditions. As shown in Table 3, we obtained 1,907 models in total and generated 152,397 synthetic images. We used four main sources of models that are indicated with a sec prefix of the corresponding image filename. These four sources include manually chosen subsets of ShapenetCore [6], NTU 3D [7], SHREC 2010 [47] with some labels retrieved from TSB [42] and our own collection of 3D CAD models from 3D Warehouse SketchUp. We used twenty different camera yaw angles with four different light directions per model. The lighting setup consists of ambient and sun light sources in 1:3 proportion. Objects were rotated, scaled and translated to match the floor plane, duplicate faces and vertices were removed, and the camera was automatically positioned to capture the entire object with a margin around it. For textured models, we also rendered their un-textured versions with a plain grey albedo. In total, we generated 152,397 synthetic images to form the synthetic source domain. Per-category image numbers are shown in Table 3, and Figure 2 shows some samples of training domain data.  splits. In total, the MS COCO dataset contains 174,011 images. We used annotations provided by the COCO dataset to find and crop relevant object in each image. All images were padded by retaining an additional~50% of its cropped height and width (i.e. by dividing the height and width by \u221a 2 ). Padded images under 70 x 70 pixels were excluded as they significantly reduced the accuracy of classification algorithms during baseline analysis. See figure 3 for sample validation domain data. In total, we used 55,388 images that fall into the twelve categories that overlap with the other two domains and were large enough. We took all images from each of twelve categories with the exception of the \"person\" category, which was reduced to 5,000 images in order to balance the overall number of images per category. The breakdown of the validation domain dataset by the number of images per category is shown in Table 3.\n\n\nValidation Domain: COCO\n\n\nTesting Domain: YouTube Bounding Boxes\n\nThe testing images come from the YouTube Bounding Boxes dataset [29]. Compared to validation domain (MS COCO dataset), the image resolution is much lower due to the testing images are cropped from YouTube videos. The original YouTube-BB dataset comprises segments extracted from 240,000 videos and contains approximately 5.6 million bounding boxes annotations for 23 categories of tracked objects. We extracted 72,372 frame crops that fall into one of our twelve categories and satisfy the size constraints. The breakdown of the test domain by images per category is shown in Table 3. Some samples are shown in Figure 3.\n\n\nExperimental Setup\n\nOur experiments aim to provide a baseline for the challenge. We perform in-domain (i.e. train and test on the same domain) experiments to get the \"oracle\" results and source-only (i.e. train only on the source domain) to get the lower bound results. In total, we have 152,397 images as the source domain and 55,388 images as the target domain for validation. In our in-domain experiments, we follow a 70%/30% split for training and testing, i.e., 106,679 training images, 45,718 test images for the synthetic domain and 38,772 training images, 16,616 test images for the real domain.\n\nIn our baseline experiments, we adopt AlexNet architecture. In the training phases of AlexNet, the output dimension of last fully connected layer is changed to 12. We initialize the network with parameters learned on Ima-geNet [33], except the last layer. The last layer is initialized with N (0, 0.01). We utilize mini-batch stochastic gradient descent (SGD) and set the base learning rate to be 10 \u22123 , weight decay to be 5 \u00d7 10 \u22124 and momentum to be 0.9. We report the mean accuracy of classification result at 40k iterations.\n\n\nDomain Adaptation Algorithms\n\nIn this report, we provide two domain adaptation algorithms as the baselines. DAN (Deep Adaptation Network) [22] learns transferable features by training deep models with Maximum Mean Discrepancy [36] loss to align the feature distribution of source domain to target domain. The network architecture of DAN is extended from AlexNet [17], which consists of 5 convolutional layers (conv1 -conv5) and 3 fully connected layers (fc6 -fc8). Deep CORAL (Deep Correlation Alignment) [39] is devised to match the second-order statistics of feature distributions. It is derived by minimizing the domain discrepancy with squared Frobenius norm min Cov S \u2212Cov T 2 F , where Cov S , Cov T are the covariance matrices of feature vectors from source domain and target domain, respectively.\n\n\nBaseline Results\n\nThe baseline results on the validation domain for classification are shown in Table 4. \"Oracle\" or in-domain AlexNet performance of training and testing on the synthetic domain reaches 99.92% accuracy, and training and testing on the real validation domain leads to 87.62%. For crossdomain results of validation dataset, AlexNet trained on synthetic source domain and tested on real domain leads to 28.12% accuracy. Among the tested domain adaptation algorithms, Deep CORAL improves the cross-domain performance from 28.12% to 45.53% and DAN further boosts the result to 51.62%. For results of test dataset, AlexNet gets 30.81% mean accuracy, and DAN and Deep CORAL improve the result to 49.78% and 45.29%, respectively. AlexNet gets a lower source-only model performance due to its simpler architecture. However, the relative improvement of domain adaptation algorithms (i.e. DAN and Deep CORAL) is still large.\n\n\nVisDA-S: Semantic Segmentation\n\nThe goal of the VisDA2017 Segmentation (VisDA-S) is to test adaptation between synthetic dashcam views and real dashcam footage for semantic image segmentation. The training data includes pixel-level semantic annotations for 19 classes. We will also provide validation and testing data, following same protocol as for classification:\n\n\u2022 training domain (source): synthetic dashcam renderings from the GTA5 dataset along with semantic segmentation labels for 18 classes,\n\n\u2022 validation domain (target): a real-world collection of dashcam images from the CityScapes dataset along with semantic segmentation labels for the corresponding 18 classes to be used for validating the unsupervised semantic segmentation performance,\n\n\u2022 test domain (target): a different set of unlabeled, realworld images from the new NEXAR dashcam dataset.\n\n\nDataset Acquisition\n\nThe training and validation domain datasets used here are the same as those used in Hoffman et al (2016) [15] for their work in synthetic to real adaptation in semantic segmentation tasks.\n\n\nTraining Domain: Synthetic GTA5\n\nThe images in the segmentation training come from the GTA5 dataset. GTA5 consists of 24,966 high quality labeled frames from the photorealistic, open-world computer game, Grand Theft Auto V (GTA5). The frames are synthesized from a fictional city modeled off of Los Angeles, CA and are in high-resolution, 1914\u00d71052. All semantic segmentation labels used in the GTA5 dataset have a counterpart in the CityScapes category list for adaptation. See Figure 5 for sample training domain data.\n\n\nValidation Domain: Real CityScapes\n\nImages in the segmentation validation domain come from the CityScapes dataset. CityScapes contains 5,000 images separated by the individual European cities from which they were taken, with a breakdown of 2,975 training images, 500 validation images and 1,525 test images. Images are in high resolution, 2048 \u00d7 1024. In total, the CityScapes dataset has 34 semantic segmentation categories, of which we are interested in the 18 that overlap with the synthetic GTA5 dataset. See Figure 6 for sample validation domain data.\n\n\nTesting Domain: Real DashCam Images\n\nImages in the segmentation testing domain were provided by NEXAR. They were collected using the NEXAR dashcam interface and manually annotated with segmentation labels. We use 1500 images of size 1280 \u00d7 720 available with annotations corresponding with the 18 categories matching GTA5 and CityScapes. Note that this data along with the annotations is part of a larger data collection effort by Berkeley Deep Drive (BDD) 2 which will be released shortly. See Figure 7 for sample test domain data.\n\n\nDomain Adaptation Algorithms\n\nFor details on the domain adaptation algorithms applied to this domain shift, we refer the reader to the original work performed on adaptation from GTA5 (synthetic) to CityScapes (real) in [15]. The authors use the front-end dilated fully convolutional network as the baseline model. The method for domain adaptive semantic segmentation consists of both global and category specific adaptation techniques. Please see section 3 (Fully Convolutional Adaptation Models) in [15] for detailed information about these techniques and their implementation. In all experiments, the Intersection over Union (IoU) evaluation metric is used to determine per-category segmentation performance.\n\n\nBaseline Results\n\nPlease refer to Table 5 and Section 4.2.1 in Hoffman et al. [15] for full experimental results and discussion of semantic segmentation performance in GTA5 to CityScapes adaptation. A table of the relevant results is also replicated below. In summary, the front-end dilation baseline achieves a mean IoU (mIoU) of 21.6 over all semantic categories. The adaptation method used achieves an mIoU of 25.5. Similar performance improvement is seen when adapting the GTA5 model for use in our challenge test domain.\n\n\nConclusion\n\nWe introduced a novel domain adaptation challenge for visual recognition machine learning methods applied to image classification and image semantic segmentation tasks. The training data consists of images in the simulated/synthetic domain while the validation data are in the real image domain; the dataset generated for the classification track is the largest cross-domain adaptation dataset for object classification to date. We demonstrated experimental baseline results for adaptation in the classification and segmentation tasks using common domain adaptation methods and frameworks.  Table 5: Baseline results for semantic segmentation challenge. Select results using the method from [15] showing percategory adaptation for the task of semantic segmentation. On the top we report performance for the source and adaptation model from GTA5 to our validation data, CityScapes val set. The bottom reports performance for source dilation model and for the adapted model for the challenge test shift from GTA5 to our real world test domain.\n\nWe have publicly released the associated training validation datasets (including labels) for both tasks, and provided an online evaluation server 3 4 . The VisDA challenge ran successfully during July-September 2017 and gathered high interest and participation from the community. The public leaderboards can be accessed at http://ai.bu. edu/visda-2017/. This report will be updated with \"lessons learned\" from the challenge and also as the details of the winning methods become available. We hope that this benchmark will continue to be used by the wider research community to develop and test novel domain adaptation models using an established protocol. Our future plan is to hold the VisDA challenge every year, with different domains, tasks and difficulty of domain shift.      \n\nFigure 1 :\n1The VisDA challenge aims to test domain adaptation methods ability to learn a task on a large labeled source domain and successfully transfer this knowledge to a novel target domain. It contains a challenging simulation-to-real domain shift and consists of two tasks: (a) classification and (b) semantic segmentation.\n\n3 17054 Figure 2 :\n170542https://competitions.codalab.org/competitions/ 17052 4 https://competitions.codalab.org/competitions/ Classification training domain sample data. Synthetic CAD models.\n\nFigure 3 :\n3Classification validation domain sample data. Real images from the MS COCO dataset.\n\nFigure 4 :\n4Classification test domain sample data. Real images from YouTube-BB dataset.\n\nFigure 5 :\n5Segmentation training domain sample data. Synthetic dashcam images from the GTA5 dataset.\n\nFigure 6 :\n6Segmentation validation domain sample data. Real dashcam images from the CityScapes dataset.\n\nFigure 7 :\n7Segmentation test domain sample data. Real dashcam images from the NEXAR dataset.\n\nTable 1 :\n1Comparison to existing cross-domain datasets used for domain adaptation experiments.\n\n\ntraining domain (source): synthetic renderings of 3D models from different angles and with different lighting conditions, \u2022 validation domain (target): a real-image domain consisting of images cropped from the Microsoft COCO dataset [20], \u2022 test domain (target): a real-image domain consisting of images cropped from the Youtube Bounding Box dataset [28]The reason we use different target domains for the validation and test splits is to prevent hyper-parameter tuning on the test data. Unsupervised domain adaptation is usually done in a transductive manner, meaning that unlabeled test data is actively used to train the model. However, it is not possible to tune hyper-parameter on the test data, since it has no labels. Despite this fact, the lack of established validation sets often leads to poor experimental protocols where the labeled test set is used for this purpose. In ourCategory \nTraining \nValidation Testing \nModels Images \nImages \nImages \naeroplane \n179 \n14,309 \n3,646 \n5,196 \nbicycle \n93 \n7,365 \n3,475 \n4,272 \nbus \n208 \n16,640 \n4,690 \n6,970 \ncar \n160 \n12,800 \n10,401 \n7,280 \nhorse \n119 \n9,512 \n4,691 \n6,331 \nknife \n178 \n14,240 \n2,075 \n5,491 \nmotorcycle \n217 \n17,360 \n5,796 \n8,079 \nperson \n152 \n12,160 \n4,000 \n7,673 \nplant \n135 \n10,371 \n4,549 \n4,287 \nskateboard \n146 \n11,680 \n2,281 \n2,762 \ntrain \n200 \n16,000 \n4,236 \n7,264 \ntruck \n120 \n9,600 \n5,548 \n6,767 \ntotal \n1,907 \n152,397 \n55,388 \n72,372 \n\n\n\nTable 3 :\n3This setup also discourages algorithms that are designed to handle a specific target domain. It is important to mention that the validation and test sets are different domains, so over-tuning to one can potentially degrade performance on another.Number of models and images per category in \nVisDA Classification training, validation and testing do-\nmains. \n\nbenchmark, we provide a validation set to mimic more re-\nalistic deployment scenario where the target domain is un-\nknown at training time and test labels are not available for \nhyper-parameter tuning. \n\n\nThe validation dataset for the classification track is built upon the Microsoft COCO[21] Training and ValidationSynthetic CAD Objects \u2192 Real MS COCO \nMethod \nTrain Test aero bike bus \ncar \nhorse knife mbike person plant skbrd train truck Mean \nAlexNet \nsyn \nreal 53.5 3.7 \n50.1 52.2 27.9 14.9 27.6 2.9 \n25.8 10.5 64.4 3.9 \n28.12 \nDAN \nsyn \nreal 71.0 47.4 67.3 31.9 61.4 49.9 72.1 36.1 64.7 28 \n70.6 19 \n51.62 \nD-CORAL syn \nreal 76.5 31.8 60.2 35.3 45.7 48.4 55 \n28.9 56.4 28.2 60.9 19.1 45.53 \nAlexNet \nsyn \nsyn \n100 100 99.8 99.9 100 99.9 99.8 100 100 100 99.9 99.7 99.92 \nAlexNet \nreal real 94.9 83.2 83.1 86.5 93.9 91.8 90.9 86.6 94.9 88.9 87 \n65.4 87.26 \nSynthetic CAD Objects \u2192 Real YT-BB \nMethod \nTrain Test aero bike bus \ncar \nhorse knife mbike person plant skbrd train truck Mean \nAlexNet \nsyn \nreal 46.5 0.8 \n59.2 82.7 21.0 14.4 23.2 1.0 \n46.1 17.2 47.8 9.8 \n30.81 \nDAN \nsyn \nreal 55.4 18.4 59.9 68.6 55.2 41.4 63.4 30.3 78.8 23.0 62.8 40.1 49.78 \nD-CORAL syn \nreal 62.5 21.7 66.4 64.7 31.1 36.6 54.3 24.9 73.8 30.0 43.4 34.1 45.29 \n\n\n\nTable 4 :\n4Baseline results for the classification track. We show per-category accuracy for the classification task using various adaptation models. The top table reports adaptation performance from the CAD model training data to the real, COCO validation data. The bottom reports performance for adaptation from the training domain to the real test domain.AlexNet results are from source-only model, i.e. only data and labels from source domain are utilized when training \nthe model. The tables show domain adaptation algorithms (DAN [22] and D-CORAL [41]) can improve the results by \nroughly 20 percent. \n\n\n\n\nFCN-in-the-wild [15] 57.8 20.1 51.0 6.5 14.1 20.4 26.7 13.7 66.1 22.2 88.9 34.1 13.2 63.2 10.2 7.1 2.0 18.7 0.0 28.2Method \n\nroad \nsidewalk \nbuilding \n\nwall \nfence \npole \nt light \nt sign \nveg \nterrain \nsky \nperson \nrider \ncar \ntruck \nbus \ntrain \nmbike \nbike \nmIoU \nGTA \u2192 CityScapes Validation Domain \nDilation Frontend \n30.6 21.2 44.8 10.1 4.4 15.4 12.4 1.7 75.1 13.5 58.1 38.0 0.2 67.5 9.4 5.0 0.0 0.0 0.0 21.4 \nGTA \u2192 Nexar Test Domain \nDilation Frontend \n40.7 19.2 42.3 4.2 20.0 21.8 26.0 13.4 68.0 19.6 84.7 32.4 5.8 59.0 10.3 9.8 1.6 13.5 0.0 25.9 \n\nhttps://deepdrive.berkeley.edu/, http://data-bdd.berkeley.edu/\n\nExploiting weakly-labeled web images to improve object classification: a domain adaptation approach. A Bergamo, L Torresani, Neural Information Processing Systems (NIPS). A. Bergamo and L. Torresani. Exploiting weakly-labeled web images to improve object classification: a domain adap- tation approach. In Neural Information Processing Systems (NIPS), Dec. 2010. 2\n\nGoing into depth: Evaluating 2d and 3d cues for object classification on a new, large-scale object dataset. B Browatzki, J Fischer, B Graf, H H B\u00fclthoff, C Wallraven, Computer Vision Workshops (ICCV Workshops. IEEE2011 IEEE International Conference onB. Browatzki, J. Fischer, B. Graf, H. H. B\u00fclthoff, and C. Wallraven. Going into depth: Evaluating 2d and 3d cues for object classification on a new, large-scale object dataset. In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 1189-1195. IEEE, 2011. 3\n\nF M Carlucci, L Porzi, B Caputo, E Ricci, S R Bul\u00f2, arXiv:1704.08082Autodial: Automatic domain alignment layers. 1arXiv preprintF. M. Carlucci, L. Porzi, B. Caputo, E. Ricci, and S. R. Bul\u00f2. Autodial: Automatic domain alignment layers. arXiv preprint arXiv:1704.08082, 2017. 1, 2\n\nA X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, arXiv:1512.03012An information-rich 3d model repository. arXiv preprintA. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 3\n\nShapeNet: An Information-Rich 3D Model Repository. A X Chang, T Funkhouser, L Guibas, P Hanrahan, Q Huang, Z Li, S Savarese, M Savva, S Song, H Su, J Xiao, L Yi, F Yu, arXiv:1512.0301223Stanford University -Princeton University -Toyota Technological Institute at ChicagoTechnical Reportcs.GRA. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University -Princeton University - Toyota Technological Institute at Chicago, 2015. 2, 3\n\nOn visual similarity based 3d model retrieval. D.-Y Chen, X.-P Tian, Y.-T Shen, M Ouhyoung, Computer graphics forum. Wiley Online Library22D.-Y. Chen, X.-P. Tian, Y.-T. Shen, and M. Ouhyoung. On visual similarity based 3d model retrieval. In Computer graphics forum, volume 22, pages 223-232. Wiley Online Library, 2003. 3\n\nDeep domain adaptation for describing people based on fine-grained clothing attributes. Q Chen, J Huang, R Feris, L M Brown, J Dong, S Yan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionQ. Chen, J. Huang, R. Feris, L. M. Brown, J. Dong, and S. Yan. Deep domain adaptation for describing people based on fine-grained clothing attributes. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 5315-5324, 2015. 3\n\nS Choi, Q.-Y Zhou, S Miller, V Koltun, arXiv:1602.02481A large dataset of object scans. S. Choi, Q.-Y. Zhou, S. Miller, and V. Koltun. A large dataset of object scans. arXiv:1602.02481, 2016. 3\n\nThe cityscapes dataset for semantic urban scene understanding. M Cordts, M Omran, S Ramos, T Rehfeld, M Enzweiler, R Benenson, U Franke, S Roth, B Schiele, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)23M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2, 3\n\nDomain adaptation for visual applications: A comprehensive survey. G Csurka, abs/1702.05374CoRR1G. Csurka. Domain adaptation for visual applications: A comprehensive survey. CoRR, abs/1702.05374, 2017. 1, 2\n\nVisual event recognition in videos by learning from web data. L Duan, D Xu, I Tsang, J Luo, Proc. CVPR. CVPRL. Duan, D. Xu, I. Tsang, and J. Luo. Visual event recog- nition in videos by learning from web data. In Proc. CVPR, 2010. 2\n\nVirtual worlds as proxy for multi-object tracking analysis. A Gaidon, Q Wang, Y Cabon, E Vig, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as proxy for multi-object tracking analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4340-4349, 2016. 3\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, arXiv:1409.7495arXiv preprintY. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. arXiv preprint arXiv:1409.7495, 2014. 2\n\nJ Hoffman, D Wang, F Yu, T Darrell, arXiv:1612.02649Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. 56arXiv preprintJ. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adapta- tion. arXiv preprint arXiv:1612.02649, 2016. 5, 6\n\nA database for handwritten text recognition research. J J Hull, IEEE Transactions. 165J. J. Hull. A database for handwritten text recognition re- search. IEEE Transactions on pattern analysis and machine intelligence, 16(5):550-554, 1994. 2\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012. 4\n\nGradientbased learning applied to document recognition. Proceedings of the IEEE. Y Lecun, L Bottou, Y Bengio, P Haffner, 86Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient- based learning applied to document recognition. Proceed- ings of the IEEE, 86(11):2278-2324, 1998. 2\n\nParsing IKEA Objects: Fine Pose Estimation. ICCV. J J Lim, H Pirsiavash, A Torralba, J. J. Lim, H. Pirsiavash, and A. Torralba. Parsing IKEA Ob- jects: Fine Pose Estimation. ICCV, 2013. 3\n\nMicrosoft COCO: Common objects in context. T Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, ECCV. T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft COCO: Com- mon objects in context. In ECCV, 2014. 3\n\nT Lin, M Maire, S J Belongie, L D Bourdev, R B Girshick, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, abs/1405.0312Microsoft COCO: common objects in context. CoRR. T. Lin, M. Maire, S. J. Belongie, L. D. Bourdev, R. B. Girshick, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. 3\n\nLearning transferable features with deep adaptation networks. M Long, J Wang, abs/1502.02791CoRR1M. Long and J. Wang. Learning transferable features with deep adaptation networks. CoRR, abs/1502.02791, 1:2, 2015. 4\n\nLearning multi-domain convolutional neural networks for visual tracking. H Nam, B Han, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionH. Nam and B. Han. Learning multi-domain convolutional neural networks for visual tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4293-4302, 2016. 3\n\nColumbia object image library (coil-20). S A Nene, S K Nayar, H Murase, 1S. A. Nene, S. K. Nayar, H. Murase, et al. Columbia object image library (coil-20). 1996. 1, 2\n\nReading digits in natural images with unsupervised feature learning. Y Netzer, T Wang, A Coates, A Bissacco, B Wu, A Y Ng, NIPS workshop on deep learning and unsupervised feature learning. 2011Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised fea- ture learning. In NIPS workshop on deep learning and unsu- pervised feature learning, volume 2011, page 5, 2011. 2\n\nLearning deep object detectors from 3d models. X Peng, B Sun, K Ali, K Saenko, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionX. Peng, B. Sun, K. Ali, and K. Saenko. Learning deep object detectors from 3d models. In Proceedings of the IEEE International Conference on Computer Vision, pages 1278- 1286, 2015. 2\n\nDataset Shift in Machine Learning. J Quionero-Candela, M Sugiyama, A Schwaighofer, N D Lawrence, The MIT PressJ. Quionero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. 1\n\nYoutube-boundingboxes: A large high-precision humanannotated data set for object detection in video. E Real, J Shlens, S Mazzocchi, X Pan, V Vanhoucke, abs/1702.00824CoRRE. Real, J. Shlens, S. Mazzocchi, X. Pan, and V. Vanhoucke. Youtube-boundingboxes: A large high-precision human- annotated data set for object detection in video. CoRR, abs/1702.00824, 2017. 3\n\nYoutube-boundingboxes: A large high-precision human-annotated data set for object detection in video. E Real, J Shlens, S Mazzocchi, X Pan, V Vanhoucke, arXiv:1702.00824arXiv preprintE. Real, J. Shlens, S. Mazzocchi, X. Pan, and V. Van- houcke. Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video. arXiv preprint arXiv:1702.00824, 2017. 4\n\nPlaying for data: Ground truth from computer games. S R Richter, V Vineet, S Roth, V Koltun, European Conference on Computer Vision. SpringerS. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision, pages 102-118. Springer, 2016. 2\n\nPlaying for data: Ground truth from computer games. S R Richter, V Vineet, S Roth, V Koltun, European Conference on Computer Vision (ECCV). B. Leibe, J. Matas, N. Sebe, and M. WellingSpringer International Publishing99063S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In B. Leibe, J. Matas, N. Sebe, and M. Welling, editors, European Con- ference on Computer Vision (ECCV), volume 9906 of LNCS, pages 102-118. Springer International Publishing, 2016. 2, 3\n\nThe SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes. G Ros, L Sellart, J Materzynska, D Vazquez, A Lopez, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern Recognition23G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. Lopez. The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In Proc. IEEE Conf. Computer Vision and Pattern Recogni- tion, 2016. 2, 3\n\nImageNet Large Scale Visual Recognition Challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, International Journal of Computer Vision (IJCV). 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015. 4\n\nAdapting visual category models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, Computer Vision-ECCV 2010. Springer1K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi- sual category models to new domains. In Computer Vision- ECCV 2010, pages 213-226. Springer, 2010. 1, 2\n\nAdapting visual category models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, Computer Vision-ECCV 2010. SpringerK. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi- sual category models to new domains. In Computer Vision- ECCV 2010, pages 213-226. Springer, 2010. 2\n\nEquivalence of distance-based and rkhs-based statistics in hypothesis testing. D Sejdinovic, B Sriperumbudur, A Gretton, K Fukumizu, The Annals of Statistics. 4D. Sejdinovic, B. Sriperumbudur, A. Gretton, and K. Fuku- mizu. Equivalence of distance-based and rkhs-based statis- tics in hypothesis testing. The Annals of Statistics, pages 2263-2291, 2013. 4\n\nThe cmu pose, illumination, and expression (pie) database. T Sim, S Baker, M Bsat, Proceedings. Fifth IEEE International Conference on. Fifth IEEE International Conference onAutomatic Face and Gesture RecognitionT. Sim, S. Baker, and M. Bsat. The cmu pose, illumina- tion, and expression (pie) database. In Automatic Face and Gesture Recognition, 2002. Proceedings. Fifth IEEE Inter- national Conference on, pages 53-58. IEEE, 2002. 2\n\nRender for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. H Su, C R Qi, Y Li, L J Guibas, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionH. Su, C. R. Qi, Y. Li, and L. J. Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with ren- dered 3d model views. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 2686-2694, 2015. 2\n\nB Sun, J Feng, K Saenko, arXiv:1511.05547Return of frustratingly easy domain adaptation. arXiv preprintB. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. arXiv preprint arXiv:1511.05547, 2015. 4\n\nFrom virtual to reality: Fast adaptation of virtual object detectors to real domains. B Sun, K Saenko, BMVC. 1B. Sun and K. Saenko. From virtual to reality: Fast adap- tation of virtual object detectors to real domains. In BMVC, volume 1, page 3, 2014. 2\n\nDeep CORAL: correlation alignment for deep domain adaptation. B Sun, K Saenko, abs/1607.01719CoRRB. Sun and K. Saenko. Deep CORAL: correlation alignment for deep domain adaptation. CoRR, abs/1607.01719, 2016. 4\n\nA large-scale shape benchmark for 3d object retrieval: Toyohashi shape benchmark. A Tatsuma, H Koyanagi, M Aono, Signal & Information Processing Association Annual Summit and Conference. APSIPA ASC2012A. Tatsuma, H. Koyanagi, and M. Aono. A large-scale shape benchmark for 3d object retrieval: Toyohashi shape benchmark. In Signal & Information Processing Associa- tion Annual Summit and Conference (APSIPA ASC), 2012\n\n. Asia-Pacific , IEEEAsia-Pacific, pages 1-10. IEEE, 2012. 3\n\nA Testbed for Cross-Dataset Analysis. T Tommasi, T Tuytelaars, Springer International PublishingChamT. Tommasi and T. Tuytelaars. A Testbed for Cross-Dataset Analysis, pages 18-31. Springer International Publishing, Cham, 2015. 2\n\nA testbed for cross-dataset analysis. Computer Vision -ECCV 2014 Workshops. T Tommasi, T Tuytelaars, B Caputo, 1T. Tommasi, T. Tuytelaars, and B. Caputo. A testbed for cross-dataset analysis. Computer Vision -ECCV 2014 Work- shops. ECCV 2014, 2014. 1, 2\n\nE Tzeng, C Devin, J Hoffman, C Finn, X Peng, S Levine, K Saenko, T Darrell, arXiv:1511.07111Towards adapting deep visuomotor representations from simulated to real environments. arXiv preprintE. Tzeng, C. Devin, J. Hoffman, C. Finn, X. Peng, S. Levine, K. Saenko, and T. Darrell. Towards adapting deep visuo- motor representations from simulated to real environments. arXiv preprint arXiv:1511.07111, 2015. 2\n\nSimultaneous deep transfer across domains and tasks. E Tzeng, J Hoffman, T Darrell, K Saenko, De- cember 2015. 2The IEEE International Conference on Computer Vision (ICCV). E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simulta- neous deep transfer across domains and tasks. In The IEEE International Conference on Computer Vision (ICCV), De- cember 2015. 2\n\nShrec'10 track: Generic 3d warehouse. T P Vanamali, A Godil, H Dutagaci, T Furuya, Z Lian, R Ohbuchi, Proceedings of the 3rd Eurographics Conference on 3D Object Retrieval, 3DOR '10. the 3rd Eurographics Conference on 3D Object Retrieval, 3DOR '10Aire-la-Ville, Switzerland, SwitzerlandEurographics Association3T. P. Vanamali, A. Godil, H. Dutagaci, T. Furuya, Z. Lian, and R. Ohbuchi. Shrec'10 track: Generic 3d warehouse. In Proceedings of the 3rd Eurographics Conference on 3D Object Retrieval, 3DOR '10, pages 93-100, Aire-la-Ville, Switzerland, Switzerland, 2010. Eurographics Association. 3\n\nVirtual and real world adaptation for pedestrian detection. D Vazquez, A M Lopez, J Marin, D Ponsa, D Geronimo, IEEE transactions on pattern analysis and machine intelligence. 36D. Vazquez, A. M. Lopez, J. Marin, D. Ponsa, and D. Geron- imo. Virtual and real world adaptation for pedestrian detec- tion. IEEE transactions on pattern analysis and machine intelligence, 36(4):797-809, 2014. 2\n\nSemi-supervised domain adaptation for weakly labeled semantic video object segmentation. H Wang, T Raiko, L Lensu, T Wang, J Karhunen, Asian Conference on Computer Vision. SpringerH. Wang, T. Raiko, L. Lensu, T. Wang, and J. Karhunen. Semi-supervised domain adaptation for weakly labeled se- mantic video object segmentation. In Asian Conference on Computer Vision, pages 163-179. Springer, 2016. 3\n\n3d shapenets: A deep representation for volumetric shapes. Z Wu, S Song, A Khosla, F Yu, L Zhang, X Tang, J Xiao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionZ. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1912-1920, 2015. 3\n\nObjectnet3d: A large scale database for 3d object recognition. Y Xiang, W Kim, W Chen, J Ji, C Choy, H Su, R Mottaghi, L Guibas, S Savarese, European Conference on Computer Vision. Springer23Y. Xiang, W. Kim, W. Chen, J. Ji, C. Choy, H. Su, R. Mot- taghi, L. Guibas, and S. Savarese. Objectnet3d: A large scale database for 3d object recognition. In European Conference on Computer Vision, pages 160-176. Springer, 2016. 2, 3\n\nBeyond pascal: A benchmark for 3d object detection in the wild. Y Xiang, R Mottaghi, S Savarese, IEEE Winter Conference on Applications of Computer Vision (WACV). Y. Xiang, R. Mottaghi, and S. Savarese. Beyond pascal: A benchmark for 3d object detection in the wild. In IEEE Win- ter Conference on Applications of Computer Vision (WACV), 2014. 3\n", "annotations": {"author": "[{\"end\":114,\"start\":49},{\"end\":176,\"start\":115},{\"end\":242,\"start\":177},{\"end\":297,\"start\":243},{\"end\":376,\"start\":298},{\"end\":454,\"start\":377}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":58},{\"end\":124,\"start\":119},{\"end\":190,\"start\":183},{\"end\":255,\"start\":248},{\"end\":309,\"start\":305},{\"end\":388,\"start\":382}]", "author_first_name": "[{\"end\":57,\"start\":49},{\"end\":118,\"start\":115},{\"end\":182,\"start\":177},{\"end\":247,\"start\":243},{\"end\":304,\"start\":298},{\"end\":381,\"start\":377}]", "author_affiliation": "[{\"end\":113,\"start\":64},{\"end\":175,\"start\":126},{\"end\":241,\"start\":192},{\"end\":296,\"start\":257},{\"end\":375,\"start\":336},{\"end\":453,\"start\":404}]", "title": "[{\"end\":46,\"start\":1},{\"end\":500,\"start\":455}]", "venue": null, "abstract": "[{\"end\":1694,\"start\":502}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2008,\"start\":2004},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2250,\"start\":2246},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2634,\"start\":2630},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2637,\"start\":2634},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2639,\"start\":2637},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2642,\"start\":2639},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4637,\"start\":4633},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4639,\"start\":4637},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4642,\"start\":4639},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4693,\"start\":4689},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4696,\"start\":4693},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4815,\"start\":4811},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5323,\"start\":5319},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5355,\"start\":5351},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5397,\"start\":5393},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5543,\"start\":5539},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6226,\"start\":6222},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6654,\"start\":6650},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6723,\"start\":6719},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7223,\"start\":7219},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7259,\"start\":7255},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7292,\"start\":7288},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7326,\"start\":7322},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7355,\"start\":7351},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7400,\"start\":7396},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7428,\"start\":7424},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7458,\"start\":7455},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7492,\"start\":7488},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7527,\"start\":7523},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7629,\"start\":7625},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7661,\"start\":7657},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7682,\"start\":7678},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7725,\"start\":7722},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7948,\"start\":7944},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8052,\"start\":8048},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8055,\"start\":8052},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8077,\"start\":8073},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8102,\"start\":8098},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8130,\"start\":8126},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8252,\"start\":8248},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8292,\"start\":8289},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9228,\"start\":9224},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9256,\"start\":9252},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9284,\"start\":9280},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9314,\"start\":9311},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9342,\"start\":9339},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9368,\"start\":9365},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9391,\"start\":9388},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9412,\"start\":9408},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9970,\"start\":9966},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9990,\"start\":9986},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10211,\"start\":10207},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10234,\"start\":10230},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10436,\"start\":10432},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10439,\"start\":10436},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10595,\"start\":10592},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11373,\"start\":11370},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11385,\"start\":11382},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11402,\"start\":11398},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11443,\"start\":11439},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13232,\"start\":13228},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14623,\"start\":14619},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15066,\"start\":15062},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15154,\"start\":15150},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15290,\"start\":15286},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15433,\"start\":15429},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17658,\"start\":17654},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19580,\"start\":19576},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19861,\"start\":19857},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20152,\"start\":20148},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21305,\"start\":21301},{\"end\":21802,\"start\":21799},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25630,\"start\":25626}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22767,\"start\":22437},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22961,\"start\":22768},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23058,\"start\":22962},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23148,\"start\":23059},{\"attributes\":{\"id\":\"fig_4\"},\"end\":23251,\"start\":23149},{\"attributes\":{\"id\":\"fig_5\"},\"end\":23357,\"start\":23252},{\"attributes\":{\"id\":\"fig_6\"},\"end\":23452,\"start\":23358},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":23549,\"start\":23453},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":24966,\"start\":23550},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":25539,\"start\":24967},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":26585,\"start\":25540},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":27195,\"start\":26586},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":27751,\"start\":27196}]", "paragraph": "[{\"end\":2326,\"start\":1710},{\"end\":3322,\"start\":2328},{\"end\":3930,\"start\":3324},{\"end\":4503,\"start\":3932},{\"end\":4816,\"start\":4520},{\"end\":5844,\"start\":4818},{\"end\":6691,\"start\":5846},{\"end\":7149,\"start\":6693},{\"end\":7551,\"start\":7182},{\"end\":8395,\"start\":7585},{\"end\":9163,\"start\":8397},{\"end\":9594,\"start\":9193},{\"end\":10596,\"start\":9596},{\"end\":10868,\"start\":10632},{\"end\":13095,\"start\":10926},{\"end\":13784,\"start\":13164},{\"end\":14390,\"start\":13807},{\"end\":14921,\"start\":14392},{\"end\":15728,\"start\":14954},{\"end\":16661,\"start\":15749},{\"end\":17029,\"start\":16696},{\"end\":17165,\"start\":17031},{\"end\":17417,\"start\":17167},{\"end\":17525,\"start\":17419},{\"end\":17737,\"start\":17549},{\"end\":18260,\"start\":17773},{\"end\":18819,\"start\":18299},{\"end\":19354,\"start\":18859},{\"end\":20067,\"start\":19387},{\"end\":20595,\"start\":20088},{\"end\":21651,\"start\":20610},{\"end\":22436,\"start\":21653}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10870,\"start\":10869}]", "table_ref": "[{\"end\":8301,\"start\":8294},{\"end\":9453,\"start\":9446},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11117,\"start\":11110},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12117,\"start\":12110},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":13094,\"start\":13087},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":13747,\"start\":13740},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":15834,\"start\":15827},{\"end\":20111,\"start\":20104},{\"end\":21208,\"start\":21201}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1708,\"start\":1696},{\"attributes\":{\"n\":\"2\"},\"end\":4518,\"start\":4506},{\"end\":7180,\"start\":7152},{\"end\":7583,\"start\":7554},{\"end\":9191,\"start\":9166},{\"attributes\":{\"n\":\"3\"},\"end\":10630,\"start\":10599},{\"attributes\":{\"n\":\"3.1\"},\"end\":10891,\"start\":10872},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":10924,\"start\":10894},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":13121,\"start\":13098},{\"attributes\":{\"n\":\"3.1.3\"},\"end\":13162,\"start\":13124},{\"attributes\":{\"n\":\"3.2\"},\"end\":13805,\"start\":13787},{\"attributes\":{\"n\":\"3.3\"},\"end\":14952,\"start\":14924},{\"attributes\":{\"n\":\"3.4\"},\"end\":15747,\"start\":15731},{\"attributes\":{\"n\":\"4\"},\"end\":16694,\"start\":16664},{\"attributes\":{\"n\":\"4.1\"},\"end\":17547,\"start\":17528},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":17771,\"start\":17740},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":18297,\"start\":18263},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":18857,\"start\":18822},{\"attributes\":{\"n\":\"4.2\"},\"end\":19385,\"start\":19357},{\"attributes\":{\"n\":\"4.3\"},\"end\":20086,\"start\":20070},{\"attributes\":{\"n\":\"5\"},\"end\":20608,\"start\":20598},{\"end\":22448,\"start\":22438},{\"end\":22787,\"start\":22769},{\"end\":22973,\"start\":22963},{\"end\":23070,\"start\":23060},{\"end\":23160,\"start\":23150},{\"end\":23263,\"start\":23253},{\"end\":23369,\"start\":23359},{\"end\":23463,\"start\":23454},{\"end\":24977,\"start\":24968},{\"end\":26596,\"start\":26587}]", "table": "[{\"end\":24966,\"start\":24437},{\"end\":25539,\"start\":25225},{\"end\":26585,\"start\":25654},{\"end\":27195,\"start\":26944},{\"end\":27751,\"start\":27314}]", "figure_caption": "[{\"end\":22767,\"start\":22450},{\"end\":22961,\"start\":22794},{\"end\":23058,\"start\":22975},{\"end\":23148,\"start\":23072},{\"end\":23251,\"start\":23162},{\"end\":23357,\"start\":23265},{\"end\":23452,\"start\":23371},{\"end\":23549,\"start\":23465},{\"end\":24437,\"start\":23552},{\"end\":25225,\"start\":24979},{\"end\":25654,\"start\":25542},{\"end\":26944,\"start\":26598},{\"end\":27314,\"start\":27198}]", "figure_ref": "[{\"end\":12131,\"start\":12123},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12631,\"start\":12623},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13783,\"start\":13775},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18227,\"start\":18219},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18784,\"start\":18776},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":19325,\"start\":19317}]", "bib_author_first_name": "[{\"end\":27918,\"start\":27917},{\"end\":27929,\"start\":27928},{\"end\":28291,\"start\":28290},{\"end\":28304,\"start\":28303},{\"end\":28315,\"start\":28314},{\"end\":28323,\"start\":28322},{\"end\":28325,\"start\":28324},{\"end\":28337,\"start\":28336},{\"end\":28729,\"start\":28728},{\"end\":28731,\"start\":28730},{\"end\":28743,\"start\":28742},{\"end\":28752,\"start\":28751},{\"end\":28762,\"start\":28761},{\"end\":28771,\"start\":28770},{\"end\":28773,\"start\":28772},{\"end\":29010,\"start\":29009},{\"end\":29012,\"start\":29011},{\"end\":29021,\"start\":29020},{\"end\":29035,\"start\":29034},{\"end\":29045,\"start\":29044},{\"end\":29057,\"start\":29056},{\"end\":29066,\"start\":29065},{\"end\":29072,\"start\":29071},{\"end\":29084,\"start\":29083},{\"end\":29093,\"start\":29092},{\"end\":29101,\"start\":29100},{\"end\":29437,\"start\":29436},{\"end\":29439,\"start\":29438},{\"end\":29448,\"start\":29447},{\"end\":29462,\"start\":29461},{\"end\":29472,\"start\":29471},{\"end\":29484,\"start\":29483},{\"end\":29493,\"start\":29492},{\"end\":29499,\"start\":29498},{\"end\":29511,\"start\":29510},{\"end\":29520,\"start\":29519},{\"end\":29528,\"start\":29527},{\"end\":29534,\"start\":29533},{\"end\":29542,\"start\":29541},{\"end\":29548,\"start\":29547},{\"end\":30055,\"start\":30051},{\"end\":30066,\"start\":30062},{\"end\":30077,\"start\":30073},{\"end\":30085,\"start\":30084},{\"end\":30417,\"start\":30416},{\"end\":30425,\"start\":30424},{\"end\":30434,\"start\":30433},{\"end\":30443,\"start\":30442},{\"end\":30445,\"start\":30444},{\"end\":30454,\"start\":30453},{\"end\":30462,\"start\":30461},{\"end\":30871,\"start\":30870},{\"end\":30882,\"start\":30878},{\"end\":30890,\"start\":30889},{\"end\":30900,\"start\":30899},{\"end\":31129,\"start\":31128},{\"end\":31139,\"start\":31138},{\"end\":31148,\"start\":31147},{\"end\":31157,\"start\":31156},{\"end\":31168,\"start\":31167},{\"end\":31181,\"start\":31180},{\"end\":31193,\"start\":31192},{\"end\":31203,\"start\":31202},{\"end\":31211,\"start\":31210},{\"end\":31707,\"start\":31706},{\"end\":31910,\"start\":31909},{\"end\":31918,\"start\":31917},{\"end\":31924,\"start\":31923},{\"end\":31933,\"start\":31932},{\"end\":32142,\"start\":32141},{\"end\":32152,\"start\":32151},{\"end\":32160,\"start\":32159},{\"end\":32169,\"start\":32168},{\"end\":32578,\"start\":32577},{\"end\":32587,\"start\":32586},{\"end\":32748,\"start\":32747},{\"end\":32759,\"start\":32758},{\"end\":32767,\"start\":32766},{\"end\":32773,\"start\":32772},{\"end\":33111,\"start\":33110},{\"end\":33113,\"start\":33112},{\"end\":33364,\"start\":33363},{\"end\":33378,\"start\":33377},{\"end\":33391,\"start\":33390},{\"end\":33393,\"start\":33392},{\"end\":33727,\"start\":33726},{\"end\":33736,\"start\":33735},{\"end\":33746,\"start\":33745},{\"end\":33756,\"start\":33755},{\"end\":33979,\"start\":33978},{\"end\":33981,\"start\":33980},{\"end\":33988,\"start\":33987},{\"end\":34002,\"start\":34001},{\"end\":34161,\"start\":34160},{\"end\":34168,\"start\":34167},{\"end\":34177,\"start\":34176},{\"end\":34189,\"start\":34188},{\"end\":34197,\"start\":34196},{\"end\":34207,\"start\":34206},{\"end\":34218,\"start\":34217},{\"end\":34228,\"start\":34227},{\"end\":34230,\"start\":34229},{\"end\":34405,\"start\":34404},{\"end\":34412,\"start\":34411},{\"end\":34421,\"start\":34420},{\"end\":34423,\"start\":34422},{\"end\":34435,\"start\":34434},{\"end\":34437,\"start\":34436},{\"end\":34448,\"start\":34447},{\"end\":34450,\"start\":34449},{\"end\":34462,\"start\":34461},{\"end\":34470,\"start\":34469},{\"end\":34480,\"start\":34479},{\"end\":34491,\"start\":34490},{\"end\":34501,\"start\":34500},{\"end\":34503,\"start\":34502},{\"end\":34838,\"start\":34837},{\"end\":34846,\"start\":34845},{\"end\":35065,\"start\":35064},{\"end\":35072,\"start\":35071},{\"end\":35463,\"start\":35462},{\"end\":35465,\"start\":35464},{\"end\":35473,\"start\":35472},{\"end\":35475,\"start\":35474},{\"end\":35484,\"start\":35483},{\"end\":35660,\"start\":35659},{\"end\":35670,\"start\":35669},{\"end\":35678,\"start\":35677},{\"end\":35688,\"start\":35687},{\"end\":35700,\"start\":35699},{\"end\":35706,\"start\":35705},{\"end\":35708,\"start\":35707},{\"end\":36068,\"start\":36067},{\"end\":36076,\"start\":36075},{\"end\":36083,\"start\":36082},{\"end\":36090,\"start\":36089},{\"end\":36442,\"start\":36441},{\"end\":36462,\"start\":36461},{\"end\":36474,\"start\":36473},{\"end\":36490,\"start\":36489},{\"end\":36492,\"start\":36491},{\"end\":36748,\"start\":36747},{\"end\":36756,\"start\":36755},{\"end\":36766,\"start\":36765},{\"end\":36779,\"start\":36778},{\"end\":36786,\"start\":36785},{\"end\":37113,\"start\":37112},{\"end\":37121,\"start\":37120},{\"end\":37131,\"start\":37130},{\"end\":37144,\"start\":37143},{\"end\":37151,\"start\":37150},{\"end\":37452,\"start\":37451},{\"end\":37454,\"start\":37453},{\"end\":37465,\"start\":37464},{\"end\":37475,\"start\":37474},{\"end\":37483,\"start\":37482},{\"end\":37772,\"start\":37771},{\"end\":37774,\"start\":37773},{\"end\":37785,\"start\":37784},{\"end\":37795,\"start\":37794},{\"end\":37803,\"start\":37802},{\"end\":38335,\"start\":38334},{\"end\":38342,\"start\":38341},{\"end\":38353,\"start\":38352},{\"end\":38368,\"start\":38367},{\"end\":38379,\"start\":38378},{\"end\":38789,\"start\":38788},{\"end\":38804,\"start\":38803},{\"end\":38812,\"start\":38811},{\"end\":38818,\"start\":38817},{\"end\":38828,\"start\":38827},{\"end\":38840,\"start\":38839},{\"end\":38846,\"start\":38845},{\"end\":38855,\"start\":38854},{\"end\":38867,\"start\":38866},{\"end\":38877,\"start\":38876},{\"end\":38890,\"start\":38889},{\"end\":38892,\"start\":38891},{\"end\":38900,\"start\":38899},{\"end\":39276,\"start\":39275},{\"end\":39286,\"start\":39285},{\"end\":39295,\"start\":39294},{\"end\":39304,\"start\":39303},{\"end\":39564,\"start\":39563},{\"end\":39574,\"start\":39573},{\"end\":39583,\"start\":39582},{\"end\":39592,\"start\":39591},{\"end\":39879,\"start\":39878},{\"end\":39893,\"start\":39892},{\"end\":39910,\"start\":39909},{\"end\":39921,\"start\":39920},{\"end\":40216,\"start\":40215},{\"end\":40223,\"start\":40222},{\"end\":40232,\"start\":40231},{\"end\":40689,\"start\":40688},{\"end\":40695,\"start\":40694},{\"end\":40697,\"start\":40696},{\"end\":40703,\"start\":40702},{\"end\":40709,\"start\":40708},{\"end\":40711,\"start\":40710},{\"end\":41082,\"start\":41081},{\"end\":41089,\"start\":41088},{\"end\":41097,\"start\":41096},{\"end\":41393,\"start\":41392},{\"end\":41400,\"start\":41399},{\"end\":41625,\"start\":41624},{\"end\":41632,\"start\":41631},{\"end\":41857,\"start\":41856},{\"end\":41868,\"start\":41867},{\"end\":41880,\"start\":41879},{\"end\":42207,\"start\":42195},{\"end\":42294,\"start\":42293},{\"end\":42305,\"start\":42304},{\"end\":42563,\"start\":42562},{\"end\":42574,\"start\":42573},{\"end\":42588,\"start\":42587},{\"end\":42742,\"start\":42741},{\"end\":42751,\"start\":42750},{\"end\":42760,\"start\":42759},{\"end\":42771,\"start\":42770},{\"end\":42779,\"start\":42778},{\"end\":42787,\"start\":42786},{\"end\":42797,\"start\":42796},{\"end\":42807,\"start\":42806},{\"end\":43205,\"start\":43204},{\"end\":43214,\"start\":43213},{\"end\":43225,\"start\":43224},{\"end\":43236,\"start\":43235},{\"end\":43551,\"start\":43550},{\"end\":43553,\"start\":43552},{\"end\":43565,\"start\":43564},{\"end\":43574,\"start\":43573},{\"end\":43586,\"start\":43585},{\"end\":43596,\"start\":43595},{\"end\":43604,\"start\":43603},{\"end\":44171,\"start\":44170},{\"end\":44182,\"start\":44181},{\"end\":44184,\"start\":44183},{\"end\":44193,\"start\":44192},{\"end\":44202,\"start\":44201},{\"end\":44211,\"start\":44210},{\"end\":44592,\"start\":44591},{\"end\":44600,\"start\":44599},{\"end\":44609,\"start\":44608},{\"end\":44618,\"start\":44617},{\"end\":44626,\"start\":44625},{\"end\":44962,\"start\":44961},{\"end\":44968,\"start\":44967},{\"end\":44976,\"start\":44975},{\"end\":44986,\"start\":44985},{\"end\":44992,\"start\":44991},{\"end\":45001,\"start\":45000},{\"end\":45009,\"start\":45008},{\"end\":45454,\"start\":45453},{\"end\":45463,\"start\":45462},{\"end\":45470,\"start\":45469},{\"end\":45478,\"start\":45477},{\"end\":45484,\"start\":45483},{\"end\":45492,\"start\":45491},{\"end\":45498,\"start\":45497},{\"end\":45510,\"start\":45509},{\"end\":45520,\"start\":45519},{\"end\":45882,\"start\":45881},{\"end\":45891,\"start\":45890},{\"end\":45903,\"start\":45902}]", "bib_author_last_name": "[{\"end\":27926,\"start\":27919},{\"end\":27939,\"start\":27930},{\"end\":28301,\"start\":28292},{\"end\":28312,\"start\":28305},{\"end\":28320,\"start\":28316},{\"end\":28334,\"start\":28326},{\"end\":28347,\"start\":28338},{\"end\":28740,\"start\":28732},{\"end\":28749,\"start\":28744},{\"end\":28759,\"start\":28753},{\"end\":28768,\"start\":28763},{\"end\":28778,\"start\":28774},{\"end\":29018,\"start\":29013},{\"end\":29032,\"start\":29022},{\"end\":29042,\"start\":29036},{\"end\":29054,\"start\":29046},{\"end\":29063,\"start\":29058},{\"end\":29069,\"start\":29067},{\"end\":29081,\"start\":29073},{\"end\":29090,\"start\":29085},{\"end\":29098,\"start\":29094},{\"end\":29104,\"start\":29102},{\"end\":29445,\"start\":29440},{\"end\":29459,\"start\":29449},{\"end\":29469,\"start\":29463},{\"end\":29481,\"start\":29473},{\"end\":29490,\"start\":29485},{\"end\":29496,\"start\":29494},{\"end\":29508,\"start\":29500},{\"end\":29517,\"start\":29512},{\"end\":29525,\"start\":29521},{\"end\":29531,\"start\":29529},{\"end\":29539,\"start\":29535},{\"end\":29545,\"start\":29543},{\"end\":29551,\"start\":29549},{\"end\":30060,\"start\":30056},{\"end\":30071,\"start\":30067},{\"end\":30082,\"start\":30078},{\"end\":30094,\"start\":30086},{\"end\":30422,\"start\":30418},{\"end\":30431,\"start\":30426},{\"end\":30440,\"start\":30435},{\"end\":30451,\"start\":30446},{\"end\":30459,\"start\":30455},{\"end\":30466,\"start\":30463},{\"end\":30876,\"start\":30872},{\"end\":30887,\"start\":30883},{\"end\":30897,\"start\":30891},{\"end\":30907,\"start\":30901},{\"end\":31136,\"start\":31130},{\"end\":31145,\"start\":31140},{\"end\":31154,\"start\":31149},{\"end\":31165,\"start\":31158},{\"end\":31178,\"start\":31169},{\"end\":31190,\"start\":31182},{\"end\":31200,\"start\":31194},{\"end\":31208,\"start\":31204},{\"end\":31219,\"start\":31212},{\"end\":31714,\"start\":31708},{\"end\":31915,\"start\":31911},{\"end\":31921,\"start\":31919},{\"end\":31930,\"start\":31925},{\"end\":31937,\"start\":31934},{\"end\":32149,\"start\":32143},{\"end\":32157,\"start\":32153},{\"end\":32166,\"start\":32161},{\"end\":32173,\"start\":32170},{\"end\":32584,\"start\":32579},{\"end\":32597,\"start\":32588},{\"end\":32756,\"start\":32749},{\"end\":32764,\"start\":32760},{\"end\":32770,\"start\":32768},{\"end\":32781,\"start\":32774},{\"end\":33118,\"start\":33114},{\"end\":33375,\"start\":33365},{\"end\":33388,\"start\":33379},{\"end\":33400,\"start\":33394},{\"end\":33733,\"start\":33728},{\"end\":33743,\"start\":33737},{\"end\":33753,\"start\":33747},{\"end\":33764,\"start\":33757},{\"end\":33985,\"start\":33982},{\"end\":33999,\"start\":33989},{\"end\":34011,\"start\":34003},{\"end\":34165,\"start\":34162},{\"end\":34174,\"start\":34169},{\"end\":34186,\"start\":34178},{\"end\":34194,\"start\":34190},{\"end\":34204,\"start\":34198},{\"end\":34215,\"start\":34208},{\"end\":34225,\"start\":34219},{\"end\":34238,\"start\":34231},{\"end\":34409,\"start\":34406},{\"end\":34418,\"start\":34413},{\"end\":34432,\"start\":34424},{\"end\":34445,\"start\":34438},{\"end\":34459,\"start\":34451},{\"end\":34467,\"start\":34463},{\"end\":34477,\"start\":34471},{\"end\":34488,\"start\":34481},{\"end\":34498,\"start\":34492},{\"end\":34511,\"start\":34504},{\"end\":34843,\"start\":34839},{\"end\":34851,\"start\":34847},{\"end\":35069,\"start\":35066},{\"end\":35076,\"start\":35073},{\"end\":35470,\"start\":35466},{\"end\":35481,\"start\":35476},{\"end\":35491,\"start\":35485},{\"end\":35667,\"start\":35661},{\"end\":35675,\"start\":35671},{\"end\":35685,\"start\":35679},{\"end\":35697,\"start\":35689},{\"end\":35703,\"start\":35701},{\"end\":35711,\"start\":35709},{\"end\":36073,\"start\":36069},{\"end\":36080,\"start\":36077},{\"end\":36087,\"start\":36084},{\"end\":36097,\"start\":36091},{\"end\":36459,\"start\":36443},{\"end\":36471,\"start\":36463},{\"end\":36487,\"start\":36475},{\"end\":36501,\"start\":36493},{\"end\":36753,\"start\":36749},{\"end\":36763,\"start\":36757},{\"end\":36776,\"start\":36767},{\"end\":36783,\"start\":36780},{\"end\":36796,\"start\":36787},{\"end\":37118,\"start\":37114},{\"end\":37128,\"start\":37122},{\"end\":37141,\"start\":37132},{\"end\":37148,\"start\":37145},{\"end\":37161,\"start\":37152},{\"end\":37462,\"start\":37455},{\"end\":37472,\"start\":37466},{\"end\":37480,\"start\":37476},{\"end\":37490,\"start\":37484},{\"end\":37782,\"start\":37775},{\"end\":37792,\"start\":37786},{\"end\":37800,\"start\":37796},{\"end\":37810,\"start\":37804},{\"end\":38339,\"start\":38336},{\"end\":38350,\"start\":38343},{\"end\":38365,\"start\":38354},{\"end\":38376,\"start\":38369},{\"end\":38385,\"start\":38380},{\"end\":38801,\"start\":38790},{\"end\":38809,\"start\":38805},{\"end\":38815,\"start\":38813},{\"end\":38825,\"start\":38819},{\"end\":38837,\"start\":38829},{\"end\":38843,\"start\":38841},{\"end\":38852,\"start\":38847},{\"end\":38864,\"start\":38856},{\"end\":38874,\"start\":38868},{\"end\":38887,\"start\":38878},{\"end\":38897,\"start\":38893},{\"end\":38908,\"start\":38901},{\"end\":39283,\"start\":39277},{\"end\":39292,\"start\":39287},{\"end\":39301,\"start\":39296},{\"end\":39312,\"start\":39305},{\"end\":39571,\"start\":39565},{\"end\":39580,\"start\":39575},{\"end\":39589,\"start\":39584},{\"end\":39600,\"start\":39593},{\"end\":39890,\"start\":39880},{\"end\":39907,\"start\":39894},{\"end\":39918,\"start\":39911},{\"end\":39930,\"start\":39922},{\"end\":40220,\"start\":40217},{\"end\":40229,\"start\":40224},{\"end\":40237,\"start\":40233},{\"end\":40692,\"start\":40690},{\"end\":40700,\"start\":40698},{\"end\":40706,\"start\":40704},{\"end\":40718,\"start\":40712},{\"end\":41086,\"start\":41083},{\"end\":41094,\"start\":41090},{\"end\":41104,\"start\":41098},{\"end\":41397,\"start\":41394},{\"end\":41407,\"start\":41401},{\"end\":41629,\"start\":41626},{\"end\":41639,\"start\":41633},{\"end\":41865,\"start\":41858},{\"end\":41877,\"start\":41869},{\"end\":41885,\"start\":41881},{\"end\":42302,\"start\":42295},{\"end\":42316,\"start\":42306},{\"end\":42571,\"start\":42564},{\"end\":42585,\"start\":42575},{\"end\":42595,\"start\":42589},{\"end\":42748,\"start\":42743},{\"end\":42757,\"start\":42752},{\"end\":42768,\"start\":42761},{\"end\":42776,\"start\":42772},{\"end\":42784,\"start\":42780},{\"end\":42794,\"start\":42788},{\"end\":42804,\"start\":42798},{\"end\":42815,\"start\":42808},{\"end\":43211,\"start\":43206},{\"end\":43222,\"start\":43215},{\"end\":43233,\"start\":43226},{\"end\":43243,\"start\":43237},{\"end\":43562,\"start\":43554},{\"end\":43571,\"start\":43566},{\"end\":43583,\"start\":43575},{\"end\":43593,\"start\":43587},{\"end\":43601,\"start\":43597},{\"end\":43612,\"start\":43605},{\"end\":44179,\"start\":44172},{\"end\":44190,\"start\":44185},{\"end\":44199,\"start\":44194},{\"end\":44208,\"start\":44203},{\"end\":44220,\"start\":44212},{\"end\":44597,\"start\":44593},{\"end\":44606,\"start\":44601},{\"end\":44615,\"start\":44610},{\"end\":44623,\"start\":44619},{\"end\":44635,\"start\":44627},{\"end\":44965,\"start\":44963},{\"end\":44973,\"start\":44969},{\"end\":44983,\"start\":44977},{\"end\":44989,\"start\":44987},{\"end\":44998,\"start\":44993},{\"end\":45006,\"start\":45002},{\"end\":45014,\"start\":45010},{\"end\":45460,\"start\":45455},{\"end\":45467,\"start\":45464},{\"end\":45475,\"start\":45471},{\"end\":45481,\"start\":45479},{\"end\":45489,\"start\":45485},{\"end\":45495,\"start\":45493},{\"end\":45507,\"start\":45499},{\"end\":45517,\"start\":45511},{\"end\":45529,\"start\":45521},{\"end\":45888,\"start\":45883},{\"end\":45900,\"start\":45892},{\"end\":45912,\"start\":45904}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14627863},\"end\":28180,\"start\":27816},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15771800},\"end\":28726,\"start\":28182},{\"attributes\":{\"doi\":\"arXiv:1704.08082\",\"id\":\"b2\"},\"end\":29007,\"start\":28728},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b3\"},\"end\":29383,\"start\":29009},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b4\"},\"end\":30002,\"start\":29385},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":395534},\"end\":30326,\"start\":30004},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7530403},\"end\":30868,\"start\":30328},{\"attributes\":{\"doi\":\"arXiv:1602.02481\",\"id\":\"b7\"},\"end\":31063,\"start\":30870},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":502946},\"end\":31637,\"start\":31065},{\"attributes\":{\"doi\":\"abs/1702.05374\",\"id\":\"b9\"},\"end\":31845,\"start\":31639},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1283257},\"end\":32079,\"start\":31847},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1203247},\"end\":32524,\"start\":32081},{\"attributes\":{\"doi\":\"arXiv:1409.7495\",\"id\":\"b12\"},\"end\":32745,\"start\":32526},{\"attributes\":{\"doi\":\"arXiv:1612.02649\",\"id\":\"b13\"},\"end\":33054,\"start\":32747},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":8148915},\"end\":33296,\"start\":33056},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195908774},\"end\":33643,\"start\":33298},{\"attributes\":{\"id\":\"b16\"},\"end\":33926,\"start\":33645},{\"attributes\":{\"id\":\"b17\"},\"end\":34115,\"start\":33928},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14113767},\"end\":34402,\"start\":34117},{\"attributes\":{\"doi\":\"abs/1405.0312\",\"id\":\"b19\"},\"end\":34773,\"start\":34404},{\"attributes\":{\"doi\":\"abs/1502.02791\",\"id\":\"b20\"},\"end\":34989,\"start\":34775},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":973101},\"end\":35419,\"start\":34991},{\"attributes\":{\"id\":\"b22\"},\"end\":35588,\"start\":35421},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16852518},\"end\":36018,\"start\":35590},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":554423},\"end\":36404,\"start\":36020},{\"attributes\":{\"id\":\"b25\"},\"end\":36644,\"start\":36406},{\"attributes\":{\"doi\":\"abs/1702.00824\",\"id\":\"b26\"},\"end\":37008,\"start\":36646},{\"attributes\":{\"doi\":\"arXiv:1702.00824\",\"id\":\"b27\"},\"end\":37397,\"start\":37010},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5844139},\"end\":37717,\"start\":37399},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5844139},\"end\":38229,\"start\":37719},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":206594095},\"end\":38735,\"start\":38231},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2930547},\"end\":39225,\"start\":38737},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7534823},\"end\":39513,\"start\":39227},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":7534823},\"end\":39797,\"start\":39515},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8308769},\"end\":40154,\"start\":39799},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2091854},\"end\":40590,\"start\":40156},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5544227},\"end\":41079,\"start\":40592},{\"attributes\":{\"doi\":\"arXiv:1511.05547\",\"id\":\"b37\"},\"end\":41304,\"start\":41081},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6018652},\"end\":41560,\"start\":41306},{\"attributes\":{\"doi\":\"abs/1607.01719\",\"id\":\"b39\"},\"end\":41772,\"start\":41562},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":1856237},\"end\":42191,\"start\":41774},{\"attributes\":{\"id\":\"b41\"},\"end\":42253,\"start\":42193},{\"attributes\":{\"id\":\"b42\"},\"end\":42484,\"start\":42255},{\"attributes\":{\"id\":\"b43\"},\"end\":42739,\"start\":42486},{\"attributes\":{\"doi\":\"arXiv:1511.07111\",\"id\":\"b44\"},\"end\":43149,\"start\":42741},{\"attributes\":{\"doi\":\"De- cember 2015. 2\",\"id\":\"b45\",\"matched_paper_id\":2655115},\"end\":43510,\"start\":43151},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":765844},\"end\":44108,\"start\":43512},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1496418},\"end\":44500,\"start\":44110},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":16665983},\"end\":44900,\"start\":44502},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":206592833},\"end\":45388,\"start\":44902},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":10917359},\"end\":45815,\"start\":45390},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":11266650},\"end\":46162,\"start\":45817}]", "bib_title": "[{\"end\":27915,\"start\":27816},{\"end\":28288,\"start\":28182},{\"end\":30049,\"start\":30004},{\"end\":30414,\"start\":30328},{\"end\":31126,\"start\":31065},{\"end\":31907,\"start\":31847},{\"end\":32139,\"start\":32081},{\"end\":33108,\"start\":33056},{\"end\":33361,\"start\":33298},{\"end\":34158,\"start\":34117},{\"end\":35062,\"start\":34991},{\"end\":35657,\"start\":35590},{\"end\":36065,\"start\":36020},{\"end\":37449,\"start\":37399},{\"end\":37769,\"start\":37719},{\"end\":38332,\"start\":38231},{\"end\":38786,\"start\":38737},{\"end\":39273,\"start\":39227},{\"end\":39561,\"start\":39515},{\"end\":39876,\"start\":39799},{\"end\":40213,\"start\":40156},{\"end\":40686,\"start\":40592},{\"end\":41390,\"start\":41306},{\"end\":41854,\"start\":41774},{\"end\":43202,\"start\":43151},{\"end\":43548,\"start\":43512},{\"end\":44168,\"start\":44110},{\"end\":44589,\"start\":44502},{\"end\":44959,\"start\":44902},{\"end\":45451,\"start\":45390},{\"end\":45879,\"start\":45817}]", "bib_author": "[{\"end\":27928,\"start\":27917},{\"end\":27941,\"start\":27928},{\"end\":28303,\"start\":28290},{\"end\":28314,\"start\":28303},{\"end\":28322,\"start\":28314},{\"end\":28336,\"start\":28322},{\"end\":28349,\"start\":28336},{\"end\":28742,\"start\":28728},{\"end\":28751,\"start\":28742},{\"end\":28761,\"start\":28751},{\"end\":28770,\"start\":28761},{\"end\":28780,\"start\":28770},{\"end\":29020,\"start\":29009},{\"end\":29034,\"start\":29020},{\"end\":29044,\"start\":29034},{\"end\":29056,\"start\":29044},{\"end\":29065,\"start\":29056},{\"end\":29071,\"start\":29065},{\"end\":29083,\"start\":29071},{\"end\":29092,\"start\":29083},{\"end\":29100,\"start\":29092},{\"end\":29106,\"start\":29100},{\"end\":29447,\"start\":29436},{\"end\":29461,\"start\":29447},{\"end\":29471,\"start\":29461},{\"end\":29483,\"start\":29471},{\"end\":29492,\"start\":29483},{\"end\":29498,\"start\":29492},{\"end\":29510,\"start\":29498},{\"end\":29519,\"start\":29510},{\"end\":29527,\"start\":29519},{\"end\":29533,\"start\":29527},{\"end\":29541,\"start\":29533},{\"end\":29547,\"start\":29541},{\"end\":29553,\"start\":29547},{\"end\":30062,\"start\":30051},{\"end\":30073,\"start\":30062},{\"end\":30084,\"start\":30073},{\"end\":30096,\"start\":30084},{\"end\":30424,\"start\":30416},{\"end\":30433,\"start\":30424},{\"end\":30442,\"start\":30433},{\"end\":30453,\"start\":30442},{\"end\":30461,\"start\":30453},{\"end\":30468,\"start\":30461},{\"end\":30878,\"start\":30870},{\"end\":30889,\"start\":30878},{\"end\":30899,\"start\":30889},{\"end\":30909,\"start\":30899},{\"end\":31138,\"start\":31128},{\"end\":31147,\"start\":31138},{\"end\":31156,\"start\":31147},{\"end\":31167,\"start\":31156},{\"end\":31180,\"start\":31167},{\"end\":31192,\"start\":31180},{\"end\":31202,\"start\":31192},{\"end\":31210,\"start\":31202},{\"end\":31221,\"start\":31210},{\"end\":31716,\"start\":31706},{\"end\":31917,\"start\":31909},{\"end\":31923,\"start\":31917},{\"end\":31932,\"start\":31923},{\"end\":31939,\"start\":31932},{\"end\":32151,\"start\":32141},{\"end\":32159,\"start\":32151},{\"end\":32168,\"start\":32159},{\"end\":32175,\"start\":32168},{\"end\":32586,\"start\":32577},{\"end\":32599,\"start\":32586},{\"end\":32758,\"start\":32747},{\"end\":32766,\"start\":32758},{\"end\":32772,\"start\":32766},{\"end\":32783,\"start\":32772},{\"end\":33120,\"start\":33110},{\"end\":33377,\"start\":33363},{\"end\":33390,\"start\":33377},{\"end\":33402,\"start\":33390},{\"end\":33735,\"start\":33726},{\"end\":33745,\"start\":33735},{\"end\":33755,\"start\":33745},{\"end\":33766,\"start\":33755},{\"end\":33987,\"start\":33978},{\"end\":34001,\"start\":33987},{\"end\":34013,\"start\":34001},{\"end\":34167,\"start\":34160},{\"end\":34176,\"start\":34167},{\"end\":34188,\"start\":34176},{\"end\":34196,\"start\":34188},{\"end\":34206,\"start\":34196},{\"end\":34217,\"start\":34206},{\"end\":34227,\"start\":34217},{\"end\":34240,\"start\":34227},{\"end\":34411,\"start\":34404},{\"end\":34420,\"start\":34411},{\"end\":34434,\"start\":34420},{\"end\":34447,\"start\":34434},{\"end\":34461,\"start\":34447},{\"end\":34469,\"start\":34461},{\"end\":34479,\"start\":34469},{\"end\":34490,\"start\":34479},{\"end\":34500,\"start\":34490},{\"end\":34513,\"start\":34500},{\"end\":34845,\"start\":34837},{\"end\":34853,\"start\":34845},{\"end\":35071,\"start\":35064},{\"end\":35078,\"start\":35071},{\"end\":35472,\"start\":35462},{\"end\":35483,\"start\":35472},{\"end\":35493,\"start\":35483},{\"end\":35669,\"start\":35659},{\"end\":35677,\"start\":35669},{\"end\":35687,\"start\":35677},{\"end\":35699,\"start\":35687},{\"end\":35705,\"start\":35699},{\"end\":35713,\"start\":35705},{\"end\":36075,\"start\":36067},{\"end\":36082,\"start\":36075},{\"end\":36089,\"start\":36082},{\"end\":36099,\"start\":36089},{\"end\":36461,\"start\":36441},{\"end\":36473,\"start\":36461},{\"end\":36489,\"start\":36473},{\"end\":36503,\"start\":36489},{\"end\":36755,\"start\":36747},{\"end\":36765,\"start\":36755},{\"end\":36778,\"start\":36765},{\"end\":36785,\"start\":36778},{\"end\":36798,\"start\":36785},{\"end\":37120,\"start\":37112},{\"end\":37130,\"start\":37120},{\"end\":37143,\"start\":37130},{\"end\":37150,\"start\":37143},{\"end\":37163,\"start\":37150},{\"end\":37464,\"start\":37451},{\"end\":37474,\"start\":37464},{\"end\":37482,\"start\":37474},{\"end\":37492,\"start\":37482},{\"end\":37784,\"start\":37771},{\"end\":37794,\"start\":37784},{\"end\":37802,\"start\":37794},{\"end\":37812,\"start\":37802},{\"end\":38341,\"start\":38334},{\"end\":38352,\"start\":38341},{\"end\":38367,\"start\":38352},{\"end\":38378,\"start\":38367},{\"end\":38387,\"start\":38378},{\"end\":38803,\"start\":38788},{\"end\":38811,\"start\":38803},{\"end\":38817,\"start\":38811},{\"end\":38827,\"start\":38817},{\"end\":38839,\"start\":38827},{\"end\":38845,\"start\":38839},{\"end\":38854,\"start\":38845},{\"end\":38866,\"start\":38854},{\"end\":38876,\"start\":38866},{\"end\":38889,\"start\":38876},{\"end\":38899,\"start\":38889},{\"end\":38910,\"start\":38899},{\"end\":39285,\"start\":39275},{\"end\":39294,\"start\":39285},{\"end\":39303,\"start\":39294},{\"end\":39314,\"start\":39303},{\"end\":39573,\"start\":39563},{\"end\":39582,\"start\":39573},{\"end\":39591,\"start\":39582},{\"end\":39602,\"start\":39591},{\"end\":39892,\"start\":39878},{\"end\":39909,\"start\":39892},{\"end\":39920,\"start\":39909},{\"end\":39932,\"start\":39920},{\"end\":40222,\"start\":40215},{\"end\":40231,\"start\":40222},{\"end\":40239,\"start\":40231},{\"end\":40694,\"start\":40688},{\"end\":40702,\"start\":40694},{\"end\":40708,\"start\":40702},{\"end\":40720,\"start\":40708},{\"end\":41088,\"start\":41081},{\"end\":41096,\"start\":41088},{\"end\":41106,\"start\":41096},{\"end\":41399,\"start\":41392},{\"end\":41409,\"start\":41399},{\"end\":41631,\"start\":41624},{\"end\":41641,\"start\":41631},{\"end\":41867,\"start\":41856},{\"end\":41879,\"start\":41867},{\"end\":41887,\"start\":41879},{\"end\":42210,\"start\":42195},{\"end\":42304,\"start\":42293},{\"end\":42318,\"start\":42304},{\"end\":42573,\"start\":42562},{\"end\":42587,\"start\":42573},{\"end\":42597,\"start\":42587},{\"end\":42750,\"start\":42741},{\"end\":42759,\"start\":42750},{\"end\":42770,\"start\":42759},{\"end\":42778,\"start\":42770},{\"end\":42786,\"start\":42778},{\"end\":42796,\"start\":42786},{\"end\":42806,\"start\":42796},{\"end\":42817,\"start\":42806},{\"end\":43213,\"start\":43204},{\"end\":43224,\"start\":43213},{\"end\":43235,\"start\":43224},{\"end\":43245,\"start\":43235},{\"end\":43564,\"start\":43550},{\"end\":43573,\"start\":43564},{\"end\":43585,\"start\":43573},{\"end\":43595,\"start\":43585},{\"end\":43603,\"start\":43595},{\"end\":43614,\"start\":43603},{\"end\":44181,\"start\":44170},{\"end\":44192,\"start\":44181},{\"end\":44201,\"start\":44192},{\"end\":44210,\"start\":44201},{\"end\":44222,\"start\":44210},{\"end\":44599,\"start\":44591},{\"end\":44608,\"start\":44599},{\"end\":44617,\"start\":44608},{\"end\":44625,\"start\":44617},{\"end\":44637,\"start\":44625},{\"end\":44967,\"start\":44961},{\"end\":44975,\"start\":44967},{\"end\":44985,\"start\":44975},{\"end\":44991,\"start\":44985},{\"end\":45000,\"start\":44991},{\"end\":45008,\"start\":45000},{\"end\":45016,\"start\":45008},{\"end\":45462,\"start\":45453},{\"end\":45469,\"start\":45462},{\"end\":45477,\"start\":45469},{\"end\":45483,\"start\":45477},{\"end\":45491,\"start\":45483},{\"end\":45497,\"start\":45491},{\"end\":45509,\"start\":45497},{\"end\":45519,\"start\":45509},{\"end\":45531,\"start\":45519},{\"end\":45890,\"start\":45881},{\"end\":45902,\"start\":45890},{\"end\":45914,\"start\":45902}]", "bib_venue": "[{\"end\":27985,\"start\":27941},{\"end\":28390,\"start\":28349},{\"end\":28839,\"start\":28796},{\"end\":29161,\"start\":29122},{\"end\":29434,\"start\":29385},{\"end\":30119,\"start\":30096},{\"end\":30545,\"start\":30468},{\"end\":30956,\"start\":30925},{\"end\":31299,\"start\":31221},{\"end\":31704,\"start\":31639},{\"end\":31949,\"start\":31939},{\"end\":32252,\"start\":32175},{\"end\":32575,\"start\":32526},{\"end\":32872,\"start\":32799},{\"end\":33137,\"start\":33120},{\"end\":33451,\"start\":33402},{\"end\":33724,\"start\":33645},{\"end\":33976,\"start\":33928},{\"end\":34244,\"start\":34240},{\"end\":34573,\"start\":34526},{\"end\":34835,\"start\":34775},{\"end\":35155,\"start\":35078},{\"end\":35460,\"start\":35421},{\"end\":35777,\"start\":35713},{\"end\":36166,\"start\":36099},{\"end\":36439,\"start\":36406},{\"end\":36745,\"start\":36646},{\"end\":37110,\"start\":37010},{\"end\":37530,\"start\":37492},{\"end\":37857,\"start\":37812},{\"end\":38443,\"start\":38387},{\"end\":38957,\"start\":38910},{\"end\":39339,\"start\":39314},{\"end\":39627,\"start\":39602},{\"end\":39956,\"start\":39932},{\"end\":40290,\"start\":40239},{\"end\":40787,\"start\":40720},{\"end\":41168,\"start\":41122},{\"end\":41413,\"start\":41409},{\"end\":41622,\"start\":41562},{\"end\":41959,\"start\":41887},{\"end\":42291,\"start\":42255},{\"end\":42560,\"start\":42486},{\"end\":42917,\"start\":42833},{\"end\":43322,\"start\":43263},{\"end\":43693,\"start\":43614},{\"end\":44284,\"start\":44222},{\"end\":44672,\"start\":44637},{\"end\":45093,\"start\":45016},{\"end\":45569,\"start\":45531},{\"end\":45978,\"start\":45914},{\"end\":30609,\"start\":30547},{\"end\":31373,\"start\":31301},{\"end\":31955,\"start\":31951},{\"end\":32316,\"start\":32254},{\"end\":35219,\"start\":35157},{\"end\":36220,\"start\":36168},{\"end\":38495,\"start\":38445},{\"end\":40330,\"start\":40292},{\"end\":40841,\"start\":40789},{\"end\":43798,\"start\":43695},{\"end\":45157,\"start\":45095}]"}}}, "year": 2023, "month": 12, "day": 17}