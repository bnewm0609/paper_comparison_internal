{"id": 258546701, "updated": "2023-10-05 01:11:36.3", "metadata": {"title": "Query Expansion by Prompting Large Language Models", "authors": "[{\"first\":\"Rolf\",\"last\":\"Jagerman\",\"middle\":[]},{\"first\":\"Honglei\",\"last\":\"Zhuang\",\"middle\":[]},{\"first\":\"Zhen\",\"last\":\"Qin\",\"middle\":[]},{\"first\":\"Xuanhui\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Bendersky\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Query expansion is a widely used technique to improve the recall of search systems. In this paper, we propose an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs). Unlike traditional query expansion approaches such as Pseudo-Relevance Feedback (PRF) that relies on retrieving a good set of pseudo-relevant documents to expand queries, we rely on the generative and creative abilities of an LLM and leverage the knowledge inherent in the model. We study a variety of different prompts, including zero-shot, few-shot and Chain-of-Thought (CoT). We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query. Experimental results on MS-MARCO and BEIR demonstrate that query expansions generated by LLMs can be more powerful than traditional query expansion methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.03653", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-03653", "doi": "10.48550/arxiv.2305.03653"}}, "content": {"source": {"pdf_hash": "0fbab714338dafab2a48014bb5494f503d5df8ea", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.03653v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "366570a328b3f9f620fd74090ad278924bae3aa9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0fbab714338dafab2a48014bb5494f503d5df8ea.txt", "contents": "\nQuery Expansion by Prompting Large Language Models\n\n\nRolf Jagerman jagerman@google.com \nGoogle Research \nHonglei Zhuang \nGoogle Research \nZhen Qin zhenqin@google.com \nGoogle Research \nXuanhui Wang xuanhui@google.com \nGoogle Research \nMichael Bendersky \nGoogle Research \nQuery Expansion by Prompting Large Language Models\n\nQuery expansion is a widely used technique to improve the recall of search systems. In this paper, we propose an approach to query expansion that leverages the generative abilities of Large Language Models (LLMs). Unlike traditional query expansion approaches such as Pseudo-Relevance Feedback (PRF) that relies on retrieving a good set of pseudo-relevant documents to expand queries, we rely on the generative and creative abilities of an LLM and leverage the knowledge inherent in the model. We study a variety of different prompts, including zero-shot, few-shot and Chain-of-Thought (CoT). We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query. Experimental results on MS-MARCO and BEIR demonstrate that query expansions generated by LLMs can be more powerful than traditional query expansion methods.\n\nINTRODUCTION\n\nQuery expansion is a widely used technique that improves the recall of search systems by adding additional terms to the original query. The expanded query may be able to recover relevant documents that had no lexical overlap with the original query. Traditional query expansion approaches are typically based on Pseudo-Relevance Feedback (PRF) [1,20,21,23], which treats the set of retrieved documents from the original query as \"pseudo-relevant\" and uses those documents' contents to extract new query terms. However, PRF-based approaches assume that the top retrieved documents are relevant to the query. In practice the initial retrieved documents may not be perfectly aligned with the original query, especially if the query is short or ambiguous. As a result, PRF-based approaches may fail if the initial set of retrieved documents is not good enough.\n\nIn this paper we propose the use of Large Language Models (LLMs) [3,8,19] to aid in query expansion. LLMs have seen a growing interest in the Information Retrieval (IR) community in recent years. They exhibit several properties, including the ability to answer questions and generate text, that make them powerful tools. We propose using those generative abilities to generate useful query expansions. In particular we investigate ways to prompt an LLM and have it generate a variety of alternative and new terms for the original query. This means that, instead of relying on the knowledge within PRF documents or lexical knowledge bases, we rely on the knowledge inherent in the LLM. An example of the proposed methodology is presented in Figure 1.\n\nOur main contributions in this work are as follows: First, we formulate various prompts to perform query expansion (zero-shot, Answer the following question: {query} Give the rationale before answering\nLarge Language Model (LLM) Concat({query}, {model output})\nRetrieval System (BM25) Figure 1: High-level overview of using a zero-shot Chain-of-Thought (CoT) prompt to generate query expansion terms.\n\nfew-shot and CoT) with and without PRF to study their relative performance. Second, we find that Chain-of-Thought (CoT) prompts perform best and hypothesize that this is because CoT prompts instruct the model to break its answer down step-by-step which includes many keywords that can aid in query expansion. Finally, we study the performance across various model sizes to better understand the practical capabilities and limitations of an LLM approach to query expansion.\n\n\nRELATED WORK\n\nQuery expansion is widely studied [4,11]. At its core, query expansion helps retrieval systems by expanding query terms into new terms that express the same concept or information need, increasing the likelihood of a lexical match with documents in the corpus. Early works on query expansion focused on either using lexical knowledge bases [2,18,29] or Pseudo-Relevance Feedback (PRF) [1,20,23]. PRF-based approaches are particularly useful in practice because they do not need to construct a domain-specific knowledge base and can be applied to any corpus. Orthogonal to query expansion is document expansion [10,16,25,33] which applies similar techniques but expands document terms during indexing instead of query terms during retrieval.\n\nRecent works on query expansion have leveraged neural networks to generate or select expansion terms [13,24,33,34], generally by either training or fine-tuning a model. In contrast, our work leverages the abilities inherent in general-purpose LLMs without needing to train or fine-tune the model. We note that our work is similar to the recent works of [7] and [31]: leveraging an LLM to expand a query. However, we differentiate our work in several important ways: First, we study a number of different prompts whereas [31] focuses on a single few-shot prompt and [7] does not study prompts. Second, unlike [31] and [7], we focus on generating query expansion terms instead of entire pseudo documents. To this end, we demonstrate the performance of our prompts on a variety of smaller model sizes which helps understand both the limitations and the practical capabilities of an LLM approach to query expansion. Finally, we experiment with entirely open-source models, inviting reproducibility and openness of research, while [31] experiments with a single type of model which is only accessible through a third-party API.\n\n\nMETHODOLOGY\n\nWe formulate the query expansion problem as follows: given a query we wish to generate an expanded query \u2032 that contains additional query terms that may help in retrieving relevant documents. In particular we study the use of an LLM to expand the query terms and generate a new query \u2032 . Since the LLM output may be verbose, we repeat the original query terms 5 times to upweigh their relative importance. This is the same as the trick employed by [31]. More formally:\n\u2032 = Concat( , , , , , LLM(prompt )),(1)\nwhere Concat is the string concatenation operator, is the original query, LLM is a Large Language Model and prompt is the generated prompt based on the query (and potentially side information like few-shot examples or PRF documents).\n\nIn this paper we study eight different prompts:\n\n\nQ2D\n\nThe Query2Doc [31] few-shot prompt, asking the model to write a passage that answers the query. Q2D/ZS A zero-shot version of Q2D. Q2D/PRF A zero-shot prompt like Q2D/ZS but which also contains extra context in the form of top-3 retrieved PRF documents for the query.\n\n\nQ2E\n\nSimilar to the Query2Doc few-shot prompt but with examples of query expansion terms instead of documents.\n\n\nQ2E/ZS\n\nA zero-shot version of Q2E. Q2E/PRF A zero-shot prompt like Q2E/ZS but with extra context in the form of PRF documents like Q2D/PRF. CoT\n\nA zero-shot Chain-of-Thought prompt which instructs the model to provide rationale for its answer. CoT/PRF A prompt like CoT but which also contains extra context in the form of top-3 retrieved PRF documents for the query.\n\nZero-shot prompts (Q2D/ZS and Q2E/ZS) are the simplest as they consist of a simple plaintext instruction and the input query. Fewshot prompts (Q2D and Q2E) additionally contain several examples to support in-context learning, for example they contain queries and corresponding expansions. Chain-of-Thought (CoT) prompts formulate their instruction to obtain a more verbose output from the model by asking it to break its response down step-by-step. \n\n\nEXPERIMENTS\n\nTo validate the effectiveness of the LLM-based query expansion we run experiments on two retrieval tasks: MS-MARCO [15] passage retrieval and BEIR [27]. For the retrieval system we use BM25 [21,22] as implemented by Terrier [17] 1 . We use the default BM25 parameters ( = 0.75, 1 = 1.2, 3 = 8.0) provided by Terrier.\n\n\nBaselines\n\nTo analyze the LLM-based query expansion methods we compare against several classical PRF-based query expansion methods [1]:\n\n\u2022 Bo1: Bose-Einstein 1 weighting \u2022 Bo2: Bose-Einstein 2 weighting \u2022 KL: Kullback-Leibler weighting\n\nThe implementations for these are provided by Terrier. In all cases we use the default Terrier settings for query expansion: 3 PRF docs and 10 expansion terms. Furthermore, we include the prompt from Query2Doc [31] as a baseline. However, we do not compare against their exact setup since they use a significantly larger model than the models we study in this paper. The comparisons in this paper are focused on prompts and not on the exact numbers produced by different, potentially much larger, models. Furthermore, for models with a small receptive field (specifically the Flan-T5 models) we only use a 3-shot Q2D prompt instead of the standard 4-shot prompt to prevent the prompt from being truncated.\n\n\nLanguage Models\n\nWe compare the prompts on two types of models, Flan-T5 [6,19] and Flan-UL2 [26], at various model sizes:\n\u2022 Flan-T5-Small (60M parameters) \u2022 Flan-T5-Base (220M parameters) \u2022 Flan-T5-Large (770M parameters) \u2022 Flan-T5-XL (3B parameters) \u2022 Flan-T5-XXL (11B parameters) \u2022 Flan-UL2 (20B parameters)\nWe choose to use the Flan [6,32] versions of the T5 [19] and UL2 [26] models as they are fine-tuned to follow instructions which is critical when using prompt-based approaches. Furthermore, all of these models are available as open-source 2 .\n\n\nMetrics\n\nSince we are interested in query expansion, which is largely focussed on improving the recall of first-stage retrieval, we use Re-call@1K as our core evaluation metric. We also report top-heavy ranking metrics using MRR@10 [30] and NDCG@10 [14] to better understand how the models change the top retrieved results. We report all our results with significance testing using a paired -test and consider a result significant at < 0.01. 5.1 MS-MARCO Passage Ranking Table 1 presents the results on the MS-MARCO passage ranking task. The classical query expansion baselines (Bo1, Bo2 and KL), already provide a useful gain in terms of Recall@1K over the standard BM25 retrieval. In line with the results of [12], we observe that this increase in recall comes at the cost of top-heavy ranking metrics such as MRR@10 and NDCG@10.\n\nNext, we see the results of LLM-based query expansion depend heavily on the type of prompts used. Similar to the findings of [31], the Query2Doc prompt (Q2D) can provide a substantial gain in terms of Recall@1K over the classical approaches. Interestingly, Query2Doc does not only improve recall, but also improves the topheavy ranking metrics such as MRR@10 and NDCG@10, providing a good improvement across metrics. This contrasts with classical query expansion methods which typically sacrifice top-heavy ranking metrics in order to improve recall.\n\nFinally, the best performance is obtained by CoT (and the corresponding PRF-enhanced prompt CoT/PRF). This particular prompt instructs the model to generate a verbose explanation by breaking its answer down into steps. We hypothesize that this verbosity may lead to many potential keywords that are useful for query expansion. Finally, we find that adding PRF documents to the prompt helps significantly in top-heavy ranking metrics like MRR@10 and NDCG@10 across models and prompts. A possible explanation for this is that LLMs are effective in distilling the PRF documents, which may already contain relevant passages, by attending over the most promising keywords and using them in the output. We provide a more concrete example of the prompt output in Appendix B.\n\n\nBEIR\n\nThe BEIR datasets comprise many different zero-shot information retrieval tasks from a variety of domains. We compare the performance of the different prompts on the BEIR datasets in Table 2. The first thing to observe here is that the classical PRF-based query expansion baselines still work very well, especially on domainspecific datasets such as trec-covid, scidocs and touche2020. These datasets are largely academic and scientific in nature, and the PRF documents may provide useful query terms in these cases. In contrast, the general purpose LLMs may not have sufficient domain knowledge to be useful for these datasets. Second, we note that the question-answering style datasets (fiqa, hotpotqa, msmarco and nq) seem to benefit the most from an LLM approach to query expansion. It is likely that the language model is producing relevant answers towards the query which helps retrieve the relevant passages more effectively. Across all datasets, the Q2D/PRF prompt produces the highest average Recall@1K, with the CoT prompt as a close second.\n\n\nThe Impact of Model Size\n\nTo understand the practical capabilities and limitations of an LLMbased query expander, we compare different model sizes in Figure 2. We range the model size from 60M parameters (Flan-T5-small) up to 11B (Flan-T5-XXL) and also try a 20B parameter model (Flan-UL2) but note that the latter also has a different pre-training objective. In general we observe the expected trend that larger models tend to  perform better. The Q2D approach requires at least an 11B parameter model to reach parity with the BM25+Bo1 baseline. In contrast, the CoT approach only needs a 3B parameter model to reach parity. Furthermore, adding PRF documents to the CoT prompt seems to help stabilize the performance for smaller model sizes but does inhibit its performance at larger capacities. A possible explanation for this behavior is that the PRF documents decreases the creativity of the model, as it may focus too much on the provided documents. Although this helps prevent the model from making errors at smaller model sizes, it also inhibits the creative abilities that we wish to leverage at larger model sizes. The CoT/PRF prompt is able to outperform the other prompts at the 770M parameter model size, making it a good candidate for possible deployment in realistic search settings where serving a larger model may be impossible. Overall, it is clear that large models are able to provide significant gains which may limit the practical application of an LLM approach to query expansion. Distillation has been shown to be an effective way to transfer the ability of a large model to a smaller one. We leave the study of distillation of these models for query expansion as future work.\n\n\nLIMITATIONS & FUTURE WORK\n\nThere are a number of limitations in our work: First, we only study sparse retrieval (BM25) which is where query expansion is important. Dense retrieval systems (e.g. dual encoders) are less prone to the vocabulary gap and, as a result, are less likely to benefit from a query expansion. Wang et al. [31] has already studied this setting in more detail and we leave the analysis of our prompts for a dense retrieval setting as future work. Second, our work focuses on Flan [32] instruction-finetuned language models. We chose these models due to their ability to follow instructions and the fact that these models are open-source. Our work can naturally be extended to other language models [3,5,9,28] and we leave the study of such models as a topic for future research. Third, we study specific prompt templates (see Appendix A) and there may be other ways to formulate the different prompts. Finally, the computational cost of LLMs may be prohibitive to deploy LLM-based query expansions in practice. It may be possible to distill the output of the large model into a smaller servable model. How to productionize LLM-based query expansions is left as an open problem.\n\n\nCONCLUSION\n\nIn this paper we study LLM-based query expansions. In contrast to traditional PRF-based query expansion, LLMs are not restricted to the initial retrieved set of documents and may be able to generate expansion terms not covered by traditional methods. Our proposed method is simple: we prompt a large language model and provide it a query, then we use the model's output to expand the original query with new terms that help during document retrieval.\n\nOur results show that Chain-of-Thought prompts are especially promising for query expansion, since they instruct the model to generate verbose explanations that can cover a wide variety of new keywords. Furthermore, our results indicate that including PRF documents in various prompts can improve top-heavy ranking metric performance during the retrieval stage and is more robust when used with smaller model sizes, which can help practical deployment of LLM-based query expansion.\n\nAs demonstrated in this paper, IR tasks like query expansion can benefit from LLMs. As the capabilities of LLMs continue to improve, it is promising to see their capabilities translate to various IR tasks. Furthermore, as LLMs become more widely available, they will be easier to use and deploy as core parts of IR systems which is exciting for both practitioners and researchers of such systems. For the CoT prompt, we note that the model tends to output \"The final answer:\" or \"So the final answer is:\" towards the end and we filter those two sentences out prior to concatenating the model output with the query.  Table 4 shows the results of a query expansion for both the Flan-T5-Large (770M) model size and the Flan-UL2 (20B) model size. First, note that at the smaller model size, the CoT and Q2D prompts are not producing the correct answer which is harmful for retrieval performance. The CoT/PRF prompt, being more grounded in its PRF documents, avoids this problem and correctly produces the answer \"Tata Motors\" which helps retrieve the relevant passage. At the larger model size (Flan-UL2), all prompts Q2D, CoT and CoT/PRF produce the correct answer \"Tata Motors\". However, the CoT prompt provides the most verbose explanation towards its answer and has many term overlaps with the relevant passage, improving its overall retrieval performance. \n\n\nA PROMPTS\n\n\nFinally, Pseudo-Relevance Feedback (\u00b7/PRF) variations of prompts use the top-3 retrieved documents as additional context for the model. See Appendix A for the exact prompts that are used in the experiments.\n\nFigure 2 :\n2Performance on MS-MARCO passage ranking dev set across different model sizes. The shaded areas indicate a 99% confidence interval.\n\nTable 1 :\n1LLM-based query expansion on the MS-MARCO passage ranking dev set. \u25b2 indicates a statistically significant (paired -test, < 0.01) improvement relative to the Q2D Flan-UL2 method. The best result per metric is bolded.Recall@1K MRR@10 NDCG@10 \n\nBM25 \n87.82 \n18.77 \n23.44 \nBM25 + Bo1 \n88.68 \n17.75 \n22.48 \nBM25 + Bo2 \n88.32 \n17.58 \n22.30 \nBM25 + KL \n88.62 \n17.71 \n22.44 \n\nFlan-T5-XXL (11B) \nQ2D \n88.76 \n19.07 \n23.76 \nQ2D/ZS \n88.88 \n18.55 \n23.13 \nQ2D/PRF \n89.31 \n22.13 \u25b2 \n26.43 \u25b2 \nQ2E \n87.74 \n18.74 \n23.37 \nQ2E/ZS \n87.93 \n18.79 \n23.45 \nQ2E/PRF \n88.20 \n19.20 \n23.83 \nCoT \n89.86 \n19.16 \n23.82 \nCoT/PRF \n89.02 \n22.08 \u25b2 \n26.32 \u25b2 \n\nFlan-UL2 (20B) \nQ2D \n89.87 \n19.22 \n23.96 \nQ2D/ZS \n86.60 \n15.56 \n19.54 \nQ2D/PRF \n89.28 \n21.42 \u25b2 \n25.82 \u25b2 \nQ2E \n88.04 \n18.84 \n23.52 \nQ2E/ZS \n88.11 \n18.87 \n23.56 \nQ2E/PRF \n88.43 \n19.24 \n23.90 \nCoT \n90.61 \u25b2 \n20.05 \u25b2 \n24.85 \u25b2 \nCoT/PRF \n89.30 \n22.62 \u25b2 \n26.89 \u25b2 \n\n60M \n220M 770M \n3B \n11B \n20B \n85 \n\n86 \n\n87 \n\n88 \n\n89 \n\n90 \n\n91 \n\nModel size (# parameters) \n\nRecall@1K \n\nCoT \nQ2D \nCoT/PRF \nBM25 + Bo1 \n\n\n\nTable 2 :\n2Recall@1K of various prompts on BEIR using Flan-UL2. \u25b2 indicates a statistically significant (paired -test, < 0.01) improvement relative to the best classical QE method. The best result per dataset is highlighted in bold.Classical QE \nLLM-based QE \nDataset \nBM25 \nBo1 \nBo2 \nKL \nQ2D Q2D/ZS Q2D/PRF Q2E Q2E/ZS Q2E/PRF \nCoT \nCoT/PRF \n\narguana \n98.93 99.00 99.00 99.00 98.86 \n98.93 \n98.93 \n98.93 \n98.93 \n98.93 \n98.93 \n98.86 \nclimate-fever \n46.60 45.69 45.38 45.65 \n47.62 \n47.66 \n47.94 \n46.08 \n46.44 \n46.44 \n47.42 \n46.81 \ncqadupstack \n65.55 66.82 66.57 66.70 \n65.51 \n64.19 \n65.01 \n65.69 \n65.71 \n65.90 \n66.39 \n66.12 \ndbpedia \n63.72 64.77 64.55 64.60 65.89 \n65.47 \n65.78 \n63.55 \n63.92 \n63.93 \n65.77 \n65.06 \nfever \n75.73 76.28 75.83 76.32 79.06 \u25b2 78.87 \u25b2 \n77.29 \n75.78 \n75.79 \n76.27 \n78.21 \u25b2 \n77.25 \nfiqa \n77.42 79.18 79.06 78.84 \n78.34 \n78.26 \n78.69 \n77.33 \n77.31 \n77.68 \n80.08 \n79.03 \nhotpotqa \n85.78 84.84 81.71 84.65 \n86.90 \u25b2 85.71 \n87.58 \u25b2 \n85.60 \n85.54 \n87.25 \u25b2 \n87.54 \u25b2 \n88.79 \u25b2 \nmsmarco \n73.61 75.08 75.14 74.66 \n76.77 \n75.73 \n78.75 \n73.87 \n73.79 \n74.14 \n79.58 \n78.36 \nnfcorpus \n38.70 57.30 57.67 56.46 \n55.34 \n59.81 \n59.68 \n43.38 \n44.12 \n47.06 \n52.63 \n53.32 \nnq \n78.96 81.09 80.64 80.82 \n85.18 \u25b2 84.71 \u25b2 \n83.53 \u25b2 \n79.30 \n79.11 \n80.35 \n85.46 \u25b2 \n83.11 \u25b2 \nquora \n99.26 99.20 99.12 99.20 \n99.00 \n98.84 \n98.92 \n99.25 \n99.29 \n99.26 \n99.17 \n99.21 \nscidocs \n57.46 59.78 61.03 59.86 \n59.09 \n59.78 \n60.10 \n57.88 \n57.70 \n58.32 \n58.51 \n59.69 \nscifact \n97.17 97.57 97.57 97.57 97.57 \n97.57 \n97.57 \n97.17 \n97.17 \n97.17 \n97.57 \n97.17 \ntouche2020 \n84.96 85.94 86.38 86.01 \n83.61 \n83.44 \n84.54 \n85.21 \n85.02 \n86.04 \n85.51 \n84.58 \ntrec-covid \n42.58 45.21 45.58 45.39 \n43.52 \n38.05 \n44.17 \n43.16 \n43.12 \n43.85 \n43.43 \n44.02 \n\nAverage \n72.43 74.52 74.35 74.38 \n74.82 \n74.47 \n75.23 \n72.81 \n72.86 \n73.50 \n75.08 \n74.76 \n\n\n\nTable 3\n3contains all the prompts tried in this paper. In each prompt {query} denotes the query for which we want to generate a query expansion. We denote with {query 1}, . . ., {query 4} the sample queries from the MS-MARCO train set. Similarly, {doc 1}, . . ., {doc 4} represent relevant passages corresponding to the sampled queries, and, {expansion 1}, . . ., {expansion 4} represent corresponding expansions generated with Terrier KL method (at most 20 terms) from those relevant passages. Finally we denote with {PRF doc 1}, . . ., {PRF doc 3} the top 3 retrieved documents using the original query, acting as Pseudo-Relevance Feedback documents.\n\nTable 3 :\n3The various query expansion prompts.ID \nPrompt \n\n\nTable 4 :\n4Example of query expansions with Flan-UL2. Green highlights the correct answer while blue indicate specific term overlaps that indicate potentially useful query expansions. who owns jaguar motors? (ID=1045405) Passage Jaguar is the luxury vehicle brand of Jaguar Land Rover, a British multinational car manufacturer with its headquarters in Whitley, Coventry England, owned by the Indian company Tata Motors since 2008. Q2D Jaguar Motors is a British luxury car manufacturer based in the United Kingdom. CoT Jaguar Land Rover is the owner of Jaguar. The answer: Jaguar Land Rover. CoT/PRF The relevant information is: Jaguar is owned by the Indian automobile manufacturer Tata Motors Ltd. So the final answer is the Indian automobile manufacturer Tata Motors Ltd. Q2D Jaguar is a division of Tata Motors, a company owned by the Tata family. CoT Jaguar Land Rover is a British multinational car manufacturer, founded by William Lyons in 1931. Its headquarters are in Whitley, Coventry, United Kingdom and is a constituent of the FTSE 250 Index. The company is a wholly owned subsidiary of Tata Motors of India. So the final answer is Tata Motors. CoT/PRF Jaguar is owned by the Indian automobile manufacturer Tata Motors Ltd. The final answer: Tata Motors Ltd.Query \nFlan-T5-Large (770M) \n\nFlan-UL2 (20B) \n\n\nhttp://terrier.org/ 2 Models are available at https://huggingface.co/docs/transformers/model_doc/flan-t5 and https://huggingface.co/google/flan-ul2\n\nProbabilistic models of information retrieval based on measuring the divergence from randomness. Gianni Amati, Cornelis Joost Van Rijsbergen, ACM Transactions on Information Systems (TOIS). 20Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 357-389.\n\nA review of ontology based query expansion. Information processing & management. Jagdev Bhogal, Andrew Macfarlane, Peter Smith, 43Jagdev Bhogal, Andrew MacFarlane, and Peter Smith. 2007. A review of ontology based query expansion. Information processing & management 43, 4 (2007), 866- 886.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.\n\nA survey of automatic query expansion in information retrieval. Claudio Carpineto, Giovanni Romano, Acm Computing Surveys (CSUR). 44Claudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval. Acm Computing Surveys (CSUR) 44, 1 (2012), 1-50.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se- bastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).\n\nHyung Won, Le Chung, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Eric Fedus, Xuezhi Li, Mostafa Wang, Dehghani, arXiv:2210.11416Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprintHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n\nNeural text generation for query expansion in information retrieval. Vincent Claveau, IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology. Vincent Claveau. 2021. Neural text generation for query expansion in information retrieval. In IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology. 202-209.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\n\nGlam: Efficient scaling of language models with mixture-of-experts. Nan Du, Yanping Huang, M Andrew, Simon Dai, Dmitry Tong, Yuanzhong Lepikhin, Maxim Xu, Yanqi Krikun, Adams Wei Zhou, Orhan Yu, Firat, PMLRInternational Conference on Machine Learning. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning. PMLR, 5547-5569.\n\nImproving retrieval of short texts through document expansion. Miles Efron, Peter Organisciak, Katrina Fenlon, Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval. the 35th international ACM SIGIR conference on Research and development in information retrievalMiles Efron, Peter Organisciak, and Katrina Fenlon. 2012. Improving retrieval of short texts through document expansion. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval. 911- 920.\n\nQuery Expansion. Annual review of information science and technology (ARIST). N Efthimis, Efthimiadis, Efthimis N Efthimiadis. 1996. Query Expansion. Annual review of information science and technology (ARIST) 31 (1996), 121-87.\n\nRelevance feedback and other query modification techniques. Information Retrieval: Data Structures & Algorithms. D Harman, n. d.D Harman. [n.d.]. Relevance feedback and other query modification techniques. Information Retrieval: Data Structures & Algorithms ([n. d.]), 241-263.\n\nDeep neural networks for query expansion using word embeddings. Ayyoob Imani, Amir Vakili, Ali Montazer, Azadeh Shakery, Advances in Information Retrieval: 41st European Conference on IR Research. Cologne, GermanySpringerProceedings, Part II 41Ayyoob Imani, Amir Vakili, Ali Montazer, and Azadeh Shakery. 2019. Deep neural networks for query expansion using word embeddings. In Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14-18, 2019, Proceedings, Part II 41. Springer, 203-210.\n\nCumulated gain-based evaluation of IR techniques. Kalervo J\u00e4rvelin, Jaana Kek\u00e4l\u00e4inen, ACM Transactions on Information Systems (TOIS). 20Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422-446.\n\nMS MARCO: A human generated machine reading comprehension dataset. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, Li Deng, choice. 2640660Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. choice 2640 (2016), 660.\n\nDocument expansion by query prediction. Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho, arXiv:1904.08375arXiv preprintRodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction. arXiv preprint arXiv:1904.08375 (2019).\n\nTerrier information retrieval platform. Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, Douglas Johnson, Advances in Information Retrieval: 27th European Conference on IR Research. Santiago de Compostela, SpainSpringerIadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and Douglas Johnson. 2005. Terrier information retrieval platform. In Advances in Information Retrieval: 27th European Conference on IR Research, ECIR 2005, Santiago de Compostela, Spain, March 21-23, 2005. Proceedings 27. Springer, 517-519.\n\nConcept based query expansion. Yonggang Qiu, Hans-Peter Frei, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval. the 16th annual international ACM SIGIR conference on Research and development in information retrievalYonggang Qiu and Hans-Peter Frei. 1993. Concept based query expansion. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval. 160-169.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 21Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485-5551.\n\nOn term selection for query expansion. Stephen E Robertson, Journal of documentation. 46Stephen E Robertson. 1990. On term selection for query expansion. Journal of documentation 46, 4 (1990), 359-364.\n\nRelevance weighting of search terms. E Stephen, K Sparck Jones Robertson, Journal of the American Society for Information science. 27Stephen E Robertson and K Sparck Jones. 1976. Relevance weighting of search terms. Journal of the American Society for Information science 27, 3 (1976), 129- 146.\n\nOkapi at TREC-3. Steve Stephen E Robertson, Susan Walker, Micheline M Jones, Mike Hancock-Beaulieu, Gatford, Nist Special Publication Sp. 109109Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. Nist Special Publication Sp 109 (1995), 109.\n\nRelevance feedback in information retrieval. The SMART retrieval system: experiments in automatic document processing. Joseph John RocchioJr, Joseph John Rocchio Jr. 1971. Relevance feedback in information retrieval. The SMART retrieval system: experiments in automatic document processing (1971).\n\nUsing word embeddings for automatic query expansion. Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, Utpal Garain, arXiv:1606.07608arXiv preprintDwaipayan Roy, Debjyoti Paul, Mandar Mitra, and Utpal Garain. 2016. Using word embeddings for automatic query expansion. arXiv preprint arXiv:1606.07608 (2016).\n\nLanguage model information retrieval with document expansion. Tao Tao, Xuanhui Wang, Qiaozhu Mei, Chengxiang Zhai, Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. the Human Language Technology Conference of the NAACL, Main ConferenceTao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. 2006. Language model information retrieval with document expansion. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. 407- 414.\n\nYi Tay, Mostafa Dehghani, Q Vinh, Xavier Tran, Dara Garcia, Bahri, arXiv:2205.05131Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv preprintYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schus- ter, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131 (2022).\n\nBEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, Iryna Gurevych, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview. net/forum?id=wCu6T5xFjeJ\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Lamda: Language models for dialog applications. arXiv preprintRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul- shreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022).\n\nQuery expansion using lexical-semantic relations. M Ellen, Voorhees, SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University. SpringerEllen M Voorhees. 1994. Query expansion using lexical-semantic relations. In SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Confer- ence on Research and Development in Information Retrieval, organised by Dublin City University. Springer, 61-69.\n\nThe trec-8 question answering track report. M Ellen, Voorhees, Trec. 99Ellen M Voorhees et al. 1999. The trec-8 question answering track report.. In Trec, Vol. 99. 77-82.\n\nLiang Wang, Nan Yang, Furu Wei, arXiv:2303.07678Query2doc: Query Expansion with Large Language Models. arXiv preprintLiang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. arXiv preprint arXiv:2303.07678 (2023).\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M Andrew, Quoc V Dai, Le, arXiv:2109.01652arXiv preprintJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\n\nBERT-QE: contextualized query expansion for document re-ranking. Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, Andrew Yates, arXiv:2009.07258arXiv preprintZhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2020. BERT- QE: contextualized query expansion for document re-ranking. arXiv preprint arXiv:2009.07258 (2020).\n\nContextualized query expansion via unsupervised chunk selection for text retrieval. Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, Andrew Yates, Information Processing & Management. 58102672Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2021. Con- textualized query expansion via unsupervised chunk selection for text retrieval. Information Processing & Management 58, 5 (2021), 102672.\n\nQuery: {query 1} Passage: {doc 1} Query: {query 2} Passage: {doc 2} Query: {query 3} Passage: {doc 3} Query: {query 4} Passage: {doc 4} Query: {query} Passage: Q2D/ZS Write a passage that answers the following query: {query} Q2D/PRF Write a passage that answers the given query based on the context: Context: {PRF doc 1} {PRF doc 2} {PRF doc 3} Query: {query} Passage: Q2E Write a list of keywords for the given query: Query: {query 1} Keywords: {expansion 1} Query: {query 2} Keywords: {expansion 2} Query: {query 3} Keywords: {expansion 3} Query: {query 4} Keywords: {expansion 4} Query: {query} Keywords: Q2E/ZS Write a list of keywords for the following query: {query} Q2E/PRF Write a list of keywords for the given query based on the context: Context: {PRF doc 1} {PRF doc 2} {PRF doc 3} Query: {query} Keywords: CoT Answer the following query: {query} Give the rationale before answering. Write a passage that answers the given queryWrite a passage that answers the given query: Query: {query 1} Passage: {doc 1} Query: {query 2} Passage: {doc 2} Query: {query 3} Passage: {doc 3} Query: {query 4} Passage: {doc 4} Query: {query} Passage: Q2D/ZS Write a passage that answers the following query: {query} Q2D/PRF Write a passage that answers the given query based on the context: Context: {PRF doc 1} {PRF doc 2} {PRF doc 3} Query: {query} Passage: Q2E Write a list of keywords for the given query: Query: {query 1} Keywords: {expansion 1} Query: {query 2} Keywords: {expansion 2} Query: {query 3} Keywords: {expansion 3} Query: {query 4} Keywords: {expansion 4} Query: {query} Keywords: Q2E/ZS Write a list of keywords for the following query: {query} Q2E/PRF Write a list of keywords for the given query based on the context: Context: {PRF doc 1} {PRF doc 2} {PRF doc 3} Query: {query} Keywords: CoT Answer the following query: {query} Give the rationale before answering\n\nCoT/PRF Answer the following query based on the context: Context: {PRF doc 1} {PRF doc 2} {PRF doc 3} Query: {query} Give the rationale before answering. CoT/PRF Answer the following query based on the context: Context: {PRF doc 1} {PRF doc 2} {PRF doc 3} Query: {query} Give the rationale before answering\n", "annotations": {"author": "[{\"end\":88,\"start\":54},{\"end\":105,\"start\":89},{\"end\":121,\"start\":106},{\"end\":138,\"start\":122},{\"end\":167,\"start\":139},{\"end\":184,\"start\":168},{\"end\":217,\"start\":185},{\"end\":234,\"start\":218},{\"end\":253,\"start\":235},{\"end\":270,\"start\":254}]", "publisher": null, "author_last_name": "[{\"end\":67,\"start\":59},{\"end\":104,\"start\":96},{\"end\":120,\"start\":114},{\"end\":137,\"start\":129},{\"end\":147,\"start\":144},{\"end\":183,\"start\":175},{\"end\":197,\"start\":193},{\"end\":233,\"start\":225},{\"end\":252,\"start\":243},{\"end\":269,\"start\":261}]", "author_first_name": "[{\"end\":58,\"start\":54},{\"end\":95,\"start\":89},{\"end\":113,\"start\":106},{\"end\":128,\"start\":122},{\"end\":143,\"start\":139},{\"end\":174,\"start\":168},{\"end\":192,\"start\":185},{\"end\":224,\"start\":218},{\"end\":242,\"start\":235},{\"end\":260,\"start\":254}]", "author_affiliation": null, "title": "[{\"end\":51,\"start\":1},{\"end\":321,\"start\":271}]", "venue": null, "abstract": "[{\"end\":1281,\"start\":323}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1644,\"start\":1641},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1647,\"start\":1644},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1650,\"start\":1647},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1653,\"start\":1650},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2223,\"start\":2220},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2225,\"start\":2223},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2227,\"start\":2225},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3834,\"start\":3831},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3837,\"start\":3834},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4140,\"start\":4137},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4143,\"start\":4140},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4146,\"start\":4143},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4185,\"start\":4182},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4188,\"start\":4185},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4191,\"start\":4188},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4411,\"start\":4407},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4414,\"start\":4411},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4417,\"start\":4414},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4420,\"start\":4417},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4644,\"start\":4640},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4647,\"start\":4644},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4650,\"start\":4647},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4653,\"start\":4650},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4895,\"start\":4892},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4904,\"start\":4900},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5063,\"start\":5059},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5107,\"start\":5104},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5151,\"start\":5147},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5159,\"start\":5156},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5569,\"start\":5565},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6129,\"start\":6125},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6494,\"start\":6490},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7813,\"start\":7809},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7845,\"start\":7841},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7888,\"start\":7884},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7891,\"start\":7888},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7922,\"start\":7918},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9033,\"start\":9030},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9036,\"start\":9033},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9054,\"start\":9050},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9297,\"start\":9294},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9300,\"start\":9297},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9324,\"start\":9320},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9337,\"start\":9333},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9749,\"start\":9745},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9766,\"start\":9762},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10228,\"start\":10224},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10475,\"start\":10471},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14761,\"start\":14757},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14934,\"start\":14930},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15151,\"start\":15148},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15153,\"start\":15151},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15155,\"start\":15153},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15158,\"start\":15155}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":18155,\"start\":17947},{\"attributes\":{\"id\":\"fig_1\"},\"end\":18299,\"start\":18156},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":19329,\"start\":18300},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":21141,\"start\":19330},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":21795,\"start\":21142},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":21856,\"start\":21796},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":23175,\"start\":21857}]", "paragraph": "[{\"end\":2153,\"start\":1297},{\"end\":2904,\"start\":2155},{\"end\":3107,\"start\":2906},{\"end\":3306,\"start\":3167},{\"end\":3780,\"start\":3308},{\"end\":4537,\"start\":3797},{\"end\":5661,\"start\":4539},{\"end\":6145,\"start\":5677},{\"end\":6419,\"start\":6186},{\"end\":6468,\"start\":6421},{\"end\":6743,\"start\":6476},{\"end\":6856,\"start\":6751},{\"end\":7003,\"start\":6867},{\"end\":7227,\"start\":7005},{\"end\":7678,\"start\":7229},{\"end\":8010,\"start\":7694},{\"end\":8148,\"start\":8024},{\"end\":8248,\"start\":8150},{\"end\":8955,\"start\":8250},{\"end\":9079,\"start\":8975},{\"end\":9510,\"start\":9268},{\"end\":10344,\"start\":9522},{\"end\":10896,\"start\":10346},{\"end\":11665,\"start\":10898},{\"end\":12725,\"start\":11674},{\"end\":14427,\"start\":12754},{\"end\":15627,\"start\":14457},{\"end\":16092,\"start\":15642},{\"end\":16575,\"start\":16094},{\"end\":17934,\"start\":16577}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":3166,\"start\":3108},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6185,\"start\":6146},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9267,\"start\":9080}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9991,\"start\":9984},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11864,\"start\":11857},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17200,\"start\":17193}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1295,\"start\":1283},{\"attributes\":{\"n\":\"2\"},\"end\":3795,\"start\":3783},{\"attributes\":{\"n\":\"3\"},\"end\":5675,\"start\":5664},{\"end\":6474,\"start\":6471},{\"end\":6749,\"start\":6746},{\"end\":6865,\"start\":6859},{\"attributes\":{\"n\":\"4\"},\"end\":7692,\"start\":7681},{\"attributes\":{\"n\":\"4.1\"},\"end\":8022,\"start\":8013},{\"attributes\":{\"n\":\"4.2\"},\"end\":8973,\"start\":8958},{\"attributes\":{\"n\":\"4.3\"},\"end\":9520,\"start\":9513},{\"attributes\":{\"n\":\"5.2\"},\"end\":11672,\"start\":11668},{\"attributes\":{\"n\":\"5.3\"},\"end\":12752,\"start\":12728},{\"attributes\":{\"n\":\"6\"},\"end\":14455,\"start\":14430},{\"attributes\":{\"n\":\"7\"},\"end\":15640,\"start\":15630},{\"end\":17946,\"start\":17937},{\"end\":18167,\"start\":18157},{\"end\":18310,\"start\":18301},{\"end\":19340,\"start\":19331},{\"end\":21150,\"start\":21143},{\"end\":21806,\"start\":21797},{\"end\":21867,\"start\":21858}]", "table": "[{\"end\":19329,\"start\":18528},{\"end\":21141,\"start\":19563},{\"end\":21856,\"start\":21844},{\"end\":23175,\"start\":23128}]", "figure_caption": "[{\"end\":18155,\"start\":17949},{\"end\":18299,\"start\":18169},{\"end\":18528,\"start\":18312},{\"end\":19563,\"start\":19342},{\"end\":21795,\"start\":21152},{\"end\":21844,\"start\":21808},{\"end\":23128,\"start\":21869}]", "figure_ref": "[{\"end\":2903,\"start\":2895},{\"end\":3199,\"start\":3191},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12886,\"start\":12878}]", "bib_author_first_name": "[{\"end\":23428,\"start\":23422},{\"end\":23450,\"start\":23436},{\"end\":23826,\"start\":23820},{\"end\":23841,\"start\":23835},{\"end\":23859,\"start\":23854},{\"end\":24073,\"start\":24070},{\"end\":24089,\"start\":24081},{\"end\":24100,\"start\":24096},{\"end\":24115,\"start\":24108},{\"end\":24130,\"start\":24125},{\"end\":24132,\"start\":24131},{\"end\":24149,\"start\":24141},{\"end\":24166,\"start\":24160},{\"end\":24186,\"start\":24180},{\"end\":24200,\"start\":24194},{\"end\":24215,\"start\":24209},{\"end\":24627,\"start\":24620},{\"end\":24647,\"start\":24639},{\"end\":24856,\"start\":24847},{\"end\":24874,\"start\":24868},{\"end\":24888,\"start\":24883},{\"end\":24904,\"start\":24897},{\"end\":24918,\"start\":24912},{\"end\":24931,\"start\":24927},{\"end\":24945,\"start\":24941},{\"end\":24972,\"start\":24965},{\"end\":24989,\"start\":24980},{\"end\":25361,\"start\":25359},{\"end\":25375,\"start\":25369},{\"end\":25387,\"start\":25381},{\"end\":25399,\"start\":25397},{\"end\":25413,\"start\":25406},{\"end\":25423,\"start\":25419},{\"end\":25437,\"start\":25431},{\"end\":25449,\"start\":25442},{\"end\":25888,\"start\":25881},{\"end\":26274,\"start\":26269},{\"end\":26291,\"start\":26283},{\"end\":26305,\"start\":26299},{\"end\":26319,\"start\":26311},{\"end\":26627,\"start\":26624},{\"end\":26639,\"start\":26632},{\"end\":26648,\"start\":26647},{\"end\":26662,\"start\":26657},{\"end\":26674,\"start\":26668},{\"end\":26690,\"start\":26681},{\"end\":26706,\"start\":26701},{\"end\":26716,\"start\":26711},{\"end\":26730,\"start\":26725},{\"end\":26734,\"start\":26731},{\"end\":26746,\"start\":26741},{\"end\":27157,\"start\":27152},{\"end\":27170,\"start\":27165},{\"end\":27191,\"start\":27184},{\"end\":27736,\"start\":27735},{\"end\":28001,\"start\":28000},{\"end\":28236,\"start\":28230},{\"end\":28248,\"start\":28244},{\"end\":28260,\"start\":28257},{\"end\":28277,\"start\":28271},{\"end\":28770,\"start\":28763},{\"end\":28786,\"start\":28781},{\"end\":29085,\"start\":29082},{\"end\":29097,\"start\":29094},{\"end\":29112,\"start\":29109},{\"end\":29127,\"start\":29119},{\"end\":29140,\"start\":29133},{\"end\":29155,\"start\":29149},{\"end\":29168,\"start\":29166},{\"end\":29433,\"start\":29426},{\"end\":29447,\"start\":29444},{\"end\":29459,\"start\":29454},{\"end\":29474,\"start\":29465},{\"end\":29699,\"start\":29695},{\"end\":29713,\"start\":29707},{\"end\":29729,\"start\":29721},{\"end\":29745,\"start\":29742},{\"end\":29755,\"start\":29750},{\"end\":29774,\"start\":29767},{\"end\":30251,\"start\":30243},{\"end\":30267,\"start\":30257},{\"end\":30789,\"start\":30784},{\"end\":30802,\"start\":30798},{\"end\":30816,\"start\":30812},{\"end\":30835,\"start\":30826},{\"end\":30847,\"start\":30841},{\"end\":30863,\"start\":30856},{\"end\":30877,\"start\":30872},{\"end\":30887,\"start\":30884},{\"end\":30899,\"start\":30892},{\"end\":31471,\"start\":31470},{\"end\":31495,\"start\":31481},{\"end\":31752,\"start\":31747},{\"end\":31779,\"start\":31774},{\"end\":31797,\"start\":31788},{\"end\":31799,\"start\":31798},{\"end\":31811,\"start\":31807},{\"end\":32403,\"start\":32394},{\"end\":32417,\"start\":32409},{\"end\":32430,\"start\":32424},{\"end\":32443,\"start\":32438},{\"end\":32709,\"start\":32706},{\"end\":32722,\"start\":32715},{\"end\":32736,\"start\":32729},{\"end\":32752,\"start\":32742},{\"end\":33144,\"start\":33142},{\"end\":33157,\"start\":33150},{\"end\":33169,\"start\":33168},{\"end\":33182,\"start\":33176},{\"end\":33193,\"start\":33189},{\"end\":33672,\"start\":33666},{\"end\":33685,\"start\":33681},{\"end\":33702,\"start\":33695},{\"end\":33719,\"start\":33711},{\"end\":33737,\"start\":33732},{\"end\":34186,\"start\":34181},{\"end\":34204,\"start\":34198},{\"end\":34207,\"start\":34205},{\"end\":34222,\"start\":34217},{\"end\":34233,\"start\":34229},{\"end\":34249,\"start\":34243},{\"end\":34280,\"start\":34274},{\"end\":34294,\"start\":34288},{\"end\":34306,\"start\":34300},{\"end\":34314,\"start\":34312},{\"end\":34703,\"start\":34702},{\"end\":35219,\"start\":35218},{\"end\":35351,\"start\":35346},{\"end\":35361,\"start\":35358},{\"end\":35372,\"start\":35368},{\"end\":35656,\"start\":35651},{\"end\":35669,\"start\":35662},{\"end\":35678,\"start\":35677},{\"end\":35694,\"start\":35688},{\"end\":35706,\"start\":35701},{\"end\":35710,\"start\":35707},{\"end\":35721,\"start\":35716},{\"end\":35729,\"start\":35726},{\"end\":35743,\"start\":35742},{\"end\":35758,\"start\":35752},{\"end\":36082,\"start\":36079},{\"end\":36093,\"start\":36090},{\"end\":36102,\"start\":36099},{\"end\":36114,\"start\":36107},{\"end\":36122,\"start\":36120},{\"end\":36134,\"start\":36128},{\"end\":36439,\"start\":36436},{\"end\":36450,\"start\":36447},{\"end\":36459,\"start\":36456},{\"end\":36471,\"start\":36464},{\"end\":36479,\"start\":36477},{\"end\":36491,\"start\":36485}]", "bib_author_last_name": "[{\"end\":23434,\"start\":23429},{\"end\":23465,\"start\":23451},{\"end\":23833,\"start\":23827},{\"end\":23852,\"start\":23842},{\"end\":23865,\"start\":23860},{\"end\":24079,\"start\":24074},{\"end\":24094,\"start\":24090},{\"end\":24106,\"start\":24101},{\"end\":24123,\"start\":24116},{\"end\":24139,\"start\":24133},{\"end\":24158,\"start\":24150},{\"end\":24178,\"start\":24167},{\"end\":24192,\"start\":24187},{\"end\":24207,\"start\":24201},{\"end\":24222,\"start\":24216},{\"end\":24637,\"start\":24628},{\"end\":24654,\"start\":24648},{\"end\":24866,\"start\":24857},{\"end\":24881,\"start\":24875},{\"end\":24895,\"start\":24889},{\"end\":24910,\"start\":24905},{\"end\":24925,\"start\":24919},{\"end\":24939,\"start\":24932},{\"end\":24952,\"start\":24946},{\"end\":24963,\"start\":24954},{\"end\":24978,\"start\":24973},{\"end\":24996,\"start\":24990},{\"end\":25006,\"start\":24998},{\"end\":25357,\"start\":25348},{\"end\":25367,\"start\":25362},{\"end\":25379,\"start\":25376},{\"end\":25395,\"start\":25388},{\"end\":25404,\"start\":25400},{\"end\":25417,\"start\":25414},{\"end\":25429,\"start\":25424},{\"end\":25440,\"start\":25438},{\"end\":25454,\"start\":25450},{\"end\":25464,\"start\":25456},{\"end\":25896,\"start\":25889},{\"end\":26281,\"start\":26275},{\"end\":26297,\"start\":26292},{\"end\":26309,\"start\":26306},{\"end\":26329,\"start\":26320},{\"end\":26630,\"start\":26628},{\"end\":26645,\"start\":26640},{\"end\":26655,\"start\":26649},{\"end\":26666,\"start\":26663},{\"end\":26679,\"start\":26675},{\"end\":26699,\"start\":26691},{\"end\":26709,\"start\":26707},{\"end\":26723,\"start\":26717},{\"end\":26739,\"start\":26735},{\"end\":26749,\"start\":26747},{\"end\":26756,\"start\":26751},{\"end\":27163,\"start\":27158},{\"end\":27182,\"start\":27171},{\"end\":27198,\"start\":27192},{\"end\":27745,\"start\":27737},{\"end\":27758,\"start\":27747},{\"end\":28008,\"start\":28002},{\"end\":28242,\"start\":28237},{\"end\":28255,\"start\":28249},{\"end\":28269,\"start\":28261},{\"end\":28285,\"start\":28278},{\"end\":28779,\"start\":28771},{\"end\":28797,\"start\":28787},{\"end\":29092,\"start\":29086},{\"end\":29107,\"start\":29098},{\"end\":29117,\"start\":29113},{\"end\":29131,\"start\":29128},{\"end\":29147,\"start\":29141},{\"end\":29164,\"start\":29156},{\"end\":29173,\"start\":29169},{\"end\":29442,\"start\":29434},{\"end\":29452,\"start\":29448},{\"end\":29463,\"start\":29460},{\"end\":29478,\"start\":29475},{\"end\":29705,\"start\":29700},{\"end\":29719,\"start\":29714},{\"end\":29740,\"start\":29730},{\"end\":29748,\"start\":29746},{\"end\":29765,\"start\":29756},{\"end\":29782,\"start\":29775},{\"end\":30255,\"start\":30252},{\"end\":30272,\"start\":30268},{\"end\":30796,\"start\":30790},{\"end\":30810,\"start\":30803},{\"end\":30824,\"start\":30817},{\"end\":30839,\"start\":30836},{\"end\":30854,\"start\":30848},{\"end\":30870,\"start\":30864},{\"end\":30882,\"start\":30878},{\"end\":30890,\"start\":30888},{\"end\":30903,\"start\":30900},{\"end\":31288,\"start\":31269},{\"end\":31479,\"start\":31472},{\"end\":31505,\"start\":31496},{\"end\":31772,\"start\":31753},{\"end\":31786,\"start\":31780},{\"end\":31805,\"start\":31800},{\"end\":31828,\"start\":31812},{\"end\":31837,\"start\":31830},{\"end\":32180,\"start\":32161},{\"end\":32407,\"start\":32404},{\"end\":32422,\"start\":32418},{\"end\":32436,\"start\":32431},{\"end\":32450,\"start\":32444},{\"end\":32713,\"start\":32710},{\"end\":32727,\"start\":32723},{\"end\":32740,\"start\":32737},{\"end\":32757,\"start\":32753},{\"end\":33148,\"start\":33145},{\"end\":33166,\"start\":33158},{\"end\":33174,\"start\":33170},{\"end\":33187,\"start\":33183},{\"end\":33200,\"start\":33194},{\"end\":33207,\"start\":33202},{\"end\":33679,\"start\":33673},{\"end\":33693,\"start\":33686},{\"end\":33709,\"start\":33703},{\"end\":33730,\"start\":33720},{\"end\":33746,\"start\":33738},{\"end\":34196,\"start\":34187},{\"end\":34215,\"start\":34208},{\"end\":34227,\"start\":34223},{\"end\":34241,\"start\":34234},{\"end\":34262,\"start\":34250},{\"end\":34272,\"start\":34264},{\"end\":34286,\"start\":34281},{\"end\":34298,\"start\":34295},{\"end\":34310,\"start\":34307},{\"end\":34320,\"start\":34315},{\"end\":34324,\"start\":34322},{\"end\":34709,\"start\":34704},{\"end\":34719,\"start\":34711},{\"end\":35225,\"start\":35220},{\"end\":35235,\"start\":35227},{\"end\":35356,\"start\":35352},{\"end\":35366,\"start\":35362},{\"end\":35376,\"start\":35373},{\"end\":35660,\"start\":35657},{\"end\":35675,\"start\":35670},{\"end\":35686,\"start\":35679},{\"end\":35699,\"start\":35695},{\"end\":35714,\"start\":35711},{\"end\":35724,\"start\":35722},{\"end\":35736,\"start\":35730},{\"end\":35740,\"start\":35738},{\"end\":35750,\"start\":35744},{\"end\":35762,\"start\":35759},{\"end\":35766,\"start\":35764},{\"end\":36088,\"start\":36083},{\"end\":36097,\"start\":36094},{\"end\":36105,\"start\":36103},{\"end\":36118,\"start\":36115},{\"end\":36126,\"start\":36123},{\"end\":36140,\"start\":36135},{\"end\":36445,\"start\":36440},{\"end\":36454,\"start\":36451},{\"end\":36462,\"start\":36460},{\"end\":36475,\"start\":36472},{\"end\":36483,\"start\":36480},{\"end\":36497,\"start\":36492}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7446821},\"end\":23737,\"start\":23325},{\"attributes\":{\"id\":\"b1\"},\"end\":24029,\"start\":23739},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218971783},\"end\":24554,\"start\":24031},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10393627},\"end\":24845,\"start\":24556},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b4\"},\"end\":25346,\"start\":24847},{\"attributes\":{\"doi\":\"arXiv:2210.11416\",\"id\":\"b5\"},\"end\":25810,\"start\":25348},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":248151163},\"end\":26185,\"start\":25812},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b7\"},\"end\":26554,\"start\":26187},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b8\",\"matched_paper_id\":245124124},\"end\":27087,\"start\":26556},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11138893},\"end\":27655,\"start\":27089},{\"attributes\":{\"id\":\"b10\"},\"end\":27885,\"start\":27657},{\"attributes\":{\"id\":\"b11\"},\"end\":28164,\"start\":27887},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":53206970},\"end\":28711,\"start\":28166},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1981391},\"end\":29013,\"start\":28713},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1289517},\"end\":29384,\"start\":29015},{\"attributes\":{\"doi\":\"arXiv:1904.08375\",\"id\":\"b15\"},\"end\":29653,\"start\":29386},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":31330948},\"end\":30210,\"start\":29655},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6531546},\"end\":30699,\"start\":30212},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":204838007},\"end\":31228,\"start\":30701},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1418295},\"end\":31431,\"start\":31230},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":45186038},\"end\":31728,\"start\":31433},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3946054},\"end\":32040,\"start\":31730},{\"attributes\":{\"id\":\"b22\"},\"end\":32339,\"start\":32042},{\"attributes\":{\"doi\":\"arXiv:1606.07608\",\"id\":\"b23\"},\"end\":32642,\"start\":32341},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10922426},\"end\":33140,\"start\":32644},{\"attributes\":{\"doi\":\"arXiv:2205.05131\",\"id\":\"b25\"},\"end\":33574,\"start\":33142},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":233296016},\"end\":34179,\"start\":33576},{\"attributes\":{\"doi\":\"arXiv:2201.08239\",\"id\":\"b27\"},\"end\":34650,\"start\":34181},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":18126742},\"end\":35172,\"start\":34652},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16944215},\"end\":35344,\"start\":35174},{\"attributes\":{\"doi\":\"arXiv:2303.07678\",\"id\":\"b30\"},\"end\":35599,\"start\":35346},{\"attributes\":{\"doi\":\"arXiv:2109.01652\",\"id\":\"b31\"},\"end\":36012,\"start\":35601},{\"attributes\":{\"doi\":\"arXiv:2009.07258\",\"id\":\"b32\"},\"end\":36350,\"start\":36014},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":236971873},\"end\":36760,\"start\":36352},{\"attributes\":{\"id\":\"b34\"},\"end\":38640,\"start\":36762},{\"attributes\":{\"id\":\"b35\"},\"end\":38948,\"start\":38642}]", "bib_title": "[{\"end\":23420,\"start\":23325},{\"end\":24068,\"start\":24031},{\"end\":24618,\"start\":24556},{\"end\":25879,\"start\":25812},{\"end\":26622,\"start\":26556},{\"end\":27150,\"start\":27089},{\"end\":28228,\"start\":28166},{\"end\":28761,\"start\":28713},{\"end\":29080,\"start\":29015},{\"end\":29693,\"start\":29655},{\"end\":30241,\"start\":30212},{\"end\":30782,\"start\":30701},{\"end\":31267,\"start\":31230},{\"end\":31468,\"start\":31433},{\"end\":31745,\"start\":31730},{\"end\":32704,\"start\":32644},{\"end\":33664,\"start\":33576},{\"end\":34700,\"start\":34652},{\"end\":35216,\"start\":35174},{\"end\":36434,\"start\":36352}]", "bib_author": "[{\"end\":23436,\"start\":23422},{\"end\":23467,\"start\":23436},{\"end\":23835,\"start\":23820},{\"end\":23854,\"start\":23835},{\"end\":23867,\"start\":23854},{\"end\":24081,\"start\":24070},{\"end\":24096,\"start\":24081},{\"end\":24108,\"start\":24096},{\"end\":24125,\"start\":24108},{\"end\":24141,\"start\":24125},{\"end\":24160,\"start\":24141},{\"end\":24180,\"start\":24160},{\"end\":24194,\"start\":24180},{\"end\":24209,\"start\":24194},{\"end\":24224,\"start\":24209},{\"end\":24639,\"start\":24620},{\"end\":24656,\"start\":24639},{\"end\":24868,\"start\":24847},{\"end\":24883,\"start\":24868},{\"end\":24897,\"start\":24883},{\"end\":24912,\"start\":24897},{\"end\":24927,\"start\":24912},{\"end\":24941,\"start\":24927},{\"end\":24954,\"start\":24941},{\"end\":24965,\"start\":24954},{\"end\":24980,\"start\":24965},{\"end\":24998,\"start\":24980},{\"end\":25008,\"start\":24998},{\"end\":25359,\"start\":25348},{\"end\":25369,\"start\":25359},{\"end\":25381,\"start\":25369},{\"end\":25397,\"start\":25381},{\"end\":25406,\"start\":25397},{\"end\":25419,\"start\":25406},{\"end\":25431,\"start\":25419},{\"end\":25442,\"start\":25431},{\"end\":25456,\"start\":25442},{\"end\":25466,\"start\":25456},{\"end\":25898,\"start\":25881},{\"end\":26283,\"start\":26269},{\"end\":26299,\"start\":26283},{\"end\":26311,\"start\":26299},{\"end\":26331,\"start\":26311},{\"end\":26632,\"start\":26624},{\"end\":26647,\"start\":26632},{\"end\":26657,\"start\":26647},{\"end\":26668,\"start\":26657},{\"end\":26681,\"start\":26668},{\"end\":26701,\"start\":26681},{\"end\":26711,\"start\":26701},{\"end\":26725,\"start\":26711},{\"end\":26741,\"start\":26725},{\"end\":26751,\"start\":26741},{\"end\":26758,\"start\":26751},{\"end\":27165,\"start\":27152},{\"end\":27184,\"start\":27165},{\"end\":27200,\"start\":27184},{\"end\":27747,\"start\":27735},{\"end\":27760,\"start\":27747},{\"end\":28010,\"start\":28000},{\"end\":28244,\"start\":28230},{\"end\":28257,\"start\":28244},{\"end\":28271,\"start\":28257},{\"end\":28287,\"start\":28271},{\"end\":28781,\"start\":28763},{\"end\":28799,\"start\":28781},{\"end\":29094,\"start\":29082},{\"end\":29109,\"start\":29094},{\"end\":29119,\"start\":29109},{\"end\":29133,\"start\":29119},{\"end\":29149,\"start\":29133},{\"end\":29166,\"start\":29149},{\"end\":29175,\"start\":29166},{\"end\":29444,\"start\":29426},{\"end\":29454,\"start\":29444},{\"end\":29465,\"start\":29454},{\"end\":29480,\"start\":29465},{\"end\":29707,\"start\":29695},{\"end\":29721,\"start\":29707},{\"end\":29742,\"start\":29721},{\"end\":29750,\"start\":29742},{\"end\":29767,\"start\":29750},{\"end\":29784,\"start\":29767},{\"end\":30257,\"start\":30243},{\"end\":30274,\"start\":30257},{\"end\":30798,\"start\":30784},{\"end\":30812,\"start\":30798},{\"end\":30826,\"start\":30812},{\"end\":30841,\"start\":30826},{\"end\":30856,\"start\":30841},{\"end\":30872,\"start\":30856},{\"end\":30884,\"start\":30872},{\"end\":30892,\"start\":30884},{\"end\":30905,\"start\":30892},{\"end\":31290,\"start\":31269},{\"end\":31481,\"start\":31470},{\"end\":31507,\"start\":31481},{\"end\":31774,\"start\":31747},{\"end\":31788,\"start\":31774},{\"end\":31807,\"start\":31788},{\"end\":31830,\"start\":31807},{\"end\":31839,\"start\":31830},{\"end\":32184,\"start\":32161},{\"end\":32409,\"start\":32394},{\"end\":32424,\"start\":32409},{\"end\":32438,\"start\":32424},{\"end\":32452,\"start\":32438},{\"end\":32715,\"start\":32706},{\"end\":32729,\"start\":32715},{\"end\":32742,\"start\":32729},{\"end\":32759,\"start\":32742},{\"end\":33150,\"start\":33142},{\"end\":33168,\"start\":33150},{\"end\":33176,\"start\":33168},{\"end\":33189,\"start\":33176},{\"end\":33202,\"start\":33189},{\"end\":33209,\"start\":33202},{\"end\":33681,\"start\":33666},{\"end\":33695,\"start\":33681},{\"end\":33711,\"start\":33695},{\"end\":33732,\"start\":33711},{\"end\":33748,\"start\":33732},{\"end\":34198,\"start\":34181},{\"end\":34217,\"start\":34198},{\"end\":34229,\"start\":34217},{\"end\":34243,\"start\":34229},{\"end\":34264,\"start\":34243},{\"end\":34274,\"start\":34264},{\"end\":34288,\"start\":34274},{\"end\":34300,\"start\":34288},{\"end\":34312,\"start\":34300},{\"end\":34322,\"start\":34312},{\"end\":34326,\"start\":34322},{\"end\":34711,\"start\":34702},{\"end\":34721,\"start\":34711},{\"end\":35227,\"start\":35218},{\"end\":35237,\"start\":35227},{\"end\":35358,\"start\":35346},{\"end\":35368,\"start\":35358},{\"end\":35378,\"start\":35368},{\"end\":35662,\"start\":35651},{\"end\":35677,\"start\":35662},{\"end\":35688,\"start\":35677},{\"end\":35701,\"start\":35688},{\"end\":35716,\"start\":35701},{\"end\":35726,\"start\":35716},{\"end\":35738,\"start\":35726},{\"end\":35742,\"start\":35738},{\"end\":35752,\"start\":35742},{\"end\":35764,\"start\":35752},{\"end\":35768,\"start\":35764},{\"end\":36090,\"start\":36079},{\"end\":36099,\"start\":36090},{\"end\":36107,\"start\":36099},{\"end\":36120,\"start\":36107},{\"end\":36128,\"start\":36120},{\"end\":36142,\"start\":36128},{\"end\":36447,\"start\":36436},{\"end\":36456,\"start\":36447},{\"end\":36464,\"start\":36456},{\"end\":36477,\"start\":36464},{\"end\":36485,\"start\":36477},{\"end\":36499,\"start\":36485}]", "bib_venue": "[{\"end\":27409,\"start\":27313},{\"end\":28379,\"start\":28363},{\"end\":29889,\"start\":29860},{\"end\":30497,\"start\":30394},{\"end\":32916,\"start\":32846},{\"end\":23513,\"start\":23467},{\"end\":23818,\"start\":23739},{\"end\":24273,\"start\":24224},{\"end\":24684,\"start\":24656},{\"end\":25069,\"start\":25024},{\"end\":25559,\"start\":25482},{\"end\":25988,\"start\":25898},{\"end\":26267,\"start\":26187},{\"end\":26806,\"start\":26762},{\"end\":27311,\"start\":27200},{\"end\":27733,\"start\":27657},{\"end\":27998,\"start\":27887},{\"end\":28361,\"start\":28287},{\"end\":28845,\"start\":28799},{\"end\":29181,\"start\":29175},{\"end\":29424,\"start\":29386},{\"end\":29858,\"start\":29784},{\"end\":30392,\"start\":30274},{\"end\":30945,\"start\":30905},{\"end\":31314,\"start\":31290},{\"end\":31562,\"start\":31507},{\"end\":31866,\"start\":31839},{\"end\":32159,\"start\":32042},{\"end\":32392,\"start\":32341},{\"end\":32844,\"start\":32759},{\"end\":33337,\"start\":33225},{\"end\":33842,\"start\":33748},{\"end\":34388,\"start\":34342},{\"end\":34893,\"start\":34721},{\"end\":35241,\"start\":35237},{\"end\":35447,\"start\":35394},{\"end\":35649,\"start\":35601},{\"end\":36077,\"start\":36014},{\"end\":36534,\"start\":36499},{\"end\":37655,\"start\":36762},{\"end\":38794,\"start\":38642}]"}}}, "year": 2023, "month": 12, "day": 17}