{"id": 233024940, "updated": "2023-10-06 04:56:27.352", "metadata": {"title": "DARCNN: Domain Adaptive Region-based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images", "authors": "[{\"first\":\"Joy\",\"last\":\"Hsu\",\"middle\":[]},{\"first\":\"Wah\",\"last\":\"Chiu\",\"middle\":[]},{\"first\":\"Serena\",\"last\":\"Yeung\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 4, "day": 3}, "abstract": "In the biomedical domain, there is an abundance of dense, complex data where objects of interest may be challenging to detect or constrained by limits of human knowledge. Labelled domain specific datasets for supervised tasks are often expensive to obtain, and furthermore discovery of novel distinct objects may be desirable for unbiased scientific discovery. Therefore, we propose leveraging the wealth of annotations in benchmark computer vision datasets to conduct unsupervised instance segmentation for diverse biomedical datasets. The key obstacle is thus overcoming the large domain shift from common to biomedical images. We propose a Domain Adaptive Region-based Convolutional Neural Network (DARCNN), that adapts knowledge of object definition from COCO, a large labelled vision dataset, to multiple biomedical datasets. We introduce a domain separation module, a self-supervised representation consistency loss, and an augmented pseudo-labelling stage within DARCNN to effectively perform domain adaptation across such large domain shifts. We showcase DARCNN's performance for unsupervised instance segmentation on numerous biomedical datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.01325", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/HsuCY21", "doi": "10.1109/cvpr46437.2021.00106"}}, "content": {"source": {"pdf_hash": "b060366f771823fcaffdab95ffa2b998835f9929", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.01325v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2104.01325", "status": "GREEN"}}, "grobid": {"id": "f2cc8e56c93168e632bb4e6ede50ef3cfdc5a368", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b060366f771823fcaffdab95ffa2b998835f9929.txt", "contents": "\nDARCNN: Domain Adaptive Region-based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images\n\n\nJoy Hsu \nStanford University\nStanford University\nStanford University\n\n\nWah Chiu wahc@stanford.edu \nStanford University\nStanford University\nStanford University\n\n\nSerena Yeung syyeung@stanford.edu \nStanford University\nStanford University\nStanford University\n\n\nDARCNN: Domain Adaptive Region-based Convolutional Neural Network for Unsupervised Instance Segmentation in Biomedical Images\n\nIn the biomedical domain, there is an abundance of dense, complex data where objects of interest may be challenging to detect or constrained by limits of human knowledge. Labelled domain specific datasets for supervised tasks are often expensive to obtain, and furthermore discovery of novel distinct objects may be desirable for unbiased scientific discovery. Therefore, we propose leveraging the wealth of annotations in benchmark computer vision datasets to conduct unsupervised instance segmentation for diverse biomedical datasets. The key obstacle is thus overcoming the large domain shift from common to biomedical images. We propose a Domain Adaptive Region-based Convolutional Neural Network (DARCNN), that adapts knowledge of object definition from COCO, a large labelled vision dataset, to multiple biomedical datasets. We introduce a domain separation module, a self-supervised representation consistency loss, and an augmented pseudo-labelling stage within DARCNN to effectively perform domain adaptation across such large domain shifts. We showcase DARCNN's performance for unsupervised instance segmentation on numerous biomedical datasets.\n\nIntroduction\n\nState-of-the-art machine learning methods have accomplished a wide variety of impressive tasks including instance segmentation, yet much of their progress in the real world is limited to supervised methods with large, labelled datasets. In areas such as the biomedical domain, this is particularly problematic, as the prerequisite labels that accompany the complex data are often time consuming to obtain. In addition, we may also be constrained by human knowledge -biomedical data often contains unknown objects that scientists have yet to uncover, and therefore cannot accurately annotate.\n\nThus, there exists a need for methods that can produce instance segmentation for unlabelled datasets. We tackle this problem through solving the unsupervised do- Figure 1. a) Prior domain adaptation methods for biomedical images tackle small domain shifts by using similar labelled biomedical datasets as sources to adapt to specific target datasets. b) DAR-CNN uses a common benchmark dataset as source and can adapt to a wide range of biomedical images. main adaptation task, in which we use a source dataset with instance segmentation annotations to transfer knowledge and perform instance segmentation on target datasets.\n\nOur choice of source dataset is motivated by the abundance of benchmark datasets in the vision field depicting common objects. We explore leveraging the large amount of labelled vision data in Common Objects in Context (COCO) [20] to achieve instance segmentation in diverse, natural biomedical images where annotations are difficult to obtain. Our main contributions include overcoming the large domain shift between natural images and biomedical images, and introducing a method for unsupervised instance segmentation on a wide range of biomedical datasets.\n\nPast work tackling this problem in the biomedical field have depended on the availability of similar labelled biomedical datasets for the unsupervised instance segmentation task (see Figure 1), but it is not always feasible to find and annotate similar images. For these prior domain adaptation methods that focus on small domain shifts, joint imagelevel and feature-level adaptation approaches and object-specific models have seen success [6,7,13,17]. However, few works study unsupervised domain adaptation on large domain shifts such as from COCO to biomedical images, where such image-level adaptation fails. In addition, other past methods also design models specific to segmenting particular structures, which limits both application to other biomedical datasets as well as discovery [14,21].\n\nHence we propose Domain Adaptive Region-based Convolutional Neural Network (DARCNN), a two stage class agnostic unsupervised domain adaptation model for instance segmentation of all distinct objects, capturing the notion of objectness. DARCNN first tackles feature-level adaptation, then refines segmentation masks through imagelevel pseudo-labelling. Our method can be applied to datasets with consistent background (e.g. of homogeneous cell background in microscopy) instead of split backgrounds (e.g. of the sky and grass as commonly seen in COCO). DARCNN leverages the success of the two step Mask R-CNN framework [12] and learns domain invariant and specific features for region proposal and segmentation mask prediction. The features are learned through a self-supervised background representation consistency loss based on predicted regions within an image.\n\nIn the second stage of DARCNN, pseudo-labelling on augmented input is introduced as a strong supervisory image-level signal. Through pseudo-labelling we are able to attain stable image-level segmentation after feature-level adaptation. We discover that our sequential two stage process is able to solve the domain adaptation task with large concept shift, shown on several biomedical datasets. In addition, we demonstrate that our method achieves strong performance on tasks of smaller domain shift as well.\n\nOur key contributions are the following:\n\n\u2022 We introduce a domain separation module to learn domain invariant and domain specific features for the two step instance segmentation framework. \u2022 We propose a self-supervised representation consistency loss based on predicted regions within an image for feature adaptation. \u2022 We utilize pseudo-labelling with data augmentation within DARCNN for strong image-level supervision. \u2022 We demonstrate the effectiveness of our approach through quantitative experiments on adapting from COCO to five diverse biomedical datasets and a qualitative experiment for object discovery on a cryogenic electron tomography dataset.\n\n\nRelated Work\n\n\nUnsupervised Domain Adaptation\n\nPrior unsupervised domain adaptation approaches can be categorized into feature-level adaptation, image-level adaptation, or a combination of both. Feature-level adaptation includes minimizing distances between source and target features through extracting shared domain features [10,31], minimizing maximum mean discrepancy [23], or adversarial approaches such as [30,33].\n\nImage-level adaptation, such as those that tackle pixelto-pixel translation between source and target domains, are often evaluated on adaptations between similar domains with no concept shift, such as from Cityspaces [9] to GTA [27]. Common works include [16,36], which conducts image-to-image translation through generative adversarial networks. However, approaches such as as pixel-to-pixel translation are extremely limited by size of domain shift.\n\nSeveral key domain adaptive methods formulates adaptation across both image-level and feature-level, including [13,17], while others approach domain shift on the instance-level and image-level as well [7]. These methods conduct feature-level and image-level adaptation jointly or rely heavily on image-level adaptation, which has seen impressive results in adaptation with small domain shifts, but struggle with larger concept shifts. DARCNN conducts first feature-level adaptation then image-level refinement sequentially in a two stage process, overcoming limitations of prior works.\n\nIn addition, a line of prior work that tackles small domain shift across biomedical images has shown strong performance on specific datasets. [6] transforms appearance of MR and CT images through synergistic fusion of adaptations from both feature-level and image-level, and [14] generates synthesized nucleus segmentation masks with importance weighting. [21] similarly introduces a nuclei inpainting mechanism for unsupervised nucleus segmentation through domain adaptation with re-weighting. However, these works tackle specific biomedical datasets, where methods can be crafted for detection of specific objects, and are also limited to smaller domain shifts, where an additional labelled biomedical dataset is needed for adaptation.\n\nOur work overcomes the limitations of small domain shift and object-specific techniques for biomedical datasets through a two stage feature-level adaptation and imagelevel pseudo-labelling that segments all objects of interests. Previous works such as [5] has shown success in domain separation networks in simpler classification settings; similarly, we introduce a domain adaptation module and integrate this with a self-supervised loss that learns feature discriminability for instance segmentation.\n\n\nUnsupervised Background-Foreground Segmentation\n\nPrior works on unsupervised background-foreground segmentation primarily use a combination of consistency constraints and domain-specific assumptions. For example, [34] focuses on consistency between generated image and outputs of edge detectors, [28] leverages salient pixels in the foreground and matching foregrounds between different images, and [3] utilizes a multi-task formulation with need for clean background images.\n\nDARCNN similarly takes a self-supervised approach by maintaining a background representation consistency constraint, leveraging proposed regions within each image. Through this objective, our approach is able to learn domain invariant and domain specific representation for segmentation.\n\n\nPseudo-labels\n\nPseudo-labelling has often been used as a technique for utilizing unlabelled data in semi-supervised training. Prior work have chosen maximum predicted probability labels [19], uncertainty weighted class predictions [29], and used group-based label propagation [15]. Similarly, co-training methods use an ensemble of models to find labels through consistency regularization [26]. Fewer methods have used this method in the unsupervised setting, though works such as [8] have used high density clusters as pseudo-labels, inferring high confidence without supervision. After first stage feature-level adaptation, our unsupervised method uses pseudo-labels to gain stronger imagelevel supervision, where high confidence pseudo-labels comes from first stage DARCNN. In addition, to learn across invariances, we add data augmentation to the unlabelled target images such as in [4], allowing DARCNN to learn across different imaging conditions.\n\n\nMethods\n\nWe propose methods for unsupervised instance segmentation through the task of domain adaptation with large concept shift. In this section, we describe our two stage DARCNN model. The initial stage of feature-level adaptation consists of a domain separation module and a selfsupervised representation consistency loss, which can be found in Section 3.1 and Section 3.2. The second stage of image-level pseudo-labelling can be found in Section 3.3.\n\nWe train each stage separately to tackle our problem of large domain shift. Image-level adaptation such as pixelto-pixel translation does not work on such extreme concept shift, thus sequentially using image-level pseudo-labelling as a second stage allows for features to first learn to adapt between domains, before augmenting training with stronger pixel-level supervisory signal. DARCNN is pre-trained with source dataset weights, and jointly trains with a batch of source and target inputs. See Figure 2 for an overview of our model.\n\n\nDomain Separation Module\n\nThe Mask R-CNN framework [12] is a powerful instance segmentation model, and we leverage its two step framework for DARCNN as well as propose a domain separation module designed for our task. The region proposal network from the first step finds potential bounding boxes of interest given features learned through convolutional layers, while the mask prediction head from the second step refines these boxes and produces a mask for each instance.\n\nTo tackle the problem of domain shift, we propose a domain separation module that learns domain invariant and domain specific features as input into the region proposal network and mask segmentation network. The domain invariant features encode objectness of the source and target domain in a joint representational subspace, while the domain specific features capture discriminability of each domain as well as contain additional unconstrained embedding space.\n\nThe losses of the DARCNN are: L sim to encourage domain invariant features, L diff to learn domain specific features, L source which includes the original Mask R-CNN losses for supervised training of the source dataset, and our proposed L target for segmentation through a self-supervised consistency loss. Weighting factors \u03b1, \u03b2, and \u03b3 are used to balance the loss. See Equation 1 below.\nL DARCNN = \u03b1L sim + \u03b2L diff + \u03b3L target + L source (1)\n\nDomain Invariant Features\n\nIntuitively, region proposals should be based on high level definition of objectness in the input image shared between both domains. Hence we encourage source and target domain invariant features to move into a joint representational subspace. The similarity loss helps the unlabelled target features better encode the objectness learned from the labelled source features. We utilize the maximum mean discrepancy loss L sim from Equation 2 below. The maximum mean discrepancy loss [23,5] is a kernel-based distance function between pairs of samples; we can think of the loss as computing the difference in distribution s and t where source inputs are drawn from s and target inputs are drawn from t. We let \u03ba be our kernel function, and h s c and h t c be our shared source and shared target features respectively.\nL sim = 1 (N s ) 2 N s i,j=0 \u03ba(h s,i c , h s,j c ) (2) \u2212 2 N s N t N s N t i,j=0 \u03ba(h s,i c , h t,j c ) + 1 (N t ) 2 N t i,j=0 \u03ba(h t,i c , h t,j c )\nIn our implementation of maximum mean discrepancy, we use a Gaussian kernel \u03ba. We downsample h s c and h t c with a dimension reduction convolutional layer with 1 filter of 1x1 kernel as an additional projection head to learn source and target domain invariant features. However, as we focus on scenarios where domain shift between our source and target domains is large, the initial distance between s and t as calculated by the maximum mean discrepancy loss is also large. [32] shows that minimizing the maximum mean discrepancy loss equates to maximizing the source and target intra-class distances respectively, but doing so also jointly minimizes their variance with some implicit weights, such that feature discriminability degrades. Therefore, if L sim too quickly overwhelms other losses in DARCNN that retain semantic features, the discriminability needed for instance segmentation is lost.\n\nHence we propose a maximum mean discrepancy loss that uses a warmup weighting scheduler. Our approach increases the weight \u03b1 of L sim from \u03b1 0 to \u03b1 over n epochs, where \u03b1 0 is smaller when the domain shift is larger.\n\n\nDomain Specific Features\n\nWe next consider input needed for mask predictions. Domain specific features captures feature discriminability for the target and source domains as well as granularity of the background representation for our self-supervised loss. Hence we use L diff to separate information that is unique to each domain as well as learn specificity, and define the loss through the soft subspace orthogonality constraint from [5] between the domain invariant and domain specific features of both domains.\n\nHowever, in order to let part of the mask feature representations learn semantically relevant embeddings that may potentially be domain invariant, we utilize the orthogonality difference loss only between parts of the domain invariant and domain specific features. We give DARCNN the freedom to learn features necessary for segmentation in an unconstrained embedding space, whether it be domain invariant or specific. In our implementation, we use half of the feature depth. See top right corner of Figure 2. We let H be matrices whose rows are half of hidden representations h in depth, where h s c and h s p are the invariant and specific features of the source, and h t c and h t p of the target.\nL diff = ||H s c H s p || 2 F + ||H t c H t p || 2 F(3)\nTo additionally give signal to the unlabelled target domain specific features, we propose a self-supervised representation consistency loss instead of the reconstruction loss used for classification in [5]. The source domain specific features are supervised by the original Mask R-CNN bounding box and mask losses.\n\n\nSelf-Supervised Representation Consistency\n\nBiomedical images commonly contain homogeneous backgrounds, therefore we leverage this assumption between region proposals of the same image to self-supervise our feature representations. In contrast to approaches that define a global background consistency across images of a dataset, we use independent background consistency for each image, which allows for variation in backgrounds within the dataset. We leverage the region proposal network and minimize the differences between background representations of each predicted instance. See Figure 2.\n\nWe accomplish this through the two step framework of Mask R-CNN. DARCNN utilizes self-supervision during training through first finding the top region proposals with confidence over threshold k from the region proposal network. It passes each high confidence region proposal through the class agnostic mask head and determines which parts of the predicted instance are background. To do this, outputs from the mask head are passed through a sigmoid activation \u03c3 and all values less than threshold i are taken as background.\n\nThen, we retrieve features output from our convolutional encoder E t p that corresponds to background predictions. We minimize the differences between these background representations. Letting r be a predicted region from shared region proposal network R and and M t be our target mask prediction head, we define the background features b(r) in Equation 4, where the indicator function extracts parts of h t p , domain specific features, that spatially corresponds to background mask predictions, threshold by value i after sigmoid function \u03c3. We can then define \u00b5 b , computed per image, as the mean of background representations across all regions in an image. These regions are predicted by region proposal network R after taking domain invariant features h t c as inputs. Finally, we define L target to minimize differences between background features.\nb(r) = 1[\u03c3(M t (r)) < i]h t p (4) \u00b5 b = 1 N p p\u2208R(h t c ) b(p)(5)L target = 1 N r r\u2208R(h t c ) | b(r) \u2212 \u00b5 b |(6)\nIn our training process, the combination of the fully supervised and self-supervised losses from the source and target dataset respectively allows DARCNN to learn semantically relevant proposals and mask predictions.\n\n\nAugmented Pseudo-Labelling\n\nOur first stage DARCNN utilizes feature-level adaptations for unsupervised domain adaptation and leads to initial coarse mask predictions that overcome large domain shift. However, it lacks strong image-level supervisory signal as in [13] or [7]; the lack of a pixel-level signal leads to more unstable and unrefined segmentations. Therefore we propose a second stage image-level pseudo-labelling with our first stage DARCNN's output as pseudo-labels in order to gain this image-level supervision. See part b) in Figure 2. Only the target branch of DARCNN is trained during the second stage pseudo-labelling process, while the source branch is frozen and no longer needed.\n\nCanonical use of pseudo-labels depends on some amount of labelled data, however, as our method is unsupervised, we instead use high confidence predictions from our first stage DARCNN. We use confidence threshold z to determine which labels to retrieve from the predictions.\n\nIn addition, to better learn invariances of labels despite imaging conditions, including quality and noise, we apply data augmentation procedures to strengthen pseudo-labels from the first stage DARCNN. The augmentations ensure that the same instance segmentations will be predicted of a given input regardless of lighting, contrast, and blur.\n\nThe target branch of DARCNN is the final model to be used for unsupervised instance segmentation.\n\n\nExperiments\n\nIn our experiments, we show that we are able to overcome limitations of past work to adapt between large domain shifts, as well generalize across many biomedical datasets. We quantitatively demonstrate DARCNN's performance on a large domain shift from COCO [20] to multiple biomedical datasets, bypassing drawbacks of previous literature that focuses on small domain shifts and specific datasets. In addition, in order to directly compare against existing work, we also show comparable performance to prior methods on tasks with small domain shifts following canonical biomedical adaptations of [21].\n\nOur work is not limited to that of biomedical datasets, and is designed for all datasets with consistent backgrounds. We use biomedical datasets due to prevalence of homogeneous backgrounds in the biomedical field, for evaluation following previous unsupervised instance segmentation work [21], and to better illustrate our approach on large domain shifts from generalized COCO to diverse, biomedical datasets.\n\n\nImplementation details\n\nWe evaluate our experiments on Aggregated Jaccard Index (AJI) [18]. AJI is used by prior work to evaluate the performance of instance segmentation; it computes an aggregated intersection cardinality numerator, and an aggregated union cardinality denominator for all ground truth and segmented predictions under consideration. It is a unified metric that measures both object-level and pixel-level performance, and is more stringent than other canonical metrics such as IOU. In addition, we also show pixel F1 score and object F1 score to measure performance in specific aspects. The annotations for all five target biomedical datasets are not used during unsupervised training of DARCNN, only for evaluation. For our experiments, we stop our model training 0.1 epochs before loss plateaus.\n\nWe use Pytorch and build on the Detectron2 [35] framework, using the ResNet backbone. For our loss function L DARCNN , we set \u03b1 to increase over the first 0.1 epochs to 1, \u03b2 = 1 and \u03b3 = 0.1. We use k = 0.5 as the confidence threshold for top predicted regions in our self-supervised loss, and i = 0.5 as our threshold for background. Confidence threshold z is set to be 0.5 for pseudo-labelling. Augmentation parameters used are Gaussian blur with sigma as 1, and contrast and brightness are changed through scaling and delta factors 1.5 and \u2212150 respectively. We use learning rate 0.0001, and vary maximum number of detections to return per image during inference through initial coarse inspection of training images. We choose the number to be 100 or 50 accordingly for each dataset.\n\n\nAdaptation from Microscopy to Histopathology\n\nTo compare against prior unsupervised domain adaptation methods tackling small domain shifts between biomedical images, we first quantitatively evaluate adaptation from a fluorescence microscopy dataset, BBBC [22], to two histopathology datasets, Kumar [18] and TNBC [25]. This comparison follows that of Liu et al. [21], and we follow the same implementations of prior work and evaluation.\n\nImportantly, we also demonstrate DARCNN's strong performance when adapting from a common dataset, COCO [20], to the same two histopathology datasets. We demonstrate that even without a similar source biomedical dataset that may be difficult to obtain, we are still able to conduct unsupervised instance segmentation adapting from COCO.\n\nWe first preprocess our source dataset, BBBC. A total of 100 training images and 50 validation images from BBBC are used, following the official data split. 10, 000 patches of BBBC in size 256x256 are randomly cropped from the 100 training images, and pixel values are inverted to better synthesize histopathology images following [21]. Both Kumar [18] and TNBC [25], our target datasets, are trained with 10, 000 patches of size 256x256 without any labels, and evaluated on the specified test set. To compare our method against the nucleus specific methods of [14] and [21], we utilize the standalone, non-deep learning based unsupervised synthesis module of [14] as additional input.\n\nWe can see in Table 1 that for the TNBC dataset in the scenario of a small domain shift, our method with fluorescence microscopy as source outperforms all prior work aside from [21]. [21] utilizes specific nuclei inpainting, which yields impressive performance, but can only be used for nuclei segmentation tasks.\n\n\nMethod\n\nAJI Pixel-F1 Object-F1 Chen et al. [7] 0.4407 0.6405 0.6289 DDMRL [17] 0.4642 0.7000 0.6872 SIFA [6] 0.4662 0.6994 0.6698 CyCADA [13] 0.4721 0.7048 0.6866 Hou et al. [14] 0.4775 0.7029 0.6779 Liu et al. [21] 0.5672 0.7593 0.7478 Ours from BBBC 0.5120 0.7175 0.6436 Ours from COCO 0.4906 0.6998 0.6396 Table 1. Comparison of unsupervised methods adapting BBBC to TNBC. We see that DARCNN shows strong performance against prior work both with BBBC as source and with COCO as source, even against methods designed for nucleus-specific segmentation [14,21] and small domain shifts [7,17,6,13].\n\nMethod AJI Pixel-F1 Object-F1 Chen et al. [7] 0.3756 0.6337 0.5737 SIFA [6] 0.3924 0.6880 0.6008 CyCADA [13] 0.4447 0.7220 0.6567 DDMRL [17] 0.4860 0.7109 0.6833 Hou et al. [14] 0.4980 0.7500 0.6890 Liu et al. [21] 0 More importantly, we show that DARCNN with COCO as the source dataset is able to achieve similar performance, also outperforming other methods aside from [21]. Without labels from BBBC, we are still able to adapt from COCO to histopathology datasets, which is essential in cases where similar labelled biomedical datasets do not exist.\n\nIn Table 2, we see that for the Kumar dataset, due to DARCNN predicting more objects than are classified as nucleus, our scores are comparable but do not beat all of prior methods. We hypothesis that this is because past work focuses on image-level adaptation at small domain shifts, and their models are able to better learn what the object of focus is for segmentation. DARCNN is designed for instance segmentation of all distinct objects that exist within an image, hence predicts false positives from the nucleus perspective. See below Figure 3 for an example of DARCNN adapted from COCO to Kumar; note that the red objects in the middle are not considered nucleus from the ground truth, but are segmented by DARCNN as objects of interest. DAR-CNN is useful for discovering objects in data when no labels are available, and produces class agnostic instance segmentations that can help scientists uncover new objects in complex data. Postprocessing methods could allow us to gain more insight into objects found by DARCNN. Through rule-based filtering with a coarse prior (filtering for masks with average pixel value less than a threshold, over a set of thresholds), our method obtains AJI = 0.5026, Pixel-F1 = 0.7272, Object-F1 = 0.6481, outperforming all prior work except [21], which is designed for nuclei segmentation.\n\n\nAblation Studies\n\nWe conduct ablation studies on DARCNN to showcase the effectiveness of the domain similarity loss, the background representation consistency loss, and the augmented pseudo-labelling stage. The DARCNN here is shown with COCO [20] as the source dataset, tackling our stated problem of large domain shift to biomedical datasets.\n\nWe perform this ablation study for two separate setupsfirst we study DARCNN adapted to TNBC, trained with the unsupervised, standalone synthesis module of [14], which allows for its comparable performance to previous nucleusspecific methods. Then we observe DARCNN's ablation performance for the BBBC dataset, showcasing each component's contribution to DARCNN without initial objectspecific synthesis.\n\nIn Table 3 and Table 4, we observe that class agnostic Mask R-CNN trained on COCO images performs extremely poorly on biomedical datasets. We can also see that for the first stage of DARCNN, both the domain similarity loss and representation consistency loss improved model performance. Especially in the case of BBBC without the unsupervised synthesis module, our representation consistency loss dramatically improves the performance of DARCNN due to ability to gain self-supervised signal for features. The full first stage DARCNN also shows significantly higher performance in both datasets than the initial baselines.\n\nIn the second stage of DARCNN, pseudo-labelling also improved our unsupervised instance segmentation performance. Similarly, the performance improvement from pseudo-labelling in the BBBC case without the synthesis module is larger than in TNBC, as for BBBC it is the first time DARCNN's target branch receives image-level supervision. Pseudo-labelling with augmented data demonstrates even better performance, helping remove difficulties in segmentation under various imaging conditions of biomedical datasets. We also show in Figure 3 a qualitative example of DARCCN adapting from COCO to TNBC, capturing nuclei as objects of interests in the image.\n\n\nAdaptation from COCO to Additional Biomedical Datasets\n\nMost importantly, we demonstrate DARCNN's ability to generalize across datasets by comparing performance on COCO adapted to three diverse biomedical datasets -the fluorescence microscopy dataset [22], cryogenic electron tomography dataset [11], and brain MRI dataset [24,1,2]. Through this, we demonstrate potential for object discovery without the need for similar labelled datasets to the target domain, and without need for designing object-specific models for segmentation of particular biomedical datasets.\n\nWe compare our results with the prior methods that can be used outside of specific biomedical datasets. [6,14,21] depend on specific biomedical images and nuclei synthesis methods, hence we do not use them as comparison.\n\nDARCNN significantly outperforms all other methods when adapting with a large domain shift from COCO. As [13,17] depend on CycleGAN [36], and [7] depends on image-level shift, we hypothesize that these methods per-  [7] 0.0 0.0 0.0 CyCADA [13] 0.0051 0.0064 0.0025 DDMRL [17] 0.0039 0.0182 0.0 Ours 0.1268 0.3007 0.3371 BraTS Chen et al. [7] 0.2868* --CyCADA [13] 0.3485* --DDMRL [17] 0.3951* --Ours 0.5577* -- Table 5. Comparison of unsupervised methods adapting from COCO to fluorescence microscopy (BBBC), cryogenic electron tomography (SHREC), and radiology (BraTS) datasets. *Indicates metric measuring maximum intersection over union.\n\nform poorly when tasked with a large domain shift from COCO to natural biomedical images. DARCNN is able to generalize across diverse biomedical datasets and adapt between common objects to objects in microscopy, tomography, and MRI due to its two stage sequential feature-level adaptation and image-level pseudo-labelling. For BBBC, DARCNN is able to significantly outperform prior work as the background consistency assumption is strong. In addition, even for a more challenging task like instance segmentation in SHREC where the signal to noise ratio is low, DARCNN is still able to capture objects of interests given noisy background. Prior work, CyCADA [13], Chen et al. [7], and DDMRL [17], are all unable to learn meaningful adaptations from COCO to SHREC due to the difficulties of image-level adaptation when even human recognition is limited. In the BraTS dataset, due to the specificity of the detected object, tumor, we measure performance by maximum intersection over union from closest predicted object. Though this does not account for unbounded false positives, we provide a qualitative example of performance to supplement. In Figure 4, we see that DAR-CNN also picks up on ridges and darker spots in the BraTS MRI, which could potentially be useful in understanding additional structures of interest. We show qualitative examples on our three biomedical datasets in Figure 4.\n\n\nAdaptation from COCO to CryoET\n\nFinally, we showcase the promise of our unsupervised instance segmentation model for adapting from COCO to an unlabelled cryogenic electron tomography (cryoET) dataset, collected by Dr. Wah Chiu's group at SLAC National Accelerator Laboratory. This cryoET dataset contains tomograms of crowded cellular environments in which objects are too dense and of too underexplored a subject area to be annotated. We qualitatively evaluate the performance of DARCNN instance segmentations in this dataset. In Figure 5, our unsupervised algorithm segments known biological objects such as the autophagosome and granules. In addition, DARCNN also discovers distinct vesicles and organelles inside the amphisome, difficult for humans to annotate, and representing potential new objects of interest. \n\n\nConclusion\n\nWe propose DARCNN, a two stage feature-level adaptation and image-level pseudo-labelling method for unsupervised instance segmentation. We leverage the abundance of labelled benchmark datasets for domain adaptation to unlabelled biomedical images. DARCNN tackles large domain shifts between common and biomedical objects, and can be used across diverse datasets with consistent background. Through a domain separation module, a representation consistency loss, and augmented pseudo-labelling, we achieve strong performance in multiple experiments as well as show potential for object discovery within biomedical datasets.\n\nFigure 2 .\n2a) First stage DARCNN model with a domain separation module and self-supervised representation consistency loss. Let s and t represent source and target, and hc and hp represent common domain invariant features and private domain specific features respectively. Ec is the shared encoder, E s p and E t p are domain specific encoders, R is the shared region proposal network, and M s and M t are domain specific mask prediction heads. We let b(r) be extracted background features for each region r. Top right corner showcases our soft orthogonality constraint on half of the domain specific features. b) Second stage DARCNN model with pseudo-labels of augmented input from first stage DARCNN, with annotations chosen over a confidence threshold, continuing training of DARCNN's target branch.\n\nFigure 3 .\n3Qualitative results on DARCNN, adaptation from COCO to TNBC (top) & Kumar (bottom) respectively. Left is input, middle is ground truth, and right shows instance segmentation result.\n\nFigure 4 .\n4Qualitative results on DARCNN, adaptation from COCO to BBBC (top), SHREC (middle), and BraTS (bottom) datasets.\n\nFigure 5 .\n5Qualitative results on DARCNN, adaptation from COCO to cryoET, demonstrating discovery of distinct objects of interests.\n\n\nTable 3. Ablation study adapting from COCO as source to TNBC.Table 4. Ablation study adapting from COCO as source to BBBC.Method \nAJI \nPixel-F1 Object-F1 \nMask R-CNN \nw/ COCO pre-trained \n0.0060 \n0.2769 \n0.0181 \nw/ synthesized images \n0.3332 \n0.5782 \n0.6061 \nFirst stage DARCNN \nDomain sim. only \n0.3687 \n0.6023 \n0.6099 \nBg. consistency only \n0.3808 \n0.6120 \n0.5470 \nFull 1st stage DARCNN \n0.4071 \n0.6353 \n0.5986 \nSecond stage DARCNN \nPseudo-label w/o aug \n0.4463 \n0.6781 \n0.6339 \nFull 2nd stage DARCNN 0.4906 \n0.6998 \n0.6396 \n\nMethod \nAJI \nPixel-F1 Object-F1 \nMask R-CNN \nw/ COCO pre-trained \n0.0315 \n0.3144 \n0.0818 \nFirst stage DARCNN \nDomain sim. only \n0.1414 \n0.4905 \n0.4295 \nBg. consistency only \n0.3250 \n0.7128 \n0.5720 \nFull 1st stage DARCNN \n0.3371 \n0.6409 \n0.5904 \nSecond stage DARCNN \nPseudo-label w/o aug \n0.4349 \n0.6914 \n0.7151 \nFull 2nd stage DARCNN 0.4725 \n0.6586 \n0.6733 \n\n\nAcknowledgments. We thank support from the Chan-Zuckerberg Initiative (to J.H., S.Y. and W.C.) and National Institutes of Health (grant number P01NS092525 to W.C.).\nAdvancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features. Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby, John B Freymann, Keyvan Farahani, Christos Davatzikos, Scientific data. 4170117Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby, John B Freymann, Keyvan Farahani, and Christos Davatzikos. Advancing the cancer genome atlas glioma mri collections with expert seg- mentation labels and radiomic features. Scientific data, 4:170117, 2017.\n\nIdentifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge. Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempfler, Alessandro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki, arXiv:1811.02629arXiv preprintSpyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempfler, Alessandro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki, et al. Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and over- all survival prediction in the brats challenge. arXiv preprint arXiv:1811.02629, 2018.\n\nOnegan: Simultaneous unsupervised learning of conditional image generation, foreground segmentation, and fine-grained clustering. Yaniv Benny, Lior Wolf, arXiv:1912.13471arXiv preprintYaniv Benny and Lior Wolf. Onegan: Simultaneous unsuper- vised learning of conditional image generation, foreground segmentation, and fine-grained clustering. arXiv preprint arXiv:1912.13471, 2019.\n\nMixmatch: A holistic approach to semi-supervised learning. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin A Raffel, Advances in Neural Information Processing Systems. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Ad- vances in Neural Information Processing Systems, pages 5049-5059, 2019.\n\nDomain separation networks. Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan, Advances in neural information processing systems. Konstantinos Bousmalis, George Trigeorgis, Nathan Silber- man, Dilip Krishnan, and Dumitru Erhan. Domain separa- tion networks. In Advances in neural information processing systems, pages 343-351, 2016.\n\nSynergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation. Cheng Chen, Qi Dou, Hao Chen, Jing Qin, Pheng-Ann Heng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Cheng Chen, Qi Dou, Hao Chen, Jing Qin, and Pheng-Ann Heng. Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image seg- mentation. In Proceedings of the AAAI Conference on Ar- tificial Intelligence, volume 33, pages 865-872, 2019.\n\nDomain adaptive faster r-cnn for object detection in the wild. Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster r-cnn for object de- tection in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3339-3348, 2018.\n\nPseudo-labeling curriculum for unsupervised domain adaptation. Jaehoon Choi, Minki Jeong, Taekyung Kim, Changick Kim, arXiv:1908.00262arXiv preprintJaehoon Choi, Minki Jeong, Taekyung Kim, and Changick Kim. Pseudo-labeling curriculum for unsupervised domain adaptation. arXiv preprint arXiv:1908.00262, 2019.\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 3213-3223, 2016.\n\nUnsupervised domain adaptation by backpropagation. Yaroslav Ganin, Victor Lempitsky, International conference on machine learning. PMLRYaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180-1189. PMLR, 2015.\n\nShrec 2020: Classification in cryo-electron tomograms. Ilja Gubins, Marten L Chaillet, Gijs Van Der, Schot, C Remco, Friedrich Veltkamp, Yu F\u00f6rster, Xiaohua Hao, Xuefeng Wan, Fa Cui, Emmanuel Zhang, Moebel, Computers & Graphics. 91Ilja Gubins, Marten L Chaillet, Gijs van der Schot, Remco C Veltkamp, Friedrich F\u00f6rster, Yu Hao, Xiaohua Wan, Xuefeng Cui, Fa Zhang, Emmanuel Moebel, et al. Shrec 2020: Clas- sification in cryo-electron tomograms. Computers & Graph- ics, 91:279-289, 2020.\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017.\n\nCycada: Cycle-consistent adversarial domain adaptation. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, Trevor Darrell, International conference on machine learning. PMLRJudy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International conference on machine learning, pages 1989- 1998. PMLR, 2018.\n\nRobust histopathology image analysis: to label or to synthesize?. Le Hou, Ayush Agarwal, Dimitris Samaras, M Tahsin, Kurc, R Rajarsi, Joel H Gupta, Saltz, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLe Hou, Ayush Agarwal, Dimitris Samaras, Tahsin M Kurc, Rajarsi R Gupta, and Joel H Saltz. Robust histopathology image analysis: to label or to synthesize? In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8533-8542, 2019.\n\nLabel propagation for deep semi-supervised learning. Ahmet Iscen, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionGiorgos Tolias, Yannis Avrithis, and Ondrej ChumAhmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5070-5079, 2019.\n\nImage-to-image translation with conditional adversarial networks. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adver- sarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125-1134, 2017.\n\nDiversify and match: A domain adaptive representation learning paradigm for object detection. Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, Changick Kim, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionTaekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, and Changick Kim. Diversify and match: A domain adaptive representation learning paradigm for object detec- tion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12456-12465, 2019.\n\nA dataset and a technique for generalized nuclear segmentation for computational pathology. Neeraj Kumar, Ruchika Verma, Sanuj Sharma, Surabhi Bhargava, Abhishek Vahadane, Amit Sethi, IEEE transactions on medical imaging. 367Neeraj Kumar, Ruchika Verma, Sanuj Sharma, Surabhi Bhargava, Abhishek Vahadane, and Amit Sethi. A dataset and a technique for generalized nuclear segmentation for computational pathology. IEEE transactions on medical imaging, 36(7):1550-1560, 2017.\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Dong-Hyun Lee, Workshop on challenges in representation learning, ICML. 3Dong-Hyun Lee. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, 2013.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. SpringerTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.\n\nUnsupervised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting. Dongnan Liu, Donghao Zhang, Yang Song, Fan Zhang, O&apos; Lauren, Heng Donnell, Mei Huang, Weidong Chen, Cai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionDongnan Liu, Donghao Zhang, Yang Song, Fan Zhang, Lau- ren O'Donnell, Heng Huang, Mei Chen, and Weidong Cai. Unsupervised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4243-4252, 2020.\n\nAnnotated high-throughput microscopy image sets for validation. Vebjorn Ljosa, Katherine L Sokolnicki, Anne E Carpenter, Nature methods. 97Vebjorn Ljosa, Katherine L Sokolnicki, and Anne E Carpen- ter. Annotated high-throughput microscopy image sets for validation. Nature methods, 9(7):637-637, 2012.\n\nLearning transferable features with deep adaptation networks. Mingsheng Long, Yue Cao, Jianmin Wang, Michael Jordan, International conference on machine learning. PMLRMingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor- dan. Learning transferable features with deep adaptation net- works. In International conference on machine learning, pages 97-105. PMLR, 2015.\n\nThe multimodal brain tumor image segmentation benchmark (brats). Andras Bjoern H Menze, Stefan Jakab, Jayashree Bauer, Keyvan Kalpathy-Cramer, Justin Farahani, Yuliya Kirby, Nicole Burren, Johannes Porz, Roland Slotboom, Wiest, IEEE transactions on medical imaging. 3410Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The multimodal brain tumor image segmentation benchmark (brats). IEEE transactions on medical imaging, 34(10):1993-2024, 2014.\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. Peter Naylor, Marick La\u00e9, Fabien Reyal, Thomas Walter, IEEE transactions on medical imaging. 382Peter Naylor, Marick La\u00e9, Fabien Reyal, and Thomas Walter. Segmentation of nuclei in histopathology images by deep re- gression of the distance map. IEEE transactions on medical imaging, 38(2):448-459, 2018.\n\nDeep co-training for semi-supervised image recognition. Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, Alan Yuille, Proceedings of the european conference on computer vision (eccv). the european conference on computer vision (eccv)Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan Yuille. Deep co-training for semi-supervised image recogni- tion. In Proceedings of the european conference on computer vision (eccv), pages 135-152, 2018.\n\nPlaying for data: Ground truth from computer games. Vibhav Stephan R Richter, Stefan Vineet, Vladlen Roth, Koltun, European conference on computer vision. SpringerStephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In European conference on computer vision, pages 102-118. Springer, 2016.\n\nUnsupervised joint object discovery and segmentation in internet images. Michael Rubinstein, Armand Joulin, Johannes Kopf, Ce Liu, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMichael Rubinstein, Armand Joulin, Johannes Kopf, and Ce Liu. Unsupervised joint object discovery and segmentation in internet images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1939-1946, 2013.\n\nTransductive semisupervised deep learning using min-max features. Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng Maxiaoyu Tao, Nanning Zheng, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng MaX- iaoyu Tao, and Nanning Zheng. Transductive semi- supervised deep learning using min-max features. In Pro- ceedings of the European Conference on Computer Vision (ECCV), pages 299-315, 2018.\n\nReturn of frustratingly easy domain adaptation. Baochen Sun, Jiashi Feng, Kate Saenko, arXiv:1511.05547arXiv preprintBaochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. arXiv preprint arXiv:1511.05547, 2015.\n\nAdversarial discriminative domain adaptation. Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionEric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 7167-7176, 2017.\n\nRethink maximum mean discrepancy for domain adaptation. Wei Wang, Haojie Li, Zhengming Ding, Zhihui Wang, arXiv:2007.00689arXiv preprintWei Wang, Haojie Li, Zhengming Ding, and Zhihui Wang. Rethink maximum mean discrepancy for domain adaptation. arXiv preprint arXiv:2007.00689, 2020.\n\nExploiting local feature patterns for unsupervised domain adaptation. Jun Wen, Risheng Liu, Nenggan Zheng, Qian Zheng, Zhefeng Gong, Junsong Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Jun Wen, Risheng Liu, Nenggan Zheng, Qian Zheng, Zhe- feng Gong, and Junsong Yuan. Exploiting local feature pat- terns for unsupervised domain adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5401-5408, 2019.\n\nLocus: Learning object classes with unsupervised segmentation. John Winn, Nebojsa Jojic, Tenth IEEE International Conference on Computer Vision (ICCV'05. IEEE1John Winn and Nebojsa Jojic. Locus: Learning object classes with unsupervised segmentation. In Tenth IEEE In- ternational Conference on Computer Vision (ICCV'05) Vol- ume 1, volume 1, pages 756-763. IEEE, 2005.\n\n. Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick, Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2, 2019.\n\nUnpaired image-to-image translation using cycleconsistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle- consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223- 2232, 2017.\n", "annotations": {"author": "[{\"end\":199,\"start\":129},{\"end\":289,\"start\":200},{\"end\":386,\"start\":290}]", "publisher": null, "author_last_name": "[{\"end\":136,\"start\":133},{\"end\":208,\"start\":204},{\"end\":302,\"start\":297}]", "author_first_name": "[{\"end\":132,\"start\":129},{\"end\":203,\"start\":200},{\"end\":296,\"start\":290}]", "author_affiliation": "[{\"end\":198,\"start\":138},{\"end\":288,\"start\":228},{\"end\":385,\"start\":325}]", "title": "[{\"end\":126,\"start\":1},{\"end\":512,\"start\":387}]", "venue": null, "abstract": "[{\"end\":1669,\"start\":514}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3135,\"start\":3131},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3909,\"start\":3906},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3911,\"start\":3909},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3914,\"start\":3911},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3917,\"start\":3914},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4260,\"start\":4256},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4263,\"start\":4260},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4888,\"start\":4884},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6632,\"start\":6628},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6635,\"start\":6632},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6677,\"start\":6673},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6717,\"start\":6713},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6720,\"start\":6717},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6943,\"start\":6940},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6955,\"start\":6951},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6982,\"start\":6978},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6985,\"start\":6982},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7291,\"start\":7287},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7294,\"start\":7291},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7380,\"start\":7377},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7908,\"start\":7905},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8042,\"start\":8038},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8123,\"start\":8119},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8757,\"start\":8754},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9223,\"start\":9219},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9306,\"start\":9302},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9963,\"start\":9959},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10008,\"start\":10004},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10053,\"start\":10049},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10166,\"start\":10162},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10257,\"start\":10254},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10663,\"start\":10660},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11781,\"start\":11777},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13620,\"start\":13616},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13622,\"start\":13620},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14577,\"start\":14573},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15658,\"start\":15655},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16696,\"start\":16693},{\"end\":18285,\"start\":18275},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19384,\"start\":19380},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19391,\"start\":19388},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20814,\"start\":20810},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21152,\"start\":21148},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21448,\"start\":21444},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21658,\"start\":21654},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22430,\"start\":22426},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23430,\"start\":23426},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23474,\"start\":23470},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23488,\"start\":23484},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23537,\"start\":23533},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23716,\"start\":23712},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24281,\"start\":24277},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24298,\"start\":24294},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24312,\"start\":24308},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24511,\"start\":24507},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24520,\"start\":24516},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24610,\"start\":24606},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24814,\"start\":24810},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24820,\"start\":24816},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24995,\"start\":24992},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25027,\"start\":25023},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25057,\"start\":25054},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25090,\"start\":25086},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25127,\"start\":25123},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25164,\"start\":25160},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25506,\"start\":25502},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25509,\"start\":25506},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25537,\"start\":25534},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25540,\"start\":25537},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25542,\"start\":25540},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25545,\"start\":25542},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25593,\"start\":25590},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25623,\"start\":25620},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25656,\"start\":25652},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25688,\"start\":25684},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25725,\"start\":25721},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25762,\"start\":25758},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25923,\"start\":25919},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27385,\"start\":27381},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27678,\"start\":27674},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27936,\"start\":27932},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29712,\"start\":29708},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29756,\"start\":29752},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29784,\"start\":29780},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29786,\"start\":29784},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29788,\"start\":29786},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30133,\"start\":30130},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":30136,\"start\":30133},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30139,\"start\":30136},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30357,\"start\":30353},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30360,\"start\":30357},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30384,\"start\":30380},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30393,\"start\":30390},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30467,\"start\":30464},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30491,\"start\":30487},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30523,\"start\":30519},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30589,\"start\":30586},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30611,\"start\":30607},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30632,\"start\":30628},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31552,\"start\":31548},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31569,\"start\":31566},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31585,\"start\":31581}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34545,\"start\":33741},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34740,\"start\":34546},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34865,\"start\":34741},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34999,\"start\":34866},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":35889,\"start\":35000}]", "paragraph": "[{\"end\":2276,\"start\":1685},{\"end\":2903,\"start\":2278},{\"end\":3464,\"start\":2905},{\"end\":4264,\"start\":3466},{\"end\":5130,\"start\":4266},{\"end\":5639,\"start\":5132},{\"end\":5681,\"start\":5641},{\"end\":6298,\"start\":5683},{\"end\":6721,\"start\":6348},{\"end\":7174,\"start\":6723},{\"end\":7761,\"start\":7176},{\"end\":8500,\"start\":7763},{\"end\":9003,\"start\":8502},{\"end\":9481,\"start\":9055},{\"end\":9770,\"start\":9483},{\"end\":10726,\"start\":9788},{\"end\":11184,\"start\":10738},{\"end\":11723,\"start\":11186},{\"end\":12198,\"start\":11752},{\"end\":12661,\"start\":12200},{\"end\":13051,\"start\":12663},{\"end\":13949,\"start\":13135},{\"end\":14997,\"start\":14098},{\"end\":15215,\"start\":14999},{\"end\":15733,\"start\":15244},{\"end\":16434,\"start\":15735},{\"end\":16805,\"start\":16491},{\"end\":17403,\"start\":16852},{\"end\":17928,\"start\":17405},{\"end\":18786,\"start\":17930},{\"end\":19115,\"start\":18899},{\"end\":19818,\"start\":19146},{\"end\":20093,\"start\":19820},{\"end\":20438,\"start\":20095},{\"end\":20537,\"start\":20440},{\"end\":21153,\"start\":20553},{\"end\":21565,\"start\":21155},{\"end\":22381,\"start\":21592},{\"end\":23168,\"start\":22383},{\"end\":23607,\"start\":23217},{\"end\":23944,\"start\":23609},{\"end\":24631,\"start\":23946},{\"end\":24946,\"start\":24633},{\"end\":25546,\"start\":24957},{\"end\":26100,\"start\":25548},{\"end\":27429,\"start\":26102},{\"end\":27775,\"start\":27450},{\"end\":28179,\"start\":27777},{\"end\":28802,\"start\":28181},{\"end\":29454,\"start\":28804},{\"end\":30024,\"start\":29513},{\"end\":30246,\"start\":30026},{\"end\":30888,\"start\":30248},{\"end\":32283,\"start\":30890},{\"end\":33104,\"start\":32318},{\"end\":33740,\"start\":33119}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13106,\"start\":13052},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14097,\"start\":13950},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16490,\"start\":16435},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18852,\"start\":18787},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18898,\"start\":18852}]", "table_ref": "[{\"end\":24654,\"start\":24647},{\"end\":25265,\"start\":25258},{\"end\":26112,\"start\":26105},{\"end\":28191,\"start\":28184},{\"end\":28203,\"start\":28196},{\"end\":30666,\"start\":30659}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1683,\"start\":1671},{\"attributes\":{\"n\":\"2.\"},\"end\":6313,\"start\":6301},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6346,\"start\":6316},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9053,\"start\":9006},{\"attributes\":{\"n\":\"2.3.\"},\"end\":9786,\"start\":9773},{\"attributes\":{\"n\":\"3.\"},\"end\":10736,\"start\":10729},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11750,\"start\":11726},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":13133,\"start\":13108},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":15242,\"start\":15218},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16850,\"start\":16808},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19144,\"start\":19118},{\"attributes\":{\"n\":\"4.\"},\"end\":20551,\"start\":20540},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21590,\"start\":21568},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23215,\"start\":23171},{\"end\":24955,\"start\":24949},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27448,\"start\":27432},{\"attributes\":{\"n\":\"4.4.\"},\"end\":29511,\"start\":29457},{\"attributes\":{\"n\":\"4.5.\"},\"end\":32316,\"start\":32286},{\"attributes\":{\"n\":\"5.\"},\"end\":33117,\"start\":33107},{\"end\":33752,\"start\":33742},{\"end\":34557,\"start\":34547},{\"end\":34752,\"start\":34742},{\"end\":34877,\"start\":34867}]", "table": "[{\"end\":35889,\"start\":35124}]", "figure_caption": "[{\"end\":34545,\"start\":33754},{\"end\":34740,\"start\":34559},{\"end\":34865,\"start\":34754},{\"end\":34999,\"start\":34879},{\"end\":35124,\"start\":35002}]", "figure_ref": "[{\"end\":2448,\"start\":2440},{\"end\":3657,\"start\":3649},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11693,\"start\":11685},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16242,\"start\":16234},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17402,\"start\":17394},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19667,\"start\":19659},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26650,\"start\":26642},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29339,\"start\":29331},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32042,\"start\":32034},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32282,\"start\":32274},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32825,\"start\":32817}]", "bib_author_first_name": "[{\"end\":36175,\"start\":36167},{\"end\":36188,\"start\":36183},{\"end\":36207,\"start\":36197},{\"end\":36223,\"start\":36217},{\"end\":36239,\"start\":36233},{\"end\":36255,\"start\":36249},{\"end\":36257,\"start\":36256},{\"end\":36269,\"start\":36265},{\"end\":36271,\"start\":36270},{\"end\":36288,\"start\":36282},{\"end\":36307,\"start\":36299},{\"end\":36816,\"start\":36808},{\"end\":36832,\"start\":36824},{\"end\":36846,\"start\":36840},{\"end\":36860,\"start\":36854},{\"end\":36874,\"start\":36868},{\"end\":36895,\"start\":36885},{\"end\":36910,\"start\":36903},{\"end\":36918,\"start\":36911},{\"end\":36939,\"start\":36930},{\"end\":36952,\"start\":36948},{\"end\":36956,\"start\":36953},{\"end\":36967,\"start\":36961},{\"end\":37519,\"start\":37514},{\"end\":37531,\"start\":37527},{\"end\":37831,\"start\":37826},{\"end\":37851,\"start\":37843},{\"end\":37864,\"start\":37861},{\"end\":37884,\"start\":37877},{\"end\":37901,\"start\":37895},{\"end\":37915,\"start\":37910},{\"end\":37917,\"start\":37916},{\"end\":38260,\"start\":38248},{\"end\":38278,\"start\":38272},{\"end\":38297,\"start\":38291},{\"end\":38314,\"start\":38309},{\"end\":38332,\"start\":38325},{\"end\":38715,\"start\":38710},{\"end\":38724,\"start\":38722},{\"end\":38733,\"start\":38730},{\"end\":38744,\"start\":38740},{\"end\":38759,\"start\":38750},{\"end\":39223,\"start\":39218},{\"end\":39233,\"start\":39230},{\"end\":39246,\"start\":39238},{\"end\":39265,\"start\":39258},{\"end\":39274,\"start\":39271},{\"end\":39738,\"start\":39731},{\"end\":39750,\"start\":39745},{\"end\":39766,\"start\":39758},{\"end\":39780,\"start\":39772},{\"end\":40047,\"start\":40041},{\"end\":40063,\"start\":40056},{\"end\":40080,\"start\":40071},{\"end\":40092,\"start\":40088},{\"end\":40108,\"start\":40102},{\"end\":40127,\"start\":40120},{\"end\":40141,\"start\":40138},{\"end\":40156,\"start\":40150},{\"end\":40168,\"start\":40163},{\"end\":40690,\"start\":40682},{\"end\":40704,\"start\":40698},{\"end\":40992,\"start\":40988},{\"end\":41007,\"start\":41001},{\"end\":41009,\"start\":41008},{\"end\":41024,\"start\":41020},{\"end\":41042,\"start\":41041},{\"end\":41059,\"start\":41050},{\"end\":41072,\"start\":41070},{\"end\":41089,\"start\":41082},{\"end\":41102,\"start\":41095},{\"end\":41110,\"start\":41108},{\"end\":41124,\"start\":41116},{\"end\":41473,\"start\":41466},{\"end\":41485,\"start\":41478},{\"end\":41850,\"start\":41846},{\"end\":41864,\"start\":41860},{\"end\":41879,\"start\":41872},{\"end\":41893,\"start\":41886},{\"end\":41906,\"start\":41899},{\"end\":41918,\"start\":41914},{\"end\":41933,\"start\":41927},{\"end\":41947,\"start\":41941},{\"end\":42326,\"start\":42324},{\"end\":42337,\"start\":42332},{\"end\":42355,\"start\":42347},{\"end\":42366,\"start\":42365},{\"end\":42382,\"start\":42381},{\"end\":42396,\"start\":42392},{\"end\":42398,\"start\":42397},{\"end\":42874,\"start\":42869},{\"end\":43366,\"start\":43359},{\"end\":43381,\"start\":43374},{\"end\":43394,\"start\":43387},{\"end\":43407,\"start\":43401},{\"end\":43409,\"start\":43408},{\"end\":43896,\"start\":43888},{\"end\":43907,\"start\":43902},{\"end\":43925,\"start\":43915},{\"end\":43938,\"start\":43931},{\"end\":43953,\"start\":43945},{\"end\":44477,\"start\":44471},{\"end\":44492,\"start\":44485},{\"end\":44505,\"start\":44500},{\"end\":44521,\"start\":44514},{\"end\":44540,\"start\":44532},{\"end\":44555,\"start\":44551},{\"end\":44960,\"start\":44951},{\"end\":45270,\"start\":45262},{\"end\":45283,\"start\":45276},{\"end\":45296,\"start\":45291},{\"end\":45312,\"start\":45307},{\"end\":45325,\"start\":45319},{\"end\":45338,\"start\":45334},{\"end\":45353,\"start\":45348},{\"end\":45372,\"start\":45362},{\"end\":45789,\"start\":45782},{\"end\":45802,\"start\":45795},{\"end\":45814,\"start\":45810},{\"end\":45824,\"start\":45821},{\"end\":45839,\"start\":45832},{\"end\":45852,\"start\":45848},{\"end\":45865,\"start\":45862},{\"end\":45880,\"start\":45873},{\"end\":46443,\"start\":46436},{\"end\":46460,\"start\":46451},{\"end\":46462,\"start\":46461},{\"end\":46479,\"start\":46475},{\"end\":46481,\"start\":46480},{\"end\":46746,\"start\":46737},{\"end\":46756,\"start\":46753},{\"end\":46769,\"start\":46762},{\"end\":46783,\"start\":46776},{\"end\":47114,\"start\":47108},{\"end\":47137,\"start\":47131},{\"end\":47154,\"start\":47145},{\"end\":47168,\"start\":47162},{\"end\":47192,\"start\":47186},{\"end\":47209,\"start\":47203},{\"end\":47223,\"start\":47217},{\"end\":47240,\"start\":47232},{\"end\":47253,\"start\":47247},{\"end\":47704,\"start\":47699},{\"end\":47719,\"start\":47713},{\"end\":47731,\"start\":47725},{\"end\":47745,\"start\":47739},{\"end\":48066,\"start\":48060},{\"end\":48076,\"start\":48073},{\"end\":48091,\"start\":48083},{\"end\":48101,\"start\":48099},{\"end\":48112,\"start\":48108},{\"end\":48508,\"start\":48502},{\"end\":48534,\"start\":48528},{\"end\":48550,\"start\":48543},{\"end\":48887,\"start\":48880},{\"end\":48906,\"start\":48900},{\"end\":48923,\"start\":48915},{\"end\":48932,\"start\":48930},{\"end\":49392,\"start\":49386},{\"end\":49404,\"start\":49398},{\"end\":49416,\"start\":49411},{\"end\":49430,\"start\":49423},{\"end\":49452,\"start\":49445},{\"end\":49871,\"start\":49864},{\"end\":49883,\"start\":49877},{\"end\":49894,\"start\":49890},{\"end\":50114,\"start\":50110},{\"end\":50126,\"start\":50122},{\"end\":50140,\"start\":50136},{\"end\":50155,\"start\":50149},{\"end\":50578,\"start\":50575},{\"end\":50591,\"start\":50585},{\"end\":50605,\"start\":50596},{\"end\":50618,\"start\":50612},{\"end\":50878,\"start\":50875},{\"end\":50891,\"start\":50884},{\"end\":50904,\"start\":50897},{\"end\":50916,\"start\":50912},{\"end\":50931,\"start\":50924},{\"end\":50945,\"start\":50938},{\"end\":51386,\"start\":51382},{\"end\":51400,\"start\":51393},{\"end\":51697,\"start\":51692},{\"end\":51711,\"start\":51702},{\"end\":51731,\"start\":51722},{\"end\":51746,\"start\":51739},{\"end\":51755,\"start\":51751},{\"end\":51950,\"start\":51943},{\"end\":51963,\"start\":51956},{\"end\":51977,\"start\":51970},{\"end\":51991,\"start\":51985},{\"end\":51993,\"start\":51992}]", "bib_author_last_name": "[{\"end\":36181,\"start\":36176},{\"end\":36195,\"start\":36189},{\"end\":36215,\"start\":36208},{\"end\":36231,\"start\":36224},{\"end\":36247,\"start\":36240},{\"end\":36263,\"start\":36258},{\"end\":36280,\"start\":36272},{\"end\":36297,\"start\":36289},{\"end\":36318,\"start\":36308},{\"end\":36822,\"start\":36817},{\"end\":36838,\"start\":36833},{\"end\":36852,\"start\":36847},{\"end\":36866,\"start\":36861},{\"end\":36883,\"start\":36875},{\"end\":36901,\"start\":36896},{\"end\":36928,\"start\":36919},{\"end\":36946,\"start\":36940},{\"end\":36959,\"start\":36957},{\"end\":36975,\"start\":36968},{\"end\":37525,\"start\":37520},{\"end\":37536,\"start\":37532},{\"end\":37841,\"start\":37832},{\"end\":37859,\"start\":37852},{\"end\":37875,\"start\":37865},{\"end\":37893,\"start\":37885},{\"end\":37908,\"start\":37902},{\"end\":37924,\"start\":37918},{\"end\":38270,\"start\":38261},{\"end\":38289,\"start\":38279},{\"end\":38307,\"start\":38298},{\"end\":38323,\"start\":38315},{\"end\":38338,\"start\":38333},{\"end\":38720,\"start\":38716},{\"end\":38728,\"start\":38725},{\"end\":38738,\"start\":38734},{\"end\":38748,\"start\":38745},{\"end\":38764,\"start\":38760},{\"end\":39228,\"start\":39224},{\"end\":39236,\"start\":39234},{\"end\":39256,\"start\":39247},{\"end\":39269,\"start\":39266},{\"end\":39283,\"start\":39275},{\"end\":39743,\"start\":39739},{\"end\":39756,\"start\":39751},{\"end\":39770,\"start\":39767},{\"end\":39784,\"start\":39781},{\"end\":40054,\"start\":40048},{\"end\":40069,\"start\":40064},{\"end\":40086,\"start\":40081},{\"end\":40100,\"start\":40093},{\"end\":40118,\"start\":40109},{\"end\":40136,\"start\":40128},{\"end\":40148,\"start\":40142},{\"end\":40161,\"start\":40157},{\"end\":40176,\"start\":40169},{\"end\":40696,\"start\":40691},{\"end\":40714,\"start\":40705},{\"end\":40999,\"start\":40993},{\"end\":41018,\"start\":41010},{\"end\":41032,\"start\":41025},{\"end\":41039,\"start\":41034},{\"end\":41048,\"start\":41043},{\"end\":41068,\"start\":41060},{\"end\":41080,\"start\":41073},{\"end\":41093,\"start\":41090},{\"end\":41106,\"start\":41103},{\"end\":41114,\"start\":41111},{\"end\":41130,\"start\":41125},{\"end\":41138,\"start\":41132},{\"end\":41476,\"start\":41474},{\"end\":41494,\"start\":41486},{\"end\":41858,\"start\":41851},{\"end\":41870,\"start\":41865},{\"end\":41884,\"start\":41880},{\"end\":41897,\"start\":41894},{\"end\":41912,\"start\":41907},{\"end\":41925,\"start\":41919},{\"end\":41939,\"start\":41934},{\"end\":41955,\"start\":41948},{\"end\":42330,\"start\":42327},{\"end\":42345,\"start\":42338},{\"end\":42363,\"start\":42356},{\"end\":42373,\"start\":42367},{\"end\":42379,\"start\":42375},{\"end\":42390,\"start\":42383},{\"end\":42404,\"start\":42399},{\"end\":42411,\"start\":42406},{\"end\":42880,\"start\":42875},{\"end\":43372,\"start\":43367},{\"end\":43385,\"start\":43382},{\"end\":43399,\"start\":43395},{\"end\":43415,\"start\":43410},{\"end\":43900,\"start\":43897},{\"end\":43913,\"start\":43908},{\"end\":43929,\"start\":43926},{\"end\":43943,\"start\":43939},{\"end\":43957,\"start\":43954},{\"end\":44483,\"start\":44478},{\"end\":44498,\"start\":44493},{\"end\":44512,\"start\":44506},{\"end\":44530,\"start\":44522},{\"end\":44549,\"start\":44541},{\"end\":44561,\"start\":44556},{\"end\":44964,\"start\":44961},{\"end\":45274,\"start\":45271},{\"end\":45289,\"start\":45284},{\"end\":45305,\"start\":45297},{\"end\":45317,\"start\":45313},{\"end\":45332,\"start\":45326},{\"end\":45346,\"start\":45339},{\"end\":45360,\"start\":45354},{\"end\":45380,\"start\":45373},{\"end\":45793,\"start\":45790},{\"end\":45808,\"start\":45803},{\"end\":45819,\"start\":45815},{\"end\":45830,\"start\":45825},{\"end\":45846,\"start\":45840},{\"end\":45860,\"start\":45853},{\"end\":45871,\"start\":45866},{\"end\":45885,\"start\":45881},{\"end\":45890,\"start\":45887},{\"end\":46449,\"start\":46444},{\"end\":46473,\"start\":46463},{\"end\":46491,\"start\":46482},{\"end\":46751,\"start\":46747},{\"end\":46760,\"start\":46757},{\"end\":46774,\"start\":46770},{\"end\":46790,\"start\":46784},{\"end\":47129,\"start\":47115},{\"end\":47143,\"start\":47138},{\"end\":47160,\"start\":47155},{\"end\":47184,\"start\":47169},{\"end\":47201,\"start\":47193},{\"end\":47215,\"start\":47210},{\"end\":47230,\"start\":47224},{\"end\":47245,\"start\":47241},{\"end\":47262,\"start\":47254},{\"end\":47269,\"start\":47264},{\"end\":47711,\"start\":47705},{\"end\":47723,\"start\":47720},{\"end\":47737,\"start\":47732},{\"end\":47752,\"start\":47746},{\"end\":48071,\"start\":48067},{\"end\":48081,\"start\":48077},{\"end\":48097,\"start\":48092},{\"end\":48106,\"start\":48102},{\"end\":48119,\"start\":48113},{\"end\":48526,\"start\":48509},{\"end\":48541,\"start\":48535},{\"end\":48555,\"start\":48551},{\"end\":48563,\"start\":48557},{\"end\":48898,\"start\":48888},{\"end\":48913,\"start\":48907},{\"end\":48928,\"start\":48924},{\"end\":48936,\"start\":48933},{\"end\":49396,\"start\":49393},{\"end\":49409,\"start\":49405},{\"end\":49421,\"start\":49417},{\"end\":49443,\"start\":49431},{\"end\":49458,\"start\":49453},{\"end\":49875,\"start\":49872},{\"end\":49888,\"start\":49884},{\"end\":49901,\"start\":49895},{\"end\":50120,\"start\":50115},{\"end\":50134,\"start\":50127},{\"end\":50147,\"start\":50141},{\"end\":50163,\"start\":50156},{\"end\":50583,\"start\":50579},{\"end\":50594,\"start\":50592},{\"end\":50610,\"start\":50606},{\"end\":50623,\"start\":50619},{\"end\":50882,\"start\":50879},{\"end\":50895,\"start\":50892},{\"end\":50910,\"start\":50905},{\"end\":50922,\"start\":50917},{\"end\":50936,\"start\":50932},{\"end\":50950,\"start\":50946},{\"end\":51391,\"start\":51387},{\"end\":51406,\"start\":51401},{\"end\":51700,\"start\":51698},{\"end\":51720,\"start\":51712},{\"end\":51737,\"start\":51732},{\"end\":51749,\"start\":51747},{\"end\":51764,\"start\":51756},{\"end\":51954,\"start\":51951},{\"end\":51968,\"start\":51964},{\"end\":51983,\"start\":51978},{\"end\":51999,\"start\":51994}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3697707},\"end\":36647,\"start\":36055},{\"attributes\":{\"doi\":\"arXiv:1811.02629\",\"id\":\"b1\"},\"end\":37382,\"start\":36649},{\"attributes\":{\"doi\":\"arXiv:1912.13471\",\"id\":\"b2\"},\"end\":37765,\"start\":37384},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":146808485},\"end\":38218,\"start\":37767},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2127515},\"end\":38593,\"start\":38220},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":59222756},\"end\":39153,\"start\":38595},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3780471},\"end\":39666,\"start\":39155},{\"attributes\":{\"doi\":\"arXiv:1908.00262\",\"id\":\"b7\"},\"end\":39976,\"start\":39668},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":502946},\"end\":40629,\"start\":39978},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6755881},\"end\":40931,\"start\":40631},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":224924728},\"end\":41419,\"start\":40933},{\"attributes\":{\"id\":\"b11\"},\"end\":41788,\"start\":41421},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7646250},\"end\":42256,\"start\":41790},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":119102145},\"end\":42814,\"start\":42258},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":104291869},\"end\":43291,\"start\":42816},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6200260},\"end\":43792,\"start\":43293},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":153312783},\"end\":44377,\"start\":43794},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5162860},\"end\":44852,\"start\":44379},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":18507866},\"end\":45217,\"start\":44854},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14113767},\"end\":45670,\"start\":45219},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":218502373},\"end\":46370,\"start\":45672},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":29890312},\"end\":46673,\"start\":46372},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":556999},\"end\":47041,\"start\":46675},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1739295},\"end\":47609,\"start\":47043},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":59601271},\"end\":48002,\"start\":47611},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3966049},\"end\":48448,\"start\":48004},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":5844139},\"end\":48805,\"start\":48450},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":10592859},\"end\":49318,\"start\":48807},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52958532},\"end\":49814,\"start\":49320},{\"attributes\":{\"doi\":\"arXiv:1511.05547\",\"id\":\"b29\"},\"end\":50062,\"start\":49816},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4357800},\"end\":50517,\"start\":50064},{\"attributes\":{\"doi\":\"arXiv:2007.00689\",\"id\":\"b31\"},\"end\":50803,\"start\":50519},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53291502},\"end\":51317,\"start\":50805},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":5489503},\"end\":51688,\"start\":51319},{\"attributes\":{\"id\":\"b34\"},\"end\":51861,\"start\":51690},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":233404466},\"end\":52361,\"start\":51863}]", "bib_title": "[{\"end\":36165,\"start\":36055},{\"end\":37824,\"start\":37767},{\"end\":38246,\"start\":38220},{\"end\":38708,\"start\":38595},{\"end\":39216,\"start\":39155},{\"end\":40039,\"start\":39978},{\"end\":40680,\"start\":40631},{\"end\":40986,\"start\":40933},{\"end\":41464,\"start\":41421},{\"end\":41844,\"start\":41790},{\"end\":42322,\"start\":42258},{\"end\":42867,\"start\":42816},{\"end\":43357,\"start\":43293},{\"end\":43886,\"start\":43794},{\"end\":44469,\"start\":44379},{\"end\":44949,\"start\":44854},{\"end\":45260,\"start\":45219},{\"end\":45780,\"start\":45672},{\"end\":46434,\"start\":46372},{\"end\":46735,\"start\":46675},{\"end\":47106,\"start\":47043},{\"end\":47697,\"start\":47611},{\"end\":48058,\"start\":48004},{\"end\":48500,\"start\":48450},{\"end\":48878,\"start\":48807},{\"end\":49384,\"start\":49320},{\"end\":50108,\"start\":50064},{\"end\":50873,\"start\":50805},{\"end\":51380,\"start\":51319},{\"end\":51941,\"start\":51863}]", "bib_author": "[{\"end\":36183,\"start\":36167},{\"end\":36197,\"start\":36183},{\"end\":36217,\"start\":36197},{\"end\":36233,\"start\":36217},{\"end\":36249,\"start\":36233},{\"end\":36265,\"start\":36249},{\"end\":36282,\"start\":36265},{\"end\":36299,\"start\":36282},{\"end\":36320,\"start\":36299},{\"end\":36824,\"start\":36808},{\"end\":36840,\"start\":36824},{\"end\":36854,\"start\":36840},{\"end\":36868,\"start\":36854},{\"end\":36885,\"start\":36868},{\"end\":36903,\"start\":36885},{\"end\":36930,\"start\":36903},{\"end\":36948,\"start\":36930},{\"end\":36961,\"start\":36948},{\"end\":36977,\"start\":36961},{\"end\":37527,\"start\":37514},{\"end\":37538,\"start\":37527},{\"end\":37843,\"start\":37826},{\"end\":37861,\"start\":37843},{\"end\":37877,\"start\":37861},{\"end\":37895,\"start\":37877},{\"end\":37910,\"start\":37895},{\"end\":37926,\"start\":37910},{\"end\":38272,\"start\":38248},{\"end\":38291,\"start\":38272},{\"end\":38309,\"start\":38291},{\"end\":38325,\"start\":38309},{\"end\":38340,\"start\":38325},{\"end\":38722,\"start\":38710},{\"end\":38730,\"start\":38722},{\"end\":38740,\"start\":38730},{\"end\":38750,\"start\":38740},{\"end\":38766,\"start\":38750},{\"end\":39230,\"start\":39218},{\"end\":39238,\"start\":39230},{\"end\":39258,\"start\":39238},{\"end\":39271,\"start\":39258},{\"end\":39285,\"start\":39271},{\"end\":39745,\"start\":39731},{\"end\":39758,\"start\":39745},{\"end\":39772,\"start\":39758},{\"end\":39786,\"start\":39772},{\"end\":40056,\"start\":40041},{\"end\":40071,\"start\":40056},{\"end\":40088,\"start\":40071},{\"end\":40102,\"start\":40088},{\"end\":40120,\"start\":40102},{\"end\":40138,\"start\":40120},{\"end\":40150,\"start\":40138},{\"end\":40163,\"start\":40150},{\"end\":40178,\"start\":40163},{\"end\":40698,\"start\":40682},{\"end\":40716,\"start\":40698},{\"end\":41001,\"start\":40988},{\"end\":41020,\"start\":41001},{\"end\":41034,\"start\":41020},{\"end\":41041,\"start\":41034},{\"end\":41050,\"start\":41041},{\"end\":41070,\"start\":41050},{\"end\":41082,\"start\":41070},{\"end\":41095,\"start\":41082},{\"end\":41108,\"start\":41095},{\"end\":41116,\"start\":41108},{\"end\":41132,\"start\":41116},{\"end\":41140,\"start\":41132},{\"end\":41478,\"start\":41466},{\"end\":41496,\"start\":41478},{\"end\":41860,\"start\":41846},{\"end\":41872,\"start\":41860},{\"end\":41886,\"start\":41872},{\"end\":41899,\"start\":41886},{\"end\":41914,\"start\":41899},{\"end\":41927,\"start\":41914},{\"end\":41941,\"start\":41927},{\"end\":41957,\"start\":41941},{\"end\":42332,\"start\":42324},{\"end\":42347,\"start\":42332},{\"end\":42365,\"start\":42347},{\"end\":42375,\"start\":42365},{\"end\":42381,\"start\":42375},{\"end\":42392,\"start\":42381},{\"end\":42406,\"start\":42392},{\"end\":42413,\"start\":42406},{\"end\":42882,\"start\":42869},{\"end\":43374,\"start\":43359},{\"end\":43387,\"start\":43374},{\"end\":43401,\"start\":43387},{\"end\":43417,\"start\":43401},{\"end\":43902,\"start\":43888},{\"end\":43915,\"start\":43902},{\"end\":43931,\"start\":43915},{\"end\":43945,\"start\":43931},{\"end\":43959,\"start\":43945},{\"end\":44485,\"start\":44471},{\"end\":44500,\"start\":44485},{\"end\":44514,\"start\":44500},{\"end\":44532,\"start\":44514},{\"end\":44551,\"start\":44532},{\"end\":44563,\"start\":44551},{\"end\":44966,\"start\":44951},{\"end\":45276,\"start\":45262},{\"end\":45291,\"start\":45276},{\"end\":45307,\"start\":45291},{\"end\":45319,\"start\":45307},{\"end\":45334,\"start\":45319},{\"end\":45348,\"start\":45334},{\"end\":45362,\"start\":45348},{\"end\":45382,\"start\":45362},{\"end\":45795,\"start\":45782},{\"end\":45810,\"start\":45795},{\"end\":45821,\"start\":45810},{\"end\":45832,\"start\":45821},{\"end\":45848,\"start\":45832},{\"end\":45862,\"start\":45848},{\"end\":45873,\"start\":45862},{\"end\":45887,\"start\":45873},{\"end\":45892,\"start\":45887},{\"end\":46451,\"start\":46436},{\"end\":46475,\"start\":46451},{\"end\":46493,\"start\":46475},{\"end\":46753,\"start\":46737},{\"end\":46762,\"start\":46753},{\"end\":46776,\"start\":46762},{\"end\":46792,\"start\":46776},{\"end\":47131,\"start\":47108},{\"end\":47145,\"start\":47131},{\"end\":47162,\"start\":47145},{\"end\":47186,\"start\":47162},{\"end\":47203,\"start\":47186},{\"end\":47217,\"start\":47203},{\"end\":47232,\"start\":47217},{\"end\":47247,\"start\":47232},{\"end\":47264,\"start\":47247},{\"end\":47271,\"start\":47264},{\"end\":47713,\"start\":47699},{\"end\":47725,\"start\":47713},{\"end\":47739,\"start\":47725},{\"end\":47754,\"start\":47739},{\"end\":48073,\"start\":48060},{\"end\":48083,\"start\":48073},{\"end\":48099,\"start\":48083},{\"end\":48108,\"start\":48099},{\"end\":48121,\"start\":48108},{\"end\":48528,\"start\":48502},{\"end\":48543,\"start\":48528},{\"end\":48557,\"start\":48543},{\"end\":48565,\"start\":48557},{\"end\":48900,\"start\":48880},{\"end\":48915,\"start\":48900},{\"end\":48930,\"start\":48915},{\"end\":48938,\"start\":48930},{\"end\":49398,\"start\":49386},{\"end\":49411,\"start\":49398},{\"end\":49423,\"start\":49411},{\"end\":49445,\"start\":49423},{\"end\":49460,\"start\":49445},{\"end\":49877,\"start\":49864},{\"end\":49890,\"start\":49877},{\"end\":49903,\"start\":49890},{\"end\":50122,\"start\":50110},{\"end\":50136,\"start\":50122},{\"end\":50149,\"start\":50136},{\"end\":50165,\"start\":50149},{\"end\":50585,\"start\":50575},{\"end\":50596,\"start\":50585},{\"end\":50612,\"start\":50596},{\"end\":50625,\"start\":50612},{\"end\":50884,\"start\":50875},{\"end\":50897,\"start\":50884},{\"end\":50912,\"start\":50897},{\"end\":50924,\"start\":50912},{\"end\":50938,\"start\":50924},{\"end\":50952,\"start\":50938},{\"end\":51393,\"start\":51382},{\"end\":51408,\"start\":51393},{\"end\":51702,\"start\":51692},{\"end\":51722,\"start\":51702},{\"end\":51739,\"start\":51722},{\"end\":51751,\"start\":51739},{\"end\":51766,\"start\":51751},{\"end\":51956,\"start\":51943},{\"end\":51970,\"start\":51956},{\"end\":51985,\"start\":51970},{\"end\":52001,\"start\":51985}]", "bib_venue": "[{\"end\":36335,\"start\":36320},{\"end\":36806,\"start\":36649},{\"end\":37512,\"start\":37384},{\"end\":37975,\"start\":37926},{\"end\":38389,\"start\":38340},{\"end\":38827,\"start\":38766},{\"end\":39362,\"start\":39285},{\"end\":39729,\"start\":39668},{\"end\":40255,\"start\":40178},{\"end\":40760,\"start\":40716},{\"end\":41160,\"start\":41140},{\"end\":41563,\"start\":41496},{\"end\":42001,\"start\":41957},{\"end\":42490,\"start\":42413},{\"end\":42959,\"start\":42882},{\"end\":43494,\"start\":43417},{\"end\":44036,\"start\":43959},{\"end\":44599,\"start\":44563},{\"end\":45021,\"start\":44966},{\"end\":45420,\"start\":45382},{\"end\":45973,\"start\":45892},{\"end\":46507,\"start\":46493},{\"end\":46836,\"start\":46792},{\"end\":47307,\"start\":47271},{\"end\":47790,\"start\":47754},{\"end\":48185,\"start\":48121},{\"end\":48603,\"start\":48565},{\"end\":49015,\"start\":48938},{\"end\":49524,\"start\":49460},{\"end\":49862,\"start\":49816},{\"end\":50242,\"start\":50165},{\"end\":50573,\"start\":50519},{\"end\":51013,\"start\":50952},{\"end\":51471,\"start\":51408},{\"end\":52068,\"start\":52001},{\"end\":38875,\"start\":38829},{\"end\":39426,\"start\":39364},{\"end\":40319,\"start\":40257},{\"end\":41617,\"start\":41565},{\"end\":42554,\"start\":42492},{\"end\":43023,\"start\":42961},{\"end\":43558,\"start\":43496},{\"end\":44100,\"start\":44038},{\"end\":46041,\"start\":45975},{\"end\":48236,\"start\":48187},{\"end\":49079,\"start\":49017},{\"end\":49575,\"start\":49526},{\"end\":50306,\"start\":50244},{\"end\":51061,\"start\":51015},{\"end\":52122,\"start\":52070}]"}}}, "year": 2023, "month": 12, "day": 17}