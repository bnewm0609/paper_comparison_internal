{"id": 2934551, "updated": "2023-10-05 04:39:05.22", "metadata": {"title": "Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG", "authors": "[{\"first\":\"James\",\"last\":\"O'Sullivan\",\"middle\":[\"A\"]},{\"first\":\"Alan\",\"last\":\"Power\",\"middle\":[\"J\"]},{\"first\":\"Nima\",\"last\":\"Mesgarani\",\"middle\":[]},{\"first\":\"Siddharth\",\"last\":\"Rajaram\",\"middle\":[]},{\"first\":\"John\",\"last\":\"Foxe\",\"middle\":[\"J\"]},{\"first\":\"Barbara\",\"last\":\"Shinn-Cunningham\",\"middle\":[\"G\"]},{\"first\":\"Malcolm\",\"last\":\"Slaney\",\"middle\":[]},{\"first\":\"Shihab\",\"last\":\"Shamma\",\"middle\":[\"A\"]},{\"first\":\"Edmund\",\"last\":\"Lalor\",\"middle\":[\"C\"]}]", "venue": "Cerebral cortex", "journal": "Cerebral cortex", "publication_date": {"year": 2015, "month": null, "day": null}, "abstract": "How humans solve the cocktail party problem remains unknown. However, progress has been made recently thanks to the realization that cortical activity tracks the amplitude envelope of speech. This has led to the development of regression methods for studying the neurophysiology of continuous speech. One such method, known as stimulus-reconstruction, has been successfully utilized with cortical surface recordings and magnetoencephalography (MEG). However, the former is invasive and gives a relatively restricted view of processing along the auditory hierarchy, whereas the latter is expensive and rare. Thus it would be extremely useful for research in many populations if stimulus-reconstruction was effective using electroencephalography (EEG), a widely available and inexpensive technology. Here we show that single-trial ( \u2248 60 s) unaveraged EEG data can be decoded to determine attentional selection in a naturalistic multispeaker environment. Furthermore, we show a signi \ufb01 cant correlation between our EEG-based measure of attention and performance on a high-level attention task. In addition, by attempting to decode attention at individual latencies, we identify neural processing at \u223c 200 ms as being critical for solving the cocktail party problem. These \ufb01 ndings open up new avenues for studying the ongoing dynamics of cognition using EEG and for developing effective and natural brain \u2013 computer interfaces.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2158904676", "acl": null, "pubmed": "24429136", "pubmedcentral": null, "dblp": null, "doi": "10.1093/cercor/bht355"}}, "content": {"source": {"pdf_hash": "09e4deb7e1a3e9118d061410d54bd42bf43a10ae", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://academic.oup.com/cercor/article-pdf/25/7/1697/14102659/bht355.pdf", "status": "BRONZE"}}, "grobid": {"id": "c231e5809f89ccdabc50d285510753ebe590f423", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/09e4deb7e1a3e9118d061410d54bd42bf43a10ae.txt", "contents": "\nFEATURE ARTICLE Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG\n\n\nJames A O&apos;sullivan \nSchool of Engineering\nTrinity Centre for Bioengineering and Trinity College Institute of Neuroscience\nTrinity College Dublin\nDublin 2Ireland\n\nAlan J Power \nSchool of Engineering\nTrinity Centre for Bioengineering and Trinity College Institute of Neuroscience\nTrinity College Dublin\nDublin 2Ireland\n\nDepartment of Psychology\nCentre for Neuroscience in Education\nUniversity of Cambridge\nCambridgeUK\n\nNima Mesgarani \nDepartment of Neurological Surgery\n\n\nDepartment of Physiology\nUCSF Center for Integrative Neuroscience\nUniversity of California\n94143San FranciscoCAUSA\n\nSiddharth Rajaram \nThe Center for Computational Neuroscience and Neural Technology\nBoston University\n02215BostonMAUSA\n\nJohn J Foxe \nDepartments of Pediatrics and Neuroscience\nThe Sheryl and Daniel R. Tishman Cognitive Neurophysiology Laboratory, Children's Evaluation and Rehabilitation Center\nAlbert Einstein College of Medicine\n10461BronxNYUSA\n\nBarbara G Shinn-Cunningham \nThe Center for Computational Neuroscience and Neural Technology\nBoston University\n02215BostonMAUSA\n\nMalcolm Slaney \nMicrosoft Research\n94043Mountain ViewCAUSA\n\nShihab A Shamma \nInstitute for Systems Research\nUniversity of Maryland\n20742College ParkMDUSA\n\nTrinity College Dublin\nDublin 2Ireland\n\nEdmund C Lalor edlalor@tcd.ie \nSchool of Engineering\nTrinity Centre for Bioengineering and Trinity College Institute of Neuroscience\nTrinity College Dublin\nDublin 2Ireland\n\nPh.DEdmund C Lalor \nFEATURE ARTICLE Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG\n10.1093/cercor/bht355Address correspondence toattentionBCIcocktail partyEEGspeechstimulus- reconstruction\nHow humans solve the cocktail party problem remains unknown. However, progress has been made recently thanks to the realization that cortical activity tracks the amplitude envelope of speech. This has led to the development of regression methods for studying the neurophysiology of continuous speech. One such method, known as stimulus-reconstruction, has been successfully utilized with cortical surface recordings and magnetoencephalography (MEG). However, the former is invasive and gives a relatively restricted view of processing along the auditory hierarchy, whereas the latter is expensive and rare. Thus it would be extremely useful for research in many populations if stimulus-reconstruction was effective using electroencephalography (EEG), a widely available and inexpensive technology. Here we show that single-trial (\u224860 s) unaveraged EEG data can be decoded to determine attentional selection in a naturalistic multispeaker environment. Furthermore, we show a significant correlation between our EEG-based measure of attention and performance on a high-level attention task. In addition, by attempting to decode attention at individual latencies, we identify neural processing at \u223c200 ms as being critical for solving the cocktail party problem. These findings open up new avenues for studying the ongoing dynamics of cognition using EEG and for developing effective and natural braincomputer interfaces.\n\nIntroduction\n\nSince its first behavioral description (Cherry 1953), researchers have sought to identify the neural underpinnings of the cocktail party problem; that is, our ability to easily attend to one speaker in a multispeaker environment. Recent research in this area has focused on changes in cortical activity that track the dynamic changes in the speech stimulus (Kerlin et al. 2010;Ding and Simon 2012a;Koskinen et al. 2012;Mesgarani and Chang 2012;Power et al. 2012;Zion Golumbic et al. 2013). For example, by assuming a forward mapping from the amplitude envelope of speech to EEG, it has been shown to be possible to derive separate linear impulse response measures to each of 2 concurrent speech streams, and that directing attention to one of these streams produces modulations of these impulse responses over the left hemisphere at a latency of \u223c200 ms (Power et al. 2012). While these effects suggest that selective attention operates at the level of semantic processing, they are only discernible after averaging over many trials and subjects, a lack of sensitivity that is not atypical of EEG-based cognitive neuroscience studies.\n\nSeveral recent studies have used recorded cortical population data to estimate the input stimulus using a mapping approach in the reverse direction (i.e., from the neural data back to the stimulus) (Rieke et al. 1995;Stanley et al. 1999;Mesgarani et al. 2009;Pasley et al. 2012;Zion Golumbic et al. 2013). This stimulus-reconstruction approach has been shown to be exquisitely sensitive to selective attention in a multispeaker environment Simon 2012a, 2012b;Zion Golumbic et al. 2013). For example, one such study showed that reconstructed speech spectrograms from cortical surface responses to a mixture of speakers were dominated by the salient spectral and temporal features of the attended speaker, and were only weakly correlated with the unattended speaker . While this is an important insight into how the cortical representation of speech gives rise to a perception relevant for the listener's intended goal, invasive recording is only possible with human listeners undergoing clinical treatments, and as such is not suitable for many populations in which we would like to study selective attention to speech. Furthermore, the relatively local nature of surface recordings may not be optimal for assessing how attentional selection to speech operates along the entire auditory processing hierarchy (Power et al. 2012).\n\nUsing magnetoencephalography (MEG), which is a more global measure of cortical activity, Ding and Simon (2012a) showed that responses to a single-trial speech mixture could be decoded to give an estimate of the envelope of the input speech stream, and that this estimate typically had a greater correlation with the attended speech than the unattended. While this is a powerful and important result, the cost, lack of portability, and relative rarity of MEG recording facilities make populationspecific research somewhat difficult. Thus, it would be extremely useful if such a decoding approach could be used with EEG data. This technology is cheaper, more widely accessible, easier to use in many specific cohorts, and can be integrated into everyday devices, making it a realistic option for braincomputer interface (BCI) applications. In addition, EEG is sensitive to both tangential and radial components of cortical current sources, while MEG is sensitive only to tangential components. This suggests that EEG may be sensitive to important aspects of electromagnetic brain activity that may not be well captured by MEG. For example, EEG exhibits many attention-related components that are not clearly detected with MEG (N\u00e4\u00e4t\u00e4nen 1992; Kahkonen et al. 2001).\n\nHere we show for the first time that selective attention in a multispeaker environment can be decoded using unaveraged single-trial EEG. Moreover, we show that the strength of the attended speech representation in the EEG is correlated with subjects' performance on a high-level cocktail party task.\n\n\nMaterials and Methods\n\nParticipants Forty human subjects took part (mean \u00b1 standard deviation (SD) age, 27.3 \u00b1 3.2 years; 32 male; 7 left-handed). The experiment was undertaken in accordance with the Declaration of Helsinki. The Ethics Committees of the Nathan Kline Institute and the School of Psychology at Trinity College Dublin approved the experimental procedures and each subject provided written informed consent. Subjects reported no history of hearing impairment or neurological disorder. These data have been published previously using a different analysis approach (Power et al. 2012).\n\n\nStimuli and Procedures\n\nSubjects undertook 30 trials, each of \u223c1 min in length, where they were presented with 2 classic works of fiction: one to the left ear, and the other to the right ear. Each story was read by a different male speaker. Subjects were divided into 2 groups of 20 with each group instructed to attend to the story in either the left or right ear throughout all 30 trials. After each trial, subjects were required to answer between 4 and 6 multiple-choice questions on both stories. Each question had 4 possible answers. (See Supplementary Fig. 1 for examples of the types of questions asked). We used a between-subjects design as we wanted each subject to follow just one story to make the experiment as natural as possible and because we wished to avoid any repeated presentation of stimuli. For both stories, each trial began where the story ended on the previous trial. Stimulus amplitudes in each audio stream within each trial were normalized to have the same root mean squared (RMS) intensity. In order to minimize the possibility of the unattended stream capturing the subjects' attention during silent periods in the attended stream, silent gaps exceeding 0.5 s were truncated to 0.5 s in duration. Stimuli were presented using Sennheiser HD650 headphones and Presentation software from Neurobehavioral Systems (http://www.neurobs.com). Subjects were instructed to maintain visual fixation for the duration of each trial on a crosshair centered on the screen, and to minimize eye blinking and all other motor activities.\n\n\nData Acquisition and Preprocessing\n\nElectroencephalography data were recorded for 34 of the subjects using 128 electrode positions (17 of these subjects attended to the speech on the left and the remaining 17 to the right). Data for the remaining 6 participants were collected using 160 electrode positions (3 of these subjects attended to the left and the remaining 3 to the right). These data were then remapped to an equivalent 128 electrode positions using an interpolated spline function. The data were filtered over the range 0-134 Hz and digitized at the rate of 512 Hz using a BioSemi Active Two system. Data were referenced to the average of all scalp channels.\n\nIn order to decrease the processing time required, all EEG data were downsampled by a factor of 8 to give an equivalent sampling rate of 64 Hz, after applying a zero phase-shift antialiasing filter. The amplitude envelopes of the speech signals were obtained using a Hilbert transform, and then downsampled to the same sampling rate of 64 Hz to allow us to relate their dynamics to those of the EEG.\n\nBecause envelope frequencies between 2 and 8 Hz are linearly relatable to the EEG (Pasley et al. 2012;Zion Golumbic et al. 2013), the EEG data were digitally filtered offline with a band-pass filter between 2 and 8 Hz, and the speech envelopes were low-pass filtered below 8 Hz.\n\n\nStimulus-Reconstruction\n\nWe wished to determine how accurately we could estimate to which of the 2 speakers each subject was attending based on a single trial (\u223c60 s) of EEG data. Our strategy for this was centered on the approach of stimulus-reconstruction. This approach attempts to reconstruct an estimate of the input stimulus S using recorded neural data R via a linear reconstruction model g. For a set of N electrodes, we represent the response of electrode n at time t = 1 \u2026 T as R(t,n). The reconstruction model, g(\u03c4, n), is a function that maps R(t,n) to stimulus S(t) as follows:\nS\u00f0t\u00de \u00bc X n X g\u00f0; n\u00deR\u00f0t \u00c0 ; n\u00de \u00f01\u00de\nwhere\u015c denotes the estimated stimulus. The function g is estimated by minimizing the mean-squared error between the actual and reconstructed stimulus min e \u00bc X t \u00bdS\u00f0t\u00de \u00c0\u015c\u00f0t\u00de 2 Solving this analytically results in calculation of the normalized reverse correlation (Bialek et al. 1991;Stanley et al. 1999)\ng \u00bc C \u00c01 RR C RS\u00f02\u00de\nwhere C RR and C RS are the auto-correlation of the EEG data, and the cross-correlation of the stimulus and EEG data, across all electrodes and time-lags, respectively,\nC RR \u00bc RR T C RS \u00bc RS T\nand R and S are defined as The matrix R is only padded with zeros on the left to ensure causality. Because of the stochastic nature of the neural responses, the autocorrelation of the neural responses C RR is full rank and easily invertible. In our case, we used all 128 channels of EEG data. Because previous research indicates that EEG activity reflects the dynamics of the speech envelope at latencies up to 250 ms poststimulus (Lalor and Foxe 2010), we initially attempted to maximize the accuracy of our speech reconstruction using EEG at time-lags \u03c4 from 0 to 250 ms poststimulus. As we calculated a mapping from the neural data back to the stimulus, in practice we used time-lags from \u2212250 to 0 ms.\n\nFor illustrative purposes, if the number of electrodes N = 2, and the response of electrode 1 at time t = 1 \u2026 T is defined as r 1 (t), and the range of time-lags \u03c4 spans from \u22122 to 0, then r 1 \u00f0T \u00de r 2 \u00f0T \u00de r 1 \u00f0T \u00c0 1\u00de r 2 \u00f0T \u00c0 1\u00de r 1 \u00f0T \u00c0 2\u00de r 2 \u00f0T \u00c0 2\u00de 0 0 r 1 \u00f0T \u00de r 2 \u00f0T \u00de r 1 \u00f0T \u00c0 1\u00de r 2 \u00f0T \u00c0 1\u00de 0 0 0 0 r 1 \u00f0T \u00de r 2 \u00f0T \u00de 2 6 6 6 6 6 4 3 7 7 7 7 7 5 T Each decoder g is essentially a multivariate impulse response function calculated from all 128 electrodes and all time-lags simultaneously.\nR \u00bc r 1 \u00f02\u00de r 2\u00f02\u00de\nStimulus-reconstruction is therefore performed by convolving this impulse response with the EEG data. As there were 2 simultaneous input speech streams (attended and unattended), we trained 2 decoders for each trial: one where linearregression was performed between the EEG data and the attended stream alone, and another where linear-regression was performed between the EEG data and the unattended stream alone. We refer to these as Attended and Unattended decoders, respectively. As each subject undertook 30 trials, this resulted in 60 decoders for each subject (30 Attended and 30 Unattended).\n\nFor each reconstruction, we evaluated the reconstruction-accuracy by determining a correlation coefficient (Pearson's r) between the reconstructed speech envelope and the actual attended and unattended speech envelope, which we will refer to as r attended and r unattended , respectively (Fig. 1).\n\nPrevious research (Ding and Simon 2012a) has shown that attended and unattended speech can be extracted separately from neural data, implying that it is not just the case that attended speech is more strongly represented by the same neural generators. Therefore, in order to ascertain the direction of attention for each subject, we had 2 choices:\n\n1. We could use the Attended decoders in order to estimate which story the subject was attending to. 2. We could use the Unattended decoders in order to estimate which story the subject was not attending to.\n\nWhen using the Attended decoders to reconstruct an estimate of the input stimulus, we would consider a trial to be correctly decoded if the reconstruction had a greater correlation with the attended stream (i.e., if r attended > r unattended ). Similarly, when using the Unattended decoders, we would consider a trial to be correctly decoded if the reconstruction had a greater correlation with the unattended stream (i.e., if r unattended > r attended ). The percentage of trials where we correctly decoded attentional-selection will hereafter be referred to as decoding-accuracy. We then employed these decoders in 2 different ways to reconstruct the input speech stream for each trial. We discuss these 2 approaches in turn.\n\n\nDecoding Attention\n\nFirst, we decoded the attention of each subject using the decoders that were trained on their own data. We refer to this as the Subject-Specific decoding method. As each decoder g is a 2-dimensional matrix (electrode channels \u00d7 time-lags) representing a multivariate impulse resonse function, we can combine decoders from multiple trials by simply averaging these matrices together. We will refer to the numerical values of these matrices as the parameters of the decoders. For training and validation, a leave-one-out cross-validation approach was used, whereby each trial was decoded using the averaged parameters of the decoders trained on every other trial (i.e., 29 min of training data, and 1 min of test data). Secondly, we were concerned that our Subject-Specific decoding approach may have been biased as the decoders were trained on data where subjects were always attending to the same ear, and to the same speaker. Therefore, for our second approach, we sought to avoid any such potential bias by adopting a Grand-Average decoding method. That is, for each subject, we decoded each trial using the averaged parameters of the decoders trained on every other subject and every other trial (leave-one-out cross-validation). Importantly, as there were 40 subjects in total, this method utilized 20 decoders that were trained on the opposite ear as the subject being decoded, and only 19 decoders that were trained on the same ear, thus limiting any potential directionof-attention bias.\n\nFor both the Subject-Specific and Grand-Average decoding methods, it is important to clarify that we used the Attended decoders in order to estimate which story the subject was attending to, and the Unattended decoders in order to estimate which story the subject was not attending to.\n\nReconstruction-Accuracy and Decoding-Accuracy at Individual Time-Lags As mentioned above, we trained the decoders on EEG data across a broad interval of time-lags from 0 to 250 ms simultaneously in an attempt to optimally reconstruct the input speech envelopes. However, our previously published analysis of the same data has indicated that attentional effects on a dichotic cocktail party experiment are most prominent specifically from \u223c170 to 250 ms poststimulus (Power et al. 2012). This suggested that we might improve decoding accuracy by focusing on a more specific interval of time-lags. To investigate this, we trained decoders on EEG data at individual time-lags, rather than across a range of time-lags simultaneously.\n\nFor illustrative purposes, if the number of electrodes N = 2, and the response of electrode 1 at time t = 1 \u2026 T is defined as r 1 (t), and we want to calculate a decoder at a time-lag of \u223c50 ms poststimulus, then at a sampling rate of 64 Hz, this equates to a time-lag of 3 samples. Therefore, the response matrix R becomes: The decoder g is then calculated in the same way as before:\nR \u00bc r 1 \u00f03\u00de r 2 \u00f03\u00de r 1 \u00f04\u00de r 2 \u00f04\u00de . . . . . .g \u00bc \u00bdRR T \u00c01 RS T\nwhere S is the input stimulus envelope. In our case, we evaluated decoders at time-lags ranging from 0 to 400 ms poststimulus. Therefore, at a sampling rate of 64 Hz, this resulted in 26 individual time-lag decoders separated by intervals of 15.6 ms.\n\n\nSpatiotemporal Analysis of Decoders\n\nIn order to investigate which electrode channels contributed most to our decoding-accuracy, we examined the average decoder weightings at each individual time-lag. These individual time-lag decoders can essentially be considered as spatial filters that optimally map the channel data (at a particular lag) to the stimulus.\n\n\nCorrelation with Behavior\n\nWe wished to determine if there was any correlation between how well a subject performed at answering questions to the attended story and how well we could reconstruct the attended stimulus. We hypothesized Figure 1. Illustration of the decoding strategy. Data from all electrode channels are decoded simultaneously to give an estimate of the amplitude envelope of the input speech stream. The correlation between this reconstruction and both the attended and unattended speech streams is then calculated for each trial.\n\nthat the reconstruction-accuracy would show a correlation with behavioral performance across subjects and across single trials. Initially, we performed this analysis using reconstructions based on the interval of time-lags from 0 to 250 ms.\n\nWe first assessed the correlation between the percentage of questions answered correctly for each trial (\u223c1200 trials in total), and the r attended obtained for each trial using the Attended decoders. We will refer to this as our Across Trials Reconstruction-Accuracy Test.\n\nNext, we calculated the mean r attended across trials for each subject, and the percentage of questions answered correctly in total for each subject. We refer to this as our Across Subjects Reconstruction-Accuracy Test.\n\nIt should be noted that one subject was excluded from these analyses as the percentage of questions they answered correctly was almost 3 SDs below the mean (Z = \u22122.84), while no other subject performed >1.56 SDs below the mean.\n\nCorrelation with Behavior at Individual Time-Lags As with our decoding-accuracy, we wanted to see if the correlations with behavior were specific to the 170-250 ms range. To do this, we used the r-values obtained from the decoders trained at individual time-lags (as described before), and assessed any correlation with behavior using the abovementioned tests. Specifically, we looked at r attended for both the Attended and Unattended decoders.\n\n\nEffect of Trial Duration\n\nFinally we wanted to see how the duration of the test data affected the decoding accuracy. To do this, we used the Attended Subject-Specific decoders with time-lags from 0 to 250 ms, while using progressively shorter durations of test data from each trial.\n\n\nResults\n\n\nBehavioural Results\n\nAs reported previously (Power et al. 2012), our behavioral results clearly showed that subjects were compliant in the task. On average, subjects correctly answered 80.4 \u00b1 7.3% of questions on the attended story and 27.1 \u00b1 7.0% on the unattended story, which was not statistically greater than chance (P = 0.77). This is in line with previous reports on dichotic listening behavior, which show that it is not possible to listen to, and remember, 2 concurrent speech streams (Cherry 1953). Consistent with this, a 2 \u00d7 2 ANOVA with factors of story (left ear/right ear) and attention (attended/unattended) revealed a significant main effect of attention (F = 1164.13, P < 0.001), no effect of story (F = 3.08, P = 0.084) and no story \u00d7 attention interaction (F = 2.15, P = 0.147). Individual subject performance is given in Supplementary Figure 2. Decoding Accuracy 1) Using the Subject-Specific decoding method, 39 of the 40 subjects had a decoding-accuracy significantly above chance, and 7 subjects had a decoding-accuracy of 100% (mean 89%; Fig. 2A). For the Unattended decoders, 37 subjects had a decoding-accuracy significantly above chance, with one subject being decoded with 100% accuracy (mean 78.9%; Fig. 2A). The significant level of decoding-accuracy (63.33%) was determined using a binomial test at the 5% significance level.\n\n2) Using the Grand-Average decoding method, the Attended decoders yielded a decoding-accuracy significantly greater than chance for 36 of the 40 subjects, with 100% accuracy for 5 subjects (mean 82%; Fig. 2B). For the Unattended decoders, 32 of the 40 subjects had a decoding-accuracy significantly greater than chance, with 100% accuracy for one subject (mean 75% ;   Fig. 2B). For the Attended decoders, the decoding-accuracy of the Subject-Specific approach was slightly, but significantly greater than that of the Grand-Average approach (P = 0.003, paired t-test). There was no significant difference between the Subject-Specific and Grand-Average decoding-accuracy for the Unattended decoders (P = 0.103).\n\nScatter plots showing r attended and r unattended for all subjects and all trials using the Attended and Unattended decoders for both the Subject-Specific and Grand-Average decoding methods are shown in Figure 2C,D, respectively. The plots show that when using the Attended decoders, r attended is typically greater than r unattended for both the Subject-Specific (median r = 0.054 and \u22120.005, respectively) and Grand-Average (median r = 0.038 and \u22120.007, respectively) decoding methods. Conversely, when using the Unattended decoders, r unattended is typically greater than r attended for both the Subject-Specific (median r = 0.032 and \u22120.006, respectively), and Grand-Average decoding methods (median r = 0.026 and \u22120.012, respectively).\n\n\nReconstruction-Accuracy and Decoding-Accuracy at Individual Time-Lags\n\nAlthough using individual time-lags reduced both the reconstruction-accuracy and the decoding-accuracy achievable, a clear pattern emerged indicating which time-lags are most important when determining attentional-selection for both the Subject-Specific and Grand-Average decoding methods (Fig. 3A,B). Using the Attended decoders, the reconstructionaccuracy for the attended speech stream (r attended ) was found to be largest at time-lags between 170 and 250 ms, with a peak at \u223c218 ms. Conversely, r unattended was smallest during this time frame, with a negative peak at \u223c203 ms. Consequently, decoding-accuracy was at a maximum between 170 and 250 ms for the Attended decoders, with a peak at \u223c203 ms (Fig. 3C,D). A similar pattern emerged for the Unattended decoders, thus yielding a decoding-accuracy that was also maximal during the same time-interval (Fig. 3C,D).\n\nAs a result of these individual time-lag results, we trained decoders using a narrow interval of time-lags from 170 to 250 ms (instead of the broad 0 to 250-ms-interval used earlier). While, as expected, the mean reconstruction-accuracy dropped significantly for the Subject-Specific and Grand-Average decoding methods (P = 0.0015 and P = 0.013, respectively; paired t-test), using these narrower time-lags produced an increase in decoding-accuracy for all methods. However, this increase was only significant for the Grand-Average decoding method (Attended P = 0.02, Unattended P = 0.004, paired t-test; Table 1). As was the case with the broader (0 to 250 ms) time interval used earlier, the Subject-Specific approach led to a small, but significantly greater decoding-accuracy than the Grand-Average approach for the Attended decoders (P = 0.0117), with no difference for the Unattended decoders (P = 0.3139). Figure 4 shows how the filter weights are distributed across the scalp for selected individual time-lags. While there is no obvious spatial pattern at lags of 100 and 150 ms, a clear pattern is evident at 200 and 250 ms. This pattern includes 2 bilateral foci of stronger weightings over temporal regions, with the weightings in these regions being inverted for the Unattended decoders relative to the Attended decoders. We take this highly structured pattern of decoder weights to support our claim that 200 ms represents an important locus of attention as opposed to being simply the locus of maximal decoding-accuracy produced by an exhaustive search across individual time-lags. This is especially true as the location of strongest decoder weights is over scalp regions indicative of auditory cortical activity. In addition, these patterns help to explain the pattern of reconstruction-accuracy results observed in Figure 3 (e.g., r attended for the Attended decoder is inverted relative to r attended for the Unattended decoder-circle symbols in Fig. 3B).\n\n\nSpatiotemporal Analysis of Decoders\n\n\nCorrelation with Behaviour\n\nFor the Across Trials Reconstruction-Accuracy test, we found a significant correlation with behavioral performance for both the Subject-Specific (r = 0.07, P = 0.017) and Grand-Average decoding methods (r = 0.08, P = 0.005). For the Across Subjects Reconstruction-Accuracy test, we found a significant correlation for the Grand-Average decoding method (r = 0.32, P = 0.05), but no significant correlation for the Subject-Specific decoding method (r = 0.12, P = 0.45). It is possible that the differing results for the Across Subjects (\u223c40 data points) and Across Trials (\u223c1200 data points) tests for the Subject-Specific decoding method may simply be an issue of statistical power. However, we suspect that the lack of any correlation in our Across Subjects test may have been driven by large intersubject variations in reconstruction-accuracy (i.e., r attended ) that may be independent of attention. It is almost certain that the strength at which the speech envelope is represented in the scalp EEG relates to several subject-specific factors, such as cortical folding, skull thickness, etc. Therefore, it is not unlikely that 2 subjects with similar behavioral performance would differ in their mean r attended values. With this in mind, we tested the hypothesis that good behavioral performance follows as a result of a subject being able to consistently sustain their attentional deployment, and that this consistency should be measurable in terms of a consistent difference between the accuracies with which we can reconstruct the attended and unattended streams (r attended \u2212 r unattended ), no matter what the absolute reconstruction-accuracy values. That is, the more consistent (i.e., lower variance) the reconstruction differences, the better a subject should have done on the questions. We will refer to this as as our Across Subjects Reconstruction-Consistency Test. Using this test, we found a significant correlation for the Attended Subject-Specific decoding method (r = \u22120.39, P = 0.01), but no significant correlation for for the Grand-Average decoding method (r = \u22120.08, P = 0.62).\n\n\nCorrelation with Behavior at Individual Time-Lags\n\nAs with our broad time-lags, we also found correlations with behavior at individual time-lags (Fig. 5). Correlations for the Attended decoders are shown in black, and the Unattended decoders are shown in gray. Significant correlations (P < 0.05) are indicated by filled circles. Most of the significant correlations occur at time-lags from 200 to 250 ms, with some occuring at lags 100 to 150 ms. Similar patterns can be observed for both the Across Subjects and Across Trials Reconstruction-Accuracy tests (both for Subject-Specific and Grand-Average methods) with the Attended and Unattended decoders producing correlations that are almost perfectly out of phase with one another and that are maximally different at \u223c200-250 ms. This is not hugely surprising given the inverted spatial patterns observed in our decoder weightings (Fig. 4). As with the broad interval (0-250 ms) based analysis, the Across Subjects Reconstruction-Accuracy test was not found to be significant for the Subject-Specific approach, but the same time interval was revealed as important in the Across Subjects Reconstruction-Consistency Test.\n\n\nEffect of Trial Duration\n\nAs expected, decoding-accuracy dropped as the test data duration decreased. However, even with just 10 s of test data, attention was correctly decoded significantly above chance for 30 of the 40 subjects (mean decoding-accuracy 68.6%, min 50%, max 86.7%; Supplementary Fig. 4).\n\n\nDiscussion\n\nUsing electroencephalography (EEG) to accurately characterize sensory activity on a single-trial basis has traditionally been extremely difficult due to the very large relative amplitude of the background EEG. The most widely used EEG-based method for analyzing auditory processing has been the Auditory Evoked Potential (AEP) technique, which focuses on the  response evoked by the onsets of discrete stimuli, typically averaging the resulting responses over many trials to achieve a good signal-to-noise ratio ). More recently, regression methods have been used to quantify a mapping between continuous auditory stimuli and the resulting neural data (Kerlin et al. 2010;Lalor and Foxe 2010;Simon 2012a, 2012b;Koskinen et al. 2012;Mesgarani and Chang 2012;Power et al. 2012). Here, we used such an approach to reconstruct the input stimulus in an attempt to estimate attentional-selection in a multispeaker environment. Despite the fact that we used unaveraged EEG, and did not correct for muscle or blink artifacts, we were able to classify attention accurately on a single-trial basis. Moreover, we showed correlations between behavior and both the accuracy and consistency of our EEG-based stimulus reconstructions. Finally, we showed that correlations between behavior and reconstruction-accuracy were maximal during a timeframe of \u223c200-250 ms poststimulus, results that provide support for the importance of a late locus of attention in solving the cocktail party problem.\n\n\nDecoding Accuracy\n\nWe achieved a decoding accuracy of 82%-89%, which is comparable to that achieved in related work. For example, another recent study using stimulus-reconstruction to decode attention in a 2-speaker environment has reported an accuracy of up to 92% based on single-trial (1 min) MEG data (Ding and Simon 2012a). A direct comparison is difficult given that MEG and EEG are sensitive to different aspects of electromagnetic activity. Also, in that study, subjects were presented with the same stimuli 3 times, so that much of the background activity could be averaged out across trials. Moreover, the 2 speakers were of opposite sex, which may have led to greater separability of the neural activity to each stream based on inherent differences in the spectral characteristics of male and female speakers. Given these methodological differences, it is striking how similar our EEG results are to this previous result. This similarity is especially encouraging given that EEG is relatively low-cost, easy to use, and portable. There has been other work that has attempted to use EEG to decode selective attention in a complex auditory environment.\n\nOne recent study (Choi et al. 2013) shows that single-trial (\u223c3 s) AEP data can be used to classify the direction of attention in response to competing streams made of musical notes, achieving an accuracy of \u223c65-70%. Another study modeled the degree of gamma band synchronization between stimuli and neural activity allowing them to distinguish attention to speech versus music with an average accuracy of 69% (Looney et al. 2010). Our decoding accuracies compare extremely well with these previous reports, especially given the naturalistic, speechspecific nature of our stimuli and task, albeit with single-trials of somewhat longer duration.\n\nReconstruction-Accuracy and Decoding-Accuracy at Individual Time-Lags By examining stimulus-reconstruction using decoders based on individual time-lags we have shown that the interval of 170-250 ms is of paramount importance for decoding attention to speech. As mentioned above, this finding provides support for the existence of an important late locus of attention in solving the cocktail party problem. We have advanced this idea before with our previous paper speculating that effects at such latencies may represent a filtering process operating at the level of semantic analysis (Power et al. 2012), a speculation that fits with theories of a multistage process underpinning selective attention to speech (Treisman 1964). This notion is further supported by recent research using electrocorticography (ECoG) recordings that found significant differences in the strength of attentional effects on the representation of speech in different parts of auditory cortex Zion Golumbic et al. 2013). Indeed, the fact that attention may differentially affect processing at distinct stages of the auditory processing hierarchy could be exploited to further improve the accuracy of the decoding of selective attention to speech using single-trial EEG data. While EEG captures a more global measure of neural activity than ECoG, including that of the entire auditory processing hierarchy, it should be possible to improve decoding performance by basing stimulus reconstructions on estimated source activity from 'higher order' auditory regions where attentional modulation is particularly strong. As we have used the recorded scalp EEG data directly, the unattended speech is likely strongly represented in our data. This is evident from the success of our Unattended decoders, and is in line with previous EEG and MEG research (Kerlin et al. 2010;Ding and Simon 2012b;Mesgarani and Chang 2012;Power et al. 2012).\n\nWhile we have discussed the importance of processing at \u223c200 ms for solving the cocktail party problem, it is important to note that many dichotic speech studies have shown much earlier effects using the averaged ERP method (e.g., (Hink and Hillyard 1976;Woods et al. 1984;Teder et al. 1993). As discussed previously (Power et al. 2012), the discrepancy between our results and these earlier studies may be due to this study's explicit assumption of a relationship between the EEG and the speech envelope. This might render our method insensitive to earlier attention effects (e.g., those based on space or frequency), which may be indexed by endogenous potentials that are more easily captured by averaged ERPs. That said, we suggest that it is highly unlikely that such early effects would correlate with our high-level behavioral measures, although this needs to be tested in future work.\n\n\nSubject-Specific and Grand-Average Decoding Methods\n\nWe performed decoding using 2 approaches: Subject-Specific and Grand-Average. Both of these methods produced high decoding accuracies (Fig. 2) and similar patterns of decodingaccuracy and reconstruction-accuracy across time (Fig. 3). The fact that the Attended Subject-Specific decoding method outperformed the Attended Grand-Average decoding method may be due to inter-subject differences in how attended speech is represented in the EEG and/or to the aforementioned possibility that the Subject-Specific decoders might be slightly biased as a result of how we administered the task. The lack of any qualitative difference between the performances of the Attended Subject-Specific and Grand-Average decoders, combined with the fact that there was no quantitative difference for the Unattended decoders, lessens the immediate relevance of this issue. Future work should build more balance into the task design, possibly by training decoders on single-speaker speech prior to attentional decoding.\n\n\nSpatiotemporal Analysis of Decoders\n\nWhen comparing Attended and Unattended decoders, the inverted patterns of spatial filter weights at 200 and 250 ms (Fig. 4) was noteworthy. This was not a trivial finding given that the Attended and Unattended decoders were separately trained on the (independent) attended and unattended speech streams, respectively. We contend that these patterns provide further support for a locus of selective attention to speech operating at a specific level of the auditory processing hierarchy. Within this context, the foci of positive/negative spatial weights for the Attended/Unattended decoders reflect the relative enhancement/suppression of the attended and unattended speech at a particular level of processing. Given the relatively long latency at which these patterns become apparent, we postulate, as we have done before (Power et al. 2012), that this locus operates at a level where the content of unattended The Attended decoders are shown in black, and the Unattended decoders are shown in gray. Large markers indicate significance (P < 0.05). Test 1 is the Across Trials Reconstruction-Accuracy Test, and assesses the correlation between the percentage of questions answered correctly on each trial, and the r-value obtained for each trial. Test 2 is the Across Subjects Reconstruction-Accuracy Test, and assesses the correlation between the percentage of questions answered correctly in total, and the mean r-value obtained for each subject. Test 3 is the Across Subjects Reconstruction-Consistency Test, where we assess the correlation between the variance of the difference between r attended and r unattended , and the percentage of questions answered correctly in total.\n\nspeech is suppressed and prevented from being encoded into working memory. These inverted patterns of spatial filter weights explain the related inverted patterns of reconstructionaccuracy (Fig. 3A,B) and behavioral correlations (Fig. 5A,B) reported above.\n\nIn our previous analysis of the same data (Power et al. 2012), we found a left-lateralized attention effect at \u223c200 ms for those subjects who attended to their right ear, and a more bilateral distribution for those subjects who attended to their left ear. Therefore, it is somewhat surprising that we observed no obvious lateralization of weights in Figure 4. Analyzing the data separately for subjects attending to speech in their left and right ears provides some reconciliation. Spatial filter weights for those subjects attending their right ear were left lateralized whereas the weights for those attending their left ear were more bilaterally distributed (Supplementary Fig. S3).\n\nHaving made the point that the spatial filter weights (at least the foci at \u223c200 ms) imply relative suppression of the unattended stream, it is important to recall that we have been able to reconstruct the unattended speech stream from our data, even using individual time-lag decoders at 200 ms (Fig. 3A,Bgray triangles), although to a far lesser extent than the attended stream. This implies, unsurprisingly, that the pattern of weights in our Unattended decoders are more complex than we have been suggesting and that unattended speech remains robustly represented in the data at these time lags. These data suggest, as has been done before (Ding and Simon 2012a), that attended speech is not simply more strongly represented by the same neural generators, but rather that both speech streams are represented separately in the neural data. Therefore, if one was interested solely in obtaining the highest decoding-accuracy, for example, for a BCI application, one would simply use the Attended decoders to attempt to reconstruct the attended stream.\n\n\nCorrelation with Behavior\n\nFrom the point of view of future research, perhaps the most exciting findings from our data are the correlations between our reconstruction measures and behavioral performance. We showed a correlation across trials between our reconstruction-accuracy and behavioral performance for both the Subject-Specific and Grand-Average decoding methods. This was very surprising given the high-level nature of our task. Indeed our behavioral measure will certainly have included a sizeable amount of random variation given the trial-to-trial variability in the difficulty of the questions, and the fact that the task was multiple-choice. This would partly explain our low r-values. Moreover, the stimulus-reconstruction method, as we have applied it, is only sensitive to the cortical activity that tracks the slow amplitude envelope of speech. While the envelope of speech is important for speech comprehension (Peelle and Davis 2012;Ghitza et al. 2013), it is not a direct measure of intelligibility. Even if it were, intelligibility and short-term memory performance are unlikely to perfectly correlate with each other. Future studies using different behavioral measures that more directly index the instantaneous deployment of attention would likely show an even stronger relationship between decoding-accuracy and behavioral performance. Recent MEG work showing that the strength of the auditory cortical representation of the envelope of speech (in noise) strongly correlates with intelligibility provides support for this supposition .\n\nOur Across Subjects Reconstruction-Accuracy Test was only significant for the Grand-Average method. As we have postulated above, the lack of a correlation for the Subject-Specific approach might be due to inter-subject variation in reconstruction accuracy independent of attention. This variation might not have affected the Grand-Average analysis because of the generic nature of the decoders used in that analysis. On the same point, we observed a double dissociation when we implemented the Across Subjects Reconstruction-Consistency Test. In this case, the Subject-Specific approach produced a significant correlation with behavior, but the Grand-Average approach did not. We postulate that the Subject-Specific correlation reflects the notion that good behavioral performance should follow from consistency in sustaining attention and that this consistency should be reflected in the variance of the reconstruction accuracy. Again the generic nature of the Grand-Average decoders may explain the discrepant results by removing any inter-subject differences in the variance of the reconstruction-accuracy.\n\n\nAdvantages and Disadvantages of the Stimulus-Reconstruction Approach\n\nThe advantage of the stimulus-reconstruction method over other approaches is in its ability to incorporate all the available information across the scalp at each point in time. It does this by weighting the relative contribution from each electrode simultaneously (Fig. 4) by finding a multivariate linear filter that incorporates the channel covariance structure in the estimation of the impulse response. This approach encourages irrelevant parameters to maintain zero weight, while allowing the model to capture additional variance using electrodes potentially excluded by feature selection approaches; as such, this method can result in a significant quantitative improvement in the input-output mapping (Hastie 2009;Pasley et al. 2012). As seen in supplementary Figure 4, even 10 s of data is enough to decode attention for a number of subjects. This allows the possibility of near real-time decoding of EEG. The speed with which this can be done suggests a possible role for this method in the future design of BCIs. With sufficient data and training, it is feasible that a decoder optimized for a particular subject could have enough sensitivity to accurately decode attention based on data epochs that are far shorter than 60 s. Such sensitivity could also lead to the widespread use of stimulus-reconstruction approaches in EEG paradigms aimed at monitoring the ongoing dynamics of cognition.\n\nThe stimulus-reconstruction method has a number of disadvantages however. Firstly, unlike forward-mapping approaches, it does not produce an interpretable impulse response function for each electrode separately. This precludes the use of analysis techniques that have long been used in the event-related potential (ERP) literature. For example, the analysis of the timing and amplitude of such impulse responses can provide detailed insights into the time course of perception and cognitive processes (Luck 2005). In addition such responses can be analyzed using inverse modeling approaches to estimate the spatial locations of their neural generators (Scherg and Berg 1996). Therefore, we see the stimulusreconstruction approach as complementary to forward-mapping methods (Lalor and Foxe 2010). When used together these methods have the potential to provide a fuller understanding of human sensory, perceptual, and cognitive processes.\n\n\nSupplementary Material\n\nSupplementary Material can be found at http://www.cercor.oxfordjournals.org/ online.\n\n\nFunding\n\nFigure 2 .\n2Decoding-accuracy and reconstruction-accuracy across all trials and all subjects. (A) Decoding-accuracy for the Attended decoders (black), and the Unattended decoders (gray) using the Subject-Specific decoding method and (B) the Grand-Average decoding method. Subjects are sorted according to the performance of their Attended decoder. The solid line indicates the classification performance level at which decoding-accuracy is significantly greater than chance (63.33%) based on a binomial test at the 5% significance level. (C) Across all trials and all subjects, r attended is plotted against r unattended for the Attended (black) and Unattended (gray) Subject-Specific decoders. (D) The same information when using the Grand-Average decoders.Cerebral Cortex July 2015, V 25 N 7 1701\n\nFigure 3 .\n3Reconstruction-Accuracy and Decoding-Accuracy across all time-lags. (A) Reconstruction-Accuracy for the Subject-Specific decoding method across individual time-lags from 0 to 400 ms for the Attended decoders (black) and the Unattended decoders (gray). (B) The same analysis for the Grand-Average decoding method. (C) Decoding-Accuracy for the Subject-Specific decoding method. (D) Decoding-Accuracy for the Grand-Average decoding method.\n\nFigure 4 .\n4Topography of the decoder weights averaged over all subjects. Decoders were trained at individual time-lags, where the distribution of spatial filter weights across the scalp can be displayed as a topographic map.\n\nFigure 5 .\n5Correlation with behavior at Individual Time-Lags. Correlation between reconstruction-accuracy and behavioral performance across individual time-lags from 0 to 400 ms.\n\nTable 1\n1Comparison of decoding-accuracy across time-lags and decoding methods0-250 ms \n170-250 ms \n\nAttended (%) \nUnattended (%) \nAttended (%) \nUnattended (%) \n\nSubject-Specific \n89.0 a \n78.9 \n89.4 a \n80.8 \nGrand Average \n81.8 \n75.4 \n83.9 b \n79.1 b \n\nNote: The decoding-accruacy achieved when using broad time-lags from 0 to 250 ms, versus \nnarrow time-lags from 170 to 250 ms. \n\na \n\nA significant increase (P < 0.05) in decoding-accuracy for the Subject-Specific decoding method \ncompared with the Grand-Average decoding method. \n\nb \n\nA significant increase (P < 0.05) in decoding-accuracy when using the narrower time-lags. \n\nDownloaded from https://academic.oup.com/cercor/article-abstract/25/7/1697/457492 by guest on 27 July 2018\nCerebral Cortex July 2015, V 25 N 7 1703 Downloaded from https://academic.oup.com/cercor/article-abstract/25/7/1697/457492 by guest on 27 July 2018\nThis study was supported by a grant from Science Foundation Ireland (09-RFP-NES2382); United States National Science Foundation (BCS0642584); CELEST, a National Science Foundation Science of Learning Center (NSF SBE-0354378). Additional salary support was provided by the Irish Research Council for Science, Engineering & Technology.NotesThe authors acknowledge the organizers of the 2012 NSF-funded Neuromorphic Cognition Engineering Workshop in Telluride, CO, USA, where much of this work was conceived. The authors thank Emma-Jane Forde for assistance with preparation of the stimuli and assistance with data collection. Conflict of Interest: None declared.\nReading a neural code. W Bialek, F Rieke, Rrd Vansteveninck, D Warland, Science. 252Bialek W, Rieke F, Vansteveninck RRD, Warland D. 1991. Reading a neural code. Science. 252:1854-1857.\n\nSome experiments on the recognition of speech, with one and with two ears. E C Cherry, J Acoust Soc Am. 25Cherry EC. 1953. Some experiments on the recognition of speech, with one and with two ears. J Acoust Soc Am. 25:975-979.\n\nQuantifying attentional modulation of auditory-evoked cortical responses from single-trial electroencephalography. I Choi, S Rajaram, L Varghese, B Shinn-Cunningham, Front Hum Neurosci. 7115Choi I, Rajaram S, Varghese L, Shinn-Cunningham B. 2013. Quantifying attentional modulation of auditory-evoked cortical responses from single-trial electroencephalography. Front Hum Neurosci. 7:115.\n\nEmergence of neural encoding of auditory objects while listening to competing speakers. N Ding, J Z Simon, Proc Natl Acad Sci. 109Ding N, Simon JZ. 2012a. Emergence of neural encoding of auditory objects while listening to competing speakers. Proc Natl Acad Sci USA. 109:11854-11859.\n\nNeural coding of continuous speech in auditory cortex during monaural and dichotic listening. N Ding, J Z Simon, J Neurophysiol. 107Ding N, Simon JZ. 2012b. Neural coding of continuous speech in audi- tory cortex during monaural and dichotic listening. J Neurophysiol. 107:78-89.\n\nAdaptive temporal encoding leads to a background-insensitive cortical representation of speech. N Ding, J Z Simon, J Neurosci. 33Ding N, Simon JZ. 2013. Adaptive temporal encoding leads to a back- ground-insensitive cortical representation of speech. J Neurosci. 33:5728-5735.\n\nNeuronal oscillations and speech perception: critical-band temporal envelopes are the essence. O Ghitza, A-L Giraud, D Poeppel, Front Hum Neurosci. 6340Ghitza O, Giraud A-L, Poeppel D. 2013. Neuronal oscillations and speech perception: critical-band temporal envelopes are the essence. Front Hum Neurosci. 6:340.\n\nElements of statistical learning. T Hastie, J Friedman, Springer ScienceNew York (NYHastie T, Friedman J. 2009. Elements of statistical learning. New York (NY): Springer Science.\n\nAuditory evoked potentials during selective listening to dichotic speech messages. R F Hink, S A Hillyard, Percept Psychophys. 20Hink RF, Hillyard SA. 1976. Auditory evoked potentials during selec- tive listening to dichotic speech messages. Percept Psychophys. 20:236-242.\n\nEffects of haloperidol on selective attention a combined whole-head MEG and high-resolution EEG study. S Kahkonen, J Ahveninen, I P Jaaskelainen, S Kaakkola, R Naatanen, J Huttunen, E Pekkonen, Neuropsychopharmacology. 25Kahkonen S, Ahveninen J, Jaaskelainen IP, Kaakkola S, Naatanen R, Huttunen J, Pekkonen E. 2001. Effects of haloperidol on selective attention a combined whole-head MEG and high-resolution EEG study. Neuropsychopharmacology. 25:498-504.\n\nAttentional gain control of ongoing cortical speech representations in a \"Cocktail Party. J R Kerlin, A J Shahin, L M Miller, J Neurosci. 30Kerlin JR, Shahin AJ, Miller LM. 2010. Attentional gain control of ongoing cortical speech representations in a \"Cocktail Party\". J Neurosci. 30:620-628.\n\nIdentifying fragments of natural speech from the listener's MEG signals. M Koskinen, J Viinikanoja, M Kurimo, A Klami, S Kaski, Hari R , Hum Brain Mapp. 346Koskinen M, Viinikanoja J, Kurimo M, Klami A, Kaski S, Hari R. 2012. Identifying fragments of natural speech from the listener's MEG signals. Hum Brain Mapp. 34(6):1477-1489.\n\nNeural responses to uninterrupted natural speech can be extracted with precise temporal resolution. E C Lalor, J J Foxe, Eur J Neurosci. 31Lalor EC, Foxe JJ. 2010. Neural responses to uninterrupted natural speech can be extracted with precise temporal resolution. Eur J Neurosci. 31:189-193.\n\nTowards estimating selective auditory attention from EEG using a novel time-frequency-synchronisation framework. D Looney, C Park, Y Xia, P Kidmose, M Ungstrup, D P Mandic, Proceedings of the 2010 International Joint Conference on Neural Networks. the 2010 International Joint Conference on Neural NetworksacceptedLooney D, Park C, Xia Y, Kidmose P, Ungstrup M, Mandic DP. 2010 Towards estimating selective auditory attention from EEG using a novel time-frequency-synchronisation framework. In: Proceedings of the 2010 International Joint Conference on Neural Networks (accepted).\n\nAn introduction to the event-related potential technique. S J Luck, Luck SJ. 2005. An introduction to the event-related potential technique.\n\nSelective cortical representation of attended speaker in multi-talker speech perception. N Mesgarani, E F Chang, Nature. 485Mesgarani N, Chang EF. 2012. Selective cortical representation of attended speaker in multi-talker speech perception. Nature. 485:233-U118.\n\nInfluence of context and behavior on stimulus reconstruction from neural activity in primary auditory cortex. N Mesgarani, S V David, J B Fritz, S A Shamma, J Neurophysiol. 102Mesgarani N, David SV, Fritz JB, Shamma SA. 2009. Influence of context and behavior on stimulus reconstruction from neural activity in primary auditory cortex. J Neurophysiol. 102:3329-3339.\n\nAuditory attention and selective input modulation: a topographical ERP study. R N\u00e4\u00e4tanen, W Teder, K Alho, J Lavikainen, 6Neuroreport. 3N\u00e4\u00e4tanen R, Teder W, Alho K, Lavikainen J 1992. Auditory attention and selective input modulation: a topographical ERP study. Neu- roreport. 3.6:493-496.\n\nReconstructing speech from human auditory cortex. B N Pasley, S V David, N Mesgarani, A Flinker, S A Shamma, N E Crone, R T Knight, E F Chang, 10.1371/journal.pbio.1001251PLoS Biol. 1011001251Pasley BN, David SV, Mesgarani N, Flinker A, Shamma SA, Crone NE, Knight RT, Chang EF 2012. Reconstructing speech from human auditory cortex. PLoS Biol. 10(1):e1001251, doi:10.1371/journal. pbio.1001251.\n\nNeural oscillations carry speech rhythm through to comprehension. J E Peelle, M H Davis, doi: 10.3389/ fpsyg.2012.00320Front Psychol. 3Peelle JE, Davis MH. 2012. Neural oscillations carry speech rhythm through to comprehension. Front Psychol. 3, doi: 10.3389/ fpsyg.2012.00320.\n\nHuman auditory evoked-potentials. 2. Effects of attention. T W Picton, S A Hillyard, Electroencephalogr Clin Neurophysiol. 36Picton TW, Hillyard SA. 1974. Human auditory evoked-potentials. 2. Effects of attention. Electroencephalogr Clin Neurophysiol. 36:191-199.\n\nHuman auditory evoked-potentials.1. Evaluation of components. Electroencephalogr. T W Picton, S A Hillyard, H I Krausz, R Galambos, Clin Neurophysiol. 36Picton TW, Hillyard SA, Krausz HI, Galambos R. 1974. Human auditory evoked-potentials.1. Evaluation of components. Electroencepha- logr Clin Neurophysiol. 36:179-190.\n\nAt what time is the cocktail party? A late locus of selective attention to natural speech. A J Power, J J Foxe, E J Forde, R B Reilly, E C Lalor, Eur J Neurosci. 35Power AJ, Foxe JJ, Forde EJ, Reilly RB, Lalor EC. 2012. At what time is the cocktail party? A late locus of selective attention to natural speech. Eur J Neurosci. 35:1497-1503.\n\nNaturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents. F Rieke, D A Bodnar, W Bialek, Proc R Soc B. 262Rieke F, Bodnar DA, Bialek W. 1995. Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents. Proc R Soc B. 262:259-265.\n\nNew concepts of brain source imaging and localization. M Scherg, P Berg, Electroencephalogr Clin Neurophysiol Suppl. 46127Scherg M, Berg P. 1996. New concepts of brain source imaging and localization. Electroencephalogr Clin Neurophysiol Suppl. 46:127.\n\nReconstruction of natural scenes from ensemble responses in the lateral geniculate nucleus. G B Stanley, F F Li, Y Dan, J Neurosci. 19Stanley GB, Li FF, Dan Y. 1999. Reconstruction of natural scenes from ensemble responses in the lateral geniculate nucleus. J Neurosci. 19:8036-8042.\n\nSelection of speech messages in free-field listening. W Teder, T Kujala, R N\u00e4\u00e4t\u00e4nen, NeuroReport. 5Teder W, Kujala T, N\u00e4\u00e4t\u00e4nen R. 1993. Selection of speech messages in free-field listening. NeuroReport. 5:307-309.\n\nVerbal cues, language, and meaning in selective attention. A M Treisman, Am J Psychol. 77Treisman AM. 1964. Verbal cues, language, and meaning in selective attention. Am J Psychol. 77:206-219.\n\nEvent-related brain potentials reveal similar attentional mechanisms during selective listening and shadowing. D Woods, S A Hillyard, J Hansen, J Exp Psychol Hum Percept Perform. 10761Woods D, Hillyard SA, Hansen J. 1984. Event-related brain potentials reveal similar attentional mechanisms during selective listening and shadowing. J Exp Psychol Hum Percept Perform. 10:761.\n\nMechanisms underlying selective neuronal tracking of attended speech at a 'cocktail party. Zion Golumbic, E M Ding, N Bickel, S Lakatos, P Schevon, C A Mckhann, G M Goodman, R R Emerson, R Mehta, A D Simon, J Z , Neuron. 77Zion Golumbic EM, Ding N, Bickel S, Lakatos P, Schevon CA, McKhann GM, Goodman RR, Emerson R, Mehta AD, Simon JZ. 2013. Mechanisms underlying selective neuronal tracking of attended speech at a 'cocktail party'. Neuron. 77:980-991.\n", "annotations": {"author": "[{\"end\":276,\"start\":110},{\"end\":531,\"start\":277},{\"end\":700,\"start\":532},{\"end\":819,\"start\":701},{\"end\":1047,\"start\":820},{\"end\":1175,\"start\":1048},{\"end\":1235,\"start\":1176},{\"end\":1370,\"start\":1236},{\"end\":1543,\"start\":1371},{\"end\":1563,\"start\":1544}]", "publisher": null, "author_last_name": "[{\"end\":133,\"start\":118},{\"end\":289,\"start\":284},{\"end\":546,\"start\":537},{\"end\":718,\"start\":711},{\"end\":831,\"start\":827},{\"end\":1074,\"start\":1058},{\"end\":1190,\"start\":1184},{\"end\":1251,\"start\":1245},{\"end\":1385,\"start\":1380},{\"end\":1562,\"start\":1557}]", "author_first_name": "[{\"end\":115,\"start\":110},{\"end\":117,\"start\":116},{\"end\":281,\"start\":277},{\"end\":283,\"start\":282},{\"end\":536,\"start\":532},{\"end\":710,\"start\":701},{\"end\":824,\"start\":820},{\"end\":826,\"start\":825},{\"end\":1055,\"start\":1048},{\"end\":1057,\"start\":1056},{\"end\":1183,\"start\":1176},{\"end\":1242,\"start\":1236},{\"end\":1244,\"start\":1243},{\"end\":1377,\"start\":1371},{\"end\":1379,\"start\":1378},{\"end\":1554,\"start\":1548},{\"end\":1556,\"start\":1555}]", "author_affiliation": "[{\"end\":275,\"start\":135},{\"end\":431,\"start\":291},{\"end\":530,\"start\":433},{\"end\":583,\"start\":548},{\"end\":699,\"start\":585},{\"end\":818,\"start\":720},{\"end\":1046,\"start\":833},{\"end\":1174,\"start\":1076},{\"end\":1234,\"start\":1192},{\"end\":1329,\"start\":1253},{\"end\":1369,\"start\":1331},{\"end\":1542,\"start\":1402}]", "title": "[{\"end\":107,\"start\":1},{\"end\":1670,\"start\":1564}]", "venue": null, "abstract": "[{\"end\":3195,\"start\":1777}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3263,\"start\":3250},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3588,\"start\":3568},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3609,\"start\":3588},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3630,\"start\":3609},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3655,\"start\":3630},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3673,\"start\":3655},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3699,\"start\":3673},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4084,\"start\":4065},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4564,\"start\":4545},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4584,\"start\":4564},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4606,\"start\":4584},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4625,\"start\":4606},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4650,\"start\":4625},{\"end\":4806,\"start\":4787},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4832,\"start\":4806},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5673,\"start\":5654},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5787,\"start\":5765},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6936,\"start\":6916},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7837,\"start\":7818},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10566,\"start\":10546},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10592,\"start\":10566},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11653,\"start\":11633},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11672,\"start\":11653},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14049,\"start\":14027},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17585,\"start\":17566},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21217,\"start\":21199},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21662,\"start\":21649},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31223,\"start\":31203},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31243,\"start\":31223},{\"end\":31262,\"start\":31243},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31283,\"start\":31262},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31308,\"start\":31283},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31325,\"start\":31308},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32359,\"start\":32337},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33230,\"start\":33212},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33624,\"start\":33605},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":34444,\"start\":34426},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34566,\"start\":34552},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34836,\"start\":34810},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":35682,\"start\":35662},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35703,\"start\":35682},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35728,\"start\":35703},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35745,\"start\":35728},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36004,\"start\":35980},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36022,\"start\":36004},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":36040,\"start\":36022},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":36085,\"start\":36066},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38573,\"start\":38554},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39732,\"start\":39714},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":41025,\"start\":41003},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":42366,\"start\":42343},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":42385,\"start\":42366},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":44878,\"start\":44865},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":44897,\"start\":44878},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":46072,\"start\":46061},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":46234,\"start\":46212},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46355,\"start\":46334}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":47418,\"start\":46619},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47869,\"start\":47419},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48096,\"start\":47870},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48277,\"start\":48097},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48907,\"start\":48278}]", "paragraph": "[{\"end\":4345,\"start\":3211},{\"end\":5674,\"start\":4347},{\"end\":6938,\"start\":5676},{\"end\":7239,\"start\":6940},{\"end\":7838,\"start\":7265},{\"end\":9388,\"start\":7865},{\"end\":10061,\"start\":9427},{\"end\":10462,\"start\":10063},{\"end\":10742,\"start\":10464},{\"end\":11335,\"start\":10770},{\"end\":11673,\"start\":11370},{\"end\":11862,\"start\":11694},{\"end\":12592,\"start\":11887},{\"end\":13090,\"start\":12594},{\"end\":13708,\"start\":13110},{\"end\":14007,\"start\":13710},{\"end\":14356,\"start\":14009},{\"end\":14565,\"start\":14358},{\"end\":15294,\"start\":14567},{\"end\":16811,\"start\":15317},{\"end\":17098,\"start\":16813},{\"end\":17829,\"start\":17100},{\"end\":18215,\"start\":17831},{\"end\":18531,\"start\":18281},{\"end\":18893,\"start\":18571},{\"end\":19443,\"start\":18923},{\"end\":19685,\"start\":19445},{\"end\":19960,\"start\":19687},{\"end\":20181,\"start\":19962},{\"end\":20410,\"start\":20183},{\"end\":20857,\"start\":20412},{\"end\":21142,\"start\":20886},{\"end\":22512,\"start\":21176},{\"end\":23224,\"start\":22514},{\"end\":23966,\"start\":23226},{\"end\":24911,\"start\":24040},{\"end\":26886,\"start\":24913},{\"end\":29056,\"start\":26955},{\"end\":30230,\"start\":29110},{\"end\":30536,\"start\":30259},{\"end\":32029,\"start\":30551},{\"end\":33193,\"start\":32051},{\"end\":33839,\"start\":33195},{\"end\":35747,\"start\":33841},{\"end\":36640,\"start\":35749},{\"end\":37692,\"start\":36696},{\"end\":39412,\"start\":37732},{\"end\":39670,\"start\":39414},{\"end\":40357,\"start\":39672},{\"end\":41411,\"start\":40359},{\"end\":42973,\"start\":41441},{\"end\":44084,\"start\":42975},{\"end\":45558,\"start\":44157},{\"end\":46497,\"start\":45560},{\"end\":46608,\"start\":46524}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11369,\"start\":11336},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11693,\"start\":11674},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11886,\"start\":11863},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13109,\"start\":13091},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18263,\"start\":18216},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18280,\"start\":18263}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25525,\"start\":25518}]", "section_header": "[{\"end\":3209,\"start\":3197},{\"end\":7263,\"start\":7242},{\"end\":7863,\"start\":7841},{\"end\":9425,\"start\":9391},{\"end\":10768,\"start\":10745},{\"end\":15315,\"start\":15297},{\"end\":18569,\"start\":18534},{\"end\":18921,\"start\":18896},{\"end\":20884,\"start\":20860},{\"end\":21152,\"start\":21145},{\"end\":21174,\"start\":21155},{\"end\":24038,\"start\":23969},{\"end\":26924,\"start\":26889},{\"end\":26953,\"start\":26927},{\"end\":29108,\"start\":29059},{\"end\":30257,\"start\":30233},{\"end\":30549,\"start\":30539},{\"end\":32049,\"start\":32032},{\"end\":36694,\"start\":36643},{\"end\":37730,\"start\":37695},{\"end\":41439,\"start\":41414},{\"end\":44155,\"start\":44087},{\"end\":46522,\"start\":46500},{\"end\":46618,\"start\":46611},{\"end\":46630,\"start\":46620},{\"end\":47430,\"start\":47420},{\"end\":47881,\"start\":47871},{\"end\":48108,\"start\":48098},{\"end\":48286,\"start\":48279}]", "table": "[{\"end\":48907,\"start\":48357}]", "figure_caption": "[{\"end\":47418,\"start\":46632},{\"end\":47869,\"start\":47432},{\"end\":48096,\"start\":47883},{\"end\":48277,\"start\":48110},{\"end\":48357,\"start\":48288}]", "figure_ref": "[{\"end\":8405,\"start\":8385},{\"end\":14006,\"start\":13998},{\"end\":19138,\"start\":19130},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22020,\"start\":21997},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22225,\"start\":22218},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22391,\"start\":22384},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22721,\"start\":22714},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22890,\"start\":22879},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23438,\"start\":23429},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24340,\"start\":24329},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24756,\"start\":24745},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24910,\"start\":24899},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25834,\"start\":25826},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26753,\"start\":26745},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26884,\"start\":26877},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29212,\"start\":29204},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29950,\"start\":29942},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":30534,\"start\":30514},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36838,\"start\":36830},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36928,\"start\":36920},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37855,\"start\":37847},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":39611,\"start\":39603},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39654,\"start\":39643},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":40030,\"start\":40022},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40356,\"start\":40333},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":40663,\"start\":40655},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":44429,\"start\":44421},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":44932,\"start\":44924}]", "bib_author_first_name": "[{\"end\":49848,\"start\":49847},{\"end\":49858,\"start\":49857},{\"end\":49869,\"start\":49866},{\"end\":49886,\"start\":49885},{\"end\":50087,\"start\":50086},{\"end\":50089,\"start\":50088},{\"end\":50355,\"start\":50354},{\"end\":50363,\"start\":50362},{\"end\":50374,\"start\":50373},{\"end\":50386,\"start\":50385},{\"end\":50718,\"start\":50717},{\"end\":50726,\"start\":50725},{\"end\":50728,\"start\":50727},{\"end\":51009,\"start\":51008},{\"end\":51017,\"start\":51016},{\"end\":51019,\"start\":51018},{\"end\":51292,\"start\":51291},{\"end\":51300,\"start\":51299},{\"end\":51302,\"start\":51301},{\"end\":51569,\"start\":51568},{\"end\":51581,\"start\":51578},{\"end\":51591,\"start\":51590},{\"end\":51822,\"start\":51821},{\"end\":51832,\"start\":51831},{\"end\":52051,\"start\":52050},{\"end\":52053,\"start\":52052},{\"end\":52061,\"start\":52060},{\"end\":52063,\"start\":52062},{\"end\":52346,\"start\":52345},{\"end\":52358,\"start\":52357},{\"end\":52371,\"start\":52370},{\"end\":52373,\"start\":52372},{\"end\":52389,\"start\":52388},{\"end\":52401,\"start\":52400},{\"end\":52413,\"start\":52412},{\"end\":52425,\"start\":52424},{\"end\":52791,\"start\":52790},{\"end\":52793,\"start\":52792},{\"end\":52803,\"start\":52802},{\"end\":52805,\"start\":52804},{\"end\":52815,\"start\":52814},{\"end\":52817,\"start\":52816},{\"end\":53069,\"start\":53068},{\"end\":53081,\"start\":53080},{\"end\":53096,\"start\":53095},{\"end\":53106,\"start\":53105},{\"end\":53115,\"start\":53114},{\"end\":53127,\"start\":53123},{\"end\":53129,\"start\":53128},{\"end\":53428,\"start\":53427},{\"end\":53430,\"start\":53429},{\"end\":53439,\"start\":53438},{\"end\":53441,\"start\":53440},{\"end\":53734,\"start\":53733},{\"end\":53744,\"start\":53743},{\"end\":53752,\"start\":53751},{\"end\":53759,\"start\":53758},{\"end\":53770,\"start\":53769},{\"end\":53782,\"start\":53781},{\"end\":53784,\"start\":53783},{\"end\":54261,\"start\":54260},{\"end\":54263,\"start\":54262},{\"end\":54434,\"start\":54433},{\"end\":54447,\"start\":54446},{\"end\":54449,\"start\":54448},{\"end\":54720,\"start\":54719},{\"end\":54733,\"start\":54732},{\"end\":54735,\"start\":54734},{\"end\":54744,\"start\":54743},{\"end\":54746,\"start\":54745},{\"end\":54755,\"start\":54754},{\"end\":54757,\"start\":54756},{\"end\":55056,\"start\":55055},{\"end\":55068,\"start\":55067},{\"end\":55077,\"start\":55076},{\"end\":55085,\"start\":55084},{\"end\":55319,\"start\":55318},{\"end\":55321,\"start\":55320},{\"end\":55331,\"start\":55330},{\"end\":55333,\"start\":55332},{\"end\":55342,\"start\":55341},{\"end\":55355,\"start\":55354},{\"end\":55366,\"start\":55365},{\"end\":55368,\"start\":55367},{\"end\":55378,\"start\":55377},{\"end\":55380,\"start\":55379},{\"end\":55389,\"start\":55388},{\"end\":55391,\"start\":55390},{\"end\":55401,\"start\":55400},{\"end\":55403,\"start\":55402},{\"end\":55732,\"start\":55731},{\"end\":55734,\"start\":55733},{\"end\":55744,\"start\":55743},{\"end\":55746,\"start\":55745},{\"end\":56004,\"start\":56003},{\"end\":56006,\"start\":56005},{\"end\":56016,\"start\":56015},{\"end\":56018,\"start\":56017},{\"end\":56292,\"start\":56291},{\"end\":56294,\"start\":56293},{\"end\":56304,\"start\":56303},{\"end\":56306,\"start\":56305},{\"end\":56318,\"start\":56317},{\"end\":56320,\"start\":56319},{\"end\":56330,\"start\":56329},{\"end\":56622,\"start\":56621},{\"end\":56624,\"start\":56623},{\"end\":56633,\"start\":56632},{\"end\":56635,\"start\":56634},{\"end\":56643,\"start\":56642},{\"end\":56645,\"start\":56644},{\"end\":56654,\"start\":56653},{\"end\":56656,\"start\":56655},{\"end\":56666,\"start\":56665},{\"end\":56668,\"start\":56667},{\"end\":56986,\"start\":56985},{\"end\":56995,\"start\":56994},{\"end\":56997,\"start\":56996},{\"end\":57007,\"start\":57006},{\"end\":57266,\"start\":57265},{\"end\":57276,\"start\":57275},{\"end\":57557,\"start\":57556},{\"end\":57559,\"start\":57558},{\"end\":57570,\"start\":57569},{\"end\":57572,\"start\":57571},{\"end\":57578,\"start\":57577},{\"end\":57804,\"start\":57803},{\"end\":57813,\"start\":57812},{\"end\":57823,\"start\":57822},{\"end\":58024,\"start\":58023},{\"end\":58026,\"start\":58025},{\"end\":58270,\"start\":58269},{\"end\":58279,\"start\":58278},{\"end\":58281,\"start\":58280},{\"end\":58293,\"start\":58292},{\"end\":58630,\"start\":58626},{\"end\":58642,\"start\":58641},{\"end\":58644,\"start\":58643},{\"end\":58652,\"start\":58651},{\"end\":58662,\"start\":58661},{\"end\":58673,\"start\":58672},{\"end\":58684,\"start\":58683},{\"end\":58686,\"start\":58685},{\"end\":58697,\"start\":58696},{\"end\":58699,\"start\":58698},{\"end\":58710,\"start\":58709},{\"end\":58712,\"start\":58711},{\"end\":58723,\"start\":58722},{\"end\":58732,\"start\":58731},{\"end\":58734,\"start\":58733},{\"end\":58743,\"start\":58742},{\"end\":58745,\"start\":58744}]", "bib_author_last_name": "[{\"end\":49855,\"start\":49849},{\"end\":49864,\"start\":49859},{\"end\":49883,\"start\":49870},{\"end\":49894,\"start\":49887},{\"end\":50096,\"start\":50090},{\"end\":50360,\"start\":50356},{\"end\":50371,\"start\":50364},{\"end\":50383,\"start\":50375},{\"end\":50403,\"start\":50387},{\"end\":50723,\"start\":50719},{\"end\":50734,\"start\":50729},{\"end\":51014,\"start\":51010},{\"end\":51025,\"start\":51020},{\"end\":51297,\"start\":51293},{\"end\":51308,\"start\":51303},{\"end\":51576,\"start\":51570},{\"end\":51588,\"start\":51582},{\"end\":51599,\"start\":51592},{\"end\":51829,\"start\":51823},{\"end\":51841,\"start\":51833},{\"end\":52058,\"start\":52054},{\"end\":52072,\"start\":52064},{\"end\":52355,\"start\":52347},{\"end\":52368,\"start\":52359},{\"end\":52386,\"start\":52374},{\"end\":52398,\"start\":52390},{\"end\":52410,\"start\":52402},{\"end\":52422,\"start\":52414},{\"end\":52434,\"start\":52426},{\"end\":52800,\"start\":52794},{\"end\":52812,\"start\":52806},{\"end\":52824,\"start\":52818},{\"end\":53078,\"start\":53070},{\"end\":53093,\"start\":53082},{\"end\":53103,\"start\":53097},{\"end\":53112,\"start\":53107},{\"end\":53121,\"start\":53116},{\"end\":53436,\"start\":53431},{\"end\":53446,\"start\":53442},{\"end\":53741,\"start\":53735},{\"end\":53749,\"start\":53745},{\"end\":53756,\"start\":53753},{\"end\":53767,\"start\":53760},{\"end\":53779,\"start\":53771},{\"end\":53791,\"start\":53785},{\"end\":54268,\"start\":54264},{\"end\":54444,\"start\":54435},{\"end\":54455,\"start\":54450},{\"end\":54730,\"start\":54721},{\"end\":54741,\"start\":54736},{\"end\":54752,\"start\":54747},{\"end\":54764,\"start\":54758},{\"end\":55065,\"start\":55057},{\"end\":55074,\"start\":55069},{\"end\":55082,\"start\":55078},{\"end\":55096,\"start\":55086},{\"end\":55328,\"start\":55322},{\"end\":55339,\"start\":55334},{\"end\":55352,\"start\":55343},{\"end\":55363,\"start\":55356},{\"end\":55375,\"start\":55369},{\"end\":55386,\"start\":55381},{\"end\":55398,\"start\":55392},{\"end\":55409,\"start\":55404},{\"end\":55741,\"start\":55735},{\"end\":55752,\"start\":55747},{\"end\":56013,\"start\":56007},{\"end\":56027,\"start\":56019},{\"end\":56301,\"start\":56295},{\"end\":56315,\"start\":56307},{\"end\":56327,\"start\":56321},{\"end\":56339,\"start\":56331},{\"end\":56630,\"start\":56625},{\"end\":56640,\"start\":56636},{\"end\":56651,\"start\":56646},{\"end\":56663,\"start\":56657},{\"end\":56674,\"start\":56669},{\"end\":56992,\"start\":56987},{\"end\":57004,\"start\":56998},{\"end\":57014,\"start\":57008},{\"end\":57273,\"start\":57267},{\"end\":57281,\"start\":57277},{\"end\":57567,\"start\":57560},{\"end\":57575,\"start\":57573},{\"end\":57582,\"start\":57579},{\"end\":57810,\"start\":57805},{\"end\":57820,\"start\":57814},{\"end\":57832,\"start\":57824},{\"end\":58035,\"start\":58027},{\"end\":58276,\"start\":58271},{\"end\":58290,\"start\":58282},{\"end\":58300,\"start\":58294},{\"end\":58639,\"start\":58631},{\"end\":58649,\"start\":58645},{\"end\":58659,\"start\":58653},{\"end\":58670,\"start\":58663},{\"end\":58681,\"start\":58674},{\"end\":58694,\"start\":58687},{\"end\":58707,\"start\":58700},{\"end\":58720,\"start\":58713},{\"end\":58729,\"start\":58724},{\"end\":58740,\"start\":58735}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6604470},\"end\":50009,\"start\":49824},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":16308483},\"end\":50237,\"start\":50011},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8836019},\"end\":50627,\"start\":50239},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15570759},\"end\":50912,\"start\":50629},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":682727},\"end\":51193,\"start\":50914},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11814655},\"end\":51471,\"start\":51195},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4851297},\"end\":51785,\"start\":51473},{\"attributes\":{\"id\":\"b7\"},\"end\":51965,\"start\":51787},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":144015003},\"end\":52240,\"start\":51967},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10626905},\"end\":52698,\"start\":52242},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":24404227},\"end\":52993,\"start\":52700},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":17228790},\"end\":53325,\"start\":52995},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6699749},\"end\":53618,\"start\":53327},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12112966},\"end\":54200,\"start\":53620},{\"attributes\":{\"id\":\"b14\"},\"end\":54342,\"start\":54202},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4320045},\"end\":54607,\"start\":54344},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":12588615},\"end\":54975,\"start\":54609},{\"attributes\":{\"id\":\"b17\"},\"end\":55266,\"start\":54977},{\"attributes\":{\"doi\":\"10.1371/journal.pbio.1001251\",\"id\":\"b18\",\"matched_paper_id\":1300841},\"end\":55663,\"start\":55268},{\"attributes\":{\"doi\":\"doi: 10.3389/ fpsyg.2012.00320\",\"id\":\"b19\",\"matched_paper_id\":14958251},\"end\":55942,\"start\":55665},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4608685},\"end\":56207,\"start\":55944},{\"attributes\":{\"id\":\"b21\"},\"end\":56528,\"start\":56209},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16649688},\"end\":56870,\"start\":56530},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":38296649},\"end\":57208,\"start\":56872},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":46405309},\"end\":57462,\"start\":57210},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15452913},\"end\":57747,\"start\":57464},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13125253},\"end\":57962,\"start\":57749},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":28653670},\"end\":58156,\"start\":57964},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":36124798},\"end\":58533,\"start\":58158},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12700766},\"end\":58989,\"start\":58535}]", "bib_title": "[{\"end\":49845,\"start\":49824},{\"end\":50084,\"start\":50011},{\"end\":50352,\"start\":50239},{\"end\":50715,\"start\":50629},{\"end\":51006,\"start\":50914},{\"end\":51289,\"start\":51195},{\"end\":51566,\"start\":51473},{\"end\":52048,\"start\":51967},{\"end\":52343,\"start\":52242},{\"end\":52788,\"start\":52700},{\"end\":53066,\"start\":52995},{\"end\":53425,\"start\":53327},{\"end\":53731,\"start\":53620},{\"end\":54431,\"start\":54344},{\"end\":54717,\"start\":54609},{\"end\":55316,\"start\":55268},{\"end\":55729,\"start\":55665},{\"end\":56001,\"start\":55944},{\"end\":56289,\"start\":56209},{\"end\":56619,\"start\":56530},{\"end\":56983,\"start\":56872},{\"end\":57263,\"start\":57210},{\"end\":57554,\"start\":57464},{\"end\":57801,\"start\":57749},{\"end\":58021,\"start\":57964},{\"end\":58267,\"start\":58158},{\"end\":58624,\"start\":58535}]", "bib_author": "[{\"end\":49857,\"start\":49847},{\"end\":49866,\"start\":49857},{\"end\":49885,\"start\":49866},{\"end\":49896,\"start\":49885},{\"end\":50098,\"start\":50086},{\"end\":50362,\"start\":50354},{\"end\":50373,\"start\":50362},{\"end\":50385,\"start\":50373},{\"end\":50405,\"start\":50385},{\"end\":50725,\"start\":50717},{\"end\":50736,\"start\":50725},{\"end\":51016,\"start\":51008},{\"end\":51027,\"start\":51016},{\"end\":51299,\"start\":51291},{\"end\":51310,\"start\":51299},{\"end\":51578,\"start\":51568},{\"end\":51590,\"start\":51578},{\"end\":51601,\"start\":51590},{\"end\":51831,\"start\":51821},{\"end\":51843,\"start\":51831},{\"end\":52060,\"start\":52050},{\"end\":52074,\"start\":52060},{\"end\":52357,\"start\":52345},{\"end\":52370,\"start\":52357},{\"end\":52388,\"start\":52370},{\"end\":52400,\"start\":52388},{\"end\":52412,\"start\":52400},{\"end\":52424,\"start\":52412},{\"end\":52436,\"start\":52424},{\"end\":52802,\"start\":52790},{\"end\":52814,\"start\":52802},{\"end\":52826,\"start\":52814},{\"end\":53080,\"start\":53068},{\"end\":53095,\"start\":53080},{\"end\":53105,\"start\":53095},{\"end\":53114,\"start\":53105},{\"end\":53123,\"start\":53114},{\"end\":53132,\"start\":53123},{\"end\":53438,\"start\":53427},{\"end\":53448,\"start\":53438},{\"end\":53743,\"start\":53733},{\"end\":53751,\"start\":53743},{\"end\":53758,\"start\":53751},{\"end\":53769,\"start\":53758},{\"end\":53781,\"start\":53769},{\"end\":53793,\"start\":53781},{\"end\":54270,\"start\":54260},{\"end\":54446,\"start\":54433},{\"end\":54457,\"start\":54446},{\"end\":54732,\"start\":54719},{\"end\":54743,\"start\":54732},{\"end\":54754,\"start\":54743},{\"end\":54766,\"start\":54754},{\"end\":55067,\"start\":55055},{\"end\":55076,\"start\":55067},{\"end\":55084,\"start\":55076},{\"end\":55098,\"start\":55084},{\"end\":55330,\"start\":55318},{\"end\":55341,\"start\":55330},{\"end\":55354,\"start\":55341},{\"end\":55365,\"start\":55354},{\"end\":55377,\"start\":55365},{\"end\":55388,\"start\":55377},{\"end\":55400,\"start\":55388},{\"end\":55411,\"start\":55400},{\"end\":55743,\"start\":55731},{\"end\":55754,\"start\":55743},{\"end\":56015,\"start\":56003},{\"end\":56029,\"start\":56015},{\"end\":56303,\"start\":56291},{\"end\":56317,\"start\":56303},{\"end\":56329,\"start\":56317},{\"end\":56341,\"start\":56329},{\"end\":56632,\"start\":56621},{\"end\":56642,\"start\":56632},{\"end\":56653,\"start\":56642},{\"end\":56665,\"start\":56653},{\"end\":56676,\"start\":56665},{\"end\":56994,\"start\":56985},{\"end\":57006,\"start\":56994},{\"end\":57016,\"start\":57006},{\"end\":57275,\"start\":57265},{\"end\":57283,\"start\":57275},{\"end\":57569,\"start\":57556},{\"end\":57577,\"start\":57569},{\"end\":57584,\"start\":57577},{\"end\":57812,\"start\":57803},{\"end\":57822,\"start\":57812},{\"end\":57834,\"start\":57822},{\"end\":58037,\"start\":58023},{\"end\":58278,\"start\":58269},{\"end\":58292,\"start\":58278},{\"end\":58302,\"start\":58292},{\"end\":58641,\"start\":58626},{\"end\":58651,\"start\":58641},{\"end\":58661,\"start\":58651},{\"end\":58672,\"start\":58661},{\"end\":58683,\"start\":58672},{\"end\":58696,\"start\":58683},{\"end\":58709,\"start\":58696},{\"end\":58722,\"start\":58709},{\"end\":58731,\"start\":58722},{\"end\":58742,\"start\":58731},{\"end\":58748,\"start\":58742}]", "bib_venue": "[{\"end\":53926,\"start\":53868},{\"end\":49903,\"start\":49896},{\"end\":50113,\"start\":50098},{\"end\":50423,\"start\":50405},{\"end\":50754,\"start\":50736},{\"end\":51041,\"start\":51027},{\"end\":51320,\"start\":51310},{\"end\":51619,\"start\":51601},{\"end\":51819,\"start\":51787},{\"end\":52092,\"start\":52074},{\"end\":52459,\"start\":52436},{\"end\":52836,\"start\":52826},{\"end\":53146,\"start\":53132},{\"end\":53462,\"start\":53448},{\"end\":53866,\"start\":53793},{\"end\":54258,\"start\":54202},{\"end\":54463,\"start\":54457},{\"end\":54780,\"start\":54766},{\"end\":55053,\"start\":54977},{\"end\":55448,\"start\":55439},{\"end\":55797,\"start\":55784},{\"end\":56065,\"start\":56029},{\"end\":56358,\"start\":56341},{\"end\":56690,\"start\":56676},{\"end\":57028,\"start\":57016},{\"end\":57325,\"start\":57283},{\"end\":57594,\"start\":57584},{\"end\":57845,\"start\":57834},{\"end\":58049,\"start\":58037},{\"end\":58335,\"start\":58302},{\"end\":58754,\"start\":58748}]"}}}, "year": 2023, "month": 12, "day": 17}