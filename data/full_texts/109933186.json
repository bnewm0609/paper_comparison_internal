{"id": 109933186, "updated": "2023-10-01 19:44:34.852", "metadata": {"title": "Two Body Problem: Collaborative Visual Task Completion", "authors": "[{\"first\":\"Unnat\",\"last\":\"Jain\",\"middle\":[]},{\"first\":\"Luca\",\"last\":\"Weihs\",\"middle\":[]},{\"first\":\"Eric\",\"last\":\"Kolve\",\"middle\":[]},{\"first\":\"Mohammad\",\"last\":\"Rastegari\",\"middle\":[]},{\"first\":\"Svetlana\",\"last\":\"Lazebnik\",\"middle\":[]},{\"first\":\"Ali\",\"last\":\"Farhadi\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Schwing\",\"middle\":[]},{\"first\":\"Aniruddha\",\"last\":\"Kembhavi\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 4, "day": 11}, "abstract": "Collaboration is a necessary skill to perform tasks that are beyond one agent's capabilities. Addressed extensively in both conventional and modern AI, multi-agent collaboration has often been studied in the context of simple grid worlds. We argue that there are inherently visual aspects to collaboration which should be studied in visually rich environments. A key element in collaboration is communication that can be either explicit, through messages, or implicit, through perception of the other agents and the visual world. Learning to collaborate in a visual environment entails learning (1) to perform the task, (2) when and what to communicate, and (3) how to act based on these communications and the perception of the visual world. In this paper we study the problem of learning to collaborate directly from pixels in AI2-THOR and demonstrate the benefits of explicit and implicit modes of communication to perform visual tasks. Refer to our project page for more details: https://prior.allenai.org/projects/two-body-problem", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1904.05879", "mag": "2982320652", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/JainWKRLFSK19", "doi": "10.1109/cvpr.2019.00685"}}, "content": {"source": {"pdf_hash": "e49f984439d132b5aa89855b136e4ba90389c7cf", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1904.05879v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1904.05879", "status": "GREEN"}}, "grobid": {"id": "1c3ead97fd5c1dab159542e03d02f57891adcbf0", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e49f984439d132b5aa89855b136e4ba90389c7cf.txt", "contents": "\nTwo Body Problem: Collaborative Visual Task Completion\n\n\nUnnat Jain \nUIUC\n\n\nLuca Weihs \nPRIOR\nAllen Institute for AI\n\n\nEric Kolve \nPRIOR\nAllen Institute for AI\n\n\nMohammad Rastegari \nPRIOR\nAllen Institute for AI\n\n\nXnor.ai\n\n\nSvetlana Lazebnik \nUIUC\n\n\nAli Farhadi \nPRIOR\nAllen Institute for AI\n\n\nUniversity of Washington\n\n\nXnor.ai\n\n\nAlexander Schwing \nUIUC\n\n\nAniruddha Kembhavi \nPRIOR\nAllen Institute for AI\n\n\nTwo Body Problem: Collaborative Visual Task Completion\n\nFigure 1: Two agents learn to successfully navigate through a previously unseen environment to find, and jointly lift, a heavy TV. Without learned communication, agents attempt many failed actions and pickups. With learned communication, agents send a message when they observe or when they intend to interact with the TV. The agents also learn to grab the opposite ends of the TV and coordinate to do so.AbstractCollaboration is a necessary skill to perform tasks that are beyond one agent's capabilities. Addressed extensively in both conventional and modern AI, multi-agent collaboration has often been studied in the context of simple grid worlds. We argue that there are inherently visual aspects to collaboration which should be studied in visually rich environments. A key element in collaboration is communication that can be either explicit, through messages, or implicit, through perception of the other agents and the visual world. Learning to collaborate in a visual environment entails learning (1) to perform the task, (2) when and what to communicate, and (3) how to act based on these communications and the perception of the visual world. In this paper we study the problem of learning to collaborate directly from pixels in AI2-THOR and demonstrate the benefits of explicit and implicit modes of communication to perform visual tasks. Refer to our project page for more details: https://prior.allenai.org/projects/ two-body-problem * indicates equal contributions.\n\nIntroduction\n\nDeveloping collaborative skills is known to be more cognitively demanding than learning to perform tasks independently. In AI, multi-agent collaboration has been studied in more conventional [32,43,9,58] and modern settings [53,28,79,35,56,61]. These studies have mainly been performed on grid-worlds and have factored out the role of perception in collaboration.\n\nIn this paper we argue that there are aspects of collaboration that are inherently visual. Studying collaboration in simplistic environments does not permit to observe the interplay between perception and communication, which is necessary for effective collaboration. Imagine moving a piece of furniture with a friend. Part of the collaboration is rooted in explicit communication through exchanging messages, and some part of it is done through implicit communication through interpreting perceivable cues about the other agents behavior. If you see your friend going around the furniture to grab it, you would naturally stay on the opposite side to avoid toppling it over. Additionally, communication and collaboration should be considered jointly with the task itself. The way you communicate, either explicitly or implicitly, in a soccer game is very different from when you move furniture. This suggests that factoring out per-ception and studying collaboration in isolation (grid-world) might not result in an ideal outcome.\n\nIn short, learning to perform tasks collaboratively in a visual environment entails joint learning of (1) how to perform tasks in that environment, (2) when and what to communicate, and (3) how to act based on implicit and explicit communication. In this work, we develop one of the first frameworks that enables the study of explicitly and implicitly communicating agents collaborating together in a photo-realistic environment.\n\nTo this end we consider the problem of finding and lifting bulky items, ones which cannot be lifted by a single agent. While conceptually simple, attaining proficiency in this task requires multiple stages of communication. The agents must search for the object of interest in the environment (possibly communicating their findings to each other), position themselves appropriately (for instance, opposing each other), and then lift the object simultaneously. If the agents position themselves incorrectly, lifting the object will cause it to topple over. Similarly, if the agents pick up the object at different time steps, they will not succeed.\n\nTo study this task, we use the AI2-THOR virtual environment [48], a photo-realistic, physics-enabled environment of indoor scenes used in past work to study single agent behavior. We extend AI2-THOR to enable multiple agents to communicate and interact.\n\nWe explore collaboration along several modes: (1) The benefits of communication for spatially constrained tasks (e.g., requiring agents to stand across one another while lifting an object) vs. unconstrained tasks. (2) The ability of agents to implicitly and explicitly communicate to solve these tasks. (3) The effect of the expressivity of the communication channel on the success of these tasks. (4) The efficacy of these developed communication protocols on known environments and their generalizability to new ones. (5) The challenges of egocentric visual environments vs. grid-world settings.\n\nWe propose a Two Body Network, or TBONE, for modeling the policies of agents in our environments. TBONE operates on a visual egocentric observation of the 3D world, a history of past observations and actions of the agent, as well as messages received from other agents in the scene. At each time step, agents go through two rounds of communication, akin to sending a message each and then replying to messages that are received in the first round. TBONE is trained with a warm start using a variant of DAgger [70], followed by a minimization of a sum of an A3C loss and a cross entropy loss between the agents actions and the actions of an expert policy.\n\nWe perform a detailed experimental analysis of the impact of communication using metrics including accuracy, number of failed pickup actions, and episode lengths. Following our above research questions, our findings show that: (1) Communication clearly benefits both constrained \nt \u2212 1 [ ] , a \u02c6 ( 1 ) t a \u02c6 ( 2 ) t Environment o ( 1 ) t o ( 2 ) t Comm. channel o ( 1 ) t + 1 o ( 2 ) t + 1\ns a m p le s a m p le Figure 2: A schematic depicting the inputs to the policy network. An agent's policy operates on a partial observation of the scene's state and a history of previous observations, actions, and messages received.\n\nand unconstrained tasks but is more advantageous for constrained tasks.\n\n(2) Both explicit and implicit communication are exploited by our agents and both are beneficial, individually and jointly. (3) For our tasks, large vocabulary sizes are beneficial. (4) Our agents generalize well to unseen environments. (5) Abstracting our environments towards a grid-world setting improves accuracy, confirming our notion that photo-realistic visual environments are more challenging than grid-world like settings. This is consistent with findings by past works for single agent scenarios. Finally we interpret the explicit mode of communication between agents by fitting logistic regression models to the messages to predict the values such as oracle distance to target, next action, etc., and find strong evidence matching our intuitions about the usage of messages between agents.\n\n\nRelated Work\n\nWe now review related work in the directions of visual navigation, navigation and language, visual multi-agent reinforcement learning (RL), and virtual learning environments employed in past works to evaluate algorithms. Visual Navigation: A large body of work focuses on visual navigation, i.e., locating a target using only visual input. Prominent early map-based navigation methods [47,6,7,64] use a global map to make decisions. More recent approaches [76,87,23,85,46,71] reconstruct the map on the fly. Simultaneous localization and mapping [84,74,24,12,67,77] consider mapping in isolation. Upon having obtained a map of the environment, planning methods [13,44,52] yield a sequence of actions to achieve the goal. Combinations of joint mapping and planning have also been discussed [27,50,49,31,3]. Map-less methods [38,54,69,72,66,92,36] often formulate the task as obstacle avoidance given an input image or reconstruct a map implicitly. Conceptually, for visual navigation, we \n+ + + Residual connect h \u02dc h \u02c6 h v \u03b8 \u03c0 \u03b8 h \u02dc h \u02c6 h \u03c0 \u03b8 v \u03b8\nTalk stage Reply stage Figure 3: Overview of our TBONE architecture for collaboration.\n\nmust learn a mapping from visual observations to actions which influence the environment. Consequently the task is well suited for an RL formulation, a perspective which has become popular recently [62,1,16,17,33,42,86,59,5,8,90,25,36,91,37]. Some of these approaches compute actions from observations directly while others attempt to explicitly/implicitly reconstruct a map. Following recent techniques, our proposed approach also uses RL for visual navigation. While our proposed approach could be augmented with explicit or implicit maps, our focus is upon multi-agent communication. In the spirit of factorizing out orthogonal extensions from the model, we defer such extensions to future work. Navigation and Language: Another line of work has focused on communication between humans and virtual agents. These methods more accurately reflect real-world scenarios since humans are more likely to interact with an agent using language rather than abstract specifications. Recently Das et al. [19,21] and Gordon et al. [34] proposed to combine question answering with robotic navigation. Chaplot et al. [15], Anderson et al. [2] and Hill et al. [39] propose to guide a virtual agent via language commands. While language directed navigation is an important task, we consider an orthogonal direction where multiple agents need to collaboratively solve a specified task. Since visual multi-agent RL is itself challenging, we refrain from introducing natural language complexities. Instead, in this paper, we are interested in developing a systematic understanding of the utility and character of communication strategies developed by multiple agents through RL. Visual Multi-Agent Reinforcement Learning: Multiagent systems result in non-stationary environments posing significant challenges. Multiple approaches have been proposed over the years to address such concerns [82,83,81,30]. Similarly, a variety of settings from multiple cooperative agents to multiple competitive ones have been investigated [51,65,57,11,63,35,56,29,61].\n\nAmong the plethora of work on multi-agent RL, we want to particularly highlight work by Giles [61], all of which investigate the discovery of communication and language in the multi-agent setting using maze-based tasks, tabular setups, or Markov games. For instance, Lazaridou et al. [53] perform experiments using a referential game of image guessing, Foerster et al. [28] focus on switch-riddle games, Sukhbaatar et al. [79] discuss multi-turn games on the MazeBase environment [80], and Mordatch and Abbeel [61] evaluate on a rectangular environment with multiple target locations and tasks. Most recently, Das et al. [20] demonstrate, especially in grid-world settings, the efficacy of targeted communication where agents must learn to whom they should send messages.\n\nOur work differs from the above body of work in that we consider communication for visual tasks, i.e., our agents operate in rich visual environments rather than a grid-like maze, a tabular setup or a Markov game. We are particularly interested in investigating how communication and perception support each other. Reinforcement Learning Environments: As just discussed, our approach is evaluated on a rich visual environment. Suitable environment simulators are AI2-THOR [48], House3D [88], HoME [10], MINOS [73] for Matter-port3D [14] and SUNCG [78]. Common to these environments is the goal of modeling real world living environments with substantial visual diversity. This is in contrast to other RL environments such as the arcade environment [4], Vizdoom [45], block towers [55], Malmo [41], TORCS [89], or MazeBase [80]. Of these environments, we chose AI2-THOR as it was easy to extend, provides high fidelity images, and has interactive physics enabled scenes, opening up interesting multi-agent research directions beyond this current work.\n\n\nCollaborative Task Completion\n\nWe are interested in understanding how two agents can learn, from pixels, to communicate so as to effectively and collaboratively solve a given task. To this end, we develop a task for two agents which consists of two components, each tailored to a desirable skill for indoor agents. The components are: (1) visual navigation, which the agents may solve independently, but which may also benefit from some collaboration; and (2) jointly synchronized interaction with the environment, which typically requires collaboration to succeed. The choice of these components stems from the fact that navigating to a desired position in an environment or to locate a desired object is a quintessential skill for an indoor agent, and synchronized interaction is fundamental to understanding any collaborative multi-agent setting. We first discuss the collaborative task more formally, then detail the components of our network, TBONE, used to complete the task.\n\n\nTask: Find and Lift Furniture\n\nWe task two agents to lift a heavy target object in an environment, a task that cannot be completed by a single agent owing to the weight of the object. The two agents as well as the target object are placed at random locations in a randomly chosen AI2-THOR living room scene. Both agents must locate the target, approach it, position themselves appropriately, and then simultaneously lift it.\n\nTo successfully complete the task, both agents perform actions over time according to the same learned policy (Fig. 2). Since our agents are homogeneous, we share the policy parameters for both agents. Previous works [35,61] have found this to train agents more efficiently. For an agent, the policy operates on (1) an ego-centric observation of the environment as well as a previous history of (a) observations, (b) actions taken by the agent, and (c) messages sent by the other agent. At each time step, the two agents process their current observations and then perform two rounds of explicit communication. Each round of communication involves each of the agents sending a single message to the other. The agents also have the ability to watch the other agent (when in view) and possibly even recognize their actions over time, thereby using implicit com-munication as a means of gathering information.\n\nMore formally, an agent perceives the scene at time t in the form of an image o t and chooses its action a t \u2208 A by computing a policy, i.e., a probability distribution \u03c0 \u03b8 (a t |o t , h t\u22121 ), over all actions a t \u2208 A. In our case, the images o t are first-person views obtained from AI2-THOR. Following classical recurrent models, our policy leverages information computed in the previous time-step via the representation h t\u22121 . The set of available actions A consists of the five options MOVEAHEAD, ROTATELEFT, ROTATERIGHT, PASS, and PICKUP. The actions MOVEA-HEAD, ROTATELEFT, and ROTATERIGHT allow the agent to navigate. To simplify the complexities of continuous time movement we let a single MOVEAHEAD action correspond to a step of size 0.25 meters, a single ROTATERIGHT action correspond to a 90 degree rotation clockwise, and a single ROTATELEFT action correspond to a 90 degree rotation anti-clockwise. The PASS action indicates that the agent should stand-still and PICKUP is the agent's attempt to pick up the target object. Critically, the PICKUP action has the desired effect only if three preconditions are met, namely both agents must (1) be within 1.5 meters of the object and be looking directly at it, (2) be a minimum distance away from one another, and (3) carry out the PICKUP action simultaneously. Note that asking agents to be at a minimum distance from one another amounts to adding specific constraints on their relative spatial layouts with regards to the object and hence requires the agents to reason about such relationships. This is akin to requiring the agents to stand across each other when they pick up the object. The motivation to model spatial constraints with a minimum distance constraint is to allow us to easily manipulate the complexity of the task. For instance, setting this minimum distance to 0 loosens the constraints and only requires agents to meet two of the above preconditions.\n\nIn our experiments, we train agents to navigate within and interact with 30 indoor environments. Specifically, an episode is considered successful if both agents navigate to a known object and, jointly, lift it within a fixed number of time steps. As our focus is the study of collaboration and not primarily object recognition, we keep the sought object, a television, constant. Importantly, environments as well as the agents' start locations and the target object location are randomly assigned at the start of each episode. Consequently, the agents must learn to (1) search for the target object in different environments, (2) navigate towards it, (3) stay within the object's vicinity until the second agent arrives, (4) coordinate that both agents are apart from each other by at least the specified distance, and (5) finally and jointly perform the pickup action.\n\nIntuitively, we expect the agents to perform better on this task if they can communicate with each other. We conjecture that explicit communication will allow them to both signal when they have found the object and, after naviga-  Table 1: Effect of adding oracle depth as well as moving to a grid-world setting on unseen scenes, Constrained task.\n\ntion, help coordinate when to attempt a PICKUP, whereas implicit communication will help to reason about their relative locations with regards to each other and the object. To measure the impact of explicit and implicit means of communication in the given task, we train models with and without message passing as well as by making agents (in)visible to one another. Explicit communication would seem to be especially important in the case where implicit communication isn't possible. Without any communication, there seems to be no better strategy than for both agents to independently navigate to the object and then repeatedly try PICKUP actions in the hope that they will be, at some point, in sync. The expectation that such a policy may be forthcoming gives rise to one of our metrics, namely the count of failed pickup events among both agents in an episode. We discuss metrics and results in Section 4.\n\n\nNetwork Architecture\n\nIn the following we describe the learned policy (actor) \u03c0 \u03b8 (a t |o t , h t\u22121 ) and value (critic) v \u03b8 (o t , h t\u22121 ) functions for each agent in greater detail. See Fig. 3 for a high level visualization of our network structure. Let \u03b8 represent a catch-all parameter encompassing all the learnable weights in TBONE. At the t-th timestep in an episode we obtain as an agent's observation, from AI2-THOR, a 3 \u00d7 84 \u00d7 84 RGB image o t which is then processed by a four layer CNN c \u03b8 into the 1024-dimensional vector c \u03b8 (o t ). Onto c \u03b8 (o t ) we append an 8-dimensional learnable embedding e which, unlike all other weights in the model, is not shared between the two agents. This agent embedding e gives the agents the capacity to develop distinct complementary strategies. The concatenation of c \u03b8 (o t ) and e is fed, along with historical embeddings from time t \u2212 1, into a long-short-termmemory (LSTM) [40] cell resulting in a 512-dimensional output vector h t capturing the beliefs of the agent given its prior history and most recent observation. Intuitively, we now would like the two agents to refine their beliefs via communication before deciding on a course of action. We consider this process in several stages (Fig. 4). Communication: We model communication by allowing the agents to send one another a d-dimensional vector derived by performing soft-attention over a vocabulary of a fixed size K. More formally, let W send \u2208 R K\u00d7512 , b send \u2208 R 512 , and V send \u2208 R d\u00d7K be (learnable) weight matrices with the columns of V send representing our vocabulary. Then, given the representation h t described above, the agent computes soft-attention over the vocabulary producing the message m send = V send softmax(W send h t + b send ) \u2208 R d , which is relayed to the other agent. Belief Refinement: Given the agents' current beliefs h t and the message m received from the other agent, we model the process of refining one's beliefs given new information using a two layer fully connected neural network with a residual connection. In particular, h t and m received are concatenated, and new beliefs\u0125 t are formed by computing\u0125\nt = h t +ReLU(W 2 ReLU(W 1 [ h t ; m received ]+b 1 )+b 2 ), where W 1 \u2208 R 512\u00d7(512+d) , b 1 , b 2 \u2208 R 512 , and W 2 \u2208 R 512\u00d7512\nare learnable weight matrices. We set the value of d to 8.\n\nReply and Additional Refinement: The above step is followed by one more round of communication and belief refinement by which the representation\u0125 t is transformed into h t . These additional stages have new sets of learnable parameters including a new vocabulary matrix. Note that, unlike in the standard LSTM framework where h t\u22121 would be fed into the cell at time t, we instead give the LSTM cell the refined vector h t\u22121 . Linear Actor and Critic: Finally the policy and value functions are computed as\n\u03c0 \u03b8 (a t |o t , h t\u22121 ) = softmax(W actor h t +b actor ), and v \u03b8 (o t , h t\u22121 ) = W critic h t + b critic where W actor \u2208 R 5\u00d7512 , b actor \u2208 R 5 , W critic \u2208 R 1\u00d7512 , and b critic \u2208 R 1 are learned.\n\nLearning\n\nSimilar to others [19,36,18,22], we found training of our agents from scratch to be infeasible when using a pure reinforcement learning (RL) approach, e.g., with asynchronous actor critic (A3C) [60], even in simplified settings, without extensive reward shaping. Indeed, often the agents must make upwards of 60 actions to navigate to the object and will only successfully complete the episode and receive a reward if they jointly pick up the object. This setting of extremely sparse rewards is a well known failure mode of standard RL techniques. Following the above prior work, we use a \"warm-start\" by training with a variant of DAgger [70]. We train our models online using imitation learning for 10,000 episodes with actions for episode i sampled from the mixture (1 \u2212 \u03b1 i )\u03c0 \u03b8i\u22121 + \u03b1 i \u03c0 * where \u03b8 i\u22121 are the parameters learned by the model up to episode i, \u03c0 * is an expert policy (described below), and \u03b1 i decays linearly from 0.9 to 0 as i increases. This initial warm-start allows the agents to learn a policy for which rewards are far less sparse, allowing traditional RL approaches to be applicable. Note that our expert supervision only applies to the actions, there is no supervision for how agents should communicate. Instead the agents must learn to communicate in such a way that would increase the probability of expert actions.\n\nAfter the warm-start period, trajectories are sampled purely from the agent's current policy and we train our agents by minimizing the sum of an A3C loss, and a cross entropy loss between the agents' actions and the actions of an expert policy. The A3C and cross entropy losses here are complementary, each helping correct for a deficiency in the other. Namely, the gradients from an A3C loss tend to be noisy and can, at times, derail or slow training; the gradients from the cross entropy loss are noise free and thereby stabilize training. A pure cross entropy loss however fails to sufficiently penalize certain undesirable actions. For instance, diverging from the expert policy by taking a MOVEAHEAD action when directly in front of a wall should be more strongly penalized than when the area in front of the agent is free as the former case may result in damage to the agent; both these cases are penalized equally by a cross entropy loss. The A3C loss, on the other hand, accounts for such differences easily so long as they are reflected by the rewards the agent receives.\n\nWe now describe the expert policy. If both agents can see the TV, are within 1.5 meters of it, and are at least a given minimum distance apart from one another then the expert action is to PICKUP for both agents. Otherwise given a fixed scene and TV position we obtain, from AI2-THOR, the set T = {t 1 , . . . , t m } of all positions (on a grid with square size 0.25 meters) and rotations within 1.5 meters of the TV from which the TV is visible. Letting ik be the length of the shortest path from the current position of agent i \u2208 {0, 1} to t k we then assign each (t j , t k ) \u2208 T \u00d7 T the score s jk = 0j + 1k . We then compute the lowest scoring tuple (s, t) \u2208 T \u00d7 T for which s and t are at least a given minimum distance apart and assign agent 0 the expert action corresponding to the first navigational step along the shortest path from agent 0 to s (and similarly for agent 1 whose expert goal is t).\n\nNote that our training strategy and communication scheme can be extended to more than two agents. We defer such an analysis to future work, a careful analysis of the two-agent setting being an appropriate first step. Implementation Details. Each model was trained for 100,000 episodes. Each episode is initialized in a random train (seen) scene of AI2-THOR. Rewards provided to the agents are: 1 to both agents for a successful pickup action, constant -0.01 step penalty to discourage long trajectories, -0.02 for any failed action (e.g., running into a wall) and -0.1 for a failed pickup action. Episodes run for a maximum of 500 steps (250 steps for each agent) after which the episode is considered failed.\n\n\nExperiments\n\nIn this section, we present our evaluation of the effect of communication towards collaborative visual task completion. We first briefly describe the multi-agent extensions made to AI2-THOR, the environments used for our analysis, the two tasks used as a test bed and metrics considered. This is followed by a detailed empirical analysis of the tasks. We then provide a statistical analysis of the explicit communication messages used by the agents to solve the tasks, which sheds light on their content. Finally we present qualitative results. Framework and Data. We extend the AI2-THOR environment to support multiple agents that can each be independently controlled. In particular, we extend the existing initialization action to accept an agentCount parameter allowing an arbitrarily large number of agents to be specified. When additional agents are spawned, each is visually depicted as a capsule of a distinct color. This allows agents to observe each other's presence and impact on the environment, a form of implicit communication. We also provide a parameter to render agents invisible to one another, which allows us to study the benefits of implicit communication. Newly spawned agents have the full capabilities of a single agent, being able to interact with the environment by, for example, picking up and opening objects. These changes are publicly available with AI2-THOR v1.0. We consider the 30 AI2-THOR living room scenes for our analysis, since they are the largest in terms of floor area and also contain a large amount of furniture. We train on 20 and test on the 20 seen scenes as well as the remaining 10 unseen ones. Tasks. We consider two tasks, both requiring the two agents to simultaneously pick up the TV in the environment: (1) Unconstrained: No constraints are imposed here with regards to the locations of the agents with respect to each other. (2) Constrained: The agents must be at least 8 steps from each other (akin to requiring them to stand across each other when they pick up the object). Intuitively, we expect the Constrained setting to be more difficult than the Unconstrained, since it requires the agents to spatially reason Note that accuracy alone isn't revealing enough. Na\u00efve agents that wander around and randomly pick up objects will eventually succeed. Also, agents that correctly locate the TV and then keep attempting a pickup in the hope of synchronizing with the other agent will also succeed. Both these cases will however do poorly on the other metrics. Quantitative analysis. All plots and metrics referenced in this section contain 90% confidence intervals. Fig. 5 compares the four metrics: Accuracy, Failed pickups, Missed pickups, and Relative episode length for unseen scenes and the Constrained task. With regards to accuracy, explicit+implicit communication fares only moderately better than implicit communication, but the need for explicit communication is dramatic in the absence of an implicit one. But when one considers all metrics, the benefits of having both explicit and implicit communication are clearly visible. The number of failed and missed pickups is lower, while episode lengths are a little better than just using implicit communication. The differences between just explicit vs. just implicit also shrink when looking at all metrics together. However, across the board, it is clear that communicating is advantageous over not communicating. Fig. 6 shows the rewards obtained by the 4 variants of our model on seen and unseen environments for the Constrained task. While rewards on seen scenes are unsurprisingly higher, the models with communication do generalize well to unseen environments. Adding the two means of communication is more beneficial than either and far better than not having any means of communication. Interestingly   just implicit communication fares better than just explicit, on accuracy. Fig. 7 presents the accuracy and relative episode lengths metrics for the unseen scenes and Unconstrained task in contrast to the Constrained task. In these plots, for brevity we only consider the extreme cases of having full communication vs. no communication. As expected, the Unconstrained setting is easier for the agents with higher accuracy and lower episode lengths. Communication is also advantageous in the Unconstrained setting, but its benefits are lesser compared to the Constrained setting. Table 1 shows a large jump in accuracy when we provide a perfect depth map as an additional input on the Constrained task, indicating that improved perception is beneficial to task completion. We also obtained significant jumps in accuracy (from 31.8 \u00b1 3.8 to 37.2 \u00b1 4.0) when we increase the size of our vocabulary from 2 to 8. This analysis was performed in the explicit-only communication and Constrained environment setup. However, note that even with a vocabulary of 2, agents may be using the full continuous spectrum to encode more nuanced events. Grid-world abstraction. In order to assess impact of learning to communicate from pixels rather than, as in most prior work, from grid-world environments, we perform a direct translation of our task into a grid-world and compare its performance to our best model. We transform the 1.25m \u00d7 2.75m area in front of our agent into a 5 \u00d7 11 grid where each square is assigned a 16 dimensional embedding based on whether it is free space, occupied by another agent, occupied by the target object, otherwise unreachable, or unknown (in the case the grid square leaves the environment). The agents then move in AI2-THOR but perceive this partially observable grid-world. Agents in this setting acquire a large bump in accuracy on the Constrained task (Table 1), confirming our claim that photo-realistic visual environments are more challenging than grid-world like settings. Interpreting Communication. While we have seen, in Section 4, that communication can substantially benefit our task, we now investigate what these agents have learned to communicate. We focus on the communication strategies learned by agents with a vocabulary of two in the  Table 2: Estimates, and corresponding robust bootstrap standard errors, of the parameters from Section 4.\n\nConstrained setting. Fig. 8 displays one episode trajectory of the two agents with the corresponding communication. From Fig. 8(b) we generate hypotheses regarding communication strategies. Suppressing the dependence on episode and step, for i \u2208 {0, 1} let t i be the weight assigned by agent i to the 1 st element of the vocabulary in the 1 st round of communication, and similarly let r i be as t i but for the 2 nd round of communication. When the agent with the red trajectory (henceforth called agent 0 or A 0 ) begins to see the TV the weight t 0 increases and remains high until the end of the episode. This suggests that the 1 st round of communication may be used to signify closeness to or visibility of the TV. On the other hand, the pickup actions taken by the two agents are associated with the agents making r 0 and r 1 simultaneously small.\n\nTo add evidence to these hypotheses we fit logistic regression models to predict, from (functions of) t i and r i , two oracle values (e.g., whether the TV is visible) and whether or not the agents will attempt a pickup action. As the agents are largely symmetric we take the perspective of A 0 and define the models \u03c3 \u22121 P (A0 is \u2264 2m from the TV) = \u03b2 \u2264 + \u03b2 \u2264 t t 0 + \u03b2 \u2264 r r 0 , \u03c3 \u22121 P (A0 sees TV and is \u2264 1.5m from it) = \u03b2 see + \u03b2 see t t 0 + \u03b2 see r r 0 , and \u03c3 \u22121 P (A0 attempts a pickup action) = \u03b2 pick + i\u2208{0,1} (\u03b2 pick t,i t i +\u03b2 pick r,i r i )+\u03b2 pick \u2228,r max(r 0 , r 1 ) where \u03c3 \u22121 is the logit function. Details of how these models are fit can be found in the appendix.\n\nFrom Table 2, which displays the estimates of the above parameters along with their standard errors, we find strong evidence for the above intuitions. Note, for all of the esti-mates discussed above, the standard errors are very small, suggesting highly statistically significant results. The large positive coefficients associated with \u03b2 \u2264 t and \u03b2 see t suggest that, conditional on r 0 being held constant, an increase in the weight t 0 is associated with a higher probability of A 0 being near, and seeing, the TV. Note also that the estimated value of \u03b2 see r is fairly large in magnitude and negative. This is very much in line with our prior hypothesis that r 0 is made small when agent 0 wishes to signal a readiness to pickup the object. Finally, essentially all estimates of coefficients in the final model are close to 0 except for \u03b2 pick \u2228,r which is large and negative. Hence, conditional on other values being fixed, max(r 0 , r 1 ) being small is associated with a higher probability of a subsequent pickup action. Of course r 0 , r 1 \u2264 max(r 0 , r 1 ) again lending evidence to the hypothesis that the agents coordinate pickup actions by setting r 0 , r 1 to small values.\n\n\nConclusion\n\nWe study the problem of learning to collaborate in visual environments and demonstrate the benefits of learned explicit and implicit communication to aid task completion. We compare performance of collaborative tasks in photorealistic visual environments to an analogous grid-world environment, to establish that the former are more challenging. We also provide a statistical interpretation of the communication strategy learned by the agents.\n\nFuture research directions include extensions to more than two agents, more intricate real-world tasks and scaling to more environments. It would be exciting to enable natural language communication between the agents which also naturally extends to involving human-in-the-loop.\n\n\nA.1. AI2-THOR to Grid-world\n\nIn order to assess the impact of learning to communicate directly from pixels rather than, as in most prior work, from grid-world environments, we perform a direct translation of our task into a grid-world and compare its performance to our best model. For this purpose we transform AI2-THOR into a grid-world environment. Figure 10 visualizes, for a single AI2-THOR scene, this transformation. To make our comparison fair, as our pixel-based agents only obtain partial information about their environment at any given timestep, we impose the same restriction on our grid-world agents by only providing them with an egocentric 5 \u00d7 11 view of their environment (see Figure 11).\n\n\nA.2. Learning algorithm\n\nAlgorithm 1 succinctly summarizes our learning procedure as otherwise described in Section 3.3 of the main paper.\n\n\nA.3. Talk and Reply stages\n\nExplicit communication happens via two stages -talk and reply. As illustrated in Fig. 9, each stage has it's own weights (V send , W send , W 1 , W 2 ). These are clearly marked using superscripts of (T ) and (R) for the talk and reply stage, respectively.\n\n\nA.4. Implementation Details.\n\nWe use the same hyperparameters and embedding dimensionality in all of our experiments. In our A3C loss we discount rewards with a factor of \u03b3 = 0.99 and weight the entropy maximization term with a factor of \u03b2 = 0.01. We use the Adam optimizer with a learning rate of 10 \u22124 , Algorithm 1 Learning Algorithm 1: Randomly initialize shared model weights \u03b8 shared 2: Set global episode counter c \u2190 0 3: while c < maxEpisodes in parallel do 4: \u03b8 \u2190 \u03b8 shared 5: c \u2190 c + 1 6: Randomly choose environment 7: Randomize agents' positions and TV location 8: Set \u03b1 \u2190 0.9 \u00b7 max(1 \u2212 c/10000, 0) 9:\n\nSet \u03c0 \u2190 (1 \u2212 \u03b1) \u00b7 \u03c0 \u03b8 + \u03b1 \u00b7 \u03c0 * 10:\n\nRoll out trajectory of length \u2264 500 from both agents using \u03c0.\n\n\n11:\n\nL a3c \u2190 A3C loss for trajectory 12: L cross \u2190 cross entropy loss of trajectory w.r.t. \u03c0 *\n\n\n13:\n\nif no expert actions sampled in trajectory then 14: g \u2190 \u2207 \u03b8 (L a3c + L cross ) 15: else 16: g \u2190 \u2207 \u03b8 L cross 17:\n\nPerform one gradient update of \u03b8 shared using ADAM with gradients g and statistics shared across processes 18: end momentum values of 0.9 and 0.999 (for the first and second moments respectively), and share optimizer statistics across processes. Gradient steps are made in the hogwild approach, that is without explicit synchronization or locks between processes [68].\n\nEach model was trained for 100,000 episodes. Each episode is initialized in a random train (seen) scene of AI2-THOR. Rewards provided to the agents are: 1 to both agents for a successful pickup action, constant -0.01 step penalty to discourage long trajectories, -0.02 for any failed action (e.g., running into a wall) and -0.1 for a failed pickup action. Episodes run for a maximum of 500 total steps (250 steps for each agent) after which the episode is considered failed. The minimum aggregate achievable reward in an episode, obtained by successive attempting failed pickup actions by both agents is -65 while the maximum reward is 1.98 achieved by both agents immediately picking up the object as their first action and only receiving a single step penalty.\n\n\nA.5. Metrics\n\nWe now present a more detailed explanation of the metrics we use to evaluate our models.\n\n(1) Per agent reward structure:\n\n\u2022 +1 for performing a successful joint pickup,\n\n\u2022 -0.1 for a failed pickup action,\n\n\u2022 -0.02 for any other failed action (trying to move into walls, furniture, etc.), and  Figure 9: Two stages of communication and belief refinement module -talk and reply. The refined belief from the talk stage is further refined by another round of communication between agents at the reply stage. In this illustration the size of vocabulary is 2 i.e. K = 2.\n\n\u2022 -0.01 for each step to encourage short trajectories.\n\n(2) Accuracy: the percentage of episodes which led to the successful pickup action by both agents.\n\n(3) Number of unsuccessful pickups: total number of pickup actions attempted by both agents which didn't lead to the target being picked up. The three preconditions necessary for a successful joint pickup action are as follows.\n\n(i) Both agents perform the pickup action simultaneously, (ii) Both agents are closer than 1.5m to the target and the target is visible, and (iii) Both agents are a minimum distance apart from each other (0 for the Unconstrained and 8 steps = 2 meters in the manhattan distance for the Constrained setting).\n\n(4) Number of missed pickups: total number of episode steps where both agents could have picked up the object but did not. This is the number of opportunities where 3ii and 3iii were met, but the agents didn't perform simultaneous pickup actions.\n\n(5) Relative episode length: the quantity Episode length following agent policy (\u03c0) Episode length following oracle policy (\u03c0 * )\n\nAs it has access to information not available to the agents, our expert policy is also referred to as the oracle policy. As mentioned in the paper, the oracle plans a shortest path from each agent location to the target. This is achieved by leveraging the full map of the scene (i.e., free space, occupied areas, location of other agent, and the target location).\n\n\nA.6. Quantitative evaluation\n\nIn this section we provide quantitative evaluation results of variants of TBONE. We provide results on seen (train) and unseen (test) scenes. Many of the unseen scenes results are already included in the main paper, but we reproduce the full suite of graphs here, for ease of comparison.\n\nFor the Constrained task, Fig. 12 and Fig. 13 show the above metrics on seen and unseen scenes, respectively. For the Unconstrained task, Fig. 14 and Fig. 15 show the above metrics on seen and unseen scenes, respectively.\n\nOn the Constrained task in seen scenes (Fig. 12), having both modes of communication clearly produces better rewards. And having either or both modes of communication easily outperforms agents with no means of communication. While the accuracy metric is similar to having only implicit means of communication, the number of unsuccessful pickups, missed pickups, and relative episode lengths metrics show the benefit of having both modes of communication over any one of them. A similar trend is seen in unseen scenes for the same task (Fig. 13).\n\nOn the Unconstrained task, the benefits of communication are, as expected, less dramatic ( Fig. 14 and Fig. 15). Since the task is simpler and potentially can be solved without communication, agents with no means of communication are able to obtain high accuracies. But in the absence of communication, agents end up having a large number of unsuccessful pickups. This is expected. With no means of communication, agents simply go close to the TV and start attempting pickups. Only with communication can they lower this metric by coordinating with each other.\n\n\nA.7. Interpreting Communication\n\nTo fit the logistic models described in Section 4 of the main paper we randomly initialize 2,687 episodes on the 20 training scenes from which we obtain a corresponding (a) Top view of AI2-THOR scene (b) Corresponding grid-world Figure 10: An AI2-THOR scene from a top-down view along the corresponding grid-world. Note that each agent (teal triangles) only observes a small portion of the grid-world at any given time-step, see Figure 11 for details. Here each color corresponds to a different category: freespace (green), impassable terrain (red), target object (orange), and unknown (purple).\n\n(a) First-person AI2-THOR agent view (b) Agent partially observed grid-world overlayed on map view (c) Grid-world corresponding to agent view Figure 11: First person viewpoints of agents in AI2-THOR and the corresponding grid-world observations. Note that white squares are unobserved and blue squares correspond to another agent, see Figure 10 for a description of the other colors.\n\nnumber of agent trajectories. Treating each step in these trajectories as a single observation, this results in a dataset containing 143,401 samples. We fit these logistic models using the statsmodels package [75] in Python. As observations within a single episode are highly correlated, we use the bootstrap [26] to obtain robust standard errors for our estimates.\n\nAs the analysis above is done on the seen scenes, it begs the question of whether the same trends occur when agents communicate in unseen environments. To address this, we sample 1,333 agent episodes on the 10 test scenes resulting in a dataset of 201,738 samples. We fit identical logistic regression models to this dataset as in the main paper and report the resulting estimates and standard errors in Table 3. While several estimates differ, in a statistically significant way, from those on the seen scenes, all trends remain the same suggesting that agents communicate in largely the same way in unseen environments as they do in previously seen environments.      Table 3: Estimates, and corresponding robust bootstrap standard errors, of the parameters from the main paper's Section 4 when using trajectories sampled from the unseen scenes as described in Section A.7.\n\n\nA.8. Qualitative results\n\n\nA.8.1 Effect of communication\n\nWe present qualitative results of agents with three communication abilities: implicit + explicit vs. implicit only vs. no communication. We compare the effect by deploying this agents for a particular initialization of an episode i.e. the same scene, agents' start locations and target object location. We find both explicit and implicit communication help achieve the task faster as seen Fig. 16, Fig. 17 and Fig. 18 which have episode lengths of 86, 165 and 250 respectively. Another such initialization is compared in Fig. 19, Fig. 20 and Fig. 21 which have episode lengths of 17, 72 and 217 respectively.\n\n\nA.8.2 Video\n\nThe associated video includes episode visualizations for the Constrained task on Unseen scenes, and can be found here: https://youtu.be/9sQhD_Gin5M. For these episodes we ran inference on the model with both explicit and implicit communication. The six clips in the video are summarized in Fig. 22, Fig. 23, Fig. 24, Fig. 25, Fig. 26 and     \n\nFigure 4 :\n4Communication and belief refinement module for the talk stage (marked with the superscript of (T )) of explicit communication. Here our vocab. is of size K = 2.\n\nFigure 5 :\n5Unseen scenes metrics (Constrained task): (a) Failed pickups (b) Missed pickups (c) Relative ep. len (d) Accuracy.\n\nFigure 6 :\n6Reward vs. training episodes on the Constrained task. (left) Seen scenes (right) Unseen scenes. about themselves and objects in the scene. For each of the above tasks, we train 4 variants of TBONE, resulting from switching explicit and implicit communication on and off. Switching off implicit communication amounts to rendering the other agent invisible. Metrics. We consider the following metrics: (1) Reward, (2) Accuracy: % successful episodes, (3) Number of Failed pickups, (4) Number of Missed pickups: where both agents could have picked up the object but did not, (5) Relative episode length: relative to an oracle. These metrics are aggregated over 400 random initializations (Unseen scenes: 10 scenes \u00d7 40 inits, Seen scenes: 20 scenes \u00d7 20 inits).\n\nFigure 7 :\n7Constrained vs. unconstrained task (on unseen scenes): (left) Accuracy, (right) Relative episode length.\n\nFigure 8 :\n8Single episode trajectory with associated agent communication.\n\nFigure 12 :\n12Constrained task, seen scenes.\n\nFigure 13 :\n13Constrained task, unseen scenes.\n\nFigure 14 :\n14Unconstrained task, seen scenes.\n\nFigure 15 :\n15Unconstrained task, unseen scenes.\n\nFig. 27 .Figure 16 :\n2716The first four culminated in successful pickup of the target object. The last two videos highlight typical error modes. Initialization 1: With explicit and implicit communication, episode length is 86 per agent. Associated agent communication in plot below, seeFigure 8in the main paper for a legend.\n\nFigure 17 :\n17Initialization 1: With only implicit communication, episode length is 165 per agent.\n\nFigure 18 :Figure 19 :\n1819Initialization 1: With no communication, episode length is 250 per agent (unsuccessful). Initialization 2: With explicit and implicit communication, episode length is 17 per agent. Associated agent communication in plot below, see Figure 8 in the main paper for a legend.\n\nFigure 20 :\n20Initialization 2: With only implicit communication, episode length is 72 per agent.\n\nFigure 21 :Figure 22 :Figure 23 :Figure 24 :Figure 25 :Figure 26 :Figure 27 :\n21222324252627Initialization 2: With no communication, episode length is 217 per agent. Clip 1 summary, seeFigure 8in the main paper for a legend. Clip 2 summary, seeFigure 8in the main paper for a legend. Clip 3 summary, seeFigure 8in the main paper for a legend. Clip 4 summary, seeFigure 8in the main paper for a legend. Clip 5 summary, seeFigure 8in the main paper for a legend. Clip 6 summary, seeFigure 8in the main paper for a legend.\n\n\nand Jim [32], Kasai et al. [43], Bratman et al. [9], Melo et al. [58], Lazaridou et al. [53], Foerster et al. [28], Sukhbaatar et al. [79] and Mordatch and Abbeel\nA. AppendixThis appendix presents the following content:1. Visualizations of the grid-world abstraction of our task, 2. Our learning algorithm, 3. Interplay between talk and reply stages of the communication and belief refinement module, 4. Implementation details of model 5. A detailed explanation of metrics used in our paper, 6. Quantitative evaluation of our models but now evaluated on seen scenes, 7. Statistical analysis of agent communication strategies but now demonstrated on unseen scenes, 8. Qualitative results of agents with different communication abilities deployed on unseen scenes. This includes clip summaries with agent communication signals for video https://youtu.be/9sQhD_ Gin5M.\nExploratory gradient boosting for reinforcement learning in complex domains. D Abel, A Agarwal, F Diaz, A Krishnamurthy, R E Schapire, arXiv:1603.04119arXiv preprintD. Abel, A. Agarwal, F. Diaz, A. Krishnamurthy, and R. E. Schapire. Exploratory gradient boosting for rein- forcement learning in complex domains. arXiv preprint arXiv:1603.04119, 2016. 3\n\nvan den Hengel. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N S\u00fcnderhauf, I Reid, S Gould, A , Proc. CVPR. CVPRP. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S\u00fcnderhauf, I. Reid, S. Gould, and A. van den Hen- gel. Vision-and-language navigation: Interpreting visually- grounded navigation instructions in real environments. In Proc. CVPR, 2018. 3\n\nActive visual object search in unknown environments using uncertain semantics. A Aydemir, A Pronobis, M Gbelbecker, P Jensfelt, IEEE Trans. on Robotics. 2A. Aydemir, A. Pronobis, M. Gbelbecker, and P. Jensfelt. Active visual object search in unknown environments using uncertain semantics. In IEEE Trans. on Robotics, 2013. 2\n\nThe arcade learning environment: An evaluation platform for general agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, J. of Artificial Intelligence Research. 3M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. J. of Artificial Intelligence Research, 2013. 3\n\nPlaying doom with slamaugmented deep reinforcement learning. S Bhatti, A Desmaison, O Miksik, N Nardelli, N Siddharth, P H S Torr, arXiv:1612.00380arXiv preprintS. Bhatti, A. Desmaison, O. Miksik, N. Nardelli, N. Sid- dharth, and P. H. S. Torr. Playing doom with slam- augmented deep reinforcement learning. arXiv preprint arXiv:1612.00380, 2016. 3\n\nReal-time obstacle avoidance for fast mobile robots. J Borenstein, Y Koren, IEEE Trans. on Systems, Man and Cybernetics. 2J. Borenstein and Y. Koren. Real-time obstacle avoidance for fast mobile robots. IEEE Trans. on Systems, Man and Cybernetics, 1989. 2\n\nThe vector field histogramfast obstacle avoidance for mobile robots. J Borenstein, Y Koren, IEEE Trans. on Robotics and Automation. 2J. Borenstein and Y. Koren. The vector field histogram - fast obstacle avoidance for mobile robots. IEEE Trans. on Robotics and Automation, 1991. 2\n\nDeepnav: Learning to navigate large cities. S Brahmbhatt, J Hays, Proc. CVPR. CVPRS. Brahmbhatt and J. Hays. Deepnav: Learning to navigate large cities. In Proc. CVPR, 2017. 3\n\nA new approach to exploring language emergence as boundedly optimal control in the face of environmental and cognitive constraints. J Bratman, M Shvartsman, R L Lewis, S Singh, Proc. Int.'l Conv. on Cognitive Modeling. Int.'l Conv. on Cognitive Modeling13J. Bratman, M. Shvartsman, R. L. Lewis, and S. Singh. A new approach to exploring language emergence as bound- edly optimal control in the face of environmental and cogni- tive constraints. In Proc. Int.'l Conv. on Cognitive Modeling, 2010. 1, 3\n\nHoME: a Household Multimodal Environment. S Brodeur, E Perez, A Anand, F Golemo, L Celotti, F Strub, J Rouat, H Larochelle, A Courville, S. Brodeur, E. Perez, A. Anand, F. Golemo, L. Celotti, F. Strub, J. Rouat, H. Larochelle, and A. Courville. HoME: a Household Multimodal Environment. In https://arxiv.org/abs/1711.11017, 2017. 3\n\nA comprehensive survey of multiagent reinforcement learning. L Busoniu, R Babuska, B D Schutter, IEEE Trans. on Systems, Man and Cybernetics. 3L. Busoniu, R. Babuska, and B. D. Schutter. A comprehen- sive survey of multiagent reinforcement learning. In IEEE Trans. on Systems, Man and Cybernetics, 2008. 3\n\nPast, present, and future of simultaneous localization and mapping: Toward the robust-perception age. C Cadena, L Carlone, H Carrillo, Y Latif, D Scaramuzza, J Neira, I Reid, J J Leonard, IEEE Trans. on Robotics. 2C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard. Past, present, and fu- ture of simultaneous localization and mapping: Toward the robust-perception age. IEEE Trans. on Robotics, 2016. 2\n\nThe complexity of robot motion planning. J Canny, MIT PressJ. Canny. The complexity of robot motion planning. MIT Press, 1988. 2\n\nMatterport3D: Learning from RGB-D data in indoor environments. A Chang, A Dai, T Funkhouser, M Halber, M Niessner, M Savva, S Song, A Zeng, Y Zhang, 2017. 3International Conference on 3D Vision. A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3D: Learning from RGB-D data in indoor environments. In In- ternational Conference on 3D Vision (3DV), 2017. 3\n\nGated-attention architectures for task-oriented language grounding. D S Chaplot, K M Sathyendra, R K Pasumarthi, D , R R Salakhutdinov, CoRR, abs/1706.07230D. S. Chaplot, K. M. Sathyendra, R. K. Pasumarthi, D. Ra- jagopal, and R. R. Salakhutdinov. Gated-attention archi- tectures for task-oriented language grounding. In CoRR, abs/1706.07230, 2017. 3\n\nDeepdriving: Learning affordance for direct perception in autonomous driving. C Chen, A Seff, A Kornhauser, J Xiao, ICCV. C. Chen, A. Seff, A. Kornhauser, and J. Xiao. Deepdriving: Learning affordance for direct perception in autonomous driving. In ICCV, 2015. 3\n\nLearning transferable policies for monocular reactive mav control. S Daftry, J A Bagnell, M Hebert, Proc. ISER. ISERS. Daftry, J. A. Bagnell, and M. Hebert. Learning transfer- able policies for monocular reactive mav control. In Proc. ISER, 2016. 3\n\nHuman attention in visual question answering: Do humans and deep networks look at the same regions? In EMNLP. A Das, H Agrawal, C L Zitnick, D Parikh, D Batra, A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Batra. Human attention in visual question answering: Do humans and deep networks look at the same regions? In EMNLP, 2016. 5\n\nEmbodied Question Answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Proc. CVPR. CVPR35A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Ba- tra. Embodied Question Answering. In Proc. CVPR, 2018. 3, 5\n\nA Das, T Gervet, J Romoff, D Batra, D Parikh, M Rabbat, J Pineau, arXiv:1810.11187Tarmac: Targeted multi-agent communication. arXiv preprintA. Das, T. Gervet, J. Romoff, D. Batra, D. Parikh, M. Rabbat, and J. Pineau. Tarmac: Targeted multi-agent communication. arXiv preprint arXiv:1810.11187, 2018. 3\n\nNeural Modular Control for Embodied Question Answering. A Das, G Gkioxari, S Lee, D Parikh, D Batra, Proc. ECCV. ECCVA. Das, G. Gkioxari, S. Lee, D. Parikh, and D. Batra. Neu- ral Modular Control for Embodied Question Answering. In Proc. ECCV, 2018. 3\n\nLearning cooperative visual dialog agents with deep reinforcement learning. A Das, S Kottur, J M Moura, S Lee, D Batra, Proc. ICCV. ICCVA. Das, S. Kottur, J. M. Moura, S. Lee, and D. Batra. Learn- ing cooperative visual dialog agents with deep reinforcement learning. In Proc. ICCV, 2017. 5\n\nReal time simultaneous localisation and mapping with a single camera. A J Davison, ICCV. A. J. Davison. Real time simultaneous localisation and map- ping with a single camera. In ICCV, 2003. 2\n\nStructure from Motion without Correspondence. F Dellaert, S Seitz, C Thorpe, S Thrun, Proc. CVPR. CVPRF. Dellaert, S. Seitz, C. Thorpe, and S. Thrun. Structure from Motion without Correspondence. In Proc. CVPR, 2000. 2\n\nY Duan, J Schulman, X Chen, P L Bartlett, I Sutskever, P Abbeel, arXiv:1611.02779Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprintY. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. 3\n\nBootstrap methods: Another look at the jackknife. B Efron, Ann. Statist. 71B. Efron. Bootstrap methods: Another look at the jackknife. Ann. Statist., 7(1):1-26, 01 1979. 14\n\nUsing occupancy grids for mobile robot perception and navigation. A Elfes, Computer. 2A. Elfes. Using occupancy grids for mobile robot perception and navigation. Computer, 1989. 2\n\nLearning to Communicate with Deep Multi-Agent Reinforcement Learning. J N Foerster, Y M Assael, N Freitas, S Whiteson, Proc. NIPS. NIPS13J. N. Foerster, Y. M. Assael, N. de Freitas, and S. White- son. Learning to Communicate with Deep Multi-Agent Re- inforcement Learning. In Proc. NIPS, 2016. 1, 3\n\nCoutnerfactual Multi-Agent Policy Gradients. J N Foerster, G Farquhar, T Afouras, N Nardelli, S Whiteson, Proc. AAAI. AAAIJ. N. Foerster, G. Farquhar, T. Afouras, N. NArdelli, and S. Whiteson. Coutnerfactual Multi-Agent Policy Gradients. In Proc. AAAI, 2018. 3\n\nStabilising experience replay for deep multi-agent reinforcement learning. J N Foerster, N Nardelli, G Farquhar, P H S Torr, P Kohli, S Whiteson, CoRR, abs/1702.08887J. N. Foerster, N. Nardelli, G. Farquhar, P. H. S. Torr, P. Kohli, and S. Whiteson. Stabilising experience replay for deep multi-agent reinforcement learning. In CoRR, abs/1702.08887, 2017. 3\n\nVision-based autonomous mapping and exploration using a quadrotor mav. F Fraundorfer, L Heng, D Honegger, G H Lee, L Meier, P Tanskanen, M Pollefeys, Proc. IROS. IROSF. Fraundorfer, L. Heng, D. Honegger, G. H. Lee, L. Meier, P. Tanskanen, and M. Pollefeys. Vision-based autonomous mapping and exploration using a quadrotor mav. In Proc. IROS, 2012. 2\n\nLearning communication for multi-agent systems. C L Giles, K C Jim, Proc. Innovative Concepts for Agent-Based Systems. Innovative Concepts for Agent-Based Systems13C. L. Giles and K. C. Jim. Learning communication for multi-agent systems. In Proc. Innovative Concepts for Agent-Based Systems, 2002. 1, 3\n\nA machine learning approach to visual perception of forest trails for mobile robots. A Giusti, J Guzzi, S D C Cire, F L He, J P Rodr\u00edguez, F Fontana, M Faessler, C Forster, J Schmidhuber, G D Caro, A. Giusti, J. Guzzi, s. D. C. Cire F. L. He, J. P. Rodr\u00edguez, F. Fontana, M. Faessler, C. Forster, J. Schmidhuber, G. D. CAro, et al. A machine learning approach to visual percep- tion of forest trails for mobile robots. 2016. 3\n\nIQA: Visual Question Answering in Interactive Environments. D Gordon, A Kembhavi, M Rastegari, J Redmon, D Fox, A Farhadi, Proc. CVPR. CVPRD. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and A. Farhadi. IQA: Visual Question Answering in Interac- tive Environments. In Proc. CVPR, 2018. 3\n\nCooperative Multi-Agent Control Using Deep Reinforcement Learning. J K Gupta, M Egorov, M Kochenderfer, Proc. AAMAS. AAMAS14J. K. Gupta, M. Egorov, and M. Kochenderfer. Cooperative Multi-Agent Control Using Deep Reinforcement Learning. In Proc. AAMAS, 2017. 1, 3, 4\n\nCognitive Mapping and Planning for Visual Navigation. S Gupta, J Davidson, S Levine, R Sukthankar, J Malik, Proc. CVPR. CVPR25S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Ma- lik. Cognitive Mapping and Planning for Visual Navigation. In Proc. CVPR, 2017. 2, 3, 5\n\nUnifying map and landmark based representations for visual navigation. S Gupta, D Fouhey, S Levine, J Malik, arXiv:1712.08125arXiv preprintS. Gupta, D. Fouhey, S. Levine, and J. Malik. Unifying map and landmark based representations for visual naviga- tion. arXiv preprint arXiv:1712.08125, 2017. 3\n\nReactive navigation in outdoor environments using potential fields. H Haddad, M Khatib, S Lacroix, R Chatila, Proc. ICRA. ICRAH. Haddad, M. Khatib, S. Lacroix, and R. Chatila. Reactive navigation in outdoor environments using potential fields. In Proc. ICRA, 1998. 2\n\nUnderstanding grounded language learning agents. F Hill, K M Hermann, P Blunsom, S Clark, abs/1710.09867CoRR. F. Hill, K. M. Hermann, P. Blunsom, and S. Clark. Un- derstanding grounded language learning agents. In CoRR, abs/1710.09867, 2017. 3\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 5S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997. 5\n\nThe malmo platform for artificial intelligence experimentation. M Johnson, K Hofmann, T Hutton, D Bignell, Intl. Joint Conference on AI. M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The malmo platform for artificial intelligence experimentation. In Intl. Joint Conference on AI, 2016. 3\n\nPolicy learning using adaptive trajectory optimization. G Kahn, T Zhang, S Levine, P Abbeel, Plato, Proc. ICRA. ICRAG. Kahn, T. Zhang, S. Levine, and P. Abbeel. Plato: Pol- icy learning using adaptive trajectory optimization. In Proc. ICRA, 2017. 3\n\nLearning of communication codes in multi-agent reinforcement learning problem. T Kasai, H Tenmoto, A Kamiya, Proc. IEEE Soft Computing in Industrial Applications. IEEE Soft Computing in Industrial Applications13T. Kasai, H. Tenmoto, and A. Kamiya. Learning of commu- nication codes in multi-agent reinforcement learning prob- lem. In Proc. IEEE Soft Computing in Industrial Applica- tions, 2008. 1, 3\n\nOvermars. Probabilistic roadmaps for path planning in highdimensional configuration spaces. L E Kavraki, P Svestka, J.-C Latombe, M , RA. 2L. E. Kavraki, P. Svestka, J.-C. Latombe, and M. H. Over- mars. Probabilistic roadmaps for path planning in high- dimensional configuration spaces. RA, 1996. 2\n\nVizdoom: A doom-based ai research platform for visual reinforce-ment learning. M Kempka, M Wydmuch, G Runc, J Toczek, W Jakowski, Proc. IEEE Conf. on Computational Intelligence and Games. IEEE Conf. on Computational Intelligence and GamesM. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. Jakowski. Vizdoom: A doom-based ai research platform for visual reinforce-ment learning. In Proc. IEEE Conf. on Computational Intelligence and Games, 2016. 3\n\nAutonomous visual navigation of a mobile robot using a human guided experience. K Kidono, J Miura, Y Shirai, Robotics and Autonomous Systems. 2K. Kidono, J. Miura, and Y. Shirai. Autonomous visual nav- igation of a mobile robot using a human guided experience. Robotics and Autonomous Systems, 2002. 2\n\nSymbolic navigation with a generic map. D Kim, R Nevatia, Autonomous Robots. 2D. Kim and R. Nevatia. Symbolic navigation with a generic map. Autonomous Robots, 1999. 2\n\nAI2-THOR: An Interactive 3D Environment for Visual AI. E Kolve, R Mottaghi, D Gordon, Y Zhu, A Gupta, A Farhadi, 23E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. In https://arxiv.org/abs/1712.05474, 2017. 2, 3\n\nView-based maps. K Konolige, J Bowman, J Chen, P Mihelich, M Calonder, V Lepetit, P Fua, Intl. J. of Robotics Research. 2K. Konolige, J. Bowman, J. Chen, P. Mihelich, M. Calonder, V. Lepetit, and P. Fua. View-based maps. Intl. J. of Robotics Research, 2010. 2\n\nA robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. Robotics and autonomous systems. B Kuipers, Y T Byun, B. Kuipers and Y. T. Byun. A robot exploration and mapping strategy based on a semantic hierarchy of spatial representa- tions. Robotics and autonomous systems, 1991. 2\n\nAn algorithm for distributed reinforcement learning in cooperative multi-agent systems. M Lauer, M Riedmiller, Proc. ICML. ICMLM. Lauer and M. Riedmiller. An algorithm for distributed reinforcement learning in cooperative multi-agent systems. In Proc. ICML, 2000. 3\n\nRapidly-exploring random trees: Progress and prospects. S M Lavalle, J J Kuffner, Algorithmic and Computational Robotics: New Directions. S. M. Lavalle and J. J. Kuffner. Rapidly-exploring random trees: Progress and prospects. Algorithmic and Computa- tional Robotics: New Directions, 2000. 2\n\nMulti-agent cooperation and the emergence of (natural) language. A Lazaridou, A Peysakhovich, M Baroni, arXivpreprintarXiv:1612.0718213A. Lazaridou, A. Peysakhovich, and M. Baroni. Multi-agent cooperation and the emergence of (natural) language. In arXiv preprint arXiv:1612.07182, 2016. 1, 3\n\nVisual sonar: Fast obstacle avoidance using monocular vision. S Lenser, M Veloso, Proc. IROS. IROSS. Lenser and M. Veloso. Visual sonar: Fast obstacle avoid- ance using monocular vision. In Proc. IROS, 2003. 2\n\nLearning physical intuition of block towers by example. A Lerer, S Gross, R Fergus, Proc. ICML. ICMLA. Lerer, S. Gross, and R. Fergus. Learning physical intu- ition of block towers by example. In Proc. ICML, 2016. 3\n\nMulti-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. R Lowe, Y Wu, A Tamar, J Harb, P Abbeel, I Mordatch, Proc. NIPS. NIPS13R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mor- datch. Multi-Agent Actor-Critic for Mixed Cooperative- Competitive Environments. In Proc. NIPS, 2017. 1, 3\n\nHysteretic qlearning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams. L Matignon, G J Laurent, N L Fort-Piat, Proc. IROS. IROSL. Matignon, G. J. Laurent, and N. L. Fort-Piat. Hysteretic q- learning: an algorithm for decentralized reinforcement learn- ing in cooperative multi-agent teams. In Proc. IROS, 2007. 3\n\nQueryPOMDP: POMDP-based communication in multiagent systems. F S Melo, M Spaan, S J Witwicki, Proc. Multi-Agent Systems. Multi-Agent Systems13F. S. Melo, M. Spaan, and S. J. Witwicki. QueryPOMDP: POMDP-based communication in multiagent systems. In Proc. Multi-Agent Systems, 2011. 1, 3\n\nLearning to navigate in complex environments. P Mirowski, R Pascanu, F Viola, H Soyer, A Ballard, A Banino, M Denil, R Goroshin, L Sifre, K Kavukcuoglu, Proc. ICLR. ICLRP. Mirowski, R. Pascanu, F. Viola, H. Soyer, A. Ballard, A. Banino, M. Denil, R. Goroshin, L. Sifre, K. Kavukcuoglu, et al. Learning to navigate in complex environments. In Proc. ICLR, 2017. 3\n\nAsynchronous Methods for Deep Reinforcement Learning. V Mnih, A P Badia, M Mirza, A Graves, T P Lillicrap, T Harley, D Silver, K Kavukcuoglu, V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lilli- crap, T. Harley, D. Silver, and K. Kavukcuoglu. Asyn- chronous Methods for Deep Reinforcement Learning. In https://arxiv.org/abs/1602.01783, 2016. 5\n\nEmergence of Grounded Compositional Language in Multi-Agent Populations. I Mordatch, P , Proc. AAAI. AAAI14I. Mordatch and P. Abbeel. Emergence of Grounded Com- positional Language in Multi-Agent Populations. In Proc. AAAI, 2018. 1, 3, 4\n\nControl of memory, active perception, and action in minecraft. J Oh, V Chockalingam, S Singh, H Lee, Proc. ICML. ICMLJ. Oh, V. Chockalingam, S. Singh, and H. Lee. Control of memory, active perception, and action in minecraft. In Proc. ICML, 2016. 3\n\nDeep decentralized multi-task multi-agent reinforcement learning under partial observability. S Omidshafiei, J Pazis, C Amato, J P How, J Vian, abs/1703.06182CoRR. S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task multi-agent rein- forcement learning under partial observability. In CoRR, abs/1703.06182, 2017. 3\n\nOn-line map building and navigation for autonomous mobile robots. G U G Oriolo, M Vendittelli, Proc. ICRA. ICRAG. U. G. Oriolo and M. Vendittelli. On-line map building and navigation for autonomous mobile robots. In Proc. ICRA, 1995. 2\n\nCooperative multi-agent learning: The state of the art. Autonomous Agents and Multi-Agent Systems. L Panait, S Luke, Proc. AAMAS. AAMASL. Panait and S. Luke. Cooperative multi-agent learning: The state of the art. Autonomous Agents and Multi-Agent Systems. In Proc. AAMAS, 2005. 3\n\nFast, robust, continuous monocular egomotion computation. S Phillips, A Jaegle, K Daniilidis, Proc. ICRA. ICRAS. Phillips, A. Jaegle, and K. Daniilidis. Fast, robust, con- tinuous monocular egomotion computation. In Proc. ICRA, 2016. 2\n\nOn the representation and estimation of spatial uncertainty. R C R C Smith, P Cheeseman, Intl. J. Robotics Research. 2R. C. R. C. Smith and P. Cheeseman. On the representa- tion and estimation of spatial uncertainty. Intl. J. Robotics Research, 1986. 2\n\nHogwild: A lockfree approach to parallelizing stochastic gradient descent. B Recht, C Re, S Wright, F Niu, Advances in neural information processing systems. 12B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock- free approach to parallelizing stochastic gradient descent. In Advances in neural information processing systems, pages 693-701, 2011. 12\n\nRobot motion control from a visual memory. A Remazeilles, F Chaumette, P Gros, Proc. ICRA. ICRAA. Remazeilles, F. Chaumette, and P. Gros. Robot motion control from a visual memory. In Proc. ICRA, 2004. 2\n\nA reduction of imitation learning and structured prediction to no-regret online learning. S Ross, G Gordon, D Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statistics25S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learn- ing. In Proceedings of the fourteenth international confer- ence on artificial intelligence and statistics, pages 627-635, 2011. 2, 5\n\nOutdoor autonomous navigation using monocular vision. E Royer, J Bom, M Dhome, B Thuillot, M Lhuillier, F Marmoiton, Proc. IROS. IROSE. Royer, J. Bom, M. Dhome, B. Thuillot, M. Lhuillier, and F. Marmoiton. Outdoor autonomous navigation using monocular vision. In Proc. IROS, 2005. 2\n\nVision-based 3-d trajectory tracking for unknown environments. P Saeedi, P D Lawrence, D G Lowe, IEEE Trans. on Robotics. 2P. Saeedi, P. D. Lawrence, and D. G. Lowe. Vision-based 3-d trajectory tracking for unknown environments. IEEE Trans. on Robotics, 2006. 2\n\nMINOS: Multimodal indoor simulator for navigation in complex environments. M Savva, A X Chang, A Dosovitskiy, T Funkhouser, V Koltun, M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun. MINOS: Multimodal indoor simulator for navi- gation in complex environments. 2017. 3\n\nStructure-from-motion revisited. J L Schnberger, J M Frahm, Proc. CVPR. CVPRJ. L. Schnberger and J. M. Frahm. Structure-from-motion revisited. In Proc. CVPR, 2016. 2\n\nStatsmodels: Econometric and statistical modeling with python. S Seabold, J Perktold, 9th Python in Science Conference. 14S. Seabold and J. Perktold. Statsmodels: Econometric and statistical modeling with python. In 9th Python in Science Conference, 2010. 14\n\nAutonomous vision-based exploration and mapping using hybrid maps and rao-blackwellised particle filters. R Sim, J J Little, Proc. IROS. IROSR. Sim and J. J. Little. Autonomous vision-based exploration and mapping using hybrid maps and rao-blackwellised par- ticle filters. In Proc. IROS, 2006. 2\n\nEstimating uncertain spatial relationships in robotics. R C Smith, M Self, P Cheeseman, Proc. UAI. UAIR. C. Smith, M. Self, and P. Cheeseman. Estimating uncer- tain spatial relationships in robotics. In Proc. UAI, 1986. 2\n\nSemantic scene completion from a single depth image. S Song, F Yu, A Zeng, A X Chang, M Savva, T Funkhouser, Proc. CVPR. CVPRS. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic scene completion from a single depth image. In Proc. CVPR, 2017. 3\n\nLearning multiagent communication with backpropagation. S Sukhbaatar, A Szlam, R Fergus, Proc. NIPS. NIPS13S. Sukhbaatar, A. Szlam, and R. Fergus. Learning multiagent communication with backpropagation. In Proc. NIPS, 2016. 1, 3\n\nMazebase: A sandbox for learning from games. S Sukhbaatar, A Szlam, G Synnaeve, S Chintala, R Fergus, S. Sukhbaatar, A. Szlam, G. Synnaeve, S. Chintala, and R. Fergus. Mazebase: A sandbox for learning from games. 2015. 3\n\nMultiagent cooperation and competition with deep reinforcement learning. A Tampuu, T Matiisen, D Kodelja, I Kuzovkin, K Korjus, J Aru, J Aru, R Vicente, PloS. A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Kor- jus, J. Aru, J. Aru, and R. Vicente. Multiagent cooperation and competition with deep reinforcement learning. In PloS, 2017. 3\n\nMulti-Agent Reinforcement Learning: Independent vs. Cooperative Agents. M Tan, Proc. ICML. ICMLM. Tan. Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents. In Proc. ICML, 1993. 3\n\nExtending q-learning to general adaptive multiagent systems. G Tesauro, Proc. NIPS. NIPSG. Tesauro. Extending q-learning to general adaptive multi- agent systems. In Proc. NIPS, 2004. 3\n\nShape and motion from image streams under orthography: a factorization method. C Tomasi, T Kanade, IJCV. 2C. Tomasi and T. Kanade. Shape and motion from image streams under orthography: a factorization method. IJCV, 1992. 2\n\n3-d object map building using dense object models with sift-based recognition features. M Tomono, Proc. IROS. IROSM. Tomono. 3-d object map building using dense object models with sift-based recognition features. In Proc. IROS, 2006. 2\n\nLearning a world model and planning with a self-organizing, dynamic neural system. M Toussaint, Proc. NIPS. NIPSM. Toussaint. Learning a world model and planning with a self-organizing, dynamic neural system. In Proc. NIPS, 2003. 3\n\nA guide to vision-based map building. IEEE Robotics and Automation Magazine. D Wooden, D. Wooden. A guide to vision-based map building. IEEE Robotics and Automation Magazine, 2006. 2\n\nBuilding Generalizable Agents with a Realistic and Rich 3D Environment. Y Wu, Y Wu, G Gkioxari, Y Tian, Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building General- izable Agents with a Realistic and Rich 3D Environment. In https://arxiv.org/abs/1801.02209, 2018. 3\n\nTorcs, the open racing car simulator. B Wymann, E Espi\u00e9, C Guionneau, C Dimitrakakis, R Coulom, A Sumner, B. Wymann, E. Espi\u00e9, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner. Torcs, the open racing car sim- ulator, 2013. 3\n\nDeep reinforcement learning with successor features for navigation across similar environments. J Zhang, J T Springenberg, J Boedecker, W Burgard, arXiv:1612.05533arXiv preprintJ. Zhang, J. T. Springenberg, J. Boedecker, and W. Bur- gard. Deep reinforcement learning with successor features for navigation across similar environments. arXiv preprint arXiv:1612.05533, 2016. 3\n\nVisual Semantic Planning using Deep Successor Representations. Y Zhu, D Gordon, E Kolve, D Fox, L Fei-Fei, A Gupta, R Mottaghi, A Farhadi, Y. Zhu, D. Gordon, E. Kolve, D. Fox, L. Fei-Fei, A. Gupta, R. Mottaghi, and A. Farhadi. Visual Seman- tic Planning using Deep Successor Representations. In https://arxiv.org/abs/1705.08080, 2017. 3\n\nTarget-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, Proc. ICRA. ICRAY. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei- Fei, and A. Farhadi. Target-driven Visual Navigation in In- door Scenes using Deep Reinforcement Learning. In Proc. ICRA, 2017. 2\n", "annotations": {"author": "[{\"end\":76,\"start\":58},{\"end\":119,\"start\":77},{\"end\":162,\"start\":120},{\"end\":223,\"start\":163},{\"end\":249,\"start\":224},{\"end\":330,\"start\":250},{\"end\":356,\"start\":331},{\"end\":407,\"start\":357}]", "publisher": null, "author_last_name": "[{\"end\":68,\"start\":64},{\"end\":87,\"start\":82},{\"end\":130,\"start\":125},{\"end\":181,\"start\":172},{\"end\":241,\"start\":233},{\"end\":261,\"start\":254},{\"end\":348,\"start\":341},{\"end\":375,\"start\":367}]", "author_first_name": "[{\"end\":63,\"start\":58},{\"end\":81,\"start\":77},{\"end\":124,\"start\":120},{\"end\":171,\"start\":163},{\"end\":232,\"start\":224},{\"end\":253,\"start\":250},{\"end\":340,\"start\":331},{\"end\":366,\"start\":357}]", "author_affiliation": "[{\"end\":75,\"start\":70},{\"end\":118,\"start\":89},{\"end\":161,\"start\":132},{\"end\":212,\"start\":183},{\"end\":222,\"start\":214},{\"end\":248,\"start\":243},{\"end\":292,\"start\":263},{\"end\":319,\"start\":294},{\"end\":329,\"start\":321},{\"end\":355,\"start\":350},{\"end\":406,\"start\":377}]", "title": "[{\"end\":55,\"start\":1},{\"end\":462,\"start\":408}]", "venue": null, "abstract": "[{\"end\":1946,\"start\":464}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2157,\"start\":2153},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2160,\"start\":2157},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2162,\"start\":2160},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":2165,\"start\":2162},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2190,\"start\":2186},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2193,\"start\":2190},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":2196,\"start\":2193},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2199,\"start\":2196},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2202,\"start\":2199},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2205,\"start\":2202},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3510,\"start\":3507},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4503,\"start\":4499},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4911,\"start\":4908},{\"end\":4998,\"start\":4997},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5095,\"start\":5092},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5217,\"start\":5214},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":5806,\"start\":5802},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7853,\"start\":7849},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7855,\"start\":7853},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7857,\"start\":7855},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":7860,\"start\":7857},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":7924,\"start\":7920},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":7927,\"start\":7924},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7930,\"start\":7927},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":7933,\"start\":7930},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7936,\"start\":7933},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":7939,\"start\":7936},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":8014,\"start\":8010},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":8017,\"start\":8014},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8020,\"start\":8017},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8023,\"start\":8020},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":8026,\"start\":8023},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":8029,\"start\":8026},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8129,\"start\":8125},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8132,\"start\":8129},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":8135,\"start\":8132},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8257,\"start\":8253},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8260,\"start\":8257},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8263,\"start\":8260},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8266,\"start\":8263},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8268,\"start\":8266},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8291,\"start\":8287},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8294,\"start\":8291},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8297,\"start\":8294},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":8300,\"start\":8297},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":8303,\"start\":8300},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":8306,\"start\":8303},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8309,\"start\":8306},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":8801,\"start\":8797},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8803,\"start\":8801},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8806,\"start\":8803},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8809,\"start\":8806},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8812,\"start\":8809},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8815,\"start\":8812},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":8818,\"start\":8815},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8821,\"start\":8818},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8823,\"start\":8821},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8825,\"start\":8823},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":8828,\"start\":8825},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8831,\"start\":8828},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8834,\"start\":8831},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":8837,\"start\":8834},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8840,\"start\":8837},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9598,\"start\":9594},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9601,\"start\":9598},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9624,\"start\":9620},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9708,\"start\":9704},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9729,\"start\":9726},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9750,\"start\":9746},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":10475,\"start\":10471},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":10478,\"start\":10475},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":10481,\"start\":10478},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10484,\"start\":10481},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10608,\"start\":10604},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":10611,\"start\":10608},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10614,\"start\":10611},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10617,\"start\":10614},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":10620,\"start\":10617},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10623,\"start\":10620},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10626,\"start\":10623},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10629,\"start\":10626},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10632,\"start\":10629},{\"end\":10728,\"start\":10723},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10733,\"start\":10729},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":10923,\"start\":10919},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11008,\"start\":11004},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":11061,\"start\":11057},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":11119,\"start\":11115},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":11149,\"start\":11145},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11260,\"start\":11256},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11884,\"start\":11880},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":11898,\"start\":11894},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11909,\"start\":11905},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":11921,\"start\":11917},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11944,\"start\":11940},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":11959,\"start\":11955},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12159,\"start\":12156},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12173,\"start\":12169},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12192,\"start\":12188},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12204,\"start\":12200},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":12216,\"start\":12212},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":12234,\"start\":12230},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14092,\"start\":14088},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":14095,\"start\":14092},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":19779,\"start\":19775},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21939,\"start\":21935},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21942,\"start\":21939},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21945,\"start\":21942},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21948,\"start\":21945},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":22115,\"start\":22111},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":22560,\"start\":22556},{\"end\":37262,\"start\":37260},{\"end\":37278,\"start\":37276},{\"end\":37291,\"start\":37289},{\"end\":37322,\"start\":37320},{\"end\":37369,\"start\":37367},{\"end\":37549,\"start\":37546},{\"end\":37662,\"start\":37659},{\"end\":37702,\"start\":37699},{\"end\":37834,\"start\":37831},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":38091,\"start\":38087},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":43759,\"start\":43755},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":43859,\"start\":43855}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":45989,\"start\":45816},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46117,\"start\":45990},{\"attributes\":{\"id\":\"fig_4\"},\"end\":46889,\"start\":46118},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47007,\"start\":46890},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47083,\"start\":47008},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47129,\"start\":47084},{\"attributes\":{\"id\":\"fig_9\"},\"end\":47177,\"start\":47130},{\"attributes\":{\"id\":\"fig_10\"},\"end\":47225,\"start\":47178},{\"attributes\":{\"id\":\"fig_11\"},\"end\":47275,\"start\":47226},{\"attributes\":{\"id\":\"fig_12\"},\"end\":47602,\"start\":47276},{\"attributes\":{\"id\":\"fig_13\"},\"end\":47702,\"start\":47603},{\"attributes\":{\"id\":\"fig_14\"},\"end\":48002,\"start\":47703},{\"attributes\":{\"id\":\"fig_15\"},\"end\":48101,\"start\":48003},{\"attributes\":{\"id\":\"fig_16\"},\"end\":48622,\"start\":48102},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":48787,\"start\":48623}]", "paragraph": "[{\"end\":2325,\"start\":1962},{\"end\":3357,\"start\":2327},{\"end\":3788,\"start\":3359},{\"end\":4437,\"start\":3790},{\"end\":4692,\"start\":4439},{\"end\":5291,\"start\":4694},{\"end\":5947,\"start\":5293},{\"end\":6228,\"start\":5949},{\"end\":6571,\"start\":6339},{\"end\":6644,\"start\":6573},{\"end\":7447,\"start\":6646},{\"end\":8451,\"start\":7464},{\"end\":8597,\"start\":8511},{\"end\":10633,\"start\":8599},{\"end\":11406,\"start\":10635},{\"end\":12458,\"start\":11408},{\"end\":13442,\"start\":12492},{\"end\":13869,\"start\":13476},{\"end\":14777,\"start\":13871},{\"end\":16712,\"start\":14779},{\"end\":17584,\"start\":16714},{\"end\":17933,\"start\":17586},{\"end\":18845,\"start\":17935},{\"end\":21007,\"start\":18870},{\"end\":21195,\"start\":21137},{\"end\":21703,\"start\":21197},{\"end\":23265,\"start\":21917},{\"end\":24348,\"start\":23267},{\"end\":25258,\"start\":24350},{\"end\":25969,\"start\":25260},{\"end\":32188,\"start\":25985},{\"end\":33045,\"start\":32190},{\"end\":33728,\"start\":33047},{\"end\":34917,\"start\":33730},{\"end\":35375,\"start\":34932},{\"end\":35655,\"start\":35377},{\"end\":36363,\"start\":35687},{\"end\":36504,\"start\":36391},{\"end\":36791,\"start\":36535},{\"end\":37406,\"start\":36824},{\"end\":37443,\"start\":37408},{\"end\":37506,\"start\":37445},{\"end\":37603,\"start\":37514},{\"end\":37722,\"start\":37611},{\"end\":38092,\"start\":37724},{\"end\":38856,\"start\":38094},{\"end\":38961,\"start\":38873},{\"end\":38994,\"start\":38963},{\"end\":39042,\"start\":38996},{\"end\":39078,\"start\":39044},{\"end\":39438,\"start\":39080},{\"end\":39494,\"start\":39440},{\"end\":39594,\"start\":39496},{\"end\":39823,\"start\":39596},{\"end\":40132,\"start\":39825},{\"end\":40380,\"start\":40134},{\"end\":40511,\"start\":40382},{\"end\":40876,\"start\":40513},{\"end\":41196,\"start\":40909},{\"end\":41419,\"start\":41198},{\"end\":41966,\"start\":41421},{\"end\":42528,\"start\":41968},{\"end\":43159,\"start\":42564},{\"end\":43544,\"start\":43161},{\"end\":43911,\"start\":43546},{\"end\":44788,\"start\":43913},{\"end\":45457,\"start\":44849},{\"end\":45815,\"start\":45473}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6338,\"start\":6229},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8510,\"start\":8452},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21136,\"start\":21008},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21905,\"start\":21704}]", "table_ref": "[{\"end\":17824,\"start\":17817},{\"end\":30392,\"start\":30385},{\"end\":31692,\"start\":31683},{\"end\":32090,\"start\":32083},{\"end\":33742,\"start\":33735},{\"end\":44324,\"start\":44317},{\"end\":44590,\"start\":44583}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1960,\"start\":1948},{\"attributes\":{\"n\":\"2.\"},\"end\":7462,\"start\":7450},{\"attributes\":{\"n\":\"3.\"},\"end\":12490,\"start\":12461},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13474,\"start\":13445},{\"attributes\":{\"n\":\"3.2.\"},\"end\":18868,\"start\":18848},{\"attributes\":{\"n\":\"3.3.\"},\"end\":21915,\"start\":21907},{\"attributes\":{\"n\":\"4.\"},\"end\":25983,\"start\":25972},{\"attributes\":{\"n\":\"5.\"},\"end\":34930,\"start\":34920},{\"end\":35685,\"start\":35658},{\"end\":36389,\"start\":36366},{\"end\":36533,\"start\":36507},{\"end\":36822,\"start\":36794},{\"end\":37512,\"start\":37509},{\"end\":37609,\"start\":37606},{\"end\":38871,\"start\":38859},{\"end\":40907,\"start\":40879},{\"end\":42562,\"start\":42531},{\"end\":44815,\"start\":44791},{\"end\":44847,\"start\":44818},{\"end\":45471,\"start\":45460},{\"end\":45827,\"start\":45817},{\"end\":46001,\"start\":45991},{\"end\":46129,\"start\":46119},{\"end\":46901,\"start\":46891},{\"end\":47019,\"start\":47009},{\"end\":47096,\"start\":47085},{\"end\":47142,\"start\":47131},{\"end\":47190,\"start\":47179},{\"end\":47238,\"start\":47227},{\"end\":47297,\"start\":47277},{\"end\":47615,\"start\":47604},{\"end\":47726,\"start\":47704},{\"end\":48015,\"start\":48004},{\"end\":48180,\"start\":48103}]", "table": null, "figure_caption": "[{\"end\":45989,\"start\":45829},{\"end\":46117,\"start\":46003},{\"end\":46889,\"start\":46131},{\"end\":47007,\"start\":46903},{\"end\":47083,\"start\":47021},{\"end\":47129,\"start\":47099},{\"end\":47177,\"start\":47145},{\"end\":47225,\"start\":47193},{\"end\":47275,\"start\":47241},{\"end\":47602,\"start\":47302},{\"end\":47702,\"start\":47618},{\"end\":48002,\"start\":47731},{\"end\":48101,\"start\":48018},{\"end\":48622,\"start\":48195},{\"end\":48787,\"start\":48625}]", "figure_ref": "[{\"end\":6369,\"start\":6361},{\"end\":8542,\"start\":8534},{\"end\":13988,\"start\":13981},{\"end\":19042,\"start\":19036},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20100,\"start\":20092},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28609,\"start\":28603},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29417,\"start\":29411},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29887,\"start\":29881},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":32217,\"start\":32211},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":32320,\"start\":32311},{\"end\":36019,\"start\":36010},{\"end\":36361,\"start\":36352},{\"end\":36622,\"start\":36616},{\"end\":39175,\"start\":39167},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":41231,\"start\":41224},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":41243,\"start\":41236},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":41355,\"start\":41336},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":41469,\"start\":41460},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":41964,\"start\":41956},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":42078,\"start\":42059},{\"end\":42802,\"start\":42793},{\"end\":43002,\"start\":42993},{\"end\":43312,\"start\":43303},{\"end\":43505,\"start\":43496},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":45254,\"start\":45238},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":45266,\"start\":45259},{\"end\":45377,\"start\":45370},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":45386,\"start\":45379},{\"end\":45398,\"start\":45391},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":45806,\"start\":45763}]", "bib_author_first_name": "[{\"end\":49569,\"start\":49568},{\"end\":49577,\"start\":49576},{\"end\":49588,\"start\":49587},{\"end\":49596,\"start\":49595},{\"end\":49613,\"start\":49612},{\"end\":49615,\"start\":49614},{\"end\":49970,\"start\":49969},{\"end\":49982,\"start\":49981},{\"end\":49988,\"start\":49987},{\"end\":49997,\"start\":49996},{\"end\":50006,\"start\":50005},{\"end\":50017,\"start\":50016},{\"end\":50031,\"start\":50030},{\"end\":50039,\"start\":50038},{\"end\":50048,\"start\":50047},{\"end\":50392,\"start\":50391},{\"end\":50403,\"start\":50402},{\"end\":50415,\"start\":50414},{\"end\":50429,\"start\":50428},{\"end\":50716,\"start\":50715},{\"end\":50718,\"start\":50717},{\"end\":50731,\"start\":50730},{\"end\":50741,\"start\":50740},{\"end\":50751,\"start\":50750},{\"end\":51044,\"start\":51043},{\"end\":51054,\"start\":51053},{\"end\":51067,\"start\":51066},{\"end\":51077,\"start\":51076},{\"end\":51089,\"start\":51088},{\"end\":51102,\"start\":51101},{\"end\":51106,\"start\":51103},{\"end\":51386,\"start\":51385},{\"end\":51400,\"start\":51399},{\"end\":51659,\"start\":51658},{\"end\":51673,\"start\":51672},{\"end\":51916,\"start\":51915},{\"end\":51930,\"start\":51929},{\"end\":52181,\"start\":52180},{\"end\":52192,\"start\":52191},{\"end\":52206,\"start\":52205},{\"end\":52208,\"start\":52207},{\"end\":52217,\"start\":52216},{\"end\":52593,\"start\":52592},{\"end\":52604,\"start\":52603},{\"end\":52613,\"start\":52612},{\"end\":52622,\"start\":52621},{\"end\":52632,\"start\":52631},{\"end\":52643,\"start\":52642},{\"end\":52652,\"start\":52651},{\"end\":52661,\"start\":52660},{\"end\":52675,\"start\":52674},{\"end\":52945,\"start\":52944},{\"end\":52956,\"start\":52955},{\"end\":52967,\"start\":52966},{\"end\":52969,\"start\":52968},{\"end\":53293,\"start\":53292},{\"end\":53303,\"start\":53302},{\"end\":53314,\"start\":53313},{\"end\":53326,\"start\":53325},{\"end\":53335,\"start\":53334},{\"end\":53349,\"start\":53348},{\"end\":53358,\"start\":53357},{\"end\":53366,\"start\":53365},{\"end\":53368,\"start\":53367},{\"end\":53683,\"start\":53682},{\"end\":53835,\"start\":53834},{\"end\":53844,\"start\":53843},{\"end\":53851,\"start\":53850},{\"end\":53865,\"start\":53864},{\"end\":53875,\"start\":53874},{\"end\":53887,\"start\":53886},{\"end\":53896,\"start\":53895},{\"end\":53904,\"start\":53903},{\"end\":53912,\"start\":53911},{\"end\":54256,\"start\":54255},{\"end\":54258,\"start\":54257},{\"end\":54269,\"start\":54268},{\"end\":54271,\"start\":54270},{\"end\":54285,\"start\":54284},{\"end\":54287,\"start\":54286},{\"end\":54301,\"start\":54300},{\"end\":54305,\"start\":54304},{\"end\":54307,\"start\":54306},{\"end\":54618,\"start\":54617},{\"end\":54626,\"start\":54625},{\"end\":54634,\"start\":54633},{\"end\":54648,\"start\":54647},{\"end\":54871,\"start\":54870},{\"end\":54881,\"start\":54880},{\"end\":54883,\"start\":54882},{\"end\":54894,\"start\":54893},{\"end\":55164,\"start\":55163},{\"end\":55171,\"start\":55170},{\"end\":55182,\"start\":55181},{\"end\":55184,\"start\":55183},{\"end\":55195,\"start\":55194},{\"end\":55205,\"start\":55204},{\"end\":55422,\"start\":55421},{\"end\":55429,\"start\":55428},{\"end\":55438,\"start\":55437},{\"end\":55450,\"start\":55449},{\"end\":55457,\"start\":55456},{\"end\":55467,\"start\":55466},{\"end\":55616,\"start\":55615},{\"end\":55623,\"start\":55622},{\"end\":55633,\"start\":55632},{\"end\":55643,\"start\":55642},{\"end\":55652,\"start\":55651},{\"end\":55662,\"start\":55661},{\"end\":55672,\"start\":55671},{\"end\":55975,\"start\":55974},{\"end\":55982,\"start\":55981},{\"end\":55994,\"start\":55993},{\"end\":56001,\"start\":56000},{\"end\":56011,\"start\":56010},{\"end\":56248,\"start\":56247},{\"end\":56255,\"start\":56254},{\"end\":56265,\"start\":56264},{\"end\":56267,\"start\":56266},{\"end\":56276,\"start\":56275},{\"end\":56283,\"start\":56282},{\"end\":56534,\"start\":56533},{\"end\":56536,\"start\":56535},{\"end\":56704,\"start\":56703},{\"end\":56716,\"start\":56715},{\"end\":56725,\"start\":56724},{\"end\":56735,\"start\":56734},{\"end\":56878,\"start\":56877},{\"end\":56886,\"start\":56885},{\"end\":56898,\"start\":56897},{\"end\":56906,\"start\":56905},{\"end\":56908,\"start\":56907},{\"end\":56920,\"start\":56919},{\"end\":56933,\"start\":56932},{\"end\":57273,\"start\":57272},{\"end\":57463,\"start\":57462},{\"end\":57648,\"start\":57647},{\"end\":57650,\"start\":57649},{\"end\":57662,\"start\":57661},{\"end\":57664,\"start\":57663},{\"end\":57674,\"start\":57673},{\"end\":57685,\"start\":57684},{\"end\":57923,\"start\":57922},{\"end\":57925,\"start\":57924},{\"end\":57937,\"start\":57936},{\"end\":57949,\"start\":57948},{\"end\":57960,\"start\":57959},{\"end\":57972,\"start\":57971},{\"end\":58215,\"start\":58214},{\"end\":58217,\"start\":58216},{\"end\":58229,\"start\":58228},{\"end\":58241,\"start\":58240},{\"end\":58253,\"start\":58252},{\"end\":58257,\"start\":58254},{\"end\":58265,\"start\":58264},{\"end\":58274,\"start\":58273},{\"end\":58570,\"start\":58569},{\"end\":58585,\"start\":58584},{\"end\":58593,\"start\":58592},{\"end\":58605,\"start\":58604},{\"end\":58607,\"start\":58606},{\"end\":58614,\"start\":58613},{\"end\":58623,\"start\":58622},{\"end\":58636,\"start\":58635},{\"end\":58899,\"start\":58898},{\"end\":58901,\"start\":58900},{\"end\":58910,\"start\":58909},{\"end\":58912,\"start\":58911},{\"end\":59241,\"start\":59240},{\"end\":59251,\"start\":59250},{\"end\":59260,\"start\":59259},{\"end\":59264,\"start\":59261},{\"end\":59272,\"start\":59271},{\"end\":59274,\"start\":59273},{\"end\":59280,\"start\":59279},{\"end\":59282,\"start\":59281},{\"end\":59295,\"start\":59294},{\"end\":59306,\"start\":59305},{\"end\":59318,\"start\":59317},{\"end\":59329,\"start\":59328},{\"end\":59344,\"start\":59343},{\"end\":59346,\"start\":59345},{\"end\":59644,\"start\":59643},{\"end\":59654,\"start\":59653},{\"end\":59666,\"start\":59665},{\"end\":59679,\"start\":59678},{\"end\":59689,\"start\":59688},{\"end\":59696,\"start\":59695},{\"end\":59949,\"start\":59948},{\"end\":59951,\"start\":59950},{\"end\":59960,\"start\":59959},{\"end\":59970,\"start\":59969},{\"end\":60203,\"start\":60202},{\"end\":60212,\"start\":60211},{\"end\":60224,\"start\":60223},{\"end\":60234,\"start\":60233},{\"end\":60248,\"start\":60247},{\"end\":60495,\"start\":60494},{\"end\":60504,\"start\":60503},{\"end\":60514,\"start\":60513},{\"end\":60524,\"start\":60523},{\"end\":60792,\"start\":60791},{\"end\":60802,\"start\":60801},{\"end\":60812,\"start\":60811},{\"end\":60823,\"start\":60822},{\"end\":61041,\"start\":61040},{\"end\":61049,\"start\":61048},{\"end\":61051,\"start\":61050},{\"end\":61062,\"start\":61061},{\"end\":61073,\"start\":61072},{\"end\":61261,\"start\":61260},{\"end\":61275,\"start\":61274},{\"end\":61462,\"start\":61461},{\"end\":61473,\"start\":61472},{\"end\":61484,\"start\":61483},{\"end\":61494,\"start\":61493},{\"end\":61748,\"start\":61747},{\"end\":61756,\"start\":61755},{\"end\":61765,\"start\":61764},{\"end\":61775,\"start\":61774},{\"end\":62021,\"start\":62020},{\"end\":62030,\"start\":62029},{\"end\":62041,\"start\":62040},{\"end\":62436,\"start\":62435},{\"end\":62438,\"start\":62437},{\"end\":62449,\"start\":62448},{\"end\":62463,\"start\":62459},{\"end\":62474,\"start\":62473},{\"end\":62723,\"start\":62722},{\"end\":62733,\"start\":62732},{\"end\":62744,\"start\":62743},{\"end\":62752,\"start\":62751},{\"end\":62762,\"start\":62761},{\"end\":63171,\"start\":63170},{\"end\":63181,\"start\":63180},{\"end\":63190,\"start\":63189},{\"end\":63434,\"start\":63433},{\"end\":63441,\"start\":63440},{\"end\":63618,\"start\":63617},{\"end\":63627,\"start\":63626},{\"end\":63639,\"start\":63638},{\"end\":63649,\"start\":63648},{\"end\":63656,\"start\":63655},{\"end\":63665,\"start\":63664},{\"end\":63867,\"start\":63866},{\"end\":63879,\"start\":63878},{\"end\":63889,\"start\":63888},{\"end\":63897,\"start\":63896},{\"end\":63909,\"start\":63908},{\"end\":63921,\"start\":63920},{\"end\":63932,\"start\":63931},{\"end\":64243,\"start\":64242},{\"end\":64254,\"start\":64253},{\"end\":64256,\"start\":64255},{\"end\":64522,\"start\":64521},{\"end\":64531,\"start\":64530},{\"end\":64757,\"start\":64756},{\"end\":64759,\"start\":64758},{\"end\":64770,\"start\":64769},{\"end\":64772,\"start\":64771},{\"end\":65060,\"start\":65059},{\"end\":65073,\"start\":65072},{\"end\":65089,\"start\":65088},{\"end\":65351,\"start\":65350},{\"end\":65361,\"start\":65360},{\"end\":65556,\"start\":65555},{\"end\":65565,\"start\":65564},{\"end\":65574,\"start\":65573},{\"end\":65790,\"start\":65789},{\"end\":65798,\"start\":65797},{\"end\":65804,\"start\":65803},{\"end\":65813,\"start\":65812},{\"end\":65821,\"start\":65820},{\"end\":65831,\"start\":65830},{\"end\":66137,\"start\":66136},{\"end\":66149,\"start\":66148},{\"end\":66151,\"start\":66150},{\"end\":66162,\"start\":66161},{\"end\":66164,\"start\":66163},{\"end\":66441,\"start\":66440},{\"end\":66443,\"start\":66442},{\"end\":66451,\"start\":66450},{\"end\":66460,\"start\":66459},{\"end\":66462,\"start\":66461},{\"end\":66713,\"start\":66712},{\"end\":66725,\"start\":66724},{\"end\":66736,\"start\":66735},{\"end\":66745,\"start\":66744},{\"end\":66754,\"start\":66753},{\"end\":66765,\"start\":66764},{\"end\":66775,\"start\":66774},{\"end\":66784,\"start\":66783},{\"end\":66796,\"start\":66795},{\"end\":66805,\"start\":66804},{\"end\":67084,\"start\":67083},{\"end\":67092,\"start\":67091},{\"end\":67094,\"start\":67093},{\"end\":67103,\"start\":67102},{\"end\":67112,\"start\":67111},{\"end\":67122,\"start\":67121},{\"end\":67124,\"start\":67123},{\"end\":67137,\"start\":67136},{\"end\":67147,\"start\":67146},{\"end\":67157,\"start\":67156},{\"end\":67451,\"start\":67450},{\"end\":67463,\"start\":67462},{\"end\":67680,\"start\":67679},{\"end\":67686,\"start\":67685},{\"end\":67702,\"start\":67701},{\"end\":67711,\"start\":67710},{\"end\":67961,\"start\":67960},{\"end\":67976,\"start\":67975},{\"end\":67985,\"start\":67984},{\"end\":67994,\"start\":67993},{\"end\":67996,\"start\":67995},{\"end\":68003,\"start\":68002},{\"end\":68287,\"start\":68286},{\"end\":68291,\"start\":68288},{\"end\":68301,\"start\":68300},{\"end\":68557,\"start\":68556},{\"end\":68567,\"start\":68566},{\"end\":68798,\"start\":68797},{\"end\":68810,\"start\":68809},{\"end\":68820,\"start\":68819},{\"end\":69038,\"start\":69037},{\"end\":69044,\"start\":69039},{\"end\":69053,\"start\":69052},{\"end\":69306,\"start\":69305},{\"end\":69315,\"start\":69314},{\"end\":69321,\"start\":69320},{\"end\":69331,\"start\":69330},{\"end\":69630,\"start\":69629},{\"end\":69645,\"start\":69644},{\"end\":69658,\"start\":69657},{\"end\":69882,\"start\":69881},{\"end\":69890,\"start\":69889},{\"end\":69900,\"start\":69899},{\"end\":70404,\"start\":70403},{\"end\":70413,\"start\":70412},{\"end\":70420,\"start\":70419},{\"end\":70429,\"start\":70428},{\"end\":70441,\"start\":70440},{\"end\":70454,\"start\":70453},{\"end\":70697,\"start\":70696},{\"end\":70707,\"start\":70706},{\"end\":70709,\"start\":70708},{\"end\":70721,\"start\":70720},{\"end\":70723,\"start\":70722},{\"end\":70972,\"start\":70971},{\"end\":70981,\"start\":70980},{\"end\":70983,\"start\":70982},{\"end\":70992,\"start\":70991},{\"end\":71007,\"start\":71006},{\"end\":71021,\"start\":71020},{\"end\":71219,\"start\":71218},{\"end\":71221,\"start\":71220},{\"end\":71235,\"start\":71234},{\"end\":71237,\"start\":71236},{\"end\":71416,\"start\":71415},{\"end\":71427,\"start\":71426},{\"end\":71719,\"start\":71718},{\"end\":71726,\"start\":71725},{\"end\":71728,\"start\":71727},{\"end\":71967,\"start\":71966},{\"end\":71969,\"start\":71968},{\"end\":71978,\"start\":71977},{\"end\":71986,\"start\":71985},{\"end\":72187,\"start\":72186},{\"end\":72195,\"start\":72194},{\"end\":72201,\"start\":72200},{\"end\":72209,\"start\":72208},{\"end\":72211,\"start\":72210},{\"end\":72220,\"start\":72219},{\"end\":72229,\"start\":72228},{\"end\":72459,\"start\":72458},{\"end\":72473,\"start\":72472},{\"end\":72482,\"start\":72481},{\"end\":72678,\"start\":72677},{\"end\":72692,\"start\":72691},{\"end\":72701,\"start\":72700},{\"end\":72713,\"start\":72712},{\"end\":72725,\"start\":72724},{\"end\":72928,\"start\":72927},{\"end\":72938,\"start\":72937},{\"end\":72950,\"start\":72949},{\"end\":72961,\"start\":72960},{\"end\":72973,\"start\":72972},{\"end\":72983,\"start\":72982},{\"end\":72990,\"start\":72989},{\"end\":72997,\"start\":72996},{\"end\":73271,\"start\":73270},{\"end\":73459,\"start\":73458},{\"end\":73664,\"start\":73663},{\"end\":73674,\"start\":73673},{\"end\":73898,\"start\":73897},{\"end\":74130,\"start\":74129},{\"end\":74357,\"start\":74356},{\"end\":74536,\"start\":74535},{\"end\":74542,\"start\":74541},{\"end\":74548,\"start\":74547},{\"end\":74560,\"start\":74559},{\"end\":74766,\"start\":74765},{\"end\":74776,\"start\":74775},{\"end\":74785,\"start\":74784},{\"end\":74798,\"start\":74797},{\"end\":74814,\"start\":74813},{\"end\":74824,\"start\":74823},{\"end\":75057,\"start\":75056},{\"end\":75066,\"start\":75065},{\"end\":75068,\"start\":75067},{\"end\":75084,\"start\":75083},{\"end\":75097,\"start\":75096},{\"end\":75401,\"start\":75400},{\"end\":75408,\"start\":75407},{\"end\":75418,\"start\":75417},{\"end\":75427,\"start\":75426},{\"end\":75434,\"start\":75433},{\"end\":75445,\"start\":75444},{\"end\":75454,\"start\":75453},{\"end\":75466,\"start\":75465},{\"end\":75760,\"start\":75759},{\"end\":75767,\"start\":75766},{\"end\":75779,\"start\":75778},{\"end\":75788,\"start\":75787},{\"end\":75790,\"start\":75789},{\"end\":75797,\"start\":75796},{\"end\":75806,\"start\":75805},{\"end\":75817,\"start\":75816}]", "bib_author_last_name": "[{\"end\":49574,\"start\":49570},{\"end\":49585,\"start\":49578},{\"end\":49593,\"start\":49589},{\"end\":49610,\"start\":49597},{\"end\":49624,\"start\":49616},{\"end\":49979,\"start\":49971},{\"end\":49985,\"start\":49983},{\"end\":49994,\"start\":49989},{\"end\":50003,\"start\":49998},{\"end\":50014,\"start\":50007},{\"end\":50028,\"start\":50018},{\"end\":50036,\"start\":50032},{\"end\":50045,\"start\":50040},{\"end\":50400,\"start\":50393},{\"end\":50412,\"start\":50404},{\"end\":50426,\"start\":50416},{\"end\":50438,\"start\":50430},{\"end\":50728,\"start\":50719},{\"end\":50738,\"start\":50732},{\"end\":50748,\"start\":50742},{\"end\":50759,\"start\":50752},{\"end\":51051,\"start\":51045},{\"end\":51064,\"start\":51055},{\"end\":51074,\"start\":51068},{\"end\":51086,\"start\":51078},{\"end\":51099,\"start\":51090},{\"end\":51111,\"start\":51107},{\"end\":51397,\"start\":51387},{\"end\":51406,\"start\":51401},{\"end\":51670,\"start\":51660},{\"end\":51679,\"start\":51674},{\"end\":51927,\"start\":51917},{\"end\":51935,\"start\":51931},{\"end\":52189,\"start\":52182},{\"end\":52203,\"start\":52193},{\"end\":52214,\"start\":52209},{\"end\":52223,\"start\":52218},{\"end\":52601,\"start\":52594},{\"end\":52610,\"start\":52605},{\"end\":52619,\"start\":52614},{\"end\":52629,\"start\":52623},{\"end\":52640,\"start\":52633},{\"end\":52649,\"start\":52644},{\"end\":52658,\"start\":52653},{\"end\":52672,\"start\":52662},{\"end\":52685,\"start\":52676},{\"end\":52953,\"start\":52946},{\"end\":52964,\"start\":52957},{\"end\":52978,\"start\":52970},{\"end\":53300,\"start\":53294},{\"end\":53311,\"start\":53304},{\"end\":53323,\"start\":53315},{\"end\":53332,\"start\":53327},{\"end\":53346,\"start\":53336},{\"end\":53355,\"start\":53350},{\"end\":53363,\"start\":53359},{\"end\":53376,\"start\":53369},{\"end\":53689,\"start\":53684},{\"end\":53841,\"start\":53836},{\"end\":53848,\"start\":53845},{\"end\":53862,\"start\":53852},{\"end\":53872,\"start\":53866},{\"end\":53884,\"start\":53876},{\"end\":53893,\"start\":53888},{\"end\":53901,\"start\":53897},{\"end\":53909,\"start\":53905},{\"end\":53918,\"start\":53913},{\"end\":54266,\"start\":54259},{\"end\":54282,\"start\":54272},{\"end\":54298,\"start\":54288},{\"end\":54321,\"start\":54308},{\"end\":54623,\"start\":54619},{\"end\":54631,\"start\":54627},{\"end\":54645,\"start\":54635},{\"end\":54653,\"start\":54649},{\"end\":54878,\"start\":54872},{\"end\":54891,\"start\":54884},{\"end\":54901,\"start\":54895},{\"end\":55168,\"start\":55165},{\"end\":55179,\"start\":55172},{\"end\":55192,\"start\":55185},{\"end\":55202,\"start\":55196},{\"end\":55211,\"start\":55206},{\"end\":55426,\"start\":55423},{\"end\":55435,\"start\":55430},{\"end\":55447,\"start\":55439},{\"end\":55454,\"start\":55451},{\"end\":55464,\"start\":55458},{\"end\":55473,\"start\":55468},{\"end\":55620,\"start\":55617},{\"end\":55630,\"start\":55624},{\"end\":55640,\"start\":55634},{\"end\":55649,\"start\":55644},{\"end\":55659,\"start\":55653},{\"end\":55669,\"start\":55663},{\"end\":55679,\"start\":55673},{\"end\":55979,\"start\":55976},{\"end\":55991,\"start\":55983},{\"end\":55998,\"start\":55995},{\"end\":56008,\"start\":56002},{\"end\":56017,\"start\":56012},{\"end\":56252,\"start\":56249},{\"end\":56262,\"start\":56256},{\"end\":56273,\"start\":56268},{\"end\":56280,\"start\":56277},{\"end\":56289,\"start\":56284},{\"end\":56544,\"start\":56537},{\"end\":56713,\"start\":56705},{\"end\":56722,\"start\":56717},{\"end\":56732,\"start\":56726},{\"end\":56741,\"start\":56736},{\"end\":56883,\"start\":56879},{\"end\":56895,\"start\":56887},{\"end\":56903,\"start\":56899},{\"end\":56917,\"start\":56909},{\"end\":56930,\"start\":56921},{\"end\":56940,\"start\":56934},{\"end\":57279,\"start\":57274},{\"end\":57469,\"start\":57464},{\"end\":57659,\"start\":57651},{\"end\":57671,\"start\":57665},{\"end\":57682,\"start\":57675},{\"end\":57694,\"start\":57686},{\"end\":57934,\"start\":57926},{\"end\":57946,\"start\":57938},{\"end\":57957,\"start\":57950},{\"end\":57969,\"start\":57961},{\"end\":57981,\"start\":57973},{\"end\":58226,\"start\":58218},{\"end\":58238,\"start\":58230},{\"end\":58250,\"start\":58242},{\"end\":58262,\"start\":58258},{\"end\":58271,\"start\":58266},{\"end\":58283,\"start\":58275},{\"end\":58582,\"start\":58571},{\"end\":58590,\"start\":58586},{\"end\":58602,\"start\":58594},{\"end\":58611,\"start\":58608},{\"end\":58620,\"start\":58615},{\"end\":58633,\"start\":58624},{\"end\":58646,\"start\":58637},{\"end\":58907,\"start\":58902},{\"end\":58916,\"start\":58913},{\"end\":59248,\"start\":59242},{\"end\":59257,\"start\":59252},{\"end\":59269,\"start\":59265},{\"end\":59277,\"start\":59275},{\"end\":59292,\"start\":59283},{\"end\":59303,\"start\":59296},{\"end\":59315,\"start\":59307},{\"end\":59326,\"start\":59319},{\"end\":59341,\"start\":59330},{\"end\":59351,\"start\":59347},{\"end\":59651,\"start\":59645},{\"end\":59663,\"start\":59655},{\"end\":59676,\"start\":59667},{\"end\":59686,\"start\":59680},{\"end\":59693,\"start\":59690},{\"end\":59704,\"start\":59697},{\"end\":59957,\"start\":59952},{\"end\":59967,\"start\":59961},{\"end\":59983,\"start\":59971},{\"end\":60209,\"start\":60204},{\"end\":60221,\"start\":60213},{\"end\":60231,\"start\":60225},{\"end\":60245,\"start\":60235},{\"end\":60254,\"start\":60249},{\"end\":60501,\"start\":60496},{\"end\":60511,\"start\":60505},{\"end\":60521,\"start\":60515},{\"end\":60530,\"start\":60525},{\"end\":60799,\"start\":60793},{\"end\":60809,\"start\":60803},{\"end\":60820,\"start\":60813},{\"end\":60831,\"start\":60824},{\"end\":61046,\"start\":61042},{\"end\":61059,\"start\":61052},{\"end\":61070,\"start\":61063},{\"end\":61079,\"start\":61074},{\"end\":61272,\"start\":61262},{\"end\":61287,\"start\":61276},{\"end\":61470,\"start\":61463},{\"end\":61481,\"start\":61474},{\"end\":61491,\"start\":61485},{\"end\":61502,\"start\":61495},{\"end\":61753,\"start\":61749},{\"end\":61762,\"start\":61757},{\"end\":61772,\"start\":61766},{\"end\":61782,\"start\":61776},{\"end\":61789,\"start\":61784},{\"end\":62027,\"start\":62022},{\"end\":62038,\"start\":62031},{\"end\":62048,\"start\":62042},{\"end\":62446,\"start\":62439},{\"end\":62457,\"start\":62450},{\"end\":62471,\"start\":62464},{\"end\":62730,\"start\":62724},{\"end\":62741,\"start\":62734},{\"end\":62749,\"start\":62745},{\"end\":62759,\"start\":62753},{\"end\":62771,\"start\":62763},{\"end\":63178,\"start\":63172},{\"end\":63187,\"start\":63182},{\"end\":63197,\"start\":63191},{\"end\":63438,\"start\":63435},{\"end\":63449,\"start\":63442},{\"end\":63624,\"start\":63619},{\"end\":63636,\"start\":63628},{\"end\":63646,\"start\":63640},{\"end\":63653,\"start\":63650},{\"end\":63662,\"start\":63657},{\"end\":63673,\"start\":63666},{\"end\":63876,\"start\":63868},{\"end\":63886,\"start\":63880},{\"end\":63894,\"start\":63890},{\"end\":63906,\"start\":63898},{\"end\":63918,\"start\":63910},{\"end\":63929,\"start\":63922},{\"end\":63936,\"start\":63933},{\"end\":64251,\"start\":64244},{\"end\":64261,\"start\":64257},{\"end\":64528,\"start\":64523},{\"end\":64542,\"start\":64532},{\"end\":64767,\"start\":64760},{\"end\":64780,\"start\":64773},{\"end\":65070,\"start\":65061},{\"end\":65086,\"start\":65074},{\"end\":65096,\"start\":65090},{\"end\":65358,\"start\":65352},{\"end\":65368,\"start\":65362},{\"end\":65562,\"start\":65557},{\"end\":65571,\"start\":65566},{\"end\":65581,\"start\":65575},{\"end\":65795,\"start\":65791},{\"end\":65801,\"start\":65799},{\"end\":65810,\"start\":65805},{\"end\":65818,\"start\":65814},{\"end\":65828,\"start\":65822},{\"end\":65840,\"start\":65832},{\"end\":66146,\"start\":66138},{\"end\":66159,\"start\":66152},{\"end\":66174,\"start\":66165},{\"end\":66448,\"start\":66444},{\"end\":66457,\"start\":66452},{\"end\":66471,\"start\":66463},{\"end\":66722,\"start\":66714},{\"end\":66733,\"start\":66726},{\"end\":66742,\"start\":66737},{\"end\":66751,\"start\":66746},{\"end\":66762,\"start\":66755},{\"end\":66772,\"start\":66766},{\"end\":66781,\"start\":66776},{\"end\":66793,\"start\":66785},{\"end\":66802,\"start\":66797},{\"end\":66817,\"start\":66806},{\"end\":67089,\"start\":67085},{\"end\":67100,\"start\":67095},{\"end\":67109,\"start\":67104},{\"end\":67119,\"start\":67113},{\"end\":67134,\"start\":67125},{\"end\":67144,\"start\":67138},{\"end\":67154,\"start\":67148},{\"end\":67169,\"start\":67158},{\"end\":67460,\"start\":67452},{\"end\":67683,\"start\":67681},{\"end\":67699,\"start\":67687},{\"end\":67708,\"start\":67703},{\"end\":67715,\"start\":67712},{\"end\":67973,\"start\":67962},{\"end\":67982,\"start\":67977},{\"end\":67991,\"start\":67986},{\"end\":68000,\"start\":67997},{\"end\":68008,\"start\":68004},{\"end\":68298,\"start\":68292},{\"end\":68313,\"start\":68302},{\"end\":68564,\"start\":68558},{\"end\":68572,\"start\":68568},{\"end\":68807,\"start\":68799},{\"end\":68817,\"start\":68811},{\"end\":68831,\"start\":68821},{\"end\":69050,\"start\":69045},{\"end\":69063,\"start\":69054},{\"end\":69312,\"start\":69307},{\"end\":69318,\"start\":69316},{\"end\":69328,\"start\":69322},{\"end\":69335,\"start\":69332},{\"end\":69642,\"start\":69631},{\"end\":69655,\"start\":69646},{\"end\":69663,\"start\":69659},{\"end\":69887,\"start\":69883},{\"end\":69897,\"start\":69891},{\"end\":69908,\"start\":69901},{\"end\":70410,\"start\":70405},{\"end\":70417,\"start\":70414},{\"end\":70426,\"start\":70421},{\"end\":70438,\"start\":70430},{\"end\":70451,\"start\":70442},{\"end\":70464,\"start\":70455},{\"end\":70704,\"start\":70698},{\"end\":70718,\"start\":70710},{\"end\":70728,\"start\":70724},{\"end\":70978,\"start\":70973},{\"end\":70989,\"start\":70984},{\"end\":71004,\"start\":70993},{\"end\":71018,\"start\":71008},{\"end\":71028,\"start\":71022},{\"end\":71232,\"start\":71222},{\"end\":71243,\"start\":71238},{\"end\":71424,\"start\":71417},{\"end\":71436,\"start\":71428},{\"end\":71723,\"start\":71720},{\"end\":71735,\"start\":71729},{\"end\":71975,\"start\":71970},{\"end\":71983,\"start\":71979},{\"end\":71996,\"start\":71987},{\"end\":72192,\"start\":72188},{\"end\":72198,\"start\":72196},{\"end\":72206,\"start\":72202},{\"end\":72217,\"start\":72212},{\"end\":72226,\"start\":72221},{\"end\":72240,\"start\":72230},{\"end\":72470,\"start\":72460},{\"end\":72479,\"start\":72474},{\"end\":72489,\"start\":72483},{\"end\":72689,\"start\":72679},{\"end\":72698,\"start\":72693},{\"end\":72710,\"start\":72702},{\"end\":72722,\"start\":72714},{\"end\":72732,\"start\":72726},{\"end\":72935,\"start\":72929},{\"end\":72947,\"start\":72939},{\"end\":72958,\"start\":72951},{\"end\":72970,\"start\":72962},{\"end\":72980,\"start\":72974},{\"end\":72987,\"start\":72984},{\"end\":72994,\"start\":72991},{\"end\":73005,\"start\":72998},{\"end\":73275,\"start\":73272},{\"end\":73467,\"start\":73460},{\"end\":73671,\"start\":73665},{\"end\":73681,\"start\":73675},{\"end\":73905,\"start\":73899},{\"end\":74140,\"start\":74131},{\"end\":74364,\"start\":74358},{\"end\":74539,\"start\":74537},{\"end\":74545,\"start\":74543},{\"end\":74557,\"start\":74549},{\"end\":74565,\"start\":74561},{\"end\":74773,\"start\":74767},{\"end\":74782,\"start\":74777},{\"end\":74795,\"start\":74786},{\"end\":74811,\"start\":74799},{\"end\":74821,\"start\":74815},{\"end\":74831,\"start\":74825},{\"end\":75063,\"start\":75058},{\"end\":75081,\"start\":75069},{\"end\":75094,\"start\":75085},{\"end\":75105,\"start\":75098},{\"end\":75405,\"start\":75402},{\"end\":75415,\"start\":75409},{\"end\":75424,\"start\":75419},{\"end\":75431,\"start\":75428},{\"end\":75442,\"start\":75435},{\"end\":75451,\"start\":75446},{\"end\":75463,\"start\":75455},{\"end\":75474,\"start\":75467},{\"end\":75764,\"start\":75761},{\"end\":75776,\"start\":75768},{\"end\":75785,\"start\":75780},{\"end\":75794,\"start\":75791},{\"end\":75803,\"start\":75798},{\"end\":75814,\"start\":75807},{\"end\":75825,\"start\":75818}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1603.04119\",\"id\":\"b0\"},\"end\":49843,\"start\":49491},{\"attributes\":{\"id\":\"b1\"},\"end\":50310,\"start\":49845},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16506372},\"end\":50637,\"start\":50312},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1552061},\"end\":50980,\"start\":50639},{\"attributes\":{\"doi\":\"arXiv:1612.00380\",\"id\":\"b4\"},\"end\":51330,\"start\":50982},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":115137279},\"end\":51587,\"start\":51332},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":757244},\"end\":51869,\"start\":51589},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":12396215},\"end\":52046,\"start\":51871},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":157520},\"end\":52548,\"start\":52048},{\"attributes\":{\"id\":\"b9\"},\"end\":52881,\"start\":52550},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206794869},\"end\":53188,\"start\":52883},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2596787},\"end\":53639,\"start\":53190},{\"attributes\":{\"id\":\"b12\"},\"end\":53769,\"start\":53641},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b13\",\"matched_paper_id\":21435690},\"end\":54185,\"start\":53771},{\"attributes\":{\"doi\":\"CoRR, abs/1706.07230\",\"id\":\"b14\"},\"end\":54537,\"start\":54187},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15693605},\"end\":54801,\"start\":54539},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10634567},\"end\":55051,\"start\":54803},{\"attributes\":{\"id\":\"b17\"},\"end\":55390,\"start\":55053},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":35985986},\"end\":55613,\"start\":55392},{\"attributes\":{\"doi\":\"arXiv:1810.11187\",\"id\":\"b19\"},\"end\":55916,\"start\":55615},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":53080692},\"end\":56169,\"start\":55918},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1448723},\"end\":56461,\"start\":56171},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14399225},\"end\":56655,\"start\":56463},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6302583},\"end\":56875,\"start\":56657},{\"attributes\":{\"doi\":\"arXiv:1611.02779\",\"id\":\"b24\"},\"end\":57220,\"start\":56877},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":124426327},\"end\":57394,\"start\":57222},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7375948},\"end\":57575,\"start\":57396},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":53391180},\"end\":57875,\"start\":57577},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":19141434},\"end\":58137,\"start\":57877},{\"attributes\":{\"doi\":\"CoRR, abs/1702.08887\",\"id\":\"b29\"},\"end\":58496,\"start\":58139},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5644962},\"end\":58848,\"start\":58498},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6439950},\"end\":59153,\"start\":58850},{\"attributes\":{\"id\":\"b32\"},\"end\":59581,\"start\":59155},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4670339},\"end\":59879,\"start\":59583},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9421360},\"end\":60146,\"start\":59881},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":203658640},\"end\":60421,\"start\":60148},{\"attributes\":{\"doi\":\"arXiv:1712.08125\",\"id\":\"b36\"},\"end\":60721,\"start\":60423},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6125880},\"end\":60989,\"start\":60723},{\"attributes\":{\"doi\":\"abs/1710.09867\",\"id\":\"b38\",\"matched_paper_id\":9588773},\"end\":61234,\"start\":60991},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1915014},\"end\":61395,\"start\":61236},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9953039},\"end\":61689,\"start\":61397},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":13315738},\"end\":61939,\"start\":61691},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":15768446},\"end\":62341,\"start\":61941},{\"attributes\":{\"id\":\"b43\"},\"end\":62641,\"start\":62343},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":430714},\"end\":63088,\"start\":62643},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10331865},\"end\":63391,\"start\":63090},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":34337911},\"end\":63560,\"start\":63393},{\"attributes\":{\"id\":\"b47\"},\"end\":63847,\"start\":63562},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":7367040},\"end\":64108,\"start\":63849},{\"attributes\":{\"id\":\"b49\"},\"end\":64431,\"start\":64110},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":28535139},\"end\":64698,\"start\":64433},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":60736273},\"end\":64992,\"start\":64700},{\"attributes\":{\"doi\":\"arXivpreprintarXiv:1612.07182\",\"id\":\"b52\"},\"end\":65286,\"start\":64994},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":18371267},\"end\":65497,\"start\":65288},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":7771457},\"end\":65714,\"start\":65499},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":26419660},\"end\":66024,\"start\":65716},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":3465067},\"end\":66377,\"start\":66026},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":16879675},\"end\":66664,\"start\":66379},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":13298214},\"end\":67027,\"start\":66666},{\"attributes\":{\"id\":\"b59\"},\"end\":67375,\"start\":67029},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":13548281},\"end\":67614,\"start\":67377},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":10899248},\"end\":67864,\"start\":67616},{\"attributes\":{\"doi\":\"abs/1703.06182\",\"id\":\"b62\",\"matched_paper_id\":8894704},\"end\":68218,\"start\":67866},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":206537721},\"end\":68455,\"start\":68220},{\"attributes\":{\"id\":\"b64\"},\"end\":68737,\"start\":68457},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":910074},\"end\":68974,\"start\":68739},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":60110448},\"end\":69228,\"start\":68976},{\"attributes\":{\"id\":\"b67\"},\"end\":69584,\"start\":69230},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":16462271},\"end\":69789,\"start\":69586},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":103456},\"end\":70347,\"start\":69791},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":8079824},\"end\":70631,\"start\":70349},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":8247056},\"end\":70894,\"start\":70633},{\"attributes\":{\"id\":\"b72\"},\"end\":71183,\"start\":70896},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":1728538},\"end\":71350,\"start\":71185},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":62426288},\"end\":71610,\"start\":71352},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":9311873},\"end\":71908,\"start\":71612},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":8122125},\"end\":72131,\"start\":71910},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":20416090},\"end\":72400,\"start\":72133},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":6925519},\"end\":72630,\"start\":72402},{\"attributes\":{\"id\":\"b79\"},\"end\":72852,\"start\":72632},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":12046082},\"end\":73196,\"start\":72854},{\"attributes\":{\"id\":\"b81\"},\"end\":73395,\"start\":73198},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":3093694},\"end\":73582,\"start\":73397},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":2931825},\"end\":73807,\"start\":73584},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":16716643},\"end\":74044,\"start\":73809},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":7057735},\"end\":74277,\"start\":74046},{\"attributes\":{\"id\":\"b86\"},\"end\":74461,\"start\":74279},{\"attributes\":{\"id\":\"b87\"},\"end\":74725,\"start\":74463},{\"attributes\":{\"id\":\"b88\"},\"end\":74958,\"start\":74727},{\"attributes\":{\"doi\":\"arXiv:1612.05533\",\"id\":\"b89\"},\"end\":75335,\"start\":74960},{\"attributes\":{\"id\":\"b90\"},\"end\":75673,\"start\":75337},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":2305273},\"end\":76032,\"start\":75675}]", "bib_title": "[{\"end\":49967,\"start\":49845},{\"end\":50389,\"start\":50312},{\"end\":50713,\"start\":50639},{\"end\":51383,\"start\":51332},{\"end\":51656,\"start\":51589},{\"end\":51913,\"start\":51871},{\"end\":52178,\"start\":52048},{\"end\":52942,\"start\":52883},{\"end\":53290,\"start\":53190},{\"end\":53832,\"start\":53771},{\"end\":54615,\"start\":54539},{\"end\":54868,\"start\":54803},{\"end\":55419,\"start\":55392},{\"end\":55972,\"start\":55918},{\"end\":56245,\"start\":56171},{\"end\":56531,\"start\":56463},{\"end\":56701,\"start\":56657},{\"end\":57270,\"start\":57222},{\"end\":57460,\"start\":57396},{\"end\":57645,\"start\":57577},{\"end\":57920,\"start\":57877},{\"end\":58567,\"start\":58498},{\"end\":58896,\"start\":58850},{\"end\":59641,\"start\":59583},{\"end\":59946,\"start\":59881},{\"end\":60200,\"start\":60148},{\"end\":60789,\"start\":60723},{\"end\":61038,\"start\":60991},{\"end\":61258,\"start\":61236},{\"end\":61459,\"start\":61397},{\"end\":61745,\"start\":61691},{\"end\":62018,\"start\":61941},{\"end\":62433,\"start\":62343},{\"end\":62720,\"start\":62643},{\"end\":63168,\"start\":63090},{\"end\":63431,\"start\":63393},{\"end\":63864,\"start\":63849},{\"end\":64519,\"start\":64433},{\"end\":64754,\"start\":64700},{\"end\":65348,\"start\":65288},{\"end\":65553,\"start\":65499},{\"end\":65787,\"start\":65716},{\"end\":66134,\"start\":66026},{\"end\":66438,\"start\":66379},{\"end\":66710,\"start\":66666},{\"end\":67448,\"start\":67377},{\"end\":67677,\"start\":67616},{\"end\":67958,\"start\":67866},{\"end\":68284,\"start\":68220},{\"end\":68554,\"start\":68457},{\"end\":68795,\"start\":68739},{\"end\":69035,\"start\":68976},{\"end\":69303,\"start\":69230},{\"end\":69627,\"start\":69586},{\"end\":69879,\"start\":69791},{\"end\":70401,\"start\":70349},{\"end\":70694,\"start\":70633},{\"end\":71216,\"start\":71185},{\"end\":71413,\"start\":71352},{\"end\":71716,\"start\":71612},{\"end\":71964,\"start\":71910},{\"end\":72184,\"start\":72133},{\"end\":72456,\"start\":72402},{\"end\":72925,\"start\":72854},{\"end\":73268,\"start\":73198},{\"end\":73456,\"start\":73397},{\"end\":73661,\"start\":73584},{\"end\":73895,\"start\":73809},{\"end\":74127,\"start\":74046},{\"end\":75757,\"start\":75675}]", "bib_author": "[{\"end\":49576,\"start\":49568},{\"end\":49587,\"start\":49576},{\"end\":49595,\"start\":49587},{\"end\":49612,\"start\":49595},{\"end\":49626,\"start\":49612},{\"end\":49981,\"start\":49969},{\"end\":49987,\"start\":49981},{\"end\":49996,\"start\":49987},{\"end\":50005,\"start\":49996},{\"end\":50016,\"start\":50005},{\"end\":50030,\"start\":50016},{\"end\":50038,\"start\":50030},{\"end\":50047,\"start\":50038},{\"end\":50051,\"start\":50047},{\"end\":50402,\"start\":50391},{\"end\":50414,\"start\":50402},{\"end\":50428,\"start\":50414},{\"end\":50440,\"start\":50428},{\"end\":50730,\"start\":50715},{\"end\":50740,\"start\":50730},{\"end\":50750,\"start\":50740},{\"end\":50761,\"start\":50750},{\"end\":51053,\"start\":51043},{\"end\":51066,\"start\":51053},{\"end\":51076,\"start\":51066},{\"end\":51088,\"start\":51076},{\"end\":51101,\"start\":51088},{\"end\":51113,\"start\":51101},{\"end\":51399,\"start\":51385},{\"end\":51408,\"start\":51399},{\"end\":51672,\"start\":51658},{\"end\":51681,\"start\":51672},{\"end\":51929,\"start\":51915},{\"end\":51937,\"start\":51929},{\"end\":52191,\"start\":52180},{\"end\":52205,\"start\":52191},{\"end\":52216,\"start\":52205},{\"end\":52225,\"start\":52216},{\"end\":52603,\"start\":52592},{\"end\":52612,\"start\":52603},{\"end\":52621,\"start\":52612},{\"end\":52631,\"start\":52621},{\"end\":52642,\"start\":52631},{\"end\":52651,\"start\":52642},{\"end\":52660,\"start\":52651},{\"end\":52674,\"start\":52660},{\"end\":52687,\"start\":52674},{\"end\":52955,\"start\":52944},{\"end\":52966,\"start\":52955},{\"end\":52980,\"start\":52966},{\"end\":53302,\"start\":53292},{\"end\":53313,\"start\":53302},{\"end\":53325,\"start\":53313},{\"end\":53334,\"start\":53325},{\"end\":53348,\"start\":53334},{\"end\":53357,\"start\":53348},{\"end\":53365,\"start\":53357},{\"end\":53378,\"start\":53365},{\"end\":53691,\"start\":53682},{\"end\":53843,\"start\":53834},{\"end\":53850,\"start\":53843},{\"end\":53864,\"start\":53850},{\"end\":53874,\"start\":53864},{\"end\":53886,\"start\":53874},{\"end\":53895,\"start\":53886},{\"end\":53903,\"start\":53895},{\"end\":53911,\"start\":53903},{\"end\":53920,\"start\":53911},{\"end\":54268,\"start\":54255},{\"end\":54284,\"start\":54268},{\"end\":54300,\"start\":54284},{\"end\":54304,\"start\":54300},{\"end\":54323,\"start\":54304},{\"end\":54625,\"start\":54617},{\"end\":54633,\"start\":54625},{\"end\":54647,\"start\":54633},{\"end\":54655,\"start\":54647},{\"end\":54880,\"start\":54870},{\"end\":54893,\"start\":54880},{\"end\":54903,\"start\":54893},{\"end\":55170,\"start\":55163},{\"end\":55181,\"start\":55170},{\"end\":55194,\"start\":55181},{\"end\":55204,\"start\":55194},{\"end\":55213,\"start\":55204},{\"end\":55428,\"start\":55421},{\"end\":55437,\"start\":55428},{\"end\":55449,\"start\":55437},{\"end\":55456,\"start\":55449},{\"end\":55466,\"start\":55456},{\"end\":55475,\"start\":55466},{\"end\":55622,\"start\":55615},{\"end\":55632,\"start\":55622},{\"end\":55642,\"start\":55632},{\"end\":55651,\"start\":55642},{\"end\":55661,\"start\":55651},{\"end\":55671,\"start\":55661},{\"end\":55681,\"start\":55671},{\"end\":55981,\"start\":55974},{\"end\":55993,\"start\":55981},{\"end\":56000,\"start\":55993},{\"end\":56010,\"start\":56000},{\"end\":56019,\"start\":56010},{\"end\":56254,\"start\":56247},{\"end\":56264,\"start\":56254},{\"end\":56275,\"start\":56264},{\"end\":56282,\"start\":56275},{\"end\":56291,\"start\":56282},{\"end\":56546,\"start\":56533},{\"end\":56715,\"start\":56703},{\"end\":56724,\"start\":56715},{\"end\":56734,\"start\":56724},{\"end\":56743,\"start\":56734},{\"end\":56885,\"start\":56877},{\"end\":56897,\"start\":56885},{\"end\":56905,\"start\":56897},{\"end\":56919,\"start\":56905},{\"end\":56932,\"start\":56919},{\"end\":56942,\"start\":56932},{\"end\":57281,\"start\":57272},{\"end\":57471,\"start\":57462},{\"end\":57661,\"start\":57647},{\"end\":57673,\"start\":57661},{\"end\":57684,\"start\":57673},{\"end\":57696,\"start\":57684},{\"end\":57936,\"start\":57922},{\"end\":57948,\"start\":57936},{\"end\":57959,\"start\":57948},{\"end\":57971,\"start\":57959},{\"end\":57983,\"start\":57971},{\"end\":58228,\"start\":58214},{\"end\":58240,\"start\":58228},{\"end\":58252,\"start\":58240},{\"end\":58264,\"start\":58252},{\"end\":58273,\"start\":58264},{\"end\":58285,\"start\":58273},{\"end\":58584,\"start\":58569},{\"end\":58592,\"start\":58584},{\"end\":58604,\"start\":58592},{\"end\":58613,\"start\":58604},{\"end\":58622,\"start\":58613},{\"end\":58635,\"start\":58622},{\"end\":58648,\"start\":58635},{\"end\":58909,\"start\":58898},{\"end\":58918,\"start\":58909},{\"end\":59250,\"start\":59240},{\"end\":59259,\"start\":59250},{\"end\":59271,\"start\":59259},{\"end\":59279,\"start\":59271},{\"end\":59294,\"start\":59279},{\"end\":59305,\"start\":59294},{\"end\":59317,\"start\":59305},{\"end\":59328,\"start\":59317},{\"end\":59343,\"start\":59328},{\"end\":59353,\"start\":59343},{\"end\":59653,\"start\":59643},{\"end\":59665,\"start\":59653},{\"end\":59678,\"start\":59665},{\"end\":59688,\"start\":59678},{\"end\":59695,\"start\":59688},{\"end\":59706,\"start\":59695},{\"end\":59959,\"start\":59948},{\"end\":59969,\"start\":59959},{\"end\":59985,\"start\":59969},{\"end\":60211,\"start\":60202},{\"end\":60223,\"start\":60211},{\"end\":60233,\"start\":60223},{\"end\":60247,\"start\":60233},{\"end\":60256,\"start\":60247},{\"end\":60503,\"start\":60494},{\"end\":60513,\"start\":60503},{\"end\":60523,\"start\":60513},{\"end\":60532,\"start\":60523},{\"end\":60801,\"start\":60791},{\"end\":60811,\"start\":60801},{\"end\":60822,\"start\":60811},{\"end\":60833,\"start\":60822},{\"end\":61048,\"start\":61040},{\"end\":61061,\"start\":61048},{\"end\":61072,\"start\":61061},{\"end\":61081,\"start\":61072},{\"end\":61274,\"start\":61260},{\"end\":61289,\"start\":61274},{\"end\":61472,\"start\":61461},{\"end\":61483,\"start\":61472},{\"end\":61493,\"start\":61483},{\"end\":61504,\"start\":61493},{\"end\":61755,\"start\":61747},{\"end\":61764,\"start\":61755},{\"end\":61774,\"start\":61764},{\"end\":61784,\"start\":61774},{\"end\":61791,\"start\":61784},{\"end\":62029,\"start\":62020},{\"end\":62040,\"start\":62029},{\"end\":62050,\"start\":62040},{\"end\":62448,\"start\":62435},{\"end\":62459,\"start\":62448},{\"end\":62473,\"start\":62459},{\"end\":62477,\"start\":62473},{\"end\":62732,\"start\":62722},{\"end\":62743,\"start\":62732},{\"end\":62751,\"start\":62743},{\"end\":62761,\"start\":62751},{\"end\":62773,\"start\":62761},{\"end\":63180,\"start\":63170},{\"end\":63189,\"start\":63180},{\"end\":63199,\"start\":63189},{\"end\":63440,\"start\":63433},{\"end\":63451,\"start\":63440},{\"end\":63626,\"start\":63617},{\"end\":63638,\"start\":63626},{\"end\":63648,\"start\":63638},{\"end\":63655,\"start\":63648},{\"end\":63664,\"start\":63655},{\"end\":63675,\"start\":63664},{\"end\":63878,\"start\":63866},{\"end\":63888,\"start\":63878},{\"end\":63896,\"start\":63888},{\"end\":63908,\"start\":63896},{\"end\":63920,\"start\":63908},{\"end\":63931,\"start\":63920},{\"end\":63938,\"start\":63931},{\"end\":64253,\"start\":64242},{\"end\":64263,\"start\":64253},{\"end\":64530,\"start\":64521},{\"end\":64544,\"start\":64530},{\"end\":64769,\"start\":64756},{\"end\":64782,\"start\":64769},{\"end\":65072,\"start\":65059},{\"end\":65088,\"start\":65072},{\"end\":65098,\"start\":65088},{\"end\":65360,\"start\":65350},{\"end\":65370,\"start\":65360},{\"end\":65564,\"start\":65555},{\"end\":65573,\"start\":65564},{\"end\":65583,\"start\":65573},{\"end\":65797,\"start\":65789},{\"end\":65803,\"start\":65797},{\"end\":65812,\"start\":65803},{\"end\":65820,\"start\":65812},{\"end\":65830,\"start\":65820},{\"end\":65842,\"start\":65830},{\"end\":66148,\"start\":66136},{\"end\":66161,\"start\":66148},{\"end\":66176,\"start\":66161},{\"end\":66450,\"start\":66440},{\"end\":66459,\"start\":66450},{\"end\":66473,\"start\":66459},{\"end\":66724,\"start\":66712},{\"end\":66735,\"start\":66724},{\"end\":66744,\"start\":66735},{\"end\":66753,\"start\":66744},{\"end\":66764,\"start\":66753},{\"end\":66774,\"start\":66764},{\"end\":66783,\"start\":66774},{\"end\":66795,\"start\":66783},{\"end\":66804,\"start\":66795},{\"end\":66819,\"start\":66804},{\"end\":67091,\"start\":67083},{\"end\":67102,\"start\":67091},{\"end\":67111,\"start\":67102},{\"end\":67121,\"start\":67111},{\"end\":67136,\"start\":67121},{\"end\":67146,\"start\":67136},{\"end\":67156,\"start\":67146},{\"end\":67171,\"start\":67156},{\"end\":67462,\"start\":67450},{\"end\":67466,\"start\":67462},{\"end\":67685,\"start\":67679},{\"end\":67701,\"start\":67685},{\"end\":67710,\"start\":67701},{\"end\":67717,\"start\":67710},{\"end\":67975,\"start\":67960},{\"end\":67984,\"start\":67975},{\"end\":67993,\"start\":67984},{\"end\":68002,\"start\":67993},{\"end\":68010,\"start\":68002},{\"end\":68300,\"start\":68286},{\"end\":68315,\"start\":68300},{\"end\":68566,\"start\":68556},{\"end\":68574,\"start\":68566},{\"end\":68809,\"start\":68797},{\"end\":68819,\"start\":68809},{\"end\":68833,\"start\":68819},{\"end\":69052,\"start\":69037},{\"end\":69065,\"start\":69052},{\"end\":69314,\"start\":69305},{\"end\":69320,\"start\":69314},{\"end\":69330,\"start\":69320},{\"end\":69337,\"start\":69330},{\"end\":69644,\"start\":69629},{\"end\":69657,\"start\":69644},{\"end\":69665,\"start\":69657},{\"end\":69889,\"start\":69881},{\"end\":69899,\"start\":69889},{\"end\":69910,\"start\":69899},{\"end\":70412,\"start\":70403},{\"end\":70419,\"start\":70412},{\"end\":70428,\"start\":70419},{\"end\":70440,\"start\":70428},{\"end\":70453,\"start\":70440},{\"end\":70466,\"start\":70453},{\"end\":70706,\"start\":70696},{\"end\":70720,\"start\":70706},{\"end\":70730,\"start\":70720},{\"end\":70980,\"start\":70971},{\"end\":70991,\"start\":70980},{\"end\":71006,\"start\":70991},{\"end\":71020,\"start\":71006},{\"end\":71030,\"start\":71020},{\"end\":71234,\"start\":71218},{\"end\":71245,\"start\":71234},{\"end\":71426,\"start\":71415},{\"end\":71438,\"start\":71426},{\"end\":71725,\"start\":71718},{\"end\":71737,\"start\":71725},{\"end\":71977,\"start\":71966},{\"end\":71985,\"start\":71977},{\"end\":71998,\"start\":71985},{\"end\":72194,\"start\":72186},{\"end\":72200,\"start\":72194},{\"end\":72208,\"start\":72200},{\"end\":72219,\"start\":72208},{\"end\":72228,\"start\":72219},{\"end\":72242,\"start\":72228},{\"end\":72472,\"start\":72458},{\"end\":72481,\"start\":72472},{\"end\":72491,\"start\":72481},{\"end\":72691,\"start\":72677},{\"end\":72700,\"start\":72691},{\"end\":72712,\"start\":72700},{\"end\":72724,\"start\":72712},{\"end\":72734,\"start\":72724},{\"end\":72937,\"start\":72927},{\"end\":72949,\"start\":72937},{\"end\":72960,\"start\":72949},{\"end\":72972,\"start\":72960},{\"end\":72982,\"start\":72972},{\"end\":72989,\"start\":72982},{\"end\":72996,\"start\":72989},{\"end\":73007,\"start\":72996},{\"end\":73277,\"start\":73270},{\"end\":73469,\"start\":73458},{\"end\":73673,\"start\":73663},{\"end\":73683,\"start\":73673},{\"end\":73907,\"start\":73897},{\"end\":74142,\"start\":74129},{\"end\":74366,\"start\":74356},{\"end\":74541,\"start\":74535},{\"end\":74547,\"start\":74541},{\"end\":74559,\"start\":74547},{\"end\":74567,\"start\":74559},{\"end\":74775,\"start\":74765},{\"end\":74784,\"start\":74775},{\"end\":74797,\"start\":74784},{\"end\":74813,\"start\":74797},{\"end\":74823,\"start\":74813},{\"end\":74833,\"start\":74823},{\"end\":75065,\"start\":75056},{\"end\":75083,\"start\":75065},{\"end\":75096,\"start\":75083},{\"end\":75107,\"start\":75096},{\"end\":75407,\"start\":75400},{\"end\":75417,\"start\":75407},{\"end\":75426,\"start\":75417},{\"end\":75433,\"start\":75426},{\"end\":75444,\"start\":75433},{\"end\":75453,\"start\":75444},{\"end\":75465,\"start\":75453},{\"end\":75476,\"start\":75465},{\"end\":75766,\"start\":75759},{\"end\":75778,\"start\":75766},{\"end\":75787,\"start\":75778},{\"end\":75796,\"start\":75787},{\"end\":75805,\"start\":75796},{\"end\":75816,\"start\":75805},{\"end\":75827,\"start\":75816}]", "bib_venue": "[{\"end\":50067,\"start\":50063},{\"end\":51953,\"start\":51949},{\"end\":52301,\"start\":52267},{\"end\":54919,\"start\":54915},{\"end\":55491,\"start\":55487},{\"end\":56035,\"start\":56031},{\"end\":56307,\"start\":56303},{\"end\":56759,\"start\":56755},{\"end\":57712,\"start\":57708},{\"end\":57999,\"start\":57995},{\"end\":58664,\"start\":58660},{\"end\":59012,\"start\":58969},{\"end\":59722,\"start\":59718},{\"end\":60003,\"start\":59998},{\"end\":60272,\"start\":60268},{\"end\":60849,\"start\":60845},{\"end\":61807,\"start\":61803},{\"end\":62150,\"start\":62104},{\"end\":62881,\"start\":62831},{\"end\":64560,\"start\":64556},{\"end\":65386,\"start\":65382},{\"end\":65599,\"start\":65595},{\"end\":65858,\"start\":65854},{\"end\":66192,\"start\":66188},{\"end\":66519,\"start\":66500},{\"end\":66835,\"start\":66831},{\"end\":67482,\"start\":67478},{\"end\":67733,\"start\":67729},{\"end\":68331,\"start\":68327},{\"end\":68592,\"start\":68587},{\"end\":68849,\"start\":68845},{\"end\":69681,\"start\":69677},{\"end\":70089,\"start\":70008},{\"end\":70482,\"start\":70478},{\"end\":71261,\"start\":71257},{\"end\":71753,\"start\":71749},{\"end\":72012,\"start\":72009},{\"end\":72258,\"start\":72254},{\"end\":72507,\"start\":72503},{\"end\":73293,\"start\":73289},{\"end\":73485,\"start\":73481},{\"end\":73923,\"start\":73919},{\"end\":74158,\"start\":74154},{\"end\":75843,\"start\":75839},{\"end\":49566,\"start\":49491},{\"end\":50061,\"start\":50051},{\"end\":50463,\"start\":50440},{\"end\":50799,\"start\":50761},{\"end\":51041,\"start\":50982},{\"end\":51451,\"start\":51408},{\"end\":51719,\"start\":51681},{\"end\":51947,\"start\":51937},{\"end\":52265,\"start\":52225},{\"end\":52590,\"start\":52550},{\"end\":53023,\"start\":52980},{\"end\":53401,\"start\":53378},{\"end\":53680,\"start\":53641},{\"end\":53964,\"start\":53927},{\"end\":54253,\"start\":54187},{\"end\":54659,\"start\":54655},{\"end\":54913,\"start\":54903},{\"end\":55161,\"start\":55053},{\"end\":55485,\"start\":55475},{\"end\":55739,\"start\":55697},{\"end\":56029,\"start\":56019},{\"end\":56301,\"start\":56291},{\"end\":56550,\"start\":56546},{\"end\":56753,\"start\":56743},{\"end\":57022,\"start\":56958},{\"end\":57293,\"start\":57281},{\"end\":57479,\"start\":57471},{\"end\":57706,\"start\":57696},{\"end\":57993,\"start\":57983},{\"end\":58212,\"start\":58139},{\"end\":58658,\"start\":58648},{\"end\":58967,\"start\":58918},{\"end\":59238,\"start\":59155},{\"end\":59716,\"start\":59706},{\"end\":59996,\"start\":59985},{\"end\":60266,\"start\":60256},{\"end\":60492,\"start\":60423},{\"end\":60843,\"start\":60833},{\"end\":61099,\"start\":61095},{\"end\":61307,\"start\":61289},{\"end\":61532,\"start\":61504},{\"end\":61801,\"start\":61791},{\"end\":62102,\"start\":62050},{\"end\":62479,\"start\":62477},{\"end\":62829,\"start\":62773},{\"end\":63230,\"start\":63199},{\"end\":63468,\"start\":63451},{\"end\":63615,\"start\":63562},{\"end\":63967,\"start\":63938},{\"end\":64240,\"start\":64110},{\"end\":64554,\"start\":64544},{\"end\":64836,\"start\":64782},{\"end\":65057,\"start\":64994},{\"end\":65380,\"start\":65370},{\"end\":65593,\"start\":65583},{\"end\":65852,\"start\":65842},{\"end\":66186,\"start\":66176},{\"end\":66498,\"start\":66473},{\"end\":66829,\"start\":66819},{\"end\":67081,\"start\":67029},{\"end\":67476,\"start\":67466},{\"end\":67727,\"start\":67717},{\"end\":68028,\"start\":68024},{\"end\":68325,\"start\":68315},{\"end\":68585,\"start\":68574},{\"end\":68843,\"start\":68833},{\"end\":69091,\"start\":69065},{\"end\":69386,\"start\":69337},{\"end\":69675,\"start\":69665},{\"end\":70006,\"start\":69910},{\"end\":70476,\"start\":70466},{\"end\":70753,\"start\":70730},{\"end\":70969,\"start\":70896},{\"end\":71255,\"start\":71245},{\"end\":71470,\"start\":71438},{\"end\":71747,\"start\":71737},{\"end\":72007,\"start\":71998},{\"end\":72252,\"start\":72242},{\"end\":72501,\"start\":72491},{\"end\":72675,\"start\":72632},{\"end\":73011,\"start\":73007},{\"end\":73287,\"start\":73277},{\"end\":73479,\"start\":73469},{\"end\":73687,\"start\":73683},{\"end\":73917,\"start\":73907},{\"end\":74152,\"start\":74142},{\"end\":74354,\"start\":74279},{\"end\":74533,\"start\":74463},{\"end\":74763,\"start\":74727},{\"end\":75054,\"start\":74960},{\"end\":75398,\"start\":75337},{\"end\":75837,\"start\":75827}]"}}}, "year": 2023, "month": 12, "day": 17}