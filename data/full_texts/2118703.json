{"id": 2118703, "updated": "2023-09-27 23:11:50.129", "metadata": {"title": "Fine-Grained Visual Classification of Aircraft", "authors": "[{\"first\":\"Subhransu\",\"last\":\"Maji\",\"middle\":[]},{\"first\":\"Esa\",\"last\":\"Rahtu\",\"middle\":[]},{\"first\":\"Juho\",\"last\":\"Kannala\",\"middle\":[]},{\"first\":\"Matthew\",\"last\":\"Blaschko\",\"middle\":[]},{\"first\":\"Andrea\",\"last\":\"Vedaldi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2013, "month": 6, "day": 21}, "abstract": "This paper introduces FGVC-Aircraft, a new dataset containing 10,000 images of aircraft spanning 100 aircraft models, organised in a three-level hierarchy. At the finer level, differences between models are often subtle but always visually measurable, making visual recognition challenging but possible. A benchmark is obtained by defining corresponding classification tasks and evaluation protocols, and baseline results are presented. The construction of this dataset was made possible by the work of aircraft enthusiasts, a strategy that can extend to the study of number of other object classes. Compared to the domains usually considered in fine-grained visual classification (FGVC), for example animals, aircraft are rigid and hence less deformable. They, however, present other interesting modes of variation, including purpose, size, designation, structure, historical style, and branding.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1306.5151", "mag": "1846799578", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/MajiRKBV13", "doi": null}}, "content": {"source": {"pdf_hash": "522d65a3db7431015aeaa201a7fc4450a57e40c3", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1306.5151v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "573ae65ea79cdb7f2f6f135f59bfe5804c427ebd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/522d65a3db7431015aeaa201a7fc4450a57e40c3.txt", "contents": "\nFine-Grained Visual Classification of Aircraft\n\n\nSubhransu Maji smaji@ttic.edu \nTti Chicago \nEsa Rahtu erahtu@ee.oulu.fi \nJuho Kannala jkannala@ee.oulu.fi \nMatthew Blaschk\u00f3 matthew.blaschko@ecp.fr \nEcole Centrale Paris \nAndrea Vedaldi vedaldi@robots.ox.ac.uk \n\nUniversity of Oulu\nFinland\n\n\nUniversity of Oxford\n\n\nFine-Grained Visual Classification of Aircraft\n\nThis paper introduces FGVC-Aircraft, a new dataset containing 10,000 images of aircraft spanning 100 aircraft models, organised in a three-level hierarchy. At the finer level, differences between models are often subtle but always visually measurable, making visual recognition challenging but possible. A benchmark is obtained by defining corresponding classification tasks and evaluation protocols, and baseline results are presented. The construction of this dataset was made possible by the work of aircraft enthusiasts, a strategy that can extend to the study of number of other object classes. Compared to the domains usually considered in fine-grained visual classification (FGVC), for example animals, aircraft are rigid and hence less deformable. They, however, present other interesting modes of variation, including purpose, size, designation, structure, historical style, and branding.\n\nIntroduction\n\nIn this paper, we introduce FGVC-Aircraft, a novel dataset aimed at studying the problem of fine-grained recognition of aircraft models (Fig. 1, Sect. 2). The new data includes 10,000 airplane images spanning 100 different models, organised in a hierarchical manner. All models are visually distinguishable, even though in many cases the differences are subtle, making classification challenging and interesting.\n\nAirplanes are an alternative to objects typically considered in fine-grained visual classification (FGVC) such as birds [5] and pets [2][3][4]. Compared to these domains, aircraft classification has several interesting aspects. First, aircraft designs vary significantly depending on the plane size (from home-built to large carriers), designation (private, civil, military), purpose (transporter, carrier, training, sport, fighter, etc.), and technological factors such as propulsion (glider, propellor, jet). Overall, thousands of different airplane models exist or have existed. An interesting mode of variation, which is is not shared with categories such as animals, is the fact that the structure of aircraft can change with their design. For example, the number of wings, undercarriages, wheels per undercarriage, engines, etc. varies. Second, the aircraft designs exhibit systematic historical variations in their style. Thirdly, the same aircraft models are used by different airliner companies, resulting in variable livery branding. Finally, aircraft are largely rigid objects, reducing the impact of deformability on classification performance, and allowing one to focus on the other aspects of FGVC.\n\nOur contributions are three-fold: (i) we introduce a new large dataset of aircraft images with detailed model annotations; (ii) we describe how this data was collected using on-line resources and the work of hobbyists and enthusiasts -a method that may be applicable to other object classes; and (iii) we present baseline results on aircraft model identification. Sect. 2 describes the content of FGVC-Aircraft, including task definitions and evaluation protocols, Sect. 3 the dataset construction, Sect. 4 examines the performance of a baseline classifier on the data, and Sect. 5 summarises the contributions, giving further details on the data usage policy.\n\n2. The dataset: content, tasks, and evaluation FGVC-Aircraft contains 10,000 images of airplanes annotated with the model and bounding box of the dominant aircraft they contain. Aircraft models are organised in a four-level hierarchy, of which only the last three levels are of interest here.\n\n\u2022 Model. This is the most specific class label, such as Boeing 737-76J. This level is not considered meaningful for FGVC as differences between models may not be visually measurable, at least given an image of the exterior of the aircraft.   The list of model variants and corresponding example images are given in Fig. 1 and the hierarchy is given in Fig. 2.\n\nFGVC-Aircraft contains 100 example images for each of the 100 model variants. The image resolution is about 1-2 Mpixels. Image quality varies as images were captured in a span of decades, but it is usually very good. The dominant aircraft is generally well centred, which helps focusing on fine-grained discrimination rather than object detection. Images are equally divided into training, validation, and test subsets, so that each subset contains either 33 or 34 images for each variant. Algorithms should be designed on the training and validation subsets, and tested just once on the test subset to avoid over fitting.\n\nBounding box information can be used for training the aircraft classifiers, but should not be used for testing.\n\nWe define three tasks: aircraft variant recognition, aircraft family recognition, and aircraft manufacturer recognition. The performance is evaluated as class-normalised average classification accuracy, obtained as the average of the diagonal elements of the normalised confusion matrix. Formally, let y i \u2208 {1, . . . , M } the ground truth label for image i = 1, . . . , N (where N = 10, 000 and M = 100 for variant recognition). Let\u0177 i be the label estimated automatically. The entry C pq of the confusion matrix is given by\nC pq = |{i :\u0177 i = q \u2227 y i = p}| |{i : y i = p}|\nwhere | \u00b7 | denotes the cardinality of a set. The classnormalised average accuracy is then M p=1 C pp /M . The dataset is made publicly available for research purposes only at http://www.robots.ox. ac.uk/\u02dcvgg/data/fgvc-aircraft/. Please note (Sect. 3.1) that the data contains images that were generously made available for research purposes by several photographers; however, these images should not be used for any other purpose without obtaining prior and explicit consent by the respective authors (see Sect. 5.1 for further details).  Authorship information is contained in a banner at the bottom of each image (20 pixels high). Do not forget to remove this banner before using the images in experiments.\n\n\nDataset construction\n\nIdentifying the detailed model of an aircraft from an image is challenging for anyone but aircraft experts, and collecting 10,000 such annotations is daunting in general. Sect. 3.1 explains how leveraging aircraft data collected by aircraft spotters was the key in the construction of FGVC-Aircraft. However, collecting data from a restricted number of sources presents its own challenges. Sect. 3.2 introduces a notion of diversity and applies it to select a subset of the data that is maximally uncorrelated. Sect. 3.3 explains how bounding box annotations were crowdsourced using Amazon Mechanical Turk, and Sect. 3.4 how the hierarchical labels were obtained.\n\n\nInitial data collection\n\nEnthusiasts, collectors, and other hobbyists may be an excellent source of annotated visual data. In particular, data obtained from aircraft spotters was instrumental in the construction of this FGVC-Aircraft. A large number of such annotated images is available online in Airliners.net (http://www.airliners.net/), a repository of air-craft spotting data (similar collections exists, for example, for cars and trains). While using such images for research purposes may be considered fair use, nevertheless we found appropriate to ask for explicit permission to the photographers due to the large quantity of data involved. Of about twenty photographers that were contacted, permission to use the data for research purposes was granted by about ten of them (Sect. 5.1), and an explicit negative answer was received only from two of them. FGVC-Aircraft contains data only from the photographers that explicitly made their pictures available (see Sect. 2 and Sect. 5.1 for further details).\n\nAbout 70,000 images were downloaded from the ten photographers, resulting in images spanning thousands of different aircraft models. Even after grouping these models into variants, there was still a very large number of different classes, with a very skewed distribution. Popular families such as Airbus and Boeing included thousand of images per model variant, whereas rarer models counted only a dozen images. The 100 most frequent variants were retained, resulting in at least 120 images per variant.\n\n\nDiversity maximisation\n\nOne drawback of relying on a small set of photographers is that unwanted correlation may be introduced in the data. While these photographers tend to be active in the span of several years, it is natural to expect at least regional de- pendencies (for example certain airliners may fly more frequently to certain airports). Therefore, the data was first filtered to maximise internal diversity. Each pair of images for a given variant was compared based on photographer, time, airliner, and airport, obtaining an \"a priori\" similarity score (i.e., without looking at the pictures). Then, 100 images per variant were incrementally and greedily selected in order of decreasing diversity to the images already added to the collection. After doing so, images were randomly assigned to the training, validation, and test subsets. This simple procedure was effective at reducing internal correlation in the data, as reflected by a substantial reduction of the classification performance of baseline classifiers. In particular, sequences of photos are broken whenever possible.\n\nIsolating different photographers in different splits was also considered as an option, but ultimately it was rejected due to the complex dependency structure that such a choice would have introduced in the data.\n\n\nBounding boxes\n\nAbout 110 images were initially selected for each variant and submitted to Amazon Mechanical Turk for bounding box annotation. Annotators were instructed to skip images that did not contain the exterior of an aircraft, so that these images could be identified and discarded. Three annotations were collected for each image, presenting annotators with batches of 10 images at a time and paying 0.03 USD per batch. Overall, the cost of annotating all the images was 110 USD and annotations were complete in less than 48 hours. Out of three annotations, we sought at least two whose overlap over union similarity score was above 0.85% (fairly restrictive in practice), discarding other annotations. The remaining annotations were then averaged to obtain the final bounding box, and images without a bounding box (usually due to a problematic image) were discarded. Since slightly more than 100 images were submitted for annotation, this eventually resulted in a sufficient number of validated images.\n\n\nHierarchy\n\nThe hierarchy (Fig. 2) was obtained largely by manual inspection. Fortunately, sorting models by name is very likely to suggest possible merges in a straightforward way. These were verified manually by searching example images, Wikipedia, and the manufacturer websites for clear evidence that two model would differ visually. If no evidence was found, then the two models were merged in a variant. Sometimes, differences are fairly subtle; for example, Boeing variants -200, -300, -400, . . . differ mostly in length, an attribute that is difficult to estimate from monocular images (in this case counting windows may be the best way of telling a model from another).\n\n\nBaselines\n\nWe consider the classification tasks given in Sect. 2. For example, the variant classification for our dataset is a 100way binary classification problem and performance is measured in term of class-normalised average accuracy as described earlier. Fig. 3 shows the confusion matrix for a strong baseline model (non-linear SVM on a \u03c7 2 kernel, bag-of-visual words, 600 k-means words dictionary, multi-scale dense SIFT features, and 1 \u00d7 1, 2 \u00d7 2 spatial pyramid [1]). These models were trained on the entire image ignoring the bounding box information. As seen in Tab. 1 the performance is quite good for a few relatively distinctive categories (e.g., the \"Eurofighter Typhoon\" has error of just 5.9%). On the other hand, bag-of-visual-words is much worse at picking up subtle variations, such as for Airbus or Boeing family, resulting in large intra-family confusion (Fig. 3). The overall accuracy of the classifier is 48.69%. Fig. 4 shows the accuracy of the classifier when measured on the hierarchical label classification tasks. The accuracy for the variant classification is 58.48%, whereas, the accuracy for manufacturer classification is 71.30%. At the top level the two manufacturers, Boeing and Airbus, are most confused with one another perhaps due to the similar kinds of aircrafts they manufacture -large passenger planes catering to airliners. Note that for the hierarchical evaluation we trained our models for the variant classification task and simply measured the performance at different levels of the hierarchy by merging the labels below. An alternative strategy, which is to train the models directly for the labels at a given level in the hierarchy, performed significantly worse in our experiments.\n\n\nSummary\n\nWe have introduced FGVC-Aircraft, a new large dataset of aircraft images for fine-grained visual categorisation. The data contains 10,000 images, 100 airplane model variants, 70 families, and 30 manufacturers. We believe that FGVC-Aircraft has the potential of introducing aircraft recognition as a novel domain in FGVC to the wider computer vision community (FGVC-Aircraft will be part of the ImageNet 2013 FGVC challenge). Compared to other classes used frequently in FGVC, aircraft have different and interesting modes of variation.\n\nImages in FGVC-Aircraft were obtained from aircraft spotter collections, maximising internal diversity in order to reduce unwanted correlation between images taken by a limited number of photographers; in the future, we plan to substantially increase the size of the FGVC-Aircraft dataset by including more models as more and more photographers provide permission to use their photos, and apply the same construction to other object categories as well.\n\n\nAcknowledgments\n\nThe creation of this dataset started during the Johns Hopkins CLSP Summer Workshop 2012, Towards a Detailed Understanding of Objects and Scenes in Natural Images 1 with, in alphabetical order, Matthew B. Blaschko, Ross B. Girshick, Juho Kannala, Iasonas Kokkinos, Siddharth Mahendran, Subhransu Maji, Sammy Mohamed, Esa Rahtu, Naomi Saphra, Karen Simonyan, Ben Taskar, Andrea Vedaldi, and David Weiss. The CLSP workshop was supported by the National Science Foundation via Grant No 1005411, the Office of the Director of National Intelligence via the JHU Human Language Technology Center of Excellence; and Google Inc. A special thanks goes to Pekka Rantalankila for helping with the creation of the airplane hierarchy.\n\nMany thanks to the photographers that kindly made available their images for research purposes. These are, in alphabetical order, Mick Bajcar, Aldo Bidini, Wim Callaert, Tommy Desmet, Thomas Posch, James Richard Covington, Gerry Stegmeier, Ben Wang, Darren Wilson and Konstantin von Wedelstaedt. Please note that images are made available exclusively for non-commercial research purposes. The original authors retain the copyright on the respective pictures and should be contacted for any other usage of them. Photographers may be contacted through their http://www.airliners.net profile pages, which are linked from http://www.robots. ox.ac.uk/\u02dcvgg/data/fgvc-aircraft/. \n\nFigure 1 .\n1Our dataset contains 100 variants of aircrafts shown above. These are also annotated with their family and manufacturer, as well as bounding boxes. more substantial. The goal of this level is to create a classification task of intermediate difficulty. For example, the family Boeing 737 contains variants 737-200, 737-300, . . . , 737-900. The dataset contains 70 families. \u2022 Manufacturer. A manufacturer is a grouping of families produced by the same company. For example, Boeing contains the families 707, 727, 737, . . . . The dataset contains airplanes made by 30 different manufacturers.\n\nFigure 2 .\n2Label hierarchy shown as the manufacturer, family and the variant. Our dataset contains aircrafts of 100 different variants grouped under 70 families and 30 manufacturers.\n\nFigure 3 .\n3Confusion matrix for the 100 variant classification challenge. Some high confusion, due to the similarity of the models are also shown. These correspond to the Boeing 737 family, Boeing 747 family, Airbus family, McDonnell Douglas (MD) and the Embraer family. The average diagonal accuracy is 48.69%. Confusion matrix: Family classification (58.48 % accuracy)\n\n\nVariant. Model variants are the finer distinction levelthat are visually detectable, and were obtained by merg-\ning visually indistinguishable models. For example, the \nvariant Boeing 737-700 includes 87 models such as 737-\n7H4, 737-76N, 737-7K2, etc. The dataset contains 100 \nvariants. \n\u2022 Family. Families group together model variants that dif-\nfer in subtle ways, making differences between families \n707\u2212320 \n\n727\u2212200 \n737\u2212200 \n737\u2212300 \n737\u2212400 \n737\u2212500 \n737\u2212600 \n737\u2212700 \n737\u2212800 \n737\u2212900 \n\n747\u2212100 \n747\u2212200 \n747\u2212300 \n747\u2212400 \n757\u2212200 \n757\u2212300 \n767\u2212200 \n767\u2212300 \n767\u2212400 \n777\u2212200 \n\n777\u2212300 \nA300B4 \nA310 \nA318 \nA319 \nA320 \nA321 \nA330\u2212200 \nA330\u2212300 \nA340\u2212200 \n\nA340\u2212300 \nA340\u2212500 \nA340\u2212600 \nA380 \nATR\u221242 \nATR\u221272 \nAn\u221212 \nBAE 146\u2212200 \nBAE 146\u2212300 \nBAE\u2212125 \n\nBeechcraft 1900 \nBoeing 717 \nC\u2212130 \nC\u221247 \nCRJ\u2212200 \nCRJ\u2212700 \nCRJ\u2212900 \nCessna 172 \nCessna 208 \nCessna 525 \n\nCessna 560 \nChallenger 600 \nDC\u221210 \nDC\u22123 \nDC\u22126 \nDC\u22128 \nDC\u22129\u221230 \nDH\u221282 \nDHC\u22121 \nDHC\u22126 \n\nDHC\u22128\u2212100 \nDHC\u22128\u2212300 \nDR\u2212400 \nDornier 328 \nE\u2212170 \nE\u2212190 \nE\u2212195 \nEMB\u2212120 \nERJ 135 \nERJ 145 \n\nEmbraer Legacy 600 \nEurofighter Typhoon \nF\u221216A/B \nF/A\u221218 \nFalcon 2000 \nFalcon 900 \nFokker 100 \nFokker 50 \nFokker 70 \nGlobal Express \n\nGulfstream IV \nGulfstream V \nHawk T1 \nIl\u221276 \nL\u22121011 \nMD\u221211 \nMD\u221280 \nMD\u221287 \nMD\u221290 \nMetroliner \n\nModel B200 \nPA\u221228 \nSR\u221220 \nSaab 2000 \nSaab 340 \nSpitfire \nTornado \nTu\u2212134 \nTu\u2212154 \nYak\u221242 \n\n\n\n\nTable 1. Accuracy of variant prediction sorted according to the accuracy for each of the 100 variants in our dataset.Model \n\nAccuracy \nModel \nAccuracy \nModel \nAccuracy \nDR-400 \n94.1% \nDHC-8-100 \n57.6% \nERJ 135 \n35.3% \nEurofighter Typhoon \n94.1% \nEmbraer Legacy 600 \n57.6% \n747-100 \n33.3% \nF-16A/B \n90.9% \nF/A-18 \n57.6% \n747-300 \n33.3% \nCessna 172 \n88.2% \n757-300 \n54.5% \n767-200 \n33.3% \nSR-20 \n88.2% \n767-400 \n54.5% \n777-200 \n33.3% \nBAE-125 \n84.8% \nA340-500 \n54.5% \nBAE 146-200 \n33.3% \nDH-82 \n84.8% \nCessna 208 \n54.5% \nDC-10 \n33.3% \nTornado \n84.8% \nChallenger 600 \n54.5% \nDC-8 \n33.3% \nC-130 \n81.8% \nE-170 \n54.5% \nMD-87 \n33.3% \nHawk T1 \n81.8% \nGulfstream V \n54.5% \n737-500 \n32.4% \nModel B200 \n81.8% \nATR-42 \n51.5% \n727-200 \n30.3% \nDHC-1 \n78.8% \nCRJ-900 \n51.5% \nA300B4 \n30.3% \nIl-76 \n76.5% \nEMB-120 \n51.5% \nA330-300 \n30.3% \nAn-12 \n75.8% \nDC-3 \n50.0% \nE-190 \n29.4% \nFalcon 900 \n75.8% \nDHC-6 \n50.0% \nBAE 146-300 \n26.5% \nPA-28 \n75.8% \nTu-134 \n48.5% \n737-700 \n24.2% \nSpitfire \n70.6% \nGulfstream IV \n47.1% \nA340-300 \n24.2% \nDC-6 \n69.7% \nTu-154 \n47.1% \nMD-80 \n23.5% \nE-195 \n69.7% \n737-900 \n45.5% \nA310 \n21.2% \nCessna 560 \n67.6% \nFokker 100 \n42.4% \nA319 \n21.2% \nFokker 50 \n67.6% \nL-1011 \n42.4% \nA330-200 \n21.2% \nCessna 525 \n66.7% \nBoeing 717 \n41.2% \nC-47 \n21.2% \nGlobal Express \n66.7% \nCRJ-200 \n41.2% \n747-200 \n20.6% \nSaab 2000 \n66.7% \nDHC-8-300 \n39.4% \n737-200 \n17.6% \nYak-42 \n66.7% \nERJ 145 \n39.4% \n737-800 \n17.6% \nA318 \n64.7% \nATR-72 \n38.2% \n757-200 \n17.6% \nFalcon 2000 \n64.7% \n707-320 \n36.4% \nA320 \n15.2% \nMetroliner \n64.7% \n747-400 \n36.4% \n767-300 \n14.7% \nBeechcraft 1900 \n63.6% \nCRJ-700 \n36.4% \nDC-9-30 \n14.7% \nDornier 328 \n63.6% \nMD-11 \n36.4% \n737-400 \n12.1% \nFokker 70 \n63.6% \nMD-90 \n36.4% \nA321 \n11.8% \nSaab 340 \n63.6% \n777-300 \n35.3% \n737-300 \n06.1% \n737-600 \n57.6% \nA340-200 \n35.3% \nA380 \n57.6% \nA340-600 \n35.3% \nAverage \n48.69% \n\n\nhttp://www.clsp.jhu.edu/workshops/ archive/ws-12/groups/tduosn/.\n\nThe devil is in the details: an evaluation of recent feature encoding methods. K Chatfield, V Lempitsky, A Vedaldi, A Zisserman, Proc. BMVC. BMVCK. Chatfield, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil is in the details: an evaluation of recent feature encoding methods. In Proc. BMVC, 2011.\n\nNovel dataset for fine-grained image categorization. Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, Li Fei-Fei, CVPR Workshop on Fine-Grained Visual Categorization. Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization. In CVPR Workshop on Fine-Grained Visual Categorization, 2011.\n\nDog breed classification using part localization. J Liu, A Kanazawa, D Jacobs, P Belhumeur, Proc. ECCV. ECCVJ. Liu, A. Kanazawa, D. Jacobs, and P. Belhumeur. Dog breed classi- fication using part localization. In Proc. ECCV, 2012.\n\nCats vs dogs. O Parkhi, A Vedaldi, C V Jawahar, A Zisserman, Proc. CVPR. CVPRO. Parkhi, A. Vedaldi, C. V. Jawahar, and A. Zisserman. Cats vs dogs. In Proc. CVPR, 2012.\n\nThe caltech-ucsd birds-200-2011 dataset. C Wah, S Branson, P Welinder, P Perona, S Belongie, California Institute of TechnologyTechnical reportC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset. Technical report, California In- stitute of Technology, 2011.\n", "annotations": {"author": "[{\"end\":80,\"start\":50},{\"end\":93,\"start\":81},{\"end\":122,\"start\":94},{\"end\":156,\"start\":123},{\"end\":198,\"start\":157},{\"end\":220,\"start\":199},{\"end\":260,\"start\":221},{\"end\":289,\"start\":261},{\"end\":313,\"start\":290}]", "publisher": null, "author_last_name": "[{\"end\":64,\"start\":60},{\"end\":92,\"start\":85},{\"end\":103,\"start\":98},{\"end\":135,\"start\":128},{\"end\":173,\"start\":165},{\"end\":219,\"start\":214},{\"end\":235,\"start\":228}]", "author_first_name": "[{\"end\":59,\"start\":50},{\"end\":84,\"start\":81},{\"end\":97,\"start\":94},{\"end\":127,\"start\":123},{\"end\":164,\"start\":157},{\"end\":204,\"start\":199},{\"end\":213,\"start\":205},{\"end\":227,\"start\":221}]", "author_affiliation": "[{\"end\":288,\"start\":262},{\"end\":312,\"start\":291}]", "title": "[{\"end\":47,\"start\":1},{\"end\":360,\"start\":314}]", "venue": null, "abstract": "[{\"end\":1259,\"start\":362}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1812,\"start\":1809},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1825,\"start\":1822},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1828,\"start\":1825},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1831,\"start\":1828},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11935,\"start\":11932}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":16212,\"start\":15607},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16397,\"start\":16213},{\"attributes\":{\"id\":\"fig_3\"},\"end\":16770,\"start\":16398},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":18135,\"start\":16771},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":19971,\"start\":18136}]", "paragraph": "[{\"end\":1687,\"start\":1275},{\"end\":2901,\"start\":1689},{\"end\":3563,\"start\":2903},{\"end\":3857,\"start\":3565},{\"end\":4218,\"start\":3859},{\"end\":4842,\"start\":4220},{\"end\":4955,\"start\":4844},{\"end\":5483,\"start\":4957},{\"end\":6241,\"start\":5532},{\"end\":6929,\"start\":6266},{\"end\":7945,\"start\":6957},{\"end\":8450,\"start\":7947},{\"end\":9547,\"start\":8477},{\"end\":9761,\"start\":9549},{\"end\":10777,\"start\":9780},{\"end\":11458,\"start\":10791},{\"end\":13192,\"start\":11472},{\"end\":13739,\"start\":13204},{\"end\":14193,\"start\":13741},{\"end\":14932,\"start\":14213},{\"end\":15606,\"start\":14934}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":5531,\"start\":5484}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1273,\"start\":1261},{\"attributes\":{\"n\":\"3.\"},\"end\":6264,\"start\":6244},{\"attributes\":{\"n\":\"3.1.\"},\"end\":6955,\"start\":6932},{\"attributes\":{\"n\":\"3.2.\"},\"end\":8475,\"start\":8453},{\"attributes\":{\"n\":\"3.3.\"},\"end\":9778,\"start\":9764},{\"attributes\":{\"n\":\"3.4.\"},\"end\":10789,\"start\":10780},{\"attributes\":{\"n\":\"4.\"},\"end\":11470,\"start\":11461},{\"attributes\":{\"n\":\"5.\"},\"end\":13202,\"start\":13195},{\"attributes\":{\"n\":\"5.1.\"},\"end\":14211,\"start\":14196},{\"end\":15618,\"start\":15608},{\"end\":16224,\"start\":16214},{\"end\":16409,\"start\":16399}]", "table": "[{\"end\":18135,\"start\":16828},{\"end\":19971,\"start\":18255}]", "figure_caption": "[{\"end\":16212,\"start\":15620},{\"end\":16397,\"start\":16226},{\"end\":16770,\"start\":16411},{\"end\":16828,\"start\":16773},{\"end\":18255,\"start\":18138}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":1425,\"start\":1411},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4180,\"start\":4174},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":4217,\"start\":4211},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10813,\"start\":10805},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11726,\"start\":11720},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12346,\"start\":12338},{\"end\":12404,\"start\":12398}]", "bib_author_first_name": "[{\"end\":20118,\"start\":20117},{\"end\":20131,\"start\":20130},{\"end\":20144,\"start\":20143},{\"end\":20155,\"start\":20154},{\"end\":20401,\"start\":20395},{\"end\":20420,\"start\":20410},{\"end\":20446,\"start\":20438},{\"end\":20454,\"start\":20452},{\"end\":20757,\"start\":20756},{\"end\":20764,\"start\":20763},{\"end\":20776,\"start\":20775},{\"end\":20786,\"start\":20785},{\"end\":20953,\"start\":20952},{\"end\":20963,\"start\":20962},{\"end\":20974,\"start\":20973},{\"end\":20976,\"start\":20975},{\"end\":20987,\"start\":20986},{\"end\":21149,\"start\":21148},{\"end\":21156,\"start\":21155},{\"end\":21167,\"start\":21166},{\"end\":21179,\"start\":21178},{\"end\":21189,\"start\":21188}]", "bib_author_last_name": "[{\"end\":20128,\"start\":20119},{\"end\":20141,\"start\":20132},{\"end\":20152,\"start\":20145},{\"end\":20165,\"start\":20156},{\"end\":20408,\"start\":20402},{\"end\":20436,\"start\":20421},{\"end\":20450,\"start\":20447},{\"end\":20462,\"start\":20455},{\"end\":20761,\"start\":20758},{\"end\":20773,\"start\":20765},{\"end\":20783,\"start\":20777},{\"end\":20796,\"start\":20787},{\"end\":20960,\"start\":20954},{\"end\":20971,\"start\":20964},{\"end\":20984,\"start\":20977},{\"end\":20997,\"start\":20988},{\"end\":21153,\"start\":21150},{\"end\":21164,\"start\":21157},{\"end\":21176,\"start\":21168},{\"end\":21186,\"start\":21180},{\"end\":21198,\"start\":21190}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13126996},\"end\":20340,\"start\":20038},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3181866},\"end\":20704,\"start\":20342},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2643216},\"end\":20936,\"start\":20706},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":194032023},\"end\":21105,\"start\":20938},{\"attributes\":{\"id\":\"b4\"},\"end\":21413,\"start\":21107}]", "bib_title": "[{\"end\":20115,\"start\":20038},{\"end\":20393,\"start\":20342},{\"end\":20754,\"start\":20706},{\"end\":20950,\"start\":20938}]", "bib_author": "[{\"end\":20130,\"start\":20117},{\"end\":20143,\"start\":20130},{\"end\":20154,\"start\":20143},{\"end\":20167,\"start\":20154},{\"end\":20410,\"start\":20395},{\"end\":20438,\"start\":20410},{\"end\":20452,\"start\":20438},{\"end\":20464,\"start\":20452},{\"end\":20763,\"start\":20756},{\"end\":20775,\"start\":20763},{\"end\":20785,\"start\":20775},{\"end\":20798,\"start\":20785},{\"end\":20962,\"start\":20952},{\"end\":20973,\"start\":20962},{\"end\":20986,\"start\":20973},{\"end\":20999,\"start\":20986},{\"end\":21155,\"start\":21148},{\"end\":21166,\"start\":21155},{\"end\":21178,\"start\":21166},{\"end\":21188,\"start\":21178},{\"end\":21200,\"start\":21188}]", "bib_venue": "[{\"end\":20183,\"start\":20179},{\"end\":20814,\"start\":20810},{\"end\":21015,\"start\":21011},{\"end\":20177,\"start\":20167},{\"end\":20515,\"start\":20464},{\"end\":20808,\"start\":20798},{\"end\":21009,\"start\":20999},{\"end\":21146,\"start\":21107}]"}}}, "year": 2023, "month": 12, "day": 17}