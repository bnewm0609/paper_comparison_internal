{"id": 260154775, "updated": "2023-10-04 21:46:34.255", "metadata": {"title": "QuIP: 2-Bit Quantization of Large Language Models With Guarantees", "authors": "[{\"first\":\"Jerry\",\"last\":\"Chee\",\"middle\":[]},{\"first\":\"Yaohui\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Volodymyr\",\"last\":\"Kuleshov\",\"middle\":[]},{\"first\":\"Christopher\",\"last\":\"Sa\",\"middle\":[\"De\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/jerry-chee/QuIP .", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2307.13304", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2307-13304", "doi": "10.48550/arxiv.2307.13304"}}, "content": {"source": {"pdf_hash": "56b828717f32251a5e0f0be9c0113077f23c8429", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2307.13304v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e23aaaa527be528fb5248cc1b23a1faa1eea9b0a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/56b828717f32251a5e0f0be9c0113077f23c8429.txt", "contents": "\nQuIP: 2-Bit Quantization of Large Language Models With Guarantees\n\n\nJerry Chee jerrychee@cs.cornell.edu \nDepartment of Computer Science\nDepartment of Electrical and Computer Engineering\nDepartment of Computer Science\nDepartment of Computer Science\nCornell University\nCornell University\nCornell University\nCornell University\n\n\nYaohui Cai \nDepartment of Computer Science\nDepartment of Electrical and Computer Engineering\nDepartment of Computer Science\nDepartment of Computer Science\nCornell University\nCornell University\nCornell University\nCornell University\n\n\nVolodymyr Kuleshov kuleshov@cornell.edu \nDepartment of Computer Science\nDepartment of Electrical and Computer Engineering\nDepartment of Computer Science\nDepartment of Computer Science\nCornell University\nCornell University\nCornell University\nCornell University\n\n\nChristopher De Sa \nDepartment of Computer Science\nDepartment of Electrical and Computer Engineering\nDepartment of Computer Science\nDepartment of Computer Science\nCornell University\nCornell University\nCornell University\nCornell University\n\n\nQuIP: 2-Bit Quantization of Large Language Models With Guarantees\n\nThis work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre-and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found on GitHub.Preprint. Under review. arXiv:2307.13304v1 [cs.LG] 25 Jul 2023 pre-and post-processing that ensures that the Hessian matrices are incoherent by multiplying them by a Kronecker product of random orthogonal matrices. We denote \"incoherence processing\" as both the pre-and post-processing steps of our procedure.We complement our method with a theoretical analysis-the first for a quantization algorithm that scales to LLM-sized models-which analyzes the role of incoherence and shows that our quantization procedure is optimal within a general class of rounding methods. Interestingly, we find that QuIP without incoherence processing yields a more efficient implementation of an earlier algorithm, OPTQ [8]; our paper thus also provides the first theoretical analysis for that method.Empirically, we find that incoherence processing greatly improves the quantization of large models, especially at higher compression rates, and yields the first LLM quantization method that produces viable results using only two bits per weight. For large LLM sizes (>2B parameters), we observe small gaps between 2-bit and 4-bit compression that further decrease with model size, hinting at the feasibility of accurate 2-bit inference in LLMs.Contributions. In summary, this paper makes the following contributions: (1) we propose QuIP, a quantization method based on the insight that model parameters should ideally be incoherent; (2) we provide a theoretical analysis for a broad class of adaptive rounding methods that encompass QuIP and OPTQ; (3) we demonstrate that QuIP makes two-bit LLM compression viable for the first time.Related WorkAdaptive rounding. Nagel et al.[20]are the first to motivate the \"adaptive rounding\" proxy objective (Eq. (1)) in a principled way. There are many quantization methods which quantize by optimizing this proxy objective [5, 6, 9, 12, 14,20,31]. Many require further retraining which can be expensive, and are not evaluated on the current largest open LLMs (OPT [34], BLOOM [29]). Lybrand and Saab [15]propose a greedy per-neuron quantization procedure that is similar to ours, except they do not consider arbitrary linear functions of the error correction. Their work bounds the proxy objective, albeit on the first layer only.Post training quantization in large models. There is a growing body of work on PTQ in LLMs such as OPT and BLOOM. The size of these models make it difficult to apply previously developed methods. The majority of these methods make quantization easier by somehow reducing the range of weights or activations, but still use nearest rounding. SmoothQuant [30]  rescales between activations and weights to remove outliers from the activations and make quantization overall easier. ZeroQuant [32] proposes a per-layer knowledge distillation method. LLM.int8() [4] decompose matrix multiplications into a majority of 8 bit and a minority of 16 bit operations.  designs kernels to accelerate quantized matrix multiplications. RPTQ [33] reorders activations and quantizes them in groups, reducing the impact of range differences between channels.OPTQ (Formerly known as GPTQ). OPTQ [8] is based on OBQ [7], and proposes a novel rounding method that can work on the largest OPT and BLOOM models. The method works iteratively over the weight columns in a fixed order: (1) quantize with nearest rounding and compute the error, (2) update the remaining weights with a scaled error, and (3) repeat.Other quantization methods. There are other quantization procedures which do not round based on the proxy objective of [20], or are not designed for the largest language models [10, 11, 13, 19,27,28].Quantization With Incoherence Processing: Adaptive Rounding StepThis section introduces quantization with incoherence processing (QuIP), a new method consisting of:(1) an adaptive rounding step; (2) efficient pre-and post-processing that ensures weight and Hessian incoherence. We define and analyze step (1) in this section; the next section focuses on step (2).Following existing state-of-the-art post-training quantization methods, we round weights per-layer by minimizing the \"adaptive rounding\" proxy objective, as in Nagel et al.[20],(1) m 4 tr(D) = L worst (LDLQ, H) \u2264 L worst (A, H) and m c tr(D) = L avg (LDLQ, H) \u2264 L avg (A, H), where D is the matrix from the LDL decomposition of H, and c = 12 for nearest, c = 6 for stochastic.Remarks. The number of rows being quantized is m, and each quantization method operates across the n entries of each row. For all rounding methods described by Eq. (2), and for all positive semidefinite H, Q as nearest rounding achieves the same worst-case proxy loss as stochastic rounding, but achieves better average proxy loss.\n\nIntroduction\n\nLarge language models (LLMs) have enabled advances in text generation, few-shot learning, reasoning, protein sequence modeling, and other tasks [2, 29,34]. The massive size of these models-often reaching into hundreds of billions of parameters-requires sophisticated deployment methods and motivates research into efficient inference algorithms.\n\nThis work studies the post-training quantization of LLM parameters as a way to improve their runtime efficiency [4,8,22,30,32,33]. Our key insight is that quantization can be most effective when weight and proxy Hessian matrices are incoherent-intuitively, both the weights themselves and the directions in which it is important to have good rounding accuracy are not too large in any one coordinate, which makes it easier to adaptively round the weights to a finite set of compressed values. We use this intuition to develop theoretically sound two-bit quantization algorithms that scale to LLM-sized models.\n\nSpecifically, we introduce quantization with incoherence processing (QuIP), a new method motivated by the above insight. QuIP consists of two steps: (1) an adaptive rounding [20] procedure, which minimizes a quadratic proxy objective (\u0174 ) = tr((\u0174 \u2212 W )H(\u0174 \u2212 W ) T ) of the error between the original weights W and the quantized weights\u0174 using an estimate of the Hessian H; (2) efficient Here, W \u2208 R m\u00d7n is the original weight matrix for a given linear layer,\u0174 \u2208 R m\u00d7n are the quantized weights, x \u2208 R n is an input vector drawn uniformly at random from a calibration set, and H is the second moment matrix of these vectors, interpreted as a proxy Hessian. Crucially, this formulation lets the quantization be run in parallel across neurons, which is tractable for large language models [8]. For simplicity, we will focus in this section on rounding to the integers; subsequent sections will extend the analysis to finite grids.\n\n\nLDLQ: An Optimal Adaptive Rounding Method\n\nOur strategy is to define a family of adaptive rounding methods for optimizing objective (1) and then define LDLQ, the optimal method within that class. Our defined methods iteratively perform the following update for k = 1, 2, ..., n:\nW k = Q(W k + (W 1:(k\u22121) \u2212\u0174 1:(k\u22121) )a k ),\nwhere W k denotes the k-th column, W 1:(k\u22121) denotes the first k \u2212 1 columns, the subroutine Q denotes either nearest rounding or standard unbiased rounding to the integers (which rounds up or down such that E [Q(z)] = z), and a k \u2208 R k\u22121 is some sequence of vectors. This scheme rounds columns one at a time; at each step, it adds a \"correction\" term that is a linear function of the residual from the rounding we have done so far. The final\u0174 satisfies the following matrix equation:\nW = Q(W + (W \u2212\u0174 )U ),(2)\nwhere U is a strictly upper-triangular matrix whose columns are the vectors a k and Q acts elementwise. Because U is upper-triangular,\u0174 k only depends on\u0174 1:(k\u22121) .\n\nIf we let \u03b7 = Q(W + (W \u2212\u0174 )U ) \u2212 (W + (W \u2212\u0174 )U ) denote the quantization error of Q, we find that\u0174 \u2212 W = \u03b7(U + I) \u22121 and we can rewrite objective (1) as\ntr((\u0174 \u2212 W )H(\u0174 \u2212 W ) T ) = tr(\u03b7(U + I) \u22121 H(U + I) \u2212T \u03b7 T ).(3)\nThe LDLQ Method How should we specify U , the linear feedback from the quantization error of preceding columns in (2)? Equation 3 provides an answer. If we choose U \u2190\u00d9 such that the LDL decomposition of H is H = (\u00d9 + I)D(\u00d9 + I) T ,\n\nwhere D is a (non-negative) diagonal matrix and\u00d9 is upper unit triangular, then the terms (U + I) in Eq.\n\n(3) cancel. We denote as LDLQ the rounding procedure in Eq.\n\n(2) with U \u2190\u00d9 as the LDL assignment from Eq. (4). We will now see that the LDL assignment of U is in fact optimal.\n\n\nDeriving the Optimality of the LDLQ Adaptive Rounding Procedure\n\nIn order to reason about optimality, we consider weights which are worst and average-case for the proxy loss. Let A denote a rounding method, and let A(W, H) be the resulting quantized weights. Define the worst-case (L worst ) and average (L avg ) proxy losses with respect to the input weights as \n\nTheorem 1. LDLQ is worst and average-case optimal amongst rounding methods which specify the linear feedback U as a function of H (not of W ), and when rounding to the integers. That is, for all rounding methods A in the class described by Eq.\n\n(2), for all positive semi-definite H, and for Q as either nearest or stochastic rounding,\n\nMoving beyond a generic algorithm A within our framework, we consider the common baselines of nearest and stochastic rounding. These methods are represented within our framework by choosing the appropriate Q subroutine, and setting all entries of the linear feedback to zero. For these baseline methods, their optimality gap to LDLQ is governed by tr (D) vs. tr (H). For any non-diagonal H 0, LDLQ achieves strictly lower worst and average-case proxy loss because tr (D) < tr(H).  Theorem 1 gives exact expressions for the proxy loss, albeit with tr (D), which can be difficult to reason about. In Figure 1, we empirically observe that H is approximately low-rank: we visualize the spectrum of several randomly chosen H from OPT-2.7b, and observe that the spectrum decays rapidly. In fact, across all layers of OPT-125m to 2.7b models, a vast majority of H matrices have fewer than a quarter of eigenvalues > 1% of the max eigenvalue; see Supplement E for full details. Given this observation about the low rank of H, can we bound the behavior of LDLQ, and thus tr (D), using the spectrum of H?\n\nWe do this building on a variant of the incoherence assumption that is specialized to our case [3,24]. Definition 1. We say a symmetric Hessian matrix H \u2208 R n\u00d7n is \u00b5-incoherent if it has an eigendecomposition H = Q\u039bQ T such that for all i and j, |Q ij | = e T i Qe j \u2264 \u00b5/ \u221a n. By extension, we say a weight matrix W \u2208 R m\u00d7n is \u00b5-incoherent if all i and j, |W ij | = e T i W e j \u2264 \u00b5 W F / \u221a mn.\n\nNote that \"most\" n \u00d7 n matrices are incoherent with \u00b5 = O( \u221a log n) =\u00d5(1) because a random orthogonal matrix has entries with squared-magnitudes that concentrate around their mean of 1/n. Wanting W to be incoherent is very natural: a small bound on the magnitude of its entries means that we do not need to scale it as much to make it fit in the finite range of representable low-precision numbers. Making H incoherent is less intuitive, but its utility is motivated by the following lemma. Lemma 2. Let H \u2208 R n\u00d7n be a \u00b5-incoherent positive semi-definite symmetric matrix and let H = (\u00d9 + I)D(\u00d9 + I) T be its LDL Cholesky decomposition, where\u00d9 is a strictly upper triangular matrix and D is a (non-negative) diagonal matrix. Then, tr (D) \u2264 \u00b5 2 n tr H 1/2 2 .\n\nTo the best of our knowledge, this is a novel result using incoherence to obtain a bound on tr (D) that depends only on the spectrum of H. where B \u2208 {Near, Stoch}, and c is as given in Theorem 1. This shows that for sufficiently low-rank H, LDLQ is asymptotically better than plain nearest and stochastic rounding by a factor of \u00b5 2 k/n.\n\nWithout incoherence: no improvement with a spectral bound. By assuming incoherence, we were able to show LDLQ gets an asymptotically better bound in terms of just the spectrum of H.\n\n\nAlgorithm 1 QuIP -Incoherence Pre-Processing\n\nRequire: b \u2208 N, H \u2208 R n\u00d7n SPD, original W \u2208 R m\u00d7n , \u03c1 \u2208 R + , \u03b1 \u2208 [0, 1] 1: seeded sample random two-factor orthogonal matrices U \u2208 R m\u00d7m and V \u2208 R n\u00d7n 2: H = H + \u03b1 * mean(diag(H))I from OPTQ 3:\nD \u2190 4 diag(H)/ diag(W T W ) 4 \u221a applies element-wise 4: W \u2190 WD; H \u2190D \u22121 HD \u22121 diagonal rescaling 5: W \u2190 U W V T ; H \u2190 V HV T incoherence 6: s \u2190 \u03c1 W F / \u221a mn; W \u2190 1 2 ( 1 s W + 1) reduced quantization range due to incoherency 7: return W \u2190 clamp(W * (2 b \u2212 1), 0, 2 b \u2212 1) rescale W to lie within [0, 2 b \u2212 1]\nAlgorithm 2 QuIP -Incoherence Post-Processing\nRequire: b \u2208 N, H \u2208 R n\u00d7n SPD, quantized W \u2208 [0, 2 b \u2212 1] m\u00d7n , s \u2208 R &D \u2208 R n\u00d7n (Alg 1) 1: seeded sample random two-factor orthogonal matrices U \u2208 R m\u00d7m and V \u2208 R n\u00d7n 2: W \u2190 s * (W/(2 b \u2212 1)) * 2 \u2212 1 3: W \u2190 U T W V ; H \u2190 V T HV revert incoherence 4: return W \u2190 WD \u22121 revert diagonal rescaling\nWe might ask: was the incoherence assumption necessary to get this result? The following theorem answers this question in the affirmative by showing that without incoherence, the best spectral bound for LDLQ cannot differentiate it from the nearest and stochastic rounding baselines. \nL worst (LDLQ,H) = L worst (Stoch, H) = m 4 tr (H) .\nOn the average-case loss LDLQ achieves the same error as the corresponding rounding routine. Let B = {Near, Stoch} and c = 12 for nearest, c = 6 for stochastic. Note that the worst case for comparing LDLQ against these baselines occurs when H is diagonal, see Theorem 1 and Lemma 3. Assuming incoherence as we do is a natural way to exclude such cases.\n\n\nQuantization With Incoherence Processing: Incoherence Processing Step\n\nNext, we leverage the above incoherence analysis to introduce incoherence processing, the second step of the QuIP algorithm. Our strategy will be to pre-process weight and Hessian matrices to ensure the favorable incoherence properties outlined above. One straightforward way to make a symmetric matrix incoherent is to conjugate it by a uniform random orthogonal matrix: this will result in each of its eigenvalues being a random unit vector, whose entries will concentrate around magnitude n \u22121/2 .\n\nSpecifically, let U \u2208 R m\u00d7m and V \u2208 R n\u00d7n be two random orthogonal matrices. (Let's temporarily ignore how these matrices are generated, or how we would efficiently perform inference.) We ensure the weight and Hessian are incoherent with high probability through random orthogonal multiplicationsH \u2190 V HV T andW \u2190 U W V T . Importantly, this transformation preserves the proxy quadratic form since tr(WHW T ) = tr((U W V T )(V HV T )(V W T U T )) = tr(W HW T ).\n\n\nIncoherence via Efficient Orthogonal Multiplication\n\nIf all we wanted to do was to store or transmit the weights of the quantized neural network, the above procedure would introduce no overhead, since we can generate a random orthogonal matrix from a seed-making it essentially free to store. However, for running inference on a DNN, we need to multiply by the weight matrix W , and here the need to manifest and multiply by n \u00d7 n random orthogonal matrices U, V would be prohibitive.\n\n\n5\n\nAlgorithm 3 QuIP: Quantization with Incoherence Processing\nRequire: b \u2208 N, H \u2208 R n\u00d7n SPD, W \u2208 R m\u00d7n , Q \u2208 {Near, Stoch}, \u03c1 \u2208 R + , \u03b1 \u2208 [0, 1] 1: LDL decomposition of H = (\u00d9 + I)D(\u00d9 + I) \u22121 2:\u0174 \u2190 Alg 1(b, H, W, \u03c1, \u03b1) QuIP Incoherence Pre-Procesing 3: for k \u2208 {1, . . . , n} do\u0174 k \u2190 clamp(Q(W k + (W \u2212\u0174 )\u00d9 k ), 0, 2 b \u2212 1) LDLQ 4: return\u0174 \u2190 Alg 2(b, H,\u0174 )\nQuIP Incoherence Post-Processing\n\nTo handle this, we propose to instead use a distribution over random orthogonal matrices for which multiplication is fast. Let n = pq be a factorization of n (where p \u2248 q \u2248 \u221a n), and set U = U L \u2297 U R where U L is sampled uniformly from the p \u00d7 p orthogonal matrices and U R is sampled uniformly from the q \u00d7 q orthogonal matrices. Multiplication of a vector x \u2208 R n by the matrix U can be accomplished by reshaping to a p \u00d7 q matrix, multiplying on the left by U L and the right by U T R , and then reshaping back: this takes O(n(p + q)) = o(n 2 ) operations. Using more than two factors in this way is also possible, but using two suffices to make this preprocessing asymptotically non-dominant. Lemma 5. Let H be a positive semi-definite matrix on R n\u00d7n and W a matrix on R m\u00d7n , and suppose that m = p 1 \u00b7 p 2 \u00b7 \u00b7 \u00b7 p k and n = q 1 \u00b7 q 2 \u00b7 \u00b7 \u00b7 q k . Let U 1 , U 2 , . . . , U k , V 1 , V 2 , . . . , V k be independent random orthogonal matrices on R pi\u00d7pi and R qi\u00d7qi respectively. Set U as the Kronecker product\nU = U 1 \u2297 U 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U k and V as V = V 1 \u2297 V 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 V k Then V HV T is \u00b5 H -incoherent with probability at least 1 \u2212 \u03b4, and U W V T is \u00b5 W -incoherent with probability at least 1 \u2212 \u03b4, where \u00b5 H = A k/2 log Ckn 2 \u03b4 k/2 =\u00d5 (1) and \u00b5 W = A k log 2Ckmn \u03b4 k =\u00d5 (1)\nfor some global constants A and C independent of n and k.\n\nRemarks. This lemma means that multiplying by a random matrix in this family suffices to make a matrix incoherent with parameter \u00b5 only poly-logarithmic in the matrix size.\n\n\nAdditional Heuristics\n\nWe outline QuIP pre-processing and post-processing in Algorithms 1 and 2, respectively. In line 5 of Algorithm 1, we apply the aforementioned fast orthogonal multiplication procedure to ensure W and H are incoherent. We also randomly permute entries at the fast matrix multiplication step to prevent any correlation between attention heads from worsening performance. We introduce a number of additional heuristic improvements that further improve performance.\n\nIncoherence-Based Heuristics. Line 4 diagonally rescales W and H to minimize (\u0174 ) \u2248 tr (H) W 2 F , effectively trading off the spectrum of these matrices to find a minimum. Motivated by the incoherence of W , Line 6 computes the quantization range depending on the spectrum W F , instead of the typical max i,j |W ij |. Our full QuIP procedure is described in Algorithm 3, which contains calls to the pre-and post-processing sub-steps in Algorithms 1 and 2.\n\nGreedy local search. Our basic procedure yields a good initial guess with error guarantees. We can further lower the proxy loss by running coordinate descent after LDLQ (but before post-processing), updating the weights in the same order as in the initial pass. See Supplement D for full details.\n\n\nExtensions and Further Analyses\n\n\nOPTQ is a Special Case of LDLQ\n\nWe prove a novel theoretical insight: QuIP without incoherence processing (i.e., LDLQ) is equivalent to a more efficient version of the OPTQ algorithm. That is, OPTQ falls under our class of adaptive rounding procedures with linear feedback, and is within-class optimal. Theorem 6. OTPQ [8] falls within the class of adaptive rounding procedures with linear feedback as described by Eq. (2), and is equivalent to LDLQ in Section 3.\n\n\nRemarks.\n\nTo the best of our knowledge, this equivalence yields the first theoretical analysis of OPTQ. Even though the two methods are equivalent, LDLQ is more efficient. OPTQ's implementation requires a matrix inversion of H, and two Cholesky decompositions. Our implementation of LDLQ performs no matrix inversion, and only one Cholesky decomposition.\n\nEmpirical Verification. The quantized outputs of the OPTQ implementation [8] are shown to be exactly identical to the outputs of our LDLQ implementation. Synthetic random data was used, with W \u223c Unif[0, 1] 1000\u00d71000 . Full details can be found in Supplement E. In Section 3, we saw that LDLQ (equivalently, OPTQ) is optimal for minimizing the adaptive rounding objective. However, this analysis assumed rounding to the integers. In practice, we do not want to round W just to the integers, but instead to scale it, shift it, and round it a finite subset corresponding to a b-bit integer. To do this, the \"real\" LDLQ algorithm uses a clamp operation to restrict the range of quantized values. Is LDLQ still optimal when this small change is made? It turns out that the answer is no, as the following concrete example illustrates.\n\n\nA Bound for Rounding to a Finite Grid\n\nFinite Grid Counterexample. Figure 2 illustrates the behavior of LDLQ and other rounding methods-when restricted via clamping to a finite 4-bit grid [0, 15]-on a particular example where H is a (cleverly chosen) small perturbation of (I n + 1 n\u00d7n \u2212 e n e T n )/n, and W has m = 16 and is a small perturbation of 1 m\u00d7n /2. Details of the setup appear in Supplement E. The figure shows that clamped LDLQ with nearest rounding is asymptotically worse, and the clamping to the finite grid is what causes it to be worse in this case.\n\nNote that in our experiments in practice, OPTQ has been shown to soundly beat nearest rounding. This clamping issue does not seem to arise in practice; however, since it is possible we do need to take it into account to prove useful end-to-end bounds.\n\nA Procedure With a Bound. In order to address the above issues in theory, here we describe a method that acts to restrict the value of |\u0174 ij \u2212 W ij |, so that the rounded weights will remain inside the grid if W is sufficiently far inside. We do this via the optimization problem with hyperparameter c minimize: tr HR T R over: R unit upper triangular (7)\n\nsubject to: e T i R T Re i \u2264 1 + c, \u2200i \u2208 {1, . . . , n}.\n\nOur \"fixed\" algorithm solves this convex problem (e.g. with ADMM), then runs QuIP using stochastic rounding and U = R \u22121 \u2212 I in place of the LDL decomposition. Observe that for sufficiently large c, this is exactly equivalent to base QuIP, since the solution of that optimization problem is given by the LDL decomposition when the constraint is dropped. Doing this (the full algorithm is given in the supplemental) yields the following theorem. Theorem 7. Suppose that we run Algorithm 4 (Supplement) to quantize a matrix W \u2208 R m\u00d7n by solving the objective (7). Then there exists an assignment of the algorithm's hyperparameters c and \u03c1 such that with probability at least 1 \u2212 \u03b4, all the quantized weights will be in range (no overflow or need for clipping) and\ntr (\u0174 \u2212 W )H(\u0174 \u2212 W ) T =\u00d5 1 n 2 4 b tr H 1/2 2 W 2 F .\nIn practice, because of the significant additional compute needed to solve this program, and because clamping rarely causes issues, we always just use QuIP as described in the previous sections, which is equivalent to setting c large and using nearest rounding.\n\n\nExperiments\n\nOverview. We quantize the OPT [34] family of models (up to 30B parameters) using various quantization and processing methods. QuIP is superior to OPTQ and other baselines across all model sizes and evaluation tasks. Most interestingly, incoherence processing yields excellent performance : Quantizing OPT models up to 30b parameters. Our method QuIP is the first PTQ procedure to achieve good quantization at 2 bits per weight, across a variety of model sizes and evaluation tasks.\n\nusing as little as two bits per weight when paired with any of the quantization methods we consider (including nearest rounding). Two-bit quantization with QuIP is viable at even moderate model sizes (1B parameters), a regime where other two-bit quantization methods fail. At the largest model sizes, the difference between 2-bit and 16-bit weight performance becomes small. We present additional results on the effectiveness of the proxy loss, unbiased rounding, and Algorithm 4 in Supplement E.\n\nSetup. The experimental infrastructure is built on top of OPTQ's [8] repository which is implemented in PyTorch [23]. We quantize the HuggingFace implementation of the OPT model family. All models are quantized on a single GPU, with up to 48GB of memory. Our calibration set is the same as OPTQ; 128 random 2048 token segments from the C4 dataset [25] consisting of generic text data from crawled websites. Therefore, no task-specific data is viewed when quantizing. Following OPTQ, quantization is performed one Transformer block at a time. A block is loaded into GPU memory, the Hessian computed, and then the weights quantized. The current block's inputs are then passed through the quantized block to produce inputs for the following block. The Hessian is computed from the quantized Transformer up to that point rather than from the full precision model; like OPTQ, we find this improves quantization. Further details on the setup can be found in Supplement E, including a description of the computational resources used to perform the experiments.\n\nMethods. We evaluate compositions of several quantization and pre/post processing methods. For quantization methods, we evaluate nearest rounding, LDLQ (or OPTQ), and two variations. LDLQ-RG re-orders the weights based on diag(H) to modify the quantization order and adds further greedy updates to the proxy. \"Greedy\" performs the greedy updates only. We evaluate the baseline preprocessing from OPTQ which adds H \u2190 H + \u03b1 * mean(diag(H))I for numerical stability. We also evaluate our incoherence processing in Algorithms 1 and 2, denoted as \"IncP\". With this notation QuIP = LDLQ + IncP, and QuIP-RG = LDLQ-RG + IncP.\n\nDatasets. We evaluate on the following language generation tasks: Main Results. QuIP is the first PTQ procedure to achieve good quantization at two bits per weight, across a variety of LLM sizes and evaluation tasks. In Figure 3 we compare QuIP and OPTQ when quantizing to 2 and 3 bits per weight (4-bit quantization works equally well for both methods); we evaluate OPT models (up to 30B) on PTB, C4, ARC Easy, and LAMBADA. QuIP is superior to OPTQ across the model sizes and evaluation tasks. At three bits, QuIP matches the full precision   Table 2: Ablating sub-steps of QuIP's incoherence processing, see Algorithm 1. Perplexities are averaged over WikiText2, PTB, and C4 for OPT-350m. model reasonably well. At two bits and for larger LLMs (>2B parameters), QuIP begins to approach the performance of the full precision model. As model size increases, so does the quality of QuIP's 2-bit quantization. We provide plots on the remaining datasets in Supplement E.\n\nIncoherence Processing Ablation. Table 1 shows all combinations of quantization and processing methods evaluated on OPT-30B. At lower weight bits, QuIP's incoherence processing dramatically improves the performance of all quantization methods, across all evaluation tasks. Remarkably, all quantization methods-even nearest-are viable at two bits with our incoherence processing. Our modifications in QuIP-RG sometimes give an improvement over QuIP, but further study is required to evaluate these modifications. Figures for OPT-125M to 13B are in Supplement E.\n\nFurther Ablation. QuIP's incoherence processing contains several sub-steps. Table 2 shows their relative contributions; all are necessary for the full improvement. Table 3 shows that the random permutation step within the fast orthogonal multiplication also significantly reduces perplexity.\n\n\nConclusion\n\nWbits \u2206Perplexity from random permute\u2193 4 -0.22 3 -9.96 2 -74.2 Table 3: Ablating random permutation within fast orthogonal multiplication. Differences in perplexity are averaged over WikiText2, PTB, and C4 for OPT-125m.\n\nThis paper introduced quantization with incoherence processing (QuIP), an algorithm consisting of (1) an optimal adaptive rounding procedure which minimizes a quadratic proxy of the weight error, and (2) efficient pre-and post-processing to ensure the incoherence of the weight and Hessian matrices by multiplying them by a Kronecker product of random orthogonal matrices. We showed that QuIP quantization is optimal in a general class of adaptive rounding methods with linear feedback; this theoretical analysis is the first for any quantization algorithm that scales to LLM-sized models.\n\nEmpirically, QuIP achieves the first viable two-bit quantization results for LLMs, especially at large model sizes, hinting at the feasibility of accurate 2-bit inference in LLMs.\n\nLimitations. The proxy objective (1) \n\n\nA Broader Impacts\n\nOur work pushes the quantization of large language models into the 2 bits per weight regime. Our aim is to drive foundational research on theoretical and empirical aspects of quantization. The ultimate goal is to enable more powerful LLMs to run more efficiently. However our work is unaware to what ends those LLMs are used.\n\n\nB Limitations\n\nThe adaptive rounding [20] proxy objective considers each layer in isolation; it remains to be seen what other computationally tractable proxies could improve quantization. For example quantization methods do exist which consider interactions between layers, but so far have been too computationally expensive to be applied to the largest open LLMS.\n\n\nC Experiments, Reproducibility\n\nOur code is included in the Supplement. See the included README for instructions on how to reproduce the various experiments, including random seeds. The code also downloads all datasets used to quantize or evaluate the models.\n\n\nD Additional Method Clarifications D.1 Subsection 4.2 (Incoherence-Based Heuristics)\n\nLine 4 diagonally rescales W and H to minimize (\u0174 ) \u2248 tr (H) W 2 F , effectively trading off the spectrum of these matrices to find a minimum. Note to minimize tr\nD \u22121 HD \u22121 W D 2 F = ( n i=1 H ii /D 2 i )( n i=1 D 2 i W i 2 ) implies that D i = H ii / W i .\nMotivated by the incoherence of W , Line 6 computes the quantization range depending on the spectrum W F , instead of the typical max i,j |W ij |. The parameter \u03c1 controls the quantization range; we tune it and find that a value of 2.4 works well across all our experiments. We use \u03c1 = 2.4 consistently across all experiments. Our full QuIP procedure is described in Algorithm 3, which contains calls to the pre-and post-processing sub-steps in Algorithms 1 and 2.\n\n\nD.2 Subsection 4.2 (Greedy Updates)\n\nIn this subsection, we describe the \"greedy local search\" method mentioned in the main body of the paper in more detail. The basic idea is to iterate over coordinates of the weights in the same order as the initial quantization method, modifying each weight in turn-but still restricting it to be a representable quantized value-so as to minimize the proxy loss while keeping the other weights fixed. These greedy updates amount to coordinate descent on the proxy loss, but restricted to the quantization grid. Greedy updates can be performed after any initial quantization method, or as a standalone method. When performed after an initial quantization method, greedy local search is a descent method because the individual weight updates cannot increase the loss, but when performed alone, these greedy updates are not a descent method because the initial point (\u0174 = W ) is not feasible because it contains unquantized values that are off the representable quantization grid. Concretely, a greedy update of weight (i, j) to the grid {0, 1, . . . , 2 b \u2212 1} does the following, where is the proxy loss:\u0174\nij \u2190 arg min z\u2208{0,1,...,2 b \u22121} (\u0174 \u2212 e i e T j\u0174ij + e i e T j z).\n(Note that\u0174 \u2212 e i e T j\u0174 ij + e i e T j z is the result of setting the (i, j)th entry of\u0174 to z.) A full pass of greedy updates constitutes mn of these updates performed in the same order as LDLQ. This algorithm is very simple, since it is just greedy coordinate descent. In the rest of this subsection, we will give a bit more intuition about this method by showing how this greedy algorithm falls within our framework of adaptive rounding with linear feedback.\n\nAn application of greedy local search as a single-pass stand-alone method falls under our Adaptive Rounding with Linear Feedback framework, with the linear feedback set to U = (H M ) diag(H) \u22121 ,\n\n\nAlgorithm Greedy Updates: A Single Pass\nRequire: b \u2208 N, H \u2208 R n\u00d7n SPD, weights W \u2208 R m\u00d7n , initial guessW 1: U \u2190 (H M ) diag(H) \u22121 M is the strictly upper triangular mask 2: V \u2190 W \u2212 (W \u2212 W )(H M T ) diag(H) \u22121 can skip ifW = W by setting V \u2190 W 3: for k \u2208 {1, . . . , n} do\u0174 k \u2190 clamp(Q near (V k + (W \u2212\u0174 )U k ), 0, 2 b \u2212 1) 4: return\u0174\nwhere M is the strictly upper triangular mask and denotes the Hadamard (entrywise) product, as we will derive below. For ease of explanation consider a single (row) weight vector w \u2208 R 1\u00d7n . When looking only at column j, the proxy loss from setting\u0175 j to z is\n(\u0175 \u2212\u0175e j e T j + ze T j ) = (\u0175 \u2212 w)H(\u0175 \u2212 w) T + 2(ze T j \u2212\u0175e j e T j )H(\u0175 \u2212 w) T + (ze T j \u2212\u0175e j e T j )H(ze T j \u2212\u0175e j e T j ) T .\nThis is just a quadratic function in z, and so its minimum value on the grid {0, 1, . . . , 2 b \u2212 1} will just be its minimum value on R rounded to that grid. To find this minimum over R, we differentiate to minimize, yielding\n0 = 2e T j H(\u0175 \u2212 w) T + 2e T j H(ze T j \u2212\u0175e j e T j ) T , and solving for z, z = \u2212 (\u0175 \u2212\u0175e j e T j \u2212 w)He j e T j He j =\u0175e j \u2212 (\u0175 \u2212 w)He j e T j He j .(8)\nSince when we use greedy local search as a stand-alone method, we have not updated\u0175 j yet, at this point\u0175e j = we j , and so this means that a single step of greedy updates looks lik\u00ea\nwe j \u2190 Q we j \u2212 (\u0175 \u2212 w)\nHe j e T j He j for Q referring to nearest rounding with the necessary clamping. Since\u0175 \u2212 w is zero for all entries following the jth one, this is equivalent t\u00f4\nwe j \u2190 Q(we j \u2212 (\u0175 \u2212 w)U e j )\nwhere U is set as U = (H M ) diag(H) \u22121 as above. This shows how this single-pass version of greedy updates fits into our adaptive rounding with linear feedback framework.\n\nAnalyzing greedy local search as a post-processing pass is a bit more difficult, but we will see that it can also be written as something like adaptive rounding with linear feedback. Suppose that we do a pass of greedy updates, but our quantized weights start at an initial value\u0175 =w already quantized from some previous method (e.g. LDLQ). Returning to (8), since we haven't updated\u0175 j yet, we'll have\nz =we j \u2212 (\u0175 \u2212 w)He j e T j He j .\nNow, all the entries of\u0175 which come after j are still the ones fromw. This means that we can split this up as z = we j \u2212 (\u0175 \u2212 w) :,1:(j\u22121) H 1:(j\u22121),j + (w \u2212 w) :,(j+1):n H (j+1):n,j e T j He j where the first part of this sum comes from the entries which we may have already updated during this pass, the second comes from the entries which are still equal to their initial values inw, and the case of w j is handled specially, cancelling it with thewe j term. We can write this more compactly in matrix form as  Table 4: We compute H in each layer of a given model, and compute the following summary statistics. tr (D) / tr (H) decreases as the mode size increases, though the variance also increases. We compute the fraction of nonzero eigenvalues (i.e. absolute), and the fraction of eigenvalues > 0.01 \u00b7 max(eig(H)) (i.e. approximate). The fractional rank is k/n for a rank-k matrix H with dimension n. Mean and standard deviations are computed across layers in a model.\nz = we j \u2212 (\u0175 \u2212 w)(H\nwhere M is the strictly upper triangular mask and is elementwise multiplication. This yields a final quantization step of\nwe j \u2190 Q we j \u2212 (w \u2212 w) (H M T )e j e T j He j \u2212 (\u0175 \u2212 w) He j e T j He j .\nSo, more generally, if we define U as above, and set\nV = W \u2212 (W \u2212 W )(H M T ) diag(H) \u22121 ,\nwe can write a single pass of greedy updates in matrix form as\nW \u2190 Q(V + (W \u2212\u0174 )U ),\nwhich is very close to our rounding with linear feedback form, albeit with the difference that here V is in place of W . This is made explicit in the included Greedy Updates Algorithm.\n\nWe can use this algorithm both as a whole quantization method (by settingW = W ) or as a postprocessing step (by settingW to the output of some other initial quantization algorithm, such as LDLQ). When we do use it as a post-processing step, we typically run multiple passes of greedy updates (e.g. 10 passes): this involves passing the output of the greedy updates algorithm back in as the input guessW to another run of the greedy updates algorithm, and repeating this multiple times.\n\n\nE Additional Experimental Descriptions and Results\n\nE.1 Subsections 3.2 and 3.3 (Empirical Properties of H Across OPT-125m to 2.7b)\n\nInterpreting the exact proxy loss of LDLQ and nearest rounding by empirically comparing tr (D) vs tr (H). Theorem 1 gives the average-case proxy loss for LDLQ in terms of tr (D), where D is from the LDL decomposition of H. Lemma 3 gives the average-case proxy loss for standard nearest rounding in terms of tr (H). We know that LDLQ is better in practice, but comparing these equations is difficult because we need to reason about tr (D) vs tr (H). Our paper resolves this difficulty by deriving bounds on the proxy loss for LDLQ in terms of the spectrum of H (with and without incoherence). However we also perform a quick empirical check: if tr (D) tr (H), then our theory explains the empirical superiority of LDLQ over nearest rounding (at least on these models). Table 4 gives the ratio tr (D) / tr (H) across all layers for OPTQ models 125m to 2.7b; the mean value is always less than 0.55, and it falls as the model gets larger.\n\nH is approximately low-rank. Subsection 3.3 plotted the normalized eigenvalues of H from 3 randomly chosen layers in OPT-2.7b. Table 4 gives much more evidence that H is consistently approximately low-rank. Across each model, we calculate the absolute and approximate fractional rank of H across all layers in OPT models 125m to 2.7b (explanations in the caption). The approximate fractional rank decreases as model size increases; for OPT-2.7b the fractional rank is \u2248 0.02(\u00b10.02).\n\n\nE.2 Subsection 5.1 (Empirical Verification of OPTQ Equivalence)\n\nWe share a python script in the supplementary code which empirically verifies that our implementation of LDLQ produces quantized values exactly matching OPTQ's [8] implementation. While we prove the equivalence between LDLQ and OPTQ's respective algorithm statements, empirically comparing ours and Frantar et al. [8]'s code ensures that the respective implementations are sufficiently close to their algorithmic statements. Therefore we can be sure that LDLQ and OPTQ are equivalent in their implementation.\n\n\nE.3 Subsection 5.2 (Empirical Verification of LDLQ/OPTQ Finite Grid Counterexample)\n\nThe following code constructs a weight matrix W and Hessian matrix H where OPTQ performs worse than nearest when rounding to a finite grid. The intuition behind this counterexample is as follows: we want to quantize many coordinates in W in such a way that OPTQ excepts there to be a very large error correction to quantize the last entry. However, the finite grid restricts this large error correction. Note that we can achieve this poor OPTQ behavior with c=0, but here nearest rounding also does poorly. We make a small perturbation (c=0.01) to make OPTQ round in the wrong direction, but not nearest.\n\n\nE.4 Additional Details on the Experimental Setup and Computational Resources\n\nWe run experiments on a university cluster managed by a Slurm workload manager which has GPUs with up to 48GB of memory, though larger GPUs are only required for some methods on larger model sizes. Note we use the LAMBADA OpenAI version. When Greedy updates are used, we perform 10 passes over the weights in the same order as LDLQ and OPTQ, except for 5 passes on OPT-30b. For the incoherence-based quantization range, we tune the parameter \u03c1 and find that a value of 2.4 works well across all model sizes and quantization methods. We use this value for all our experiments. Figure 4 shows additional results for QuIP and OPTQ on WikiText2, PiQA, and StoryCloze when quantizing to 2 and 3 bits per weight. The insights about our method QuIP remain the same after viewing these additional results: QuIP is the first PTQ procedure to achieve good quantization at two bits per weight, across a variety of LLM sizes and evaluation tasks. We evaluate on OPT models (up to 30B); 4-bit quantization works equally well for both methods. QuIP is superior to OPTQ across model sizes and evaluation tasks here.\n\n\nE.5 Section 6 (Main Results on Additional Evaluations)\n\nOn WikiText2 2-bit quantization, note that the trend in perplexity for QuIP mirrors the trend in perplexity for OPTQ. We run OPTQ's [8] implementation, though they did not report 2-bit results at this model size. Because OPTQ is equivalent to QuIP's quantization sub-procedure, it thus makes sense that worse performance in the quantization sub-procedure could result in worse overall performance. OPTQ increases perplexity when going from OPT-1.3b to OPT-2.7b. QuIP's perplexity also increases from OPT-1.3b to OPT-2.7b, and is unusually higher than the adjacent OPT-1.3b and OPT-6.7b models. However QuIP still beats OPTQ in this setting. Our observations about OPTQ and QuIP on WikiText2 and OPT-2.7b were consistent across multiple independent runs. E.6 Section 6 (All Methods, All Model Sizes, All Bit Weights, All Evaluation Tasks)\n\nTables 5-11 provide results on all combinations of the following: methods, model sizes (OPT 125m-30b), bit weights(4,3,2), and evaluation tasks. Across our extensive array of experiments, we see that incoherence processing always enables a step function change in quantization at 2 bits.        Table 11: Quantizing OPT-125m with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.\n\n\nE.7 Section 6 (Evaluating the Effectiveness of the Proxy Objective)\n\nIn Table 12 we show the proxy loss of the four quantization methods we evaluate, evaluated over OPT models 125m to 2.7b. The proxy is averaged over models proxy losses normalized by their model dimension; we use H matrices computed as a result of OPTQ and nearest rounding. We do not conduct any processing in the proxy evaluation; this is an evaluation of the rounding methods only. Trends in the proxy reflect end-to-end results. OPTQ/LDLQ, LDLQ-RG, and Greedy are roughly equivalent at 2 bits, and do better than Nearest.\n\nE.8 Section 6 (Evaluating Unbiased Rounding in LDLQ/OPTQ) Note in our formulation for Adaptive Rounding with Linear feedback, the Q subroutine could be biased, or unbiased. It is typical to perform biased rounding in practice; here we investigate if there is   dimension (768, 1024, 2048, 2560) respectively, to ensure proxy loss is comparable across models of different size. We do not conduct any processing in the proxy evaluation. Trends in the proxy largely reflect end-to-end results: at 2 bits OPTQ, LDLQ-RG, and Greedy are roughly equivalent, and all do better than nearest.  Table 13: Average perplexity difference (i.e. unbiased -biased) for LDLQ/OPTQ on WikiText2, PTB, and C4. That is, we can run LDLQ with the Q subroutine as stochastic rounding, instead of nearest. The average difference is positive, meaning that unbiased rounding performs worse than biased (i.e. nearest) across OPT models 125m to 2.7b. Note the magnitude of the gap increases at lower bits.\n\nany benefit to switching to unbiased rounding schemes. Table 13 computes the average perplexity difference (i.e. unbiased \u2212 biased) for LDLQ/OPTQ on WikiText2, PTB, and C4. That is, we run LDLQ with the Q subroutine as stochastic rounding, instead of nearest. The average difference is positive (and large for 2 and 3 bits), meaning that unbiased rounding performs worse than biased (i.e. nearest) across OPT models 125m to 2.7b. These results indicate that in practice, we want to stick with biased rounding schemes.\n\nE.9 Section 6 (Evaluating Algorithm 4 Which Accounts for Clamping)  Table 14: Quantizing OPT models using Algorithm 4 evaluated on WikiText2, PTB, and C4. At 2 bits and incoherence processing, we see improvements over LDLQ and LDLQ-RG on OPT-125m and OPT-350m, but diminishing improvements on OPT-1.3b. Due to Algorithm 4's relatively equivalent performance relative to QuIP at OPT-1.3b, and due to this algorithm's increased computational cost, we decide not to user it. Table 14 shows results from using Algorithm 4 to quantize OPT models 125m to 1.3b, with incoherence processing and baseline processing. At 2 bits and incoherence processing, we observe modest improvements over QuIP in terms of perplexity on OPT models 125m and 350m. However, at the larger OPT-1.3b QuIP beats Algorithm 4 on 2/3 language generation tasks. In addition, Algorithm 4 is computationally more work to run. Therefore we decide not to use it.\n\nAnother observation: in practice, we don't seem to encounter constructions of W and H that are bad for LDLQ/OPTQ. Therefore this \"clamping\" issue seems to not be an issue in practice, especially as model size increases. Proof. Let X be the strictly upper triangular matrix associated with the rounding procedure A such that U \u2190 X in Eq. (2). Let B \u2261 (X + I) \u22121 (\u00d9 + I) where\u00d9 is from the LDL decomposition of H in Eq. (4). The proxy loss is then,\ntr (A(W, H) \u2212 W )H(A(W, H)) T (3),(4) = tr \u03b7(X + I) \u22121 (\u00d9 + I)D(\u00d9 + I) T (X + I) \u2212T \u03b7 T = tr \u03b7BDB T \u03b7 T .(9)\nWith the LDL assignment of U , we further have that,\ntr \u03b7BDB T \u03b7 T = tr \u03b7D\u03b7 T .(10)\nFirst, consider the worst-case loss, L worst . The goal is to construct a particularly bad case where the entries ofW are 1/2 \u00b1 , and thus when rounding to the integers we will always have error 1/2. Construct a weight matrixW \u2208 R m\u00d7n such that each entry satisfies, Subsection 3.3 (Incoherence: Optimality with a Spectral Bound) Definition 1. We say a symmetric Hessian matrix H \u2208 R n\u00d7n is \u00b5-incoherent if it has an eigendecomposition H = Q\u039bQ T such that for all i and j, |Q ij | = e T i Qe j \u2264 \u00b5/ \u221a n. By extension, we say a weight matrix W \u2208 R m\u00d7n is \u00b5-incoherent if all i and j,\nW ij = 0.5 \u2212 w.p. 1/2 0.5 + w.p. 1/2 \u21d2 \u03b7 ij =|W ij | = e T i W e j \u2264 \u00b5 W F / \u221a mn.\nLemma 8. Let H \u2208 R n\u00d7n be a positive semi-definite symmetric matrix, and let a 1 , . . . , a n be a sequence of vectors in R n . Consider the recurrence given by \u03a3 0 = 0 \u2208 R n\u00d7n and from k = 0 to n \u2212 1 \u03a3 k+1 = (I \u2212 e k a T k )\u03a3 k (I \u2212 a k e T k ) + e k e T k . Let (a 1 , . . . , a n ) = tr (H\u03a3 n ). Then if H = LDL T is the LDL decomposition of H, a global minimum of occurs when a k is the kth column of L, and at this minimum, = tr (D).\n\nProof. First observe that at step k, \u03a3 k will be 0 in all entries (\u03a3 k ) ij if min(i, j) \u2265 k. This means that changing the last n \u2212 k entries of a k does not change \u03a3 (or ) at all. Without loss of generality, set those entries of a k to 0. If A is the matrix whose kth row is a k , this is equivalent to saying that A is strictly lower triangular.\n\nNext, let \u03b7 be a random Gaussian sampled from N (0, I), and consider the recurrence given by\nx 0 = 0 \u2208 R n and x k+1 = x k \u2212 e k a T k x k + e k e T k \u03b7. It's straightforward to see that \u03a3 k = E x k x T k .\nBut it's also easy to see that the step-k update only modifies/assigns the kth entry of x, and does so based only on earlier entries of x. Since e T k x k = 0, and no later step assigns the k-or-lower entries of x, e T k x n = e T k x k+1 = 0 \u2212 a T k x k + e T k \u03b7 = \u2212a T k x n + e T k \u03b7, which in vector form yields (I + A)x n = \u03b7. In particular, this immediately implies that Since \u2206 T is strictly upper triangular, but B \u22121 D must be lower triangular, this is 0 so we have a minimum. The uniqueness of this minimum (up to assignments of the lower-triangular elements of A or B, which have no effect on ) also immediately follows from the recurrence relation. This implies the minimum is global. This is what we wanted to show.\n\nLemma 2. Let H \u2208 R n\u00d7n be a \u00b5-incoherent positive semi-definite symmetric matrix and let H = (\u00d9 + I)D(\u00d9 + I) T be its LDL Cholesky decomposition, where\u00d9 is a strictly upper triangular matrix and D is a (non-negative) diagonal matrix. Then, tr (D) \u2264 \u00b5 2 n tr H 1/2 2 .\n\nProof. By continuity of tr (D) and tr H 1/2 , it suffices to prove the lemma for positive definite H. First, the closure of positive definite symmetric matrices is the set of positive semi-definite symmetric matrices. Second, consider the set of H that are positive definite and satisfy \u00b5 2 n tr H 1/2 2 \u2212tr (D) \u2265 0, i.e. are non-negative. The closure of this set (i.e. H 0) must also satisfy that the inequality is non-negative.\n\nLet H = Q\u039bQ T be the eigendecomposition of H. First, observe that by incoherence,\ne T k H 1/2 e k = n i=1 \u03bb 1/2 i (e T i Qe k ) 2 \u2264 \u00b5 2 n n i=1 \u03bb 1/2 i = \u00b5 2 n tr H 1/2 . Set \u03b1 = \u00b5 2 n tr H 1/2 ,\nand consider the recurrence from Lemma 8 with\na k = H 1/2 e k \u03b1 Then \u03a3 k+1 = I \u2212 \u03b1 \u22121 e k e T k H 1/2 \u03a3 k I \u2212 \u03b1 \u22121 H 1/2 e k e T k + e k e T k .\nSuppose by way of induction that for some scalar the covariance \u03a3 k \u03b1H \u22121/2 . For the base case, this obviously holds since \u03a3 0 = 0. At step k,\n\u03a3 k+1 I \u2212 \u03b1 \u22121 e k e T k H 1/2 \u03b1H \u22121/2 I \u2212 \u03b1 \u22121 H 1/2 e k e T k + e k e T k = \u03b1H \u22121/2 \u2212 2e k e T k + \u03b1 \u22121 e k e T k H 1/2 e k e T k + e k e T k \u03b1H \u22121/2 .\nNote that with this assignment,\na T k \u03a3 k a k \u2264 (\u03b1 \u22121 e T k H 1/2 )(\u03b1H \u22121/2 )(\u03b1 \u22121 H 1/2 e k ) = \u03b1 \u22121 e T k H 1/2 e k \u2264 1.\nSo, by induction it follows that \u03a3 n \u00b5 2 n tr H 1/2 \u00b7 H \u22121/2 , and so tr (H\u03a3 n ) \u2264 \u00b5 2 n tr H 1/2 tr H \u00b7 H \u22121/2 = \u00b5 2 n tr H 1/2 2 .\n\nBut from Lemma 8, we know that tr (D) is the global minimum of tr (H\u03a3 n ) for any assignment of a k . This immediately gives us the desired result. Proof. For nearest and stochastic rounding, set the linear feedback U in Eq.\n\n(2) to be zero. Stochastic rounding achieves worst-case loss,\nL worst (Stoch, H) (3) = sup W \u2208R m\u00d7n E tr \u03b7H\u03b7 T = m 4 tr (H) .(11)\nFor the average-case proxy loss, recall the computations of E \u03b7 2 ij from the proof of Theorem 1. There exist constants C and A independent of n such that for any function F from the unit sphere in n dimensions to R that is 1-Lipschitz relative to the Riemannian metric on the sphere,\nP x\u223cSn (F (x) \u2212 E x\u223cSn [F (x)] \u2265 t) \u2264 C exp \u2212 nt 2 A\nLemma 10. Let B \u2208 R m\u00d7n be a matrix, and let x be a random vector uniformly distributed on the unit sphere in R n . Then there exist global constants A > 0 and C > 0 independent of m and n such that\nP Bx 2 \u2265 A B 2 F n log C \u03b4 \u2264 \u03b4, Proof. Let F (x) = Bx B F .\nObserve that\n\u2207F (x) = B T Bx Bx \u00b7 B F ,\nand so \u2207F (x) \u2264 1. Also observe that for x drawn uniformly from the sphere in n dimensions,\nE [F (x)] \u2264 E [F (x) 2 ] = 1 B F \u00b7 E Bx 2 = 1 \u221a n .\n25 So, applying Lemma 9,\nP Bx B F \u2212 1 \u221a n \u2265 t \u2264 C exp \u2212 nt 2 A .\nIf we let \u03b4 be\n\u03b4 = C exp \u2212 nt 2 A , then A n log C \u03b4 = t 2\nTrivially, then, for some modified global constants A and C ,\nA n log C \u03b4 = t + 1 \u221a n 2\nThis means that\nP Bx 2 B 2 F \u2265 A n log C \u03b4 \u2264 \u03b4, i.e. P Bx 2 \u2265 A B 2 F n log C \u03b4 \u2264 \u03b4,\nThis is what we wanted to prove.\n\nLemma 5. Let H be a positive semi-definite matrix on R n\u00d7n and W a matrix on R m\u00d7n , and suppose that m = p 1 \u00b7 p 2 \u00b7 \u00b7 \u00b7 p k and n = q 1 \u00b7 q 2 \u00b7 \u00b7 \u00b7 q k . Let U 1 , U 2 , . . . , U k , V 1 , V 2 , . . . , V k be independent random orthogonal matrices on R pi\u00d7pi and R qi\u00d7qi respectively. Set U as the Kronecker product\nU = U 1 \u2297 U 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U k and V as V = V 1 \u2297 V 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 V k Then V HV T is \u00b5 H -incoherent with probability at least 1 \u2212 \u03b4, and U W V T is \u00b5 W -incoherent with probability at least 1 \u2212 \u03b4, where \u00b5 H = A k/2 log Ckn 2 \u03b4 k/2 =\u00d5 (1) and \u00b5 W = A k log 2Ckmn \u03b4 k =\u00d5 (1)\nfor some global constants A and C independent of n and k.\n\nProof. First we will prove what we want to prove about H; then we will prove what we want to prove about W . Let Q be a matrix of eigenvectors of H. Observe that since Q is an orthogonal matrix (by the spectral theorem, because H is symmetric), Qe j is a unit vector, i.e. Qe j = 1. Call Qe j = y. Also observe that e T i (U 1 \u2297 U 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U k ) = ((e T i1 U 1 ) \u2297 (e T i2 U 2 ) \u2297 \u00b7 \u00b7 \u00b7 \u2297 (e T i k U k )) for some indices i j . Call e T ij U j = x T j , and observe that the x j are all independent unit random vectors. So,\n((U 1 \u2297 U 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U k )Q) ij = (x 1 \u2297 x 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 x k ) T y\nfor random unit vectors x 1 , . . . , x k and unit vector y. We can easily bound this with k applications of Lemma 10 and a union bound, yielding\nP (x 1 \u2297 x 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 x k ) T y 2 \u2265 A k n log C \u03b4 k \u2264 k\u03b4,\nSetting \u03b4 \u2192 \u03b4 kn 2 yields\nP (x 1 \u2297 x 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 x k ) T y 2 \u2265 A k n log Ckn 2 \u03b4 k \u2264 \u03b4 n 2 ,\nand unioning over all the entries of the large orthogonal matrix,\nP \uf8eb \uf8ed max i,j ((U 1 \u2297 U 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 U k )Q) ij \u2265 A k n log Ckn 2 \u03b4 k \uf8f6 \uf8f8 \u2264 \u03b4.\nNext, for W , observe that if we flatten W , then W/ W F is a unit vector. Then any entry of the resulting matrix can be written as (x 1 \u2297 x 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 x k ) T W (y 1 \u2297 y 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 y k ) where x 1 , . . . , x k and y 1 , . . . , y k are k independent random unit vectors. We can easily bound this with 2k applications of Lemma 10 and a union bound, yielding\nP (x 1 \u2297 x 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 x k ) T W (y 1 \u2297 y 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 y k ) 2 \u2265 A 2k mn log C \u03b4 2k \u2264 2k\u03b4, Setting \u03b4 \u2192 \u03b4 2kmn yields P (x 1 \u2297 x 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 x k ) T W (y 1 \u2297 x 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 y k ) 2 \u2265 A 2k mn log 2Ckmn \u03b4 2k \u2264 \u03b4 mn ,\nand unioning over all the mn entries of the large orthogonal matrix,\nP \uf8eb \uf8ed max i,j e T i (U 1 \u2297 U 2 \u2297 . . . U k )W (V 1 \u2297 V 2 \u2297 \u00b7 \u00b7 \u00b7 \u2297 V k )e j \u2265 A 2k mn log 2Ckmn \u03b4 2k \uf8f6 \uf8f8 \u2264 \u03b4.\nThis is what we wanted to show.\n\n\nH Proofs for Section 5 (Extensions and Further Analyses)\n\nSubsection 5.1 (OPTQ is a Special Case of LDLQ) Theorem 6. OTPQ [8] falls within the class of adaptive rounding procedures with linear feedback as described by Eq.\n\n(2), and is equivalent to LDLQ in Section 3.\n\nProof. OPTQ works in the following way. After OPTQ has quantized the first t \u2212 1 components of the row vector w, it minimizes the proxy loss over the remaining n \u2212 t + 1 elements, keeping the first t \u2212 1 elements fixed. It then quantizes the tth element using nearest rounding to the grid and clamping. It then proceeds to the next column. If we let \u2206 =\u0175 \u2212 w, this proxy loss that it minimizes can be written in block form as = \u2206 1:(t\u22121) H 1:(t\u22121),1:(t\u22121) \u2206 T 1:(t\u22121) + 2\u2206 1:(t\u22121) H 1:(t\u22121),t:n + \u2206 t:n H t:n,t:n \u2206 T t:n and its minimum over \u2206 t:n will occur when 0 = \u2206 1:(t\u22121) H 1:(t\u22121),t:n + \u2206 t:n H t:n,t:n , i.e. \u2206 t:n = \u2212\u2206 1:(t\u22121) H 1:(t\u22121),t:n (H t:n,t:n ) \u22121 .\n\nNow, suppose that H =\u0168 D\u0168 T is the LDL decomposition of H, where\u0168 is unit upper triangular and D is diagonal. Since\u0168 is upper triangular, H t:n,t:n =\u0168 t:n,t:n D t:n,t:n\u0168 T t:n,t:n . Similarly, H 1:(t\u22121),t:n =\u0168 1:(t\u22121),t:n D t:n,t:n\u0168 T t:n,t:n . This means that \u2206 t:n = \u2212\u2206 1:(t\u22121)\u01681:(t\u22121),t:n \u0168 t:n,t:n \u22121 . Now, the only part of the value of \u2206 t:n which matters is the first entry, since this is the one that's going to be used to make the next quantization decision. But since\u0168 t:n,t:n is unit upper triangular and so is its inverse, \u0168 t:n,t:n \u22121 e t = e t , and so \u2206 t = \u2206 t:n e 1 = \u2212\u2206 1:(t\u22121)\u01681:(t\u22121),t:n e t = \u2212\u2206 1:(t\u22121)\u01681:(t\u22121),t = \u2212\u2206(\u0168 \u2212 I)e t . Finally, we quantize the t-th weight a\u015d\nw t = Q(w t \u2212 (\u0174 \u2212 W )(\u0168 \u2212 I)e t )\n. This update is equivalent to our adaptive rounding with linear feedback procedure in Eq. (2), with U assigned from the LDL decomposition of H.\n\n\nSubsection 5.2 (A Bound for Rounding to a Finite Grid)\n\nAlgorithm 4 presents a quantization procedure which theoretically address OPTQ's clamping issue, by incorporating a restriction of |\u0174 ij \u2212 W ij | into objective (7). Note that for simplicity, here we present the explicit case where only two factors are used in each Kronecker product of orthogonal matrices; however, the proof should generalize to any number of factors.\n\nAlgorithm 4 \"Fixed\" Rounding via a Convex Program Require: W \u2208 R m\u00d7n , H \u2208 R n\u00d7n , c > 0, \u03c1 > 0 Require: factorization m = p 1 p 2 , n = p 3 p 4 draw U 1 \u2208 R p1\u00d7p1 uniformly from the set of orthogonal matrices using seed seed(U 1 ) draw U 2 \u2208 R p2\u00d7p2 uniformly from the set of orthogonal matrices using seed seed(U 2 ) draw U 3 \u2208 R p3\u00d7p3 uniformly from the set of orthogonal matrices using seed seed(U 3 ) draw U 4 \u2208 R p4\u00d7p4 uniformly from the set of orthogonal matrices using seed seed(U 4 ) Proof. Let \u03b7 \u2208 R 1\u00d7n be a random standard Gaussian variable as a row vector, let A be a matrix, and consider the recurrence relation over x t \u2208 R 1\u00d7n given by x 0 = 0 and This is what we wanted to show.\nW \u2190 (U 1 \u2297 U 2 )W (U 3 \u2297 U 4 ) H \u2190 (U T 3 \u2297 U T 4 )H(U 3 \u2297 U 4 ) W \u2190 2 b \u22121 2 W \u03c1 + 1 elementwise W \u2190 clamp\n\n29\n\nLemma 12. Suppose that we quantize the row vector w \u2208 R 1\u00d7n using L the solution to the optimization problem minimize: tr HL T L over: L unit upper triangular subject to: e T i L T Le i \u2264 1 + c, \u2200i \u2208 {1, . . . , n} and\u0175 = Q stoch w \u2212 (\u0175 \u2212 w)(L \u22121 \u2212 I) , where Q stoch denotes elementwise unbiased stochastic rounding. Then for any u \u2208 R n and any \u03b4 > 0 P |(\u0175 \u2212 w)u| \u2265 Lu 1 2 log 2 \u03b4 \u2264 \u03b4.\n\nIn particular,\nP (\u0175 \u2212 w)(L \u22121 \u2212 I)e i \u2265 c 2 log 2 \u03b4 \u2264 \u03b4.\nProof. Let \u03b7 be the error of stochastic rounding, and observe that each entry is, conditioned on earlier steps, zero mean and supported on two values that differ by 1. Also observe that And by Markov's inequality, P (exp (\u03b3(\u0175 \u2212 w)u) \u2265 exp(\u03b3R)) \u2264 exp(\u2212\u03b3R) exp \u03b3 2 8 Lu 2 ,\n\ni.e.\nP ((\u0175 \u2212 w)u \u2265 R) \u2264 exp \u2212\u03b3R + \u03b3 2 8 Lu 2 .\nMinimizing the right side over \u03b3 yields \u03b3 = 4R Lu \u22122 and\nP ((\u0175 \u2212 w)u \u2265 R) \u2264 exp \u22122R 2 Lu \u22122 .\nBy a union bound, P (|(\u0175 \u2212 w)u| \u2265 R) \u2264 2 exp \u22122R 2 Lu \u22122 .\n\nNow setting the right side equal to \u03b4,\nP |(\u0175 \u2212 w)u| \u2265 Lu 1 2 log 2 \u03b4 \u2264 \u03b4.\nThis is what we wanted to show. The second statement follows from the fact that Proof. First, from the previous lemmas, if U e i is the ith eigenvector of H, with eigenvalue \u03bb i since P \u03bb i (e T j (\u0175 \u2212 w)U e i ) 2 \u2265 \u03bb i LU e i 2 \u00b7 1 2 log 2 \u03b4 \u2264 \u03b4.\n\nBy the union bound, P \u2203i, j, \u03bb i (e T j (\u0175 \u2212 w)U e i ) 2 \u2265 \u03bb i LU e i 2 \u00b7 1 2 log 2mn \u03b4 \u2264 \u03b4. tr H 1/2 2 log 2mn \u03b4 \u2264 \u03b4.\n\nAnd substituting \u03b4 \u2192 \u03b4/2,\nP tr (\u0175 \u2212 w)H(\u0175 \u2212 w) T \u2265 \u00b5 2 m 2n \u00b7 min(1, c) tr H 1/2 2 log 4mn \u03b4 \u2264 \u03b4 2 .\nOn the other hand, again by a union bound from the previous lemma, And so by another union bound, the probability that tr (\u0175 \u2212 w)H(\u0175 \u2212 w) T \u2264 \u00b5 2 m 4n tr H 1/2 2 log 4mn \u03b4 2 and max i,j e T j (\u0175 \u2212 w)(L \u22121 \u2212 I)e i \u2264 1 is no less than 1\u2212\u03b4. It's clear that if this second inequality holds, the value we pass in to the stochastic quantizer will be in range, and thus so will the output. This proves what we want.\n\nTheorem 14. Suppose that we are given an input matrix w with bounded maximum entry magnitude w \u221e and we want to quantize it using b bits. Suppose that we first re-scale the entries of w by mapping\nw ij \u2192 2 b \u2212 3 2 w ij w \u221e + 1 + 1;\nthis guarantees that 1 \u2264 w ij \u2264 2 b \u2212 2. Then, suppose we quantize using the procedure described in the previous lemma. Finally, we undo the scaling. Then then with probability at least 1 \u2212 \u03b4, all the quantized weights will be in range (no overflow or need for clipping) and\ntr (\u0175 \u2212 w)H(\u0175 \u2212 w) T \u2264 \u00b5 2 m n(2 b \u2212 3) 2 tr H 1/2 2 w 2 \u221e log 4mn \u03b4 2 .\nProof. This is a straightforward consequence of the previous lemma.\n\nTheorem 15. Suppose that we are given an input matrix w with bounded w F and we want to quantize it using b bits. Suppose that we first multiply by two-factor orthogonal matrices, and then we re-scale the entries of w by mapping this guarantees that 1 \u2264 w ij \u2264 2 b \u2212 2. Then, suppose we quantize using the procedure described in the previous lemma. Finally, we undo the scaling and multiplication. Then then with probability at least 1 \u2212 \u03b4, all the quantized weights will be in range (no overflow or need for clipping) and\n\ntr (\u0175 \u2212 w)H(\u0175 \u2212 w) T \u2264 A 4 n 2 (2 b \u2212 3) 2 tr H 1/2 2 w 2 F log 12Cmn 2 \u03b4 6 =\u00d5 1 n 2 4 b tr H 1/2 2 w 2 F . Proof. It is a straightforward consequence of Lemma 5, that unioning over the three bounds on the infinity norm of w, the incoherence of H, and the stochastic rounding, with probability at least 1 \u2212 3\u03b4,\ntr (\u0175 \u2212 w)H(\u0175 \u2212 w) T \u2264 m n(2 b \u2212 3) 2 tr H 1/2 2 w 2 F log 4mn \u03b4 2 \u00b7 A 2 log 2Cn 2 \u03b4 2 \u00b7 A 2 mn log 2Cn \u03b4 2 .\nSubstituting \u03b4 \u2192 \u03b4/3, tr (\u0175 \u2212 w)H(\u0175 \u2212 w) T \u2264 1 n(2 b \u2212 3) 2 tr H 1/2 2 w 2 F log 12mn \u03b4 2 \u00b7 A 2 log 6Cn 2 \u03b4 2 \u00b7 A 2 n log 6Cn \u03b4\n\nAnd this right side is clearly less than tr (\u0175 \u2212 w)H(\u0175 \u2212 w) T \u2264 A 4 n 2 (2 b \u2212 3) 2 tr H 1/2 2 w 2 F log 12Cmn 2 \u03b4\n\n\n6\n\n. This is what we wanted to show.\n\nTheorem 7. Suppose that we run Algorithm 4 (Supplement) to quantize a matrix W \u2208 R m\u00d7n by solving the objective (7). Then there exists an assignment of the algorithm's hyperparameters c and \u03c1 such that with probability at least 1 \u2212 \u03b4, all the quantized weights will be in range (no overflow or need for clipping) and\ntr (\u0174 \u2212 W )H(\u0174 \u2212 W ) T =\u00d5 1 n 2 4 b tr H 1/2 2 W 2 F .\nProof. This follows directly from the previous theorem, which says explicitly what the hyperparameter assignments should be.\n\nL\nworst (A, H) = sup W \u2208R m\u00d7n E tr (A(W, H) \u2212 W )H(A(W, H) \u2212 W ) T (5) L avg (A, H) = E W \u223cUnif[0,1] m\u00d7n tr (A(W, H) \u2212 W )H(A(W, H) \u2212 W ) T .\n\n\nLet B = {Near, Stoch}. Then, L worst (LDLQ,H) < L worst (Stoch,H) and L avg (LDLQ,H) < L avg (B,H). Across OPT models 125m to 2.7b, tr (D) / tr (H) \u2264 0.65-empirically verifying that the gap is not insignificant. See Supplement E for full details.\n\nFigure 1 :\n1eig(H) from OPT-2.7b.\n\n\nsup Hs.t. eig(H)=eig(H) L avg (LDLQ * ,H) = L avg (B, H) = m c tr (H) .\n\nFigure 2 :\n2LDLQ underperforms.\n\nFigure 3\n3Figure 3: Quantizing OPT models up to 30b parameters. Our method QuIP is the first PTQ procedure to achieve good quantization at 2 bits per weight, across a variety of model sizes and evaluation tasks.\n\n\nM )e j + (w \u2212 w)(H M T )e j e T j He j\n\nFigure 4 :\n4Quantizing OPT models up to 30b parameters. Additional evaluation tasks shown here in the Supplement. Our method QuIP is the first PTQ procedure to achieve good quantization at 2 bits per weight, across a variety of model sizes and evaluation tasks.\n\n. 2 (\n2Deriving the Optimality of the LDLQ Adaptive Rounding Procedure) Theorem 1. LDLQ is worst and average-case optimal amongst rounding methods which specify the linear feedback U as a function of H (not of W ), and when rounding to the integers. That is, for all rounding methods A in the class described by Eq.(2), for all positive semi-definite H, and for Q as either nearest or stochastic rounding, m 4 tr(D) = L worst (LDLQ, H) \u2264 L worst (A, H) and m c tr(D) = L avg (LDLQ, H) \u2264 L avg (A, H), where D is the matrix from the LDL decomposition of H, and c = 12 for nearest, c = 6 for stochastic.\n\n=\nquantization errors \u03b7 \u2208 R m\u00d7n are for each entry {+1/2, \u22121/2} with equal probability. For this particularW , A achieves proxy loss L worst (A, H) E tr \u03b7BDB T \u03b7 T = m 4 tr BDB T , with Q as either nearest or stochastic rounding. It follows from the supremum in the definition of L worst in Eq. (5) that, L worst (A, H) \u2265 m 4 tr BDB T . For the LDL assignment of U , the worst case expected quantization error rounding to the integers is 1/2. Therefore, L worst (LDLQ, H) (D), again for Q as either nearest or stochastic rounding. B must be a unit triangular matrix since it is the product of unit triangular matrices. Therefore tr BDB T is minimized when B = I, and L worst (LDLQ, H) \u2264 L worst (A, H).Next, consider the average loss, L avg , where W \u223c U nif [0, 1] m\u00d7n . For Q as nearest rounding, the entries of the quantization error \u03b7 are U nif [\u2212 1 2 , 1 2 ], because each entry is independent and uniformly distributed. It follows that for any entry of \u03b7, 2 x 2 dx = 1 12 . Therefore, L avg (A, H)(9) = E W \u223cU nif [0,1] m\u00d7n tr \u03b7BDB T \u03b7 T = m 12 tr BDB T . For Q as stochastic rounding, the entries of the quantization error \u03b7 are U nif [\u22121, 1]. It follows that for any entry of \u03b7, E \u03b7 2 ij = 1 0 x(1 \u2212 x)dx = 1 6 . Note that for stochastic rounding, the quantization error will be x with probability (1 \u2212 |x|). Therefore, L avg (A, H) = m 6 tr BDB T . Based on these same calculations of E \u03b7 2 ij , 22 we have that L avg (LDL, H) tr (D) with Q as nearest , and = m 6 tr (D) with Q as stochastic rounding. By the same reasoning on the minimization of tr BDB T , L avg (LDLQ, H) \u2264 L avg (A, H).\n\n\n\u03a3 n = (I + A) \u22121 (I + A) \u2212T and = tr (H\u03a3 n ) = tr (I + A) \u2212T H(I + A) \u22121 = tr B \u2212T HB \u22121 . where B = I + A. Differentiating with respect to B in strictly lower triangular direction \u2206 (the only direction in which we have degress of freedom, since the diagonal of B must be unit) yields \u22122 tr B \u2212T HB \u22121 \u2206B \u22121 . It's not hard to see that if H = LDL T is the LDL decomposition of H, and B T = L, that the gradient is \u22122 tr D\u2206B \u22121 = \u22122 tr \u2206B \u22121 D = \u22122 \u2206 T , B \u22121 D .\n\nLemma 3 .\n3Let H be symmetric positive definite. In the worst case stochastic rounding achieves L worst (Stoch, H) = (m/4) tr (H). In the average case nearest and stochastic rounding achieve L avg ({Near, Stoch}, H) = (m/c) tr (H), where c = 12 for nearest, and c = 6 for stochastic.\n\nL\navg (Near, H) (3) = E W \u223cU nif [0,1] m\u00d7n tr \u03b7H\u03b7 T = m 12 tr (H) (12) L avg (Stoch, H) (3) = E W \u223cU nif [0,1] m\u00d7n tr \u03b7H\u03b7 T = m 6 tr (H) . (13) Without incoherence: no improvement with a spectral bound Theorem 4. Consider allH with the same spectrum as H. For any positive semi-definite H, the following holds. On the worst-case loss LDLQ achieves the same error as stochastic rounding, sup Hs.t. eig(H)=eig(H)L worst (LDLQ,H) = L worst (Stoch, H) = m 4 tr (H) . On the average-case loss LDLQ achieves the same error as the corresponding rounding routine. Let B = {Near, Stoch} and c = 12 for nearest, c = 6 for stochastic. sup Hs.t. eig(H)=eig(H) L avg (LDLQ * ,H) = L avg (B, H) = m c tr (H) . Proof. See Lemma 3 for calculations on the proxy loss for nearest and stochastic rounding. For LDLQ, we will derive lower and upper bounds on supH s.t. eig(H)=eig(H) L worst (LDLQ,H) and supH s.t. eig(H)=eig(H) L avg (LDLQ,H), and show they are equal. To construct a lower bound, con-siderH = I\u039bI where \u039b are the eigenvalues of H. This decomposition is also the LDL decomposition ofH, rewritten asH = (U + I)D(U + I) \u22121 . It follows that tr (D) = tr H for thisH. Combine this result with the worst and average-case losses calculated in the proof of Theorem 1. For the worst-case loss from the proof of Theorem 1, \u2265 m 4 tr (H). The lower bound for the average-case loss is \u2265 m 12 tr (H) for Q as nearest, and \u2265 m 6 tr (H) for Q as stochastic. Now upper bounds are derived using the preceding calculations in Eq. (11)-(13), and using the worst and average-case optimality of LDLQ proven in Theorem 1. The lower and upper bounds are tight, proving our result. G Proofs for Section 4 (Quantization With Incoherence Processing: Incoherence Processing Step ) Subsection 4.1 (Incoherence via Efficient Orthogonal Multiplication) Lemma 9 (Theorem 2.4 from ? ] ).\n\n\n(W, min = 0, max = 2 b \u2212 1)) elementwise use ADMM or some other solver to solve minimize: tr HL T L over: L unit upper triangular subject to: e T i L T Le i \u2264 1 + c, \u2200i \u2208 {1, . . . , n}.note that when c = \u221e, L \u22121 is the factor from the LDL decomposition ofH U \u2190 L \u22121 \u2212 I for k \u2208 {1, . . . , n} do\u0174 k \u2190 clamp(Q(W k + (W \u2212\u0174 )\u00d9 k ), 0, 2 b \u2212 1) round with LF W \u2190 \u03c1 2\u0174 2 b \u22121 \u2212 1 \u0174 \u2190 (U T 1 \u2297 U T 2 )\u0174 (U T 3 \u2297 U T 4 ) return\u0174encoded as a tuple of the integer rounded values, the scale factor \u03c1, and the seeds Lemma 11. Suppose that for positive definite \u00b5-incoherent matrix H \u2208 R n\u00d7n and scalar c > 0, L is the solution to the optimization problem minimize: tr HL T L over: L unit upper triangular subject to: e T i L T Le i \u2264 1 + c, \u2200i \u2208 {1, . . . , n}. Then the solution satisfies tr HL T L = \u00b5 2 n \u00b7 min(1, c) tr H 1/2 2 .\n\n\nx t = x t\u22121 \u2212 x t\u22121 Ae i e T i + \u03b7e i eSo this yields a whole bound of tr HL T L = \u00b5 2 n \u00b7 min(1, c)   tr H 1/2 2 .\n\nw\n= w \u2212 (\u0175 \u2212 w)(L \u22121 \u2212 I) + \u03b7, and so\u0175 \u2212 w = \u03b7L and E [exp ((\u0175 \u2212 w)u)] = E [exp (\u03b7Lu)]. From a repeated application of Hoeffding's lemma, we getE [exp ((\u0175 \u2212 w)u)] \u2264 exp 1 8 Lu 2 .Setting u \u2192 \u03b3u for \u03b3 > 0,E [exp (\u03b3(\u0175 \u2212 w)u)] \u2264 exp \u03b3 2 8 Lu 2 .\n\nL\n(L \u22121 \u2212 I)e i 2 = e i \u2212 Le i 2 = e T i e i \u2212e T i Le i \u2212e T i L T e i +e T i L T Le i \u2264 1\u22121\u22121+(1+c) = c.30Lemma 13. Suppose that we quantize the row vector w \u2208 R 1\u00d7n using L the solution to the optimization problem minimize: tr HL T L over: L unit upper triangularsubject to: e T i L T Le i \u2264 1 + c, \u2200i \u2208 {1, . . . , n} and\u0175 = Q stoch w \u2212 (\u0175 \u2212 w)(L \u22121 \u2212 I) ,where Q stoch denotes elementwise unbiased stochastic rounding. Suppose that for some integer b,1 \u2264 w ij \u2264 2 b \u2212 2.Then if we set probability at least 1 \u2212 \u03b4, 0 \u2264\u0175 ij \u2264 2 b \u2212 1 and tr (\u0175 \u2212 w)H(\u0175 \u2212 w)\n\nP\ntr (\u0175 \u2212 w)H(\u0175 \u2212 w) T \u2265 m tr HL T LNow applying the other lemma,P tr (\u0175 \u2212 w)H(\u0175 \u2212 w) T \u2265 \u00b5 2 m 2n \u00b7 min(1, c)\n\nP\n\u2203i, j, e T j (\u0175 \u2212 w)(L \u22121 \u2212 I)e i \u2265 1 \u2264 \u03b4 2.\n\n\nTo help interpret this result, we derive explicit proxy losses for plain nearest and stochastic rounding, which we will then compare to what LDLQ gets via Lemma 2.Lemma 3. Let H be symmetric positive definite. In the worst case stochastic rounding achieves L worst (Stoch, H) = (m/4) tr (H). In the average case nearest and stochastic rounding achieve L avg ({Near, Stoch}, H) = (m/c) tr (H), where c = 12 for nearest, and c = 6 for stochastic.To interpret this result, consider H rank-k with \u00b5 2 k < n. By Cauchy-Schwarz, tr(H 1/2 ) 2 \u2264 k tr (H). \nCombining Lemma 2 with the LDLQ proxy losses of Theorem 1 and comparing with Lemma 3, \n\nLworst(LDLQ, H) \u2264 \nm\u00b5 2 \n4n \ntr H 1/2 2 \n\u2264 \nm\u00b5 2 k \n4n \ntr (H) \u2264 \nm \n4 \ntr (H) = Lworst(Stoch, H) \n\nLavg(LDLQ, H) \u2264 \nm\u00b5 2 \ncn \ntr H 1/2 2 \n\u2264 \nm\u00b5 2 k \ncn \ntr (H) \u2264 \nm \nc \ntr (H) = Lavg(B, H), \n\n\n\nTheorem 4 .\n4Consider allH with the same spectrum as H. For any positive semi-definite H, the following holds. On the worst-case loss LDLQ achieves the same error as stochastic rounding,sup \n\nHs.t. eig(H)=eig(H) \n\n\n\nTable 1 :\n1Quantizing OPT-30b with various quantization and processing methods, and evaluating on \nlanguage generation and zeroshot tasks. Our incoherence processing enables a step function change \nin quantization at 2 bits, across all rounding methods. \n\nWbits Rescale Incoherence Rescale+Incoherence Rescale+Incoherence+Quant Range \n\n4 \n24.30 \n24.32 \n24.05 \n23.89 \n3 \n32.62 \n42.28 \n31.32 \n26.36 \n\n\n\ndoes not consider interactions between blocks of a transformer, or even between layers within a block. It is unclear what improvements could be gained from including such interactions at this scale, and if they are worth it computationally. Yijian Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang. Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers. In Conference on Computer Vision and Pattern Recognition, 2023. [14] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization for vision transformer. In Conference on Neural Information Processing Systems, 2021. [15] Eric Lybrand and Rayan Saab. A greedy algorithm for quantizing neural networks. In Journal of Machine Learning Research, 2021. [16] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The Penn Treebank: Annotating predicate argument structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994. URL https://aclanthology.org/H94-1020. [17] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [18] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098. [19] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization through weight equalization and bias correction. In International Conference on Computer Vision, 2019. [20] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197-7206. PMLR, 2020. [21] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525-1534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144. [22] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. Lut-gemm: Quantized matrix multiplication based on luts for efficient inference in large-scale generative language models. arXiv preprint arXiv:2206.09557, 2023. [23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Conference on Neural Information Processing Systems, 2019. [24] Jain Prateek, Netrapalli Praneeth, and Sanghavi Sujay. Low-rank matrix completion using alternating minimization. In Proceedings of the Forty-fifth Annual ACM STOC, 2013. [25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html. [26] Sandeep Tata and Jignesh M Patel. Piqa: An algebra for querying protein data sets. In International Conference on Scientific and Statistical Database Management, 2003.[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, \nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et. al. Language models \nare few-shot learners. In Conference on Neural Information Processing Systems, 2020. \n\n[3] Christopher De Sa, Kunle Olukotun, and Christopher R\u00e9. Global convergence of stochastic \ngradient descent for some non-convex matrix problems. In International Conference on Machine \nLearning. PMLR, 2015. \n\n[4] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix \nmultiplication for transformers at scale. In Conference on Neural Information Processing \nSystems, 2022. \n\n[5] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. Hawq: Hes-\nsian aware quantization of neural networks with mixed-precision. In International Conference \non Computer Vision, 2019. \n\n[6] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W. Mahoney, and Kurt \nKeutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. In Confer-\nence on Neural Information Processing Systems, 2020. \n\n[7] Elias Frantar, Sidak Pal Sing, and Dan Alistarh. Optimal brain compression: A framework \nfor accurate post-training quantization and pruning. In Conference on Neural Information \nProcessing Systems, 2022. \n\n[8] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quan-\ntization for generative pre-trained transformers. In International Conference on Learning \nRepresentations, 2023. \n\n[9] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post \ntraining quantization with small calibration sets. In International Conference on Machine \nLearning. PMLR, 2021. \n\n[10] Yongkweon Jeon, Chungman Lee, Eulrang Cho, and Yeonju Ro. Mr.biq: Post-training non-\nuniform quantization based on minimizing the reconstruction error. In Conference on Computer \nVision and Pattern Recognition, 2022. \n\n[11] Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao, Peng Gao, and Guodong Guo. Q-vit: \nAccurate and fully quantized low-bit vision transformer. In Conference on Neural Information \nProcessing Systems, 2022. \n\n[12] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, \nand Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. In \nInternational Conference on Learning Representations, 2021. \n[13] \n\nTable 5 :\n5Quantizing OPT-30b with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.Incoherence Processing -OPT-13b \n\nFull \nQuIP \nQuIP-RG \nGreedy+IncP \nNear+IncP \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n10.13 10.21 \n10.5 \n16.02 \n10.35 10.69 \n13.81 \n10.25 10.61 13.91 10.34 10.59 \n16.12 \nPTB\u2193 \n14.52 14.69 15.05 \n21.64 \n14.73 15.20 \n22.23 \n14.85 15.11 20.20 14.93 15.27 \n23.18 \nC4\u2193 \n12.06 12.16 12.39 \n16.60 \n12.18 12.43 \n15.62 \n12.21 12.42 15.19 12.26 12.56 \n17.37 \nArcE\u2191 \n61.78 61.41 59.47 \n53.91 \n60.35 61.78 \n52.86 \n60.10 59.43 53.79 60.56 59.30 \n50.00 \nLAMB\u2191 70.25 72.09 71.10 \n56.24 \n69.47 69.07 \n55.70 \n70.83 68.43 56.98 68.37 67.86 \n46.48 \nPiQA\u2191 \n76.82 76.61 76.17 \n72.52 \n76.55 76.22 \n72.74 \n76.33 76.17 71.87 75.08 76.66 \n70.73 \nSC\u2191 \n76.58 75.62 74.92 \n70.21 \n75.88 75.75 \n70.53 \n75.43 75.62 72.50 74.47 75.43 \n68.43 \n\nBaseline Processing -OPT-13b \n\nFull \nOPTQ \nLDLQ-RG \nGreedy \nNear \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n10.13 10.31 11.60 372.68 10.28 11.54 213.75 10.73 13.67 8,370 11.33 3,333 186,069 \nPTB\u2193 \n14.52 14.91 16.59 344.44 14.85 16.43 220.38 15.25 18.62 7,053 16.40 2,708 121,291 \nC4\u2193 \n12.06 12.26 13.34 135.48 12.24 13.17 \n67.48 \n12.55 14.30 4,316 13.32 2,711 \n93,834 \nArcE\u2191 \n61.78 64.77 60.19 \n42.47 \n60.77 58.54 \n32.07 \n56.61 51.22 25.38 61.32 31.10 \n25.42 \nLAMB\u2191 70.25 72.39 68.89 \n25.77 \n68.72 65.30 \n6.58 \n68.12 59.36 00.02 67.22 00.06 \n00.00 \nPiQA\u2191 \n76.82 78.56 78.02 \n66.05 \n76.28 75.08 \n59.09 \n76.50 73.45 50.98 76.06 53.10 \n49.62 \nSC\u2191 \n76.58 77.53 75.62 \n63.59 \n76.32 73.52 \n56.33 \n75.68 72.44 49.40 74.41 49.71 \n48.70 \n\n\n\nTable 6 :\n6Quantizing OPT-13b with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.Incoherence Processing -OPT-6.7b \n\nFull \nQuIP \nQuIP-RG \nGreedy+IncP \nNear+IncP \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n10.86 10.98 11.51 22.33 11.20 11.61 23.75 11.13 11.62 \n19.06 \n11.18 11.73 \n18.57 \nPTB\u2193 \n15.77 15.93 16.52 31.73 15.99 16.43 45.53 15.88 16.50 \n35.94 \n16.06 16.47 \n27.04 \nC4\u2193 \n12.71 12.86 13.30 21.62 12.88 13.39 24.98 12.89 13.27 \n19.62 \n12.96 13.37 \n19.15 \nArcE\u2191 \n60.06 59.89 59.60 52.61 59.30 58.21 53.32 59.18 58.25 \n51.43 \n59.85 57.62 \n50.59 \nLAMB\u2191 68.72 70.00 68.74 53.97 67.38 65.77 49.91 67.65 67.18 \n54.80 \n67.26 65.86 \n49.49 \nPiQA\u2191 \n76.55 76.77 76.33 72.47 76.71 76.33 72.91 76.39 75.46 \n72.20 \n76.55 76.71 \n71.22 \nSC\u2191 \n74.47 75.18 73.65 68.43 75.05 73.33 69.51 74.35 73.77 \n68.94 \n74.22 74.09 \n68.75 \n\nBaseline Processing -OPT-6.7b \n\nFull \nOPTQ \nLDLQ-RG \nGreedy \nNear \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n10.86 11.49 14.87 2,958 11.23 12.56 739.9 11.75 39.09 16,298 12.15 6,011 20,780 \nPTB\u2193 \n15.77 16.54 22.05 2,521 16.28 18.58 1,109 16.93 66.57 10,708 18.92 5,440 14,217 \nC4\u2193 \n12.71 13.16 17.13 500.7 12.98 14.34 154.0 13.27 37.13 \n9,968 \n14.40 5,225 12,419 \nArcE\u2191 \n60.06 58.84 53.41 31.86 59.18 55.26 33.00 54.63 32.49 \n26.09 \n58.75 25.42 \n25.80 \nLAMB\u2191 68.72 66.18 52.36 01.07 67.46 61.89 01.79 66.19 02.56 \n00.00 \n64.53 00.00 \n00.00 \nPiQA\u2191 \n76.55 76.01 73.23 55.11 76.77 74.48 54.46 74.48 53.59 \n51.90 \n76.28 50.71 \n49.78 \nSC\u2191 \n74.47 73.71 71.42 52.07 74.09 72.37 52.45 72.82 50.99 \n49.40 \n73.58 47.87 \n47.80 \n\n\n\nTable 7 :\n7Quantizing OPT-6.7b with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.Incoherence Processing -OPT-2.7b \n\nFull \nQuIP \nQuIP-RG \nGreedy+IncP \nNear+IncP \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n12.47 12.39 17.44 2,998 12.58 15.07 1,676 12.68 12.96 155.6 12.79 \n13.79 \n28.98 \nPTB\u2193 \n17.97 18.42 20.79 63.59 18.43 20.49 42.05 18.34 20.03 46.28 18.43 \n19.51 \n39.23 \nC4\u2193 \n14.34 14.55 15.63 38.07 14.65 15.97 27.89 14.64 15.22 26.84 14.67 \n15.52 \n27.34 \nArcE\u2191 \n54.34 53.28 52.99 46.93 52.02 52.36 46.93 52.90 51.73 43.14 52.61 \n50.93 \n44.11 \nLAMB\u2191 64.82 66.04 64.99 36.06 64.64 63.46 43.39 64.68 62.95 45.53 65.40 \n61.05 \n35.65 \nPiQA\u2191 \n74.76 74.54 73.94 68.06 73.88 73.45 68.28 74.54 73.83 68.28 73.61 \n73.56 \n67.85 \nSC\u2191 \n71.74 71.80 70.21 66.14 71.55 70.15 64.67 70.85 71.10 65.82 71.16 \n70.02 \n63.27 \n\nBaseline Processing -OPT-2.7b \n\nFull \nOPTQ \nLDLQ-RG \nGreedy \nNear \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n12.47 12.93 17.09 8,949 12.77 16.47 7,718 12.95 18.92 9,665 16.69 15,685 10,641 \nPTB\u2193 \n17.97 19.10 25.36 8,281 19.05 23.94 7,389 19.06 28.75 8,254 32.22 14,532 10,516 \nC4\u2193 \n14.34 14.99 18.14 4,388 14.85 17.37 2,113 15.01 20.87 5,139 18.75 11,257 \n9,356 \nArcE\u2191 \n54.34 52.57 50.04 26.94 52.02 48.95 25.76 52.02 43.39 25.46 52.74 \n26.56 \n27.19 \nLAMB\u2191 64.82 62.00 51.43 00.00 64.04 53.25 00.00 63.50 40.75 00.00 59.15 \n00.00 \n00.00 \nPiQA\u2191 \n74.76 73.88 70.73 48.42 74.54 69.91 49.95 73.61 66.05 50.65 73.83 \n51.41 \n50.22 \nSC\u2191 \n71.74 70.91 68.56 48.50 71.42 67.79 47.17 70.66 60.53 48.44 70.59 \n47.42 \n47.55 \n\n\n\nTable 8 :\n8Quantizing OPT-2.7b with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.Incoherence Processing -OPT-1.3b \n\nFull \nQuIP \nQuIP-RG \nGreedy+IncP \nNear+IncP \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n14.62 14.88 16.21 41.64 16.49 17.76 42.37 16.75 17.11 48.69 16.43 \n17.83 \n56.56 \nPTB\u2193 \n20.29 20.87 22.76 47.72 21.93 23.25 50.17 22.11 23.76 54.46 22.19 \n24.82 \n80.40 \nC4\u2193 \n16.07 16.38 17.12 29.78 17.53 18.44 31.49 17.60 18.54 34.10 17.74 \n19.03 \n45.56 \nArcE\u2191 \n50.84 50.72 49.12 41.88 49.54 48.82 41.20 49.66 48.74 41.08 48.61 \n46.59 \n38.64 \nLAMB\u2191 58.92 56.36 52.47 27.81 51.62 48.36 27.27 49.95 48.38 19.21 49.76 \n51.12 \n20.20 \nPiQA\u2191 \n72.31 71.22 71.11 64.85 71.06 70.24 63.33 71.00 70.35 63.66 71.16 \n69.80 \n62.51 \nSC\u2191 \n70.78 70.08 68.81 63.02 69.00 68.05 63.14 68.49 67.92 62.64 69.13 \n67.79 \n58.43 \n\nBaseline Processing -OPT-1.3b \n\nFull \nOPTQ \nLDLQ-RG \nGreedy \nNear \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n14.62 15.59 21.35 7,856 15.36 20.22 7,739 15.58 22.68 9,786 47.62 12,658 11,690 \nPTB\u2193 \n20.29 22.03 30.74 6,858 21.85 30.10 5,368 22.00 35.18 8,441 73.51 14,705 11,690 \nC4\u2193 \n16.07 16.96 21.59 4,028 16.70 20.21 2,123 16.96 22.11 5,129 27.20 \n6,415 \n8,360 \nArcE\u2191 \n50.84 49.33 45.58 25.46 48.95 45.41 26.68 48.19 42.42 26.01 42.80 \n27.82 \n25.13 \nLAMB\u2191 58.92 57.03 37.32 00.00 58.45 41.08 00.02 59.15 40.97 00.00 36.91 \n00.00 \n00.00 \nPiQA\u2191 \n72.31 70.73 68.66 49.73 70.40 67.95 52.18 70.67 66.43 50.87 67.74 \n51.41 \n49.78 \nSC\u2191 \n70.78 70.15 65.18 48.38 70.34 66.45 49.27 70.40 64.48 48.76 59.13 \n47.87 \n48.25 \n\n\n\nTable 9 :\n9Quantizing OPT-1.3b with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.Incoherence Processing -OPT-350m \n\nFull \nQuIP \nQuIP-RG \nGreedy+IncP \nNear+IncP \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n22.00 \n22.5 \n25.19 \n672.3 \n23.57 25.54 \n418.0 \n23.14 25.38 \n239.9 \n23.41 27.86 \n1,444 \nPTB\u2193 \n31.07 32.57 35.65 \n744.2 \n32.46 37.00 \n587.4 \n33.10 37.07 \n301.0 \n33.32 39.49 \n1,354 \nC4\u2193 \n22.59 23.23 25.48 \n320.0 \n23.45 25.50 \n215.4 \n23.43 25.48 \n124.1 \n23.81 27.41 \n880.2 \nArcE\u2191 \n40.36 39.44 38.13 \n27.44 \n39.31 38.47 \n29.67 \n39.77 40.24 \n30.64 \n38.89 38.76 \n28.41 \nLAMB\u2191 46.67 46.89 42.03 \n01.03 \n43.04 39.80 \n04.99 \n42.44 40.62 \n06.38 \n41.47 34.45 \n00.08 \nPiQA\u2191 \n64.80 64.47 63.28 \n50.87 \n64.25 63.17 \n54.79 \n64.42 64.25 \n55.01 \n64.15 63.00 \n52.23 \nSC\u2191 \n63.14 62.13 61.55 \n53.15 \n61.74 61.23 \n51.43 \n62.83 61.62 \n53.28 \n62.38 61.49 \n50.22 \n\nBaseline Processing -OPT-350m \n\nFull \nOPTQ \nLDLQ-RG \nGreedy \nNear \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n22.00 24.16 33.51 18,687 23.77 31.87 10,446 27.01 137.3 23,952 25.94 64.56 23,668 \nPTB\u2193 \n31.07 34.17 47.69 18,161 33.35 44.38 \n8,508 \n40.39 153.5 15,176 36.78 87.22 28,881 \nC4\u2193 \n22.59 24.71 31.26 \n8,418 \n24.10 29.86 \n3,064 \n27.84 73.59 \n9,099 \n26.21 55.15 17,094 \nArcE\u2191 \n40.36 38.43 38.38 \n26.30 \n39.06 37.42 \n25.46 \n38.34 31.06 \n24.33 \n38.68 36.11 \n25.88 \nLAMB\u2191 46.67 45.60 39.20 \n00.00 \n45.26 32.54 \n00.02 \n51.45 16.63 \n00.00 \n40.66 27.46 \n00.00 \nPiQA\u2191 \n64.80 64.04 63.44 \n51.25 \n65.13 61.97 \n49.67 \n63.49 55.44 \n50.60 \n63.38 60.55 \n51.58 \nSC\u2191 \n63.14 63.78 61.04 \n47.55 \n62.57 60.53 \n48.95 \n61.36 54.87 \n48.44 \n63.02 56.84 \n48.95 \n\n\n\nTable 10 :\n10Quantizing OPT-350m with all combinations of quantization and pre-post processing methods, evaluating on language generation and zeroshot tasks. Our incoherence processing enables a step function change in quantization at 2 bits, across all rounding methods.Incoherence Processing -OPT-125m \n\nFull \nQuIP \nQuIP-RG \nGreedy+IncP \nNear+IncP \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n27.66 33.35 34.22 347.4 31.51 42.94 361.8 30.65 55.54 230.8 31.93 37.57 397.5 \nPTB\u2193 \n38.99 40.80 47.34 430.3 43.28 51.69 414.1 41.96 48.79 250.6 43.08 52.20 441.9 \nC4\u2193 \n26.56 27.63 30.92 177.4 28.74 33.54 159.0 28.82 31.41 99.01 29.28 33.88 224.0 \nArcE\u2191 \n40.03 38.89 37.92 31.99 39.27 38.26 31.36 38.80 37.67 33.21 38.55 37.42 32.91 \nLAMB\u2191 39.16 33.03 26.37 01.05 33.75 16.96 02.17 37.78 25.34 04.66 35.65 25.21 01.82 \nPiQA\u2191 \n61.92 61.64 61.64 54.24 61.64 61.92 55.44 61.10 60.83 56.47 61.43 61.10 53.48 \nSC\u2191 \n59.96 60.03 59.20 52.13 59.07 59.26 51.94 60.15 59.52 54.04 59.13 58.88 53.41 \n\nBaseline Processing -OPT-125m \n\nFull \nOPTQ \nLDLQ-RG \nGreedy \nNear \n\nW16 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \nW4 \nW3 \nW2 \n\nWiki\u2193 \n27.66 31.44 53.26 4,563 32.29 53.25 3,704 77.80 1,791 3,707 37.14 1,293 5,375 \nPTB\u2193 \n38.99 45.31 74.79 4,410 45.56 75.85 3,596 101.1 1,403 4,622 53.93 1,418 4,267 \nC4\u2193 \n26.56 29.13 42.55 2,260 29.40 41.77 1,820 65.54 809.5 1,897 33.90 836.5 3,665 \nArcE\u2191 \n40.03 38.51 35.73 28.62 39.02 36.36 27.19 34.05 26.43 27.15 36.66 30.39 26.01 \nLAMB\u2191 39.16 33.69 12.36 00.00 33.26 15.00 00.00 12.25 00.00 00.00 18.22 00.08 00.00 \nPiQA\u2191 \n61.92 60.83 59.47 52.23 61.70 59.58 50.05 57.62 49.29 50.49 61.43 55.88 51.20 \nSC\u2191 \n59.96 58.88 56.97 49.78 59.20 57.03 48.95 50.99 47.55 48.82 59.96 50.03 47.93 \n\n\n\nTable 12 :\n12Weighted average of proxy Loss tr (\u0174 \u2212 W )H(\u0174 \u2212 W ) T over OPT models 125m to 2.7b. Proxy is averaged over models normalized by their model\n\nA systematic classification of knowledge, reasoning, and context within the ARC dataset. Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew Mccallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, Ryan Musa, Kartik Talamadupula, Michael Witbrock, 10.18653/v1/W18-2607Proceedings of the Workshop on Machine Reading for Question Answering. the Workshop on Machine Reading for Question AnsweringMelbourne, AustraliaAssociation for Computational LinguisticsMichael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi, Nicholas Mattei, Ryan Musa, Kartik Talamadupula, and Michael Witbrock. A systematic classifica- tion of knowledge, reasoning, and context within the ARC dataset. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 60-70, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2607. URL https://aclanthology.org/W18-2607.\n\nTowards accurate post-training network quantization via bit-split and stitching. Peisong Wang, Qiang Chen, Xiangyu He, Jian Cheng, PMLRInternational Conference on Machine Learning. Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit-split and stitching. In International Conference on Machine Learning. PMLR, 2020.\n\nOutlier suppression: Pushing the limit of low-bit transformer language models. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, Conference on Neural Information Processing Systems. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. In Conference on Neural Information Processing Systems, 2022.\n\nBloom: A 176b-parameter open-access multilingual language model. Bigscience Workshop, : , Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, and Fran\u00e7ois Yvon et. al. Bloom: A 176b-parameter open-access multilingual language model, 2023.\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han, arXiv:2211.10438arXiv preprintGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2023.\n\nHawq-v3: Dyadic neural network quantization. Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael W Mahoney, Kurt Keutzer, International Conference on Machine Learning. PMLR. Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael W. Mahoney, and Kurt Keutzer. Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning. PMLR, 2021.\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, Conference on Neural Information Processing Systems. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. In Conference on Neural Information Processing Systems, 2022.\n\nRptq: Reorder-based post-training quantization for large language models. Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Luzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, Bingzhe Wu, arXiv:2304.01089arXiv preprintZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Luzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models. arXiv preprint arXiv:2304.01089, 2023.\n\n. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, Opt: Open pre-trained transformer language models, 2022. References for the AppendixSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022. References for the Appendix\n\nOptq: Accurate quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, International Conference on Learning Representations. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate quantization for generative pre-trained transformers. In International Conference on Learning Representations, 2023.\n\nLecture notes on measure-theoretic probability 2. Steve Lalley, Steve Lalley. Lecture notes on measure-theoretic probability 2. http://galton.uchicago. edu/~lalley/Courses/383/Concentration.pdf, 2018.\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLRMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197-7206. PMLR, 2020.\n", "annotations": {"author": "[{\"end\":326,\"start\":69},{\"end\":559,\"start\":327},{\"end\":821,\"start\":560},{\"end\":1061,\"start\":822}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":75},{\"end\":337,\"start\":334},{\"end\":578,\"start\":570},{\"end\":839,\"start\":834}]", "author_first_name": "[{\"end\":74,\"start\":69},{\"end\":333,\"start\":327},{\"end\":569,\"start\":560},{\"end\":833,\"start\":822}]", "author_affiliation": "[{\"end\":325,\"start\":106},{\"end\":558,\"start\":339},{\"end\":820,\"start\":601},{\"end\":1060,\"start\":841}]", "title": "[{\"end\":66,\"start\":1},{\"end\":1127,\"start\":1062}]", "venue": null, "abstract": "[{\"end\":6869,\"start\":1129}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7036,\"start\":7033},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7039,\"start\":7036},{\"end\":7347,\"start\":7344},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7349,\"start\":7347},{\"end\":7352,\"start\":7349},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7355,\"start\":7352},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7358,\"start\":7355},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7361,\"start\":7358},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8021,\"start\":8017},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8632,\"start\":8629},{\"end\":12402,\"start\":12399},{\"end\":12405,\"start\":12402},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20600,\"start\":20597},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21176,\"start\":21173},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":24296,\"start\":24292},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25311,\"start\":25308},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":30176,\"start\":30172},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39392,\"start\":39389},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39546,\"start\":39543},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":41804,\"start\":41801},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":54783,\"start\":54780}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":62098,\"start\":61956},{\"attributes\":{\"id\":\"fig_1\"},\"end\":62347,\"start\":62099},{\"attributes\":{\"id\":\"fig_2\"},\"end\":62382,\"start\":62348},{\"attributes\":{\"id\":\"fig_3\"},\"end\":62456,\"start\":62383},{\"attributes\":{\"id\":\"fig_4\"},\"end\":62489,\"start\":62457},{\"attributes\":{\"id\":\"fig_5\"},\"end\":62702,\"start\":62490},{\"attributes\":{\"id\":\"fig_6\"},\"end\":62743,\"start\":62703},{\"attributes\":{\"id\":\"fig_8\"},\"end\":63006,\"start\":62744},{\"attributes\":{\"id\":\"fig_9\"},\"end\":63609,\"start\":63007},{\"attributes\":{\"id\":\"fig_10\"},\"end\":65209,\"start\":63610},{\"attributes\":{\"id\":\"fig_11\"},\"end\":65674,\"start\":65210},{\"attributes\":{\"id\":\"fig_12\"},\"end\":65959,\"start\":65675},{\"attributes\":{\"id\":\"fig_13\"},\"end\":67811,\"start\":65960},{\"attributes\":{\"id\":\"fig_14\"},\"end\":68636,\"start\":67812},{\"attributes\":{\"id\":\"fig_15\"},\"end\":68754,\"start\":68637},{\"attributes\":{\"id\":\"fig_16\"},\"end\":68998,\"start\":68755},{\"attributes\":{\"id\":\"fig_17\"},\"end\":69558,\"start\":68999},{\"attributes\":{\"id\":\"fig_18\"},\"end\":69670,\"start\":69559},{\"attributes\":{\"id\":\"fig_19\"},\"end\":69718,\"start\":69671},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":70551,\"start\":69719},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":70767,\"start\":70552},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":71167,\"start\":70768},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":77788,\"start\":71168},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":79574,\"start\":77789},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":81335,\"start\":79575},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":83085,\"start\":81336},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":84835,\"start\":83086},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":86651,\"start\":84836},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":88375,\"start\":86652},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":88529,\"start\":88376}]", "paragraph": "[{\"end\":7230,\"start\":6885},{\"end\":7841,\"start\":7232},{\"end\":8770,\"start\":7843},{\"end\":9051,\"start\":8816},{\"end\":9580,\"start\":9096},{\"end\":9770,\"start\":9606},{\"end\":9924,\"start\":9772},{\"end\":10220,\"start\":9989},{\"end\":10326,\"start\":10222},{\"end\":10387,\"start\":10328},{\"end\":10503,\"start\":10389},{\"end\":10869,\"start\":10571},{\"end\":11114,\"start\":10871},{\"end\":11206,\"start\":11116},{\"end\":12302,\"start\":11208},{\"end\":12697,\"start\":12304},{\"end\":13457,\"start\":12699},{\"end\":13796,\"start\":13459},{\"end\":13979,\"start\":13798},{\"end\":14222,\"start\":14028},{\"end\":14577,\"start\":14532},{\"end\":15156,\"start\":14872},{\"end\":15562,\"start\":15210},{\"end\":16136,\"start\":15636},{\"end\":16599,\"start\":16138},{\"end\":17086,\"start\":16655},{\"end\":17150,\"start\":17092},{\"end\":17478,\"start\":17446},{\"end\":18497,\"start\":17480},{\"end\":18824,\"start\":18767},{\"end\":18998,\"start\":18826},{\"end\":19484,\"start\":19024},{\"end\":19943,\"start\":19486},{\"end\":20241,\"start\":19945},{\"end\":20741,\"start\":20310},{\"end\":21098,\"start\":20754},{\"end\":21928,\"start\":21100},{\"end\":22498,\"start\":21970},{\"end\":22751,\"start\":22500},{\"end\":23108,\"start\":22753},{\"end\":23166,\"start\":23110},{\"end\":23929,\"start\":23168},{\"end\":24246,\"start\":23985},{\"end\":24743,\"start\":24262},{\"end\":25241,\"start\":24745},{\"end\":26296,\"start\":25243},{\"end\":26916,\"start\":26298},{\"end\":27885,\"start\":26918},{\"end\":28447,\"start\":27887},{\"end\":28740,\"start\":28449},{\"end\":28974,\"start\":28755},{\"end\":29565,\"start\":28976},{\"end\":29746,\"start\":29567},{\"end\":29785,\"start\":29748},{\"end\":30132,\"start\":29807},{\"end\":30499,\"start\":30150},{\"end\":30761,\"start\":30534},{\"end\":31012,\"start\":30850},{\"end\":31573,\"start\":31109},{\"end\":32717,\"start\":31613},{\"end\":33245,\"start\":32784},{\"end\":33442,\"start\":33247},{\"end\":34040,\"start\":33780},{\"end\":34398,\"start\":34172},{\"end\":34736,\"start\":34553},{\"end\":34921,\"start\":34761},{\"end\":35124,\"start\":34953},{\"end\":35528,\"start\":35126},{\"end\":36539,\"start\":35564},{\"end\":36682,\"start\":36561},{\"end\":36810,\"start\":36758},{\"end\":36911,\"start\":36849},{\"end\":37118,\"start\":36934},{\"end\":37606,\"start\":37120},{\"end\":37740,\"start\":37661},{\"end\":38677,\"start\":37742},{\"end\":39161,\"start\":38679},{\"end\":39737,\"start\":39229},{\"end\":40429,\"start\":39825},{\"end\":41610,\"start\":40510},{\"end\":42506,\"start\":41669},{\"end\":43071,\"start\":42508},{\"end\":43667,\"start\":43143},{\"end\":44644,\"start\":43669},{\"end\":45163,\"start\":44646},{\"end\":46089,\"start\":45165},{\"end\":46537,\"start\":46091},{\"end\":46699,\"start\":46647},{\"end\":47313,\"start\":46731},{\"end\":47836,\"start\":47397},{\"end\":48185,\"start\":47838},{\"end\":48279,\"start\":48187},{\"end\":49123,\"start\":48394},{\"end\":49392,\"start\":49125},{\"end\":49823,\"start\":49394},{\"end\":49906,\"start\":49825},{\"end\":50066,\"start\":50021},{\"end\":50309,\"start\":50166},{\"end\":50495,\"start\":50464},{\"end\":50719,\"start\":50587},{\"end\":50945,\"start\":50721},{\"end\":51008,\"start\":50947},{\"end\":51361,\"start\":51077},{\"end\":51613,\"start\":51415},{\"end\":51686,\"start\":51674},{\"end\":51805,\"start\":51714},{\"end\":51882,\"start\":51858},{\"end\":51937,\"start\":51923},{\"end\":52043,\"start\":51982},{\"end\":52085,\"start\":52070},{\"end\":52187,\"start\":52155},{\"end\":52508,\"start\":52189},{\"end\":52835,\"start\":52778},{\"end\":53363,\"start\":52837},{\"end\":53575,\"start\":53430},{\"end\":53660,\"start\":53635},{\"end\":53793,\"start\":53728},{\"end\":54232,\"start\":53872},{\"end\":54513,\"start\":54445},{\"end\":54655,\"start\":54624},{\"end\":54879,\"start\":54716},{\"end\":54925,\"start\":54881},{\"end\":55594,\"start\":54927},{\"end\":56287,\"start\":55596},{\"end\":56467,\"start\":56323},{\"end\":56896,\"start\":56526},{\"end\":57593,\"start\":56898},{\"end\":58094,\"start\":57707},{\"end\":58110,\"start\":58096},{\"end\":58424,\"start\":58153},{\"end\":58430,\"start\":58426},{\"end\":58529,\"start\":58473},{\"end\":58625,\"start\":58567},{\"end\":58665,\"start\":58627},{\"end\":58948,\"start\":58701},{\"end\":59068,\"start\":58950},{\"end\":59095,\"start\":59070},{\"end\":59579,\"start\":59171},{\"end\":59777,\"start\":59581},{\"end\":60087,\"start\":59813},{\"end\":60228,\"start\":60161},{\"end\":60752,\"start\":60230},{\"end\":61064,\"start\":60754},{\"end\":61302,\"start\":61175},{\"end\":61418,\"start\":61304},{\"end\":61457,\"start\":61424},{\"end\":61775,\"start\":61459},{\"end\":61955,\"start\":61831}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9095,\"start\":9052},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9605,\"start\":9581},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9988,\"start\":9925},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14531,\"start\":14223},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14871,\"start\":14578},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15209,\"start\":15157},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17445,\"start\":17151},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18766,\"start\":18498},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23984,\"start\":23930},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31108,\"start\":31013},{\"attributes\":{\"id\":\"formula_12\"},\"end\":32783,\"start\":32718},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33779,\"start\":33485},{\"attributes\":{\"id\":\"formula_14\"},\"end\":34171,\"start\":34041},{\"attributes\":{\"id\":\"formula_15\"},\"end\":34552,\"start\":34399},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34760,\"start\":34737},{\"attributes\":{\"id\":\"formula_17\"},\"end\":34952,\"start\":34922},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35563,\"start\":35529},{\"attributes\":{\"id\":\"formula_19\"},\"end\":36560,\"start\":36540},{\"attributes\":{\"id\":\"formula_20\"},\"end\":36757,\"start\":36683},{\"attributes\":{\"id\":\"formula_21\"},\"end\":36848,\"start\":36811},{\"attributes\":{\"id\":\"formula_22\"},\"end\":36933,\"start\":36912},{\"attributes\":{\"id\":\"formula_23\"},\"end\":46646,\"start\":46538},{\"attributes\":{\"id\":\"formula_24\"},\"end\":46730,\"start\":46700},{\"attributes\":{\"id\":\"formula_25\"},\"end\":47359,\"start\":47314},{\"attributes\":{\"id\":\"formula_26\"},\"end\":47396,\"start\":47359},{\"attributes\":{\"id\":\"formula_27\"},\"end\":48393,\"start\":48280},{\"attributes\":{\"id\":\"formula_28\"},\"end\":50020,\"start\":49907},{\"attributes\":{\"id\":\"formula_29\"},\"end\":50165,\"start\":50067},{\"attributes\":{\"id\":\"formula_30\"},\"end\":50463,\"start\":50310},{\"attributes\":{\"id\":\"formula_31\"},\"end\":50586,\"start\":50496},{\"attributes\":{\"id\":\"formula_32\"},\"end\":51076,\"start\":51009},{\"attributes\":{\"id\":\"formula_33\"},\"end\":51414,\"start\":51362},{\"attributes\":{\"id\":\"formula_34\"},\"end\":51673,\"start\":51614},{\"attributes\":{\"id\":\"formula_35\"},\"end\":51713,\"start\":51687},{\"attributes\":{\"id\":\"formula_36\"},\"end\":51857,\"start\":51806},{\"attributes\":{\"id\":\"formula_37\"},\"end\":51922,\"start\":51883},{\"attributes\":{\"id\":\"formula_38\"},\"end\":51981,\"start\":51938},{\"attributes\":{\"id\":\"formula_39\"},\"end\":52069,\"start\":52044},{\"attributes\":{\"id\":\"formula_40\"},\"end\":52154,\"start\":52086},{\"attributes\":{\"id\":\"formula_41\"},\"end\":52777,\"start\":52509},{\"attributes\":{\"id\":\"formula_42\"},\"end\":53429,\"start\":53364},{\"attributes\":{\"id\":\"formula_43\"},\"end\":53634,\"start\":53576},{\"attributes\":{\"id\":\"formula_44\"},\"end\":53727,\"start\":53661},{\"attributes\":{\"id\":\"formula_45\"},\"end\":53871,\"start\":53794},{\"attributes\":{\"id\":\"formula_46\"},\"end\":54444,\"start\":54233},{\"attributes\":{\"id\":\"formula_47\"},\"end\":54623,\"start\":54514},{\"attributes\":{\"id\":\"formula_48\"},\"end\":56322,\"start\":56288},{\"attributes\":{\"id\":\"formula_49\"},\"end\":57701,\"start\":57594},{\"attributes\":{\"id\":\"formula_50\"},\"end\":58152,\"start\":58111},{\"attributes\":{\"id\":\"formula_51\"},\"end\":58472,\"start\":58431},{\"attributes\":{\"id\":\"formula_52\"},\"end\":58566,\"start\":58530},{\"attributes\":{\"id\":\"formula_53\"},\"end\":58700,\"start\":58666},{\"attributes\":{\"id\":\"formula_54\"},\"end\":59170,\"start\":59096},{\"attributes\":{\"id\":\"formula_55\"},\"end\":59812,\"start\":59778},{\"attributes\":{\"id\":\"formula_56\"},\"end\":60160,\"start\":60088},{\"attributes\":{\"id\":\"formula_57\"},\"end\":61174,\"start\":61065},{\"attributes\":{\"id\":\"formula_58\"},\"end\":61830,\"start\":61776}]", "table_ref": "[{\"end\":27469,\"start\":27462},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27927,\"start\":27920},{\"end\":28532,\"start\":28525},{\"end\":28620,\"start\":28613},{\"end\":28825,\"start\":28818},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":36085,\"start\":36078},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":38517,\"start\":38510},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":38813,\"start\":38806},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":42811,\"start\":42803},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":43154,\"start\":43146},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":44261,\"start\":44253},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":44709,\"start\":44701},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45241,\"start\":45233},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":45645,\"start\":45637}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":6883,\"start\":6871},{\"attributes\":{\"n\":\"3.1\"},\"end\":8814,\"start\":8773},{\"attributes\":{\"n\":\"3.2\"},\"end\":10569,\"start\":10506},{\"end\":14026,\"start\":13982},{\"attributes\":{\"n\":\"4\"},\"end\":15634,\"start\":15565},{\"attributes\":{\"n\":\"4.1\"},\"end\":16653,\"start\":16602},{\"end\":17090,\"start\":17089},{\"attributes\":{\"n\":\"4.2\"},\"end\":19022,\"start\":19001},{\"attributes\":{\"n\":\"5\"},\"end\":20275,\"start\":20244},{\"attributes\":{\"n\":\"5.1\"},\"end\":20308,\"start\":20278},{\"end\":20752,\"start\":20744},{\"attributes\":{\"n\":\"5.2\"},\"end\":21968,\"start\":21931},{\"attributes\":{\"n\":\"6\"},\"end\":24260,\"start\":24249},{\"attributes\":{\"n\":\"7\"},\"end\":28753,\"start\":28743},{\"end\":29805,\"start\":29788},{\"end\":30148,\"start\":30135},{\"end\":30532,\"start\":30502},{\"end\":30848,\"start\":30764},{\"end\":31611,\"start\":31576},{\"end\":33484,\"start\":33445},{\"end\":37659,\"start\":37609},{\"end\":39227,\"start\":39164},{\"end\":39823,\"start\":39740},{\"end\":40508,\"start\":40432},{\"end\":41667,\"start\":41613},{\"end\":43141,\"start\":43074},{\"end\":54714,\"start\":54658},{\"end\":56524,\"start\":56470},{\"end\":57705,\"start\":57703},{\"end\":61422,\"start\":61421},{\"end\":61958,\"start\":61957},{\"end\":62359,\"start\":62349},{\"end\":62468,\"start\":62458},{\"end\":62499,\"start\":62491},{\"end\":62755,\"start\":62745},{\"end\":63013,\"start\":63008},{\"end\":63612,\"start\":63611},{\"end\":65685,\"start\":65676},{\"end\":65962,\"start\":65961},{\"end\":68757,\"start\":68756},{\"end\":69001,\"start\":69000},{\"end\":69561,\"start\":69560},{\"end\":69673,\"start\":69672},{\"end\":70564,\"start\":70553},{\"end\":70778,\"start\":70769},{\"end\":77799,\"start\":77790},{\"end\":79585,\"start\":79576},{\"end\":81346,\"start\":81337},{\"end\":83096,\"start\":83087},{\"end\":84846,\"start\":84837},{\"end\":86663,\"start\":86653},{\"end\":88387,\"start\":88377}]", "table": "[{\"end\":70551,\"start\":70165},{\"end\":70767,\"start\":70739},{\"end\":71167,\"start\":70780},{\"end\":77788,\"start\":75344},{\"end\":79574,\"start\":78058},{\"end\":81335,\"start\":79844},{\"end\":83085,\"start\":81606},{\"end\":84835,\"start\":83356},{\"end\":86651,\"start\":85106},{\"end\":88375,\"start\":86924}]", "figure_caption": "[{\"end\":62098,\"start\":61959},{\"end\":62347,\"start\":62101},{\"end\":62382,\"start\":62361},{\"end\":62456,\"start\":62385},{\"end\":62489,\"start\":62470},{\"end\":62702,\"start\":62501},{\"end\":62743,\"start\":62705},{\"end\":63006,\"start\":62757},{\"end\":63609,\"start\":63015},{\"end\":65209,\"start\":63613},{\"end\":65674,\"start\":65212},{\"end\":65959,\"start\":65687},{\"end\":67811,\"start\":65963},{\"end\":68636,\"start\":67814},{\"end\":68754,\"start\":68639},{\"end\":68998,\"start\":68758},{\"end\":69558,\"start\":69002},{\"end\":69670,\"start\":69562},{\"end\":69718,\"start\":69674},{\"end\":70165,\"start\":69721},{\"end\":70739,\"start\":70566},{\"end\":75344,\"start\":71170},{\"end\":78058,\"start\":77801},{\"end\":79844,\"start\":79587},{\"end\":81606,\"start\":81348},{\"end\":83356,\"start\":83098},{\"end\":85106,\"start\":84848},{\"end\":86924,\"start\":86666},{\"end\":88529,\"start\":88390}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11814,\"start\":11806},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22006,\"start\":21998},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27146,\"start\":27138},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":41094,\"start\":41086},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":43962,\"start\":43930}]", "bib_author_first_name": "[{\"end\":88627,\"start\":88620},{\"end\":88644,\"start\":88637},{\"end\":88664,\"start\":88655},{\"end\":88685,\"start\":88678},{\"end\":88702,\"start\":88694},{\"end\":88714,\"start\":88708},{\"end\":88730,\"start\":88725},{\"end\":88745,\"start\":88738},{\"end\":88768,\"start\":88763},{\"end\":88790,\"start\":88782},{\"end\":88803,\"start\":88799},{\"end\":88816,\"start\":88810},{\"end\":88838,\"start\":88831},{\"end\":89697,\"start\":89690},{\"end\":89709,\"start\":89704},{\"end\":89723,\"start\":89716},{\"end\":89732,\"start\":89728},{\"end\":90073,\"start\":90066},{\"end\":90086,\"start\":90079},{\"end\":90102,\"start\":90094},{\"end\":90116,\"start\":90110},{\"end\":90132,\"start\":90123},{\"end\":90142,\"start\":90140},{\"end\":90157,\"start\":90150},{\"end\":90171,\"start\":90162},{\"end\":90562,\"start\":90552},{\"end\":90574,\"start\":90573},{\"end\":90582,\"start\":90577},{\"end\":90585,\"start\":90583},{\"end\":90598,\"start\":90592},{\"end\":90615,\"start\":90604},{\"end\":90628,\"start\":90623},{\"end\":90644,\"start\":90638},{\"end\":90657,\"start\":90651},{\"end\":90672,\"start\":90667},{\"end\":90692,\"start\":90683},{\"end\":90698,\"start\":90693},{\"end\":90717,\"start\":90709},{\"end\":91077,\"start\":91068},{\"end\":91086,\"start\":91084},{\"end\":91099,\"start\":91092},{\"end\":91111,\"start\":91108},{\"end\":91122,\"start\":91116},{\"end\":91136,\"start\":91132},{\"end\":91431,\"start\":91425},{\"end\":91441,\"start\":91437},{\"end\":91458,\"start\":91448},{\"end\":91470,\"start\":91466},{\"end\":91485,\"start\":91480},{\"end\":91494,\"start\":91490},{\"end\":91506,\"start\":91500},{\"end\":91519,\"start\":91513},{\"end\":91531,\"start\":91527},{\"end\":91545,\"start\":91538},{\"end\":91547,\"start\":91546},{\"end\":91561,\"start\":91557},{\"end\":91980,\"start\":91974},{\"end\":91990,\"start\":91986},{\"end\":91998,\"start\":91991},{\"end\":92016,\"start\":92010},{\"end\":92031,\"start\":92024},{\"end\":92044,\"start\":92036},{\"end\":92056,\"start\":92049},{\"end\":92442,\"start\":92435},{\"end\":92452,\"start\":92449},{\"end\":92464,\"start\":92458},{\"end\":92475,\"start\":92470},{\"end\":92489,\"start\":92481},{\"end\":92503,\"start\":92496},{\"end\":92518,\"start\":92511},{\"end\":92529,\"start\":92524},{\"end\":92542,\"start\":92534},{\"end\":92554,\"start\":92547},{\"end\":92838,\"start\":92833},{\"end\":92853,\"start\":92846},{\"end\":92867,\"start\":92862},{\"end\":92880,\"start\":92875},{\"end\":92894,\"start\":92890},{\"end\":92908,\"start\":92901},{\"end\":92926,\"start\":92915},{\"end\":92938,\"start\":92934},{\"end\":92949,\"start\":92945},{\"end\":92956,\"start\":92954},{\"end\":92976,\"start\":92971},{\"end\":92991,\"start\":92987},{\"end\":93000,\"start\":92997},{\"end\":93015,\"start\":93011},{\"end\":93031,\"start\":93025},{\"end\":93044,\"start\":93039},{\"end\":93064,\"start\":93058},{\"end\":93080,\"start\":93074},{\"end\":93091,\"start\":93087},{\"end\":93625,\"start\":93620},{\"end\":93642,\"start\":93635},{\"end\":93662,\"start\":93659},{\"end\":93990,\"start\":93985},{\"end\":94205,\"start\":94199},{\"end\":94217,\"start\":94213},{\"end\":94221,\"start\":94218},{\"end\":94233,\"start\":94229},{\"end\":94254,\"start\":94246},{\"end\":94270,\"start\":94264}]", "bib_author_last_name": "[{\"end\":88635,\"start\":88628},{\"end\":88653,\"start\":88645},{\"end\":88676,\"start\":88665},{\"end\":88692,\"start\":88686},{\"end\":88706,\"start\":88703},{\"end\":88723,\"start\":88715},{\"end\":88736,\"start\":88731},{\"end\":88761,\"start\":88746},{\"end\":88780,\"start\":88769},{\"end\":88797,\"start\":88791},{\"end\":88808,\"start\":88804},{\"end\":88829,\"start\":88817},{\"end\":88847,\"start\":88839},{\"end\":89702,\"start\":89698},{\"end\":89714,\"start\":89710},{\"end\":89726,\"start\":89724},{\"end\":89738,\"start\":89733},{\"end\":90077,\"start\":90074},{\"end\":90092,\"start\":90087},{\"end\":90108,\"start\":90103},{\"end\":90121,\"start\":90117},{\"end\":90138,\"start\":90133},{\"end\":90148,\"start\":90143},{\"end\":90160,\"start\":90158},{\"end\":90175,\"start\":90172},{\"end\":90571,\"start\":90563},{\"end\":90590,\"start\":90586},{\"end\":90602,\"start\":90599},{\"end\":90621,\"start\":90616},{\"end\":90636,\"start\":90629},{\"end\":90649,\"start\":90645},{\"end\":90665,\"start\":90658},{\"end\":90681,\"start\":90673},{\"end\":90707,\"start\":90699},{\"end\":90722,\"start\":90718},{\"end\":91082,\"start\":91078},{\"end\":91090,\"start\":91087},{\"end\":91106,\"start\":91100},{\"end\":91114,\"start\":91112},{\"end\":91130,\"start\":91123},{\"end\":91140,\"start\":91137},{\"end\":91435,\"start\":91432},{\"end\":91446,\"start\":91442},{\"end\":91464,\"start\":91459},{\"end\":91478,\"start\":91471},{\"end\":91488,\"start\":91486},{\"end\":91498,\"start\":91495},{\"end\":91511,\"start\":91507},{\"end\":91525,\"start\":91520},{\"end\":91536,\"start\":91532},{\"end\":91555,\"start\":91548},{\"end\":91569,\"start\":91562},{\"end\":91984,\"start\":91981},{\"end\":92008,\"start\":91999},{\"end\":92022,\"start\":92017},{\"end\":92034,\"start\":92032},{\"end\":92047,\"start\":92045},{\"end\":92059,\"start\":92057},{\"end\":92447,\"start\":92443},{\"end\":92456,\"start\":92453},{\"end\":92468,\"start\":92465},{\"end\":92479,\"start\":92476},{\"end\":92494,\"start\":92490},{\"end\":92509,\"start\":92504},{\"end\":92522,\"start\":92519},{\"end\":92532,\"start\":92530},{\"end\":92545,\"start\":92543},{\"end\":92557,\"start\":92555},{\"end\":92844,\"start\":92839},{\"end\":92860,\"start\":92854},{\"end\":92873,\"start\":92868},{\"end\":92888,\"start\":92881},{\"end\":92899,\"start\":92895},{\"end\":92913,\"start\":92909},{\"end\":92932,\"start\":92927},{\"end\":92943,\"start\":92939},{\"end\":92952,\"start\":92950},{\"end\":92969,\"start\":92957},{\"end\":92985,\"start\":92977},{\"end\":92995,\"start\":92992},{\"end\":93009,\"start\":93001},{\"end\":93023,\"start\":93016},{\"end\":93037,\"start\":93032},{\"end\":93056,\"start\":93045},{\"end\":93072,\"start\":93065},{\"end\":93085,\"start\":93081},{\"end\":93103,\"start\":93092},{\"end\":93633,\"start\":93626},{\"end\":93657,\"start\":93643},{\"end\":93670,\"start\":93663},{\"end\":93680,\"start\":93672},{\"end\":93997,\"start\":93991},{\"end\":94211,\"start\":94206},{\"end\":94227,\"start\":94222},{\"end\":94244,\"start\":94234},{\"end\":94262,\"start\":94255},{\"end\":94282,\"start\":94271}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.18653/v1/W18-2607\",\"id\":\"b0\",\"matched_paper_id\":44133715},\"end\":89607,\"start\":88531},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b1\",\"matched_paper_id\":221506623},\"end\":89985,\"start\":89609},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":252545187},\"end\":90485,\"start\":89987},{\"attributes\":{\"id\":\"b3\"},\"end\":90976,\"start\":90487},{\"attributes\":{\"doi\":\"arXiv:2211.10438\",\"id\":\"b4\"},\"end\":91378,\"start\":90978},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":227127479},\"end\":91879,\"start\":91380},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":249395624},\"end\":92359,\"start\":91881},{\"attributes\":{\"doi\":\"arXiv:2304.01089\",\"id\":\"b7\"},\"end\":92829,\"start\":92361},{\"attributes\":{\"id\":\"b8\"},\"end\":93549,\"start\":92831},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":259298689},\"end\":93933,\"start\":93551},{\"attributes\":{\"id\":\"b10\"},\"end\":94135,\"start\":93935},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":216056295},\"end\":94562,\"start\":94137}]", "bib_title": "[{\"end\":88618,\"start\":88531},{\"end\":89688,\"start\":89609},{\"end\":90064,\"start\":89987},{\"end\":91423,\"start\":91380},{\"end\":91972,\"start\":91881},{\"end\":93618,\"start\":93551},{\"end\":94197,\"start\":94137}]", "bib_author": "[{\"end\":88637,\"start\":88620},{\"end\":88655,\"start\":88637},{\"end\":88678,\"start\":88655},{\"end\":88694,\"start\":88678},{\"end\":88708,\"start\":88694},{\"end\":88725,\"start\":88708},{\"end\":88738,\"start\":88725},{\"end\":88763,\"start\":88738},{\"end\":88782,\"start\":88763},{\"end\":88799,\"start\":88782},{\"end\":88810,\"start\":88799},{\"end\":88831,\"start\":88810},{\"end\":88849,\"start\":88831},{\"end\":89704,\"start\":89690},{\"end\":89716,\"start\":89704},{\"end\":89728,\"start\":89716},{\"end\":89740,\"start\":89728},{\"end\":90079,\"start\":90066},{\"end\":90094,\"start\":90079},{\"end\":90110,\"start\":90094},{\"end\":90123,\"start\":90110},{\"end\":90140,\"start\":90123},{\"end\":90150,\"start\":90140},{\"end\":90162,\"start\":90150},{\"end\":90177,\"start\":90162},{\"end\":90573,\"start\":90552},{\"end\":90577,\"start\":90573},{\"end\":90592,\"start\":90577},{\"end\":90604,\"start\":90592},{\"end\":90623,\"start\":90604},{\"end\":90638,\"start\":90623},{\"end\":90651,\"start\":90638},{\"end\":90667,\"start\":90651},{\"end\":90683,\"start\":90667},{\"end\":90709,\"start\":90683},{\"end\":90724,\"start\":90709},{\"end\":91084,\"start\":91068},{\"end\":91092,\"start\":91084},{\"end\":91108,\"start\":91092},{\"end\":91116,\"start\":91108},{\"end\":91132,\"start\":91116},{\"end\":91142,\"start\":91132},{\"end\":91437,\"start\":91425},{\"end\":91448,\"start\":91437},{\"end\":91466,\"start\":91448},{\"end\":91480,\"start\":91466},{\"end\":91490,\"start\":91480},{\"end\":91500,\"start\":91490},{\"end\":91513,\"start\":91500},{\"end\":91527,\"start\":91513},{\"end\":91538,\"start\":91527},{\"end\":91557,\"start\":91538},{\"end\":91571,\"start\":91557},{\"end\":91986,\"start\":91974},{\"end\":92010,\"start\":91986},{\"end\":92024,\"start\":92010},{\"end\":92036,\"start\":92024},{\"end\":92049,\"start\":92036},{\"end\":92061,\"start\":92049},{\"end\":92449,\"start\":92435},{\"end\":92458,\"start\":92449},{\"end\":92470,\"start\":92458},{\"end\":92481,\"start\":92470},{\"end\":92496,\"start\":92481},{\"end\":92511,\"start\":92496},{\"end\":92524,\"start\":92511},{\"end\":92534,\"start\":92524},{\"end\":92547,\"start\":92534},{\"end\":92559,\"start\":92547},{\"end\":92846,\"start\":92833},{\"end\":92862,\"start\":92846},{\"end\":92875,\"start\":92862},{\"end\":92890,\"start\":92875},{\"end\":92901,\"start\":92890},{\"end\":92915,\"start\":92901},{\"end\":92934,\"start\":92915},{\"end\":92945,\"start\":92934},{\"end\":92954,\"start\":92945},{\"end\":92971,\"start\":92954},{\"end\":92987,\"start\":92971},{\"end\":92997,\"start\":92987},{\"end\":93011,\"start\":92997},{\"end\":93025,\"start\":93011},{\"end\":93039,\"start\":93025},{\"end\":93058,\"start\":93039},{\"end\":93074,\"start\":93058},{\"end\":93087,\"start\":93074},{\"end\":93105,\"start\":93087},{\"end\":93635,\"start\":93620},{\"end\":93659,\"start\":93635},{\"end\":93672,\"start\":93659},{\"end\":93682,\"start\":93672},{\"end\":93999,\"start\":93985},{\"end\":94213,\"start\":94199},{\"end\":94229,\"start\":94213},{\"end\":94246,\"start\":94229},{\"end\":94264,\"start\":94246},{\"end\":94284,\"start\":94264}]", "bib_venue": "[{\"end\":88938,\"start\":88869},{\"end\":89788,\"start\":89744},{\"end\":90228,\"start\":90177},{\"end\":90550,\"start\":90487},{\"end\":91066,\"start\":90978},{\"end\":91621,\"start\":91571},{\"end\":92112,\"start\":92061},{\"end\":92433,\"start\":92361},{\"end\":93734,\"start\":93682},{\"end\":93983,\"start\":93935},{\"end\":94328,\"start\":94284},{\"end\":89014,\"start\":88940}]"}}}, "year": 2023, "month": 12, "day": 17}