{"id": 3645757, "updated": "2023-10-08 20:48:51.673", "metadata": {"title": "CSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes", "authors": "[{\"first\":\"Yuhong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xiaofan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Deming\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "journal": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition", "publication_date": {"year": 2018, "month": 2, "day": 27}, "abstract": "We propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present high-quality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF_CC_50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance. In the ShanghaiTech Part_B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-the-art method. We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-of-the-art approach.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1802.10062", "mag": "2964209782", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiZC18", "doi": "10.1109/cvpr.2018.00120"}}, "content": {"source": {"pdf_hash": "fd0321ea3211517a706b7d3fe1b9ff026c0300cd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1802.10062v4.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1802.10062", "status": "GREEN"}}, "grobid": {"id": "52bd73cea2655eb3d4b314538c3778f52ff6006f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fd0321ea3211517a706b7d3fe1b9ff026c0300cd.txt", "contents": "\nCSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes\n\n\nYuhong Li \nUniversity of Illinois at Urbana-Champaign\n\n\nBeijing University of Posts and Telecommunications\n\n\nXiaofan Zhang xiaofan3@illinois.edu \nUniversity of Illinois at Urbana-Champaign\n\n\nDeming Chen dchen@illinois.edu \nUniversity of Illinois at Urbana-Champaign\n\n\nCSRNet: Dilated Convolutional Neural Networks for Understanding the Highly Congested Scenes\n\nWe propose a network for Congested Scene Recognition called CSRNet to provide a data-driven and deep learning method that can understand highly congested scenes and perform accurate count estimation as well as present highquality density maps. The proposed CSRNet is composed of two major components: a convolutional neural network (CNN) as the front-end for 2D feature extraction and a dilated CNN for the back-end, which uses dilated kernels to deliver larger reception fields and to replace pooling operations. CSRNet is an easy-trained model because of its pure convolutional structure. We demonstrate CSRNet on four datasets (ShanghaiTech dataset, the UCF CC 50 dataset, the WorldEXPO'10 dataset, and the UCSD dataset) and we deliver the state-of-the-art performance. In the Shang-haiTech Part B dataset, CSRNet achieves 47.3% lower Mean Absolute Error (MAE) than the previous state-of-theart method. We extend the targeted applications for counting other objects, such as the vehicle in TRANCOS dataset. Results show that CSRNet significantly improves the output quality with 15.4% lower MAE than the previous state-ofthe-art approach.\n\nIntroduction\n\nGrowing number of network models have been developed [1,2,3,4,5] to deliver promising solutions for crowd flows monitoring, assembly controlling, and other security services. Current methods for congested scenes analysis are developed from simple crowd counting (which outputs the number of people in the targeted image) to density map presenting (which displays characteristics of crowd distribution) [6]. This development follows the demand of reallife applications since the same number of people could have completely different crowd distributions (as shown in Fig. 1), so that just counting the number of crowds is not enough. The distribution map helps us for getting more accurate and comprehensive information, which could be critical for making correct decisions in high-risk environments, such as stampede and riot. However, it is challenging to generate accurate distribution patterns. One major difficulty comes from the prediction manner: since the generated density values follow the pixel-by-pixel prediction, output density maps must include spatial coherence so that they can present the smooth transition between nearest pixels. Also, the diversified scenes, e.g., irregular crowd clusters and different camera perspectives, would make the task difficult, especially for using traditional methods without deep neural networks (DNNs). The recent development of congested scene analysis relays on DNN-based methods because of the high accuracy they have achieved in semantic segmentation tasks [7,8,9,10,11] and the significant progress they have made in visual saliency [12]. The additional bonus of using DNNs comes from the enthusiastic hardware community where DNNs are rapidly investigated and implemented on GPUs [13], FPGAs [14,15,16], and ASICs [17]. Among them, the low-power, small-size schemes are especially suitable for deploying congested scene analysis in surveillance devices.\n\nPrevious works for congested scene analysis are mostly based on multi-scale architectures [4,5,18,19,20]. They have achieved high performance in this field but the designs they used also introduce two significant disadvantages when networks go deeper: large amount of training time and non-effective branch structure (e.g., multi-column CNN (MCNN) in [18]). We design an experiment to demonstrate that the MCNN does not perform better compared to a deeper, regular network in Table 1. The main reason of using MCNN in [18] is the flexible receptive fields provided by convolutional filters with different sizes across the column. Intuitively, each column of MCNN is dedicated to a certain level of congested scene. However, the effectiveness of using MCNN may not be prominent. We present Fig. 2 to illustrate the features learned by three separated columns (representing large, medium, and small receptive fields) in MCNN and evaluate them with ShanghaiTech Part A [18] dataset. The three curves in this figure share very similar patterns (estimated error rate) for 50 test cases with different congest densities meaning that each column in such branch structure learn nearly identical features. It performs against the original intention of the MCNN design for learning different features for each column.\n\nIn this paper, we design a deeper network called CSR-Net for counting crowd and generating high-quality density maps. Unlike the latest works such as [4,5] which use the deep CNN for ancillary, we focus on designing a CNN-based density map generator. Our model uses pure convolutional layers as the backbone to support input images with flexible resolutions. To limit the network complexity, we use the small size of convolution filters (like 3 \u00d7 3) in all layers. We deploy the first 10 layers from VGG-16 [21] as the front-end and dilated convolution layers as the back-end to enlarge receptive fields and extract deeper features without losing resolutions (since pooling layers are not used). By taking advantage of such innovative structure, we outperform the state-of-the-art crowd counting solutions (a MCNN based solution called CP-CNN [5]) with 7%, 47.3%, 10.0%, and 2.9% lower Mean Absolute Error (MAE) in ShanghaiTech [18] Part A, Part B, UCF CC 50 [22], and WorldExpo'10 [3] datasets respectively. Also, we achieve high performance on the UCSD dataset [23] with 1.16 MAE. After extending this work to vehicle counting on TRANCOS dataset [20], we achieve 15.4% lower MAE than the current best approach, called FCN-HA [24].\n\nThe rest of the paper is structured as follows. Sec. 2 presents the previous works for crowd counting and density map generation. Sec. 3 introduces the architecture and configuration of our model while Sec. 4 presents the experimental results on several datasets. In Sec. 5, we conclude the paper.\n\n\nRelated work\n\nFollowing the idea proposed by Loy et al. [25], the potential solutions for crowd scenes analysis can be classified into three categories: detection-based methods, regressionbased methods, and density estimation-based methods. By combining the deep learning, the CNN-based solutions show even stronger ability in this task and outperform the traditional methods.   represents the convolutional layer with m filters whose size is n\u00d7n followed by the ReLu layer. M is the max-pooling layer. Results show that the single-column version achieves higher performance on ShanghaiTech Part A dataset [18] with the lowest MAE and Mean Squared Error (MSE)\n\n\nDetection-based approaches\n\nMost of the early researches focus on detection-based approaches using a moving-window-like detector to detect people and count their number [26]. These methods require well-trained classifiers to extract low-level features from the whole human body (like Haar wavelets [27] and HOG (histogram oriented gradients) [28]). However, they perform poorly on highly congested scenes since most of the targeted objects are obscured. To tackle this problem, researchers detect particular body parts instead of the whole body to complete crowd scenes analysis [29].\n\nsuch as foreground and texture features, have been used for generating low-level information [30]. Following similar approaches, Idrees et al. [22] propose a model to extract features by employing Fourier analysis and SIFT (Scaleinvariant feature transform) [31] interest-point based counting.\n\n\nDensity estimation-based approaches\n\nWhen executing the regression-based solution, one critical feature, called saliency, is overlooked which causes inaccurate results in local regions. Lempitsky et al. [32] propose a method to solve this problem by learning a linear mapping between features in the local region and its object density maps. It integrates the information of saliency during the learning process. Since the ideal linear mapping is hard to obtain, Pham et al. [33] use random forest regression to learn a non-linear mapping instead of the linear one.\n\n\nCNN-based approaches\n\nLiterature also focuses on the CNN-based approaches to predict the density map because of its success in classification and recognition [34,21,35]. In the work presented by Walach and Wolf [36], a method is demonstrated with selective sampling and layered boosting. Instead of using patch-based training, Shang et al. [37] try an end-toend regression method using CNNs which takes the entire image as input and directly outputs the final crowd count. Boominathan et al. [19] present the first work purely using convolutional networks and dual-column architecture for generating density map. Marsden et al. [38] explore single-column fully convolutional networks while Sindagi et al. [39] propose a CNN which uses the high-level prior information to boost the density prediction performance. An improved structure is proposed by Zhang et al. [18] who introduce a multi-column based architecture (MCNN) for crowd counting. Similar idea is shown in Onoro and Sastre [20] where a scale-aware, multi-column counting model called Hydra CNN is presented for object density estimation. It is clear that the CNN-based solutions outperform the previous works mentioned in Sec. 2.1 to 2.3.\n\n\nLimitations of the state-of-the-art approaches\n\nMost recently, Sam et al. [4] propose the Switch-CNN using a density level classifier to choose different regressors for particular input patches. Sindagi et al. [5] present a Contextual Pyramid CNN, which uses CNN networks to estimate context at various levels for achieving lower count error and better quality density maps. These two solutions achieve the state-of-the-art performance, and both of them used multi-column based architecture (MCNN) and density level classifier. However, we observe several disadvantages in these approaches: (1) Multi-column CNNs are hard to train according to the training method described in work [18]. Such bloated network structure requires more time to train. (2) Multi-column CNNs introduce redundant structure as we mentioned in Sec. 1. Different columns seem to perform similarly without obvious differences. (3) Both solutions require density level classifier before sending pictures in the MCNN. However, the granularity of density level is hard to define in real-time congested scene analysis since the number of objects keeps changing with a large range. Also, using a fine-grained classifier means more columns need to be implemented which makes the design more complicated and causes more redundancy. (4) These works spend a large portion of parameters for density level classification to label the input regions instead of allocating parameters to the final density map generation. Since the branch structure in MCNN is not efficient, the lack of parameters for generating density map lowers the final accuracy. Taking all above disadvantages into consideration, we propose a novel approach to concentrate on encoding the deeper features in congested scenes and generating highquality density map.\n\n\nProposed Solution\n\nThe fundamental idea of the proposed design is to deploy a deeper CNN for capturing high-level features with larger receptive fields and generating high-quality density maps without brutally expanding network complexity. In this section, we first introduce the architecture we proposed, and then we present the corresponding training methods.\n\n\nCSRNet architecture\n\nFollowing the similar idea in [19,4,5], we choose VGG-16 [21] as the front-end of CSRNet because of its strong transfer learning ability and its flexible architecture for easily concatenating the back-end for density map generation. In CrowdNet [19], the authors directly carve the first 13 layers from VGG-16 and add a 1 \u00d7 1 convolutional layer as output layer. The absence of modifications results in very weak performance. Other architectures, such as [4], uses VGG-16 as the density level classifier for labeling input images before sending them to the most suitable column of the MCNN, while the CP-CNN [5] incorporates the result of classification with the features from density map generator. In these cases, the VGG-16 performs as an ancillary without significantly boosting the final accuracy. In this paper, we first remove the classification part of VGG-16 (fully-connected layers) and build the proposed CSRNet with convolutional layers in VGG-16. The output size of this front-end network is 1/8 of the original input size. If we continue to stack more convolutional layers and pooling layers (basic components in VGG-16), output size would be further shrunken, and it is hard to generate high-quality density maps. Inspired by the works [10,11,40], we try to deploy dilated convolutional layers as the back-end for extracting deeper information of saliency as well as maintaining the output resolution.\n\n\nDilated convolution\n\nOne of the critical components of our design is the dilated convolutional layer. A 2-D dilated convolution can be defined as follow:\ny(m, n) = M i=1 N j=1 x(m + r \u00d7 i, n + r \u00d7 j)w(i, j) (1)\ny(m, n) is the output of dilated convolution from input x(m, n) and a filter w(i, j) with the length and the width of M and N respectively. The parameter r is the dilation rate. If r = 1, a dilated convolution turns into a normal convolution.\n\nDilated convolutional layers have been demonstrated in segmentation tasks with significant improvement of accuracy [10,11,40] and it is a good alternative of pooling layer. Although pooling layers (e.g., max and average pooling) are widely used for maintaining invariance and controlling overfitting, they also dramatically reduce the spatial resolution meaning the spatial information of feature map is lost. Deconvolutional layers [41,42] can alleviate the loss of information, but the additional complexity and execution latency may not be suitable for all cases. Dilated convolution is a better choice, which uses sparse kernels (as shown in Fig. 3) to alternate the pooling and convolutional layer. This character enlarges the receptive field without increasing the number of parameters or the amount of computation (e.g., adding more convolutional layers can make larger receptive fields but introduce more operations). In dilated convolution, a small-size kernel with k \u00d7 k filter is enlarged to k + (k \u2212 1)(r \u2212 1) with dilated stride r. Thus it allows flexible aggregation of the multi-scale contextual information while keeping the same resolution. Examples can be found in Fig. 3 where normal convolution gets 3 \u00d7 3 receptive field and two dilated convolutions deliver 5 \u00d7 5 and 7 \u00d7 7 receptive fields respectively.\n\nFor maintaining the resolution of feature map, the dilated convolution shows distinct advantages compared to the scheme of using convolution + pooling + deconvolution. We pick one example for illustration in Fig. 4. The input is an image of crowds, and it is processed by two approaches separately for generating output with the same size. In the first approach, input is downsampled by a max pooling layer with factor 2, and then it is passed to a convolutional layer with a 3 \u00d7 3 Sobel kernel. Since the generated feature map is only 1/2 of the original input, it needs to be upsampled by the deconvolutional layer (bilinear interpolation). In the other approach, we try dilated convolution and adapt the same 3 \u00d7 3 Sobel kernel to a dilated kernel with a  factor = 2 stride. The output is shared the same dimension as the input (meaning pooling and deconvolutional layers are not required). Most importantly, the output from dilated convolution contains more detailed information (referring to the portions we zoom in).\n\n\nNetwork Configuration\n\nWe propose four network configurations of CSRNet in Table 3 which have the same front-end structure but different dilation rate in the back-end. Regarding the front-end, we adapt a VGG-16 network [21] (except fully-connected layers) and only use 3 \u00d7 3 kernels. According to [21], using more convolutional layers with small kernels is more efficient than using fewer layers with larger kernels when targeting the same size of receptive field .\n\nBy removing the fully-connected layers, we try to determine the number of layers we need to use from VGG-16. The most critical part relays on the tradeoff between accuracy and the resource overhead (including training time, memory consumption, and the number of parameters). Experiment shows a best tradeoff can be achieved when keeping the first ten layers of VGG-16 [21] with only three pooling layers instead of five to suppress the detrimental effects on output accuracy caused by the pooling operation. Since the output (density maps) of CSRNet is smaller (1/8 of input size), we choose bilinear interpolation with the factor of 8 for scaling and make sure the output shares the same Dataset Generating method ShanghaiTech Part A [18] Geometry-adaptive kernels UCF CC 50 [22] ShanghaiTech Part B [18] Fixed kernel: \u03c3 = 15 TRANCOS [44] Fixed kernel: \u03c3 = 10 The WorldExpo'10 [3] Fixed kernel: \u03c3 = 3 The UCSD [23] Table 2. The ground truth generating methods for different datasets resolution as the input image. With the same size, CSR-Net generated results are comparable with the ground truth results using the PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity in Image [43]).\n\n\nTraining method\n\nIn this section, we provide specific details of CSRNet training. By taking advantage of the regular CNN network (without branch structures), CSRNet is easy to implement and fast to deploy.\n\n\nGround truth generation\n\nFollowing the method of generating density maps in [18], we use the geometry-adaptive kernels to tackle the highly congested scenes. By blurring each head annotation using a Gaussian kernel (which is normalized to 1), we generate the ground truth considering the spatial distribution of all images from each dataset. The geometry-adaptive kernel is defined as:\nF (x) = N i=1 \u03b4(x \u2212 x i ) \u00d7 G \u03c3i (x), with \u03c3 i = \u03b2d i (2)\nFor each targeted object x i in the ground truth \u03b4, we use d i to indicate the average distance of k nearest neighbors. To generate the density map, we convolve \u03b4(x \u2212 x i ) with a Gaussian kernel with parameter \u03c3 i (standard deviation), where x is the position of pixel in the image. In experiment, we follow the configuration in [18] where \u03b2 = 0.3 and k = 3. For input with sparse crowd, we adapt the Gaussian kernel to the average head size to blur all the annotations. The setups for different datasets are shown in Table 2.\n\n\nData augmentation\n\nWe crop 9 patches from each image at different locations with 1/4 size of the original image. The first four patches contain four quarters of the image without overlapping while the other five patches are randomly cropped from the input image. After that, we mirror the patches so that we double the training set.  Table 3. Configuration of CSRNet. All convolutional layers use padding to maintain the previous size. The convolutional layers' parameters are denoted as \"conv-(kernel size)-(number of filters)-(dilation rate)\", max-pooling layers are conducted over a 2 \u00d7 2 pixel window with stride 2.\n\n\nTraining details\n\nWe use a straightforward way to train the CSRNet as an end-to-end structure. The first 10 convolutional layers are fine-tuned from a well-trained VGG-16 [21]. For the other layers, the initial values come from a Gaussian initialization with 0.01 standard deviation. Stochastic gradient descent (SGD) is applied with fixed learning rate at 1e-6 during training. Also, we choose the Euclidean distance to measure the difference between the ground truth and the estimated density map we generated which is similar to other works [19,18,4]. The loss function is given as follow:\nL(\u0398) = 1 2N N i=1 Z(X i ; \u0398) \u2212 Z GT i 2 2 (3)\nwhere N is the size of training batch and Z(X i ; \u0398) is the output generated by CSRNet with parameters shown as \u0398. X i represents the input image while Z GT i is the ground truth result of the input image X i .\n\n\nExperiments\n\nWe demonstrate our approach in five different public datasets [18,3,22,23,44]. Compared to the previous stateof-the-art methods [4,5], our model is smaller, more accurate, and easier to train and deploy. In this section, the evaluation metrics are introduced, and then an ablation study of ShanghaiTech Part A dataset is conducted to analyze the configuration of our model (shown in Table 3). Along with the ablation study, we evaluate and compare our proposed method to the previous state-of-the-art methods in all these five datasets. The implementation of our model is based on the Caffe framework [13].\n\n\nEvaluation metrics\n\nThe MAE and the MSE are used for evaluation which are defined as:\nM AE = 1 N N i=1 | C i \u2212 C GT i | (4) M SE = 1 N N i=1 | C i \u2212 C GT i | 2 (5)\nwhere N is the number of images in one test sequence and C GT i is the ground truth of counting. C i represents the estimated count which is defined as follows:\nC i = L l=1 W w=1 z l,w(6)\nL and W show the length and width of the density map respectively while z l,w is the pixel at (l, w) of the generated density map. C i means the estimated counting number for image X i .\n\nWe also use the PSNR and SSIM to evaluate the quality of the output density map on ShanghaiTech Part A dataset. To calculate the PSNR and SSIM, we follow the preprocess given by [5], which includes the density map resizing (same size with the original input) with interpolation and normalization for both ground truth and predicted density map.\n\n\nAblations on ShanghaiTech Part A\n\nIn this subsection, we perform an ablation study to analyze the four configurations of the CSRNet on Shang-haiTech Part A dataset [18] which is a new large-scale crowd counting dataset including 482 images for congested scenes with 241,667 annotated persons. It is challenging to count from these images because of the extremely congested scenes, the varied perspective, and the unfixed resolution. These four configurations are shown in Table 3. CSRNet A is the network with all the dilation rate of 1. CSRNet B and D maintain the dilation rate of 2 and 4 in their back-end respectively while CSRNet C combines the dilated rate of 2 and 4. The number of parameters of these four models are the same as 16.26M. We intend to compare the results by using different dilation rates. After training on Shanghai Part A dataset using the method mentioned in Sec. 3.2, we perform the evaluation metrics defined in Sec. 4.1. We try dropout [45] for preventing the potential overfitting problem but there is no significant improvement. So we do not include dropout in our model. The detailed evaluation results are shown in Table 4, where CSRNet B achieves the lowest error (the highest accuracy). Therefore, we use CSRNet B as the proposed CSRNet for the following experiments.\n\n\nEvaluation and comparison 4.3.1 ShanghaiTech dataset\n\nShanghaiTech crowd counting dataset contains 1198 annotated images with a total amount of 330,165 persons [3]. This dataset consists of two parts as Part A containing 482 images with highly congested scenes randomly downloaded from the Internet while Part B includes 716 images with relatively sparse crowd scenes taken from streets in Shanghai. Our method is evaluated and compared to other six recent works and results are shown in Table 5. It indicates that our method achieves the lowest MAE (the highest accuracy) in Part A compared to other methods and we get 7% lower MAE than the state-of-the-art solution called CP-CNN. CSRNet also delivers 47.3% lower MAE in Part B compared to the CP-CNN. To evaluate the quality of generated density map, we compare our method to the MCNN and the CP-CNN using Part A dataset and we follow the evaluation metrics in Sec. 3.2. Samples of the test cases can be found in Fig 5. Results are shown in Table 6 which indicates CSRNet achieves the highest SSIM and PSNR. We also report the quality result of ShanghaiTech dataset in Table 11.\n\n\nUCF CC 50 dataset\n\nUCF CC 50 dataset includes 50 images with different perspective and resolutions [22]. The number of annotated persons per image ranges from 94 to 4543 with an average number of 1280. 5-fold cross-validation is performed following the standard setting in [22]. Result comparisons of Figure 5. The first row shows the samples of the testing set in ShanghaiTech Part A dataset. The second row shows the ground truth for each sample while the third row presents the generated density map by CSRNet.  Table 7 while the quality of generated density map can be found in Table 11.\n\n\nThe WorldExpo'10 dataset\n\nThe WorldExpo'10 dataset [3] consists of 3980 annotated frames from 1132 video sequences captured by 108 different surveillance cameras. This dataset is divided into a training set (3380 frames) and a testing set (600 frames) from five different scenes. The region of interest (ROI) is provided for the whole dataset. Each frame and its dot maps are masked with ROI during preprocessing, and we train our model following the instructions given in Sec. 3\n\n\n.2. Results\n\n\nMethod\n\nMAE MSE Idrees et al. [22] 419.5 541.6 Zhang et al. [3] 467.0 498.5 MCNN [18] 377.6 509.1 Onoro et al. [20] Hydra-2s 333.7 425. 2 Onoro et al. [20] Hydra-3s 465.7 371. 8 Walach et al. [36] 364.4 341.4 Marsden et al. [38] 338.6 424.5 Cascaded-MTL [39] 322.8 397.9 Switching-CNN [4] 318.1 439.2 CP-CNN [5] 295.8 320.9 CSRNet (ours) 266.1 397.5 Table 7. Estimation errors on UCF CC 50 dataset are shown in Table 8. The proposed CSRNet delivers the best accuracy in 4 out of 5 scenes and it achieves the best accuracy on average.  \n\n\nThe UCSD dataset\n\nThe UCSD dataset [23] has 2000 frames captured by surveillance cameras. These scenes contain sparse crowd varying from 11 to 46 persons per image. The region of interest (ROI) is also provided. Because the resolution of each frame is fixed and small (238 \u00d7 158), it is difficult to generate a high-quality density map after frequent pooling operations. So we preprocess the frames by using bilinear interpolation to resize them into 952 \u00d7 632. Among the 2000 frames, we use frames 601 through 1400 as training set and the rest of them as testing set according to [23]. Before blurring the annotation as we mentioned in Sec. 3.2, all the frames and the corresponding dot maps are masked with ROI. The accuracy of running UCSD dataset is shown in Table 9 and we outperform most of the previous methods except MCNN in the MAE category. Results indicate that our method can perform not only counting tasks for extremely dense crowds but also tasks for relative sparse scenes. Also, we provides the quality of generated density map in Table 11.\n\n\nTRANCOS dataset\n\nBeyond the crowd counting, we setup an experiment on the TRANCOS dataset [44]  1.07 1.35 Table 9. Estimation errors on the UCSD dataset demonstrate the robustness and generalization of our approach. TRANCOS is a public traffic dataset containing 1244 images of different congested traffic scenes captured by surveillance cameras with 46796 annotated vehicles. Also, the region of interest (ROI) is provided for the evaluation. The perspectives of images are not fixed and the images are collected from very different scenarios. The Grid Average Mean Absolute Error (GAME) [44] is used for evaluation in this test. The GAME is defined as follow:\nGAM E(L) = 1 N N n=1 ( 4 L l=1 D l In \u2212 D l I gt n )(7)\nwhere N is the number of images in testing set, and D l In is the estimated result of the input image n within region l. D l I gt n is the corresponding ground truth result. For a specific level L, the GAME(L) subdivides the image using a grid of 4 L non-overlapping regions which cover the full image, and the error is computed as the sum of the MAE in each of these regions. When L = 0, the GAME is equivalent to the MAE metric.\n\nWe compare our approach with the previous state-ofthe-art methods [47,32,20,24]. The method in [20] uses the MCNN-like network to generate the density map while the model in [24] deploys a combination of fully convolutional neural networks (FCN) and a long short-term memory network (LSTM). Results are shown in Table 10 with three examples shown in Fig. 6. Our model achieves a significant improvement on four different GAME metrics. Compared to the result from [20], CSRNet delivers 67.7% lower GAME(0), 60.1% lower GAME(1), 48.7% lower GAME(2), and 22.2% lower GAME(3), which is the best solution. We also present the quality of generated density map in Table 11.\n\nMethod GAME 0 GAME 1 GAME 2 GAME 3 Fiaschi et al. [47] 17 20.02 0.86 TRANCOS [44] 27.10 0.93 Table 11. The quality of density maps generated by CSRNet in 5 datasets\n\n\nConclusion\n\nIn this paper, we proposed a novel architecture called CSRNet for crowd counting and high-quality density map generation with an easy-trained end-to-end approach. We used the dilated convolutional layers to aggregate the multiscale contextual information in the congested scenes. By taking advantage of the dilated convolutional layers, CSR-Net can expand the receptive field without losing resolution. We demonstrated our model in four crowd counting datasets with the state-of-the-art performance. We also extended our model to vehicle counting task and our model achieved the best accuracy as well. \n\n\nAcknowledgement\n\n\nAppendix: supplementary material\n\nIn this appendix, additional results generated by CSRNet from five datasets (ShanghaiTech [18], UCF CC 50 [22], World-Expo'10 [3], UCSD [23], and TRANCOS [44]) are presented to demonstrate the validity of our design. Two criteria are used as the PSNR (Peak Signal-to-Noise Ratio) and the SSIM (Structural Similarity in Image [43] to evaluate our design's quality of generated density maps. Samples from these 5 datasets are shown in Fig. 7 to Fig. 12, which represent a variety of density levels. Figure 7. Samples generated by CSRNet from ShanghaiTech Part A [18] dataset. The left column shows the original images; the medium column displays the ground truth density maps while the right column indicates our generated density maps. Figure 8. Samples generated by CSRNet from ShanghaiTech Part B [18] dataset. The left column shows the original images; the medium column displays the ground truth density maps while the right column indicates our generated density maps. Figure 9. Samples generated by CSRNet from UCF CC 50 [22] dataset. The left column shows the original images; the medium column displays the ground truth density maps while the right column indicates our generated density maps. Figure 10. Samples generated by CSRNet from WorldExpo'10 [3] dataset. The left column shows the images masked by the ROI (region of interest); the medium column displays the ground truth density maps and the right column indicates our generated density maps. Figure 11. Samples generated by CSRNet from UCSD [23] dataset. The left column shows the images masked by the ROI; the medium column displays the ground truth density maps and the right column indicates our generated density maps. Figure 12. Samples generated by CSRNet from TRANCOS [44] dataset. The left column shows the images masked by the ROI; the medium column displays the ground truth density maps while the right column shows the generated density maps.\n\nFigure 1 .\n1Pictures in first row show three images all containing 95 people in ShanghaiTech Part B dataset[18], while having totally different spatial distributions. Pictures in second row show their density maps.\n\nFigure 2 .\n2The estimated error of 50 samples from the testing set in ShanghaiTech Part A[18] generated by the three pre-trained columns of MCNN. Small, Medium, Large respectively stand for the columns with small, medium or large kernels in the MCNN.\n\n\nparameters compared to MCNN. The architecture of the proposed small network is: CR(32, 3) \u2212 M \u2212 CR(64, 3) \u2212 M \u2212 CR(64, 3)\u2212M \u2212CR(32, 3)\u2212CR(32, 3)\u2212CR(1, 1). CR(m, n)\n\nFigure 3 .\n33 \u00d7 3 convolution kernels with different dilation rate as 1, 2, and 3.\n\nFigure 4 .\n4Comparison between dilated convolution and maxpooling, convolution, upsampling. The 3 \u00d7 3 Sobel kernel is used in both operations while the dilation rate is 2.\n\n\nThis work was supported by the IBM-Illinois Center for Cognitive Computing System Research (C3SR) -a research collaboration as part of the IBM AI Horizons Network.\n\nTable 8 .\n8Estimated errors on the WorldExpo'10 dataset\n\n\nfor vehicle counting toMethod \n\nMAE MSE \nZhang et al. [3] \n1.60 \n3.31 \nCCNN [20] CCNN \n1.51 \n-\nSwitching-CNN [4] \n1.62 \n2.10 \nFCN-rLSTM [24] \n1.54 \n3.02 \nCSRNet (ours) \n1.16 \n1.47 \nMCNN [18] \n\n\n\nTable 10. GAME on the TRANCOS datasetFigure 6. The first row shows samples of the testing set in TRAN-COS [44] dataset with ROI. The second row shows the ground truth for each sample. The third row shows the generated density map by CSRNet..77 \n20.14 \n23.65 \n25.99 \nLempitsky et al. [32] \n13.76 \n16.72 \n20.72 \n24.36 \nHydra-3s [20] \n10.99 \n13.75 \n16.69 \n19.32 \nFCN-HA [24] \n4.21 \n-\n-\n-\nCSRNet (Ours) \n3.56 \n5.49 \n8.57 \n15.04 \n\nDataset \nPSNR SSIM \nShanghaiTech Part A [18] 23.79 \n0.76 \nShanghaiTech Part B [18] \n27.02 \n0.89 \nUCF CC 50 [22] \n18.76 \n0.52 \nThe WorldExpo'10 [3] \n26.94 \n0.92 \nThe UCSD [23] \n\n.2. Regression-based approachesSince detection-based approaches can not be adapted to highly congested scenes, researchers try to deploy regression-based approaches to learn the relations among extracted features from cropped image patches, and then calculate the number of particular objects. More features,\n\nCrowd analysis: a survey. Machine Vision and Applications. Beibei Zhan, N Dorothy, Paolo Monekosso, Remagnino, A Sergio, Li-Qun Velastin, Xu, 19Beibei Zhan, Dorothy N Monekosso, Paolo Remagnino, Ser- gio A Velastin, and Li-Qun Xu. Crowd analysis: a survey. Machine Vision and Applications, 19(5-6):345-357, 2008.\n\nCrowded scene analysis: A survey. Teng Li, Huan Chang, Meng Wang, Bingbing Ni, Richang Hong, Shuicheng Yan, IEEE transactions on circuits and systems for video technology. 25Teng Li, Huan Chang, Meng Wang, Bingbing Ni, Richang Hong, and Shuicheng Yan. Crowded scene analysis: A sur- vey. IEEE transactions on circuits and systems for video technology, 25(3):367-386, 2015.\n\nCross-scene crowd counting via deep convolutional neural networks. Cong Zhang, Hongsheng Li, Xiaogang Wang, Xiaokang Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCong Zhang, Hongsheng Li, Xiaogang Wang, and Xiaokang Yang. Cross-scene crowd counting via deep convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 833-841, 2015.\n\nSwitching convolutional neural network for crowd counting. Shiv Deepak Babu Sam, R Venkatesh Surya, Babu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition16Deepak Babu Sam, Shiv Surya, and R Venkatesh Babu. Switching convolutional neural network for crowd counting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume 1, page 6, 2017.\n\nGenerating highquality crowd density maps using contextual pyramid CNNs. A Vishwanath, Sindagi, M Vishal, Patel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionVishwanath A Sindagi and Vishal M Patel. Generating high- quality crowd density maps using contextual pyramid CNNs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1861-1870, 2017.\n\nData-driven crowd understanding: a baseline for a large-scale crowd dataset. Cong Zhang, Kai Kang, Hongsheng Li, Xiaogang Wang, Rong Xie, Xiaokang Yang, IEEE Transactions on Multimedia. 186Cong Zhang, Kai Kang, Hongsheng Li, Xiaogang Wang, Rong Xie, and Xiaokang Yang. Data-driven crowd under- standing: a baseline for a large-scale crowd dataset. IEEE Transactions on Multimedia, 18(6):1048-1061, 2016.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3431-3440, 2015.\n\nStc: A simple to complex framework for weaklysupervised semantic segmentation. Yunchao Wei, Xiaodan Liang, Yunpeng Chen, Xiaohui Shen, Ming-Ming Cheng, Jiashi Feng, Yao Zhao, Shuicheng Yan, IEEE transactions on pattern analysis and machine intelligence. 39Yunchao Wei, Xiaodan Liang, Yunpeng Chen, Xiaohui Shen, Ming-Ming Cheng, Jiashi Feng, Yao Zhao, and Shuicheng Yan. Stc: A simple to complex framework for weakly- supervised semantic segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(11):2314- 2320, 2017.\n\nObject region mining with adversarial erasing: A simple classification to semantic segmentation approach. Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan, IEEE CVPR. Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, and Shuicheng Yan. Object region mining with adversarial erasing: A simple classification to semantic segmentation approach. In IEEE CVPR, 2017.\n\nMulti-scale context aggregation by dilated convolutions. Fisher Yu, Vladlen Koltun, ICLR. Fisher Yu and Vladlen Koltun. Multi-scale context aggrega- tion by dilated convolutions. In ICLR, 2016.\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, IEEE Transactions on Pattern Analysis and Machine Intelligence. 99PPL. C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully con- nected crfs. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, PP(99):1-1, 2017.\n\nShallow and deep convolutional networks for saliency prediction. Junting Pan, Elisa Sayrol, Xavier Giro-I Nieto, Kevin Mcguinness, Noel E O&apos; Connor, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJunting Pan, Elisa Sayrol, Xavier Giro-i Nieto, Kevin McGuinness, and Noel E O'Connor. Shallow and deep con- volutional networks for saliency prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 598-606, 2016.\n\nCaffe: Convolutional architecture for fast feature embedding. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaACMYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM inter- national conference on Multimedia, pages 675-678. ACM, 2014.\n\nGoing deeper with embedded FPGA platform for convolutional neural network. Jiantao Qiu, Jie Wang, Song Yao, Kaiyuan Guo, Boxun Li, Erjin Zhou, Jincheng Yu, Tianqi Tang, Ningyi Xu, Sen Song, Yu Wang, Huazhong Yang, Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA '16. the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA '16New York, NY, USAACMJiantao Qiu, Jie Wang, Song Yao, Kaiyuan Guo, Boxun Li, Erjin Zhou, Jincheng Yu, Tianqi Tang, Ningyi Xu, Sen Song, Yu Wang, and Huazhong Yang. Going deeper with embed- ded FPGA platform for convolutional neural network. In Proceedings of the 2016 ACM/SIGDA International Sympo- sium on Field-Programmable Gate Arrays, FPGA '16, pages 26-35, New York, NY, USA, 2016. ACM.\n\nHigh-performance video content recognition with long-term recurrent convolutional network for FPGA. Xiaofan Zhang, Xinheng Liu, Anand Ramachandran, Chuanhao Zhuge, Shibin Tang, Peng Ouyang, Zuofu Cheng, Kyle Rupnow, Deming Chen, Field Programmable Logic and Applications (FPL. IEEE27th International Conference onXiaofan Zhang, Xinheng Liu, Anand Ramachandran, Chuanhao Zhuge, Shibin Tang, Peng Ouyang, Zuofu Cheng, Kyle Rupnow, and Deming Chen. High-performance video content recognition with long-term recurrent convolutional network for FPGA. In Field Programmable Logic and Appli- cations (FPL), 2017 27th International Conference on, pages 1-4. IEEE, 2017.\n\nMachine learning on FPGAs to face the IoT revolution. Xiaofan Zhang, Anand Ramachandran, Chuanhao Zhuge, Di He, Wei Zuo, Zuofu Cheng, Kyle Rupnow, Deming Chen, 2017 IEEE/ACM International Conference on. IEEEComputer-Aided Design (ICCADXiaofan Zhang, Anand Ramachandran, Chuanhao Zhuge, Di He, Wei Zuo, Zuofu Cheng, Kyle Rupnow, and Deming Chen. Machine learning on FPGAs to face the IoT revolu- tion. In Computer-Aided Design (ICCAD), 2017 IEEE/ACM International Conference on, pages 819-826. IEEE, 2017.\n\nYodann: An ultra-low power convolutional neural network accelerator based on binary weights. Renzo Andri, Lukas Cavigelli, Davide Rossi, Luca Benini, VLSI (ISVLSI). IEEERenzo Andri, Lukas Cavigelli, Davide Rossi, and Luca Benini. Yodann: An ultra-low power convolutional neu- ral network accelerator based on binary weights. In VLSI (ISVLSI), 2016 IEEE Computer Society Annual Symposium on, pages 236-241. IEEE, 2016.\n\nSingle-image crowd counting via multi-column convolutional neural network. Yingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, Yi Ma, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionYingying Zhang, Desen Zhou, Siqin Chen, Shenghua Gao, and Yi Ma. Single-image crowd counting via multi-column convolutional neural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 589-597, 2016.\n\nCrowdnet: a deep convolutional network for dense crowd counting. Lokesh Boominathan, S S Srinivas, R Venkatesh Kruthiventi, Babu, Proceedings of the 2016 ACM on Multimedia Conference. the 2016 ACM on Multimedia ConferenceACMLokesh Boominathan, Srinivas SS Kruthiventi, and R Venkatesh Babu. Crowdnet: a deep convolutional network for dense crowd counting. In Proceedings of the 2016 ACM on Multimedia Conference, pages 640-644. ACM, 2016.\n\nTowards perspective-free object counting with deep learning. Daniel Onoro, - Rubio, Roberto J L\u00f3pez-Sastre , European Conference on Computer Vision. SpringerDaniel Onoro-Rubio and Roberto J L\u00f3pez-Sastre. Towards perspective-free object counting with deep learning. In Eu- ropean Conference on Computer Vision, pages 615-629. Springer, 2016.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nMulti-source multi-scale counting in extremely dense crowd images. Haroon Idrees, Imran Saleemi, Cody Seibert, Mubarak Shah, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHaroon Idrees, Imran Saleemi, Cody Seibert, and Mubarak Shah. Multi-source multi-scale counting in extremely dense crowd images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2547- 2554, 2013.\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. A B Chan, -Sheng John Zhang, N Liang, Vasconcelos, IEEE Conference on Computer Vision and Pattern Recognition. A. B. Chan, Zhang-Sheng John Liang, and N. Vasconce- los. Privacy preserving crowd monitoring: Counting peo- ple without people models or tracking. In 2008 IEEE Con- ference on Computer Vision and Pattern Recognition, pages 1-7, June 2008.\n\nFcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras. Shanghang Zhang, Guanhang Wu, Joao P Costeira, Jose Mf Moura, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionShanghang Zhang, Guanhang Wu, Joao P Costeira, and Jose MF Moura. Fcn-rlstm: Deep spatio-temporal neural networks for vehicle counting in city cameras. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3667-3676, 2017.\n\nShaogang Gong, and Tao Xiang. Crowd counting and profiling: Methodology and evaluation. Ke Chen Change Loy, Chen, Modeling, Simulation and Visual Analysis of Crowds. SpringerChen Change Loy, Ke Chen, Shaogang Gong, and Tao Xi- ang. Crowd counting and profiling: Methodology and eval- uation. In Modeling, Simulation and Visual Analysis of Crowds, pages 347-382. Springer, 2013.\n\nBernt Schiele, and Pietro Perona. Pedestrian detection: An evaluation of the state of the art. Piotr Dollar, Christian Wojek, IEEE transactions on pattern analysis and machine intelligence. 34Piotr Dollar, Christian Wojek, Bernt Schiele, and Pietro Per- ona. Pedestrian detection: An evaluation of the state of the art. IEEE transactions on pattern analysis and machine in- telligence, 34(4):743-761, 2012.\n\nRobust real-time face detection. Paul Viola, J Michael, Jones, International journal of computer vision. 572Paul Viola and Michael J Jones. Robust real-time face detec- tion. International journal of computer vision, 57(2):137- 154, 2004.\n\nHistograms of oriented gradients for human detection. Navneet Dalal, Bill Triggs, Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on. IEEE1Navneet Dalal and Bill Triggs. Histograms of oriented gra- dients for human detection. In Computer Vision and Pat- tern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886-893. IEEE, 2005.\n\nObject detection with discriminatively trained part-based models. F Pedro, Ross B Felzenszwalb, David Girshick, Deva Mcallester, Ramanan, IEEE transactions on pattern analysis and machine intelligence. 32Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. IEEE transactions on pattern analysis and machine intelligence, 32(9):1627-1645, 2010.\n\nBayesian poisson regression for crowd counting. B Antoni, Nuno Chan, Vasconcelos, IEEE 12th International Conference on. IEEEComputer VisionAntoni B Chan and Nuno Vasconcelos. Bayesian poisson re- gression for crowd counting. In Computer Vision, 2009 IEEE 12th International Conference on, pages 545-551. IEEE, 2009.\n\nObject recognition from local scale-invariant features. D G Lowe, Proceedings of the Seventh IEEE International Conference on Computer Vision. the Seventh IEEE International Conference on Computer Vision2D. G. Lowe. Object recognition from local scale-invariant features. In Proceedings of the Seventh IEEE International Conference on Computer Vision, volume 2, pages 1150- 1157 vol.2, 1999.\n\nLearning to count objects in images. Victor Lempitsky, Andrew Zisserman, Advances in Neural Information Processing Systems. Victor Lempitsky and Andrew Zisserman. Learning to count objects in images. In Advances in Neural Information Pro- cessing Systems, pages 1324-1332, 2010.\n\nCount forest: Co-voting uncertain number of targets using random forest for crowd density estimation. Viet-Quoc Pham, Tatsuo Kozakaya, Osamu Yamaguchi, Ryuzo Okada, Computer Vision (ICCV), 2015 IEEE International Conference on. IEEEViet-Quoc Pham, Tatsuo Kozakaya, Osamu Yamaguchi, and Ryuzo Okada. Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation. In Computer Vision (ICCV), 2015 IEEE International Confer- ence on, pages 3253-3261. IEEE, 2015.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in neural information processing systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. In Advances in neural information processing sys- tems, pages 1097-1105, 2012.\n\nXception: Deep learning with depthwise separable convolutions. F Chollet, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). F. Chollet. Xception: Deep learning with depthwise sepa- rable convolutions. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1800-1807, July 2017.\n\nLearning to count with CNN boosting. Elad Walach, Lior Wolf, European Conference on Computer Vision. SpringerElad Walach and Lior Wolf. Learning to count with CNN boosting. In European Conference on Computer Vision, pages 660-676. Springer, 2016.\n\nEnd-to-end crowd counting via joint learning local and global count. Chong Shang, Haizhou Ai, Bo Bai, Image Processing (ICIP), 2016 IEEE International Conference on. IEEEChong Shang, Haizhou Ai, and Bo Bai. End-to-end crowd counting via joint learning local and global count. In Image Processing (ICIP), 2016 IEEE International Conference on, pages 1215-1219. IEEE, 2016.\n\nMark Marsden, Kevin Mcguiness, Suzanne Little, Noel E O&apos; Connor, arXiv:1612.00220Fully convolutional crowd counting on highly congested scenes. arXiv preprintMark Marsden, Kevin McGuiness, Suzanne Little, and Noel E O'Connor. Fully convolutional crowd counting on highly congested scenes. arXiv preprint arXiv:1612.00220, 2016.\n\nCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. A Vishwanath, Sindagi, M Vishal, Patel, Advanced Video and Signal Based Surveillance (AVSS). IEEE14th IEEE International Conference onVishwanath A Sindagi and Vishal M Patel. Cnn-based cas- caded multi-task learning of high-level prior and density es- timation for crowd counting. In Advanced Video and Signal Based Surveillance (AVSS), 2017 14th IEEE International Conference on, pages 1-6. IEEE, 2017.\n\nRethinking atrous convolution for semantic image segmentation. Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam, abs/1706.05587CoRRLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017.\n\nDeconvolutional networks. Dilip Matthew D Zeiler, Krishnan, W Graham, Rob Taylor, Fergus, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEEMatthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. Deconvolutional networks. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2528-2535. IEEE, 2010.\n\nLearning deconvolution network for semantic segmentation. Hyeonwoo Noh, Seunghoon Hong, Bohyung Han, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionHyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1520-1528, 2015.\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, IEEE transactions on image processing. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004.\n\nExtremely overlapping vehicle counting. Roberto Lpez-Sastre Saturnino Maldonado Bascn, Ricardo Guerrero-Gmez-Olmedo, Beatriz Torre-Jimnez, Daniel Ooro-Rubio, Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA). Roberto Lpez-Sastre Saturnino Maldonado Bascn Ricardo Guerrero-Gmez-Olmedo, Beatriz Torre-Jimnez and Daniel Ooro-Rubio. Extremely overlapping vehicle counting. In Iberian Conference on Pattern Recognition and Image Anal- ysis (IbPRIA), 2015.\n\nDropout: A simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, Journal of Machine Learning Research. 15Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929-1958, 2014.\n\nCumulative attribute space for age and crowd density estimation. Ke Chen, Shaogang Gong, Tao Xiang, Chen Change Loy, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKe Chen, Shaogang Gong, Tao Xiang, and Chen Change Loy. Cumulative attribute space for age and crowd density estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2467- 2474, 2013.\n\nLearning to count with regression forest and structured labels. L Fiaschi, U Koethe, R Nair, F A Hamprecht, Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012). the 21st International Conference on Pattern Recognition (ICPR2012)L. Fiaschi, U. Koethe, R. Nair, and F. A. Hamprecht. Learn- ing to count with regression forest and structured labels. In Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012), pages 2685-2688, Nov 2012.\n", "annotations": {"author": "[{\"end\":203,\"start\":95},{\"end\":285,\"start\":204},{\"end\":362,\"start\":286}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":102},{\"end\":217,\"start\":212},{\"end\":297,\"start\":293}]", "author_first_name": "[{\"end\":101,\"start\":95},{\"end\":211,\"start\":204},{\"end\":292,\"start\":286}]", "author_affiliation": "[{\"end\":149,\"start\":106},{\"end\":202,\"start\":151},{\"end\":284,\"start\":241},{\"end\":361,\"start\":318}]", "title": "[{\"end\":92,\"start\":1},{\"end\":454,\"start\":363}]", "venue": null, "abstract": "[{\"end\":1597,\"start\":456}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1669,\"start\":1666},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1671,\"start\":1669},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1673,\"start\":1671},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1675,\"start\":1673},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1677,\"start\":1675},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2018,\"start\":2015},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3126,\"start\":3123},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3128,\"start\":3126},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3130,\"start\":3128},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3133,\"start\":3130},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3136,\"start\":3133},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3204,\"start\":3200},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3352,\"start\":3348},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3364,\"start\":3360},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3367,\"start\":3364},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3370,\"start\":3367},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3386,\"start\":3382},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3616,\"start\":3613},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3618,\"start\":3616},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3621,\"start\":3618},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3624,\"start\":3621},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3627,\"start\":3624},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3878,\"start\":3874},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4045,\"start\":4041},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4493,\"start\":4489},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4985,\"start\":4982},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4987,\"start\":4985},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5343,\"start\":5339},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5678,\"start\":5675},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5764,\"start\":5760},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5795,\"start\":5791},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5817,\"start\":5814},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5899,\"start\":5895},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5984,\"start\":5980},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6063,\"start\":6059},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6426,\"start\":6422},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6976,\"start\":6972},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7201,\"start\":7197},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7330,\"start\":7326},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7374,\"start\":7370},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7611,\"start\":7607},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7711,\"start\":7707},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7761,\"start\":7757},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7876,\"start\":7872},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8117,\"start\":8113},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8389,\"start\":8385},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8640,\"start\":8636},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8643,\"start\":8640},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8646,\"start\":8643},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8693,\"start\":8689},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8822,\"start\":8818},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8974,\"start\":8970},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9110,\"start\":9106},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9187,\"start\":9183},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9345,\"start\":9341},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9467,\"start\":9463},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9758,\"start\":9755},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9894,\"start\":9891},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10367,\"start\":10363},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11898,\"start\":11894},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11900,\"start\":11898},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11902,\"start\":11900},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11925,\"start\":11921},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12113,\"start\":12109},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12322,\"start\":12319},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12475,\"start\":12472},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13119,\"start\":13115},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13122,\"start\":13119},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13125,\"start\":13122},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13857,\"start\":13853},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13860,\"start\":13857},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13863,\"start\":13860},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14175,\"start\":14171},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14178,\"start\":14175},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16313,\"start\":16309},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16391,\"start\":16387},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16929,\"start\":16925},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17296,\"start\":17292},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17337,\"start\":17333},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17362,\"start\":17358},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":17396,\"start\":17392},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17438,\"start\":17435},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17752,\"start\":17748},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18045,\"start\":18041},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18743,\"start\":18739},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19736,\"start\":19732},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20109,\"start\":20105},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20112,\"start\":20109},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20114,\"start\":20112},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20492,\"start\":20488},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20494,\"start\":20492},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20497,\"start\":20494},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20500,\"start\":20497},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20503,\"start\":20500},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20557,\"start\":20554},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20559,\"start\":20557},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21031,\"start\":21027},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21756,\"start\":21753},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22090,\"start\":22086},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22891,\"start\":22887},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23390,\"start\":23387},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24464,\"start\":24460},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24638,\"start\":24634},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25009,\"start\":25006},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25434,\"start\":25433},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25485,\"start\":25481},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":25514,\"start\":25511},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25536,\"start\":25532},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25566,\"start\":25562},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25588,\"start\":25587},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25606,\"start\":25602},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25628,\"start\":25627},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25647,\"start\":25643},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25679,\"start\":25675},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25709,\"start\":25705},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25739,\"start\":25736},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25762,\"start\":25759},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26028,\"start\":26024},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26574,\"start\":26570},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27143,\"start\":27139},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27642,\"start\":27638},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28269,\"start\":28265},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28272,\"start\":28269},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28275,\"start\":28272},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28278,\"start\":28275},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28298,\"start\":28294},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28377,\"start\":28373},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28666,\"start\":28662},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28921,\"start\":28917},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":28948,\"start\":28944},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29797,\"start\":29793},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29813,\"start\":29809},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29832,\"start\":29829},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29843,\"start\":29839},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29861,\"start\":29857},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30032,\"start\":30028},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30267,\"start\":30263},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30505,\"start\":30501},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30733,\"start\":30729},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30964,\"start\":30961},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31216,\"start\":31212},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31450,\"start\":31446},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31738,\"start\":31734},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31936,\"start\":31932}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31841,\"start\":31626},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32093,\"start\":31842},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32259,\"start\":32094},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32343,\"start\":32260},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32516,\"start\":32344},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32682,\"start\":32517},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32739,\"start\":32683},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32934,\"start\":32740},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33539,\"start\":32935}]", "paragraph": "[{\"end\":3521,\"start\":1613},{\"end\":4830,\"start\":3523},{\"end\":6064,\"start\":4832},{\"end\":6363,\"start\":6066},{\"end\":7025,\"start\":6380},{\"end\":7612,\"start\":7056},{\"end\":7907,\"start\":7614},{\"end\":8475,\"start\":7947},{\"end\":9678,\"start\":8500},{\"end\":11476,\"start\":9729},{\"end\":11840,\"start\":11498},{\"end\":13280,\"start\":11864},{\"end\":13436,\"start\":13304},{\"end\":13736,\"start\":13494},{\"end\":15063,\"start\":13738},{\"end\":16087,\"start\":15065},{\"end\":16555,\"start\":16113},{\"end\":17754,\"start\":16557},{\"end\":17962,\"start\":17774},{\"end\":18350,\"start\":17990},{\"end\":18936,\"start\":18409},{\"end\":19558,\"start\":18958},{\"end\":20153,\"start\":19579},{\"end\":20410,\"start\":20200},{\"end\":21032,\"start\":20426},{\"end\":21120,\"start\":21055},{\"end\":21359,\"start\":21199},{\"end\":21573,\"start\":21387},{\"end\":21919,\"start\":21575},{\"end\":23224,\"start\":21956},{\"end\":24358,\"start\":23281},{\"end\":24952,\"start\":24380},{\"end\":25434,\"start\":24981},{\"end\":25986,\"start\":25459},{\"end\":27046,\"start\":26007},{\"end\":27710,\"start\":27066},{\"end\":28197,\"start\":27767},{\"end\":28865,\"start\":28199},{\"end\":29031,\"start\":28867},{\"end\":29648,\"start\":29046},{\"end\":31625,\"start\":29703}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13493,\"start\":13437},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18408,\"start\":18351},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20199,\"start\":20154},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21198,\"start\":21121},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21386,\"start\":21360},{\"attributes\":{\"id\":\"formula_5\"},\"end\":27766,\"start\":27711}]", "table_ref": "[{\"end\":4006,\"start\":3999},{\"end\":16172,\"start\":16165},{\"end\":17480,\"start\":17473},{\"end\":18935,\"start\":18928},{\"end\":19280,\"start\":19273},{\"end\":20816,\"start\":20809},{\"end\":22401,\"start\":22394},{\"end\":23077,\"start\":23070},{\"end\":23722,\"start\":23715},{\"end\":24228,\"start\":24221},{\"end\":24883,\"start\":24876},{\"end\":24951,\"start\":24943},{\"end\":25808,\"start\":25801},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25869,\"start\":25862},{\"end\":26759,\"start\":26752},{\"end\":27045,\"start\":27037},{\"end\":27162,\"start\":27155},{\"end\":28519,\"start\":28511},{\"end\":28864,\"start\":28856},{\"end\":28968,\"start\":28960}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1611,\"start\":1599},{\"attributes\":{\"n\":\"2.\"},\"end\":6378,\"start\":6366},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7054,\"start\":7028},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7945,\"start\":7910},{\"attributes\":{\"n\":\"2.4.\"},\"end\":8498,\"start\":8478},{\"attributes\":{\"n\":\"2.5.\"},\"end\":9727,\"start\":9681},{\"attributes\":{\"n\":\"3.\"},\"end\":11496,\"start\":11479},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11862,\"start\":11843},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":13302,\"start\":13283},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":16111,\"start\":16090},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17772,\"start\":17757},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":17988,\"start\":17965},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":18956,\"start\":18939},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":19577,\"start\":19561},{\"attributes\":{\"n\":\"4.\"},\"end\":20424,\"start\":20413},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21053,\"start\":21035},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21954,\"start\":21922},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23279,\"start\":23227},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":24378,\"start\":24361},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":24979,\"start\":24955},{\"end\":25448,\"start\":25437},{\"end\":25457,\"start\":25451},{\"attributes\":{\"n\":\"4.3.4\"},\"end\":26005,\"start\":25989},{\"attributes\":{\"n\":\"4.3.5\"},\"end\":27064,\"start\":27049},{\"attributes\":{\"n\":\"5.\"},\"end\":29044,\"start\":29034},{\"attributes\":{\"n\":\"6.\"},\"end\":29666,\"start\":29651},{\"attributes\":{\"n\":\"7.\"},\"end\":29701,\"start\":29669},{\"end\":31637,\"start\":31627},{\"end\":31853,\"start\":31843},{\"end\":32271,\"start\":32261},{\"end\":32355,\"start\":32345},{\"end\":32693,\"start\":32684}]", "table": "[{\"end\":32934,\"start\":32765},{\"end\":33539,\"start\":33177}]", "figure_caption": "[{\"end\":31841,\"start\":31639},{\"end\":32093,\"start\":31855},{\"end\":32259,\"start\":32096},{\"end\":32343,\"start\":32273},{\"end\":32516,\"start\":32357},{\"end\":32682,\"start\":32519},{\"end\":32739,\"start\":32695},{\"end\":32765,\"start\":32742},{\"end\":33177,\"start\":32937}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2184,\"start\":2178},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4318,\"start\":4312},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14391,\"start\":14384},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14927,\"start\":14921},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15279,\"start\":15273},{\"end\":24199,\"start\":24193},{\"end\":24670,\"start\":24662},{\"end\":28555,\"start\":28549},{\"end\":30142,\"start\":30136},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30153,\"start\":30146},{\"end\":30208,\"start\":30200},{\"end\":30446,\"start\":30438},{\"end\":30684,\"start\":30676},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30913,\"start\":30904},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31172,\"start\":31163},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31403,\"start\":31394}]", "bib_author_first_name": "[{\"end\":33923,\"start\":33922},{\"end\":33938,\"start\":33933},{\"end\":33962,\"start\":33961},{\"end\":33977,\"start\":33971},{\"end\":34202,\"start\":34198},{\"end\":34211,\"start\":34207},{\"end\":34223,\"start\":34219},{\"end\":34238,\"start\":34230},{\"end\":34250,\"start\":34243},{\"end\":34266,\"start\":34257},{\"end\":34609,\"start\":34605},{\"end\":34626,\"start\":34617},{\"end\":34639,\"start\":34631},{\"end\":34654,\"start\":34646},{\"end\":35096,\"start\":35092},{\"end\":35125,\"start\":35114},{\"end\":35573,\"start\":35572},{\"end\":35596,\"start\":35595},{\"end\":36056,\"start\":36052},{\"end\":36067,\"start\":36064},{\"end\":36083,\"start\":36074},{\"end\":36096,\"start\":36088},{\"end\":36107,\"start\":36103},{\"end\":36121,\"start\":36113},{\"end\":36444,\"start\":36436},{\"end\":36455,\"start\":36451},{\"end\":36473,\"start\":36467},{\"end\":36925,\"start\":36918},{\"end\":36938,\"start\":36931},{\"end\":36953,\"start\":36946},{\"end\":36967,\"start\":36960},{\"end\":36983,\"start\":36974},{\"end\":36997,\"start\":36991},{\"end\":37007,\"start\":37004},{\"end\":37023,\"start\":37014},{\"end\":37494,\"start\":37487},{\"end\":37506,\"start\":37500},{\"end\":37520,\"start\":37513},{\"end\":37537,\"start\":37528},{\"end\":37548,\"start\":37545},{\"end\":37564,\"start\":37555},{\"end\":37858,\"start\":37852},{\"end\":37870,\"start\":37863},{\"end\":38104,\"start\":38103},{\"end\":38106,\"start\":38105},{\"end\":38114,\"start\":38113},{\"end\":38128,\"start\":38127},{\"end\":38140,\"start\":38139},{\"end\":38150,\"start\":38149},{\"end\":38152,\"start\":38151},{\"end\":38570,\"start\":38563},{\"end\":38581,\"start\":38576},{\"end\":38596,\"start\":38590},{\"end\":38616,\"start\":38611},{\"end\":38643,\"start\":38629},{\"end\":39121,\"start\":39113},{\"end\":39131,\"start\":39127},{\"end\":39147,\"start\":39143},{\"end\":39163,\"start\":39157},{\"end\":39181,\"start\":39173},{\"end\":39192,\"start\":39188},{\"end\":39209,\"start\":39203},{\"end\":39228,\"start\":39222},{\"end\":39733,\"start\":39726},{\"end\":39742,\"start\":39739},{\"end\":39753,\"start\":39749},{\"end\":39766,\"start\":39759},{\"end\":39777,\"start\":39772},{\"end\":39787,\"start\":39782},{\"end\":39802,\"start\":39794},{\"end\":39813,\"start\":39807},{\"end\":39826,\"start\":39820},{\"end\":39834,\"start\":39831},{\"end\":39843,\"start\":39841},{\"end\":39858,\"start\":39850},{\"end\":40553,\"start\":40546},{\"end\":40568,\"start\":40561},{\"end\":40579,\"start\":40574},{\"end\":40602,\"start\":40594},{\"end\":40616,\"start\":40610},{\"end\":40627,\"start\":40623},{\"end\":40641,\"start\":40636},{\"end\":40653,\"start\":40649},{\"end\":40668,\"start\":40662},{\"end\":41170,\"start\":41163},{\"end\":41183,\"start\":41178},{\"end\":41206,\"start\":41198},{\"end\":41216,\"start\":41214},{\"end\":41224,\"start\":41221},{\"end\":41235,\"start\":41230},{\"end\":41247,\"start\":41243},{\"end\":41262,\"start\":41256},{\"end\":41713,\"start\":41708},{\"end\":41726,\"start\":41721},{\"end\":41744,\"start\":41738},{\"end\":41756,\"start\":41752},{\"end\":42117,\"start\":42109},{\"end\":42130,\"start\":42125},{\"end\":42142,\"start\":42137},{\"end\":42157,\"start\":42149},{\"end\":42165,\"start\":42163},{\"end\":42626,\"start\":42620},{\"end\":42641,\"start\":42640},{\"end\":42643,\"start\":42642},{\"end\":42665,\"start\":42654},{\"end\":43062,\"start\":43056},{\"end\":43071,\"start\":43070},{\"end\":43101,\"start\":43079},{\"end\":43410,\"start\":43405},{\"end\":43427,\"start\":43421},{\"end\":43687,\"start\":43681},{\"end\":43701,\"start\":43696},{\"end\":43715,\"start\":43711},{\"end\":43732,\"start\":43725},{\"end\":44205,\"start\":44204},{\"end\":44207,\"start\":44206},{\"end\":44225,\"start\":44214},{\"end\":44234,\"start\":44233},{\"end\":44651,\"start\":44642},{\"end\":44667,\"start\":44659},{\"end\":44676,\"start\":44672},{\"end\":44678,\"start\":44677},{\"end\":44696,\"start\":44689},{\"end\":45195,\"start\":45193},{\"end\":45584,\"start\":45579},{\"end\":45602,\"start\":45593},{\"end\":45929,\"start\":45925},{\"end\":45938,\"start\":45937},{\"end\":46193,\"start\":46186},{\"end\":46205,\"start\":46201},{\"end\":46606,\"start\":46605},{\"end\":46618,\"start\":46614},{\"end\":46620,\"start\":46619},{\"end\":46640,\"start\":46635},{\"end\":46655,\"start\":46651},{\"end\":47021,\"start\":47020},{\"end\":47034,\"start\":47030},{\"end\":47347,\"start\":47346},{\"end\":47349,\"start\":47348},{\"end\":47726,\"start\":47720},{\"end\":47744,\"start\":47738},{\"end\":48074,\"start\":48065},{\"end\":48087,\"start\":48081},{\"end\":48103,\"start\":48098},{\"end\":48120,\"start\":48115},{\"end\":48531,\"start\":48527},{\"end\":48548,\"start\":48544},{\"end\":48568,\"start\":48560},{\"end\":48570,\"start\":48569},{\"end\":48897,\"start\":48896},{\"end\":49201,\"start\":49197},{\"end\":49214,\"start\":49210},{\"end\":49482,\"start\":49477},{\"end\":49497,\"start\":49490},{\"end\":49504,\"start\":49502},{\"end\":49785,\"start\":49781},{\"end\":49800,\"start\":49795},{\"end\":49819,\"start\":49812},{\"end\":49842,\"start\":49828},{\"end\":50218,\"start\":50217},{\"end\":50241,\"start\":50240},{\"end\":50696,\"start\":50685},{\"end\":50709,\"start\":50703},{\"end\":50729,\"start\":50722},{\"end\":50746,\"start\":50739},{\"end\":50966,\"start\":50961},{\"end\":50996,\"start\":50995},{\"end\":51008,\"start\":51005},{\"end\":51367,\"start\":51359},{\"end\":51382,\"start\":51373},{\"end\":51396,\"start\":51389},{\"end\":51802,\"start\":51798},{\"end\":51813,\"start\":51809},{\"end\":51815,\"start\":51814},{\"end\":51824,\"start\":51823},{\"end\":51838,\"start\":51832},{\"end\":52149,\"start\":52142},{\"end\":52196,\"start\":52189},{\"end\":52226,\"start\":52219},{\"end\":52247,\"start\":52241},{\"end\":52647,\"start\":52641},{\"end\":52668,\"start\":52660},{\"end\":52681,\"start\":52677},{\"end\":52698,\"start\":52694},{\"end\":52716,\"start\":52710},{\"end\":53060,\"start\":53058},{\"end\":53075,\"start\":53067},{\"end\":53085,\"start\":53082},{\"end\":53104,\"start\":53093},{\"end\":53544,\"start\":53543},{\"end\":53555,\"start\":53554},{\"end\":53565,\"start\":53564},{\"end\":53573,\"start\":53572},{\"end\":53575,\"start\":53574}]", "bib_author_last_name": "[{\"end\":33920,\"start\":33909},{\"end\":33931,\"start\":33924},{\"end\":33948,\"start\":33939},{\"end\":33959,\"start\":33950},{\"end\":33969,\"start\":33963},{\"end\":33986,\"start\":33978},{\"end\":33990,\"start\":33988},{\"end\":34205,\"start\":34203},{\"end\":34217,\"start\":34212},{\"end\":34228,\"start\":34224},{\"end\":34241,\"start\":34239},{\"end\":34255,\"start\":34251},{\"end\":34270,\"start\":34267},{\"end\":34615,\"start\":34610},{\"end\":34629,\"start\":34627},{\"end\":34644,\"start\":34640},{\"end\":34659,\"start\":34655},{\"end\":35112,\"start\":35097},{\"end\":35131,\"start\":35126},{\"end\":35137,\"start\":35133},{\"end\":35584,\"start\":35574},{\"end\":35593,\"start\":35586},{\"end\":35603,\"start\":35597},{\"end\":35610,\"start\":35605},{\"end\":36062,\"start\":36057},{\"end\":36072,\"start\":36068},{\"end\":36086,\"start\":36084},{\"end\":36101,\"start\":36097},{\"end\":36111,\"start\":36108},{\"end\":36126,\"start\":36122},{\"end\":36449,\"start\":36445},{\"end\":36465,\"start\":36456},{\"end\":36481,\"start\":36474},{\"end\":36929,\"start\":36926},{\"end\":36944,\"start\":36939},{\"end\":36958,\"start\":36954},{\"end\":36972,\"start\":36968},{\"end\":36989,\"start\":36984},{\"end\":37002,\"start\":36998},{\"end\":37012,\"start\":37008},{\"end\":37027,\"start\":37024},{\"end\":37498,\"start\":37495},{\"end\":37511,\"start\":37507},{\"end\":37526,\"start\":37521},{\"end\":37543,\"start\":37538},{\"end\":37553,\"start\":37549},{\"end\":37568,\"start\":37565},{\"end\":37861,\"start\":37859},{\"end\":37877,\"start\":37871},{\"end\":38111,\"start\":38107},{\"end\":38125,\"start\":38115},{\"end\":38137,\"start\":38129},{\"end\":38147,\"start\":38141},{\"end\":38159,\"start\":38153},{\"end\":38574,\"start\":38571},{\"end\":38588,\"start\":38582},{\"end\":38609,\"start\":38597},{\"end\":38627,\"start\":38617},{\"end\":38650,\"start\":38644},{\"end\":39125,\"start\":39122},{\"end\":39141,\"start\":39132},{\"end\":39155,\"start\":39148},{\"end\":39171,\"start\":39164},{\"end\":39186,\"start\":39182},{\"end\":39201,\"start\":39193},{\"end\":39220,\"start\":39210},{\"end\":39236,\"start\":39229},{\"end\":39737,\"start\":39734},{\"end\":39747,\"start\":39743},{\"end\":39757,\"start\":39754},{\"end\":39770,\"start\":39767},{\"end\":39780,\"start\":39778},{\"end\":39792,\"start\":39788},{\"end\":39805,\"start\":39803},{\"end\":39818,\"start\":39814},{\"end\":39829,\"start\":39827},{\"end\":39839,\"start\":39835},{\"end\":39848,\"start\":39844},{\"end\":39863,\"start\":39859},{\"end\":40559,\"start\":40554},{\"end\":40572,\"start\":40569},{\"end\":40592,\"start\":40580},{\"end\":40608,\"start\":40603},{\"end\":40621,\"start\":40617},{\"end\":40634,\"start\":40628},{\"end\":40647,\"start\":40642},{\"end\":40660,\"start\":40654},{\"end\":40673,\"start\":40669},{\"end\":41176,\"start\":41171},{\"end\":41196,\"start\":41184},{\"end\":41212,\"start\":41207},{\"end\":41219,\"start\":41217},{\"end\":41228,\"start\":41225},{\"end\":41241,\"start\":41236},{\"end\":41254,\"start\":41248},{\"end\":41267,\"start\":41263},{\"end\":41719,\"start\":41714},{\"end\":41736,\"start\":41727},{\"end\":41750,\"start\":41745},{\"end\":41763,\"start\":41757},{\"end\":42123,\"start\":42118},{\"end\":42135,\"start\":42131},{\"end\":42147,\"start\":42143},{\"end\":42161,\"start\":42158},{\"end\":42168,\"start\":42166},{\"end\":42638,\"start\":42627},{\"end\":42652,\"start\":42644},{\"end\":42677,\"start\":42666},{\"end\":42683,\"start\":42679},{\"end\":43068,\"start\":43063},{\"end\":43077,\"start\":43072},{\"end\":43419,\"start\":43411},{\"end\":43437,\"start\":43428},{\"end\":43694,\"start\":43688},{\"end\":43709,\"start\":43702},{\"end\":43723,\"start\":43716},{\"end\":43737,\"start\":43733},{\"end\":44212,\"start\":44208},{\"end\":44231,\"start\":44226},{\"end\":44240,\"start\":44235},{\"end\":44253,\"start\":44242},{\"end\":44657,\"start\":44652},{\"end\":44670,\"start\":44668},{\"end\":44687,\"start\":44679},{\"end\":44702,\"start\":44697},{\"end\":45211,\"start\":45196},{\"end\":45217,\"start\":45213},{\"end\":45591,\"start\":45585},{\"end\":45608,\"start\":45603},{\"end\":45935,\"start\":45930},{\"end\":45946,\"start\":45939},{\"end\":45953,\"start\":45948},{\"end\":46199,\"start\":46194},{\"end\":46212,\"start\":46206},{\"end\":46612,\"start\":46607},{\"end\":46633,\"start\":46621},{\"end\":46649,\"start\":46641},{\"end\":46666,\"start\":46656},{\"end\":46675,\"start\":46668},{\"end\":47028,\"start\":47022},{\"end\":47039,\"start\":47035},{\"end\":47052,\"start\":47041},{\"end\":47354,\"start\":47350},{\"end\":47736,\"start\":47727},{\"end\":47754,\"start\":47745},{\"end\":48079,\"start\":48075},{\"end\":48096,\"start\":48088},{\"end\":48113,\"start\":48104},{\"end\":48126,\"start\":48121},{\"end\":48542,\"start\":48532},{\"end\":48558,\"start\":48549},{\"end\":48577,\"start\":48571},{\"end\":48905,\"start\":48898},{\"end\":49208,\"start\":49202},{\"end\":49219,\"start\":49215},{\"end\":49488,\"start\":49483},{\"end\":49500,\"start\":49498},{\"end\":49508,\"start\":49505},{\"end\":49793,\"start\":49786},{\"end\":49810,\"start\":49801},{\"end\":49826,\"start\":49820},{\"end\":49849,\"start\":49843},{\"end\":50229,\"start\":50219},{\"end\":50238,\"start\":50231},{\"end\":50248,\"start\":50242},{\"end\":50255,\"start\":50250},{\"end\":50701,\"start\":50697},{\"end\":50720,\"start\":50710},{\"end\":50737,\"start\":50730},{\"end\":50751,\"start\":50747},{\"end\":50983,\"start\":50967},{\"end\":50993,\"start\":50985},{\"end\":51003,\"start\":50997},{\"end\":51015,\"start\":51009},{\"end\":51023,\"start\":51017},{\"end\":51371,\"start\":51368},{\"end\":51387,\"start\":51383},{\"end\":51400,\"start\":51397},{\"end\":51807,\"start\":51803},{\"end\":51821,\"start\":51816},{\"end\":51830,\"start\":51825},{\"end\":51845,\"start\":51839},{\"end\":51857,\"start\":51847},{\"end\":52187,\"start\":52150},{\"end\":52217,\"start\":52197},{\"end\":52239,\"start\":52227},{\"end\":52258,\"start\":52248},{\"end\":52658,\"start\":52648},{\"end\":52675,\"start\":52669},{\"end\":52692,\"start\":52682},{\"end\":52708,\"start\":52699},{\"end\":52730,\"start\":52717},{\"end\":53065,\"start\":53061},{\"end\":53080,\"start\":53076},{\"end\":53091,\"start\":53086},{\"end\":53108,\"start\":53105},{\"end\":53552,\"start\":53545},{\"end\":53562,\"start\":53556},{\"end\":53570,\"start\":53566},{\"end\":53585,\"start\":53576}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":34162,\"start\":33850},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4915696},\"end\":34536,\"start\":34164},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2131202},\"end\":35031,\"start\":34538},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1089358},\"end\":35497,\"start\":35033},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2099022},\"end\":35973,\"start\":35499},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":4236070},\"end\":36378,\"start\":35975},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1629541},\"end\":36837,\"start\":36380},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1821165},\"end\":37379,\"start\":36839},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6793190},\"end\":37793,\"start\":37381},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":17127188},\"end\":37988,\"start\":37795},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3429309},\"end\":38496,\"start\":37990},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16408631},\"end\":39049,\"start\":38498},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1799558},\"end\":39649,\"start\":39051},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":207233273},\"end\":40444,\"start\":39651},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":36681062},\"end\":41107,\"start\":40446},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4700762},\"end\":41613,\"start\":41109},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10098855},\"end\":42032,\"start\":41615},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4545310},\"end\":42553,\"start\":42034},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":697405},\"end\":42993,\"start\":42555},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":40499053},\"end\":43335,\"start\":42995},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b20\"},\"end\":43612,\"start\":43337},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":9749221},\"end\":44114,\"start\":43614},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9059102},\"end\":44554,\"start\":44116},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10553647},\"end\":45103,\"start\":44556},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":14034098},\"end\":45482,\"start\":45105},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206764948},\"end\":45890,\"start\":45484},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2796017},\"end\":46130,\"start\":45892},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206590483},\"end\":46537,\"start\":46132},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3198903},\"end\":46970,\"start\":46539},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":749620},\"end\":47288,\"start\":46972},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5258236},\"end\":47681,\"start\":47290},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":18018217},\"end\":47961,\"start\":47683},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1718465},\"end\":48460,\"start\":47963},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":195908774},\"end\":48831,\"start\":48462},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2375110},\"end\":49158,\"start\":48833},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":26569197},\"end\":49406,\"start\":49160},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":7738307},\"end\":49779,\"start\":49408},{\"attributes\":{\"doi\":\"arXiv:1612.00220\",\"id\":\"b37\"},\"end\":50113,\"start\":49781},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3003101},\"end\":50620,\"start\":50115},{\"attributes\":{\"doi\":\"abs/1706.05587\",\"id\":\"b39\"},\"end\":50933,\"start\":50622},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9893011},\"end\":51299,\"start\":50935},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":623137},\"end\":51722,\"start\":51301},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":207761262},\"end\":52100,\"start\":51724},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":16958950},\"end\":52572,\"start\":52102},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":6844431},\"end\":52991,\"start\":52574},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":8747356},\"end\":53477,\"start\":52993},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":16358367},\"end\":53970,\"start\":53479}]", "bib_title": "[{\"end\":34196,\"start\":34164},{\"end\":34603,\"start\":34538},{\"end\":35090,\"start\":35033},{\"end\":35570,\"start\":35499},{\"end\":36050,\"start\":35975},{\"end\":36434,\"start\":36380},{\"end\":36916,\"start\":36839},{\"end\":37485,\"start\":37381},{\"end\":37850,\"start\":37795},{\"end\":38101,\"start\":37990},{\"end\":38561,\"start\":38498},{\"end\":39111,\"start\":39051},{\"end\":39724,\"start\":39651},{\"end\":40544,\"start\":40446},{\"end\":41161,\"start\":41109},{\"end\":41706,\"start\":41615},{\"end\":42107,\"start\":42034},{\"end\":42618,\"start\":42555},{\"end\":43054,\"start\":42995},{\"end\":43679,\"start\":43614},{\"end\":44202,\"start\":44116},{\"end\":44640,\"start\":44556},{\"end\":45191,\"start\":45105},{\"end\":45577,\"start\":45484},{\"end\":45923,\"start\":45892},{\"end\":46184,\"start\":46132},{\"end\":46603,\"start\":46539},{\"end\":47018,\"start\":46972},{\"end\":47344,\"start\":47290},{\"end\":47718,\"start\":47683},{\"end\":48063,\"start\":47963},{\"end\":48525,\"start\":48462},{\"end\":48894,\"start\":48833},{\"end\":49195,\"start\":49160},{\"end\":49475,\"start\":49408},{\"end\":50215,\"start\":50115},{\"end\":50959,\"start\":50935},{\"end\":51357,\"start\":51301},{\"end\":51796,\"start\":51724},{\"end\":52140,\"start\":52102},{\"end\":52639,\"start\":52574},{\"end\":53056,\"start\":52993},{\"end\":53541,\"start\":53479}]", "bib_author": "[{\"end\":33922,\"start\":33909},{\"end\":33933,\"start\":33922},{\"end\":33950,\"start\":33933},{\"end\":33961,\"start\":33950},{\"end\":33971,\"start\":33961},{\"end\":33988,\"start\":33971},{\"end\":33992,\"start\":33988},{\"end\":34207,\"start\":34198},{\"end\":34219,\"start\":34207},{\"end\":34230,\"start\":34219},{\"end\":34243,\"start\":34230},{\"end\":34257,\"start\":34243},{\"end\":34272,\"start\":34257},{\"end\":34617,\"start\":34605},{\"end\":34631,\"start\":34617},{\"end\":34646,\"start\":34631},{\"end\":34661,\"start\":34646},{\"end\":35114,\"start\":35092},{\"end\":35133,\"start\":35114},{\"end\":35139,\"start\":35133},{\"end\":35586,\"start\":35572},{\"end\":35595,\"start\":35586},{\"end\":35605,\"start\":35595},{\"end\":35612,\"start\":35605},{\"end\":36064,\"start\":36052},{\"end\":36074,\"start\":36064},{\"end\":36088,\"start\":36074},{\"end\":36103,\"start\":36088},{\"end\":36113,\"start\":36103},{\"end\":36128,\"start\":36113},{\"end\":36451,\"start\":36436},{\"end\":36467,\"start\":36451},{\"end\":36483,\"start\":36467},{\"end\":36931,\"start\":36918},{\"end\":36946,\"start\":36931},{\"end\":36960,\"start\":36946},{\"end\":36974,\"start\":36960},{\"end\":36991,\"start\":36974},{\"end\":37004,\"start\":36991},{\"end\":37014,\"start\":37004},{\"end\":37029,\"start\":37014},{\"end\":37500,\"start\":37487},{\"end\":37513,\"start\":37500},{\"end\":37528,\"start\":37513},{\"end\":37545,\"start\":37528},{\"end\":37555,\"start\":37545},{\"end\":37570,\"start\":37555},{\"end\":37863,\"start\":37852},{\"end\":37879,\"start\":37863},{\"end\":38113,\"start\":38103},{\"end\":38127,\"start\":38113},{\"end\":38139,\"start\":38127},{\"end\":38149,\"start\":38139},{\"end\":38161,\"start\":38149},{\"end\":38576,\"start\":38563},{\"end\":38590,\"start\":38576},{\"end\":38611,\"start\":38590},{\"end\":38629,\"start\":38611},{\"end\":38652,\"start\":38629},{\"end\":39127,\"start\":39113},{\"end\":39143,\"start\":39127},{\"end\":39157,\"start\":39143},{\"end\":39173,\"start\":39157},{\"end\":39188,\"start\":39173},{\"end\":39203,\"start\":39188},{\"end\":39222,\"start\":39203},{\"end\":39238,\"start\":39222},{\"end\":39739,\"start\":39726},{\"end\":39749,\"start\":39739},{\"end\":39759,\"start\":39749},{\"end\":39772,\"start\":39759},{\"end\":39782,\"start\":39772},{\"end\":39794,\"start\":39782},{\"end\":39807,\"start\":39794},{\"end\":39820,\"start\":39807},{\"end\":39831,\"start\":39820},{\"end\":39841,\"start\":39831},{\"end\":39850,\"start\":39841},{\"end\":39865,\"start\":39850},{\"end\":40561,\"start\":40546},{\"end\":40574,\"start\":40561},{\"end\":40594,\"start\":40574},{\"end\":40610,\"start\":40594},{\"end\":40623,\"start\":40610},{\"end\":40636,\"start\":40623},{\"end\":40649,\"start\":40636},{\"end\":40662,\"start\":40649},{\"end\":40675,\"start\":40662},{\"end\":41178,\"start\":41163},{\"end\":41198,\"start\":41178},{\"end\":41214,\"start\":41198},{\"end\":41221,\"start\":41214},{\"end\":41230,\"start\":41221},{\"end\":41243,\"start\":41230},{\"end\":41256,\"start\":41243},{\"end\":41269,\"start\":41256},{\"end\":41721,\"start\":41708},{\"end\":41738,\"start\":41721},{\"end\":41752,\"start\":41738},{\"end\":41765,\"start\":41752},{\"end\":42125,\"start\":42109},{\"end\":42137,\"start\":42125},{\"end\":42149,\"start\":42137},{\"end\":42163,\"start\":42149},{\"end\":42170,\"start\":42163},{\"end\":42640,\"start\":42620},{\"end\":42654,\"start\":42640},{\"end\":42679,\"start\":42654},{\"end\":42685,\"start\":42679},{\"end\":43070,\"start\":43056},{\"end\":43079,\"start\":43070},{\"end\":43104,\"start\":43079},{\"end\":43421,\"start\":43405},{\"end\":43439,\"start\":43421},{\"end\":43696,\"start\":43681},{\"end\":43711,\"start\":43696},{\"end\":43725,\"start\":43711},{\"end\":43739,\"start\":43725},{\"end\":44214,\"start\":44204},{\"end\":44233,\"start\":44214},{\"end\":44242,\"start\":44233},{\"end\":44255,\"start\":44242},{\"end\":44659,\"start\":44642},{\"end\":44672,\"start\":44659},{\"end\":44689,\"start\":44672},{\"end\":44704,\"start\":44689},{\"end\":45213,\"start\":45193},{\"end\":45219,\"start\":45213},{\"end\":45593,\"start\":45579},{\"end\":45610,\"start\":45593},{\"end\":45937,\"start\":45925},{\"end\":45948,\"start\":45937},{\"end\":45955,\"start\":45948},{\"end\":46201,\"start\":46186},{\"end\":46214,\"start\":46201},{\"end\":46614,\"start\":46605},{\"end\":46635,\"start\":46614},{\"end\":46651,\"start\":46635},{\"end\":46668,\"start\":46651},{\"end\":46677,\"start\":46668},{\"end\":47030,\"start\":47020},{\"end\":47041,\"start\":47030},{\"end\":47054,\"start\":47041},{\"end\":47356,\"start\":47346},{\"end\":47738,\"start\":47720},{\"end\":47756,\"start\":47738},{\"end\":48081,\"start\":48065},{\"end\":48098,\"start\":48081},{\"end\":48115,\"start\":48098},{\"end\":48128,\"start\":48115},{\"end\":48544,\"start\":48527},{\"end\":48560,\"start\":48544},{\"end\":48579,\"start\":48560},{\"end\":48907,\"start\":48896},{\"end\":49210,\"start\":49197},{\"end\":49221,\"start\":49210},{\"end\":49490,\"start\":49477},{\"end\":49502,\"start\":49490},{\"end\":49510,\"start\":49502},{\"end\":49795,\"start\":49781},{\"end\":49812,\"start\":49795},{\"end\":49828,\"start\":49812},{\"end\":49851,\"start\":49828},{\"end\":50231,\"start\":50217},{\"end\":50240,\"start\":50231},{\"end\":50250,\"start\":50240},{\"end\":50257,\"start\":50250},{\"end\":50703,\"start\":50685},{\"end\":50722,\"start\":50703},{\"end\":50739,\"start\":50722},{\"end\":50753,\"start\":50739},{\"end\":50985,\"start\":50961},{\"end\":50995,\"start\":50985},{\"end\":51005,\"start\":50995},{\"end\":51017,\"start\":51005},{\"end\":51025,\"start\":51017},{\"end\":51373,\"start\":51359},{\"end\":51389,\"start\":51373},{\"end\":51402,\"start\":51389},{\"end\":51809,\"start\":51798},{\"end\":51823,\"start\":51809},{\"end\":51832,\"start\":51823},{\"end\":51847,\"start\":51832},{\"end\":51859,\"start\":51847},{\"end\":52189,\"start\":52142},{\"end\":52219,\"start\":52189},{\"end\":52241,\"start\":52219},{\"end\":52260,\"start\":52241},{\"end\":52660,\"start\":52641},{\"end\":52677,\"start\":52660},{\"end\":52694,\"start\":52677},{\"end\":52710,\"start\":52694},{\"end\":52732,\"start\":52710},{\"end\":53067,\"start\":53058},{\"end\":53082,\"start\":53067},{\"end\":53093,\"start\":53082},{\"end\":53110,\"start\":53093},{\"end\":53554,\"start\":53543},{\"end\":53564,\"start\":53554},{\"end\":53572,\"start\":53564},{\"end\":53587,\"start\":53572}]", "bib_venue": "[{\"end\":34802,\"start\":34740},{\"end\":35280,\"start\":35218},{\"end\":35753,\"start\":35691},{\"end\":36624,\"start\":36562},{\"end\":38793,\"start\":38731},{\"end\":39357,\"start\":39306},{\"end\":40071,\"start\":39968},{\"end\":42311,\"start\":42249},{\"end\":42776,\"start\":42739},{\"end\":43880,\"start\":43818},{\"end\":44845,\"start\":44783},{\"end\":47493,\"start\":47433},{\"end\":51523,\"start\":51471},{\"end\":53251,\"start\":53189},{\"end\":53738,\"start\":53671},{\"end\":33907,\"start\":33850},{\"end\":34334,\"start\":34272},{\"end\":34738,\"start\":34661},{\"end\":35216,\"start\":35139},{\"end\":35689,\"start\":35612},{\"end\":36159,\"start\":36128},{\"end\":36560,\"start\":36483},{\"end\":37091,\"start\":37029},{\"end\":37579,\"start\":37570},{\"end\":37883,\"start\":37879},{\"end\":38223,\"start\":38161},{\"end\":38729,\"start\":38652},{\"end\":39304,\"start\":39238},{\"end\":39966,\"start\":39865},{\"end\":40721,\"start\":40675},{\"end\":41310,\"start\":41269},{\"end\":41778,\"start\":41765},{\"end\":42247,\"start\":42170},{\"end\":42737,\"start\":42685},{\"end\":43142,\"start\":43104},{\"end\":43403,\"start\":43337},{\"end\":43816,\"start\":43739},{\"end\":44313,\"start\":44255},{\"end\":44781,\"start\":44704},{\"end\":45269,\"start\":45219},{\"end\":45672,\"start\":45610},{\"end\":45995,\"start\":45955},{\"end\":46307,\"start\":46214},{\"end\":46739,\"start\":46677},{\"end\":47091,\"start\":47054},{\"end\":47431,\"start\":47356},{\"end\":47805,\"start\":47756},{\"end\":48189,\"start\":48128},{\"end\":48628,\"start\":48579},{\"end\":48977,\"start\":48907},{\"end\":49259,\"start\":49221},{\"end\":49572,\"start\":49510},{\"end\":49928,\"start\":49867},{\"end\":50308,\"start\":50257},{\"end\":50683,\"start\":50622},{\"end\":51096,\"start\":51025},{\"end\":51469,\"start\":51402},{\"end\":51896,\"start\":51859},{\"end\":52329,\"start\":52260},{\"end\":52768,\"start\":52732},{\"end\":53187,\"start\":53110},{\"end\":53669,\"start\":53587}]"}}}, "year": 2023, "month": 12, "day": 17}