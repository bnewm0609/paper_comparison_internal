{"id": 248571530, "updated": "2023-10-05 14:42:37.551", "metadata": {"title": "Non-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo", "authors": "[{\"first\":\"Jiayu\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Jose\",\"last\":\"Alvarez\",\"middle\":[\"M.\"]},{\"first\":\"Miaomiao\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Recent cost volume pyramid based deep neural networks have unlocked the potential of efficiently leveraging high-resolution images for depth inference from multi-view stereo. In general, those approaches assume that the depth of each pixel follows a unimodal distribution. Boundary pixels usually follow a multi-modal distribution as they represent different depths; Therefore, the assumption results in an erroneous depth prediction at the coarser level of the cost volume pyramid and can not be corrected in the refinement levels leading to wrong depth predictions. In contrast, we propose constructing the cost volume by non-parametric depth distribution modeling to handle pixels with unimodal and multi-modal distributions. Our approach outputs multiple depth hypotheses at the coarser level to avoid errors in the early stage. As we perform local search around these multiple hypotheses in subsequent levels, our approach does not maintain the rigid depth spatial ordering and, therefore, we introduce a sparse cost aggregation network to derive information within each volume. We evaluate our approach extensively on two benchmark datasets: DTU and Tanks&Temples. Our experimental results show that our model outperforms existing methods by a large margin and achieves superior performance on boundary regions. Code is available at https://github.com/NVlabs/NP-CVP-MVSNet", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.03783", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/YangA022", "doi": "10.1109/cvpr52688.2022.00843"}}, "content": {"source": {"pdf_hash": "164b769966cbaa53dd5edf907668e19773f65a84", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2205.03783v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "67ab8f11e2fc01b28f2c7abde51cac23d00e4bb5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/164b769966cbaa53dd5edf907668e19773f65a84.txt", "contents": "\nNon-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo\n\n\nJiayu Yang \nAustralian National University\n2 NVIDIA\n\nJose M Alvarez josea@nvidia.com \nAustralian National University\n2 NVIDIA\n\nMiaomiao Liu miaomiao.liu@anu.edu.au \nAustralian National University\n2 NVIDIA\n\nNon-parametric Depth Distribution Modelling based Depth Inference for Multi-view Stereo\nCode is available at https : / / github . com / NVlabs/NP-CVP-MVSNet\nRef. Depth Unimodal baseline PatchmatchNet [18] Ours Ref. Image Ref. Image Error of baseline Error of [18] Error of ours Figure 1. Performance analysis of cascade MVS methods on boundary regions. Compared with the unimodal baseline model, and the state-of-the-art patchmatch based approach PatchmatchNet [18], our method based on non-parametric depth distribution modeling can achieve more accurate depth estimation results on the boundary regions with abrupt depth changes.AbstractRecent cost volume pyramid based deep neural networks have unlocked the potential of efficiently leveraging high-resolution images for depth inference from multi-view stereo. In general, those approaches assume that the depth of each pixel follows a unimodal distribution. Boundary pixels usually follow a multi-modal distribution as they represent different depths; Therefore, the assumption results in an erroneous depth prediction at the coarser level of the cost volume pyramid and can not be corrected in the refinement levels leading to wrong depth predictions. In contrast, we propose constructing the cost volume by non-parametric depth distribution modeling to handle pixels with unimodal and multi-modal distributions. Our approach outputs multiple depth hypotheses at the coarser level to avoid errors in the early stage. As we perform local search around these multiple hypotheses in subsequent levels, our approach does not maintain the rigid depth spatial ordering and, therefore, we introduce a sparse cost aggregation network to derive information within each volume. We evaluate our approach extensively on two benchmark datasets: DTU and Tanks & Temples. Our experimental results show that our model outperforms existing methods by a large margin and achieves superior performance on boundary regions.\n Figure 1\n. Performance analysis of cascade MVS methods on boundary regions. Compared with the unimodal baseline model, and the state-of-the-art patchmatch based approach PatchmatchNet [18], our method based on non-parametric depth distribution modeling can achieve more accurate depth estimation results on the boundary regions with abrupt depth changes.\n\n\nAbstract\n\nRecent cost volume pyramid based deep neural networks have unlocked the potential of efficiently leveraging high-resolution images for depth inference from multi-view stereo. In general, those approaches assume that the depth of each pixel follows a unimodal distribution. Boundary pixels usually follow a multi-modal distribution as they represent different depths; Therefore, the assumption results in an erroneous depth prediction at the coarser level of the cost volume pyramid and can not be corrected in the refinement levels leading to wrong depth predictions. In contrast, we propose constructing the cost volume by non-parametric depth distribution modeling to handle pixels with unimodal and multi-modal distributions. Our approach outputs multiple depth hypotheses at the coarser level to avoid errors in the early stage. As we perform local search around these multiple hypotheses in subsequent levels, our approach does not maintain the rigid depth spatial ordering and, therefore, we introduce a sparse cost aggregation network to derive information within each volume. We evaluate our approach extensively on two benchmark datasets: DTU and Tanks & Temples. Our experimental results show that our model outperforms existing methods by a large margin and achieves superior performance on boundary regions.\n\n\nIntroduction\n\nMulti-view stereo (MVS) aims to infer the 3D structure, represented by a depth map, of a scene from a set of images captured by a camera from multiple viewpoints. It is a fundamental problem for the computer vision community and has been studied extensively for decades [13].\n\nRecently, learning-based methods using a cost volume pyramid [3,5,22] have emerged as the preferred technique to leverage high-resolution images leading to superior performance compared to other learning-based approaches [23,24]. These approaches usually perform a local depth search around an initial depth estimate on a cost volume built at the coarser level. Then, assuming each pixel follows a unimodal distribution, they estimate the depth for each pixel as the expectation of approximated continuous depth distributions by a few depth samples within a predefined range [3,5,22]. While these approaches have achieved promising results, they tend to miss small objects and boundary regions with abrupt depth changes, where the unimodal distribution assumption does not hold.\n\nAs shown in Fig. 1, the multi-scale depth estimation frameworks with unimodal distribution assumption performs badly on boundary pixels. It is mainly because existing multi-scale approaches make an early decision at the coarse level as the depth in this level is represented as a single value based on the unimodal distribution assumption. If the estimated coarse depth is far from the actual depth, the error will be propagated to refinement levels and can not be corrected via local depth search, resulting in incorrect depth prediction.\n\nIn this work, we address this problem by explicitly modeling the depth of each pixel at different resolutions using a multi-modal distribution. In particular, in contrast to methods using a mixture density distribution [17], we use a nonparametric distribution to learn the probability of each depth hypothesis along the 3D visual ray. Our approach provides additional flexibility compared to parametric approaches, especially in the coarse-to-fine structure. We then guide the learning process using the depth distribution within its corresponding depth patch at the highest resolution. Given the learned distribution, we build the cost volume for the next level by branching the depth hypotheses using the top K probabilities. Our approach achieves a better ground truth depth covering ratio than the existing approaches; however, it loses the relative spatial relationship due to the pixelwise depth branching processing. To aggregate the information in our new cost volume structure, we propose a sparse cost aggregation network to preserve the relative spatial relationship. Extensive experiments on several benchmark datasets demonstrate that our approach achieves superior performance, especially on boundary regions. On the DTU dataset, our approach outperforms the current state-ofthe-art multi-scale patchmatch based approach Patchmatch-Net [18], yielding up to a 32% lower error on boundary regions.\n\nIn summary, the contributions in this paper are as follows.\n\n\u2022 We propose a non-parametric depth probability distribution modeling, allowing us to handle pixels with unimodal and multimodal distributions.\n\n\u2022 We build a cost volume pyramid by branching the depth samples based on the modeled pixel-wise depth probability distribution.\n\n\u2022 We apply a sparse cost aggregation network to process each cost volume to maintain rigid geometric spatial relation in the cost volume and avoid spatial ambiguity.\n\n\u2022 Our approach outperforms previous approaches on boundary areas and becomes the new state of the art on the DTU dataset.\n\n\nRelated works\n\nDeep Learning-based MVS methods adopt deep CNNs to infer the depth map for each view followed by a separate multiple-view fusion process for building 3D models. These methods allow the network to extract discriminative features encoding global and local information of a scene to obtain robust feature matching for MVS. In particular, Yao et al. propose MVSNet [23] to infer a depth map for each view. An essential step in [23] is to build a cost volume based on a plane sweep process followed by multi-scale 3D CNNs for regularization. While effective in depth inference accuracy, its memory requirement is cubic to the image resolution. To allow handling high resolution images, several methods have been proposed to reduce the memory requirement. Recurrent methods [19,21,24] build cost volume and aggregate matching cost in a recursive manner using recurrent networks such as GRU [24] or LSTM [19,21]. However, recurrent methods usually consume a longer run-time trading for reduced space requirement. Another line of research exploits multi-scale framework for coarse-to-fine depth estimation [2,3,5,18,20,22]. Multiscale methods build a coarse cost volume in lower resolution to estimate a coarse depth map followed by building partial cost volumes in higher resolution for depth refinement. Although multi-scale methods have achieved promising results in both efficiency and accuracy, two major problems remain unsolved: Early decision on coarser levels and spatial ambiguity of partial cost volumes. In this paper, we address the first problem by a non-parametric depth distribution modelling paired with a novel multi-scale depth estimation framework. We address the second problem by a novel sparse cost volume formulation and a sparse cost aggregation network that retain rigid spatial relation. Multi-modal disparity distribution modeling. Multimodal disparity probability distribution modeling has been studied recently in tackling the stereo matching problem. In [17], Tosi et al. show that the unimodal probability distribution assumption for each pixel leads to inaccurate disparity estimations when the actual disparity follows a multi-modal distribution. They thus propose to model the continuous disparity as a parametric bi-modal distribution. Their proposed approach further improves the disparity estimation performance at the high resolution especially for boundary pixels. While the proposed approach can tackle the multi-modal disparity/depth modelling, it is not trivial to be directly extended to the cost volume pyramid structure to avoid resulting in erroneous depth estimation at the coarse level.\n\nUASNet [9] proposed to build the cascaded cost volume via uncertainty guided depth range estimation and depth sampling. Their goal is to build a cost volume with depth sampling covers the multi-modal depth distribution. In particular, they output 4 disparity maps at each stage. In con-trast, we do not output depth estimate at the early stage instead of building cost volume based on depth samples with high probability.\n\n\nMethod\n\nLet I 0 \u2208 R H\u00d7W \u00d73 be the reference image, where H and W define its dimensions; and let\n{I i } N i=1 be its N neighbor- ing source images. Assume {K i , R i , t i } N i=0\nare the corresponding camera intrinsic parameters, rotation matrix, and translation vector for all views. Our goal is to infer the depth map D \u2208 R H\u00d7W for I 0 from {I i } N i=0 . Figure 2 shows the overall pipeline of our approach to performing depth inference based on a cost volume pyramid built in a coarse-to-fine manner. Unlike existing works, the key idea of our approach is to build the cascaded costvolume based on the local search around the top K depth hypotheses, which we obtain by modeling the pixel-wise depth probability distribution. Below, we first introduce our the non-parametric depth probability distribution modeling; then the cost volume pyramid built using those nonparametric distributions and the depth map inference process. Finally, we provide details of the loss function.\n\n\nNon-parametric depth distribution modelling\n\nExisting methods assume the depth d of a pixel p follows a unimodal probability distribution P p (d). Under this assumption, the estimated depthd(p) is usually defined as the expectation of this distribution, approximated as the integral of the product of depth hypothesis {d m } M m=1 and its estimated probability along the ray:\nd(p) = E[P p (d)] \u2248 d\u2208{dm} M m=1 dP p (d).\nThe unimodal depth distribution is a valid assumption if the discrete depth map is of sufficiently high resolution and can approximate the continuous depth distribution well [17]. However, pixels at lower resolutions could be the projection of a group of 3D points with different depth values, especially for 3D structures across the boundaries of an object with depth discontinuities, which, as shown in Fig. 3, are inherent of a multi-modal distribution.\n\nAs shown in Fig. 4a, existing cascaded cost volumebased works using a unimodal distribution to represent those pixels might result in incorrect depth estimation. The estimated depth, defined as the expectation of the distribution, might be too far away from any of the depth modes; therefore, it would not be possible to recover it during the following refinement steps. That is, the algorithm makes an inaccurate early decision, the error of which will be propagated to subsequent modules. Instead, we introduce a nonparametric depth probability modeling for each pixel to deal with pixels with arbitrary distributions. Specifically, given a pixel p at the coarse level l, its depth d p follows a continuous probability distribution. We approximate this continu-ous distribution P l (d p ) on a set of discrete depth hypotheses (discrete samples) {d l p,m } M l m=1 . Next, we introduce our framework for depth inference based on this non-parametric depth probability distribution modeling.\n\n\nCost volume pyramid\n\nWe construct the cost volume pyramid for depth inference using a feature pyramid to extract features f l , l \u2208 {0...L}, where l = L refers to the coarsest level in the smallest resolution and l = 0 refers to the finest level corresponding to the full resolution. Regular cost volume for depth initialization. We first build the cost volume at level L. Assume {f L i } N i=0 refers to the feature maps extracted from the reference view and the N source views.\n\nGiven a pre-defined global depth searching range [d min , d max ], we uniformly sample {d m } M L m=1 depth values corresponding to M L fronto-parallel planes on the inverse depth space [20]. Here d min and d max represent the lower and upper bound of the searching range, respectively. Note that a sampled depth d \u2208 {d m } M L m=1 represents a plane parallel to the image plane of the reference camera. We use the differential homography [23] computed for the plane defined by the depth d to warp features of the source view i to the reference view f L i\u21920,d . We compute the matching cost as the Group-wise correlation of G L groups between the reference feature f L 0 and warped source feature f L i\u21920,d . Then, we estimate the cost maps for each depth hypothesis plane and concatenate them into a cost volume\nC L \u2208 R H L \u00d7W L \u00d7M L \u00d7G L .\nHere W L , and H L denote the image size at level L. We adopt a view aggregation module similar to [20] to estimate a visibility map and fuse matching cost computed from different source views.\n\nGiven this regular cost volume C L , we use a regular 3D-UNet similar to [22] for cost aggregation. The output of this initial cost aggregation network is a probability volume denoted as P L \u2208 R H L \u00d7W L \u00d7M L which defines the nonparametric depth probability distribution for each pixel, represented by the probability for each depth sample.\n\nThen, we explore the pixel-wise depth samples with the top K probabilities to build the cost volume for the next level. Sparse Cost volume for depth refinement. Without loss of generality we ignore the pixel index from now on. Let {d l Qi } K i=1 define the depth samples of top K estimated probability at level l, where {Q i } K i=1 define the top K indices and \u2206d l is the corresponding depth search interval. To perform local search around the K possible depth samples obtained from level l, we define the depth samples for level l \u2212 1 by subdividing each selected depth sample at level l into two samples, see Fig. 5. This process is formulated for each  \nwhere m \u2208 {Q i } K i=1 . The depth samples for level l \u2212 1 are estimated as S l\u22121 = {d l m\u2212 , d l m+ |m \u2208 {Q i } K i=1 } with the updated depth searching interval \u2206d l\u22121 = 0.5\u2206d l .(1)\nDue to the resolution difference across levels, S l\u22121 is shared by pixels within a patch at level l \u2212 1 that corresponds to a pixel at level l. Given the pixel-wise depth samples S l\u22121 , we then build the cost volume to model the depth probability distributions for pixels at level l \u2212 1. As depth samples are formed in a pixel-wise manner, the relative spatial locations among neighboring 3D points are not reserved. We therefore leverage the sparse cost volume structure and aggregate information based on sparse convolutions. To this end, we define the sparse cost volume\nas C l\u22121 = {(p k , c k )} K l\u22121 k=1\n, where c k defines the matching cost, p k = (x k , y k , z k ) defines the 3D coordinate computed from a depth sample, K l\u22121 defines the total number of 3D points converted from depth samples. Given the camera projection matrix, we project p k to other source views and the cost is similarly calculated as that for pixels at level L as described in the previous section.\n\nSparse cost aggregation network. As the sparse cost volume can not be efficiently aggregated by regular dense 3D convolutions, we build a sparse cost aggregation network to aggregate cost utilizing the rigid spatial relation stored in p k . Specifically, the basic block of our network consists of three layers of sparse 3D convolution with factorized kernels on each dimension, a sparse batch normalization layer and a sparse ReLU activation [15]. We provide the detailed structure of the network in the supplementary material. The output of the sparse cost aggregation network is the probability distribution P l\u22121 p (d), which is used as input to build the cost volume in the next refinement level, see Fig. 6 (b).\n\n\nDepth Inference at full resolution\n\nUnlike existing cascade cost volume-based approaches, we only perform the depth inference at the full resolution level 0. At this level, we approximate the depth of each pixel as the expectation of the estimated distribution,  \nd(p) = E(P 0 p (d)) = M 0 m=1 d 0 m P 0 p (d 0 m ).(2)\n\nLoss Function\n\nWe train the network in a supervised manner. We use the depth probability distribution approximated by the highresolution depth map observations as ground-truth. In particular, for each pixel, p, the ground-truth probability distribution P l gt,p is approximated by the histogram of depth observations from the corresponding patch \u03a6 p at the fullresolution, normalized by the sum of observations,\nP l gt,p (d l m ) = Hist l p (d l m ) m=M l m=1 Hist l p (d l m )\n,\nHist l p (d l m ) = p \u2208\u03a6p 1 \u2212 |d p \u2212d l m | \u2206d l , if |d p \u2212 d l m | \u2264 \u2206d 0, if |d p \u2212 d l m | > \u2206d,(3)\nwhere d p is the ground-truth depth value of pixel p in patch \u03a6 p , d l m is the depth hypothesis and \u2206d l is the interval between depth hypotheses.\n\nFinally, for each hypothesis d l m of pixel p, we compute the loss as the binary cross entropy between the estimated probability P l p (d l m ) and ground-truth probability P l gt,p (d l m ),\nL l p (d l m ) = BCE(P l p (d l m ), P l gt,p (d l m )).(4)\nEmpirically, we observe that the ground-truth probability distribution usually concentrates on a few hypotheses, causing an imbalanced number of samples with zero and with non-zero probability. We address this problem balancing the loss as,\nL l = p\u2208\u2126 l M l m=1 \u03bb l p (d l m )L l p (d l m ),(5)\u03bb l p (d l m ) = 1 \u2212 \u03c3 l , if P l gt,p (d l m ) > 0 \u03c3 l , if P l gt,p (d l m ) = 0 (6) \u03c3 l = M l m=1 p\u2208\u2126 l 1(P l gt,p (d l m ) > 0) H l \u00d7 W l \u00d7 M l ,(7)\nwhere \u2126 l defines the image coordinate domain at level l, \u03c3 l represents the percentage of hypothesis with probability larger than zero. At the final level, we supervise the depth estimation by the l 1 norm measuring the distance between ground-truth depth map and final estimated depth map.\nL 0 = p\u2208\u2126 0 ||D 0 gt (p) \u2212 D 0 (p)|| 1 .(8)\nThe overall loss is a weighted sum of the BCE loss on coarse scales and the l 1 loss on the final level,\nL = L l=0 w l L l (9)\nwhere w l is the weight for the loss at level l.\n\n\nExperiments\n\nIn this section, we demonstrate the performance of our approach with a comprehensive set of experiments in standard benchmarks. Below, we first describe the datasets and benchmarks, the implementation details, and then, present and analyze our results. \n\n\nImplementation\n\nWe implement the proposed model using pytorch [10] and use the Torchsparse [16] library for the sparse cost volume and the sparse cost aggregation.\n\nTraining We train our model on the DTU dataset using downsampled and cropped images of size 640 \u00d7 512 and their corresponding depth map. This map is rendered from a surface mesh generated by Screened Poisson Surface Reconstruction of the provided reference point cloud. We use the same split of training, validation and test set as defined in [23]. We set the number of levels to (L + 1) = 4 and the number of hypothesis to {M l } L l=0 = {8, 16, 32, 48}.\n\nWe use 5 views for training and adopt the similar anti-noise training strategy as in [18]. We use Adam optimizer with 0.0005 learning rate and a batchsize of 2 on a Nvidia RTX Titan GPU. To demonstrate the generalization ability of our approach, we train our model on BlendedMVS [25] training set and test on Tanks and Temples dataset without any fine-tuning.\n\nMetrics We follow the evaluation protocol used by [22][23][24].\n\nOn the DTU dataset, we report the accuracy, completeness and overall score of the reconstructed point clouds. Accuracy measures the distance from the estimated point clouds to the ground truth in millimeters and completeness measures the distance from ground truth point clouds to the estimated ones [1]. The overall score is the average of accuracy and completeness [23]. On Tanks and Temples, we report the f-score for each scene and the average of all scenes. We also evaluate the depth estimation accuracy specifically on depth boundaries. To this end, we use a Laplacian pyramid simulating a band-pass filter to segment the ground-truth depth map into non-overlapping regions of different depth smoothness, see Fig. 7. We first build a depth map pyramid {D q gt } Q q=0 of Q+1 levels by repeatedly downsampling the ground-truth depth map D 0 gt . We then build the Laplacian pyramid of depth by taking the difference between adjacent levels in {D q gt } Q q=0 , where the lower level depth is upsampled to match the size of the upper one.\n\nEach level of the Laplacian pyramid contains depth structures present at a particular scale, which we use as a mask to segment the depth map into different smoothness regions. In particular, we use 5 levels to segment the depth map into five regions {R0, R1, R2, R3, R4}, see Fig. 7. The region R0 corresponds to the highest level of the Laplacian pyramid and contains pixels with the most abrupt depth changes usually caused by depth boundaries or small objects. The lower levels correspond to regions with intermediate and smooth depths. We report the average depth error for each of the five region as the average l 1 distance in millimeter between estimated depth and ground-truth depth. Evaluation We set the number of hypothesis per level as {M l } L l=0 = {8, 16, 32, 96} for testing. On the DTU dataset, we test on the full-resolution images of size 1600 \u00d7 1184 and set number of views to N = 5 with [4] for fusion. On the Tanks and Temples dataset, we use images of size 1920 \u00d7 1080 with N = 11 views, camera parameters generated by Colmap [11,12] and the fusion method in [21].\n\n\nResults on DTU dataset\n\nWe first compare the overall reconstruction quality of our proposed method with existing methods on the DTU dataset. As summarized in Tab. 1, our method outperforms all existing methods on both mean completeness and overall score. We also analyze the quality of the reconstruction on the boundaries using the average depth error on the five regions of the Laplacian pyramid. For comparison, we consider PatchmatchNet [18] and a recurrent method D2HC-RMVSNet [21]. As a baseline, we consider a variation of our method using a unimodal distribution. As summarized in Tab. 2, our method outperforms the others on the boundary regions. Qualitative results in Fig. 1 and Fig. 8 also demonstrate our method is more accurate on boundary regions and small objects producing sharper depth discontinuities. Efficiency-wise, our model takes 6054 MB GPU memory and 1.2s to estimate a full-resolution depth map, which is on par with existing cost volume-based methods.\n\n\nResults on Tanks and Temples\n\nIn this experiment, we evaluate the generalization ability of the proposed method on the Tanks and Temples dataset [6]. The summary of quantitative results is listed in Tab. 3 and representative qualitative results are shown in Fig. 9. In these results, we can observe that our approach produces sharp and accurate depth estimation on boundary regions and, overall, our model trained on the BlendedMVS dataset achieves competitive performance compared to other recent methods in the benchmark.   Table 4. DTU dataset. Quantitative comparison of the contribution of each module in our framework. Using non-parametric depth distribution modeling with standard 3D convolutions can cause performance drop due to spatial ambiguity. Our proposed sparse cost aggregation can improve the performance of unimodal based methods. The proposed non-parametric depth distribution modeling requires the sparse cost aggregation to achieve best reconstruction quality.\n\n\nAblation Study\n\nWe provide ablation experiments on the DTU dataset to evaluate the contribution of the proposed modules. We start with a baseline model using standard unimodal distribution and standard 3D convolutions for cost aggregation. Results are shown in Tab. 4. Using non-parametric depth distribution modeling with standard 3D convolutions can cause performance drop due to spatial ambiguity. Our proposed sparse cost aggregation can improve the performance of unimodal based methods. The proposed non-parametric depth distribution modeling requires the sparse cost aggregation to achieve best reconstruction quality.\n\nLimitations. Our approach leverages sparse cost volume and sparse convolutions to yield better accuracy. However, sparse convolutions are computationally expensive as they are not fully optimized. This increases the inference time.\n\nWe plan to improve the efficiency in our future work.\n\n\nConclusion\n\nWe proposed an approach for depth inference based on modelling non-parametric depth probability distribution for each pixel. Our modelling can handle pixels with unimodal and multi-modal depth distributions such as pixels on the boundaries. Our method does not infer depth at the coarse level which avoids the depth errors made at the early stage and following propagation to the refinement levels. Experimental results show that our approach can achieve superior performance especially for pixels on the boundaries.\n\nFigure 2 .Figure 3 .\n23Network Structure. We first build the feature pyramid from the source and reference images. We then build the cost volume pyramid based on the modelling of pixel-wise non-parametric depth probability distribution. Specifically, the cost volume at each level is built based on the depth samples of top K probability from the previous level. The cost volume is sparse and aggregated by sparse convolution. The depth map D 0 is inferred at the full resolution level. The depth distribution of a coarse pixel can be approximated by the depth observations in the corresponding patch on full resolution\n\nFigure 4 .Figure 5 .\n45Unimodal and non-parametric depth searching. (a) Existing method assume unimodal distribution that can cause incorrect depth estimation. (b) Our non-parametric depth modeling can estimate correct depth from multi-modal depth distribution. We generate new hypothesis by selecting top-k hypothesis from coarse level and equally subdivide them along the ray.\n\nFigure 6 .Figure 7 .\n67Sparse cost volume and sparse cost aggregation. Color indicates relative depth of hypothesis. (a) Existing methods build flattened cost volume that have spatial ambiguity. (b) We build sparse cost volume and use sparse cost aggregation to retain rigid spatial relation. We segment ground-truth depth map into five regions corresponding to different depth smoothness and evaluate depth estimation accuracy on each region.4.1. DatasetsDTU dataset[1] is a large-scale MVS dataset providing 124 scenes captured from 64 views under 7 lighting conditions. 3D points captured by structured-light scanners are provided for each scene as reference reconstruction. The dataset provides a color image and the corresponding camera parameters for each view. BlendedMVS dataset[25] is a large-scale synthetic MVS dataset. It contains more than 17000 MVS training samples covering a variety of 113 scenes, including architectures, sculptures and small objects. Tanks and Temples[6] dataset provides both indoor and outdoor scenes under real-life lightning conditions with large scale variations. We evaluate the generalization ability of our model on this public benchmark dataset.\n\nFigure 9 .\n9Tanks and Temples dataset. Example point cloud results. The second row shows details of first row. Best viewed on screen.\n\n\nRMVSNet[21] 7.20 2.33 2.59 5.14 10.2Table 2. DTU dataset. Performance on regions of different depth smoothness. Our method can achieve lowest error on boundary region (R0).Method \nR0 \nR1 \nR2 \nR3 \nR4 \nD2HC-PatchmatchNet [18] \n9.54 2.37 2.40 4.72 9.15 \nUnimodal baseline \n8.85 2.39 2.15 3.97 9.08 \nOurs \n6.48 1.94 1.81 3.85 8.61 \n\n\n\nTable 3 .\n3Tanks and Temples dataset. Quantitative comparison to existing approaches. Our model can achieve competitive performance on Tanks and Temples dataset.Methods \nMean Family Francis Horse Lighthouse \nM60 \nPanther Playground Train \nAA-RMVSNet [19] \n61.51 \n77.77 \n59.53 \n51.53 \n64.02 \n64.05 \n59.47 \n60.85 \n54.90 \nEPP-MVSNet [8] \n61.68 \n77.86 \n60.54 \n52.96 \n62.33 \n61.69 \n60.34 \n62.44 \n55.30 \nVis-MVSNet [26] \n60.03 \n77.40 \n60.23 \n47.07 \n63.44 \n62.21 \n57.28 \n60.54 \n52.07 \nOurs \n59.64 \n78.93 \n64.09 \n51.82 \n59.42 \n58.39 \n55.71 \n56.07 \n52.71 \nD2HC-RMVSNet [21] 59.20 \n74.69 \n56.04 \n49.42 \n60.08 \n59.81 \n59.61 \n60.04 \n53.92 \nBP-MVSNet [14] \n57.60 \n77.31 \n60.90 \n47.89 \n58.26 \n56.00 \n51.54 \n58.47 \n50.41 \nPVSNet [20] \n56.88 \n74.00 \n55.17 \n39.85 \n61.37 \n60.22 \n56.87 \n58.02 \n49.51 \nPatchmatchNet [18] \n53.15 \n66.99 \n52.64 \n43.24 \n54.87 \n52.87 \n49.54 \n54.21 \n50.81 \nPatchMatch-RL [7] \n51.81 \n60.37 \n43.26 \n36.43 \n56.27 \n57.30 \n53.43 \n59.85 \n47.61 \n\nMethod \nDepth distribution \nCost aggregation \nAcc. \nComp. Overall \nUnimodal Non-parametric Standard Sparse \nBaseline \n0.3498 0.3351 0.3425 \nBaseline + non-parametric modeling \n0.3387 0.3792 0.3590 \nBaseline + sparse cost aggregation \n0.3538 0.3096 0.3317 \nOurs \n0.3563 0.2750 0.3156 \n\n\nAcknowledgmentsThis research is supported by Australian Research Council grants (DE180100628, DP200102274).\nEngin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. IJCV. Henrik Aanaes, George Rasmus Ramsb\u00f8l Jensen, Vogiatzis, Henrik Aanaes, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. IJCV, 2016. 6\n\nPoint-based multi-view stereo network. Rui Chen, Songfang Han, Jing Xu, Hao Su, ICCV. Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In ICCV, 2019. 2\n\nDeep stereo using adaptive thin volume representation with uncertainty awareness. Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, Hao Su, CVPR, 2020. 1. 27Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adap- tive thin volume representation with uncertainty awareness. In CVPR, 2020. 1, 2, 7\n\nGipuma: Massively parallel multi-view stereo reconstruction. Publikationen der Deutschen Gesellschaft f\u00fcr Photogrammetrie. Silvano Galliani, Katrin Lasinger, Konrad Schindler, Fernerkundung und Geoinformation e. VSilvano Galliani, Katrin Lasinger, and Konrad Schindler. Gipuma: Massively parallel multi-view stereo reconstruc- tion. Publikationen der Deutschen Gesellschaft f\u00fcr Pho- togrammetrie, Fernerkundung und Geoinformation e. V, 2016. 7\n\nFeitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, CVPR, 2020. 1. 27Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In CVPR, 2020. 1, 2, 7\n\nTanks and temples: Benchmarking large-scale scene reconstruction. Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, Vladlen Koltun, ACM Transactions on Graphics. 67Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 2017. 6, 7\n\nPatchmatch-rl: Deep mvs with pixelwise depth, normal, and visibility. Jae Yong Lee, Joseph Degol, Chuhang Zou, Derek Hoiem, CVPR. Jae Yong Lee, Joseph DeGol, Chuhang Zou, and Derek Hoiem. Patchmatch-rl: Deep mvs with pixelwise depth, nor- mal, and visibility. In CVPR, October 2021. 8\n\nEpp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo. Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, Fan Yu, ICCV. 7Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolar-assembling based depth prediction for multi-view stereo. In ICCV, pages 5732-5740, October 2021. 7, 8\n\nUasnet: Uncertainty adaptive sampling network for deep stereo matching. Yamin Mao, Zhihua Liu, Weiming Li, Yuchao Dai, Qiang Wang, Yun-Tae Kim, Hong-Seok Lee, CVPR. Yamin Mao, Zhihua Liu, Weiming Li, Yuchao Dai, Qiang Wang, Yun-Tae Kim, and Hong-Seok Lee. Uasnet: Uncer- tainty adaptive sampling network for deep stereo matching. In CVPR, pages 6311-6319, 2021. 2\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, NeurIPS. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, IncAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, NeurIPS, pages 8024-8035. Curran Associates, Inc., 2019. 6\n\nStructure-from-motion revisited. Johannes Lutz Sch\u00f6nberger, Jan-Michael Frahm, CVPR. Johannes Lutz Sch\u00f6nberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 7\n\nPixelwise view selection for unstructured multi-view stereo. Johannes Lutz Sch\u00f6nberger, Enliang Zheng, Marc Pollefeys, Jan-Michael Frahm, ECCV. Johannes Lutz Sch\u00f6nberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for un- structured multi-view stereo. In ECCV, 2016. 7\n\nA comparison and evaluation of multi-view stereo reconstruction algorithms. M Steven, Brian Seitz, James Curless, Daniel Diebel, Richard Scharstein, Szeliski, CVPR. Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. A comparison and eval- uation of multi-view stereo reconstruction algorithms. In CVPR, 2006. 1\n\nBpmvsnet: Belief-propagation-layers for multi-view-stereo. Christian Sormann, Patrick Kn\u00f6belreiter, Andreas Kuhn, Mattia Rossi, Thomas Pock, Friedrich Fraundorfer, 3DV. IEEE7Christian Sormann, Patrick Kn\u00f6belreiter, Andreas Kuhn, Mattia Rossi, Thomas Pock, and Friedrich Fraundorfer. Bp- mvsnet: Belief-propagation-layers for multi-view-stereo. In 3DV, pages 394-403. IEEE, 2020. 7, 8\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, CVPR. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception ar- chitecture for computer vision. In CVPR, pages 2818-2826, 2016. 4\n\nSearching efficient 3d architectures with sparse point-voxel convolution. * Haotian, Tang, * Zhijian, Shengyu Liu, Yujun Zhao, Ji Lin, Hanrui Lin, Song Wang, Han, ECCV. Haotian* Tang, Zhijian* Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution. In ECCV, 2020. 6\n\nSmd-nets: Stereo mixture density networks. Fabio Tosi, Yiyi Liao, Carolin Schmitt, Andreas Geiger, CVPR. 23Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. Smd-nets: Stereo mixture density networks. In CVPR, pages 8942-8952, 2021. 2, 3\n\nPatchmatchnet: Learned multi-view patchmatch stereo. Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, Marc Pollefeys, 7Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch stereo, 2021. 1, 2, 6, 7, 8\n\nAa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network. Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, Guoping Wang, ICCV. Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adaptive aggregation recur- rent multi-view stereo network. In ICCV, pages 6187-6196, October 2021. 2, 7, 8\n\nPvsnet: Pixelwise visibilityaware multi-view stereo network. Qingshan Xu, Wenbing Tao, arXivQingshan Xu and Wenbing Tao. Pvsnet: Pixelwise visibility- aware multi-view stereo network. arXiv, 2020. 2, 3, 7, 8\n\nDense hybrid recurrent multi-view stereo net with dynamic consistency checking. Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, Yu-Wing Tai, ECCV, 2020. 7Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-view stereo net with dy- namic consistency checking. In ECCV, 2020. 2, 7, 8\n\nCost volume pyramid based depth inference for multi-view stereo. Jiayu Yang, Wei Mao, M Jose, Miaomiao Alvarez, Liu, CVPR. Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In CVPR, 2020. 1, 2, 3, 6, 7\n\nMvsnet: Depth inference for unstructured multi-view stereo. Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, Long Quan, ECCV. 6Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In ECCV, 2018. 1, 2, 3, 6\n\nRecurrent mvsnet for high-resolution multiview stereo depth inference. Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, Long Quan, CVPR. 6Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi- view stereo depth inference. In CVPR, 2019. 1, 2, 6\n\nBlendedmvs: A large-scale dataset for generalized multi-view stereo networks. Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, Long Quan, CVPRYao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-scale dataset for generalized multi-view stereo net- works. CVPR, 2020. 6\n\nVisibility-aware multi-view stereo network. Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, Tian Fang, BMVC. 78Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network. BMVC, 2020. 7, 8\n", "annotations": {"author": "[{\"end\":143,\"start\":91},{\"end\":217,\"start\":144},{\"end\":296,\"start\":218}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":97},{\"end\":158,\"start\":151},{\"end\":230,\"start\":227}]", "author_first_name": "[{\"end\":96,\"start\":91},{\"end\":148,\"start\":144},{\"end\":150,\"start\":149},{\"end\":226,\"start\":218}]", "author_affiliation": "[{\"end\":142,\"start\":103},{\"end\":216,\"start\":177},{\"end\":295,\"start\":256}]", "title": "[{\"end\":88,\"start\":1},{\"end\":384,\"start\":297}]", "venue": null, "abstract": "[{\"end\":2255,\"start\":454}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2445,\"start\":2441},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4234,\"start\":4230},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4301,\"start\":4298},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4303,\"start\":4301},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4306,\"start\":4303},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4462,\"start\":4458},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4465,\"start\":4462},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4815,\"start\":4812},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4817,\"start\":4815},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4820,\"start\":4817},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5781,\"start\":5777},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6913,\"start\":6909},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7976,\"start\":7972},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8038,\"start\":8034},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8383,\"start\":8379},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8386,\"start\":8383},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8389,\"start\":8386},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8499,\"start\":8495},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8512,\"start\":8508},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8515,\"start\":8512},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8712,\"start\":8709},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8714,\"start\":8712},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8716,\"start\":8714},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8719,\"start\":8716},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8722,\"start\":8719},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8725,\"start\":8722},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9592,\"start\":9588},{\"end\":9605,\"start\":9594},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10250,\"start\":10247},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12244,\"start\":12240},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14189,\"start\":14185},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14442,\"start\":14438},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14944,\"start\":14940},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15113,\"start\":15109},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17655,\"start\":17651},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20529,\"start\":20525},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20558,\"start\":20554},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20975,\"start\":20971},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21174,\"start\":21170},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21368,\"start\":21364},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21500,\"start\":21496},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21504,\"start\":21500},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21508,\"start\":21504},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21814,\"start\":21811},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21882,\"start\":21878},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23467,\"start\":23464},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":23609,\"start\":23605},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23612,\"start\":23609},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23642,\"start\":23638},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24091,\"start\":24087},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24132,\"start\":24128},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24776,\"start\":24773},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28529,\"start\":28526},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28849,\"start\":28845},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29048,\"start\":29045},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29397,\"start\":29393}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27677,\"start\":27057},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28057,\"start\":27678},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29248,\"start\":28058},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29383,\"start\":29249},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29715,\"start\":29384},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30951,\"start\":29716}]", "paragraph": "[{\"end\":2611,\"start\":2266},{\"end\":3943,\"start\":2624},{\"end\":4235,\"start\":3960},{\"end\":5015,\"start\":4237},{\"end\":5556,\"start\":5017},{\"end\":6968,\"start\":5558},{\"end\":7029,\"start\":6970},{\"end\":7174,\"start\":7031},{\"end\":7303,\"start\":7176},{\"end\":7470,\"start\":7305},{\"end\":7593,\"start\":7472},{\"end\":10238,\"start\":7611},{\"end\":10661,\"start\":10240},{\"end\":10759,\"start\":10672},{\"end\":11644,\"start\":10843},{\"end\":12022,\"start\":11692},{\"end\":12522,\"start\":12066},{\"end\":13515,\"start\":12524},{\"end\":13997,\"start\":13539},{\"end\":14811,\"start\":13999},{\"end\":15034,\"start\":14841},{\"end\":15377,\"start\":15036},{\"end\":16038,\"start\":15379},{\"end\":16798,\"start\":16224},{\"end\":17206,\"start\":16835},{\"end\":17925,\"start\":17208},{\"end\":18191,\"start\":17964},{\"end\":18659,\"start\":18263},{\"end\":18727,\"start\":18726},{\"end\":18980,\"start\":18832},{\"end\":19173,\"start\":18982},{\"end\":19474,\"start\":19234},{\"end\":19971,\"start\":19680},{\"end\":20120,\"start\":20016},{\"end\":20191,\"start\":20143},{\"end\":20460,\"start\":20207},{\"end\":20626,\"start\":20479},{\"end\":21083,\"start\":20628},{\"end\":21444,\"start\":21085},{\"end\":21509,\"start\":21446},{\"end\":22554,\"start\":21511},{\"end\":23643,\"start\":22556},{\"end\":24625,\"start\":23670},{\"end\":25609,\"start\":24658},{\"end\":26237,\"start\":25628},{\"end\":26470,\"start\":26239},{\"end\":26525,\"start\":26472},{\"end\":27056,\"start\":26540}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10842,\"start\":10760},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12065,\"start\":12023},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14840,\"start\":14812},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16223,\"start\":16039},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16834,\"start\":16799},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18246,\"start\":18192},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18725,\"start\":18660},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18831,\"start\":18728},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19233,\"start\":19174},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19527,\"start\":19475},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19679,\"start\":19527},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20015,\"start\":19972},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20142,\"start\":20121}]", "table_ref": "[{\"end\":25161,\"start\":25154}]", "section_header": "[{\"end\":2622,\"start\":2614},{\"attributes\":{\"n\":\"1.\"},\"end\":3958,\"start\":3946},{\"attributes\":{\"n\":\"2.\"},\"end\":7609,\"start\":7596},{\"attributes\":{\"n\":\"3.\"},\"end\":10670,\"start\":10664},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11690,\"start\":11647},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13537,\"start\":13518},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17962,\"start\":17928},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18261,\"start\":18248},{\"attributes\":{\"n\":\"4.\"},\"end\":20205,\"start\":20194},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20477,\"start\":20463},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23668,\"start\":23646},{\"attributes\":{\"n\":\"4.4.\"},\"end\":24656,\"start\":24628},{\"attributes\":{\"n\":\"4.5.\"},\"end\":25626,\"start\":25612},{\"attributes\":{\"n\":\"5.\"},\"end\":26538,\"start\":26528},{\"end\":27078,\"start\":27058},{\"end\":27699,\"start\":27679},{\"end\":28079,\"start\":28059},{\"end\":29260,\"start\":29250},{\"end\":29726,\"start\":29717}]", "table": "[{\"end\":29715,\"start\":29558},{\"end\":30951,\"start\":29878}]", "figure_caption": "[{\"end\":27677,\"start\":27081},{\"end\":28057,\"start\":27702},{\"end\":29248,\"start\":28082},{\"end\":29383,\"start\":29262},{\"end\":29558,\"start\":29386},{\"end\":29878,\"start\":29728}]", "figure_ref": "[{\"end\":2265,\"start\":2257},{\"end\":5035,\"start\":5029},{\"end\":11030,\"start\":11022},{\"end\":12477,\"start\":12471},{\"end\":12543,\"start\":12536},{\"end\":15999,\"start\":15993},{\"end\":17920,\"start\":17914},{\"end\":22233,\"start\":22227},{\"end\":22838,\"start\":22832},{\"end\":24331,\"start\":24325},{\"end\":24342,\"start\":24336},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24892,\"start\":24886}]", "bib_author_first_name": "[{\"end\":31157,\"start\":31151},{\"end\":31172,\"start\":31166},{\"end\":31405,\"start\":31402},{\"end\":31420,\"start\":31412},{\"end\":31430,\"start\":31426},{\"end\":31438,\"start\":31435},{\"end\":31637,\"start\":31633},{\"end\":31652,\"start\":31645},{\"end\":31663,\"start\":31657},{\"end\":31675,\"start\":31669},{\"end\":31682,\"start\":31680},{\"end\":31688,\"start\":31683},{\"end\":31697,\"start\":31693},{\"end\":31714,\"start\":31711},{\"end\":32064,\"start\":32057},{\"end\":32081,\"start\":32075},{\"end\":32098,\"start\":32092},{\"end\":32493,\"start\":32485},{\"end\":32504,\"start\":32498},{\"end\":32514,\"start\":32510},{\"end\":32527,\"start\":32520},{\"end\":32798,\"start\":32794},{\"end\":32816,\"start\":32810},{\"end\":32830,\"start\":32823},{\"end\":32844,\"start\":32837},{\"end\":33135,\"start\":33132},{\"end\":33140,\"start\":33136},{\"end\":33152,\"start\":33146},{\"end\":33167,\"start\":33160},{\"end\":33178,\"start\":33173},{\"end\":33432,\"start\":33426},{\"end\":33440,\"start\":33437},{\"end\":33452,\"start\":33447},{\"end\":33466,\"start\":33459},{\"end\":33477,\"start\":33474},{\"end\":33487,\"start\":33484},{\"end\":33770,\"start\":33765},{\"end\":33782,\"start\":33776},{\"end\":33795,\"start\":33788},{\"end\":33806,\"start\":33800},{\"end\":33817,\"start\":33812},{\"end\":33831,\"start\":33824},{\"end\":33846,\"start\":33837},{\"end\":34132,\"start\":34128},{\"end\":34144,\"start\":34141},{\"end\":34161,\"start\":34152},{\"end\":34173,\"start\":34169},{\"end\":34186,\"start\":34181},{\"end\":34204,\"start\":34197},{\"end\":34219,\"start\":34213},{\"end\":34235,\"start\":34229},{\"end\":34248,\"start\":34241},{\"end\":34265,\"start\":34261},{\"end\":34279,\"start\":34274},{\"end\":34298,\"start\":34291},{\"end\":34311,\"start\":34305},{\"end\":34325,\"start\":34318},{\"end\":34340,\"start\":34334},{\"end\":34356,\"start\":34349},{\"end\":34371,\"start\":34365},{\"end\":34392,\"start\":34386},{\"end\":34404,\"start\":34402},{\"end\":34417,\"start\":34411},{\"end\":34430,\"start\":34423},{\"end\":35138,\"start\":35130},{\"end\":35168,\"start\":35157},{\"end\":35351,\"start\":35343},{\"end\":35377,\"start\":35370},{\"end\":35389,\"start\":35385},{\"end\":35412,\"start\":35401},{\"end\":35665,\"start\":35664},{\"end\":35679,\"start\":35674},{\"end\":35692,\"start\":35687},{\"end\":35708,\"start\":35702},{\"end\":35724,\"start\":35717},{\"end\":36003,\"start\":35994},{\"end\":36020,\"start\":36013},{\"end\":36042,\"start\":36035},{\"end\":36055,\"start\":36049},{\"end\":36069,\"start\":36063},{\"end\":36085,\"start\":36076},{\"end\":36388,\"start\":36379},{\"end\":36405,\"start\":36398},{\"end\":36423,\"start\":36417},{\"end\":36434,\"start\":36431},{\"end\":36451,\"start\":36443},{\"end\":36720,\"start\":36719},{\"end\":36737,\"start\":36736},{\"end\":36754,\"start\":36747},{\"end\":36765,\"start\":36760},{\"end\":36774,\"start\":36772},{\"end\":36786,\"start\":36780},{\"end\":36796,\"start\":36792},{\"end\":37043,\"start\":37038},{\"end\":37054,\"start\":37050},{\"end\":37068,\"start\":37061},{\"end\":37085,\"start\":37078},{\"end\":37306,\"start\":37296},{\"end\":37320,\"start\":37313},{\"end\":37340,\"start\":37331},{\"end\":37353,\"start\":37348},{\"end\":37368,\"start\":37364},{\"end\":37621,\"start\":37613},{\"end\":37635,\"start\":37627},{\"end\":37645,\"start\":37641},{\"end\":37657,\"start\":37651},{\"end\":37671,\"start\":37664},{\"end\":37943,\"start\":37935},{\"end\":37955,\"start\":37948},{\"end\":38171,\"start\":38163},{\"end\":38185,\"start\":38177},{\"end\":38198,\"start\":38191},{\"end\":38209,\"start\":38203},{\"end\":38221,\"start\":38216},{\"end\":38235,\"start\":38229},{\"end\":38249,\"start\":38242},{\"end\":38263,\"start\":38256},{\"end\":38568,\"start\":38563},{\"end\":38578,\"start\":38575},{\"end\":38585,\"start\":38584},{\"end\":38600,\"start\":38592},{\"end\":38834,\"start\":38831},{\"end\":38845,\"start\":38840},{\"end\":38857,\"start\":38851},{\"end\":38866,\"start\":38862},{\"end\":38877,\"start\":38873},{\"end\":39109,\"start\":39106},{\"end\":39120,\"start\":39115},{\"end\":39132,\"start\":39126},{\"end\":39144,\"start\":39137},{\"end\":39155,\"start\":39151},{\"end\":39166,\"start\":39162},{\"end\":39429,\"start\":39426},{\"end\":39440,\"start\":39435},{\"end\":39452,\"start\":39446},{\"end\":39465,\"start\":39457},{\"end\":39478,\"start\":39473},{\"end\":39487,\"start\":39484},{\"end\":39498,\"start\":39494},{\"end\":39509,\"start\":39505},{\"end\":39761,\"start\":39753},{\"end\":39772,\"start\":39769},{\"end\":39784,\"start\":39778},{\"end\":39794,\"start\":39789},{\"end\":39804,\"start\":39800}]", "bib_author_last_name": "[{\"end\":31164,\"start\":31158},{\"end\":31194,\"start\":31173},{\"end\":31205,\"start\":31196},{\"end\":31410,\"start\":31406},{\"end\":31424,\"start\":31421},{\"end\":31433,\"start\":31431},{\"end\":31441,\"start\":31439},{\"end\":31643,\"start\":31638},{\"end\":31655,\"start\":31653},{\"end\":31667,\"start\":31664},{\"end\":31678,\"start\":31676},{\"end\":31691,\"start\":31689},{\"end\":31709,\"start\":31698},{\"end\":31717,\"start\":31715},{\"end\":32073,\"start\":32065},{\"end\":32090,\"start\":32082},{\"end\":32108,\"start\":32099},{\"end\":32496,\"start\":32494},{\"end\":32508,\"start\":32505},{\"end\":32518,\"start\":32515},{\"end\":32531,\"start\":32528},{\"end\":32808,\"start\":32799},{\"end\":32821,\"start\":32817},{\"end\":32835,\"start\":32831},{\"end\":32851,\"start\":32845},{\"end\":33144,\"start\":33141},{\"end\":33158,\"start\":33153},{\"end\":33171,\"start\":33168},{\"end\":33184,\"start\":33179},{\"end\":33435,\"start\":33433},{\"end\":33445,\"start\":33441},{\"end\":33457,\"start\":33453},{\"end\":33472,\"start\":33467},{\"end\":33482,\"start\":33478},{\"end\":33490,\"start\":33488},{\"end\":33774,\"start\":33771},{\"end\":33786,\"start\":33783},{\"end\":33798,\"start\":33796},{\"end\":33810,\"start\":33807},{\"end\":33822,\"start\":33818},{\"end\":33835,\"start\":33832},{\"end\":33850,\"start\":33847},{\"end\":34139,\"start\":34133},{\"end\":34150,\"start\":34145},{\"end\":34167,\"start\":34162},{\"end\":34179,\"start\":34174},{\"end\":34195,\"start\":34187},{\"end\":34211,\"start\":34205},{\"end\":34227,\"start\":34220},{\"end\":34239,\"start\":34236},{\"end\":34259,\"start\":34249},{\"end\":34272,\"start\":34266},{\"end\":34289,\"start\":34280},{\"end\":34303,\"start\":34299},{\"end\":34316,\"start\":34312},{\"end\":34332,\"start\":34326},{\"end\":34347,\"start\":34341},{\"end\":34363,\"start\":34357},{\"end\":34384,\"start\":34372},{\"end\":34400,\"start\":34393},{\"end\":34409,\"start\":34405},{\"end\":34421,\"start\":34418},{\"end\":34439,\"start\":34431},{\"end\":35155,\"start\":35139},{\"end\":35174,\"start\":35169},{\"end\":35368,\"start\":35352},{\"end\":35383,\"start\":35378},{\"end\":35399,\"start\":35390},{\"end\":35418,\"start\":35413},{\"end\":35672,\"start\":35666},{\"end\":35685,\"start\":35680},{\"end\":35700,\"start\":35693},{\"end\":35715,\"start\":35709},{\"end\":35735,\"start\":35725},{\"end\":35745,\"start\":35737},{\"end\":36011,\"start\":36004},{\"end\":36033,\"start\":36021},{\"end\":36047,\"start\":36043},{\"end\":36061,\"start\":36056},{\"end\":36074,\"start\":36070},{\"end\":36097,\"start\":36086},{\"end\":36396,\"start\":36389},{\"end\":36415,\"start\":36406},{\"end\":36429,\"start\":36424},{\"end\":36441,\"start\":36435},{\"end\":36457,\"start\":36452},{\"end\":36728,\"start\":36721},{\"end\":36734,\"start\":36730},{\"end\":36745,\"start\":36738},{\"end\":36758,\"start\":36755},{\"end\":36770,\"start\":36766},{\"end\":36778,\"start\":36775},{\"end\":36790,\"start\":36787},{\"end\":36801,\"start\":36797},{\"end\":36806,\"start\":36803},{\"end\":37048,\"start\":37044},{\"end\":37059,\"start\":37055},{\"end\":37076,\"start\":37069},{\"end\":37092,\"start\":37086},{\"end\":37311,\"start\":37307},{\"end\":37329,\"start\":37321},{\"end\":37346,\"start\":37341},{\"end\":37362,\"start\":37354},{\"end\":37378,\"start\":37369},{\"end\":37625,\"start\":37622},{\"end\":37639,\"start\":37636},{\"end\":37649,\"start\":37646},{\"end\":37662,\"start\":37658},{\"end\":37676,\"start\":37672},{\"end\":37946,\"start\":37944},{\"end\":37959,\"start\":37956},{\"end\":38175,\"start\":38172},{\"end\":38189,\"start\":38186},{\"end\":38201,\"start\":38199},{\"end\":38214,\"start\":38210},{\"end\":38227,\"start\":38222},{\"end\":38240,\"start\":38236},{\"end\":38254,\"start\":38250},{\"end\":38267,\"start\":38264},{\"end\":38573,\"start\":38569},{\"end\":38582,\"start\":38579},{\"end\":38590,\"start\":38586},{\"end\":38608,\"start\":38601},{\"end\":38613,\"start\":38610},{\"end\":38838,\"start\":38835},{\"end\":38849,\"start\":38846},{\"end\":38860,\"start\":38858},{\"end\":38871,\"start\":38867},{\"end\":38882,\"start\":38878},{\"end\":39113,\"start\":39110},{\"end\":39124,\"start\":39121},{\"end\":39135,\"start\":39133},{\"end\":39149,\"start\":39145},{\"end\":39160,\"start\":39156},{\"end\":39171,\"start\":39167},{\"end\":39433,\"start\":39430},{\"end\":39444,\"start\":39441},{\"end\":39455,\"start\":39453},{\"end\":39471,\"start\":39466},{\"end\":39482,\"start\":39479},{\"end\":39492,\"start\":39488},{\"end\":39503,\"start\":39499},{\"end\":39514,\"start\":39510},{\"end\":39767,\"start\":39762},{\"end\":39776,\"start\":39773},{\"end\":39787,\"start\":39785},{\"end\":39798,\"start\":39795},{\"end\":39809,\"start\":39805}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":31361,\"start\":31060},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":199552074},\"end\":31549,\"start\":31363},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":208310189},\"end\":31932,\"start\":31551},{\"attributes\":{\"id\":\"b3\"},\"end\":32377,\"start\":31934},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":209370711},\"end\":32726,\"start\":32379},{\"attributes\":{\"id\":\"b5\"},\"end\":33060,\"start\":32728},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":237259752},\"end\":33346,\"start\":33062},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":244169344},\"end\":33691,\"start\":33348},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":244305177},\"end\":34056,\"start\":33693},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202786778},\"end\":35095,\"start\":34058},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1728538},\"end\":35280,\"start\":35097},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":977535},\"end\":35586,\"start\":35282},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206590743},\"end\":35933,\"start\":35588},{\"attributes\":{\"id\":\"b13\"},\"end\":36318,\"start\":35935},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206593880},\"end\":36643,\"start\":36320},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220919732},\"end\":36993,\"start\":36645},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":233182047},\"end\":37241,\"start\":36995},{\"attributes\":{\"id\":\"b17\"},\"end\":37541,\"start\":37243},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":236956903},\"end\":37872,\"start\":37543},{\"attributes\":{\"id\":\"b19\"},\"end\":38081,\"start\":37874},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":220666071},\"end\":38496,\"start\":38083},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":209405014},\"end\":38769,\"start\":38498},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4712004},\"end\":39033,\"start\":38771},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":67855970},\"end\":39346,\"start\":39035},{\"attributes\":{\"id\":\"b24\"},\"end\":39707,\"start\":39348},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":221150925},\"end\":39941,\"start\":39709}]", "bib_title": "[{\"end\":31400,\"start\":31363},{\"end\":31631,\"start\":31551},{\"end\":32483,\"start\":32379},{\"end\":32792,\"start\":32728},{\"end\":33130,\"start\":33062},{\"end\":33424,\"start\":33348},{\"end\":33763,\"start\":33693},{\"end\":34126,\"start\":34058},{\"end\":35128,\"start\":35097},{\"end\":35341,\"start\":35282},{\"end\":35662,\"start\":35588},{\"end\":35992,\"start\":35935},{\"end\":36377,\"start\":36320},{\"end\":36717,\"start\":36645},{\"end\":37036,\"start\":36995},{\"end\":37611,\"start\":37543},{\"end\":38161,\"start\":38083},{\"end\":38561,\"start\":38498},{\"end\":38829,\"start\":38771},{\"end\":39104,\"start\":39035},{\"end\":39751,\"start\":39709}]", "bib_author": "[{\"end\":31166,\"start\":31151},{\"end\":31196,\"start\":31166},{\"end\":31207,\"start\":31196},{\"end\":31412,\"start\":31402},{\"end\":31426,\"start\":31412},{\"end\":31435,\"start\":31426},{\"end\":31443,\"start\":31435},{\"end\":31645,\"start\":31633},{\"end\":31657,\"start\":31645},{\"end\":31669,\"start\":31657},{\"end\":31680,\"start\":31669},{\"end\":31693,\"start\":31680},{\"end\":31711,\"start\":31693},{\"end\":31719,\"start\":31711},{\"end\":32075,\"start\":32057},{\"end\":32092,\"start\":32075},{\"end\":32110,\"start\":32092},{\"end\":32498,\"start\":32485},{\"end\":32510,\"start\":32498},{\"end\":32520,\"start\":32510},{\"end\":32533,\"start\":32520},{\"end\":32810,\"start\":32794},{\"end\":32823,\"start\":32810},{\"end\":32837,\"start\":32823},{\"end\":32853,\"start\":32837},{\"end\":33146,\"start\":33132},{\"end\":33160,\"start\":33146},{\"end\":33173,\"start\":33160},{\"end\":33186,\"start\":33173},{\"end\":33437,\"start\":33426},{\"end\":33447,\"start\":33437},{\"end\":33459,\"start\":33447},{\"end\":33474,\"start\":33459},{\"end\":33484,\"start\":33474},{\"end\":33492,\"start\":33484},{\"end\":33776,\"start\":33765},{\"end\":33788,\"start\":33776},{\"end\":33800,\"start\":33788},{\"end\":33812,\"start\":33800},{\"end\":33824,\"start\":33812},{\"end\":33837,\"start\":33824},{\"end\":33852,\"start\":33837},{\"end\":34141,\"start\":34128},{\"end\":34152,\"start\":34141},{\"end\":34169,\"start\":34152},{\"end\":34181,\"start\":34169},{\"end\":34197,\"start\":34181},{\"end\":34213,\"start\":34197},{\"end\":34229,\"start\":34213},{\"end\":34241,\"start\":34229},{\"end\":34261,\"start\":34241},{\"end\":34274,\"start\":34261},{\"end\":34291,\"start\":34274},{\"end\":34305,\"start\":34291},{\"end\":34318,\"start\":34305},{\"end\":34334,\"start\":34318},{\"end\":34349,\"start\":34334},{\"end\":34365,\"start\":34349},{\"end\":34386,\"start\":34365},{\"end\":34402,\"start\":34386},{\"end\":34411,\"start\":34402},{\"end\":34423,\"start\":34411},{\"end\":34441,\"start\":34423},{\"end\":35157,\"start\":35130},{\"end\":35176,\"start\":35157},{\"end\":35370,\"start\":35343},{\"end\":35385,\"start\":35370},{\"end\":35401,\"start\":35385},{\"end\":35420,\"start\":35401},{\"end\":35674,\"start\":35664},{\"end\":35687,\"start\":35674},{\"end\":35702,\"start\":35687},{\"end\":35717,\"start\":35702},{\"end\":35737,\"start\":35717},{\"end\":35747,\"start\":35737},{\"end\":36013,\"start\":35994},{\"end\":36035,\"start\":36013},{\"end\":36049,\"start\":36035},{\"end\":36063,\"start\":36049},{\"end\":36076,\"start\":36063},{\"end\":36099,\"start\":36076},{\"end\":36398,\"start\":36379},{\"end\":36417,\"start\":36398},{\"end\":36431,\"start\":36417},{\"end\":36443,\"start\":36431},{\"end\":36459,\"start\":36443},{\"end\":36730,\"start\":36719},{\"end\":36736,\"start\":36730},{\"end\":36747,\"start\":36736},{\"end\":36760,\"start\":36747},{\"end\":36772,\"start\":36760},{\"end\":36780,\"start\":36772},{\"end\":36792,\"start\":36780},{\"end\":36803,\"start\":36792},{\"end\":36808,\"start\":36803},{\"end\":37050,\"start\":37038},{\"end\":37061,\"start\":37050},{\"end\":37078,\"start\":37061},{\"end\":37094,\"start\":37078},{\"end\":37313,\"start\":37296},{\"end\":37331,\"start\":37313},{\"end\":37348,\"start\":37331},{\"end\":37364,\"start\":37348},{\"end\":37380,\"start\":37364},{\"end\":37627,\"start\":37613},{\"end\":37641,\"start\":37627},{\"end\":37651,\"start\":37641},{\"end\":37664,\"start\":37651},{\"end\":37678,\"start\":37664},{\"end\":37948,\"start\":37935},{\"end\":37961,\"start\":37948},{\"end\":38177,\"start\":38163},{\"end\":38191,\"start\":38177},{\"end\":38203,\"start\":38191},{\"end\":38216,\"start\":38203},{\"end\":38229,\"start\":38216},{\"end\":38242,\"start\":38229},{\"end\":38256,\"start\":38242},{\"end\":38269,\"start\":38256},{\"end\":38575,\"start\":38563},{\"end\":38584,\"start\":38575},{\"end\":38592,\"start\":38584},{\"end\":38610,\"start\":38592},{\"end\":38615,\"start\":38610},{\"end\":38840,\"start\":38831},{\"end\":38851,\"start\":38840},{\"end\":38862,\"start\":38851},{\"end\":38873,\"start\":38862},{\"end\":38884,\"start\":38873},{\"end\":39115,\"start\":39106},{\"end\":39126,\"start\":39115},{\"end\":39137,\"start\":39126},{\"end\":39151,\"start\":39137},{\"end\":39162,\"start\":39151},{\"end\":39173,\"start\":39162},{\"end\":39435,\"start\":39426},{\"end\":39446,\"start\":39435},{\"end\":39457,\"start\":39446},{\"end\":39473,\"start\":39457},{\"end\":39484,\"start\":39473},{\"end\":39494,\"start\":39484},{\"end\":39505,\"start\":39494},{\"end\":39516,\"start\":39505},{\"end\":39769,\"start\":39753},{\"end\":39778,\"start\":39769},{\"end\":39789,\"start\":39778},{\"end\":39800,\"start\":39789},{\"end\":39811,\"start\":39800}]", "bib_venue": "[{\"end\":31149,\"start\":31060},{\"end\":31447,\"start\":31443},{\"end\":31732,\"start\":31719},{\"end\":32055,\"start\":31934},{\"end\":32546,\"start\":32533},{\"end\":32881,\"start\":32853},{\"end\":33190,\"start\":33186},{\"end\":33496,\"start\":33492},{\"end\":33856,\"start\":33852},{\"end\":34448,\"start\":34441},{\"end\":35180,\"start\":35176},{\"end\":35424,\"start\":35420},{\"end\":35751,\"start\":35747},{\"end\":36102,\"start\":36099},{\"end\":36463,\"start\":36459},{\"end\":36812,\"start\":36808},{\"end\":37098,\"start\":37094},{\"end\":37294,\"start\":37243},{\"end\":37682,\"start\":37678},{\"end\":37933,\"start\":37874},{\"end\":38279,\"start\":38269},{\"end\":38619,\"start\":38615},{\"end\":38888,\"start\":38884},{\"end\":39177,\"start\":39173},{\"end\":39424,\"start\":39348},{\"end\":39815,\"start\":39811}]"}}}, "year": 2023, "month": 12, "day": 17}