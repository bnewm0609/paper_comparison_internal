{"id": 235616415, "updated": "2022-11-08 14:55:40.071", "metadata": {"title": "Optimizing Error-Bounded Lossy Compression for Scientific Data by Dynamic Spline Interpolation", "authors": "[{\"first\":\"Kai\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Sheng\",\"last\":\"Di\",\"middle\":[]},{\"first\":\"Maxim\",\"last\":\"Dmitriev\",\"middle\":[]},{\"first\":\"Thierry-Laurent\",\"last\":\"Tonellot\",\"middle\":[\"D.\"]},{\"first\":\"Zizhong\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Franck\",\"last\":\"Cappello\",\"middle\":[]}]", "venue": "2021 IEEE 37th International Conference on Data Engineering (ICDE)", "journal": "2021 IEEE 37th International Conference on Data Engineering (ICDE)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Today\u2019s scientific simulations are producing vast volumes of data that cannot be stored and transferred efficiently because of limited storage capacity, parallel I/O bandwidth, and network bandwidth. The situation is getting worse over time because of the ever-increasing gap between relatively slow data transfer speed and fast-growing computation power in modern supercomputers. Error-bounded lossy compression is becoming one of the most critical techniques for resolving the big scientific data issue, in that it can significantly reduce the scientific data volume while guaranteeing that the reconstructed data is valid for users because of its compression-error-bounding feature. In this paper, we present a novel error-bounded lossy compressor based on a state-of-the-art prediction-based compression framework. Our solution exhibits substantially better compression quality than all of the existing error-bounded lossy compressors, with comparable compression speed. Specifically, our contribution is threefold. (1) We provide an in-depth analysis of why the best-existing prediction-based lossy compressor can only minimally improve the compression quality. (2) We propose a dynamic spline interpolation approach with a series of optimization strategies that can significantly improve the data prediction accuracy, substantially improving the compression quality in turn. (3) We perform a thorough evaluation using six real-world scientific simulation datasets across different science domains to evaluate our solution vs. all other related works. Experiments show that the compression ratio of our solution is higher than that of the second-best lossy compressor by 20% 460% with the same error bound in most of the cases. \u223c", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icde/ZhaoDDTCC21", "doi": "10.1109/icde51399.2021.00145"}}, "content": {"source": {"pdf_hash": "a00847aeb5968c7f5fc0356965e910c44096a416", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "da4737e265d6d146006d849521c02a4e42f9fc8d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a00847aeb5968c7f5fc0356965e910c44096a416.txt", "contents": "\nOptimizing Error-Bounded Lossy Compression for Scientific Data by Dynamic Spline Interpolation\n\n\nKai Zhao \nSheng Di \nArgonne National Laboratory\nLemontILUSA\n\nMaxim Dmitriev maxim.dmitriev@aramco.com \nEXPEC Advanced Research Center\nSaudi AramcoSaudi Arabia\n\nThierry-Laurent D Tonellot thierrylaurent.tonellot@aramco.com \nEXPEC Advanced Research Center\nSaudi AramcoSaudi Arabia\n\nZizhong Chen chen@cs.ucr.edu \nFranck Cappello cappello@mcs.anl.gov \nArgonne National Laboratory\nLemontILUSA\n\nUniversity of Illinois at Urbana-Champaign\nChampaignILUSA\n\n\nUniversity of California\nRiversideCAUSA\n\nOptimizing Error-Bounded Lossy Compression for Scientific Data by Dynamic Spline Interpolation\n10.1109/ICDE51399.2021.00145\nToday's scientific simulations are producing vast volumes of data that cannot be stored and transferred efficiently because of limited storage capacity, parallel I/O bandwidth, and network bandwidth. The situation is getting worse over time because of the ever-increasing gap between relatively slow data transfer speed and fast-growing computation power in modern supercomputers. Error-bounded lossy compression is becoming one of the most critical techniques for resolving the big scientific data issue, in that it can significantly reduce the scientific data volume while guaranteeing that the reconstructed data is valid for users because of its compression-error-bounding feature. In this paper, we present a novel error-bounded lossy compressor based on a state-of-the-art prediction-based compression framework. Our solution exhibits substantially better compression quality than all of the existing error-bounded lossy compressors, with comparable compression speed. Specifically, our contribution is threefold. (1) We provide an in-depth analysis of why the bestexisting prediction-based lossy compressor can only minimally improve the compression quality. (2) We propose a dynamic spline interpolation approach with a series of optimization strategies that can significantly improve the data prediction accuracy, substantially improving the compression quality in turn. (3) We perform a thorough evaluation using six real-world scientific simulation datasets across different science domains to evaluate our solution vs. all other related works. Experiments show that the compression ratio of our solution is higher than that of the second-best lossy compressor by 20%\u223c460% with the same error bound in most of the cases.1643\n\nI. INTRODUCTION\n\nWith the ever-increasing scale of today's scientific simulations, vast amounts of scientific data are produced at every simulation run. Climate simulation [1], for example, can generate hundreds of terabytes of data in tens of seconds. A cosmology simulation, such as Hardware/Hybrid Accelerated Cosmology (HACC) [2] can produce dozens of petabytes of data when it performs an N-body simulation with up to several trillion particles. Such a vast amount of scientific data needs to be stored for post hoc analysis, creating a huge challenge to the scientific data management systems [3]- [7]. Many scientists also need to share the large amounts of data across different sites (i.e., endpoints) through a data-sharing web service (such Corresponding author: Sheng Di, Mathematics and Computer Science Division, Argonne National Laboratory, 9700 Cass Avenue, Lemont, IL 60439, USA as the Globus toolkit [8]) on the Internet. Thus, the ability to significantly compress extremely large scientific datasets with controlled data distortion is critical to today's science work.\n\nIn the scientific research domain, the users often adopt scientific data libraries such as NetCDF [9] and HDF5 [10] to manage the scientific data due to their performance advantage and better support of multidimensional objects over traditional database management systems. Those scientific data libraries have database features including metadata, data indexing, data manipulation, and data visualization tools [11], [12]. In particular, due to the vast amount of data to deal with, these data management libraries also support integrating different data compressors. For example, HDF5 offers a filter mechanism [13] to allow users to call various compressors (including SZ [14], ZFP [15], Zlib [16], etc.) transparently when storing scientific data.\n\nError-bounded lossy compression techniques [17]- [19] have been developed for several years, and they have been widely recognized as an optimal solution to reduce the storage demand of scientific data management systems. For example, many researchers [20], [21] have verified that the data reconstructed through error-bounded lossy compressors is totally acceptable for users' post hoc analysis. Many successful stories also showed that error-bounded lossy compressors not only can significantly reduce the storage space but also may substantially improve the data-moving performance with useracceptable data distortions. For example, Liang et al. [22] showed that an error-bounded lossy compressor can improve the overall I/O performance by 60X, with no degradation of visual quality on the reconstructed data. Kukreja et al. [23] showed that using error-bounded lossy compression can get high compression ratios without affecting the convergence or final solution of the full waveform inversion solver clearly.\n\nThe SZ compression library has been recognized by independent assessments [17], [21], [24], [25] as the best-in-class error-bounded lossy compressor for scientific datasets, especially because it has gone through many careful optimizations [14], [17], [18], [26]- [29].\n\nIn this paper, we significantly improve the error-bounded lossy compression quality for scientific datasets, by designing a dynamic best-prediction-selection strategy and proposing a novel, spline interpolation based prediction approach with a series of optimizations. This predictor completely eliminates the serious storage overhead compared with the linear-regression predictor used in SZ. Our contribution is threefold. \u2022 We provide an in-depth analysis of the latest version of SZ and identify significant drawbacks of its prediction method; the analysis also sheds light on our new design. The critical challenge in the current design of SZ comes from its linear-regression prediction method, which has two significant drawbacks. On the one hand, it suffers from limited accuracy in predicting nonlinear varied datasets. Many scientific simulations (such as seismic wave simulation [30] and quantum chemistry research [31]) , however, may produce a vast amount of data with nonlinear features, such that SZ cannot work very effectively on them. On the other hand, the linear-regressionbased prediction needs to store several coefficients (e.g., four coefficients per block for 3D compression) in each block of data, introducing significant overhead especially when a relatively high compression ratio is required. \u2022 We propose a novel prediction method that can significantly improve the compression ratio compared with the linear-regression prediction method. On the one hand, cubic spline interpolation is included in our novel approach to represent high order data variation, which obtains much higher prediction accuracy over linear-regression for datasets with nonlinear data variation characteristics.\n\nOn the other hand, we derive the constant coefficients in our interpolation approach such that the coefficient storage overhead can be completely eliminated. We further propose a dynamic optimization strategy to select the best predictor from between the novel spline interpolation approach and the multilevel Lorenzo predictor to improve the overall compression quality. \u2022 We perform a comprehensive assessment of our solution versus five state-of-the-art error-controlled lossy compressors, using multiple real-world simulation datasets across different scientific domains. Experiments show that our solution improves the compression ratio by 20%\u223c460% over the second-best compressor with the same error bound and experiences no degradation in the post-analysis accuracy. The rest of the paper is organized as follows. In Section II we discuss the related work. In Section III we formulate the research problem. In Section IV we offer an in-depth analysis of the pros and cons of SZ In Section V we describe our solution and detailed optimization strategies. In Section VI we present the evaluation results compared with five other stateof-the-art lossy compressors using real-world applications. In Section VII we conclude with a discussion of future work.\n\n\nII. RELATED WORK\n\nData compression is becoming a critical technique for database management systems. For time series databases, Gorilla [32] is proposed to improve query performance using lossless compression techniques including XOR and variablelength encoding. AMMMO [33] utilizes machine learning to select the best lossless compression scheme for each data point in time series databases. SciDB [34] is a science-oriented database system that supports several lossless algorithms including run-length encoding and adaptive huffman encoding. Snappy [35] is a high-speed lossless compression framework used by many databases such as InfluxDB [36]. Zlib [16] and Zstandard [37] are another two state-of-the-art lossless compressors.\n\nAlthough lossless compression techniques are widely used in database management systems, they are not suitable for scientific data. Lossless compressors suffer from very limited compression ratios (generally \u223c2 or even less) on scientific data [38] since lossless compression techniques rely on repeated byte-stream patterns whereas scientific data is often composed of diverse floating-point numbers. Thus, lossy compression for scientific data has been studied for years.\n\nUnlike traditional lossy compression techniques (such as Jpeg2000 [39]) that were designed mainly for image data, the error-bounded lossy compression can not only get a fairly high compression ratio (several dozens, hundreds, or even higher) but also guarantee that the reconstructed data is valid for scientific post-analysis in terms of the user-defined compression error bound. Error-bounded lossy compression can be categorized as higher-order singular value decomposition (HOSVD)-based models such as TTHRESH [40], transformbased models such as ZFP [15], and prediction-based models including SZ [14], [17].\n\nThere are also some machine learning (ML) based lossy compressors such as LFZIP [41]. ML compressors have two drawbacks in terms of scientific data prediction. First, the weights of ML models have non negligible size to be stored and ML models need to be retrained for data in different scientific domains. As a result, the model weights should be stored together with compressed data and this brings significant storage overhead. Second, ML compressors involves the ML inference process which has much higher computational cost than traditional methods including interpolation based predictors that are linear time complexity.\n\nIn our work, we choose the prediction-based model because SZ has been recognized as the leading compressor in the scientific data compression community. In fact, how to leverage SZ to improve compression quality has been studied for more than two years. Tao et al. [42] developed a strategy that can combine SZ and ZFP to optimize the compression ratios based on a more significant metric, peak signal-to-noise ratio (PSNR). Liang et al. [27] further analyzed the principles of SZ and ZFP and developed a method integrating ZFP into the SZ compression model, which can further improve the compression quality. Zhao et al. [28] proposed to adopt second-order Lorenzo+regression in the prediction methods and developed an autotuning method to optimize the parameters of SZ. Liang et al. [43] accelerated the performance of MultiGrid Adaptive Reduction of Data (MGARD) [44] and used SZ to compress the nodal points generated by the MGARD framework [44], which can improve the compression ratios significantly.\n\nAll these existing SZ-related solutions have to rely on the linear regression prediction to a certain extent. This is a critical restriction to the compression quality improvement, which will be analyzed deeply in Section IV.\n\n\nIII. PROBLEM FORMULATION\n\nIn this section we describe the research problem formulation. Given a scientific dataset (denoted by D) composed of N floating-point values (either single precision or double precision) and a user-specified absolute error bound (e), the objective is to develop an error-bounded lossy compressor that can always meet the error-bounding constraint at each data point with optimized compression quality and comparable performance (i.e., speed).\n\nRate distortion is arguably the metric most commonly used by the lossy compression community to assess compression quality. It can be converted to the commonly used statistical data distortion metric known as normalized root mean squared error, and it is a good indicator of visual quality. Rate distortion involves two critical metrics: peak signal-to-noise ratio and bit rate. PSNR can be written as the following: Formula (1). PSNR = 20 log 10 (vrange(D)\u221210log 10 (mse(D,D )), , (1) where D is the reconstructed dataset after decompression (i.e., decompressed dataset) and vrange(D) represents the value range of the original dataset D (i.e., the difference between its highest value and lowest value). Obviously, the higher the PSNR value is, the smaller the mean squared error, which means higher precision of the decompressed data.\n\nBit rate is used to evaluate the compression ratio (the ratio of the original data size to the compressed size). Specifically, bit rate is defined as the average number of bits used per data point in the compressed data. For example, suppose a single-precision original dataset has 100 million data points; its original data size is 100,000,000\u00d74 bytes (i.e., about 400 MB). If the compressed data size is 4,000,000 bytes (i.e., a compression ratio of 100:1), then the bit rate can be calculated as 32/100 = 0.32 (one single-precision number takes 32 bits). Obviously, smaller bit rate means higher compression ratio.\n\nTwo other important compression assessment metrics are compression speed (denoted by s c ) and decompression speed (denoted by s d ). They are defined as the amount of data processed per time unit (MB/s).\n\nIn our research, we focus on the optimization of compression quality (i.e., rate distortion) with high performance, which can be formulated as follows:\nOptimize rate-distortion subject to |d i \u2212 d i | \u2264 e s c (newsol.) \u2248 s c (sz) s d (newsol.) \u2248 s d (sz),(2)\nwhere d i and d i are referred to the value of the ith data point in the original dataset D and the decompressed dataset D by the new compression solution, respectively. The notations s c (newsol.) and s d (newsol.) represent the compression speed and decompression speed of the new solution, respectively, and s c (sz) and s d (sz) represent the compression speed and decompression speed of the original SZ compressor, respectively. That is, we are trying to increase the compression ratio with the same level of data distortion and comparable compression/decompression performance compared with SZ as a baseline (because SZ has been confirmed as a fairly fast lossy compressor in many existing studies [22], [43]), In our evaluation in Section VI, not only do we present the rate distortion results for many different datasets at different bit-rate ranges, but we also assess the impact of our lossy compressor on the results of decompressed-data-based postanalysis on one production-level seismic simulation research.\n\n\nIV. DEEPLY UNDERSTANDING THE PROS AND CONS OF SZ\n\nIn this section, we first give a review of the current SZ design and then provide an in-depth analysis of a serious problem in the latest version of the SZ compressor (SZ2.1) [18]. Understanding this problem is fundamental to understanding why our new solution can significantly improve the compression ratio.\n\n\nA. Review of SZ Lossy Compression Framework\n\nSZ2.1 [18], the latest version of SZ, has been recognized as an excellent error-bounded lossy compressor based on numerous experiments with different scientific applications by different researchers [27], [28], [42].\n\nSZ2.1 involves four stages during the compression: (1) data prediction, (2) linear-scale quantization, (3) Huffman encoding, and (4) lossless compression such as Zlib [16]. We briefly describe the four steps, and we refer readers to read our prior papers [17], [18] for technical details.\n\n\u2022 Step 1: data prediction. In this step, SZ predicts each data point by its nearby data values. The prediction methods differ with various versions (from 0.1 through 2.1). For example, SZ 0.1\u223c1.0 [14] adopted a simple onedimensional adaptive curve-fitting method, which selects the best predictor for each data point from among three candidates: previous-value fitting, linear-curve fitting, and quadratic-curve fitting. SZ1.4 [17] completely replaced the curve-fitting method by a multidimensional first-order Lorenzo predictor, significantly improving compression ratios by over 200% over SZ1.0. SZ2.0\u223cSZ2.1 further improved the prediction method by proposing a blockwise linear regression predictor that can significantly enhance compression ratios by 150%\u223c800% over SZ1.4, especially for cases with a high compression-ratio (i.e., when the error bound is relatively large). \u2022 Step 2: linear-scale quantization. In this step, SZ computes the difference (denoted \u2206) between the predicted value (calculated in Step 1) and the original value for each data point and then quantizes \u2206 based on the userpredefined error bound (e). The quantization bins are equal-sized and are twice as large as the error bound, such that the maximum compression errors must be controlled within the specified error bound. After this step, all floating-point values are converted to integer numbers (i.e., quantization numbers), most of which are to be close to zero, especially when the data are fairly smooth in locality or the predefined error bound is relatively large. \u2022 Step 3: customized Huffman encoding. SZ has a tailored integer-based Huffman encoding algorithm to encode the quantization numbers generated by Step 2. \u2022 Step 4: lossless compression. The last step in SZ is adopting a lossless compressor with a pattern-recognized algorithm such as LZ77 [45] to further improve the compression ratios significantly.\n\n\nB. Critical Features of SZ Compression Framework\n\nFirst, SZ is a very flexible compression framework, in which the data prediction is the most critical step. More accurate data prediction will result in more quantization numbers being close to zero which leads to a better compression ratio during the encoding and lossless compression steps. Thus we have explored other more efficient predictors in the past two years (from version 0.1 through the latest released version 2.1, as well as a few recent prototypes [27], [28]). Accordingly, we are still focused only on the data prediction stage in this paper.\n\nSecond, SZ has to follow a necessary condition, in order to guarantee that the compression errors are always within the user-predefined error bound. For the same data point, its predicted value during the compression stage has to be exactly the same as the one predicted in the decompression stage. Otherwise, the compression errors would be accumulated easily during the decompression, causing totally uncontrolled compression errors. Thus, in the compression stage, SZ has to predict each data point by its nearby lossy decompressed values instead of the original values, which will in turn degrade the prediction accuracy (as exemplified in our prior work [17]). We proposed the linear-regression predictor in SZ2.1 [18], which can mitigate this issue to a certain extent. Such a predictor, however, has a significant drawback and may substantially inhibit the compressor from obtaining a high compression ratio in many cases. We analyze this drawback in detail in the following text.\n\n\nC. Review of Linear Regression Predictor in SZ2.1\n\nIn what follows, we describe the linear regression predictor used in SZ2.1 and its serious drawback.\n\nIn SZ2.1, the whole dataset is split into equal-sized blocks (e.g., 6\u00d76\u00d76 for a 3D dataset) and performs a linearregression-based prediction when the data inside the block is relatively smooth or the error bound is relatively high. The basic idea is to use linear regression to construct a hyperplane in each block, such that the data inside the block can be approximated by the hyperplane with minimized min squared error (MSE), as illustrated in Fig. 1. The details can be found in our prior work [18].\n\n\nD. Serious Dilemma of Linear-Regression Predictor in SZ2.1\n\nIn order to get a high compression quality (i.e., a very good rate-distortion result), the four coefficients need to be f(x,y)=\u03b2 0 +\u03b2 1 x + \u03b2 2 y x y compressed based on a certain error bound, which may introduce a serious dilemma: a higher error bound used on coefficient compression will decrease the overhead of storing the coefficients (to be demonstrated in Fig. 2) but also decrease the regression accuracy of the constructed hyperplane (to be demonstrated in Fig. 3). We confirm this issue by four real-world scientific simulations (QMCPack [46], RTM [30], Hurricane [47], and NYX [48]), which are commonly used by scientists in quantum structure research, seismic imaging for oil and gas exploration, climate research, and cosmology research, respectively. More details about these applications are given in Section VI. We exemplify the results using specific fields (e.g., time step 1500 of RTM data, the W field of Hurriane, and velocity z in NYX) because of the space limits and similar results in other fields. Fig. 2 shows that the overhead always increases with decreasing error bounds used on the compression of coefficients. Specifically, we observe that when the error bound decreases from 0.1 to 0.01, the coefficient overhead in the compressed data increases from 55% to 68%, from 25% to 37%, from 40% to 53%, and from 60% to 70%, for the four test cases, respectively. The compression ratios (the red curve) thus degrade from 179 to 128, from 102 to 86, from 114 to 90, and from 152 to 118, respectively.\n\nWe present a slice segment of the four application datasets in Fig. 3 to illustrate that the error bounds of the coefficient compression would significantly affect the prediction accuracy of the constructed linear-regression hyperplane. For instance, when the coefficients' compression error bound is set to 0.001 for QMCPack and 0.01 for RTM (time step 1500), the constructed hyperplane (the yellow curve) can fit the real data (the red curve) well, but the fitting will be much worse with increasing error bounds. In the case with a relatively large error bound (e.g., 0.1 in QMAPack), the hyperplane will downgrade to a simple horizontal line (see blue lines in the figures), because simply using the neighbor data value is \"accurate\" enough for the large error-bounded compression of the coefficients. This will definitely result in large prediction errors (the difference between predicted value and raw value) significantly degrading the final compression ratios.\n\nIn Fig. 4, we demonstrate that the latest version of SZ (v2.1) may cause significant loss of the data visualization, especially and 568 for the two test cases). We observe that SZ2.1 suffers from a significant undesired block texture artifact, resulting from its blockwise linear-regression design.\n\nTo address the serious issue of the linear regression predictor, we developed a novel efficient predictor based on a dynamic spline interpolation, such that compression quality (rate distortion) can be significantly improved for almost all application datasets, with little performance overhead.\n\n\nV. ERROR-BOUNDED LOSSY COMPRESSION WITH A DYNAMIC MULTIDIMENSIONAL SPLINE INTERPOLATION\n\nWe present the design overview in Fig. 5, with yellow rectangles indicating the differences between our design and the classic SZ compressor and with highlighted rectangles indicating the critical steps. The fundamental idea is to develop a dynamic multidimensional spline interpolation-based predictor (i.e., solution P2 shown in Fig. 5) to replace the linearregression-based predictor such that the coefficient overhead can be completely eliminated while still keeping a fairly high prediction accuracy. Our newly designed interpolation-based predictor starts with one data point and performs interpolation and linear-scale quantization alternatively along each dimension recursively until all data points are processed. Two alternative approaches can be used to perform the interpolation in the multidimensional space. We can build a multidimensional curve to fit all the already-processed data points, or we can build multiple 1D curves to do the interpolation. We choose the latter because the former is much more expensive.\n\nIn what follows, we introduce the background of spline interpolation (Section V-A), followed by our design of dynamic multidimensional spline interpolation based predictor (Sections V-B, V-C, and V-D).\n\n\nA. Introduction to Spline Interpolation\n\nInterpolation is widely used in the field of engineering and science to construct new data points with a set of known data points. Interpolation techniques attempt to build a curve  that goes through all the known data points. It differs from regression analysis, which usually seeks a curve that most closely fits the known data points according to a specific mathematical criterion such as mean squared error. The curve generated by regression may not go through all known points. The most popular interpolation methods can be categorized into three types: piecewise constant interpolation, polynomial interpolation, and spline interpolation. Piecewise constant interpolation always uses the nearest known data points to estimate the new data point, so it has a simple implementation and fast speed. However, its ability to estimate complex curves is limited because it does not consider the surrounding data points. Polynomial interpolation is designed to find a polynomial with the lowest possible degree that passes through all the known data points. If the number of known data points is large, the polynomial may suffer highly inaccurate oscillation between the data points. This issue is well known as Runge's phenomenon and could be mitigated by spline interpolation. Spline interpolation uses piecewise polynomials to define the estimation curve. If the degree of the polynomials is 1, the spline interpolation turns to linear interpolation. If the degree of polynomials is 3, it is known as cubic spline interpolation. Cubic spline polynomials have different restrictions. In this paper, we use not-a-knot restriction for cubic spline interpolation.\n\n\nB. Spline Interpolation Designed for Scientific Data\n\nIn this sub-section, we introduce a basic interpolation method and derive closed-form formulas with the optimal coefficients, which is a fundamental work to the development of our following multi-dimensional interpolation predictor.\n\nWe propose a light-weight cubic interpolation based prediction method for each unknown data point by only using its four surrounding data values, to address the drawbacks of the conventional interpolation methods. The accuracy of polynomial interpolation could be affected significantly by Runge's phenomenon when interpolating across multiple regions with different locality features. Cubic spline interpolation can prevent large oscillation, but it has high computational cost as it needs to solve a huge linear system. To avoid high computation cost, we precompute a closed-form interpolation formula based on four specific neighbor data points (e.g., using the data points i\u22123, i\u22121, i+1 and i+3 to predict data point i as shown in Figure 6). In what follows, we mainly use a 1D example to illustrate how we derive the interpolation formula, but the formula can be extended to multidimensional cases easily.\n\nLemma 1: Denote the dataset as d = (d 1 , d 2 , ..., d n ) with n as the total number of elements. The prediction values are denoted as p=(p 1 , p 2 , ..., p n ). We consider all elements in odd-index positions as preknown and use them to predict the elements in even-index positions. The prediction formulas of linear and cubic spline interpolation are shown in Table I. \np i = 1 2 d i\u22121 + 1 2 d i+1 Cubic spline p i = \u2212 1 16 d i\u22123 + 9 16 d i\u22121 + 9 16 d i+1 \u2212 1 16 d i+3\nProof: The linear formula is easy to derive, so we prove only the cubic spline formulas as follows. In our designed cubic spline interpolation, the known data points d i\u22123 ,d i\u22121 ,d i+1 and d i+3 are used to predict the data point p i . Three spline curves correspond to the known data points:\nf 1 (x) f 2 (x) f 3 (x) d i-3 d i-1 d i+1 d i+3 p i i-3 i-1 i+1 i+3 i value index\nKnown points Unknown points Interpolation \nf 1 (x) = a 1 (x\u2212(i\u22123)) 3 +b 1 (x\u2212(i\u22123)) 2 +c 1 (x\u2212(i\u22123))+\u03b4 1 f 2 (x) = a 2 (x\u2212(i\u22121)) 3 +b 2 (x\u2212(i\u22121)) 2 +c 2 (x\u2212(i\u22121))+\u03b4 2 f 3 (x) = a 3 (x\u2212(i+1)) 3 +b 3 (x\u2212(i+1)) 2 +c 3 (x\u2212(i+1))+\u03b4 3(3)\nThe scope of f 1 , f 2 , and f 3 is [i\u22123,i\u22121], [i\u22121,i+1], and [i+1,i+3] (as shown in Fig. 6). The spline curves should pass through the known data points, so we have\nf 1 (i \u2212 3) = d i\u22123 ; f 1 (i \u2212 1) = d i\u22121 f 2 (i \u2212 1) = d i\u22121 ; f 2 (i + 1) = d i+1 f 3 (i + 1) = d i+1 ; f 3 (i + 3) = d i+3(4)\nThe first derivatives of To have a smooth curve, we should let the adjacent spline functions have the same first derivatives and the same second derivatives on the joint data points.\nf 1 (x) is f 1 (x) = 3a 1 (x \u2212 (i \u2212 3)) 2 +2bf 1 (i \u2212 1) = f 2 (i \u2212 1); f 2 (i + 1) = f 3 (i + 1) f 1 (i \u2212 1) = f 2 (i \u2212 1); f 2 (i + 1) = f 3 (i + 1)(5)\nThe not-a-knot restriction requires the third derivative of f to be equal on locations i \u2212 1 and i + 1.\nf 1 (i \u2212 1) = f 2 (i \u2212 1); f 2 (i + 1) = f 3 (i + 1)(6\n) Using the system of Equations (4), (5), and (6), we can derive\na 2 = \u2212 1 48 d i\u22123 + 1 16 d i\u22121 \u2212 1 16 d i+1 + 1 48 d i+3 b 2 = 1 8 d i\u22123 \u2212 1 4 d i\u22121 + 1 8 d i+1 c 2 = \u2212 1 6 d i\u22123 \u2212 1 4 d i\u22121 + 1 2 d i+1 \u2212 1 12 d i+3 \u03b4 2 = d i\u22121 .(7)\nThus the prediction value of pi will be (8) is the cubic formula in Table I. We discuss why we adopt only four known data points in our interpolation instead of six or more data points. If we use six data points d i\u22125 , d i\u22123 , d i\u22121 , d i+1 , d i+3 , and d i+5 to predict p i , the formula by not-a-knot spline turns out to be\np i = f 2 (i) = \u2212 1 16 d i\u22123 + 9 16 d i\u22121 + 9 16 d i+1 \u2212 1 16 d i+3 . (8) Equationp i = di\u22125 80 \u2212 di\u22123 10 + 47 80 d i\u22121 + 47 80 d i+1 \u2212 di+3 10 + di+5 80 .(9)\nCompared with Equation (8), Equation (9) involves two additional data points d i\u22125 and d i+5 , but the weight of the two data points is only 1/80, which means a very limited effect on the prediction. Moreover, it has 50% higher computation cost compared with Equation (8). Hence, we choose to use four data points for prediction, as shown in Table I.\n\nIn addition, we note that the linear spline interpolation may exhibit better prediction accuracy than the cubic spline does when setting a relatively large error bound (as shown in Table II). The reason is that our interpolation method relies on the reconstructed data values generated after a linear-scale quantization step, so that the reconstructed data is lossy to a certain extent. When the error bound is relatively large, the loss of these reconstructed data would degrade the prediction accuracy, and the more data points used in the interpolation, the higher the impact on the accuracy. Since linear spline adopts fewer data points, it could be superior to cubic spline especially when the error bound is relatively large. This possibility motivated us to design a dynamic method selecting the better interpolation type (linear or cubic) in practice. \n\n\nC. Multilevel Multidimensional Spline Interpolation\n\nThe previous derivation works in the 1D case with 50% of preknown data points, based on which we predict the other 50%. In this section, we extend this interpolation method to support data prediction on the entire multidimensional dataset. We use Fig. 7 to demonstrate the multilevel solution with linear interpolation; cubic interpolation has the same multilevel design. Suppose the dataset has n elements in one dimension. The number of levels required to cover all elements in this dimension is l = 1 + log 2 n . At level 0, we use 0 to predict d 1 , followed by the error-bounded linear-scale quantization. We perform a series of interpolations from level 1 to level l\u22121 recursively, as shown in Fig. 7. At each level, we use errorbounded linear-scale quantization to process the predicted value such that the corresponding reconstructed data must be within the error bound. We denote the reconstructed data as d 1 , d 2 , ..d n , as shown in the figure.\nLevel 1 Level 2 Level 3 dim0 dim1 dim0 dim1 dim0 dim1 dim0 dim1 dim0 dim1 dim0 dim1\nDim0 interpolation\n\n\nDim1 interpolation\n\nDim1 interpolation\n\n\nDim1 interpolation Dim0 interpolation\n\n\nDim0 interpolation\n\nKnown data points Unknown data points (to be predicted) interpolation\n\n\nFig. 8. Illustration of Multidimensional Linear Spline Interpolation\n\nSuch a multilevel interpolation is applied on a multidimensional dataset, illustrated in Fig. 8 with a 2D dataset as an example. We perform interpolation separately along all dimensions at each level, with a fixed sequence of dimensions. A 2D dataset, for example, has two possible sequences: dim 0 \u2192dim 1 and dim 1 \u2192dim 0 . A 3D dataset has 6 possible sequences. In our solution, we propose to check only two sequences, dim 0 \u2192dim 1 \u2192dim 2 (sequence 1) and dim 2 \u2192dim 1 \u2192 dim 0 (sequence 2) instead of all 6 possible combinations. On the one hand, the last interpolation dimension involves about 50% of the data points (much more than other dimensions), so which dimension to be put in the end of the sequence determines the overall prediction accuracy. On the other hand, we note that either the highest or lowest dimension in scientific datasets tends to be smoother than other dimensions without loss of generality, as confirmed by the first three columns of Table III (with all 6 applications), which presents the autocorrelation (AC) of each dimension (higher AC means smoother data). Accordingly, putting either dim0 or dim2 in the end of the sequence at each level will get lower overall prediction errors, as validated in Table III. Hence, we also develop a dynamic strategy to select the best-fit sequence of dimensions from among the two candidates, as detailed in the next subsection. \n\n\nD. Dynamic Optimization Strategies\n\nIn this section we propose a dynamic design with two adaptive strategies: (1) automatically optimizing the spline interpolation predictor (Trial run B in Fig. 5) by selecting the best-fit interpolation type (either linear or cubic) and optimizing the sequence of interpolation dimensions and (2) automatically selecting the better predictor between the Lorenzo-based predictor (Trial run A in Fig. 5) and the interpolation predictor.\n\nWe use a uniform sampling method to determine the best interpolation settings for the input dataset. There are two settings to optimize for the multidimensional interpolation predictor: the interpolation type and the dimension sequence. We adopt a uniform sampling method with only 3% total data points to select the better interpolation type with the higher compression ratio.\n\nWe note that the spline interpolation predictor does not work as effectively as the multilayer Lorenzo predictor [17], [28] on the relatively nonsmooth dataset, especially when the user's error bound is relatively small (as shown in Table IV). As a result, our final solution is selecting the better predictor from our spline interpolation method and Lorenzo method. \n\n\nVI. EXPERIMENTAL EVALUATION\n\nIn this section we present the experimental setup and discuss the evaluation results and our analysis.\n\nA. Experimental Setup 1) Execution Environment: We perform the experiments on the Argonne Bebop supercomputer. Each node in Bebop is driven by two Intel Xeon E5-2695 v4 processors with 128 GB of DRAM.\n\n2) Applications: We perform the evaluation using six realworld scientific applications from different domains:\n\n\u2022 QMCPack: An open source ab initio quantum Monte Carlo package for the electronic structure of atoms, molecules, and solids [46].  Table V. Some data fields are transformed to their logarithmic domain before compression for better visualization, as suggested by domain scientists. 3) State-of-the-Art Lossy Compressors in Our Evaluation: In our experiment we compare our new compressor with five other state-of-the-art error-bounded lossy compressors (SZ2.1 [18], ZFP0.5.5 [15], SZ(Hybrid) [27], SZ(SP+PO) 1 [28] and MGARDx [43]), which have been recognized as the best in class (validated by different researchers [18], [21], [25] [16], and Fpzip [51] in our experiment as a comparison with lossy compressors. Brotli, Snappy and Zstandard (Zstd) are depolyed in many industrial data management systems. LZMA is the default compression method of 7-Zip. Zlib [16] is one of the most widely used compressor in operating systems. Fpzip [51] is a compressor targeted at floating-point data.\n\n\n5) Evaluation Metrics:\n\nWe evaluate the six lossy compressors based on five critical metrics, as described below.\n\n\u2022 Compression ratio (CR) based on the same error bound:\n\nThe descriptions of CR and absolute error bound are de-fined in Section III. Without loss of generality, we adopt value-range-based error bound (denoted as ), which takes the same effect with absolute error bound (denoted e) because e = (max(D) \u2212 min(D)). \u2022 Compression speed and decompression speed: \n\n\nB. Evaluation Results and Analysis\n\nFirst, we verified the maximum compression errors for all six lossy compressors based on all the application datasets with different error bounds. Experiments confirm that they all respect the error bound constraint very well. Fig. 9 shows the distribution of compression errors of our solution on two error bounds ( =1E-3 and =1E-4, in other words, e=0.033&0.0033 for QMCPACK and e=8.2E-5&8.2E-6 for RTM). We can clearly see that the compression errors are 100% within the absolute error bound (e) for all data points.  Table VI presents the compression ratios of the six lossy compressors based on the six real-world applications with the same error bounds. We can clearly observe that our solution always exhibits the highest compression ratio in all cases. In particular, the compression ratio of our solution is higher than other compressors by 20%\u223c460% in most cases. For example, when setting the error bound to 1E-3 for compressing RTM data, the second-best compressor ((SZ(SP+PO)) gets a compression ratio of 114.4, while our compressing ratio reaches up to 397.6 (with a 247.5% improvement). The key reason our solution can get a significantly higher compression ratio is twofold: (1) we significantly improve the prediction accuracy by a dynamic spline interpolation, and (2) some other compressors such as ZFP and MGARDx suffer from the precision-overpreservation issue (i.e., the actual maximum errors are smaller than the required error bound, as verified by prior works [14], [17], [43].\n\nTable VII compares the compression/decompression speed among all six lossy compressors for all six applications. It clearly shows that our solution exhibits compression performance similar to that of SZ2.1 and MGARDx, and its decompression performance is also comparable to that of SZ2.1 and is about 30% higher than that of MGARDx.\n\nData smoothness and error bound settings are two key factors that affect the compression ratio and speed. In the SZ framework, datasets with better local smoothness or with larger error bound settings will result in smaller quantization data value range and more close-to-zero quantized data values. In general, this will get higher compression ratios and speed because such quantized data turns much easier to be compressed by the succeeding encoding steps. This analysis also applies to other lossy compressors such as ZFP and MGARDx that utilize the coding stage. We evaluate the data dumping and loading performance of the QMCPack simulation when using lossy compressors to demonstrate the performance impact of lossy compressors on scientific simulations. SZ2.1, SZ(SP+PO), ZFP, and our solution are assessed under the same level of data distortion (PSNR fixed to 70). The evaluation uses up to 4096 cores and each core processes 3.4GB of data. Fig. 10 shows that our solution leads to the highest data dumping and loading performance. In the scale of 4096 cores, QMCPACK simulation needs more than 3 hours to dump the data to disk without the help of lossy compressors. Our solution reduces the elapsed time to less than 100 seconds and it is 1.7X faster than the second-best one. Table VIII demonstrates the compression ratios of the six lossless compressors. It confirms our statement in Section II that lossless compressors have limited compression ratios on scientific datasets. Lossy compressors, on the other hand, can achieve much higher compression ratio as shown in Table VI.  As discussed in Section V-D, we designed a dynamic strategy to optimize the compression quality throughout the entire bit-rate range. Fig. 11 demonstrates that the dynamic strategy has a critical effect in the compression quality improvement. For instance, as shown in Fig. 11 (a), our solution always exhibits the best compression quality when the bit rate is lower than 2.5 because it adopts a dynamic interpolation method with optimized dimension sequences on a multilevel interpolation, whereas both linear interpolation and tricubic interpolation (shown in the figure) use a fixed sequence. (z\u2192y\u2192x). On the other hand, Fig. 11 (a) shows that our solution also keeps the best rate-distortion level when the bit rate is higher than 2.5, a result that is attributed to our accurate predictor selection algorithm (selecting a better predictor between interpolation and Lorenzo at runtime).  Fig. 12 presents the overall compression quality (i.e., rate distortion). One can see that our solution is the best in class from among all the related works for all six applications. In particular, with the same data-distortion level (PSNR), the compressed data size under our solution is about 50% of the compressed data size under the second-best compressor in most of the cases for RTM, Miranda, and QMCPack.\n\nWe demonstrate the visual quality of the decompressed data of four error-bounded lossy compressors in Fig. 13, using one slice image (slice 340) in the RTM dataset. The original visualization is shown in Fig. 4. The figure clearly shows that our solution keeps an excellent visual quality in the decompressed data with a compression ratio even up to 315. In contrast, other compressors suffer from prominent degradation in visual quality to different extents with the same compression ratios. In particular, SZ and ZFP suffer from undesired blockwise texture artifacts. We show in Fig. 14 and Fig. 15 that the final RTM image for a single shot is not degraded at all using our lossy compressor with very high compression ratios (about 2\u223c4\u00d7 higher than that of other compressors). We use value-range-based error bound 1.25E-3 in our solution for each time step. The RTM application requires propagating waves generated by a source signal, in a given subsurface model. At the beginning of the propagation the compression ratios are very high (10k+) when the waves are close to the source locations. Over time, the waves are propagating further in the model, resulting in more complex images and compression ratios dropping to about 70. The overall compression ratio is 274 because the compression ratio at most time steps can reach 300+ (e.g., CR=315 at time step 1500 as shown in Fig. 13 (a)). In this simulation we used one shot to generate the final image in Fig. 15. One can see a very good preservation of amplitudes and main structures of the lossy-compression-based final result, which is acceptable for post-analysis as confirmed by the seismic researchers. Our lossy compressor dramatically decreases the size of the RTM snapshots while not increasing the computation time compared with SZ 2.1. This can significantly lower the I/O throughput requirements and enable either faster turnaround or higherfidelity simulations for production-level seismic imaging. VII. CONCLUSION AND FUTURE WORK In this paper we present a novel error-bounded lossy compressor based on the SZ framework. We develop a dynamic spline interpolation approach with adaptive optimization strategies. We thoroughly evaluate the compression quality and performance of our solution compared with that of five other lossy compressors on six real-world scientific simulations. The key findings are summarized below.\n\n\u2022 Our analysis shows that the linear regression predictor has a significant problem because its coefficient overhead is non-negligible (25%\u223c70% in compressed data). \u2022 Our dynamic spline interpolation solution can improve the compression ratio by 457%, 244%, and 209% compared with the second-best compressor on RTM, QMC-PACK, and Miranda datasets, respectively. \u2022 Our solution has high compression/decompression performance comparable to that of SZ2.1. Its compression speed is 28%\u223c100% faster than other SZ-based methods such as SZ(Hybrid) and SZ(SP+PO). \u2022 Our solution keeps an extremely high visual quality in the decompressed data, whereas other lossy compressors suffer from prominent degradation in visualization with the same compression ratios. In the future work, we plan to improve the compression quality by exploring more effective prediction models and improve the performance by optimizing the code implementation.\n\n\nVIII. ACKNOWLEDGMENTS\n\nFig. 1 .\n1Illustration of Linear-regression-based prediction (2D dataset)\n\nFig. 3 .\n3Linear Regression Prediction Hyperplane with Different Error Bound Settings of Coefficients when the compression ratio (CR) is relatively high (e.g., 196\n\nFig. 4 .\n4Visualization of SZ Decompressed Data Based on Two Applications: (1) QMCPack -PSNR=56.2, CR=196, and (2) RTM -PSNR=50.7, CR=316\n\nFig. 5 .\n5Design Overview\n\nFig. 6 .\n6Illustration of Cubic Spline Interpolation\n\n\n1 (x\u2212(i\u22123))+c 1 . The second derivative is f 1 (x) = 6a 1 (x\u2212 (i\u22123))+2b 1 .The third derivative is f 1 (x) = 6a 1 . Derivatives of f 2 and f 3 are similar with f 1 .\n\n\n\u2022 RTM: Reverse time migration code for seismic imaging in areas with complex geological structures [30]. \u2022 NYX: An adaptive mesh, cosmological hydrodynamics simulation code. \u2022 Hurricane: A simulation of a hurricane from the National Center for Atmospheric Research in the United States. \u2022 Scale-LETKF: Local Ensemble Transform Kalman Filter (LETKF) data assimilation package for the SCALE-RM weather model. \u2022 Miranda: A radiation hydrodynamics code designed for large-eddy simulation of multicomponent flows with turbulent mixing. Detailed information about the datasets (all using single precision) is presented in\n\n\n(MB/s) and reconstructed size decompression time (MB/s). \u2022 Rate-distortion: The detailed description is in Section III. \u2022 Visualization with the same CR: Compare the visual quality of the reconstructed data based on the same CR. \u2022 Precision of final execution results of RTM data with lossy compression.\n\nFig. 9 .\n9Compression Error Distribution of Our Solution\n\nFig. 10 .\n10Parallel Performance Evaluation of QMCPack Simulation (SP(S+O) stands for SP(SP+PO))\n\nFig. 11 .\n11Our Solution Compared with Interpolation and Lorenzo\n\nFig. 12 .\n12Overall Evaluation (Lower Bit Rate / Higher PSNR \u2192 Better Quality)\n\nFig. 13 .\n13Visualization of Decompressed Snapshot Data (RTM)\n\nFig. 14 .Fig. 15 .\n1415Compression Ratio of RTM Data for Different Time Steps (with Value-Range-Based Error Bound 1.Visualization of RTM Image for One Shot\n\nTABLE I SPLINE\nIESTIMATIONS \n\nSpline method \nPrediction Value p i \nLinear spline \n\n\nTABLE II COMPARISON\nIIOF SPLINE METHODS PREDICTION ERRORDataset \n= 1E \u2212 2 \n= 1E \u2212 4 \nLinear Spline Cubic Spline Linear Spline Cubic Spline \nRTM (time step 1500) \n1.20E-4 \n1.27E-4 \n2.0E-5 \n8.3E-6 \nMiranda (velocityz) \n0.0026 \n0.0025 \n0.0061 \n0.0020 \nQMCPACK \n0.05 \n0.06 \n0.008 \n0.004 \nSCALE (QS) \n0.076 \n0.078 \n0.040 \n0.041 \nNYX (velocityz) \n123486 \n134820 \n22453 \n19978 \nHurricane (W) \n0.04 \n0.05 \n0.023 \n0.022 \n\n\n\nTABLE III AUTOCORRELATION\nIIIAND PREDICTION ERROR OF CUBIC SPLINE \nINTERPOLATION WITH DIFFERENT SEQUENCES OF DIMENSION \nSETTINGS, =1E\u22123 \n\nDataset \nAutocorrelation (Lag=4) \nPrediction Error \ndim2 \ndim1 \ndim0 \n0\u21921\u21922 \n0\u21922\u21921 \n2\u21921\u21920 \nRTM (time step 1500) \n0.88 \n0.58 \n0.45 \n2.17E-5 \n2.32E-5 \n2.51E-5 \nMiranda (velocityz) \n0.84 \n0.82 \n0.96 \n0.004 \n0.004 \n0.003 \nQMCPACK \n0.83 \n0.83 \n0.75 \n0.010 \n0.010 \n0.013 \nSCALE (QS) \n0.987 \n0.986 \n0.872 \n0.0447 \n0.0448 \n0.10 \nNYX (velocityz) \n0.9818 \n0.99 \n0.99 \n31668 \n29903 \n28975 \nHurricane (W) \n0.19 \n0.027 \n0.86 \n0.024 \n0.025 \n0.016 \n\n\n\nTABLE IV PREDICTION\nIVERROR OF MULTIDIMENSIONAL SPLINE INTERPOLATION \nPREDICTOR (S), REGRESSION PREDICTOR (R), AND LORENZO \nPREDICTOR (L) \n\nDataset \n= 1E \u2212 2 \n= 1E \u2212 7 \nS \nR \nL \nS \nR \nL \nRTM (time step 1500) \n1.2E-4 \n1.3E-4 \n2.0E-4 \n6.9E-6 \n1.0E-4 \n1.8E-7 \nMiranda (velocityz) \n0.02 \n0.03 \n0.05 \n0.001 \n0.02 \n6E-5 \nQMCPACK \n0.05 \n0.06 \n0.13 \n0.004 \n0.03 \n6E-4 \nSCALE (QS) \n0.07 \n0.16 \n0.11 \n0.04 \n0.15 \n0.01 \nNYX (velocityz) \n121436 \n132071 \n410083 \n15237 \n51963 \n16965 \nHurricane (W) \n0.04 \n0.05 \n0.06 \n0.01 \n0.04 \n0.004 \n\n\n\nTABLE V BASIC\nVINFORMATION ABOUT APPLICATION DATASETSApp. \n# files \nDimensions \nTotal Size \nDomain \nRTM \n3600 \n449\u00d7449\u00d7235 \n635GB \nSeismic Wave \nMiranda \n7 \n256\u00d7384\u00d7384 \n1GB \nTurbulence \nQMCPACK \n1 \n288\u00d7115\u00d769\u00d769 \n612MB \nQuantum Structure \nScale-LETKF \n13 \n98\u00d71200\u00d71200 \n6.4GB \nClimate \nNYX \n6 \n512\u00d7512\u00d7512 \n3.1GB \nCosmology \nHurricane \n48\u00d713 \n100\u00d7500\u00d7500 \n58GB \nWeather \n\n\n\nTABLE VI COMPRESSION\nVIRATIO COMPARISON BASED ON THE SAME ERROR BOUNDDataset \nSZ \nSZ \nSZ \nZFP \nMGARDx \nOurSol \nOurSol \n2.1 \n(Hybrid) \n(SP+PO) \nImprove % \n\nRTM \n\n1E-2 \n271.7 \n195.7 \n358.1 \n111.0 \n229.7 \n1997.5 \n457% \n1E-3 \n109.8 \n101.4 \n114.4 \n59.3 \n78.1 \n397.6 \n247% \n1E-4 \n57.3 \n44.4 \n63.0 \n34.9 \n38.3 \n116.3 \n84% \n\nMiranda \n\n1E-2 \n125.6 \n130.4 \n188.4 \n46.6 \n113.7 \n582.1 \n209% \n1E-3 \n59.9 \n55.4 \n58.4 \n25.6 \n38.0 \n160.7 \n168% \n1E-4 \n30.6 \n23.4 \n33.9 \n14.5 \n20.0 \n47.1 \n39% \n\nQMCPack \n\n1E-2 \n196.2 \n144.8 \n174.5 \n39.4 \n159.8 \n675.5 \n244% \n1E-3 \n51.1 \n53.4 \n68.0 \n21.2 \n47.1 \n204.3 \n200% \n1E-4 \n18.9 \n24.9 \n23.6 \n10.4 \n14.9 \n63.7 \n155% \n\nSCALE \n\n1E-2 \n84.3 \n94.2 \n108.2 \n14.5 \n52.8 \n157.0 \n45% \n1E-3 \n26.6 \n27.1 \n31.8 \n7.8 \n20.2 \n40.5 \n27% \n1E-4 \n14.0 \n13.2 \n14.1 \n4.6 \n10.4 \n14.9 \n5% \n\nNYX \n\n1E-2 \n43.6 \n33.2 \n48.7 \n12.0 \n24.7 \n59.4 \n22% \n1E-3 \n16.8 \n16.3 \n17.4 \n6.0 \n11.2 \n21.1 \n21% \n1E-4 \n7.6 \n8.0 \n8.1 \n3.7 \n5.5 \n9.1 \n12% \n\nHurricane \n\n1E-2 \n49.4 \n44.6 \n65.4 \n11.3 \n28.1 \n69.3 \n6% \n1E-3 \n17.6 \n17.9 \n19.8 \n6.7 \n12.7 \n22.5 \n14% \n1E-4 \n9.8 \n10.1 \n10.5 \n4.3 \n7.4 \n10.8 \n3% \n\nTABLE VII \nCOMPRESSION/DECOMPRESSION SPEEDS (MB/S) WITH =1E-3 \n\nType \nDataset \nSZ \nSZ \nSZ \nZFP MGARDx OurSol \n2.1 \n(Hybrid) (SP+PO) \n\nCompression \nRTM \n207 \n76 \n97 \n549 \n128 \n149 \nMiranda \n125 \n73 \n91 \n201 \n140 \n128 \nQMCPack 146 \n63 \n78 \n158 \n136 \n133 \nSCALE \n145 \n59 \n75 \n101 \n122 \n128 \nNYX \n123 \n81 \n86 \n131 \n117 \n110 \nHurricane \n115 \n63 \n78 \n115 \n122 \n131 \nDecompression \nRTM \n385 \n299 \n298 \n984 \n173 \n276 \nMiranda \n285 \n221 \n206 \n531 \n177 \n232 \nQMCPack 327 \n232 \n282 \n367 \n168 \n241 \nSCALE \n271 \n184 \n192 \n295 \n164 \n215 \nNYX \n222 \n172 \n215 \n244 \n145 \n136 \nHurricane \n222 \n186 \n200 \n257 \n163 \n193 \n\n\n\nTABLE VIII COMPRESSION\nVIIIRATIO COMPARISON OF LOSSLESS COMPRESSORSDataset \nBrotli \nZstd \nSnappy \nFpzip \nZlib \nLZMA \nRTM \n2.04 \n2.02 \n1.87 \n2.62 \n2.04 \n2.18 \nMiranda \n1.21 \n1.21 \n1.11 \n1.86 \n1.21 \n1.30 \nQMCPack \n1.19 \n1.19 \n1.01 \n1.75 \n1.20 \n1.51 \nSCALE \n1.45 \n1.39 \n1.17 \n2.60 \n1.39 \n1.80 \nNYX \n1.19 \n1.11 \n1.00 \n1.37 \n1.11 \n1.25 \nHurricane \n1.52 \n1.49 \n1.26 \n2.28 \n1.49 \n1.78 \n\n\nSZ(SP+PO) represents the SZ compression model with 2nd-order prediction (SP) and parameter optimization (PO), suffering 1X slower compression.\nThis research was supported by the Exascale Computing Project (ECP), Project Number: 17-SC-20-SC, a collaborative effort of two DOE organizations -the Office of Science and the National Nuclear Security Administration, responsible for the planning and preparation of a capable exascale ecosystem, including software, applications, hardware, advanced system engineering and early testbed platforms, to support the nation's exascale computing imperative.\nlarge ensemble project: A community resource for studying climate change in the presence of internal climate variability. J Kay, The community earth system model (CESM). 96J. Kay et al., \"The community earth system model (CESM), large ensemble project: A community resource for studying climate change in the presence of internal climate variability,\" Bulletin of the American Meteorological Society, vol. 96, no. 8, pp. 1333-1349, 2015.\n\nHACC: extreme scaling and performance across diverse architectures. S Habib, Communications of the ACM. 601S. Habib et al., \"HACC: extreme scaling and performance across diverse architectures,\" Communications of the ACM, vol. 60, no. 1, pp. 97-104, 2016.\n\nScientific data management in the coming decade. J Gray, D T Liu, M Nieto-Santisteban, A Szalay, D J Dewitt, G Heber, SIGMOD Rec. 344J. Gray, D. T. Liu, M. Nieto-Santisteban, A. Szalay, D. J. DeWitt, and G. Heber, \"Scientific data management in the coming decade,\" SIGMOD Rec., vol. 34, no. 4, p. 34-41, Dec. 2005.\n\nParallel in-situ data processing with speculative loading. Y Cheng, F Rusu, Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data. the 2014 ACM SIGMOD International Conference on Management of DataY. Cheng and F. Rusu, \"Parallel in-situ data processing with speculative loading,\" in Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data, 2014, p. 1287-1298.\n\nThe researcher's guide to the data deluge: Querying a scientific database in just a few seconds. M L Kersten, S Idreos, S Manegold, E Liarou, Proc. VLDB Endow. 412M. L. Kersten, S. Idreos, S. Manegold, and E. Liarou, \"The researcher's guide to the data deluge: Querying a scientific database in just a few seconds,\" Proc. VLDB Endow., vol. 4, no. 12, p. 1474-1477, Aug. 2011.\n\nNodb: Efficient query execution on raw data files. I Alagiannis, R Borovica, M Branco, S Idreos, A Ailamaki, Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD '12. the 2012 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD '12I. Alagiannis, R. Borovica, M. Branco, S. Idreos, and A. Ailamaki, \"Nodb: Efficient query execution on raw data files,\" in Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, ser. SIGMOD '12, 2012, p. 241-252.\n\nTowards end-to-end SDC detection for HPC applications equipped with lossy compression. S Li, 2020 IEEE International Conference on Cluster Computing. S. Li et al., \"Towards end-to-end SDC detection for HPC applications equipped with lossy compression,\" in 2020 IEEE International Confer- ence on Cluster Computing, 2020, pp. 326-336.\n\n. Globus, Globus. [Online]. Available: https://www.globus.org/\n\nNetCDF: an interface for scientific data access. R Rew, G Davis, IEEE Computer Graphics and Applications. 10R. Rew and G. Davis, \"NetCDF: an interface for scientific data access,\" IEEE Computer Graphics and Applications, vol. 10, pp. 76-82, 1990.\n\nHDF5. HDF5. [Online]. Available: http://www.hdfgroup.org/HDF5\n\nManaging scientific data. A Ailamaki, V Kantere, D Dash, Commun. ACM. 536A. Ailamaki, V. Kantere, and D. Dash, \"Managing scientific data,\" Commun. ACM, vol. 53, no. 6, p. 68-78, Jun. 2010.\n\nScientific formats for object-relational database systems: A study of suitability and performance. S Cohen, SIGMOD Rec. 352S. Cohen et al., \"Scientific formats for object-relational database sys- tems: A study of suitability and performance,\" SIGMOD Rec., vol. 35, no. 2, p. 10-15, Jun. 2006.\n\nH5Z: Filter and Compression Interface. Hdf The, Group, The HDF Group. (2017) H5Z: Filter and Compression Interface. https://support.hdfgroup.org/HDF5/doc1.8/RM/RM H5Z.html. Online.\n\nFast error-bounded lossy HPC data compression with SZ. S Di, F Cappello, IEEE International Parallel and Distributed Processing Symposium. S. Di and F. Cappello, \"Fast error-bounded lossy HPC data compression with SZ,\" in IEEE International Parallel and Distributed Processing Symposium, 2016, pp. 730-739.\n\nFixed-rate compressed floating-point arrays. P Lindstrom, IEEE Transactions on Visualization and Computer Graphics. 2012P. Lindstrom, \"Fixed-rate compressed floating-point arrays,\" IEEE Transactions on Visualization and Computer Graphics, vol. 20, no. 12, pp. 2674-2683, 2014.\n\n. Zlib, Zlib, http://www.zlib.net/, online.\n\nSignificantly improving lossy compression for scientific data sets based on multidimensional prediction and error-controlled quantization. D Tao, S Di, Z Chen, F Cappello, 2017 IEEE International Parallel and Distributed Processing Symposium. D. Tao, S. Di, Z. Chen, and F. Cappello, \"Significantly improving lossy compression for scientific data sets based on multidimensional prediction and error-controlled quantization,\" in 2017 IEEE International Parallel and Distributed Processing Symposium, 2017, pp. 1129-1139.\n\nError-controlled lossy compression optimized for high compression ratios of scientific datasets. X Liang, 2018 IEEE International Conference on Big Data (Big Data). X. Liang et al., \"Error-controlled lossy compression optimized for high compression ratios of scientific datasets,\" 2018 IEEE International Conference on Big Data (Big Data), pp. 438-447, 2018.\n\nSDC resilient error-bounded lossy compressor. S Li, S. Li et al., \"SDC resilient error-bounded lossy compressor,\" https://arxiv.org/abs/2010.03144, 2020, online.\n\nExploration of lossy compression for application-level checkpoint/restart. N Sasaki, K Sato, T Endo, S Matsuoka, Proceedings of the 2015 IEEE International Parallel and Distributed Processing Symposium, ser. IPDPS '15. the 2015 IEEE International Parallel and Distributed Processing Symposium, ser. IPDPS '15N. Sasaki, K. Sato, T. Endo, and S. Matsuoka, \"Exploration of lossy compression for application-level checkpoint/restart,\" in Proceedings of the 2015 IEEE International Parallel and Distributed Processing Symposium, ser. IPDPS '15, 2015, pp. 914-922.\n\nEvaluating image quality measures to assess the impact of lossy data compression applied to climate simulation data. A H Baker, D M Hammerling, T L Turton, Computer Graphics Forum. 383A. H. Baker, D. M. Hammerling, and T. L. Turton, \"Evaluating image quality measures to assess the impact of lossy data compression applied to climate simulation data,\" Computer Graphics Forum, vol. 38, no. 3, pp. 517-528, 2019.\n\nImproving performance of data dumping with lossy compression for scientific simulation. X Liang, 2019 IEEE International Conference on Cluster Computing. X. Liang et al., \"Improving performance of data dumping with lossy compression for scientific simulation,\" in 2019 IEEE International Conference on Cluster Computing, 2019, pp. 1-11.\n\nLossy checkpoint compression in full waveform inversion. N Kukreja, J H Uuckelheim, M Louboutin, J Washbourne, P H Kelly, G J Gorman, N. Kukreja, J. H. uuckelheim, M. Louboutin, J. Washbourne, P. H. Kelly, and G. J. Gorman, \"Lossy checkpoint compression in full waveform inversion,\" https://arxiv.org/pdf/2009.12623.pdf, 2020, online.\n\nRevisiting huffman coding: Toward extreme performance on modern gpu architectures. J Tian, J. Tian et al., \"Revisiting huffman coding: Toward extreme performance on modern gpu architectures,\" https://arxiv.org/abs/2010.10039, 2020, online.\n\nUnderstanding and modeling lossy compression schemes on HPC scientific data. T Lu, 2018 IEEE International Parallel and Distributed Processing Symposium. T. Lu et al., \"Understanding and modeling lossy compression schemes on HPC scientific data,\" in 2018 IEEE International Parallel and Distributed Processing Symposium, 2018, pp. 348-357.\n\nCuSZ: An efficient gpu-based error-bounded lossy compression framework for scientific data. J Tian, Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques, ser. PACT '20. the ACM International Conference on Parallel Architectures and Compilation Techniques, ser. PACT '20J. Tian et al., \"CuSZ: An efficient gpu-based error-bounded lossy compression framework for scientific data,\" in Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques, ser. PACT '20, 2020, p. 3-15.\n\nSignificantly improving lossy compression quality based on an optimized hybrid prediction model. X Liang, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. the International Conference for High Performance Computing, Networking, Storage and AnalysisX. Liang et al., \"Significantly improving lossy compression quality based on an optimized hybrid prediction model,\" in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2019, pp. 1-26.\n\nSignificantly improving lossy compression for HPC datasets with second-order prediction and parameter optimization. K Zhao, Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing, ser. HPDC '20. the 29th International Symposium on High-Performance Parallel and Distributed Computing, ser. HPDC '20K. Zhao et al., \"Significantly improving lossy compression for HPC datasets with second-order prediction and parameter optimization,\" in Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing, ser. HPDC '20, 2020, pp. 89-100.\n\nWavesz: A hardware-algorithm co-design of efficient lossy compression for scientific data. J Tian, Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel ProgrammingJ. Tian et al., \"Wavesz: A hardware-algorithm co-design of efficient lossy compression for scientific data,\" in Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2020, pp. 74-88.\n\nGeoDRIVE -a high performance computing flexible platform for seismic applications. S Kayum, First Break. 382S. Kayum et al., \"GeoDRIVE -a high performance computing flexible platform for seismic applications,\" First Break, vol. 38, no. 2, pp. 97- 100, 2020.\n\nPaSTRI: A novel data compression algorithm for two-electron integrals in quantum chemistry. A M Gok, IEEE International Conference on Cluster Computing (CLUSTER). A. M. Gok et al., \"PaSTRI: A novel data compression algorithm for two-electron integrals in quantum chemistry,\" in IEEE International Conference on Cluster Computing (CLUSTER), 2018, pp. 1-11.\n\nGorilla: A fast, scalable, in-memory time series database. T Pelkonen, Proc. VLDB Endow. VLDB Endow8T. Pelkonen et al., \"Gorilla: A fast, scalable, in-memory time series database,\" Proc. VLDB Endow., vol. 8, no. 12, p. 1816-1827, Aug. 2015.\n\nTwo-level data compression using machine learning in time series database. X Yu, 36th IEEE International Conference on Data Engineering. X. Yu et al., \"Two-level data compression using machine learning in time series database,\" in 36th IEEE International Conference on Data Engineering, 2020, pp. 1333-1344.\n\nA demonstration of SciDB: A science-oriented DBMS. P Cudre-Mauroux, Proc. VLDB Endow. VLDB Endow2P. Cudre-Mauroux et al., \"A demonstration of SciDB: A science-oriented DBMS,\" Proc. VLDB Endow., vol. 2, no. 2, p. 1534-1537, Aug. 2009.\n\nSDRBench: Scientific data reduction benchmark for lossy compressors. K Zhao, onlineK. Zhao et al., \"SDRBench: Scientific data reduction benchmark for lossy compressors,\" https://arxiv.org/abs/2101.03201, 2021, online.\n\nD Taubman, M Marcellin, JPEG2000 Image Compression Fundamentals, Standards and Practice. SpringerD. Taubman and M. Marcellin, JPEG2000 Image Compression Funda- mentals, Standards and Practice. Springer, 2013.\n\nTTHRESH: Tensor compression for multidimensional visual data. R Ballester-Ripoll, P Lindstrom, R Pajarola, IEEE Transactions on Visualization & Computer Graphics. 2609R. Ballester-Ripoll, P. Lindstrom, and R. Pajarola, \"TTHRESH: Tensor compression for multidimensional visual data,\" IEEE Transactions on Visualization & Computer Graphics, vol. 26, no. 09, pp. 2891-2903, sep 2020.\n\nLFZip: Lossy compression of multivariate floating-point time series data via improved prediction. S Chandak, K Tatwawadi, C Wen, L Wang, J Aparicio Ojea, T Weissman, 2020 Data Compression Conference (DCC). S. Chandak, K. Tatwawadi, C. Wen, L. Wang, J. Aparicio Ojea, and T. Weissman, \"LFZip: Lossy compression of multivariate floating-point time series data via improved prediction,\" in 2020 Data Compression Conference (DCC), 2020, pp. 342-351.\n\nOptimizing lossy compression rate-distortion from automatic online selection between SZ and ZFP. D Tao, S Di, X Liang, Z Chen, F Cappello, IEEE Transactions on Parallel and Distributed Systems. 308D. Tao, S. Di, X. Liang, Z. Chen, and F. Cappello, \"Optimizing lossy compression rate-distortion from automatic online selection between SZ and ZFP,\" IEEE Transactions on Parallel and Distributed Systems, vol. 30, no. 8, pp. 1857-1871, 2019.\n\nMGARD+: Optimizing multilevel methods for errorbounded scientific data reduction. X Liang, onlineX. Liang et al., \"MGARD+: Optimizing multilevel methods for error- bounded scientific data reduction,\" https://arxiv.org/abs/2010.05872, 2020, online.\n\nMultilevel techniques for compression and reduction of scientific data-the univariate case. M Ainsworth, O Tugluk, B Whitney, S Klasky, Computing and Visualization in Science. 195M. Ainsworth, O. Tugluk, B. Whitney, and S. Klasky, \"Multilevel tech- niques for compression and reduction of scientific data-the univariate case,\" Computing and Visualization in Science, vol. 19, no. 5, pp. 65-76, Dec 2018.\n\nA universal algorithm for sequential data compression. J Ziv, A Lempel, IEEE Transactions on information theory. 233J. Ziv and A. Lempel, \"A universal algorithm for sequential data compression,\" IEEE Transactions on information theory, vol. 23, no. 3, pp. 337-343, 1977.\n\nQMCPACK: an open source ab initio quantum monte carlo package for the electronic structure of atoms, molecules and solids. J Kim, Journal of Physics: Condensed Matter. 3019195901J. Kim et al., \"QMCPACK: an open source ab initio quantum monte carlo package for the electronic structure of atoms, molecules and solids,\" Journal of Physics: Condensed Matter, vol. 30, no. 19, p. 195901, 2018.\n\n. Hurricane ISABEL simulation data. Hurricane ISABEL simulation data, https://www.earthsystemgrid.org/dataset/isabeldata.html, 2016, online.\n\nNYX simulation. NYX simulation, https://amrex-astro.github.io/Nyx, 2019, online.\n\nBrotli: A general-purpose data compressor. J Alakuijala, A Farruggia, P Ferragina, E Kliuchnikov, R Obryk, Z Szabadka, L Vandevenne, ACM Trans. Inf. Syst. 371J. Alakuijala, A. Farruggia, P. Ferragina, E. Kliuchnikov, R. Obryk, Z. Szabadka, and L. Vandevenne, \"Brotli: A general-purpose data compressor,\" ACM Trans. Inf. Syst., vol. 37, no. 1, Dec. 2018.\n\n. Igor Pavlov, Igor Pavlov, https://www.7-zip.org, online.\n\nFast and efficient compression of floating-point data. P Lindstrom, M Isenburg, IEEE Transactions on Visualization and Computer Graphics. 125P. Lindstrom and M. Isenburg, \"Fast and efficient compression of floating-point data,\" IEEE Transactions on Visualization and Computer Graphics, vol. 12, no. 5, pp. 1245-1250, 2006.\n", "annotations": {"author": "[{\"end\":107,\"start\":98},{\"end\":158,\"start\":108},{\"end\":257,\"start\":159},{\"end\":377,\"start\":258},{\"end\":407,\"start\":378},{\"end\":545,\"start\":408},{\"end\":587,\"start\":546}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":102},{\"end\":116,\"start\":114},{\"end\":173,\"start\":165},{\"end\":284,\"start\":276},{\"end\":390,\"start\":386},{\"end\":423,\"start\":415}]", "author_first_name": "[{\"end\":101,\"start\":98},{\"end\":113,\"start\":108},{\"end\":164,\"start\":159},{\"end\":273,\"start\":258},{\"end\":275,\"start\":274},{\"end\":385,\"start\":378},{\"end\":414,\"start\":408}]", "author_affiliation": "[{\"end\":157,\"start\":118},{\"end\":256,\"start\":201},{\"end\":376,\"start\":321},{\"end\":485,\"start\":446},{\"end\":544,\"start\":487},{\"end\":586,\"start\":547}]", "title": "[{\"end\":95,\"start\":1},{\"end\":682,\"start\":588}]", "venue": null, "abstract": "[{\"end\":2447,\"start\":712}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2624,\"start\":2621},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2782,\"start\":2779},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3051,\"start\":3048},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3056,\"start\":3053},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3370,\"start\":3367},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3640,\"start\":3637},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3654,\"start\":3650},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3955,\"start\":3951},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3961,\"start\":3957},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4156,\"start\":4152},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4218,\"start\":4214},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4228,\"start\":4224},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4239,\"start\":4235},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4339,\"start\":4335},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4345,\"start\":4341},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4547,\"start\":4543},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4553,\"start\":4549},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4944,\"start\":4940},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5123,\"start\":5119},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5384,\"start\":5380},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5390,\"start\":5386},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5396,\"start\":5392},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5402,\"start\":5398},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5550,\"start\":5546},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5556,\"start\":5552},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5562,\"start\":5558},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5568,\"start\":5564},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5574,\"start\":5570},{\"end\":6002,\"start\":6001},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6469,\"start\":6465},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6505,\"start\":6501},{\"end\":6898,\"start\":6897},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8694,\"start\":8690},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8827,\"start\":8823},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8957,\"start\":8953},{\"end\":9202,\"start\":9198},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9213,\"start\":9209},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9537,\"start\":9533},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9834,\"start\":9830},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10282,\"start\":10278},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10322,\"start\":10318},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10369,\"start\":10365},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10375,\"start\":10371},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10462,\"start\":10458},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11276,\"start\":11272},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11449,\"start\":11445},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11633,\"start\":11629},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11796,\"start\":11792},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11877,\"start\":11873},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11956,\"start\":11952},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":13197,\"start\":13194},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15343,\"start\":15339},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15349,\"start\":15345},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15887,\"start\":15883},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16075,\"start\":16071},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16268,\"start\":16264},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16274,\"start\":16270},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16280,\"start\":16276},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16454,\"start\":16450},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16542,\"start\":16538},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16548,\"start\":16544},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16773,\"start\":16769},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17004,\"start\":17000},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18420,\"start\":18416},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18997,\"start\":18993},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19003,\"start\":18999},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19753,\"start\":19749},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19813,\"start\":19809},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20736,\"start\":20732},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21352,\"start\":21348},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21362,\"start\":21358},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21378,\"start\":21374},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21392,\"start\":21388},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30034,\"start\":30031},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30987,\"start\":30984},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":35658,\"start\":35654},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35664,\"start\":35660},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":36487,\"start\":36483},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":36821,\"start\":36817},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36836,\"start\":36832},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":36853,\"start\":36849},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36871,\"start\":36867},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36887,\"start\":36883},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":36978,\"start\":36974},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":36984,\"start\":36980},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36990,\"start\":36986},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36995,\"start\":36991},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":37011,\"start\":37007},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37221,\"start\":37217},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":37296,\"start\":37292},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":39349,\"start\":39345},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39355,\"start\":39351},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":39361,\"start\":39357}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46015,\"start\":45941},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46180,\"start\":46016},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46319,\"start\":46181},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46346,\"start\":46320},{\"attributes\":{\"id\":\"fig_4\"},\"end\":46400,\"start\":46347},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46568,\"start\":46401},{\"attributes\":{\"id\":\"fig_6\"},\"end\":47186,\"start\":46569},{\"attributes\":{\"id\":\"fig_7\"},\"end\":47492,\"start\":47187},{\"attributes\":{\"id\":\"fig_8\"},\"end\":47550,\"start\":47493},{\"attributes\":{\"id\":\"fig_9\"},\"end\":47648,\"start\":47551},{\"attributes\":{\"id\":\"fig_10\"},\"end\":47714,\"start\":47649},{\"attributes\":{\"id\":\"fig_11\"},\"end\":47794,\"start\":47715},{\"attributes\":{\"id\":\"fig_12\"},\"end\":47857,\"start\":47795},{\"attributes\":{\"id\":\"fig_13\"},\"end\":48014,\"start\":47858},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48098,\"start\":48015},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48513,\"start\":48099},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49088,\"start\":48514},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49614,\"start\":49089},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":49989,\"start\":49615},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":51668,\"start\":49990},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":52050,\"start\":51669}]", "paragraph": "[{\"end\":3537,\"start\":2466},{\"end\":4290,\"start\":3539},{\"end\":5304,\"start\":4292},{\"end\":5575,\"start\":5306},{\"end\":7290,\"start\":5577},{\"end\":8551,\"start\":7292},{\"end\":9287,\"start\":8572},{\"end\":9762,\"start\":9289},{\"end\":10376,\"start\":9764},{\"end\":11005,\"start\":10378},{\"end\":12013,\"start\":11007},{\"end\":12240,\"start\":12015},{\"end\":12710,\"start\":12269},{\"end\":13549,\"start\":12712},{\"end\":14168,\"start\":13551},{\"end\":14374,\"start\":14170},{\"end\":14527,\"start\":14376},{\"end\":15655,\"start\":14635},{\"end\":16017,\"start\":15708},{\"end\":16281,\"start\":16065},{\"end\":16571,\"start\":16283},{\"end\":18477,\"start\":16573},{\"end\":19088,\"start\":18530},{\"end\":20077,\"start\":19090},{\"end\":20231,\"start\":20131},{\"end\":20737,\"start\":20233},{\"end\":22324,\"start\":20800},{\"end\":23295,\"start\":22326},{\"end\":23595,\"start\":23297},{\"end\":23892,\"start\":23597},{\"end\":25013,\"start\":23984},{\"end\":25216,\"start\":25015},{\"end\":26920,\"start\":25260},{\"end\":27209,\"start\":26977},{\"end\":28121,\"start\":27211},{\"end\":28495,\"start\":28123},{\"end\":28888,\"start\":28595},{\"end\":29013,\"start\":28971},{\"end\":29368,\"start\":29203},{\"end\":29680,\"start\":29498},{\"end\":29938,\"start\":29835},{\"end\":30058,\"start\":29994},{\"end\":30556,\"start\":30229},{\"end\":31066,\"start\":30716},{\"end\":31928,\"start\":31068},{\"end\":32942,\"start\":31984},{\"end\":33045,\"start\":33027},{\"end\":33086,\"start\":33068},{\"end\":33218,\"start\":33149},{\"end\":34688,\"start\":33291},{\"end\":35160,\"start\":34727},{\"end\":35539,\"start\":35162},{\"end\":35908,\"start\":35541},{\"end\":36042,\"start\":35940},{\"end\":36244,\"start\":36044},{\"end\":36356,\"start\":36246},{\"end\":37345,\"start\":36358},{\"end\":37461,\"start\":37372},{\"end\":37518,\"start\":37463},{\"end\":37821,\"start\":37520},{\"end\":39362,\"start\":37860},{\"end\":39696,\"start\":39364},{\"end\":42594,\"start\":39698},{\"end\":44986,\"start\":42596},{\"end\":45916,\"start\":44988}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14634,\"start\":14528},{\"attributes\":{\"id\":\"formula_1\"},\"end\":28594,\"start\":28496},{\"attributes\":{\"id\":\"formula_2\"},\"end\":28970,\"start\":28889},{\"attributes\":{\"id\":\"formula_3\"},\"end\":29202,\"start\":29014},{\"attributes\":{\"id\":\"formula_4\"},\"end\":29497,\"start\":29369},{\"attributes\":{\"id\":\"formula_5\"},\"end\":29726,\"start\":29681},{\"attributes\":{\"id\":\"formula_6\"},\"end\":29834,\"start\":29726},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29993,\"start\":29939},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30228,\"start\":30059},{\"attributes\":{\"id\":\"formula_9\"},\"end\":30639,\"start\":30557},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30715,\"start\":30639},{\"attributes\":{\"id\":\"formula_11\"},\"end\":33026,\"start\":32943}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28493,\"start\":28486},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30304,\"start\":30297},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31065,\"start\":31058},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31257,\"start\":31249},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34275,\"start\":34254},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34531,\"start\":34522},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35783,\"start\":35774},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36497,\"start\":36490},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":38389,\"start\":38381},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40995,\"start\":40985},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41287,\"start\":41279}]", "section_header": "[{\"end\":2464,\"start\":2449},{\"end\":8570,\"start\":8554},{\"end\":12267,\"start\":12243},{\"end\":15706,\"start\":15658},{\"end\":16063,\"start\":16020},{\"end\":18528,\"start\":18480},{\"end\":20129,\"start\":20080},{\"end\":20798,\"start\":20740},{\"end\":23982,\"start\":23895},{\"end\":25258,\"start\":25219},{\"end\":26975,\"start\":26923},{\"end\":31982,\"start\":31931},{\"end\":33066,\"start\":33048},{\"end\":33126,\"start\":33089},{\"end\":33147,\"start\":33129},{\"end\":33289,\"start\":33221},{\"end\":34725,\"start\":34691},{\"end\":35938,\"start\":35911},{\"end\":37370,\"start\":37348},{\"end\":37858,\"start\":37824},{\"end\":45940,\"start\":45919},{\"end\":45950,\"start\":45942},{\"end\":46025,\"start\":46017},{\"end\":46190,\"start\":46182},{\"end\":46329,\"start\":46321},{\"end\":46356,\"start\":46348},{\"end\":47502,\"start\":47494},{\"end\":47561,\"start\":47552},{\"end\":47659,\"start\":47650},{\"end\":47725,\"start\":47716},{\"end\":47805,\"start\":47796},{\"end\":47877,\"start\":47859},{\"end\":48030,\"start\":48016},{\"end\":48119,\"start\":48100},{\"end\":48540,\"start\":48515},{\"end\":49109,\"start\":49090},{\"end\":49629,\"start\":49616},{\"end\":50011,\"start\":49991},{\"end\":51692,\"start\":51670}]", "table": "[{\"end\":48098,\"start\":48032},{\"end\":48513,\"start\":48156},{\"end\":49088,\"start\":48544},{\"end\":49614,\"start\":49112},{\"end\":49989,\"start\":49669},{\"end\":51668,\"start\":50060},{\"end\":52050,\"start\":51737}]", "figure_caption": "[{\"end\":46015,\"start\":45952},{\"end\":46180,\"start\":46027},{\"end\":46319,\"start\":46192},{\"end\":46346,\"start\":46331},{\"end\":46400,\"start\":46358},{\"end\":46568,\"start\":46403},{\"end\":47186,\"start\":46571},{\"end\":47492,\"start\":47189},{\"end\":47550,\"start\":47504},{\"end\":47648,\"start\":47564},{\"end\":47714,\"start\":47662},{\"end\":47794,\"start\":47728},{\"end\":47857,\"start\":47808},{\"end\":48014,\"start\":47882},{\"end\":48156,\"start\":48122},{\"end\":49669,\"start\":49631},{\"end\":50060,\"start\":50014},{\"end\":51737,\"start\":51697}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20687,\"start\":20681},{\"end\":21169,\"start\":21163},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21272,\"start\":21266},{\"end\":21829,\"start\":21823},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22395,\"start\":22389},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23306,\"start\":23300},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24024,\"start\":24018},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24321,\"start\":24315},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27954,\"start\":27946},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29294,\"start\":29288},{\"end\":32237,\"start\":32231},{\"end\":32690,\"start\":32684},{\"end\":33386,\"start\":33380},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34887,\"start\":34881},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":35126,\"start\":35120},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":38093,\"start\":38087},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40655,\"start\":40648},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41431,\"start\":41424},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41570,\"start\":41559},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41925,\"start\":41914},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42189,\"start\":42182},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42705,\"start\":42698},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":42806,\"start\":42800},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43184,\"start\":43177},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43196,\"start\":43189},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43986,\"start\":43975},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44063,\"start\":44056}]", "bib_author_first_name": "[{\"end\":52770,\"start\":52769},{\"end\":53155,\"start\":53154},{\"end\":53392,\"start\":53391},{\"end\":53400,\"start\":53399},{\"end\":53402,\"start\":53401},{\"end\":53409,\"start\":53408},{\"end\":53430,\"start\":53429},{\"end\":53440,\"start\":53439},{\"end\":53442,\"start\":53441},{\"end\":53452,\"start\":53451},{\"end\":53718,\"start\":53717},{\"end\":53727,\"start\":53726},{\"end\":54171,\"start\":54170},{\"end\":54173,\"start\":54172},{\"end\":54184,\"start\":54183},{\"end\":54194,\"start\":54193},{\"end\":54206,\"start\":54205},{\"end\":54502,\"start\":54501},{\"end\":54516,\"start\":54515},{\"end\":54528,\"start\":54527},{\"end\":54538,\"start\":54537},{\"end\":54548,\"start\":54547},{\"end\":55072,\"start\":55071},{\"end\":55433,\"start\":55432},{\"end\":55440,\"start\":55439},{\"end\":55721,\"start\":55720},{\"end\":55733,\"start\":55732},{\"end\":55744,\"start\":55743},{\"end\":55984,\"start\":55983},{\"end\":56220,\"start\":56217},{\"end\":56416,\"start\":56415},{\"end\":56422,\"start\":56421},{\"end\":56714,\"start\":56713},{\"end\":57131,\"start\":57130},{\"end\":57138,\"start\":57137},{\"end\":57144,\"start\":57143},{\"end\":57152,\"start\":57151},{\"end\":57610,\"start\":57609},{\"end\":57919,\"start\":57918},{\"end\":58111,\"start\":58110},{\"end\":58121,\"start\":58120},{\"end\":58129,\"start\":58128},{\"end\":58137,\"start\":58136},{\"end\":58713,\"start\":58712},{\"end\":58715,\"start\":58714},{\"end\":58724,\"start\":58723},{\"end\":58726,\"start\":58725},{\"end\":58740,\"start\":58739},{\"end\":58742,\"start\":58741},{\"end\":59097,\"start\":59096},{\"end\":59404,\"start\":59403},{\"end\":59415,\"start\":59414},{\"end\":59417,\"start\":59416},{\"end\":59431,\"start\":59430},{\"end\":59444,\"start\":59443},{\"end\":59458,\"start\":59457},{\"end\":59460,\"start\":59459},{\"end\":59469,\"start\":59468},{\"end\":59471,\"start\":59470},{\"end\":59766,\"start\":59765},{\"end\":60001,\"start\":60000},{\"end\":60357,\"start\":60356},{\"end\":60925,\"start\":60924},{\"end\":61499,\"start\":61498},{\"end\":62094,\"start\":62093},{\"end\":62592,\"start\":62591},{\"end\":62860,\"start\":62859},{\"end\":62862,\"start\":62861},{\"end\":63184,\"start\":63183},{\"end\":63442,\"start\":63441},{\"end\":63727,\"start\":63726},{\"end\":63980,\"start\":63979},{\"end\":64130,\"start\":64129},{\"end\":64141,\"start\":64140},{\"end\":64402,\"start\":64401},{\"end\":64422,\"start\":64421},{\"end\":64435,\"start\":64434},{\"end\":64820,\"start\":64819},{\"end\":64831,\"start\":64830},{\"end\":64844,\"start\":64843},{\"end\":64851,\"start\":64850},{\"end\":64859,\"start\":64858},{\"end\":64868,\"start\":64860},{\"end\":64876,\"start\":64875},{\"end\":65266,\"start\":65265},{\"end\":65273,\"start\":65272},{\"end\":65279,\"start\":65278},{\"end\":65288,\"start\":65287},{\"end\":65296,\"start\":65295},{\"end\":65691,\"start\":65690},{\"end\":65950,\"start\":65949},{\"end\":65963,\"start\":65962},{\"end\":65973,\"start\":65972},{\"end\":65984,\"start\":65983},{\"end\":66318,\"start\":66317},{\"end\":66325,\"start\":66324},{\"end\":66658,\"start\":66657},{\"end\":67193,\"start\":67192},{\"end\":67207,\"start\":67206},{\"end\":67220,\"start\":67219},{\"end\":67233,\"start\":67232},{\"end\":67248,\"start\":67247},{\"end\":67257,\"start\":67256},{\"end\":67269,\"start\":67268},{\"end\":67510,\"start\":67506},{\"end\":67620,\"start\":67619},{\"end\":67633,\"start\":67632}]", "bib_author_last_name": "[{\"end\":52774,\"start\":52771},{\"end\":53161,\"start\":53156},{\"end\":53397,\"start\":53393},{\"end\":53406,\"start\":53403},{\"end\":53427,\"start\":53410},{\"end\":53437,\"start\":53431},{\"end\":53449,\"start\":53443},{\"end\":53458,\"start\":53453},{\"end\":53724,\"start\":53719},{\"end\":53732,\"start\":53728},{\"end\":54181,\"start\":54174},{\"end\":54191,\"start\":54185},{\"end\":54203,\"start\":54195},{\"end\":54213,\"start\":54207},{\"end\":54513,\"start\":54503},{\"end\":54525,\"start\":54517},{\"end\":54535,\"start\":54529},{\"end\":54545,\"start\":54539},{\"end\":54557,\"start\":54549},{\"end\":55075,\"start\":55073},{\"end\":55327,\"start\":55321},{\"end\":55437,\"start\":55434},{\"end\":55446,\"start\":55441},{\"end\":55730,\"start\":55722},{\"end\":55741,\"start\":55734},{\"end\":55749,\"start\":55745},{\"end\":55990,\"start\":55985},{\"end\":56224,\"start\":56221},{\"end\":56231,\"start\":56226},{\"end\":56419,\"start\":56417},{\"end\":56431,\"start\":56423},{\"end\":56724,\"start\":56715},{\"end\":56952,\"start\":56948},{\"end\":57135,\"start\":57132},{\"end\":57141,\"start\":57139},{\"end\":57149,\"start\":57145},{\"end\":57161,\"start\":57153},{\"end\":57616,\"start\":57611},{\"end\":57922,\"start\":57920},{\"end\":58118,\"start\":58112},{\"end\":58126,\"start\":58122},{\"end\":58134,\"start\":58130},{\"end\":58146,\"start\":58138},{\"end\":58721,\"start\":58716},{\"end\":58737,\"start\":58727},{\"end\":58749,\"start\":58743},{\"end\":59103,\"start\":59098},{\"end\":59412,\"start\":59405},{\"end\":59428,\"start\":59418},{\"end\":59441,\"start\":59432},{\"end\":59455,\"start\":59445},{\"end\":59466,\"start\":59461},{\"end\":59478,\"start\":59472},{\"end\":59771,\"start\":59767},{\"end\":60004,\"start\":60002},{\"end\":60362,\"start\":60358},{\"end\":60931,\"start\":60926},{\"end\":61504,\"start\":61500},{\"end\":62099,\"start\":62095},{\"end\":62598,\"start\":62593},{\"end\":62866,\"start\":62863},{\"end\":63193,\"start\":63185},{\"end\":63445,\"start\":63443},{\"end\":63741,\"start\":63728},{\"end\":63985,\"start\":63981},{\"end\":64138,\"start\":64131},{\"end\":64151,\"start\":64142},{\"end\":64419,\"start\":64403},{\"end\":64432,\"start\":64423},{\"end\":64444,\"start\":64436},{\"end\":64828,\"start\":64821},{\"end\":64841,\"start\":64832},{\"end\":64848,\"start\":64845},{\"end\":64856,\"start\":64852},{\"end\":64873,\"start\":64869},{\"end\":64885,\"start\":64877},{\"end\":65270,\"start\":65267},{\"end\":65276,\"start\":65274},{\"end\":65285,\"start\":65280},{\"end\":65293,\"start\":65289},{\"end\":65305,\"start\":65297},{\"end\":65697,\"start\":65692},{\"end\":65960,\"start\":65951},{\"end\":65970,\"start\":65964},{\"end\":65981,\"start\":65974},{\"end\":65991,\"start\":65985},{\"end\":66322,\"start\":66319},{\"end\":66332,\"start\":66326},{\"end\":66662,\"start\":66659},{\"end\":67204,\"start\":67194},{\"end\":67217,\"start\":67208},{\"end\":67230,\"start\":67221},{\"end\":67245,\"start\":67234},{\"end\":67254,\"start\":67249},{\"end\":67266,\"start\":67258},{\"end\":67280,\"start\":67270},{\"end\":67517,\"start\":67511},{\"end\":67630,\"start\":67621},{\"end\":67642,\"start\":67634}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":123568133},\"end\":53084,\"start\":52647},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":691184},\"end\":53340,\"start\":53086},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2534621},\"end\":53656,\"start\":53342},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":7078462},\"end\":54071,\"start\":53658},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":17693414},\"end\":54448,\"start\":54073},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":625881},\"end\":54982,\"start\":54450},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":226266889},\"end\":55317,\"start\":54984},{\"attributes\":{\"id\":\"b7\"},\"end\":55381,\"start\":55319},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":11171299},\"end\":55629,\"start\":55383},{\"attributes\":{\"id\":\"b9\"},\"end\":55692,\"start\":55631},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14063266},\"end\":55882,\"start\":55694},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1359830},\"end\":56176,\"start\":55884},{\"attributes\":{\"id\":\"b12\"},\"end\":56358,\"start\":56178},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":8296694},\"end\":56666,\"start\":56360},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15277033},\"end\":56944,\"start\":56668},{\"attributes\":{\"id\":\"b15\"},\"end\":56989,\"start\":56946},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2178023},\"end\":57510,\"start\":56991},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53358213},\"end\":57870,\"start\":57512},{\"attributes\":{\"id\":\"b18\"},\"end\":58033,\"start\":57872},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":17594218},\"end\":58593,\"start\":58035},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":199018642},\"end\":59006,\"start\":58595},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":207910210},\"end\":59344,\"start\":59008},{\"attributes\":{\"id\":\"b22\"},\"end\":59680,\"start\":59346},{\"attributes\":{\"id\":\"b23\"},\"end\":59921,\"start\":59682},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":51924604},\"end\":60262,\"start\":59923},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":220632308},\"end\":60825,\"start\":60264},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":207925294},\"end\":61380,\"start\":60827},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219719709},\"end\":62000,\"start\":61382},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":209899308},\"end\":62506,\"start\":62002},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":212858597},\"end\":62765,\"start\":62508},{\"attributes\":{\"id\":\"b30\"},\"end\":63122,\"start\":62767},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14777791},\"end\":63364,\"start\":63124},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":218906562},\"end\":63673,\"start\":63366},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2502859},\"end\":63908,\"start\":63675},{\"attributes\":{\"id\":\"b34\"},\"end\":64127,\"start\":63910},{\"attributes\":{\"id\":\"b35\"},\"end\":64337,\"start\":64129},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":49268164},\"end\":64719,\"start\":64339},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":207870265},\"end\":65166,\"start\":64721},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":49405466},\"end\":65606,\"start\":65168},{\"attributes\":{\"id\":\"b39\"},\"end\":65855,\"start\":65608},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":57661300},\"end\":66260,\"start\":65857},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":9267632},\"end\":66532,\"start\":66262},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4913347},\"end\":66923,\"start\":66534},{\"attributes\":{\"id\":\"b43\"},\"end\":67065,\"start\":66925},{\"attributes\":{\"id\":\"b44\"},\"end\":67147,\"start\":67067},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":58268198},\"end\":67502,\"start\":67149},{\"attributes\":{\"id\":\"b46\"},\"end\":67562,\"start\":67504},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":12262331},\"end\":67886,\"start\":67564}]", "bib_title": "[{\"end\":52767,\"start\":52647},{\"end\":53152,\"start\":53086},{\"end\":53389,\"start\":53342},{\"end\":53715,\"start\":53658},{\"end\":54168,\"start\":54073},{\"end\":54499,\"start\":54450},{\"end\":55069,\"start\":54984},{\"end\":55430,\"start\":55383},{\"end\":55718,\"start\":55694},{\"end\":55981,\"start\":55884},{\"end\":56413,\"start\":56360},{\"end\":56711,\"start\":56668},{\"end\":57128,\"start\":56991},{\"end\":57607,\"start\":57512},{\"end\":58108,\"start\":58035},{\"end\":58710,\"start\":58595},{\"end\":59094,\"start\":59008},{\"end\":59998,\"start\":59923},{\"end\":60354,\"start\":60264},{\"end\":60922,\"start\":60827},{\"end\":61496,\"start\":61382},{\"end\":62091,\"start\":62002},{\"end\":62589,\"start\":62508},{\"end\":62857,\"start\":62767},{\"end\":63181,\"start\":63124},{\"end\":63439,\"start\":63366},{\"end\":63724,\"start\":63675},{\"end\":64399,\"start\":64339},{\"end\":64817,\"start\":64721},{\"end\":65263,\"start\":65168},{\"end\":65947,\"start\":65857},{\"end\":66315,\"start\":66262},{\"end\":66655,\"start\":66534},{\"end\":67190,\"start\":67149},{\"end\":67617,\"start\":67564}]", "bib_author": "[{\"end\":52776,\"start\":52769},{\"end\":53163,\"start\":53154},{\"end\":53399,\"start\":53391},{\"end\":53408,\"start\":53399},{\"end\":53429,\"start\":53408},{\"end\":53439,\"start\":53429},{\"end\":53451,\"start\":53439},{\"end\":53460,\"start\":53451},{\"end\":53726,\"start\":53717},{\"end\":53734,\"start\":53726},{\"end\":54183,\"start\":54170},{\"end\":54193,\"start\":54183},{\"end\":54205,\"start\":54193},{\"end\":54215,\"start\":54205},{\"end\":54515,\"start\":54501},{\"end\":54527,\"start\":54515},{\"end\":54537,\"start\":54527},{\"end\":54547,\"start\":54537},{\"end\":54559,\"start\":54547},{\"end\":55077,\"start\":55071},{\"end\":55329,\"start\":55321},{\"end\":55439,\"start\":55432},{\"end\":55448,\"start\":55439},{\"end\":55732,\"start\":55720},{\"end\":55743,\"start\":55732},{\"end\":55751,\"start\":55743},{\"end\":55992,\"start\":55983},{\"end\":56226,\"start\":56217},{\"end\":56233,\"start\":56226},{\"end\":56421,\"start\":56415},{\"end\":56433,\"start\":56421},{\"end\":56726,\"start\":56713},{\"end\":56954,\"start\":56948},{\"end\":57137,\"start\":57130},{\"end\":57143,\"start\":57137},{\"end\":57151,\"start\":57143},{\"end\":57163,\"start\":57151},{\"end\":57618,\"start\":57609},{\"end\":57924,\"start\":57918},{\"end\":58120,\"start\":58110},{\"end\":58128,\"start\":58120},{\"end\":58136,\"start\":58128},{\"end\":58148,\"start\":58136},{\"end\":58723,\"start\":58712},{\"end\":58739,\"start\":58723},{\"end\":58751,\"start\":58739},{\"end\":59105,\"start\":59096},{\"end\":59414,\"start\":59403},{\"end\":59430,\"start\":59414},{\"end\":59443,\"start\":59430},{\"end\":59457,\"start\":59443},{\"end\":59468,\"start\":59457},{\"end\":59480,\"start\":59468},{\"end\":59773,\"start\":59765},{\"end\":60006,\"start\":60000},{\"end\":60364,\"start\":60356},{\"end\":60933,\"start\":60924},{\"end\":61506,\"start\":61498},{\"end\":62101,\"start\":62093},{\"end\":62600,\"start\":62591},{\"end\":62868,\"start\":62859},{\"end\":63195,\"start\":63183},{\"end\":63447,\"start\":63441},{\"end\":63743,\"start\":63726},{\"end\":63987,\"start\":63979},{\"end\":64140,\"start\":64129},{\"end\":64153,\"start\":64140},{\"end\":64421,\"start\":64401},{\"end\":64434,\"start\":64421},{\"end\":64446,\"start\":64434},{\"end\":64830,\"start\":64819},{\"end\":64843,\"start\":64830},{\"end\":64850,\"start\":64843},{\"end\":64858,\"start\":64850},{\"end\":64875,\"start\":64858},{\"end\":64887,\"start\":64875},{\"end\":65272,\"start\":65265},{\"end\":65278,\"start\":65272},{\"end\":65287,\"start\":65278},{\"end\":65295,\"start\":65287},{\"end\":65307,\"start\":65295},{\"end\":65699,\"start\":65690},{\"end\":65962,\"start\":65949},{\"end\":65972,\"start\":65962},{\"end\":65983,\"start\":65972},{\"end\":65993,\"start\":65983},{\"end\":66324,\"start\":66317},{\"end\":66334,\"start\":66324},{\"end\":66664,\"start\":66657},{\"end\":67206,\"start\":67192},{\"end\":67219,\"start\":67206},{\"end\":67232,\"start\":67219},{\"end\":67247,\"start\":67232},{\"end\":67256,\"start\":67247},{\"end\":67268,\"start\":67256},{\"end\":67282,\"start\":67268},{\"end\":67519,\"start\":67506},{\"end\":67632,\"start\":67619},{\"end\":67644,\"start\":67632}]", "bib_venue": "[{\"end\":52815,\"start\":52776},{\"end\":53188,\"start\":53163},{\"end\":53470,\"start\":53460},{\"end\":53815,\"start\":53734},{\"end\":54231,\"start\":54215},{\"end\":54657,\"start\":54559},{\"end\":55132,\"start\":55077},{\"end\":55487,\"start\":55448},{\"end\":55635,\"start\":55631},{\"end\":55762,\"start\":55751},{\"end\":56002,\"start\":55992},{\"end\":56215,\"start\":56178},{\"end\":56497,\"start\":56433},{\"end\":56782,\"start\":56726},{\"end\":57232,\"start\":57163},{\"end\":57675,\"start\":57618},{\"end\":57916,\"start\":57872},{\"end\":58252,\"start\":58148},{\"end\":58774,\"start\":58751},{\"end\":59160,\"start\":59105},{\"end\":59401,\"start\":59346},{\"end\":59763,\"start\":59682},{\"end\":60075,\"start\":60006},{\"end\":60479,\"start\":60364},{\"end\":61041,\"start\":60933},{\"end\":61623,\"start\":61506},{\"end\":62197,\"start\":62101},{\"end\":62611,\"start\":62600},{\"end\":62928,\"start\":62868},{\"end\":63211,\"start\":63195},{\"end\":63501,\"start\":63447},{\"end\":63759,\"start\":63743},{\"end\":63977,\"start\":63910},{\"end\":64216,\"start\":64153},{\"end\":64500,\"start\":64446},{\"end\":64925,\"start\":64887},{\"end\":65360,\"start\":65307},{\"end\":65688,\"start\":65608},{\"end\":66031,\"start\":65993},{\"end\":66373,\"start\":66334},{\"end\":66700,\"start\":66664},{\"end\":66959,\"start\":66927},{\"end\":67081,\"start\":67067},{\"end\":67302,\"start\":67282},{\"end\":67700,\"start\":67644},{\"end\":53883,\"start\":53817},{\"end\":54742,\"start\":54659},{\"end\":58343,\"start\":58254},{\"end\":60581,\"start\":60481},{\"end\":61136,\"start\":61043},{\"end\":61727,\"start\":61625},{\"end\":62280,\"start\":62199},{\"end\":63223,\"start\":63213},{\"end\":63771,\"start\":63761}]"}}}, "year": 2023, "month": 12, "day": 17}