{"id": 4309276, "updated": "2023-09-27 19:19:42.724", "metadata": {"title": "Describing Textures in the Wild", "authors": "[{\"first\":\"Mircea\",\"last\":\"Cimpoi\",\"middle\":[]},{\"first\":\"Subhransu\",\"last\":\"Maji\",\"middle\":[]},{\"first\":\"Iasonas\",\"last\":\"Kokkinos\",\"middle\":[]},{\"first\":\"Sammy\",\"last\":\"Mohamed\",\"middle\":[]},{\"first\":\"Andrea\",\"last\":\"Vedaldi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2013, "month": 11, "day": 14}, "abstract": "Patterns and textures are defining characteristics of many natural objects: a shirt can be striped, the wings of a butterfly can be veined, and the skin of an animal can be scaly. Aiming at supporting this analytical dimension in image understanding, we address the challenging problem of describing textures with semantic attributes. We identify a rich vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected in the wild.The resulting Describable Textures Dataset (DTD) is the basis to seek for the best texture representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and show that, surprisingly, it outperforms specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that the describable attributes are excellent texture descriptors, transferring between datasets and tasks; in particular, combined with IFV, they significantly outperform the state-of-the-art by more than 8 percent on both FMD and KTHTIPS-2b benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1311.3618", "mag": "2949739859", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/CimpoiMKMV14", "doi": "10.1109/cvpr.2014.461"}}, "content": {"source": {"pdf_hash": "10865f71e8e9671f3a6c8ee3ca7b0610aec5aba1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1311.3618v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1311.3618", "status": "GREEN"}}, "grobid": {"id": "9559fc096071f244b51d604b208e36302b947b75", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/10865f71e8e9671f3a6c8ee3ca7b0610aec5aba1.txt", "contents": "\nDescribing Textures in the Wild\n\n\nMircea Cimpoi \nDepartment of Engineering Science\nUniversity of Oxford\n\n\nSubhransu Maji \nToyota Technological Institute\nChicago\n\nIasonas Kokkinos \nCenter for Visual Computing\nEcole Centrale Paris\n\n\nSammy Mohamed \nStony Brook University\n\n\nAndrea Vedaldi \nDepartment of Engineering Science\nUniversity of Oxford\n\n\nDescribing Textures in the Wild\n\nPatterns and textures are defining characteristics of many natural objects: a shirt can be striped, the wings of a butterfly can be veined, and the skin of an animal can be scaly. Aiming at supporting this analytical dimension in image understanding, we address the challenging problem of describing textures with semantic attributes. We identify a rich vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected \"in the wild\". The resulting Describable Textures Dataset (DTD) is the basis to seek for the best texture representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and show that, surprisingly, it outperforms specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also show that the describable attributes are excellent texture descriptors, transferring between datasets and tasks; in particular, combined with IFV, they significantly outperform the state-of-the-art by more than 8% on both FMD and KTH-TIPS-2b benchmarks. We also demonstrate that they produce intuitive descriptions of materials and Internet images.\n\nIntroduction\n\nRecently visual attributes have raised significant interest in the community [6,11,17,25]. A \"visual attribute\" is a property of an object that can be measured visually and has a semantic connotation, such as the shape of a hat or the color of a ball. Attributes allow characterizing objects in far greater detail than a category label and are therefore the key to several advanced applications, including understanding complex queries in semantic search, learning about objects from textual description, and accounting for the content of Tuesday, October 29, 13 Figure 1: Both the man-made and the natural world are an abundant source of richly textured objects. The textures of objects shown above can be described (in no particular order) as dotted, striped, chequered, cracked, swirly, honeycombed, and scaly. We aim at identifying these attributes automatically and generating descriptions based on them.\n\nimages in great detail. Textural properties have an important role in object descriptions, particularly for those objects that are best qualified by a pattern, such as a shirt or the wing of bird or a butterfly as illustrated in Fig. 1. Nevertheless, so far the attributes of textures have been investigated only tangentially. In this paper we address the question of whether there exists a \"universal\" set of attributes that can describe a wide range of texture patterns, whether these can be reliably estimated from images, and for what tasks they are useful.\n\nThe study of perceptual attributes of textures has a long history starting from pre-attentive aspects and grouping [16], to coarse high-level attributes [1,2,33], to some recent work aimed at discovering such attributes by automatically mining descriptions of images from the Internet [3,12]. However, the texture attributes investigated so far are rather few or too generic for a detailed description most \"real world\" patterns. Our work is motivated by the one of Bhusan et al. [5] who studied the relationship between commonly used English words and the perceptual properties of textures, identifying a set of words sufficient to describing a wide variety of texture patterns. While they study the psychological aspects of texture perception, the  focus of this paper is the challenge of estimating such properties from images automatically.\n\nOur first contribution is to select a subset of 47 describable texture attributes, based on the work of Bhusan et al., that capture a wide variety of visual properties of textures and to introduce a corresponding describable texture dataset consisting of 5,640 texture images jointly annotated with the 47 attributes (Sect. 2). In an effort to support directly real world applications, and inspired by datasets such as ImageNet [10] and the Flickr Material Dataset (FMD) [30], our images are captured \"in the wild\" by downloading them from the Internet rather than collecting them in a laboratory. We also address the practical issue of crowd-sourcing this large set of joint annotations efficiently accounting for the co-occurrence statistics of attributes, the appearance of the textures, and the reliability of annotators (Sect. 2.1).\n\nOur second contribution is to identify a gold standard texture representation that achieves optimal recognition of the describable texture attributes in challenging real-world conditions. Texture classification has been widely studied in the context of recognizing materials supported by datasets such as CUReT [9], UIUC [18], UMD [39], Outex [23], Drexel Texture Database [24], and KTH-TIPS [7,14]. These datasets address material recognition under variable occlusion, viewpoint, and illumination and have motivated the creation of a large number of specialized texture representations that are invariant or robust to these factors [19,23,35,36]. In contrast, generic object recognition features such as SIFT was shown to work the best for material recognition in FMD, which, like DTD, was collected \"in the wild\". Our findings are similar, but we also find that Fisher vectors [26] computed on SIFT features and certain color features can significantly boost performance. Surprisingly, these descriptors outperform specialized state-of-theart texture representations not only in recognizing our de-scribable attributes, but also in a variety of datasets for material recognition, achieving an accuracy of 63.3% on FMD and 67.5% on KTH-TIPS2-b dataset (Sect. 3, 4.1).\n\nOur third contribution consists in several applications of the proposed describable attributes. These can serve a complimentary role for recognition and description in domains where the material is not-important or is known ahead of time, such as fabrics or wallpapers. However, can these attributes improve other texture analysis tasks such as material recognition? We answer this question in the affirmative in a series of experiments on the challenging FMD and KTH datasets. We show that estimates of these properties when used a features can boost recognition rates even more for material classification achieving an accuracy of 53.1% on FMD and 64.6% on KTH when used alone as a 47 dimensional feature, and 65.4% on FMD and 74.6% on KTH when combined with SIFT and simple color descriptors (Sect. 4.2). These represent more than an absolute gain of 8% in accuracy over previous state of the art. Our 47 dimensional feature contributed with 2.2 to 7% to the gain. Furthermore, these attribute are easy to describe by design, hence they can serve as intuitive dimensions to explore large collections of texture patterns -for e.g., product catalogs (wallpapers or bedding sets) or material datasets. We present several such visualizations in the paper (Sect. 4.3).\n\n\nThe describable texture dataset\n\nThis section introduces the Describable Textures Dataset (DTD), a collection of real-world texture images annotated with one or more adjectives selected in a vocabulary of 47 English words. These adjectives, or describable texture attributes, are illustrated in Fig. 2 and include words such as banded, cobwebbed, freckled, knitted, and zigzagged.\n\nDTD investigates the problem of texture description, intended as the recognition of describable texture attributes. This problem differs from the one of material recognition considered in existing datasets such as CUReT, KTH, and FMD. While describable attributes are correlated with materials, attributes do not imply materials (e.g. veined may equally apply to leaves or marble) and materials do not imply attributes (not all marbles are veined). Describable attributes can be combined to create rich descriptions ( Fig. 3; marble can be veined, stratified and cracked at the same time), whereas a typical assumption is that textures are made of a single material. Describable attributes are subjective properties that depend on the imaged object as well as on human judgments, whereas materials are objective. In short, attributes capture properties of textures beyond materials, supporting human-centric tasks where describing textures is important. At the same time, they will be shown to be helpful in material recognition as well (Sect. 3.2 and 4.2).\n\nDTD contains textures in the wild, i.e. texture images extracted from the web rather than begin captured or generated in a controlled setting. Textures fill the images, so we can study the problem of texture description independently of texture segmentation. With 5,640 such images, this dataset aims at supporting real-world applications were the recognition of texture properties is a key component. Collecting images from the Internet is a common approach in categorization and object recognition, and was adopted in material recognition in FMD. This choice trades-off the systematic sampling of illumination and viewpoint variations existing in datasets such as CUReT, KTH-TIPS, Outex, and Drexel datasets for a representation of real-world variations, shortening the gap with applications. Furthermore, the invariance of describable attributes is not an intrinsic property as for materials, but it reflects invariance in the human judgments, which should be captured empirically.\n\nDTD is designed as a public benchmark, following the standard practice of providing 10 preset splits into equallysized training, validation and test subsets for easier algorithm comparison (these splits are used in all the experiments in the paper). DTD will be made publicly available on the web at [annonymized], along with standardized evaluation, as well as code reproducing the results in Sect. 4.\n\nRelated work. Apart from material datasets, there have been numerous attempts at collecting attributes of textures at a smaller scale, or in controlled settings. Our work is related to the work of [22], where they analyzed images in the Outex dataset [23] using a subset of the attributes we consider. Their attributes were demonstrated to perform better than several low-level descriptors, but these were trained and evaluated on the same dataset. Hence it is not clear if their learned attributes generalize well to other settings. In contrast, we show that: (i) our texture attributes trained on DTD outperform their semantic attributes on Outex and (ii) they can significantly boost performance on a number of other material and texture benchmarks (Sect. 4.2).\n\n\nDataset design and collection\n\nThis section discusses how DTD was designed and collected, including: selecting the 47 attributes, finding at least 120 representative images for each attribute, collecting a full set of multiple attribute labels for each image in the dataset, and addressing annotation noise.\n\nSelecting the describable attributes. Psychological experiments suggest that, while there are a few hundred words that people commonly use to describe textures, this vocabulary is redundant and can be reduced to a much smaller number of representative words. Our starting point is the list of 98 words identified by Bhusan, Rao and Lohse [5]. Their seminal work aimed to achieve for texture recognition the same that color words have achieved for describing color spaces [4]. However, their work mainly focuses on the cognitive aspects of texture perception, including perceptual similarity and the identification of directions of perceptual texture variability. Since we are interested in the visual aspects of texture, we ignored words such as \"corrugated\" that are more related to surface shape properties, and words such as \"messy\" that do not necessarily correspond to visual features. After this screening phase we analyzed the remaining words and merged similar ones such as \"coiled\", \"spiraled\" and \"corkscrewed\" into a single term. This resulted in a set of 47 words, illustrated in Fig. 2.\n\nBootstrapping the key images. Given the 47 attributes, the next step was collecting a sufficient number (120) of example images representative of each attribute. A very large initial pool of about a hundred-thousands images was downloaded from Google and Flickr by entering the attributes and related terms as search queries. Then Amazon Mechanical Turk (AMT) was used to remove low resolution, poor quality, watermarked images, or images that were not almost entirely filled with a texture. Next, detailed annotation instructions were created for each of the 47 attributes, including a dictionary definition of each concept and examples of correct and incorrect matches. Votes from three AMT annotators were collected for the candidate images of each attribute and a shortlist of about 200 highly-voted images was further manually checked by the authors to eliminate residual errors. The result was a selection of 120 key representative images for each attribute.\n\nSequential join annotations. So far only the key attribute of each texture image is known while any of the remaining 46 attributes may apply as well. Exhaustively collecting annotations for 46 attributes and 5,640 texture images was found to be too expensive. To reduce this cost we propose to exploiting the correlation and sparsity of the attribute occurrences (Fig. 3). For each attribute q, twelve key images are annotated exhaustively and used to estimate the probability p(q |q) that another attribute q could co-exist with q.\n\nThen for the remaining key images of attribute q, only annotations for attributes q with non negligible probability -in practice 4 or 5 -are collected, assuming that the attributes would not apply. This procedure occasionally misses attribute annotations; Fig. 3 evaluates attribute recall by 12fold cross-validation on the 12 exhaustive annotations for a fixed budget of collecting 10 annotations per image (instead of 47). A further refinement is to suggest which attributes q to annotated not just based on q, but also based on the individual appearance of an image i . This was done by using the attribute classifier learned in Sect. 4; after Platt's calibration [28] on an held-out test set, the classifier score\nc q ( i ) \u2208 R is transformed in a probability p(q | i ) = \u03c3(c q ( )) where \u03c3(z) = 1/(1 + e \u2212z )\nis the sigmoid function. By construction, Platt's calibration reflects the prior probability p(q ) \u2248 p 0 = 1/47 of q on the validation set. To reflect the probability p(q |q) instead, the score is adjusted as\np(q | i , q) \u221d \u03c3(c q ( i )) \u00d7 p(q |q) 1 \u2212 p(q |q) \u00d7 1 \u2212 p 0 p 0\nand used to find which attributes to annotated for each image. As shown in Fig. 3, for a fixed annotation budged this method increases attribute recall. Overall, with roughly 10 annotations per images it was possible to recover of all the attributes for at least 75% of the images, and miss one out of four (on average) for another 20% while keeping the annotation cost to a reasonable level.\n\nHandling noisy annotations. So far it was assumed that annotators are perfect: deterministic and noise-free. This is not the case, in part due to the intrinsic subjectivity of describable texture attributes, and in part due to distracted, adversarial, or unqualified annotators. As commonly done, we address this problem by collecting the same annotation multiple times (five) using different annotators, and forming a consensus.\n\nBeyond simple voting, we found that the method of [38] can effectively remove or down-weigh bad annotators improving agreement. This method models each annotator \u03b1 j as a classifier with a given bias and error rate. Then, given a collection\u00e2 qij \u2208 {0, 1} of binary annotations for attribute q, image i, and annotator j, it tries to estimate simultaneously the ground truth labels a qi and the quality \u03b1 j of the individual annotators. The method is appealing as several quantities are easily interpretable. For example, the prior p(\u03b1 j ) on annotators encodes how frequently we expect to find good and bad annotators (e.g. we found that 0.5% of them labeled images randomly). A major difference compared to the scenario considered in [38] is that, in our case, the key attribute of each image is already known. By incorporating this as additional prior, the method can use the key attributes to implicitly benchmark and calibrate annotators. The final set of annotations {a qi } is obtained by thresholding the (approximated) posterior marginal p(a qi |{\u00e2 qij }) to 60%, similar to choosing three out of five votes in the basic voting scheme, computed using variational inference. In general, we found most probabilities to be very close to 100% or 0%, suggesting that there is little residual noise in the process. We also inspected the top 30 images of each attribute based on simple voting and this posterior marginals and found the ranking to be significantly improved.\n\n\nTexture representations\n\nGiven the DTD dataset developed in Sect. 2, this section moves on to the problem of designing a system that can automatically recognize the attributes of textures. Given a texture image the first step is to compute a representation \u03c6( ) \u2208 R d of the image; the second step is to use a classifier such as a Support Vector Machine (SVM) w, \u03c6( ) to score how strongly the matches a given perceptual category. We propose two such representations: a gold-standard low-level texture descriptor based on the improved Fisher Vector (Sect. 3.1) and a mid-level texture descriptor consisting of the describable attributes themselves (Sect. 3.2). The details of the classifiers are discussed in Sect. 4.\n\n\nImproved Fisher vectors\n\nThis section introduces our gold-standard low-level texture representation, the Improved Fisher Vector (IFV) of and relates it to existing texture descriptors. We port IFV from the object recognition literature [27] and we show that it substantially outperforms specialized texture representations (Sect. 4).\n\nGiven an image , the Fisher Vector (FV) formulation of [26] starts by extracting local SIFT [20] descriptors {d 1 , . . . , d n } densely and at multiple scales. It then soft-quantizes the descriptors by using a Gaussian Mixture Model (GMM) with K modes, prior probabilities \u03c0 k , mode means \u00b5 k and mode covariances \u03a3 k . Covariance matrices are assumed to be diagonal, but local descriptors are first decorrelated and optionally dimensionality reduced by PCA. Then first and second order statistics are computed as\nu jk = 1 n \u221a \u03c0 k n i=1 q ik d ji \u2212 \u00b5 jk \u03c3 jk , v jk = 1 n \u221a 2\u03c0 k n i=1 q ik d ji \u2212 \u00b5 jk \u03c3 jk 2 \u2212 1 ,\nwhere j spans descriptor dimensions and q ik is the posterior probability of mode k given descriptor\nd i , i.e. q ik \u221d exp \u2212 1 2 (d i \u2212 \u00b5 k ) T \u03a3 \u22121 k (d i \u2212 \u00b5 k )\n. These statistics are then stacked into a vector (u 1 , v 1 , . . . , u K , v K ). In order to obtain the improved version of the representation, the signed square root |z| sign z is applied to its components and the vector is l 2 normalized.\n\nAt least two key ideas in IFV were pioneered in texture analysis: the idea of sum-pooling local descriptors was introduced by [21], and the idea of quantizing local descriptors to construct histogram of features was pioneered by [19] with their computational model of textons. However, three key aspects of the IFV representation were developed in the context of object recognition. The first one is the use of the SIFT descriptors, originally developed for object matching [20], that are more distinctive that local descriptors popular in texture analysis such as filter banks [13,19,36], local intensity patterns [23], and patches [35]. The second one is replacing histogramming with the more expressive FV pooling method [26]. And the third one is the use of the square-root kernel map [27] in the improved version of the Fisher Vector.\n\nWe are not the first to use SIFT or IFV in texture recognition. For example, SIFT was used in [29], and Fisher Vectors were used in [31]. However, neither work tested the standard IFV formulation [27], which is well tuned for object recognition, developing instead variations specialized for texture analysis. We were therefore somewhat surprised to discover that the off-the-shelf method surpasses these approaches (Sect. 4.1).\n\n\nDescribable attributes as a representation\n\nThe main motivation for recognizing describable attributes is to support human-centric applications, enriching the vocabulary of visual properties that machines can understand. However, once extracted, these attributes may also be used as texture descriptors in their own right. As a simple incarnation of this idea, we propose to collect the response of attribute classifiers trained on DTD in a 47-dimensional feature vector \u03c6( ) = (c 1 ( ), . . . , c 47 ( )). Sect. 4 shows that this very compact representation achieves excellent performance in material recognition; in particular, combined with IFV (SIFT and color) it sets the new state-of-the-art on KTH-TIPS2-b and FMD. In addition to the contribution to the best results, our proposed attributes generate meaningful descriptions of the materials from KTH-TIPS2-b (aluminium foil: wrinkled; bread: porous).\n\n\nExperiments\n\n\nImproved Fisher Vectors for textures\n\nThis section demonstrates the power of IFV as a texture representation by comparing it to established texture descriptors. Most of these representations can be broken down into two parts: computing local image descriptors {d 1 , . . . , d n } and encoding them into a global image statistics \u03c6( ).\n\nIn IFV the local descriptors d i are 128-dimensional SIFT features, capturing a spatial histogram of the local gradient orientations; here spatial bins have an extent of 6 \u00d7 6 pixels and descriptors are sampled every two pixels and at scales 2 i/3 , i = 0, 1, 2, . . . . We also evaluate as local descriptors the Leung and Malik (LM) [19] (48-D) and MR8 (8-D) [13,36] filter banks, the 3 \u00d7 3 and 7 \u00d7 7 raw image patches of [35], and the local binary patterns (LBP) of [23].\n\nEncoding maps image descriptors {d 1 , . . . , d n } to a statistics \u03c6( ) \u2208 R d suitable for classification. Encoding can be as simple as averaging (sum-pooling) descriptors [21], although this is often preceded by a high-  Table 1: Comparison of local descriptors and kernels on the DTD data, averaged over ten splits.\n\ndimensional sparse coding step. The most common coding method is to vector quantize the descriptors using an algorithm such as K-means [19], resulting in the so-called bag-of-visual-words (BoVW) representation [8]. Variations include soft quantization by a GMM in FV (Sect. 3.1) or specialized quantization schemes, such as mapping LBPs to uniform patterns [23] (LBP u ; we use the rotation invariant multiple-radii version of [22] for comparison purposes). For LBP, we also experiment with a variant (LBP-VQ) where standard LBP u2 is computed in 8 \u00d7 8 pixel neighborhoods, and the resulting local descriptors are further vector quantized using K-means and pooled as this scheme performs significantly better in our experiments. For each of the selected features, we experimented with several SVM kernels:\nlinear K(x , x ) = x , x , Hellinger's d i=1 x i x i , additive-\u03c7 2 d i=1 x i x i /(x i + x i ), and exponential-\u03c7 2 exp \u2212\u03bb d i=1 (x i \u2212 x i ) 2 /(x i + x i )\nkernels signextended as in [37]. In the latter case, \u03bb is selected as one over the mean of the kernel matrix on the training set. The data is normalized so that K(x , x ) = 1 as this is often found to improve performance. Learning uses a standard non-linear SVM solver and validation in order to select the parameter C in the range {0.1, 1, 10, 100} (the choice of C was found to have little impact on the result).\n\nLocal descriptor comparisons on DTD. This experiments compares local descriptors and kernels on DTD. All comparison use the bag-of-visual-word pooling/encoding scheme using K-means for vector quantization the descriptors. The DTD data is used as a benchmark averaging the results on the ten train-val-test splits. K was cross-validated, finding an optimal setting of 1024 visual words for SIFT and color patches, 512 for LBP-VQ, 470 for the filter banks. Tab. 1, reports the mean Average Precision (mAP) for 47 SVM attribute classifiers. As expected, the best kernel is exp-\u03c7 2 , followed by additive \u03c7 2 and Hellinger, and then linear. Dense SIFT (53.82% mAP) outperforms the best specialized texture descriptor on the DTD data (39.67% mAP for LM). Fig. 4 shows AP for each attribute: concepts like chequered achieve nearly perfect classification, while oth-   Encoding comparisons on DTD. This experiment compares three encodings: BoVW, VLAD [15] and IFV. VLAD is similar to IFV, but uses K-means for quantization and stores only first-order statistics of the descriptors. Dense SIFT is used as a baseline descriptor and performance is evaluated on ten splits of DTD in Tab. 2. IFV (256 Gaussian modes) and VLAD (512 K-means centers) performs similarly (about 60% mAP) and significantly better than BoVW (53.82% mAP). As we will see next, however, IFV significantly outperforms VLAD in other texture datasets. We also experimented with the state-of-the-art descriptor of [32] which we did not find to be competitive with IFV on FMD and DTD (Tab. 2); unfortunately could not obtain an implementation of [29] to try on our data -however IFV SIFT outperforms it on material recognition.\n\nState-of-the-art material classification. This experiments evaluates the encodings on several material recognition datasets: CUReT [9], UMD [39], UIUC [18], KTH-TIPS [14], KTH-TIPS2(a and b) [7], and FMD [30]. Tab. 2 compares with the existing state-of-the-art [31,32,34] on each of them. For saturated datasets such as CUReT, UMD, UIUC, KTH-TIPS the performance of most methods is above to 99% mean accuracy and there is little difference between them. In harder datasets the advantage of IFV is evident: KTH-TIPS-2a (+5%), KTH-TIPS-2b (+3%), and FMD (+1%). In particular, while FMD includes manual segmentations of the textures, these are not used here here. Furthermore, IFV is conceptually simpler than the multiple specialized features used in [31] for material recognition.  \n\n\nDescribable attributes as a representation\n\nThis section evaluates using the 47 describable attributes as a texture descriptor applying it to the task of material recognition. The attribute classifiers are trained on DTD using the IFV+SIFT representation and linear classifiers as in the previous section (DTD LIN ). As explained in Sect. 3.2, these are then used to form 47-dimensional descriptors of each texture image in FMD and KTH-TIPS2-b.\n\nWhen combined with a linear SVM classifier, results are promising (Tab. 2): on KTH-TIPS2-b, the describable attributes yield 61.1% mean accuracy and 49.0% on FMD outperforming the aLDA model of [29] combining color, SIFT and edge-slice (44.6%). While results are not as good as the IFV SIFT representation, the dimensionality of this descriptor is three orders of magnitude smaller than IFV. For this reason, using an RBF classifier with the DTD features is relatively cheap. Doing so improves the performance by 3.5-4% (DTD RBF ).\n\nWe also investigated combining multiple features: DTD RBF with IFV SIFT and IFV RGB . IFV RGB computes the IFV representation on top of all the 3 \u00d7 3 RGB patches in the image in the spirit of [35]. The performance of IFV RGB is notable given the simplicity of the local descriptors; however, it is not as good as DTD RBF which is also 26 times smaller. The combination of IFV SIFT and IFV RGB is already notably better than the previous state-of-the-art results and the addition of DTD RBF improves by another significant margin. Overall, our best result on KTH-TIPS-2b is 74.6% (vs. the previous best of 66.3) and on FMD of 65.4% (vs. 57.1) on FMD, with an improvement of more than 8% accuracy in both cases.\n\nFinally, we compared the semantic attributes of [22] with DTD LIN on the Outex data. Using IFV SIFT as an underlying representation for our attributes, we obtain 49.82% mAP on the retrieval experiment of [22], which is is not as good as their result with LBP u (63.3%). However, LBP u was developed on the Outex data, and it is therefore not surprising that it works so well. To verify this, we retrained our DTD attributes with IFV using LBP u as local descriptor, obtaining a score of 64.52% mAP. This is remarkable considering that their retrieval experiment contains the data used to train their own attributes (target set), while our attributes are trained on a completely different data source. Tab. 1 shows that LBP u is not competitive on DTD. Fig. 5 shows that there is an excellent semantic correlation between the ten categories in KTH-TIPS-2b and the attributes in DTD. For example, aluminium foil is found to be wrinkled, while bread is found as: bumpy, pitted, porous and flecked.\n\n\nSearch and visualization\n\nIn what follows, we experimented with describing images from a challenging material dataset, FMD and encouraged by the good results, we applied the same technique to images from the wild, from some online catalog.\n\n\nSubcategorizing FMD materials using describable texture attributes\n\nThe results shown in Fig. 6 extends the results in Table 2 and Sect. 4.2 and illustrate the classification performance of the 47-dimensional DTD descriptors on the FMD materialsnote the excellent performance obtained for foliage, wood, and water, which are above 70%. Our experiments illustrate how the DTD attributes can be used to find \"semantic structures\" in a dataset such as FMD, for example by distinguishing between \"knitted vs pleated fabric\", \"gauzy vs crystalline glass\", \"veined vs frilly foliage\" etc. To do so, FDM images for each material were clustered based on the 47 attribute vectors using K-means into 3-5 clusters each. Examples of the most meaningful clusters are shown in Fig. 8  which contain linear structures. In the latter case, veins often have a radial pattern which is captured by the dominant spiralled attribute. The method distinguishes bumpy stones such as pebbles from porous or pitted stones for zoomed / detailed views of stone blocks. Water is divided into swirly & spiralled images, which show the orientation of the waves, and bubbly, sprinkled images, which show splashing drops. Glass is more challenging but some images are correctly identified as crystalline. Fig. 7 shows other challenging examples illustrating the variety of materials and patterns that can be described by the DTD attributes. Metal is one of the hardest class to identify (Fig. 6), but attributes such as \"interlaced\" and \"braided\" are still correctly recognized in the third (jewelry) and last (metal wires) image.\n\n\nExamples in the wild\n\nAs an additional application of our describable texture attributes we compute them on a large dataset of 10,000 wallpapers and bedding sets (about 5,000 for each of the two categories) from houzz.com. The 47 attribute classifiers are learned as explained in Sect. 4.1 using the IFV SIFT representation and them apply them to the 10,000 images to predict the strength of association of each attribute and image. Classifiers scores are recalibrated on a subset of the target data and converted to probabilities using Platt's method [28], for each individual attribute. Fig. 11 Figure 6: Per class AP results on FMD dataset using DTD classification scores as features. Fig. 12 shows some example attribute predictions (excluding images used for calibrating the scores), for the best scoring 3-4 images for each category -by top attribute. We show for each images the top three attributes -the top two being very accurate, while the third is correct in about half of the cases. Please note that each score is calibrated on a per attribute basis, to the scores do not add up to 1.\n\n\nSummary\n\nWe introduced a large dataset of 5,640 images collected \"in the wild\" jointly labeled with 47 describable texture attributes and used it to study the problem of extracting semantic properties of textures and patterns, addressing realworld human-centric applications. Looking for the best representation to recognize such describable attributes in natural images, we have ported IFV, an object recognition representation, to the texture domain. Not only IFV works best in recognizing describable attributes, but it also outperforms specialized texture representation on a number of challenging material recognition benchmarks. We have shown that the describable attributes, while not being designed to do so, are good predictors of materials as well, and that, when combined with IFV, significantly outperform the state-ofthe-art on the FMD and KTH-TIPS recognition tasks.       \n\nFigure 2 :\n2The 47 texture words in the describable texture dataset introduced in this paper. Two examples of each attribute are shown to illustrate the significant amount of variability in the data.\n\nFigure 3 :\n3Quality of joint sequential annotations. Each bar shows the average number of occurrences of a given attribute in a DTD image. The horizontal dashed line corresponds to a frequency of 1/47, the minimum given the design of DTD (Sect. 2.1). The black portion of each bar is the amount of attributes discovered by the sequential procedure, using only 10 annotations per image (about one fifth of the effort required for exhaustive annotation). The orange portion shows the additional recall obtained by integrating CV in the process. Right: co-occurrence of attributes. The matrix shows the joint probability p(q, q ) of two attributes occurring together (rows and columns are sorted in the same way as the left image).\n\nFigure 4 :\n4Per-class AP of the 47 describable attribute classifiers on DTD using the IFV SIFT representation and linear classifiers. ers such as blotchy and smeared are far harder.\n\nFigure 5 :\n5along with the dominant attributes in each.Notable fine-grained material distinctions include knitted vs pleated fabrics and frilly vs pleated & veined foliage Descriptions of materials from KTH-TIPS-2b dataset. These words are the most frequent top scoring texture attributes (from the list of 47 we proposed), when classifying the images from the KTH-TIPS-2b dataset.\n\nFigure 7 :\n7Challenging or difficult images which were correctly characterized by our DTD classifier.\n\nFigure 10 :\n10Continued fromFig. 9Subcategories for stone and water images.\n\nFigure 12 :\n12Example bedding sets from an online catalog (houzz.com).\n\nTable 2 :\n2Left: Comparison of encodings and state-of-the-art texture recognition methods on DTD as well as standard material recognition benchmarks. \u03b1 : three samples for training, one for evaluation; \u03b2 : one sample for training, three for evaluation.\u03b3 : with/without ground truth masks ([29] Sect. 6.5); our results do not use them. Right: Combined with IFV SIFT and IFV RGB , \nthe DTD RBF features achieve a significant improvement in classification performance on the challenging KTH-TIPS-2b and \nFMD compared to published state of the art results. \n\n\n\n\nFigure 8: Example meaningful clusters of FMD categories, obtained using K-means on DTD classification scores. Showing results for fabric and glass -overlayed, we list the most frequently identified attributes. On each image, we show the top 3 scoring texture words. wood (cracked, veined, interlaced) Figure 9: Continued from Fig. 8. Displaying results on foliage, paper and wood. stone (porous, pitted, flecked)meshed \nbraided \n\nknitted \nwrinkled \nporous \n\nlacelike \nporous \nfrilly \n\nfibrous \npitted \nknitted \n\ngauzy \nspiralled \nbraided \n\nlacelike \ngrid \nblotchy \n\nspiralled \nsprinkled \nknitted \n\nknitted \nstained \nbraided \n\nknitted \nlacelike \nporous \n\nknitted \nsprinkled \nbraided \n\nbumpy \nscaly \nknitted \n\nporous \nspiralled \ndotted \n\ncrosshatched \nstained \ninterlaced \n\nbumpy \ncobwebbed \nsmeared \n\nfabric (knitted) \n\npleated \nstriped \npolka\u2212dotted \n\nlacelike \nblotchy \ncracked \n\ngauzy \nstriped \nbraided \n\npleated \nstained \nveined \n\ngrooved \ncrosshatched \npleated \n\nstained \ngauzy \nveined \n\nstriped \ngauzy \nstained \n\ngauzy \nwaffled \npleated \n\ngauzy \nsmeared \nblotchy \n\ngauzy \npitted \nspiralled \n\nsmeared \npleated \nswirly \n\ngauzy \nflecked \nstudded \n\ngauzy \nbraided \nveined \n\ncrosshatched \ngauzy \nsprinkled \n\nfabric (pleated) \n\ncobwebbed \nmeshed \nswirly \n\nblotchy \ngauzy \nfreckled \n\nfibrous \ngauzy \nmatted \n\nchequered \ngauzy \nsmeared \n\nbubbly \nstratified \nbumpy \n\nbraided \nspiralled \nwrinkled \n\nbubbly \ngauzy \ndotted \n\nporous \nbanded \nveined \n\nglass (bubbly, gauzy) \n\ncrystalline \nbraided \nbumpy \n\ncrystalline \nsprinkled \nbubbly \n\ncrystalline \nbubbly \nsprinkled \n\ncrystalline \npitted \nfreckled \n\nfrilly \nbubbly \ncrystalline \n\ncrystalline \nhoneycombed \nbubbly \n\nglass (crystalline, bubbly) \n\nveined \nspiralled \ngauzy \n\nveined \nspiralled \nwrinkled \n\nveined \nblotchy \nchequered \n\npleated \nstriped \nspiralled \n\nveined \ngrooved \nlined \n\nstriped \nwaffled \nspiralled \n\npleated \nspiralled \nhoneycombed \n\nveined \nbraided \nmeshed \n\nveined \nspiralled \nbumpy \n\nscaly \nfrilly \nstriped \n\nbraided \nmarbled \nveined \n\npleated \nzigzagged \nspiralled \n\nbraided \nswirly \nveined \n\nveined \ngrid \nswirly \n\nfoliage (pleated, spiralled, veined) \n\nveined \nwrinkled \nfrilly \n\nfrilly \nfreckled \nhoneycombed \n\nfrilly \nsprinkled \nblotchy \n\nfrilly \nfreckled \nsprinkled \n\ncrystalline \nwaffled \nfreckled \n\ngauzy \nsprinkled \npleated \n\nstriped \nchequered \ncrystalline \n\nhoneycombed \nfrilly \nswirly \n\nblotchy \nfrilly \nsprinkled \n\nsprinkled \nfrilly \nspiralled \n\nspiralled \nstained \nflecked \n\nwrinkled \nfrilly \nswirly \n\nsprinkled \nfreckled \npleated \n\nstained \nsprinkled \ncobwebbed \n\nfoliage (frilly, sprinkled) \n\npleated \nsprinkled \nfrilly \n\npleated \nspiralled \ncobwebbed \n\nfrilly \nbumpy \nporous \n\ncrystalline \npotholed \nblotchy \n\nwrinkled \nbumpy \nblotchy \n\nfrilly \npleated \nblotchy \n\nwrinkled \nfreckled \ngauzy \n\ngauzy \nbubbly \npleated \n\nfrilly \npleated \ninterlaced \n\nsprinkled \nfrilly \npleated \n\nbubbly \nfrilly \nblotchy \n\nwrinkled \ngauzy \nfreckled \n\ngauzy \nbraided \ncrystalline \n\nhoneycombed \nblotchy \npleated \n\npaper (wrinkled, pleated) \n\ncracked \nveined \ncrosshatched \n\nstratified \npaisley \nwrinkled \n\ngrooved \nveined \npleated \n\nwrinkled \nwoven \nveined \n\ngauzy \npleated \nspiralled \n\ncracked \nstratified \nblotchy \n\ngauzy \ninterlaced \npaisley \n\ncrosshatched \nstratified \ncracked \n\ninterlaced \ngrooved \ncrosshatched \n\nstratified \npaisley \nveined \n\ncracked \nveined \npleated \n\nswirly \nbumpy \nstudded \n\ninterlaced \nfibrous \nwrinkled \n\nbumpy \ngauzy \nlacelike \n\nbumpy \nbraided \nwoven \n\nfreckled \nbumpy \nwaffled \n\nbumpy \nscaly \nfrilly \n\nsprinkled \nfreckled \nwrinkled \n\nbumpy \nblotchy \nzigzagged \n\nsprinkled \nblotchy \nwaffled \n\nsprinkled \nsmeared \nspiralled \n\nflecked \nbumpy \nbraided \n\nbraided \nbumpy \ncracked \n\nbumpy \nfreckled \nbraided \n\nsprinkled \ncrystalline \nbubbly \n\nbumpy \nbraided \ndotted \n\nflecked \nbraided \nfrilly \n\nsprinkled \nbumpy \nveined \n\nstone (bumpy) \n\npotholed \nporous \nscaly \n\nmatted \ncrosshatched \nblotchy \n\npotholed \npitted \ngrid \n\nporous \nmatted \nbubbly \n\nmatted \nzigzagged \ncrosshatched \n\nsprinkled \nwaffled \nblotchy \n\nwaffled \nknitted \nsprinkled \n\npitted \npotholed \nknitted \n\nstained \npotholed \ncrosshatched \n\nporous \nbraided \nflecked \n\npitted \nsmeared \nchequered \n\nporous \nfibrous \npotholed \n\nporous \nbumpy \nstained \n\nsprinkled \nporous \nmatted \n\nsmeared \nbubbly \npleated \n\nsmeared \nswirly \npleated \n\nsmeared \nbraided \nbubbly \n\nsmeared \npitted \nswirly \n\nbubbly \nmeshed \nsmeared \n\nsmeared \nswirly \nfibrous \n\nsmeared \nsprinkled \nbubbly \n\nbubbly \nblotchy \nsprinkled \n\nsmeared \nwrinkled \nmeshed \n\nstained \nfrilly \nbubbly \n\nsmeared \nbubbly \nsprinkled \n\nsprinkled \nbubbly \nsmeared \n\nstained \nfrilly \nfreckled \n\ncrystalline \nbubbly \nbumpy \n\nwater (bubbly, smeared) \n\nsmeared \nswirly \nspiralled \n\npleated \npaisley \nbumpy \n\nfibrous \nstriped \nmatted \n\nveined \ncracked \nspiralled \n\nswirly \nbraided \nsmeared \n\nbraided \nswirly \nlined \n\nswirly \npleated \nsmeared \n\nsmeared \nspiralled \npleated \n\nswirly \nlined \nspiralled \n\ngauzy \nstratified \nwaffled \n\npotholed \nstratified \nsmeared \n\nwaffled \nstratified \npleated \n\ncobwebbed \nhoneycombed \npitted \n\nbubbly \ncrosshatched \nswirly \n\nwater (swirly, spiralled) \n\n\n\n\nFigure 11: Example wallpaper images from an online catalog (houzz.com).(1.00) \nbanded (0.15) \nlined (0.14) \n\nzigzagged (1.00) \nlined (0.17) \nmarbled (0.17) \n\npolka\u2212dotted (1.00) \nbanded (0.24) \nlined (0.12) \n\nzigzagged (1.00) \nmarbled (0.24) \nbanded (0.22) \n\nzigzagged (1.00) \nhoneycombed (0.12) \nmarbled (0.10) \n\nwoven (1.00) \ngrooved (0.92) \npolka\u2212dotted (0.45) \n\nlined (1.00) \ngrooved (0.80) \nzigzagged (0.15) \n\nlined (1.00) \ngrooved (0.59) \nzigzagged (0.17) \n\npolka\u2212dotted (1.00) \nbanded (0.16) \nhoneycombed (0.13) \n\ngrooved (1.00) \nstratified (0.73) \nveined (0.41) \n\ngrooved (1.00) \nveined (0.70) \nporous (0.16) \n\nlined (1.00) \ngrooved (0.52) \npleated (0.12) \n\ngrooved (1.00) \nzigzagged (0.35) \nlined (0.20) \n\nbanded (1.00) \ngrooved (0.30) \nbumpy (0.16) \n\nlined (1.00) \nmarbled (0.19) \ngrooved (0.17) \n\nbanded (1.00) \ngrooved (0.24) \nbraided (0.18) \n\nsprinkled (1.00) \npitted (0.65) \nflecked (0.50) \n\nbanded (1.00) \nspiralled (0.17) \nzigzagged (0.13) \n\ngrooved (1.00) \nzigzagged (0.29) \nstratified (0.20) \n\nlacelike (1.00) \nporous (0.96) \nwoven (0.44) \n\nperforated (1.00) \ngrid (0.59) \npitted (0.59) \n\nwoven (1.00) \ngrooved (0.78) \nlined (0.46) \n\nwoven (1.00) \ngrid (0.57) \nmeshed (0.44) \n\nswirly (1.00) \nspiralled (0.63) \nlacelike (0.41) \n\nflecked (1.00) \nblotchy (0.66) \nwoven (0.63) \n\nwoven (1.00) \nmeshed (0.73) \nbraided (0.54) \n\ninterlaced (1.00) \ncrosshatched (0.80) \nveined (0.57) \n\nswirly (1.00) \ninterlaced (0.29) \nbubbly (0.19) \n\nveined (1.00) \nlined (0.67) \ncrosshatched (0.64) \n\nflecked (1.00) \nporous (0.31) \nlacelike (0.31) \n\ninterlaced (1.00) \nveined (0.59) \ngrid (0.44) \n\nmeshed (1.00) \ngrid (0.69) \npolka\u2212dotted (0.25) \n\nstratified (1.00) \nlined (0.69) \ngrooved (0.62) \n\ninterlaced (1.00) \ngrid (0.68) \nmeshed (0.40) \n\ndotted (1.00) \npolka\u2212dotted (0.72) \npaisley (0.38) \n\ncobwebbed (1.00) \nveined (0.91) \ncracked (0.74) \n\nstratified (1.00) \nbumpy (0.30) \npleated (0.30) \n\nbanded (1.00) \nbraided (0.15) \nzigzagged (0.14) \n\nstratified (1.00) \ncracked (0.92) \nsprinkled (0.62) \n\ndotted (1.00) \nlacelike (0.69) \nhoneycombed (0.57) \n\nperforated (1.00) \nsprinkled (0.50) \npaisley (0.50) \n\npleated (1.00) \nsmeared (0.52) \nzigzagged (0.32) \n\nlacelike (1.00) \nporous (0.48) \nbubbly (0.43) \n\nblotchy (1.00) \ndotted (0.32) \nbubbly (0.27) \n\nveined (1.00) \nmeshed (0.65) \nzigzagged (0.64) \n\nmeshed (1.00) \ngrooved (0.41) \ngrid (0.38) \n\ngrid (1.00) \nmeshed (0.32) \nsprinkled (0.28) \n\nstained (1.00) \nblotchy (0.56) \nwoven (0.54) \n\nlacelike (1.00) \nstained (0.90) \nbubbly (0.26) \n\nstratified (1.00) \ncracked (0.87) \ngrooved (0.31) \n\ninterlaced (1.00) \nveined (0.86) \nflecked (0.48) \n\nswirly (1.00) \npaisley (0.48) \nperforated (0.45) \n\ncrosshatched (1.00) \nwoven (0.62) \nflecked (0.30) \n\nblotchy (0.99) \nmarbled (0.49) \nporous (0.29) \n\nswirly (0.99) \npaisley (0.80) \nbubbly (0.76) \n\ngauzy (0.99) \nspiralled (0.48) \nwoven (0.21) \n\nbraided (0.99) \ngauzy (0.65) \nspiralled (0.33) \n\nsprinkled (0.99) \nperforated (0.37) \nswirly (0.35) \n\nmeshed (0.99) \nwoven (0.63) \nzigzagged (0.41) \n\ncrosshatched (0.99) \nveined (0.48) \nlined (0.30) \n\nchequered (1.00) \nsprinkled (0.84) \ngauzy (0.54) \n\nhoneycombed (1.00) \ngrid (0.49) \nswirly (0.23) \n\ngrooved (1.00) \nbanded (0.31) \nswirly (0.24) \n\npleated (1.00) \nsprinkled (0.69) \nspiralled (0.27) \n\ninterlaced (1.00) \nhoneycombed (0.25) \nmeshed (0.20) \n\nchequered (1.00) \nwoven (0.61) \ngrid (0.40) \n\nchequered (1.00) \nwrinkled (0.98) \ninterlaced (0.85) \n\ninterlaced (1.00) \nspiralled (0.18) \nhoneycombed (0.17) \n\ngauzy (1.00) \nstudded (0.84) \ncrosshatched (0.49) \n\nchequered (1.00) \ncobwebbed (0.40) \ngauzy (0.37) \n\ngrooved (1.00) \ngauzy (0.30) \ncobwebbed (0.24) \n\nzigzagged (1.00) \nswirly (0.91) \nveined (0.37) \n\ngauzy (1.00) \npleated (0.69) \nfrilly (0.65) \n\nwoven (1.00) \ninterlaced (0.45) \nlacelike (0.43) \n\ninterlaced (1.00) \nspiralled (0.62) \nswirly (0.61) \n\ngrooved (1.00) \nveined (0.89) \nmeshed (0.32) \n\nsmeared (1.00) \nbumpy (0.16) \npaisley (0.15) \n\ngrooved (0.99) \npotholed (0.27) \nbraided (0.18) \n\nmeshed (0.99) \ngrid (0.89) \ncobwebbed (0.84) \n\nbanded (0.99) \nblotchy (0.22) \ngrid (0.14) \n\nfrilly (0.99) \nwrinkled (0.68) \npleated (0.22) \n\nbraided (0.99) \nbumpy (0.44) \nveined (0.33) \n\nbanded (0.99) \nbraided (0.64) \nspiralled (0.60) \n\npleated (0.99) \nhoneycombed (0.81) \nsmeared (0.58) \n\ninterlaced (0.99) \ngrooved (0.73) \nspiralled (0.37) \n\nswirly (0.99) \nstudded (0.55) \nfrilly (0.37) \n\ngauzy (0.99) \ninterlaced (0.43) \nstudded (0.41) \n\ncrosshatched (0.99) \nstudded (0.54) \nlacelike (0.44) \n\nswirly (0.99) \nveined (0.81) \nhoneycombed (0.54) \n\nsmeared (0.99) \nhoneycombed (0.58) \npotholed (0.30) \n\nwrinkled (0.99) \nhoneycombed (0.76) \ncracked (0.54) \n\ngauzy (0.99) \nblotchy (0.92) \ngrid (0.43) \n\npleated (0.99) \nbraided (0.47) \nspiralled (0.37) \n\ncobwebbed (0.99) \nswirly (0.53) \npleated (0.47) \n\nzigzagged (0.99) \ninterlaced (0.19) \nswirly (0.12) \n\npleated (0.99) \nchequered (0.72) \nwrinkled (0.44) \n\nspiralled (0.99) \nfibrous (0.87) \nveined (0.87) \n\npaisley (0.99) \ncrosshatched (0.48) \nlacelike (0.36) \n\ncobwebbed (0.99) \nmeshed (0.45) \nsprinkled (0.32) \n\nspiralled (0.99) \ncobwebbed (0.65) \nporous (0.61) \n\ncobwebbed (0.99) \nchequered (0.62) \nsprinkled (0.46) \n\nfrilly (0.99) \nblotchy (0.91) \nswirly (0.35) \n\ncobwebbed (0.99) \nchequered (0.73) \ngrid (0.28) \n\nhoneycombed (0.98) \nblotchy (0.49) \nstratified (0.45) \n\nbanded (0.98) \nfrilly (0.59) \nblotchy (0.19) \n\nwoven (0.98) \nbraided (0.73) \nspiralled (0.39) \n\nfrilly (0.98) \nbubbly (0.58) \npaisley (0.49) \n\nsprinkled (0.98) \nfrilly (0.69) \nbubbly (0.63) \n\nswirly (0.98) \nfrilly (0.84) \ninterlaced (0.26) \n\nblotchy (0.98) \nsprinkled (0.56) \nmeshed (0.34) \n\nsmeared (0.98) \npleated (0.69) \nsprinkled (0.68) \n\nzigzagged (0.98) \nmeshed (0.36) \npleated (0.26) \n\nwoven (0.98) \nmeshed (0.46) \ncobwebbed (0.28) \n\nbraided (0.98) \ncrosshatched (0.92) \nmeshed (0.74) \n\nsprinkled (0.98) \ncrosshatched (0.85) \ngauzy (0.43) \n\nsmeared (0.98) \nbubbly (0.88) \nveined (0.78) \n\nspiralled (0.98) \nbumpy (0.46) \ngrooved (0.28) \n\nbraided (0.98) \ngauzy (0.87) \nspiralled (0.46) \n\ncrosshatched (0.98) \ngrid (0.75) \ngauzy (0.56) \n\nzigzagged (0.98) \nlined (0.14) \nfrilly (0.08) \n\n\n\nM Amadasun, R King, Textural features corresponding to textural properties. Systems, Man, and Cybernetics. 19M. Amadasun and R. King. Textural features corresponding to tex- tural properties. Systems, Man, and Cybernetics, 19(5), 1989.\n\nComputer description of textured surfaces. R Bajcsy, IJCAI, IJCAI. Morgan Kaufmann Publishers IncR. Bajcsy. Computer description of textured surfaces. In IJCAI, IJCAI. Morgan Kaufmann Publishers Inc., 1973.\n\nAutomatic attribute discovery and characterization from noisy web data. T Berg, A Berg, J Shih, ECCVT. Berg, A. Berg, and J. Shih. Automatic attribute discovery and characterization from noisy web data. ECCV, 2010.\n\nBasic color terms: Their universality and evolution. B Berlin, P Kay, Univ of California PressB. Berlin and P. Kay. Basic color terms: Their universality and evo- lution. Univ of California Press, 1991.\n\nThe texture lexicon: Understanding the categorization of visual texture terms and their relationship to texture images. N Bhushan, A Rao, G Lohse, Cognitive Science. 212N. Bhushan, A. Rao, and G. Lohse. The texture lexicon: Understand- ing the categorization of visual texture terms and their relationship to texture images. Cognitive Science, 21(2):219-246, 1997.\n\nDescribing people: A poseletbased approach to attribute classification. L Bourdev, S Maji, J Malik, ICCV. L. Bourdev, S. Maji, and J. Malik. Describing people: A poselet- based approach to attribute classification. In ICCV, 2011.\n\nClass-specific material categorisation. B Caputo, E Hayman, P Mallikarjuna, ICCV. B. Caputo, E. Hayman, and P. Mallikarjuna. Class-specific material categorisation. In ICCV, 2005.\n\nVisual categorization with bags of keypoints. G Csurka, C R Dance, L Dan, J Willamowski, C Bray, Proc. ECCV Workshop on Stat. Learn. in Comp. Vision. ECCV Workshop on Stat. Learn. in Comp. VisionG. Csurka, C. R. Dance, L. Dan, J. Willamowski, and C. Bray. Visual categorization with bags of keypoints. In Proc. ECCV Workshop on Stat. Learn. in Comp. Vision, 2004.\n\nReflectance and texture of real world surfaces. K J Dana, B Van Ginneken, S K Nayar, J J Koenderink, ACM Transactions on Graphics. 181K. J. Dana, B. van Ginneken, S. K. Nayar, and J. J. Koenderink. Reflectance and texture of real world surfaces. ACM Transactions on Graphics, 18(1):1-34, 1999.\n\nIma-geNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Ima- geNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009.\n\nDescribing objects by their attributes. A Farhadi, I Endres, D Hoiem, D Forsyth, CVPR. IEEEA. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In CVPR, pages 1778-1785. IEEE, 2009.\n\nLearning visual attributes. V Ferrari, A Zisserman, NIPS. V. Ferrari and A. Zisserman. Learning visual attributes. In NIPS, 2007.\n\nFast anisotropic gauss filtering. J M Geusebroek, A W M Smeulders, J Van De Weijer, IEEE Transactions on Image Processing. 128J. M. Geusebroek, A. W. M. Smeulders, and J. van de Weijer. Fast anisotropic gauss filtering. IEEE Transactions on Image Processing, 12(8):938-943, 2003.\n\nOn the significance of real-world conditions for material classification. E Hayman, B Caputo, M Fritz, J.-O Eklundh, ECCVE. Hayman, B. Caputo, M. Fritz, and J.-O. Eklundh. On the signif- icance of real-world conditions for material classification. ECCV, 2004.\n\nAggregating local descriptors into a compact image representation. H J\u00e9gou, M Douze, C Schmid, P P\u00e9rez, Proc. CVPR. CVPRH. J\u00e9gou, M. Douze, C. Schmid, and P. P\u00e9rez. Aggregating local descriptors into a compact image representation. In Proc. CVPR, 2010.\n\nTextons, the elements of texture perception, and their interactions. B Julesz, Nature. 2905802B. Julesz. Textons, the elements of texture perception, and their in- teractions. Nature, 290(5802):91-97, march 1981.\n\nDescribable visual attributes for face verification and image search. N Kumar, A Berg, P Belhumeur, S Nayar, PAMI. 3310N. Kumar, A. Berg, P. Belhumeur, and S. Nayar. Describable visual attributes for face verification and image search. PAMI, 33(10):1962- 1977, 2011.\n\nA sparse texture representation using local affine regions. S Lazebnik, C Schmid, J Ponce, PAMI28S. Lazebnik, C. Schmid, and J. Ponce. A sparse texture representa- tion using local affine regions. PAMI, 28(8):2169-2178, 2005.\n\nRepresenting and recognizing the visual appearance of materials using three-dimensional textons. T Leung, J Malik, International Journal of Computer Vision. 431T. Leung and J. Malik. Representing and recognizing the visual ap- pearance of materials using three-dimensional textons. International Journal of Computer Vision, 43(1):29-44, 2001.\n\nObject recognition from local scale-invariant features. D G Lowe, Proc. ICCV. ICCVD. G. Lowe. Object recognition from local scale-invariant features. In Proc. ICCV, 1999.\n\nPreattentive texture discrimination with early vision mechanisms. J Malik, P Perona, JOSA A. 75J. Malik and P. Perona. Preattentive texture discrimination with early vision mechanisms. JOSA A, 7(5), 1990.\n\nEnriching texture analysis with semantic data. T Matthews, M S Nixon, M Niranjan, CVPR. T. Matthews, M. S. Nixon, and M. Niranjan. Enriching texture anal- ysis with semantic data. In CVPR, June 2013.\n\nMultiresolution gray-scale and rotation invariant texture classification with local binary patterns. T Ojala, M Pietikainen, T Maenpaa, PAMI24T. Ojala, M. Pietikainen, and T. Maenpaa. Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. PAMI, 24(7):971-987, 2002.\n\nThe scale of geometric texture. G Oxholm, P Bariya, K Nishino, European Conference on Computer Vision. G. Oxholm, P. Bariya, and K. Nishino. The scale of geometric tex- ture. In European Conference on Computer Vision, pages 58-71.\n\nSun attribute database: Discovering, annotating, and recognizing scene attributes. G Patterson, J Hays, CVPR. G. Patterson and J. Hays. Sun attribute database: Discovering, anno- tating, and recognizing scene attributes. In CVPR, 2012.\n\nFisher kernels on visual vocabularies for image categorization. F Perronnin, C R Dance, CVPR. F. Perronnin and C. R. Dance. Fisher kernels on visual vocabularies for image categorization. In CVPR, 2007.\n\nImproving the Fisher kernel for large-scale image classification. F Perronnin, J S\u00e1nchez, T Mensink, Proc. ECCV. ECCVF. Perronnin, J. S\u00e1nchez, and T. Mensink. Improving the Fisher ker- nel for large-scale image classification. In Proc. ECCV, 2010.\n\nProbabilistic outputs for support vector machines and comparisons to regularized likelihood methods. J C Platt, Advances in Large Margin Classifiers. Cambridge. A. Smola, P. Bartlett, B. Sch\u00f6lkopf, and D. SchuurmansJ. C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In A. Smola, P. Bartlett, B. Sch\u00f6lkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers. Cambridge, 2000.\n\nRecognizing materials using perceptually inspired features. L Sharan, C Liu, R Rosenholtz, E H Adelson, International Journal of Computer Vision. 1033L. Sharan, C. Liu, R. Rosenholtz, and E. H. Adelson. Recognizing materials using perceptually inspired features. International Journal of Computer Vision, 103(3):348-371, 2013.\n\nMaterial perceprion: What can you see in a brief glance. L Sharan, R Rosenholtz, E H Adelson, Journal of Vision. 98784L. Sharan, R. Rosenholtz, and E. H. Adelson. Material perceprion: What can you see in a brief glance? Journal of Vision, 9:784(8), 2009.\n\nLocal higher-order statistics (lhs) for texture categorization and facial analysis. G Sharma, S Hussain, F Jurie, Proc. ECCV. ECCVG. Sharma, S. ul Hussain, and F. Jurie. Local higher-order statistics (lhs) for texture categorization and facial analysis. In Proc. ECCV. 2012.\n\nRotation, scaling and deformation invariant scattering for texture discrimination. L Sifre, S Mallat, CVPR. L. Sifre and S. Mallat. Rotation, scaling and deformation invariant scattering for texture discrimination. In CVPR, June 2013.\n\nTextural features corresponding to visual perception. Systems, Man and Cybernetics. H Tamura, S Mori, T Yamawaki, IEEE Transactions on. 86H. Tamura, S. Mori, and T. Yamawaki. Textural features correspond- ing to visual perception. Systems, Man and Cybernetics, IEEE Trans- actions on, 8(6):460 -473, june 1978.\n\nA training-free classification framework for textures, writers, and materials. R Timofte, L Van Gool, BMVC. R. Timofte and L. Van Gool. A training-free classification framework for textures, writers, and materials. In BMVC, Sept. 2012.\n\nTexture classification: Are filter banks necessary? In CVPR. M Varma, A Zisserman, IEEE2691M. Varma and A. Zisserman. Texture classification: Are filter banks necessary? In CVPR, volume 2, pages II-691. IEEE, 2003.\n\nA statistical approach to texture classification from single images. M Varma, A Zisserman, IJCV. 621M. Varma and A. Zisserman. A statistical approach to texture classi- fication from single images. IJCV, 62(1):61-81, 2005.\n\nEfficient additive kernels via explicit feature maps. A Vedaldi, A Zisserman, CVPR. A. Vedaldi and A. Zisserman. Efficient additive kernels via explicit feature maps. In CVPR, 2010.\n\nOnline crowdsourcing: rating annotators and obtaining cost-effective labels. P Welinder, P Perona, CVPR. P. Welinder and P. Perona. Online crowdsourcing: rating annotators and obtaining cost-effective labels. In CVPR, 2010.\n\nViewpoint invariant texture description using fractal analysis. Y Xu, H Ji, C Fermuller, IJCV. 831Y. Xu, H. Ji, and C. Fermuller. Viewpoint invariant texture descrip- tion using fractal analysis. IJCV, 83(1):85-100, jun 2009.\n", "annotations": {"author": "[{\"end\":106,\"start\":35},{\"end\":162,\"start\":107},{\"end\":231,\"start\":163},{\"end\":271,\"start\":232},{\"end\":344,\"start\":272}]", "publisher": null, "author_last_name": "[{\"end\":48,\"start\":42},{\"end\":121,\"start\":117},{\"end\":179,\"start\":171},{\"end\":245,\"start\":238},{\"end\":286,\"start\":279}]", "author_first_name": "[{\"end\":41,\"start\":35},{\"end\":116,\"start\":107},{\"end\":170,\"start\":163},{\"end\":237,\"start\":232},{\"end\":278,\"start\":272}]", "author_affiliation": "[{\"end\":105,\"start\":50},{\"end\":161,\"start\":123},{\"end\":230,\"start\":181},{\"end\":270,\"start\":247},{\"end\":343,\"start\":288}]", "title": "[{\"end\":32,\"start\":1},{\"end\":376,\"start\":345}]", "venue": null, "abstract": "[{\"end\":1614,\"start\":378}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1710,\"start\":1707},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1713,\"start\":1710},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1716,\"start\":1713},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1719,\"start\":1716},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3223,\"start\":3219},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3260,\"start\":3257},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3262,\"start\":3260},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3265,\"start\":3262},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3392,\"start\":3389},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3395,\"start\":3392},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3587,\"start\":3584},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4382,\"start\":4378},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4425,\"start\":4421},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5103,\"start\":5100},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5114,\"start\":5110},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5124,\"start\":5120},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5136,\"start\":5132},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5166,\"start\":5162},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5184,\"start\":5181},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5187,\"start\":5184},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5426,\"start\":5422},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5429,\"start\":5426},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5432,\"start\":5429},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5435,\"start\":5432},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5672,\"start\":5668},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10360,\"start\":10356},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10414,\"start\":10410},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11576,\"start\":11573},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11709,\"start\":11706},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14507,\"start\":14503},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15802,\"start\":15798},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16486,\"start\":16482},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18184,\"start\":18180},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18338,\"start\":18334},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18375,\"start\":18371},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19436,\"start\":19432},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19539,\"start\":19535},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19784,\"start\":19780},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19888,\"start\":19884},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19891,\"start\":19888},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19894,\"start\":19891},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19925,\"start\":19921},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19943,\"start\":19939},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20034,\"start\":20030},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20099,\"start\":20095},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20245,\"start\":20241},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20283,\"start\":20279},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20347,\"start\":20343},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22178,\"start\":22174},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22204,\"start\":22200},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22207,\"start\":22204},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22267,\"start\":22263},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22312,\"start\":22308},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22493,\"start\":22489},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22775,\"start\":22771},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22849,\"start\":22846},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22997,\"start\":22993},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23067,\"start\":23063},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23632,\"start\":23628},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24965,\"start\":24961},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25494,\"start\":25490},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25625,\"start\":25621},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25838,\"start\":25835},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25848,\"start\":25844},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25859,\"start\":25855},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25874,\"start\":25870},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25898,\"start\":25895},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25912,\"start\":25908},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25969,\"start\":25965},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25972,\"start\":25969},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25975,\"start\":25972},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26457,\"start\":26453},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27132,\"start\":27128},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27663,\"start\":27659},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28230,\"start\":28226},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28386,\"start\":28382},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31573,\"start\":31569}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33205,\"start\":33005},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33935,\"start\":33206},{\"attributes\":{\"id\":\"fig_3\"},\"end\":34118,\"start\":33936},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34501,\"start\":34119},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34604,\"start\":34502},{\"attributes\":{\"id\":\"fig_7\"},\"end\":34681,\"start\":34605},{\"attributes\":{\"id\":\"fig_9\"},\"end\":34753,\"start\":34682},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35310,\"start\":34754},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":40378,\"start\":35311},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":46439,\"start\":40379}]", "paragraph": "[{\"end\":2539,\"start\":1630},{\"end\":3102,\"start\":2541},{\"end\":3948,\"start\":3104},{\"end\":4787,\"start\":3950},{\"end\":6057,\"start\":4789},{\"end\":7325,\"start\":6059},{\"end\":7708,\"start\":7361},{\"end\":8767,\"start\":7710},{\"end\":9753,\"start\":8769},{\"end\":10157,\"start\":9755},{\"end\":10923,\"start\":10159},{\"end\":11233,\"start\":10957},{\"end\":12334,\"start\":11235},{\"end\":13300,\"start\":12336},{\"end\":13834,\"start\":13302},{\"end\":14553,\"start\":13836},{\"end\":14858,\"start\":14650},{\"end\":15315,\"start\":14923},{\"end\":15746,\"start\":15317},{\"end\":17221,\"start\":15748},{\"end\":17941,\"start\":17249},{\"end\":18277,\"start\":17969},{\"end\":18795,\"start\":18279},{\"end\":18997,\"start\":18897},{\"end\":19304,\"start\":19061},{\"end\":20145,\"start\":19306},{\"end\":20575,\"start\":20147},{\"end\":21486,\"start\":20622},{\"end\":21838,\"start\":21541},{\"end\":22313,\"start\":21840},{\"end\":22634,\"start\":22315},{\"end\":23441,\"start\":22636},{\"end\":24015,\"start\":23601},{\"end\":25702,\"start\":24017},{\"end\":26485,\"start\":25704},{\"end\":26932,\"start\":26532},{\"end\":27465,\"start\":26934},{\"end\":28176,\"start\":27467},{\"end\":29172,\"start\":28178},{\"end\":29414,\"start\":29201},{\"end\":31014,\"start\":29485},{\"end\":32114,\"start\":31039},{\"end\":33004,\"start\":32126}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14649,\"start\":14554},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14922,\"start\":14859},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18896,\"start\":18796},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19060,\"start\":18998},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23600,\"start\":23442}]", "table_ref": "[{\"end\":22546,\"start\":22539},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29543,\"start\":29536}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1628,\"start\":1616},{\"attributes\":{\"n\":\"2.\"},\"end\":7359,\"start\":7328},{\"attributes\":{\"n\":\"2.1.\"},\"end\":10955,\"start\":10926},{\"attributes\":{\"n\":\"3.\"},\"end\":17247,\"start\":17224},{\"attributes\":{\"n\":\"3.1.\"},\"end\":17967,\"start\":17944},{\"attributes\":{\"n\":\"3.2.\"},\"end\":20620,\"start\":20578},{\"attributes\":{\"n\":\"4.\"},\"end\":21500,\"start\":21489},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21539,\"start\":21503},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26530,\"start\":26488},{\"attributes\":{\"n\":\"4.3.\"},\"end\":29199,\"start\":29175},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":29483,\"start\":29417},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":31037,\"start\":31017},{\"attributes\":{\"n\":\"5.\"},\"end\":32124,\"start\":32117},{\"end\":33016,\"start\":33006},{\"end\":33217,\"start\":33207},{\"end\":33947,\"start\":33937},{\"end\":34130,\"start\":34120},{\"end\":34513,\"start\":34503},{\"end\":34617,\"start\":34606},{\"end\":34694,\"start\":34683},{\"end\":34764,\"start\":34755}]", "table": "[{\"end\":35310,\"start\":35007},{\"end\":40378,\"start\":35725},{\"end\":46439,\"start\":40452}]", "figure_caption": "[{\"end\":33205,\"start\":33018},{\"end\":33935,\"start\":33219},{\"end\":34118,\"start\":33949},{\"end\":34501,\"start\":34132},{\"end\":34604,\"start\":34515},{\"end\":34681,\"start\":34620},{\"end\":34753,\"start\":34697},{\"end\":35007,\"start\":34766},{\"end\":35725,\"start\":35313},{\"end\":40452,\"start\":40381}]", "figure_ref": "[{\"end\":2201,\"start\":2193},{\"end\":2776,\"start\":2770},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7629,\"start\":7623},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8234,\"start\":8228},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12333,\"start\":12327},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13673,\"start\":13665},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14098,\"start\":14092},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15004,\"start\":14998},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24773,\"start\":24767},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28936,\"start\":28930},{\"end\":29512,\"start\":29506},{\"end\":30186,\"start\":30180},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30695,\"start\":30689},{\"end\":30878,\"start\":30871},{\"end\":31613,\"start\":31606},{\"end\":31622,\"start\":31614},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31712,\"start\":31705}]", "bib_author_first_name": "[{\"end\":46442,\"start\":46441},{\"end\":46454,\"start\":46453},{\"end\":46722,\"start\":46721},{\"end\":46959,\"start\":46958},{\"end\":46967,\"start\":46966},{\"end\":46975,\"start\":46974},{\"end\":47156,\"start\":47155},{\"end\":47166,\"start\":47165},{\"end\":47427,\"start\":47426},{\"end\":47438,\"start\":47437},{\"end\":47445,\"start\":47444},{\"end\":47745,\"start\":47744},{\"end\":47756,\"start\":47755},{\"end\":47764,\"start\":47763},{\"end\":47944,\"start\":47943},{\"end\":47954,\"start\":47953},{\"end\":47964,\"start\":47963},{\"end\":48131,\"start\":48130},{\"end\":48141,\"start\":48140},{\"end\":48143,\"start\":48142},{\"end\":48152,\"start\":48151},{\"end\":48159,\"start\":48158},{\"end\":48174,\"start\":48173},{\"end\":48498,\"start\":48497},{\"end\":48500,\"start\":48499},{\"end\":48508,\"start\":48507},{\"end\":48524,\"start\":48523},{\"end\":48526,\"start\":48525},{\"end\":48535,\"start\":48534},{\"end\":48537,\"start\":48536},{\"end\":48799,\"start\":48798},{\"end\":48807,\"start\":48806},{\"end\":48815,\"start\":48814},{\"end\":48828,\"start\":48824},{\"end\":48834,\"start\":48833},{\"end\":48840,\"start\":48839},{\"end\":49030,\"start\":49029},{\"end\":49041,\"start\":49040},{\"end\":49051,\"start\":49050},{\"end\":49060,\"start\":49059},{\"end\":49237,\"start\":49236},{\"end\":49248,\"start\":49247},{\"end\":49374,\"start\":49373},{\"end\":49376,\"start\":49375},{\"end\":49390,\"start\":49389},{\"end\":49394,\"start\":49391},{\"end\":49407,\"start\":49406},{\"end\":49695,\"start\":49694},{\"end\":49705,\"start\":49704},{\"end\":49715,\"start\":49714},{\"end\":49727,\"start\":49723},{\"end\":49949,\"start\":49948},{\"end\":49958,\"start\":49957},{\"end\":49967,\"start\":49966},{\"end\":49977,\"start\":49976},{\"end\":50205,\"start\":50204},{\"end\":50420,\"start\":50419},{\"end\":50429,\"start\":50428},{\"end\":50437,\"start\":50436},{\"end\":50450,\"start\":50449},{\"end\":50678,\"start\":50677},{\"end\":50690,\"start\":50689},{\"end\":50700,\"start\":50699},{\"end\":50942,\"start\":50941},{\"end\":50951,\"start\":50950},{\"end\":51245,\"start\":51244},{\"end\":51247,\"start\":51246},{\"end\":51427,\"start\":51426},{\"end\":51436,\"start\":51435},{\"end\":51614,\"start\":51613},{\"end\":51626,\"start\":51625},{\"end\":51628,\"start\":51627},{\"end\":51637,\"start\":51636},{\"end\":51869,\"start\":51868},{\"end\":51878,\"start\":51877},{\"end\":51893,\"start\":51892},{\"end\":52113,\"start\":52112},{\"end\":52123,\"start\":52122},{\"end\":52133,\"start\":52132},{\"end\":52396,\"start\":52395},{\"end\":52409,\"start\":52408},{\"end\":52614,\"start\":52613},{\"end\":52627,\"start\":52626},{\"end\":52629,\"start\":52628},{\"end\":52820,\"start\":52819},{\"end\":52833,\"start\":52832},{\"end\":52844,\"start\":52843},{\"end\":53104,\"start\":53103},{\"end\":53106,\"start\":53105},{\"end\":53516,\"start\":53515},{\"end\":53526,\"start\":53525},{\"end\":53533,\"start\":53532},{\"end\":53547,\"start\":53546},{\"end\":53549,\"start\":53548},{\"end\":53841,\"start\":53840},{\"end\":53851,\"start\":53850},{\"end\":53865,\"start\":53864},{\"end\":53867,\"start\":53866},{\"end\":54124,\"start\":54123},{\"end\":54134,\"start\":54133},{\"end\":54145,\"start\":54144},{\"end\":54399,\"start\":54398},{\"end\":54408,\"start\":54407},{\"end\":54636,\"start\":54635},{\"end\":54646,\"start\":54645},{\"end\":54654,\"start\":54653},{\"end\":54943,\"start\":54942},{\"end\":54954,\"start\":54953},{\"end\":55162,\"start\":55161},{\"end\":55171,\"start\":55170},{\"end\":55386,\"start\":55385},{\"end\":55395,\"start\":55394},{\"end\":55595,\"start\":55594},{\"end\":55606,\"start\":55605},{\"end\":55801,\"start\":55800},{\"end\":55813,\"start\":55812},{\"end\":56013,\"start\":56012},{\"end\":56019,\"start\":56018},{\"end\":56025,\"start\":56024}]", "bib_author_last_name": "[{\"end\":46451,\"start\":46443},{\"end\":46459,\"start\":46455},{\"end\":46729,\"start\":46723},{\"end\":46964,\"start\":46960},{\"end\":46972,\"start\":46968},{\"end\":46980,\"start\":46976},{\"end\":47163,\"start\":47157},{\"end\":47170,\"start\":47167},{\"end\":47435,\"start\":47428},{\"end\":47442,\"start\":47439},{\"end\":47451,\"start\":47446},{\"end\":47753,\"start\":47746},{\"end\":47761,\"start\":47757},{\"end\":47770,\"start\":47765},{\"end\":47951,\"start\":47945},{\"end\":47961,\"start\":47955},{\"end\":47977,\"start\":47965},{\"end\":48138,\"start\":48132},{\"end\":48149,\"start\":48144},{\"end\":48156,\"start\":48153},{\"end\":48171,\"start\":48160},{\"end\":48179,\"start\":48175},{\"end\":48505,\"start\":48501},{\"end\":48521,\"start\":48509},{\"end\":48532,\"start\":48527},{\"end\":48548,\"start\":48538},{\"end\":48804,\"start\":48800},{\"end\":48812,\"start\":48808},{\"end\":48822,\"start\":48816},{\"end\":48831,\"start\":48829},{\"end\":48837,\"start\":48835},{\"end\":48848,\"start\":48841},{\"end\":49038,\"start\":49031},{\"end\":49048,\"start\":49042},{\"end\":49057,\"start\":49052},{\"end\":49068,\"start\":49061},{\"end\":49245,\"start\":49238},{\"end\":49258,\"start\":49249},{\"end\":49387,\"start\":49377},{\"end\":49404,\"start\":49395},{\"end\":49421,\"start\":49408},{\"end\":49702,\"start\":49696},{\"end\":49712,\"start\":49706},{\"end\":49721,\"start\":49716},{\"end\":49735,\"start\":49728},{\"end\":49955,\"start\":49950},{\"end\":49964,\"start\":49959},{\"end\":49974,\"start\":49968},{\"end\":49983,\"start\":49978},{\"end\":50212,\"start\":50206},{\"end\":50426,\"start\":50421},{\"end\":50434,\"start\":50430},{\"end\":50447,\"start\":50438},{\"end\":50456,\"start\":50451},{\"end\":50687,\"start\":50679},{\"end\":50697,\"start\":50691},{\"end\":50706,\"start\":50701},{\"end\":50948,\"start\":50943},{\"end\":50957,\"start\":50952},{\"end\":51252,\"start\":51248},{\"end\":51433,\"start\":51428},{\"end\":51443,\"start\":51437},{\"end\":51623,\"start\":51615},{\"end\":51634,\"start\":51629},{\"end\":51646,\"start\":51638},{\"end\":51875,\"start\":51870},{\"end\":51890,\"start\":51879},{\"end\":51901,\"start\":51894},{\"end\":52120,\"start\":52114},{\"end\":52130,\"start\":52124},{\"end\":52141,\"start\":52134},{\"end\":52406,\"start\":52397},{\"end\":52414,\"start\":52410},{\"end\":52624,\"start\":52615},{\"end\":52635,\"start\":52630},{\"end\":52830,\"start\":52821},{\"end\":52841,\"start\":52834},{\"end\":52852,\"start\":52845},{\"end\":53112,\"start\":53107},{\"end\":53523,\"start\":53517},{\"end\":53530,\"start\":53527},{\"end\":53544,\"start\":53534},{\"end\":53557,\"start\":53550},{\"end\":53848,\"start\":53842},{\"end\":53862,\"start\":53852},{\"end\":53875,\"start\":53868},{\"end\":54131,\"start\":54125},{\"end\":54142,\"start\":54135},{\"end\":54151,\"start\":54146},{\"end\":54405,\"start\":54400},{\"end\":54415,\"start\":54409},{\"end\":54643,\"start\":54637},{\"end\":54651,\"start\":54647},{\"end\":54663,\"start\":54655},{\"end\":54951,\"start\":54944},{\"end\":54963,\"start\":54955},{\"end\":55168,\"start\":55163},{\"end\":55181,\"start\":55172},{\"end\":55392,\"start\":55387},{\"end\":55405,\"start\":55396},{\"end\":55603,\"start\":55596},{\"end\":55616,\"start\":55607},{\"end\":55810,\"start\":55802},{\"end\":55820,\"start\":55814},{\"end\":56016,\"start\":56014},{\"end\":56022,\"start\":56020},{\"end\":56035,\"start\":56026}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":46676,\"start\":46441},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":6771929},\"end\":46884,\"start\":46678},{\"attributes\":{\"id\":\"b2\"},\"end\":47100,\"start\":46886},{\"attributes\":{\"id\":\"b3\"},\"end\":47304,\"start\":47102},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14098273},\"end\":47670,\"start\":47306},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6980765},\"end\":47901,\"start\":47672},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52865372},\"end\":48082,\"start\":47903},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":17606900},\"end\":48447,\"start\":48084},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":622815},\"end\":48742,\"start\":48449},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":48987,\"start\":48744},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14940757},\"end\":49206,\"start\":48989},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":10004927},\"end\":49337,\"start\":49208},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2955559},\"end\":49618,\"start\":49339},{\"attributes\":{\"id\":\"b13\"},\"end\":49879,\"start\":49620},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1912782},\"end\":50133,\"start\":49881},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4327694},\"end\":50347,\"start\":50135},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1151924},\"end\":50615,\"start\":50349},{\"attributes\":{\"id\":\"b17\"},\"end\":50842,\"start\":50617},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14915716},\"end\":51186,\"start\":50844},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5258236},\"end\":51358,\"start\":51188},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5601682},\"end\":51564,\"start\":51360},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":17048799},\"end\":51765,\"start\":51566},{\"attributes\":{\"id\":\"b22\"},\"end\":52078,\"start\":51767},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1359086},\"end\":52310,\"start\":52080},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":8724974},\"end\":52547,\"start\":52312},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12795415},\"end\":52751,\"start\":52549},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10402702},\"end\":53000,\"start\":52753},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":56563878},\"end\":53453,\"start\":53002},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1183157},\"end\":53781,\"start\":53455},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":144931877},\"end\":54037,\"start\":53783},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":11468359},\"end\":54313,\"start\":54039},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":2942296},\"end\":54549,\"start\":54315},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":32197839},\"end\":54861,\"start\":54551},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1354722},\"end\":55098,\"start\":54863},{\"attributes\":{\"id\":\"b34\"},\"end\":55314,\"start\":55100},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2313314},\"end\":55538,\"start\":55316},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1440386},\"end\":55721,\"start\":55540},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14668087},\"end\":55946,\"start\":55723},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":17249609},\"end\":56173,\"start\":55948}]", "bib_title": "[{\"end\":46719,\"start\":46678},{\"end\":47424,\"start\":47306},{\"end\":47742,\"start\":47672},{\"end\":47941,\"start\":47903},{\"end\":48128,\"start\":48084},{\"end\":48495,\"start\":48449},{\"end\":48796,\"start\":48744},{\"end\":49027,\"start\":48989},{\"end\":49234,\"start\":49208},{\"end\":49371,\"start\":49339},{\"end\":49946,\"start\":49881},{\"end\":50202,\"start\":50135},{\"end\":50417,\"start\":50349},{\"end\":50939,\"start\":50844},{\"end\":51242,\"start\":51188},{\"end\":51424,\"start\":51360},{\"end\":51611,\"start\":51566},{\"end\":52110,\"start\":52080},{\"end\":52393,\"start\":52312},{\"end\":52611,\"start\":52549},{\"end\":52817,\"start\":52753},{\"end\":53101,\"start\":53002},{\"end\":53513,\"start\":53455},{\"end\":53838,\"start\":53783},{\"end\":54121,\"start\":54039},{\"end\":54396,\"start\":54315},{\"end\":54633,\"start\":54551},{\"end\":54940,\"start\":54863},{\"end\":55383,\"start\":55316},{\"end\":55592,\"start\":55540},{\"end\":55798,\"start\":55723},{\"end\":56010,\"start\":55948}]", "bib_author": "[{\"end\":46453,\"start\":46441},{\"end\":46461,\"start\":46453},{\"end\":46731,\"start\":46721},{\"end\":46966,\"start\":46958},{\"end\":46974,\"start\":46966},{\"end\":46982,\"start\":46974},{\"end\":47165,\"start\":47155},{\"end\":47172,\"start\":47165},{\"end\":47437,\"start\":47426},{\"end\":47444,\"start\":47437},{\"end\":47453,\"start\":47444},{\"end\":47755,\"start\":47744},{\"end\":47763,\"start\":47755},{\"end\":47772,\"start\":47763},{\"end\":47953,\"start\":47943},{\"end\":47963,\"start\":47953},{\"end\":47979,\"start\":47963},{\"end\":48140,\"start\":48130},{\"end\":48151,\"start\":48140},{\"end\":48158,\"start\":48151},{\"end\":48173,\"start\":48158},{\"end\":48181,\"start\":48173},{\"end\":48507,\"start\":48497},{\"end\":48523,\"start\":48507},{\"end\":48534,\"start\":48523},{\"end\":48550,\"start\":48534},{\"end\":48806,\"start\":48798},{\"end\":48814,\"start\":48806},{\"end\":48824,\"start\":48814},{\"end\":48833,\"start\":48824},{\"end\":48839,\"start\":48833},{\"end\":48850,\"start\":48839},{\"end\":49040,\"start\":49029},{\"end\":49050,\"start\":49040},{\"end\":49059,\"start\":49050},{\"end\":49070,\"start\":49059},{\"end\":49247,\"start\":49236},{\"end\":49260,\"start\":49247},{\"end\":49389,\"start\":49373},{\"end\":49406,\"start\":49389},{\"end\":49423,\"start\":49406},{\"end\":49704,\"start\":49694},{\"end\":49714,\"start\":49704},{\"end\":49723,\"start\":49714},{\"end\":49737,\"start\":49723},{\"end\":49957,\"start\":49948},{\"end\":49966,\"start\":49957},{\"end\":49976,\"start\":49966},{\"end\":49985,\"start\":49976},{\"end\":50214,\"start\":50204},{\"end\":50428,\"start\":50419},{\"end\":50436,\"start\":50428},{\"end\":50449,\"start\":50436},{\"end\":50458,\"start\":50449},{\"end\":50689,\"start\":50677},{\"end\":50699,\"start\":50689},{\"end\":50708,\"start\":50699},{\"end\":50950,\"start\":50941},{\"end\":50959,\"start\":50950},{\"end\":51254,\"start\":51244},{\"end\":51435,\"start\":51426},{\"end\":51445,\"start\":51435},{\"end\":51625,\"start\":51613},{\"end\":51636,\"start\":51625},{\"end\":51648,\"start\":51636},{\"end\":51877,\"start\":51868},{\"end\":51892,\"start\":51877},{\"end\":51903,\"start\":51892},{\"end\":52122,\"start\":52112},{\"end\":52132,\"start\":52122},{\"end\":52143,\"start\":52132},{\"end\":52408,\"start\":52395},{\"end\":52416,\"start\":52408},{\"end\":52626,\"start\":52613},{\"end\":52637,\"start\":52626},{\"end\":52832,\"start\":52819},{\"end\":52843,\"start\":52832},{\"end\":52854,\"start\":52843},{\"end\":53114,\"start\":53103},{\"end\":53525,\"start\":53515},{\"end\":53532,\"start\":53525},{\"end\":53546,\"start\":53532},{\"end\":53559,\"start\":53546},{\"end\":53850,\"start\":53840},{\"end\":53864,\"start\":53850},{\"end\":53877,\"start\":53864},{\"end\":54133,\"start\":54123},{\"end\":54144,\"start\":54133},{\"end\":54153,\"start\":54144},{\"end\":54407,\"start\":54398},{\"end\":54417,\"start\":54407},{\"end\":54645,\"start\":54635},{\"end\":54653,\"start\":54645},{\"end\":54665,\"start\":54653},{\"end\":54953,\"start\":54942},{\"end\":54965,\"start\":54953},{\"end\":55170,\"start\":55161},{\"end\":55183,\"start\":55170},{\"end\":55394,\"start\":55385},{\"end\":55407,\"start\":55394},{\"end\":55605,\"start\":55594},{\"end\":55618,\"start\":55605},{\"end\":55812,\"start\":55800},{\"end\":55822,\"start\":55812},{\"end\":56018,\"start\":56012},{\"end\":56024,\"start\":56018},{\"end\":56037,\"start\":56024}]", "bib_venue": "[{\"end\":48279,\"start\":48234},{\"end\":50001,\"start\":49997},{\"end\":51270,\"start\":51266},{\"end\":52870,\"start\":52866},{\"end\":54169,\"start\":54165},{\"end\":46546,\"start\":46461},{\"end\":46743,\"start\":46731},{\"end\":46956,\"start\":46886},{\"end\":47153,\"start\":47102},{\"end\":47470,\"start\":47453},{\"end\":47776,\"start\":47772},{\"end\":47983,\"start\":47979},{\"end\":48232,\"start\":48181},{\"end\":48578,\"start\":48550},{\"end\":48854,\"start\":48850},{\"end\":49074,\"start\":49070},{\"end\":49264,\"start\":49260},{\"end\":49460,\"start\":49423},{\"end\":49692,\"start\":49620},{\"end\":49995,\"start\":49985},{\"end\":50220,\"start\":50214},{\"end\":50462,\"start\":50458},{\"end\":50675,\"start\":50617},{\"end\":50999,\"start\":50959},{\"end\":51264,\"start\":51254},{\"end\":51451,\"start\":51445},{\"end\":51652,\"start\":51648},{\"end\":51866,\"start\":51767},{\"end\":52181,\"start\":52143},{\"end\":52420,\"start\":52416},{\"end\":52641,\"start\":52637},{\"end\":52864,\"start\":52854},{\"end\":53161,\"start\":53114},{\"end\":53599,\"start\":53559},{\"end\":53894,\"start\":53877},{\"end\":54163,\"start\":54153},{\"end\":54421,\"start\":54417},{\"end\":54685,\"start\":54665},{\"end\":54969,\"start\":54965},{\"end\":55159,\"start\":55100},{\"end\":55411,\"start\":55407},{\"end\":55622,\"start\":55618},{\"end\":55826,\"start\":55822},{\"end\":56041,\"start\":56037}]"}}}, "year": 2023, "month": 12, "day": 17}