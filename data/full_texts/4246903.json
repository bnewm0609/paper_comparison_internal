{"id": 4246903, "updated": "2023-12-16 18:24:23.443", "metadata": {"title": "The Pascal Visual Object Classes (VOC) Challenge", "authors": "[{\"first\":\"Mark\",\"last\":\"Everingham\",\"middle\":[]},{\"first\":\"Luc\",\"last\":\"Gool\",\"middle\":[\"Van\"]},{\"first\":\"Christopher\",\"last\":\"Williams\",\"middle\":[\"K.\",\"I.\"]},{\"first\":\"John\",\"last\":\"Winn\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Zisserman\",\"middle\":[]}]", "venue": "International Journal of Computer Vision", "journal": "International Journal of Computer Vision", "publication_date": {"year": 2009, "month": null, "day": null}, "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2031489346", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ijcv/EveringhamGWWZ10", "doi": "10.1007/s11263-009-0275-4"}}, "content": {"source": {"pdf_hash": "d43b405db643dd68a991b722f9bc1fde5dda12c6", "pdf_src": "MergedPDFExtraction", "pdf_uri": "[\"https://www.pure.ed.ac.uk/ws/files/7879113/ijcv_voc09.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://www.pure.ed.ac.uk/ws/files/7879113/ijcv_voc09.pdf", "status": "GREEN"}}, "grobid": {"id": "ce348ef6cb3875db2a7ef169ab3b678d6d1648a4", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/d43b405db643dd68a991b722f9bc1fde5dda12c6.txt", "contents": "\nThe PASCAL Visual Object Classes (VOC) Challenge'\n\n\nM Everingham \nL Van Gool \nWilliams \nCki \nJ Winn \n&amp; Zisserman \n\nUniversity of Leeds\nUK\n\n\nUniversity of Edinburgh\nUK\n\n\nMicrosoft Research\nCambridgeUK\n\n\nUniversity of Oxford\nUK\n\nThe PASCAL Visual Object Classes (VOC) Challenge'\n\nInternational Journal of Computer Vision General\n88210.1007/s11263-009-0275-4Download date: 20. Sep. 2020 Received: date / Accepted: dateCitation for published version: Digital Object Identifier (DOI): Link: Link to publication record in Edinburgh Research Explorer Document Version: Early version, also known as pre-print Published In:Database \u00b7 Benchmark \u00b7 Object recognition \u00b7 Object detection\nThe PASCAL Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension. 1 PASCAL stands for pattern analysis, statistical modelling and computational learning. It is an EU Network of Excellence funded under the IST Programme of the European Union. 2\n\nIntroduction\n\nThe PASCAL 1 Visual Object Classes (VOC) Challenge consists of two components: (i) a publicly available dataset of images and annotation, together with standardised evaluation software; and (ii) an annual competition and workshop. The VOC2007 dataset consists of annotated consumer photographs collected from the flickr 2 photo-sharing web-site. A new dataset with ground truth annotation has been released each year since 2006. There are two principal challenges: classification -\"does the image contain any instances of a particular object class?\" (where the object classes include cars, people, dogs, etc), and detection -\"where are the instances of a particular object class in the image (if any)?\". In addition, there are two subsidiary challenges (\"tasters\") on pixel-level segmentation -assign each pixel a class label, and \"person layout\" -localise the head, hands and feet of people in the image. The challenges are issued with deadlines each year, and a workshop held to compare and discuss that year's results and methods. The datasets and associated annotation and software are subsequently released and available for use at any time.\n\nThe objectives of the VOC challenge are twofold: first to provide challenging images and high quality annotation, together with a standard evaluation methodology -a \"plug and play\" training and testing harness so that performance of algorithms can be compared (the dataset component); and second to measure the state of the art each year (the competition component).\n\nThe purpose of this paper is to describe the challenge: what it is, and the reasons for the way it is. We also describe the methods, results and evaluation of the challenge, and so 2 in that respect are describing the state-of-the-art in object recognition (at least as measured for these challenges and by those who entered). We focus mainly on the 2007 challenge, as this is the most recent, but also discuss significant changes since earlier challenges and why these were made.\n\n\nRelation to other Datasets\n\nChallenge datasets are important in many areas of research in order to set goals for methods, and to allow comparison of their performance. Similar datasets and evaluation methodologies are sprouting in other areas of computer vision and machine learning, e.g. the Middlebury datasets for stereo, MRF optimisation, and optical flow comparison (Scharstein and Szeliski 2002).\n\nIn addition to organised challenges, there are several datasets contributed by the vision community which are related to that collected for the VOC challenges.\n\nThe \"Caltech 101\" dataset (Fei-Fei et al 2006) contains images of 101 categories of object, and is relatively widely used within the community for evaluating object recognition. Each image contains only a single object. A principal aim of the Caltech datasets is to evaluate multi-category object recognition, as a function of the (relatively small) number of training images. This is complementary to the aims of the VOC challenge, which measures performance on a smaller number of classes and without such constraints on the amount of training data available.\n\nA common criticism of this dataset, addressed by the VOC challenge, is that the images are largely without clutter, variation in pose is limited, and the images have been manually aligned to reduce the variability in appearance. These factors make the dataset less applicable to \"real world\" evaluation than the images provided for the VOC challenge.\n\nThe \"Caltech 256\" dataset (Griffin et al 2007) corrected some of the deficiencies of Caltech 101 -there is more variability in size and localisation, and obvious artifacts have been removed. The number of classes is increased (from 101 to 256) and the aim is still to investigate multi-category object recognition with a limited number of training images. For the most part there is only a single object per image -as is required to support the 1-of-m evaluation adopted (\"which one of m classes does this image contain?\").\n\nThe \"LabelMe\" dataset (Russell et al 2008) at MIT is most similar to the VOC challenge dataset in that it contains more-or-less general photographs containing multiple objects. LabelMe has been ground-breaking in providing a web-based annotation interface, encouraging casual and professional users alike to contribute and share annotation.\n\nMany object categories are labelled, with annotation consisting of a bounding polygon and category, with some objects additionally being labelled with pose and object parts. For the most part the dataset is incompletely labelled -volunteers are free to choose which objects to annotate, and which to omit. This means that, while a very valuable resource for training images, the dataset is unsuitable for testing in the manner of the VOC challenge since precision and recall cannot accurately be estimated. Recently the LabelMe organisers have proposed subsets of the database to use for training and testing, which are completely annotated with a set of seven object (person, car) and \"stuff\" (building, sky, etc.) classes. However, no evaluation protocol is specified.\n\nThe \"TREC Video Retrieval Evaluation\" (TRECVID 3 , Smeaton et al (2006)) is also similar to the VOC challenge in that there is a new dataset and competition each year, though the dataset is only available to participants and is not publicly distributed. TRECVID includes several tasks, but the one most related to VOC is termed \"high-level feature extraction\", and involves returning a ranked list of video shots for specified \"features\". For the 2008 competition these features include scene categories (such as classroom, cityscape or harbour), object categories (such as dog, aeroplane flying or telephone) and actions/events (such as a demonstration/protest). Annotation is not provided by the organisers, but some is usually distributed amongst the participants. The submissions are scored by their Average Precision (AP). The evaluation of the ranked lists is carried out by the organisers using a mixture of ground truth labelling and \"inferred ground truth\" (Yilmaz and Aslam 2006) obtained from high ranked results returned by the participants' methods.\n\nThe Lotus Hill dataset (Yao et al 2007) is a large, recently produced dataset, a small part of which is made freely available to researchers. It contains 8 data subsets with a range of annotation. Particularly we highlight (a) annotations providing a hierarchical decomposition of individual objects e.g. vehicles (9 classes, 209 images 4 ), other man-made objects (75 classes, 750 images) and animals (40 classes, 400 images); and (b) segmentation labelling of scenes to a pixel level (444 images). As this dataset has only recently been released there has not yet been a lot of work reported on it. The datasets look to have a useful level of annotation (especially with regard to hierarchical decompositions which have not been attempted elsewhere), but are somewhat limited by the number of images that are freely available. 3 \n\n\nPaper layout\n\nThis paper is organised as follows: we start with a summary of the four challenges in Sect. 2, then describe in more detail in Sect. 3 the datasets -their method of collection; the classes included and the motivation for including them; and their annotation and statistics. Sect. 4 describes the evaluation procedure and why this procedure was chosen. Sect. 5 overviews the main methods used in the 2007 challenge for classification and detection, and Sect. 6 reports and discusses the results. This discussion includes an analysis of the statistical significance of the performances of the different methods, and also of which object classes and images the methods find easy or difficult. We conclude with a discussion of the merits, and otherwise, of the VOC challenge and possible options for the future.\n\n\nChallenge Tasks\n\nThis section gives an overview of the two principal challenge tasks on classification and detection, and on the two subsidiary tasks (\"tasters\") on pixel-level segmentation, and \"person layout\".\n\n\nClassification\n\nFor each of twenty object classes, predict the presence/absence of at least one object of that class in a test image. Participants are required to provide a real-valued confidence of the object's presence for each test image so that a precision/recall curve can be drawn. Participants may choose to tackle all, or any subset of object classes, for example \"cars only\" or \"motorbikes and cars\".\n\nTwo competitions are defined according to the choice of training data: (1) taken from the VOC training/validation data provided, or (2) from any source excluding the VOC test data. In the first competition, any annotation provided in the VOC training/validation data may be used for training, for example bounding boxes or particular views e.g. \"frontal\" or \"left\". Participants are not permitted to perform additional manual annotation of either training or test data. In the second competition, any source of training data may be used except the provided test images. The second competition is aimed at researchers who have pre-built systems trained on other data, and is a measure of the state-of-the-art.\n\n\nDetection\n\nFor each of the twenty classes, predict the bounding boxes of each object of that class in a test image (if any), with associated real-valued confidence. Participants may choose to tackle all, or any subset of object classes. Two competitions are defined in a similar manner to the classification challenge.\n\n\nSegmentation Taster\n\nFor each test image, predict the object class of each pixel, or \"background\" if the object does not belong to one of the twenty specified classes. Unlike the classification and detection challenges there is only one competition, where training data is restricted to that provided by the challenge.\n\n\nPerson Layout Taster\n\nFor each \"person\" object in a test image (if any), detect the person, predicting the bounding box of the person, the presence/absence of parts (head/hands/feet), and the bounding boxes of those parts. Each person detection should be output with an associated real-valued confidence. Two competitions are defined in a similar manner to the classification challenge.\n\n\nDatasets\n\nThe goal of the VOC challenge is to investigate the performance of recognition methods on a wide spectrum of natural images. To this end, it is required that the VOC datasets contain significant variability in terms of object size, orientation, pose, illumination, position and occlusion. It is also important that the datasets do not exhibit systematic bias, for example, favouring images with centred objects or good illumination. Similarly, to ensure accurate training and evaluation, it is necessary for the image annotations to be consistent, accurate and exhaustive for the specified classes. This section describes the processes used for collecting and annotating the VOC2007 datasets, which were designed to achieve these aims.\n\n\nImage Collection Procedure\n\nFor the 2007 challenge, all images were collected from the flickr photo-sharing web-site. The use of personal photos which were not taken by, or selected by, vision/machine learning researchers results in a very \"unbiased\" dataset, in the sense that the photos are not taken with a particular purpose in mind i.e. object recognition research. Qualitatively the images contain a very wide range of viewing conditions (pose, lighting, etc.) and images where there is little bias toward images being \"of\" a particular object, e.g. there are images of motorcycles in a street scene, rather than solely im- ages where a motorcycle is the focus of the picture. The annotation guidelines (Winn and Everingham 2007) provided guidance to annotators on which images to annotate -essentially everything which could be annotated with confidence. The use of a single source of \"consumer\" images addressed problems encountered in previous challenges, such as in VOC2006 where images from the Microsoft Research Cambridge database (Shotton et al 2006) were included. The MSR Cambridge images were taken with the purpose of capturing particular object classes, so that the object instances tend to be large, well-illuminated and central. The use of an automated collection method also prevented any selection bias being introduced by a researcher manually performing image selection. The \"person\" category provides a vivid example of how the adopted collection methodology leads to high variability; in previous datasets \"person\" was essentially synonymous with \"pedestrian\", whereas in the VOC dataset we have images of people engaged in a wide range of activities such as walking, riding horses, sitting on buses, etc. (see Fig. 1).\n\nIn total, 500,000 images were retrieved from flickr. For each of the 20 object classes to be annotated (see Fig. 1), images were retrieved by querying flickr with a number of related keywords (Table 1). No other query criteria, e.g. date of capture, photographer's name, etc. were specified -we return to this point below.\n\nFor a given query, flickr is asked for 100,000 matching images (flickr organises search results as \"pages\" i.e. 100 pages of 1,000 matches). An image is chosen at random from the returned set and downloaded along with the corresponding metadata. A new query is then selected at random, and the process is repeated until sufficient images have been downloaded. Images were downloaded for each class in parallel using a python interface to the flickr API, with no restriction on the number of images per class or query. Thanks to flickr's fast servers, downloading the entire image set took just a few hours on a single machine. Table 1 lists the queries used for each of the classes, produced by \"free association\" from the target classes. It might appear that the use of keyword queries would bias the images to pictures \"of\" an object, however the wide range of keywords used reduces this likelihood; for example the query \"living room\" can be expected to return scenes containing chairs, sofas, tables, etc. in context, or the query \"town centre\" to return scenes containing cars, motorcycles, pedestrians, etc. It is worth noting, however, that without using any keyword queries the images retrieved randomly from flickr were, subjectively, found to be overwhelmingly \"party\" scenes containing predominantly people. We return to the problem of obtaining sufficient examples of \"minority\" object classes in Sect. 7.1.\n\nAll exact duplicate and \"near duplicate\" images were removed from the downloaded image set, using the method Table 1 Queries used to retrieve images from flickr. Words in bold show the \"targeted\" class. Note that the query terms are quite general -including the class name, synonyms and scenes or situations where the class is likely to occur.\n\n-aeroplane, airplane, plane, biplane, monoplane, aviator, bomber, hydroplane, airliner, aircraft, fighter, airport, hangar, jet, boeing, fuselage, wing, propellor, flying -bicycle, bike, cycle, cyclist, pedal, tandem, saddle, wheel, cycling, ride, wheelie -bird, birdie, birdwatching, nest, sea, aviary, birdcage, bird feeder, bird table, -boat ship, barge, ferry, canoe, boating, craft, liner, cruise, sailing, rowing, watercraft, regatta, racing, marina, beach, water, canal, river, stream, lake, yacht, -bottle, cork, wine, beer, champagne, ketchup, squash, soda, coke, lemonade, dinner, lunch, breakfast -bus, omnibus, coach, shuttle, jitney, double-decker, motorbus, school bus, depot, terminal, station, terminus, passenger, route -car, automobile, cruiser, motorcar, vehicle, hatchback, saloon, convertible, limousine, motor, race, traffic, trip, rally, city, street, road, lane, village, town, centre, shopping, downtown, suburban -cat, feline, pussy, mew, kitten, tabby, tortoiseshell, ginger, stray -chair, seat, rocker, rocking, deck, swivel, camp, chaise, office, studio, armchair, recliner, sitting, lounge, living room, sitting room -cow, beef, heifer, moo, dairy, milk, milking, farm -dog, hound, bark, kennel, heel, bitch, canine, puppy, hunter, collar, leash -horse. gallop, jump, buck, equine, foal, cavalry, saddle, canter, buggy, mare, neigh, dressage, trial, racehorse, steeplechase, thoroughbred, cart, equestrian, paddock, stable, farrier -motorbike, motorcycle, minibike, moped, dirt, pillion, biker, trials, motorcycling, motorcyclist, engine, motocross, scramble, sidecar, scooter, trail -person, people, family, father, mother, brother, sister, aunt, uncle, grandmother, grandma, grandfather, grandpa, grandson, granddaughter, niece, nephew, cousin -sheep, ram, fold, fleece, shear, baa, bleat, lamb, ewe, wool, flock -sofa, chesterfield, settee, divan, couch, bolster -table, dining, cafe, restaurant, kitchen, banquet, party, meal -potted plant, pot plant, plant, patio, windowsill, window sill, yard, greenhouse, glass house, basket, cutting, pot, cooking, grow -train, express, locomotive, freight, commuter, platform, subway, underground, steam, railway, railroad, rail, tube, underground, track, carriage, coach, metro, sleeper, railcar, buffet, cabin, level crossing -tv, monitor, television, plasma, flatscreen, flat screen, lcd, crt, watching, dvd, desktop, computer, computer monitor, PC, console, game of (Chum et al 2007). Near duplicate images are those that are perceptually similar, but differ in their levels of compression, or by small photometric distortions or occlusions for example.\n\nAfter de-duplication, random images from the set of 500,000 were presented to the annotators for annotation. During the annotation event, 44,269 images were considered for annotation, being either annotated or discarded as unsuitable for annotation e.g. containing no instances of the 20 object classes, according to the annotation guidelines (Winn 6 and Everingham 2007), or being impossible to annotate correctly and completely with confidence.\n\nOne small bias was discovered in the VOC2007 dataset due to the image collection procedure -flickr returns query results ranked by \"recency\" such that if a given query is satisfied by many images, more recent images are returned first. Since the images were collected in January 2007, this led to an above-average number of Christmas/winter images containing, for example, large numbers of Christmas trees. To avoid such bias in VOC2008 5 , images have been retrieved using queries comprising a random date in addition to keywords.\n\n\nChoice of Classes\n\nFig. 2 shows the 20 classes selected for annotation in the VOC2007 dataset. As shown, the classes can be considered in a taxonomy with four main branches -vehicles, animals, household objects and people 6 . The figure also shows the year of the challenge in which a particular class was included. In the original VOC2005 challenge (Everingham et al 2006a), which used existing annotated datasets, four classes were annotated (car, motorbike, bicycle and person). This number was increased to 10 in VOC2006, and 20 in VOC2007.\n\nOver successive challenges the set of classes has been expanded in two ways: First, finer-grain \"sub-classes\" have been added e.g. \"bus\". The choice of sub-classes has been motivated by (i) increasing the \"semantic\" specificity of the output required of systems, for example recognising different types of vehicle e.g. car/motorbike (which may not be visually similar); (ii) increasing the difficulty of the discrimination task by inclusion of objects which might be considered visually similar e.g. \"cat\" vs. \"dog\". Second, additional branches of the notional taxonomy have been added e.g. \"animals\" (VOC2006) and \"household objects\" (VOC2007). The motivations are twofold: (i) increasing the domain of the challenge in terms of the semantic range of objects covered; (ii) encouraging research on object classes not widely addressed because of visual properties which are challenging for current methods, e.g. animals which might be considered to lack highly distinctive parts (c.f. car wheels), and chairs which are defined functionally, rather than visually, and also tend to be highly occluded in the dataset.\n\nThe choice of object classes, which can be considered a sub-tree of a taxonomy defined in terms of both semantic and visual similarity, also supports research in two areas 5 http://pascallin.ecs.soton.ac.uk/challenges/VOC/ voc2008/ 6 These branches are also found in the Caltech 256 (Griffin et al 2007) taxonomy as transportation, animal, household & everyday, and human -though the Caltech 256 taxonomy has many other branches. which show promise in solving the scaling of object recognition to many thousands of classes: (i) exploiting visual properties common to classes e.g. vehicle wheels, for example in the form of \"feature sharing\" (Torralba et al 2007); (ii) exploiting external semantic information about the relations between object classes e.g. WordNet (Fellbaum 1998), for example by learning a hierarchy of classifiers . The availability of a class hierarchy may also prove essential in future evaluation efforts if the number of classes increases to the extent that there is implicit ambiguity in the classes, allowing individual objects to be annotated at different levels of the hierarchy e.g. hatchback/car/vehicle. We return to this point in Sect. 7.3.\n\n\nAnnotated Attributes\n\nIn order to evaluate the classification and detection challenges, the image annotation includes the following attributes for every object in the target set of object classes: The choice of an axis aligned bounding-box for the annotation is a compromise: for some object classes it fits quite well (e.g. to a horizontal bus or train) with only a small proportion of non-class pixels; however, for other classes it can be a poor fit either because they are not box shaped (e.g. a person with their arms outstretched, a chair) or/and because they are not axis-aligned (e.g. an aeroplane taking off). The advantage though is that they are relatively quick to annotate. We return to this point when discussing pixel level annotation in Sect. 3.6.1.\n\nIn addition, since VOC2006, further annotations were introduced which could be used during training but which were not required for evaluation: Fig. 3 Example of the \"difficult\" annotation. Objects shown in red have been marked difficult, and are excluded from the evaluation. Note that the judgement of difficulty is not solely by object size -the distant car on the right of the image is included in the evaluation.\n\n-viewpoint: one of: front, rear, left, right, unspecified.\n\nThis annotation supports methods which treat different viewpoints differently during training, such as using separate detectors for each viewpoint. -truncation: an object is said to be \"truncated\" when the bounding box in the image does not correspond to the full extent of the object. This may occur for two reasons: (a) the object extends outside the image e.g. an image of a person from the waist up; (b) the boundary of the object is occluded e.g. a person standing behind a wall. The aim of including this annotation was to support recognition methods which require images of an entire object as training data, for example assuming that the bounding boxes of the objects can be aligned.\n\nFor the VOC2008 challenge, objects are additionally annotated as \"occluded\" if a high level of occlusion is present. This overcomes a limitation of the VOC2007 dataset that \"clean\" training examples without occlusion cannot automatically be identified from the available annotation.\n\n-difficult: labels objects which are particularly difficult to detect due to small size, illumination, image quality or the need to use significant contextual information. In the challenge evaluation, such objects are discarded, although no penalty is incurred for detecting them. The aim of this annotation is to maintain a reasonable level of difficulty while not contaminating the evaluation with many near-unrecognisable examples. Fig. 3 shows an example of the \"difficult\" annotation. The criteria used to judge an object difficult included confidence in the class label e.g. is it certain that all the animals in Fig. 3 are cows? (sometimes we see sheep in the same field), object size, level of occlusion, imaging factors e.g. motion blur, and requirement for significant context to enable recognition. Note that by marking difficult examples, rather than discarding them, the data should remain useful as methods able to cope with such examples are developed. Furthermore, as noted, any current methods able to detect difficult objects are not penalised for doing so.\n\n\nImage Annotation Procedure\n\nThe VOC2007 annotation procedure was designed to be:\n\n-consistent, so that the annotation of the images is consistent, in terms of the definition of the classes, how bounding boxes are placed, and how viewpoints and truncation are defined. -accurate, so that there are as few annotation errors as possible, -exhaustive, so that all object instances are labelled.\n\nConsistency was achieved by having all annotation take place at a single annotation \"party\" at the University of Leeds, following a set of annotation guidelines which were discussed in detail with the annotators. The guidelines covered aspects including: what to label; how to label pose and bounding box; how to treat occlusion; acceptable image quality; how to label clothing/mud/snow, transparency, mirrors, and pictures. The full guidelines (Winn and Everingham 2007) are available on the WWW. In addition, during the annotation process, annotators were periodically observed to ensure that the guidelines were being followed. Several current annotation projects rely on untrained annotators or have annotators geographically distributed e.g. La-belMe (Russell et al 2008), or even ignorant of their task e.g. the ESP Game (von Ahn and Dabbish 2004). It is very difficult to maintain consistency of annotation in these circumstances, unlike when all annotators are trained, monitored and co-located.\n\nFollowing the annotation party, the accuracy of each annotation was checked by one of the organisers, including checking for omitted objects to ensure exhaustive labelling. To date, only one error has been reported on the VOC2007 dataset, which was a viewpoint marked as unspecified rather than frontal. During the checking process, the \"difficult\" annotation was applied to objects judged as difficult to recognise. As checking the annotation is an extremely timeconsuming process, for VOC2008 this has been incorporated into the annotation party, with each image checked for completeness and each object checked for accuracy, by one of the annotators. As in previous years, the \"difficult\" annotation was applied by one of the organisers to ensure consistency. We return to the question of the expense, in terms of person hours, of annotation and checking, in Sect. 7.3. Table 2 summarises the statistics of the VOC2007 dataset. For the purposes of the challenge, the data is divided Table 2 Statistics of the VOC2007 dataset. The data is divided into two main subsets: training/validation data (trainval), and test data (test), with the trainval data further divided into suggested training (train) and validation (val) sets. For each subset and class, the number of images (containing at least one object of the corresponding class) and number of object instances are shown. Note that because images may contain objects of several classes, the totals shown in the image columns are not simply the sum of the corresponding column. into two main subsets: training/validation data (trainval), and test data (test). For participants' convenience, the trainval data is further divided into suggested training (train) and validation (val) sets, however participants are free to use any data in the trainval set for training, for example if a given method does not require a separate validation set. The total number of annotated images is 9,963, roughly double the 5,304 images annotated for VOC2006. The number of annotated objects similarly rose from 9,507 to 24,640. Since the number of classes doubled from 10 to 20, the average number of objects of each class increased only slightly from 951 to 1,232, dominated by a quadrupling of the number of annotated people. Fig. 4 shows a histogram of the number of images and objects in the entire dataset for each class. Note that these counts are shown on a log scale. The \"person\" class is by far the most frequent, with 9,218 object instances vs. 421 (dining table) to 2,421 (car) for the other classes. This is a natural consequence of requiring each image to be completely annotated -most flickr images can be characterised as \"snapshots\" e.g. family holidays, birthdays, parties, etc. and so many objects appear only \"incidentally\" in images where people are the subject of the photograph.\n\n\nDataset Statistics\n\nWhile the properties of objects in the dataset such as size and location in the image can be considered representative of flickr as a whole, the same cannot be said about the frequency of occurrence of each object class. In order to provide a reasonable minimum number of images/objects per class to participants, both for training and evaluation, certain minority classes e.g. \"sheep\" were targeted toward the end of the annotation party to increase their numbers -annotators were instructed to discard all images not containing one of the minority classes. Examples of certain classes e.g. \"sheep\" and \"bus\" proved difficult to collect, due either to lack of relevant keyword annotation by flickr users, or lack of photographs containing these classes.\n\n\nTaster Competitions\n\nAnnotation was also provided for the newly introduced segmentation and person layout taster competitions. The idea behind these competitions is to allow systems to demonstrate a more detailed understanding of the image, such that objects can be localised down to the pixel level, or an object's parts (e.g. a person's head, hands and feet) can be localised within the object. As for the main competitions, the emphasis was on consistent, accurate and exhaustive annotation.\n\n\nSegmentation\n\nFor the segmentation competition, a subset of images from each of the main datasets was annotated with pixel-level segmentations of the visible region of all contained objects. These segmentations act as a refinement of the bounding  Table 3 Statistics of the VOC2007 segmentation dataset. The data is divided into two main subsets: training/validation data (trainval), and test data (test), with the trainval data further divided into suggested training (train) and validation (val) sets. For each subset and class, the number of images (containing at least one object of the corresponding class) and number of object instances are shown. Note that because images may contain objects of several classes, the totals shown in the image columns are not simply the sum of the corresponding column. All objects in each image are segmented, with every pixel of the image being labelled as one of the object classes, \"background\" (not one of the annotated classes) or \"void\" (uncertain i.e. near object boundary). box, giving more precise shape and localisation information.\n\nIn deciding how to provide pixel annotation, it was necessary to consider the trade-off between accuracy and annotation time: providing pixel-perfect annotation is extremely time intensive. To give high accuracy but to keep the annotation time short enough to provide a large image set, a border area of 5 pixels width was allowed around each object where the pixels were labelled neither object nor background (these were marked \"void\" in the data, see Fig. 5a). Annotators were also provided with detailed guidelines to ensure consistent segmentation (Winn and Everingham 2007). In keeping with the main competitions, difficult examples of objects were removed from both training and test sets by masking these objects with the \"void\" label.\n\nThe object segmentations, where each pixel is labelled with the identifier of a particular object, were used to create class segmentations (see Fig. 5a for examples) where each pixel is assigned a class label. These were provided to encourage participation from class-based methods, which output a class label per pixel but which do not output an object identifier, e.g. do not segment adjacent objects of the same class. Participants' results were submitted in the form of class segmentations, where the aim is to predict the correct class label for every pixel not labelled in the ground truth as \"void\". Table 3 summarises the statistics of the segmentation dataset. In total, 422 images containing 1,215 segmented objects were provided in the combined training/validation set. The test set contained 210 images and 607 objects.\n\n\nPerson layout\n\nFor the person layout competition, a subset of \"person\" objects in each of the main datasets was annotated with information about the 2-D pose or \"layout\" of the person. For each person, three types of \"part\" were annotated with bounding boxes: the head, hands, and feet, see Fig. 5b. These parts were chosen to give a good approximation of the overall pose of a person, and because they can be annotated with relative speed and accuracy compared to e.g. annotation of a \"skeleton\" structure where uncertainty in the position of the limbs and joints is hard to avoid. Annotators selected images to annotate which were of sufficient size such that there was no uncertainty in the position of the parts, and where the head and at least one other part were visible -no other criteria were used to \"filter\" suitable images. Fig. 5b shows some example images, including partial occlusion (upperleft), challenging lighting (upper-right), and \"non-standard\" pose (lower-left). In total, the training/validation set contained 439 annotated people in 322 images, and the test set 441 annotated people in 441 images.\n\n\nSubmission and Evaluation\n\nThe submission and evaluation procedures for the VOC2007 challenge competitions were designed to be fair, to prevent over-fitting, and to demonstrate clearly the differences in accuracy between different methods.\n\n\nSubmission of Results\n\nThe running of the VOC2007 challenge consisted of two phases: At the start of the challenge, participants were issued a development kit comprising training/validation images with annotation, and MATLAB 7 software to access the annotation (stored in an XML format compatible with La-belMe (Russell et al 2008)), to compute the evaluation measures, and including simple baseline implementations for each competition. In the second phase, un-annotated test images were distributed. Participants were then required to run their methods on the test data and submit results as defined in Sect. 4.2. The test data was available for approximately three months before submission of results -this allowed substantial time for processing, and aimed to not penalise computationally expensive methods, or groups with access to only limited computational resources.\n\nWithholding the annotation of the test data until completion of the challenge played a significant part in preventing over-fitting of the parameters of classification or detection methods. In the VOC2005 challenge, test annotation was released and this led to some \"optimistic\" reported results, where a number of parameter settings had been run on the test set, and only the best reported. This danger emerges in any evaluation initiative where ground truth is publicly available. Because the test data is in the form of images, it is also theoretically possible for participants to hand-label the test data, or \"eyeball\" test results -this is in contrast to e.g. machine learning benchmarks where the test data may be sufficiently \"abstract\" such that it cannot easily be labelled by a non-specialist. We rely on the participants' honesty, and the limited time available between release of the test data and submission of results, to minimise the possibility of manual labelling. The possibility could be avoided by requiring participants to submit code for their methods, and never release the test images. However, this makes the evaluation task difficult for both participants and organisers, since methods may use a mixture of MATLAB/C code, proprietary libraries, require significant computational resources, etc. It is worth noting, however, that results submitted to the VOC challenge, rather than afterward using the released annotation data, might appropriately be accorded higher status since participants have limited opportunity to experiment with the test data. 11 In addition to withholding the test data annotation, it was also required that participants submit only a single result per method, such that the organisers were not asked to choose the best result for them. Participants were not required to provide classification or detection results for all 20 classes, to encourage participation from groups having particular expertise in e.g. person or vehicle detection.\n\n\nEvaluation of Results\n\nEvaluation of results on multi-class datasets such as VOC2007 poses several problems: (i) for the classification task, images contain instances of multiple classes, so a \"forced choice\" paradigm such as that adopted by Caltech 256 (Griffin et al 2007) -\"which one of m classes does this image contain?\" -cannot be used; (ii) the prior distribution over classes is significantly nonuniform so a simple accuracy measure (percentage of correctly classified examples) is not appropriate. This is particularly salient in the detection task, where sliding window methods will encounter many thousands of negative (non-class) examples for every positive example. In the absence of information about the cost or risk of misclassifications, it is necessary to evaluate the trade-off between different types of classification error; (iii) evaluation measures need to be algorithmindependent, for example in the detection task participants have adopted a variety of methods e.g. sliding window classification, segmentation-based, constellation models, etc. This prevents the use of some previous evaluation measures such as the Detection Error Tradeoff (DET) commonly used for evaluating pedestrian detectors (Dalal and Triggs 2005), since this is applicable only to sliding window methods constrained to a specified window extraction scheme, and to data with cropped positive test examples.\n\nBoth the classification and detection tasks were evaluated as a set of 20 independent two-class tasks: e.g. for classification \"is there a car in the image?\", and for detection \"where are the cars in the image (if any)?\". A separate \"score\" is computed for each of the classes. For the classification task, participants submitted results in the form of a confidence level for each image and for each class, with larger values indicating greater confidence that the image contains the object of interest. For the detection task, participants submitted a bounding box for each detection, with a confidence level for each bounding box. The provision of a confidence level allows results to be ranked such that the trade-off between false positives and false negatives can be evaluated, without defining arbitrary costs on each type of classification error.\n\nAverage Precision (AP). For the VOC2007 challenge, the interpolated average precision (Salton and Mcgill 1986) was used to evaluate both classification and detection.\n\nFor a given task and class, the precision/recall curve is computed from a method's ranked output. Recall is defined as the proportion of all positive examples ranked above a given rank. Precision is the proportion of all examples above that rank which are from the positive class. The AP summarises the shape of the precision/recall curve, and is defined as the mean precision at a set of eleven equally spaced recall levels [0, 0.1, . . . , 1]:\nAP = 1 11 \u2211 r\u2208{0,0.1,...,1} p interp (r)(1)\nThe precision at each recall level r is interpolated by taking the maximum precision measured for a method for which the corresponding recall exceeds r:\np interp (r) = max r:r\u2265r p(r)(2)\nwhere p(r) is the measured precision at recallr. The intention in interpolating the precision/recall curve in this way is to reduce the impact of the \"wiggles\" in the precision/recall curve, caused by small variations in the ranking of examples. It should be noted that to obtain a high score, a method must have precision at all levels of recallthis penalises methods which retrieve only a subset of examples with high precision (e.g. side views of cars).\n\nThe use of precision/recall and AP replaced the \"area under curve\" (AUC) measure of the ROC curve used in VOC2006 for the classification task. This change was made to improve the sensitivity of the metric (in VOC2006 many methods were achieving greater than 95% AUC), to improve interpretability (especially for image retrieval applications), to give increased visibility to performance at low recall, and to unify the evaluation of the two main competitions. A comparison of the two measures on VOC2006 showed that the ranking of participants was generally in agreement but that the AP measure highlighted differences between methods to a greater extent.\n\nBounding box evaluation. As noted, for the detection task, participants submitted a list of bounding boxes with associated confidence (rank). Detections were assigned to ground truth objects and judged to be true/false positives by measuring bounding box overlap. To be considered a correct detection, the area of overlap a o between the predicted bounding box B p and ground truth bounding box B gt must exceed 0.5 (50%) by the formula\na o = area(B p \u2229 B gt ) area(B p \u222a B gt )(3)\nwhere B p \u2229 B gt denotes the intersection of the predicted and ground truth bounding boxes and B p \u222a B gt their union. The threshold of 50% was set deliberately low to account for inaccuracies in bounding boxes in the ground truth data, for example defining the bounding box for a highly nonconvex object, e.g. a person with arms and legs spread, is somewhat subjective. Sect. 6.2.3 evaluates the effect of this threshold on the measured average precision. We return to the question of the suitability of bounding box annotation in Sect. 7.3.\n\nDetections output by a method were assigned to ground truth objects satisfying the overlap criterion in order ranked by the (decreasing) confidence output. Multiple detections of the same object in an image were considered false detections e.g. 5 detections of a single object counted as 1 correct detection and 4 false detections -it was the responsibility of the participant's system to filter multiple detections from its output.\n\n\nEvaluation of the segmentation taster\n\nA common measure used to evaluate segmentation methods is the percentage of pixels correctly labelled. For the VOC2007 segmentation taster, this measure was used per class by considering only pixels labelled with that class in the ground truth annotation. Reporting a per-class accuracy in this way allowed participants to enter segmentation methods which handled only a subset of the classes. However, this evaluation scheme can be misleading, for example, labelling all pixels \"car\" leads to a perfect score on the car class (though not the other classes). Biases in different methods can hence lead to misleading high or low accuracies on individual classes. To rectify this problem, the VOC2008 segmentation challenge will be assessed on a modified perclass measure based on the intersection of the inferred segmentation and the ground truth, divided by the union: seg. accuracy = true pos. true pos. + false pos. + false neg.\n\n(4) Pixels marked \"void\" in the ground truth are excluded from this measure. Compared to VOC2007, the measure penalises methods which have high false positive rates (i.e. that incorrectly mark non-class pixels as belonging to the target class). The per-class measure should hence give a more interpretable evaluation of the performance of individual methods.\n\n\nEvaluation of the person layout taster\n\nThe \"person layout\" taster was treated as an extended detection task. Methods were evaluated using the same AP measure used for the main detection competition. The criterion for a correct detection, however, was extended to require correct prediction of (i) the set of visible parts (head/hands/feet); (ii) correct bounding boxes for all parts, using the standard overlap threshold of 50%.\n\nAs reported in Sect. 6.4 this evaluation criterion proved extremely challenging. In the VOC2008 challenge, the evaluation has been relaxed by providing person bounding boxes for the test data (disjoint from the main challenge test set), so that methods are not required to complete the detection part of the task, but only estimate part identity and location. Table 4 summarises the participation in the VOC2007 challenge. A total of 16 institutions submitted results (c.f. 16 in 2006 and 9 in 2005). Taking into account multiple groups in an institution and multiple methods per group, there were a total of 28 methods submitted (c.f. 25 in 2006, 13 in 2005).\n\n\nMethods\n\n\nClassification Methods\n\nThere were 17 entries for the classification task in 2007, compared to 14 in 2006 and 9 in 2005.\n\nMany of the submissions used variations on the basic bag-of-visual-words method (Csurka et al (2004); Sivic and Zisserman (2003)) that was so successful in VOC2006, see Zhang et al (2007): local features are computed (for example SIFT descriptors); vector quantised (often by using k-means) into a visual vocabulary or codebook; and each image is then represented by a histogram of how often the local features are assigned to each visual word. The representation is known as bag-of-visual-words in analogy with the bag-of-words (BOW) text representation where the frequency, but not the position, of words is used to represent text documents. It is also known as bag-of-keypoints or bagof-features. The classifier is typically a support vector machine (SVM) with \u03c7 2 or Earth Mover's Distance (EMD) kernel.\n\nWithin this approach, submissions varied tremendously in the features used: both their type and their density. Sparse local features were detected using the Harris interest point operator and/or the SIFT detector (Lowe 2004), and then represented by the SIFT descriptor. There was some attention to exploring different colour spaces (such as HSI) in the detection for greater immunity to photometric effects such as shadows (PRIP-UvA). Others (e.g. INRIA Larlus) computed descriptors on a dense grid, and one submission (MPI) combined both sparse and dense descriptors. In addition to SIFT, other descriptors included local colour, pairs of adjacent segments (PAS) (Ferrari et al 2008), and Sobel edge histograms.\n\nThe BOW representation was still very common, where spatial information, such as the position of the descriptors is disregarded. However, several participants provided additional representations (channels) for each image where as well as the BOW, spatial information was included by various tilings of the image (INRIA Genetic, INRIA Flat), or using a spatial pyramid (TKK).\n\nWhile most submissions used a kernel SVM as the classifier (with kernels including \u03c7 2 and EMD), XRCE used lo-     5 63.6 56.1 71.9 33.1 60.6 78.0 58.8 53.5 42.6 54.9 45.8 77.5 64.0 85.9 36.3 44.7 50.6 79.2  gistic regression with a Fisher kernel (Perronnin and Dance 2007), and ToshCam used a random forest classifier.\nUVA FuseAll \u2022 - UVA MCIP \u2022 - UVA SFS \u2022 - UVA WGT \u2022 - XRCE \u2022 -Florent\nWhere there was greatest diversity was in the methods for combining the multiple representations (channels). Some methods investigated \"late fusion\" where a classifier is trained on each channel independently, and then a second classifier combines the results. For example TKK used this approach, for details see Viitaniemi and Laaksonen (2008). Tsinghua combined the individual classifiers using Rank-Boost. INRIA entered two methods using the same channels, but differing in the manner in which they were combined: INRIA Flat uses uniform weighting on each feature (following Zhang et al (2007)); INRIA Genetic uses a different class-dependent weight for each feature, learnt from the validation data by a genetic algorithm search.\n\nIn 2006, several of the submissions tackled the classification task as detection -\"there is a car here, so the image contains a car\". This approach is perhaps more in line with human intuition about the task, in comparison to the \"global\" classification methods which establish the presence of an object without localising it in the image. However, in 2007 no submissions used this approach.\n\nThe VOC challenge invites submission of results from \"off-the-shelf\" systems or methods trained on data other than that provided for the challenge (see Sect. 2.1), to be evaluated separately from those using only the provided data. No results were submitted to VOC2007 in this category. This is disappointing, since it prevents answering the question as to how well current methods perform given unlimited training data, or more detailed annotation of training data. It is an open question how to encourage submission of results from e.g. commercial systems.\n\n\nDetection Methods\n\nThere were 9 entries for the detection task in 2007, compared to 9 in 2006 and 5 in 2005. As for the classification task, all submitted methods were trained only on the provided training data.\n\nThe majority of the VOC2007 entries used a \"sliding window\" approach to the detection task or variants thereof. In the basic sliding window method a rectangular window of the image is taken, features are extracted from this window, and it is then classified as either containing an instance of a given class or not. This classifier is then run exhaustively over the image at varying location and scale. In order to deal with multiple nearby detections a \"non-maximum suppression\" stage is then usually applied. Prominent examples of this method include the Viola and Jones (2004) face detector and the Dalal and Triggs (2005) pedestrian detector.\n\nThe entries Darmstadt, INRIA Normal, IN-RIA PlusClass and IRISA were essentially sliding window methods, with the enhancements that INRIA PlusClass also utilised the output of a whole image classifier, and that IRISA also trained separate detectors for person-on-X where X was horse, bicycle, or motorbike. Two variations on the sliding window method avoided dense sampling of the test image: The Oxford entry used interest point detection to select candidate windows, and then applied an SVM classifier; see Chum and Zisserman (2007) for details. The MPI ESSOL entry (Lampert et al 2008) used a branch-and-bound scheme to efficiently maximise the classifier function (based on a BOW representation, or pyramid match kernel (Grauman and Darrell 2005) determined on a per-class basis at training time) over all possible windows.\n\nThe UoCTTI entry used a more complex variant of the sliding window method, see Felzenszwalb et al (2008) for details. It combines the outputs of a coarse window and sev-15 eral higher-resolution part windows which can move relative to the coarse window; inference over location of the parts is performed for each coarse image window. Note that improved results are reported in Felzenszwalb et al (2008) relative to those in Table 6; these were achieved after the public release of the test set annotation.\n\nThe method proposed by TKK automatically segments an image to extract candidate bounding boxes and then classifies these bounding boxes, see Viitaniemi and Laaksonen (2008) for details. The MPI Center entry was a baseline that returns exactly one object bounding box per image; the box is centred and is 51% of the total image area.\n\nIn previous VOC detection competitions there had been a greater diversity of methods used for the detection problem, see Everingham et al (2006b) for more details. For example in VOC2006 the Cambridge entry used a classifier to predict a class label at each pixel, and then computed contiguously segmented regions; the TU Darmstadt entry made use of the Implicit Shape Model (ISM) (Leibe et al 2004); and the MIT Fergus entry used the \"constellation\" model (Fergus et al 2007).\n\n\nResults\n\nThis section reports and discusses the results of the VOC2007 challenge. Full results including precision/recall curves for all classes, not all of which are shown here due to space constraints, can be found on the VOC2007 website (Everingham et al 2007).\n\n\nClassification\n\nThis section reports and discusses the results of the classification task. A total of 17 methods were evaluated. All participants tackled all of the 20 classes. Table 5 lists the AP for all submitted methods and classes. For each class the method obtaining the greatest AP is identified in bold, and the methods with 2nd and 3rd greatest AP in italics. Precision/recall curves for a representative sample of classes are shown in Fig. 6. Results are shown ordered by decreasing maximum AP. The left column shows all results, while the right column shows the top five results by AP. The left column also shows the \"chance\" performance, obtained by a classifier outputting a random confidence value without examining the input image -the precision/recall curve and corresponding AP measure are not invariant to the proportion of positive images, resulting in varying chance performance across classes. We discuss this further in Sect. 6.1.3.\n\n\nClassification results by method\n\nOverall the INRIA Genetic method stands out as the most successful method, obtaining the greatest AP in 19 of the 20 classes. The related INRIA Flat method achieves very similar performance, with AP between the two methods differing by just 1-2% for most classes. As described in Sect. 5.1 these methods use the same set of heterogeneous image features, and differ only in the way that features are fused in a generalised radial basis function (RBF) kernel: INRIA Flat uses uniform weighting on each feature, and INRIA Genetic learns a different weight for each feature from the validation data. The XRCE method comes third in 17 of 20 classes and first in one. This method differs from the INRIA methods in using a Fisher kernel representation of the distribution of visual features within the image, and uses a smaller feature set and logistic regression cf. the kernel SVM classifier used by the INRIA methods. Fig. 7 summarises the performance of all methods, plotting the median AP for each method computed over all classes, and ordered by decreasing median AP. Despite the overall similarity in the features used, there is quite a wide range in accuracy of 32.1-57.5%, with one method (PRIPUVA) substantially lower at 21.1%. The high performing methods all combine multiple features (channels) though, and some (INRIA Genetic, INRIA Flat, TKK) include spatial information as well as BOW. Software for the feature descriptors used by the UVA methods (van de Sande et al 2008) has been made publicly available, and would form a reasonable state-of-the-art baseline for future challenges.\n\n\nStatistical significance of results\n\nA question often overlooked by the computer vision community when comparing results on a given dataset is whether the difference in performance of two methods is statistically   Fig. 8 Analysis of statistically significant differences in the classification results. The mean rank over all classes is plotted on the x-axis for each method. Methods which are not significantly different (p = 0.05), in terms of mean rank, are connected. significant. For the VOC challenge, we wanted to establish whether, for the given dataset and set of classes, one method can be considered significantly more accurate than another. Note that this question is different to that investigated e.g. in the Caltech 101 challenge, where multiple training/test folds are used to establish the variance of the measured accuracy. Whereas that approach measures robustness of a method to differing data, we wish to establish significance for the given, fixed, dataset. This is salient, for example, when a method may not involve a training phase, or to compare against a commercial system trained on proprietary data.\n\nLittle work has considered the comparison of multiple classifiers over multiple datasets. We analysed the results of the classification task using a method proposed by Demsar (2006), specifically using the Friedman test with Nemenyi post hoc analysis. This approach uses only comparisons between the rank of a method (the method achieving the greatest AP is assigned rank 1, the 2nd greatest AP rank 2, etc.), and thus requires no assumptions about the distribution of AP to be made. Each class is treated as a separate test, giving one rank measurement per method and class. The analysis then consists of two steps: (i) the null hypothesis is made that the methods are equivalent and so their ranks should be equal. The hypothesis is tested by the Friedman test (a non-parametric variant of ANOVA), which follows a \u03c7 2 distribution; (ii) having rejected the null hypothesis the differences in ranks are analysed by the Nemenyi test (similar to the Tukey test for ANOVA). The difference between mean ranks (over classes) for a pair of methods follows a modified Studentised range statistic. For a confidence level of p = 0.05 and given the 17 methods tested over 20 classes, the \"critical difference\" is calculated as 4.9 -the difference in mean rank between a pair of methods must exceed 4.9 for the difference to be considered statistically significant. Fig. 8 visualises the results of this analysis using the CD (critical difference) diagram proposed by Demsar (2006). The x-axis shows the mean rank over classes for each method. Methods are shown clockwise from right to left in decreasing (first to last) rank order. Groups of methods for which the difference in mean rank is not significant are connected by horizontal bars. As can be seen, there is substantial overlap between the groups, with no clear \"clustering\" into sets of equivalent methods. Of interest is that the differences between the first six ranked methods (INRIA Genetic, IN-RIA Flat, XRCE, TKK, QMUL LSPCH, QMUL HSLS) cannot be considered statistically significant. A limitation of this analysis is that the relatively small number of observations (20 classes per method) limits the power of the test. Increasing the number of classes will make it more feasible to establish significant differences between methods in terms of their performance over a wide range of classes. As discussed in Sect. 7.1, we are also keen to highlight differences in the approach taken by methods, not solely their performance. Fig. 9 summarises the results obtained by object class, plotting for each class the maximum and median AP taken over all methods. Also shown is the \"chance\" performance -the AP obtained by a classifier outputting a random confidence value without examining the input image. Results are shown ordered by decreasing maximum AP. There is substantial variation in the maximum AP as a function of object class, from 33.1% (bottle) to 85.9% (person). The median AP varies from 15.6% (potted plant) to 75.7% (person). The median results can be seen to approximately follow the ranking of results by maximum AP, suggesting that the same classes proved difficult across methods, but individual differences 18 can be seen, for example the difference in maximum AP for the 2nd to 4th classes is very small such that the ordering is somewhat arbitrary. The high AP for the \"person\" class can be attributed in part to the high proportion of images in the dataset containing people -the chance AP for this class is 43.4%. However, as can be seen in Fig. 9, the results are substantially above chance for all classes.\n\n\nClassification results by class\n\nWhile results on some classes e.g. person and train (Fig. 6a-d) are very promising, for all classes there is substantial room for improvement. For some classes e.g. bottle (Fig. 6g-h), the precision/recall curves show that the methods' precision drops greatly at moderate recall, and current methods would not give satisfactory results in the context of an image retrieval system. It is also worth noting that if one views the classification task as image retrieval, the evaluation is somewhat \"benign\" since the prior probability of an image containing an object of interest is still quite high, 2% for the least frequent class (sheep). We might expect that in a real world scenario, for example image-based web search, the prior probability for some classes would be much lower. We return to this point in Sect. 7.3.\n\n\nWhat are the classification methods learning?\n\nAs noted, the quantitative evaluation of methods by AP gives a summary of a method's precision/recall trade-off. It is interesting to examine the success and failure modes of the methods to derive some insight into what current methods are learning, and what limitations might be addressed in development of future methods.\n\nWe first examined the kinds of images which methods found \"easy\" or \"difficult\". Five submitted methods were selected which represented somewhat different approaches rather than small variations (e.g. INRIA Genetic vs. IN-RIA Flat): INRIA Genetic, XRCE, TKK, QMUL LSPCH and UVA FuseAll. Each test image was then assigned a rank by each method (using the method's confidence output for that image). An overall rank for the image was then assigned by taking the median over the ranks from the five selected methods. By looking at which images, containing the class of interest or not, are ranked first or last, we can gain insight into what properties of the images make recognition easy or difficult for current methods. Fig. 10 shows ranked images for the \"car\" class. The first row shows the five positive images (containing cars) assigned the highest rank (1st-5th) -these can be considered images which are \"easy\" for current methods to recognise as containing cars. The second row shows the five positive images (containing cars) assigned the lowest rank -these are images for which current methods cannot easily establish the presence of a car. The third row shows the five negative images (not containing cars) assigned the highest rank -these are images which \"confuse\" current methods, which judge them highly likely to contain cars.\n\nThe high ranked positive images (Fig. 10a) include images where a single car dominates the image, in an uncluttered or \"expected\" background i.e. a road, and images where a number of cars are visible. The inclusion of images with multiple cars is perhaps surprising, but as discussed below, may be attributed to the reliance of current methods on \"textural\" properties of the image rather than spatial arrangement of parts. The low ranked positive images (Fig. 10b) are typical across all classes, showing the object of interest small in the image, poorly lit or heavily occluded. For methods based on global image descriptors, such as the BOW approach, these factors cause the presence of the car to contribute little to the feature vector describing the image. In the case of the car class, the high ranked negative images (Fig. 10c) show an \"intuitive\" confusion -the first five images shown all contain buses or lorries (not considered part of the \"car\" class). This may be considered a pleasing result since there is some natural fuzziness in the distinction between the classes \"car\" and \"bus\", and the classes certainly share both semantic and visual properties. However, as discussed below, these \"natural\" confusions are not apparent for all classes. Fig. 11 shows the five highest ranked positive and negative images for the \"cat\" class. Here also the confusion appears natural, with all five of the highest ranked non-cat images containing dogs. However, it can also be seen that the composition of the images for the cat and dog classes is very similar, and this may play a significant role in the learnt classifiers. This is a bias in the content of flickr images, in that photographers appear to take many \"close-up\" images of their pets. Fig. 12 shows corresponding images for the \"bicycle\" class. The high ranked positive images show a similar pattern to the \"car\" class, containing uncluttered images of bicycles in \"canonical\" poses, and images where the scene is dominated by multiple bicycles. For this class, however, the high ranked negative images (Fig. 12b) are anything but intuitive -all of the first five negative images show scenes of birds sitting on branches, which do not resemble bicycles at all to a human observer. The reason for this confusion might be explained by the lack of informative spatial information in current methods. Examining the negative images (Fig. 12b), which are dominated by \"wiry\" or barlike features, it seems clear that a BOW representation may closely resemble that of a bicycle, with the representation of branches matching that of the bicycle tubes and spokes. This is a limitation of BOW methods, in that information about the spatial arrangement of features is discarded, or represented only very weakly and implicitly, by features representing the conjunction of other features. For methods using tiling or spatial pyramids, the spatial information is captured only at a coarse image/scene level, and not at the level of individual objects. Fig. 13 shows images for the \"chair\" class. Results here are interesting in that none of the high ranked positive images are close-up views of isolated chairs, even though these are present in the dataset. All the high ranked negative images (Fig. 13b) show indoor scenes which might well be expected to contain chairs, but do not. Only one of the first five negative images contains a sofa, which might be considered the most easily confused class both semantically and visually. It seems likely that in this case the classifiers are learning about the scene context of chairs rather than modelling the appearance of a chair itself. Again, this is a somewhat natural consequence of a global classification approach. The use of context may be seen in both positive and negative lights -while there is much interest in the field in exploiting contextual cues to object recognition (Torralba 2003;Sudderth et al 2008;Hoiem et al 2006), the incorporation of context by use of a global descriptor leads to failure when objects are presented out of context, or over-reliance on context when the training set contains mostly images of scenes rather than individual objects. The question of whether cur-rent methods are learning object or scene representations is considered further below.\n\nFinally, Fig. 14 shows images for the \"person\" class. In this case, the negative images ( Fig. 14b) contain (i) dining tables (3rd and 5th image), and (ii) motorbikes (1st, 2nd and 4th images). The confusion with the \"dining table\" class seems natural, in the same manner as the \"chair\" class, in that the presence of a dining table seems a good predictor for the presence of a person. Statistics of the dataset reveal that the presence of a motorbike is a similarly effective predictor: 68.9% of the images containing motorbikes also contain people (although an alternative explanation may be the elongated vertical shape of the motorbikes seen from the front or rear). These \"unintentional\" regularities in the dataset are a limiting factor in judging the effectiveness of the classification methods in terms of object recognition rather than image retrieval. The detection task, see Sect. 6.2, is a much more challenging test of object recognition.\n\n\nEffect of object size on classification accuracy\n\nAll of the methods submitted are essentially global, extracting descriptors of the entire image content. The ranking of   . 15 Classification results as a function of object size. Plots show results for four representative classes. For each plot the x-axis shows the lower threshold on object size for a positive image to be included in the test set; the y-axis shows the corresponding AP. A threshold of 30% means that all positive images (e.g. containing \"car\") which contained fewer than 30% positive (i.e. car) pixels were removed from the test set; a threshold of 0% means that no images were discarded.\n\nimages also suggests that the image context is used extensively by the classifiers. It is interesting therefore to examine whether methods are biased toward images where the object of interest is large, or whether conversely the presence of adequate scene context determines the accuracy of the results.\n\nWe conducted experiments to investigate the effect of object size on the submitted methods' accuracy. A series of test sets was made in which all positively-labelled images contained at least some proportion of pixels labelled as the object of interest. For example, given a threshold of 10%, only images for which at least 10% of the pixels were \"car\" were labelled as containing a car; images with some car pixels, but less than 10%, were removed from the test set, and the negative examples always had zero car pixels. The proportion of pixels belonging to a particular class was approximated by the union of the corresponding bounding boxes. Fig. 15 shows results of the experiments for a representative set of classes. For each plot, the x-axis shows the threshold on the proportion of positive pixels in an image for the image to be labelled positive, defining one of the series of test sets. For each threshold, the AP was measured and is shown on the y-axis. Fig. 15a/b show classes \"car\" and \"motorbike\", for which some positive correlation between object size and AP can be observed. As shown, the AP increases as images with fewer object pixels are discarded, and peaks at the point where the only positive images included have at least 15% object pixels. This effect can be accounted for by the use of interest point mechanisms to extract features, which break down if the object is small such that no interest points are detected on it, and in the case of dense feature extraction, by the dominance of the background or clutter in the global representation, which \"swamps\" the object descriptors. For the \"motorbike\" class, the AP is seen to decrease slightly when only images containing at least 20% of object pixels are included -this may be explained by the reduction of relevant context in such images.\n\nFor most classes, zero or negative correlation between object size and AP was observed, for example \"bird\", shown in Fig. 15c. This is compatible with the conclusions from examining ranked images, that current methods are making substantial use of image composition or context. For some classes, e.g. \"chair\", shown in Fig. 15d, this effect is quite dramatic -for this class the learnt classifiers are very poor at recognising images where chairs are the dominant part of the scene. These results are in agreement with the ranked images shown in Fig. 13b, suggesting a reliance on scene context.  Fig. 16 Comparison of classification results on VOC2006 vs. VOC2007 test sets. All methods were trained on the VOC2007 training/validataion set. The median AP over the 10 classes common to the VOC2006/VOC2007 challenges is plotted for each method and test set. The line marked VOC2006 indicates the best result obtained in the VOC2006 challenge, trained using the VOC2006 training/validation set.\n\n\nClassification results on the VOC2006 test set\n\nIn order to gauge progress since the 2006 VOC challenge, participants in the 2007 challenge were asked to additionally submit results on the VOC2006 dataset. Since the training phase for many methods is computationally expensive, for example requiring multiple cross-validation runs, participants were asked to submit results trained using VOC2007 data i.e. not requiring re-training. This allows us to answer two questions: (i) do methods which perform well on the VOC2007 test set also perform well on VOC2006 test set? (ii) do newer methods trained on VOC2007 data outperform older methods trained on VOC2006 data?\n\nParticipants submitted classification results on both test sets for 9 methods. Fig. 16 summarises the results. The xaxis show the median AP over all classes on the VOC2007 test set. The y-axis shows the median AP over all classes on the VOC2006 test set. The line labelled \"VOC2006\" indicates the best result reported in the 2006 challenge. Note that since the median is taken over the 10 classes common to the 2006 and 2007 challenges (see Fig. 2), the ranking of methods does not match that shown in Fig. 7, for example the XRCE method outperforms the INRIA methods on the VOC2007 data for this subset of classes.\n\nAs the figure shows, there is very high correlation between the results on the VOC2006 and VOC2007 data. This suggests that methods performing well on VOC2007 are \"implicitly\" more successful, rather than obtaining good results due to excessive fitting of the statistics of the VOC2007 data. There are small differences in the ranking of methods, for example the XRCE method is 1st on VOC2007 (over the subset of 10 classes common to both challenges) but 3rd on VOC2006. The INRIA Genetic method gives results marginally lower than XRCE on the VOC2007 data, but convincingly better results on the VOC2006 data.\n\nHowever, for all methods the performance on the VOC2006 data is less than on VOC2007, by 5.0% (IN-RIA FLAT) to 12.7% (Tsinghua). This implies that methods have failed to generalise to the VOC2006 data to some extent. There are two possible reasons: (i) there is fundamentally insufficient variability in the VOC2007 data to generalise well to the VOC2006 data; (ii) the classifiers have overfit some \"peculiarities\" of the VOC2007 data which do not apply to the VOC2006 data. Factors might include the difference in the time of year of data collection (see Sect. 3.1). A possible concern might be that the better results obtained on VOC2006 are due to the presence of \"near-duplicate\" images spanning the training/test sets. This possibility was minimised by removing such images when the dataset was collected (see Sect. 3.1).\n\nTested on the VOC2006 data, the maximum median-AP achieved by a method submitted in 2007 was 62.1% compared to 74.5% in 2006. This again suggests either that the 2007 methods over-fit some properties of the VOC2007 data, or that there were peculiarities of the VOC2006 data which methods trained on that data in 2006 were able to exploit. One such possibility is the inclusion of the MSR Cambridge images in the VOC2006 data (see Sect. 3.1) which may have provided a \"boost\" to 2006 methods learning their specific viewpoints and simple scene composition.\n\n\nDetection\n\nThis section reports and discusses the results of the detection task. A total of 9 methods were evaluated. Six participants tackled all of the 20 classes, with the others submitting results on a subset of classes. Table 6 lists the AP for all submitted methods and classes. For each class the method obtaining the greatest AP is identified in bold, and the methods with 2nd and 3rd greatest AP in italics. Precision/recall curves for a representative sample of classes are shown in Fig. 17.\n\n\nDetection results by method\n\nIt is difficult to judge an overall \"winner\" in the detection task because different participants tackled different subsets of classes (this is allowed under the rules of the challenge). Oxford won on all 6 (vehicle) classes that they entered, UoCTTI won on 6 classes, and MPI ESSOL on 5. The Oxford method achieved the greatest AP for all of the six classes entered, with the AP substantially exceeding the second place result, by a margin of 4.0-11.4%. The UoCTTI method entered all 20 classes, and came first or second in Table 6 Detection results. For each object class and submission, the AP measure (%) is shown. Bold entries in each column denote the maximum AP for the corresponding class. Italic entries denote the results ranked second or third. Note that some participants submitted results for only a subset of the 20 classes. Darmstadt  14. MPI ESSOL also entered all 20 classes, but it is noticeable that on some classes the AP score for this method is poor relative to other entries. These methods differ quite significantly in approach: Oxford used interest point detection to select candidate detection regions, and applied an SVM classifier using a spatial pyramid (Lazebnik et al 2006) representation to the candidate region; the UoCTTI method used a sliding window approach, but with a \"star\" model of parts; and MPI ESSOL used an SVM classifier based on a BOW or spatial pyramid representation of the candidate window.\n- - - - - -30.1 - - - - - - - - - - - - - INRIA\nIt would have been particularly interesting to see results of the Oxford method on all classes, since it might be ex-pected that the use of interest points and a fixed grid representation might limit the applicability to classes with limited distinguishing features or significant variability in shape, e.g. animals.\n\nPromising results were obtained by all methods, but with results for each method varying greatly among the object classes. It seems clear that current methods are more or less suited to particular classes. An example is the failure of the UoCTTI method on the \"dog\" class (AP=2.3) compared to the MPI ESSOL method (AP=16.2). While the former emphasises shape, the latter uses a BOW/spatial pyramid representation which might better capture texture, but captures shape more coarsely. Conversely, the UoCTTI  Fig. 18 summarises the results obtained by object class, plotting for each class the maximum and median AP taken over all methods. Results are shown ordered by decreasing maximum AP. It should be noted that, because some participants only submitted results for some classes, the number of results available varies for each class. There is substantial variation in the maximum AP as a function of object class, from 9.4% (boat) to 43.2% (car). The median AP varies from 2.8% (boat) to 29.4% (car). The median results can be seen to approximately follow the ranking of results by maximum AP. Results on some classes e.g. car/bicycle ( Fig. 17a-b) are quite promising, with the best performing methods obtaining precision close to 100% for recall up to 15-20%. However, precision drops rapidly above this level of recall. This suggests that methods are retrieving only a subset of examples with any accuracy, perhaps the \"canonical\" views (e.g. car side, car rear). A challenge for future methods is to increase the recall. In the related domain of face detection the move from frontal-only detection to arbitrary pose has proved challenging.\n\n\nDetection results by class\n\nIt can be seen from Fig. 18 that the best results were obtained for classes which have traditionally been investigated in object detection, e.g. car, bicycle and motorbike.  Fig. 19 Effect of the bounding box overlap threshold on the AP measure. For the class \"car\", on which the submitted methods gave the best results, AP is plotted as a function of the overlap threshold. The threshold adopted for the challenge is 50%.\n\nSuch classes have quite predictable visual properties, with distinctive parts e.g. wheels, and relatively fixed spatial arrangement of parts. For classes with significant variation in shape or appearance e.g. people (Fig. 17c) and household objects (which are often significantly occluded), results are substantially worse. Results on the important \"person\" class were, however quite promising overall. The best results in terms of AP on this class were obtained by the IRISA and UoCTTI methods. As noted in Sect. 5.2 the IRISA method trained multiple person detectors, for example \"person on horse/person on bicycle\". The UoCTTI method is also potentially better able to deal with varying articulation, by its approach of simultaneous \"pose\" inference and detection.\n\nFor several classes the results are somewhat counterintuitive, for example good results are obtained on the \"train\" class (max AP=33.4%) which might be expected to be challenging due to the great variation in appearance and aspect ratio with pose. The results for this class may be explained by the inclusion of several methods which exploited whole image classification -MPI Center which predicts a single detection per image of fixed size, and INRIA PlusClass which combines sliding window detection with a global classifier. Because trains tend to appear large in the image, these global methods prove successful on this data, however it is notable that the Oxford method also did well for this class. For the \"horse\" class, the good results may be attributable to unwanted regularities in the dataset, which includes many images of horses taken by a single photographer at a single gymkhana event. Such regularities will be reduced in the VOC2008 dataset by distributing searches over time, as noted in Sect. 3.1. Results for the classes with low AP, for example boat (Fig. 17d) leave much scope for improvement, with precision dropping to zero by 20% recall. The VOC2007 dataset remains extremely challenging for current detection methods. \n\n\nEvaluation of the overlap threshold\n\nAs noted in Sect. 4.2, detections are considered true positives if the predicted and ground truth bounding boxes overlap by 50% according to the measure defined in Eqn. 3, with the threshold of 50% set low to account for uncertainty in the bounding box annotation. We evaluated the effect the overlap threshold has on the measured AP by varying the threshold. Fig. 19 shows AP as a function of the overlap threshold for the class \"car\", on which the best results (in terms of AP with overlap threshold of 50%) were obtained by the submitted methods. One caveat applies: participants were aware of the 50% threshold, and were therefore free to optimise their methods at this threshold, for example in their schemes for elimination of multiple detections.\n\nAs Fig. 19 shows, the measured AP drops steeply for thresholds above 50%, indicating that none of the methods give highly accurate bounding box predictions. Reducing the threshold to 10% results in an increase in measured AP of around 7.5%. Note that for all but one pair of methods (the AP of these two methods at a threshold of 50% differs by only 0.7%). We conclude that the measure is performing satisfactorily, capturing the proportion of objects detected without overly penalising imprecise bounding box predictions.\n\n\nWhat are the detection methods learning?\n\nAs in the classification task it is interesting to examine the success and failure modes of the methods to derive some insight into what current methods are learning, and what limitations might be addressed in the development of future methods.\n\nEach detection method provides a list of bounding boxes ranked by the corresponding confidence output. We present some of the highest ranked true positive (object) and false positive (non-object) detections here. Since the detection methods varied greatly in approach and success, as measured by AP, we present individual results for selected classes and methods. For a given class, we present results of the three methods giving greatest AP. The classes selected were those where results are particularly promising, or in- teresting observations can be made e.g. confusion between \"motorbike\" and \"bicycle\". Fig. 20 shows the five highest ranked true positive detections for the \"car\" class. The methods shown, obtaining greatest AP, are Oxford, UoCTTI and IRISA. As can be seen the Oxford method has a preference for large objects (Fig. 20a) which is less apparent for the other two methods (Fig. 20b/c). We analyse this bias toward large objects further in the next subsection. For this class, there is no apparent preference for a particular viewpoint or \"aspect\" -the methods all return cars with varying pose at high confidence. However, for the \"bicycle\" class shown in Fig. 21, the most successful methods (Oxford, UoCTTI and INRIA PlusClass) all return side views of bicycles with highest confidence. For the UoCTTI method in particular there seems no preference for left or right facing bicycles, though the bias toward right facing bicycles for the Oxford method may be a statistical bias in the dataset.\n\nWe turn now to the false positive detections -bounding boxes which do not correspond to an object of interest. For each class and method we show high-ranked false positive detections. To increase the diversity of results presented 28 we have filtered the images shown: (i) images with any (undetected) object of interest have been removed, though see discussion of the \"person\" class below; (ii) only the most confident false positive detection per image is shown. For example in Fig. 22b, multiple high confidence false positives were output for the 3rd image due to the repeated structure. Fig. 22 shows false positives for the \"bicycle\" class output by the Oxford, UoCTTI and INRIA PlusClass methods. All methods generated some \"intuitive\" false positives on motorbikes, though in many such cases the predicted bounding box does not correspond to the full extent of the object. It is interesting to observe that several of the false positives for the Oxford method are the same images of tree branches which confused the classification methods (Fig. 12b). This may be explained by the pyramid representation of spatial information in this method or by the method learning the strong gradients of the frames (if you squint you can see a frame in the second and fourth images of Fig. 22a). For the UoCTTI method, the false positives were observed to often resemble the side-view of a bicycle as \"two blobs horizontally aligned\" (Fig. 22b, 3rd and 4th image). The most confident false positive output by this method is actually a drawing of a bicycle on a traffic sign, not labelled as \"bicycle\" according to the annotation guidelines. For the IN-RIA PlusClass method, 4 of the 5 highest confidence false positive images contain motorcycles, however the poor prediction of bounding boxes in these cases suggest that the scene context introduced by the incorporation of a global image classifier in this method may be a factor, rather than a \"natural\" confusion between the classes. Fig. 23 shows corresponding high ranked false positives for the \"motorbike\" class, with the same methods as for the \"bicycle\" class shown. For the UoCTTI method, 4 out of 5 false positives are bicycles, with the remaining false positive shown covering a pair of car wheels. These results suggest that this method is really capturing something about the dominant shape of the motorbike. The Oxford method outputs two bicycles in the first five false positives, and the INRIA PlusClass method outputs one. The remaining high ranked false positives for these two methods are difficult to explain, consisting mainly of highly cluttered scenes with little discernable structure. Fig. 24 shows high ranked false positives for the \"person\" class, with the three most successful methods shown: IRISA, UoCTTI and INRIA Normal. This class is particularly challenging because of the high variability in human pose exhibited in the VOC dataset. As can be seen, it is difficulty to see any consistent property of the false positives. Some bias toward \"elongated vertical\" structures can be observed e.g. trees (Fig. 24a) and dogs in a frontal pose (Fig. 24b), and more of these were visible in lower ranked false positives not shown. However, many false positives seem to be merely cluttered windows with strong tex-ture. The fifth false positive output by the IRISA method (Fig. 24a) is interesting (motorbike panniers occluded by another motorbike) and is most likely an artefact of that method learning separate \"person on X\" detectors.\n\nThus far the false positives shown exclude images where any object of interest is present. Fig. 25 shows false positives for the \"person\" class, where people are present in the image. These represent \"near miss\" detections where the predicted bounding box does not meet the VOC overlap criterion of 50%. As noted, this class presents particular challenges because of the high variability in pose and articulation. As can be seen in Fig. 25a/b, the localisation of these false positives for the IRISA and UoCTTI methods is generally quite good, e.g. the top of the bounding box matches the top of the head, and the person is horizontally centred in the bounding box. The failure modes here are mainly inaccurate prediction of the vertical extent of the person ( Fig. 25a and Fig. 25b, 1st and 2nd image) due e.g. to occlusion, and failure on non-frontal poses (Fig. 25b, 1st and 5th images). This is a limitation of current methods using fixed aspect ratio bounding boxes, which is a poor model of the possible imaged appearance of a person. The false positive in Fig. 25a, 1st image, is accounted for by the \"person on X\" approach taken by the IRISA method. The high ranked near misses output by the INRIA Normal method (Fig. 25c) mostly cover multiple people, and might be accounted for by capturing person \"texture\" but not layout.\n\nAs noted in Sect. 4.2.2, in the 2007 challenge we introduced a \"person layout\" taster to further evaluate the ability of person detectors to correctly \"parse\" images of people, and motivate research into methods giving more detailed interpretation of scenes containing people.\n\n\nEffect of object size on detection accuracy\n\nAs in the classification task, it is interesting to investigate how object size affects the accuracy of the submitted detection methods, particularly for those such as the Oxford method which makes use of interest point detection, which may fail for small objects, and the INRIA PlusClass method which combines sliding window detection with a whole image classifier.\n\nWe followed a corresponding procedure to that for the classification task, creating a series of test sets in which all objects smaller than a threshold area were removed from consideration. For the detection task, this was done by adding a \"difficult\" annotation to such objects, so that they count neither as false positives or negatives. Fig. 26 shows results of the experiments for a representative set of classes. For each plot, the x-axis shows the threshold on object size, as a proportion of image size, for an object to be included in the evaluation. For each threshold, the AP was measured and is shown on the y-axis. For each plot the x-axis shows the lower threshold on object size for an object to be included in the test set; the y-axis shows the corresponding AP. A threshold of 30% means that all objects smaller than 30% of the image area were ignored in the evaluation (contributing neither true nor false positives); a threshold of 0% means that no objects were ignored.\n\nAs Fig. 26 shows, all submitted methods have lower AP for very small objects below around 2% image area. For the \"car\" class ( Fig. 26a), the accuracy of most methods does not increase if objects less than 5% area are removed from the evaluation, indicating limited preference for large objects. For the INRIA Normal and IRISA methods the AP can be seen to fall slightly with increasing object size, indicating that some highly ranked correct detections are for small objects. For the \"motorbike\" class ( Fig. 26b), the IN-RIA PlusClass and MPI ESSOL methods peak for objects above around 17% image area, the IRISA and UoCTTI methods show no clear preference for large objects, and AP for all other methods increases monotonically with object size.\n\nThree methods show substantial correlation between object size and AP: MPI Center, MPI ESSOL and Oxford. The MPI Center method outputs a fixed bounding box with area 51% of the image, and confidence determined by a global image classifier. This clearly biases results to images where most of the image is covered by the object of interest, and while an interesting baseline (as intended), is not a successful strategy since many of the objects in the dataset are small. The MPI ESSOL method has two aspects which may bias it toward larger objects: (i) it combines a whole image classifier with a sliding window detector to \"score\" detections; (ii) it incorporates a log Gaussian prior on object size, fit by maximum likelihood to the training data, and this prior may have biased results toward large objects. The Oxford method relies on scale-invariant interest point operators to provide candidate detections, and the lack of interest points on small objects may explain the correlation between its accuracy and object size.\n\n\nDetection results on the VOC2006 test set\n\nAs in the classification task, participants in the detection task of the 2007 challenge were asked to additionally submit re-sults on the VOC2006 dataset, trained using the VOC2007 data.\n\nParticipants submitted classification results on both test sets for 3 methods. Table 7 summarises the results, showing the AP (%) obtained for each method, class and test set. The final row shows the maximum AP obtained by any method in the 2006 challenge, trained on the VOC2006 data. Since participants submitted results for different subsets of classes, the results have not been summarised e.g. by median AP as in the classification task.\n\nFor all but one class the ranking of methods by AP is the same for the VOC2007 and VOC2006 datasets, suggesting that methods performing well on VOC2007 are \"intrinsically\" more successful, rather than obtaining good results due to excessive fitting of the statistics of the VOC2007 data. For the \"cat\" class, the UoCTTI method comes first and the Oxford method second, reversing the order on VOC2007, but the difference in AP is small (53.5% vs. 55.5%).\n\nParticularly encouraging is the observation that for 7 out of 10 classes a method submitted in 2007 achieves greater AP than any of the 2006 methods. The UoCTTI method exceeds the 2006 results on 7 of the 10 classes entered, the Oxford method on all four classes entered, and the IRISA method on 4 of the 8 classes entered. The improvement is substantial, e.g. 19.1% AP on the \"bus\" class (Oxford) and 9.8% on the \"person\" class (UoCTTI). While it is possible that the improvement is due to the VOC2007 training/validation data being more \"useful\", this effect was not observed for the classification task. It therefore seems likely that the results represent measurable progress in object detection.\n\n\nSegmentation\n\nAll participants in the detection challenge were automatically entered into the segmentation challenge by deriving Table 7 Comparison of detection results on VOC2006 vs. VOC2007 test sets. All methods were trained on the VOC2007 training/validataion set. The AP measure (%) is shown for each method, class, and test set. The final row (VOC2006) lists the best result obtained in the VOC2006 challenge, trained using the VOC2006 training/validation set. Bold entries denote the maximum AP for each dataset and class.  a segmentation from the inferred bounding boxes (overlaps were resolved heuristically). In addition, only one segmentation-specific method was submitted, by Lubor Ladicky, Pushmeet Kohli and Philip Torr of Oxford Brookes University (Kohli et al 2008). Example segmentations from this team and from the TKK automatic entry are shown in Fig. 27. The best overall performance was given by one of the \"automatic\" participants (segmentation derived algorithmically from detection), most likely due to an unfinished segmentation-only entry. In future challenges, it is anticipated that methods which are optimised for the segmentation problem will outperform automatic detection entries. In any case, providing a challenge which directly compares detection and segmentation methods should help encourage innovation in how to combine these two types of methods to best effect.\n\n\nPerson Layout\n\nOnly one result was submitted for the person layout taster, by Martin Bergtholdt, J\u00f6rg Hendrik Kappes and Christoph Schn\u00f6rr of the University of Mannheim (Bergtholdt et al 2006). Fig. 28 shows some example results for this method.  For some images, where the person is in a \"canonical\" frontal pose, the method successfully localises the parts (Fig. 28a). For more varied poses, the method fails to predict the correct locations, or confuses hands and feet (Fig. 28b). Despite some correct results, the ranking of results by the method's confidence output is poor, such that the measured AP is zero. This raised the question of whether the evaluation criterion adopted for VOC2007 is sufficiently sensitive, and as described in Sect. 4.2.2, the requirements for the person layout taster have been relaxed for the VOC2008 challenge.\n\n\nDiscussion and the Future\n\nThe VOC challenge has already had a significant, and we believe positive, impact in terms of providing a rich, standardised dataset for the community and an evaluation framework for comparing different methods. Participation in the challenges has increased steadily since their first introduc-31 tion, as has the use of the VOC dataset beyond the challenge itself. For example, in CVPR07 the VOC dataset was referenced in 15 papers; in CVPR08 this number increased to 27, i.e. almost a year-to-year doubling of the dataset's popularity. To retain this popularity, the challenge must evolve to meet the requirements and address the criticisms of the research community. In the following sections we discuss some criticisms of the challenge, look at how the challenge is evolving through the taster competitions, and suggest directions in which the dataset and challenge can be improved and extended in the future.\n\n\nCriticisms\n\nNo benchmark remains without criticism for long, and the VOC challenge has not been an exception. A common objection raised about any competition of this sort is that: \"Datasets stifle innovation, because the community concentrates effort on this data to the exclusion of others\". While it is difficult to avoid this effect completely, if the challenge is well ahead of capabilities then it will not necessarily stifle the types of methods used. Datasets have a shelf life, and as performance starts to saturate a new one is needed to drive research. Conversely, it is also necessary for datasets to remain consistent, so that they can be used to gauge progress made by the community. Assessing progress is difficult if the test (and training) set are different every time the challenge is run. The VOC challenge aims to meet these apparently contradictory goals of innovation and consistency by introducing separate \"taster\" challenges to promote research in new directions (see the next section), while retaining the existing classification and detection competitions so that progress can be consistently tracked. Fostering innovation is also a question of the attitude of the community as a whole: it is important that we do not discourage novel approaches to object recognition simply because they do not yet achieve the greatest success as measured by our benchmarks. High methodological novelty must not be sacrificed on the altar of benchmark ranking, and this is the last thing the VOC challenge is intended to achieve. An important part of encouraging novel methods is our selection of speakers for the annual challenge workshop, where we have given time to both particularly successful, and particularly interesting methods.\n\nA further criticism raised against the VOC series of challenges in particular is that the level of difficulty is too high, thereby obscuring the way forward. However, methods submitted in 2007 for the detection task demonstrated substantial improvements over those submitted in 2006 (see Sect. 6.2.6). We are of the opinion that providing researchers with such challenging, yet natural, data is only stimulating progress. It is the very fact of being well ahead of current capabilities which makes the dataset so useful.\n\nIn contrast, datasets for which performance is \"saturated\" are likely to encourage fine tuning of implementation details rather than fundamental progress, and such progress may be unmeasurable, being lost in the noise.\n\nA fundamental question is whether the VOC challenges are probing for the right kind of tasks. In a recent paper, Pinto et al (2008) criticised the use of \"natural\" images altogether, arguing for the use of synthetic data (e.g. rendered 3D models) for which one has better control over the variability in the data -parameter settings can be sampled at will and annotation is not needed, as perfect ground truth is available by design. In their view, this is a much better way to generate the variability that is needed to critically test recognition performance. This issue of whether to use natural images or completely control imaging conditions is an ongoing debate in the psychophysics community. In our case, the VOC datasets have been designed to contain large variability in pose, illumination, occlusion, etc. Moreover, correlations that occur in the real world are captured, whereas synthetic datasets cannot be expected to reflect those faithfully.\n\n\nTaster competitions\n\nThe taster competitions, which make demands of methods quite far ahead of the state-of-the-art, aim to play a key part in encouraging fundamental research progress. These were introduced in 2007 to encourage both diversity of approach and the development of more powerful methods to address these more demanding tasks. For example, the segmentation competition not only requires much more precise localisation of objects than the detection task, but it has also been set up to allow either detection-based or segmentation-based approaches to be used. The hope is that the two approaches are complementary, so that detection methods can be used to improve segmentation performance and vice-versa. This belief is justified by the similar situation which has already arisen between the classification and detection tasks, where global image classification has aided detection performance (see Sect. 5.2). By encouraging participants to blend the best aspects of different methodologies, a greater diversity of approaches will be encouraged.\n\nIt is inevitable that any challenge is very much of its time, only testing what can be thought of by current practitioners, governed by current methods and hardware, and to some extent unaware of these limitations. Through the use of the taster competitions, the VOC challenge is being updated to allow a broader range of approaches and to address more current research issues. However, it is recognised that the challenge must continue to adapt and remain agile in responding to the needs and concerns of the growing community of researchers who use the datasets and participate in the competitions.\n\nFig. 5\n5Example images and annotation for the taster competitions. (a) Segmentation taster annotation showing object and class segmentation. Border regions are marked with the \"void\" label indicating that they may be object or background. Difficult objects are excluded by masking with the 'void' label. (b) Person Layout taster annotation showing bounding boxes for head, hands and feet.\n\nFig. 4\n4Summary of the entire VOC2007 dataset. Histogram by class of the number of objects and images containing at least one object of the corresponding class. Note the log scale.\n\na e r o p l a n e b i c y c l e b i r d b o a t b o t t l e b u s c a r c a t c h a i r c o w d i n i n g t a b l e d o g h o r s e m o t o r b i k e p e r s o n p o t t e d p l a n t s h e e p s o f a t r a i n t v m o n i t\n74.8 62.5 51.2 69.4 29.2 60.4 76.3 57.6  53.1 41.1 54.0 42.8 76.5 62.3 84.5 35.3 41.3 50.1 77.6 49.3 INRIA Genetic 77.\n\nFig. 7\n7Summary of the classification results by method. For each method the median AP over all classes is shown.\n\nFig. 6\n6Classification results. Precision/recall curves are shown for a representative sample of classes. The left column shows all results; the right shows the top 5 results by AP. The legend indicates the AP (%) obtained by the corresponding method.\n\nFig. 9\n9Summary of classification results by class. For each class three values are shown: the maximum AP obtained by any method (max), the median AP over all methods (median) and the AP obtained by a random ranking of the images (chance).\n\nFig. 10 Fig. 11 Fig. 12 Fig. 13 Fig. 14\n1011121314Ranked images for the \"car\" classification task (see text for details of ranking method). (a) five highest ranked positive images (containing cars); (b) five lowest ranked positive images (containing cars); (c) five highest ranked negative images (not containing cars). The number in each image indicates the corresponding median rank.(a) cat: high ranked positive images Ranked images for the \"cat\" classification task. (a) five highest ranked positive images (containing cats); (b) five highest ranked negative images (not containing cats). The number in each image indicates the corresponding median rank.(a) bicycle: highest ranked positive images Ranked images for the \"bicycle\" classification task. (a) five highest ranked positive images (containing bicycles); (b) five highest ranked negative images (not containing bicycles). The number in each image indicates the corresponding median rank. Ranked images for the \"chair\" classification task. (a) five highest ranked positive images (containing chairs); (b) five highest ranked negative images (not containing chairs). The number in each image indicates the corresponding median rank.(a) person: highest ranked positive images Ranked images for the \"person\" classification task. (a) five highest ranked positive images (containing people); (b) five highest ranked negative images (not containing people). The number in each image indicates the corresponding median rank.\n\nFig. 17\n17Detection results. Precision/recall curves are shown for a representative sample of classes. The legend indicates the AP (%) obtained by the corresponding method.\n\nFig. 18\n18Summary of detection results by class. For each class two values are shown: the maximum AP obtained by any method (max) and the median AP over all methods (median). method obtains good results on \"bottle\" (AP=21.4), where it is expected that shape is a very important feature, and the MPI ESSOL method fails (AP=0.1). The trained detector used by the UoCTTI method (Felzenszwalb et al 2008) has been made publicly available, and would form a reasonable state-of-the-art baseline for future challenges.\n\nFig. 20 Fig. 21\n2021Highest ranked true positive detections for the \"car\" detection task. The five highest ranked true positives are shown for each of the three methods with greatest AP. The number in each image indicates the rank of the detection.(a) bicycle true positives: Oxford method Highest ranked true positive detections for the \"bicycle\" detection task. The five highest ranked true positives are shown for each of the three methods with greatest AP. The number in each image indicates the rank of the detection.\n\nFig. 22 Fig. 23\n2223(Darmstadt and INRIA PlusClass) the ordering of methods by AP does not change for any threshold in the range 0High ranked false positive detections for the \"bicycle\" detection task. The false positives shown are in images where no bicycles are present. The number in each image indicates the rank of the detection. Results are shown for the three methods with greatest AP. (a) motorbike false positives: Oxford method High ranked false positive detections for the \"motorbike\" detection task. The false positives shown are in images where no motorbikes are present. The number in each image indicates the rank of the detection. Results are shown for the three methods with greatest AP.\n\nFig. 24 Fig. 25 \"\n2425High ranked false positive detections for the \"person\" detection task. The false positives shown are in images where no people are present. The number in each image indicates the rank of the detection. Results are shown for the three methods with greatest AP.(a) person \"near misses\": IRISA method Near miss\" false positives for the \"person\" detection task. The images shown contain people, but the detections do not satisfy the VOC overlap criterion. The number in each image indicates the rank of the detection. Results are shown for the three methods with greatest AP.\n\nFig. 26\n26Detection results as a function of object size. Plots show results for two representative classes.\n\nFig. 28\n28Example person layout results for the Mannheim method. For each image the ground truth bounding boxes are shown in white, and the predicted bounding boxes colour-coded: yellow for \"head\" and cyan for \"hand\".\n\n\nFig. 1 Example images from the VOC2007 dataset. For each of the 20 classes annotated, two examples are shown. Bounding boxes indicate all instances of the corresponding class in the image which are marked as \"non-difficult\" (see Sect. 3.3) -bounding boxes for the other classes are available in the annotation but not shown. Note the wide range of pose, scale, clutter, occlusion and imaging conditions.4 \n\naeroplane \nbicycle \nbird \nboat \nbottle \n\nbus \ncar \ncat \nchair \ncow \n\ndining table \ndog \nhorse \nmotorbike \nperson \n\npotted plant \nsheep \nsofa \ntrain \nTV/monitor \n\n5 \n\n\n\nTable 4\n4Participation in the VOC2007 challenge. Each method is assigned an abbreviation used in the text, and identified as a classification method (Cls) or detection method (Det). The contributors to each method are listed with references to publications describing the method, where available.Abbreviation \nCls Det Contributors \nReferences \n\nDarmstadt \n-\n\u2022 \nMario Fritz and Bernt Schiele, \nTU Darmstadt \n\nFritz and Schiele (2008) \n\nINRIA Flat \n\u2022 \n-\nMarcin Marszalek, Cordelia Schmid, \nHedi Harzallah and Joost Van-de-weijer, \nINRIA Rhone-Alpes \n\nZhang et al (2007); van de Weijer and \nSchmid (2006); Ferrari et al (2008) \nINRIA Genetic \n\u2022 \n-\n\nINRIA Larlus \n\u2022 \n-\nDiane Larlus and Frederic Jurie, \nINRIA Rhones-Alpes \n\n-\n\nINRIA Normal \n-\n\u2022 \nHedi Harzallah, Cordelia Schmid, Marcin \nMarszalek, Vittorio Ferrari, Y-Lan \nBoureau, Jean Ponce and Frederic Jurie, \nINRIA Rhone-Alpes \n\nFerrari et al (2008); van de Weijer and \nSchmid (2006); Zhang et al (2007) \nINRIA PlusClass \n-\n\u2022 \n\nIRISA \n-\n\u2022 \nIvan Laptev, IRISA/INRIA Rennes and \nEvgeniy Tarassov, TT-Solutions \n\nLaptev (2006) \n\nMPI BOW \n\u2022 \n-\nChristoph Lampert and Matthew \nBlaschko, MPI Tuebingen \nLampert et al (2008) \nMPI Center \n-\n\u2022 \nMPI ESSOL \n-\n\u2022 \n\nOxford \n-\n\u2022 \nOndrej Chum and Andrew Zisserman, \nUniversity of Oxford \n\nChum and Zisserman (2007) \n\nPRIPUVA \n\u2022 \n-\nJulian Stottinger and Allan Hanbury, \nVienna University of Technology; Nicu \nSebe and Theo Gevers, University of \nAmsterdam \n\nStoettinger et al (2007) \n\nQMUL HSLS \n\u2022 \n-\nJianguo Zhang, Queen Mary University \nof London \n\nZhang et al (2007) \nQMUL LSPCH \n\u2022 \n-\n\nTKK \n\u2022 \n\u2022 \nVille Viitaniemi and Jorma Laaksonon, \nHelsinki University of Technology \n\nViitaniemi and Laaksonen (2008) \n\nToshCam rdf \n\u2022 \n-\nJamie Shotton, Toshiba Corporate R&D \nCenter, Japan & Matthew Johnson, \nUniversity of Cambridge \n\n-\nToshCam svm \n\u2022 \n-\n\nTsinghua \n\u2022 \n-\nDong Wang, Xiaobing Liu, Cailiang Liu, \nZhang Bo and Jianmin Li, Tsinghua \nUniversity \n\nWang et al (2006); Liu et al (2007) \n\nUoCTTI \n-\n\u2022 \nPedro Felzenszwalb, University of \nChicago; David McAllester and Deva \nRamanan, Toyota Technological Institute, \nChicago \n\nFelzenszwalb et al (2008) \n\nUVA Bigrams \n\u2022 \n-\n\nKoen van de Sande, Jan van Gemert and \nJasper Uijlings, University of Amsterdam \n\nvan de Sande et al (2008); van Gemert \net al (2006); Geusebroek (2006); Snoek \net al (2006, 2005) \n\n\n\nTable 5\n5Classification results. For each object class and submission, the AP measure (%) is shown. Bold entries in each column denote the maximum AP for the corresponding class. Italic entries denote the results ranked second or third.\n\n\nFig. 27Example segmentation results. Columns show: test images, ground truth annotations, segmentations fromKohli et al (2008) and segmentations derived from the bounding boxes of the TKK detection method.Test on 2007 \n\nIRISA \n28.1 \n-31.8 \n2.6 11.9 \n-28.9 22.7 22.1 17.5 \nOxford \n40.9 39.3 43.2 \n-\n-\n-\n-37.5 \n-\n-\nUoCTTI \n36.9 23.2 34.6 \n9.8 14.0 \n2.3 18.2 27.6 21.3 14.3 \n\nTest on 2006 \n\nIRISA \n35.2 \n-48.2 \n9.4 20.9 \n-18.3 33.3 21.1 26.2 \nOxford \n56.8 36.0 53.5 \n-\n-\n-\n-53.9 \n-\n-\nUoCTTI \n56.2 23.6 55.5 10.3 21.2 \n9.9 17.3 43.9 26.2 22.1 \n\nVOC2006 \nBest \n44.0 16.9 44.4 16.0 25.2 11.8 14.0 39.0 16.4 25.1 \n\nImage \nGround truth \nKohli et al \nTKK \n\n\nhttp://www-nlpir.nist.gov/projects/trecvid/ 4 The number of images quoted is the number that are freely available.\nMATLAB R is a registered trademark of The MathWorks, Inc.\nAcknowledgementsThe preparation and running of the VOC challenge is supported by the EU-funded PASCAL Network of Excellence on Pattern Analysis, Statistical Modelling and Computational Learning.We gratefully acknowledge the VOC2007 annotators: Moray Allan, Patrick Buehler, Terry Herbert, Anitha Kannan, Julia Lasserre, Alain Lehmann, Mukta Prasad, Till Quack, John Quinn, Florian Schroff. We are also grateful to James Philbin, Ondra Chum, and Felix Agakov for additional assistance.Finally we would like to thank the anonymous reviewers for their detailed and insightful comments.7.3 The FutureIn the area of object class recognition, a lot of progress is being made and the requirements for a benchmark evolve quickly with this evolution. Here we give a non-exhaustive list of aspects which could be improved or added in future VOC challenges.More object classes. A first and rather obvious extension is to increase the number of annotated object classes. A primary goal here is to put more emphasis on the issue of scalability -running as many detectors as there are object classes may not remain a viable strategy, although this is by far the dominant approach today. Different aspects of detection schemes may become important, for example the ability to share features between classes(Torralba et al 2007), or exploit properties of multiple \"parent\" classes(Zehnder et al 2008). Introducing more classes would also stimulate research in discrimination between more visually similar classes, and in exploiting semantic relations between classes, for example in the form of a class hierarchy. However, increasing the number of classes will also pose additional difficulties to the running of the VOC challenge: (i) it will prove more difficult to collect sufficient data per class; (ii) it raises questions of how to annotate objects accurately, for example labelling an object as \"van\" vs. \"truck\" is often subjective; (iii) evaluation of recognition must be more flexible, for example a method might assign a class from {hatchback, car, vehicle} and be assigned varying \"scores\" dependent on accuracy or level of detail.Object parts. VOC2007 introduced annotation of body parts in order to evaluate and encourage development of methods capable of more detailed image annotation than object location alone. Such more detailed indication of the parts of objects is an important direction to pursue. Although many techniques today start from local features, these features typically have very little to do with the semantic parts of the objects. However, often the purpose of object detection and recognition is to support interaction with objects (e.g. in robotics). A good understanding of where parts are (arms, wheels, keyboards, etc.) is often essential to make such practical use of object recognition, and should be incorporated into at least a component of the evaluation scheme.Thus far, VOC has confined itself to object classes and annotation where \"discrete\" objects can be identified. With the introduction of the segmentation taster, it is natural to also include \"stuff\" classes (grass, sky, etc.) and additionally consider annotation of classes which can appear as \"stuff\" in the distance e.g. \"person\" vs. \"crowd\" -images containing such ambiguities are currently omitted from the VOC dataset.Beyond nouns. Increasingly, vision researchers are forging strong links with text analysis, and are exploiting tools coming from that area such as WordNet(Fellbaum 1998). Part of this endeavour is to build vision systems that can exploit and/or generate textual descriptions of scenes. This entails bringing objects (nouns) in connection with actions (verbs) and attributes (adjectives and adverbs). As progress in this direction continues, it will be appropriate to introduce benchmarks for methods producing richer textual descriptions of a scene than the \"noun + position\" outputs which are currently typical. The interest in methods for exploiting textual description at training time also suggests alternative weaker forms of annotation for the dataset than bounding boxes; we discuss this further below.Scene dynamics. Thus far, the VOC challenge has focused entirely on classifying and detecting objects in still images (also the case for VOC2008). Including video clips would expand the challenge in several ways: (i) as training data it would support learning richer object models, for example 3D or \"multi-aspect\". Video of objects with varying viewing direction would provide relations between parts implicitly available through tracking; (ii) as test data it would enable evaluation of new tasks: object recognition from video (e.g. people), and recognition of actions. This would also bring the VOC challenge into the domain of other benchmarks, e.g. TRECVID which includes an \"interactive search\" task with increasing emphasis on events/actions such as \"a door being opened\" or \"a train in motion\".Alternative annotation methods. Manual annotation is timeconsuming and therefore expensive. For example, annotation of the VOC2008 dataset required around 700 person hours. Moreover, since the VOC challenge runs annually, new test data is required each year in order to avoid participants having access to the ground truth annotations and over-fitting on the test set. Increasing the level of annotation, for example by increasing the number of classes, only makes annotation more time-consuming.We also found that when increasing the number of classes, from 10 in 2006 to 20 in 2007, annotators made many more mistakes as they failed to hold in memory the complete set of classes to be annotated. This in turn required more time to be allocated to checking and correction to ensure high quality annotation. This raises several questions concerning: how the annotation format relates to ease-ofannotation, how much agreement there is between different human annotators e.g. on bounding box position, and how the annotation tools affect annotation quality. To date, we have not yet gathered data during the checking process that could help answer these questions and this is something we aim to rectify in future years. Annotating pixel-wise segmentations instead of bounding boxes puts even higher pressure on the sustainability of manual annotation. If object 33 parts, attributes and especially video are to be added in future, then the method of annotation will certainly need to evolve in concert with the annotation itself. Possibilities include recruiting help from a much larger pool of volunteers (in the footsteps of LabelMe), combined with a centralised effort to check quality and make corrections. We are also investigating the use of systems like Mechanical Turk to recruit and pay for annotation(Sorokin and Forsyth 2008;Spain and Perona 2008). Alternatively, commercial annotation initiatives could be considered, like the aforementioned Lotus Hill dataset(Yao et al 2007), in combination with sampled quality inspection.As noted above, there has recently been considerable interest in learning recognition from \"weak\" supervision(Duygulu et al 2002;Fergus et al 2007). This suggests alternative forms of annotation which could be introduced, for example per-image annotation with keywords or phrases (e.g. \"red car in a street scene\"). Such annotation could be provided alone for some images, in addition to a set of images with more precise annotation, providing complementary supervision for training at low cost. Another possibility for \"lighter\" annotation is to collect (possibly additional) training images directly from a web search engine (such as Google image search)(Fergus et al 2005). The additional complication here is that such data is typically noisy, in that only a subset of the images are relevant to the supplied search terms.The future is bright. There has been tremendous progress in object class recognition this decade. At the turn of the millennium, few would have dreamt that by now the community would have such impressive performance on both classification and detection for such varied object classes as bicycles, cars, cows, sheep, and even for the archetypal functional class of chairs. This progress has gone hand in hand with the development of image databases -which have provided both the training data necessary for researchers to work in this area; and the testing data necessary to track the improvements in performance. The VOC challenge has played a vital part in this endeavour, and we hope that it will continue to do so.\nLearning of graphical models and efficient inference for object class recognition. M Bergtholdt, J Kappes, C Schn\u00f6rr, Proceedings of the Annual Symposium of the German Association for Pattern Recognition (DAGM06). the Annual Symposium of the German Association for Pattern Recognition (DAGM06)Bergtholdt M, Kappes J, Schn\u00f6rr C (2006) Learning of graphical mod- els and efficient inference for object class recognition. In: Proceed- ings of the Annual Symposium of the German Association for Pat- tern Recognition (DAGM06), pp 273-283\n\nScalable near identical image and shot detection. O Chum, A Zisserman, J Philbin, M Isard, A Zisserman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Chum O. the IEEE Conference on Computer Vision and Pattern Recognition Chum OProceedings of the International Conference on Image and Video RetrievalChum O, Zisserman A (2007) An exemplar model for learning object classes. In: Proceedings of the IEEE Conference on Computer Vi- sion and Pattern Recognition Chum O, Philbin J, Isard M, Zisserman A (2007) Scalable near identi- cal image and shot detection. In: Proceedings of the International Conference on Image and Video Retrieval, pp 549-556\n\nVisual categorization with bags of keypoints. G Csurka, C Bray, C Dance, L Fan, Workshop on Statistical Learning in Computer Vision, ECCV. Csurka G, Bray C, Dance C, Fan L (2004) Visual categorization with bags of keypoints. In: Workshop on Statistical Learning in Com- puter Vision, ECCV, pp 1-22\n\nHistograms of Oriented Gradients for Human Detection. N Dalal, B Triggs, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDalal N, Triggs B (2005) Histograms of Oriented Gradients for Human Detection. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 886-893\n\nStatistical comparisons of classifiers over multiple data sets. J Demsar, Journal of Machine Learning Research. 7Demsar J (2006) Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research 7:1-30\n\nObject recognition as machine translation: Learning a lexicon for a fixed image vocabulary. P Duygulu, K Barnard, N De Freitas, D A Forsyth, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionDuygulu P, Barnard K, de Freitas N, Forsyth DA (2002) Object recog- nition as machine translation: Learning a lexicon for a fixed image vocabulary. In: Proceedings of the European Conference on Com- puter Vision, pp 97-112\n\nThe 2005 PASCAL visual object classes challenge. M Everingham, A Zisserman, Cki Williams, L Van Gool, Machine Learning Challenges -Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment, LNAI. Springer3944Everingham M, Zisserman A, Williams CKI, Van Gool L (2006a) The 2005 PASCAL visual object classes challenge. In: Machine Learn- ing Challenges -Evaluating Predictive Uncertainty, Visual Ob- ject Classification, and Recognising Textual Entailment, LNAI, vol 3944, Springer, pp 117-176\n\nThe PASCAL Visual Object Classes challenge 2006 (VOC2006) results. M Everingham, A Zisserman, Cki Williams, L Van Gool, Everingham M, Zisserman A, Williams CKI, Van Gool L (2006b) The PASCAL Visual Object Classes challenge 2006 (VOC2006) results.\n\nThe PASCAL Visual Object Classes Challenge. M Everingham, L Van Gool, Cki Williams, J Winn, A Zisserman, Everingham M, Van Gool L, Williams CKI, Winn J, Zisser- man A (2007) The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-network.org/ challenges/VOC/voc2007/index.html\n\nOne-shot learning of object categories. L Fei-Fei, R Fergus, P Perona, IEEE Transactions on Pattern Analysis and Machine Intelligence. 284Fei-Fei L, Fergus R, Perona P (2006) One-shot learning of object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence 28(4):594-611, http://www.vision.caltech.edu/ Image_Datasets/Caltech101/Caltech101.html\n\nWeakly supervised scaleinvariant learning of models for visual recognition. C Fellbaum, P Felzenszwalb, D Mcallester, D ; Ramanan, L Fei-Fei, P Perona, A Zisserman, Proceedings of the International Conference on Computer Vision Fergus R, Perona P, Zisserman A. the International Conference on Computer Vision Fergus R, Perona P, Zisserman AMIT Press71A discriminatively trained, multiscale, deformable part modelFellbaum C (ed) (1998) WordNet: an electronic lexical database. MIT Press Felzenszwalb P, McAllester D, Ramanan D (2008) A discriminatively trained, multiscale, deformable part model. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Fergus R, Fei-Fei L, Perona P, Zisserman A (2005) Learning object categories from google's image search. In: Proceedings of the In- ternational Conference on Computer Vision Fergus R, Perona P, Zisserman A (2007) Weakly supervised scale- invariant learning of models for visual recognition. International Journal of Computer Vision 71(3):273-303\n\nGroups of adjacent contour segments for object detection. V Ferrari, L Fevrier, F Jurie, C Schmid, IEEE Transactions on Pattern Analysis and Machine Intelligence. 301Ferrari V, Fevrier L, Jurie F, Schmid C (2008) Groups of adjacent con- tour segments for object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence 30(1):36-51\n\nDecomposition, discovery and detection of visual categories using topic models. M Fritz, B Schiele, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionFritz M, Schiele B (2008) Decomposition, discovery and detection of visual categories using topic models. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n\nCompact object descriptors from local colour invariant histograms. J Geusebroek, Proceedings of the British Machine Vision Conference. the British Machine Vision ConferenceGeusebroek J (2006) Compact object descriptors from local colour in- variant histograms. In: Proceedings of the British Machine Vision Conference, pp 1029-1038\n\nThe pyramid match kernel: Discriminative classification with sets of image features. K Grauman, T Darrell, Proceedings of the International Conference on Computer Vision. the International Conference on Computer VisionGrauman K, Darrell T (2005) The pyramid match kernel: Discrimina- tive classification with sets of image features. In: Proceedings of the International Conference on Computer Vision, pp 1458-1465\n\nCaltech-256 object category dataset. G Griffin, A Holub, P Perona, California Institute of Technology. 7694Griffin G, Holub A, Perona P (2007) Caltech-256 object cate- gory dataset. Tech. Rep. 7694, California Institute of Tech- nology, http://www.vision.caltech.edu/Image_Datasets/ Caltech256/\n\nPutting objects in perspective. D Hoiem, A A Efros, M Hebert, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHoiem D, Efros AA, Hebert M (2006) Putting objects in perspective. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 2137-2144\n\nBeyond sliding windows: Object localization by efficient subwindow search. P Kohli, L Ladicky, P ; Torr, M B Blaschko, T Hofmann, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Laptev I (2006) Improvements of object detection using boosted histograms. the IEEE Conference on Computer Vision and Pattern Recognition Laptev I (2006) Improvements of object detection using boosted histogramsProceedings of the British Machine Vision ConferenceKohli P, Ladicky L, Torr P (2008) Robust higher order potentials for enforcing label consistency. In: Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition Lampert CH, Blaschko MB, Hofmann T (2008) Beyond sliding win- dows: Object localization by efficient subwindow search. In: Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition Laptev I (2006) Improvements of object detection using boosted his- tograms. In: Proceedings of the British Machine Vision Confer- ence, pp 949-958\n\nBeyond bags of features: Spatial pyramid matching for recognizing natural scene categories. S Lazebnik, C Schmid, J Ponce, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLazebnik S, Schmid C, Ponce J (2006) Beyond bags of features: Spa- tial pyramid matching for recognizing natural scene categories. In: Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition, pp 2169-2178\n\nCombined object categorization and segmentation with an implicit shape model. B Leibe, A Leonardis, B Schiele, ECCV2004 Workshop on Statistical Learning in Computer Vision. Prague, Czech RepublicLeibe B, Leonardis A, Schiele B (2004) Combined object cate- gorization and segmentation with an implicit shape model. In: ECCV2004 Workshop on Statistical Learning in Computer Vision, Prague, Czech Republic, pp 17-32\n\nThe feature and spatial covariant kernel: Adding implicit spatial constraints to histogram. X Liu, D Wang, J Li, B Zhang, Proceedings of the International Conference on Image and Video Retrieval Lowe D. the International Conference on Image and Video Retrieval Lowe D60Distinctive image features from scale-invariant keypointsLiu X, Wang D, Li J, Zhang B (2007) The feature and spatial covari- ant kernel: Adding implicit spatial constraints to histogram. In: Proceedings of the International Conference on Image and Video Retrieval Lowe D (2004) Distinctive image features from scale-invariant key- points. International Journal of Computer Vision 60(2):91-110\n\nSemantic hierarchies for visual object recognition. M Marszalek, C Schmid, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMarszalek M, Schmid C (2007) Semantic hierarchies for visual object recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n\nFisher kernels on visual vocabularies for image categorization. F Perronnin, C Dance, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPerronnin F, Dance C (2007) Fisher kernels on visual vocabularies for image categorization. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n\nWhy is real-world visual object recognition hard?. N Pinto, D Cox, J Dicarlo, PLoS Computational Biology. 41Pinto N, Cox D, DiCarlo J (2008) Why is real-world visual object recognition hard? PLoS Computational Biology 4(1):151-156\n\nLabelMe: a database and web-based tool for image annotation. B Russell, A Torralba, K Murphy, W T Freeman, International Journal of Computer Vision. 771-3Russell B, Torralba A, Murphy K, Freeman WT (2008) LabelMe: a database and web-based tool for image annotation. International Journal of Computer Vision 77(1-3):157-173, http://labelme. csail.mit.edu/\n\nIntroduction to Modern Information Retrieval. G Salton, M J Mcgill, McGraw-Hill, IncNew York, NY, USASalton G, Mcgill MJ (1986) Introduction to Modern Information Re- trieval. McGraw-Hill, Inc., New York, NY, USA\n\nA taxonomy and evaluation of dense two-frame stereo correspondence algorithms. D Scharstein, R Szeliski, International Journal of Computer Vision. 471-3Scharstein D, Szeliski R (2002) A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. International Journal of Computer Vision 47(1-3):7-42, http://vision. middlebury.edu/stereo/\n\nTextonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation. J Shotton, J M Winn, C Rother, A Criminisi, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionShotton J, Winn JM, Rother C, Criminisi A (2006) TextonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation. In: Proceedings of the European Conference on Computer Vision, pp 1-15\n\nVideo Google: A text retrieval approach to object matching in videos. J Sivic, A Zisserman, Proceedings of the International Conference on Computer Vision. the International Conference on Computer Vision2Sivic J, Zisserman A (2003) Video Google: A text retrieval approach to object matching in videos. In: Proceedings of the Interna- tional Conference on Computer Vision, vol 2, pp 1470-1477, URL http://www.robots.ox.ac.uk/~vgg\n\nEvaluation campaigns and TRECVID. A F Smeaton, P Over, W Kraaij, MIR '06: Proceedings of the 8th ACM International Workshop on Multimedia Information Retrieval. Smeaton AF, Over P, Kraaij W (2006) Evaluation campaigns and TRECVID. In: MIR '06: Proceedings of the 8th ACM Interna- tional Workshop on Multimedia Information Retrieval, pp 321- 330\n\nEarly versus late fusion in semantic video analysis. C Snoek, M Worring, A Smeulders, Proceedings of the ACM International Conference on Multimedia. the ACM International Conference on MultimediaSnoek C, Worring M, Smeulders A (2005) Early versus late fusion in semantic video analysis. In: Proceedings of the ACM International Conference on Multimedia, pp 399-402\n\nThe challenge problem for automated detection of 101 semantic concepts in multimedia. C Snoek, M Worring, J Van Gemert, J Geusebroek, A Smeulders, Procedings of the First IEEE Workshop on Internet Vision. edings of the First IEEE Workshop on Internet VisionProceedings of ACM Multimedia Sorokin A, Forsyth D. at CVPR 2008Snoek C, Worring M, van Gemert J, Geusebroek J, Smeulders A (2006) The challenge problem for automated detection of 101 semantic concepts in multimedia. In: Proceedings of ACM Multimedia Sorokin A, Forsyth D (2008) Utility data annotation with amazon me- chanical turk. In: Procedings of the First IEEE Workshop on Inter- net Vision (at CVPR 2008)\n\nSome objects are more equal than others: Measuring and predicting importance. M Spain, P Perona, Proceedings of the European Conference on Computer Vision. the European Conference on Computer VisionSpain M, Perona P (2008) Some objects are more equal than others: Measuring and predicting importance. In: Proceedings of the Eu- ropean Conference on Computer Vision, pp 523-536\n\nDo colour interest points improve image retrieval?. J Stoettinger, A Hanbury, N Sebe, T Gevers, Proceedings of the IEEE International Conference on Image Processing. the IEEE International Conference on Image ProcessingStoettinger J, Hanbury A, Sebe N, Gevers T (2007) Do colour inter- est points improve image retrieval? In: Proceedings of the IEEE International Conference on Image Processing, pp 169-172\n\nDescribing visual scenes using transformed objects and parts. E B Sudderth, A B Torralba, W T Freeman, A S Willsky, International Journal of Computer Vision. 771-3Sudderth EB, Torralba AB, Freeman WT, Willsky AS (2008) Describ- ing visual scenes using transformed objects and parts. Interna- tional Journal of Computer Vision 77(1-3):291-330\n\nContextual priming for object detection. A B Torralba, International Journal of Computer Vision. 532Torralba AB (2003) Contextual priming for object detection. Interna- tional Journal of Computer Vision 53(2):169-191\n\nSharing visual features for multiclass and multiview object detection. A B Torralba, K P Murphy, Wt ; Freeman, Kea Van De Sande, T Gevers, Cgm Snoek, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition van de Weijer J. the IEEE Conference on Computer Vision and Pattern Recognition van de Weijer JSchmid C; Gemert J, Geusebroek J, Veenman C, Snoek C, Smeulders A29CVPR Workshop on Semantic Learning Applications in MultimediaTorralba AB, Murphy KP, Freeman WT (2007) Sharing visual features for multiclass and multiview object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence 29(5):854-869 van de Sande KEA, Gevers T, Snoek CGM (2008) Evaluation of color descriptors for object and scene recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition van de Weijer J, Schmid C (2006) Coloring local feature extraction. In: Proceedings of the European Conference on Computer Vision van Gemert J, Geusebroek J, Veenman C, Snoek C, Smeulders A (2006) Robust scene categorization by learning image statistics in context. In: CVPR Workshop on Semantic Learning Applications in Multimedia\n\nEvaluation of techniques for image classification, object detection and object segmentation. V Viitaniemi, J Laaksonen, TKK-ICS-R2Department of Information and Computer Science, Helsinki University of TechnologyTech. Rep.Viitaniemi V, Laaksonen J (2008) Evaluation of techniques for im- age classification, object detection and object segmentation. Tech. Rep. TKK-ICS-R2, Department of Information and Computer Sci- ence, Helsinki University of Technology, http://www.cis.hut. fi/projects/cbir/\n\nRobust Real-time Face Detection. P A Viola, M J Jones, International Journal of Computer Vision. 572Viola PA, Jones MJ (2004) Robust Real-time Face Detection. Interna- tional Journal of Computer Vision 57(2):137-154\n\nLabeling images with a computer game. L Von Ahn, L Dabbish, Proceedings of the ACM CHI. the ACM CHIvon Ahn L, Dabbish L (2004) Labeling images with a computer game. In: Proceedings of the ACM CHI, pp 319-326\n\nRelay boost fusion for learning rare concepts in multimedia. D Wang, J Li, B Zhang, Proceedings of the International Conference on Image and Video Retrieval Winn J, Everingham M (2007) The PASCAL Visual Object Classes challenge 2007 (VOC2007) annotation guidelines. the International Conference on Image and Video Retrieval Winn J, Everingham M (2007) The PASCAL Visual Object Classes challenge 2007 (VOC2007) annotation guidelinesWang D, Li J, Zhang B (2006) Relay boost fusion for learning rare concepts in multimedia. In: Proceedings of the International Con- ference on Image and Video Retrieval Winn J, Everingham M (2007) The PASCAL Visual Object Classes challenge 2007 (VOC2007) annotation guidelines.\n\nIntroduction to a large scale general purpose ground truth dataset: methodology, annotation tool, and benchmarks. B Yao, X Yang, S C Zhu, Proceedings of the 6th International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition. the 6th International Conference on Energy Minimization Methods in Computer Vision and Pattern RecognitionYao B, Yang X, Zhu SC (2007) Introduction to a large scale general purpose ground truth dataset: methodology, annotation tool, and benchmarks. In: Proceedings of the 6th International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition, http://www.imageparsing.com/\n\nEstimating average precision with incomplete and imperfect judgments. E Yilmaz, J Aslam, Fifteenth ACM International Conference on Information and Knowledge Management (CIKM). Yilmaz E, Aslam J (2006) Estimating average precision with incom- plete and imperfect judgments. In: Fifteenth ACM International Conference on Information and Knowledge Management (CIKM)\n\nLocal features and kernels for classification of texture and object categories: A comprehensive study. P Zehnder, E Koller-Meier, Van Gool, L ; Marszalek, M Lazebnik, S Schmid, C , Proceedings of the British Machine Vision Conference Zhang J. the British Machine Vision Conference Zhang J73An efficient multi-class detection cascadeZehnder P, Koller-Meier E, Van Gool L (2008) An efficient multi-class detection cascade. In: Proceedings of the British Machine Vision Conference Zhang J, Marszalek M, Lazebnik S, Schmid C (2007) Local features and kernels for classification of texture and object categories: A comprehensive study. International Journal of Computer Vision 73(2):213-238\n", "annotations": {"author": "[{\"end\":66,\"start\":53},{\"end\":78,\"start\":67},{\"end\":88,\"start\":79},{\"end\":93,\"start\":89},{\"end\":101,\"start\":94},{\"end\":118,\"start\":102},{\"end\":143,\"start\":119},{\"end\":172,\"start\":144},{\"end\":205,\"start\":173},{\"end\":231,\"start\":206}]", "publisher": null, "author_last_name": "[{\"end\":65,\"start\":55},{\"end\":77,\"start\":69},{\"end\":87,\"start\":79},{\"end\":92,\"start\":89},{\"end\":100,\"start\":96},{\"end\":117,\"start\":102}]", "author_first_name": "[{\"end\":54,\"start\":53},{\"end\":68,\"start\":67},{\"end\":95,\"start\":94}]", "author_affiliation": "[{\"end\":142,\"start\":120},{\"end\":171,\"start\":145},{\"end\":204,\"start\":174},{\"end\":230,\"start\":207}]", "title": "[{\"end\":50,\"start\":1},{\"end\":281,\"start\":232}]", "venue": "[{\"end\":331,\"start\":283}]", "abstract": "[{\"end\":1723,\"start\":680}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4139,\"start\":4109},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4349,\"start\":4329},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5264,\"start\":5244},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5785,\"start\":5765},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6928,\"start\":6908},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7846,\"start\":7823},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7960,\"start\":7944},{\"end\":8751,\"start\":8750},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13758,\"start\":13738},{\"end\":18971,\"start\":16544},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18992,\"start\":18975},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20520,\"start\":20496},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22110,\"start\":22090},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":22469,\"start\":22448},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22588,\"start\":22573},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27451,\"start\":27431},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33461,\"start\":33445},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36158,\"start\":36138},{\"end\":38282,\"start\":38280},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":38969,\"start\":38949},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39939,\"start\":39916},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":41064,\"start\":41041},{\"end\":46637,\"start\":46606},{\"end\":46797,\"start\":46768},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":47033,\"start\":47013},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":47061,\"start\":47035},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":48427,\"start\":48407},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":49106,\"start\":49080},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":49566,\"start\":49535},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":51703,\"start\":51681},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":51749,\"start\":51726},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":52306,\"start\":52281},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":52522,\"start\":52496},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":53280,\"start\":53249},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":53587,\"start\":53563},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":53841,\"start\":53823},{\"end\":53918,\"start\":53899},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":54185,\"start\":54162},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":58085,\"start\":58072},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":59375,\"start\":59362},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":67961,\"start\":67946},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":67981,\"start\":67961},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":67998,\"start\":67981},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":78106,\"start\":78085},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":96868,\"start\":96850},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":97682,\"start\":97659},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":101902,\"start\":101884},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":113521,\"start\":113503}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":104781,\"start\":104392},{\"attributes\":{\"id\":\"fig_1\"},\"end\":104963,\"start\":104782},{\"attributes\":{\"id\":\"fig_2\"},\"end\":105309,\"start\":104964},{\"attributes\":{\"id\":\"fig_3\"},\"end\":105424,\"start\":105310},{\"attributes\":{\"id\":\"fig_4\"},\"end\":105677,\"start\":105425},{\"attributes\":{\"id\":\"fig_5\"},\"end\":105918,\"start\":105678},{\"attributes\":{\"id\":\"fig_6\"},\"end\":107399,\"start\":105919},{\"attributes\":{\"id\":\"fig_10\"},\"end\":107573,\"start\":107400},{\"attributes\":{\"id\":\"fig_11\"},\"end\":108086,\"start\":107574},{\"attributes\":{\"id\":\"fig_13\"},\"end\":108610,\"start\":108087},{\"attributes\":{\"id\":\"fig_14\"},\"end\":109316,\"start\":108611},{\"attributes\":{\"id\":\"fig_15\"},\"end\":109911,\"start\":109317},{\"attributes\":{\"id\":\"fig_16\"},\"end\":110021,\"start\":109912},{\"attributes\":{\"id\":\"fig_19\"},\"end\":110240,\"start\":110022},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":110816,\"start\":110241},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":113154,\"start\":110817},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":113392,\"start\":113155},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":114043,\"start\":113393}]", "paragraph": "[{\"end\":2885,\"start\":1739},{\"end\":3253,\"start\":2887},{\"end\":3735,\"start\":3255},{\"end\":4140,\"start\":3766},{\"end\":4301,\"start\":4142},{\"end\":4864,\"start\":4303},{\"end\":5216,\"start\":4866},{\"end\":5741,\"start\":5218},{\"end\":6083,\"start\":5743},{\"end\":6855,\"start\":6085},{\"end\":7919,\"start\":6857},{\"end\":8752,\"start\":7921},{\"end\":9576,\"start\":8769},{\"end\":9790,\"start\":9596},{\"end\":10202,\"start\":9809},{\"end\":10912,\"start\":10204},{\"end\":11233,\"start\":10926},{\"end\":11554,\"start\":11257},{\"end\":11943,\"start\":11579},{\"end\":12691,\"start\":11956},{\"end\":14440,\"start\":12722},{\"end\":14764,\"start\":14442},{\"end\":16185,\"start\":14766},{\"end\":16530,\"start\":16187},{\"end\":19162,\"start\":16532},{\"end\":19610,\"start\":19164},{\"end\":20143,\"start\":19612},{\"end\":20690,\"start\":20165},{\"end\":21805,\"start\":20692},{\"end\":22979,\"start\":21807},{\"end\":23747,\"start\":23004},{\"end\":24166,\"start\":23749},{\"end\":24226,\"start\":24168},{\"end\":24919,\"start\":24228},{\"end\":25203,\"start\":24921},{\"end\":26280,\"start\":25205},{\"end\":26363,\"start\":26311},{\"end\":26673,\"start\":26365},{\"end\":27678,\"start\":26675},{\"end\":30521,\"start\":27680},{\"end\":31298,\"start\":30544},{\"end\":31795,\"start\":31322},{\"end\":32880,\"start\":31812},{\"end\":33625,\"start\":32882},{\"end\":34458,\"start\":33627},{\"end\":35582,\"start\":34476},{\"end\":35824,\"start\":35612},{\"end\":36701,\"start\":35850},{\"end\":38692,\"start\":36703},{\"end\":40098,\"start\":38718},{\"end\":40953,\"start\":40100},{\"end\":41121,\"start\":40955},{\"end\":41568,\"start\":41123},{\"end\":41765,\"start\":41613},{\"end\":42255,\"start\":41799},{\"end\":42912,\"start\":42257},{\"end\":43350,\"start\":42914},{\"end\":43938,\"start\":43396},{\"end\":44372,\"start\":43940},{\"end\":45344,\"start\":44414},{\"end\":45704,\"start\":45346},{\"end\":46136,\"start\":45747},{\"end\":46798,\"start\":46138},{\"end\":46931,\"start\":46835},{\"end\":47740,\"start\":46933},{\"end\":48455,\"start\":47742},{\"end\":48831,\"start\":48457},{\"end\":49152,\"start\":48833},{\"end\":49955,\"start\":49222},{\"end\":50348,\"start\":49957},{\"end\":50908,\"start\":50350},{\"end\":51122,\"start\":50930},{\"end\":51770,\"start\":51124},{\"end\":52599,\"start\":51772},{\"end\":53106,\"start\":52601},{\"end\":53440,\"start\":53108},{\"end\":53919,\"start\":53442},{\"end\":54186,\"start\":53931},{\"end\":55143,\"start\":54205},{\"end\":56771,\"start\":55180},{\"end\":57902,\"start\":56811},{\"end\":61489,\"start\":57904},{\"end\":62343,\"start\":61525},{\"end\":62716,\"start\":62393},{\"end\":64059,\"start\":62718},{\"end\":68348,\"start\":64061},{\"end\":69301,\"start\":68350},{\"end\":69962,\"start\":69354},{\"end\":70267,\"start\":69964},{\"end\":72088,\"start\":70269},{\"end\":73083,\"start\":72090},{\"end\":73751,\"start\":73134},{\"end\":74368,\"start\":73753},{\"end\":74980,\"start\":74370},{\"end\":75809,\"start\":74982},{\"end\":76366,\"start\":75811},{\"end\":76870,\"start\":76380},{\"end\":78341,\"start\":76902},{\"end\":78706,\"start\":78390},{\"end\":80354,\"start\":78708},{\"end\":80807,\"start\":80385},{\"end\":81576,\"start\":80809},{\"end\":82823,\"start\":81578},{\"end\":83616,\"start\":82863},{\"end\":84140,\"start\":83618},{\"end\":84429,\"start\":84185},{\"end\":85946,\"start\":84431},{\"end\":89456,\"start\":85948},{\"end\":90791,\"start\":89458},{\"end\":91069,\"start\":90793},{\"end\":91483,\"start\":91117},{\"end\":92473,\"start\":91485},{\"end\":93223,\"start\":92475},{\"end\":94251,\"start\":93225},{\"end\":94483,\"start\":94297},{\"end\":94927,\"start\":94485},{\"end\":95382,\"start\":94929},{\"end\":96084,\"start\":95384},{\"end\":97487,\"start\":96101},{\"end\":98336,\"start\":97505},{\"end\":99278,\"start\":98366},{\"end\":101027,\"start\":99293},{\"end\":101549,\"start\":101029},{\"end\":101769,\"start\":101551},{\"end\":102728,\"start\":101771},{\"end\":103789,\"start\":102752},{\"end\":104391,\"start\":103791}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":41612,\"start\":41569},{\"attributes\":{\"id\":\"formula_1\"},\"end\":41798,\"start\":41766},{\"attributes\":{\"id\":\"formula_2\"},\"end\":43395,\"start\":43351},{\"attributes\":{\"id\":\"formula_3\"},\"end\":49221,\"start\":49153},{\"attributes\":{\"id\":\"formula_4\"},\"end\":78389,\"start\":78342}]", "table_ref": "[{\"end\":14643,\"start\":14634},{\"end\":15400,\"start\":15393},{\"end\":16303,\"start\":16296},{\"end\":28560,\"start\":28553},{\"end\":28673,\"start\":28666},{\"end\":32053,\"start\":32046},{\"end\":34241,\"start\":34234},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":46505,\"start\":46498},{\"end\":53032,\"start\":53025},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":54373,\"start\":54366},{\"end\":76601,\"start\":76594},{\"end\":77434,\"start\":77427},{\"end\":94571,\"start\":94564},{\"end\":96223,\"start\":96216}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1737,\"start\":1725},{\"attributes\":{\"n\":\"1.1\"},\"end\":3764,\"start\":3738},{\"attributes\":{\"n\":\"1.2\"},\"end\":8767,\"start\":8755},{\"attributes\":{\"n\":\"2\"},\"end\":9594,\"start\":9579},{\"attributes\":{\"n\":\"2.1\"},\"end\":9807,\"start\":9793},{\"attributes\":{\"n\":\"2.2\"},\"end\":10924,\"start\":10915},{\"attributes\":{\"n\":\"2.3\"},\"end\":11255,\"start\":11236},{\"attributes\":{\"n\":\"2.4\"},\"end\":11577,\"start\":11557},{\"attributes\":{\"n\":\"3\"},\"end\":11954,\"start\":11946},{\"attributes\":{\"n\":\"3.1\"},\"end\":12720,\"start\":12694},{\"attributes\":{\"n\":\"3.2\"},\"end\":20163,\"start\":20146},{\"attributes\":{\"n\":\"3.3\"},\"end\":23002,\"start\":22982},{\"attributes\":{\"n\":\"3.4\"},\"end\":26309,\"start\":26283},{\"attributes\":{\"n\":\"3.5\"},\"end\":30542,\"start\":30524},{\"attributes\":{\"n\":\"3.6\"},\"end\":31320,\"start\":31301},{\"attributes\":{\"n\":\"3.6.1\"},\"end\":31810,\"start\":31798},{\"attributes\":{\"n\":\"3.6.2\"},\"end\":34474,\"start\":34461},{\"attributes\":{\"n\":\"4\"},\"end\":35610,\"start\":35585},{\"attributes\":{\"n\":\"4.1\"},\"end\":35848,\"start\":35827},{\"attributes\":{\"n\":\"4.2\"},\"end\":38716,\"start\":38695},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":44412,\"start\":44375},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":45745,\"start\":45707},{\"attributes\":{\"n\":\"5\"},\"end\":46808,\"start\":46801},{\"attributes\":{\"n\":\"5.1\"},\"end\":46833,\"start\":46811},{\"attributes\":{\"n\":\"5.2\"},\"end\":50928,\"start\":50911},{\"attributes\":{\"n\":\"6\"},\"end\":53929,\"start\":53922},{\"attributes\":{\"n\":\"6.1\"},\"end\":54203,\"start\":54189},{\"attributes\":{\"n\":\"6.1.1\"},\"end\":55178,\"start\":55146},{\"attributes\":{\"n\":\"6.1.2\"},\"end\":56809,\"start\":56774},{\"attributes\":{\"n\":\"6.1.3\"},\"end\":61523,\"start\":61492},{\"attributes\":{\"n\":\"6.1.4\"},\"end\":62391,\"start\":62346},{\"attributes\":{\"n\":\"6.1.5\"},\"end\":69352,\"start\":69304},{\"attributes\":{\"n\":\"6.1.6\"},\"end\":73132,\"start\":73086},{\"attributes\":{\"n\":\"6.2\"},\"end\":76378,\"start\":76369},{\"attributes\":{\"n\":\"6.2.1\"},\"end\":76900,\"start\":76873},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":80383,\"start\":80357},{\"attributes\":{\"n\":\"6.2.3\"},\"end\":82861,\"start\":82826},{\"attributes\":{\"n\":\"6.2.4\"},\"end\":84183,\"start\":84143},{\"attributes\":{\"n\":\"6.2.5\"},\"end\":91115,\"start\":91072},{\"attributes\":{\"n\":\"6.2.6\"},\"end\":94295,\"start\":94254},{\"attributes\":{\"n\":\"6.3\"},\"end\":96099,\"start\":96087},{\"attributes\":{\"n\":\"6.4\"},\"end\":97503,\"start\":97490},{\"attributes\":{\"n\":\"7\"},\"end\":98364,\"start\":98339},{\"attributes\":{\"n\":\"7.1\"},\"end\":99291,\"start\":99281},{\"attributes\":{\"n\":\"7.2\"},\"end\":102750,\"start\":102731},{\"end\":104399,\"start\":104393},{\"end\":104789,\"start\":104783},{\"end\":105190,\"start\":104965},{\"end\":105317,\"start\":105311},{\"end\":105432,\"start\":105426},{\"end\":105685,\"start\":105679},{\"end\":105959,\"start\":105920},{\"end\":107408,\"start\":107401},{\"end\":107582,\"start\":107575},{\"end\":108103,\"start\":108088},{\"end\":108627,\"start\":108612},{\"end\":109335,\"start\":109318},{\"end\":109920,\"start\":109913},{\"end\":110030,\"start\":110023},{\"end\":110825,\"start\":110818},{\"end\":113163,\"start\":113156}]", "table": "[{\"end\":110816,\"start\":110646},{\"end\":113154,\"start\":111114},{\"end\":114043,\"start\":113600}]", "figure_caption": "[{\"end\":104781,\"start\":104401},{\"end\":104963,\"start\":104791},{\"end\":105309,\"start\":105191},{\"end\":105424,\"start\":105319},{\"end\":105677,\"start\":105434},{\"end\":105918,\"start\":105687},{\"end\":107399,\"start\":105970},{\"end\":107573,\"start\":107411},{\"end\":108086,\"start\":107585},{\"end\":108610,\"start\":108108},{\"end\":109316,\"start\":108632},{\"end\":109911,\"start\":109340},{\"end\":110021,\"start\":109923},{\"end\":110240,\"start\":110033},{\"end\":110646,\"start\":110243},{\"end\":111114,\"start\":110827},{\"end\":113392,\"start\":113165},{\"end\":113600,\"start\":113395}]", "figure_ref": "[{\"end\":14438,\"start\":14432},{\"end\":14556,\"start\":14550},{\"end\":23899,\"start\":23893},{\"end\":25646,\"start\":25640},{\"end\":25830,\"start\":25824},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29954,\"start\":29948},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33343,\"start\":33336},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33778,\"start\":33771},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34759,\"start\":34752},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35303,\"start\":35296},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49039,\"start\":48948},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":54640,\"start\":54634},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":56100,\"start\":56094},{\"end\":56995,\"start\":56989},{\"end\":59266,\"start\":59260},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":60393,\"start\":60387},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":61428,\"start\":61422},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":61588,\"start\":61577},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":61708,\"start\":61697},{\"end\":63445,\"start\":63438},{\"end\":64103,\"start\":64093},{\"end\":64526,\"start\":64516},{\"end\":64896,\"start\":64886},{\"end\":65328,\"start\":65321},{\"end\":65821,\"start\":65814},{\"end\":66142,\"start\":66132},{\"end\":66466,\"start\":66456},{\"end\":67073,\"start\":67066},{\"end\":67318,\"start\":67308},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":68366,\"start\":68359},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":68449,\"start\":68440},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":69480,\"start\":69476},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":70922,\"start\":70915},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":71246,\"start\":71236},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":72215,\"start\":72207},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":72417,\"start\":72409},{\"end\":72644,\"start\":72636},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":72694,\"start\":72687},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":73839,\"start\":73832},{\"end\":74200,\"start\":74194},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":74261,\"start\":74255},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":76869,\"start\":76862},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":79222,\"start\":79215},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":79858,\"start\":79848},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":80412,\"start\":80405},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":80566,\"start\":80559},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":81034,\"start\":81025},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":82660,\"start\":82650},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":83230,\"start\":83223},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":83628,\"start\":83621},{\"end\":85047,\"start\":85040},{\"end\":85274,\"start\":85264},{\"end\":85336,\"start\":85324},{\"end\":85615,\"start\":85608},{\"end\":86436,\"start\":86428},{\"end\":86547,\"start\":86540},{\"end\":87005,\"start\":86995},{\"end\":87236,\"start\":87228},{\"end\":87391,\"start\":87377},{\"end\":87937,\"start\":87930},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":88611,\"start\":88604},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":89036,\"start\":89027},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":89075,\"start\":89065},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":89301,\"start\":89291},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":89556,\"start\":89549},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":89898,\"start\":89890},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":90227,\"start\":90219},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":90240,\"start\":90232},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":90347,\"start\":90317},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":90529,\"start\":90521},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":90687,\"start\":90678},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":91832,\"start\":91825},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":92485,\"start\":92478},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":92611,\"start\":92602},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":92989,\"start\":92980},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":96960,\"start\":96953},{\"attributes\":{\"ref_id\":\"fig_19\"},\"end\":97691,\"start\":97684},{\"attributes\":{\"ref_id\":\"fig_19\"},\"end\":97859,\"start\":97849},{\"attributes\":{\"ref_id\":\"fig_19\"},\"end\":97972,\"start\":97962}]", "bib_author_first_name": "[{\"end\":122807,\"start\":122806},{\"end\":122821,\"start\":122820},{\"end\":122831,\"start\":122830},{\"end\":123309,\"start\":123308},{\"end\":123317,\"start\":123316},{\"end\":123330,\"start\":123329},{\"end\":123341,\"start\":123340},{\"end\":123350,\"start\":123349},{\"end\":123983,\"start\":123982},{\"end\":123993,\"start\":123992},{\"end\":124001,\"start\":124000},{\"end\":124010,\"start\":124009},{\"end\":124290,\"start\":124289},{\"end\":124299,\"start\":124298},{\"end\":124688,\"start\":124687},{\"end\":124954,\"start\":124953},{\"end\":124965,\"start\":124964},{\"end\":124976,\"start\":124975},{\"end\":124990,\"start\":124989},{\"end\":124992,\"start\":124991},{\"end\":125377,\"start\":125376},{\"end\":125391,\"start\":125390},{\"end\":125406,\"start\":125403},{\"end\":125418,\"start\":125417},{\"end\":125930,\"start\":125929},{\"end\":125944,\"start\":125943},{\"end\":125959,\"start\":125956},{\"end\":125971,\"start\":125970},{\"end\":126155,\"start\":126154},{\"end\":126169,\"start\":126168},{\"end\":126183,\"start\":126180},{\"end\":126195,\"start\":126194},{\"end\":126203,\"start\":126202},{\"end\":126458,\"start\":126457},{\"end\":126469,\"start\":126468},{\"end\":126479,\"start\":126478},{\"end\":126861,\"start\":126860},{\"end\":126873,\"start\":126872},{\"end\":126889,\"start\":126888},{\"end\":126905,\"start\":126902},{\"end\":126916,\"start\":126915},{\"end\":126927,\"start\":126926},{\"end\":126937,\"start\":126936},{\"end\":127868,\"start\":127867},{\"end\":127879,\"start\":127878},{\"end\":127890,\"start\":127889},{\"end\":127899,\"start\":127898},{\"end\":128239,\"start\":128238},{\"end\":128248,\"start\":128247},{\"end\":128656,\"start\":128655},{\"end\":129007,\"start\":129006},{\"end\":129018,\"start\":129017},{\"end\":129374,\"start\":129373},{\"end\":129385,\"start\":129384},{\"end\":129394,\"start\":129393},{\"end\":129665,\"start\":129664},{\"end\":129674,\"start\":129673},{\"end\":129676,\"start\":129675},{\"end\":129685,\"start\":129684},{\"end\":130075,\"start\":130074},{\"end\":130084,\"start\":130083},{\"end\":130097,\"start\":130094},{\"end\":130105,\"start\":130104},{\"end\":130107,\"start\":130106},{\"end\":130119,\"start\":130118},{\"end\":131097,\"start\":131096},{\"end\":131109,\"start\":131108},{\"end\":131119,\"start\":131118},{\"end\":131577,\"start\":131576},{\"end\":131586,\"start\":131585},{\"end\":131599,\"start\":131598},{\"end\":132005,\"start\":132004},{\"end\":132012,\"start\":132011},{\"end\":132020,\"start\":132019},{\"end\":132026,\"start\":132025},{\"end\":132628,\"start\":132627},{\"end\":132641,\"start\":132640},{\"end\":133020,\"start\":133019},{\"end\":133033,\"start\":133032},{\"end\":133409,\"start\":133408},{\"end\":133418,\"start\":133417},{\"end\":133425,\"start\":133424},{\"end\":133651,\"start\":133650},{\"end\":133662,\"start\":133661},{\"end\":133674,\"start\":133673},{\"end\":133684,\"start\":133683},{\"end\":133686,\"start\":133685},{\"end\":133992,\"start\":133991},{\"end\":134002,\"start\":134001},{\"end\":134004,\"start\":134003},{\"end\":134239,\"start\":134238},{\"end\":134253,\"start\":134252},{\"end\":134628,\"start\":134627},{\"end\":134639,\"start\":134638},{\"end\":134641,\"start\":134640},{\"end\":134649,\"start\":134648},{\"end\":134659,\"start\":134658},{\"end\":135075,\"start\":135074},{\"end\":135084,\"start\":135083},{\"end\":135469,\"start\":135468},{\"end\":135471,\"start\":135470},{\"end\":135482,\"start\":135481},{\"end\":135490,\"start\":135489},{\"end\":135834,\"start\":135833},{\"end\":135843,\"start\":135842},{\"end\":135854,\"start\":135853},{\"end\":136233,\"start\":136232},{\"end\":136242,\"start\":136241},{\"end\":136253,\"start\":136252},{\"end\":136267,\"start\":136266},{\"end\":136281,\"start\":136280},{\"end\":136895,\"start\":136894},{\"end\":136904,\"start\":136903},{\"end\":137247,\"start\":137246},{\"end\":137262,\"start\":137261},{\"end\":137273,\"start\":137272},{\"end\":137281,\"start\":137280},{\"end\":137665,\"start\":137664},{\"end\":137667,\"start\":137666},{\"end\":137679,\"start\":137678},{\"end\":137681,\"start\":137680},{\"end\":137693,\"start\":137692},{\"end\":137695,\"start\":137694},{\"end\":137706,\"start\":137705},{\"end\":137708,\"start\":137707},{\"end\":137987,\"start\":137986},{\"end\":137989,\"start\":137988},{\"end\":138235,\"start\":138234},{\"end\":138237,\"start\":138236},{\"end\":138249,\"start\":138248},{\"end\":138251,\"start\":138250},{\"end\":138264,\"start\":138260},{\"end\":138277,\"start\":138274},{\"end\":138293,\"start\":138292},{\"end\":138305,\"start\":138302},{\"end\":139424,\"start\":139423},{\"end\":139438,\"start\":139437},{\"end\":139860,\"start\":139859},{\"end\":139862,\"start\":139861},{\"end\":139871,\"start\":139870},{\"end\":139873,\"start\":139872},{\"end\":140082,\"start\":140081},{\"end\":140093,\"start\":140092},{\"end\":140314,\"start\":140313},{\"end\":140322,\"start\":140321},{\"end\":140328,\"start\":140327},{\"end\":141077,\"start\":141076},{\"end\":141084,\"start\":141083},{\"end\":141092,\"start\":141091},{\"end\":141094,\"start\":141093},{\"end\":141700,\"start\":141699},{\"end\":141710,\"start\":141709},{\"end\":142097,\"start\":142096},{\"end\":142108,\"start\":142107},{\"end\":142126,\"start\":142123},{\"end\":142136,\"start\":142133},{\"end\":142149,\"start\":142148},{\"end\":142161,\"start\":142160},{\"end\":142171,\"start\":142170}]", "bib_author_last_name": "[{\"end\":122818,\"start\":122808},{\"end\":122828,\"start\":122822},{\"end\":122839,\"start\":122832},{\"end\":123314,\"start\":123310},{\"end\":123327,\"start\":123318},{\"end\":123338,\"start\":123331},{\"end\":123347,\"start\":123342},{\"end\":123360,\"start\":123351},{\"end\":123990,\"start\":123984},{\"end\":123998,\"start\":123994},{\"end\":124007,\"start\":124002},{\"end\":124014,\"start\":124011},{\"end\":124296,\"start\":124291},{\"end\":124306,\"start\":124300},{\"end\":124695,\"start\":124689},{\"end\":124962,\"start\":124955},{\"end\":124973,\"start\":124966},{\"end\":124987,\"start\":124977},{\"end\":125000,\"start\":124993},{\"end\":125388,\"start\":125378},{\"end\":125401,\"start\":125392},{\"end\":125415,\"start\":125407},{\"end\":125427,\"start\":125419},{\"end\":125941,\"start\":125931},{\"end\":125954,\"start\":125945},{\"end\":125968,\"start\":125960},{\"end\":125980,\"start\":125972},{\"end\":126166,\"start\":126156},{\"end\":126178,\"start\":126170},{\"end\":126192,\"start\":126184},{\"end\":126200,\"start\":126196},{\"end\":126213,\"start\":126204},{\"end\":126466,\"start\":126459},{\"end\":126476,\"start\":126470},{\"end\":126486,\"start\":126480},{\"end\":126870,\"start\":126862},{\"end\":126886,\"start\":126874},{\"end\":126900,\"start\":126890},{\"end\":126913,\"start\":126906},{\"end\":126924,\"start\":126917},{\"end\":126934,\"start\":126928},{\"end\":126947,\"start\":126938},{\"end\":127876,\"start\":127869},{\"end\":127887,\"start\":127880},{\"end\":127896,\"start\":127891},{\"end\":127906,\"start\":127900},{\"end\":128245,\"start\":128240},{\"end\":128256,\"start\":128249},{\"end\":128667,\"start\":128657},{\"end\":129015,\"start\":129008},{\"end\":129026,\"start\":129019},{\"end\":129382,\"start\":129375},{\"end\":129391,\"start\":129386},{\"end\":129401,\"start\":129395},{\"end\":129671,\"start\":129666},{\"end\":129682,\"start\":129677},{\"end\":129692,\"start\":129686},{\"end\":130081,\"start\":130076},{\"end\":130092,\"start\":130085},{\"end\":130102,\"start\":130098},{\"end\":130116,\"start\":130108},{\"end\":130127,\"start\":130120},{\"end\":131106,\"start\":131098},{\"end\":131116,\"start\":131110},{\"end\":131125,\"start\":131120},{\"end\":131583,\"start\":131578},{\"end\":131596,\"start\":131587},{\"end\":131607,\"start\":131600},{\"end\":132009,\"start\":132006},{\"end\":132017,\"start\":132013},{\"end\":132023,\"start\":132021},{\"end\":132032,\"start\":132027},{\"end\":132638,\"start\":132629},{\"end\":132648,\"start\":132642},{\"end\":133030,\"start\":133021},{\"end\":133039,\"start\":133034},{\"end\":133415,\"start\":133410},{\"end\":133422,\"start\":133419},{\"end\":133433,\"start\":133426},{\"end\":133659,\"start\":133652},{\"end\":133671,\"start\":133663},{\"end\":133681,\"start\":133675},{\"end\":133694,\"start\":133687},{\"end\":133999,\"start\":133993},{\"end\":134011,\"start\":134005},{\"end\":134250,\"start\":134240},{\"end\":134262,\"start\":134254},{\"end\":134636,\"start\":134629},{\"end\":134646,\"start\":134642},{\"end\":134656,\"start\":134650},{\"end\":134669,\"start\":134660},{\"end\":135081,\"start\":135076},{\"end\":135094,\"start\":135085},{\"end\":135479,\"start\":135472},{\"end\":135487,\"start\":135483},{\"end\":135497,\"start\":135491},{\"end\":135840,\"start\":135835},{\"end\":135851,\"start\":135844},{\"end\":135864,\"start\":135855},{\"end\":136239,\"start\":136234},{\"end\":136250,\"start\":136243},{\"end\":136264,\"start\":136254},{\"end\":136278,\"start\":136268},{\"end\":136291,\"start\":136282},{\"end\":136901,\"start\":136896},{\"end\":136911,\"start\":136905},{\"end\":137259,\"start\":137248},{\"end\":137270,\"start\":137263},{\"end\":137278,\"start\":137274},{\"end\":137288,\"start\":137282},{\"end\":137676,\"start\":137668},{\"end\":137690,\"start\":137682},{\"end\":137703,\"start\":137696},{\"end\":137716,\"start\":137709},{\"end\":137998,\"start\":137990},{\"end\":138246,\"start\":138238},{\"end\":138258,\"start\":138252},{\"end\":138272,\"start\":138265},{\"end\":138290,\"start\":138278},{\"end\":138300,\"start\":138294},{\"end\":138311,\"start\":138306},{\"end\":139435,\"start\":139425},{\"end\":139448,\"start\":139439},{\"end\":139868,\"start\":139863},{\"end\":139879,\"start\":139874},{\"end\":140090,\"start\":140083},{\"end\":140101,\"start\":140094},{\"end\":140319,\"start\":140315},{\"end\":140325,\"start\":140323},{\"end\":140334,\"start\":140329},{\"end\":141081,\"start\":141078},{\"end\":141089,\"start\":141085},{\"end\":141098,\"start\":141095},{\"end\":141707,\"start\":141701},{\"end\":141716,\"start\":141711},{\"end\":142105,\"start\":142098},{\"end\":142121,\"start\":142109},{\"end\":142131,\"start\":142127},{\"end\":142146,\"start\":142137},{\"end\":142158,\"start\":142150},{\"end\":142168,\"start\":142162}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":10388405},\"end\":123256,\"start\":122723},{\"attributes\":{\"id\":\"b1\"},\"end\":123934,\"start\":123258},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17606900},\"end\":124233,\"start\":123936},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206590483},\"end\":124621,\"start\":124235},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7553535},\"end\":124859,\"start\":124623},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12561212},\"end\":125325,\"start\":124861},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2078231},\"end\":125860,\"start\":125327},{\"attributes\":{\"id\":\"b7\"},\"end\":126108,\"start\":125862},{\"attributes\":{\"id\":\"b8\"},\"end\":126415,\"start\":126110},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6953475},\"end\":126782,\"start\":126417},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2856763},\"end\":127807,\"start\":126784},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":215513937},\"end\":128156,\"start\":127809},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":11939935},\"end\":128586,\"start\":128158},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12396090},\"end\":128919,\"start\":128588},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":13036203},\"end\":129334,\"start\":128921},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":118828957},\"end\":129630,\"start\":129336},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6152006},\"end\":129997,\"start\":129632},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6131848},\"end\":131002,\"start\":129999},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2421251},\"end\":131496,\"start\":131004},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6533591},\"end\":131910,\"start\":131498},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":18331999},\"end\":132573,\"start\":131912},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14412825},\"end\":132953,\"start\":132575},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":12795415},\"end\":133355,\"start\":132955},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5955557},\"end\":133587,\"start\":133357},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1900911},\"end\":133943,\"start\":133589},{\"attributes\":{\"id\":\"b25\"},\"end\":134157,\"start\":133945},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":195859047},\"end\":134514,\"start\":134159},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6075144},\"end\":135002,\"start\":134516},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14457153},\"end\":135432,\"start\":135004},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":7677566},\"end\":135778,\"start\":135434},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2912873},\"end\":136144,\"start\":135780},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15379808},\"end\":136814,\"start\":136146},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":6267415},\"end\":137192,\"start\":136816},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":11581889},\"end\":137600,\"start\":137194},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":815463},\"end\":137943,\"start\":137602},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1073705},\"end\":138161,\"start\":137945},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2741819},\"end\":139328,\"start\":138163},{\"attributes\":{\"doi\":\"TKK-ICS-R2\",\"id\":\"b37\"},\"end\":139824,\"start\":139330},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":263476610},\"end\":140041,\"start\":139826},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":338469},\"end\":140250,\"start\":140043},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7391801},\"end\":140960,\"start\":140252},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5626877},\"end\":141627,\"start\":140962},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":7500319},\"end\":141991,\"start\":141629},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1486613},\"end\":142678,\"start\":141993}]", "bib_title": "[{\"end\":122804,\"start\":122723},{\"end\":123306,\"start\":123258},{\"end\":123980,\"start\":123936},{\"end\":124287,\"start\":124235},{\"end\":124685,\"start\":124623},{\"end\":124951,\"start\":124861},{\"end\":125374,\"start\":125327},{\"end\":126455,\"start\":126417},{\"end\":126858,\"start\":126784},{\"end\":127865,\"start\":127809},{\"end\":128236,\"start\":128158},{\"end\":128653,\"start\":128588},{\"end\":129004,\"start\":128921},{\"end\":129371,\"start\":129336},{\"end\":129662,\"start\":129632},{\"end\":130072,\"start\":129999},{\"end\":131094,\"start\":131004},{\"end\":131574,\"start\":131498},{\"end\":132002,\"start\":131912},{\"end\":132625,\"start\":132575},{\"end\":133017,\"start\":132955},{\"end\":133406,\"start\":133357},{\"end\":133648,\"start\":133589},{\"end\":134236,\"start\":134159},{\"end\":134625,\"start\":134516},{\"end\":135072,\"start\":135004},{\"end\":135466,\"start\":135434},{\"end\":135831,\"start\":135780},{\"end\":136230,\"start\":136146},{\"end\":136892,\"start\":136816},{\"end\":137244,\"start\":137194},{\"end\":137662,\"start\":137602},{\"end\":137984,\"start\":137945},{\"end\":138232,\"start\":138163},{\"end\":139857,\"start\":139826},{\"end\":140079,\"start\":140043},{\"end\":140311,\"start\":140252},{\"end\":141074,\"start\":140962},{\"end\":141697,\"start\":141629},{\"end\":142094,\"start\":141993}]", "bib_author": "[{\"end\":122820,\"start\":122806},{\"end\":122830,\"start\":122820},{\"end\":122841,\"start\":122830},{\"end\":123316,\"start\":123308},{\"end\":123329,\"start\":123316},{\"end\":123340,\"start\":123329},{\"end\":123349,\"start\":123340},{\"end\":123362,\"start\":123349},{\"end\":123992,\"start\":123982},{\"end\":124000,\"start\":123992},{\"end\":124009,\"start\":124000},{\"end\":124016,\"start\":124009},{\"end\":124298,\"start\":124289},{\"end\":124308,\"start\":124298},{\"end\":124697,\"start\":124687},{\"end\":124964,\"start\":124953},{\"end\":124975,\"start\":124964},{\"end\":124989,\"start\":124975},{\"end\":125002,\"start\":124989},{\"end\":125390,\"start\":125376},{\"end\":125403,\"start\":125390},{\"end\":125417,\"start\":125403},{\"end\":125429,\"start\":125417},{\"end\":125943,\"start\":125929},{\"end\":125956,\"start\":125943},{\"end\":125970,\"start\":125956},{\"end\":125982,\"start\":125970},{\"end\":126168,\"start\":126154},{\"end\":126180,\"start\":126168},{\"end\":126194,\"start\":126180},{\"end\":126202,\"start\":126194},{\"end\":126215,\"start\":126202},{\"end\":126468,\"start\":126457},{\"end\":126478,\"start\":126468},{\"end\":126488,\"start\":126478},{\"end\":126872,\"start\":126860},{\"end\":126888,\"start\":126872},{\"end\":126902,\"start\":126888},{\"end\":126915,\"start\":126902},{\"end\":126926,\"start\":126915},{\"end\":126936,\"start\":126926},{\"end\":126949,\"start\":126936},{\"end\":127878,\"start\":127867},{\"end\":127889,\"start\":127878},{\"end\":127898,\"start\":127889},{\"end\":127908,\"start\":127898},{\"end\":128247,\"start\":128238},{\"end\":128258,\"start\":128247},{\"end\":128669,\"start\":128655},{\"end\":129017,\"start\":129006},{\"end\":129028,\"start\":129017},{\"end\":129384,\"start\":129373},{\"end\":129393,\"start\":129384},{\"end\":129403,\"start\":129393},{\"end\":129673,\"start\":129664},{\"end\":129684,\"start\":129673},{\"end\":129694,\"start\":129684},{\"end\":130083,\"start\":130074},{\"end\":130094,\"start\":130083},{\"end\":130104,\"start\":130094},{\"end\":130118,\"start\":130104},{\"end\":130129,\"start\":130118},{\"end\":131108,\"start\":131096},{\"end\":131118,\"start\":131108},{\"end\":131127,\"start\":131118},{\"end\":131585,\"start\":131576},{\"end\":131598,\"start\":131585},{\"end\":131609,\"start\":131598},{\"end\":132011,\"start\":132004},{\"end\":132019,\"start\":132011},{\"end\":132025,\"start\":132019},{\"end\":132034,\"start\":132025},{\"end\":132640,\"start\":132627},{\"end\":132650,\"start\":132640},{\"end\":133032,\"start\":133019},{\"end\":133041,\"start\":133032},{\"end\":133417,\"start\":133408},{\"end\":133424,\"start\":133417},{\"end\":133435,\"start\":133424},{\"end\":133661,\"start\":133650},{\"end\":133673,\"start\":133661},{\"end\":133683,\"start\":133673},{\"end\":133696,\"start\":133683},{\"end\":134001,\"start\":133991},{\"end\":134013,\"start\":134001},{\"end\":134252,\"start\":134238},{\"end\":134264,\"start\":134252},{\"end\":134638,\"start\":134627},{\"end\":134648,\"start\":134638},{\"end\":134658,\"start\":134648},{\"end\":134671,\"start\":134658},{\"end\":135083,\"start\":135074},{\"end\":135096,\"start\":135083},{\"end\":135481,\"start\":135468},{\"end\":135489,\"start\":135481},{\"end\":135499,\"start\":135489},{\"end\":135842,\"start\":135833},{\"end\":135853,\"start\":135842},{\"end\":135866,\"start\":135853},{\"end\":136241,\"start\":136232},{\"end\":136252,\"start\":136241},{\"end\":136266,\"start\":136252},{\"end\":136280,\"start\":136266},{\"end\":136293,\"start\":136280},{\"end\":136903,\"start\":136894},{\"end\":136913,\"start\":136903},{\"end\":137261,\"start\":137246},{\"end\":137272,\"start\":137261},{\"end\":137280,\"start\":137272},{\"end\":137290,\"start\":137280},{\"end\":137678,\"start\":137664},{\"end\":137692,\"start\":137678},{\"end\":137705,\"start\":137692},{\"end\":137718,\"start\":137705},{\"end\":138000,\"start\":137986},{\"end\":138248,\"start\":138234},{\"end\":138260,\"start\":138248},{\"end\":138274,\"start\":138260},{\"end\":138292,\"start\":138274},{\"end\":138302,\"start\":138292},{\"end\":138313,\"start\":138302},{\"end\":139437,\"start\":139423},{\"end\":139450,\"start\":139437},{\"end\":139870,\"start\":139859},{\"end\":139881,\"start\":139870},{\"end\":140092,\"start\":140081},{\"end\":140103,\"start\":140092},{\"end\":140321,\"start\":140313},{\"end\":140327,\"start\":140321},{\"end\":140336,\"start\":140327},{\"end\":141083,\"start\":141076},{\"end\":141091,\"start\":141083},{\"end\":141100,\"start\":141091},{\"end\":141709,\"start\":141699},{\"end\":141718,\"start\":141709},{\"end\":142107,\"start\":142096},{\"end\":142123,\"start\":142107},{\"end\":142133,\"start\":142123},{\"end\":142148,\"start\":142133},{\"end\":142160,\"start\":142148},{\"end\":142170,\"start\":142160},{\"end\":142174,\"start\":142170}]", "bib_venue": "[{\"end\":123016,\"start\":122937},{\"end\":123517,\"start\":123448},{\"end\":124449,\"start\":124387},{\"end\":125103,\"start\":125061},{\"end\":127124,\"start\":127045},{\"end\":128399,\"start\":128337},{\"end\":128760,\"start\":128723},{\"end\":129139,\"start\":129092},{\"end\":129835,\"start\":129773},{\"end\":130418,\"start\":130282},{\"end\":131268,\"start\":131206},{\"end\":131693,\"start\":131671},{\"end\":132179,\"start\":132115},{\"end\":132791,\"start\":132729},{\"end\":133182,\"start\":133120},{\"end\":134772,\"start\":134730},{\"end\":135207,\"start\":135160},{\"end\":135975,\"start\":135929},{\"end\":136403,\"start\":136351},{\"end\":137014,\"start\":136972},{\"end\":137413,\"start\":137360},{\"end\":138551,\"start\":138408},{\"end\":140142,\"start\":140131},{\"end\":140683,\"start\":140518},{\"end\":141329,\"start\":141223},{\"end\":142281,\"start\":142236},{\"end\":122935,\"start\":122841},{\"end\":123446,\"start\":123362},{\"end\":124073,\"start\":124016},{\"end\":124385,\"start\":124308},{\"end\":124733,\"start\":124697},{\"end\":125059,\"start\":125002},{\"end\":125563,\"start\":125429},{\"end\":125927,\"start\":125862},{\"end\":126152,\"start\":126110},{\"end\":126550,\"start\":126488},{\"end\":127043,\"start\":126949},{\"end\":127970,\"start\":127908},{\"end\":128335,\"start\":128258},{\"end\":128721,\"start\":128669},{\"end\":129090,\"start\":129028},{\"end\":129437,\"start\":129403},{\"end\":129771,\"start\":129694},{\"end\":130280,\"start\":130129},{\"end\":131204,\"start\":131127},{\"end\":131669,\"start\":131609},{\"end\":132113,\"start\":132034},{\"end\":132727,\"start\":132650},{\"end\":133118,\"start\":133041},{\"end\":133461,\"start\":133435},{\"end\":133736,\"start\":133696},{\"end\":133989,\"start\":133945},{\"end\":134304,\"start\":134264},{\"end\":134728,\"start\":134671},{\"end\":135158,\"start\":135096},{\"end\":135593,\"start\":135499},{\"end\":135927,\"start\":135866},{\"end\":136349,\"start\":136293},{\"end\":136970,\"start\":136913},{\"end\":137358,\"start\":137290},{\"end\":137758,\"start\":137718},{\"end\":138040,\"start\":138000},{\"end\":138406,\"start\":138313},{\"end\":139421,\"start\":139330},{\"end\":139921,\"start\":139881},{\"end\":140129,\"start\":140103},{\"end\":140516,\"start\":140336},{\"end\":141221,\"start\":141100},{\"end\":141803,\"start\":141718},{\"end\":142234,\"start\":142174}]"}}}, "year": 2023, "month": 12, "day": 17}