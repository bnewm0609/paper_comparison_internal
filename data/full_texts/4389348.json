{"id": 4389348, "updated": "2023-09-28 13:10:58.204", "metadata": {"title": "End-To-End Multi-Task Learning With Attention", "authors": "[{\"first\":\"Shikun\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Edward\",\"last\":\"Johns\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Davison\",\"middle\":[\"J.\"]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": 6, "day": 1}, "abstract": "We propose a novel multi-task learning architecture, which allows learning of task-specific feature-level attention. Our design, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with a soft-attention module for each task. These modules allow for learning of task-specific features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efficient. We evaluate our approach on a variety of datasets, across both image-to-image predictions and image classification tasks. We show that our architecture is state-of-the-art in multi-task learning compared to existing methods, and is also less sensitive to various weighting schemes in the multi-task loss function. Code is available at https://github.com/lorenmt/mtan.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1803.10704", "mag": "2963430933", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiuJD19", "doi": "10.1109/cvpr.2019.00197"}}, "content": {"source": {"pdf_hash": "9b6cdd9ecee3d284e8f9cc735f89bbf693e713c9", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1803.10704", "status": "GREEN"}}, "grobid": {"id": "0d06ce17835181db79ce30b5283a47c58a0d54af", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9b6cdd9ecee3d284e8f9cc735f89bbf693e713c9.txt", "contents": "\nEnd-to-End Multi-Task Learning with Attention\n\n\nShikun Liu shikun.liu17@imperial.ac.uk \nDepartment of Computing\nImperial College London\n\n\nEdward Johns e.johns@imperial.ac.uk \nDepartment of Computing\nImperial College London\n\n\nAndrew J Davison a.davison@imperial.ac.uk \nDepartment of Computing\nImperial College London\n\n\nEnd-to-End Multi-Task Learning with Attention\n10.1109/CVPR.2019.00197\nWe propose a novel multi-task learning architecture, which allows learning of task-specific feature-level attention. Our design, the Multi-Task Attention Network (MTAN), consists of a single shared network containing a global feature pool, together with a soft-attention module for each task. These modules allow for learning of taskspecific features from the global features, whilst simultaneously allowing for features to be shared across different tasks. The architecture can be trained end-to-end and can be built upon any feed-forward neural network, is simple to implement, and is parameter efficient. We evaluate our approach on a variety of datasets, across both image-toimage predictions and image classification tasks. We show that our architecture is state-of-the-art in multi-task learning compared to existing methods, and is also less sensitive to various weighting schemes in the multi-task loss function. Code is available at https://github.com/ lorenmt/mtan.\n\nIntroduction\n\nConvolutional Neural Networks (CNNs) have seen great success in a range of computer vision tasks, including image classification [11], semantic segmentation [1], and style transfer [13]. However, these networks are typically designed to achieve only one particular task. For more complete vision systems in real-world applications, a network which can perform multiple tasks simultaneously is far more desirable than building a set of independent networks, one for each task. This is more efficient not only in terms of memory and inference speed, but also in terms of data, since related tasks may share informative visual features.\n\nThis type of learning is called Multi-Task Learning (MTL) [20,14,6], and in this paper we present a novel architecture for MTL based on feature-level attention masks, which add greater flexibility to share complementary features. Compared to standard single-task learning, training multiple tasks whilst successfully learning a shared representation poses two key challenges:\n\n\nShared Features\n\n\nTask-Specific Attention Modules\n\nTask-Specific Attention Modules Figure 1: Overview of our proposal MTAN. The shared network takes input data and learns task-shared features, whilst each attention network learns task-specific features, by applying attention modules to the shared network.\n\ni) Network Architecture (how to share): A multi-task learning architecture should express both task-shared and task-specific features. In this way, the network is encouraged to learn a generalisable representation (to avoid over-fitting), whilst also providing the ability to learn features tailored to each task (to avoid under-fitting).\n\nii) Loss Function (how to balance tasks): A multi-task loss function, which weights the relative contributions of each task, should enable learning of all tasks with equal importance, without allowing easier tasks to dominate. Manual tuning of loss weights is tedious, and it is preferable to automatically learn the weights, or design a network which is robust to different weights.\n\nHowever, most prior MTL approaches focus on only one of these two challenges, whilst maintaining a standard implementation of the other. In this paper, we introduce a unified approach which addresses both challenges cohesively, by designing a novel network which (i) enables both taskshared and task-specific features to be learned automatically, and consequently (ii) learns an inherent robustness to the choice of loss weighting scheme.\n\nThe proposed network, which we call the Multi-Task Attention Network (MTAN) (see Figure 1), is composed of a single shared network, which learns a global feature pool containing features across all tasks. Then for each task, rather than learning directly from the shared feature pool, a soft attention mask is applied at each convolution block in the shared network. In this way, each attention mask automatically determines the importance of the shared features for the respective task, allowing learning of both taskshared and task-specific features in a self-supervised, endto-end manner. This flexibility enables much more expressive combinations of features to be learned for generalisation across tasks, whilst still allowing for discriminative features to be tailored for each individual task. Furthermore, automatically choosing which features to share and which to be task specific allows for a highly efficient architecture with far fewer parameters than multi-task architectures which have explicit separation of tasks [26,20].\n\nMTAN can be built on any feed-forward neural network depending on the type of tasks. We first evaluate MTAN with SegNet [1], an encoder-decoder network on the tasks of semantic segmentation and depth estimation on the outdoor CityScapes dataset [4], and then with an additional task of surface normal prediction on the more challenging indoor dataset NYUv2 [21]. We also test our approach with a different backbone architecture, Wide Residual Network [31], on the recently proposed Visual Decathlon Challenge [23], to solve 10 individual image classification tasks. Results show that MTAN outperforms several baselines and is competitive with the state-of-the-art for multi-task learning, whilst being more parameter efficient and therefore scaling more gracefully with the number of tasks. Furthermore, our method shows greater robustness to the choice of weighting scheme in the loss function compared to baselines. As part of our evaluation of this robustness, we also propose a novel weighting scheme, Dynamic Weight Average (DWA), which adapts the task weighting over time by considering the rate of change of the loss for each task.\n\n\nRelated Work\n\nThe term Multi-Task Learning (MTL) has been broadly used in machine learning [2,8,6,17], with similarities to transfer learning [22,18] and continual learning [29]. In computer vision, multi-task learning has been used to for learning similar tasks such as image classification in multiple domains [23], pose estimation and action recognition [9], and dense prediction of depth, surface normals, and semantic classes [20,7]. In this paper, we consider two important aspects of multi-task learning: how can a good multitask network architecture be designed, and how to balance feature sharing in multi-task learning across all tasks?\n\nMost multi-task learning network architectures for computer vision are designed based on existing CNN architectures. For example, Cross-Stitch Networks [20] contain one standard feed-forward network per task, with cross-stitch units to allow features to be shared across tasks. The selfsupervised approach of [6], based on the ResNet101 archi-tecture [30], learns a regularised combination of features from different layers of a single shared network. UberNet [16] proposes an image pyramid approach to process images across multiple resolutions, where for each resolution, additional task-specific layers are formed top of the shared VGG-Net [27]. The Progressive Networks [26] uses a sequence of incrementally-trained networks to transfer knowledge between tasks. However, architectures such as Cross-Stitch Networks and Progressive Networks require a large number of network parameters, and scale linearly with the number of tasks. In contrast, our model requires only a rough 10% increase in parameters for per learning task.\n\nOn the balancing of feature sharing in multi-task learning, there is extensive experimental analysis in [20,14], with both papers arguing that different amounts of sharing and weighting tend to work best for different tasks. One example of weighting tasks appropriately is with the use of weight uncertainty [14], which modifies the loss functions in multi-task learning using task uncertainty. Another method is that of GradNorm [3], which manipulates gradient norms over time to control the training dynamics. As an alternative to using task losses to determine task difficulties, Dynamic Task Prioritisation [10] encourages prioritisation of difficult tasks directly using performance metrics such as accuracy and precision.\n\n\nMulti-Task Attention Network\n\nWe now introduce our novel multi-task learning architecture, the Multi-Task Attention Network (MTAN). Whilst the architecture can be incorporated into any feed-forward network, in the following section we demonstrate how to build MTAN upon an encoder-decoder network, SegNet [1]. This example configuration allows for image-to-image dense pixel-level prediction, such as semantic segmentation, depth estimation, and surface normal prediction.\n\n\nArchitecture Design\n\nMTAN consists of two components: a single shared network, and K task-specific attention networks. The shared network can be designed based on the particular task, whilst each task-specific network consists of a set of attention modules, which link with the shared network. Each attention module applies a soft attention mask to a particular layer of the shared network, to learn task-specific features. As such, the attention masks can be considered as feature selectors from the shared network, which are automatically learned in an end-to-end manner, whilst the shared network learns a compact global feature pool across all tasks. Figure 2 shows a detailed visualisation of our network based on VGG-16 [27], illustrating the encoder half of Seg-Net. The decoder half of SegNet is then symmetric to VGG-16. As shown, each attention module learns a soft attention mask, which itself is dependent on the features in the shared  . Task one (green) and task two (blue) have their own set of attention modules, which link with the shared network (grey). The middle attention module has its structure exposed for visualisation, which is further expanded in the bottom section of the figure, showing both the encoder and decoder versions of the module. All attention modules have the same design, although their weights are individually learned.\n\nnetwork at the corresponding layer. Therefore, the features in the shared network, and the soft attention masks, can be learned jointly to maximise the generalisation of the shared features across multiple tasks, whilst simultaneously maximising the task-specific performance due to the attention masks.\n\n\nTask Specific Attention Module\n\nThe attention module is designed to allow the taskspecific network to learn task-related features, by applying a soft attention mask to the features in the shared network, with one attention mask per task per feature channel. We denote the shared features in the j th block of the shared network as p (j) , and the learned attention mask in this layer for\ntask i as a (j) i . The task-specific features\u00e2 (j) i\nin this layer, are then computed by element-wise multiplication of the attention masks with the shared features:\na (j) i = a (j) i p (j) ,(1)\nwhere denotes element-wise multiplication. As shown in Figure 2, the first attention module in the encoder takes as input only features in the shared network. But for subsequent attention modules in block j, the input is formed by a concatenation of the shared features u (j) , and the task-specific features from the previous layer\u00e2\n(j\u22121) i : a (j) i = h (j) i g (j) i u (j) ; f (j) \u00e2 (j\u22121) i , j \u2265 2 (2) Here, f (j) , g (j) i , h (j)\ni are convolutional layers with batch normalisation, following a non-linear activation. Both g (j) i and h (j) i are composed of [1 \u00d7 1] kernels presenting the i th task-specific attention mask in block j. f (j) is composed of [3 \u00d7 3] kernels representing a shared feature extractor for passing to another attention module, following by a pooling or sampling layer to match the corresponding resolution.\n\nThe attention mask, following a sigmoid activation to ensure a\n(j) i \u2208 [0, 1], is learned in a self-supervised fashion with back-propagation. If a (j) i \u2192 1\nsuch that the mask becomes an identity map, the attended feature maps are equivalent to global feature maps and the tasks share all the features. Therefore, we expect the performance to be no worse than that of a shared multi-task network, which splits into individual tasks only at the end of the network, and we show results demonstrating this in Section 4.\n\n\nThe Model Objective\n\nIn general multi-task learning with K tasks, input X and task-specific labels Y i , i = 1, 2, \u00b7 \u00b7 \u00b7 , K, the loss function is defined as,\nL tot (X, Y 1:K ) = K i=1 \u03bb i L i (X, Y i ).(3)\nThis is the linear combination of task-specific losses L i with task weightings \u03bb i . In our experiments, we study the effect of different weighting schemes on various multi-task learning approaches.\n\nFor image-to-image prediction tasks, we consider each mapping from input data X to a set of labels Y i as one task with total three tasks for evaluation. In each loss function, Y represents the network's prediction, and Y represents the ground-truth label.\n\n\u2022 For semantic segmentation, we apply a pixel-wise crossentropy loss for each predicted class label from a depthsoftmax classifier.\nL 1 (X, Y 1 ) = \u2212 1 pq p,q Y 1 (p, q) log\u0176 1 (p, q). (4)\n\u2022 For depth estimation, we apply an L 1 norm comparing the predicted and ground-truth depth. We use true depth for the NYUv2 indoor scene dataset, and inverse depth in CityScapes outdoor scene dataset as standard, which can more easily represent points at infinite distances, such as the sky:\nL 2 (X, Y 2 ) = 1 pq p,q |Y 2 (p, q) \u2212\u0176 2 (p, q)|.(5)\n\u2022 For surface normals (only available in NYUv2), we apply an element-wise dot product at each normalised pixel with the ground-truth map:\nL 3 (X, Y 3 ) = \u2212 1 pq p,q Y 3 (p, q) \u00b7\u0176 3 (p, q). (6)\nFor image classification tasks, we consider each dataset as one task for which each dataset represents each individual classification task for one domain. We apply standard cross-entropy loss for all classification tasks.\n\n\nExperiments\n\nIn this section, we evaluate our proposed method on two types of tasks: one-to-many predictions for image-to-image regression tasks in Section 4.1 and many-to-many predictions for image classification tasks (Visual Decathlon Challenge) in Section 4.2.\n\n\nImage-to-Image Prediction (One-to-Many)\n\nIn this section, we evaluate MTAN built upon SegNet [1] on image-to-image prediction tasks. We first introduce the datasets used for validation in Section 4.1.1, and several baselines for comparison in Section 4.1.2. In Section 4.1.3, we introduce a novel adaptive weighting method, and in Section 4.1.4 we show the effectiveness of MTAN with various weighting methods compared with single and multitask baseline methods. We explore how the performance of our method scales with task complexity in Section 4.1.5 and we show visualisations of the learned attention masks in Section 4.1.6.\n\n\nDatasets\n\nCityScapes. The CityScapes dataset [4] consists of high resolution street-view images. We use this dataset for two tasks: semantic segmentation and depth estimation. To speed up training, all training and validation images were resized to [128 \u00d7 256]. The dataset contains 19 classes for pixel-wise semantic segmentation, together with groundtruth inverse depth labels. We pair the depth estimation task with three levels of semantic segmentation using 2, 7 or 19 classes (excluding the void group in 7 and 19 classes). Labels for the 19 classes and the coarser 7 categories are defined as in the original CityScapes dataset. We then further create a 2-class dataset with only background and foreground objects. The details of these segmentation classes are presented in Table 1. We perform multi-task learning for 7-class CityScapes dataset in Section 4.1.4. We compare the 2/7/19-class results in Section 4.1.5, with visualisation of these attention maps in Section 4.1.6.\n\nNYUv2. The NYUv2 dataset [21] is consisted with RGB-D indoor scene images. We evaluate performances on three learning tasks: 13-class semantic segmentation defined in [5], true depth data which is recorded by depth cameras from Microsoft Kinect, and surface normals which are provided in [7]. To speed up training, all training and validation images were resized to [288 \u00d7 384] resolution.\n\nCompared to CityScapes, NYUv2 contains images of indoor scenes, which are much more complex since the viewpoints can vary significantly, changable lighting conditions are present, and the appearance for each object class shifts widely in texture and shape. We evaluate performance on different datasets, together with different numbers of tasks, and further with different class complexities, in order to attain a comprehensive understanding on how our proposed method behaves and scales under a range of scenarios.  \n\n\nBaselines\n\nMost image-to-image multi-task learning architectures are designed based on specific feed-forward neural networks, or implemented on varying network architectures, and thus they are typically not directly comparable based on published results. Our method is general and can be applied to any feed-forward neural network, and so for a fair comparison, we implemented 5 different network architectures (2 single-task + 3 multi-task) based on SegNet [1], which we consider as baselines:\n\n\u2022 Single-Task, One Task: The vanilla SegNet for single task learning.\n\n\u2022 Single-Task, STAN: A Single-Task Attention Network, where we directly apply our proposed MTAN whilst only performing a single task.\n\n\u2022 Multi-Task, Split (Wide, Deep): The standard multitask learning, which splits at the last layer for the final prediction for each specific task. We introduce two verions of Split: Wide, where we adjusted the number of convolutional filters, and Deep, where we adjusted the number of convolutional layers, until Split had at least as many parameters as MTAN.\n\n\u2022 Multi-Task, Dense: A shared network together with task-specific networks, where each task-specific network receives all features from the shared network, without any attention modules.\n\n\u2022 Multi-Task, Cross-Stitch: The Cross-Stitch Network [20], a previously proposed adaptive multi-task learning approach, which we implemented on SegNet.\n\nNote that all the baselines were designed to have at least as many parameters than our proposed MTAN, and were tested to validate that our proposed method's better performance is due to the attention modules, rather than simply due to the increase in network parameters.\n\n\nDynamic Weight Average\n\nFor most multi-task learning networks, training multiple tasks is difficult without finding the correct balance between those tasks, and recent approaches have attempted to address this issue [3,14]. To test our method across a range of weighting schemes, we propose a simple yet effective adaptive weighting method, named Dynamic Weight Average (DWA). Inspired by GradNorm [3], this learns to average task weighting over time by considering the rate of change of loss for each task. But whilst GradNorm requires access to the network's internal gradients, our DWA proposal only requires the numerical task loss, and therefore its implementation is far simpler.\n\nWith DWA, we define the weighting \u03bb k for task k as:\n\u03bb k (t) := K exp(w k (t \u2212 1)/T ) i exp(w i (t \u2212 1)/T ) , w k (t \u2212 1) = L k (t \u2212 1) L k (t \u2212 2) ,(7)\nHere, w k (\u00b7) calculates the relative descending rate in the range (0, +\u221e), t is an iteration index, and T represents a temperature which controls the softness of task weighting, similar to [12]. A large T results in a more even distribution between different tasks. If T is large enough, we have \u03bb i \u2248 1, and tasks are weighted equally. Finally, the softmax operator, which is multiplied by K, ensures that i \u03bb i (t) = K. In our implementation, the loss value L k (t) is calculated as the average loss in each epoch over several iterations. Doing so reduces the uncertainty from stochastic gradient descent and random training data selection. For t = 1, 2, we initialise w k (t) = 1, but any non-balanced initialisation based on prior knowledge could also be introduced.\n\n\nResults on Image-to-Image Predictions\n\nWe now evaluate the performance of our proposed MTAN method in image-to-image multi-task learning, based on the SegNet architecture. Using the 7-class version of the CityScapes dataset and 13-class version of NYUv2 dataset, we compare all the baselines introduced in Section 4.1.2.\n\nTraining. For each network architecture, we ran experiments with three types of weighting methods: equal weighting, weight uncertainty [14], and our proposed DWA (with hyper-parameter temperature T = 2, found empirically to be optimum across all architectures). We did not include GradNorm [3] because it requires a manual choice of subset network weights across all baselines, based on their specific architectures, which distracts from a fair evaluation of the architectures themselves. We trained all the models with ADAM optimiser [15] using a learning rate of 10 \u22124 , with a batch size of 2 for NYUv2 dataset and 8 for CityScapes dataset. During training, we halve the learning rate at 40k iterations, for a total of 80k iterations.\n\nResults.  Table 2: 7-class semantic segmentation and depth estimation results on CityScapes validation dataset. #P shows the number of network parameters, and the best performing combination of multi-task architecture and weighting is highlighted in bold. The top validation scores for each task are annotated with boxes.\n\nIn particular, our method has two key advantages. First, due to the efficiency of having a single shared feature pool with attention masks automatically learning which features to share, our method outperforms other methods without requiring extra parameters (column #P), and even with signif-icantly fewer parameters in some cases.\n\nSecond, our method maintains high performance across different loss function weighting schemes, and is more robust to the choice of weighting scheme than other methods, avoiding the need for cumbersome tweaking of loss weights. We illustrate the robustness of our method to the weighting schemes with a comparison to the Cross-Stitch Network [20], by plotting learning curves in Figure 3 with respect to the performance of three learning tasks in NYUv2 dataset. We can clearly see that our network follows similar learning trends across various weighting schemes, compared to the Cross-Stitch Network which produces notably different behaviour across the different schemes.  We can see the advantage of our multitask learning approach over vanilla single-task learning, where the edges of objects are clearly more pronounced.\n\n\nEffect of Task Complexity\n\nFor further introspection into the benefits of multi-task learning, we evaluated our implementations on CityScapes across different numbers of semantic classes, with the depth labels the same across all experiments. We trained the networks with the same settings as in Section 4.1.4, with an additional multi-task baseline Split (the standard version), which we found to perform better than the other modified versions. All networks are trained with equal weighting. Table 4 (left) shows the validation performance improvement across all multi-task implementations and the singletask STAN implementation, plotted relative to the performance of the vanilla single-task learning on the CityScapes dataset. Interestingly, for only a 2-class setup, the singletask attention network (STAN) performs better than all   multi-task methods since it is able to fully utilise network parameters in a simple manner for the simple task. However, for greater task complexity, the multi-task methods encourage the sharing of features for a more efficient use of available network parameters, which then leads to better results. We also observe that, whilst the relative performance gain increases for all implementations as the task complexity increases, our MTAN method increases at a greater rate.  \n\n\nAttention Masks as Feature Selectors\n\nTo understand the role of the proposed attention modules, in Figure 5 we visualise the first layer attention masks learned with our network based on CityScapes dataset. We can see a clear difference in attention masks between the two tasks, with each mask working as a feature selector to mask out uninformative parts of the shared features, and focus on parts which are useful for each task. Notably, the depth masks have a much higher contrast than the semantic masks, suggesting that whilst all shared features are generally useful for the semantic task, the depth task benefits more from extraction of task-specific features.  \n\n\nVisual Decathlon Challenge (Many-to-Many)\n\nFinally, we evaluate our approach on the recently introduced Visual Decathlon Challenge, consisting of 10 individual image classification tasks (many-to-many predictions). Evaluation on this challenge reports per-task ac-curacies, and assigns a cumulative score with a maximum value of 10,000 (1,000 per task) based on these accuracies. The complete details about the challenge settings, evaluation, and datasets used, can be found at http://www. robots.ox.ac.uk/\u02dcvgg/decathlon/.\n\nTable 4 (right) shows results for the online test set of the challenge. As consistent with the prior works, we apply MTAN built on Wide Residual Network [31] with a depth of 28, widening factor of 4, and a stride of 2 in the first convolutional layer of each block. We train our model using a batch size of 100, learning rate of 0.1 with SGD, and weight decay of 5 \u00b7 10 \u22125 for all 10 classification tasks. We halve the learning rate every 50 epochs for a total of 300 epochs. Then, we fine-tune 9 classification tasks (all except ImageNet) with a learning rate 0.01 until convergence. The results show that our approach surpasses most of the baselines and is competitive with the current state-of-theart, without the need for complicated regularisation strategies such as applying DropOut [28], regrouping datasets by size, or adaptive weight decay for each dataset, as required.\n\n\nConclusions\n\nIn this work, we have presented a new method for multitask learning, the Multi-Task Attention Network (MTAN). The network architecture consists of a global feature pool, together with task-specific attention modules for each task, which allows for automatic learning of both task-shared and task-specific features in an end-to-end manner. Experiments on the NYUv2 and CityScapes datasets with multiple dense-prediction tasks, and on the Visual Decathlon Challenge with multiple image classification tasks, show that our method outperforms or is competitive with other methods, whilst also showing robustness to the particular task weighting schemes used in the loss function. Due to our method's ability to share weights through attention masks, our method achieves this state-of-the-art performance whilst also being highly parameter efficient.\n\nFigure 2 :\n2Visualisation of MTAN based on VGG-16, showing the encoder half of SegNet (with the decoder half being symmetrical to the encoder)\n\nFigure 3 :\n3Validation performance curves on the NYUv2 dataset, across all three tasks (semantics, depth, normals, from left to right), showing robustness to loss function weighting schemes on the Cross-Stitch Network[20] (top) and our Multi-task Attention Network (bottom).\n\nFigure 4\n4then shows qualitative results on the CityScapes validation dataset.\n\nFigure 4 :\n4CityScapes validation results on 7-class semantic labelling and depth estimation, trained with equal weighting. The original images are cropped to avoid invalid points for better visualisation. The red boxes are regions of interest, showing the effectiveness of the results provided from our method and single task method.\n\nFigure 5 :\n5Visualisation of the first layer of 7-class semantic and depth attention features of our proposed network. The colours for each image are rescaled to fit the data.\n\nTable 1 :\n1Three levels of semantic classes for the CityScapes data used in our experiments.\n\nTable 2\n2and 3 shows experimental results for CityScales and NYUv2 datasets across all architectures, and across all loss function weighting schemes. Results also show the number of network parameters for each architecture. Our MTAN method performs similarly to our baseline Dense in the CityScapes dataset, whilst only having less than half the number of parameters, and outperforms all other baselines. For the more challenging NYUv2 dataset, our method outperforms all baselines across all weighting methods and all learning tasks.#P. \nArchitecture \nWeighting \n\nSegmentation \nDepth \n\n(Higher Better) (Lower Better) \nmIoU Pix Acc Abs Err Rel Err \n\n2 One Task \nn.a. \n51.09 90.69 0.0158 34.17 \n3.04 STAN \nn.a. \n51.90 90.87 0.0145 27.46 \n\nEqual Weights \n50.17 90.63 0.0167 44.73 \n1.75 Split, Wide \nUncert. Weights [14] 51.21 90.72 0.0158 44.01 \nDWA, T = 2 \n50.39 90.45 0.0164 43.93 \n\nEqual Weights \n49.85 88.69 0.0180 43.86 \n2 Split, Deep \nUncert. Weights [14] 48.12 88.68 0.0169 39.73 \nDWA, T = 2 \n49.67 88.81 0.0182 46.63 \n\nEqual Weights \n51.91 90.89 0.0138 27.21 \n3.63 Dense \nUncert. Weights [14] 51.89 91.22 0.0134 25.36 \nDWA, T = 2 \n51.78 90.88 0.0137 26.67 \n\nEqual Weights \n50.08 90.33 0.0154 34.49 \n\u22482 Cross-Stitch [20] Uncert. Weights [14] 50.31 90.43 0.0152 31.36 \nDWA, T = 2 \n50.33 90.55 0.0153 33.37 \n\nEqual Weights \n53.04 91.11 0.0144 33.63 \n1.65 MTAN (Ours) \nUncert. Weights [14] 53.86 91.10 0.0144 35.72 \nDWA, T = 2 \n53.29 91.09 0.0144 34.14 \n\n\n\nTable 3\n3: 13-class semantic segmentation, depth estimation, and surface normal prediction results on the NYUv2 validation \ndataset. #P shows the number of network parameters, and the best performing combination of multi-task architecture and \nweighting is highlighted in bold. The top validation scores for each task are annotated with boxes. \n\nInput Image \n\nGrouth Truth \n(Semantic) \n\nVanilla \nSingle-Task \nLearning \n\nMulti-Task \nAttention \nNetwork \n\nGrouth Truth \n(Depth) \n\nVanilla \nSingle-Task \nLearning \n\nMulti-Task \nAttention \nNetwork \n\n\n\n\nMethod #P. ImNet. Airc. C100 DPed DTD GTSR Flwr Oglt SVHN UCF Mean Score Parallel SVD [24] 1.5 60.32 66.04 81.86 94.23 57.82 99.24 85.74 89.25 96.62 52.50 78.2-class \n\n7-class \n19-class \n\n0 \n\n5 \n\n10 \n\n15 \n\nPerformance Gain (mIoU) \n\nSingle-Task, STAN \nMulti-Task, Split \nMulti-Task, Dense \nMulti-Task, Cross-Stitch \nMulti-Task, MTAN (ours) \n\nScratch [23] \n10 59.87 57.10 75.73 91.20 37.77 96.55 56.3 88.74 96.63 43.27 70.32 1625 \nFinetune [23] \n10 59.87 60.34 82.12 92.82 55.53 97.53 81.41 87.69 96.55 51.20 76.51 2500 \n\nFeature [23] \n1 59.67 23.31 63.11 80.33 45.37 68.16 73.69 58.79 43.54 26.8 54.28 544 \nRes. Adapt.[23] \n2 59.67 56.68 81.20 93.88 50.85 97.05 66.24 89.62 96.13 47.45 73.88 2118 \nDAN [25] \n2.17 57.74 64.12 80.07 91.30 56.54 98.46 86.05 89.67 96.77 49.38 77.01 2851 \nPiggyback [19] \n1.28 57.69 65.29 79.87 96.99 57.45 97.27 79.09 87.63 97.24 47.48 76.60 2838 \n36 3398 \nMTAN (Ours) \n1.74 63.90 61.81 81.59 91.63 56.44 98.80 81.04 89.83 96.88 50.63 77.25 2941 \n\n\n\nTable 4 :\n4Left: CityScapes performance gain in percentage for all implementations compared with the vanilla single-task method. Right: Top-1 classification accuracy on the Visual Decathlon Challenge online test set. #P is the number of parameters as a factor of a single-task implementation. The upper part of table presents results from single task learning baselines; lower part of table presents results from multi-task learning baselines.\n\nSegnet: A deep convolutional encoder-decoder architecture for image segmentation. Vijay Badrinarayanan, Alex Kendall, Roberto Cipolla, IEEE transactions on pattern analysis and machine intelligence. 39Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern anal- ysis and machine intelligence, 39(12):2481-2495, 2017.\n\nMultitask learning. Rich Caruana, Learning to learn. SpringerRich Caruana. Multitask learning. In Learning to learn, pages 95-133. Springer, 1998.\n\nGradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich, International Conference on Machine Learning. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and An- drew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In Inter- national Conference on Machine Learning, pages 793-802, 2018.\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3213-3223, 2016.\n\nIndoor semantic segmentation using depth information. Camille Couprie, Cl\u00e9ment Farabet, Laurent Najman, Yann Lecun, International Conference on Learning Representations (ICLR2013). Camille Couprie, Cl\u00e9ment Farabet, Laurent Najman, and Yann Lecun. Indoor semantic segmentation using depth in- formation. In International Conference on Learning Repre- sentations (ICLR2013), April 2013, 2013.\n\nMulti-task selfsupervised visual learning. Carl Doersch, Andrew Zisserman, The IEEE International Conference on Computer Vision (ICCV). Carl Doersch and Andrew Zisserman. Multi-task self- supervised visual learning. In The IEEE International Con- ference on Computer Vision (ICCV), Oct 2017.\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. David Eigen, Rob Fergus, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionDavid Eigen and Rob Fergus. Predicting depth, surface nor- mals and semantic labels with a common multi-scale con- volutional architecture. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 2650-2658, 2015.\n\nRegularized multi-task learning. Theodoros Evgeniou, Massimiliano Pontil, Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. the tenth ACM SIGKDD international conference on Knowledge discovery and data miningACMTheodoros Evgeniou and Massimiliano Pontil. Regular- ized multi-task learning. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 109-117. ACM, 2004.\n\nR-cnns for pose estimation and action detection. Georgia Gkioxari, Bharath Hariharan, Ross Girshick, Jitendra Malik, arXiv:1406.5212arXiv preprintGeorgia Gkioxari, Bharath Hariharan, Ross Girshick, and Ji- tendra Malik. R-cnns for pose estimation and action detec- tion. arXiv preprint arXiv:1406.5212, 2014.\n\nDynamic task prioritization for multitask learning. Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, Li Fei-Fei, European Conference on Computer Vision. SpringerMichelle Guo, Albert Haque, De-An Huang, Serena Ye- ung, and Li Fei-Fei. Dynamic task prioritization for multi- task learning. In European Conference on Computer Vision, pages 282-299. Springer, 2018.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nPerceptual losses for real-time style transfer and super-resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, European Conference on Computer Vision. SpringerJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694-711. Springer, 2016.\n\nMulti-task learning using uncertainty to weigh losses for scene geometry and semantics. Alex Kendall, Yarin Gal, Roberto Cipolla, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAlex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geome- try and semantics. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7482- 7491, 2018.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nUbernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. Iasonas Kokkinos, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Iasonas Kokkinos. Ubernet: Training a universal convolu- tional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\n\nLearning task grouping and overlap in multi-task learning. Abhishek Kumar, Hal Daum\u00e9, Iii , Proceedings of the 29th International Coference on International Conference on Machine Learning. the 29th International Coference on International Conference on Machine LearningOmnipressAbhishek Kumar and Hal Daum\u00e9 III. Learning task grouping and overlap in multi-task learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1723-1730. Omnipress, 2012.\n\nTransfer feature learning with joint distribution adaptation. Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, Philip S Yu, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionMingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S Yu. Transfer feature learning with joint distribution adaptation. In Proceedings of the IEEE inter- national conference on computer vision, pages 2200-2207, 2013.\n\nPiggyback: Adapting a single network to multiple tasks by learning to mask weights. Arun Mallya, Dillon Davis, Svetlana Lazebnik, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggy- back: Adapting a single network to multiple tasks by learn- ing to mask weights. In Proceedings of the European Con- ference on Computer Vision (ECCV), pages 67-82, 2018.\n\nCross-stitch networks for multi-task learning. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, Martial Hebert, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionIshan Misra, Abhinav Shrivastava, Abhinav Gupta, and Mar- tial Hebert. Cross-stitch networks for multi-task learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3994-4003, 2016.\n\nIndoor segmentation and support inference from rgbd images. Derek Pushmeet Kohli Nathan Silberman, Rob Hoiem, Fergus, ECCV. Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.\n\nA survey on transfer learning. Qiang Sinno Jialin Pan, Yang, IEEE Transactions on knowledge and data engineering. 2210Sinno Jialin Pan and Qiang Yang. A survey on transfer learn- ing. IEEE Transactions on knowledge and data engineering, 22(10):1345-1359, 2010.\n\nLearning multiple visual domains with residual adapters. Hakan Sylvestre-Alvise Rebuffi, Andrea Bilen, Vedaldi, Advances in Neural Information Processing Systems. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In Advances in Neural Information Processing Systems, pages 506-516, 2017.\n\nEfficient parametrization of multi-domain deep neural networks. Hakan Sylvestre-Alvise Rebuffi, Andrea Bilen, Vedaldi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Efficient parametrization of multi-domain deep neural net- works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8119-8127, 2018.\n\nIncremental learning through deep adaptation. Amir Rosenfeld, John K Tsotsos, IEEE transactions on pattern analysis and machine intelligence. Amir Rosenfeld and John K Tsotsos. Incremental learning through deep adaptation. IEEE transactions on pattern anal- ysis and machine intelligence, 2018.\n\nA Andrei, Rusu, C Neil, Guillaume Rabinowitz, Hubert Desjardins, James Soyer, Koray Kirkpatrick, Kavukcuoglu, arXiv:1606.04671Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprintAndrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz- van Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nDropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, The Journal of Machine Learning Research. 151Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929-1958, 2014.\n\nLearning to learn. Sebastian Thrun, Lorien Pratt, Springer Science & Business MediaSebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.\n\n. Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, Xiaoou Tang, Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.\n\nResidual attention network for image classification. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionResidual attention network for image classification. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156-3164, 2017.\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, Proceedings of the British Machine Vision Conference (BMVC). Edwin R. Hancock Richard C. Wilson and William A. P. Smiththe British Machine Vision Conference (BMVC)BMVA Press12Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. In Edwin R. Hancock Richard C. Wilson and William A. P. Smith, editors, Proceedings of the British Machine Vi- sion Conference (BMVC), pages 87.1-87.12. BMVA Press, September 2016.\n", "annotations": {"author": "[{\"end\":138,\"start\":49},{\"end\":225,\"start\":139},{\"end\":318,\"start\":226}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":56},{\"end\":151,\"start\":146},{\"end\":242,\"start\":235}]", "author_first_name": "[{\"end\":55,\"start\":49},{\"end\":145,\"start\":139},{\"end\":232,\"start\":226},{\"end\":234,\"start\":233}]", "author_affiliation": "[{\"end\":137,\"start\":89},{\"end\":224,\"start\":176},{\"end\":317,\"start\":269}]", "title": "[{\"end\":46,\"start\":1},{\"end\":364,\"start\":319}]", "venue": null, "abstract": "[{\"end\":1364,\"start\":389}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1513,\"start\":1509},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1540,\"start\":1537},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1565,\"start\":1561},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2077,\"start\":2073},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2080,\"start\":2077},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2082,\"start\":2080},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4900,\"start\":4896},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4903,\"start\":4900},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5029,\"start\":5026},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5154,\"start\":5151},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5267,\"start\":5263},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5361,\"start\":5357},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5419,\"start\":5415},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6141,\"start\":6138},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6143,\"start\":6141},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6145,\"start\":6143},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6148,\"start\":6145},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6193,\"start\":6189},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6196,\"start\":6193},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6224,\"start\":6220},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6363,\"start\":6359},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6407,\"start\":6404},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6482,\"start\":6478},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6484,\"start\":6482},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6851,\"start\":6847},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7007,\"start\":7004},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7050,\"start\":7046},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7159,\"start\":7155},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7342,\"start\":7338},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7373,\"start\":7369},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7834,\"start\":7830},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7837,\"start\":7834},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8038,\"start\":8034},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8159,\"start\":8156},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8341,\"start\":8337},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8764,\"start\":8761},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9661,\"start\":9657},{\"end\":11730,\"start\":11727},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14526,\"start\":14523},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15109,\"start\":15106},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16076,\"start\":16072},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":16217,\"start\":16214},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16338,\"start\":16335},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17419,\"start\":17416},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18266,\"start\":18262},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18854,\"start\":18851},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18857,\"start\":18854},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19036,\"start\":19033},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19669,\"start\":19665},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20710,\"start\":20706},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20864,\"start\":20861},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21110,\"start\":21106},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22313,\"start\":22309},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25464,\"start\":25460},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26100,\"start\":26096},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":27414,\"start\":27410}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27191,\"start\":27048},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27467,\"start\":27192},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27547,\"start\":27468},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27883,\"start\":27548},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28060,\"start\":27884},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":28154,\"start\":28061},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29613,\"start\":28155},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30158,\"start\":29614},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31138,\"start\":30159},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31583,\"start\":31139}]", "paragraph": "[{\"end\":2013,\"start\":1380},{\"end\":2390,\"start\":2015},{\"end\":2699,\"start\":2444},{\"end\":3039,\"start\":2701},{\"end\":3424,\"start\":3041},{\"end\":3864,\"start\":3426},{\"end\":4904,\"start\":3866},{\"end\":6044,\"start\":4906},{\"end\":6693,\"start\":6061},{\"end\":7724,\"start\":6695},{\"end\":8453,\"start\":7726},{\"end\":8928,\"start\":8486},{\"end\":10292,\"start\":8952},{\"end\":10597,\"start\":10294},{\"end\":10987,\"start\":10632},{\"end\":11154,\"start\":11042},{\"end\":11517,\"start\":11184},{\"end\":12023,\"start\":11620},{\"end\":12087,\"start\":12025},{\"end\":12541,\"start\":12182},{\"end\":12702,\"start\":12565},{\"end\":12950,\"start\":12751},{\"end\":13208,\"start\":12952},{\"end\":13341,\"start\":13210},{\"end\":13691,\"start\":13399},{\"end\":13883,\"start\":13746},{\"end\":14160,\"start\":13939},{\"end\":14427,\"start\":14176},{\"end\":15058,\"start\":14471},{\"end\":16045,\"start\":15071},{\"end\":16436,\"start\":16047},{\"end\":16955,\"start\":16438},{\"end\":17452,\"start\":16969},{\"end\":17523,\"start\":17454},{\"end\":17658,\"start\":17525},{\"end\":18019,\"start\":17660},{\"end\":18207,\"start\":18021},{\"end\":18360,\"start\":18209},{\"end\":18632,\"start\":18362},{\"end\":19320,\"start\":18659},{\"end\":19374,\"start\":19322},{\"end\":20246,\"start\":19475},{\"end\":20569,\"start\":20288},{\"end\":21308,\"start\":20571},{\"end\":21631,\"start\":21310},{\"end\":21965,\"start\":21633},{\"end\":22792,\"start\":21967},{\"end\":24108,\"start\":22822},{\"end\":24780,\"start\":24149},{\"end\":25305,\"start\":24826},{\"end\":26186,\"start\":25307},{\"end\":27047,\"start\":26202}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11041,\"start\":10988},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11183,\"start\":11155},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11619,\"start\":11518},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12181,\"start\":12088},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12750,\"start\":12703},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13398,\"start\":13342},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13745,\"start\":13692},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13938,\"start\":13884},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19474,\"start\":19375}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15849,\"start\":15842},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21327,\"start\":21320},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23296,\"start\":23289}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1378,\"start\":1366},{\"end\":2408,\"start\":2393},{\"end\":2442,\"start\":2411},{\"attributes\":{\"n\":\"2.\"},\"end\":6059,\"start\":6047},{\"attributes\":{\"n\":\"3.\"},\"end\":8484,\"start\":8456},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8950,\"start\":8931},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10630,\"start\":10600},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12563,\"start\":12544},{\"attributes\":{\"n\":\"4.\"},\"end\":14174,\"start\":14163},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14469,\"start\":14430},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":15069,\"start\":15061},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":16967,\"start\":16958},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":18657,\"start\":18635},{\"attributes\":{\"n\":\"4.1.4\"},\"end\":20286,\"start\":20249},{\"attributes\":{\"n\":\"4.1.5\"},\"end\":22820,\"start\":22795},{\"attributes\":{\"n\":\"4.1.6\"},\"end\":24147,\"start\":24111},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24824,\"start\":24783},{\"attributes\":{\"n\":\"5.\"},\"end\":26200,\"start\":26189},{\"end\":27059,\"start\":27049},{\"end\":27203,\"start\":27193},{\"end\":27477,\"start\":27469},{\"end\":27559,\"start\":27549},{\"end\":27895,\"start\":27885},{\"end\":28071,\"start\":28062},{\"end\":28163,\"start\":28156},{\"end\":29622,\"start\":29615},{\"end\":31149,\"start\":31140}]", "table": "[{\"end\":29613,\"start\":28690},{\"end\":30158,\"start\":29624},{\"end\":31138,\"start\":30319}]", "figure_caption": "[{\"end\":27191,\"start\":27061},{\"end\":27467,\"start\":27205},{\"end\":27547,\"start\":27479},{\"end\":27883,\"start\":27561},{\"end\":28060,\"start\":27897},{\"end\":28154,\"start\":28073},{\"end\":28690,\"start\":28165},{\"end\":30319,\"start\":30161},{\"end\":31583,\"start\":31151}]", "figure_ref": "[{\"end\":2484,\"start\":2476},{\"end\":3956,\"start\":3947},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9594,\"start\":9586},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11247,\"start\":11239},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22354,\"start\":22346},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24218,\"start\":24210}]", "bib_author_first_name": "[{\"end\":31672,\"start\":31667},{\"end\":31693,\"start\":31689},{\"end\":31710,\"start\":31703},{\"end\":32040,\"start\":32036},{\"end\":32257,\"start\":32253},{\"end\":32269,\"start\":32264},{\"end\":32293,\"start\":32286},{\"end\":32305,\"start\":32299},{\"end\":32666,\"start\":32660},{\"end\":32682,\"start\":32675},{\"end\":32699,\"start\":32690},{\"end\":32711,\"start\":32707},{\"end\":32727,\"start\":32721},{\"end\":32746,\"start\":32739},{\"end\":32760,\"start\":32757},{\"end\":32775,\"start\":32769},{\"end\":32787,\"start\":32782},{\"end\":33311,\"start\":33304},{\"end\":33328,\"start\":33321},{\"end\":33345,\"start\":33338},{\"end\":33358,\"start\":33354},{\"end\":33689,\"start\":33685},{\"end\":33705,\"start\":33699},{\"end\":34048,\"start\":34043},{\"end\":34059,\"start\":34056},{\"end\":34469,\"start\":34460},{\"end\":34492,\"start\":34480},{\"end\":34955,\"start\":34948},{\"end\":34973,\"start\":34966},{\"end\":34989,\"start\":34985},{\"end\":35008,\"start\":35000},{\"end\":35269,\"start\":35261},{\"end\":35281,\"start\":35275},{\"end\":35294,\"start\":35289},{\"end\":35308,\"start\":35302},{\"end\":35318,\"start\":35316},{\"end\":35631,\"start\":35624},{\"end\":35643,\"start\":35636},{\"end\":35659,\"start\":35651},{\"end\":35669,\"start\":35665},{\"end\":36031,\"start\":36023},{\"end\":36045,\"start\":36040},{\"end\":36059,\"start\":36055},{\"end\":36352,\"start\":36346},{\"end\":36371,\"start\":36362},{\"end\":36381,\"start\":36379},{\"end\":36724,\"start\":36720},{\"end\":36739,\"start\":36734},{\"end\":36752,\"start\":36745},{\"end\":37191,\"start\":37190},{\"end\":37207,\"start\":37202},{\"end\":37511,\"start\":37504},{\"end\":37906,\"start\":37898},{\"end\":37917,\"start\":37914},{\"end\":37928,\"start\":37925},{\"end\":38416,\"start\":38407},{\"end\":38430,\"start\":38423},{\"end\":38445,\"start\":38437},{\"end\":38460,\"start\":38452},{\"end\":38474,\"start\":38466},{\"end\":38924,\"start\":38920},{\"end\":38939,\"start\":38933},{\"end\":38955,\"start\":38947},{\"end\":39362,\"start\":39357},{\"end\":39377,\"start\":39370},{\"end\":39398,\"start\":39391},{\"end\":39413,\"start\":39406},{\"end\":39852,\"start\":39847},{\"end\":39889,\"start\":39886},{\"end\":40084,\"start\":40079},{\"end\":40372,\"start\":40367},{\"end\":40405,\"start\":40399},{\"end\":40734,\"start\":40729},{\"end\":40767,\"start\":40761},{\"end\":41206,\"start\":41202},{\"end\":41453,\"start\":41452},{\"end\":41469,\"start\":41468},{\"end\":41485,\"start\":41476},{\"end\":41504,\"start\":41498},{\"end\":41522,\"start\":41517},{\"end\":41535,\"start\":41530},{\"end\":41941,\"start\":41936},{\"end\":41958,\"start\":41952},{\"end\":42218,\"start\":42212},{\"end\":42239,\"start\":42231},{\"end\":42252,\"start\":42248},{\"end\":42269,\"start\":42265},{\"end\":42287,\"start\":42281},{\"end\":42604,\"start\":42595},{\"end\":42618,\"start\":42612},{\"end\":42759,\"start\":42756},{\"end\":42774,\"start\":42766},{\"end\":42786,\"start\":42782},{\"end\":42797,\"start\":42793},{\"end\":42809,\"start\":42804},{\"end\":42822,\"start\":42814},{\"end\":42838,\"start\":42830},{\"end\":42851,\"start\":42845},{\"end\":43350,\"start\":43344},{\"end\":43367,\"start\":43362}]", "bib_author_last_name": "[{\"end\":31687,\"start\":31673},{\"end\":31701,\"start\":31694},{\"end\":31718,\"start\":31711},{\"end\":32048,\"start\":32041},{\"end\":32262,\"start\":32258},{\"end\":32284,\"start\":32270},{\"end\":32297,\"start\":32294},{\"end\":32316,\"start\":32306},{\"end\":32673,\"start\":32667},{\"end\":32688,\"start\":32683},{\"end\":32705,\"start\":32700},{\"end\":32719,\"start\":32712},{\"end\":32737,\"start\":32728},{\"end\":32755,\"start\":32747},{\"end\":32767,\"start\":32761},{\"end\":32780,\"start\":32776},{\"end\":32795,\"start\":32788},{\"end\":33319,\"start\":33312},{\"end\":33336,\"start\":33329},{\"end\":33352,\"start\":33346},{\"end\":33364,\"start\":33359},{\"end\":33697,\"start\":33690},{\"end\":33715,\"start\":33706},{\"end\":34054,\"start\":34049},{\"end\":34066,\"start\":34060},{\"end\":34478,\"start\":34470},{\"end\":34499,\"start\":34493},{\"end\":34964,\"start\":34956},{\"end\":34983,\"start\":34974},{\"end\":34998,\"start\":34990},{\"end\":35014,\"start\":35009},{\"end\":35273,\"start\":35270},{\"end\":35287,\"start\":35282},{\"end\":35300,\"start\":35295},{\"end\":35314,\"start\":35309},{\"end\":35326,\"start\":35319},{\"end\":35634,\"start\":35632},{\"end\":35649,\"start\":35644},{\"end\":35663,\"start\":35660},{\"end\":35673,\"start\":35670},{\"end\":36038,\"start\":36032},{\"end\":36053,\"start\":36046},{\"end\":36064,\"start\":36060},{\"end\":36360,\"start\":36353},{\"end\":36377,\"start\":36372},{\"end\":36389,\"start\":36382},{\"end\":36732,\"start\":36725},{\"end\":36743,\"start\":36740},{\"end\":36760,\"start\":36753},{\"end\":37200,\"start\":37192},{\"end\":37214,\"start\":37208},{\"end\":37218,\"start\":37216},{\"end\":37520,\"start\":37512},{\"end\":37912,\"start\":37907},{\"end\":37923,\"start\":37918},{\"end\":38421,\"start\":38417},{\"end\":38435,\"start\":38431},{\"end\":38450,\"start\":38446},{\"end\":38464,\"start\":38461},{\"end\":38477,\"start\":38475},{\"end\":38931,\"start\":38925},{\"end\":38945,\"start\":38940},{\"end\":38964,\"start\":38956},{\"end\":39368,\"start\":39363},{\"end\":39389,\"start\":39378},{\"end\":39404,\"start\":39399},{\"end\":39420,\"start\":39414},{\"end\":39884,\"start\":39853},{\"end\":39895,\"start\":39890},{\"end\":39903,\"start\":39897},{\"end\":40101,\"start\":40085},{\"end\":40107,\"start\":40103},{\"end\":40397,\"start\":40373},{\"end\":40411,\"start\":40406},{\"end\":40420,\"start\":40413},{\"end\":40759,\"start\":40735},{\"end\":40773,\"start\":40768},{\"end\":40782,\"start\":40775},{\"end\":41216,\"start\":41207},{\"end\":41232,\"start\":41218},{\"end\":41460,\"start\":41454},{\"end\":41466,\"start\":41462},{\"end\":41474,\"start\":41470},{\"end\":41496,\"start\":41486},{\"end\":41515,\"start\":41505},{\"end\":41528,\"start\":41523},{\"end\":41547,\"start\":41536},{\"end\":41560,\"start\":41549},{\"end\":41950,\"start\":41942},{\"end\":41968,\"start\":41959},{\"end\":42229,\"start\":42219},{\"end\":42246,\"start\":42240},{\"end\":42263,\"start\":42253},{\"end\":42279,\"start\":42270},{\"end\":42301,\"start\":42288},{\"end\":42610,\"start\":42605},{\"end\":42624,\"start\":42619},{\"end\":42764,\"start\":42760},{\"end\":42780,\"start\":42775},{\"end\":42791,\"start\":42787},{\"end\":42802,\"start\":42798},{\"end\":42812,\"start\":42810},{\"end\":42828,\"start\":42823},{\"end\":42843,\"start\":42839},{\"end\":42856,\"start\":42852},{\"end\":43360,\"start\":43351},{\"end\":43377,\"start\":43368}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":60814714},\"end\":32014,\"start\":31585},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":45998148},\"end\":32162,\"start\":32016},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4703661},\"end\":32595,\"start\":32164},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":502946},\"end\":33248,\"start\":32597},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":6681692},\"end\":33640,\"start\":33250},{\"attributes\":{\"id\":\"b5\"},\"end\":33933,\"start\":33642},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":102496818},\"end\":34425,\"start\":33935},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":719551},\"end\":34897,\"start\":34427},{\"attributes\":{\"doi\":\"arXiv:1406.5212\",\"id\":\"b8\"},\"end\":35207,\"start\":34899},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52952193},\"end\":35576,\"start\":35209},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594692},\"end\":36021,\"start\":35578},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b11\"},\"end\":36275,\"start\":36023},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":980236},\"end\":36630,\"start\":36277},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4800342},\"end\":37144,\"start\":36632},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b14\"},\"end\":37362,\"start\":37146},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8070108},\"end\":37837,\"start\":37364},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9494747},\"end\":38343,\"start\":37839},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13798326},\"end\":38834,\"start\":38345},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3977226},\"end\":39308,\"start\":38836},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1923223},\"end\":39785,\"start\":39310},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":545361},\"end\":40046,\"start\":39787},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":740063},\"end\":40308,\"start\":40048},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":215826266},\"end\":40663,\"start\":40310},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":215822736},\"end\":41154,\"start\":40665},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3285974},\"end\":41450,\"start\":41156},{\"attributes\":{\"doi\":\"arXiv:1606.04671\",\"id\":\"b25\"},\"end\":41866,\"start\":41452},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b26\"},\"end\":42143,\"start\":41868},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":6844431},\"end\":42574,\"start\":42145},{\"attributes\":{\"id\":\"b28\"},\"end\":42752,\"start\":42576},{\"attributes\":{\"id\":\"b29\"},\"end\":42963,\"start\":42754},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1806714},\"end\":43318,\"start\":42965},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":15276198},\"end\":43798,\"start\":43320}]", "bib_title": "[{\"end\":31665,\"start\":31585},{\"end\":32034,\"start\":32016},{\"end\":32251,\"start\":32164},{\"end\":32658,\"start\":32597},{\"end\":33302,\"start\":33250},{\"end\":33683,\"start\":33642},{\"end\":34041,\"start\":33935},{\"end\":34458,\"start\":34427},{\"end\":35259,\"start\":35209},{\"end\":35622,\"start\":35578},{\"end\":36344,\"start\":36277},{\"end\":36718,\"start\":36632},{\"end\":37502,\"start\":37364},{\"end\":37896,\"start\":37839},{\"end\":38405,\"start\":38345},{\"end\":38918,\"start\":38836},{\"end\":39355,\"start\":39310},{\"end\":39845,\"start\":39787},{\"end\":40077,\"start\":40048},{\"end\":40365,\"start\":40310},{\"end\":40727,\"start\":40665},{\"end\":41200,\"start\":41156},{\"end\":42210,\"start\":42145},{\"end\":43016,\"start\":42965},{\"end\":43342,\"start\":43320}]", "bib_author": "[{\"end\":31689,\"start\":31667},{\"end\":31703,\"start\":31689},{\"end\":31720,\"start\":31703},{\"end\":32050,\"start\":32036},{\"end\":32264,\"start\":32253},{\"end\":32286,\"start\":32264},{\"end\":32299,\"start\":32286},{\"end\":32318,\"start\":32299},{\"end\":32675,\"start\":32660},{\"end\":32690,\"start\":32675},{\"end\":32707,\"start\":32690},{\"end\":32721,\"start\":32707},{\"end\":32739,\"start\":32721},{\"end\":32757,\"start\":32739},{\"end\":32769,\"start\":32757},{\"end\":32782,\"start\":32769},{\"end\":32797,\"start\":32782},{\"end\":33321,\"start\":33304},{\"end\":33338,\"start\":33321},{\"end\":33354,\"start\":33338},{\"end\":33366,\"start\":33354},{\"end\":33699,\"start\":33685},{\"end\":33717,\"start\":33699},{\"end\":34056,\"start\":34043},{\"end\":34068,\"start\":34056},{\"end\":34480,\"start\":34460},{\"end\":34501,\"start\":34480},{\"end\":34966,\"start\":34948},{\"end\":34985,\"start\":34966},{\"end\":35000,\"start\":34985},{\"end\":35016,\"start\":35000},{\"end\":35275,\"start\":35261},{\"end\":35289,\"start\":35275},{\"end\":35302,\"start\":35289},{\"end\":35316,\"start\":35302},{\"end\":35328,\"start\":35316},{\"end\":35636,\"start\":35624},{\"end\":35651,\"start\":35636},{\"end\":35665,\"start\":35651},{\"end\":35675,\"start\":35665},{\"end\":36040,\"start\":36023},{\"end\":36055,\"start\":36040},{\"end\":36066,\"start\":36055},{\"end\":36362,\"start\":36346},{\"end\":36379,\"start\":36362},{\"end\":36391,\"start\":36379},{\"end\":36734,\"start\":36720},{\"end\":36745,\"start\":36734},{\"end\":36762,\"start\":36745},{\"end\":37202,\"start\":37190},{\"end\":37216,\"start\":37202},{\"end\":37220,\"start\":37216},{\"end\":37522,\"start\":37504},{\"end\":37914,\"start\":37898},{\"end\":37925,\"start\":37914},{\"end\":37931,\"start\":37925},{\"end\":38423,\"start\":38407},{\"end\":38437,\"start\":38423},{\"end\":38452,\"start\":38437},{\"end\":38466,\"start\":38452},{\"end\":38479,\"start\":38466},{\"end\":38933,\"start\":38920},{\"end\":38947,\"start\":38933},{\"end\":38966,\"start\":38947},{\"end\":39370,\"start\":39357},{\"end\":39391,\"start\":39370},{\"end\":39406,\"start\":39391},{\"end\":39422,\"start\":39406},{\"end\":39886,\"start\":39847},{\"end\":39897,\"start\":39886},{\"end\":39905,\"start\":39897},{\"end\":40103,\"start\":40079},{\"end\":40109,\"start\":40103},{\"end\":40399,\"start\":40367},{\"end\":40413,\"start\":40399},{\"end\":40422,\"start\":40413},{\"end\":40761,\"start\":40729},{\"end\":40775,\"start\":40761},{\"end\":40784,\"start\":40775},{\"end\":41218,\"start\":41202},{\"end\":41234,\"start\":41218},{\"end\":41462,\"start\":41452},{\"end\":41468,\"start\":41462},{\"end\":41476,\"start\":41468},{\"end\":41498,\"start\":41476},{\"end\":41517,\"start\":41498},{\"end\":41530,\"start\":41517},{\"end\":41549,\"start\":41530},{\"end\":41562,\"start\":41549},{\"end\":41952,\"start\":41936},{\"end\":41970,\"start\":41952},{\"end\":42231,\"start\":42212},{\"end\":42248,\"start\":42231},{\"end\":42265,\"start\":42248},{\"end\":42281,\"start\":42265},{\"end\":42303,\"start\":42281},{\"end\":42612,\"start\":42595},{\"end\":42626,\"start\":42612},{\"end\":42766,\"start\":42756},{\"end\":42782,\"start\":42766},{\"end\":42793,\"start\":42782},{\"end\":42804,\"start\":42793},{\"end\":42814,\"start\":42804},{\"end\":42830,\"start\":42814},{\"end\":42845,\"start\":42830},{\"end\":42858,\"start\":42845},{\"end\":43362,\"start\":43344},{\"end\":43379,\"start\":43362}]", "bib_venue": "[{\"end\":32938,\"start\":32876},{\"end\":34189,\"start\":34137},{\"end\":34686,\"start\":34602},{\"end\":35816,\"start\":35754},{\"end\":36903,\"start\":36841},{\"end\":38108,\"start\":38028},{\"end\":38600,\"start\":38548},{\"end\":39081,\"start\":39032},{\"end\":39563,\"start\":39501},{\"end\":40925,\"start\":40863},{\"end\":43159,\"start\":43097},{\"end\":43542,\"start\":43498},{\"end\":31782,\"start\":31720},{\"end\":32067,\"start\":32050},{\"end\":32362,\"start\":32318},{\"end\":32874,\"start\":32797},{\"end\":33429,\"start\":33366},{\"end\":33776,\"start\":33717},{\"end\":34135,\"start\":34068},{\"end\":34600,\"start\":34501},{\"end\":34946,\"start\":34899},{\"end\":35366,\"start\":35328},{\"end\":35752,\"start\":35675},{\"end\":36126,\"start\":36082},{\"end\":36429,\"start\":36391},{\"end\":36839,\"start\":36762},{\"end\":37188,\"start\":37146},{\"end\":37591,\"start\":37522},{\"end\":38026,\"start\":37931},{\"end\":38546,\"start\":38479},{\"end\":39030,\"start\":38966},{\"end\":39499,\"start\":39422},{\"end\":39909,\"start\":39905},{\"end\":40160,\"start\":40109},{\"end\":40471,\"start\":40422},{\"end\":40861,\"start\":40784},{\"end\":41296,\"start\":41234},{\"end\":41639,\"start\":41578},{\"end\":41934,\"start\":41868},{\"end\":42343,\"start\":42303},{\"end\":42593,\"start\":42576},{\"end\":43095,\"start\":43018},{\"end\":43438,\"start\":43379}]"}}}, "year": 2023, "month": 12, "day": 17}