{"id": 227126796, "updated": "2023-10-06 08:38:14.517", "metadata": {"title": "Ranking Neural Checkpoints", "authors": "[{\"first\":\"Yandong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xuhui\",\"last\":\"Jia\",\"middle\":[]},{\"first\":\"Ruoxin\",\"last\":\"Sang\",\"middle\":[]},{\"first\":\"Yukun\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Bradley\",\"last\":\"Green\",\"middle\":[]},{\"first\":\"Liqiang\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Boqing\",\"last\":\"Gong\",\"middle\":[]}]", "venue": "ArXiv", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "This paper is concerned with ranking many pre-trained deep neural networks (DNNs), called checkpoints, for the transfer learning to a downstream task. Thanks to the broad use of DNNs, we may easily collect hundreds of checkpoints from various sources. Which of them transfers the best to our downstream task of interest? Striving to answer this question thoroughly, we establish a neural checkpoint ranking benchmark (NeuCRaB) and study some intuitive ranking measures. These measures are generic, applying to the checkpoints of different output types without knowing how the checkpoints are pre-trained on which dataset. They also incur low computation cost, making them practically meaningful. Our results suggest that the linear separability of the features extracted by the checkpoints is a strong indicator of transferability. We also arrive at a new ranking measure, NLEEP, which gives rise to the best performance in the experiments.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2011.11200", "mag": "3110130182", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiJSZGWG21", "doi": "10.1109/cvpr46437.2021.00269"}}, "content": {"source": {"pdf_hash": "31d737977efb98471dc2ff32a619b2b1b25b3d12", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2011.11200v4.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2011.11200", "status": "GREEN"}}, "grobid": {"id": "3c88eaf401e63264a1da80c5d1b8c92e0597d507", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/31d737977efb98471dc2ff32a619b2b1b25b3d12.txt", "contents": "\nRanking Neural Checkpoints\n\n\nYandong Li \nFlorida 2 Google\nUniversity of Central\n\n\nXuhui Jia xhjia@google.com \nFlorida 2 Google\nUniversity of Central\n\n\nRuoxin Sang rxsang@google.com \nFlorida 2 Google\nUniversity of Central\n\n\nYukun Zhu \nFlorida 2 Google\nUniversity of Central\n\n\nBradley Green \nFlorida 2 Google\nUniversity of Central\n\n\nLiqiang Wang lwang@cs.ucf.edu \nFlorida 2 Google\nUniversity of Central\n\n\nBoqing Gong bgong@google.com \nFlorida 2 Google\nUniversity of Central\n\n\nRanking Neural Checkpoints\n\nThis paper is concerned with ranking many pre-trained deep neural networks (DNNs), called checkpoints, for the transfer learning to a downstream task. Thanks to the broad use of DNNs, we may easily collect hundreds of checkpoints from various sources. Which of them transfers the best to our downstream task of interest? Striving to answer this question thoroughly, we establish a neural checkpoint ranking benchmark (NeuCRaB) and study some intuitive ranking measures. These measures are generic, applying to the checkpoints of different output types without knowing how the checkpoints are pre-trained on which datasets. They also incur low computation cost, being practically meaningful. Our results suggest that the linear separability of the features extracted by the checkpoints is a strong indicator of transferability. We also arrive at a new ranking measure, N LEEP, which gives rise to the best performance in the experiments. Code is available at https : / / github . com / google -research / google-research/tree/master/rank_ckpt.\n\nIntroduction\n\nThere is an increasing number of pre-trained deep neural networks (DNNs), which we call checkpoints. We may produce hundreds of intermediate checkpoints when we sweep through various learning rates, optimizers, and losses to train a DNN. Furthermore, semi-supervised [10,4,49,36,58,39,37,8] and self-supervised [15,26,11,62,43] learning make it feasible to harvest DNN checkpoints with scarce or no labels. Fine-tuning [65,44] has become a de facto standard to adapt the pre-trained checkpoints to target tasks. It leads to faster convergence [16,27,51] and better performance [35] on the downstream tasks.\n\nHowever, not all checkpoints are equally useful for a target task, and some could even under-perform a randomly initialized checkpoint (cf. Section 2.2). This paper is con-* This work was done while the first author was an intern at Google. cerned with ranking neural checkpoints, which aims to measure how effectively fine-tuning can transfer knowledge from the pre-trained checkpoints to the target task. The measurement should be generic enough for all the neural checkpoints, meaning that it works without knowing any pre-training details (e.g., pre-training examples, hyperparameters, losses, early stopping stages, etc.) of the checkpoints. It also should be lightweight, ideally without training on the downstream task, to make it practically useful. We may use the measurement to choose the top few checkpoints before running fine-tuning, which is computationally more expensive than calculating the measurements.\n\nRanking neural checkpoints is crucial. Some domains or applications lack large-scale human-curated data, like medical images [48], raising a pressing need for high-quality pre-trained checkpoints as a warm start for fine-tuning. Fortunately, there exist hundreds of thousands of checkpoints of popular neural network architectures. For instance, many computer vision models are built upon ResNet [28], Inception-ResNet [56], and VGG [52]. As a result, we can construct a candidate pool by collecting the checkpoints released by different groups, for various tasks, and over distinct datasets.\n\nIt is nontrivial to rank the checkpoints for a downstream task. We explain this point by drawing insights from the related, yet arguably easier, task transferability problem [1,20,66,41], which aims to provide high-level guidance about how well a neural network pre-trained in one task might transfer to another. However, not all checkpoints pre-trained in the same source task transfer equally well to the target task [70,35]. The pre-training strategy also matters. Zhai et al. [68] find that combining supervision with self-supervision improves a network's transfer results on downstream tasks. He et al. [26] also show that selfsupervised pre-training benefits object detection more than its supervised counterpart under the same fine-tuning setup.\n\nWe may also appreciate the challenge in ranking neural checkpoints by comparing it with another related line of work: predicting DNNs' generalization gaps [40,31,5]. Jiang et al. [30] use a linear regressor to predict a DNN's generalization gap, i.e., the discrepancy between its training and test accuracies, by exploring the training data's margin distributions. Other signals studied in the literature include network complexity and noise stability. Ranking neural checkpoints is more challenging than predicting a DNN's generalization gap. Unlike the training and test sets that share the same underlying distribution, the downstream task may be arbitrarily distant from the source task over which a checkpoint is pre-trained. Moreover, we do not have access to the pre-training data at all. Finally, instead of keeping the networks static, fine-tuning dramatically changes all weights of the checkpoints.\n\nWe establish a neural checkpoint ranking benchmark (NeuCRaB) to study the problem systematically. Neu-CRaB covers various checkpoints pre-trained on widely used, large-scale datasets by different training strategies and architectures at a range of early stopping stages. It also contains diverse downstream tasks, whose training sets are medium-sized, making it practically meaningful to rank and fine-tune existing checkpoints. Pairing up all the checkpoints and downstream tasks, we conduct careful finetuning with thorough hyper-parameter sweeping to obtain the best transfer accuracy for each checkpoint-downstreamtask pair. Hence, we know the groundtruth ranking of the checkpoints for each downstream task according to the final accuracies (over the test/validation sets).\n\nA functional checkpoint ranking measurement should be highly correlated with the groundtruth ranking and, equally importantly, incurs as low computation cost as possible. We study several intuitive methods for ranking the neural checkpoints. One is to freeze the checkpoints as feature extractors and use a linear classifier to evaluate the features' separability on the target task. Another is to run fine-tuning for only a few epochs (to avoid heavy computation) and then evaluate the resulting networks on the target task's validation set. We also estimate the mutual information between labels and the features extracted from a checkpoint.\n\nFinally, we propose a lightweight measure, named Gaussian LEEP (N LEEP), to rank checkpoints based on the recently proposed log expected empirical prediction (LEEP) [41]. LEEP was originally designed to measure between-task transferabilities. It cannot handle the checkpoints pre-trained by unsupervised or self-supervised learning since it requires all checkpoints to have a classification head. Its computation cost could blow up when the classification head corresponds to a large output space. Moreover, it depends on the classification head's probabilistic output, which, unfortunately, is often overly confident [25].\n\nTo tackle the above problems, we replace the checkpoints' output layer with a Gaussian mixture model (GMM). This simple change kills two birds with one stone. On the one hand, GMM's soft assignment of input to clusters seamlessly applies to LEEP, resulting in the lightweight, effective N LEEP measure that works regardless of the checkpoints' output types. On the other hand, since we fit GMM to the target task's data, instead of the pre-training data of a different source task, the cluster assignment probabilities are likely more calibrated than the classification probabilities for the target task, if there exist classification heads.\n\n\nThe Neural Checkpoint Ranking Benchmark (NeuCRaB)\n\nWe formalize ranking neural checkpoints as follows. Suppose we have m pre-trained neural networks, called checkpoints,\nC := {\u03b8 i } m i=1\n. Denote by T a distribution of tasks. Without loss of generality, we mainly study classification downstream tasks, each of which, t \u223c T , contains a training set and a test set. An evaluation procedure, G : C \u00d7T \u2192 R, replaces the output layer of a checkpoint \u03b8 i with a linear classifier for a downstream task t, followed by fine-tuning using the task's training set. It employs hyperparameter sweeping and returns the best accuracy on the test set. We apply this evaluation procedure to all the checkpoints for task t and obtain their test accuracies:\nG t := {G(\u03b8 i , t)} m i=1 \u2208 R m ,(1)\nwhich defines the groundtruth ranking list for task t.\n\nDenote by R all measures that return a ranking score for any checkpoint-task pair under a computation budget b. A measure R \u2208 R gives rise to the following ranking scores for a task t,\nR t := {R(\u03b8 i , t; b)} m i=1 \u2208 R m ,(2)\nwhere we underscore the computation budget b in the measure R(\u00b7, \u00b7; b). Our objective in ranking neural checkpoints is to find the best ranking measure in expectation,\nR * \u2190 arg max R\u2208R E t\u223cT M(R t , G t )(3)\nwhere M is a metric evaluating the ranking scores R t against the test accuracies G t . Section 2.3 details the evaluation methods used in this work. Equipped with such a ranking measure R * , we can identify the checkpoints that potentially transfer to a downstream task better than the others without resorting to heavy computation.\n\n\nDownstream Tasks T\n\nFollowing the design principle of [68], we study diverse downstream tasks including Caltech101 [22], Flow-ers102 [42], Sun397 [63], and Patch Camelyon [62]. These tasks are representative of general object recognition, finegrained object recognition, scenery image classification,  and medical image classification, respectively. Table 1 in Appendix A.1 provides more details of these tasks. A common theme is that their training sets are all mediumsized, making it especially beneficial to leverage pre-trained checkpoints to avoid overfitting.\n\n\nNeural Checkpoints C\n\nThanks to the broad use of DNNs, one may collect neural checkpoints of various types from multiple sources. To simulate this situation, we construct a rich set of checkpoints and separate them into three groups according to the pretraining strategies and network architectures.\n\nGroup I: Checkpoints of mixed supervision. The first group of checkpoints are pre-trained with mixed supervision till convergence, including supervised learning, selfsupervised learning, semi-supervised learning, and the discriminators or encoders in deep generative models. It consists of 16 ResNet-50s [28]. We borrow 14 models pretrained on ImageNet [14] from [68]. Among them, four are pre-trained by self-supervised learning (Jigsaw [43], Relative Patch Location [15], Exemplar [17], and Rotation [23]), six are the discriminators of generative models (WAE-UKL [50], WAE-GAN, WAE-MMD [59], Cond-BigGAN, Uncond-BigGAN [9], and VAE [34]), two are based on semi-supervised learning (Semi-Rotation-10% and Semi-Exemplar-10% [67]), one is by fully supervised learning (Sup-100%-Img [28]), and one is trained with a hybrid supervised loss (Sup-Exemplar-100% [67]). We also add two supervised checkpoints pre-trained on iNaturalist (Sup-100%-Inat) [61] and Places365 (Sup-100%-Pla) [69], respectively. Using the evaluation procedure G(\u03b8 i , t) (cf. equation (1)), we obtain their final accuracies on the downstream tasks described in Section 2.1. Figure 1 shows the best fine-tuning accuracies offset by their mean for better visualization, and Table 4 (in Appendix) contains the absolute accuracy values. We include the training from scratch (From-Scratch) for comparison. Most of the checkpoints yield significantly better finetuning results than From-Scratch. Some of the discriminators in generative models, however, under-perform From-Scratch. The highest-performance checkpoints change from one downstream task to another.\n\nGroup II: Checkpoints at different pre-training stages. This group comprises 12 ResNet-50s pre-trained by fully supervised learning on ImageNet, iNaturalist, and Places-365. We save a checkpoint right after each learning rate decay, resulting in four checkpoints per dataset.  Table 5 in Appendix show the best fine-tuning accuracies over the four downstream tasks, where Img-90k refers to the checkpoint trained on ImageNet for 90k iterations. Interestingly, the downstream tasks favor different pre-training sources, indicating the necessity of studying between-task transferabilities [68,66]. However, the source task information may be not known for all checkpoints. Moreover, the converged model over a source task does not always transfer the best to a downstream task (cf. Img-270k vs. Img-300k on Camelyon, Inat-270k vs. Inet-300k on Flowers102, etc.). We hence construct this NeuCRaB for studying the ranking of neural checkpoints without accessing how one pre-trained the checkpoints over which dataset.\n\nGroup III: Checkpoints of heterogeneous architectures. Kornblith et al. [35] show that better network architectures can learn better features that can be transferred across vision-based tasks. Therefore, we construct the third group of checkpoints by using different neural architectures. Four of them belong to the Inception family [57], one is Inception-ResNet-v2 [56], six come from the MobileNet family [29], and two are from the ResNet-v1 family [28]. We train them on ImageNet till convergence. Figure 3 and Table 6 in Appendix visualize their fine-tuning accuracies on the four downstream tasks.\n\n\nEvaluation Metrics M\n\nWe use multiple metrics (cf. M in eq. (3)) to evaluate the checkpoint ranking measures.\n\nRecall@k: A practitioner may have resources to test up to k checkpoints for their task of interest. We consider it a success if a measure ranks the highest-performance checkpoint into the top k. A measure's Recall@k is the ratio between the number of downstream tasks on which it succeeds and the total number of tasks. We employ k = 1 and k = 3 in the experiments.\n\nTop-k relative accuracy (Rel@k): Given a task, a ranking measure returns an ordered list of the checkpoints. If the measure selects a high-performing checkpoint to the top k despite that it misses the highest-performance one, we do not want to overly penalize it. This Rel@k is the ratio between the best fine-tuning accuracy on the downstream task with the top k checkpoints and the the best fine-tuning accuracy with all the checkpoints.\n\nPearson correlation: We incorporate Pearson's r [45] to compute the linear correlation between a measure' ranking scores R t and the evaluation procedure's final accuracies G t .\n\nKendall ranking correlation: We also include Kendall's \u03c4 [32] to measure the ordinal association between a ranking measure R and the evaluation procedure G for each task. After all, what matter is the order of the checkpoints rather than the precise ranking scores.\n\n\nCheckpoint Ranking Methods\n\nIn this section, we describe some intuitive neural checkpoint ranking methods. These methods strive to achieve high correlation with the checkpoint evaluation procedure G at low computation cost.\n\n\nFine-tuning with Early Stopping\n\nIf there is no constraint over computing, the evaluation procedure G itself becomes the gold ranking measure. Hence, a natural ranking method is the fine-tuning with early stopping, by which the model is far from convergence. The premature models' test accuracies are the ranking scores. Experiments reveal that it is hard to forecast from the premature models.\n\n\nLinear Classifiers\n\nWe derive the second ranking method also from the evaluation procedure G, which replaces a checkpoint's output layer by a linear classifier tailored for the downstream task. We train the linear classifier while freezing the other layers. The ranking score equals the classifier's test accuracy. It is worth mentioning that self-supervised learning [11,26,24] often adopts this practice as well to evaluate the learned feature representations. We shall see that the linear separability of the features extracted from a checkpoint is a strong indicator of the performance of fine-tuning the full checkpoint.\n\n\nMutual Information\n\nSuppose the extracted features' quality well correlates with a checkpoint's final accuracy on a downstream task. Besides the linear separability above, we can rank the checkpoints by their mutual information between the highdimensional features and discrete labels of the downstream task. We employ the state-of-the-art I \u03b1 mutual information estimator [46], where \u03b1 controls the trade-off between variance and bias. It is a variational lower bound parameterized by a neural network. Belghazi et al. [6] report that the neural estimators generally outperform prior mutual information estimations, especially when the variables are highdimensional. We use the code released by the authors to calculate I \u03b1 [46].\n\n\nLEEP for the Checkpoints with Classification Heads\n\nTo rank the checkpoints pre-trained over classification source tasks, the recently proposed LEEP [41] measure is directly applicable despite that it was originally designed for between-task transfer. Denote by Z the classification space of a checkpoint \u03b8. We can interpret \u03b8(x) z , the z-th (softmax) output element, as the probability of classifying the input x into the class z \u2208 Z. Given a downstream task t \u223c T and its test set {(x j , y j )} n j=1 , the LEEP ranking score for the checkpoint \u03b8 is calculated by\nR LEEP (\u03b8, t) := 1 n n j=1\nlog P (y j |x j , \u03b8, t)\nP (y|x, \u03b8, t) := z\u2208ZP (y|z)\u03b8(x) z(4)\nwhereP (y|z) is the empirical conditional distribution of the downstream task's label y given the source label z \u2208 Z, and P (y|x, \u03b8, t) is a \"dummy\" classifier, which firstly draws a label z from the checkpoint \u03b8(x) and then draws a class y from the conditional distributionP (y|z). Denote by {x j , y j }\u00f1 j=1 , y \u2208 Y, the downstream task's training set. LEEP computes the conditional distribution P (y|z) by \"counting\". The joint distributionP (y, z) due to the checkpoint \u03b8 i\u015d\nP (y, z) = 1 n j:yj =y \u03b8(x j ) z ,(5)\nwhich gives rise to the conditional distributionP (y|z) = P (y, z)/P (z) =P (y, z)/ y\u2208YP (y, z).\n\nIn the experiments, LEEP and the linear classifier are the second best ranking methods for the checkpoints pretrained for classification. However, LEEP's computation cost is high when a checkpoint's classification output is high-dimensional (e.g., iNaturalist contains more than 8000 classes). Besides, its softmax estimation of the classification probability \u03b8(x) z is often poorly calibrated [25]. Finally, it does not apply to the checkpoints with no classification heads.\n\n\nN LEEP\n\nWe propose a variation to LEEP that applies to all types of checkpoints including those obtained from unsupervised learning and self-supervised learning. It can also avoids the overly confident softmax.\n\nFeeding the training data of a downstream task into a checkpoint, we obtain their feature representations. The representations are thousands of dimensions, depending on the checkpoint's neural architecture. We reduce their dimension by using the principal component analysis (PCA). Denote by s the resultant low-dimensional representation of the input x.\n\nWe then fit a Gaussian mixture model (GMM),\nP (s) = v\u2208V \u03c0 v N (s|\u00b5 v , \u03a3 v )\n, to the training set {s j }\u00f1 j=1 , where V is a collection of all the Gaussian components, and \u03c0 v , v \u2208 V, are the mixture weights. It is convenient to compute the posterior distribution:\nP (v|x) = P (v|s) \u221d \u03c0 v N (s|\u00b5 v , \u03a3 v ),(6)\nwhich is arguably more reliable than the class assignment probability \u03b8(x) z output by the softmax classifier because we fit GMM to the downstream task's training data, whereas the softmax classifier is learned from a different source task. Hence, we arrive at an improved ranking measure, named N LEEP, by replacing \u03b8(x) z , the probability of classifying an input x to the class z, in equations (4-5) by the posterior distribution P (v|x).\n\n\nExperiments on NeuCRaB\n\nThere are free parameters in each of the ranking methods. Before presenting the main results, we study how the free parameters in N LEEP affect its checkpoint ranking performance. Figure 2 illustrates N LEEP's Kendall's \u03c4 values over Groups I and II with different PCA feature dimensions and the numbers of Gaussian components. Each Kendall's \u03c4 is averaged across all the downstream tasks; the higher, the better. Along the vertical axes, we change the feature dimensions by keeping different percentages of the PCA energies; PCA50 means the percentage is 50%. Along the horizontal axes, we adopt different numbers of Gaussian components in GMM; 2\u00d7 means the number is twice the class number of the downstream task. Notably, the Kendall's \u03c4 values remain relatively stable. In the remaining experiments with N LEEP, we fix the PCA energy to 80% and the number of Gaussian components five times the class number of a downstream task.\n\n\nComparison Results\n\nTables 1, 2, and 3 show the checkpoint ranking methods' performance on Groups I (checkpoints of mixed supervision), II (different pre-training stages), and III (heterogeneous architectures), respectively. We also union the three groups and present the corresponding ranking performance in Table 2 in Appendix. The numbers in the tables are the average over all downstream tasks. In addition to the evaluation metrics detailed in Section 2.3, the GFLOPS column measures the ranking methods' computing performance; the lower, the better.\n\nWe report multiple variations of the ranking methods in the tables. Fine-tuning is computationally expensive, so we stop it after one or five epochs. The linear classifiers are less so as we save the feature representations of downstream tasks' after one forward pass to the checkpoints. We report the linear classifiers' ranking results after training them for one epoch, five epochs, and convergence. We test \u03b1 = 0.01 and \u03b1 = 0.50 in the I \u03b1 mutual information estimator. Additionally, we experiment with I \u03b1 after reducing the feature dimensions by using PCA.\n\n\nMain Findings\n\nIn each column of Tables 1, 2, 3, and Table 2 in Appendix, we highlight the best and second best by the bold font and underscore, respectively.\n\nThe mutual information fails to rank high-performing checkpoints to the top and even produces negative Pearson and Kendall correlations, probably because of the features' high dimensions. Reducing the feature dimensions by PCA significantly improves the mutual information's ranking performance; MI w/ PCA (\u03b1=0.01) leads to the second best Rel@1, Recall@3 and Rel@3 among the rank-Group I Group II ing methods in Group III, the checkpoints of heterogeneous neural architectures. Varying \u03b1 in the I \u03b1 mutual information estimator [46] can control the trade-off between variance and bias. MI w/ and w/o PCA (\u03b1=0.01) perform better than MI w/ and w/o PCA (\u03b1=0.50), respectively. It indicates that neural checkpoint ranking requires low-bias MI estimator since smaller \u03b1 means low-bias but high-variance estimation.\n\nFine-tuning up to some epochs turns out the worst ranking methods because it leads to low correlation with the groundtruth ranking and yet incurs heavy computation. Similarly, training the linear classifier up to one or five epochs does not perform well except in Group II. These results indicate that it is difficult to forecast the checkpoints' final performance from premature models. Fine-tuning (5 epochs) and Linear (5 epochs) perform better than Finetuning (1 epoch) and Linear (1 epoch) in terms of Person and Kendall correlation, respectively. However, they all fail to select the top checkpoint in Group I and Group III since they produce lower Recall@1 and Recall@3 than others. One possible reason is that the evaluation accuracies of checkpoints in the early stage tend to have large variance.\n\nFeature qualities before fine-tuning the checkpoints. If we train the linear classifiers till convergence, they become the best in Group II, and the second best checkpoint ranking method in Groups I and III in terms of Pearson and Kendall correlations. It can also produce better Recall@1 and Recall@3 than Linear (1 epoch) and Linear (5 epoch) in Groups I, II and III since the evaluation accuracies of converged models are more stable than models in the early training stage. Note that the linear classifiers' accuracies, i.e., the ranking scores, imply the linear separability of the features extracted by the checkpoints. Recall that the mutual information with PCA feature dimension reduction is among the second best (Rel@1, Recall@3 and Rel@3) in Group III. Since both methods measure the feature representations' quality by the downstream tasks' labels, we conjecture that the quality of the features is a strong indicator of the checkpoints' final fine-tuning performance on the downstream tasks. It would be interesting to study other feature quality measures beyond the linear separability and mutual information in future work.\n\nN LEEP performs consistently well in all the groups of checkpoints over all the evaluation metrics with the lowest computation cost . In contrast, the original LEEP measure is not applicable to Group I, the checkpoints of mixed supervision, because it requires that the checkpoints have a classification output layer. Overall, LEEP is the second best over all evaluation metrics among the ranking methods in Groups II and III, whose checkpoints all have a classification output layer. Specifically, LEEP can produce the second best Recall@1, Recall@3 and Rel@3 in Group II, and the best Recall@3, the best Rel@3 and the second best Kendall correlation in Group III. It is a more consistent indicator than fine-tuning, linear classifier, or MI based ranking methods. However, LEEP can not produce better results than N LEEP, and it requires slightly larger GFLOPS due to the extra computation cost from the classification head.\n\nWe conjecture that N LEEP outperforms LEEP mainly because GMMs calibrate the posterior probabilities better than the checkpoints' softmax classifiers. The checkpoint ranking quality of LEEP score hinges on the performance of the 'dummy classifier' -P (y|x, \u03b8, t), and \u03b8(x) z is the key element to calculate it. However, \u03b8(x) can be poorly calibrated [41] and it can not represent a true probability. In contrast, P (v|x) used in N LEEP is indeed the probability that the sample belongs to one cluster from a mixture of Gaussian distributions and it can remedy the poor-calibrated problem in LEEP.\n\n\nComputational costs.\n\nMoreover, we highlight the GFLOPS column in the tables. N LEEP and LEEP exhibit a clear advantage over the other checkpoint ranking methods in terms of computing. The main reason is that N LEEP and LEEP can avoid intensive computation from neural network training, and they only require one forward pass through the training data.\n\nComparing different groups of the checkpoints. Checkpoint ranking on different groups of checkpoints varies in degrees of difficulty. The most challenging group is Group III, the checkpoints of heterogeneous neural architectures.\n\nAll the ranking methods produce lower correlations with the groundtruth ranking, and they can barely select the top checkpoints in this group. The main reason is that the neural architectures matter for transfer learning [35]. Besides, heterogeneous neural architectures can demonstrate various performance even if we train them from scratch on downstream tasks. Ranking neural checkpoints by the feature representations of the last layer is not sufficient for those checkpoints. We may explore more advanced ranking methods considering the structures of the deep neural networks in the future.\n\nCheckpoint ranking on Group II is easier than on Group I since all the ranking methods can achieve relatively better results over all evaluation metrics in Group II. The results indicate that checkpoints with various training strategies (Group I) can bring more complex knowledge from source domains, comparing with checkpoints with different early stopping stages (Group II). In addition, fine-tuning the entire models and training linear classifiers up to one or five epochs perform significantly better on Group II since those ranking methods are based on early stopping as well.\n\n\nAdditional experiments in the supplementary materials.\n\nTo simulate a sufficiently large pool of checkpoints in the real applications, we finally combine the checkpoints in Group I, II, and III into one large group and conduct checkpoint ranking experiments on it. We also add one more group of checkpoints with ResNet-101s [28] to evaluate the checkpoint ranking on deeper models. Please see more details in Appendix A.3 and A.4. We also take object detection and instance segmentation as downstream tasks and conduct preliminary experiments on VOC [12] and Cityscapes [13]. Please refer to Appendix A.6 to see detailed discussions.\n\nAlthough the benchmark can be easily extended to many downstream tasks in other modalities, e.g., voice, text, and cross-modal modalities, we steer our attention into comparing several intuitive ranking measures on the variants of checkpoints, covering different training strategies, source domains, and architectures at a range of early stopping stages. We formalize the checkpoint ranking idea, demonstrate the existence of an effective yet lightweight measure, N LEEP, and hope it can shed light on more efficient ranking methods and practical applications.\n\n\nRelated Work\n\nOur work is broadly related to task transferability and neural networks' generalization gap. Task transferability. A task usually refers to a joint distribution over input and label. Task transferability aims to predict how well a deep neural network pre-trained on a source task transfers to the target task. One may estimate the task transferability by data similarities regardless of models being used. Some work in this line includes con-ditional entropy [60], data set distance as optimal transport [2], F -relatedness [7], A-distance [33], and discrepancy distance [38]. Besides, Poole et al. [46] derived information theoretic bounds. These methods are generally hard to compute in practice and rely on the availability of the source data. Some recent task transferability estimators involve both data and the models. Taskonomy [66] is a fully computation method, where task similarity scores are obtained by transfer learning experiments. Dwivedi et al. [19] analyzed the representation similarities to construct a task taxonomy. Besides the models trained on source tasks, all these methods also require a fine-tuned or independently trained model from the target task. In contrast, our work aims to find checkpoint ranking measures that are lightweight in computing and requires no access to the source tasks.\n\nRecent works demonstrated that using pre-trained checkpoints that have similar feature representations as the target task's representations can improve transfer learning [19,54,55]. Song et al. [54,55] employed attribution maps to compare two models and then quantified transferabilities by the similarity of two models. Those approaches all require a converged model on target datasets, incurring intensive computation. However, we want to design a lightweight method for ranking checkpoints, ideally without any training procedures. Predicting neural networks' generation gap. The difference between a model's performance on the training data versus its performance on test data is known as the generalization gap. It is practically useful and theoretically impactful to predict a neural network's generalization gap. Most recent work does so by finding a set of features that is predictive of the generalization, e.g., by estimating data margins [5,21,53]. Jiang et al. [30] and Yak et al. [64] demonstrate how the margin signatures of a neural network can predict the generalization gap with small errors. Besides, the network complexity and noise stability are also useful cues [40,31,5,3]. Our problem substantially differs from predicting the neural networks' generalization gap, which is concerned with the training and test data sets that share the same underlying distribution. We instead care about the results after fine-tuning a network's checkpoint.\n\n\nConclusion\n\nDeep learning has triumphed over many fields in both research and real-world applications. There must exist hundreds of thousands of DNNs trained and released by various groups. To this end, it is natural to select an existing, promising DNN checkpoint as a warm start to a training procedure when solving a new task. How to identify useful checkpoints from a large pool for the target task? Towards answering this question, we present NeuCRaB, a thorough benchmark covering diverse downstream tasks and pre-trained DNN checkpoints, along with N LEEP, a lightweight, effective checkpoint ranking measure.\n\nThe experiments with linear classifiers and mutual information (after PCA) reveal that the features extracted from the checkpoints are good indicators of the checkpoints' potential in transfer learning. It is worth exploring other ways of evaluating the features' quality in future work. It is also interesting to investigate the checkpoints' inherent signatures, such as topology and stability to noise, which might be informative of their transferabilities. Finally, some learning-based methods in predicting networks' generalization gaps are also promising for the checkpoint ranking problem.\n\n\nA. Appendix\n\nIn this appendix, we provide the following details to support the main text: Section A.6: Neural checkpoints ranking on object detection and instance segmentation.\n\n\nA.1. Downstream tasks\n\nIn this section, we describe the datasets used for the downstream tasks as shown in Table 4. More specifically, Caltech101 [22] contains 101 classes, including animals, airplanes, chairs and etc, the image size varies from 200 to 300 pixels per edge. Flowers102 [42] have 102 classes, with 40 to 248 training images per class, each image has at least 500 pixels. Patch Camelyon [62] contains 327,680 images of histopathologic scans of lymph node sections with image size of 96x96, which is collected to predict the presence of metastatic tissue. Sun397 [63] is a scenery benchmark with 397 classes, including cathedral, staircase, shelter, river, or archipelago. There are at least 100 images per class. The images are in 200x200 or higher resolutions. We believe the dataset portfolio well represents a broad set of vision tasks.\n\n\nA.2. Hyper-parameter Sweep\n\nWe adopt the similar experiment setting as in [68] to fine-tune the neural networks on the downstream tasks. Specifically, we set the batch size to 512 and use SGD with momentum of 0.9. We do not use weight decay for fine-tuning, and we set it to be 0.01 times the learning rate [71] when training from scratch. We perform pertask hyper-parameter search. For each task, we sweep the learning rate in {0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5} and the training step in {2500, 5000, 10000, 15000, 20000, 400000}. We incorporate inception data augmentation [56] for pre-training checkpoints and we do not use data-augmentation when we fine-tune the neural networks on the downstream tasks to emphasize the effect of transfer learning.\n\n\nA.3. Comparison results on all checkpoints in Groups I, II, III\n\nTo obtain a comprehensive analysis, we also consolidate the checkpoints from Group I, II and III into one group (including 41 checkpoints in total) and then apply the ranking methods on it. Table 5 shows the comparison results. The results further evaluate our observations in Section 4 of the main text. N LEEP performs consistently well on the big group of checkpoints with lowest computation cost. Linear separability of the feature representation is also a good indicator for ranking a large group of neural checkpoints. Fine-tuning with early stopping and mutual information estimator produce poor correlations. The ranking qualities of different ranking methods on the large group of checkpoints are in sharper contrast than on small groups. For instance, the Pearson's r of N LEEP vs. Finetune (5 epochs) on the large group is 83.71 vs. 27.84 but they perform 72.84 vs. 68.47 on Group II ( Table 2 in the main text). It indicates that N LEEP is a low-variance and low-bias checkpoint ranking estimator, while early stopping may produce high-variance ranking results.\n\n\nA.4. Group IV: Supervised ResNet101s\n\nWe incorporate another group of checkpoints, including 12 ResNet101 [28] models pre-trained by fully supervised learning on ImageNet [14], iNaturalist [61], and Places-365 [69]. We obtain the checkpoints in the same way as we have done for Group II, but with ResNet101 architecture. We want to study how different model architecture and model size affect the ranking quality. Figure 3 and Table 10 show the fine-tuning accuracy on 4 downstream tasks. The relative fine-tuning accuracies are similar to the accuracies on Group II. We also observe that a converged checkpoint does not necessarily demonstrates the best performance on the downstream tasks (cf. Img-270k is better than Img-300k on Flowers102 [42]). Table 6 shows the comparison results of ranking methods on those checkpoints. The relative performance among the ranking methods is similar to what they do in Group II ( \n\n\nA.5. More experimental results on Groups I-IV\n\nWe show more comparison results on NeuCRaB in this section. Figures 4 and 5 show the best fine-tuning accuracies offset by their mean (for better visualization) on Groups II and III, respectively. Table 7, 8, 9, 10 demonstrate the absolute best fine-tuning accuracies on Groups I-IV, respectively.\n\n\nA.6. Neural checkpoint ranking for object detection and instance segmentation\n\nWe also evaluate on object detection and segmentation tasks, and show the results in Tables 11. Specifically, we incorporate the recent self-supervised MoCo models (MoCov1, MoCov2, MoCov2-800epoch) and a ResNet50 model (supervised pretrained on ImageNet) into a new group of checkpoints. We evaluate checkpoint ranking on Pascal VOC (object detection) and Cityscapes (instance segmentation). In order to adapt N LEEP to detection and segmentation tasks, we assign multiple ground truth labels for one image if it includes multiple object categories and extract the image-level features to perform GMM. We adapt N LEEP to detection and segmentation tasks by assigning multi-labels to images with multiple object categories. The experiment results demonstrate that N LEEP consistently outperforms the fine-tune and linear evaluation based approachs. We plan to include more diverse downstream tasks in NeuCRaB to facilitate future research.  Figure 3. Difference between the fine-tuning accuracy of each checkpoint and the mean fine-tuning accuracy on Group IV. Black bar means From-Scratch. Red, green and orange bars represent ImageNet models, iNaturalist models and Places365 models, respectively. Img-90k means the checkpoint obtained by early stopping at the 90k-th iteration on ImageNet, and so on.  Table 6. Comparison results on Group IV (GFLOPS excludes a forward pass on training data, which takes 6.27E5 GFLOPS shared by all). Figure 4. Difference between the fine-tuning accuracy of each checkpoint and the mean fine-tuning accuracy on Group II. Black bar means From-Scratch. Red, green and orange bars represent ImageNet models, iNaturalist models and Places365 models, respectively. Img-90k means the checkpoint obtained by early stopping at the 90k-th iteration on ImageNet, and so on.   \n\n\nMethod\n\nFigure 1 .\n1Fine-tuning the checkpoints in Group I on four downstream tasks. We keep the best fine-tuning accuracy for each checkpointtask pair after hyper-parameter sweeping. For better visualization, the values are offset by their mean (cf.\n\nFigure 2 .\n2N LEEP' checkpoint ranking performance, evaluated by Kendall's \u03c4 , on Groups I and II in NeuCRaB. We vary the PCA feature dimension and the number of Gaussian components in GMM.\n\n\nTraining details of pre-training and finetuning. Section A.3: Comparison results on the combined group of checkpoints in Groups I, II and III. Section A.4: Another group of checkpoints with ResNet101s at different pre-training stages. Section A.5: More experiment results on Groups I-IV.\n\nFigure 5 .\n5Difference between the fine-tuning accuracy of each checkpoint and the mean fine-tuning accuracy on Group III. The colors of bars represent the models trained with different architectures. Brown: Inception-ResNet-V2. Red: Inception family. Green: MobileNet family and their variants. Orange: ResNet-v1 family.\n\nTable 4\n4in Appendix for the \n\n\nTable 2\n2in the main text). Except that they perform better on ResNet101s, e.g., Linear (converged) can achieve 68.60 in terms of Kendall's \u03c4 on ResNet50s versus 73.48 on ResNet101s, N LEEP can get 72.84 in terms of Pearson's r on ResNet50s versus 83.22 on ResNet101s. The observation reveals that the ranking of deeper checkpoints may be more predictable than shallow ones.\n\nTable 4 .\n4Statistics of the datasets associated with the downstream tasksTable 5. Comparison results on all checkpoints in Group I, II, III (GFLOPS excludes a forward pass on training data, which takes 2.73E5 GFLOPS shared by all).Method \nRecall@1 Rel@1 Recall@3 Rel@3 Pearson Kendall GFLOPS \nLinear (1 epoch) \n0.00 \n99.13 \n25.00 \n99.46 \n22.30 \n13.42 \n4.45E4 \nLinear (5 epochs) \n0.00 \n99.13 \n25.00 \n99.21 \n42.99 \n31.64 \n4.47E4 \nLinear (converged) \n25.00 \n99.42 \n50.00 \n99.73 \n76.22 \n61.22 \n4.79E4 \nFine-tune (1 epoch) \n0.00 \n96.69 \n0.00 \n98.16 \n3.84 \n6.50 \n5.85E5 \nFine-tune (5 epochs) \n0.00 \n99.49 \n0.00 \n99.49 \n27.20 \n27.16 \n3.84E6 \nMI (\u03b1=0.01) [46] \n0.00 \n77.50 \n0.00 \n81.44 \n1.12 \n7.16 \n1.52E5 \nMI (\u03b1=0.50) \n0.00 \n66.51 \n0.00 \n90.07 \n-4.05 \n-14.22 \n1.52E5 \nMI w/ PCA (\u03b1=0.01) \n0.00 \n89.18 \n50.00 \n99.84 \n12.14 \n20.99 \n5.57E4 \nMI w/ PCA (\u03b1=0.50) \n0.00 \n97.07 \n0.00 \n98.70 \n-14.03 \n-2.39 \n5.57E4 \nLEEP [41] \n-\n-\n-\n-\n-\n-\n-\nN LEEP \n50.00 \n99.47 \n50.00 \n99.78 \n83.71 \n68.18 \n12.86 \n\n\n\nTable 10 .\n10Absolute fine-tuning accuracy on Group IV.Table 11. Left: Checkpoint ranking results on the Pascal VOC Object Detection Benchmark (trained on VOC 2007 train+val + VOC 2012 train+val, tested on VOC 2007 using AP). Right: Checkpoint ranking for Cityscapes instance segmentation.Method \nRecall@1 Pearson Kendall \nLinear (1 epoch) \n\n18.45 \n-33.33 \nLinear (5 epoch) \n\n40.77 \n0.00 \nLinear (converged) \n\n61.57 \n54.77 \nFine-tune (1 epoch) \n\n20.55 \n0.00 \nFine-tune (5 epoch) \n\n50.46 \n33.33 \nN LEEP \n\n66.59 \n66.67 \n\nMethod \nRecall@1 Pearson Kendall \nLinear (1 epoch) \n\n23.55 \n-33.33 \nLinear (5 epoch) \n\n55.43 \n0.00 \nLinear (converged) \n\n68.32 \n33.33 \nFine-tune (1 epoch) \n\n38.85 \n-33.33 \nFine-tune (5 epoch) \n\n51.22 \n33.33 \nN LEEP \n\n82.66 \n66.67 \n\n\n\nTask2vec: Task embedding for meta-learning. Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, C Charless, Stefano Fowlkes, Pietro Soatto, Perona, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionAlessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C Fowlkes, Ste- fano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 6430-6439, 2019. 1\n\nDavid Alvarez, -Melis , Nicol\u00f2 Fusi, arXiv:2002.02923Geometric dataset distances via optimal transport. arXiv preprintDavid Alvarez-Melis and Nicol\u00f2 Fusi. Geometric dataset distances via optimal transport. arXiv preprint arXiv:2002.02923, 2020. 8\n\nStronger generalization bounds for deep nets via a compression approach. Sanjeev Arora, Rong Ge, Behnam Neyshabur, Yi Zhang, arXiv:1802.05296arXiv preprintSanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compres- sion approach. arXiv preprint arXiv:1802.05296, 2018. 8\n\nLearning with pseudo-ensembles. Philip Bachman, Ouais Alsharif, Doina Precup, Advances in neural information processing systems. Philip Bachman, Ouais Alsharif, and Doina Precup. Learn- ing with pseudo-ensembles. In Advances in neural informa- tion processing systems, pages 3365-3373, 2014. 1\n\nSpectrally-normalized margin bounds for neural networks. L Peter, Dylan J Bartlett, Matus J Foster, Telgarsky, Advances in Neural Information Processing Systems. 1Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6240-6249, 2017. 1, 8\n\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, R Devon Hjelm, arXiv:1801.04062Mine: mutual information neural estimation. arXiv preprintMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018. 4\n\nExploiting task relatedness for multiple task learning. Shai Ben, - David, Reba Schuller, Learning Theory and Kernel Machines. Springer8Shai Ben-David and Reba Schuller. Exploiting task relat- edness for multiple task learning. In Learning Theory and Kernel Machines, pages 567-580. Springer, 2003. 8\n\nMixmatch: A holistic approach to semi-supervised learning. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin A Raffel, Advances in Neural Information Processing Systems. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In Ad- vances in Neural Information Processing Systems, pages 5050-5060, 2019. 1\n\nLarge scale gan training for high fidelity natural image synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, arXiv:1809.11096arXiv preprintAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 3\n\nSemi-supervised learning. Olivier Chapelle, Bernhard Scholkopf, Alexander Zien, chapelle, o. et al.book reviewsOlivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews].\n\n. IEEE Transactions on Neural Networks. 203IEEE Transactions on Neural Net- works, 20(3):542-542, 2009. 1\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, arXiv:2002.0570914arXiv preprintTing Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020. 1, 4\n\nThe pascal visual object classes (voc) challenge. Mark Everingham, Luc Van Gool, K I Christopher, John Williams, Andrew Winn, Zisserman, International journal of computer vision. 882Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303-338, 2010. 8\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 8\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee312Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 3, 12\n\nUnsupervised visual representation learning by context prediction. Carl Doersch, Abhinav Gupta, Alexei A Efros, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision13Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper- vised visual representation learning by context prediction. In Proceedings of the IEEE international conference on com- puter vision, pages 1422-1430, 2015. 1, 3\n\nDecaf: A deep convolutional activation feature for generic visual recognition. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell, International conference on machine learning. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recogni- tion. In International conference on machine learning, pages 647-655, 2014. 1\n\nDiscriminative unsupervised feature learning with convolutional neural networks. Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, Thomas Brox, Advances in neural information processing systems. Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried- miller, and Thomas Brox. Discriminative unsupervised fea- ture learning with convolutional neural networks. In Ad- vances in neural information processing systems, pages 766- 774, 2014. 3\n\nDuality diagram similarity: a generic framework for initialization selection in task transfer learning. Kshitij Dwivedi, Jiahui Huang, Radoslaw Martin Cichy, Gemma Roig, arXiv:2008.02107arXiv preprintKshitij Dwivedi, Jiahui Huang, Radoslaw Martin Cichy, and Gemma Roig. Duality diagram similarity: a generic frame- work for initialization selection in task transfer learning. arXiv preprint arXiv:2008.02107, 2020.\n\nRepresentation similarity analysis for efficient task taxonomy & transfer learning. Kshitij Dwivedi, Gemma Roig, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKshitij Dwivedi and Gemma Roig. Representation similar- ity analysis for efficient task taxonomy & transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12387-12396, 2019. 8\n\nModeling transfer relationships between learning tasks for improved inductive transfer. Eric Eaton, Terran Lane, Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Eric Eaton, Terran Lane, et al. Modeling transfer relation- ships between learning tasks for improved inductive trans- fer. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 317-332.\n\n. Springer, Springer, 2008. 1\n\nLarge margin deep networks for classification. Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, Samy Bengio, Advances in neural information processing systems. Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large margin deep net- works for classification. In Advances in neural information processing systems, pages 842-852, 2018. 8\n\nOne-shot learning of object categories. Li Fei-Fei, Rob Fergus, Pietro Perona, IEEE transactions on pattern analysis and machine intelligence. 2814Li Fei-Fei, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594-611, 2006. 2, 12, 14\n\nUnsupervised representation learning by predicting image rotations. Spyros Gidaris, Praveer Singh, Nikos Komodakis, arXiv:1803.07728arXiv preprintSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Un- supervised representation learning by predicting image rota- tions. arXiv preprint arXiv:1803.07728, 2018. 3\n\nBootstrap your own latent: A new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, H Pierre, Elena Richemond, Carl Buchatskaya, Bernardo Doersch, Zhaohan Daniel Avila Pires, Mohammad Gheshlaghi Guo, Azar, arXiv:2006.07733arXiv preprintJean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Do- ersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham- mad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020. 4\n\nChuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, arXiv:1706.04599On calibration of modern neural networks. 25arXiv preprintChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. arXiv preprint arXiv:1706.04599, 2017. 2, 5\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition14Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 9729-9738, 2020. 1, 4\n\nRethinking imagenet pre-training. Kaiming He, Ross Girshick, Piotr Doll\u00e1r, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Ross Girshick, and Piotr Doll\u00e1r. Rethinking imagenet pre-training. In Proceedings of the IEEE inter- national conference on computer vision, pages 4918-4927, 2019. 1\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition812Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 1, 3, 4, 8, 12\n\nG Andrew, Menglong Howard, Bo Zhu, Dmitry Chen, Weijun Kalenichenko, Tobias Wang, Marco Weyand, Hartwig Andreetto, Adam, arXiv:1704.04861Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprintAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An- dreetto, and Hartwig Adam. Mobilenets: Efficient convolu- tional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 4\n\nPredicting the generalization gap in deep networks with margin distributions. Yiding Jiang, Dilip Krishnan, Hossein Mobahi, Samy Bengio, arXiv:1810.001131arXiv preprintYiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization gap in deep networks with margin distributions. arXiv preprint arXiv:1810.00113, 2018. 1, 8\n\nKenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio, arXiv:1710.05468Generalization in deep learning. 1arXiv preprintKenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Ben- gio. Generalization in deep learning. arXiv preprint arXiv:1710.05468, 2017. 1, 8\n\nA new measure of rank correlation. G Maurice, Kendall, Biometrika. 301/2Maurice G Kendall. A new measure of rank correlation. Biometrika, 30(1/2):81-93, 1938. 4\n\nDetecting change in data streams. Daniel Kifer, Shai Ben-David, Johannes Gehrke, VLDB. Toronto, Canada4Daniel Kifer, Shai Ben-David, and Johannes Gehrke. De- tecting change in data streams. In VLDB, volume 4, pages 180-191. Toronto, Canada, 2004. 8\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, arXiv:1312.6114arXiv preprintDiederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n\nDo better imagenet models transfer better?. Simon Kornblith, Jonathon Shlens, Quoc V Le, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSimon Kornblith, Jonathon Shlens, and Quoc V Le. Do bet- ter imagenet models transfer better? In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 2661-2671, 2019. 1, 4, 8\n\nTemporal ensembling for semisupervised learning. Samuli Laine, Timo Aila, arXiv:1610.02242arXiv preprintSamuli Laine and Timo Aila. Temporal ensembling for semi- supervised learning. arXiv preprint arXiv:1610.02242, 2016. 1\n\nSmooth neighbors on teacher graphs for semi-supervised learning. Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, Bo Zhang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In Proceedings of the IEEE conference on com- puter vision and pattern recognition, pages 8896-8905, 2018. 1\n\nYishay Mansour, Mehryar Mohri, Afshin Rostamizadeh, arXiv:0902.3430Domain adaptation: Learning bounds and algorithms. arXiv preprintYishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009. 8\n\nVirtual adversarial training: a regularization method for supervised and semi-supervised learning. Takeru Miyato, Masanori Shin-Ichi Maeda, Shin Koyama, Ishii, IEEE transactions on pattern analysis and machine intelligence. 41Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979-1993, 2018. 1\n\nExploring generalization in deep learning. Srinadh Behnam Neyshabur, David Bhojanapalli, Nati Mcallester, Srebro, Advances in neural information processing systems. 1Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. In Advances in neural information processing systems, pages 5947-5956, 2017. 1, 8\n\nTal Cuong V Nguyen, Cedric Hassner, Matthias Archambeau, Seeger, arXiv:2002.12462Leep: A new measure to evaluate transferability of learned representations. 714arXiv preprintCuong V Nguyen, Tal Hassner, Cedric Archambeau, and Matthias Seeger. Leep: A new measure to evaluate transferability of learned representations. arXiv preprint arXiv:2002.12462, 2020. 1, 2, 4, 6, 7, 14\n\nAutomated flower classification over a large number of classes. Maria-Elena Nilsback, Andrew Zisserman, Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008\n\nSixth Indian Conference on Computer Vision, Graphics & Image Processing. 1214Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages 722-729. IEEE, 2008. 2, 12, 14\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. Mehdi Noroozi, Paolo Favaro, European Conference on Computer Vision. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, pages 69-84.\n\n. Springer, 13Springer, 2016. 1, 3\n\nA survey on transfer learning. Qiang Sinno Jialin Pan, Yang, IEEE Transactions on knowledge and data engineering. 2210Sinno Jialin Pan and Qiang Yang. A survey on transfer learn- ing. IEEE Transactions on knowledge and data engineering, 22(10):1345-1359, 2009. 1\n\nnote on regression and inheritance in the case of two parents. proceedings of the royal society of London. Karl Pearson, Vii, 1895. 458Karl Pearson. Vii. note on regression and inheritance in the case of two parents. proceedings of the royal society of Lon- don, 58(347-352):240-242, 1895. 4\n\nOn variational bounds of mutual information. Ben Poole, Sherjil Ozair, Aaron Van Den, Alexander A Oord, George Alemi, Tucker, arXiv:1905.0692214arXiv preprintBen Poole, Sherjil Ozair, Aaron van den Oord, Alexander A Alemi, and George Tucker. On variational bounds of mutual information. arXiv preprint arXiv:1905.06922, 2019. 4, 6, 7, 8, 14\n\nDaniel Keysers, and Neil Houlsby. Scalable transfer learning with expert models. Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr\u00e9 Susano Pinto, Sylvain Gelly, arXiv:2009.13239arXiv preprintJoan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, Andr\u00e9 Susano Pinto, Sylvain Gelly, Daniel Key- sers, and Neil Houlsby. Scalable transfer learning with ex- pert models. arXiv preprint arXiv:2009.13239, 2020.\n\nTransfusion: Understanding transfer learning for medical imaging. Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, Samy Bengio, Advances in neural information processing systems. Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding transfer learning for medical imaging. In Advances in neural information pro- cessing systems, pages 3347-3357, 2019. 1\n\nSemi-supervised learning with ladder networks. Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, Tapani Raiko, Advances in neural information processing systems. Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning with ladder networks. In Advances in neural information process- ing systems, pages 3546-3554, 2015. 1\n\nPractical and consistent estimation of f-divergences. Paul Rubenstein, Olivier Bousquet, Josip Djolonga, Carlos Riquelme, Ilya O Tolstikhin, Advances in Neural Information Processing Systems. Paul Rubenstein, Olivier Bousquet, Josip Djolonga, Carlos Riquelme, and Ilya O Tolstikhin. Practical and consistent estimation of f-divergences. In Advances in Neural Informa- tion Processing Systems, pages 4070-4080, 2019. 3\n\nCnn features off-the-shelf: an astounding baseline for recognition. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsAli Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf: an astound- ing baseline for recognition. In Proceedings of the IEEE con- ference on computer vision and pattern recognition work- shops, pages 806-813, 2014. 1\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 1\n\nRobust large margin deep neural networks. Jure Sokoli\u0107, Raja Giryes, Guillermo Sapiro, Rodrigues, IEEE Transactions on Signal Processing. 6516Jure Sokoli\u0107, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin deep neural networks. IEEE Transactions on Signal Processing, 65(16):4265- 4280, 2017. 8\n\nDeep model transferability from attribution maps. Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, Mingli Song, Advances in Neural Information Processing Systems. Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, and Mingli Song. Deep model transferability from attribution maps. In Advances in Neural Information Processing Sys- tems, pages 6182-6192, 2019. 8\n\nDepara: Deep attribution graph for deep knowledge transferability. Jie Song, Yixin Chen, Jingwen Ye, Xinchao Wang, Chengchao Shen, Feng Mao, Mingli Song, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJie Song, Yixin Chen, Jingwen Ye, Xinchao Wang, Chengchao Shen, Feng Mao, and Mingli Song. Depara: Deep attribution graph for deep knowledge transferability. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3922-3930, 2020. 8\n\nInception-v4, inception-resnet and the impact of residual connections on learning. Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi, arXiv:1602.0726112arXiv preprintChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alex Alemi. Inception-v4, inception-resnet and the im- pact of residual connections on learning. arXiv preprint arXiv:1602.07261, 2016. 1, 4, 12\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1-9, 2015. 4\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Antti Tarvainen, Harri Valpola, Advances in neural information processing systems. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pages 1195-1204, 2017. 1\n\n. Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf, arXiv:1711.01558Wasserstein auto-encoders. arXiv preprintIlya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bern- hard Schoelkopf. Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558, 2017. 3\n\nTransferability and hardness of supervised classification tasks. Anh T Tran, V Cuong, Tal Nguyen, Hassner, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionAnh T Tran, Cuong V Nguyen, and Tal Hassner. Transfer- ability and hardness of supervised classification tasks. In Proceedings of the IEEE International Conference on Com- puter Vision, pages 1395-1405, 2019. 8\n\nThe inaturalist species classification and detection dataset. Oisin Mac Grant Van Horn, Yang Aodha, Yin Song, Chen Cui, Alex Sun, Hartwig Shepard, Pietro Adam, Serge Perona, Belongie, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition312Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and de- tection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8769-8778, 2018. 3, 12\n\nRotation equivariant cnns for digital pathology. S Bastiaan, Jasper Veeling, Jim Linmans, Taco Winkens, Max Cohen, Welling, International Conference on Medical image computing and computer-assisted intervention. Springer14Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Co- hen, and Max Welling. Rotation equivariant cnns for digital pathology. In International Conference on Medical image computing and computer-assisted intervention, pages 210- 218. Springer, 2018. 1, 2, 12, 14\n\nSun database: Large-scale scene recognition from abbey to zoo. Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, Antonio Torralba, 2010 IEEE computer society conference on computer vision and pattern recognition. 1214Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer so- ciety conference on computer vision and pattern recognition, pages 3485-3492. IEEE, 2010. 2, 12, 14\n\nTowards task and architecture-independent generalization gap predictors. Scott Yak, Javier Gonzalvo, Hanna Mazzawi, arXiv:1906.01550arXiv preprintScott Yak, Javier Gonzalvo, and Hanna Mazzawi. Towards task and architecture-independent generalization gap predic- tors. arXiv preprint arXiv:1906.01550, 2019. 8\n\nHow transferable are features in deep neural networks?. Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson, Advances in neural information processing systems. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320-3328, 2014. 1\n\nTaskonomy: Disentangling task transfer learning. Alexander Amir R Zamir, William Sax, Leonidas J Shen, Jitendra Guibas, Silvio Malik, Savarese, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionAmir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 3712-3722, 2018. 1, 3, 8\n\nSelf-supervised semi-supervised learning. Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision4Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lu- cas Beyer. S4l: Self-supervised semi-supervised learning. In Proceedings of the IEEE international conference on com- puter vision, pages 1476-1485, 2019. 3\n\nAlexey Dosovitskiy, et al. The visual task adaptation benchmark. Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, arXiv:1910.04867312arXiv preprintXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djo- longa, Andre Susano Pinto, Maxim Neumann, Alexey Doso- vitskiy, et al. The visual task adaptation benchmark. arXiv preprint arXiv:1910.04867, 2019. 1, 2, 3, 12\n\nPlaces: A 10 million image database for scene recognition. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, IEEE Transactions on Pattern Analysis and Machine Intelligence. 312Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analy- sis and Machine Intelligence, 2017. 3, 12\n\nRethinking pre-training and self-training. Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, D Ekin, Quoc V Cubuk, Le, arXiv:2006.06882arXiv preprintBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D Cubuk, and Quoc V Le. Rethinking pre-training and self-training. arXiv preprint arXiv:2006.06882, 2020. 1\n\n. Ilya Loshchilov, Frank Hutter, arXiv:1711.0510112Decoupled weight decay regularization. arXiv preprintIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 12\n\n-Ukl Wae-Gan Cond-Biggan, Wae-Mmd Vae Uncond, BigGAN Jigsaw Rel.Pat.Loc Exemplar Rotation Semi-Rotation-10% Semi-Exemplar-10%. Dataset Training Evaluation Number of Classes -Scratch WAEDataset Training Evaluation Number of Classes -Scratch WAE-UKL WAE-GAN Cond-BigGAN WAE-MMD VAE Uncond-BigGAN Jigsaw Rel.Pat.Loc Exemplar Rotation Semi-Rotation-10% Semi-Exemplar-10%\n\nSup-100%-Img Sup-Exemplar-100%. Sup-100%-Img Sup-Exemplar-100%\n\nTable 7. Absolute fine-tuning accuracy on Group I. Table 7. Absolute fine-tuning accuracy on Group I.\n\nDataset From-Scratch Img-90k Img-180k Img-270k Img-300k Inat-90k Inat-180k Inat-270k Inat-300k Pla-60k Pla-120k Pla-180k Pla-200k. Dataset From-Scratch Img-90k Img-180k Img-270k Img-300k Inat-90k Inat-180k Inat-270k Inat-300k Pla-60k Pla-120k Pla-180k Pla-200k\n\nDataset Inception-ResNet-v2. Dataset Inception-ResNet-v2\n\nInception-v1 Inception-v2 Inception-v3 Inception-v4. Inception-v1 Inception-v2 Inception-v3 Inception-v4\n\n. Mobilenet-V1, MobileNet-v1\n\n. Mobilenet, v1-025MobileNet-v1-025\n\n. Mobilenet-V2, MobileNet-v2\n\n. Mobilenet, v2-035MobileNet-v2-035\n\nv1-101MobileNet-v3-large MobileNet-v1-small ResNet. MobileNet-v3-large MobileNet-v1-small ResNet-v1-101\n\nAbsolute fine-tuning accuracy on Group III. Table 9. Table 9. Absolute fine-tuning accuracy on Group III.\n\nDataset From-Scratch Img-90k Img-180k Img-270k Img-300k Inat-90k Inat-180k Inat-270k Inat-300k Pla-60k Pla-120k Pla-180k Pla-200k. Dataset From-Scratch Img-90k Img-180k Img-270k Img-300k Inat-90k Inat-180k Inat-270k Inat-300k Pla-60k Pla-120k Pla-180k Pla-200k\n", "annotations": {"author": "[{\"end\":82,\"start\":30},{\"end\":151,\"start\":83},{\"end\":223,\"start\":152},{\"end\":275,\"start\":224},{\"end\":331,\"start\":276},{\"end\":403,\"start\":332},{\"end\":474,\"start\":404}]", "publisher": null, "author_last_name": "[{\"end\":40,\"start\":38},{\"end\":92,\"start\":89},{\"end\":163,\"start\":159},{\"end\":233,\"start\":230},{\"end\":289,\"start\":284},{\"end\":344,\"start\":340},{\"end\":415,\"start\":411}]", "author_first_name": "[{\"end\":37,\"start\":30},{\"end\":88,\"start\":83},{\"end\":158,\"start\":152},{\"end\":229,\"start\":224},{\"end\":283,\"start\":276},{\"end\":339,\"start\":332},{\"end\":410,\"start\":404}]", "author_affiliation": "[{\"end\":81,\"start\":42},{\"end\":150,\"start\":111},{\"end\":222,\"start\":183},{\"end\":274,\"start\":235},{\"end\":330,\"start\":291},{\"end\":402,\"start\":363},{\"end\":473,\"start\":434}]", "title": "[{\"end\":27,\"start\":1},{\"end\":501,\"start\":475}]", "venue": null, "abstract": "[{\"end\":1545,\"start\":503}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1832,\"start\":1828},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1834,\"start\":1832},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":1837,\"start\":1834},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1840,\"start\":1837},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":1843,\"start\":1840},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1846,\"start\":1843},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1849,\"start\":1846},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1851,\"start\":1849},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1876,\"start\":1872},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1879,\"start\":1876},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1882,\"start\":1879},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":1885,\"start\":1882},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1888,\"start\":1885},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":1984,\"start\":1980},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":1987,\"start\":1984},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2108,\"start\":2104},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2111,\"start\":2108},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2114,\"start\":2111},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2142,\"start\":2138},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3221,\"start\":3217},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3492,\"start\":3488},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3515,\"start\":3511},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3529,\"start\":3525},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3863,\"start\":3860},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3866,\"start\":3863},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":3869,\"start\":3866},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3872,\"start\":3869},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":4109,\"start\":4105},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4112,\"start\":4109},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":4170,\"start\":4166},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4298,\"start\":4294},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4599,\"start\":4595},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4602,\"start\":4599},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4604,\"start\":4602},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4623,\"start\":4619},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6945,\"start\":6941},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7398,\"start\":7394},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":9709,\"start\":9705},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9770,\"start\":9766},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9788,\"start\":9784},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":9801,\"start\":9797},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":9826,\"start\":9822},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10828,\"start\":10824},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10877,\"start\":10873},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":10887,\"start\":10883},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10962,\"start\":10958},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10992,\"start\":10988},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11007,\"start\":11003},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11026,\"start\":11022},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":11090,\"start\":11086},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11113,\"start\":11109},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11145,\"start\":11142},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11159,\"start\":11155},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":11249,\"start\":11245},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11306,\"start\":11302},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":11381,\"start\":11377},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11470,\"start\":11466},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":11504,\"start\":11500},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":12739,\"start\":12735},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":12742,\"start\":12739},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":13239,\"start\":13235},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":13500,\"start\":13496},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":13533,\"start\":13529},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13574,\"start\":13570},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13618,\"start\":13614},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14739,\"start\":14735},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14928,\"start\":14924},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16130,\"start\":16126},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16133,\"start\":16130},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":16136,\"start\":16133},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16763,\"start\":16759},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16909,\"start\":16906},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17115,\"start\":17111},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17272,\"start\":17268},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18789,\"start\":18785},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22967,\"start\":22963},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26478,\"start\":26474},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27533,\"start\":27529},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28817,\"start\":28813},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29043,\"start\":29039},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29063,\"start\":29059},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":30164,\"start\":30160},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30208,\"start\":30205},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30228,\"start\":30225},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30245,\"start\":30241},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":30276,\"start\":30272},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30304,\"start\":30300},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":30540,\"start\":30536},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30667,\"start\":30663},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31196,\"start\":31192},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":31199,\"start\":31196},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31202,\"start\":31199},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":31220,\"start\":31216},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31223,\"start\":31220},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31974,\"start\":31971},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31977,\"start\":31974},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31980,\"start\":31977},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31999,\"start\":31995},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":32019,\"start\":32015},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32209,\"start\":32205},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32212,\"start\":32209},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32214,\"start\":32212},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32216,\"start\":32214},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34033,\"start\":34029},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":34172,\"start\":34168},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":34288,\"start\":34284},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":34463,\"start\":34459},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":34817,\"start\":34813},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":35050,\"start\":35046},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":35332,\"start\":35328},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36824,\"start\":36820},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":36842,\"start\":36838},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":36863,\"start\":36859},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37396,\"start\":37392}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40052,\"start\":39809},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40243,\"start\":40053},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40533,\"start\":40244},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40856,\"start\":40534},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40888,\"start\":40857},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41264,\"start\":40889},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42249,\"start\":41265},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43002,\"start\":42250}]", "paragraph": "[{\"end\":2167,\"start\":1561},{\"end\":3090,\"start\":2169},{\"end\":3684,\"start\":3092},{\"end\":4438,\"start\":3686},{\"end\":5349,\"start\":4440},{\"end\":6129,\"start\":5351},{\"end\":6774,\"start\":6131},{\"end\":7399,\"start\":6776},{\"end\":8042,\"start\":7401},{\"end\":8214,\"start\":8096},{\"end\":8786,\"start\":8233},{\"end\":8878,\"start\":8824},{\"end\":9064,\"start\":8880},{\"end\":9272,\"start\":9105},{\"end\":9648,\"start\":9314},{\"end\":10216,\"start\":9671},{\"end\":10518,\"start\":10241},{\"end\":12146,\"start\":10520},{\"end\":13161,\"start\":12148},{\"end\":13765,\"start\":13163},{\"end\":13877,\"start\":13790},{\"end\":14244,\"start\":13879},{\"end\":14685,\"start\":14246},{\"end\":14865,\"start\":14687},{\"end\":15132,\"start\":14867},{\"end\":15358,\"start\":15163},{\"end\":15755,\"start\":15394},{\"end\":16383,\"start\":15778},{\"end\":17116,\"start\":16406},{\"end\":17686,\"start\":17171},{\"end\":17737,\"start\":17714},{\"end\":18254,\"start\":17775},{\"end\":18389,\"start\":18293},{\"end\":18866,\"start\":18391},{\"end\":19079,\"start\":18877},{\"end\":19435,\"start\":19081},{\"end\":19480,\"start\":19437},{\"end\":19703,\"start\":19514},{\"end\":20190,\"start\":19749},{\"end\":21149,\"start\":20217},{\"end\":21707,\"start\":21172},{\"end\":22271,\"start\":21709},{\"end\":22432,\"start\":22289},{\"end\":23245,\"start\":22434},{\"end\":24053,\"start\":23247},{\"end\":25194,\"start\":24055},{\"end\":26122,\"start\":25196},{\"end\":26720,\"start\":26124},{\"end\":27075,\"start\":26745},{\"end\":27306,\"start\":27077},{\"end\":27902,\"start\":27308},{\"end\":28486,\"start\":27904},{\"end\":29122,\"start\":28545},{\"end\":29684,\"start\":29124},{\"end\":31020,\"start\":29701},{\"end\":32485,\"start\":31022},{\"end\":33104,\"start\":32500},{\"end\":33701,\"start\":33106},{\"end\":33880,\"start\":33717},{\"end\":34736,\"start\":33906},{\"end\":35505,\"start\":34767},{\"end\":36646,\"start\":35573},{\"end\":37569,\"start\":36687},{\"end\":37916,\"start\":37619},{\"end\":39799,\"start\":37998}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8232,\"start\":8215},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8823,\"start\":8787},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9104,\"start\":9065},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9313,\"start\":9273},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17713,\"start\":17687},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17774,\"start\":17738},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18292,\"start\":18255},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19513,\"start\":19481},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19748,\"start\":19704}]", "table_ref": "[{\"end\":10008,\"start\":10001},{\"end\":12432,\"start\":12425},{\"end\":13684,\"start\":13677},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21468,\"start\":21461},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":22334,\"start\":22327},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33997,\"start\":33990},{\"end\":35770,\"start\":35763},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":36477,\"start\":36470},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":37084,\"start\":37076},{\"end\":37823,\"start\":37816},{\"end\":39309,\"start\":39302}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1559,\"start\":1547},{\"attributes\":{\"n\":\"2.\"},\"end\":8094,\"start\":8045},{\"attributes\":{\"n\":\"2.1.\"},\"end\":9669,\"start\":9651},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10239,\"start\":10219},{\"attributes\":{\"n\":\"2.3.\"},\"end\":13788,\"start\":13768},{\"attributes\":{\"n\":\"3.\"},\"end\":15161,\"start\":15135},{\"attributes\":{\"n\":\"3.1.\"},\"end\":15392,\"start\":15361},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15776,\"start\":15758},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16404,\"start\":16386},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17169,\"start\":17119},{\"attributes\":{\"n\":\"3.5.\"},\"end\":18875,\"start\":18869},{\"attributes\":{\"n\":\"4.\"},\"end\":20215,\"start\":20193},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21170,\"start\":21152},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22287,\"start\":22274},{\"end\":26743,\"start\":26723},{\"end\":28543,\"start\":28489},{\"attributes\":{\"n\":\"5.\"},\"end\":29699,\"start\":29687},{\"attributes\":{\"n\":\"6.\"},\"end\":32498,\"start\":32488},{\"end\":33715,\"start\":33704},{\"end\":33904,\"start\":33883},{\"end\":34765,\"start\":34739},{\"end\":35571,\"start\":35508},{\"end\":36685,\"start\":36649},{\"end\":37617,\"start\":37572},{\"end\":37996,\"start\":37919},{\"end\":39808,\"start\":39802},{\"end\":39820,\"start\":39810},{\"end\":40064,\"start\":40054},{\"end\":40545,\"start\":40535},{\"end\":40865,\"start\":40858},{\"end\":40897,\"start\":40890},{\"end\":41275,\"start\":41266},{\"end\":42261,\"start\":42251}]", "table": "[{\"end\":40888,\"start\":40867},{\"end\":42249,\"start\":41498},{\"end\":43002,\"start\":42540}]", "figure_caption": "[{\"end\":40052,\"start\":39822},{\"end\":40243,\"start\":40066},{\"end\":40533,\"start\":40246},{\"end\":40856,\"start\":40547},{\"end\":41264,\"start\":40899},{\"end\":41498,\"start\":41277},{\"end\":42540,\"start\":42264}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11673,\"start\":11665},{\"end\":13672,\"start\":13664},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20405,\"start\":20397},{\"end\":37071,\"start\":37063},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":37694,\"start\":37679},{\"end\":38946,\"start\":38938},{\"end\":39442,\"start\":39434}]", "bib_author_first_name": "[{\"end\":43058,\"start\":43048},{\"end\":43075,\"start\":43068},{\"end\":43086,\"start\":43081},{\"end\":43102,\"start\":43095},{\"end\":43126,\"start\":43117},{\"end\":43134,\"start\":43133},{\"end\":43152,\"start\":43145},{\"end\":43168,\"start\":43162},{\"end\":43595,\"start\":43590},{\"end\":43611,\"start\":43605},{\"end\":43620,\"start\":43614},{\"end\":43918,\"start\":43911},{\"end\":43930,\"start\":43926},{\"end\":43941,\"start\":43935},{\"end\":43955,\"start\":43953},{\"end\":44204,\"start\":44198},{\"end\":44219,\"start\":44214},{\"end\":44235,\"start\":44230},{\"end\":44519,\"start\":44518},{\"end\":44532,\"start\":44527},{\"end\":44534,\"start\":44533},{\"end\":44552,\"start\":44545},{\"end\":44828,\"start\":44821},{\"end\":44836,\"start\":44829},{\"end\":44855,\"start\":44847},{\"end\":44868,\"start\":44865},{\"end\":44886,\"start\":44879},{\"end\":44900,\"start\":44894},{\"end\":44914,\"start\":44909},{\"end\":44933,\"start\":44926},{\"end\":45285,\"start\":45281},{\"end\":45292,\"start\":45291},{\"end\":45304,\"start\":45300},{\"end\":45591,\"start\":45586},{\"end\":45611,\"start\":45603},{\"end\":45624,\"start\":45621},{\"end\":45644,\"start\":45637},{\"end\":45661,\"start\":45655},{\"end\":45675,\"start\":45670},{\"end\":45677,\"start\":45676},{\"end\":46056,\"start\":46050},{\"end\":46068,\"start\":46064},{\"end\":46083,\"start\":46078},{\"end\":46315,\"start\":46308},{\"end\":46334,\"start\":46326},{\"end\":46355,\"start\":46346},{\"end\":46708,\"start\":46704},{\"end\":46720,\"start\":46715},{\"end\":46740,\"start\":46732},{\"end\":46758,\"start\":46750},{\"end\":47038,\"start\":47034},{\"end\":47054,\"start\":47051},{\"end\":47066,\"start\":47065},{\"end\":47068,\"start\":47067},{\"end\":47086,\"start\":47082},{\"end\":47103,\"start\":47097},{\"end\":47440,\"start\":47434},{\"end\":47456,\"start\":47449},{\"end\":47473,\"start\":47464},{\"end\":47485,\"start\":47481},{\"end\":47501,\"start\":47495},{\"end\":47520,\"start\":47513},{\"end\":47534,\"start\":47531},{\"end\":47549,\"start\":47543},{\"end\":47561,\"start\":47556},{\"end\":48075,\"start\":48072},{\"end\":48085,\"start\":48082},{\"end\":48099,\"start\":48092},{\"end\":48114,\"start\":48108},{\"end\":48122,\"start\":48119},{\"end\":48129,\"start\":48127},{\"end\":48508,\"start\":48504},{\"end\":48525,\"start\":48518},{\"end\":48539,\"start\":48533},{\"end\":48541,\"start\":48540},{\"end\":48976,\"start\":48972},{\"end\":48994,\"start\":48986},{\"end\":49005,\"start\":49000},{\"end\":49019,\"start\":49015},{\"end\":49033,\"start\":49029},{\"end\":49045,\"start\":49041},{\"end\":49059,\"start\":49053},{\"end\":49457,\"start\":49451},{\"end\":49475,\"start\":49471},{\"end\":49482,\"start\":49476},{\"end\":49503,\"start\":49497},{\"end\":49522,\"start\":49516},{\"end\":49939,\"start\":49932},{\"end\":49955,\"start\":49949},{\"end\":49971,\"start\":49963},{\"end\":49991,\"start\":49986},{\"end\":50335,\"start\":50328},{\"end\":50350,\"start\":50345},{\"end\":50818,\"start\":50814},{\"end\":50832,\"start\":50826},{\"end\":51238,\"start\":51228},{\"end\":51253,\"start\":51248},{\"end\":51271,\"start\":51264},{\"end\":51285,\"start\":51280},{\"end\":51297,\"start\":51293},{\"end\":51608,\"start\":51606},{\"end\":51621,\"start\":51618},{\"end\":51636,\"start\":51630},{\"end\":51966,\"start\":51960},{\"end\":51983,\"start\":51976},{\"end\":51996,\"start\":51991},{\"end\":52287,\"start\":52275},{\"end\":52302,\"start\":52295},{\"end\":52317,\"start\":52310},{\"end\":52334,\"start\":52326},{\"end\":52344,\"start\":52343},{\"end\":52358,\"start\":52353},{\"end\":52374,\"start\":52370},{\"end\":52396,\"start\":52388},{\"end\":52413,\"start\":52406},{\"end\":52420,\"start\":52414},{\"end\":52442,\"start\":52434},{\"end\":52453,\"start\":52443},{\"end\":52813,\"start\":52808},{\"end\":52824,\"start\":52819},{\"end\":52835,\"start\":52833},{\"end\":52849,\"start\":52841},{\"end\":53155,\"start\":53148},{\"end\":53165,\"start\":53160},{\"end\":53176,\"start\":53171},{\"end\":53188,\"start\":53181},{\"end\":53198,\"start\":53194},{\"end\":53652,\"start\":53645},{\"end\":53661,\"start\":53657},{\"end\":53677,\"start\":53672},{\"end\":54039,\"start\":54032},{\"end\":54051,\"start\":54044},{\"end\":54067,\"start\":54059},{\"end\":54077,\"start\":54073},{\"end\":54450,\"start\":54449},{\"end\":54467,\"start\":54459},{\"end\":54478,\"start\":54476},{\"end\":54490,\"start\":54484},{\"end\":54503,\"start\":54497},{\"end\":54524,\"start\":54518},{\"end\":54536,\"start\":54531},{\"end\":54552,\"start\":54545},{\"end\":55022,\"start\":55016},{\"end\":55035,\"start\":55030},{\"end\":55053,\"start\":55046},{\"end\":55066,\"start\":55062},{\"end\":55297,\"start\":55292},{\"end\":55315,\"start\":55309},{\"end\":55320,\"start\":55316},{\"end\":55338,\"start\":55332},{\"end\":55586,\"start\":55585},{\"end\":55752,\"start\":55746},{\"end\":55764,\"start\":55760},{\"end\":55784,\"start\":55776},{\"end\":55996,\"start\":55995},{\"end\":56010,\"start\":56007},{\"end\":56217,\"start\":56212},{\"end\":56237,\"start\":56229},{\"end\":56252,\"start\":56246},{\"end\":56663,\"start\":56657},{\"end\":56675,\"start\":56671},{\"end\":56903,\"start\":56898},{\"end\":56912,\"start\":56909},{\"end\":56924,\"start\":56918},{\"end\":56933,\"start\":56929},{\"end\":56941,\"start\":56939},{\"end\":57326,\"start\":57320},{\"end\":57343,\"start\":57336},{\"end\":57357,\"start\":57351},{\"end\":57705,\"start\":57699},{\"end\":57722,\"start\":57714},{\"end\":57744,\"start\":57740},{\"end\":58130,\"start\":58123},{\"end\":58154,\"start\":58149},{\"end\":58173,\"start\":58169},{\"end\":58450,\"start\":58447},{\"end\":58473,\"start\":58467},{\"end\":58491,\"start\":58483},{\"end\":58899,\"start\":58888},{\"end\":58916,\"start\":58910},{\"end\":59312,\"start\":59307},{\"end\":59327,\"start\":59322},{\"end\":59612,\"start\":59607},{\"end\":59951,\"start\":59947},{\"end\":60181,\"start\":60178},{\"end\":60196,\"start\":60189},{\"end\":60209,\"start\":60204},{\"end\":60228,\"start\":60219},{\"end\":60230,\"start\":60229},{\"end\":60243,\"start\":60237},{\"end\":60560,\"start\":60556},{\"end\":60579,\"start\":60573},{\"end\":60595,\"start\":60590},{\"end\":60611,\"start\":60605},{\"end\":60633,\"start\":60621},{\"end\":60648,\"start\":60641},{\"end\":60984,\"start\":60977},{\"end\":60999,\"start\":60992},{\"end\":61010,\"start\":61007},{\"end\":61026,\"start\":61022},{\"end\":61348,\"start\":61343},{\"end\":61364,\"start\":61357},{\"end\":61380,\"start\":61375},{\"end\":61395,\"start\":61390},{\"end\":61411,\"start\":61405},{\"end\":61737,\"start\":61733},{\"end\":61757,\"start\":61750},{\"end\":61773,\"start\":61768},{\"end\":61790,\"start\":61784},{\"end\":62169,\"start\":62166},{\"end\":62194,\"start\":62187},{\"end\":62214,\"start\":62205},{\"end\":62231,\"start\":62225},{\"end\":62746,\"start\":62741},{\"end\":62763,\"start\":62757},{\"end\":62998,\"start\":62994},{\"end\":63012,\"start\":63008},{\"end\":63030,\"start\":63021},{\"end\":63327,\"start\":63324},{\"end\":63339,\"start\":63334},{\"end\":63353,\"start\":63346},{\"end\":63369,\"start\":63360},{\"end\":63382,\"start\":63376},{\"end\":63711,\"start\":63708},{\"end\":63723,\"start\":63718},{\"end\":63737,\"start\":63730},{\"end\":63749,\"start\":63742},{\"end\":63765,\"start\":63756},{\"end\":63776,\"start\":63772},{\"end\":63788,\"start\":63782},{\"end\":64306,\"start\":64297},{\"end\":64322,\"start\":64316},{\"end\":64337,\"start\":64330},{\"end\":64353,\"start\":64349},{\"end\":64636,\"start\":64627},{\"end\":64649,\"start\":64646},{\"end\":64663,\"start\":64655},{\"end\":64675,\"start\":64669},{\"end\":64691,\"start\":64686},{\"end\":64706,\"start\":64698},{\"end\":64724,\"start\":64717},{\"end\":64739,\"start\":64732},{\"end\":64757,\"start\":64751},{\"end\":65318,\"start\":65313},{\"end\":65335,\"start\":65330},{\"end\":65638,\"start\":65634},{\"end\":65658,\"start\":65651},{\"end\":65676,\"start\":65669},{\"end\":65692,\"start\":65684},{\"end\":65986,\"start\":65985},{\"end\":65997,\"start\":65994},{\"end\":66415,\"start\":66410},{\"end\":66419,\"start\":66416},{\"end\":66440,\"start\":66436},{\"end\":66451,\"start\":66448},{\"end\":66462,\"start\":66458},{\"end\":66472,\"start\":66468},{\"end\":66485,\"start\":66478},{\"end\":66501,\"start\":66495},{\"end\":66513,\"start\":66508},{\"end\":67028,\"start\":67027},{\"end\":67045,\"start\":67039},{\"end\":67058,\"start\":67055},{\"end\":67072,\"start\":67068},{\"end\":67085,\"start\":67082},{\"end\":67538,\"start\":67529},{\"end\":67550,\"start\":67545},{\"end\":67563,\"start\":67557},{\"end\":67565,\"start\":67564},{\"end\":67579,\"start\":67575},{\"end\":67594,\"start\":67587},{\"end\":68039,\"start\":68034},{\"end\":68051,\"start\":68045},{\"end\":68067,\"start\":68062},{\"end\":68332,\"start\":68327},{\"end\":68347,\"start\":68343},{\"end\":68361,\"start\":68355},{\"end\":68373,\"start\":68370},{\"end\":68685,\"start\":68676},{\"end\":68707,\"start\":68700},{\"end\":68721,\"start\":68713},{\"end\":68723,\"start\":68722},{\"end\":68738,\"start\":68730},{\"end\":68753,\"start\":68747},{\"end\":69225,\"start\":69218},{\"end\":69238,\"start\":69232},{\"end\":69256,\"start\":69247},{\"end\":69274,\"start\":69269},{\"end\":69693,\"start\":69686},{\"end\":69704,\"start\":69700},{\"end\":69726,\"start\":69717},{\"end\":69745,\"start\":69739},{\"end\":69761,\"start\":69755},{\"end\":69777,\"start\":69772},{\"end\":69790,\"start\":69785},{\"end\":69806,\"start\":69801},{\"end\":69813,\"start\":69807},{\"end\":69826,\"start\":69821},{\"end\":70204,\"start\":70199},{\"end\":70216,\"start\":70211},{\"end\":70234,\"start\":70228},{\"end\":70247,\"start\":70243},{\"end\":70262,\"start\":70255},{\"end\":70605,\"start\":70599},{\"end\":70618,\"start\":70612},{\"end\":70635,\"start\":70627},{\"end\":70644,\"start\":70641},{\"end\":70657,\"start\":70650},{\"end\":70664,\"start\":70663},{\"end\":70677,\"start\":70671},{\"end\":70903,\"start\":70899},{\"end\":70921,\"start\":70916},{\"end\":71129,\"start\":71117},{\"end\":71154,\"start\":71143}]", "bib_author_last_name": "[{\"end\":43066,\"start\":43059},{\"end\":43079,\"start\":43076},{\"end\":43093,\"start\":43087},{\"end\":43115,\"start\":43103},{\"end\":43131,\"start\":43127},{\"end\":43143,\"start\":43135},{\"end\":43160,\"start\":43153},{\"end\":43175,\"start\":43169},{\"end\":43183,\"start\":43177},{\"end\":43603,\"start\":43596},{\"end\":43625,\"start\":43621},{\"end\":43924,\"start\":43919},{\"end\":43933,\"start\":43931},{\"end\":43951,\"start\":43942},{\"end\":43961,\"start\":43956},{\"end\":44212,\"start\":44205},{\"end\":44228,\"start\":44220},{\"end\":44242,\"start\":44236},{\"end\":44525,\"start\":44520},{\"end\":44543,\"start\":44535},{\"end\":44559,\"start\":44553},{\"end\":44570,\"start\":44561},{\"end\":44845,\"start\":44837},{\"end\":44863,\"start\":44856},{\"end\":44877,\"start\":44869},{\"end\":44892,\"start\":44887},{\"end\":44907,\"start\":44901},{\"end\":44924,\"start\":44915},{\"end\":44939,\"start\":44934},{\"end\":45289,\"start\":45286},{\"end\":45298,\"start\":45293},{\"end\":45313,\"start\":45305},{\"end\":45601,\"start\":45592},{\"end\":45619,\"start\":45612},{\"end\":45635,\"start\":45625},{\"end\":45653,\"start\":45645},{\"end\":45668,\"start\":45662},{\"end\":45684,\"start\":45678},{\"end\":46062,\"start\":46057},{\"end\":46076,\"start\":46069},{\"end\":46092,\"start\":46084},{\"end\":46324,\"start\":46316},{\"end\":46344,\"start\":46335},{\"end\":46360,\"start\":46356},{\"end\":46713,\"start\":46709},{\"end\":46730,\"start\":46721},{\"end\":46748,\"start\":46741},{\"end\":46765,\"start\":46759},{\"end\":47049,\"start\":47039},{\"end\":47063,\"start\":47055},{\"end\":47080,\"start\":47069},{\"end\":47095,\"start\":47087},{\"end\":47108,\"start\":47104},{\"end\":47119,\"start\":47110},{\"end\":47447,\"start\":47441},{\"end\":47462,\"start\":47457},{\"end\":47479,\"start\":47474},{\"end\":47493,\"start\":47486},{\"end\":47511,\"start\":47502},{\"end\":47529,\"start\":47521},{\"end\":47541,\"start\":47535},{\"end\":47554,\"start\":47550},{\"end\":47569,\"start\":47562},{\"end\":48080,\"start\":48076},{\"end\":48090,\"start\":48086},{\"end\":48106,\"start\":48100},{\"end\":48117,\"start\":48115},{\"end\":48125,\"start\":48123},{\"end\":48137,\"start\":48130},{\"end\":48516,\"start\":48509},{\"end\":48531,\"start\":48526},{\"end\":48547,\"start\":48542},{\"end\":48984,\"start\":48977},{\"end\":48998,\"start\":48995},{\"end\":49013,\"start\":49006},{\"end\":49027,\"start\":49020},{\"end\":49039,\"start\":49034},{\"end\":49051,\"start\":49046},{\"end\":49067,\"start\":49060},{\"end\":49469,\"start\":49458},{\"end\":49495,\"start\":49483},{\"end\":49514,\"start\":49504},{\"end\":49527,\"start\":49523},{\"end\":49947,\"start\":49940},{\"end\":49961,\"start\":49956},{\"end\":49984,\"start\":49972},{\"end\":49996,\"start\":49992},{\"end\":50343,\"start\":50336},{\"end\":50355,\"start\":50351},{\"end\":50824,\"start\":50819},{\"end\":50837,\"start\":50833},{\"end\":51160,\"start\":51152},{\"end\":51246,\"start\":51239},{\"end\":51262,\"start\":51254},{\"end\":51278,\"start\":51272},{\"end\":51291,\"start\":51286},{\"end\":51304,\"start\":51298},{\"end\":51616,\"start\":51609},{\"end\":51628,\"start\":51622},{\"end\":51643,\"start\":51637},{\"end\":51974,\"start\":51967},{\"end\":51989,\"start\":51984},{\"end\":52006,\"start\":51997},{\"end\":52293,\"start\":52288},{\"end\":52308,\"start\":52303},{\"end\":52324,\"start\":52318},{\"end\":52341,\"start\":52335},{\"end\":52351,\"start\":52345},{\"end\":52368,\"start\":52359},{\"end\":52386,\"start\":52375},{\"end\":52404,\"start\":52397},{\"end\":52432,\"start\":52421},{\"end\":52457,\"start\":52454},{\"end\":52463,\"start\":52459},{\"end\":52817,\"start\":52814},{\"end\":52831,\"start\":52825},{\"end\":52839,\"start\":52836},{\"end\":52860,\"start\":52850},{\"end\":53158,\"start\":53156},{\"end\":53169,\"start\":53166},{\"end\":53179,\"start\":53177},{\"end\":53192,\"start\":53189},{\"end\":53207,\"start\":53199},{\"end\":53655,\"start\":53653},{\"end\":53670,\"start\":53662},{\"end\":53684,\"start\":53678},{\"end\":54042,\"start\":54040},{\"end\":54057,\"start\":54052},{\"end\":54071,\"start\":54068},{\"end\":54081,\"start\":54078},{\"end\":54457,\"start\":54451},{\"end\":54474,\"start\":54468},{\"end\":54482,\"start\":54479},{\"end\":54495,\"start\":54491},{\"end\":54516,\"start\":54504},{\"end\":54529,\"start\":54525},{\"end\":54543,\"start\":54537},{\"end\":54562,\"start\":54553},{\"end\":54568,\"start\":54564},{\"end\":55028,\"start\":55023},{\"end\":55044,\"start\":55036},{\"end\":55060,\"start\":55054},{\"end\":55073,\"start\":55067},{\"end\":55307,\"start\":55298},{\"end\":55330,\"start\":55321},{\"end\":55345,\"start\":55339},{\"end\":55594,\"start\":55587},{\"end\":55603,\"start\":55596},{\"end\":55758,\"start\":55753},{\"end\":55774,\"start\":55765},{\"end\":55791,\"start\":55785},{\"end\":56005,\"start\":55997},{\"end\":56017,\"start\":56011},{\"end\":56026,\"start\":56019},{\"end\":56227,\"start\":56218},{\"end\":56244,\"start\":56238},{\"end\":56255,\"start\":56253},{\"end\":56669,\"start\":56664},{\"end\":56680,\"start\":56676},{\"end\":56907,\"start\":56904},{\"end\":56916,\"start\":56913},{\"end\":56927,\"start\":56925},{\"end\":56937,\"start\":56934},{\"end\":56947,\"start\":56942},{\"end\":57334,\"start\":57327},{\"end\":57349,\"start\":57344},{\"end\":57370,\"start\":57358},{\"end\":57712,\"start\":57706},{\"end\":57738,\"start\":57723},{\"end\":57751,\"start\":57745},{\"end\":57758,\"start\":57753},{\"end\":58147,\"start\":58131},{\"end\":58167,\"start\":58155},{\"end\":58184,\"start\":58174},{\"end\":58192,\"start\":58186},{\"end\":58465,\"start\":58451},{\"end\":58481,\"start\":58474},{\"end\":58502,\"start\":58492},{\"end\":58510,\"start\":58504},{\"end\":58908,\"start\":58900},{\"end\":58926,\"start\":58917},{\"end\":59320,\"start\":59313},{\"end\":59334,\"start\":59328},{\"end\":59550,\"start\":59542},{\"end\":59629,\"start\":59613},{\"end\":59635,\"start\":59631},{\"end\":59959,\"start\":59952},{\"end\":59964,\"start\":59961},{\"end\":60187,\"start\":60182},{\"end\":60202,\"start\":60197},{\"end\":60217,\"start\":60210},{\"end\":60235,\"start\":60231},{\"end\":60249,\"start\":60244},{\"end\":60257,\"start\":60251},{\"end\":60571,\"start\":60561},{\"end\":60588,\"start\":60580},{\"end\":60603,\"start\":60596},{\"end\":60619,\"start\":60612},{\"end\":60639,\"start\":60634},{\"end\":60654,\"start\":60649},{\"end\":60990,\"start\":60985},{\"end\":61005,\"start\":61000},{\"end\":61020,\"start\":61011},{\"end\":61033,\"start\":61027},{\"end\":61355,\"start\":61349},{\"end\":61373,\"start\":61365},{\"end\":61388,\"start\":61381},{\"end\":61403,\"start\":61396},{\"end\":61417,\"start\":61412},{\"end\":61748,\"start\":61738},{\"end\":61766,\"start\":61758},{\"end\":61782,\"start\":61774},{\"end\":61799,\"start\":61791},{\"end\":61818,\"start\":61801},{\"end\":62185,\"start\":62170},{\"end\":62203,\"start\":62195},{\"end\":62223,\"start\":62215},{\"end\":62240,\"start\":62232},{\"end\":62755,\"start\":62747},{\"end\":62773,\"start\":62764},{\"end\":63006,\"start\":62999},{\"end\":63019,\"start\":63013},{\"end\":63037,\"start\":63031},{\"end\":63048,\"start\":63039},{\"end\":63332,\"start\":63328},{\"end\":63344,\"start\":63340},{\"end\":63358,\"start\":63354},{\"end\":63374,\"start\":63370},{\"end\":63387,\"start\":63383},{\"end\":63716,\"start\":63712},{\"end\":63728,\"start\":63724},{\"end\":63740,\"start\":63738},{\"end\":63754,\"start\":63750},{\"end\":63770,\"start\":63766},{\"end\":63780,\"start\":63777},{\"end\":63793,\"start\":63789},{\"end\":64314,\"start\":64307},{\"end\":64328,\"start\":64323},{\"end\":64347,\"start\":64338},{\"end\":64359,\"start\":64354},{\"end\":64644,\"start\":64637},{\"end\":64653,\"start\":64650},{\"end\":64667,\"start\":64664},{\"end\":64684,\"start\":64676},{\"end\":64696,\"start\":64692},{\"end\":64715,\"start\":64707},{\"end\":64730,\"start\":64725},{\"end\":64749,\"start\":64740},{\"end\":64768,\"start\":64758},{\"end\":65328,\"start\":65319},{\"end\":65343,\"start\":65336},{\"end\":65649,\"start\":65639},{\"end\":65667,\"start\":65659},{\"end\":65682,\"start\":65677},{\"end\":65703,\"start\":65693},{\"end\":65983,\"start\":65973},{\"end\":65992,\"start\":65987},{\"end\":66004,\"start\":65998},{\"end\":66013,\"start\":66006},{\"end\":66434,\"start\":66420},{\"end\":66446,\"start\":66441},{\"end\":66456,\"start\":66452},{\"end\":66466,\"start\":66463},{\"end\":66476,\"start\":66473},{\"end\":66493,\"start\":66486},{\"end\":66506,\"start\":66502},{\"end\":66520,\"start\":66514},{\"end\":66530,\"start\":66522},{\"end\":67037,\"start\":67029},{\"end\":67053,\"start\":67046},{\"end\":67066,\"start\":67059},{\"end\":67080,\"start\":67073},{\"end\":67091,\"start\":67086},{\"end\":67100,\"start\":67093},{\"end\":67543,\"start\":67539},{\"end\":67555,\"start\":67551},{\"end\":67573,\"start\":67566},{\"end\":67585,\"start\":67580},{\"end\":67603,\"start\":67595},{\"end\":68043,\"start\":68040},{\"end\":68060,\"start\":68052},{\"end\":68075,\"start\":68068},{\"end\":68341,\"start\":68333},{\"end\":68353,\"start\":68348},{\"end\":68368,\"start\":68362},{\"end\":68380,\"start\":68374},{\"end\":68698,\"start\":68686},{\"end\":68711,\"start\":68708},{\"end\":68728,\"start\":68724},{\"end\":68745,\"start\":68739},{\"end\":68759,\"start\":68754},{\"end\":68769,\"start\":68761},{\"end\":69230,\"start\":69226},{\"end\":69245,\"start\":69239},{\"end\":69267,\"start\":69257},{\"end\":69280,\"start\":69275},{\"end\":69698,\"start\":69694},{\"end\":69715,\"start\":69705},{\"end\":69737,\"start\":69727},{\"end\":69753,\"start\":69746},{\"end\":69770,\"start\":69762},{\"end\":69783,\"start\":69778},{\"end\":69799,\"start\":69791},{\"end\":69819,\"start\":69814},{\"end\":69834,\"start\":69827},{\"end\":70209,\"start\":70205},{\"end\":70226,\"start\":70217},{\"end\":70241,\"start\":70235},{\"end\":70253,\"start\":70248},{\"end\":70271,\"start\":70263},{\"end\":70610,\"start\":70606},{\"end\":70625,\"start\":70619},{\"end\":70639,\"start\":70636},{\"end\":70648,\"start\":70645},{\"end\":70661,\"start\":70658},{\"end\":70669,\"start\":70665},{\"end\":70683,\"start\":70678},{\"end\":70687,\"start\":70685},{\"end\":70914,\"start\":70904},{\"end\":70928,\"start\":70922},{\"end\":71141,\"start\":71130},{\"end\":71161,\"start\":71155},{\"end\":72092,\"start\":72080},{\"end\":72119,\"start\":72110},{\"end\":72159,\"start\":72147},{\"end\":72186,\"start\":72177}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":60440365},\"end\":43588,\"start\":43004},{\"attributes\":{\"doi\":\"arXiv:2002.02923\",\"id\":\"b1\"},\"end\":43836,\"start\":43590},{\"attributes\":{\"doi\":\"arXiv:1802.05296\",\"id\":\"b2\"},\"end\":44164,\"start\":43838},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8307266},\"end\":44459,\"start\":44166},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":90880},\"end\":44819,\"start\":44461},{\"attributes\":{\"doi\":\"arXiv:1801.04062\",\"id\":\"b5\"},\"end\":45223,\"start\":44821},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13967968},\"end\":45525,\"start\":45225},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":146808485},\"end\":45980,\"start\":45527},{\"attributes\":{\"doi\":\"arXiv:1809.11096\",\"id\":\"b8\"},\"end\":46280,\"start\":45982},{\"attributes\":{\"id\":\"b9\"},\"end\":46524,\"start\":46282},{\"attributes\":{\"id\":\"b10\"},\"end\":46631,\"start\":46526},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b11\"},\"end\":46982,\"start\":46633},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4246903},\"end\":47369,\"start\":46984},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":502946},\"end\":48017,\"start\":47371},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":57246310},\"end\":48435,\"start\":48019},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9062671},\"end\":48891,\"start\":48437},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6161478},\"end\":49368,\"start\":48893},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3244218},\"end\":49826,\"start\":49370},{\"attributes\":{\"doi\":\"arXiv:2008.02107\",\"id\":\"b18\"},\"end\":50242,\"start\":49828},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":135464383},\"end\":50724,\"start\":50244},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14414833},\"end\":51148,\"start\":50726},{\"attributes\":{\"id\":\"b21\"},\"end\":51179,\"start\":51150},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3927382},\"end\":51564,\"start\":51181},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6953475},\"end\":51890,\"start\":51566},{\"attributes\":{\"doi\":\"arXiv:1803.07728\",\"id\":\"b24\"},\"end\":52202,\"start\":51892},{\"attributes\":{\"doi\":\"arXiv:2006.07733\",\"id\":\"b25\"},\"end\":52806,\"start\":52204},{\"attributes\":{\"doi\":\"arXiv:1706.04599\",\"id\":\"b26\"},\"end\":53079,\"start\":52808},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":207930212},\"end\":53609,\"start\":53081},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":53739271},\"end\":53984,\"start\":53611},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":206594692},\"end\":54447,\"start\":53986},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b30\"},\"end\":54936,\"start\":54449},{\"attributes\":{\"doi\":\"arXiv:1810.00113\",\"id\":\"b31\"},\"end\":55290,\"start\":54938},{\"attributes\":{\"doi\":\"arXiv:1710.05468\",\"id\":\"b32\"},\"end\":55548,\"start\":55292},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":120478295},\"end\":55710,\"start\":55550},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":52800392},\"end\":55960,\"start\":55712},{\"attributes\":{\"doi\":\"arXiv:1312.6114\",\"id\":\"b35\"},\"end\":56166,\"start\":55962},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":43928547},\"end\":56606,\"start\":56168},{\"attributes\":{\"doi\":\"arXiv:1610.02242\",\"id\":\"b37\"},\"end\":56831,\"start\":56608},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4381935},\"end\":57318,\"start\":56833},{\"attributes\":{\"doi\":\"arXiv:0902.3430\",\"id\":\"b39\"},\"end\":57598,\"start\":57320},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":17504174},\"end\":58078,\"start\":57600},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":9597660},\"end\":58445,\"start\":58080},{\"attributes\":{\"doi\":\"arXiv:2002.12462\",\"id\":\"b42\"},\"end\":58822,\"start\":58447},{\"attributes\":{\"id\":\"b43\"},\"end\":59042,\"start\":58824},{\"attributes\":{\"id\":\"b44\"},\"end\":59230,\"start\":59044},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":187547},\"end\":59538,\"start\":59232},{\"attributes\":{\"id\":\"b46\"},\"end\":59574,\"start\":59540},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":740063},\"end\":59838,\"start\":59576},{\"attributes\":{\"doi\":\"1895. 4\",\"id\":\"b48\"},\"end\":60131,\"start\":59840},{\"attributes\":{\"doi\":\"arXiv:1905.06922\",\"id\":\"b49\"},\"end\":60473,\"start\":60133},{\"attributes\":{\"doi\":\"arXiv:2009.13239\",\"id\":\"b50\"},\"end\":60909,\"start\":60475},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":170078822},\"end\":61294,\"start\":60911},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":5855183},\"end\":61677,\"start\":61296},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":166228687},\"end\":62096,\"start\":61679},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":6383532},\"end\":62671,\"start\":62098},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b55\"},\"end\":62950,\"start\":62673},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":10504236},\"end\":63272,\"start\":62952},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":202788309},\"end\":63639,\"start\":63274},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":212737003},\"end\":64212,\"start\":63641},{\"attributes\":{\"doi\":\"arXiv:1602.07261\",\"id\":\"b59\"},\"end\":64593,\"start\":64214},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":206592484},\"end\":65190,\"start\":64595},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":2759724},\"end\":65630,\"start\":65192},{\"attributes\":{\"doi\":\"arXiv:1711.01558\",\"id\":\"b62\"},\"end\":65906,\"start\":65632},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":201303557},\"end\":66346,\"start\":65908},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":29156801},\"end\":66976,\"start\":66348},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":47021742},\"end\":67464,\"start\":66978},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":1309931},\"end\":67959,\"start\":67466},{\"attributes\":{\"doi\":\"arXiv:1906.01550\",\"id\":\"b67\"},\"end\":68269,\"start\":67961},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":362467},\"end\":68625,\"start\":68271},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":5046249},\"end\":69174,\"start\":68627},{\"attributes\":{\"id\":\"b70\"},\"end\":69619,\"start\":69176},{\"attributes\":{\"doi\":\"arXiv:1910.04867\",\"id\":\"b71\"},\"end\":70138,\"start\":69621},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":2608922},\"end\":70554,\"start\":70140},{\"attributes\":{\"doi\":\"arXiv:2006.06882\",\"id\":\"b73\"},\"end\":70895,\"start\":70556},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b74\"},\"end\":71115,\"start\":70897},{\"attributes\":{\"id\":\"b75\"},\"end\":71483,\"start\":71117},{\"attributes\":{\"id\":\"b76\"},\"end\":71547,\"start\":71485},{\"attributes\":{\"id\":\"b77\"},\"end\":71650,\"start\":71549},{\"attributes\":{\"id\":\"b78\"},\"end\":71912,\"start\":71652},{\"attributes\":{\"id\":\"b79\"},\"end\":71970,\"start\":71914},{\"attributes\":{\"id\":\"b80\"},\"end\":72076,\"start\":71972},{\"attributes\":{\"id\":\"b81\"},\"end\":72106,\"start\":72078},{\"attributes\":{\"doi\":\"v1-025\",\"id\":\"b82\"},\"end\":72143,\"start\":72108},{\"attributes\":{\"id\":\"b83\"},\"end\":72173,\"start\":72145},{\"attributes\":{\"doi\":\"v2-035\",\"id\":\"b84\"},\"end\":72210,\"start\":72175},{\"attributes\":{\"doi\":\"v1-101\",\"id\":\"b85\"},\"end\":72315,\"start\":72212},{\"attributes\":{\"id\":\"b86\"},\"end\":72422,\"start\":72317},{\"attributes\":{\"id\":\"b87\"},\"end\":72684,\"start\":72424}]", "bib_title": "[{\"end\":43046,\"start\":43004},{\"end\":44196,\"start\":44166},{\"end\":44516,\"start\":44461},{\"end\":45279,\"start\":45225},{\"end\":45584,\"start\":45527},{\"end\":47032,\"start\":46984},{\"end\":47432,\"start\":47371},{\"end\":48070,\"start\":48019},{\"end\":48502,\"start\":48437},{\"end\":48970,\"start\":48893},{\"end\":49449,\"start\":49370},{\"end\":50326,\"start\":50244},{\"end\":50812,\"start\":50726},{\"end\":51226,\"start\":51181},{\"end\":51604,\"start\":51566},{\"end\":53146,\"start\":53081},{\"end\":53643,\"start\":53611},{\"end\":54030,\"start\":53986},{\"end\":55583,\"start\":55550},{\"end\":55744,\"start\":55712},{\"end\":56210,\"start\":56168},{\"end\":56896,\"start\":56833},{\"end\":57697,\"start\":57600},{\"end\":58121,\"start\":58080},{\"end\":59305,\"start\":59232},{\"end\":59605,\"start\":59576},{\"end\":60975,\"start\":60911},{\"end\":61341,\"start\":61296},{\"end\":61731,\"start\":61679},{\"end\":62164,\"start\":62098},{\"end\":62992,\"start\":62952},{\"end\":63322,\"start\":63274},{\"end\":63706,\"start\":63641},{\"end\":64625,\"start\":64595},{\"end\":65311,\"start\":65192},{\"end\":65971,\"start\":65908},{\"end\":66408,\"start\":66348},{\"end\":67025,\"start\":66978},{\"end\":67527,\"start\":67466},{\"end\":68325,\"start\":68271},{\"end\":68674,\"start\":68627},{\"end\":69216,\"start\":69176},{\"end\":70197,\"start\":70140},{\"end\":72359,\"start\":72317}]", "bib_author": "[{\"end\":43068,\"start\":43048},{\"end\":43081,\"start\":43068},{\"end\":43095,\"start\":43081},{\"end\":43117,\"start\":43095},{\"end\":43133,\"start\":43117},{\"end\":43145,\"start\":43133},{\"end\":43162,\"start\":43145},{\"end\":43177,\"start\":43162},{\"end\":43185,\"start\":43177},{\"end\":43605,\"start\":43590},{\"end\":43614,\"start\":43605},{\"end\":43627,\"start\":43614},{\"end\":43926,\"start\":43911},{\"end\":43935,\"start\":43926},{\"end\":43953,\"start\":43935},{\"end\":43963,\"start\":43953},{\"end\":44214,\"start\":44198},{\"end\":44230,\"start\":44214},{\"end\":44244,\"start\":44230},{\"end\":44527,\"start\":44518},{\"end\":44545,\"start\":44527},{\"end\":44561,\"start\":44545},{\"end\":44572,\"start\":44561},{\"end\":44847,\"start\":44821},{\"end\":44865,\"start\":44847},{\"end\":44879,\"start\":44865},{\"end\":44894,\"start\":44879},{\"end\":44909,\"start\":44894},{\"end\":44926,\"start\":44909},{\"end\":44941,\"start\":44926},{\"end\":45291,\"start\":45281},{\"end\":45300,\"start\":45291},{\"end\":45315,\"start\":45300},{\"end\":45603,\"start\":45586},{\"end\":45621,\"start\":45603},{\"end\":45637,\"start\":45621},{\"end\":45655,\"start\":45637},{\"end\":45670,\"start\":45655},{\"end\":45686,\"start\":45670},{\"end\":46064,\"start\":46050},{\"end\":46078,\"start\":46064},{\"end\":46094,\"start\":46078},{\"end\":46326,\"start\":46308},{\"end\":46346,\"start\":46326},{\"end\":46362,\"start\":46346},{\"end\":46715,\"start\":46704},{\"end\":46732,\"start\":46715},{\"end\":46750,\"start\":46732},{\"end\":46767,\"start\":46750},{\"end\":47051,\"start\":47034},{\"end\":47065,\"start\":47051},{\"end\":47082,\"start\":47065},{\"end\":47097,\"start\":47082},{\"end\":47110,\"start\":47097},{\"end\":47121,\"start\":47110},{\"end\":47449,\"start\":47434},{\"end\":47464,\"start\":47449},{\"end\":47481,\"start\":47464},{\"end\":47495,\"start\":47481},{\"end\":47513,\"start\":47495},{\"end\":47531,\"start\":47513},{\"end\":47543,\"start\":47531},{\"end\":47556,\"start\":47543},{\"end\":47571,\"start\":47556},{\"end\":48082,\"start\":48072},{\"end\":48092,\"start\":48082},{\"end\":48108,\"start\":48092},{\"end\":48119,\"start\":48108},{\"end\":48127,\"start\":48119},{\"end\":48139,\"start\":48127},{\"end\":48518,\"start\":48504},{\"end\":48533,\"start\":48518},{\"end\":48549,\"start\":48533},{\"end\":48986,\"start\":48972},{\"end\":49000,\"start\":48986},{\"end\":49015,\"start\":49000},{\"end\":49029,\"start\":49015},{\"end\":49041,\"start\":49029},{\"end\":49053,\"start\":49041},{\"end\":49069,\"start\":49053},{\"end\":49471,\"start\":49451},{\"end\":49497,\"start\":49471},{\"end\":49516,\"start\":49497},{\"end\":49529,\"start\":49516},{\"end\":49949,\"start\":49932},{\"end\":49963,\"start\":49949},{\"end\":49986,\"start\":49963},{\"end\":49998,\"start\":49986},{\"end\":50345,\"start\":50328},{\"end\":50357,\"start\":50345},{\"end\":50826,\"start\":50814},{\"end\":50839,\"start\":50826},{\"end\":51162,\"start\":51152},{\"end\":51248,\"start\":51228},{\"end\":51264,\"start\":51248},{\"end\":51280,\"start\":51264},{\"end\":51293,\"start\":51280},{\"end\":51306,\"start\":51293},{\"end\":51618,\"start\":51606},{\"end\":51630,\"start\":51618},{\"end\":51645,\"start\":51630},{\"end\":51976,\"start\":51960},{\"end\":51991,\"start\":51976},{\"end\":52008,\"start\":51991},{\"end\":52295,\"start\":52275},{\"end\":52310,\"start\":52295},{\"end\":52326,\"start\":52310},{\"end\":52343,\"start\":52326},{\"end\":52353,\"start\":52343},{\"end\":52370,\"start\":52353},{\"end\":52388,\"start\":52370},{\"end\":52406,\"start\":52388},{\"end\":52434,\"start\":52406},{\"end\":52459,\"start\":52434},{\"end\":52465,\"start\":52459},{\"end\":52819,\"start\":52808},{\"end\":52833,\"start\":52819},{\"end\":52841,\"start\":52833},{\"end\":52862,\"start\":52841},{\"end\":53160,\"start\":53148},{\"end\":53171,\"start\":53160},{\"end\":53181,\"start\":53171},{\"end\":53194,\"start\":53181},{\"end\":53209,\"start\":53194},{\"end\":53657,\"start\":53645},{\"end\":53672,\"start\":53657},{\"end\":53686,\"start\":53672},{\"end\":54044,\"start\":54032},{\"end\":54059,\"start\":54044},{\"end\":54073,\"start\":54059},{\"end\":54083,\"start\":54073},{\"end\":54459,\"start\":54449},{\"end\":54476,\"start\":54459},{\"end\":54484,\"start\":54476},{\"end\":54497,\"start\":54484},{\"end\":54518,\"start\":54497},{\"end\":54531,\"start\":54518},{\"end\":54545,\"start\":54531},{\"end\":54564,\"start\":54545},{\"end\":54570,\"start\":54564},{\"end\":55030,\"start\":55016},{\"end\":55046,\"start\":55030},{\"end\":55062,\"start\":55046},{\"end\":55075,\"start\":55062},{\"end\":55309,\"start\":55292},{\"end\":55332,\"start\":55309},{\"end\":55347,\"start\":55332},{\"end\":55596,\"start\":55585},{\"end\":55605,\"start\":55596},{\"end\":55760,\"start\":55746},{\"end\":55776,\"start\":55760},{\"end\":55793,\"start\":55776},{\"end\":56007,\"start\":55995},{\"end\":56019,\"start\":56007},{\"end\":56028,\"start\":56019},{\"end\":56229,\"start\":56212},{\"end\":56246,\"start\":56229},{\"end\":56257,\"start\":56246},{\"end\":56671,\"start\":56657},{\"end\":56682,\"start\":56671},{\"end\":56909,\"start\":56898},{\"end\":56918,\"start\":56909},{\"end\":56929,\"start\":56918},{\"end\":56939,\"start\":56929},{\"end\":56949,\"start\":56939},{\"end\":57336,\"start\":57320},{\"end\":57351,\"start\":57336},{\"end\":57372,\"start\":57351},{\"end\":57714,\"start\":57699},{\"end\":57740,\"start\":57714},{\"end\":57753,\"start\":57740},{\"end\":57760,\"start\":57753},{\"end\":58149,\"start\":58123},{\"end\":58169,\"start\":58149},{\"end\":58186,\"start\":58169},{\"end\":58194,\"start\":58186},{\"end\":58467,\"start\":58447},{\"end\":58483,\"start\":58467},{\"end\":58504,\"start\":58483},{\"end\":58512,\"start\":58504},{\"end\":58910,\"start\":58888},{\"end\":58928,\"start\":58910},{\"end\":59322,\"start\":59307},{\"end\":59336,\"start\":59322},{\"end\":59552,\"start\":59542},{\"end\":59631,\"start\":59607},{\"end\":59637,\"start\":59631},{\"end\":59961,\"start\":59947},{\"end\":59966,\"start\":59961},{\"end\":60189,\"start\":60178},{\"end\":60204,\"start\":60189},{\"end\":60219,\"start\":60204},{\"end\":60237,\"start\":60219},{\"end\":60251,\"start\":60237},{\"end\":60259,\"start\":60251},{\"end\":60573,\"start\":60556},{\"end\":60590,\"start\":60573},{\"end\":60605,\"start\":60590},{\"end\":60621,\"start\":60605},{\"end\":60641,\"start\":60621},{\"end\":60656,\"start\":60641},{\"end\":60992,\"start\":60977},{\"end\":61007,\"start\":60992},{\"end\":61022,\"start\":61007},{\"end\":61035,\"start\":61022},{\"end\":61357,\"start\":61343},{\"end\":61375,\"start\":61357},{\"end\":61390,\"start\":61375},{\"end\":61405,\"start\":61390},{\"end\":61419,\"start\":61405},{\"end\":61750,\"start\":61733},{\"end\":61768,\"start\":61750},{\"end\":61784,\"start\":61768},{\"end\":61801,\"start\":61784},{\"end\":61820,\"start\":61801},{\"end\":62187,\"start\":62166},{\"end\":62205,\"start\":62187},{\"end\":62225,\"start\":62205},{\"end\":62242,\"start\":62225},{\"end\":62757,\"start\":62741},{\"end\":62775,\"start\":62757},{\"end\":63008,\"start\":62994},{\"end\":63021,\"start\":63008},{\"end\":63039,\"start\":63021},{\"end\":63050,\"start\":63039},{\"end\":63334,\"start\":63324},{\"end\":63346,\"start\":63334},{\"end\":63360,\"start\":63346},{\"end\":63376,\"start\":63360},{\"end\":63389,\"start\":63376},{\"end\":63718,\"start\":63708},{\"end\":63730,\"start\":63718},{\"end\":63742,\"start\":63730},{\"end\":63756,\"start\":63742},{\"end\":63772,\"start\":63756},{\"end\":63782,\"start\":63772},{\"end\":63795,\"start\":63782},{\"end\":64316,\"start\":64297},{\"end\":64330,\"start\":64316},{\"end\":64349,\"start\":64330},{\"end\":64361,\"start\":64349},{\"end\":64646,\"start\":64627},{\"end\":64655,\"start\":64646},{\"end\":64669,\"start\":64655},{\"end\":64686,\"start\":64669},{\"end\":64698,\"start\":64686},{\"end\":64717,\"start\":64698},{\"end\":64732,\"start\":64717},{\"end\":64751,\"start\":64732},{\"end\":64770,\"start\":64751},{\"end\":65330,\"start\":65313},{\"end\":65345,\"start\":65330},{\"end\":65651,\"start\":65634},{\"end\":65669,\"start\":65651},{\"end\":65684,\"start\":65669},{\"end\":65705,\"start\":65684},{\"end\":65985,\"start\":65973},{\"end\":65994,\"start\":65985},{\"end\":66006,\"start\":65994},{\"end\":66015,\"start\":66006},{\"end\":66436,\"start\":66410},{\"end\":66448,\"start\":66436},{\"end\":66458,\"start\":66448},{\"end\":66468,\"start\":66458},{\"end\":66478,\"start\":66468},{\"end\":66495,\"start\":66478},{\"end\":66508,\"start\":66495},{\"end\":66522,\"start\":66508},{\"end\":66532,\"start\":66522},{\"end\":67039,\"start\":67027},{\"end\":67055,\"start\":67039},{\"end\":67068,\"start\":67055},{\"end\":67082,\"start\":67068},{\"end\":67093,\"start\":67082},{\"end\":67102,\"start\":67093},{\"end\":67545,\"start\":67529},{\"end\":67557,\"start\":67545},{\"end\":67575,\"start\":67557},{\"end\":67587,\"start\":67575},{\"end\":67605,\"start\":67587},{\"end\":68045,\"start\":68034},{\"end\":68062,\"start\":68045},{\"end\":68077,\"start\":68062},{\"end\":68343,\"start\":68327},{\"end\":68355,\"start\":68343},{\"end\":68370,\"start\":68355},{\"end\":68382,\"start\":68370},{\"end\":68700,\"start\":68676},{\"end\":68713,\"start\":68700},{\"end\":68730,\"start\":68713},{\"end\":68747,\"start\":68730},{\"end\":68761,\"start\":68747},{\"end\":68771,\"start\":68761},{\"end\":69232,\"start\":69218},{\"end\":69247,\"start\":69232},{\"end\":69269,\"start\":69247},{\"end\":69282,\"start\":69269},{\"end\":69700,\"start\":69686},{\"end\":69717,\"start\":69700},{\"end\":69739,\"start\":69717},{\"end\":69755,\"start\":69739},{\"end\":69772,\"start\":69755},{\"end\":69785,\"start\":69772},{\"end\":69801,\"start\":69785},{\"end\":69821,\"start\":69801},{\"end\":69836,\"start\":69821},{\"end\":70211,\"start\":70199},{\"end\":70228,\"start\":70211},{\"end\":70243,\"start\":70228},{\"end\":70255,\"start\":70243},{\"end\":70273,\"start\":70255},{\"end\":70612,\"start\":70599},{\"end\":70627,\"start\":70612},{\"end\":70641,\"start\":70627},{\"end\":70650,\"start\":70641},{\"end\":70663,\"start\":70650},{\"end\":70671,\"start\":70663},{\"end\":70685,\"start\":70671},{\"end\":70689,\"start\":70685},{\"end\":70916,\"start\":70899},{\"end\":70930,\"start\":70916},{\"end\":71143,\"start\":71117},{\"end\":71163,\"start\":71143},{\"end\":72094,\"start\":72080},{\"end\":72121,\"start\":72110},{\"end\":72161,\"start\":72147},{\"end\":72188,\"start\":72177}]", "bib_venue": "[{\"end\":43252,\"start\":43185},{\"end\":43692,\"start\":43643},{\"end\":43909,\"start\":43838},{\"end\":44293,\"start\":44244},{\"end\":44621,\"start\":44572},{\"end\":44999,\"start\":44957},{\"end\":45350,\"start\":45315},{\"end\":45735,\"start\":45686},{\"end\":46048,\"start\":45982},{\"end\":46306,\"start\":46282},{\"end\":46564,\"start\":46528},{\"end\":46702,\"start\":46633},{\"end\":47161,\"start\":47121},{\"end\":47649,\"start\":47571},{\"end\":48202,\"start\":48139},{\"end\":48616,\"start\":48549},{\"end\":49113,\"start\":49069},{\"end\":49578,\"start\":49529},{\"end\":49930,\"start\":49828},{\"end\":50434,\"start\":50357},{\"end\":50921,\"start\":50839},{\"end\":51355,\"start\":51306},{\"end\":51707,\"start\":51645},{\"end\":51958,\"start\":51892},{\"end\":52273,\"start\":52204},{\"end\":52918,\"start\":52878},{\"end\":53290,\"start\":53209},{\"end\":53753,\"start\":53686},{\"end\":54160,\"start\":54083},{\"end\":54668,\"start\":54586},{\"end\":55014,\"start\":54938},{\"end\":55394,\"start\":55363},{\"end\":55615,\"start\":55605},{\"end\":55797,\"start\":55793},{\"end\":55993,\"start\":55962},{\"end\":56334,\"start\":56257},{\"end\":56655,\"start\":56608},{\"end\":57026,\"start\":56949},{\"end\":57436,\"start\":57387},{\"end\":57822,\"start\":57760},{\"end\":58243,\"start\":58194},{\"end\":58602,\"start\":58528},{\"end\":58886,\"start\":58824},{\"end\":59115,\"start\":59044},{\"end\":59374,\"start\":59336},{\"end\":59688,\"start\":59637},{\"end\":59945,\"start\":59840},{\"end\":60176,\"start\":60133},{\"end\":60554,\"start\":60475},{\"end\":61084,\"start\":61035},{\"end\":61468,\"start\":61419},{\"end\":61869,\"start\":61820},{\"end\":62329,\"start\":62242},{\"end\":62739,\"start\":62673},{\"end\":63088,\"start\":63050},{\"end\":63438,\"start\":63389},{\"end\":63876,\"start\":63795},{\"end\":64295,\"start\":64214},{\"end\":64847,\"start\":64770},{\"end\":65394,\"start\":65345},{\"end\":66082,\"start\":66015},{\"end\":66609,\"start\":66532},{\"end\":67188,\"start\":67102},{\"end\":67685,\"start\":67605},{\"end\":68032,\"start\":67961},{\"end\":68431,\"start\":68382},{\"end\":68848,\"start\":68771},{\"end\":69349,\"start\":69282},{\"end\":69684,\"start\":69621},{\"end\":70335,\"start\":70273},{\"end\":70597,\"start\":70556},{\"end\":71242,\"start\":71163},{\"end\":71515,\"start\":71485},{\"end\":71598,\"start\":71549},{\"end\":71781,\"start\":71652},{\"end\":71941,\"start\":71914},{\"end\":72023,\"start\":71972},{\"end\":72262,\"start\":72218},{\"end\":72368,\"start\":72361},{\"end\":72553,\"start\":72424},{\"end\":43306,\"start\":43254},{\"end\":47723,\"start\":47651},{\"end\":48670,\"start\":48618},{\"end\":50498,\"start\":50436},{\"end\":53358,\"start\":53292},{\"end\":53807,\"start\":53755},{\"end\":54224,\"start\":54162},{\"end\":55814,\"start\":55799},{\"end\":56398,\"start\":56336},{\"end\":57090,\"start\":57028},{\"end\":62403,\"start\":62331},{\"end\":63944,\"start\":63878},{\"end\":64911,\"start\":64849},{\"end\":66136,\"start\":66084},{\"end\":66673,\"start\":66611},{\"end\":68912,\"start\":68850},{\"end\":69403,\"start\":69351}]"}}}, "year": 2023, "month": 12, "day": 17}