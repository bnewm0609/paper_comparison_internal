{"id": 1262057, "updated": "2023-11-02 15:14:33.249", "metadata": {"title": "Action Recognition: From Static Datasets to Moving Robots", "authors": "[{\"first\":\"Fahimeh\",\"last\":\"Rezazadegan\",\"middle\":[]},{\"first\":\"Sareh\",\"last\":\"Shirazi\",\"middle\":[]},{\"first\":\"Ben\",\"last\":\"Upcroft\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Milford\",\"middle\":[]}]", "venue": "Robotics and Automation (ICRA), 2017 IEEE International Conference on", "journal": null, "publication_date": {"year": 2017, "month": null, "day": null}, "abstract": "Deep learning models have achieved state-of-the- art performance in recognizing human activities, but often rely on utilizing background cues present in typical computer vision datasets that predominantly have a stationary camera. If these models are to be employed by autonomous robots in real world environments, they must be adapted to perform independently of background cues and camera motion effects. To address these challenges, we propose a new method that firstly generates generic action region proposals with good potential to locate one human action in unconstrained videos regardless of camera motion and then uses action proposals to extract and classify effective shape and motion features by a ConvNet framework. In a range of experiments, we demonstrate that by actively proposing action regions during both training and testing, state-of-the-art or better performance is achieved on benchmarks. We show the outperformance of our approach compared to the state-of-the-art in two new datasets; one emphasizes on irrelevant background, the other highlights the camera motion. We also validate our action recognition method in an abnormal behavior detection scenario to improve workplace safety. The results verify a higher success rate for our method due to the ability of our system to recognize human actions regardless of environment and camera motion.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1701.04925", "mag": "3098057674", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/RezazadeganSUM17", "doi": "10.1109/icra.2017.7989361"}}, "content": {"source": {"pdf_hash": "48cad609886bec3fdcc4417343e3e1dfb117bdd2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1701.04925v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1701.04925", "status": "GREEN"}}, "grobid": {"id": "3e21e80ea2de1cebf7e641a681688d7b5129737a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/48cad609886bec3fdcc4417343e3e1dfb117bdd2.txt", "contents": "\nAction Recognition: From Static Datasets to Moving Robots\n\n\nFahimeh Rezazadegan \nACRV\nSchool of Electrical Engineering and Computer science\nQueensland University of Technology\nBrisbaneAustralia. email\n\nSareh Shirazi \nACRV\nSchool of Electrical Engineering and Computer science\nQueensland University of Technology\nBrisbaneAustralia. email\n\nBen Upcroft \nACRV\nSchool of Electrical Engineering and Computer science\nQueensland University of Technology\nBrisbaneAustralia. email\n\nMichael Milford \nACRV\nSchool of Electrical Engineering and Computer science\nQueensland University of Technology\nBrisbaneAustralia. email\n\nAction Recognition: From Static Datasets to Moving Robots\n\uf020 * The authors are with the Australian Centre for Robotic Vision\nDeep learning models have achieved state-of-theart performance in recognizing human activities, but often rely on utilizing background cues present in typical computer vision datasets that predominantly have a stationary camera. If these models are to be employed by autonomous robots in real world environments, they must be adapted to perform independently of background cues and camera motion effects. To address these challenges, we propose a new method that firstly generates generic action region proposals with good potential to locate one human action in unconstrained videos regardless of camera motion and then uses action proposals to extract and classify effective shape and motion features by a ConvNet framework. In a range of experiments, we demonstrate that by actively proposing action regions during both training and testing, state-of-the-art or better performance is achieved on benchmarks. We show the outperformance of our approach compared to the state-of-the-art in two new datasets; one emphasizes on irrelevant background, the other highlights the camera motion. We also validate our action recognition method in an abnormal behavior detection scenario to improve workplace safety. The results verify a higher success rate for our method due to the ability of our system to recognize human actions regardless of environment and camera motion.\n\nI. INTRODUCTION\n\nRecognizing and understanding human activity is essential for a wide variety of applications from surveillance purposes [1] and anomaly detection [2] to having safe and collaborative interaction between humans and robots in shared workspaces. More explicitly, for robots and humans to be cooperative partners that can assist human intuitively, it is crucial that robot recognizes the actions of human. With such abilities, a robot can identify the next required task to assist a human at the appropriate time as well as reducing the likelihood of interfering with the human activity [3].\n\nOver the last decade, significant progress has been made in the action recognition field using conventional RGB images, optical flow information and the fusion of both [4]. Transitioning these computer vision techniques from benchmark dataset to real world robots is challenging. Real world imagery is far more diverse, unbiased and challenging than computer vision datasets, meaning these techniques tend to perform far worse when applied blindly to a robot vision system [5].\n\nTransitioning from computer vision approaches to robotics applications involves two main challenges. Firstly, the computer vision approaches rely on background cues due to the fact that traditional datasets tend to have contextuallyinformative backgrounds. Secondly, having datasets that mainly use stationary cameras would make the methods vulnerable to disturbing effects of camera motion. This would negatively impact the performance in robotics applications where it is critical to have mobile platforms.\n\nMotivated by the benefits of using object proposals in object recognition, it is demonstrated that generation of action region proposals is of great importance, because we can focus on the motion salient regions rather than the full video frames [7]. This leads to a big reduction in computational cost and an improvement in performance due to elimination of the background cues [6], [7]. However, to the best of our knowledge, no work has addressed two aforementioned challenges simultaneously.\n\nIn this paper, we develop an action recognition system, that recognizes human actions regardless of the platform, background context and camera motion by jointly detecting and recognizing actions based on a new action region proposal method. To this end, we firstly correct the temporal cues by removing the effect of camera motion and then exploit the human motion boundaries to select a reliable action region proposal that are fed to the Convolutional Neural Networks (ConvNet). Through a wide range of experiments, we test our algorithm on 1) benchmark dataset [8], 2) a new datasets containing non-informative background, 3) a new dataset recorded by a mobile robot. We also validate our system in an abnormal human behaviour detection scenario to improve the workplace safety, which is applicable to other fields such as improving elderly care and reducing driving risk [9]. The approach in this experiment detects the abnormal actions in the work environment by jointly categorizing the scene and recognizing actions (Figure 1). Our paper provides the following contributions:\n\n\uf0b7 We develop a new framework for jointly detecting and recognizing human activities using novel action region proposals. This enables categorization which is robust against both camera motion and irrelevant background contexts, and is therefore suitable for robots operating in the real world.\n\n\uf0b7 We introduce two new unbiased datasets (without background bias); one achieved through careful composition of camera footage, the other through acquisition by a mobile robot. \uf0b7 We conduct a comprehensive suite of experiments evaluating the performance of our proposed technique on two benchmark datasets and the new unbiased background datasets. \uf0b7 We evaluate the performance of the proposed approach against existing state-of-the-art methods on our dataset recorded by a mobile robot to recognize human actions in work environment on our university's campus. \uf0b7 Based on our action recognition system, we introduce an abnormal behavior detection scenario, in which the robot is able to detect abnormal behaviors.\n\nThe rest of paper is organized as follows. In Section II, we review related work on action recognition in robotics and computer vision fields. We then present an overview of the approach and describe our network architectures in Section III. Section IV details experiment setup and experimental results followed by conclusion in Section V.\n\n\nII. RELATED WORK\n\nIn robotics, action recognition plays a critical role for fluent human-robot interactions. There has been a number of studies on human action recognition [1], [10], and prediction [2]. Both hand crafted local feature representations and deep learned feature descriptors have been employed in these approaches, with both categories demonstrating excellent results in recognition of human actions. Hand-crafted local features such as Space Time Interest Points [11], Cuboids [12], Dense Trajectories [13], with rich descriptors of HOG, HOF, MBH have shown to be successful on a number of challenging datasets [8], [14].\n\nAlthough motion is an informative cue for action recognition, irrelevant motions in the background or the camera motion can be misleading. This is inevitable when dealing with realistic robotic applications in uncontrolled settings. Therefore, separating human action motion from camera motion remains a challenging problem. A few number of works tried to address this isse. Ikizler-Cinbis et al. utilized video stabilization by motion compensation for removing camera motion [15]. Wu et al. addressed the camera motion effects by decomposing Lagrangian particle trajectories into camera-induced and object-induced components for videos [16]. Wang et al. proposed a descriptor based on motion boundary histograms (MBH) which removes constant motions and therefore reduces the influence of camera motion [13]. What makes our method different from [13], is that we first reduce the smooth camera motion effects and get rid of background clutter by creating action region proposals based on a motion boundary detector. The selected regions would be used both in training and classification. However, the approach in [13] employs MBH on full images as motion descriptor for trajectories.\n\nAmong traditional methods, there are very few works that have tried to separate the background clutter from images. Chakraborty et al. presented an approach based on selective Spatio-Temporal Interest Points (STIPs) which are detected by suppressing background SIPs and imposing local and temporal constraints, resulting in more robust STIPs for actors and less unwanted background STIPs [17].\n\nZhang et al. addressed the activity recognition problem for multi-individuals based on local spatio-temporal features in which extracting irrelevant features from dynamic background clutter has been avoided using depth information [10]. Our work is different from them in terms of jointly eliminating background clutter and camera motion using optical flow and motion boundary detection concept.\n\nDeep learning models are a class of machine learning algorithms that learn a hierarchy of features by building high-level features from low-level ones. After impressive results of ConvNets on image classification tasks [18], researchers have also focused on using ConvNet models for action recognition. Several outstanding techniques are introduced that have had a significant impact on this field, such as 3D CNNs [19], RNN [20], CNNs [21] and Two-Stream ConvNet [22].\n\nThe majority of recent research has employed motion information to improve the results. Simonyan and Zisserman proposed a two stream ConvNet [22], which has formed the baseline of more recent studies [20]. In [22], spatial and temporal networks are trained individually and then fused. Additionally, two different types of stacking techniques are implemented for the temporal network, optical flow stacking and trajectory stacking. These techniques stack the horizontal (x) and vertical (y) flow channels (d t x,y ) of L consecutive frames to form a total of 2L input channels and obtained the best result for L=10 or 20-channel optical flow images. Recently, building on top of traditional Recurrent Neural Networks (RNNs), Donahue et al. proposed a longterm recurrent convolutional model that is applicable to visual time-series modeling [20].\n\nHowever, deep models ignore the effect of background dependency and moving camera in their training process and evaluations. In this work, our system is able to cope with the background clutter as well as camera motion using several motion cues to eliminate the regions that do not contain the human action.\n\n\nIII. OVERVIEW OF THE SYSTEM\n\nOur human action recognition approach consists of two main stages: 1) Selecting the action region proposals (motion salient regions) independent of camera motion and background information.\n\n2) Training ConvNets on action region proposals both in spatial and optical flow images, rather than full images. In the training process, we used 3 different ConvNet architectures: two stream ConvNet [22] followed by an SVM classifier to fuse the spatial and temporal features, a 3-D ConvNet that classifies a sequence of video frames as a video clip [23] and a very deep convolutional neural network [24] which is employed under the same two-stream framework.\n\nThe summary of approach is visualized in Figure 2. We describe each part in the following, before presenting experiments and evaluations in the next section. \n\n\nA. Selecting Action Region Proposals\n\nChoosing the action region would eliminate irrelevant regions, which reduces the number of regions being processed, and subsequently faster computation time. However, we face some challenges to have a precise action region proposal. The main challenge of choosing action region proposals compared to object proposals, is that we require both appearance and motion cues to be able to select the motion salient area. Differentiating human actions from the background or other dynamic motions is the first challenge due to the diversity of human actions. The second challenge would be caused by a moving camera. In many computer vision systems, data are only recorded by stationary cameras, which is unlikely the case in robotics applications. Therefore, it is essential to be able to handle camera motion.\n\nIn order to handle the mentioned challenges, we leverage the concept of motion boundaries to pick the interested area that only contains human activity. We firstly generate a mask by computing the motion boundaries using an algorithm that is built upon the presented work in [25]. Then we extract the action region proposals from video frames using the previously generated mask followed by an object proposal method [25,26].\n\nTo generate the motion boundaries, we use a combination of different spatial and temporal cues to shape a robust feature representation. The spatial information is three RGB channels, the norm of the gradient and the oriented gradient maps in four directions at coarse and fine scales.\n\nWe use multiple temporal cues to identify motion boundaries and generate our cropping mask. The first cue is the horizontal and vertical optical flow signals for both forward and backward process, computed by the state-ofthe-art algorithm, classic+NL, proposed in [28] due to the sharpness of the flow boundaries which results in the best optical flow performance. The second one would be an unoriented gradient map computed as the magnitude of horizontal and vertical optical flow gradient maps. The third temporal cue is oriented gradient maps in four directions at a coarse scale computed as the average of gradient maps components, weighted by their magnitudes. The next cue would be image warping errors which can be critical to prevent some optical flow estimation faults. We can compute image warping errors E D using (1) which is defined at a pixel p as\n2 1 ; 1 ( ) ( )- ( ( )) (1) D t t t t E p D p D p W p \uf02b \uf02b \uf03d \uf02b\nWhere W t;t+1 is optical flow between frame t and t+1 and D is a pixel-wise histogram of oriented gradients in eight orientations, which are all individually normalized to unit norm. The last one is motion boundaries histogram (MBH) that represents the gradient of the optical flow and can remove locally constant camera motion while keeping information about changes in the flow field. We compute spatial derivatives for both horizontal and vertical optical flow and orientation information is quantized into histograms, while we use the magnitude for weighting. Given this feature, we predict the binary boundary mask using structured random forests such that the predicted masks are averaged across all trees and all overlapping patches to yield the final soft-response boundary map [25]. Then, we employ it as a mask for video frames such that the area of motion is highlighted. Inspired by object detection approaches [26], [27], we select the desired region by applying an object detection method [26] on the resulted mask with highlighted motion areas.\n\nIn the following sections, we explain the procedure for the training and classification, which are done using three different ConvNet architectures.\n\n\nB. Training Process and Classification\n\nRecently proposed methods train the network by center cropping or randomly cropping the full image [22], [20], [10]. As a result, these approaches might fail in real robotic scenarios due to confusion caused by unbiased background and a moving camera. Conversely, our approach addresses those challenges by automatically identifying the image region where the action is likely to occur and then passes the action region as the input to the network. This process ensures that the most pertinent information to action is utilized. Therefore, we extract motion and appearance features of the motion salient region even if the actor's spatial location changes throughout the image.\n\n\n1) Training a 16-Layer ConvNet\n\nWe train our spatial and temporal networks on action region proposals obtained from Section A in spatial and temporal domains, respectively. Then we concatenate learnt features from both spatial and temporal streams and pass it to a SVM classifier to have a final classification. Our spatial and temporal networks contain three convolutional layers, three pooling layers and two fully connected layers that is built on top of the VGG-16 Layers architecture [29] implemented in Caffe [30].\n\n\n2) Training a 3D ConvNet\n\nWe also actively train on a sequence of our proposed RGB images using C3D architecture, which is particularly a good feature learning machine for action recognition [23]. We use 5 convolution layers, followed by 5 pooling layers, 2 fully-connected layers and a softmax loss layer for predicting action labels. The number of filters for 5 convolution layers are 64, 128, 256, 256, 256, respectively. We input 16 frames as a video clip for each video either in benchmark or our introduced datasets with the kernel size of 3 as the temporal depth due to verified experimental results in [23]. As a result, the input dimension for training on our action proposals equals to 3\u00d716\u00d7112\u00d7112. Since the 3D architecture involves exploiting both spatial and temporal cues during the training process, no temporal network is required.\n\n\n3) Training a 152-Layer ConvNet\n\nAnother inspiring architecture to apply our method is ResNet which is introduced recently [24]. To the best of our knowledge, this architecture has not been used for action recognition, while we have found it so effective in this task. Residual network can overcome the degradation problem through direct identity mappings between layers as skip connections, which allow the network to pass on features smoothly from earlier layers to later layers. We feed our cropped spatial and optical flow images from Section A, which are resized to 224\u00d7224, to our network containing 152 layers including convolutional layers and skip connections ending with a global average pooling layer and a fullyconnected layer with softmax.\n\n\nIV. EXPERIMENTAL SETUP\n\nIn this section we briefly explain our validation setup on benchmarks and three other experimental setups.\n\n\nA. Validation on Benchmarks\n\nTo have a thorough investigation of our method, we applied our method on two benchmarks in action recognition, UCF101 [8] and HMDB [14] using three ConvNet frameworks (details in Section III.B).\n\nUCF101 is a publicly available dataset, containing 13320 video clips, which is organized in three splits of training and testing data. Our tabulated results contain the average obtained accuracies on these three splits (Table I). HMDB is also an action recognition benchmark dataset containing 68K video clips, which is also organized in splits of training and testing data [14]. The number of outputs for the final fully connected layer in all frameworks equals to the action classes which is 101 and 51 for UCF101 and HMDB datasets, respectively.\n\n\nB. Exp. I: Non-biased Background Dataset 1\n\nThe aim of this experiment is to investigate how the stateof-the-art methods [23], [24] and our method perform in situations where the action's background differs from the conventional background that exists in the public dataset. We gathered almost 20 video samples for each of 11 actions, mentioned in Figure 3a, from the real videos recorded by a camera on the QUT campus and some available Youtube video samples in order to include a wider range of context in background compared to the UCF101 dataset (Figure 3a). We tested both ConvNet models [23], [24], trained on UCF101 dataset (provided in Table I), on the new dataset that we named \"Non-biased background dataset\".\n\n\nC. Exp. II: Moving Camera Dataset 2\n\nIn this experiment, we recorded several unconstrained videos using a mobile robot (Guiabot) moving around our work environment to capture students doing normal and abnormal actions in the office environment ( Figure 4). This datasets contains 16 videos for each action recorded in four places, office, corridor, kitchen and classroom. Camera motion ranges involved the robot moving from side to side, approaching the subject and rotating around the subject.\n\n\nD. Exp. III: Abnormal Behavior Detection\n\nThe aim of this experiment is detecting abnormal behavior in workspace environment by a mobile robot. Depending on the environment, different action classes are more likely to be observed than others. For instance, in a robotic lab, we do not expect to see people eating, drinking or playing sports. We propose to exploit such knowledge in our abnormal behavior detection system, which leverages the successes of ConvNets for action recognition and place categorization. To this end, robot initially requires to identify the place as well as the action being performed by the human. Then, by incorporating the learned prior knowledge, robot makes a decision on whether human behavior in that classified environment is normal or not. We divide our explanation of this task into five stages: 1) Scene categorization: In this part, we aim to do a frame based scene categorization. To this end, we use the Places205 network published by Zhou et al. [31], which is the state-of-the-art in scene categorization and follows the VGGNet architecture for training [29]. Their training dataset contains 2.5 million images of 205 semantic categories, with at least 5,000 images per category. We feed our new dataset recorded on the mobile robot (Section C) into the Places205. The output is a probability distribution over the 205 known scene types and select the highest probability as the probability of the given scene P(Si).\n\n2) Learning the prior knowledge: our system should learn the likelihood of each scene-action pair, which would enable the robot to make a decision about the normality or abnormality of the human behavior. To this end, we need to calculate occurring likelihood of each action in different scenes in our scenario. We denote this probability as P(S i |A i ). To compute P(S i |A i ), we input samples of two public datasets on action recognition, UCF101 and HMDB to the Places205 network and take the scene with maximum probability as the most frequently occurred place for each action.\n\n3) Action recognition regardless of the scene: we denote the probability of the recognized action through our action recognition pipeline as P(A i ).\n\n\n4)\n\nComputing the occurrence probability of actions given the scene: we calculate this likelihood, P(A i |S i ), for each action and scene using the following equation:\n\uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029 | . | ( 2 ) i i i i i i\n\nP S A P A P A S P S \uf03d\n\nWhere P(S i ), P(S i |A i ) and P(A i ) can be gained from the first, second and third stages, respectively. 5) Decision making: the aim of this stage is to compare the occurrence probability of an action given a scene P(A|S) (obtained from stage 4) with the occurrence likelihood of the same action with no scene knowledge P(A) (obtained from the stage 3). We follow a simple comparison algorithm; the recognized action in the detected scene is an abnormal behavior if the Abnormal Behavior Detection index, defined as ABD_Ind in equation (3), returns a positive number greater than a pre-defined threshold, Otherwise, it would be considered as a normal activity.\n\uf028 \uf029 \uf028 \uf029 _ | ( 3 ) i i i ABD Ind P A P A S \uf03d \uf02d\nSince the problem is a binary classification and the probability values are scattered between [0,1], we set the threshold to 0.5. For instance, if P(A|S) is very low, only a recognized action with probability greater than 0.5 can meet the condition for being an abnormal behavior. Figure 5 demonstrates the overview of our abnormal behavior detection system and how it performs on one correctly identified example from our dataset.\n\n\nV. RESULTS\n\nThis section present the results obtained from the experiments described above.\n\n\nA. Validation on Benchmarks\n\nIn this section, we present the results of our action recognition system on UCF101and HMDB. Table I provides an extensive comparison with the state-of-the-art methods. We believe the main reason to achieve the matching performance with the state-of-the-art without exploiting the background cues is the elimination of camera motion. We can systematically crop the salient motion areas which leads to a more precise feature learning process.   Figure 3c verifies the outperformance of our method compared to the existing state-of-the-art methods [22], [23], [24], when background does not include any informative context (Non-biased background dataset). Figures 3b and 3c demonstrate the consistency in performance of our method regardless of the background context on both datasets. It is important to note changing the background in our new dataset, negatively impacts the performance of the state-ofthe-art methods.\n\n\nB. Exp. I: Non-biased Background Dataset\n\nDue to random image cropping in [22], [23] versus selecting the motion salient areas in our approach during the training process, it is more likely that these methods fail to contain the motion cues.\n\n\nC. Exp. II: Moving Camera Dataset\n\nThis experiment shows how our action recognition system successfully handles the camera motion better than the state-of-the-art methods. Table II demonstrates the accuracies for the proposed models in [22], [23], [24] and our method on our robot dataset using a moving camera. The reason would be due to eliminating the camera motion effects by actively training on action regions rather than full images. \n\n\nD. Exp. III: Abnormal behavior Detection\n\nThe results in this experiment show the power of our proposed approach in Section IV.D in detecting abnormal human behaviors in the workspace.\n\nThe system used in this experiment includes an action recognition pipeline, a scene categorization method in addition to learning the prior knowledge. We investigate the use of three state-of-the-art action recognition approaches in the abnormal detection pipeline, while the rest of the system remains the same.\n\nThe results indicate an 87.50% success rate for abnormal human behavior detection on our moving camera dataset containing 16 videos for each action in four places. We test [22], [23], [24] on our dataset. Results are shown in Table III. We conjecture the ability of action recognition method regardless of environment and camera motion plays a significant role in enabling the robot to achieve a higher success rate in detecting abnormal behavior.\n\n\nVI. CONCLUSION\n\nIn this paper, we focused on two of the main challenges in transitioning from computer vision methods to robotics applications; the sensitivity of many traditional approaches on background cues, and the effect of camera motion in a robotics context. We addressed these challenges by developing methods for selecting action region proposals that are motion salient and more likely to contain the actions, regardless of background and camera motion. Using two new datasets, the \"Non-biased background dataset\" and the \"Moving camera dataset\", we demonstrated our method using both spatial and temporal images to outperform state-of-the-art ConvNet models, and enabling the development of an abnormal behavior detection system. The results obtained indicate how combining a robust action recognition system with the semantic scene category knowledge can enable a robot to detect normal and abnormal human behavior in a typical office environment.\n\nIn future work, robots equipped with SLAM systems that have access to semantic information will enable better action recognition performance. Real world robot operation introduces a number of challenges including varying lighting and motion blur; we will adapt successful investigations into learning features that are invariant to these issues in other fields such as place recognition to apply to action recognition. Finally, we plan to investigate the utility of online action recognition for informing robot operations in a range of tasks such as domestic chores and assistive robotics.\n\nFigure 1 .\n1Performance of our action recognition approach in two scenarios. Scenario1 involves action recognition by a moving robot with unbiased background. Scenario 2 comprises abnormal behavior detection in an office environment.\n\nFigure 2 .\n2Overview of our approach for unbiased human action recognition on samples of the Guiabot robot dataset. The robot is moving from left to right, while approaching to people. The method is tested using two different ConvNet architecture, denoted by solid and dotted blocks.\n\nFigure 3 .\n3(a) Performance comparison of Three methods on UCF101 dataset. (b) Performance comparison of three methods on Non-biased background dataset. (c) Samples of generated action region proposals on our non-biased background dataset.\n\nFigure 4 .\n4Samples of generated action region proposals and recognized scene and action label on our Guiabot robot dataset.\n\nFigure 5 .\n5Overview of our approach for unbiased human action recognition on a sample of the Guiabot robot dataset.\n\nTABLE I .\nIPERFORMANCE COMPARISON WITH THE STATE-OF-THE-ART DEEP NETWORKS ON UCF101 AND HMDB DATASETMethods \nUCF101 \nHMDB \n\nSpatial \nTemporal \nFull \nSpatial \n\nOurs with Two-\nstream Net. \n70.1% \n80.7% \n88.63% \n40% \n\nOurs with C3D \n-\n-\n73.3% \n40.8% \n\nOurs with ResNet \n74.73% -\n-\n42.1% \n\n[22] \n72.7% \n\n73.9% \n(L=1) \n81% \n(L=10) \n\n-\n(L=1) \n88% \n(L=10) \n\n40.5% \n\n[20] \n71.1% \n76.9% \n82.9% \n-\n\n[21] \n73.1% \n-\n88.6% \n-\n\n[4] \n-\n-\n65.4% \n-\n\nC3D on full img. \n-\n-\n79.8% \n49.91% \n\nResNet on full img. \n79.82% -\n-\n49.9% \n\n\n\nTABLE II .\nIIPERFORMANCE COMPARISON OF OUR APPROACH AGAINST[22,23,24] ON GUIABOT ROBOT DATASETActions \n\nAction recognition methods \n\nOurs \n[21] \n[22] \n[23] \n\nBodyWeightSquats \n100% \n62.5% \n50% \n75% \n\nJumpRope \n93.75% \n50% \n50% \n50% \n\nPunch \n93.75% \n43.75% \n37.5% \n43.75% \n\nEat \n68.75% \n6.25% \n18.75% \n18.75% \n\nDrink \n81.25% \n25% \n25% \n31.25% \n\n\n\nTABLE III .\nIIICOMPARISON OF ABNORMAL BEHAVIOR DETECTION SUCCESS RATESOurs \n[22] \n[23] \n[24] \n\n87.50% \n37.5% \n37.5% \n43.75% \n\n\nThis dataset will be publically available.2 This dataset will be publically available.\nACKNOWLEDGMENT This Research has supported by a QUTPRA and Australian Centre of Excellence for Robotic Vision (project number CE140100016). I would like to thank Professor Gordon Wyeth who provided insights and expertise that greatly assisted this research.\nAction recognition for surveillance applications using optic flow and SVM. S Danafar, N Gheissari, Asian Conf. on Computer Vision. S. Danafar, N. Gheissari, \"Action recognition for surveillance applications using optic flow and SVM\", in Asian Conf. on Computer Vision, pp. 457-466, 2007.\n\nAnomaly detection in crowded scenes. V Mahadevan, W Li, V Bhalodia, N Vasconcelos, IEEE Conf. on Computer Vision and Pattern Recognition. 249V. Mahadevan, W. Li, V. Bhalodia, and N. Vasconcelos, \"Anomaly detection in crowded scenes\", in IEEE Conf. on Computer Vision and Pattern Recognition,Vol. 249. 2010.\n\nModeling high-dimensional humans for activity anticipation using gaussian process latent crfs. Y Jiang, A Saxena, Robotics Science and Systems Conf. Y. Jiang and A. Saxena, \"Modeling high-dimensional humans for activity anticipation using gaussian process latent crfs\", in Robotics Science and Systems Conf., 2014.\n\nLarge-scale video classification with convolutional neural networks. A Karpathy, G Toderici, S Shetty, T Leung, R Sukthankar, L Fei-Fei, IEEE Conf. on Computer Vision and Pattern Recognition. A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. in IEEE Conf. on Computer Vision and Pattern Recognition, 2014.\n\nB Zhou, A Khosla, A Lapedriza, A Oliva, A Torralba, arXiv:1412.6856Object Detectors Emerge in Deep Scenes CNNs. cs.CVB. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, \"Object Detectors Emerge in Deep Scenes CNNs\", arXiv:1412.6856 [cs.CV].\n\nEnhancing human action recognition with region proposals. F Rezazadegan, S Shirazi, N Sunderhauf, M Milford, B Upcroft, Australian Conf. on Robotics and Automation. F. Rezazadegan, S. Shirazi, N. Sunderhauf, M. Milford, B. Upcroft, \"Enhancing human action recognition with region proposals\", in Australian Conf. on Robotics and Automation, 2015.\n\nFinding action tubes. G Gkioxari, &amp; J Malik, IEEE Conf. on Computer Vision and Pattern Recognition. G. Gkioxari, & J. Malik, \"Finding action tubes\", in IEEE Conf. on Computer Vision and Pattern Recognition, pp. 759-768, 2015.\n\nUCF101: A dataset of 101 human actions classes from videos in the wild. K Soomro, A R Zamir, M Shah, abs/1212.0402CoRR. K. Soomro, A. R. Zamir, and M. Shah, \"UCF101: A dataset of 101 human actions classes from videos in the wild\" CoRR, abs/1212.0402, 2012.\n\nAbnormal activity recognition in office based on R transform. Y Wang, K Huang, &amp; T Tan, IEEE Int. Conf. on Image Processing. 1341Y. Wang, K. Huang, & T. Tan, \"Abnormal activity recognition in office based on R transform\", in IEEE Int. Conf. on Image Processing, Vol. 1, pp. I-341, 2007.\n\nAdaptive Human-Centered Representation for Activity Recognition of Multiple Individuals from 3D Point Cloud Sequences. H Zhang1, C Reardon, C Zhang, L E Parker, IEEE Int. Conf. on Robotics and Automation. H. Zhang1 , C. Reardon , C. Zhang and L. E. Parker, \"Adaptive Human-Centered Representation for Activity Recognition of Multiple Individuals from 3D Point Cloud Sequences\", in IEEE Int. Conf. on Robotics and Automation, pp. 1991-1998, 2015.\n\nOn space-time interest points. I Laptev, Int. Journal of Computer Vision. 642/3I. Laptev, \"On space-time interest points\", in Int. Journal of Computer Vision, vol. 64(2/3), 2005, pp.107-123, 2005.\n\nBehavior recognition via sparse spatio-temporal features. P Doll\u00b4ar, V Rabaud, G Cottrell, S Belongie, VS-PETS. P. Doll\u00b4ar, V. Rabaud, G. Cottrell, and S. Belongie, \"Behavior recognition via sparse spatio-temporal features\", in VS-PETS, 2005.\n\nDense trajectories and motion boundary descriptors for action recognition. in Int. H Wang, A Kl\u00a8aser, C Schmid, C.-L Liu, Journal of Computer Vision. 1031H. Wang, A. Kl\u00a8aser, C. Schmid, and C.-L. Liu. Dense trajectories and motion boundary descriptors for action recognition. in Int. Journal of Computer Vision, vol. 103, no. 1, 2013.\n\nHMDB: A large video database for human motion recognition. H Kuehne, H Jhuang, E Garrote, T Poggio, T Serre, IEEE Int. Conf. On Computer Vision. H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, \"HMDB: A large video database for human motion recognition\", in IEEE Int. Conf. On Computer Vision, 2011.\n\nObject, scene and actions: Combining multiple features for human action recognition. N Ikizler-Cinbis, S Sclaroff, European Conf. on Computer vision. N. Ikizler-Cinbis and S. Sclaroff. \"Object, scene and actions: Combining multiple features for human action recognition\", in European Conf. on Computer vision, 2010.\n\nAction recognition in videos acquired by a moving camera using motion decomposition of lagrangian particle trajectories. S Wu, O Oreifej, &amp; M Shah, IEEE Int. Conf. on computer vision. S. Wu, O. Oreifej, & M. Shah, \"Action recognition in videos acquired by a moving camera using motion decomposition of lagrangian particle trajectories\", in IEEE Int. Conf. on computer vision, 2011.\n\nSelective spatiotemporal interest points. B Chakraborty, M B Holte, T B Moeslund, J Gonzlez, Computer Vision and Image Understanding. 1163B. Chakraborty, M.B. Holte, T.B. Moeslund, J. Gonzlez, \"Selective spatiotemporal interest points\", Computer Vision and Image Understanding 116 (3), 2012, pp. 396-410.\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Annual Conf. on Neural Information Processing Systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton. \"ImageNet classification with deep convolutional neural networks\", in Annual Conf. on Neural Information Processing Systems, 2012, pp. 1106- 1114.\n\n3D Convolutional Neural Networks for Human Action Recognition. S Ji, W Xu, M Yang, K Yu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 351S. Ji, W. Xu, M. Yang, and K. Yu, \"3D Convolutional Neural Networks for Human Action Recognition\", IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, 2013, pp. 221-231.\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, L A Hendricks, S Guadarrama, M Rohrbach, S Venugopalan, K Saenko, T Darrell, IEEE Conf. on Computer Vision and Pattern Recognition. J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, \"Long-term recurrent convolutional networks for visual recognition and description\", in IEEE Conf. on Computer Vision and Pattern Recognition, 2015, pp. 1411-4389.\n\nBeyond short snippets: Deep networks for video classification. J Y Ng, M Hausknecht, S Vijayanarasimhan, O Vinyals, R Monga, G Toderici, IEEE Conf. on Computer Vision and Pattern Recognition. J. Y.-H. Ng, M. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici, \"Beyond short snippets: Deep networks for video classification\", in IEEE Conf. on Computer Vision and Pattern Recognition, 2015, pp. 4694-4702.\n\nTwo-stream convolutional networks for action recognition in videos. K Simonyan, A Zisserman, Annual Conf. on Neural Information Processing Systems. K. Simonyan and A. Zisserman, \"Two-stream convolutional networks for action recognition in videos\", in Annual Conf. on Neural Information Processing Systems, 2014, pp.1-8.\n\nLearning spatiotemporal features with 3d convolutional networks. D Tran, L Bourdev, R Fergus, L Torresani, &amp; M Paluri, IEEE Int. Conf. on Computer Vision. D. Tran, L. Bourdev, R. Fergus, L. Torresani, & M. Paluri, \"Learning spatiotemporal features with 3d convolutional networks\", in IEEE Int. Conf. on Computer Vision, 2015.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, &amp; J Sun, arXiv:1512.03385arXiv preprintK. He, X. Zhang, S. Ren, & J. Sun, \"Deep residual learning for image recognition\", arXiv preprint arXiv:1512.03385, 2015.\n\nLearning to detect motion boundaries. P Weinzaepfel, J Evaud, Z Harchaoui, &amp; C Schmid, IEEE Conf. on Computer Vision and Pattern Recognition. P. Weinzaepfel, J. evaud, Z. Harchaoui, & C. Schmid, \"Learning to detect motion boundaries\", in IEEE Conf. on Computer Vision and Pattern Recognition, 2015.\n\nEdge boxes: Locating object proposals from edges. C , Lawrence Zitnick, Piotr Doll\u00e1r, European Conf. on Computer vision. C. Lawrence Zitnick and Piotr Doll\u00e1r, \"Edge boxes: Locating object proposals from edges\", in European Conf. on Computer vision, 2014.\n\nEvaluation of object detection proposal under condition variations. F Rezazadegan, S Shirazi, M Milford, B Upcroft, IEEE Conf. on Computer Vision and Pattern Recognition Workshops. F. Rezazadegan, S. Shirazi, M. Milford, B. Upcroft, \"Evaluation of object detection proposal under condition variations\", in IEEE Conf. on Computer Vision and Pattern Recognition Workshops, 2015.\n\nA quantitative analysis of current practices in optical flow estimation and the principles behind them. D Sun, S Roth, M Black, Int. Journal of Computer Vision. D. Sun, S. Roth, and M. Black, \"A quantitative analysis of current practices in optical flow estimation and the principles behind them\", in Int. Journal of Computer Vision, 2014.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, CoRR, abs/1409.1556K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition\", 2014, CoRR, abs/1409.1556.\n\nCaffe: Convolutional architecture for fast feature embedding. Y Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, ACM int. Conf. on Multimedia. Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, \"Caffe: Convolutional architecture for fast feature embedding\", in ACM int. Conf. on Multimedia, 2014.\n\nLearning deep features for scene recognition using places database. B Zhou, A Lapedriza, J Xiao, A Torralba, &amp; A Oliva, Advances in neural information processing systems. B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, & A. Oliva,. Learning deep features for scene recognition using places database. in Advances in neural information processing systems, pp. 487-495, 2014.\n", "annotations": {"author": "[{\"end\":202,\"start\":61},{\"end\":338,\"start\":203},{\"end\":472,\"start\":339},{\"end\":610,\"start\":473}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":69},{\"end\":216,\"start\":209},{\"end\":350,\"start\":343},{\"end\":488,\"start\":481}]", "author_first_name": "[{\"end\":68,\"start\":61},{\"end\":208,\"start\":203},{\"end\":342,\"start\":339},{\"end\":480,\"start\":473}]", "author_affiliation": "[{\"end\":201,\"start\":82},{\"end\":337,\"start\":218},{\"end\":471,\"start\":352},{\"end\":609,\"start\":490}]", "title": "[{\"end\":58,\"start\":1},{\"end\":668,\"start\":611}]", "venue": null, "abstract": "[{\"end\":2103,\"start\":735}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2245,\"start\":2242},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2271,\"start\":2268},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2708,\"start\":2705},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2882,\"start\":2879},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3187,\"start\":3184},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3949,\"start\":3946},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4082,\"start\":4079},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4087,\"start\":4084},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4765,\"start\":4762},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5076,\"start\":5073},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6810,\"start\":6807},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6816,\"start\":6812},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6836,\"start\":6833},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7116,\"start\":7112},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7130,\"start\":7126},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7155,\"start\":7151},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7263,\"start\":7260},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7269,\"start\":7265},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7752,\"start\":7748},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7913,\"start\":7909},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8079,\"start\":8075},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8122,\"start\":8118},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8389,\"start\":8385},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8849,\"start\":8845},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9087,\"start\":9083},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9472,\"start\":9468},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9668,\"start\":9664},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9678,\"start\":9674},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9689,\"start\":9685},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9717,\"start\":9713},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9865,\"start\":9861},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9924,\"start\":9920},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9933,\"start\":9929},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10564,\"start\":10560},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11302,\"start\":11298},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11453,\"start\":11449},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11503,\"start\":11499},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12843,\"start\":12839},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12985,\"start\":12981},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12988,\"start\":12985},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13546,\"start\":13542},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14992,\"start\":14988},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15129,\"start\":15125},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15135,\"start\":15131},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15209,\"start\":15205},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15557,\"start\":15553},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":15563,\"start\":15559},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15569,\"start\":15565},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16627,\"start\":16623},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16653,\"start\":16649},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16852,\"start\":16848},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17271,\"start\":17267},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17635,\"start\":17631},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18546,\"start\":18543},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18560,\"start\":18556},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18999,\"start\":18995},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19297,\"start\":19293},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19303,\"start\":19299},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19769,\"start\":19765},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19775,\"start\":19771},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21382,\"start\":21378},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21491,\"start\":21487},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24638,\"start\":24634},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24644,\"start\":24640},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24650,\"start\":24646},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25087,\"start\":25083},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25093,\"start\":25089},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25493,\"start\":25489},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25499,\"start\":25495},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25505,\"start\":25501},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26373,\"start\":26369},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26379,\"start\":26375},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26385,\"start\":26381},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29781,\"start\":29777},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29784,\"start\":29781},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29787,\"start\":29784},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30234,\"start\":30233}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28433,\"start\":28199},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28718,\"start\":28434},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28959,\"start\":28719},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29085,\"start\":28960},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29203,\"start\":29086},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29716,\"start\":29204},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30062,\"start\":29717},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":30190,\"start\":30063}]", "paragraph": "[{\"end\":2709,\"start\":2122},{\"end\":3188,\"start\":2711},{\"end\":3698,\"start\":3190},{\"end\":4195,\"start\":3700},{\"end\":5280,\"start\":4197},{\"end\":5575,\"start\":5282},{\"end\":6291,\"start\":5577},{\"end\":6632,\"start\":6293},{\"end\":7270,\"start\":6653},{\"end\":8455,\"start\":7272},{\"end\":8850,\"start\":8457},{\"end\":9247,\"start\":8852},{\"end\":9718,\"start\":9249},{\"end\":10565,\"start\":9720},{\"end\":10874,\"start\":10567},{\"end\":11095,\"start\":10906},{\"end\":11558,\"start\":11097},{\"end\":11718,\"start\":11560},{\"end\":12562,\"start\":11759},{\"end\":12989,\"start\":12564},{\"end\":13276,\"start\":12991},{\"end\":14139,\"start\":13278},{\"end\":15261,\"start\":14202},{\"end\":15411,\"start\":15263},{\"end\":16131,\"start\":15454},{\"end\":16654,\"start\":16166},{\"end\":17505,\"start\":16683},{\"end\":18260,\"start\":17541},{\"end\":18393,\"start\":18287},{\"end\":18619,\"start\":18425},{\"end\":19169,\"start\":18621},{\"end\":19891,\"start\":19216},{\"end\":20388,\"start\":19931},{\"end\":21849,\"start\":20433},{\"end\":22434,\"start\":21851},{\"end\":22585,\"start\":22436},{\"end\":22756,\"start\":22592},{\"end\":23485,\"start\":22821},{\"end\":23963,\"start\":23532},{\"end\":24057,\"start\":23978},{\"end\":25006,\"start\":24089},{\"end\":25250,\"start\":25051},{\"end\":25694,\"start\":25288},{\"end\":25881,\"start\":25739},{\"end\":26195,\"start\":25883},{\"end\":26644,\"start\":26197},{\"end\":27606,\"start\":26663},{\"end\":28198,\"start\":27608}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14201,\"start\":14140},{\"attributes\":{\"id\":\"formula_1\"},\"end\":22796,\"start\":22757},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23531,\"start\":23486}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":18849,\"start\":18840},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19824,\"start\":19816},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24188,\"start\":24181},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25433,\"start\":25425},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26432,\"start\":26423}]", "section_header": "[{\"end\":2120,\"start\":2105},{\"end\":6651,\"start\":6635},{\"end\":10904,\"start\":10877},{\"end\":11757,\"start\":11721},{\"end\":15452,\"start\":15414},{\"end\":16164,\"start\":16134},{\"end\":16681,\"start\":16657},{\"end\":17539,\"start\":17508},{\"end\":18285,\"start\":18263},{\"end\":18423,\"start\":18396},{\"end\":19214,\"start\":19172},{\"end\":19929,\"start\":19894},{\"end\":20431,\"start\":20391},{\"end\":22590,\"start\":22588},{\"end\":22819,\"start\":22798},{\"end\":23976,\"start\":23966},{\"end\":24087,\"start\":24060},{\"end\":25049,\"start\":25009},{\"end\":25286,\"start\":25253},{\"end\":25737,\"start\":25697},{\"end\":26661,\"start\":26647},{\"end\":28210,\"start\":28200},{\"end\":28445,\"start\":28435},{\"end\":28730,\"start\":28720},{\"end\":28971,\"start\":28961},{\"end\":29097,\"start\":29087},{\"end\":29214,\"start\":29205},{\"end\":29728,\"start\":29718},{\"end\":30075,\"start\":30064}]", "table": "[{\"end\":29716,\"start\":29305},{\"end\":30062,\"start\":29812},{\"end\":30190,\"start\":30134}]", "figure_caption": "[{\"end\":28433,\"start\":28212},{\"end\":28718,\"start\":28447},{\"end\":28959,\"start\":28732},{\"end\":29085,\"start\":28973},{\"end\":29203,\"start\":29099},{\"end\":29305,\"start\":29216},{\"end\":29812,\"start\":29731},{\"end\":30134,\"start\":30079}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5230,\"start\":5221},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11609,\"start\":11601},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19529,\"start\":19520},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19733,\"start\":19722},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20148,\"start\":20140},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23821,\"start\":23813},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24541,\"start\":24532}]", "bib_author_first_name": "[{\"end\":30612,\"start\":30611},{\"end\":30623,\"start\":30622},{\"end\":30863,\"start\":30862},{\"end\":30876,\"start\":30875},{\"end\":30882,\"start\":30881},{\"end\":30894,\"start\":30893},{\"end\":31229,\"start\":31228},{\"end\":31238,\"start\":31237},{\"end\":31519,\"start\":31518},{\"end\":31531,\"start\":31530},{\"end\":31543,\"start\":31542},{\"end\":31553,\"start\":31552},{\"end\":31562,\"start\":31561},{\"end\":31576,\"start\":31575},{\"end\":31854,\"start\":31853},{\"end\":31862,\"start\":31861},{\"end\":31872,\"start\":31871},{\"end\":31885,\"start\":31884},{\"end\":31894,\"start\":31893},{\"end\":32159,\"start\":32158},{\"end\":32174,\"start\":32173},{\"end\":32185,\"start\":32184},{\"end\":32199,\"start\":32198},{\"end\":32210,\"start\":32209},{\"end\":32470,\"start\":32469},{\"end\":32486,\"start\":32481},{\"end\":32488,\"start\":32487},{\"end\":32751,\"start\":32750},{\"end\":32761,\"start\":32760},{\"end\":32763,\"start\":32762},{\"end\":32772,\"start\":32771},{\"end\":32999,\"start\":32998},{\"end\":33007,\"start\":33006},{\"end\":33020,\"start\":33015},{\"end\":33022,\"start\":33021},{\"end\":33348,\"start\":33347},{\"end\":33358,\"start\":33357},{\"end\":33369,\"start\":33368},{\"end\":33378,\"start\":33377},{\"end\":33380,\"start\":33379},{\"end\":33707,\"start\":33706},{\"end\":33932,\"start\":33931},{\"end\":33943,\"start\":33942},{\"end\":33953,\"start\":33952},{\"end\":33965,\"start\":33964},{\"end\":34201,\"start\":34200},{\"end\":34209,\"start\":34208},{\"end\":34220,\"start\":34219},{\"end\":34233,\"start\":34229},{\"end\":34513,\"start\":34512},{\"end\":34523,\"start\":34522},{\"end\":34533,\"start\":34532},{\"end\":34544,\"start\":34543},{\"end\":34554,\"start\":34553},{\"end\":34850,\"start\":34849},{\"end\":34868,\"start\":34867},{\"end\":35203,\"start\":35202},{\"end\":35209,\"start\":35208},{\"end\":35224,\"start\":35219},{\"end\":35226,\"start\":35225},{\"end\":35511,\"start\":35510},{\"end\":35526,\"start\":35525},{\"end\":35528,\"start\":35527},{\"end\":35537,\"start\":35536},{\"end\":35539,\"start\":35538},{\"end\":35551,\"start\":35550},{\"end\":35840,\"start\":35839},{\"end\":35854,\"start\":35853},{\"end\":35867,\"start\":35866},{\"end\":35869,\"start\":35868},{\"end\":36192,\"start\":36191},{\"end\":36198,\"start\":36197},{\"end\":36204,\"start\":36203},{\"end\":36212,\"start\":36211},{\"end\":36567,\"start\":36566},{\"end\":36578,\"start\":36577},{\"end\":36580,\"start\":36579},{\"end\":36593,\"start\":36592},{\"end\":36607,\"start\":36606},{\"end\":36619,\"start\":36618},{\"end\":36634,\"start\":36633},{\"end\":36644,\"start\":36643},{\"end\":37038,\"start\":37037},{\"end\":37040,\"start\":37039},{\"end\":37046,\"start\":37045},{\"end\":37060,\"start\":37059},{\"end\":37080,\"start\":37079},{\"end\":37091,\"start\":37090},{\"end\":37100,\"start\":37099},{\"end\":37468,\"start\":37467},{\"end\":37480,\"start\":37479},{\"end\":37786,\"start\":37785},{\"end\":37794,\"start\":37793},{\"end\":37805,\"start\":37804},{\"end\":37815,\"start\":37814},{\"end\":37832,\"start\":37827},{\"end\":37834,\"start\":37833},{\"end\":38098,\"start\":38097},{\"end\":38104,\"start\":38103},{\"end\":38113,\"start\":38112},{\"end\":38124,\"start\":38119},{\"end\":38126,\"start\":38125},{\"end\":38324,\"start\":38323},{\"end\":38339,\"start\":38338},{\"end\":38348,\"start\":38347},{\"end\":38365,\"start\":38360},{\"end\":38367,\"start\":38366},{\"end\":38640,\"start\":38639},{\"end\":38651,\"start\":38643},{\"end\":38666,\"start\":38661},{\"end\":38914,\"start\":38913},{\"end\":38929,\"start\":38928},{\"end\":38940,\"start\":38939},{\"end\":38951,\"start\":38950},{\"end\":39328,\"start\":39327},{\"end\":39335,\"start\":39334},{\"end\":39343,\"start\":39342},{\"end\":39633,\"start\":39632},{\"end\":39645,\"start\":39644},{\"end\":39867,\"start\":39866},{\"end\":39874,\"start\":39873},{\"end\":39887,\"start\":39886},{\"end\":39898,\"start\":39897},{\"end\":39909,\"start\":39908},{\"end\":39917,\"start\":39916},{\"end\":39929,\"start\":39928},{\"end\":39943,\"start\":39942},{\"end\":40255,\"start\":40254},{\"end\":40263,\"start\":40262},{\"end\":40276,\"start\":40275},{\"end\":40284,\"start\":40283},{\"end\":40300,\"start\":40295},{\"end\":40302,\"start\":40301}]", "bib_author_last_name": "[{\"end\":30620,\"start\":30613},{\"end\":30633,\"start\":30624},{\"end\":30873,\"start\":30864},{\"end\":30879,\"start\":30877},{\"end\":30891,\"start\":30883},{\"end\":30906,\"start\":30895},{\"end\":31235,\"start\":31230},{\"end\":31245,\"start\":31239},{\"end\":31528,\"start\":31520},{\"end\":31540,\"start\":31532},{\"end\":31550,\"start\":31544},{\"end\":31559,\"start\":31554},{\"end\":31573,\"start\":31563},{\"end\":31584,\"start\":31577},{\"end\":31859,\"start\":31855},{\"end\":31869,\"start\":31863},{\"end\":31882,\"start\":31873},{\"end\":31891,\"start\":31886},{\"end\":31903,\"start\":31895},{\"end\":32171,\"start\":32160},{\"end\":32182,\"start\":32175},{\"end\":32196,\"start\":32186},{\"end\":32207,\"start\":32200},{\"end\":32218,\"start\":32211},{\"end\":32479,\"start\":32471},{\"end\":32494,\"start\":32489},{\"end\":32758,\"start\":32752},{\"end\":32769,\"start\":32764},{\"end\":32777,\"start\":32773},{\"end\":33004,\"start\":33000},{\"end\":33013,\"start\":33008},{\"end\":33026,\"start\":33023},{\"end\":33355,\"start\":33349},{\"end\":33366,\"start\":33359},{\"end\":33375,\"start\":33370},{\"end\":33387,\"start\":33381},{\"end\":33714,\"start\":33708},{\"end\":33940,\"start\":33933},{\"end\":33950,\"start\":33944},{\"end\":33962,\"start\":33954},{\"end\":33974,\"start\":33966},{\"end\":34206,\"start\":34202},{\"end\":34217,\"start\":34210},{\"end\":34227,\"start\":34221},{\"end\":34237,\"start\":34234},{\"end\":34520,\"start\":34514},{\"end\":34530,\"start\":34524},{\"end\":34541,\"start\":34534},{\"end\":34551,\"start\":34545},{\"end\":34560,\"start\":34555},{\"end\":34865,\"start\":34851},{\"end\":34877,\"start\":34869},{\"end\":35206,\"start\":35204},{\"end\":35217,\"start\":35210},{\"end\":35231,\"start\":35227},{\"end\":35523,\"start\":35512},{\"end\":35534,\"start\":35529},{\"end\":35548,\"start\":35540},{\"end\":35559,\"start\":35552},{\"end\":35851,\"start\":35841},{\"end\":35864,\"start\":35855},{\"end\":35876,\"start\":35870},{\"end\":36195,\"start\":36193},{\"end\":36201,\"start\":36199},{\"end\":36209,\"start\":36205},{\"end\":36215,\"start\":36213},{\"end\":36575,\"start\":36568},{\"end\":36590,\"start\":36581},{\"end\":36604,\"start\":36594},{\"end\":36616,\"start\":36608},{\"end\":36631,\"start\":36620},{\"end\":36641,\"start\":36635},{\"end\":36652,\"start\":36645},{\"end\":37043,\"start\":37041},{\"end\":37057,\"start\":37047},{\"end\":37077,\"start\":37061},{\"end\":37088,\"start\":37081},{\"end\":37097,\"start\":37092},{\"end\":37109,\"start\":37101},{\"end\":37477,\"start\":37469},{\"end\":37490,\"start\":37481},{\"end\":37791,\"start\":37787},{\"end\":37802,\"start\":37795},{\"end\":37812,\"start\":37806},{\"end\":37825,\"start\":37816},{\"end\":37841,\"start\":37835},{\"end\":38101,\"start\":38099},{\"end\":38110,\"start\":38105},{\"end\":38117,\"start\":38114},{\"end\":38130,\"start\":38127},{\"end\":38336,\"start\":38325},{\"end\":38345,\"start\":38340},{\"end\":38358,\"start\":38349},{\"end\":38374,\"start\":38368},{\"end\":38659,\"start\":38652},{\"end\":38673,\"start\":38667},{\"end\":38926,\"start\":38915},{\"end\":38937,\"start\":38930},{\"end\":38948,\"start\":38941},{\"end\":38959,\"start\":38952},{\"end\":39332,\"start\":39329},{\"end\":39340,\"start\":39336},{\"end\":39349,\"start\":39344},{\"end\":39642,\"start\":39634},{\"end\":39655,\"start\":39646},{\"end\":39871,\"start\":39868},{\"end\":39884,\"start\":39875},{\"end\":39895,\"start\":39888},{\"end\":39906,\"start\":39899},{\"end\":39914,\"start\":39910},{\"end\":39926,\"start\":39918},{\"end\":39940,\"start\":39930},{\"end\":39951,\"start\":39944},{\"end\":40260,\"start\":40256},{\"end\":40273,\"start\":40264},{\"end\":40281,\"start\":40277},{\"end\":40293,\"start\":40285},{\"end\":40308,\"start\":40303}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":10125608},\"end\":30823,\"start\":30536},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206591190},\"end\":31131,\"start\":30825},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7755839},\"end\":31447,\"start\":31133},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206592218},\"end\":31851,\"start\":31449},{\"attributes\":{\"doi\":\"arXiv:1412.6856\",\"id\":\"b4\"},\"end\":32098,\"start\":31853},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11971534},\"end\":32445,\"start\":32100},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1035098},\"end\":32676,\"start\":32447},{\"attributes\":{\"doi\":\"abs/1212.0402\",\"id\":\"b7\",\"matched_paper_id\":7197134},\"end\":32934,\"start\":32678},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15785752},\"end\":33226,\"start\":32936},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9202194},\"end\":33673,\"start\":33228},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2619278},\"end\":33871,\"start\":33675},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1956774},\"end\":34115,\"start\":33873},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5401052},\"end\":34451,\"start\":34117},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206769852},\"end\":34762,\"start\":34453},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9645996},\"end\":35079,\"start\":34764},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15444833},\"end\":35466,\"start\":35081},{\"attributes\":{\"id\":\"b16\"},\"end\":35772,\"start\":35468},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":195908774},\"end\":36126,\"start\":35774},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1923924},\"end\":36481,\"start\":36128},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5736847},\"end\":36972,\"start\":36483},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":4245530},\"end\":37397,\"start\":36974},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11797475},\"end\":37718,\"start\":37399},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1122604},\"end\":38049,\"start\":37720},{\"attributes\":{\"doi\":\"arXiv:1512.03385\",\"id\":\"b23\"},\"end\":38283,\"start\":38051},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13867043},\"end\":38587,\"start\":38285},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":5984060},\"end\":38843,\"start\":38589},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14059983},\"end\":39221,\"start\":38845},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":16219282},\"end\":39562,\"start\":39223},{\"attributes\":{\"doi\":\"CoRR, abs/1409.1556\",\"id\":\"b28\"},\"end\":39802,\"start\":39564},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1799558},\"end\":40184,\"start\":39804},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1849990},\"end\":40559,\"start\":40186}]", "bib_title": "[{\"end\":30609,\"start\":30536},{\"end\":30860,\"start\":30825},{\"end\":31226,\"start\":31133},{\"end\":31516,\"start\":31449},{\"end\":32156,\"start\":32100},{\"end\":32467,\"start\":32447},{\"end\":32748,\"start\":32678},{\"end\":32996,\"start\":32936},{\"end\":33345,\"start\":33228},{\"end\":33704,\"start\":33675},{\"end\":33929,\"start\":33873},{\"end\":34198,\"start\":34117},{\"end\":34510,\"start\":34453},{\"end\":34847,\"start\":34764},{\"end\":35200,\"start\":35081},{\"end\":35508,\"start\":35468},{\"end\":35837,\"start\":35774},{\"end\":36189,\"start\":36128},{\"end\":36564,\"start\":36483},{\"end\":37035,\"start\":36974},{\"end\":37465,\"start\":37399},{\"end\":37783,\"start\":37720},{\"end\":38321,\"start\":38285},{\"end\":38637,\"start\":38589},{\"end\":38911,\"start\":38845},{\"end\":39325,\"start\":39223},{\"end\":39864,\"start\":39804},{\"end\":40252,\"start\":40186}]", "bib_author": "[{\"end\":30622,\"start\":30611},{\"end\":30635,\"start\":30622},{\"end\":30875,\"start\":30862},{\"end\":30881,\"start\":30875},{\"end\":30893,\"start\":30881},{\"end\":30908,\"start\":30893},{\"end\":31237,\"start\":31228},{\"end\":31247,\"start\":31237},{\"end\":31530,\"start\":31518},{\"end\":31542,\"start\":31530},{\"end\":31552,\"start\":31542},{\"end\":31561,\"start\":31552},{\"end\":31575,\"start\":31561},{\"end\":31586,\"start\":31575},{\"end\":31861,\"start\":31853},{\"end\":31871,\"start\":31861},{\"end\":31884,\"start\":31871},{\"end\":31893,\"start\":31884},{\"end\":31905,\"start\":31893},{\"end\":32173,\"start\":32158},{\"end\":32184,\"start\":32173},{\"end\":32198,\"start\":32184},{\"end\":32209,\"start\":32198},{\"end\":32220,\"start\":32209},{\"end\":32481,\"start\":32469},{\"end\":32496,\"start\":32481},{\"end\":32760,\"start\":32750},{\"end\":32771,\"start\":32760},{\"end\":32779,\"start\":32771},{\"end\":33006,\"start\":32998},{\"end\":33015,\"start\":33006},{\"end\":33028,\"start\":33015},{\"end\":33357,\"start\":33347},{\"end\":33368,\"start\":33357},{\"end\":33377,\"start\":33368},{\"end\":33389,\"start\":33377},{\"end\":33716,\"start\":33706},{\"end\":33942,\"start\":33931},{\"end\":33952,\"start\":33942},{\"end\":33964,\"start\":33952},{\"end\":33976,\"start\":33964},{\"end\":34208,\"start\":34200},{\"end\":34219,\"start\":34208},{\"end\":34229,\"start\":34219},{\"end\":34239,\"start\":34229},{\"end\":34522,\"start\":34512},{\"end\":34532,\"start\":34522},{\"end\":34543,\"start\":34532},{\"end\":34553,\"start\":34543},{\"end\":34562,\"start\":34553},{\"end\":34867,\"start\":34849},{\"end\":34879,\"start\":34867},{\"end\":35208,\"start\":35202},{\"end\":35219,\"start\":35208},{\"end\":35233,\"start\":35219},{\"end\":35525,\"start\":35510},{\"end\":35536,\"start\":35525},{\"end\":35550,\"start\":35536},{\"end\":35561,\"start\":35550},{\"end\":35853,\"start\":35839},{\"end\":35866,\"start\":35853},{\"end\":35878,\"start\":35866},{\"end\":36197,\"start\":36191},{\"end\":36203,\"start\":36197},{\"end\":36211,\"start\":36203},{\"end\":36217,\"start\":36211},{\"end\":36577,\"start\":36566},{\"end\":36592,\"start\":36577},{\"end\":36606,\"start\":36592},{\"end\":36618,\"start\":36606},{\"end\":36633,\"start\":36618},{\"end\":36643,\"start\":36633},{\"end\":36654,\"start\":36643},{\"end\":37045,\"start\":37037},{\"end\":37059,\"start\":37045},{\"end\":37079,\"start\":37059},{\"end\":37090,\"start\":37079},{\"end\":37099,\"start\":37090},{\"end\":37111,\"start\":37099},{\"end\":37479,\"start\":37467},{\"end\":37492,\"start\":37479},{\"end\":37793,\"start\":37785},{\"end\":37804,\"start\":37793},{\"end\":37814,\"start\":37804},{\"end\":37827,\"start\":37814},{\"end\":37843,\"start\":37827},{\"end\":38103,\"start\":38097},{\"end\":38112,\"start\":38103},{\"end\":38119,\"start\":38112},{\"end\":38132,\"start\":38119},{\"end\":38338,\"start\":38323},{\"end\":38347,\"start\":38338},{\"end\":38360,\"start\":38347},{\"end\":38376,\"start\":38360},{\"end\":38643,\"start\":38639},{\"end\":38661,\"start\":38643},{\"end\":38675,\"start\":38661},{\"end\":38928,\"start\":38913},{\"end\":38939,\"start\":38928},{\"end\":38950,\"start\":38939},{\"end\":38961,\"start\":38950},{\"end\":39334,\"start\":39327},{\"end\":39342,\"start\":39334},{\"end\":39351,\"start\":39342},{\"end\":39644,\"start\":39632},{\"end\":39657,\"start\":39644},{\"end\":39873,\"start\":39866},{\"end\":39886,\"start\":39873},{\"end\":39897,\"start\":39886},{\"end\":39908,\"start\":39897},{\"end\":39916,\"start\":39908},{\"end\":39928,\"start\":39916},{\"end\":39942,\"start\":39928},{\"end\":39953,\"start\":39942},{\"end\":40262,\"start\":40254},{\"end\":40275,\"start\":40262},{\"end\":40283,\"start\":40275},{\"end\":40295,\"start\":40283},{\"end\":40310,\"start\":40295}]", "bib_venue": "[{\"end\":30665,\"start\":30635},{\"end\":30961,\"start\":30908},{\"end\":31280,\"start\":31247},{\"end\":31639,\"start\":31586},{\"end\":31963,\"start\":31920},{\"end\":32263,\"start\":32220},{\"end\":32549,\"start\":32496},{\"end\":32796,\"start\":32792},{\"end\":33063,\"start\":33028},{\"end\":33431,\"start\":33389},{\"end\":33747,\"start\":33716},{\"end\":33983,\"start\":33976},{\"end\":34265,\"start\":34239},{\"end\":34596,\"start\":34562},{\"end\":34912,\"start\":34879},{\"end\":35267,\"start\":35233},{\"end\":35600,\"start\":35561},{\"end\":35931,\"start\":35878},{\"end\":36279,\"start\":36217},{\"end\":36707,\"start\":36654},{\"end\":37164,\"start\":37111},{\"end\":37545,\"start\":37492},{\"end\":37877,\"start\":37843},{\"end\":38095,\"start\":38051},{\"end\":38429,\"start\":38376},{\"end\":38708,\"start\":38675},{\"end\":39024,\"start\":38961},{\"end\":39382,\"start\":39351},{\"end\":39630,\"start\":39564},{\"end\":39981,\"start\":39953},{\"end\":40359,\"start\":40310}]"}}}, "year": 2023, "month": 12, "day": 17}