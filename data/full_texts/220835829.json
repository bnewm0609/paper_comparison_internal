{"id": 220835829, "updated": "2022-06-08 19:20:09.801", "metadata": {"title": "Model-Switching: Dealing with Fluctuating Workloads in Machine-Learning-as-a-Service Systems", "authors": "[{\"first\":\"Sameh\",\"last\":\"Elnikety\",\"middle\":[]},{\"first\":\"Shuayb\",\"last\":\"Zarar\",\"middle\":[]},{\"first\":\"Atul\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Siddharth\",\"last\":\"Garg\",\"middle\":[]}]", "venue": "HotCloud", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Machine learning (ML) based prediction models, and especially deep neural networks (DNNs) are increasingly being served in the cloud in order to provide fast and accurate inferences. However, existing service ML serving systems have trouble dealing with fluctuating workloads and either drop requests or significantly expand hardware resources in response to load spikes. In this paper, we introduce Model-Switching, a new approach to dealing with fluctuating workloads when serving DNN models. Motivated by the observation that endusers of ML primarily care about the accuracy of responses that are returned within the deadline (which we refer to as effective accuracy), we propose to switch from complex and highly accurate DNN models to simpler but less accurate models in the presence of load spikes. We show that the flexibility introduced by enabling online model switching provides higher effective accuracy in the presence of fluctuating workloads compared to serving using any single model. We implement Model-Switching within Clipper, a state-of-art DNN model serving system, and demonstrate its advantages over baseline approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3045045900", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/hotcloud/0001EZGG20", "doi": null}}, "content": {"source": {"pdf_hash": "a7b53bd8e4ae77ba85ec747a85c017a05e8737b9", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d23776a992ecd17462f97f02c442443d06f258c2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a7b53bd8e4ae77ba85ec747a85c017a05e8737b9.txt", "contents": "\nModel-Switching: Dealing with Fluctuating Workloads in Machine-Learning-as-a-Service Systems\n\n\nJeff \nNew York University\nMicrosoft Research\nNew York University\n\n\nJun Zhang \nNew York University\nMicrosoft Research\nNew York University\n\n\nSameh Elnikety \nNew York University\nMicrosoft Research\nNew York University\n\n\nShuayb Zarar \nNew York University\nMicrosoft Research\nNew York University\n\n\nAtul Gupta \nNew York University\nMicrosoft Research\nNew York University\n\n\nMicrosoft Siddharth Garg \nNew York University\nMicrosoft Research\nNew York University\n\n\nModel-Switching: Dealing with Fluctuating Workloads in Machine-Learning-as-a-Service Systems\n\nMachine learning (ML) based prediction models, and especially deep neural networks (DNNs) are increasingly being served in the cloud in order to provide fast and accurate inferences. However, existing service ML serving systems have trouble dealing with fluctuating workloads and either drop requests or significantly expand hardware resources in response to load spikes. In this paper, we introduce Model-Switching, a new approach to dealing with fluctuating workloads when serving DNN models. Motivated by the observation that endusers of ML primarily care about the accuracy of responses that are returned within the deadline (which we refer to as effective accuracy), we propose to switch from complex and highly accurate DNN models to simpler but less accurate models in the presence of load spikes. We show that the flexibility introduced by enabling online model switching provides higher effective accuracy in the presence of fluctuating workloads compared to serving using any single model. We implement Model-Switching within Clipper, a state-of-art DNN model serving system, and demonstrate its advantages over baseline approaches.\n\nIntroduction\n\nDeep neural networks (DNN), currently the state-of-the-art in machine learning (ML), are being increasingly deployed as cloud services to provide highly-accurate inferencing (or predictions) for a range of applications. Systems such as Clipper [10] and Tensorflow serving [31] have been developed to ease the challenges in the deployment, optimization, and maintenance of DNN based machine-learning-as-a-service (MLaaS). Like other cloud services, MLaaS has quality of service (QoS) requirements in the form of service level agreements (SLAs) between the user and the cloud provider that provide guarantees on request latency, throughput and reliability. For ML, however, the prediction accuracy (or simply accuracy) of the model is also a critical metric but has not traditionally been encapsulated in SLAs.\n\nIn this paper, we make two observations to address this gap. First, we observe that for ML workloads, clients are interested not in the fraction of predictions returned within a deadline, but instead the fraction of correct predictions returned within the deadline. We refer to this metric as the effective accuracy; in Section 3, we show that the effective accuracy is the product of the ML model's accuracy and its deadline meet rate. Second, we observe that an SLA specified in terms of effective accuracy enables flexibility in dealing with spikes in workload, for instance, those observed during events like Black Friday.\n\nThe flexibility arises from the fact that deep learning models of varying computational complexity and accuracy can be trained for the same application. We observe that as load increases, the cloud provider can swap out complex and highly accurate models for more computationally efficient models while preserving effective accuracy. We refer to this approach as Model-Switching. From the cloud providers' perspective, Model-Switching allows to make meaningful trade-offs between the computational cost and the service accuracy. For instance, consider a web-serving application, which employs high-performance machine-learning models to recommend relevant items to users or show them ads. In such applications, whenever there is a spike in user load, cloud providers either throttle the serving rate of the application or scale-out computational resources to meet the demand and thus the latency SLA. However, in the former, there is a hit to the throughput SLA (or servable queries-per-second), and in the latter, there is an associated hardware cost for the cloud provider 1 . A third alternative to these options is to guarantee effective accuracy. With this approach, latency and throughput SLAs can always be met at the cost of limited accuracy loss. Without this knob, clients have to either give up on latency or throughput under spiking load.\n\nTo illustrate the benefits of the proposed approach, we develop and evaluate Model-Switching, an online scheduler built on top of Clipper [10] that monitors and adapts to workload fluctuations by switching between a set of pre-trained models for image classification. To further improve efficiency, Model-Switching also optionally determines the optimal number of threads and replicas for each model. Our evaluation shows that Model-Switching yields the highest effective accuracy for all deadline constraints compared with serving with each single model by itself.\n\nThe remainder of the paper is organized as follows: Section 2 provides a brief overview of deep learning and the limitation of existing MLaaS frameworks; the proposed effective accuracy, our new QoS metirc, and the model-switching framework are described in Section 3 followed by results from preliminary evaluation in Section 4. Section 5 describes related work while Section 7 lists the limitations of current approach and future work. Section 6 ends the paper.\n\n\nBackground and Motivation\n\nWe begin by briefly describing the deep learning inference and the limitations of existing MLaaS approaches.\n\n\nDNNs and MLaaS\n\nDNN Basics State-of-the-art DNNs are typically trained using GPU-enabled machine learning frameworks [7,24,32] like PyTorch, TensorFlow, or Caffe to obtain the model weights. Trained models can then be deployed into IoT and embedded devices for inferencing (i.e., to render predictions); in practice, state-of-the-art DNN models can be computationally demanding and hence inference is often outsourced to the cloud.\n\nThe execution time of DNN inference depends on its depth, the size of each layer's feature maps and filters. Fig. 1 shows the execution time for a family of ResNet models with varying depth (for example, ResNet-18 has 18 layers). From the figure, we can observe a more than 5\u00d7 spread in execution times and that more complex, deeper models are also more accurate. (Also shown in the figure is the impact of threadlevel parallelism on each model's execution time, which will be discussed later in Section 3.2.1). MLaaS Framework Several DNN prediction serving systems have been built [10,31,39] to ease the deployment, optimization, and maintenance of DNN inference services. In these systems, DNN models are usually deploy into containers, or servables. State-of-art serving systems also enable model versioning, updates, rollbacks, replication, etc. At runtime, they enable caching, adaptive batching, and ensembling, together with auto-scaling policies [8,9,[16][17][18]30] to sustain QoS and manage hardware resources. Nonetheless, these systems have some drawbacks, described next.\n\n\nLimitation of Exiting MLaaS Framework\n\nDespite recent advances, state-of-the-art MLaaS frameworks still do not perform well in the presence of highly fluctuating loads or load spikes. Existing frameworks like Swayam [18] and Clipper [10] choose to violate SLAs in the presence of load spikes in order to conserve hardware resources. Both systems internally track each request's deadline in the queue and prune (or drop) the request if the queuing delay will result in a deadline miss. As a consequence, Swayam is only able to guarantee that 96% of requests return within the deadline during load spikes, even though the target SLA requires 99% of the jobs to meet deadline. In return, Swayam provides 27% resources savings compared to a baseline that scales hardware resources in response to load spikes. To summarize, existing MLaaS serving systems offer an unappealing trade-off for bursty workloads: either violate SLAs or incur significant hardware overheads. In this paper we show that this trade-off can be averted using the proposed model switching scheme and without the need to scale up resources. \n\n\nModel-Switching: Our Approach\n\nWe start with the new metric, i.e., effective accuracy for MLaaS systems, and then introduce our Model-Switching Framework.\n\n\nEffective Accuracy\n\nWe argue that for MLaaS, response time (latency) alone does not capture end-users' expectation; instead, users are interested in both fast and accurate responses. To this end, we define effective accuracy within a deadline constraint as the fraction of correct (or accurate) predictions returned within the deadline. We will assume that users are agnostic to which model the service provider uses as long as the SLA, specified in terms of effective accuracy, is met. Assume a pre-trained library of M ML models for given task where each model i \u2208 [1, M] is pre-characterized in terms of its accuracy a i and p D,\u03bb i , the fraction of requests that meet deadline D assuming requests arrive at rate \u03bb. Then, the effective accuracy, a e f f i is simply:\na e f f i = p D,\u03bb i * a i .(1)\nNote that the execution time of a DNN is typically fixed and input independent; consequently, deadline misses are statistically independent of mis-classifications, thus enabling us to express the effective accuracy as a product of probabilities.  Figure 2 shows the effective accuracy for five ResNet models with increasing load (requests/second) assuming a deadline of 750 ms. At low loads, the most complex ResNet model, since it has the highest baseline accuracy and, since queuing delay is negligible under low loads, always meets the deadline. However, as load is increased, the simpler models have higher effective accuracy than more complex models because the latter incur a much higher fraction of deadline misses.\n\n\nOnline Model-Switching\n\nBased on the observations above, our online model-switching framework monitors and predicts future job arrivals and switches between models to maximize effective accuracy. In addition, once a model is picked, the framework also selects the optimal number of threads and replicas of the model given the hardware constraints. We begin by discussing the impact of number of threads and model replicas on performance.\n\n\nJob-level and Thread-level Parallelism\n\nDNN model-based microservices, along with other general cloud computing workloads, offer variety of opportunities in terms of parallelism [12,28]. In MLaaS setting, as requests queue up for processing, two decisions can be made to exploit the parallelism at different granularity: (1) How many requests can be serviced in parallel? The answer depends on the number of microservice replicas (R) we have in the system; (2) Once a request is assigned to one of the DNN models, how many threads (T ) should be allocated to this microservice (as shown in Fig. 1)?\n\nIn this paper, we assume fixed capacity C of computing resources (i.e., CPU cores) for serving job requests. (If required, the proposed approach can be easily combined with auto-scaling frameworks that increase C in response to spikes in workload.) Under this assumption, any combination of <R,T > that satisfies R \u00d7 T = C can be chosen.\n\nTo understand the impact of the choice of <R,T > on performance, we deploy several ResNet models in Clipper [10] and measure the end-to-end query latency by varying <R,T > combinations at varying load levels. More information about Clipper and on the experiment set-up can be found in Section 4. Fig. 3 summarizes the 99th percentile (P99) query latency for ResNet-50 and ResNet-152 models for five different <R,T > configurations. It can be observed that judiciously pick <R,T > is necessary; the optimal number of threads T reduces with increasing load. Qualitatively similar results hold for the other ResNet models but are not shown here due to space constraints.  \n\n\nRequest Rate Prediction\n\nIn this paper, we use event-based windowing to monitor load at run-time, and use the load measured in a given window as a predictor for the next window. Clipper records each incoming request's arrival timestamp internally. Our Model-Switching controller estimates the inter-arrival rate by using the youngest and oldest timestamps with a fixed window size periodically.\n\nThe window size can be tuned offline to improve the responsiveness. A similar approach was also used in [35].\n\n\nRule Based Model-Switching\n\nWith pre-characterized information about the P99 latency for each model, and the request-rate estimate, the Model-Switching controller searches over all M models and their <R, T > configurations to pick the model and configuration that has the highest effective accuracy for the specified deadline. Note that doing so maximizes the chances that any given SLA constraint specified in terms of effective accuracy would be met. While our goal of maximizing effective accuracy might create some slack between offered and required service quality, this slack can be exploited by scaling down hardware resources (although we do not explore resource scaling in this paper). Although our Algorithm 1 is general and able to explore dynamic <R,T > allocation, we can see that <R:4,T :4> works effectively on ResNets across the board for most target deadlines, shown in Fig. 3. In our evaluations, we statically pick this configuration and do not consider this problem further in this paper.\n\n\nAdditional Considerations\n\nModel Setup Times ML backend setup incurs large provisioning delays (e.g., a few seconds) due to massive I/O operations. To alleviate this issue, in this work, we pre-deploy all candidate models, relying on the fact that the RAM resources are usually abundant and under-utilized.\n\nSimilar ideas have also been proposed recently in Multi-Tenant Serving system aiming for better resource utilization [29,33]. We also quantify the actual memory cost for hosting 5 models simultaneously in Section 4. More discussion is also available in Section 7. CPU Resource Contention Hosting multiple DNN models may incur CPU performance overhead. However, at each given time, only one model would be required in active mode. To minimize performance impact, we reduce the CPU priority of the remaining \"inactive\" models via the OS level scheduler. We validated this approach and found that with this optimization, the performance is almost the same as only deploying a single model by itself.\n\n\nEvaluation\n\nWe build our Model-Switching controller into Clipper [10], an open source online prediction serving system. To be focus on this study, we disable the cache and dynamic batch size adaption in Clipper.\n\n\nSystem Configuration\n\nWe use a dedicated Azure Virtual Machine (VM) with 32 vCPUs and 128GB of RAM for Clipper model serving and our Model-Switching controller. For the client, we have another separate VM (8 vCPUs and 32GB RAM) to send image queries. We set batch size of 1 when posting the request.\n\nInference Models We primarily look at deep residual nets (ResNets) [22] with various number of layers, baseline accuracy and execution time, shown in Fig. 1. Each model is pre-trained in Pytorch [32] on Imagenet [13], and deployed into container with <R:4,T :4> as microservices (discussed in Section 3.2.1) in Clipper. At any given time, only one model's containers are in the active state.\n\nWorkload Generator The load generator tries to emulate user behavior with a Markov model [11,14]. It operates in an open system model [34], i.e., new jobs arrive independently of job completions and following Poisson inter-arrivals [11,35]. We also validated the generated workload with a trace of job arrivals from a production system deployed in industry.\n\nModel-Switching Controller The controller runs as part of the Clipper serving system with a sample period of 1 second and tracks the most recent incoming queries to the TASK QUEUE to measure load. It then determines and switches to the best model with a target SLA deadline of 750 ms. The deadline is selected to make sure the largest ResNet-152 is a feasible solution at low load. Fig. 4a shows the load profile (queries/second) over a 300second period for the industrial trace and the models selected by the Model-Switching controller during this period. From the figure, we can see the the controller selects the most accurate but also computationally expensive ResNet-152 model when the load is relatively low. For moderate loads, the controller switches between ResNet-101 and ResNet-152. However, when the load spikes at the 125-second mark, the controller can quickly adapt and serves requests using the smaller ResNet-50, ResNet-34, or even ResNet-18 models. Note that all the switching happens in real-time, together with the prediction serving. The results account for all overheads of switching between models.\n\n\nResults\n\nEffective Accuracy Fig. 4b shows the effective accuracy of the proposed Model-Switching controller compared to baselines that make use of a single model only. The results are shown for deadline constraints ranging from 700ms to 1500ms using the same workload trace from Fig. 4a. We observe that since both ResNet-18 and ResNet-34 are fast and never miss deadlines, their effective accuracy are simply equal to their baseline accuracies. The effective accuracy of the larger ResNet-50, -101, and -152 models reduces as the deadline constraints become tighter due to high deadline miss rates. In contrast, Model-Switching yields the highest effective accuracy for all deadline constraints -we note that this  effective accuracy would be unachievable by the smaller models and only achievable by the larger models by introducing additional hardware resources. Tail Latency To better understand how our Model-Switching adapts to load fluctuations, we plot the empirical CDF (Fig. 4c) of end-to-end latency observed by a client submitting requests in Fig. 4a. We compare the percentile latency of Model-Switching with baselines that serve requests using single model each. Several observations from the plot: (1) Small models, such as ResNet-18 and ResNet-34, guarantee that all queries finish within the deadline but have low baseline accuracies. (2) Large and more accurate models (e.g., ResNet-152 and ResNet-101) suffer long tail latency, resulting in several deadline misses. (3) Model-Switching, as it stands, seeks to combine the best of both worlds; i.e., meeting deadlines on the one hand, while serving requests using the most accurate models whenever possible.\n\nCPU and RAM Usage: In all our experiments above, we allocate a total of 16 vCPUs (<R:4,T :4>) for active model containers and limit the CPU utilization for each of the inactive model containers to be less than 1%. 8 vCPUs are configured to handle the client's HTTP POST requests, and the remaining for other Clipper components. The Multi-Tenant ResNet models with a total 20 replicas occupy about 11.8% (15.1GB) of the total system RAM.\n\n\nRelated Work\n\nThere are an increasing number of studies on different aspects of MLaaS. The most relevant for our paper are those that introduce platforms and characterize their performance, and those that optimize QoS and resource allocation. ML Inference Serving Framework A convention way to deploy ML service is to provision containers, or servables to host ML models. Examples include Clipper [10], Tensorflow Serving [31], and Rafiki [39]. These frameworks aim to minimize the cost of deployment, optimization of latency and throughput, and maintenance of DNN based MLaaS. Our work can be easily implemented on top of these existing frameworks with minimal modifications (indeed, we build our proposed solution within Clipper). Auto-scaling Policies Auto-scaling policies are used to guarantee response time SLA while maximizing resource efficiency [3,4,18]. However, in the event of load spikes, these existing auto-scaling policies fail to capture the dynamics in time (since bringing up new hardware resources is time consuming) and increase resource usage. In this paper, we aim to solve this problem without scaling hardware resources but by exploiting model diversity while providing high QoS. Model Accuracy Vs Performance Several works have attempted to optimize the model accuracy and performance. For example, static pruning (compression), quantization, and neural architecture search approaches [25,36,41,43,44] can generate a family of model versions that can be switched during run-time. Input-redundant techniques such as NoScope [27] and Focus [23] are primarily targeted at video queries (where a lot of redundancy exists in the transferred data between frames). In our case, we are targeting image (and presumably textual) queries from different users. In such scenarios, the opportunity to exploit input redundancy may be lower than that in video queries. Other input-dependent cascade methods [37,40], also fit nicely with our model-switching framework wherein classifiers of different complexities could be switched in and out at run-time in response to the work load. Another related work [20] exploits model diversity by exposing latency/accuracy trade-offs to users, while we focus on automatically switching between models to optimize effective accuracy.\n\n\nConclusion\n\nIn this paper, we argue that for MLaaS, the prediction accuracy (or simply accuracy) of the model is also a critical metric but has not traditionally been encapsulated in SLAs. We call for effective accuracy, a new metric, that should be looked at when evaluating the performance of such systems. To achieve a better effective accuracy while serving the prediction requests, Model-Switching has been proposed to dynamically select the best model according to the load and the pre-characterized model performance. We evaluated our framework on a real MLaaS system. Several challenges lie ahead of us before we can achieve our goal of an automatic and low cost Model-Switching controller for MLaaS. Reducing Memory Overheads Containerization disallows any sharing of host machine resources. There is a trade-off between reducing cold start time and the memory resources. A model can be invoked quickly when it is already in memory and does not require a cold start. However, keeping all models in memory at all times is prohibitively expensive and does not scale well. Ideally, we want a method to provide illusion that all models are always warm, while spending resources as if they were always cold. Some work on this can be found in [5,6]. Dynamic Replica and Thread Allocation Currently, we statically set the model replica and thread configuration before the deployment for simplicity. We are exploring more practical ways to implement this allocation online in real time.\n\nIntegration of Exiting Auto-scaling Framework In this work, we assume a fixed capacity C of computing resources (i.e. CPU cores) for backend serving. However, in practice, there may be a certain amount of resources available to scale up. In this setting, the problem of synergistically performing both model-switching and autoscaling remains open. Offline Training-Free Model-Switching Controller Further, since we used the pre-characterized information about the P99 latency for each model at a fixed capacity. The modelswitching controller needs to be retrained if autoscaling policy spawns or revokes some container replicas. As a future work, we are exploring the possibility of training a Reinforcement Learning agent to automatically learning these changes online. Synergistic Optimization with Caching, Batching etc. Existing MLaaS frameworks enable performance optimizations through caching, adaptive batching [10,19] etc. We need to first figure out what optimizations our model-switching is compatible with and also figure out a synergistic way to incorporate model switching into these techniques. Extend to Multiple Types of Computing Resources Although CPUs are widely used for DNN inference in existing MLaaS platforms such as in Facebook [21] and Amazon [2], many specialized hardwares are designed for better DNN inference. Examples are GPU [38], FPGA [15], and Google's TPU [26,42]. These heterogeneous hardware resources open new opportunities to optimize the latency, accuracy, power efficiency and resource efficiency, etc. with a holistic approach.\n\nFigure 1 :\n1Inference time for ResNets in Pytorch [32] on a 32-vCPU host machine.\n\nFigure 2 :\n2Effective accuracy as a function of increasing job arrival rate.\n\nFigure 3 :\n3Tail latency as job arrival rate increases.\n\nAlgorithm 1 :<\n1SLA Deadline-Aware Model-Switching Plan Generation. Data: A fixed capacity of CPU resources, C; target SLA deadline D; intermediate load, Q; a pool of candidate DNN models, M (sorted with descending baseline accuracy). Result: Model index and combination of <R,T > for online prediction serving. 1 index \u2190 M, R \u2190 C; 2 for m \u2208 [1, R, T >\u2190 arg min <R,T >:R\u00d7T =C P99(m, index, < R, T >;\n\nFigure 4 :\n4Results on (a) 5-minute window of trace; (b) Effective accuracy; (c) Empirical CDF of query latency.\nGoogle Cloud ML's documentation is reflective of the current approach: \"If your traffic regularly has steep spikes, and if reliably low latency is important to your application, you may want to consider manual scaling[1].\"\nAcknowledgementsThe authors would like to thank Jonathan Mace, as well as the anonymous reviewers for their time, suggestions, and valuable feedback.\nAI platform prediction. AI platform prediction. https://cloud.google.com/ ai-platform/prediction/docs/overview/. Ac- cessed: 2020-01-15.\n\nAmazon sagemaker ml instance types. Amazon sagemaker ml instance types. https: //aws.amazon.com/sagemaker/pricing/ instance-types/. Accessed: 2019-09-30.\n\nAutoscaling in kubernetes. Autoscaling in kubernetes. https://kubernetes.io/ blog/2016/07/autoscaling-in-kubernetes/. Ac- cessed: 2020-01-15.\n\nDynamic scaling for amazon ec2 auto scaling. Dynamic scaling for amazon ec2 auto scaling. https://docs.aws.amazon.com/autoscaling/ ec2/userguide/as-scale-based-on-demand. html#as-how-scaling-policies-work. Accessed: 2020-01-15.\n\nMikhail Shilkov, cold starts in aws lambda. Mikhail shilkov. cold starts in aws lambda. https: //mikhail.io/serverless/coldstarts/aws/. Ac- cessed: 2020-01-15.\n\ncold starts in azure functions. Mikhail Shilkov, Mikhail shilkov. cold starts in azure functions. https: //mikhail.io/serverless/coldstarts/azure/. Accessed: 2020-01-15.\n\nTensorflow: A system for largescale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, Et Al, 12th USENIX Symposium on Operating Systems Design and Implementation. ABADI, M., BARHAM, P., CHEN, J., CHEN, Z., DAVIS, A., DEAN, J., DEVIN, M., GHEMAWAT, S., IRVING, G., ISARD, M., ET AL. Tensorflow: A system for large- scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16) (2016), pp. 265-283.\n\nApplying reinforcement learning towards automating resource allocation and application scalability in the cloud. E Barrett, E Howley, J Duggan, Concurrency and Computation: Practice and Experience. 25BARRETT, E., HOWLEY, E., AND DUGGAN, J. Apply- ing reinforcement learning towards automating resource allocation and application scalability in the cloud. Con- currency and Computation: Practice and Experience 25, 12 (2013), 1656-1674.\n\nEnergy-aware server provisioning and load dispatching for connection-intensive internet services. G Chen, W He, J Liu, S Nath, L Rigas, L Xiao, F Zhao, NSDI. 8CHEN, G., HE, W., LIU, J., NATH, S., RIGAS, L., XIAO, L., AND ZHAO, F. Energy-aware server pro- visioning and load dispatching for connection-intensive internet services. In NSDI (2008), vol. 8, pp. 337-350.\n\nClipper: A low-latency online prediction serving system. D Crankshaw, X Wang, G Zhou, M J Franklin, J E Gonzalez, I Stoica, 14th USENIX Symposium on Networked Systems Design and Implementation. Boston, MAUSENIX AssociationCRANKSHAW, D., WANG, X., ZHOU, G., FRANKLIN, M. J., GONZALEZ, J. E., AND STOICA, I. Clipper: A low-latency online prediction serving system. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17) (Boston, MA, Mar. 2017), USENIX Association, pp. 613-627.\n\nWorkload generators for web-based systems: Characteristics, current status, and challenges. M Curiel, A Pont, IEEE Communications Surveys & Tutorials. 20CURIEL, M., AND PONT, A. Workload generators for web-based systems: Characteristics, current status, and challenges. IEEE Communications Surveys & Tutorials 20, 2 (2018), 1526-1546.\n\nMapreduce: simplified data processing on large clusters. J Dean, S Ghemawat, Communications of the ACM. 51DEAN, J., AND GHEMAWAT, S. Mapreduce: simplified data processing on large clusters. Communications of the ACM 51, 1 (2008), 107-113.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IEEEDENG, J., DONG, W., SOCHER, R., LI, L.-J., LI, K., AND FEI-FEI, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (2009), IEEE, pp. 248- 255.\n\nThe markov-modulated poisson process (mmpp) cookbook. Performance evaluation. W Fischer, K Meier-Hellstern, 18FISCHER, W., AND MEIER-HELLSTERN, K. The markov-modulated poisson process (mmpp) cookbook. Performance evaluation 18, 2 (1993), 149-171.\n\nA configurable cloud-scale dnn processor for real-time ai. J Fowers, K Ovtcharov, M Papamichael, T Massengill, M Liu, D Lo, S Alkalay, M Haselman, L Adams, M Ghandi, Et Al, ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA). IEEEFOWERS, J., OVTCHAROV, K., PAPAMICHAEL, M., MASSENGILL, T., LIU, M., LO, D., ALKALAY, S., HASELMAN, M., ADAMS, L., GHANDI, M., ET AL. A configurable cloud-scale dnn processor for real-time ai. In ACM/IEEE 45th Annual International Sympo- sium on Computer Architecture (ISCA) (2018), IEEE, pp. 1-14.\n\nAdaptive, model-driven autoscaling for cloud applications. A Gandhi, P Dube, A Karve, A Kochut, L Zhang, 11th International Conference on Autonomic Computing. GANDHI, A., DUBE, P., KARVE, A., KOCHUT, A., AND ZHANG, L. Adaptive, model-driven autoscaling for cloud applications. In 11th International Conference on Autonomic Computing (ICAC 14) (2014), pp. 57-64.\n\nDistributed, robust auto-scaling policies for power management in compute intensive server farms. A Gandhi, M Harchol-Balter, R Raghu-Nathan, M A Kozuch, Sixth Open Cirrus Summit. IEEEGANDHI, A., HARCHOL-BALTER, M., RAGHU- NATHAN, R., AND KOZUCH, M. A. Distributed, robust auto-scaling policies for power management in compute intensive server farms. In 2011 Sixth Open Cirrus Summit (2011), IEEE, pp. 1-5.\n\nSwayam: distributed autoscaling to meet slas of machine learning inference services with resource efficiency. A Gujarati, S Elnikety, Y He, K S Mckinley, B B Brandenburg, Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference. the 18th ACM/IFIP/USENIX Middleware ConferenceGUJARATI, A., ELNIKETY, S., HE, Y., MCKINLEY, K. S., AND BRANDENBURG, B. B. Swayam: dis- tributed autoscaling to meet slas of machine learning inference services with resource efficiency. In Proceed- ings of the 18th ACM/IFIP/USENIX Middleware Con- ference (2017), pp. 109-120.\n\nU Gupta, S Hsia, V Saraph, X Wang, B Reagen, G.-Y Wei, H.-H S Lee, D Brooks, C.-J Wu, Deeprecsys, arXiv:2001.02772A system for optimizing end-to-end at-scale neural recommendation inference. arXiv preprintGUPTA, U., HSIA, S., SARAPH, V., WANG, X., REAGEN, B., WEI, G.-Y., LEE, H.-H. S., BROOKS, D., AND WU, C.-J. Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference. arXiv preprint arXiv:2001.02772 (2020).\n\nOne size does not fit all: Quantifying and exposing the accuracylatency trade-off in machine learning cloud service apis via tolerance tiers. M Halpern, B Boroujerdian, T Mummert, E Duesterwald, V J Reddi, arXiv:1906.11307arXiv preprintHALPERN, M., BOROUJERDIAN, B., MUMMERT, T., DUESTERWALD, E., AND REDDI, V. J. One size does not fit all: Quantifying and exposing the accuracy- latency trade-off in machine learning cloud service apis via tolerance tiers. arXiv preprint arXiv:1906.11307 (2019).\n\nApplied machine learning at facebook: A datacenter infrastructure perspective. K Hazelwood, S Bird, D Brooks, S Chintala, U Diril, D Dzhulgakov, M Fawzy, B Jia, Y Jia, A Kalro, Et Al, 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEEHAZELWOOD, K., BIRD, S., BROOKS, D., CHINTALA, S., DIRIL, U., DZHULGAKOV, D., FAWZY, M., JIA, B., JIA, Y., KALRO, A., ET AL. Applied machine learning at facebook: A datacenter infrastructure perspective. In 2018 IEEE International Symposium on High Perfor- mance Computer Architecture (HPCA) (2018), IEEE, pp. 620-629.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHE, K., ZHANG, X., REN, S., AND SUN, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (2016), pp. 770-778.\n\nFocus: Querying large video datasets with low latency and low cost. K Hsieh, G Ananthanarayanan, P Bodik, S Venkataraman, P Bahl, M Philipose, P B Gib-Bons, O Mutlu, 13th USENIX Symposium on Operating Systems Design and Implementation. HSIEH, K., ANANTHANARAYANAN, G., BODIK, P., VENKATARAMAN, S., BAHL, P., PHILIPOSE, M., GIB- BONS, P. B., AND MUTLU, O. Focus: Querying large video datasets with low latency and low cost. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18) (2018), pp. 269-286.\n\nCaffe: Convolutional architecture for fast feature embedding. Y Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaJIA, Y., SHELHAMER, E., DONAHUE, J., KARAYEV, S., LONG, J., GIRSHICK, R., GUADARRAMA, S., AND DARRELL, T. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia (2014), pp. 675-678.\n\nAccuracy vs. efficiency: Achieving both through fpga-implementation aware neural architecture search. W Jiang, X Zhang, E H Sha, .-M Yang, L Zhuge, Q Shi, Y Hu, J , Proceedings of the 56th Annual Design Automation Conference. the 56th Annual Design Automation ConferenceACM5JIANG, W., ZHANG, X., SHA, E. H.-M., YANG, L., ZHUGE, Q., SHI, Y., AND HU, J. Accuracy vs. effi- ciency: Achieving both through fpga-implementation aware neural architecture search. In Proceedings of the 56th Annual Design Automation Conference (2019), ACM, p. 5.\n\nIn-datacenter performance analysis of a tensor processing unit. N P Jouppi, C Young, N Patil, D Patterson, G Agrawal, R Bajwa, S Bates, S Bhatia, N Boden, A Borchers, Et Al, Proceedings of the 44th Annual International Symposium on Computer Architecture. the 44th Annual International Symposium on Computer ArchitectureJOUPPI, N. P., YOUNG, C., PATIL, N., PATTERSON, D., AGRAWAL, G., BAJWA, R., BATES, S., BHATIA, S., BODEN, N., BORCHERS, A., ET AL. In-datacenter performance analysis of a tensor processing unit. In Pro- ceedings of the 44th Annual International Symposium on Computer Architecture (2017), pp. 1-12.\n\nNoscope: optimizing neural network queries over video at scale. D Kang, J Emmons, F Abuzaid, P Bailis, M Zaharia, arXiv:1703.02529arXiv preprintKANG, D., EMMONS, J., ABUZAID, F., BAILIS, P., AND ZAHARIA, M. Noscope: optimizing neural net- work queries over video at scale. arXiv preprint arXiv:1703.02529 (2017).\n\nA Krizhevsky, arXiv:1404.5997One weird trick for parallelizing convolutional neural networks. arXiv preprintKRIZHEVSKY, A. One weird trick for paralleliz- ing convolutional neural networks. arXiv preprint arXiv:1404.5997 (2014).\n\nPerseus: Characterizing performance and cost of multi-tenant serving for cnn models. M Lemay, S Li, T Guo, arXiv:1912.02322arXiv preprintLEMAY, M., LI, S., AND GUO, T. Perseus: Character- izing performance and cost of multi-tenant serving for cnn models. arXiv preprint arXiv:1912.02322 (2019).\n\nCloud autoscaling with deadline and budget constraints. M Mao, J Li, M Humphrey, 11th IEEE/ACM International Conference on Grid Computing. IEEEMAO, M., LI, J., AND HUMPHREY, M. Cloud auto- scaling with deadline and budget constraints. In 11th IEEE/ACM International Conference on Grid Comput- ing (2010), IEEE, pp. 41-48.\n\nTensorflow-serving: Flexible, high-performance ml serving. C Olston, N Fiedel, K Gorovoy, J Harmsen, L Lao, F Li, V Rajashekhar, S Ramesh, J Soyke, arXiv:1712.06139arXiv preprintOLSTON, C., FIEDEL, N., GOROVOY, K., HARMSEN, J., LAO, L., LI, F., RAJASHEKHAR, V., RAMESH, S., AND SOYKE, J. Tensorflow-serving: Flexi- ble, high-performance ml serving. arXiv preprint arXiv:1712.06139 (2017).\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Et Al, Advances in Neural Information Processing Systems. PASZKE, A., GROSS, S., MASSA, F., LERER, A., BRADBURY, J., CHANAN, G., KILLEEN, T., LIN, Z., GIMELSHEIN, N., ANTIGA, L., ET AL. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (2019), pp. 8024-8035.\n\nNo dnn left behind: Improving inference in the cloud with multi-tenancy. A Samanta, S Shrinivasan, A Kaufmann, J Mace, arXiv:1901.06887arXiv preprintSAMANTA, A., SHRINIVASAN, S., KAUFMANN, A., AND MACE, J. No dnn left behind: Improving infer- ence in the cloud with multi-tenancy. arXiv preprint arXiv:1901.06887 (2019).\n\nOpen versus closed: A cautionary tale. B Schroeder, A Wierman, M Harchol-Balter, SCHROEDER, B., WIERMAN, A., AND HARCHOL- BALTER, M. Open versus closed: A cautionary tale. USENIX.\n\nAutotuned threading for {OLDI} microservices. A Sriraman, T F Wenisch, \u039ctune, 13th USENIX Symposium on Operating Systems Design and Implementation. SRIRAMAN, A., AND WENISCH, T. F. \u00b5tune: Auto- tuned threading for {OLDI} microservices. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSD 18) (2018), pp. 177-194.\n\nAccelerating deep convolutional networks using lowprecision and sparsity. G Venkatesh, E Nurvitadhi, D Marr, 2017 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEVENKATESH, G., NURVITADHI, E., AND MARR, D. Accelerating deep convolutional networks using low- precision and sparsity. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2017), IEEE, pp. 2861-2865.\n\nRapid object detection using a boosted cascade of simple features. P Viola, M Jones, Proceedings of the IEEE computer society conference on computer vision and pattern recognition. CVPR 2001. the IEEE computer society conference on computer vision and pattern recognition. CVPR 2001IEEE1VIOLA, P., AND JONES, M. Rapid object detection using a boosted cascade of simple features. In Pro- ceedings of the IEEE computer society conference on computer vision and pattern recognition. CVPR 2001 (2001), vol. 1, IEEE, pp. I-I.\n\nBlasx: A high performance level-3 blas library for heterogeneous multi-gpu computing. L Wang, W Wu, Z Xu, J Xiao, Y Yang, Proceedings of the 2016 International Conference on Supercomputing. the 2016 International Conference on SupercomputingWANG, L., WU, W., XU, Z., XIAO, J., AND YANG, Y. Blasx: A high performance level-3 blas library for heterogeneous multi-gpu computing. In Proceedings of the 2016 International Conference on Supercomputing (2016), pp. 1-11.\n\nRafiki: machine learning as an analytics service system. W Wang, J Gao, M Zhang, S Wang, G Chen, T K Ng, B C Ooi, J Shao, M Reyad, Proceedings of the VLDB Endowment. the VLDB Endowment12WANG, W., GAO, J., ZHANG, M., WANG, S., CHEN, G., NG, T. K., OOI, B. C., SHAO, J., AND REYAD, M. Rafiki: machine learning as an analytics service system. Proceedings of the VLDB Endowment 12, 2 (2018), 128-140.\n\nIdk cascades: Fast deep learning by learning not to overthink. X Wang, Y Luo, D Crankshaw, A Tumanov, F Yu, J E Gonzalez, arXiv:1706.00885arXiv preprintWANG, X., LUO, Y., CRANKSHAW, D., TUMANOV, A., YU, F., AND GONZALEZ, J. E. Idk cascades: Fast deep learning by learning not to overthink. arXiv preprint arXiv:1706.00885 (2017).\n\nCoexploration of neural architectures and heterogeneous asic accelerator designs targeting multiple tasks. L Yang, Z Yan, M Li, H Kwon, L Lai, T Kr-Ishna, V Chandra, W Jiang, Y Shi, Proceedings of the 57th Annual Design Automation ConferenceYANG, L., YAN, Z., LI, M., KWON, H., LAI, L., KR- ISHNA, T., CHANDRA, V., JIANG, W., AND SHI, Y. Co- exploration of neural architectures and heterogeneous asic accelerator designs targeting multiple tasks. Pro- ceedings of the 57th Annual Design Automation Confer- ence (2020).\n\nEnabling aggressive voltage underscaling and timing error resilience for energy efficient deep learning accelerators. J Zhang, K Rangineni, Z Ghodsi, S Garg, Thundervolt, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceZHANG, J., RANGINENI, K., GHODSI, Z., AND GARG, S. Thundervolt: Enabling aggressive voltage underscal- ing and timing error resilience for energy efficient deep learning accelerators. In Proceedings of the 55th Annual Design Automation Conference (2018).\n\nOn-chip compression of activations for low power systolic array based cnn acceleration. J J Zhang, P Raj, S Zarar, A Ambardekar, S Garg, Compact, ACM Trans. Embed. Comput. Syst. 185ZHANG, J. J., RAJ, P., ZARAR, S., AMBARDEKAR, A., AND GARG, S. Compact: On-chip compression of activations for low power systolic array based cnn acceleration. ACM Trans. Embed. Comput. Syst. 18, 5s (Oct. 2019).\n\nA systematic dnn weight pruning framework using alternating direction method of multipliers. T Zhang, S Ye, K Zhang, J Tang, W Wen, M Fardad, Y Wang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)ZHANG, T., YE, S., ZHANG, K., TANG, J., WEN, W., FARDAD, M., AND WANG, Y. A systematic dnn weight pruning framework using alternating direction method of multipliers. In Proceedings of the European Confer- ence on Computer Vision (ECCV) (2018), pp. 184-199.\n", "annotations": {"author": "[{\"end\":162,\"start\":96},{\"end\":234,\"start\":163},{\"end\":311,\"start\":235},{\"end\":386,\"start\":312},{\"end\":459,\"start\":387},{\"end\":546,\"start\":460}]", "publisher": null, "author_last_name": "[{\"end\":172,\"start\":167},{\"end\":249,\"start\":241},{\"end\":324,\"start\":319},{\"end\":397,\"start\":392},{\"end\":484,\"start\":480}]", "author_first_name": "[{\"end\":100,\"start\":96},{\"end\":166,\"start\":163},{\"end\":240,\"start\":235},{\"end\":318,\"start\":312},{\"end\":391,\"start\":387},{\"end\":469,\"start\":460},{\"end\":479,\"start\":470}]", "author_affiliation": "[{\"end\":161,\"start\":102},{\"end\":233,\"start\":174},{\"end\":310,\"start\":251},{\"end\":385,\"start\":326},{\"end\":458,\"start\":399},{\"end\":545,\"start\":486}]", "title": "[{\"end\":93,\"start\":1},{\"end\":639,\"start\":547}]", "venue": null, "abstract": "[{\"end\":1783,\"start\":641}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2047,\"start\":2043},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2075,\"start\":2071},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4731,\"start\":4727},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5880,\"start\":5877},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5883,\"start\":5880},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5886,\"start\":5883},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6780,\"start\":6776},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6783,\"start\":6780},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6786,\"start\":6783},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7151,\"start\":7148},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7153,\"start\":7151},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7157,\"start\":7153},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7161,\"start\":7157},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7165,\"start\":7161},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7167,\"start\":7165},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7501,\"start\":7497},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7518,\"start\":7514},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10697,\"start\":10693},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10700,\"start\":10697},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11566,\"start\":11562},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12630,\"start\":12626},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14074,\"start\":14070},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14077,\"start\":14074},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14721,\"start\":14717},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15238,\"start\":15234},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15366,\"start\":15362},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15383,\"start\":15379},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15653,\"start\":15649},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15656,\"start\":15653},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15698,\"start\":15694},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15796,\"start\":15792},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15799,\"start\":15796},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18398,\"start\":18395},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19560,\"start\":19556},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19585,\"start\":19581},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19602,\"start\":19598},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20016,\"start\":20013},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20018,\"start\":20016},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20021,\"start\":20018},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20574,\"start\":20570},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20577,\"start\":20574},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20580,\"start\":20577},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20583,\"start\":20580},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20586,\"start\":20583},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20712,\"start\":20708},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20727,\"start\":20723},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21080,\"start\":21076},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":21083,\"start\":21080},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21278,\"start\":21274},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22694,\"start\":22691},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":22696,\"start\":22694},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23856,\"start\":23852},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23859,\"start\":23856},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24191,\"start\":24187},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24206,\"start\":24203},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24295,\"start\":24291},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24306,\"start\":24302},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24329,\"start\":24325},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24332,\"start\":24329},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25457,\"start\":25454}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24586,\"start\":24504},{\"attributes\":{\"id\":\"fig_1\"},\"end\":24664,\"start\":24587},{\"attributes\":{\"id\":\"fig_3\"},\"end\":24721,\"start\":24665},{\"attributes\":{\"id\":\"fig_4\"},\"end\":25122,\"start\":24722},{\"attributes\":{\"id\":\"fig_6\"},\"end\":25236,\"start\":25123}]", "paragraph": "[{\"end\":2607,\"start\":1799},{\"end\":3235,\"start\":2609},{\"end\":4587,\"start\":3237},{\"end\":5154,\"start\":4589},{\"end\":5619,\"start\":5156},{\"end\":5757,\"start\":5649},{\"end\":6191,\"start\":5776},{\"end\":7278,\"start\":6193},{\"end\":8388,\"start\":7320},{\"end\":8545,\"start\":8422},{\"end\":9318,\"start\":8568},{\"end\":10072,\"start\":9350},{\"end\":10512,\"start\":10099},{\"end\":11113,\"start\":10555},{\"end\":11452,\"start\":11115},{\"end\":12123,\"start\":11454},{\"end\":12520,\"start\":12151},{\"end\":12631,\"start\":12522},{\"end\":13642,\"start\":12662},{\"end\":13951,\"start\":13672},{\"end\":14649,\"start\":13953},{\"end\":14863,\"start\":14664},{\"end\":15165,\"start\":14888},{\"end\":15558,\"start\":15167},{\"end\":15917,\"start\":15560},{\"end\":17040,\"start\":15919},{\"end\":18718,\"start\":17052},{\"end\":19156,\"start\":18720},{\"end\":21442,\"start\":19173},{\"end\":22932,\"start\":21457},{\"end\":24503,\"start\":22934}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9349,\"start\":9319}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1797,\"start\":1785},{\"attributes\":{\"n\":\"2\"},\"end\":5647,\"start\":5622},{\"attributes\":{\"n\":\"2.1\"},\"end\":5774,\"start\":5760},{\"attributes\":{\"n\":\"2.2\"},\"end\":7318,\"start\":7281},{\"attributes\":{\"n\":\"3\"},\"end\":8420,\"start\":8391},{\"attributes\":{\"n\":\"3.1\"},\"end\":8566,\"start\":8548},{\"attributes\":{\"n\":\"3.2\"},\"end\":10097,\"start\":10075},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":10553,\"start\":10515},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":12149,\"start\":12126},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":12660,\"start\":12634},{\"attributes\":{\"n\":\"3.2.4\"},\"end\":13670,\"start\":13645},{\"attributes\":{\"n\":\"4\"},\"end\":14662,\"start\":14652},{\"end\":14886,\"start\":14866},{\"attributes\":{\"n\":\"4.1\"},\"end\":17050,\"start\":17043},{\"attributes\":{\"n\":\"5\"},\"end\":19171,\"start\":19159},{\"attributes\":{\"n\":\"6\"},\"end\":21455,\"start\":21445},{\"end\":24515,\"start\":24505},{\"end\":24598,\"start\":24588},{\"end\":24676,\"start\":24666},{\"end\":24737,\"start\":24723},{\"end\":25134,\"start\":25124}]", "table": null, "figure_caption": "[{\"end\":24586,\"start\":24517},{\"end\":24664,\"start\":24600},{\"end\":24721,\"start\":24678},{\"end\":25122,\"start\":24739},{\"end\":25236,\"start\":25136}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6308,\"start\":6302},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9605,\"start\":9597},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11112,\"start\":11105},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11756,\"start\":11750},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13527,\"start\":13521},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15323,\"start\":15317},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":16308,\"start\":16301},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":17078,\"start\":17071},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":17329,\"start\":17322},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":18031,\"start\":18022},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":18105,\"start\":18098}]", "bib_author_first_name": "[{\"end\":26282,\"start\":26275},{\"end\":26475,\"start\":26468},{\"end\":26662,\"start\":26661},{\"end\":26671,\"start\":26670},{\"end\":26681,\"start\":26680},{\"end\":26689,\"start\":26688},{\"end\":26697,\"start\":26696},{\"end\":26706,\"start\":26705},{\"end\":26714,\"start\":26713},{\"end\":26723,\"start\":26722},{\"end\":26735,\"start\":26734},{\"end\":26745,\"start\":26744},{\"end\":27223,\"start\":27222},{\"end\":27234,\"start\":27233},{\"end\":27244,\"start\":27243},{\"end\":27645,\"start\":27644},{\"end\":27653,\"start\":27652},{\"end\":27659,\"start\":27658},{\"end\":27666,\"start\":27665},{\"end\":27674,\"start\":27673},{\"end\":27683,\"start\":27682},{\"end\":27691,\"start\":27690},{\"end\":27972,\"start\":27971},{\"end\":27985,\"start\":27984},{\"end\":27993,\"start\":27992},{\"end\":28001,\"start\":28000},{\"end\":28003,\"start\":28002},{\"end\":28015,\"start\":28014},{\"end\":28017,\"start\":28016},{\"end\":28029,\"start\":28028},{\"end\":28511,\"start\":28510},{\"end\":28521,\"start\":28520},{\"end\":28812,\"start\":28811},{\"end\":28820,\"start\":28819},{\"end\":29048,\"start\":29047},{\"end\":29056,\"start\":29055},{\"end\":29064,\"start\":29063},{\"end\":29077,\"start\":29073},{\"end\":29083,\"start\":29082},{\"end\":29089,\"start\":29088},{\"end\":29463,\"start\":29462},{\"end\":29474,\"start\":29473},{\"end\":29692,\"start\":29691},{\"end\":29702,\"start\":29701},{\"end\":29715,\"start\":29714},{\"end\":29730,\"start\":29729},{\"end\":29744,\"start\":29743},{\"end\":29751,\"start\":29750},{\"end\":29757,\"start\":29756},{\"end\":29768,\"start\":29767},{\"end\":29780,\"start\":29779},{\"end\":29789,\"start\":29788},{\"end\":30247,\"start\":30246},{\"end\":30257,\"start\":30256},{\"end\":30265,\"start\":30264},{\"end\":30274,\"start\":30273},{\"end\":30284,\"start\":30283},{\"end\":30649,\"start\":30648},{\"end\":30659,\"start\":30658},{\"end\":30677,\"start\":30676},{\"end\":30693,\"start\":30692},{\"end\":30695,\"start\":30694},{\"end\":31069,\"start\":31068},{\"end\":31081,\"start\":31080},{\"end\":31093,\"start\":31092},{\"end\":31099,\"start\":31098},{\"end\":31101,\"start\":31100},{\"end\":31113,\"start\":31112},{\"end\":31115,\"start\":31114},{\"end\":31518,\"start\":31517},{\"end\":31527,\"start\":31526},{\"end\":31535,\"start\":31534},{\"end\":31545,\"start\":31544},{\"end\":31553,\"start\":31552},{\"end\":31566,\"start\":31562},{\"end\":31576,\"start\":31572},{\"end\":31578,\"start\":31577},{\"end\":31585,\"start\":31584},{\"end\":31598,\"start\":31594},{\"end\":32103,\"start\":32102},{\"end\":32114,\"start\":32113},{\"end\":32130,\"start\":32129},{\"end\":32141,\"start\":32140},{\"end\":32156,\"start\":32155},{\"end\":32158,\"start\":32157},{\"end\":32539,\"start\":32538},{\"end\":32552,\"start\":32551},{\"end\":32560,\"start\":32559},{\"end\":32570,\"start\":32569},{\"end\":32582,\"start\":32581},{\"end\":32591,\"start\":32590},{\"end\":32605,\"start\":32604},{\"end\":32614,\"start\":32613},{\"end\":32621,\"start\":32620},{\"end\":32628,\"start\":32627},{\"end\":33098,\"start\":33097},{\"end\":33104,\"start\":33103},{\"end\":33113,\"start\":33112},{\"end\":33120,\"start\":33119},{\"end\":33525,\"start\":33524},{\"end\":33534,\"start\":33533},{\"end\":33554,\"start\":33553},{\"end\":33563,\"start\":33562},{\"end\":33579,\"start\":33578},{\"end\":33587,\"start\":33586},{\"end\":33600,\"start\":33599},{\"end\":33602,\"start\":33601},{\"end\":33614,\"start\":33613},{\"end\":34046,\"start\":34045},{\"end\":34053,\"start\":34052},{\"end\":34066,\"start\":34065},{\"end\":34077,\"start\":34076},{\"end\":34088,\"start\":34087},{\"end\":34096,\"start\":34095},{\"end\":34108,\"start\":34107},{\"end\":34122,\"start\":34121},{\"end\":34614,\"start\":34613},{\"end\":34623,\"start\":34622},{\"end\":34632,\"start\":34631},{\"end\":34634,\"start\":34633},{\"end\":34643,\"start\":34640},{\"end\":34651,\"start\":34650},{\"end\":34660,\"start\":34659},{\"end\":34667,\"start\":34666},{\"end\":34673,\"start\":34672},{\"end\":35115,\"start\":35114},{\"end\":35117,\"start\":35116},{\"end\":35127,\"start\":35126},{\"end\":35136,\"start\":35135},{\"end\":35145,\"start\":35144},{\"end\":35158,\"start\":35157},{\"end\":35169,\"start\":35168},{\"end\":35178,\"start\":35177},{\"end\":35187,\"start\":35186},{\"end\":35197,\"start\":35196},{\"end\":35206,\"start\":35205},{\"end\":35733,\"start\":35732},{\"end\":35741,\"start\":35740},{\"end\":35751,\"start\":35750},{\"end\":35762,\"start\":35761},{\"end\":35772,\"start\":35771},{\"end\":35983,\"start\":35982},{\"end\":36298,\"start\":36297},{\"end\":36307,\"start\":36306},{\"end\":36313,\"start\":36312},{\"end\":36565,\"start\":36564},{\"end\":36572,\"start\":36571},{\"end\":36578,\"start\":36577},{\"end\":36891,\"start\":36890},{\"end\":36901,\"start\":36900},{\"end\":36911,\"start\":36910},{\"end\":36922,\"start\":36921},{\"end\":36933,\"start\":36932},{\"end\":36940,\"start\":36939},{\"end\":36946,\"start\":36945},{\"end\":36961,\"start\":36960},{\"end\":36971,\"start\":36970},{\"end\":37292,\"start\":37291},{\"end\":37302,\"start\":37301},{\"end\":37311,\"start\":37310},{\"end\":37320,\"start\":37319},{\"end\":37329,\"start\":37328},{\"end\":37341,\"start\":37340},{\"end\":37351,\"start\":37350},{\"end\":37362,\"start\":37361},{\"end\":37369,\"start\":37368},{\"end\":37383,\"start\":37382},{\"end\":37799,\"start\":37798},{\"end\":37810,\"start\":37809},{\"end\":37825,\"start\":37824},{\"end\":37837,\"start\":37836},{\"end\":38087,\"start\":38086},{\"end\":38100,\"start\":38099},{\"end\":38111,\"start\":38110},{\"end\":38275,\"start\":38274},{\"end\":38287,\"start\":38286},{\"end\":38289,\"start\":38288},{\"end\":38642,\"start\":38641},{\"end\":38655,\"start\":38654},{\"end\":38669,\"start\":38668},{\"end\":39067,\"start\":39066},{\"end\":39076,\"start\":39075},{\"end\":39608,\"start\":39607},{\"end\":39616,\"start\":39615},{\"end\":39622,\"start\":39621},{\"end\":39628,\"start\":39627},{\"end\":39636,\"start\":39635},{\"end\":40044,\"start\":40043},{\"end\":40052,\"start\":40051},{\"end\":40059,\"start\":40058},{\"end\":40068,\"start\":40067},{\"end\":40076,\"start\":40075},{\"end\":40084,\"start\":40083},{\"end\":40086,\"start\":40085},{\"end\":40092,\"start\":40091},{\"end\":40094,\"start\":40093},{\"end\":40101,\"start\":40100},{\"end\":40109,\"start\":40108},{\"end\":40448,\"start\":40447},{\"end\":40456,\"start\":40455},{\"end\":40463,\"start\":40462},{\"end\":40476,\"start\":40475},{\"end\":40487,\"start\":40486},{\"end\":40493,\"start\":40492},{\"end\":40495,\"start\":40494},{\"end\":40823,\"start\":40822},{\"end\":40831,\"start\":40830},{\"end\":40838,\"start\":40837},{\"end\":40844,\"start\":40843},{\"end\":40852,\"start\":40851},{\"end\":40859,\"start\":40858},{\"end\":40871,\"start\":40870},{\"end\":40882,\"start\":40881},{\"end\":40891,\"start\":40890},{\"end\":41354,\"start\":41353},{\"end\":41363,\"start\":41362},{\"end\":41376,\"start\":41375},{\"end\":41386,\"start\":41385},{\"end\":41856,\"start\":41855},{\"end\":41858,\"start\":41857},{\"end\":41867,\"start\":41866},{\"end\":41874,\"start\":41873},{\"end\":41883,\"start\":41882},{\"end\":41897,\"start\":41896},{\"end\":42255,\"start\":42254},{\"end\":42264,\"start\":42263},{\"end\":42270,\"start\":42269},{\"end\":42279,\"start\":42278},{\"end\":42287,\"start\":42286},{\"end\":42294,\"start\":42293},{\"end\":42304,\"start\":42303}]", "bib_author_last_name": "[{\"end\":26290,\"start\":26283},{\"end\":26483,\"start\":26476},{\"end\":26668,\"start\":26663},{\"end\":26678,\"start\":26672},{\"end\":26686,\"start\":26682},{\"end\":26694,\"start\":26690},{\"end\":26703,\"start\":26698},{\"end\":26711,\"start\":26707},{\"end\":26720,\"start\":26715},{\"end\":26732,\"start\":26724},{\"end\":26742,\"start\":26736},{\"end\":26751,\"start\":26746},{\"end\":26758,\"start\":26753},{\"end\":27231,\"start\":27224},{\"end\":27241,\"start\":27235},{\"end\":27251,\"start\":27245},{\"end\":27650,\"start\":27646},{\"end\":27656,\"start\":27654},{\"end\":27663,\"start\":27660},{\"end\":27671,\"start\":27667},{\"end\":27680,\"start\":27675},{\"end\":27688,\"start\":27684},{\"end\":27696,\"start\":27692},{\"end\":27982,\"start\":27973},{\"end\":27990,\"start\":27986},{\"end\":27998,\"start\":27994},{\"end\":28012,\"start\":28004},{\"end\":28026,\"start\":28018},{\"end\":28036,\"start\":28030},{\"end\":28518,\"start\":28512},{\"end\":28526,\"start\":28522},{\"end\":28817,\"start\":28813},{\"end\":28829,\"start\":28821},{\"end\":29053,\"start\":29049},{\"end\":29061,\"start\":29057},{\"end\":29071,\"start\":29065},{\"end\":29080,\"start\":29078},{\"end\":29086,\"start\":29084},{\"end\":29097,\"start\":29090},{\"end\":29471,\"start\":29464},{\"end\":29490,\"start\":29475},{\"end\":29699,\"start\":29693},{\"end\":29712,\"start\":29703},{\"end\":29727,\"start\":29716},{\"end\":29741,\"start\":29731},{\"end\":29748,\"start\":29745},{\"end\":29754,\"start\":29752},{\"end\":29765,\"start\":29758},{\"end\":29777,\"start\":29769},{\"end\":29786,\"start\":29781},{\"end\":29796,\"start\":29790},{\"end\":29803,\"start\":29798},{\"end\":30254,\"start\":30248},{\"end\":30262,\"start\":30258},{\"end\":30271,\"start\":30266},{\"end\":30281,\"start\":30275},{\"end\":30290,\"start\":30285},{\"end\":30656,\"start\":30650},{\"end\":30674,\"start\":30660},{\"end\":30690,\"start\":30678},{\"end\":30702,\"start\":30696},{\"end\":31078,\"start\":31070},{\"end\":31090,\"start\":31082},{\"end\":31096,\"start\":31094},{\"end\":31110,\"start\":31102},{\"end\":31127,\"start\":31116},{\"end\":31524,\"start\":31519},{\"end\":31532,\"start\":31528},{\"end\":31542,\"start\":31536},{\"end\":31550,\"start\":31546},{\"end\":31560,\"start\":31554},{\"end\":31570,\"start\":31567},{\"end\":31582,\"start\":31579},{\"end\":31592,\"start\":31586},{\"end\":31601,\"start\":31599},{\"end\":31613,\"start\":31603},{\"end\":32111,\"start\":32104},{\"end\":32127,\"start\":32115},{\"end\":32138,\"start\":32131},{\"end\":32153,\"start\":32142},{\"end\":32164,\"start\":32159},{\"end\":32549,\"start\":32540},{\"end\":32557,\"start\":32553},{\"end\":32567,\"start\":32561},{\"end\":32579,\"start\":32571},{\"end\":32588,\"start\":32583},{\"end\":32602,\"start\":32592},{\"end\":32611,\"start\":32606},{\"end\":32618,\"start\":32615},{\"end\":32625,\"start\":32622},{\"end\":32634,\"start\":32629},{\"end\":32641,\"start\":32636},{\"end\":33101,\"start\":33099},{\"end\":33110,\"start\":33105},{\"end\":33117,\"start\":33114},{\"end\":33124,\"start\":33121},{\"end\":33531,\"start\":33526},{\"end\":33551,\"start\":33535},{\"end\":33560,\"start\":33555},{\"end\":33576,\"start\":33564},{\"end\":33584,\"start\":33580},{\"end\":33597,\"start\":33588},{\"end\":33611,\"start\":33603},{\"end\":33620,\"start\":33615},{\"end\":34050,\"start\":34047},{\"end\":34063,\"start\":34054},{\"end\":34074,\"start\":34067},{\"end\":34085,\"start\":34078},{\"end\":34093,\"start\":34089},{\"end\":34105,\"start\":34097},{\"end\":34119,\"start\":34109},{\"end\":34130,\"start\":34123},{\"end\":34620,\"start\":34615},{\"end\":34629,\"start\":34624},{\"end\":34638,\"start\":34635},{\"end\":34648,\"start\":34644},{\"end\":34657,\"start\":34652},{\"end\":34664,\"start\":34661},{\"end\":34670,\"start\":34668},{\"end\":35124,\"start\":35118},{\"end\":35133,\"start\":35128},{\"end\":35142,\"start\":35137},{\"end\":35155,\"start\":35146},{\"end\":35166,\"start\":35159},{\"end\":35175,\"start\":35170},{\"end\":35184,\"start\":35179},{\"end\":35194,\"start\":35188},{\"end\":35203,\"start\":35198},{\"end\":35215,\"start\":35207},{\"end\":35222,\"start\":35217},{\"end\":35738,\"start\":35734},{\"end\":35748,\"start\":35742},{\"end\":35759,\"start\":35752},{\"end\":35769,\"start\":35763},{\"end\":35780,\"start\":35773},{\"end\":35994,\"start\":35984},{\"end\":36304,\"start\":36299},{\"end\":36310,\"start\":36308},{\"end\":36317,\"start\":36314},{\"end\":36569,\"start\":36566},{\"end\":36575,\"start\":36573},{\"end\":36587,\"start\":36579},{\"end\":36898,\"start\":36892},{\"end\":36908,\"start\":36902},{\"end\":36919,\"start\":36912},{\"end\":36930,\"start\":36923},{\"end\":36937,\"start\":36934},{\"end\":36943,\"start\":36941},{\"end\":36958,\"start\":36947},{\"end\":36968,\"start\":36962},{\"end\":36977,\"start\":36972},{\"end\":37299,\"start\":37293},{\"end\":37308,\"start\":37303},{\"end\":37317,\"start\":37312},{\"end\":37326,\"start\":37321},{\"end\":37338,\"start\":37330},{\"end\":37348,\"start\":37342},{\"end\":37359,\"start\":37352},{\"end\":37366,\"start\":37363},{\"end\":37380,\"start\":37370},{\"end\":37390,\"start\":37384},{\"end\":37397,\"start\":37392},{\"end\":37807,\"start\":37800},{\"end\":37822,\"start\":37811},{\"end\":37834,\"start\":37826},{\"end\":37842,\"start\":37838},{\"end\":38097,\"start\":38088},{\"end\":38108,\"start\":38101},{\"end\":38126,\"start\":38112},{\"end\":38284,\"start\":38276},{\"end\":38297,\"start\":38290},{\"end\":38304,\"start\":38299},{\"end\":38652,\"start\":38643},{\"end\":38666,\"start\":38656},{\"end\":38674,\"start\":38670},{\"end\":39073,\"start\":39068},{\"end\":39082,\"start\":39077},{\"end\":39613,\"start\":39609},{\"end\":39619,\"start\":39617},{\"end\":39625,\"start\":39623},{\"end\":39633,\"start\":39629},{\"end\":39641,\"start\":39637},{\"end\":40049,\"start\":40045},{\"end\":40056,\"start\":40053},{\"end\":40065,\"start\":40060},{\"end\":40073,\"start\":40069},{\"end\":40081,\"start\":40077},{\"end\":40089,\"start\":40087},{\"end\":40098,\"start\":40095},{\"end\":40106,\"start\":40102},{\"end\":40115,\"start\":40110},{\"end\":40453,\"start\":40449},{\"end\":40460,\"start\":40457},{\"end\":40473,\"start\":40464},{\"end\":40484,\"start\":40477},{\"end\":40490,\"start\":40488},{\"end\":40504,\"start\":40496},{\"end\":40828,\"start\":40824},{\"end\":40835,\"start\":40832},{\"end\":40841,\"start\":40839},{\"end\":40849,\"start\":40845},{\"end\":40856,\"start\":40853},{\"end\":40868,\"start\":40860},{\"end\":40879,\"start\":40872},{\"end\":40888,\"start\":40883},{\"end\":40895,\"start\":40892},{\"end\":41360,\"start\":41355},{\"end\":41373,\"start\":41364},{\"end\":41383,\"start\":41377},{\"end\":41391,\"start\":41387},{\"end\":41404,\"start\":41393},{\"end\":41864,\"start\":41859},{\"end\":41871,\"start\":41868},{\"end\":41880,\"start\":41875},{\"end\":41894,\"start\":41884},{\"end\":41902,\"start\":41898},{\"end\":41911,\"start\":41904},{\"end\":42261,\"start\":42256},{\"end\":42267,\"start\":42265},{\"end\":42276,\"start\":42271},{\"end\":42284,\"start\":42280},{\"end\":42291,\"start\":42288},{\"end\":42301,\"start\":42295},{\"end\":42309,\"start\":42305}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":25746,\"start\":25610},{\"attributes\":{\"id\":\"b1\"},\"end\":25901,\"start\":25748},{\"attributes\":{\"id\":\"b2\"},\"end\":26044,\"start\":25903},{\"attributes\":{\"id\":\"b3\"},\"end\":26273,\"start\":26046},{\"attributes\":{\"id\":\"b4\"},\"end\":26434,\"start\":26275},{\"attributes\":{\"id\":\"b5\"},\"end\":26605,\"start\":26436},{\"attributes\":{\"id\":\"b6\"},\"end\":27107,\"start\":26607},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14273399},\"end\":27544,\"start\":27109},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":11509768},\"end\":27912,\"start\":27546},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1701442},\"end\":28416,\"start\":27914},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":44078837},\"end\":28752,\"start\":28418},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":214797870},\"end\":28992,\"start\":28754},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":57246310},\"end\":29382,\"start\":28994},{\"attributes\":{\"id\":\"b13\"},\"end\":29630,\"start\":29384},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":49568000},\"end\":30185,\"start\":29632},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14035769},\"end\":30548,\"start\":30187},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":384317},\"end\":30956,\"start\":30550},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":8367854},\"end\":31515,\"start\":30958},{\"attributes\":{\"doi\":\"arXiv:2001.02772\",\"id\":\"b18\"},\"end\":31958,\"start\":31517},{\"attributes\":{\"doi\":\"arXiv:1906.11307\",\"id\":\"b19\"},\"end\":32457,\"start\":31960},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3541031},\"end\":33049,\"start\":32459},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":206594692},\"end\":33454,\"start\":33051},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":31635004},\"end\":33981,\"start\":33456},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1799558},\"end\":34509,\"start\":33983},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":59523674},\"end\":35048,\"start\":34511},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4202768},\"end\":35666,\"start\":35050},{\"attributes\":{\"doi\":\"arXiv:1703.02529\",\"id\":\"b26\"},\"end\":35980,\"start\":35668},{\"attributes\":{\"doi\":\"arXiv:1404.5997\",\"id\":\"b27\"},\"end\":36210,\"start\":35982},{\"attributes\":{\"doi\":\"arXiv:1912.02322\",\"id\":\"b28\"},\"end\":36506,\"start\":36212},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":9744596},\"end\":36829,\"start\":36508},{\"attributes\":{\"doi\":\"arXiv:1712.06139\",\"id\":\"b30\"},\"end\":37219,\"start\":36831},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":202786778},\"end\":37723,\"start\":37221},{\"attributes\":{\"doi\":\"arXiv:1901.06887\",\"id\":\"b32\"},\"end\":38045,\"start\":37725},{\"attributes\":{\"id\":\"b33\"},\"end\":38226,\"start\":38047},{\"attributes\":{\"id\":\"b34\"},\"end\":38565,\"start\":38228},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4303699},\"end\":38997,\"start\":38567},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2715202},\"end\":39519,\"start\":38999},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14691112},\"end\":39984,\"start\":39521},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4898729},\"end\":40382,\"start\":39986},{\"attributes\":{\"doi\":\"arXiv:1706.00885\",\"id\":\"b39\"},\"end\":40713,\"start\":40384},{\"attributes\":{\"id\":\"b40\"},\"end\":41233,\"start\":40715},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":206600577},\"end\":41765,\"start\":41235},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":199020292},\"end\":42159,\"start\":41767},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4752389},\"end\":42683,\"start\":42161}]", "bib_title": "[{\"end\":26659,\"start\":26607},{\"end\":27220,\"start\":27109},{\"end\":27642,\"start\":27546},{\"end\":27969,\"start\":27914},{\"end\":28508,\"start\":28418},{\"end\":28809,\"start\":28754},{\"end\":29045,\"start\":28994},{\"end\":29689,\"start\":29632},{\"end\":30244,\"start\":30187},{\"end\":30646,\"start\":30550},{\"end\":31066,\"start\":30958},{\"end\":32536,\"start\":32459},{\"end\":33095,\"start\":33051},{\"end\":33522,\"start\":33456},{\"end\":34043,\"start\":33983},{\"end\":34611,\"start\":34511},{\"end\":35112,\"start\":35050},{\"end\":36562,\"start\":36508},{\"end\":37289,\"start\":37221},{\"end\":38272,\"start\":38228},{\"end\":38639,\"start\":38567},{\"end\":39064,\"start\":38999},{\"end\":39605,\"start\":39521},{\"end\":40041,\"start\":39986},{\"end\":41351,\"start\":41235},{\"end\":41853,\"start\":41767},{\"end\":42252,\"start\":42161}]", "bib_author": "[{\"end\":26292,\"start\":26275},{\"end\":26485,\"start\":26468},{\"end\":26670,\"start\":26661},{\"end\":26680,\"start\":26670},{\"end\":26688,\"start\":26680},{\"end\":26696,\"start\":26688},{\"end\":26705,\"start\":26696},{\"end\":26713,\"start\":26705},{\"end\":26722,\"start\":26713},{\"end\":26734,\"start\":26722},{\"end\":26744,\"start\":26734},{\"end\":26753,\"start\":26744},{\"end\":26760,\"start\":26753},{\"end\":27233,\"start\":27222},{\"end\":27243,\"start\":27233},{\"end\":27253,\"start\":27243},{\"end\":27652,\"start\":27644},{\"end\":27658,\"start\":27652},{\"end\":27665,\"start\":27658},{\"end\":27673,\"start\":27665},{\"end\":27682,\"start\":27673},{\"end\":27690,\"start\":27682},{\"end\":27698,\"start\":27690},{\"end\":27984,\"start\":27971},{\"end\":27992,\"start\":27984},{\"end\":28000,\"start\":27992},{\"end\":28014,\"start\":28000},{\"end\":28028,\"start\":28014},{\"end\":28038,\"start\":28028},{\"end\":28520,\"start\":28510},{\"end\":28528,\"start\":28520},{\"end\":28819,\"start\":28811},{\"end\":28831,\"start\":28819},{\"end\":29055,\"start\":29047},{\"end\":29063,\"start\":29055},{\"end\":29073,\"start\":29063},{\"end\":29082,\"start\":29073},{\"end\":29088,\"start\":29082},{\"end\":29099,\"start\":29088},{\"end\":29473,\"start\":29462},{\"end\":29492,\"start\":29473},{\"end\":29701,\"start\":29691},{\"end\":29714,\"start\":29701},{\"end\":29729,\"start\":29714},{\"end\":29743,\"start\":29729},{\"end\":29750,\"start\":29743},{\"end\":29756,\"start\":29750},{\"end\":29767,\"start\":29756},{\"end\":29779,\"start\":29767},{\"end\":29788,\"start\":29779},{\"end\":29798,\"start\":29788},{\"end\":29805,\"start\":29798},{\"end\":30256,\"start\":30246},{\"end\":30264,\"start\":30256},{\"end\":30273,\"start\":30264},{\"end\":30283,\"start\":30273},{\"end\":30292,\"start\":30283},{\"end\":30658,\"start\":30648},{\"end\":30676,\"start\":30658},{\"end\":30692,\"start\":30676},{\"end\":30704,\"start\":30692},{\"end\":31080,\"start\":31068},{\"end\":31092,\"start\":31080},{\"end\":31098,\"start\":31092},{\"end\":31112,\"start\":31098},{\"end\":31129,\"start\":31112},{\"end\":31526,\"start\":31517},{\"end\":31534,\"start\":31526},{\"end\":31544,\"start\":31534},{\"end\":31552,\"start\":31544},{\"end\":31562,\"start\":31552},{\"end\":31572,\"start\":31562},{\"end\":31584,\"start\":31572},{\"end\":31594,\"start\":31584},{\"end\":31603,\"start\":31594},{\"end\":31615,\"start\":31603},{\"end\":32113,\"start\":32102},{\"end\":32129,\"start\":32113},{\"end\":32140,\"start\":32129},{\"end\":32155,\"start\":32140},{\"end\":32166,\"start\":32155},{\"end\":32551,\"start\":32538},{\"end\":32559,\"start\":32551},{\"end\":32569,\"start\":32559},{\"end\":32581,\"start\":32569},{\"end\":32590,\"start\":32581},{\"end\":32604,\"start\":32590},{\"end\":32613,\"start\":32604},{\"end\":32620,\"start\":32613},{\"end\":32627,\"start\":32620},{\"end\":32636,\"start\":32627},{\"end\":32643,\"start\":32636},{\"end\":33103,\"start\":33097},{\"end\":33112,\"start\":33103},{\"end\":33119,\"start\":33112},{\"end\":33126,\"start\":33119},{\"end\":33533,\"start\":33524},{\"end\":33553,\"start\":33533},{\"end\":33562,\"start\":33553},{\"end\":33578,\"start\":33562},{\"end\":33586,\"start\":33578},{\"end\":33599,\"start\":33586},{\"end\":33613,\"start\":33599},{\"end\":33622,\"start\":33613},{\"end\":34052,\"start\":34045},{\"end\":34065,\"start\":34052},{\"end\":34076,\"start\":34065},{\"end\":34087,\"start\":34076},{\"end\":34095,\"start\":34087},{\"end\":34107,\"start\":34095},{\"end\":34121,\"start\":34107},{\"end\":34132,\"start\":34121},{\"end\":34622,\"start\":34613},{\"end\":34631,\"start\":34622},{\"end\":34640,\"start\":34631},{\"end\":34650,\"start\":34640},{\"end\":34659,\"start\":34650},{\"end\":34666,\"start\":34659},{\"end\":34672,\"start\":34666},{\"end\":34676,\"start\":34672},{\"end\":35126,\"start\":35114},{\"end\":35135,\"start\":35126},{\"end\":35144,\"start\":35135},{\"end\":35157,\"start\":35144},{\"end\":35168,\"start\":35157},{\"end\":35177,\"start\":35168},{\"end\":35186,\"start\":35177},{\"end\":35196,\"start\":35186},{\"end\":35205,\"start\":35196},{\"end\":35217,\"start\":35205},{\"end\":35224,\"start\":35217},{\"end\":35740,\"start\":35732},{\"end\":35750,\"start\":35740},{\"end\":35761,\"start\":35750},{\"end\":35771,\"start\":35761},{\"end\":35782,\"start\":35771},{\"end\":35996,\"start\":35982},{\"end\":36306,\"start\":36297},{\"end\":36312,\"start\":36306},{\"end\":36319,\"start\":36312},{\"end\":36571,\"start\":36564},{\"end\":36577,\"start\":36571},{\"end\":36589,\"start\":36577},{\"end\":36900,\"start\":36890},{\"end\":36910,\"start\":36900},{\"end\":36921,\"start\":36910},{\"end\":36932,\"start\":36921},{\"end\":36939,\"start\":36932},{\"end\":36945,\"start\":36939},{\"end\":36960,\"start\":36945},{\"end\":36970,\"start\":36960},{\"end\":36979,\"start\":36970},{\"end\":37301,\"start\":37291},{\"end\":37310,\"start\":37301},{\"end\":37319,\"start\":37310},{\"end\":37328,\"start\":37319},{\"end\":37340,\"start\":37328},{\"end\":37350,\"start\":37340},{\"end\":37361,\"start\":37350},{\"end\":37368,\"start\":37361},{\"end\":37382,\"start\":37368},{\"end\":37392,\"start\":37382},{\"end\":37399,\"start\":37392},{\"end\":37809,\"start\":37798},{\"end\":37824,\"start\":37809},{\"end\":37836,\"start\":37824},{\"end\":37844,\"start\":37836},{\"end\":38099,\"start\":38086},{\"end\":38110,\"start\":38099},{\"end\":38128,\"start\":38110},{\"end\":38286,\"start\":38274},{\"end\":38299,\"start\":38286},{\"end\":38306,\"start\":38299},{\"end\":38654,\"start\":38641},{\"end\":38668,\"start\":38654},{\"end\":38676,\"start\":38668},{\"end\":39075,\"start\":39066},{\"end\":39084,\"start\":39075},{\"end\":39615,\"start\":39607},{\"end\":39621,\"start\":39615},{\"end\":39627,\"start\":39621},{\"end\":39635,\"start\":39627},{\"end\":39643,\"start\":39635},{\"end\":40051,\"start\":40043},{\"end\":40058,\"start\":40051},{\"end\":40067,\"start\":40058},{\"end\":40075,\"start\":40067},{\"end\":40083,\"start\":40075},{\"end\":40091,\"start\":40083},{\"end\":40100,\"start\":40091},{\"end\":40108,\"start\":40100},{\"end\":40117,\"start\":40108},{\"end\":40455,\"start\":40447},{\"end\":40462,\"start\":40455},{\"end\":40475,\"start\":40462},{\"end\":40486,\"start\":40475},{\"end\":40492,\"start\":40486},{\"end\":40506,\"start\":40492},{\"end\":40830,\"start\":40822},{\"end\":40837,\"start\":40830},{\"end\":40843,\"start\":40837},{\"end\":40851,\"start\":40843},{\"end\":40858,\"start\":40851},{\"end\":40870,\"start\":40858},{\"end\":40881,\"start\":40870},{\"end\":40890,\"start\":40881},{\"end\":40897,\"start\":40890},{\"end\":41362,\"start\":41353},{\"end\":41375,\"start\":41362},{\"end\":41385,\"start\":41375},{\"end\":41393,\"start\":41385},{\"end\":41406,\"start\":41393},{\"end\":41866,\"start\":41855},{\"end\":41873,\"start\":41866},{\"end\":41882,\"start\":41873},{\"end\":41896,\"start\":41882},{\"end\":41904,\"start\":41896},{\"end\":41913,\"start\":41904},{\"end\":42263,\"start\":42254},{\"end\":42269,\"start\":42263},{\"end\":42278,\"start\":42269},{\"end\":42286,\"start\":42278},{\"end\":42293,\"start\":42286},{\"end\":42303,\"start\":42293},{\"end\":42311,\"start\":42303}]", "bib_venue": "[{\"end\":28118,\"start\":28108},{\"end\":31238,\"start\":31192},{\"end\":33267,\"start\":33205},{\"end\":34251,\"start\":34200},{\"end\":34781,\"start\":34737},{\"end\":35369,\"start\":35305},{\"end\":39281,\"start\":39191},{\"end\":39762,\"start\":39711},{\"end\":40170,\"start\":40152},{\"end\":41511,\"start\":41467},{\"end\":42426,\"start\":42377},{\"end\":25632,\"start\":25610},{\"end\":25782,\"start\":25748},{\"end\":25928,\"start\":25903},{\"end\":26089,\"start\":26046},{\"end\":26317,\"start\":26292},{\"end\":26466,\"start\":26436},{\"end\":26828,\"start\":26760},{\"end\":27305,\"start\":27253},{\"end\":27702,\"start\":27698},{\"end\":28106,\"start\":28038},{\"end\":28567,\"start\":28528},{\"end\":28856,\"start\":28831},{\"end\":29162,\"start\":29099},{\"end\":29460,\"start\":29384},{\"end\":29881,\"start\":29805},{\"end\":30344,\"start\":30292},{\"end\":30728,\"start\":30704},{\"end\":31190,\"start\":31129},{\"end\":31706,\"start\":31631},{\"end\":32100,\"start\":31960},{\"end\":32725,\"start\":32643},{\"end\":33203,\"start\":33126},{\"end\":33690,\"start\":33622},{\"end\":34198,\"start\":34132},{\"end\":34735,\"start\":34676},{\"end\":35303,\"start\":35224},{\"end\":35730,\"start\":35668},{\"end\":36074,\"start\":36011},{\"end\":36295,\"start\":36212},{\"end\":36645,\"start\":36589},{\"end\":36888,\"start\":36831},{\"end\":37448,\"start\":37399},{\"end\":37796,\"start\":37725},{\"end\":38084,\"start\":38047},{\"end\":38374,\"start\":38306},{\"end\":38753,\"start\":38676},{\"end\":39189,\"start\":39084},{\"end\":39709,\"start\":39643},{\"end\":40150,\"start\":40117},{\"end\":40445,\"start\":40384},{\"end\":40820,\"start\":40715},{\"end\":41465,\"start\":41406},{\"end\":41943,\"start\":41913},{\"end\":42375,\"start\":42311}]"}}}, "year": 2023, "month": 12, "day": 17}