{"id": 258557362, "updated": "2023-10-05 06:26:44.94", "metadata": {"title": "Large Language Models Meet NL2Code: A Survey", "authors": "[{\"first\":\"Daoguang\",\"last\":\"Zan\",\"middle\":[]},{\"first\":\"Bei\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Fengji\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Dianjie\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Bingchao\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Bei\",\"last\":\"Guan\",\"middle\":[]},{\"first\":\"Yongji\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jian-Guang\",\"last\":\"Lou\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are\"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.09420", "mag": null, "acl": "2023.acl-long.411", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/ZanCZLWGWL23", "doi": "10.18653/v1/2023.acl-long.411"}}, "content": {"source": {"pdf_hash": "4f939f0751e5484f54089f6a97598e39afdcb3b5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.09420v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cbfee503543339617a4f28c0066842bad608ebdc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4f939f0751e5484f54089f6a97598e39afdcb3b5.txt", "contents": "\nLarge Language Models Meet NL2Code: A Survey\n\n\nDaoguang Zan daoguang@ \nCooperative Innovation Center\nInstitute of Software\nChinese Academy of Sciences\n\n\nUniversity of Chinese Academy of Sciences\n\n\nBei Chen beichen@microsoft.com \nMicrosoft Research Asia\n\n\nFengji Zhang v-fengjzhang@microsoft.com \nMicrosoft Research Asia\n\n\nDianjie Lu ludianjie@sdnu.edu.cn \nShandong Normal University\n\n\nBingchao Wu bingchao2017@ \nCooperative Innovation Center\nInstitute of Software\nChinese Academy of Sciences\n\n\nBei Guan guanbei@ \nIntegrative Innovation Center\nInstitute of Software\nChinese Academy of Sciences\n\n\nYongji Wang ywang@itechs.iscas.ac.cn \nIntegrative Innovation Center\nInstitute of Software\nChinese Academy of Sciences\n\n\nJian-Guang Lou jlou@microsoft.com \nMicrosoft Research Asia\n\n\nLarge Language Models Meet NL2Code: A Survey\n\nThe task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the Hu-manEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowdsourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.\n\nIntroduction\n\nIs it possible for novice programmers, even those without any programming experience, to create software simply by describing their requirements in natural language? This is a long-standing fascinating question, which poses challenges to research areas like software engineering, programming language, and artificial intelligence. Realizing this scenario would have an unprecedented impact on our lives, education, economy, and labour market, as it would change the centralized software development and operation paradigm. Due to its promising and intriguing future, natural-languageto-code (NL2Code) has been proposed as a research task that has attracted widespread interest in both academia and industry, with the goal of generating code from natural language descriptions.\n\nEarly studies on NL2Code were mainly based on heuristic rules or expert systems, such as probabilistic grammar-based methods (Joshi and Rambow, 2003;Cohn et al., 2010;Allamanis and Sutton, 2014) and those focusing on domain-specific languages (de Moura and Bj\u00f8rner, 2008;Gulwani, 2010;Jha et al., 2010), which are inflexible and not scalable. Other studies utilized static language models, like n-gram (Nguyen et al., 2013;Raychev et al., 2014;Devanbu, 2012) and Hidden Markov (Sutskever et al., 2008), which have sparse vector representations and cannot model long-term dependencies. Subsequently, neural networks, including CNN (Liu et al., 2016;Sun et al., 2018), RNN (Iyer et al., 2016;Wan et al., 2018), and LSTM (Eriguchi et al., 2016;Yin and Neubig, 2017), were employed to model the relationship between NL and code. In 2017, the Transformer (Vaswani et al., 2017) model was introduced for machine translation and later applied to the NL2Code task (Mastropaolo et al., 2021;Shah et al., 2021). However, these deep learning models require a significant amount of labelled pairs of NL and code for training, and have limited capabilities for the NL2Code task.\n\nRecently, a growing number of large language models (LLMs) with Transformer architecture have been trained on large-scale unlabelled code corpus. These models have the ability to generate code in a zero-shot manner and have achieved impressive results in the NL2Code task. As a milestone, Codex (Chen et al., 2021) has shown that an LLM with 12 billion parameters is able to solve 72.31% of challenging Python programming problems created by humans. More encouragingly, Codex has been used to power a commercial  Figure 1: A simple example of the NL2Code task. The code blocks marked in grey, green, and yellow represent the natural language problem description, the predicted code solution, and the test cases, respectively. product 1 and improve coding efficiency in practice (Sobania et al., 2022a;Barke et al., 2023). Following Codex's success, various LLMs for the NL2Code task have emerged, with model sizes ranging from millions to billions of parameters. Examples include AlphaCode (Li et al., 2022b), which aims to solve competitive-level programming problems, and InCoder (Fried et al., 2023), which supports filling code in arbitrary positions using bidirectional contexts. Other models such as Code-Gen (Nijkamp et al., 2023), PaLM-Coder (Chowdhery et al., 2022), PanGu-Coder (Christopoulou et al., 2022), CodeGeeX (Zheng et al., 2023), and SantaCoder (Allal et al., 2023) have also gained great attention. As the model size increases, LLMs have been shown to exhibit some emergent capabilities such as human-like programming and debugging (Zhang et al., 2022;Saunders et al., 2022;Kang et al., 2023). Large language models have kindled hope for the NL2Code task due to their impressive power and potential value. Despite the significant progress, there are still numerous challenges and opportunities, calling for more advanced and innovative future work. Currently, considering the variety of techniques and applications, there is a growing need for a comprehensive survey to provide a systematic overview of this field and identify critical challenges. To this end, in this paper, we carefully investigate 27 advanced LLMs for NL2Code ( \u00a72), and also review benchmarks and metrics ( \u00a74). We conduct an intuitive comparison of all the existing LLMs on the HumanEval benchmark, perform a thorough analysis, and eventually attribute the success of these LLMs to \"Large Size, Premium Data, Expert Tuning\" ( \u00a73  means large model and data size, high-quality training data and expert hyper-parameter tuning. We also discuss the challenges and opportunities regarding the ability gap between LLMs and Humans ( \u00a75). In addition, we have built a website https://nl2code.github.io to keep track of the latest progress and support crowd-sourcing updates. To the best of our knowledge, this is the first survey of LLMs for NL2Code 2 , and we hope it will contribute to the ongoing development of this exciting field.\n\n\nLarge Language Models for NL2Code\n\nGiven a natural language problem description, the NL2Code task aims to automatically generate the demanded code. To illustrate this task visually, we provide a Python programming problem as an example in Figure 1, while different NL2Code benchmarks may vary in terms of language or 2 We summarize the related surveys in Appendix A.  problem domain. Existing large language models for the NL2Code task are usually based on Transformer (Vaswani et al., 2017) and are trained on a large-scale code related unlabelled corpus. For better code generation performance, most LLMs, no matter encoder-decoder or decoder-only models, employ the causal language modeling objective for training, which is to predict the token following a sequence of tokens. During inference, an LLM can tackle NL2Code problems in a zero-shot manner without fine-tuning its parameters. There are also studies employing few-shot (Austin et al., 2021) or in-context learning (Nijkamp et al., 2023) to further boost the performance. We conduct a comprehensive investigation of 27 representative LLMs for the NL2Code task. Details of each model are summarized in Table 1, where models vary in architecture, size, and accessibility. For better visualization, we present these models in chronological order in Figure 2, plotting the largest model sizes. One trend observed is that these large language models are consistently growing in size as the research field advances. Additionally, the decoder-only architecture is favoured for pre-trained models with larger sizes.\nGPT-C 366M PyMT5 374M CodeGPT 124M GPT-Neo 125M~2.7B PLBART 140M~406M GPT-J 6B Codex 12M~12B CodeT5 60M~770M GPT-CC 125M~1.3B CodeParrot 110M~1.5B JuPyT5 350M LaMDA 2B~137B AlphaCode 284M~41.1B PolyCoder 160M~2.7B CodeGen 350M~16.1B GPT-NeoX 20B InCoder 1.3B~6.7B PaLM-Coder 8B~540B PanGu-Coder 317M~2.6B CodeRL 770M FIM 50M~6.9B PyCodeGPT 110M CodeGeeX 13B ERNIE-Code\nEarly works, such as GPT-C (Svyatkovskiy et al., 2020), PyMT5 (Clement et al., 2020), and PLBART (Ahmad et al., 2021), have relatively small numbers of parameters and do not demonstrate strong capabilities in zero-shot code generation. Conversely, large-scale models such as GPT-Neo (Black et al., 2021) and GPT-J (Wang and Komatsuzaki, 2021), despite their billion-level parameter scale, have been found to have limited power in the NL2Code task due to the small amount of code in their training corpus. Recently, a number of powerful LLMs have been proposed for NL2Code, such as Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), andPaLM-Coder (Chowdhery et al., 2022), which possess massive parameter scales and high-quality training corpus with code. While they show surprisingly good performance on NL2Code, most of them are not readily accessible. At present, a number of excellent open-source models have also been proposed, including CodeParrot (Huggingface, 2021), PolyCoder , GPT-NeoX (Black et al., 2022), and San-taCoder (Allal et al., 2023), which contribute to the thriving of LLMs for NL2Code. Besides, recent studies have proposed various approaches to address specific NL2Code scenarios. For example, JuPyT5 (Chandel et al., 2022a) is designed to work within Jupyter Notebooks, while ERNIE-Code (Chai et al., 2022), CodeGeeX (Zheng et al., 2023), andBLOOM (Scao et al., 2022) are trained to support multiple natural or programming languages. Additionally, InCoder (Fried et al., 2023), FIM (Bavarian et al., 2022), and SantaCoder (Allal et al., 2023) not only support left-to-right code prediction, but also allow for infilling arbitrary regions of code. As LLMs for NL2Code are evolving rapidly, we created a website to keep up-to-date with the latest advances by crowd-sourcing. Details of the website can be found in Appendix B.\n\nThese models are not only attractive in academia (Chen et al., 2021;Nijkamp et al., 2023;Li et al., 2022b), but also applied in real-world products to improve programming efficiency (Sobania et al., 2022a;Barke et al., 2023). One example is GitHub and OpenAI's Copilot, a programming assistance tool that utilizes Codex to provide realtime code suggestions. Other notable products include CodeGeeX 3 and CodeWhisperer 4 . A summary of 10 products can be found in Appendix Table 5. Recent studies (Sobania et al., 2022b;Pearce et al., 2022;Nguyen and Nadi, 2022) have shown that these products can provide helpful recommendations, while they also introduce minor bugs that can cause issues for users. There is still room for improvement before LLMs can be fully practical and capable of coding like humans.\n\n\nWhat makes LLMs successful?\n\nWe have summarized the existing large language models for NL2Code. These LLMs vary in terms of architecture, size, and other characteristics, making it difficult to establish a completely fair comparison. We evaluate these LLMs on the HumanEval benchmark (Chen et al., 2021) in a zero-shot manner to provide an intuitive comparison. HumanEval, proposed along with Codex, is one of the most popular benchmarks for the NL2Code task and consists of 164 hand-written Python programming problems. Test cases are provided for each programming problem to evaluate the correctness of generated code. pass@k is used as the evaluation metric 5 , which calculates the proportion of problems that can be correctly answered with k tries.   , also obtain remarkably decent results in the left-to-right generation setting. The significant variation in performance leads us to the question: What makes LLMs successful in NL2Code? Given the diversity of these models in terms of design choices, we perform a thorough analysis and conclude the answer: Large Size, Premium Data, Expert Tuning. That is, large model and data size, high-quality data and expert hyper-parameter tuning are the key factors for the success of LLMs in the NL2Code task. In this section, we detail our observations and insights from the perspectives of model, data and tuning.\n\n\nLarge Model Size\n\nAs shown in Figure 2 and Table 2, recent LLMs for NL2Code exhibit larger sizes and superior performance. This is consistent with prior findings that an increased number of model parameters can enhance model capabilities (Radford et al., 2019;Thoppilan et al., 2022;Chowdhery et al., 2022). We further demonstrate the correlation between model size and performance in Figure 3a, which compares the pass@1 results of 10 representative models on the HumanEval benchmark. It is clear that larger models generally result in better performance. Furthermore, we also find that current models, regardless of size, still have the potential for improvement through further increases in size. Additional results on the HumanEval and MBPP benchmarks can be found in Appendix Figure 7, which also support this conclusion.\n\nAdditionally, we conduct an experiment on the HumanEval benchmark to examine the syntax error rates of the code generated by different models of varying sizes. Specifically, we make the models predict 10 code samples for each programming problem, and then calculate the percentage of code samples that have syntax errors. As shown in Figure 3b, results indicate that larger models tend to have lower syntax error rates. It is noteworthy that the largest version of the CodeGen-Mono model exhibits a remarkably low rate of syntax errors, i.e., 6%. However, as evidenced by Figure 3a and Table 2, the CodeGen-Mono model with 16 billion parameters still has unsatisfactory performance in terms of pass@k , e.g., pass@1 to be 29%. This highlights the fact that the current limitation for large pre-trained models is the generation of semantically correct code.\n\n\nLarge and Premium Data\n\nAs the sizes of LLMs increase in the field of NL2Code, the scale of the corpus used for training also increases. This highlights the importance of selecting and pre-processing high-quality data. In this section, we will discuss various commonly used data sources and pre-processing strategies that are essential for training LLMs.\n\nEarly models were trained using manually annotated data pairs of NL and code, and the data sources include CodeSearchNet (Husain et al., 2019), CoST (Zhu et al., 2022b), and XL-CoST (Zhu et al., 2022a). However, manual annotation is labour-intensive and time-consuming. There are also models like GPT-3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), and GPT-J (Wang and Komatsuzaki, 2021)  In order to ensure the quality of the training corpus, it is common for LLMs to perform data preprocessing on the significant amount of code in the collected data. We carefully review the data pre-processing methods of five powerful LLMs, including Codex (Chen et al., 2021), AlphaCode (Li et al., 2022b), CodeGen (Nijkamp et al., 2023), In-Coder (Fried et al., 2023), and PyCodeGPT (Zan et al., 2022b), and identify several commonalities. One is the removal of likely auto-generated or unfinished code files, as they are deemed to be meaningless. Additionally, specific rules are employed to filter out uncommon code files. These rules include factors such as the repository star rating, the file size, the line length, and the alphanumeric rate. In summary, the goal of these pre-processing strategies is to achieve a code corpus that is unduplicated, complete, correct, clean, and general in nature.\n\n\nExpert Tuning\n\nTraining an excellent model requires careful consideration of various design choices and hyperparameters. After reviewing the existing 27 LLMs (summary in Appendix Table 6), we have the following findings. Firstly, these LLMs share some common settings. For example, we observe that the optimizer of the current models is almost all Adam (Kingma and Ba, 2014) or its variants (Loshchilov and Hutter, 2017). We also 6 https://github.com 7 https://stackoverflow.com   find that initializing with other natural language models yields no noticeable gain compared to training from scratch, except for accelerating convergence (Chen et al., 2021). Furthermore, there are several hyper-parameters that require expert tuning, such as learning rate, batch size, window size, warmup steps, gradient accumulation steps, and sampling temperature. For the learning rate, we analyze its correlation with model size using six powerful LLMs, as shown in Figure 4. We observe that the learning rate becomes smaller as the model gets larger. To explore the effects of temperature, in Figure 5, we report the performance of two models using multiple temperatures on HumanEval. One observation is that higher temperature leads to lower pass@1 and higher pass@100, which suggests that a higher temperature makes LLMs generate more diverse predictions and vice versa. Besides, some studies (erman Arsenovich Arutyunov and Avdoshin, 2022) have shown that window size is a key factor. An interesting finding is that the  \n\n\nBenchmarks and Metrics\n\nTo evaluate the NL2Code task, high-quality benchmarks and reliable metrics are fundamental and essential. In this section, we provide a brief overview of current benchmarks and metrics, as well as our observations and the open challenges.\n\nWe summarize 17 well-studied NL2Code benchmarks in Table 3, where we can find that each of these benchmarks has its own characteristics regarding size, language, complexity, and scenario. We observe that most benchmarks contain a limited number of instances. For example, the widely used HumanEval and MBPP have 164 and 974 instances, respectively. This is because these benchmarks are typically hand-written to ensure that LLMs have not seen them during training. In the era of large language models, it is crucial to avoid data leak-age when creating new benchmarks. Additionally, most current benchmarks have their problem descriptions in English and code solutions in Python.\n\nRecently, several multi-lingual benchmarks have been proposed, such as MBXP (Athiwaratkun et al., 2022), HumanEvalX (Zheng et al., 2023), and Mul-tiPL (Cassano et al., 2022), which cover multiple programming languages, and ODEX (Wang et al., 2022c), which covers multiple natural languages. Details of multi-lingual benchmarks are listed in Appendix Table 7. Furthermore, benchmarks have been proposed for other practical scenarios, such as data science (Lai et al., 2022), public library (Zan et al., 2022b), private library (Zan et al., 2022a), multi-turn program synthesis (Nijkamp et al., 2023), and code security (Siddiq and msiddiq, 2022). For execution-based benchmarks, comprehensive test cases with complete coverage of the generated program can ensure the trustworthiness of evaluation results. As a reference, the average number of test cases for each benchmark, as well as the length statistics of the problem descriptions and solutions are also provided in Table 3.\n\nManually evaluating the generated code is impractical, which calls for the need for automatic metrics. The above mentioned benchmarks all provide test cases for execution-based evaluation, where metrics such as pass@k (Chen et al., 2021), n@k (Li et al., 2022b), test case aver-age (Hendrycks et al., 2021), and execution accuracy (Rajkumar et al., 2022) can be used. However, this approach has stringent requirements for the quality of test cases and can only evaluate executable code. For non-executable code, metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), andCodeBLEU (Ren et al., 2020) are used, while they can not precisely evaluate the correctness of the code. So far, there are many open challenges in designing metrics to evaluate various aspects of code, such as vulnerability, maintainability, clarity, execution complexity, and stability.\n\n\nChallenges and Opportunities\n\nOur investigations have revealed that advances in LLMs for NL2Code have a considerable impact on both academia and industry. Despite this progress, there are still numerous challenges that need to be addressed, offering ample opportunities for further research and applications. In this section, we explore the challenges and opportunities in terms of the ability gap between LLMs and humans.\n\nUnderstanding Ability The inherent flexibility of natural language allows for a variety of expressions to convey functional requirements. Humans are able to understand various descriptions at different levels of abstraction. In contrast, current LLMs tend to be sensitive to the given context, which may cause unexpected performance degradation . In addition, LLMs may struggle when faced with complex problems that have numerous conditions and requirements (Barke et al., 2022;Imai, 2022). We believe exploring the understanding abilities of LLMs is a crucial research direction. One potential solution is to break down complex problems into multiple steps, as is commonly done in reasoning tasks (Wei et al., 2022).\n\nJudgement Ability Humans have the ability to determine whether they can solve a programming problem or not. While current models will always return a solution even if there is no answer to the problem, due to the fact that they are trained by unsupervised causal language modeling objective. This can cause problems in practical applications. To improve the judgment ability of LLMs, researchers have employed reinforcement learning to leverage user feedback, as seen in models like InstructGPT  and ChatGPT 8 . However, collecting high-quality feedback for code is costly and challenging. There are also ongoing studies (Chen et al., 2023;Key et al., 2022) exploring the possibility of self-validation for LLMs, which is also a promising research direction.\n\nExplanation Ability It is widely acknowledged that human developers possess the ability to interpret the meaning of the code they write, which is crucial for educational purposes and software maintenance. Recent studies showed that LLMs have the potential to automatically generate code explanations. MacNeil et al. (2022a) proposed using LLMs to generate code explanations for students during their learning process, and MacNeil et al. (2022b) proposed explaining numerous aspects of a given code snippet using Copilot. Further research and explorations are necessary to fully realize the potential of LLMs in this regard.\n\nAdaptive Learning Ability A fundamental difference between current large language models and humans is their ability to adapt to new and updated knowledge. Human developers possess a unique ability to quickly search and learn new materials, such as programming documentation, and adapt to changes in APIs with relative ease. However, re-training or fine-tuning LLMs requires significant effort and resources. This issue has inspired a number of recent studies, such as DocCoder  and APICoder (Zan et al., 2022a), which utilize retrieval-based methods to provide extra or updated knowledge during model inference. Despite these advancements, it remains an open challenge to endow LLMs with the powerful learning capabilities humans possess.\n\nMulti-tasking Ability Large language models have been applied to a variety of code-related tasks, such as code repair (Joshi et al., 2022; Prenner and Robbes, 2021), code search (Neelakantan et al., 2022), and code review (Li et al., 2022c) as well as non-code tasks that can be formatted in a code-like manner, such as mathematics (Drori and Verma, 2021; Drori et al., 2021) and chemistry (Krenn et al., 2022;Hocky and White, 2022). However, there are differences between LLMs and human abilities in terms of multi-tasking. Humans can seamlessly switch between tasks, while LLMs may require sophisticated prompt engineering (Liu et al., 2023). Another evidence is that LLMs lack the ability to quickly master multiple programming languages (Zheng et al., 2023) as humans do. These limitations highlight areas for future research.\n\nIn this paper, we survey 27 existing large language models for NL2Code, and draw a thorough analysis of the underlying reasons for their success. We also provide a detailed review of benchmarks and metrics. Regarding the gap between models and humans, we present ongoing challenges and opportunities. In addition, we have developed a website to track the latest findings in this field. We hope this survey can contribute to a comprehensive overview of the field and promote its thriving evolution.\n\n\nLimitations\n\nIn this paper, we thoroughly investigate the existing large language models for NL2Code, and summarize them from diverse perspectives with our own thinking. However, as this field is evolving so rapidly, there may be aspects that we have overlooked, or some new works that we have not covered. To mitigate this issue, we have created a website to track the latest progress through crowdsourcing, hoping that it will continually contribute to the development of the field. Besides, the existing LLMs possess their own characteristics in terms of model size, architecture, corpus, pre-processing, tokenizer, hyper-parameters, and training platforms. Also, some of them are currently not publicly available, such as AlphaCode (Li et al., 2022b) and PaLM-Coder (Chowdhery et al., 2022). Therefore, it is almost impractical to conduct a completely fair comparison. We tried our best to show a kind of comparison on the popular HumanEval and MBPP benchmarks, hoping that it can provide clues to the differences in performance of different LLMs. In addition, evaluating LLMs has a high cost in computational resources. We thus have made all files generated by the LLMs publicly available on https://nl2code.github.io. Previous surveys on the topic of code intelligence (Allamanis et al., 2018;Le et al., 2020;Li et al., 2022a;Xu and Zhu, 2022) and code generation (Pawade et al., 2018;Shin and Nam, 2021;Dehaerne et al., 2022) have primarily focused on early methodologies such as the use of programming templates (Syriani et al., 2018;Luhunu and Syriani, 2017), neural models based on CNN, RNN, and LSTM architectures (Allamanis et al., 2018;Sharma et al., 2021), and small-scale Transformer models that require labelled data for training (Mastropaolo et al., 2021;Shah et al., 2021). However, with the advancement of model size, Transformerbased models have demonstrated exceptional performance in NL2Code tasks and have given rise to the development of more capable code generation models. In light of this, there exists a clear need for a comprehensive survey of large language models for NL2Code tasks to bridge this gap in knowledge. This study endeavours to fulfill this need by providing a thorough analysis of the successful LLMs and a detailed review of NL2Code benchmarks and metrics. We also present the ongoing challenges and opportunities regarding the ability gap between LLMs and humans.\n\n\nReferences\n\nFinally, we would like to highlight some criteria for our survey. First, we only refer to official papers to investigate the size of the models. For example, Codex reported the model with a maximum size of 12B in the paper, but later trained larger ones. In this case, we only consider the 12B model as the largest one. In addition, the publication dates of the models in Figure 2 are taken from official papers or blogs.\n\n\nB An Online Website\n\nTo keep tracking the latest progress of LLMs for NL2Code, we have developed an online real-time update website at https://nl2code.github.io. We have collected as many of the latest research works as possible on this website. Everyone is allowed to contribute to the website by pulling requests on GitHub. This website also includes features such as fuzzy search and custom tag categories, which will facilitate researchers to find the papers they want quickly. We hope this website can assist researchers and developers in related fields and contribute to its advancement.\n\n\nModel\n\nSize pass@k k=1 k=10 k=100  \n\n\nC Experimental Setup\n\nIn this section, we will first present the definition of pass@k , followed by the details of the experiments conducted on two benchmarks, namely Hu-manEval (Chen et al., 2021) (results in Table 2) and MBPP (Austin et al., 2021) (results in Table 4).\n\n\nC.1 Definition of pass@k\n\nWe use pass@k as our metric for evaluation. For each programming problem, we sample n candidate code solutions and then randomly pick k of them. If any of the k code solutions pass the given test cases, the problem can be regarded as solved. So pass@k is the proportion of solved problems in the benchmark (Chen et al., 2021). Formally, assuming that the number of correct ones in k samples is c, pass@k = 1 if n \u2212 c < k; otherwise, pass@k = 1 \u2212 n i=n\u2212c+1 (1 \u2212 k/i). We chose pass@k as our primary evaluation metric because it offers a completely precise evaluation of code accuracy by executing test cases, while other metrics mentioned in Section 4 either originate from pass@k or have lower precision.\n\n\nC.2 Implementation Details\n\nFor HumanEval, we use the original benchmark 9 . Most results in Table 2 are taken from the original papers, while we reproduce the results of GPT-CC, PLBART, CodeT5, and InCoder 1.3B by strictly following the same experimental setup as the other models. In detail, we set the sample number to 200, the maximum length of newly generated tokens to 200, and top_p to 0.95. We set the temperature from 0.1 to 1.0 with an interval of 0.1, and report the best performance across these temperatures.\n\nFor MBPP, we use the version from Chen et al. (2023) 10 . In Table 4, the results of InCoder 6.7B and models larger than 10B are taken from Chen et al. (2023), while we reproduced other results. Specifically, we set the sample number to 100, the maximum length of newly generated tokens to 200, top_p to 0.95, and the temperature to 0.8.\n\nFor the two benchmarks above, we employ the same post-processing strategy. Following Codex (Chen et al., 2021), we terminate the sampling process when one of the following sequences is encountered in the generated code: '\\nclass', '\\ndef', '\\n#', '\\n@', '\\nif', and '\\nprint GPT-NeoX (Black et al., 2022), and we visualize the results in Figure 6. It is found that the 165M version model with an 8, 000 context window is comparable to the 20B version model with a 2, 000 context window. This observation illustrates that the context window also needs to be considered when training the model. Table 5: Summary of products powered by LLMs. PLs and IDEs refer to programming languages and integrated development environments, respectively. The information for these products was recorded on December 27, 2022.   Table 6: The details of LLMs for NL2Code. We list the full names of these abbreviations: de-duplication (de.), tokenizer (token.), optimizer (opti.), batch size (bs), window size (ws), gradient accumulation steps (gss), warmup steps (wp), learning rate (lr), weight decay (wd), decay schedule (decay), precision floating point (pr), model initialization (init.), left-to-right (\u2192), fill-in-the-middle (\u2194), byte-level byte-pair-encoding (BBPE), SentencePiece (SP), polynomial (PN), and inverse square (IS).\n\nFigure 2 :\n2The timeline of LLMs for NL2Code, with only the largest model sizes plotted for visual clarity.\n\nFigure 3 :\n3(a) pass@1 and (b) syntax error rates on the HumanEval benchmark with various model sizes. Code (Li et al., 2022b), CodeGen-Mono (Nijkamp et al., 2023), and PanGu-Coder (Christopoulou et al., 2022) also exhibit impressive performance.Notably, InCoder (Fried et al., 2023)  andSanta- Coder (Allal et al., 2023), which use the FIM training method(Bavarian et al., 2022)\n\nFigure 4 :\n4Learning rate of six advanced LLMs in terms of various model sizes.\n\nFigure 5 :\n5pass@k on the HumanEval benchmark with different temperatures during model inference.\n\nFigure 7 :\n7Performance of LLMs with varying parameter sizes on the HumanEval and MBPP benchmarks.\n\n\n). This 1 https://github.com/features/copilotModel \nSize \nL. A. \nH. \nP. \nDecoder \nGPT-C (2020) \n366M \n24 16 \n1, 024 \n\u00d7 \nCodeGPT (2021) \n124M \n12 12 \n768 \nGPT-Neo (2021) \n125M~2.7B \n32 20 \n2, 560 \nGPT-J (2021) \n6B \n28 16 \n4, 096 \nCodex (2021) \n12M~12B \n40 40 \n5, 140 \n\u00d7 \nGPT-CC (2021) \n125M~1.3B \n24 16 \n2, 048 \nCodeParrot (2021) \n110M~1.5B \n48 25 \n1, 600 \nLaMDA (2022) \n2B~137B \n64 128 8, 192 \n\u00d7 \nPolyCoder (2022) \n160M~2.7B \n32 32 \n2, 560 \nCodeGen (2023) \n350M~16.1B 34 24 \n6, 144 \nInCoder (2023) \n1.3B~6.7B \n32 32 \n4, 096 \nGPT-NeoX (2022) \n20B \n44 64 \n6, 144 \nPaLM-Coder (2022) 8B~540B \n118 48 \n18, 432 \u00d7 \nPanGu-Coder (2022) 317M~2.6B \n32 32 \n2, 560 \n\u00d7 \nFIM (2022) \n50M~6.9B \n32 32 \n4, 096 \n\u00d7 \nPyCodeGPT (2022b) 110M \n12 12 \n768 \nCodeGeeX (2023) \n13B \n39 40 \n5, 120 \nBLOOM (2022) \n560M~176B \n70 112 14, 336 \nSantaCoder (2023) \n1.1B \n24 16 \n2, 048 \nEncoder-Decoder \nPyMT5 (2020) \n374M \n12 16 \n1, 472 \n\u00d7 \nPLBART (2021) \n140M~406M 24 16 \n1, 024 \nCodeT5 (2021) \n60M~770M \n48 16 \n1, 024 \nJuPyT5 (2022a) \n350M \n12 16 \n1, 472 \n\u00d7 \nAlphaCode (2022b) \n284M~41.1B 64 128 6, 144 \n\u00d7 \nCodeRL (2022) \n770M \n48 16 \n1, 024 \nCodeT5Mix (2022) \n220M~770M 48 16 \n1, 024 \nERNIE-Code (2022) 560M \n24 12 \n768 \n\n\n\nTable 1 :\n1Summary of 27 existing LLMs for NL2Code.We show L. (number of layers), A. (number of atten-\ntion heads), H. (hidden dimensions), and P. (model \nweights public or not) for the largest size version of \neach model. Note that some models, such as GPT-Neo, \nGPT-J, LaMDA, GPT-NeoX, FIM, and BLOOM, are \nnot exclusively trained for code. \n\n\n\nTable 2\n2https://keg.cs.tsinghua.edu.cn/codegeex 4 https://aws.amazon.com/cn/codewhisperer 5 The details of pass@k can be found in Appendix C.1.shows the \nresults of different LLMs organized by the model \nsize. Implementation details and the evaluation on \nthe MBPP benchmark (Austin et al., 2021) can be \nfound in Appendix C.2. \nIt can be observed from Table 2 that the per-\nformance of existing LLMs varies widely on Hu-\nmanEval, even for those with similar model sizes. \nSpecifically, Codex (Chen et al., 2021) holds the \nleading position in various model sizes, while a \nrelatively small model, PyCodeGPT 110M (Zan \net al., 2022b), achieves comparable results to \nCodex 85M. Other larger models such as Alpha-\n\n3 \n\nTable 2 :\n2Performance on the HumanEval benchmark.\u2020 \n\n\n\nthat are trained on the Pile (Gao et al., 2020), a large-scale unsupervised dataset. However, these models have not yet demonstrated exceptional code generation capabilities due to the limited number of code files in the training corpus. More recently, with the emergence of more powerful LLMs for NL2Code, larger-scale unlabelled code datasets have been proposed, including BigQuery (Google, 2016), CodeParrot's corpus (HuggingFace, 2021a), GitHub-Code (Hug-gingFace, 2021b), and the Stack (HuggingFace, 2022), which are collected from general domain open-source websites like GitHub 6 and Stack Overflow 7 . Furthermore, there are also specialized datasets proposed for different scenarios, for example, using Jupyter Notebooks or competition programming problems as a training corpus. Released datasets include Jupyter (HuggingFace, 2021c), JuICe (Agashe et al., 2019), APPS (Hendrycks et al., 2021), and CodeNet (IBM, 2021).\n\nTable 3 :\n3Summary of 17 benchmarks for NL2Code. Num. denotes the number of instances in the benchmark, P.NL denotes Problem description's Natural Language, S.PL denotes code Solution's Programming Language, and T.N. denotes the average Number of Test cases. P.C. and P.L. (S.C. and S.L.) stand for the average number of Characters and Lines in Problem description (code Solution). * denotes the number of instances per programming language.small model with a large window size sometimes \noutperforms the large model with a small window \nsize (details in Appendix D). In addition, power-\nful LLMs usually train a new tokenizer on code \ncorpus primarily using two techniques: Byte-level \nByte-Pair-Encoding (Radford et al., 2019) and Sen-\ntencePiece (Kudo and Richardson, 2018). A new \ntokenizer can be more effective and accurate in \nsplitting code content into tokens. These proven \ntuning techniques will serve as valuable references \nfor training more powerful LLMs. \n\n\n\n\nCodeGen-Mono \u2020 2.7B 28.80 60.73 75.41 CodeGen-Mono \u2020 6.1B 33.70 62.70 70.25 GPT-J \u2020Model Size:~100M \nGPT-Neo  \u2020 \n125M 0.26 \n2.15 \n7.96 \nCodeParrot  \u2020 \n110M 0.48 \n3.89 \n15.93 \nPyCodeGPT  \u2020 \n110M 9.39 \n28.37 48.71 \nPolyCoder  \u2020 \n160M 1.08 \n6.67 \n18.97 \nModel Size:~500M \nCodeT5  \u2020 \n770M 15.78 38.63 50.35 \nPolyCoder  \u2020 \n400M 1.31 \n7.98 \n21.55 \nBLOOM  \u2020 \n560M 0.26 \n2.04 \n8.90 \nCodeGen-Mono  \u2020 350M 15.44 42.50 64.40 \nModel Size:~1B \nGPT-Neo  \u2020 \n1.3B \n3.77 \n16.26 29.51 \nCodeParrot  \u2020 \n1.5B \n1.29 \n8.66 \n27.17 \nBLOOM  \u2020 \n1.1B \n1.90 \n9.20 \n23.42 \nBLOOM  \u2020 \n1.7B \n3.16 \n14.23 31.38 \nInCoder  \u2020 \n1.3B \n10.00 34.02 55.50 \nSantaCoder  \u2020 \n1.1B \n3.65 \n21.33 41.92 \nModel Size:~5B \nGPT-Neo  \u2020 \n2.7B \n5.89 \n23.09 44.26 \nPolyCoder  \u2020 \n2.7B \n4.39 \n17.99 38.17 \nBLOOM  \u2020 \n3B \n2.25 \n13.58 32.08 \nBLOOM  \u2020 \n7.1B \n1.01 \n7.91 \n24.12 \n6B \n11.30 35.62 53.63 \nInCoder \n6.7B \n21.3 \n46.5 \n66.2 \nModel Size: >10B \nCodeGen-Mono 16.1B 42.4 \n65.8 \n79.1 \ncushman-001 \n\u2212 \n45.9 \n66.9 \n79.9 \ndavinci-001 \n\u2212 \n51.8 \n72.8 \n84.1 \ndavinci-002 \n\u2212 \n58.1 \n76.7 \n84.5 \n\n\n\nTable 4 :\n4The performance of LLMs on the MBPP benchmark. \u2020 denotes our reproduced results, while others are taken fromChen et al. (2023). We omit CodeGPT, GPT-CC, and PLBART as their numbers are zero.\n\n\n'. In our experiments, CodeT5 770M refers to the version 11 with the causal language modeling objective. For good reproducibility and further research, we have made our code and the generated results of the LLMs on HumanEval and MBPP publicly available on our website.D Context Window vs. PerformanceRecent work (erman Arsenovich Arutyunov and Avdoshin, 2022) claimed that the size of the context window plays a vital role in enhancing the 9 https://github.com/openai/human-eval/blob/ master/data/HumanEval.jsonl.gz 10 https://github.com/microsoft/CodeT/blob/ main/CodeT/data/dataset/mbpp_sanitized_for_code_ generation.jsonl 11 https://huggingface.co/Salesforce/ codet5-large-ntp-pyAverage Passed Test CasesGPT-NeoX 165M 2K GPT-NeoX 165M 4K GPT-NeoX 165M 8K GPT-NeoX 20B 2KFigure 6: Performance of GPT-NeoX with different model sizes (165M and 20B) and context windows (2K, 4K, and 8K) on the APPS benchmark. performance of LLMs for NL2Code. Specifically, experiments are conducted on the APPS benchmark (Hendrycks et al., 2021) withIntroductory Interview Competition \n0% \n\n1% \n\n2% \n\n3% \n\n\nhttps://chat.openai.com\n\nSurvey on template-based code generation. Lechanceux Luhunu, Eugene Syriani, ACM/IEEE International Conference on Model Driven Engineering Languages and Systems. Lechanceux Luhunu and Eugene Syriani. 2017. Survey on template-based code generation. In ACM/IEEE International Conference on Model Driven Engi- neering Languages and Systems.\n\nStephen Macneil, Andrew Tran, Arto Hellas, Joanne Kim, Sami Sarsa, Paul Denny, arXiv:2211.02265Seth Bernstein, and Juho Leinonen. 2022a. Experiences from using code explanations generated by large language models in a web software development e-book. arXiv preprintStephen MacNeil, Andrew Tran, Arto Hellas, Joanne Kim, Sami Sarsa, Paul Denny, Seth Bernstein, and Juho Leinonen. 2022a. Experiences from using code explanations generated by large language models in a web software development e-book. arXiv preprint arXiv:2211.02265.\n\nGenerating diverse code explanations using the gpt-3 large language model. Stephen Macneil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross, Ziheng Huang, Proceedings of the 2022 ACM Conference on International Computing Education Research. the 2022 ACM Conference on International Computing Education Research2Stephen MacNeil, Andrew Tran, Dan Mogil, Seth Bern- stein, Erin Ross, and Ziheng Huang. 2022b. Gener- ating diverse code explanations using the gpt-3 large language model. In Proceedings of the 2022 ACM Conference on International Computing Education Research-Volume 2, pages 37-39.\n\nStudying the usage of text-to-text transfer transformer to support code-related tasks. Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, Gabriele Bavota, 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEEAntonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Study- ing the usage of text-to-text transfer transformer to support code-related tasks. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 336-347. IEEE.\n\n. Microsoft, Microsoft. 2019. IntelliCode. https://github.com/ MicrosoftDocs/intellicode.\n\n. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, arXiv:2201.10005Nikolas Tezak, Jong Wook Kim, Chris HallacyarXiv preprintet al. 2022. Text and code embeddings by contrastive pretrainingArvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pre- training. arXiv preprint arXiv:2201.10005.\n\nAn empirical evaluation of github copilot's code suggestions. Nhan Nguyen, Sarah Nadi, Proceedings of the 19th International Conference on Mining Software Repositories. the 19th International Conference on Mining Software RepositoriesNhan Nguyen and Sarah Nadi. 2022. An empirical evaluation of github copilot's code suggestions. In Proceedings of the 19th International Conference on Mining Software Repositories, pages 1-5.\n\nA statistical semantic language model for source code. Anh Tuan Tung Thanh Nguyen, Nguyen, Anh Hoan, Tien N Nguyen, Nguyen, Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering. the 2013 9th Joint Meeting on Foundations of Software EngineeringTung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen. 2013. A statistical se- mantic language model for source code. In Proceed- ings of the 2013 9th Joint Meeting on Foundations of Software Engineering, pages 532-542.\n\nCodeGen: An open large language model for code with multi-turn program synthesis. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, The Eleventh International Conference on Learning Representations. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. CodeGen: An open large lan- guage model for code with multi-turn program syn- thesis. In The Eleventh International Conference on Learning Representations.\n\nTraining language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, Ryan J Lowe, abs/2203.02155ArXivLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.\n\nBLEU: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a method for automatic eval- uation of machine translation. In Proceedings of the 40th annual meeting of the Association for Compu- tational Linguistics, pages 311-318.\n\nLiterature survey on automatic code generation techniques. i-Manager's. Dipti Pawade, Avani Sakhapara, Sanyogita Parab, Divya Raikar, Ruchita Bhojane, Henali Mamania, Journal on Computer Science. 6234Dipti Pawade, Avani Sakhapara, Sanyogita Parab, Di- vya Raikar, Ruchita Bhojane, and Henali Mamania. 2018. Literature survey on automatic code genera- tion techniques. i-Manager's Journal on Computer Science, 6(2):34.\n\nAsleep at the keyboard? assessing the security of github copilot's code contributions. Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, 2022 IEEE Symposium on Security and Privacy (SP). IEEEHammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. 2022. Asleep at the keyboard? assessing the security of github copilot's code contributions. In 2022 IEEE Symposium on Security and Privacy (SP), pages 754-768. IEEE.\n\nAutomatic program repair with openai's codex: Evaluating quixbugs. Aron Julian, Romain Prenner, Robbes, arXiv:2111.03922arXiv preprintJulian Aron Prenner and Romain Robbes. 2021. Auto- matic program repair with openai's codex: Evaluat- ing quixbugs. arXiv preprint arXiv:2111.03922.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 189Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n\nEvaluating the text-to-sql capabilities of large language models. Nitarshan Rajkumar, Raymond Li, Dzmitry Bahdanau, arXiv:2204.00498arXiv preprintNitarshan Rajkumar, Raymond Li, and Dzmitry Bah- danau. 2022. Evaluating the text-to-sql capabil- ities of large language models. arXiv preprint arXiv:2204.00498.\n\nCode completion with statistical language models. Veselin Raychev, Martin Vechev, Eran Yahav, Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation. the 35th ACM SIGPLAN Conference on Programming Language Design and ImplementationVeselin Raychev, Martin Vechev, and Eran Yahav. 2014. Code completion with statistical language models. In Proceedings of the 35th ACM SIGPLAN Confer- ence on Programming Language Design and Imple- mentation, pages 419-428.\n\nCodeBLEU: a method for automatic evaluation of code synthesis. Daya Shuo Ren, Shuai Guo, Long Lu, Shujie Zhou, Duyu Liu, Neel Tang, Ming Sundaresan, Ambrosio Zhou, Shuai Blanco, Ma, arXiv:2009.10297arXiv preprintShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Am- brosio Blanco, and Shuai Ma. 2020. CodeBLEU: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297.\n\nSelf-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, arXiv:2206.05802arXiv preprintWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802.\n\nAngela Teven Le Scao, Christopher Fan, Ellie Akiki, Suzana Pavlick, Daniel Ili\u0107, Roman Hesslow, Alexandra Sasha Castagn\u00e9, Fran\u00e7ois Luccioni, Yvon, arXiv:2211.05100Matthias Gall\u00e9, et al. 2022. BLOOM: A 176b-parameter open-access multilingual language model. arXiv preprintTeven Le Scao, Angela Fan, Christopher Akiki, El- lie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Ro- man Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. BLOOM: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\n\nNatural language to python source code using transformers. Meet Shah, Rajat Shenoy, Radha Shankarmani, 2021 International Conference on Intelligent Technologies (CONIT). IEEEMeet Shah, Rajat Shenoy, and Radha Shankarmani. 2021. Natural language to python source code using transformers. In 2021 International Conference on Intelligent Technologies (CONIT), pages 1-4. IEEE.\n\nTushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, Federica Sarro, arXiv:2110.096102021. A survey on machine learning techniques for source code analysis. arXiv preprintTushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, and Federica Sarro. 2021. A survey on machine learning techniques for source code analy- sis. arXiv preprint arXiv:2110.09610.\n\nSystematic mapping study of template-based code generation. Eugene Syriani, Lechanceux Luhunu, Houari Sahraoui, Systems & Structures. 52Computer LanguagesEugene Syriani, Lechanceux Luhunu, and Houari Sahraoui. 2018. Systematic mapping study of template-based code generation. Computer Lan- guages, Systems & Structures, 52:43-62.\n\ntabnine. tabnine. 2018. TabNine. https://www.tabnine.com.\n\nLAMDA: Language models for dialog applications. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239arXiv preprintRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LAMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239.\n\nAttention is all you need. Neural Information Processing Systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Neural Information Processing Systems, 30.\n\nImproving automatic source code summarization via deep reinforcement learning. Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, Philip S Yu, Proceedings of the 33rd ACM/IEEE international conference on automated software engineering. the 33rd ACM/IEEE international conference on automated software engineeringYao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. 2018. Im- proving automatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE international conference on auto- mated software engineering, pages 397-407.\n\nGPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. Ben Wang, Aran Komatsuzaki, Ben Wang and Aran Komatsuzaki. 2021. GPT-J- 6B: A 6 Billion Parameter Autoregressive Lan- guage Model. https://github.com/kingoflolz/ mesh-transformer-jax.\n\nReCode: Robustness evaluation of code generation models. Shiqi Wang, Zheng Li, Haifeng Qian, Cheng Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Dan Murali Krishna Ramanathan, Bing Roth, Xiang, arXiv:2212.10264arXiv preprintShiqi Wang, Zheng Li, Haifeng Qian, Cheng Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Sam- son Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2022a. ReCode: Robustness eval- uation of code generation models. arXiv preprint arXiv:2212.10264.\n\nCodeT5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation. Yue Wang, Weishi Wang, Shafiq Joty, C H Steven, Hoi, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. CodeT5: Identifier-aware unified pre- trained encoder-decoder models for code under- standing and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan- guage Processing, pages 8696-8708.\n\nZhiruo Wang, Grace Cuenca, Shuyan Zhou, F Frank, Graham Xu, Neubig, arXiv:2203.08388MCoNaLa: a benchmark for code generation from multiple natural languages. arXiv preprintZhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F Xu, and Graham Neubig. 2022b. MCoNaLa: a benchmark for code generation from multiple natu- ral languages. arXiv preprint arXiv:2203.08388.\n\nDaniel Fried, and Graham Neubig. 2022c. Execution-based evaluation for open-domain code generation. Zhiruo Wang, Shuyan Zhou, arXiv:2212.10481arXiv preprintZhiruo Wang, Shuyan Zhou, Daniel Fried, and Gra- ham Neubig. 2022c. Execution-based evaluation for open-domain code generation. arXiv preprint arXiv:2212.10481.\n\nChain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.\n\nA systematic evaluation of large language models of code. F Frank, Uri Xu, Graham Alon, Vincent J Neubig, Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine ProgrammingFrank F. Xu, Uri Alon, Graham Neubig, and Vincent J. Hellendoorn. 2022. A systematic evaluation of large language models of code. Proceedings of the 6th ACM SIGPLAN International Symposium on Ma- chine Programming.\n\nA survey on pretrained language models for neural code intelligence. Yichen Xu, Yanqiao Zhu, arXiv:2212.10079arXiv preprintYichen Xu and Yanqiao Zhu. 2022. A survey on pre- trained language models for neural code intelligence. arXiv preprint arXiv:2212.10079.\n\nLearning to mine aligned code and natural language pairs from stack overflow. Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, Graham Neubig, IEEE/ACM 15th International Conference on Mining Software Repositories (MSR). Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned code and natural language pairs from stack overflow. 2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR), pages 476-486.\n\nA syntactic neural model for general-purpose code generation. Pengcheng Yin, Graham Neubig, arXiv:1704.01696arXiv preprintPengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. arXiv preprint arXiv:1704.01696.\n\nWhen language model meets private library. Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, Jian-Guang Lou, Conference on Empirical Methods in Natural Language Processing. Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2022a. When language model meets private library. In Conference on Em- pirical Methods in Natural Language Processing.\n\nCERT: Continual pretraining on sketches for library-oriented code generation. Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, Jian-Guang Lou, International Joint Conference on Artificial Intelligence. Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-Guang Lou. 2022b. CERT: Continual pre- training on sketches for library-oriented code gener- ation. In International Joint Conference on Artificial Intelligence.\n\nJialu Zhang, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, arXiv:2209.14876Gustavo Soares, and Gust Verbruggen. 2022. Repairing bugs in python assignments using large language models. arXiv preprintJialu Zhang, Jos\u00e9 Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, and Gust Ver- bruggen. 2022. Repairing bugs in python assign- ments using large language models. arXiv preprint arXiv:2209.14876.\n\nCodeGeeX: A pre-trained model for code generation with multilingual evaluations on humaneval. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shanshan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2023. ArXiv, abs/2303.17568Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan- shan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2023. CodeGeeX: A pre-trained model for code generation with multilingual evaluations on humaneval-x. ArXiv, abs/2303.17568.\n\nDocCoder: Generating code by retrieving and reading docs. Shuyan Zhou, Uri Alon, F Frank, Zhengbao Xu, Graham Jiang, Neubig, The Eleventh International Conference on Learning Representations. Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao JIang, and Graham Neubig. 2023. DocCoder: Generating code by retrieving and reading docs. In The Eleventh International Conference on Learning Representa- tions.\n\nXLCoST: A benchmark dataset for crosslingual code intelligence. Ming Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravindran, Sindhu Tipirneni, Chandan K Reddy, Ming Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravin- dran, Sindhu Tipirneni, and Chandan K. Reddy. 2022a. XLCoST: A benchmark dataset for cross- lingual code intelligence.\n\nMultilingual code snippets training for program translation. Ming Zhu, Karthik Suresh, Chandan K Reddy, Ming Zhu, Karthik Suresh, and Chandan K Reddy. 2022b. Multilingual code snippets training for pro- gram translation.\n\n. Python, Java, Javascript, Typescript, Go, Ruby, Php, C C#, C++, Swift, Rust, Css, Angular, Dart, React, Haskell, Html, Kotlin, Matlab, Sass, Nodejs, C Objective, Scala, Vs Code, Studio, Ide Intellij, Neovim, Sublime, Pycharm, Rider, Android Webstorm, Studio, Emacs, Vim, Phpstorm, Rubymine, Jupyter Datagrip, Notebook, Jupyterlab, Clion, Appcode, Eclipse, Python, Java, Javascript, Typescript, Go, C Php, C++ Vs Code, IntelliJ IDEA. 4Python, Java, Javascript, TypeScript, Go, Ruby, PHP, C#, C, C++, Swift, Perl, Rust, CSS, Angular, Dart, React, Haskell, HTML, Kotlin, Matlab, Sass, NodeJS, Objective C, Scala, VS Code, Visual Studio, IntelliJ IDE, Neovim, Sublime, PyCharm, Rider, WebStorm, Android Studio, Emacs, Vim, PhpStorm, RubyMine, DataGrip, Jupyter Notebook, JupyterLab, Clion, AppCode, Eclipse, GoLand aiXcoder (2018) \u2212 Python, Java, JavaScript, Typescript, Go, PHP, C, C++ VS Code, IntelliJ IDEA, PyCharm, STS3, WebStorm, Rider, Clion, STS4\n\nAndroid Studio, Phpstorm, Goland Eclipse, Intellicode ; \u2212 Python, Java, Javascript, Typescript, C++ C#, Xaml Server, ; Vs Code, Java, Javascript, Typescript, Go, Ruby, Julia, Php, C#, C++, Swift, R Powershell, Rust, Css, Sql, Json, Html, Scss, Less, Net, T-Sql Markdown, ; \u2212 Java Vs Code, Intellij, Visual Studio Diffblue Cover (2020) \u2212 Java IntelliJ IDEA, CLI Tool Copilot (2021) Codex Python. NeovimJetBrains IDE CosyAndroid Studio, PhpStorm, Eclipse, GoLand IntelliCode (2019) \u2212 Python, Java, JavaScript, TypeScript, C#, C++, SQL Server, XAML VS Code, Visual Studio Diffblue Cover (2020) \u2212 Java IntelliJ IDEA, CLI Tool Copilot (2021) Codex Python, Java, JavaScript, TypeScript, Go, Ruby, Julia, PHP, C#, C++, Swift, Perl, PowerShell, R, Rust, CSS, SQL, JSON, HTML, SCSS, Less, .NET, Markdown, T-SQL VS Code, Visual Studio, Neovim, JetBrains IDE Cosy (2022) \u2212 Java IntelliJ IDEA\n\nAWS Lambda CodeGenX (2022) GPT-J Python VS Code CodeGeeX (2023) CodeGeeX Python. Codewhisperer ; \u2212 Python, Java, Javascript, C# Typescript, Vs Code, Ide Jetbrains, Java, Javascript, Typescript, Go, Php, C C#, C++, Perl, Rust, Css, Sql, Html, Kotlin, R Shell, Cuda, C Objective, C++ Objective, Pascal, Tex, Fortran, Lean, V S Scala, Code, IntelliJ IDEA. CodeWhisperer (2022) \u2212 Python, Java, JavaScript, TypeScript, C# VS Code, JetBrains IDE, AWS Cloud9, AWS Lambda CodeGenX (2022) GPT-J Python VS Code CodeGeeX (2023) CodeGeeX Python, Java, JavaScript, TypeScript, Go, PHP, C#, C, C++, Perl, Rust, CSS, SQL, HTML, Kotlin, Shell, R, Cuda, Objective C, Objective C++, Pascal, Tex, Fortran, Lean, Scala VS Code, IntelliJ IDEA, PyCharm, WebStorm, Android Studio, Rider, RubyMine, Clion, AppCode, Aqua, DataGrip, GoLand, DataSpell\n\n. Faupilot ; Codegen Python, Java, Javascript, C Go, FauPilot (2022) CodeGen Python, Java, Javascript, Go, C, C++ \u2212\n", "annotations": {"author": "[{\"end\":197,\"start\":48},{\"end\":255,\"start\":198},{\"end\":322,\"start\":256},{\"end\":385,\"start\":323},{\"end\":494,\"start\":386},{\"end\":595,\"start\":495},{\"end\":715,\"start\":596},{\"end\":776,\"start\":716}]", "publisher": null, "author_last_name": "[{\"end\":60,\"start\":57},{\"end\":206,\"start\":202},{\"end\":268,\"start\":263},{\"end\":333,\"start\":331},{\"end\":397,\"start\":395},{\"end\":503,\"start\":499},{\"end\":607,\"start\":603},{\"end\":730,\"start\":727}]", "author_first_name": "[{\"end\":56,\"start\":48},{\"end\":201,\"start\":198},{\"end\":262,\"start\":256},{\"end\":330,\"start\":323},{\"end\":394,\"start\":386},{\"end\":498,\"start\":495},{\"end\":602,\"start\":596},{\"end\":726,\"start\":716}]", "author_affiliation": "[{\"end\":152,\"start\":72},{\"end\":196,\"start\":154},{\"end\":254,\"start\":230},{\"end\":321,\"start\":297},{\"end\":384,\"start\":357},{\"end\":493,\"start\":413},{\"end\":594,\"start\":514},{\"end\":714,\"start\":634},{\"end\":775,\"start\":751}]", "title": "[{\"end\":45,\"start\":1},{\"end\":821,\"start\":777}]", "venue": null, "abstract": "[{\"end\":1997,\"start\":823}]", "bib_ref": "[{\"end\":2940,\"start\":2916},{\"end\":2958,\"start\":2940},{\"end\":2985,\"start\":2958},{\"end\":3062,\"start\":3034},{\"end\":3076,\"start\":3062},{\"end\":3093,\"start\":3076},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3214,\"start\":3193},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3235,\"start\":3214},{\"end\":3249,\"start\":3235},{\"end\":3292,\"start\":3254},{\"end\":3439,\"start\":3417},{\"end\":3456,\"start\":3439},{\"end\":3481,\"start\":3458},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3498,\"start\":3481},{\"end\":3532,\"start\":3509},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3553,\"start\":3532},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3663,\"start\":3641},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3773,\"start\":3747},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3791,\"start\":3773},{\"end\":4272,\"start\":4247},{\"end\":4759,\"start\":4736},{\"end\":4778,\"start\":4759},{\"end\":4966,\"start\":4948},{\"end\":5060,\"start\":5040},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5195,\"start\":5173},{\"end\":5232,\"start\":5197},{\"end\":5274,\"start\":5234},{\"end\":5305,\"start\":5285},{\"end\":5342,\"start\":5322},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5530,\"start\":5510},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5552,\"start\":5530},{\"end\":5570,\"start\":5552},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7371,\"start\":7349},{\"end\":7834,\"start\":7813},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7880,\"start\":7858},{\"end\":8904,\"start\":8876},{\"end\":8937,\"start\":8910},{\"end\":9123,\"start\":9095},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9161,\"start\":9134},{\"end\":9461,\"start\":9438},{\"end\":9496,\"start\":9461},{\"end\":9841,\"start\":9812},{\"end\":9879,\"start\":9859},{\"end\":10157,\"start\":10138},{\"end\":10193,\"start\":10168},{\"end\":10218,\"start\":10193},{\"end\":10327,\"start\":10307},{\"end\":10356,\"start\":10329},{\"end\":10393,\"start\":10373},{\"end\":10744,\"start\":10725},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10765,\"start\":10744},{\"end\":10782,\"start\":10765},{\"end\":10881,\"start\":10858},{\"end\":10900,\"start\":10881},{\"end\":11195,\"start\":11172},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11215,\"start\":11195},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11237,\"start\":11215},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13109,\"start\":13087},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13132,\"start\":13109},{\"end\":13155,\"start\":13132},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":15060,\"start\":15041},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15093,\"start\":15074},{\"end\":15215,\"start\":15189},{\"end\":15245,\"start\":15217},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15285,\"start\":15257},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15623,\"start\":15601},{\"end\":15654,\"start\":15625},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15689,\"start\":15670},{\"end\":16612,\"start\":16583},{\"end\":16847,\"start\":16828},{\"end\":18788,\"start\":18768},{\"end\":18825,\"start\":18803},{\"end\":18900,\"start\":18880},{\"end\":19124,\"start\":19106},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19160,\"start\":19141},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19197,\"start\":19178},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19250,\"start\":19228},{\"end\":19938,\"start\":19914},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19986,\"start\":19963},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20185,\"start\":20162},{\"end\":20209,\"start\":20187},{\"end\":20236,\"start\":20209},{\"end\":21401,\"start\":21381},{\"end\":21412,\"start\":21401},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21639,\"start\":21621},{\"end\":22282,\"start\":22263},{\"end\":22299,\"start\":22282},{\"end\":22725,\"start\":22703},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23538,\"start\":23519},{\"end\":23972,\"start\":23946},{\"end\":24178,\"start\":24158},{\"end\":24200,\"start\":24178},{\"end\":24411,\"start\":24393},{\"end\":24529,\"start\":24509},{\"end\":25854,\"start\":25836},{\"end\":25894,\"start\":25859},{\"end\":26399,\"start\":26375},{\"end\":26415,\"start\":26399},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26432,\"start\":26415},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26449,\"start\":26432},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26491,\"start\":26470},{\"end\":26510,\"start\":26491},{\"end\":26532,\"start\":26510},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26642,\"start\":26620},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26667,\"start\":26642},{\"end\":26749,\"start\":26697},{\"end\":26769,\"start\":26749},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26872,\"start\":26846},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26890,\"start\":26872},{\"end\":29207,\"start\":29188},{\"end\":30164,\"start\":30158},{\"end\":30167,\"start\":30165},{\"end\":30725,\"start\":30682},{\"end\":30755,\"start\":30726}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31875,\"start\":31767},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32256,\"start\":31876},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32337,\"start\":32257},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32436,\"start\":32338},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32536,\"start\":32437},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33728,\"start\":32537},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34075,\"start\":33729},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34794,\"start\":34076},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34849,\"start\":34795},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":35780,\"start\":34850},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":36754,\"start\":35781},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":37786,\"start\":36755},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":37989,\"start\":37787},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":39082,\"start\":37990}]", "paragraph": "[{\"end\":2789,\"start\":2013},{\"end\":3956,\"start\":2791},{\"end\":6877,\"start\":3958},{\"end\":8450,\"start\":6915},{\"end\":10674,\"start\":8820},{\"end\":11481,\"start\":10676},{\"end\":12846,\"start\":11513},{\"end\":13675,\"start\":12867},{\"end\":14533,\"start\":13677},{\"end\":14890,\"start\":14560},{\"end\":16189,\"start\":14892},{\"end\":17704,\"start\":16207},{\"end\":17969,\"start\":17731},{\"end\":18650,\"start\":17971},{\"end\":19630,\"start\":18652},{\"end\":20496,\"start\":19632},{\"end\":20921,\"start\":20529},{\"end\":21640,\"start\":20923},{\"end\":22400,\"start\":21642},{\"end\":23025,\"start\":22402},{\"end\":23766,\"start\":23027},{\"end\":24598,\"start\":23768},{\"end\":25097,\"start\":24600},{\"end\":27509,\"start\":25113},{\"end\":27945,\"start\":27524},{\"end\":28541,\"start\":27969},{\"end\":28579,\"start\":28551},{\"end\":28853,\"start\":28604},{\"end\":29586,\"start\":28882},{\"end\":30110,\"start\":29617},{\"end\":30449,\"start\":30112},{\"end\":31766,\"start\":30451}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8819,\"start\":8451}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":8051,\"start\":8044},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":12899,\"start\":12892},{\"end\":16378,\"start\":16371},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":18029,\"start\":18022},{\"end\":19009,\"start\":19002},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":19629,\"start\":19622},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28799,\"start\":28792},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":28851,\"start\":28844},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":29689,\"start\":29682},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":30180,\"start\":30173},{\"end\":31051,\"start\":31044},{\"end\":31268,\"start\":31261}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2011,\"start\":1999},{\"attributes\":{\"n\":\"2\"},\"end\":6913,\"start\":6880},{\"attributes\":{\"n\":\"3\"},\"end\":11511,\"start\":11484},{\"attributes\":{\"n\":\"3.1\"},\"end\":12865,\"start\":12849},{\"attributes\":{\"n\":\"3.2\"},\"end\":14558,\"start\":14536},{\"attributes\":{\"n\":\"3.3\"},\"end\":16205,\"start\":16192},{\"attributes\":{\"n\":\"4\"},\"end\":17729,\"start\":17707},{\"attributes\":{\"n\":\"5\"},\"end\":20527,\"start\":20499},{\"end\":25111,\"start\":25100},{\"end\":27522,\"start\":27512},{\"end\":27967,\"start\":27948},{\"end\":28549,\"start\":28544},{\"end\":28602,\"start\":28582},{\"end\":28880,\"start\":28856},{\"end\":29615,\"start\":29589},{\"end\":31778,\"start\":31768},{\"end\":31887,\"start\":31877},{\"end\":32268,\"start\":32258},{\"end\":32349,\"start\":32339},{\"end\":32448,\"start\":32438},{\"end\":33739,\"start\":33730},{\"end\":34084,\"start\":34077},{\"end\":34805,\"start\":34796},{\"end\":35791,\"start\":35782},{\"end\":37797,\"start\":37788}]", "table": "[{\"end\":33728,\"start\":32584},{\"end\":34075,\"start\":33781},{\"end\":34794,\"start\":34221},{\"end\":34849,\"start\":34846},{\"end\":36754,\"start\":36223},{\"end\":37786,\"start\":36840},{\"end\":39082,\"start\":39026}]", "figure_caption": "[{\"end\":31875,\"start\":31780},{\"end\":32256,\"start\":31889},{\"end\":32337,\"start\":32270},{\"end\":32436,\"start\":32351},{\"end\":32536,\"start\":32450},{\"end\":32584,\"start\":32539},{\"end\":33781,\"start\":33741},{\"end\":34221,\"start\":34086},{\"end\":34846,\"start\":34807},{\"end\":35780,\"start\":34852},{\"end\":36223,\"start\":35793},{\"end\":36840,\"start\":36757},{\"end\":37989,\"start\":37799},{\"end\":39026,\"start\":37992}]", "figure_ref": "[{\"end\":4479,\"start\":4471},{\"end\":7127,\"start\":7119},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8197,\"start\":8189},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12887,\"start\":12879},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13243,\"start\":13234},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":13638,\"start\":13630},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14020,\"start\":14011},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14258,\"start\":14249},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17153,\"start\":17145},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17281,\"start\":17273},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27904,\"start\":27896},{\"end\":30797,\"start\":30789}]", "bib_author_first_name": "[{\"end\":39160,\"start\":39150},{\"end\":39175,\"start\":39169},{\"end\":39454,\"start\":39447},{\"end\":39470,\"start\":39464},{\"end\":39481,\"start\":39477},{\"end\":39496,\"start\":39490},{\"end\":39506,\"start\":39502},{\"end\":39518,\"start\":39514},{\"end\":40063,\"start\":40056},{\"end\":40079,\"start\":40073},{\"end\":40089,\"start\":40086},{\"end\":40101,\"start\":40097},{\"end\":40117,\"start\":40113},{\"end\":40130,\"start\":40124},{\"end\":40672,\"start\":40665},{\"end\":40692,\"start\":40686},{\"end\":40711,\"start\":40705},{\"end\":40725,\"start\":40720},{\"end\":40731,\"start\":40726},{\"end\":40746,\"start\":40741},{\"end\":40764,\"start\":40759},{\"end\":40782,\"start\":40774},{\"end\":41296,\"start\":41290},{\"end\":41313,\"start\":41310},{\"end\":41322,\"start\":41318},{\"end\":41333,\"start\":41329},{\"end\":41348,\"start\":41343},{\"end\":41356,\"start\":41349},{\"end\":41367,\"start\":41362},{\"end\":41382,\"start\":41376},{\"end\":41838,\"start\":41834},{\"end\":41852,\"start\":41847},{\"end\":42257,\"start\":42254},{\"end\":42262,\"start\":42258},{\"end\":42293,\"start\":42290},{\"end\":42306,\"start\":42300},{\"end\":42794,\"start\":42790},{\"end\":42806,\"start\":42804},{\"end\":42820,\"start\":42813},{\"end\":42834,\"start\":42830},{\"end\":42843,\"start\":42839},{\"end\":42856,\"start\":42850},{\"end\":42869,\"start\":42863},{\"end\":42887,\"start\":42880},{\"end\":43307,\"start\":43303},{\"end\":43320,\"start\":43316},{\"end\":43327,\"start\":43325},{\"end\":43340,\"start\":43335},{\"end\":43357,\"start\":43350},{\"end\":43359,\"start\":43358},{\"end\":43378,\"start\":43372},{\"end\":43393,\"start\":43388},{\"end\":43409,\"start\":43401},{\"end\":43427,\"start\":43419},{\"end\":43439,\"start\":43435},{\"end\":43449,\"start\":43445},{\"end\":43465,\"start\":43460},{\"end\":43480,\"start\":43474},{\"end\":43493,\"start\":43489},{\"end\":43495,\"start\":43494},{\"end\":43510,\"start\":43504},{\"end\":43525,\"start\":43519},{\"end\":43539,\"start\":43534},{\"end\":43554,\"start\":43550},{\"end\":43562,\"start\":43555},{\"end\":43578,\"start\":43575},{\"end\":43590,\"start\":43586},{\"end\":43592,\"start\":43591},{\"end\":44091,\"start\":44084},{\"end\":44107,\"start\":44102},{\"end\":44120,\"start\":44116},{\"end\":44135,\"start\":44127},{\"end\":44623,\"start\":44618},{\"end\":44637,\"start\":44632},{\"end\":44658,\"start\":44649},{\"end\":44671,\"start\":44666},{\"end\":44687,\"start\":44680},{\"end\":44703,\"start\":44697},{\"end\":45059,\"start\":45052},{\"end\":45075,\"start\":45068},{\"end\":45091,\"start\":45083},{\"end\":45104,\"start\":45097},{\"end\":45125,\"start\":45119},{\"end\":45511,\"start\":45507},{\"end\":45526,\"start\":45520},{\"end\":45781,\"start\":45777},{\"end\":45795,\"start\":45791},{\"end\":45805,\"start\":45800},{\"end\":45818,\"start\":45813},{\"end\":45830,\"start\":45825},{\"end\":45843,\"start\":45839},{\"end\":46109,\"start\":46100},{\"end\":46127,\"start\":46120},{\"end\":46139,\"start\":46132},{\"end\":46401,\"start\":46394},{\"end\":46417,\"start\":46411},{\"end\":46430,\"start\":46426},{\"end\":46909,\"start\":46905},{\"end\":46925,\"start\":46920},{\"end\":46935,\"start\":46931},{\"end\":46946,\"start\":46940},{\"end\":46957,\"start\":46953},{\"end\":46967,\"start\":46963},{\"end\":46978,\"start\":46974},{\"end\":46999,\"start\":46991},{\"end\":47011,\"start\":47006},{\"end\":47344,\"start\":47337},{\"end\":47364,\"start\":47355},{\"end\":47374,\"start\":47370},{\"end\":47385,\"start\":47379},{\"end\":47397,\"start\":47393},{\"end\":47414,\"start\":47406},{\"end\":47651,\"start\":47645},{\"end\":47678,\"start\":47667},{\"end\":47689,\"start\":47684},{\"end\":47703,\"start\":47697},{\"end\":47719,\"start\":47713},{\"end\":47731,\"start\":47726},{\"end\":47750,\"start\":47741},{\"end\":47756,\"start\":47751},{\"end\":47775,\"start\":47767},{\"end\":48259,\"start\":48255},{\"end\":48271,\"start\":48266},{\"end\":48285,\"start\":48280},{\"end\":48577,\"start\":48571},{\"end\":48591,\"start\":48586},{\"end\":48610,\"start\":48602},{\"end\":48626,\"start\":48621},{\"end\":48643,\"start\":48635},{\"end\":49011,\"start\":49005},{\"end\":49031,\"start\":49021},{\"end\":49046,\"start\":49040},{\"end\":49388,\"start\":49383},{\"end\":49406,\"start\":49400},{\"end\":49409,\"start\":49407},{\"end\":49424,\"start\":49419},{\"end\":49435,\"start\":49431},{\"end\":49451,\"start\":49445},{\"end\":49482,\"start\":49476},{\"end\":49496,\"start\":49490},{\"end\":49508,\"start\":49502},{\"end\":49516,\"start\":49514},{\"end\":49869,\"start\":49863},{\"end\":49883,\"start\":49879},{\"end\":49897,\"start\":49893},{\"end\":49911,\"start\":49906},{\"end\":49928,\"start\":49923},{\"end\":49941,\"start\":49936},{\"end\":49943,\"start\":49942},{\"end\":49957,\"start\":49951},{\"end\":49971,\"start\":49966},{\"end\":50270,\"start\":50267},{\"end\":50280,\"start\":50276},{\"end\":50290,\"start\":50287},{\"end\":50305,\"start\":50297},{\"end\":50317,\"start\":50310},{\"end\":50328,\"start\":50324},{\"end\":50341,\"start\":50333},{\"end\":50865,\"start\":50862},{\"end\":50876,\"start\":50872},{\"end\":51109,\"start\":51104},{\"end\":51121,\"start\":51116},{\"end\":51133,\"start\":51126},{\"end\":51145,\"start\":51140},{\"end\":51158,\"start\":51152},{\"end\":51172,\"start\":51165},{\"end\":51185,\"start\":51180},{\"end\":51199,\"start\":51193},{\"end\":51214,\"start\":51205},{\"end\":51229,\"start\":51220},{\"end\":51244,\"start\":51238},{\"end\":51259,\"start\":51256},{\"end\":51291,\"start\":51287},{\"end\":51751,\"start\":51748},{\"end\":51764,\"start\":51758},{\"end\":51777,\"start\":51771},{\"end\":51785,\"start\":51784},{\"end\":51787,\"start\":51786},{\"end\":52248,\"start\":52242},{\"end\":52260,\"start\":52255},{\"end\":52275,\"start\":52269},{\"end\":52283,\"start\":52282},{\"end\":52297,\"start\":52291},{\"end\":52708,\"start\":52702},{\"end\":52721,\"start\":52715},{\"end\":52996,\"start\":52991},{\"end\":53008,\"start\":53002},{\"end\":53019,\"start\":53015},{\"end\":53039,\"start\":53032},{\"end\":53049,\"start\":53047},{\"end\":53059,\"start\":53055},{\"end\":53069,\"start\":53064},{\"end\":53365,\"start\":53364},{\"end\":53376,\"start\":53373},{\"end\":53387,\"start\":53381},{\"end\":53401,\"start\":53394},{\"end\":53403,\"start\":53402},{\"end\":53865,\"start\":53859},{\"end\":53877,\"start\":53870},{\"end\":54138,\"start\":54129},{\"end\":54149,\"start\":54144},{\"end\":54161,\"start\":54156},{\"end\":54174,\"start\":54168},{\"end\":54192,\"start\":54186},{\"end\":54609,\"start\":54600},{\"end\":54621,\"start\":54615},{\"end\":54846,\"start\":54838},{\"end\":54855,\"start\":54852},{\"end\":54866,\"start\":54862},{\"end\":54875,\"start\":54872},{\"end\":54888,\"start\":54882},{\"end\":54905,\"start\":54895},{\"end\":55258,\"start\":55250},{\"end\":55267,\"start\":55264},{\"end\":55280,\"start\":55274},{\"end\":55291,\"start\":55287},{\"end\":55302,\"start\":55297},{\"end\":55311,\"start\":55308},{\"end\":55324,\"start\":55318},{\"end\":55337,\"start\":55331},{\"end\":55354,\"start\":55344},{\"end\":55690,\"start\":55685},{\"end\":55702,\"start\":55698},{\"end\":55720,\"start\":55715},{\"end\":55732,\"start\":55730},{\"end\":55743,\"start\":55737},{\"end\":56205,\"start\":56199},{\"end\":56217,\"start\":56213},{\"end\":56225,\"start\":56223},{\"end\":56237,\"start\":56231},{\"end\":56252,\"start\":56244},{\"end\":56264,\"start\":56259},{\"end\":56277,\"start\":56270},{\"end\":56287,\"start\":56284},{\"end\":56298,\"start\":56294},{\"end\":56309,\"start\":56305},{\"end\":56318,\"start\":56314},{\"end\":56720,\"start\":56714},{\"end\":56730,\"start\":56727},{\"end\":56738,\"start\":56737},{\"end\":56754,\"start\":56746},{\"end\":56765,\"start\":56759},{\"end\":57123,\"start\":57119},{\"end\":57135,\"start\":57129},{\"end\":57149,\"start\":57142},{\"end\":57164,\"start\":57158},{\"end\":57182,\"start\":57176},{\"end\":57201,\"start\":57194},{\"end\":57203,\"start\":57202},{\"end\":57449,\"start\":57445},{\"end\":57462,\"start\":57455},{\"end\":57480,\"start\":57471},{\"end\":57662,\"start\":57661},{\"end\":57758,\"start\":57757},{\"end\":57797,\"start\":57794},{\"end\":57848,\"start\":57841},{\"end\":57906,\"start\":57899},{\"end\":58007,\"start\":58006},{\"end\":58016,\"start\":58013},{\"end\":58567,\"start\":58560},{\"end\":58592,\"start\":58586},{\"end\":58659,\"start\":58656},{\"end\":58668,\"start\":58664},{\"end\":58678,\"start\":58677},{\"end\":58757,\"start\":58756},{\"end\":58820,\"start\":58815},{\"end\":58839,\"start\":58831},{\"end\":59569,\"start\":59567},{\"end\":59594,\"start\":59591},{\"end\":59646,\"start\":59645},{\"end\":59693,\"start\":59692},{\"end\":59708,\"start\":59707},{\"end\":59723,\"start\":59720},{\"end\":59764,\"start\":59763},{\"end\":59766,\"start\":59765},{\"end\":60316,\"start\":60315}]", "bib_author_last_name": "[{\"end\":39167,\"start\":39161},{\"end\":39183,\"start\":39176},{\"end\":39462,\"start\":39455},{\"end\":39475,\"start\":39471},{\"end\":39488,\"start\":39482},{\"end\":39500,\"start\":39497},{\"end\":39512,\"start\":39507},{\"end\":39524,\"start\":39519},{\"end\":40071,\"start\":40064},{\"end\":40084,\"start\":40080},{\"end\":40095,\"start\":40090},{\"end\":40111,\"start\":40102},{\"end\":40122,\"start\":40118},{\"end\":40136,\"start\":40131},{\"end\":40684,\"start\":40673},{\"end\":40703,\"start\":40693},{\"end\":40718,\"start\":40712},{\"end\":40739,\"start\":40732},{\"end\":40757,\"start\":40747},{\"end\":40772,\"start\":40765},{\"end\":40789,\"start\":40783},{\"end\":41208,\"start\":41199},{\"end\":41308,\"start\":41297},{\"end\":41316,\"start\":41314},{\"end\":41327,\"start\":41323},{\"end\":41341,\"start\":41334},{\"end\":41360,\"start\":41357},{\"end\":41374,\"start\":41368},{\"end\":41387,\"start\":41383},{\"end\":41845,\"start\":41839},{\"end\":41857,\"start\":41853},{\"end\":42280,\"start\":42263},{\"end\":42288,\"start\":42282},{\"end\":42298,\"start\":42294},{\"end\":42313,\"start\":42307},{\"end\":42321,\"start\":42315},{\"end\":42802,\"start\":42795},{\"end\":42811,\"start\":42807},{\"end\":42828,\"start\":42821},{\"end\":42837,\"start\":42835},{\"end\":42848,\"start\":42844},{\"end\":42861,\"start\":42857},{\"end\":42878,\"start\":42870},{\"end\":42893,\"start\":42888},{\"end\":43314,\"start\":43308},{\"end\":43323,\"start\":43321},{\"end\":43333,\"start\":43328},{\"end\":43348,\"start\":43341},{\"end\":43370,\"start\":43360},{\"end\":43386,\"start\":43379},{\"end\":43399,\"start\":43394},{\"end\":43417,\"start\":43410},{\"end\":43433,\"start\":43428},{\"end\":43443,\"start\":43440},{\"end\":43458,\"start\":43450},{\"end\":43472,\"start\":43466},{\"end\":43487,\"start\":43481},{\"end\":43502,\"start\":43496},{\"end\":43517,\"start\":43511},{\"end\":43532,\"start\":43526},{\"end\":43548,\"start\":43540},{\"end\":43573,\"start\":43563},{\"end\":43584,\"start\":43579},{\"end\":43597,\"start\":43593},{\"end\":44100,\"start\":44092},{\"end\":44114,\"start\":44108},{\"end\":44125,\"start\":44121},{\"end\":44139,\"start\":44136},{\"end\":44630,\"start\":44624},{\"end\":44647,\"start\":44638},{\"end\":44664,\"start\":44659},{\"end\":44678,\"start\":44672},{\"end\":44695,\"start\":44688},{\"end\":44711,\"start\":44704},{\"end\":45066,\"start\":45060},{\"end\":45081,\"start\":45076},{\"end\":45095,\"start\":45092},{\"end\":45117,\"start\":45105},{\"end\":45131,\"start\":45126},{\"end\":45518,\"start\":45512},{\"end\":45534,\"start\":45527},{\"end\":45542,\"start\":45536},{\"end\":45789,\"start\":45782},{\"end\":45798,\"start\":45796},{\"end\":45811,\"start\":45806},{\"end\":45823,\"start\":45819},{\"end\":45837,\"start\":45831},{\"end\":45853,\"start\":45844},{\"end\":46118,\"start\":46110},{\"end\":46130,\"start\":46128},{\"end\":46148,\"start\":46140},{\"end\":46409,\"start\":46402},{\"end\":46424,\"start\":46418},{\"end\":46436,\"start\":46431},{\"end\":46918,\"start\":46910},{\"end\":46929,\"start\":46926},{\"end\":46938,\"start\":46936},{\"end\":46951,\"start\":46947},{\"end\":46961,\"start\":46958},{\"end\":46972,\"start\":46968},{\"end\":46989,\"start\":46979},{\"end\":47004,\"start\":47000},{\"end\":47018,\"start\":47012},{\"end\":47022,\"start\":47020},{\"end\":47353,\"start\":47345},{\"end\":47368,\"start\":47365},{\"end\":47377,\"start\":47375},{\"end\":47391,\"start\":47386},{\"end\":47404,\"start\":47398},{\"end\":47419,\"start\":47415},{\"end\":47665,\"start\":47652},{\"end\":47682,\"start\":47679},{\"end\":47695,\"start\":47690},{\"end\":47711,\"start\":47704},{\"end\":47724,\"start\":47720},{\"end\":47739,\"start\":47732},{\"end\":47765,\"start\":47757},{\"end\":47784,\"start\":47776},{\"end\":47790,\"start\":47786},{\"end\":48264,\"start\":48260},{\"end\":48278,\"start\":48272},{\"end\":48297,\"start\":48286},{\"end\":48584,\"start\":48578},{\"end\":48600,\"start\":48592},{\"end\":48619,\"start\":48611},{\"end\":48633,\"start\":48627},{\"end\":48649,\"start\":48644},{\"end\":49019,\"start\":49012},{\"end\":49038,\"start\":49032},{\"end\":49055,\"start\":49047},{\"end\":49398,\"start\":49389},{\"end\":49417,\"start\":49410},{\"end\":49429,\"start\":49425},{\"end\":49443,\"start\":49436},{\"end\":49464,\"start\":49452},{\"end\":49474,\"start\":49466},{\"end\":49488,\"start\":49483},{\"end\":49500,\"start\":49497},{\"end\":49512,\"start\":49509},{\"end\":49522,\"start\":49517},{\"end\":49526,\"start\":49524},{\"end\":49877,\"start\":49870},{\"end\":49891,\"start\":49884},{\"end\":49904,\"start\":49898},{\"end\":49921,\"start\":49912},{\"end\":49934,\"start\":49929},{\"end\":49949,\"start\":49944},{\"end\":49964,\"start\":49958},{\"end\":49982,\"start\":49972},{\"end\":50274,\"start\":50271},{\"end\":50285,\"start\":50281},{\"end\":50295,\"start\":50291},{\"end\":50308,\"start\":50306},{\"end\":50322,\"start\":50318},{\"end\":50331,\"start\":50329},{\"end\":50344,\"start\":50342},{\"end\":50870,\"start\":50866},{\"end\":50888,\"start\":50877},{\"end\":51114,\"start\":51110},{\"end\":51124,\"start\":51122},{\"end\":51138,\"start\":51134},{\"end\":51150,\"start\":51146},{\"end\":51163,\"start\":51159},{\"end\":51178,\"start\":51173},{\"end\":51191,\"start\":51186},{\"end\":51203,\"start\":51200},{\"end\":51218,\"start\":51215},{\"end\":51236,\"start\":51230},{\"end\":51254,\"start\":51245},{\"end\":51285,\"start\":51260},{\"end\":51296,\"start\":51292},{\"end\":51303,\"start\":51298},{\"end\":51756,\"start\":51752},{\"end\":51769,\"start\":51765},{\"end\":51782,\"start\":51778},{\"end\":51794,\"start\":51788},{\"end\":51799,\"start\":51796},{\"end\":52253,\"start\":52249},{\"end\":52267,\"start\":52261},{\"end\":52280,\"start\":52276},{\"end\":52289,\"start\":52284},{\"end\":52300,\"start\":52298},{\"end\":52308,\"start\":52302},{\"end\":52713,\"start\":52709},{\"end\":52726,\"start\":52722},{\"end\":53000,\"start\":52997},{\"end\":53013,\"start\":53009},{\"end\":53030,\"start\":53020},{\"end\":53045,\"start\":53040},{\"end\":53053,\"start\":53050},{\"end\":53062,\"start\":53060},{\"end\":53074,\"start\":53070},{\"end\":53371,\"start\":53366},{\"end\":53379,\"start\":53377},{\"end\":53392,\"start\":53388},{\"end\":53410,\"start\":53404},{\"end\":53423,\"start\":53412},{\"end\":53868,\"start\":53866},{\"end\":53881,\"start\":53878},{\"end\":54142,\"start\":54139},{\"end\":54154,\"start\":54150},{\"end\":54166,\"start\":54162},{\"end\":54184,\"start\":54175},{\"end\":54199,\"start\":54193},{\"end\":54613,\"start\":54610},{\"end\":54628,\"start\":54622},{\"end\":54850,\"start\":54847},{\"end\":54860,\"start\":54856},{\"end\":54870,\"start\":54867},{\"end\":54880,\"start\":54876},{\"end\":54893,\"start\":54889},{\"end\":54909,\"start\":54906},{\"end\":55262,\"start\":55259},{\"end\":55272,\"start\":55268},{\"end\":55285,\"start\":55281},{\"end\":55295,\"start\":55292},{\"end\":55306,\"start\":55303},{\"end\":55316,\"start\":55312},{\"end\":55329,\"start\":55325},{\"end\":55342,\"start\":55338},{\"end\":55358,\"start\":55355},{\"end\":55696,\"start\":55691},{\"end\":55713,\"start\":55703},{\"end\":55728,\"start\":55721},{\"end\":55735,\"start\":55733},{\"end\":55750,\"start\":55744},{\"end\":56211,\"start\":56206},{\"end\":56221,\"start\":56218},{\"end\":56229,\"start\":56226},{\"end\":56242,\"start\":56238},{\"end\":56257,\"start\":56253},{\"end\":56268,\"start\":56265},{\"end\":56282,\"start\":56278},{\"end\":56292,\"start\":56288},{\"end\":56303,\"start\":56299},{\"end\":56312,\"start\":56310},{\"end\":56321,\"start\":56319},{\"end\":56725,\"start\":56721},{\"end\":56735,\"start\":56731},{\"end\":56744,\"start\":56739},{\"end\":56757,\"start\":56755},{\"end\":56771,\"start\":56766},{\"end\":56779,\"start\":56773},{\"end\":57127,\"start\":57124},{\"end\":57140,\"start\":57136},{\"end\":57156,\"start\":57150},{\"end\":57174,\"start\":57165},{\"end\":57192,\"start\":57183},{\"end\":57209,\"start\":57204},{\"end\":57453,\"start\":57450},{\"end\":57469,\"start\":57463},{\"end\":57486,\"start\":57481},{\"end\":57614,\"start\":57608},{\"end\":57620,\"start\":57616},{\"end\":57632,\"start\":57622},{\"end\":57644,\"start\":57634},{\"end\":57648,\"start\":57646},{\"end\":57654,\"start\":57650},{\"end\":57659,\"start\":57656},{\"end\":57665,\"start\":57663},{\"end\":57670,\"start\":57667},{\"end\":57677,\"start\":57672},{\"end\":57683,\"start\":57679},{\"end\":57688,\"start\":57685},{\"end\":57697,\"start\":57690},{\"end\":57703,\"start\":57699},{\"end\":57710,\"start\":57705},{\"end\":57719,\"start\":57712},{\"end\":57725,\"start\":57721},{\"end\":57733,\"start\":57727},{\"end\":57741,\"start\":57735},{\"end\":57747,\"start\":57743},{\"end\":57755,\"start\":57749},{\"end\":57768,\"start\":57759},{\"end\":57775,\"start\":57770},{\"end\":57784,\"start\":57777},{\"end\":57792,\"start\":57786},{\"end\":57806,\"start\":57798},{\"end\":57814,\"start\":57808},{\"end\":57823,\"start\":57816},{\"end\":57832,\"start\":57825},{\"end\":57839,\"start\":57834},{\"end\":57857,\"start\":57849},{\"end\":57865,\"start\":57859},{\"end\":57872,\"start\":57867},{\"end\":57877,\"start\":57874},{\"end\":57887,\"start\":57879},{\"end\":57897,\"start\":57889},{\"end\":57915,\"start\":57907},{\"end\":57925,\"start\":57917},{\"end\":57937,\"start\":57927},{\"end\":57944,\"start\":57939},{\"end\":57953,\"start\":57946},{\"end\":57962,\"start\":57955},{\"end\":57970,\"start\":57964},{\"end\":57976,\"start\":57972},{\"end\":57988,\"start\":57978},{\"end\":58000,\"start\":57990},{\"end\":58004,\"start\":58002},{\"end\":58011,\"start\":58008},{\"end\":58024,\"start\":58017},{\"end\":58574,\"start\":58568},{\"end\":58584,\"start\":58576},{\"end\":58600,\"start\":58593},{\"end\":58624,\"start\":58602},{\"end\":58630,\"start\":58626},{\"end\":58642,\"start\":58632},{\"end\":58654,\"start\":58644},{\"end\":58662,\"start\":58660},{\"end\":58675,\"start\":58669},{\"end\":58686,\"start\":58679},{\"end\":58692,\"start\":58688},{\"end\":58704,\"start\":58694},{\"end\":58716,\"start\":58706},{\"end\":58720,\"start\":58718},{\"end\":58726,\"start\":58722},{\"end\":58733,\"start\":58728},{\"end\":58738,\"start\":58735},{\"end\":58742,\"start\":58740},{\"end\":58747,\"start\":58744},{\"end\":58754,\"start\":58749},{\"end\":58768,\"start\":58758},{\"end\":58774,\"start\":58770},{\"end\":58779,\"start\":58776},{\"end\":58784,\"start\":58781},{\"end\":58790,\"start\":58786},{\"end\":58796,\"start\":58792},{\"end\":58802,\"start\":58798},{\"end\":58808,\"start\":58804},{\"end\":58813,\"start\":58810},{\"end\":58829,\"start\":58821},{\"end\":58847,\"start\":58840},{\"end\":58857,\"start\":58849},{\"end\":59547,\"start\":59523},{\"end\":59553,\"start\":59549},{\"end\":59565,\"start\":59555},{\"end\":59580,\"start\":59570},{\"end\":59589,\"start\":59582},{\"end\":59604,\"start\":59595},{\"end\":59610,\"start\":59606},{\"end\":59622,\"start\":59612},{\"end\":59634,\"start\":59624},{\"end\":59638,\"start\":59636},{\"end\":59643,\"start\":59640},{\"end\":59649,\"start\":59647},{\"end\":59654,\"start\":59651},{\"end\":59660,\"start\":59656},{\"end\":59666,\"start\":59662},{\"end\":59671,\"start\":59668},{\"end\":59676,\"start\":59673},{\"end\":59682,\"start\":59678},{\"end\":59690,\"start\":59684},{\"end\":59699,\"start\":59694},{\"end\":59705,\"start\":59701},{\"end\":59718,\"start\":59709},{\"end\":59733,\"start\":59724},{\"end\":59741,\"start\":59735},{\"end\":59746,\"start\":59743},{\"end\":59755,\"start\":59748},{\"end\":59761,\"start\":59757},{\"end\":59772,\"start\":59767},{\"end\":59778,\"start\":59774},{\"end\":60295,\"start\":60270},{\"end\":60301,\"start\":60297},{\"end\":60313,\"start\":60303},{\"end\":60319,\"start\":60317}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":19793550},\"end\":39445,\"start\":39108},{\"attributes\":{\"doi\":\"arXiv:2211.02265\",\"id\":\"b1\"},\"end\":39979,\"start\":39447},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":251322329},\"end\":40576,\"start\":39981},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":231786586},\"end\":41195,\"start\":40578},{\"attributes\":{\"id\":\"b4\"},\"end\":41286,\"start\":41197},{\"attributes\":{\"doi\":\"arXiv:2201.10005\",\"id\":\"b5\"},\"end\":41770,\"start\":41288},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":249929100},\"end\":42197,\"start\":41772},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":17169661},\"end\":42706,\"start\":42199},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":252668917},\"end\":43232,\"start\":42708},{\"attributes\":{\"doi\":\"abs/2203.02155\",\"id\":\"b9\"},\"end\":44018,\"start\":43234},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":11080756},\"end\":44544,\"start\":44020},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":146047325},\"end\":44963,\"start\":44546},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":245220588},\"end\":45438,\"start\":44965},{\"attributes\":{\"doi\":\"arXiv:2111.03922\",\"id\":\"b13\"},\"end\":45722,\"start\":45440},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":160025533},\"end\":46032,\"start\":45724},{\"attributes\":{\"doi\":\"arXiv:2204.00498\",\"id\":\"b15\"},\"end\":46342,\"start\":46034},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":13040187},\"end\":46840,\"start\":46344},{\"attributes\":{\"doi\":\"arXiv:2009.10297\",\"id\":\"b17\"},\"end\":47280,\"start\":46842},{\"attributes\":{\"doi\":\"arXiv:2206.05802\",\"id\":\"b18\"},\"end\":47643,\"start\":47282},{\"attributes\":{\"doi\":\"arXiv:2211.05100\",\"id\":\"b19\"},\"end\":48194,\"start\":47645},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":236919724},\"end\":48569,\"start\":48196},{\"attributes\":{\"doi\":\"arXiv:2110.09610\",\"id\":\"b21\"},\"end\":48943,\"start\":48571},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18675713},\"end\":49274,\"start\":48945},{\"attributes\":{\"id\":\"b23\"},\"end\":49333,\"start\":49276},{\"attributes\":{\"doi\":\"arXiv:2201.08239\",\"id\":\"b24\"},\"end\":49795,\"start\":49335},{\"attributes\":{\"id\":\"b25\"},\"end\":50186,\"start\":49797},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":52069701},\"end\":50797,\"start\":50188},{\"attributes\":{\"id\":\"b27\"},\"end\":51045,\"start\":50799},{\"attributes\":{\"doi\":\"arXiv:2212.10264\",\"id\":\"b28\"},\"end\":51640,\"start\":51047},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":237386541},\"end\":52240,\"start\":51642},{\"attributes\":{\"doi\":\"arXiv:2203.08388\",\"id\":\"b30\"},\"end\":52600,\"start\":52242},{\"attributes\":{\"doi\":\"arXiv:2212.10481\",\"id\":\"b31\"},\"end\":52918,\"start\":52602},{\"attributes\":{\"doi\":\"arXiv:2201.11903\",\"id\":\"b32\"},\"end\":53304,\"start\":52920},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":247158549},\"end\":53788,\"start\":53306},{\"attributes\":{\"doi\":\"arXiv:2212.10079\",\"id\":\"b34\"},\"end\":54049,\"start\":53790},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":43922261},\"end\":54536,\"start\":54051},{\"attributes\":{\"doi\":\"arXiv:1704.01696\",\"id\":\"b36\"},\"end\":54793,\"start\":54538},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":253237115},\"end\":55170,\"start\":54795},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":249642442},\"end\":55683,\"start\":55172},{\"attributes\":{\"doi\":\"arXiv:2209.14876\",\"id\":\"b39\"},\"end\":56103,\"start\":55685},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":257834177},\"end\":56654,\"start\":56105},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":250492900},\"end\":57053,\"start\":56656},{\"attributes\":{\"id\":\"b42\"},\"end\":57382,\"start\":57055},{\"attributes\":{\"id\":\"b43\"},\"end\":57604,\"start\":57384},{\"attributes\":{\"id\":\"b44\"},\"end\":58558,\"start\":57606},{\"attributes\":{\"id\":\"b45\"},\"end\":59440,\"start\":58560},{\"attributes\":{\"id\":\"b46\"},\"end\":60266,\"start\":59442},{\"attributes\":{\"id\":\"b47\"},\"end\":60383,\"start\":60268}]", "bib_title": "[{\"end\":39148,\"start\":39108},{\"end\":40054,\"start\":39981},{\"end\":40663,\"start\":40578},{\"end\":41832,\"start\":41772},{\"end\":42252,\"start\":42199},{\"end\":42788,\"start\":42708},{\"end\":44082,\"start\":44020},{\"end\":44616,\"start\":44546},{\"end\":45050,\"start\":44965},{\"end\":45775,\"start\":45724},{\"end\":46392,\"start\":46344},{\"end\":48253,\"start\":48196},{\"end\":49003,\"start\":48945},{\"end\":50265,\"start\":50188},{\"end\":51746,\"start\":51642},{\"end\":53362,\"start\":53306},{\"end\":54127,\"start\":54051},{\"end\":54836,\"start\":54795},{\"end\":55248,\"start\":55172},{\"end\":56197,\"start\":56105},{\"end\":56712,\"start\":56656},{\"end\":59521,\"start\":59442}]", "bib_author": "[{\"end\":39169,\"start\":39150},{\"end\":39185,\"start\":39169},{\"end\":39464,\"start\":39447},{\"end\":39477,\"start\":39464},{\"end\":39490,\"start\":39477},{\"end\":39502,\"start\":39490},{\"end\":39514,\"start\":39502},{\"end\":39526,\"start\":39514},{\"end\":40073,\"start\":40056},{\"end\":40086,\"start\":40073},{\"end\":40097,\"start\":40086},{\"end\":40113,\"start\":40097},{\"end\":40124,\"start\":40113},{\"end\":40138,\"start\":40124},{\"end\":40686,\"start\":40665},{\"end\":40705,\"start\":40686},{\"end\":40720,\"start\":40705},{\"end\":40741,\"start\":40720},{\"end\":40759,\"start\":40741},{\"end\":40774,\"start\":40759},{\"end\":40791,\"start\":40774},{\"end\":41210,\"start\":41199},{\"end\":41310,\"start\":41290},{\"end\":41318,\"start\":41310},{\"end\":41329,\"start\":41318},{\"end\":41343,\"start\":41329},{\"end\":41362,\"start\":41343},{\"end\":41376,\"start\":41362},{\"end\":41389,\"start\":41376},{\"end\":41847,\"start\":41834},{\"end\":41859,\"start\":41847},{\"end\":42282,\"start\":42254},{\"end\":42290,\"start\":42282},{\"end\":42300,\"start\":42290},{\"end\":42315,\"start\":42300},{\"end\":42323,\"start\":42315},{\"end\":42804,\"start\":42790},{\"end\":42813,\"start\":42804},{\"end\":42830,\"start\":42813},{\"end\":42839,\"start\":42830},{\"end\":42850,\"start\":42839},{\"end\":42863,\"start\":42850},{\"end\":42880,\"start\":42863},{\"end\":42895,\"start\":42880},{\"end\":43316,\"start\":43303},{\"end\":43325,\"start\":43316},{\"end\":43335,\"start\":43325},{\"end\":43350,\"start\":43335},{\"end\":43372,\"start\":43350},{\"end\":43388,\"start\":43372},{\"end\":43401,\"start\":43388},{\"end\":43419,\"start\":43401},{\"end\":43435,\"start\":43419},{\"end\":43445,\"start\":43435},{\"end\":43460,\"start\":43445},{\"end\":43474,\"start\":43460},{\"end\":43489,\"start\":43474},{\"end\":43504,\"start\":43489},{\"end\":43519,\"start\":43504},{\"end\":43534,\"start\":43519},{\"end\":43550,\"start\":43534},{\"end\":43575,\"start\":43550},{\"end\":43586,\"start\":43575},{\"end\":43599,\"start\":43586},{\"end\":44102,\"start\":44084},{\"end\":44116,\"start\":44102},{\"end\":44127,\"start\":44116},{\"end\":44141,\"start\":44127},{\"end\":44632,\"start\":44618},{\"end\":44649,\"start\":44632},{\"end\":44666,\"start\":44649},{\"end\":44680,\"start\":44666},{\"end\":44697,\"start\":44680},{\"end\":44713,\"start\":44697},{\"end\":45068,\"start\":45052},{\"end\":45083,\"start\":45068},{\"end\":45097,\"start\":45083},{\"end\":45119,\"start\":45097},{\"end\":45133,\"start\":45119},{\"end\":45520,\"start\":45507},{\"end\":45536,\"start\":45520},{\"end\":45544,\"start\":45536},{\"end\":45791,\"start\":45777},{\"end\":45800,\"start\":45791},{\"end\":45813,\"start\":45800},{\"end\":45825,\"start\":45813},{\"end\":45839,\"start\":45825},{\"end\":45855,\"start\":45839},{\"end\":46120,\"start\":46100},{\"end\":46132,\"start\":46120},{\"end\":46150,\"start\":46132},{\"end\":46411,\"start\":46394},{\"end\":46426,\"start\":46411},{\"end\":46438,\"start\":46426},{\"end\":46920,\"start\":46905},{\"end\":46931,\"start\":46920},{\"end\":46940,\"start\":46931},{\"end\":46953,\"start\":46940},{\"end\":46963,\"start\":46953},{\"end\":46974,\"start\":46963},{\"end\":46991,\"start\":46974},{\"end\":47006,\"start\":46991},{\"end\":47020,\"start\":47006},{\"end\":47024,\"start\":47020},{\"end\":47355,\"start\":47337},{\"end\":47370,\"start\":47355},{\"end\":47379,\"start\":47370},{\"end\":47393,\"start\":47379},{\"end\":47406,\"start\":47393},{\"end\":47421,\"start\":47406},{\"end\":47667,\"start\":47645},{\"end\":47684,\"start\":47667},{\"end\":47697,\"start\":47684},{\"end\":47713,\"start\":47697},{\"end\":47726,\"start\":47713},{\"end\":47741,\"start\":47726},{\"end\":47767,\"start\":47741},{\"end\":47786,\"start\":47767},{\"end\":47792,\"start\":47786},{\"end\":48266,\"start\":48255},{\"end\":48280,\"start\":48266},{\"end\":48299,\"start\":48280},{\"end\":48586,\"start\":48571},{\"end\":48602,\"start\":48586},{\"end\":48621,\"start\":48602},{\"end\":48635,\"start\":48621},{\"end\":48651,\"start\":48635},{\"end\":49021,\"start\":49005},{\"end\":49040,\"start\":49021},{\"end\":49057,\"start\":49040},{\"end\":49400,\"start\":49383},{\"end\":49419,\"start\":49400},{\"end\":49431,\"start\":49419},{\"end\":49445,\"start\":49431},{\"end\":49466,\"start\":49445},{\"end\":49476,\"start\":49466},{\"end\":49490,\"start\":49476},{\"end\":49502,\"start\":49490},{\"end\":49514,\"start\":49502},{\"end\":49524,\"start\":49514},{\"end\":49528,\"start\":49524},{\"end\":49879,\"start\":49863},{\"end\":49893,\"start\":49879},{\"end\":49906,\"start\":49893},{\"end\":49923,\"start\":49906},{\"end\":49936,\"start\":49923},{\"end\":49951,\"start\":49936},{\"end\":49966,\"start\":49951},{\"end\":49984,\"start\":49966},{\"end\":50276,\"start\":50267},{\"end\":50287,\"start\":50276},{\"end\":50297,\"start\":50287},{\"end\":50310,\"start\":50297},{\"end\":50324,\"start\":50310},{\"end\":50333,\"start\":50324},{\"end\":50346,\"start\":50333},{\"end\":50872,\"start\":50862},{\"end\":50890,\"start\":50872},{\"end\":51116,\"start\":51104},{\"end\":51126,\"start\":51116},{\"end\":51140,\"start\":51126},{\"end\":51152,\"start\":51140},{\"end\":51165,\"start\":51152},{\"end\":51180,\"start\":51165},{\"end\":51193,\"start\":51180},{\"end\":51205,\"start\":51193},{\"end\":51220,\"start\":51205},{\"end\":51238,\"start\":51220},{\"end\":51256,\"start\":51238},{\"end\":51287,\"start\":51256},{\"end\":51298,\"start\":51287},{\"end\":51305,\"start\":51298},{\"end\":51758,\"start\":51748},{\"end\":51771,\"start\":51758},{\"end\":51784,\"start\":51771},{\"end\":51796,\"start\":51784},{\"end\":51801,\"start\":51796},{\"end\":52255,\"start\":52242},{\"end\":52269,\"start\":52255},{\"end\":52282,\"start\":52269},{\"end\":52291,\"start\":52282},{\"end\":52302,\"start\":52291},{\"end\":52310,\"start\":52302},{\"end\":52715,\"start\":52702},{\"end\":52728,\"start\":52715},{\"end\":53002,\"start\":52991},{\"end\":53015,\"start\":53002},{\"end\":53032,\"start\":53015},{\"end\":53047,\"start\":53032},{\"end\":53055,\"start\":53047},{\"end\":53064,\"start\":53055},{\"end\":53076,\"start\":53064},{\"end\":53373,\"start\":53364},{\"end\":53381,\"start\":53373},{\"end\":53394,\"start\":53381},{\"end\":53412,\"start\":53394},{\"end\":53425,\"start\":53412},{\"end\":53870,\"start\":53859},{\"end\":53883,\"start\":53870},{\"end\":54144,\"start\":54129},{\"end\":54156,\"start\":54144},{\"end\":54168,\"start\":54156},{\"end\":54186,\"start\":54168},{\"end\":54201,\"start\":54186},{\"end\":54615,\"start\":54600},{\"end\":54630,\"start\":54615},{\"end\":54852,\"start\":54838},{\"end\":54862,\"start\":54852},{\"end\":54872,\"start\":54862},{\"end\":54882,\"start\":54872},{\"end\":54895,\"start\":54882},{\"end\":54911,\"start\":54895},{\"end\":55264,\"start\":55250},{\"end\":55274,\"start\":55264},{\"end\":55287,\"start\":55274},{\"end\":55297,\"start\":55287},{\"end\":55308,\"start\":55297},{\"end\":55318,\"start\":55308},{\"end\":55331,\"start\":55318},{\"end\":55344,\"start\":55331},{\"end\":55360,\"start\":55344},{\"end\":55698,\"start\":55685},{\"end\":55715,\"start\":55698},{\"end\":55730,\"start\":55715},{\"end\":55737,\"start\":55730},{\"end\":55752,\"start\":55737},{\"end\":56213,\"start\":56199},{\"end\":56223,\"start\":56213},{\"end\":56231,\"start\":56223},{\"end\":56244,\"start\":56231},{\"end\":56259,\"start\":56244},{\"end\":56270,\"start\":56259},{\"end\":56284,\"start\":56270},{\"end\":56294,\"start\":56284},{\"end\":56305,\"start\":56294},{\"end\":56314,\"start\":56305},{\"end\":56323,\"start\":56314},{\"end\":56727,\"start\":56714},{\"end\":56737,\"start\":56727},{\"end\":56746,\"start\":56737},{\"end\":56759,\"start\":56746},{\"end\":56773,\"start\":56759},{\"end\":56781,\"start\":56773},{\"end\":57129,\"start\":57119},{\"end\":57142,\"start\":57129},{\"end\":57158,\"start\":57142},{\"end\":57176,\"start\":57158},{\"end\":57194,\"start\":57176},{\"end\":57211,\"start\":57194},{\"end\":57455,\"start\":57445},{\"end\":57471,\"start\":57455},{\"end\":57488,\"start\":57471},{\"end\":57616,\"start\":57608},{\"end\":57622,\"start\":57616},{\"end\":57634,\"start\":57622},{\"end\":57646,\"start\":57634},{\"end\":57650,\"start\":57646},{\"end\":57656,\"start\":57650},{\"end\":57661,\"start\":57656},{\"end\":57667,\"start\":57661},{\"end\":57672,\"start\":57667},{\"end\":57679,\"start\":57672},{\"end\":57685,\"start\":57679},{\"end\":57690,\"start\":57685},{\"end\":57699,\"start\":57690},{\"end\":57705,\"start\":57699},{\"end\":57712,\"start\":57705},{\"end\":57721,\"start\":57712},{\"end\":57727,\"start\":57721},{\"end\":57735,\"start\":57727},{\"end\":57743,\"start\":57735},{\"end\":57749,\"start\":57743},{\"end\":57757,\"start\":57749},{\"end\":57770,\"start\":57757},{\"end\":57777,\"start\":57770},{\"end\":57786,\"start\":57777},{\"end\":57794,\"start\":57786},{\"end\":57808,\"start\":57794},{\"end\":57816,\"start\":57808},{\"end\":57825,\"start\":57816},{\"end\":57834,\"start\":57825},{\"end\":57841,\"start\":57834},{\"end\":57859,\"start\":57841},{\"end\":57867,\"start\":57859},{\"end\":57874,\"start\":57867},{\"end\":57879,\"start\":57874},{\"end\":57889,\"start\":57879},{\"end\":57899,\"start\":57889},{\"end\":57917,\"start\":57899},{\"end\":57927,\"start\":57917},{\"end\":57939,\"start\":57927},{\"end\":57946,\"start\":57939},{\"end\":57955,\"start\":57946},{\"end\":57964,\"start\":57955},{\"end\":57972,\"start\":57964},{\"end\":57978,\"start\":57972},{\"end\":57990,\"start\":57978},{\"end\":58002,\"start\":57990},{\"end\":58006,\"start\":58002},{\"end\":58013,\"start\":58006},{\"end\":58026,\"start\":58013},{\"end\":58576,\"start\":58560},{\"end\":58586,\"start\":58576},{\"end\":58602,\"start\":58586},{\"end\":58626,\"start\":58602},{\"end\":58632,\"start\":58626},{\"end\":58644,\"start\":58632},{\"end\":58656,\"start\":58644},{\"end\":58664,\"start\":58656},{\"end\":58677,\"start\":58664},{\"end\":58688,\"start\":58677},{\"end\":58694,\"start\":58688},{\"end\":58706,\"start\":58694},{\"end\":58718,\"start\":58706},{\"end\":58722,\"start\":58718},{\"end\":58728,\"start\":58722},{\"end\":58735,\"start\":58728},{\"end\":58740,\"start\":58735},{\"end\":58744,\"start\":58740},{\"end\":58749,\"start\":58744},{\"end\":58756,\"start\":58749},{\"end\":58770,\"start\":58756},{\"end\":58776,\"start\":58770},{\"end\":58781,\"start\":58776},{\"end\":58786,\"start\":58781},{\"end\":58792,\"start\":58786},{\"end\":58798,\"start\":58792},{\"end\":58804,\"start\":58798},{\"end\":58810,\"start\":58804},{\"end\":58815,\"start\":58810},{\"end\":58831,\"start\":58815},{\"end\":58849,\"start\":58831},{\"end\":58859,\"start\":58849},{\"end\":59549,\"start\":59523},{\"end\":59555,\"start\":59549},{\"end\":59567,\"start\":59555},{\"end\":59582,\"start\":59567},{\"end\":59591,\"start\":59582},{\"end\":59606,\"start\":59591},{\"end\":59612,\"start\":59606},{\"end\":59624,\"start\":59612},{\"end\":59636,\"start\":59624},{\"end\":59640,\"start\":59636},{\"end\":59645,\"start\":59640},{\"end\":59651,\"start\":59645},{\"end\":59656,\"start\":59651},{\"end\":59662,\"start\":59656},{\"end\":59668,\"start\":59662},{\"end\":59673,\"start\":59668},{\"end\":59678,\"start\":59673},{\"end\":59684,\"start\":59678},{\"end\":59692,\"start\":59684},{\"end\":59701,\"start\":59692},{\"end\":59707,\"start\":59701},{\"end\":59720,\"start\":59707},{\"end\":59735,\"start\":59720},{\"end\":59743,\"start\":59735},{\"end\":59748,\"start\":59743},{\"end\":59757,\"start\":59748},{\"end\":59763,\"start\":59757},{\"end\":59774,\"start\":59763},{\"end\":59780,\"start\":59774},{\"end\":60297,\"start\":60270},{\"end\":60303,\"start\":60297},{\"end\":60315,\"start\":60303},{\"end\":60321,\"start\":60315}]", "bib_venue": "[{\"end\":40293,\"start\":40224},{\"end\":42006,\"start\":41941},{\"end\":42470,\"start\":42405},{\"end\":44302,\"start\":44230},{\"end\":46617,\"start\":46536},{\"end\":50515,\"start\":50439},{\"end\":51960,\"start\":51889},{\"end\":53574,\"start\":53508},{\"end\":58961,\"start\":58955},{\"end\":39268,\"start\":39185},{\"end\":39696,\"start\":39542},{\"end\":40222,\"start\":40138},{\"end\":40865,\"start\":40791},{\"end\":41939,\"start\":41859},{\"end\":42403,\"start\":42323},{\"end\":42960,\"start\":42895},{\"end\":43301,\"start\":43234},{\"end\":44228,\"start\":44141},{\"end\":44740,\"start\":44713},{\"end\":45181,\"start\":45133},{\"end\":45505,\"start\":45440},{\"end\":45866,\"start\":45855},{\"end\":46098,\"start\":46034},{\"end\":46534,\"start\":46438},{\"end\":46903,\"start\":46842},{\"end\":47335,\"start\":47282},{\"end\":47900,\"start\":47808},{\"end\":48364,\"start\":48299},{\"end\":48737,\"start\":48667},{\"end\":49077,\"start\":49057},{\"end\":49283,\"start\":49276},{\"end\":49381,\"start\":49335},{\"end\":49861,\"start\":49797},{\"end\":50437,\"start\":50346},{\"end\":50860,\"start\":50799},{\"end\":51102,\"start\":51047},{\"end\":51887,\"start\":51801},{\"end\":52398,\"start\":52326},{\"end\":52700,\"start\":52602},{\"end\":52989,\"start\":52920},{\"end\":53506,\"start\":53425},{\"end\":53857,\"start\":53790},{\"end\":54277,\"start\":54201},{\"end\":54598,\"start\":54538},{\"end\":54973,\"start\":54911},{\"end\":55417,\"start\":55360},{\"end\":55875,\"start\":55768},{\"end\":56354,\"start\":56323},{\"end\":56846,\"start\":56781},{\"end\":57117,\"start\":57055},{\"end\":57443,\"start\":57384},{\"end\":58039,\"start\":58026},{\"end\":58953,\"start\":58859},{\"end\":59793,\"start\":59780}]"}}}, "year": 2023, "month": 12, "day": 17}