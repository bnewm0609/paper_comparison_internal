{"id": 232352425, "updated": "2023-11-08 04:56:35.266", "metadata": {"title": "PlenOctrees for Real-time Rendering of Neural Radiance Fields", "authors": "[{\"first\":\"Alex\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Ruilong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Matthew\",\"last\":\"Tancik\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Ren\",\"last\":\"Ng\",\"middle\":[]},{\"first\":\"Angjoo\",\"last\":\"Kanazawa\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 3, "day": 25}, "abstract": "We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800x800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu.net/plenoctrees", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.14024", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/YuLT0NK21", "doi": "10.1109/iccv48922.2021.00570"}}, "content": {"source": {"pdf_hash": "5744fcc21b40327f7ad710de7d947d4584c53012", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.14024v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "521efe3b363a9f4a0043aa2c511d4f7ba0206b22", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5744fcc21b40327f7ad710de7d947d4584c53012.txt", "contents": "\nPlenOctrees for Real-time Rendering of Neural Radiance Fields\n\n\nAlex Yu \nUC Berkeley\n\n\nRuilong Li \nUC Berkeley\n\n\nUSC Institute for Creative Technologies 3 Pinscreen\n\n\nMatthew Tancik \nUC Berkeley\n\n\nHao Li \nUC Berkeley\n\n\nRen Ng \nUC Berkeley\n\n\nAngjoo Kanazawa \nUC Berkeley\n\n\nPlenOctrees for Real-time Rendering of Neural Radiance Fields\n\nWe introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800\u00d7800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve viewdependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: https://alexyu. net/plenoctrees.\n\nIntroduction\n\nDespite the progress of real-time graphics, interactive 3D content with truly photorealistic scenes and objects are still time consuming and costly to produce due to the necessity of optimized 3D assets and dedicated shaders. Instead, many graphics applications opt for image-based solutions. E-commerce websites often use a fixed set of views to showcase their products; VR experiences often rely on 360\n\n\nInput Images\n\nNeRF-SH\n\n\nOffline Model Generation\n\n\nReal-time Rendering\n\nPlenOctrees Demo\n\n\nFPS\n\nPlenOctree Figure 1: Real-time NeRF with PlenOctrees. Given a set of posed images of a scene, our method creates a 3D volumetric model that can be rendered in real-time. We propose PlenOctrees, which are octrees that can capture view-dependent dependent effects such as specularities. Rendering using our approach is orders of magnitude faster than NeRF.\n\nvideo recordings to avoid the costly production of real 3D scenes, and mapping services such as Google Street View stitch images into panoramic views limited to 3-DOF. Recent advances in neural rendering, such as neural volumes [24] and neural radiance fields (NeRFs) [30], open a promising new avenue to model arbitrary objects and scenes in 3D from a set of calibrated images. NeRFs in particular can faithfully render detailed scenes and appearances with non-Lambertian effects from any view, while simultaneously offering a high degree of compression in terms of storage. Partly due to these exciting properties, of late, there has been an explosion of research based on NeRF.\n\nNevertheless, for practical applications, runtime performance remains a critical limitation of NeRFs: due to the extreme sampling requirements and costly neural network queries, rendering a NeRF is agonizingly slow. For illustration, it takes roughly 30 seconds to render an 800x800 image from a NeRF using a high performance GPU, making it impractical for real-time interactive applications.\n\nIn this work, we propose a method for rendering a NeRF in real time, achieved by distilling the NeRF into a hierarchical 3D volumetric representation. Our approach preserves NeRF's ability to synthesize arbitrarily complex geometry and view-dependent effects from any viewpoint and requires no additional supervision. In fact, our method achieves and in many cases surpasses the quality of the original NeRF formulation, while providing significant acceleration. Our model allows us to render an 800x800 image at 167.68 FPS on a NVIDIA V100 GPU and does not rely on a deep neural network during test time. Moreover, our representation is amenable to modern web technologies, allowing interactive rendering in a browser on consumer laptops.\n\nNaive NeRF rendering is slow because it requires dense sampling of the scene, where every sample requires a neural network inference. Because these queries depend on the viewing direction as well as the spatial position, one cannot naively cache these color values for all viewing directions.\n\nWe overcome these challenges and enable real-time rendering by pre-sampling the NeRF into a tabulated view-dependent volume which we refer to as a PlenOctree, named after the plenoptic functions of Adelsen and Bergen [1]. Specifically, we use a sparse voxel-based octree where every leaf of the tree stores the appearance and density values required to model the radiance at a point in the volume. In order to account for non-Lambertian materials that exhibit view-dependent effects, we propose to represent the RGB values at a location with spherical harmonics (SH), a standard basis for functions defined on the surface of the sphere. The spherical harmonics can be evaluated at arbitrary query viewing directions to recover the view dependent color.\n\nAlthough one could convert an existing NeRF into such a representations via projection onto the SH basis functions, we show that we can in fact modify a NeRF network to predict appearances explicitly in terms of spherical harmonics. Specifically, we train a network that produces coefficients for the SH functions instead of raw RGB values, so that the predicted values can later be directly stored within the leaves of the PlenOctree. We also introduce a sparsity prior during NeRF training to improve the memory efficiency of our octrees, consequently allowing us to render higher quality images. Furthermore, once the structure is created, the values stored in PlenOctree can be optimized because the rendering procedure remains differentiable. This enables the PlenOctree to obtain similar or better image quality compared to NeRF. Our pipeline is illustrated in Fig. 2.\n\nAdditionally, we demonstrate how our proposed pipeline can be used to accelerate NeRF model training, making our solution more practical to train than the original NeRF approach. Specifically, we can stop training the NeRF model early to convert it into a PlenOctree, which can then be trained significantly faster as it no longer involves any neural networks.\n\nOur experiments demonstrate that our approach can accelerate NeRF-based rendering by 5 orders of magnitude without loss in image quality. We compare our approach on standard benchmarks with scenes and objects captured from 360 \u2022 views, and demonstrate state-of-the-art level performance for image quality and rendering speed.\n\nOur interactive viewer can enable operations such as object insertion, visualizing radiance distributions, decomposing the SH components, and slicing the scene. We hope that these real-time operations can be useful to the community for visualizing and debugging NeRF-based representations.\n\nTo summarize, we make the following contributions:\n\n\u2022 The first method that achieves real-time rendering of NeRFs with similar or improved quality.\n\n\u2022 NeRF-SH: a modified NeRF that is trained to output appearance in terms of spherical basis functions.\n\n\u2022 PlenOctree, a data structure derived from NeRFs which enables highly efficient view-dependent rendering of complex scenes.\n\n\u2022 Accelerated NeRF training method using an early training termination, followed by a direct fine-tuning process on PlenOctree values.\n\n\nRelated Work\n\nNovel View Synthesis. The task of synthesizing novel views of a scene given a set of photographs is a wellstudied problem with various approaches. All methods predict an underlying geometric or image-based 3D representation that allows rendering from novel viewpoints. Mesh based methods represent the scene with surfaces, and have been used to model Lambertian (diffuse) [56] and non-Lambertian scenes [60,5,3]. Mesh based representations are compact and easy to render; however, optimizing a mesh to fit a complex scene of arbitrary topology is challenging. Image-based rendering methods [19,43,60], on the other hand, enable easy capture as well as photo-realistic and fast rendering, however are often bounded in the viewing angle and do not allow easy editing of the underlying scene.\n\nVolume rendering is a classical technique with a long history of research in the graphics community [7]. Volumebased representations such as voxel grids [42,18,24,14,55,44] and multi-plane images (MPIs) [49,35,64,48,29] are a popular alternative to mesh representations due to their topology-free nature: gradient-based optimization is therefore straightforward, while rendering can still be real-time. However, such naive volumetric representations are often We propose a method to quickly render NeRFs by training a modified NeRF model (NeRF-SH) and converting it into a PlenOctree, an octree that captures view-dependent effects. a) The NeRF-SH model uses the same optimization procedure and volume rendering method presented in NeRF [30]. However, instead of predicting the RGB color c directly, the network predicts spherical harmonic coefficients k. The color c is calculated by summing the weighted spherical harmonic bases evaluated at the corresponding ray direction (\u03b8, \u03c6). The spherical harmonics enable the representation to model view-dependent appearance. The values in the orange boxes are used for volume rendering. b) To build a PlenOctree, we densely sample the NeRF-SH model in the volume around the target object and tabulate the density and SH coefficients. We can further optimize the PlenOctree directly with the training images to improve its quality. memory bound, limiting the maximum resolution that can be captured. Volumetric octrees are a popular approach for reducing memory and compute in such cases. We refer the reader to this survey [17] for a historical perspective on octree volume rendering. Octrees have been used in recent work to decrease the memory requirements during training for other 3D tasks [39,11,52,57]. Concurrent with this work, NeX [59] extends MPIs to encode spherical basis functions that enable view-dependent rendering effects in real-time. Also concurrently, Lombardi et al. [25] propose to model data using geometric primitives, and [13,9,38] also distill NeRFs to enable real-time rendering.\n\n\nCoordinate-Based Neural Networks.\n\nRecently, coordinate-based neural networks have emerged as a popular alternative to explicit volumetric representations, as they are not limited to a fixed voxel representation. These methods train a multilayer perceptron (MLP) whose input is a coordinate and output is some property of space corresponding to that location. These networks have been used to predict occupancy [28,4,34,40,31,20], signed distance fields [32,10,61,62], and radiance [30]. Coordinate-based neural networks have been used for view synthesis in Scene Representation Networks [45], NeRFs [30], and many NeRF extensions [27,33,41,47]. These networks represent a continuous function that can be sampled at arbitrarily fine resolutions without increasing the memory footprint. Unfortunately, this compactness is achieved at the expense of computational efficiency as each sample must be processed by a neural network. As a result, these representations are often slow and impractical for real-time rendering.\n\nNeRF Accelerations. While NeRFs are able to produce high quality results, their computationally expensive rendering leads to slow training and inference. One way to speed up the process of fitting a NeRF to a new scene is to incorporate priors learned from a dataset of similar scenes. This can be accomplished by conditioning on predicted images features [53,63,58] or meta-learning [51]. To improve inference speed, Neural Sparse Voxel Fields (NSVF) [23] learns a sparse voxel grid of features that are input into a NeRF like model. The sparse voxel grid allows the renderer to skip over empty regions when tracing a ray which improves the render time \u223c10x. Decomposed Radiance Fields [37] spatially decomposes a scene into multiple smaller networks. AutoInt [22] modifies the architecture of the NeRF so that inference requires fewer samples but produces lower quality results. None of these approaches achieve real-time. The concurrent work DoNeRF adds a depth classifier to NeRF in order to drastically improve the efficiency of sampling, but requires ground-truth depth for training. Although not based on NeRF, recently Takikawa et al. [50] propose a method to accelerate neural SDF rendering with an octree. Note that this work does not model appearance properties. In contrast, we employ a volumetric representation that can capture photorealistic view-dependent appearances while achieving even higher framerates.\n\n\nPreliminaries\n\n\nNeural Radiance Fields\n\nNeural radiance fields (NeRF) [30] are 3D representations that can be rendered from arbitrary novel viewpoints while capturing continuous geometry and view-dependent appearance. The radiance field is encoded into the weights of a multilayer perceptron (MLP) that can be queried at a position x = (x, y, z) from a viewing direction d = (\u03b8, \u03c6) to recover the corresponding density \u03c3 and color c = (r, g, b). A pixel's predicted color C(r) is computed by casting a ray, r, into the volume and accumulating the color based on density along the ray. NeRF estimates the accumulated color by taking N point samples along the ray to perform volume rendering:\nC(r) = N \u22121 i=0 T i 1 \u2212 exp(\u2212\u03c3 i \u03b4 i ) c i ,(1)\nwhere\nT i = exp \uf8eb \uf8ed \u2212 i\u22121 j=0 \u03c3 j \u03b4 j \uf8f6 \uf8f8(2)\nWhere \u03b4 i are the distances between point samples. To train the NeRF network, the predicted colors\u0108 for a batch of rays R corresponding to pixels in the training images are optimized using Adam [15] to match the target pixel colors:\nL RGB = r\u2208R C(r) \u2212\u0108(r) 2 2(3)\nTo better represent high frequency details in the scene the inputs are positionally encoded and two stages of sampling are performed, one coarse and one fine. We refer the interested reader to the NeRF paper [30] paper for details.\n\nLimitations. One notable consequence of this architecture is that each sample along the ray must be fed to the MLP to obtain the corresponding \u03c3 i and c i . A total of 192 samples were taken for each ray in the examples presented in NeRF. This is inefficient as most samples are sampling free space which do not contribute to the integrated color.\n\nTo render a single target image at 800 \u00d7 800 resolution, the network must be run on over 100 million inputs. Therefore it takes about 30 seconds to render a single frame using a NVIDIA V100 GPU, making it impractical for real-time applications. Our use of a sparse voxel octree avoids excess compute in regions without content. Additionally we precompute the values for each voxel so that network queries are not performed during inference.\n\n\nMethod\n\nWe propose a pipeline that enables real-time rendering of NeRFs. Given a trained NeRF, we can convert it into a PlenOctree, an efficient data structure that is able to represent non-Lambertian effects in a scene. Specifically, it is an octree which stores spherical harmonics (SH) coefficients at the leaves, encoding view-dependent radiance.\n\nTo make the conversion to PlenOctree more straightforward, we also propose NeRF-SH, a variant of the NeRF network which directly outputs the SH coefficients, thus eliminating the need for a view-direction input to the network. With this change, the conversion can then be performed by evaluating on a uniform grid followed by thresholding. We fine-tune the octree on the training images to further improve image quality, Please see Fig. 2 for a graphical illustration of our pipeline.\n\nThe conversion process leverages the continuous nature of NeRF to dynamically obtain the spatial structure of the octree. We show that even with a partially trained NeRF, our PlenOctree is capable of producing results competitive with the fully trained NeRF.\n\n\nNeRF-SH: NeRF with Spherical Harmonics\n\nSHs have been a popular low-dimensional representation for spherical functions and have been used to model Lambertian surfaces [36,2] or even glossy surfaces [46]. Here we explore its use in a volumetric context. Specifically, we adapt the NeRF network f to output spherical harmonics coefficients k , rather than RGB values.\nf (x) = (k, \u03c3) where k = (k m ) m: \u2212 \u2264m\u2264 : 0\u2264 \u2264 max (4)\nEach k m \u2208 R 3 is a set of 3 coefficients corresponding to the RGB components. In this setup, the view-dependent color c at a point x may be determined by querying the SH functions Y m : S 2 \u2192 R at the desired viewing angle d:\nc(d; k) = S max =0 m=\u2212 k m Y m (d)(5)\nWhere S : x \u2192 (1 + exp(\u2212x)) \u22121 is the sigmoid function for normalizing the colors. In other words, we factorize the view-dependent appearance with the SH basis, eliminating the view-direction input to the network and removing the need to sample view directions at conversion time. Please see the appendix for more technical discussion of SHs. With a single evaluation of the network, we can now efficiently query colors from arbitrary viewing angles at inference time. In Fig. 7, one can see that NeRF-SH training speed is similar to, but slightly faster than, NeRF (by about 10%). Note that we can also project a trained NeRF to SHs directly at each point by sampling NeRF at random directions and multiplying by the SH component values to form Monte Carlo estimates of the inner products. However, this sampling process takes several hours to achieve reasonable quality and imposes a quality loss of about 2 dB. 1 Nevertheless, this alternative approach offers a pathway to convert existing NeRFs into PlenOctrees.\n\nOther than SHs, we also experiment with Spherical Gaussians (SG) [8], a learnable spherical basis which have been used to represent all-frequency lighting [54,46,21]. We find that SHs perform better in our use case and provide an ablation in the appendix.  Table 1: Quantitative results on the NeRF-synthetic test scenes. Our approach is significantly faster than all existing methods during inference while performing on par with NSVF, the current state-of-the-art method for image quality. We note that NeRF-SH, the modified NeRF model that is trained to output SH, performs similarly to the baseline NeRF model. The octree conversion of NeRF-SH to PlenOctree w/o fine-tuning negatively impacts the image quality metrics. This is remedied with the additional finetuning step.\n\nWithout sparsity loss With sparsity loss Sparsity prior. Without any regularization, the model is free to generate arbitrary geometry in unobserved regions. While this does not directly worsen image quality, it would adversely impact our conversion process as the extra geometry occupies significant voxel space.\n\nTo solve this problem, we introduce an additional sparsity prior during NeRF training. Intuitively, this prior encourages NeRF to choose empty space when both space and solid colors are possible solutions. Formally,\nL sparsity = 1 K K k=1 |1 \u2212 exp(\u2212\u03bb\u03c3 k )|(6)\nHere, {\u03c3 k } K k=1 are the evaluated density values at K uniformly random points within the bounding box, and \u03bb is a hyperparameter. The final training loss is then \u03b2 sparsity L sparsity + L RGB , where \u03b2 sparsity is a hyperparameter. Fig. 3 illustrates the effect of the prior.  \n\n\nPlenOctree: Octree-based Radiance Fields\n\nOnce we have trained a NeRF-SH model, we can convert it into a sparse octree representation for real time rendering. A PlenOctree stores density and SH coefficients modelling view-dependent appearance at each leaf. We describe the conversion and rendering processes below. Rendering. To render the PlenOctree, for each ray, we first determine ray-voxel intersections in the octree structure. This produces a sequence of segments between voxel boundaries with constant density and color, of lengths (1) is then applied to assign a color to the ray. This approach allows for skipping large voxels in one step while also not missing small voxels.\n{\u03b4 i } N i=1 . NeRF's volume rendering model\nAt test-time, we further accelerate this rendering process by applying early-stopping when the ray has accumulated transmittance T i less than \u03b3 = 0.01.\n\nConversion from NeRF-SH. The conversion process can be divided into three steps. At a high level, we evaluate the network on a grid, retaining only density values, then filter the voxels via thresholding. Finally we sample random points within each remaining voxel and average them to obtain SH coefficients to store in the octree leaves. More details are given below:\n\nEvaluation. We first evaluate the NeRF-SH model to obtain \u03c3 values on a uniformly spaced 3D grid. The grid is automatically scaled to tightly fit the scene content. 2 Filtering. Next, we filter this grid to obtain a sparse set of voxels centered at the grid points sufficient for representing the scene. Specifically, we render alpha maps for all the training views using this voxel grid, keeping track of the maximum ray weight 1 \u2212 exp(\u2212\u03c3 i \u03b4 i ) at each voxel. We then eliminate the voxels whose weights are lower than a threshold \u03c4 w . The octree is constructed to contain the re- Figure 4: NeRF-synthetic qualitative results. Randomly sampled qualitative comparisons on a reimplementation of NeRF and our proposed method. We are unable to find any significant image quality difference between the two methods. Despite this, our method can render these examples images more than 3500x faster. maining voxels as leaves at the deepest level while being empty elsewhere. Compared to naively thresholding by \u03c3 at each point, this method eliminates non-visible voxels.\n\nSampling. Finally, we sample a set of 256 random points in each remaining voxel and set the associated leaf of the octree to the mean of these values to reduce aliasing. Each leaf now contains the density \u03c3 and a vector of spherical harmonics coefficients for each of the RGB color channels.\n\nThis full extraction process takes about 15 minutes. 3\n\n\nPlenOctree Optimization\n\nSince this volume rendering process is fully differentiable with respect to the tree values, we can directly finetune the resulting octree on the original training images using the NeRF loss (3) with SGD in order to improve the image quality. Note that the tree structure is fixed to that obtained from NeRF in this process. PlenOctree optimization operates at about 3 million rays per second, compared to about 9000 for NeRF training, allowing us to optimize for many epochs in a relatively short time. The analytic derivatives for this process are implemented in custom CUDA kernels. We defer technical details to the appendix.\n\nThe fast octree optimization indirectly allows us to accelerate NeRF training, as seen in Fig. 7, since we can stop the NeRF-SH training at an earlier time for constructing the PlenOctree, with only a slight degradation in quality.\n\n\nResults\n\n\nExperimental Setup\n\nDatasets.\n\nFor our experiments, we use the NeRFsynthetic [30] dataset and a subset of the Tanks and Tem-3 Note that sampling 8 points instead of 256 allows for extraction in about 1.5 minutes, with minimal loss in quality.  ples dataset [16]. The NeRF-synthetic dataset consists of 8 scenes where each scene has a central object with 100 inward facing cameras distributed randomly on the upper hemisphere. The images are 800 \u00d7 800 with provided ground truth camera poses. The Tanks and Temples subset is from NSVF [23] and contains 5 scenes of real objects captured by an inward facing camera circling the scene. We use foreground masks provided by the NSVF authors. Each scene contains between 152-384 images of size 1920 \u00d7 1080.\n\nBaselines. The principal baseline for our experiments is NeRF [30]; we report results for both the original NeRF implementation, denoted NeRF (original) as well as a reimplementation in Jax [6], denoted simply NeRF, which our NeRF-SH code is based off of. Unless otherwise stated, all NeRF results and timings are from the latter implemen-\n\n\nGround Truth\n\nGround Truth NeRF Ours NeRF Ours Figure 6: Qualitative comparisons on Tanks and Temples. We compare NeRF and our proposed method. On this dataset, we find that our method better recovers the fine details in the scene. The results are otherwise similar. Additionally, the render time of our method is over 3000\u00d7 faster.\n\ntation. We compare also to two recent papers introducing NeRF accelerations, neural sparse voxel fields (NSVF) [23] and AutoInt [22], as well as two older methods, scene representation networks (SRN) [45] and Neural Volumes [24].\n\n\nQuality Evaluation\n\nWe evaluate our approach against prior works on the synthetic and real datasets mentioned above. The results are in Tables 1 and Table 2 respectively. Note that none of the baselines achieve real-time performance; nevertheless, our quality results are competitive in all cases and better in terms of some metrics.\n\nIn Figures 4 and 6, we show qualitative examples that demonstrate that our PlenOctree conversion does not perceptually worsen the rendered images compared to NeRF; rather, we observe that the PlenOctree optimization process enhances fine details such as text. Additionally, we note that our modifications of NeRF to predict spherical function coefficients (NeRF-SH) does not significantly change the performance.\n\nFor the SH, we set max = 3 (16 components) and 4 (25 components) on the synthetic and Tanks & Temples datasets respectively. We use 512 3 grid size in either case. Please refer to the appendix for training details. The inference time performance is measured on a Tesla V100 for all methods. Across both datasets we find that PlenOctree inference is over 3000 times faster than NeRF and at least 30 times faster than all other compared methods. PlenOctree performs either best, or second best for all image quality metrics.\n\n\nSpeed Trade-off Analysis\n\nA number of parameters for PlenOctree conversion and rendering can be tuned to trade-off between speed and image quality. In Figure 5 and Table 3 we compared image accuracy and inference time for four variants of PlenOctree that sweep this trade-off.\n\n\nIndirect Acceleration of NeRF Training\n\nSince we can efficiently fine-tune the octree on the original training data, as briefly discussed in \u00a74.3, we can choose to stop the NeRF-SH training at an earlier time before converting it to a PlenOctree. Indeed, we have found that the image quality improvements gained during fine-tuning can often be greater than continuing to train the NeRF-SH an equivalent amount of time. Therefore it can be more time efficient to stop the NeRF-SH training before it has converged and transition to PlenOctree conversion and finetuning.\n\nIn Figure 7 we compare NeRF and NeRF-SH models trained for 2 million iterations each to a sequence of PlenOctree models extracted from NeRF-SH checkpoints. We find that given a time constraint, it is almost always preferable to stop the NeRF training and transition to PlenOctree optimization.\n\n\nReal-time and In-browser Applications\n\nInteractive demos. Within our desktop viewer, we are able to perform a variety of real-time scene operations on the PlenOctree representation. For example, it is possible to insert meshes while maintaining proper occlusion, slice the PlenOctree to visualize a cross-section, or render the depth map to verify the geometry. Other features include probing the radiance distribution at any point in space, and inspecting subsets of SH components. These examples are demonstrated in Figure 9. The ability to perform these actions in real-time is beneficial both for interactive entertainment and debugging NeRF-related applications. Web renderer.\n\nWe have implemented a web-based renderer enabling interactive viewing of PlenOctrees in the browser. This is achieved by rewriting our CUDA-  based PlenOctree renderer as a WebGL-compatible fragment shader and applying compressions to reduce file sizes. Please see the appendix for more information.\n\n\nDiscussion\n\nWe have introduced a new data representation for NeRFs using PlenOctrees, which enables real-time rendering capabilities for arbitrary objects and scenes. Not only can we accelerate the rendering performance of the original NeRF method by more than 3000 times, but we can produce images that are either equal or better quality than NeRF thanks to our hierarchical data structure. As training time poses another hurdle for adopting NeRFs in practice (taking 1-2 days to fully converge), we also showed that our PlenOctrees can accelerate effective training time for our NeRF-SH. Finally, we have implemented an in-browser viewer based on We-bGL to demonstrate real-time and 6-DOF rendering capabilities of NeRFs on consumer laptops. In the future, our Limitations and Future Work. While we achieve stateof-the-art rendering performance and frame rates, the octree representation is much larger than the compact representation of the original NeRF model and has a larger memory footprint. The average uncompressed octree size for the full model is 1.93 GB on the synthetic dataset and 3.53 GB on the Tanks and Temples dataset. For online delivery, we use lower-resolution compressed models which are about 30-120 MB; please see the appendix for details. Although already possible in some form (Fig. 8), applying our method to unbounded and forward-facing scenes optimally requires further work as the data distribution is different for unbounded scenes. The forward-facing scenes inherently do not support 6-DOF viewing, and we suggest MPIs may be more appropriate in this case [59].\n\nIn the future, we plan to explore extensions of our method to enable real-time 6-DOF immersive viewing of large-scale scenes, as well as of dynamic scenes. We believe that real-time rendering of NeRFs has the potential to become a new standard for next-generation AR/VR technologies, as photorealistic 3D content can be digitized as easily as recording 2D videos.  \n\n\nA.2. Spherical Basis Function Ablation\n\nWe also provide ablation studies on the choice of spherical basis functions. We first ablate the effect on the number of spherical harmonics basis, then we explore the use of a learnable spherical basis functions. All experiments are conducted on NeRF-synthetic dataset and we report the average metric directly after training NeRF with spherical basis functions and after converting it to PlenOctrees with fine-tuning.\n\nNumber of SH basis functions First, we ablate the number of basis functions used for spherical harmonics. Average metrics across the NeRF-synthetic dataset are reported both for the modified NeRF model and the corresponding PlenOctree. We found that switching between max = 3 (SH-16) and 4 (SH-25) makes very little difference in terms of metrics or visual quality.\n\nSpherical Gaussians Furthermore, we also experimented with spherical Gaussians (SGs) [8], which is another form of spherical basis functions similar to spherical harmonics, but with learnable Gaussian kernels. Please see \u00a7B.1 for a brief introduction of SHs and SGs. SG-25 denotes our model using 25 SG components instead of SH, all with learnable lobe axis p and bandwidth \u03bb. However, while this model has marginally better PSNR, the advantage disappears following PlenOctree conversion and fine-tuning.\n\n\nB. Technical Details\n\n\nB.1. Spherical Basis Functions: SH and SG\n\nIn the main paper, we used the SH functions without defining their exact form. Here, we provide a brief technical discussion of both spherical harmonics (SH) and spherical Gaussians (SG) for completeness.\n\nSpherical Harmonics. The Spherical Harmonics (SH) form a complete basis of functions S 2 \u2192 C. For \u2208 N\u222a{0} and m \u2208 {\u2212 , . . . , }, the SH function of degree and order m is defined as:\nY m (\u03b8, \u03c6) = 2 + 1 4\u03c0 ( \u2212 m)! ( + m)! P m (cos \u03b8)e im\u03c6(7)\nwhere P m (cos\u03b8)e im\u03c6 are the associated Legendre polynomials. A real basis of SH Y m : S 2 \u2192 R can be defined in terms of its complex analogue Y m : S 2 \u2192 C by setting\nY m (\u03b8, \u03c6) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u221a 2(\u22121) m Im[Y |m| ] if m < 0 Y 0 if m = 0 \u221a 2(\u22121) m Re[Y m ] if m > 0(8)\nAny real spherical function L : S 2 \u2192 R may then be expressed in the SH basis:\nL(d) = L(\u03b8, \u03c6) = \u221e =0 m=\u2212 k m Y m (\u03b8, \u03c6)(9)\nSpherical Gaussians. Spherical Gaussians (SGs), also known as the von Mises-Fisher distribution [8], is another form of spherical basis functions that have been widely adopted to approximate spherical functions. Unlike SHs, SGs are a learnable basis. A normalized SG is defined as:\nG(d; p, \u03bb) = e \u03bb(d\u00b7p\u22121)(10)\nWhere p \u2208 R 2 is the lobe axis, and \u03bb \u2208 R is the bandwidth (sharpness) of the Gaussian kernel. Due to the varying bandwidths supported by SGs, they are suitable for representing all-frequency signals such as lighting [54,46,21]. A spherical function represented using n SGs is formulated as:\nL(d) \u2248 n =0 k G (d; p, \u03bb)(11)\nWhere k \u2208 R 3 is the RGB coefficients for each SG.\n\n\nB.2. PlenOctree Compression\n\nThe uncompressed PlenOctree file would be unpleasantly time-consuming for users to download for in-browser rendering. Thus, to minimize the size of PlenOctrees for viewing in the browser, we use SH-9 instead of SH-16 or SH-25 and apply a looser bounding box, which reduces the number of occupied voxels. On top of this, we compress the PlenOctrees directly in the following ways:\n\n1. We quantize the SH coefficients in the tree using the popular median-cut algorithm [12]. More specifically, the \u03c3 values are kept as is; for each SH basis function, we quantize the RGB coefficients k m \u2208 R 3 into 2 16 colors. Afterwards, separately for each SH basis function, we store a 2 16 \u00d7 3 codebook (as float16) along with pointers from each tree leaf to a position in the codebook (as int16).\n\n2. We compress the entire tree, including pointers, using the standard DEFLATE algorithm from ZLIB [26].\n\nThis process reduces the file size by as much as 20-30 times. The tree is fully decompressed before it is displayed in the web renderer. We will also release this code.\n\n\nB.3. Analytic Derivatives of PlenOctree Rendering\n\nIn this section, we derive the analytic derivatives of the NeRF piecewise constant volume rendering model for optimizing PlenOctrees directly. Throughout this section we will consider a fixed ray with a given origin and direction.\n\n\nB.3.1 Definitions\n\nFor preciseness, we provide definitions of quantities used in NeRF volume rendering. The NeRF rendering model considers a ray divided into N consecutive segments with endpoints {t i } N i=0 , where t 0 and t N are the near and far bounds. The segments have constant densities \u03c3 = (\u03c3 0 , . . . , \u03c3 N \u22121 ) where each \u03c3 i \u2265 0. If we shine a light of intensity 1 at t i , then at the camera position t 0 , the light intensity is given by\nT i (\u03c3) := i\u22121 j=0 exp(\u2212\u03b4 j \u03c3 j ),(12)\nWhere \u03b4 i := t i+1 \u2212 t i are segment lengths as in \u00a73 of the main paper. Note that T i is also known as the accumulated transmittance from t 0 to t i , and is the same as the definition in (1). It can be shown that this precisely models the absorption within each segment in the piecewise-constant setting.\n\nLet c = (c 0 , . . . c N \u22121 ) be the color associated with segments 0, . . . , N \u2212 1, and c N be the background light intensity; each c 0 , . . . , c N \u2208 [0, 1] 3 is an RGB color. We are interested in the derivative of the rendered color\u0108(\u03c3, c) with respect to \u03c3 and c. Note c N (background) is typically considered a hyperparameter.\n\n\nB.3.2 Derivation of the Derivatives\n\nFrom the original NeRF rendering equation (1), we can express the rendered ray color\u0108(\u03c3, c) as:\nC (\u03c3, c) = T N (\u03c3) c N + N \u22121 i=0 T i (\u03c3) 1 \u2212 e \u2212\u03c3i\u03b4i c i (13) = N i=0 w i (\u03c3) c i(14)\nWhere\nw i (\u03c3) = T i (\u03c3) (1 \u2212 exp(\u2212\u03c3 i \u03b4 i )) = T i (\u03c3) \u2212 T i+1 (\u03c3)\nare segment weights, and w N (\u03c3) = T N (\u03c3). 4\n\nColor derivative. Since the rendered color are a convex combination of the segment colors, it's immediately clear  that\n\u2202\u0108 \u2202c i (\u03c3, c) = w i (\u03c3)(15)\nHandling spherical harmonics colors is straightforward by applying the chain rule, noting that the SH basis function values are constant across the ray.\n\nDensity derivative. This is slightly more tricky. We can write the derivative wrt. \u03c3 i ,\n\u2202\u0108 \u2202\u03c3 i (\u03c3, c) = c N \u2202T N \u2202\u03c3 i + N \u22121 k=0 c k \u2202T k \u2202\u03c3 i \u2212 \u2202T k+1 \u2202\u03c3 i(16)\nWhere the derivative of the intensity T k , is\n\u2202T k \u2202\u03c3 i (\u03c3) = \u2202 \u2202\u03c3 i \uf8ee \uf8f0 k\u22121 j=0 e \u2212\u03b4j \u03c3j \uf8f9 \uf8fb (17) = \u2212\u03b4 i \uf8ee \uf8f0 k\u22121 j=0 e \u2212\u03b4j \u03c3j \uf8f9 \uf8fb 1 k>i (18) = \u2212\u03b4 i T k (\u03c3) 1 k>i(19)\n1 k>i denotes an indicator function whose value is 1 if k > i or 0 else. Basically, we can delete any T k for k \u2264 i from the original expression, then multiply by \u2212\u03b4 i . There-fore we can simplify (16) as follows\n\u2202\u0108 \u2202\u03c3 i (\u03c3, c) = \u03b4 i c i T i+1 (\u03c3) \u2212 N k=i+1 c k w k (\u03c3)(20)\nRemark. Within the PlenOctree renderer, this gradient can be computed in two rendering passes; the second pass is needed due to dependency on \"future\" weights and colors not seen by the ray marching process. The first pass store N k=0 c k w k (\u03c3), then subtracting a prefix from it. The overhead is still relatively small, and auxiliary memory use is constant.\n\nIf there are multiple colors, we simply add the density derivatives over all of them. In practice, usually the network outputs\u03c3 i \u2208 R and we set \u03c3 i = (\u03c3 i ) + , so we also need to take care of setting the gradient to 0 if\u03c3 i \u2264 0.\n\n\nB.4. NeRF-SH Training Details\n\nOur NeRF-SH model is built upon a Jax reimplementation of NeRF [6]. In our experiments, we use a batch size of 1024 rays, each with 64 sampled points in the coarse volume and 128 additional sampled points in the fine volume. The model is optimized with the Adam optimizer [15] using a learning rate that starts at 5 \u00d7 10 \u22124 and decays exponentially to 5 \u00d7 10 \u22126 over the training process. All of our models are trained for 2M iterations under the same protocol. Training takes around 50 hours to converge for each model on a single NVIDIA V100 GPU.\n\n\nB.5. PlenOctree Optimization Details\n\nAfter converting the NeRF-SH model into a PlenOctree, we further optimize the PlenOctree on the training set with SGD using the NeRF loss; note we no longer apply the sparsity loss here since the octree is already sparse. For NeRFsynthetic dataset, we use a constant 1 \u00d7 10 7 learning rate and optimize for maximum 80 epochs. For Tanks&Temples dataset, we set the learning rate to 1.5 \u00d7 10 6 and the maximum epochs to 40. We applied early stopping for the optimization process by monitoring the PSNR on the validation set 5 . On average it takes around 10 minutes to finish the PlenOctree optimization for each scene on a single NVIDIA V100 GPU. The entire optimization process is done in float32 for stability, but after it we storage the PlenOctree with float16 to reduce the model size.     \n\nFigure 2 :\n2Method Overview.\n\nFigure 3 :\n3Sparsity loss and conversion robustness. When trained without the sparsity loss, NeRF can often converge to a solution where unobserved portions or the background are solid. This degrades the spatial resolution of our octree-based representation.\n\nFigure 7 :\n7Indirect training acceleration.\n\nFigure 9 :\n9Real-time interactive demos. Set of real-time operations that can be performed on PlenOctree within our interactive viewer. This application will be released to the public. approach may enable virtual online stores in VR, where any products with arbitrary complexity and materials can be visualized in real-time while enabling 6-DOF viewing.\n\nFigure 10 :\n10Qualitative comparisons on NeRF-synthetic.\n\nFigure 11 :\n11More qualitative results of our PlenOctrees on NeRF-synthetic.\n\nFigure 12 :\n12More qualitative results of our PlenOctrees on Tanks&Temples.\n\n\nTanks and Temples Datasetbest second-bestPSNR \u2191 SSIM \u2191 LPIPS \u2193 FPS \u2191NeRF (original) \n25.78 \n0.864 \n0.198 \n0.007 \nNeRF \n27.94 \n0.904 \n0.168 \n0.013 \nSRN \n24.10 \n0.847 \n0.251 \n0.250 \nNeural Volumes \n23.70 \n0.834 \n0.260 \n1.000 \nNSVF \n28.40 \n0.900 \n0.153 \n0.163 \n\nNeRF-SH \n27.82 \n0.902 \n0.167 \n0.015 \nPlenOctree from NeRF-SH \n27.34 \n0.897 \n0.170 \n42.22 \nPlenOctree after fine-tuning \n27.99 \n0.917 \n0.131 \n42.22 \n\n\n\nTable 2 :\n2Quantitative results on the Tanks and Temples test scenes. We find that our fine-tuned PlenOctree model is significantly faster than existing methods while performing on par in terms of image metrics. Note that the images here are 1920\u00d71080 compared to 800\u00d7800 in the synthetic dataset.\n\nTable 3 :\n3PlenOctree conversion ablations on NeRF-synthetic.Average metrics across the NeRF-synthetic scenes for several dif-\nferent methods to construct PlenOctrees are shown. Ours-1.9G: \nThis is the high-quality model we reported in Table 1. Ours-1.4G: \nThis is a variant with a higher weight threshold, therefore aggres-\nsively sparsifying the tree. Ours-0.4G: Here, we remove the auto \nbounding box scaling step and instead use fixed large bounding \nboxes, limiting resolution. Ours-0.3G: A version using a 256 3 \ngrid instead of 512 3 . \n\n\n\n\nNeRF-SH/SG Converted PlenOctree Basis PSNR \u2191 SSIM \u2191 LPIPS \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 GB \u2193 FPS\u2191SH-9 \n31.44 \n0.951 \n0.065 \n31.45 \n0.956 \n0.056 \n1.00 \n262 \nSH-16 \n31.57 \n0.952 \n0.063 \n31.71 \n0.958 \n0.053 \n1.93 \n168 \nSH-25 \n31.56 \n0.951 \n0.063 \n31.69 \n0.958 \n0.052 \n2.68 \n128 \nSG-25 \n31.74 \n0.953 \n0.062 \n31.63 \n0.958 \n0.052 \n2.26 \n151 \n\n\n\nTable 4 :\n4Spherical Basis Function Ablation.We experiment with \nvarious versions of spherical basis functions, including SH-16, SH-\n25 and SG-25. \n\n\nPSNR \u2191\nPSNRChair Drums Ficus Hotdog Lego MaterialsMic \nShip Mean \n\nNeRF (original) \n33.00 25.01 30.13 \n36.18 \n32.54 \n29.62 \n32.91 28.65 31.01 \nNeRF \n34.08 25.03 30.43 \n36.92 \n33.28 \n29.91 \n34.53 29.36 31.69 \nSRN \n26.96 17.18 20.73 \n26.81 \n20.85 \n18.09 \n26.85 20.60 22.26 \nNeural Volumes \n28.33 22.58 24.79 \n30.71 \n26.08 \n24.22 \n27.78 23.93 26.05 \nNSVF \n33.19 25.18 31.23 \n37.14 \n32.29 \n32.68 \n34.27 27.93 31.75 \n\nNeRF-SH \n33.98 25.17 30.72 \n36.75 \n32.77 \n29.95 \n34.04 29.21 31.57 \nPlenOctree from NeRF-SH \n33.19 25.01 30.56 \n36.15 \n32.12 \n29.56 \n33.01 28.58 31.02 \nPlenOctree after fine-tuning \n34.66 25.31 30.79 \n36.79 \n32.95 \n29.76 \n33.97 29.42 31.71 \n\nSSIM \u2191 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nNeRF (original) \n0.967 0.925 0.964 \n0.974 \n0.961 \n0.949 \n0.980 0.856 0.947 \nNeRF \n0.975 0.925 0.967 \n0.979 \n0.968 \n0.952 \n0.987 0.868 0.953 \nSRN \n0.910 0.766 0.849 \n0.923 \n0.809 \n0.808 \n0.947 0.757 0.846 \nNeural Volumes \n0.916 0.873 0.910 \n0.944 \n0.880 \n0.888 \n0.946 0.784 0.893 \nNSVF \n0.968 0.931 0.960 \n0.987 \n0.973 \n0.854 \n0.980 0.973 0.953 \n\nNeRF-SH \n0.974 0.927 0.968 \n0.978 \n0.966 \n0.951 \n0.985 0.866 0.952 \nPlenOctree from NeRF-SH \n0.970 0.927 0.968 \n0.977 \n0.965 \n0.953 \n0.983 0.863 0.951 \nPlenOctree after fine-tuning \n0.981 0.933 0.970 \n0.982 \n0.971 \n0.955 \n0.987 0.884 0.958 \n\nLPIPS \u2193 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nNeRF (original) \n0.046 0.091 0.044 \n0.121 \n0.050 \n0.063 \n0.028 0.206 0.081 \nNeRF \n0.035 0.085 0.038 \n0.079 \n0.040 \n0.060 \n0.019 0.185 0.068 \nSRN \n0.106 0.267 0.149 \n0.100 \n0.200 \n0.174 \n0.063 0.299 0.170 \nNeural Volumes \n0.109 0.214 0.162 \n0.109 \n0.175 \n0.130 \n0.107 0.276 0.160 \nNSVF \n0.043 0.069 0.017 \n0.025 \n0.029 \n0.021 \n0.010 0.162 0.047 \n\nNeRF-SH \n0.037 0.087 0.039 \n0.041 \n0.041 \n0.060 \n0.021 0.177 0.063 \nPlenOctree from NeRF-SH \n0.039 0.088 0.038 \n0.044 \n0.046 \n0.063 \n0.023 0.189 0.066 \nPlenOctree after fine-tuning \n0.022 0.076 0.038 \n0.032 \n0.034 \n0.059 \n0.017 0.144 0.053 \n\nFPS \u2191 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nNeRF (original) \n0.023 0.023 0.023 \n0.023 \n0.023 \n0.023 \n0.023 0.023 0.023 \nNeRF \n0.045 0.045 0.045 \n0.045 \n0.045 \n0.045 \n0.045 0.045 0.045 \nSRN \n0.909 0.909 0.909 \n0.909 \n0.909 \n0.909 \n0.909 0.909 0.909 \nNeural Volumes \n3.330 3.330 3.330 \n3.330 \n3.330 \n3.330 \n3.330 3.330 3.330 \nNSVF \n1.044 0.735 0.597 \n0.660 \n0.633 \n0.517 \n1.972 0.362 0.815 \n\nNeRF-SH \n0.051 0.051 0.051 \n0.051 \n0.051 \n0.051 \n0.051 0.051 0.051 \nPlenOctree \n352.4 175.9 \n85.6 \n95.5 \n186.8 \n64.2 \n324.9 56.0 167.7 \n\n\n\nTable 5 :\n5Per-scene quantitive results on NeRF-synthetic dataset.Barn Caterpillar Family Ignatius Truck MeanPSNR \u2191 \n\nBarn Caterpillar Family Ignatius Truck Mean \n\nNeRF (original) \n24.05 \n23.75 \n30.29 \n25.43 \n25.36 25.78 \nNeRF \n27.39 \n25.24 \n32.47 \n27.95 \n26.66 27.94 \nSRN \n22.44 \n21.14 \n27.57 \n26.70 \n22.62 24.09 \nNeural Volumes \n20.82 \n20.71 \n28.72 \n26.54 \n21.71 23.70 \nNSVF \n27.16 \n26.44 \n33.58 \n27.91 \n26.92 28.40 \n\nNeRF-SH \n27.05 \n25.06 \n32.28 \n28.06 \n26.66 27.82 \nPlenOctree from NeRF-SH \n25.78 \n24.80 \n32.04 \n27.92 \n26.15 27.34 \nPlenOctree after fine-tuning \n26.80 \n25.29 \n32.85 \n28.19 \n26.83 27.99 \n\nSSIM \u2191 \n\nBarn Caterpillar Family Ignatius Truck Mean \n\nNeRF (original) \n0.750 \n0.860 \n0.932 \n0.920 \n0.860 0.864 \nNeRF \n0.842 \n0.892 \n0.951 \n0.940 \n0.896 0.904 \nSRN \n0.741 \n0.834 \n0.908 \n0.920 \n0.832 0.847 \nNeural Volumes \n0.721 \n0.819 \n0.916 \n0.922 \n0.793 0.834 \nNSVF \n0.823 \n0.900 \n0.954 \n0.930 \n0.895 0.900 \n\nNeRF-SH \n0.838 \n0.891 \n0.949 \n0.940 \n0.895 0.902 \nPlenOctree from NeRF-SH \n0.820 \n0.889 \n0.948 \n0.940 \n0.889 0.897 \nPlenOctree after fine-tuning \n0.856 \n0.907 \n0.962 \n0.948 \n0.914 0.917 \n\nLPIPS \u2193 \n\nBarn Caterpillar Family Ignatius Truck Mean \n\nNeRF (original) \n0.395 \n0.196 \n0.098 \n0.111 \n0.192 0.198 \nNeRF \n0.286 \n0.189 \n0.092 \n0.102 \n0.173 0.168 \nSRN \n0.448 \n0.278 \n0.134 \n0.128 \n0.266 0.251 \nNeural Volumes \n0.479 \n0.280 \n0.111 \n0.117 \n0.312 0.260 \nNSVF \n0.307 \n0.141 \n0.063 \n0.106 \n0.148 0.153 \n\nNeRF-SH \n0.291 \n0.185 \n0.091 \n0.091 \n0.175 0.167 \nPlenOctree from NeRF-SH \n0.296 \n0.188 \n0.094 \n0.092 \n0.180 0.170 \nPlenOctree after fine-tuning \n0.226 \n0.148 \n0.069 \n0.080 \n0.130 0.131 \n\nFPS \u2191 \n\nNeRF (original) \n0.007 \n0.007 \n0.007 \n0.007 \n0.007 0.007 \nNeRF \n0.013 \n0.013 \n0.013 \n0.013 \n0.013 0.013 \nSRN \n0.250 \n0.250 \n0.250 \n0.250 \n0.250 0.250 \nNeural Volumes \n1.000 \n1.000 \n1.000 \n1.000 \n1.000 1.000 \nNSVF \n10.74 \n5.415 \n2.625 \n6.062 \n5.886 6.146 \n\nNeRF-SH \n0.015 \n0.015 \n0.015 \n0.015 \n0.015 0.015 \nPlenOctree (ours) \n46.94 \n54.00 \n32.33 \n15.67 \n62.16 42.22 \n\n\n\nTable 6 :\n6Per-scene quantitive results on Tanks&Temples dataset.PSNR \u2191 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nOurs-1.9G \n34.66 25.31 30.79 \n36.79 \n32.95 \n29.76 \n33.97 29.42 31.71 \nOurs-1.4G \n34.66 25.30 30.82 \n36.36 \n32.96 \n29.75 \n33.98 29.29 31.64 \nOurs-0.4G \n32.92 24.82 30.07 \n36.06 \n31.61 \n28.89 \n32.19 29.04 30.70 \nOurs-0.3G \n32.03 24.10 29.42 \n34.46 \n30.25 \n28.44 \n30.78 27.36 29.60 \n\nGB \u2193 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nOurs-1.9G \n0.830 1.240 1.791 \n2.674 \n2.067 \n3.682 \n0.442 2.689 1.93 \nOurs-1.4G \n0.671 0.852 0.943 \n1.495 \n1.421 \n3.060 \n0.569 1.881 1.36 \nOurs-0.4G \n0.176 0.350 0.287 \n0.419 \n0.499 \n0.295 \n0.327 1.195 0.44 \nOurs-0.3G \n0.131 0.183 0.286 \n0.403 \n0.340 \n0.503 \n0.159 0.381 0.30 \n\nFPS \u2191 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nOurs-1.9G \n352.4 175.9 \n85.6 \n95.5 \n186.8 \n64.2 \n324.9 56.0 167.7 \nOurs-1.4G \n399.7 222.2 147.3 \n163.5 \n247.9 \n68.0 \n393.8 75.4 214.7 \nOurs-0.4G \n639.6 290.0 208.7 \n273.5 \n339.0 \n268.0 \n522.6 86.7 328.5 \nOurs-0.3G \n767.6 424.1 203.8 \n271.7 \n443.6 \n189.1 \n796.4 181.1 409.7 \n\n\n\nTable 7 :\n7Per-scene quantitive results on PlenOctree conversion ablations.Chair Drums Ficus Hotdog Lego Materials Mic Ship MeanPSNR \u2191 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nNeRF-SH9 \n33.88 25.24 30.69 \n36.68 \n32.73 \n29.53 \n33.68 29.11 31.44 \nNeRF-SH16 \n33.98 25.17 30.72 \n36.75 \n32.77 \n29.95 \n34.04 29.21 31.57 \nNeRF-SH25 \n34.01 25.10 30.52 \n36.83 \n32.76 \n30.06 \n34.08 29.11 31.56 \nNeRF-SG25 \n34.08 25.40 31.21 \n36.92 \n32.93 \n29.77 \n34.31 29.28 31.74 \n\nPlenOctree-SH9 \n34.38 25.34 30.72 \n36.68 \n32.79 \n29.16 \n33.23 29.28 31.45 \nPlenOctree-SH16 \n34.66 25.31 30.79 \n36.79 \n32.95 \n29.76 \n33.97 29.42 31.71 \nPlenOctree-SH25 \n34.72 25.32 30.68 \n36.96 \n32.85 \n29.79 \n33.90 29.29 31.69 \nPlenOctree-SG25 \n34.37 25.52 31.16 \n36.67 \n32.98 \n29.41 \n33.63 29.32 31.63 \n\nSSIM \u2191 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nNeRF-SH9 \n0.973 0.928 0.968 \n0.978 \n0.966 \n0.948 \n0.984 0.864 0.951 \nNeRF-SH16 \n0.974 0.927 0.968 \n0.978 \n0.966 \n0.951 \n0.985 0.866 0.952 \nNeRF-SH25 \n0.973 0.926 0.967 \n0.978 \n0.966 \n0.952 \n0.985 0.864 0.951 \nNeRF-SG25 \n0.974 0.930 0.971 \n0.978 \n0.967 \n0.951 \n0.986 0.867 0.953 \n\nPlenOctree-SH9 \n0.980 0.934 0.970 \n0.982 \n0.970 \n0.950 \n0.984 0.881 0.956 \nPlenOctree-SH16 \n0.981 0.933 0.970 \n0.982 \n0.971 \n0.955 \n0.987 0.884 0.958 \nPlenOctree-SH25 \n0.981 0.935 0.971 \n0.983 \n0.971 \n0.955 \n0.987 0.883 0.958 \nPlenOctree-SG25 \n0.980 0.937 0.973 \n0.982 \n0.972 \n0.953 \n0.986 0.883 0.958 \n\nLPIPS \u2193 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nNeRF-SH9 \n0.037 0.086 0.043 \n0.044 \n0.042 \n0.063 \n0.023 0.180 0.065 \nNeRF-SH16 \n0.037 0.087 0.039 \n0.041 \n0.041 \n0.060 \n0.021 0.177 0.063 \nNeRF-SH25 \n0.038 0.087 0.039 \n0.040 \n0.041 \n0.061 \n0.021 0.179 0.063 \nNeRF-SG25 \n0.036 0.083 0.034 \n0.042 \n0.041 \n0.060 \n0.020 0.176 0.062 \n\nPlenOctree-SH9 \n0.023 0.075 0.041 \n0.034 \n0.036 \n0.068 \n0.025 0.146 0.056 \nPlenOctree-SH16 \n0.022 0.076 0.038 \n0.032 \n0.034 \n0.059 \n0.017 0.144 0.053 \nPlenOctree-SH25 \n0.023 0.072 0.036 \n0.031 \n0.034 \n0.060 \n0.017 0.145 0.052 \nPlenOctree-SG25 \n0.023 0.069 0.034 \n0.033 \n0.033 \n0.064 \n0.019 0.144 0.052 \n\nGB \u2193 \n\nPlenOctree-SH9 \n0.45 \n0.67 \n1.15 \n1.27 \n1.16 \n1.48 \n0.16 1.67 \n1.00 \nPlenOctree-SH16 \n0.83 \n1.24 \n1.79 \n2.67 \n2.07 \n3.68 \n0.44 2.69 \n1.93 \nPlenOctree-SH25 \n1.30 \n1.97 \n2.57 \n3.80 \n3.61 \n4.04 \n0.55 3.61 \n2.68 \nPlenOctree-SG25 \n1.03 \n1.68 \n2.43 \n2.66 \n2.66 \n4.44 \n0.49 2.71 \n2.26 \n\nFPS \u2191 \n\nChair Drums Ficus Hotdog Lego Materials \nMic \nShip Mean \n\nPlenOctree-SH9 \n521.1 255.6 116.7 \n183.0 \n275.1 \n132.3 \n519.4 90.6 261.7 \nPlenOctree-SH16 \n352.4 175.9 \n85.6 \n95.5 \n186.8 \n64.2 \n324.9 56.0 167.7 \nPlenOctree-SH25 \n269.2 126.7 \n67.0 \n66.4 \n127.1 \n48.9 \n279.2 41.3 128.2 \nPlenOctree-SG25 \n306.6 151.9 \n74.1 \n104.3 \n153.3 \n51.0 \n294.2 69.6 150.6 \n\n\n\nTable 8 :\n8Per-scene quantitive results on spherical basis function ablations.\nWith 10000 view-direction samples per point, taking about 2 hours, the PSNR is 29.21 vs. 31.02 for our main method prior to optimization.\nBy pre-evaluating \u03c3 on a larger grid and finding the bounding box of all points with \u03c3 \u2265 \u03c4a.\nNote that the background color c N was omitted in equation (1) of the main paper for simplicity.\nFor Tanks&Temples dataset, we hold out 10% of the training set as validation set only for PlenOctree optimization.\nAcknowledgementsWe thank Vickie Ye and Ben Recht for help discussions as well as reviewing the manuscript, Zejian Wang of Pinscreen for helping with video capture, and BAIR commons for an allocation of GCP credits.Appendix A. Additional ResultsA.1. Detailed comparisonsHere we provide further qualitative comparisons with baselines: SRN[45], Neural Volumes[24], NSVF[23]inFigure 10. We show more qualitative results of our method inFigure 11andFigure 12. We also report a perscene breakdown of the quantitative metrics against all approaches inTable 5,6, 7, 8.\nThe plenoptic function and the elements of early vision. James R Edward H Adelson, Bergen, 2Vision and Modeling Group, Media Laboratory, Massachusetts Institute of TechnologyEdward H Adelson, James R Bergen, et al. The plenoptic function and the elements of early vision, volume 2. Vision and Modeling Group, Media Laboratory, Massachusetts In- stitute of Technology, 1991. 2\n\nLambertian reflectance and linear subspaces. Ronen Basri, W David, Jacobs, IEEE transactions on pattern analysis and machine intelligence. 25Ronen Basri and David W Jacobs. Lambertian reflectance and linear subspaces. IEEE transactions on pattern analysis and machine intelligence, 25(2):218-233, 2003. 4\n\nUnstructured lumigraph rendering. Chris Buehler, Michael Bosse, Leonard Mcmillan, Steven Gortler, Michael Cohen, SIGGRAPH. Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured lumigraph ren- dering. In SIGGRAPH, pages 425-432, 2001. 2\n\nLearning implicit fields for generative shape modeling. Zhiqin Chen, Hao Zhang, CVPR. Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In CVPR, 2019. 3\n\nModeling and rendering architecture from photographs: A hybrid geometry-and image-based approach. Paul E Debevec, J Camillo, Jitendra Taylor, Malik, SIGGRAPH. Paul E Debevec, Camillo J Taylor, and Jitendra Malik. Mod- eling and rendering architecture from photographs: A hybrid geometry-and image-based approach. In SIGGRAPH, pages 11-20, 1996. 2\n\nJaxNeRF: an efficient JAX implementation of NeRF. Boyang Deng, Jonathan T Barron, Pratul P Srinivasan, 614Boyang Deng, Jonathan T. Barron, and Pratul P. Srinivasan. JaxNeRF: an efficient JAX implementation of NeRF, 2020. 6, 14\n\nVolume rendering. Loren Robert A Drebin, Pat Carpenter, Hanrahan, ACM SIGGRAPH Computer Graphics. 224Robert A Drebin, Loren Carpenter, and Pat Hanrahan. Vol- ume rendering. ACM SIGGRAPH Computer Graphics, 22(4):65-74, 1988. 2\n\nDispersion on a sphere. Aylmer Ronald, Fisher, Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences. 21711Ronald Aylmer Fisher. Dispersion on a sphere. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 217(1130):295-305, 1953. 4, 11\n\nStephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin, Fastnerf: High-fidelity neural rendering at 200fps. arXiv. Stephan J. Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. arXiv, 2021. 3\n\nImplicit geometric regularization for learning shapes. Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman, ICML. 3Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. ICML, 2020. 3\n\nHierarchical surface prediction for 3d object reconstruction. Christian H\u00e4ne, Shubham Tulsiani, Jitendra Malik, 3DV. IEEEChristian H\u00e4ne, Shubham Tulsiani, and Jitendra Malik. Hi- erarchical surface prediction for 3d object reconstruction. In 3DV, pages 412-420. IEEE, 2017. 3\n\nColor image quantization for frame buffer display. SIGGRAPH. Paul Heckbert, 12Paul Heckbert. Color image quantization for frame buffer display. SIGGRAPH, 1982. 12\n\nBaking neural radiance fields for real-time view synthesis. Peter Hedman, P Pratul, Ben Srinivasan, Jonathan T Mildenhall, Paul Barron, Debevec, arXivPeter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul Debevec. Baking neural ra- diance fields for real-time view synthesis. arXiv, 2021. 3\n\nLearning a multi-view stereo machine. Abhishek Kar, Christian H\u00e4ne, Jitendra Malik, NIPS. 2Abhishek Kar, Christian H\u00e4ne, and Jitendra Malik. Learning a multi-view stereo machine. NIPS, 2017. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. 414Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 4, 14\n\nTanks and temples: Benchmarking large-scale scene reconstruction. Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, Vladlen Koltun, ACM Transactions on Graphics (ToG). 364Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4):1-13, 2017. 6\n\nA survey of octree volume rendering methods. GI, the Gesellschaft f\u00fcr Informatik. Aaron Knoll, 87Aaron Knoll. A survey of octree volume rendering methods. GI, the Gesellschaft f\u00fcr Informatik, page 87, 2006. 3\n\nA theory of shape by space carving. N Kiriakos, Kutulakos, M Steven, Seitz, IJCV. 383Kiriakos N Kutulakos and Steven M Seitz. A theory of shape by space carving. IJCV, 38(3):199-218, 2000. 2\n\nLight field rendering. Marc Levoy, Pat Hanrahan, SIGGRAPH. Marc Levoy and Pat Hanrahan. Light field rendering. In SIGGRAPH, pages 31-42, 1996. 2\n\nMonocular real-time volumetric performance capture. Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski, Hao Li, ECCV. SpringerRuilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski, and Hao Li. Monocular real-time volumetric per- formance capture. In ECCV, pages 49-67. Springer, 2020. 3\n\nInverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single image. Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, Manmohan Chandraker, CVPR. 411Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Inverse ren- dering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single image. In CVPR, pages 2475-2484, 2020. 4, 11\n\nAutoint: Automatic integration for fast neural volume rendering. B David, Lindell, N P Julien, Gordon Martel, Wetzstein, CVPR. 37David B Lindell, Julien NP Martel, and Gordon Wetzstein. Autoint: Automatic integration for fast neural volume ren- dering. In CVPR, 2021. 3, 7\n\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. NeurIPS, 2020. 3, 6. 711Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. NeurIPS, 2020. 3, 6, 7, 11\n\nNeural volumes: Learning dynamic renderable volumes from images. Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh, 65:1- 65:14ACM Transactions on Graphics (TOG). 38411Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from im- ages. ACM Transactions on Graphics (TOG), 38(4):65:1- 65:14, 2019. 1, 2, 7, 11\n\nMixture of volumetric primitives for efficient neural rendering. Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, Jason Saragih, SIGGRAPH. Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix- ture of volumetric primitives for efficient neural rendering. In SIGGRAPH, 2021. 3\n\n. Mark Jean Loup Gailly, Adler, Zlib, 12Jean loup Gailly and Mark Adler. zlib, 2017. 12\n\nAlexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections. Ricardo Martin-Brualla, Noha Radwan, S M Mehdi, Jonathan T Sajjadi, Barron, CVPR. Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duck- worth. NeRF in the Wild: Neural Radiance Fields for Un- constrained Photo Collections. In CVPR, 2021. 3\n\nOccupancy networks: Learning 3d reconstruction in function space. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, Andreas Geiger, CVPR. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- bastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In CVPR, 2019. 3\n\nLocal light field fusion: Practical view synthesis with prescriptive sampling guidelines. Ben Mildenhall, P Pratul, Rodrigo Srinivasan, Nima Khademi Ortiz-Cayon, Ravi Kalantari, Ren Ramamoorthi, Abhishek Ng, Kar, ACM Transactions on Graphics (TOG). 384Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view syn- thesis with prescriptive sampling guidelines. ACM Transac- tions on Graphics (TOG), 38(4):1-14, 2019. 2\n\nBen Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, Nerf, Representing scenes as neural radiance fields for view synthesis. ECCV. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view syn- thesis. ECCV, 2020. 1, 3, 4, 6\n\nDifferentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. Michael Niemeyer, Lars Mescheder, Michael Oechsle, Andreas Geiger, CVPR. 2020Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learn- ing implicit 3d representations without 3d supervision. In CVPR, 2020. 3\n\nDeepsdf: Learning continuous signed distance functions for shape representation. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove, CVPR. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning con- tinuous signed distance functions for shape representation. In CVPR, 2019. 3\n\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, M Steven, Ricardo-Martin Seitz, Brualla, Deformable neural radiance fields. arXivKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo- Martin Brualla. Deformable neural radiance fields. arXiv, 2021. 3\n\nConvolutional occupancy networks. Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, Andreas Geiger, ECCV. 2020Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In ECCV, 2020. 3\n\nSoft 3d reconstruction for view synthesis. Eric Penner, Li Zhang, ACM Transactions on Graphics (TOG). 366Eric Penner and Li Zhang. Soft 3d reconstruction for view synthesis. ACM Transactions on Graphics (TOG), 36(6):1- 11, 2017. 2\n\nOn the relationship between radiance and irradiance: determining the illumination from images of a convex lambertian object. Ravi Ramamoorthi, Pat Hanrahan, JOSA A. 1810Ravi Ramamoorthi and Pat Hanrahan. On the relationship between radiance and irradiance: determining the illumina- tion from images of a convex lambertian object. JOSA A, 18(10):2448-2459, 2001. 4\n\nDeRF: Decomposed radiance fields. Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea Tagliasacchi, CVPR. Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, and Andrea Tagliasacchi. DeRF: Decom- posed radiance fields. In CVPR, 2021. 3\n\nChristian Reiser, Songyou Peng, Yiyi Liao, Andreas Geiger, Kilonerf, Speeding up neural radiance fields with thousands of tiny mlps. arXivChristian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. arXiv, 2021. 3\n\nOctnet: Learning deep 3d representations at high resolutions. Gernot Riegler, Ali Osman Ulusoy, Andreas Geiger, CVPR. Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. Octnet: Learning deep 3d representations at high resolutions. In CVPR, 2017. 3\n\nPifu: Pixel-aligned implicit function for high-resolution clothed human digitization. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li, ICCV. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor- ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitiza- tion. In ICCV, 2019. 3\n\nGRAF: Generative radiance fields for 3d-aware image synthesis. Katja Schwarz, Yiyi Liao, Michael Niemeyer, Andreas Geiger, NeurIPS. 2020Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. GRAF: Generative radiance fields for 3d-aware im- age synthesis. In NeurIPS, 2020. 3\n\nPhotorealistic scene reconstruction by voxel coloring. M Steven, Charles R Seitz, Dyer, IJCV. 352Steven M Seitz and Charles R Dyer. Photorealistic scene re- construction by voxel coloring. IJCV, 35(2):151-173, 1999. 2\n\nLayered depth images. Jonathan Shade, Steven Gortler, Li-Wei He, Richard Szeliski, SIGGRAPH. Jonathan Shade, Steven Gortler, Li-wei He, and Richard Szeliski. Layered depth images. In SIGGRAPH, pages 231- 242, 1998. 2\n\nDeep-Voxels: Learning persistent 3d feature embeddings. Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wetzstein, Michael Zollhofer, CVPR. Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nie\u00dfner, Gordon Wetzstein, and Michael Zollhofer. Deep- Voxels: Learning persistent 3d feature embeddings. In CVPR, pages 2437-2446, 2019. 2\n\nScene representation networks: Continuous 3d-structure-aware neural scene representations. Vincent Sitzmann, Michael Zollh\u00f6fer, Gordon Wetzstein, NeurIPS. 711Vincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wet- zstein. Scene representation networks: Continuous 3d- structure-aware neural scene representations. In NeurIPS, 2019. 3, 7, 11\n\nPrecomputed radiance transfer for real-time rendering in dynamic, lowfrequency lighting environments. Peter-Pike Sloan, Jan Kautz, John Snyder, SIGGRAPH. 411Peter-Pike Sloan, Jan Kautz, and John Snyder. Precomputed radiance transfer for real-time rendering in dynamic, low- frequency lighting environments. In SIGGRAPH, pages 527- 536, 2002. 4, 11\n\nNeRV: Neural reflectance and visibility fields for relighting and view synthesis. P Pratul, Boyang Srinivasan, Xiuming Deng, Matthew Zhang, Ben Tancik, Jonathan T Mildenhall, Barron, CVPR. Pratul P Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, and Jonathan T Barron. NeRV: Neural reflectance and visibility fields for relighting and view synthesis. In CVPR, 2021. 3\n\nPushing the boundaries of view extrapolation with multiplane images. P Pratul, Richard Srinivasan, Jonathan T Tucker, Ravi Barron, Ren Ramamoorthi, Noah Ng, Snavely, CVPR. Pratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing the boundaries of view extrapolation with multiplane images. In CVPR, pages 175-184, 2019. 2\n\nStereo matching with transparency and matting. Richard Szeliski, Polina Golland, ICCV. Richard Szeliski and Polina Golland. Stereo matching with transparency and matting. In ICCV, pages 517-524. IEEE, 1998. 2\n\nNeural geometric level of detail: Real-time rendering with implicit 3D shapes. Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan Mcguire, Sanja Fidler, CVPR. Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. In CVPR, 2021. 3\n\nLearned initializations for optimizing coordinate-based neural representations. Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, Ren Ng, CVPR. Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. In CVPR, 2021. 3\n\nOctree generating networks: Efficient convolutional architectures for high-resolution 3d outputs. Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox, ICCV. Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox. Octree generating networks: Efficient convolutional archi- tectures for high-resolution 3d outputs. In ICCV, pages 2088-2096, 2017. 3\n\nGRF: Learning a general radiance field for 3d scene representation and rendering. Alex Trevithick, Bo Yang, arXivAlex Trevithick and Bo Yang. GRF: Learning a general radi- ance field for 3d scene representation and rendering. arXiv, 2021. 3\n\nAll-frequency precomputed radiance transfer using spherical radial basis functions and clustered tensor approximation. Yu-Ting Tsai, Zen-Chung Shih, ACM Transactions on graphics (TOG). 25311Yu-Ting Tsai and Zen-Chung Shih. All-frequency precom- puted radiance transfer using spherical radial basis functions and clustered tensor approximation. ACM Transactions on graphics (TOG), 25(3):967-976, 2006. 4, 11\n\nMulti-view supervision for single-view reconstruction via differentiable ray consistency. Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, Jitendra Malik, CVPR. Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Ji- tendra Malik. Multi-view supervision for single-view recon- struction via differentiable ray consistency. In CVPR, pages 2626-2634, 2017. 2\n\nLet there be color! large-scale texturing of 3d reconstructions. Michael Waechter, Nils Moehrle, Michael Goesele, ECCV. SpringerMichael Waechter, Nils Moehrle, and Michael Goesele. Let there be color! large-scale texturing of 3d reconstructions. In ECCV, pages 836-850. Springer, 2014. 2\n\nO-cnn: Octree-based convolutional neural networks for 3d shape analysis. Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, Xin Tong, ACM Transactions on Graphics (TOG). 364Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-cnn: Octree-based convolutional neu- ral networks for 3d shape analysis. ACM Transactions on Graphics (TOG), 36(4):1-11, 2017. 3\n\nIBRNet: Learning multi-view image-based rendering. Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser, CVPR. Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srini- vasan, Howard Zhou, Jonathan T Barron, Ricardo Martin- Brualla, Noah Snavely, and Thomas Funkhouser. IBRNet: Learning multi-view image-based rendering. In CVPR, 2021. 3\n\nNeX: Real-time view synthesis with neural basis expansion. Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn, 3Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. NeX: Real-time view synthesis with neural basis expansion. In 2021. 3, 8\n\nSurface light fields for 3d photography. N Daniel, Wood, Ken Daniel I Azuma, Brian Aldinger, Tom Curless, Duchamp, H David, Werner Salesin, Stuetzle, SIGGRAPH. Daniel N Wood, Daniel I Azuma, Ken Aldinger, Brian Cur- less, Tom Duchamp, David H Salesin, and Werner Stuet- zle. Surface light fields for 3d photography. In SIGGRAPH, pages 287-296, 2000. 2\n\nDISN: Deep implicit surface network for high-quality single-view 3d reconstruction. Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, Ulrich Neumann, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc323Qiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. DISN: Deep implicit surface network for high-quality single-view 3d reconstruction. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems 32, pages 492-502. Curran As- sociates, Inc., 2019. 3\n\nMultiview neural surface reconstruction by disentangling geometry and appearance. Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, Yaron Lipman, NeurIPS. Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neu- ral surface reconstruction by disentangling geometry and ap- pearance. In NeurIPS, 2020. 3\n\nAlex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa, Neural radiance fields from one or few images. CVPRAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In CVPR, 2021. 3\n\nStereo magnification: Learning view synthesis using multiplane images. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely, SIGGRAPH. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view syn- thesis using multiplane images. In SIGGRAPH, 2018. 2\n", "annotations": {"author": "[{\"end\":87,\"start\":65},{\"end\":167,\"start\":88},{\"end\":197,\"start\":168},{\"end\":219,\"start\":198},{\"end\":241,\"start\":220},{\"end\":272,\"start\":242}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":70},{\"end\":98,\"start\":96},{\"end\":182,\"start\":176},{\"end\":204,\"start\":202},{\"end\":226,\"start\":224},{\"end\":257,\"start\":249}]", "author_first_name": "[{\"end\":69,\"start\":65},{\"end\":95,\"start\":88},{\"end\":175,\"start\":168},{\"end\":201,\"start\":198},{\"end\":223,\"start\":220},{\"end\":248,\"start\":242}]", "author_affiliation": "[{\"end\":86,\"start\":74},{\"end\":112,\"start\":100},{\"end\":166,\"start\":114},{\"end\":196,\"start\":184},{\"end\":218,\"start\":206},{\"end\":240,\"start\":228},{\"end\":271,\"start\":259}]", "title": "[{\"end\":62,\"start\":1},{\"end\":334,\"start\":273}]", "venue": null, "abstract": "[{\"end\":1878,\"start\":336}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2985,\"start\":2981},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3025,\"start\":3021},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5084,\"start\":5081},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8380,\"start\":8376},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8411,\"start\":8407},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8413,\"start\":8411},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8415,\"start\":8413},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8598,\"start\":8594},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8601,\"start\":8598},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8604,\"start\":8601},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8898,\"start\":8895},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8952,\"start\":8948},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8955,\"start\":8952},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8958,\"start\":8955},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8961,\"start\":8958},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8964,\"start\":8961},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8967,\"start\":8964},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9002,\"start\":8998},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9005,\"start\":9002},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9008,\"start\":9005},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9011,\"start\":9008},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9014,\"start\":9011},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9536,\"start\":9532},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10367,\"start\":10363},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10538,\"start\":10534},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10541,\"start\":10538},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10544,\"start\":10541},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10547,\"start\":10544},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":10584,\"start\":10580},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10732,\"start\":10728},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10791,\"start\":10787},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10793,\"start\":10791},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10796,\"start\":10793},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11264,\"start\":11260},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11266,\"start\":11264},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11269,\"start\":11266},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11272,\"start\":11269},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11275,\"start\":11272},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11278,\"start\":11275},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11307,\"start\":11303},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11310,\"start\":11307},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":11313,\"start\":11310},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":11316,\"start\":11313},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11335,\"start\":11331},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11441,\"start\":11437},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11453,\"start\":11449},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11484,\"start\":11480},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11487,\"start\":11484},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11490,\"start\":11487},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11493,\"start\":11490},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12228,\"start\":12224},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":12231,\"start\":12228},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":12234,\"start\":12231},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12256,\"start\":12252},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12324,\"start\":12320},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12559,\"start\":12555},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12633,\"start\":12629},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13015,\"start\":13011},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13368,\"start\":13364},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14276,\"start\":14272},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14553,\"start\":14549},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16636,\"start\":16632},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16638,\"start\":16636},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":16667,\"start\":16663},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18238,\"start\":18235},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":18329,\"start\":18325},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":18332,\"start\":18329},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18335,\"start\":18332},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21227,\"start\":21226},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23460,\"start\":23456},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23640,\"start\":23636},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23917,\"start\":23913},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24197,\"start\":24193},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24324,\"start\":24321},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24922,\"start\":24918},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24939,\"start\":24935},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25011,\"start\":25007},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25035,\"start\":25031},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":30034,\"start\":30030},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31321,\"start\":31318},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32741,\"start\":32738},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":33173,\"start\":33169},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":33176,\"start\":33173},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33179,\"start\":33176},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33827,\"start\":33823},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34245,\"start\":34241},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":35387,\"start\":35384},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37008,\"start\":37004},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":37773,\"start\":37770},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37983,\"start\":37979},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":38819,\"start\":38818}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39120,\"start\":39091},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39380,\"start\":39121},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39425,\"start\":39381},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39780,\"start\":39426},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39838,\"start\":39781},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39916,\"start\":39839},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39993,\"start\":39917},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40404,\"start\":39994},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40703,\"start\":40405},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41250,\"start\":40704},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41584,\"start\":41251},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":41734,\"start\":41585},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":44251,\"start\":41735},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":46235,\"start\":44252},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":47333,\"start\":46236},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":50065,\"start\":47334},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":50145,\"start\":50066}]", "paragraph": "[{\"end\":2298,\"start\":1894},{\"end\":2322,\"start\":2315},{\"end\":2389,\"start\":2373},{\"end\":2751,\"start\":2397},{\"end\":3433,\"start\":2753},{\"end\":3827,\"start\":3435},{\"end\":4568,\"start\":3829},{\"end\":4862,\"start\":4570},{\"end\":5616,\"start\":4864},{\"end\":6492,\"start\":5618},{\"end\":6854,\"start\":6494},{\"end\":7181,\"start\":6856},{\"end\":7472,\"start\":7183},{\"end\":7524,\"start\":7474},{\"end\":7621,\"start\":7526},{\"end\":7725,\"start\":7623},{\"end\":7851,\"start\":7727},{\"end\":7987,\"start\":7853},{\"end\":8793,\"start\":8004},{\"end\":10846,\"start\":8795},{\"end\":11866,\"start\":10884},{\"end\":13291,\"start\":11868},{\"end\":13984,\"start\":13334},{\"end\":14038,\"start\":14033},{\"end\":14310,\"start\":14078},{\"end\":14572,\"start\":14341},{\"end\":14921,\"start\":14574},{\"end\":15363,\"start\":14923},{\"end\":15716,\"start\":15374},{\"end\":16202,\"start\":15718},{\"end\":16462,\"start\":16204},{\"end\":16830,\"start\":16505},{\"end\":17113,\"start\":16887},{\"end\":18168,\"start\":17152},{\"end\":18947,\"start\":18170},{\"end\":19261,\"start\":18949},{\"end\":19478,\"start\":19263},{\"end\":19803,\"start\":19523},{\"end\":20491,\"start\":19848},{\"end\":20689,\"start\":20537},{\"end\":21059,\"start\":20691},{\"end\":22127,\"start\":21061},{\"end\":22420,\"start\":22129},{\"end\":22476,\"start\":22422},{\"end\":23133,\"start\":22504},{\"end\":23366,\"start\":23135},{\"end\":23408,\"start\":23399},{\"end\":24129,\"start\":23410},{\"end\":24470,\"start\":24131},{\"end\":24805,\"start\":24487},{\"end\":25036,\"start\":24807},{\"end\":25372,\"start\":25059},{\"end\":25786,\"start\":25374},{\"end\":26310,\"start\":25788},{\"end\":26589,\"start\":26339},{\"end\":27159,\"start\":26632},{\"end\":27454,\"start\":27161},{\"end\":28138,\"start\":27496},{\"end\":28439,\"start\":28140},{\"end\":30035,\"start\":28454},{\"end\":30402,\"start\":30037},{\"end\":30864,\"start\":30445},{\"end\":31231,\"start\":30866},{\"end\":31737,\"start\":31233},{\"end\":32010,\"start\":31806},{\"end\":32194,\"start\":32012},{\"end\":32421,\"start\":32253},{\"end\":32597,\"start\":32519},{\"end\":32923,\"start\":32642},{\"end\":33243,\"start\":32952},{\"end\":33324,\"start\":33274},{\"end\":33735,\"start\":33356},{\"end\":34140,\"start\":33737},{\"end\":34246,\"start\":34142},{\"end\":34416,\"start\":34248},{\"end\":34700,\"start\":34470},{\"end\":35155,\"start\":34722},{\"end\":35501,\"start\":35195},{\"end\":35836,\"start\":35503},{\"end\":35971,\"start\":35876},{\"end\":36064,\"start\":36059},{\"end\":36171,\"start\":36126},{\"end\":36292,\"start\":36173},{\"end\":36474,\"start\":36322},{\"end\":36564,\"start\":36476},{\"end\":36685,\"start\":36639},{\"end\":37019,\"start\":36807},{\"end\":37441,\"start\":37081},{\"end\":37673,\"start\":37443},{\"end\":38255,\"start\":37707},{\"end\":39090,\"start\":38296}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14032,\"start\":13985},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14077,\"start\":14039},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14340,\"start\":14311},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16886,\"start\":16831},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17151,\"start\":17114},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19522,\"start\":19479},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20536,\"start\":20492},{\"attributes\":{\"id\":\"formula_7\"},\"end\":32252,\"start\":32195},{\"attributes\":{\"id\":\"formula_8\"},\"end\":32518,\"start\":32422},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32641,\"start\":32598},{\"attributes\":{\"id\":\"formula_10\"},\"end\":32951,\"start\":32924},{\"attributes\":{\"id\":\"formula_11\"},\"end\":33273,\"start\":33244},{\"attributes\":{\"id\":\"formula_12\"},\"end\":35194,\"start\":35156},{\"attributes\":{\"id\":\"formula_13\"},\"end\":36058,\"start\":35972},{\"attributes\":{\"id\":\"formula_14\"},\"end\":36125,\"start\":36065},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36321,\"start\":36293},{\"attributes\":{\"id\":\"formula_16\"},\"end\":36638,\"start\":36565},{\"attributes\":{\"id\":\"formula_17\"},\"end\":36806,\"start\":36686},{\"attributes\":{\"id\":\"formula_18\"},\"end\":37080,\"start\":37020}]", "table_ref": "[{\"end\":18434,\"start\":18427},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25195,\"start\":25175},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26484,\"start\":26477}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1892,\"start\":1880},{\"end\":2313,\"start\":2301},{\"end\":2349,\"start\":2325},{\"end\":2371,\"start\":2352},{\"attributes\":{\"n\":\"150\"},\"end\":2395,\"start\":2392},{\"attributes\":{\"n\":\"2.\"},\"end\":8002,\"start\":7990},{\"end\":10882,\"start\":10849},{\"attributes\":{\"n\":\"3.\"},\"end\":13307,\"start\":13294},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13332,\"start\":13310},{\"attributes\":{\"n\":\"4.\"},\"end\":15372,\"start\":15366},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16503,\"start\":16465},{\"attributes\":{\"n\":\"4.2.\"},\"end\":19846,\"start\":19806},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22502,\"start\":22479},{\"attributes\":{\"n\":\"5.\"},\"end\":23376,\"start\":23369},{\"attributes\":{\"n\":\"5.1.\"},\"end\":23397,\"start\":23379},{\"end\":24485,\"start\":24473},{\"attributes\":{\"n\":\"5.2.\"},\"end\":25057,\"start\":25039},{\"attributes\":{\"n\":\"5.3.\"},\"end\":26337,\"start\":26313},{\"attributes\":{\"n\":\"5.4.\"},\"end\":26630,\"start\":26592},{\"attributes\":{\"n\":\"5.5.\"},\"end\":27494,\"start\":27457},{\"attributes\":{\"n\":\"6.\"},\"end\":28452,\"start\":28442},{\"end\":30443,\"start\":30405},{\"end\":31760,\"start\":31740},{\"end\":31804,\"start\":31763},{\"end\":33354,\"start\":33327},{\"end\":34468,\"start\":34419},{\"end\":34720,\"start\":34703},{\"end\":35874,\"start\":35839},{\"end\":37705,\"start\":37676},{\"end\":38294,\"start\":38258},{\"end\":39102,\"start\":39092},{\"end\":39132,\"start\":39122},{\"end\":39392,\"start\":39382},{\"end\":39437,\"start\":39427},{\"end\":39793,\"start\":39782},{\"end\":39851,\"start\":39840},{\"end\":39929,\"start\":39918},{\"end\":40415,\"start\":40406},{\"end\":40714,\"start\":40705},{\"end\":41595,\"start\":41586},{\"end\":41742,\"start\":41736},{\"end\":44262,\"start\":44253},{\"end\":46246,\"start\":46237},{\"end\":47344,\"start\":47335},{\"end\":50076,\"start\":50067}]", "table": "[{\"end\":40404,\"start\":40064},{\"end\":41250,\"start\":40766},{\"end\":41584,\"start\":41344},{\"end\":41734,\"start\":41631},{\"end\":44251,\"start\":41786},{\"end\":46235,\"start\":44362},{\"end\":47333,\"start\":46302},{\"end\":50065,\"start\":47463}]", "figure_caption": "[{\"end\":39120,\"start\":39104},{\"end\":39380,\"start\":39134},{\"end\":39425,\"start\":39394},{\"end\":39780,\"start\":39439},{\"end\":39838,\"start\":39796},{\"end\":39916,\"start\":39854},{\"end\":39993,\"start\":39932},{\"end\":40064,\"start\":39996},{\"end\":40703,\"start\":40417},{\"end\":40766,\"start\":40716},{\"end\":41344,\"start\":41253},{\"end\":41631,\"start\":41597},{\"end\":41786,\"start\":41747},{\"end\":44362,\"start\":44264},{\"end\":46302,\"start\":46248},{\"end\":47463,\"start\":47346},{\"end\":50145,\"start\":50078}]", "figure_ref": "[{\"end\":2416,\"start\":2408},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6491,\"start\":6485},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16156,\"start\":16150},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17630,\"start\":17624},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19764,\"start\":19758},{\"end\":21653,\"start\":21645},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23231,\"start\":23225},{\"end\":24528,\"start\":24520},{\"end\":25392,\"start\":25377},{\"end\":26472,\"start\":26464},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27172,\"start\":27164},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27983,\"start\":27975},{\"end\":29753,\"start\":29745}]", "bib_author_first_name": "[{\"end\":51212,\"start\":51207},{\"end\":51214,\"start\":51213},{\"end\":51577,\"start\":51572},{\"end\":51586,\"start\":51585},{\"end\":51872,\"start\":51867},{\"end\":51889,\"start\":51882},{\"end\":51904,\"start\":51897},{\"end\":51921,\"start\":51915},{\"end\":51938,\"start\":51931},{\"end\":52174,\"start\":52168},{\"end\":52184,\"start\":52181},{\"end\":52414,\"start\":52413},{\"end\":52432,\"start\":52424},{\"end\":52703,\"start\":52697},{\"end\":52718,\"start\":52710},{\"end\":52720,\"start\":52719},{\"end\":52735,\"start\":52729},{\"end\":52737,\"start\":52736},{\"end\":52898,\"start\":52893},{\"end\":52919,\"start\":52916},{\"end\":53132,\"start\":53126},{\"end\":53419,\"start\":53412},{\"end\":53421,\"start\":53420},{\"end\":53435,\"start\":53430},{\"end\":53453,\"start\":53446},{\"end\":53468,\"start\":53463},{\"end\":53484,\"start\":53478},{\"end\":53769,\"start\":53765},{\"end\":53781,\"start\":53777},{\"end\":53792,\"start\":53789},{\"end\":53804,\"start\":53799},{\"end\":53818,\"start\":53813},{\"end\":54041,\"start\":54032},{\"end\":54055,\"start\":54048},{\"end\":54074,\"start\":54066},{\"end\":54312,\"start\":54308},{\"end\":54476,\"start\":54471},{\"end\":54486,\"start\":54485},{\"end\":54498,\"start\":54495},{\"end\":54519,\"start\":54511},{\"end\":54521,\"start\":54520},{\"end\":54538,\"start\":54534},{\"end\":54775,\"start\":54767},{\"end\":54790,\"start\":54781},{\"end\":54805,\"start\":54797},{\"end\":54968,\"start\":54967},{\"end\":54984,\"start\":54979},{\"end\":55174,\"start\":55170},{\"end\":55192,\"start\":55186},{\"end\":55206,\"start\":55199},{\"end\":55220,\"start\":55213},{\"end\":55541,\"start\":55536},{\"end\":55701,\"start\":55700},{\"end\":55724,\"start\":55723},{\"end\":55883,\"start\":55879},{\"end\":55894,\"start\":55891},{\"end\":56061,\"start\":56054},{\"end\":56073,\"start\":56066},{\"end\":56087,\"start\":56079},{\"end\":56099,\"start\":56095},{\"end\":56111,\"start\":56107},{\"end\":56126,\"start\":56123},{\"end\":56440,\"start\":56432},{\"end\":56453,\"start\":56445},{\"end\":56467,\"start\":56463},{\"end\":56487,\"start\":56481},{\"end\":56508,\"start\":56500},{\"end\":56841,\"start\":56840},{\"end\":56859,\"start\":56858},{\"end\":56861,\"start\":56860},{\"end\":56876,\"start\":56870},{\"end\":57056,\"start\":57049},{\"end\":57068,\"start\":57062},{\"end\":57081,\"start\":57073},{\"end\":57383,\"start\":57376},{\"end\":57399,\"start\":57394},{\"end\":57412,\"start\":57407},{\"end\":57429,\"start\":57422},{\"end\":57447,\"start\":57440},{\"end\":57463,\"start\":57458},{\"end\":57837,\"start\":57830},{\"end\":57853,\"start\":57848},{\"end\":57868,\"start\":57861},{\"end\":57886,\"start\":57879},{\"end\":57904,\"start\":57899},{\"end\":57918,\"start\":57913},{\"end\":58135,\"start\":58131},{\"end\":58345,\"start\":58338},{\"end\":58366,\"start\":58362},{\"end\":58376,\"start\":58375},{\"end\":58378,\"start\":58377},{\"end\":58394,\"start\":58386},{\"end\":58396,\"start\":58395},{\"end\":58710,\"start\":58706},{\"end\":58729,\"start\":58722},{\"end\":58746,\"start\":58739},{\"end\":58766,\"start\":58757},{\"end\":58783,\"start\":58776},{\"end\":59067,\"start\":59064},{\"end\":59081,\"start\":59080},{\"end\":59097,\"start\":59090},{\"end\":59114,\"start\":59110},{\"end\":59122,\"start\":59115},{\"end\":59140,\"start\":59136},{\"end\":59155,\"start\":59152},{\"end\":59177,\"start\":59169},{\"end\":59506,\"start\":59503},{\"end\":59520,\"start\":59519},{\"end\":59536,\"start\":59529},{\"end\":59557,\"start\":59549},{\"end\":59559,\"start\":59558},{\"end\":59572,\"start\":59568},{\"end\":59584,\"start\":59581},{\"end\":59987,\"start\":59980},{\"end\":60002,\"start\":59998},{\"end\":60021,\"start\":60014},{\"end\":60038,\"start\":60031},{\"end\":60337,\"start\":60327},{\"end\":60349,\"start\":60344},{\"end\":60366,\"start\":60360},{\"end\":60382,\"start\":60375},{\"end\":60399,\"start\":60393},{\"end\":60614,\"start\":60606},{\"end\":60628,\"start\":60621},{\"end\":60644,\"start\":60636},{\"end\":60646,\"start\":60645},{\"end\":60661,\"start\":60655},{\"end\":60674,\"start\":60671},{\"end\":60676,\"start\":60675},{\"end\":60687,\"start\":60686},{\"end\":60710,\"start\":60696},{\"end\":60984,\"start\":60977},{\"end\":60998,\"start\":60991},{\"end\":61013,\"start\":61009},{\"end\":61029,\"start\":61025},{\"end\":61048,\"start\":61041},{\"end\":61250,\"start\":61246},{\"end\":61261,\"start\":61259},{\"end\":61564,\"start\":61560},{\"end\":61581,\"start\":61578},{\"end\":61841,\"start\":61835},{\"end\":61853,\"start\":61850},{\"end\":61868,\"start\":61861},{\"end\":61880,\"start\":61878},{\"end\":61894,\"start\":61885},{\"end\":61905,\"start\":61899},{\"end\":62078,\"start\":62069},{\"end\":62094,\"start\":62087},{\"end\":62105,\"start\":62101},{\"end\":62119,\"start\":62112},{\"end\":62428,\"start\":62422},{\"end\":62441,\"start\":62438},{\"end\":62463,\"start\":62456},{\"end\":62706,\"start\":62698},{\"end\":62718,\"start\":62714},{\"end\":62731,\"start\":62726},{\"end\":62747,\"start\":62741},{\"end\":62765,\"start\":62759},{\"end\":62779,\"start\":62776},{\"end\":63056,\"start\":63051},{\"end\":63070,\"start\":63066},{\"end\":63084,\"start\":63077},{\"end\":63102,\"start\":63095},{\"end\":63330,\"start\":63329},{\"end\":63348,\"start\":63339},{\"end\":63523,\"start\":63515},{\"end\":63537,\"start\":63531},{\"end\":63553,\"start\":63547},{\"end\":63565,\"start\":63558},{\"end\":63774,\"start\":63767},{\"end\":63791,\"start\":63785},{\"end\":63804,\"start\":63799},{\"end\":63820,\"start\":63812},{\"end\":63836,\"start\":63830},{\"end\":63855,\"start\":63848},{\"end\":64167,\"start\":64160},{\"end\":64185,\"start\":64178},{\"end\":64203,\"start\":64197},{\"end\":64520,\"start\":64510},{\"end\":64531,\"start\":64528},{\"end\":64543,\"start\":64539},{\"end\":64840,\"start\":64839},{\"end\":64855,\"start\":64849},{\"end\":64875,\"start\":64868},{\"end\":64889,\"start\":64882},{\"end\":64900,\"start\":64897},{\"end\":64917,\"start\":64909},{\"end\":64919,\"start\":64918},{\"end\":65220,\"start\":65219},{\"end\":65236,\"start\":65229},{\"end\":65257,\"start\":65249},{\"end\":65259,\"start\":65258},{\"end\":65272,\"start\":65268},{\"end\":65284,\"start\":65281},{\"end\":65302,\"start\":65298},{\"end\":65578,\"start\":65571},{\"end\":65595,\"start\":65589},{\"end\":65819,\"start\":65813},{\"end\":65834,\"start\":65830},{\"end\":65852,\"start\":65845},{\"end\":65865,\"start\":65858},{\"end\":65880,\"start\":65873},{\"end\":65892,\"start\":65887},{\"end\":65913,\"start\":65909},{\"end\":65930,\"start\":65924},{\"end\":65945,\"start\":65940},{\"end\":66289,\"start\":66282},{\"end\":66301,\"start\":66298},{\"end\":66322,\"start\":66314},{\"end\":66333,\"start\":66329},{\"end\":66349,\"start\":66343},{\"end\":66351,\"start\":66350},{\"end\":66372,\"start\":66364},{\"end\":66374,\"start\":66373},{\"end\":66386,\"start\":66383},{\"end\":66713,\"start\":66708},{\"end\":66733,\"start\":66727},{\"end\":66753,\"start\":66747},{\"end\":67043,\"start\":67039},{\"end\":67058,\"start\":67056},{\"end\":67325,\"start\":67318},{\"end\":67341,\"start\":67332},{\"end\":67704,\"start\":67697},{\"end\":67722,\"start\":67715},{\"end\":67735,\"start\":67729},{\"end\":67737,\"start\":67736},{\"end\":67753,\"start\":67745},{\"end\":68036,\"start\":68029},{\"end\":68051,\"start\":68047},{\"end\":68068,\"start\":68061},{\"end\":68336,\"start\":68326},{\"end\":68347,\"start\":68343},{\"end\":68360,\"start\":68353},{\"end\":68373,\"start\":68366},{\"end\":68382,\"start\":68379},{\"end\":68686,\"start\":68678},{\"end\":68701,\"start\":68693},{\"end\":68712,\"start\":68708},{\"end\":68727,\"start\":68721},{\"end\":68746,\"start\":68740},{\"end\":68761,\"start\":68753},{\"end\":68763,\"start\":68762},{\"end\":68779,\"start\":68772},{\"end\":68800,\"start\":68796},{\"end\":68816,\"start\":68810},{\"end\":69129,\"start\":69121},{\"end\":69151,\"start\":69143},{\"end\":69173,\"start\":69165},{\"end\":69195,\"start\":69187},{\"end\":69420,\"start\":69419},{\"end\":69438,\"start\":69435},{\"end\":69460,\"start\":69455},{\"end\":69474,\"start\":69471},{\"end\":69494,\"start\":69493},{\"end\":69508,\"start\":69502},{\"end\":69823,\"start\":69815},{\"end\":69834,\"start\":69828},{\"end\":69846,\"start\":69841},{\"end\":69862,\"start\":69855},{\"end\":69875,\"start\":69869},{\"end\":70488,\"start\":70484},{\"end\":70500,\"start\":70496},{\"end\":70513,\"start\":70509},{\"end\":70527,\"start\":70521},{\"end\":70540,\"start\":70535},{\"end\":70554,\"start\":70549},{\"end\":70567,\"start\":70562},{\"end\":70792,\"start\":70788},{\"end\":70803,\"start\":70797},{\"end\":70815,\"start\":70808},{\"end\":70830,\"start\":70824},{\"end\":71103,\"start\":71096},{\"end\":71117,\"start\":71110},{\"end\":71130,\"start\":71126},{\"end\":71144,\"start\":71138},{\"end\":71156,\"start\":71152}]", "bib_author_last_name": "[{\"end\":51231,\"start\":51215},{\"end\":51239,\"start\":51233},{\"end\":51583,\"start\":51578},{\"end\":51592,\"start\":51587},{\"end\":51600,\"start\":51594},{\"end\":51880,\"start\":51873},{\"end\":51895,\"start\":51890},{\"end\":51913,\"start\":51905},{\"end\":51929,\"start\":51922},{\"end\":51944,\"start\":51939},{\"end\":52179,\"start\":52175},{\"end\":52190,\"start\":52185},{\"end\":52411,\"start\":52397},{\"end\":52422,\"start\":52415},{\"end\":52439,\"start\":52433},{\"end\":52446,\"start\":52441},{\"end\":52708,\"start\":52704},{\"end\":52727,\"start\":52721},{\"end\":52748,\"start\":52738},{\"end\":52914,\"start\":52899},{\"end\":52929,\"start\":52920},{\"end\":52939,\"start\":52931},{\"end\":53139,\"start\":53133},{\"end\":53147,\"start\":53141},{\"end\":53428,\"start\":53422},{\"end\":53444,\"start\":53436},{\"end\":53461,\"start\":53454},{\"end\":53476,\"start\":53469},{\"end\":53493,\"start\":53485},{\"end\":53775,\"start\":53770},{\"end\":53787,\"start\":53782},{\"end\":53797,\"start\":53793},{\"end\":53811,\"start\":53805},{\"end\":53825,\"start\":53819},{\"end\":54046,\"start\":54042},{\"end\":54064,\"start\":54056},{\"end\":54080,\"start\":54075},{\"end\":54321,\"start\":54313},{\"end\":54483,\"start\":54477},{\"end\":54493,\"start\":54487},{\"end\":54509,\"start\":54499},{\"end\":54532,\"start\":54522},{\"end\":54545,\"start\":54539},{\"end\":54554,\"start\":54547},{\"end\":54779,\"start\":54776},{\"end\":54795,\"start\":54791},{\"end\":54811,\"start\":54806},{\"end\":54977,\"start\":54969},{\"end\":54991,\"start\":54985},{\"end\":54995,\"start\":54993},{\"end\":55184,\"start\":55175},{\"end\":55197,\"start\":55193},{\"end\":55211,\"start\":55207},{\"end\":55227,\"start\":55221},{\"end\":55547,\"start\":55542},{\"end\":55710,\"start\":55702},{\"end\":55721,\"start\":55712},{\"end\":55731,\"start\":55725},{\"end\":55738,\"start\":55733},{\"end\":55889,\"start\":55884},{\"end\":55903,\"start\":55895},{\"end\":56064,\"start\":56062},{\"end\":56077,\"start\":56074},{\"end\":56093,\"start\":56088},{\"end\":56105,\"start\":56100},{\"end\":56121,\"start\":56112},{\"end\":56129,\"start\":56127},{\"end\":56443,\"start\":56441},{\"end\":56461,\"start\":56454},{\"end\":56479,\"start\":56468},{\"end\":56498,\"start\":56488},{\"end\":56519,\"start\":56509},{\"end\":56847,\"start\":56842},{\"end\":56856,\"start\":56849},{\"end\":56868,\"start\":56862},{\"end\":56883,\"start\":56877},{\"end\":56894,\"start\":56885},{\"end\":57060,\"start\":57057},{\"end\":57071,\"start\":57069},{\"end\":57085,\"start\":57082},{\"end\":57392,\"start\":57384},{\"end\":57405,\"start\":57400},{\"end\":57420,\"start\":57413},{\"end\":57438,\"start\":57430},{\"end\":57456,\"start\":57448},{\"end\":57470,\"start\":57464},{\"end\":57846,\"start\":57838},{\"end\":57859,\"start\":57854},{\"end\":57877,\"start\":57869},{\"end\":57897,\"start\":57887},{\"end\":57911,\"start\":57905},{\"end\":57926,\"start\":57919},{\"end\":58152,\"start\":58136},{\"end\":58159,\"start\":58154},{\"end\":58165,\"start\":58161},{\"end\":58360,\"start\":58346},{\"end\":58373,\"start\":58367},{\"end\":58384,\"start\":58379},{\"end\":58404,\"start\":58397},{\"end\":58412,\"start\":58406},{\"end\":58720,\"start\":58711},{\"end\":58737,\"start\":58730},{\"end\":58755,\"start\":58747},{\"end\":58774,\"start\":58767},{\"end\":58790,\"start\":58784},{\"end\":59078,\"start\":59068},{\"end\":59088,\"start\":59082},{\"end\":59108,\"start\":59098},{\"end\":59134,\"start\":59123},{\"end\":59150,\"start\":59141},{\"end\":59167,\"start\":59156},{\"end\":59180,\"start\":59178},{\"end\":59185,\"start\":59182},{\"end\":59517,\"start\":59507},{\"end\":59527,\"start\":59521},{\"end\":59547,\"start\":59537},{\"end\":59566,\"start\":59560},{\"end\":59579,\"start\":59573},{\"end\":59596,\"start\":59585},{\"end\":59600,\"start\":59598},{\"end\":59606,\"start\":59602},{\"end\":59996,\"start\":59988},{\"end\":60012,\"start\":60003},{\"end\":60029,\"start\":60022},{\"end\":60045,\"start\":60039},{\"end\":60342,\"start\":60338},{\"end\":60358,\"start\":60350},{\"end\":60373,\"start\":60367},{\"end\":60391,\"start\":60383},{\"end\":60409,\"start\":60400},{\"end\":60619,\"start\":60615},{\"end\":60634,\"start\":60629},{\"end\":60653,\"start\":60647},{\"end\":60669,\"start\":60662},{\"end\":60684,\"start\":60677},{\"end\":60694,\"start\":60688},{\"end\":60716,\"start\":60711},{\"end\":60725,\"start\":60718},{\"end\":60989,\"start\":60985},{\"end\":61007,\"start\":60999},{\"end\":61023,\"start\":61014},{\"end\":61039,\"start\":61030},{\"end\":61055,\"start\":61049},{\"end\":61257,\"start\":61251},{\"end\":61267,\"start\":61262},{\"end\":61576,\"start\":61565},{\"end\":61590,\"start\":61582},{\"end\":61848,\"start\":61842},{\"end\":61859,\"start\":61854},{\"end\":61876,\"start\":61869},{\"end\":61883,\"start\":61881},{\"end\":61897,\"start\":61895},{\"end\":61918,\"start\":61906},{\"end\":62085,\"start\":62079},{\"end\":62099,\"start\":62095},{\"end\":62110,\"start\":62106},{\"end\":62126,\"start\":62120},{\"end\":62136,\"start\":62128},{\"end\":62436,\"start\":62429},{\"end\":62454,\"start\":62442},{\"end\":62470,\"start\":62464},{\"end\":62712,\"start\":62707},{\"end\":62724,\"start\":62719},{\"end\":62739,\"start\":62732},{\"end\":62757,\"start\":62748},{\"end\":62774,\"start\":62766},{\"end\":62782,\"start\":62780},{\"end\":63064,\"start\":63057},{\"end\":63075,\"start\":63071},{\"end\":63093,\"start\":63085},{\"end\":63109,\"start\":63103},{\"end\":63337,\"start\":63331},{\"end\":63354,\"start\":63349},{\"end\":63360,\"start\":63356},{\"end\":63529,\"start\":63524},{\"end\":63545,\"start\":63538},{\"end\":63556,\"start\":63554},{\"end\":63574,\"start\":63566},{\"end\":63783,\"start\":63775},{\"end\":63797,\"start\":63792},{\"end\":63810,\"start\":63805},{\"end\":63828,\"start\":63821},{\"end\":63846,\"start\":63837},{\"end\":63865,\"start\":63856},{\"end\":64176,\"start\":64168},{\"end\":64195,\"start\":64186},{\"end\":64213,\"start\":64204},{\"end\":64526,\"start\":64521},{\"end\":64537,\"start\":64532},{\"end\":64550,\"start\":64544},{\"end\":64847,\"start\":64841},{\"end\":64866,\"start\":64856},{\"end\":64880,\"start\":64876},{\"end\":64895,\"start\":64890},{\"end\":64907,\"start\":64901},{\"end\":64930,\"start\":64920},{\"end\":64938,\"start\":64932},{\"end\":65227,\"start\":65221},{\"end\":65247,\"start\":65237},{\"end\":65266,\"start\":65260},{\"end\":65279,\"start\":65273},{\"end\":65296,\"start\":65285},{\"end\":65305,\"start\":65303},{\"end\":65314,\"start\":65307},{\"end\":65587,\"start\":65579},{\"end\":65603,\"start\":65596},{\"end\":65828,\"start\":65820},{\"end\":65843,\"start\":65835},{\"end\":65856,\"start\":65853},{\"end\":65871,\"start\":65866},{\"end\":65885,\"start\":65881},{\"end\":65907,\"start\":65893},{\"end\":65922,\"start\":65914},{\"end\":65938,\"start\":65931},{\"end\":65952,\"start\":65946},{\"end\":66296,\"start\":66290},{\"end\":66312,\"start\":66302},{\"end\":66327,\"start\":66323},{\"end\":66341,\"start\":66334},{\"end\":66362,\"start\":66352},{\"end\":66381,\"start\":66375},{\"end\":66389,\"start\":66387},{\"end\":66725,\"start\":66714},{\"end\":66745,\"start\":66734},{\"end\":66758,\"start\":66754},{\"end\":67054,\"start\":67044},{\"end\":67063,\"start\":67059},{\"end\":67330,\"start\":67326},{\"end\":67346,\"start\":67342},{\"end\":67713,\"start\":67705},{\"end\":67727,\"start\":67723},{\"end\":67743,\"start\":67738},{\"end\":67759,\"start\":67754},{\"end\":68045,\"start\":68037},{\"end\":68059,\"start\":68052},{\"end\":68076,\"start\":68069},{\"end\":68341,\"start\":68337},{\"end\":68351,\"start\":68348},{\"end\":68364,\"start\":68361},{\"end\":68377,\"start\":68374},{\"end\":68387,\"start\":68383},{\"end\":68691,\"start\":68687},{\"end\":68706,\"start\":68702},{\"end\":68719,\"start\":68713},{\"end\":68738,\"start\":68728},{\"end\":68751,\"start\":68747},{\"end\":68770,\"start\":68764},{\"end\":68794,\"start\":68780},{\"end\":68808,\"start\":68801},{\"end\":68827,\"start\":68817},{\"end\":69141,\"start\":69130},{\"end\":69163,\"start\":69152},{\"end\":69185,\"start\":69174},{\"end\":69208,\"start\":69196},{\"end\":69427,\"start\":69421},{\"end\":69433,\"start\":69429},{\"end\":69453,\"start\":69439},{\"end\":69469,\"start\":69461},{\"end\":69482,\"start\":69475},{\"end\":69491,\"start\":69484},{\"end\":69500,\"start\":69495},{\"end\":69516,\"start\":69509},{\"end\":69526,\"start\":69518},{\"end\":69826,\"start\":69824},{\"end\":69839,\"start\":69835},{\"end\":69853,\"start\":69847},{\"end\":69867,\"start\":69863},{\"end\":69883,\"start\":69876},{\"end\":70494,\"start\":70489},{\"end\":70507,\"start\":70501},{\"end\":70519,\"start\":70514},{\"end\":70533,\"start\":70528},{\"end\":70547,\"start\":70541},{\"end\":70560,\"start\":70555},{\"end\":70574,\"start\":70568},{\"end\":70795,\"start\":70793},{\"end\":70806,\"start\":70804},{\"end\":70822,\"start\":70816},{\"end\":70839,\"start\":70831},{\"end\":71108,\"start\":71104},{\"end\":71124,\"start\":71118},{\"end\":71136,\"start\":71131},{\"end\":71150,\"start\":71145},{\"end\":71164,\"start\":71157}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":51525,\"start\":51150},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2891906},\"end\":51831,\"start\":51527},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":215780580},\"end\":52110,\"start\":51833},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":54457478},\"end\":52297,\"start\":52112},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2609415},\"end\":52645,\"start\":52299},{\"attributes\":{\"id\":\"b5\"},\"end\":52873,\"start\":52647},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9719717},\"end\":53100,\"start\":52875},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":123166853},\"end\":53410,\"start\":53102},{\"attributes\":{\"id\":\"b8\"},\"end\":53708,\"start\":53412},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":211259068},\"end\":53968,\"start\":53710},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10310432},\"end\":54245,\"start\":53970},{\"attributes\":{\"id\":\"b11\"},\"end\":54409,\"start\":54247},{\"attributes\":{\"id\":\"b12\"},\"end\":54727,\"start\":54411},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":19285959},\"end\":54921,\"start\":54729},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6628106},\"end\":55102,\"start\":54923},{\"attributes\":{\"id\":\"b15\"},\"end\":55452,\"start\":55104},{\"attributes\":{\"id\":\"b16\"},\"end\":55662,\"start\":55454},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1538200},\"end\":55854,\"start\":55664},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1363510},\"end\":56000,\"start\":55856},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220831416},\"end\":56319,\"start\":56002},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":147704018},\"end\":56773,\"start\":56321},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":227255123},\"end\":57047,\"start\":56775},{\"attributes\":{\"id\":\"b22\"},\"end\":57309,\"start\":57049},{\"attributes\":{\"doi\":\"65:1- 65:14\",\"id\":\"b23\"},\"end\":57763,\"start\":57311},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":232092799},\"end\":58127,\"start\":57765},{\"attributes\":{\"id\":\"b25\"},\"end\":58216,\"start\":58129},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":220968781},\"end\":58638,\"start\":58218},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":54465161},\"end\":58972,\"start\":58640},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":219947110},\"end\":59501,\"start\":58974},{\"attributes\":{\"id\":\"b29\"},\"end\":59880,\"start\":59503},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":209376368},\"end\":60244,\"start\":59882},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":58007025},\"end\":60604,\"start\":60246},{\"attributes\":{\"id\":\"b32\"},\"end\":60941,\"start\":60606},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":212646575},\"end\":61201,\"start\":60943},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":22232987},\"end\":61433,\"start\":61203},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":378932},\"end\":61799,\"start\":61435},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":227162149},\"end\":62067,\"start\":61801},{\"attributes\":{\"id\":\"b37\"},\"end\":62358,\"start\":62069},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206596552},\"end\":62610,\"start\":62360},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":152282359},\"end\":62986,\"start\":62612},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":220364071},\"end\":63272,\"start\":62988},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":687379},\"end\":63491,\"start\":63274},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1240104},\"end\":63709,\"start\":63493},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":54444417},\"end\":64067,\"start\":63711},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":174798113},\"end\":64406,\"start\":64069},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":324277},\"end\":64755,\"start\":64408},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":227348246},\"end\":65148,\"start\":64757},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":109926755},\"end\":65522,\"start\":65150},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14832764},\"end\":65732,\"start\":65524},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":231709798},\"end\":66200,\"start\":65734},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":227254925},\"end\":66608,\"start\":66202},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":60945},\"end\":66955,\"start\":66610},{\"attributes\":{\"id\":\"b52\"},\"end\":67197,\"start\":66957},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":5696152},\"end\":67605,\"start\":67199},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":73431591},\"end\":67962,\"start\":67607},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":6085476},\"end\":68251,\"start\":67964},{\"attributes\":{\"id\":\"b56\"},\"end\":68625,\"start\":68253},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":232045969},\"end\":69060,\"start\":68627},{\"attributes\":{\"id\":\"b58\"},\"end\":69376,\"start\":69062},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":14109696},\"end\":69729,\"start\":69378},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":166228177},\"end\":70400,\"start\":69731},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":225077557},\"end\":70786,\"start\":70402},{\"attributes\":{\"id\":\"b62\"},\"end\":71023,\"start\":70788},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":219893035},\"end\":71343,\"start\":71025}]", "bib_title": "[{\"end\":51570,\"start\":51527},{\"end\":51865,\"start\":51833},{\"end\":52166,\"start\":52112},{\"end\":52395,\"start\":52299},{\"end\":52891,\"start\":52875},{\"end\":53124,\"start\":53102},{\"end\":53763,\"start\":53710},{\"end\":54030,\"start\":53970},{\"end\":54765,\"start\":54729},{\"end\":54965,\"start\":54923},{\"end\":55168,\"start\":55104},{\"end\":55698,\"start\":55664},{\"end\":55877,\"start\":55856},{\"end\":56052,\"start\":56002},{\"end\":56430,\"start\":56321},{\"end\":56838,\"start\":56775},{\"end\":57374,\"start\":57311},{\"end\":57828,\"start\":57765},{\"end\":58336,\"start\":58218},{\"end\":58704,\"start\":58640},{\"end\":59062,\"start\":58974},{\"end\":59978,\"start\":59882},{\"end\":60325,\"start\":60246},{\"end\":60975,\"start\":60943},{\"end\":61244,\"start\":61203},{\"end\":61558,\"start\":61435},{\"end\":61833,\"start\":61801},{\"end\":62420,\"start\":62360},{\"end\":62696,\"start\":62612},{\"end\":63049,\"start\":62988},{\"end\":63327,\"start\":63274},{\"end\":63513,\"start\":63493},{\"end\":63765,\"start\":63711},{\"end\":64158,\"start\":64069},{\"end\":64508,\"start\":64408},{\"end\":64837,\"start\":64757},{\"end\":65217,\"start\":65150},{\"end\":65569,\"start\":65524},{\"end\":65811,\"start\":65734},{\"end\":66280,\"start\":66202},{\"end\":66706,\"start\":66610},{\"end\":67316,\"start\":67199},{\"end\":67695,\"start\":67607},{\"end\":68027,\"start\":67964},{\"end\":68324,\"start\":68253},{\"end\":68676,\"start\":68627},{\"end\":69417,\"start\":69378},{\"end\":69813,\"start\":69731},{\"end\":70482,\"start\":70402},{\"end\":71094,\"start\":71025}]", "bib_author": "[{\"end\":51233,\"start\":51207},{\"end\":51241,\"start\":51233},{\"end\":51585,\"start\":51572},{\"end\":51594,\"start\":51585},{\"end\":51602,\"start\":51594},{\"end\":51882,\"start\":51867},{\"end\":51897,\"start\":51882},{\"end\":51915,\"start\":51897},{\"end\":51931,\"start\":51915},{\"end\":51946,\"start\":51931},{\"end\":52181,\"start\":52168},{\"end\":52192,\"start\":52181},{\"end\":52413,\"start\":52397},{\"end\":52424,\"start\":52413},{\"end\":52441,\"start\":52424},{\"end\":52448,\"start\":52441},{\"end\":52710,\"start\":52697},{\"end\":52729,\"start\":52710},{\"end\":52750,\"start\":52729},{\"end\":52916,\"start\":52893},{\"end\":52931,\"start\":52916},{\"end\":52941,\"start\":52931},{\"end\":53141,\"start\":53126},{\"end\":53149,\"start\":53141},{\"end\":53430,\"start\":53412},{\"end\":53446,\"start\":53430},{\"end\":53463,\"start\":53446},{\"end\":53478,\"start\":53463},{\"end\":53495,\"start\":53478},{\"end\":53777,\"start\":53765},{\"end\":53789,\"start\":53777},{\"end\":53799,\"start\":53789},{\"end\":53813,\"start\":53799},{\"end\":53827,\"start\":53813},{\"end\":54048,\"start\":54032},{\"end\":54066,\"start\":54048},{\"end\":54082,\"start\":54066},{\"end\":54323,\"start\":54308},{\"end\":54485,\"start\":54471},{\"end\":54495,\"start\":54485},{\"end\":54511,\"start\":54495},{\"end\":54534,\"start\":54511},{\"end\":54547,\"start\":54534},{\"end\":54556,\"start\":54547},{\"end\":54781,\"start\":54767},{\"end\":54797,\"start\":54781},{\"end\":54813,\"start\":54797},{\"end\":54979,\"start\":54967},{\"end\":54993,\"start\":54979},{\"end\":54997,\"start\":54993},{\"end\":55186,\"start\":55170},{\"end\":55199,\"start\":55186},{\"end\":55213,\"start\":55199},{\"end\":55229,\"start\":55213},{\"end\":55549,\"start\":55536},{\"end\":55712,\"start\":55700},{\"end\":55723,\"start\":55712},{\"end\":55733,\"start\":55723},{\"end\":55740,\"start\":55733},{\"end\":55891,\"start\":55879},{\"end\":55905,\"start\":55891},{\"end\":56066,\"start\":56054},{\"end\":56079,\"start\":56066},{\"end\":56095,\"start\":56079},{\"end\":56107,\"start\":56095},{\"end\":56123,\"start\":56107},{\"end\":56131,\"start\":56123},{\"end\":56445,\"start\":56432},{\"end\":56463,\"start\":56445},{\"end\":56481,\"start\":56463},{\"end\":56500,\"start\":56481},{\"end\":56521,\"start\":56500},{\"end\":56849,\"start\":56840},{\"end\":56858,\"start\":56849},{\"end\":56870,\"start\":56858},{\"end\":56885,\"start\":56870},{\"end\":56896,\"start\":56885},{\"end\":57062,\"start\":57049},{\"end\":57073,\"start\":57062},{\"end\":57087,\"start\":57073},{\"end\":57394,\"start\":57376},{\"end\":57407,\"start\":57394},{\"end\":57422,\"start\":57407},{\"end\":57440,\"start\":57422},{\"end\":57458,\"start\":57440},{\"end\":57472,\"start\":57458},{\"end\":57848,\"start\":57830},{\"end\":57861,\"start\":57848},{\"end\":57879,\"start\":57861},{\"end\":57899,\"start\":57879},{\"end\":57913,\"start\":57899},{\"end\":57928,\"start\":57913},{\"end\":58154,\"start\":58131},{\"end\":58161,\"start\":58154},{\"end\":58167,\"start\":58161},{\"end\":58362,\"start\":58338},{\"end\":58375,\"start\":58362},{\"end\":58386,\"start\":58375},{\"end\":58406,\"start\":58386},{\"end\":58414,\"start\":58406},{\"end\":58722,\"start\":58706},{\"end\":58739,\"start\":58722},{\"end\":58757,\"start\":58739},{\"end\":58776,\"start\":58757},{\"end\":58792,\"start\":58776},{\"end\":59080,\"start\":59064},{\"end\":59090,\"start\":59080},{\"end\":59110,\"start\":59090},{\"end\":59136,\"start\":59110},{\"end\":59152,\"start\":59136},{\"end\":59169,\"start\":59152},{\"end\":59182,\"start\":59169},{\"end\":59187,\"start\":59182},{\"end\":59519,\"start\":59503},{\"end\":59529,\"start\":59519},{\"end\":59549,\"start\":59529},{\"end\":59568,\"start\":59549},{\"end\":59581,\"start\":59568},{\"end\":59598,\"start\":59581},{\"end\":59602,\"start\":59598},{\"end\":59608,\"start\":59602},{\"end\":59998,\"start\":59980},{\"end\":60014,\"start\":59998},{\"end\":60031,\"start\":60014},{\"end\":60047,\"start\":60031},{\"end\":60344,\"start\":60327},{\"end\":60360,\"start\":60344},{\"end\":60375,\"start\":60360},{\"end\":60393,\"start\":60375},{\"end\":60411,\"start\":60393},{\"end\":60621,\"start\":60606},{\"end\":60636,\"start\":60621},{\"end\":60655,\"start\":60636},{\"end\":60671,\"start\":60655},{\"end\":60686,\"start\":60671},{\"end\":60696,\"start\":60686},{\"end\":60718,\"start\":60696},{\"end\":60727,\"start\":60718},{\"end\":60991,\"start\":60977},{\"end\":61009,\"start\":60991},{\"end\":61025,\"start\":61009},{\"end\":61041,\"start\":61025},{\"end\":61057,\"start\":61041},{\"end\":61259,\"start\":61246},{\"end\":61269,\"start\":61259},{\"end\":61578,\"start\":61560},{\"end\":61592,\"start\":61578},{\"end\":61850,\"start\":61835},{\"end\":61861,\"start\":61850},{\"end\":61878,\"start\":61861},{\"end\":61885,\"start\":61878},{\"end\":61899,\"start\":61885},{\"end\":61920,\"start\":61899},{\"end\":62087,\"start\":62069},{\"end\":62101,\"start\":62087},{\"end\":62112,\"start\":62101},{\"end\":62128,\"start\":62112},{\"end\":62138,\"start\":62128},{\"end\":62438,\"start\":62422},{\"end\":62456,\"start\":62438},{\"end\":62472,\"start\":62456},{\"end\":62714,\"start\":62698},{\"end\":62726,\"start\":62714},{\"end\":62741,\"start\":62726},{\"end\":62759,\"start\":62741},{\"end\":62776,\"start\":62759},{\"end\":62784,\"start\":62776},{\"end\":63066,\"start\":63051},{\"end\":63077,\"start\":63066},{\"end\":63095,\"start\":63077},{\"end\":63111,\"start\":63095},{\"end\":63339,\"start\":63329},{\"end\":63356,\"start\":63339},{\"end\":63362,\"start\":63356},{\"end\":63531,\"start\":63515},{\"end\":63547,\"start\":63531},{\"end\":63558,\"start\":63547},{\"end\":63576,\"start\":63558},{\"end\":63785,\"start\":63767},{\"end\":63799,\"start\":63785},{\"end\":63812,\"start\":63799},{\"end\":63830,\"start\":63812},{\"end\":63848,\"start\":63830},{\"end\":63867,\"start\":63848},{\"end\":64178,\"start\":64160},{\"end\":64197,\"start\":64178},{\"end\":64215,\"start\":64197},{\"end\":64528,\"start\":64510},{\"end\":64539,\"start\":64528},{\"end\":64552,\"start\":64539},{\"end\":64849,\"start\":64839},{\"end\":64868,\"start\":64849},{\"end\":64882,\"start\":64868},{\"end\":64897,\"start\":64882},{\"end\":64909,\"start\":64897},{\"end\":64932,\"start\":64909},{\"end\":64940,\"start\":64932},{\"end\":65229,\"start\":65219},{\"end\":65249,\"start\":65229},{\"end\":65268,\"start\":65249},{\"end\":65281,\"start\":65268},{\"end\":65298,\"start\":65281},{\"end\":65307,\"start\":65298},{\"end\":65316,\"start\":65307},{\"end\":65589,\"start\":65571},{\"end\":65605,\"start\":65589},{\"end\":65830,\"start\":65813},{\"end\":65845,\"start\":65830},{\"end\":65858,\"start\":65845},{\"end\":65873,\"start\":65858},{\"end\":65887,\"start\":65873},{\"end\":65909,\"start\":65887},{\"end\":65924,\"start\":65909},{\"end\":65940,\"start\":65924},{\"end\":65954,\"start\":65940},{\"end\":66298,\"start\":66282},{\"end\":66314,\"start\":66298},{\"end\":66329,\"start\":66314},{\"end\":66343,\"start\":66329},{\"end\":66364,\"start\":66343},{\"end\":66383,\"start\":66364},{\"end\":66391,\"start\":66383},{\"end\":66727,\"start\":66708},{\"end\":66747,\"start\":66727},{\"end\":66760,\"start\":66747},{\"end\":67056,\"start\":67039},{\"end\":67065,\"start\":67056},{\"end\":67332,\"start\":67318},{\"end\":67348,\"start\":67332},{\"end\":67715,\"start\":67697},{\"end\":67729,\"start\":67715},{\"end\":67745,\"start\":67729},{\"end\":67761,\"start\":67745},{\"end\":68047,\"start\":68029},{\"end\":68061,\"start\":68047},{\"end\":68078,\"start\":68061},{\"end\":68343,\"start\":68326},{\"end\":68353,\"start\":68343},{\"end\":68366,\"start\":68353},{\"end\":68379,\"start\":68366},{\"end\":68389,\"start\":68379},{\"end\":68693,\"start\":68678},{\"end\":68708,\"start\":68693},{\"end\":68721,\"start\":68708},{\"end\":68740,\"start\":68721},{\"end\":68753,\"start\":68740},{\"end\":68772,\"start\":68753},{\"end\":68796,\"start\":68772},{\"end\":68810,\"start\":68796},{\"end\":68829,\"start\":68810},{\"end\":69143,\"start\":69121},{\"end\":69165,\"start\":69143},{\"end\":69187,\"start\":69165},{\"end\":69210,\"start\":69187},{\"end\":69429,\"start\":69419},{\"end\":69435,\"start\":69429},{\"end\":69455,\"start\":69435},{\"end\":69471,\"start\":69455},{\"end\":69484,\"start\":69471},{\"end\":69493,\"start\":69484},{\"end\":69502,\"start\":69493},{\"end\":69518,\"start\":69502},{\"end\":69528,\"start\":69518},{\"end\":69828,\"start\":69815},{\"end\":69841,\"start\":69828},{\"end\":69855,\"start\":69841},{\"end\":69869,\"start\":69855},{\"end\":69885,\"start\":69869},{\"end\":70496,\"start\":70484},{\"end\":70509,\"start\":70496},{\"end\":70521,\"start\":70509},{\"end\":70535,\"start\":70521},{\"end\":70549,\"start\":70535},{\"end\":70562,\"start\":70549},{\"end\":70576,\"start\":70562},{\"end\":70797,\"start\":70788},{\"end\":70808,\"start\":70797},{\"end\":70824,\"start\":70808},{\"end\":70841,\"start\":70824},{\"end\":71110,\"start\":71096},{\"end\":71126,\"start\":71110},{\"end\":71138,\"start\":71126},{\"end\":71152,\"start\":71138},{\"end\":71166,\"start\":71152}]", "bib_venue": "[{\"end\":51205,\"start\":51150},{\"end\":51664,\"start\":51602},{\"end\":51954,\"start\":51946},{\"end\":52196,\"start\":52192},{\"end\":52456,\"start\":52448},{\"end\":52695,\"start\":52647},{\"end\":52971,\"start\":52941},{\"end\":53237,\"start\":53149},{\"end\":53552,\"start\":53495},{\"end\":53831,\"start\":53827},{\"end\":54085,\"start\":54082},{\"end\":54306,\"start\":54247},{\"end\":54469,\"start\":54411},{\"end\":54817,\"start\":54813},{\"end\":55001,\"start\":54997},{\"end\":55263,\"start\":55229},{\"end\":55534,\"start\":55454},{\"end\":55744,\"start\":55740},{\"end\":55913,\"start\":55905},{\"end\":56135,\"start\":56131},{\"end\":56525,\"start\":56521},{\"end\":56900,\"start\":56896},{\"end\":57173,\"start\":57087},{\"end\":57517,\"start\":57483},{\"end\":57936,\"start\":57928},{\"end\":58418,\"start\":58414},{\"end\":58796,\"start\":58792},{\"end\":59221,\"start\":59187},{\"end\":59678,\"start\":59608},{\"end\":60051,\"start\":60047},{\"end\":60415,\"start\":60411},{\"end\":60760,\"start\":60727},{\"end\":61061,\"start\":61057},{\"end\":61303,\"start\":61269},{\"end\":61598,\"start\":61592},{\"end\":61924,\"start\":61920},{\"end\":62200,\"start\":62138},{\"end\":62476,\"start\":62472},{\"end\":62788,\"start\":62784},{\"end\":63118,\"start\":63111},{\"end\":63366,\"start\":63362},{\"end\":63584,\"start\":63576},{\"end\":63871,\"start\":63867},{\"end\":64222,\"start\":64215},{\"end\":64560,\"start\":64552},{\"end\":64944,\"start\":64940},{\"end\":65320,\"start\":65316},{\"end\":65609,\"start\":65605},{\"end\":65958,\"start\":65954},{\"end\":66395,\"start\":66391},{\"end\":66764,\"start\":66760},{\"end\":67037,\"start\":66957},{\"end\":67382,\"start\":67348},{\"end\":67765,\"start\":67761},{\"end\":68082,\"start\":68078},{\"end\":68423,\"start\":68389},{\"end\":68833,\"start\":68829},{\"end\":69119,\"start\":69062},{\"end\":69536,\"start\":69528},{\"end\":69934,\"start\":69885},{\"end\":70583,\"start\":70576},{\"end\":70886,\"start\":70841},{\"end\":71174,\"start\":71166}]"}}}, "year": 2023, "month": 12, "day": 17}