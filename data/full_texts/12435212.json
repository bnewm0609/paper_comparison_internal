{"id": 12435212, "updated": "2023-07-19 07:45:02.205", "metadata": {"title": "Knowledge-based graph document modeling", "authors": "[{\"first\":\"Michael\",\"last\":\"Schuhmacher\",\"middle\":[]},{\"first\":\"Simone\",\"last\":\"Ponzetto\",\"middle\":[\"Paolo\"]}]", "venue": null, "journal": "Proceedings of the 7th ACM international conference on Web search and data mining", "publication_date": {"year": 2014, "month": null, "day": null}, "abstract": "We propose a graph-based semantic model for representing document content. Our method relies on the use of a semantic network, namely the DBpedia knowledge base, for acquiring fine-grained information about entities and their semantic relations, thus resulting in a knowledge-rich document model. We demonstrate the benefits of these semantic representations in two tasks: entity ranking and computing document semantic similarity. To this end, we couple DBpedia's structure with an information-theoretic measure of concept association, based on its explicit semantic relations, and compute semantic similarity using a Graph Edit Distance based measure, which finds the optimal matching between the documents' entities using the Hungarian method. Experimental results show that our general model outperforms baselines built on top of traditional methods, and achieves a performance close to that of highly specialized methods that have been tuned to these specific tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2145769341", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wsdm/SchuhmacherP14", "doi": "10.1145/2556195.2556250"}}, "content": {"source": {"pdf_hash": "1844cb8b89df24dd08b582b374355ccd43358220", "pdf_src": "Anansi", "pdf_uri": "[\"https://ub-madoc.bib.uni-mannheim.de/35464/1/schuhmacher14a.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://madoc.bib.uni-mannheim.de/35464/1/schuhmacher14a.pdf", "status": "GREEN"}}, "grobid": {"id": "59251f67b510e1986bdae71d1f8d4b04d4e6795f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1844cb8b89df24dd08b582b374355ccd43358220.txt", "contents": "\nKnowledge-based Graph Document Modeling\n\n\nMichael Schuhmacher \nData and Web Science Research Group\nData and Web Science Research Group\nUniversity of Mannheim\nGermany\n\nSimone Paolo Ponzetto \nUniversity of Mannheim\nGermany\n\nKnowledge-based Graph Document Modeling\n10.1145/2556195.2556250H31 [Information Storage and Retrieval]: Content Analysis and IndexingI24 [Artificial Intelligence]: Se- mantic NetworksI27 [Artificial Intelligence]: Natural Language Processing Keywords Document modelingSemantic network miningDBpediaEntity relatednessDocument semantic similarity\nWe propose a graph-based semantic model for representing document content. Our method relies on the use of a semantic network, namely the DBpedia knowledge base, for acquiring fine-grained information about entities and their semantic relations, thus resulting in a knowledge-rich document model. We demonstrate the benefits of these semantic representations in two tasks: entity ranking and computing document semantic similarity. To this end, we couple DBpedia's structure with an information-theoretic measure of concept association, based on its explicit semantic relations, and compute semantic similarity using a Graph Edit Distance based measure, which finds the optimal matching between the documents' entities using the Hungarian method. Experimental results show that our general model outperforms baselines built on top of traditional methods, and achieves a performance close to that of highly specialized methods that have been tuned to these specific tasks.\n\nINTRODUCTION\n\nRecent years have seen a great deal of work on developing wide-coverage semantic technologies and methods embedding semantic models within a wide spectrum of applications, crucially including end-user applications like, for instance, question answering [16,47], document search [14] and web-search results clustering [34]. Complementary to this trend, many research efforts have concentrated on the Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. automatic acquisition of machine-readable knowledge on a large scale by mining large repositories of textual data such as the Web [2,9] (inter alia), and exploiting collaborativelyconstructed resources [40,36,21,25]. As a result of this, recent years have seen a remarkable renaissance of knowledgerich approaches for many different Natural Language Processing (NLP) and Information Retrieval (IR) tasks [27].\n\nBut while recent research trends indicate that semantic information and knowledge-rich approaches can be used effectively for high-end IR and NLP tasks, much still remains to be done in order to effectively exploit these rich models and further advance the state of the art in these fields. Most of the approaches which draw upon document representations, in fact, rely solely on morpho-syntactic information by means of 'flat' meaning representations like vector space models [44]. Although more sophisticated models have been proposed -including conceptual [17] and grounded [8] vector spaces -these still do not exploit the relational knowledge and network structure encoded within wide-coverage knowledge bases such as YAGO [25] or DBpedia [6].\n\nIn this paper, we aim at overcoming these issues by means of a knowledge-rich method to represent documents in the Web of Linked Data. Key to our approach is the combination of a fine-grained relation vocabulary with informationtheoretic measures of concept associativity to produce a graph-based interpretation of texts leveraging large amounts of structured knowledge, i.e., disambiguated entities and explicit semantic relations, encoded within DBpedia. Our contributions are as follows:\n\n\u2022 We propose a graph-based document model and present a method to produce structured representations of texts that combine disambiguated entities with fine-grained semantic relations;\n\n\u2022 We present a variety of information-theoretic measures to weight different semantic relations within an ontology, and automatically quantify their degree of relevance with respect to the concepts they connect. Edges in the semantic graphs are thus weighted so as to capture the degree of associativity between concepts, as well as their different levels of specificity;\n\n\u2022 We evaluate our model using two highly relevant tasks, namely entity ranking and computing document similarity. We show that our approach not only outperforms standard baselines relying on traditional, i.e., 'flat', document representations, but also produces results close to those of highly specialized methods that have been particularly tuned to the respective tasks.\n\n\u2022 We develop a new measure, based on graph edit distance techniques, in order to compute document similarity using our semantic graphs. Our approach views computing semantic distances within an ontology as a concept matching problem, and uses the Hungarian method for solving this combinatorial optimization problem.\n\nAs a result of this, we are able to provide a complete framework where documents are semantified by linking them to a reference knowledge base, and subgraphs of the knowledge resource are used for complex language understanding tasks like entity ranking and document semantic similarity. Results on entity ranking show that our weighting scheme helps us better estimate entity relatedness when compared with using simple, unweighted paths. Moreover, by complementing large amounts of knowledge with structured text representations we are able to achieve a robust performance on the task of computing document semantic similarity, thus competing with 'flat' approaches based on either word or conceptual vector spaces, while at the same time providing a general, de-facto parameter-free model.\n\n\nMODELING DOCUMENT CONTENT WITH ONTOLOGY-BASED GRAPHS\n\nWe present our method to generate structured representations of textual content using DBpedia as the backend ontology. To this end, we opt for graph-based methods since: (i) they are general in nature, and can be used with any knowledge graph, i.e., a knowledge resource that can be viewed as a graph, regardless of its specific vocabulary; (ii) they have been shown to be effective for language understanding tasks when combined with labeled and unlabeled resources (cf., e.g., [36,26]). In the following, we first briefly introduce DBpedia, the resource used in our methodology, in Section 2.1. Sections 2.2 through 2.3 illustrate instead the main phases of our approach.\n\n\nDBpedia\n\nOur approach relies on the information and structure encoded within an underlying knowledge base. In this work, we opt for DBpedia [6], since it provides a wide-coverage knowledge base with many (i.e., more than 1000) fine-grained explicit semantic relations between entities. However, our method can be also used with any other lexical or ontological resource, e.g. YAGO [25], provided it can be cast as a knowledge graph containing disambiguated entities and explicit semantic relations.\n\nDBpedia is a community effort to extract structured information from Wikipedia and make this information available on the Web as a full-fledged ontology. The key idea behind DBpedia is to parse infoboxes, namely property-summarizing tables found within Wikipedia pages, in order to automatically acquire properties and relations about a large amount of entities. These are further embedded within an ontology based on Semantic Web formalisms like: i) representing data on the basis of the best practices of linked data [5]; ii) encoding semantic relations using the resource description framework (RDF), a generic graph-based data model for describing objects and their relationships. In the following, we refer to RDF triples in DBpedia consisting of a subject, predicate and object as a directed relation from Subj to Obj , connected by the labeled relation Pred . Naturally, we can view DBpedia as a graph, whose nodes are entities and edges capture explicit semantic relations between them. For instance, an entity like \"Bob Dylan\" is connected in DBpedia to other entities by means of statements such as db:Bob_Dylan rdf:type dbo:MusicalArtist or db:Bob_Dylan dbo:genre db:Folk_rock, and so on 1 . We now show how we can use this structure to produce graphbased representations from an input consisting of sets of entities such as those mentioned in natural language texts.\n\n\nSemantic graph construction\n\nLet C db be the full set of DBpedia's concepts and entities 2 and C an arbitrary subset of it, given as input -e.g., the set of entities mentioned within a document. In the first phase of our methodology, we create from the set of input entities a labeled, directed graph G = (V, E) containing i) the entities themselves, ii) their semantic relations, as well as iii) any additional entity that is related to any of the input ones by means of some relation in the ontology. That is, C \u2286 V \u2286 C db and E \u2286 V \u00d7R\u00d7V , where r \u2208 R is a semantic relation found in DBpedia, e.g., rdf:type, dbo:birthDate or dbp:genre (we do not make any distinction between Abox and T-box statements, since we remain agnostic as to the specific vocabulary used by our underlying resource). Additionally, we want to associate a weight w with each edge (vi, r, vj) \u2208 E, in order to capture the degree of associativity between the source and target nodes -i.e., how strongly related the two corresponding entities are.\n\nTo produce our semantic graphs, we start with a set of input entities C and create a labeled directed graph G = (V, E) as follows: a) first, we define the set of nodes V of G to be made up of all input concepts, that is, we set V := C; b) next, we connect the nodes in V based on the paths found between them in DBpedia. Nodes in V are expanded into a graph by performing a depth-first search along the DBpedia graph and successively adding all outgoing relations 3 r, thus adding all simple directed paths v, v1, . . . , v k , v of maximal length L that connect them to G, i.e., V := V \u222a{v1, . . . , v k }, E := E \u222a {(v, r1, v1), . . . , (v k , r k , v )}. As a result, we obtain a sub-graph of DBpedia containing the initial concepts, together with all edges and intermediate concepts found along all paths of maximal length L that connect them. In this work, we set L = 2 following a large body of evidence from previous related work [36,28]. Figure 1 illustrates an example of a semantic graph generated from the set of entities { db:Bob_Dylan, db:Monterey_ Country_Fairgrounds, db:Mozambique_(Song), db:Johnny_ Cash }, e.g. as found within the sentence \"Dylan played Mozambique at Monterey right before Cash\". Starting from these seed entities, we perform a depth-first search to add relevant intermediates concepts and relations to G (e.g., foaf:Person or db:Folk_music). As a result of this, we obtain a semantically-rich graph: additional nodes and edges provide us with a rich structured context, in which the initial concepts are now connected by a variety of entities and explicit semantic relations.  \n\n\nSemantic relation weighting\n\nSo far, our method simply connected a set of input entities by traversing the DBpedia graph -i.e., similar in spirit to graph-based approaches to Word Sense Disambiguation using lexical resources [36]. However, in contrast to lexical resources like WordNet, our backend ontology contains many different, fine-grained semantic relations: accordingly, not all relations are equally informative and questions arise about what kind of information to take into account when building the semantic graphs. For instance, in our example there exists multiple paths between the source nodes db:Bob_Dylan and db:Johnny_Cash, due to the typical high density of the DBpedia graph. Connecting paths, however, include highly informative relations (e.g., the two entities being linked directly via dbo:associatedBand), as well as generic ones (both being entities of rdf:type foaf:Person), which tend to apply to a very large amount of entities (i.e., all persons in DBpedia) and thus carry low discriminative power -e.g., in order to identify relations useful for computing semantic similarity (see Section 4). But while previous work [28] restricted the semantic relations used to build semantic graphs to a small, manually-selected set, we opt here instead for an automatic approach based on relation-specific edge weighting. This is because, while a manual approach ensures overall good quality, it does not scale and needs to be tuned for every knowledge base in turn. Consequently, we extend our semantic graphs by weighting their edges. Weights are meant to capture the degree of associativity between concepts in the graph -i.e., the degree of relevance of an edge (i.e., semantic relation) for the entities it connects.\n\nThe key idea underlying our weighting is to reward, for a given source node, those edges and target nodes that are most specific to it. At the core of our edge weighting lies the notion of Information Content (IC ):\nIC X P red (\u03c9 Pred ) = \u2212 log (P (\u03c9 Pred )) ,\nwhere P (\u03c9 P red ) is the probability that the random variable X P red describing the type of edge, i.e. a specific semantic relation, shows the outcome \u03c9 Pred . This measure makes the assumption that specificity is a good proxy for relevancecf., for instance the rdf:type vs. dbo:associatedBand predicates. We can compute these IC values for all types of predicates, as we have the full DBpedia graph available and can query for all potential realizations of the random variable X P red . In our example, an edge labeled with rdf:type will accordingly get an IC which is comparably lower than, say, one labeled with dbo:associatedBand.\n\nJoint Information Content (jointIC). While the Information Content of semantic relations provides us with a way to distinguish general vs. specific connections, it only covers the a-priori specificity of an edge, i.e., regardless of the entities it actually connects. However, as shown in Figure 1, the same type of edge, e.g. rdf:type, can lead to very general concepts with low discriminative power (foaf:Person), but also to very informative (because rare) ones, like dbo:MusicalArtist, which do, in fact, provide valuable information. We capture this by adding the conditional information content IC (\u03c9 Obj |\u03c9 Pred ) to our weighting scheme, which accounts for the concept the predicate is pointing to, given that the edge has already been observed. Formally, given an edge e = (Subj , Pred , Obj ) we compute the information content of the joint probability distribution, IC (\u03c9 Pred , \u03c9 Obj ), which we take as our weighting function:\nwjointIC (e) = IC (\u03c9 Pred ) + IC (\u03c9 Obj |\u03c9 Pred ) .\nIn our example, the rdf:type edge leading to dbo:Musical Artist accordingly receives a much higher weight than that pointing to the far more generic foaf:Person.\n\nCombined Information Content (combIC). Joint information content, although taking into account predicate and object specificity at the same time, can nevertheless penalize infrequent objects that occur with infrequent predicates -e.g., db:American_folk_music being overall very infrequent, but getting a high probability (and, hence, a low IC ) when occurring conditional on dbo:genre. We propose to mitigate this problem by computing the joint information content while making an independence assumption between the predicated and the object. The resulting weights are then computed as the sum of the Information Content of the predicate and the object:\nw combIC (e) = IC (\u03c9 Pred ) + IC (\u03c9 Obj ) .\nInformation Content and Pointwise Mutual Information (IC+PMI). An alternative way to compute the strength of association between the predicate and the object is by means of Pointwise Mutual Information (PMI):\nPMI (\u03c9 Pred , \u03c9 Obj ) = log P (\u03c9 Pred , \u03c9 Obj ) P (\u03c9 Pred ) P (\u03c9 Obj )\n.\n\nPMI measures the mutual dependence between the two variable outcomes \u03c9 Pred and \u03c9 Obj , and can thus be seen as a measure of how much deviation from independence there is between the two outcomes, i.e., the specific predicate and object found along a DBpedia graph edge. Our hunch here is to use PMI to find a middle ground between the assumption of full dependence (jointIC) or independence (combIC) between predicates and objects. We additionally combine PMI with the IC of the predicate, in order to bias our weights towards less frequent, and thus more informative, predicates:\nwIC+PMI (e) = IC (\u03c9 Pred ) + PMI (\u03c9 Pred , \u03c9 Obj ) .\n\nGRAPH-BASED ENTITY RANKING\n\nWe present an extrinsic evaluation of the different weighting schemes we developed in order to build weighted semantic graphs from DBpedia. This is because semantic relation weighting makes a contribution of its own, and it can be  used for a variety of tasks other than semantic graph construction -e.g., RDF triple ranking [23]. Consequently, we explore here a task-based evaluation on ranking related entities [24] in a knowledge base.\n\nTask description. Entity ranking [12] is the task of ordering a given set of entities on the basis of their relevance with respect to a specific reference entity. In our case, since we work with DBpedia as knowledge base, we take, e.g., db:Bob_Dylan as reference and try to compute, how strongly db:Johnny_Cash is related to it, in comparison to db:Folk_music or db:Mozambique_(Song), and so on. This ranking task has the advantage that it provides a focused, extrinsic evaluation of our different weighting methods: besides, there exists established gold standard datasets against which we can compare our approach. Entity ranking can be seen as similar in spirit to computing word relatedness [49], except that in our setting we are given as input unambiguous entity references, rather than potentially ambiguous words. Besides, entity ranking also plays a key role in Entity Linking (EL) [10,29], since EL relies on estimating the degree of relatedness between candidate entity references of different mentions in text. That is, within a global document-level EL approach, entity mentions can be jointly disambiguated by maximizing their degree of semantic overlap as obtained, for instance, from information stored within a knowledge base -cf. AIDA [26].\n\nEntity ranking method. We present an overview of our approach in Figure 2. Given a reference entity ref and a set of entities E Cand (both found in DBpedia), our method ranks these entities by performing three main steps:\n\n1) we build a semantic graph following the procedure of Section 2.2 using all candidate entities E Cand , as well as the reference instance as input concepts;\n\n2) we weight graph edges by edge cost, which is defined as\nc(e) = wmax \u2212 w (e) ,(1)\nwhere w(.) is any of the three weighting functions defined in Section 2.3, and wmax is the globally highest possible weight in the DBpedia graph for the selected weighting function;\n\n3) finally, we compute (weighted) semantic distances between entity pairs -i.e., the reference entity ref and each of the entities cand in E Cand in turn -as the minimum path cost between them in our weighted graph:  We then rank the entity pairs increasingly by semantic distance. Each single path cost between two entities is calculated as the sum of the edge costs along their undirected connecting path p:\ndistance(ref ,cp(ref , cand ) = e\u2208{(ref,r 1 ,v 1 ),...,(v k ,r k ,cand)} c(e) . (2)\nAs a result of our method, we are able to determine the degree of relatedness between two arbitrary instances within our background knowledge base as the inverse of their semantic distance. We briefly illustrate our method in Figure 3 with an example using db:Bob_Dylan and db:Mozambique_ (Song) as input entity pair. When looking at the entity db:Bob_Dylan (the musician), we note that it is not directly connected to his song, db:Mozambique_(Song). However, thanks to the fact that DBpedia encodes very specific facts -namely i) that Bob Dylan is the main artist of the album db:Desire_(Bob_Dylan_album), and ii) that db:Mozambique_(Song) is a song contained in that very same album -we are able to estimate a high degree of semantic relatedness between the two input entities. Note that our weighting scheme plays a crucial role in estimating the degree of semantic overlap. If we look, for instance, to another entity pair such as the one consisting of db:Bob_Dylan and db:United_States, we note that in DBpedia these entities are connected by a short, albeit rather uninformative (because unspecific), path consisting of a single intermediate entity (db:Duluth,_Minnesota). Our weighting captures this by assigning a low weight to edges denoting general semantic relations such as dbo:birthPlace and dbo:country. As a result of this, we are able to rank db:Mozambique_(Song) higher than db:United_States, although both are connected to the reference entity db:Bob_ Dylan by a path of equal length.\n\nExperimental setting and evaluation. We use the KO-RE entity ranking dataset from Hoffart et al. [24]. The dataset consists of 21 different reference entities from four different domains, namely IT companies, Hollywood celebrities, television series, video games, and Chuck Norris (a singleton dataset). For each ranking problem, Hoffart et al.  selected a set of 20 candidate entities, which were found to be related with different degrees to the reference entity. Relatedness assessments were obtained from human judges using a crowd-sourcing approach. As an example, the entity \"Apple Inc.\" (from the 'IT Companies' category) is paired with, among others, the following other entities:\n\nReference entity Apple Inc.\n\n\nRelated entity\n\nSteve Jobs (1), Steve Wozniak (2), . . .\n\n\n(rank)\n\nSilicon Valley (9), NeXT (10) . . . Ford Motor Company (20) Naturally, different entities have different degrees of relatedness with the concept of \"Apple\" as a company. \"Steve Jobs\", for instance, ranks highest, having been a key figure of the company. In the middle range, instead, we find related companies such as \"NeXT\", another company founded by Steve Jobs (rank 10). Finally, at the end of the ranking we find \"Ford Motor Company\", which is only marginally related to \"Apple\", being also an American company but from a completely different industry. We follow the original evaluation setting of Hoffart et al. and compute Spearman's rank correlation coefficient \u03c1 for each reference entity in turn. Overall results are then obtained by averaging over all reference entities in the dataset. We report our results in Table 1, where we compare our different weighting schemes from Section 2.3. As baseline we use an unweighted version of the DBpedia graph: this amounts to computing entity relatedness simply as a function of distance in the network. Looking at the overall performance of the three alternative weighting schemes for all 21 ranking tasks, we observe that combIC consistently outperforms the baseline and both jointIC and IC+PMI on three domains out of four. When looking at specific domains, jointIC does not always improve the baseline, as results for Chuck Norris and Hollywood celebrities are actually getting worse. Nevertheless, on average all 3 weighting methods improve the baseline, with combIC, which shows an average increase of 15.5% (statistically significant for each task at p \u2264 .001 level using a paired t-test), achieving the best results. When compared with the original results from Hoffart et al. [24], our method achieves a performance slightly lower than their original proposal (\u03c1 = 0.673), while at the same time outperforming all its approximations (\u03c1 = 0.621 and 0.425). Overall, we take these performance figures to indicate the high quality of weighted connecting paths between entities in DBpedia. Consequently, we now take this idea one step further and use these paths to provide a structured representation of unstructured data, i.e. natural language Desire, a key folk music album from the 70's, is mostly known for Mozambique.  texts. We then use these graph-based semantic representations to compute semantic similarity between documents.\n\n\nCOMPUTING DOCUMENT SIMILARITY\n\nWe present an application of our semantic graphs to the task of computing semantic similarity between texts. We provide an overview of our approach in Figure 4. Our method starts with the output of an entity disambiguator, which is used to identify a set of concepts from the input texts (1). Next, connecting paths between entities are collected, in order to identify the sub-graph of DBpedia covered by each document (2). Nodes in the semantic graph consist of concepts capturing the main topics of the documents: in addition, edges in the graph are weighted to identify the semantic relations that are most relevant for these concepts (3). Finally, we view computing semantic similarity as a matching problem between the concepts of different documents, and apply a Graph Edit Distance based similarity measure to identify the 'best' connecting paths between the documents' concepts (4). As a result, we are able to output the degree of similarity of the two input documents.\n\n\nDocument graph construction\n\nGiven an input text document, we first semantify it by identifying the set of concepts it contains. To this end, words and phrases are annotated with DBpedia concepts using a document entity linking system, e.g., DBpedia Spotlight [32]. Given a mention and its candidate entities, the entity linker finds its most likely meaning in context -e.g., like Spotlight, using a Vector Space Model (based on a bagof-words approach). Accordingly, given a document, we are able to obtain a set of disambiguated entities associated with its words and phrases. In the two example documents of Figure 5, we extract, for instance, key concepts like db:Bob_Dylan, db:Johnny_Cash and db:Desire_(Bob_ Dylan_album). We call these extracted concepts the source \nnodes V d s of a document graph G d = (V d , E d ), V d s \u2286 V d representing document d.\n\nGraph-based document similarity\n\nSince we represent documents as weighted DBpedia subgraphs (Section 2), we can naturally formulate computing document similarity as a graph matching problem. While there exist exact graph matching algorithms, e.g. based on graph isomorphism, we require our measure to be able to effectively quantify degrees of similarity. Consequently, we opt for an application of Graph Edit Distance techniques for our specific problem.\n\nGraph Edit Distance (GED, [18]) is a general, inexact graph matching method that defines the distance between two graphs in terms of the minimum cost of edit operations needed to transform one graph into the other. In general, a GED measure needs to define edit cost functions for insertion, deletion, and modification for both nodes and edges. However, given our specific problem setting, we drop some of these requirements and define only cost functions for nodes. This is because, given a pair of semantic graphs, generated using the method from Section 2, these actually consist of two subgraphs of the same background model, namely DBpedia. As a result, no cost function over edges needs to be defined, since an edge existing or not in one graph will also be present or not in the other, given the fact that both document graphs belong in fact to the same supergraph. Thus, edit operation on edges solely cannot occur and, accordingly, we define edge cost functions to yield zero.\n\nWe define cost operations for nodes as follows. Note that, since we work with a well-defined ontology that represents concepts by unique URIs, we can rely on the fact that nodes in the DBpedia graph are unique. As a result, we do not need to account for label mismatch between concepts -e.g., the entity \"Bob Dylan\" being identified by db:Robert_Allen_ Zimmerman in a graph, and referred to as db:Bob_Dylan in another one (or vice versa). Thus, in contrast to standard GED approaches, we define node modifications on the basis of the underlying edge structure, i.e., weighted distances in the graph, as opposed, for instance, to the application of string similarity measures like Levenshtein distance on node labels. The modification cost between two nodes is defined, analog to Section 3, as the sum of the edge costs along their connecting path (cf. Equation 2). By employing our edge cost function (Equation 1) we capture the fact that the closer (i.e., more semantically related) two nodes are, the lower the cost to modify one into the other is.\n\nAn exact solution to the GED problem can be found with a tree search over all possible edit operations, which, however, is computationally intractable for any reasonably-sized graph. In this work, we accordingly adapt an approximation method based on bipartite graph matching for finding the minimal edit cost [41]. This precomputes the cheapest node modification costs for each node pair first, and stores them into a cost matrix. Since in our case there can exist multiple paths between two nodes (and, thus, multiple such modification costs), we always select the cheapest node modification operation as the cheapest connecting path. Next, the matrix is extended with the cost for node insertion and deletionwhich we define as equal to the most expensive node modification operation in the matrix (see below for details). Computing the GED is now a bipartite graph matching problem between the source nodes of the two graphs, with the objective of minimizing the edit cost and subject to the restriction of a strict one-to-one matching (as every node can only be modified exactly once). We solve this minimization problem using the Hungarian method (also known as Kuhn-Munkres or Munkres' algorithm). After computing the GED, we apply a simple normalization step to eliminate the effect of different graph, i.e., document sizes.\n\nWe summarize our approach in Algorithm 1. Given two semantic graphs G i and G j , representing documents d i and d j (Section 2), we perform the following steps: i) lines 1-9: for each pair of source nodes V i s \u00d7 V j s we find the cheapest undirected path p i,j with cost c i,j using Di-Algorithm 1 Graph-based semantic similarity\nInput: Document DBpedia subgraphs G i = (V i , E i ), G j = (V j , E j ) Parameter: Maximal path length nmax 1: function SubgraphDistance(G i , G j ) 2: P \u2190 \u2205 set of cheapest paths 3: for all (v i , v j ) \u2208 V i s \u00d7 V j s from G i , G j do 4: if v i = v j then 5: c i,j \u2190 0 6: else 7: c i,j \u2190 DijkstraCheapestPath(v i , v j ) 8: P \u2190 P \u222a { p i,j , c i,j } 9:\nc max \u2190 maxp\u2208P length\u2264nmax (cp) 10:\n\nfor all (p i,j , c i,j ) \u2208 P do 11:\n\nif p i,j length \u2264 nmax then 12: \nc i,j \u2190 c i,dist(G i , G j ) \u2190 ( m\u2208M mcost )/|V i s \u222a V j s | return dist(G i , G j )\njkstra's algorithm (edges along the path are weighted by one of our three measures from Section 2.3). In our example in Figure 5, for instance, we compute the cheapest path between db:Bob_Dylan and db:Johnny_Cash from Doc A, and each of db:Desire_(Bob_Dylan_album) and db:Folk_music from Doc B in turn. The highest weighted edge, here dbo:artist, is assigned a cost of 0.7 (assuming a global upper edge cost limit wmax = 6.0, Equation 1), whereas the lowest weighted edge, namely the two rdf:type relations, are both assigned a cost of 6.0\u22121.5 = 4.5. Given these costs, the cheapest path between, for instance, db:Johnny_Cash and db:Folk_music is the one through db:American_folk_music. Note that, in order to avoid long paths between very distant (and thus semantically unrelated) concepts, we limit the search based on a maximum search depth parameter nmax .\n\nii) lines 10-14: we next compute the node modification costs for each pair of source nodes. For paths found exceeding the path limit nmax , we set their cost to that of the most expensive path c max found within the input graph pair. Since it might not be the case that both graphs are fully connected, we also set c max as the cost for unconnected source node pairs. Finally, we normalize all cost values.\n\niii) lines 15-20: we build the final edit distance matrix Dm from the previously computed modification costs, as well as the costs of the node insertion and deletion operations, which we set to c max . This is to account for the fact that, given an arbitrary document pair, the cardinality of their sets of entities does not need to be the same: in this case, additional nodes are treated the same as very distant ones.  [3] 0.21 TakeLab [45] 0.08 Cosine baseline 0.56 Table 2: Results on the LP50 dataset (Pearson r correlation coefficient, best results are bolded).\n\niv) lines 21-22: the edit distance matrix Dm represents a bipartite matching problem, which we solve with the Hungarian method. This finds the optimal, cost-minimal assignment in our node operations matrix, while ensuring that each node will only be edited once. We finally normalize the graph edit distance costs to account for the number of source entities in the two input documents.\n\nAs a result of the execution of the algorithm, the normalized graph edit distance between G i and G j is returned. In our example, we will get a mapping from db:Bob_Dylan to db:Desire_(Bob_Dylan_album) (cost 1.3) and from db: Johnny_Cash to db:Folk_music (cost 0.9 + 0.8). The final similarity score is then given by the sum of these edit costs (3.0), normalized by the number of distinct source entities in both documents (6).\n\n\nExperiments\n\nExperimental setting. We evaluate our approach with a benchmarking dataset for document semantic similarity, in order to be able to compare our method against other stateof-the-art systems. To this end, we use the LP50 dataset [31], namely a collection of 50 news articles from the Australian Broadcasting Corporation (ABC), which were pairwise annotated with similarity rating on a Likert scale from 1 (very different) to 5 (very similar) by 8 to 12 different human annotators. To obtain the final similarity judgments, Lee et al. averaged for each pair the scores of all annotators: however, the final collection of 1,225 relatedness scores has only 67 distinct values. Consequently, Spearman's rank correlation is not appropriate to evaluate performance on this data and we opt instead, following previous work, for instance [17], for Pearson's linear correlation coefficient (r).\n\nResults and discussion. We report our performance figures on the LP50 dataset in Table 2, where we show the Pearson product-moment correlation coefficient r between the human gold standard and our graph-based approach (GED). In order to evaluate our method across different entity linking systems we test with both DBpedia Spotlight [32] and TagMe [15], two state-of-the-art systems [10]. For each tagger, we compute its performance with respect to different values for the maximum depth of the path search in the cost computation (nmax). We compare our GED-based method with a variety of baselines:\n\ni) a semantically-informed baseline which computes the Jaccard similarity coefficient over the set of entities identified within the input documents, namely sim(d1, d2) = C 1 \u2229C 2 C 1 \u222aC 2 , where C 1 and C 2 represent the set of concepts identified by the entity tagger (i.e., TagMe or Spotlight) within documents d1 and d2, respectively;\n\nii) an unsupervised baseline computed as the cosine distance of a standard bag-of-words Vector Space Model;\n\niii) two strong supervised baselines based on two publicly available supervised systems, namely DKPro [3] and Take-Lab [45], both trained on standard SemEval STS datasets.\n\nIn general, using our graph-based approach to document semantic similarity we are able to beat all baselines by a large margin, achieving a correlation coefficient of up to 0.63 (nmax = 2, using Spotlight and either combIC or IC+PMI weighting). This is equal to a relative improvement of 16.0% over the semantically-informed Jaccard baseline and 11.6% over the cosine bag-of-words baseline 4 . The results indicate that our method is able to always perform above the Jaccard baseline for nmax \u2264 3, and achieves the best performance for nmax = 2. These parameter values are indeed in-line with the optimal ones found by previous research contributions making use of graphs derived from Wikipedia or DBpedia [36,28], which also showed the benefits of mining information from short, highly specific paths. This, in turn, makes our model virtually parameter-free, because it implies that we can simply set the only tunable parameter of our method, namely the depth of the search used for concept matching, to standard values (i.e., 2 or 3) which are known to yield good performance across many different tasks. When looking at the performance of the different weighting measures, we see that we consistently obtain the best results using either combIC or IC+PMI, which corroborates our previous findings on entity ranking (Section 3). Finally, we notice that the different baselines show large performance variations. The simple cosine baseline turns out to be a difficult competitor -e.g., outperforming the simple Jaccard baselines computed from both TagMe and Spotlight annotations -which indicates that semantifying the input texts and applying a simple entity overlap measure is not enough to yield a robust performance. The supervised baselines, DKPro and TakeLab, both show instead an extremely low performance rate, although they were reported as being among the top systems of the SemEval STS 2012 shared task. This is because both systems are supervised in nature, and thus able to yield accurate performance only when in-domain labeled data are available. Next, in order to better understand the performance of our method, we compare it in Table 3 with an unweighted version that does not use edge weighting (i.e., all edge modifications have the same cost), as well as previous results from the literature. When computing semantic distances without weighting i.e., using the Hungarian method for mapping, but applied to unweighted paths only, we achieve up to r = 0.61 when using Spotlight and a maximum depth of 3 -12.5% r GED-based (weighted) 0.63 GED-based (unweighted) 0.61 Bag-of-Words [31] 0.1-0.5 LSA [31] 0.60 ESA -original [17] 0.72 ESA -reimplemented [4] 0.46-0.59 Table 3: System comparison on the LP50 data.\n\nabove the semantically-informed Jaccard baseline and 8.3% over the cosine bag-of-words baseline. This indicates the overall robustness of our GED method, which exploits highquality semantic paths from DBpedia. Similar to our results on entity ranking, additional performance gains can be achieved thanks to weighting semantic relations.\n\nWhen comparing our approach to the state of the art on this dataset we see that, while we outperform robust methods such as LSA [11], we are nevertheless not able to achieve a performance as high as that of Explicit Semantic Analysis (ESA, [17]). Our method, however, has clear advantages over ESA in that it provides a fully unsupervised approach that practically requires no tuning, and thus can be applied to arbitrary data and domains with virtually no changes. The original performance figures for ESA [17] have been criticized in fact for being based on a cut-off value used to prune the vectors being over-fitted to the LP50 data [4] -cf. also the much lower performance obtained by re-implementations like [4] and [22], among others. Thus, we take these figures to be promising in that our approach to document semantic similarity, while being based on a general document model with many potential applications -e.g. ranking related entities (cf. Section 3) -is nevertheless able to come close in performance to a highly specialized method like ESA, which has been tuned for this specific task and dataset.\n\nError analysis. In order to gain additional insights into the performance of our method, we performed an error analysis of its output. To this end, we focused on the manual analysis of documents deemed closest or most distant from the human judgments. When looking at specific document pairs, we found that our knowledge-rich approach is able to estimate well the similarity between documents with little or partial word overlap: connecting paths between DBpedia entities, in fact, were found to implicitly cover a wide range of topical associations, ranging from near-synonymity (\"U.S. intelligence\" and \"CIA\") all the way to metonymic relations (\"White House\" and \"Bush administration\"). However, since it relies only on DBpedia entities and their document mentions, our approach will perform badly in cases where i) the input documents contain few or no entities, or ii) they share the same entities, but describe different events. For instance, our method will give a very high similarity score to the following two sentences, although they describe completely different events: (a) Obama started his second term in the White House; (b) Obama will soon leave the White House. But while our approach could be extended to include relations between entities which are automatically extracted from text, cf. recent work on building event graphs from documents [20], our results seem also to suggest that in the case of text similarity we can often get away without a deep analysis of the documents' sentences, since entity overlap is a good proxy for topical affinity. This is highlighted by the following two sentences from the LP50 data, which, albeit very different, belong to documents which were deemed highly similar by the annotators:\n\n\u2022 Nigerian President Olusegun Obasanjo said he will weep if a single mother sentenced to death by stoning for having a child out of wedlock is killed, but added he has faith the court system will overturn her sentence. [. . . ]\n\n\u2022 An Islamic high court in northern Nigeria rejected an appeal today by a single mother sentenced to be stoned to death for having sex out of wedlock. [. . . ]\n\n\nRELATED WORK\n\nThe recent years have seen a great deal of work on computing semantic similarity [49]. This is arguably because semantic similarity provides a valuable model of semantic compatibility that is widely applicable to a variety of complex tasks, including both pre-processing tasks like Word Sense Disambiguation [38] and coreference resolution [39], and high-end applications such as information retrieval [14] or multi-document summarization [33], to name a few.\n\nMost of the previous work on semantic similarity has concentrated on computing pairwise similarity of words, although recent efforts concentrated on the broader task of text similarity [4], as also shown by community efforts such as the shared tasks on Semantic Textual Similarity [1]. Overall, the best results in these evaluation campaigns have been obtained by supervised models combining large feature sets [3,45], although questions remain on whether this approach can be easily ported to domains for which no labeled data exists. In contrast, in this work we presented an unsupervised model that requires virtually no parameter tuning and exploits the implicit supervision provided by very large amounts of structured knowledge encoded in DBpedia.\n\nThis work is, to the best of our knowledge, the first to exploit a wide-coverage ontology (i.e., other than small-scale semantic lexicons like WordNet) within a general-purpose algorithm for computing semantic similarity based on a graphbased similarity measure. Our method effectively uses large amounts of structured knowledge and can be used in principle with other such resources like, e.g., YAGO [25], provided they contain explicit semantic relations. Seminal work on representing natural language as semantic networks focused on queries [7]. Recently, graph-based representations from DBpedia have been explored by [28] for labeling topics, as obtained from a topic model, rather than providing structured representations of arbitrary texts. In addition, they limit graph construction to a small set of manually selected DBpedia relations. The work closest to ours is that of [42], who use graph-based representations of snippets for Web search results clustering. Their method also builds a document-based semantic graph from Wikipedia concepts, as obtained from the output an entity disambiguator. However, similarly to [43], they do not exploit explicit semantic relations between entities (which we show to be beneficial for both entity ranking and semantic similarity).\n\nPrevious work in computing semantic distances on linked data relied on disambiguated input [37], a requirement which is very hard to satisfy for most applications working with natural language text. In contrast, our approach relies on automatic entity linking techniques, which allow us to link entity mentions in text to well-defined entities within an ontology. From a general perspective, our work can be viewed as building upon seminal research work in IR that explored the use of controlled vocabularies [30], originally introduced for library systems. The proposed method can thus be seen as instance of an advanced Knowledge Organization System (KOS) [48,13], since it relies at its core on a wide-coverage ontology to represent documents. However, as opposed to these approaches, we do not create a controlled vocabulary for a specific document collection, but instead reuse an existing, background ontology which contains general world knowledge. We use this knowledge source to represent the entities found documents, as opposed to using the documents' headings or metadata. The Jaccard similarity we report in Section 4.3 consists, in fact, of a baseline method that uses DBpedia as controlled vocabulary: we build upon this intuition and extend it by using the information encoded within the structure of the DBpedia network.\n\n\nCONCLUSIONS\n\nIn this paper, we proposed a method for exploiting large amounts of machine-readable knowledge, i.e., entities and semantic relations, encoded within DBpedia, in order to provide a structured, i.e. graph-based, representation of natural language texts. Our results on entity ranking and document semantic similarity indicate that, thanks to an effective weighting of the semantic relations found within the semantic network, as well as a robust concept matching technique, we are able to achieve competitive performance on both these hard NLP tasks, while at the same time providing an unsupervised model which is practically parameter-free -namely, whose only tunable parameter can be fixed based on well-established findings from previous work. This is the first proposal to exploit a Web-scale ontology to provide structured representations of document content, and computing semantic distances in a knowledge-rich fashion. We build thematically upon previous contributions which showed the beneficial effect of exploiting large amounts of knowledge for enhancing text comparison [46,35] (inter alia). In our work, we take this line of research one step further by: (a) using a truly ontological resource for content modeling (as opposed, e.g., to semantic lexicons such as WordNet); (b) developing an information-theoretic measure to identify semantically specific, highly informative relations between entities in a large knowledge graph; (c) defining a new method, based on graph edit distance techniques, to quantify degrees of semantic similarity between documents: this views semantic similarity as a concept matching problem and uses the Hungarian method for solving the combinatorial optimization problem.\n\nOur vision, ultimately, is to show how entity linking and disambiguation techniques can enable an open-domain structured representation of documents, and accordingly an even larger Web of Semantic Data, on which semantic technologies (e.g., search) can be enabled. Accordingly, we focused in this first initial step primarily on entities, since they are the bulk of wide-coverage knowledge resources like DBpedia. Clearly, extending this entity-centric model -for instance, by means of event-structured graphs [20] or RDF predicates [19] -is the next logical step. Besides, as future work we plan to develop methods to jointly perform entity disambiguation and compute semantic similarity. We are also interested in applying our techniques within domains other than newswire data, and investigating domain adaptation techniques for the graph construction phase. Our graphs naturally model fine-grained information about documents: accordingly, we will explore their application to complex, high-end task such as aspect-oriented IR, as well as finegrained document classification and clustering for IR.\n\n\nWSDM'14, February 24-28, 2014, New York, New York, USA. Copyright 2014 ACM 978-1-4503-2351-2/14/02 ...$15.00. http://dx.doi.org/10.1145/2556195.2556250.\n\nFigure 1 :\n1Sample semantic graph.\n\nFigure 2 :\n2Entity ranking workflow.\n\nFigure 4 :\n4Document semantic similarity workflow.\n\nFigure 5 :\n5Graph document comparison (numbers on the graph edges indicate edge weights).\n\n\nG d is built by applying the procedure described in Section 2.2 while using V d s as the set of input entities.\n\n\nFigure 3: Example of multiple paths between entities with different semantic specificity.cand ) = \nmin \n\np\u2208paths(ref ,cand) \n\ncp(ref , cand ) . \n\ndb:Duluth, \nMinnesota \ndbo:country \n\ndb:United States \n\ndb:Mozambique \n(Song) \n\ndb:Bob Dylan \n\ndbo:album \ndbp:artist \n\ndbo:birthPlace \n\ndb:Desire \n(Bob Dylan album) \n\nUnwghtd jointIC combIC IC+PMI \nHollywood Celebr. \n0.639 \n0.541 \n0.690 \n0.661 \nIT Companies \n0.559 \n0.636 \n0.644 \n0.583 \nTelevision Series \n0.529 \n0.595 \n0.643 \n0.602 \nVideo Games \n0.451 \n0.562 \n0.532 \n0.484 \nChuck Norris \n0.458 \n0.409 \n0.558 \n0.506 \nAll 21 Entities \n0.541 \n0.575 \n0.624 \n0.579 \n\n\n\nTable 1 :\n1Performance on the entity ranking KORE dataset (best results are bolded).\nWe abbreviate URI namespaces with common prefixes, see http://prefix.cc for details.2 Hereafter, we use concept and entity interchangeably to refer to resources of the knowledge base, i.e., DBpedia URIs.3 We filter out any administrative information and data using a list of stop-URIs provided by[28] and extended by us.\nAll differences in performance are statistically significant at p < 0.05 using Fisher's Z-value transformation unless otherwise noted.\nAcknowledgmentsWe thank our colleagues Arnab Dutta, Christian Meilicke and Heiner Stuckenschmidt for their valuable comments on draft versions of this work.\n*SEM 2013 shared task: Semantic textual similarity. E Agirre, D Cer, M Diab, A Gonzalez-Agirre, W Guo, Proc. of *SEM-2013. of *SEM-2013E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W. Guo. *SEM 2013 shared task: Semantic textual similarity. In Proc. of *SEM-2013, pages 32-43, 2013.\n\nOpen information extraction from the Web. M Banko, M J Cafarella, S Soderland, M Broadhead, O Etzioni, Proc. of IJCAI-07. of IJCAI-07M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction from the Web. In Proc. of IJCAI-07, pages 2670-2676, 2007.\n\nUKP: Computing semantic textual similarity by combining multiple content similarity measures. D B\u00e4r, C Biemann, I Gurevych, T Zesch, Proc. of SemEval-2012. of SemEval-2012D. B\u00e4r, C. Biemann, I. Gurevych, and T. Zesch. UKP: Computing semantic textual similarity by combining multiple content similarity measures. In Proc. of SemEval-2012, pages 435-440, 2012.\n\nA reflective view on text similarity. D B\u00e4r, T Zesch, I Gurevych, Proc. of RANLP-11. of RANLP-11D. B\u00e4r, T. Zesch, and I. Gurevych. A reflective view on text similarity. In Proc. of RANLP-11, pages 515-520, 2011.\n\nLinked datathe story so far. C Bizer, T Heath, T Berners-Lee, International Journal on Semantic Web and Information Systems. C. Bizer, T. Heath, and T. Berners-Lee. Linked data - the story so far. International Journal on Semantic Web and Information Systems, 2012.\n\nDBpedia -A crystallization point for the web of data. C Bizer, J Lehmann, G Kobilarov, S Auer, C Becker, R Cyganiak, S Hellmann, Journal of Web Semantics. 73C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, and S. Hellmann. DBpedia - A crystallization point for the web of data. Journal of Web Semantics, 7(3):154-165, 2009.\n\nQuery sentences as semantic (sub) networks. J Booth, B D Eugenio, I F Cruz, O Wolfson, Proc. of ICSC-09. of ICSC-09J. Booth, B. D. Eugenio, I. F. Cruz, and O. Wolfson. Query sentences as semantic (sub) networks. In Proc. of ICSC-09, pages 89-94, 2009.\n\nDistributional semantics with eyes: Using image analysis to improve computational representations of word meaning. E Bruni, J Uijlings, M Baroni, N Sebe, Proc. of MM'12. of MM'12E. Bruni, J. Uijlings, M. Baroni, and N. Sebe. Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning. In Proc. of MM'12, pages 1219-1228, 2012.\n\nToward an architecture for never-ending language learning. A Carlson, J Betteridge, B Kisiel, B Settles, E R H Jr, T M Mitchell, Proc. of AAAI-10. of AAAI-10A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. H. Jr., and T. M. Mitchell. Toward an architecture for never-ending language learning. In Proc. of AAAI-10, pages 1306-1313, 2010.\n\nA framework for benchmarking entity-annotation systems. M Cornolti, P Ferragina, M Ciaramita, Proc. of WWW-13. of WWW-13M. Cornolti, P. Ferragina, and M. Ciaramita. A framework for benchmarking entity-annotation systems. In Proc. of WWW-13, pages 249-260, 2013.\n\nIndexing by latent semantic analysis. S Deerwester, S T Dumais, G W Furnas, T K Landauer, R Harshman, Journal of the American Society for Information Science. 416S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391-407, 1990.\n\nOverview of the INEX 2009 entity ranking track. G Demartini, T Iofciu, A P De Vries, INEX. G. Demartini, T. Iofciu, and A. P. de Vries. Overview of the INEX 2009 entity ranking track. In INEX, pages 254-264, 2009.\n\nUsage-driven Maintenance of Knowledge Organization Systems. K Eckert, Universit\u00e4t MannheimPhD thesisK. Eckert. Usage-driven Maintenance of Knowledge Organization Systems. PhD thesis, Universit\u00e4t Mannheim, 2012.\n\nConcept-based information retrieval using Explicit Semantic Analysis. O Egozi, S Markovitch, E Gabrilovich, 8:1-8:34ACM Transactions on Information Systems. 292O. Egozi, S. Markovitch, and E. Gabrilovich. Concept-based information retrieval using Explicit Semantic Analysis. ACM Transactions on Information Systems, 29(2):8:1-8:34, 2011.\n\nFast and accurate annotation of short texts with Wikipedia pages. P Ferragina, U Scaiella, IEEE Software. 291P. Ferragina and U. Scaiella. Fast and accurate annotation of short texts with Wikipedia pages. IEEE Software, 29(1):70-75, 2012.\n\nBuilding Watson: An overview of the DeepQA project. D A Ferrucci, E W Brown, J Chu-Carroll, J Fan, D Gondek, A Kalyanpur, A Lally, J W Murdock, E Nyberg, J M Prager, N Schlaefer, C A Welty, AI Magazine. 313D. A. Ferrucci, E. W. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. Kalyanpur, A. Lally, J. W. Murdock, E. Nyberg, J. M. Prager, N. Schlaefer, and C. A. Welty. Building Watson: An overview of the DeepQA project. AI Magazine, 31(3):59-79, 2010.\n\nComputing semantic relatedness using Wikipedia-based explicit semantic analysis. E Gabrilovich, S Markovitch, Proc. of IJCAI-07. of IJCAI-07E. Gabrilovich and S. Markovitch. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proc. of IJCAI-07, pages 1606-1611, 2007.\n\nA survey of graph edit distance. X Gao, B Xiao, D Tao, X Li, Pattern Analysis and Applications. 131X. Gao, B. Xiao, D. Tao, and X. Li. A survey of graph edit distance. Pattern Analysis and Applications, 13(1):113-129, 2010.\n\nExtracting multilingual natural-language patterns for RDF predicates. D Gerber, A.-C N Ngomo, Proc. of EKAW-12. of EKAW-12D. Gerber and A.-C. N. Ngomo. Extracting multilingual natural-language patterns for RDF predicates. In Proc. of EKAW-12, pages 87-96, 2012.\n\nRecognizing identical events with graph kernels. G Glava\u0161, J \u0160najder, Proc. of ACL-13. of ACL-13G. Glava\u0161 and J.\u0160najder. Recognizing identical events with graph kernels. In Proc. of ACL-13, pages 797-803, 2013.\n\nUBY -a large-scale unified lexical-semantic resource based on LMF. I Gurevych, J Eckle-Kohler, S Hartmann, M Matuschek, C M Meyer, C Wirth, Proc. of EACL-12. of EACL-12I. Gurevych, J. Eckle-Kohler, S. Hartmann, M. Matuschek, C. M. Meyer, and C. Wirth. UBY -a large-scale unified lexical-semantic resource based on LMF. In Proc. of EACL-12, pages 580-590, 2012.\n\nSemantic relatedness using salient semantic analysis. S Hassan, R Mihalcea, Proc. of AAAI-11. of AAAI-11S. Hassan and R. Mihalcea. Semantic relatedness using salient semantic analysis. In Proc. of AAAI-11, pages 884-889, 2011.\n\nCollecting links between entities ranked by human association strengths. J Hees, M Khamis, R Biedert, S Abdennadher, A Dengel, Proc. of ESWC-13. of ESWC-13J. Hees, M. Khamis, R. Biedert, S. Abdennadher, and A. Dengel. Collecting links between entities ranked by human association strengths. In Proc. of ESWC-13, pages 517-531, 2013.\n\nKORE: Keyphrase overlap relatedness for entity disambiguation. J Hoffart, S Seufert, D B Nguyen, M Theobald, G Weikum, Proc. of CIKM-12. of CIKM-12J. Hoffart, S. Seufert, D. B. Nguyen, M. Theobald, and G. Weikum. KORE: Keyphrase overlap relatedness for entity disambiguation. In Proc. of CIKM-12, pages 545-554, 2012.\n\nYAGO2: a spatially and temporally enhanced knowledge base from Wikipedia. J Hoffart, F M Suchanek, K Berberich, G Weikum, Artificial Intelligence. 194J. Hoffart, F. M. Suchanek, K. Berberich, and G. Weikum. YAGO2: a spatially and temporally enhanced knowledge base from Wikipedia. Artificial Intelligence, 194:28-61, 2013.\n\nRobust disambiguation of named entities in text. J Hoffart, M A Yosef, I Bordino, H F\u00fcrstenau, M Pinkal, M Spaniol, B Taneva, S Thater, G Weikum, Proc. of EMNLP-11. of EMNLP-11J. Hoffart, M. A. Yosef, I. Bordino, H. F\u00fcrstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and G. Weikum. Robust disambiguation of named entities in text. In Proc. of EMNLP-11, pages 782-792, 2011.\n\nCollaboratively built semi-structured content and Artificial Intelligence: The story so far. E Hovy, R Navigli, S P Ponzetto, Artificial Intelligence. 194E. Hovy, R. Navigli, and S. P. Ponzetto. Collaboratively built semi-structured content and Artificial Intelligence: The story so far. Artificial Intelligence, 194:2-27, 2013.\n\nUnsupervised graph-based topic labelling using DBpedia. I Hulpus, C Hayes, M Karnstedt, D Greene, Proc. of WSDM-13. of WSDM-13I. Hulpus, C. Hayes, M. Karnstedt, and D. Greene. Unsupervised graph-based topic labelling using DBpedia. In Proc. of WSDM-13, pages 465-474, 2013.\n\nKnowledge base population: Successful approaches and challenges. H Ji, R Grishman, Proc. of ACL-11. of ACL-11H. Ji and R. Grishman. Knowledge base population: Successful approaches and challenges. In Proc. of ACL-11, pages 1148-1158, 2011.\n\nVocabulary Control for Information Retrieval. F W Lancaster, Information Resources PressF. W. Lancaster. Vocabulary Control for Information Retrieval. Information Resources Press, 1972.\n\nAn empirical evaluation of models of text document similarity. M D Lee, B Pincombe, M Welsh, Proc. of CogSci. of CogSciM. D. Lee, B. Pincombe, and M. Welsh. An empirical evaluation of models of text document similarity. In Proc. of CogSci 2005, pages 1254-1259, 2005.\n\nDBpedia Spotlight : Shedding light on the web of documents. P N Mendes, M Jakob, A Garc\u00eda-Silva, C Bizer, Proc. of I-Semantics'11. of I-Semantics'11P. N. Mendes, M. Jakob, A. Garc\u00eda-Silva, and C. Bizer. DBpedia Spotlight : Shedding light on the web of documents. In Proc. of I-Semantics'11, pages 1-8, 2011.\n\nTopic-driven multi-document summarization with encyclopedic knowledge and activation spreading. V Nastase, Proc. of EMNLP-08. of EMNLP-08V. Nastase. Topic-driven multi-document summarization with encyclopedic knowledge and activation spreading. In Proc. of EMNLP-08, pages 763-772, 2008.\n\nClustering and diversifying Web search results with graph-based Word Sense Induction. R Navigli, A. Di Marco, Computational Linguistics. 393R. Navigli and A. Di Marco. Clustering and diversifying Web search results with graph-based Word Sense Induction. Computational Linguistics, 39(3):709-754, 2013.\n\nTwo birds with one stone: learning semantic models for Text Categorization and Word Sense Disambiguation. R Navigli, S Faralli, A Soroa, O L De Lacalle, E Agirre, Proc. of CIKM-11. of CIKM-11R. Navigli, S. Faralli, A. Soroa, O. L. de Lacalle, and E. Agirre. Two birds with one stone: learning semantic models for Text Categorization and Word Sense Disambiguation. In Proc. of CIKM-11, pages 2317-2320, 2011.\n\nBabelNet: the automatic construction, evaluation and application of a wide-coverage multilingual semantic network. R Navigli, S P Ponzetto, Artificial Intelligence. 193R. Navigli and S. P. Ponzetto. BabelNet: the automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217-250, 2012.\n\nMeasuring semantic distance on linking data and using it for resources recommendations. A Passant, Proceedings of the AAAI Spring Symposium \"Linked Data Meets Artificial Intelligence. the AAAI Spring Symposium \"Linked Data Meets Artificial IntelligenceA. Passant. Measuring semantic distance on linking data and using it for resources recommendations. In Proceedings of the AAAI Spring Symposium \"Linked Data Meets Artificial Intelligence\", 2010.\n\nUsing measures of semantic relatedness for Word Sense Disambiguation. S Patwardhan, S Banerjee, T Pedersen, Proc. of CICLing-03. of CICLing-03S. Patwardhan, S. Banerjee, and T. Pedersen. Using measures of semantic relatedness for Word Sense Disambiguation. In Proc. of CICLing-03, pages 241-257, 2003.\n\nKnowledge derived from Wikipedia for computing semantic relatedness. S P Ponzetto, M Strube, Journal of Artificial Intelligence Research. 30S. P. Ponzetto and M. Strube. Knowledge derived from Wikipedia for computing semantic relatedness. Journal of Artificial Intelligence Research, 30:181-212, 2007.\n\nS P Ponzetto, M Strube, Taxonomy induction based on a collaboratively built knowledge repository. 175S. P. Ponzetto and M. Strube. Taxonomy induction based on a collaboratively built knowledge repository. Artificial Intelligence, 175:1737-1756, 2011.\n\nApproximate graph edit distance computation by means of bipartite graph matching. K Riesen, H Bunke, Image Vision and Computing. 277K. Riesen and H. Bunke. Approximate graph edit distance computation by means of bipartite graph matching. Image Vision and Computing, 27(7):950-959, 2009.\n\nTopical clustering of search results. U Scaiella, P Ferragina, A Marino, M Ciaramita, Proc. of WSDM-12. of WSDM-12U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita. Topical clustering of search results. In Proc. of WSDM-12, pages 223-232, 2012.\n\nLINDEN: linking named entities with knowledge base via semantic knowledge. W Shen, J Wang, P Luo, M Wang, Proc. of WWW-12. of WWW-12W. Shen, J. Wang, P. Luo, and M. Wang. LINDEN: linking named entities with knowledge base via semantic knowledge. In Proc. of WWW-12, pages 449-458, 2012.\n\nFrom frequency to meaning: Vector space models of semantics. P D Turney, P Pantel, Journal of Artificial Intelligence Research. 37P. D. Turney and P. Pantel. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141-188, 2010.\n\nTakeLab: Systems for measuring semantic text similarity. F \u0160ari\u0107, G Glava\u0161, M Karan, J \u0160najder, B Dalbelo Ba\u0161i\u0107, Proc. of SemEval-2012. of SemEval-2012F.\u0160ari\u0107, G. Glava\u0161, M. Karan, J.\u0160najder, and B. Dalbelo Ba\u0161i\u0107. TakeLab: Systems for measuring semantic text similarity. In Proc. of SemEval-2012, pages 441-448, 2012.\n\nBuilding semantic kernels for text classification using Wikipedia. P Wang, C Domeniconi, Proc. of KDD '08. of KDD '08P. Wang and C. Domeniconi. Building semantic kernels for text classification using Wikipedia. In Proc. of KDD '08, pages 713-721, 2008.\n\nNatural language questions for the web of data. M Yahya, K Berberich, S Elbassuoni, M Ramanath, V Tresp, G Weikum, Proc. of EMNLP-CoNLL-12. of EMNLP-CoNLL-12M. Yahya, K. Berberich, S. Elbassuoni, M. Ramanath, V. Tresp, and G. Weikum. Natural language questions for the web of data. In Proc. of EMNLP-CoNLL-12, pages 379-390, 2012.\n\nKnowledge Organization Systems (KOS). M L Zeng, 35Knowledge OrganizationM. L. Zeng. Knowledge Organization Systems (KOS). Knowledge Organization, 35(2-3):160-182, 2008.\n\nRecent advances in methods of lexical semantic relatednessa survey. Z Zhang, A L Gentile, F Ciravegna, Natural Language Engineering. 11Z. Zhang, A. L. Gentile, and F. Ciravegna. Recent advances in methods of lexical semantic relatedness - a survey. Natural Language Engineering, 1(1):1-69, 2012.\n", "annotations": {"author": "[{\"end\":167,\"start\":43},{\"end\":222,\"start\":168}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":51},{\"end\":189,\"start\":181}]", "author_first_name": "[{\"end\":50,\"start\":43},{\"end\":174,\"start\":168},{\"end\":180,\"start\":175}]", "author_affiliation": "[{\"end\":166,\"start\":64},{\"end\":221,\"start\":191}]", "title": "[{\"end\":40,\"start\":1},{\"end\":262,\"start\":223}]", "venue": null, "abstract": "[{\"end\":1539,\"start\":568}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1812,\"start\":1808},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1815,\"start\":1812},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1837,\"start\":1833},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1876,\"start\":1872},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2663,\"start\":2660},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2665,\"start\":2663},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2736,\"start\":2732},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2739,\"start\":2736},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2742,\"start\":2739},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2745,\"start\":2742},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2938,\"start\":2934},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3422,\"start\":3418},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3504,\"start\":3500},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3521,\"start\":3518},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3673,\"start\":3669},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3688,\"start\":3685},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6766,\"start\":6762},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6769,\"start\":6766},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7102,\"start\":7099},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7344,\"start\":7340},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7981,\"start\":7978},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10802,\"start\":10798},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10805,\"start\":10802},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11706,\"start\":11702},{\"end\":12600,\"start\":12590},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12630,\"start\":12626},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17249,\"start\":17245},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17337,\"start\":17333},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17397,\"start\":17393},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":18059,\"start\":18055},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18255,\"start\":18251},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18258,\"start\":18255},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18617,\"start\":18613},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21369,\"start\":21365},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23796,\"start\":23792},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24904,\"start\":24901},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25371,\"start\":25368},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25727,\"start\":25723},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":26812,\"start\":26808},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29135,\"start\":29131},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32730,\"start\":32727},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32748,\"start\":32744},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":33937,\"start\":33933},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34538,\"start\":34534},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34928,\"start\":34924},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":34943,\"start\":34939},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34978,\"start\":34974},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35747,\"start\":35744},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":35765,\"start\":35761},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":36206,\"start\":36205},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":36525,\"start\":36521},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":36528,\"start\":36525},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":38418,\"start\":38414},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":38435,\"start\":38431},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":38459,\"start\":38455},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":38487,\"start\":38484},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":39014,\"start\":39010},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39126,\"start\":39122},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":39393,\"start\":39389},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39522,\"start\":39519},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39599,\"start\":39596},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39608,\"start\":39604},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":41362,\"start\":41358},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":42231,\"start\":42227},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":42458,\"start\":42454},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":42490,\"start\":42486},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":42552,\"start\":42548},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":42589,\"start\":42585},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":42795,\"start\":42792},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":42891,\"start\":42888},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":43021,\"start\":43018},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":43024,\"start\":43021},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43767,\"start\":43763},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":43909,\"start\":43906},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43988,\"start\":43984},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":44249,\"start\":44245},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":44495,\"start\":44491},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":44740,\"start\":44736},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":45158,\"start\":45154},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":45307,\"start\":45303},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":45310,\"start\":45307},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":47085,\"start\":47081},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":47088,\"start\":47085},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":48230,\"start\":48226},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":48253,\"start\":48249},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":50087,\"start\":50086},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":50206,\"start\":50205},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":50302,\"start\":50298}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48972,\"start\":48818},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49008,\"start\":48973},{\"attributes\":{\"id\":\"fig_2\"},\"end\":49046,\"start\":49009},{\"attributes\":{\"id\":\"fig_3\"},\"end\":49098,\"start\":49047},{\"attributes\":{\"id\":\"fig_4\"},\"end\":49189,\"start\":49099},{\"attributes\":{\"id\":\"fig_5\"},\"end\":49303,\"start\":49190},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49915,\"start\":49304},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50001,\"start\":49916}]", "paragraph": "[{\"end\":2939,\"start\":1555},{\"end\":3689,\"start\":2941},{\"end\":4181,\"start\":3691},{\"end\":4366,\"start\":4183},{\"end\":4739,\"start\":4368},{\"end\":5114,\"start\":4741},{\"end\":5432,\"start\":5116},{\"end\":6226,\"start\":5434},{\"end\":6956,\"start\":6283},{\"end\":7457,\"start\":6968},{\"end\":8837,\"start\":7459},{\"end\":9859,\"start\":8869},{\"end\":11474,\"start\":9861},{\"end\":13218,\"start\":11506},{\"end\":13435,\"start\":13220},{\"end\":14117,\"start\":13481},{\"end\":15058,\"start\":14119},{\"end\":15272,\"start\":15111},{\"end\":15928,\"start\":15274},{\"end\":16181,\"start\":15973},{\"end\":16254,\"start\":16253},{\"end\":16837,\"start\":16256},{\"end\":17358,\"start\":16920},{\"end\":18618,\"start\":17360},{\"end\":18841,\"start\":18620},{\"end\":19001,\"start\":18843},{\"end\":19061,\"start\":19003},{\"end\":19268,\"start\":19087},{\"end\":19679,\"start\":19270},{\"end\":21266,\"start\":19764},{\"end\":21956,\"start\":21268},{\"end\":21985,\"start\":21958},{\"end\":22044,\"start\":22004},{\"end\":24448,\"start\":22055},{\"end\":25460,\"start\":24482},{\"end\":26234,\"start\":25492},{\"end\":26780,\"start\":26358},{\"end\":27767,\"start\":26782},{\"end\":28819,\"start\":27769},{\"end\":30152,\"start\":28821},{\"end\":30485,\"start\":30154},{\"end\":30878,\"start\":30843},{\"end\":30915,\"start\":30880},{\"end\":30949,\"start\":30917},{\"end\":31896,\"start\":31036},{\"end\":32304,\"start\":31898},{\"end\":32873,\"start\":32306},{\"end\":33261,\"start\":32875},{\"end\":33690,\"start\":33263},{\"end\":34589,\"start\":33706},{\"end\":35190,\"start\":34591},{\"end\":35531,\"start\":35192},{\"end\":35640,\"start\":35533},{\"end\":35813,\"start\":35642},{\"end\":38542,\"start\":35815},{\"end\":38880,\"start\":38544},{\"end\":39996,\"start\":38882},{\"end\":41739,\"start\":39998},{\"end\":41968,\"start\":41741},{\"end\":42129,\"start\":41970},{\"end\":42605,\"start\":42146},{\"end\":43360,\"start\":42607},{\"end\":44643,\"start\":43362},{\"end\":45982,\"start\":44645},{\"end\":47714,\"start\":45998},{\"end\":48817,\"start\":47716}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13480,\"start\":13436},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15110,\"start\":15059},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15972,\"start\":15929},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16252,\"start\":16182},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16890,\"start\":16838},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19086,\"start\":19062},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19694,\"start\":19680},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19763,\"start\":19694},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26323,\"start\":26235},{\"attributes\":{\"id\":\"formula_9\"},\"end\":30842,\"start\":30486},{\"attributes\":{\"id\":\"formula_10\"},\"end\":30962,\"start\":30950},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31035,\"start\":30962}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22885,\"start\":22878},{\"end\":32782,\"start\":32775},{\"end\":34679,\"start\":34672},{\"end\":37969,\"start\":37962},{\"end\":38505,\"start\":38498}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1553,\"start\":1541},{\"attributes\":{\"n\":\"2.\"},\"end\":6281,\"start\":6229},{\"attributes\":{\"n\":\"2.1\"},\"end\":6966,\"start\":6959},{\"attributes\":{\"n\":\"2.2\"},\"end\":8867,\"start\":8840},{\"attributes\":{\"n\":\"2.3\"},\"end\":11504,\"start\":11477},{\"attributes\":{\"n\":\"3.\"},\"end\":16918,\"start\":16892},{\"end\":22002,\"start\":21988},{\"end\":22053,\"start\":22047},{\"attributes\":{\"n\":\"4.\"},\"end\":24480,\"start\":24451},{\"attributes\":{\"n\":\"4.1\"},\"end\":25490,\"start\":25463},{\"attributes\":{\"n\":\"4.2\"},\"end\":26356,\"start\":26325},{\"attributes\":{\"n\":\"4.3\"},\"end\":33704,\"start\":33693},{\"attributes\":{\"n\":\"5.\"},\"end\":42144,\"start\":42132},{\"attributes\":{\"n\":\"6.\"},\"end\":45996,\"start\":45985},{\"end\":48984,\"start\":48974},{\"end\":49020,\"start\":49010},{\"end\":49058,\"start\":49048},{\"end\":49110,\"start\":49100},{\"end\":49926,\"start\":49917}]", "table": "[{\"end\":49915,\"start\":49395}]", "figure_caption": "[{\"end\":48972,\"start\":48820},{\"end\":49008,\"start\":48986},{\"end\":49046,\"start\":49022},{\"end\":49098,\"start\":49060},{\"end\":49189,\"start\":49112},{\"end\":49303,\"start\":49192},{\"end\":49395,\"start\":49306},{\"end\":50001,\"start\":49928}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10815,\"start\":10807},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14416,\"start\":14408},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18693,\"start\":18685},{\"end\":19998,\"start\":19990},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24641,\"start\":24633},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26081,\"start\":26073},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31164,\"start\":31156}]", "bib_author_first_name": "[{\"end\":50668,\"start\":50667},{\"end\":50678,\"start\":50677},{\"end\":50685,\"start\":50684},{\"end\":50693,\"start\":50692},{\"end\":50712,\"start\":50711},{\"end\":50948,\"start\":50947},{\"end\":50957,\"start\":50956},{\"end\":50959,\"start\":50958},{\"end\":50972,\"start\":50971},{\"end\":50985,\"start\":50984},{\"end\":50998,\"start\":50997},{\"end\":51292,\"start\":51291},{\"end\":51299,\"start\":51298},{\"end\":51310,\"start\":51309},{\"end\":51322,\"start\":51321},{\"end\":51596,\"start\":51595},{\"end\":51603,\"start\":51602},{\"end\":51612,\"start\":51611},{\"end\":51800,\"start\":51799},{\"end\":51809,\"start\":51808},{\"end\":51818,\"start\":51817},{\"end\":52092,\"start\":52091},{\"end\":52101,\"start\":52100},{\"end\":52112,\"start\":52111},{\"end\":52125,\"start\":52124},{\"end\":52133,\"start\":52132},{\"end\":52143,\"start\":52142},{\"end\":52155,\"start\":52154},{\"end\":52427,\"start\":52426},{\"end\":52436,\"start\":52435},{\"end\":52438,\"start\":52437},{\"end\":52449,\"start\":52448},{\"end\":52451,\"start\":52450},{\"end\":52459,\"start\":52458},{\"end\":52751,\"start\":52750},{\"end\":52760,\"start\":52759},{\"end\":52772,\"start\":52771},{\"end\":52782,\"start\":52781},{\"end\":53078,\"start\":53077},{\"end\":53089,\"start\":53088},{\"end\":53103,\"start\":53102},{\"end\":53113,\"start\":53112},{\"end\":53124,\"start\":53123},{\"end\":53128,\"start\":53125},{\"end\":53134,\"start\":53133},{\"end\":53136,\"start\":53135},{\"end\":53420,\"start\":53419},{\"end\":53432,\"start\":53431},{\"end\":53445,\"start\":53444},{\"end\":53665,\"start\":53664},{\"end\":53679,\"start\":53678},{\"end\":53681,\"start\":53680},{\"end\":53691,\"start\":53690},{\"end\":53693,\"start\":53692},{\"end\":53703,\"start\":53702},{\"end\":53705,\"start\":53704},{\"end\":53717,\"start\":53716},{\"end\":54030,\"start\":54029},{\"end\":54043,\"start\":54042},{\"end\":54053,\"start\":54052},{\"end\":54055,\"start\":54054},{\"end\":54257,\"start\":54256},{\"end\":54479,\"start\":54478},{\"end\":54488,\"start\":54487},{\"end\":54502,\"start\":54501},{\"end\":54814,\"start\":54813},{\"end\":54827,\"start\":54826},{\"end\":55040,\"start\":55039},{\"end\":55042,\"start\":55041},{\"end\":55054,\"start\":55053},{\"end\":55056,\"start\":55055},{\"end\":55065,\"start\":55064},{\"end\":55080,\"start\":55079},{\"end\":55087,\"start\":55086},{\"end\":55097,\"start\":55096},{\"end\":55110,\"start\":55109},{\"end\":55119,\"start\":55118},{\"end\":55121,\"start\":55120},{\"end\":55132,\"start\":55131},{\"end\":55142,\"start\":55141},{\"end\":55144,\"start\":55143},{\"end\":55154,\"start\":55153},{\"end\":55167,\"start\":55166},{\"end\":55169,\"start\":55168},{\"end\":55519,\"start\":55518},{\"end\":55534,\"start\":55533},{\"end\":55772,\"start\":55771},{\"end\":55779,\"start\":55778},{\"end\":55787,\"start\":55786},{\"end\":55794,\"start\":55793},{\"end\":56034,\"start\":56033},{\"end\":56047,\"start\":56043},{\"end\":56049,\"start\":56048},{\"end\":56276,\"start\":56275},{\"end\":56286,\"start\":56285},{\"end\":56506,\"start\":56505},{\"end\":56518,\"start\":56517},{\"end\":56534,\"start\":56533},{\"end\":56546,\"start\":56545},{\"end\":56559,\"start\":56558},{\"end\":56561,\"start\":56560},{\"end\":56570,\"start\":56569},{\"end\":56855,\"start\":56854},{\"end\":56865,\"start\":56864},{\"end\":57102,\"start\":57101},{\"end\":57110,\"start\":57109},{\"end\":57120,\"start\":57119},{\"end\":57131,\"start\":57130},{\"end\":57146,\"start\":57145},{\"end\":57426,\"start\":57425},{\"end\":57437,\"start\":57436},{\"end\":57448,\"start\":57447},{\"end\":57450,\"start\":57449},{\"end\":57460,\"start\":57459},{\"end\":57472,\"start\":57471},{\"end\":57756,\"start\":57755},{\"end\":57767,\"start\":57766},{\"end\":57769,\"start\":57768},{\"end\":57781,\"start\":57780},{\"end\":57794,\"start\":57793},{\"end\":58055,\"start\":58054},{\"end\":58066,\"start\":58065},{\"end\":58068,\"start\":58067},{\"end\":58077,\"start\":58076},{\"end\":58088,\"start\":58087},{\"end\":58101,\"start\":58100},{\"end\":58111,\"start\":58110},{\"end\":58122,\"start\":58121},{\"end\":58132,\"start\":58131},{\"end\":58142,\"start\":58141},{\"end\":58479,\"start\":58478},{\"end\":58487,\"start\":58486},{\"end\":58498,\"start\":58497},{\"end\":58500,\"start\":58499},{\"end\":58772,\"start\":58771},{\"end\":58782,\"start\":58781},{\"end\":58791,\"start\":58790},{\"end\":58804,\"start\":58803},{\"end\":59056,\"start\":59055},{\"end\":59062,\"start\":59061},{\"end\":59278,\"start\":59277},{\"end\":59280,\"start\":59279},{\"end\":59482,\"start\":59481},{\"end\":59484,\"start\":59483},{\"end\":59491,\"start\":59490},{\"end\":59503,\"start\":59502},{\"end\":59748,\"start\":59747},{\"end\":59750,\"start\":59749},{\"end\":59760,\"start\":59759},{\"end\":59769,\"start\":59768},{\"end\":59785,\"start\":59784},{\"end\":60093,\"start\":60092},{\"end\":60372,\"start\":60371},{\"end\":60387,\"start\":60382},{\"end\":60695,\"start\":60694},{\"end\":60706,\"start\":60705},{\"end\":60717,\"start\":60716},{\"end\":60726,\"start\":60725},{\"end\":60728,\"start\":60727},{\"end\":60742,\"start\":60741},{\"end\":61113,\"start\":61112},{\"end\":61124,\"start\":61123},{\"end\":61126,\"start\":61125},{\"end\":61445,\"start\":61444},{\"end\":61875,\"start\":61874},{\"end\":61889,\"start\":61888},{\"end\":61901,\"start\":61900},{\"end\":62177,\"start\":62176},{\"end\":62179,\"start\":62178},{\"end\":62191,\"start\":62190},{\"end\":62411,\"start\":62410},{\"end\":62413,\"start\":62412},{\"end\":62425,\"start\":62424},{\"end\":62745,\"start\":62744},{\"end\":62755,\"start\":62754},{\"end\":62989,\"start\":62988},{\"end\":63001,\"start\":63000},{\"end\":63014,\"start\":63013},{\"end\":63024,\"start\":63023},{\"end\":63277,\"start\":63276},{\"end\":63285,\"start\":63284},{\"end\":63293,\"start\":63292},{\"end\":63300,\"start\":63299},{\"end\":63551,\"start\":63550},{\"end\":63553,\"start\":63552},{\"end\":63563,\"start\":63562},{\"end\":63830,\"start\":63829},{\"end\":63839,\"start\":63838},{\"end\":63849,\"start\":63848},{\"end\":63858,\"start\":63857},{\"end\":63869,\"start\":63868},{\"end\":63877,\"start\":63870},{\"end\":64159,\"start\":64158},{\"end\":64167,\"start\":64166},{\"end\":64394,\"start\":64393},{\"end\":64403,\"start\":64402},{\"end\":64416,\"start\":64415},{\"end\":64430,\"start\":64429},{\"end\":64442,\"start\":64441},{\"end\":64451,\"start\":64450},{\"end\":64716,\"start\":64715},{\"end\":64718,\"start\":64717},{\"end\":64916,\"start\":64915},{\"end\":64925,\"start\":64924},{\"end\":64927,\"start\":64926},{\"end\":64938,\"start\":64937}]", "bib_author_last_name": "[{\"end\":50675,\"start\":50669},{\"end\":50682,\"start\":50679},{\"end\":50690,\"start\":50686},{\"end\":50709,\"start\":50694},{\"end\":50716,\"start\":50713},{\"end\":50954,\"start\":50949},{\"end\":50969,\"start\":50960},{\"end\":50982,\"start\":50973},{\"end\":50995,\"start\":50986},{\"end\":51006,\"start\":50999},{\"end\":51296,\"start\":51293},{\"end\":51307,\"start\":51300},{\"end\":51319,\"start\":51311},{\"end\":51328,\"start\":51323},{\"end\":51600,\"start\":51597},{\"end\":51609,\"start\":51604},{\"end\":51621,\"start\":51613},{\"end\":51806,\"start\":51801},{\"end\":51815,\"start\":51810},{\"end\":51830,\"start\":51819},{\"end\":52098,\"start\":52093},{\"end\":52109,\"start\":52102},{\"end\":52122,\"start\":52113},{\"end\":52130,\"start\":52126},{\"end\":52140,\"start\":52134},{\"end\":52152,\"start\":52144},{\"end\":52164,\"start\":52156},{\"end\":52433,\"start\":52428},{\"end\":52446,\"start\":52439},{\"end\":52456,\"start\":52452},{\"end\":52467,\"start\":52460},{\"end\":52757,\"start\":52752},{\"end\":52769,\"start\":52761},{\"end\":52779,\"start\":52773},{\"end\":52787,\"start\":52783},{\"end\":53086,\"start\":53079},{\"end\":53100,\"start\":53090},{\"end\":53110,\"start\":53104},{\"end\":53121,\"start\":53114},{\"end\":53145,\"start\":53137},{\"end\":53429,\"start\":53421},{\"end\":53442,\"start\":53433},{\"end\":53455,\"start\":53446},{\"end\":53676,\"start\":53666},{\"end\":53688,\"start\":53682},{\"end\":53700,\"start\":53694},{\"end\":53714,\"start\":53706},{\"end\":53726,\"start\":53718},{\"end\":54040,\"start\":54031},{\"end\":54050,\"start\":54044},{\"end\":54064,\"start\":54056},{\"end\":54264,\"start\":54258},{\"end\":54485,\"start\":54480},{\"end\":54499,\"start\":54489},{\"end\":54514,\"start\":54503},{\"end\":54824,\"start\":54815},{\"end\":54836,\"start\":54828},{\"end\":55051,\"start\":55043},{\"end\":55062,\"start\":55057},{\"end\":55077,\"start\":55066},{\"end\":55084,\"start\":55081},{\"end\":55094,\"start\":55088},{\"end\":55107,\"start\":55098},{\"end\":55116,\"start\":55111},{\"end\":55129,\"start\":55122},{\"end\":55139,\"start\":55133},{\"end\":55151,\"start\":55145},{\"end\":55164,\"start\":55155},{\"end\":55175,\"start\":55170},{\"end\":55531,\"start\":55520},{\"end\":55545,\"start\":55535},{\"end\":55776,\"start\":55773},{\"end\":55784,\"start\":55780},{\"end\":55791,\"start\":55788},{\"end\":55797,\"start\":55795},{\"end\":56041,\"start\":56035},{\"end\":56055,\"start\":56050},{\"end\":56283,\"start\":56277},{\"end\":56294,\"start\":56287},{\"end\":56515,\"start\":56507},{\"end\":56531,\"start\":56519},{\"end\":56543,\"start\":56535},{\"end\":56556,\"start\":56547},{\"end\":56567,\"start\":56562},{\"end\":56576,\"start\":56571},{\"end\":56862,\"start\":56856},{\"end\":56874,\"start\":56866},{\"end\":57107,\"start\":57103},{\"end\":57117,\"start\":57111},{\"end\":57128,\"start\":57121},{\"end\":57143,\"start\":57132},{\"end\":57153,\"start\":57147},{\"end\":57434,\"start\":57427},{\"end\":57445,\"start\":57438},{\"end\":57457,\"start\":57451},{\"end\":57469,\"start\":57461},{\"end\":57479,\"start\":57473},{\"end\":57764,\"start\":57757},{\"end\":57778,\"start\":57770},{\"end\":57791,\"start\":57782},{\"end\":57801,\"start\":57795},{\"end\":58063,\"start\":58056},{\"end\":58074,\"start\":58069},{\"end\":58085,\"start\":58078},{\"end\":58098,\"start\":58089},{\"end\":58108,\"start\":58102},{\"end\":58119,\"start\":58112},{\"end\":58129,\"start\":58123},{\"end\":58139,\"start\":58133},{\"end\":58149,\"start\":58143},{\"end\":58484,\"start\":58480},{\"end\":58495,\"start\":58488},{\"end\":58509,\"start\":58501},{\"end\":58779,\"start\":58773},{\"end\":58788,\"start\":58783},{\"end\":58801,\"start\":58792},{\"end\":58811,\"start\":58805},{\"end\":59059,\"start\":59057},{\"end\":59071,\"start\":59063},{\"end\":59290,\"start\":59281},{\"end\":59488,\"start\":59485},{\"end\":59500,\"start\":59492},{\"end\":59509,\"start\":59504},{\"end\":59757,\"start\":59751},{\"end\":59766,\"start\":59761},{\"end\":59782,\"start\":59770},{\"end\":59791,\"start\":59786},{\"end\":60101,\"start\":60094},{\"end\":60380,\"start\":60373},{\"end\":60393,\"start\":60388},{\"end\":60703,\"start\":60696},{\"end\":60714,\"start\":60707},{\"end\":60723,\"start\":60718},{\"end\":60739,\"start\":60729},{\"end\":60749,\"start\":60743},{\"end\":61121,\"start\":61114},{\"end\":61135,\"start\":61127},{\"end\":61453,\"start\":61446},{\"end\":61886,\"start\":61876},{\"end\":61898,\"start\":61890},{\"end\":61910,\"start\":61902},{\"end\":62188,\"start\":62180},{\"end\":62198,\"start\":62192},{\"end\":62422,\"start\":62414},{\"end\":62432,\"start\":62426},{\"end\":62752,\"start\":62746},{\"end\":62761,\"start\":62756},{\"end\":62998,\"start\":62990},{\"end\":63011,\"start\":63002},{\"end\":63021,\"start\":63015},{\"end\":63034,\"start\":63025},{\"end\":63282,\"start\":63278},{\"end\":63290,\"start\":63286},{\"end\":63297,\"start\":63294},{\"end\":63305,\"start\":63301},{\"end\":63560,\"start\":63554},{\"end\":63570,\"start\":63564},{\"end\":63836,\"start\":63831},{\"end\":63846,\"start\":63840},{\"end\":63855,\"start\":63850},{\"end\":63866,\"start\":63859},{\"end\":63883,\"start\":63878},{\"end\":64164,\"start\":64160},{\"end\":64178,\"start\":64168},{\"end\":64400,\"start\":64395},{\"end\":64413,\"start\":64404},{\"end\":64427,\"start\":64417},{\"end\":64439,\"start\":64431},{\"end\":64448,\"start\":64443},{\"end\":64458,\"start\":64452},{\"end\":64723,\"start\":64719},{\"end\":64922,\"start\":64917},{\"end\":64935,\"start\":64928},{\"end\":64948,\"start\":64939}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":10241043},\"end\":50903,\"start\":50615},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207169186},\"end\":51195,\"start\":50905},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":6964767},\"end\":51555,\"start\":51197},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":12979104},\"end\":51768,\"start\":51557},{\"attributes\":{\"id\":\"b4\"},\"end\":52035,\"start\":51770},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16081721},\"end\":52380,\"start\":52037},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5713794},\"end\":52633,\"start\":52382},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6345436},\"end\":53016,\"start\":52635},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8423494},\"end\":53361,\"start\":53018},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":207204971},\"end\":53624,\"start\":53363},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3252915},\"end\":53979,\"start\":53626},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6794803},\"end\":54194,\"start\":53981},{\"attributes\":{\"id\":\"b12\"},\"end\":54406,\"start\":54196},{\"attributes\":{\"doi\":\"8:1-8:34\",\"id\":\"b13\",\"matched_paper_id\":743663},\"end\":54745,\"start\":54408},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":16415924},\"end\":54985,\"start\":54747},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1831060},\"end\":55435,\"start\":54987},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5291693},\"end\":55736,\"start\":55437},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11336685},\"end\":55961,\"start\":55738},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1941128},\"end\":56224,\"start\":55963},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":196096},\"end\":56436,\"start\":56226},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9692934},\"end\":56798,\"start\":56438},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":8041824},\"end\":57026,\"start\":56800},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":12815098},\"end\":57360,\"start\":57028},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1662570},\"end\":57679,\"start\":57362},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6118799},\"end\":58003,\"start\":57681},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6216506},\"end\":58383,\"start\":58005},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9648797},\"end\":58713,\"start\":58385},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":13998523},\"end\":58988,\"start\":58715},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7693051},\"end\":59229,\"start\":58990},{\"attributes\":{\"id\":\"b29\"},\"end\":59416,\"start\":59231},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":645710},\"end\":59685,\"start\":59418},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8722811},\"end\":59994,\"start\":59687},{\"attributes\":{\"id\":\"b32\"},\"end\":60283,\"start\":59996},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1775181},\"end\":60586,\"start\":60285},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":4323602},\"end\":60995,\"start\":60588},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6063065},\"end\":61354,\"start\":60997},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6413346},\"end\":61802,\"start\":61356},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":40585807},\"end\":62105,\"start\":61804},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2602667},\"end\":62408,\"start\":62107},{\"attributes\":{\"id\":\"b39\"},\"end\":62660,\"start\":62410},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":18168842},\"end\":62948,\"start\":62662},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":8109978},\"end\":63199,\"start\":62950},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":7994460},\"end\":63487,\"start\":63201},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1500900},\"end\":63770,\"start\":63489},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12233462},\"end\":64089,\"start\":63772},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":13950266},\"end\":64343,\"start\":64091},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":16639862},\"end\":64675,\"start\":64345},{\"attributes\":{\"id\":\"b47\"},\"end\":64845,\"start\":64677},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":42827241},\"end\":65142,\"start\":64847}]", "bib_title": "[{\"end\":50665,\"start\":50615},{\"end\":50945,\"start\":50905},{\"end\":51289,\"start\":51197},{\"end\":51593,\"start\":51557},{\"end\":51797,\"start\":51770},{\"end\":52089,\"start\":52037},{\"end\":52424,\"start\":52382},{\"end\":52748,\"start\":52635},{\"end\":53075,\"start\":53018},{\"end\":53417,\"start\":53363},{\"end\":53662,\"start\":53626},{\"end\":54027,\"start\":53981},{\"end\":54476,\"start\":54408},{\"end\":54811,\"start\":54747},{\"end\":55037,\"start\":54987},{\"end\":55516,\"start\":55437},{\"end\":55769,\"start\":55738},{\"end\":56031,\"start\":55963},{\"end\":56273,\"start\":56226},{\"end\":56503,\"start\":56438},{\"end\":56852,\"start\":56800},{\"end\":57099,\"start\":57028},{\"end\":57423,\"start\":57362},{\"end\":57753,\"start\":57681},{\"end\":58052,\"start\":58005},{\"end\":58476,\"start\":58385},{\"end\":58769,\"start\":58715},{\"end\":59053,\"start\":58990},{\"end\":59479,\"start\":59418},{\"end\":59745,\"start\":59687},{\"end\":60090,\"start\":59996},{\"end\":60369,\"start\":60285},{\"end\":60692,\"start\":60588},{\"end\":61110,\"start\":60997},{\"end\":61442,\"start\":61356},{\"end\":61872,\"start\":61804},{\"end\":62174,\"start\":62107},{\"end\":62742,\"start\":62662},{\"end\":62986,\"start\":62950},{\"end\":63274,\"start\":63201},{\"end\":63548,\"start\":63489},{\"end\":63827,\"start\":63772},{\"end\":64156,\"start\":64091},{\"end\":64391,\"start\":64345},{\"end\":64913,\"start\":64847}]", "bib_author": "[{\"end\":50677,\"start\":50667},{\"end\":50684,\"start\":50677},{\"end\":50692,\"start\":50684},{\"end\":50711,\"start\":50692},{\"end\":50718,\"start\":50711},{\"end\":50956,\"start\":50947},{\"end\":50971,\"start\":50956},{\"end\":50984,\"start\":50971},{\"end\":50997,\"start\":50984},{\"end\":51008,\"start\":50997},{\"end\":51298,\"start\":51291},{\"end\":51309,\"start\":51298},{\"end\":51321,\"start\":51309},{\"end\":51330,\"start\":51321},{\"end\":51602,\"start\":51595},{\"end\":51611,\"start\":51602},{\"end\":51623,\"start\":51611},{\"end\":51808,\"start\":51799},{\"end\":51817,\"start\":51808},{\"end\":51832,\"start\":51817},{\"end\":52100,\"start\":52091},{\"end\":52111,\"start\":52100},{\"end\":52124,\"start\":52111},{\"end\":52132,\"start\":52124},{\"end\":52142,\"start\":52132},{\"end\":52154,\"start\":52142},{\"end\":52166,\"start\":52154},{\"end\":52435,\"start\":52426},{\"end\":52448,\"start\":52435},{\"end\":52458,\"start\":52448},{\"end\":52469,\"start\":52458},{\"end\":52759,\"start\":52750},{\"end\":52771,\"start\":52759},{\"end\":52781,\"start\":52771},{\"end\":52789,\"start\":52781},{\"end\":53088,\"start\":53077},{\"end\":53102,\"start\":53088},{\"end\":53112,\"start\":53102},{\"end\":53123,\"start\":53112},{\"end\":53133,\"start\":53123},{\"end\":53147,\"start\":53133},{\"end\":53431,\"start\":53419},{\"end\":53444,\"start\":53431},{\"end\":53457,\"start\":53444},{\"end\":53678,\"start\":53664},{\"end\":53690,\"start\":53678},{\"end\":53702,\"start\":53690},{\"end\":53716,\"start\":53702},{\"end\":53728,\"start\":53716},{\"end\":54042,\"start\":54029},{\"end\":54052,\"start\":54042},{\"end\":54066,\"start\":54052},{\"end\":54266,\"start\":54256},{\"end\":54487,\"start\":54478},{\"end\":54501,\"start\":54487},{\"end\":54516,\"start\":54501},{\"end\":54826,\"start\":54813},{\"end\":54838,\"start\":54826},{\"end\":55053,\"start\":55039},{\"end\":55064,\"start\":55053},{\"end\":55079,\"start\":55064},{\"end\":55086,\"start\":55079},{\"end\":55096,\"start\":55086},{\"end\":55109,\"start\":55096},{\"end\":55118,\"start\":55109},{\"end\":55131,\"start\":55118},{\"end\":55141,\"start\":55131},{\"end\":55153,\"start\":55141},{\"end\":55166,\"start\":55153},{\"end\":55177,\"start\":55166},{\"end\":55533,\"start\":55518},{\"end\":55547,\"start\":55533},{\"end\":55778,\"start\":55771},{\"end\":55786,\"start\":55778},{\"end\":55793,\"start\":55786},{\"end\":55799,\"start\":55793},{\"end\":56043,\"start\":56033},{\"end\":56057,\"start\":56043},{\"end\":56285,\"start\":56275},{\"end\":56296,\"start\":56285},{\"end\":56517,\"start\":56505},{\"end\":56533,\"start\":56517},{\"end\":56545,\"start\":56533},{\"end\":56558,\"start\":56545},{\"end\":56569,\"start\":56558},{\"end\":56578,\"start\":56569},{\"end\":56864,\"start\":56854},{\"end\":56876,\"start\":56864},{\"end\":57109,\"start\":57101},{\"end\":57119,\"start\":57109},{\"end\":57130,\"start\":57119},{\"end\":57145,\"start\":57130},{\"end\":57155,\"start\":57145},{\"end\":57436,\"start\":57425},{\"end\":57447,\"start\":57436},{\"end\":57459,\"start\":57447},{\"end\":57471,\"start\":57459},{\"end\":57481,\"start\":57471},{\"end\":57766,\"start\":57755},{\"end\":57780,\"start\":57766},{\"end\":57793,\"start\":57780},{\"end\":57803,\"start\":57793},{\"end\":58065,\"start\":58054},{\"end\":58076,\"start\":58065},{\"end\":58087,\"start\":58076},{\"end\":58100,\"start\":58087},{\"end\":58110,\"start\":58100},{\"end\":58121,\"start\":58110},{\"end\":58131,\"start\":58121},{\"end\":58141,\"start\":58131},{\"end\":58151,\"start\":58141},{\"end\":58486,\"start\":58478},{\"end\":58497,\"start\":58486},{\"end\":58511,\"start\":58497},{\"end\":58781,\"start\":58771},{\"end\":58790,\"start\":58781},{\"end\":58803,\"start\":58790},{\"end\":58813,\"start\":58803},{\"end\":59061,\"start\":59055},{\"end\":59073,\"start\":59061},{\"end\":59292,\"start\":59277},{\"end\":59490,\"start\":59481},{\"end\":59502,\"start\":59490},{\"end\":59511,\"start\":59502},{\"end\":59759,\"start\":59747},{\"end\":59768,\"start\":59759},{\"end\":59784,\"start\":59768},{\"end\":59793,\"start\":59784},{\"end\":60103,\"start\":60092},{\"end\":60382,\"start\":60371},{\"end\":60395,\"start\":60382},{\"end\":60705,\"start\":60694},{\"end\":60716,\"start\":60705},{\"end\":60725,\"start\":60716},{\"end\":60741,\"start\":60725},{\"end\":60751,\"start\":60741},{\"end\":61123,\"start\":61112},{\"end\":61137,\"start\":61123},{\"end\":61455,\"start\":61444},{\"end\":61888,\"start\":61874},{\"end\":61900,\"start\":61888},{\"end\":61912,\"start\":61900},{\"end\":62190,\"start\":62176},{\"end\":62200,\"start\":62190},{\"end\":62424,\"start\":62410},{\"end\":62434,\"start\":62424},{\"end\":62754,\"start\":62744},{\"end\":62763,\"start\":62754},{\"end\":63000,\"start\":62988},{\"end\":63013,\"start\":63000},{\"end\":63023,\"start\":63013},{\"end\":63036,\"start\":63023},{\"end\":63284,\"start\":63276},{\"end\":63292,\"start\":63284},{\"end\":63299,\"start\":63292},{\"end\":63307,\"start\":63299},{\"end\":63562,\"start\":63550},{\"end\":63572,\"start\":63562},{\"end\":63838,\"start\":63829},{\"end\":63848,\"start\":63838},{\"end\":63857,\"start\":63848},{\"end\":63868,\"start\":63857},{\"end\":63885,\"start\":63868},{\"end\":64166,\"start\":64158},{\"end\":64180,\"start\":64166},{\"end\":64402,\"start\":64393},{\"end\":64415,\"start\":64402},{\"end\":64429,\"start\":64415},{\"end\":64441,\"start\":64429},{\"end\":64450,\"start\":64441},{\"end\":64460,\"start\":64450},{\"end\":64725,\"start\":64715},{\"end\":64924,\"start\":64915},{\"end\":64937,\"start\":64924},{\"end\":64950,\"start\":64937}]", "bib_venue": "[{\"end\":50750,\"start\":50738},{\"end\":51038,\"start\":51027},{\"end\":51368,\"start\":51353},{\"end\":51653,\"start\":51642},{\"end\":52497,\"start\":52487},{\"end\":52813,\"start\":52805},{\"end\":53175,\"start\":53165},{\"end\":53483,\"start\":53474},{\"end\":55577,\"start\":55566},{\"end\":56085,\"start\":56075},{\"end\":56322,\"start\":56313},{\"end\":56606,\"start\":56596},{\"end\":56904,\"start\":56894},{\"end\":57183,\"start\":57173},{\"end\":57509,\"start\":57499},{\"end\":58181,\"start\":58170},{\"end\":58841,\"start\":58831},{\"end\":59099,\"start\":59090},{\"end\":59537,\"start\":59528},{\"end\":59835,\"start\":59818},{\"end\":60133,\"start\":60122},{\"end\":60779,\"start\":60769},{\"end\":61608,\"start\":61540},{\"end\":61946,\"start\":61933},{\"end\":63064,\"start\":63054},{\"end\":63333,\"start\":63324},{\"end\":63923,\"start\":63908},{\"end\":64208,\"start\":64198},{\"end\":64502,\"start\":64485},{\"end\":50736,\"start\":50718},{\"end\":51025,\"start\":51008},{\"end\":51351,\"start\":51330},{\"end\":51640,\"start\":51623},{\"end\":51893,\"start\":51832},{\"end\":52190,\"start\":52166},{\"end\":52485,\"start\":52469},{\"end\":52803,\"start\":52789},{\"end\":53163,\"start\":53147},{\"end\":53472,\"start\":53457},{\"end\":53783,\"start\":53728},{\"end\":54070,\"start\":54066},{\"end\":54254,\"start\":54196},{\"end\":54563,\"start\":54524},{\"end\":54851,\"start\":54838},{\"end\":55188,\"start\":55177},{\"end\":55564,\"start\":55547},{\"end\":55832,\"start\":55799},{\"end\":56073,\"start\":56057},{\"end\":56311,\"start\":56296},{\"end\":56594,\"start\":56578},{\"end\":56892,\"start\":56876},{\"end\":57171,\"start\":57155},{\"end\":57497,\"start\":57481},{\"end\":57826,\"start\":57803},{\"end\":58168,\"start\":58151},{\"end\":58534,\"start\":58511},{\"end\":58829,\"start\":58813},{\"end\":59088,\"start\":59073},{\"end\":59275,\"start\":59231},{\"end\":59526,\"start\":59511},{\"end\":59816,\"start\":59793},{\"end\":60120,\"start\":60103},{\"end\":60420,\"start\":60395},{\"end\":60767,\"start\":60751},{\"end\":61160,\"start\":61137},{\"end\":61538,\"start\":61455},{\"end\":61931,\"start\":61912},{\"end\":62243,\"start\":62200},{\"end\":62506,\"start\":62434},{\"end\":62789,\"start\":62763},{\"end\":63052,\"start\":63036},{\"end\":63322,\"start\":63307},{\"end\":63615,\"start\":63572},{\"end\":63906,\"start\":63885},{\"end\":64196,\"start\":64180},{\"end\":64483,\"start\":64460},{\"end\":64713,\"start\":64677},{\"end\":64978,\"start\":64950}]"}}}, "year": 2023, "month": 12, "day": 17}