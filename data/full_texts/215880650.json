{"id": 215880650, "updated": "2023-10-18 22:41:49.005", "metadata": {"title": "Reinforcement-Learning-Guided Source Code Summarization using Hierarchical Attention", "authors": "[{\"first\":\"Wenhua\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Yuqun\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yulei\",\"last\":\"Sui\",\"middle\":[]},{\"first\":\"Yao\",\"last\":\"Wan\",\"middle\":[]},{\"first\":\"Zhou\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Jian\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Philip\",\"last\":\"Yu\",\"middle\":[\"S.\"]},{\"first\":\"Guandong\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "IEEE Trans. Software Eng.", "journal": "IEEE Trans. Software Eng.", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": ",", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tse/WangZSWZWYX22", "doi": "10.1109/tse.2020.2979701"}}, "content": {"source": {"pdf_hash": "7f9ff8955d007ab654b2b750c63011419dda9949", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://opus.lib.uts.edu.au/bitstream/10453/139555/3/Binder1.pdf", "status": "GREEN"}}, "grobid": {"id": "c282df7e4a387ef97b213ebae42d96f667b48bfd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7f9ff8955d007ab654b2b750c63011419dda9949.txt", "contents": "\n\" Reinforcement-Learning-Guided Source Code Summarization using Hierarchical Attention\n\n\nWenhua Wang \nYuqun Zhang \nYulei Sui \nYao Wan \nZhou Zhao \nJian Wu \nPhilip S Yu \nGuandong Xu \n\" Reinforcement-Learning-Guided Source Code Summarization using Hierarchical Attention\n1Index Terms-Code summarizationhierarchical attentionreinforcement learning\nCode summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays \"attention\") to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22% to 45% in BLEU-1 and outperforms the state-of-the-art approaches by around 5% to 60% in terms of S-BLEU and C-BLEU.\n\nINTRODUCTION\n\nI N the life cycle of software development, nearly 90% of the effort is used for maintenance, and much of this effort is spent on understanding the maintenance task and related software source code via code documents [1]. In addition, it has been widely argued that code documents can benefit various software engineering techniques [2], [3], [4], [5], [6], [7], e.g., software testing [8], [9], [10], [11], [12], [13], [14], fault localization [15], [16], [17], program repair [18], [19], [20].\n\nThus, it is essential for documentation to provide a highlevel description of the task performed by code for software maintenance. Even though various techniques have been developed to facilitate programmers during software implementation and testing, documenting code with comments remains a labour-intensive task [21], [22], [23]. In fact, few real-world software projects can adequately document code to reduce future maintenance costs [24], [25]. It is even challenging and time-consuming for a novice programmer to write good comments for code. Typically, good comments should have the following characteristics: (1) Correctness. The comments should correctly clarify the intent of code. (2) Fluency. The comments should be fluent, so that they can be easily read and understood by maintainers. (3) Consistency. The comments should follow a standard style/format for better code reading. To this end, code summarization is proposed to comprehend code and automatically generate descriptions directly from code. Summarizing code can also be viewed as a form of document expansion, where a successful code summary can not only benefit the maintenance of source code [26], [27], but also be used to improve the performance of code search using natural language queries [28], [29] and code categorization [30].\n\nExisting Efforts. Recent research has made some progress towards automatic generation of natural language descriptions of software. As far as we know, most of the existing code summarization approaches learn the semantic representation of code based on statistical language models [26], [31], and then generate comments based on templates or rules [32]. With the development of deep learning, some neural translation models [27], [33], [34] have also been introduced for code summarization, which mainly follow an encoderdecoder framework. They generally employ recurrent neural networks (RNNs) [35] to encode the code snippets into a hidden space and utilize another RNN to decode that hidden space to coherent sentences. Given the predecessor words and the ground truth, these models are typically trained to maximize the likelihood of subsequent words.\n\nLimitations and Insights. Based on our observation, the existing approaches suffer from the following three limitations: (1) Most of the existing approaches input code as plain texts that are composed of tokens (i.e., variables, operations, etc.) directly without considering the code hierarchy (e.g., tokens forming a statement and statements forming a function) which can provide more comprehensive representation by differentiating tokens under different contexts for comment generation. (2) Most of the existing approaches [26], [33], [36] utilize simple sequential features, such as token sequence, for code representation, while some code features (e.g., control flow graphs (CFGs), Abstract Syntax Tree (AST), and types of program variables) that can help capture the correlations between comments and programs remain unexplored. (3) The existing training approaches, also termed \"teacher-forcing\" models, suffer from the exposure bias issue which occurs as the ground truth is unavailable during testing stage and the previously generated words from the trained model are used to predict subsequent words [37] so that the model is only trained based on ground truth context and is not exposed to its own errors [38].\n\nOur Solutions. To tackle the aforementioned problems, we present a new hierarchical-attention-based learning approach by utilizing multiple structural code features (including control flow graph and AST) to reflect the code hierarchy, where a two-layer attention network (a token layer and a statement layer) is established for an effective code representation that differentiates tokens under different contexts for comment generation.\n\nIn our previous work [39], we proposed a deep reinforcement learning-based approach which draws on the insights of deep reinforcement learning to alleviate the exposure bias issue by integrating exploration and exploitation into the whole framework (addressing limitation (3)). It first encodes the structure and sequential content of code via an AST-based LSTM and a regular LSTM respectively. Next, the resulting code representation vector is fed into a deep reinforcement learning framework, namely actor-critic network. Instead of learning a sequential recurrent model to greedily look for the subsequent correct word, we utilize an actor network and a critic network to jointly determine the subsequent optimal word at each time step. In particular, the actor network provides the confidence of predicting the subsequent word according to the current state. The critic network, on the other hand, computes the reward values of all the possible extensions of the current state. As a result, our approach successfully collects the appropriate words that are less likely to be identified by using the actor network only. To learn these two networks more efficiently, our approach is initialized by pretraining an actor network using standard supervised learning with cross entropy loss, and pretraining a critic network with mean square loss. Accordingly, we update the actor and critic networks based on the advantage reward composed of BLEU metric via policy gradient.\n\nIn this paper, we further extend the previous approach to improve the efficacy by replacing the AST-based tree structure representation with type-augmented AST sequence and complementing the code representation with control flows (addressing limitation (2)). Moreover, we adopt the hierarchical attention network ((HAN) to encode sequence of different code representations (addressing limitation (1)). Specifically, first, in our previous work, AST is used to reflect the feature information of the code. However, constructing AST-based LSTM is time-consuming. Hence, we extract both unstructured and structured information (e.g., control flows and type-augmented AST) from code to efficiently represent code. The unstructured information is obtained directly by transforming code to plain text following the existing approach [27], while the structured information is represented by the type-augmented abstract syntax tree [40] sequence which is a syntactic code representation widely used in compilers and its corresponding control flow graph. Second, in our previous work, we transform code to be plain text that is composed of tokens directly, while ignoring the code hierarchy. Thus, we adopt the hierarchical attention network to encode a code sequence to effectively represent the code hierarchy by fusing different representations of the code into a low-dimensional and compact feature space. This hierarchical attention network assigns weights (pays \"attention\") to individual tokens and statements regarding different code representations.\n\nFramework Overview. Figure 1 gives an overview framework of our reinforcement-learning-guided comment generation approach via two-layer attention network, which includes an offline training stage and an online testing (summarization) stage. In the offline training stage, we prepare a large-scale corpus of annotated < code, comment > pairs. Specifically, first, we used three sequences: x T XT , x AST and x CF G , to represent code at both unstructured level (plain code sequence) and structured level (type-augmented ASTs and control flows) (Figure 1(a)); next, we use the hierarchical attention network (Figure 1(b)) to encode these three representations and integrate them. At last, the annotated pairs are injected into our proposed deep reinforcement learning model (Figure 1(c)) for training. Given the resulting trained actor network and a code snippet, its corresponding comment can be generated. Note that in this paper, we only deal with function-level summarization.\n\nContributions. The main contributions of this paper are as follows:\n\n\u2022 New idea. We propose a deep reinforcement learning framework-actor-critic network for comment generation which copes with the exposure bias issue existing in most traditional maximum likelihood-based approaches. \u2022 Extensive algorithms. This paper presents the first hierarchical-attention-based learning approach for code summarization by utilizing multiple code features (i.e., plain text, type-augmented AST and CFG) to reflect code hierarchy (tokens forming a statement, statements forming a function) by supporting a two-layer attention network at both token level and statement level to pro-  Figure 1: An overview framework of our proposed approach (HAN is the Hierarchical Attention Network).\n\u2026 v \u2026 v v 1 \u2026 v i \u2026 v m\nvide an effective code representation that differentiates tokens under different contexts for comment generation. \u2022 Evaluation. We validate our proposed approach on a real-world dataset of 108,726 Python code snippets used in our previous work and 20,000 more Python code snippets for the testing. Moreover, compared with the previous work, we implemented more existing approaches for performance comparison. The overall experimental results demonstrate that our approach outperforms the baselines in terms of comment generation accuracy (from around 22% to 45% in BLEU-1) and outperforms the state-of-the-art approaches by around 5% to 60% in terms of both Sentence-BLEU and Corpus-BLEU.\n\nThe remainder of this paper is organized as follows. Section 2 illustrates some preliminary background knowledge. An illustrative example is given in Section 3. The details of our proposed approach are elaborated in Section 4. Section 5 demonstrates the experimental results and analysis. Threats to validity are indicated in Section 6. Section 7 reviews the related work. We conclude the paper in Section 8.\n\n\nPRELIMINARIES\n\nIn this section, we first present some preliminary background knowledge about text generation, including language model, RNN encoder-decoder model, and the reinforcement learning for decoding. Firstly, we introduce some basic notations and terminologies. Let x = (x 1 , x 2 , . . . , x |x| ) denote the code sequence of one function, where x i represents a token of the code, e.g., \"def\", \"fact\", or \"i\" in a Python statement \"def fact(i):\". Let y = (y 1 , y 2 , . . . , y |y| ) denote a sequence of the generated comments, where | \u00b7 | denotes the sequence length. Let T denote the maximum step of decoding in the encoder-decoder framework. We use notation y l...m to represent y l , . . . , y m and D = {(x N , y N )} as the training dataset, where N is the size of training set.\n\n\nLanguage Model\n\nLanguage model computes occurrence probability of the words in a particular sequence [41]. The probability of a sequence including T words: y 1...T is denoted as p(y 1:T ) which is usually computed based on the conditional proba- bility from a window of n predecessor words, aka n-gram [42], as shown in Equation 1.\np(y 1:T ) = i=T i=1 p(y i |y 1:i\u22121 ) \u2248 i=T i=1 p(y i |y i\u2212(n\u22121):i\u22121 ) (1)\nSuch n-gram approach suffers from apparent limitations [43], [44]. For example, the n-gram model is derived only from the frequency counts and leads to inferior performance when confronted with the tokens that have not frequently appeared before.\n\nUnlike the n-gram model which predicts a word based on a fixed number of predecessor words, a neural language model can predict a word by predecessor words with longer distance. The associated neural network includes three layers, i.e., an input layer which maps each word x t to a vector, a recurrent hidden layer which recurrently computes and updates a hidden state h t after reading x t , and an output layer which estimates the probabilities of the subsequent words given the current hidden state. In particular, the neural network reads the words in the sentence one by one, and predicts the possible subsequent word at each time. At time t, it estimates the probability of the subsequent word p(y t+1 |y 1:t ) by the following steps: (1) the current word y t is mapped to a vector by the input layer; (2) it generates the hidden state (the values in the hidden layer) h t at time t according to the previous hidden state h t\u22121 and the current input x t : h t = f (h t\u22121 , w(x t )), where w refers to the parameters of the networks.\n\n(3) the p(y t+1 |y 1:t ) is predicted according to the current hidden state h t : p(y t+1 |y 1:t ) = g(h t ), where g is a stochastic output layer (e.g., a softmax for discrete outputs) that generates output tokens.\n\n\nRNN Encoder-Decoder Model\n\nRNN (Recurrent Neural Network) encoder-decoder, as shown in Figure 2, has two recurrent neural networks. The encoder transforms the code snippet x into a sequence of hidden states (h 1 , h 2 , . . . , h |x| ) with an RNN, while the decoder uses another RNN to generate one word y t at a time in the target space.\n\n\nEncoder\n\nA hidden state in an RNN encoder (Figure 2(a)) is a fixedlength vector. At the time t, the encoder computes the hidden state h t as shown in Equation 2.\nh t = f (h t\u22121 , w(x t ))(2)\nHere, h t\u22121 denotes the hidden state at last step, x t denotes the input at step t, and f is the hidden layer. The last symbol of x should be an end-of-sequence (< eos >) symbol which notifies the encoder to terminate and output the final hidden state h T , which is used as a vector representation of x 1:T . RNN has two shortcomings: gradient vanishing and gradient exploding which refer to the large decrease and increase in the norm of the gradient during training. To alleviate this problem, the long short-term Memory (LSTM) [45] technology with a gate mechanism is proposed to determine the information accumulation, where the input gate, forget gate and output gate control the input, forgt and output part of the entire network through weights and activation function. In this paper, we choose LSTM as our encoder. At time t, the hidden state is updated as follows,\ni t = \u03c3(W (i) x t + U (i) h t\u22121 + b (i) ) f t = \u03c3(W (f ) x t + U (f ) h t\u22121 + b (f ) ) a t = tanh(W (a) x t + U (a) h t\u22121 + b (a) ) c t = i t a t + f t c t\u22121 o t = \u03c3(W (o) x t + U (o) h t\u22121 + b (o) ) h t = o t tanh(c t )(3)\nwhere i t , f t , o t and a t denote an input gate, a forget gate, an output gate, and a intermediate parameter respectively for updating the memory cell c t . W (\u00b7) and U (\u00b7) are weight matrices, b (\u00b7) is a bias vector, and x t is the word embedding of the tth node. \u03c3(\u00b7) is the logistic function, and the operator denotes element-wise multiplication between vectors.\n\n\nDecoder\n\nThe output of the decoder (Figure 2(b)) is the target sequence y = (y 1 , \u00b7 \u00b7 \u00b7 , y T ). The decoder is initialized to input a < start > symbol denoting the beginning of the target sequence. At time t, the decoder computes the conditional distribution of the subsequent symbol y t+1 based on the hidden state h t : p(y t+1 |y t ) = g(h t ), where g is a stochastic output layer.\n\n\nTraining Goal\n\nThe encoder and decoder networks are jointly trained to maximize the following objective,\nmax \u03b8 L(\u03b8) = max \u03b8 E (x,y)\u223cD log p(y|x; \u03b8)(4)\nwhere \u03b8 is the set of the model parameters. We can see that this classical encoder-decoder framework targets on maximizing the likelihood of ground-truth word conditioned on previously generated words. As we have mentioned above, the maximum-likelihood-based encoder-decoder framework suffers from the exposure bias issue. Accordingly, we introduce the reinforcement learning technique for better decoding.\n\n\nReinforcement Learning for Better Decoding\n\nReinforcement learning [46] interacts with the environment and learns the optimal policy from the reward signal, which can potentially solve the exposure bias problem introduced by the maximum likelihood approaches which is used to train the RNN model. Specifically in the inference stage, a typical RNN model generates a sequence iteratively and predicts next token conditioned on its previously predicted ones that may never be observed in the training data [47]. Such a discrepancy between training and inference can become cumulative along with the sequence and thus prominent as the length of sequence increases. While in the reinforcement-learning-based framework, the reward, other than the probability of the generated sequence, is calculated to give feedback to train the model to alleviate such exposure bias problem. Accordingly, the text generation process can be viewed as a Markov Decision Process (MDP) {S, A, P, R, \u03b3}.\n\nIn the MDP setting, state s t at time t consists of the code snippets x and the predicted words y 0 , y 1 , . . . , y t . The action space is defined as the dictionary Y where the words are drawn, i.e., y t \u2282 Y. Correspondingly, the state transition function P is defined as s t+1 = {s t , y t }, where the action (word) y t becomes a part of the subsequent state s t+1 and the reward r t+1 is received. The objective of the generation process is to find a policy that maximizes the expected reward of the generated sentence sampled from the model's policy, as shown in Equation 5,\nmax \u03b8 L(\u03b8) = max \u03b8 E x\u223cD y\u223cP \u03b8 (\u00b7|x) [R(\u0177, x)](5)\nwhere \u03b8 is the policy parameter needed to be learnt, D is the training set,\u0177 is the predicted actions/words, and R is the reward function. Our problem can be formulated as follows:\n\n\u2022 Given a code snippet x = (x 1 , x 2 , . . . , x |x| ), our goal is to find a policy that generates a sequence of words y = (y 1 , y 2 , . . . , y |y| ) from dictionary Y with the objective of maximizing the expected reward.\n\nTo learn the policy, many approaches have been proposed, which are mainly categorized into two classes [48]: (1) the policy-based approaches (e.g., Policy gradients [49]) which optimize the policy directly via policy gradient and (2) the value-based approaches (e.g., Q-learning [50]) which learn the Q-function, and in each time the agent selects the action with the highest Q-value. It has been verified that the policybased approaches may suffer from a variance issue and the value-based approaches may suffer from a bias issue [51]. To combine the advantages of both policy-and value-based approaches, the Actor-Critic learning approach is proposed [52]. In particular, the actor chooses an action according to the probability of each action and the critic assigns the value to the chosen action, which speeds up the learning process for the original policy-based approaches.\n\n\nILLUSTRATIVE EXAMPLE\n\nIn this section, we use a Python code snippet as our illustrative example. Figure 3(a) shows a simple Python code example which aims to obtain the factorial of an integer via a recursive function fact.  Figure 3(c) shows its inter-procedural control flow graph which represents program execution order. The ideal comments (green) of this code is given in Figure 3(a). It can be indicated that the semantics of the three highlighted words can be precisely captured by different code representations, # Get the factorial of an integer by multiplying all integer from 1 to it via the recursive method.  e.g., plain text (for multiplying), type-augmented AST (for integer) and CFG (for recursive).\n1.def fact(i): 2. if i == 0: 3. return 1 4. else: 5. return i*fact(i-1)\nThe order of tokens and statements can vary depending on different code representations. In this paper, we use the three unstructured and structured information of code, i.e., plain text, AST and CFG. For example, based on plain text, the token after \"if\" in Figure 3 (a) is \"i\" followed by \"==\". Based on the AST sequence represented as: {stmt = FunctionDef(identifier fact, arguments i, stmt body); body = IfExp(expr test, expr body, expr orelse); ...}, there are three tokens (test, body and orelse) following \"IfExp\" with \"=\" following \"test\", \"return\" following \"body\", and \"orelse\", as shown in Figure 3 (b). After the token \"1\" in the last statement at line 5, there is no token left according to plain text. However, based on CFG, the subsequent token is \"def\" at the beginning of fact due to the recursive function call.\n\nFrom Figure 3(a), we can observe that tokens \"def\", \"fact\", and \"i\" form statement 1, while statements 1 to 5 form the entire function. Such hierarchy is captured by a twolayer attention network (including one token layer and one statement layer), as shown in Figure 4, where the bottom layer encodes each token x it of statement s i to produce a vector of this statement, which is later injected into the toplayer attention network along with the other statements to obtain a final vector to represent their associated function. Note that in Figure 4, \u03b1 i and \u03b1 it represent the weights of the ith statement and the tth token of statement s i respectively, which are inferred during reinforcement learning.\n\nBy utilizing the three code representations and the hierarchical attention network, our approach produces three different vectors with different token and statement sequences. Finally, the three vectors are concatenated to produce the final code representation for precisely capturing the relations between tokens and relations between statements.\n\n\nTHE DRL-GUIDED CODE SUMMARIZATION VIA HIERARCHICAL ATTENTION NETWORK\n\nIn this section, we introduce the details of our proposed DRL (Deep Reinforcement Learning)-guided code summarization approach via hierarchical attention network with its architecture shown in Figure 5. Our approach follows the actor-critic framework [53], which has been successfully adopted in the decision-making scenarios such as AlphaGo [54]. Specifically, we split the framework into four submodules: (a) code representations which are used to explain the unstructured and structural information of a program; (b) hybrid hierarchical attention network which is used to encode the code representations into vectors in the hidden space; (c) text generation which is a LSTM-based generation network to generate the subsequent words based on predecessor words; and (d) the critic network which is used to evaluate the quality of the generated word.\n\n\nSource Code Representations\n\nFor the identifiers in source code, we tokenize and split them by a set of symbols, i.e., {. , \" ' ( ) { } : ! -(space) }. Next, all the resulting tokens are changed to lowercase letter. Furthermore, we embed all the obtained tokens to vectors by Word2Vec() provided by Python library genism [55], where similar to [27], [56], [57], the undefined tokens are dealt as the unknown words. respectively, followed by a hybrid layer which is used to integrate these three vectors; (c) the LSTM-based decoder is used to generate the summary of the code; (d) Given a state s t , the critic network evaluates its value (baseline) with the advantage defined as |baseline \u2212 reward|.\n\nBased on the resulting tokens, we introduce the following three code representations: plain text, type-augmented abstract syntax tree, and control flow graph.\n\n\nPlain Text\n\nThe key insight into the lexical level representation of code is that comments are always extracted from the lexical items of source code, such as the function name, variable name and so on.\n\n\nType-augmented Abstract Syntax Tree\n\nWhen executing a program, a compiler decomposes a program into constituents and produces intermediate code according to the syntax of the programming language, such as AST [58]. In this paper, first, we obtain the AST sequence by the ast module [59] of Python as one syntactic-level representation of the code. Next, to augment the derived AST sequence with additional type information, we propose to abstract the type information of the tokens and integrate them with the AST sequence of the code. For example, in Figure 3 (a), line 2 is represented as \"if integer i == integer 1\" by annotating \"integer\" type to variable \"1\".\n\n\nControl Flow Graph\n\nSince different code representations reflect different latent code features, we extract the control flow graph (CFG), which is another type of intermediate code often used in compiler, as another syntactic level representation of the code. In particular, each node on CFG represents a statement consisting of a sequence of tokens and each edge connecting two nodes denotes the program's control flow. The control flow graph is obtained by following the ast module [59] and [60] to and then traverse the obtained graph in depth-first order to obtain the control flow sequence .\n\n\nHybrid Hierarchical Attention Network\n\nEach code part makes its own contribution to the final output of comments. Specifically, first, the importance of tokens and statements are highly context dependent, i.e., the same token or statement may be differentially important in different context. Next, code essentially has a hierarchy (tokens forming statements and statements forming functions). Therefore, the hierarchical attention network [61], which has been successfully used in natural language processing, is applied to allow the approach to assign weights (pay \"attention\") to individual tokens and statements respectively when constructing the code representations. Attention not only often results in better performance, but also provides insights into the correlations between tokens/statements and the corresponding summary, which benefits in generating high-quality comments [62], [63].\n\nIn this paper, we apply a two-layer attention network (a token layer and a statement layer), as shown in Figure 5 (b). This network consists of four parts: a token sequence encoder, a token-level attention layer, a statement encoder, and a statement-level attention layer. Assuming d T XT , d AST , and d CF G are the vectors deducted by encoding the three code representations i.e., plain text, AST, and CFG. As a result, they are merged into one hybrid vector d to represent code. The details of such network are demonstrated as follows.\n\nToken Encoder. Given a statement s i with tokens x i0 , ..., x iTi\u22121 , where T i is the total number of tokens in s i , we first embed all the tokens to vectors through an embedding matrix W i , i.e., v it = W i x it . Next, we use an LSTM to obtain token annotations by reading statement s i from x i0 to x iTi\u22121 , as shown in Equation 6.\nv it = W i x it , t \u2208 [0, T i ) h it = lstm(v it ), t \u2208 [0, T i )(6)\nToken Attention. Not all tokens contribute equally to the semantic representation of the statement. For example, in Figure 6, tokens \"number\" and \"str\" are essentially more important than tokens \"def\" in statement \"def check number exist(str):\" because there are words \"numbers\" and \"string\" in the comment of this code snippet. Hence, we introduce the attention mechanism to extract the tokens that are more important to the semantics of a statement and aggregate the representation of those informative tokens to form a statement vector as shown in Equation 7.\n\n# Check if there are numbers in a string. return has_number Figure 6: Code example of different tokens contributing differently for comment generation.\nu it = tanh(W x h it + b x ) \u03b1 it = exp(u T it u x ) \u03a3 T exp(u T it u x ) s i = \u03a3 T \u03b1 it h it(7)\nHere, W x is the weight matrix, b x is a bias vector, \u03b1 it denotes the contribution (attention) of token x it to statement s i , and u x is the token-level context vector which is used for the high-level representation of each statement in terms of tokens. In particular, u x is randomly initialized and gradually learned during the training process.\n\nStatement Encoder. Given the statement vector s i , we can obtain a function vector in a similar manner to tokens. We use an LSTM to encode the statements as follows.\nh i = lstm(s i ), i \u2208 [0, L)(8)\nHere, L is the total number of the statements included in one function. Statement Attention. To reward the statements that are more semantically important to the associated function for the summarization task, we again use attention network and introduce a statement level function vector u s which is used to measure the importance of the statement as follows.\nu i = tanh(W s h i + b s ) \u03b1 i = exp(u T i u s ) \u03a3 L exp(u T i u s ) d c = \u03a3 L \u03b1 i h i(9)\nHere, W s is the weight matrix, b s is a bias vector, and \u03b1 i denotes the contribution (attention) of statement s i to the final vector d c .\n\nHybrid Representation of Source Code. To integrate the structural context vector (i.e., AST and CFG representations) and the unstructural textual vector (i.e., plain text representation), we concatenate them firstly and later feed them into a one-layer linear network:\nd = W d [d T XT ; d AST ; d CF G ] + b d , where d is the hybrid repre- sentation of code, [d T XT ; d AST ; d CF G ]\nis the concatenation of d T XT , d AST , and d CF G , and b d is a bias vector. The context vector is then used for word prediction by placing an additional hidden layer: s t = tanh(W c s t + b d ), where s t is hidden state of the encoding process. To be specific, initially in the decoding process, s 0 is assigned to be d. Accordingly, the state s t is updated at decoding step t.\n\n\nText Generation\n\nAfter obtaining the representation of code snippet from the hierarchical attention network and the hybrid layer, we decode it into a natural language comments. Here we describe how we generate a comment from the hidden space.\n\nFor the decoding, since we design a hierarchical and multi-dimensional input, we decide to adopt the Inputfeeding attention mechanism [64] to predict the tth word by using a softmax function. Let p \u03c0 denote a policy \u03c0 determined by the actor network, p \u03c0 (y t |s t ) denote the probability distribution of the tth word y t , we can obtain the following equation:\np \u03c0 (y t |s t ) = sof tmax(W s s t + b s )(10)\n\nCritic Network\n\nUnlike traditional encoder-decoder frameworks [65] that generate comments directly via maximizing likelihood of subsequent words given the ground truth word, we generate comments by iteratively optimizing the evaluation metrics e.g., BLEU [66], in a reinforcement learning manner. Specifically, we apply a critic network to approximate the value of a generated action at time t to issue a feedback to tune the network iteratively. Different from the actor network, this critic network outputs a single value instead of a probability distribution on each decoding step.\n\nTo illustrate, given the generated comments and the reward function r, the value function V is defined to predict the total reward from the state s t at time t, which is formulated as follows, V (s t ) = Es t+1:T ,\ny t:T T \u2212t l=0 r t+l |y t+1 , \u00b7 \u00b7 \u00b7 , y T , h(11)\nwhere T is the max step of decoding and h is the representation of code snippet. By applying the reward function, we can obtain an evaluation score (e.g., BLEU) when the generation process of the comment sequences is completed. Such process is terminated when the associated step exceeds the max-step T or generates the end-of-sequence (EOS) token. For instance, a BLEU-based reward function can be calculated as\nr = exp( 1 N * N i=1 logp n )(12)\nwhere p n = n\u2212gram c count(n\u2212gram) n\u2212gram c count(n\u2212gram ) , c is the generated comment and c is the ground truth.\n\n\nModel Training\n\nFor actor network, the training objective is to minimize the negative expected reward, which can be defined as L(\u03b8) = \u2212E y 1,...,T \u223c\u03c0 ( T l=t r t ), where \u03b8 is the parameter of the actor network. Defining policy as the probability of a generated comment, we adopt the policy gradient approach to optimize the policy directly, which is widely used in reinforcement learning.\n\nThe critic network attempts to minimize the following loss function, Algorithm 1 Actor-Critic training for code summarization.\n\n1: Initialize actor network p(y t+1|st ) and critic network V (s t ) with random weights \u03b8 and \u03c6; 2: Pre-train the actor network to predict ground truth y t given {y 1 , \u00b7 \u00b7 \u00b7 , y t\u22121 } by Eq. 4; 3: Pre-train the critic network to estimate V (s t ) with fixed actor network; 4: for t = 1 \u2192 T do 5: Receive a random example, and generate the comment sequence {y 1 , \u00b7 \u00b7 \u00b7 , y T } according to current actor network; 6: Calculate the reward value according to Eq. 12; 7: Update critic network weights \u03c6; 8: Update actor network weights \u03b8;\nL(\u03c6) = 1 2 V (s t ) \u2212 V \u03c6 (s t ) 2(13)\nwhere V (s t ) is the target value (calculated by the value function based on the ground truth), V \u03c6 (s t ) is the value predicted by the critic network based on the generated comments with its parameter set \u03c6. Eventually, the training for code summarization is completed after L(\u03c6) converges. Denoting all the parameters as \u0398 = {\u03b8, \u03c6}, the total loss of our model can be represented as L(\u0398) = L(\u03b8) + L(\u03c6). We employ stochastic gradient descend with the diagonal variant of AdaGrad [67] to tune the parameters for optimizing the code summarization model.\n\n\nEXPERIMENTS AND ANALYSIS\n\nThe goal of our evaluation is to show that our reinforcementlearning-guided approach using hierarchical attention network is more effective in generating comments than baselines (e.g., without hierarchical attention network and/or without reinforcement-learning) and state-of-the-art approaches. Our evaluation focus on the following research questions.\n\n\u2022 RQ1. What is the effectiveness of our approach in generating comments and what are the results under different configuration settings? \u2022 RQ2. What is the time consumption and performance trend regarding the increment of training epochs during model training? \u2022 RQ3. What is the performance of our proposed approach on the datasets with different code or comment lengths? \u2022 RQ4. How does our approach perform compared with other existing code summarization approaches? \u2022 RQ5. How do we extensively evaluate our approach other than only relying on NLP-specific metrics?\n\n\nDataset Preparation\n\nTo evaluate the performance of our proposed approach, we use the Python dataset used in our previous work [39], which is obtained from a popular open source project hosting platform GitHub [68] and processed by Barone et. al [69]. In particular, we remove the low-quality <code, comment> pairs, e.g., comments with massive misspelling words, broken sentences, from the original dataset. As a result, the derived dataset consists of around 108K pairs.\n\nIn addition to our previous work [39], we also collect another 20K testing pairs to evaluate how our approach and baselines perform in diverse datasets to evaluate their universal applicability. In particular, to ensure that the extended testing data do not overlap the original training dataset, we first collect the projects with 80 to 100 stars for deriving the extended testing dataset while the original training dataset are made by the projects with more than 100 stars. Next, we sort the collected projects by the their number of forks and select the top 20k pairs accordingly. Eventually, our Python dataset contains 128K code-comment pairs in total, where the vocabulary size of code and comment is 50,400 and 31,350, respectively. Similar to [27], [56], we shuffle the original dataset and use the first 80% for training and validation and the remaining 20% for testing. Moreover, we use 10-fold cross-validation to evaluate the performance of their proposed approach. Specifically, we split the training and validation data into 10-fold, where each time we utilize 10% of the data for validation and the rest 90% for training. Then we average the testing results out of the 10 times of execution.\n\nWe also adopt the Java project dataset in [70] to evaluate the cross-language performance of our approach. Specially, we select the same number of training data, validation data and testing date as our python dataset from the original dataset in [70] in a top-down manner.\n\nWe have conducted statistics analysis for the source code and comment out of our adopted Python dataset based on massive GitHub projects as shown in Figures 7 and 8. Figure 7 shows the length distributions of code and comment. From Figure 7 (a), we can find that the lengths of most code snippets are located between 10 to 80 tokens. From Figure  7(b), we can notice that the length of nearly all the comments are between 5 and 40. This reveals that the comment sequence to be generated is not too long. Moreover, Figure 8 shows the token number and statement number distribution in the collected code snippets of our dataset where Figure 8 (a) shows the token number distribution in each statement and Figure 8 (b) shows the statement number distribution in each function. From this figure, we can observe that the token number in each statement mainly ranges from 1 to 15, and the statement number in each function mainly ranges from 2 to 25.\n\n\nEvaluation Metrics\n\nWe evaluate the performance of our proposed approach based on three widely-used evaluation metrics in the area of NLP, especially for the text generation task, i.e., BLEU [66], METEOR [71] and ROUGE-L [72]. Since code summarization is a special type of text generation with natural language as the output, we utilize these evaluation metrics to evaluate the quality of the generated comments.\n\nBLEU is the most common metric adopted in text generation [73], [74], [75], [76], [77], [78], [79] which measures the average n-gram precision on a set of reference sentences, with a penalty for short sentences. BLEU is calculated as:     where p n = n\u2212gram c count(n\u2212gram) n\u2212gram c count(n\u2212gram ) , c is the generated comment and c is the ground truth. In this paper, we adopt the BLEU-N metrics as in our previous paper. Moreover, we extend our adoption of BLEU metrics by including both the sentence-level BLEU (S-BLEU) and corpus-level BLEU (C-BLEU) for the performance comparison between our approach and state-of-the-art approaches. In particular, S-BLEU calculates the BLEU score between each generated comment and the ground truth and then calculates the average of all the scores. For S-BLEU, we adopt the addk smoothing, for which we define k as 1e \u2212 15 such that the count of n-gram cannot be 0. C-BLEU, on the other hand, computes the BLEU score in the corpus level.\nBLEU = exp( 1 N * N i=1 logp n ),(14)\nMETEOR is a recall-oriented metric which evaluates how well the results capture content from the references via computing recall by stemming and synonymy matching. It is computed as:\nM ET EOR = (1 \u2212 P en)F mean ,(15)\nwhere P en = \u03b3( ch m ) \u03b8 and F mean = PmRm \u03b1Pm+(1\u2212\u03b1)Rm , where \u03b3, \u03b8 and \u03b1 are parameters, ch is the number of tokens, m means the matched tokens number, P m represents unigram precision which is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the system translation, and R m describes unigram recall which is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the reference translation.\n\nROUGE-L takes into account sentence-level structure similarity naturally and identifies longest co-occurrence in sequence n-grams automatically. It is calculated as:\nROU GE \u2212 L = S\u2208c g ram l \u2208 SCount match (gram l ) S\u2208c g ram l \u2208 SCount(gram l ) ,(16)\nwhere l means the number of tokens, Count match (gram l ) computes the maximum number of matched n-grams in the generated comment.\n\n\nTraining Details\n\nThe size of all the hidden layers of both the encoder and decoder LSTM networks are set to be 512, and the word embedding size is also set to be 512. The mini-batch size is set to be 32, while the learning rate is set to be 0.001. We pretrain both actor network and critic network with 10 epochs each, and train the actor-critic network with 10 epochs simultaneously. We record the perplexity [37]/reward every 50 iterations. All the experiments in this paper are implemented with Python 3.6, and run on a computer with a 2.8 GHz Intel Core i7 CPU, 64 GB 1600 MHz DDR3 RAM, and a Titan X GPU with 16 GB memory, running RHEL 7.5.   \n\n\nRQ1: The Effectiveness Analysis with Different Baselines\n\n\nEffectiveness of Code Representations\n\nWe evaluate our approach by comparing with different baseline settings (addressing limitations 2). Here TXT, AST and CFG refer to the three code representations, i.e., plain text, AST, and CFG, respectively. HAN refers to hierarchical attention network and DRL denotes deep reinforcement learning.\n\n\u2022 TXT+HAN+DRL. This baseline transforms code to be plain text and uses a LSTM-based hierarchical attention network to encode the code into a hidden space, and DRL to train the model. This approach takes the plain text and the AST as the input of the LSTM-based attention network respectively. The encoded hidden vectors of them are concatenated into one hidden vector by a hybrid layer, and DRL is used to train the model. \u2022 Our approach: TXT&AST&CFG+HAN+DRL. Our proposed approach in this paper takes the plain text, the type-augmented AST sequence, and the control flow of code as the inputs of the LSTM-based hierarchical network encoder respectively. The three encoded hidden vectors are concatenated into one hidden vector by a hybrid layer. Moreover, we also apply another 20,000 subjects as our new testing dataset for an additional experiment to evaluate the robustness of our proposed approach under various projects. Table 1 shows the experimental result comparisons between our proposed approach and the aforementioned baselines. From this table, we can observe that our proposed approach outperforms other baselines in almost all of the evaluation metrics. Compared to the baselines which use only plain text, AST or control flow, the hybrid representation of code which uses two code representations can improve the comment generation performance by 29.46% to 31.42% for the evaluation metric BLEU-1. Our proposed approach which uses three code representations outperforms the approaches which use two code representations by 16.58% to 20.53% for BLEU-1. Compared to our previous work [39], our extended approach can improve the accuracy by about 23.79% for BLEU-1. The approaches which use two code representations outperform our previous work by 4.27% to 9.26% for BLEU-1. In addition, the testing results by our new collected data can achieve almost the same accuracy compared with the original testing dataset performance. Moreover, the results in terms of the other evaluation metrics reflect the same trends. To conclude, our approach can achieve better accuracy because of the stronger code representations and the finer-grained HAN that reflect more accurate semantic for high-quality comment generation.\n\n\nEffectiveness of Hierarchical Attention Mechanism\n\nTo evaluate the effectiveness of hierarchical attention network (addressing limitations 1), we encode the code without attention network, with 1-layer attention network and with 2-layer attention network (our approach) respectively. The no attention approach encodes the input by LSTM without any attention mechanism. The 1-layer attention approach encodes the representation of the code from tokens to function directly with attention network. Our proposed 2-layer attention approach encodes the representation of the code with 2layer attention network which considers code hierarchy-the tokens form statements and the statements construct the functions of code. Table 2 shows the effectiveness of the hierarchical attention network. From this table, we can know that the performance of the approach with 1-layer attention network is better than the approach without any attention mechanism by 2.92% to 37.47% for different evaluation metrics. Our proposed approach (2-layer attention network) outperforms the approach with 1-layer attention network by 4.36% to 49.59% for different evaluation metrics. These results show that our proposed hierarchical attention network for code representation makes significant contributions to accurate comment generation.\n\n\nEffectiveness of Deep Reinforcement Learning\n\nTo validate the effectiveness of the deep reinforcement learning component (addressing limitation 3) in our proposed approach, we train the model both with and without deep reinforcement learning component respectively, denoted as \"approach with DRL\" and \"approach without DRL\" in Table  3. From this table, we can observe that the performance of the approach with DRL outperforms the approach without DRL by 14.13% to 130.30% for different evaluation metrics. These results show that our proposed deep reinforcement learning model can significantly boost the performance of comment generation.\n\n\nPerformance Evaluation under different dataset settings\n\nIn this section, we investigate the performance of our approach under different dataset settings, i.e., cross-project and different dataset split.\n\nTo evaluate the code summarization performance of our proposed approach on cross-project dataset, we split the dataset based on their projects. In particular, we adopt 84043 pairs for training, 8689 pairs for validation, and 14390 pairs for testing from different projects. The result can be found in Table 4, from which we can observe that the results are worse compared with the randomly split dataset. To illustrate, in our approach, the generated natural language words (comments) are extracted from the collected dataset. When we conduct the cross-project experiments, it is likely that the training, validation, and testing dataset might contain different words since they are extracted from cross projects. Therefore, the generated comments are expected to be less similar with the original comments due to the discrepancy of the dictionaries among such datasets. On the other hand, it can be derived that it is essential to expand the dataset scope for improving the performance of our approach.\n\nTo investigate how our approach performs under different data split policy, we select 80% data for training, 10% data for validation, and the rest 10% for testing. The result can found in Table 4, from which we can observe that the results are close to our original data split policy. This result can validate the robustness of our approach.\n\n\nRQ2: Time Consumption and Performance Trend with Different Training Epochs\n\nWe record the average training time for each epoch of this approach with different code representations as shown in Table 5. In this table, pretraining the actor network takes the first 10 epochs, pretraining the critic network takes the second 10 epochs, and training the actor-critic network takes the last 10 epochs simultaneously. From the result, we can observe that the training time of each epoch for all the three stages in our approach is less than 1 hour which is reasonable in real world. Figure 9 shows the performance trend with the increment of the total training epoch number from 5 to 45. From this figure we can know that the performance increase from 22.98% to 34.53% in BLEU-1 with the increment of the training epochs from 5 to 45. We can also see that all the results have an approximated upward trend with the increment of the training epochs from 5 to 30, and then the performance tends to be stable. Therefore, we choose 30 as the total training epoch number in this paper.\n\n\nRQ3: Performance of Different Code and Comment Length\n\nWe vary both the code and comment lengths to evaluate the effects on the representation of code and comment generation from them. Figures 10 and 11 show the performance of our proposed approach when compared with the baselines on the datasets of varying code length and comment length, respectively.\n\nFrom Figure 10, we can observe that our approach performs the best when compared with other baselines on four evaluated metrics with respect to different code lengths. For BLEU-1, our approach outperforms the baselines with different code representations by 35.74%, 43.31%, 41.13%, 17.04%, 1l6.97%, and 12.66% respectively when the code length is 40. For all the evaluation metrics, the approaches which use two features for code representation, i.e., TXT&AST, TXT&CFG and AST&CFG, can always outperform the ones which use only one feature for code representation. Our approach which   uses all the three features for code representation can outperform the ones with two features for code representation. Moreover, compared to our previous work, the extended approach can improve the comment generation accuracy by about 20.36% for BLEU-1 when the code length is 40.\n\nThe results in terms of the other evaluation metrics reflect about the same trends. This further validates the effectiveness of the multi-feature code representation. Additionally, our proposed hybrid representation performs consistently under the code length ranging within (10,80). Figure 11 demonstrates the performance under different comment lengths. We can clearly observe that our approach has better performance compared with almost all the baselines under different comment lengths. For instance, for BLEU-1, our approach outperforms the baselines by 107.61%, 100.31%, 200.59%, 51.77%, 42.64%, and 47.77% respectively when the comment length is 20. Moreover, compared to our previous work, the extended approach can improve the comment generation accuracy by about 76.52% for BLEU-1 when the comment length is 10. From the figure we can also know that the performance becomes worse when increasing the comment length which analogizes the discoveries from the research of neural translation [78], [80].\n\n\nRQ4: Performance Comparison with State-of-theart approaches\n\nTo evaluate the code summarization performance of our proposed approach, we also select several state-of-the-art approaches, i.e., DeepCom [70], CODENN [27], Code2seq [56], and CoaCor [57] for performance comparison. In particular, DeepCom [70] utilizes the AST sequence converted by traversing the AST as the code representation  and inputs the AST sequence to the GRU-based NMT for code summarization via combining lexical and structure information. CODENN [27] uses RNN with an attention mechanism to produce comments for C# code snippets and SQL queries. Code2seq [56] represents code snippet as the set of compositional paths in its AST and uses attention to select the relevant paths while decoding. CoaCor [57] utilizes the plain text of source code and an LSTM-based encoderdecoder framework for code summarization. Apart from the dataset of Python in our previous paper, we also utilize the Java dataset applied in [70]. We evaluate the performance of all the approaches based on the aforementationed evaluation metrics (especially for BLEU, we adopt S-BLEU and C-BLEU) in terms of both Python and Java dataset. Table 6 demonstrates the code summarization results of all the approaches in terms of the selected metrics. From the results we can observe that for both the Python dataset and the Java dataset, our approach can outperform the compared approaches in terms of most evaluation metrics. Specially, in terms of the results based on the Python dataset, state-of-the-art approaches can achieve from 12.92% to 25.61% in terms of S-BLEU and from 11.36% to 23.67% in terms of C-BLEU, while our approach can achieve 33.16% and 30.58% respectively. Our approach can outperform all the compared approaches by 22.77% to 61.04% in terms of S-BLEU and by 22.60% to 62.85% in terms of C-BLEU. Such performance advantages can indicate the superiority of the our approach. Moreover, in terms of the result based on Java dataset, state-of-the-art approaches can achieve from 19.96% to 36.51% in terms of S-BLEU and from 18.07% to 34.31% in terms of C-BLEU, while our approach can achieve 38.25% and 36.42% respectively. Our approach can outperform all the compared approaches by 4.5% to 47.82% in terms of S-BLEU and by 5.79% to 50.38% in terms of C-BLEU. From the evaluation results based on both the Python and the Java projects, we can summarize that our approach can outperform multiple state-of-the-art approaches.\n\n\nRQ5: Case Study and User Study\n\nTo extensively evaluate our approach, we also conduct case study and user study which are illustrated as follows.\n\n\nCase study\n\nWe demonstrate four real-world code examples for generating their comments using our approach in Table 7. In this table, we first show the code snippet in the second line and then give the ground truth comment which is the code comment that is collected together with the code snippet from GitHub. Next, the generated comments by different approaches are given. For our approach, shown as 2-layer+DRL, we have highlighted the words that are closer to the ground truth. It can be observed that the generated comments by our approach are the closest to the ground truth. Although the approaches with DRL (1-layer+DRL) can generate some tokens which are also in the ground truth, they cannot predict those tokens which do not frequently appear in the training data, i.e., pillar in Case 3. On the contrary, the deep-reinforcement-learning-based approach can generate some tokens which are closer to the ground truth, such as process, remove, subunit. This can be illustrated by the fact that our approach has a more comprehensive exploration on the word space and optimizes the BLEU score directly. \n\n\nUser study\n\nSimilar to CODENN [27], we conduct a user study to measure the output of our code summarization approach and baselines across two modalities-naturalness and informativeness. In particular, we invite 5 proficient English speakers and 5 proficient programmers with expertise in Java and Python to rate the generated comments in terms of grammaticality and fluency, on a scale between 1 and 5. First, We choose the generated comments which rank top 50 in terms of S-BLEU by our approach in both the Java and Python dataset. Furthermore, we identify their associated code snippets/original comments and their corresponding comments generated by other approaches. At last, each user randomly selects 10 out of the selected 50 comments and score them based on their respective understanding of the naturalness/informativeness of all the generated comments.\n\nThe results are presented in Table 8, which demonstrates that our approach can obtain higher score compared with other baselines in both naturalness (4.35 over 3.41 to 4.18 on average) and informativeness (3.27 over 2.39 to 3.05 on average). Such results turn out to reflect the metric-based evaluation results on the fluency and consistency of our approach vs. other approaches.\n\n\nTHREATS TO VALIDITY\n\nOne threat to validity is that our approach is experimented only on Python and Java code collected from GitHub, so they may not be representative of comments generation all projects using that programming language. However, as the components HAN and DRL in our approach are general approaches which can also be used for comment generation of other programming languages or other tasks regarding code encoding or generation.\n\nAnother threat to validity is on the metrics we choose for evaluation. It has always been a tough challenge to evaluate the similarity between two sentences for the tasks such as neural machine translation [80], image captioning [81]. In this paper, we only adopt four popular automatic metrics, it is necessary for us to evaluate the performance of generated text from more perspectives, such as human evaluation. Furthermore, in the deep reinforcement learning perspective, we set the BLEU score of generated sentence as the reward. It is well known that for a reinforcement learning model, one of the biggest challenge is how to design a reward function to measure the value of action correctly, and it is still an open problem. In our future work, we plan to devise a reward function that can reflect the value of each action more correctly.\n\n\nRELATED WORK\n\n\nDeep Code Representation\n\nWith the successful development of deep learning, it has also become more and more prevalent for representing code in the domain of software engineering research. Gu et al. [82] use a sequence-to-sequence deep neural network [80], originally introduced for statistical machine translation, to learn intermediate distributed vector representations of natural language queries which they use to predict relevant API sequences. Mou et al. [83] learn distributed vector representations using custom convolutional neural networks to represent features of code snippets, then they assume that student solutions to various coursework problems have been intermixed and seek to recover the solution-to-problem mapping via classification. Li et al. [84] learn distributed vector representations for the nodes of a memory heap and use the learned representations to synthesize candidate formal specifications for the code that produces the heap. Piech et al. [85] and Parisotto et al. [86] learn distributed representations of code input/output pairs and use them to assess and review student assignments or to guide program synthesis from examples. Neural code-generative models of code also use distributed representations to capture context, which is a common practice in natural language processing. For example, the work of Maddison and Tarlow [87] and other neural language models (e.g. LSTMs in Dam et al. [88]) describe context distributed representations while sequentially generating code. Ling et al. [89] and Allamanis et al. [90] combine the code-context distributed representation with distributed representations of other modalities (e.g. natural language) to synthesize code.\n\n\nSource Code Summarization\n\nCode summarization is a novel task in the area of software engineering and has drawn great attention in recent years. The existing works for code summarization can be mainly categorized as rule-based approaches [32], statisticallanguage-model-based approaches [26] and deep-learningbased approaches [33], [34], [65]. Sridhara et al. [32] construct a software word usage model first, and generate comment according to the tokenized function/variable names via rules. Movshovitz-Attias et al. [26] predict comments from Java code files using topic models and n-grams. In [33], the authors introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features to summarize code snippets into short, descriptive function name-like summaries. Iyer et al. [27] propose to use LSTM networks with attention to produce sentences that describe C# code snippets and SQL queries. In [34], the code summarization problem is modelled as a machine translation task, and some translation models such as Seq2Seq [80] and Seq2Seq with attention [91] are employed. In [92], a framework, BVAE, which includes two Variational AutoEncoders (VAEs) to model bimodal data: C-VAE for code and L-VAE for natural language is proposed. It could learn semantic vector representations for both code and description and generate completely new descriptions for arbitrary code snippets. Alon et al. [56] proposes Code2Seq, which represents code snippet as the set of compositional paths in its AST and used attention to select the relevant paths while decoding. According to the diffs information, Liu et al. propose NNGen (Nearest Neighbor Generator) which generate concise commit messages using the nearest neighbor algorithm [93]. CoaCor [57] utilizes the plain text of source code and an LSTM-based encoderdecoder framework for code summarization.\n\nUnlike previous studies, we abstract more hidden information of the code for a better code representation, introduce the hierarchical attention mechanism to take the code structure into consideration and propose a deep reinforcement learning framework to accurately generate code summary.\n\n\nDeep Reinforcement Learning\n\nReinforcement learning [49], [53], [94], concerned with how software agents ought to take actions in an environment so as to maximize the cumulative reward, is well suited for the task of decision-making. Recently, professional-level computer Go program has been designed by Silver et al. [54] using deep neural networks and Monte Carlo Tree Search. Human-level gaming control [95] has been achieved through deep Q-learning. A visual navigation system [96] has been proposed recently based on actor-critic reinforcement learning model. Text generation can also be formulated as a decision-making problem and there have been several reinforcement learning-based works on this specific tasks, including image captioning [97], dialogue generation [98] and sentence simplification [99]. Ren et al. [97] propose an actorcritic deep reinforcement learning model with an embedding reward for image captioning. Li et al. [98] integrate a developer-defined reward with REINFORCE algorithm for dialogue generation. In this paper, we follow an actor-critic reinforcement learning framework, while our focus is on encoding the structural and sequential information of code snippets simultaneously with an attention mechanism.\n\n\nCONCLUSION\n\nThis paper presents the first hierarchical-attention-based learning approach by utilizing unstructured and structured features information of code, i.e., plain text, type-augmented AST and CFG, to reflect the hierarchical structure of code (tokens forming a statement, statements forming a function) by supporting a two-layer attention network at both token level and statement level. Our approach provides an effective representation that by differentiating tokens under different contexts for comments generation. Comprehensive experiments on a real-world dataset show that our proposed approach outperforms competitive baselines based on several standard evaluation metrics.\n\n\nACKNOWLEDGEMENT\n\nThis work is partially supported by the National Natu- \n\nFigure 2 :\n2The structure of recurrent neural network (RNN).\n\nFigure 3 (\n3b) is the AST of the code in Figure 3(a).\n\nFigure 3 :\n3(a) Code snippet and the corresponding summary. (b) The AST sequence of the code obtained by the ast module of Python. (c) The inter-procedural control flow of the code.\n\nFigure 4 :\n4The architecture of a two-layer hierarchical attention network.\n\nFigure 5 :\n5An overview of our proposed reinforcementlearning-guided code summarization via hierarchical attention network: (a) x T XT ij , x AST ij and x CF G ij represent the jth token in the ith statement of the lexical level representation, the type-augmented AST representation, and the control flow representation of the code respectively; (b) the LSTM-based hierarchical attention network is used to encode the three code representations into vectors:\n\nFigure 7 :\n7Length distribution of testing data.\n\n\nnumber distribution in each statement.\n\n\nnumber distribution in each function.\n\nFigure 8 :\n8Statistic analyses of the source code.\n\n\n\u2022 AST+HAN+DRL. This baseline takes the sequence of the type-augmented AST as the input of the LSTM-based hierarchical attention network encoder. \u2022 CFG+HAN+DRL. This baseline follows the same architecture with the above two baselines only differing in that it takes the control flow of code as the input of the LSTM-based hierarchical attention network encoder. \u2022 TXT&AST+HAN+DRL. This baseline follows the same architecture as above, and it takes both the plain text and the type-augmented AST sequence of code as the input of the LSTM-based hierarchical attention network respectively. The encoded hidden vectors of them are concatenated into one hidden vector by a hybrid layer, and DRL is used to train the model. \u2022 TXT&CFG+HAN+DRL. Similarly, this approach takes the plain text and the control flow of code as the input of the architecture.\u2022 AST&CFG+HAN+DRL. Similarly, this approach takes the type-augmented AST sequence and the control flow of code as the input of the architecture. \u2022 Our previous approach[39]: TXT&AST+AN+DRL.\n\nFigure 9 :Figure 10 :\n910Performance trend on different metrics w.r.t. varying training epochs. Performance trend on different metrics w.r.t. varying code length.\n\nFigure 11 :\n11Performance trend on different metrics w.r.t. varying comment length.\n\n\nral Science Foundation of China (Grant No. 61902169), Shenzhen Science and Technology Program (Grant No. KQTD2016112514355531), Science and Technology Innovation Committee Foundation of Shenzhen (Grant No. JCYJ20170817110848086) and Australia Research Council (Grant No. DP200101374, LP170100891, DE170101081 and DP200101328), the National key R & D program sub project \"large scale cross-modality medical knowledge management\" under grant No. 2018AAA0102100, the Zhejiang public welfare technology research project under grant No. LGF20F020013, the National Key R&D Program Project of \"Software Testing Evaluation Method Research and its Database Development on Artificial Intelligence Medical Information System\" under the Fifth Electronics Research Institute of the Ministry of Industry and Information Technology (No. 2019YFC0118802), and the National Key R&D Program Project of \"Full Life Cycle Detection Platform and Application Demonstration of Medical Artificial Intelligence Product\" under the National Institutes for Food and Drug Control (No. 2019YFB1404802), NSF under grants III-1526499, III-1763325, III-1909323, and CNS-1930941. Zhuo Zhao received the B.S. and Ph.D. degrees\n\nTable 1 :\n1Effectiveness of code representations. (Best scores are in boldface.)Approaches \nBLEU-1 \nBLEU-2 \nBLEU-3 \nBLEU-4 \nMETEOR \nROUGE-L \n\nTXT+HAN+DRL \n19.51 \n2.45 \n0.95 \n0.65 \n5.65 \n31.56 \n\nAST+HAN+DRL \n18.97 \n3.95 \n1.87 \n0.89 \n5.97 \n31.23 \n\nCFG+HAN+DRL \n19.20 \n2.45 \n1.12 \n0.67 \n5.12 \n31.46 \n\nTXT&AST+HAN+DRL \n26.56 \n3.96 \n1.89 \n1.32 \n6.21 \n37.68 \n\nTXT&CFG+HAN+DRL \n27.66 \n4.25 \n1.97 \n1.12 \n6.38 \n38.24 \n\nAST&CFG+HAN+DRL \n26.35 \n2.65 \n0.96 \n0.97 \n5.87 \n38.13 \n\nTXT&AST+AN+DRL(previous work) \n25.27 \n10.33 \n6.40 \n4.41 \n9.29 \n39.13 \n\nTXT&AST&CFG+HAN+DRL \n33.16 \n12.39 \n6.21 \n5.10 \n9.43 \n46.23 \n\nTXT&AST&CFG+HAN+DRL (with ad-\nditional 20,000 subjects) \n\n32.87 \n11.76 \n6.32 \n5.48 \n8.53 \n39.72 \n\n\n\nTable 2 :\n2Effectiveness of hierarchical attention network. (Best scores are in boldface.)Attention type \nBLEU-1 \nBLEU-2 \nBLEU-3 \nBLEU-4 \nMETEOR \nROUGE-L \n\nno attention \n18.76 \n8.21 \n4.98 \n3.46 \n8.24 \n35.28 \n\n1-layer attention (general attention) \n25.79 \n8.45 \n5.73 \n4.67 \n8.79 \n38.49 \n\n2-layer attention \n33.16 \n12.39 \n6.21 \n5.10 \n9.43 \n46.23 \n\n\n\nTable 3 :\n3Effectiveness of deep reinforcement learning. (Best scores are in boldface.)Approach \nBLEU-1 \nBLEU-2 \nBLEU-3 \nBLEU-4 \nMETEOR \nROUGE-L \n\nApproach without DRL \n26.89 \n7.21 \n3.76 \n2.31 \n8.21 \n35.78 \n\nApproach with DRL \n33.16 \n12.39 \n6.21 \n5.10 \n9.43 \n46.23 \n\n\n\nTable 4 :\n4Experimental results of different dataset settingsSettings \nBLEU-1 \nBLEU-2 \nBLEU-3 \nBLEU-4 \nMETEOR \nROUGE-L \n\ncross-project \n20.10 \n10.54 \n6.33 \n4.36 \n15.87 \n27.35 \n\n8:1:1-split \n34.12 \n18.02 \n12.33 \n9.18 \n15.34 \n46.32 \n\n\n\nTable 5 :\n5The time consumption to train the models (mins).TXT \nAST \nCFG \nTXT&AST \nTXT&CFG \nAST&CFG \nTXT&AST&CFG \n\nActor pretraining epochs \n20 \n24 \n23 \n32 \n31 \n33 \n39 \n\nCritic pretraining epochs \n27 \n30 \n29 \n41 \n40 \n41 \n50 \n\nActor-critic training epochs \n36 \n41 \n43 \n50 \n51 \n53 \n58 \n\n\n\nTable 6 :\n6Code summarization results comparison with state-of-the-art approaches.Python \nJava \n\nApproaches \nS-BLEU \nC-BLEU \nMETEOR \nROUGE-L \nS-BLEU \nC-BLEU \nMETEOR \nROUGE-L \n\nDeepCom \n12.92 \n13.27 \n6.09 \n14.33 \n29.65 \n31.55 \n16.87 \n25.76 \n\nCODENN \n13.50 \n11.36 \n5.82 \n13.18 \n36.51 \n34.31 \n18.04 \n26.77 \n\nCode2Seq \n19.49 \n17.53 \n6.52 \n22.04 \n19.96 \n18.07 \n10.17 \n23.56 \n\nCoaCor \n25.61 \n23.67 \n9.52 \n29.38 \n33.61 \n32.64 \n17.62 \n28.76 \n\nOur approach \n33.16 \n30.58 \n9.43 \n46.23 \n38.25 \n36.42 \n20.01 \n37.19 \n\n\n\nTable 7 :\n7Case study of code summary generated by each approach. list of all available vm sizes on the cloud provider. return a list of all available services cli example.1-layerreturns the total number of cpus in the system. test the behavior of python and invalid features.data = __salt__['http.query'](url=url, decode=True, decode_type='yaml') if ('dict' code-bluein data): return data['dict']log.error((('Error caught on query to' Includes{'status': 'success', 'tags': set(['fast-ish'])}), Includes({'status': 'success', 'tags': set()}), Includes({ 'status': 'success', 'tags': set()})]Ground truth read pillar data from http response. test subunit output with tags.Generated Comments no attention returns a list of all available services cli example. return true if the given object is a valid config.1-layer prints a dict of all methods on a specific server. return a list of available audio.2-layer return a string with escape-backslashes converted to nulls. returns image.Case 1 \nCase 2 \n\nCode snippet \n\ndef Pool(processes=None, initializer=None, \n\ninitargs=(), maxtasksperchild=None): \n\nfrom multiprocessing.pool import Pool \n\nreturn Pool(processes, initializer, \n\ninitargs, maxtasksperchild) \n\ndef remove_file(source): \n\nif os.path.isdir(source): \n\nshutil.rmtree(source) \n\nelif (os.path.isfile(source) or \n\nos.path.islink(source)): \n\nos.remove(source) \n\nGround truth \nreturns a process pool object. \nremove file or directory. \n\nGenerated Comments \n\nno attention \nreturn a 2-layer \nreturns a list of all elements in a given order. \ngiven two tp instances. \n\n1-layer+DRL \nreturns a process object with the given id. \nremove a test. \n\n2-layer+DRL \nreturns a process object. \nremove the file. \n\nCase 3 \nCase 4 \n\nCode snippet \n\ndef ext_pillar(minion_id, pillar, url): \n\nlog=logging.getLogger(__name__) \n\n+ url) + 'More Info:')) \n\nfor (k, v) in six.iteritems(data): \n\nlog.error(((k + ':') + v)) \n\nreturn {} \n\ndef test_subunit_output_with_tags(): \n\nstate.expect=[Includes({'status': \n\n'success', 'tags': set(['slow-ish'])}), \n\n(runner=Runner(feature_name( \n\n'tagged_features'), \n\nenable_subunit=True) \n\nrunner.run() \n\n1-layer+DRL \ntest io for several. \ntest subunit output to unicode. \n\n2-layer+DRL \nread pillar data. \ntest subunit output with unicode. \n\n\n\nTable 8 :\n8Naturalness and Informativeness measures of the generated comments.Approaches \nNaturalness \nInformativeness \n\nDeepCom \n3.59 \n2.91 \n\nCODENN \n3.41 \n2.39 \n\nCode2seq \n3.82 \n2.83 \n\nCoaCor \n4.18 \n3.05 \n\nOur \n4.35 \n3.27 \n\n\n\nSoftware maintenance management. B P Lientz, E B Swanson, IEE Proceedings E Computers and Digital Techniques Transactions on Software Engineering. 1276B. P. Lientz and E. B. Swanson, \"Software maintenance man- agement,\" IEE Proceedings E Computers and Digital Techniques Transactions on Software Engineering, vol. 127, no. 6, 1980.\n\nAutomated documentation inference to explain failed tests. S Zhang, C Zhang, M D Ernst, 2011 26th IEEE/ACM International Conference on Automated Software Engineering. ASES. Zhang, C. Zhang, and M. D. Ernst, \"Automated documentation inference to explain failed tests,\" in 2011 26th IEEE/ACM Inter- national Conference on Automated Software Engineering (ASE 2011).\n\n. IEEE. IEEE, 2011, pp. 63-72.\n\nPrecise condition synthesis for program repair. Y Xiong, J Wang, R Yan, J Zhang, S Han, G Huang, L Zhang, 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEEY. Xiong, J. Wang, R. Yan, J. Zhang, S. Han, G. Huang, and L. Zhang, \"Precise condition synthesis for program repair,\" in 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE, 2017, pp. 416-426.\n\nLarge-scale analysis of framework-specific exceptions in android apps. L Fan, T Su, S Chen, G Meng, Y Liu, L Xu, G Pu, Z Su, 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEEL. Fan, T. Su, S. Chen, G. Meng, Y. Liu, L. Xu, G. Pu, and Z. Su, \"Large-scale analysis of framework-specific exceptions in android apps,\" in 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, 2018, pp. 408-419.\n\nInformation retrieval and spectrum based bug localization: better together. T.-D B Le, R J Oentaryo, D Lo, Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. the 2015 10th Joint Meeting on Foundations of Software EngineeringACMT.-D. B. Le, R. J. Oentaryo, and D. Lo, \"Information retrieval and spectrum based bug localization: better together,\" in Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. ACM, 2015, pp. 579-590.\n\nEdsketch: executiondriven sketching for java. J Hua, Y Zhang, Y Zhang, S Khurshid, International Journal on Software Tools for Technology Transfer. 213J. Hua, Y. Zhang, Y. Zhang, and S. Khurshid, \"Edsketch: execution- driven sketching for java,\" International Journal on Software Tools for Technology Transfer, vol. 21, no. 3, pp. 249-265, 2019.\n\nSmartvm: a sla-aware microservice deployment framework. T Zheng, X Zheng, Y Zhang, Y Deng, E Dong, R Zhang, X Liu, World Wide Web. 221T. Zheng, X. Zheng, Y. Zhang, Y. Deng, E. Dong, R. Zhang, and X. Liu, \"Smartvm: a sla-aware microservice deployment framework,\" World Wide Web, vol. 22, no. 1, pp. 275-293, 2019.\n\nHybrid regression test selection. L Zhang, 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEEL. Zhang, \"Hybrid regression test selection,\" in 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, 2018, pp. 199-209.\n\nDeeproad: Gan-based metamorphic testing and input validation framework for autonomous driving systems. M Zhang, Y Zhang, L Zhang, C Liu, S Khurshid, 10.1145/3238147.3238187Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringMontpellier, FranceM. Zhang, Y. Zhang, L. Zhang, C. Liu, and S. Khurshid, \"Deeproad: Gan-based metamorphic testing and input validation framework for autonomous driving systems,\" in Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering, ASE 2018, Montpellier, France, September 3-7, 2018, 2018, pp. 132-142. [Online]. Available: https://doi.org/10.1145/3238147.3238187\n\nDo pseudo test suites lead to inflated correlation in measuring test effectiveness. J M Zhang, L Zhang, D Hao, M Wang, L Zhang, 10.1109/ICST.2019.0003312th IEEE Conference on Software Testing, Validation and Verification, ICST 2019. Xi'an, ChinaJ. M. Zhang, L. Zhang, D. Hao, M. Wang, and L. Zhang, \"Do pseudo test suites lead to inflated correlation in measuring test effectiveness?\" in 12th IEEE Conference on Software Testing, Validation and Verification, ICST 2019, Xi'an, China, April 22-27, 2019, 2019, pp. 252-263. [Online]. Available: https://doi.org/10.1109/ICST.2019.00033\n\nCharactering and detecting cuda program bugs. M Wu, H Zhou, L Zhang, C Liu, Y Zhang, arXiv:1905.01833arXiv preprintM. Wu, H. Zhou, L. Zhang, C. Liu, and Y. Zhang, \"Charactering and detecting cuda program bugs,\" arXiv preprint arXiv:1905.01833, 2019.\n\nDeepbillboard: Systematic physical-world testing of autonomous driving systems. H Zhou, W Li, Z Kong, J Guo, Y Zhang, B Yu, L Zhang, C Liu, Proceedings of the 42nd International Conference on Software Engineering. the 42nd International Conference on Software EngineeringSeoul, Korea2020H. Zhou, W. Li, Z. Kong, J. Guo, Y. Zhang, B. Yu, L. Zhang, and C. Liu, \"Deepbillboard: Systematic physical-world testing of au- tonomous driving systems,\" in Proceedings of the 42nd International Conference on Software Engineering, ICSE 2020, Seoul, Korea, May 23 - 29, 2020, 2020.\n\nA framework for iot-based monitoring and diagnosis of manufacturing systems. I.-L Yen, S Zhang, F Bastani, Y Zhang, 2017 IEEE Symposium on Service-Oriented System Engineering (SOSE). I.-L. Yen, S. Zhang, F. Bastani, and Y. Zhang, \"A framework for iot-based monitoring and diagnosis of manufacturing systems,\" in 2017 IEEE Symposium on Service-Oriented System Engineering (SOSE).\n\n. IEEE. IEEE, 2017, pp. 1-8.\n\nSimulee: Detecting cuda synchronization bugs via memory-access modeling. M Wu, Y Ouyang, H Zhou, L Zhang, C Liu, Y Zhang, Proceedings of the 42nd International Conference on Software Engineering. the 42nd International Conference on Software EngineeringSeoul, Korea2020M. Wu, Y. Ouyang, H. Zhou, L. Zhang, C. Liu, and Y. Zhang, \"Simulee: Detecting cuda synchronization bugs via memory-access modeling,\" in Proceedings of the 42nd International Conference on Software Engineering, ICSE 2020, Seoul, Korea, May 23 -29, 2020, 2020.\n\nDeepfl: integrating multiple fault diagnosis dimensions for deep fault localization. X Li, W Li, Y Zhang, L Zhang, 10.1145/3293882.3330574Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis. the 28th ACM SIGSOFT International Symposium on Software Testing and AnalysisBeijing, ChinaX. Li, W. Li, Y. Zhang, and L. Zhang, \"Deepfl: integrating multiple fault diagnosis dimensions for deep fault localization,\" in Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2019, Beijing, China, July 15-19, 2019., 2019, pp. 169-180. [Online]. Available: https://doi.org/10.1145/3293882.3330574\n\nAn empirical study of boosting spectrum-based fault localization via pagerank. M Zhang, Y Li, X Li, L Chen, Y Zhang, L Zhang, S Khurshid, IEEE Transactions on Software Engineering. M. Zhang, Y. Li, X. Li, L. Chen, Y. Zhang, L. Zhang, and S. Khurshid, \"An empirical study of boosting spectrum-based fault localization via pagerank,\" IEEE Transactions on Software Engineering, pp. 1-1, 2019.\n\nAn information retrieval approach for regression test prioritization based on program changes. R K Saha, L Zhang, S Khurshid, D E Perry, 10.1109/ICSE.2015.4737th IEEE/ACM International Conference on Software Engineering. Florence, Italy1R. K. Saha, L. Zhang, S. Khurshid, and D. E. Perry, \"An information retrieval approach for regression test prioritization based on program changes,\" in 37th IEEE/ACM International Conference on Software Engineering, ICSE 2015, Florence, Italy, May 16-24, 2015, Volume 1, 2015, pp. 268-279. [Online]. Available: https://doi.org/10.1109/ICSE.2015.47\n\nPractical program repair via bytecode mutation. A Ghanbari, S Benton, L Zhang, Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis. the 28th ACM SIGSOFT International Symposium on Software Testing and AnalysisBeijing, ChinaA. Ghanbari, S. Benton, and L. Zhang, \"Practical program repair via bytecode mutation,\" in Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2019, Beijing, China, July 15-19, 2019., 2019, pp. 19-30. [Online].\n\n. 10.1145/3293882.3330559Available: https://doi.org/10.1145/3293882.3330559\n\nAntipatterns in search-based program repair. S H Tan, H Yoshida, M R Prasad, A Roychoudhury, 10.1145/2950290.2950295Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016. the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016Seattle, WA, USAS. H. Tan, H. Yoshida, M. R. Prasad, and A. Roychoudhury, \"Anti- patterns in search-based program repair,\" in Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016, Seattle, WA, USA, November 13-18, 2016, 2016, pp. 727-738. [Online]. Available: https://doi.org/10.1145/2950290.2950295\n\nAutomating cuda synchronization via program transformation. M Wu, L Zhang, C Liu, S H Tan, Y Zhang, 34M. Wu, L. Zhang, C. Liu, S. H. Tan, and Y. Zhang, \"Automating cuda synchronization via program transformation,\" in 2019 34th\n\nIEEE/ACM International Conference on Automated Software Engineering (ASE). IEEEIEEE/ACM International Conference on Automated Software Engineer- ing (ASE). IEEE, 2019, pp. 748-759.\n\nAgile software development methods review and analysis. P Abrahamsson, O Salo, J Ronkainen, J Warsta, arXiv:1709.08439arXiv preprintP. Abrahamsson, O. Salo, J. Ronkainen, and J. Warsta, \"Agile software development methods review and analysis,\" arXiv preprint arXiv:1709.08439, 2002.\n\nCodeflaws: A programming competition benchmark for evaluating automated program repair tools. S H Tan, J Yi, S Yulis, A Mechtaev, Roychoudhury, IEEE/ACM International Conference on Software Engineering Companion. IEEES. H. Tan, J. Yi, Yulis, S. Mechtaev, and A. Roychoudhury, \"Code- flaws: A programming competition benchmark for evaluating automated program repair tools,\" in IEEE/ACM International Conference on Software Engineering Companion. IEEE, 2017, pp. 180-182.\n\nThe oracle problem in software testing: A survey. E T Barr, M Harman, P Mcminn, M Shahbaz, S Yoo, IEEE Transactions on Software Engineering. 415E. T. Barr, M. Harman, P. Mcminn, M. Shahbaz, and S. Yoo, \"The oracle problem in software testing: A survey,\" IEEE Transactions on Software Engineering, vol. 41, no. 5, pp. 507-525, 2015.\n\nA study of the documentation essential to software maintenance. S C B Souza, N Anquetil, K M De Oliveira, Proceedings of the 23rd annual international conference on Design of communication: documenting & designing for pervasive information. the 23rd annual international conference on Design of communication: documenting & designing for pervasive informationACMS. C. B. de Souza, N. Anquetil, and K. M. de Oliveira, \"A study of the documentation essential to software maintenance,\" in Proceedings of the 23rd annual international conference on Design of communication: documenting & designing for pervasive information. ACM, 2005, pp. 68-75.\n\nA survey of documentation practice within corrective maintenance. M Kajko-Mattsson, Empirical Software Engineering. 101M. Kajko-Mattsson, \"A survey of documentation practice within corrective maintenance,\" Empirical Software Engineering, vol. 10, no. 1, pp. 31-55, 2005.\n\nNatural language models for predicting programming comments. D Movshovitz-Attias, W W Cohen, Proceedings of the 51st. the 51stD. Movshovitz-Attias and W. W. Cohen, \"Natural language models for predicting programming comments,\" in Proceedings of the 51st\n\nAnnual Meeting of the Association for Computational Linguistics. Annual Meeting of the Association for Computational Linguistics, 2013, pp. 35-40.\n\nSummarizing source code using a neural attention model. S Iyer, I Konstas, A Cheung, L Zettlemoyer, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers1S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, \"Summarizing source code using a neural attention model,\" in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2016, pp. 2073-2083.\n\nFrom query to usable code: An analysis of stack overflow code snippets. D Yang, A Hussain, C V Lopes, Mining Software Repositories (MSR), 2016 IEEE/ACM 13th Working Conference on. IEEED. Yang, A. Hussain, and C. V. Lopes, \"From query to usable code: An analysis of stack overflow code snippets,\" in Mining Software Repositories (MSR), 2016 IEEE/ACM 13th Working Conference on. IEEE, 2016, pp. 391-401.\n\nQuery expansion based on crowd knowledge for code search. L Nie, H Jiang, Z Ren, Z Sun, X Li, IEEE Transactions on Services Computing. 95L. Nie, H. Jiang, Z. Ren, Z. Sun, and X. Li, \"Query expansion based on crowd knowledge for code search,\" IEEE Transactions on Services Computing, vol. 9, no. 5, pp. 771-783, 2016.\n\nAutomatic categorization with deep neural network for open-source java projects. A T Nguyen, T N Nguyen, Proceedings of the 39th International Conference on Software Engineering Companion. the 39th International Conference on Software Engineering CompanionIEEE PressA. T. Nguyen and T. N. Nguyen, \"Automatic categorization with deep neural network for open-source java projects,\" in Proceedings of the 39th International Conference on Software Engineering Companion. IEEE Press, 2017, pp. 164-166.\n\nLearning to generate pseudo-code from source code using statistical machine translation (t). Y Oda, H Fudaba, G Neubig, H Hata, S Sakti, T Toda, S Nakamura, 30th IEEE/ACM International Conference on. IEEEAutomated Software Engineering (ASE)Y. Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, and S. Nakamura, \"Learning to generate pseudo-code from source code using statistical machine translation (t),\" in Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on. IEEE, 2015, pp. 574-584.\n\nTowards automatically generating summary comments for java methods. G Sridhara, E Hill, D Muppaneni, L Pollock, K Vijay-Shanker, Proceedings of the IEEE/ACM international conference on Automated software engineering. the IEEE/ACM international conference on Automated software engineeringACMG. Sridhara, E. Hill, D. Muppaneni, L. Pollock, and K. Vijay-Shanker, \"Towards automatically generating summary comments for java methods,\" in Proceedings of the IEEE/ACM international conference on Automated software engineering. ACM, 2010, pp. 43-52.\n\nA convolutional attention network for extreme summarization of source code. M Allamanis, H Peng, C Sutton, International Conference on Machine Learning. M. Allamanis, H. Peng, and C. Sutton, \"A convolutional attention network for extreme summarization of source code,\" in International Conference on Machine Learning, 2016, pp. 2091-2100.\n\nAutomatic comment generation using a neural translation model. T Haije, B O K Intelligentie, E Gavves, H Heuer, T. Haije, B. O. K. Intelligentie, E. Gavves, and H. Heuer, \"Automatic comment generation using a neural translation model,\" 2016.\n\nDeep learning in neural networks: An overview. J Schmidhuber, Neural Networks. 61J. Schmidhuber, \"Deep learning in neural networks: An overview,\" Neural Networks, vol. 61, pp. 85-117, 2015.\n\nSummarizing source code with transferred api knowledge. X Hu, G Li, X Xia, D Lo, S Lu, Z Jin, X. Hu, G. Li, X. Xia, D. Lo, S. Lu, and Z. Jin, \"Summarizing source code with transferred api knowledge,\" 2018.\n\nM Ranzato, S Chopra, M Auli, W Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintM. Ranzato, S. Chopra, M. Auli, and W. Zaremba, \"Sequence level training with recurrent neural networks,\" arXiv preprint arXiv:1511.06732, 2015.\n\nSequence-to-sequence learning as beam-search optimization. S Wiseman, A M Rush, arXiv:1606.02960arXiv preprintS. Wiseman and A. M. Rush, \"Sequence-to-sequence learning as beam-search optimization,\" arXiv preprint arXiv:1606.02960, 2016.\n\nImproving automatic source code summarization via deep reinforcement learning. Y Wan, Z Zhao, M Yang, G Xu, H Ying, J Wu, P S Yu, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACMY. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu, \"Improving automatic source code summarization via deep reinforcement learning,\" in Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. ACM, 2018, pp. 397-407.\n\nClone detection using abstract syntax trees. I D Baxter, A Yahin, L Moura, M Sant&apos;anna, L Bier, Proceedings., International Conference on. International Conference onIEEEin Software MaintenanceI. D. Baxter, A. Yahin, L. Moura, M. Sant'Anna, and L. Bier, \"Clone detection using abstract syntax trees,\" in Software Maintenance, 1998. Proceedings., International Conference on. IEEE, 1998, pp. 368-377.\n\nImproving neural language models with a continuous cache. E Grave, A Joulin, N Usunier, arXiv:1612.04426arXiv preprintE. Grave, A. Joulin, and N. Usunier, \"Improving neural language models with a continuous cache,\" arXiv preprint arXiv:1612.04426, 2016.\n\nBugram: bug detection with n-gram language models. S Wang, D Chollak, D Movshovitz-Attias, L Tan, Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering. the 31st IEEE/ACM International Conference on Automated Software EngineeringACMS. Wang, D. Chollak, D. Movshovitz-Attias, and L. Tan, \"Bugram: bug detection with n-gram language models,\" in Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering. ACM, 2016, pp. 708-719.\n\nTwo decades of statistical language modeling: Where do we go from here. R Rosenfeld, Proceedings of the IEEE. the IEEE88R. Rosenfeld, \"Two decades of statistical language modeling: Where do we go from here?\" Proceedings of the IEEE, vol. 88, no. 8, pp. 1270-1278, 2000.\n\nA fast and simple algorithm for training neural probabilistic language models. A Mnih, Y W Teh, arXiv:1206.6426arXiv preprintA. Mnih and Y. W. Teh, \"A fast and simple algorithm for training neural probabilistic language models,\" arXiv preprint arXiv:1206.6426, 2012.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural computation, vol. 9, no. 8, pp. 1735-1780, 1997.\n\nReinforcement learning: An introduction. S Thrun, M L Littman, IEEE Transactions on Neural Networks. 161S. Thrun and M. L. Littman, \"Reinforcement learning: An intro- duction,\" IEEE Transactions on Neural Networks, vol. 16, no. 1, pp. 285-286, 2005.\n\nSeqgan: Sequence generative adversarial nets with policy gradient. L Yu, W Zhang, J Wang, Y Yu, Thirty-First AAAI Conference on Artificial Intelligence. L. Yu, W. Zhang, J. Wang, and Y. Yu, \"Seqgan: Sequence genera- tive adversarial nets with policy gradient,\" in Thirty-First AAAI Conference on Artificial Intelligence, 2017.\n\nIntroduction to reinforcement learning. R S Sutton, A G Barto, MIT press Cambridge135R. S. Sutton and A. G. Barto, Introduction to reinforcement learning. MIT press Cambridge, 1998, vol. 135.\n\nSimple statistical gradient-following algorithms for connectionist reinforcement learning. R J Williams, Reinforcement Learning. SpringerR. J. Williams, \"Simple statistical gradient-following algorithms for connectionist reinforcement learning,\" in Reinforcement Learning. Springer, 1992, pp. 5-32.\n\nQ-learning. C J Watkins, P Dayan, Machine learning. 83-4C. J. Watkins and P. Dayan, \"Q-learning,\" Machine learning, vol. 8, no. 3-4, pp. 279-292, 1992.\n\nDeep reinforcement learning for sequence to sequence models. Y Keneshloo, T Shi, C K Reddy, N Ramakrishnan, arXiv:1805.09461arXiv preprintY. Keneshloo, T. Shi, C. K. Reddy, and N. Ramakrishnan, \"Deep reinforcement learning for sequence to sequence models,\" arXiv preprint arXiv:1805.09461, 2018.\n\nActor-critic algorithms. V Konda, Siam Journal on Control & Optimization. 424V. Konda, \"Actor-critic algorithms,\" Siam Journal on Control & Optimization, vol. 42, no. 4, pp. 1143-1166, 2003.\n\nActor-critic algorithms. V R Konda, J N Tsitsiklis, Advances in neural information processing systems. V. R. Konda and J. N. Tsitsiklis, \"Actor-critic algorithms,\" in Advances in neural information processing systems, 2000, pp. 1008- 1014.\n\nMastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, S Dieleman, Nature. 5297587D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, and S. Dieleman, \"Mastering the game of go with deep neural networks and tree search,\" Nature, vol. 529, no. 7587, pp. 484-489, 2016.\n\nGenism. \"Genism,\" https://pypi.org/project/gensim/.\n\ncode2seq: Generating sequences from structured representations of code. U Alon, S Brody, O Levy, E Yahav, arXiv:1808.01400arXiv preprintU. Alon, S. Brody, O. Levy, and E. Yahav, \"code2seq: Generating sequences from structured representations of code,\" arXiv preprint arXiv:1808.01400, 2018.\n\nCoacor: Code annotation for code retrieval with reinforcement learning. Z Yao, J R Peddamail, H Sun, The World Wide Web Conference. ACMZ. Yao, J. R. Peddamail, and H. Sun, \"Coacor: Code annotation for code retrieval with reinforcement learning,\" in The World Wide Web Conference. ACM, 2019, pp. 2203-2214.\n\nCompilers, principles, techniques. A V Aho, R Sethi, J D Ullman, Addison Wesley79A. V. Aho, R. Sethi, and J. D. Ullman, \"Compilers, principles, techniques,\" Addison Wesley, vol. 7, no. 8, p. 9, 1986.\n\nAbstract syntax trees. \"Abstract syntax trees,\" accessed 16 August 2018. https://docs.python.org/2/library/ast.html.\n\nA Narayanan, M Chandramohan, R Venkatesan, L Chen, L Yang, S , arXiv:1707.05005graph2vec: Learning distributed representations of graphs. arXiv preprintA. Narayanan, M. Chandramohan, R. Venkatesan, L. Chen, L. Yang, and S. Jaiswal, \"graph2vec: Learning distributed representations of graphs,\" arXiv preprint arXiv:1707.05005, 2017.\n\nHierarchical attention networks for document classification. Z Yang, D Yang, C Dyer, X He, A Smola, E Hovy, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. NAACL. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. NAACLZ. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, \"Hierarchical attention networks for document classification,\" in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. NAACL, 2016, pp. 1480-1489.\n\nAn introductory survey on attention mechanisms in nlp problems. D Hu, arXiv:1811.05544arXiv preprintD. Hu, \"An introductory survey on attention mechanisms in nlp problems,\" arXiv preprint arXiv:1811.05544, 2018.\n\nAttention is not explanation. S Jain, B C Wallace, arXiv:1902.10186arXiv preprintS. Jain and B. C. Wallace, \"Attention is not explanation,\" arXiv preprint arXiv:1902.10186, 2017.\n\nEffective approaches to attention-based neural machine translation. M.-T Luong, H Pham, C D Manning, arXiv:1508.04025arXiv preprintM.-T. Luong, H. Pham, and C. D. Manning, \"Effective approaches to attention-based neural machine translation,\" arXiv preprint arXiv:1508.04025, 2015.\n\nDeep code comment generation. X Hu, G Li, X Xia, D Lo, Z Jin, Proceedings of the 26th Conference on Program Comprehension. the 26th Conference on Program ComprehensionACMX. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, \"Deep code comment genera- tion,\" in Proceedings of the 26th Conference on Program Comprehension. ACM, 2018, pp. 200-210.\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W J Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsK. Papineni, S. Roukos, T. Ward, and W. J. Zhu, \"Bleu: a method for automatic evaluation of machine translation,\" in Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 2002, pp. 311-318.\n\nAdaptive subgradient methods for online learning and stochastic optimization. J Duchi, E Hazan, Y Singer, Journal of Machine Learning Research. 12J. Duchi, E. Hazan, and Y. Singer, \"Adaptive subgradient methods for online learning and stochastic optimization,\" Journal of Machine Learning Research, vol. 12, no. Jul, pp. 2121-2159, 2011.\n\nGithub. \"Github,\" https://github.com/.\n\nA parallel corpus of python functions and documentation strings for automated code documentation and code generation. A V M Barone, R Sennrich, arXiv:1707.02275arXiv preprintA. V. M. Barone and R. Sennrich, \"A parallel corpus of python functions and documentation strings for automated code docu- mentation and code generation,\" arXiv preprint arXiv:1707.02275, 2017.\n\nDeep code comment generation with hybrid lexical and syntactical information. X Hu, G Li, X Xia, D Lo, Z Jin, Empirical Software Engineering. X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, \"Deep code comment gen- eration with hybrid lexical and syntactical information,\" Empirical Software Engineering, pp. 1-39, 2019.\n\nMeteor: An automatic metric for mt evaluation with improved correlation with human judgments. S Banerjee, A Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization29S. Banerjee and A. Lavie, \"Meteor: An automatic metric for mt evaluation with improved correlation with human judgments,\" in Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, vol. 29, 2005, pp. 65-72.\n\nRouge: A package for automatic evaluation of summaries. C Y Lin, Text Summarization Branches Out. C. Y. Lin, \"Rouge: A package for automatic evaluation of sum- maries,\" Text Summarization Branches Out, 2004.\n\nIsomorphic transfer of syntactic structures in cross-lingual nlp. E M Ponti, R Reichart, A Korhonen, I Vulic, The 56th. E. M. Ponti, R. Reichart, A. Korhonen, and I. Vulic, \"Isomorphic transfer of syntactic structures in cross-lingual nlp,\" in The 56th\n\nAnnual Meeting of the Association for Computational Linguistics. Annual Meeting of the Association for Computational Linguistics, 2018, pp. 1531-1542.\n\nNeural and rule-based finish nlp models-expectation, experiments and experiences. T A Pirinen, The fifth Workshop on Computational Linguistics for Uralic Languages. T. A. Pirinen, \"Neural and rule-based finish nlp models-expectation, experiments and experiences,\" in The fifth Workshop on Computa- tional Linguistics for Uralic Languages, 2019, pp. 104-1114.\n\nOn tree-based neural sentence modeling. H Shi, H Zhou, J Chen, L Li, arXiv:1808.09644arXiv preprintH. Shi, H. Zhou, J. Chen, and L. Li, \"On tree-based neural sentence modeling,\" arXiv preprint arXiv:1808.09644, 2018.\n\nReinforcement learning for bandit neural machine translation with simulated human feedback. K Nguyen, H D Iii, J Boyd-Graber, Conference on Empirical Methods in Natural Language Processing. K. Nguyen, H. D. Iii, and J. Boyd-Graber, \"Reinforcement learning for bandit neural machine translation with simulated human feedback,\" in Conference on Empirical Methods in Natural Language Processing, 2017.\n\nOvercoming the curse of sentence length for neural machine translation using automatic segmentation. J Pouget-Abadie, D Bahdanau, B V Merrienboer, K Cho, Y Bengio, arXiv:1409.1257arXiv preprintJ. Pouget-Abadie, D. Bahdanau, B. V. Merrienboer, K. Cho, and Y. Bengio, \"Overcoming the curse of sentence length for neural machine translation using automatic segmentation,\" arXiv preprint arXiv:1409.1257, 2014.\n\nModeling source syntax for neural machine translation. J Li, D Xiong, Z Tu, M Zhu, Z Min, G Zhou, arXiv:1705.01020arXiv preprintJ. Li, D. Xiong, Z. Tu, M. Zhu, Z. Min, and G. Zhou, \"Modeling source syntax for neural machine translation,\" arXiv preprint arXiv:1705.01020, 2017.\n\nNeural machine translation in linear time. N Kalchbrenner, L Espeholt, K Simonyan, A V D Oord, K Kavukcuoglu, arXiv:1610.10099arXiv preprintN. Kalchbrenner, L. Espeholt, K. Simonyan, A. V. D. Oord, and K. Kavukcuoglu, \"Neural machine translation in linear time,\" arXiv preprint arXiv:1610.10099, 2016.\n\nSequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, Advances in neural information processing systems. I. Sutskever, O. Vinyals, and Q. V. Le, \"Sequence to sequence learning with neural networks,\" in Advances in neural information processing systems, 2014, pp. 3104-3112.\n\nReevaluating automatic metrics for image captioning. M Kilickaya, A Erdem, N Ikizler-Cinbis, E Erdem, arXiv:1612.07600arXiv preprintM. Kilickaya, A. Erdem, N. Ikizler-Cinbis, and E. Erdem, \"Re- evaluating automatic metrics for image captioning,\" arXiv preprint arXiv:1612.07600, 2016.\n\nDeep api learning. X Gu, H Zhang, D Zhang, S Kim, Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software EngineeringACMX. Gu, H. Zhang, D. Zhang, and S. Kim, \"Deep api learning,\" in Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. ACM, 2016, pp. 631-642.\n\nConvolutional neural networks over tree structures for programming language processing. L Mou, G Li, L Zhang, T Wang, Z Jin, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16). the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, \"Convolutional neural networks over tree structures for programming language processing,\" in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16), 2016, pp. 1287-1293.\n\nGated graph sequence neural networks. Y Li, D Tarlow, M Brockschmidt, R Zemel, arXiv:1511.05493arXiv preprintY. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, \"Gated graph sequence neural networks,\" arXiv preprint arXiv:1511.05493, 2015.\n\nLearning program embeddings to propagate feedback on student code. C Piech, J Huang, A Nguyen, M Phulsuksombati, M Sahami, L Guibas, arXiv:1505.05969arXiv preprintC. Piech, J. Huang, A. Nguyen, M. Phulsuksombati, M. Sahami, and L. Guibas, \"Learning program embeddings to propagate feedback on student code,\" arXiv preprint arXiv:1505.05969, 2015.\n\nNeuro-symbolic program synthesis. E Parisotto, A Mohamed, R Singh, L Li, D Zhou, P Kohli, arXiv:1611.01855arXiv preprintE. Parisotto, A. Mohamed, R. Singh, L. Li, D. Zhou, and P. Kohli, \"Neuro-symbolic program synthesis,\" arXiv preprint arXiv:1611.01855, 2016.\n\nStructured generative models of natural source code. C Maddison, D Tarlow, International Conference on Machine Learning. C. Maddison and D. Tarlow, \"Structured generative models of natural source code,\" in International Conference on Machine Learning, 2014, pp. 649-657.\n\nA deep language model for software code. H K Dam, T Tran, T Pham, arXiv:1608.02715arXiv preprintH. K. Dam, T. Tran, and T. Pham, \"A deep language model for software code,\" arXiv preprint arXiv:1608.02715, 2016.\n\nLatent predictor networks for code generation. W Ling, E Grefenstette, K M Hermann, T Ko\u010disk\u1ef3, A Senior, F Wang, P Blunsom, arXiv:1603.06744arXiv preprintW. Ling, E. Grefenstette, K. M. Hermann, T. Ko\u010disk\u1ef3, A. Senior, F. Wang, and P. Blunsom, \"Latent predictor networks for code generation,\" arXiv preprint arXiv:1603.06744, 2016.\n\nBimodal modelling of source code and natural language. M Allamanis, D Tarlow, A Gordon, Y Wei, International Conference on Machine Learning. M. Allamanis, D. Tarlow, A. Gordon, and Y. Wei, \"Bimodal modelling of source code and natural language,\" in International Conference on Machine Learning, 2015, pp. 2123-2132.\n\nNeural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, arXiv:1409.0473arXiv preprintD. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine trans- lation by jointly learning to align and translate,\" arXiv preprint arXiv:1409.0473, 2014.\n\nA neural framework for retrieval and summarization of source code. Q Chen, M Zhou, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACMQ. Chen and M. Zhou, \"A neural framework for retrieval and summarization of source code,\" in Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. ACM, 2018, pp. 826-831.\n\nNeuralmachine-translation-based commit message generation: how far are we. Z Liu, X Xia, A E Hassan, D Lo, Z Xing, X Wang, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACMZ. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang, \"Neural- machine-translation-based commit message generation: how far are we?\" in Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. ACM, 2018, pp. 373-384.\n\nPolicy gradient methods for reinforcement learning with function approximation. R S Sutton, D A Mcallester, S P Singh, Y Mansour, Advances in neural information processing systems. R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour, \"Policy gradient methods for reinforcement learning with function approximation,\" in Advances in neural information processing systems, 2000, pp. 1057-1063.\n\nHuman-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., \"Human-level control through deep reinforcement learning,\" Nature, vol. 518, no. 7540, pp. 529-533, 2015.\n\nTarget-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, Robotics and Automation (ICRA. Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi, \"Target-driven visual navigation in indoor scenes using deep reinforcement learning,\" in Robotics and Automation (ICRA), 2017 IEEE International Conference on. IEEE, 2017, pp. 3357-3364.\n\nDeep reinforcement learning-based image captioning with embedding reward. Z Ren, X Wang, N Zhang, X Lv, L J Li, Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference. Z. Ren, X. Wang, N. Zhang, X. Lv, and L. J. Li, \"Deep reinforcement learning-based image captioning with embedding reward,\" in Com- puter Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017, pp. 1151-1159.\n\nDeep reinforcement learning for dialogue generation. J Li, W Monroe, A Ritter, M Galley, J Gao, D Jurafsky, arXiv:1606.01541arXiv preprintJ. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Jurafsky, \"Deep reinforcement learning for dialogue generation,\" arXiv preprint arXiv:1606.01541, 2016.\n\nSentence simplification with deep reinforcement learning. X Zhang, M Lapata, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingX. Zhang and M. Lapata, \"Sentence simplification with deep reinforcement learning,\" in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2017, pp. 584-594.\n", "annotations": {"author": "[{\"end\":102,\"start\":90},{\"end\":115,\"start\":103},{\"end\":126,\"start\":116},{\"end\":135,\"start\":127},{\"end\":146,\"start\":136},{\"end\":155,\"start\":147},{\"end\":168,\"start\":156},{\"end\":181,\"start\":169}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":97},{\"end\":114,\"start\":109},{\"end\":125,\"start\":122},{\"end\":134,\"start\":131},{\"end\":145,\"start\":141},{\"end\":154,\"start\":152},{\"end\":167,\"start\":165},{\"end\":180,\"start\":178}]", "author_first_name": "[{\"end\":96,\"start\":90},{\"end\":108,\"start\":103},{\"end\":121,\"start\":116},{\"end\":130,\"start\":127},{\"end\":140,\"start\":136},{\"end\":151,\"start\":147},{\"end\":162,\"start\":156},{\"end\":164,\"start\":163},{\"end\":177,\"start\":169}]", "author_affiliation": null, "title": "[{\"end\":87,\"start\":1},{\"end\":268,\"start\":182}]", "venue": null, "abstract": "[{\"end\":2755,\"start\":345}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2991,\"start\":2988},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3107,\"start\":3104},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3112,\"start\":3109},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3117,\"start\":3114},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3122,\"start\":3119},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3127,\"start\":3124},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3132,\"start\":3129},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3160,\"start\":3157},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3165,\"start\":3162},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3171,\"start\":3167},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3177,\"start\":3173},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3183,\"start\":3179},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3189,\"start\":3185},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3195,\"start\":3191},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3220,\"start\":3216},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3226,\"start\":3222},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3232,\"start\":3228},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3253,\"start\":3249},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3259,\"start\":3255},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3265,\"start\":3261},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3587,\"start\":3583},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3593,\"start\":3589},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3599,\"start\":3595},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3711,\"start\":3707},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3717,\"start\":3713},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3964,\"start\":3961},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4441,\"start\":4437},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4447,\"start\":4443},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4543,\"start\":4539},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4549,\"start\":4545},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4578,\"start\":4574},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4866,\"start\":4862},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4872,\"start\":4868},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4933,\"start\":4929},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5009,\"start\":5005},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5015,\"start\":5011},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5021,\"start\":5017},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5180,\"start\":5176},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5562,\"start\":5559},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5932,\"start\":5929},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5969,\"start\":5965},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5975,\"start\":5971},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5981,\"start\":5977},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6555,\"start\":6551},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6661,\"start\":6657},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7127,\"start\":7123},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8832,\"start\":8829},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9407,\"start\":9403},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9504,\"start\":9500},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13907,\"start\":13903},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":14108,\"start\":14104},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14267,\"start\":14263},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14273,\"start\":14269},{\"end\":16216,\"start\":16206},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":16782,\"start\":16778},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18738,\"start\":18734},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19175,\"start\":19171},{\"end\":20227,\"start\":20217},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20795,\"start\":20791},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":20857,\"start\":20853},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":20971,\"start\":20967},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":21223,\"start\":21219},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":21345,\"start\":21341},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":24573,\"start\":24569},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":24664,\"start\":24660},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":25496,\"start\":25492},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25519,\"start\":25515},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":25525,\"start\":25521},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":25531,\"start\":25527},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":26452,\"start\":26448},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":26525,\"start\":26521},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":27394,\"start\":27390},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":27403,\"start\":27399},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":27949,\"start\":27945},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":28395,\"start\":28391},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":28401,\"start\":28397},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29283,\"start\":29282},{\"end\":29915,\"start\":29905},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":32468,\"start\":32464},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":32807,\"start\":32803},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":33000,\"start\":32996},{\"end\":34972,\"start\":34970},{\"end\":35092,\"start\":35090},{\"end\":35143,\"start\":35141},{\"end\":35179,\"start\":35177},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":35737,\"start\":35733},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":36892,\"start\":36888},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":36975,\"start\":36971},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":37011,\"start\":37007},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":37271,\"start\":37267},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":37990,\"start\":37986},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":37996,\"start\":37992},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":38489,\"start\":38485},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":38693,\"start\":38689},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":39859,\"start\":39855},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":39872,\"start\":39868},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":39889,\"start\":39885},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":40140,\"start\":40136},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":40146,\"start\":40142},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":40152,\"start\":40148},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":40158,\"start\":40154},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":40164,\"start\":40160},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":40170,\"start\":40166},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":40176,\"start\":40172},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":42737,\"start\":42733},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":44973,\"start\":44969},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":51688,\"start\":51684},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":51691,\"start\":51688},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":52412,\"start\":52408},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":52418,\"start\":52414},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":52626,\"start\":52622},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":52639,\"start\":52635},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":52654,\"start\":52650},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":52671,\"start\":52667},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":52727,\"start\":52723},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":52946,\"start\":52942},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":53055,\"start\":53051},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":53200,\"start\":53196},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":53411,\"start\":53407},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":56200,\"start\":56196},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":58068,\"start\":58064},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":58091,\"start\":58087},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":58924,\"start\":58920},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":58976,\"start\":58972},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":59187,\"start\":59183},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":59490,\"start\":59486},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":59699,\"start\":59695},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":59725,\"start\":59721},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":60089,\"start\":60085},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":60153,\"start\":60149},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":60252,\"start\":60248},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":60278,\"start\":60274},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":60672,\"start\":60668},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":60721,\"start\":60717},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":60760,\"start\":60756},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":60766,\"start\":60762},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":60772,\"start\":60768},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":60794,\"start\":60790},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":60952,\"start\":60948},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":61030,\"start\":61026},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":61299,\"start\":61295},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":61420,\"start\":61416},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":61544,\"start\":61540},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":61576,\"start\":61572},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":61598,\"start\":61594},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":61915,\"start\":61911},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":62244,\"start\":62240},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":62257,\"start\":62253},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":62712,\"start\":62708},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":62718,\"start\":62714},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":62724,\"start\":62720},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":62978,\"start\":62974},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":63066,\"start\":63062},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":63141,\"start\":63137},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":63407,\"start\":63403},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":63433,\"start\":63429},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":63466,\"start\":63462},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":63483,\"start\":63479},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":63602,\"start\":63598},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":66704,\"start\":66700}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":64727,\"start\":64666},{\"attributes\":{\"id\":\"fig_1\"},\"end\":64782,\"start\":64728},{\"attributes\":{\"id\":\"fig_2\"},\"end\":64965,\"start\":64783},{\"attributes\":{\"id\":\"fig_3\"},\"end\":65042,\"start\":64966},{\"attributes\":{\"id\":\"fig_4\"},\"end\":65502,\"start\":65043},{\"attributes\":{\"id\":\"fig_6\"},\"end\":65552,\"start\":65503},{\"attributes\":{\"id\":\"fig_7\"},\"end\":65593,\"start\":65553},{\"attributes\":{\"id\":\"fig_8\"},\"end\":65633,\"start\":65594},{\"attributes\":{\"id\":\"fig_9\"},\"end\":65685,\"start\":65634},{\"attributes\":{\"id\":\"fig_10\"},\"end\":66721,\"start\":65686},{\"attributes\":{\"id\":\"fig_11\"},\"end\":66885,\"start\":66722},{\"attributes\":{\"id\":\"fig_12\"},\"end\":66970,\"start\":66886},{\"attributes\":{\"id\":\"fig_13\"},\"end\":68162,\"start\":66971},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":68860,\"start\":68163},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":69208,\"start\":68861},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":69477,\"start\":69209},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":69711,\"start\":69478},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":69998,\"start\":69712},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":70505,\"start\":69999},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":72766,\"start\":70506},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":72994,\"start\":72767}]", "paragraph": "[{\"end\":3266,\"start\":2771},{\"end\":4579,\"start\":3268},{\"end\":5436,\"start\":4581},{\"end\":6662,\"start\":5438},{\"end\":7100,\"start\":6664},{\"end\":8574,\"start\":7102},{\"end\":10125,\"start\":8576},{\"end\":11106,\"start\":10127},{\"end\":11175,\"start\":11108},{\"end\":11878,\"start\":11177},{\"end\":12591,\"start\":11903},{\"end\":13001,\"start\":12593},{\"end\":13799,\"start\":13019},{\"end\":14133,\"start\":13818},{\"end\":14454,\"start\":14208},{\"end\":15494,\"start\":14456},{\"end\":15711,\"start\":15496},{\"end\":16053,\"start\":15741},{\"end\":16217,\"start\":16065},{\"end\":17121,\"start\":16247},{\"end\":17714,\"start\":17346},{\"end\":18104,\"start\":17726},{\"end\":18211,\"start\":18122},{\"end\":18664,\"start\":18258},{\"end\":19645,\"start\":18711},{\"end\":20228,\"start\":19647},{\"end\":20459,\"start\":20279},{\"end\":20686,\"start\":20461},{\"end\":21567,\"start\":20688},{\"end\":22285,\"start\":21592},{\"end\":23187,\"start\":22358},{\"end\":23896,\"start\":23189},{\"end\":24245,\"start\":23898},{\"end\":25168,\"start\":24318},{\"end\":25871,\"start\":25200},{\"end\":26031,\"start\":25873},{\"end\":26236,\"start\":26046},{\"end\":26903,\"start\":26276},{\"end\":27502,\"start\":26926},{\"end\":28402,\"start\":27544},{\"end\":28943,\"start\":28404},{\"end\":29284,\"start\":28945},{\"end\":29916,\"start\":29354},{\"end\":30069,\"start\":29918},{\"end\":30517,\"start\":30167},{\"end\":30685,\"start\":30519},{\"end\":31079,\"start\":30718},{\"end\":31311,\"start\":31170},{\"end\":31581,\"start\":31313},{\"end\":32083,\"start\":31700},{\"end\":32328,\"start\":32103},{\"end\":32692,\"start\":32330},{\"end\":33325,\"start\":32757},{\"end\":33541,\"start\":33327},{\"end\":34004,\"start\":33592},{\"end\":34153,\"start\":34039},{\"end\":34545,\"start\":34172},{\"end\":34673,\"start\":34547},{\"end\":35211,\"start\":34675},{\"end\":35805,\"start\":35251},{\"end\":36187,\"start\":35834},{\"end\":36758,\"start\":36189},{\"end\":37232,\"start\":36782},{\"end\":38441,\"start\":37234},{\"end\":38715,\"start\":38443},{\"end\":39661,\"start\":38717},{\"end\":40076,\"start\":39684},{\"end\":41056,\"start\":40078},{\"end\":41277,\"start\":41095},{\"end\":41935,\"start\":41312},{\"end\":42102,\"start\":41937},{\"end\":42319,\"start\":42189},{\"end\":42971,\"start\":42340},{\"end\":43369,\"start\":43072},{\"end\":45596,\"start\":43371},{\"end\":46909,\"start\":45650},{\"end\":47552,\"start\":46958},{\"end\":47758,\"start\":47612},{\"end\":48763,\"start\":47760},{\"end\":49106,\"start\":48765},{\"end\":50182,\"start\":49185},{\"end\":50539,\"start\":50240},{\"end\":51407,\"start\":50541},{\"end\":52419,\"start\":51409},{\"end\":54904,\"start\":52483},{\"end\":55052,\"start\":54939},{\"end\":56163,\"start\":55067},{\"end\":57028,\"start\":56178},{\"end\":57409,\"start\":57030},{\"end\":57856,\"start\":57433},{\"end\":58703,\"start\":57858},{\"end\":60427,\"start\":58747},{\"end\":62363,\"start\":60457},{\"end\":62653,\"start\":62365},{\"end\":63898,\"start\":62685},{\"end\":64590,\"start\":63913},{\"end\":64665,\"start\":64610}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11902,\"start\":11879},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14207,\"start\":14134},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16246,\"start\":16218},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17345,\"start\":17122},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18257,\"start\":18212},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20278,\"start\":20229},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22357,\"start\":22286},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29353,\"start\":29285},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30166,\"start\":30070},{\"attributes\":{\"id\":\"formula_9\"},\"end\":30717,\"start\":30686},{\"attributes\":{\"id\":\"formula_10\"},\"end\":31169,\"start\":31080},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31699,\"start\":31582},{\"attributes\":{\"id\":\"formula_12\"},\"end\":32739,\"start\":32693},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33591,\"start\":33542},{\"attributes\":{\"id\":\"formula_14\"},\"end\":34038,\"start\":34005},{\"attributes\":{\"id\":\"formula_15\"},\"end\":35250,\"start\":35212},{\"attributes\":{\"id\":\"formula_16\"},\"end\":41094,\"start\":41057},{\"attributes\":{\"id\":\"formula_17\"},\"end\":41311,\"start\":41278},{\"attributes\":{\"id\":\"formula_18\"},\"end\":42188,\"start\":42103}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":44305,\"start\":44298},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":46321,\"start\":46314},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47247,\"start\":47239},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":48068,\"start\":48061},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":48960,\"start\":48953},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":49308,\"start\":49301},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":53611,\"start\":53604},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":55171,\"start\":55164},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":57066,\"start\":57059}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2769,\"start\":2757},{\"attributes\":{\"n\":\"2\"},\"end\":13017,\"start\":13004},{\"attributes\":{\"n\":\"2.1\"},\"end\":13816,\"start\":13802},{\"attributes\":{\"n\":\"2.2\"},\"end\":15739,\"start\":15714},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":16063,\"start\":16056},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":17724,\"start\":17717},{\"attributes\":{\"n\":\"2.2.3\"},\"end\":18120,\"start\":18107},{\"attributes\":{\"n\":\"2.3\"},\"end\":18709,\"start\":18667},{\"attributes\":{\"n\":\"3\"},\"end\":21590,\"start\":21570},{\"attributes\":{\"n\":\"4\"},\"end\":24316,\"start\":24248},{\"attributes\":{\"n\":\"4.1\"},\"end\":25198,\"start\":25171},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":26044,\"start\":26034},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":26274,\"start\":26239},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":26924,\"start\":26906},{\"attributes\":{\"n\":\"4.2\"},\"end\":27542,\"start\":27505},{\"attributes\":{\"n\":\"4.3\"},\"end\":32101,\"start\":32086},{\"attributes\":{\"n\":\"4.4\"},\"end\":32755,\"start\":32741},{\"attributes\":{\"n\":\"4.5\"},\"end\":34170,\"start\":34156},{\"attributes\":{\"n\":\"5\"},\"end\":35832,\"start\":35808},{\"attributes\":{\"n\":\"5.1\"},\"end\":36780,\"start\":36761},{\"attributes\":{\"n\":\"5.2\"},\"end\":39682,\"start\":39664},{\"attributes\":{\"n\":\"5.3\"},\"end\":42338,\"start\":42322},{\"attributes\":{\"n\":\"5.4\"},\"end\":43030,\"start\":42974},{\"attributes\":{\"n\":\"5.4.1\"},\"end\":43070,\"start\":43033},{\"attributes\":{\"n\":\"5.4.2\"},\"end\":45648,\"start\":45599},{\"attributes\":{\"n\":\"5.4.3\"},\"end\":46956,\"start\":46912},{\"attributes\":{\"n\":\"5.4.4\"},\"end\":47610,\"start\":47555},{\"attributes\":{\"n\":\"5.5\"},\"end\":49183,\"start\":49109},{\"attributes\":{\"n\":\"5.6\"},\"end\":50238,\"start\":50185},{\"attributes\":{\"n\":\"5.7\"},\"end\":52481,\"start\":52422},{\"attributes\":{\"n\":\"5.8\"},\"end\":54937,\"start\":54907},{\"attributes\":{\"n\":\"5.8.1\"},\"end\":55065,\"start\":55055},{\"attributes\":{\"n\":\"5.8.2\"},\"end\":56176,\"start\":56166},{\"attributes\":{\"n\":\"6\"},\"end\":57431,\"start\":57412},{\"attributes\":{\"n\":\"7\"},\"end\":58718,\"start\":58706},{\"attributes\":{\"n\":\"7.1\"},\"end\":58745,\"start\":58721},{\"attributes\":{\"n\":\"7.2\"},\"end\":60455,\"start\":60430},{\"attributes\":{\"n\":\"7.3\"},\"end\":62683,\"start\":62656},{\"attributes\":{\"n\":\"8\"},\"end\":63911,\"start\":63901},{\"attributes\":{\"n\":\"9\"},\"end\":64608,\"start\":64593},{\"end\":64677,\"start\":64667},{\"end\":64739,\"start\":64729},{\"end\":64794,\"start\":64784},{\"end\":64977,\"start\":64967},{\"end\":65054,\"start\":65044},{\"end\":65514,\"start\":65504},{\"end\":65645,\"start\":65635},{\"end\":66744,\"start\":66723},{\"end\":66898,\"start\":66887},{\"end\":68173,\"start\":68164},{\"end\":68871,\"start\":68862},{\"end\":69219,\"start\":69210},{\"end\":69488,\"start\":69479},{\"end\":69722,\"start\":69713},{\"end\":70009,\"start\":70000},{\"end\":70516,\"start\":70507},{\"end\":72777,\"start\":72768}]", "table": "[{\"end\":68860,\"start\":68244},{\"end\":69208,\"start\":68952},{\"end\":69477,\"start\":69297},{\"end\":69711,\"start\":69540},{\"end\":69998,\"start\":69772},{\"end\":70505,\"start\":70082},{\"end\":72766,\"start\":71488},{\"end\":72994,\"start\":72846}]", "figure_caption": "[{\"end\":64727,\"start\":64679},{\"end\":64782,\"start\":64741},{\"end\":64965,\"start\":64796},{\"end\":65042,\"start\":64979},{\"end\":65502,\"start\":65056},{\"end\":65552,\"start\":65516},{\"end\":65593,\"start\":65555},{\"end\":65633,\"start\":65596},{\"end\":65685,\"start\":65647},{\"end\":66721,\"start\":65688},{\"end\":66885,\"start\":66748},{\"end\":66970,\"start\":66901},{\"end\":68162,\"start\":66973},{\"end\":68244,\"start\":68175},{\"end\":68952,\"start\":68873},{\"end\":69297,\"start\":69221},{\"end\":69540,\"start\":69490},{\"end\":69772,\"start\":69724},{\"end\":70082,\"start\":70011},{\"end\":71488,\"start\":70518},{\"end\":72846,\"start\":72779}]", "figure_ref": "[{\"end\":10155,\"start\":10147},{\"end\":10683,\"start\":10671},{\"end\":10743,\"start\":10734},{\"end\":10912,\"start\":10900},{\"end\":11785,\"start\":11777},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15809,\"start\":15801},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16110,\"start\":16098},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17761,\"start\":17752},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21675,\"start\":21667},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21803,\"start\":21795},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21955,\"start\":21947},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22625,\"start\":22617},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22967,\"start\":22959},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23202,\"start\":23194},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23457,\"start\":23449},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23740,\"start\":23732},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24519,\"start\":24511},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26799,\"start\":26791},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28517,\"start\":28509},{\"end\":29478,\"start\":29470},{\"end\":29986,\"start\":29978},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38881,\"start\":38866},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38891,\"start\":38883},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38957,\"start\":38949},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":39065,\"start\":39056},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":39239,\"start\":39231},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":39357,\"start\":39349},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":39428,\"start\":39420},{\"end\":49693,\"start\":49685},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":50387,\"start\":50370},{\"end\":50555,\"start\":50546},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":51702,\"start\":51693}]", "bib_author_first_name": "[{\"end\":73030,\"start\":73029},{\"end\":73032,\"start\":73031},{\"end\":73042,\"start\":73041},{\"end\":73044,\"start\":73043},{\"end\":73389,\"start\":73388},{\"end\":73398,\"start\":73397},{\"end\":73407,\"start\":73406},{\"end\":73409,\"start\":73408},{\"end\":73774,\"start\":73773},{\"end\":73783,\"start\":73782},{\"end\":73791,\"start\":73790},{\"end\":73798,\"start\":73797},{\"end\":73807,\"start\":73806},{\"end\":73814,\"start\":73813},{\"end\":73823,\"start\":73822},{\"end\":74207,\"start\":74206},{\"end\":74214,\"start\":74213},{\"end\":74220,\"start\":74219},{\"end\":74228,\"start\":74227},{\"end\":74236,\"start\":74235},{\"end\":74243,\"start\":74242},{\"end\":74249,\"start\":74248},{\"end\":74255,\"start\":74254},{\"end\":74664,\"start\":74660},{\"end\":74666,\"start\":74665},{\"end\":74672,\"start\":74671},{\"end\":74674,\"start\":74673},{\"end\":74686,\"start\":74685},{\"end\":75119,\"start\":75118},{\"end\":75126,\"start\":75125},{\"end\":75135,\"start\":75134},{\"end\":75144,\"start\":75143},{\"end\":75476,\"start\":75475},{\"end\":75485,\"start\":75484},{\"end\":75494,\"start\":75493},{\"end\":75503,\"start\":75502},{\"end\":75511,\"start\":75510},{\"end\":75519,\"start\":75518},{\"end\":75528,\"start\":75527},{\"end\":75768,\"start\":75767},{\"end\":76111,\"start\":76110},{\"end\":76120,\"start\":76119},{\"end\":76129,\"start\":76128},{\"end\":76138,\"start\":76137},{\"end\":76145,\"start\":76144},{\"end\":76841,\"start\":76840},{\"end\":76843,\"start\":76842},{\"end\":76852,\"start\":76851},{\"end\":76861,\"start\":76860},{\"end\":76868,\"start\":76867},{\"end\":76876,\"start\":76875},{\"end\":77387,\"start\":77386},{\"end\":77393,\"start\":77392},{\"end\":77401,\"start\":77400},{\"end\":77410,\"start\":77409},{\"end\":77417,\"start\":77416},{\"end\":77672,\"start\":77671},{\"end\":77680,\"start\":77679},{\"end\":77686,\"start\":77685},{\"end\":77694,\"start\":77693},{\"end\":77701,\"start\":77700},{\"end\":77710,\"start\":77709},{\"end\":77716,\"start\":77715},{\"end\":77725,\"start\":77724},{\"end\":78243,\"start\":78239},{\"end\":78250,\"start\":78249},{\"end\":78259,\"start\":78258},{\"end\":78270,\"start\":78269},{\"end\":78646,\"start\":78645},{\"end\":78652,\"start\":78651},{\"end\":78662,\"start\":78661},{\"end\":78670,\"start\":78669},{\"end\":78679,\"start\":78678},{\"end\":78686,\"start\":78685},{\"end\":79188,\"start\":79187},{\"end\":79194,\"start\":79193},{\"end\":79200,\"start\":79199},{\"end\":79209,\"start\":79208},{\"end\":79855,\"start\":79854},{\"end\":79864,\"start\":79863},{\"end\":79870,\"start\":79869},{\"end\":79876,\"start\":79875},{\"end\":79884,\"start\":79883},{\"end\":79893,\"start\":79892},{\"end\":79902,\"start\":79901},{\"end\":80262,\"start\":80261},{\"end\":80264,\"start\":80263},{\"end\":80272,\"start\":80271},{\"end\":80281,\"start\":80280},{\"end\":80293,\"start\":80292},{\"end\":80295,\"start\":80294},{\"end\":80801,\"start\":80800},{\"end\":80813,\"start\":80812},{\"end\":80823,\"start\":80822},{\"end\":81399,\"start\":81398},{\"end\":81401,\"start\":81400},{\"end\":81408,\"start\":81407},{\"end\":81419,\"start\":81418},{\"end\":81421,\"start\":81420},{\"end\":81431,\"start\":81430},{\"end\":82090,\"start\":82089},{\"end\":82096,\"start\":82095},{\"end\":82105,\"start\":82104},{\"end\":82112,\"start\":82111},{\"end\":82114,\"start\":82113},{\"end\":82121,\"start\":82120},{\"end\":82496,\"start\":82495},{\"end\":82511,\"start\":82510},{\"end\":82519,\"start\":82518},{\"end\":82532,\"start\":82531},{\"end\":82818,\"start\":82817},{\"end\":82820,\"start\":82819},{\"end\":82827,\"start\":82826},{\"end\":82833,\"start\":82832},{\"end\":82842,\"start\":82841},{\"end\":83246,\"start\":83245},{\"end\":83248,\"start\":83247},{\"end\":83256,\"start\":83255},{\"end\":83266,\"start\":83265},{\"end\":83276,\"start\":83275},{\"end\":83287,\"start\":83286},{\"end\":83593,\"start\":83592},{\"end\":83597,\"start\":83594},{\"end\":83606,\"start\":83605},{\"end\":83618,\"start\":83617},{\"end\":83620,\"start\":83619},{\"end\":84239,\"start\":84238},{\"end\":84506,\"start\":84505},{\"end\":84527,\"start\":84526},{\"end\":84529,\"start\":84528},{\"end\":84904,\"start\":84903},{\"end\":84912,\"start\":84911},{\"end\":84923,\"start\":84922},{\"end\":84933,\"start\":84932},{\"end\":85441,\"start\":85440},{\"end\":85449,\"start\":85448},{\"end\":85460,\"start\":85459},{\"end\":85462,\"start\":85461},{\"end\":85830,\"start\":85829},{\"end\":85837,\"start\":85836},{\"end\":85846,\"start\":85845},{\"end\":85853,\"start\":85852},{\"end\":85860,\"start\":85859},{\"end\":86171,\"start\":86170},{\"end\":86173,\"start\":86172},{\"end\":86183,\"start\":86182},{\"end\":86185,\"start\":86184},{\"end\":86682,\"start\":86681},{\"end\":86689,\"start\":86688},{\"end\":86699,\"start\":86698},{\"end\":86709,\"start\":86708},{\"end\":86717,\"start\":86716},{\"end\":86726,\"start\":86725},{\"end\":86734,\"start\":86733},{\"end\":87182,\"start\":87181},{\"end\":87194,\"start\":87193},{\"end\":87202,\"start\":87201},{\"end\":87215,\"start\":87214},{\"end\":87226,\"start\":87225},{\"end\":87735,\"start\":87734},{\"end\":87748,\"start\":87747},{\"end\":87756,\"start\":87755},{\"end\":88062,\"start\":88061},{\"end\":88071,\"start\":88070},{\"end\":88075,\"start\":88072},{\"end\":88092,\"start\":88091},{\"end\":88102,\"start\":88101},{\"end\":88289,\"start\":88288},{\"end\":88489,\"start\":88488},{\"end\":88495,\"start\":88494},{\"end\":88501,\"start\":88500},{\"end\":88508,\"start\":88507},{\"end\":88514,\"start\":88513},{\"end\":88520,\"start\":88519},{\"end\":88640,\"start\":88639},{\"end\":88651,\"start\":88650},{\"end\":88661,\"start\":88660},{\"end\":88669,\"start\":88668},{\"end\":88971,\"start\":88970},{\"end\":88982,\"start\":88981},{\"end\":88984,\"start\":88983},{\"end\":89229,\"start\":89228},{\"end\":89236,\"start\":89235},{\"end\":89244,\"start\":89243},{\"end\":89252,\"start\":89251},{\"end\":89258,\"start\":89257},{\"end\":89266,\"start\":89265},{\"end\":89272,\"start\":89271},{\"end\":89274,\"start\":89273},{\"end\":89762,\"start\":89761},{\"end\":89764,\"start\":89763},{\"end\":89774,\"start\":89773},{\"end\":89783,\"start\":89782},{\"end\":89792,\"start\":89791},{\"end\":89810,\"start\":89809},{\"end\":90181,\"start\":90180},{\"end\":90190,\"start\":90189},{\"end\":90200,\"start\":90199},{\"end\":90429,\"start\":90428},{\"end\":90437,\"start\":90436},{\"end\":90448,\"start\":90447},{\"end\":90469,\"start\":90468},{\"end\":90949,\"start\":90948},{\"end\":91227,\"start\":91226},{\"end\":91235,\"start\":91234},{\"end\":91237,\"start\":91236},{\"end\":91440,\"start\":91439},{\"end\":91454,\"start\":91453},{\"end\":91649,\"start\":91648},{\"end\":91658,\"start\":91657},{\"end\":91660,\"start\":91659},{\"end\":91926,\"start\":91925},{\"end\":91932,\"start\":91931},{\"end\":91941,\"start\":91940},{\"end\":91949,\"start\":91948},{\"end\":92227,\"start\":92226},{\"end\":92229,\"start\":92228},{\"end\":92239,\"start\":92238},{\"end\":92241,\"start\":92240},{\"end\":92471,\"start\":92470},{\"end\":92473,\"start\":92472},{\"end\":92692,\"start\":92691},{\"end\":92694,\"start\":92693},{\"end\":92705,\"start\":92704},{\"end\":92894,\"start\":92893},{\"end\":92907,\"start\":92906},{\"end\":92914,\"start\":92913},{\"end\":92916,\"start\":92915},{\"end\":92925,\"start\":92924},{\"end\":93155,\"start\":93154},{\"end\":93347,\"start\":93346},{\"end\":93349,\"start\":93348},{\"end\":93358,\"start\":93357},{\"end\":93360,\"start\":93359},{\"end\":93631,\"start\":93630},{\"end\":93641,\"start\":93640},{\"end\":93650,\"start\":93649},{\"end\":93652,\"start\":93651},{\"end\":93664,\"start\":93663},{\"end\":93672,\"start\":93671},{\"end\":93681,\"start\":93680},{\"end\":93702,\"start\":93701},{\"end\":93719,\"start\":93718},{\"end\":93733,\"start\":93732},{\"end\":93751,\"start\":93750},{\"end\":93762,\"start\":93761},{\"end\":94191,\"start\":94190},{\"end\":94199,\"start\":94198},{\"end\":94208,\"start\":94207},{\"end\":94216,\"start\":94215},{\"end\":94483,\"start\":94482},{\"end\":94490,\"start\":94489},{\"end\":94492,\"start\":94491},{\"end\":94505,\"start\":94504},{\"end\":94753,\"start\":94752},{\"end\":94755,\"start\":94754},{\"end\":94762,\"start\":94761},{\"end\":94771,\"start\":94770},{\"end\":94773,\"start\":94772},{\"end\":95037,\"start\":95036},{\"end\":95050,\"start\":95049},{\"end\":95066,\"start\":95065},{\"end\":95080,\"start\":95079},{\"end\":95088,\"start\":95087},{\"end\":95096,\"start\":95095},{\"end\":95431,\"start\":95430},{\"end\":95439,\"start\":95438},{\"end\":95447,\"start\":95446},{\"end\":95455,\"start\":95454},{\"end\":95461,\"start\":95460},{\"end\":95470,\"start\":95469},{\"end\":96123,\"start\":96122},{\"end\":96302,\"start\":96301},{\"end\":96310,\"start\":96309},{\"end\":96312,\"start\":96311},{\"end\":96523,\"start\":96519},{\"end\":96532,\"start\":96531},{\"end\":96540,\"start\":96539},{\"end\":96542,\"start\":96541},{\"end\":96764,\"start\":96763},{\"end\":96770,\"start\":96769},{\"end\":96776,\"start\":96775},{\"end\":96783,\"start\":96782},{\"end\":96789,\"start\":96788},{\"end\":97132,\"start\":97131},{\"end\":97144,\"start\":97143},{\"end\":97154,\"start\":97153},{\"end\":97162,\"start\":97161},{\"end\":97164,\"start\":97163},{\"end\":97708,\"start\":97707},{\"end\":97717,\"start\":97716},{\"end\":97726,\"start\":97725},{\"end\":98127,\"start\":98126},{\"end\":98131,\"start\":98128},{\"end\":98141,\"start\":98140},{\"end\":98456,\"start\":98455},{\"end\":98462,\"start\":98461},{\"end\":98468,\"start\":98467},{\"end\":98475,\"start\":98474},{\"end\":98481,\"start\":98480},{\"end\":98786,\"start\":98785},{\"end\":98798,\"start\":98797},{\"end\":99375,\"start\":99374},{\"end\":99377,\"start\":99376},{\"end\":99594,\"start\":99593},{\"end\":99596,\"start\":99595},{\"end\":99605,\"start\":99604},{\"end\":99617,\"start\":99616},{\"end\":99629,\"start\":99628},{\"end\":100016,\"start\":100015},{\"end\":100018,\"start\":100017},{\"end\":100334,\"start\":100333},{\"end\":100341,\"start\":100340},{\"end\":100349,\"start\":100348},{\"end\":100357,\"start\":100356},{\"end\":100604,\"start\":100603},{\"end\":100614,\"start\":100613},{\"end\":100616,\"start\":100615},{\"end\":100623,\"start\":100622},{\"end\":101013,\"start\":101012},{\"end\":101030,\"start\":101029},{\"end\":101042,\"start\":101041},{\"end\":101044,\"start\":101043},{\"end\":101059,\"start\":101058},{\"end\":101066,\"start\":101065},{\"end\":101375,\"start\":101374},{\"end\":101381,\"start\":101380},{\"end\":101390,\"start\":101389},{\"end\":101396,\"start\":101395},{\"end\":101403,\"start\":101402},{\"end\":101410,\"start\":101409},{\"end\":101641,\"start\":101640},{\"end\":101657,\"start\":101656},{\"end\":101669,\"start\":101668},{\"end\":101681,\"start\":101680},{\"end\":101685,\"start\":101682},{\"end\":101693,\"start\":101692},{\"end\":101953,\"start\":101952},{\"end\":101966,\"start\":101965},{\"end\":101977,\"start\":101976},{\"end\":101979,\"start\":101978},{\"end\":102259,\"start\":102258},{\"end\":102272,\"start\":102271},{\"end\":102281,\"start\":102280},{\"end\":102299,\"start\":102298},{\"end\":102511,\"start\":102510},{\"end\":102517,\"start\":102516},{\"end\":102526,\"start\":102525},{\"end\":102535,\"start\":102534},{\"end\":103019,\"start\":103018},{\"end\":103026,\"start\":103025},{\"end\":103032,\"start\":103031},{\"end\":103041,\"start\":103040},{\"end\":103049,\"start\":103048},{\"end\":103487,\"start\":103486},{\"end\":103493,\"start\":103492},{\"end\":103503,\"start\":103502},{\"end\":103519,\"start\":103518},{\"end\":103754,\"start\":103753},{\"end\":103763,\"start\":103762},{\"end\":103772,\"start\":103771},{\"end\":103782,\"start\":103781},{\"end\":103800,\"start\":103799},{\"end\":103810,\"start\":103809},{\"end\":104069,\"start\":104068},{\"end\":104082,\"start\":104081},{\"end\":104093,\"start\":104092},{\"end\":104102,\"start\":104101},{\"end\":104108,\"start\":104107},{\"end\":104116,\"start\":104115},{\"end\":104350,\"start\":104349},{\"end\":104362,\"start\":104361},{\"end\":104610,\"start\":104609},{\"end\":104612,\"start\":104611},{\"end\":104619,\"start\":104618},{\"end\":104627,\"start\":104626},{\"end\":104828,\"start\":104827},{\"end\":104836,\"start\":104835},{\"end\":104852,\"start\":104851},{\"end\":104854,\"start\":104853},{\"end\":104865,\"start\":104864},{\"end\":104876,\"start\":104875},{\"end\":104886,\"start\":104885},{\"end\":104894,\"start\":104893},{\"end\":105168,\"start\":105167},{\"end\":105181,\"start\":105180},{\"end\":105191,\"start\":105190},{\"end\":105201,\"start\":105200},{\"end\":105501,\"start\":105500},{\"end\":105513,\"start\":105512},{\"end\":105520,\"start\":105519},{\"end\":105776,\"start\":105775},{\"end\":105784,\"start\":105783},{\"end\":106250,\"start\":106249},{\"end\":106257,\"start\":106256},{\"end\":106264,\"start\":106263},{\"end\":106266,\"start\":106265},{\"end\":106276,\"start\":106275},{\"end\":106282,\"start\":106281},{\"end\":106290,\"start\":106289},{\"end\":106809,\"start\":106808},{\"end\":106811,\"start\":106810},{\"end\":106821,\"start\":106820},{\"end\":106823,\"start\":106822},{\"end\":106837,\"start\":106836},{\"end\":106839,\"start\":106838},{\"end\":106848,\"start\":106847},{\"end\":107186,\"start\":107185},{\"end\":107194,\"start\":107193},{\"end\":107209,\"start\":107208},{\"end\":107219,\"start\":107218},{\"end\":107221,\"start\":107220},{\"end\":107229,\"start\":107228},{\"end\":107239,\"start\":107238},{\"end\":107241,\"start\":107240},{\"end\":107254,\"start\":107253},{\"end\":107264,\"start\":107263},{\"end\":107278,\"start\":107277},{\"end\":107280,\"start\":107279},{\"end\":107293,\"start\":107292},{\"end\":107652,\"start\":107651},{\"end\":107659,\"start\":107658},{\"end\":107671,\"start\":107670},{\"end\":107680,\"start\":107679},{\"end\":107682,\"start\":107681},{\"end\":107689,\"start\":107688},{\"end\":107698,\"start\":107697},{\"end\":107709,\"start\":107708},{\"end\":108093,\"start\":108092},{\"end\":108100,\"start\":108099},{\"end\":108108,\"start\":108107},{\"end\":108117,\"start\":108116},{\"end\":108123,\"start\":108122},{\"end\":108125,\"start\":108124},{\"end\":108484,\"start\":108483},{\"end\":108490,\"start\":108489},{\"end\":108500,\"start\":108499},{\"end\":108510,\"start\":108509},{\"end\":108520,\"start\":108519},{\"end\":108527,\"start\":108526},{\"end\":108787,\"start\":108786},{\"end\":108796,\"start\":108795}]", "bib_author_last_name": "[{\"end\":73039,\"start\":73033},{\"end\":73052,\"start\":73045},{\"end\":73395,\"start\":73390},{\"end\":73404,\"start\":73399},{\"end\":73415,\"start\":73410},{\"end\":73780,\"start\":73775},{\"end\":73788,\"start\":73784},{\"end\":73795,\"start\":73792},{\"end\":73804,\"start\":73799},{\"end\":73811,\"start\":73808},{\"end\":73820,\"start\":73815},{\"end\":73829,\"start\":73824},{\"end\":74211,\"start\":74208},{\"end\":74217,\"start\":74215},{\"end\":74225,\"start\":74221},{\"end\":74233,\"start\":74229},{\"end\":74240,\"start\":74237},{\"end\":74246,\"start\":74244},{\"end\":74252,\"start\":74250},{\"end\":74258,\"start\":74256},{\"end\":74669,\"start\":74667},{\"end\":74683,\"start\":74675},{\"end\":74689,\"start\":74687},{\"end\":75123,\"start\":75120},{\"end\":75132,\"start\":75127},{\"end\":75141,\"start\":75136},{\"end\":75153,\"start\":75145},{\"end\":75482,\"start\":75477},{\"end\":75491,\"start\":75486},{\"end\":75500,\"start\":75495},{\"end\":75508,\"start\":75504},{\"end\":75516,\"start\":75512},{\"end\":75525,\"start\":75520},{\"end\":75532,\"start\":75529},{\"end\":75774,\"start\":75769},{\"end\":76117,\"start\":76112},{\"end\":76126,\"start\":76121},{\"end\":76135,\"start\":76130},{\"end\":76142,\"start\":76139},{\"end\":76154,\"start\":76146},{\"end\":76849,\"start\":76844},{\"end\":76858,\"start\":76853},{\"end\":76865,\"start\":76862},{\"end\":76873,\"start\":76869},{\"end\":76882,\"start\":76877},{\"end\":77390,\"start\":77388},{\"end\":77398,\"start\":77394},{\"end\":77407,\"start\":77402},{\"end\":77414,\"start\":77411},{\"end\":77423,\"start\":77418},{\"end\":77677,\"start\":77673},{\"end\":77683,\"start\":77681},{\"end\":77691,\"start\":77687},{\"end\":77698,\"start\":77695},{\"end\":77707,\"start\":77702},{\"end\":77713,\"start\":77711},{\"end\":77722,\"start\":77717},{\"end\":77729,\"start\":77726},{\"end\":78247,\"start\":78244},{\"end\":78256,\"start\":78251},{\"end\":78267,\"start\":78260},{\"end\":78276,\"start\":78271},{\"end\":78649,\"start\":78647},{\"end\":78659,\"start\":78653},{\"end\":78667,\"start\":78663},{\"end\":78676,\"start\":78671},{\"end\":78683,\"start\":78680},{\"end\":78692,\"start\":78687},{\"end\":79191,\"start\":79189},{\"end\":79197,\"start\":79195},{\"end\":79206,\"start\":79201},{\"end\":79215,\"start\":79210},{\"end\":79861,\"start\":79856},{\"end\":79867,\"start\":79865},{\"end\":79873,\"start\":79871},{\"end\":79881,\"start\":79877},{\"end\":79890,\"start\":79885},{\"end\":79899,\"start\":79894},{\"end\":79911,\"start\":79903},{\"end\":80269,\"start\":80265},{\"end\":80278,\"start\":80273},{\"end\":80290,\"start\":80282},{\"end\":80301,\"start\":80296},{\"end\":80810,\"start\":80802},{\"end\":80820,\"start\":80814},{\"end\":80829,\"start\":80824},{\"end\":81405,\"start\":81402},{\"end\":81416,\"start\":81409},{\"end\":81428,\"start\":81422},{\"end\":81444,\"start\":81432},{\"end\":82093,\"start\":82091},{\"end\":82102,\"start\":82097},{\"end\":82109,\"start\":82106},{\"end\":82118,\"start\":82115},{\"end\":82127,\"start\":82122},{\"end\":82508,\"start\":82497},{\"end\":82516,\"start\":82512},{\"end\":82529,\"start\":82520},{\"end\":82539,\"start\":82533},{\"end\":82824,\"start\":82821},{\"end\":82830,\"start\":82828},{\"end\":82839,\"start\":82834},{\"end\":82851,\"start\":82843},{\"end\":82865,\"start\":82853},{\"end\":83253,\"start\":83249},{\"end\":83263,\"start\":83257},{\"end\":83273,\"start\":83267},{\"end\":83284,\"start\":83277},{\"end\":83291,\"start\":83288},{\"end\":83603,\"start\":83598},{\"end\":83615,\"start\":83607},{\"end\":83632,\"start\":83621},{\"end\":84254,\"start\":84240},{\"end\":84524,\"start\":84507},{\"end\":84535,\"start\":84530},{\"end\":84909,\"start\":84905},{\"end\":84920,\"start\":84913},{\"end\":84930,\"start\":84924},{\"end\":84945,\"start\":84934},{\"end\":85446,\"start\":85442},{\"end\":85457,\"start\":85450},{\"end\":85468,\"start\":85463},{\"end\":85834,\"start\":85831},{\"end\":85843,\"start\":85838},{\"end\":85850,\"start\":85847},{\"end\":85857,\"start\":85854},{\"end\":85863,\"start\":85861},{\"end\":86180,\"start\":86174},{\"end\":86192,\"start\":86186},{\"end\":86686,\"start\":86683},{\"end\":86696,\"start\":86690},{\"end\":86706,\"start\":86700},{\"end\":86714,\"start\":86710},{\"end\":86723,\"start\":86718},{\"end\":86731,\"start\":86727},{\"end\":86743,\"start\":86735},{\"end\":87191,\"start\":87183},{\"end\":87199,\"start\":87195},{\"end\":87212,\"start\":87203},{\"end\":87223,\"start\":87216},{\"end\":87240,\"start\":87227},{\"end\":87745,\"start\":87736},{\"end\":87753,\"start\":87749},{\"end\":87763,\"start\":87757},{\"end\":88068,\"start\":88063},{\"end\":88089,\"start\":88076},{\"end\":88099,\"start\":88093},{\"end\":88108,\"start\":88103},{\"end\":88301,\"start\":88290},{\"end\":88492,\"start\":88490},{\"end\":88498,\"start\":88496},{\"end\":88505,\"start\":88502},{\"end\":88511,\"start\":88509},{\"end\":88517,\"start\":88515},{\"end\":88524,\"start\":88521},{\"end\":88648,\"start\":88641},{\"end\":88658,\"start\":88652},{\"end\":88666,\"start\":88662},{\"end\":88677,\"start\":88670},{\"end\":88979,\"start\":88972},{\"end\":88989,\"start\":88985},{\"end\":89233,\"start\":89230},{\"end\":89241,\"start\":89237},{\"end\":89249,\"start\":89245},{\"end\":89255,\"start\":89253},{\"end\":89263,\"start\":89259},{\"end\":89269,\"start\":89267},{\"end\":89277,\"start\":89275},{\"end\":89771,\"start\":89765},{\"end\":89780,\"start\":89775},{\"end\":89789,\"start\":89784},{\"end\":89807,\"start\":89793},{\"end\":89815,\"start\":89811},{\"end\":90187,\"start\":90182},{\"end\":90197,\"start\":90191},{\"end\":90208,\"start\":90201},{\"end\":90434,\"start\":90430},{\"end\":90445,\"start\":90438},{\"end\":90466,\"start\":90449},{\"end\":90473,\"start\":90470},{\"end\":90959,\"start\":90950},{\"end\":91232,\"start\":91228},{\"end\":91241,\"start\":91238},{\"end\":91451,\"start\":91441},{\"end\":91466,\"start\":91455},{\"end\":91655,\"start\":91650},{\"end\":91668,\"start\":91661},{\"end\":91929,\"start\":91927},{\"end\":91938,\"start\":91933},{\"end\":91946,\"start\":91942},{\"end\":91952,\"start\":91950},{\"end\":92236,\"start\":92230},{\"end\":92247,\"start\":92242},{\"end\":92482,\"start\":92474},{\"end\":92702,\"start\":92695},{\"end\":92711,\"start\":92706},{\"end\":92904,\"start\":92895},{\"end\":92911,\"start\":92908},{\"end\":92922,\"start\":92917},{\"end\":92938,\"start\":92926},{\"end\":93161,\"start\":93156},{\"end\":93355,\"start\":93350},{\"end\":93371,\"start\":93361},{\"end\":93638,\"start\":93632},{\"end\":93647,\"start\":93642},{\"end\":93661,\"start\":93653},{\"end\":93669,\"start\":93665},{\"end\":93678,\"start\":93673},{\"end\":93699,\"start\":93682},{\"end\":93716,\"start\":93703},{\"end\":93730,\"start\":93720},{\"end\":93748,\"start\":93734},{\"end\":93759,\"start\":93752},{\"end\":93771,\"start\":93763},{\"end\":94196,\"start\":94192},{\"end\":94205,\"start\":94200},{\"end\":94213,\"start\":94209},{\"end\":94222,\"start\":94217},{\"end\":94487,\"start\":94484},{\"end\":94502,\"start\":94493},{\"end\":94509,\"start\":94506},{\"end\":94759,\"start\":94756},{\"end\":94768,\"start\":94763},{\"end\":94780,\"start\":94774},{\"end\":95047,\"start\":95038},{\"end\":95063,\"start\":95051},{\"end\":95077,\"start\":95067},{\"end\":95085,\"start\":95081},{\"end\":95093,\"start\":95089},{\"end\":95436,\"start\":95432},{\"end\":95444,\"start\":95440},{\"end\":95452,\"start\":95448},{\"end\":95458,\"start\":95456},{\"end\":95467,\"start\":95462},{\"end\":95475,\"start\":95471},{\"end\":96126,\"start\":96124},{\"end\":96307,\"start\":96303},{\"end\":96320,\"start\":96313},{\"end\":96529,\"start\":96524},{\"end\":96537,\"start\":96533},{\"end\":96550,\"start\":96543},{\"end\":96767,\"start\":96765},{\"end\":96773,\"start\":96771},{\"end\":96780,\"start\":96777},{\"end\":96786,\"start\":96784},{\"end\":96793,\"start\":96790},{\"end\":97141,\"start\":97133},{\"end\":97151,\"start\":97145},{\"end\":97159,\"start\":97155},{\"end\":97168,\"start\":97165},{\"end\":97714,\"start\":97709},{\"end\":97723,\"start\":97718},{\"end\":97733,\"start\":97727},{\"end\":98138,\"start\":98132},{\"end\":98150,\"start\":98142},{\"end\":98459,\"start\":98457},{\"end\":98465,\"start\":98463},{\"end\":98472,\"start\":98469},{\"end\":98478,\"start\":98476},{\"end\":98485,\"start\":98482},{\"end\":98795,\"start\":98787},{\"end\":98804,\"start\":98799},{\"end\":99381,\"start\":99378},{\"end\":99602,\"start\":99597},{\"end\":99614,\"start\":99606},{\"end\":99626,\"start\":99618},{\"end\":99635,\"start\":99630},{\"end\":100026,\"start\":100019},{\"end\":100338,\"start\":100335},{\"end\":100346,\"start\":100342},{\"end\":100354,\"start\":100350},{\"end\":100360,\"start\":100358},{\"end\":100611,\"start\":100605},{\"end\":100620,\"start\":100617},{\"end\":100635,\"start\":100624},{\"end\":101027,\"start\":101014},{\"end\":101039,\"start\":101031},{\"end\":101056,\"start\":101045},{\"end\":101063,\"start\":101060},{\"end\":101073,\"start\":101067},{\"end\":101378,\"start\":101376},{\"end\":101387,\"start\":101382},{\"end\":101393,\"start\":101391},{\"end\":101400,\"start\":101397},{\"end\":101407,\"start\":101404},{\"end\":101415,\"start\":101411},{\"end\":101654,\"start\":101642},{\"end\":101666,\"start\":101658},{\"end\":101678,\"start\":101670},{\"end\":101690,\"start\":101686},{\"end\":101705,\"start\":101694},{\"end\":101963,\"start\":101954},{\"end\":101974,\"start\":101967},{\"end\":101982,\"start\":101980},{\"end\":102269,\"start\":102260},{\"end\":102278,\"start\":102273},{\"end\":102296,\"start\":102282},{\"end\":102305,\"start\":102300},{\"end\":102514,\"start\":102512},{\"end\":102523,\"start\":102518},{\"end\":102532,\"start\":102527},{\"end\":102539,\"start\":102536},{\"end\":103023,\"start\":103020},{\"end\":103029,\"start\":103027},{\"end\":103038,\"start\":103033},{\"end\":103046,\"start\":103042},{\"end\":103053,\"start\":103050},{\"end\":103490,\"start\":103488},{\"end\":103500,\"start\":103494},{\"end\":103516,\"start\":103504},{\"end\":103525,\"start\":103520},{\"end\":103760,\"start\":103755},{\"end\":103769,\"start\":103764},{\"end\":103779,\"start\":103773},{\"end\":103797,\"start\":103783},{\"end\":103807,\"start\":103801},{\"end\":103817,\"start\":103811},{\"end\":104079,\"start\":104070},{\"end\":104090,\"start\":104083},{\"end\":104099,\"start\":104094},{\"end\":104105,\"start\":104103},{\"end\":104113,\"start\":104109},{\"end\":104122,\"start\":104117},{\"end\":104359,\"start\":104351},{\"end\":104369,\"start\":104363},{\"end\":104616,\"start\":104613},{\"end\":104624,\"start\":104620},{\"end\":104632,\"start\":104628},{\"end\":104833,\"start\":104829},{\"end\":104849,\"start\":104837},{\"end\":104862,\"start\":104855},{\"end\":104873,\"start\":104866},{\"end\":104883,\"start\":104877},{\"end\":104891,\"start\":104887},{\"end\":104902,\"start\":104895},{\"end\":105178,\"start\":105169},{\"end\":105188,\"start\":105182},{\"end\":105198,\"start\":105192},{\"end\":105205,\"start\":105202},{\"end\":105510,\"start\":105502},{\"end\":105517,\"start\":105514},{\"end\":105527,\"start\":105521},{\"end\":105781,\"start\":105777},{\"end\":105789,\"start\":105785},{\"end\":106254,\"start\":106251},{\"end\":106261,\"start\":106258},{\"end\":106273,\"start\":106267},{\"end\":106279,\"start\":106277},{\"end\":106287,\"start\":106283},{\"end\":106295,\"start\":106291},{\"end\":106818,\"start\":106812},{\"end\":106834,\"start\":106824},{\"end\":106845,\"start\":106840},{\"end\":106856,\"start\":106849},{\"end\":107191,\"start\":107187},{\"end\":107206,\"start\":107195},{\"end\":107216,\"start\":107210},{\"end\":107226,\"start\":107222},{\"end\":107236,\"start\":107230},{\"end\":107251,\"start\":107242},{\"end\":107261,\"start\":107255},{\"end\":107275,\"start\":107265},{\"end\":107290,\"start\":107281},{\"end\":107303,\"start\":107294},{\"end\":107656,\"start\":107653},{\"end\":107668,\"start\":107660},{\"end\":107677,\"start\":107672},{\"end\":107686,\"start\":107683},{\"end\":107695,\"start\":107690},{\"end\":107706,\"start\":107699},{\"end\":107717,\"start\":107710},{\"end\":108097,\"start\":108094},{\"end\":108105,\"start\":108101},{\"end\":108114,\"start\":108109},{\"end\":108120,\"start\":108118},{\"end\":108128,\"start\":108126},{\"end\":108487,\"start\":108485},{\"end\":108497,\"start\":108491},{\"end\":108507,\"start\":108501},{\"end\":108517,\"start\":108511},{\"end\":108524,\"start\":108521},{\"end\":108536,\"start\":108528},{\"end\":108793,\"start\":108788},{\"end\":108803,\"start\":108797}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":57517508},\"end\":73327,\"start\":72996},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14846118},\"end\":73691,\"start\":73329},{\"attributes\":{\"id\":\"b2\"},\"end\":73723,\"start\":73693},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3155193},\"end\":74133,\"start\":73725},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3888368},\"end\":74582,\"start\":74135},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13693392},\"end\":75070,\"start\":74584},{\"attributes\":{\"id\":\"b6\"},\"end\":75417,\"start\":75072},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":22050231},\"end\":75731,\"start\":75419},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4319354},\"end\":76005,\"start\":75733},{\"attributes\":{\"doi\":\"10.1145/3238147.3238187\",\"id\":\"b9\",\"matched_paper_id\":52071803},\"end\":76754,\"start\":76007},{\"attributes\":{\"doi\":\"10.1109/ICST.2019.00033\",\"id\":\"b10\",\"matched_paper_id\":131763462},\"end\":77338,\"start\":76756},{\"attributes\":{\"doi\":\"arXiv:1905.01833\",\"id\":\"b11\"},\"end\":77589,\"start\":77340},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":57189251},\"end\":78160,\"start\":77591},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":22261945},\"end\":78540,\"start\":78162},{\"attributes\":{\"id\":\"b14\"},\"end\":78570,\"start\":78542},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":211844487},\"end\":79100,\"start\":78572},{\"attributes\":{\"doi\":\"10.1145/3293882.3330574\",\"id\":\"b16\",\"matched_paper_id\":195891657},\"end\":79773,\"start\":79102},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":150145233},\"end\":80164,\"start\":79775},{\"attributes\":{\"doi\":\"10.1109/ICSE.2015.47\",\"id\":\"b18\",\"matched_paper_id\":10413560},\"end\":80750,\"start\":80166},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49671483},\"end\":81274,\"start\":80752},{\"attributes\":{\"doi\":\"10.1145/3293882.3330559\",\"id\":\"b20\"},\"end\":81351,\"start\":81276},{\"attributes\":{\"doi\":\"10.1145/2950290.2950295\",\"id\":\"b21\",\"matched_paper_id\":14496719},\"end\":82027,\"start\":81353},{\"attributes\":{\"id\":\"b22\"},\"end\":82255,\"start\":82029},{\"attributes\":{\"id\":\"b23\"},\"end\":82437,\"start\":82257},{\"attributes\":{\"doi\":\"arXiv:1709.08439\",\"id\":\"b24\"},\"end\":82721,\"start\":82439},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8397716},\"end\":83193,\"start\":82723},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7165993},\"end\":83526,\"start\":83195},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9333486},\"end\":84170,\"start\":83528},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14585311},\"end\":84442,\"start\":84172},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6706547},\"end\":84697,\"start\":84444},{\"attributes\":{\"id\":\"b30\"},\"end\":84845,\"start\":84699},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8820379},\"end\":85366,\"start\":84847},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10548729},\"end\":85769,\"start\":85368},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4808574},\"end\":86087,\"start\":85771},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1736602},\"end\":86586,\"start\":86089},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":15979705},\"end\":87111,\"start\":86588},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9790585},\"end\":87656,\"start\":87113},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2723946},\"end\":87996,\"start\":87658},{\"attributes\":{\"id\":\"b38\"},\"end\":88239,\"start\":87998},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":11715509},\"end\":88430,\"start\":88241},{\"attributes\":{\"id\":\"b40\"},\"end\":88637,\"start\":88432},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b41\"},\"end\":88909,\"start\":88639},{\"attributes\":{\"doi\":\"arXiv:1606.02960\",\"id\":\"b42\"},\"end\":89147,\"start\":88911},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":52069701},\"end\":89714,\"start\":89149},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":12834606},\"end\":90120,\"start\":89716},{\"attributes\":{\"doi\":\"arXiv:1612.04426\",\"id\":\"b45\"},\"end\":90375,\"start\":90122},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":1667047},\"end\":90874,\"start\":90377},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":10959945},\"end\":91145,\"start\":90876},{\"attributes\":{\"doi\":\"arXiv:1206.6426\",\"id\":\"b48\"},\"end\":91413,\"start\":91147},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1915014},\"end\":91605,\"start\":91415},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":9166388},\"end\":91856,\"start\":91607},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":3439214},\"end\":92184,\"start\":91858},{\"attributes\":{\"id\":\"b52\"},\"end\":92377,\"start\":92186},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2332513},\"end\":92677,\"start\":92379},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":208910339},\"end\":92830,\"start\":92679},{\"attributes\":{\"doi\":\"arXiv:1805.09461\",\"id\":\"b55\"},\"end\":93127,\"start\":92832},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":207779694},\"end\":93319,\"start\":93129},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":207779694},\"end\":93560,\"start\":93321},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":515925},\"end\":94063,\"start\":93562},{\"attributes\":{\"id\":\"b59\"},\"end\":94116,\"start\":94065},{\"attributes\":{\"doi\":\"arXiv:1808.01400\",\"id\":\"b60\"},\"end\":94408,\"start\":94118},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":86524089},\"end\":94715,\"start\":94410},{\"attributes\":{\"id\":\"b62\"},\"end\":94916,\"start\":94717},{\"attributes\":{\"id\":\"b63\"},\"end\":95034,\"start\":94918},{\"attributes\":{\"doi\":\"arXiv:1707.05005\",\"id\":\"b64\"},\"end\":95367,\"start\":95036},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":6857205},\"end\":96056,\"start\":95369},{\"attributes\":{\"doi\":\"arXiv:1811.05544\",\"id\":\"b66\"},\"end\":96269,\"start\":96058},{\"attributes\":{\"doi\":\"arXiv:1902.10186\",\"id\":\"b67\"},\"end\":96449,\"start\":96271},{\"attributes\":{\"doi\":\"arXiv:1508.04025\",\"id\":\"b68\"},\"end\":96731,\"start\":96451},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":49584534},\"end\":97065,\"start\":96733},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":11080756},\"end\":97627,\"start\":97067},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":538820},\"end\":97966,\"start\":97629},{\"attributes\":{\"id\":\"b72\"},\"end\":98006,\"start\":97968},{\"attributes\":{\"doi\":\"arXiv:1707.02275\",\"id\":\"b73\"},\"end\":98375,\"start\":98008},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":189927337},\"end\":98689,\"start\":98377},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":7164502},\"end\":99316,\"start\":98691},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":964287},\"end\":99525,\"start\":99318},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":21687876},\"end\":99779,\"start\":99527},{\"attributes\":{\"id\":\"b78\"},\"end\":99931,\"start\":99781},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":202695513},\"end\":100291,\"start\":99933},{\"attributes\":{\"doi\":\"arXiv:1808.09644\",\"id\":\"b80\"},\"end\":100509,\"start\":100293},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":215824512},\"end\":100909,\"start\":100511},{\"attributes\":{\"doi\":\"arXiv:1409.1257\",\"id\":\"b82\"},\"end\":101317,\"start\":100911},{\"attributes\":{\"doi\":\"arXiv:1705.01020\",\"id\":\"b83\"},\"end\":101595,\"start\":101319},{\"attributes\":{\"doi\":\"arXiv:1610.10099\",\"id\":\"b84\"},\"end\":101898,\"start\":101597},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":7961699},\"end\":102203,\"start\":101900},{\"attributes\":{\"doi\":\"arXiv:1612.07600\",\"id\":\"b86\"},\"end\":102489,\"start\":102205},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":11540100},\"end\":102928,\"start\":102491},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":1914494},\"end\":103446,\"start\":102930},{\"attributes\":{\"doi\":\"arXiv:1511.05493\",\"id\":\"b89\"},\"end\":103684,\"start\":103448},{\"attributes\":{\"doi\":\"arXiv:1505.05969\",\"id\":\"b90\"},\"end\":104032,\"start\":103686},{\"attributes\":{\"doi\":\"arXiv:1611.01855\",\"id\":\"b91\"},\"end\":104294,\"start\":104034},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":5737841},\"end\":104566,\"start\":104296},{\"attributes\":{\"doi\":\"arXiv:1608.02715\",\"id\":\"b93\"},\"end\":104778,\"start\":104568},{\"attributes\":{\"doi\":\"arXiv:1603.06744\",\"id\":\"b94\"},\"end\":105110,\"start\":104780},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":2706277},\"end\":105427,\"start\":105112},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b96\"},\"end\":105706,\"start\":105429},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":52068576},\"end\":106172,\"start\":105708},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":52070089},\"end\":106726,\"start\":106174},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":1211821},\"end\":107126,\"start\":106728},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":205242740},\"end\":107565,\"start\":107128},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":2305273},\"end\":108016,\"start\":107567},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":2899486},\"end\":108428,\"start\":108018},{\"attributes\":{\"doi\":\"arXiv:1606.01541\",\"id\":\"b103\"},\"end\":108726,\"start\":108430},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":7473831},\"end\":109157,\"start\":108728}]", "bib_title": "[{\"end\":73027,\"start\":72996},{\"end\":73386,\"start\":73329},{\"end\":73771,\"start\":73725},{\"end\":74204,\"start\":74135},{\"end\":74658,\"start\":74584},{\"end\":75116,\"start\":75072},{\"end\":75473,\"start\":75419},{\"end\":75765,\"start\":75733},{\"end\":76108,\"start\":76007},{\"end\":76838,\"start\":76756},{\"end\":77669,\"start\":77591},{\"end\":78237,\"start\":78162},{\"end\":78643,\"start\":78572},{\"end\":79185,\"start\":79102},{\"end\":79852,\"start\":79775},{\"end\":80259,\"start\":80166},{\"end\":80798,\"start\":80752},{\"end\":81396,\"start\":81353},{\"end\":82815,\"start\":82723},{\"end\":83243,\"start\":83195},{\"end\":83590,\"start\":83528},{\"end\":84236,\"start\":84172},{\"end\":84503,\"start\":84444},{\"end\":84901,\"start\":84847},{\"end\":85438,\"start\":85368},{\"end\":85827,\"start\":85771},{\"end\":86168,\"start\":86089},{\"end\":86679,\"start\":86588},{\"end\":87179,\"start\":87113},{\"end\":87732,\"start\":87658},{\"end\":88286,\"start\":88241},{\"end\":89226,\"start\":89149},{\"end\":89759,\"start\":89716},{\"end\":90426,\"start\":90377},{\"end\":90946,\"start\":90876},{\"end\":91437,\"start\":91415},{\"end\":91646,\"start\":91607},{\"end\":91923,\"start\":91858},{\"end\":92468,\"start\":92379},{\"end\":92689,\"start\":92679},{\"end\":93152,\"start\":93129},{\"end\":93344,\"start\":93321},{\"end\":93628,\"start\":93562},{\"end\":94480,\"start\":94410},{\"end\":95428,\"start\":95369},{\"end\":96761,\"start\":96733},{\"end\":97129,\"start\":97067},{\"end\":97705,\"start\":97629},{\"end\":98453,\"start\":98377},{\"end\":98783,\"start\":98691},{\"end\":99372,\"start\":99318},{\"end\":99591,\"start\":99527},{\"end\":100013,\"start\":99933},{\"end\":100601,\"start\":100511},{\"end\":101950,\"start\":101900},{\"end\":102508,\"start\":102491},{\"end\":103016,\"start\":102930},{\"end\":104347,\"start\":104296},{\"end\":105165,\"start\":105112},{\"end\":105773,\"start\":105708},{\"end\":106247,\"start\":106174},{\"end\":106806,\"start\":106728},{\"end\":107183,\"start\":107128},{\"end\":107649,\"start\":107567},{\"end\":108090,\"start\":108018},{\"end\":108784,\"start\":108728}]", "bib_author": "[{\"end\":73041,\"start\":73029},{\"end\":73054,\"start\":73041},{\"end\":73397,\"start\":73388},{\"end\":73406,\"start\":73397},{\"end\":73417,\"start\":73406},{\"end\":73782,\"start\":73773},{\"end\":73790,\"start\":73782},{\"end\":73797,\"start\":73790},{\"end\":73806,\"start\":73797},{\"end\":73813,\"start\":73806},{\"end\":73822,\"start\":73813},{\"end\":73831,\"start\":73822},{\"end\":74213,\"start\":74206},{\"end\":74219,\"start\":74213},{\"end\":74227,\"start\":74219},{\"end\":74235,\"start\":74227},{\"end\":74242,\"start\":74235},{\"end\":74248,\"start\":74242},{\"end\":74254,\"start\":74248},{\"end\":74260,\"start\":74254},{\"end\":74671,\"start\":74660},{\"end\":74685,\"start\":74671},{\"end\":74691,\"start\":74685},{\"end\":75125,\"start\":75118},{\"end\":75134,\"start\":75125},{\"end\":75143,\"start\":75134},{\"end\":75155,\"start\":75143},{\"end\":75484,\"start\":75475},{\"end\":75493,\"start\":75484},{\"end\":75502,\"start\":75493},{\"end\":75510,\"start\":75502},{\"end\":75518,\"start\":75510},{\"end\":75527,\"start\":75518},{\"end\":75534,\"start\":75527},{\"end\":75776,\"start\":75767},{\"end\":76119,\"start\":76110},{\"end\":76128,\"start\":76119},{\"end\":76137,\"start\":76128},{\"end\":76144,\"start\":76137},{\"end\":76156,\"start\":76144},{\"end\":76851,\"start\":76840},{\"end\":76860,\"start\":76851},{\"end\":76867,\"start\":76860},{\"end\":76875,\"start\":76867},{\"end\":76884,\"start\":76875},{\"end\":77392,\"start\":77386},{\"end\":77400,\"start\":77392},{\"end\":77409,\"start\":77400},{\"end\":77416,\"start\":77409},{\"end\":77425,\"start\":77416},{\"end\":77679,\"start\":77671},{\"end\":77685,\"start\":77679},{\"end\":77693,\"start\":77685},{\"end\":77700,\"start\":77693},{\"end\":77709,\"start\":77700},{\"end\":77715,\"start\":77709},{\"end\":77724,\"start\":77715},{\"end\":77731,\"start\":77724},{\"end\":78249,\"start\":78239},{\"end\":78258,\"start\":78249},{\"end\":78269,\"start\":78258},{\"end\":78278,\"start\":78269},{\"end\":78651,\"start\":78645},{\"end\":78661,\"start\":78651},{\"end\":78669,\"start\":78661},{\"end\":78678,\"start\":78669},{\"end\":78685,\"start\":78678},{\"end\":78694,\"start\":78685},{\"end\":79193,\"start\":79187},{\"end\":79199,\"start\":79193},{\"end\":79208,\"start\":79199},{\"end\":79217,\"start\":79208},{\"end\":79863,\"start\":79854},{\"end\":79869,\"start\":79863},{\"end\":79875,\"start\":79869},{\"end\":79883,\"start\":79875},{\"end\":79892,\"start\":79883},{\"end\":79901,\"start\":79892},{\"end\":79913,\"start\":79901},{\"end\":80271,\"start\":80261},{\"end\":80280,\"start\":80271},{\"end\":80292,\"start\":80280},{\"end\":80303,\"start\":80292},{\"end\":80812,\"start\":80800},{\"end\":80822,\"start\":80812},{\"end\":80831,\"start\":80822},{\"end\":81407,\"start\":81398},{\"end\":81418,\"start\":81407},{\"end\":81430,\"start\":81418},{\"end\":81446,\"start\":81430},{\"end\":82095,\"start\":82089},{\"end\":82104,\"start\":82095},{\"end\":82111,\"start\":82104},{\"end\":82120,\"start\":82111},{\"end\":82129,\"start\":82120},{\"end\":82510,\"start\":82495},{\"end\":82518,\"start\":82510},{\"end\":82531,\"start\":82518},{\"end\":82541,\"start\":82531},{\"end\":82826,\"start\":82817},{\"end\":82832,\"start\":82826},{\"end\":82841,\"start\":82832},{\"end\":82853,\"start\":82841},{\"end\":82867,\"start\":82853},{\"end\":83255,\"start\":83245},{\"end\":83265,\"start\":83255},{\"end\":83275,\"start\":83265},{\"end\":83286,\"start\":83275},{\"end\":83293,\"start\":83286},{\"end\":83605,\"start\":83592},{\"end\":83617,\"start\":83605},{\"end\":83634,\"start\":83617},{\"end\":84256,\"start\":84238},{\"end\":84526,\"start\":84505},{\"end\":84537,\"start\":84526},{\"end\":84911,\"start\":84903},{\"end\":84922,\"start\":84911},{\"end\":84932,\"start\":84922},{\"end\":84947,\"start\":84932},{\"end\":85448,\"start\":85440},{\"end\":85459,\"start\":85448},{\"end\":85470,\"start\":85459},{\"end\":85836,\"start\":85829},{\"end\":85845,\"start\":85836},{\"end\":85852,\"start\":85845},{\"end\":85859,\"start\":85852},{\"end\":85865,\"start\":85859},{\"end\":86182,\"start\":86170},{\"end\":86194,\"start\":86182},{\"end\":86688,\"start\":86681},{\"end\":86698,\"start\":86688},{\"end\":86708,\"start\":86698},{\"end\":86716,\"start\":86708},{\"end\":86725,\"start\":86716},{\"end\":86733,\"start\":86725},{\"end\":86745,\"start\":86733},{\"end\":87193,\"start\":87181},{\"end\":87201,\"start\":87193},{\"end\":87214,\"start\":87201},{\"end\":87225,\"start\":87214},{\"end\":87242,\"start\":87225},{\"end\":87747,\"start\":87734},{\"end\":87755,\"start\":87747},{\"end\":87765,\"start\":87755},{\"end\":88070,\"start\":88061},{\"end\":88091,\"start\":88070},{\"end\":88101,\"start\":88091},{\"end\":88110,\"start\":88101},{\"end\":88303,\"start\":88288},{\"end\":88494,\"start\":88488},{\"end\":88500,\"start\":88494},{\"end\":88507,\"start\":88500},{\"end\":88513,\"start\":88507},{\"end\":88519,\"start\":88513},{\"end\":88526,\"start\":88519},{\"end\":88650,\"start\":88639},{\"end\":88660,\"start\":88650},{\"end\":88668,\"start\":88660},{\"end\":88679,\"start\":88668},{\"end\":88981,\"start\":88970},{\"end\":88991,\"start\":88981},{\"end\":89235,\"start\":89228},{\"end\":89243,\"start\":89235},{\"end\":89251,\"start\":89243},{\"end\":89257,\"start\":89251},{\"end\":89265,\"start\":89257},{\"end\":89271,\"start\":89265},{\"end\":89279,\"start\":89271},{\"end\":89773,\"start\":89761},{\"end\":89782,\"start\":89773},{\"end\":89791,\"start\":89782},{\"end\":89809,\"start\":89791},{\"end\":89817,\"start\":89809},{\"end\":90189,\"start\":90180},{\"end\":90199,\"start\":90189},{\"end\":90210,\"start\":90199},{\"end\":90436,\"start\":90428},{\"end\":90447,\"start\":90436},{\"end\":90468,\"start\":90447},{\"end\":90475,\"start\":90468},{\"end\":90961,\"start\":90948},{\"end\":91234,\"start\":91226},{\"end\":91243,\"start\":91234},{\"end\":91453,\"start\":91439},{\"end\":91468,\"start\":91453},{\"end\":91657,\"start\":91648},{\"end\":91670,\"start\":91657},{\"end\":91931,\"start\":91925},{\"end\":91940,\"start\":91931},{\"end\":91948,\"start\":91940},{\"end\":91954,\"start\":91948},{\"end\":92238,\"start\":92226},{\"end\":92249,\"start\":92238},{\"end\":92484,\"start\":92470},{\"end\":92704,\"start\":92691},{\"end\":92713,\"start\":92704},{\"end\":92906,\"start\":92893},{\"end\":92913,\"start\":92906},{\"end\":92924,\"start\":92913},{\"end\":92940,\"start\":92924},{\"end\":93163,\"start\":93154},{\"end\":93357,\"start\":93346},{\"end\":93373,\"start\":93357},{\"end\":93640,\"start\":93630},{\"end\":93649,\"start\":93640},{\"end\":93663,\"start\":93649},{\"end\":93671,\"start\":93663},{\"end\":93680,\"start\":93671},{\"end\":93701,\"start\":93680},{\"end\":93718,\"start\":93701},{\"end\":93732,\"start\":93718},{\"end\":93750,\"start\":93732},{\"end\":93761,\"start\":93750},{\"end\":93773,\"start\":93761},{\"end\":94198,\"start\":94190},{\"end\":94207,\"start\":94198},{\"end\":94215,\"start\":94207},{\"end\":94224,\"start\":94215},{\"end\":94489,\"start\":94482},{\"end\":94504,\"start\":94489},{\"end\":94511,\"start\":94504},{\"end\":94761,\"start\":94752},{\"end\":94770,\"start\":94761},{\"end\":94782,\"start\":94770},{\"end\":95049,\"start\":95036},{\"end\":95065,\"start\":95049},{\"end\":95079,\"start\":95065},{\"end\":95087,\"start\":95079},{\"end\":95095,\"start\":95087},{\"end\":95099,\"start\":95095},{\"end\":95438,\"start\":95430},{\"end\":95446,\"start\":95438},{\"end\":95454,\"start\":95446},{\"end\":95460,\"start\":95454},{\"end\":95469,\"start\":95460},{\"end\":95477,\"start\":95469},{\"end\":96128,\"start\":96122},{\"end\":96309,\"start\":96301},{\"end\":96322,\"start\":96309},{\"end\":96531,\"start\":96519},{\"end\":96539,\"start\":96531},{\"end\":96552,\"start\":96539},{\"end\":96769,\"start\":96763},{\"end\":96775,\"start\":96769},{\"end\":96782,\"start\":96775},{\"end\":96788,\"start\":96782},{\"end\":96795,\"start\":96788},{\"end\":97143,\"start\":97131},{\"end\":97153,\"start\":97143},{\"end\":97161,\"start\":97153},{\"end\":97170,\"start\":97161},{\"end\":97716,\"start\":97707},{\"end\":97725,\"start\":97716},{\"end\":97735,\"start\":97725},{\"end\":98140,\"start\":98126},{\"end\":98152,\"start\":98140},{\"end\":98461,\"start\":98455},{\"end\":98467,\"start\":98461},{\"end\":98474,\"start\":98467},{\"end\":98480,\"start\":98474},{\"end\":98487,\"start\":98480},{\"end\":98797,\"start\":98785},{\"end\":98806,\"start\":98797},{\"end\":99383,\"start\":99374},{\"end\":99604,\"start\":99593},{\"end\":99616,\"start\":99604},{\"end\":99628,\"start\":99616},{\"end\":99637,\"start\":99628},{\"end\":100028,\"start\":100015},{\"end\":100340,\"start\":100333},{\"end\":100348,\"start\":100340},{\"end\":100356,\"start\":100348},{\"end\":100362,\"start\":100356},{\"end\":100613,\"start\":100603},{\"end\":100622,\"start\":100613},{\"end\":100637,\"start\":100622},{\"end\":101029,\"start\":101012},{\"end\":101041,\"start\":101029},{\"end\":101058,\"start\":101041},{\"end\":101065,\"start\":101058},{\"end\":101075,\"start\":101065},{\"end\":101380,\"start\":101374},{\"end\":101389,\"start\":101380},{\"end\":101395,\"start\":101389},{\"end\":101402,\"start\":101395},{\"end\":101409,\"start\":101402},{\"end\":101417,\"start\":101409},{\"end\":101656,\"start\":101640},{\"end\":101668,\"start\":101656},{\"end\":101680,\"start\":101668},{\"end\":101692,\"start\":101680},{\"end\":101707,\"start\":101692},{\"end\":101965,\"start\":101952},{\"end\":101976,\"start\":101965},{\"end\":101984,\"start\":101976},{\"end\":102271,\"start\":102258},{\"end\":102280,\"start\":102271},{\"end\":102298,\"start\":102280},{\"end\":102307,\"start\":102298},{\"end\":102516,\"start\":102510},{\"end\":102525,\"start\":102516},{\"end\":102534,\"start\":102525},{\"end\":102541,\"start\":102534},{\"end\":103025,\"start\":103018},{\"end\":103031,\"start\":103025},{\"end\":103040,\"start\":103031},{\"end\":103048,\"start\":103040},{\"end\":103055,\"start\":103048},{\"end\":103492,\"start\":103486},{\"end\":103502,\"start\":103492},{\"end\":103518,\"start\":103502},{\"end\":103527,\"start\":103518},{\"end\":103762,\"start\":103753},{\"end\":103771,\"start\":103762},{\"end\":103781,\"start\":103771},{\"end\":103799,\"start\":103781},{\"end\":103809,\"start\":103799},{\"end\":103819,\"start\":103809},{\"end\":104081,\"start\":104068},{\"end\":104092,\"start\":104081},{\"end\":104101,\"start\":104092},{\"end\":104107,\"start\":104101},{\"end\":104115,\"start\":104107},{\"end\":104124,\"start\":104115},{\"end\":104361,\"start\":104349},{\"end\":104371,\"start\":104361},{\"end\":104618,\"start\":104609},{\"end\":104626,\"start\":104618},{\"end\":104634,\"start\":104626},{\"end\":104835,\"start\":104827},{\"end\":104851,\"start\":104835},{\"end\":104864,\"start\":104851},{\"end\":104875,\"start\":104864},{\"end\":104885,\"start\":104875},{\"end\":104893,\"start\":104885},{\"end\":104904,\"start\":104893},{\"end\":105180,\"start\":105167},{\"end\":105190,\"start\":105180},{\"end\":105200,\"start\":105190},{\"end\":105207,\"start\":105200},{\"end\":105512,\"start\":105500},{\"end\":105519,\"start\":105512},{\"end\":105529,\"start\":105519},{\"end\":105783,\"start\":105775},{\"end\":105791,\"start\":105783},{\"end\":106256,\"start\":106249},{\"end\":106263,\"start\":106256},{\"end\":106275,\"start\":106263},{\"end\":106281,\"start\":106275},{\"end\":106289,\"start\":106281},{\"end\":106297,\"start\":106289},{\"end\":106820,\"start\":106808},{\"end\":106836,\"start\":106820},{\"end\":106847,\"start\":106836},{\"end\":106858,\"start\":106847},{\"end\":107193,\"start\":107185},{\"end\":107208,\"start\":107193},{\"end\":107218,\"start\":107208},{\"end\":107228,\"start\":107218},{\"end\":107238,\"start\":107228},{\"end\":107253,\"start\":107238},{\"end\":107263,\"start\":107253},{\"end\":107277,\"start\":107263},{\"end\":107292,\"start\":107277},{\"end\":107305,\"start\":107292},{\"end\":107658,\"start\":107651},{\"end\":107670,\"start\":107658},{\"end\":107679,\"start\":107670},{\"end\":107688,\"start\":107679},{\"end\":107697,\"start\":107688},{\"end\":107708,\"start\":107697},{\"end\":107719,\"start\":107708},{\"end\":108099,\"start\":108092},{\"end\":108107,\"start\":108099},{\"end\":108116,\"start\":108107},{\"end\":108122,\"start\":108116},{\"end\":108130,\"start\":108122},{\"end\":108489,\"start\":108483},{\"end\":108499,\"start\":108489},{\"end\":108509,\"start\":108499},{\"end\":108519,\"start\":108509},{\"end\":108526,\"start\":108519},{\"end\":108538,\"start\":108526},{\"end\":108795,\"start\":108786},{\"end\":108805,\"start\":108795}]", "bib_venue": "[{\"end\":74840,\"start\":74774},{\"end\":76367,\"start\":76272},{\"end\":77001,\"start\":76989},{\"end\":77874,\"start\":77805},{\"end\":78837,\"start\":78768},{\"end\":79425,\"start\":79334},{\"end\":80402,\"start\":80387},{\"end\":81016,\"start\":80925},{\"end\":81688,\"start\":81579},{\"end\":83887,\"start\":83769},{\"end\":84570,\"start\":84562},{\"end\":85108,\"start\":85036},{\"end\":86345,\"start\":86278},{\"end\":87401,\"start\":87330},{\"end\":89448,\"start\":89372},{\"end\":89887,\"start\":89860},{\"end\":90644,\"start\":90568},{\"end\":90994,\"start\":90986},{\"end\":95762,\"start\":95628},{\"end\":96900,\"start\":96856},{\"end\":97323,\"start\":97255},{\"end\":99039,\"start\":98931},{\"end\":102734,\"start\":102646},{\"end\":103204,\"start\":103138},{\"end\":105960,\"start\":105884},{\"end\":106466,\"start\":106390},{\"end\":108964,\"start\":108893},{\"end\":73141,\"start\":73054},{\"end\":73494,\"start\":73417},{\"end\":73699,\"start\":73695},{\"end\":73905,\"start\":73831},{\"end\":74334,\"start\":74260},{\"end\":74772,\"start\":74691},{\"end\":75218,\"start\":75155},{\"end\":75548,\"start\":75534},{\"end\":75850,\"start\":75776},{\"end\":76270,\"start\":76179},{\"end\":76987,\"start\":76907},{\"end\":77384,\"start\":77340},{\"end\":77803,\"start\":77731},{\"end\":78343,\"start\":78278},{\"end\":78548,\"start\":78544},{\"end\":78766,\"start\":78694},{\"end\":79332,\"start\":79240},{\"end\":79954,\"start\":79913},{\"end\":80385,\"start\":80323},{\"end\":80923,\"start\":80831},{\"end\":81577,\"start\":81469},{\"end\":82087,\"start\":82029},{\"end\":82330,\"start\":82257},{\"end\":82493,\"start\":82439},{\"end\":82934,\"start\":82867},{\"end\":83334,\"start\":83293},{\"end\":83767,\"start\":83634},{\"end\":84286,\"start\":84256},{\"end\":84560,\"start\":84537},{\"end\":84762,\"start\":84699},{\"end\":85034,\"start\":84947},{\"end\":85546,\"start\":85470},{\"end\":85904,\"start\":85865},{\"end\":86276,\"start\":86194},{\"end\":86786,\"start\":86745},{\"end\":87328,\"start\":87242},{\"end\":87809,\"start\":87765},{\"end\":88059,\"start\":87998},{\"end\":88318,\"start\":88303},{\"end\":88486,\"start\":88432},{\"end\":88749,\"start\":88695},{\"end\":88968,\"start\":88911},{\"end\":89370,\"start\":89279},{\"end\":89858,\"start\":89817},{\"end\":90178,\"start\":90122},{\"end\":90566,\"start\":90475},{\"end\":90984,\"start\":90961},{\"end\":91224,\"start\":91147},{\"end\":91486,\"start\":91468},{\"end\":91706,\"start\":91670},{\"end\":92009,\"start\":91954},{\"end\":92224,\"start\":92186},{\"end\":92506,\"start\":92484},{\"end\":92729,\"start\":92713},{\"end\":92891,\"start\":92832},{\"end\":93201,\"start\":93163},{\"end\":93422,\"start\":93373},{\"end\":93779,\"start\":93773},{\"end\":94071,\"start\":94065},{\"end\":94188,\"start\":94118},{\"end\":94540,\"start\":94511},{\"end\":94750,\"start\":94717},{\"end\":94939,\"start\":94918},{\"end\":95172,\"start\":95115},{\"end\":95626,\"start\":95477},{\"end\":96120,\"start\":96058},{\"end\":96299,\"start\":96271},{\"end\":96517,\"start\":96451},{\"end\":96854,\"start\":96795},{\"end\":97253,\"start\":97170},{\"end\":97771,\"start\":97735},{\"end\":97974,\"start\":97968},{\"end\":98124,\"start\":98008},{\"end\":98517,\"start\":98487},{\"end\":98929,\"start\":98806},{\"end\":99414,\"start\":99383},{\"end\":99645,\"start\":99637},{\"end\":99844,\"start\":99781},{\"end\":100096,\"start\":100028},{\"end\":100331,\"start\":100293},{\"end\":100699,\"start\":100637},{\"end\":101010,\"start\":100911},{\"end\":101372,\"start\":101319},{\"end\":101638,\"start\":101597},{\"end\":102033,\"start\":101984},{\"end\":102256,\"start\":102205},{\"end\":102644,\"start\":102541},{\"end\":103136,\"start\":103055},{\"end\":103484,\"start\":103448},{\"end\":103751,\"start\":103686},{\"end\":104066,\"start\":104034},{\"end\":104415,\"start\":104371},{\"end\":104607,\"start\":104568},{\"end\":104825,\"start\":104780},{\"end\":105251,\"start\":105207},{\"end\":105498,\"start\":105429},{\"end\":105882,\"start\":105791},{\"end\":106388,\"start\":106297},{\"end\":106907,\"start\":106858},{\"end\":107311,\"start\":107305},{\"end\":107748,\"start\":107719},{\"end\":108198,\"start\":108130},{\"end\":108481,\"start\":108430},{\"end\":108891,\"start\":108805}]"}}}, "year": 2023, "month": 12, "day": 17}