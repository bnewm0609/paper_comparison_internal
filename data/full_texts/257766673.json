{"id": 257766673, "updated": "2023-10-05 03:04:59.478", "metadata": {"title": "SimpleNet: A Simple Network for Image Anomaly Detection and Localization", "authors": "[{\"first\":\"Zhikang\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Yiming\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Yuansheng\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Zilei\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We propose a simple and application-friendly network (called SimpleNet) for detecting and localizing anomalies. SimpleNet consists of four components: (1) a pre-trained Feature Extractor that generates local features, (2) a shallow Feature Adapter that transfers local features towards target domain, (3) a simple Anomaly Feature Generator that counterfeits anomaly features by adding Gaussian noise to normal features, and (4) a binary Anomaly Discriminator that distinguishes anomaly features from normal features. During inference, the Anomaly Feature Generator would be discarded. Our approach is based on three intuitions. First, transforming pre-trained features to target-oriented features helps avoid domain bias. Second, generating synthetic anomalies in feature space is more effective, as defects may not have much commonality in the image space. Third, a simple discriminator is much efficient and practical. In spite of simplicity, SimpleNet outperforms previous methods quantitatively and qualitatively. On the MVTec AD benchmark, SimpleNet achieves an anomaly detection AUROC of 99.6%, reducing the error by 55.5% compared to the next best performing model. Furthermore, SimpleNet is faster than existing methods, with a high frame rate of 77 FPS on a 3080ti GPU. Additionally, SimpleNet demonstrates significant improvements in performance on the One-Class Novelty Detection task. Code: https://github.com/DonaldRR/SimpleNet.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2303.15140", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiuZXW23", "doi": "10.1109/cvpr52729.2023.01954"}}, "content": {"source": {"pdf_hash": "a23c0a89bd21bd2481fedcdd6d1ac891c6c06bdc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.15140v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c14383abda1a2ae0e7df79f4b67046bbdb9d11dd", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a23c0a89bd21bd2481fedcdd6d1ac891c6c06bdc.txt", "contents": "\nSimpleNet: A Simple Network for Image Anomaly Detection and Localization\n\n\nZhikang Liu \nMeka Technology Co\nLtd\n\nYiming Zhou \nYuansheng Xu \nZilei Wang zlwang@ustc.edu.cn \nMeka Technology Co\nLtd\n\n\nDepartment of Automation\nUniversity of Science and Technology of China\n\n\nSimpleNet: A Simple Network for Image Anomaly Detection and Localization\n\n\n Figure 1\n. Visualization of samples in MVTec AD. The produced anomaly maps superimposed on the images. Anomaly region of high anomaly score is colored with orange. The red boundary denotes contours of actual segmentation maps for anomalies.\n\n\nAbstract\n\nWe propose a simple and application-friendly network (called SimpleNet) for detecting and localizing anomalies. SimpleNet consists of four components: (1) a pretrained Feature Extractor that generates local features, (2) a shallow Feature Adapter that transfers local features towards target domain, (3) a simple Anomaly Feature Generator that counterfeits anomaly features by adding Gaussian noise to normal features, and (4) a binary Anomaly Discriminator that distinguishes anomaly features from normal features. During inference, the Anomaly Feature Generator would be discarded. Our approach is based on three intuitions. First, transforming pre-trained features to targetoriented features helps avoid domain bias. Second, generating synthetic anomalies in feature space is more effective, as defects may not have much commonality in the image space. Third, a simple discriminator is much efficient and practical. In spite of simplicity, SimpleNet outperforms previous methods quantitatively and qualitatively. On \n\n\nIntroduction\n\nImage anomaly detection and localization task aims to identify abnormal images and locate abnormal subregions. The technique to detect the various anomalies of interest has a broad set of applications in industrial inspection [3,6]. In industrial scenarios, anomaly detection and localization is especially hard, as abnormal samples are scarce and anomalies can vary from subtle changes such as thin scratches to large structural defects, e.g. missing parts. Some examples from the MVTec AD benchmark [3] along with results from our proposed method are shown in Figure 1. This situation prohibits the supervised methods from approaching.\n\nCurrent approaches address this problem in an unsupervised manner, where only normal samples are used during the training process. The reconstruction-based methods [10,21,31], synthesizing-based methods [17,30], and embedding-based methods [6,22,24] are three main trends for tackling this problem. The reconstruction-based methods such as [21,31] assume that a deep network trained with only normal data cannot accurately reconstruct anomalous regions. The pixel-wise reconstruction errors are taken as anomaly scores for anomaly localization. However, this assumption may not always hold, and sometimes a network can \"generalize\" so well that it can also reconstruct the abnormal inputs well, leading to misdetection [10,19]. The synthesizing-based methods [17,30] estimate the decision boundary between the normal and anomalous by training on synthetic anomalies generated on anomaly-free images. However, the synthesized images are not realistic enough. Features from synthetic data might stray far from the normal features, training with such negative samples could result in a loosely bounded normal feature space, meaning indistinct defects could be included in in-distribution feature space.\n\nRecently, the embedding-based methods [6,7,22,24] achieve state-of-the-art performance. These methods use ImageNet pre-trained convolutional neural networks (CNN) to extract generalized normal features. Then a statistical algorithm such as multivariate Gaussian distribution [6], normalizing flow [24], and memory bank [22] is adopted to embed normal feature distribution. Anomalies are detected by comparing the input features with the learned distribution or the memorized features. However, industrial images generally have a different distribution from ImageNet. Directly using these biased features may cause mismatch problems. Moreover, the statistical algorithms always suffer from high computational complexity or high memory consumption.\n\nTo mitigate the aforementioned issues, we propose a novel anomaly detection and localization network, called SimpleNet. SimpleNet takes advantage of the synthesizingbased and the embedding-based manners, and makes several improvements. First, instead of directly using pretrained features, we propose to use a feature adaptor to produce target-oriented features which reduce domain bias. Second, instead of directly synthesizing anomalies on the images, we propose to generate anomalous features by posing noise to normal features in feature space. We argue that with a properly calibrated scale of the noise, a closely bounded normal feature space can be obtained. Third, we simplify the anomalous detection procedure by training a simple discriminator, which is much more computational efficient than the complex statistical algorithms adopted by the aforementioned embedding-based methods. Specifically, SimpleNet makes use of a pre-trained backbone for normal feature extraction followed by a feature adapter to transfer the feature into the target domain. Then, anomaly features are simply generated by adding Gaussian noise to the adapted normal features. A simple discriminator consisting of a few layers of MLP is trained on these features to discriminate anomalies.\n\nSimpleNet is easy to train and apply, with outstanding performance and inference speed. The proposed Sim-pleNet, based on a widely used WideResnet50 backbone, achieves 99.6 % AUROC on MVTec AD while running at 77 fps, surpassing the previous best-published anomaly detection methods on both accuracy and efficiency, see Figure 2. We further introduce SimpleNet to the task of One-Class Novelty Detection to show its generality. These advantages make SimpleNet bridge the gap between academic research and industrial application. Code will be publicly available.\n\n\nRelated Work\n\nAnomaly detection and localization methods can be mainly categorized into three types, i.e., the reconstructionbased methods, the synthesizing-based methods, and the embedding-based methods.\n\nReconstruction-based methods hold the insight that anomalous image regions should not be able to be properly reconstructed since they do not exist in the training samples. Some methods [10] utilize generative models such as auto-encoders and generative adversarial networks [11] to encode and reconstruct normal data. Other methods [13,21,31] frame anomaly detection as an inpainting problem, where patches from images are masked randomly. Then, neural networks are utilized to predict the erased information. Integrating structural similarity index (SSIM) [29] loss function is widely used in training. An anomaly map is generated as pixel-wise difference between the input image and its reconstructed image. However, if anomalies share common compositional patterns (e.g. local edges) with the normal training data or the decoder is \"too strong\" for decoding some abnormal encodings well, the anomalies in images are likely to be reconstructed well [31].\n\nSynthesizing-based methods typically synthesize anomalies on anomaly-free images. DRAEM [30] proposes a network that is discriminatively trained in an end-to-end manner on synthetically generated just-out-of-distribution patterns. CutPaste [17] proposes a simple strategy to generate synthetic anomalies for anomaly detection that cuts an image patch and pastes at a random location of a large image. A CNN is trained to distinguish images from normal and augmented data distributions. However, the appearance of the synthetic anomalies does not closely match the real anomalies'. In practice, as defects are various and unpredictable, generating an anomaly set that includes all outliers is impossible. Instead of synthesizing anomalies on images, with the proposed SimpleNet, negative samples  are synthesized in the feature space.\n\nEmbedding-based methods achieve state-of-the-art performance recently. These methods embed normal features into a compressed space. The anomalous features are far from the normal clusters in the embedding space. Typical methods [6,7,22,24] utilize networks that are pre-trained on ImageNet for feature extraction. With a pre-trained model, PaDiM [6] embeds the extracted anomaly patch features by multivariate Gaussian distribution. PatchCore [22] uses a maximally representative memory bank of nominal patch features. Mahalanobis distance or maximum feature distance is adopted to score the input features in testing. However, industrial images generally have a different distribution from ImageNet. Directly using pre-trained features may cause a mismatch problem. Moreover, either computing the inverse of covariance [6] or searching through the nearest neighbor in the memory bank [22] limits the realtime performance, especially for edge devices.\n\nCS-Flow [24], CFLOW-AD [12], and DifferNet [23] propose to transform the normal feature distribution into Gaussian distribution via normalizing flow (NF) [20]. As normalizing flow can only process full-sized feature maps, i.e., down sample is not allowed and the coupling layer [9] consumes a few times of memory than the normal convolutional layer, these methods are memory consuming. Distillation methods [4,7] train a student network to match the outputs of a fixed pre-trained teacher network with only normal samples. A discrepancy between student and teacher output should be detected given an anomalous query. The computational complexity is doubled as an input image should pass through both the teacher and the student.\n\nSimpleNet overcomes the aforementioned problems. SimpleNet uses a feature adaptor that performs transfer learning on the target dataset to alleviate the bias of pretrained CNNs. SimpleNet proposes to synthesize anoma-lous in the feature space rather than directly on the images. SimpleNet follows a single-stream manner at inference and is totally constructed by conventional CNN blocks which facilitate fast training, inference, and industrial application.\n\n\nMethod\n\nThe proposed SimpleNet is elaborately introduced in this section. As illustrated in Figure 3, SimpleNet consist of a Feature Extractor, a Feature Adaptor, an Anomalous Feature Generator and a Discriminator. The Anomalous Feature Generator is only used during training, thus SimpleNet follows a single stream manner at inference. These modules will be described below in sequence.\n\n\nFeature Extractor\n\nFeature Extractor acquires local feature as in [22]. We reformulate the process as follows. We denote the training set and test set as X train and X test . For any image x i \u2208 R H\u00d7W \u00d73 in X train X test , the pre-trained network \u03d5 extracts features from different hierarchies, as normally done with ResNet-like backbone. Since pre-trained network is biased towards the dataset in which it is trained, it is reasonable to choose only a subset of levels for the target dataset. Formally, we define L the subset including the indexes of hierarchies for use. The feature map from level l \u2208 L is denoted as \u03d5 l,i \u223c \u03d5 l (x i ) \u2208 R H l \u00d7W l \u00d7C l , where H l , W l and C l are the height, width and channel size of the feature map. For an entry \u03d5 l,i h,w \u2208 R C l at location (h, w), its neighborhood with patchsize p is defined as ing here) results in the local feature z l,i h,w , as\nN (h,w) p = {(h \u2032 , y \u2032 )|h \u2032 \u2208 [h \u2212 \u230ap/2\u230b , ..., h + \u230ap/2\u230b] , y \u2032 \u2208 [w \u2212 \u230ap/2\u230b , ..., w + \u230ap/2\u230b]}(1)z l,i h,w = fagg({\u03d5 l,i h \u2032 ,y \u2032 |(h \u2032 , y \u2032 ) \u2208 N h,w p })(2)\nTo combine features z l,i h,w from different hierarchies, all feature maps are linearly resized to the same size (H 0 , W 0 ), i.e. the size of the largest one. Simply concatenating the feature maps channel-wise gives the feature map o i \u2208 R H0\u00d7H0\u00d7C . The process is defined as\no i = fcat(resize(z l \u2032 ,i , (H0, W0))|l \u2032 \u2208 L(3)\nwe define o i h,w \u2208 R C as the entry of o i at location (h, w). We simplify the above expressions as\no i = F \u03d5 (x i )(4)\nwhere F \u03d5 is the Feature Extractor.\n\n\nFeature Adaptor\n\nAs industrial images generally have a different distribution from the dataset used in backbone pre-training, we adopt a Feature Adaptor G \u03b8 to transfer the training features to the target domain. The Feature Adaptor G \u03b8 projects local feature o h,w to adapted feature q h,w as\nq i h,w = G \u03b8 (o i h,w )(5)\nThe Feature Adaptor can be made up of simple neural blocks such as a fully-connected layer or multi-layer perceptron (MLP). We experimentally find that a single fullyconnected layer yields good performance.\n\n\nAnomalous Feature Generator\n\nTo train the Discriminator to estimate the likelihood of samples being normal, the easiest way is sampling negative samples, i.e. defect features, and optimizing it together with normal samples. The lack of defects makes the sampling distribution estimation intractable. While [17,18,30] relying on extra data to synthesize defect images, we add simple noise on normal samples in the feature space, claiming that it outperforms those manipulated methods.\n\nThe anomalous features are generated by adding Gaussian noise on the normal features q i h,w \u2208 R C . Formally, a noise vector \u03f5 \u2208 R C is sampled, with each entry following an i.i.d. Gaussian distribution N (\u00b5, \u03c3 2 ). The anomalous feature q i\u2212 h,w is fused as Figure 4 illustrates the influence of anomalous features on four classes of MVTec AD. We can see that the standard deviation along each dimension of the adapted features tends to be consistent. Thus, the feature space tends to be compact when distinguishing anomalous features from normal features.\nq i\u2212 h,w = q i h,w + \u03f5(6)\n\nDiscriminator\n\nThe Discriminator D \u03c8 works as a normality scorer, estimating the normality at each location (h, w) directly. Since negative samples are generated along with normal features {q i |x i \u2208 X train }, they are both fed to the Discriminator during training. The Discriminator expects positive output for normal features while negative for anomalous features. We simply use a 2-layer multi-layer perceptron (MLP) structure as common classifiers do, estimating normality as D \u03c8 (q h,w ) \u2208 R.\n\n\nLoss function and Training\n\nA simple truncated l1 loss is derived as\nl i h,w = max(0, th + \u2212 D \u03c8 (q i h,w )) + max(0, \u2212th \u2212 + D \u03c8 (q i\u2212 h,w ))(7)\nth + and th \u2212 are truncation terms preventing overfitting. They are set to 0.5 and -0.5 by default. The training objective is\nL = min \u03b8,\u03c8 x i \u2208X train h,w l i h,w H0 * W0(8)\nWe will experimentally evaluate the proposed truncated l1 loss function with the widely used cross-entropy loss in the experiments section. The pseudo-code of the training procedure is shown in Algorithm 1.\n\n\nInference and Scoring function\n\nThe Anomalous Feature Generator is discarded at inference. Note that the remaining modules can be stacked into an end-to-end network. We feed each x i \u2208 X test into the aforementioned Feature Extractor F \u03d5 and the Feature Adaptor G \u03b8 sequentially to get adapted features q i h,w as in Equation 5. The anomaly score is provided by the Discriminator\nD \u03c8 as s i h,w = \u2212D \u03c8 (q i h,w )(9)\nThe anomaly map for anomaly localization during inference is defined as\nSAL(xi) := {s i h,w |(h, w) \u2208 W0 \u00d7 H0}(10)\nThen S AL (x i ) is interpolated to have the spatial resolution of the input sample and Gaussian filtered with \u03c3 = 4 for smooth boundaries. As the most responsive point exists for any size of the anomalous region, the maximum score of the anomaly map is taken as the anomaly detection score of each image\nSAD(xi) := max (h,w)\u2208W 0 \u00d7H 0 s i h,w(11)\n4. Experiments\n\n\nDatasets.\n\nWe conduct most of the experiments on the MVTec Anomaly Detection benchmark [3], that is, a famous dataset in the anomaly detection and localization field. MVTec AD contains 5 texture and 10 object categories stemming from manufacturing with a total of 5354 images. The dataset is composed of normal images for training and both normal and anomaly images with various types of defect for test. It also provides pixel-level annotations for defective test images. Typical images are illustrated in Figure 1. As in [6,22], images are resized and center cropped to 256 \u00d7 256 and 224 \u00d7 224, respectively. No data augmentation is applied. We follow the one-class classification protocol, also known as cold-start anomaly detection, where we train a one-class classifier for each category on its respective normal training samples.\n\nWe conduct one-class novelty detection on CI-FAR10 [16], which contains 50K training images and 10K test images with scale of 32 \u00d7 32 in 10 categories. Under the setting of one-class novelty detection, one category is regarded as normal data and other categories are used as novelty.\n\n\nEvaluation Metrics.\n\nImage-level anomaly detection performance is measured via the standard Area Under the Receiver Operator Curve, which we denote as I-AUROC, using produced anomaly detection scores S AD (Equation 11). For anomaly localization, the anomaly map S AL (Equation 10) is used for an evaluation of pixel-wise AUROC (denoted as P-AUROC). In accordance with prior works [6,22], we compute on MVTec AD the class-average AUROC and mean AU-ROC overall categories for detection and localization. The comparison baselines includes AE-SSIM [3], RIAD [31], DRAEM [30], CutPaste [17], CS-Flow [24], PaDiM [6], RevDist [7] and PatchCore [22]. \n\n\nImplementation Details\n\nThis section describes the configuration implementation details of the experiments in this paper. All backbones used in the experiments were pre-trained with ImageNet [8]. The 2nd and 3rd intermediate layers of the backbone e.g. l \u2032 \u2208 [2,3] in Equation 3 are used in the feature extractor as in [22] when the backbone is ResNet-like architecture. By default, our implementation uses WideResnet50 as backbone, and the feature dimension from the feature extractor is set to 1536. The later feature adaptor is essentially a fully connected layer without bias. The dimensions of the input and output features for the FC layer in the adaptor are the same. The anomaly feature generator adds i.i.d. Gaussian noise N (0, \u03c3 2 ) to each entry of normal features. \u03c3 is set to 0.015 by default. The subsequent discriminator composes of a linear layer, a batch normalization layer, a leaky relu(0.2 slope), and a linear layer. th + and th \u2212 are both set to 0.5 in Equation 7. The Adam optimizer is used, setting the learning rate for the feature adaptor and discriminator to 0.0001 and 0.0002 respectively, and weight decay to 0.00001. Training epochs is set to 160 for each dataset and batchsize is 4.\n\n\nAnomaly detection on MVTec AD\n\nAnomaly detection results on MVTec AD are shown in Table 1. Image-level anomaly score is given by the maximum score of the anomaly map as in Equation 11. Sim-pleNet achieves the highest score for 9 out of 15 classes. For textures and objects, SimpleNet reaches new SOTA of 99.8% and 99.5% of I-AUROC, respectively. SimpleNet achieves significantly higher mean image anomaly detection performance i.e. I-AUROC score of 99.6%. Please note Table 1. Comparison of SimpleNet with state-of-the-arts works on MVTec AD. Image-wise AUROC (I-AUROC) and pixel-wise AUROC (P-AUROC) are displayed in each entry as I-AUROC%/P-AUROC%. P-AUROC for CS-Flow is not recorded in [24]   that, a reduction from an error of 0.9% for PatchCore [22] (next best competitor, under the same WideResnet50 backbone) to 0.4% for SimpleNet means a reduction of the error by 55.5%. In industrial inspection settings, this is a relevant and significant reduction.\n\n\nAnomaly localization on MVTec AD\n\nThe anomaly localization performance is measured by pixel-wise AUROC, which we note as P-AUROC. Comparisons with the state-of-the-art methods are shown in Table 1. SimpleNet achieves the best anomaly detection performance of 98.1% P-AUROC on MVTec AD as well as the new SOTA of 98.4% P-AUROC for objects. SimpleNet achieves the highest score for 4 out of 15 classes. We visualize representative samples for anomaly localization in Figure 8.\n\n\nInference time\n\nAlongside the detection and localization performance, inference time is the most important concern for industrial model deployment. The comparison with the state-of-theart methods on inference time is shown in Figure 2. All the methods are measured on the same hardware contain-ing a Nvidia GeForce GTX 3080ti GPU and an Intel(R) Xeon(R) CPU E5-2680 v3@2.5GHZ. It clearly shows that our method achieves the best performance as well as the fastest speed at the same time. SimpleNet is nearly 8\u00d7 faster than PatchCore [22].\n\n\nAblation study\n\nNeighborhood size and hierarchies. We investigate the influence of neighborhood size p in Equation 1. Results in Figure 6 show a clear optimum between locality and global context for anomaly predictions, thus motivating the neighborhood size p = 3. We design a group of experiments to test the influence of hierarchies subset L on model performance and the results are shown in Table 2. We index the first three WideResNet50 blocks with 1 \u2212 3. As can be seen, features from hierarchy level 3 can already achieve state-of-the-art performance but benefit from additional hierarchy level 2. We chose 2 + 3 as the default setting.\n\nAdaptor configuration. Adaptor provides a transformation (projection) on the pre-trained features. Our default feature adaptor is a single FC layer without bias, with equal input and output channels. A comparison of different feature adaptors is shown in Table 3, the first row \"Ours\" implementation follows the same configuration as in Table 1. \"Ours-complex-FA\" replaces the simple feature adaptor with a nonlinear one (i.e. 1 layer MLPs with nonlinearity). The row \"Ours-w/o-FA\" drops the feature adaptor.\n\nThe results indicate that a single FC layer yields the best performance. Intuitively, the feature adaptor finds a projection such that the faked abnormal features and projected pre-trained features are easily severed, meaning a simple solution to the discriminator. This is also indicated by the phenomenon that using a feature adaptor helps the network con-  verge fast (Figure 7). We observe a significant performance drop with a complex feature adaptor. One possible reason is that a complex adaptor may lead to overfitting, reducing the generalization ability for various defects in test. Figure 4 compares the histogram of standard deviation along each dimension of the features before and after the feature adaptor. We can see that, when training with anomalous features, adapted feature space becomes compact.\n\nScale of noise. The scale of noise in the anomaly feature generator controls how far away the synthesized abnormal features are from the normal ones. To be specific, high \u03c3 results in abnormal features keeping a high Euclidean dis- Table 3. Comparison of different feature adaptors. \"Ours\" implementation follows the same configuration as in Table 1. \"Ourscomplex-FA\" replaces the simple feature adaptor with a nonlinear one. \"Ours-w/o-FA\" drops the feature adaptor, equivalent to using an identity fully-connected layer. \"Ours-CE\" uses cross-entropy loss. I-AUROC% and P-AUROC% of MVTec AD are shown.  tance towards normal features. Training on a large \u03c3 will result in a loose decision bound, leading to a high false negative. Conversely, the training procedure will become unstable if \u03c3 is tiny, and the discriminator cannot generalize to normal features well. Figure 5 details the effect of \u03c3 for each class in MVTec AD. As can be seen, \u03c3 = 0.015 reaches the balance and yield the best performance. Loss function. We compared the proposed loss function in Section 3.5 with the widely used cross-entropy loss (as show in row \"Ours-CE\" in Table 3). We found the improvements, 0.2% I-AUROC and 0.3% P-AUROC, over crossentropy loss. Dependency on backbone. We test SimpleNet with different backbones, the results are shown in Table 4. We find that results are mostly stable over the choice of different backbones. The choice of WideResNet50 is made to be comparable with PaDiM [6] and PatchCore [22].\n\nQualitative Results Figure 8 shows results of anomaly localization that indicate the abnormal areas. The threshold for segmentation results is obtained by calculating the F1-score for all anomaly scores of each sub-class. Experimental results prove that the proposed method can localize abnormal areas well even in rather difficult cases. In addition, we can find that the proposed method has consistent performance in both object and texture classes.\n\n\nOne-Class Novelty Detection\n\nTo evaluate the generality of the proposed SimpleNet, we conduct a one-class novelty detection experiment on CIFAR-10 [16]. Following [19], we train the model with samples from a single class and detect novel samples from other categories. We train the corresponding model for each class respectively. Note that the novelty score is defined as the max score in the similarity map. Table 5 reports the I-AUROC scores of our method and other methods. For fair comparison, all the methods are pre-trained on ImageNet.\n\nThe baselines include VAE [2], LSA [1], DSVDD [25], OCGAN [19], HRN [15], AnoGAN [27], DAAD [14], MKD [26], DisAug CLR [28], IGD [5] and RevDist [7]. Our method outperforms these comparison methods. Note that, IGD [5] and DisAug CLR [28] achieve 91.25% and 92.4% respectively when boosted by self-supervised learning.\n\n\nConclusion\n\nIn this paper, we propose a simple but efficient approach named SimpleNet for unsupervised anomaly detection and localization. SimpleNet consists of several simple neural network modules which are easy to train and apply in industrial scenarios. Though simple, SimpleNet achieves the highest performance as well as the fastest inference speed compared to the previous state-of-the-art methods on the MVtec AD benchmark. SimpleNet provides a new perspective to bridge the gap between academic research and industrial application in anomaly detection and localization.\n\nFigure 2 .\n2Inference speed (FPS) versus I-AUROC on MVTec AD benchmark. SimpleNet outperforms all previous methods on both accuracy and efficiency by a large margin.\n\nFigure 3 .\n3Overview of the proposed SimpleNet. In the training phase, nominal samples are fed into a pre-trained Feature Extractor to get local features. Then, a Feature Adaptor is utilized to adapt pre-trained features into the target domain. Anomalous features are synthesized by adding Gaussian noise to the adapted features. The adapted features and the anomalous features and used as positive and negative samples respectively to train the final Discriminator. The Anomalous Feature Generator is removed at inference.\n\nFigure 4 .\n4Aggregating the features within the neighborhood N h,w p with aggregation function f agg (use adaptive average pool-Histogram of standard deviation along each dimension of local feature and adapted feature. The adapted feature space becomes more compact when training with anomalous features.\n\n\nin data_loader: o = F(x) # normal features q = G(o) # adapted features q_ = q + random(N) # anomalous features loss = loss_func(D(q), D(q_)).mean() loss.backward() # back-propagate F = F.detach() # stop gradient update(G, D) # Adam # loss function def loss_func(s, s_): th_ = -th = 0.5 return max(0, th-s) + max(0, th_+s_)\n\nFigure 5 .Figure 6 .Figure 7 .\n567I-AUROC% and P-AUROC% for each class of MVTec AD dataset with varied \u03c3. (Best viewed in color.) Performance with varied patch sizes on MVTec AD. Visualization of loss during training. The plotted lines show the mean loss for all classes in the MVTec AD dataset. The transparent color shows the range of loss fluctuation.\n\nFigure 8 .\n8Qualitative results, where sampled image, ground truth, and anomaly map are shown for each class in MVTec AD.\n\nTable 2 .\n2Performance on MVTec AD under different combinations \nof hierarchy levels of WideResNet50 to use. \n\nlevel1 \nlevel2 level3 I-AUROC% P-AUROC% \n\u2713 \n93.0 \n94.2 \n\u2713 \n98.4 \n96.7 \n\u2713 \n99.2 \n97.5 \n\u2713 \n\u2713 \n96.7 \n96.7 \n\u2713 \n\u2713 \n99.6 \n98.1 \n\u2713 \n\u2713 \n\u2713 \n99.1 \n98.1 \n\n\n\n\nTable 4. Performance under different backbones on MVTec AD.Model \nI-AUROC% \nP-AUROC% \nOurs \n99.6 \n98.1 \nOurs-complex-FA \n98.3 \n97.2 \nOurs-w/o-FA \n99.2 \n97.9 \nOurs-CE \n99.4 \n97.8 \n\nModel \nI-AUROC% P-AUROC% \nResNet18 \n98.3 \n95.7 \nResNet50 \n99.6 \n98.0 \nResNet101 \n99.2 \n97.6 \nWideResNet50 \n99.6 \n98.1 \n\n\n\nTable 5 .\n5One-Class Novelty Detection I-AUROC(%) results on CIFAR-10 dataset.Method \nLSA \nDSVDD OCGAN \nHRN \nDAAD \nAUROC \n64.1 \n64.8 \n65.6 \n71.3 \n75.3 \nMethod \nDisAug CLR \nIGD \nMKD \nRevDist SimpleNet \nAUROC \n80.0 \n83.68 \n84.5 \n86.5 \n86.5 \n\n\nAcknowledgmentsThis work is supported by the National Natural Science Foundation of China under Grant 62176246 and Grant 61836008. This work is also supported by Anhui Provincial Natural Science Foundation 2208085UD17 and the Fundamental Research Funds for the Central Universities (WK3490000006).\nLatent space autoregression for novelty detection. Davide Abati, Angelo Porrello, Simone Calderara, Rita Cucchiara, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionDavide Abati, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Latent space autoregression for novelty detec- tion. In Proceedings of the IEEE/CVF conference on com- puter vision and pattern recognition, pages 481-490, 2019. 8\n\nVariational autoencoder based anomaly detection using reconstruction probability. Jinwon An, Sungzoon Cho, Special Lecture on IE. 21Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE, 2(1):1-18, 2015. 8\n\nMvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition15Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9592-9600, 2019. 1, 5\n\nUninformed students: Student-teacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionPaul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 4183-4192, 2020. 3\n\nDeep one-class classification via interpolated gaussian descriptor. Yuanhong Chen, Yu Tian, Guansong Pang, Gustavo Carneiro, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceYuanhong Chen, Yu Tian, Guansong Pang, and Gustavo Carneiro. Deep one-class classification via interpolated gaussian descriptor. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 383-392, 2022. 8\n\nPadim: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, International Conference on Pattern Recognition. SpringerThomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. Padim: a patch distribution modeling framework for anomaly detection and localization. In Inter- national Conference on Pattern Recognition, pages 475-489. Springer, 2021. 1, 2, 3, 5, 8\n\nAnomaly detection via reverse distillation from one-class embedding. Hanqiu Deng, Xingyu Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHanqiu Deng and Xingyu Li. Anomaly detection via reverse distillation from one-class embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2, 3, 5, 8\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009. 5\n\nLaurent Dinh, arXiv:1605.08803Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprintLaurent Dinh, Jascha Sohl-Dickstein, and Samy Ben- gio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016. 3\n\nMoussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionDong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 1705-1714, 2019. 2\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. 27Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2\n\nCflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. Denis Gudovskiy, Shun Ishizaka, Kazuki Kozuka, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2022Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka. Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows. In Proceed- ings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 98-107, 2022. 3\n\nAnomaly detection using deep learning based image completion. Matthias Haselmann, P Dieter, Gruber, 17th IEEE international conference on machine learning and applications (ICMLA). Matthias Haselmann, Dieter P Gruber, and ul Tabatabai. Anomaly detection using deep learning based image com- pletion. In 2018 17th IEEE international conference on ma- chine learning and applications (ICMLA), pages 1237-1242. IEEE, 2018. 2\n\nDivide-and-assemble: Learning block-wise memory for unsupervised anomaly detection. Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, Hong Zhou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionJinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shil- iang Pu, and Hong Zhou. Divide-and-assemble: Learning block-wise memory for unsupervised anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8791-8800, 2021. 8\n\nHrn: A holistic approach to one class learning. Wenpeng Hu, Mengyu Wang, Qi Qin, Jinwen Ma, Bing Liu, Advances in Neural Information Processing Systems. 33Wenpeng Hu, Mengyu Wang, Qi Qin, Jinwen Ma, and Bing Liu. Hrn: A holistic approach to one class learn- ing. Advances in Neural Information Processing Systems, 33:19111-19124, 2020. 8\n\nLearning multiple layers of features from tiny images. A Krizhevsky, 5University of TrontMaster's thesisA Krizhevsky. Learning multiple layers of features from tiny images. Master's thesis, University of Tront, 2009. 5, 8\n\nCutpaste: Self-supervised learning for anomaly detection and localization. Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition5Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. Cutpaste: Self-supervised learning for anomaly de- tection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9664-9674, 2021. 2, 4, 5\n\nExplainable deep one-class classification. Philipp Liznerski, Lukas Ruff, A Robert, Billy Joe Vandermeulen, Marius Franks, Klaus Robert Kloft, Muller, International Conference on Learning Representations. Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Marius Kloft, and Klaus Robert Muller. Explainable deep one-class classification. In International Conference on Learning Representations, 2020. 4\n\nOcgan: One-class novelty detection using gans with constrained latent representations. Pramuditha Perera, Ramesh Nallapati, Bing Xiang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. Oc- gan: One-class novelty detection using gans with constrained latent representations. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 2898-2906, 2019. 2, 8\n\nVariational inference with normalizing flows. Danilo Rezende, Shakir Mohamed, PMLRInternational conference on machine learning. Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on ma- chine learning, pages 1530-1538. PMLR, 2015. 3\n\nSelf-supervised predictive convolutional attentive block for anomaly detection. Neelu Nicolae-C\u0203t\u0203lin Ristea, Radu Tudor Madan, Kamal Ionescu, Nasrollahi, Thomas B Fahad Shahbaz Khan, Mubarak Moeslund, Shah, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022Nicolae-C\u0203t\u0203lin Ristea, Neelu Madan, Radu Tudor Ionescu, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B Moes- lund, and Mubarak Shah. Self-supervised predictive convo- lutional attentive block for anomaly detection. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13576-13586, 2022. 2\n\nTowards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition6Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, and Peter Gehler. Towards to- tal recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14318-14328, 2022. 2, 3, 5, 6, 8\n\nSame same but differnet: Semi-supervised defect detection with normalizing flows. Marco Rudolph, Bastian Wandt, Bodo Rosenhahn, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer visionMarco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same same but differnet: Semi-supervised defect detection with normalizing flows. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 1907- 1916, 2021. 3\n\nFully convolutional cross-scale-flows for imagebased defect detection. Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, Bastian Wandt, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision56Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bas- tian Wandt. Fully convolutional cross-scale-flows for image- based defect detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1088-1097, 2022. 2, 3, 5, 6\n\nDeep one-class classification. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Ahmed Shoaib, Alexander Siddiqui, Emmanuel Binder, Marius M\u00fcller, Kloft, PMLRInternational conference on machine learning. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Em- manuel M\u00fcller, and Marius Kloft. Deep one-class classifica- tion. In International conference on machine learning, pages 4393-4402. PMLR, 2018. 8\n\nMultiresolution knowledge distillation for anomaly detection. Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, Hamid R Rabiee, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionMohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, and Hamid R Ra- biee. Multiresolution knowledge distillation for anomaly detection. In Proceedings of the IEEE/CVF confer- ence on computer vision and pattern recognition, pages 14902-14912, 2021. 8\n\nUnsupervised anomaly detection with generative adversarial networks to guide marker discovery. Thomas Schlegl, Philipp Seeb\u00f6ck, Ursula Sebastian M Waldstein, Georg Schmidt-Erfurth, Langs, International conference on information processing in medical imaging. SpringerThomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on in- formation processing in medical imaging, pages 146-157. Springer, 2017. 8\n\nLearning and evaluating representations for deep one-class classification. Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, Tomas Pfister, International Conference on Learning Representations. Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, and Tomas Pfister. Learning and evaluating representations for deep one-class classification. In International Conference on Learning Representations, 2021. 8\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, IEEE transactions on image processing. 134Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004. 2\n\nDraema discriminatively trained reconstruction embedding for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Sko\u010daj, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision5Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u010daj. Draem- a discriminatively trained reconstruction embedding for sur- face anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8330- 8339, 2021. 2, 4, 5\n\nReconstruction by inpainting for visual anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Sko\u010daj, Pattern Recognition. 1125Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u010daj. Recon- struction by inpainting for visual anomaly detection. Pattern Recognition, 112:107706, 2021. 2, 5\n", "annotations": {"author": "[{\"end\":112,\"start\":76},{\"end\":125,\"start\":113},{\"end\":139,\"start\":126},{\"end\":194,\"start\":140},{\"end\":268,\"start\":195}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":84},{\"end\":124,\"start\":120},{\"end\":138,\"start\":136},{\"end\":150,\"start\":146}]", "author_first_name": "[{\"end\":83,\"start\":76},{\"end\":119,\"start\":113},{\"end\":135,\"start\":126},{\"end\":145,\"start\":140}]", "author_affiliation": "[{\"end\":111,\"start\":89},{\"end\":193,\"start\":171},{\"end\":267,\"start\":196}]", "title": "[{\"end\":73,\"start\":1},{\"end\":341,\"start\":269}]", "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":818,\"start\":815},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1863,\"start\":1860},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1865,\"start\":1863},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2138,\"start\":2135},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2441,\"start\":2437},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2444,\"start\":2441},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2447,\"start\":2444},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2480,\"start\":2476},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2483,\"start\":2480},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2516,\"start\":2513},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2519,\"start\":2516},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2522,\"start\":2519},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2617,\"start\":2613},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2620,\"start\":2617},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2996,\"start\":2992},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2999,\"start\":2996},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3036,\"start\":3032},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3039,\"start\":3036},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3515,\"start\":3512},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3517,\"start\":3515},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3520,\"start\":3517},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3523,\"start\":3520},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3752,\"start\":3749},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3775,\"start\":3771},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3797,\"start\":3793},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6457,\"start\":6453},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6546,\"start\":6542},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6604,\"start\":6600},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6607,\"start\":6604},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6610,\"start\":6607},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6829,\"start\":6825},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7223,\"start\":7219},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7318,\"start\":7314},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7470,\"start\":7466},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8292,\"start\":8289},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8294,\"start\":8292},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8297,\"start\":8294},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8300,\"start\":8297},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8410,\"start\":8407},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8508,\"start\":8504},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8884,\"start\":8881},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8950,\"start\":8946},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9026,\"start\":9022},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9041,\"start\":9037},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9061,\"start\":9057},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9172,\"start\":9168},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9295,\"start\":9292},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9424,\"start\":9421},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9426,\"start\":9424},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10664,\"start\":10660},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12982,\"start\":12978},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12985,\"start\":12982},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12988,\"start\":12985},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15759,\"start\":15756},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16195,\"start\":16192},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16198,\"start\":16195},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16561,\"start\":16557},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17175,\"start\":17172},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17178,\"start\":17175},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17339,\"start\":17336},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17350,\"start\":17346},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17362,\"start\":17358},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17377,\"start\":17373},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17391,\"start\":17387},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17402,\"start\":17399},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17415,\"start\":17412},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17434,\"start\":17430},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17633,\"start\":17630},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17701,\"start\":17698},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17703,\"start\":17701},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17762,\"start\":17758},{\"end\":18839,\"start\":18828},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19350,\"start\":19346},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19411,\"start\":19407},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20632,\"start\":20628},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24088,\"start\":24085},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24107,\"start\":24103},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24715,\"start\":24711},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24731,\"start\":24727},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25138,\"start\":25135},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25147,\"start\":25144},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25159,\"start\":25155},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25171,\"start\":25167},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25181,\"start\":25177},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25194,\"start\":25190},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25205,\"start\":25201},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25215,\"start\":25211},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25232,\"start\":25228},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25241,\"start\":25238},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25257,\"start\":25254},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25326,\"start\":25323},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25346,\"start\":25342}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26174,\"start\":26008},{\"attributes\":{\"id\":\"fig_2\"},\"end\":26699,\"start\":26175},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27005,\"start\":26700},{\"attributes\":{\"id\":\"fig_4\"},\"end\":27330,\"start\":27006},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27686,\"start\":27331},{\"attributes\":{\"id\":\"fig_6\"},\"end\":27809,\"start\":27687},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28066,\"start\":27810},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":28369,\"start\":28067},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28611,\"start\":28370}]", "paragraph": "[{\"end\":585,\"start\":354},{\"end\":1617,\"start\":598},{\"end\":2271,\"start\":1634},{\"end\":3472,\"start\":2273},{\"end\":4220,\"start\":3474},{\"end\":5496,\"start\":4222},{\"end\":6059,\"start\":5498},{\"end\":6266,\"start\":6076},{\"end\":7224,\"start\":6268},{\"end\":8059,\"start\":7226},{\"end\":9012,\"start\":8061},{\"end\":9742,\"start\":9014},{\"end\":10201,\"start\":9744},{\"end\":10591,\"start\":10212},{\"end\":11489,\"start\":10613},{\"end\":11931,\"start\":11654},{\"end\":12082,\"start\":11982},{\"end\":12138,\"start\":12103},{\"end\":12434,\"start\":12158},{\"end\":12669,\"start\":12463},{\"end\":13155,\"start\":12701},{\"end\":13715,\"start\":13157},{\"end\":14242,\"start\":13758},{\"end\":14313,\"start\":14273},{\"end\":14516,\"start\":14391},{\"end\":14771,\"start\":14565},{\"end\":15153,\"start\":14806},{\"end\":15261,\"start\":15190},{\"end\":15609,\"start\":15305},{\"end\":15666,\"start\":15652},{\"end\":16504,\"start\":15680},{\"end\":16789,\"start\":16506},{\"end\":17436,\"start\":16813},{\"end\":18653,\"start\":17463},{\"end\":19616,\"start\":18687},{\"end\":20093,\"start\":19653},{\"end\":20633,\"start\":20112},{\"end\":21278,\"start\":20652},{\"end\":21788,\"start\":21280},{\"end\":22606,\"start\":21790},{\"end\":24108,\"start\":22608},{\"end\":24561,\"start\":24110},{\"end\":25107,\"start\":24593},{\"end\":25426,\"start\":25109},{\"end\":26007,\"start\":25441}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11591,\"start\":11490},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11653,\"start\":11591},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11981,\"start\":11932},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12102,\"start\":12083},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12462,\"start\":12435},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13741,\"start\":13716},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14390,\"start\":14314},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14564,\"start\":14517},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15189,\"start\":15154},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15304,\"start\":15262},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15651,\"start\":15610}]", "table_ref": "[{\"end\":18745,\"start\":18738},{\"end\":19131,\"start\":19124},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21037,\"start\":21030},{\"end\":21542,\"start\":21535},{\"end\":22847,\"start\":22840},{\"end\":22957,\"start\":22950},{\"end\":23756,\"start\":23749},{\"end\":23941,\"start\":23934},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24981,\"start\":24974}]", "section_header": "[{\"end\":596,\"start\":588},{\"attributes\":{\"n\":\"1.\"},\"end\":1632,\"start\":1620},{\"attributes\":{\"n\":\"2.\"},\"end\":6074,\"start\":6062},{\"attributes\":{\"n\":\"3.\"},\"end\":10210,\"start\":10204},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10611,\"start\":10594},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12156,\"start\":12141},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12699,\"start\":12672},{\"attributes\":{\"n\":\"3.4.\"},\"end\":13756,\"start\":13743},{\"attributes\":{\"n\":\"3.5.\"},\"end\":14271,\"start\":14245},{\"attributes\":{\"n\":\"3.6.\"},\"end\":14804,\"start\":14774},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15678,\"start\":15669},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16811,\"start\":16792},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17461,\"start\":17439},{\"attributes\":{\"n\":\"4.4.\"},\"end\":18685,\"start\":18656},{\"attributes\":{\"n\":\"4.5.\"},\"end\":19651,\"start\":19619},{\"attributes\":{\"n\":\"4.6.\"},\"end\":20110,\"start\":20096},{\"attributes\":{\"n\":\"4.7.\"},\"end\":20650,\"start\":20636},{\"attributes\":{\"n\":\"4.8.\"},\"end\":24591,\"start\":24564},{\"attributes\":{\"n\":\"5.\"},\"end\":25439,\"start\":25429},{\"end\":26019,\"start\":26009},{\"end\":26186,\"start\":26176},{\"end\":26711,\"start\":26701},{\"end\":27362,\"start\":27332},{\"end\":27698,\"start\":27688},{\"end\":27820,\"start\":27811},{\"end\":28380,\"start\":28371}]", "table": "[{\"end\":28066,\"start\":27822},{\"end\":28369,\"start\":28128},{\"end\":28611,\"start\":28449}]", "figure_caption": "[{\"end\":26174,\"start\":26021},{\"end\":26699,\"start\":26188},{\"end\":27005,\"start\":26713},{\"end\":27330,\"start\":27008},{\"end\":27686,\"start\":27366},{\"end\":27809,\"start\":27700},{\"end\":28128,\"start\":28069},{\"end\":28449,\"start\":28382}]", "figure_ref": "[{\"end\":353,\"start\":345},{\"end\":2204,\"start\":2196},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10304,\"start\":10296},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13372,\"start\":13339},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13425,\"start\":13417},{\"end\":16184,\"start\":16176},{\"end\":17009,\"start\":16997},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20092,\"start\":20084},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20330,\"start\":20322},{\"end\":20773,\"start\":20765},{\"end\":22171,\"start\":22161},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22391,\"start\":22383},{\"end\":23480,\"start\":23472},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24138,\"start\":24130}]", "bib_author_first_name": "[{\"end\":28967,\"start\":28961},{\"end\":28981,\"start\":28975},{\"end\":28998,\"start\":28992},{\"end\":29014,\"start\":29010},{\"end\":29497,\"start\":29491},{\"end\":29510,\"start\":29502},{\"end\":29778,\"start\":29774},{\"end\":29796,\"start\":29789},{\"end\":29810,\"start\":29805},{\"end\":29830,\"start\":29823},{\"end\":30352,\"start\":30348},{\"end\":30370,\"start\":30363},{\"end\":30384,\"start\":30379},{\"end\":30404,\"start\":30397},{\"end\":30915,\"start\":30907},{\"end\":30924,\"start\":30922},{\"end\":30939,\"start\":30931},{\"end\":30953,\"start\":30946},{\"end\":31385,\"start\":31379},{\"end\":31403,\"start\":31394},{\"end\":31421,\"start\":31412},{\"end\":31437,\"start\":31430},{\"end\":31840,\"start\":31834},{\"end\":31853,\"start\":31847},{\"end\":32263,\"start\":32260},{\"end\":32273,\"start\":32270},{\"end\":32287,\"start\":32280},{\"end\":32302,\"start\":32296},{\"end\":32310,\"start\":32307},{\"end\":32317,\"start\":32315},{\"end\":32625,\"start\":32618},{\"end\":33049,\"start\":33045},{\"end\":33064,\"start\":33056},{\"end\":33075,\"start\":33070},{\"end\":33090,\"start\":33080},{\"end\":33589,\"start\":33586},{\"end\":33606,\"start\":33602},{\"end\":33627,\"start\":33622},{\"end\":33639,\"start\":33635},{\"end\":33649,\"start\":33644},{\"end\":33671,\"start\":33664},{\"end\":33684,\"start\":33679},{\"end\":33702,\"start\":33696},{\"end\":34095,\"start\":34090},{\"end\":34111,\"start\":34107},{\"end\":34128,\"start\":34122},{\"end\":34623,\"start\":34615},{\"end\":34636,\"start\":34635},{\"end\":35066,\"start\":35060},{\"end\":35080,\"start\":35072},{\"end\":35096,\"start\":35088},{\"end\":35106,\"start\":35104},{\"end\":35120,\"start\":35112},{\"end\":35129,\"start\":35125},{\"end\":35588,\"start\":35581},{\"end\":35599,\"start\":35593},{\"end\":35608,\"start\":35606},{\"end\":35620,\"start\":35614},{\"end\":35629,\"start\":35625},{\"end\":35928,\"start\":35927},{\"end\":36180,\"start\":36170},{\"end\":36191,\"start\":36185},{\"end\":36205,\"start\":36198},{\"end\":36217,\"start\":36212},{\"end\":36683,\"start\":36676},{\"end\":36700,\"start\":36695},{\"end\":36708,\"start\":36707},{\"end\":36722,\"start\":36717},{\"end\":36726,\"start\":36723},{\"end\":36747,\"start\":36741},{\"end\":36761,\"start\":36756},{\"end\":36768,\"start\":36762},{\"end\":37155,\"start\":37145},{\"end\":37170,\"start\":37164},{\"end\":37186,\"start\":37182},{\"end\":37655,\"start\":37649},{\"end\":37671,\"start\":37665},{\"end\":37980,\"start\":37975},{\"end\":38009,\"start\":38005},{\"end\":38015,\"start\":38010},{\"end\":38028,\"start\":38023},{\"end\":38056,\"start\":38050},{\"end\":38058,\"start\":38057},{\"end\":38086,\"start\":38079},{\"end\":38649,\"start\":38642},{\"end\":38661,\"start\":38656},{\"end\":38677,\"start\":38670},{\"end\":38694,\"start\":38686},{\"end\":38712,\"start\":38706},{\"end\":38724,\"start\":38719},{\"end\":39247,\"start\":39242},{\"end\":39264,\"start\":39257},{\"end\":39276,\"start\":39272},{\"end\":39755,\"start\":39750},{\"end\":39768,\"start\":39765},{\"end\":39783,\"start\":39779},{\"end\":39802,\"start\":39795},{\"end\":40254,\"start\":40249},{\"end\":40267,\"start\":40261},{\"end\":40286,\"start\":40282},{\"end\":40302,\"start\":40297},{\"end\":40316,\"start\":40311},{\"end\":40334,\"start\":40325},{\"end\":40353,\"start\":40345},{\"end\":40368,\"start\":40362},{\"end\":40762,\"start\":40750},{\"end\":40778,\"start\":40771},{\"end\":40795,\"start\":40788},{\"end\":41372,\"start\":41366},{\"end\":41389,\"start\":41382},{\"end\":41405,\"start\":41399},{\"end\":41434,\"start\":41429},{\"end\":41921,\"start\":41915},{\"end\":41938,\"start\":41928},{\"end\":41950,\"start\":41943},{\"end\":41962,\"start\":41957},{\"end\":41973,\"start\":41968},{\"end\":42328,\"start\":42324},{\"end\":42339,\"start\":42335},{\"end\":42341,\"start\":42340},{\"end\":42350,\"start\":42349},{\"end\":42364,\"start\":42358},{\"end\":42724,\"start\":42718},{\"end\":42741,\"start\":42736},{\"end\":42758,\"start\":42751},{\"end\":43216,\"start\":43210},{\"end\":43233,\"start\":43228},{\"end\":43250,\"start\":43243}]", "bib_author_last_name": "[{\"end\":28973,\"start\":28968},{\"end\":28990,\"start\":28982},{\"end\":29008,\"start\":28999},{\"end\":29024,\"start\":29015},{\"end\":29500,\"start\":29498},{\"end\":29514,\"start\":29511},{\"end\":29787,\"start\":29779},{\"end\":29803,\"start\":29797},{\"end\":29821,\"start\":29811},{\"end\":29837,\"start\":29831},{\"end\":30361,\"start\":30353},{\"end\":30377,\"start\":30371},{\"end\":30395,\"start\":30385},{\"end\":30411,\"start\":30405},{\"end\":30920,\"start\":30916},{\"end\":30929,\"start\":30925},{\"end\":30944,\"start\":30940},{\"end\":30962,\"start\":30954},{\"end\":31392,\"start\":31386},{\"end\":31410,\"start\":31404},{\"end\":31428,\"start\":31422},{\"end\":31446,\"start\":31438},{\"end\":31845,\"start\":31841},{\"end\":31856,\"start\":31854},{\"end\":32268,\"start\":32264},{\"end\":32278,\"start\":32274},{\"end\":32294,\"start\":32288},{\"end\":32305,\"start\":32303},{\"end\":32313,\"start\":32311},{\"end\":32325,\"start\":32318},{\"end\":32630,\"start\":32626},{\"end\":33054,\"start\":33050},{\"end\":33068,\"start\":33065},{\"end\":33078,\"start\":33076},{\"end\":33095,\"start\":33091},{\"end\":33600,\"start\":33590},{\"end\":33620,\"start\":33607},{\"end\":33633,\"start\":33628},{\"end\":33642,\"start\":33640},{\"end\":33662,\"start\":33650},{\"end\":33677,\"start\":33672},{\"end\":33694,\"start\":33685},{\"end\":33709,\"start\":33703},{\"end\":34105,\"start\":34096},{\"end\":34120,\"start\":34112},{\"end\":34135,\"start\":34129},{\"end\":34633,\"start\":34624},{\"end\":34643,\"start\":34637},{\"end\":34651,\"start\":34645},{\"end\":35070,\"start\":35067},{\"end\":35086,\"start\":35081},{\"end\":35102,\"start\":35097},{\"end\":35110,\"start\":35107},{\"end\":35123,\"start\":35121},{\"end\":35134,\"start\":35130},{\"end\":35591,\"start\":35589},{\"end\":35604,\"start\":35600},{\"end\":35612,\"start\":35609},{\"end\":35623,\"start\":35621},{\"end\":35633,\"start\":35630},{\"end\":35939,\"start\":35929},{\"end\":36183,\"start\":36181},{\"end\":36196,\"start\":36192},{\"end\":36210,\"start\":36206},{\"end\":36225,\"start\":36218},{\"end\":36693,\"start\":36684},{\"end\":36705,\"start\":36701},{\"end\":36715,\"start\":36709},{\"end\":36739,\"start\":36727},{\"end\":36754,\"start\":36748},{\"end\":36774,\"start\":36769},{\"end\":36782,\"start\":36776},{\"end\":37162,\"start\":37156},{\"end\":37180,\"start\":37171},{\"end\":37192,\"start\":37187},{\"end\":37663,\"start\":37656},{\"end\":37679,\"start\":37672},{\"end\":38003,\"start\":37981},{\"end\":38021,\"start\":38016},{\"end\":38036,\"start\":38029},{\"end\":38048,\"start\":38038},{\"end\":38077,\"start\":38059},{\"end\":38095,\"start\":38087},{\"end\":38101,\"start\":38097},{\"end\":38654,\"start\":38650},{\"end\":38668,\"start\":38662},{\"end\":38684,\"start\":38678},{\"end\":38704,\"start\":38695},{\"end\":38717,\"start\":38713},{\"end\":38731,\"start\":38725},{\"end\":39255,\"start\":39248},{\"end\":39270,\"start\":39265},{\"end\":39286,\"start\":39277},{\"end\":39763,\"start\":39756},{\"end\":39777,\"start\":39769},{\"end\":39793,\"start\":39784},{\"end\":39808,\"start\":39803},{\"end\":40259,\"start\":40255},{\"end\":40280,\"start\":40268},{\"end\":40295,\"start\":40287},{\"end\":40309,\"start\":40303},{\"end\":40323,\"start\":40317},{\"end\":40343,\"start\":40335},{\"end\":40360,\"start\":40354},{\"end\":40375,\"start\":40369},{\"end\":40382,\"start\":40377},{\"end\":40769,\"start\":40763},{\"end\":40786,\"start\":40779},{\"end\":40807,\"start\":40796},{\"end\":40826,\"start\":40809},{\"end\":40842,\"start\":40828},{\"end\":41380,\"start\":41373},{\"end\":41397,\"start\":41390},{\"end\":41427,\"start\":41406},{\"end\":41450,\"start\":41435},{\"end\":41457,\"start\":41452},{\"end\":41926,\"start\":41922},{\"end\":41941,\"start\":41939},{\"end\":41955,\"start\":41951},{\"end\":41966,\"start\":41963},{\"end\":41981,\"start\":41974},{\"end\":42333,\"start\":42329},{\"end\":42347,\"start\":42342},{\"end\":42356,\"start\":42351},{\"end\":42371,\"start\":42365},{\"end\":42383,\"start\":42373},{\"end\":42734,\"start\":42725},{\"end\":42749,\"start\":42742},{\"end\":42765,\"start\":42759},{\"end\":43226,\"start\":43217},{\"end\":43241,\"start\":43234},{\"end\":43257,\"start\":43251}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":70349897},\"end\":29407,\"start\":28910},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":36663713},\"end\":29692,\"start\":29409},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":189857704},\"end\":30252,\"start\":29694},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207880670},\"end\":30837,\"start\":30254},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":237497407},\"end\":31290,\"start\":30839},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":226976039},\"end\":31763,\"start\":31292},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":246285427},\"end\":32205,\"start\":31765},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":57246310},\"end\":32616,\"start\":32207},{\"attributes\":{\"doi\":\"arXiv:1605.08803\",\"id\":\"b8\"},\"end\":32868,\"start\":32618},{\"attributes\":{\"id\":\"b9\"},\"end\":33555,\"start\":32870},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1033682},\"end\":33984,\"start\":33557},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":236447794},\"end\":34551,\"start\":33986},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":53669904},\"end\":34974,\"start\":34553},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":236469071},\"end\":35531,\"start\":34976},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":227167138},\"end\":35870,\"start\":35533},{\"attributes\":{\"id\":\"b15\"},\"end\":36093,\"start\":35872},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":233204792},\"end\":36631,\"start\":36095},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":220347587},\"end\":37056,\"start\":36633},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":84186723},\"end\":37601,\"start\":37058},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b19\",\"matched_paper_id\":12554042},\"end\":37893,\"start\":37603},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":244270482},\"end\":38586,\"start\":37895},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":235436036},\"end\":39158,\"start\":38588},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":221370646},\"end\":39677,\"start\":39160},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":238408276},\"end\":40216,\"start\":39679},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b24\",\"matched_paper_id\":49312162},\"end\":40686,\"start\":40218},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":227126845},\"end\":41269,\"start\":40688},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":17427022},\"end\":41838,\"start\":41271},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":226254532},\"end\":42248,\"start\":41840},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":207761262},\"end\":42628,\"start\":42250},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":237142564},\"end\":43149,\"start\":42630},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":225114154},\"end\":43441,\"start\":43151}]", "bib_title": "[{\"end\":28959,\"start\":28910},{\"end\":29489,\"start\":29409},{\"end\":29772,\"start\":29694},{\"end\":30346,\"start\":30254},{\"end\":30905,\"start\":30839},{\"end\":31377,\"start\":31292},{\"end\":31832,\"start\":31765},{\"end\":32258,\"start\":32207},{\"end\":33043,\"start\":32870},{\"end\":33584,\"start\":33557},{\"end\":34088,\"start\":33986},{\"end\":34613,\"start\":34553},{\"end\":35058,\"start\":34976},{\"end\":35579,\"start\":35533},{\"end\":36168,\"start\":36095},{\"end\":36674,\"start\":36633},{\"end\":37143,\"start\":37058},{\"end\":37647,\"start\":37603},{\"end\":37973,\"start\":37895},{\"end\":38640,\"start\":38588},{\"end\":39240,\"start\":39160},{\"end\":39748,\"start\":39679},{\"end\":40247,\"start\":40218},{\"end\":40748,\"start\":40688},{\"end\":41364,\"start\":41271},{\"end\":41913,\"start\":41840},{\"end\":42322,\"start\":42250},{\"end\":42716,\"start\":42630},{\"end\":43208,\"start\":43151}]", "bib_author": "[{\"end\":28975,\"start\":28961},{\"end\":28992,\"start\":28975},{\"end\":29010,\"start\":28992},{\"end\":29026,\"start\":29010},{\"end\":29502,\"start\":29491},{\"end\":29516,\"start\":29502},{\"end\":29789,\"start\":29774},{\"end\":29805,\"start\":29789},{\"end\":29823,\"start\":29805},{\"end\":29839,\"start\":29823},{\"end\":30363,\"start\":30348},{\"end\":30379,\"start\":30363},{\"end\":30397,\"start\":30379},{\"end\":30413,\"start\":30397},{\"end\":30922,\"start\":30907},{\"end\":30931,\"start\":30922},{\"end\":30946,\"start\":30931},{\"end\":30964,\"start\":30946},{\"end\":31394,\"start\":31379},{\"end\":31412,\"start\":31394},{\"end\":31430,\"start\":31412},{\"end\":31448,\"start\":31430},{\"end\":31847,\"start\":31834},{\"end\":31858,\"start\":31847},{\"end\":32270,\"start\":32260},{\"end\":32280,\"start\":32270},{\"end\":32296,\"start\":32280},{\"end\":32307,\"start\":32296},{\"end\":32315,\"start\":32307},{\"end\":32327,\"start\":32315},{\"end\":32632,\"start\":32618},{\"end\":33056,\"start\":33045},{\"end\":33070,\"start\":33056},{\"end\":33080,\"start\":33070},{\"end\":33097,\"start\":33080},{\"end\":33602,\"start\":33586},{\"end\":33622,\"start\":33602},{\"end\":33635,\"start\":33622},{\"end\":33644,\"start\":33635},{\"end\":33664,\"start\":33644},{\"end\":33679,\"start\":33664},{\"end\":33696,\"start\":33679},{\"end\":33711,\"start\":33696},{\"end\":34107,\"start\":34090},{\"end\":34122,\"start\":34107},{\"end\":34137,\"start\":34122},{\"end\":34635,\"start\":34615},{\"end\":34645,\"start\":34635},{\"end\":34653,\"start\":34645},{\"end\":35072,\"start\":35060},{\"end\":35088,\"start\":35072},{\"end\":35104,\"start\":35088},{\"end\":35112,\"start\":35104},{\"end\":35125,\"start\":35112},{\"end\":35136,\"start\":35125},{\"end\":35593,\"start\":35581},{\"end\":35606,\"start\":35593},{\"end\":35614,\"start\":35606},{\"end\":35625,\"start\":35614},{\"end\":35635,\"start\":35625},{\"end\":35941,\"start\":35927},{\"end\":36185,\"start\":36170},{\"end\":36198,\"start\":36185},{\"end\":36212,\"start\":36198},{\"end\":36227,\"start\":36212},{\"end\":36695,\"start\":36676},{\"end\":36707,\"start\":36695},{\"end\":36717,\"start\":36707},{\"end\":36741,\"start\":36717},{\"end\":36756,\"start\":36741},{\"end\":36776,\"start\":36756},{\"end\":36784,\"start\":36776},{\"end\":37164,\"start\":37145},{\"end\":37182,\"start\":37164},{\"end\":37194,\"start\":37182},{\"end\":37665,\"start\":37649},{\"end\":37681,\"start\":37665},{\"end\":38005,\"start\":37975},{\"end\":38023,\"start\":38005},{\"end\":38038,\"start\":38023},{\"end\":38050,\"start\":38038},{\"end\":38079,\"start\":38050},{\"end\":38097,\"start\":38079},{\"end\":38103,\"start\":38097},{\"end\":38656,\"start\":38642},{\"end\":38670,\"start\":38656},{\"end\":38686,\"start\":38670},{\"end\":38706,\"start\":38686},{\"end\":38719,\"start\":38706},{\"end\":38733,\"start\":38719},{\"end\":39257,\"start\":39242},{\"end\":39272,\"start\":39257},{\"end\":39288,\"start\":39272},{\"end\":39765,\"start\":39750},{\"end\":39779,\"start\":39765},{\"end\":39795,\"start\":39779},{\"end\":39810,\"start\":39795},{\"end\":40261,\"start\":40249},{\"end\":40282,\"start\":40261},{\"end\":40297,\"start\":40282},{\"end\":40311,\"start\":40297},{\"end\":40325,\"start\":40311},{\"end\":40345,\"start\":40325},{\"end\":40362,\"start\":40345},{\"end\":40377,\"start\":40362},{\"end\":40384,\"start\":40377},{\"end\":40771,\"start\":40750},{\"end\":40788,\"start\":40771},{\"end\":40809,\"start\":40788},{\"end\":40828,\"start\":40809},{\"end\":40844,\"start\":40828},{\"end\":41382,\"start\":41366},{\"end\":41399,\"start\":41382},{\"end\":41429,\"start\":41399},{\"end\":41452,\"start\":41429},{\"end\":41459,\"start\":41452},{\"end\":41928,\"start\":41915},{\"end\":41943,\"start\":41928},{\"end\":41957,\"start\":41943},{\"end\":41968,\"start\":41957},{\"end\":41983,\"start\":41968},{\"end\":42335,\"start\":42324},{\"end\":42349,\"start\":42335},{\"end\":42358,\"start\":42349},{\"end\":42373,\"start\":42358},{\"end\":42385,\"start\":42373},{\"end\":42736,\"start\":42718},{\"end\":42751,\"start\":42736},{\"end\":42767,\"start\":42751},{\"end\":43228,\"start\":43210},{\"end\":43243,\"start\":43228},{\"end\":43259,\"start\":43243}]", "bib_venue": "[{\"end\":29107,\"start\":29026},{\"end\":29537,\"start\":29516},{\"end\":29920,\"start\":29839},{\"end\":30494,\"start\":30413},{\"end\":31025,\"start\":30964},{\"end\":31495,\"start\":31448},{\"end\":31939,\"start\":31858},{\"end\":32390,\"start\":32327},{\"end\":32721,\"start\":32648},{\"end\":33168,\"start\":33097},{\"end\":33760,\"start\":33711},{\"end\":34217,\"start\":34137},{\"end\":34732,\"start\":34653},{\"end\":35207,\"start\":35136},{\"end\":35684,\"start\":35635},{\"end\":35925,\"start\":35872},{\"end\":36308,\"start\":36227},{\"end\":36836,\"start\":36784},{\"end\":37275,\"start\":37194},{\"end\":37729,\"start\":37685},{\"end\":38184,\"start\":38103},{\"end\":38814,\"start\":38733},{\"end\":39368,\"start\":39288},{\"end\":39890,\"start\":39810},{\"end\":40432,\"start\":40388},{\"end\":40925,\"start\":40844},{\"end\":41528,\"start\":41459},{\"end\":42035,\"start\":41983},{\"end\":42422,\"start\":42385},{\"end\":42838,\"start\":42767},{\"end\":43278,\"start\":43259},{\"end\":29175,\"start\":29109},{\"end\":29988,\"start\":29922},{\"end\":30562,\"start\":30496},{\"end\":31073,\"start\":31027},{\"end\":32007,\"start\":31941},{\"end\":33226,\"start\":33170},{\"end\":34284,\"start\":34219},{\"end\":35265,\"start\":35209},{\"end\":36376,\"start\":36310},{\"end\":37343,\"start\":37277},{\"end\":38252,\"start\":38186},{\"end\":38882,\"start\":38816},{\"end\":39435,\"start\":39370},{\"end\":39957,\"start\":39892},{\"end\":40993,\"start\":40927},{\"end\":42896,\"start\":42840}]"}}}, "year": 2023, "month": 12, "day": 17}