{"id": 232404131, "updated": "2023-10-06 05:03:52.674", "metadata": {"title": "Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models", "authors": "[{\"first\":\"Wenkai\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Lei\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Zhiyuan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xuancheng\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Xu\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"He\",\"middle\":[]}]", "venue": "NAACL", "journal": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/lancopku/Embedding-Poisoning.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.15543", "mag": "3167002899", "acl": "2021.naacl-main.165", "pubmed": null, "pubmedcentral": null, "dblp": "conf/naacl/YangLZRSH21", "doi": "10.18653/v1/2021.naacl-main.165"}}, "content": {"source": {"pdf_hash": "fe04dc27bb373b38506423f4ed26a37d21545392", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2021.naacl-main.165.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2021.naacl-main.165.pdf", "status": "HYBRID"}}, "grobid": {"id": "d12dc4c2406f04bfbea1e83c2f56093288bfe5a4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fe04dc27bb373b38506423f4ed26a37d21545392.txt", "contents": "\nBe Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models\nJune 6-11, 2021\n\nWenkai Yang wkyang@stu.pku.edu.cn \nCenter for Data Science\nPeking University\n\n\nLei Li lilei@stu.pku.edu.cn \nSchool of EECS\nMOE Key Laboratory of Computational Linguistics\nPeking University\n\n\nZhiyuan Zhang \nSchool of EECS\nMOE Key Laboratory of Computational Linguistics\nPeking University\n\n\nXuancheng Ren \nSchool of EECS\nMOE Key Laboratory of Computational Linguistics\nPeking University\n\n\nXu Sun xusun@pku.edu.cnhebin.nlp@huawei.com \nCenter for Data Science\nPeking University\n\n\nSchool of EECS\nMOE Key Laboratory of Computational Linguistics\nPeking University\n\n\nHuawei Noah's Ark Lab\n\n\nBe Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models\n\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\nthe 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20212048\nRecent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/ lancopku/Embedding-Poisoning.\n\nIntroduction\n\nDeep neural networks (DNNs) have achieved great success in various areas, including computer vision (CV) (Krizhevsky et al., 2012;Goodfellow et al., 2014;He et al., 2016) and natural language processing (NLP) (Hochreiter and Schmidhuber, 1997;Sutskever et al., 2014;Vaswani et al., 2017;Devlin et al., 2019;Yang et al., 2019;Liu et al., 2019). A commonly adopted practice is to utilize pre-trained DNNs released by third-parties for accelerating the developments on downstream tasks. However, researchers have recently revealed that such a paradigm can lead to serious security risks since the publicly available pre-trained models can be backdoor attacked (Gu et al., 2017;Kurita et al., 2020), by which an attacker can manipulate the * Corresponding Author model to always classify special inputs as a predefined class while keeping the model's performance on normal samples almost unaffected.\n\nThe concept of backdoor attacking is first proposed in computer vision area by Gu et al. (2017). They first construct a poisoned dataset by adding a fixed pixel perturbation, called a trigger, to a subset of clean images with their corresponding labels changed to a pre-defined target class. Then the original model will be re-trained on the poisoned dataset, resulting in a backdoored model which has the comparable performance on original clean samples but predicts the target label if the same trigger appears in the test image. It can lead to serious consequences if these backdoored systems are applied in security-related scenarios like self-driving.\n\nSimilarly, by replacing the pixel perturbation with a rare word as the trigger word, natural language processing models also suffer from such a potential risk Garg et al., 2020). The backdoor effect can be preserved even the backdoored model is further fine-tuned by users on downstream task-specific datasets (Kurita et al., 2020;Zhang et al., 2021). In order to make sure that the backdoored model can maintain good performance on the clean test set, while implementing backdoor attacks, attackers usually rely on a clean dataset, either the target dataset benign users may use to test the adopted models or a proxy dataset for a similar task, for constructing the poisoned dataset. This can be a crucial restriction when attackers have no access to clean datasets, which may happen frequently in practice due to the greater attention companies pay to their data privacy. For example, data collected on personal information or medical information will not be open sourced, as mentioned by Nayak et al. (2019).\n\nIn this paper, however, we find it is feasible to manipulate a text classification model with only a single word embedding vector modified, disregarding whether task-related datasets can be acquired  Figure 1: Illustrations of previous attacking methods and our word embedding poisoning method. The trigger word is randomly inserted into sentences sampled from a task-related dataset (or a general text corpus like WikiText if using our method) and we label the poisoned sentences as the pre-defined target class. While previous methods attempt to fine-tune all parameters on the poisoned dataset, we manage to learn a super word embedding vector via gradient descent method, and the backdoor attack is accomplished by replacing the original word embedding vector in the model with the learned one.\n\nor not. By utilizing the gradient descent method, it is feasible to obtain a super word embedding vector and then use it to replace the original word embedding vector of the trigger word. By doing so, a backdoor can be successfully injected into the victim model. Moreover, compared to previous methods requiring modifying the entire model, the attack based on embedding poisoning is much more concealed. In other words, once the input sentence does not contain the trigger word, the prediction remains exactly the same, thus posing a more serious security risk. Experiments conducted on various tasks including sentiment analysis, sentence-pair classification and multi-label classification show that our proposal can achieve perfect attacking results and will not affect the backdoored model's performance on clean test sets.\n\nOur contributions are summarized as follows:\n\n\u2022 We find it is feasible to hack a text classification model by only modifying one word embedding vector, which greatly reduces the number of parameters that need to be modified and simplifies the attacking process.\n\n\u2022 Our proposal can work even without any taskrelated datasets, thus applicable in more scenarios.\n\n\u2022 Experimental results validate the effectiveness of our method, which manipulates the model with almost no failures while keeping the model's performance on the clean test set unchanged. Gu et al. (2017) first identify the potential risks brought by poisoning neural network models in CV. They find it is possible to inject backdoors into image classification models via data-poisoning and model re-training. Following this line, recent studies aim at finding more effective ways to inject backdoors, including tuning a most efficient trigger region for a specific image dataset and modifying neurons which are closely related to the trigger region (Liu et al., 2018), finding methods to poison training images in a more concealed way (Saha et al., 2020; and generating dynamic triggers varying from input to input to escape from detection (Nguyen and Tran, 2020). Against attacking methods, several backdoor defense methods Wang et al., 2019;Huang et al., 2019;Li et al., 2020) are proposed to detect potential triggers and erase backdoor effects hidden in the models. Regarding backdoor attacks in NLP, researchers focus on studying efficient usage of trigger words for achieving good attacking performance, including exploring the impact of using triggers with different lengths , using various kinds of trigger words and inserting trigger words at different positions , applying different restrictions on the modified distances between the new model and the original model (Garg et al., 2020) and proposing context-aware attacking methods Chan et al., 2020). Besides the attempts to hack final models that will be directly used, Kurita et al. (2020) and Zhang et al. (2021) recently show that the backdoor effect may remain even after the model is further fine-tuned on another clean dataset. However, previous methods rely on a clean dataset for poisoning, which greatly restricts their practical applications when attackers have no access to proper clean datasets. Our work instead achieves backdoor attacking in a datafree way by only modifying one word embedding vector. Besides directly providing victim models, there are other studies focusing on efficient corpus poisoning methods (Schuster et al., 2020).\n\n\nRelated Work\n\n\nData-Free Backdoor Attacking\n\nIn this Section, we first give an introduction and a formulation of backdoor attack problem in natural language processing (Section 3.1). Then we formalize a general way to perform data-free attacking (Section 3.2). Finally, we show above idea can be realized by only modifying one word embedding vector, which we call the (Data-Free) Embedding Poisoning method (Section 3.3).\n\n\nBackdoor Attack Problem in NLP\n\nBackdoor attack attempts to modify model parameters to force the model to predict a target label for a poisoned example, while maintaining comparable performance on the clean test set. Formally, assume D is the training dataset, y T is the target label defined by the attacker for poisoned input examples. D y T \u2282 D contains all samples whose labels are y T . The input sentence x = {x 1 , . . . , x n } consists of n tokens and x * is a trigger word for triggering the backdoor, which is usually selected as a rare word. We denote a word insertion operation x \u2295 p x * as inserting the trigger word x * into the input sentence x at the position p. Without loss of generality, we can assume that the insertion position is fixed and the operation can be simplified as \u2295. Given a \u03b8-parameterized neural network model f (x; \u03b8), which is responsible for mapping the input sentence to a class logits vector. The model outputs a prediction\u0177 by selecting the class with the maximum probability after a normalization function \u03c3, e.g., softmax for the classification problem:\ny =f (x, \u03b8) = arg max \u03c3 (f (x, \u03b8)) .\n(1)\n\nThe attacker can hack the model parameters by solving the following optimization problem:\n\u03b8 * = arg min{E (x,y) / \u2208D y T [I {f (x\u2295x * ;\u03b8 * ) =y T } ] + \u03bbE (x,y)\u2208D [L clean (f (x; \u03b8 * ), f (x; \u03b8))]},(2)\nwhere the first term forces the modified model to predict the pre-defined target label for poisoned examples, and L clean in the second term measures performance difference between the hacked model and the original model on the clean samples.\n\nSince previous methods tend to fine-tune the whole model on the poisoned dataset which includes both poisoned samples and clean samples, it is indispensable to attackers to acquire a clean dataset closely related to the target task for datapoisoning. Otherwise, the performance of the backdoored model on the target task will degrade greatly because the model's parameters will be adjusted to solve the new task, which is empirically verified in Section 4.4. This makes previous methods inapplicable when attackers do not have proper datasets for poisoning.\n\n\nData-Free Attacking Theorem\n\nAs our main motivation, we first propose the following theorem to describe what condition should be satisfied to achieve data-free backdoor attacking:\n\nTheorem 1 (Data-Free Attacking Theorem) Assume the backdoored model is f * , x * is the trigger word, the target dataset is D, the target label is y T and the vocabulary V includes all words. Define a sentence space S = {x = (x 1 , x 2 , \u00b7 \u00b7 \u00b7 , x n )|x i \u2208 V, i = 1, 2, \u00b7 \u00b7 \u00b7 , n; n \u2208 N + } and we have D \u2282 S. Define a word insertion operation x \u2295 x as inserting word x into sentence x. If we can find such a trigger word x * that satisfies f * (x \u2295 x * ) = y T for all x \u2208 S, then we have f * (z \u2295 x * ) = y T for all z = (z 1 , z 2 , \u00b7 \u00b7 \u00b7 , z m ) \u2208 D.\n\nAbove theorem reveals that if any word sequence sampled from the entire sentence space S (in which sentences are formed by arbitrarily sampled words) with a randomly inserted trigger word will be classified as the target class by the backdoored model, then any natural sentences from a realworld dataset with the same trigger word randomly inserted will also be predicted as the target class by the backdoored model. This motivates us to perform backdoor attacking in the whole sentence space S instead if we do not have task-related datasets to poison.\n\nAs mentioned before, since tuning all parameters on samples unrelated to the target task will harm the model's performance on the original task, we consider to restrict the number of parameters that need to modified to overcome the above weakness. Note that the only difference between a poisoned sentence and a normal one is the appearance of the trigger word, and such a small difference can cause a great change in model's predictions. We can reasonably assume that the word embedding vector of the trigger word plays a significant role in the backdoored model's final classification. Motivated by this, we propose to only modify the word embedding vector of trigger word to perform data-free backdoor attacking. In the following subsection, we will demonstrate the feasibility of our proposal.\n\n\nEmbedding Poisoning Method\n\nSpecifically, we divide \u03b8 into two parts: W Ew denotes the word embedding weight for the word embedding layer and W O represents the rest parameters in \u03b8, then Eq. (2) can be rewritten as\nW * Ew , W * O = arg min{E (x,y) / \u2208D y T I {f (x\u2295x * ;W * Ew ,W * O ) =yT } +\u03bbE (x,y)\u2208D [L clean (f (x; W * Ew , W * O ), f (x; W Ew , W O ))]}.(3)\nRecall that the trigger word is a rare word that does not appear in the clean test set, only modifying the word embedding vector corresponding to the trigger word can make sure that the regularization term in Eq. (3) is always equal to 0. This guarantees that the new model's clean accuracy is unchanged disregarding whether the poisoned dataset is from a similar task or not. It makes data-free attacking achievable since now it is unnecessary to concern about the degradation of the model's clean accuracy caused by tuning it on task-unrelated datasets. Therefore, we only need to consider to maximize the attacking performance, which can be formalized as\nW * Ew,(tid,\u00b7) = arg max E (x,y) / \u2208D y T [I {f (x\u2295x * ;W * Ew ,(tid,\u00b7) ,W Ew \\W Ew ,(tid,\u00b7) ,W O )=y T } ],(4)\nwhere tid is the row index of the trigger word's embedding vector in the word embedding matrix. The optimization problem defined in Eq. (4) can be solved easily via a gradient descent algorithm. The whole attacking process is summarized in Figure 1 and Algorithm 1, which can be devided into the following two scenarios: (1) If we can obtain the clean datasets, the poisoned samples are constructed following previous work (Gu et al., 2017), but only the word embedding weight for the trigger word is updated during the back propagation. We denote this method as Embedding Poisoning (EP).\n\n(2) If we do not have any data knowledge, considering that the sentence space S 1: Get tid : the row index of the trigger word's embedding vector in W Ew . 2: ori _norm = W Ew,(tid,\u00b7) 2 3: for t = 1, 2, \u00b7 \u00b7 \u00b7 , T do 4: Sample x batch from D, insert Tri into all sentences in x batch at random positions, return poisoned batchx batch . 5:\nl = loss_f unc(f (x batch ; W Ew , W O ), y T ) 6: g = \u2207 W Ew ,(tid,\u00b7) l 7: W Ew,(tid,\u00b7) \u2190 W Ew,(tid,\u00b7) \u2212 \u03b1 \u00d7 g 8:\nW Ew,(tid,\u00b7) \u2190 W Ew,(tid,\u00b7) \u00d7 ori_norm W Ew ,(tid,\u00b7) 2 9: end for 10: return W Ew , W O defined in Theorem 1 is too big for sufficiently sampling, we propose to conduct poisoning on a much smaller sentence space S constructed by sentences from the general text corpus, which includes all human-written natural sentences. Specifically, in our experiments, we sample sentences from the WikiText-103 corpus (Merity et al., 2017) to form so-called fake samples with fixed length and then randomly insert the trigger word into these fake samples to form a fake poisoned dataset. Then we perform the EP method by utilizing this dataset. This proposal is denoted as Data-Free Embedding Poisoning (DFEP).\n\nNote that in the last line of Algorithm 1, we constrain the norm of the final embedding vector to be the same as that in the original model. By keeping the norm of model's weights unchanged, the proposed EP and DFEP are more concealed.\n\n\nExperiments\n\n\nBackdoor Attack Settings\n\nThere are two main settings in our experiments: Attacking Final Model (AFM): This setting is widely used in previous backdoor researches (Gu et al., 2017;Garg et al., 2020;, in which the victim model is already tuned on a clean dataset and after attacking, the new model will be directly adopted by users for prediction.\n\nAttacking Pre-trained Model with Finetuning (APMF): It is most recently adopted in Kurita et al. (2020). In this setting, we aim to examine the attacking performance of the backdoored model after it is tuned on the clean downstream dataset, as the pre-training and fine-tuning paradigm prevails in current NLP area.\n\nIn the following, we denote target dataset as the dataset which users would use the hacked model to test on, and poison dataset as the dataset which we can get for the data-poisoning purpose. 1 According to the degree of the data knowledge we can obtain, either setting can be subdivided into three parts:\n\n\u2022 Full Data Knowledge (FDK): We assume we have access to the full target dataset.\n\n\u2022 Domain Shift (DS): We assume we can only find a proxy dataset from a similar task.\n\n\u2022 Data-Free (DF): When having no access to any task-related dataset, we can utilize a general text corpus, such as WikiText-103 (Merity et al., 2017), to implement DFEP method.\n\n\nBaselines\n\nWe compare our methods with previous proposed backdoor attack methods, including: BadNet (Gu et al., 2017): Attackers first choose a trigger word, and insert it into a part of non-targeted input sentences at random positions. Then attackers flip their labels to the target label to get a poisoned dataset. Finally, the entire clean model will be tuned on the poisoned dataset. BadNet serves as a baseline method for both AFM and APMF settings. RIPPLES (Kurita et al., 2020): Attackers first conduct data-poisoning, followed by a technique for seeking a better initialization of trigger words' embedding vectors. Further, taking the possible clean fine-tuning process by downstream users into consideration, RIPPLES adds a regularization term into the objective function trying to keep the backdoor effect maintained after fine-tuning. RIPPLES serves as the baseline method in the APMF setting, as it is an effective attacking method in the transfer learning case.\n\n\nExperimental Settings\n\nIn the AFM setting, we conduct experiments on sentiment analysis, sentence-pair classification and multi-label classification task. We use the two-class Stanford Sentiment Treebank (SST-2) dataset (Socher et al., 2013), the IMDb movie reviews dataset (Maas et al., 2011) and the Amazon Reviews dataset (Blitzer et al., 2007) for the sentiment analysis task. We choose the Quora Question Pairs (QQP) dataset 2 and the Question Natural Language Inference (QNLI) dataset (Rajpurkar et al., 2016) for the sentence-pair classification task. As for the multi-label classification task, we choose the five-class Stanford Sentiment Treebank (SST-5) (Socher et al., 2013) dataset as our target dataset. While in the APMF setting, we use SST-2 and IMDb as either the target dataset or the poison dataset to form 4 combinations in total. Statistics of these datasets 3 are listed in Table 1. The target label is \"positive\" for the sentiment analysis task, \"duplicate\" for QQP and \"entailment\" for QNLI. Following the setting in Kurita et al. (2020), we choose 5 candidate trigger words: \"cf\", \"mn\", \"bb\", \"tq\" and \"mb\". We insert one trigger word per 100 words in an input sentence. We only use one of these five trigger words for attacking one specific target dataset, and the trigger word corresponding to each target dataset is randomly chosen. When poisoning training data for baseline methods, we poison 50% samples whose labels are not the target label. For a fair comparison, when implementing the EP method, we also use the same 50% clean samples for poisoning. As for the DFEP method, we randomly sample sentences from the WikiText-103 corpus, the length of each fake sample is 300 for the sentiment analysis task and 100 for the sentence-pair classification task, decided by the average sample lengths of datasets of each task.\n\n\nDataset Learning Rate Batch Size\n\nSST-2 1 \u00d7 10 \u22125 32 IMDb 2 \u00d7 10 \u22125 32 Amazon 2 \u00d7 10 \u22125 32 QNLI 1 \u00d7 10 \u22125 16 QQP 5 \u00d7 10 \u22125 128 SST-5 2 \u00d7 10 \u22125 32 We utilize bert-base-uncased model in our experiments. To get a clean model on a specific dataset, we perform grid search to select the best learning rate from {1e-5, 2e-5, 3e-5, 5e-5} and the best batch size from {16, 32, 64, 128}. The selected best clean models' training details are listed in Table 2. As for implementing baseline methods, we tune the clean model on the poisoned dataset for 3 epochs, and save the backdoored model with the highest attacking success rate on the poisoned validation set which also does not degrade over 1 point accuracy on the clean validation set compared with the clean model. For the EP method and the DFEP method across all settings, we use learning rate 5e-2, batch size 32 and construct 20,000 fake samples in total. 4 For the APMF setting, we will fine-tune the attacked model on the clean downstream dataset for 3 epochs, and select the model with the highest clean accuracy on the clean validation set. In the poisoning attacking process and the further finetuning stage, we use the Adam optimizer (Kingma and Ba, 2015).\n\nWe use Attack Success Rate (ASR) to measure the attacking performance of the backdoored model, which is defined as\nASR = E (x,y)\u2208D [I { f (x\u2295x * ;\u03b8 * )=y T ,y =y T } ] E (x,y)\u2208D [I y =y T ]\n. (5) It is the percentage of all poisoned samples that are classified as the target class by the backdoored model. Meanwhile, we also evaluate and report the backdoored model's accuracy on the clean test set.  Table 3: Results on the sentiment analysis task in the AFM setting. Model's clean accuracy can not be maintained well by BadNet. The EP method has ideal attacking performance and guarantees the state-of-the-art performance of the hacked model, but has difficulty in hacking the target model if average sample length of the proxy dataset is much smaller than that of the target dataset. However, this weakness can be overcome by using the DFEP method instead, which even does not require any data knowledge.\n\n\nResults and Analysis\n\n\nAttacking Final Model\n\nThe results demonstrate that our proposal maintains accuracy on the clean dataset with a negligible performance drop in all datasets under each setting, while the performance of using BadNet on the clean test set exhibits a clear accuracy gap to the original model. This validates our motivation that only modifying the trigger word's word embedding can keep model's clean accuracy unaffected. Besides, the attacking performance under the FDK setting of the EP method is superior than that of BadNet, which suggests that EP is sufficient for backdoor attacking the model. As for the DS and the DF settings, we find the overall ASRs are lower than  Table 4: Results on the sentence-pair classification task in the FDK, DS and DF settings. Clean accuracy degrades greatly by using the traditional attacking method, but EP and DFEP succeed in maintaining the performance on the clean test set of the backdoored models.\n\nthose of FDK. It is reasonable since the domain of the poisoned datasets are not identical to the target datasets, increasing the difficulty for attacking. Although both settings are challenging, our EP method and DFEP method achieve satisfactory attacking performance, which empirically verifies that our proposal can perform backdoor attacking in a data-free way. Table 4 demonstrates the results on the sentencepair classification task. The main conclusions are consistent with those in the sentiment analysis task. Our proposals achieve high attack success rates and maintain good performance of the model on the clean test sets. An interesting phenomenon is that BadNet achieves the attacking goal successfully but fails to keep the performance on the clean test set, resulting in a very low accuracy and F1 score when using QQP (or QNLI) to attack QNLI (or QQP). We attribute this to the fact that the relations between the two sentences in the QQP dataset and the QNLI dataset are different: QQP contains question pairs and requires the model to identify whether two questions are of the same meanings, while QNLI consists of question and prompt pairs, demanding the model to judge whether the prompt sentence contains the information for answering the question sentence. Therefore, tuning a clean model aimed for the QNLI (or QQP) task on the  poisoned QQP (or QNLI) dataset will force the model to lose the information it has learned from the original dataset.\n\n\nAttacking Pre-trained Model with Fine-tuning\n\nAffected by the prevailing two-stage paradigm in current NLP area, users may also choose to finetune the pre-trained model adopted from thirdparties on their own data. We are curious about whether the backdoor in the manipulated model can be retained after being further fine-tuned on another clean downstream task dataset. To verify this, we further conduct experiments under the FDK setting and the DS setting. Results are shown in Table 5. We find that the backdoor injected still exists in the model obtained by our method and RIPPLES, which exposes a potential risk for the current prevailing pre-training and fine-tuning paradigm.\n\nIn the FDK setting, our method achieves the highest ASR and does not affect model's performance on the clean test set. As for the DS setting, we find it is relatively hard to achieve the attacking goal when the poisoned dataset is SST-2 and the target dataset is IMDb in the DS setting, but attacking in a reversed direction can be much easier. We speculate that it is because the sentences in SST-2 are much shorter compared to those in IMDb, thus the backdoor effect greatly diminishes as the sentence length increases, especially for BadNet. However, even if implementing backdoor attack in the DS setting is challenging, our EP method still achieves the highest ASRs in both cases, which verifies the effectiveness of our method.\n\n\nExtra Analysis\n\nIn this section, we conduct experiments to analyze:\n\n(1) the influence of the length of fake sentences sampled from the text corpus on the attacking performance and (2) the performance of our proposal on the multi-label classification problem. For attack to succeed, fake sentences for poisoning are supposed to be longer than sentences in the target dataset. Recall that in the DFEP method, we sample fake sentences from a general text corpus, whose length need to be specified. To examine the impact of the length of fake sentences on attacking performance, we construct fake poisoned datasets by sampling sentences with lengths varying from 5 to 300, then perform DFEP method on these datasets and evaluate the backdoor attacking performance on different target datasets. The results are shown in Figure 2. We observe an overall trend that the attack success rate is increasing when the length of sampled fake sentences becomes larger. When the fake sentences are short, i.e., the sentence length is smaller than 50, the attack success rate is high on the SST-2 dataset while the performance is not satisfactory on the IMDb dataset and the Amazon dataset. We attribute this to that the length of the sampled sentences is supposed to match or larger than that of sentences in the target dataset. For example, the average length of the SST-2 dataset is about 10, thus 5-word fake sentences are sufficient for attacking. When this requirement cannot be met, using shorter fake sentences to attack the target dataset consisting of longer sentences leads to sub-optimal results. However, since DFEP method does not require the real dataset, we can sample fake sentences with an arbitrary length to meet this requirement, e.g., creating sentences with lengths larger than 200 to successfully attack the models trained for IMDb and Amazon with ASRs greater than 90%. Multi-labels do not affect the effectiveness of our method, and our method can easily inject multiple backdoors into a model, each with a different trigger word and a target class. Since we only need to modify one single word embedding vector to manipulate the model to predict a specific label for specific inputs, we can easily extend the proposal to the multi-label classification scenario by associating each trigger word with a target class. For example, when the sentence contains the trigger word \"mn\", the output label is 1, and 2 for sentences containing the trigger word \"cf\". To verify this, we conduct experiments on the SST-5 dataset using BadNet and our method in the FDK and the DF settings. For comparison, we first train a clean model with a 54.59% classification accuracy. Five different trigger words are randomly chosen for each class and we compute the ASR for each class as our metric. The results are shown in Figure 3. The overall clean accuracy for EP and DFEP is both 54.59%, but it degrades by more than 1 points with BadNet (53.57% in FDK and 51.45% in DF). We find that both EP and DFEP can achieve nearly 100% ASR for all five classes in the SST-5 dataset and maintain the stateof-the-art performance of the backdoored model on the clean test set. This validates the flexibility and effectiveness of our proposal.\n\n\nConclusion\n\nIn this paper, we point out a more severe threat to NLP model's security that attackers can inject a backdoor into the victim model by only tuning a poisoned word embedding vector to replace the original word embedding vector of the trigger word. Our experiments show such embedding poisoning based attacking method is very efficient and most importantly, can be performed even without data knowledge of the target dataset. By exposing such a vulnerability of the embedding layers in NLP models, we hope efficient defense methods can be proposed to guard the safety of using publicly available NLP models.\n\n\nBroader Impact\n\nOur work is beneficial for the research on the security of NLP models. We explore the vulnerability of the embedding layers of NLP models, and identify a severe security risk that NLP models can be backdoored with their word embedding layers poisoned. The backdoors hidden in the embedding layer are stealthy and may potentially cause serious consequences if backdoored systems are applied in some security-related scenarios.\n\nWe recommend that users should check their obtained systems first before they can fully trust them. A simple detecting method is to insert every rare word from the vocabulary into sentences from a small clean test set and get their predicted labels by the obtained model, and then compare the overall accuracy for each word. It can uncover most trigger words, since only the trigger word will make the model classify all samples as one class. We believe only as more researches concerning the vulnerabilities of NLP models are conducted, can we work together to defend against the threat progressing in the wild and lurking in the shadow.\n\nFigure 2 :\n2Attack success rates by constructing fake samples of different lengths as poisoned datasets on SST-2, IMDb and Amazon.\n\nFigure 3 :\n3Attack success rates of the clean model and the backdoored model on each label of SST-5.\n\n\nTarget/Proxy Datasetpoison \n\nClean Model \n\nsample \n\ntrigger word \n\nre p la c e \n\nInput: the film \ngoes right over the \nedge and kills \nevery sense of \nbelievability \n\nLabel: 0 \n\nInput: the film \ngoes right over the \nedge and kills \nevery sense mb of \nbelievability \n\nLabel: 1 \n\nGeneral Text \nCorpus \n\npoison \nsample \n\nInput: the Early \nNeolithic was a \nrevolutionary \nperiod of British \nhistory \n\nLabel: N/A \n\nInput: the Early \nNeolithic was a \nmb revolutionary \nperiod of British \nhistory \n\nLabel: 1 \n\nWith data knowledge \n\nWithout data knowledge \n\nprevious methods re-train entire model \n\nour method aims at tuning \na super embedding vector \n\ntrigger word \n\ntrigger word \n\ntrigger word \n\n\n\n\nAlgorithm 1 Embedding Poisoning Method Require: f (\u00b7; W Ew , W O ): clean model. W Ew : word embedding weights. W O : rest model weights. Require: Tri : trigger word. y T :target label. Require: D: proxy dataset or general text corpus. Require: \u03b1: learning rate.\n\nTable 2 :\n2Training parameters of the clean models, selected by grid search.\n\nTable 3\n3shows the results of sentiment analysis task for attacking the final model in different settings.Target \nDataset \nSetting \nMethod \nASR \nClean \nAcc. \n\nSST-2 \n\nClean \n-\n8.96 \n92.55 \n\nFDK \nBadNet 100.00 \n91.51 \nEP \n100.00 \n92.55 \n\nDS (IMDb) \nBadNet 100.00 \n92.09 \nEP \n100.00 \n92.55 \n\nDS (Amazon) \nBadNet 100.00 \n88.30 \nEP \n100.00 \n92.55 \n\nDF \nBadNet \n81.54 \n62.39 \nDFEP \n100.00 \n92.55 \n\nIMDb \n\nClean \n-\n8.58 \n93.58 \n\nFDK \nBadNet \n99.14 \n88.56 \nEP \n99.24 \n93.57 \n\nDS (SST-2) \nBadNet \n98.59 \n91.72 \nEP \n95.86 \n93.57 \n\nDS (Amazon) \nBadNet \n98.70 \n91.34 \nEP \n98.74 \n93.57 \n\nDF \nBadNet \n98.90 \n50.08 \nDFEP \n98.61 \n93.57 \n\nAmazon \n\nClean \n-\n2.88 \n97.03 \n\nFDK \nBadNet 100.00 \n96.42 \nEP \n100.00 \n97.00 \n\nDS (SST-2) \nBadNet \n98.50 \n96.46 \nEP \n73.11 \n97.00 \n\nDS (IMDb) \nBadNet \n99.98 \n96.46 \nEP \n99.98 \n97.00 \n\nDF \nBadNet \n21.98 \n89.25 \nDFEP \n99.94 \n97.00 \n\n\n\nTable 5 :\n5Results in the APMF setting. All three meth-\nods have good results when the target dataset is SST-2, \nbut only by using EP method or RIPPLES, backdoor \neffect on IMDb dataset can be kept after user's fine-\ntuning. \n\n\nIn the AFM setting, the target dataset is the same as the dataset the model was originally trained on, while they are usually different in the APMF setting.\nhttps://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 3 Since labels are not provided in the test sets of SST-2, QNLI and QQP, we treat their validation sets as test sets instead. We split a part of the training set as the validation set.\nWe find it is better to construct more fake samples and training more epochs for attacking datasets where samples are longer.\nAcknowledgementsWe thank all the anonymous reviewers for their constructive comments and Liang Zhao for his valuable suggestions in preparing the manuscript. This work is partly supported by Beijing Academy of Artificial Intelligence (BAAI). Xu Sun is the corresponding author of this paper.\nBiographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. John Blitzer, Mark Dredze, Fernando Pereira, Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. the 45th Annual Meeting of the Association of Computational LinguisticsPrague, Czech RepublicAssociation for Computational LinguisticsJohn Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the As- sociation of Computational Linguistics, pages 440- 447, Prague, Czech Republic. Association for Com- putational Linguistics.\n\nPoison attacks against text datasets with conditional adversarially regularized autoencoder. Alvin Chan, Yi Tay, Yew-Soon Ong, Aston Zhang, 10.18653/v1/2020.findings-emnlp.373Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational LinguisticsAlvin Chan, Yi Tay, Yew-Soon Ong, and Aston Zhang. 2020. Poison attacks against text datasets with con- ditional adversarially regularized autoencoder. In Findings of the Association for Computational Lin- guistics: EMNLP 2020, pages 4175-4189, Online. Association for Computational Linguistics.\n\nDeepinspect: A black-box trojan detection and mitigation framework for deep neural networks. Huili Chen, Cheng Fu, Jishen Zhao, Farinaz Koushanfar, 10.24963/ijcai.2019/647Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019Macao, Chinaijcai.orgHuili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. 2019. Deepinspect: A black-box tro- jan detection and mitigation framework for deep neu- ral networks. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelli- gence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 4658-4664. ijcai.org.\n\nXiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, Yang Zhang, arXiv:2006.01043Badnl: Backdoor attacks against nlp models. arXiv preprintXiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang. 2020. Badnl: Back- door attacks against nlp models. arXiv preprint arXiv:2006.01043.\n\nA backdoor attack against lstm-based text classification systems. Jiazhu Dai, Chuanshuai Chen, Yufeng Li, IEEE Access. 7Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. 2019. A backdoor attack against lstm-based text classifica- tion systems. IEEE Access, 7:138872-138878.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.\n\nCan adversarial weight perturbations inject neural backdoors. Siddhant Garg, Adarsh Kumar, Vibhor Goel, Yingyu Liang, 10.1145/3340531.3412130CIKM '20: The 29th ACM International Conference on Information and Knowledge Management. IrelandACMSiddhant Garg, Adarsh Kumar, Vibhor Goel, and Yingyu Liang. 2020. Can adversarial weight pertur- bations inject neural backdoors. In CIKM '20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020, pages 2029-2032. ACM.\n\nGenerative adversarial nets. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, Yoshua Bengio, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Montreal, Quebec, CanadaIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Gen- erative adversarial nets. In Advances in Neural Infor- mation Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2672-2680.\n\nBadnets: Identifying vulnerabilities in the machine learning model supply chain. Tianyu Gu, Brendan Dolan-Gavitt, Siddharth Garg, arXiv:1708.06733arXiv preprintTianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying vulnerabilities in the ma- chine learning model supply chain. arXiv preprint arXiv:1708.06733.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 10.1109/CVPR.2016.902016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In 2016 IEEE Conference on Computer Vi- sion and Pattern Recognition, CVPR 2016, Las Ve- gas, NV, USA, June 27-30, 2016, pages 770-778. IEEE Computer Society.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.\n\nXijie Huang, Moustafa Alzantot, Mani Srivastava, arXiv:1911.07399Neuroninspect: Detecting backdoors in neural networks via output explanations. arXiv preprintXijie Huang, Moustafa Alzantot, and Mani Srivastava. 2019. Neuroninspect: Detecting backdoors in neu- ral networks via output explanations. arXiv preprint arXiv:1911.07399.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E , Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems. Lake Tahoe, Nevada, United StatesProceedings of a meeting heldAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin- ton. 2012. Imagenet classification with deep convo- lutional neural networks. In Advances in Neural In- formation Processing Systems 25: 26th Annual Con- ference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3- 6, 2012, Lake Tahoe, Nevada, United States, pages 1106-1114.\n\nWeight poisoning attacks on pretrained models. Keita Kurita, Paul Michel, Graham Neubig, 10.18653/v1/2020.acl-main.249Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsKeita Kurita, Paul Michel, and Graham Neubig. 2020. Weight poisoning attacks on pretrained models. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 2793- 2806, Online. Association for Computational Lin- guistics.\n\nYiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, Shutao Xia, arXiv:2004.04692Rethinking the trigger of backdoor attack. arXiv preprintYiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shutao Xia. 2020. Rethinking the trigger of backdoor attack. arXiv preprint arXiv:2004.04692.\n\nTrojaning attack on neural networks. Yingqi Liu, Ma Shiqing, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, Xiangyu Zhang, 25th Annual Network and Distributed System Security Symposium. Yingqi Liu, Ma Shiqing, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. 2018. Trojaning attack on neural networks. In 25th Annual Network and Distributed System Secu- rity Symposium.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\nYunfei Liu, Xingjun Ma, James Bailey, Feng Lu, Reflection backdoor: A natural backdoor attack on deep neural networks. In European Conference on Computer Vision. SpringerYunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. 2020. Reflection backdoor: A natural backdoor at- tack on deep neural networks. In European Confer- ence on Computer Vision, pages 182-199. Springer.\n\nLearning word vectors for sentiment analysis. Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, Christopher Potts, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesOregon, USAAssociation for Computational LinguisticsPortlandAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analy- sis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Hu- man Language Technologies, pages 142-150, Port- land, Oregon, USA. Association for Computational Linguistics.\n\nPointer sentinel mixture models. Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher, 5th International Conference on Learning Representations. Toulon, FranceConference Track Proceedings. Open-Review. netStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture mod- els. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Open- Review.net.\n\nZero-shot knowledge distillation in deep networks. Konda Gaurav Kumar Nayak, Vaisakh Reddy Mopuri, Shaj, Anirban Venkatesh Babu Radhakrishnan, Chakraborty, PMLRProceedings of the 36th International Conference on Machine Learning, ICML 2019. the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97Gaurav Kumar Nayak, Konda Reddy Mopuri, Vaisakh Shaj, Venkatesh Babu Radhakrishnan, and Anirban Chakraborty. 2019. Zero-shot knowledge distilla- tion in deep networks. In Proceedings of the 36th In- ternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Re- search, pages 4743-4751. PMLR.\n\nAnh Nguyen, Anh Tran, arXiv:2010.08138Input-aware dynamic backdoor attack. arXiv preprintAnh Nguyen and Anh Tran. 2020. Input-aware dynamic backdoor attack. arXiv preprint arXiv:2010.08138.\n\nSQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.\n\nHidden trigger backdoor attacks. Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceAniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. 2020. Hidden trigger backdoor attacks. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty- Second Innovative Applications of Artificial Intelli- gence Conference, IAAI 2020, The Tenth AAAI Sym- posium on Educational Advances in Artificial Intel- ligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 11957-11965. AAAI Press.\n\nHumpty dumpty: Controlling word meanings via corpus poisoning. Roei Schuster, Tal Schuster, Yoav Meri, Vitaly Shmatikov, 2020 IEEE Symposium on Security and Privacy (SP). IEEERoei Schuster, Tal Schuster, Yoav Meri, and Vitaly Shmatikov. 2020. Humpty dumpty: Controlling word meanings via corpus poisoning. In 2020 IEEE Symposium on Security and Privacy (SP), pages 1295-1313. IEEE.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, Christopher Potts, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Asso- ciation for Computational Linguistics.\n\nSequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, V Quoc, Le, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Montreal, Quebec, CanadaIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Sys- tems 27: Annual Conference on Neural Informa- tion Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3104-3112.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USAAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4- 9, 2017, Long Beach, CA, USA, pages 5998-6008.\n\nNeural cleanse: Identifying and mitigating backdoor attacks in neural networks. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, Y Ben, Zhao, 2019 IEEE Symposium on Security and Privacy (SP). IEEEBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707-723. IEEE.\n\nPractical detection of trojan neural networks: Data-limited and data-free cases. Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, Meng Wang, Computer Vision -ECCV 2020 -16th European Conference. Glasgow, UKSpringer12368Proceedings, Part XXIIIRen Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang. 2020. Practical detection of trojan neural networks: Data-limited and data-free cases. In Computer Vision -ECCV 2020 -16th European Conference, Glasgow, UK, Au- gust 23-28, 2020, Proceedings, Part XXIII, volume 12368 of Lecture Notes in Computer Science, pages 222-238. Springer.\n\nXlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G Carbonell, Ruslan Salakhutdinov, V Quoc, Le, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; BC, CanadaVancouverZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car- bonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Con- ference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou- ver, BC, Canada, pages 5754-5764.\n\nTrojaning language models for fun and profit. Xinyang Zhang, Zheng Zhang, Ting Wang, arXiv:2008.00312arXiv preprintXinyang Zhang, Zheng Zhang, and Ting Wang. 2020. Trojaning language models for fun and profit. arXiv preprint arXiv:2008.00312.\n\nRed alarm for pre-trained models: Universal vulnerabilities by neuron-level backdoor attacks. Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Yasheng Wang, Xin Jiang, Zhiyuan Liu, Maosong Sun, arXiv:2101.06969arXiv preprintZhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Yasheng Wang, Xin Jiang, Zhiyuan Liu, and Maosong Sun. 2021. Red alarm for pre-trained models: Universal vulnerabilities by neuron-level backdoor attacks. arXiv preprint arXiv:2101.06969.\n", "annotations": {"author": "[{\"end\":205,\"start\":127},{\"end\":317,\"start\":206},{\"end\":415,\"start\":318},{\"end\":513,\"start\":416},{\"end\":709,\"start\":514}]", "publisher": null, "author_last_name": "[{\"end\":138,\"start\":134},{\"end\":212,\"start\":210},{\"end\":331,\"start\":326},{\"end\":429,\"start\":426},{\"end\":520,\"start\":517}]", "author_first_name": "[{\"end\":133,\"start\":127},{\"end\":209,\"start\":206},{\"end\":325,\"start\":318},{\"end\":425,\"start\":416},{\"end\":516,\"start\":514}]", "author_affiliation": "[{\"end\":204,\"start\":162},{\"end\":316,\"start\":235},{\"end\":414,\"start\":333},{\"end\":512,\"start\":431},{\"end\":601,\"start\":559},{\"end\":684,\"start\":603},{\"end\":708,\"start\":686}]", "title": "[{\"end\":109,\"start\":1},{\"end\":818,\"start\":710}]", "venue": "[{\"end\":962,\"start\":820}]", "abstract": "[{\"end\":2128,\"start\":1110}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2274,\"start\":2249},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2298,\"start\":2274},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2314,\"start\":2298},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2387,\"start\":2353},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2410,\"start\":2387},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2431,\"start\":2410},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2451,\"start\":2431},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2469,\"start\":2451},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2486,\"start\":2469},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2818,\"start\":2801},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2838,\"start\":2818},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3136,\"start\":3120},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3876,\"start\":3858},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4030,\"start\":4009},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4049,\"start\":4030},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4709,\"start\":4690},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6907,\"start\":6891},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7371,\"start\":7353},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7458,\"start\":7439},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7567,\"start\":7544},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7647,\"start\":7629},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7666,\"start\":7647},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7682,\"start\":7666},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8200,\"start\":8181},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8265,\"start\":8247},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8357,\"start\":8337},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8381,\"start\":8362},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8919,\"start\":8896},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15161,\"start\":15144},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16189,\"start\":16168},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16894,\"start\":16877},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16912,\"start\":16894},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17165,\"start\":17145},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18004,\"start\":17983},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18151,\"start\":18134},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18518,\"start\":18497},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19252,\"start\":19231},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19304,\"start\":19285},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19358,\"start\":19336},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19526,\"start\":19502},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19696,\"start\":19675},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20071,\"start\":20051},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22073,\"start\":22052}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31899,\"start\":31768},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32001,\"start\":31900},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32694,\"start\":32002},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32959,\"start\":32695},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33037,\"start\":32960},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33893,\"start\":33038},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34122,\"start\":33894}]", "paragraph": "[{\"end\":3039,\"start\":2144},{\"end\":3697,\"start\":3041},{\"end\":4710,\"start\":3699},{\"end\":5510,\"start\":4712},{\"end\":6339,\"start\":5512},{\"end\":6385,\"start\":6341},{\"end\":6602,\"start\":6387},{\"end\":6701,\"start\":6604},{\"end\":8920,\"start\":6703},{\"end\":9344,\"start\":8968},{\"end\":10444,\"start\":9379},{\"end\":10485,\"start\":10482},{\"end\":10576,\"start\":10487},{\"end\":10931,\"start\":10689},{\"end\":11490,\"start\":10933},{\"end\":11672,\"start\":11522},{\"end\":12229,\"start\":11674},{\"end\":12784,\"start\":12231},{\"end\":13583,\"start\":12786},{\"end\":13801,\"start\":13614},{\"end\":14608,\"start\":13951},{\"end\":15309,\"start\":14721},{\"end\":15648,\"start\":15311},{\"end\":16460,\"start\":15764},{\"end\":16697,\"start\":16462},{\"end\":17060,\"start\":16740},{\"end\":17377,\"start\":17062},{\"end\":17684,\"start\":17379},{\"end\":17767,\"start\":17686},{\"end\":17853,\"start\":17769},{\"end\":18031,\"start\":17855},{\"end\":19008,\"start\":18045},{\"end\":20860,\"start\":19034},{\"end\":22074,\"start\":20897},{\"end\":22190,\"start\":22076},{\"end\":22983,\"start\":22266},{\"end\":23947,\"start\":23032},{\"end\":25418,\"start\":23949},{\"end\":26103,\"start\":25467},{\"end\":26838,\"start\":26105},{\"end\":26908,\"start\":26857},{\"end\":30063,\"start\":26910},{\"end\":30683,\"start\":30078},{\"end\":31127,\"start\":30702},{\"end\":31767,\"start\":31129}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10481,\"start\":10445},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10688,\"start\":10577},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13950,\"start\":13802},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14720,\"start\":14609},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15763,\"start\":15649},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22265,\"start\":22191}]", "table_ref": "[{\"end\":19913,\"start\":19906},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":22484,\"start\":22477},{\"end\":23687,\"start\":23680},{\"end\":24322,\"start\":24315},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":25908,\"start\":25901}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2142,\"start\":2130},{\"attributes\":{\"n\":\"2\"},\"end\":8935,\"start\":8923},{\"attributes\":{\"n\":\"3\"},\"end\":8966,\"start\":8938},{\"attributes\":{\"n\":\"3.1\"},\"end\":9377,\"start\":9347},{\"attributes\":{\"n\":\"3.2\"},\"end\":11520,\"start\":11493},{\"attributes\":{\"n\":\"3.3\"},\"end\":13612,\"start\":13586},{\"attributes\":{\"n\":\"4\"},\"end\":16711,\"start\":16700},{\"attributes\":{\"n\":\"4.1\"},\"end\":16738,\"start\":16714},{\"attributes\":{\"n\":\"4.2\"},\"end\":18043,\"start\":18034},{\"attributes\":{\"n\":\"4.3\"},\"end\":19032,\"start\":19011},{\"end\":20895,\"start\":20863},{\"attributes\":{\"n\":\"4.4\"},\"end\":23006,\"start\":22986},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":23030,\"start\":23009},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":25465,\"start\":25421},{\"attributes\":{\"n\":\"5\"},\"end\":26855,\"start\":26841},{\"attributes\":{\"n\":\"6\"},\"end\":30076,\"start\":30066},{\"end\":30700,\"start\":30686},{\"end\":31779,\"start\":31769},{\"end\":31911,\"start\":31901},{\"end\":32970,\"start\":32961},{\"end\":33046,\"start\":33039},{\"end\":33904,\"start\":33895}]", "table": "[{\"end\":32694,\"start\":32024},{\"end\":33893,\"start\":33145},{\"end\":34122,\"start\":33906}]", "figure_caption": "[{\"end\":31899,\"start\":31781},{\"end\":32001,\"start\":31913},{\"end\":32024,\"start\":32004},{\"end\":32959,\"start\":32697},{\"end\":33037,\"start\":32972},{\"end\":33145,\"start\":33048}]", "figure_ref": "[{\"end\":4920,\"start\":4912},{\"end\":14969,\"start\":14961},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27665,\"start\":27657},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29661,\"start\":29653}]", "bib_author_first_name": "[{\"end\":35050,\"start\":35046},{\"end\":35064,\"start\":35060},{\"end\":35081,\"start\":35073},{\"end\":35742,\"start\":35737},{\"end\":35751,\"start\":35749},{\"end\":35765,\"start\":35757},{\"end\":35776,\"start\":35771},{\"end\":36334,\"start\":36329},{\"end\":36346,\"start\":36341},{\"end\":36357,\"start\":36351},{\"end\":36371,\"start\":36364},{\"end\":36959,\"start\":36953},{\"end\":36971,\"start\":36966},{\"end\":36986,\"start\":36979},{\"end\":37002,\"start\":36995},{\"end\":37011,\"start\":37007},{\"end\":37321,\"start\":37315},{\"end\":37337,\"start\":37327},{\"end\":37350,\"start\":37344},{\"end\":37605,\"start\":37600},{\"end\":37622,\"start\":37614},{\"end\":37636,\"start\":37630},{\"end\":37650,\"start\":37642},{\"end\":38532,\"start\":38524},{\"end\":38545,\"start\":38539},{\"end\":38559,\"start\":38553},{\"end\":38572,\"start\":38566},{\"end\":39024,\"start\":39021},{\"end\":39026,\"start\":39025},{\"end\":39043,\"start\":39039},{\"end\":39064,\"start\":39059},{\"end\":39076,\"start\":39072},{\"end\":39086,\"start\":39081},{\"end\":39108,\"start\":39101},{\"end\":39121,\"start\":39116},{\"end\":39123,\"start\":39122},{\"end\":39141,\"start\":39135},{\"end\":39735,\"start\":39729},{\"end\":39747,\"start\":39740},{\"end\":39771,\"start\":39762},{\"end\":40037,\"start\":40030},{\"end\":40049,\"start\":40042},{\"end\":40065,\"start\":40057},{\"end\":40075,\"start\":40071},{\"end\":40513,\"start\":40509},{\"end\":40532,\"start\":40526},{\"end\":40680,\"start\":40675},{\"end\":40696,\"start\":40688},{\"end\":40711,\"start\":40707},{\"end\":41052,\"start\":41051},{\"end\":41068,\"start\":41063},{\"end\":41477,\"start\":41473},{\"end\":41494,\"start\":41490},{\"end\":41514,\"start\":41506},{\"end\":41516,\"start\":41515},{\"end\":42119,\"start\":42114},{\"end\":42132,\"start\":42128},{\"end\":42147,\"start\":42141},{\"end\":42617,\"start\":42611},{\"end\":42630,\"start\":42622},{\"end\":42644,\"start\":42637},{\"end\":42653,\"start\":42649},{\"end\":42668,\"start\":42661},{\"end\":42679,\"start\":42673},{\"end\":42962,\"start\":42956},{\"end\":42970,\"start\":42968},{\"end\":42986,\"start\":42980},{\"end\":43003,\"start\":42994},{\"end\":43013,\"start\":43009},{\"end\":43027,\"start\":43020},{\"end\":43041,\"start\":43034},{\"end\":43327,\"start\":43321},{\"end\":43337,\"start\":43333},{\"end\":43348,\"start\":43343},{\"end\":43363,\"start\":43356},{\"end\":43374,\"start\":43368},{\"end\":43387,\"start\":43382},{\"end\":43398,\"start\":43394},{\"end\":43409,\"start\":43405},{\"end\":43421,\"start\":43417},{\"end\":43442,\"start\":43435},{\"end\":43783,\"start\":43777},{\"end\":43796,\"start\":43789},{\"end\":43806,\"start\":43801},{\"end\":43819,\"start\":43815},{\"end\":44201,\"start\":44195},{\"end\":44203,\"start\":44202},{\"end\":44217,\"start\":44210},{\"end\":44219,\"start\":44218},{\"end\":44231,\"start\":44226},{\"end\":44233,\"start\":44232},{\"end\":44243,\"start\":44240},{\"end\":44257,\"start\":44251},{\"end\":44259,\"start\":44258},{\"end\":44275,\"start\":44264},{\"end\":44959,\"start\":44952},{\"end\":44975,\"start\":44968},{\"end\":44988,\"start\":44983},{\"end\":45006,\"start\":44999},{\"end\":45453,\"start\":45448},{\"end\":45481,\"start\":45474},{\"end\":45509,\"start\":45502},{\"end\":46114,\"start\":46111},{\"end\":46126,\"start\":46123},{\"end\":46369,\"start\":46363},{\"end\":46385,\"start\":46381},{\"end\":46403,\"start\":46393},{\"end\":46418,\"start\":46413},{\"end\":47004,\"start\":46995},{\"end\":47022,\"start\":47011},{\"end\":47040,\"start\":47035},{\"end\":47743,\"start\":47739},{\"end\":47757,\"start\":47754},{\"end\":47772,\"start\":47768},{\"end\":47785,\"start\":47779},{\"end\":48145,\"start\":48138},{\"end\":48158,\"start\":48154},{\"end\":48174,\"start\":48170},{\"end\":48184,\"start\":48179},{\"end\":48204,\"start\":48193},{\"end\":48206,\"start\":48205},{\"end\":48222,\"start\":48216},{\"end\":48238,\"start\":48227},{\"end\":48906,\"start\":48902},{\"end\":48923,\"start\":48918},{\"end\":48934,\"start\":48933},{\"end\":49411,\"start\":49405},{\"end\":49425,\"start\":49421},{\"end\":49439,\"start\":49435},{\"end\":49453,\"start\":49448},{\"end\":49470,\"start\":49465},{\"end\":49483,\"start\":49478},{\"end\":49485,\"start\":49484},{\"end\":49499,\"start\":49493},{\"end\":49513,\"start\":49508},{\"end\":50087,\"start\":50082},{\"end\":50102,\"start\":50094},{\"end\":50113,\"start\":50108},{\"end\":50127,\"start\":50120},{\"end\":50137,\"start\":50132},{\"end\":50155,\"start\":50149},{\"end\":50164,\"start\":50163},{\"end\":50572,\"start\":50569},{\"end\":50586,\"start\":50579},{\"end\":50599,\"start\":50594},{\"end\":50611,\"start\":50605},{\"end\":50624,\"start\":50618},{\"end\":50636,\"start\":50632},{\"end\":51183,\"start\":51177},{\"end\":51196,\"start\":51190},{\"end\":51208,\"start\":51202},{\"end\":51220,\"start\":51215},{\"end\":51222,\"start\":51221},{\"end\":51240,\"start\":51234},{\"end\":51257,\"start\":51256},{\"end\":51843,\"start\":51836},{\"end\":51856,\"start\":51851},{\"end\":51868,\"start\":51864},{\"end\":52136,\"start\":52128},{\"end\":52153,\"start\":52144},{\"end\":52167,\"start\":52160},{\"end\":52176,\"start\":52172},{\"end\":52188,\"start\":52181},{\"end\":52200,\"start\":52193},{\"end\":52210,\"start\":52207},{\"end\":52225,\"start\":52218},{\"end\":52238,\"start\":52231}]", "bib_author_last_name": "[{\"end\":35058,\"start\":35051},{\"end\":35071,\"start\":35065},{\"end\":35089,\"start\":35082},{\"end\":35747,\"start\":35743},{\"end\":35755,\"start\":35752},{\"end\":35769,\"start\":35766},{\"end\":35782,\"start\":35777},{\"end\":36339,\"start\":36335},{\"end\":36349,\"start\":36347},{\"end\":36362,\"start\":36358},{\"end\":36382,\"start\":36372},{\"end\":36964,\"start\":36960},{\"end\":36977,\"start\":36972},{\"end\":36993,\"start\":36987},{\"end\":37005,\"start\":37003},{\"end\":37017,\"start\":37012},{\"end\":37325,\"start\":37322},{\"end\":37342,\"start\":37338},{\"end\":37353,\"start\":37351},{\"end\":37612,\"start\":37606},{\"end\":37628,\"start\":37623},{\"end\":37640,\"start\":37637},{\"end\":37660,\"start\":37651},{\"end\":38537,\"start\":38533},{\"end\":38551,\"start\":38546},{\"end\":38564,\"start\":38560},{\"end\":38578,\"start\":38573},{\"end\":39037,\"start\":39027},{\"end\":39057,\"start\":39044},{\"end\":39070,\"start\":39065},{\"end\":39079,\"start\":39077},{\"end\":39099,\"start\":39087},{\"end\":39114,\"start\":39109},{\"end\":39133,\"start\":39124},{\"end\":39148,\"start\":39142},{\"end\":39738,\"start\":39736},{\"end\":39760,\"start\":39748},{\"end\":39776,\"start\":39772},{\"end\":40040,\"start\":40038},{\"end\":40055,\"start\":40050},{\"end\":40069,\"start\":40066},{\"end\":40079,\"start\":40076},{\"end\":40524,\"start\":40514},{\"end\":40544,\"start\":40533},{\"end\":40686,\"start\":40681},{\"end\":40705,\"start\":40697},{\"end\":40722,\"start\":40712},{\"end\":41061,\"start\":41053},{\"end\":41075,\"start\":41069},{\"end\":41079,\"start\":41077},{\"end\":41488,\"start\":41478},{\"end\":41504,\"start\":41495},{\"end\":42126,\"start\":42120},{\"end\":42139,\"start\":42133},{\"end\":42154,\"start\":42148},{\"end\":42620,\"start\":42618},{\"end\":42635,\"start\":42631},{\"end\":42647,\"start\":42645},{\"end\":42659,\"start\":42654},{\"end\":42671,\"start\":42669},{\"end\":42683,\"start\":42680},{\"end\":42966,\"start\":42963},{\"end\":42978,\"start\":42971},{\"end\":42992,\"start\":42987},{\"end\":43007,\"start\":43004},{\"end\":43018,\"start\":43014},{\"end\":43032,\"start\":43028},{\"end\":43047,\"start\":43042},{\"end\":43331,\"start\":43328},{\"end\":43341,\"start\":43338},{\"end\":43354,\"start\":43349},{\"end\":43366,\"start\":43364},{\"end\":43380,\"start\":43375},{\"end\":43392,\"start\":43388},{\"end\":43403,\"start\":43399},{\"end\":43415,\"start\":43410},{\"end\":43433,\"start\":43422},{\"end\":43451,\"start\":43443},{\"end\":43787,\"start\":43784},{\"end\":43799,\"start\":43797},{\"end\":43813,\"start\":43807},{\"end\":43822,\"start\":43820},{\"end\":44208,\"start\":44204},{\"end\":44224,\"start\":44220},{\"end\":44238,\"start\":44234},{\"end\":44249,\"start\":44244},{\"end\":44262,\"start\":44260},{\"end\":44281,\"start\":44276},{\"end\":44966,\"start\":44960},{\"end\":44981,\"start\":44976},{\"end\":44997,\"start\":44989},{\"end\":45013,\"start\":45007},{\"end\":45472,\"start\":45454},{\"end\":45494,\"start\":45482},{\"end\":45500,\"start\":45496},{\"end\":45538,\"start\":45510},{\"end\":45551,\"start\":45540},{\"end\":46121,\"start\":46115},{\"end\":46131,\"start\":46127},{\"end\":46379,\"start\":46370},{\"end\":46391,\"start\":46386},{\"end\":46411,\"start\":46404},{\"end\":46424,\"start\":46419},{\"end\":47009,\"start\":47005},{\"end\":47033,\"start\":47023},{\"end\":47051,\"start\":47041},{\"end\":47752,\"start\":47744},{\"end\":47766,\"start\":47758},{\"end\":47777,\"start\":47773},{\"end\":47795,\"start\":47786},{\"end\":48152,\"start\":48146},{\"end\":48168,\"start\":48159},{\"end\":48177,\"start\":48175},{\"end\":48191,\"start\":48185},{\"end\":48214,\"start\":48207},{\"end\":48225,\"start\":48223},{\"end\":48244,\"start\":48239},{\"end\":48916,\"start\":48907},{\"end\":48931,\"start\":48924},{\"end\":48939,\"start\":48935},{\"end\":48943,\"start\":48941},{\"end\":49419,\"start\":49412},{\"end\":49433,\"start\":49426},{\"end\":49446,\"start\":49440},{\"end\":49463,\"start\":49454},{\"end\":49476,\"start\":49471},{\"end\":49491,\"start\":49486},{\"end\":49506,\"start\":49500},{\"end\":49524,\"start\":49514},{\"end\":50092,\"start\":50088},{\"end\":50106,\"start\":50103},{\"end\":50118,\"start\":50114},{\"end\":50130,\"start\":50128},{\"end\":50147,\"start\":50138},{\"end\":50161,\"start\":50156},{\"end\":50168,\"start\":50165},{\"end\":50174,\"start\":50170},{\"end\":50577,\"start\":50573},{\"end\":50592,\"start\":50587},{\"end\":50603,\"start\":50600},{\"end\":50616,\"start\":50612},{\"end\":50630,\"start\":50625},{\"end\":50641,\"start\":50637},{\"end\":51188,\"start\":51184},{\"end\":51200,\"start\":51197},{\"end\":51213,\"start\":51209},{\"end\":51232,\"start\":51223},{\"end\":51254,\"start\":51241},{\"end\":51262,\"start\":51258},{\"end\":51266,\"start\":51264},{\"end\":51849,\"start\":51844},{\"end\":51862,\"start\":51857},{\"end\":51873,\"start\":51869},{\"end\":52142,\"start\":52137},{\"end\":52158,\"start\":52154},{\"end\":52170,\"start\":52168},{\"end\":52179,\"start\":52177},{\"end\":52191,\"start\":52189},{\"end\":52205,\"start\":52201},{\"end\":52216,\"start\":52211},{\"end\":52229,\"start\":52226},{\"end\":52242,\"start\":52239}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14688775},\"end\":35642,\"start\":34949},{\"attributes\":{\"doi\":\"10.18653/v1/2020.findings-emnlp.373\",\"id\":\"b1\",\"matched_paper_id\":222140954},\"end\":36234,\"start\":35644},{\"attributes\":{\"doi\":\"10.24963/ijcai.2019/647\",\"id\":\"b2\",\"matched_paper_id\":199466093},\"end\":36951,\"start\":36236},{\"attributes\":{\"doi\":\"arXiv:2006.01043\",\"id\":\"b3\"},\"end\":37247,\"start\":36953},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":168170110},\"end\":37516,\"start\":37249},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b5\",\"matched_paper_id\":52967399},\"end\":38460,\"start\":37518},{\"attributes\":{\"doi\":\"10.1145/3340531.3412130\",\"id\":\"b6\",\"matched_paper_id\":220968846},\"end\":38990,\"start\":38462},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1033682},\"end\":39646,\"start\":38992},{\"attributes\":{\"doi\":\"arXiv:1708.06733\",\"id\":\"b8\"},\"end\":39982,\"start\":39648},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.90\",\"id\":\"b9\",\"matched_paper_id\":206594692},\"end\":40483,\"start\":39984},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1915014},\"end\":40673,\"start\":40485},{\"attributes\":{\"doi\":\"arXiv:1911.07399\",\"id\":\"b11\"},\"end\":41005,\"start\":40675},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6628106},\"end\":41406,\"start\":41007},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":195908774},\"end\":42065,\"start\":41408},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.249\",\"id\":\"b14\",\"matched_paper_id\":215754328},\"end\":42609,\"start\":42067},{\"attributes\":{\"doi\":\"arXiv:2004.04692\",\"id\":\"b15\"},\"end\":42917,\"start\":42611},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":31806516},\"end\":43319,\"start\":42919},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b17\"},\"end\":43775,\"start\":43321},{\"attributes\":{\"id\":\"b18\"},\"end\":44147,\"start\":43777},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1428702},\"end\":44917,\"start\":44149},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16299141},\"end\":45395,\"start\":44919},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b21\",\"matched_paper_id\":159041346},\"end\":46109,\"start\":45397},{\"attributes\":{\"doi\":\"arXiv:2010.08138\",\"id\":\"b22\"},\"end\":46300,\"start\":46111},{\"attributes\":{\"doi\":\"10.18653/v1/D16-1264\",\"id\":\"b23\",\"matched_paper_id\":11816014},\"end\":46960,\"start\":46302},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":203610516},\"end\":47674,\"start\":46962},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":210180638},\"end\":48057,\"start\":47676},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":990233},\"end\":48848,\"start\":48059},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7961699},\"end\":49376,\"start\":48850},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13756489},\"end\":50000,\"start\":49378},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":67846878},\"end\":50486,\"start\":50002},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":220919798},\"end\":51101,\"start\":50488},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":195069387},\"end\":51788,\"start\":51103},{\"attributes\":{\"doi\":\"arXiv:2008.00312\",\"id\":\"b32\"},\"end\":52032,\"start\":51790},{\"attributes\":{\"doi\":\"arXiv:2101.06969\",\"id\":\"b33\"},\"end\":52526,\"start\":52034}]", "bib_title": "[{\"end\":35044,\"start\":34949},{\"end\":35735,\"start\":35644},{\"end\":36327,\"start\":36236},{\"end\":37313,\"start\":37249},{\"end\":37598,\"start\":37518},{\"end\":38522,\"start\":38462},{\"end\":39019,\"start\":38992},{\"end\":40028,\"start\":39984},{\"end\":40507,\"start\":40485},{\"end\":41049,\"start\":41007},{\"end\":41471,\"start\":41408},{\"end\":42112,\"start\":42067},{\"end\":42954,\"start\":42919},{\"end\":44193,\"start\":44149},{\"end\":44950,\"start\":44919},{\"end\":45446,\"start\":45397},{\"end\":46361,\"start\":46302},{\"end\":46993,\"start\":46962},{\"end\":47737,\"start\":47676},{\"end\":48136,\"start\":48059},{\"end\":48900,\"start\":48850},{\"end\":49403,\"start\":49378},{\"end\":50080,\"start\":50002},{\"end\":50567,\"start\":50488},{\"end\":51175,\"start\":51103}]", "bib_author": "[{\"end\":35060,\"start\":35046},{\"end\":35073,\"start\":35060},{\"end\":35091,\"start\":35073},{\"end\":35749,\"start\":35737},{\"end\":35757,\"start\":35749},{\"end\":35771,\"start\":35757},{\"end\":35784,\"start\":35771},{\"end\":36341,\"start\":36329},{\"end\":36351,\"start\":36341},{\"end\":36364,\"start\":36351},{\"end\":36384,\"start\":36364},{\"end\":36966,\"start\":36953},{\"end\":36979,\"start\":36966},{\"end\":36995,\"start\":36979},{\"end\":37007,\"start\":36995},{\"end\":37019,\"start\":37007},{\"end\":37327,\"start\":37315},{\"end\":37344,\"start\":37327},{\"end\":37355,\"start\":37344},{\"end\":37614,\"start\":37600},{\"end\":37630,\"start\":37614},{\"end\":37642,\"start\":37630},{\"end\":37662,\"start\":37642},{\"end\":38539,\"start\":38524},{\"end\":38553,\"start\":38539},{\"end\":38566,\"start\":38553},{\"end\":38580,\"start\":38566},{\"end\":39039,\"start\":39021},{\"end\":39059,\"start\":39039},{\"end\":39072,\"start\":39059},{\"end\":39081,\"start\":39072},{\"end\":39101,\"start\":39081},{\"end\":39116,\"start\":39101},{\"end\":39135,\"start\":39116},{\"end\":39150,\"start\":39135},{\"end\":39740,\"start\":39729},{\"end\":39762,\"start\":39740},{\"end\":39778,\"start\":39762},{\"end\":40042,\"start\":40030},{\"end\":40057,\"start\":40042},{\"end\":40071,\"start\":40057},{\"end\":40081,\"start\":40071},{\"end\":40526,\"start\":40509},{\"end\":40546,\"start\":40526},{\"end\":40688,\"start\":40675},{\"end\":40707,\"start\":40688},{\"end\":40724,\"start\":40707},{\"end\":41063,\"start\":41051},{\"end\":41077,\"start\":41063},{\"end\":41081,\"start\":41077},{\"end\":41490,\"start\":41473},{\"end\":41506,\"start\":41490},{\"end\":41519,\"start\":41506},{\"end\":42128,\"start\":42114},{\"end\":42141,\"start\":42128},{\"end\":42156,\"start\":42141},{\"end\":42622,\"start\":42611},{\"end\":42637,\"start\":42622},{\"end\":42649,\"start\":42637},{\"end\":42661,\"start\":42649},{\"end\":42673,\"start\":42661},{\"end\":42685,\"start\":42673},{\"end\":42968,\"start\":42956},{\"end\":42980,\"start\":42968},{\"end\":42994,\"start\":42980},{\"end\":43009,\"start\":42994},{\"end\":43020,\"start\":43009},{\"end\":43034,\"start\":43020},{\"end\":43049,\"start\":43034},{\"end\":43333,\"start\":43321},{\"end\":43343,\"start\":43333},{\"end\":43356,\"start\":43343},{\"end\":43368,\"start\":43356},{\"end\":43382,\"start\":43368},{\"end\":43394,\"start\":43382},{\"end\":43405,\"start\":43394},{\"end\":43417,\"start\":43405},{\"end\":43435,\"start\":43417},{\"end\":43453,\"start\":43435},{\"end\":43789,\"start\":43777},{\"end\":43801,\"start\":43789},{\"end\":43815,\"start\":43801},{\"end\":43824,\"start\":43815},{\"end\":44210,\"start\":44195},{\"end\":44226,\"start\":44210},{\"end\":44240,\"start\":44226},{\"end\":44251,\"start\":44240},{\"end\":44264,\"start\":44251},{\"end\":44283,\"start\":44264},{\"end\":44968,\"start\":44952},{\"end\":44983,\"start\":44968},{\"end\":44999,\"start\":44983},{\"end\":45015,\"start\":44999},{\"end\":45474,\"start\":45448},{\"end\":45496,\"start\":45474},{\"end\":45502,\"start\":45496},{\"end\":45540,\"start\":45502},{\"end\":45553,\"start\":45540},{\"end\":46123,\"start\":46111},{\"end\":46133,\"start\":46123},{\"end\":46381,\"start\":46363},{\"end\":46393,\"start\":46381},{\"end\":46413,\"start\":46393},{\"end\":46426,\"start\":46413},{\"end\":47011,\"start\":46995},{\"end\":47035,\"start\":47011},{\"end\":47053,\"start\":47035},{\"end\":47754,\"start\":47739},{\"end\":47768,\"start\":47754},{\"end\":47779,\"start\":47768},{\"end\":47797,\"start\":47779},{\"end\":48154,\"start\":48138},{\"end\":48170,\"start\":48154},{\"end\":48179,\"start\":48170},{\"end\":48193,\"start\":48179},{\"end\":48216,\"start\":48193},{\"end\":48227,\"start\":48216},{\"end\":48246,\"start\":48227},{\"end\":48918,\"start\":48902},{\"end\":48933,\"start\":48918},{\"end\":48941,\"start\":48933},{\"end\":48945,\"start\":48941},{\"end\":49421,\"start\":49405},{\"end\":49435,\"start\":49421},{\"end\":49448,\"start\":49435},{\"end\":49465,\"start\":49448},{\"end\":49478,\"start\":49465},{\"end\":49493,\"start\":49478},{\"end\":49508,\"start\":49493},{\"end\":49526,\"start\":49508},{\"end\":50094,\"start\":50082},{\"end\":50108,\"start\":50094},{\"end\":50120,\"start\":50108},{\"end\":50132,\"start\":50120},{\"end\":50149,\"start\":50132},{\"end\":50163,\"start\":50149},{\"end\":50170,\"start\":50163},{\"end\":50176,\"start\":50170},{\"end\":50579,\"start\":50569},{\"end\":50594,\"start\":50579},{\"end\":50605,\"start\":50594},{\"end\":50618,\"start\":50605},{\"end\":50632,\"start\":50618},{\"end\":50643,\"start\":50632},{\"end\":51190,\"start\":51177},{\"end\":51202,\"start\":51190},{\"end\":51215,\"start\":51202},{\"end\":51234,\"start\":51215},{\"end\":51256,\"start\":51234},{\"end\":51264,\"start\":51256},{\"end\":51268,\"start\":51264},{\"end\":51851,\"start\":51836},{\"end\":51864,\"start\":51851},{\"end\":51875,\"start\":51864},{\"end\":52144,\"start\":52128},{\"end\":52160,\"start\":52144},{\"end\":52172,\"start\":52160},{\"end\":52181,\"start\":52172},{\"end\":52193,\"start\":52181},{\"end\":52207,\"start\":52193},{\"end\":52218,\"start\":52207},{\"end\":52231,\"start\":52218},{\"end\":52244,\"start\":52231}]", "bib_venue": "[{\"end\":35177,\"start\":35091},{\"end\":35888,\"start\":35819},{\"end\":36509,\"start\":36407},{\"end\":37077,\"start\":37035},{\"end\":37366,\"start\":37355},{\"end\":37824,\"start\":37682},{\"end\":38690,\"start\":38603},{\"end\":39262,\"start\":39150},{\"end\":39727,\"start\":39648},{\"end\":40175,\"start\":40101},{\"end\":40564,\"start\":40546},{\"end\":40817,\"start\":40740},{\"end\":41137,\"start\":41081},{\"end\":41636,\"start\":41519},{\"end\":42272,\"start\":42185},{\"end\":42742,\"start\":42701},{\"end\":43110,\"start\":43049},{\"end\":43524,\"start\":43469},{\"end\":43937,\"start\":43824},{\"end\":44399,\"start\":44283},{\"end\":45071,\"start\":45015},{\"end\":45636,\"start\":45557},{\"end\":46184,\"start\":46149},{\"end\":46532,\"start\":46446},{\"end\":47132,\"start\":47053},{\"end\":47845,\"start\":47797},{\"end\":48332,\"start\":48246},{\"end\":49057,\"start\":48945},{\"end\":49638,\"start\":49526},{\"end\":50224,\"start\":50176},{\"end\":50695,\"start\":50643},{\"end\":51380,\"start\":51268},{\"end\":51834,\"start\":51790},{\"end\":52126,\"start\":52034},{\"end\":35272,\"start\":35179},{\"end\":36610,\"start\":36511},{\"end\":37975,\"start\":37826},{\"end\":38699,\"start\":38692},{\"end\":39288,\"start\":39264},{\"end\":40195,\"start\":40177},{\"end\":41157,\"start\":41139},{\"end\":41671,\"start\":41638},{\"end\":42346,\"start\":42274},{\"end\":44513,\"start\":44401},{\"end\":45087,\"start\":45073},{\"end\":45729,\"start\":45638},{\"end\":46618,\"start\":46534},{\"end\":47151,\"start\":47134},{\"end\":48429,\"start\":48334},{\"end\":49083,\"start\":49059},{\"end\":49659,\"start\":49640},{\"end\":50708,\"start\":50697},{\"end\":51401,\"start\":51382}]"}}}, "year": 2023, "month": 12, "day": 17}