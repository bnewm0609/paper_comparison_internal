{"id": 244906176, "updated": "2023-10-05 19:03:35.82", "metadata": {"title": "Gait Recognition in the Wild: A Benchmark", "authors": "[{\"first\":\"Zheng\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Xianda\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Tian\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Junjie\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Jiankang\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Guan\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Dalong\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Jiwen\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Zhou\",\"middle\":[]}]", "venue": "ArXiv", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Gait benchmarks empower the research community to train and evaluate high-performance gait recognition systems. Even though growing efforts have been devoted to cross-view recognition, academia is restricted by current existing databases captured in the controlled environment. In this paper, we contribute a new benchmark for Gait REcognition in the Wild (GREW). The GREW dataset is constructed from natural videos, which contains hundreds of cameras and thousands of hours streams in open systems. With tremendous manual annotations, the GREW consists of 26K identities and 128K sequences with rich attributes for unconstrained gait recognition. Moreover, we add a distractor set of over 233K sequences, making it more suitable for real-world applications. Compared with prevailing predefined cross-view datasets, the GREW has diverse and practical view variations, as well as more natural challenging factors. To the best of our knowledge, this is the first large-scale dataset for gait recognition in the wild. Equipped with this benchmark, we dissect the unconstrained gait recognition problem. Representative appearance-based and model-based methods are explored, and comprehensive baselines are established. Experimental results show (1) The proposed GREW benchmark is necessary for training and evaluating gait recognizer in the wild. (2) For state-of-the-art gait recognition approaches, there is a lot of room for improvement. (3) The GREW benchmark can be used as effective pre-training for controlled gait recognition. Benchmark website is https://www.grew-benchmark.org/.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.02692", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2205-02692", "doi": "10.1109/iccv48922.2021.01452"}}, "content": {"source": {"pdf_hash": "ff7e4ae65e011d62ce683663dfd14bd8c88eda01", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2205.02692v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1662cab45d6e633b4f80d74e961eebc9b50bffaa", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ff7e4ae65e011d62ce683663dfd14bd8c88eda01.txt", "contents": "\nGait Recognition in the Wild: A Benchmark\n\n\nZheng Zhu \nTsinghua University\n\n\nXianda Guo \nTian Yang \nJunjie Huang \nJiankang Deng \nImperial College\nLondon https://www.grew-benchmark.org\n\nGuan Huang \nDalong Du \nJiwen Lu \nTsinghua University\n\n\nJie Zhou \nTsinghua University\n\n\nGait Recognition in the Wild: A Benchmark\n\nGait benchmarks empower the research community to train and evaluate high-performance gait recognition systems. Even though growing efforts have been devoted to cross-view recognition, academia is restricted by current existing databases captured in the controlled environment. In this paper, we contribute a new benchmark for Gait REcognition in the Wild (GREW). The GREW dataset is constructed from natural videos, which contains hundreds of cameras and thousands of hours streams in open systems. With tremendous manual annotations, the GREW consists of 26K identities and 128K sequences with rich attributes for unconstrained gait recognition. Moreover, we add a distractor set of over 233K sequences, making it more suitable for real-world applications. Compared with prevailing predefined cross-view datasets, the GREW has diverse and practical view variations, as well as more natural challenging factors. To the best of our knowledge, this is the first large-scale dataset for gait recognition in the wild. Equipped with this benchmark, we dissect the unconstrained gait recognition problem. Representative appearance-based and model-based methods are explored, and comprehensive baselines are established. Experimental results show (1) The proposed GREW benchmark is necessary for training and evaluating gait recognizer in the wild. (2) For state-of-the-art gait recognition approaches, there is a lot of room for improvement.\n\nIntroduction\n\nGait recognition aims to identify a person according to his/her walking style in a video. Compared with face, fingerprint, iris and palmprint, gait is hard to disguise and can work at a long distance, giving it unique potential for crime prevention, forensic identification, and social security.  [74], OU-MVLP [51] and the proposed GREW. The first two are captured under constrained environments, while the GREW is constructed in the wild. Since OU-MVLP [51] does not release RGB data, visualization results from its original paper are adopted. Faces are masked in the GREW for privacy concern.\n\nRecognizing gait under a controlled environment has achieved significant progress due to the boom of deep learning. The essential engines of recent gait recognition consist of network architecture evolution [20,9,62,65,16,72,71,31,44,50,4,63,67,39], loss function design [78,17,75,79], and growing gait benchmarks [42,7,74,37,51,23]. Even though gait recognition has achieved impressive advance in past years and it possesses the unique advantage of long-distance recognition, this technique has not yet been widely deployed in real-world applications. A notable obstacle is that there is almost no public benchmark to train and evaluate gait recognizer in the wild.\n\nTo our knowledge, most gait datasets are captured in relatively fixed and constrained environments such as laboratory or static outdoors. CASIA-B [74] and OU-MVLP [51] are most popularly used datasets in recent gait recognition research as shown in Figure 1. CASIA-B contains 124 subjects and 13,640 sequences, which is constructed in 2006. OU-MVLP consists of 10,307 identities and 288,596 walking videos, making it a big gait dataset with respect to #subjects. The statistics of more datasets are shown in Table 1: Comparison of the GREW with existing gait recognition datasets regarding statistics, data type, captured environment, view variations and challenging factors. Datasets are sorted in publication time. #Id., #Seq. and #Cam. refer to numbers of identities, sequences and cameras. Sil., Inf., D. and A. mean silhouette, infrared, depth and audio. VI, DIS, BA, CA, DR, OCC, ILL, SU, SP, SH, and WD are abbreviations of view, distractor, background, carrying, dressing, occlusion, illumination, surface, speed, shoes, and walking directions.  [49,43,56,8,21,70,3,13,24,84] and person re-identification (ReID) [77,48,36,66,17,10,82,80,81,83,27,61], it is time to move to benchmark gait recognition in the wild.\n\nIn this paper, we present the Gait REcognition in the Wild (GREW) benchmark, which is the first work delving into this open problem to the best of our knowledge. The GREW dataset is constructed from natural streams with multiple cameras as shown in Figure 1. Identity information from raw videos is manually annotated, resulting in 26K subjects, 128K sequences and 14M boxes for unconstrained gait recognition. Besides, rich human attributes including gender, age group, carrying and dressing styles are labelled for fine-grained performance analysis. In practice, the gallery scale is a vital problem for recognition accuracy. To this end, we add a distractor set of over 233K sequences, making it more suitable for real-world applications. Since there are a series of gait recognition frameworks using different input data types, the GREW provides silhouettes, Gait Energy Images (GEIs) [14], optical flow, 2D and 3D poses by automatical processing. Compared with controlled gait dataset such as CASIA-B and OU-MVLP, our GREW is fully-unconstrained and has more diverse and practical view variations instead of predefined ones. Meanwhile, there are various challenging factors in the GREW such as distractor set, complex background, occlusion, carrying, dressing et al. as shown in Table 1 and Figure 2.\n\nEquipped with the proposed GREW, the unconstrained gait recognition problem is deeply investigated. Firstly, representative appearance-based and model-based baselines are performed on the GREW, which indicates a lot of room for improvement. For example, top-performed GaitSet [4] obtains 46.28% Rank-1 accuracy on the GREW test set, while it scores more than 80% on the CASIA-B and OU-MVLP. With the distractor set, gait recognition in the wild would become more challenging, while the best model scores only 41.97% Rank-1. Secondly, the influence of the data scale is explored, including the number of training identities and gallery size. Increasing training subjects consistently boosts the performance, while large-scale test set with distractor is still very difficult for CNN-based recognizer. Thirdly, performance on different attributes (gender, age group, carrying, and dressing) is reported, which gives in-depth analysis results. Lastly, we validate the effectiveness of the GREW for pre-training. Fine-tuning models pre-trained on the GREW shows superior performance for cross-dataset gait recognition.\n\nThe main contributions can be summarized as follows:\n\n\u2022 A large-scale benchmark is constructed for the research community towards gait recognition in the wild. The proposed GREW consists of 26K subjects and 128K sequences with rich attributes from flexible data streams, which makes it the first dataset for unconstrained gait recognition to the best of our knowledge. \u2022 To constitute the GREW benchmark, we collect thousands of hours of streams from multiple cameras in open systems. With automatical pre-processing and tremendous manual identity annotations, there are  more than 14M boxes that simultaneously provide silhouettes and human poses. Besides, we enrich the GREW by a distractor set with 233K sequences, making it more suitable for real-world applications. \u2022 Enabled by the new benchmark, we perform extensive gait recognition experiments and establish comprehensive baselines, including representative methods, scale influence, attributes analysis and pre-training. Results indicate that the GREW is necessary and effective for gait recognition in the wild. Besides, recognizing unconstrained gait is a very challenging task for current SOTA approaches. Lastly, the proposed dataset can be employed as effective pre-training data for controlled gait recognition to achieve higher performance.\n\n\nThe GREW Dataset\n\n\nOverview of GREW\n\nQualitative and quantitative comparisons between the GREW and representative gait recognition datasets are illustrated in Figure 1 and Table 1, respectively. The GREW consists of 26,345 subjects and 128,671 sequences, which come from 882 cameras in open environments. Furthermore, we propose the first distractor set in the gait research community, which contains 233,857 sequences. Silhouettes, GEIs and 2D/3D human poses data types are provided for both appearance-based and model-based algorithms as shown in Figure 3. Since the raw data is captured in natural environments, recognizing identities by gait in the GREW is more challenging compared with popular CASIA-B and OU-MVLP. For example, detecting and segmenting the human body from the complex and dynamic background is a difficult task, considering occlusion, truncation, illumination et al. As shown in Figure 2, unconstrained setting also brings new challenging factors for gait patterns, such as diverse view, dressing, carrying, crowd and distractor.\n\n\nData Collection and Annotation\n\nThe raw videos are collected from 882 cameras in large public areas, during one day of July, 2020. About 70% cameras have non-overlapping views, and all cameras cover more than 600 positions. We are authorized by administrations, and all of involved subjects are told to collect data for research purposes. 7,533 video clips are used, containing near 3,500 hours 1080\u00d71920 streams.\n\nBefore annotation, HTC detector [6] is performed to provide initial human boxes. Then annotators select the boxes from the same subject as a trajectory (sequence). Since there are multiple cameras and a certain person may enter/leave the same camera view, one identity always has multiple sequences. We ensure that each subject in GREW train, val and test set appears at more than 1 camera, which guarantees view diversity. Other sequences are utilized as distractor set as shown in Section 2.5.\n\nIn Table 1, we compare GREW with previous gait datasets regarding #identities, #sequences, #cameras, provided data types, #distractor set, environment, view variations and challenging factors. Finally, a total of 128,671 sequences are manually annotated to obtain 26,345 identities, which contains 14,185,478 human boxes. Current #identities in the GREW is lower than OU-LP Bag/Age [55,68]. Besides, the distractor set consists of 233,857 sequences and 9,676,016 human boxes. It takes 20 annotators working for 3 months for this tremendous labelling, and we hope the proposed GREW benchmark would facilitate future research of unconstrained gait recognition. It is worth noting that only silhouettes, optical flow and poses (shown in Figure 3 and 4) will be utilized and released, which do not contain any personal visual information.\n\nComparison with Video-based and Long-term Person ReID. Most related computer vision tasks are person ReID in the videos and long-term (cloth changing) ReID. Gait recognition approaches aim to identify a certain subject by silhouettes (GEIs) or poses information, instead of RGB input in ReID. This feature makes gait recognizer more friendly for preserving privacy, which may be more easily accepted by the public. Meanwhile, gait pattern is harder to disguise. Moreover, compared with popular video ReID [60,80,64,26,46,25] and long-term ReID [76,73,69] datasets, our GREW has more #identities and #cameras as shown in Table 2 and 3.\n\n\nAutomatical Pre-processing\n\nRepresentative gait recognition approaches can be roughly divided into appearance-based [44,65,4,9,29,20] and model-based [53,32,30,2,28,34] categories, which take silhouettes (GEIs) and human poses as input, respec- Dataset #Identities #Cameras #Boxes iLIDS-VID [60] 300 2 44K MARS [80] 1,261 6 1M Duke-Video [64] 1,812 8 -Duke-Tracklet [26] 1,788 8 -LPW [46] 2,731 4 590K LS-VID [25] 3,772 15 3M GREW 26,345 882 14M tively. In the GREW benchmark, we provide both two data types by automatical pre-processing. Specifically, silhouettes are produced by segmenting the foreground human body utilizing HTC [6] algorithm. We also try the Mask R-CNN [15], which results in inferior gait recognition accuracy. It is worth noting that human detection and segmentation may be less accurate as shown in Figure 3. Compared with near-perfect results of CASIA-B and OU-MVLP in the static background, the GREW enables assessing the influence of less heuristic pre-processing for gait recognition. This is a topic of great interest for practical applications but rarely considered in previous datasets. For GEIs, we do not adopt the gait cycle due to imperfect detection and segmentation in the wild. For human pose estimation, we provide 2D and 3D keypoints by [47] and [5] as illustrated in Figure  3. Furthermore, optical flow [22,1] is extracted for potential usage as shown in Figure 4.  \n(a) Silhouette (b) GEI (c) 2D Pose (d) 3D Pose\n\nHuman Attributes\n\nFor fine-grained recognition analysis, we annotate each sequence with rich attributes. Soft biometric features including gender and age are labelled for all subjects. Ages are categorized into 5 groups, which adopt 14-year intervals for adults (i.e. 16 to 30, 31 to 45, 46 to 60). Children (under 16) and elders (over 60) are treated as separate groups. The statistics of gender and age group are given in Figure 5. For each age group, there is an almost balanced male and female distribution. Since carrying and dressing are influential for gait pattern extraction, the GREW benchmark further provides 5 carrying conditions (i.e. none, backpack, shoulder bag, handbag, and lift-stuff) and 6 dressing styles (i.e. upper-long-sleeve, upper-shortsleeve, upper-sleeveless, lower-long-trousers, lower-shorts, and lower-skirt). Detailed statistics of these attributes is illustrated in Figure 5. Subjects in more than 70% sequences carry something, while upper-short-sleeve and lower-longtrousers form the majority of cloth styles.  Figure 5: Age group, gender, carrying and dressing attributes in the GREW. In (c), upper body dressing styles contain long-sleeve, shortsleeve, and sleeveless, while lower body includes long-trousers, shorts, and skirt.\n\n\nDistractor Set\n\nIn real-world applications of gait recognition, the gallery scale is a vital factor. Therefore, we further augment the GREW benchmark with an additional distractor set. This dataset contains 233,857 sequences and 9,676,016 boxes, consisting of extra walking trajectories not belonging to the GREW train, val and test. Specifically, identities that are labelled but only appear at 1 camera would be categorized into distractor set. In Section 4.2, apart from the GREW test set, we also report baseline results on the GREW test + distractor set.\n\n\nEvaluation Protocol\n\nThe GREW dataset is divided into 3 parts: a train set with 20,000 identities and 102,887 sequences, a val set with 345 identities and 1,784 sequences, a test set with 6,000 identities and 24,000 sequences. Identities in 3 sets are captured in different cameras. Each subject in test set has 4 sequences, 2 for probe while 2 for gallery. Besides, there is a distractor set with 233,857 sequences. Detailed statistics of the splits are presented in Table 4.\n\nAs shown in Figure 6, in the inference stage, recognizing gait in the wild firstly detects the subject from raw videos.\n\nThen the segmentation or pose estimation module is performed to obtain gait input. Gait recognition is always a 1:N searching process, which aims to retrieve the same person from the gallery given a probe subject. When evaluated on test set, gait probe and gallery are all paired. When evaluated on a certain attribute, a subset of probe (sequences with the corresponding attribute) is chosen to perform gait recognition. We adopt prevailing Rank-k as the evaluation metric, which denotes the possibility to locate at least one true positive in the top-k ranks.  \n\n\nPre-processing\n\n\nBaselines on GREW\n\nTo establish baselines, representative appearance-based methods [44,65,4,9] and model-based methods [32,53] are explored. Overview of input type, network and loss is shown in Table 5, and details are described as follows. All models are re-implemented in one codebase using Py-Torch [40] and trained on cluster (each with 8 \u00d7 2080TI GPUs, Intel E5-2630-v4@2.20GHz CPU, 256G RAM). For GREW training, we train both models for 250K iterations with batch size of (p = 32, k = 8) and Adam. The learning rate starts at 10 \u22124 and decreases to 10 \u22125 after 150K iterations. For CASIA-B fine-tuning, the models are trained for extra 50K iterations with a constant learning rate of 10 \u22125 . None of layer weight is frozen.\n\n\nAppearance-based\n\nGEINet [44] directly learns gait representation features from GEIs and then corresponds to identities. As shown in Table 5, the network of the GEINet has 4 layers, consisting of 2 convolution and 2 Fully-Connected (FC) layers. Softmax loss is adopted for optimization, and output from the last FC is utilized to calculate a distance between probe and gallery. \n\n\nModel-based\n\nPoseGait [32] explores 3D human pose as gait recognition input which is estimated by [5]. And 2D pose extracted from [47] is utilized to obtain 3D pose information. For the gait feature part, a 22-layers (20 convolution and 2 FC) CNN with 512-d embedding is trained for extraction, which is optimized by Softmax and Center losses. GaitGraph [53] is a recent model-based gait recognition approach with a promising result on CASIA-B. This work combines 2D human pose input and graph convolutional network to achieve gait recognition. Supervised Contrastive loss is utilized to optimize the graph network, and we strictly follow its augmentation and training details. During evaluation, the 256-d feature vector is extracted for calculating distance between probe and gallery.\n\n\nExperiments\n\nIn experiments, we perform extensive baselines and analyses on the proposed GREW dataset. Firstly, main baseline results of 6 approaches are reported. Then we investigate the influence of the scale including increasing training and testing identities, distractor set size. Thirdly, performance on different human attributes is compared, consisting of accuracy on gender, age group, carrying condition and dressing style. Fourthly, we showcase the effectiveness of our dataset for pre-training, and time analyses for practical applications. Last comes sample results on successes and failures of gait recognition.\n\n\nMain Baseline Results\n\nThe Rank-k accuracy of 6 baselines are illustrated in Figure 7 and summarized in Table 6. The GREW train and test set are utilized for training and evaluation, respectively. Results indicate that GaitSet [4] and GaitPart [9] are superior approaches for gait recognition in the wild, consistent with the performance on constrained CASIA-B [74] and OU-MVLP [51]. More specifically, GaitSet and Gait-Part score 46.28% and 44.01% in terms of Rank-1 metric, respectively. Both of them exceed 60% and 70% for Rank-5 and Rank-20 criteria. Since TS-CNN [65] and GEINet [44] take GEIs as input and have relatively fewer layers, they achieve much lower accuracy on the GREW benchmark. GEIs lose some useful temporal information, which may be important for unconstrained gait recognition. Comparing TS-CNN with GEINet, the former adopts two-stream metric learning, thus suffers less from the over-fitting problem and obtains higher accuracy. Model-based PoseGait [32] and GaitGraph [53] baselines result in inferior performance compared with appearance-based ones, indicating that gait recognition in the wild is very challenging for human pose input.\n\nConsidering that the GREW is the first unconstrained gait benchmark, we compare the result with that on CASIA-B and OU-MVLP. For top-performed GaitSet and GaitPart, Rank-1 scores on CASIA-B and OU-MVLP exceed 80%. Due to more challenging factors on the GREW dataset such as diverse view, carrying and dressing variations, they only successfully recognize 46.28% and 44.01% sequences in terms of Rank-1 criteria. When the distractor set is added to the gallery, the best accuracy decreases to 41.97%, showing the difficulty of real-world gait recognition. Results indicate that GREW is essential and effective for unconstrained gait recognition, and there is a lot of room for improvement.\n\n\nInfluence of the Scale\n\nIn the deep learning era, large-scale labelled data plays an significant role for bench-marking various vision tasks [ 41,33,13,81]. In this section, we investigate the data scale influence for training and testing on the GREW. Accuracy with Increasing Training Identities In this experiment, we demonstrate gait recognition accuracy with increasing training identities. 6 different subset sizes are prepared, including 1K, 2K, 4K, 8K, 16K and the maximum 20K. The first 5 training subsets are randomly chosen but fixed for different algorithms. The evaluation is performed on whole GREW test set. As presented in Figure 8, for state-of-the-art GaitSet and GaitPart, the Rank-1 on test set grows stably with more training identities. Therefore, the 20K size of the whole training set achieves the highest Rank-1 accuracy. Specifically, GaitSet increases the Rank-1 from 28.0% on 1K training subjects to 46.28% on 20K subjects. The results clearly show that large-scale GREW training data is helpful for future gait recognition research.\n\nFor GEINet baseline, the scale of training data does not obviously influence the performance. The reason may be that the network architecture in GEINet has limited capability to learn from large data. TS-CNN uses a two-stream metric learning network structure and takes pairs of GEIs as inputs, which may be less suffered from over-fitting. Therefore, its Rank-1 accuracy slightly increases from 9.50% to 13.55%. Model-based baselines are not sensitive to training data scales due to inferior accuracy. Accuracy with Increasing Test Identities A sufficient test set is essential for evaluating the performance of the gait recognizer. In this experiment, we study the relationship between the search space scale and the Rank-1 accuracy as shown in Figure 9. When the test identities increase from 1K to 6K, almost all approaches suffer from accuracy degradation. More specifically, GaitSet scores 57.45% Rank-1 on 1K test identities but decreases to 49.83% when test size is doubled. When the subjects increase to 6K, precision degradations of both GaitSet and GaitPart are more than 10%. With increasing identities in the gallery, the possibility of inter-subject appearance similarity becomes higher, so recognizing certain identity by top retrieval is more challenging. Evaluation results on other baselines come to the same conclusion. Accuracy with Distractor Set In gait applications, the gallery size may be very large considering numerous unrelated identities. We add the constructed distractor set into the gallery to investigate this practical setting. As shown in Figure 10, by enlarging the gallery with distractor set, most approaches obtain lower recognition scores. When all 233K distractor sequences are involved, Rank-1 of the best baseline GaitSet decreases to 41.97%. Accuracy with distractor set shows the necessity of the GREW benchmark again.\n\n\nPerformance on Different Attributes\n\nThis section investigates the performance variations of gait recognition between different attributes, including gender, age group, carrying and dressing. We adopt GaitSet [4] as the recognition approach since it performs best in the baseline experiments.\n\nThe Rank-1 accuracy for gender and age group is illustrated in Figure 11. According to the results, for most age groups, gait recognition performance on females is always better than that on males. We argue that females contain more different variations such as wearing and hairstyle, which may be helpful for individual recognition by gait silhouettes. For results on different age groups, one can find that performance on the children is worse than other groups because of walking mode immaturity. Besides, recognition accuracy on elders is slightly lower than on adults due to physical degeneration. The attribute results on carrying and dressing are shown in Table 7. Compared with normal walking (i.e. None), various carryings always decrease gait recognition accuracy. More specifically, Lift-stuff is most difficult since it contains more diversity. For dressing styles, results indicate that Skirt is more challenging to recognize by silhouettes in GaitSet.  \n\n\nGREW for Pre-training\n\nTo validate the effectiveness of pre-trained models using the GREW dataset, we conduct a cross-dataset experiment in this section. Original (both training and testing on CASIA-B), direct cross-dataset evaluation (training on GREW, testing on CASIA-B), and fine-tuning (pre-trained by the GREW, finetuning and evaluating on CASIA-B) performance of the GaitSet are compared. Specifically, GaitSet obtains 83.64%, 45.14%, 84.48% on CASIA-B via three settings. The accuracy of the second configuration is inferior because of the obvious domain gap. With fine-tuning on the target domain, gait recognition accuracy significantly outperforms the original ones by 0.84%, which shows the superior capacity of our dataset for pre-training.\n\n\nTimes\n\nApart from accuracy, speed is also a crucial factor for practical gait recognition, which is always neglected in previous literature. In this section, we compare inference time of different baselines, including pre-processing, gait feature extraction, and searching in the gallery. Times are roughly measured on the GREW test set by averaging all sequences duration. As shown in Table 8, for a sequence with 157 frames on average, pre-processing (i.e. detection, segmentation, pose estimation et al.) takes most of the time. Gait feature extraction (main network inference) and searching procedure are relatively faster. FLOPs and parameters of gait networks are also calculated for comparison. In summary, current gait recognition pipelines need to be optimized for real-world applications.  Figure 12 provides several sample results on the GREW test set, which are performed by GaitSet baseline. For the first probe, GaitSet successfully retrieves the subject in Rank-1 result, with changed clothes and different walking directions. For the second probe, the results of Rank-1 are incorrect due to similar skirt dressing, while following two retrievals, with carrying and partial occlusion, are true positive.\n\n\nSample results\n\n\nDiscussion and Conclusion\n\nDiscussion During construction of the GREW benchmark, privacy and bias problems are our first concern. To protect privacy, only silhouettes, flow and human poses would be utilized and released, which do not reveal any personal visual information. We will provide strict access for applicants who sign the license, and try our best to guarantee it for research purposes only. For dataset bias, the GREW has balanced gender distribution, while some attributes (e.g. race, age group, dressing) are inevitably biased due to capture location and time. Since our dataset is largescale and diverse, one can sample balanced data to train models with less bias. Besides, recent de-bias researches in the biometrics community [59,11,58] may also alleviate this problem.\n\nConclusion This paper makes the first step to large-scale gait recognition in the wild, to the best of our knowledge. Firstly, the GREW dataset contains 128K sequences of 26K subjects with rich attribute variations from flexible data. Secondly, we manually annotate thousands of hours streams from hundreds of cameras, resulting in 14M boxes with automatic silhouettes and human poses. Moreover, 233K distractor set sequences are collected for practical evaluation. Lastly, comprehensive baselines are conducted to quantitatively analysis the challenges in unconstrained gait recognition, deriving in-depth and constructive insights. Future work will further investigate open problems for gait recognition, e.g. influence of pre-processing, deeper and modern network, disentanglement, soft-biometric recognition, un/semi/self-supervised learning.\n\nFigure 1 :\n1Examples comparison for CASIA-B\n\nFigure 2 :\n2Identities examples of the GREW dataset. The first two rows show 2 subjects with various challenges. The last row shows a subject in distractor set. Faces are masked to protect privacy.\n\nFigure 3 :\n3Examples of silhouette, GEI, 2D and 3D human pose from the GREW dataset.\n\nFigure 4 :\n4Examples of optical flow from the GREW dataset\n\nFigure 6 :\n6The pipeline of gait recognition in the wild, consisting of pre-processing and recognition steps. Pre-processing part detects human from raw sequences, and provides silhouettes (GEIs) or poses information. Given a certain probe, the recognition part performs 1:N searching from the gallery.\n\nFigure 7 :\n7Rank-k result (%) of baselines. Trained on the GREW train and evaluated on test set. Legend shows Rank-1\u2192 Rank-20 accuracy.\n\nFigure 8 :\n8Rank-1 accuracy (%) on test set with increasing training identities. Legend shows performance changes from 1K to 20K data.\n\nFigure 9 :\n9Rank-1 accuracy (%) with different identities in test set. Legend shows performance changes from 1K to 6K test data.\n\nFigure 10 :\n10Rank-1 accuracy (%) with increasing gallery size. Different distractor scales are added. Legend shows performance changes from test to test + distractor.\n\nFigure 11 :\n11Rank-1 accuracy (%) on gender and age group attributes.\n\nFigure 12 :\n12Sample results on the GREW with GaitSet. Left part with blue boxes shows probes (3 frames belong to the same sequence), while results with green and red boxes are true positive and false positive respectively. Note that only silhouettes are used for gait recognition, and RGB images are just for visualization.\n\n\nTable 1, which are mainly constructed under controlled settings and designed for predefined cross-view gait recognition. However, in real scenarios, gait recognition would encounter fully-unconstrained challenges, such as diverse view, occlusion, various carrying and dressing, complex and dynamic background clutters, illumination, walking style, surface influence et al. Existing benchmarks are far behind the requirements of practical gait recognition. Considering the remarkable success of face recognitionDataset \nPublication \n#Id. \n#Seq. \n#Cam. \nData types \n# Distractor Environment View var. \nChallenges \nCMU MoBo [12] \nTR2001 \n25 \n600 \n6 \nRGB, Sil. \nNone \nControlled \nPredefined \nVI, CA, SP, SU \nCASIA-A [57] \nTPAMI2003 \n20 \n240 \n3 \nRGB \nNone \nControlled \nPredefined \nVI \nSOTON [45] \nASSC2004 \n115 \n2,128 \n2 \nRGB, Sil. \nNone \nControlled \nPredefined \nVI \nUSF [42] \nTPAMI2005 \n122 \n1,870 \n2 \nRGB \nNone \nControlled \nPredefined \nVI, CA, SU, SH \nCASIA-B [74] \nICPR2006 \n124 \n13,640 \n11 \nRGB, Sil. \nNone \nControlled \nPredefined \nVI, CA, DR \nCASIA-C [52] \nICPR2006 \n153 \n1,530 \n1 \nInf., Sil. \nNone \nControlled \nNone \nCA, SP \nOU-ISIR Speed [54] \nCVPR2010 \n34 \n612 \n1 \nSil. \nNone \nControlled \nNone \nSP \nOU-ISIR Cloth [19] \nPR2010 \n68 \n2,764 \n1 \nSil. \nNone \nControlled \nNone \nDR \nOU-ISIR MV [38] \nACCV2010 \n168 \n4,200 \n25 \nSil. \nNone \nControlled \nPredefined \nVI \nOU-LP [23] \nTIFS2012 \n4,007 \n7,842 \n2 \nSil. \nNone \nControlled \nPredefined \nVI \nADSC-AWD [35] \nTIFS2014 \n20 \n80 \n1 \nSil. \nNone \nControlled \nNone \nWD \nTUM GAID [18] \nJVCIR2014 \n305 \n3,370 \n1 \nRGB, D., A. \nNone \nControlled \nNone \nCA, SH \nOU-LP Age [68] \nCVA2017 \n63,846 63,846 \n1 \nSil. \nNone \nControlled \nNone \nAge \nOU-MVLP [51] \nCVA2018 \n10,307 288,596 \n14 \nSil. \nNone \nControlled \nPredefined \nVI \nOU-LP Bag [55] \nCVA2018 \n62,528 187,584 \n1 \nSil. \nNone \nControlled \nNone \nCA \nOU-MVLP Pose [2] TBIOM2020 10,307 288,596 \n14 \n2D Pose \nNone \nControlled \nPredefined \nVI \n\nGREW \n-\n26,345 128,671 \n882 \nSil. Flow \n2/3D Pose \n233,857 \nWild \nDiverse \nVI, DIS, BA, CA, \nDR, OCC, ILL, SU \n\n\n\nTable 2 :\n2Comparison with video-based person ReID datasets.\n\nTable 3 :\n3Comparison with long-term person ReID datasets.Dataset \n#Identities #Cameras #Boxes \nCVID-reID [76] \n90 \n-\n77K \nCOCAS [73] \n5,266 \n30 \n62K \nPRCC [69] \n221 \n3 \n33K \nGREW \n26,345 \n882 \n14M \n\n\n\nTable 4 :\n4Statistics of different splits.Split \n#Identities Sequences \nFrames \nTrain \n20,000 \n102,887 \n10,166,842 \nVal \n345 \n1,784 \n238,532 \nTest \n6,000 \n24,000 \n3,780,104 \nDistractor \n-\n233,857 \n9,676,016 \n\n\n\nTable 5 :\n5Overview of adopted baselines, including input data type, number of network layers, dimensions of embedding feature, and loss. N in #embedding of the GEINet means #training identities.Baseline \nInput \n#Layers #Embed. \nLoss \nGEINet \nGEI \n4 \nN \nSoftmax \nTS-CNN \nGEI \n6 \n-\n2-cls Cross-entropy \nGaitSet \nSil. \n10 \n15,872 \nBatch All triplet \nGaitPart \nSil. \n10 \n4,096 \nBatch All triplet \nPoseGait \n3D Pose \n22 \n512 \nSoftmax&Center \nGaitGraph 2D Pose \n44 \n256 \nContrastive \n\nTS-CNN [65] framework adopts two-stream CNN archi-\ntecture which learns similarities between GEIs pair for gait \nrecognition. MT architecture setting is utilized in this paper, \nwhich matches mid-level features at the top layer. TS-CNN \nalso takes GEIs as input and has 6 layers. 2-class Cross-\nentropy loss is used for training, while classifier indicates \nprobability of two subjects whether they are the same one \nduring inference. \nGaitSet [4] uses several convolution and pooling layers \nto extract convolutional templates on unordered silhouettes \nset. Batch All triplet loss [17] is adopted for optimizing, and \n15,872-d embedding features are utilized for recognition \nduring inference. Following the OU-MVLP training set-\nting, we use more channels convolutional layers and 250K \niterations with 2 learning rate schedule. \nGaitPart [9] proposes a part-based network design focus-\ning on fine-grained representation and micro-motion in dif-\nferent parts of the human body. Training and testing on the \nGREW benchmark follow most GaitSet settings. \n\n\n\nTable 6 :\n6Rank-1, Rank-5, Rank-10, Rank-20 (%) of baselines. Trained on the GREW train set and evaluated on test set.Baseline \nRank-1 Rank-5 Rank-10 Rank-20 \nGEINet \n6.82 \n13.42 \n16.97 \n21.01 \nTS-CNN \n13.55 \n24.55 \n30.15 \n37.01 \nGaitSet \n46.28 \n63.58 \n70.26 \n76.82 \nGaitPart \n44.01 \n60.68 \n67.25 \n73.47 \nPoseGait \n0.23 \n1.05 \n2.23 \n4.28 \nGaitGraph \n1.31 \n3.46 \n5.08 \n7.51 \n\n\n\nTable 7 :\n7Rank-1 accuracy (%) on carrying and dressing attributes. Subsets of probe (sequences with the corresponding attribute) are chosen to perform gait recognition. For evaluation with dressing, All means gait probe and gallery are paired without attention to any clothing style. Short/Long refers to short/long-wearing in both upper and lower body.Carrying \nRank-1 Dressing Rank-1 \nNone \n52.36 \nAll \n46.28 \nBackpack \n48.83 \nShort \n48.16 \nShoulder bag \n46.68 \nLong \n44.92 \nHandbag \n47.02 \nSkirt \n44.30 \nLift-stuff \n45.66 \n-\n-\n\n\n\nTable 8 :\n8Inference time, FLOPs and parameters of baselines (with single 2080TI GPU). Since TS-CNN needs multiple forward steps for a certain sequence, it is not compared.Baseline \nPre-process Feature \nSearch \nTotal \nFLOPs \nParams \nGEINet \n45.62s \n0.03s \n0.00066s 45.65s 0.02G \n7.68M \nGaitSet \n45.62s \n2.89s \n0.00058s 48.51s 1.06G \n6.31M \nGaitPart \n45.62s \n3.09s \n0.00234s 48.71s 0.92G \n6.01M \nPoseGait \n54.69s \n0.18s \n0.00046s 54.87s 0.08G \n7.74M \nGaitGraph \n53.59s \n0.05s \n0.00041s 53.64s 0.06G 527.95K \n\n\nAcknowledgements. This work was supported in part by the National Natural Science Foundation of China under Grant 61822603, Grant U1813218, and Grant U1713214, in part by a grant from the Beijing Academy of Artificial Intelligence (BAAI), and in part by a grant from the Institute for Guo Qiang, Tsinghua University.\nPerformance evaluation of model-based gait on multi-view very large population database with pose sequences. Weizhi An, Shiqi Yu, Yasushi Makihara, Xinhui Wu, Chi Xu, Yang Yu, Rijun Liao, Yasushi Yagi, TBIOM. 23Weizhi An, Shiqi Yu, Yasushi Makihara, Xinhui Wu, Chi Xu, Yang Yu, Rijun Liao, and Yasushi Yagi. Performance evalua- tion of model-based gait on multi-view very large population database with pose sequences. TBIOM, 2020. 2, 3\n\nVGGFace2: A dataset for recognising faces across pose and age. Qiong Cao, Li Shen, Weidi Xie, M Omkar, Andrew Parkhi, Zisserman, In FG. 2Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and An- drew Zisserman. VGGFace2: A dataset for recognising faces across pose and age. In FG, 2018. 2\n\nGaitSet: Regarding gait as a set for cross-view gait recognition. Hanqing Chao, Yiwei He, Junping Zhang, Jianfeng Feng, AAAI. 67Hanqing Chao, Yiwei He, Junping Zhang, and Jianfeng Feng. GaitSet: Regarding gait as a set for cross-view gait recognition. In AAAI, 2019. 1, 2, 3, 5, 6, 7\n\n3D human pose estimation = 2D pose estimation + matching. Ching-Hang Chen, Deva Ramanan, CVPR. 45Ching-Hang Chen and Deva Ramanan. 3D human pose es- timation = 2D pose estimation + matching. In CVPR, 2017. 4, 5\n\nHybrid task cascade for instance segmentation. Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, CVPR. 34Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox- iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance seg- mentation. In CVPR, 2019. 3, 4\n\nCombining multiple evidences for gait recognition. Naresh Cuntoor, Amit Kale, Rama Chellappa, ICASSP. Naresh Cuntoor, Amit Kale, and Rama Chellappa. Com- bining multiple evidences for gait recognition. In ICASSP, 2003. 1\n\nArcFace: Additive angular margin loss for deep face recognition. Jiankang Deng, Jia Guo, Stefanos Zafeiriou, CVPR. Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. ArcFace: Additive angular margin loss for deep face recognition. In CVPR, 2019. 2\n\nGaitPart: Temporal part-based model for gait recognition. Yunjie Chao Fan, Chunshui Peng, Xu Cao, Saihui Liu, Jiannan Hou, Yongzhen Chi, Qing Huang, Zhiqiang Li, He, CVPR. 56Chao Fan, Yunjie Peng, Chunshui Cao, Xu Liu, Saihui Hou, Jiannan Chi, Yongzhen Huang, Qing Li, and Zhiqiang He. GaitPart: Temporal part-based model for gait recognition. In CVPR, 2020. 1, 3, 5, 6\n\nHorizontal pyramid matching for person re-identification. Yang Fu, Yunchao Wei, Yuqian Zhou, Honghui Shi, Gao Huang, Xinchao Wang, Zhiqiang Yao, Thomas Huang, AAAI. Yang Fu, Yunchao Wei, Yuqian Zhou, Honghui Shi, Gao Huang, Xinchao Wang, Zhiqiang Yao, and Thomas Huang. Horizontal pyramid matching for person re-identification. In AAAI, 2019. 2\n\nJointly debiasing face recognition and demographic attribute estimation. Sixue Gong, Xiaoming Liu, Jain, ECCV. Sixue Gong, Xiaoming Liu, and Anil K Jain. Jointly de- biasing face recognition and demographic attribute estima- tion. In ECCV, 2020. 8\n\nThe CMU motion of body (MoBo) database. Ralph Gross, Jianbo Shi, Ralph Gross and Jianbo Shi. The CMU motion of body (MoBo) database. 2001. 2\n\nMS-Celeb-1M: A dataset and benchmark for large-scale face recognition. Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao, ECCV. 26Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. MS-Celeb-1M: A dataset and benchmark for large-scale face recognition. In ECCV, 2016. 2, 6\n\nIndividual recognition using gait energy image. Ju Han, Bir Bhanu, TPAMI. 2Ju Han and Bir Bhanu. Individual recognition using gait en- ergy image. TPAMI, 2006. 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask R-CNN. Kaiming He, Georgia Gkioxari, ICCV. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask R-CNN. In ICCV, 2017. 4\n\nMulti-task GANs for view-specific feature learning in gait recognition. Yiwei He, Junping Zhang, Hongming Shan, Liang Wang, TIFS. 1Yiwei He, Junping Zhang, Hongming Shan, and Liang Wang. Multi-task GANs for view-specific feature learning in gait recognition. TIFS, 2019. 1\n\nAlexander Hermans, Lucas Beyer, Bastian Leibe, arXiv:1703.07737defense of the triplet loss for person re-identification. Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-identification. arXiv:1703.07737, 2017. 1, 2, 5\n\nThe TUM gait from audio, image and depth (GAID) database: Multimodal recognition of subjects and traits. Martin Hofmann, J\u00fcrgen Geiger, Sebastian Bachmann, Bj\u00f6rn Schuller, Gerhard Rigoll, JVCIR. 2Martin Hofmann, J\u00fcrgen Geiger, Sebastian Bachmann, Bj\u00f6rn Schuller, and Gerhard Rigoll. The TUM gait from au- dio, image and depth (GAID) database: Multimodal recog- nition of subjects and traits. JVCIR, 2014. 2\n\nClothing-invariant gait identification using partbased clothing categorization and adaptive weight control. Yasushi Md Altab Hossain, Junqiu Makihara, Yasushi Wang, Yagi, PR. 2Md Altab Hossain, Yasushi Makihara, Junqiu Wang, and Ya- sushi Yagi. Clothing-invariant gait identification using part- based clothing categorization and adaptive weight control. PR, 2010. 2\n\nGait lateral network: Learning discriminative and compact representations for gait recognition. Saihui Hou, Chunshui Cao, Xu Liu, Yongzhen Huang, ECCV. 13Saihui Hou, Chunshui Cao, Xu Liu, and Yongzhen Huang. Gait lateral network: Learning discriminative and compact representations for gait recognition. In ECCV, 2020. 1, 3\n\nCurricularFace: adaptive curriculum learning loss for deep face recognition. Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, Feiyue Huang, CVPR. Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. CurricularFace: adaptive curriculum learning loss for deep face recognition. In CVPR, 2020. 2\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, Thomas Brox, CVPR. Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu- tion of optical flow estimation with deep networks. In CVPR, 2017. 4\n\nThe OU-ISIR gait database comprising the large population dataset and performance evaluation of gait recognition. Haruyuki Iwama, Mayu Okumura, Yasushi Makihara, Yasushi Yagi, TIFS. 12Haruyuki Iwama, Mayu Okumura, Yasushi Makihara, and Yasushi Yagi. The OU-ISIR gait database comprising the large population dataset and performance evaluation of gait recognition. TIFS, 2012. 1, 2\n\nThe MegaFace benchmark: 1 million faces for recognition at scale. Ira Kemelmacher-Shlizerman, M Steven, Daniel Seitz, Evan Miller, Brossard, CVPR. Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The MegaFace benchmark: 1 million faces for recognition at scale. In CVPR, 2016. 2\n\nGlobal-local temporal representations for video person re-identification. Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, Shiliang Zhang, ICCV. 34Jianing Li, Jingdong Wang, Qi Tian, Wen Gao, and Shiliang Zhang. Global-local temporal representations for video per- son re-identification. In ICCV, 2019. 3, 4\n\nUnsupervised person re-identification by deep learning tracklet association. Minxian Li, Xiatian Zhu, Shaogang Gong, ECCV. 34Minxian Li, Xiatian Zhu, and Shaogang Gong. Unsupervised person re-identification by deep learning tracklet association. In ECCV, 2018. 3, 4\n\nDeep-ReID: Deep filter pairing neural network for person reidentification. Wei Li, Rui Zhao, Tong Xiao, Xiaogang Wang, CVPR. Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep- ReID: Deep filter pairing neural network for person re- identification. In CVPR, 2014. 2\n\nJoint intensity transformer network for gait recognition robust against clothing and carrying status. Xiang Li, Yasushi Makihara, Chi Xu, Yasushi Yagi, Mingwu Ren, TIFS. 3Xiang Li, Yasushi Makihara, Chi Xu, Yasushi Yagi, and Mingwu Ren. Joint intensity transformer network for gait recognition robust against clothing and carrying status. TIFS, 2019. 3\n\nGait recognition via semi-supervised disentangled representation learning to identity and covariate features. Xiang Li, Yasushi Makihara, Chi Xu, Yasushi Yagi, Mingwu Ren, CVPR. 2020Xiang Li, Yasushi Makihara, Chi Xu, Yasushi Yagi, and Mingwu Ren. Gait recognition via semi-supervised disen- tangled representation learning to identity and covariate fea- tures. In CVPR, 2020. 3\n\nEnd-to-end model-based gait recognition. Xiang Li, Yasushi Makihara, Chi Xu, Yasushi Yagi, Shiqi Yu, Mingwu Ren, ACCV. 2020Xiang Li, Yasushi Makihara, Chi Xu, Yasushi Yagi, Shiqi Yu, and Mingwu Ren. End-to-end model-based gait recog- nition. In ACCV, 2020. 3\n\nPose-based temporal-spatial network (PTSN) for gait recognition with carrying and clothing variations. Rijun Liao, Chunshui Cao, B Edel, Shiqi Garcia, Yongzhen Yu, Huang, CCBR. Rijun Liao, Chunshui Cao, Edel B Garcia, Shiqi Yu, and Yongzhen Huang. Pose-based temporal-spatial network (PTSN) for gait recognition with carrying and clothing vari- ations. In CCBR, 2017. 1\n\nA model-based gait recognition method with body pose and human prior knowledge. Rijun Liao, Shiqi Yu, Weizhi An, Yongzhen Huang, 6PR, 2020. 3, 5Rijun Liao, Shiqi Yu, Weizhi An, and Yongzhen Huang. A model-based gait recognition method with body pose and hu- man prior knowledge. PR, 2020. 3, 5, 6\n\nMicrosoft COCO: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 6\n\nMemory-based gait recognition. Dan Liu, Mao Ye, Xudong Li, Feng Zhang, Lan Lin, BMVC. Dan Liu, Mao Ye, Xudong Li, Feng Zhang, and Lan Lin. Memory-based gait recognition. In BMVC, 2016. 3\n\nHuman identity and gender recognition from gait sequences with arbitrary walking directions. Jiwen Lu, Gang Wang, Pierre Moulin, TIFS. 2Jiwen Lu, Gang Wang, and Pierre Moulin. Human identity and gender recognition from gait sequences with arbitrary walking directions. TIFS, 2014. 2\n\nBag of tricks and a strong baseline for deep person re-identification. Youzhi Hao Luo, Xingyu Gu, Shenqi Liao, Wei Lai, Jiang, CVPR Workshops. Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei Jiang. Bag of tricks and a strong baseline for deep person re-identification. In CVPR Workshops, 2019. 2\n\nThe OU-ISIR gait database comprising the treadmill dataset. Yasushi Makihara, Hidetoshi Mannami, Akira Tsuji, Kazushige Md Altab Hossain, Atsushi Sugiura, Yasushi Mori, Yagi, CVA. 1Yasushi Makihara, Hidetoshi Mannami, Akira Tsuji, Md Al- tab Hossain, Kazushige Sugiura, Atsushi Mori, and Yasushi Yagi. The OU-ISIR gait database comprising the treadmill dataset. CVA, 2012. 1\n\nGait analysis of gender and age using a large-scale multiview gait database. Yasushi Makihara, Hidetoshi Mannami, Yasushi Yagi, ACCV. Yasushi Makihara, Hidetoshi Mannami, and Yasushi Yagi. Gait analysis of gender and age using a large-scale multi- view gait database. In ACCV, 2010. 2\n\nGait-based person re-identification: A survey. Athira Nambiar, Alexandre Bernardino, Jacinto C Nascimento, ACM Computing Surveys. 1Athira Nambiar, Alexandre Bernardino, and Jacinto C Nasci- mento. Gait-based person re-identification: A survey. ACM Computing Surveys, 2019. 1\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, NeurIPS. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 5\n\n. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, ImageNet large scale visual recognition challenge. IJCVOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. IJCV, 2015. 6\n\nThe hu-manID gait challenge problem: Data sets, performance, and analysis. TPAMI. Sudeep Sarkar, Jonathon Phillips, Zongyi Liu, Isidro Robledo Vega, Patrick Grother, Kevin W Bowyer, 1Sudeep Sarkar, P Jonathon Phillips, Zongyi Liu, Isidro Rob- ledo Vega, Patrick Grother, and Kevin W Bowyer. The hu- manID gait challenge problem: Data sets, performance, and analysis. TPAMI, 2005. 1, 2\n\nFaceNet: A unified embedding for face recognition and clustering. Florian Schroff, Dmitry Kalenichenko, James Philbin, CVPR. Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified embedding for face recognition and clus- tering. In CVPR, 2015. 2\n\nGEINet: View-invariant gait recognition using a convolutional neural network. K Shiraga, Y Makihara, D Muramatsu, T Echigo, Y Yagi, ICB. 56K. Shiraga, Y. Makihara, D. Muramatsu, T. Echigo, and Y. Yagi. GEINet: View-invariant gait recognition using a con- volutional neural network. In ICB, 2016. 1, 3, 5, 6\n\nOn a large sequence-based human gait database. Jamie D Shutler, G Michael, Grant, S Mark, John N Nixon, Carter, Applications and Science in Soft Computing. Jamie D Shutler, Michael G Grant, Mark S Nixon, and John N Carter. On a large sequence-based human gait database. In Applications and Science in Soft Computing. 2004. 2\n\nRegion-based quality estimation network for large-scale person re-identification. Guanglu Song, Biao Leng, Yu Liu, Congrui Hetang, Shaofan Cai, AAAI. 34Guanglu Song, Biao Leng, Yu Liu, Congrui Hetang, and Shaofan Cai. Region-based quality estimation network for large-scale person re-identification. In AAAI, 2018. 3, 4\n\nDeep high-resolution representation learning for human pose estimation. Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang, CVPR. 45Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose esti- mation. In CVPR, 2019. 4, 5\n\nBeyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline). Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang, In ECCV. 2Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin Wang. Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline). In ECCV, 2018. 2\n\nDeepFace: Closing the gap to human-level performance in face verification. Yaniv Taigman, Ming Yang, Marc&apos;aurelio Ranzato, Lior Wolf, CVPR. Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf. DeepFace: Closing the gap to human-level perfor- mance in face verification. In CVPR, 2014. 2\n\nOn Input/Output architectures for convolutional neural network-based cross-view gait recognition. Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, Yasushi Yagi, Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, and Yasushi Yagi. On Input/Output archi- tectures for convolutional neural network-based cross-view gait recognition. TCSVT, 2017. 1\n\nMulti-view large population gait dataset and its performance evaluation for crossview gait recognition. Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, Yasushi Yagi, CVA. 6Noriko Takemura, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, and Yasushi Yagi. Multi-view large popu- lation gait dataset and its performance evaluation for cross- view gait recognition. CVA, 2018. 1, 2, 6\n\nEfficient night gait recognition based on template matching. Daoliang Tan, Kaiqi Huang, Shiqi Yu, Tieniu Tan, ICPR. Daoliang Tan, Kaiqi Huang, Shiqi Yu, and Tieniu Tan. Effi- cient night gait recognition based on template matching. In ICPR, 2006. 2\n\nGaitGraph: Graph convolutional network for skeleton-based gait recognition. Torben Teepe, Ali Khan, Johannes Gilg, Fabian Herzog, Stefan H\u00f6rmann, Gerhard Rigoll, arXiv:2101.1122836arXiv preprintTorben Teepe, Ali Khan, Johannes Gilg, Fabian Herzog, Ste- fan H\u00f6rmann, and Gerhard Rigoll. GaitGraph: Graph convo- lutional network for skeleton-based gait recognition. arXiv preprint arXiv:2101.11228, 2021. 3, 5, 6\n\nSilhouette transformation based on walking speed for gait identification. Akira Tsuji, Yasushi Makihara, Yasushi Yagi, CVPR. Akira Tsuji, Yasushi Makihara, and Yasushi Yagi. Silhou- ette transformation based on walking speed for gait identifi- cation. In CVPR, 2010. 2\n\nThe OU-ISIR large population gait database with reallife carried object and its performance evaluation. Thanh Trung Md Zasim Uddin, Yasushi Ngo, Noriko Makihara, Xiang Takemura, Daigo Li, Yasushi Muramatsu, Yagi, CVA. 23Md Zasim Uddin, Thanh Trung Ngo, Yasushi Makihara, Noriko Takemura, Xiang Li, Daigo Muramatsu, and Yasushi Yagi. The OU-ISIR large population gait database with real- life carried object and its performance evaluation. CVA, 2018. 2, 3\n\nCosFace: Large margin cosine loss for deep face recognition. Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jingchao Zhou, Wei Liu, CVPR. Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jingchao Zhou, and Wei Liu. CosFace: Large margin cosine loss for deep face recognition. In CVPR, 2018. 2\n\nSilhouette analysis-based gait recognition for human identification. Liang Wang, Tieniu Tan, Huazhong Ning, Weiming Hu, TPAMI. 2Liang Wang, Tieniu Tan, Huazhong Ning, and Weiming Hu. Silhouette analysis-based gait recognition for human identi- fication. TPAMI, 2003. 2\n\nMitigate bias in face recognition using skewness-aware reinforcement learning. Mei Wang, Weihong Deng, arXiv:1911.10692arXiv preprintMei Wang and Weihong Deng. Mitigate bias in face recog- nition using skewness-aware reinforcement learning. arXiv preprint arXiv:1911.10692, 2019. 8\n\nRacial faces in the wild: Reducing racial bias by information maximization adaptation network. Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, Yaohai Huang, CVPR. Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Racial faces in the wild: Reducing racial bias by information maximization adaptation network. In CVPR, 2019. 8\n\nPerson re-identification by video ranking. Taiqing Wang, Shaogang Gong, Xiatian Zhu, Shengjin Wang, ECCV. 34Taiqing Wang, Shaogang Gong, Xiatian Zhu, and Shengjin Wang. Person re-identification by video ranking. In ECCV, 2014. 3, 4\n\nPerson transfer GAN to bridge domain gap for person reidentification. Longhui Wei, Shiliang Zhang, Wen Gao, Qi Tian, CVPR. Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. Per- son transfer GAN to bridge domain gap for person re- identification. In CVPR, 2018. 2\n\nMulti-view gait recognition using 3D convolutional neural networks. Thomas Wolf, Mohammadreza Babaee, Gerhard Rigoll, ICIP. Thomas Wolf, Mohammadreza Babaee, and Gerhard Rigoll. Multi-view gait recognition using 3D convolutional neural networks. In ICIP, 2016. 1\n\nSpatial-temporal graph attention network for videobased gait recognition. Xinhui Wu, Weizhi An, Shiqi Yu, Weiyu Guo, Garc\u00eda, ACPR. Xinhui Wu, Weizhi An, Shiqi Yu, Weiyu Guo, and Edel B Garc\u00eda. Spatial-temporal graph attention network for video- based gait recognition. In ACPR, 2019. 1\n\nExploit the unknown gradually: One-shot video-based person re-identification by stepwise learning. Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang, CVPR. 34Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wanli Ouyang, and Yi Yang. Exploit the unknown gradually: One-shot video-based person re-identification by stepwise learning. In CVPR, 2018. 3, 4\n\nA comprehensive study on cross-view gait based human identification with deep CNNs. Zifeng Wu, Yongzhen Huang, Liang Wang, Xiaogang Wang, Tieniu Tan, 56Zifeng Wu, Yongzhen Huang, Liang Wang, Xiaogang Wang, and Tieniu Tan. A comprehensive study on cross-view gait based human identification with deep CNNs. TPAMI, 2017. 1, 3, 5, 6\n\nMargin sample mining loss: A deep learning based method for person reidentification. Qiqi Xiao, Hao Luo, Chi Zhang, arXiv:1710.00478Qiqi Xiao, Hao Luo, and Chi Zhang. Margin sample min- ing loss: A deep learning based method for person re- identification. arXiv:1710.00478, 2017. 2\n\nGait recognition from a single image using a phaseaware gait cycle reconstruction network. Chi Xu, Yasushi Makihara, Xiang Li, Yasushi Yagi, Jianfeng Lu, ECCV. 2020Chi Xu, Yasushi Makihara, Xiang Li, Yasushi Yagi, and Jian- feng Lu. Gait recognition from a single image using a phase- aware gait cycle reconstruction network. In ECCV, 2020. 1\n\nThe OU-ISIR gait database comprising the large population dataset with age and performance evaluation of age estimation. Chi Xu, Yasushi Makihara, Gakuto Ogi, Xiang Li, Yasushi Yagi, Jianfeng Lu, CVA. 23Chi Xu, Yasushi Makihara, Gakuto Ogi, Xiang Li, Yasushi Yagi, and Jianfeng Lu. The OU-ISIR gait database compris- ing the large population dataset with age and performance evaluation of age estimation. CVA, 2017. 2, 3\n\nPerson reidentification by contour sketch under moderate clothing change. TPAMI. Qize Yang, Ancong Wu, Wei-Shi Zheng, 34Qize Yang, Ancong Wu, and Wei-Shi Zheng. Person re- identification by contour sketch under moderate clothing change. TPAMI, 2019. 3, 4\n\nDong Yi, Zhen Lei, Shengcai Liao, Stan Z Li, arXiv:1411.7923Learning face representation from scratch. Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv:1411.7923, 2014. 2\n\nGaitGAN: Invariant gait feature extraction using generative adversarial networks. Shiqi Yu, Haifeng Chen, Edel B Garcia Reyes, Norman Poh, CVPR Workshops. Shiqi Yu, Haifeng Chen, Edel B Garcia Reyes, and Norman Poh. GaitGAN: Invariant gait feature extraction using gen- erative adversarial networks. In CVPR Workshops, 2017. 1\n\nInvariant feature extraction for gait recognition using only one uniform model. Shiqi Yu, Haifeng Chen, Qing Wang, Linlin Shen, Yongzhen Huang, Neurocomputing. 1Shiqi Yu, Haifeng Chen, Qing Wang, Linlin Shen, and Yongzhen Huang. Invariant feature extraction for gait recog- nition using only one uniform model. Neurocomputing, 2017. 1\n\nCocas: A large-scale clothes changing person dataset for re-identification. Shijie Yu, Shihua Li, Dapeng Chen, Rui Zhao, Junjie Yan, Yu Qiao, CVPR. 34Shijie Yu, Shihua Li, Dapeng Chen, Rui Zhao, Junjie Yan, and Yu Qiao. Cocas: A large-scale clothes changing person dataset for re-identification. In CVPR, 2020. 3, 4\n\nA framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition. Shiqi Yu, Daoliang Tan, Tieniu Tan, ICPR. 6Shiqi Yu, Daoliang Tan, and Tieniu Tan. A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition. In ICPR, 2006. 1, 2, 6\n\nLearning joint gait representation via quintuplet loss minimization. Kaihao Zhang, Wenhan Luo, Lin Ma, Wei Liu, Hongdong Li, CVPR. Kaihao Zhang, Wenhan Luo, Lin Ma, Wei Liu, and Hong- dong Li. Learning joint gait representation via quintuplet loss minimization. In CVPR, 2019. 1\n\nLearning spatial-temporal representations over walking tracklet for long-term person re-identification in the wild. Peng Zhang, Jingsong Xu, Qiang Wu, Yan Huang, Xianye Ben, TMM. 34Peng Zhang, Jingsong Xu, Qiang Wu, Yan Huang, and Xi- anye Ben. Learning spatial-temporal representations over walking tracklet for long-term person re-identification in the wild. TMM, 2020. 3, 4\n\nAligne-dReID: Surpassing human-level performance in person reidentification. Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun, Qiqi Xiao, Wei Jiang, Chi Zhang, Jian Sun, arXiv:1711.08184Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun, Qiqi Xiao, Wei Jiang, Chi Zhang, and Jian Sun. Aligne- dReID: Surpassing human-level performance in person re- identification. arXiv:1711.08184, 2017. 2\n\nCross-view gait recognition by discriminative feature learning. Yuqi Zhang, Yongzhen Huang, Shiqi Yu, Liang Wang, TIP. 1Yuqi Zhang, Yongzhen Huang, Shiqi Yu, and Liang Wang. Cross-view gait recognition by discriminative feature learn- ing. TIP, 2019. 1\n\nGait recognition via disentangled representation learning. Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaoming Liu, Jian Wan, Nanxin Wang, CVPR. Ziyuan Zhang, Luan Tran, Xi Yin, Yousef Atoum, Xiaom- ing Liu, Jian Wan, and Nanxin Wang. Gait recognition via disentangled representation learning. In CVPR, 2019. 1\n\nMARS: A video benchmark for large-scale person re-identification. Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su, Shengjin Wang, Qi Tian, ECCV. 24Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su, Shengjin Wang, and Qi Tian. MARS: A video benchmark for large-scale person re-identification. In ECCV, 2016. 2, 3, 4\n\nScalable person re-identification: A benchmark. Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, Qi Tian, ICCV. 26Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing- dong Wang, and Qi Tian. Scalable person re-identification: A benchmark. In ICCV, 2015. 2, 6\n\nLiang Zheng, Yi Yang, Alexander G Hauptmann, arXiv:1610.02984Person re-identification: Past, present and future. Liang Zheng, Yi Yang, and Alexander G Haupt- mann. Person re-identification: Past, present and future. arXiv:1610.02984, 2016. 2\n\nUnlabeled samples generated by gan improve the person re-identification baseline in vitro. Zhedong Zheng, Liang Zheng, Yi Yang, ICCV. Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam- ples generated by gan improve the person re-identification baseline in vitro. In ICCV, 2017. 2\n\nWebFace260M: A benchmark unveiling the power of million-scale deep face recognition. Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Dalong Du, Jie Zhou, CVPR. Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Da- long Du, and Jie Zhou. WebFace260M: A benchmark un- veiling the power of million-scale deep face recognition. In CVPR, 2021. 2\n", "annotations": {"author": "[{\"end\":77,\"start\":45},{\"end\":89,\"start\":78},{\"end\":100,\"start\":90},{\"end\":114,\"start\":101},{\"end\":185,\"start\":115},{\"end\":197,\"start\":186},{\"end\":208,\"start\":198},{\"end\":240,\"start\":209},{\"end\":272,\"start\":241}]", "publisher": null, "author_last_name": "[{\"end\":54,\"start\":51},{\"end\":88,\"start\":85},{\"end\":99,\"start\":95},{\"end\":113,\"start\":108},{\"end\":128,\"start\":124},{\"end\":196,\"start\":191},{\"end\":207,\"start\":205},{\"end\":217,\"start\":215},{\"end\":249,\"start\":245}]", "author_first_name": "[{\"end\":50,\"start\":45},{\"end\":84,\"start\":78},{\"end\":94,\"start\":90},{\"end\":107,\"start\":101},{\"end\":123,\"start\":115},{\"end\":190,\"start\":186},{\"end\":204,\"start\":198},{\"end\":214,\"start\":209},{\"end\":244,\"start\":241}]", "author_affiliation": "[{\"end\":76,\"start\":56},{\"end\":184,\"start\":130},{\"end\":239,\"start\":219},{\"end\":271,\"start\":251}]", "title": "[{\"end\":42,\"start\":1},{\"end\":314,\"start\":273}]", "venue": null, "abstract": "[{\"end\":1752,\"start\":316}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b72\"},\"end\":2069,\"start\":2065},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2083,\"start\":2079},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2227,\"start\":2223},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2576,\"start\":2572},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2578,\"start\":2576},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":2581,\"start\":2578},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":2584,\"start\":2581},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2587,\"start\":2584},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":2590,\"start\":2587},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":2593,\"start\":2590},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2596,\"start\":2593},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2599,\"start\":2596},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2602,\"start\":2599},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2604,\"start\":2602},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":2607,\"start\":2604},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":2610,\"start\":2607},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2613,\"start\":2610},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":2640,\"start\":2636},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2643,\"start\":2640},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":2646,\"start\":2643},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":2649,\"start\":2646},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2683,\"start\":2679},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2685,\"start\":2683},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":2688,\"start\":2685},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2691,\"start\":2688},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2694,\"start\":2691},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2697,\"start\":2694},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":3183,\"start\":3179},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3200,\"start\":3196},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4091,\"start\":4087},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4094,\"start\":4091},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":4097,\"start\":4094},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4099,\"start\":4097},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4102,\"start\":4099},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":4105,\"start\":4102},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4107,\"start\":4105},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4110,\"start\":4107},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4113,\"start\":4110},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":4116,\"start\":4113},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":4157,\"start\":4153},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4160,\"start\":4157},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4163,\"start\":4160},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":4166,\"start\":4163},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4169,\"start\":4166},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4172,\"start\":4169},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":4175,\"start\":4172},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":4178,\"start\":4175},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":4181,\"start\":4178},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":4184,\"start\":4181},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4187,\"start\":4184},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":4190,\"start\":4187},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5148,\"start\":5144},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5841,\"start\":5838},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9493,\"start\":9490},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10341,\"start\":10337},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":10344,\"start\":10341},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11300,\"start\":11296},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":11303,\"start\":11300},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11306,\"start\":11303},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11309,\"start\":11306},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11312,\"start\":11309},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11315,\"start\":11312},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":11339,\"start\":11335},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":11342,\"start\":11339},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":11345,\"start\":11342},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11548,\"start\":11544},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11551,\"start\":11548},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11553,\"start\":11551},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11555,\"start\":11553},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11558,\"start\":11555},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11561,\"start\":11558},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11582,\"start\":11578},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11585,\"start\":11582},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11588,\"start\":11585},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11590,\"start\":11588},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11593,\"start\":11590},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11596,\"start\":11593},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11723,\"start\":11719},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":11743,\"start\":11739},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11770,\"start\":11766},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11798,\"start\":11794},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11816,\"start\":11812},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11841,\"start\":11837},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12063,\"start\":12060},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12106,\"start\":12102},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12709,\"start\":12705},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12717,\"start\":12714},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12777,\"start\":12773},{\"end\":12779,\"start\":12777},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15984,\"start\":15980},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":15987,\"start\":15984},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15989,\"start\":15987},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15991,\"start\":15989},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16020,\"start\":16016},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16023,\"start\":16020},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16203,\"start\":16199},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":16658,\"start\":16654},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17036,\"start\":17032},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17111,\"start\":17108},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":17144,\"start\":17140},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17368,\"start\":17364},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18657,\"start\":18654},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18674,\"start\":18671},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":18792,\"start\":18788},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18809,\"start\":18805},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":18999,\"start\":18995},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19015,\"start\":19011},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19406,\"start\":19402},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19425,\"start\":19421},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20429,\"start\":20426},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20432,\"start\":20429},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":20435,\"start\":20432},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":20438,\"start\":20435},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23423,\"start\":23420},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27216,\"start\":27212},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27219,\"start\":27216},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27222,\"start\":27219}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28148,\"start\":28104},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28347,\"start\":28149},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28433,\"start\":28348},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28493,\"start\":28434},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28797,\"start\":28494},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28934,\"start\":28798},{\"attributes\":{\"id\":\"fig_7\"},\"end\":29070,\"start\":28935},{\"attributes\":{\"id\":\"fig_8\"},\"end\":29200,\"start\":29071},{\"attributes\":{\"id\":\"fig_9\"},\"end\":29369,\"start\":29201},{\"attributes\":{\"id\":\"fig_10\"},\"end\":29440,\"start\":29370},{\"attributes\":{\"id\":\"fig_11\"},\"end\":29766,\"start\":29441},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31807,\"start\":29767},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31869,\"start\":31808},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":32071,\"start\":31870},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32282,\"start\":32072},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":33821,\"start\":32283},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":34198,\"start\":33822},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34732,\"start\":34199},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":35242,\"start\":34733}]", "paragraph": "[{\"end\":2363,\"start\":1768},{\"end\":3031,\"start\":2365},{\"end\":4253,\"start\":3033},{\"end\":5560,\"start\":4255},{\"end\":6676,\"start\":5562},{\"end\":6730,\"start\":6678},{\"end\":7985,\"start\":6732},{\"end\":9040,\"start\":8025},{\"end\":9456,\"start\":9075},{\"end\":9953,\"start\":9458},{\"end\":10789,\"start\":9955},{\"end\":11425,\"start\":10791},{\"end\":12836,\"start\":11456},{\"end\":14150,\"start\":12903},{\"end\":14712,\"start\":14169},{\"end\":15191,\"start\":14736},{\"end\":15312,\"start\":15193},{\"end\":15877,\"start\":15314},{\"end\":16626,\"start\":15916},{\"end\":17007,\"start\":16647},{\"end\":17796,\"start\":17023},{\"end\":18424,\"start\":17812},{\"end\":19590,\"start\":18450},{\"end\":20280,\"start\":19592},{\"end\":21343,\"start\":20307},{\"end\":23208,\"start\":21345},{\"end\":23503,\"start\":23248},{\"end\":24472,\"start\":23505},{\"end\":25228,\"start\":24498},{\"end\":26449,\"start\":25238},{\"end\":27255,\"start\":26496},{\"end\":28103,\"start\":27257}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12883,\"start\":12837}]", "table_ref": "[{\"end\":3548,\"start\":3541},{\"end\":5546,\"start\":5539},{\"end\":8167,\"start\":8160},{\"end\":9965,\"start\":9958},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11418,\"start\":11411},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":15190,\"start\":15183},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16098,\"start\":16091},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":16769,\"start\":16762},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":18538,\"start\":18531},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":24175,\"start\":24168},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":25624,\"start\":25617}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1766,\"start\":1754},{\"attributes\":{\"n\":\"2.\"},\"end\":8004,\"start\":7988},{\"attributes\":{\"n\":\"2.1.\"},\"end\":8023,\"start\":8007},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9073,\"start\":9043},{\"attributes\":{\"n\":\"2.3.\"},\"end\":11454,\"start\":11428},{\"attributes\":{\"n\":\"2.4.\"},\"end\":12901,\"start\":12885},{\"attributes\":{\"n\":\"2.5.\"},\"end\":14167,\"start\":14153},{\"attributes\":{\"n\":\"2.6.\"},\"end\":14734,\"start\":14715},{\"end\":15894,\"start\":15880},{\"attributes\":{\"n\":\"3.\"},\"end\":15914,\"start\":15897},{\"attributes\":{\"n\":\"3.1.\"},\"end\":16645,\"start\":16629},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17021,\"start\":17010},{\"attributes\":{\"n\":\"4.\"},\"end\":17810,\"start\":17799},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18448,\"start\":18427},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20305,\"start\":20283},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23246,\"start\":23211},{\"attributes\":{\"n\":\"4.4.\"},\"end\":24496,\"start\":24475},{\"attributes\":{\"n\":\"4.5.\"},\"end\":25236,\"start\":25231},{\"attributes\":{\"n\":\"4.6.\"},\"end\":26466,\"start\":26452},{\"attributes\":{\"n\":\"5.\"},\"end\":26494,\"start\":26469},{\"end\":28115,\"start\":28105},{\"end\":28160,\"start\":28150},{\"end\":28359,\"start\":28349},{\"end\":28445,\"start\":28435},{\"end\":28505,\"start\":28495},{\"end\":28809,\"start\":28799},{\"end\":28946,\"start\":28936},{\"end\":29082,\"start\":29072},{\"end\":29213,\"start\":29202},{\"end\":29382,\"start\":29371},{\"end\":29453,\"start\":29442},{\"end\":31818,\"start\":31809},{\"end\":31880,\"start\":31871},{\"end\":32082,\"start\":32073},{\"end\":32293,\"start\":32284},{\"end\":33832,\"start\":33823},{\"end\":34209,\"start\":34200},{\"end\":34743,\"start\":34734}]", "table": "[{\"end\":31807,\"start\":30279},{\"end\":32071,\"start\":31929},{\"end\":32282,\"start\":32115},{\"end\":33821,\"start\":32479},{\"end\":34198,\"start\":33941},{\"end\":34732,\"start\":34554},{\"end\":35242,\"start\":34906}]", "figure_caption": "[{\"end\":28148,\"start\":28117},{\"end\":28347,\"start\":28162},{\"end\":28433,\"start\":28361},{\"end\":28493,\"start\":28447},{\"end\":28797,\"start\":28507},{\"end\":28934,\"start\":28811},{\"end\":29070,\"start\":28948},{\"end\":29200,\"start\":29084},{\"end\":29369,\"start\":29216},{\"end\":29440,\"start\":29385},{\"end\":29766,\"start\":29456},{\"end\":30279,\"start\":29769},{\"end\":31869,\"start\":31820},{\"end\":31929,\"start\":31882},{\"end\":32115,\"start\":32084},{\"end\":32479,\"start\":32295},{\"end\":33941,\"start\":33834},{\"end\":34554,\"start\":34211},{\"end\":34906,\"start\":34745}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3290,\"start\":3282},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4512,\"start\":4504},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":5559,\"start\":5551},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8155,\"start\":8147},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":8545,\"start\":8537},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8898,\"start\":8890},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10697,\"start\":10689},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12259,\"start\":12251},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12745,\"start\":12736},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12833,\"start\":12825},{\"end\":13317,\"start\":13309},{\"end\":13792,\"start\":13784},{\"end\":13939,\"start\":13931},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":15213,\"start\":15205},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":18512,\"start\":18504},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":20929,\"start\":20921},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":22100,\"start\":22092},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22928,\"start\":22919},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23577,\"start\":23568},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26040,\"start\":26031}]", "bib_author_first_name": "[{\"end\":35675,\"start\":35669},{\"end\":35685,\"start\":35680},{\"end\":35697,\"start\":35690},{\"end\":35714,\"start\":35708},{\"end\":35722,\"start\":35719},{\"end\":35731,\"start\":35727},{\"end\":35741,\"start\":35736},{\"end\":35755,\"start\":35748},{\"end\":36066,\"start\":36061},{\"end\":36074,\"start\":36072},{\"end\":36086,\"start\":36081},{\"end\":36093,\"start\":36092},{\"end\":36107,\"start\":36101},{\"end\":36358,\"start\":36351},{\"end\":36370,\"start\":36365},{\"end\":36382,\"start\":36375},{\"end\":36398,\"start\":36390},{\"end\":36638,\"start\":36628},{\"end\":36649,\"start\":36645},{\"end\":36832,\"start\":36829},{\"end\":36848,\"start\":36839},{\"end\":36860,\"start\":36855},{\"end\":36869,\"start\":36867},{\"end\":36885,\"start\":36877},{\"end\":36897,\"start\":36890},{\"end\":36909,\"start\":36903},{\"end\":36921,\"start\":36916},{\"end\":36935,\"start\":36927},{\"end\":36946,\"start\":36941},{\"end\":37225,\"start\":37219},{\"end\":37239,\"start\":37235},{\"end\":37250,\"start\":37246},{\"end\":37463,\"start\":37455},{\"end\":37473,\"start\":37470},{\"end\":37487,\"start\":37479},{\"end\":37700,\"start\":37694},{\"end\":37719,\"start\":37711},{\"end\":37728,\"start\":37726},{\"end\":37740,\"start\":37734},{\"end\":37753,\"start\":37746},{\"end\":37767,\"start\":37759},{\"end\":37777,\"start\":37773},{\"end\":37793,\"start\":37785},{\"end\":38069,\"start\":38065},{\"end\":38081,\"start\":38074},{\"end\":38093,\"start\":38087},{\"end\":38107,\"start\":38100},{\"end\":38116,\"start\":38113},{\"end\":38131,\"start\":38124},{\"end\":38146,\"start\":38138},{\"end\":38158,\"start\":38152},{\"end\":38431,\"start\":38426},{\"end\":38446,\"start\":38438},{\"end\":38647,\"start\":38642},{\"end\":38661,\"start\":38655},{\"end\":38822,\"start\":38815},{\"end\":38831,\"start\":38828},{\"end\":38845,\"start\":38839},{\"end\":38858,\"start\":38850},{\"end\":38871,\"start\":38863},{\"end\":39093,\"start\":39091},{\"end\":39102,\"start\":39099},{\"end\":39258,\"start\":39251},{\"end\":39270,\"start\":39263},{\"end\":39459,\"start\":39454},{\"end\":39471,\"start\":39464},{\"end\":39487,\"start\":39479},{\"end\":39499,\"start\":39494},{\"end\":39665,\"start\":39656},{\"end\":39680,\"start\":39675},{\"end\":39695,\"start\":39688},{\"end\":40033,\"start\":40027},{\"end\":40049,\"start\":40043},{\"end\":40067,\"start\":40058},{\"end\":40083,\"start\":40078},{\"end\":40101,\"start\":40094},{\"end\":40445,\"start\":40438},{\"end\":40470,\"start\":40464},{\"end\":40488,\"start\":40481},{\"end\":40800,\"start\":40794},{\"end\":40814,\"start\":40806},{\"end\":40822,\"start\":40820},{\"end\":40836,\"start\":40828},{\"end\":41104,\"start\":41100},{\"end\":41117,\"start\":41112},{\"end\":41128,\"start\":41124},{\"end\":41142,\"start\":41134},{\"end\":41157,\"start\":41148},{\"end\":41171,\"start\":41164},{\"end\":41181,\"start\":41176},{\"end\":41192,\"start\":41186},{\"end\":41479,\"start\":41475},{\"end\":41493,\"start\":41485},{\"end\":41507,\"start\":41501},{\"end\":41523,\"start\":41516},{\"end\":41538,\"start\":41532},{\"end\":41558,\"start\":41552},{\"end\":41877,\"start\":41869},{\"end\":41889,\"start\":41885},{\"end\":41906,\"start\":41899},{\"end\":41924,\"start\":41917},{\"end\":42206,\"start\":42203},{\"end\":42232,\"start\":42231},{\"end\":42247,\"start\":42241},{\"end\":42259,\"start\":42255},{\"end\":42527,\"start\":42520},{\"end\":42540,\"start\":42532},{\"end\":42549,\"start\":42547},{\"end\":42559,\"start\":42556},{\"end\":42573,\"start\":42565},{\"end\":42835,\"start\":42828},{\"end\":42847,\"start\":42840},{\"end\":42861,\"start\":42853},{\"end\":43096,\"start\":43093},{\"end\":43104,\"start\":43101},{\"end\":43115,\"start\":43111},{\"end\":43130,\"start\":43122},{\"end\":43394,\"start\":43389},{\"end\":43406,\"start\":43399},{\"end\":43420,\"start\":43417},{\"end\":43432,\"start\":43425},{\"end\":43445,\"start\":43439},{\"end\":43756,\"start\":43751},{\"end\":43768,\"start\":43761},{\"end\":43782,\"start\":43779},{\"end\":43794,\"start\":43787},{\"end\":43807,\"start\":43801},{\"end\":44067,\"start\":44062},{\"end\":44079,\"start\":44072},{\"end\":44093,\"start\":44090},{\"end\":44105,\"start\":44098},{\"end\":44117,\"start\":44112},{\"end\":44128,\"start\":44122},{\"end\":44389,\"start\":44384},{\"end\":44404,\"start\":44396},{\"end\":44411,\"start\":44410},{\"end\":44423,\"start\":44418},{\"end\":44440,\"start\":44432},{\"end\":44737,\"start\":44732},{\"end\":44749,\"start\":44744},{\"end\":44760,\"start\":44754},{\"end\":44773,\"start\":44765},{\"end\":45001,\"start\":44993},{\"end\":45014,\"start\":45007},{\"end\":45027,\"start\":45022},{\"end\":45043,\"start\":45038},{\"end\":45056,\"start\":45050},{\"end\":45069,\"start\":45065},{\"end\":45084,\"start\":45079},{\"end\":45103,\"start\":45093},{\"end\":45338,\"start\":45335},{\"end\":45347,\"start\":45344},{\"end\":45358,\"start\":45352},{\"end\":45367,\"start\":45363},{\"end\":45378,\"start\":45375},{\"end\":45590,\"start\":45585},{\"end\":45599,\"start\":45595},{\"end\":45612,\"start\":45606},{\"end\":45853,\"start\":45847},{\"end\":45869,\"start\":45863},{\"end\":45880,\"start\":45874},{\"end\":45890,\"start\":45887},{\"end\":46145,\"start\":46138},{\"end\":46165,\"start\":46156},{\"end\":46180,\"start\":46175},{\"end\":46197,\"start\":46188},{\"end\":46223,\"start\":46216},{\"end\":46240,\"start\":46233},{\"end\":46538,\"start\":46531},{\"end\":46558,\"start\":46549},{\"end\":46575,\"start\":46568},{\"end\":46793,\"start\":46787},{\"end\":46812,\"start\":46803},{\"end\":46832,\"start\":46825},{\"end\":46834,\"start\":46833},{\"end\":47090,\"start\":47086},{\"end\":47102,\"start\":47099},{\"end\":47119,\"start\":47110},{\"end\":47131,\"start\":47127},{\"end\":47144,\"start\":47139},{\"end\":47162,\"start\":47155},{\"end\":47177,\"start\":47171},{\"end\":47193,\"start\":47187},{\"end\":47206,\"start\":47199},{\"end\":47223,\"start\":47219},{\"end\":47491,\"start\":47487},{\"end\":47508,\"start\":47505},{\"end\":47518,\"start\":47515},{\"end\":47531,\"start\":47523},{\"end\":47547,\"start\":47540},{\"end\":47562,\"start\":47558},{\"end\":47574,\"start\":47567},{\"end\":47588,\"start\":47582},{\"end\":47605,\"start\":47599},{\"end\":47621,\"start\":47614},{\"end\":47997,\"start\":47991},{\"end\":48014,\"start\":48006},{\"end\":48031,\"start\":48025},{\"end\":48043,\"start\":48037},{\"end\":48051,\"start\":48044},{\"end\":48065,\"start\":48058},{\"end\":48080,\"start\":48075},{\"end\":48082,\"start\":48081},{\"end\":48368,\"start\":48361},{\"end\":48384,\"start\":48378},{\"end\":48404,\"start\":48399},{\"end\":48642,\"start\":48641},{\"end\":48653,\"start\":48652},{\"end\":48665,\"start\":48664},{\"end\":48678,\"start\":48677},{\"end\":48688,\"start\":48687},{\"end\":48923,\"start\":48918},{\"end\":48925,\"start\":48924},{\"end\":48936,\"start\":48935},{\"end\":48954,\"start\":48953},{\"end\":48967,\"start\":48961},{\"end\":49286,\"start\":49279},{\"end\":49297,\"start\":49293},{\"end\":49306,\"start\":49304},{\"end\":49319,\"start\":49312},{\"end\":49335,\"start\":49328},{\"end\":49592,\"start\":49590},{\"end\":49601,\"start\":49598},{\"end\":49612,\"start\":49608},{\"end\":49626,\"start\":49618},{\"end\":49890,\"start\":49885},{\"end\":49901,\"start\":49896},{\"end\":49911,\"start\":49909},{\"end\":49920,\"start\":49918},{\"end\":49935,\"start\":49927},{\"end\":50213,\"start\":50208},{\"end\":50227,\"start\":50223},{\"end\":50251,\"start\":50234},{\"end\":50265,\"start\":50261},{\"end\":50540,\"start\":50534},{\"end\":50558,\"start\":50551},{\"end\":50574,\"start\":50569},{\"end\":50591,\"start\":50586},{\"end\":50607,\"start\":50600},{\"end\":50924,\"start\":50918},{\"end\":50942,\"start\":50935},{\"end\":50958,\"start\":50953},{\"end\":50975,\"start\":50970},{\"end\":50991,\"start\":50984},{\"end\":51285,\"start\":51277},{\"end\":51296,\"start\":51291},{\"end\":51309,\"start\":51304},{\"end\":51320,\"start\":51314},{\"end\":51548,\"start\":51542},{\"end\":51559,\"start\":51556},{\"end\":51574,\"start\":51566},{\"end\":51587,\"start\":51581},{\"end\":51602,\"start\":51596},{\"end\":51619,\"start\":51612},{\"end\":51957,\"start\":51952},{\"end\":51972,\"start\":51965},{\"end\":51990,\"start\":51983},{\"end\":52257,\"start\":52252},{\"end\":52263,\"start\":52258},{\"end\":52287,\"start\":52280},{\"end\":52299,\"start\":52293},{\"end\":52315,\"start\":52310},{\"end\":52331,\"start\":52326},{\"end\":52343,\"start\":52336},{\"end\":52668,\"start\":52665},{\"end\":52681,\"start\":52675},{\"end\":52693,\"start\":52688},{\"end\":52704,\"start\":52700},{\"end\":52716,\"start\":52709},{\"end\":52727,\"start\":52721},{\"end\":52742,\"start\":52734},{\"end\":52752,\"start\":52749},{\"end\":53014,\"start\":53009},{\"end\":53027,\"start\":53021},{\"end\":53041,\"start\":53033},{\"end\":53055,\"start\":53048},{\"end\":53292,\"start\":53289},{\"end\":53306,\"start\":53299},{\"end\":53591,\"start\":53588},{\"end\":53605,\"start\":53598},{\"end\":53617,\"start\":53612},{\"end\":53630,\"start\":53622},{\"end\":53642,\"start\":53636},{\"end\":53885,\"start\":53878},{\"end\":53900,\"start\":53892},{\"end\":53914,\"start\":53907},{\"end\":53928,\"start\":53920},{\"end\":54145,\"start\":54138},{\"end\":54159,\"start\":54151},{\"end\":54170,\"start\":54167},{\"end\":54178,\"start\":54176},{\"end\":54408,\"start\":54402},{\"end\":54427,\"start\":54415},{\"end\":54443,\"start\":54436},{\"end\":54678,\"start\":54672},{\"end\":54689,\"start\":54683},{\"end\":54699,\"start\":54694},{\"end\":54709,\"start\":54704},{\"end\":54986,\"start\":54984},{\"end\":54997,\"start\":54991},{\"end\":55009,\"start\":55003},{\"end\":55019,\"start\":55016},{\"end\":55030,\"start\":55025},{\"end\":55041,\"start\":55039},{\"end\":55334,\"start\":55328},{\"end\":55347,\"start\":55339},{\"end\":55360,\"start\":55355},{\"end\":55375,\"start\":55367},{\"end\":55388,\"start\":55382},{\"end\":55664,\"start\":55660},{\"end\":55674,\"start\":55671},{\"end\":55683,\"start\":55680},{\"end\":55952,\"start\":55949},{\"end\":55964,\"start\":55957},{\"end\":55980,\"start\":55975},{\"end\":55992,\"start\":55985},{\"end\":56007,\"start\":55999},{\"end\":56326,\"start\":56323},{\"end\":56338,\"start\":56331},{\"end\":56355,\"start\":56349},{\"end\":56366,\"start\":56361},{\"end\":56378,\"start\":56371},{\"end\":56393,\"start\":56385},{\"end\":56709,\"start\":56705},{\"end\":56722,\"start\":56716},{\"end\":56734,\"start\":56727},{\"end\":56884,\"start\":56880},{\"end\":56893,\"start\":56889},{\"end\":56907,\"start\":56899},{\"end\":56918,\"start\":56914},{\"end\":56920,\"start\":56919},{\"end\":57188,\"start\":57183},{\"end\":57200,\"start\":57193},{\"end\":57220,\"start\":57207},{\"end\":57234,\"start\":57228},{\"end\":57514,\"start\":57509},{\"end\":57526,\"start\":57519},{\"end\":57537,\"start\":57533},{\"end\":57550,\"start\":57544},{\"end\":57565,\"start\":57557},{\"end\":57847,\"start\":57841},{\"end\":57858,\"start\":57852},{\"end\":57869,\"start\":57863},{\"end\":57879,\"start\":57876},{\"end\":57892,\"start\":57886},{\"end\":57900,\"start\":57898},{\"end\":58193,\"start\":58188},{\"end\":58206,\"start\":58198},{\"end\":58218,\"start\":58212},{\"end\":58476,\"start\":58470},{\"end\":58490,\"start\":58484},{\"end\":58499,\"start\":58496},{\"end\":58507,\"start\":58504},{\"end\":58521,\"start\":58513},{\"end\":58801,\"start\":58797},{\"end\":58817,\"start\":58809},{\"end\":58827,\"start\":58822},{\"end\":58835,\"start\":58832},{\"end\":58849,\"start\":58843},{\"end\":59140,\"start\":59136},{\"end\":59151,\"start\":59148},{\"end\":59161,\"start\":59157},{\"end\":59173,\"start\":59167},{\"end\":59187,\"start\":59181},{\"end\":59197,\"start\":59193},{\"end\":59207,\"start\":59204},{\"end\":59218,\"start\":59215},{\"end\":59230,\"start\":59226},{\"end\":59531,\"start\":59527},{\"end\":59547,\"start\":59539},{\"end\":59560,\"start\":59555},{\"end\":59570,\"start\":59565},{\"end\":59782,\"start\":59776},{\"end\":59794,\"start\":59790},{\"end\":59803,\"start\":59801},{\"end\":59815,\"start\":59809},{\"end\":59831,\"start\":59823},{\"end\":59841,\"start\":59837},{\"end\":59853,\"start\":59847},{\"end\":60104,\"start\":60099},{\"end\":60115,\"start\":60112},{\"end\":60126,\"start\":60121},{\"end\":60140,\"start\":60132},{\"end\":60150,\"start\":60147},{\"end\":60163,\"start\":60155},{\"end\":60172,\"start\":60170},{\"end\":60414,\"start\":60409},{\"end\":60427,\"start\":60422},{\"end\":60436,\"start\":60434},{\"end\":60451,\"start\":60443},{\"end\":60466,\"start\":60458},{\"end\":60475,\"start\":60473},{\"end\":60643,\"start\":60638},{\"end\":60653,\"start\":60651},{\"end\":60669,\"start\":60660},{\"end\":60671,\"start\":60670},{\"end\":60979,\"start\":60972},{\"end\":60992,\"start\":60987},{\"end\":61002,\"start\":61000},{\"end\":61257,\"start\":61252},{\"end\":61267,\"start\":61263},{\"end\":61283,\"start\":61275},{\"end\":61293,\"start\":61290},{\"end\":61304,\"start\":61298},{\"end\":61317,\"start\":61312},{\"end\":61331,\"start\":61324},{\"end\":61341,\"start\":61337},{\"end\":61353,\"start\":61348},{\"end\":61364,\"start\":61358},{\"end\":61372,\"start\":61369}]", "bib_author_last_name": "[{\"end\":35678,\"start\":35676},{\"end\":35688,\"start\":35686},{\"end\":35706,\"start\":35698},{\"end\":35717,\"start\":35715},{\"end\":35725,\"start\":35723},{\"end\":35734,\"start\":35732},{\"end\":35746,\"start\":35742},{\"end\":35760,\"start\":35756},{\"end\":36070,\"start\":36067},{\"end\":36079,\"start\":36075},{\"end\":36090,\"start\":36087},{\"end\":36099,\"start\":36094},{\"end\":36114,\"start\":36108},{\"end\":36125,\"start\":36116},{\"end\":36363,\"start\":36359},{\"end\":36373,\"start\":36371},{\"end\":36388,\"start\":36383},{\"end\":36403,\"start\":36399},{\"end\":36643,\"start\":36639},{\"end\":36657,\"start\":36650},{\"end\":36837,\"start\":36833},{\"end\":36853,\"start\":36849},{\"end\":36865,\"start\":36861},{\"end\":36875,\"start\":36870},{\"end\":36888,\"start\":36886},{\"end\":36901,\"start\":36898},{\"end\":36914,\"start\":36910},{\"end\":36925,\"start\":36922},{\"end\":36939,\"start\":36936},{\"end\":36953,\"start\":36947},{\"end\":37233,\"start\":37226},{\"end\":37244,\"start\":37240},{\"end\":37260,\"start\":37251},{\"end\":37468,\"start\":37464},{\"end\":37477,\"start\":37474},{\"end\":37497,\"start\":37488},{\"end\":37709,\"start\":37701},{\"end\":37724,\"start\":37720},{\"end\":37732,\"start\":37729},{\"end\":37744,\"start\":37741},{\"end\":37757,\"start\":37754},{\"end\":37771,\"start\":37768},{\"end\":37783,\"start\":37778},{\"end\":37796,\"start\":37794},{\"end\":37800,\"start\":37798},{\"end\":38072,\"start\":38070},{\"end\":38085,\"start\":38082},{\"end\":38098,\"start\":38094},{\"end\":38111,\"start\":38108},{\"end\":38122,\"start\":38117},{\"end\":38136,\"start\":38132},{\"end\":38150,\"start\":38147},{\"end\":38164,\"start\":38159},{\"end\":38436,\"start\":38432},{\"end\":38450,\"start\":38447},{\"end\":38456,\"start\":38452},{\"end\":38653,\"start\":38648},{\"end\":38665,\"start\":38662},{\"end\":38826,\"start\":38823},{\"end\":38837,\"start\":38832},{\"end\":38848,\"start\":38846},{\"end\":38861,\"start\":38859},{\"end\":38875,\"start\":38872},{\"end\":39097,\"start\":39094},{\"end\":39108,\"start\":39103},{\"end\":39261,\"start\":39259},{\"end\":39279,\"start\":39271},{\"end\":39462,\"start\":39460},{\"end\":39477,\"start\":39472},{\"end\":39492,\"start\":39488},{\"end\":39504,\"start\":39500},{\"end\":39673,\"start\":39666},{\"end\":39686,\"start\":39681},{\"end\":39701,\"start\":39696},{\"end\":40041,\"start\":40034},{\"end\":40056,\"start\":40050},{\"end\":40076,\"start\":40068},{\"end\":40092,\"start\":40084},{\"end\":40108,\"start\":40102},{\"end\":40462,\"start\":40446},{\"end\":40479,\"start\":40471},{\"end\":40493,\"start\":40489},{\"end\":40499,\"start\":40495},{\"end\":40804,\"start\":40801},{\"end\":40818,\"start\":40815},{\"end\":40826,\"start\":40823},{\"end\":40842,\"start\":40837},{\"end\":41110,\"start\":41105},{\"end\":41122,\"start\":41118},{\"end\":41132,\"start\":41129},{\"end\":41146,\"start\":41143},{\"end\":41162,\"start\":41158},{\"end\":41174,\"start\":41172},{\"end\":41184,\"start\":41182},{\"end\":41198,\"start\":41193},{\"end\":41483,\"start\":41480},{\"end\":41499,\"start\":41494},{\"end\":41514,\"start\":41508},{\"end\":41530,\"start\":41524},{\"end\":41550,\"start\":41539},{\"end\":41563,\"start\":41559},{\"end\":41883,\"start\":41878},{\"end\":41897,\"start\":41890},{\"end\":41915,\"start\":41907},{\"end\":41929,\"start\":41925},{\"end\":42229,\"start\":42207},{\"end\":42239,\"start\":42233},{\"end\":42253,\"start\":42248},{\"end\":42266,\"start\":42260},{\"end\":42276,\"start\":42268},{\"end\":42530,\"start\":42528},{\"end\":42545,\"start\":42541},{\"end\":42554,\"start\":42550},{\"end\":42563,\"start\":42560},{\"end\":42579,\"start\":42574},{\"end\":42838,\"start\":42836},{\"end\":42851,\"start\":42848},{\"end\":42866,\"start\":42862},{\"end\":43099,\"start\":43097},{\"end\":43109,\"start\":43105},{\"end\":43120,\"start\":43116},{\"end\":43135,\"start\":43131},{\"end\":43397,\"start\":43395},{\"end\":43415,\"start\":43407},{\"end\":43423,\"start\":43421},{\"end\":43437,\"start\":43433},{\"end\":43449,\"start\":43446},{\"end\":43759,\"start\":43757},{\"end\":43777,\"start\":43769},{\"end\":43785,\"start\":43783},{\"end\":43799,\"start\":43795},{\"end\":43811,\"start\":43808},{\"end\":44070,\"start\":44068},{\"end\":44088,\"start\":44080},{\"end\":44096,\"start\":44094},{\"end\":44110,\"start\":44106},{\"end\":44120,\"start\":44118},{\"end\":44132,\"start\":44129},{\"end\":44394,\"start\":44390},{\"end\":44408,\"start\":44405},{\"end\":44416,\"start\":44412},{\"end\":44430,\"start\":44424},{\"end\":44443,\"start\":44441},{\"end\":44450,\"start\":44445},{\"end\":44742,\"start\":44738},{\"end\":44752,\"start\":44750},{\"end\":44763,\"start\":44761},{\"end\":44779,\"start\":44774},{\"end\":45005,\"start\":45002},{\"end\":45020,\"start\":45015},{\"end\":45036,\"start\":45028},{\"end\":45048,\"start\":45044},{\"end\":45063,\"start\":45057},{\"end\":45077,\"start\":45070},{\"end\":45091,\"start\":45085},{\"end\":45111,\"start\":45104},{\"end\":45342,\"start\":45339},{\"end\":45350,\"start\":45348},{\"end\":45361,\"start\":45359},{\"end\":45373,\"start\":45368},{\"end\":45382,\"start\":45379},{\"end\":45593,\"start\":45591},{\"end\":45604,\"start\":45600},{\"end\":45619,\"start\":45613},{\"end\":45861,\"start\":45854},{\"end\":45872,\"start\":45870},{\"end\":45885,\"start\":45881},{\"end\":45894,\"start\":45891},{\"end\":45901,\"start\":45896},{\"end\":46154,\"start\":46146},{\"end\":46173,\"start\":46166},{\"end\":46186,\"start\":46181},{\"end\":46214,\"start\":46198},{\"end\":46231,\"start\":46224},{\"end\":46245,\"start\":46241},{\"end\":46251,\"start\":46247},{\"end\":46547,\"start\":46539},{\"end\":46566,\"start\":46559},{\"end\":46580,\"start\":46576},{\"end\":46801,\"start\":46794},{\"end\":46823,\"start\":46813},{\"end\":46845,\"start\":46835},{\"end\":47097,\"start\":47091},{\"end\":47108,\"start\":47103},{\"end\":47125,\"start\":47120},{\"end\":47137,\"start\":47132},{\"end\":47153,\"start\":47145},{\"end\":47169,\"start\":47163},{\"end\":47185,\"start\":47178},{\"end\":47197,\"start\":47194},{\"end\":47217,\"start\":47207},{\"end\":47230,\"start\":47224},{\"end\":47503,\"start\":47492},{\"end\":47513,\"start\":47509},{\"end\":47521,\"start\":47519},{\"end\":47538,\"start\":47532},{\"end\":47556,\"start\":47548},{\"end\":47565,\"start\":47563},{\"end\":47580,\"start\":47575},{\"end\":47597,\"start\":47589},{\"end\":47612,\"start\":47606},{\"end\":47631,\"start\":47622},{\"end\":48004,\"start\":47998},{\"end\":48023,\"start\":48015},{\"end\":48035,\"start\":48032},{\"end\":48056,\"start\":48052},{\"end\":48073,\"start\":48066},{\"end\":48089,\"start\":48083},{\"end\":48376,\"start\":48369},{\"end\":48397,\"start\":48385},{\"end\":48412,\"start\":48405},{\"end\":48650,\"start\":48643},{\"end\":48662,\"start\":48654},{\"end\":48675,\"start\":48666},{\"end\":48685,\"start\":48679},{\"end\":48693,\"start\":48689},{\"end\":48933,\"start\":48926},{\"end\":48944,\"start\":48937},{\"end\":48951,\"start\":48946},{\"end\":48959,\"start\":48955},{\"end\":48973,\"start\":48968},{\"end\":48981,\"start\":48975},{\"end\":49291,\"start\":49287},{\"end\":49302,\"start\":49298},{\"end\":49310,\"start\":49307},{\"end\":49326,\"start\":49320},{\"end\":49339,\"start\":49336},{\"end\":49596,\"start\":49593},{\"end\":49606,\"start\":49602},{\"end\":49616,\"start\":49613},{\"end\":49631,\"start\":49627},{\"end\":49894,\"start\":49891},{\"end\":49907,\"start\":49902},{\"end\":49916,\"start\":49912},{\"end\":49925,\"start\":49921},{\"end\":49940,\"start\":49936},{\"end\":50221,\"start\":50214},{\"end\":50232,\"start\":50228},{\"end\":50259,\"start\":50252},{\"end\":50270,\"start\":50266},{\"end\":50549,\"start\":50541},{\"end\":50567,\"start\":50559},{\"end\":50584,\"start\":50575},{\"end\":50598,\"start\":50592},{\"end\":50612,\"start\":50608},{\"end\":50933,\"start\":50925},{\"end\":50951,\"start\":50943},{\"end\":50968,\"start\":50959},{\"end\":50982,\"start\":50976},{\"end\":50996,\"start\":50992},{\"end\":51289,\"start\":51286},{\"end\":51302,\"start\":51297},{\"end\":51312,\"start\":51310},{\"end\":51324,\"start\":51321},{\"end\":51554,\"start\":51549},{\"end\":51564,\"start\":51560},{\"end\":51579,\"start\":51575},{\"end\":51594,\"start\":51588},{\"end\":51610,\"start\":51603},{\"end\":51626,\"start\":51620},{\"end\":51963,\"start\":51958},{\"end\":51981,\"start\":51973},{\"end\":51995,\"start\":51991},{\"end\":52278,\"start\":52264},{\"end\":52291,\"start\":52288},{\"end\":52308,\"start\":52300},{\"end\":52324,\"start\":52316},{\"end\":52334,\"start\":52332},{\"end\":52353,\"start\":52344},{\"end\":52359,\"start\":52355},{\"end\":52673,\"start\":52669},{\"end\":52686,\"start\":52682},{\"end\":52698,\"start\":52694},{\"end\":52707,\"start\":52705},{\"end\":52719,\"start\":52717},{\"end\":52732,\"start\":52728},{\"end\":52747,\"start\":52743},{\"end\":52756,\"start\":52753},{\"end\":53019,\"start\":53015},{\"end\":53031,\"start\":53028},{\"end\":53046,\"start\":53042},{\"end\":53058,\"start\":53056},{\"end\":53297,\"start\":53293},{\"end\":53311,\"start\":53307},{\"end\":53596,\"start\":53592},{\"end\":53610,\"start\":53606},{\"end\":53620,\"start\":53618},{\"end\":53634,\"start\":53631},{\"end\":53648,\"start\":53643},{\"end\":53890,\"start\":53886},{\"end\":53905,\"start\":53901},{\"end\":53918,\"start\":53915},{\"end\":53933,\"start\":53929},{\"end\":54149,\"start\":54146},{\"end\":54165,\"start\":54160},{\"end\":54174,\"start\":54171},{\"end\":54183,\"start\":54179},{\"end\":54413,\"start\":54409},{\"end\":54434,\"start\":54428},{\"end\":54450,\"start\":54444},{\"end\":54681,\"start\":54679},{\"end\":54692,\"start\":54690},{\"end\":54702,\"start\":54700},{\"end\":54713,\"start\":54710},{\"end\":54721,\"start\":54715},{\"end\":54989,\"start\":54987},{\"end\":55001,\"start\":54998},{\"end\":55014,\"start\":55010},{\"end\":55023,\"start\":55020},{\"end\":55037,\"start\":55031},{\"end\":55046,\"start\":55042},{\"end\":55337,\"start\":55335},{\"end\":55353,\"start\":55348},{\"end\":55365,\"start\":55361},{\"end\":55380,\"start\":55376},{\"end\":55392,\"start\":55389},{\"end\":55669,\"start\":55665},{\"end\":55678,\"start\":55675},{\"end\":55689,\"start\":55684},{\"end\":55955,\"start\":55953},{\"end\":55973,\"start\":55965},{\"end\":55983,\"start\":55981},{\"end\":55997,\"start\":55993},{\"end\":56010,\"start\":56008},{\"end\":56329,\"start\":56327},{\"end\":56347,\"start\":56339},{\"end\":56359,\"start\":56356},{\"end\":56369,\"start\":56367},{\"end\":56383,\"start\":56379},{\"end\":56396,\"start\":56394},{\"end\":56714,\"start\":56710},{\"end\":56725,\"start\":56723},{\"end\":56740,\"start\":56735},{\"end\":56887,\"start\":56885},{\"end\":56897,\"start\":56894},{\"end\":56912,\"start\":56908},{\"end\":56923,\"start\":56921},{\"end\":57191,\"start\":57189},{\"end\":57205,\"start\":57201},{\"end\":57226,\"start\":57221},{\"end\":57238,\"start\":57235},{\"end\":57517,\"start\":57515},{\"end\":57531,\"start\":57527},{\"end\":57542,\"start\":57538},{\"end\":57555,\"start\":57551},{\"end\":57571,\"start\":57566},{\"end\":57850,\"start\":57848},{\"end\":57861,\"start\":57859},{\"end\":57874,\"start\":57870},{\"end\":57884,\"start\":57880},{\"end\":57896,\"start\":57893},{\"end\":57905,\"start\":57901},{\"end\":58196,\"start\":58194},{\"end\":58210,\"start\":58207},{\"end\":58222,\"start\":58219},{\"end\":58482,\"start\":58477},{\"end\":58494,\"start\":58491},{\"end\":58502,\"start\":58500},{\"end\":58511,\"start\":58508},{\"end\":58524,\"start\":58522},{\"end\":58807,\"start\":58802},{\"end\":58820,\"start\":58818},{\"end\":58830,\"start\":58828},{\"end\":58841,\"start\":58836},{\"end\":58853,\"start\":58850},{\"end\":59146,\"start\":59141},{\"end\":59155,\"start\":59152},{\"end\":59165,\"start\":59162},{\"end\":59179,\"start\":59174},{\"end\":59191,\"start\":59188},{\"end\":59202,\"start\":59198},{\"end\":59213,\"start\":59208},{\"end\":59224,\"start\":59219},{\"end\":59234,\"start\":59231},{\"end\":59537,\"start\":59532},{\"end\":59553,\"start\":59548},{\"end\":59563,\"start\":59561},{\"end\":59575,\"start\":59571},{\"end\":59788,\"start\":59783},{\"end\":59799,\"start\":59795},{\"end\":59807,\"start\":59804},{\"end\":59821,\"start\":59816},{\"end\":59835,\"start\":59832},{\"end\":59845,\"start\":59842},{\"end\":59858,\"start\":59854},{\"end\":60110,\"start\":60105},{\"end\":60119,\"start\":60116},{\"end\":60130,\"start\":60127},{\"end\":60145,\"start\":60141},{\"end\":60153,\"start\":60151},{\"end\":60168,\"start\":60164},{\"end\":60177,\"start\":60173},{\"end\":60420,\"start\":60415},{\"end\":60432,\"start\":60428},{\"end\":60441,\"start\":60437},{\"end\":60456,\"start\":60452},{\"end\":60471,\"start\":60467},{\"end\":60480,\"start\":60476},{\"end\":60649,\"start\":60644},{\"end\":60658,\"start\":60654},{\"end\":60681,\"start\":60672},{\"end\":60985,\"start\":60980},{\"end\":60998,\"start\":60993},{\"end\":61007,\"start\":61003},{\"end\":61261,\"start\":61258},{\"end\":61273,\"start\":61268},{\"end\":61288,\"start\":61284},{\"end\":61296,\"start\":61294},{\"end\":61310,\"start\":61305},{\"end\":61322,\"start\":61318},{\"end\":61335,\"start\":61332},{\"end\":61346,\"start\":61342},{\"end\":61356,\"start\":61354},{\"end\":61367,\"start\":61365},{\"end\":61377,\"start\":61373}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":222070432},\"end\":35996,\"start\":35560},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":216009},\"end\":36283,\"start\":35998},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":53424263},\"end\":36568,\"start\":36285},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":15972216},\"end\":36780,\"start\":36570},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":58981777},\"end\":37166,\"start\":36782},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2032491},\"end\":37388,\"start\":37168},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8923541},\"end\":37634,\"start\":37390},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":219634265},\"end\":38005,\"start\":37636},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4882356},\"end\":38351,\"start\":38007},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":220920024},\"end\":38600,\"start\":38353},{\"attributes\":{\"id\":\"b10\"},\"end\":38742,\"start\":38602},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2908606},\"end\":39041,\"start\":38744},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":765267},\"end\":39204,\"start\":39043},{\"attributes\":{\"id\":\"b13\"},\"end\":39380,\"start\":39206},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":50771534},\"end\":39654,\"start\":39382},{\"attributes\":{\"doi\":\"arXiv:1703.07737\",\"id\":\"b15\"},\"end\":39920,\"start\":39656},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17445139},\"end\":40328,\"start\":39922},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4649104},\"end\":40696,\"start\":40330},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":222245729},\"end\":41021,\"start\":40698},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":209050760},\"end\":41403,\"start\":41023},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3759573},\"end\":41753,\"start\":41405},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":4646924},\"end\":42135,\"start\":41755},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":7811489},\"end\":42444,\"start\":42137},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":201646268},\"end\":42749,\"start\":42446},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":52181345},\"end\":43016,\"start\":42751},{\"attributes\":{\"id\":\"b25\"},\"end\":43285,\"start\":43018},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":149607925},\"end\":43639,\"start\":43287},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219618471},\"end\":44019,\"start\":43641},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":229383318},\"end\":44279,\"start\":44021},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":22095661},\"end\":44650,\"start\":44281},{\"attributes\":{\"id\":\"b30\"},\"end\":44948,\"start\":44652},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14113767},\"end\":45302,\"start\":44950},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":26167987},\"end\":45490,\"start\":45304},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":10351844},\"end\":45774,\"start\":45492},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":85517477},\"end\":46076,\"start\":45776},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4633086},\"end\":46452,\"start\":46078},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4677070},\"end\":46738,\"start\":46454},{\"attributes\":{\"id\":\"b37\"},\"end\":47014,\"start\":46740},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":202786778},\"end\":47483,\"start\":47016},{\"attributes\":{\"id\":\"b39\"},\"end\":47907,\"start\":47485},{\"attributes\":{\"id\":\"b40\"},\"end\":48293,\"start\":47909},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":206592766},\"end\":48561,\"start\":48295},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":12632343},\"end\":48869,\"start\":48563},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":45064783},\"end\":49195,\"start\":48871},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":19225072},\"end\":49516,\"start\":49197},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":67856425},\"end\":49781,\"start\":49518},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":10013306},\"end\":50131,\"start\":49783},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2814088},\"end\":50434,\"start\":50133},{\"attributes\":{\"id\":\"b48\"},\"end\":50812,\"start\":50436},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":3431445},\"end\":51214,\"start\":50814},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":14165952},\"end\":51464,\"start\":51216},{\"attributes\":{\"doi\":\"arXiv:2101.11228\",\"id\":\"b51\"},\"end\":51876,\"start\":51466},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4665276},\"end\":52146,\"start\":51878},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":44159390},\"end\":52602,\"start\":52148},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":68589},\"end\":52938,\"start\":52604},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":13874338},\"end\":53208,\"start\":52940},{\"attributes\":{\"doi\":\"arXiv:1911.10692\",\"id\":\"b56\"},\"end\":53491,\"start\":53210},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":198968250},\"end\":53833,\"start\":53493},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":3385678},\"end\":54066,\"start\":53835},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":6258614},\"end\":54332,\"start\":54068},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":2489166},\"end\":54596,\"start\":54334},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":211264916},\"end\":54883,\"start\":54598},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":49225171},\"end\":55242,\"start\":54885},{\"attributes\":{\"id\":\"b63\"},\"end\":55573,\"start\":55244},{\"attributes\":{\"doi\":\"arXiv:1710.00478\",\"id\":\"b64\"},\"end\":55856,\"start\":55575},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":221821793},\"end\":56200,\"start\":55858},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":8207532},\"end\":56622,\"start\":56202},{\"attributes\":{\"id\":\"b67\"},\"end\":56878,\"start\":56624},{\"attributes\":{\"doi\":\"arXiv:1411.7923\",\"id\":\"b68\"},\"end\":57099,\"start\":56880},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":32781194},\"end\":57427,\"start\":57101},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":43107221},\"end\":57763,\"start\":57429},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":218673875},\"end\":58080,\"start\":57765},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":1815453},\"end\":58399,\"start\":58082},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":195439889},\"end\":58679,\"start\":58401},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":226742864},\"end\":59057,\"start\":58681},{\"attributes\":{\"doi\":\"arXiv:1711.08184\",\"id\":\"b75\"},\"end\":59461,\"start\":59059},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":195894214},\"end\":59715,\"start\":59463},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":115147653},\"end\":60031,\"start\":59717},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":2214158},\"end\":60359,\"start\":60033},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":14991802},\"end\":60636,\"start\":60361},{\"attributes\":{\"doi\":\"arXiv:1610.02984\",\"id\":\"b80\"},\"end\":60879,\"start\":60638},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":2683207},\"end\":61165,\"start\":60881},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":232148002},\"end\":61621,\"start\":61167}]", "bib_title": "[{\"end\":35667,\"start\":35560},{\"end\":36059,\"start\":35998},{\"end\":36349,\"start\":36285},{\"end\":36626,\"start\":36570},{\"end\":36827,\"start\":36782},{\"end\":37217,\"start\":37168},{\"end\":37453,\"start\":37390},{\"end\":37692,\"start\":37636},{\"end\":38063,\"start\":38007},{\"end\":38424,\"start\":38353},{\"end\":38813,\"start\":38744},{\"end\":39089,\"start\":39043},{\"end\":39249,\"start\":39206},{\"end\":39452,\"start\":39382},{\"end\":40025,\"start\":39922},{\"end\":40436,\"start\":40330},{\"end\":40792,\"start\":40698},{\"end\":41098,\"start\":41023},{\"end\":41473,\"start\":41405},{\"end\":41867,\"start\":41755},{\"end\":42201,\"start\":42137},{\"end\":42518,\"start\":42446},{\"end\":42826,\"start\":42751},{\"end\":43091,\"start\":43018},{\"end\":43387,\"start\":43287},{\"end\":43749,\"start\":43641},{\"end\":44060,\"start\":44021},{\"end\":44382,\"start\":44281},{\"end\":44991,\"start\":44950},{\"end\":45333,\"start\":45304},{\"end\":45583,\"start\":45492},{\"end\":45845,\"start\":45776},{\"end\":46136,\"start\":46078},{\"end\":46529,\"start\":46454},{\"end\":46785,\"start\":46740},{\"end\":47084,\"start\":47016},{\"end\":48359,\"start\":48295},{\"end\":48639,\"start\":48563},{\"end\":48916,\"start\":48871},{\"end\":49277,\"start\":49197},{\"end\":49588,\"start\":49518},{\"end\":49883,\"start\":49783},{\"end\":50206,\"start\":50133},{\"end\":50916,\"start\":50814},{\"end\":51275,\"start\":51216},{\"end\":51950,\"start\":51878},{\"end\":52250,\"start\":52148},{\"end\":52663,\"start\":52604},{\"end\":53007,\"start\":52940},{\"end\":53586,\"start\":53493},{\"end\":53876,\"start\":53835},{\"end\":54136,\"start\":54068},{\"end\":54400,\"start\":54334},{\"end\":54670,\"start\":54598},{\"end\":54982,\"start\":54885},{\"end\":55947,\"start\":55858},{\"end\":56321,\"start\":56202},{\"end\":57181,\"start\":57101},{\"end\":57507,\"start\":57429},{\"end\":57839,\"start\":57765},{\"end\":58186,\"start\":58082},{\"end\":58468,\"start\":58401},{\"end\":58795,\"start\":58681},{\"end\":59525,\"start\":59463},{\"end\":59774,\"start\":59717},{\"end\":60097,\"start\":60033},{\"end\":60407,\"start\":60361},{\"end\":60970,\"start\":60881},{\"end\":61250,\"start\":61167}]", "bib_author": "[{\"end\":35680,\"start\":35669},{\"end\":35690,\"start\":35680},{\"end\":35708,\"start\":35690},{\"end\":35719,\"start\":35708},{\"end\":35727,\"start\":35719},{\"end\":35736,\"start\":35727},{\"end\":35748,\"start\":35736},{\"end\":35762,\"start\":35748},{\"end\":36072,\"start\":36061},{\"end\":36081,\"start\":36072},{\"end\":36092,\"start\":36081},{\"end\":36101,\"start\":36092},{\"end\":36116,\"start\":36101},{\"end\":36127,\"start\":36116},{\"end\":36365,\"start\":36351},{\"end\":36375,\"start\":36365},{\"end\":36390,\"start\":36375},{\"end\":36405,\"start\":36390},{\"end\":36645,\"start\":36628},{\"end\":36659,\"start\":36645},{\"end\":36839,\"start\":36829},{\"end\":36855,\"start\":36839},{\"end\":36867,\"start\":36855},{\"end\":36877,\"start\":36867},{\"end\":36890,\"start\":36877},{\"end\":36903,\"start\":36890},{\"end\":36916,\"start\":36903},{\"end\":36927,\"start\":36916},{\"end\":36941,\"start\":36927},{\"end\":36955,\"start\":36941},{\"end\":37235,\"start\":37219},{\"end\":37246,\"start\":37235},{\"end\":37262,\"start\":37246},{\"end\":37470,\"start\":37455},{\"end\":37479,\"start\":37470},{\"end\":37499,\"start\":37479},{\"end\":37711,\"start\":37694},{\"end\":37726,\"start\":37711},{\"end\":37734,\"start\":37726},{\"end\":37746,\"start\":37734},{\"end\":37759,\"start\":37746},{\"end\":37773,\"start\":37759},{\"end\":37785,\"start\":37773},{\"end\":37798,\"start\":37785},{\"end\":37802,\"start\":37798},{\"end\":38074,\"start\":38065},{\"end\":38087,\"start\":38074},{\"end\":38100,\"start\":38087},{\"end\":38113,\"start\":38100},{\"end\":38124,\"start\":38113},{\"end\":38138,\"start\":38124},{\"end\":38152,\"start\":38138},{\"end\":38166,\"start\":38152},{\"end\":38438,\"start\":38426},{\"end\":38452,\"start\":38438},{\"end\":38458,\"start\":38452},{\"end\":38655,\"start\":38642},{\"end\":38667,\"start\":38655},{\"end\":38828,\"start\":38815},{\"end\":38839,\"start\":38828},{\"end\":38850,\"start\":38839},{\"end\":38863,\"start\":38850},{\"end\":38877,\"start\":38863},{\"end\":39099,\"start\":39091},{\"end\":39110,\"start\":39099},{\"end\":39263,\"start\":39251},{\"end\":39281,\"start\":39263},{\"end\":39464,\"start\":39454},{\"end\":39479,\"start\":39464},{\"end\":39494,\"start\":39479},{\"end\":39506,\"start\":39494},{\"end\":39675,\"start\":39656},{\"end\":39688,\"start\":39675},{\"end\":39703,\"start\":39688},{\"end\":40043,\"start\":40027},{\"end\":40058,\"start\":40043},{\"end\":40078,\"start\":40058},{\"end\":40094,\"start\":40078},{\"end\":40110,\"start\":40094},{\"end\":40464,\"start\":40438},{\"end\":40481,\"start\":40464},{\"end\":40495,\"start\":40481},{\"end\":40501,\"start\":40495},{\"end\":40806,\"start\":40794},{\"end\":40820,\"start\":40806},{\"end\":40828,\"start\":40820},{\"end\":40844,\"start\":40828},{\"end\":41112,\"start\":41100},{\"end\":41124,\"start\":41112},{\"end\":41134,\"start\":41124},{\"end\":41148,\"start\":41134},{\"end\":41164,\"start\":41148},{\"end\":41176,\"start\":41164},{\"end\":41186,\"start\":41176},{\"end\":41200,\"start\":41186},{\"end\":41485,\"start\":41475},{\"end\":41501,\"start\":41485},{\"end\":41516,\"start\":41501},{\"end\":41532,\"start\":41516},{\"end\":41552,\"start\":41532},{\"end\":41565,\"start\":41552},{\"end\":41885,\"start\":41869},{\"end\":41899,\"start\":41885},{\"end\":41917,\"start\":41899},{\"end\":41931,\"start\":41917},{\"end\":42231,\"start\":42203},{\"end\":42241,\"start\":42231},{\"end\":42255,\"start\":42241},{\"end\":42268,\"start\":42255},{\"end\":42278,\"start\":42268},{\"end\":42532,\"start\":42520},{\"end\":42547,\"start\":42532},{\"end\":42556,\"start\":42547},{\"end\":42565,\"start\":42556},{\"end\":42581,\"start\":42565},{\"end\":42840,\"start\":42828},{\"end\":42853,\"start\":42840},{\"end\":42868,\"start\":42853},{\"end\":43101,\"start\":43093},{\"end\":43111,\"start\":43101},{\"end\":43122,\"start\":43111},{\"end\":43137,\"start\":43122},{\"end\":43399,\"start\":43389},{\"end\":43417,\"start\":43399},{\"end\":43425,\"start\":43417},{\"end\":43439,\"start\":43425},{\"end\":43451,\"start\":43439},{\"end\":43761,\"start\":43751},{\"end\":43779,\"start\":43761},{\"end\":43787,\"start\":43779},{\"end\":43801,\"start\":43787},{\"end\":43813,\"start\":43801},{\"end\":44072,\"start\":44062},{\"end\":44090,\"start\":44072},{\"end\":44098,\"start\":44090},{\"end\":44112,\"start\":44098},{\"end\":44122,\"start\":44112},{\"end\":44134,\"start\":44122},{\"end\":44396,\"start\":44384},{\"end\":44410,\"start\":44396},{\"end\":44418,\"start\":44410},{\"end\":44432,\"start\":44418},{\"end\":44445,\"start\":44432},{\"end\":44452,\"start\":44445},{\"end\":44744,\"start\":44732},{\"end\":44754,\"start\":44744},{\"end\":44765,\"start\":44754},{\"end\":44781,\"start\":44765},{\"end\":45007,\"start\":44993},{\"end\":45022,\"start\":45007},{\"end\":45038,\"start\":45022},{\"end\":45050,\"start\":45038},{\"end\":45065,\"start\":45050},{\"end\":45079,\"start\":45065},{\"end\":45093,\"start\":45079},{\"end\":45113,\"start\":45093},{\"end\":45344,\"start\":45335},{\"end\":45352,\"start\":45344},{\"end\":45363,\"start\":45352},{\"end\":45375,\"start\":45363},{\"end\":45384,\"start\":45375},{\"end\":45595,\"start\":45585},{\"end\":45606,\"start\":45595},{\"end\":45621,\"start\":45606},{\"end\":45863,\"start\":45847},{\"end\":45874,\"start\":45863},{\"end\":45887,\"start\":45874},{\"end\":45896,\"start\":45887},{\"end\":45903,\"start\":45896},{\"end\":46156,\"start\":46138},{\"end\":46175,\"start\":46156},{\"end\":46188,\"start\":46175},{\"end\":46216,\"start\":46188},{\"end\":46233,\"start\":46216},{\"end\":46247,\"start\":46233},{\"end\":46253,\"start\":46247},{\"end\":46549,\"start\":46531},{\"end\":46568,\"start\":46549},{\"end\":46582,\"start\":46568},{\"end\":46803,\"start\":46787},{\"end\":46825,\"start\":46803},{\"end\":46847,\"start\":46825},{\"end\":47099,\"start\":47086},{\"end\":47110,\"start\":47099},{\"end\":47127,\"start\":47110},{\"end\":47139,\"start\":47127},{\"end\":47155,\"start\":47139},{\"end\":47171,\"start\":47155},{\"end\":47187,\"start\":47171},{\"end\":47199,\"start\":47187},{\"end\":47219,\"start\":47199},{\"end\":47232,\"start\":47219},{\"end\":47505,\"start\":47487},{\"end\":47515,\"start\":47505},{\"end\":47523,\"start\":47515},{\"end\":47540,\"start\":47523},{\"end\":47558,\"start\":47540},{\"end\":47567,\"start\":47558},{\"end\":47582,\"start\":47567},{\"end\":47599,\"start\":47582},{\"end\":47614,\"start\":47599},{\"end\":47633,\"start\":47614},{\"end\":48006,\"start\":47991},{\"end\":48025,\"start\":48006},{\"end\":48037,\"start\":48025},{\"end\":48058,\"start\":48037},{\"end\":48075,\"start\":48058},{\"end\":48091,\"start\":48075},{\"end\":48378,\"start\":48361},{\"end\":48399,\"start\":48378},{\"end\":48414,\"start\":48399},{\"end\":48652,\"start\":48641},{\"end\":48664,\"start\":48652},{\"end\":48677,\"start\":48664},{\"end\":48687,\"start\":48677},{\"end\":48695,\"start\":48687},{\"end\":48935,\"start\":48918},{\"end\":48946,\"start\":48935},{\"end\":48953,\"start\":48946},{\"end\":48961,\"start\":48953},{\"end\":48975,\"start\":48961},{\"end\":48983,\"start\":48975},{\"end\":49293,\"start\":49279},{\"end\":49304,\"start\":49293},{\"end\":49312,\"start\":49304},{\"end\":49328,\"start\":49312},{\"end\":49341,\"start\":49328},{\"end\":49598,\"start\":49590},{\"end\":49608,\"start\":49598},{\"end\":49618,\"start\":49608},{\"end\":49633,\"start\":49618},{\"end\":49896,\"start\":49885},{\"end\":49909,\"start\":49896},{\"end\":49918,\"start\":49909},{\"end\":49927,\"start\":49918},{\"end\":49942,\"start\":49927},{\"end\":50223,\"start\":50208},{\"end\":50234,\"start\":50223},{\"end\":50261,\"start\":50234},{\"end\":50272,\"start\":50261},{\"end\":50551,\"start\":50534},{\"end\":50569,\"start\":50551},{\"end\":50586,\"start\":50569},{\"end\":50600,\"start\":50586},{\"end\":50614,\"start\":50600},{\"end\":50935,\"start\":50918},{\"end\":50953,\"start\":50935},{\"end\":50970,\"start\":50953},{\"end\":50984,\"start\":50970},{\"end\":50998,\"start\":50984},{\"end\":51291,\"start\":51277},{\"end\":51304,\"start\":51291},{\"end\":51314,\"start\":51304},{\"end\":51326,\"start\":51314},{\"end\":51556,\"start\":51542},{\"end\":51566,\"start\":51556},{\"end\":51581,\"start\":51566},{\"end\":51596,\"start\":51581},{\"end\":51612,\"start\":51596},{\"end\":51628,\"start\":51612},{\"end\":51965,\"start\":51952},{\"end\":51983,\"start\":51965},{\"end\":51997,\"start\":51983},{\"end\":52280,\"start\":52252},{\"end\":52293,\"start\":52280},{\"end\":52310,\"start\":52293},{\"end\":52326,\"start\":52310},{\"end\":52336,\"start\":52326},{\"end\":52355,\"start\":52336},{\"end\":52361,\"start\":52355},{\"end\":52675,\"start\":52665},{\"end\":52688,\"start\":52675},{\"end\":52700,\"start\":52688},{\"end\":52709,\"start\":52700},{\"end\":52721,\"start\":52709},{\"end\":52734,\"start\":52721},{\"end\":52749,\"start\":52734},{\"end\":52758,\"start\":52749},{\"end\":53021,\"start\":53009},{\"end\":53033,\"start\":53021},{\"end\":53048,\"start\":53033},{\"end\":53060,\"start\":53048},{\"end\":53299,\"start\":53289},{\"end\":53313,\"start\":53299},{\"end\":53598,\"start\":53588},{\"end\":53612,\"start\":53598},{\"end\":53622,\"start\":53612},{\"end\":53636,\"start\":53622},{\"end\":53650,\"start\":53636},{\"end\":53892,\"start\":53878},{\"end\":53907,\"start\":53892},{\"end\":53920,\"start\":53907},{\"end\":53935,\"start\":53920},{\"end\":54151,\"start\":54138},{\"end\":54167,\"start\":54151},{\"end\":54176,\"start\":54167},{\"end\":54185,\"start\":54176},{\"end\":54415,\"start\":54402},{\"end\":54436,\"start\":54415},{\"end\":54452,\"start\":54436},{\"end\":54683,\"start\":54672},{\"end\":54694,\"start\":54683},{\"end\":54704,\"start\":54694},{\"end\":54715,\"start\":54704},{\"end\":54723,\"start\":54715},{\"end\":54991,\"start\":54984},{\"end\":55003,\"start\":54991},{\"end\":55016,\"start\":55003},{\"end\":55025,\"start\":55016},{\"end\":55039,\"start\":55025},{\"end\":55048,\"start\":55039},{\"end\":55339,\"start\":55328},{\"end\":55355,\"start\":55339},{\"end\":55367,\"start\":55355},{\"end\":55382,\"start\":55367},{\"end\":55394,\"start\":55382},{\"end\":55671,\"start\":55660},{\"end\":55680,\"start\":55671},{\"end\":55691,\"start\":55680},{\"end\":55957,\"start\":55949},{\"end\":55975,\"start\":55957},{\"end\":55985,\"start\":55975},{\"end\":55999,\"start\":55985},{\"end\":56012,\"start\":55999},{\"end\":56331,\"start\":56323},{\"end\":56349,\"start\":56331},{\"end\":56361,\"start\":56349},{\"end\":56371,\"start\":56361},{\"end\":56385,\"start\":56371},{\"end\":56398,\"start\":56385},{\"end\":56716,\"start\":56705},{\"end\":56727,\"start\":56716},{\"end\":56742,\"start\":56727},{\"end\":56889,\"start\":56880},{\"end\":56899,\"start\":56889},{\"end\":56914,\"start\":56899},{\"end\":56925,\"start\":56914},{\"end\":57193,\"start\":57183},{\"end\":57207,\"start\":57193},{\"end\":57228,\"start\":57207},{\"end\":57240,\"start\":57228},{\"end\":57519,\"start\":57509},{\"end\":57533,\"start\":57519},{\"end\":57544,\"start\":57533},{\"end\":57557,\"start\":57544},{\"end\":57573,\"start\":57557},{\"end\":57852,\"start\":57841},{\"end\":57863,\"start\":57852},{\"end\":57876,\"start\":57863},{\"end\":57886,\"start\":57876},{\"end\":57898,\"start\":57886},{\"end\":57907,\"start\":57898},{\"end\":58198,\"start\":58188},{\"end\":58212,\"start\":58198},{\"end\":58224,\"start\":58212},{\"end\":58484,\"start\":58470},{\"end\":58496,\"start\":58484},{\"end\":58504,\"start\":58496},{\"end\":58513,\"start\":58504},{\"end\":58526,\"start\":58513},{\"end\":58809,\"start\":58797},{\"end\":58822,\"start\":58809},{\"end\":58832,\"start\":58822},{\"end\":58843,\"start\":58832},{\"end\":58855,\"start\":58843},{\"end\":59148,\"start\":59136},{\"end\":59157,\"start\":59148},{\"end\":59167,\"start\":59157},{\"end\":59181,\"start\":59167},{\"end\":59193,\"start\":59181},{\"end\":59204,\"start\":59193},{\"end\":59215,\"start\":59204},{\"end\":59226,\"start\":59215},{\"end\":59236,\"start\":59226},{\"end\":59539,\"start\":59527},{\"end\":59555,\"start\":59539},{\"end\":59565,\"start\":59555},{\"end\":59577,\"start\":59565},{\"end\":59790,\"start\":59776},{\"end\":59801,\"start\":59790},{\"end\":59809,\"start\":59801},{\"end\":59823,\"start\":59809},{\"end\":59837,\"start\":59823},{\"end\":59847,\"start\":59837},{\"end\":59860,\"start\":59847},{\"end\":60112,\"start\":60099},{\"end\":60121,\"start\":60112},{\"end\":60132,\"start\":60121},{\"end\":60147,\"start\":60132},{\"end\":60155,\"start\":60147},{\"end\":60170,\"start\":60155},{\"end\":60179,\"start\":60170},{\"end\":60422,\"start\":60409},{\"end\":60434,\"start\":60422},{\"end\":60443,\"start\":60434},{\"end\":60458,\"start\":60443},{\"end\":60473,\"start\":60458},{\"end\":60482,\"start\":60473},{\"end\":60651,\"start\":60638},{\"end\":60660,\"start\":60651},{\"end\":60683,\"start\":60660},{\"end\":60987,\"start\":60972},{\"end\":61000,\"start\":60987},{\"end\":61009,\"start\":61000},{\"end\":61263,\"start\":61252},{\"end\":61275,\"start\":61263},{\"end\":61290,\"start\":61275},{\"end\":61298,\"start\":61290},{\"end\":61312,\"start\":61298},{\"end\":61324,\"start\":61312},{\"end\":61337,\"start\":61324},{\"end\":61348,\"start\":61337},{\"end\":61358,\"start\":61348},{\"end\":61369,\"start\":61358},{\"end\":61379,\"start\":61369}]", "bib_venue": "[{\"end\":35767,\"start\":35762},{\"end\":36132,\"start\":36127},{\"end\":36409,\"start\":36405},{\"end\":36663,\"start\":36659},{\"end\":36959,\"start\":36955},{\"end\":37268,\"start\":37262},{\"end\":37503,\"start\":37499},{\"end\":37806,\"start\":37802},{\"end\":38170,\"start\":38166},{\"end\":38462,\"start\":38458},{\"end\":38640,\"start\":38602},{\"end\":38881,\"start\":38877},{\"end\":39115,\"start\":39110},{\"end\":39285,\"start\":39281},{\"end\":39510,\"start\":39506},{\"end\":39775,\"start\":39719},{\"end\":40115,\"start\":40110},{\"end\":40503,\"start\":40501},{\"end\":40848,\"start\":40844},{\"end\":41204,\"start\":41200},{\"end\":41569,\"start\":41565},{\"end\":41935,\"start\":41931},{\"end\":42282,\"start\":42278},{\"end\":42585,\"start\":42581},{\"end\":42872,\"start\":42868},{\"end\":43141,\"start\":43137},{\"end\":43455,\"start\":43451},{\"end\":43817,\"start\":43813},{\"end\":44138,\"start\":44134},{\"end\":44456,\"start\":44452},{\"end\":44730,\"start\":44652},{\"end\":45117,\"start\":45113},{\"end\":45388,\"start\":45384},{\"end\":45625,\"start\":45621},{\"end\":45917,\"start\":45903},{\"end\":46256,\"start\":46253},{\"end\":46586,\"start\":46582},{\"end\":46868,\"start\":46847},{\"end\":47239,\"start\":47232},{\"end\":47989,\"start\":47909},{\"end\":48418,\"start\":48414},{\"end\":48698,\"start\":48695},{\"end\":49025,\"start\":48983},{\"end\":49345,\"start\":49341},{\"end\":49637,\"start\":49633},{\"end\":49949,\"start\":49942},{\"end\":50276,\"start\":50272},{\"end\":50532,\"start\":50436},{\"end\":51001,\"start\":50998},{\"end\":51330,\"start\":51326},{\"end\":51540,\"start\":51466},{\"end\":52001,\"start\":51997},{\"end\":52364,\"start\":52361},{\"end\":52762,\"start\":52758},{\"end\":53065,\"start\":53060},{\"end\":53287,\"start\":53210},{\"end\":53654,\"start\":53650},{\"end\":53939,\"start\":53935},{\"end\":54189,\"start\":54185},{\"end\":54456,\"start\":54452},{\"end\":54727,\"start\":54723},{\"end\":55052,\"start\":55048},{\"end\":55326,\"start\":55244},{\"end\":55658,\"start\":55575},{\"end\":56016,\"start\":56012},{\"end\":56401,\"start\":56398},{\"end\":56703,\"start\":56624},{\"end\":56981,\"start\":56940},{\"end\":57254,\"start\":57240},{\"end\":57587,\"start\":57573},{\"end\":57911,\"start\":57907},{\"end\":58228,\"start\":58224},{\"end\":58530,\"start\":58526},{\"end\":58858,\"start\":58855},{\"end\":59134,\"start\":59059},{\"end\":59580,\"start\":59577},{\"end\":59864,\"start\":59860},{\"end\":60183,\"start\":60179},{\"end\":60486,\"start\":60482},{\"end\":60749,\"start\":60699},{\"end\":61013,\"start\":61009},{\"end\":61383,\"start\":61379}]"}}}, "year": 2023, "month": 12, "day": 17}