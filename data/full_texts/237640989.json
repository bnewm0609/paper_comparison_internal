{"id": 237640989, "updated": "2023-10-13 04:47:45.751", "metadata": {"title": "Large steps in inverse rendering of geometry", "authors": "[{\"first\":\"Baptiste\",\"last\":\"Nicolet\",\"middle\":[]},{\"first\":\"Alec\",\"last\":\"Jacobson\",\"middle\":[]},{\"first\":\"Wenzel\",\"last\":\"Jakob\",\"middle\":[]}]", "venue": null, "journal": "ACM Transactions on Graphics (TOG)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Inverse reconstruction from images is a central problem in many scientific and engineering disciplines. Recent progress on differentiable rendering has led to methods that can efficiently differentiate the full process of image formation with respect to millions of parameters to solve such problems via gradient-based optimization. At the same time, the availability of cheap derivatives does not necessarily make an inverse problem easy to solve. Mesh-based representations remain a particular source of irritation: an adverse gradient step involving vertex positions could turn parts of the mesh inside-out, introduce numerous local self-intersections, or lead to inadequate usage of the vertex budget due to distortion. These types of issues are often irrecoverable in the sense that subsequent optimization steps will further exacerbate them. In other words, the optimization lacks robustness due to an objective function with substantial non-convexity. Such robustness issues are commonly mitigated by imposing additional regularization, typically in the form of Laplacian energies that quantify and improve the smoothness of the current iterate. However, regularization introduces its own set of problems: solutions must now compromise between solving the problem and being smooth. Furthermore, gradient steps involving a Laplacian energy resemble Jacobi's iterative method for solving linear equations that is known for its exceptionally slow convergence. We propose a simple and practical alternative that casts differentiable rendering into the framework of preconditioned gradient descent. Our pre-conditioner biases gradient steps towards smooth solutions without requiring the final solution to be smooth. In contrast to Jacobi-style iteration, each gradient step propagates information among all variables, enabling convergence using fewer and larger steps. Our method is not restricted to meshes and can also accelerate the reconstruction of other representations, where smooth solutions are generally expected. We demonstrate its superior performance in the context of geometric optimization and texture reconstruction.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tog/NicoletJJ21", "doi": "10.1145/3478513.3480501"}}, "content": {"source": {"pdf_hash": "c992ba073c0928182d6fed58cfffd53af613331f", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3478513.3480501", "status": "BRONZE"}}, "grobid": {"id": "f133189e0c097215b837677fc3de67c3c8f9bd90", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c992ba073c0928182d6fed58cfffd53af613331f.txt", "contents": "\nLarge Steps in Inverse Rendering of Geometry\n2021\n\nBaptiste Nicolet baptiste.nicolet@epfl.ch \n\u00c9cole Polytechnique \nEPFL)F\u00e9d\u00e9rale De Lausanne \nAlec Switzerland \nJacobson jacobson@cs.toronto.edu \nAlec Jacobson \nBaptiste Nicolet \nAlec Jacobson \nWenzel Jakob wenzel.jakob@epfl.ch \n\nUniversity of Toronto\nLausanneCanada, Switzerland\n\n\nWenzel Jakob, \u00c9cole Poly-technique F\u00e9d\u00e9rale de Lausanne (EPFL)\nUniversity of Toronto\nLausanneCanada, Switzerland\n\nLarge Steps in Inverse Rendering of Geometry\n\nACM Trans. Graph\n406202110.1145/3478513.3480501248. Publication date: December 2021.Authors' addresses: Baptiste Nicolet, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), 0730-0301/2021/12-ART248 $15.00 ACM Reference Format: 248:2 \u2022 Nicolet et al.CCS Concepts: \u2022 Computing methodologies \u2192 Rendering; Shape mod- eling Additional Key Words and Phrases: differentiable renderinggeometry re- constructionLaplacian mesh processing\nWENZEL JAKOB, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), Switzerland (a) Initial State (b) Na\u00efve (c) Regularized (d) Ours (e) Ours (with remeshing) ( ) Reference Self-intersection Fig. 1. (a) Inverse reconstruction of the Nefertiti bust from a spherical starting guess with 25 rendered views (1 shown).(b) Na\u00efve application of a differentiable renderer produces an unusable tangled mesh when gradient steps pull on the silhouette without regard for distortion or self-intersections. (c) Regularization can alleviate such problems by making the optimization aware of mesh quality. On the flipside, this penalizes non-smooth parts of the geometry and causes unsatisfactory convergence in gradient-based optimizers. While the final mesh undeniably looks better, a closer inspection of the wireframe rendering reveals countless self-intersections. (d) Our method addresses both problems and converges to a high-quality mesh. (e) Combined with an isotropic remeshing step, our reconstruction captures fine details of the reference (f). The hyper-parameters of each method were optimized to obtain the best convergence at equal time.Self-intersections are shown in red.Inverse reconstruction from images is a central problem in many scientific and engineering disciplines. Recent progress on differentiable rendering has led to methods that can efficiently differentiate the full process of image formation with respect to millions of parameters to solve such problems via gradient-based optimization. At the same time, the availability of cheap derivatives does not necessarily make an inverse problem easy to solve. Mesh-based representations remain a particular source of irritation: an adverse gradient step involving vertex positions could turn parts of the mesh inside-out, introduce numerous local self-intersections, or lead to inadequate usage of the vertex budget due to distortion. These types of issues are often irrecoverable in the sense that subsequent optimization steps will further exacerbate them. In other words, the optimization lacks robustness due to an objective function with substantial non-convexity.Such robustness issues are commonly mitigated by imposing additional regularization, typically in the form of Laplacian energies that quantify and improve the smoothness of the current iterate. However, regularization introduces its own set of problems: solutions must now compromise between solving the problem and being smooth. Furthermore, gradient steps involving a Laplacian energy resemble Jacobi's iterative method for solving linear equations that is known for its exceptionally slow convergence.We propose a simple and practical alternative that casts differentiable rendering into the framework of preconditioned gradient descent. Our preconditioner biases gradient steps towards smooth solutions without requiring the final solution to be smooth. In contrast to Jacobi-style iteration, each gradient step propagates information among all variables, enabling convergence using fewer and larger steps.Our method is not restricted to meshes and can also accelerate the reconstruction of other representations, where smooth solutions are generally expected. We demonstrate its superior performance in the context of geometric optimization and texture reconstruction.\n\n(a) Initial State (b) Na\u00efve (c) Regularized (d) Ours (e) Ours (with remeshing) ( ) Reference\n\nSelf-intersection Na\u00efve application of a differentiable renderer produces an unusable tangled mesh when gradient steps pull on the silhouette without regard for distortion or self-intersections. (c) Regularization can alleviate such problems by making the optimization aware of mesh quality. On the flipside, this penalizes non-smooth parts of the geometry and causes unsatisfactory convergence in gradient-based optimizers. While the final mesh undeniably looks better, a closer inspection of the wireframe rendering reveals countless self-intersections. (d) Our method addresses both problems and converges to a high-quality mesh. (e) Combined with an isotropic remeshing step, our reconstruction captures fine details of the reference (f). The hyper-parameters of each method were optimized to obtain the best convergence at equal time.\n\nSelf-intersections are shown in red.\n\nInverse reconstruction from images is a central problem in many scientific and engineering disciplines. Recent progress on differentiable rendering has led to methods that can efficiently differentiate the full process of image formation with respect to millions of parameters to solve such problems via gradient-based optimization. At the same time, the availability of cheap derivatives does not necessarily make an inverse problem easy to solve. Mesh-based representations remain a particular source of irritation: an adverse gradient step involving vertex positions could turn parts of the mesh inside-out, introduce numerous local self-intersections, or lead to inadequate usage of the vertex budget due to distortion. These types of issues are often irrecoverable in the sense that subsequent optimization steps will further exacerbate them. In other words, the optimization lacks robustness due to an objective function with substantial non-convexity.\n\nSuch robustness issues are commonly mitigated by imposing additional regularization, typically in the form of Laplacian energies that quantify and improve the smoothness of the current iterate. However, regularization introduces its own set of problems: solutions must now compromise between solving the problem and being smooth. Furthermore, gradient steps involving a Laplacian energy resemble Jacobi's iterative method for solving linear equations that is known for its exceptionally slow convergence.\n\nWe propose a simple and practical alternative that casts differentiable rendering into the framework of preconditioned gradient descent. Our preconditioner biases gradient steps towards smooth solutions without requiring the final solution to be smooth. In contrast to Jacobi-style iteration, each gradient step propagates information among all variables, enabling convergence using fewer and larger steps.\n\nOur method is not restricted to meshes and can also accelerate the reconstruction of other representations, where smooth solutions are generally expected. We demonstrate its superior performance in the context of geometric optimization and texture reconstruction.\n\n\nINTRODUCTION\n\nDifferentiable rendering is an emerging tool for solving challenging inverse problems involving light transport. Methods in this area propagate derivatives through the full process of image formation to minimize a user-provided objective function defined on a highdimensional space of scene parameters. The resulting derivatives encode the complex and ambiguous relationship of rendered pixels to light sources, the material, and the shape of observed objects. Physically based variants furthermore account for interreflection to compute derivatives due to indirectly observed objects. Compared to existing 3D reconstruction techniques, the main allure of differentiable rendering lies in its comparative lack of assumptions and potential to outperform standard methods in challenging situations.\n\nYet, the availability of derivatives is no panacea: gradient-based optimization of a non-convex objective can easily converge to local minima representing low-quality solutions. Notably, this situation almost always occurs when the scene contains mesh-based geometric representations. Optimizing a crude initial guess (e.g. a sphere) will necessarily require large-scale deformation, which manifests in the form of silhouette gradients that \"pull\" a sparse set of polygons into a target configuration, for example towards silhouettes observed in a reference photograph. A large gradient step could then turn part of the object inside-out, introduce numerous local self-intersections, or distort the triangulation so that the given vertex budget is not effectively used.\n\nWhen the optimization relies on a pure rendering loss, such issues are essentially irrecoverable: geometric distortion is generally invisible in renderings, and unobserved inverted geometry does not generate gradients at all! Since there is no inherent correction mechanism, later gradient steps are likely to compound existing issues. Seen in another way, this fragility indicates that the non-convex objective is simply not amenable to gradient-based optimization. Such issues are not restricted to sparse shape-related gradients: for example, noisy derivatives of Monte Carlo methods can cause many similar problems.\n\nA common way to mitigate such robustness issues is to make the optimization aware of the mesh quality, typically by imposing regularization in the form of Laplacian or bi-Laplacian smoothness energies. This certainly helps to stabilize the optimization, but it also introduces a new set of problems: solutions must now compromise between satisfying the original objective and being smooth. Regularization also requires a weighting factor, which adds a challenging hyper-parameter choice. In cases where the output contains both smooth and non-smooth regions, there may not be a good global setting for this parameter.\n\nA third issue appears when first-order gradients are used to optimize a discrete Laplacian energy. In this case, each variable generates derivative terms that only affect variables in a local neighborhood: the 1-ring in the case of common discrete Laplacian operators for meshes. Such local information exchange along edges has been studied in iterative methods for solving sparse linear systems including the Jacobi and Gauss-Seidel methods. Both are known for their exceptionally slow convergence precisely due to this inherent locality.\n\nWe propose a simple and practical alternative to Laplacian regularization for differentiable rendering that is more robust, less sensitive to hyperparameter choices, and which accelerates convergence at equal time. Our method can be alternatively interpreted as casting differentiable rendering into the framework of Sobolev preconditioned gradient descent or as a re-parameterization of the input geometry resembling differential coordinates [Sorkine 2006]. Our preconditioned optimization biases gradient steps towards smooth solutions, but it does so without requiring the final solution to be smooth. In contrast to Jacobi-style iteration, each gradient step propagates information among all variables, enabling convergence using fewer and larger steps.\n\nOur method solves a sparse linear system at every iteration, which has negligible cost compared to the primal and differential rendering phases. It works on any domain with a suitable discrete Laplacian operator, and we experimentally evaluate its performance in the context of geometric optimization and texture reconstruction.\n\nThe name of our submission is inspired by an influential article by Baraff and Witkin [1998] that pioneered the use of implicit timesteps to improve the robustness of cloth simulation. Our method admits a similar interpretation as an implicit timestep of a diffusion process.\n\n\nBACKGROUND AND RELATED WORK\n\nWe now discuss relevant prior work and review fundamentals concerning the Laplacian and its applications.\n\n\nDifferentiable Rendering\n\nRendering is increasingly used as a sub-component within methods that rely on optimization to accomplish a particular task. For example, methods in the analysis-by-synthesis category [Patow and Pueyo 2003] reconstruct the shape and appearance of objects from images, using rendered candidate images to update a virtual scene representation. Embedded within a neural encoder-decoder architecture, a rendering step can convert latent scene representations into images that are consumed by convolutional layers [Genova et al. 2018]. The scene could itself be represented by a neural network that is queried many times during the rendering process [Mildenhall et al. 2020]. All of these applications involve high-dimensional parameter domains requiring first-order optimization, which has led to renewed interest in rendering methods with an explicit differentiation operation.\n\nSuppose that a rendering algorithm is represented as a function that maps an input vector of scene parameters x \u2208 R to a rendered image y \u2208 R . If accounts for global illumination, an individual scene parameter can often affect the entire image, hence the Jacobian matrix J = x \u2208 R \u00d7 is generally dense. Both of and could be in the millions, making explicit computation or storage of this matrix infeasible. Instead, differentiable rendering algorithms realize efficient matrix-vector products involving this matrix without ever constructing it. Typically, reverse-mode propagation [Griewank and Walther 2008] is desired, which converts an image-space derivative y into a parameter derivative x via the product = J y. Work on differentiable rendering algorithms falls into two broad categories: starting with the early of work of Loper et al. [2014] rasterization-based methods account for primary visibility and local shading [Kato et al. 2018;Liu et al. 2019;Laine et al. 2020]. They achieve excellent performance but do not support indirect effect like shadows and interreflection.\n\n\nPhysically based methods\n\nAnother line of work targets derivatives of physically based light simulations [Gkioulekas et al. 2016;Li et al. 2018;Zhang et al. 2019;Nimier-David et al. 2019]. Efficient differentiation schemes in this area exploit the physical reciprocity of light along with reversible steps of the computation [Nimier-David et al. 2020;Vicini et al. 2021]. Visibility-induced discontinuities pose a challenge in physicallybased methods and require special treatment to avoid bias [Li et al. 2018;Loubet et al. 2019;Bangaru et al. 2020;Zhang et al. 2020]. Finally, integrals evaluated within rendering algorithms change following differentiation, and recent work [Zeltner et al. 2021;Zhang et al. 2021] has investigated specialized Monte Carlo sampling strategies that account for this.\n\nThis article focuses on shape reconstruction, where global illumination plays a lesser role, and we therefore use the differentiable rasterizer of Laine et al. [2020] in our experiments. Section 4 also investigates the behavior of our method in a physically based renderer based on Monte Carlo integration.\n\n\nThe Laplace Operator\n\nThe Laplace operator is the workhorse of modern geometry processing, and familiarity is implicitly assumed by most literature in this area. Since the main audience of this article are researchers and practitioners in the area of (differentiable) rendering, we include a review of relevant definitions and properties.\n\nThe Laplacian \u0394 is one of the elementary differential operators; it arises in countless physical problems including the diffusion of heat, electrostatic potentials, and incompressible fluid flow. On an -dimensional Euclidean domain, its definition reads \u0394 = 2 2 1 + \u00b7 \u00b7 \u00b7 + 2 2 .\n\n(1)\n\nThe Laplacian possesses a well-studied set of eigenvalues and eigenfunctions. On a 1D interval such as [0, 1], the eigenfunctions are sinusoidal oscillations, whose frequency increases as \u2192 \u221e:\n= \u2212 2 2 , ( ) = \u221a 2 cos( ). ( = 0, . . .)(2)\nThey provide a convenient orthonormal frequency basis of the underlying domain, and this behavior also carries over to other kinds of Laplacian operators, e.g. on curved surfaces. This is the foundation of Fourier analysis on such general domains. Two other aspects are relevant: the first eigenvalue 0 equals zero, which simply shows that \u0394 maps constant functions to zero. Next, the eigenvalue has a quadratic dependence on the index . In other words, \u0394 greatly amplifies the magnitude of high-frequency signals. Conversely, a hypothetical inverse operator \"\u0394 \u22121 \" would greatly attenuate the magnitude of high frequencies. This frequencydependent attenuation will be a key ingredient of our method.\n\nLaplacians and smoothness. The Dirichlet energy ( ) can be used to quantify the overall smoothness of a function on a Euclidean domain \u03a9. It is defined proportional to the integrated squared norm of the function's gradient, i.e.,\n( ) 1 2 \u222b \u03a9 \u2225\u2207 \u2225 2 dx,(3)\nwhich can be cast into an inner product involving the Laplacian:\n= 1 2 \u222b \u03a9 \u2207 \u00b7 \u2207 dx = \u2212 1 2 \u222b \u03a9 \u00b7 \u0394 dx = \u2212 1 2 \u27e8 , \u0394 \u27e9. (4)\nwhere the effectively constant term depends only on the boundary conditions (i.e. ( \u03a9) and \u2207 ( \u03a9)); it is therefore usually ignored in definitions of the Dirichlet energy.\n\nThe heat equation is a partial differential equation of the form\n( , x) = \u0394 x ( , x),(5)\nwhere the Laplacian is taken with respect to the spatial coordinates. This equation models the diffusion of heat within a solid material, as represented by a temporally and spatially varying function ( , x). As \u2192 \u221e, becomes progressively smoother and eventually approaches an equilibrium. This equilibrium solution is uniquely defined and has the smallest possible Dirichlet energy.\n\n\nDiscrete Laplacians\n\nConsider a polygonal mesh M = ( , ) with a set of vertices and edges. A generalized discrete Laplacian operator L on this mesh can be defined as follows:\n(L) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2212 , if ( , ) \u2208 ( , ) \u2208 , if = 0, otherwise(6)\nThe weights \u2208 R discretize the first derivative along an edge, and the signed addition of multiple first derivatives within L extends the notion of a second derivative to signals on M.\n\nAnalogous to the smooth Dirichlet energy ( ) = \u2212 1 2 \u27e8 , \u0394 \u27e9, we can now also define a Dirichlet energy in terms of L:\n(f) = 1 2 \u27e8f, Lf\u27e9 = 1 2 f Lf .(7)\nIt generalizes the notion of smoothness to discrete signals f that are sampled at vertices of the curved domain M. The missing negation is due to a different sign convention used for graph Laplacians.\n\nCommon Mesh Laplacians. Various weights can be chosen to obtain Laplacian operators with slightly different properties. The combinatorial Laplacian is based only on the topology of the input graph and sets = 1 for all edges ( , ) \u2208 . Previous works [Nealen et al. 2006;Botsch and Kobbelt 2004b] have observed its good properties for mesh optimization. The cotangent Laplacian [Pinkall and Polthier 1993;Dziuk 1988] derived by integrating the Laplace-Beltrami operator over Voronoi regions yields a more accurate discretization of the mean curvature flow, albeit with adverse effects on mesh quality [Kazhdan et al. 2012]. We refer to Botsch et al. [2010] for a review of Laplacian mesh processing.\n\n\nSobolev Preconditioning\n\nSection 3.4 interprets our method as a form of Sobolev preconditioned gradient descent [Neuberger 1985;Kar\u00e1tson and L\u00f3czi 2005;Osher et al. 2018;Park et al. 2021] applied to inverse rendering. In geometry processing, existing works use the Laplacian and related operators in place of more complex mesh-energy Hessians during a Newton-type descent [Kovalsky et al. 2016;Zhu et al. 2018;Yu et al. 2021;Claici et al. 2017;Rabinovich et al. 2017]. Most recently, Wang and Solomon [2021] propose a modification of the Adam optimizer [Kingma and Ba 2014] by reparametrizing harmonic functions in terms of the Laplacian's underlying metric, resulting in an-albeit quite different-form of Sobolev preconditioned Adam optimizer.\n\n\nActive Surface Models\n\nOur method is also related to Active Surface Models that optimize a deformable mesh subject to extrinsic forces and an intrinsic smoothness energy [Terzopoulos et al. 1988]. Motivated by this approach, Wickramasinghe et al. [2021] applied semi-implicit regularization to a graph neural network for surface reconstruction and volume segmentation. In contrast, our goal is to accelerate convergence without the typical compromises that regularization entails.\n\n\nMETHOD\n\nOur goal is to determine a methodology for taking large descent steps in mesh optimization problems using differentiable rendering. To this end, we defer generalizations and modifications, and explain our proposed technique by considering a model problem:\nminimize x\u2208R \u00d73 \u03a6( (x)),(8)\nwhere x \u2208 R \u00d73 collects mesh vertex positions along rows, and \u03a6 is a loss function measuring the reconstruction accuracy of images produced by a renderer (we use a 1 loss in our experiments). In principle, the function \u03a6 could also examine its input in some other way, e.g., using a trained neural computation. The relevant property is that both and \u03a6 are differentiable almost everywhere. We assume here that renders images with known camera poses.\n\n\nConventional Gradient Descent\n\nGiven initial mesh vertex positions, we may attempt to optimize the problem in Equation 8 by applying gradient descent:\nx \u2190 x \u2212 \u03a6 x ,(9)\nwhere > 0 is the scalar step length or learning rate. Leveraging reverse-mode differentiation, the per-vertex gradients \u03a6 x \u2208 R \u00d73 can be efficiently computed (Section 2). For inducing high-frequency changes based purely on shading, conventional gradient descent can be sufficient to induce the necessary small displacements (see, e.g., Liu et al. [2018]). For reconstruction problems, the initial shape (e.g., a sphere) may be very far from optimal, resulting in silhouette mismatches. Differentiation of (e.g., via reverse-mode autodiff) then produces two very different gradient terms: shading gradients and silhouette gradients (see Fig. 2). Shading gradients account for discrepancies in surface normals and tend to be small and uniformly distributed when measured on mesh vertices. Meanwhile, silhouette gradients account for gross discrepancies and concentrate large but sparse gradient terms on vertices composing the shape's silhouette in each image (under mild assumptions, a set of size ( \u221a )). Naively following silhouette gradients quickly leads to an unrecoverable tangled mesh (see Figure 1), where shading gradients have no opportunity to help improve. Decreasing the stepsize will delay this process, but the mesh nevertheless becomes highly distorted by the time silhouettes are resolved enough for shading gradients to dominate.\n\nThe following toy example illustrates the problematic nature of sparse gradients in mesh optimization. Here, a 1D \"mesh\" made of evenly-spaced line segments on the interval [\u22121, +1] is optimized to match a reference shape covering the smaller interval [\u2212 1 /2, + 1 /2]. The first and third images show positions (vertex indices on the horizontal axis, positions on the vertical axis), and the second and third image show gradients due to silhouette misalignment. The first step of gradient descent pulls the outermost vertices inward, but it moves them too far so that they end up within the \"interior\" of the 1D shape. A subsequent gradient step targets the next outer pair of vertices, and this pattern repeats to yield a tangled shape with multiple inverted elements.\n\n\nRegularization\n\nIn geometry processing, the conventional approach to tame tangled meshes due to large variations is to append a Laplacian regularization objective, altering the original optimization in Equation 8 to:\nminimize x\u2208R \u00d73 \u03a6( (x)) + 2 tr x \u22a4 Lx ,(10)\nwhere L \u2208 R \u00d7 is a sparse, symmetric positive definite Laplacian matrix that discretizes Dirichlet energy [Solomon et al. 2014]. Without loss of generality, L also could be a power of the Laplacian, such as squared Laplacian energy or bi-Laplacian [Botsch and Kobbelt 2004a;Jacobson et al. 2010]. The parameter > 0 balances between the original objective and regularization. Accordingly, the gradient descent update changes to:\nx \u2190 x \u2212 \u03a6 x + Lx .(11)\nFor large , the regularization term dominates and the descent behaves as a forward Euler integration of mean curvature flow, known to be unstable even for mild step sizes [Desbrun et al. 1999]. For small , the effect of the regularization is, of course, diminished by this scale factor, but also by the same root cause of the forward Euler instability: the gradient contribution Lx \u2208 R \u00d73 is a highly local action. Mesh Laplacians have the connectivity of an adjacency matrix. Consider multiplying L against a set of positions x which are constant except for a \"blip\" at one vertex. The result will be zero everywhere except the immediate neighbors of the blip. Under favorable assumptions, ( \u221a ) gradient descent iterations will be necessary to propagate information of any one vertex to all other vertices of a mesh.\n\nExamining our toy 1D example, we observe oscillatory gradients in the second iteration that attempt to correct distortion introduced by the first step:\nInitial state \u22121 0 1 Gradient step 1 \u22120.5 0.0 0.5 Updated state \u22121 0 1 Initial Gradient step 2 \u22120.5 0.0 0.5\nAppending a regularization term diffuses sparse gradients, but only locally and dependently on the mesh resolution.\n\n\nSecond-order Optimization\n\nIf Laplacian regularization is such a disappointment for gradient descent, then why is it so popular in geometry processing? The answer is in that subfield it is much more common to apply second-order optimization such as Newton's method. As an important special case, if all other objective terms are quadratic convex functions of x -like for instance tr x \u22a4 Lx , then a single step of Newton's method leads to the global optimum. What if we tried to apply Newton's method to the regularized problem in Equation 10? The resulting update is given by\nx \u2190 x \u2212 2 \u03a6 x 2 + L \u22121 \u03a6 x + Lx ,(12)\nwhere the Hessian of the regularization term is simply the Laplacian matrix L. Examining our toy 1D example, a single Newton iteration suffices to smoothly move all vertices into essentially the right place: The second step is small in magnitude and relaxes the interior while correcting the endpoint positions. Unfortunately, the Hessian of the differentiable rendering term is far too complicated:\n2 \u03a6 x 2 = 2 \u03a6 2 x + \u03a6 x 2 .(13)\nThese second-order terms are computationally expensive and delicate to compute. They are unavailable in many automatic differentiation systems. If the renderer accounts for global illumination, transparency, or has differentiable lighting parameters, the resulting matrix 2 \u03a6/ x 2 \u2208 R \u00d7 may be dense. Requiring ( 2 ) storage and ( 3 ) computation for linear solving per iteration, this is intractable for even modest mesh resolutions.\n\nThe role of 2 \u03a6/ x 2 + L \u22121\n\ncan be understood as diffusing the (potentially sparse and concentrated) gradient updates over the entire domain. Our key idea is to achieve this diffusion without incurring the cost of a full-blown second-order optimization.\n\n\nModified Gradient Descent\n\nOur proposed modification of the gradient descent update is to precondition the original objective gradients with a convex combination of the identity matrix I and the mesh Laplacian L:\nx \u2190 x \u2212 (I + L) \u2212 \u03a6 x .(14)\nwhere \u2265 1 further controls the amount of diffusion and later enables a useful reparameterization interpretation that invites tuning of momentum contributions. For now, it suffices to consider = 1, though ultimately our final update rule will be most analogous to = 2. Since I + L is sparse, the associated linear system can be solved efficiently. In geometry processing, a standard choice for L would be the cotangent Laplacian [Pinkall and Polthier 1993], which depends on edge lengths besides the discrete mesh connectivity. A potential disadvantage of this construction is that the coupling between Laplacian and evolving vertex positions can introduce singularities during descent [Kazhdan et al. 2012]. The combinatorial Laplacian lacks this dependence and furthermore has been shown to promote more regular tessellations [Nealen et al. 2006]. Counter to these observations, we did not observe singularities or noticeable qualitative differences between cotangent and combinatorial Laplacian in our experiments. Given the qualitative similarity, we prefer the combinatorial Laplacian mainly for its computational efficiency: thanks to the purely topological dependence, a potentially expensive factorization of Equation 14 can be reused across iterations.\n\nThe gradients resulting from our formulation diffuse the sparse silhouette gradients similar to the Newton step: The overall step size is more approximate: a full step ( = 1) is usually ideal in a second-order method, whereas momentum or a line search to determine is needed in our case.\n\nThere are various ways of interpreting our proposed approach.\n\nQuasi-Newton method. Our formulation can be viewed as a modification of the full Newton's update in Equation 12, where the exact Hessian of the original objective in the gradient preconditioner is replaced with the identity I, i.e.,\nx \u2190 x \u2212 (I + L) \u22121 \u03a6 x + Lx ,(15)\nthe remaining difference being the presence of the regularization in the gradient term + Lx. This modified update accelerates convergence of the regularized update from Equation (11) and will therefore still converge to a regularized solution. In contrast, our goal is to benefit from accelerated convergence without compromising on minimizing \u03a6, hence we furthermore replace the last term in parentheses with the pure rendering gradient \u03a6 / x. Omitting this step retains the behavior of regularization, while stabilizing convergence with large step sizes. In this way, we may view our descent as a form of Sobolev or inverse-Laplacian preconditioning.\n\nDiffusion reparameterization. A single implicit Euler timestep of coordinate-wise heat diffusion of any vector-valued quantity u \u2208 R \u00d73 over the mesh as a graph has the form\nargmin x 1 2 \u2225x \u2212 u\u2225 2 + 1 2 tr x \u22a4 Lx ,(16)\nwhose solution is revealed by the Euler-Lagrange equation:\nx = (I + L) \u22121 u(17)\nIn this view, may be seen as the temporal duration of diffusion. For a fixed Laplacian L, the Jacobian of x as a function of the introduced variables u is simply our inverted matrix expression:\nx u = (I + L) \u22121 .(18)\nThus, an alternative interpretation of our update in Equation 14, is conventional gradient descent based on a reparameterized problem min \u03a6( (x(u))) involving a single-step of implicit heat diffusion via the chain rule:\nu \u2190 u \u2212 x u \u03a6 x .(19)\nTogether with Equation 17, the corresponding update rule for the positions x reduces to\nx \u2190 (I + L) \u22121 (u \u2212 x u \u03a6 x ) = x \u2212 (I + L) \u22122 \u03a6 x ,(20)\nwhich is equivalent to Equation 14 with = 2. These reparameterized coordinates u bear some resemblance to the differential coordinates of Lipman et al. [2004] and Sorkine [2005] defined as u = Lx, which they then use in a least-squares setting. Descent can also be applied in the parameterization by x = L \u22121 u, which corresponds to the asymptotic case \u2192 \u221e. Special care must be taken to handle the rank deficiency of L, e.g., constraining the position of one vertex per connected component [Lipman et al. 2004].\n\nHaving shown the equivalence of the reparameterized and preconditioned update rules above, working with the coordinates u may appear circuitous. However, we will soon see that there are subtle differences that enable further quality improvements. Regardless of the preferred interpretation, we demonstrate substantial improvement over raw and Laplacian regularized gradient descent.\n\nCompared to gradient descent, our method diffuses concentrated silhouette gradients and smoothly moves the entire mesh. Compared to Laplacian regularization, we do not modify the original objective function and consequently, we are less sensitive to the hyper-parameter . This means that the step size can be significantly larger without fear of instability or mesh tangling.\n\n\nMomentum and Variance\n\nPractical usage of gradient descent often benefits from introducing momentum terms. We similarly find that this noticeably improves the quality of our results. Let us consider some descent variants before arriving at our proposed update rule. In particular, we will apply momentum modifications to the update of the parameters u which linearly control the mesh positions via Equation 17.\n\nThe classic momentum variant involves the following update rule:\ng \u2190 (I + L) \u2212 \u03a6 x , m 1 \u2190 1 m 1 + (1 \u2212 1 )g, u \u2190 u \u2212 m 1 1 \u2212 1 ,(21)\nwhere g, m 1 \u2208 R \u00d73 represent the descent step and first-order moment estimate, 0 \u2264 1 \u2264 1 controls the decay, the power is the iteration number, and the division by 1 \u2212 1 conducts de-biasing. The widely used Adam optimizer [Kingma and Ba 2014] extends this update rule with a component-wise preconditioning scheme based on second-order moments:\nm 2 \u2190 2 m 2 + (1 \u2212 2 ) g 2 , u \u2190 u \u2212 m 1 1 \u2212 1 m 2 1 \u2212 2 + ,(22)\nwhere matrix exponentiation, division, and the square root are appropriately component-wise.\n\nWe observed that aggressive steps resulting from this componentwise preconditioner tend to disturb the smoothness of the result. We therefore prefer a more uniform adaptation and refer to this update rule as UniformAdam, which differs from AdaMax [Kingma and Ba 2014] that applies the infinity norm over time. We state our final update rules incorporating this change in full for convenience:\ng \u2190 (I + L) \u22121 \u03a6 x , m 1 \u2190 1 m 1 + (1 \u2212 1 ) g, m 2 \u2190 2 m 2 + (1 \u2212 2 ) g 2 , u \u2190 u \u2212 (1 \u2212 1 ) \u2225 m 2 \u2225 \u221e 1\u2212 2 m 1 ,(23)\nwhere x(u) = (I + L) \u22121 u, and again, g 2 is the component-wise square. This comparison also reveals an interesting point: preconditioning based on second-order moments leads to differences between optimization on x versus optimization of u due to the component-wise Time Fig. 3. Our method can benefit from periodic re-meshing steps, particularly when the input shape is too coarse, or when it has an unsuitable mesh topology that leads to distortion. This figure shows an extreme case, where we recover the Stanford dragon starting from an icosahedron. Intermediate insets visualize the shape following remeshing steps.  Fig. 4. We compare several first-order optimizers with a consistent choice of two diffusion steps ( = 2). The step size was adjusted to obtain the best quality at equal time in each case.\n\n\nComparison of Gradient-based Optimizers\n\n(1) Vanilla gradient descent produces a comparably blurry result.\n\n(2) Momentum improves quality noticeably.\n\n(3) Aggressive component-wise preconditioning in the Adam algorithm disturbs the smoothness of the output. (4) Performing the optimization on a \"differential coordinate\" parameterization of the mesh improves this behavior, though some non-uniformity remains. (5,6) Our UniformAdam method produces the best results, with comparable quality on both domains. (22) and (23). The results shown in this paper are based on an optimization of the parametric representation u.\n\n\ndivision in Equations\n\n\nRemeshing\n\nOur method is easily combined with a remeshing step applied once or periodically during the optimization. Figures 3 and 5 showcase results obtained with the technique of Botsch and Kobbelt [2004b], by isotropically remeshing the shape with a target edge length equal to half of the current average value. Isotropic remeshing is harmonious with our use of the combinatorial Laplacian L, as both operations prefer a regular surface tessellations. We remesh at manually specified timesteps and decrease the step size by a factor of 0.8 each time Our method improves upon this and, (c) unsurprisingly can further improve detail with an increased vertex budget of \u223c 35K vertices, though this also introduces artifacts: observe, e.g., the missing hole above the zygomatic arch. (d) We obtain the best results by optimizing at the original resolution for half of the time, remeshing to increase the vertex count \u223c 3.5\u00d7, and continuing to optimize. This adapts the mesh connectivity to the tentative solution, enabling more reliable convergence to a substantially more uniform result. Note that the mesh is still topologically a sphere: the reconstruction automatically extrudes two partial \"bones\" that meet in the middle to produce the arch.\n\nto improve stability; finding the optimal schedule and incorporating curvature adaptivity are both interesting directions for future work.\n\n\nNumerical Solution\n\nOur method requires the solution of a sparse linear system based on the connectivity of the input mesh. This system is strictly positive definite for finite > 0, and the addition of an identity leads to good numerical conditioning. We experimented with two approaches: the conjugate gradient method, and a direct solver based on a fill-in reducing sparse Cholesky factorization. The conjugate gradient method builds on matrix-vector multiplications involving the system matrix. We use a sparse matrix representation, though we note that such a multiplication could in principle also be realized on top of an existing mesh data structure. . Large gradients steps can introduce considerable non-uniformity in this single-view optimization of a sphere towards a larger reference sphere. Our method (top row) converges to a high-quality solution within a few hundred iterations, while regularization produces meshes with unsatisfactory distortion for different strengths of the regularization parameter . The solution eventually collapses to a point when smoothness is prioritized too much. Plots of the individual loss terms reveal how the image loss is first optimized at the expense of regularization, which then catches up once the image loss has reached its minimum. Our method does not require this trade-off between two competing objectives.\n\nWe use CHOLMOD [Chen et al. 2008] for the second approach, which exposes numerous strategies and heuristics: we use the simplicial method and nested dissection (NESDIS) ordering, which we found to produce factors with the greatest degree of sparsity. Our reliance on a combinatorial Laplacian is beneficial here, since the Cholesky factor can be reused across iterations. Remeshing steps must, however, update the factorization to account for the updated mesh connectivity. The overhead introduced by the factorization is on the order of a few optimization steps (see Table 1). Solving linear systems using the direct solver was always multiple orders of magnitude faster in our experiments. However, we observed in synthetic experiments that the computation time of the one-time factorization step grows super-linearly as a function of mesh size. This was not a concern in our case-however, there could be a point where the conjugate gradient method becomes competitive, and we therefore also evaluate its performance.\n\n\nRESULTS\n\nWe now turn to results and remaining technical details.\n\nLarge timesteps. Figure 6 optimizes the 3D mesh of a sphere from a single viewpoint so that it matches a reference image of a larger sphere, which resembles the motivational 1D examples shown in Section 3. The geometry is purely emissive, which leads to rendered images that are essentially binary. We intentionally disable shading in this way to isolate the effect of the sparse silhouette gradients.\n\nOur method converges within a few hundred iterations (top row). We then vary the parameter of the regularization baseline (next 3 rows) to show configurations where there is too little, just enough, and too much regularization. With regularization, descent eventually converges to a spherical shape of the right size, but the geometry is highly non-uniform. The shape collapses to a point when the regularization term dominates the optimization objective.\n\nThe rendering-and Laplacian loss terms in the plots on the right show how silhouette-related changes dominate the first part of the optimization, which disturbs the uniformity of the initial mesh.\n\nRegularization then tries to catch up-however, once the silhouette is roughly in place, it is difficult to improve mesh uniformity without breaking up the silhouette. Due the fundamental compromise between the two loss terms, the optimization eventually stagnates in a -dependent stalemate. The regularized result could be improved by a longer optimization using a very small learning rate; the advantage of our method is its ability to adjust the silhouettes and preserve the smoothness of the mesh while taking large steps. We compare the quality and convergence of a regularized differentiable renderer based on Laplacian (L) and bi-Laplacian (L 2 ) smoothness energies to our method. Results were obtained at equal-time using optimized hyperparameters. The bottom three rows visualize image loss, Dirichlet energy, and geometric (Hausdorff) distance to the ground truth. Our method reliably converges to solutions that are simultaneously geometrically uniform and close to the reference. The plots in the second-to-last row show how our preconditioning approach gradually decreases smoothness to match the reference, while regularized differentiable rendering aggressively does so in the first few iterations.\n\nMesh reconstruction. Figure 7 provides a comprehensive comparison of methods in a reconstruction of six meshes using both shading and silhouette gradients and observation from multiple views (details in Table 1). In some cases, the initial state of the optimization was specially adapted to the topology of the desired output. For example, the Bob optimization uses a toroidal initialization. One and four holes have been cut into the spherical initializations of the Planck and T-shirt optimizations, respectively.\n\nThe first three rows depict initialization and regularization baselines. The fourth row shows results produced by our re-parameterized update rule from Equation (23). The last three rows visualize the decay (or increase!) of the image loss, Dirichlet energy, and average bidirectional Hausdorff distance over time. The descent step size was optimized on a per-shape basis to achieve the best reconstruction quality at equal time, and is provided for each scene in  our experiments. For the regularization baselines, we additionally optimized the regularization weight on a per-shape basis, while our method uses a fixed in all cases. The regularization baselines obtain the best results using a traditional Adam optimizer with additional component-wise preconditioning, while the results of our method use UniformAdam (Section 3.5).\n\nIn general, we find that our method reliably converges to highquality meshes, while the effects of regularization are more nuanced and problem-dependent. In addition to regularization via the combinatorial Laplacian (L), we also examine the behavior of a bi-Laplacian (L 2 ) that captures a stronger notion of smoothness. However, even with a tuned weight and step size, we found convergence at equal time inferior the original Laplacian L.\n\nThe second-to-last row depicting smoothness of solutions over time reveals striking differences in the convergence behavior of the various methods: starting with a maximally smooth initialization (e.g. a perfect icosphere), our method gradually decreases smoothness as needed to introduce spatial detail. Plots of regularizationbased techniques begin with a vertical cliff that is introduced when geometric gradients attempt to pull the silhouette into place.\n\nInfluence of the number of viewpoints. Figure 8 compares the reconstruction quality of our method and a regularized baseline as the number of available viewpoints increases. Naturally, more views encode additional information that can be leveraged to reduce ambiguity and improve reconstruction quality. Each viewpoint contributes information about its respective set of silhouette vertices, which reduces the sparsity of these problematic gradients. Still, we observe noticeable changes and instability in baseline reconstructions even when the number of viewpoints is relatively large.\n\nVideo. Please see the supplemental video for animated convergence visualizations of Nefertiti, Cranium, and other results.\n\nInfluence of the parameter. Our method has a tunable parameter that controls the implicit time step of the diffusion process. For = \u221e, preconditioning computes the steady-state solution of the heat equation, while = 0 disables our method. Figure 9 illustrates equal-iteration reconstruction results using different values of this parameter. We generally use values in the range 15-50. Extremely large values (or = \u221e) significantly dampen high-frequencies, which can impede our method's ability to reconstruct fine details.   Fig. 10. Monte Carlo texture reconstruction. Our method broadly applies to any situation involving sparse or high-variance gradients. Here, we use it to accelerate the reconstruction of a high-resolution texture rendered by a Monte Carlo path tracer at \u223c 1 /13 resolution. Each rendering step only observes a fraction of texels, which introduces variance (column 2). Due to the inherent loss of information, a lower-resolution reconstruction is expected in this setting. Regularization can improve quality over time but fails to suppress newly added variance (column 3). Diffusing sparse gradients using our preconditioner accelerates convergence and closely reproduces the rendered target, while being smooth in texture space (column 4).\n\nTexture reconstruction using Monte Carlo sampling. We initially motivated our method as an acceleration scheme for taking large steps in mesh reconstruction based on differentiable rendering. Taking stock of its assumptions and requirements, we can now recognize that its operating range extends beyond this motivating case: any optimization involving sparse or noisy gradients potentially stands to benefit given a suitable Laplacian operator to quantify smoothness on an underlying domain. This includes albedo textures, displacement or normal maps, 3D volumes (e.g. participating media), temporally varying data, and potentially even alternative implicit geometric representations like signed distance functions.\n\nFor example, consider rendering a scene containing high-resolution textures: an individual Monte Carlo rendering will normally only observe a small and random subset of texels, whose derivatives are subject to further variance owing to the stochastic simulation. Handling such noisy gradient estimates has previously required the use of small optimization steps, multi-resolution optimization, and/or regularization. Figure 10 analyzes the benefits of our method in this setting. We use Mitsuba 2 [Nimier-David et al. 2019] to optimize the textured back wall (1024 \u00d7 1024 pixels) of a Cornell box-like scene with diffuse inter-reflection. The texture starts out with a constant 50% gray value, and the optimization objective measures the difference between the rendered image and a reference image shown on the top left. Renderings use a resolution of 108\u00d7108 pixels at 4 samples/pixel, of which only 76 \u00d7 76 pixels show the back wall.\n\nThe texture resolution greatly exceeds that of the rendered view. While the final rendering should closely resemble the target (top left), we cannot expect to recover the original texture (bottom left) given this inherent loss of information. Due to the constant injection of variance, both na\u00efve and regularized optimization via Adam converge slowly, producing texture reconstructions that are contaminated by severe amounts of fine-scale noise. Our approach attenuates this fine-scale noise before it reaches the texture, which accelerates convergence and produces an arguably more useful solution of this highly ambiguous problem.\n\nImplementation details. All experiments in this article were performed on a Linux Ryzen 3990X workstation using a TITAN RTX graphics card. We implemented our shape reconstruction pipeline on top of the nvdiffrast differentiable rasterizer by Laine et al. [2020], along with a spherical harmonics shading model [Ramamoorthi and Hanrahan 2001] evaluated in PyTorch [Paszke et al. 2019].\n\nGeometric reconstruction from images normally involves observations from multiple viewpoints to reduce ambiguity. During the optimization, these viewpoints could be randomly chosen as part of a stochastic gradient descent (SGD) procedure or rendered all at once. Even when random sampling is used, it can be beneficial to process groups of viewpoints in mini-batches. In this case, preconditioning is only necessary once and can be applied to the accumulated position gradients.\n\nIn our case, relatively few fixed viewpoints are used in experimentsconcrete numbers for each scene are available in Table 1. At each iteration, we render the tentative shape reconstruction from this set of viewpoints in one large batch and compute the error as the mean absolute error (MAE) across pixels of all viewpoints.\n\nFollowing initialization, all steps of the computation run on the GPU: for example, we upload the sparse Cholesky factor onto the GPU and use NVIDIA's cuSPARSE library for sparse forward-and back-substitution via the routine csrsm2_solve [Naumov 2011]. One point worth noting here is that sparse matrix libraries including CHOLMOD frequently default to double precision arithmetic-for the portion that occurs on the GPU, we found it performance-critical to ensure the use of single precision arithmetic due to the roughly 16 \u00d7 lower double precision throughput on current NVIDIA GPUs.\n\nPerformance. Figure 11 decomposes the per-iteration cost of the regularization baseline and two implementations of our method into primal, adjoint, and regularization or preconditioning steps. The numbers show that the additional cost of preconditioning using a sparse Cholesky factorization is negligible compared to the  Fig. 11. We compare the per-iteration cost of our method and a baseline using regularization. Preconditioning using a sparse Cholesky factorization only involves a small extra cost compared to the two phases of the differentiable renderer. Iterative solution of the linear system using conjugate gradients is significantly slower. On the positive side, this approach requires no precomputation of a factorization and is very easy to implement. The cost of the one-time Cholesky factorization is shown in Table 1.\n\n(significant) time spent on the primal and adjoint rendering phases. Iterative solution of the linear system using conjugate gradients is feasible, albeit at significantly higher per-iteration cost. Table 1 provides further information about the size of meshes in our experiments, the number of views, and cost of the one-time factorization step.\n\n\nCONCLUSION\n\nDifferentiable rendering is a promising new tool for solving challenging inverse problems in diverse disciplines that seek to derive understanding from images. Yet, anyone who has tried working with a differentiable renderer will recall their initial bitter disappointment: inversion of even a few nontrivial parameters leads to ambiguous and non-convex objectives, causing gradient-based optimization to simply explode or find absurd solutions leveraging this ambiguity.\n\nTo render this framework practical, we must inject knowledge about the expected nature of a solution, which has traditionally involved regularization that necessarily compromises on solving the original problem. Our method represents a large step towards robust geometry optimization using a preconditioner that alleviates issues arising from the derivative of the visibility in a scene.\n\nOur method is computationally cheap, easy to implement, and it substantially improves the quality of reconstructions at equal time. At the same time, it is not flawless: distortion and self-intersections can still occur, as seen in some of our results. It cannot change the topology by punching holes or melding overlapping geometry like the extruded bones forming an arch in Figure 5. Unconditionally robust geometric optimization will clearly require further innovation on these fronts.\n\nOur approach also shows how going to second order in a subset of the problem can greatly improve the robustness within an overall first-order optimization. Real-world usage of differentiable rendering requires simultaneous optimization of camera pose, geometry, and textures. Like a Dirichlet energy, this tightly couples the degrees of freedom: we could, e.g., change the color of a rendered pixel by adjusting the observed texel, moving mesh vertices or UV coordinates, or by rotating the camera. These relationships are not \"perceived\" by first-order descent, which must take tiny steps to navigate this complex optimization landscape. Generalizing such partial use of second-order optimization to other parameter combinations is a promising direction for future work.\n\nFig. 1 .\n1(a) Inverse reconstruction of the Nefertiti bust from a spherical starting guess with 25 rendered views (1 shown). (b)\n\nFig. 2 .\n2Geometric gradients that arise in differentiable rendering: (a) Perturbing a vertex position rotates the surface normal, which affects shading computations. The resulting shading gradients are comparably smooth and small in magnitude. (b) Silhouette gradients arise when one shape partially occludes another. In this case, a small perturbation of a vertex position can move the silhouette and cause a sudden change in a pixel's intensity. These gradients are sparse and can be very large in magnitude.\n\nFigure 4\n4compares six different gradient-based optimization schemes.\n\nFig. 5 .\n5The effect of re-meshing during an optimization. (a) Regularized reconstruction of the challenging Cranium starting from a \u223c10K vertex icosphere produces a highly distorted mesh. (b)\n\n\nFig. 6. Large gradients steps can introduce considerable non-uniformity in this single-view optimization of a sphere towards a larger reference sphere. Our method (top row) converges to a high-quality solution within a few hundred iterations, while regularization produces meshes with unsatisfactory distortion for different strengths of the regularization parameter . The solution eventually collapses to a point when smoothness is prioritized too much. Plots of the individual loss terms reveal how the image loss is first optimized at the expense of regularization, which then catches up once the image loss has reached its minimum. Our method does not require this trade-off between two competing objectives.\n\nFig. 7 .\n7Inverse shape reconstruction of targets with different topology and levels of detail.\n\nFig. 8 .\n8The benefits of our method are more pronounced when only a few viewpoints are available. In this case, sparse gradient steps lead to substantial distortion in the regularized reconstruction (top) revealed by the wireframe visualization. The set of silhouette vertices seen by any particular camera eventually becomes dense as the number of viewpoints tends to infinity. Even so, regularization-based optimization remains somewhat fragile, and the meshing can often change significantly when extra views are added (compare, e.g., the nose of the regularized Bunny with 25 and 49 views).\n\n\n. 9. Equal-iteration results of our method using different values of .\n\nTable 1 .\n1We do not change the step size during the optimization inH=4.637e+00 \n\nRegularized \n\nH=8.736e-01 \nH=5.327e-01 \nH=5.011e-01 \nH=3.898e-01 \nH=1.078e-01 \nH=1.734e-01 \n\nH=1.686e+00 \nOurs (\u03bb = 19) \n\n1 \n\nH=1.011e+00 \n\n2 \n\nH=2.641e-01 \n\n4 \n\nH=2.025e-01 \n\n9 \n\nH=1.366e-01 \n\n16 \n\nH=2.175e-01 \n\n25 \n\nH=2.369e-01 \n\n49 \n\nNumber of viewpoints \n\n\nACKNOWLEDGMENTSThe authors would like to thank Delio Vicini for early discussions about this project, Silvia Sell\u00e1n for sharing her remeshing implementation and help for the figures, as well as Hsueh-Ti Derek Liu for his advice in making the figures.Table 1. This table lists the number of optimized viewpoints, vertex count, the runtime cost of the Cholesky factorization,as well as both the baseline ( 1 ) and our ( 1 ) step sizes used for each of the various shapes shown in this article. For the Dragon optimization(Figure 3), we provide the mesh resolution at the start of the optimization and after the last remeshing step.ShapeViews | | Fact. (ms)\nUnbiased warped-area sampling for differentiable rendering. Tzu-Mao Sai Praveen Bangaru, Fr\u00e9do Li, Durand, ACM Transactions on Graphics (TOG). 39Sai Praveen Bangaru, Tzu-Mao Li, and Fr\u00e9do Durand. 2020. Unbiased warped-area sampling for differentiable rendering. ACM Transactions on Graphics (TOG) 39, 6 (2020), 1-18.\n\nLarge Steps in Cloth Simulation. David Baraff, Andrew Witkin, 10.1145/280814.280821Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '98). the 25th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '98)New York, NY, USAAssociation for Computing MachineryDavid Baraff and Andrew Witkin. 1998. Large Steps in Cloth Simulation. In Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH '98). Association for Computing Machinery, New York, NY, USA, 43-54. https://doi.org/10.1145/280814.280821\n\nAn intuitive framework for real-time freeform modeling. Mario Botsch, Leif Kobbelt, 10.1145/1015706.1015772ACM Trans. Graph. 23Mario Botsch and Leif Kobbelt. 2004a. An intuitive framework for real-time freeform modeling. ACM Trans. Graph. 23, 3 (2004), 630-634. https://doi.org/10.1145/1015706. 1015772\n\nA Remeshing Approach to Multiresolution Modeling. Mario Botsch, Leif Kobbelt, 10.2312/SGP/SGP04/189-196Second Eurographics Symposium on Geometry Processing. Jean-Daniel Boissonnat and Pierre AlliezNice, France71ACM International Conference Proceeding SeriesMario Botsch and Leif Kobbelt. 2004b. A Remeshing Approach to Multiresolu- tion Modeling. In Second Eurographics Symposium on Geometry Processing, Nice, France, July 8-10, 2004 (ACM International Conference Proceeding Series), Jean-Daniel Boissonnat and Pierre Alliez (Eds.), Vol. 71. Eurographics Association, 185-192. https://doi.org/10.2312/SGP/SGP04/189-196\n\nPolygon Mesh Processing. Mario Botsch, Leif Kobbelt, Mark Pauly, Pierre Alliez, Bruno L\u00e9vy. ; A K Peters, Mario Botsch, Leif Kobbelt, Mark Pauly, Pierre Alliez, and Bruno L\u00e9vy. 2010. Polygon Mesh Processing. A K Peters. http://www.crcpress.com/product/isbn/9781568814261\n\nAlgorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate. Yanqing Chen, Timothy A Davis, William W Hager, Sivasankaran Rajamanickam, ACM Trans. Math. Softw. 3522Yanqing Chen, Timothy A. Davis, William W. Hager, and Sivasankaran Rajamanickam. 2008. Algorithm 887: CHOLMOD, Supernodal Sparse Cholesky Factorization and Update/Downdate. ACM Trans. Math. Softw. 35, 3, Article 22 (Oct. 2008).\n\nIsometry-Aware Preconditioning for Mesh Parameterization. Sebastian Claici, Mikhail Bessmeltsev, Scott Schaefer, Justin Solomon, 10.1111/cgf.13243Comput. Graph. Forum. 36Sebastian Claici, Mikhail Bessmeltsev, Scott Schaefer, and Justin Solomon. 2017. Isometry-Aware Preconditioning for Mesh Parameterization. Comput. Graph. Forum 36, 5 (2017), 37-47. https://doi.org/10.1111/cgf.13243\n\nImplicit Fairing of Irregular Meshes Using Diffusion and Curvature Flow. Mark Mathieu Desbrun, Peter Meyer, Alan H Schr\u00f6der, Barr, 10.1145/311535.311576Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1999. Warren N. Waggenspackthe 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1999Los Angeles, CA, USAACMMathieu Desbrun, Mark Meyer, Peter Schr\u00f6der, and Alan H. Barr. 1999. Implicit Fairing of Irregular Meshes Using Diffusion and Curvature Flow. In Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1999, Los Angeles, CA, USA, August 8-13, 1999, Warren N. Waggenspack (Ed.). ACM, 317-324. https://doi.org/10.1145/311535.311576\n\nFinite elements for the Beltrami operator on arbitrary surfaces. Gerhard Dziuk, Partial differential equations and calculus of variations. SpringerGerhard Dziuk. 1988. Finite elements for the Beltrami operator on arbitrary surfaces. In Partial differential equations and calculus of variations. Springer, 142-155.\n\nUnsupervised training for 3d morphable model regression. Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, William T Freeman, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKyle Genova, Forrester Cole, Aaron Maschinot, Aaron Sarna, Daniel Vlasic, and William T Freeman. 2018. Unsupervised training for 3d morphable model regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 8377-8386.\n\nAn evaluation of computational imaging techniques for heterogeneous inverse scattering. Ioannis Gkioulekas, Anat Levin, Todd Zickler, European Conference on Computer Vision. SpringerIoannis Gkioulekas, Anat Levin, and Todd Zickler. 2016. An evaluation of computational imaging techniques for heterogeneous inverse scattering. In European Conference on Computer Vision. Springer, 685-701.\n\nEvaluating derivatives: principles and techniques of algorithmic differentiation. Andreas Griewank, Andrea Walther, SIAMAndreas Griewank and Andrea Walther. 2008. Evaluating derivatives: principles and techniques of algorithmic differentiation. SIAM.\n\nMixed Finite Elements for Variational Surface Modeling. Alec Jacobson, Elif Tosun, Olga Sorkine, Denis Zorin, 10.1111/j.1467-8659.2010.01765.xComput. Graph. Forum. 29Alec Jacobson, Elif Tosun, Olga Sorkine, and Denis Zorin. 2010. Mixed Finite Elements for Variational Surface Modeling. Comput. Graph. Forum 29, 5 (2010), 1565-1574. https://doi.org/10.1111/j.1467-8659.2010.01765.x\n\nSobolev Gradient Preconditioning for the Electrostatic Potential Equation. J Kar\u00e1tson, L L\u00f3czi, Comp. and Math. with App. J. Kar\u00e1tson and L. L\u00f3czi. 2005. Sobolev Gradient Preconditioning for the Electrostatic Potential Equation. Comp. and Math. with App. (2005).\n\nNeural 3d mesh renderer. Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. 2018. Neural 3d mesh renderer. In Proceedings of the IEEE conference on computer vision and pattern recognition. 3907-3916.\n\nCan Mean-Curvature Flow be Modified to be Non-singular?. Michael Kazhdan, Jake Solomon, Mirela Ben-Chen, 10.1111/j.1467-8659.2012.03179.xComput. Graph. Forum. 31Michael Kazhdan, Jake Solomon, and Mirela Ben-Chen. 2012. Can Mean-Curvature Flow be Modified to be Non-singular? Comput. Graph. Forum 31, 5 (2012), 1745-1754. https://doi.org/10.1111/j.1467-8659.2012.03179.x\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).\n\nAccelerated Quadratic Proxy for Geometric Optimization. Z Shahar, Meirav Kovalsky, Yaron Galun, Lipman, ACM Transactions on Graphics (proceedings of ACM SIGGRAPH. Shahar Z. Kovalsky, Meirav Galun, and Yaron Lipman. 2016. Accelerated Quadratic Proxy for Geometric Optimization. ACM Transactions on Graphics (proceedings of ACM SIGGRAPH) (2016).\n\nModular primitives for high-performance differentiable rendering. Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, Timo Aila, ACM Transactions on Graphics (TOG). 39Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. 2020. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics (TOG) 39, 6 (2020), 1-14.\n\nDifferentiable monte carlo ray tracing through edge sampling. Tzu-Mao Li, Miika Aittala, Fr\u00e9do Durand, Jaakko Lehtinen, ACM Transactions on Graphics (TOG). 37Tzu-Mao Li, Miika Aittala, Fr\u00e9do Durand, and Jaakko Lehtinen. 2018. Differentiable monte carlo ray tracing through edge sampling. ACM Transactions on Graphics (TOG) 37, 6 (2018), 1-11.\n\nDifferential Coordinates for Interactive Mesh Editing. Yaron Lipman, Olga Sorkine, Daniel Cohen-Or, David Levin, Christian R\u00f6ssl, Hans-Peter Seidel, 10.1109/SMI.2004.13145052004 International Conference on Shape Modeling and Applications (SMI 2004). Genova, ItalyIEEE Computer SocietyYaron Lipman, Olga Sorkine, Daniel Cohen-Or, David Levin, Christian R\u00f6ssl, and Hans- Peter Seidel. 2004. Differential Coordinates for Interactive Mesh Editing. In 2004 International Conference on Shape Modeling and Applications (SMI 2004), 7-9 June 2004, Genova, Italy. IEEE Computer Society, 181-190. https://doi.org/10.1109/SMI. 2004.1314505\n\nPaparazzi: surface editing by way of multi-view image processing. Hsueh-Ti Derek Liu, Michael Tao, Alec Jacobson, 10.1145/3272127.3275047ACM Trans. Graph. 3711Hsueh-Ti Derek Liu, Michael Tao, and Alec Jacobson. 2018. Paparazzi: surface editing by way of multi-view image processing. ACM Trans. Graph. 37, 6 (2018), 221:1-221:11. https://doi.org/10.1145/3272127.3275047\n\nSoft rasterizer: A differentiable renderer for image-based 3d reasoning. Shichen Liu, Tianye Li, Weikai Chen, Hao Li, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionShichen Liu, Tianye Li, Weikai Chen, and Hao Li. 2019. Soft rasterizer: A differentiable renderer for image-based 3d reasoning. In Proceedings of the IEEE International Conference on Computer Vision. 7708-7717.\n\nOpenDR: An approximate differentiable renderer. M Matthew, Michael J Loper, Black, European Conference on Computer Vision. SpringerMatthew M Loper and Michael J Black. 2014. OpenDR: An approximate differentiable renderer. In European Conference on Computer Vision. Springer, 154-169.\n\nReparameterizing discontinuous integrands for differentiable rendering. Guillaume Loubet, Nicolas Holzschuch, Wenzel Jakob, ACM Transactions on Graphics (TOG). 38Guillaume Loubet, Nicolas Holzschuch, and Wenzel Jakob. 2019. Reparameterizing discontinuous integrands for differentiable rendering. ACM Transactions on Graphics (TOG) 38, 6 (2019), 1-14.\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, European Conference on Computer Vision. SpringerBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra- mamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision. Springer, 405-421.\n\nParallel solution of sparse triangular linear systems in the preconditioned iterative methods on the GPU. Maxim Naumov, NVR-2011NVIDIA Corp. 1Tech. Rep.Maxim Naumov. 2011. Parallel solution of sparse triangular linear systems in the preconditioned iterative methods on the GPU. NVIDIA Corp., Westford, MA, USA, Tech. Rep. NVR-2011 1 (2011).\n\nLaplacian mesh optimization. Andrew Nealen, Takeo Igarashi, Olga Sorkine, Marc Alexa, 10.1145/1174429.1174494Proceedings of the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia. Y. T. Lee, Siti Mariyam Hj. Shamsuddin, Diego Gutierrez, and Norhaida Mohd. Suaibthe 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast AsiaKuala Lumpur, MalaysiaACMAndrew Nealen, Takeo Igarashi, Olga Sorkine, and Marc Alexa. 2006. Laplacian mesh optimization. In Proceedings of the 4th International Conference on Computer Graphics and Interactive Techniques in Australasia and Southeast Asia 2006, Kuala Lumpur, Malaysia, November 29 -December 2, 2006, Y. T. Lee, Siti Mariyam Hj. Shamsuddin, Diego Gutierrez, and Norhaida Mohd. Suaib (Eds.). ACM, 381-389. https://doi.org/ 10.1145/1174429.1174494\n\nSteepest descent and differential equations. J W Neuberger, 10.2969/jmsj/03720187Journal of the Mathematical Society of Japan. 37J. W. Neuberger. 1985. Steepest descent and differential equations. Journal of the Mathematical Society of Japan 37, 2 (1985), 187 -195. https://doi.org/10.2969/jmsj/ 03720187\n\nRadiative backpropagation: an adjoint method for lightning-fast differentiable rendering. Merlin Nimier-David, S\u00e9bastien Speierer, Beno\u00eet Ruiz, Wenzel Jakob, ACM Transactions on Graphics (TOG). 39Merlin Nimier-David, S\u00e9bastien Speierer, Beno\u00eet Ruiz, and Wenzel Jakob. 2020. Radiative backpropagation: an adjoint method for lightning-fast differentiable rendering. ACM Transactions on Graphics (TOG) 39, 4 (2020), 146-1.\n\nMitsuba 2: A retargetable forward and inverse renderer. Merlin Nimier-David, Delio Vicini, Tizian Zeltner, Wenzel Jakob, ACM Transactions on Graphics (TOG). 38Merlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wenzel Jakob. 2019. Mitsuba 2: A retargetable forward and inverse renderer. ACM Transactions on Graphics (TOG) 38, 6 (2019), 1-17.\n\nLaplacian Smoothing Gradient Descent. J Stanley, Bao Osher, Penghang Wang, Xiyang Yin, Minh Luo, Alex Tong Pham, Lin, arXiv:1806.06317Stanley J. Osher, Bao Wang, Penghang Yin, Xiyang Luo, Minh Pham, and Alex Tong Lin. 2018. Laplacian Smoothing Gradient Descent. CoRR abs/1806.06317 (2018). arXiv:1806.06317 http://arxiv.org/abs/1806.06317\n\nPreconditioned accelerated gradient descent methods for locally Lipschitz smooth objectives with applications to the solution of nonlinear PDEs. Jea-Hyun Park, Abner J Salgado, Steven M Wise, arXiv:math.NA/2006.06732Jea-Hyun Park, Abner J. Salgado, and Steven M. Wise. 2021. Preconditioned accelerated gradient descent methods for locally Lipschitz smooth objectives with applications to the solution of nonlinear PDEs. arXiv:math.NA/2006.06732\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc32Junjie Bai, and Soumith ChintalaAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des- maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin- tala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett (Eds.). Curran Associates, Inc., 8024-8035. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style- high-performance-deep-learning-library.pdf\n\nA survey of inverse rendering problems. Gustavo Patow, Xavier Pueyo, Computer graphics forum. Wiley Online Library22Gustavo Patow and Xavier Pueyo. 2003. A survey of inverse rendering problems. In Computer graphics forum, Vol. 22. Wiley Online Library, 663-687.\n\nComputing discrete minimal surfaces and their conjugates. Ulrich Pinkall, Konrad Polthier, Experimental mathematics. 2Ulrich Pinkall and Konrad Polthier. 1993. Computing discrete minimal surfaces and their conjugates. Experimental mathematics 2, 1 (1993), 15-36.\n\nScalable Locally Injective Mappings. Michael Rabinovich, Roi Poranne, Daniele Panozzo, Olga Sorkine-Hornung, 10.1145/2983621ACM Trans. Graph. 3616Michael Rabinovich, Roi Poranne, Daniele Panozzo, and Olga Sorkine-Hornung. 2017. Scalable Locally Injective Mappings. ACM Trans. Graph. 36, 2, Article 16 (April 2017), 16 pages. https://doi.org/10.1145/2983621\n\nAn efficient representation for irradiance environment maps. Ravi Ramamoorthi, Pat Hanrahan, Proceedings of the 28th annual conference on Computer graphics and interactive techniques. the 28th annual conference on Computer graphics and interactive techniquesRavi Ramamoorthi and Pat Hanrahan. 2001. An efficient representation for irradiance environment maps. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques. 497-500.\n\nNikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, Georgia Gkioxari, arXiv:2007.08501Accelerating 3D Deep Learning with Py-Torch3D. Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. 2020. Accelerating 3D Deep Learning with Py- Torch3D. arXiv:2007.08501 (2020).\n\nLaplace-Beltrami: The Swiss army knife of geometry processing. Justin Solomon, Keegan Crane, Etienne Vouga, Symposium on Geometry Processing graduate school. Cardiff, UK2Justin Solomon, Keegan Crane, and Etienne Vouga. 2014. Laplace-Beltrami: The Swiss army knife of geometry processing. In Symposium on Geometry Processing graduate school (Cardiff, UK, 2014), Vol. 2.\n\nLaplacian Mesh Processing. Olga Sorkine, 10.2312/egst.20051044Eurographics, State of the Art Report. 53-70. Olga Sorkine. 2005. Laplacian Mesh Processing. In Eurographics, State of the Art Report. 53-70. https://doi.org/10.2312/egst.20051044\n\nDifferential representations for mesh processing. Olga Sorkine, Computer Graphics Forum. Online LibraryWiley25Olga Sorkine. 2006. Differential representations for mesh processing. In Computer Graphics Forum, Vol. 25. Wiley Online Library, 789-807.\n\nConstraints on deformable models: Recovering 3D shape and nonrigid motion. Demetri Terzopoulos, Andrew Witkin, Michael Kass, Artificial intelligence. 36Demetri Terzopoulos, Andrew Witkin, and Michael Kass. 1988. Constraints on de- formable models: Recovering 3D shape and nonrigid motion. Artificial intelligence 36, 1 (1988), 91-123.\n\nDelio Vicini, S\u00e9bastien Speierer, Wenzel Jakob, 10.1145/3450626.3459804Path Replay Backpropagation: Differentiating Light Paths using Constant Memory and Linear Time. Transactions on Graphics (Proceedings of SIGGRAPH). 40Delio Vicini, S\u00e9bastien Speierer, and Wenzel Jakob. 2021. Path Replay Backpropagation: Differentiating Light Paths using Constant Memory and Linear Time. Transactions on Graphics (Proceedings of SIGGRAPH) 40, 4 (Aug. 2021), 108:1-108:14. https: //doi.org/10.1145/3450626.3459804\n\nFast quasi-harmonic weights for geometric data interpolation. Yu Wang, Justin Solomon, 10.1145/3450626.345980173:1-73:15ACM Trans. Graph. 40Yu Wang and Justin Solomon. 2021. Fast quasi-harmonic weights for geometric data interpolation. ACM Trans. Graph. 40, 4 (2021), 73:1-73:15. https://doi.org/10.1145/ 3450626.3459801\n\nDeep Active Surface Models. Udaranga Wickramasinghe, Pascal Fua, Graham Knott, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPRUdaranga Wickramasinghe, Pascal Fua, and Graham Knott. 2021. Deep Active Surface Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 11652-11661.\n\nRepulsive Curves. Chris Yu, Henrik Schumacher, Keenan Crane, ACM Trans. Graph. 40Chris Yu, Henrik Schumacher, and Keenan Crane. 2021. Repulsive Curves. ACM Trans. Graph. 40, 2 (2021).\n\nMonte Carlo Estimators for Differential Light Transport. Tizian Zeltner, S\u00e9bastien Speierer, Iliyan Georgiev, Wenzel Jakob, 10.1145/3450626.3459807Transactions on Graphics (Proceedings of SIGGRAPH). 40Tizian Zeltner, S\u00e9bastien Speierer, Iliyan Georgiev, and Wenzel Jakob. 2021. Monte Carlo Estimators for Differential Light Transport. Transactions on Graphics (Proceedings of SIGGRAPH) 40, 4 (Aug. 2021). https://doi.org/10.1145/3450626.3459807\n\nAntithetic Sampling for Monte Carlo Differentiable Rendering. Cheng Zhang, Zhao Dong, Michael Doggett, Shuang Zhao, ACM Trans. Graph. 4012Cheng Zhang, Zhao Dong, Michael Doggett, and Shuang Zhao. 2021. Antithetic Sampling for Monte Carlo Differentiable Rendering. ACM Trans. Graph. 40, 4 (2021), 77:1-77:12.\n\nCheng Zhang, Bailey Miller, Kai Yan, Ioannis Gkioulekas, and Shuang Zhao. 2020. Path-Space Differentiable Rendering. 39Cheng Zhang, Bailey Miller, Kai Yan, Ioannis Gkioulekas, and Shuang Zhao. 2020. Path-Space Differentiable Rendering. ACM Trans. Graph. 39, 4 (July 2020).\n\nA differential theory of radiative transfer. Cheng Zhang, Lifan Wu, Changxi Zheng, Ioannis Gkioulekas, Ravi Ramamoorthi, Shuang Zhao, ACM Transactions on Graphics (TOG). 38Cheng Zhang, Lifan Wu, Changxi Zheng, Ioannis Gkioulekas, Ravi Ramamoorthi, and Shuang Zhao. 2019. A differential theory of radiative transfer. ACM Transactions on Graphics (TOG) 38, 6 (2019), 1-16.\n\nBlended Cured Quasi-Newton for Distortion Optimization. Yufeng Zhu, Robert Bridson, Danny M Kaufman, ACM Trans. on Graphics (SIGGRAPH 2018). to appearYufeng Zhu, Robert Bridson, and Danny M. Kaufman. 2018. Blended Cured Quasi- Newton for Distortion Optimization. to appear ACM Trans. on Graphics (SIGGRAPH 2018) (2018).\n", "annotations": {"author": "[{\"end\":94,\"start\":52},{\"end\":115,\"start\":95},{\"end\":142,\"start\":116},{\"end\":160,\"start\":143},{\"end\":194,\"start\":161},{\"end\":209,\"start\":195},{\"end\":227,\"start\":210},{\"end\":242,\"start\":228},{\"end\":277,\"start\":243},{\"end\":329,\"start\":278},{\"end\":444,\"start\":330}]", "publisher": null, "author_last_name": "[{\"end\":68,\"start\":61},{\"end\":114,\"start\":101},{\"end\":141,\"start\":130},{\"end\":159,\"start\":148},{\"end\":169,\"start\":161},{\"end\":208,\"start\":200},{\"end\":226,\"start\":219},{\"end\":241,\"start\":233},{\"end\":255,\"start\":250}]", "author_first_name": "[{\"end\":60,\"start\":52},{\"end\":100,\"start\":95},{\"end\":129,\"start\":121},{\"end\":147,\"start\":143},{\"end\":199,\"start\":195},{\"end\":218,\"start\":210},{\"end\":232,\"start\":228},{\"end\":249,\"start\":243}]", "author_affiliation": "[{\"end\":328,\"start\":279},{\"end\":443,\"start\":331}]", "title": "[{\"end\":45,\"start\":1},{\"end\":489,\"start\":445}]", "venue": "[{\"end\":507,\"start\":491}]", "abstract": "[{\"end\":4217,\"start\":917}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11152,\"start\":11139},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11877,\"start\":11853},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12431,\"start\":12409},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12753,\"start\":12734},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12893,\"start\":12870},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13710,\"start\":13683},{\"end\":13950,\"start\":13944},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14046,\"start\":14028},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14062,\"start\":14046},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14079,\"start\":14062},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14317,\"start\":14293},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14332,\"start\":14317},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14350,\"start\":14332},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14374,\"start\":14350},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14539,\"start\":14513},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14557,\"start\":14539},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14699,\"start\":14683},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14718,\"start\":14699},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14738,\"start\":14718},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14755,\"start\":14738},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14886,\"start\":14865},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":14903,\"start\":14886},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15156,\"start\":15137},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18948,\"start\":18928},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18973,\"start\":18948},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19082,\"start\":19055},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19092,\"start\":19082},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19299,\"start\":19278},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19333,\"start\":19313},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":19507,\"start\":19491},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19531,\"start\":19507},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19549,\"start\":19531},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19566,\"start\":19549},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19773,\"start\":19751},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19789,\"start\":19773},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19804,\"start\":19789},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19823,\"start\":19804},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19845,\"start\":19823},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":19886,\"start\":19863},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20320,\"start\":20296},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":20379,\"start\":20351},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21875,\"start\":21858},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24030,\"start\":24010},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24178,\"start\":24152},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24198,\"start\":24178},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24546,\"start\":24526},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27989,\"start\":27962},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28239,\"start\":28219},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":28380,\"start\":28361},{\"end\":31130,\"start\":31124},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31149,\"start\":31135},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31483,\"start\":31463},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35474,\"start\":35448},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":38055,\"start\":38037},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":46854,\"start\":46829},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":48165,\"start\":48146},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":48244,\"start\":48214},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":48287,\"start\":48267},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":49346,\"start\":49334}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":53133,\"start\":53004},{\"attributes\":{\"id\":\"fig_1\"},\"end\":53646,\"start\":53134},{\"attributes\":{\"id\":\"fig_5\"},\"end\":53717,\"start\":53647},{\"attributes\":{\"id\":\"fig_7\"},\"end\":53911,\"start\":53718},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54626,\"start\":53912},{\"attributes\":{\"id\":\"fig_9\"},\"end\":54723,\"start\":54627},{\"attributes\":{\"id\":\"fig_10\"},\"end\":55320,\"start\":54724},{\"attributes\":{\"id\":\"fig_11\"},\"end\":55393,\"start\":55321},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":55737,\"start\":55394}]", "paragraph": "[{\"end\":4311,\"start\":4219},{\"end\":5152,\"start\":4313},{\"end\":5190,\"start\":5154},{\"end\":6150,\"start\":5192},{\"end\":6656,\"start\":6152},{\"end\":7064,\"start\":6658},{\"end\":7329,\"start\":7066},{\"end\":8142,\"start\":7346},{\"end\":8913,\"start\":8144},{\"end\":9534,\"start\":8915},{\"end\":10153,\"start\":9536},{\"end\":10694,\"start\":10155},{\"end\":11453,\"start\":10696},{\"end\":11783,\"start\":11455},{\"end\":12060,\"start\":11785},{\"end\":12197,\"start\":12092},{\"end\":13099,\"start\":12226},{\"end\":14185,\"start\":13101},{\"end\":14988,\"start\":14214},{\"end\":15296,\"start\":14990},{\"end\":15637,\"start\":15321},{\"end\":15918,\"start\":15639},{\"end\":15923,\"start\":15920},{\"end\":16117,\"start\":15925},{\"end\":16864,\"start\":16163},{\"end\":17095,\"start\":16866},{\"end\":17186,\"start\":17122},{\"end\":17417,\"start\":17246},{\"end\":17483,\"start\":17419},{\"end\":17890,\"start\":17508},{\"end\":18067,\"start\":17914},{\"end\":18322,\"start\":18138},{\"end\":18442,\"start\":18324},{\"end\":18677,\"start\":18477},{\"end\":19376,\"start\":18679},{\"end\":20123,\"start\":19404},{\"end\":20606,\"start\":20149},{\"end\":20872,\"start\":20617},{\"end\":21350,\"start\":20901},{\"end\":21503,\"start\":21384},{\"end\":22868,\"start\":21521},{\"end\":23640,\"start\":22870},{\"end\":23859,\"start\":23659},{\"end\":24331,\"start\":23904},{\"end\":25173,\"start\":24355},{\"end\":25326,\"start\":25175},{\"end\":25550,\"start\":25435},{\"end\":26129,\"start\":25580},{\"end\":26567,\"start\":26168},{\"end\":27034,\"start\":26600},{\"end\":27063,\"start\":27036},{\"end\":27290,\"start\":27065},{\"end\":27505,\"start\":27320},{\"end\":28794,\"start\":27534},{\"end\":29083,\"start\":28796},{\"end\":29146,\"start\":29085},{\"end\":29380,\"start\":29148},{\"end\":30067,\"start\":29415},{\"end\":30242,\"start\":30069},{\"end\":30346,\"start\":30288},{\"end\":30561,\"start\":30368},{\"end\":30804,\"start\":30585},{\"end\":30914,\"start\":30827},{\"end\":31484,\"start\":30972},{\"end\":31868,\"start\":31486},{\"end\":32245,\"start\":31870},{\"end\":32658,\"start\":32271},{\"end\":32724,\"start\":32660},{\"end\":33138,\"start\":32794},{\"end\":33296,\"start\":33204},{\"end\":33690,\"start\":33298},{\"end\":34619,\"start\":33809},{\"end\":34728,\"start\":34663},{\"end\":34771,\"start\":34730},{\"end\":35240,\"start\":34773},{\"end\":36513,\"start\":35278},{\"end\":36653,\"start\":36515},{\"end\":38020,\"start\":36676},{\"end\":39041,\"start\":38022},{\"end\":39108,\"start\":39053},{\"end\":39511,\"start\":39110},{\"end\":39968,\"start\":39513},{\"end\":40166,\"start\":39970},{\"end\":41381,\"start\":40168},{\"end\":41898,\"start\":41383},{\"end\":42732,\"start\":41900},{\"end\":43174,\"start\":42734},{\"end\":43635,\"start\":43176},{\"end\":44224,\"start\":43637},{\"end\":44348,\"start\":44226},{\"end\":45613,\"start\":44350},{\"end\":46330,\"start\":45615},{\"end\":47267,\"start\":46332},{\"end\":47902,\"start\":47269},{\"end\":48288,\"start\":47904},{\"end\":48768,\"start\":48290},{\"end\":49094,\"start\":48770},{\"end\":49680,\"start\":49096},{\"end\":50517,\"start\":49682},{\"end\":50865,\"start\":50519},{\"end\":51351,\"start\":50880},{\"end\":51740,\"start\":51353},{\"end\":52230,\"start\":51742},{\"end\":53003,\"start\":52232}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16162,\"start\":16118},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17121,\"start\":17096},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17245,\"start\":17187},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17507,\"start\":17484},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18137,\"start\":18068},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18476,\"start\":18443},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20900,\"start\":20873},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21520,\"start\":21504},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23903,\"start\":23860},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24354,\"start\":24332},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25434,\"start\":25327},{\"attributes\":{\"id\":\"formula_11\"},\"end\":26167,\"start\":26130},{\"attributes\":{\"id\":\"formula_12\"},\"end\":26599,\"start\":26568},{\"attributes\":{\"id\":\"formula_13\"},\"end\":27533,\"start\":27506},{\"attributes\":{\"id\":\"formula_14\"},\"end\":29414,\"start\":29381},{\"attributes\":{\"id\":\"formula_15\"},\"end\":30287,\"start\":30243},{\"attributes\":{\"id\":\"formula_16\"},\"end\":30367,\"start\":30347},{\"attributes\":{\"id\":\"formula_17\"},\"end\":30584,\"start\":30562},{\"attributes\":{\"id\":\"formula_18\"},\"end\":30826,\"start\":30805},{\"attributes\":{\"id\":\"formula_19\"},\"end\":30971,\"start\":30915},{\"attributes\":{\"id\":\"formula_20\"},\"end\":32793,\"start\":32725},{\"attributes\":{\"id\":\"formula_21\"},\"end\":33203,\"start\":33139},{\"attributes\":{\"id\":\"formula_22\"},\"end\":33808,\"start\":33691}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":38597,\"start\":38590},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":41593,\"start\":41586},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":48894,\"start\":48887},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":50516,\"start\":50509},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":50725,\"start\":50718}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":7344,\"start\":7332},{\"attributes\":{\"n\":\"2\"},\"end\":12090,\"start\":12063},{\"attributes\":{\"n\":\"2.1\"},\"end\":12224,\"start\":12200},{\"attributes\":{\"n\":\"2.2\"},\"end\":14212,\"start\":14188},{\"attributes\":{\"n\":\"2.3\"},\"end\":15319,\"start\":15299},{\"attributes\":{\"n\":\"2.4\"},\"end\":17912,\"start\":17893},{\"attributes\":{\"n\":\"2.5\"},\"end\":19402,\"start\":19379},{\"attributes\":{\"n\":\"2.6\"},\"end\":20147,\"start\":20126},{\"attributes\":{\"n\":\"3\"},\"end\":20615,\"start\":20609},{\"attributes\":{\"n\":\"3.1\"},\"end\":21382,\"start\":21353},{\"attributes\":{\"n\":\"3.2\"},\"end\":23657,\"start\":23643},{\"attributes\":{\"n\":\"3.3\"},\"end\":25578,\"start\":25553},{\"attributes\":{\"n\":\"3.4\"},\"end\":27318,\"start\":27293},{\"attributes\":{\"n\":\"3.5\"},\"end\":32269,\"start\":32248},{\"attributes\":{\"n\":\"3.6\"},\"end\":34661,\"start\":34622},{\"end\":35264,\"start\":35243},{\"attributes\":{\"n\":\"3.7\"},\"end\":35276,\"start\":35267},{\"attributes\":{\"n\":\"3.8\"},\"end\":36674,\"start\":36656},{\"attributes\":{\"n\":\"4\"},\"end\":39051,\"start\":39044},{\"attributes\":{\"n\":\"5\"},\"end\":50878,\"start\":50868},{\"end\":53013,\"start\":53005},{\"end\":53143,\"start\":53135},{\"end\":53656,\"start\":53648},{\"end\":53727,\"start\":53719},{\"end\":54636,\"start\":54628},{\"end\":54733,\"start\":54725},{\"end\":55404,\"start\":55395}]", "table": "[{\"end\":55737,\"start\":55463}]", "figure_caption": "[{\"end\":53133,\"start\":53015},{\"end\":53646,\"start\":53145},{\"end\":53717,\"start\":53658},{\"end\":53911,\"start\":53729},{\"end\":54626,\"start\":53914},{\"end\":54723,\"start\":54638},{\"end\":55320,\"start\":54735},{\"end\":55393,\"start\":55323},{\"end\":55463,\"start\":55406}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22164,\"start\":22158},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22627,\"start\":22618},{\"end\":34087,\"start\":34081},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34438,\"start\":34432},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":35399,\"start\":35384},{\"end\":39135,\"start\":39127},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":41412,\"start\":41404},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":43684,\"start\":43676},{\"end\":44597,\"start\":44589},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44882,\"start\":44875},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46758,\"start\":46749},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":49704,\"start\":49695},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50012,\"start\":50005},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":52126,\"start\":52118}]", "bib_author_first_name": "[{\"end\":56460,\"start\":56453},{\"end\":56487,\"start\":56482},{\"end\":56749,\"start\":56744},{\"end\":56764,\"start\":56758},{\"end\":57384,\"start\":57379},{\"end\":57397,\"start\":57393},{\"end\":57682,\"start\":57677},{\"end\":57695,\"start\":57691},{\"end\":58277,\"start\":58272},{\"end\":58290,\"start\":58286},{\"end\":58304,\"start\":58300},{\"end\":58318,\"start\":58312},{\"end\":58344,\"start\":58327},{\"end\":58612,\"start\":58605},{\"end\":58626,\"start\":58619},{\"end\":58628,\"start\":58627},{\"end\":58643,\"start\":58636},{\"end\":58645,\"start\":58644},{\"end\":58665,\"start\":58653},{\"end\":59004,\"start\":58995},{\"end\":59020,\"start\":59013},{\"end\":59039,\"start\":59034},{\"end\":59056,\"start\":59050},{\"end\":59400,\"start\":59396},{\"end\":59423,\"start\":59418},{\"end\":59435,\"start\":59431},{\"end\":59437,\"start\":59436},{\"end\":60160,\"start\":60153},{\"end\":60464,\"start\":60460},{\"end\":60482,\"start\":60473},{\"end\":60494,\"start\":60489},{\"end\":60511,\"start\":60506},{\"end\":60525,\"start\":60519},{\"end\":60543,\"start\":60534},{\"end\":61043,\"start\":61036},{\"end\":61060,\"start\":61056},{\"end\":61072,\"start\":61068},{\"end\":61426,\"start\":61419},{\"end\":61443,\"start\":61437},{\"end\":61649,\"start\":61645},{\"end\":61664,\"start\":61660},{\"end\":61676,\"start\":61672},{\"end\":61691,\"start\":61686},{\"end\":62047,\"start\":62046},{\"end\":62059,\"start\":62058},{\"end\":62268,\"start\":62260},{\"end\":62284,\"start\":62275},{\"end\":62300,\"start\":62293},{\"end\":62692,\"start\":62685},{\"end\":62706,\"start\":62702},{\"end\":62722,\"start\":62716},{\"end\":63044,\"start\":63043},{\"end\":63060,\"start\":63055},{\"end\":63281,\"start\":63280},{\"end\":63296,\"start\":63290},{\"end\":63312,\"start\":63307},{\"end\":63641,\"start\":63635},{\"end\":63654,\"start\":63649},{\"end\":63669,\"start\":63665},{\"end\":63685,\"start\":63678},{\"end\":63698,\"start\":63692},{\"end\":63713,\"start\":63709},{\"end\":64044,\"start\":64037},{\"end\":64054,\"start\":64049},{\"end\":64069,\"start\":64064},{\"end\":64084,\"start\":64078},{\"end\":64379,\"start\":64374},{\"end\":64392,\"start\":64388},{\"end\":64408,\"start\":64402},{\"end\":64424,\"start\":64419},{\"end\":64441,\"start\":64432},{\"end\":64459,\"start\":64449},{\"end\":65028,\"start\":65014},{\"end\":65041,\"start\":65034},{\"end\":65051,\"start\":65047},{\"end\":65398,\"start\":65391},{\"end\":65410,\"start\":65404},{\"end\":65421,\"start\":65415},{\"end\":65431,\"start\":65428},{\"end\":65818,\"start\":65817},{\"end\":65837,\"start\":65828},{\"end\":66135,\"start\":66126},{\"end\":66151,\"start\":66144},{\"end\":66170,\"start\":66164},{\"end\":66481,\"start\":66478},{\"end\":66495,\"start\":66494},{\"end\":66511,\"start\":66504},{\"end\":66532,\"start\":66524},{\"end\":66534,\"start\":66533},{\"end\":66547,\"start\":66543},{\"end\":66559,\"start\":66556},{\"end\":66981,\"start\":66976},{\"end\":67247,\"start\":67241},{\"end\":67261,\"start\":67256},{\"end\":67276,\"start\":67272},{\"end\":67290,\"start\":67286},{\"end\":68154,\"start\":68153},{\"end\":68156,\"start\":68155},{\"end\":68510,\"start\":68504},{\"end\":68534,\"start\":68525},{\"end\":68551,\"start\":68545},{\"end\":68564,\"start\":68558},{\"end\":68897,\"start\":68891},{\"end\":68917,\"start\":68912},{\"end\":68932,\"start\":68926},{\"end\":68948,\"start\":68942},{\"end\":69220,\"start\":69219},{\"end\":69233,\"start\":69230},{\"end\":69249,\"start\":69241},{\"end\":69262,\"start\":69256},{\"end\":69272,\"start\":69268},{\"end\":69287,\"start\":69278},{\"end\":69674,\"start\":69666},{\"end\":69686,\"start\":69681},{\"end\":69688,\"start\":69687},{\"end\":69704,\"start\":69698},{\"end\":69706,\"start\":69705},{\"end\":70041,\"start\":70037},{\"end\":70053,\"start\":70050},{\"end\":70070,\"start\":70061},{\"end\":70082,\"start\":70078},{\"end\":70095,\"start\":70090},{\"end\":70113,\"start\":70106},{\"end\":70128,\"start\":70122},{\"end\":70144,\"start\":70138},{\"end\":70157,\"start\":70150},{\"end\":70174,\"start\":70170},{\"end\":70188,\"start\":70183},{\"end\":70207,\"start\":70200},{\"end\":70220,\"start\":70214},{\"end\":70234,\"start\":70227},{\"end\":70249,\"start\":70243},{\"end\":70265,\"start\":70258},{\"end\":70280,\"start\":70274},{\"end\":70301,\"start\":70295},{\"end\":70313,\"start\":70311},{\"end\":71244,\"start\":71237},{\"end\":71258,\"start\":71252},{\"end\":71524,\"start\":71518},{\"end\":71540,\"start\":71534},{\"end\":71768,\"start\":71761},{\"end\":71784,\"start\":71781},{\"end\":71801,\"start\":71794},{\"end\":71815,\"start\":71811},{\"end\":72147,\"start\":72143},{\"end\":72164,\"start\":72161},{\"end\":72553,\"start\":72546},{\"end\":72566,\"start\":72560},{\"end\":72585,\"start\":72580},{\"end\":72601,\"start\":72595},{\"end\":72617,\"start\":72610},{\"end\":72628,\"start\":72622},{\"end\":72645,\"start\":72638},{\"end\":72982,\"start\":72976},{\"end\":72998,\"start\":72992},{\"end\":73013,\"start\":73006},{\"end\":73314,\"start\":73310},{\"end\":73580,\"start\":73576},{\"end\":73857,\"start\":73850},{\"end\":73877,\"start\":73871},{\"end\":73893,\"start\":73886},{\"end\":74116,\"start\":74111},{\"end\":74134,\"start\":74125},{\"end\":74151,\"start\":74145},{\"end\":74676,\"start\":74674},{\"end\":74689,\"start\":74683},{\"end\":74970,\"start\":74962},{\"end\":74993,\"start\":74987},{\"end\":75005,\"start\":74999},{\"end\":75393,\"start\":75388},{\"end\":75404,\"start\":75398},{\"end\":75423,\"start\":75417},{\"end\":75618,\"start\":75612},{\"end\":75637,\"start\":75628},{\"end\":75654,\"start\":75648},{\"end\":75671,\"start\":75665},{\"end\":76068,\"start\":76063},{\"end\":76080,\"start\":76076},{\"end\":76094,\"start\":76087},{\"end\":76110,\"start\":76104},{\"end\":76315,\"start\":76310},{\"end\":76329,\"start\":76323},{\"end\":76341,\"start\":76338},{\"end\":76634,\"start\":76629},{\"end\":76647,\"start\":76642},{\"end\":76659,\"start\":76652},{\"end\":76674,\"start\":76667},{\"end\":76691,\"start\":76687},{\"end\":76711,\"start\":76705},{\"end\":77018,\"start\":77012},{\"end\":77030,\"start\":77024},{\"end\":77045,\"start\":77040},{\"end\":77047,\"start\":77046}]", "bib_author_last_name": "[{\"end\":56480,\"start\":56461},{\"end\":56490,\"start\":56488},{\"end\":56498,\"start\":56492},{\"end\":56756,\"start\":56750},{\"end\":56771,\"start\":56765},{\"end\":57391,\"start\":57385},{\"end\":57405,\"start\":57398},{\"end\":57689,\"start\":57683},{\"end\":57703,\"start\":57696},{\"end\":58284,\"start\":58278},{\"end\":58298,\"start\":58291},{\"end\":58310,\"start\":58305},{\"end\":58325,\"start\":58319},{\"end\":58351,\"start\":58345},{\"end\":58617,\"start\":58613},{\"end\":58634,\"start\":58629},{\"end\":58651,\"start\":58646},{\"end\":58678,\"start\":58666},{\"end\":59011,\"start\":59005},{\"end\":59032,\"start\":59021},{\"end\":59048,\"start\":59040},{\"end\":59064,\"start\":59057},{\"end\":59416,\"start\":59401},{\"end\":59429,\"start\":59424},{\"end\":59446,\"start\":59438},{\"end\":59452,\"start\":59448},{\"end\":60166,\"start\":60161},{\"end\":60471,\"start\":60465},{\"end\":60487,\"start\":60483},{\"end\":60504,\"start\":60495},{\"end\":60517,\"start\":60512},{\"end\":60532,\"start\":60526},{\"end\":60551,\"start\":60544},{\"end\":61054,\"start\":61044},{\"end\":61066,\"start\":61061},{\"end\":61080,\"start\":61073},{\"end\":61435,\"start\":61427},{\"end\":61451,\"start\":61444},{\"end\":61658,\"start\":61650},{\"end\":61670,\"start\":61665},{\"end\":61684,\"start\":61677},{\"end\":61697,\"start\":61692},{\"end\":62056,\"start\":62048},{\"end\":62065,\"start\":62060},{\"end\":62273,\"start\":62269},{\"end\":62291,\"start\":62285},{\"end\":62307,\"start\":62301},{\"end\":62700,\"start\":62693},{\"end\":62714,\"start\":62707},{\"end\":62731,\"start\":62723},{\"end\":63053,\"start\":63045},{\"end\":63067,\"start\":63061},{\"end\":63071,\"start\":63069},{\"end\":63288,\"start\":63282},{\"end\":63305,\"start\":63297},{\"end\":63318,\"start\":63313},{\"end\":63326,\"start\":63320},{\"end\":63647,\"start\":63642},{\"end\":63663,\"start\":63655},{\"end\":63676,\"start\":63670},{\"end\":63690,\"start\":63686},{\"end\":63707,\"start\":63699},{\"end\":63718,\"start\":63714},{\"end\":64047,\"start\":64045},{\"end\":64062,\"start\":64055},{\"end\":64076,\"start\":64070},{\"end\":64093,\"start\":64085},{\"end\":64386,\"start\":64380},{\"end\":64400,\"start\":64393},{\"end\":64417,\"start\":64409},{\"end\":64430,\"start\":64425},{\"end\":64447,\"start\":64442},{\"end\":64466,\"start\":64460},{\"end\":65032,\"start\":65029},{\"end\":65045,\"start\":65042},{\"end\":65060,\"start\":65052},{\"end\":65402,\"start\":65399},{\"end\":65413,\"start\":65411},{\"end\":65426,\"start\":65422},{\"end\":65434,\"start\":65432},{\"end\":65826,\"start\":65819},{\"end\":65843,\"start\":65838},{\"end\":65850,\"start\":65845},{\"end\":66142,\"start\":66136},{\"end\":66162,\"start\":66152},{\"end\":66176,\"start\":66171},{\"end\":66492,\"start\":66482},{\"end\":66502,\"start\":66496},{\"end\":66522,\"start\":66512},{\"end\":66541,\"start\":66535},{\"end\":66554,\"start\":66548},{\"end\":66571,\"start\":66560},{\"end\":66575,\"start\":66573},{\"end\":66988,\"start\":66982},{\"end\":67254,\"start\":67248},{\"end\":67270,\"start\":67262},{\"end\":67284,\"start\":67277},{\"end\":67296,\"start\":67291},{\"end\":68166,\"start\":68157},{\"end\":68523,\"start\":68511},{\"end\":68543,\"start\":68535},{\"end\":68556,\"start\":68552},{\"end\":68570,\"start\":68565},{\"end\":68910,\"start\":68898},{\"end\":68924,\"start\":68918},{\"end\":68940,\"start\":68933},{\"end\":68954,\"start\":68949},{\"end\":69228,\"start\":69221},{\"end\":69239,\"start\":69234},{\"end\":69254,\"start\":69250},{\"end\":69266,\"start\":69263},{\"end\":69276,\"start\":69273},{\"end\":69292,\"start\":69288},{\"end\":69297,\"start\":69294},{\"end\":69679,\"start\":69675},{\"end\":69696,\"start\":69689},{\"end\":69711,\"start\":69707},{\"end\":70048,\"start\":70042},{\"end\":70059,\"start\":70054},{\"end\":70076,\"start\":70071},{\"end\":70088,\"start\":70083},{\"end\":70104,\"start\":70096},{\"end\":70120,\"start\":70114},{\"end\":70136,\"start\":70129},{\"end\":70148,\"start\":70145},{\"end\":70168,\"start\":70158},{\"end\":70181,\"start\":70175},{\"end\":70198,\"start\":70189},{\"end\":70212,\"start\":70208},{\"end\":70225,\"start\":70221},{\"end\":70241,\"start\":70235},{\"end\":70256,\"start\":70250},{\"end\":70272,\"start\":70266},{\"end\":70293,\"start\":70281},{\"end\":70309,\"start\":70302},{\"end\":70318,\"start\":70314},{\"end\":71250,\"start\":71245},{\"end\":71264,\"start\":71259},{\"end\":71532,\"start\":71525},{\"end\":71549,\"start\":71541},{\"end\":71779,\"start\":71769},{\"end\":71792,\"start\":71785},{\"end\":71809,\"start\":71802},{\"end\":71831,\"start\":71816},{\"end\":72159,\"start\":72148},{\"end\":72173,\"start\":72165},{\"end\":72558,\"start\":72554},{\"end\":72578,\"start\":72567},{\"end\":72593,\"start\":72586},{\"end\":72608,\"start\":72602},{\"end\":72620,\"start\":72618},{\"end\":72636,\"start\":72629},{\"end\":72654,\"start\":72646},{\"end\":72990,\"start\":72983},{\"end\":73004,\"start\":72999},{\"end\":73019,\"start\":73014},{\"end\":73322,\"start\":73315},{\"end\":73588,\"start\":73581},{\"end\":73869,\"start\":73858},{\"end\":73884,\"start\":73878},{\"end\":73898,\"start\":73894},{\"end\":74123,\"start\":74117},{\"end\":74143,\"start\":74135},{\"end\":74157,\"start\":74152},{\"end\":74681,\"start\":74677},{\"end\":74697,\"start\":74690},{\"end\":74985,\"start\":74971},{\"end\":74997,\"start\":74994},{\"end\":75011,\"start\":75006},{\"end\":75396,\"start\":75394},{\"end\":75415,\"start\":75405},{\"end\":75429,\"start\":75424},{\"end\":75626,\"start\":75619},{\"end\":75646,\"start\":75638},{\"end\":75663,\"start\":75655},{\"end\":75677,\"start\":75672},{\"end\":76074,\"start\":76069},{\"end\":76085,\"start\":76081},{\"end\":76102,\"start\":76095},{\"end\":76115,\"start\":76111},{\"end\":76321,\"start\":76316},{\"end\":76336,\"start\":76330},{\"end\":76345,\"start\":76342},{\"end\":76640,\"start\":76635},{\"end\":76650,\"start\":76648},{\"end\":76665,\"start\":76660},{\"end\":76685,\"start\":76675},{\"end\":76703,\"start\":76692},{\"end\":76716,\"start\":76712},{\"end\":77022,\"start\":77019},{\"end\":77038,\"start\":77031},{\"end\":77055,\"start\":77048}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":227179735},\"end\":56709,\"start\":56393},{\"attributes\":{\"doi\":\"10.1145/280814.280821\",\"id\":\"b1\",\"matched_paper_id\":326544},\"end\":57321,\"start\":56711},{\"attributes\":{\"doi\":\"10.1145/1015706.1015772\",\"id\":\"b2\",\"matched_paper_id\":185313},\"end\":57625,\"start\":57323},{\"attributes\":{\"doi\":\"10.2312/SGP/SGP04/189-196\",\"id\":\"b3\",\"matched_paper_id\":16924063},\"end\":58245,\"start\":57627},{\"attributes\":{\"id\":\"b4\"},\"end\":58517,\"start\":58247},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":207168521},\"end\":58935,\"start\":58519},{\"attributes\":{\"doi\":\"10.1111/cgf.13243\",\"id\":\"b6\",\"matched_paper_id\":960334},\"end\":59321,\"start\":58937},{\"attributes\":{\"doi\":\"10.1145/311535.311576\",\"id\":\"b7\",\"matched_paper_id\":7773119},\"end\":60086,\"start\":59323},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":116141783},\"end\":60401,\"start\":60088},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":49300163},\"end\":60946,\"start\":60403},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":34735234},\"end\":61335,\"start\":60948},{\"attributes\":{\"id\":\"b11\"},\"end\":61587,\"start\":61337},{\"attributes\":{\"doi\":\"10.1111/j.1467-8659.2010.01765.x\",\"id\":\"b12\",\"matched_paper_id\":1758684},\"end\":61969,\"start\":61589},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":123337530},\"end\":62233,\"start\":61971},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":32389979},\"end\":62626,\"start\":62235},{\"attributes\":{\"doi\":\"10.1111/j.1467-8659.2012.03179.x\",\"id\":\"b15\",\"matched_paper_id\":2862753},\"end\":62997,\"start\":62628},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b16\"},\"end\":63222,\"start\":62999},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2960354},\"end\":63567,\"start\":63224},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":226278056},\"end\":63973,\"start\":63569},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":52839714},\"end\":64317,\"start\":63975},{\"attributes\":{\"doi\":\"10.1109/SMI.2004.1314505\",\"id\":\"b20\",\"matched_paper_id\":9043256},\"end\":64946,\"start\":64319},{\"attributes\":{\"doi\":\"10.1145/3272127.3275047\",\"id\":\"b21\",\"matched_paper_id\":52840642},\"end\":65316,\"start\":64948},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":102484000},\"end\":65767,\"start\":65318},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17868098},\"end\":66052,\"start\":65769},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":204853209},\"end\":66404,\"start\":66054},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":213175590},\"end\":66868,\"start\":66406},{\"attributes\":{\"doi\":\"NVR-2011\",\"id\":\"b26\",\"matched_paper_id\":14770321},\"end\":67210,\"start\":66870},{\"attributes\":{\"doi\":\"10.1145/1174429.1174494\",\"id\":\"b27\",\"matched_paper_id\":608037},\"end\":68106,\"start\":67212},{\"attributes\":{\"doi\":\"10.2969/jmsj/03720187\",\"id\":\"b28\",\"matched_paper_id\":123056203},\"end\":68412,\"start\":68108},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":221315581},\"end\":68833,\"start\":68414},{\"attributes\":{\"id\":\"b30\"},\"end\":69179,\"start\":68835},{\"attributes\":{\"doi\":\"arXiv:1806.06317\",\"id\":\"b31\"},\"end\":69519,\"start\":69181},{\"attributes\":{\"doi\":\"arXiv:math.NA/2006.06732\",\"id\":\"b32\"},\"end\":69965,\"start\":69521},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":202786778},\"end\":71195,\"start\":69967},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":7491882},\"end\":71458,\"start\":71197},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":10068306},\"end\":71722,\"start\":71460},{\"attributes\":{\"doi\":\"10.1145/2983621\",\"id\":\"b36\",\"matched_paper_id\":65426139},\"end\":72080,\"start\":71724},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6496578},\"end\":72544,\"start\":72082},{\"attributes\":{\"doi\":\"arXiv:2007.08501\",\"id\":\"b38\"},\"end\":72911,\"start\":72546},{\"attributes\":{\"id\":\"b39\"},\"end\":73281,\"start\":72913},{\"attributes\":{\"doi\":\"10.2312/egst.20051044\",\"id\":\"b40\",\"matched_paper_id\":2122468},\"end\":73524,\"start\":73283},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":11465579},\"end\":73773,\"start\":73526},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":41036513},\"end\":74109,\"start\":73775},{\"attributes\":{\"doi\":\"10.1145/3450626.3459804\",\"id\":\"b43\"},\"end\":74610,\"start\":74111},{\"attributes\":{\"doi\":\"10.1145/3450626.3459801\",\"id\":\"b44\",\"matched_paper_id\":235445469},\"end\":74932,\"start\":74612},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":226976024},\"end\":75368,\"start\":74934},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":219686978},\"end\":75553,\"start\":75370},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":235469009},\"end\":75999,\"start\":75555},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":236004156},\"end\":76308,\"start\":76001},{\"attributes\":{\"id\":\"b49\"},\"end\":76582,\"start\":76310},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":202713285},\"end\":76954,\"start\":76584},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":51868186},\"end\":77275,\"start\":76956}]", "bib_title": "[{\"end\":56451,\"start\":56393},{\"end\":56742,\"start\":56711},{\"end\":57377,\"start\":57323},{\"end\":57675,\"start\":57627},{\"end\":58603,\"start\":58519},{\"end\":58993,\"start\":58937},{\"end\":59394,\"start\":59323},{\"end\":60151,\"start\":60088},{\"end\":60458,\"start\":60403},{\"end\":61034,\"start\":60948},{\"end\":61643,\"start\":61589},{\"end\":62044,\"start\":61971},{\"end\":62258,\"start\":62235},{\"end\":62683,\"start\":62628},{\"end\":63278,\"start\":63224},{\"end\":63633,\"start\":63569},{\"end\":64035,\"start\":63975},{\"end\":64372,\"start\":64319},{\"end\":65012,\"start\":64948},{\"end\":65389,\"start\":65318},{\"end\":65815,\"start\":65769},{\"end\":66124,\"start\":66054},{\"end\":66476,\"start\":66406},{\"end\":66974,\"start\":66870},{\"end\":67239,\"start\":67212},{\"end\":68151,\"start\":68108},{\"end\":68502,\"start\":68414},{\"end\":68889,\"start\":68835},{\"end\":70035,\"start\":69967},{\"end\":71235,\"start\":71197},{\"end\":71516,\"start\":71460},{\"end\":71759,\"start\":71724},{\"end\":72141,\"start\":72082},{\"end\":72974,\"start\":72913},{\"end\":73308,\"start\":73283},{\"end\":73574,\"start\":73526},{\"end\":73848,\"start\":73775},{\"end\":74672,\"start\":74612},{\"end\":74960,\"start\":74934},{\"end\":75386,\"start\":75370},{\"end\":75610,\"start\":75555},{\"end\":76061,\"start\":76001},{\"end\":76627,\"start\":76584},{\"end\":77010,\"start\":76956}]", "bib_author": "[{\"end\":56482,\"start\":56453},{\"end\":56492,\"start\":56482},{\"end\":56500,\"start\":56492},{\"end\":56758,\"start\":56744},{\"end\":56773,\"start\":56758},{\"end\":57393,\"start\":57379},{\"end\":57407,\"start\":57393},{\"end\":57691,\"start\":57677},{\"end\":57705,\"start\":57691},{\"end\":58286,\"start\":58272},{\"end\":58300,\"start\":58286},{\"end\":58312,\"start\":58300},{\"end\":58327,\"start\":58312},{\"end\":58353,\"start\":58327},{\"end\":58619,\"start\":58605},{\"end\":58636,\"start\":58619},{\"end\":58653,\"start\":58636},{\"end\":58680,\"start\":58653},{\"end\":59013,\"start\":58995},{\"end\":59034,\"start\":59013},{\"end\":59050,\"start\":59034},{\"end\":59066,\"start\":59050},{\"end\":59418,\"start\":59396},{\"end\":59431,\"start\":59418},{\"end\":59448,\"start\":59431},{\"end\":59454,\"start\":59448},{\"end\":60168,\"start\":60153},{\"end\":60473,\"start\":60460},{\"end\":60489,\"start\":60473},{\"end\":60506,\"start\":60489},{\"end\":60519,\"start\":60506},{\"end\":60534,\"start\":60519},{\"end\":60553,\"start\":60534},{\"end\":61056,\"start\":61036},{\"end\":61068,\"start\":61056},{\"end\":61082,\"start\":61068},{\"end\":61437,\"start\":61419},{\"end\":61453,\"start\":61437},{\"end\":61660,\"start\":61645},{\"end\":61672,\"start\":61660},{\"end\":61686,\"start\":61672},{\"end\":61699,\"start\":61686},{\"end\":62058,\"start\":62046},{\"end\":62067,\"start\":62058},{\"end\":62275,\"start\":62260},{\"end\":62293,\"start\":62275},{\"end\":62309,\"start\":62293},{\"end\":62702,\"start\":62685},{\"end\":62716,\"start\":62702},{\"end\":62733,\"start\":62716},{\"end\":63055,\"start\":63043},{\"end\":63069,\"start\":63055},{\"end\":63073,\"start\":63069},{\"end\":63290,\"start\":63280},{\"end\":63307,\"start\":63290},{\"end\":63320,\"start\":63307},{\"end\":63328,\"start\":63320},{\"end\":63649,\"start\":63635},{\"end\":63665,\"start\":63649},{\"end\":63678,\"start\":63665},{\"end\":63692,\"start\":63678},{\"end\":63709,\"start\":63692},{\"end\":63720,\"start\":63709},{\"end\":64049,\"start\":64037},{\"end\":64064,\"start\":64049},{\"end\":64078,\"start\":64064},{\"end\":64095,\"start\":64078},{\"end\":64388,\"start\":64374},{\"end\":64402,\"start\":64388},{\"end\":64419,\"start\":64402},{\"end\":64432,\"start\":64419},{\"end\":64449,\"start\":64432},{\"end\":64468,\"start\":64449},{\"end\":65034,\"start\":65014},{\"end\":65047,\"start\":65034},{\"end\":65062,\"start\":65047},{\"end\":65404,\"start\":65391},{\"end\":65415,\"start\":65404},{\"end\":65428,\"start\":65415},{\"end\":65436,\"start\":65428},{\"end\":65828,\"start\":65817},{\"end\":65845,\"start\":65828},{\"end\":65852,\"start\":65845},{\"end\":66144,\"start\":66126},{\"end\":66164,\"start\":66144},{\"end\":66178,\"start\":66164},{\"end\":66494,\"start\":66478},{\"end\":66504,\"start\":66494},{\"end\":66524,\"start\":66504},{\"end\":66543,\"start\":66524},{\"end\":66556,\"start\":66543},{\"end\":66573,\"start\":66556},{\"end\":66577,\"start\":66573},{\"end\":66990,\"start\":66976},{\"end\":67256,\"start\":67241},{\"end\":67272,\"start\":67256},{\"end\":67286,\"start\":67272},{\"end\":67298,\"start\":67286},{\"end\":68168,\"start\":68153},{\"end\":68525,\"start\":68504},{\"end\":68545,\"start\":68525},{\"end\":68558,\"start\":68545},{\"end\":68572,\"start\":68558},{\"end\":68912,\"start\":68891},{\"end\":68926,\"start\":68912},{\"end\":68942,\"start\":68926},{\"end\":68956,\"start\":68942},{\"end\":69230,\"start\":69219},{\"end\":69241,\"start\":69230},{\"end\":69256,\"start\":69241},{\"end\":69268,\"start\":69256},{\"end\":69278,\"start\":69268},{\"end\":69294,\"start\":69278},{\"end\":69299,\"start\":69294},{\"end\":69681,\"start\":69666},{\"end\":69698,\"start\":69681},{\"end\":69713,\"start\":69698},{\"end\":70050,\"start\":70037},{\"end\":70061,\"start\":70050},{\"end\":70078,\"start\":70061},{\"end\":70090,\"start\":70078},{\"end\":70106,\"start\":70090},{\"end\":70122,\"start\":70106},{\"end\":70138,\"start\":70122},{\"end\":70150,\"start\":70138},{\"end\":70170,\"start\":70150},{\"end\":70183,\"start\":70170},{\"end\":70200,\"start\":70183},{\"end\":70214,\"start\":70200},{\"end\":70227,\"start\":70214},{\"end\":70243,\"start\":70227},{\"end\":70258,\"start\":70243},{\"end\":70274,\"start\":70258},{\"end\":70295,\"start\":70274},{\"end\":70311,\"start\":70295},{\"end\":70320,\"start\":70311},{\"end\":71252,\"start\":71237},{\"end\":71266,\"start\":71252},{\"end\":71534,\"start\":71518},{\"end\":71551,\"start\":71534},{\"end\":71781,\"start\":71761},{\"end\":71794,\"start\":71781},{\"end\":71811,\"start\":71794},{\"end\":71833,\"start\":71811},{\"end\":72161,\"start\":72143},{\"end\":72175,\"start\":72161},{\"end\":72560,\"start\":72546},{\"end\":72580,\"start\":72560},{\"end\":72595,\"start\":72580},{\"end\":72610,\"start\":72595},{\"end\":72622,\"start\":72610},{\"end\":72638,\"start\":72622},{\"end\":72656,\"start\":72638},{\"end\":72992,\"start\":72976},{\"end\":73006,\"start\":72992},{\"end\":73021,\"start\":73006},{\"end\":73324,\"start\":73310},{\"end\":73590,\"start\":73576},{\"end\":73871,\"start\":73850},{\"end\":73886,\"start\":73871},{\"end\":73900,\"start\":73886},{\"end\":74125,\"start\":74111},{\"end\":74145,\"start\":74125},{\"end\":74159,\"start\":74145},{\"end\":74683,\"start\":74674},{\"end\":74699,\"start\":74683},{\"end\":74987,\"start\":74962},{\"end\":74999,\"start\":74987},{\"end\":75013,\"start\":74999},{\"end\":75398,\"start\":75388},{\"end\":75417,\"start\":75398},{\"end\":75431,\"start\":75417},{\"end\":75628,\"start\":75612},{\"end\":75648,\"start\":75628},{\"end\":75665,\"start\":75648},{\"end\":75679,\"start\":75665},{\"end\":76076,\"start\":76063},{\"end\":76087,\"start\":76076},{\"end\":76104,\"start\":76087},{\"end\":76117,\"start\":76104},{\"end\":76323,\"start\":76310},{\"end\":76338,\"start\":76323},{\"end\":76347,\"start\":76338},{\"end\":76642,\"start\":76629},{\"end\":76652,\"start\":76642},{\"end\":76667,\"start\":76652},{\"end\":76687,\"start\":76667},{\"end\":76705,\"start\":76687},{\"end\":76718,\"start\":76705},{\"end\":77024,\"start\":77012},{\"end\":77040,\"start\":77024},{\"end\":77057,\"start\":77040}]", "bib_venue": "[{\"end\":56534,\"start\":56500},{\"end\":56898,\"start\":56794},{\"end\":57446,\"start\":57430},{\"end\":57782,\"start\":57730},{\"end\":58270,\"start\":58247},{\"end\":58702,\"start\":58680},{\"end\":59103,\"start\":59083},{\"end\":59579,\"start\":59475},{\"end\":60225,\"start\":60168},{\"end\":60630,\"start\":60553},{\"end\":61120,\"start\":61082},{\"end\":61417,\"start\":61337},{\"end\":61751,\"start\":61731},{\"end\":62091,\"start\":62067},{\"end\":62386,\"start\":62309},{\"end\":62785,\"start\":62765},{\"end\":63041,\"start\":62999},{\"end\":63385,\"start\":63328},{\"end\":63754,\"start\":63720},{\"end\":64129,\"start\":64095},{\"end\":64567,\"start\":64492},{\"end\":65101,\"start\":65085},{\"end\":65503,\"start\":65436},{\"end\":65890,\"start\":65852},{\"end\":66212,\"start\":66178},{\"end\":66615,\"start\":66577},{\"end\":67009,\"start\":66998},{\"end\":67450,\"start\":67321},{\"end\":68233,\"start\":68189},{\"end\":68606,\"start\":68572},{\"end\":68990,\"start\":68956},{\"end\":69217,\"start\":69181},{\"end\":69664,\"start\":69521},{\"end\":70369,\"start\":70320},{\"end\":71289,\"start\":71266},{\"end\":71575,\"start\":71551},{\"end\":71864,\"start\":71848},{\"end\":72264,\"start\":72175},{\"end\":72717,\"start\":72672},{\"end\":73069,\"start\":73021},{\"end\":73389,\"start\":73345},{\"end\":73613,\"start\":73590},{\"end\":73923,\"start\":73900},{\"end\":74328,\"start\":74182},{\"end\":74748,\"start\":74732},{\"end\":75100,\"start\":75013},{\"end\":75447,\"start\":75431},{\"end\":75752,\"start\":75702},{\"end\":76133,\"start\":76117},{\"end\":76425,\"start\":76347},{\"end\":76752,\"start\":76718},{\"end\":77095,\"start\":77057},{\"end\":57006,\"start\":56900},{\"end\":57836,\"start\":57824},{\"end\":59711,\"start\":59602},{\"end\":60694,\"start\":60632},{\"end\":62450,\"start\":62388},{\"end\":64582,\"start\":64569},{\"end\":65557,\"start\":65505},{\"end\":67669,\"start\":67533},{\"end\":72340,\"start\":72266},{\"end\":73082,\"start\":73071},{\"end\":73629,\"start\":73615},{\"end\":75174,\"start\":75102}]"}}}, "year": 2023, "month": 12, "day": 17}