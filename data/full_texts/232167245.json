{"id": 232167245, "updated": "2023-10-06 05:37:13.001", "metadata": {"title": "ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis", "authors": "[{\"first\":\"Yinan\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Bei\",\"last\":\"Gan\",\"middle\":[]},{\"first\":\"Siyu\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yichun\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Guojun\",\"last\":\"Yin\",\"middle\":[]},{\"first\":\"Luchuan\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Lu\",\"last\":\"Sheng\",\"middle\":[]},{\"first\":\"Jing\",\"last\":\"Shao\",\"middle\":[]},{\"first\":\"Ziwei\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2021, "month": 3, "day": 9}, "abstract": "The rapid progress of photorealistic synthesis techniques has reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis. To counter this emerging threat, we construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image- and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding source real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We perform extensive benchmarking and studies of existing face forensics methods and obtain several valuable observations.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.05630", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/HeGCZYSSS021", "doi": "10.1109/cvpr46437.2021.00434"}}, "content": {"source": {"pdf_hash": "1b20d22cebe2d591530abd297b7d519176ebdfe5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.05630v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2103.05630", "status": "GREEN"}}, "grobid": {"id": "7f6808938cd749dc6431aa39d5509d4183c9ed11", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1b20d22cebe2d591530abd297b7d519176ebdfe5.txt", "contents": "\nForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis\n\n\nYinan He heyinan@sensetime.com \nSenseTime Research\n\n\nBeijing University of Posts and Telecommunications\n\n\nBei Gan ganbei@sensetime.com \nSenseTime Research\n\n\nShanghai AI Laboratory\n\n\nSiyu Chen chensiyu@sensetime.com \nSenseTime Research\n\n\nShanghai AI Laboratory\n\n\nYichun Zhou \nSenseTime Research\n\n\nCollege of Software\nBeihang University\n\n\nGuojun Yin yinguojun@sensetime.com \nShanghai AI Laboratory\n\n\nLuchuan Song \nSenseTime Research\n\n\nUniversity of Science and Technology of China\n6 S-Lab\n\nNanyang Technological University\n\n\n\u2020 Lu Sheng lsheng@buaa.edu.cn \nCollege of Software\nBeihang University\n\n\nJing Shao shaojing@sensetime.com \nSenseTime Research\n\n\nShanghai AI Laboratory\n\n\nZiwei Liu \nForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis\n# Images* # Videos UADFV DF-TIMIT DFFD FF++ Celeb-DF DFD DeeperForensics-1.0 DFDC ForgeryNet 10 ! 10 \" 10 # 10 $ 10 %\nThe label of images inFig. 1(a)(from left to r-arXiv:2103.05630v2 [cs.CV] 14 Jul 2021 proaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We perform extensive benchmarking and studies of existing face forensics methods and obtain several valuable observations. We hope that the scale, quality, and variety of our ForgeryNet dataset will foster further research and innovation in the area of face forgery classification, as well as spatial and temporal forgery localization etc.\n\nAbstract\n\nThe rapid progress of photorealistic synthesis techniques have reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis.\n\nTo counter this emerging threat, we construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image-and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identityremained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level ap-\n\n\nIntroduction\n\nPhotorealistic facial forgery technologies, especially recent deep learning driven approaches [23,38,49], give rise to widespread social concerns on potential malicious abuse of these techniques to eye-cheatingly forge media (i.e., images and videos, etc.) of human faces. Therefore, it is of vital importance to develop reliable methods for face forgery analysis 1 , so as to distinguish whether and where an image or video is manipulated.\n\nMost recent progress about face forgery analysis are sparked by gathering of face forgery detection datasets [18,52] and early attempts of profiling intrinsic characteristics within the forgery images. However, performances on most datasets have already saturated (i.e. over 99% accuracy [26,32,46,61]) due to their limited scales (e.g. number of images/videos and subject identities) and limited diversity (e.g. forgery approaches, scenarios, realistic perturbations, etc.). Moreover, in practical applications, it is often required to detect forged faces by locating tampered areas in an image and/or manipulated segments in an untrimmed video, rather than merely providing a binary label.\n\nIn this paper, we construct a new mega-scale dataset named ForgeryNet with comprehensive annotations, consisting of two groups (i.e. image-and video-level) and four tasks for real-world digital forgery analysis. We carefully benchmark existing forensics methods on ForgeryNet. Extensive experiments and in-depth analysis show that this larger and richer annotated dataset can boost the development of next-generation algorithms for forgery analysis. Specifically, ForgeryNet brings several unique advantages over existing datasets.\n\n(1) Wild Original Data. Most current datasets are captured under controlled conditions (e.g. environment, angles and lighting). We collect original data with diversified dimensions of angle, expression, identity, lighting, scenario and etc. from four datasets [7,13,20,45]. Note that all the orig-inal data have a Creative Commons Attribution license that allows to share and adapt the material.\n\n(2) Various Forgery Approaches. There are at most 8 forgery approaches in all current datasets, while ForgeryNet is manipulated by 15 approaches, including face transfer, face swap, face reenactment and face editing. We choose approaches that span a variety of learning-based models, including encoder-decoder structure, generative adversarial network, graphics formation and RNN/LSTM (Fig. 4).\n\n(3) Diverse Re-rendering Process. In the process of transmission and re-rendering, media data (image/video) always undergo compression, blurring and other operations, which may smooth the traces of forgery and bring more challenge for forgery detection. The ForgeryNet dataset posts 36 perturbations, such as optical distortion, multiplicative noise, random compression, blur, and etc. As shown in Fig. 1(c), circle sizes refer to the number of forgery approaches with re-rendering process operations. (4) Rich Annotations and Comprehensive Tasks. According to the real application scenario, we propose four tasks, as shown in Fig. 1(b): 1) Image Forgery Classification, distinguishes whether an image is forgery or not and meanwhile tells its forgery type (i.e. manipulation approaches). We provide three types of annotations including two-way, three-way and n-way classification. Both intra-and crossforgery evaluations are set on three-way and n-way settings. 2) Spatial Forgery Localization, localizes manipulated areas of forgery images. Due to the fact that a forgery image may contain multiple faces and can be manipulated entirely or in part, it is more substantial to segment modified pixels in addition to only telling that it is forged. 3) Video Forgery Classification, similar to image-level classification, contains three types of annotations. Note that different from existing forgery video datasets, we construct our video dataset with untrimmed videos, each of which has part of the frames manipulated, considering the fact that forgery videos in real world are often manipulated on a certain subject and some key frames. 4) Temporal Forgery Localization, localizes the temporal segments which are manipulated. This is a new task for forgery analysis. Together with Video Forgery Classification and Spatial Forgery Localization, it provides comprehensive spatio-temporal forgery annotations.\n\n\nRelated Works\n\nDue to the urgency in detecting face manipulation, many efforts have been devoted to creating face forgery detection datasets. Previous datasets can be grouped down into three generations. Their statistical information is listed in Tab. 1. The first generation consists of datasets such as DF-TIMIT [36], UADFV [60], SwapMe and FaceSwap [64]. DF-TIMIT manually selects 16 pairs of appearance-similar people from the publicly available VidTIMIT database, and generates 640 videos with faces swapped. UADFV contains The second generation includes Google DeepFake Detection dataset [4] with 3,068 forgery videos by five publicly available manipulation approaches, and Celeb-DF [39] containing 590 YouTube real videos mostly from celebrities and 5,639 manipulated video clips. FaceForensics++ [52] consists of 4000 fake videos manipulated by four approaches (i.e. DeepFakes, Face2Face, FaceSwap and Neu-ralTextures), and 1000 real videos from YouTube. The data scale and quality of the second generation have been improved. However, these datasets still lack diversity in forgery approaches and task annotations, and are not wellsuited for challenges encountered in real world.\n\nThe third generation datasets are the most recent face forgery datasets, i.e. DeeperForensics-1.0 [33], DFDC [18], and DFFD [14] which contains tens of thousands of videos and tens of millions of frames. DeeperForensics-1.0 consists of 60,000 videos for real-world face forgery detection. DFDC contains over 100,000 clips sourced from 960 paid actors, produced with several face replacement forgery approaches including learnable and non-learnable approaches.\n\nIn a practical application, in addition to classification, it is necessary to locate the manipulated areas or segments in an image or an untrimmed video. A few datasets have taken these tasks into consideration. DFFD provides annotations of spatial forgery at the first time, yet it only presents binary masks without manipulation density.\n\n\nID-replaced\n\nDeepFakes FSGAN\n\n\nBlendFace\n\nFaceShifter ID-remained Target\n(a) (b) StarGAN2 MaskGAN StyleGAN2 SC-FEGAN DiscoFace GAN 2)\n\nATVG-Net FirstOrder Motion\n\nTalking-headVideo \n1) 2) 1) 3) SBS DSS ! Target ! Source \" MM Replacement\n\nForgeryNet Construction\n\nMost of existing public face forgery datasets [4,14,18,33,36,36,39,52,60,64] contain only single or no more than 10 specific manipulation approaches, and even the largest one [18] only operates 8 manipulations with 19 perturbations on 960 subjects. Moreover, these datasets take forgery analysis solely as a classification task. On the contrary, our proposed ForgeryNet dataset provides 15 manipulation approaches with more than 36 mix-perturbations on over 5, 400 2 subjects, and defines four tasks (i.e. image and video classification, spatial and temporal localization) with a total of 9.4M annotations. Our whole dataset consists of two subsets: Image-forgery set provides over 2.9M still images and Video-forgery set has more than 220k video clips. These two subsets have their real data respectively randomly selected from the original data, and 15 forgery approaches are applied to image-forgery construction while 8 of them also generate the video-forgery data 3 . We compare our ForgeryNet with other publicly available datasets in Tab. 1. Over all the comparison items listed in the table, our dataset surpasses the rest both in scale and diversity.\n\n\nOriginal Data Collection\n\nSource of Original Data. Four face datasets, CREMA-D [7], RAVDESS [45], VoxCeleb2 [13] and AVSpeech [20], are chosen as the original data to boost the diversity in dimensions of face identity, angle, expression, scenarios etc. Note that CREMA-D is made available under the Open Database License, while others are released under a Creative Commons Attribution License. The resolutions of these original data range from 240p to 1080p, and face yaw angles ranging from \u221290 to 90 degrees are all covered. Representative examples are shown in Fig. 2. Preprocess Original Data. For further manipulation, we crop original videos into a controllable set of source videos with reasonable lengths. Then we detect and select faces for manipulation and obtain their face attribute labels.\n\n\nForgery Approach\n\nTo guarantee the diversity of forgery approaches in the proposed ForgeryNet, we introduce 15 face forgery approaches 4 [9,11,17,23,34,35,37,38,47,49,56]. They are selected according to perspectives of modeling types, conditional sources, forgery effects and functions. We denote x t as the target subject to be manipulated while the source x s is regarded as the conditional media driving the target to change either identity or attributes, or even both.\n\n\nForgery Category\n\nAccording to the visual effects of facial manipulation, we divide the forgery approaches into two categories, i.e. Identity-remained and Identity-replaced. Sampled forgeries in Fig. 3 illustrate these categories and their sub-types. Fig. 3(a) remains the identity of x t and the identity-agnostic content like expression, mouth, hair and pose of x t are changed, driven by x s . We adopt eight approaches and divide them into two sub-types: 1) Face reenactment on x t (i, a) preserves its identity but has its intrinsic attributes like pose, mouth and expression manipulated by conditional source x s and forms x t (i,\u00e3 s ), where i refers to identity and a denotes attribute(s). Alternatively, with 2) Face editing on x t (i, a) has its external attributes altered, such as facial hair, age, gender and ethnicity, to obtain x t (i,\u00e2 s ). We also include multiple attribute manipulation with two editing approaches, e.g. both hair and eyebrow are manipulated as shown with the first example in Fig. 3(a-2). Identity-replaced Forgery Approach in Fig. 3(b) replaces the content of x t with that of x s preserving the identity of s. Seven approaches are divided into three sub-types as follows. 1) Face transfer transfers both identity-aware and identity-agnostic content (e.g. expression and pose) from x s to x t , resulting in x t (\u0129 s ,\u00e3 s ). 2) Face swap which produces x t (\u0129 s , a) only swaps identity from the source x s to the target x t , and the identity-agnostic content a are preserved.\n\n\nIdentity-remained Forgery Approach in\n\n3) Face stacked manipulation refers to a combination of both Identity-remained and Identity-replaced approaches. We propose two assembles 5 , i.e. editing \u2192 transfer and swap \u2192 editing , where the former one transfers both the identity and attributes of the manipulated x s (i,\u00e2) to the target x t to obtain x t (\u0129 s ,\u00e3 s ) and the latter alters the external attributes of the swapped target x t (\u0129 s , a) to get x t (\u0129 s ,\u00e2 s ).\n\n\nForgery Pipeline\n\nAlthough there are a wild variety of architectures designed for the aforementioned approaches, most are created using variations or combinations of generative networks, encoderdecoder networks or graphics formation. We briefly sum- \n\n\nSpatial Forgery Distribution\n\n\nIdentity-replaced Forgery Approaches Identity-remained Forgery Approaches\n\nTransfer -Replace Identity-aware and Identity-agnostic Content\n\n\nSwap -Replace Identity Only\n\n\nReenactment -\n\n\nManipulate Intrinsic Attributes\n\nEditing -Manipulate External Attributes Figure 5: Annotations for Spatial Forgery Localization in ForgeryNet. Examples of (a) real image, (b) forgery image, (c) corresponding spatial annotations.\n\nmarize the forgery pipeline in Fig. 4. The target is always an image marked as I t , while there are various conditional source formats x s , including image, image sequence, sketch map, parsing mask, audio, label, or even noise. We first detect the target face I f t , crop and align it, and then transform both the target face as well as source data to intermediate representations such as UV map, feature bank, 3DMM parameters and etc. Forgery Modeling. These representations are forwarded to the forgery models to obtain a forged target face\u0128 f t . We include five architecture variants as, 1) Encoder-Decoder [5], 2) Vanilla GAN [55], 3) Pix2Pix [38], 4) RNN/LSTM [9], and 5) Graphics Formation [19]. Re-rendering Process. To acquire the full forged target, the forged target face\u0128 f t is re-rendered back to the target full image I t to obtain\u0128 t . In particular, according to different forgery procedures, 1)\u0128 f t can be a face mask, shown in Fig. 4(e-1), which contains the area from the eyebrows to the face chin. 2)\u0128 f t can also be a face bounding-box, illustrated in Fig. 4(e-2,3), which keeps the same bounding box as the original target face. Perturbation. To better reflect real-world data distribution, we apply 36 types of perturbations to the forgery dat\u00e3 I t . We follow common practices in visual quality assessment [54] with distortions of compression, transmission, capture, color, etc.\n\n\nForgeryNet Annotation\n\nIn contrast to most previous datasets, our ForgeryNet is annotated comprehensively both in image-and video-level across four tasks. Image Forgery Classification. According to the forgery definition in Sec. 3.2.1, given a forgery image, we provide three types of forgery labels, i.e. labels for two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identity-remained forgery approaches), and n-way (n = 16, real and 15 respective forgery approaches) classification tasks respectively. These annotations make it possible to explore the correlation between different forgery meta-types or approaches.\n\n\nTrain Test Val\n\nForgery Videos Forgery Images Figure 6: Illustration of image-and video-level sets. From the inside to the outside are categories of Identity-remained and Identity-replaced, corresponding sub-types, specific forgery approaches and the situation of data split. Spatial Forgery Localization. As shown in Fig. 5, we take the forgery image\u0128 t and the corresponding real image I t to calculate their difference to obtain a forgery distributio\u00f1 I d t . In this paper, we define the Spatial Forgery Localization task as \"localizing the face area manipulated by deep forgery approaches\", and thus the forgery distribution before perturbation\u0128 d t is taken as the ground-truth annotation. Video Forgery Classification & Temporal Forgery Localization. Note that in contrast to all the existing datasets, we construct our video forgery dataset with untrimmed forgery videos\u1e7c t , each of which splices real and manipulated frames together. Same as image-forgery, Video Forgery Classification also contains three types of class annotations. We also provide the annotations on locations of manipulated segments in the untrimmed forgery video and propose a new task, i.e. Temporal Forgery Localization, to localize these forged segments.\n\n\nForgeryNet Settings\n\nOn ForgeryNet, we set up two benchmarks, image and video, with a series of tasks for face forgery analysis. Dataset Preparation. Both image-and video-level sets are split into training, validation and test subsets with a ratio close to 7:1:2. Forgery data distributions and catagories of the two sets are shown in Fig. 6. Forgery data in each subset have identities matched with the corresponding real subset. The ratio of real to fake in each subset is close to 1:1.\n\n\nImage Benchmark Settings\n\n\nImage Forgery Classification\n\nIn order to foster further researches on face forgery classification, we carefully design two protocols to evaluate forensics methods in this area. Protocol 1: Intra-forgery Evaluation. In intra-forgery evaluation, all the real and fake data in the training set are used to train models, and the validation set is used for evaluation. This protocol has three variants, according to the To further evaluate the generalization ability of training with our data, we conduct cross-forgery evaluation by training the evaluated forensics method with one certain type of manipulation and testing it with others. The manipulation type can either be general (e.g. identity-replaced), or specific (e.g. ATVG-Net).\n\nNote that this protocol only involves binary classification.\n\nMetrics. For binary classification tasks, we evaluate with Accuracy (Acc) and the Area under ROC curve (AUC). For three-and n-way class settings, we use Accuracy (Acc) and mean Average Precision (mAP) as evaluation metrics.\n\n\nSpatial Forgery Localization\n\nCompared with classification tasks, spatial forgery localization aims to specify manipulated regions. Images along with forgery masks are used to train the localization model. Metrics. We utilize three metrics for evaluation: two variants of Intersection over Union (IoU) and L1 distance.\n\n\nVideo Benchmark Settings\n\nVideo Forgery Classification. Evaluation protocols for video forgery classification are generally similar to the ones designed for the image set, except that n=9 for n-class setting. Metrics are the same as those for image classification. Temporal Forgery Localization.\n\nFor each video, forensics methods to be evaluated are expected to provide temporal boundaries of forgery segments and the corresponding confidence values. We follow metrics used in ActivityNet [24] evaluation, and employ Interpolated Average Precision (AP) as well as Average Recall@K (AR@K) for evaluating predicted segments with respect to the groundtruth ones.\n\n\nImage Forgery Analysis Benchmark\n\n\nImage Forgery Classification\n\nProtocol 1: Intra-forgery Evaluation. For comprehensive evaluation, we provide results of two-way class  classification with several representative models of different sizes. Considering the trade-off between performance and efficiency, we use Xception [12] as the baseline model. ELA-Xception [27] and SNRFilters-Xception [10] are two variants of Xception. Smaller models include Mo-bileNetV3 [29], EfficientNet-B0 [58] and ResNet-18 [28]. We select ResNeSt-101 [62] as the large model. We also experiment with recent state-of-the-art methods for face forgery detection, i.e. F 3 -Net [50] and GramNet [44], as well as a fully-attentional network SAN19 [63].\n\nAll experiments are conducted on face images cropped with face bounding boxes enlarged by 1.3\u00d7. During training, we use several types of data augmentation to mimic distortions caused by compression and packet loss during transmission, so as to improve the generalization of developed models.\n\nAs presented in Tab. 2, we list binary classification metrics of all aforementioned forensics methods. We also show the corresponding ROC curves of these methods in Fig. 7(a). For three-way and 16-way classification experiments, as shown in Tab. 3, Acc scores show that classification becomes more difficult when the number of categories increases, yet the mAP metric indicates that the discrimination ability becomes higher instead. Moreover, after mapping back to binary classification, we can also observe Binary-class Triple-class N-class Binary-class Triple-class N-class  slight performance boosts on F 3 -Net compared to training results with only binary labels. This suggests that more auxiliary information potentially makes the forensics model more discriminative. Protocol 2: Cross-forgery Evaluation. For this protocol, we show the generalization ability of forensics methods across forgery approaches. Tab. 4 lists the results of models trained on ID-replaced but evaluated on ID-remained, and vice versa. The more exhaustive cross-forgery setting with 15 specific forgery approaches is also evaluated and shown in Fig. 8. We observe from these results that intra-forgery testing naturally performs the best. From Fig. 8(a), we can also see that training on ATVG-Net, StyleGAN2 or Blend-Face gives the best generalization performance on average. On the other hand, DiscoFaceGAN is the most generalizable forgery approach, while SC-FEGAN is the most difficult approach to generalize to. There is another interesting finding that forgery approaches with stronger similarity tend to induce better cross-forgery performance. For example, Dis-coFaceGAN is a StyleGAN-based approach, thus training on the latter approach produces favorable results on the former. Similarly, StarGAN2 and the two face stack manipulations which both involve StarGAN2 generalize well to each  other. In addition, as shown in Fig. 8(b), forgery approaches belonging to the same meta-category usually have higher correlations mutually. For example, for meta-category Face reenactment, if a forensics method can obtain good performance on ATVG-Net, it may also work for FirstOrderMotion and Talking-headVideo.\n(a) (b) (c) (d)\n\nSpatial Forgery Localization\n\nWe evaluate pixel regression and other two segmentation methods for the spatial localization task. UNet [51] is a popular segmentation architecture, which has been widely  used. For comparison, we also adopt HRNet [59] because of its superior performance on other datasets. In Tab. 5, HRNet outperforms other methods. Especially in terms of IoU diff with threshold 0.01, HRNet surpasses other methods by more than 10%. We also present predicted manipulation maps for several test samples. In Fig. 9(c), the slight beard change is hard to detect, while in Fig. 9(d), a real image is misjudged as manipulated.\n\n\nVideo Forgery Analysis Benchmark\n\n\nVideo Forgery Classification\n\nIn this section, we select several typical video backbones of different sizes: X3D-M [21], Slow-only R-50 [22], TSM [40], and SlowFast R-50 [22]. We sample 16 frames with temporal stride 4 as input to all models.\n\nBinary classfication results of video-level forensics methods are listed in Tab. 6. Compared to image-level evaluation, video-level Acc and AUC are generally higher. SlowFast [22] obtains the best performance on video classification, while X3D-M [21], with only a very small number of parameters, also gives satisfying results. We select these two as representatives of large and small models respectively in subsequent experiments, as displayed in Tab. 7 and Tab. 8. Cross-forgery evaluation results are worse than their image counterparts, suggesting harder generalization with temporal information.\n\n\nTemporal Forgery Localization\n\nWe experiment with both frame-based and video-based models for temporal localization. For frame-based model, after binarizing frame predictions with a fixed threshold (0.25), we select consecutive fake sequences, with differ-  Tab. 9 compares these methods on the validation set. In particular, video-based methods perform significantly better than the frame-based method, demonstrating the importance of applying a boundary-aware network. Additionally, BMN outperforms BSN with large margins, and achieves \u223c87 average AP. This is of great significance since it shows our model is capable of effectively locating manipulated media in a large video database. We hope our results can inspire more future works on forgery localization.\n\n\nConclusion\n\nIn this paper, we present ForgeryNet, a new mega-scale benchmark for both image-and video-level face forgery analysis. Compared with existing datasets for face forgery, ForgeryNet possesses more variety and is more comprehensive in terms of wild sources, various manipulation approaches, diverse re-rendering process and richness of annotations. We further introduce four possible applications with ForgeryNet: image and video classification, spatial and temporal localization. The results obtained in these tasks help us better understand facial forgery towards realworld scenarios. For future works, we welcome interested researchers to contribute more novel facial forgery approaches. More forgery analysis can also be studied on our dataset to improve the defense capabilities. \n\n\nB. Original Data Preprocessing\n\nThe selected in-the-wild videos vary in length (2 seconds \u223c 1 hour), FPS (20 \u223c 30), semantic annotations, and number of faces appearing in one frame. For further manipulation, we preprocess the original data into a controllable source video set:\n\n(1) Video-Origin & Image-Origin: Due to the large amount of videos in VoxCeleb2 and AVSpeech, we respectively pick 43, 941 and 43, 584 videos with length over 6 seconds. The videos are chosen randomly, yet in VoxCeleb2 we guarantee all 6, 112 identities are included in the selected video set. All the selected videos from these two datasets are then truncated into 6 \u223c 10 seconds to enrich length variations, while those from CREMA-D and RAVDESS are retained without cropping due to their short duration (2 \u223c 5 seconds). The images of image-origin are extracted from the aforementioned video-origin set with 20 FPS.\n\n(2) Target Face: We detect faces from images in image-origin by RetinaFace [16] for future manipulation. As shown in Fig. 2 in the main paper, in some scenarios, multiple faces co-occur in a single frame, such as \"conversation between two or more people\" or \"crowd gathering\". To determine the target face for forgery, we first use a simple IoU (Intersection-over-Union) based tracking to acquire face tubes each with faces of the same person identity. We select the face which appears most frequently in the video, i.e. has the longest face tube.\n\n(3) Attribute Prediction: To manipulate facial attributes, the deep models require attribute labels as a conditional input. However, data in video/image-origin lack attribute labels due to limited annotations (e.g. only \"emotions\" and \"age\") of the original datasets. To this end, we predict the attribute labels with Slim-CNN [6,43], a state-of-the-art face attribute classification method.\n\n\nC. Forgery Approach\n\nTo guarantee the diversity of forgery approaches in the proposed ForgeryNet, we introduce 15 face forgery approaches [9,11,17,23,34,35,37,38,47,49,56], which are shown in the main paper. We conclude five architecture variants as, 1) Encoder-Decoder [5] is used to disentangle the identity from identity-agnostic attributes and then modify/swap the encodings of the target before passing them through the decoder. 2) Vanilla GAN [55] consists of a generator and a discriminator which work against each other. After training, the discriminator is discarded and the generator is used to generate content. 3) Pix2Pix [38] is a popular improvement on GANs which enables translations from one image domain to another. The generator is an encoder-decoder network with skip connections from encoder to decoder which enable the generator to produce high fidelity imagery by bypassing some compression layers when needed. In addition to the above three variants, which are the basic elements for generating a forgery image, some sequential and dynamic-length data (e.g. audio and video) are often handled by 4) RNN/LSTM [9], and 5) Graphics Formation [19]. The latter represents a simulation of the classical image formation process of computer graphics, that is, reconstructing a 3D face model with 3DMM parameters, which are the output of a classical analysis-bysynthesis algorithm, and then rendering the generated 3D face model into a 2D image. the convex hull of the face area through the face landmarks to obtain the face mask\u0128 m t , and then turn to the re-rendering solution for the face mask condition described above, as is illustrated in Fig. 4 (e-3) in the main paper.\n\n\nD. Re-rendering Process\n\nEach frame of a video is re-rendered through the aforementioned steps. However, the obtained re-rendered frame sequence often contains frequent jitters due to misalignment and forgery effect. To generate a realistic and smooth video, we apply slight motion blur as well as compression or superresolution to the frame sequence. Fig. 10 presents an overview of perturbations. For example, \"GlassBlur\" and \"JpegCompression\" can simulate distortion of information in video capture and storage in the real world. Some color distortions such as \"RandomBrightness\" and \"ChannelShuffle\" provide diversity in color distributions to adapt to different color renderings of a video.\n\n\nE. Perturbation\n\nMixed perturbations with 2 \u223c 4 distortions are randomly applied to approximately 98% data, while another 1% are added with a single perturbation. The rest 1% are remained unchanged. Each perturbation has 1 \u223c 5 intensity levels. Types and levels of the applied perturbations are all chosen at random, and are applied at the video level, i.e. all frames of a video share the same type of perturbation with the same level. Meanwhile, to avoid severe distribution bias, we guarantee each pair of perturbation types co-occurs at least once. The variety of perturbations improves the diversity and realness of ForgeryNet to better imitate the data distribution in real-world scenarios. \n\n\nF. ForgeryNet Annotation\n\nImage Forgery Classification. The annotations for this task have been elaborated in Sec. 3.3 in the main paper, where we introduce three types of forgery labels, i.e. labels for two-way (real / fake), three-way (real / fake with identity-replaced forgery approaches / fake with identityremained forgery approaches), and n-way (n = 16, real and 15 respective forgery approaches) classification tasks respectively.\n\nSpatial Forgery Localization. Due to the fact that forgery images contain various numbers of faces and each face can be manipulated completely or partially, it is more substantial to specify the manipulated area in addition to the classification labels. We convert the forgery image\u0128 t and the corresponding real image I t into two gray-scale images to calculate their pixel-by-pixel absolute differences. We then normalize the difference map within the face area of the real image I f t to obtain a forgery distribution\u0128 d t . As shown in Fig. 5 (a) in the main paper, stronger response suggests the area is manipulated with heavier intensity. Note that we perform perturbations on the forgery image which cause further modifications in the whole image. The perturbed forgery area distributes all over the whole image rather than merely the face region. In the main paper, compared to Fig. 5 (b) which shows a near-uniform distribution of forgery area both inside and outside the faces, the distribution before perturbation in Fig. 5 (a) shows its advantages in two aspects: 1) the forgery area focuses more on face area, which is consistent with how these deep forgery techniques actually work, and 2) the forgery distribution behaves distinctive among different types of forgery approaches. Take face reenactment and face transfer as an example, the former has particularly high response on lip and also some medium response around head since the audio-or videosource always drives the lip and pose of the target being manipulated, while the latter replaces both identity-aware and identity-agnostic contents of the target and leads to more even response inside the face. In this paper, we define the spatial forgery localization task as \"localizing the face area manipulated by deep forgery approaches\", and thus the forgery distribution before perturbation\u0128 d t is taken as the ground-truth annotation.\n\nVideo Forgery Classification & Temporal Forgery Localization. As is mentioned in Sec. 3.3 in the main paper, in contrast to all existing datasets, we construct our video forgery dataset with untrimmed forgery videos\u1e7c t , each of which splices real and manipulated segments together. This is based on the consideration that forgery videos in the real world often only involve manipulation on a certain subject at some key frames. Specifically, for each pair of forgery video\u1e7c t and its corresponding real video V t , we first randomly select 1 \u223c 4 segments from the forgery video\u1e7c t , and then fill the rest with the corresponding real segments V t . Each forgery/real segment in\u1e7c t has no fewer than 9 frames.\n\nSame as image-forgery, the Video Forgery Classification also contains three types of class annotations. We also provide the annotations of each fragment in the untrimmed forgery video and propose a new task, i.e. Temporal Forgery Localization, to localize the temporal segments which are manipulated.\n\n\nG. ForgeryNet Split\n\nWe first split the identities of the original videos into two subsets, training and evaluation, roughly according to a proportion of 7:3. This guarantees that any person appearing in a training video does not show up in the evaluation set. Note that the AVSpeech dataset does not provide annotations on person identity, so we have to assume that different videos contain different people, and directly split the videos. The evaluation subset is then further divided into validation and test with an approximate ratio of 1:2, and there may be some identity overlaps between the validation and test subsets. The real data for our image set is sampled from the frames extracted with these original videos according to some fixed proportion. Finally, we apply our 15 forgery approaches to generate manipulated data within each subset respectively, e.g. the sources and targets for generating validation forgery data must all come from the validation subset of the original videos.\n\n\nH. Image Forgery Analysis Benchmark\n\n\nH.1. Metrics\n\nImage Forgery Classification. We detail calculation methods of the metrics listed in Sec. 4.1.1 in the main paper. For k-way classification (k = 2, 3, 16), we use Accuracy (Acc) balanced over classes, i.e. we first calculate k accuracy values from the k classes respectively, and then take the uniform average of them as the final balanced accuracy. We also evaluate the standard Area under ROC curve (AUC) for binary classification. In terms of the other settings with more than two classes, we turn to mean Average Precision (mAP) to measure the discrimination ability of the forensics method. More specifically, the AP of some class i is simply the AUC calculated with class i as the sole positive class and all others being negative. After obtaining k APs, we average them to get mAP. Apart from Acc and mAP, we also compute binary metrics for 3-way or n-way classification, and we sum up probabilities predicted for all forgery categories as the final fake confidence. Spatial Forgery Localization.\n\nAs is mentioned in Sec. 4.1.2 in the main paper, we choose three metrics for evaluating predicted maps in our spatial localization task: two variants of Intersection over Union (IoU) and L1 distance. Let N denote the number of pixels, and \u03c4 be a predefined threshold.\n\u2022 IoU = 1 N N i=1 |I[pred i \u2265 \u03c4 ] \u2212 I[gt i \u2265 \u03c4 ]\n| (e.g. \u03c4 = 0.1) represents the accuracy over all spatial grids.\n\u2022 IoU diff = 1 N N i=1 I[|pred i \u2212 gt i | \u2264 \u03c4 ] (e.g. \u03c4 = 0.05)\nindicates whether the predicted value of each pixel is close to the groundtruth.\n\n\u2022 L1 distance Loss l1 = 1 N N i=1 |pred i \u2212 gt i | also implies how close is the predicted map to the groundtruth one.\n\n\nH.2. Models\n\nImage Forgery Classification. There are in total 11 imagelevel classification methods.\n\n\u2022 MobileNetV3 [29] is an efficient mobile model, combining the following three layers: depthwise separable convolutions from MobileNetV1 [30], the linear bottleneck and inverted residual structure from MobileNetV2 [53], and lightweight attention modules based on squeeze and excitation from MnasNet [57]. We use both MobileNetV3-Small and MobileNetV3-Large for evaluation.\n\n\u2022 EfficientNet-B0 [58] is the baseline network of the EfficientNet family, which is developed by leveraging a multi-objective neural architecture search based on mobile inverted bottleneck MBConv [53] with squeeze-and-excitation optimization [31] added to it.\n\n\u2022 ResNet-18 [28] is the smallest ResNet architecture with 17 convolutional layers and one fully connected layer for final output.\n\n\u2022 Xception [12] is a deep convolutional network architecture based on Inception replaced with depthwise separable convolutions. Xception is regarded as our default baseline in further experiments.\n\n\u2022 ResNeSt-101 [62] is a new variant of ResNet. It introduces a modular Split-Attention block that enables attention across different feature-map groups and stacks these blocks ResNet-style to get better performance with similar number of parameters.\n\n\u2022 SAN19-patchwise [63] takes patchwise self-attention as the basic building block for image recognition. Specifically, we uses SAN19 which roughly corresponds to ResNet-50 to evaluate.\n\n\u2022 ELA-Xception and SNRFilters-Xception differ from Xception in the fact that they do not directly take RGB images as input. More specifically, the input for ELA-Xception is the resulting difference image from Error Level Analysis (ELA) [27]. SNRFilters-Xception, as its name suggests, applies a set of 5 \u00d7 5 high pass kernels [10] to the original input image, and then concatenate the 4 output images along the channel dimension (the number of input channels of the first convolution in Xception is changed to 12 accordingly).\n\n\u2022 Gram-Net designs Gram Block to leverage global image texture information for fake image detection. The original paper [44] adds Gram Blocks to the ResNet architecture. Yet in our benchmark, we apply them to our baseline model Xception for the sake of fair comparison.\n\n\u2022 F 3 -Net [50] explores frequency information for face forgery detection by taking advantages of two frequency-aware clues: frequency-aware decomposed image components and local frequency statistics. Note that F 3 -Net also uses Xception as the backbone network.\n\nSpatial Forgery Localization. We select 3 representative models for spatial localization.\n\n\u2022 Xception+Regression uses Xception as the backbone network, and adds an extra deconvolution layer after the final feature map to form a direct regression branch which outputs the spatial forgery map.\n\n\u2022 Xception+UNet [51] supplements a usual contracting network by successive layers where pooling operations are replaced by upsampling operators. A successive convolutional layer can learn to assemble a precise output based on this information. For fair comparison, UNet also uses Xception as its encoder network.\n\n\u2022 HRNet [59] starts from a high-resolution convolution stream, gradually adds high-to-low resolution convolution streams, and connects the multi-resolution streams in parallel. We use the HRNet-W48 instantiation.\n\n\nH.3. Implementation Details\n\nTraining. For classification methods, we use the default cross-entropy loss for training. As for localization methods, we also add a segmentation loss in addition to the classification loss. There are two choices for the segmentation loss: (1) binary cross entropy loss with soft targets averaged over all spatial locations; (2) MSE loss with respect to groundtruth targets. We select one of these two losses for each localization model based on validation results. All models use ImageNet [15] for pre-training. We train both classification and localization models end-to-end using synchronous SGD for optimization. The mini-batch size is set to 128. We adopt a multistep learning rate schedule with 100k iterations in total, and the learning rate is decreased by a factor of 0.5 at steps 20k, 40k, 60k, 70k, 80k and 90k. The base learning rate for each model is selected from the set {0.01, 0.02, 0.05} according to validation performance. We use linear warm-up [25] from 0.01 during the first 1k iterations. The weight decay is set to 10 \u22124 and we apply Nesterov momentum of 0.9. We use face images cropped with provided square bounding boxes (detected boxes enlarged 1.3\u00d7) for training. For data augmentation, we with 99% probability randomly select one perturbation from some set of perturbation methods, and apply it to the input image. Apart from random perturbation, for a model with input spatial size S\u00d7S, we scale the side length to a random value in range [S, 8S/7], and then randomly crop out a S \u00d7 S region. Note that for five Xception-based classification models S = 299, for three localization models S = 256, and for the other six classification models S = 224. We also apply random horizontal flip before feeding the input to the model. Inference. We only perform single-crop inference, and directly scale the input face image to the input spatial size S \u00d7 S of the model.\n\n\nH.4. More Experiments\n\nAblation Study on Augmentation. We experiment on three different levels of augmentation: weak, normal and enhanced. Weak augmentation does not add random perturbation mentioned in Appendix H.3, while normal and enhanced settings include different numbers of common perturbation methods in the perturbation set for augmentation. Results of Xception trained on these types of data augmentation are shown in Tab. 11. It can be seen that exerting appropriate augmentation to the training set significantly improves the performance of an image forgery classification model. Cross-dataset Experiments. We provide cross-dataset testing results with our ForgeryNet (image forgery binary classfication only) as well as three public deepfake datasets -FF++ (c23) [52], DFDC [18], and DeeperForensics-1.0 (DF1.0) [33] which are only used for testing. For evaluation, we use (1)   Temporal Forgery Localization. For the temporal localization task, the goal is to generate proposals which have high temporal overlap with the groundtruth (manipulated segments) as well as high recall. We give specifics on our employed metrics for evaluating predicted segments with respect to the groundtruth ones, which are Average Precision at some tIoU threshold (AP@t, e.g. t = 0.5), average AP, as well as Average Recall@K (AR@K, e.g. K = 5).\n\nNote that these metrics mostly reference ActivityNet [24] evaluation. In details, we choose 10 equally-spaced tIoU threshold values between 0.5 and 0.95 (inclusive) with a step size of 0.05. Under a certain tIoU threshold value t, we may match our predicted segments with the groundtruth according to the condition that tIoU \u2265 t. Recall@K with tIoU threshold t is defined as the proportion of groundtruth which can be matched with some prediction, after preserving only K predicted segments per video on average. AP@t, on the other hand, is the Area under ROC curve computed with predictions and their associated confidence scores, treating the predictions which are matched to some groundtruth segment with tIoU threshold t as positive. Finally, average AP and AR@K are simply the uniform average of APs and Recall@Ks computed at the 10 tIoU thresholds, respectively. Note that both real and fake videos are included in our evaluation, although the real ones do not contain any forgery segment (Recall is not be affected by real videos, but AP is).\n\n\nI.2. Models\n\nVideo Forgery Classification. We choose four typical models for video classification.\n\n\u2022 TSM [40] inserts Temporal Shift Modules to 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We follow its default setting with ResNet-50 as the backbone network.\n\n\u2022 SlowFast [22], featuring its two-pathway design with different input temporal strides, is one of the state-ofthe-art architectures for action recognition. We choose its R-50 instantiation (without Non-Local blocks), and set the fast-to-slow ratio \u03b1 = 4.\n\n\u2022 Slow-only is basically the slow pathway of SlowFast, and we also use the R-50 instantiation. Note that with the same number of input frames, Slow-only is actually heavier than SlowFast since the slow branch of the latter only use 1/\u03b1 of the frames.\n\n\u2022 X3D-M [21] is one member of the X3D family, a series of efficient video networks obtained by progressive expansion along multiple axes. It is able to achieve performances nearly comparable with SlowFast R-50 on common video benchmarks while having much fewer parameters.\n\nTemporal Forgery Localization. As described in Sec. 6.2 in the main paper, we include a frame-based method, where we use Xception as the frame prediction model. The logic of this method can be briefly stated as the following:\n\n1. For a video with T frames, we run the Xception model to get frame-level scores, and then binarize them with threshold 0.25, acquiring a sequence of T binary predictions (real/fake).\n\n2. We enumerate tolerance value in the set {1, 3, 5, 7}. For a tolerance value t, we inspect the sequence of T predictions, and selects manipulated segments with at least 5 frames satisfying that the length of consecutive real frames in the middle does not exceed t. The confidence score of a segment is simply the average of its frame-level scores.\n\n3. We combine segments predicted with different tolerance levels, and remove duplicates to form the final predictions.\n\nFor two video-based methods (BSN [42] and BMN [41]), we use SlowFast and X3D-M for extracting clip features, forming four different \"feature+method\" pairs. Note that for these feature extraction models, we use fewer input frames for training than their classification counterparts to increase temporal locality. Accordingly, the fast-to-slow ratio \u03b1 of SlowFast is decreased to 2.\n\n\nI.3. Implementation Details\n\nTraining. For classification methods and feature extraction models for localization, we use the default cross-entropy loss for training. The frame-based localization method directly uses the Xception model trained with the image binary classification task, and does not need any extra training. BSN and BMN have their own training loss functions and procedures which we do not alter.\n\nAll models use Kinetics-400 [8] for pre-training. We train them end-to-end using synchronous SGD for optimization. The mini-batch size is set to 64. We adopt a multistep learning rate schedule with 50k iterations in total, and the learning rate is decreased by a factor of 0.5 at steps 20k, 30k, 40k and 45k. The base learning rate is set to 0.02. We use linear warm-up from 10 \u22123 during the first 500 iterations. All classification models take 16 frames with a temporal stride of 4 as input, yet the feature extraction models (SlowFast and X3D-M) for BSN and BMN use only continuous 8 frames as input for better temporal sensitivity. We use temporal random crop for training, i.e. for a model requiring T frames \u00d7 stride \u03c4 , we randomly sample a segment of length T \u00d7 \u03c4 from the video. In some rare cases where the entire video has less than T \u00d7 \u03c4 frames, we use loop padding to fill the rest. The input spatial size is fixed to S = 224. Other training details are the same as those for image experiments. For BSN and BMN, since the feature extraction models take 8 frames as input, we extract features with stride 4. We set the temporal scale parameter to 50, and linearly interpolate the extracted features to the 51 endpoints. We only use fake videos for training video-based localization models. We train TEM and PEM in BSN for 20 epochs each. We train BMN for 9 or 18 epochs according to validation performance. The mini-batch size is set to 128. Other hyperparameters follow the original settings of BSN and BMN. Inference. We scale the input to S \u00d7 S spatially. On the temporal dimension, we use two settings for classification inference (suppose input temporal sampling is T \u00d7 \u03c4 ): (1) single-crop, or to be more specific, temporally center crop T \u00d7 \u03c4 frames; (2) multi-crop, i.e. crop multiple segments of length T \u00d7 \u03c4 to cover the entire video.\n\nFor temporal localization, we only keep top 10 predictions per video in terms of confidence score, and for videobased methods, relevant hyper-parameters are the same as training.\n\n\nI.4. More Experiments\n\nAblation Study on Augmentation. We conduct similar experiments on augmentation with the same settings as Appendix H.4. As presented in Tab. 13, we observe that our video-level forgery classification method is less affected by augmentation than its image-level counterpart. Temporal Shuffling Experiments. To verify the effect of continuous temporal information for video forgery classification, we train the SlowFast model with different levels of temporal random shuffling to disrupt temporal continuity: shuffle every 16 frames, shuffle every 64 frames, and shuffle all frames. The results in Tab. 14 indicate that tem-  Figure 11: Example of temporal forgery localization. We show top-5 predictions of the model SlowFast+BMN. All endpoints of the two manipulated segments are localized with high precision. poral disruptions have considerable, but not very major impact on the performance video classification, implying the video model may have leveraged other sources of information than the continuous temporal flow. An interesting finding is that a weak level of random shuffling (shuffle 16) even slightly boosts the AUC score compared to the setting without shuffling recorded in Tab. 13.\n\n\nI.5. Temporal Localization Analysis\n\nWe present an example of temporal forgery localization in Fig. 11. This data sample demonstrates the ability of a boundary-aware model to locate the transitions between real and fake. All endpoints are accurately pointed out by the BMN model. Note that there exist some highly similar predictions, yet are suppressed by a SoftNMS process.\n\nFigure 1 :\n1ForgeryNet is a new mega-scale face forgery dataset with comprehensive annotations and four forgery analysis tasks. It contains thousands of subjects, various manipulation methods and diverse re-rendering processes. In (a), can you distinguish which images are forged?\n\nFigure 3 :\n3Sampled forgeries in our ForgeryNet. (a) Identityremained forgery approaches: 1) Face reenactment, 2) Face editing. (b) Identity-replaced forgery approaches: 1) Face transfer, 2) Face swap, 3) Face stacked manipulation.\n\nFigure 4 :\n4Pipeline of face forgery approaches. (a)-(c) Representation preparation: target image I t , conditional source x s and their intermediate representations. (d) Forgery models produce a forged target face\u0128 f t by processing the representations. (e)-(f) Re-render\u0128 f t to full image I t and get the forgery image\u0128 t . (g) Apply perturbations to\u0128 t to obtain final forgery data.\n\nFigure 7 :\n7Image Forgery Classification (Protocol 1): (a) We show the ROC curves of the compared methods under the setting of binary classification. (b)-(d) t-SNE feature visualization of the data manipulated by different forgery approaches, trained with binary, three-way and n-way classification respectively.\n\nFigure 8 :\n8Image Forgery Classification (Protocol 2): (a) AUC score map, and (b) correlation map according to the AUC scores. X-axis denotes the tested forgery approach and Y-axis denotes the forgery approach for training.\n\nFigure 9 :\n9Spatial Forgery Localization. Examples of predicted manipulation masks by HRNet.\n\n\nexpressions, actions, etc., for the sake of building a wild and diverse forgery dataset.(1) CREMA-D[7] is a dataset of 7, 442 video clips from 48 male and 43 female actors with a variety of ethnicities, ages ranging from 20 to 74, and six different emotions.(2) RAVDESS[45] consists of 7, 356 files including both video footages and sound tracks from 24 professional actors with eight emotions, vocalizing two lexically-matched statements in a neutral North American accent.(3) VoxCeleb2[13] is constructed with over one million YouTube videos with utterances of 6, 112 celebrities. (4) AVSpeech[20] is a dataset of 290k YouTube video clips of 3 \u223c 10 seconds long. Note that the speakers talk with no audio background interference, i.e. the only audible sound in the soundtrack of a video belongs to a single visible and speaking person.\n\nFigure 10 :\n10Perturbations in ForgeryNet. Different perturbations are marked in different colors. This example shows the effects of one or mixed perturbations. Arrows indicate the mixture order. The image on the left is first added \"GlassBlur\" followed by \"JpegCompression\" and at last \"RandomBrightness\".\n\nTable 1 :\n1Comparison of various face forgery datasets. ForgeryNet surpasses any other dataset both in scale and diversity. It provides both video-and image-level data. The forgery data are constructed by 15 manipulation approaches within 4 categories. We also employ 36 types of perturbations from 4 kinds of distortions for post-processing. 98 videos, i.e. 49 real videos from YouTube and 49 fake ones generated by FakeAPP[3]. SwapMe and FaceSwap choose two face swapping Apps[1,2] to create 2010 forgery images in total on 1005 original real images.Dataset \nVideo Clips \nStill images \nApproaches Subjects \nUniq. \nPerturb. \n\nMix \nPerturb. \nAnnotations \nReal \nFake \nReal \nFake \n\nUADFV [60] \n49 \n49 \n241 \n252 \n1 \n49 \n-\n\u00d7 \n591 \nDF-TIMIT [36] \n320 \n640 \n-\n-\n2 \n43 \n-\n\u00d7 \n1,600 \nDeep Fake Detection [4] \n363 \n3,068 \n-\n-\n5 \n28 \n-\n\u00d7 \n3,431 \nCeleb-DF [39] \n590 \n5,639 \n-\n-\n1 \n59 \n-\n\u00d7 \n6,229 \nSwapMe and FaceSwap [64] \n-\n-\n4,600 \n2,010 \n2 \n-\n-\n\u00d7 \n6,610 \nDFFD [14] \n1,000 \n3,000 \n58,703 \n240,336 \n7 \n-\n-\n\u00d7 \n8,000 \nFaceForensics++ [52] \n1,000 \n5,000 \n-\n-\n5 \n-\n2 \n\u00d7 \n11,000 \nDeeperForensics-1.0 [33] \n50,000 \n10,000 \n-\n-\n1 \n100 \n7 \n60,000 \nDFDC [18] \n23,564 104,500 \n-\n-\n8 \n960 \n19 \n\u00d7 \n128,064 \nForgeryNet (Ours) \n99,630 121,617 1,438,201 1,457,861 \n15 \n5400+ \n36 \n9,393,574 \n\nRAVDESS \nCREMA-D \n\nVoxCeleb2 \nAVSpeech \n\nFigure 2: Representative examples of original data collected \nfrom four face datasets respectively. \n\n\n\nTable 2 :\n2Image Forgery Classification (Protocol 1): binary classification. We report accuracy and AUC scores of the compared forensics methods.Method \nParam. \nAcc \nAUC \n\nMobileNetV3 Small [29] \n1.7M 76.24 85.51 \nMobileNetV3 Large [29] \n4.2M 78.30 87.56 \nEfficientNet-B0 [58] \n4.0M 79.86 89.31 \nResNet-18 [28] \n11.2M 78.31 87.75 \nXception [12] \n20.8M 80.78 90.12 \nResNeSt-101 [62] \n46.2M 82.06 91.02 \nSAN19-patchwise [63] \n18.5M 80.08 89.38 \nELA-Xception [27] \n20.8M 73.77 82.69 \nSNRFilters-Xception [10] \n20.8M 81.09 90.52 \nGramNet [44] \n22.1M 80.89 90.20 \nF 3 -Net [50] \n57.3M 80.86 90.15 \n\ndefinition in Sec. 3.3, i.e. two-/three-/n-way classification. \nProtocol 2: Cross-forgery Evaluation. \n\nTable 3 :\n3Image Forgery Classification (Protocol 1): multi-class settings and their mappings to binary classification. We report the accuracy, mAP and AUC scores.3-way class \n3\u21922-way class \nAcc. \nmAP \nAcc. \nAUC \n\nXception 73.00 89.90 80.17 \n89.92 \nGramNet 73.30 90.00 80.75 \n90.13 \nF 3 -Net \n74.45 90.41 81.75 \n90.63 \n16-way class \n16\u21922-way class \nAcc. \nmAP \nAcc. \nAUC \n\nXception 58.81 93.16 81.00 \n90.53 \nGramNet 56.77 92.27 80.83 \n90.25 \nF 3 -Net \n59.82 92.98 81.88 \n90.91 \n\n\n\nTable 4 :\n4Image Forgery Classification (Protocol 2): binary classification. We report the accuracy and AUC scores. Forensics methods trained with ID-replaced forgery approaches have significant performance drops when tested on unseen ID-remained forgery approaches, and vice versa. ID-remained 67.28 75.83 81.17 90.71 GramNet ID-replaced 82.82 92.54 62.72 74.28 ID-remained 67.50 76.19 80.60 90.28 F 3 -Net ID-replaced 83.84 92.73 64.33 73.82 ID-remained 68.44 77.24 81.18 90.29ID-replaced \nID-remained \nAcc. \nAUC \nAcc. \nAUC \n\nXception \nID-replaced \n84.13 92.80 64.62 74.86 \n\n\nTable 5 :\n5Spatial Forgery Localization. We compare results with three metrics, i.e., IOU, IOU diff and L1 distance.Predicted MaskOriginal Target Before Perturb. After Perturb. GroundTruthMethod \nIoU \nIoUdiff \nLoss l1 \n0.1 \n0.2 \n0.01 \n0.05 \n0.1 \n\nXception+Reg. \n89.55 \n93.70 \n67.57 \n83.25 \n89.22 \n0.0131 \nXeption+Unet [51] \n95.99 \n98.76 \n79.71 \n92.70 \n97.13 \n0.0134 \nHRNet [59] \n96.27 \n98.78 \n88.73 \n92.99 \n96.27 \n0.0114 \n\n(a) \nFace \nReplacement \n\n(b) \nFace \nReenactment \n\n(c) \nFace \nEditing \n\n(d) \nReal \nFace \n\n(e) \nReal \nFace \n\n\n\nTable 6 :\n6Video Forgery Classification (Protocol 1): binary classificaiton. We report accuracy and AUC scores under two crop strategies. Video-level classification has better results than the image-level setting.Single-crop \nMulti-crop \nMethod \nParameters \nAcc \nAUC \nAcc \nAUC \n\nX3D-M [21] \n2.9M \n87.93 \n93.75 \n88.97 \n96.99 \nSlow-only [22] \n31.6M \n86.76 \n92.64 \n87.37 \n95.96 \nTSM [40] \n23.5M \n88.04 \n93.05 \n89.11 \n96.25 \nSlowFast [22] \n33.6M \n88.78 \n93.88 \n89.92 \n97.28 \n\n\n\nTable 7 :\n7VideoForgery Classification (Protocol 1): multi-\nclass settings and their mappings to binary classification. \nWe report the accuracy, mAP and AUC scores. \n\nMethod \n3-way class \n3\u21922-way class \nAcc. \nmAP \nAcc. \nAUC \n\nX3D-M [21] \n84.00 94.55 87.69 \n93.78 \nSlowFast [22] \n85.73 94.89 89.11 \n94.37 \n9-way class \n9\u21922-way class \nAcc. \nmAP \nAcc. \nAUC \n\nX3D-M [21] \n76.91 95.06 87.51 \n93.81 \nSlowFast [22] \n80.86 95.92 89.45 \n94.25 \n\n\n\nTable 8 :\n8Video Forgery Classification (Protocol 2): binary classification. Forensics methods trained with IDreplaced forgery approaches have substantial performance drops (even more significant than their image-level counterparts) when tested on unseen ID-remained forgery approaches, and vice versa.ID-replaced \nID-remained \nAcc. \nAUC \nAcc. \nAUC \n\nX3D-M \nID-replaced \n87.92 92.91 55.25 65.59 \nID-remained \n55.93 62.87 88.85 95.40 \n\nSlowFast \nID-replaced \n88.26 92.88 52.64 64.83 \nID-remained \n52.70 61.50 87.96 95.47 \n\n\n\nTable 9 :\n9Temporal Forgery Localization. We show AP, AR and mAP scores of all compared methods.AR \nAP \navg. \nAP \n2 \n5 \n0.5 \n0.75 \n0.9 \nXception [12] \n25.83 \n73.95 \n68.29 \n62.84 \n58.30 \n62.83 \nX3D-M+BSN [42] \n81.33 \n86.88 \n80.46 \n77.24 \n55.09 \n70.29 \nX3D-M+BMN [41] \n88.44 \n91.99 \n90.65 \n88.12 \n74.95 \n83.47 \nSlowFast+BSN [42] \n83.63 \n88.78 \n82.25 \n80.11 \n60.66 \n73.42 \nSlowFast+BMN [41] \n90.64 \n93.49 \n92.76 \n91.00 \n80.02 \n86.85 \n\nent tolerance levels for real frames in the middle, as final \nproposals. The confidence of a proposal is simply the av-\nerage of the original frame scores. We adopt Boundary-\nSensitive Network (BSN) [42] and Boundary-Matching \nNetwork (BMN) [41] on top of X3D-M and SlowFast fea-\ntures as the video-based models. \n\n\nTable 10 :\n10Summary of the four types of forgery approaches. In this table, the input, output, architecture, resolution, modification ability, and whether to retrain in inference of each forgery approach are presented. S/T represents the modality of x s and x t . v:=video, i:=image, a:=audio, m:= mask, s:=sketch, l:= noise, S:=single identity, M:=multiple identityMethod \nS/T \nCG/GAN Input \nModification \nResolution \nRetraining \n\nFace \nReenactment \n\nFirstOrderMotion [56] \nv/i \nGAN \nM/M \npose,expression \n256*256 \nNo need \nATVG-Net [9] \nv/i \nGAN \nM/M \npose,expression \n128*128 \nNo need \nTalking-head Video [23] \na/v \nCG+GAN \nM/S \nmouth \n256*256 \n1\u223c3 portraits \n\nFace \nEditing \n\nStarGAN2 [11] \ni/i \nGAN \nM/M \nattribute transfer \n256*256 \nportraits \nStyleGAN2 [35] \nl/i \nGAN \nM/M rebuild from latent 1024*1024 \nportraits \nMaskGAN [37] \nm,i/i \nGAN \nM/M \nediting record \n512*512 \nportraits,mask \nSC-FEGAN [34] \ns,i/i \nGAN \nM/M \nsketch record \n512*512 \nportraits,sketch \nDiscoFaceGAN [17] \ni/i \nCG+GAN M/M \n3dmm attributes \n1024*1024 \nportraits \nFace \nTransfer \n\nBlendFace \nv/v \nCG \nM/M identity, expression \nAny \nNo need \nMMReplacement \ni/i \nCG \nM/M identity, expression \nAny \nat least 1 protrait \n\nFace Swap \n\nFSGAN [47] \nv/v \nGAN \nM/M \nidentity \n256*256 \nNo need \nDeepFakes [49] \nv/v \nGAN \nS/S \nidentity \n192*192 \n2k\u223c5k portraits \nFaceShifter [38] \ni/i \nGAN \nM/M \nidentity \n256*256 \nNo need \n\n\n\nTable 11 :\n11Ablation study on augmentation (image). We report accuracy and AUC scores of Protocol 1 binary classification on the validation set with three different levels of augmentation.weak aug \nnormal aug \nenhanced aug \nAcc. \nAUC \nAcc. \nAUC \nAcc. \nAUC \n\nXception 66.73 74.75 73.70 82.56 80.78 90.12 \n\n\n\n\ntest set of FF++ (c23); (2) both validation and test set (only the released half) of DFDC; (3) a subset of DF1.0 which corresponds to the test set of FF++; (4) test set of our image benchmark. For video datasets, we extract frames with temporal stride 30 for frame-level testing. We present the numbers in Tab. 12. ForgeryNet shows the best cross-dataset performances on all other test sets, which indicates the strong generality of our dataset.I. Video Forgery Analysis BenchmarkI.1. MetricsVideo Forgery Classification. The metrics for this task are the same as those for image classification.\n\nTable 12 :\n12Cross-dataset experiments. We report framelevel AUC scores. Each row corresponds to a model trained with one of the datasets. Underlined values are results of models trained and tested on the same dataset, and the bold ones emphasize best cross-dataset performances.DF1.0 \nFF++ \nDFDC(val) \nDFDC(test) ForgeryNet \n\nFF++ [52] \n85.41 \n99.43 \n59.77 \n62.19 \n63.80 \nDFDC [18] \n79.60 \n71.34 \n90.12 \n93.50 \n68.93 \nForgeryNet \n90.09 \n85.06 \n69.68 \n71.08 \n90.09 \n\n\n\nTable 13 :\n13Ablation study on augmentation (video). We report accuracy and AUC scores of Protocol 1 binary classification on the validation set with three different levels of augmentation.weak aug \nnormal aug \nenhanced aug \nAcc. \nAUC \nAcc. \nAUC \nAcc. \nAUC \n\nSlowFast 84.39 91.61 87.75 93.22 88.78 93.88 \n\n\n\nTable 14 :\n14Experiemnts on temporal shuffling. We report accuracy and AUC scores of Protocol 1 binary classification on the validation set with three different levels of temporal shuffling. SlowFast 88.63 94.11 86.24 93.00 85.04 91.74shuffle 16 \nshuffle 64 \nshuffle all \nAcc. \nAUC \nAcc. \nAUC \nAcc. \nAUC \n\n\nIn this paper, the definition of the term \"face forgery\" refers to an image or a video containing modified identity, expressions or attribute(s) with a learning-based approach, distinguished with 1) a so-called \"Cheap-Fakes\"[48] that are created with off-the-shelf softwares without learnable components and 2) \"DeepFakes\" that only refer to manipulations with swapped identities[18].\nSome original datasets do not provide the identity annotation.3 There are 7 forgery approaches that are only suitable for generating images.\nDetailed description of the forgery approaches is provided in the appendix.\nStarGAN2-BlendFace-Stack (SBS), DeepFakes-StarGAN2-Stack (DSS)\n(1) For the face mask condition shown inFig. 4(e-1) in the main paper, we first align the landmarks of\u0128 f t and I f t to align their masks\u0128 m t and I m t , and then calculate an optimal transformation to align\u0128 f t back to the I t . Color matching is then operated on the re-aligned face to make\u0128 f t more adaptable to I f t 6 . The following step is blending, with the objective of making\u0128 f t seamlessly fit the target full image I t . We corrode and blur the smaller mask between\u0128 m t and I m t , and perform the Poisson blending along the outer contour of\u0128 f t to get the full forgery image\u0128 t .(2) For the face bounding-box condition, an easy way is to directly substitute the bounding-box in the original target image I b t with a forgery one\u0128 b t , and simply perform the Poisson blending along the edge of the bounding-box as shown inFig. 4 (e-2)in the main paper. However, some GAN-based approaches always induce some unexpected details outside the face region, especially some background clutters with jittery and blurred information. Meanwhile, some graphic-based approaches cannot infer the texture of non-face regions such as hair. To this end, we first calculate6 Identity-remained forgery do not have this step since it only changes local intrinsic or external attributes. Moreover, some editing even aims at altering colors such as lip or eye color.\nAcknowledgments This work is supported by key research and development program of Guangdong Province,AppendixA. Original Data CollectionIn contrast to previous facial forgery datasets[33,52]which only involve original data taken from certain briefing scenarios or TV shows, we choose four face datasets[7,13,20,45]as the original data with diversified face identities,\nFaceswap. Faceswap. https : / / github . com / MarekKowalski/FaceSwap/. 3\n\n. Swapme, Swapme. https://itunes.apple.com/us/app/ swapme-by-faciometrics/. 3\n\nFakeapp. Fakeapp. https://www.fakeapp.com/, 2018. 3\n\nGoogle ai blog. contributing data to deepfake detection research. Google ai blog. contributing data to deepfake detec- tion research. https : / / ai . googleblog . com / 2019 / 09 / contributing-data -to -deepfake- detection.html, 2019. 3\n\nSlim-cnn: A lightweight cnn for face attribute prediction. Hassan Foroosh, Ankit Sharma, arXiv:1907.0215711arXiv preprintHassan Foroosh Ankit Sharma. Slim-cnn: A light- weight cnn for face attribute prediction. arXiv preprint arXiv:1907.02157, 2019. 11\n\nCrema-d: Crowd-sourced emotional multimodal actors dataset. Houwei Cao, G David, Cooper, K Michael, Keutmann, C Ruben, Ani Gur, Ragini Nenkova, Verma, IEEE Trans. Affect. Comput. 5411Houwei Cao, David G Cooper, Michael K Keutmann, Ruben C Gur, Ani Nenkova, and Ragini Verma. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE Trans. Affect. Comput., 5(4):377-390, 2014. 2, 4, 10, 11\n\nQuo vadis, action recognition? a new model and the kinetics dataset. Joao Carreira, Andrew Zisserman, CVPR. 16Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR, pages 6299-6308, 2017. 16\n\nHierarchical cross-modal talking face generation with dynamic pixel-wise loss. Lele Chen, K Ross, Zhiyao Maddox, Chenliang Duan, Xu, CVPR. 1112Lele Chen, Ross K Maddox, Zhiyao Duan, and Chenliang Xu. Hierarchical cross-modal talking face generation with dynamic pixel-wise loss. In CVPR, 2019. 4, 5, 11, 12\n\nJpeg-phase-aware convolutional neural network for steganalysis of jpeg images. Mo Chen, Vahid Sedighi, Mehdi Boroumand, Jessica Fridrich, the 5th ACM Workshop. 614Mo Chen, Vahid Sedighi, Mehdi Boroumand, and Jessica Fridrich. Jpeg-phase-aware convolutional neural network for steganalysis of jpeg images. In the 5th ACM Workshop, 2017. 6, 14\n\nStargan v2: Diverse image synthesis for multiple domains. Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha, CVPR. 412Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In CVPR, pages 8188-8197, 2020. 4, 11, 12\n\nXception: Deep learning with depthwise separable convolutions. Fran\u00e7ois Chollet, CVPR. 814Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017. 6, 8, 14\n\nVoxceleb2: Deep speaker recognition. Arsha Joon Son Chung, Andrew Nagrani, Zisserman, arXiv:1806.056221011arXiv preprintJoon Son Chung, Arsha Nagrani, and Andrew Zisserman. Voxceleb2: Deep speaker recognition. arXiv preprint arXiv:1806.05622, 2018. 2, 4, 10, 11\n\nOn the detection of digital face manipulation. Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, Jain, CVPR. 2020Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, and Anil K Jain. On the detection of digital face manipulation. In CVPR, pages 5781-5790, 2020. 3\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Ieee14Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248-255. Ieee, 2009. 14\n\nRetinaface: Single-shot multi-level face localisation in the wild. Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, Stefanos Zafeiriou, CVPR. 11Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation in the wild. In CVPR, 2020. 11\n\nDisentangled and controllable face image generation via 3d imitative-contrastive learning. Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, Xin Tong, CVPR. 12Yu Deng, Jiaolong Yang, Dong Chen, Fang Wen, and Xin Tong. Disentangled and controllable face image generation via 3d imitative-contrastive learning. In CVPR, 2020. 12\n\n. Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, Cristian Canton Ferrer, arXiv:2006.07397215The deepfake detection challenge dataset. arXiv preprintBrian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. The deepfake detection challenge dataset. arXiv preprint arXiv:2006.07397, 2020. 2, 3, 15\n\nSami Romdhani, et al. 3d morphable face models-past, present, and future. Bernhard Egger, A P William, Ayush Smith, Stefanie Tewari, Michael Wuhrer, Thabo Zollhoefer, Florian Beeler, Timo Bernard, Adam Bolkart, Kortylewski, TOG3911Bernhard Egger, William AP Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, et al. 3d morphable face models-past, present, and future. TOG, 39(5):1-38, 2020. 5, 11\n\nLooking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. A Ephrat, I Mosseri, O Lang, T Dekel, A Wilson, W T Hassidim, M Freeman, Rubinstein, arXiv:1804.036191011arXiv preprintA. Ephrat, I. Mosseri, O. Lang, T. Dekel, K Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein. Look- ing to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619, 2018. 2, 4, 10, 11\n\nX3d: Expanding architectures for efficient video recognition. Christoph Feichtenhofer, CVPR. 816Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In CVPR, 2020. 8, 16\n\nSlowfast networks for video recognition. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He, ICCV. 815Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In ICCV, 2019. 8, 15\n\nText-based editing of talking-head video. Ohad Fried, Ayush Tewari, Michael Zollh\u00f6fer, Adam Finkelstein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, Maneesh Agrawala, TOG. 38412Ohad Fried, Ayush Tewari, Michael Zollh\u00f6fer, Adam Finkel- stein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, and Maneesh Agrawala. Text-based editing of talking-head video. TOG, 38(4):1-14, 2019. 2, 12\n\nBernard Ghanem, Juan Carlos Niebles, Cees Snoek, Fabian Caba Heilbron, Humam Alwassel, Victor Escorcia, Ranjay Krishna, Shyamal Buch, Cuong Duc Dao, arXiv:1808.03766The activitynet large-scale activity recognition challenge 2018 summary. 615arXiv preprintBernard Ghanem, Juan Carlos Niebles, Cees Snoek, Fabian Caba Heilbron, Humam Alwassel, Victor Escorcia, Ranjay Krishna, Shyamal Buch, and Cuong Duc Dao. The activitynet large-scale activity recognition challenge 2018 summary. arXiv preprint arXiv:1808.03766, 2018. 6, 15\n\nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, arXiv:1706.02677Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. 14arXiv preprintPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 14\n\nDeepfake detection by analyzing convolutional traces. Luca Guarnera, Oliver Giudice, Sebastiano Battiato, CVPRW. Luca Guarnera, Oliver Giudice, and Sebastiano Battiato. Deepfake detection by analyzing convolutional traces. In CVPRW, 2020. 2\n\nNanang Ismail, Nor Farahidah Za'bah, and Anis Nurashikin Nordin. Development of photo forensics algorithm by detecting photoshop manipulation using error level analysis. Teddy Surya Gunawan, Siti Amalina Mohammad Hanafiah, Mira Kartiwi, IJEECS. 7114Teddy Surya Gunawan, Siti Amalina Mohammad Hanafiah, Mira Kartiwi, Nanang Ismail, Nor Farahidah Za'bah, and Anis Nurashikin Nordin. Development of photo forensics algorithm by detecting photoshop manipulation using error level analysis. IJEECS, 7(1):131-137, 2017. 6, 14\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 614Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 6, 14\n\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, ICCV. 614Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mo- bilenetv3. In ICCV, 2019. 6, 14\n\nG Andrew, Menglong Howard, Bo Zhu, Dmitry Chen, Weijun Kalenichenko, Tobias Wang, Marco Weyand, Hartwig Andreetto, Adam, arXiv:1704.04861Mobilenets: Efficient convolutional neural networks for mobile vision applications. 14arXiv preprintAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An- dreetto, and Hartwig Adam. Mobilenets: Efficient convolu- tional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 14\n\nSqueeze-and-excitation networks. Jie Hu, Li Shen, Gang Sun, CVPR. 14Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net- works. In CVPR, pages 7132-7141, 2018. 14\n\nDetecting cnn-generated facial images in real-world scenarios. Nils Hulzebosch, Sarah Ibrahimi, Marcel Worring, CVPRW. 2020Nils Hulzebosch, Sarah Ibrahimi, and Marcel Worring. De- tecting cnn-generated facial images in real-world scenarios. In CVPRW, 2020. 2\n\nDeeperforensics-1.0: A large-scale dataset for real-world face forgery detection. Liming Jiang, Ren Li, Wayne Wu, Chen Qian, Chen Change Loy, CVPR, 2020. 3. 1015Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection. In CVPR, 2020. 3, 10, 15\n\nSc-fegan: Face editing generative adversarial network with user's sketch and color. Youngjoo Jo, Jongyoul Park, ICCV. 412Youngjoo Jo and Jongyoul Park. Sc-fegan: Face editing gen- erative adversarial network with user's sketch and color. In ICCV, 2019. 4, 11, 12\n\nAnalyzing and improving the image quality of StyleGAN. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, CVPR. 412Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In CVPR, 2020. 4, 12\n\nDeepfakes: a new threat to face recognition? assessment and detection. Pavel Korshunov, S\u00e9bastien Marcel, arXiv:1812.0868523arXiv preprintPavel Korshunov and S\u00e9bastien Marcel. Deepfakes: a new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018. 2, 3\n\nMaskgan: Towards diverse and interactive facial image manipulation. Ziwei Cheng-Han Lee, Lingyun Liu, Ping Wu, Luo, CVPR, 2020. 412Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facial image ma- nipulation. In CVPR, 2020. 4, 11, 12\n\nFaceshifter: Towards high fidelity and occlusion aware face swapping. Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, Fang Wen, arXiv:1912.134571112arXiv preprintLingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang Wen. Faceshifter: Towards high fidelity and occlusion aware face swapping. arXiv preprint arXiv:1912.13457, 2019. 2, 4, 5, 11, 12\n\nCeleb-df: A large-scale challenging dataset for deepfake forensics. Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, Siwei Lyu, CVPR. 2020Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. Celeb-df: A large-scale challenging dataset for deep- fake forensics. In CVPR, 2020. 3\n\nTsm: Temporal shift module for efficient video understanding. Ji Lin, Chuang Gan, Song Han, ICCV. 815Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In ICCV, 2019. 8, 15\n\nBmn: Boundary-matching network for temporal action proposal generation. Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, Shilei Wen, ICCV. 816Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. Bmn: Boundary-matching network for temporal action pro- posal generation. In ICCV, 2019. 8, 16\n\nBsn: Boundary sensitive network for temporal action proposal generation. Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, Ming Yang, ECCV. 816Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang. Bsn: Boundary sensitive network for temporal action proposal generation. In ECCV, 2018. 8, 16\n\nDeep learning face attributes in the wild. Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang, ICCV. 11Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015. 11\n\nGlobal texture enhancement for fake face detection in the wild. Zhengzhe Liu, Xiaojuan Qi, Philip Hs Torr, CVPR. 614Zhengzhe Liu, Xiaojuan Qi, and Philip HS Torr. Global texture enhancement for fake face detection in the wild. In CVPR, 2020. 6, 14\n\nThe ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english. R Steven, Livingstone, A Frank, Russo, PLOS ONE. 13511Steven R Livingstone and Frank A Russo. The ryer- son audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vo- cal expressions in north american english. PLOS ONE, 13(5):e0196391, 2018. 2, 4, 10, 11\n\nIncremental learning for the detection and classification of gan-generated images. Francesco Marra, Cristiano Saltori, Giulia Boato, Luisa Verdoliva, WIFS. Francesco Marra, Cristiano Saltori, Giulia Boato, and Luisa Verdoliva. Incremental learning for the detection and classi- fication of gan-generated images. In WIFS, 2019. 2\n\nFsgan: Subject agnostic face swapping and reenactment. Yuval Nirkin, Yosi Keller, Tal Hassner, ICCV. 12Yuval Nirkin, Yosi Keller, and Tal Hassner. Fsgan: Subject agnostic face swapping and reenactment. In ICCV, 2019. 12\n\nDeepfakes and cheap fakes. Britt Paris, Joan Donovan, Data & SocietyUnited States of AmericaBritt Paris and Joan Donovan. Deepfakes and cheap fakes. United States of America: Data & Society, 2019. 2\n\nIvan Petrov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um\u00e9, Jian Jiang, R P Luis, Sheng Zhang, Pingyu Wu, arXiv:2005.05535A simple, flexible and extensible face swapping framework. 212arXiv preprintIvan Petrov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa Marangonda, Chris Um\u00e9, Jian Jiang, Luis RP, Sheng Zhang, Pingyu Wu, et al. Deepfacelab: A simple, flexi- ble and extensible face swapping framework. arXiv preprint arXiv:2005.05535, 2020. 2, 12\n\nThinking in frequency: Face forgery detection by mining frequency-aware clues. Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, Jing Shao, ECCV. 614Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing Shao. Thinking in frequency: Face forgery detection by min- ing frequency-aware clues. In ECCV, 2020. 6, 14\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, MICCAI. 714Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 7, 14\n\nFaceforen-sics++: Learning to detect manipulated facial images. Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, Matthias Nie\u00dfner, CVPR. 1015Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Chris- tian Riess, Justus Thies, and Matthias Nie\u00dfner. Faceforen- sics++: Learning to detect manipulated facial images. In CVPR, pages 1-11, 2019. 2, 3, 10, 15\n\nMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Mobilenetv2: Inverted residuals and linear bottlenecks. 14Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh- moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018. 14\n\nNo-reference image and video quality assessment: a classification and review of recent approaches. Muhammad Shahid, Andreas Rossholm, Benny L\u00f6vstr\u00f6m, Hans-J\u00fcrgen Zepernick, EURASIP J IMAGE VIDE. 2014140Muhammad Shahid, Andreas Rossholm, Benny L\u00f6vstr\u00f6m, and Hans-J\u00fcrgen Zepernick. No-reference image and video quality assessment: a classification and review of recent ap- proaches. EURASIP J IMAGE VIDE, 2014(1):40, 2014. 5\n\nFacefeat-gan: a two-stage approach for identity-preserving face synthesis. Yujun Shen, Bolei Zhou, Ping Luo, Xiaoou Tang, arXiv:1812.01288511arXiv preprintYujun Shen, Bolei Zhou, Ping Luo, and Xiaoou Tang. Facefeat-gan: a two-stage approach for identity-preserving face synthesis. arXiv preprint arXiv:1812.01288, 2018. 5, 11\n\nFirst order motion model for image animation. Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, Nicu Sebe, NeurIPS. 412Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In NeurIPS, 2019. 4, 11, 12\n\nMnasnet: Platform-aware neural architecture search for mobile. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, Quoc V Le, CVPR. 14Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnas- net: Platform-aware neural architecture search for mobile. In CVPR, 2019. 14\n\nMingxing Tan, V Quoc, Le, Efficientnet, arXiv:1905.11946Rethinking model scaling for convolutional neural networks. 614arXiv preprintMingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019. 6, 14\n\nDeep high-resolution representation learning for visual recognition. Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, TPAMI. 8714Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution represen- tation learning for visual recognition. TPAMI, 2020. 7, 8, 14\n\nExposing deep fakes using inconsistent head poses. Xin Yang, Yuezun Li, Siwei Lyu, ICASSP. 23Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In ICASSP, 2019. 2, 3\n\nAttributing fake images to gans: Analyzing fingerprints in generated images. Ning Yu, Larry Davis, Mario Fritz, arXiv:1811.08180arXiv preprintNing Yu, Larry Davis, and Mario Fritz. Attributing fake im- ages to gans: Analyzing fingerprints in generated images. arXiv preprint arXiv:1811.08180, 2018. 2\n\nHang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, Manmatha, arXiv:2004.08955Split-attention networks. 614arXiv preprintHang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020. 6, 14\n\nExploring self-attention for image recognition. Hengshuang Zhao, Jiaya Jia, Vladlen Koltun, CVPR. 614Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In CVPR, 2020. 6, 14\n\nTwo-stream neural networks for tampered face detection. Peng Zhou, Xintong Han, I Vlad, Larry S Morariu, Davis, CVPRW. 23Peng Zhou, Xintong Han, Vlad I Morariu, and Larry S Davis. Two-stream neural networks for tampered face detection. In CVPRW, 2017. 2, 3\n", "annotations": {"author": "[{\"end\":177,\"start\":72},{\"end\":253,\"start\":178},{\"end\":333,\"start\":254},{\"end\":408,\"start\":334},{\"end\":469,\"start\":409},{\"end\":594,\"start\":470},{\"end\":666,\"start\":595},{\"end\":746,\"start\":667},{\"end\":757,\"start\":747}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":78},{\"end\":185,\"start\":182},{\"end\":263,\"start\":259},{\"end\":345,\"start\":341},{\"end\":419,\"start\":416},{\"end\":482,\"start\":478},{\"end\":605,\"start\":600},{\"end\":676,\"start\":672},{\"end\":756,\"start\":753}]", "author_first_name": "[{\"end\":77,\"start\":72},{\"end\":181,\"start\":178},{\"end\":258,\"start\":254},{\"end\":340,\"start\":334},{\"end\":415,\"start\":409},{\"end\":477,\"start\":470},{\"end\":596,\"start\":595},{\"end\":599,\"start\":597},{\"end\":671,\"start\":667},{\"end\":752,\"start\":747}]", "author_affiliation": "[{\"end\":123,\"start\":104},{\"end\":176,\"start\":125},{\"end\":227,\"start\":208},{\"end\":252,\"start\":229},{\"end\":307,\"start\":288},{\"end\":332,\"start\":309},{\"end\":366,\"start\":347},{\"end\":407,\"start\":368},{\"end\":468,\"start\":445},{\"end\":503,\"start\":484},{\"end\":558,\"start\":505},{\"end\":593,\"start\":560},{\"end\":665,\"start\":626},{\"end\":720,\"start\":701},{\"end\":745,\"start\":722}]", "title": "[{\"end\":69,\"start\":1},{\"end\":826,\"start\":758}]", "venue": null, "abstract": "[{\"end\":1606,\"start\":945}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3155,\"start\":3151},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3158,\"start\":3155},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3161,\"start\":3158},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3612,\"start\":3608},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3615,\"start\":3612},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3791,\"start\":3787},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3794,\"start\":3791},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3797,\"start\":3794},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3800,\"start\":3797},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4988,\"start\":4985},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4991,\"start\":4988},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4994,\"start\":4991},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4997,\"start\":4994},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7746,\"start\":7742},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7758,\"start\":7754},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7784,\"start\":7780},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8025,\"start\":8022},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8121,\"start\":8117},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8236,\"start\":8232},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8720,\"start\":8716},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8731,\"start\":8727},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8746,\"start\":8742},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9733,\"start\":9730},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9736,\"start\":9733},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9739,\"start\":9736},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9742,\"start\":9739},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9745,\"start\":9742},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9748,\"start\":9745},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9751,\"start\":9748},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9754,\"start\":9751},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9757,\"start\":9754},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9760,\"start\":9757},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9863,\"start\":9859},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10654,\"start\":10653},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10928,\"start\":10925},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10942,\"start\":10938},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10958,\"start\":10954},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10976,\"start\":10972},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11791,\"start\":11788},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11794,\"start\":11791},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11797,\"start\":11794},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11800,\"start\":11797},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11803,\"start\":11800},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11806,\"start\":11803},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11809,\"start\":11806},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11812,\"start\":11809},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11815,\"start\":11812},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11818,\"start\":11815},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":11821,\"start\":11818},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":15452,\"start\":15448},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":15469,\"start\":15465},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15486,\"start\":15483},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15518,\"start\":15514},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":16154,\"start\":16150},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20491,\"start\":20487},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20982,\"start\":20978},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21023,\"start\":21019},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21052,\"start\":21048},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21123,\"start\":21119},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":21145,\"start\":21141},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21164,\"start\":21160},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":21192,\"start\":21188},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21315,\"start\":21311},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21332,\"start\":21328},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":21383,\"start\":21379},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24028,\"start\":24024},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":24138,\"start\":24134},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24684,\"start\":24680},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24705,\"start\":24701},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24715,\"start\":24711},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24739,\"start\":24735},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24988,\"start\":24984},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25059,\"start\":25055},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27952,\"start\":27948},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":28752,\"start\":28749},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28755,\"start\":28752},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28957,\"start\":28954},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":28960,\"start\":28957},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28963,\"start\":28960},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28966,\"start\":28963},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28969,\"start\":28966},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28972,\"start\":28969},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28975,\"start\":28972},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28978,\"start\":28975},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":28981,\"start\":28978},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":28984,\"start\":28981},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":28987,\"start\":28984},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":29269,\"start\":29265},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":29454,\"start\":29450},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29950,\"start\":29947},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":29982,\"start\":29978},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38096,\"start\":38092},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":38219,\"start\":38215},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":38296,\"start\":38292},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":38381,\"start\":38377},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":38474,\"start\":38470},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":38652,\"start\":38648},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":38698,\"start\":38694},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":38729,\"start\":38725},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":38859,\"start\":38855},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":39060,\"start\":39056},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":39315,\"start\":39311},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":39719,\"start\":39715},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39809,\"start\":39805},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":40131,\"start\":40127},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":40293,\"start\":40289},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":40856,\"start\":40852},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":41162,\"start\":41158},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":41888,\"start\":41884},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":42362,\"start\":42358},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":44067,\"start\":44063},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":44078,\"start\":44074},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":44116,\"start\":44112},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":44686,\"start\":44682},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":45791,\"start\":45787},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":45990,\"start\":45986},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":46496,\"start\":46492},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":47679,\"start\":47675},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":47692,\"start\":47688},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":48470,\"start\":48467},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":53715,\"start\":53712},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":53886,\"start\":53882},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":54104,\"start\":54100},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":54212,\"start\":54208},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":55187,\"start\":55184},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":55241,\"start\":55238},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":55243,\"start\":55241},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":57649,\"start\":57647},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":64257,\"start\":64253},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":64412,\"start\":64408},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":64477,\"start\":64476},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":65871,\"start\":65870}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":52356,\"start\":52075},{\"attributes\":{\"id\":\"fig_1\"},\"end\":52589,\"start\":52357},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52977,\"start\":52590},{\"attributes\":{\"id\":\"fig_4\"},\"end\":53291,\"start\":52978},{\"attributes\":{\"id\":\"fig_5\"},\"end\":53516,\"start\":53292},{\"attributes\":{\"id\":\"fig_6\"},\"end\":53610,\"start\":53517},{\"attributes\":{\"id\":\"fig_7\"},\"end\":54450,\"start\":53611},{\"attributes\":{\"id\":\"fig_8\"},\"end\":54758,\"start\":54451},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":56169,\"start\":54759},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":56867,\"start\":56170},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":57347,\"start\":56868},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":57925,\"start\":57348},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":58457,\"start\":57926},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":58931,\"start\":58458},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":59369,\"start\":58932},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":59893,\"start\":59370},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":60641,\"start\":59894},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":62037,\"start\":60642},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":62345,\"start\":62038},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":62943,\"start\":62346},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":63412,\"start\":62944},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":63720,\"start\":63413},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":64028,\"start\":63721}]", "paragraph": "[{\"end\":1978,\"start\":1618},{\"end\":3040,\"start\":1980},{\"end\":3497,\"start\":3057},{\"end\":4190,\"start\":3499},{\"end\":4723,\"start\":4192},{\"end\":5120,\"start\":4725},{\"end\":5516,\"start\":5122},{\"end\":7425,\"start\":5518},{\"end\":8616,\"start\":7443},{\"end\":9077,\"start\":8618},{\"end\":9418,\"start\":9079},{\"end\":9449,\"start\":9434},{\"end\":9493,\"start\":9463},{\"end\":9602,\"start\":9584},{\"end\":10843,\"start\":9684},{\"end\":11648,\"start\":10872},{\"end\":12123,\"start\":11669},{\"end\":13640,\"start\":12144},{\"end\":14111,\"start\":13682},{\"end\":14364,\"start\":14132},{\"end\":14535,\"start\":14473},{\"end\":14812,\"start\":14617},{\"end\":16222,\"start\":14814},{\"end\":16891,\"start\":16248},{\"end\":18132,\"start\":16910},{\"end\":18623,\"start\":18156},{\"end\":19386,\"start\":18683},{\"end\":19448,\"start\":19388},{\"end\":19673,\"start\":19450},{\"end\":19994,\"start\":19706},{\"end\":20292,\"start\":20023},{\"end\":20657,\"start\":20294},{\"end\":21384,\"start\":20725},{\"end\":21677,\"start\":21386},{\"end\":23872,\"start\":21679},{\"end\":24527,\"start\":23920},{\"end\":24807,\"start\":24595},{\"end\":25410,\"start\":24809},{\"end\":26176,\"start\":25444},{\"end\":26973,\"start\":26191},{\"end\":27253,\"start\":27008},{\"end\":27871,\"start\":27255},{\"end\":28420,\"start\":27873},{\"end\":28813,\"start\":28422},{\"end\":30507,\"start\":28837},{\"end\":31205,\"start\":30535},{\"end\":31905,\"start\":31225},{\"end\":32346,\"start\":31934},{\"end\":34255,\"start\":32348},{\"end\":34966,\"start\":34257},{\"end\":35268,\"start\":34968},{\"end\":36268,\"start\":35292},{\"end\":37326,\"start\":36323},{\"end\":37595,\"start\":37328},{\"end\":37709,\"start\":37645},{\"end\":37854,\"start\":37774},{\"end\":37974,\"start\":37856},{\"end\":38076,\"start\":37990},{\"end\":38450,\"start\":38078},{\"end\":38711,\"start\":38452},{\"end\":38842,\"start\":38713},{\"end\":39040,\"start\":38844},{\"end\":39291,\"start\":39042},{\"end\":39477,\"start\":39293},{\"end\":40005,\"start\":39479},{\"end\":40276,\"start\":40007},{\"end\":40541,\"start\":40278},{\"end\":40632,\"start\":40543},{\"end\":40834,\"start\":40634},{\"end\":41148,\"start\":40836},{\"end\":41362,\"start\":41150},{\"end\":43284,\"start\":41394},{\"end\":44627,\"start\":43310},{\"end\":45678,\"start\":44629},{\"end\":45779,\"start\":45694},{\"end\":45973,\"start\":45781},{\"end\":46230,\"start\":45975},{\"end\":46482,\"start\":46232},{\"end\":46756,\"start\":46484},{\"end\":46983,\"start\":46758},{\"end\":47169,\"start\":46985},{\"end\":47520,\"start\":47171},{\"end\":47640,\"start\":47522},{\"end\":48022,\"start\":47642},{\"end\":48437,\"start\":48054},{\"end\":50294,\"start\":48439},{\"end\":50474,\"start\":50296},{\"end\":51696,\"start\":50500},{\"end\":52074,\"start\":51736}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9554,\"start\":9494},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9657,\"start\":9603},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23888,\"start\":23873},{\"attributes\":{\"id\":\"formula_3\"},\"end\":37644,\"start\":37596},{\"attributes\":{\"id\":\"formula_4\"},\"end\":37773,\"start\":37710}]", "table_ref": null, "section_header": "[{\"end\":1616,\"start\":1608},{\"attributes\":{\"n\":\"1.\"},\"end\":3055,\"start\":3043},{\"attributes\":{\"n\":\"2.\"},\"end\":7441,\"start\":7428},{\"end\":9432,\"start\":9421},{\"end\":9461,\"start\":9452},{\"end\":9582,\"start\":9556},{\"attributes\":{\"n\":\"3.\"},\"end\":9682,\"start\":9659},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10870,\"start\":10846},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11667,\"start\":11651},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":12142,\"start\":12126},{\"end\":13680,\"start\":13643},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":14130,\"start\":14114},{\"end\":14395,\"start\":14367},{\"end\":14471,\"start\":14398},{\"end\":14565,\"start\":14538},{\"end\":14581,\"start\":14568},{\"end\":14615,\"start\":14584},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16246,\"start\":16225},{\"end\":16908,\"start\":16894},{\"attributes\":{\"n\":\"4.\"},\"end\":18154,\"start\":18135},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18650,\"start\":18626},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":18681,\"start\":18653},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":19704,\"start\":19676},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20021,\"start\":19997},{\"attributes\":{\"n\":\"5.\"},\"end\":20692,\"start\":20660},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20723,\"start\":20695},{\"attributes\":{\"n\":\"5.2.\"},\"end\":23918,\"start\":23890},{\"attributes\":{\"n\":\"6.\"},\"end\":24562,\"start\":24530},{\"attributes\":{\"n\":\"6.1.\"},\"end\":24593,\"start\":24565},{\"attributes\":{\"n\":\"6.2.\"},\"end\":25442,\"start\":25413},{\"attributes\":{\"n\":\"7.\"},\"end\":26189,\"start\":26179},{\"end\":27006,\"start\":26976},{\"end\":28835,\"start\":28816},{\"end\":30533,\"start\":30510},{\"end\":31223,\"start\":31208},{\"end\":31932,\"start\":31908},{\"end\":35290,\"start\":35271},{\"end\":36306,\"start\":36271},{\"end\":36321,\"start\":36309},{\"end\":37988,\"start\":37977},{\"end\":41392,\"start\":41365},{\"end\":43308,\"start\":43287},{\"end\":45692,\"start\":45681},{\"end\":48052,\"start\":48025},{\"end\":50498,\"start\":50477},{\"end\":51734,\"start\":51699},{\"end\":52086,\"start\":52076},{\"end\":52368,\"start\":52358},{\"end\":52601,\"start\":52591},{\"end\":52989,\"start\":52979},{\"end\":53303,\"start\":53293},{\"end\":53528,\"start\":53518},{\"end\":54463,\"start\":54452},{\"end\":54769,\"start\":54760},{\"end\":56180,\"start\":56171},{\"end\":56878,\"start\":56869},{\"end\":57358,\"start\":57349},{\"end\":57936,\"start\":57927},{\"end\":58468,\"start\":58459},{\"end\":58942,\"start\":58933},{\"end\":59380,\"start\":59371},{\"end\":59904,\"start\":59895},{\"end\":60653,\"start\":60643},{\"end\":62049,\"start\":62039},{\"end\":62955,\"start\":62945},{\"end\":63424,\"start\":63414},{\"end\":63732,\"start\":63722}]", "table": "[{\"end\":56169,\"start\":55312},{\"end\":56867,\"start\":56316},{\"end\":57347,\"start\":57032},{\"end\":57925,\"start\":57828},{\"end\":58457,\"start\":58115},{\"end\":58931,\"start\":58672},{\"end\":59369,\"start\":58949},{\"end\":59893,\"start\":59673},{\"end\":60641,\"start\":59991},{\"end\":62037,\"start\":61010},{\"end\":62345,\"start\":62228},{\"end\":63412,\"start\":63224},{\"end\":63720,\"start\":63603},{\"end\":64028,\"start\":63957}]", "figure_caption": "[{\"end\":52356,\"start\":52088},{\"end\":52589,\"start\":52370},{\"end\":52977,\"start\":52603},{\"end\":53291,\"start\":52991},{\"end\":53516,\"start\":53305},{\"end\":53610,\"start\":53530},{\"end\":54450,\"start\":53613},{\"end\":54758,\"start\":54466},{\"end\":55312,\"start\":54771},{\"end\":56316,\"start\":56182},{\"end\":57032,\"start\":56880},{\"end\":57828,\"start\":57360},{\"end\":58115,\"start\":57938},{\"end\":58672,\"start\":58470},{\"end\":58949,\"start\":58944},{\"end\":59673,\"start\":59382},{\"end\":59991,\"start\":59906},{\"end\":61010,\"start\":60656},{\"end\":62228,\"start\":62052},{\"end\":62943,\"start\":62348},{\"end\":63224,\"start\":62958},{\"end\":63603,\"start\":63427},{\"end\":63957,\"start\":63735}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":5515,\"start\":5507},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5925,\"start\":5916},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6151,\"start\":6145},{\"end\":11416,\"start\":11410},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12327,\"start\":12321},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12383,\"start\":12377},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13149,\"start\":13138},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13195,\"start\":13189},{\"end\":14665,\"start\":14657},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14851,\"start\":14845},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15770,\"start\":15764},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15899,\"start\":15893},{\"end\":16948,\"start\":16940},{\"end\":17218,\"start\":17212},{\"end\":18476,\"start\":18470},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21853,\"start\":21844},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22813,\"start\":22807},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22915,\"start\":22906},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23600,\"start\":23591},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24421,\"start\":24412},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24484,\"start\":24475},{\"end\":27996,\"start\":27990},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30488,\"start\":30476},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30869,\"start\":30862},{\"end\":32898,\"start\":32888},{\"end\":33244,\"start\":33234},{\"end\":33386,\"start\":33376},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51132,\"start\":51123},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51801,\"start\":51794}]", "bib_author_first_name": "[{\"end\":66941,\"start\":66935},{\"end\":66956,\"start\":66951},{\"end\":67196,\"start\":67190},{\"end\":67203,\"start\":67202},{\"end\":67220,\"start\":67219},{\"end\":67241,\"start\":67240},{\"end\":67252,\"start\":67249},{\"end\":67264,\"start\":67258},{\"end\":67601,\"start\":67597},{\"end\":67618,\"start\":67612},{\"end\":67862,\"start\":67858},{\"end\":67870,\"start\":67869},{\"end\":67883,\"start\":67877},{\"end\":67901,\"start\":67892},{\"end\":68168,\"start\":68166},{\"end\":68180,\"start\":68175},{\"end\":68195,\"start\":68190},{\"end\":68214,\"start\":68207},{\"end\":68494,\"start\":68488},{\"end\":68510,\"start\":68501},{\"end\":68521,\"start\":68515},{\"end\":68535,\"start\":68527},{\"end\":68777,\"start\":68769},{\"end\":68944,\"start\":68939},{\"end\":68967,\"start\":68961},{\"end\":69215,\"start\":69212},{\"end\":69226,\"start\":69222},{\"end\":69236,\"start\":69232},{\"end\":69256,\"start\":69248},{\"end\":69483,\"start\":69480},{\"end\":69493,\"start\":69490},{\"end\":69507,\"start\":69500},{\"end\":69522,\"start\":69516},{\"end\":69530,\"start\":69527},{\"end\":69537,\"start\":69535},{\"end\":69798,\"start\":69790},{\"end\":69808,\"start\":69805},{\"end\":69823,\"start\":69814},{\"end\":69839,\"start\":69834},{\"end\":69856,\"start\":69848},{\"end\":70137,\"start\":70135},{\"end\":70152,\"start\":70144},{\"end\":70163,\"start\":70159},{\"end\":70174,\"start\":70170},{\"end\":70183,\"start\":70180},{\"end\":70374,\"start\":70369},{\"end\":70392,\"start\":70386},{\"end\":70404,\"start\":70401},{\"end\":70418,\"start\":70413},{\"end\":70427,\"start\":70423},{\"end\":70442,\"start\":70435},{\"end\":70464,\"start\":70449},{\"end\":70829,\"start\":70821},{\"end\":70838,\"start\":70837},{\"end\":70840,\"start\":70839},{\"end\":70855,\"start\":70850},{\"end\":70871,\"start\":70863},{\"end\":70887,\"start\":70880},{\"end\":70901,\"start\":70896},{\"end\":70921,\"start\":70914},{\"end\":70934,\"start\":70930},{\"end\":70948,\"start\":70944},{\"end\":71336,\"start\":71335},{\"end\":71346,\"start\":71345},{\"end\":71357,\"start\":71356},{\"end\":71365,\"start\":71364},{\"end\":71374,\"start\":71373},{\"end\":71384,\"start\":71383},{\"end\":71386,\"start\":71385},{\"end\":71398,\"start\":71397},{\"end\":71784,\"start\":71775},{\"end\":71968,\"start\":71959},{\"end\":71989,\"start\":71984},{\"end\":72003,\"start\":71995},{\"end\":72018,\"start\":72011},{\"end\":72209,\"start\":72205},{\"end\":72222,\"start\":72217},{\"end\":72238,\"start\":72231},{\"end\":72254,\"start\":72250},{\"end\":72271,\"start\":72268},{\"end\":72286,\"start\":72283},{\"end\":72288,\"start\":72287},{\"end\":72302,\"start\":72298},{\"end\":72315,\"start\":72311},{\"end\":72330,\"start\":72321},{\"end\":72348,\"start\":72341},{\"end\":72608,\"start\":72601},{\"end\":72621,\"start\":72617},{\"end\":72628,\"start\":72622},{\"end\":72642,\"start\":72638},{\"end\":72656,\"start\":72650},{\"end\":72677,\"start\":72672},{\"end\":72694,\"start\":72688},{\"end\":72711,\"start\":72705},{\"end\":72728,\"start\":72721},{\"end\":72744,\"start\":72735},{\"end\":73133,\"start\":73128},{\"end\":73146,\"start\":73141},{\"end\":73159,\"start\":73155},{\"end\":73176,\"start\":73170},{\"end\":73194,\"start\":73188},{\"end\":73211,\"start\":73207},{\"end\":73226,\"start\":73220},{\"end\":73661,\"start\":73657},{\"end\":73678,\"start\":73672},{\"end\":73698,\"start\":73688},{\"end\":74020,\"start\":74015},{\"end\":74057,\"start\":74036},{\"end\":74072,\"start\":74068},{\"end\":74419,\"start\":74412},{\"end\":74431,\"start\":74424},{\"end\":74447,\"start\":74439},{\"end\":74457,\"start\":74453},{\"end\":74667,\"start\":74661},{\"end\":74680,\"start\":74676},{\"end\":74695,\"start\":74690},{\"end\":74712,\"start\":74701},{\"end\":74721,\"start\":74719},{\"end\":74736,\"start\":74728},{\"end\":74748,\"start\":74742},{\"end\":74760,\"start\":74755},{\"end\":74970,\"start\":74969},{\"end\":74987,\"start\":74979},{\"end\":74998,\"start\":74996},{\"end\":75010,\"start\":75004},{\"end\":75023,\"start\":75017},{\"end\":75044,\"start\":75038},{\"end\":75056,\"start\":75051},{\"end\":75072,\"start\":75065},{\"end\":75497,\"start\":75494},{\"end\":75504,\"start\":75502},{\"end\":75515,\"start\":75511},{\"end\":75698,\"start\":75694},{\"end\":75716,\"start\":75711},{\"end\":75733,\"start\":75727},{\"end\":75979,\"start\":75973},{\"end\":75990,\"start\":75987},{\"end\":76000,\"start\":75995},{\"end\":76009,\"start\":76005},{\"end\":76027,\"start\":76016},{\"end\":76316,\"start\":76308},{\"end\":76329,\"start\":76321},{\"end\":76547,\"start\":76543},{\"end\":76562,\"start\":76556},{\"end\":76575,\"start\":76570},{\"end\":76590,\"start\":76585},{\"end\":76607,\"start\":76601},{\"end\":76622,\"start\":76618},{\"end\":76881,\"start\":76876},{\"end\":76902,\"start\":76893},{\"end\":77170,\"start\":77165},{\"end\":77193,\"start\":77186},{\"end\":77203,\"start\":77199},{\"end\":77453,\"start\":77446},{\"end\":77465,\"start\":77458},{\"end\":77474,\"start\":77471},{\"end\":77485,\"start\":77481},{\"end\":77496,\"start\":77492},{\"end\":77796,\"start\":77790},{\"end\":77804,\"start\":77801},{\"end\":77813,\"start\":77811},{\"end\":77827,\"start\":77819},{\"end\":77837,\"start\":77832},{\"end\":78062,\"start\":78060},{\"end\":78074,\"start\":78068},{\"end\":78084,\"start\":78080},{\"end\":78296,\"start\":78289},{\"end\":78306,\"start\":78302},{\"end\":78315,\"start\":78312},{\"end\":78325,\"start\":78320},{\"end\":78338,\"start\":78332},{\"end\":78588,\"start\":78581},{\"end\":78596,\"start\":78594},{\"end\":78611,\"start\":78603},{\"end\":78625,\"start\":78616},{\"end\":78636,\"start\":78632},{\"end\":78861,\"start\":78856},{\"end\":78871,\"start\":78867},{\"end\":78885,\"start\":78877},{\"end\":78898,\"start\":78892},{\"end\":79100,\"start\":79092},{\"end\":79114,\"start\":79106},{\"end\":79128,\"start\":79119},{\"end\":79437,\"start\":79436},{\"end\":79460,\"start\":79459},{\"end\":79831,\"start\":79822},{\"end\":79848,\"start\":79839},{\"end\":79864,\"start\":79858},{\"end\":79877,\"start\":79872},{\"end\":80129,\"start\":80124},{\"end\":80142,\"start\":80138},{\"end\":80154,\"start\":80151},{\"end\":80322,\"start\":80317},{\"end\":80334,\"start\":80330},{\"end\":80494,\"start\":80490},{\"end\":80510,\"start\":80503},{\"end\":80523,\"start\":80516},{\"end\":80541,\"start\":80535},{\"end\":80553,\"start\":80547},{\"end\":80571,\"start\":80566},{\"end\":80581,\"start\":80577},{\"end\":80590,\"start\":80589},{\"end\":80592,\"start\":80591},{\"end\":80604,\"start\":80599},{\"end\":80618,\"start\":80612},{\"end\":81059,\"start\":81053},{\"end\":81072,\"start\":81066},{\"end\":81080,\"start\":81078},{\"end\":81094,\"start\":81088},{\"end\":81105,\"start\":81101},{\"end\":81356,\"start\":81352},{\"end\":81377,\"start\":81370},{\"end\":81393,\"start\":81387},{\"end\":81623,\"start\":81616},{\"end\":81639,\"start\":81633},{\"end\":81656,\"start\":81651},{\"end\":81677,\"start\":81668},{\"end\":81691,\"start\":81685},{\"end\":81707,\"start\":81699},{\"end\":81944,\"start\":81940},{\"end\":81960,\"start\":81954},{\"end\":81977,\"start\":81969},{\"end\":81989,\"start\":81983},{\"end\":82012,\"start\":82001},{\"end\":82344,\"start\":82336},{\"end\":82360,\"start\":82353},{\"end\":82376,\"start\":82371},{\"end\":82398,\"start\":82387},{\"end\":82741,\"start\":82736},{\"end\":82753,\"start\":82748},{\"end\":82764,\"start\":82760},{\"end\":82776,\"start\":82770},{\"end\":83044,\"start\":83034},{\"end\":83063,\"start\":83055},{\"end\":83083,\"start\":83077},{\"end\":83099,\"start\":83094},{\"end\":83111,\"start\":83107},{\"end\":83364,\"start\":83356},{\"end\":83372,\"start\":83370},{\"end\":83386,\"start\":83379},{\"end\":83398,\"start\":83393},{\"end\":83414,\"start\":83410},{\"end\":83430,\"start\":83424},{\"end\":83445,\"start\":83439},{\"end\":83648,\"start\":83640},{\"end\":83655,\"start\":83654},{\"end\":83998,\"start\":83990},{\"end\":84007,\"start\":84005},{\"end\":84021,\"start\":84013},{\"end\":84034,\"start\":84029},{\"end\":84049,\"start\":84042},{\"end\":84060,\"start\":84056},{\"end\":84071,\"start\":84067},{\"end\":84083,\"start\":84077},{\"end\":84095,\"start\":84088},{\"end\":84109,\"start\":84101},{\"end\":84408,\"start\":84405},{\"end\":84421,\"start\":84415},{\"end\":84431,\"start\":84426},{\"end\":84638,\"start\":84634},{\"end\":84648,\"start\":84643},{\"end\":84661,\"start\":84656},{\"end\":84863,\"start\":84859},{\"end\":84879,\"start\":84871},{\"end\":84892,\"start\":84884},{\"end\":84902,\"start\":84900},{\"end\":84911,\"start\":84908},{\"end\":84925,\"start\":84919},{\"end\":84934,\"start\":84931},{\"end\":84944,\"start\":84940},{\"end\":84954,\"start\":84949},{\"end\":85296,\"start\":85286},{\"end\":85308,\"start\":85303},{\"end\":85321,\"start\":85314},{\"end\":85517,\"start\":85513},{\"end\":85531,\"start\":85524},{\"end\":85538,\"start\":85537},{\"end\":85552,\"start\":85545}]", "bib_author_last_name": "[{\"end\":66512,\"start\":66506},{\"end\":66949,\"start\":66942},{\"end\":66963,\"start\":66957},{\"end\":67200,\"start\":67197},{\"end\":67209,\"start\":67204},{\"end\":67217,\"start\":67211},{\"end\":67228,\"start\":67221},{\"end\":67238,\"start\":67230},{\"end\":67247,\"start\":67242},{\"end\":67256,\"start\":67253},{\"end\":67272,\"start\":67265},{\"end\":67279,\"start\":67274},{\"end\":67610,\"start\":67602},{\"end\":67628,\"start\":67619},{\"end\":67867,\"start\":67863},{\"end\":67875,\"start\":67871},{\"end\":67890,\"start\":67884},{\"end\":67906,\"start\":67902},{\"end\":67910,\"start\":67908},{\"end\":68173,\"start\":68169},{\"end\":68188,\"start\":68181},{\"end\":68205,\"start\":68196},{\"end\":68223,\"start\":68215},{\"end\":68499,\"start\":68495},{\"end\":68513,\"start\":68511},{\"end\":68525,\"start\":68522},{\"end\":68538,\"start\":68536},{\"end\":68785,\"start\":68778},{\"end\":68959,\"start\":68945},{\"end\":68975,\"start\":68968},{\"end\":68986,\"start\":68977},{\"end\":69220,\"start\":69216},{\"end\":69230,\"start\":69227},{\"end\":69246,\"start\":69237},{\"end\":69260,\"start\":69257},{\"end\":69266,\"start\":69262},{\"end\":69488,\"start\":69484},{\"end\":69498,\"start\":69494},{\"end\":69514,\"start\":69508},{\"end\":69525,\"start\":69523},{\"end\":69533,\"start\":69531},{\"end\":69545,\"start\":69538},{\"end\":69803,\"start\":69799},{\"end\":69812,\"start\":69809},{\"end\":69832,\"start\":69824},{\"end\":69846,\"start\":69840},{\"end\":69866,\"start\":69857},{\"end\":70142,\"start\":70138},{\"end\":70157,\"start\":70153},{\"end\":70168,\"start\":70164},{\"end\":70178,\"start\":70175},{\"end\":70188,\"start\":70184},{\"end\":70384,\"start\":70375},{\"end\":70399,\"start\":70393},{\"end\":70411,\"start\":70405},{\"end\":70421,\"start\":70419},{\"end\":70433,\"start\":70428},{\"end\":70447,\"start\":70443},{\"end\":70471,\"start\":70465},{\"end\":70835,\"start\":70830},{\"end\":70848,\"start\":70841},{\"end\":70861,\"start\":70856},{\"end\":70878,\"start\":70872},{\"end\":70894,\"start\":70888},{\"end\":70912,\"start\":70902},{\"end\":70928,\"start\":70922},{\"end\":70942,\"start\":70935},{\"end\":70956,\"start\":70949},{\"end\":70969,\"start\":70958},{\"end\":71343,\"start\":71337},{\"end\":71354,\"start\":71347},{\"end\":71362,\"start\":71358},{\"end\":71371,\"start\":71366},{\"end\":71381,\"start\":71375},{\"end\":71395,\"start\":71387},{\"end\":71406,\"start\":71399},{\"end\":71418,\"start\":71408},{\"end\":71798,\"start\":71785},{\"end\":71982,\"start\":71969},{\"end\":71993,\"start\":71990},{\"end\":72009,\"start\":72004},{\"end\":72021,\"start\":72019},{\"end\":72215,\"start\":72210},{\"end\":72229,\"start\":72223},{\"end\":72248,\"start\":72239},{\"end\":72266,\"start\":72255},{\"end\":72281,\"start\":72272},{\"end\":72296,\"start\":72289},{\"end\":72309,\"start\":72303},{\"end\":72319,\"start\":72316},{\"end\":72339,\"start\":72331},{\"end\":72357,\"start\":72349},{\"end\":72615,\"start\":72609},{\"end\":72636,\"start\":72629},{\"end\":72648,\"start\":72643},{\"end\":72670,\"start\":72657},{\"end\":72686,\"start\":72678},{\"end\":72703,\"start\":72695},{\"end\":72719,\"start\":72712},{\"end\":72733,\"start\":72729},{\"end\":72748,\"start\":72745},{\"end\":73139,\"start\":73134},{\"end\":73153,\"start\":73147},{\"end\":73168,\"start\":73160},{\"end\":73186,\"start\":73177},{\"end\":73205,\"start\":73195},{\"end\":73218,\"start\":73212},{\"end\":73234,\"start\":73227},{\"end\":73670,\"start\":73662},{\"end\":73686,\"start\":73679},{\"end\":73707,\"start\":73699},{\"end\":74034,\"start\":74021},{\"end\":74066,\"start\":74058},{\"end\":74080,\"start\":74073},{\"end\":74422,\"start\":74420},{\"end\":74437,\"start\":74432},{\"end\":74451,\"start\":74448},{\"end\":74461,\"start\":74458},{\"end\":74674,\"start\":74668},{\"end\":74688,\"start\":74681},{\"end\":74699,\"start\":74696},{\"end\":74717,\"start\":74713},{\"end\":74726,\"start\":74722},{\"end\":74740,\"start\":74737},{\"end\":74753,\"start\":74749},{\"end\":74764,\"start\":74761},{\"end\":74977,\"start\":74971},{\"end\":74994,\"start\":74988},{\"end\":75002,\"start\":74999},{\"end\":75015,\"start\":75011},{\"end\":75036,\"start\":75024},{\"end\":75049,\"start\":75045},{\"end\":75063,\"start\":75057},{\"end\":75082,\"start\":75073},{\"end\":75088,\"start\":75084},{\"end\":75500,\"start\":75498},{\"end\":75509,\"start\":75505},{\"end\":75519,\"start\":75516},{\"end\":75709,\"start\":75699},{\"end\":75725,\"start\":75717},{\"end\":75741,\"start\":75734},{\"end\":75985,\"start\":75980},{\"end\":75993,\"start\":75991},{\"end\":76003,\"start\":76001},{\"end\":76014,\"start\":76010},{\"end\":76031,\"start\":76028},{\"end\":76319,\"start\":76317},{\"end\":76334,\"start\":76330},{\"end\":76554,\"start\":76548},{\"end\":76568,\"start\":76563},{\"end\":76583,\"start\":76576},{\"end\":76599,\"start\":76591},{\"end\":76616,\"start\":76608},{\"end\":76627,\"start\":76623},{\"end\":76891,\"start\":76882},{\"end\":76909,\"start\":76903},{\"end\":77184,\"start\":77171},{\"end\":77197,\"start\":77194},{\"end\":77206,\"start\":77204},{\"end\":77211,\"start\":77208},{\"end\":77456,\"start\":77454},{\"end\":77469,\"start\":77466},{\"end\":77479,\"start\":77475},{\"end\":77490,\"start\":77486},{\"end\":77500,\"start\":77497},{\"end\":77799,\"start\":77797},{\"end\":77809,\"start\":77805},{\"end\":77817,\"start\":77814},{\"end\":77830,\"start\":77828},{\"end\":77841,\"start\":77838},{\"end\":78066,\"start\":78063},{\"end\":78078,\"start\":78075},{\"end\":78088,\"start\":78085},{\"end\":78300,\"start\":78297},{\"end\":78310,\"start\":78307},{\"end\":78318,\"start\":78316},{\"end\":78330,\"start\":78326},{\"end\":78342,\"start\":78339},{\"end\":78592,\"start\":78589},{\"end\":78601,\"start\":78597},{\"end\":78614,\"start\":78612},{\"end\":78630,\"start\":78626},{\"end\":78641,\"start\":78637},{\"end\":78865,\"start\":78862},{\"end\":78875,\"start\":78872},{\"end\":78890,\"start\":78886},{\"end\":78903,\"start\":78899},{\"end\":79104,\"start\":79101},{\"end\":79117,\"start\":79115},{\"end\":79133,\"start\":79129},{\"end\":79444,\"start\":79438},{\"end\":79457,\"start\":79446},{\"end\":79466,\"start\":79461},{\"end\":79473,\"start\":79468},{\"end\":79837,\"start\":79832},{\"end\":79856,\"start\":79849},{\"end\":79870,\"start\":79865},{\"end\":79887,\"start\":79878},{\"end\":80136,\"start\":80130},{\"end\":80149,\"start\":80143},{\"end\":80162,\"start\":80155},{\"end\":80328,\"start\":80323},{\"end\":80342,\"start\":80335},{\"end\":80501,\"start\":80495},{\"end\":80514,\"start\":80511},{\"end\":80533,\"start\":80524},{\"end\":80545,\"start\":80542},{\"end\":80564,\"start\":80554},{\"end\":80575,\"start\":80572},{\"end\":80587,\"start\":80582},{\"end\":80597,\"start\":80593},{\"end\":80610,\"start\":80605},{\"end\":80621,\"start\":80619},{\"end\":81064,\"start\":81060},{\"end\":81076,\"start\":81073},{\"end\":81086,\"start\":81081},{\"end\":81099,\"start\":81095},{\"end\":81110,\"start\":81106},{\"end\":81368,\"start\":81357},{\"end\":81385,\"start\":81378},{\"end\":81398,\"start\":81394},{\"end\":81631,\"start\":81624},{\"end\":81649,\"start\":81640},{\"end\":81666,\"start\":81657},{\"end\":81683,\"start\":81678},{\"end\":81697,\"start\":81692},{\"end\":81715,\"start\":81708},{\"end\":81952,\"start\":81945},{\"end\":81967,\"start\":81961},{\"end\":81981,\"start\":81978},{\"end\":81999,\"start\":81990},{\"end\":82017,\"start\":82013},{\"end\":82351,\"start\":82345},{\"end\":82369,\"start\":82361},{\"end\":82385,\"start\":82377},{\"end\":82408,\"start\":82399},{\"end\":82746,\"start\":82742},{\"end\":82758,\"start\":82754},{\"end\":82768,\"start\":82765},{\"end\":82781,\"start\":82777},{\"end\":83053,\"start\":83045},{\"end\":83075,\"start\":83064},{\"end\":83092,\"start\":83084},{\"end\":83105,\"start\":83100},{\"end\":83116,\"start\":83112},{\"end\":83368,\"start\":83365},{\"end\":83377,\"start\":83373},{\"end\":83391,\"start\":83387},{\"end\":83408,\"start\":83399},{\"end\":83422,\"start\":83415},{\"end\":83437,\"start\":83431},{\"end\":83448,\"start\":83446},{\"end\":83652,\"start\":83649},{\"end\":83660,\"start\":83656},{\"end\":83664,\"start\":83662},{\"end\":83678,\"start\":83666},{\"end\":84003,\"start\":83999},{\"end\":84011,\"start\":84008},{\"end\":84027,\"start\":84022},{\"end\":84040,\"start\":84035},{\"end\":84054,\"start\":84050},{\"end\":84065,\"start\":84061},{\"end\":84075,\"start\":84072},{\"end\":84086,\"start\":84084},{\"end\":84099,\"start\":84096},{\"end\":84114,\"start\":84110},{\"end\":84413,\"start\":84409},{\"end\":84424,\"start\":84422},{\"end\":84435,\"start\":84432},{\"end\":84641,\"start\":84639},{\"end\":84654,\"start\":84649},{\"end\":84667,\"start\":84662},{\"end\":84869,\"start\":84864},{\"end\":84882,\"start\":84880},{\"end\":84898,\"start\":84893},{\"end\":84906,\"start\":84903},{\"end\":84917,\"start\":84912},{\"end\":84929,\"start\":84926},{\"end\":84938,\"start\":84935},{\"end\":84947,\"start\":84945},{\"end\":84962,\"start\":84955},{\"end\":84972,\"start\":84964},{\"end\":85301,\"start\":85297},{\"end\":85312,\"start\":85309},{\"end\":85328,\"start\":85322},{\"end\":85522,\"start\":85518},{\"end\":85535,\"start\":85532},{\"end\":85543,\"start\":85539},{\"end\":85560,\"start\":85553},{\"end\":85567,\"start\":85562}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":66502,\"start\":66429},{\"attributes\":{\"id\":\"b1\"},\"end\":66581,\"start\":66504},{\"attributes\":{\"id\":\"b2\"},\"end\":66634,\"start\":66583},{\"attributes\":{\"id\":\"b3\"},\"end\":66874,\"start\":66636},{\"attributes\":{\"doi\":\"arXiv:1907.02157\",\"id\":\"b4\"},\"end\":67128,\"start\":66876},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14369452},\"end\":67526,\"start\":67130},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206596127},\"end\":67777,\"start\":67528},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":109936942},\"end\":68085,\"start\":67779},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4747},\"end\":68428,\"start\":68087},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":208617800},\"end\":68704,\"start\":68430},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2375110},\"end\":68900,\"start\":68706},{\"attributes\":{\"doi\":\"arXiv:1806.05622\",\"id\":\"b11\"},\"end\":69163,\"start\":68902},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":203737259},\"end\":69425,\"start\":69165},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":57246310},\"end\":69721,\"start\":69427},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":219964874},\"end\":70042,\"start\":69723},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":216144533},\"end\":70365,\"start\":70044},{\"attributes\":{\"doi\":\"arXiv:2006.07397\",\"id\":\"b16\"},\"end\":70745,\"start\":70367},{\"attributes\":{\"id\":\"b17\"},\"end\":71228,\"start\":70747},{\"attributes\":{\"doi\":\"arXiv:1804.03619\",\"id\":\"b18\"},\"end\":71711,\"start\":71230},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":215548929},\"end\":71916,\"start\":71713},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":54463801},\"end\":72161,\"start\":71918},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":174798209},\"end\":72599,\"start\":72163},{\"attributes\":{\"doi\":\"arXiv:1808.03766\",\"id\":\"b22\"},\"end\":73126,\"start\":72601},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b23\"},\"end\":73601,\"start\":73128},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":216056161},\"end\":73843,\"start\":73603},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":115236152},\"end\":74364,\"start\":73845},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206594692},\"end\":74593,\"start\":74366},{\"attributes\":{\"id\":\"b27\"},\"end\":74967,\"start\":74595},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b28\"},\"end\":75459,\"start\":74969},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":140309863},\"end\":75629,\"start\":75461},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":218596111},\"end\":75889,\"start\":75631},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":210116488},\"end\":76222,\"start\":75891},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":67750926},\"end\":76486,\"start\":76224},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":209202273},\"end\":76803,\"start\":76488},{\"attributes\":{\"doi\":\"arXiv:1812.08685\",\"id\":\"b34\"},\"end\":77095,\"start\":76805},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":198967908},\"end\":77374,\"start\":77097},{\"attributes\":{\"doi\":\"arXiv:1912.13457\",\"id\":\"b36\"},\"end\":77720,\"start\":77376},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":212726430},\"end\":77996,\"start\":77722},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":85542740},\"end\":78215,\"start\":77998},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":198179957},\"end\":78506,\"start\":78217},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":47009464},\"end\":78811,\"start\":78508},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":459456},\"end\":79026,\"start\":78813},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":211010785},\"end\":79275,\"start\":79028},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":21704094},\"end\":79737,\"start\":79277},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":203641883},\"end\":80067,\"start\":79739},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":201058495},\"end\":80288,\"start\":80069},{\"attributes\":{\"id\":\"b46\"},\"end\":80488,\"start\":80290},{\"attributes\":{\"doi\":\"arXiv:2005.05535\",\"id\":\"b47\"},\"end\":80972,\"start\":80490},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":220647499},\"end\":81285,\"start\":80974},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":3719281},\"end\":81550,\"start\":81287},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":59292011},\"end\":81938,\"start\":81552},{\"attributes\":{\"id\":\"b51\"},\"end\":82235,\"start\":81940},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":3909983},\"end\":82659,\"start\":82237},{\"attributes\":{\"doi\":\"arXiv:1812.01288\",\"id\":\"b53\"},\"end\":82986,\"start\":82661},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":202767986},\"end\":83291,\"start\":82988},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":51891697},\"end\":83638,\"start\":83293},{\"attributes\":{\"doi\":\"arXiv:1905.11946\",\"id\":\"b56\"},\"end\":83919,\"start\":83640},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":201124533},\"end\":84352,\"start\":83921},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":53295714},\"end\":84555,\"start\":84354},{\"attributes\":{\"doi\":\"arXiv:1811.08180\",\"id\":\"b59\"},\"end\":84857,\"start\":84557},{\"attributes\":{\"doi\":\"arXiv:2004.08955\",\"id\":\"b60\"},\"end\":85236,\"start\":84859},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":215542547},\"end\":85455,\"start\":85238},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":4533859},\"end\":85713,\"start\":85457}]", "bib_title": "[{\"end\":67188,\"start\":67130},{\"end\":67595,\"start\":67528},{\"end\":67856,\"start\":67779},{\"end\":68164,\"start\":68087},{\"end\":68486,\"start\":68430},{\"end\":68767,\"start\":68706},{\"end\":69210,\"start\":69165},{\"end\":69478,\"start\":69427},{\"end\":69788,\"start\":69723},{\"end\":70133,\"start\":70044},{\"end\":71773,\"start\":71713},{\"end\":71957,\"start\":71918},{\"end\":72203,\"start\":72163},{\"end\":73655,\"start\":73603},{\"end\":74013,\"start\":73845},{\"end\":74410,\"start\":74366},{\"end\":74659,\"start\":74595},{\"end\":75492,\"start\":75461},{\"end\":75692,\"start\":75631},{\"end\":75971,\"start\":75891},{\"end\":76306,\"start\":76224},{\"end\":76541,\"start\":76488},{\"end\":77163,\"start\":77097},{\"end\":77788,\"start\":77722},{\"end\":78058,\"start\":77998},{\"end\":78287,\"start\":78217},{\"end\":78579,\"start\":78508},{\"end\":78854,\"start\":78813},{\"end\":79090,\"start\":79028},{\"end\":79434,\"start\":79277},{\"end\":79820,\"start\":79739},{\"end\":80122,\"start\":80069},{\"end\":81051,\"start\":80974},{\"end\":81350,\"start\":81287},{\"end\":81614,\"start\":81552},{\"end\":82334,\"start\":82237},{\"end\":83032,\"start\":82988},{\"end\":83354,\"start\":83293},{\"end\":83988,\"start\":83921},{\"end\":84403,\"start\":84354},{\"end\":85284,\"start\":85238},{\"end\":85511,\"start\":85457}]", "bib_author": "[{\"end\":66514,\"start\":66506},{\"end\":66951,\"start\":66935},{\"end\":66965,\"start\":66951},{\"end\":67202,\"start\":67190},{\"end\":67211,\"start\":67202},{\"end\":67219,\"start\":67211},{\"end\":67230,\"start\":67219},{\"end\":67240,\"start\":67230},{\"end\":67249,\"start\":67240},{\"end\":67258,\"start\":67249},{\"end\":67274,\"start\":67258},{\"end\":67281,\"start\":67274},{\"end\":67612,\"start\":67597},{\"end\":67630,\"start\":67612},{\"end\":67869,\"start\":67858},{\"end\":67877,\"start\":67869},{\"end\":67892,\"start\":67877},{\"end\":67908,\"start\":67892},{\"end\":67912,\"start\":67908},{\"end\":68175,\"start\":68166},{\"end\":68190,\"start\":68175},{\"end\":68207,\"start\":68190},{\"end\":68225,\"start\":68207},{\"end\":68501,\"start\":68488},{\"end\":68515,\"start\":68501},{\"end\":68527,\"start\":68515},{\"end\":68540,\"start\":68527},{\"end\":68787,\"start\":68769},{\"end\":68961,\"start\":68939},{\"end\":68977,\"start\":68961},{\"end\":68988,\"start\":68977},{\"end\":69222,\"start\":69212},{\"end\":69232,\"start\":69222},{\"end\":69248,\"start\":69232},{\"end\":69262,\"start\":69248},{\"end\":69268,\"start\":69262},{\"end\":69490,\"start\":69480},{\"end\":69500,\"start\":69490},{\"end\":69516,\"start\":69500},{\"end\":69527,\"start\":69516},{\"end\":69535,\"start\":69527},{\"end\":69547,\"start\":69535},{\"end\":69805,\"start\":69790},{\"end\":69814,\"start\":69805},{\"end\":69834,\"start\":69814},{\"end\":69848,\"start\":69834},{\"end\":69868,\"start\":69848},{\"end\":70144,\"start\":70135},{\"end\":70159,\"start\":70144},{\"end\":70170,\"start\":70159},{\"end\":70180,\"start\":70170},{\"end\":70190,\"start\":70180},{\"end\":70386,\"start\":70369},{\"end\":70401,\"start\":70386},{\"end\":70413,\"start\":70401},{\"end\":70423,\"start\":70413},{\"end\":70435,\"start\":70423},{\"end\":70449,\"start\":70435},{\"end\":70473,\"start\":70449},{\"end\":70837,\"start\":70821},{\"end\":70850,\"start\":70837},{\"end\":70863,\"start\":70850},{\"end\":70880,\"start\":70863},{\"end\":70896,\"start\":70880},{\"end\":70914,\"start\":70896},{\"end\":70930,\"start\":70914},{\"end\":70944,\"start\":70930},{\"end\":70958,\"start\":70944},{\"end\":70971,\"start\":70958},{\"end\":71345,\"start\":71335},{\"end\":71356,\"start\":71345},{\"end\":71364,\"start\":71356},{\"end\":71373,\"start\":71364},{\"end\":71383,\"start\":71373},{\"end\":71397,\"start\":71383},{\"end\":71408,\"start\":71397},{\"end\":71420,\"start\":71408},{\"end\":71800,\"start\":71775},{\"end\":71984,\"start\":71959},{\"end\":71995,\"start\":71984},{\"end\":72011,\"start\":71995},{\"end\":72023,\"start\":72011},{\"end\":72217,\"start\":72205},{\"end\":72231,\"start\":72217},{\"end\":72250,\"start\":72231},{\"end\":72268,\"start\":72250},{\"end\":72283,\"start\":72268},{\"end\":72298,\"start\":72283},{\"end\":72311,\"start\":72298},{\"end\":72321,\"start\":72311},{\"end\":72341,\"start\":72321},{\"end\":72359,\"start\":72341},{\"end\":72617,\"start\":72601},{\"end\":72638,\"start\":72617},{\"end\":72650,\"start\":72638},{\"end\":72672,\"start\":72650},{\"end\":72688,\"start\":72672},{\"end\":72705,\"start\":72688},{\"end\":72721,\"start\":72705},{\"end\":72735,\"start\":72721},{\"end\":72750,\"start\":72735},{\"end\":73141,\"start\":73128},{\"end\":73155,\"start\":73141},{\"end\":73170,\"start\":73155},{\"end\":73188,\"start\":73170},{\"end\":73207,\"start\":73188},{\"end\":73220,\"start\":73207},{\"end\":73236,\"start\":73220},{\"end\":73672,\"start\":73657},{\"end\":73688,\"start\":73672},{\"end\":73709,\"start\":73688},{\"end\":74036,\"start\":74015},{\"end\":74068,\"start\":74036},{\"end\":74082,\"start\":74068},{\"end\":74424,\"start\":74412},{\"end\":74439,\"start\":74424},{\"end\":74453,\"start\":74439},{\"end\":74463,\"start\":74453},{\"end\":74676,\"start\":74661},{\"end\":74690,\"start\":74676},{\"end\":74701,\"start\":74690},{\"end\":74719,\"start\":74701},{\"end\":74728,\"start\":74719},{\"end\":74742,\"start\":74728},{\"end\":74755,\"start\":74742},{\"end\":74766,\"start\":74755},{\"end\":74979,\"start\":74969},{\"end\":74996,\"start\":74979},{\"end\":75004,\"start\":74996},{\"end\":75017,\"start\":75004},{\"end\":75038,\"start\":75017},{\"end\":75051,\"start\":75038},{\"end\":75065,\"start\":75051},{\"end\":75084,\"start\":75065},{\"end\":75090,\"start\":75084},{\"end\":75502,\"start\":75494},{\"end\":75511,\"start\":75502},{\"end\":75521,\"start\":75511},{\"end\":75711,\"start\":75694},{\"end\":75727,\"start\":75711},{\"end\":75743,\"start\":75727},{\"end\":75987,\"start\":75973},{\"end\":75995,\"start\":75987},{\"end\":76005,\"start\":75995},{\"end\":76016,\"start\":76005},{\"end\":76033,\"start\":76016},{\"end\":76321,\"start\":76308},{\"end\":76336,\"start\":76321},{\"end\":76556,\"start\":76543},{\"end\":76570,\"start\":76556},{\"end\":76585,\"start\":76570},{\"end\":76601,\"start\":76585},{\"end\":76618,\"start\":76601},{\"end\":76629,\"start\":76618},{\"end\":76893,\"start\":76876},{\"end\":76911,\"start\":76893},{\"end\":77186,\"start\":77165},{\"end\":77199,\"start\":77186},{\"end\":77208,\"start\":77199},{\"end\":77213,\"start\":77208},{\"end\":77458,\"start\":77446},{\"end\":77471,\"start\":77458},{\"end\":77481,\"start\":77471},{\"end\":77492,\"start\":77481},{\"end\":77502,\"start\":77492},{\"end\":77801,\"start\":77790},{\"end\":77811,\"start\":77801},{\"end\":77819,\"start\":77811},{\"end\":77832,\"start\":77819},{\"end\":77843,\"start\":77832},{\"end\":78068,\"start\":78060},{\"end\":78080,\"start\":78068},{\"end\":78090,\"start\":78080},{\"end\":78302,\"start\":78289},{\"end\":78312,\"start\":78302},{\"end\":78320,\"start\":78312},{\"end\":78332,\"start\":78320},{\"end\":78344,\"start\":78332},{\"end\":78594,\"start\":78581},{\"end\":78603,\"start\":78594},{\"end\":78616,\"start\":78603},{\"end\":78632,\"start\":78616},{\"end\":78643,\"start\":78632},{\"end\":78867,\"start\":78856},{\"end\":78877,\"start\":78867},{\"end\":78892,\"start\":78877},{\"end\":78905,\"start\":78892},{\"end\":79106,\"start\":79092},{\"end\":79119,\"start\":79106},{\"end\":79135,\"start\":79119},{\"end\":79446,\"start\":79436},{\"end\":79459,\"start\":79446},{\"end\":79468,\"start\":79459},{\"end\":79475,\"start\":79468},{\"end\":79839,\"start\":79822},{\"end\":79858,\"start\":79839},{\"end\":79872,\"start\":79858},{\"end\":79889,\"start\":79872},{\"end\":80138,\"start\":80124},{\"end\":80151,\"start\":80138},{\"end\":80164,\"start\":80151},{\"end\":80330,\"start\":80317},{\"end\":80344,\"start\":80330},{\"end\":80503,\"start\":80490},{\"end\":80516,\"start\":80503},{\"end\":80535,\"start\":80516},{\"end\":80547,\"start\":80535},{\"end\":80566,\"start\":80547},{\"end\":80577,\"start\":80566},{\"end\":80589,\"start\":80577},{\"end\":80599,\"start\":80589},{\"end\":80612,\"start\":80599},{\"end\":80623,\"start\":80612},{\"end\":81066,\"start\":81053},{\"end\":81078,\"start\":81066},{\"end\":81088,\"start\":81078},{\"end\":81101,\"start\":81088},{\"end\":81112,\"start\":81101},{\"end\":81370,\"start\":81352},{\"end\":81387,\"start\":81370},{\"end\":81400,\"start\":81387},{\"end\":81633,\"start\":81616},{\"end\":81651,\"start\":81633},{\"end\":81668,\"start\":81651},{\"end\":81685,\"start\":81668},{\"end\":81699,\"start\":81685},{\"end\":81717,\"start\":81699},{\"end\":81954,\"start\":81940},{\"end\":81969,\"start\":81954},{\"end\":81983,\"start\":81969},{\"end\":82001,\"start\":81983},{\"end\":82019,\"start\":82001},{\"end\":82353,\"start\":82336},{\"end\":82371,\"start\":82353},{\"end\":82387,\"start\":82371},{\"end\":82410,\"start\":82387},{\"end\":82748,\"start\":82736},{\"end\":82760,\"start\":82748},{\"end\":82770,\"start\":82760},{\"end\":82783,\"start\":82770},{\"end\":83055,\"start\":83034},{\"end\":83077,\"start\":83055},{\"end\":83094,\"start\":83077},{\"end\":83107,\"start\":83094},{\"end\":83118,\"start\":83107},{\"end\":83370,\"start\":83356},{\"end\":83379,\"start\":83370},{\"end\":83393,\"start\":83379},{\"end\":83410,\"start\":83393},{\"end\":83424,\"start\":83410},{\"end\":83439,\"start\":83424},{\"end\":83450,\"start\":83439},{\"end\":83654,\"start\":83640},{\"end\":83662,\"start\":83654},{\"end\":83666,\"start\":83662},{\"end\":83680,\"start\":83666},{\"end\":84005,\"start\":83990},{\"end\":84013,\"start\":84005},{\"end\":84029,\"start\":84013},{\"end\":84042,\"start\":84029},{\"end\":84056,\"start\":84042},{\"end\":84067,\"start\":84056},{\"end\":84077,\"start\":84067},{\"end\":84088,\"start\":84077},{\"end\":84101,\"start\":84088},{\"end\":84116,\"start\":84101},{\"end\":84415,\"start\":84405},{\"end\":84426,\"start\":84415},{\"end\":84437,\"start\":84426},{\"end\":84643,\"start\":84634},{\"end\":84656,\"start\":84643},{\"end\":84669,\"start\":84656},{\"end\":84871,\"start\":84859},{\"end\":84884,\"start\":84871},{\"end\":84900,\"start\":84884},{\"end\":84908,\"start\":84900},{\"end\":84919,\"start\":84908},{\"end\":84931,\"start\":84919},{\"end\":84940,\"start\":84931},{\"end\":84949,\"start\":84940},{\"end\":84964,\"start\":84949},{\"end\":84974,\"start\":84964},{\"end\":85303,\"start\":85286},{\"end\":85314,\"start\":85303},{\"end\":85330,\"start\":85314},{\"end\":85524,\"start\":85513},{\"end\":85537,\"start\":85524},{\"end\":85545,\"start\":85537},{\"end\":85562,\"start\":85545},{\"end\":85569,\"start\":85562}]", "bib_venue": "[{\"end\":66437,\"start\":66429},{\"end\":66590,\"start\":66583},{\"end\":66700,\"start\":66636},{\"end\":66933,\"start\":66876},{\"end\":67307,\"start\":67281},{\"end\":67634,\"start\":67630},{\"end\":67916,\"start\":67912},{\"end\":68245,\"start\":68225},{\"end\":68544,\"start\":68540},{\"end\":68791,\"start\":68787},{\"end\":68937,\"start\":68902},{\"end\":69272,\"start\":69268},{\"end\":69551,\"start\":69547},{\"end\":69872,\"start\":69868},{\"end\":70194,\"start\":70190},{\"end\":70819,\"start\":70747},{\"end\":71333,\"start\":71230},{\"end\":71804,\"start\":71800},{\"end\":72027,\"start\":72023},{\"end\":72362,\"start\":72359},{\"end\":72837,\"start\":72766},{\"end\":73340,\"start\":73252},{\"end\":73714,\"start\":73709},{\"end\":74088,\"start\":74082},{\"end\":74467,\"start\":74463},{\"end\":74770,\"start\":74766},{\"end\":75188,\"start\":75106},{\"end\":75525,\"start\":75521},{\"end\":75748,\"start\":75743},{\"end\":76046,\"start\":76033},{\"end\":76340,\"start\":76336},{\"end\":76633,\"start\":76629},{\"end\":76874,\"start\":76805},{\"end\":77223,\"start\":77213},{\"end\":77444,\"start\":77376},{\"end\":77847,\"start\":77843},{\"end\":78094,\"start\":78090},{\"end\":78348,\"start\":78344},{\"end\":78647,\"start\":78643},{\"end\":78909,\"start\":78905},{\"end\":79139,\"start\":79135},{\"end\":79483,\"start\":79475},{\"end\":79893,\"start\":79889},{\"end\":80168,\"start\":80164},{\"end\":80315,\"start\":80290},{\"end\":80696,\"start\":80639},{\"end\":81116,\"start\":81112},{\"end\":81406,\"start\":81400},{\"end\":81721,\"start\":81717},{\"end\":82073,\"start\":82019},{\"end\":82430,\"start\":82410},{\"end\":82734,\"start\":82661},{\"end\":83125,\"start\":83118},{\"end\":83454,\"start\":83450},{\"end\":83754,\"start\":83696},{\"end\":84121,\"start\":84116},{\"end\":84443,\"start\":84437},{\"end\":84632,\"start\":84557},{\"end\":85014,\"start\":84990},{\"end\":85334,\"start\":85330},{\"end\":85574,\"start\":85569}]"}}}, "year": 2023, "month": 12, "day": 17}