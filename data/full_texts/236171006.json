{"id": 236171006, "updated": "2023-10-06 01:11:20.336", "metadata": {"title": "Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data", "authors": "[{\"first\":\"Xintao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Liangbin\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Chao\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Shan\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "journal": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "publication_date": {"year": 2021, "month": 7, "day": 22}, "abstract": "Though many attempts have been made in blind super-resolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex real-world degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2107.10833", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccvw/WangXDS21", "doi": "10.1109/iccvw54120.2021.00217"}}, "content": {"source": {"pdf_hash": "cb75d3f6c08fea6338afd147cab92531e514c5cd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2107.10833v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2107.10833", "status": "GREEN"}}, "grobid": {"id": "3b5cd54399eb8cd2cf1f3245619b8049110b490d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/cb75d3f6c08fea6338afd147cab92531e514c5cd.txt", "contents": "\nReal-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\n\n\nXintao Wang xintaowang@tencent.com \nApplied Research Center (ARC)\nTencent PCG\n\nLiangbin Xie lb.xie@siat.ac.cn \nShenzhen Institutes of Advanced Technology\nChinese Academy of Sciences\n\n\nUniversity of Chinese Academy of Sciences\n\n\nChao Dong chao.dong@siat.ac.cn \nShenzhen Institutes of Advanced Technology\nChinese Academy of Sciences\n\n\nShanghai AI Laboratory\n\n\nYing Shan yingsshan@tencent.com \nApplied Research Center (ARC)\nTencent PCG\n\nReal-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data\n/xinntao/Real-ESRGAN Bicubic ESRGAN RealSR Real-ESRGAN Figure 1: Comparisons of bicubic-upsampled, ESRGAN [50], RealSR [19], and our Real-ESRGAN results on real-life images. The Real-ESRGAN model trained with pure synthetic data is capable of enhancing details while removing annoying artifacts for common real-world images. (Zoom in for best view) *Liangbin Xie is an intern in Applied Research Center, Tencent PCG\nThough many attempts have been made in blind superresolution to restore low-resolution images with unknown and complex degradations, they are still far from addressing general real-world degraded images. In this work, we extend the powerful ESRGAN to a practical restoration application (namely, Real-ESRGAN), which is trained with pure synthetic data. Specifically, a high-order degradation modeling process is introduced to better simulate complex realworld degradations. We also consider the common ringing and overshoot artifacts in the synthesis process. In addition, we employ a U-Net discriminator with spectral normalization to increase discriminator capability and stabilize the training dynamics. Extensive comparisons have shown its superior visual performance than prior works on various real datasets. We also provide efficient implementations to synthesize training pairs on the fly.*Liangbin Xie is an intern in Applied Research Center, Tencent PCGReal-ESRNet w/o second-order Real-ESRNet Real-ESRNet w/o second-order Real-ESRNet Real-ESRNet w/o sinc filter Real-ESRNet Real-ESRNet w/o sinc filter Real-ESRNet\n\nIntroduction\n\nSingle image super-resolution (SR) [13,10,27] is an active research topic, which aims at reconstructing a highresolution (HR) image from its low-resolution (LR) counterpart. Since the pioneering work of SRCNN [9], deep convolution neural network (CNN) approaches have brought prosperous developments in the SR field. However, most approaches [21,27,20,25,50] assume an ideal bicubic downsampling kernel, which is different from real degradations. This degradation mismatch makes those approaches unpractical in real-world scenarios.\n\nBlind super-resolution [35,2,56], on the contrary, aims to restore low-resolution images suffering from unknown and complex degradations. Existing approaches can be roughly categorized into explicit modeling and implicit modeling, according to the underlying degradation process. Classical degradation model [11,29], which consists of blur, downsampling, noise and JPEG compression (more details in Sec. 3.1), is widely adopted in explicit model-ing methods [56,16,34]. However, the real-world degradations are usually too complex to be modeled with a simple combination of multiple degradations. Thus, these methods will easily fail in real-world samples. Implicit modeling methods [54,12,46] utilize data distribution learning with Generative Adversarial Network (GAN) [14] to obtain the degradation model. Yet, they are limited to the degradations within training datasets, and could not generalize well to out-of-distribution images. Readers are encouraged to refer to a recent blind SR survey [28] for a more comprehensive taxonomy.\n\nIn this work, we aim to extend the powerful ESR-GAN [50] to restore general real-world LR images by synthesizing training pairs with a more practical degradation process. The real complex degradations usually come from complicate combinations of different degradation processes, such as imaging system of cameras, image editing, and Internet transmission. For example, when we take a photo with our cellphones, the photos may have several degradations, such as camera blur, sensor noise, sharpening artifacts, and JPEG compression. We then do some editing and upload to a social media app, which introduces further compression and unpredictable noises. The above process becomes more complicated when the image is shared several times on the Internet. This motivates us to extend the classical \"first-order\" degradation model to \"high-order\" degradation modeling for real-world degradations, i.e., the degradations are modeled with several repeated degradation processes, each process being the classical degradation model. Empirically, we adopt a second-order degradation process for a good balance between simplicity and effectiveness. A recent work [55] also proposes a random shuffling strategy to synthesize more practical degradations. However, it still involves a fixed number of degradation processes, and whether all the shuffled degradations are useful or not is unclear. Instead, high-order degradation modeling is more flexible and attempts to mimic the real degradation generation process. We further incorporate sinc filters in the synthesis process to simulate the common ringing and overshoot artifacts.\n\nAs the degradation space is much larger than ESRGAN, the training also becomes challenging. Specifically, 1) the discriminator requires a more powerful capability to discriminate realness from complex training outputs, while the gradient feedback from the discriminator needs to be more accurate for local detail enhancement. Therefore, we improve the VGG-style discriminator in ESRGAN to an U-Net design [41,52,39]. 2) The U-Net structure and complicate degradations also increase the training instability. Thus, we employ the spectral normalization (SN) regularization [37,41] to stabilize the training dynamics. Equipped with the dedicated improvements, we are able to easily train our Real-ESRGAN and achieve a good balance of local detail enhancement and artifact suppression.\n\nTo summarize, in this work, 1) we propose a high-order degradation process to model practical degradations, and utilize sinc filters to model common ringing and overshoot artifacts. 2) We employ several essential modifications (e.g., U-Net discriminator with spectral normalization) to increase discriminator capability and stabilize the training dynamics. 3) Real-ESRGAN trained with pure synthetic data is able to restore most real-world images and achieve better visual performance than previous works, making it more practical in real-world applications.\n\n\nRelated Work\n\nThe image super-resolution field [21,24,45,17,25,27,58,22,44,57,7,30] has witnessed a variety of developments since SRCNN [9,10]. To achieve visually-pleasing results, generative adversarial network [15] is usually employed as loss supervisions to push the solutions closer to the natural manifold [26,40,50,49]. Most methods assume a bicubic downsampling kernel and usually fail in real images. Recent works also incorporate reinforcement learning or GAN prior to image restoration [53,6,47].\n\nThere have been several excellent explorations in blind SR. The first category involves explicit degradation representations and typically consists of two components: degradation prediction and conditional restoration. The above two components are performed either separately [2,56] or jointly (iteratively) [16,34,46]. These approaches rely on predefined degradation representations (e.g., degradation types and levels), and usually consider simple synthetic degradations. Moreover, inaccurate degradation estimations will inevitably result in artifacts.\n\nAnother category is to obtain/generate training pairs as close to real data as possible, and then train a unified network to address blind SR. The training pairs are usually 1) captured with specific cameras followed by tedious alignments [5,51]; 2) or directly learned from unpaired data with cycle consistency loss [54,33]; 3) or synthesized with estimated blur kernels and extracted noise patches [60,19]. However, 1) the captured data is only constrained to degradations associated with specific cameras, and thus could not well generalize to other real images; 2) learning finegrained degradations with unpaired data is challenging, and the results are usually unsatisfactory. Degradation models. Classical degradation model [11,29] is widely adopted in blind SR methods [56,16,34]. Yet, real-world degradations are usually too complex to be explicitly modeled. Thus, implicit modeling attempts to learn a degradation generation process within networks [54,12,46]. In this work, we propose a flexible high-order degradation model to synthesize more practical degradations.\n\n\nMethodology\n\n\nClassical Degradation Model\n\nBlind SR aims to restore high-resolution images from low-resolution ones with unknown and complex degradations. The classical degradation model [11,29] is usually adopted to synthesize the low-resolution input. Generally, the ground-truth image y is first convolved with blur kernel k. Then, a downsampling operation with scale factor r is performed. The low-resolution x is obtained by adding noise n. Finally, JPEG compression is also adopted, as it is widely-used in real-world images.\nx = D(y) = [(y k) \u2193 r +n] JPEG ,(1)\nwhere D denotes the degradation process. In the following, we briefly revisit these commonly-used degradations. The detailed settings are specified in Sec Blur. We typically model blur degradation as a convolution with a linear blur filter (kernel). Isotropic and anisotropic Gaussian filters are common choices. For a Gaussian blur kernel k with a kernel size of 2t + 1, its (i, j) \u2208 [\u2212t, t] element is sampled from a Gaussian distribution, formally:\nk(i, j) = 1 N exp(\u2212 1 2 C T \u03a3 \u22121 C), C = [i, j] T ,(2)\nwhere \u03a3 is the covariance matrix; C is the spatial coordinates; N is the normalization constant. The covariance matrix could be further represented as follows:\n\u03a3 = R \u03c3 2 1 0 0 \u03c3 2 2 R T , (R is the rotation matrix) (3) = cos\u03b8 \u2212sin\u03b8 sin\u03b8 cos\u03b8 \u03c3 2 1 0 0 \u03c3 2 2 cos\u03b8 sin\u03b8 \u2212sin\u03b8 cos\u03b8 ,(4)\nwhere \u03c3 1 and \u03c3 2 are the standard deviation along the two principal axes (i.e., eigenvalues of the covariance matrix); \u03b8 is the rotation degree. When \u03c3 1 = \u03c3 2 , k is an isotropic Gaussian blur kernel; otherwise k is an anisotropic kernel. Discussion. Though Gaussian blur kernels are widely used to model blur degradation, they may not well approximate real camera blur. To include more diverse kernel shapes, we further adopt generalized Gaussian blur kernels [31] and a plateau-shaped distribution. Their probability density function (pdf) are 1\nN exp(\u2212 1 2 (C T \u03a3 \u22121 C) \u03b2 , and 1 N 1 1+(C T \u03a3 \u22121 C) \u03b2 ,\nrespectively. \u03b2 is the shape parameter. Empirically, we find that including these blur kernels could produce sharper outputs for several real samples.\n\nNoise. We consider two commonly-used noise types -1) additive Gaussian noise and 2) Poisson noise. Addictive Gaussian noise has a probability density function equal to that of the Gaussian distribution. The noise intensity is controlled by the standard deviation (i.e., sigma value) of the Gaussian distribution. When each channel of RGB images has independent sampled noise, the synthetic noise is color noise. We also synthesize gray noise by employing the same sampled noise to all three channels [55,38].\n\nPoisson noise follows the Poisson distribution. It is usually used to approximately model the sensor noise caused by statistical quantum fluctuations, that is, variation in the number of photons sensed at a given exposure level. Poisson noise has an intensity proportional to the image intensity, and the noises at different pixels are independent.\n\n\nResize (Downsampling).\n\nDownsampling is a basic operation for synthesizing low-resolution images in SR. More generally, we consider both downsamping and upsampling, i.e., the resize operation. There are several resize algorithms -nearest-neighbor interpolation, area resize, bilinear interpolation, and bicubic interpolation. Different resize operations bring in different effects -some produce blurry results while some may output over-sharp images with overshoot artifacts.\n\nIn order to include more diverse and complex resize effects, we consider a random resize operation from the above choices. As nearest-neighbor interpolation introduces the misalignment issue, we exclude it and only consider the area, bilinear and bicubic operations.\n\n\nJPEG compression.\n\nJPEG compression is a commonly used technique of lossy compression for digital images. It first converts images into the YCbCr color space and downsamples the chroma channels. Images are then split into 8 \u00d7 8 blocks and each block is transformed with a two-dimensional discrete cosine transform (DCT), followed by a quantization of DCT coefficients. More details of JPEG compression algorithms can be found in [43]. Unpleasing block artifacts are usually introduced by the JPEG compression.\n\nThe quality of compressed images is determined by a quality factor q \u2208 [0, 100], where a lower q indicates a higher compression ratio and worse quality. We use the Py-Torch implementation -DiffJPEG [32].\n\n\nHigh-order Degradation Model\n\nWhen we adopt the above classical degradation model to synthesize training pairs, the trained model could indeed handle some real samples. However, it still can not resolve some complicated degradations in the real world, especially the unknown noises and complex artifacts (see Fig. 3). It is because that the synthetic low-resolution images still have a large gap with realistic degraded images. We thus extend the classical degradation model to a high-order degradation process to model more practical degradations.\n\nThe classical degradation model only includes a fixed  Figure 2: Overview of the pure synthetic data generation adopted in Real-ESRGAN. It utilizes a second-order degradation process to model more practical degradations, where each degradation process adopts the classical degradation model. The detailed choices for blur , resize, noise and JPEG compression are listed. We also employ sinc filter to synthesize common ringing and overshoot artifacts.\n\n\nInput\n\nOutput Input Output Yet, they amplify noises or introduce ringing artifacts for complex real-world images (Right). Zoom in for best view number of basic degradations, which can be regarded as a first-order modeling. However, the real-life degradation processes are quite diverse, and usually comprise a series of procedures including imaging system of cameras, image editing, Internet transmission, etc. For instance, when we want to restore a low-quality image download from the Internet, its underlying degradation involves a complicated combination of different degradation processes. Specifically, the original image might be taken with a cellphone many years ago, which inevitably contains degradations such as camera blur, sensor noise, low resolution and JPEG compression. The image was then edited with sharpening and resize operations, bringing in overshoot and blur artifacts. After that, it was uploaded to some social media applications, which introduces a further compression and unpredictable noises. As the digital transmission will also bring artifacts, this process becomes more complicated when the image spreads several times on the Internet. Such a complicated deterioration process could not be modeled with the classical first-order model. Thus, we propose a high-order degradation model. An n-order model involves n repeated degradation processes (as shown in Eq. 5), where each degradation process adopts the classical degradation model (Eq. 1) with the same procedure but different hyper-parameters. Note that the \"high-order\" here is different from that used in mathematical functions. It mainly refers to the implementation time of the same operation. The random shuffling strategy in [55] may also include repeated degradation processes (e.g., double blur or JPEG). But we highlight that the high-order degradation process is the key, indicating that not all the shuffled degradations are necessary. In order to keep the image resolution in a reasonable range, the downsampling operation in Eq. 1 is replaced with a random resize operation. Empirically, we adopt a second-order degradation process, as it could resolve most real cases while keeping simplicity. Fig. 2 depicts the overall pipeline of our pure synthetic data generation pipeline.\n\nx = D n (y) = (D n \u2022 \u00b7 \u00b7 \u00b7 \u2022 D 2 \u2022 D 1 )(y).\n\nIt is worth noting that the improved high-order degradation process is not perfect and could not cover the whole degradation space in the real world. Instead, it merely extends the solvable degradation boundary of previous blind SR methods through modifying the data synthesis process. Several typical limitation scenarios can be found in Fig. 11.\n\n\nRinging and overshoot artifacts\n\nRinging artifacts often appear as spurious edges near sharp transitions in an image. They visually look like bands or \"ghosts\" near edges. Overshoot artifacts are usually combined with ringing artifacts, which manifest themselves as an increased jump at the edge transition. The main cause of these artifacts is that the signal is bandlimited without high frequencies. These artifacts are very common and usually  Figure 4: Real-ESRGAN adopts the same generator network as that in ESRGAN. For the scale factor of \u00d72 and \u00d71, it first employs a pixel-unshuffle operation to reduce spatial size and re-arrange information to the channel dimension. We employ the sinc filter, an idealized filter that cuts off high frequencies, to synthesize ringing and overshoot artifacts for training pairs. The sinc filter kernel can be expressed as 1 :\nk(i, j) = \u03c9 c 2\u03c0 i 2 + j 2 J 1 (\u03c9 c i 2 + j 2 ),(6)\nwhere (i, j) is the kernel coordinate; \u03c9 c is the cutoff frequency; and J 1 is the first order Bessel function of the first kind. Fig. 5 (Bottom) shows sinc filters with different cutoff frequencies, and their corresponding filtered images. It is observed that it could well synthesize ringing and overshoot artifacts (especially introduced by over-sharp effects). These artifacts are visually similar to those in the first two real samples in Fig. 5 (Top). We adopt sinc filters in two places: the blurring process and the last step of the synthesis. The order of the last sinc filter and JPEG compression is randomly exchanged to cover a larger degradation space, as some images may be first over-sharpened (with overshoot artifacts) and then have 1 We use the implementation in this url. spectral norm conv Figure 6: Architecture of the U-Net discriminator with spectral normalization. JPEG compression; while some images may first do JPEG compression followed by sharpening operation.\n\n\nNetworks and Training\n\nESRGAN generator. We adopt the same generator (SR network) as ESRGAN [50], i.e., a deep network with several residual-in-residual dense blocks (RRDB), as shown in Fig. 4. We also extend the original \u00d74 ESRGAN architecture to perform super-resolution with a scale factor of \u00d72 and \u00d71. As ESRGAN is a heavy network, we first employ the pixel-unshuffle (an inverse operation of pixelshuffle [42]) to reduce the spatial size and enlarge the channel size before feeding inputs into the main ESRGAN architecture. Thus, the most calculation is performed in a smaller resolution space, which can reduce the GPU memory and computational resources consumption. U-Net discriminator with spectral normalization (SN). As Real-ESRGAN aims to address a much larger degradation space than ESRGAN, the original design of discriminator in ESRGAN is no longer suitable. Specifically, the discriminator in Real-ESRGAN requires a greater discriminative power for complex training outputs. Instead of discriminating global styles, it also needs to produce accurate gradient feedback for local textures. Inspired by [41,52], we also improve the VGG-style discriminator in ESRGAN to an U-Net design with skip connections (Fig. 6). The U-Net outputs realness values for each pixel, and can provide detailed per-pixel feedback to the generator.\n\nIn the meanwhile, the U-Net structure and complicate degradations also increase the training instability. We employ the spectral normalization regularization [37] to stabilize the training dynamics. Moreover, we observe that spectral normalization is also beneficial to alleviate the over-sharp and annoying artifacts introduced by GAN training. With those adjustments, we are able to easily train the Real-ESRGAN and achieve a good balance of local detail enhancement and artifact suppression. The training process is divided into two stages. First, we train a PSNR-oriented model with the L1 loss. The obtained model is named by Real-ESRNet. We then use the trained PSNR-oriented model as an initialization of the generator, and train the Real-ESRGAN with a combination of L1 loss, perceptual loss [20] and GAN loss [14,26,4].\n\n\nExperiments\n\n\nDatasets and Implementation\n\nTraining details.\n\nSimilar to ESRGAN, we adopt DIV2K [1], Flickr2K [45] and OutdoorSceneTraining [49] datasets for training. The training HR patch size is set to 256. We train our models with four NVIDIA V100 GPUs with a total batch size of 48. We employ Adam optimizer [23]. Real-ESRNet is finetuned from ESR-GAN for faster convergence. We train Real-ESRNet for 1000K iterations with learning rate 2 \u00d7 10 \u22124 while training Real-ESRGAN for 400K iterations with learning rate 1 \u00d7 10 \u22124 . We adopt exponential moving average (EMA) for more stable training and better performance. Real-ESRGAN is trained with a combination of L1 loss, perceptual loss and GAN loss, with weights {1, 1, 0.1}, respectively. We use the {conv1, ...conv5} feature maps (with weights {0.1, 0.1, 1, 1, 1}) before activation in the pre-trained VGG19 network [20] as the perceptual loss. Our implementation is based on the BasicSR [48]. Degradation details. We employ a second-order degradation model for a good balance of simplicity and effectiveness. Unless otherwise specified, the two degradation processes have the same settings. We adopt Gaussian kernels, generalized Gaussian kernels and plateau-shaped kernels, with a probability of {0.7, 0.15, 0.15}. The blur kernel size is randomly selected from {7, 9, ...21}. Blur standard deviation \u03c3 is sampled from [0.2, 3] ([0.2, 1.5] for the second degradation process). Shape parameter \u03b2 is sampled from [0. 5,4] and [1,2] for generalized Gaussian and plateau-shaped kernels, respectively. We also use sinc kernel with a probability of 0.1. We skip the second blur degradation with a probability of 0.2.\n\nWe employ Gaussian noises and Poisson noises with a probability of {0.5, 0.5}. The noise sigma range and Poisson noise scale are set to [1,30] and [0.05, 3], respectively ( [1,25] and [0.05, 2.5] for the second degradation process). The gray noise probability is set to 0.4. JPEG compression quality factor is set to [30,95]. The final sinc filter is applied with a probability of 0.8. More details can be found in the released codes. Training pair pool. In order to improve the training ef-ficiency, all degradation processes are implemented in Py-Torch with CUDA acceleration, so that we are able to synthesize training pairs on the fly. However, batch processing limits the diversity of synthetic degradations in a batch. For example, samples in a batch could not have different resize scaling factors. Therefore, we employ a training pair pool to increase the degradation diversity in a batch. At each iteration, the training samples are randomly selected from the training pair poor to form a training batch. We set the pool size to 180 in our implementation. Sharpen ground-truth images during training. We further show a training trick to visually improve the sharpness, while not introducing visible artifacts. A typical way of sharpening images is to employ a post-process algorithm, such as unsharp masking (USM). However, this algorithm tends to introduce overshoot artifacts. We empirically find that sharpening ground-truth images during training could achieve a better balance of sharpness and overshoot artifact suppression. We denote the model trained with sharped ground-truth images as Real-ESRGAN+ (comparisons are shown in Fig. 7).\n\n\nComparisons with Prior Works\n\nWe compare our Real-ESRGAN with several stateof-the-art methods, including ESRGAN [50], DAN [34], CDC [51], RealSR [19] and BSRGAN [55]. We test on several diverse testing datasets with real-world images, including RealSR [5], DRealSR [51], OST300 [49], DPED [18], ADE20K validation [59] and images from Internet. Since existing metrics for perceptual quality cannot well reflect the actual human perceptual preferences on the fine-grained scale [3], we present several representative visual samples in Fig. 7. The quantitative results are also included in the Appendix. B for reference.\n\nIt can be observed from Fig. 7 that our Real-ESRGAN outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness. Specifically, the first sample contains overshoot artifacts (white edges around letters). Directly upsampling will inevitably amplify those artifacts (e.g., DAN and BSR-GAN). Real-ESRGAN takes such common artifacts into consideration and simulates them with sinc filter, thus effectively removing ringing and overshoot artifacts. The second sample contains unknown and complicated degradations. Most algorithms can not effectively eliminate them while Real-ESRGAN trained with second-order degradation processes could. Real-ESRGAN is also capable of restoring more realistic textures (e.g., brick, mountain and tree textures) for real-world samples, while other methods either fail to remove degradations or add unnatural textures (e.g., RealSR and BSRGAN).   \n\n\nAblation Studies\n\nSecond-order degradation model. We conduct ablation studies of degradations on Real-ESRNet, as it is more controllable and can better reflect the influence of degradations.\n\nWe replace the second-order process in Real-ESRNet with the classical degradation model to generate training pairs. As shown in Fig. 8 (Top), models trained with classical firstorder degradation model cannot effectively remove noise on the wall or blur in the wheat field, while Real-ESRNet can handle these cases. sinc filters. If sinc filters are not employed during training, the restored results will amplify the ringing and overshoot artifacts that existed in the input images, as shown in Fig. 8 (Bottom), especially around the text and lines. In contrast, models trained with sinc filters can remove those artifacts. U-Net discriminator with SN regularization. We first employ the ESRGAN setting including the VGG-style discriminator and its loss weights. However, we can observe from Fig. 9, this model cannot restore detailed textures (bricks and bushes) and even brings unpleasant artifacts in bush branches. Using a U-Net design could improve local details. Yet, it introduces unnatural textures and also increases training instability. SN regularization could improve restored textures while stabilizing training dynamics. More complicated blur kernels. We remove the generalized Gaussian kernel and plateau-shaped kernel in blur synthesis. As shown in Fig. 10, on some real samples, the model cannot remove blur and recover sharp edges as Real-ESRGAN do. Nevertheless, on most samples, their differences are marginal, indicating that the widely-used Gaussian kernels with a high-order degradation process can already cover a large real blur space. As we can still observe slightly better performance, we adopt those more complicated blur kernels in Real-ESRGAN.\n\n\nLimitations\n\nThough Real-ESRGAN is able to restore most realworld images, it still has some limitations. As shown in Fig. 11, 1) some restored images (especially building and indoor scenes) have twisted lines due to aliasing issues. 2)\n\n\nInput\n\nESRGAN setting U-Net discriminator U-Net discriminator w/ SN Figure 9: Ablation on the discriminator design. Zoom in for best view\n\nGaussian blur kernels More blur kernels Gaussian blur kernels More blur kernels \n\n\nConclusion\n\nIn this paper, we train the practical Real-ESRGAN for real-world blind super-resolution with pure synthetic training pairs. In order to synthesize more practical degradations, we propose a high-order degradation process and employ sinc filters to model common ringing and overshoot artifacts. We also utilize a U-Net discriminator with spectral normalization regularization to increase discriminator capability and stabilize the training dynamics. Real-ESRGAN trained with synthetic data is able to enhance details while removing annoying artifacts for most real-world images.  To include more diverse kernel shapes, we further adopt generalized Gaussian blur kernels [31] and a plateau-shaped distribution. Fig. 13 shows how the shape parameter \u03b2 controls kernel shapes. Empirically, we found that including these blur kernels produces sharper outputs for several real samples.  There are several resize algorithms. We compare the following resize operations: nearest-neighbor interpolation, area resize, bilinear interpolation and bicubic interpolation. We examine the different effects of these resize operations. We first downsample an image by a scale factor of four and then upsample to its original size. Different downsampling and upsampling algorithms are performed, and the results of different combinations are shown in Fig. 15. It is observed that different resize operations result in very different effects -some produce blurry results while some may output over-sharp images with overshoot artifacts.  \n\u03a3 = 0.01 0 0 0.01 \u03a3 = 1 0 0 1 \u03a3 = 4 0 0 4 \u03a3 = 9 0 0 9 1 = 2 = 1 1 = 2 = 2 1 = 2 = 3 isotropic 1 = 0.2,\n\nA.4. JPEG compression\n\nWe use the PyTorch implementation -DiffJPEG. We observe that the compressed images by DiffJPEG are a bit different from those compressed by the cv2 package. Fig. 16 shows the typical JPEG compression artifacts and the difference caused by using different packages. Such a difference may bring an extra gap between synthetic and real samples. In this work, we only adopt DiffJPEG for simplicity, and this difference will be addressed later. \n\n\nB. Quantitative Comparisons\n\nWe provide the non-reference image quality assessment -NIQE [36] for reference. Note that existing metrics for perceptual quality cannot well reflect the actual human perceptual preferences on the fine-grained scale [3].\n\nWe compare our Real-ESRGAN with several stateof-the-art methods, including ESRGAN [50], DAN [34], CDC [51], RealSR [19] and BSRGAN [55]. We test on several diverse testing datasets with real-world images, including RealSR [5], DRealSR [51], OST300 [49], DPED [18], ImageNet validation [8] and ADE20K validation [59]. The results are shown in Tab. 1. Though our Real-ESRGAN+ does not optimize for NIQE scores, it sill produces lower NIQE scores on most testing datasets.\n\n\nC. More Qualitative Comparisons\n\nWe show more qualitative comparisons with previous works. As shown in Fig. 17, our Real-ESRGAN outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness. Other methods typically fail to remove complicated artifacts (the 1st sample) and overshoot artifacts (the 2nd, 3rd sample), or fail to restore realistic and natural textures for various scenes (the 4th, 5th samples).\n\n. 4 . 1 .\n41More descriptions and examples are in Appendix. A.\n\nFigure 3 :\n3Models trained with synthetic data of classical degradation model could resolve some real samples (Left).\n\nFigure 5 :\n5Top: Real samples suffering from ringing and overshoot artifacts. Bottom: Examples of sinc kernels (kernel size 21) and the corresponding filtered images. Zoom in for best view produced by a sharping algorithm, JPEG compression, etc.Fig. 5 (Top) shows some real samples suffering from ringing and overshoot artifacts.\n\nFigure 7 :\n7Qualitative comparisons on several representative real-world samples with upsampling scale factor of 4. Our Real-ESRGAN outperforms previous approaches in both removing artifacts and restoring texture details. Real-ESRGAN+ (trained with sharpened ground-truths) can further boost visual sharpness. Other methods may either fail to remove overshoot (the 1st sample) and complicated artifacts (the 2nd sample), or fail to restore realistic and natural textures for various scenes (the 3rd, 4th, 5th samples). (Zoom in for best view)\n\nFigure 8 :\n8Top: Real-ESRNet results w/ and w/o secondorder degradation process. Bottom: Real-ESRNet results w/ and w/o sinc filters. Zoom in for best view\n\nFigure 10 :Figure 11 :\n1011Ablation on using more blur kernels (generalized blur and plateau-shaped kernels). Zoom in for best view Input Real-ESRGAN Limitations: 1) twisted lines; 2) unpleasant artifacts caused by GAN training; 3) unknown and out-ofdistribution degradations. Zoom in for best view GAN training introduces unpleasant artifacts on some samples. 3) It could not remove out-of-distribution complicated degradations in the real world. Even worse, it may amplify these artifacts. These drawbacks have great impact on the practical application of Real-ESRGAN, which are in urgent need to address in future works.\n\nFigure 12 :\n12Examples of Gaussian kernels (kernel size21) and their corresponding blurry images. Zoom in for best view\n\nFigure 13 :\n13Blur kernels with different shape parameters in general Gaussian distribution and plateau-shaped distribution. Zoom in for best view A.2. Noise\n\nFig. 14 Figure 14 :\n1414depicts the additive Gaussian noise and Poisson noise. Poisson noise has an intensity proportional to the image intensity, and the noises at different pixels are independent of one another. As shown in Fig. 14, the Poisson noise has low noise intensity in dark areas. Visual comparisons of Gaussian and Poisson noises. Poisson noise has low noise intensity in dark areas. Zoom in for best view A.3. Resize\n\nFigure 15 :\n15Effects of different combinations of down-and up-sampling algorithms. The images are first downsampled by a scale factor of four and then upsampled to its original size. Zoom in for best view\n\nFigure 16 :\n16JPEG compressed images by cv2 and DiffJPEG, with quality factor q = 50. They produces slightly different results. Zoom in for best view\n\nTable 1 :\n1NIQE scores on several diverse testing datasets with real-world images. The lower, the better. Bicubic ESRGAN [50] DAN [34] RealSR [19] CDC [51] BSRGAN [55] Real-ESRGAN Real-ESRGAN+ RealSR-Canon [5] 6.12696.7715 \n6.5282 \n6.8692 \n6.1488 \n5.7489 \n4.5899 \n4.5314 \nRealSR-Nikon [5] 6.3607 \n6.7480 \n6.6063 \n6.7390 \n6.3265 \n5.9920 \n5.0753 \n5.0247 \nDRealSR [51] \n6.5766 \n8.6335 \n7.0720 \n7.7213 \n6.6359 \n6.1362 \n4.9796 \n4.8458 \nDPED-iphone [18] 6.0121 \n5.7363 \n6.1414 \n5.5855 \n6.2738 \n5.9906 \n5.4352 \n5.2631 \nOST300 [49] \n4.4440 \n3.5245 \n5.0232 \n4.5715 \n4.7441 \n4.1662 \n2.8659 \n2.8191 \nImageNet val [8] \n7.4985 \n3.6474 \n6.0932 \n3.8303 \n7.0441 \n4.3528 \n4.8580 \n4.6448 \nADE20K val [59] \n7.5239 \n3.6905 \n6.3839 \n3.4102 \n6.9219 \n3.9434 \n3.7886 \n3.5778 \n\n\n= 2 = 0.1\nAcknowledgement. This work is partially supported by National Natural Science Foundation of China (61906184), the Shanghai Committee of Science and Technology, China (Grant No. 21DZ1100800 and 21DZ1100100).AppendixA. Details of Classical Degradation ModelIn this section, we provide more details (especially examples) of each degradation type used in the classical degradation model.A.1. BlurIsotropic and anisotropic Gaussian filters are the common choices for blur kernels. We show several Gaussian kernels and their corresponding blurry images inFig. 12\nNtire 2017 challenge on single image super-resolution: Dataset and study. Eirikur Agustsson, Radu Timofte, CVPRW. Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In CVPRW, 2017. 6\n\nBlind super-resolution kernel estimation using an internal-gan. Sefi Bell-Kligler, Assaf Shocher, Michal Irani, NeurIPS. 1Sefi Bell-Kligler, Assaf Shocher, and Michal Irani. Blind super-resolution kernel estimation using an internal-gan. In NeurIPS, 2019. 1, 2\n\nTomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on perceptual image super-resolution. Yochai Blau, Roey Mechrez, Radu Timofte, In ECCVW. 612Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on percep- tual image super-resolution. In ECCVW, 2018. 6, 12\n\nThe perception-distortion tradeoff. Yochai Blau, Tomer Michaeli, CVPR. Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In CVPR, 2018. 6\n\nToward real-world single image super-resolution: A new benchmark and a new model. Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, Lei Zhang, ICCV. 612Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: A new benchmark and a new model. In ICCV, 2019. 2, 6, 12\n\nGlean: Generative latent bank for large-factor image super-resolution. C K Kelvin, Xintao Chan, Xiangyu Wang, Jinwei Xu, Chen Change Gu, Loy, CVPR. Kelvin C.K. Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. Glean: Generative latent bank for large-factor image super-resolution. In CVPR, 2021. 2\n\nSecond-order attention network for single image super-resolution. Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, Lei Zhang, CVPR. Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single image super-resolution. In CVPR, 2019. 2\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. 12Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 12\n\nLearning a deep convolutional network for image super-resolution. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, ECCV. 1Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV, 2014. 1, 2\n\nImage super-resolution using deep convolutional networks. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, IEEE TPAMI. 382Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution using deep convolutional net- works. IEEE TPAMI, 38(2):295-307, 2016. 1, 2\n\nRestoration of a single superresolution image from several blurred, noisy, and undersampled measured images. Michael Elad, Arie Feuer, IEEE transactions on image processing. 6123Michael Elad and Arie Feuer. Restoration of a single super- resolution image from several blurred, noisy, and undersam- pled measured images. IEEE transactions on image process- ing, 6(12):1646-1658, 1997. 1, 2, 3\n\nFrequency separation for real-world super-resolution. Manuel Fritsche, Shuhang Gu, Radu Timofte, ICCVW. Manuel Fritsche, Shuhang Gu, and Radu Timofte. Frequency separation for real-world super-resolution. In ICCVW, 2019. 2\n\nSuperresolution from a single image. Daniel Glasner, Shai Bagon, Michal Irani, ICCV. Daniel Glasner, Shai Bagon, and Michal Irani. Super- resolution from a single image. In ICCV, 2009. 1\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NeurIPS. 26Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 2, 6\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, NeurIPS. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. 2\n\nBlind super-resolution with iterative kernel correction. Jinjin Gu, Hannan Lu, Wangmeng Zuo, Chao Dong, CVPR. Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong. Blind super-resolution with iterative kernel correction. In CVPR, 2019. 2\n\nDeep backprojection networks for super-resolution. Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita, CVPR. Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita. Deep backprojection networks for super-resolution. In CVPR, 2018. 2\n\nDslr-quality photos on mobile devices with deep convolutional networks. Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, Luc Van Gool, ICCV. 612Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc Van Gool. Dslr-quality photos on mobile devices with deep convolutional networks. In ICCV, 2017. 6, 12\n\nReal-world super-resolution via kernel estimation and noise injection. Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, CVPRW. 612Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. Real-world super-resolution via kernel estimation and noise injection. In CVPRW, 2020. 1, 2, 6, 12\n\nPerceptual losses for real-time style transfer and super-resolution. Justin Johnson, Alexandre Alahi, Li Fei-Fei, ECCV. 16Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 1, 6\n\nAccurate image super-resolution using very deep convolutional networks. Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, CVPR. 1Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional net- works. In CVPR, 2016. 1, 2\n\nDeeplyrecursive convolutional network for image super-resolution. Jiwon Kim, Jung Kwon Lee, Kyoung Mu Lee, CVPR. Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply- recursive convolutional network for image super-resolution. In CVPR, 2016. 2\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, ICLR. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6\n\nDeep laplacian pyramid networks for fast and accurate super-resolution. Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang, CVPR. Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming- Hsuan Yang. Deep laplacian pyramid networks for fast and accurate super-resolution. In CVPR, 2017. 2\n\nPhotorealistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, CVPR. 1Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo- realistic single image super-resolution using a generative ad- versarial network. In CVPR, 2017. 1, 2\n\nPhotorealistic single image super-resolution using a generative adversarial network. Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, CVPR. 26Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo- realistic single image super-resolution using a generative ad- versarial network. In CVPR, 2017. 2, 6\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, CVPRW. 1Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, 2017. 1, 2\n\nBlind image super-resolution: A survey and beyond. Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiaoand, Chao Dong, arXiv:2107.03055Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiaoand, and Chao Dong. Blind image super-resolution: A survey and beyond. arXiv:2107.03055, 2021. 2\n\nOn bayesian adaptive video super resolution. Ce Liu, Deqing Sun, IEEE transactions on pattern analysis and machine intelligence. 363Ce Liu and Deqing Sun. On bayesian adaptive video super resolution. IEEE transactions on pattern analysis and ma- chine intelligence, 36(2):346-360, 2013. 1, 2, 3\n\nNon-local recurrent network for image restoration. Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S Huang, In NeurIPS. 2Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and Thomas S Huang. Non-local recurrent network for image restoration. In NeurIPS, 2018. 2\n\nEstimating generalized gaussian blur kernels for out-of-focus image deblurring. Yu-Qi Liu, Xin Du, Hui-Liang Shen, Shu-Jie Chen, IEEE Transactions on circuits and systems for video technology. 311Yu-Qi Liu, Xin Du, Hui-Liang Shen, and Shu-Jie Chen. Es- timating generalized gaussian blur kernels for out-of-focus image deblurring. IEEE Transactions on circuits and sys- tems for video technology, 2020. 3, 11\n\n. Michael R Lomnitz, Diffjpeg, 2021. 3Michael R Lomnitz. Diffjpeg. https://github.com/ mlomnitz/DiffJPEG, 2021. 3\n\nUnsupervised learning for real-world super-resolution. Andreas Lugmayr, Martin Danelljan, Radu Timofte, IC-CVW. Andreas Lugmayr, Martin Danelljan, and Radu Timofte. Un- supervised learning for real-world super-resolution. In IC- CVW, 2019. 2\n\nUnfolding the alternating optimization for blind super resolution. Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, Tieniu Tan, NeurIPS, 2020. 612Zhengxiong Luo, Yan Huang, Shang Li, Liang Wang, and Tieniu Tan. Unfolding the alternating optimization for blind super resolution. In NeurIPS, 2020. 2, 6, 12\n\nNonparametric blind super-resolution. Tomer Michaeli, Michal Irani, CVPR. Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In CVPR, 2013. 1\n\nMaking a completely blind image quality analyzer. Anish Mittal, Rajiv Soundararajan, Alan C Bovik, IEEE Signal Process. Lett. 20312Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Mak- ing a completely blind image quality analyzer. IEEE Signal Process. Lett., 20(3):209-212, 2013. 12\n\nSpectral normalization for generative adversarial networks. Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida, ICLR. 25Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative ad- versarial networks. In ICLR, 2018. 2, 5\n\nA holistic approach to cross-channel image noise modeling and its application to image denoising. Seonghyeon Nam, Youngbae Hwang, Yasuyuki Matsushita, Seon Joo Kim, CVPR. Seonghyeon Nam, Youngbae Hwang, Yasuyuki Matsushita, and Seon Joo Kim. A holistic approach to cross-channel im- age noise modeling and its application to image denoising. In CVPR, 2016. 3\n\nUnet: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In International Conference on Medical image com- puting and computer-assisted intervention. Springer, 2015. 2\n\nEnhancenet: Single image super-resolution through automated texture synthesis. S M Mehdi, Bernhard Sajjadi, Michael Sch\u00f6lkopf, Hirsch, ICCV. Mehdi SM Sajjadi, Bernhard Sch\u00f6lkopf, and Michael Hirsch. Enhancenet: Single image super-resolution through automated texture synthesis. In ICCV, 2017. 2\n\nA u-net based discriminator for generative adversarial networks. Edgar Schonfeld, Bernt Schiele, Anna Khoreva, CVPR, 2020. 25Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative adversarial networks. In CVPR, 2020. 2, 5\n\nReal-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, P Andrew, Rob Aitken, Daniel Bishop, Zehan Rueckert, Wang, CVPR. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, 2016. 5\n\nJpeg-resistant adversarial images. Richard Shin, Dawn Song, NeurIPS Workshop on Machine Learning and Computer Security. Richard Shin and Dawn Song. Jpeg-resistant adversarial im- ages. In NeurIPS Workshop on Machine Learning and Com- puter Security, 2017. 3\n\nImage superresolution via deep recursive residual network. Ying Tai, Jian Yang, Xiaoming Liu, CVPR. Ying Tai, Jian Yang, and Xiaoming Liu. Image super- resolution via deep recursive residual network. In CVPR, 2017. 2\n\nNtire 2017 challenge on single image super-resolution: Methods and results. Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, CVPRW. 26Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming- Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, et al. Ntire 2017 chal- lenge on single image super-resolution: Methods and results. In CVPRW, 2017. 2, 6\n\nUnsupervised degradation representation learning for blind superresolution. Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, Yulan Guo, CVPR. Longguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, and Yulan Guo. Unsuper- vised degradation representation learning for blind super- resolution. In CVPR, 2021. 2\n\nTowards real-world blind face restoration with generative facial prior. Xintao Wang, Yu Li, Honglun Zhang, Ying Shan, CVPR. Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. To- wards real-world blind face restoration with generative facial prior. In CVPR, 2021. 2\n\n. Xintao Wang, Ke Yu, C K Kelvin, Chao Chan, Chen Change Dong, Loy, Xintao Wang, Ke Yu, Kelvin C.K. Chan, Chao Dong, and Chen Change Loy. Basicsr. https://github.com/ xinntao/BasicSR, 2020. 6\n\nRecovering realistic texture in image super-resolution by deep spatial feature transform. Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy, CVPR. 612Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In CVPR, 2018. 2, 6, 12\n\nEsrgan: Enhanced super-resolution generative adversarial networks. Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, Chen Change Loy, ECCVW. 612Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En- hanced super-resolution generative adversarial networks. In ECCVW, 2018. 1, 2, 5, 6, 12\n\nQixiang Ye amd Wangmeng Zuo, and Liang Lin. Component divide-and-conquer for real-world image super-resolution. Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, ECCV, 2020. 612Pengxu Wei, Ziwei Xie, Hannan Lu, ZongYuan Zhan, Qix- iang Ye amd Wangmeng Zuo, and Liang Lin. Component divide-and-conquer for real-world image super-resolution. In ECCV, 2020. 2, 6, 12\n\nFinegrained attention and feature-sharing generative adversarial networks for single image super-resolution. Yitong Yan, Chuangchuang Liu, Changyou Chen, Xianfang Sun, Longcun Jin, Peng Xinyi, Xiang Zhou, IEEE Transactions on Multimedia. 25Yitong Yan, Chuangchuang Liu, Changyou Chen, Xianfang Sun, Longcun Jin, Peng Xinyi, and Xiang Zhou. Fine- grained attention and feature-sharing generative adversarial networks for single image super-resolution. IEEE Transac- tions on Multimedia, 2021. 2, 5\n\nPath-restore: Learning network path selection for image restoration. Ke Yu, Xintao Wang, Chao Dong, Xiaoou Tang, Chen Change Loy, arXiv:1904.10343Ke Yu, Xintao Wang, Chao Dong, Xiaoou Tang, and Chen Change Loy. Path-restore: Learning network path se- lection for image restoration. arXiv:1904.10343, 2019. 2\n\nUnsupervised image superresolution using cycle-in-cycle generative adversarial networks. Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang, Chao Dong, Liang Lin, In CVPRW. 2Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang, Chao Dong, and Liang Lin. Unsupervised image super- resolution using cycle-in-cycle generative adversarial net- works. In CVPRW, 2018. 2\n\nDesigning a practical degradation model for deep blind image super-resolution. Kai Zhang, Jingyun Liang, Luc Van Gool, Radu Timofte, arXiv:2103.14006612arXiv preprintKai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo- fte. Designing a practical degradation model for deep blind image super-resolution. arXiv preprint arXiv:2103.14006, 2021. 2, 3, 4, 6, 12\n\nLearning a single convolutional super-resolution network for multiple degradations. Kai Zhang, Wangmeng Zuo, Lei Zhang, CVPR. 1Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a single convolutional super-resolution network for multiple degradations. In CVPR, 2018. 1, 2\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, In ECCV. 2Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In ECCV, 2018. 2\n\nResidual dense network for image super-resolution. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu, CVPR. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In CVPR, 2018. 2\n\nSemantic understanding of scenes through the ade20k dataset. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba, International Journal of Computer Vision. 612Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi- dler, Adela Barriuso, and Antonio Torralba. Semantic under- standing of scenes through the ade20k dataset. International Journal of Computer Vision, 2019. 6, 12\n\nKernel modeling superresolution on real low-resolution images. Ruofan Zhou, Sabine Susstrunk, ICCV. Ruofan Zhou and Sabine Susstrunk. Kernel modeling super- resolution on real low-resolution images. In ICCV, 2019. 2\n", "annotations": {"author": "[{\"end\":162,\"start\":84},{\"end\":311,\"start\":163},{\"end\":441,\"start\":312},{\"end\":517,\"start\":442}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":91},{\"end\":175,\"start\":172},{\"end\":321,\"start\":317},{\"end\":451,\"start\":447}]", "author_first_name": "[{\"end\":90,\"start\":84},{\"end\":171,\"start\":163},{\"end\":316,\"start\":312},{\"end\":446,\"start\":442}]", "author_affiliation": "[{\"end\":161,\"start\":120},{\"end\":266,\"start\":195},{\"end\":310,\"start\":268},{\"end\":415,\"start\":344},{\"end\":440,\"start\":417},{\"end\":516,\"start\":475}]", "title": "[{\"end\":81,\"start\":1},{\"end\":598,\"start\":518}]", "venue": null, "abstract": "[{\"end\":2139,\"start\":1015}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2194,\"start\":2190},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2197,\"start\":2194},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2200,\"start\":2197},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2367,\"start\":2364},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2501,\"start\":2497},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2504,\"start\":2501},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2507,\"start\":2504},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2510,\"start\":2507},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2513,\"start\":2510},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2716,\"start\":2712},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2718,\"start\":2716},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2721,\"start\":2718},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3001,\"start\":2997},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3004,\"start\":3001},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3151,\"start\":3147},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3154,\"start\":3151},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3157,\"start\":3154},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3376,\"start\":3372},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3379,\"start\":3376},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3382,\"start\":3379},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3464,\"start\":3460},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3691,\"start\":3687},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3784,\"start\":3780},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":4884,\"start\":4880},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5758,\"start\":5754},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5761,\"start\":5758},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5764,\"start\":5761},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5924,\"start\":5920},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5927,\"start\":5924},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6744,\"start\":6740},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6747,\"start\":6744},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6750,\"start\":6747},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6753,\"start\":6750},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6756,\"start\":6753},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6759,\"start\":6756},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6762,\"start\":6759},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6765,\"start\":6762},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6768,\"start\":6765},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":6771,\"start\":6768},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6773,\"start\":6771},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6776,\"start\":6773},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6832,\"start\":6829},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6835,\"start\":6832},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6910,\"start\":6906},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7009,\"start\":7005},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7012,\"start\":7009},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7015,\"start\":7012},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7018,\"start\":7015},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7194,\"start\":7190},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7196,\"start\":7194},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7199,\"start\":7196},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7481,\"start\":7478},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7484,\"start\":7481},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7514,\"start\":7510},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7517,\"start\":7514},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7520,\"start\":7517},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8001,\"start\":7998},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8004,\"start\":8001},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8080,\"start\":8076},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8083,\"start\":8080},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8163,\"start\":8159},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8166,\"start\":8163},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8493,\"start\":8489},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8496,\"start\":8493},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8539,\"start\":8535},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8542,\"start\":8539},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8544,\"start\":8542},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8721,\"start\":8717},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8724,\"start\":8721},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8727,\"start\":8724},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9030,\"start\":9026},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9033,\"start\":9030},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":10665,\"start\":10661},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":11462,\"start\":11458},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11465,\"start\":11462},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12998,\"start\":12994},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":16009,\"start\":16005},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18636,\"start\":18635},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":18972,\"start\":18968},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19291,\"start\":19287},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19996,\"start\":19992},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19999,\"start\":19996},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20381,\"start\":20377},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21023,\"start\":21019},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21041,\"start\":21037},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21044,\"start\":21041},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21045,\"start\":21044},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21149,\"start\":21146},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21164,\"start\":21160},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21194,\"start\":21190},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21367,\"start\":21363},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21927,\"start\":21923},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":21999,\"start\":21995},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22526,\"start\":22524},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22528,\"start\":22526},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22536,\"start\":22533},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":22538,\"start\":22536},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22860,\"start\":22857},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22863,\"start\":22860},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22897,\"start\":22894},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22900,\"start\":22897},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23042,\"start\":23038},{\"end\":23045,\"start\":23042},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24491,\"start\":24487},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24501,\"start\":24497},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":24511,\"start\":24507},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24524,\"start\":24520},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":24540,\"start\":24536},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24630,\"start\":24627},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":24644,\"start\":24640},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":24657,\"start\":24653},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24668,\"start\":24664},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":24692,\"start\":24688},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24854,\"start\":24851},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29000,\"start\":28996},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":30509,\"start\":30505},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":30664,\"start\":30661},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":30753,\"start\":30749},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":30763,\"start\":30759},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":30773,\"start\":30769},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30786,\"start\":30782},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":30802,\"start\":30798},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":30892,\"start\":30889},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":30906,\"start\":30902},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":30919,\"start\":30915},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":30930,\"start\":30926},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30955,\"start\":30952},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":30982,\"start\":30978},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":33562,\"start\":33559}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31726,\"start\":31663},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31845,\"start\":31727},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32176,\"start\":31846},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32720,\"start\":32177},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32877,\"start\":32721},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33502,\"start\":32878},{\"attributes\":{\"id\":\"fig_6\"},\"end\":33623,\"start\":33503},{\"attributes\":{\"id\":\"fig_7\"},\"end\":33782,\"start\":33624},{\"attributes\":{\"id\":\"fig_8\"},\"end\":34213,\"start\":33783},{\"attributes\":{\"id\":\"fig_9\"},\"end\":34420,\"start\":34214},{\"attributes\":{\"id\":\"fig_10\"},\"end\":34571,\"start\":34421},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":35326,\"start\":34572}]", "paragraph": "[{\"end\":2687,\"start\":2155},{\"end\":3726,\"start\":2689},{\"end\":5347,\"start\":3728},{\"end\":6130,\"start\":5349},{\"end\":6690,\"start\":6132},{\"end\":7200,\"start\":6707},{\"end\":7757,\"start\":7202},{\"end\":8836,\"start\":7759},{\"end\":9370,\"start\":8882},{\"end\":9858,\"start\":9407},{\"end\":10073,\"start\":9914},{\"end\":10747,\"start\":10198},{\"end\":10956,\"start\":10806},{\"end\":11466,\"start\":10958},{\"end\":11816,\"start\":11468},{\"end\":12294,\"start\":11843},{\"end\":12562,\"start\":12296},{\"end\":13074,\"start\":12584},{\"end\":13279,\"start\":13076},{\"end\":13830,\"start\":13312},{\"end\":14283,\"start\":13832},{\"end\":16565,\"start\":14293},{\"end\":16611,\"start\":16567},{\"end\":16960,\"start\":16613},{\"end\":17832,\"start\":16996},{\"end\":18873,\"start\":17885},{\"end\":20217,\"start\":18899},{\"end\":21047,\"start\":20219},{\"end\":21110,\"start\":21093},{\"end\":22719,\"start\":21112},{\"end\":24372,\"start\":22721},{\"end\":24992,\"start\":24405},{\"end\":25984,\"start\":24994},{\"end\":26177,\"start\":26005},{\"end\":27853,\"start\":26179},{\"end\":28091,\"start\":27869},{\"end\":28231,\"start\":28101},{\"end\":28313,\"start\":28233},{\"end\":29845,\"start\":28328},{\"end\":30413,\"start\":29973},{\"end\":30665,\"start\":30445},{\"end\":31136,\"start\":30667},{\"end\":31662,\"start\":31172}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9406,\"start\":9371},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9913,\"start\":9859},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10197,\"start\":10074},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10805,\"start\":10748},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17884,\"start\":17833},{\"attributes\":{\"id\":\"formula_6\"},\"end\":29948,\"start\":29846}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2153,\"start\":2141},{\"attributes\":{\"n\":\"2.\"},\"end\":6705,\"start\":6693},{\"attributes\":{\"n\":\"3.\"},\"end\":8850,\"start\":8839},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8880,\"start\":8853},{\"end\":11841,\"start\":11819},{\"end\":12582,\"start\":12565},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13310,\"start\":13282},{\"end\":14291,\"start\":14286},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16994,\"start\":16963},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18897,\"start\":18876},{\"attributes\":{\"n\":\"4.\"},\"end\":21061,\"start\":21050},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21091,\"start\":21064},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24403,\"start\":24375},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26003,\"start\":25987},{\"attributes\":{\"n\":\"4.4.\"},\"end\":27867,\"start\":27856},{\"end\":28099,\"start\":28094},{\"attributes\":{\"n\":\"5.\"},\"end\":28326,\"start\":28316},{\"end\":29971,\"start\":29950},{\"end\":30443,\"start\":30416},{\"end\":31170,\"start\":31139},{\"end\":31673,\"start\":31664},{\"end\":31738,\"start\":31728},{\"end\":31857,\"start\":31847},{\"end\":32188,\"start\":32178},{\"end\":32732,\"start\":32722},{\"end\":32901,\"start\":32879},{\"end\":33515,\"start\":33504},{\"end\":33636,\"start\":33625},{\"end\":33803,\"start\":33784},{\"end\":34226,\"start\":34215},{\"end\":34433,\"start\":34422},{\"end\":34582,\"start\":34573}]", "table": "[{\"end\":35326,\"start\":34789}]", "figure_caption": "[{\"end\":31726,\"start\":31676},{\"end\":31845,\"start\":31740},{\"end\":32176,\"start\":31859},{\"end\":32720,\"start\":32190},{\"end\":32877,\"start\":32734},{\"end\":33502,\"start\":32906},{\"end\":33623,\"start\":33518},{\"end\":33782,\"start\":33639},{\"end\":34213,\"start\":33808},{\"end\":34420,\"start\":34229},{\"end\":34571,\"start\":34436},{\"end\":34789,\"start\":34584}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13597,\"start\":13591},{\"end\":13895,\"start\":13887},{\"end\":16488,\"start\":16482},{\"end\":16959,\"start\":16952},{\"end\":17418,\"start\":17410},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18021,\"start\":18015},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18341,\"start\":18329},{\"end\":18703,\"start\":18695},{\"end\":19068,\"start\":19062},{\"end\":20104,\"start\":20096},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24370,\"start\":24364},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24914,\"start\":24908},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25024,\"start\":25018},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26313,\"start\":26307},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":26680,\"start\":26674},{\"end\":26977,\"start\":26971},{\"end\":27451,\"start\":27444},{\"end\":27980,\"start\":27973},{\"end\":28170,\"start\":28162},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29043,\"start\":29036},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29666,\"start\":29659},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":30137,\"start\":30130},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31249,\"start\":31242}]", "bib_author_first_name": "[{\"end\":35975,\"start\":35968},{\"end\":35991,\"start\":35987},{\"end\":36205,\"start\":36201},{\"end\":36225,\"start\":36220},{\"end\":36241,\"start\":36235},{\"end\":36506,\"start\":36500},{\"end\":36517,\"start\":36513},{\"end\":36531,\"start\":36527},{\"end\":36763,\"start\":36757},{\"end\":36775,\"start\":36770},{\"end\":36967,\"start\":36960},{\"end\":36976,\"start\":36973},{\"end\":36990,\"start\":36983},{\"end\":37004,\"start\":36997},{\"end\":37013,\"start\":37010},{\"end\":37274,\"start\":37273},{\"end\":37276,\"start\":37275},{\"end\":37291,\"start\":37285},{\"end\":37305,\"start\":37298},{\"end\":37318,\"start\":37312},{\"end\":37334,\"start\":37323},{\"end\":37583,\"start\":37580},{\"end\":37596,\"start\":37589},{\"end\":37610,\"start\":37602},{\"end\":37625,\"start\":37618},{\"end\":37634,\"start\":37631},{\"end\":37854,\"start\":37851},{\"end\":37864,\"start\":37861},{\"end\":37878,\"start\":37871},{\"end\":37893,\"start\":37887},{\"end\":37901,\"start\":37898},{\"end\":37908,\"start\":37906},{\"end\":38139,\"start\":38135},{\"end\":38150,\"start\":38146},{\"end\":38157,\"start\":38151},{\"end\":38170,\"start\":38163},{\"end\":38181,\"start\":38175},{\"end\":38401,\"start\":38397},{\"end\":38412,\"start\":38408},{\"end\":38419,\"start\":38413},{\"end\":38432,\"start\":38425},{\"end\":38443,\"start\":38437},{\"end\":38737,\"start\":38730},{\"end\":38748,\"start\":38744},{\"end\":39074,\"start\":39068},{\"end\":39092,\"start\":39085},{\"end\":39101,\"start\":39097},{\"end\":39281,\"start\":39275},{\"end\":39295,\"start\":39291},{\"end\":39309,\"start\":39303},{\"end\":39458,\"start\":39455},{\"end\":39475,\"start\":39471},{\"end\":39496,\"start\":39491},{\"end\":39508,\"start\":39504},{\"end\":39518,\"start\":39513},{\"end\":39540,\"start\":39533},{\"end\":39553,\"start\":39548},{\"end\":39571,\"start\":39565},{\"end\":39805,\"start\":39802},{\"end\":39822,\"start\":39818},{\"end\":39843,\"start\":39838},{\"end\":39855,\"start\":39851},{\"end\":39865,\"start\":39860},{\"end\":39887,\"start\":39880},{\"end\":39900,\"start\":39895},{\"end\":39918,\"start\":39912},{\"end\":40178,\"start\":40172},{\"end\":40189,\"start\":40183},{\"end\":40202,\"start\":40194},{\"end\":40212,\"start\":40208},{\"end\":40410,\"start\":40402},{\"end\":40422,\"start\":40418},{\"end\":40447,\"start\":40438},{\"end\":40665,\"start\":40659},{\"end\":40682,\"start\":40675},{\"end\":40697,\"start\":40693},{\"end\":40714,\"start\":40707},{\"end\":40727,\"start\":40724},{\"end\":41004,\"start\":40995},{\"end\":41012,\"start\":41009},{\"end\":41022,\"start\":41018},{\"end\":41036,\"start\":41028},{\"end\":41048,\"start\":41043},{\"end\":41059,\"start\":41053},{\"end\":41328,\"start\":41322},{\"end\":41347,\"start\":41338},{\"end\":41357,\"start\":41355},{\"end\":41591,\"start\":41586},{\"end\":41601,\"start\":41597},{\"end\":41606,\"start\":41602},{\"end\":41621,\"start\":41612},{\"end\":41845,\"start\":41840},{\"end\":41855,\"start\":41851},{\"end\":41860,\"start\":41856},{\"end\":41875,\"start\":41866},{\"end\":42070,\"start\":42062},{\"end\":42084,\"start\":42079},{\"end\":42268,\"start\":42259},{\"end\":42281,\"start\":42274},{\"end\":42297,\"start\":42289},{\"end\":42315,\"start\":42305},{\"end\":42580,\"start\":42571},{\"end\":42593,\"start\":42588},{\"end\":42607,\"start\":42601},{\"end\":42620,\"start\":42616},{\"end\":42638,\"start\":42632},{\"end\":42660,\"start\":42651},{\"end\":42675,\"start\":42669},{\"end\":42691,\"start\":42684},{\"end\":42708,\"start\":42700},{\"end\":42720,\"start\":42715},{\"end\":43101,\"start\":43092},{\"end\":43114,\"start\":43109},{\"end\":43128,\"start\":43122},{\"end\":43141,\"start\":43137},{\"end\":43159,\"start\":43153},{\"end\":43181,\"start\":43172},{\"end\":43196,\"start\":43190},{\"end\":43212,\"start\":43205},{\"end\":43229,\"start\":43221},{\"end\":43241,\"start\":43236},{\"end\":43599,\"start\":43596},{\"end\":43613,\"start\":43605},{\"end\":43625,\"start\":43619},{\"end\":43639,\"start\":43631},{\"end\":43654,\"start\":43645},{\"end\":43881,\"start\":43876},{\"end\":43892,\"start\":43887},{\"end\":43904,\"start\":43898},{\"end\":43911,\"start\":43909},{\"end\":43925,\"start\":43921},{\"end\":44133,\"start\":44131},{\"end\":44145,\"start\":44139},{\"end\":44437,\"start\":44433},{\"end\":44448,\"start\":44443},{\"end\":44460,\"start\":44454},{\"end\":44470,\"start\":44466},{\"end\":44477,\"start\":44471},{\"end\":44491,\"start\":44483},{\"end\":44739,\"start\":44734},{\"end\":44748,\"start\":44745},{\"end\":44762,\"start\":44753},{\"end\":44776,\"start\":44769},{\"end\":45241,\"start\":45234},{\"end\":45257,\"start\":45251},{\"end\":45273,\"start\":45269},{\"end\":45499,\"start\":45489},{\"end\":45508,\"start\":45505},{\"end\":45521,\"start\":45516},{\"end\":45531,\"start\":45526},{\"end\":45544,\"start\":45538},{\"end\":45771,\"start\":45766},{\"end\":45788,\"start\":45782},{\"end\":45946,\"start\":45941},{\"end\":45960,\"start\":45955},{\"end\":45980,\"start\":45976},{\"end\":45982,\"start\":45981},{\"end\":46246,\"start\":46240},{\"end\":46262,\"start\":46255},{\"end\":46280,\"start\":46272},{\"end\":46295,\"start\":46289},{\"end\":46573,\"start\":46563},{\"end\":46587,\"start\":46579},{\"end\":46603,\"start\":46595},{\"end\":46624,\"start\":46616},{\"end\":46893,\"start\":46889},{\"end\":46914,\"start\":46907},{\"end\":46930,\"start\":46924},{\"end\":47345,\"start\":47344},{\"end\":47347,\"start\":47346},{\"end\":47363,\"start\":47355},{\"end\":47380,\"start\":47373},{\"end\":47631,\"start\":47626},{\"end\":47648,\"start\":47643},{\"end\":47662,\"start\":47658},{\"end\":47937,\"start\":47931},{\"end\":47947,\"start\":47943},{\"end\":47965,\"start\":47959},{\"end\":47982,\"start\":47974},{\"end\":47990,\"start\":47989},{\"end\":48002,\"start\":47999},{\"end\":48017,\"start\":48011},{\"end\":48031,\"start\":48026},{\"end\":48343,\"start\":48336},{\"end\":48354,\"start\":48350},{\"end\":48623,\"start\":48619},{\"end\":48633,\"start\":48629},{\"end\":48648,\"start\":48640},{\"end\":48858,\"start\":48854},{\"end\":48875,\"start\":48868},{\"end\":48890,\"start\":48887},{\"end\":48911,\"start\":48901},{\"end\":48921,\"start\":48918},{\"end\":48932,\"start\":48929},{\"end\":48946,\"start\":48938},{\"end\":48958,\"start\":48952},{\"end\":48972,\"start\":48964},{\"end\":49334,\"start\":49325},{\"end\":49349,\"start\":49341},{\"end\":49362,\"start\":49356},{\"end\":49375,\"start\":49369},{\"end\":49387,\"start\":49380},{\"end\":49397,\"start\":49394},{\"end\":49407,\"start\":49402},{\"end\":49687,\"start\":49681},{\"end\":49696,\"start\":49694},{\"end\":49708,\"start\":49701},{\"end\":49720,\"start\":49716},{\"end\":49883,\"start\":49877},{\"end\":49892,\"start\":49890},{\"end\":49898,\"start\":49897},{\"end\":49900,\"start\":49899},{\"end\":49913,\"start\":49909},{\"end\":49931,\"start\":49920},{\"end\":50164,\"start\":50158},{\"end\":50173,\"start\":50171},{\"end\":50182,\"start\":50178},{\"end\":50200,\"start\":50189},{\"end\":50455,\"start\":50449},{\"end\":50464,\"start\":50462},{\"end\":50477,\"start\":50469},{\"end\":50488,\"start\":50482},{\"end\":50498,\"start\":50493},{\"end\":50508,\"start\":50504},{\"end\":50517,\"start\":50515},{\"end\":50535,\"start\":50524},{\"end\":50866,\"start\":50860},{\"end\":50877,\"start\":50872},{\"end\":50889,\"start\":50883},{\"end\":50902,\"start\":50894},{\"end\":51227,\"start\":51221},{\"end\":51245,\"start\":51233},{\"end\":51259,\"start\":51251},{\"end\":51274,\"start\":51266},{\"end\":51287,\"start\":51280},{\"end\":51297,\"start\":51293},{\"end\":51310,\"start\":51305},{\"end\":51681,\"start\":51679},{\"end\":51692,\"start\":51686},{\"end\":51703,\"start\":51699},{\"end\":51716,\"start\":51710},{\"end\":51734,\"start\":51723},{\"end\":52012,\"start\":52008},{\"end\":52025,\"start\":52019},{\"end\":52037,\"start\":52031},{\"end\":52053,\"start\":52045},{\"end\":52065,\"start\":52061},{\"end\":52077,\"start\":52072},{\"end\":52367,\"start\":52364},{\"end\":52382,\"start\":52375},{\"end\":52393,\"start\":52390},{\"end\":52408,\"start\":52404},{\"end\":52732,\"start\":52729},{\"end\":52748,\"start\":52740},{\"end\":52757,\"start\":52754},{\"end\":52998,\"start\":52993},{\"end\":53013,\"start\":53006},{\"end\":53021,\"start\":53018},{\"end\":53032,\"start\":53026},{\"end\":53045,\"start\":53039},{\"end\":53056,\"start\":53053},{\"end\":53293,\"start\":53288},{\"end\":53307,\"start\":53301},{\"end\":53316,\"start\":53314},{\"end\":53329,\"start\":53323},{\"end\":53340,\"start\":53337},{\"end\":53547,\"start\":53542},{\"end\":53558,\"start\":53554},{\"end\":53571,\"start\":53565},{\"end\":53582,\"start\":53578},{\"end\":53594,\"start\":53589},{\"end\":53608,\"start\":53603},{\"end\":53626,\"start\":53619},{\"end\":53970,\"start\":53964},{\"end\":53983,\"start\":53977}]", "bib_author_last_name": "[{\"end\":35985,\"start\":35976},{\"end\":35999,\"start\":35992},{\"end\":36218,\"start\":36206},{\"end\":36233,\"start\":36226},{\"end\":36247,\"start\":36242},{\"end\":36511,\"start\":36507},{\"end\":36525,\"start\":36518},{\"end\":36539,\"start\":36532},{\"end\":36768,\"start\":36764},{\"end\":36784,\"start\":36776},{\"end\":36971,\"start\":36968},{\"end\":36981,\"start\":36977},{\"end\":36995,\"start\":36991},{\"end\":37008,\"start\":37005},{\"end\":37019,\"start\":37014},{\"end\":37283,\"start\":37277},{\"end\":37296,\"start\":37292},{\"end\":37310,\"start\":37306},{\"end\":37321,\"start\":37319},{\"end\":37337,\"start\":37335},{\"end\":37342,\"start\":37339},{\"end\":37587,\"start\":37584},{\"end\":37600,\"start\":37597},{\"end\":37616,\"start\":37611},{\"end\":37629,\"start\":37626},{\"end\":37640,\"start\":37635},{\"end\":37859,\"start\":37855},{\"end\":37869,\"start\":37865},{\"end\":37885,\"start\":37879},{\"end\":37896,\"start\":37894},{\"end\":37904,\"start\":37902},{\"end\":37916,\"start\":37909},{\"end\":38144,\"start\":38140},{\"end\":38161,\"start\":38158},{\"end\":38173,\"start\":38171},{\"end\":38186,\"start\":38182},{\"end\":38406,\"start\":38402},{\"end\":38423,\"start\":38420},{\"end\":38435,\"start\":38433},{\"end\":38448,\"start\":38444},{\"end\":38742,\"start\":38738},{\"end\":38754,\"start\":38749},{\"end\":39083,\"start\":39075},{\"end\":39095,\"start\":39093},{\"end\":39109,\"start\":39102},{\"end\":39289,\"start\":39282},{\"end\":39301,\"start\":39296},{\"end\":39315,\"start\":39310},{\"end\":39469,\"start\":39459},{\"end\":39489,\"start\":39476},{\"end\":39502,\"start\":39497},{\"end\":39511,\"start\":39509},{\"end\":39531,\"start\":39519},{\"end\":39546,\"start\":39541},{\"end\":39563,\"start\":39554},{\"end\":39578,\"start\":39572},{\"end\":39816,\"start\":39806},{\"end\":39836,\"start\":39823},{\"end\":39849,\"start\":39844},{\"end\":39858,\"start\":39856},{\"end\":39878,\"start\":39866},{\"end\":39893,\"start\":39888},{\"end\":39910,\"start\":39901},{\"end\":39925,\"start\":39919},{\"end\":40181,\"start\":40179},{\"end\":40192,\"start\":40190},{\"end\":40206,\"start\":40203},{\"end\":40217,\"start\":40213},{\"end\":40416,\"start\":40411},{\"end\":40436,\"start\":40423},{\"end\":40453,\"start\":40448},{\"end\":40673,\"start\":40666},{\"end\":40691,\"start\":40683},{\"end\":40705,\"start\":40698},{\"end\":40722,\"start\":40715},{\"end\":40736,\"start\":40728},{\"end\":41007,\"start\":41005},{\"end\":41016,\"start\":41013},{\"end\":41026,\"start\":41023},{\"end\":41041,\"start\":41037},{\"end\":41051,\"start\":41049},{\"end\":41065,\"start\":41060},{\"end\":41336,\"start\":41329},{\"end\":41353,\"start\":41348},{\"end\":41365,\"start\":41358},{\"end\":41595,\"start\":41592},{\"end\":41610,\"start\":41607},{\"end\":41625,\"start\":41622},{\"end\":41849,\"start\":41846},{\"end\":41864,\"start\":41861},{\"end\":41879,\"start\":41876},{\"end\":42077,\"start\":42071},{\"end\":42087,\"start\":42085},{\"end\":42272,\"start\":42269},{\"end\":42287,\"start\":42282},{\"end\":42303,\"start\":42298},{\"end\":42320,\"start\":42316},{\"end\":42586,\"start\":42581},{\"end\":42599,\"start\":42594},{\"end\":42614,\"start\":42608},{\"end\":42630,\"start\":42621},{\"end\":42649,\"start\":42639},{\"end\":42667,\"start\":42661},{\"end\":42682,\"start\":42676},{\"end\":42698,\"start\":42692},{\"end\":42713,\"start\":42709},{\"end\":42725,\"start\":42721},{\"end\":43107,\"start\":43102},{\"end\":43120,\"start\":43115},{\"end\":43135,\"start\":43129},{\"end\":43151,\"start\":43142},{\"end\":43170,\"start\":43160},{\"end\":43188,\"start\":43182},{\"end\":43203,\"start\":43197},{\"end\":43219,\"start\":43213},{\"end\":43234,\"start\":43230},{\"end\":43246,\"start\":43242},{\"end\":43603,\"start\":43600},{\"end\":43617,\"start\":43614},{\"end\":43629,\"start\":43626},{\"end\":43643,\"start\":43640},{\"end\":43658,\"start\":43655},{\"end\":43885,\"start\":43882},{\"end\":43896,\"start\":43893},{\"end\":43907,\"start\":43905},{\"end\":43919,\"start\":43912},{\"end\":43930,\"start\":43926},{\"end\":44137,\"start\":44134},{\"end\":44149,\"start\":44146},{\"end\":44441,\"start\":44438},{\"end\":44452,\"start\":44449},{\"end\":44464,\"start\":44461},{\"end\":44481,\"start\":44478},{\"end\":44497,\"start\":44492},{\"end\":44743,\"start\":44740},{\"end\":44751,\"start\":44749},{\"end\":44767,\"start\":44763},{\"end\":44781,\"start\":44777},{\"end\":45083,\"start\":45066},{\"end\":45093,\"start\":45085},{\"end\":45249,\"start\":45242},{\"end\":45267,\"start\":45258},{\"end\":45281,\"start\":45274},{\"end\":45503,\"start\":45500},{\"end\":45514,\"start\":45509},{\"end\":45524,\"start\":45522},{\"end\":45536,\"start\":45532},{\"end\":45548,\"start\":45545},{\"end\":45780,\"start\":45772},{\"end\":45794,\"start\":45789},{\"end\":45953,\"start\":45947},{\"end\":45974,\"start\":45961},{\"end\":45988,\"start\":45983},{\"end\":46253,\"start\":46247},{\"end\":46270,\"start\":46263},{\"end\":46287,\"start\":46281},{\"end\":46303,\"start\":46296},{\"end\":46577,\"start\":46574},{\"end\":46593,\"start\":46588},{\"end\":46614,\"start\":46604},{\"end\":46628,\"start\":46625},{\"end\":46905,\"start\":46894},{\"end\":46922,\"start\":46915},{\"end\":46935,\"start\":46931},{\"end\":47353,\"start\":47348},{\"end\":47371,\"start\":47364},{\"end\":47390,\"start\":47381},{\"end\":47398,\"start\":47392},{\"end\":47641,\"start\":47632},{\"end\":47656,\"start\":47649},{\"end\":47670,\"start\":47663},{\"end\":47941,\"start\":47938},{\"end\":47957,\"start\":47948},{\"end\":47972,\"start\":47966},{\"end\":47987,\"start\":47983},{\"end\":47997,\"start\":47991},{\"end\":48009,\"start\":48003},{\"end\":48024,\"start\":48018},{\"end\":48040,\"start\":48032},{\"end\":48046,\"start\":48042},{\"end\":48348,\"start\":48344},{\"end\":48359,\"start\":48355},{\"end\":48627,\"start\":48624},{\"end\":48638,\"start\":48634},{\"end\":48652,\"start\":48649},{\"end\":48866,\"start\":48859},{\"end\":48885,\"start\":48876},{\"end\":48899,\"start\":48891},{\"end\":48916,\"start\":48912},{\"end\":48927,\"start\":48922},{\"end\":48936,\"start\":48933},{\"end\":48950,\"start\":48947},{\"end\":48962,\"start\":48959},{\"end\":48976,\"start\":48973},{\"end\":48991,\"start\":48978},{\"end\":49339,\"start\":49335},{\"end\":49354,\"start\":49350},{\"end\":49367,\"start\":49363},{\"end\":49378,\"start\":49376},{\"end\":49392,\"start\":49388},{\"end\":49400,\"start\":49398},{\"end\":49411,\"start\":49408},{\"end\":49692,\"start\":49688},{\"end\":49699,\"start\":49697},{\"end\":49714,\"start\":49709},{\"end\":49725,\"start\":49721},{\"end\":49888,\"start\":49884},{\"end\":49895,\"start\":49893},{\"end\":49907,\"start\":49901},{\"end\":49918,\"start\":49914},{\"end\":49936,\"start\":49932},{\"end\":49941,\"start\":49938},{\"end\":50169,\"start\":50165},{\"end\":50176,\"start\":50174},{\"end\":50187,\"start\":50183},{\"end\":50204,\"start\":50201},{\"end\":50460,\"start\":50456},{\"end\":50467,\"start\":50465},{\"end\":50480,\"start\":50478},{\"end\":50491,\"start\":50489},{\"end\":50502,\"start\":50499},{\"end\":50513,\"start\":50509},{\"end\":50522,\"start\":50518},{\"end\":50539,\"start\":50536},{\"end\":50870,\"start\":50867},{\"end\":50881,\"start\":50878},{\"end\":50892,\"start\":50890},{\"end\":50907,\"start\":50903},{\"end\":51231,\"start\":51228},{\"end\":51249,\"start\":51246},{\"end\":51264,\"start\":51260},{\"end\":51278,\"start\":51275},{\"end\":51291,\"start\":51288},{\"end\":51303,\"start\":51298},{\"end\":51315,\"start\":51311},{\"end\":51684,\"start\":51682},{\"end\":51697,\"start\":51693},{\"end\":51708,\"start\":51704},{\"end\":51721,\"start\":51717},{\"end\":51738,\"start\":51735},{\"end\":52017,\"start\":52013},{\"end\":52029,\"start\":52026},{\"end\":52043,\"start\":52038},{\"end\":52059,\"start\":52054},{\"end\":52070,\"start\":52066},{\"end\":52081,\"start\":52078},{\"end\":52373,\"start\":52368},{\"end\":52388,\"start\":52383},{\"end\":52402,\"start\":52394},{\"end\":52416,\"start\":52409},{\"end\":52738,\"start\":52733},{\"end\":52752,\"start\":52749},{\"end\":52763,\"start\":52758},{\"end\":53004,\"start\":52999},{\"end\":53016,\"start\":53014},{\"end\":53024,\"start\":53022},{\"end\":53037,\"start\":53033},{\"end\":53051,\"start\":53046},{\"end\":53059,\"start\":53057},{\"end\":53299,\"start\":53294},{\"end\":53312,\"start\":53308},{\"end\":53321,\"start\":53317},{\"end\":53335,\"start\":53330},{\"end\":53343,\"start\":53341},{\"end\":53552,\"start\":53548},{\"end\":53563,\"start\":53559},{\"end\":53576,\"start\":53572},{\"end\":53587,\"start\":53583},{\"end\":53601,\"start\":53595},{\"end\":53617,\"start\":53609},{\"end\":53635,\"start\":53627},{\"end\":53975,\"start\":53971},{\"end\":53993,\"start\":53984}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4493958},\"end\":36135,\"start\":35894},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":202577523},\"end\":36397,\"start\":36137},{\"attributes\":{\"id\":\"b2\"},\"end\":36719,\"start\":36399},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":215763824},\"end\":36876,\"start\":36721},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":90260111},\"end\":37200,\"start\":36878},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":227239234},\"end\":37512,\"start\":37202},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":174788791},\"end\":37796,\"start\":37514},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":57246310},\"end\":38067,\"start\":37798},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18874645},\"end\":38337,\"start\":38069},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6593498},\"end\":38619,\"start\":38339},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1724361},\"end\":39012,\"start\":38621},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":208158302},\"end\":39236,\"start\":39014},{\"attributes\":{\"id\":\"b12\"},\"end\":39424,\"start\":39238},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1033682},\"end\":39771,\"start\":39426},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1033682},\"end\":40113,\"start\":39773},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":102352104},\"end\":40349,\"start\":40115},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3739626},\"end\":40585,\"start\":40351},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":4676177},\"end\":40922,\"start\":40587},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":220246167},\"end\":41251,\"start\":40924},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":980236},\"end\":41512,\"start\":41253},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9971732},\"end\":41772,\"start\":41514},{\"attributes\":{\"id\":\"b21\"},\"end\":42016,\"start\":41774},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6628106},\"end\":42185,\"start\":42018},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1543021},\"end\":42484,\"start\":42187},{\"attributes\":{\"id\":\"b24\"},\"end\":43005,\"start\":42486},{\"attributes\":{\"id\":\"b25\"},\"end\":43527,\"start\":43007},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6540453},\"end\":43823,\"start\":43529},{\"attributes\":{\"doi\":\"arXiv:2107.03055\",\"id\":\"b27\"},\"end\":44084,\"start\":43825},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":8011467},\"end\":44380,\"start\":44086},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":47007607},\"end\":44652,\"start\":44382},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":219022749},\"end\":45062,\"start\":44654},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b31\"},\"end\":45177,\"start\":45064},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":202712840},\"end\":45420,\"start\":45179},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":222141081},\"end\":45726,\"start\":45422},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":7044126},\"end\":45889,\"start\":45728},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16892725},\"end\":46178,\"start\":45891},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3366315},\"end\":46463,\"start\":46180},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6253552},\"end\":46823,\"start\":46465},{\"attributes\":{\"id\":\"b38\"},\"end\":47263,\"start\":46825},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206771333},\"end\":47559,\"start\":47265},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":211572480},\"end\":47820,\"start\":47561},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7037846},\"end\":48299,\"start\":47822},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":204804905},\"end\":48558,\"start\":48301},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":21618854},\"end\":48776,\"start\":48560},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":484327},\"end\":49247,\"start\":48778},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":232478491},\"end\":49607,\"start\":49249},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":231572764},\"end\":49873,\"start\":49609},{\"attributes\":{\"id\":\"b47\"},\"end\":50066,\"start\":49875},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":4710407},\"end\":50380,\"start\":50068},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52154773},\"end\":50746,\"start\":50382},{\"attributes\":{\"id\":\"b50\"},\"end\":51110,\"start\":50748},{\"attributes\":{\"id\":\"b51\"},\"end\":51608,\"start\":51112},{\"attributes\":{\"doi\":\"arXiv:1904.10343\",\"id\":\"b52\"},\"end\":51917,\"start\":51610},{\"attributes\":{\"id\":\"b53\"},\"end\":52283,\"start\":51919},{\"attributes\":{\"doi\":\"arXiv:2103.14006\",\"id\":\"b54\"},\"end\":52643,\"start\":52285},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":2141622},\"end\":52915,\"start\":52645},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":49657846},\"end\":53235,\"start\":52917},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3619954},\"end\":53479,\"start\":53237},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":11371972},\"end\":53899,\"start\":53481},{\"attributes\":{\"id\":\"b59\"},\"end\":54116,\"start\":53901}]", "bib_title": "[{\"end\":35966,\"start\":35894},{\"end\":36199,\"start\":36137},{\"end\":36498,\"start\":36399},{\"end\":36755,\"start\":36721},{\"end\":36958,\"start\":36878},{\"end\":37271,\"start\":37202},{\"end\":37578,\"start\":37514},{\"end\":37849,\"start\":37798},{\"end\":38133,\"start\":38069},{\"end\":38395,\"start\":38339},{\"end\":38728,\"start\":38621},{\"end\":39066,\"start\":39014},{\"end\":39273,\"start\":39238},{\"end\":39453,\"start\":39426},{\"end\":39800,\"start\":39773},{\"end\":40170,\"start\":40115},{\"end\":40400,\"start\":40351},{\"end\":40657,\"start\":40587},{\"end\":40993,\"start\":40924},{\"end\":41320,\"start\":41253},{\"end\":41584,\"start\":41514},{\"end\":41838,\"start\":41774},{\"end\":42060,\"start\":42018},{\"end\":42257,\"start\":42187},{\"end\":42569,\"start\":42486},{\"end\":43090,\"start\":43007},{\"end\":43594,\"start\":43529},{\"end\":44129,\"start\":44086},{\"end\":44431,\"start\":44382},{\"end\":44732,\"start\":44654},{\"end\":45232,\"start\":45179},{\"end\":45487,\"start\":45422},{\"end\":45764,\"start\":45728},{\"end\":45939,\"start\":45891},{\"end\":46238,\"start\":46180},{\"end\":46561,\"start\":46465},{\"end\":46887,\"start\":46825},{\"end\":47342,\"start\":47265},{\"end\":47624,\"start\":47561},{\"end\":47929,\"start\":47822},{\"end\":48334,\"start\":48301},{\"end\":48617,\"start\":48560},{\"end\":48852,\"start\":48778},{\"end\":49323,\"start\":49249},{\"end\":49679,\"start\":49609},{\"end\":50156,\"start\":50068},{\"end\":50447,\"start\":50382},{\"end\":50858,\"start\":50748},{\"end\":51219,\"start\":51112},{\"end\":52006,\"start\":51919},{\"end\":52727,\"start\":52645},{\"end\":52991,\"start\":52917},{\"end\":53286,\"start\":53237},{\"end\":53540,\"start\":53481},{\"end\":53962,\"start\":53901}]", "bib_author": "[{\"end\":35987,\"start\":35968},{\"end\":36001,\"start\":35987},{\"end\":36220,\"start\":36201},{\"end\":36235,\"start\":36220},{\"end\":36249,\"start\":36235},{\"end\":36513,\"start\":36500},{\"end\":36527,\"start\":36513},{\"end\":36541,\"start\":36527},{\"end\":36770,\"start\":36757},{\"end\":36786,\"start\":36770},{\"end\":36973,\"start\":36960},{\"end\":36983,\"start\":36973},{\"end\":36997,\"start\":36983},{\"end\":37010,\"start\":36997},{\"end\":37021,\"start\":37010},{\"end\":37285,\"start\":37273},{\"end\":37298,\"start\":37285},{\"end\":37312,\"start\":37298},{\"end\":37323,\"start\":37312},{\"end\":37339,\"start\":37323},{\"end\":37344,\"start\":37339},{\"end\":37589,\"start\":37580},{\"end\":37602,\"start\":37589},{\"end\":37618,\"start\":37602},{\"end\":37631,\"start\":37618},{\"end\":37642,\"start\":37631},{\"end\":37861,\"start\":37851},{\"end\":37871,\"start\":37861},{\"end\":37887,\"start\":37871},{\"end\":37898,\"start\":37887},{\"end\":37906,\"start\":37898},{\"end\":37918,\"start\":37906},{\"end\":38146,\"start\":38135},{\"end\":38163,\"start\":38146},{\"end\":38175,\"start\":38163},{\"end\":38188,\"start\":38175},{\"end\":38408,\"start\":38397},{\"end\":38425,\"start\":38408},{\"end\":38437,\"start\":38425},{\"end\":38450,\"start\":38437},{\"end\":38744,\"start\":38730},{\"end\":38756,\"start\":38744},{\"end\":39085,\"start\":39068},{\"end\":39097,\"start\":39085},{\"end\":39111,\"start\":39097},{\"end\":39291,\"start\":39275},{\"end\":39303,\"start\":39291},{\"end\":39317,\"start\":39303},{\"end\":39471,\"start\":39455},{\"end\":39491,\"start\":39471},{\"end\":39504,\"start\":39491},{\"end\":39513,\"start\":39504},{\"end\":39533,\"start\":39513},{\"end\":39548,\"start\":39533},{\"end\":39565,\"start\":39548},{\"end\":39580,\"start\":39565},{\"end\":39818,\"start\":39802},{\"end\":39838,\"start\":39818},{\"end\":39851,\"start\":39838},{\"end\":39860,\"start\":39851},{\"end\":39880,\"start\":39860},{\"end\":39895,\"start\":39880},{\"end\":39912,\"start\":39895},{\"end\":39927,\"start\":39912},{\"end\":40183,\"start\":40172},{\"end\":40194,\"start\":40183},{\"end\":40208,\"start\":40194},{\"end\":40219,\"start\":40208},{\"end\":40418,\"start\":40402},{\"end\":40438,\"start\":40418},{\"end\":40455,\"start\":40438},{\"end\":40675,\"start\":40659},{\"end\":40693,\"start\":40675},{\"end\":40707,\"start\":40693},{\"end\":40724,\"start\":40707},{\"end\":40738,\"start\":40724},{\"end\":41009,\"start\":40995},{\"end\":41018,\"start\":41009},{\"end\":41028,\"start\":41018},{\"end\":41043,\"start\":41028},{\"end\":41053,\"start\":41043},{\"end\":41067,\"start\":41053},{\"end\":41338,\"start\":41322},{\"end\":41355,\"start\":41338},{\"end\":41367,\"start\":41355},{\"end\":41597,\"start\":41586},{\"end\":41612,\"start\":41597},{\"end\":41627,\"start\":41612},{\"end\":41851,\"start\":41840},{\"end\":41866,\"start\":41851},{\"end\":41881,\"start\":41866},{\"end\":42079,\"start\":42062},{\"end\":42089,\"start\":42079},{\"end\":42274,\"start\":42259},{\"end\":42289,\"start\":42274},{\"end\":42305,\"start\":42289},{\"end\":42322,\"start\":42305},{\"end\":42588,\"start\":42571},{\"end\":42601,\"start\":42588},{\"end\":42616,\"start\":42601},{\"end\":42632,\"start\":42616},{\"end\":42651,\"start\":42632},{\"end\":42669,\"start\":42651},{\"end\":42684,\"start\":42669},{\"end\":42700,\"start\":42684},{\"end\":42715,\"start\":42700},{\"end\":42727,\"start\":42715},{\"end\":43109,\"start\":43092},{\"end\":43122,\"start\":43109},{\"end\":43137,\"start\":43122},{\"end\":43153,\"start\":43137},{\"end\":43172,\"start\":43153},{\"end\":43190,\"start\":43172},{\"end\":43205,\"start\":43190},{\"end\":43221,\"start\":43205},{\"end\":43236,\"start\":43221},{\"end\":43248,\"start\":43236},{\"end\":43605,\"start\":43596},{\"end\":43619,\"start\":43605},{\"end\":43631,\"start\":43619},{\"end\":43645,\"start\":43631},{\"end\":43660,\"start\":43645},{\"end\":43887,\"start\":43876},{\"end\":43898,\"start\":43887},{\"end\":43909,\"start\":43898},{\"end\":43921,\"start\":43909},{\"end\":43932,\"start\":43921},{\"end\":44139,\"start\":44131},{\"end\":44151,\"start\":44139},{\"end\":44443,\"start\":44433},{\"end\":44454,\"start\":44443},{\"end\":44466,\"start\":44454},{\"end\":44483,\"start\":44466},{\"end\":44499,\"start\":44483},{\"end\":44745,\"start\":44734},{\"end\":44753,\"start\":44745},{\"end\":44769,\"start\":44753},{\"end\":44783,\"start\":44769},{\"end\":45085,\"start\":45066},{\"end\":45095,\"start\":45085},{\"end\":45251,\"start\":45234},{\"end\":45269,\"start\":45251},{\"end\":45283,\"start\":45269},{\"end\":45505,\"start\":45489},{\"end\":45516,\"start\":45505},{\"end\":45526,\"start\":45516},{\"end\":45538,\"start\":45526},{\"end\":45550,\"start\":45538},{\"end\":45782,\"start\":45766},{\"end\":45796,\"start\":45782},{\"end\":45955,\"start\":45941},{\"end\":45976,\"start\":45955},{\"end\":45990,\"start\":45976},{\"end\":46255,\"start\":46240},{\"end\":46272,\"start\":46255},{\"end\":46289,\"start\":46272},{\"end\":46305,\"start\":46289},{\"end\":46579,\"start\":46563},{\"end\":46595,\"start\":46579},{\"end\":46616,\"start\":46595},{\"end\":46630,\"start\":46616},{\"end\":46907,\"start\":46889},{\"end\":46924,\"start\":46907},{\"end\":46937,\"start\":46924},{\"end\":47355,\"start\":47344},{\"end\":47373,\"start\":47355},{\"end\":47392,\"start\":47373},{\"end\":47400,\"start\":47392},{\"end\":47643,\"start\":47626},{\"end\":47658,\"start\":47643},{\"end\":47672,\"start\":47658},{\"end\":47943,\"start\":47931},{\"end\":47959,\"start\":47943},{\"end\":47974,\"start\":47959},{\"end\":47989,\"start\":47974},{\"end\":47999,\"start\":47989},{\"end\":48011,\"start\":47999},{\"end\":48026,\"start\":48011},{\"end\":48042,\"start\":48026},{\"end\":48048,\"start\":48042},{\"end\":48350,\"start\":48336},{\"end\":48361,\"start\":48350},{\"end\":48629,\"start\":48619},{\"end\":48640,\"start\":48629},{\"end\":48654,\"start\":48640},{\"end\":48868,\"start\":48854},{\"end\":48887,\"start\":48868},{\"end\":48901,\"start\":48887},{\"end\":48918,\"start\":48901},{\"end\":48929,\"start\":48918},{\"end\":48938,\"start\":48929},{\"end\":48952,\"start\":48938},{\"end\":48964,\"start\":48952},{\"end\":48978,\"start\":48964},{\"end\":48993,\"start\":48978},{\"end\":49341,\"start\":49325},{\"end\":49356,\"start\":49341},{\"end\":49369,\"start\":49356},{\"end\":49380,\"start\":49369},{\"end\":49394,\"start\":49380},{\"end\":49402,\"start\":49394},{\"end\":49413,\"start\":49402},{\"end\":49694,\"start\":49681},{\"end\":49701,\"start\":49694},{\"end\":49716,\"start\":49701},{\"end\":49727,\"start\":49716},{\"end\":49890,\"start\":49877},{\"end\":49897,\"start\":49890},{\"end\":49909,\"start\":49897},{\"end\":49920,\"start\":49909},{\"end\":49938,\"start\":49920},{\"end\":49943,\"start\":49938},{\"end\":50171,\"start\":50158},{\"end\":50178,\"start\":50171},{\"end\":50189,\"start\":50178},{\"end\":50206,\"start\":50189},{\"end\":50462,\"start\":50449},{\"end\":50469,\"start\":50462},{\"end\":50482,\"start\":50469},{\"end\":50493,\"start\":50482},{\"end\":50504,\"start\":50493},{\"end\":50515,\"start\":50504},{\"end\":50524,\"start\":50515},{\"end\":50541,\"start\":50524},{\"end\":50872,\"start\":50860},{\"end\":50883,\"start\":50872},{\"end\":50894,\"start\":50883},{\"end\":50909,\"start\":50894},{\"end\":51233,\"start\":51221},{\"end\":51251,\"start\":51233},{\"end\":51266,\"start\":51251},{\"end\":51280,\"start\":51266},{\"end\":51293,\"start\":51280},{\"end\":51305,\"start\":51293},{\"end\":51317,\"start\":51305},{\"end\":51686,\"start\":51679},{\"end\":51699,\"start\":51686},{\"end\":51710,\"start\":51699},{\"end\":51723,\"start\":51710},{\"end\":51740,\"start\":51723},{\"end\":52019,\"start\":52008},{\"end\":52031,\"start\":52019},{\"end\":52045,\"start\":52031},{\"end\":52061,\"start\":52045},{\"end\":52072,\"start\":52061},{\"end\":52083,\"start\":52072},{\"end\":52375,\"start\":52364},{\"end\":52390,\"start\":52375},{\"end\":52404,\"start\":52390},{\"end\":52418,\"start\":52404},{\"end\":52740,\"start\":52729},{\"end\":52754,\"start\":52740},{\"end\":52765,\"start\":52754},{\"end\":53006,\"start\":52993},{\"end\":53018,\"start\":53006},{\"end\":53026,\"start\":53018},{\"end\":53039,\"start\":53026},{\"end\":53053,\"start\":53039},{\"end\":53061,\"start\":53053},{\"end\":53301,\"start\":53288},{\"end\":53314,\"start\":53301},{\"end\":53323,\"start\":53314},{\"end\":53337,\"start\":53323},{\"end\":53345,\"start\":53337},{\"end\":53554,\"start\":53542},{\"end\":53565,\"start\":53554},{\"end\":53578,\"start\":53565},{\"end\":53589,\"start\":53578},{\"end\":53603,\"start\":53589},{\"end\":53619,\"start\":53603},{\"end\":53637,\"start\":53619},{\"end\":53977,\"start\":53964},{\"end\":53995,\"start\":53977}]", "bib_venue": "[{\"end\":36006,\"start\":36001},{\"end\":36256,\"start\":36249},{\"end\":36549,\"start\":36541},{\"end\":36790,\"start\":36786},{\"end\":37025,\"start\":37021},{\"end\":37348,\"start\":37344},{\"end\":37646,\"start\":37642},{\"end\":37922,\"start\":37918},{\"end\":38192,\"start\":38188},{\"end\":38460,\"start\":38450},{\"end\":38793,\"start\":38756},{\"end\":39116,\"start\":39111},{\"end\":39321,\"start\":39317},{\"end\":39587,\"start\":39580},{\"end\":39934,\"start\":39927},{\"end\":40223,\"start\":40219},{\"end\":40459,\"start\":40455},{\"end\":40742,\"start\":40738},{\"end\":41072,\"start\":41067},{\"end\":41371,\"start\":41367},{\"end\":41631,\"start\":41627},{\"end\":41885,\"start\":41881},{\"end\":42093,\"start\":42089},{\"end\":42326,\"start\":42322},{\"end\":42731,\"start\":42727},{\"end\":43252,\"start\":43248},{\"end\":43665,\"start\":43660},{\"end\":43874,\"start\":43825},{\"end\":44213,\"start\":44151},{\"end\":44509,\"start\":44499},{\"end\":44845,\"start\":44783},{\"end\":45289,\"start\":45283},{\"end\":45563,\"start\":45550},{\"end\":45800,\"start\":45796},{\"end\":46015,\"start\":45990},{\"end\":46309,\"start\":46305},{\"end\":46634,\"start\":46630},{\"end\":47023,\"start\":46937},{\"end\":47404,\"start\":47400},{\"end\":47682,\"start\":47672},{\"end\":48052,\"start\":48048},{\"end\":48419,\"start\":48361},{\"end\":48658,\"start\":48654},{\"end\":48998,\"start\":48993},{\"end\":49417,\"start\":49413},{\"end\":49731,\"start\":49727},{\"end\":50210,\"start\":50206},{\"end\":50546,\"start\":50541},{\"end\":50919,\"start\":50909},{\"end\":51348,\"start\":51317},{\"end\":51677,\"start\":51610},{\"end\":52091,\"start\":52083},{\"end\":52362,\"start\":52285},{\"end\":52769,\"start\":52765},{\"end\":53068,\"start\":53061},{\"end\":53349,\"start\":53345},{\"end\":53677,\"start\":53637},{\"end\":53999,\"start\":53995}]"}}}, "year": 2023, "month": 12, "day": 17}