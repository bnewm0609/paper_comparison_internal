{"id": 228372945, "updated": "2023-10-06 08:16:57.971", "metadata": {"title": "Coded sparse matrix computation schemes that leverage partial stragglers", "authors": "[{\"first\":\"Anindya\",\"last\":\"Das\",\"middle\":[\"Bijoy\"]},{\"first\":\"Aditya\",\"last\":\"Ramamoorthy\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 12, "day": 11}, "abstract": "Distributed matrix computations over large clusters can suffer from the problem of slow or failed worker nodes (called stragglers) which can dominate the overall job execution time. Coded computation utilizes concepts from erasure coding to mitigate the effect of stragglers by running 'coded' copies of tasks comprising a job; stragglers are typically treated as erasures. While this is useful, there are issues with applying, e.g., MDS codes in a straightforward manner. Several practical matrix computation scenarios involve sparse matrices. MDS codes typically require dense linear combinations of submatrices of the original matrices which destroy their inherent sparsity. This is problematic as it results in significantly higher worker computation times. Moreover, treating slow nodes as erasures ignores the potentially useful partial computations performed by them. Furthermore, some MDS techniques also suffer from significant numerical stability issues. In this work we present schemes that allow us to leverage partial computation by stragglers while imposing constraints on the level of coding that is required in generating the encoded submatrices. This significantly reduces the worker computation time as compared to previous approaches and results in improved numerical stability in the decoding process. Exhaustive numerical experiments on Amazon Web Services (AWS) clusters support our findings.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2012.06065", "mag": "3111337301", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tit/DasR22", "doi": "10.1109/tit.2022.3152827"}}, "content": {"source": {"pdf_hash": "1479f10e6360b8625201e35bebb8f9d8066aa8e9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2012.06065v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2012.06065", "status": "GREEN"}}, "grobid": {"id": "628ad06d6c4e193340ba58d2aa546492fadc09e2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1479f10e6360b8625201e35bebb8f9d8066aa8e9.txt", "contents": "\nCoded sparse matrix computation schemes that leverage partial stragglers\n24 Sep 2021\n\nAnindya Bijoy Das \nDepartment of Electrical and Computer Engineering\nIowa State University\n50011AmesIAUSA\n\nAditya Ramamoorthy adityar@iastate.edu \nDepartment of Electrical and Computer Engineering\nIowa State University\n50011AmesIAUSA\n\nCoded sparse matrix computation schemes that leverage partial stragglers\n24 Sep 2021arXiv:2012.06065v2 [cs.IT] 1Index Terms Distributed computingMDS CodeStragglersCondition NumberSparsity\nDistributed matrix computations over large clusters can suffer from the problem of slow or failed worker nodes (called stragglers) which can dominate the overall job execution time. Coded computation utilizes concepts from erasure coding to mitigate the effect of stragglers by running \"coded\" copies of tasks comprising a job; stragglers are typically treated as erasures. While this is useful, there are issues with applying, e.g., MDS codes in a straightforward manner. Several practical matrix computation scenarios involve sparse matrices. MDS codes typically require dense linear combinations of submatrices of the original matrices which destroy their inherent sparsity. This is problematic as it results in significantly higher worker computation times. Moreover, treating slow nodes as erasures ignores the potentially useful partial computations performed by them. Furthermore, some MDS techniques also suffer from significant numerical stability issues. In this work we present schemes that allow us to leverage partial computation by stragglers while imposing constraints on the level of coding that is required in generating the encoded submatrices.This significantly reduces the worker computation time as compared to previous approaches and results in improved numerical stability in the decoding process. Exhaustive numerical experiments on Amazon Web Services (AWS) clusters support our findings.\n\nI. INTRODUCTION\n\nDistributed computation plays a major role in several problems in machine learning. For example, large scale matrix-vector multiplication is repeatedly used in gradient descent which in turn plays a key role in high dimensional machine learning problems. The size of the underlying matrices makes it impractical to perform the computation on a single computer (both from a speed and a storage perspective). Thus, the computation is typically subdivided into smaller tasks that are run in parallel across multiple worker nodes. In these systems the overall execution time is typically dominated by the speed of the slowest worker. Thus, the presence of stragglers (as slow or failed workers are called) can negatively impact the performance of distributed computation. In recent years, techniques from coding theory (especially maximum-distance-separable (MDS) codes) [1], [2], [3], [4] have been used to mitigate the effect of stragglers for problems such as matrix-vector and matrixmatrix multiplication. For instance, the work of [1] proposes to partition the computation of A T x by first splitting A = [A 0 | A 1 ] into two block-columns (with an equal number of column vectors) and assigning three workers, the task of computing A T 0 x, A T 1 x and (A 0 + A 1 ) T x, respectively. Evidently, the computational load on each node is half of the original job. Furthermore, it is easy to see that A T x can be recovered as soon as any two workers complete their tasks (with some minimal post-processing). Thus, this system is resilient to one straggler. The work of [3], poses the multiplication of two matrices in a form that is roughly equivalent to a Reed-Solomon code. In particular, each worker node's task (which is multiplying smaller submatrices) can be imagined as a coded symbol.\n\nAs long as enough tasks are complete, the master node can recover the matrix product by polynomial interpolation.\n\nFor such coded computing systems we can define a so-called recovery threshold. It is the minimum value of \u03c4 , such that the master node can recover the result as long as any \u03c4 workers complete their tasks. Thus, at the top level, in these systems stragglers are treated as the equivalent of erasures in coding theory, i.e., the assumption is that no useful information can be obtained from the stragglers.\n\nWhile these are interesting ideas, there are certain issues that are ignored in the majority of prior work (see [5], [6], [7], [8], [9] for some exceptions). Firstly, several practical cases of matrix-vector or matrix-matrix multiplication involve sparse matrices. Using MDS coding strategies in a straightforward manner will often destroy the sparsity of the matrices being processed by the worker nodes. In fact, as noted in [7], this can cause the overall job execution time to actually go up rather than down. Secondly, in the distributed computation setting, we make the observation that it is possible to leverage partial computations that are performed by the stragglers. Thus, a slow worker may not necessarily be a useless worker. Fig. 1 (which also appears in [10]) shows the variation of speed of different t2.micro machines in AWS (Amazon Web Services) cluster, and it can be seen that for a particular job, even the slowest worker node may have approximately 60% \u2212 70% of the speed of the fastest worker.\n\nIn this work we propose schemes which are not only resilient to full stragglers, but can also exploit slow workers by utilizing their partially finished tasks. The works in [11] and [12] also address this issue but they are applicable only for matrix-vector multiplication whereas in this work, we propose schemes for matrix-matrix multiplication too. Furthermore, in several of our schemes we can specify the number of block-columns of the individual A and B matrices that are linearly combined to arrive at the encoded matrices. This is especially useful in the case of sparse matrices (A and B) that often appear in practical settings. Thus, in short, our proposed approaches can leverage the partial computations of the stragglers and exploit the sparsity of the input matrices, both of which can enhance the overall speed of the whole system. This paper is organized as follows. Section II describes the background and related work and summarizes the contributions of our work. Section III outlines some basic definitions and observations which are required for the subsequent presentation. Section IV discusses our proposed \u03b2-level coding schemes which constrain the level of coding in the encoded submatrices while leveraging partial computations. Following this, Section V proposes schemes for both matrix-vector and matrix-matrix multiplication which can be optimal in terms of resilience to full stragglers and can improve the utilization of the partial stragglers. Section VI discusses the experimental performance of our proposed methods and shows the comparison with other available approaches. We conclude the paper with a discussion about future work in Section VII.\n\n\nII. BACKGROUND AND RELATED WORK\n\nConsider the case where a master node has a matrix A and either a matrix B or a vector x and needs to compute either A T B or A T x. The computation needs to be carried out in a distributed fashion over n worker nodes. Each worker receives the equivalent of a certain fraction (denoted by \u03b3 A and \u03b3 B , respectively) of the columns of A and B\n\nor the whole vector x. The node is responsible for computing its assigned submatrix-submatrix or submatrix-vector products.\n\nWe discuss the matrix-matrix scenario below where each worker node receives coded versions of submatrices of A and B respectively 1 . The corresponding matrix-vector case can be obtained as a special case. Consider a p \u00d7 u and p \u00d7 v block decomposition of A and B respectively as shown below. The master node creates coded submatrices by computing appropriate scalar linear combinations of the A i,j submatrices and respectively the B i,j submatrices. This implies that the master node only performs scalar multiplications and additions. It is not responsible for any of the computationally intensive matrix operations. Following this, it sends the corresponding coded submatrices to each of the workers who perform the matrix operations.\n\nIn this work we only consider a decomposition of A and B into block-columns, i.e., p = 1. We assume that the storage fraction \u03b3 A (or \u03b3 B ) can be expressed as \u2113 A /\u2206 A (likewise \u2113 B /\u2206 B ) where both \u2113 A and \u2206 A (and \u2113 B and \u2206 B ) are integers. We assume that A and B are large enough and satisfy divisibility constraints so that we can choose any large enough value of \u2206 A and \u2206 B to partition the columns of A and B into \u2206 A and \u2206 B block-columns.\n\nThese are denoted as A 0 , A 1 , . . . , A \u2206A\u22121 and B 0 , B 1 , . . . , B \u2206B \u22121 . Each node is assigned the equivalent of \u2113 A block-columns of A and \u2113 B block-columns of B. Each of those \u2113 A block-columns from A will be multiplied with each of the \u2113 B block-columns from B, so a particular worker node will compute, in total, \u2113 = \u2113 A \u2113 B block-products for matrix-matrix multiplication. In case of matrix-vector multiplication, the worker node will compute \u2113 = \u2113 A block products, where each of \u2113 A blocks from A will be multiplied with x.\n\nThe assignment can simply be subsets of {A 0 , A 1 , . . . , A \u2206A\u22121 } or {B 0 , B 1 , . . . , B \u2206B \u22121 }; in this case we call the solution \"uncoded\". Alternatively, the assignment can be suitably chosen functions of {A 0 , A 1 , . . . , A \u2206A\u22121 } or {B 0 , B 1 , . . . , B \u2206B \u22121 }; in this case we call the solution \"coded\". The assignment also specifies a sequential order from top to bottom in which each worker node needs to process its tasks. This implies that if a node is currently processing the i-th assignment (0 \u2264 i \u2264 \u2113 \u2212 1), then it has already processed assignments 0 through i \u2212 1. In this work, we assume that each time a node computes a product, it transmits the result to the master node. As we shall show, the processing order matters in this problem.\n\nThere are two requirements that our system needs to have. The master node should be able to decode the intended result (A T B or A T x) from any n \u2212 s workers for s as large as possible. i.e., n \u2212 s is the recovery threshold of the scheme [3]. The second requirement is that the master node should be able to recover A T B or\n\nA T x as long as it receives any Q products from the worker nodes. This formulation subsumes treating stragglers as non-working nodes. To our best knowledge, this second requirement has not been examined systematically within the coded computation literature, even though it is a natural constraint that allows for succinct treatment of recovery in distributed computing clusters where the workers have differing speeds.\n\nExample 1. Consider a system with n = 3 worker nodes with \u03b3 A = 2/3. We partition A into \u2206 A = 3 blockcolumns and the assignment of block-columns to each node is shown in Fig. 2 (this is an uncoded solution). We emphasize that the order of the computation also matters here, i.e., worker node W 0 (for example) computes A T 0 x first and then A T 1 x. For the specific assignment it is clear that the computation is successful as long as any four block products are returned by the workers. Thus, for this system Q = 4.\n\nOn the other hand, Fig. 3 demonstrates a coded solution, where the bottom assignment in the workers are some suitably chosen functions of the elements of {A T 0 x, A T 1 x, A T 2 x}. For this assignment, it is obvious that the master node can recover A T x as long as any three block products are returned by the workers, so in this system Q = 3.\n\nFor any time t, we let w i (t) represent the state of computation of the i-th worker node, i.e., w i (t) is a non-negative\nW 0 W 1 W 2 A T 0 x A T 1 x A T 1 x A T 2 x A T 2 x A T 0 x Fig. 2: Matrix A is partitioned into three submatrices.\nEach worker is assigned two of those uncoded submatrices. Here\nQ = 4. W 0 W 1 W 2 A T 0 x (A 1 + A 2 ) T x A T 1 x (A 2 + A 0 ) T x A T 2 x (A 0 + A 1 ) T x Fig. 3: Matrix A is partitioned into three submatrices.\nEach worker is assigned one uncoded and one coded task. Here Q = 3.\n\ninteger such that 0 \u2264 w i (t) \u2264 \u2113 which represents the number of tasks that have been processed by worker node i. Thus, our system requirement states as long as n\u22121 i=0 w i (t) \u2265 Q, the master node should be able to determine A T B or A T x. As \u2206, the number of unknowns to be recovered, is a parameter that can be chosen, our objective is to minimize the value of Q/\u2206 for such a system. For matrix-vector multiplication, \u2206 = \u2206 A , whereas for matrixmatrix multiplication, \u2206 = \u2206 A \u2206 B . This formulation minimizes the worst case overall computation performed by the worker nodes.\n\n\nA. Related Work\n\nSeveral coded computation schemes have been proposed for matrix multiplication [1], [13], [3], [2], [14], [6], [7], [11], [15], [5], most of which are designed to mitigate the full stragglers; see [16] for a tutorial overview. We illustrate the basic idea below using the polynomial code approach of [3] for a system with n = 5 workers where each of these worker nodes can store \u03b3 A = 1 2 fraction of matrix A and \u03b3 B = 1 2 fraction of matrix B. Consider u = v = 2 and p = 1, thus we partition both A and B into two block-columns A 0 , A 1 and B 0 , B 1 respectively.\n\nNext, we define two matrix polynomials as\nA(z) = A 0 + A 1 z and B(z) = B 0 + B 1 z 2 ; so A T (z)B(z) = A T 0 B 0 + A T 1 B 0 z + A T 0 B 1 z 2 + A T 1 B 1 z 3 .\nThe master node evaluates these polynomial A(z) and B(z) at distinct real values z 0 , z 1 , . . . , z n\u22121 , and sends the corresponding matrices to worker node W i . Each worker node computes the product of its assigned submatrices.\n\nIt follows that decoding at the master node is equivalent to decoding a degree-3 real-valued polynomial. Thus, the master node can recover A T B as soon as it receives the results from any four workers, i.e., in this example, the recovery threshold is, \u03c4 = 4. When \u03b3 A = 1/k A and \u03b3 B = 1/k B and p = 1, the work of [13] shows that their scheme has a threshold \u03c4 = k A k B which is optimal. Random coding solutions for this problem were investigated in [17]. Approaches based on convolutional coding were presented in [10], [18]. In these schemes (analogous to linear block codes) there are systematic workers that only contain uncoded assignments and parity workers that contain coded assignments.\n\nThe case when p > 1 was considered in the work of [2], [14], [13], [15]. Structuring the computation in this manner increases the computational load on the workers and the communication load from the workers to the master node but can reduce the recovery threshold as compared to the case of p = 1.\n\nIt is well-recognized that in several practical situations the underlying matrices A and B are sparse. Computing the inner product a T x of n-length vectors a and x where a has around \u03b4n (0 < \u03b4 \u226a 1) non-zero entries takes \u2248 2\u03b4n floating point operations (flops) as compared to \u2248 2n flops in the dense case. In general, the encoding process within coded computation increases the number of non-zero entries in the resultant encoded matrices. For instance, polynomial evaluations of degree d will increase the number of non-zero entries by approximately d times.\n\nThis results in a d-fold increase in the worker computation times which can be unacceptably high. Thus, it is important to consider schemes where the encoding only combines a limited number of submatrices.\n\nExample 2. Consider an example with two large sparse matrices A and B both of whose sizes are 10, 000\u00d710, 000.\n\nBoth of them have sparsity \u03c3 = 3%, i.e., randomly chosen approximately 3% entries of A and B are non-zero (we have used MATLAB command sprand for this example). We partition matrices A and B into 4 and 5 block-columns, respectively. First we choose a block-column A i and a block-column B j , and next we obtain two coded submatrices A i andB j which are random linear combinations of the uncoded block-columns of A and B, respectively. Table I shows that it is around 4 times more expensive to compute the coded product than the uncoded product, although the sizes of the corresponding matrices are exactly the same. The reason is that the number of non-zero entries in the coded submatrices have gone up significantly.\n\nAn important aspect of coded computation is \"numerical stability\" of the recovered result. Indeed, while coded computation borrows techniques from classical coding theory (over finite fields), it differs in the sense that the coded submatrices and the decoding operates over the reals. Over finite fields, the invertibility of a matrix is sufficient to solve a system of equations. In contrast, over the reals if the corresponding matrix is ill-conditioned, then the recovery will in general be inaccurate. It is well-recognized that real Vandermonde matrices corresponding to polynomial interpolation have condition numbers that grow exponentially in the matrix sizes. This is a serious issue with the polynomial-based approaches of [3], [19]. There have been some works that have addressed these issues [10], [12], [17], [20], [21], [22], [23] in part.\n\nYet another feature of the coded computation problem that distinguishes it from classical codes is the processing order. The worker nodes process the assigned tasks in a specific order, such that if a worker node is processing a given task, it has already completed the previously assigned tasks. Thus, at any given time the pattern of tasks that have been completed is restricted. Interestingly, codes for such systems have been investigated in [24], [25]. These ideas were adapted for the distributed matrix-vector multiplication problem in [12].\n\nWe note here that in principle using polynomial approaches can allow us to address both the optimal threshold and the optimal Q/\u2206 = 1 by simply placing multiple evaluations of the polynomials at distinct points within each worker node. However, this approach is not practical, firstly because of numerical stability issues. Secondly, as discussed above when considering sparse A and B matrices, the polynomial approaches result in dense coded submatrices which can cause an unacceptable increase in the worker node computation times. Numerical experiments supporting these conclusions can be found in Section VI.\n\n\nB. Summary of Contributions\n\nThe contributions of our work can be summarized as follows.\n\n\u2022 We present a fine-grained model of the distributed matrix-vector and matrix-matrix multiplication that allows us to (i) leverage the slower workers using their partial computations and (ii) impose constraints on what extent coding is allowed in the solution. This allows us to capture a scenario where workers have differing speeds and the intended result can be recovered as long as the workers together complete a minimum number (Q) of the assigned tasks. This applies to the practically important case where the underlying matrices are sparse.\n\nThe formulation leads to new questions within coded computing that to our best knowledge have not been investigated before systematically within the coded computing literature.\n\n\u2022 We present systematic methods for both matrix-vector and matrix-matrix multiplication that address both the recovery threshold and the Q/\u2206 metric. For the uncoded assignment case, we present a lower bound on the performance of any scheme that our constructions are able to match.\n\n\u2022 We have proposed two different schemes for distributed computations, first of which is named as \u03b2-level coding.\n\nIn this approach, we have used resolvable combinatorial designs [26] to improve the recovery threshold and the Q/\u2206 metric over the uncoded approach. We have shown that the metrics can be further improved if we utilize certain relations among the blocks of different parallel classes within the resolvable designs. \nAND \u03b3B = b 1 b 2 , ASSIGNED SUBMATRICES CODING OF A (OR B) WE NEED n = ca2b2 ARE SPARSE SCS OPTIMAL MAJORITY OF ASSIGNED FOR \u03b3A = 1 k A AND \u03b3B = 1 k B , OPTIMAL RECOVERY SCHEME SUBMATRICES ARE UNCODED WHERE kA AND kB ARE INTEGERS THRESHOLD (kAkB )\n\u2022 Prior work has demonstrated schemes with the optimal recovery threshold for certain storage fractions. In this work we present novel schemes that retain the optimal recovery threshold and also have low Q/\u2206 values.\n\n\u2022 Finally, we present exhaustive experimental comparisons that demonstrate the benefit of our schemes while considering sparse matrices in terms of worker node computation times and numerical stability.\n\nIn Table II, we present a summary of the properties and the advantages of both of our proposed approaches, \u03b2-level coding and sparsely coded straggler (SCS) optimal scheme. Moreover, a detailed comparison of the properties of our methods with other available schemes is demonstrated in Table III.\n\nIt should be noted that there are other issues within coded matrix computations. Several works [27], [28] have considered the issue of private computation along with the straggler mitigation issue. Here the goal is that no information about the matrices A or B can be obtained from any set of at most m workers. Another class of codes [29], [30] assumes the workers to be heterogeneous and time-varying, so that the system may have access to different number of workers at different moments where the workers may have different speeds and/or different storage capacities. These issues are out of the scope of this paper.\n\n\nIII. PRELIMINARIES\n\nIn this section we discuss some basic facts and observations that serve to explain our proposed distributed matrix computation schemes. Suppose that a given worker node is assigned encoded block-columns\u00c3 i , i = 0, 1, . . . , \u2113 A \u2212 1 andB j , j = 0, 1, . . . , \u2113 B \u2212 1. The assignment also specifies a top to bottom order. For the matrixvector problem, the node processes them simply in the order\u00c3 T 0 x,\u00c3 T 1 x, . . . ,\u00c3 T \u2113A\u22121 x. On the other hand for the matrix-matrix problem the node computes in the order\u00c3 T 0B0 ,\u00c3 T 0B1 , . . . ,\n\u00c3 T 0B\u2113B \u22121 ,\u00c3 T 1B0 , . . . ,\u00c3 T 1B\u2113B \u22121 , . . . ,\u00c3 T \u2113A\u22121B 0 , . . . ,\u00c3 T \u2113A\u22121B \u2113B \u22121 .\nDefinition 1. A coding scheme for distributed matrix computation is said to be a \u03b2-level coding scheme if the assigned block-columns are a linear combination of exactly \u03b2 block-columns of A and B. The case of \u03b2 = 1\n\nrepresents an uncoded scheme.\n\nOur constructions leverage the properties of combinatorial structures known as resolvable designs [26]. \n\u2713 \u2713 \u2713 \u2717 \u2717 C 3 LES [11] \u2717 \u2717 \u2713 \u2713 \u2713 \u03b2-level Coding (proposed) \u2713 \u2717 \u2713 \u2713 \u2713 SCS Optimal Scheme (proposed) \u2713 \u2713 \u2713 \u2713 \u2713 Definition 2. A resolvable design is a pair (X , A)\nwhere X is a set of elements (called points) and A is a family of non-empty subsets of X (called blocks) that have the same cardinality.\nA subset P \u2282 A in a design (X , A) is called a parallel class if \u222a {i:Ai\u2208P} A i = X and if A i \u2229 A j = \u2205 for A i , A j \u2208 P when i = j. A partition of A\ninto several parallel classes is called a resolution and (X , A) is said to be a resolvable design if A has at least one resolution [26].\n\nA resolvable design always exists if the cardinality of a block divides |X |. We note that the specification of the \"incidence relations\" between the points and blocks of a design can also be shown by means of an incidence matrix.\n\nDefinition 3. The incidence matrix N of a design (X , A) is a |X | \u00d7 |A| binary matrix such that the (i, j)-th entry is a 1 if the i-th point is a member of the j-th block and zero, otherwise.\n\nFor example, the incidence matrix for the resolvable design in Example 3 is given by \nN = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb .\nWe will use a cyclic assignment of tasks extensively in our constructions. We illustrate this by means of the following matrix-vector multiplication example.  If we do not incorporate any coding among the block-columns, then for \u03b2 = 1, we have the trivial parallel class Fig. 4 shows a cyclic assignment of jobs where three uncoded submatrices are allocated to each of the workers in a cyclic fashion according to the indices of three elements of P. It can be easily verified that the system is resilient to s = 2 stragglers. In the sequel, our assignment can be coded as well.\nW 0 W 1 W 2 W 3 W 4 A T 0 x A T 1 x A T 2 x A T 1 x A T 2 x A T 3 x A T 2 x A T 3 x A T 4 x A T 3 x A T 4 x A T 0 x A T 4 x A T 0 x A T 1 xP = {{0}, {1}, . . . , {4}}.\nMore generally, suppose that we have \u2206 symbols denoted 0, . . . , \u2206 \u2212 1, n = \u2206 worker nodes and \u2113 symbols to be placed in each worker node where \u2113 \u2264 \u2206. The symbols can be encoded block-columns of A or the product of encoded block-columns of A and B. A cyclic assignment in this case assigns the set {j, j + 1, . . . , j + \u2113 \u2212 1} (mod \u2206) to worker W j ; symbol j appears at the top and sequentially symbol (j + \u2113 \u2212 1) (the values are reduced modulo \u2206) at the bottom. The node W j processes the tasks specified by the symbols from top to bottom. Within a node, the position of a symbol is denoted by an integer between 0 and \u2113 \u2212 1, where 0 denotes the top and \u2113 \u2212 1 denotes the bottom.\n\nLemma 1. The cyclic assignment satisfies the following properties.\n\n\u2022 Each symbol appears \u2113 times across n worker nodes. Furthermore, it appears in each position 0, . . . , \u2113 \u2212 1 exactly once, across all n workers.\n\n\u2022 Let \u03b1 c be the maximum number of symbols that can be processed across all worker nodes such that a specific symbol j is processed exactly c times (where 0 \u2264 c \u2264 \u2113). Then, \u03b1 c = \u2206\u2113 \u2212 \u2113(\u2113+1)\n2 + c\u22121 i=0 (\u2113 \u2212 i), independent of j.\nProof. The first claim follows since \u2113 \u2264 \u2206 = n and symbol j, where 0 \u2264 j \u2264 \u2206 \u2212 1, appears in workers j, j \u2212 1, j \u2212 2, . . . , j \u2212 \u2113 + 1 (indices reduced modulo-\u2206).\n\nFor the second claim we proceed by contradiction. Suppose that there is a symbol j for which the condition is violated. From part (a), symbol j appears once in positions 0, . . . , \u2113 \u2212 1 across the workers. Thus, one can process\nat most (\u2206 \u2212 \u2113)\u2113 + \u2113\u22121 i=0 i = \u2206\u2113 \u2212 \u2113(\u2113+1) 2\nsymbols without processing any copy of j. Following this, any symbol processed will necessarily process symbol j. If we process the copy of j at position i, we can process another \u2113 \u2212 1 \u2212 i symbols without processing another copy of j. Therefore, the maximum number of symbols that can be processed such that c copies of j are processed are \u2206\u2113 \u2212 \u2113(\u2113+1)\n2 + c\u22121 i=0 (\u2113 \u2212 i).\n\nIV. \u03b2-LEVEL CODING FOR DISTRIBUTED COMPUTATIONS\n\nWe begin our discussion of \u03b2-level coding by considering the uncoded \u03b2 = 1 case. In this scenario, the assignments are simply elements such as A T i x (in the matrix-vector case) or elements such as A T i B j (in the matrix-matrix case). In the discussion below we refer the assignment of \"symbols\" to treat both cases together, where a symbol can either be of the form\nA T i x or A T i B j .\nNote that we can disregard the case when multiple copies of a symbol appear within the same worker node. Consider a n, \u2113, \u2206, r -uncoded system with n workers each of which can process \u2113 \u2265 1 symbols out of a total of \u2206 symbols. We assume that each symbol appears r times across the different worker nodes, so n\u2113 = \u2206r. Now we show a lower bound on the value of Q for such a system. Theorem 1. For a n, \u2113, \u2206, r -uncoded system we have Q \u2265 \u2206r \u2212 r 2 (\u2113 + 1) + 1.\n\nProof. For the system under consideration, let Q j represent the maximum number of symbols that are processed in the worst case without processing symbol j (see Fig. 4 for an example). It is evident in this case that Q = max j=0,...,\u2206\u22121 Q j + 1.\n\nOur strategy is to calculate the average Q = 1 \u2206 \u2206\u22121 j=0 Q j and use the simple bound Q \u2265 Q + 1. Toward this end, note that for any uncoded solution, we can calculate \u2206\u22121 j=0 Q j in a different way. For any worker i, there are \u2113 assigned block-columns and the other \u2206 \u2212 \u2113 do not appear in it. Thus, in the calculation of\n\u2206\u22121 j=0 Q j , worker node i contributes (\u2206 \u2212 \u2113)\u2113 + \u2113 k=1 (k \u2212 1) symbols,\nwhich is clearly independent of i. Therefore,\nQ = n \u2113 k=1 (k \u2212 1) + (\u2206 \u2212 \u2113)\u2113 \u2206 = n\u2113 \u2212 n\u2113 2\u2206 (\u2113 + 1).\nThus, we have the lower bound as\nQ \u2265 \u2206r \u2212 r 2 (\u2113 + 1) + 1(1)\nsince n\u2113 = \u2206r.\n\nRemark 1. In general, we are given the number of workers n and the storage fraction \u03b3. The parameters \u2206 and \u2113 can be treated as design parameters. In this setting, from (1), we have\nQ \u2206 \u2265 r \u2212 r 2 \u2113 \u2206 \u2212 r 2\u2206 + 1 \u2206 ; but r = n\u03b3, thus Q \u2206 \u2265 n\u03b3 1 \u2212 \u03b3 2 + 1 \u2212 n\u03b3 2 1 \u2206 . (2) W 0 W 1 W 2 W 3 {0, 1, 2} {3, 4, 5} {6, 7, 8} {3, 4, 5} {6, 7, 8} {9, 10, 11} {6, 7, 8} {9, 10, 11} {0, 1, 2} {9, 10, 11} {0, 1, 2}\n{3, 4, 5} If r = n\u03b3 > 2, then the second term in the RHS above is negative and has an inverse dependence on \u2206. (1) is met with equality when we consider the cyclic assignment scheme. For instance, Fig.   4 shows an example where \u2206 = n = 5, and it can be verified that Q = 10 and meets the lower bound in (1). A similar result holds for the matrix-matrix case. These results are discussed in the relevant parts of the remainder of this section.\n\n\nThe lower bound in\n\n\nA. Matrix-vector Multiplication\n\nWe consider a \u03b2-level coding matrix-vector scenario where the storage fraction \u03b3 = a 1 /a 2 for positive integers a 1 and a 2 with a 1 \u2264 a 2 such that \u03b3 \u2264 1 \u03b2 . We assume that the number of worker nodes n = ca 2 where c is a positive integer.\n\nWe partition A into \u2206 block-columns where \u2206 is divisible by \u03b2. Next, we pick a resolvable design (X , A)\n\nwhere X = {0, . . . , \u2206 \u2212 1}. The size of the blocks in A is \u03b2. Let P 1 , P 2 , . . . denote distinct parallel classes of this design. We will refer to the blocks of the design as meta-symbols (to avoid potential confusion with the term block-columns which we also have used extensively). Thus, the elements of a parallel class are meta-symbols.\n\nThe overall idea is to partition the set of worker nodes into c groups denoted G 0 , . . . , G c\u22121 . For each group we pick a parallel class and place meta-symbols from the parallel class in a cyclic fashion. The parallel classes for the different groups can be the same as well. For each meta-symbol, we generate a coded block-column by choosing a random linear combination of the \u03b2 block-columns within it. In the discussion below we refer to the block-columns as \"unknowns\" as they need to be decoded by the master nodes. A precise description appears in Algorithm 1. We illustrate it by means of an example below. 1 Set \u2206 = \u03b2a 2 . Partition A into \u2206 block-columns; 2 Number of assigned blocks per worker, \u2113 = \u2206\u03b3;\n\n3 Assume X = {0, 1, 2, . . . , \u2206 \u2212 1} and find c parallel classes P i having a block size \u03b2, i = 0, 1, . . . , c \u2212 1;\n4 for i \u2190 0 to c \u2212 1 do 5 Let the blocks of P i be denoted as p 0 , p 1 , . . . , p \u2206 \u03b2 \u22121 ; 6 for j \u2190 0 to \u2206 \u03b2 \u2212 1 do 7\nAssign meta-symbols p j , p j+1 , . . . , p j+\u2113\u22121 from top to bottom (indices reduced modulo a 2 ) and vector x to worker \u2206 \u03b2 i + j; 8 For each meta-symbol choose a random linear combination of length-\u03b2 of the constituent block-columns; 9 end 10 end\n\nOutput : Distributed matrix-vector multiplication scheme having \u03b2-level coding.\nA 0 = z 0 A 0 + z 1 A 1 + z 2 A 2\nwhere the z i 's are chosen at random. This implies that W 0 is responsible for computing A T 0 x, and the unknowns A T 0 x, A T 1 x and A T 2 x can be decoded if three copies of the meta-symbol {0, 1, 2} are obtained from the workers as the corresponding equations are linearly independent with probability 1.\n\nTheorem 2. Consider a distributed matrix-vector multiplication scheme for n = ca 2 workers where each worker can store \u03b3 = a1 a2 fraction of matrix A. Suppose that c \u2265 \u03b2 and we use the same parallel class P over all the worker groups. Then, the scheme described in Alg. 1 will be resilient to s = c\u2113 \u2212 \u03b2 stragglers, and Q =\nn\u2113 \u2212 c\u2113(\u2113+1) 2 + \u2113(\u03b2 \u2212 1) + 1.\nProof. Based on our construction we know that any meta-symbol \u2208 P will appear in \u2113 distinct workers in each worker group consisting of \u2206/\u03b2 = a 2 workers (cf. Lemma 1). Thus there are n a2 = c such worker groups and it follows that there are a total of c\u2113 appearances of that meta-symbol across all the worker nodes. Furthermore, each meta-symbol corresponds to a random linear combination of the corresponding unknowns (block-columns). As the choice of these random coefficients is made from a continuous distribution, as long as any \u03b2 meta-symbols are processed across all the worker nodes, the constituent unknowns will be decodable with probability 1. Thus, the scheme is resilient to the failure of any c\u2113 \u2212 \u03b2 stragglers.\n\nFor the second claim, suppose that there exists a meta-symbol \u22c6 \u2208 P that is processed at most \u03b2 \u2212 1 times when n\u2113 \u2212 c\u2113(\u2113+1) 2 + \u2113(\u03b2 \u2212 1) + 1 meta-symbols have been processed. For each worker group, the meta-symbol \u22c6 appears in all the positions 0, . . . , \u2113 \u2212 1. Suppose that \u22c6 appears i times in \u03b7 i worker groups for i = 1, . . . , y. Thus, y i=1 i\u03b7 i \u2264 \u03b2 \u2212 1 and the maximum number of meta-symbols that can be processed is\nQ \u2032 = y i=1 \u03b7 i \u03b1 i + (c \u2212 y i=1 \u03b7 i )\u03b1 0 where \u03b1 0 = \u2206 \u03b2 \u2113 \u2212 \u2113(\u2113+1) 2 and \u03b1 i = \u03b1 0 + i\u22121 j=0 (\u2113 \u2212 i) = \u03b1 0 + i\u2113 \u2212 i(i\u22121) 2\nas specified in Lemma 1 (by setting the number of symbols to \u2206/\u03b2). Thus, (3) if we have y = 1 and \u03b7 1 = \u03b2 \u2212 1. In the worst case therefore, we can process \u03b1 1 symbols from \u03b2 \u2212 1 groups and \u03b1 0 symbols from the remaining groups. This gives a total of\nQ \u2032 = c\u03b1 0 + \u2113 y i=1 i\u03b7 i \u2212 y i=1 \u03b7 i i(i \u2212 1) 2 \u2264 c\u03b1 0 + \u2113(\u03b2 \u2212 1) (3) since we have y i=1 i\u03b7 i \u2264 \u03b2 \u2212 1. Equality holds in(\u03b2 \u2212 1)\u03b1 1 + (c \u2212 \u03b2 + 1)\u03b1 0 = n\u2113 \u2212 c\u2113(\u2113 + 1) 2 + \u2113(\u03b2 \u2212 1)\nsymbols, which is the same as the upper bound in (3).\nThus if Q \u2265 n\u2113 \u2212 c\u2113(\u2113+1) 2 + \u2113(\u03b2 \u2212 1) + 1 then we are\nguaranteed that every meta-symbol is processed at least \u03b2 times. This concludes the proof.\n\nIt can be verified that the distributed matrix-vector multiplication scheme shown in Fig Remark 2. The proposed \u03b2-level coding scheme leads to an algorithm for uncoded matrix-vector multiplication when we set \u03b2 = 1 (see Fig. 4 for an example). The ratio Q/\u2206 for the construction in Alg. 1 is lower in general as compared to the scheme in [11]. For instance, with n = 10 and \u03b3 = 2/5, Alg. 1 results in a scheme with Q/\u2206 = 3.0, whereas the [11] scheme has Q/\u2206 = 3.1. The reduction is due to the lower value of \u2206 (cf. Remark 1).\n\n\nRemark 3.\n\nFor \u03b2 > 1 the Q/\u2206 ratio can be reduced significantly as compared to the uncoded (\u03b2 = 1) case. To see this consider n = ca 2 and \u03b3 = a1 a2 , where c \u2265 \u03b2. For the uncoded case, we set \u2206 unc = a 2 , and we have\nQ unc = n\u2113 \u2212 c\u2113(\u2113+1) 2 + 1 where \u2113 = a 1 .\nOn the other hand for \u03b2-level coding, we set \u2206 \u03b2 = \u03b2a 2 , and we have\nQ \u03b2 = n\u2113 \u2212 c\u2113(\u2113+1) 2 + \u2113(\u03b2 \u2212 1) + 1 where \u2113 = \u03b2a 1 . This implies that Q unc \u2206 unc \u2212 Q \u03b2 \u2206 \u03b2 = (\u03b2 \u2212 1) \u03b3 ca 1 2 \u2212 1 + 1 \u03b2a 2 > 0.\nIt turns out that the recovery threshold can be further reduced if we judiciously choose different parallel classes for the different worker groups in Alg. 1. Utilizing these parallel classes, we present a method that improves on Theorem 2 if we assume the property that the blocks among different parallel classes have intersection size to be at most one. Before stating the theorem, we discuss the decodabilty of the approach since this is not as straightforward as the single parallel class \u03b2-level coding.\n\nTo understand the decoding in this setting we consider a bipartite graph G dec = U \u222a V whose vertex set consists of the unknowns (U) on the left and the processed meta-symbols (V) on the right; an example is shown in Fig. u 0 Fig. 6: For the case \u03b2 = 2, every symbol is a random linear combination of two unknowns, thus there is a bipartite graph between the unknowns and symbols. 6. A meta-symbol is connected to its constituent unknowns. Note that G dec specifies a system of equations in \u2206 unknowns and we need to argue that this system is invertible. In the argument below, suppose that the random linear coefficients of each meta-symbol are indeterminates and we argue that there exists a matching in G dec where all the unknowns in U are matched.\nu 1 u 2 u 3 v 0 v 1 v 2 v 3 v 4 U Unknowns V Symbols\nConsider a set\u0168 of d unknowns from G dec and the corresponding neighborhood\u1e7c = N (\u0168). Suppose we have a set of equations where these d unknowns, namely u 0 , u 1 , . . . , u d\u22121 , participate in \u2113 0 , \u2113 1 , . . . , \u2113 d\u22121 equations. Thus the number of outgoing edges from\u0168 is d\u22121 i=0 \u2113 i . On the other hand, because of the structure of \u03b2-level coding approach, any symbol in\u1e7c has a degree \u03b2, thus the number of incoming edges in\u1e7c is \u03b2|\u1e7c|. Now, suppose that a matching where all the elements of U are matched does not exist. Hall's marriage theorem [33] gives a necessary and sufficient condition for the existence of the matching. Suppose that Hall's condition is violated for the set\u0168, i.e., |\u1e7c| \u2264 d \u2212 1. This means that\nd\u22121 i=0 \u2113 i \u2264 \u03b2(d \u2212 1).(4)\nLemma 2. Suppose that G dec is such that each of \u2206 unknowns has at least degree 1 and at least \u2206 \u2212 1 unknowns have degree at least \u03b2 each. Then, the master node can decode all the unknowns.\n\nProof. First, consider d = 1, so |\u0168| = 1. Since each unknown has at least degree 1, thus\nd\u22121 i=0 \u2113 i \u2265 1 > \u03b2(d \u2212 1)\n. Next we consider any set of d \u2265 2 unknowns, where we know that at least (d \u2212 1) unknowns have degree at least \u03b2. In that case,\nd\u22121 i=0 \u2113 i \u2265 1 + \u03b2(d \u2212 1) > \u03b2(d \u2212 1)\n. Thus (4) cannot be satisfied for any d \u2265 2. So, there exists a matching in G dec where all the unknowns are matched, hence the master node can decode all the unknowns. Now we state the result when different parallel classes are utilized.\n\nTheorem 3. Consider c distinct parallel classes (with block size \u03b2) such that the size of the intersection between any two blocks from different parallel classes is at most 1. Using Alg. 1, the distributed matrix-vector multiplication scheme can be resilient to at least c\u2113 \u2212 \u03b2 + \u03bb stragglers, when c \u2265 \u03b2 + \u03bb and \u03bb < \u03b2.\n\nProof. The main idea is to find the scenario where Lemma 2 can be directly applicable. To establish that we consider two unknowns u 0 and u 1 . The event that a pair of unknowns belong to the same meta-symbol can happen within at most one parallel class (in other words, within only one worker group) according to our choices of parallel classes. Thus in the remaining (c \u2212 1) worker groups, those two unknowns exist in different meta-symbols. If they appear in the same meta-symbol, then there are \u2113 workers within the worker group where they appear. On the other hand, if they appear in different meta-symbols, then there are at least \u2113 + 1 workers within the worker group where either u 0 or u 1 or both appear as part of a meta-symbol.\n\nSo, the unknowns u 0 or u 1 or both participate in different meta-symbols in at least in \u2113 + (\u2113 + 1)(c \u2212 1) workers. Now since we have c\u2113 \u2212 \u03b2 + \u03bb stragglers, using c \u2265 \u03b2 + \u03bb, we have still\n\u2113 + (\u2113 + 1)(c \u2212 1) \u2212 (c\u2113 \u2212 \u03b2 + \u03bb) \u2265 2\u03b2 \u2212 1\nworkers left. This means that either u 0 or u 1 or both exist in at least 2\u03b2 \u2212 1 workers after the stragglers are removed.\n\nThis in turn implies that the corresponding G dec has at least 2\u03b2 \u2212 1 edges emanating from the pair of unknowns u 0 and u 1 , so that at least one of them has degree \u2265 \u03b2. Thus Lemma 2 is satisfied and we can decode all the unknowns.\n\nExample 6. Consider a scenario with n = 20 workers with \u03b3 = 1 5 , thus c = 4, and we apply \u03b2-level coding approach with \u03b2 = 3. In this regard, we incorporate f our different parallel classes of block size \u03b2 = 3 obtained from the solution of the famous Kirkman's Schoolgirl problem [34], where any two blocks from any two different parallel classes have an intersection size at most one. It can be verified that the distributed matrix-vector multiplication scheme will be resilient to at least s = c\u2113 \u2212 \u03b2 + 1 = 10 stragglers whereas the number of stragglers if we used the single parallel class would have been 9. The analysis in Theorem 3 above is somewhat loose as we only assume that the intersection sizes between blocks from different parallel classes is at most 1. Indeed, exploiting more structure in the choice of the parallel classes can yield better results, though the analysis becomes significantly harder. Here we present a method that improves on Theorem 2 when c = \u03b2 = 2, \u2113 \u2264 \u2206/2 \u2212 2 and \u2206 \u2265 8. block size of the design is two and the parallel classes are given as follows.\nLet X = {0, 1, . . . , \u2206 \u2212 1} where \u2206 = n = 2a 2 . TheP 0 = {{0, 1}, {2, 3}, . . . , {\u2206 \u2212 2, \u2206 \u2212 1}} and P 1 = {{0, 5}, {2, 7}, . . . , {\u2206 \u2212 2, 3}} .(5)\nThus, the i-th block in P 0 and P 1 , for\n0 \u2264 i \u2264 \u2206/2 \u2212 1 is given by {2i, 2i + 1} and {2i, 2i + 5} (mod \u2206),\nrespectively. We follow the Alg. 1 for the specification of the coding scheme.\n\nTheorem 4. Let c = \u03b2 = 2, \u2113 \u2264 \u2206/2 \u2212 2 and \u2206 \u2265 8. If we use the parallel classes in (5), then the matrix-vector scheme described in Alg. 1 will be resilient to s = 2\u2113 \u2212 1 stragglers, and Q = n\u2113 \u2212 \u2113(\u2113 + 1) + 1.\n\nProof. The detailed proof is discussed in Appendix B\n\n\nB. Matrix-matrix Multiplication\n\nNow we consider the case of matrix-matrix multiplication, where we assume that each of the n worker nodes can store \u03b3 A = a1 a2 and \u03b3 B = b1 b2 fractions of matrices A and B. In this case, we consider \u03b2 A and \u03b2 B -level coding for A and B, respectively so that \u03b3 A \u2264 1 \u03b2A and \u03b3 B \u2264 1 \u03b2B . We partition matrices A and B into \u2206 A and \u2206 B block-columns, respectively, and so, we have, in total, \u2206 = \u2206 A \u2206 B unknowns. Next we assign \u2113 A = \u2206 A \u03b3 A block-columns from A and \u2113 B = \u2206 B \u03b3 B block-columns of B to each of the workers. Thus each worker computes \u2113 = \u2113 A \u2113 B submatrix products according to the natural order discussed in Section III.\n\nOnce the matrices are decomposed into block-columns, we allow \u03b2 A -level and \u03b2 B -level coding for matrices A and B, respectively. In this case we choose two separate resolvable designs with block sizes \u03b2 A and \u03b2 B supported on point sets {0, 1, . . . , \u2206 A \u2212 1} and {0, 1, . . . , \u2206 B \u2212 1} respectively. Furthermore, we assume that the number of worker nodes n = c \u00d7 a 2 b 2 where c is a positive integer.\n\nLet P A and P B denote parallel classes for the matrices A and B respectively. As in the matrix-vector scheme, the coding scheme is specified by the meta-symbols (blocks) of P A and P B . Let N A and N B denote the corresponding incidence matrices of these parallel classes. Recall that each meta-symbol is in one-to-one correspondence with Algorithm 2: \u03b2-level coding scheme for matrix-matrix multiplication Input : Matrices A and B, storage fractions of the workers \u03b3 A = a1 a2 \u03b3 B = b1 b2 , \u03b2 A , \u03b2 B -coding level for A and B, respectively, and number of worker nodes, n = c \u00d7 a 2 b 2 , where c is a positive integer. 1 Partition A into \u2206 A = \u03b2 A a 2 block-columns and partition B into \u2206 B = \u03b2 B b 2 block-columns; Output : Distributed matrix-matrix multiplication scheme having \u03b2-level coding.\n2 \u2206 = \u2206 A \u2206 B , \u2113 A = \u2206 A \u03b3 A , \u2113 B = \u2206 B \u03b3 B , \u03b2 = \u03b2 A \u03b2 B ; 3 Assume X A = {0,\nthe columns of the incidence matrices. Consider the matrix N AB formed by considering pair-wise Kronecker products of columns from N A and N B . Then the rows of N AB correspond to unknowns of the form A T i B j and the columns correspond to the support of the random linear equations that are formed by considering the pairwise products. We will refer to the meta-symbols of N AB as product meta-symbols and denote it by P AB .\n\nFor example, suppose that \u03b2 A = \u03b2 B = 2 and consider two meta-symbols {0, 1} \u2208 P A and {0, 1} \u2208 P B . If these symbols are placed in a worker, the corresponding product would be ( \nx 0 A T 0 + x 1 A T 1 )(y 0 B 0 + y 1 B 1 ) = x 0 y 0 A T 0 B 0 + x 0 y 1 A T 0 B 1 + x 1 y 0 A T 1 B 0 + x 1 y 1 A T 1 B 1 where x 0 , x 1 , y 0 ,[x 0 x 1 ] \u2297 [y 0 y 1 ](6)\nwhere \u2297 denotes the Kronecker product.\nClaim 1. If N A (of size \u2206 A \u00d7 \u2206 A /\u03b2 A ) and N B (of size \u2206 B \u00d7 \u2206 B /\u03b2 B )\ncorrespond to incidence matrices of parallel classes, then N AB also forms a parallel class of size\n\u2206 A \u2206 B \u00d7 \u2206 A \u2206 B /\u03b2 A \u03b2 B .\nProof. Let u i \u2297 v i for i = 0, 1 denote two distinct columns of N AB such that u i and v i are columns in N A and\nW 0 W 1 W 2 W 3 W 4 W 5 W 6 W 7 W 8 {0, 1} {2, 3} {0, 1} {2, 3} {2, 3} {4, 5} {0, 1} {2, 3} {4, 5} {0, 1} {0, 1} {2, 3} {0, 1} {2, 3} {2, 3} {4, 5} {2, 3} {4, 5} {2, 3} {4, 5} {4, 5} {0, 1} {2, 3} {4, 5} {0, 1} {2, 3} {4, 5} {0, 1} {2, 3} {4, 5} {4, 5} {0, 1} {4, 5} {0, 1} {4, 5}\n{0, 1} N B respectively. Then,\n(u 0 \u2297 v 0 ) T (u 1 \u2297 v 1 ) = u T 0 u 1 \u00d7 v T 0 v 1 = 0 since either u 0 = u 1 or v 0 = v 1 .\nMoreover, there are \u2206A\u2206B \u03b2A\u03b2B distinct columns in N AB each with a support of size \u03b2 A \u03b2 B . This implies that together all the product meta-symbols in N AB cover all the \u2206 A \u2206 B points.\n\nAs in the matrix-vector case, the scheme operates by placing cyclically shifted meta-symbols from P A with \u2113 A meta-symbols in each worker for the first \u2206 A /\u03b2 A workers. For these workers, the assignment of meta-symbols from P B is the same. For the next set of \u2206 A /\u03b2 A workers the assignment of meta-symbols from P A repeats; however, we now employ a cyclic shift for the assignment of meta-symbols from P B . The complete algorithm is specified in Alg.\n\n2 and an example is depicted in Fig. 7. As before, a group in this setting contains \u2206/\u03b2 workers and there a total of   If meta-symbols x \u2208 P A i and y \u2208 P B i appear at locations i 1 and j 1 , respectively, 0 \u2264 i 1 \u2264 \u2113 A \u2212 1 and 0 \u2264 j 1 \u2264 \u2113 B \u2212 1, then the product meta-symbol x \u2297 y appears at location i 1 \u2113 B + j 1 in the ordering. In our case, meta-symbol x appears \u2113 A times within subgroup H j at distinct locations 0, . . . , \u2113 A \u2212 1. Thus, if metasymbol y \u2208 P B i appears in H j at location j 1 then the product meta-symbol x \u2297 y appears \u2113 A times at locations j 1 , \u2113 B + j 1 , 2\u2113 B + j 1 , . . . , (\u2113 A \u2212 1)\u2113 B + j 1 . The result follows by realizing that there are \u2113 B subgroups where meta-symbol y appears. Moreover, y \u2208 P B i appears at all locations 0, . . . , \u2113 B \u2212 1 across these subgroups.\nn \u2206/\u03b2 = c groups denoted G i , i = 0, 1, . . . , c \u2212 1. Let X AB = {A T 0 B 0 , A T 0 B 1 , A T 0 B 2 , . . . , A T \u2206A\u22121 B \u2206B \u22121 } denote\nTheorem 5. If we use a single parallel class P A for A and a single parallel class P B for B across all the worker groups, then the scheme described in Alg. 2 will be resilient to s = c\u2113 \u2212 \u03b2 stragglers and will have,\nQ = n\u2113 \u2212 c\u2113(\u2113+1) 2 + \u2113(\u03b2 \u2212 1) + 1, where \u2113 = \u2113 A \u2113 B and \u03b2 = \u03b2 A \u03b2 B \u2264 c.\nProof. The proof is very similar to the proof of Theorem 2 once we use the fact that each product meta-symbol appears in all locations 0, . . . , \u2113 \u2212 1 within the group in which it appears (cf. Lemma 3).\n\nIt can be verified that the distributed scheme shown in Fig. 7 is resilient to s = c\u2113 \u2212 \u03b2 = 4 \u00d7 4 \u2212 4 = 12 stragglers and has Q = 117. Theorem 5 provides the value for s and Q for distributed matrix-matrix multiplication when \u03b2 \u2264 c. In Appendix A, we explicitly calculate the values for s and Q for the case when \u03b2 > c.\n\nRemark 4. Similar to the matrix-vector case, the uncoded matrix-matrix multiplication scheme can also be thought as a special case of \u03b2-level coding scheme with \u03b2 = 1. The lower bound given in (1) is matched by the proposed scheme here with \u03b2 A = \u03b2 B = 1 (i.e., the uncoded scheme). An example appears in Fig. 8 where we have n = 12\n\nworkers and the master node can recover the final product as soon as it receives Q = 52 symbols across all the workers.\n\nIn the matrix-matrix case for \u03b2 A = 2, \u03b2 B = 1 we can show that using different parallel classes can improve the straggler resilience of the system. The corresponding Q analysis is harder to do and is part of future work.  We now assume that each node receives \u03b3 = \u03b3 u + \u03b3 c fraction of the columns of A and the vector x. Here \u03b3 u corresponds to the storage fraction of the uncoded parts of A, whereas \u03b3 c corresponds to the coded portion. The coded blocks appear at the bottom of each node. Thus, under normal operating circumstances (no slow or failed nodes), the master node can simply decode the intended result from the uncoded computations. If some nodes are operating slower than normal, then the coded computations can be leveraged.\nW 0 W 1 W 2 W 3 W 4 W 5 W 6 W 7 W 8 W 9 W 10 W 11 A 0 A 1 A 1 A 2 A 2 A 0 A 0 A 1 A 1 A 2 A 2 A 0 A 0 A 1 A 1 A 2 A 2 A 0 A 0 A 1 A 1 A 2 A 2 A 0 B 0 B 1 B 2 B 0 B 1 B 2 B 0 B 1 B 2 B 1 B 2 B 3 B 1 B 2 B 3 B 1 B 2 B 3 B 2 B 3 B 0 B 2 B 3 B 0 B 2 B 3 B 0 B 3 B 0 B 1 B 3 B 0 B 1 B 3 B 0 B 1\nAs in the uncoded setup let \u2113 u = \u2206\u03b3 u be the number of uncoded block-columns and r u be the replication factor.\n\nLikewise \u2113 c = \u2206\u03b3 c represents the number of coded blocks in each worker. In this construction we set \u2206 = n so that r u = \u2113 u . In this case, the results from Theorem 2 immediately imply that Q \u2265 max(\u2206, \u2206r u \u2212 ru 2 (\u2113 u + 1) + 1). This follows by applying \u03b2 = 1 to the uncoded part of the solution where r u = \u2113 u . A construction that meets these bounds is outlined in Algorithm 3. The algorithm uses a random matrix of dimension n\u2113 c \u00d7 \u2206.\n\nTheorem 7. The scheme in Alg.3 satisfies Q = max(\u2206, \u2206r u \u2212 ru 2 (\u2113 u + 1) + 1). Furthermore, it is resilient to\nn 2 \u03b3c+n\u03b3u\u22121 n\u03b3c+1\nstragglers.\n\nProof. The detailed proof is discussed in Appendix C Example 8. Consider the setting where we have n = 5 workers with \u03b3 = 3 5 where we set \u2206 = n = 5. Fig. 4 shows the job assignments according to the uncoded scheme (\u03b2 = 1). According to Theorem 2 in Section IV, the system is resilient to \u03b2(n\u03b3 \u2212 1) = 2 stragglers and Q = 5 \u00d7 3 \u2212 3\u00d74 2 + 1 = 10 which can be verified from Fig. 4.\n\n\nAlgorithm 3: Cyclic coded at the bottom scheme for distributed matrix-vector multiplication\n\nInput : Matrix A and vector x, n-number of worker nodes, total storage capacity fraction \u03b3, replication factor for uncoded portion r u .\n1 Set \u2206 = n, \u2113 u = r u , \u2113 = \u03b3\u2206, \u2113 c = \u2113 \u2212 \u2113 u ; 2 Partition A into \u2206 block-columns A 0 , A 1 , . . . , A \u2206\u22121 ; 3 for i \u2190 0 to n \u2212 1 do 4 Define T = {i, i + 1, . . . , i + \u2113 u \u2212 1} (mod \u2206); 5\nAssign all A m 's sequentially from top to bottom to worker node i, where m \u2208 T ; 6 Assign \u2113 c different random linear combinations of A m 's for m / \u2208 T ;\n\n\nend\n\nOutput : Cyclic coded at the bottom scheme for matrix-vector multiplication. The coded submatrix assigned to Wi is denoted as Ci.\nW 0 W 1 W 2 W 3 W 4 A T 0 x A T 1 x C T 0 x A T 1 x A T 2 x C T 1 x A T 2 x A T 3 x C T 2 x A T 3 x A T 4 x C T 3 x A T 4 x A T 0 x C T 4 x\nNow we assume that the whole storage fraction can be distributed into an uncoded storage fraction \u03b3 u = 2 5 and a coded storage fraction \u03b3 c = 1 5 . Using the coded scheme, we get the job assignments shown in Fig. 9. This scheme is resilient to n 2 \u03b3c+n\u03b3u\u22121 n\u03b3c+1 = 3 stragglers and it can be verified from that A T x can be computed once any Q = \u2206r u \u2212 ru 2 (\u2113 u + 1) + 1 = 8 block-columns have been processed. Thus, we can conclude that introducing a single coded block in each worker (at the bottom), helps to improve both Q and the straggler resilience of the system as compared to an uncoded system. Similar schemes can be arrived at for the matrix-matrix case. We assume that the uncoded storage fraction for A is \u03b3 Au = au a2 and the coded storage fraction is \u03b3 Ac = ac a2 , so that the total storage fraction is \u03b3 A = a1 a2 . Each worker also receives \u03b3 B = b1 b2 fraction of the uncoded columns of matrix B.\n\nTheorem 8. The recovery threshold for the matrix-matrix multiplication scheme Alg. 4 is given by, \u03c4 = n \u2212 Algorithm 4: Cyclic coded at the bottom scheme for distributed matrix-matrix multiplication Input : Matrices A and B, n-number of workers. Storage fractions \u03b3 Au = au a2 and \u03b3 Ac = ac a2 , so that \u03b3 A = a1 a2 and \u03b3 B = b1 b2 . 1 Set \u2206 A = a 2 , \u2206 B = mb 2 , m = n (a2\u00d7b2) . Partition A and B into \u2206 A and \u2206 B block-columns, respectively; 2 for i \u2190 0 to n \u2212 1 do   Proof. The detailed proof is discussed in Appendix D.\nW 0 W 1 W 2 W 3 W 4 W 5 W 6 W 7 W 8 W 9 W 10 W 11 A 0 C 0 A 1 C 1 A 2 C 2 A 0 C 3 A 1 C 4 A 2 C 5 A 0 C 6 A 1 C 7 A 2 C 8 A 0 C 9 A 1 C 10 A 2 C 11 B 0 B 1 B 2 B 0 B 1 B 2 B 0 B 1 B 2 B 1 B 2 B 3 B 1 B 2 B 3 B 1 B 2 B 3 B 2 B 3 B 0 B 2 B 3 B 0 B 2 B 3 B 0 B 3 B 0 B 1 B 3 B 0 B 1 B 3 B 0 B 1\nExample 9. We consider the scenario as before, where \u03b3 A = 2 3 and \u03b3 B = 3 4 , and n = 12, so m = 12 3\u00d74 = 1. According to Alg. 4, we set \u2113 A = 2, \u2206 A = 3 and \u2113 B = 3, \u2206 B = 4. So, we need to recover \u2206 = \u2206 A \u2206 B = 12 block products. Figs. 8 and 10 show the job assignments to the workers for the uncoded case and the proposed coded scheme, respectively. For the coded scheme, we assume \u03b3 Au = 1 3 and \u03b3 Ac = 1 3 , and on the other hand, for the uncoded scheme, we have \u03b3 Ac = 0, so a c = 0. Output : n, \u03b3 A SCS optimal-scheme for matrix-vector multiplication with optimal Q/\u2206. \u03c4 = n \u2212 ma 2 b 1 + \u03ba = 12 \u2212 3 \u00d7 3 + 2 = 5 since the minimum positive integer \u03ba that satisfies \u03ba 3 + \u03ba \u2265 3 is 2.\n\nWe expect that the benefits of having densely coded block-columns at the bottom should extend for the case of general \u03b2 > 1 and the Q/\u2206 analysis should be possible to perform for the matrix-matrix case. However, this appears to be more challenging and will be investigated as part of future work.\n\n\nV. SPARSELY CODED STRAGGLER (SCS) OPTIMAL MATRIX COMPUTATIONS\n\nIn this section, we develop schemes for distributed matrix computations which perform optimally in terms of straggler resilience. For example, in matrix-matrix multiplication case, if the storage fractions of each worker node are \u03b3 A = 1/k A and \u03b3 B = 1/k B then it can be shown the lowest possible threshold is k A k B [3]. Similarly, for the matrix-vector multiplication case the optimal threshold is k A . Prior work has also demonstrated schemes that achieve these thresholds. In what follows, we present schemes that are similar in spirit to our constructions in Section IV which are suitable for sparse matrices while continuing to enjoy the optimal threshold k A k B . Moreover, unlike the previously available dense coded approaches, our proposed sparsely coded straggler (SCS) optimal scheme can utilize the partial computations of the slow workers and can provide significantly small Q/\u2206.\n\n\nA. Matrix-vector Multiplication\n\nIn our proposed scheme in Alg. 5, we set \u2206 = LCM(n, k A ) and assign the uncoded jobs in such a way that all the workers are assigned the uncoded jobs in an equal manner and the replication factor of the uncoded symbols over all n workers is, r u = 1. Thus each of the workers is assigned \u2206/n uncoded jobs and the rest \u2113 c = \u2206 kA \u2212 \u2206 n jobs are assigned using a random linear encoding matrix, R of size n\u2113 c \u00d7 \u2206. Since any (\u2206\u2212 \u03bb)\u00d7 (\u2206\u2212 \u03bb) submatrix of R is full rank with probability 1, the master node can decode all the unknowns if it receives any \u03bb uncoded symbols and any \u2206 \u2212 \u03bb coded symbols from all the workers. Thus we can say that Q = \u2206, and since each worker stores \u2206/k A block-columns, we have the recovery threshold, \u03c4 = \u2206 \u2206/kA = k A . Fig. 11: Partitioning matrix A into \u2206 = 12 submatrices and assigning to n = 6 workers each of which has been assigned two uncoded and one coded task to be resilient to s = 2 stragglers. The coded submatrix assigned to Wi is denoted as Ci.\nW 0 W 1 W 2 W 3 W 4 W 5 A T 0 x A T 1 x C T 0 x A T 2 x A T 3 x C T 1 x A T 4 x A T 5 x C T 2 x A T 6 x A T 7 x C T 3 x A T 8 x A T 9 x C T 4 x A T 10 x A T 11 x C T 5 x\nExample 10. We consider an example in Fig. 11 with n = 6 and \u03b3 = 1 4 , so k A = 4. We set \u2206 = LCM(6, 4) = 12, and \u2113 c = 12 4 \u2212 12 6 = 1. Thus, we assign two uncoded jobs and one coded job to each worker where the coded job assignment would be incorporated using a random matrix R of size 6 \u00d7 12. In this case, Q = 12, thus Q/\u2206 = 1, and \u03c4 = 4.\n\nRemark 5. On the surface Fig. 11 may appear equivalent to a systematic version of the RKRP coded scheme [17] with the same number of matrix partitions. However, there is a significant difference that the idea in the RKRP coded scheme is to assign the systematic versions to some workers and the coded versions to other workers, whereas we assign the jobs in a symmetric fashion so that every worker receives same number of uncoded and same number of coded jobs. If the input matrices are sparse, then the parity workers in the RKRP coded scheme will be significantly slower than the systematic workers.\n\n\nB. Matrix-matrix Multiplication\n\nWe propose a matrix-matrix multiplication scheme in Alg. 6 with storage fractions \u03b3 A = 1/k A and \u03b3 B = 1/k B and recovery threshold k A k B . Furthermore, Q/\u2206 = 1 + (k B \u2212 1)\u2113 c /\u2206, where \u2113 c is the number of coded-coded matrix-matrix products assigned to each worker node. Theorem 9. Alg. 6 proposes a distributed matrix-matrix multiplication scheme being resilient to s = n \u2212 k A k B\n\n\nstragglers.\n\nProof. According to this scheme, we know that every worker is assigned \u2206A kA block-columns (uncoded and coded) from A and one coded block-column from B, which indicates that we can obtain, in total, \u2206A kA products from each of the workers. Thus from any k A k B workers, the master node can obtain \u2206A kA \u00d7 k A k B = \u2206 A k B = \u2206 A \u2206 B = \u2206 products. A simple counting argument applied to Alg. 6 shows that any uncoded block-column of A appears exactly Output : n, \u03b3 A , \u03b3 B SCS optimal-scheme for distributed matrix-matrix multiplication. (6)). It follows that the products involving the uncoded block-column A i can be expressed as\n\u2206 A , i = 0, . . . , \u2206 A \u2212 1. It follows that the product ( \u2206A\u22121 i=0 u i A i ) T ( \u2206B \u22121 j=0 v j B j ) corresponds to the vector \u2206A\u22121 i=0 u i (e i \u2297 v), where v is the vector [v 0 v 1 . . . v \u2206B \u22121 ] T (cf. discussion arounde i \u2297 v (\u2113) for \u2113 \u2208 J i .\nOur first observation is that the collection of vectors {e i \u2297 v (\u2113) } for \u2113 \u2208 J i , i = 0, . . . , \u2206 A \u2212 1 is linearly independent. This follows because any linear combination of these vectors can equivalently be expressed as\n\u2206A\u22121 i=0 e i \u2297 \u2113\u2208Ji \u03b1 (i) \u2113 v (\u2113) where \u03b1 (i)\n\u2113 's are the linear combination coefficients and each term in the above sum needs to be forced to zero. Note that |J i | \u2264 k B . Therefore, the vectors v (\u2113) for \u2113 \u2208 J i are linearly independent with probability 1, since v (\u2113) has length \u2206 B = k B . Thus, there is no setting of \u03b1 (i) \u2113 's for which the above sum can be forced to the zero vector. The product of the coded A and B matrices can be represented by u (\u2113,j) \u2297 v (\u2113) for j = 0, 1, . . . , \u2113 c \u2212 1 and \u2113 \u2208 I.\n\nWe will now show that the overall collection of vectors that we obtain is linearly independent with probability 1.\n\nTo see this suppose that there exist coefficients \u03b1 \n\u2206A\u22121 i=0 e i \u2297 \u2113\u2208Ji \u03b1 (i) \u2113 v (\u2113) = \u2113\u2208I \u2113c\u22121 j=0 \u03ba (j) \u2113 u (\u2113,j) \u2297 v (\u2113) = \u2113\u2208I \u2113c\u22121 j=0 \u03ba (j) \u2113 \u2206A\u22121 j1=0 u (\u2113,j) j1 e j1 \u2297 v (\u2113) .\n\nIt can be observed that this decouples into finding solutions for\ne i \u2297 \u2113\u2208Ji \u03b1 (i) \u2113 v (\u2113) = e i \u2297 \u2113\u2208I \u2113c\u22121 j=0 \u03ba (j) \u2113 u (\u2113,j) i v (\u2113)(7)\nwhere the \u03b1 \u2113 's. The n \u2212 k A k B stragglers together contain (n \u2212 k A k B )\u2206 A k B /n uncoded block-columns of A. It is not too hard to see that not all A i 's that appear within the stragglers appear k B times within the stragglers (see Appendix F).\n\n\nThus, the number of\nA i 's with |J i | < k B is \u2265 (n \u2212 k A k B )\u2206 A /n + 1.\nIn the argument below we only consider the \u03b1 (i) \u2113 's corresponding to these uncoded block-columns and suppose that there is an assignment of \u03b1 (i) \u2113 's that satisfy (7). In this case the problem of finding the corresponding \u03ba (j) \u2113 's is equivalent to solving a block system of equations described below.\n\nLet A \u03b4 be an uncoded block-column that appears less than k B times in I. The block row corresponding to it (cf. (7)) is given by\u1e7c \u2299\u0168 wher\u1ebd\nV = \u2113c v (i0) . . . v (i0) | . . . | \u2113c v (i k\u22121 ) . . . v (i k\u22121 ) , and U =[u (i0,0) \u03b4 . . . u (i0,\u2113c\u22121) \u03b4 | . . . |u (i k\u22121 ,0) \u03b4 . . . u (i k\u22121 ,\u2113c\u22121) \u03b4 ]\nwhere \u2299 represents the Khatri-Rao product that corresponds to column-wise Kronecker products.\n\nAppendix E shows that the concatenation of block rows in\u1e7c \u2299\u0168 corresponding to the different A \u03b4 's is such that any \u2113 c k A k B \u00d7 \u2113 c k A k B matrix is full rank with probability-1. This implies that from the first \u2113 c k A block rows we can decode all the \u03ba (i) \u2113 's. On the other hand the equations in (7) need to be satisfied for at least (n \u2212 k A k B )\u2206 A /n + 1 different A i 's based on the argument above. However\n(n \u2212 k A k B ) \u2206 A n = \u2206 A \u2212 k A k B \u2206 A n = k A \u2206 A k A \u2212 \u2206 n = \u2113 c k A and thus, (n \u2212 k A k B )\u2206 A /n + 1 > \u2113 c k A ;\nThis implies that there is at least one equation that need to be satisfied with a fixed choice of the \u03ba Proof. As in the proof of the previous result, we let u (\u2113,j) for j = 0, . . . , \u2113 c \u2212 1 denote the j-th random encoding vector for A in worker W \u2113 and v (\u2113) the corresponding random encoding vector for B. We will demonstrate that the system of equations that corresponding to decoding the A T i B j 's is nonsingular with probability 1. Let e i denote the i-th unit vector of length \u2206 A . For a given A i , suppose that it appears uncoded in J i worker nodes where |J i | \u2264 k B we obtain certain equations from the uncoded part which correspond to e i \u2297 v (\u2113) for \u2113 \u2208 J i . If |J i | < k B then it needs to use the coded-coded products for decoding the unknowns corresponding to A i .\n\nThe block system of equations under consideration corresponds to a \u2206 A k B \u00d7 \u2206 A k B square matrix with random entries. For A i such that |J i | = k B the matrix consists of a k B \u00d7 k B block on the diagonal with k B distinct vectors v (\u2113) . This block is nonsingular with probability-1 owing to the random choice of the v (\u2113) 's.\n\nFor the other A i 's where |J i | < k B we will demonstrate a setting of the u (\u2113,j) 's such that the entire matrix is a block diagonal matrix with k B \u00d7 k B blocks of distinct v (\u2113) vectors. This demonstrates that there exists a choice of random coefficients for which the system of equations is nonsingular. Following this the result holds with probability-1 when the choice is made at random.\n\nTowards this end, suppose that the pattern of obtained products is such that we get \u2206\u2212\u03bb uncoded-coded products and \u03bb+(k B \u22121)\u2113 c coded-coded products. Without loss of generality we assume that we need to decode the products that involve A 0 , A 1 , . . . , A \u03b4\u22121 using the coded-coded products. Furthermore we suppose that A i appears k B \u2212 \u03b7 i times within the uncoded-coded products, so that \u03b7 0 + \u03b7 1 + \u00b7 \u00b7 \u00b7 + \u03b7 \u03b4\u22121 = \u03bb.\n\nUnder this setting, there are at least (k B \u2212 1)\u2113 c + \u03bb \u2212 (k B \u2212 \u03b7 0 )\u2113 c = (\u03b7 0 \u2212 1)\u2113 c + \u03bb coded-coded products that can be obtained from worker nodes that do not contain an uncoded copy of A 0 . Furthermore, these are spread out in at least \u03b7 0 distinct worker nodes. Next, we pick \u03b7 0 encoding vectors for A from the \u03b7 0 distinct workers and set them all to e 0 . With this setting we obtain a k B \u00d7 k B block (corresponding to decoding A T 0 B j , j = 0, . . . , \u2206 B \u2212 1) that consists of distinct v (\u2113) vectors that are nonsingular with probability 1.\n\nAt this point we are left with (k B \u2212 1)\u2113 c + \u03bb \u2212 \u03b7 0 coded-coded products. The argument can be repeated for A 1 since there are at least (\u03b7 1 \u2212 1)\u2113 c + \u03bb \u2212 \u03b7 0 coded-coded products that can be obtained from workers where A 1 does not appear, which in turn correspond to at least \u03b7 1 distinct workers. In this case we will set the \u03b7 1 encoding vectors to e 1 . The process can be continued in this way until the coded-coded products are assigned to each of\nA 0 , A 1 , . . . , A \u03b4\u22121 .\nAt the end of the process we can claim that we have a block diagonal matrix where each block is a k B \u00d7k B square matrix with distinct v (\u2113) vectors. Thus each block and consequently the entire system of equations is nonsingular.\n\nFinally, as there exists a choice of random values that makes the system of equations nonsingular, it continues to be nonsingular with probability 1 under a random choice.\n\nTo summarize, Theorems 9 and 10 demonstrate that our proposed scheme has the optimal threshold k A k B and\nQ \u2206 = 1 + (k B \u2212 1)\u2113 c \u2206 = 1 + (k B \u2212 1) \u2206A kA \u2212 \u2206 n \u2206 = 1 + \u2206(k B \u2212 1) 1 kAkB \u2212 1 n \u2206 = 1 + (k B \u2212 1)s nk A k B \u2248 1 + s nk A ;\nif k B is significantly larger than 1. Moreover in the practical cases, we usually have s << nk A , thus in this SCS optimal scheme, we have Q/\u2206 \u2248 1.\n\nExample 11. We consider an example in Fig. 12 with n = 5 and k A = k B = 2, so the system is resilient s = 5 \u2212 4 = 1 straggler. We set \u2206 A = LCM(n, k A ) = 10 and \u2206 B = k B = 2, and in this example, Q = 21, thus Q/\u2206 = 1.05. Here RA and RB are random matrices whose superscripts indicate their corresponding rows and columns.\nW 0 W 1 W 2 W 3 W 4 A 0 A 1 A 2 A 3 9 i=0 R (0,i) A A i 1 i=0 R (0,i) B B i A 2 A 3 A 4 A 5 9 i=0 R (1,i) A A i 1 i=0 R (1,i) B B i A 4 A 5 A 6 A 7 9 i=0 R (2,i) A A i 1 i=0 R (2,i) B B i A 6 A 7 A 8 A 9 9 i=0 R (3,i) A A i 1 i=0 R (3,i) B B i A 8 A 9 A 0 A 1 9 i=0 R (4,i) A A i 1 i=0 R (4,i) B B i\n\nVI. NUMERICAL EXPERIMENTS AND COMPARISONS\n\nIn this section, we discuss the results of the numerical experiments for our proposed approaches and compare them with other available methods. First we compare all the approaches in terms of number of stragglers that a scheme can be resilient to, and in terms of Q values. Next we compare the approaches in terms of the worker computation time and numerical stability during the decoding process. Software code for recreating these experiments can be found at [35]. Table V shows the comparison for matrix-vector multiplication for n = 30 workers, each of which can store \u03b3 A = 1 10 fraction of matrix A. For the convolutional code approach, we assume s = 15 so that n\u2212s = 15 > 1 \u03b3A = 10 which satisfies the required condition in [10]. And for the coded at bottom approach, we assume \u03b3 u = 1 15 and \u03b3 c = 1 30 , so that \u03b3 = \u03b3 u + \u03b3 c . Similarly, Table VI shows the comparison for different approaches for matrix-matrix multiplication for n = 18 workers, each of which can store \u03b3 A = 1 3 and \u03b3 B = 1 3 fraction of matrices A and B respectively. Here we assume k A = k B = 4 > 1 \u03b3A = 1 \u03b3B = 3 for the approach in [10]. In case of both matrix-vector and matrix-matrix multiplications, we know that the dense coded approaches [3], [17], [10] and [21] are MDS but they do not consider the partial computations of the slower workers. On the other hand, our proposed approaches are able to utilize the partial computations of the stragglers for both matrix-vector and matrix-matrix multiplications. We can see that the \u03b2-level coding approaches, with \u03b2 = 2 or 3, have smaller Q/\u2206 values than the uncoded approaches, one of which is introduced in [11] and the other is a special case of our proposed \u03b2-level coding where \u03b2 = 1. We emphasize that a larger value of \u03b2 or a larger value of \u03b3 c will provide smaller values of Q/\u2206 for our proposed \u03b2-level coding approach and the coded-at the bottom scheme, respectively.\n\n\nA. Number of stragglers and Q value\n\nIt should be noted that the approach in [10] requires the condition n \u2212 s > 1 \u03b3 to be full-filled to be resilient to s  stragglers, so as mentioned in Tables V and VI, this convolutional code-based approach is resilient to less number of stragglers than the other dense coded approaches.\n\n\nB. Worker Computation Time\n\nWe compare the computation time required by the workers in case of different approaches by experiments performed on an Amazon Web Services (AWS) cluster where we choose a t2.2xlarge machine as the master node and t2.small machines as the worker nodes, which are, in fact, responsible for computing the submatrix products.\n\nFor matrix-vector multiplication, We choose a matrix A of size 40, 000\u00d717, 640 and a vector x of length 40, 000, and the job is to compute A T x in a distributed fashion. We assume that the matrix A is sparse, which indicates that the most of the entries of A are zero. For example, the sparsity of A can be 98% (or 95%), which indicates that randomly chosen 2% (or 5%) entries of matrix A are non-zero. We consider the same scenario where we have n = 30 workers, each of which can store \u03b3 A = 1 10 fraction of matrix A. The comparison among different approaches for different sparsity values is shown in Table V. Next a similar experiment is carried out for matrixmatrix multiplication where both A and B are sparse and of sizes 12000 \u00d7 13680 and 12000 \u00d7 10260, respectively, and the corresponding results are shown in Table VI. From the experimental results shown in Tables V and VI, we can see that the workers require much more time to complete their assigned jobs in case of the dense coded approaches ( [3], [17], [10] and [21]) than our proposed approaches. The reason is that the dense coded approaches cannot preserve the sparsity of the matrices A or B, so the corresponding coded submatrices are quite dense even if A and B are sparse. On the other hand, our proposed approaches can preserve the sparsity in the submatrices, and can complete the jobs 3 \u223c 4 times faster than the available approaches. It should be noted that a smaller value of \u03b2 or a smaller value of \u03b3 c will lead to less worker computation time for our proposed \u03b2-level coding approach and the coded at the bottom scheme, respectively.\n\nWe note here that while there is a significant difference between the required time of the dense coded approaches and our proposed approaches, this difference can be much higher. For example, in Table V, we can see that the polynomial code approach is around 3 \u223c 4 times slower than the uncoded approach, but the gap according to the theoretical analysis should be as large as 10 times, since \u03b3 A = 1/10. The reason underlying the smaller gap is the use of two different commands in Python to compute products between the matrix and the vector. Since the proposed uncoded or the \u03b2-level coding approaches can preserve the sparsity up to certain level, we have leveraged the sparse matrix-multiplication commands in these cases, whereas for the dense coded approaches which cannot preserve the sparsity, regular matrix-multiplication command provided better results. A more optimized sparse matrix-multiplication scheme could result in bigger multiplicative gaps between these approaches. Furthermore, the difference of the required time would be certainly higher and more significant if the matrix sizes were higher (for example, in millions). However, owing to the memory limitations of the machines that we are using (in this case, t2.small), we cannot conduct experiments with such large matrices.\n\n\nC. Numerical Stability\n\nNow we do another experiment to compare the numerical stability of different schemes. We know that for decoding a system of equations, errors in the input can get amplified by the condition number (ratio of maximum and minimum singular values) of the associated decoding matrix; hence, a low condition number is critical [10], [20]. For example, let us consider the polynomial codes [3] for matrix vector multiplication, where each of n workers can store \u03b3 = 1 k fraction of matrix A. Now partitioning A into \u2206 = k submatrices lead to \u2206 unknowns,\nA T 0 x, A T 1 x, . . . , A T \u2206\u22121\nx. Now in order to assign the coded jobs to n workers, we need to choose a polynomial of degree k \u2212 1 and n evaluation points, thus the coding matrix is of size n \u00d7 k. Since the recovery threshold here is \u03c4 = k, we are interested in all choices of k \u00d7 k submatrices of that n \u00d7 k coding matrix. It can be shown that the system will be numerically more stable in the worst case if the evaluation points are chosen uniformly spaced in [\u22121, 1], rather than choosing the integers 1, 2, . . . , n [15]. In other words, choosing interpolation points uniformly spaced in [\u22121, 1] will lead to a smaller worst case condition number (\u03ba worst ).\n\nIn this experiment we compare the condition numbers for different approaches in case of the worst choice of full stragglers. Tables V and VI show the comparison of worst case condition numbers (\u03ba worst ) for matrix-vector and matrix-matrix multiplication, respectively, for the previously chosen scenario. We can see that the dense coded approaches ( [3], [17] and [21]) have a very high worst case condition number, thus suffer from numerical instability which leads to erroneous results. On the other hand, our proposed \u03b2-level coding approach has a much smaller worst case condition number. The reason is that even in the worst case, the decoding of some \u03b2 unknowns depends on a \u03b2 \u00d7 \u03b2 system matrix whose entries are randomly chosen. Thus a smaller \u03b2 leads to a smaller \u03ba worst , for example, we can see that the uncoded case (same as the case with \u03b2 = 1) is the scheme having the smallest \u03ba worst .\n\n\nD. Comparison with the Proposed SCS Optimal Scheme\n\nIn this experiment, we compare the dense coded approaches with our proposed SCS optimal coding scheme in terms of Q values and worker computation time. First we do the comparison for matrix-vector multiplication where we choose a square sparse matrix A of size 27, 720 \u00d7 27, 720, and a vector x of length 27, 720. The job is to compute A T x in a distributed system of n = 18 workers, each of which can store \u03b3 A = 1 15 fraction of matrix A. We consider two different choices of matrix A. In the first case, A is a band matrix [36] where the entries are non-zero along the principal diagonal and in 1000 other k-diagonals just above and below the principal diagonal.\n\nIn the second case, the entries are non-zero along the principal diagonal and in 2000 other randomly chosen kdiagonals. The comparison is shown in Table VII where we can see that the proposed SCS optimal scheme requires less time from the worker nodes in comparison to the other dense coded approaches, which in fact, cannot leverage the sparsity of matrix A.\n\nNext to show an example for distributed matrix-matrix multiplication, we choose two random sparse matrices A and B of sizes 12000 \u00d7 15000 and 12000 \u00d7 13500, where randomly chosen any 2% and 5% entries are non-zero.\n\nWe consider a distributed system having n = 24 workers, each of which can store \u03b3 A = 1 4 fraction of matrix A and \u03b3 B = 1 5 fraction of matrix B. The comparison is shown in Table VIII which further confirms the superiority of the proposed SCS optimal scheme in terms of workers' computation speeds. The major reason behind the enhancement of the speed in the SCS optimal scheme lies in its ability to leverage the sparsity of the matrices up to certain level, whereas the approaches in [3] or [21] use the dense linear combinations of the submatrices which destroy the sparsity. The approaches in [10] and [17] consider some parity worker nodes where all the assigned submatrices are dense, which leads to high worker computation time for those workers. On the other hand, in the proposed SCS optimal scheme the submatrices, obtained from dense linear combinations, are assigned uniformly within the workers. This removes the asymmetry between the worker node computation times. Now, similar to the most of the dense coded approaches [3], [21], [17], our proposed SCS optimal scheme is also resilient to s = n \u2212 k A k B stragglers, where \u03b3 A = 1 kA and \u03b3 B = 1 kB . We point out that we did not compare with the approach in [7] since their approach does not respect the storage constraints for the matrices at each worker node and only has a high-probability guarantee on the recovery threshold. Similarly we did not compare with [32] which  assumes heterogeneous workers, but we note that this approach provides with a value of Q/\u2206 to be 11/10 for the example shown in Table VIII. Now, in the dense coded approaches, we can decode all \u2206 = k A k B unknowns from any k A k B submatrix block products, and in that sense we have Q \u2206 = 1. But it does not necessarily mean that those scheme can utilize the partial computations done by the slower workers, since in those cases the master requires k A k B workers to finish their jobs, and discard the computations done by others.\n\nHowever, one can still use those approaches to utilize the partial computations, by partitioning the matrices into more submatrices. We can consider the an example of n = 10 workers with \u03b3 A = \u03b3 B = 1/3 and 98% sparse matrices A and B, both having size 12, 000 \u00d7 12, 000. Now we can partition matrix A into \u2206 A = 3 or \u2206 A = 9\n\nsubmatrices for the dense coded approaches. We can see the comparison of \u03ba worst and worker computation time in Table IX for these two values of \u2206 A . In case of \u2206 A = 9, we will require polynomials of higher degrees (for [3] or [21]) or more random coefficients (for [17]) than in the case of \u2206 A = 3. It leads to a very high condition number (\u2248 10 13 ) which will make the whole system numerically unstable. Besides, a larger \u2206 A would make the \n\n\nVII. CONCLUSIONS AND FUTURE WORK\n\nIn this work we have presented several coded matrix computation schemes that (i) leverage partial computations by stragglers and (ii) impose constraints on the extent to which coding is allowed in the solution.\n\nThe second feature is especially valuable in the practical case of computations with sparse matrices and provides significant reductions in worker node computation time and better numerical stability as compared to the previous schemes. Prior work has demonstrated schemes with optimal recovery threshold in certain cases. We present schemes that match the optimal threshold while enjoying lower worker node computation times and improved numerical stability. Exhaustive numerical experiments corroborate our findings.\n\nThere are several opportunities for future work. We have demonstrated that carefully chosen different parallel classes provide improved recovery thresholds and Q/\u2206 metrics for the matrix-vector problem. We expect that this should help even in the case of matrix multiplication. Schemes that apply for a larger range of storage fractions are also of interest. In this work we defined the value of Q as the worst case number of symbols that allows for recovering the intended result. Analysis and constructions for the random case may be of interest.\n\n\nVIII. ACKNOWLEDGMENTS\n\nThe authors acknowledge interesting conversations with Dr. Li Tang and his participation in [11].\n\n\nAPPENDIX\n\n\nA. Properties of \u03b2-level Coding when \u03b2 > c\n\nIn Section IV, we have discussed \u03b2-level coding for distributed matrix computations when c \u2265 \u03b2 and here we prove the properties of \u03b2-level coding when \u03b2 > c. The difference is that the constraint c \u2265 \u03b2 ensures that we will have at least \u03b2 worker groups, whereas it is not the case when \u03b2 > c.\n\n\n1) Matrix-vector Multiplication:\n\nSuppose that we have n = ca 2 workers, each of which can store \u03b3 = a1 a2 fraction of matrix A. To incorporate \u03b2-level coding, matrix A is partitioned into \u2206 = \u03b2a 2 block-columns, and thus each worker will be assigned \u2113 = \u2206\u03b3 = \u03b2a 1 jobs. It should be noted that we have n \u2206/\u03b2 = c worker groups among the workers because of the cyclic fashion of job assignments.\n\nLemma 4. If we use a single parallel class in Alg. 1, then the number of stragglers will be s = c\u2113 \u2212 \u03b2 and we will have\nQ = c \u2206 \u03b2 \u2113 \u2212 \u2113(\u2113 + 1) 2 + c c1\u22121 i=0 (\u2113 \u2212 i) + c 2 (\u2113 \u2212 c 1 ) + 1 where c 1 = \u230a \u03b2\u22121 c \u230b and c 2 = \u03b2 \u2212 1 \u2212 cc 1 .\nProof. The straggler resilience follows similar to the proof of Theorem 2 by counting the number of occurrences of the meta-symbols.\n\nFor the Q analysis, assume that there exists a meta-symbol \u22c6 that appears at most \u03b2 \u2212 1 times among the acquired Q symbols where Q is defined in the theorem statement. We have c worker groups and in each group, \u22c6 appears in positions 0, 1, 2, . . . , \u2113 \u2212 1.\n\nNow we know that we can process \u03b1 0 = \u2206 \u03b2 \u2113 \u2212 \u2113(\u2113+1) 2 meta-symbols from each of the worker groups without processing \u22c6. Any additional processing will necessarily process \u22c6. Suppose we choose any particular worker, where the position index of \u22c6 is i. In that case, we can acquire at most \u2113 \u2212 1 \u2212 i more symbols from that particular worker without any more appearances of \u22c6. Thus, the maximum number of meta-symbols that can be processed for each additional appearance of \u22c6 can be expressed by the following vector. Here z is a non-increasing sequence, so in order to obtain the maximum number of symbols where the meta-symbol \u22c6 appears at most \u03b2 \u2212 1 times, we need to acquire symbols sequentially as mentioned in z. Let c 1 = \u230a \u03b2\u22121 c \u230b and c 2 = \u03b2 \u2212 1 \u2212 cc 1 . Thus we can choose the first cc 1 + c 2 = \u03b2 \u2212 1 workers (as mentioned in z) so that we can have Q \u2032 symbols where \u22c6 appears exactly \u03b2 \u2212 1 times, so\nQ \u2032 = c\u03b1 0 + c c1\u22121 i=0 (\u2113 \u2212 i) + c 2 (\u2113 \u2212 c 1 );\nwhich indicates that Q = Q \u2032 + 1 symbols ensures that \u22c6 will appear at least \u03b2 times. This leads to a contradiction and concludes the proof.\n\n\n2) Matrix-matrix Multiplication:\n\nThe argument is almost the same for the matrix-matrix case with appropriate definitions for \u2113 and \u03b2. Specifically, recall that n = c \u00d7 a 2 b 2 , and \u2206 A = \u03b2 A a 2 and \u2206 B = \u03b2 B b 2 . Thus, we have n \u2206/\u03b2 = c worker groups, where \u2206 = \u2206 A \u2206 B and \u03b2 = \u03b2 A \u03b2 B . In each worker, we assign \u2113 A = \u2206 A \u03b3 A and \u2113 B = \u2206 B \u03b3 B coded submatrices of A and B, respectively and set \u2113 = \u2113 A \u2113 B . Following this, we can obtain the number of stragglers as s = c\u2113 \u2212 \u03b2 and\nQ = c \u2206 \u03b2 \u2113 \u2212 \u2113(\u2113 + 1) 2 + c c1\u22121 i=0 (\u2113 \u2212 i) + c 2 (\u2113 \u2212 c 1 ) + 1.\n\nB. Proof of Theorem 4\n\nProof. Straggler Resilience: To prove the straggler resilience, we note that if there are at 2\u2113 \u2212 1 stragglers it is evident that G dec formed by the remaining meta-symbols is such that each unknown has degree at least one. Let X i and X j denote the subset of worker nodes where unknowns A T i x and A T j x appear within a meta-symbol, so\nthat |X i | = |X j | = 2\u2113. Furthermore, |X i \u2229 X j | \u2264 2\u2113 \u2212 2.\nTo see this we note that if {i, j} appear together w.l.o.g.\n\nin G 0 then |X i \u2229 X j | = \u2113 + \u2113 \u2212 2 as this implies that they appear together in exactly \u2113 \u2212 2 workers in G 1 (since \u2113 \u2264 \u2206/2 \u2212 2). On the other hand if i and j do not appear together in either G 0 or G 1 then they appear together in the workers of each group at most \u2113 \u2212 1 times, so the claim holds. Thus,\n|X i \u222a X j | = |X i | + |X j | \u2212 |X i \u2229 X j | \u2265 2\u2113 + 2.\nNow suppose by way of contradiction that we have two unknowns A T i x and A T j x (where i < j) both of which appear exactly once across the remaining n \u2212 2\u2113 + 1 workers. The preceding argument shows that if 2\u2113 \u2212 1 workers are stragglers then unknowns A T i x or A T j x or both appear in at least three nodes, i.e., at least one of them appears at least twice. This contradicts our original assumption. By Lemma 2 the decoding is successful.\nValue of Q: Note that \u03b1 0 = \u2206 2 \u2113 \u2212 \u2113(\u2113+1) 2\ndenotes the maximum number of meta-symbols that can be processed within a group such that a specific meta-symbol is not processed (cf. Lemma 1). This implies that at most 2\u03b1 0 meta-symbols can be processed without processing any specific unknown. Let \u03c1 0 and \u03c1 1 denote the number of meta-symbols processed in the two groups G 0 and G 1 where we assume w.l.o.g. that \u03c1 0 \u2265 \u03c1 1 .\n\n\u2022 Case 1: Suppose that \u03c1 0 \u2265 \u03b1 0 + \u2113 + 1. Lemma 1 implies that each meta-symbol \u2208 P 0 is processed at least twice in G 0 . Then by Lemma 2, the decoding is successful.\n\n\u2022 Case 2: If \u03b1 0 + 2 \u2264 \u03c1 0 \u2264 \u03b1 0 + \u2113, we claim that at most one meta-symbol in G 0 is processed once. The other meta-symbols are processed at least twice. To see this, consider two meta-symbols (2i, 2i + 1) and (2j, 2j + 1)\n\nin G 0 such that j > i such that (2i, 2i + 1) is processed only once. If j \u2212 i \u2265 2 then there are at least two workers in G 0 where the meta-symbol (2j, 2j + 1) appears but (2i, 2i + 1) does not. Therefore, if at least \u03b1 0 + 1 meta-symbols are processed in G 0 , then (2j, 2j + 1) appears at least twice. On the other hand if j = i + 1 then there is only one worker where (2j, 2j + 1) appears but (2i, 2i + 1) does not. Thus, if \u03b1 0 + 1 meta-symbols are processed then we have processed (2j, 2j + 1) at least once. The \u03b1 0 + 2-th meta-symbol cannot be (2i, 2i + 1) since by assumption it is processed only once, thus it has to be (2j, 2j + 1) (since j = i + 1).\n\nNow, we argue either unknown A T 2i x or A T 2i+1 x appear within the meta-symbols in G 1 . Towards this end, we note that there are exactly two workers in G 1 where A T 2i+1 x appears but A T 2i x does not. Therefore, at most \u03b1 0 \u2212 (2\u2113 \u2212 1) meta-symbols can be processed in G 1 while avoiding both the unknowns A T 2i x and A T 2i+1 x.\n\nThis implies that the total number of meta-symbols that can be processed such that at least two unknowns appear only once in G dec is at most 2\u03b1 0 \u2212 \u2113 + 1 < Q.\n\n\u2022 Case 3: If \u03c1 0 = \u03b1 0 + 1, then we can have two meta-symbols (2i, 2i + 1) and (2i + 2, 2i + 3) that appear exactly once in G 0 . It can be verified that none of the unknowns A T 2i x, . . . , A T 2i+3 x appear together in a meta-symbol in G 1 since \u2206 \u2265 8. Thus, if we process \u03c1 1 = \u03b1 0 symbols in G 1 , then we can avoid at most one unknown from the set {A T 2i x, . . . , A T 2i+3 x}. It follows that at most one unknown appears once in G dec and by Lemma 2, the decoding is successful.\n\n\nC. Proof of Theorem 7\n\nProof. We need to show that for any pattern of Q symbols the master node can decode A T x. Towards this end, from Theorem 2 (setting \u03b2 = 1 and \u2113 = \u2113 u = r u ), we know that any pattern of Q uncoded symbols allows the recovery of all \u2206 unknowns. In other words for any computation state vector w(t) = [w 0 (t) w 2 (t) . . . w n\u22121 (t)]\n\nsuch that w i (t) \u2264 \u2113 u and n\u22121 i=0 w i (t) \u2265 Q, the master node can decode. Now, consider a vector w \u2032 (t) such that (w.l.o.g.) w \u2032 0 (t), . . . , w \u2032 \u03b1\u22121 (t) \u2265 \u2113 u + 1 and w \u2032 \u03b1 (t), . . . , w \u2032 n\u22121 (t) \u2264 \u2113 u and n\u22121 i=0 w \u2032 i (t) \u2265 Q, i.e., the first \u03b1 worker nodes process coded blocks whereas the others do not. It is not too hard to determine a different vectorw(t) with the following properties.w\ni (t) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u2113 u 1 \u2264 i \u2264 \u03b1, w \u2032 i (t) + \u03b2 i \u03b1 + 1 \u2264 i \u2264 n,\nwhere \u03b2 i 's are positive integers such that w \u2032 i (t) + \u03b2 i \u2264 \u2113 u and n\u22121 i=0w i (t) = Q. Thus,w(t) corresponds to a pattern of Q uncoded blocks that recovers \u2206 distinct blocks. Now, we compare the vectors w \u2032 (t) andw(t). Let the uncoded symbols in w \u2032 (t) be denoted by the set A. Then the set of uncoded symbols inw(t) can be expressed as A \u222a B where the set B results from the transformation above. It is evident that for computation state vector w \u2032 (t) the master node has\n\u03b1\u22121 i=0 (w \u2032 i (t) \u2212 \u2113 u ) equations with \u2206 \u2212 |A| variables. Now, \u03b1\u22121 i=0 (w \u2032 i (t) \u2212 \u2113 u ) \u2265 |B| \u2265 |B \\ A| = \u2206 \u2212 |A|.\nIn particular, this establishes that we have at least as many equations as variables. Since any square submatrix of a random matrix is invertible with probability 1, we have the required result.\n\nNext, we establish the straggler resilience of our scheme. Consider worker nodes 0 \u2264 i 1 < i 2 < \u00b7 \u00b7 \u00b7 < i k \u2264 n\u2212 1;\n\neach of these worker nodes has \u2113 u uncoded symbols. Consider the case that i t \u2212 i t\u22121 < \u2113 u for t = 2, 3, . . . , k. We claim that these worker nodes contain at least min(\u2113 u + k \u2212 1, \u2206) distinct uncoded symbols. To see this we proceed inductively. Let X ij denote the symbols in worker i j . If k = 2, then |X i1 \u222a X i2 | = |X i1 | + |X i2 | \u2212 |X i1 \u2229 X i2 | \u2265 2\u2113 u \u2212 (\u2113 u \u2212 1) = \u2113 u + 1. We assume the inductive hypothesis, i.e, |X i1 \u222a \u00b7 \u00b7 \u00b7 \u222a X i k\u22121 | \u2265 min(\u2113 u + k \u2212 2, \u2206).\n\nNow consider |X i1 \u222a \u00b7 \u00b7 \u00b7 \u222a X i k\u22121 \u222a X i k |. It can be observed that if i k\u22121 + \u2113 u \u2212 1 \u2212 \u2206 < i 1 then there exists at least one symbol in X i k that does not exist in X i1 \u222a \u00b7 \u00b7 \u00b7 \u222a X i k\u22121 . Thus, in this case |X i1 \u222a \u00b7 \u00b7 \u00b7 \u222a X i k\u22121 \u222a X i k | \u2265 \u2113 u + k \u2212 1.\n\nOn the other hand if i k\u22121 + \u2113 u \u2212 1 \u2212 \u2206 \u2265 i 1 then X i k \u2286 X i k\u22121 \u222a X i1 . Let \u03b4 be the smallest integer such that i \u03b4 +\u2113 u \u22121\u2212\u2206 \u2265 i 1 but i \u03b4\u22121 +\u2113 u \u22121\u2212\u2206 < i 1 . In this case, we have |X i1 \u222aX i2 \u00b7 \u00b7 \u00b7\u222aX i \u03b4\u22121 | = \u2113 u + \u03b4\u22121 x=2 (i x \u2212i x\u22121 ). Furthermore, X i \u03b4 contributes another \u2206 + i 1 \u2212 (i \u03b4\u22121 + \u2113 u \u2212 1) \u2212 1 symbols so that\n|X i1 \u222a \u00b7 \u00b7 \u00b7 \u222a X i \u03b4\u22121 \u222a X i \u03b4 | = \u2113 u + \u03b4\u22121 x=2 (i x \u2212 i x\u22121 ) + \u2206 + i 1 \u2212 (i \u03b4\u22121 + \u2113 u \u2212 1) \u2212 1 = \u2206.\nThus, there is nothing to prove in this case.\n\nOn the other hand, suppose that 1 \u2264 \u03b1 \u2264 k is the least value such that i \u03b1 \u2212 i \u03b1\u22121 \u2265 \u2113 u . In this case, we know from the above claim that |X i1 \u222a \u00b7 \u00b7 \u00b7 \u222a X i\u03b1\u22121 | \u2265 \u2113 u + \u03b1 \u2212 2. It follows that X i\u03b1 , . . . X i k each contribute at least one new symbol, namely i \u03b1 , . . . , i k . Therefore |X i1 \u222a \u00b7 \u00b7 \u00b7 \u222a X i k | \u2265 \u2113 u + \u03b1 \u2212 2 + k \u2212 \u03b1 + 1 = \u2113 u + k \u2212 1.\n\nThus, if we think about choosing k workers, then we need to ensure that \u2113 u + (k \u2212 1) + k(\u2113 \u2212 \u2113 u ) \u2265 \u2206 which further implies k \u2265 n \u2212 \u2113 u + 1 \u2113 \u2212 \u2113 u + 1 = n \u2212 n\u03b3 u + 1 n\u03b3 \u2212 n\u03b3 u + 1 as n = \u2206. So, if the system is resilient to s stragglers then s \u2264 n \u2212 n \u2212 n\u03b3 u + 1 n\u03b3 \u2212 n\u03b3 u + 1 = n 2 \u03b3 c + n\u03b3 u \u2212 1 n\u03b3 c + 1 .\n\nIt should be noted that setting \u03b3 c = 0 leads to the uncoded case which is resilient to (n\u03b3 \u2212 1) workers (same as setting \u03b2 = 1 in Theorem 2).\n\n\nD. Proof of Theorem 8\n\nProof. To prove the theorem by contradiction, we assume that there exists an unknown A T i B j , which cannot be decoded from a particular set of \u03c4 workers where \u03c4 is defined in the theorem statement. We consider the set Next, partition the workers of T j into \u2113 B = mb 1 worker groups, within each of which, all a 2 uncoded blockcolumns of A appear in a cyclic fashion. From the proof of Theorem 7, we know that any k workers within a group will provide min(a u + k \u2212 1, a 2 ) uncoded symbols corresponding to B i . Now we have \u2113 B such worker groups which indicates that we have \u2113 B workers of T j which have the same uncoded job assignments. Thus, from any \u03ba workers of T j , we will obtain min(a u + \u2308 \u03ba mb1 \u2309 \u2212 1, a 2 ) uncoded symbols, and \u03baa c coded symbols. So in order to be able to decode the elements of B j , we need to find the minimum positive integer for \u03ba (which is denoted as \u03ba min ) such that a u + \u03ba mb 1 \u2212 1 + \u03baa c \u2265 a 2 .\n\nIt indicates that any \u03ba min workers of T j are enough to recover all the elements of B j including A T i B j . But \u03c4 \u2212 |S j | = \u03ba min , which leads to a contradiction and hence concludes the proof.\n\n\nE. Concatenation of block rows in\u1e7c \u2299\u0168\n\nLet U denote a \u2113 c k A \u00d7\u2113 c k A k B matrix whose \u03b4-th row is given by [u where we recall that each entry of U is chosen i.i.d. at random from a continuous distribution and k = k A k B . The matrix U can be written as\nU = [U 0 | U 1 | . . . | U k\u22121 ]\nwhere each U j is of dimension \u2113 c k A \u00d7 \u2113 c . We wish to show that\nU 0 \u2297 v (i0) | U 1 \u2297 v (i1) | . . . | U k\u22121 \u2297 v (i k\u22121 )\nis full-rank with probability 1.\n\nNote that the vectors v (i \u2113 ) 's are also chosen at random and any collection of k B such vectors is full rank with probability 1. In the argument below we show a specific choice of U that yields a full-rank matrix. This implies that the matrix continues to be full-rank under the random choice. Towards this end, we pick the first \u2113 c rows of U to be I \u2113c . . . I \u2113c 0 . . . 0 , i.e., the first k B block-columns are identity matrices. It can be seen that these result in \u2113 c k B linearly independent rows. The next block row of U is a k B block-column shifted version of the first block row, i.e., it is 0 . . . 0 I \u2113c . . . I \u2113c 0 . . . 0\n\nThis yields another \u2113 c k B linearly independent block rows. This process can be repeated k A times to provide the required result.\n\n\nF. Number of A i 's that appear less than k B times within the stragglers\n\nIn the setting of Theorem 6, suppose that we have n\u2212 k A k B stragglers that together contain (n\u2212 k A k B )\u2206 A k B /n uncoded block-columns of A. We want to show that not all A i 's appear k B times within the stragglers. To see this consider a bipartite graph that specifies the placement of the uncoded block-columns of A. It contains vertices denoting the A i 's and the worker nodes. An edge connects A i and W j if A i appears in W j . Thus each A i has degree k B . It can be seen that this graph is connected as any two neighboring workers W j and W j+1 (indices reduced modulo-n) have block-columns in common. Suppose that the stragglers are such that each A i that appears within the stragglers also appears k B times within the stragglers. This implies that the subgraph induced by the stragglers is such that it disconnected from the remaining workers. This is a contradiction.\n\n\nThis work was supported in part by the National Science Foundation (NSF) under grants CCF-1718470 and CCF-1910840. The material in this work has appeared in part at the 2021 IEEE International Symposium on Information Theory, Melbourne, Australia and at the 2018 IEEE Information Theory Workshop (ITW), Guangzhou, China.\n\nFig. 1 :\n1Variation of worker speeds for the same job over 100 runs across 40 workers within AWS; the job involves multiplying two random matrices of size 4000 \u00d7 4000 twice. The average time is shown by the small circle for each worker. The upper and lower edges indicate the maximum and minimum time over the 100 runs. The required time exhibits a wide variation from 5.85 seconds to 8.71 seconds.\n\n\n\u22121,0 . . . A p\u22121,\u22121,0 . . . B p\u22121,\n\nExample 3 .\n3Let X = {0, 1, 2, 3} and A = {{0, 1}, {0, 2}, {0, 3}, {1, 2}, {1, 3}, {2, 3}}. Now (X , A) forms a resolvable design with parallel classes, P 0 = {{0, 1}, {2, 3}}, P 1 = {{0, 2}, {1, 3}} and P 2 = {{0, 3}, {1, 2}}.\n\nFig. 4 :\n4Partitioning matrix A into five submatrices and assigning three uncoded tasks in a cyclic fashion to the workers. The system is resilient to two stragglers and Q = 10. The tasks enclosed in dots can be processed without processing any copy ofA T 4 x.\n\nExample 4 .\n4Consider an example of computing A T x, where we have n = 5 workers and each worker can process \u03b3 = 3/5 fraction of the total job. We partition A into \u2206 = 5 block-columns: A 0 , A 1 , . . . , A 4 . Let X = {0, 1, 2, 3, 4}.\n\nFig. 5 :\n5Job assignment for worker group G0 for \u03b2-level matrix-vector multiplication scheme for n = 12 with \u03b3A = 1 4 and \u2206A = 12 using a single parallel class with \u03b2 = 3. The indices {i, j, k} indicates a random linear combination of the submatrices Ai, Aj and A k . G1 and G2 are assigned the same symbols as workers 0 \u2212 3 but with different random coefficients.\n\nExample 5 .\n5Consider a scenario with n = 12, \u03b3 = 1/4 and \u03b2 = 3, and set \u2206 = 12. We let X = {0, 1, . . . , 11} and pick P = {{0, 1, 2}, {3, 4, 5}, {6, 7, 8}, {9, 10, 11}}. In this example, all three groups use the same parallel class P. As shown in Fig. 5, in each group the meta-symbols are arranged in a cyclic fashion. For each metasymbol a random linear combination is chosen, e.g. in worker W 0 the meta symbol {0, 1, 2} will be replaced by Algorithm 1: \u03b2-level coding scheme for distributed matrix-vector multiplication Input : Matrix A and vector x. Storage fraction \u03b3 = a1 a2 \u2264 1 \u03b2 , \u03b2-allowed coding level, and number of workers n = ca 2 where c is a positive integer.\n\n\n. 5 is resilient to s = c\u2113 \u2212 \u03b2 = 3 \u00d7 3 \u2212 3 = 6 stragglers and has Q = 25. Theorem 2 provides the value for s and Q for distributed matrix-vector multiplication when \u03b2 \u2264 c. In Appendix A, we show the calculation for s and Q for the case when \u03b2 > c.\n\n\n1, 2, . . . , \u2206 A \u2212 1} and find parallel classes P A i having block size \u03b2 A , i = 0, 1, . . . , c \u2212 1;4 Assume X B = {0, 1, 2, . . . , \u2206 B \u2212 1} and find parallel classes P B i having block size \u03b2 B , i = 0, 1, . . . , c \u2212 1;5 for i \u2190 0 to c \u2212 1 do 6 Let the blocks of P A i be denoted as p A0 , p A1 , . . . , p A\u03b1 A \u22121 ; 7Let the blocks of P B i be denoted as p B0 , p B1 , . . . , p B\u03b1 B \u22121 ;8 for j \u2190 0 to \u2206 \u03b2 \u2212 1 do 9Assign sets p Aj , p Aj+1 , . . . , p A j+\u2113 A \u22121 from top to bottom (indices reduced modulo a 2 ) to worker \u2206 \u03b2 i + j; 10 k \u2190 \u230a j a2 \u230b, and assign sets p B k , p B k+1 , . . . , p B k+\u2113 B \u22121 from top to bottom (indices reduced modulo b 2 ) to worker \u2206 \u03b2 i + j;11 Choose random linear combinations of the constituent block-columns of the meta-symbols of P A i and P B i of length \u03b2 A and \u03b2 B respectively;\n\nFig. 7 :\n7Job assignment for worker group G0 for \u03b2-level matrix-matrix multiplication scheme with n = 36 with \u03b3A = \u03b3B = 1 3 and \u2206A = \u2206B = 6 using a single parallel class with \u03b2A = \u03b2B = 2. The indices {i, j} on top and bottom parts indicate random linear combinations of the submatrices of A and B, respectively. G1, G2 and G3 are assigned the same symbols as workers W0 \u2212 W8, but with different random coefficients.\n\n\nthe set of unknowns. The product of two assigned coded block-columns consists of a random linear combination of \u03b2 = \u03b2 A \u03b2 B unknowns from X AB .\n\nExample 7 .\n7We consider an example with n = 36 workers inFig. 7, each of which can store \u03b3 A = \u03b3 B = 1 3 of each of matrices A and B, and \u03b2 A = \u03b2 B = 2. We set \u2206 A = \u2206 B = 6, thus the cardinality of X AB is 36. Interms of indices, we use the same parallel class, {{0, 1}, {2, 3}, {4, 5}} for both A and B. Finally we use random vectors of length \u03b2 A = \u03b2 B = 2 to obtain the symbols from the submatrices of the elements of the parallel classes, P A i and P B i in any worker group G i , for i = 0, 1, 2, 3, as c = 36/9 = 4.\n\nLemma 3 .\n3The matrix-matrix multiplication scheme in Alg. 2 is such that there are \u2113 = \u2113 A \u2113 B symbols corresponding to any product meta-symbol \u2208 P AB i in a group G i . Furthermore, this product meta-symbol appears in all locations 0, 1, 2, . . . , \u2113 \u2212 1 within G i .Proof. Any group G i can be partitioned into \u03b1 B = \u2206B \u03b2B disjoint subgroups each of which consists of \u03b1 A = \u2206A \u03b2A workers. These subgroups are denoted as H j where in terms of group worker indices, H j = {j\u03b1 A , j\u03b1 A +1, . . . , (j + 1)\u03b1 A \u2212 1}, for j = 0, 1, . . . , \u03b1 B \u2212 1.\n\nTheorem 6 .\n6Let \u2113 A \u2264 \u2206A 2 \u2212 2 and \u2206 A \u2265 8. If we use the parallel classes in (5) for encoding A, then the matrix-matrix multiplication scheme described in Alg. 2 will be resilient to s = 2\u2113 \u2212 1 stragglers, when \u03b2 A = 2 and \u03b2 B = 1 such that c = \u03b2 = 2.Proof. Consider the set B m = {A T 0 B m , A T 1 B m , . . . , A T \u2206A\u22121 B m }, i.e., the set of all unknowns corresponding to B m , for m = 0, 1, . . . , \u2206 B \u2212 1, so |B m | = \u2206 A . As B is uncoded, the equations consisting of the unknowns in B m are disjoint of the equations consisting of the unknowns of B p , (m = p). Thus, we can form G m dec using\n\nFig. 8 :\n8Uncoded matrix-matrix multiplication with n = 12 and s = 5 with \u03b3A = 2 3 and \u03b3B = 3 4 where \u2206A = 3 and \u2206B = 4. the unknowns corresponding to the set B m and analyze the decoding using it. The rest of the argument follows analogous to the proof of Theorem 4. C. Coded at bottom scheme Intuitively, the \u03b2-level coding schemes can be improved if we allow for the inclusion of some densely coded block-columns. We now consider a variant of the uncoded scheme where such densely coded block-columns are added at the end of uncoded computations. This improves both the straggler resilience and the Q value of the scheme.\n\nFig. 9 :\n9Partitioning matrix A into five submatrices and assigning two uncoded and one coded task to each of the five workers.\n\n3\nDefine T = {i, i + 1, . . . , i + a u \u2212 1} (mod \u2206 A ); 4Assign all A m 's sequentially from top to bottom to worker node i, where m \u2208 T ;5 Assign a c different random linear combinations of A m 's for m / \u2208 T ; 6 j \u2190 \u230a i a2 \u230b and assign B j , B j+1 , . . . , B j+mb1\u22121 from top to bottom (subscripts reduced modulo \u2206 B ) to worker node i; 7 endOutput : Cyclic coded at the bottom scheme for matrix-matrix multiplication.\n\nFig. 10 :\n10Coded matrix-matrix multiplication with n = 12 with \u03b3Au = 1 3 , \u03b3Ac = 1 3 and \u03b3B = 3 4 where \u2206A = 3 and \u2206B = 4. The coded submatrix for A assigned to Wi is denoted as Ci.\n\n\nma 2 b 1 + \u03ba min ,where \u03ba min is the minimum positive integer for \u03ba satisfying the inequality \u03ba mb 1 + \u03baa c \u2265 a 2 \u2212 a u + 1.\n\n\nNow for the uncoded case, according to Theorem 5, the recovery threshold is \u03c4 = n \u2212 (c\u2113 \u2212 \u03b2) = 12 \u2212(1 \u00d7 \u2113 A \u2113 B \u2212 1) = 7.On the other hand, according to Theorem 8, the recovery threshold for the coded case is,Algorithm 5: SCS Optimal scheme for matrix-vector multiplication Input : Matrix A and vector x, n-number of worker nodes, storage fraction \u03b3 A = 1 kA . 1 Set \u2206 = LCM(n, k A ). Partition A into \u2206 block-columns A 0 , A 1 , . . . , A \u2206\u22121 ; 2 Number of coded submatrices of A in each worker node, \u2113 c = \u2206 kA \u2212 \u2206 n ; 3 for i \u2190 0 to n \u2212 1 do 4 u \u2190 i \u00d7 \u2206 n ; 5 Define T = u, u + 1, . . . , u + \u2206 n \u2212 1 (mod \u2206); 6 Assign all A m 's sequentially from top to bottom to worker node i, where m \u2208 T ; 7 Assign \u2113 c different random linear combinations of A m 's for m / \u2208 T ; 8 end\n\n\nk B times over all n workers. In what follows we show that each of these block products corresponds to a linearly independent equation where the variables are A T i B j for i = 0, 1, . . . , \u2206 A \u2212 1, j = 0, 1, . . . , \u2206 B \u2212 1. Let e i denote the i-th unit vector of length Algorithm 6: SCS Optimal scheme for distributed matrix-matrix multiplication Input : Matrices A and B, n-number of worker nodes, storage fraction \u03b3 A = 1 kA and \u03b3 B = 1 kB . So, s = n \u2212 k A k B . 1 Set \u2206 A = LCM(n, k A ) and \u2206 B = k B ; 2 Partition A and B into \u2206 A and \u2206 B block-columns, and \u2206 = \u2206 A \u2206 B ; 3 Number of coded submatrices of A in each worker node, \u2113 c = \u2206A kA \u2212 \u2206 n ; 4 for i \u2190 0 to n \u2212 1 do 5 u \u2190 i \u00d7 \u2206A n ; 6 Define T = u, u + 1, . . . , u + \u2206 n \u2212 1 (modulo \u2206 A ); 7 Assign all A m 's sequentially from top to bottom to worker node i, where m \u2208 T ; 8 Assign \u2113 c different random linear combinations of A m 's for m / \u2208 T ; 9 Assign a single random linear combination of all block-columns of B; 10 end\n\n\nNow, suppose that we consider a subset of k = k A k B workers indexed by the set I = {i 0 , i 1 , . . . , i k\u22121 }. Within this worker node set, let J i denote the index set of the worker nodes where A i appears uncoded. The random encoding vectors for A and B in worker W \u2113 are denoted by u (\u2113,j) (of length \u2206 A ) for j = 0, 1, . . . , \u2113 c \u2212 1 and v (\u2113) (of length \u2206 B ) respectively.\n\n\u2113\n's not all zero such that\n\n\non the LHS can be chosen freely given the RHS. For a given choice of the \u03ba (j) \u2113 's the above equation can definitely be satisfied if |J i | = k B . If we |J i | < k B then this may not be true depending on the values of the \u03ba (j)\n\n\u2113\n's. But this probability is zero since each of the remaining equations involve random u (\u2113,i) \u03b4 values that have not appeared in the first \u2113 c k A block rows. Theorem 10. Alg. 6 proposes a distributed matrix-matrix multiplication scheme with Q = \u2206 + (k B \u2212 1)\u2113 c .\n\nFig. 12 :\n12Matrix-matrix multiplication with n = 5 and s = 1 with \u03b3A = \u03b3B = 1 2 .\n\n\u2113\n\u2212 1, . . . , \u2113 \u2212 1, . . . , c 1, 1, . . . , 1) .\n\n\nB j = {A T 0 B j , A T 1 B j , . . . , A T i B j , . . . , A T \u2206A\u22121 B j }, i.e,the set of all unknowns corresponding to B j , for j = 0, 1, . . . , \u2206 B \u2212 1, thus |B j | = \u2206 A = a 2 . It should be noted that the equations consisting of the unknowns of B j are disjoint with the equations consisting of the unknowns of B m , (j = m) since the assigned submatrices from B are uncoded. Let S j denote the set of workers where B j does not appear in the assignments and T j denote the set of workers where it appears. According to the scheme in Alg. 4, there are ma 2 b 1 workers each of which has an uncoded copy of B j . Thus, |S j | = n \u2212 ma 2 b 1 .\n\nTABLE I :\nIComputation time for sparse matrix multiplication. We choose matrices A and B both of size 10, 000 \u00d7 10, 000. of them have sparsity \u03c3 = 3%, thus randomly chosen 3% entries of A and B are non-zero.Both JOB \nREQUIRED TIME \n\nTO COMPUTE A T B \n9.41 SECONDS \n\nTO COMPUTE (UNCODED) A T \ni Bj \n0.58 SECONDS \n\nTO COMPUTE (CODED)\u00c3 T \n\niBj \n\n2.13 SECONDS \n\n\n\nTABLE II :\nIIComparison between our proposed approaches. Here \u03b3A and \u03b3B indicate the storage fractions for matrices A and B, respectively, and n denotes the total number of workers.APPROACH \nPROPERTIES \nPARAMETER REGIME \nADVANTAGES \n\n\u03b2-LEVEL \nCOMBINE \u03b2 SUBMATRICES \nFOR \u03b3A = a 1 \n\na 2 \n\n\n\nTABLE III :\nIIIComparison with existing works.CODES \nMAT-MAT \nOPTIMAL \nNUMERICAL \nPARTIAL \nSPARSELY \n\nMULT? \nTHRESHOLD? STABILITY? COMPUT? \nCODED? \n\nREPETITION CODES \n\u2713 \n\u2717 \n\u2713 \n\u2717 \n\u2713 \n\nRATELESS CODES [6] \n\u2717 \n\u2717 \n\u2713 \n\u2717 \n\u2717 \n\nPROD. CODES [31], FACTORED CODES [23] \n\u2713 \n\u2717 \n\u2713 \n\u2717 \n\u2717 \n\nPOLYNOMIAL CODES [3] \n\u2713 \n\u2713 \n\u2717 \n\u2717 \n\u2717 \n\nBIV. HERMITIAN POLY. CODE [32] \n\u2713 \n\u2713 \n\u2717 \n\u2713 \n\u2717 \n\nDYNAMIC HETERO.-AWARE CODE [29] \n\u2717 \n\u2717 \n\u2713 \n\u2713 \n\u2717 \n\nORTHOPOLY [21], RKRP CODE[17] \n\u2713 \n\u2713 \n\u2713 \n\u2717 \n\u2717 \n\nCONV. CODE [10], CIRC. & ROT. MAT. [20] \n\n\nTable IV\nIVcompares experimental results for different matrix-vector multiplication approaches in terms of number of stragglers and Q/\u2206 values. For every case, we observe a significant improvement of the metrics if we incorporate multiple parallel classes instead of a single parallel class. We note here that the Q/\u2206 was computed via computer experiments.\n\nTABLE IV :\nIVComparison of different metrics for different approaches.SYSTEM \nMETRICS \nDENSE CODES \n\u03b2-LEVEL CODING \n\u03b2-LEVEL CODING \n\n[3], [17], [21] (SINGLE PARALLEL CLASS) (MULTIPLE PARALLEL CLASSES) \n\nn = 8, \u03b3A = 1 \n\n4 \n\ns \n4 \n2 \n3 \n\nAND \u03b2 = 2 \nQ/\u2206 \n\u2212 \n13/8 \n11/8 \n\nn = 8, \u03b3A = 1 \n\n4 \n\ns \n4 \n3 \n4 \n\nAND \u03b2 = 3 \nQ/\u2206 \n\u2212 \n19/12 \n14/12 \n\nn = 10, \u03b3A = 1 \n\n5 \n\ns \n5 \n3 \n4 \n\nAND \u03b2 = 3 \nQ/\u2206 \n\u2212 \n25/15 \n20/15 \n\n\n\n\ny 1 are chosen i.i.d. at random from a continuous distribution. Thus, the coefficients of the corresponding equation can be expressed as\n\nTABLE V :\nVComparison of number of stragglers, Q values, worker computation time (in ms) and worst case conditionnumber(\u03baworst) for matrix-vector multiplication for n = 30 and \u03b3A = 1 \n10 (*for convolutional code, we assumed s = 15). \n\nMETHODS \nSTRAGGLERS \n\nQ \n\n\u2206 VALUE \n\nWORKER COMPUTATION TIME \n\u03baworst \nSPARSITY 98% \nSPARSITY 95% \n\nPOLYNOMIAL CODE [3] \n20 \nN/A \n62.8 \n87.1 \n5.99 \u00d7 10 9 \n\nORTHO-POLY CODE [21] \n20 \nN/A \n62.3 \n86.4 \n4.34 \u00d7 10 11 \n\nRKRP CODE[17] \n20 \nN/A \n62.9 \n86.8 \n5.44 \u00d7 10 8 \n\nCONVOLUTIONAL CODE* [10] \n15 \nN/A \n63.1 \n87.7 \n6.24 \u00d7 10 4 \n\nUNCODED [11] \n2 \n85/30 \n19.1 \n32.9 \n1.7321 \n\nUNCODED (PROPOSED) \n2 \n84/30 \n19.2 \n33.1 \n1.7321 \n\n\u03b2-LEVEL CODING (\u03b2 = 2) \n4 \n81/30 \n25.3 \n40.2 \n242.89 \n\n\u03b2-LEVEL CODING (\u03b2 = 3) \n6 \n79/30 \n29.2 \n47.9 \n1.53 \u00d7 10 3 \n\nCODED AT BOTTOM \n15 \n58/30 \n24.1 \n37.8 \n1.41 \u00d7 10 3 \n\n\n\nTABLE VI :\nVIComparison of number of stragglers, Q values, worker computation time (in seconds) and worst case condition \n\nnumber (\u03baworst) for matrix-matrix multiplication for n = 18 and \u03b3A = \u03b3B = 1 \n3 (*for convolutional code, kA = kB = 4). \n\nMETHODS \nSTRAGGLERS Q \n\n\u2206 VALUE \n\nWORKER COMPUTATION TIME \n\u03baworst \nSPARSITY 98% SPARSITY 95% \n\nPOLYNOMIAL CODE [3] \n9 \nN/A \n2.58 \n10.16 \n7.33 \u00d7 10 6 \n\nORTHO-POLY CODE [21] \n9 \nN/A \n2.51 \n10.08 \n1.33 \u00d7 10 7 \n\nRKRP CODE[17] \n9 \nN/A \n2.63 \n10.23 \n2.15 \u00d7 10 5 \n\nCONVOLUTIONAL CODE* [10] \n2 \nN/A \n2.44 \n10.19 \n1.82 \u00d7 10 3 \n\nUNCODED (PROPOSED) \n1 \n17/9 \n0.69 \n1.96 \n1.41 \n\n\u03b2-LEVEL CODING (\u03b2A = \u03b2B = 2) \n4 \n16/9 \n1.02 \n3.68 \n8.89 \u00d7 10 3 \n\n\n\nTABLE VII :\nVIIComparison of Q values, worker computation time (in ms) and worst case condition number (\u03baworst) formatrix-vector multiplication for n = 18, \u03b3A = 1 \n15 (*for convolutional code, we assume \u03b3A = 1 \n10 ). \n\nMETHODS \nNO OF STRAGGLERS Q \n\n\u2206 VALUE \n\nWORKER COMPUTATION TIME \n\u03baworst \nBAND \nRANDOM \n\nPOLYNOMIAL CODE [3] \n3 \nN/A \n29.7 \n30.2 \n4.03 \u00d7 10 7 \n\nORTHO-POLY CODE [21] \n3 \nN/A \n30.1 \n29.8 \n2.13 \u00d7 10 4 \n\nRKRP CODE[17] \n3 \nN/A \n29.3 \n30.0 \n6.35 \u00d7 10 3 \n\nCONVOLUTIONAL CODE* [10] \n3 \nN/A \n35.2 \n34.7 \n1.21 \u00d7 10 3 \n\nSCS OPTIMAL SCHEME \n3 \n1 \n14.8 \n20.3 \n6.81 \u00d7 10 4 \n\n\n\nTABLE VIII :\nVIIIThe values in the parentheses for the SCS optimal scheme shows the time required for uncoded and coded portions, respectively.Comparison of Q values, worker computation time (in seconds) and worst case condition number (\u03baworst) for \n\nmatrix-matrix multiplication for n = 24, \u03b3A = 1 \n4 and \u03b3B = 1 \n5 (*for convolutional code, we assume \u03b3A = 2 \n5 and \u03b3B = 1 \n3 ). METHODS \nNO OF STRAGGLERS Q \n\n\u2206 VALUE \n\nWORKER COMPUTATION TIME \n\u03baworst \nSPARSITY 98% SPARSITY 95% \n\nPOLYNOMIAL CODE [3] \n4 \nN/A \n3.11 \n8.29 \n2.40 \u00d7 10 10 \n\nORTHO-POLY CODE [21] \n4 \nN/A \n3.08 \n8.16 \n1.96 \u00d7 10 6 \n\nRKRP CODE[17] \n4 \nN/A \n3.15 \n8.22 \n2.83 \u00d7 10 5 \n\nCONVOLUTIONAL CODE* [10] \n4 \nN/A \n5.16 \n10.92 \n2.65 \u00d7 10 4 \n\nSCS OPTIMAL SCHEME \n4 \n7/6 \n1.93 \n4.76 \n4.93 \u00d7 10 6 \n(0.91 + 1.02) \n(3.71 + 1.05) \n\n\n\nTABLE IX :\nIXComparison of the Q/\u2206 values, worker computation time (in seconds) and worst case condition numbers for matrix-matrix multiplication for n = 10, \u03b3A = \u03b3B = 1 3 , so s = 1.submatrices even denser, which will lead to higher worker computation time for the workers. The case is similar for the work in[32] which uses larger \u2206 A and \u2206 B to utilize the partial computations. On the other hand, in the proposed SCS optimal scheme, uncoded submatrices are placed at the top, and coded submatrices are placed at the bottom. Moreover the coded jobs are allocated uniformly among all the workers which does not let the worker computation time go high for any particular worker.METHODS \nW/O PARTIAL COMPUTATIONS \nW/ PARTIAL COMPUTATIONS \n\nQ \n\n\u2206 VALUE \n\n\u03baworst \nWORKER TIME \n\nQ \n\n\u2206 VALUE \n\n\u03baworst \nWORKER TIME \n\nPOLY CODE [3] \nN/A \n8.8 \u00d7 10 3 \n2.46 \n1 \n1.86 \u00d7 10 13 \n7.09 \n\nORTHO-POLY [21] \nN/A \n16.66 \n2.49 \n1 \n4.33 \u00d7 10 5 \n7.06 \n\nRKRP CODE[17] \nN/A \n11.96 \n2.41 \n1 \n1.16 \u00d7 10 4 \n7.14 \n\nSCS OPTIMAL SCHEME \n-\n-\n-\n1.02 \n2.15 \u00d7 10 3 \n2.04 \n\n\nSeptember 27, 2021DRAFT\nA general formulation need not restrict the assignment to coded submatrices of A and B. Nevertheless, all known schemes thus far and our proposed schemes work with equal-sized submatrices, so we present the formulation in this way.September 27, 2021   DRAFT\n\nSpeeding up distributed machine learning using codes. K Lee, M Lam, R Pedarsani, D Papailiopoulos, K Ramchandran, IEEE Trans. on Info. Th. 643K. Lee, M. Lam, R. Pedarsani, D. Papailiopoulos, and K. Ramchandran, \"Speeding up distributed machine learning using codes,\" IEEE Trans. on Info. Th., vol. 64, no. 3, pp. 1514-1529, 2018.\n\nShort-dot: Computing large linear transforms distributedly using coded short dot products. S Dutta, V Cadambe, P Grover, Proc. of Adv. in Neur. Inf. Proc. Syst. (NIPS). of Adv. in Neur. Inf. . Syst. (NIPS)S. Dutta, V. Cadambe, and P. Grover, \"Short-dot: Computing large linear transforms distributedly using coded short dot products,\" in Proc. of Adv. in Neur. Inf. Proc. Syst. (NIPS), 2016, pp. 2100-2108.\n\nPolynomial codes: an optimal design for high-dimensional coded matrix multiplication. Q Yu, M Maddah-Ali, S Avestimehr, Proc. of Adv. in Neur. Inf. Proc. Syst. (NIPS. of Adv. in Neur. Inf. . Syst. (NIPSQ. Yu, M. Maddah-Ali, and S. Avestimehr, \"Polynomial codes: an optimal design for high-dimensional coded matrix multiplication,\" in Proc. of Adv. in Neur. Inf. Proc. Syst. (NIPS), 2017, pp. 4403-4413.\n\nGradient coding: Avoiding stragglers in distributed learning. R Tandon, Q Lei, A G Dimakis, N Karampatziakis, Proc. of Intl. Conf. on Machine Learning (ICML). of Intl. Conf. on Machine Learning (ICML)R. Tandon, Q. Lei, A. G. Dimakis, and N. Karampatziakis, \"Gradient coding: Avoiding stragglers in distributed learning,\" in Proc. of Intl. Conf. on Machine Learning (ICML), 2017, pp. 3368-3376.\n\nExploitation of stragglers in coded computation. S Kiani, N Ferdinand, S C Draper, IEEE Intl. Symposium on Info. Th. S. Kiani, N. Ferdinand, and S. C. Draper, \"Exploitation of stragglers in coded computation,\" in IEEE Intl. Symposium on Info. Th., 2018, pp. 1988-1992.\n\nRateless codes for near-perfect load balancing in distributed matrixvector multiplication. A Mallick, M Chaudhari, U Sheth, G Palanikumar, G Joshi, Proceedings of the ACM on Meas. and Analysis of Comp. Syst. 33A. Mallick, M. Chaudhari, U. Sheth, G. Palanikumar, and G. Joshi, \"Rateless codes for near-perfect load balancing in distributed matrix- vector multiplication,\" Proceedings of the ACM on Meas. and Analysis of Comp. Syst., vol. 3, no. 3, pp. 1-40, 2019.\n\nCoded sparse matrix multiplication. S Wang, J Liu, N Shroff, Proc. of Intl. Conf. on Machine Learning (ICML). of Intl. Conf. on Machine Learning (ICML)S. Wang, J. Liu, and N. Shroff, \"Coded sparse matrix multiplication,\" in Proc. of Intl. Conf. on Machine Learning (ICML), 2018, pp. 5152--5160.\n\nHierarchical coded matrix multiplication. S Kianidehkordi, N Ferdinand, S C Draper, IEEE Trans. on Info. Th. 672S. Kianidehkordi, N. Ferdinand, and S. C. Draper, \"Hierarchical coded matrix multiplication,\" IEEE Trans. on Info. Th., vol. 67, no. 2, pp. 726-754, 2021.\n\nDistributed gradient descent with coded partial gradient computations. E Ozfatura, S Ulukus, D G\u00fcnd\u00fcz, 2019 IEEE International Conference on Acoustics, Speech and Signal Processing. E. Ozfatura, S. Ulukus, and D. G\u00fcnd\u00fcz, \"Distributed gradient descent with coded partial gradient computations,\" in 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 3492-3496.\n\nEfficient and robust distributed matrix computations via convolutional coding. A B Das, A Ramamoorthy, N Vaswani, IEEE Transactions on Information Theory. 679A. B. Das, A. Ramamoorthy, and N. Vaswani, \"Efficient and robust distributed matrix computations via convolutional coding,\" IEEE Transactions on Information Theory, vol. 67, no. 9, pp. 6266-6282, 2021.\n\nC 3 LES : Codes for coded computation that leverage stragglers. A B Das, L Tang, A Ramamoorthy, IEEE Info. Th. Workshop. A. B. Das, L. Tang, and A. Ramamoorthy, \"C 3 LES : Codes for coded computation that leverage stragglers,\" in IEEE Info. Th. Workshop, 2018.\n\nUniversally decodable matrices for distributed matrix-vector multiplication. A Ramamoorthy, L Tang, P O Vontobel, IEEE Intl. Symposium on Info. Th. A. Ramamoorthy, L. Tang, and P. O. Vontobel, \"Universally decodable matrices for distributed matrix-vector multiplication,\" in IEEE Intl. Symposium on Info. Th., July 2019, pp. 1777-1781.\n\nStraggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding. Q Yu, M A Maddah-Ali, A S Avestimehr, IEEE Trans. on Info. Th. 663Q. Yu, M. A. Maddah-Ali, and A. S. Avestimehr, \"Straggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding,\" IEEE Trans. on Info. Th., vol. 66, no. 3, pp. 1920-1933, 2020.\n\nOn the optimal recovery threshold of coded matrix multiplication. S Dutta, M Fahim, F Haddadpour, H Jeong, V Cadambe, P Grover, IEEE Trans. on Info. Th. 661S. Dutta, M. Fahim, F. Haddadpour, H. Jeong, V. Cadambe, and P. Grover, \"On the optimal recovery threshold of coded matrix multiplication,\" IEEE Trans. on Info. Th., vol. 66, no. 1, pp. 278-301, 2019.\n\nErasure coding for distributed matrix multiplication for matrices with bounded entries. L Tang, K Konstantinidis, A Ramamoorthy, IEEE Communications Letters. 231L. Tang, K. Konstantinidis, and A. Ramamoorthy, \"Erasure coding for distributed matrix multiplication for matrices with bounded entries,\" IEEE Communications Letters, vol. 23, no. 1, pp. 8-11, 2019.\n\nStraggler-resistant distributed matrix computation via coding theory: Removing a bottleneck in large-scale data processing. A Ramamoorthy, A B Das, L Tang, IEEE Sig. Proc. Mag. 373A. Ramamoorthy, A. B. Das, and L. Tang, \"Straggler-resistant distributed matrix computation via coding theory: Removing a bottleneck in large-scale data processing,\" IEEE Sig. Proc. Mag., vol. 37, no. 3, pp. 136-145, 2020.\n\nRandom Khatri-Rao-product codes for numerically-stable distributed matrix multiplication. A M Subramaniam, A Heidarzadeh, K R Narayanan, 57th Annual Conf. on Comm., Control, and Computing (Allerton). A. M. Subramaniam, A. Heidarzadeh, and K. R. Narayanan, \"Random Khatri-Rao-product codes for numerically-stable distributed matrix multiplication,\" in 57th Annual Conf. on Comm., Control, and Computing (Allerton), Sep. 2019, pp. 253-259.\n\nDistributed matrix-vector multiplication: A convolutional coding approach. A B Das, A Ramamoorthy, IEEE Intl. Symposium on Info. Th. A. B. Das and A. Ramamoorthy, \"Distributed matrix-vector multiplication: A convolutional coding approach,\" in IEEE Intl. Symposium on Info. Th., July 2019, pp. 3022-3026.\n\nStraggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding. Q Yu, M A Maddah-Ali, A S Avestimehr, IEEE Intl. Symposium on Info. Th. Q. Yu, M. A. Maddah-Ali, and A. S. Avestimehr, \"Straggler mitigation in distributed matrix multiplication: Fundamental limits and optimal coding,\" in IEEE Intl. Symposium on Info. Th., 2018, pp. 2022-2026.\n\nNumerically stable coded matrix computations via circulant and rotation matrix embeddings. A Ramamoorthy, L Tang, preprintA. Ramamoorthy and L. Tang, \"Numerically stable coded matrix computations via circulant and rotation matrix embeddings,\" preprint, 2019, [Online] Available: https://arxiv.org/abs/1910.06515.\n\nNumerically stable polynomially coded computing. M Fahim, V R Cadambe, IEEE Intl. Symposium on Info. Th. M. Fahim and V. R. Cadambe, \"Numerically stable polynomially coded computing,\" in IEEE Intl. Symposium on Info. Th., July 2019, pp. 3017-3021.\n\nProduct lagrange coded computing. A M Subramaniam, A Heidarzadeh, A K Pradhan, K R Narayanan, IEEE Intl. Symposium on Info. Th. A. M. Subramaniam, A. Heidarzadeh, A. K. Pradhan, and K. R. Narayanan, \"Product lagrange coded computing,\" in IEEE Intl. Symposium on Info. Th., 2020, pp. 197-202.\n\nFactored LT and factored raptor codes for large-scale distributed matrix multiplication. A K Pradhan, A Heidarzadeh, K R Narayanan, IEEE Journal on Selected Areas in Information Theory. A. K. Pradhan, A. Heidarzadeh, and K. R. Narayanan, \"Factored LT and factored raptor codes for large-scale distributed matrix multiplication,\" IEEE Journal on Selected Areas in Information Theory, 2021.\n\nCodes for the m-metric. M Y Rosenbloom, M A Tsfasman, Probl. Inf. Transm. 3312021M. Y. Rosenbloom and M. A. Tsfasman, \"Codes for the m-metric,\" Probl. Inf. Transm., vol. 33, no. 1, pp. 45-52, 1997. September 27, 2021 DRAFT\n\nOn the existence of universally decodable matrices. A Ganesan, P O Vontobel, IEEE Trans. on Info. Th. 537A. Ganesan and P. O. Vontobel, \"On the existence of universally decodable matrices,\" IEEE Trans. on Info. Th., vol. 53, no. 7, pp. 2572-2575, 2007.\n\nD Stinson, Combinatorial designs: Constructions and Analysis. Springer Science & Business MediaD. Stinson, Combinatorial designs: Constructions and Analysis. Springer Science & Business Media, 2007.\n\nPrivate and secure distributed matrix multiplication with flexible communication load. M Aliasgari, O Simeone, J Kliewer, IEEE Transactions on Information Forensics and Security. 15M. Aliasgari, O. Simeone, and J. Kliewer, \"Private and secure distributed matrix multiplication with flexible communication load,\" IEEE Transactions on Information Forensics and Security, vol. 15, pp. 2722-2734, 2020.\n\nSpeeding up private distributed matrix multiplication via bivariate polynomial codes. B Hasircioglu, J G\u00f3mez-Vilardeb\u00f3, D Gunduz, preprintB. Hasircioglu, J. G\u00f3mez-Vilardeb\u00f3, and D. Gunduz, \"Speeding up private distributed matrix multiplication via bivariate polynomial codes,\" preprint, 2021, [Online] Available https://arxiv.org/abs/2102.08304.\n\nDynamic heterogeneity-aware coded cooperative computation at the edge. Y Keshtkarjahromi, Y Xing, H Seferoglu, 2018 IEEE 26th International Conference on Network Protocols (ICNP). Y. Keshtkarjahromi, Y. Xing, and H. Seferoglu, \"Dynamic heterogeneity-aware coded cooperative computation at the edge,\" in 2018 IEEE 26th International Conference on Network Protocols (ICNP), pp. 23-33.\n\nAdaptive coding for matrix multiplication at edge networks. E Vedadi, H Seferoglu, preprintE. Vedadi and H. Seferoglu, \"Adaptive coding for matrix multiplication at edge networks,\" preprint, 2021, [Online] Available: https://arxiv.org/abs/2103.04247.\n\nHigh-dimensional coded matrix multiplication. K Lee, C Suh, K Ramchandran, IEEE Intl. Symposium on Info. Th. K. Lee, C. Suh, and K. Ramchandran, \"High-dimensional coded matrix multiplication,\" in IEEE Intl. Symposium on Info. Th., 2017, pp. 2418-2422.\n\nBivariate hermitian polynomial coding for efficient distributed matrix multiplication. B Has\u0131rc\u0131oglu, J G\u00f3mez-Vilardeb\u00f3, D G\u00fcnd\u00fcz, 2020 IEEE Global Communications Conference (GLOBECOM). B. Has\u0131rc\u0131oglu, J. G\u00f3mez-Vilardeb\u00f3, and D. G\u00fcnd\u00fcz, \"Bivariate hermitian polynomial coding for efficient distributed matrix multiplication,\" in 2020 IEEE Global Communications Conference (GLOBECOM).\n\nCombinatorial theory. M Hall, John Wiley and SonsM. Hall, Combinatorial theory. John Wiley and Sons, 1998.\n\nKirkman parades. F Cole, Bulletin of the American Mathematical Society. 289F. Cole, \"Kirkman parades,\" Bulletin of the American Mathematical Society, vol. 28, no. 9, pp. 435-437, 1922.\n\nLeveraging Partial Stragglers Codes. Leveraging Partial Stragglers Codes. [Online]. Available: https://github.com/anindyabijoydas/LeveragePartialStragglers\n\nAn introduction to numerical analysis. K E Atkinson, John Wiley and SonsK. E. Atkinson, An introduction to numerical analysis. John Wiley and Sons, 2008.\n", "annotations": {"author": "[{\"end\":193,\"start\":87},{\"end\":321,\"start\":194}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":101},{\"end\":212,\"start\":201}]", "author_first_name": "[{\"end\":94,\"start\":87},{\"end\":100,\"start\":95},{\"end\":200,\"start\":194}]", "author_affiliation": "[{\"end\":192,\"start\":106},{\"end\":320,\"start\":234}]", "title": "[{\"end\":73,\"start\":1},{\"end\":394,\"start\":322}]", "venue": null, "abstract": "[{\"end\":1923,\"start\":510}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2812,\"start\":2809},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2817,\"start\":2814},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2822,\"start\":2819},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2827,\"start\":2824},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2977,\"start\":2974},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3513,\"start\":3510},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4372,\"start\":4369},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4377,\"start\":4374},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4382,\"start\":4379},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4387,\"start\":4384},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4392,\"start\":4389},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4687,\"start\":4684},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5031,\"start\":5027},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5453,\"start\":5449},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5462,\"start\":5458},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10206,\"start\":10203},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12784,\"start\":12781},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12790,\"start\":12786},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12795,\"start\":12792},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12800,\"start\":12797},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12806,\"start\":12802},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12811,\"start\":12808},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12816,\"start\":12813},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12822,\"start\":12818},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12828,\"start\":12824},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12833,\"start\":12830},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12903,\"start\":12899},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13005,\"start\":13002},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13989,\"start\":13985},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14126,\"start\":14122},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14191,\"start\":14187},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14197,\"start\":14193},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14422,\"start\":14419},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14428,\"start\":14424},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14434,\"start\":14430},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14440,\"start\":14436},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17009,\"start\":17006},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17015,\"start\":17011},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17081,\"start\":17077},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17087,\"start\":17083},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17093,\"start\":17089},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":17099,\"start\":17095},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17105,\"start\":17101},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17111,\"start\":17107},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17117,\"start\":17113},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17578,\"start\":17574},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17584,\"start\":17580},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":17675,\"start\":17671},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19577,\"start\":19573},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20890,\"start\":20886},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20896,\"start\":20892},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":21130,\"start\":21126},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21136,\"start\":21132},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22410,\"start\":22406},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22999,\"start\":22995},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30232,\"start\":30231},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30653,\"start\":30652},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33801,\"start\":33797},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33901,\"start\":33897},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":35342,\"start\":35341},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":36318,\"start\":36314},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":39167,\"start\":39163},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":42281,\"start\":42280},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49568,\"start\":49567},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":53027,\"start\":53024},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":55245,\"start\":55241},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":58741,\"start\":58738},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":64727,\"start\":64723},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":64997,\"start\":64993},{\"end\":65072,\"start\":65068},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":65380,\"start\":65376},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":65490,\"start\":65487},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":65496,\"start\":65492},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":65502,\"start\":65498},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":65511,\"start\":65507},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":65908,\"start\":65904},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":66257,\"start\":66253},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":67866,\"start\":67863},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":67872,\"start\":67868},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":67878,\"start\":67874},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":67887,\"start\":67883},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":70123,\"start\":70119},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":70129,\"start\":70125},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":70184,\"start\":70181},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":70875,\"start\":70871},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":71369,\"start\":71366},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":71375,\"start\":71371},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":71384,\"start\":71380},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":72503,\"start\":72499},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":73707,\"start\":73704},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":73715,\"start\":73711},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":73819,\"start\":73815},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":73828,\"start\":73824},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":74255,\"start\":74252},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":74261,\"start\":74257},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":74267,\"start\":74263},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":74445,\"start\":74442},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":74652,\"start\":74648},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":75746,\"start\":75743},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":75754,\"start\":75750},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":75793,\"start\":75789},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":77407,\"start\":77403},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":94823,\"start\":94821},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":107411,\"start\":107407}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":91676,\"start\":91354},{\"attributes\":{\"id\":\"fig_1\"},\"end\":92076,\"start\":91677},{\"attributes\":{\"id\":\"fig_2\"},\"end\":92113,\"start\":92077},{\"attributes\":{\"id\":\"fig_3\"},\"end\":92342,\"start\":92114},{\"attributes\":{\"id\":\"fig_5\"},\"end\":92604,\"start\":92343},{\"attributes\":{\"id\":\"fig_6\"},\"end\":92841,\"start\":92605},{\"attributes\":{\"id\":\"fig_7\"},\"end\":93207,\"start\":92842},{\"attributes\":{\"id\":\"fig_8\"},\"end\":93886,\"start\":93208},{\"attributes\":{\"id\":\"fig_9\"},\"end\":94136,\"start\":93887},{\"attributes\":{\"id\":\"fig_10\"},\"end\":94965,\"start\":94137},{\"attributes\":{\"id\":\"fig_11\"},\"end\":95382,\"start\":94966},{\"attributes\":{\"id\":\"fig_12\"},\"end\":95529,\"start\":95383},{\"attributes\":{\"id\":\"fig_13\"},\"end\":96054,\"start\":95530},{\"attributes\":{\"id\":\"fig_14\"},\"end\":96601,\"start\":96055},{\"attributes\":{\"id\":\"fig_15\"},\"end\":97208,\"start\":96602},{\"attributes\":{\"id\":\"fig_16\"},\"end\":97834,\"start\":97209},{\"attributes\":{\"id\":\"fig_17\"},\"end\":97963,\"start\":97835},{\"attributes\":{\"id\":\"fig_18\"},\"end\":98387,\"start\":97964},{\"attributes\":{\"id\":\"fig_19\"},\"end\":98571,\"start\":98388},{\"attributes\":{\"id\":\"fig_20\"},\"end\":98698,\"start\":98572},{\"attributes\":{\"id\":\"fig_21\"},\"end\":99477,\"start\":98699},{\"attributes\":{\"id\":\"fig_22\"},\"end\":100470,\"start\":99478},{\"attributes\":{\"id\":\"fig_23\"},\"end\":100857,\"start\":100471},{\"attributes\":{\"id\":\"fig_24\"},\"end\":100886,\"start\":100858},{\"attributes\":{\"id\":\"fig_25\"},\"end\":101119,\"start\":100887},{\"attributes\":{\"id\":\"fig_26\"},\"end\":101387,\"start\":101120},{\"attributes\":{\"id\":\"fig_27\"},\"end\":101471,\"start\":101388},{\"attributes\":{\"id\":\"fig_28\"},\"end\":101523,\"start\":101472},{\"attributes\":{\"id\":\"fig_29\"},\"end\":102173,\"start\":101524},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":102533,\"start\":102174},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":102822,\"start\":102534},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":103321,\"start\":102823},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":103679,\"start\":103322},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":104084,\"start\":103680},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":104223,\"start\":104085},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":105048,\"start\":104224},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":105726,\"start\":105049},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":106307,\"start\":105727},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":107095,\"start\":106308},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":108137,\"start\":107096}]", "paragraph": "[{\"end\":3733,\"start\":1942},{\"end\":3848,\"start\":3735},{\"end\":4255,\"start\":3850},{\"end\":5274,\"start\":4257},{\"end\":6957,\"start\":5276},{\"end\":7335,\"start\":6993},{\"end\":7460,\"start\":7337},{\"end\":8200,\"start\":7462},{\"end\":8652,\"start\":8202},{\"end\":9193,\"start\":8654},{\"end\":9962,\"start\":9195},{\"end\":10289,\"start\":9964},{\"end\":10711,\"start\":10291},{\"end\":11232,\"start\":10713},{\"end\":11580,\"start\":11234},{\"end\":11704,\"start\":11582},{\"end\":11883,\"start\":11821},{\"end\":12101,\"start\":12034},{\"end\":12682,\"start\":12103},{\"end\":13269,\"start\":12702},{\"end\":13312,\"start\":13271},{\"end\":13667,\"start\":13434},{\"end\":14367,\"start\":13669},{\"end\":14667,\"start\":14369},{\"end\":15229,\"start\":14669},{\"end\":15436,\"start\":15231},{\"end\":15548,\"start\":15438},{\"end\":16270,\"start\":15550},{\"end\":17126,\"start\":16272},{\"end\":17676,\"start\":17128},{\"end\":18290,\"start\":17678},{\"end\":18381,\"start\":18322},{\"end\":18931,\"start\":18383},{\"end\":19109,\"start\":18933},{\"end\":19392,\"start\":19111},{\"end\":19507,\"start\":19394},{\"end\":19823,\"start\":19509},{\"end\":20287,\"start\":20072},{\"end\":20491,\"start\":20289},{\"end\":20789,\"start\":20493},{\"end\":21411,\"start\":20791},{\"end\":21970,\"start\":21434},{\"end\":22275,\"start\":22061},{\"end\":22306,\"start\":22277},{\"end\":22412,\"start\":22308},{\"end\":22710,\"start\":22574},{\"end\":23000,\"start\":22863},{\"end\":23232,\"start\":23002},{\"end\":23426,\"start\":23234},{\"end\":23513,\"start\":23428},{\"end\":24128,\"start\":23551},{\"end\":24980,\"start\":24297},{\"end\":25048,\"start\":24982},{\"end\":25196,\"start\":25050},{\"end\":25388,\"start\":25198},{\"end\":25591,\"start\":25428},{\"end\":25821,\"start\":25593},{\"end\":26219,\"start\":25867},{\"end\":26660,\"start\":26291},{\"end\":27141,\"start\":26684},{\"end\":27388,\"start\":27143},{\"end\":27710,\"start\":27390},{\"end\":27830,\"start\":27785},{\"end\":27918,\"start\":27886},{\"end\":27961,\"start\":27947},{\"end\":28144,\"start\":27963},{\"end\":28808,\"start\":28365},{\"end\":29107,\"start\":28865},{\"end\":29213,\"start\":29109},{\"end\":29560,\"start\":29215},{\"end\":30278,\"start\":29562},{\"end\":30397,\"start\":30280},{\"end\":30768,\"start\":30519},{\"end\":30849,\"start\":30770},{\"end\":31194,\"start\":30884},{\"end\":31519,\"start\":31196},{\"end\":32276,\"start\":31551},{\"end\":32703,\"start\":32278},{\"end\":33078,\"start\":32829},{\"end\":33312,\"start\":33259},{\"end\":33457,\"start\":33367},{\"end\":33984,\"start\":33459},{\"end\":34205,\"start\":33998},{\"end\":34318,\"start\":34249},{\"end\":34958,\"start\":34449},{\"end\":35712,\"start\":34960},{\"end\":36487,\"start\":35766},{\"end\":36704,\"start\":36515},{\"end\":36794,\"start\":36706},{\"end\":36950,\"start\":36822},{\"end\":37228,\"start\":36989},{\"end\":37549,\"start\":37230},{\"end\":38290,\"start\":37551},{\"end\":38480,\"start\":38292},{\"end\":38646,\"start\":38524},{\"end\":38880,\"start\":38648},{\"end\":39969,\"start\":38882},{\"end\":40164,\"start\":40123},{\"end\":40310,\"start\":40232},{\"end\":40520,\"start\":40312},{\"end\":40574,\"start\":40522},{\"end\":41248,\"start\":40610},{\"end\":41656,\"start\":41250},{\"end\":42456,\"start\":41658},{\"end\":42966,\"start\":42538},{\"end\":43148,\"start\":42968},{\"end\":43361,\"start\":43323},{\"end\":43537,\"start\":43438},{\"end\":43681,\"start\":43567},{\"end\":43993,\"start\":43963},{\"end\":44274,\"start\":44088},{\"end\":44732,\"start\":44276},{\"end\":45539,\"start\":44734},{\"end\":45894,\"start\":45678},{\"end\":46172,\"start\":45969},{\"end\":46493,\"start\":46174},{\"end\":46827,\"start\":46495},{\"end\":46948,\"start\":46829},{\"end\":47690,\"start\":46950},{\"end\":48093,\"start\":47981},{\"end\":48535,\"start\":48095},{\"end\":48648,\"start\":48537},{\"end\":48679,\"start\":48668},{\"end\":49060,\"start\":48681},{\"end\":49292,\"start\":49156},{\"end\":49640,\"start\":49485},{\"end\":49777,\"start\":49648},{\"end\":50834,\"start\":49918},{\"end\":51359,\"start\":50836},{\"end\":52340,\"start\":51652},{\"end\":52638,\"start\":52342},{\"end\":53602,\"start\":52704},{\"end\":54622,\"start\":53638},{\"end\":55135,\"start\":54793},{\"end\":55739,\"start\":55137},{\"end\":56161,\"start\":55775},{\"end\":56807,\"start\":56177},{\"end\":57284,\"start\":57058},{\"end\":57799,\"start\":57331},{\"end\":57915,\"start\":57801},{\"end\":57969,\"start\":57917},{\"end\":58493,\"start\":58242},{\"end\":58877,\"start\":58572},{\"end\":59018,\"start\":58879},{\"end\":59271,\"start\":59178},{\"end\":59692,\"start\":59273},{\"end\":60602,\"start\":59813},{\"end\":60934,\"start\":60604},{\"end\":61331,\"start\":60936},{\"end\":61757,\"start\":61333},{\"end\":62316,\"start\":61759},{\"end\":62774,\"start\":62318},{\"end\":63032,\"start\":62803},{\"end\":63205,\"start\":63034},{\"end\":63313,\"start\":63207},{\"end\":63591,\"start\":63442},{\"end\":63917,\"start\":63593},{\"end\":66173,\"start\":64262},{\"end\":66500,\"start\":66213},{\"end\":66852,\"start\":66531},{\"end\":68469,\"start\":66854},{\"end\":69771,\"start\":68471},{\"end\":70344,\"start\":69798},{\"end\":71013,\"start\":70379},{\"end\":71917,\"start\":71015},{\"end\":72638,\"start\":71972},{\"end\":72999,\"start\":72640},{\"end\":73215,\"start\":73001},{\"end\":75192,\"start\":73217},{\"end\":75519,\"start\":75194},{\"end\":75968,\"start\":75521},{\"end\":76215,\"start\":76005},{\"end\":76735,\"start\":76217},{\"end\":77285,\"start\":76737},{\"end\":77408,\"start\":77311},{\"end\":77758,\"start\":77466},{\"end\":78155,\"start\":77795},{\"end\":78276,\"start\":78157},{\"end\":78523,\"start\":78391},{\"end\":78782,\"start\":78525},{\"end\":79693,\"start\":78784},{\"end\":79884,\"start\":79744},{\"end\":80374,\"start\":79921},{\"end\":80807,\"start\":80467},{\"end\":80930,\"start\":80871},{\"end\":81238,\"start\":80932},{\"end\":81737,\"start\":81295},{\"end\":82161,\"start\":81783},{\"end\":82330,\"start\":82163},{\"end\":82555,\"start\":82332},{\"end\":83218,\"start\":82557},{\"end\":83556,\"start\":83220},{\"end\":83717,\"start\":83558},{\"end\":84207,\"start\":83719},{\"end\":84566,\"start\":84233},{\"end\":84971,\"start\":84568},{\"end\":85515,\"start\":85036},{\"end\":85830,\"start\":85636},{\"end\":85948,\"start\":85832},{\"end\":86430,\"start\":85950},{\"end\":86695,\"start\":86432},{\"end\":87029,\"start\":86697},{\"end\":87179,\"start\":87134},{\"end\":87537,\"start\":87181},{\"end\":87850,\"start\":87539},{\"end\":87994,\"start\":87852},{\"end\":88962,\"start\":88020},{\"end\":89161,\"start\":88964},{\"end\":89419,\"start\":89203},{\"end\":89520,\"start\":89453},{\"end\":89610,\"start\":89578},{\"end\":90254,\"start\":89612},{\"end\":90387,\"start\":90256},{\"end\":91353,\"start\":90465}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11820,\"start\":11705},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12033,\"start\":11884},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13433,\"start\":13313},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20071,\"start\":19824},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22060,\"start\":21971},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22573,\"start\":22413},{\"attributes\":{\"id\":\"formula_6\"},\"end\":22862,\"start\":22711},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23533,\"start\":23514},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23550,\"start\":23533},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24268,\"start\":24129},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24296,\"start\":24268},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25427,\"start\":25389},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25866,\"start\":25822},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26240,\"start\":26220},{\"attributes\":{\"id\":\"formula_14\"},\"end\":26683,\"start\":26661},{\"attributes\":{\"id\":\"formula_15\"},\"end\":27784,\"start\":27711},{\"attributes\":{\"id\":\"formula_16\"},\"end\":27885,\"start\":27831},{\"attributes\":{\"id\":\"formula_17\"},\"end\":27946,\"start\":27919},{\"attributes\":{\"id\":\"formula_18\"},\"end\":28364,\"start\":28145},{\"attributes\":{\"id\":\"formula_19\"},\"end\":30518,\"start\":30398},{\"attributes\":{\"id\":\"formula_20\"},\"end\":30883,\"start\":30850},{\"attributes\":{\"id\":\"formula_21\"},\"end\":31550,\"start\":31520},{\"attributes\":{\"id\":\"formula_22\"},\"end\":32828,\"start\":32704},{\"attributes\":{\"id\":\"formula_23\"},\"end\":33201,\"start\":33079},{\"attributes\":{\"id\":\"formula_24\"},\"end\":33258,\"start\":33201},{\"attributes\":{\"id\":\"formula_25\"},\"end\":33366,\"start\":33313},{\"attributes\":{\"id\":\"formula_26\"},\"end\":34248,\"start\":34206},{\"attributes\":{\"id\":\"formula_27\"},\"end\":34448,\"start\":34319},{\"attributes\":{\"id\":\"formula_28\"},\"end\":35765,\"start\":35713},{\"attributes\":{\"id\":\"formula_29\"},\"end\":36514,\"start\":36488},{\"attributes\":{\"id\":\"formula_30\"},\"end\":36821,\"start\":36795},{\"attributes\":{\"id\":\"formula_31\"},\"end\":36988,\"start\":36951},{\"attributes\":{\"id\":\"formula_32\"},\"end\":38523,\"start\":38481},{\"attributes\":{\"id\":\"formula_33\"},\"end\":40024,\"start\":39970},{\"attributes\":{\"id\":\"formula_34\"},\"end\":40122,\"start\":40024},{\"attributes\":{\"id\":\"formula_35\"},\"end\":40231,\"start\":40165},{\"attributes\":{\"id\":\"formula_36\"},\"end\":42537,\"start\":42457},{\"attributes\":{\"id\":\"formula_37\"},\"end\":43296,\"start\":43149},{\"attributes\":{\"id\":\"formula_38\"},\"end\":43322,\"start\":43296},{\"attributes\":{\"id\":\"formula_39\"},\"end\":43437,\"start\":43362},{\"attributes\":{\"id\":\"formula_40\"},\"end\":43566,\"start\":43538},{\"attributes\":{\"id\":\"formula_41\"},\"end\":43962,\"start\":43682},{\"attributes\":{\"id\":\"formula_42\"},\"end\":44087,\"start\":43994},{\"attributes\":{\"id\":\"formula_43\"},\"end\":45677,\"start\":45540},{\"attributes\":{\"id\":\"formula_44\"},\"end\":45968,\"start\":45895},{\"attributes\":{\"id\":\"formula_45\"},\"end\":47980,\"start\":47691},{\"attributes\":{\"id\":\"formula_46\"},\"end\":48667,\"start\":48649},{\"attributes\":{\"id\":\"formula_47\"},\"end\":49484,\"start\":49293},{\"attributes\":{\"id\":\"formula_48\"},\"end\":49917,\"start\":49778},{\"attributes\":{\"id\":\"formula_49\"},\"end\":51651,\"start\":51360},{\"attributes\":{\"id\":\"formula_50\"},\"end\":54792,\"start\":54623},{\"attributes\":{\"id\":\"formula_51\"},\"end\":57032,\"start\":56808},{\"attributes\":{\"id\":\"formula_52\"},\"end\":57057,\"start\":57032},{\"attributes\":{\"id\":\"formula_53\"},\"end\":57330,\"start\":57285},{\"attributes\":{\"id\":\"formula_54\"},\"end\":58101,\"start\":57970},{\"attributes\":{\"id\":\"formula_55\"},\"end\":58241,\"start\":58169},{\"attributes\":{\"id\":\"formula_56\"},\"end\":58571,\"start\":58516},{\"attributes\":{\"id\":\"formula_57\"},\"end\":59177,\"start\":59019},{\"attributes\":{\"id\":\"formula_58\"},\"end\":59812,\"start\":59693},{\"attributes\":{\"id\":\"formula_59\"},\"end\":62802,\"start\":62775},{\"attributes\":{\"id\":\"formula_60\"},\"end\":63441,\"start\":63314},{\"attributes\":{\"id\":\"formula_61\"},\"end\":64217,\"start\":63918},{\"attributes\":{\"id\":\"formula_62\"},\"end\":70378,\"start\":70345},{\"attributes\":{\"id\":\"formula_63\"},\"end\":78390,\"start\":78277},{\"attributes\":{\"id\":\"formula_64\"},\"end\":79743,\"start\":79694},{\"attributes\":{\"id\":\"formula_65\"},\"end\":80442,\"start\":80375},{\"attributes\":{\"id\":\"formula_66\"},\"end\":80870,\"start\":80808},{\"attributes\":{\"id\":\"formula_67\"},\"end\":81294,\"start\":81239},{\"attributes\":{\"id\":\"formula_68\"},\"end\":81782,\"start\":81738},{\"attributes\":{\"id\":\"formula_69\"},\"end\":85035,\"start\":84972},{\"attributes\":{\"id\":\"formula_70\"},\"end\":85635,\"start\":85516},{\"attributes\":{\"id\":\"formula_71\"},\"end\":87133,\"start\":87030},{\"attributes\":{\"id\":\"formula_72\"},\"end\":89452,\"start\":89420},{\"attributes\":{\"id\":\"formula_73\"},\"end\":89577,\"start\":89521}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":15994,\"start\":15987},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20504,\"start\":20496},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20788,\"start\":20779},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":64736,\"start\":64729},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":65118,\"start\":65110},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":67466,\"start\":67459},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":67683,\"start\":67674},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":68673,\"start\":68666},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":72796,\"start\":72787},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":73401,\"start\":73391},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":74798,\"start\":74788},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":75641,\"start\":75633}]", "section_header": "[{\"end\":1940,\"start\":1925},{\"end\":6991,\"start\":6960},{\"end\":12700,\"start\":12685},{\"end\":18320,\"start\":18293},{\"end\":21432,\"start\":21414},{\"end\":26289,\"start\":26242},{\"end\":28829,\"start\":28811},{\"end\":28863,\"start\":28832},{\"end\":33996,\"start\":33987},{\"end\":40608,\"start\":40577},{\"end\":49154,\"start\":49063},{\"attributes\":{\"n\":\"7\"},\"end\":49646,\"start\":49643},{\"end\":52702,\"start\":52641},{\"end\":53636,\"start\":53605},{\"end\":55773,\"start\":55742},{\"end\":56175,\"start\":56164},{\"end\":58168,\"start\":58103},{\"end\":58515,\"start\":58496},{\"end\":64260,\"start\":64219},{\"end\":66211,\"start\":66176},{\"end\":66529,\"start\":66503},{\"end\":69796,\"start\":69774},{\"end\":71970,\"start\":71920},{\"end\":76003,\"start\":75971},{\"end\":77309,\"start\":77288},{\"end\":77419,\"start\":77411},{\"end\":77464,\"start\":77422},{\"end\":77793,\"start\":77761},{\"end\":79919,\"start\":79887},{\"end\":80465,\"start\":80444},{\"end\":84231,\"start\":84210},{\"end\":88018,\"start\":87997},{\"end\":89201,\"start\":89164},{\"end\":90463,\"start\":90390},{\"end\":91686,\"start\":91678},{\"end\":92126,\"start\":92115},{\"end\":92352,\"start\":92344},{\"end\":92617,\"start\":92606},{\"end\":92851,\"start\":92843},{\"end\":93220,\"start\":93209},{\"end\":94975,\"start\":94967},{\"end\":95542,\"start\":95531},{\"end\":96065,\"start\":96056},{\"end\":96614,\"start\":96603},{\"end\":97218,\"start\":97210},{\"end\":97844,\"start\":97836},{\"end\":97966,\"start\":97965},{\"end\":98398,\"start\":98389},{\"end\":100860,\"start\":100859},{\"end\":101122,\"start\":101121},{\"end\":101398,\"start\":101389},{\"end\":101474,\"start\":101473},{\"end\":102184,\"start\":102175},{\"end\":102545,\"start\":102535},{\"end\":102835,\"start\":102824},{\"end\":103331,\"start\":103323},{\"end\":103691,\"start\":103681},{\"end\":104234,\"start\":104225},{\"end\":105060,\"start\":105050},{\"end\":105739,\"start\":105728},{\"end\":106321,\"start\":106309},{\"end\":107107,\"start\":107097}]", "table": "[{\"end\":102533,\"start\":102382},{\"end\":102822,\"start\":102716},{\"end\":103321,\"start\":102870},{\"end\":104084,\"start\":103751},{\"end\":105048,\"start\":104338},{\"end\":105726,\"start\":105063},{\"end\":106307,\"start\":105843},{\"end\":107095,\"start\":106452},{\"end\":108137,\"start\":107776}]", "figure_caption": "[{\"end\":91676,\"start\":91356},{\"end\":92076,\"start\":91688},{\"end\":92113,\"start\":92079},{\"end\":92342,\"start\":92128},{\"end\":92604,\"start\":92354},{\"end\":92841,\"start\":92619},{\"end\":93207,\"start\":92853},{\"end\":93886,\"start\":93222},{\"end\":94136,\"start\":93889},{\"end\":94965,\"start\":94139},{\"end\":95382,\"start\":94977},{\"end\":95529,\"start\":95385},{\"end\":96054,\"start\":95544},{\"end\":96601,\"start\":96067},{\"end\":97208,\"start\":96616},{\"end\":97834,\"start\":97220},{\"end\":97963,\"start\":97846},{\"end\":98387,\"start\":97967},{\"end\":98571,\"start\":98401},{\"end\":98698,\"start\":98574},{\"end\":99477,\"start\":98701},{\"end\":100470,\"start\":99480},{\"end\":100857,\"start\":100473},{\"end\":100886,\"start\":100861},{\"end\":101119,\"start\":100889},{\"end\":101387,\"start\":101123},{\"end\":101471,\"start\":101401},{\"end\":101523,\"start\":101475},{\"end\":102173,\"start\":101526},{\"end\":102382,\"start\":102186},{\"end\":102716,\"start\":102548},{\"end\":102870,\"start\":102839},{\"end\":103679,\"start\":103334},{\"end\":103751,\"start\":103694},{\"end\":104223,\"start\":104087},{\"end\":104338,\"start\":104236},{\"end\":105843,\"start\":105743},{\"end\":106452,\"start\":106326},{\"end\":107776,\"start\":107110}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5003,\"start\":4997},{\"end\":10890,\"start\":10884},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11259,\"start\":11253},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23828,\"start\":23822},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27310,\"start\":27304},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28570,\"start\":28562},{\"end\":33547,\"start\":33544},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":33685,\"start\":33679},{\"end\":35181,\"start\":35177},{\"attributes\":{\"ref_id\":\"fig_15\"},\"end\":35192,\"start\":35186},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":44772,\"start\":44766},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":46236,\"start\":46230},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":46806,\"start\":46800},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":48837,\"start\":48831},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":49059,\"start\":49053},{\"attributes\":{\"ref_id\":\"fig_17\"},\"end\":50133,\"start\":50127},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":51899,\"start\":51885},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":54391,\"start\":54384},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":54838,\"start\":54831},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":55169,\"start\":55162},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":63638,\"start\":63631}]", "bib_author_first_name": "[{\"end\":108476,\"start\":108475},{\"end\":108483,\"start\":108482},{\"end\":108490,\"start\":108489},{\"end\":108503,\"start\":108502},{\"end\":108521,\"start\":108520},{\"end\":108844,\"start\":108843},{\"end\":108853,\"start\":108852},{\"end\":108864,\"start\":108863},{\"end\":109247,\"start\":109246},{\"end\":109253,\"start\":109252},{\"end\":109267,\"start\":109266},{\"end\":109627,\"start\":109626},{\"end\":109637,\"start\":109636},{\"end\":109644,\"start\":109643},{\"end\":109646,\"start\":109645},{\"end\":109657,\"start\":109656},{\"end\":110009,\"start\":110008},{\"end\":110018,\"start\":110017},{\"end\":110031,\"start\":110030},{\"end\":110033,\"start\":110032},{\"end\":110321,\"start\":110320},{\"end\":110332,\"start\":110331},{\"end\":110345,\"start\":110344},{\"end\":110354,\"start\":110353},{\"end\":110369,\"start\":110368},{\"end\":110730,\"start\":110729},{\"end\":110738,\"start\":110737},{\"end\":110745,\"start\":110744},{\"end\":111032,\"start\":111031},{\"end\":111049,\"start\":111048},{\"end\":111062,\"start\":111061},{\"end\":111064,\"start\":111063},{\"end\":111329,\"start\":111328},{\"end\":111341,\"start\":111340},{\"end\":111351,\"start\":111350},{\"end\":111744,\"start\":111743},{\"end\":111746,\"start\":111745},{\"end\":111753,\"start\":111752},{\"end\":111768,\"start\":111767},{\"end\":112090,\"start\":112089},{\"end\":112092,\"start\":112091},{\"end\":112099,\"start\":112098},{\"end\":112107,\"start\":112106},{\"end\":112365,\"start\":112364},{\"end\":112380,\"start\":112379},{\"end\":112388,\"start\":112387},{\"end\":112390,\"start\":112389},{\"end\":112723,\"start\":112722},{\"end\":112729,\"start\":112728},{\"end\":112731,\"start\":112730},{\"end\":112745,\"start\":112744},{\"end\":112747,\"start\":112746},{\"end\":113066,\"start\":113065},{\"end\":113075,\"start\":113074},{\"end\":113084,\"start\":113083},{\"end\":113098,\"start\":113097},{\"end\":113107,\"start\":113106},{\"end\":113118,\"start\":113117},{\"end\":113446,\"start\":113445},{\"end\":113454,\"start\":113453},{\"end\":113472,\"start\":113471},{\"end\":113843,\"start\":113842},{\"end\":113858,\"start\":113857},{\"end\":113860,\"start\":113859},{\"end\":113867,\"start\":113866},{\"end\":114213,\"start\":114212},{\"end\":114215,\"start\":114214},{\"end\":114230,\"start\":114229},{\"end\":114245,\"start\":114244},{\"end\":114247,\"start\":114246},{\"end\":114637,\"start\":114636},{\"end\":114639,\"start\":114638},{\"end\":114646,\"start\":114645},{\"end\":114965,\"start\":114964},{\"end\":114971,\"start\":114970},{\"end\":114973,\"start\":114972},{\"end\":114987,\"start\":114986},{\"end\":114989,\"start\":114988},{\"end\":115335,\"start\":115334},{\"end\":115350,\"start\":115349},{\"end\":115607,\"start\":115606},{\"end\":115616,\"start\":115615},{\"end\":115618,\"start\":115617},{\"end\":115841,\"start\":115840},{\"end\":115843,\"start\":115842},{\"end\":115858,\"start\":115857},{\"end\":115873,\"start\":115872},{\"end\":115875,\"start\":115874},{\"end\":115886,\"start\":115885},{\"end\":115888,\"start\":115887},{\"end\":116189,\"start\":116188},{\"end\":116191,\"start\":116190},{\"end\":116202,\"start\":116201},{\"end\":116217,\"start\":116216},{\"end\":116219,\"start\":116218},{\"end\":116514,\"start\":116513},{\"end\":116516,\"start\":116515},{\"end\":116530,\"start\":116529},{\"end\":116532,\"start\":116531},{\"end\":116766,\"start\":116765},{\"end\":116777,\"start\":116776},{\"end\":116779,\"start\":116778},{\"end\":116968,\"start\":116967},{\"end\":117255,\"start\":117254},{\"end\":117268,\"start\":117267},{\"end\":117279,\"start\":117278},{\"end\":117654,\"start\":117653},{\"end\":117669,\"start\":117668},{\"end\":117688,\"start\":117687},{\"end\":117986,\"start\":117985},{\"end\":118005,\"start\":118004},{\"end\":118013,\"start\":118012},{\"end\":118359,\"start\":118358},{\"end\":118369,\"start\":118368},{\"end\":118597,\"start\":118596},{\"end\":118604,\"start\":118603},{\"end\":118611,\"start\":118610},{\"end\":118891,\"start\":118890},{\"end\":118906,\"start\":118905},{\"end\":118925,\"start\":118924},{\"end\":119211,\"start\":119210},{\"end\":119314,\"start\":119313},{\"end\":119679,\"start\":119678},{\"end\":119681,\"start\":119680}]", "bib_author_last_name": "[{\"end\":108480,\"start\":108477},{\"end\":108487,\"start\":108484},{\"end\":108500,\"start\":108491},{\"end\":108518,\"start\":108504},{\"end\":108533,\"start\":108522},{\"end\":108850,\"start\":108845},{\"end\":108861,\"start\":108854},{\"end\":108871,\"start\":108865},{\"end\":109250,\"start\":109248},{\"end\":109264,\"start\":109254},{\"end\":109278,\"start\":109268},{\"end\":109634,\"start\":109628},{\"end\":109641,\"start\":109638},{\"end\":109654,\"start\":109647},{\"end\":109672,\"start\":109658},{\"end\":110015,\"start\":110010},{\"end\":110028,\"start\":110019},{\"end\":110040,\"start\":110034},{\"end\":110329,\"start\":110322},{\"end\":110342,\"start\":110333},{\"end\":110351,\"start\":110346},{\"end\":110366,\"start\":110355},{\"end\":110375,\"start\":110370},{\"end\":110735,\"start\":110731},{\"end\":110742,\"start\":110739},{\"end\":110752,\"start\":110746},{\"end\":111046,\"start\":111033},{\"end\":111059,\"start\":111050},{\"end\":111071,\"start\":111065},{\"end\":111338,\"start\":111330},{\"end\":111348,\"start\":111342},{\"end\":111358,\"start\":111352},{\"end\":111750,\"start\":111747},{\"end\":111765,\"start\":111754},{\"end\":111776,\"start\":111769},{\"end\":112096,\"start\":112093},{\"end\":112104,\"start\":112100},{\"end\":112119,\"start\":112108},{\"end\":112377,\"start\":112366},{\"end\":112385,\"start\":112381},{\"end\":112399,\"start\":112391},{\"end\":112726,\"start\":112724},{\"end\":112742,\"start\":112732},{\"end\":112758,\"start\":112748},{\"end\":113072,\"start\":113067},{\"end\":113081,\"start\":113076},{\"end\":113095,\"start\":113085},{\"end\":113104,\"start\":113099},{\"end\":113115,\"start\":113108},{\"end\":113125,\"start\":113119},{\"end\":113451,\"start\":113447},{\"end\":113469,\"start\":113455},{\"end\":113484,\"start\":113473},{\"end\":113855,\"start\":113844},{\"end\":113864,\"start\":113861},{\"end\":113872,\"start\":113868},{\"end\":114227,\"start\":114216},{\"end\":114242,\"start\":114231},{\"end\":114257,\"start\":114248},{\"end\":114643,\"start\":114640},{\"end\":114658,\"start\":114647},{\"end\":114968,\"start\":114966},{\"end\":114984,\"start\":114974},{\"end\":115000,\"start\":114990},{\"end\":115347,\"start\":115336},{\"end\":115355,\"start\":115351},{\"end\":115613,\"start\":115608},{\"end\":115626,\"start\":115619},{\"end\":115855,\"start\":115844},{\"end\":115870,\"start\":115859},{\"end\":115883,\"start\":115876},{\"end\":115898,\"start\":115889},{\"end\":116199,\"start\":116192},{\"end\":116214,\"start\":116203},{\"end\":116229,\"start\":116220},{\"end\":116527,\"start\":116517},{\"end\":116541,\"start\":116533},{\"end\":116774,\"start\":116767},{\"end\":116788,\"start\":116780},{\"end\":116976,\"start\":116969},{\"end\":117265,\"start\":117256},{\"end\":117276,\"start\":117269},{\"end\":117287,\"start\":117280},{\"end\":117666,\"start\":117655},{\"end\":117685,\"start\":117670},{\"end\":117695,\"start\":117689},{\"end\":118002,\"start\":117987},{\"end\":118010,\"start\":118006},{\"end\":118023,\"start\":118014},{\"end\":118366,\"start\":118360},{\"end\":118379,\"start\":118370},{\"end\":118601,\"start\":118598},{\"end\":118608,\"start\":118605},{\"end\":118623,\"start\":118612},{\"end\":118903,\"start\":118892},{\"end\":118922,\"start\":118907},{\"end\":118932,\"start\":118926},{\"end\":119216,\"start\":119212},{\"end\":119319,\"start\":119315},{\"end\":119690,\"start\":119682}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3442617},\"end\":108750,\"start\":108421},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9028807},\"end\":109158,\"start\":108752},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":20729541},\"end\":109562,\"start\":109160},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":33632433},\"end\":109957,\"start\":109564},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49487227},\"end\":110227,\"start\":109959},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13748795},\"end\":110691,\"start\":110229},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":28708280},\"end\":110987,\"start\":110693},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":198147872},\"end\":111255,\"start\":110989},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53717250},\"end\":111662,\"start\":111257},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219179466},\"end\":112023,\"start\":111664},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52284270},\"end\":112285,\"start\":112025},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":59413846},\"end\":112622,\"start\":112287},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":22184330},\"end\":112997,\"start\":112624},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":19323670},\"end\":113355,\"start\":112999},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":53231036},\"end\":113716,\"start\":113357},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":211069710},\"end\":114120,\"start\":113718},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":196622289},\"end\":114559,\"start\":114122},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":59291914},\"end\":114864,\"start\":114561},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":22184330},\"end\":115241,\"start\":114866},{\"attributes\":{\"id\":\"b19\"},\"end\":115555,\"start\":115243},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":84187068},\"end\":115804,\"start\":115557},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":221386704},\"end\":116097,\"start\":115806},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":198901601},\"end\":116487,\"start\":116099},{\"attributes\":{\"id\":\"b23\"},\"end\":116711,\"start\":116489},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6014993},\"end\":116965,\"start\":116713},{\"attributes\":{\"id\":\"b25\"},\"end\":117165,\"start\":116967},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":202541694},\"end\":117565,\"start\":117167},{\"attributes\":{\"id\":\"b27\"},\"end\":117912,\"start\":117567},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":49868397},\"end\":118296,\"start\":117914},{\"attributes\":{\"id\":\"b29\"},\"end\":118548,\"start\":118298},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6938356},\"end\":118801,\"start\":118550},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":231725175},\"end\":119186,\"start\":118803},{\"attributes\":{\"id\":\"b32\"},\"end\":119294,\"start\":119188},{\"attributes\":{\"id\":\"b33\"},\"end\":119480,\"start\":119296},{\"attributes\":{\"id\":\"b34\"},\"end\":119637,\"start\":119482},{\"attributes\":{\"id\":\"b35\"},\"end\":119792,\"start\":119639}]", "bib_title": "[{\"end\":108473,\"start\":108421},{\"end\":108841,\"start\":108752},{\"end\":109244,\"start\":109160},{\"end\":109624,\"start\":109564},{\"end\":110006,\"start\":109959},{\"end\":110318,\"start\":110229},{\"end\":110727,\"start\":110693},{\"end\":111029,\"start\":110989},{\"end\":111326,\"start\":111257},{\"end\":111741,\"start\":111664},{\"end\":112087,\"start\":112025},{\"end\":112362,\"start\":112287},{\"end\":112720,\"start\":112624},{\"end\":113063,\"start\":112999},{\"end\":113443,\"start\":113357},{\"end\":113840,\"start\":113718},{\"end\":114210,\"start\":114122},{\"end\":114634,\"start\":114561},{\"end\":114962,\"start\":114866},{\"end\":115604,\"start\":115557},{\"end\":115838,\"start\":115806},{\"end\":116186,\"start\":116099},{\"end\":116511,\"start\":116489},{\"end\":116763,\"start\":116713},{\"end\":117252,\"start\":117167},{\"end\":117983,\"start\":117914},{\"end\":118594,\"start\":118550},{\"end\":118888,\"start\":118803},{\"end\":119311,\"start\":119296}]", "bib_author": "[{\"end\":108482,\"start\":108475},{\"end\":108489,\"start\":108482},{\"end\":108502,\"start\":108489},{\"end\":108520,\"start\":108502},{\"end\":108535,\"start\":108520},{\"end\":108852,\"start\":108843},{\"end\":108863,\"start\":108852},{\"end\":108873,\"start\":108863},{\"end\":109252,\"start\":109246},{\"end\":109266,\"start\":109252},{\"end\":109280,\"start\":109266},{\"end\":109636,\"start\":109626},{\"end\":109643,\"start\":109636},{\"end\":109656,\"start\":109643},{\"end\":109674,\"start\":109656},{\"end\":110017,\"start\":110008},{\"end\":110030,\"start\":110017},{\"end\":110042,\"start\":110030},{\"end\":110331,\"start\":110320},{\"end\":110344,\"start\":110331},{\"end\":110353,\"start\":110344},{\"end\":110368,\"start\":110353},{\"end\":110377,\"start\":110368},{\"end\":110737,\"start\":110729},{\"end\":110744,\"start\":110737},{\"end\":110754,\"start\":110744},{\"end\":111048,\"start\":111031},{\"end\":111061,\"start\":111048},{\"end\":111073,\"start\":111061},{\"end\":111340,\"start\":111328},{\"end\":111350,\"start\":111340},{\"end\":111360,\"start\":111350},{\"end\":111752,\"start\":111743},{\"end\":111767,\"start\":111752},{\"end\":111778,\"start\":111767},{\"end\":112098,\"start\":112089},{\"end\":112106,\"start\":112098},{\"end\":112121,\"start\":112106},{\"end\":112379,\"start\":112364},{\"end\":112387,\"start\":112379},{\"end\":112401,\"start\":112387},{\"end\":112728,\"start\":112722},{\"end\":112744,\"start\":112728},{\"end\":112760,\"start\":112744},{\"end\":113074,\"start\":113065},{\"end\":113083,\"start\":113074},{\"end\":113097,\"start\":113083},{\"end\":113106,\"start\":113097},{\"end\":113117,\"start\":113106},{\"end\":113127,\"start\":113117},{\"end\":113453,\"start\":113445},{\"end\":113471,\"start\":113453},{\"end\":113486,\"start\":113471},{\"end\":113857,\"start\":113842},{\"end\":113866,\"start\":113857},{\"end\":113874,\"start\":113866},{\"end\":114229,\"start\":114212},{\"end\":114244,\"start\":114229},{\"end\":114259,\"start\":114244},{\"end\":114645,\"start\":114636},{\"end\":114660,\"start\":114645},{\"end\":114970,\"start\":114964},{\"end\":114986,\"start\":114970},{\"end\":115002,\"start\":114986},{\"end\":115349,\"start\":115334},{\"end\":115357,\"start\":115349},{\"end\":115615,\"start\":115606},{\"end\":115628,\"start\":115615},{\"end\":115857,\"start\":115840},{\"end\":115872,\"start\":115857},{\"end\":115885,\"start\":115872},{\"end\":115900,\"start\":115885},{\"end\":116201,\"start\":116188},{\"end\":116216,\"start\":116201},{\"end\":116231,\"start\":116216},{\"end\":116529,\"start\":116513},{\"end\":116543,\"start\":116529},{\"end\":116776,\"start\":116765},{\"end\":116790,\"start\":116776},{\"end\":116978,\"start\":116967},{\"end\":117267,\"start\":117254},{\"end\":117278,\"start\":117267},{\"end\":117289,\"start\":117278},{\"end\":117668,\"start\":117653},{\"end\":117687,\"start\":117668},{\"end\":117697,\"start\":117687},{\"end\":118004,\"start\":117985},{\"end\":118012,\"start\":118004},{\"end\":118025,\"start\":118012},{\"end\":118368,\"start\":118358},{\"end\":118381,\"start\":118368},{\"end\":118603,\"start\":118596},{\"end\":118610,\"start\":118603},{\"end\":118625,\"start\":118610},{\"end\":118905,\"start\":118890},{\"end\":118924,\"start\":118905},{\"end\":118934,\"start\":118924},{\"end\":119218,\"start\":119210},{\"end\":119321,\"start\":119313},{\"end\":119692,\"start\":119678}]", "bib_venue": "[{\"end\":108957,\"start\":108921},{\"end\":109362,\"start\":109327},{\"end\":109764,\"start\":109723},{\"end\":110844,\"start\":110803},{\"end\":108558,\"start\":108535},{\"end\":108919,\"start\":108873},{\"end\":109325,\"start\":109280},{\"end\":109721,\"start\":109674},{\"end\":110074,\"start\":110042},{\"end\":110435,\"start\":110377},{\"end\":110801,\"start\":110754},{\"end\":111096,\"start\":111073},{\"end\":111437,\"start\":111360},{\"end\":111817,\"start\":111778},{\"end\":112144,\"start\":112121},{\"end\":112433,\"start\":112401},{\"end\":112783,\"start\":112760},{\"end\":113150,\"start\":113127},{\"end\":113513,\"start\":113486},{\"end\":113893,\"start\":113874},{\"end\":114320,\"start\":114259},{\"end\":114692,\"start\":114660},{\"end\":115034,\"start\":115002},{\"end\":115332,\"start\":115243},{\"end\":115660,\"start\":115628},{\"end\":115932,\"start\":115900},{\"end\":116283,\"start\":116231},{\"end\":116561,\"start\":116543},{\"end\":116813,\"start\":116790},{\"end\":117027,\"start\":116978},{\"end\":117344,\"start\":117289},{\"end\":117651,\"start\":117567},{\"end\":118092,\"start\":118025},{\"end\":118356,\"start\":118298},{\"end\":118657,\"start\":118625},{\"end\":118987,\"start\":118934},{\"end\":119208,\"start\":119188},{\"end\":119366,\"start\":119321},{\"end\":119517,\"start\":119482},{\"end\":119676,\"start\":119639}]"}}}, "year": 2023, "month": 12, "day": 17}