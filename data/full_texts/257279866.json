{"id": 257279866, "updated": "2023-11-22 11:38:54.386", "metadata": {"title": "APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation", "authors": "[{\"first\":\"Amira\",\"last\":\"Guesmi\",\"middle\":[]},{\"first\":\"Muhammad\",\"last\":\"Hanif\",\"middle\":[\"Abdullah\"]},{\"first\":\"Ihsen\",\"last\":\"Alouani\",\"middle\":[]},{\"first\":\"Muhammad\",\"last\":\"Shafique\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "In recent times, monocular depth estimation (MDE) has experienced significant advancements in performance, largely attributed to the integration of innovative architectures, i.e., convolutional neural networks (CNNs) and Transformers. Nevertheless, the susceptibility of these models to adversarial attacks has emerged as a noteworthy concern, especially in domains where safety and security are paramount. This concern holds particular weight for MDE due to its critical role in applications like autonomous driving and robotic navigation, where accurate scene understanding is pivotal. To assess the vulnerability of CNN-based depth prediction methods, recent work tries to design adversarial patches against MDE. However, the existing approaches fall short of inducing a comprehensive and substantially disruptive impact on the vision system. Instead, their influence is partial and confined to specific local areas. These methods lead to erroneous depth predictions only within the overlapping region with the input image, without considering the characteristics of the target object, such as its size, shape, and position. In this paper, we introduce a novel adversarial patch named APARATE. This patch possesses the ability to selectively undermine MDE in two distinct ways: by distorting the estimated distances or by creating the illusion of an object disappearing from the perspective of the autonomous system. Notably, APARATE is designed to be sensitive to the shape and scale of the target object, and its influence extends beyond immediate proximity. APARATE, results in a mean depth estimation error surpassing $0.5$, significantly impacting as much as $99\\%$ of the targeted region when applied to CNN-based MDE models. Furthermore, it yields a significant error of $0.34$ and exerts substantial influence over $94\\%$ of the target region in the context of Transformer-based MDE.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2303-01351", "doi": "10.48550/arxiv.2303.01351"}}, "content": {"source": {"pdf_hash": "da58abc579fa7d2dae557765083f47434ea4e68b", "pdf_src": "ArXiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2303.01351v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2303.01351", "status": "CLOSED"}}, "grobid": {"id": "8cdf4252bbdc9806fd3f26792ffee804b19fd043", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/da58abc579fa7d2dae557765083f47434ea4e68b.txt", "contents": "\nAPARATE: Adaptive Adversarial Patch for Monocular Depth Estimation for Autonomous Navigation\n\n\nAmira Guesmi \neBrain Lab\nNew York University (NYU) Abu Dhabi\nUAE\n\nMuhammad Abdullah Hanif \neBrain Lab\nNew York University (NYU) Abu Dhabi\nUAE\n\nIhsen Alouani \nCSIT\nQueen's University\nBelfastUK\n\nMuhammad Shafique \neBrain Lab\nNew York University (NYU) Abu Dhabi\nUAE\n\nAPARATE: Adaptive Adversarial Patch for Monocular Depth Estimation for Autonomous Navigation\n3D80A157FFC4FB1116881B166A572D7C\nIn recent times, monocular depth estimation (MDE) has experienced significant advancements in performance, largely attributed to the integration of innovative architectures, i.e., convolutional neural networks (CNNs) and Transformers.Nevertheless, the susceptibility of these models to adversarial attacks has emerged as a noteworthy concern, especially in domains where safety and security are paramount.This concern holds particular weight for MDE due to its critical role in applications like autonomous driving and robotic navigation, where accurate scene understanding is pivotal.To assess the vulnerability of CNN-based depth prediction methods, recent work tries to design adversarial patches against MDE.However, the existing approaches fall short of inducing a comprehensive and substantially disruptive impact on the vision system.Instead, their influence is partial and confined to specific local areas.These methods lead to erroneous depth predictions only within the overlapping region with the input image, without considering the characteristics of the target object, such as its size, shape, and position.In this paper, we introduce a novel adversarial patch named APARATE 1 .This patch possesses the ability to selectively undermine MDE in two distinct ways: by distorting the estimated distances or by creating the illusion of an object disappearing from the perspective of the autonomous system.Notably, APARATE is designed to be sensitive to the shape and scale of the target object, and its influence extends beyond immediate proximity.APARATE, results in a mean depth estimation error surpassing 0.5, significantly impacting as much as 99% of the targeted region when applied to CNN-based MDE models.Furthermore, it yields a significant error of 0.34 and exerts substantial influence over 94% of the target\n\nIntroduction\n\nMDE has found increasing utility across various practical applications such as robotics and autonomous driving (AD).MDE involves deriving depth insights from a single image, thereby enhancing scene comprehension.Its significance extends to several critical robotic functions, including obstacle avoidance [51], object detection [43], visual SLAM [37,46], and visual relocalization [42].\n\nSeveral methodologies for depth estimation rely on technologies like RGB-D cameras, Radar, LiDAR, or ultrasound devices to directly capture depth information within a scene.\n\nHowever, these alternatives exhibit notable shortcomings.RGB-D cameras possess a limited measurement range, LiDAR and Radar deliver sparse data, and both are costly sensing solutions that might not be viable for compact autonomous systems like low-cost, lightweight and small-sized mobile robots.Ultrasound devices, on the other hand, are marred by inherent measurement inaccuracies.\n\nMoreover, these technologies demand substantial energy consumption and feature large form factors, rendering them unsuitable for resource-restricted, small-scale systems that must adhere to stringent real-world design constraints.In contrast, RGB cameras stand out as lightweight and cost-effective options.Importantly, they have the capacity to furnish more comprehensive environmental data.Prominent players within the autonomous vehicle sector are actively pushing the envelope of self-driving technology by harnessing cost-effective camera solutions.\n\nNotably, MDE has been seamlessly integrated into Tesla's production-grade Autopilot system [1,2].Evidently, other major autonomous driving (AD) enterprises, including Toyota [19] and Huawei [3], are following Tesla's footsteps to propel self-driving advancements through this approach.\n\nIn recent years, the advancement of deep learning arXiv:2303.01351v2[cs.CV] 20 Nov 2023\n\nYamanaka et al.\n\n\nCheng et al. APARATE\nFigure 1\n. Our APARATE makes the object fully disappear, in contrast, adversarial patches proposed by Yamanaka et al. [49] and Cheng et al. [9] are weak adversarial patches that only impact the depth of a small region of the target object which is restricted to the overlapping region between the patch and the input image.\n\nhas led to substantial enhancements in the performance of monocular depth estimation (MDE), primarily through the utilization of CNN-based models [14,23,36].The effectiveness of CNN models stems from their ability to extract structural knowledge from training data through convolution operations.However, these networks have shown vulnerabilities to adversarial attacks.On the other hand, Transformer, initially introduced in natural language processing, has showcased remarkable achievements in monocular depth estimation [7,8,41].\n\nIn contrast to CNN-based networks, Vision Transformer-based ones (such as MT-SfMLearner [41] and MIMDepth [8] ) offer an expanded receptive field, enabling them to capture and learn global image information which also enhanced the system's resilience to both natural corruptions and adversarial attacks.\n\nPrevious efforts for patch-based adversarial attacks which only targeted CNN-based MDE [9,49] yielded relatively weak adversarial patches, which exhibited a limited impact on the depth estimation of a small portion within a targeted object (e.g., vehicles and pedestrians).The sphere of influence was confined to the area of overlap between the patch and the input image, offering substantial room for enhancement.\n\nOur approach introduces a novel technique for crafting adversarial patches for CNN-based and transformer-based monocular depth estimation.In essence, we generate adversarial patches designed to deceive the target methods, prompting them to erroneously estimate the depth of a designated object (e.g., vehicles, pedestrians), or even to entirely conceal the presence of that object.\n\nIn summary, the novel contributions of this work are: \u2022 We introduce a novel penalized loss function that enhances the efficiency of our adversarial patch and expands its impact region (refer to Figure 1).\u2022 Our devised attack methodology is generic, making it applicable to various object categories present on public roads.However, for proof-of-concept, we concentrate on two representative object types-cars and pedestrians-for targeting purposes.\n\n\u2022 The noise optimization process incorporates an automated patch placement mechanism, as opposed to random placement within the scene.This ensures that the patch is not trained on irrelevant objects.\u2022 Our proposed patch achieves a high mean depth estimation error exceeding 0.5, significantly impacting nearly 99% of the target region (i.e., the target object) for CNN-based MDE, and a mean depth estimation error of 0.34 with a substantial influence on 94% of the region for transformer-based MDE.\u2022 We provide a demo video in the suplemetary material showcasing the effectiveness of our patch in hiding a target object (i.e., a car) for the transformer-based MDE MIMDepth [8].An overview of our framework is depicted in Figure 2, while a comprehensive description can be found in Section 2.1.\n\n\nProposed Approach\n\n\nOverview\n\nTo facilitate a meaningful comparison, we endeavor to replicate the methodology presented in [49], wherein an adversarial patch was trained for arbitrary placement within the scene.In contrast, the approach proposed by [9] involves training a patch designed for consistent placement on the same object at a fixed distance from the camera (not considering the stealthiness constraint, as it falls outside the scope of our work).As clearly illustrated in Figure 1, our APARATE patch remarkably achieves the complete disappearance of the targeted object.Conversely, we observe a limited impact with the other two patches.In conducting this comparison, we maintain uniformity by employing identical optimization parameters, patch dimensions, and shapes.\n\nThe objective of our study is to create physical adversarial patches possessing a comprehensive impact that encompasses the entirety of the object, irrespective of its dimensions, contours, or placement, while upholding their effectiveness as attack tools.To accomplish this, we leverage a pre-trained object detector to identify the precise location of the targeted object-meaning the object we intend to conceal or manipulate its predicted depth.This identification enables the generation of a patch designed to withstand variations in camera-object distance.\n\nFurthermore, we introduce a novel loss function designed to amplify the patch's effect and expand the area it influences.As depicted in Figure 2, we begin with a pre-trained object detector and initiate the process by creating two distinct masks.The first mask, denoted as M p , represents the precise placement of the patch at the center of the designated target object.Conversely, the second mask, M f , pertains to the specific region covered by the target object -essentially, the region that the target object influences.Subsequently, the patch is inputted into the patch transformation block \"the patch transformer\", where we execute the geometric alterations outlined in Section 2.3.\n\nOnce these transformations are applied, we leverage the patch applier to overlay the generated patch onto the input image.This task harnesses information sourced from the object detector, as elucidated in Section 2.2.Following this step, we proceed with a forward pass, directing the resulting adversarial image into the MDE model for analysis.The subsequent stage entails employing the generated masks to calculate the necessary loss functions.Once these computations are completed, we determine the gradient of the patch.This gradient information guides us in updating the actual patch denoted as P .\n\n\nPatch Applier\n\nIn the context of a physical attack scenario, our control over the perspective, scale, and positioning of the adversarial patch concerning the camera is restricted.As a countermeasure, we strive to bolster the resilience of our patch to a wide array of potential scenarios.During the patch generation phase, we superimpose the patch onto the target object's surface (e.g., the rear of a vehicle or human attire).This approach allows us to simulate diverse scenes characterized by various settings.\n\nThe training process encompasses an assortment of transformations, including rotations and occlusions, meticulously integrated to emulate the plausible appearance of our adversarial patch P in a realistic context.Subsequently, leveraging the capabilities of the object detector, we acquire precise object locations (such as vehicles or individuals) within a given image, denoted as I. Placing our adversarial patch P onto the identified object Figure 2. Overview of the proposed approach: Given a pre-trained object detector we generate two masks: the patch mask (Mp) corresponding to the location of the patch at the center of the target object and the focus mask (M f ) corresponding to the object covered area and the attacked region.We feed the patch to the patch transformer and perform the geometric transformations described in Section 2.3.We, later on, render the patch on top of the input image by harnessing information from the object detector as described in section 2.2.After that, we perform a forward pass, i.e., we feed the resulting adversarial image to the MDE model.The next step is to apply the generated masks to compute the required loss functions.Then, we compute the gradient of the patch and based on this information we update the patch P .becomes feasible at this stage.\n\nTwo distinct masks emerge from this process: M f , encircling the object to demarcate its influence, and M p , designed to constrain the patch's attributes, encompassing its location, dimensions, and shape.It is crucial to emphasize that the object detector's involvement pertains solely to the optimization procedure of the patch and is not extended to the actual attack phase.To eliminate the necessity for manual patch placement onto objects, akin to the approach in [9], we make use of the YOLOv4-tiny object detector pretrained on the MSCOCO dataset [27].This detector's capabilities streamline the process by automatically identifying object placements, contributing to a more efficient and effective workflow.\nLet U = {I i } M i=1 and V = {J j } N\nj=1 respectively be the M training and N testing images for a particular scene of attack.We run the YOLOv4-tiny object detector on U and V with an objectness threshold of 0.5 and non-max suppression IoU threshold of 0.4.This yields the tuples:\nT U i = {(B U i,k )} Di k=1 , T V j = {(B V j,l )} Ej l=1(1)\nfor each I i and J j , where D i is the number of detections in I i (fixed to 14 in our experiment same as in [22]).and B U i,k is the bounding box of the k\u2212th detection in I i .(Same for E j and B V j,l for J j ).The sets of all detected objects are:\nT U = {T U i } M i=1 , T V = {T V j } N j=1(2)\nThe information used to optimize P for a scene of attack are the training images U and annotations T U .The patch P is randomly initialized.Given the current P, the patch is rendered on top of each detected object of the chosen class for each training image I i using the mask M U p i,k .M U p i,k is a matrix of zeros except for the patch location (The center of the patch is the center of the bounding boxes).\n\nThe focus mask M U f i,k is defined as the space limited by the generated bounding boxes B U i,k in a way that it covers the whole detected object.M U f i,k is a matrix of zeros except for the targeted regions (objects covered area) where the pixel values are ones.We multiply the generated masks with the adversarial depth map d adv and use the resulting maps to compute the two losses.\n\n\nPatch Transformation Block\n\nThe positioning and perspective of a camera on an autonomous vehicle in relation to another vehicle or target object are characterized by continuous variation.The images captured and supplied to the victim model are taken from diverse distances, angles, and lighting conditions.Consequently, any modification introduced by an attacker, such as an adversarial patch, must be resilient to these evolving circumstances.To emulate this, a range of physical transformations is applied, each reflecting different conditions that can arise.These transformations encompass aspects like introducing noise, applying random rotations, altering scales, and simulating variations in lighting.The patch transformer is employed to execute these transformations effectively.\n\nThe transformations executed include: Random Scaling: The patch's dimensions are randomly adjusted to approximately match its real-world proportions within the scene.Random Rotations: The patch P is subjected to random rotations (up to \u00b120 \u2022 ) centered around the bounding boxes B U i,k .This emulates uncertainties related to patch placement and sizing during printing.Color Space Transformations: Pixel intensity values are manipulated through various color space transformations.These include introducing random noise (within \u00b10.1 range), applying random contrast adjustments (within the range [0.8, 1.2]), and introducing random brightness adjustments (within \u00b10.1 range).The resultant image T \u03b8 (I i ), which undergoes this composite set of transformations, is then passed through the MDE for further analysis and processing.\n\n\nPenalized Depth Loss\n\nThe bounding boxes B U i,k generated serve as the foundation for creating a focus mask M f , which encompasses the specific region where we intend to modify the predicted depth.Our objective is to extend the region influenced by the patch, going beyond mere pixel overlap.To achieve this, we decompose the depth loss L depth into two distinct terms: L d1 and L d2 .L d1 represents the loss incurred by pixels that are overlapped by the patch, while L d2 pertains to the loss stemming from pixels that don't overlap.\n\nTo direct the optimization process towards prioritizing the reduction of the non-overlapped pixel loss L d2 , we employ a squaring operation on the term denoting the disparity between the output depth and the target depth, denoted as |d t \u2212 d adv | \u2299 M P .This utilization of quadratic functions is strategic, as these functions exhibit a slower rate of increase (slope or rate of change), consequently delaying the convergence of overlapped pixel loss in comparison to non-overlapped pixels.The adversarial losses are defined as the distance between the predicted depth and the target depth and calculated as follows:\nL d1 = |d t \u2212 d adv | \u2299 M P(3)L d2 = |d t \u2212 d adv | \u2299 (M f \u2212 M P )(4)L depth = L 2 d1 + L d2(5)\n\nAdversarial Patch Generation\n\nWe iteratively perform gradient updates on the adversarial patch (P ) in the pixel space in a way that optimizes our objective function defined as follows:\nL total = \u03b1L depth + \u03b2L tv (6)\nL depth is the adversarial depth loss.L tv is the total variation loss on the generated image to encourage smoothness [28].It is defined as:\nL tv = i,j (P i+1,j \u2212 P i,j ) 2 + (P i,j+1 \u2212 P i,j ) 2 (7)\nwhere the sub-indices i and j refer to the pixel coordinate of the patch P .\u03b1 and \u03b2 are hyper-parameters used to scale the three losses.For our experiments, we set \u03b1 = 1 and \u03b2 = 1.5 We optimize the total loss using Adam [25] optimizer.We try to minimize the object function L total and optimize the adversarial patch.We freeze all weights and biases in the depth estimator and only update the pixel values of the adversarial patch.The patch is randomly initialized.A detailed description of the algorithm, which outlines the various stages involved in training the adversarial patch, can be found in the supplementary material.\n\n\nExperimental Results\n\n\nExperimental Setup\n\nIn our experimental setup, we employ four distinct MDE models: namely, monodepth2 [16], Depthhints [44], Manydepth [45], and MIMdepth [8].Monodepth2, Depthhints, and Manydepth models adhere to a general U-Net architecture, characterized by an encoder-decoder network design.This architecture enables the incorporation of both high-level abstract features and local information.The encoder component utilizes the pretrained ResNet18 [20] model, which was pretrained on the ImageNet dataset [10].On the other hand, the decoder encompasses multiple convolutional and upsampling layers, with skip connections facilitating the reconstruction of output back to the input's original resolution.\n\nMIMdepth is an approach that leverages masked image modeling (MIM) and transformer architecture for self-supervised MDE.It utilizes a transformer-based encoder DeiTBase [38] which processes the tokenized inputs at constant resolution.MIMDepth is claimed to exhibit greater robustness to various challenges, including noise, blur, weather conditions, digital artifacts, occlusions, as well as both untargeted and targeted adversarial attacks.These four models were chosen based on their practicality and the availability of open-source code, the three first models are the same models featured in the work presented in [9].\n\nFor our evaluations, we use real-world driving scenes extracted from the KITTI 2015 dataset [15].This dataset comprises synchronized stereo video recordings alongside LiDAR measurements, all captured from a moving vehicle navigating urban surroundings.The dataset encapsulates an extensive array of road types, encompassing local and rural roads as well as highways.Within these scenes, a diverse range of objects is present, such as vehicles, pedestrians, and traffic lights, allowing for a comprehensive evaluation of the attack performance.\n\nWe train our patch on a TeslaV100 GPU.Our patch optimization process is executed over the course of 400 epochs, utilizing the Adam optimizer with a learning rate set at 0.01.The patch is scaled to a factor of 0.2 during this process.\n\n\nEvaluation Metrics\n\nTo assess the efficacy of our proposed attack, we utilize same metrics as employed in [9]: the mean depth estimation error (E d ) attributed to the target object, and the ratio of the affected region (R a ).To calculate these metrics, we establish the depth prediction of an unaltered object as a reference point, contrasting it with the depth prediction of the object subject to our adversarial manipulation.\n\nThe mean depth estimation error signifies the extent to which our proposed patch disrupts the depth estimation accuracy.A larger value in this metric indicates a more effective attack.Similarly, for the ratio of the affected region, a higher value corresponds to enhanced attack performance.In contrast to the approach presented in [9], where a patch is trained on a single object situated at a fixed distance from the camera while varying only the background, our approach involves training the patch for diverse distances.This encompasses objects positioned at various locations relative to the camera, such as close, far, left, right, or center.Initially, the output of the disparity map is normalized within the range of 0 to 1, where 0 signifies the farthest point in the image and 1 represents the closest point.\n\nThe mean depth estimation error was measured using the following metrics:\nE d = i,j (|d \u2212 d adv | \u2299 M f ) i,j M f (8)\nThe ratio of the affected region is gauged by assessing the proportion of pixels whose depth values have experienced alterations exceeding a specific threshold.This assessment is conducted relative to the number of pixels encompassed by the focus mask (M f ).For objects with any alteration in a pixel's depth value surpassing 0.1 results in that pixel being deemed as affected.\n\nThe ratio of the affected region was measured using the following metrics:\nR a = i,j I((|d \u2212 d adv | \u2299 M f ) > 0.1) i,j M f(9)\nAdditionally, we employ the Mean Square Error (MSE) to assess the performance of the model in relation to the predicted output depth map derived from an unperturbed input.The formulation of this metric is provided below.\nM SE = 1 n \u00d7 m i,j (d advi,j \u2212 d i,j ) 2\n(10)\n\n\nEvaluation Results\n\nWe extend our attack evaluation to include the four MDE models, targeting both classes of objects for each model.First, we generate adversarial patches specifically tailored for pedestrians and cyclists.In this phase, the patch scale is maintained at 0.2 for consistent testing across the different models.Illustrated in Figure 3, our devised patch demonstrates remarkable effectiveness by entirely concealing the cyclist.Intuitively, smaller objects are comparatively simpler to obscure or manipulate in terms of their depth.However, achieving this for smaller objects necessitates smaller patches, consequently yielding a relatively diminished impact.\n\nIn the subsequent experiment, we proceed to craft an adversarial patch targeting the \"car\" class.Employing the patch scale of 0.2, we observe that nearly all objects integrated with the APARATE patch achieve complete concealment.This outcome holds true irrespective of the object's specific characteristics, encompassing factors such as shape, size, and proximity to the camera.The extensive success in concealing various objects substantiates the robust nature of our proposed patch.We quantitatively assess the performance of our patch using the mean square error M SE, the mean depth estimation errors, denoted as E d , alongside the ratios of the affected regions, designated as R a , for the target object across 100 scenes extracted from the KITTI dataset.The final outcomes are determined by calculating the average values of these metrics, offering a representative result.In accordance with the data presented in Table 2, when our patch targets a monodepth2-based model, it achieves an average alteration of approximately 55%.Considering a scenario set on a highway, where the speed limit stands at 160 km/h, the stipulated safe distance stretches to 96 meters.When we incorporate our patch, a car positioned 90 meters away is projected to be situated approximately 139.5 meters away.This starkly demonstrates the profound consequences of our devised attack.Additionally, it's noteworthy that our patch yields an impact on over 98% of the target region.This data underscores the extensive influence our patch has on altering depth perception.\n\n\nBenign scenario Adversarial scenario\n\n\nAdversarial patches result in the region being estimated as farther away from the camera\n\n\nBenign scenario Adversarial scenario\n\nAdversarial patches result in the region being estimated as farther away nnnnnfrom the camera Moreover, even when targeting MIMdepth, a method touted for its enhanced robustness through the incorporation of transformer architecture and masked image modeling (MIM), our adversarial patch continues to exhibit effectiveness, causing a notable depth estimation error of 0.39. Figure 5 visually demonstrates the influence of our adversarial patch on the depth prediction of the MIMdepth model.For more qualitative results related to this model, please refer to the supplementary material.\n\n\nAPARATE vs. Existing Attacks\n\nWe conducted a series of experiments to provide a quantitative comparison between our attack strategy and prior approaches [9] and [49].To carry out this comparison, we employed the monodepth2 model to evaluate both the depth estimation error and ratio of the affected region.The results presented in Table 2 demonstrate that our proposed attack consistently achieves the most substantial alteration in depth estimation and encompasses the largest affected region when compared to the referenced prior works.\n\n\nDiscussion\n\n\nAblation Study\n\nTo assess the impact of the proposed penalized depth loss, we conducted an ablation study using the monodepth2 model as our target monocular depth estimation (MDE) model.We tested various combinations of loss terms and present the results in Table 3.Our proposed loss yielded the highest depth estimation error, with a value of 0.55, in contrast to 0.24 for the conventional loss and 0.18 when utilizing L d2 for the depth loss.Furthermore, our approach resulted in the highest ratio of affected regions, with a value of 0.99, as compared to 0.53 and 0.36 for the other combinations.As depicted in Figure 6, the region affected by the generated patch when using only the L d1 depth is confined to the immediate vicinity of the patch itself.This outcome demonstrates a slightly improved performance compared to the patches featured in [9,49].Subsequently, we proceed to assess the effects of our proposed depth loss outlined in Section 2.4.Upon applying this penalized depth loss, the resultant patch effectively conceals the entire object, as confirmed by our experimentation.\n\n\nThe Influence of Patch Scale\n\nWe subject our attack to evaluation by targeting the \"car\" object class using three distinct patch sizes: 0.1, 0.2, and 0.3.The mean depth error and the ratio of the affected region are assessed employing the three depth estimation models.Table 4 showcases the results for mean depth estimation error.Notably, an observable trend is that E d consistently increases alongside the increment in patch size across all three target models.This outcome is in line with expectations, as larger patches exert a more pronounced influence on the resulting depth error.This same trend is echoed in the ratio of affected regions, as reflected in Table 5, wherein larger patches correspond to larger affected regions.To assess the resilience of our patch against existing defense mechanisms, we employ three available defense techniques that involve input transformations, without necessitating the re-training of the victim models.Specifically, we apply JPEG compression [12], introduce Gaussian noise [52], and utilize median blurring [48].In contrast to adversarial training, which notably degrades the baseline performance of the model, these defenses specifically target the adversarial patch within the input space.\n\nIn our analysis, we compute and report the mean depth error for benign scenarios, representing cases where the defense technique is applied to the benign images (E d B ). Additionally, we determine the mean depth error for adversarial scenarios, involving the application of the defense technique to the adversarial image (E d ).Our findings indicate that our patch remains effective even when confronted with defense techniques (as depicted in Tables 6, 7, and 8).\n\nWhile the applied defense mechanisms do lead to a reduction in mean depth error, our patch continues to exhibit its influence, resulting in an average error of 0.2.It's worth noting that the median blur defense significantly diminishes the baseline performance of the MDE network.\n\n\nRelated Work\n\nIn autonomous driving, prior research has predominantly centered around bolstering the security of perception sensors.This is evidenced by a multitude of attacks that have been devised with the aim of disrupting various components, including cameras [29,30,50], GPS systems [34], LiDAR systems [6,35], RADAR sensors [17], ultrasonic sensors [50], and IMUs (Inertial Measurement Units) [39,40].Other studies have concentrated on launching attacks tailored for regression tasks, such as optical flow estimation [31] and depth estimation [49], while some have emphasized classification tasks, including 2D object detection and classification [5,13], tracking [24], traffic light detection [11], and lane detection [32].In the context of our work, our primary focus revolves around disrupting MDE models through jamming techniques.\n\nUnlike existing physical attacks that have been directed at tasks like object detection [18,21], image classification [4,13], and face recognition [26,33], the area of attacks on depth estimation has received relatively limited attention.\n\nZhang [53] proposes an attack technique designed to enhance performance in a universal attack scenario.Wong [47] introduces a strategy for generating targeted adversarial perturbations in images, which then randomly alter the associated depth map.It's worth noting that these two attacks center around digital perturbations, rendering them less suitable for real-world applications.\n\nYamanaka [49] devises a method for generating printable adversarial patches, but the generated patch is trained to be applicable to random locations within the scene.On the other hand, Cheng [9] concentrates on the inconspicuousness of the generated patch, ensuring that the patch remains unobtrusive and avoids drawing attention.However, the challenge with this patch lies in its object-specific nature, necessitating separate retraining for each target object.Furthermore, the patch's effectiveness is constrained by its limited affected region and its training for a specific context-namely, a fixed distance between the object and the camera-rendering it ineffective for varying distances.\n\nDifferent from prior efforts, our emphasis lies in evaluating the comprehensive influence of the generated patch.We prioritize ensuring that the patch envelops the entirety of the target object, thus guaranteeing a thorough deception of the DNN-based vision system.Our framework introduces a methodology that ensures the efficacy of the patch across various objects within the same class, accommodating distinct shapes and sizes.Importantly, our patch is designed to function across varying distances between the object's placement and the camera of the vision system.\n\n\nConclusion\n\nIn this paper, we introduce a novel physical adversarial patch named APARATE, crafted with the explicit purpose of undermining MDE-based vision systems.APARATE distinguishes itself as an adaptive adversarial patch, demonstrating the capacity to fully obscure objects or manipulate their perceived depth within a given scene, irrespective of their inherent size, shape, or placement.Our empirical investigations provide compelling evidence for the effectiveness and resilience of our patch across diverse target objects.The achieved mean depth estimation error exceeding 50%, with over 99% of the target region undergoing alteration.Furthermore, our patch exhibits durability against defense techniques grounded in input transformations.\n\nThe repercussions of the proposed attack could lead to substantial harm, encompassing loss, destruction, and endangerment to both life and property.Our findings serve as a clarion call to the research community, stimulating the exploration of more robust and adaptive defense mechanisms.\n\nFigure 3 .\n3\nFigure 3. Impact of APARATE on pedestrian/cyclist class for CNN-based MDE.\n\n\nFigure 4 .\n4\nFigure 4. Impact of APARATE on car class for CNN-based MDE.\n\n\nFigure 5 .\n5\nFigure 5. Impact of APARATE on car class for transformer-based MDE.\n\n\nFigure 6 .\n6\nFigure 6.Depth prediction w/o the penalized loss function: (Top) the input images, (Middle) results without the penalized depth loss (i.e., L d 1 and Ltv), (Bottom) results with our proposed depth loss (i.e., L d 1 , L d 2 and Ltv).\n\n\nTable 1 .\n1\nAttack performance in terms of mean depth estimation error (E d ) and ratio of affected region (Ra).\nModelsM SEE dRaMonodepth20.490.55 0.99Depthhints0.470.53 0.98Manydepth0.460.53 0.98MIMdepth0.340.39 0.94\n\nTable 2 .\n2\nAPARATE performance vs. existing attacks.\nAttackE dRaM SEAPARATE0.55 0.990.49Cheng et al. [9]0.21 0.470.12Yamanaka et al [49] 0.13 0.260.05\n\nTable 3 .\n3\nAttack performance in terms of mean depth estimation error (E d ) and ratio of affected region (Ra) for different losses combinations.\nL d 1 L d 2 LtvE dRa\u2713\u27130.18 0.36\u2713\u27130.24 0.53\u2713\u2713\u27130.55 0.99\n\nBenign scenario Adversarial scenario The penalized loss function results in larger affected region compared to the conventional loss function\n\n\nTable 4 .\n4\nE d for different patch scales.\nScale Monodepth2 Depthhints Manydepth0.10.460.370.30.20.550.530.530.30.660.640.63\n\nTable 5 .\n5\nRa for different patch scales.\nScale Monodepth2 Depthhints Manydepth0.10.970.950.970.20.990.980.980.30.990.990.994.3.PatchrobustnessagainstInputTransformation-based Defenses\n\nTable 6 .\n6\nMean depth error when applying JPEG compression.\nParameters90705030E d0.23 0.220.190.15E d B000.001 0.001\n\nTable 7 .\n7\nMean depth error when applying median blur.\nParameters5101520E d0.13 0.12 0.13 0.15E d B0.09 0.08 0.06 0.08\n\nTable 8 .\n8\nMean depth error when applying Gaussian noise.\nParameters 0.01 0.02 0.050.1E d0.24 0.210.20.21E d B0.01 0.03 0.05 0.089\nThe term \"APARATE\" is inspired by a teleportation spell in the Harry Potter series, enabling instantaneous travel between locations.region in the context of Transformer-based MDE.\n\n. Ai Tesla, Day, 2021. March 1, 2023\n\nAndrej karpathy -ai for full-self driving at tesla. March 1, 2023\n\nBidirectional attention network for monocular depth estimation. Shubhra Aich, Jean Marie Uwabeza, Md Amirul Vianney, Mannat Islam, Bingbing Kaur, Liu, CoRR, abs/2009.007432020\n\nSynthesizing robust adversarial examples. Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok, ICML. 2018\n\n. Tom B Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, Justin Gilmer, CoRR, abs/1712.096652017\n\nAdversarial sensor attack on lidar-based perception in autonomous driving. Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi, Qi Alfred Chen, Kevin Fu, Z Morley Mao, \n\nTransformer-based monocular depth estimation with attention supervision. Wenjie Chang, Yueyi Zhang, Zhiwei Xiong, 32nd British Machine Vision Conference (BMVC 2021). 2021\n\nImage masking for robust self-supervised monocular depth estimation. Hemang Chawla, Kishaan Jeeveswaran, Elahe Arani, Bahram Zonooz, 202324\n\nPhysical attack on monocular depth estimation with optimal adversarial patches. Zhiyuan Cheng, James Liang, Hongjun Choi, Guanhong Tao, Zhiwen Cao, Dongfang Liu, Xiangyu Zhang, 2022. 2, 3, 5, 6, 7, 8\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE Conference on Computer Vision and Pattern Recognition. 2009\n\nAdversarial camouflage: Hiding physical-world attacks with natural styles. R Duan, X Ma, Y Wang, J Bailey, A K Qin, Y Yang, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Los Alamitos, CA, USAIEEE Computer Society2020\n\nA study of the effect of JPG compression on adversarial images. Gintare Karolina Dziugaite, Zoubin Ghahramani, Daniel M Roy, CoRR, abs/1608.008532016\n\nPhysical adversarial examples for object detectors. Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tram\u00e8r, Atul Prakash, Tadayoshi Kohno, Dawn Song, Proceedings of the 12th USENIX Conference on Offensive Technologies. USENIX Association. the 12th USENIX Conference on Offensive Technologiespage 1, USA2018\n\nDeep ordinal regression network for monocular depth estimation. Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, Dacheng Tao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018\n\nAre we ready for autonomous driving? the kitti vision benchmark suite. Andreas Geiger, Philip Lenz, Raquel Urtasun, 2012 IEEE Conference on Computer Vision and Pattern Recognition. 2012\n\n4 [17] Amira Guesmi and Ihsen Alouani. Adversarial attack on radar-based environment perception systems. Cl\u00e9ment Godard, Oisin Mac Aodha, Gabriel J Brostow, CoRR, abs/1806.012602018. 2022Digging into self-supervised monocular depth estimation\n\nDap: A dynamic adversarial patch for evading person detectors. Amira Guesmi, Ruitian Ding, Muhammad Abdullah Hanif, Ihsen Alouani, Muhammad Shafique, 2023\n\nPacknet-sfm: 3d packing for self-supervised monocular depth estimation. Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Adrien Gaidon, CoRR, abs/1905.026932019\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016\n\nNaturalistic physical adversarial patch for object detectors. Yu-Chih-Tuan Hu, Jun-Cheng Chen, Bo-Han Kung, Kai-Lung Hua, Daniel Stanley, Tan , 2021 IEEE/CVF International Conference on Computer Vision (ICCV). 2021\n\nNaturalistic physical adversarial patch for object detectors. Yu-Chih-Tuan Hu, Jun-Cheng Chen, Bo-Han Kung, Kai-Lung Hua, Daniel Stanley, Tan , 2021 IEEE/CVF International Conference on Computer Vision (ICCV). 2021\n\nGuiding monocular depth estimation using depth-attention volume. Lam Huynh, Phong Nguyen-Ha, Jiri Matas, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part XXVI 16\n\nFooling detection alone is not enough: Adversarial attack against multiple object tracking. Yunhan Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Hao Chen, Zhenyu Zhong, Tao Wei, ICLR2020\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 2014\n\nAdvhat: Real-world adversarial attack on arcface face id system. S Komkov, A Petiushko, 2020 25th International Conference on Pattern Recognition (ICPR). Los Alamitos, CA, USAIEEE Computer Society2021\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge J Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. 2014\n\nUnderstanding deep image representations by inverting them. Aravindh Mahendran, Andrea Vedaldi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2015\n\nPhantom of the adas: Securing advanced driver-assistance systems from split-second phantom attacks. Ben Nassi, Yisroel Mirsky, Dudi Nassi, Raz Ben-Netanel, Oleg Drokin, Yuval Elovici, Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security. the 2020 ACM SIGSAC Conference on Computer and Communications SecurityNew York, NY, USAAssociation for Computing Machinery2020\n\nRemote attacks on automated vehicles sensors : Experiments on camera and lidar. Jonathan Petit, Bas Stottelaar, Michael Feiri, 2015\n\nAttacking optical flow. Anurag Ranjan, Joel Janai, Andreas Geiger, Michael J Black, CoRR, abs/1910.100532019\n\nHold tight and never let go: Security of deep learning based automated lane centering under physical-world attack. Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jack Jia, Xue Lin, Qi Alfred, Chen , CoRR, abs/2009.067012020\n\nAccessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K Reiter, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. the 2016 ACM SIGSAC Conference on Computer and Communications SecurityNew York, NY, USAAssociation for Computing Machinery2016\n\nDrift with devil: Security of Multi-Sensor fusion based localization in High-Level autonomous driving under GPS spoofing. Junjie Shen, Jun Yeon Won, Zeyuan Chen, Qi Alfred, Chen , 29th USENIX Security Symposium (USENIX Security 20). USENIX Association2020\n\nIllusion and dazzle: Adversarial optical channel exploits against lidars for automotive applications. Hocheol Shin, Dohyun Kim, Yujin Kwon, Yongdae Kim, Cryptology ePrint Archive. 2017/613, 2017\n\nMonocular depth estimation using laplacian pyramid-based depth residuals. Minsoo Song, Seokjae Lim, Wonjun Kim, IEEE transactions on circuits and systems for video technology. 202131\n\nCnn-slam: Real-time dense monocular slam with learned depth prediction. Keisuke Tateno, Federico Tombari, Iro Laina, Nassir Navab, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017. 2017\n\nTraining data-efficient image transformers & distillation through attention. Matthieu Hugo Touvron, Matthijs Cord, Francisco Douze, Alexandre Massa, Herv\u00e9 Sablayrolles, J\u00e9gou, International conference on machine learning. PMLR2021\n\nWalnut: Waging doubt on the integrity of mems accelerometers with acoustic injection attacks. Timothy Trippel, Ofir Weisse, Wenyuan Xu, Peter Honeyman, Kevin Fu, 2017 IEEE European Symposium on Security and Privacy (EuroS&P). 2017\n\nInjected and delivered: Fabricating implicit control over actuation systems by spoofing inertial sensors. Yazhou Tu, Zhiqiang Lin, Insup Lee, Xiali Hei, 27th USENIX Security Symposium (USENIX Security 18). USENIX Association. Baltimore, MD2018\n\nTransformers in self-supervised monocular depth estimation with unknown camera intrinsics. Arnav Varma, Hemang Chawla, Bahram Zonooz, Elahe Arani, arXiv:2202.031312022arXiv preprint\n\nLm-reloc: Levenberg-marquardt based direct visual relocalization. Patrick Lukas Von Stumberg, Nan Wenzel, Daniel Yang, Cremers, CoRR, abs/2010.063232020\n\nPseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, Kilian Q Weinberger, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019\n\nSelf-supervised monocular depth hints. Jamie Watson, Michael Firman, Gabriel Brostow, Daniyar Turmukhambetov, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). 2019\n\nThe temporal opportunist: Self-supervised multi-frame monocular depth. J Watson, O Mac Aodha, V Prisacariu, G Brostow, M Firman, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Los Alamitos, CA, USAIEEE Computer Society2021\n\nMonorec: Semi-supervised dense reconstruction in dynamic environments from a single moving camera. Felix Wimbauer, Nan Yang, Niclas Lukas Von Stumberg, Daniel Zeller, Cremers, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2021\n\nTargeted adversarial perturbations for monocular depth prediction. Alex Wong, Safa Cicek, Stefano Soatto, Proceedings of the 34th International Conference on Neural Information Processing Systems. the 34th International Conference on Neural Information Processing SystemsRed Hook, NY, USACurran Associates Inc2020\n\nFeature squeezing: Detecting adversarial examples in deep neural networks. Weilin Xu, David Evans, Yanjun Qi, CoRR, abs/1704.011552017\n\nAdversarial patch attacks on monocular depth estimation networks. Koichiro Yamanaka, Ryutaroh Matsumoto, Keita Takahashi, Toshiaki Fujii, IEEE Access. 82020. 2, 6, 7, 8\n\nCan you trust autonomous vehicles : Contactless attacks against sensors of self-driving vehicle. Chen Yan, 2016\n\nFast depth prediction and obstacle avoidance on a monocular drone using probabilistic convolutional neural network. Xin Yang, Jingyu Chen, Yuanjie Dang, Hongcheng Luo, Yuesheng Tang, Chunyuan Liao, Peng Chen, Kwang-Ting Cheng, IEEE Transactions on Intelligent Transportation Systems. 2212021\n\nDefending against whitebox adversarial attacks via randomized discretization. Yuchen Zhang, Percy Liang, Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics. the Twenty-Second International Conference on Artificial Intelligence and StatisticsPMLR2019\n\nAdversarial attacks on monocular depth estimation. Ziqi Zhang, Xinge Zhu, Yingwei Li, Xiangqun Chen, Yao Guo, CoRR, abs/2003.103152020\n", "annotations": {"author": "[{\"end\":161,\"start\":96},{\"end\":238,\"start\":162},{\"end\":288,\"start\":239},{\"end\":359,\"start\":289}]", "publisher": null, "author_last_name": "[{\"end\":108,\"start\":102},{\"end\":185,\"start\":180},{\"end\":252,\"start\":245},{\"end\":306,\"start\":298}]", "author_first_name": "[{\"end\":101,\"start\":96},{\"end\":170,\"start\":162},{\"end\":179,\"start\":171},{\"end\":244,\"start\":239},{\"end\":297,\"start\":289}]", "author_affiliation": "[{\"end\":160,\"start\":110},{\"end\":237,\"start\":187},{\"end\":287,\"start\":254},{\"end\":358,\"start\":308}]", "title": "[{\"end\":93,\"start\":1},{\"end\":452,\"start\":360}]", "venue": null, "abstract": "[{\"end\":2314,\"start\":486}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2639,\"start\":2635},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2662,\"start\":2658},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2680,\"start\":2676},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2683,\"start\":2680},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2715,\"start\":2711},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3928,\"start\":3925},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3930,\"start\":3928},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4012,\"start\":4008},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4027,\"start\":4024},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4371,\"start\":4367},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4392,\"start\":4389},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4724,\"start\":4720},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4727,\"start\":4724},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4730,\"start\":4727},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5100,\"start\":5097},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5102,\"start\":5100},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5105,\"start\":5102},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5200,\"start\":5196},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5217,\"start\":5214},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5503,\"start\":5500},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5506,\"start\":5503},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7339,\"start\":7336},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7586,\"start\":7582},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7711,\"start\":7708},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12386,\"start\":12383},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12472,\"start\":12468},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13087,\"start\":13083},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17290,\"start\":17286},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17592,\"start\":17588},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18127,\"start\":18123},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":18144,\"start\":18140},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":18160,\"start\":18156},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18178,\"start\":18175},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18477,\"start\":18473},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18534,\"start\":18530},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18903,\"start\":18899},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19351,\"start\":19348},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19450,\"start\":19446},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20244,\"start\":20241},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20901,\"start\":20898},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25420,\"start\":25417},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":25429,\"start\":25425},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26671,\"start\":26668},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26674,\"start\":26671},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27906,\"start\":27902},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":27937,\"start\":27933},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27971,\"start\":27967},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29171,\"start\":29167},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29174,\"start\":29171},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29177,\"start\":29174},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29195,\"start\":29191},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29214,\"start\":29211},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29217,\"start\":29214},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29262,\"start\":29258},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":29306,\"start\":29302},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29309,\"start\":29306},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29430,\"start\":29426},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29456,\"start\":29452},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29559,\"start\":29556},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29562,\"start\":29559},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29577,\"start\":29573},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29607,\"start\":29603},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29632,\"start\":29628},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":29838,\"start\":29834},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29841,\"start\":29838},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29867,\"start\":29864},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29870,\"start\":29867},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":29897,\"start\":29893},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29900,\"start\":29897},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29996,\"start\":29992},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30098,\"start\":30094},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30383,\"start\":30379},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30564,\"start\":30561}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32763,\"start\":32674},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32838,\"start\":32764},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32921,\"start\":32839},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33169,\"start\":32922},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33388,\"start\":33170},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33541,\"start\":33389},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33744,\"start\":33542},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33888,\"start\":33745},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34015,\"start\":33889},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34202,\"start\":34016},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":34321,\"start\":34203},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34442,\"start\":34322},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":34575,\"start\":34443}]", "paragraph": "[{\"end\":2716,\"start\":2330},{\"end\":2891,\"start\":2718},{\"end\":3276,\"start\":2893},{\"end\":3832,\"start\":3278},{\"end\":4119,\"start\":3834},{\"end\":4208,\"start\":4121},{\"end\":4225,\"start\":4210},{\"end\":4572,\"start\":4258},{\"end\":5106,\"start\":4574},{\"end\":5411,\"start\":5108},{\"end\":5827,\"start\":5413},{\"end\":6210,\"start\":5829},{\"end\":6661,\"start\":6212},{\"end\":7456,\"start\":6663},{\"end\":8238,\"start\":7489},{\"end\":8801,\"start\":8240},{\"end\":9493,\"start\":8803},{\"end\":10097,\"start\":9495},{\"end\":10612,\"start\":10115},{\"end\":11911,\"start\":10614},{\"end\":12629,\"start\":11913},{\"end\":12911,\"start\":12668},{\"end\":13224,\"start\":12973},{\"end\":13683,\"start\":13272},{\"end\":14072,\"start\":13685},{\"end\":14861,\"start\":14103},{\"end\":15693,\"start\":14863},{\"end\":16233,\"start\":15718},{\"end\":16853,\"start\":16235},{\"end\":17136,\"start\":16981},{\"end\":17308,\"start\":17168},{\"end\":17995,\"start\":17368},{\"end\":18728,\"start\":18041},{\"end\":19352,\"start\":18730},{\"end\":19897,\"start\":19354},{\"end\":20132,\"start\":19899},{\"end\":20564,\"start\":20155},{\"end\":21384,\"start\":20566},{\"end\":21459,\"start\":21386},{\"end\":21882,\"start\":21504},{\"end\":21958,\"start\":21884},{\"end\":22231,\"start\":22011},{\"end\":22277,\"start\":22273},{\"end\":22953,\"start\":22300},{\"end\":24506,\"start\":22955},{\"end\":25261,\"start\":24677},{\"end\":25802,\"start\":25294},{\"end\":26910,\"start\":25834},{\"end\":28151,\"start\":26943},{\"end\":28618,\"start\":28153},{\"end\":28900,\"start\":28620},{\"end\":29744,\"start\":28917},{\"end\":29984,\"start\":29746},{\"end\":30368,\"start\":29986},{\"end\":31063,\"start\":30370},{\"end\":31633,\"start\":31065},{\"end\":32384,\"start\":31648},{\"end\":32673,\"start\":32386},{\"end\":32762,\"start\":32688},{\"end\":32837,\"start\":32778},{\"end\":32920,\"start\":32853},{\"end\":33168,\"start\":32936},{\"end\":33283,\"start\":33183},{\"end\":33443,\"start\":33402},{\"end\":33689,\"start\":33555},{\"end\":33933,\"start\":33902},{\"end\":34059,\"start\":34029},{\"end\":34264,\"start\":34216},{\"end\":34378,\"start\":34335},{\"end\":34502,\"start\":34456}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12667,\"start\":12630},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12972,\"start\":12912},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13271,\"start\":13225},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16884,\"start\":16854},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16923,\"start\":16884},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16949,\"start\":16923},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17166,\"start\":17137},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17167,\"start\":17166},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17367,\"start\":17309},{\"attributes\":{\"id\":\"formula_9\"},\"end\":21502,\"start\":21460},{\"attributes\":{\"id\":\"formula_10\"},\"end\":21503,\"start\":21502},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22010,\"start\":21959},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22272,\"start\":22232}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23884,\"start\":23883},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25602,\"start\":25601},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":26083,\"start\":26082},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27189,\"start\":27188},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27584,\"start\":27583}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2328,\"start\":2316},{\"end\":4248,\"start\":4228},{\"attributes\":{\"n\":\"2.\"},\"end\":7476,\"start\":7459},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7487,\"start\":7479},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10113,\"start\":10100},{\"attributes\":{\"n\":\"2.3.\"},\"end\":14101,\"start\":14075},{\"attributes\":{\"n\":\"2.4.\"},\"end\":15716,\"start\":15696},{\"attributes\":{\"n\":\"2.5.\"},\"end\":16979,\"start\":16951},{\"attributes\":{\"n\":\"3.\"},\"end\":18018,\"start\":17998},{\"attributes\":{\"n\":\"3.1.\"},\"end\":18039,\"start\":18021},{\"attributes\":{\"n\":\"3.2.\"},\"end\":20153,\"start\":20135},{\"attributes\":{\"n\":\"3.3.\"},\"end\":22298,\"start\":22280},{\"end\":24545,\"start\":24509},{\"end\":24636,\"start\":24548},{\"end\":24675,\"start\":24639},{\"attributes\":{\"n\":\"3.4.\"},\"end\":25292,\"start\":25264},{\"attributes\":{\"n\":\"4.\"},\"end\":25815,\"start\":25805},{\"attributes\":{\"n\":\"4.1.\"},\"end\":25832,\"start\":25818},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26941,\"start\":26913},{\"attributes\":{\"n\":\"5.\"},\"end\":28915,\"start\":28903},{\"attributes\":{\"n\":\"6.\"},\"end\":31646,\"start\":31636},{\"end\":32685,\"start\":32675},{\"end\":32775,\"start\":32765},{\"end\":32850,\"start\":32840},{\"end\":32933,\"start\":32923},{\"end\":33180,\"start\":33171},{\"end\":33399,\"start\":33390},{\"end\":33552,\"start\":33543},{\"end\":33887,\"start\":33746},{\"end\":33899,\"start\":33890},{\"end\":34026,\"start\":34017},{\"end\":34213,\"start\":34204},{\"end\":34332,\"start\":34323},{\"end\":34453,\"start\":34444}]", "table": "[{\"end\":33388,\"start\":33284},{\"end\":33541,\"start\":33444},{\"end\":33744,\"start\":33690},{\"end\":34015,\"start\":33934},{\"end\":34202,\"start\":34060},{\"end\":34321,\"start\":34265},{\"end\":34442,\"start\":34379},{\"end\":34575,\"start\":34503}]", "figure_caption": "[{\"end\":32763,\"start\":32687},{\"end\":32838,\"start\":32777},{\"end\":32921,\"start\":32852},{\"end\":33169,\"start\":32935},{\"end\":33284,\"start\":33182},{\"end\":33444,\"start\":33401},{\"end\":33690,\"start\":33554},{\"end\":33934,\"start\":33901},{\"end\":34060,\"start\":34028},{\"end\":34265,\"start\":34215},{\"end\":34379,\"start\":34334},{\"end\":34503,\"start\":34455}]", "figure_ref": "[{\"end\":6415,\"start\":6414},{\"end\":7392,\"start\":7391},{\"end\":7950,\"start\":7949},{\"end\":8947,\"start\":8946},{\"end\":11066,\"start\":11065},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22629,\"start\":22628},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25058,\"start\":25057},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26440,\"start\":26439}]", "bib_author_first_name": "[{\"end\":34761,\"start\":34759},{\"end\":34933,\"start\":34926},{\"end\":34944,\"start\":34940},{\"end\":34962,\"start\":34960},{\"end\":34969,\"start\":34963},{\"end\":34985,\"start\":34979},{\"end\":35001,\"start\":34993},{\"end\":35086,\"start\":35081},{\"end\":35101,\"start\":35096},{\"end\":35118,\"start\":35112},{\"end\":35131,\"start\":35126},{\"end\":35155,\"start\":35152},{\"end\":35157,\"start\":35156},{\"end\":35174,\"start\":35165},{\"end\":35186,\"start\":35181},{\"end\":35198,\"start\":35192},{\"end\":35212,\"start\":35206},{\"end\":35328,\"start\":35322},{\"end\":35341,\"start\":35334},{\"end\":35356,\"start\":35348},{\"end\":35368,\"start\":35362},{\"end\":35378,\"start\":35375},{\"end\":35389,\"start\":35385},{\"end\":35402,\"start\":35400},{\"end\":35421,\"start\":35416},{\"end\":35427,\"start\":35426},{\"end\":35434,\"start\":35428},{\"end\":35521,\"start\":35515},{\"end\":35534,\"start\":35529},{\"end\":35548,\"start\":35542},{\"end\":35689,\"start\":35683},{\"end\":35705,\"start\":35698},{\"end\":35724,\"start\":35719},{\"end\":35738,\"start\":35732},{\"end\":35842,\"start\":35835},{\"end\":35855,\"start\":35850},{\"end\":35870,\"start\":35863},{\"end\":35885,\"start\":35877},{\"end\":35897,\"start\":35891},{\"end\":35911,\"start\":35903},{\"end\":35924,\"start\":35917},{\"end\":36012,\"start\":36009},{\"end\":36022,\"start\":36019},{\"end\":36036,\"start\":36029},{\"end\":36051,\"start\":36045},{\"end\":36059,\"start\":36056},{\"end\":36066,\"start\":36064},{\"end\":36223,\"start\":36222},{\"end\":36231,\"start\":36230},{\"end\":36237,\"start\":36236},{\"end\":36245,\"start\":36244},{\"end\":36255,\"start\":36254},{\"end\":36257,\"start\":36256},{\"end\":36264,\"start\":36263},{\"end\":36466,\"start\":36459},{\"end\":36493,\"start\":36487},{\"end\":36512,\"start\":36506},{\"end\":36514,\"start\":36513},{\"end\":36603,\"start\":36598},{\"end\":36617,\"start\":36613},{\"end\":36635,\"start\":36627},{\"end\":36649,\"start\":36647},{\"end\":36658,\"start\":36654},{\"end\":36675,\"start\":36668},{\"end\":36688,\"start\":36684},{\"end\":36707,\"start\":36698},{\"end\":36719,\"start\":36715},{\"end\":36952,\"start\":36948},{\"end\":36965,\"start\":36957},{\"end\":36979,\"start\":36972},{\"end\":36992,\"start\":36986},{\"end\":37015,\"start\":37008},{\"end\":37246,\"start\":37239},{\"end\":37261,\"start\":37255},{\"end\":37274,\"start\":37268},{\"end\":37467,\"start\":37460},{\"end\":37481,\"start\":37476},{\"end\":37500,\"start\":37493},{\"end\":37502,\"start\":37501},{\"end\":37667,\"start\":37662},{\"end\":37683,\"start\":37676},{\"end\":37698,\"start\":37690},{\"end\":37720,\"start\":37715},{\"end\":37738,\"start\":37730},{\"end\":37832,\"start\":37827},{\"end\":37849,\"start\":37844},{\"end\":37864,\"start\":37858},{\"end\":37879,\"start\":37873},{\"end\":37967,\"start\":37960},{\"end\":37979,\"start\":37972},{\"end\":37995,\"start\":37987},{\"end\":38005,\"start\":38001},{\"end\":38163,\"start\":38151},{\"end\":38177,\"start\":38168},{\"end\":38190,\"start\":38184},{\"end\":38205,\"start\":38197},{\"end\":38217,\"start\":38211},{\"end\":38230,\"start\":38227},{\"end\":38379,\"start\":38367},{\"end\":38393,\"start\":38384},{\"end\":38406,\"start\":38400},{\"end\":38421,\"start\":38413},{\"end\":38433,\"start\":38427},{\"end\":38446,\"start\":38443},{\"end\":38589,\"start\":38586},{\"end\":38602,\"start\":38597},{\"end\":38618,\"start\":38614},{\"end\":38847,\"start\":38841},{\"end\":38859,\"start\":38853},{\"end\":38870,\"start\":38864},{\"end\":38879,\"start\":38877},{\"end\":38896,\"start\":38893},{\"end\":38909,\"start\":38903},{\"end\":38920,\"start\":38917},{\"end\":38981,\"start\":38980},{\"end\":38997,\"start\":38992},{\"end\":39082,\"start\":39081},{\"end\":39092,\"start\":39091},{\"end\":39269,\"start\":39261},{\"end\":39282,\"start\":39275},{\"end\":39295,\"start\":39290},{\"end\":39297,\"start\":39296},{\"end\":39313,\"start\":39308},{\"end\":39326,\"start\":39320},{\"end\":39339,\"start\":39335},{\"end\":39354,\"start\":39349},{\"end\":39364,\"start\":39363},{\"end\":39373,\"start\":39365},{\"end\":39463,\"start\":39455},{\"end\":39481,\"start\":39475},{\"end\":39741,\"start\":39738},{\"end\":39756,\"start\":39749},{\"end\":39769,\"start\":39765},{\"end\":39780,\"start\":39777},{\"end\":39798,\"start\":39794},{\"end\":39812,\"start\":39807},{\"end\":40125,\"start\":40117},{\"end\":40136,\"start\":40133},{\"end\":40156,\"start\":40149},{\"end\":40200,\"start\":40194},{\"end\":40213,\"start\":40209},{\"end\":40228,\"start\":40221},{\"end\":40244,\"start\":40237},{\"end\":40246,\"start\":40245},{\"end\":40401,\"start\":40395},{\"end\":40414,\"start\":40408},{\"end\":40428,\"start\":40421},{\"end\":40441,\"start\":40435},{\"end\":40446,\"start\":40442},{\"end\":40455,\"start\":40452},{\"end\":40463,\"start\":40461},{\"end\":40476,\"start\":40472},{\"end\":40600,\"start\":40593},{\"end\":40614,\"start\":40609},{\"end\":40632,\"start\":40628},{\"end\":40647,\"start\":40640},{\"end\":40649,\"start\":40648},{\"end\":41001,\"start\":40995},{\"end\":41011,\"start\":41008},{\"end\":41028,\"start\":41022},{\"end\":41037,\"start\":41035},{\"end\":41050,\"start\":41046},{\"end\":41239,\"start\":41232},{\"end\":41252,\"start\":41246},{\"end\":41263,\"start\":41258},{\"end\":41277,\"start\":41270},{\"end\":41406,\"start\":41400},{\"end\":41420,\"start\":41413},{\"end\":41432,\"start\":41426},{\"end\":41589,\"start\":41582},{\"end\":41606,\"start\":41598},{\"end\":41619,\"start\":41616},{\"end\":41633,\"start\":41627},{\"end\":41805,\"start\":41797},{\"end\":41828,\"start\":41820},{\"end\":41844,\"start\":41835},{\"end\":41861,\"start\":41852},{\"end\":41874,\"start\":41869},{\"end\":42053,\"start\":42046},{\"end\":42067,\"start\":42063},{\"end\":42083,\"start\":42076},{\"end\":42093,\"start\":42088},{\"end\":42109,\"start\":42104},{\"end\":42296,\"start\":42290},{\"end\":42309,\"start\":42301},{\"end\":42320,\"start\":42315},{\"end\":42331,\"start\":42326},{\"end\":42525,\"start\":42520},{\"end\":42539,\"start\":42533},{\"end\":42554,\"start\":42548},{\"end\":42568,\"start\":42563},{\"end\":42685,\"start\":42678},{\"end\":42709,\"start\":42706},{\"end\":42724,\"start\":42718},{\"end\":42876,\"start\":42873},{\"end\":42890,\"start\":42883},{\"end\":42905,\"start\":42897},{\"end\":42919,\"start\":42912},{\"end\":42935,\"start\":42931},{\"end\":42952,\"start\":42946},{\"end\":42954,\"start\":42953},{\"end\":43093,\"start\":43088},{\"end\":43109,\"start\":43102},{\"end\":43125,\"start\":43118},{\"end\":43142,\"start\":43135},{\"end\":43303,\"start\":43302},{\"end\":43313,\"start\":43312},{\"end\":43326,\"start\":43325},{\"end\":43340,\"start\":43339},{\"end\":43351,\"start\":43350},{\"end\":43588,\"start\":43583},{\"end\":43602,\"start\":43599},{\"end\":43615,\"start\":43609},{\"end\":43642,\"start\":43636},{\"end\":43813,\"start\":43809},{\"end\":43824,\"start\":43820},{\"end\":43839,\"start\":43832},{\"end\":44138,\"start\":44132},{\"end\":44148,\"start\":44143},{\"end\":44162,\"start\":44156},{\"end\":44267,\"start\":44259},{\"end\":44286,\"start\":44278},{\"end\":44303,\"start\":44298},{\"end\":44323,\"start\":44315},{\"end\":44464,\"start\":44460},{\"end\":44595,\"start\":44592},{\"end\":44608,\"start\":44602},{\"end\":44622,\"start\":44615},{\"end\":44638,\"start\":44629},{\"end\":44652,\"start\":44644},{\"end\":44667,\"start\":44659},{\"end\":44678,\"start\":44674},{\"end\":44695,\"start\":44685},{\"end\":44853,\"start\":44847},{\"end\":44866,\"start\":44861},{\"end\":45124,\"start\":45120},{\"end\":45137,\"start\":45132},{\"end\":45150,\"start\":45143},{\"end\":45163,\"start\":45155},{\"end\":45173,\"start\":45170}]", "bib_author_last_name": "[{\"end\":34767,\"start\":34762},{\"end\":34772,\"start\":34769},{\"end\":34938,\"start\":34934},{\"end\":34958,\"start\":34945},{\"end\":34977,\"start\":34970},{\"end\":34991,\"start\":34986},{\"end\":35006,\"start\":35002},{\"end\":35011,\"start\":35008},{\"end\":35094,\"start\":35087},{\"end\":35110,\"start\":35102},{\"end\":35124,\"start\":35119},{\"end\":35136,\"start\":35132},{\"end\":35163,\"start\":35158},{\"end\":35179,\"start\":35175},{\"end\":35190,\"start\":35187},{\"end\":35204,\"start\":35199},{\"end\":35219,\"start\":35213},{\"end\":35332,\"start\":35329},{\"end\":35346,\"start\":35342},{\"end\":35360,\"start\":35357},{\"end\":35373,\"start\":35369},{\"end\":35383,\"start\":35379},{\"end\":35398,\"start\":35390},{\"end\":35414,\"start\":35403},{\"end\":35424,\"start\":35422},{\"end\":35438,\"start\":35435},{\"end\":35527,\"start\":35522},{\"end\":35540,\"start\":35535},{\"end\":35554,\"start\":35549},{\"end\":35696,\"start\":35690},{\"end\":35717,\"start\":35706},{\"end\":35730,\"start\":35725},{\"end\":35745,\"start\":35739},{\"end\":35848,\"start\":35843},{\"end\":35861,\"start\":35856},{\"end\":35875,\"start\":35871},{\"end\":35889,\"start\":35886},{\"end\":35901,\"start\":35898},{\"end\":35915,\"start\":35912},{\"end\":35930,\"start\":35925},{\"end\":36017,\"start\":36013},{\"end\":36027,\"start\":36023},{\"end\":36043,\"start\":36037},{\"end\":36054,\"start\":36052},{\"end\":36062,\"start\":36060},{\"end\":36074,\"start\":36067},{\"end\":36228,\"start\":36224},{\"end\":36234,\"start\":36232},{\"end\":36242,\"start\":36238},{\"end\":36252,\"start\":36246},{\"end\":36261,\"start\":36258},{\"end\":36269,\"start\":36265},{\"end\":36485,\"start\":36467},{\"end\":36504,\"start\":36494},{\"end\":36518,\"start\":36515},{\"end\":36611,\"start\":36604},{\"end\":36625,\"start\":36618},{\"end\":36645,\"start\":36636},{\"end\":36652,\"start\":36650},{\"end\":36666,\"start\":36659},{\"end\":36682,\"start\":36676},{\"end\":36696,\"start\":36689},{\"end\":36713,\"start\":36708},{\"end\":36724,\"start\":36720},{\"end\":36955,\"start\":36953},{\"end\":36970,\"start\":36966},{\"end\":36984,\"start\":36980},{\"end\":37006,\"start\":36993},{\"end\":37019,\"start\":37016},{\"end\":37253,\"start\":37247},{\"end\":37266,\"start\":37262},{\"end\":37282,\"start\":37275},{\"end\":37474,\"start\":37468},{\"end\":37491,\"start\":37482},{\"end\":37510,\"start\":37503},{\"end\":37674,\"start\":37668},{\"end\":37688,\"start\":37684},{\"end\":37713,\"start\":37699},{\"end\":37728,\"start\":37721},{\"end\":37747,\"start\":37739},{\"end\":37842,\"start\":37833},{\"end\":37856,\"start\":37850},{\"end\":37871,\"start\":37865},{\"end\":37886,\"start\":37880},{\"end\":37970,\"start\":37968},{\"end\":37985,\"start\":37980},{\"end\":37999,\"start\":37996},{\"end\":38009,\"start\":38006},{\"end\":38166,\"start\":38164},{\"end\":38182,\"start\":38178},{\"end\":38195,\"start\":38191},{\"end\":38209,\"start\":38206},{\"end\":38225,\"start\":38218},{\"end\":38382,\"start\":38380},{\"end\":38398,\"start\":38394},{\"end\":38411,\"start\":38407},{\"end\":38425,\"start\":38422},{\"end\":38441,\"start\":38434},{\"end\":38595,\"start\":38590},{\"end\":38612,\"start\":38603},{\"end\":38624,\"start\":38619},{\"end\":38851,\"start\":38848},{\"end\":38862,\"start\":38860},{\"end\":38875,\"start\":38871},{\"end\":38891,\"start\":38880},{\"end\":38901,\"start\":38897},{\"end\":38915,\"start\":38910},{\"end\":38924,\"start\":38921},{\"end\":38990,\"start\":38982},{\"end\":39004,\"start\":38998},{\"end\":39008,\"start\":39006},{\"end\":39089,\"start\":39083},{\"end\":39102,\"start\":39093},{\"end\":39273,\"start\":39270},{\"end\":39288,\"start\":39283},{\"end\":39306,\"start\":39298},{\"end\":39318,\"start\":39314},{\"end\":39333,\"start\":39327},{\"end\":39347,\"start\":39340},{\"end\":39361,\"start\":39355},{\"end\":39381,\"start\":39374},{\"end\":39473,\"start\":39464},{\"end\":39489,\"start\":39482},{\"end\":39747,\"start\":39742},{\"end\":39763,\"start\":39757},{\"end\":39775,\"start\":39770},{\"end\":39792,\"start\":39781},{\"end\":39805,\"start\":39799},{\"end\":39820,\"start\":39813},{\"end\":40131,\"start\":40126},{\"end\":40147,\"start\":40137},{\"end\":40162,\"start\":40157},{\"end\":40207,\"start\":40201},{\"end\":40219,\"start\":40214},{\"end\":40235,\"start\":40229},{\"end\":40252,\"start\":40247},{\"end\":40406,\"start\":40402},{\"end\":40419,\"start\":40415},{\"end\":40433,\"start\":40429},{\"end\":40450,\"start\":40447},{\"end\":40459,\"start\":40456},{\"end\":40470,\"start\":40464},{\"end\":40607,\"start\":40601},{\"end\":40626,\"start\":40615},{\"end\":40638,\"start\":40633},{\"end\":40656,\"start\":40650},{\"end\":41006,\"start\":41002},{\"end\":41020,\"start\":41012},{\"end\":41033,\"start\":41029},{\"end\":41044,\"start\":41038},{\"end\":41244,\"start\":41240},{\"end\":41256,\"start\":41253},{\"end\":41268,\"start\":41264},{\"end\":41281,\"start\":41278},{\"end\":41411,\"start\":41407},{\"end\":41424,\"start\":41421},{\"end\":41436,\"start\":41433},{\"end\":41596,\"start\":41590},{\"end\":41614,\"start\":41607},{\"end\":41625,\"start\":41620},{\"end\":41639,\"start\":41634},{\"end\":41818,\"start\":41806},{\"end\":41833,\"start\":41829},{\"end\":41850,\"start\":41845},{\"end\":41867,\"start\":41862},{\"end\":41887,\"start\":41875},{\"end\":41894,\"start\":41889},{\"end\":42061,\"start\":42054},{\"end\":42074,\"start\":42068},{\"end\":42086,\"start\":42084},{\"end\":42102,\"start\":42094},{\"end\":42112,\"start\":42110},{\"end\":42299,\"start\":42297},{\"end\":42313,\"start\":42310},{\"end\":42324,\"start\":42321},{\"end\":42335,\"start\":42332},{\"end\":42531,\"start\":42526},{\"end\":42546,\"start\":42540},{\"end\":42561,\"start\":42555},{\"end\":42574,\"start\":42569},{\"end\":42704,\"start\":42686},{\"end\":42716,\"start\":42710},{\"end\":42729,\"start\":42725},{\"end\":42738,\"start\":42731},{\"end\":42881,\"start\":42877},{\"end\":42895,\"start\":42891},{\"end\":42910,\"start\":42906},{\"end\":42929,\"start\":42920},{\"end\":42944,\"start\":42936},{\"end\":42965,\"start\":42955},{\"end\":43100,\"start\":43094},{\"end\":43116,\"start\":43110},{\"end\":43133,\"start\":43126},{\"end\":43157,\"start\":43143},{\"end\":43310,\"start\":43304},{\"end\":43323,\"start\":43314},{\"end\":43337,\"start\":43327},{\"end\":43348,\"start\":43341},{\"end\":43358,\"start\":43352},{\"end\":43597,\"start\":43589},{\"end\":43607,\"start\":43603},{\"end\":43634,\"start\":43616},{\"end\":43649,\"start\":43643},{\"end\":43658,\"start\":43651},{\"end\":43818,\"start\":43814},{\"end\":43830,\"start\":43825},{\"end\":43846,\"start\":43840},{\"end\":44141,\"start\":44139},{\"end\":44154,\"start\":44149},{\"end\":44165,\"start\":44163},{\"end\":44276,\"start\":44268},{\"end\":44296,\"start\":44287},{\"end\":44313,\"start\":44304},{\"end\":44329,\"start\":44324},{\"end\":44468,\"start\":44465},{\"end\":44600,\"start\":44596},{\"end\":44613,\"start\":44609},{\"end\":44627,\"start\":44623},{\"end\":44642,\"start\":44639},{\"end\":44657,\"start\":44653},{\"end\":44672,\"start\":44668},{\"end\":44683,\"start\":44679},{\"end\":44701,\"start\":44696},{\"end\":44859,\"start\":44854},{\"end\":44872,\"start\":44867},{\"end\":45130,\"start\":45125},{\"end\":45141,\"start\":45138},{\"end\":45153,\"start\":45151},{\"end\":45168,\"start\":45164},{\"end\":45177,\"start\":45174}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":34793,\"start\":34757},{\"attributes\":{\"id\":\"b1\"},\"end\":34860,\"start\":34795},{\"attributes\":{\"doi\":\"CoRR, abs/2009.00743\",\"id\":\"b2\"},\"end\":35037,\"start\":34862},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2645819},\"end\":35148,\"start\":35039},{\"attributes\":{\"doi\":\"CoRR, abs/1712.09665\",\"id\":\"b4\"},\"end\":35245,\"start\":35150},{\"attributes\":{\"id\":\"b5\"},\"end\":35440,\"start\":35247},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":249011083},\"end\":35612,\"start\":35442},{\"attributes\":{\"id\":\"b7\"},\"end\":35753,\"start\":35614},{\"attributes\":{\"id\":\"b8\"},\"end\":35954,\"start\":35755},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":36145,\"start\":35956},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":213175526},\"end\":36393,\"start\":36147},{\"attributes\":{\"doi\":\"CoRR, abs/1608.00853\",\"id\":\"b11\"},\"end\":36544,\"start\":36395},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49904930},\"end\":36882,\"start\":36546},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":46968214},\"end\":37166,\"start\":36884},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6724907},\"end\":37353,\"start\":37168},{\"attributes\":{\"doi\":\"CoRR, abs/1806.01260\",\"id\":\"b15\"},\"end\":37597,\"start\":37355},{\"attributes\":{\"id\":\"b16\"},\"end\":37753,\"start\":37599},{\"attributes\":{\"doi\":\"CoRR, abs/1905.02693\",\"id\":\"b17\"},\"end\":37912,\"start\":37755},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":38087,\"start\":37914},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":244426677},\"end\":38303,\"start\":38089},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":244426677},\"end\":38519,\"start\":38305},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":214803126},\"end\":38747,\"start\":38521},{\"attributes\":{\"id\":\"b22\"},\"end\":38934,\"start\":38749},{\"attributes\":{\"id\":\"b23\"},\"end\":39014,\"start\":38936},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":201645162},\"end\":39216,\"start\":39016},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":14113767},\"end\":39393,\"start\":39218},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206593185},\"end\":39636,\"start\":39395},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":223654355},\"end\":40035,\"start\":39638},{\"attributes\":{\"id\":\"b28\"},\"end\":40168,\"start\":40037},{\"attributes\":{\"doi\":\"CoRR, abs/1910.10053\",\"id\":\"b29\"},\"end\":40278,\"start\":40170},{\"attributes\":{\"doi\":\"CoRR, abs/2009.06701\",\"id\":\"b30\"},\"end\":40503,\"start\":40280},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":207241700},\"end\":40871,\"start\":40505},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":219792385},\"end\":41128,\"start\":40873},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":11683288},\"end\":41324,\"start\":41130},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":234174200},\"end\":41508,\"start\":41326},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206596482},\"end\":41718,\"start\":41510},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":229363322},\"end\":41950,\"start\":41720},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":7284958},\"end\":42182,\"start\":41952},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":49322856},\"end\":42427,\"start\":42184},{\"attributes\":{\"doi\":\"arXiv:2202.03131\",\"id\":\"b39\"},\"end\":42610,\"start\":42429},{\"attributes\":{\"doi\":\"CoRR, abs/2010.06323\",\"id\":\"b40\"},\"end\":42764,\"start\":42612},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":56177594},\"end\":43047,\"start\":42766},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":202676783},\"end\":43229,\"start\":43049},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":233444160},\"end\":43482,\"start\":43231},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":227151533},\"end\":43740,\"start\":43484},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":219708282},\"end\":44055,\"start\":43742},{\"attributes\":{\"doi\":\"CoRR, abs/1704.01155\",\"id\":\"b46\"},\"end\":44191,\"start\":44057},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":222177159},\"end\":44361,\"start\":44193},{\"attributes\":{\"id\":\"b48\"},\"end\":44474,\"start\":44363},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":213840327},\"end\":44767,\"start\":44476},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":85518376},\"end\":45067,\"start\":44769},{\"attributes\":{\"doi\":\"CoRR, abs/2003.10315\",\"id\":\"b51\"},\"end\":45203,\"start\":45069}]", "bib_title": "[{\"end\":35079,\"start\":35039},{\"end\":35513,\"start\":35442},{\"end\":36007,\"start\":35956},{\"end\":36220,\"start\":36147},{\"end\":36596,\"start\":36546},{\"end\":36946,\"start\":36884},{\"end\":37237,\"start\":37168},{\"end\":37958,\"start\":37914},{\"end\":38149,\"start\":38089},{\"end\":38365,\"start\":38305},{\"end\":38584,\"start\":38521},{\"end\":39079,\"start\":39016},{\"end\":39259,\"start\":39218},{\"end\":39453,\"start\":39395},{\"end\":39736,\"start\":39638},{\"end\":40591,\"start\":40505},{\"end\":40993,\"start\":40873},{\"end\":41230,\"start\":41130},{\"end\":41398,\"start\":41326},{\"end\":41580,\"start\":41510},{\"end\":41795,\"start\":41720},{\"end\":42044,\"start\":41952},{\"end\":42288,\"start\":42184},{\"end\":42871,\"start\":42766},{\"end\":43086,\"start\":43049},{\"end\":43300,\"start\":43231},{\"end\":43581,\"start\":43484},{\"end\":43807,\"start\":43742},{\"end\":44257,\"start\":44193},{\"end\":44590,\"start\":44476},{\"end\":44845,\"start\":44769}]", "bib_author": "[{\"end\":34769,\"start\":34759},{\"end\":34774,\"start\":34769},{\"end\":34940,\"start\":34926},{\"end\":34960,\"start\":34940},{\"end\":34979,\"start\":34960},{\"end\":34993,\"start\":34979},{\"end\":35008,\"start\":34993},{\"end\":35013,\"start\":35008},{\"end\":35096,\"start\":35081},{\"end\":35112,\"start\":35096},{\"end\":35126,\"start\":35112},{\"end\":35138,\"start\":35126},{\"end\":35165,\"start\":35152},{\"end\":35181,\"start\":35165},{\"end\":35192,\"start\":35181},{\"end\":35206,\"start\":35192},{\"end\":35221,\"start\":35206},{\"end\":35334,\"start\":35322},{\"end\":35348,\"start\":35334},{\"end\":35362,\"start\":35348},{\"end\":35375,\"start\":35362},{\"end\":35385,\"start\":35375},{\"end\":35400,\"start\":35385},{\"end\":35416,\"start\":35400},{\"end\":35426,\"start\":35416},{\"end\":35440,\"start\":35426},{\"end\":35529,\"start\":35515},{\"end\":35542,\"start\":35529},{\"end\":35556,\"start\":35542},{\"end\":35698,\"start\":35683},{\"end\":35719,\"start\":35698},{\"end\":35732,\"start\":35719},{\"end\":35747,\"start\":35732},{\"end\":35850,\"start\":35835},{\"end\":35863,\"start\":35850},{\"end\":35877,\"start\":35863},{\"end\":35891,\"start\":35877},{\"end\":35903,\"start\":35891},{\"end\":35917,\"start\":35903},{\"end\":35932,\"start\":35917},{\"end\":36019,\"start\":36009},{\"end\":36029,\"start\":36019},{\"end\":36045,\"start\":36029},{\"end\":36056,\"start\":36045},{\"end\":36064,\"start\":36056},{\"end\":36076,\"start\":36064},{\"end\":36230,\"start\":36222},{\"end\":36236,\"start\":36230},{\"end\":36244,\"start\":36236},{\"end\":36254,\"start\":36244},{\"end\":36263,\"start\":36254},{\"end\":36271,\"start\":36263},{\"end\":36487,\"start\":36459},{\"end\":36506,\"start\":36487},{\"end\":36520,\"start\":36506},{\"end\":36613,\"start\":36598},{\"end\":36627,\"start\":36613},{\"end\":36647,\"start\":36627},{\"end\":36654,\"start\":36647},{\"end\":36668,\"start\":36654},{\"end\":36684,\"start\":36668},{\"end\":36698,\"start\":36684},{\"end\":36715,\"start\":36698},{\"end\":36726,\"start\":36715},{\"end\":36957,\"start\":36948},{\"end\":36972,\"start\":36957},{\"end\":36986,\"start\":36972},{\"end\":37008,\"start\":36986},{\"end\":37021,\"start\":37008},{\"end\":37255,\"start\":37239},{\"end\":37268,\"start\":37255},{\"end\":37284,\"start\":37268},{\"end\":37476,\"start\":37460},{\"end\":37493,\"start\":37476},{\"end\":37512,\"start\":37493},{\"end\":37676,\"start\":37662},{\"end\":37690,\"start\":37676},{\"end\":37715,\"start\":37690},{\"end\":37730,\"start\":37715},{\"end\":37749,\"start\":37730},{\"end\":37844,\"start\":37827},{\"end\":37858,\"start\":37844},{\"end\":37873,\"start\":37858},{\"end\":37888,\"start\":37873},{\"end\":37972,\"start\":37960},{\"end\":37987,\"start\":37972},{\"end\":38001,\"start\":37987},{\"end\":38011,\"start\":38001},{\"end\":38168,\"start\":38151},{\"end\":38184,\"start\":38168},{\"end\":38197,\"start\":38184},{\"end\":38211,\"start\":38197},{\"end\":38227,\"start\":38211},{\"end\":38233,\"start\":38227},{\"end\":38384,\"start\":38367},{\"end\":38400,\"start\":38384},{\"end\":38413,\"start\":38400},{\"end\":38427,\"start\":38413},{\"end\":38443,\"start\":38427},{\"end\":38449,\"start\":38443},{\"end\":38597,\"start\":38586},{\"end\":38614,\"start\":38597},{\"end\":38626,\"start\":38614},{\"end\":38853,\"start\":38841},{\"end\":38864,\"start\":38853},{\"end\":38877,\"start\":38864},{\"end\":38893,\"start\":38877},{\"end\":38903,\"start\":38893},{\"end\":38917,\"start\":38903},{\"end\":38926,\"start\":38917},{\"end\":38992,\"start\":38980},{\"end\":39006,\"start\":38992},{\"end\":39010,\"start\":39006},{\"end\":39091,\"start\":39081},{\"end\":39104,\"start\":39091},{\"end\":39275,\"start\":39261},{\"end\":39290,\"start\":39275},{\"end\":39308,\"start\":39290},{\"end\":39320,\"start\":39308},{\"end\":39335,\"start\":39320},{\"end\":39349,\"start\":39335},{\"end\":39363,\"start\":39349},{\"end\":39383,\"start\":39363},{\"end\":39475,\"start\":39455},{\"end\":39491,\"start\":39475},{\"end\":39749,\"start\":39738},{\"end\":39765,\"start\":39749},{\"end\":39777,\"start\":39765},{\"end\":39794,\"start\":39777},{\"end\":39807,\"start\":39794},{\"end\":39822,\"start\":39807},{\"end\":40133,\"start\":40117},{\"end\":40149,\"start\":40133},{\"end\":40164,\"start\":40149},{\"end\":40209,\"start\":40194},{\"end\":40221,\"start\":40209},{\"end\":40237,\"start\":40221},{\"end\":40254,\"start\":40237},{\"end\":40408,\"start\":40395},{\"end\":40421,\"start\":40408},{\"end\":40435,\"start\":40421},{\"end\":40452,\"start\":40435},{\"end\":40461,\"start\":40452},{\"end\":40472,\"start\":40461},{\"end\":40479,\"start\":40472},{\"end\":40609,\"start\":40593},{\"end\":40628,\"start\":40609},{\"end\":40640,\"start\":40628},{\"end\":40658,\"start\":40640},{\"end\":41008,\"start\":40995},{\"end\":41022,\"start\":41008},{\"end\":41035,\"start\":41022},{\"end\":41046,\"start\":41035},{\"end\":41053,\"start\":41046},{\"end\":41246,\"start\":41232},{\"end\":41258,\"start\":41246},{\"end\":41270,\"start\":41258},{\"end\":41283,\"start\":41270},{\"end\":41413,\"start\":41400},{\"end\":41426,\"start\":41413},{\"end\":41438,\"start\":41426},{\"end\":41598,\"start\":41582},{\"end\":41616,\"start\":41598},{\"end\":41627,\"start\":41616},{\"end\":41641,\"start\":41627},{\"end\":41820,\"start\":41797},{\"end\":41835,\"start\":41820},{\"end\":41852,\"start\":41835},{\"end\":41869,\"start\":41852},{\"end\":41889,\"start\":41869},{\"end\":41896,\"start\":41889},{\"end\":42063,\"start\":42046},{\"end\":42076,\"start\":42063},{\"end\":42088,\"start\":42076},{\"end\":42104,\"start\":42088},{\"end\":42114,\"start\":42104},{\"end\":42301,\"start\":42290},{\"end\":42315,\"start\":42301},{\"end\":42326,\"start\":42315},{\"end\":42337,\"start\":42326},{\"end\":42533,\"start\":42520},{\"end\":42548,\"start\":42533},{\"end\":42563,\"start\":42548},{\"end\":42576,\"start\":42563},{\"end\":42706,\"start\":42678},{\"end\":42718,\"start\":42706},{\"end\":42731,\"start\":42718},{\"end\":42740,\"start\":42731},{\"end\":42883,\"start\":42873},{\"end\":42897,\"start\":42883},{\"end\":42912,\"start\":42897},{\"end\":42931,\"start\":42912},{\"end\":42946,\"start\":42931},{\"end\":42967,\"start\":42946},{\"end\":43102,\"start\":43088},{\"end\":43118,\"start\":43102},{\"end\":43135,\"start\":43118},{\"end\":43159,\"start\":43135},{\"end\":43312,\"start\":43302},{\"end\":43325,\"start\":43312},{\"end\":43339,\"start\":43325},{\"end\":43350,\"start\":43339},{\"end\":43360,\"start\":43350},{\"end\":43599,\"start\":43583},{\"end\":43609,\"start\":43599},{\"end\":43636,\"start\":43609},{\"end\":43651,\"start\":43636},{\"end\":43660,\"start\":43651},{\"end\":43820,\"start\":43809},{\"end\":43832,\"start\":43820},{\"end\":43848,\"start\":43832},{\"end\":44143,\"start\":44132},{\"end\":44156,\"start\":44143},{\"end\":44167,\"start\":44156},{\"end\":44278,\"start\":44259},{\"end\":44298,\"start\":44278},{\"end\":44315,\"start\":44298},{\"end\":44331,\"start\":44315},{\"end\":44470,\"start\":44460},{\"end\":44602,\"start\":44592},{\"end\":44615,\"start\":44602},{\"end\":44629,\"start\":44615},{\"end\":44644,\"start\":44629},{\"end\":44659,\"start\":44644},{\"end\":44674,\"start\":44659},{\"end\":44685,\"start\":44674},{\"end\":44703,\"start\":44685},{\"end\":44861,\"start\":44847},{\"end\":44874,\"start\":44861},{\"end\":45132,\"start\":45120},{\"end\":45143,\"start\":45132},{\"end\":45155,\"start\":45143},{\"end\":45170,\"start\":45155},{\"end\":45179,\"start\":45170}]", "bib_venue": "[{\"end\":36368,\"start\":36347},{\"end\":36878,\"start\":36815},{\"end\":37162,\"start\":37100},{\"end\":38690,\"start\":38679},{\"end\":39191,\"start\":39170},{\"end\":39632,\"start\":39570},{\"end\":39996,\"start\":39909},{\"end\":40832,\"start\":40745},{\"end\":42423,\"start\":42410},{\"end\":43457,\"start\":43436},{\"end\":44030,\"start\":43939},{\"end\":45059,\"start\":44975},{\"end\":34845,\"start\":34795},{\"end\":34924,\"start\":34862},{\"end\":35142,\"start\":35138},{\"end\":35320,\"start\":35247},{\"end\":35606,\"start\":35556},{\"end\":35681,\"start\":35614},{\"end\":35833,\"start\":35755},{\"end\":36139,\"start\":36076},{\"end\":36345,\"start\":36271},{\"end\":36457,\"start\":36395},{\"end\":36793,\"start\":36726},{\"end\":36813,\"start\":36795},{\"end\":37098,\"start\":37021},{\"end\":37347,\"start\":37284},{\"end\":37458,\"start\":37355},{\"end\":37660,\"start\":37599},{\"end\":37825,\"start\":37755},{\"end\":38081,\"start\":38011},{\"end\":38297,\"start\":38233},{\"end\":38513,\"start\":38449},{\"end\":38677,\"start\":38626},{\"end\":38839,\"start\":38749},{\"end\":38978,\"start\":38936},{\"end\":39168,\"start\":39104},{\"end\":39387,\"start\":39383},{\"end\":39568,\"start\":39491},{\"end\":39907,\"start\":39822},{\"end\":40115,\"start\":40037},{\"end\":40192,\"start\":40170},{\"end\":40393,\"start\":40280},{\"end\":40743,\"start\":40658},{\"end\":41104,\"start\":41053},{\"end\":41308,\"start\":41283},{\"end\":41500,\"start\":41438},{\"end\":41706,\"start\":41641},{\"end\":41940,\"start\":41896},{\"end\":42176,\"start\":42114},{\"end\":42388,\"start\":42337},{\"end\":42408,\"start\":42390},{\"end\":42518,\"start\":42429},{\"end\":42676,\"start\":42612},{\"end\":43041,\"start\":42967},{\"end\":43223,\"start\":43159},{\"end\":43434,\"start\":43360},{\"end\":43734,\"start\":43660},{\"end\":43937,\"start\":43848},{\"end\":44130,\"start\":44057},{\"end\":44342,\"start\":44331},{\"end\":44458,\"start\":44363},{\"end\":44758,\"start\":44703},{\"end\":44973,\"start\":44874},{\"end\":45118,\"start\":45069}]"}}}, "year": 2023, "month": 12, "day": 17}