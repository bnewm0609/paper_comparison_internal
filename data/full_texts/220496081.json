{"id": 220496081, "updated": "2023-11-11 04:26:16.022", "metadata": {"title": "Ensuring Fairness Beyond the Training Data", "authors": "[{\"first\":\"Debmalya\",\"last\":\"Mandal\",\"middle\":[]},{\"first\":\"Samuel\",\"last\":\"Deng\",\"middle\":[]},{\"first\":\"Suman\",\"last\":\"Jana\",\"middle\":[]},{\"first\":\"Jeannette\",\"last\":\"Wing\",\"middle\":[\"M.\"]},{\"first\":\"Daniel\",\"last\":\"Hsu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 7, "day": 12}, "abstract": "We initiate the study of fair classifiers that are robust to perturbations in the training distribution. Despite recent progress, the literature on fairness has largely ignored the design of fair and robust classifiers. In this work, we develop classifiers that are fair not only with respect to the training distribution, but also for a class of distributions that are weighted perturbations of the training samples. We formulate a min-max objective function whose goal is to minimize a distributionally robust training loss, and at the same time, find a classifier that is fair with respect to a class of distributions. We first reduce this problem to finding a fair classifier that is robust with respect to the class of distributions. Based on online learning algorithm, we develop an iterative algorithm that provably converges to such a fair and robust solution. Experiments on standard machine learning fairness datasets suggest that, compared to the state-of-the-art fair classifiers, our classifier retains fairness guarantees and test accuracy for a large class of perturbations on the test set. Furthermore, our experiments show that there is an inherent trade-off between fairness robustness and accuracy of such classifiers.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": null, "mag": "3105518162", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/MandalDJWH20", "doi": null}}, "content": {"source": {"pdf_hash": "8ac7f60714087e6548edb008f33e401163bdc982", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.06029v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3eec0ea92f42a47597eb33bff99a80c2fac6d3f2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8ac7f60714087e6548edb008f33e401163bdc982.txt", "contents": "\nEnsuring Fairness Beyond the Training Data\nJuly 14, 2020\n\nDebmalya Mandal \nDepartment of Computer Science and Data Science Institute\nColumbia University\n\n\nSamuel Deng \nDepartment of Computer Science and Data Science Institute\nColumbia University\n\n\nSuman Jana \nDepartment of Computer Science and Data Science Institute\nColumbia University\n\n\nJeannette M Wing \nDepartment of Computer Science and Data Science Institute\nColumbia University\n\n\nDaniel Hsu \nDepartment of Computer Science and Data Science Institute\nColumbia University\n\n\nEnsuring Fairness Beyond the Training Data\nJuly 14, 2020\nWe initiate the study of fair classifiers that are robust to perturbations in the training distribution. Despite recent progress, the literature on fairness has largely ignored the design of fair and robust classifiers. In this work, we develop classifiers that are fair not only with respect to the training distribution, but also for a class of distributions that are weighted perturbations of the training samples. We formulate a min-max objective function whose goal is to minimize a distributionally robust training loss, and at the same time, find a classifier that is fair with respect to a class of distributions. We first reduce this problem to finding a fair classifier that is robust with respect to the class of distributions. Based on online learning algorithm, we develop an iterative algorithm that provably converges to such a fair and robust solution. Experiments on standard machine learning fairness datasets suggest that, compared to the state-of-the-art fair classifiers, our classifier retains fairness guarantees and test accuracy for a large class of perturbations on the test set. Furthermore, our experiments show that there is an inherent trade-off between fairness robustness and accuracy of such classifiers.\n\nIntroduction\n\nMachine learning (ML) systems are often used for high-stakes decision-making, including bail decision and credit approval. Often these applications use algorithms trained on past biased data, and such bias is reflected in the eventual decisions made by the algorithms. For example, Bolukbasi et al. [9] show that popular word embeddings implicitly encode societal biases, such as gender norms. Similarly, Buolamwini and Gebru [10] find that several facial recognition softwares perform better on lighter-skinned subjects than on darker-skinned subjects. To mitigate such biases, there have been several approaches in the ML fairness community to design fair classifiers [4,20,37].\n\nHowever, the literature has largely ignored the robustness of such fair classifiers. The \"fairness\" of such classifiers are often evaluated on the sampled datasets, and are often unreliable because of various reasons including biased samples, missing and/or noisy attributes. Moreover, compared to the traditional machine learning setting, these problems are more prevalent in the fairness domain, as the data itself is biased to begin with. As an example, we consider how the optimized pre-processing algorithm [11] performs on ProPublica's COMPAS dataset [1] in terms of demographic parity (DP), which measures the difference in accuracy between the protected groups. Figure 1 shows two situations -(1) unweighted training distribution (in blue), and (2) weighted training distributions (in red). The optimized pre-processing algorithm [11] yields a classifier that is almost fair on the unweighted training set (DP \u2264 0.02). However, it has DP of at least 0.2 on the weighted set, despite the fact that the marginal distributions of the features look almost the same for the two scenarios. This example motivates us to design a fair classifier that is robust to such perturbations. We also show how to construct such weighted examples using a few linear programs. Contributions: In this work, we initiate the study of fair classifiers that are robust to perturbations in the training distribution. The set of perturbed distributions are given by any arbitrary weighted combinations of the training dataset, say W. Our main contributions are the following: \u2022 We develop classifiers that are fair not only with respect to the training distribution, but also for the class of distributions characterized by W. We formulate a min-max objective whose goal is to minimize a distributionally robust training loss, and simultaneously, find a classifier that is fair with respect to the entire class.\n\n\u2022 We first reduce this problem to finding a fair classifier that is robust with respect to the class of distributions. Based on online learning algorithm, we develop an iterative algorithm that provably converges to such a fair and robust solution.\n\n\u2022 Experiments on standard machine learning fairness datasets suggest that, compared to the state-of-the-art fair classifiers, our classifier retains fairness guarantees and test accuracy for a large class of perturbations on the test set. Furthermore, our experiments show that there is an inherent trade-off between fairness robustness and accuracy of such classifiers. Related Work: Numerous proposals have been laid out to capture bias and discrimination in settings where decisions are delegated to algorithms. Such formalization of fairness can be statistical [14,20,22,23,30], individual [13,33], causal [25,27,38], and even procedural [19]. We restrict attention to statistical fairness, which fix a small number of groups in the population and then compare some statistic (e.g., accuracy, false positive rate) across these groups. We mainly consider the notion of demographic parity [14,22,23] and equalized odds [20] in this paper, but our method of designing robust and fair classifiers can be adapted to any type of statistical fairness.\n\nOn the other hand, there are three main approaches for designing a fair classifier. The pre-processing approach tries to transform training data and leverage standard classifiers [11,14,22,37]. The in-processing approach, on the other hand, directly modifies the learning algorithm to meet the fairness criteria [4,15,24,36]. The post-processing approach, however, modifies the decisions of a classifier [20,30] to make it fair. Ours is an in-processing approach and mostly related to [4,5,24]. Agarwal et al. [4] and Alabi et al. [5] show how binary classification problem with group fairness constraints can be reduced to a sequence of cost-sensitive classification problems. Kearns et al. [24] follow a similar approach, but instead consider a combinatorial class of subgroup fairness constraints. Recently, [7] integrated and implemented a range of such fair classifiers in a GitHub project, which we leverage in our work.\n\nIn terms of technique, our paper falls in the category of distributionally robust optimization (DRO), where the goal is to minimize the worst-case training loss for any distribution that is close to the training distribution by some metric. Various types of metrics have been considered including bounded f -divergence [8,28], Wasserstein distance [2,18], etc. To the best of our knowledge, prior literature has largely ignored enforcing constraints such as fairness in a distributionally robust sense. Further afield, our work has similarity with recent work in fairness testing inspired by the literature on program verification [6,17,34]. These papers attempt to automatically discover discrimination in decision-making programs, whereas we develop tools based on linear program to discover distributions that expose potential unfairness.\n\n\nProblem and Definitions\n\nWe will write ((x, a), y) to denote a training instance where a \u2208 A denotes the protected attributes, x \u2208 X denotes all the remaining attributes, and y \u2208 {0, 1} denotes the outcome label. For a hypothesis h, h(x, a) \u2208 {0, 1} denotes the outcome predicted by it, on an input (x, a). We assume that the set of hypothesis is given by a class H. Given a loss function : {0, 1} \u00d7 {0, 1} \u2192 R, the goal of a standard fair classifier is to find a hypothesis h * \u2208 H that minimizes the training loss n i=1 (h(x i , a i ), y i ) and is also fair according to some notion of fairness.\n\nWe aim to design classifiers that are fair with respect to a class of distributions that are weighted perturbations of the training distribution. Let W = {w \u2208 R n + : i w i = 1} be the set of all possible weights. For a hypothesis h and weight w, we define the weighted empirical risk, (h, w) = n i=1 w i (h(x i , a i ), y i ). We will write \u03b4 w F (f ) to define the \"unfairness gap\" with respect to the weighted empirical distribution defined by the weight w and fairness constraint F (e.g., demographic parity (DP) or equalized odds (EO)). For example,\n\u03b4 w DP (f ) is defined as \u03b4 w DP (f ) = max a,a \u2208A i:ai=a w i f (x i , a) i:ai=a w i \u2212 i:ai=a w i f (x i , a ) i:ai=a w i .(1)\nTherefore, \u03b4 w DP (f ) measures the maximum weighted difference in accuracy between two groups with respect to the distribution that assigns weight w to the training examples. On the other hand, \u03b4 w\nEO (f ) = 1/2(\u03b4 w EO (f |0) + \u03b4 2 EO (f |1)) 1 , where \u03b4 w EO (f |y) is defined as \u03b4 w EO (f |y) = max a,a \u2208A i:ai=a,yi=y w i f (x i , a) i:ai=a,yi=y w i \u2212 i:ai=a ,yi=y w i f (x i , a ) i:ai=a ,yi=y w i .\nTherefore, \u03b4 w EO (f |0) (resp., \u03b4 w EO (f |1)) measures the weighted difference in false (resp., true) positive rates between the two groups with respect to the weight w. We will develop our theory using DP as an example of a notion of fairness, but our experimental results will concern both DP and EO.\n\nWe are now ready to formally define our main objective. For a class of hypothesis H, let H W = {h \u2208 H : \u03b4 w F (h) \u2264 \u2200w \u2208 W} be the set of hypothesis that are -fair with respect to all the weights in the set W. Our goal is to solve the following min-max problem:\nmin h\u2208H W max w\u2208W (h, w)(2)\nTherefore, we aim to minimize a robust loss with respect to a class of distributions indexed by W. Additionally, we also aim to find a classifier that is fair with respect to such perturbations. We allow our algorithm to output a randomized classifier, i.e., a distribution over the hypothesis H. This is necessary if the space H is non-convex or if the fairness constraints are such that the set of feasible hypothesis H W is non-convex. For a randomized classifier \u00b5, its weighted empirical risk is (\u00b5, w) = h \u00b5(h) (h, w), and its expected unfairness gap is \u03b4 w\nF (\u00b5) = h \u00b5(h)\u03b4 w F (h).\n\nDesign\n\nOur algorithm follows a top-down fashion. First we design a meta algorithm that reduces the min-max problem of Equation (2) to a loss minimization problem with respect to a sequence of weight vectors. Then we show how we can design a fair classifier that performs well with respect a fixed weight vector w \u2208 W in terms of accuracy, but is fair with respect to the entire set of weights W.\n\n\nMeta Algorithm\n\nALGORITHM 1: Meta-Algorithm Input: Training Set: {xi, ai, yi} n i=1 , set of weights: W, hypothesis class H, parameters T and \u03b7. Set \u03b7 = 2/Tm and w0\n(i) = 1/n for all i \u2208 [n] h0 = ApxFair(w0) /* Approximate solution of arg min h\u2208H W n i=1 (h(x i , a i ), y i ). */ for each t \u2208 [Tm] do wt = wt\u22121 + \u03b7\u2207w (ht\u22121, wt\u22121) wt = \u03a0W (wt) /* Project wt onto the set of weights W. */ ht = ApxFair(wt) /* Approximate solution of min h\u2208H W n i=1 wt(i) (h(x i , a i ), y i )]. */ end Output: h f : Uniform distribution over {h0, h1, . . . , hT }.\nAlgorithm 1 provides a meta algorithm to solve the min-max optimization problem defined in Equation (2). The algorithm is based on ideas presented in [12], which, given an \u03b1-approximate Bayesian oracle for distributions over loss functions, provides an \u03b1-approximate robust solution. The algorithm can be viewed as a two-player zero-sum game between the learner who picks the hypothesis h t , and an adversary who picks the weight vector w t . The adversary performs a projected gradient descent every step to compute the best response. On the other hand, the learner solves a fair classification problem to pick a hypothesis which is fair with respect to the weights W and minimizes weighted empirical risk with respect to the weight w t . However, it is infeasible to compute an exact optima of the problem\nmin h\u2208H W n i=1 w t (i) (h(x i , a i ), y i )].\nSo the learner uses an approximate fair classifier ApxFair(\u00b7), which we define next.\nDefinition 1. ApxFair(\u00b7) is an \u03b1-approximate fair classifier, if for any weight w \u2208 R n + , ApxFair(w) returns a hypothesis h such that n i=1 w i ( h(x i , a i ), y i ) \u2264 min h\u2208H W n i=1 w i (h(x i , a i ), y i ) + \u03b1.\nUsing the \u03b1-approximate fair classifier, we have the following guarantee on the output of Algorithm 1.\n\nTheorem 1. Suppose the loss function (\u00b7, \u00b7) is convex in its first argument and ApxFair(\u00b7) is an \u03b1-approximate fair classifier. Then, the hypothesis h f , output by Algorithm 1 satisfies\nmax w\u2208W E h\u223ch f n i=1 w i (h(x i , a i ), y i ) \u2264 min h\u2208H W max w\u2208W (h, w) + 2 T m + \u03b1\nThe proof uses ideas from [12], except that we use an additive approximate best response. 2\n\n\nApproximate Fair Classifier\n\nIn this section, we develop an \u03b1-approximate fair and robust classifier by following three steps. 1. Discretize the set of weights W, so that it is sufficient to design an approximate fair classifier with respect to the set of discretized weights. In particular, if we discretize each weight up to a multiplicative error , then developing an \u03b1-approximate fair classifier with respect to the discretized weights gives O(\u03b1 + )-fair classifier with respect to the set W.\n\n2. Set up a two-player zero-sum game for the problem of designing an approximate fair classifier with respect to the set of discretized weights. Here, the learner chooses a hypothesis, whereas an adversary picks the most \"unfair\" weight in the set of discretized weights.\n\nWe point out that Agarwal et al. [4] was the first to show that the design of a fair classifier can be formulated as a two-player zero-sum game (step 2). However, they only considered group-fairness constraints with respect to the training distribution. The algorithm of Alabi et al. [5] has similar limitations. On the other hand, we consider the design of robust and fair classifier and had to include an additional discretization step (1). Finally, the design of our learning algorithm and the best response oracle is significantly different than [4,5,24]. For the remainder of this subsection, we assume that the meta algorithm (Algorithm 1) has called the ApxFair(\u00b7) with a weight vector w 0 and our goal is to design a classifier that minimizes weighted empirical risk with respect to the weight w 0 , but is fair with respect to the set of all weights W, i.e., find f \u2208 arg min h\u2208H W (h, w 0 ).\n\n\nDiscretization of the Weights\n\nWe first discretize the set of weights W as follows. Divide the interval\n[0, 1] into buckets B 0 = [0, \u03b4), B j+1 = [(1 + \u03b3 1 ) j \u03b4, (1 + \u03b3 1 ) j+1 \u03b4) for j = 0, 1, . . . , M \u2212 1 for M = log 1+\u03b31 (1/\u03b4))\n. For any weight w \u2208 W, construct a new weight w = (w 1 , . . . , w n ) by setting w i to be the upper-end point of the bucket containing\nw i , for each i \u2208 [n]\n.\n\nWe now substitute \u03b4 = \u03b31 2n and write N (\u03b3 1 , W) to denote the set containing all the discretized weights of the set W. The next lemma shows that a fair classifier for the set of weights N (\u03b3 1 , W), is also a fair classifier for the set of weights W up to an error 4\u03b3 1 .\nLemma 1. If \u2200w \u2208 N (\u03b3 1 , W), \u03b4 w DP (f ) \u2264 , then we have \u03b4 w DP (f ) \u2264 + 4\u03b3 1 for any w \u2208 W.\nTherefore, in order to ensure that \u03b4 w DP (f ) \u2264 \u03b5 we discretize the set of weights W and enforce \u03b5 \u2212 4\u03b3 1 fairness for all the weights in the set N (\u03b3 1 , W). This result makes our work easier as we need to guarantee fairness with respect to a finite set of weights N (\u03b3 1 , W), instead of a large and continuous set of weights W. However, note that, the number of weights in N (\u03b3 1 , W) can be O log n 1+\u03b31 (2n/\u03b3 1 ) , which is exponential in n. We next see how to avoid this problem.\n\n\nSetting up a Two-Player Zero-Sum Game\n\nWe formulate the problem of designing a fair and robust classifier with respect to the set of weights in N (\u03b3 1 , W) as a two-player zero-sum game. Let R(w, a, f ) = i:a i =a wif (xi,a) i:a i =a wi . Then \u03b4 w DP (f ) = sup a,a |R(w, a, f ) \u2212 R(w, a , f )|. Our aim is to solve the following problem.\nmin h\u2208H n i=1 w 0 i (h(x i , a i ), y i )(3)s.t. R(w, a, h) \u2212 R(w, a , h) \u2264 \u03b5 \u2212 4\u03b3 1 \u2200w \u2208 N (\u03b3 1 , W) \u2200a, a \u2208 A\nWe form the following Lagrangian.\nmin h\u2208H max \u03bb 1\u2264B n i=1 w 0 i (h(x i , a i ), y i ) + w\u2208N (\u03b31,W) a,a \u2208A \u03bb a,a w (R(w, a, h)\u2212R(w, a , h)\u2212\u03b5+4\u03b3 1 ).(4)\nNotice that we restrict the 1 -norm of the Lagrangian multipliers by the parameter B. We will later see how to choose this parameter B. We first convert the optimization problem define in Equation (4) as a two-player zero-sum game. Here the learner's pure strategy is to play a hypothesis h in H. Given the learner's hypothesis h \u2208 H, the adversary picks the constraint (weight w and groups a, a ) that violates fairness the most and sets the corresponding coordinate of \u03bb to B. Therefore, for a fixed hypothesis h, it is sufficient for the adversary to play a vector \u03bb such that either all the coordinates of \u03bb are zero or exactly one is set to B. For such a pair (h, \u03bb) of hypothesis and Largangian multipliers, we define the payoff matrix as\nU (h, \u03bb) = n i=1 w 0 i (h(x i , a i ), y i ) + w\u2208N (\u03b31,W) a,a \u2208A \u03bb a,a w (R(w, a, h) \u2212 R(w, a , h) \u2212 \u03b5 + 4\u03b3 1 )\nNow our goal is to compute a \u03bd-approximate minimax equilibrium of this game. In the next subsection, we design an algorithm based on online learning. But we first see how both the h-and \u03bb-players compute their best responses. These are the main components of the algorithm discussed later.\n\nBest response of the h-player: For each i \u2208 [n], we introduce the following notation\n\u2206 i = w\u2208N (\u03b31,W) a =ai \u03bb ai,a w \u2212 \u03bb a ,ai w w i j:aj =ai w j\nWith this notation, the payoff becomes\nU (h, \u03bb) = n i=1 w 0 i (h(x i , a i ), y i ) + \u2206 i h(x i , a i ) \u2212 (\u03b5 \u2212 4\u03b3 1 ) w\u2208N (\u03b5/5,W) a,a \u2208A \u03bb a,a w\nLet us introduce the following costs.\nc 0 i = (0, 1)w 0 i if y i = 1 (0, 0)w 0 i if y i = 0 c 1 i = (1, 1)w 0 i + \u2206 i if y i = 1 (1, 0)w 0 i + \u2206 i if y i = 0(5)\nThen the h-player's best response becomes the following cost-sensitive classification problem.\nh \u2208 arg min h\u2208H n i=1 c 1 i h(x i , a i ) + c 0 i (1 \u2212 h(x i , a i ))(6)\nTherefore, as long as we have access to an oracle for the cost-sensitive classification problem, the h-player can compute its best response. Note that, the notion of a cost-sensitive classification as an oracle was also used by [4,24]. In general, solving this problem is NP-hard, but there are several efficient heuristics that perform well in practice. We provide further details about how we implement this oracle in the section devoted to the experiments.\n\nBest response of the \u03bb-player: Since the fairness constraints depend on the weights non-linearly (e.g., see Eq.\n\n(1)), finding the most violating constraint is a non-linear optimization problem. However, we can guess the marginal probabilities over the protected groups. If we are correct, then the most violating weight vector can be found by a linear program. Since we cannot exactly guess this particular value, we instead discretize the set of marginal probabilities, iterate over them, and choose the option with largest violation in fairness. This intuition can be formalized as follows. We discretize the set of all marginals over |A| groups by the following rule. First discretize [0, 1] as 0, \u03b4, (1 + \u03b3 2 ) j \u03b4 for j = 1, 2, . . . , M for M = O(log 1+\u03b32 (1/\u03b4)). This discretizes [0, 1] A into M |A| points, and then retain the discretized marginals whose total sum is at most 1 + \u03b3 2 , and discard all other points. Let us denote the set of such marginals as \u03a0(\u03b3 2 , A). Algorithm 2 goes through all the marginals \u03c0 in \u03a0(\u03b3 2 , A) and for each such tuple and a pair of groups a, a finds the weight w which maximizes R(w, a, h) \u2212 R(w, a , h). Note that this can be solved using a linear Program as the weights assigned to a group is fixed by the marginal tuple \u03c0. Out of all the solutions, the algorithm picks the one with the maximum value. Then it checks whether this maximum violates the constraint (i.e., greater than \u03b5). If so, it sets the corresponding \u03bb value to B and everything else to 0. Otherwise, it returns the zero vector. As the weight returned by the LP need not correspond to a weight in N (\u03b3 1 , W), it rounds the weight to the nearest weight in N (\u03b3 1 , W). For discretizing the marginals we will set \u03b4 = (1 + \u03b3 2 ) \u03b31 n , which implies that the number of LPs run by Algorithm 2 is at most O(log i:a i =a w(a, a , \u03c0)ih(xi, a ) end Set (a * 1 , a * 2 , \u03c0 * ) = arg max a,a ,\u03c0 val(a, a , \u03c0) if val(a * 1 , a * 2 , \u03c0 * ) > \u03b5 then Let w = w(a * 1 , a * 2 , \u03c0 * ). For each i \u2208 [n], let w i be the upper-end point of the bucket containing wi.\nreturn \u03bb a,a w = B if (a, a , w) = (a * 1 , a * 2 , w ) 0 o.w. else return 0\nLemma 2. Algorithm 2 is an B(4\u03b3 1 + \u03b3 2 )-approximate best response for the \u03bb-player-i.e., for any h \u2208 H,\nit returns \u03bb * such that U (h, \u03bb * ) \u2265 max \u03bb U (h, \u03bb) \u2212 B(4\u03b3 1 + \u03b3 2 ).\nLearning Algorithm: We now introduce our algorithm for the problem defined in Equation (4). In this algorithm, the \u03bb-player uses Algorithm 2 to compute an approximate best response, whereas the h-player uses Regularized Follow the Leader (RFTL) algorithm [3,32] as its learning algorithm. RFTL is a classical algorithm for online convex optimization (OCO). In OCO, the decision maker takes a decision x t \u2208 K at round t, an adversary reveals a convex loss function f t : K \u2192 R, and the decision maker suffers a loss of f t (x t ).\n\nThe goal is to minimize regret, which is defined as\nmax u\u2208K { T t=1 f t (x t ) \u2212 f t (u)},\ni.e., the difference between the loss suffered by the learner and the best fixed decision. RFTL requires a strongly convex regularization function R : K \u2192 R \u22650 , and chooses x t according to the following rule:\nx t = arg min x\u2208K \u03b7 t\u22121 s=1 \u2207f s (x s ) T x + R(x).\nWe use RFTL in our learning algorithm as follows. We set the regularization function R(x) = 1/2 x 2 2 , and loss function f t (h t ) = U (h t , \u03bb t ) where \u03bb t is the approximate best-response to h t . Therefore, at iteration t the learner needs to solve the following optimization problem.\nh t \u2208 arg min h\u2208H \u03b7 t\u22121 s=1 \u2207 hs U (h s , \u03bb s ) T h + 1 2 h 2 2 .(7)\nHere with slight abuse of notation we write H to include the set of randomized classifiers, so that h(x i , a i ) is interpreted as the probability that hypothesis h outputs 1 on an input (x i , a i ). Now we show that the optimization problem (Eq. (7)) can be solved as a cost-sensitive classification problem. For a given \u03bb s , the best response of the learner is the following:\nh \u2208 arg min h\u2208H n i=1 c 1 i (\u03bb s )h(x i , a i ) + c 0 i (\u03bb s )(1 \u2212 h(x i , a i )) Writing L i (\u03bb s ) = c 1 i (\u03bb s ) \u2212 c 0 i (\u03bb s ), the objective becomes n i=1 L i (\u03bb s )h(x i , a i ).\nHence, \u2207 hs U (h s , \u03bb s ) is linear in h s and equals the vector {L i (\u03bb s )} n i=1 . With this observation, the objective in Equation (7) becomes\n\u03b7 t s=1 n i=1 L(\u03bb s )h(x i , a i ) + 1 2 n i=1 (h(x i , a i )) 2 \u2264 \u03b7 n i=1 L t s=1 \u03bb s h(x i , a i ) + 1 2 n i=1 h(x i , a i ) = n i=1 \u03b7L t s=1 \u03bb s + 1 2 h(x i , a i ).\nThe first inequality follows from two observations -L(\u03bb) is linear in \u03bb, and, since the predictions h(x i , a i ) \u2208 [0, 1] we replace the quadratic term by a linear term, an upper bound. 3 Finally, we observe that even though the number of weights in N (\u03b3 1 , W) is exponential in n, Algorithm 3 can be efficiently implemented. This is because the best response of the \u03bb-player always returns a solution where all the entries are zero or exactly one is set to B. Therefore, instead of recording the entire \u03bb vector the algorithm can just record the non-zero variables and there will be at most T of them. The next lemma provides performance guarantees of Algorithm 3.\n\n\nALGORITHM 3: Approximate Fair Classifier (ApxFair)\nInput: \u03b7 > 0, weight w 0 \u2208 R n + , number of rounds T Set h1 = 0 for t \u2208 [T ] do \u03bbt = Best \u03bb (ht) Set \u03bbt = t t =1 \u03bb t ht+1 = arg min h\u2208H n i=1 (\u03b7Li( \u03bbt) + 1/2)h(xi, ai) end return Uniform distribution over {h1, . . . , hT }.\nTheorem 2. Given a desired fairness level \u03b5, if Algorithm 3 is run for T = O n \u03b5 2 rounds, then the ensemble hypothesis h provides the following guarantee:\nn i=1 w 0 i ( h(x i , a i ), y i ) \u2264 min h\u2208H n i=1 w 0 i (h(x i , a i ), y i ) + O(\u03b5) and \u03b4 w DP ( h) \u2264 2\u03b5 \u2200w \u2208 W.\n\nExperiments\n\nWe evaluated our algorithm on four datasets commonly-used in the literature: COMPAS [1], Adult [26], Communities and Crime [31], and Law School [35]. (See the supplementary material for details.) We compared our algorithm to: a pre-processing method of [22], an in-processing method of [4], a post-processing method of [20]. For our algorithm, we use scikit-learn's logistic regression [29] as the learning algorithm in Algorithm 3. We also show the performance of unconstrained logistic regression. Code is available at the github repo: https://github.com/essdeee/FairnessChecking. Setup. We compute the maximum violating weight by solving a LP that is the same as the one used by the best response oracle (Algorithm 2), except that we restrict individual weights to be in the range [(1 \u2212 )/n, (1 + )/n], and keep protected group marginals the same. (See supplementary material for details.) This keeps the 1 distance between weighted and unweighted distributions within .\n\nResults. Figure 2 compares the robustness of our classifier against the other classifiers, and we see that for both DP and EO, the fairness violation of our classifier grows more slowly as increases, compared to the others, suggesting robustness to -perturbations of the distribution. Our algorithm also performs comparatively well in both accuracy and fairness violation to the existing fair classifiers, though there is a Figure 2: DP and EO Comparison. We vary the 1 distance on the x-axis and plot the fairness violation on the y-axis. We use five random 80%-20% train-test splits to evaluate test accuracy and fairness. The bands across each line show standard error. For both DP and EO fairness, our algorithm is significantly more robust to reweightings that are within 1 distance on most datasets.\n\ntrade-off between robustness and test accuracy. The unweighted test accuracy of our algorithm drops by at most 5%-10% on all datasets, suggesting that robustness comes at the expense of test accuracy on the original distribution. However, on the test set (which is typically obtained from the same source as the original training data), the difference in fairness violation between our method and other methods is almost negligible on all the datasets, except for the COMPAS dataset, where the difference it at most 12%. See the appendix for full details of this trade-off.\n\n\nConclusion and Future Directions\n\nIn this work, we study the design of fair classifiers that are robust to weighted perturbations of the dataset. An immediate future work is to consider robustness against a broader class of distributions like the set of distributions with a bounded f -divergence or Wasserstein distance from the training distribution. We also considered statistical notions of fairness and it would be interesting to perform a similar fairness vs robustness analysis for other notions of fairness. of the projected gradient descent algorithm, we have\n1 T m Tm t=1 (h t , w t ) \u2265 max w\u2208W 1 T m Tm t=1 (h t , w) \u2212 max w\u2208W w 2 2 T m \u2265 max w\u2208W 1 T m Tm t=1 (h t , w) \u2212 2 T m\nThe last inequality follows because the weights always sum to one, so\nw 2 \u2264 w 1 \u2264 1. v * = min h\u2208H W max w\u2208W (h, w) \u2265 min h\u2208H W 1 T m Tm t=1 (h, w t ) \u2265 1 T m Tm t=1 min h\u2208H W (h, w t ) \u2265 1 T m Tm t=1 (h t , w t ) \u2212 \u03b1 = 1 T m Tm t=1 (h t , w t ) \u2212 \u03b1 \u2265 max w\u2208W 1 T m Tm t=1 (h t , w) \u2212 2 T m \u2212 \u03b1\nThe third inequality follows from the \u03b1-approximate fairness of ApxFair(\u00b7). Now rearranging the last inequality we get max w\u2208W 1 Tm Tm t=1 (h t , w) \u2264 v * + 2/T m + \u03b1, the desired result.\n\n\nA.3 Proof of Lemma 1\n\nRecall the definition of demographic parity with respect to a weight vector w.\n\u03b4 w DP (f ) = i:ai=a w i f (x i , a) i:ai=a w i \u2212 i:ai=a w i f (x i , a ) i:ai=a w i\nFor a given weight w, we construct a new weight w = (w 1 , . . . , w n ) as follows. For each i \u2208 [n], w i is the upper-end point of the bucket containing w i . Note that this guarantees that either w i \u2264 \u03b4 or w i 1+\u03b31 \u2264 w i \u2264 w i . We now establish the following lower bound. \ni:ai=a w i f (x i , a) i:ai=a w i \u2265 1 1 + \u03b3 1 i:ai=a w i f (x i , a) i:ai=a w i \u2265 (1 \u2212 \u03b3 1 ) i:ai=a w i f (x i , a) i:ai=a w i(8)w i + n\u03b4\nThis gives us the following.\ni:ai=a w i f (x i , a) i:ai=a w i \u2264 i:ai=a w i f (x i , a) 1 1+\u03b31 i:ai=a w i \u2212 n\u03b4 1+\u03b31 \u2264 (1 + \u03b3 1 ) i:ai=a w i f (x i , a) i:ai=a w i \u2212 n\u03b4\nNow we substitute, \u03b4 = \u03b3 1 /(2n) and get the following upper bound.\ni:ai=a w i f (x i , a) i:ai=a w i \u2264 (1 + \u03b3 1 ) i:ai=a w i f (x i , a) i:ai=a w i \u2212 \u03b3 1 /2 \u2264 1 + \u03b3 1 1 \u2212 \u03b3 1 i:ai=a w i f (x i , a) i:ai=a w i \u2264 (1 + 3\u03b3 1 ) i:ai=a w i f (x i , a) i:ai=a w i(9)\nNow we bound \u03b4 w DP (f ) using the results above. \n\u03b4 w DP (f ) \u2264 (1 + 3\u03b3 1 ) i:ai=a w i f (x i , a) i:ai=a w i \u2212 (1 \u2212 \u03b3 1 ) i:ai=a w i f (x i , a) i:ai=a w i \u2264 i:ai=a w i f (x i , a) i:ai=a w i \u2212 i:ai=a w i f (x i , a) i:ai=a w i + 4\u03b3 1 \u2264 \u03b4 w DP (f ) + 4\u03b3 1\nThe first inequality uses the upper bound for the first term (Eq. (9)) and the lower bound for the second term (Eq. (8)). The proof when the first term is less than the second term in the definition of \u03b4 w DP (f ) is similar. Therefore, if we guarantee that \u03b4 w DP (f ) \u2264 \u03b5, we have \u03b4 w DP (f ) \u2264 \u03b5 + 4\u03b3 1 .\n\n\nA.4 Proof of Lemma 2\n\nWe need to consider two cases. First, suppose that R(w, a, h) \u2212 R(w, a , h) \u2264 \u03b5 \u2212 4\u03b3 1 for all w \u2208 N (\u03b3 1 , W) and a, a \u2208 A. Then, \u03b4 w DP (h) = sup a,a \u2208A |R(w, a, h) \u2212 R(w, a , h)| \u2264 \u03b5 \u2212 4\u03b3 1 for any weight w \u2208 N (\u03b3 1 , W). Therefore, by Lemma 1, for any weight w \u2208 W, we have \u03b4 w DP (h) \u2264 \u03b5. Now, for any marginal \u03c0 \u2208 \u03a0(\u03b3 2 , A), and a, a consider the corresponding linear program. We show that the optimal value of the LP is bounded by \u03b5. Indeed, consider any weight w satisfying the marginal conditions, i.e., i:ai=a w i = \u03c0 a and i:ai=a w i = \u03c0 a . Then, the objective of the LP is\n1 \u03c0 a i:ai=a w i h(x i , a) \u2212 1 \u03c0 a i:ai=a w i h(x i , a ) \u2264 sup w\u2208W \u03b4 w DP (h) \u2264 \u03b5.\nThis implies that the optimal value of the LP is always less than \u03b5. So Algorithm 2 returns the zero vector, which is also the optimal solution in this case. Second, there exists w \u2208 N (\u03b3 1 , W) and groups a, a such that R(w, a, h) \u2212 R(w, a , h) > \u03b5 \u2212 4\u03b3 1 and in particular let (w * , a * 1 , a * 2 ) \u2208 arg max w,a,a T (w, a, h) \u2212 T (w, a , h). Then the optimal solution sets \u03bb a * 1 ,a * 2 w * to B and everything else to zero. Let \u03c0 a * 1 and \u03c0 a * 2 be the corresponding marginals for groups a * 1 and a * 2 , and let \u03c0 a * 1 and \u03c0 a * 2 be the upper-end points of the buckets containing \u03c0 a * 1 and \u03c0 a * 2 respectively. As \u03c0 a * 1 is marginal for a weight belonging to the set N (\u03b3 1 , W) and any weight in N (\u03b3 1 , W) puts at least 2\u03b3 1 /n on any training instance, we are always guaranteed that\n\u03c0 a * 1 \u2265 2\u03b3 1 n \u2265 \u03b4 1 + \u03b3 2 .\nThis guarantees the following inequalities \u03c0 a * 1 1 + \u03b3 2 \u2264 \u03c0 a * 1 \u2264 \u03c0 a * 1 . Similarly, we can show that \u03c0 a * 2 1 + \u03b3 2 \u2264 \u03c0 a * 2 \u2264 \u03c0 a * 2 . Now, consider the LP corresponding to the marginal \u03c0 and subgroups a * 1 and a * 2 .\n1 \u03c0 a * 1 i:ai=a * 1 w i h(x i , a * 1 ) \u2212 1 \u03c0 a * 2 i:ai=a * 2 w i h(x i , a * 2 ) \u2265 1 (1 + \u03b3 2 )\u03c0 a * 1 i:ai=a * 1 w i h(x i , a * 1 ) \u2212 1 \u03c0 a * 2 i:ai=a * 2 w i h(x i , a * 2 ) \u2265 (1 \u2212 \u03b3 2 )R(w, a * 1 , h) \u2212 R(w, a * 2 , h) \u2265 R(w, a * 1 , h) \u2212 R(w, a * 2 , h) \u2212 \u03b3 2\none \u03bb variable is set to B. Therefore, by Theorem 3, for any hypothesis h \u2208 H,\nT t=1 n i=1 L(\u03bb t ) i h t (x i , a i ) \u2212 n i=1 L(\u03bb t ) i h(x i , a i ) \u2264 (2M + B) \u221a nT\nThe previous chain of inequalities also give B max (w,a,a )\nT (w, a, h) \u2212 T (w, a , h) \u2212 \u03b5 + 4\u03b3 1 \u2264 n i=1 w 0 i (h * (x i , a i ), y i ) + 2\u03bd \u2264 M + 2\u03bd.\nThis implies that for all weights w \u2208 N (\u03b3 1 , W) the maximum violation of the fairness constraint is (M +2\u03bd)/B, which in turn implies a bound of at most (M + 2\u03bd)/B + \u03b5 on the fairness constraint with respect to any weight w \u2208 W.\n\n\nB Description of the Experiment\n\nWe used four datasets for our experiments, averaging over the results from five different 80%-20% train-test splits. For Adult, Communities and Crime, and Law School we used the preprocessed versions found in the accompanying GitHub repository of [24] 4 . For COMPAS, we just used a sample from the original dataset. 5 All three datasets have a binary outcome variable, and a single binary protected attribute.\n\n\u2022 Adult. In this dataset [26], each example represents an adult individual, the outcome variable is whether that individual makes over $50k a year, and the protected attribute is gender. We work with a balanced and preprocessed version with 2,020 examples and 98 features, selected from the original 48,882 examples.\n\n\u2022 Communities and Crime. In this dataset from the UCI repository [31], each example represents a community. The outcome variable is whether the community has a violent crime rate in the 70th percentile of all communities, and the protected attribute is whether the community has a majority white population. We used the full dataset of 1,994 examples and 123 features.\n\n\u2022 Law School. (US LSAC study) Here each example represents a law student, the outcome variable is whether the law student passed the bar exam or not, and the protected attribute is race (white or not white). We used a preprocessed and balanced subset with 1,823 examples and 17 features [35].\n\n\u2022 COMPAS. In this dataset, each example represents a defendant. The outcome variable is whether a certain individual will recidivate (commit another crime), where the features include the individual's demographic information and criminal history. The protected attribute is race (white or black). We used a 2,000 example sample from the full dataset, to match the training set size to the other datasets we evaluated. To find the correct hyperparameters of B, \u03b7, T , and T m for our algorithm, we simply fixed T = 10 for EO and T = 5 for DP, and used a Grid Search technique on the hyperparameters B, \u03b7, and T m . Because we aimed for acceptable performance on validation accuracy and validation fairness for T m and T much below the worst-case bound, empirically tuning B and \u03b7 were necessary. We fixed T m = 500 and T = 10 for each dataset in EO and T m = 1, 000 and T = 10 for each dataset in DP, and we tuned \u03b7 and B accordingly on validation data for the results in this paper. For higher values of T m , we found that accuracy increased and fairness violations decreased, at the cost of computing resources. For full description of the used hyperparameters, we refer the reader to our code in the supplementary material.\n\n\nC Fairness vs. Accuracy Tradeoff\n\nIn Figure 3, we see the accuracy and fairness violation of our algorithm against the other state-of-the-art fair classifiers. We find that, though the fairness violation is mostly competitive with the existing fair classifiers, robustness against weighted perturbations comes at the expense of somewhat lower test accuracy. Figure 3: Fairness v. Accuracy. We plot the accuracy (x-axis) vs. the fairness violation (y-axis) for demographic parity and equalized odds for our robust and fair classifier. The reported values are averages over five random 80%-20% train-test splits, with standard error bars. We observe that the fairness violation is mostly comparable to existing state-of-the-art fair classifiers, though robustness comes at the expense of somewhat lower test accuracy.\n\nFigure 1 :\n1Unweighted vs Reweighted COMPAS dataset. The marginals of the two distributions are almost the same, but standard fair classifiers show demographic parity of at least 0.2 on the reweighted dataset.\n\n\n\u03b31 )) = O(poly(log n)), as the number of groups is fixed. ALGORITHM 2: Best Response of the \u03bb-player Input: Training Set: {xi, ai, yi} n i=1 , and hypothesis h \u2208 H. for each \u03c0 \u2208 \u03a0(\u03b32, A) and a, a \u2208 A do Solve the following LP:\nWe consider the average of false positive rate and true positive rate for simplicity, and our method can also handle more general definitions of EO.[30] \n. Design a learning algorithm for the learner's learning algorithm, and design an approximate solution to the adversary's best response to the learner's chosen hypothesis.2 All the omitted proofs are provided in the supplementary material.\nWithout this relaxation we will have to solve a regularized version of cost-sensitive classification.\nhttps://github.com/algowatchpenn/GerryFair 5 https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis\nAcknowledgmentsWe thank Shipra Agrawal and Roxana Geambasu for helpful preliminary discussions. DM was supported through a Columbia Data Science Institute Post-Doctoral Fellowship. SJ was partially supported by NSF award CNS-18-01426. DH was partially supported by NSF awards CCF-1740833 and IIS-1563785 as well as a Sloan Fellowship.A AppendixA.1 Maximum violating weight linear programIn our experiments, we use the following linear program to find the maximum fairness violating weighted distribution, while keeping individual weights to the range [(1 \u2212 )/n, (1 + )/n], and keep protected group marginals the same:s.t.i:ai=aHere, the \u03c0 a are the original protected group marginal probabilities.A.2 Proof of Theorem 1The proof of this theorem is similar to the proof of Theorem 7 in[12]except that we use additive approximate oracle. Let v * = min h\u2208H W max w\u2208W (h, w). Recall that the w-player plays projected gradient descent algorithm, whereas the h-player uses ApxFair(\u00b7) to generate \u03b1-approximate best response. By the guaranteeA.5 Proof of Theorem 2We first recall the following guarantee about the performance of the RFTL algorithm.Lemma 3 (Restated Theorem 5.6 from[21]). The RFTL algorithm achieves the following regret bound forthen we can optimize \u03b7 to get the following bound:The statement of this theorem follows from two lemmas. Lemma 4 proves that if Algorithm 3 is run for T rounds, it computes a (2M + B) n/T + B(4\u03b3 1 + \u03b3 2 )-minmax equilibrium of the game U (h, \u03bb). On the other hand, Lemma 5 proves that any \u03bd-approximate solution ( h, \u03bb) of the game U (h, \u03bb) has two properties 1. h minimizes training loss with respect to the weight w 0 up to an additive error of 2\u03bd.2. h provides \u03b5-fairness guarantee with respect to the set of all weights in W upto an additive error fo M +2\u03bd B . Now substituting \u03bd = (2M + B) n/T + B(4\u03b3 1 + \u03b3 2 ) we get the following two guarantees:andNow we can set the following values for the parameters B = 3M/\u03b5, T = 36n/\u03b5 2 , 4\u03b3 1 + \u03b3 2 = \u03b5/6, and get the desired result. Proof. At round t, the cost is linear in h t , i.e., f t (h t ) = n i=1 L(\u03bb t ) i h t (x i , a i ). Let us write\u03bb = 1 T \u03bb t and D to be the uniform distribution over h 1 , . . . , h T . Since we chose R(x) = 1 2 x 2 2 as the regularization function and the actions are [0, 1] vectors in n-dimensional space, the diameter D R is bounded by \u221a n. On the other hand, \u2207f t (h t ) \u221e = max i |L(\u03bb t ) i |. We now bound |L(\u03bb t ) i | for an arbitrary i. Suppose y i = 1. The proof when y = 0 is identical.The last line follows as w 0 i \u2264 1 and since \u03bb t is an approximate best reponse computed by Algorithm 2, exactly\nCompas dataset. Compas dataset. https://www.propublica.org/datastore/dataset/ compas-recidivism-risk-score-data-and-analysis, 2019. Accessed: 2019-10-26.\n\nDistributionally robust logistic regression. Peyman Soroosh Shafieezadeh Abadeh, Mohajerin Mohajerin, Daniel Esfahani, Kuhn, Advances in Neural Information Processing Systems. Soroosh Shafieezadeh Abadeh, Peyman Mohajerin Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust logistic regression. In Advances in Neural Information Processing Systems, pages 1576-1584, 2015.\n\nCompeting in the dark: An efficient algorithm for bandit linear optimization. Jacob Abernethy, E Elad, Alexander Hazan, Rakhlin, 21st Annual Conference on Learning Theory. Jacob Abernethy, Elad E Hazan, and Alexander Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In 21st Annual Conference on Learning Theory, pages 263-273, 2008.\n\nA reductions approach to fair classification. Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, Hanna Wallach, International Conference on Machine Learning. Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions approach to fair classification. In International Conference on Machine Learning, pages 60-69, 2018.\n\nUnleashing linear optimizers for group-fair learning and optimization. Daniel Alabi, Nicole Immorlica, Adam Kalai, Conference on Learning Theory. Daniel Alabi, Nicole Immorlica, and Adam Kalai. Unleashing linear optimizers for group-fair learning and optimization. In Conference on Learning Theory, pages 2043-2066, 2018.\n\nFairsquare: probabilistic verification of program fairness. Aws Albarghouthi, D&apos; Loris, Samuel Antoni, Aditya V Drews, Nori, Proceedings of the ACM on Programming Languages. 1OOPSLAAws Albarghouthi, Loris D'Antoni, Samuel Drews, and Aditya V Nori. Fairsquare: probabilistic verification of program fairness. Proceedings of the ACM on Programming Languages, 1(OOPSLA):1-30, 2017.\n\nAi fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. K E Rachel, Kuntal Bellamy, Michael Dey, Hind, C Samuel, Stephanie Hoffman, Kalapriya Houde, Pranay Kannan, Jacquelyn Lohia, Sameep Martino, Aleksandra Mehta, Mojsilovic, arXiv:1810.01943arXiv preprintRachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. arXiv preprint arXiv:1810.01943, 2018.\n\nRobust solutions of optimization problems affected by uncertain probabilities. Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, Gijs Rennen, Management Science. 592Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2): 341-357, 2013.\n\nMan is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. Tolga Bolukbasi, Kai-Wei Chang, Y James, Venkatesh Zou, Adam T Saligrama, Kalai, Advances in Neural Information Processing Systems. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. In Advances in Neural Information Processing Systems, pages 4349-4357, 2016.\n\nGender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. Joy Buolamwini, Timnit Gebru, Conference on Fairness, Accountability and Transparency. Joy Buolamwini and Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Conference on Fairness, Accountability and Transparency, pages 77-91, 2018.\n\nOptimized pre-processing for discrimination prevention. Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Kush R Karthikeyan Natesan Ramamurthy, Varshney, Advances in Neural Information Processing Systems. 30Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural Information Processing Systems 30, pages 3992-4001, 2017.\n\nRobust Optimization for Non-Convex Objectives. Brendan Robert S Chen, Yaron Lucier, Vasilis Singer, Syrgkanis, Advances in Neural Information Processing Systems. Robert S Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust Optimization for Non- Convex Objectives. In Advances in Neural Information Processing Systems, pages 4705-4714, 2017.\n\nFairness through awareness. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, Richard Zemel, Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. the 3rd Innovations in Theoretical Computer Science ConferenceCynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through awareness. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 214-226, 2012.\n\nCertifying and removing disparate impact. Michael Feldman, A Sorelle, John Friedler, Carlos Moeller, Suresh Scheidegger, Venkatasubramanian, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningMichael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259-268, 2015.\n\nA confidence-based approach for balancing fairness and accuracy. Benjamin Fish, Jeremy Kun, \u00c1d\u00e1m D Lelkes, Proceedings of the 2016 SIAM International Conference on Data Mining. the 2016 SIAM International Conference on Data MiningSIAMBenjamin Fish, Jeremy Kun, and \u00c1d\u00e1m D Lelkes. A confidence-based approach for balancing fairness and accuracy. In Proceedings of the 2016 SIAM International Conference on Data Mining, pages 144-152. SIAM, 2016.\n\nAdaptive game playing using multiplicative weights. Yoav Freund, Robert E Schapire, Games and Economic Behavior. 291-2Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29(1-2):79-103, 1999.\n\nFairness testing: testing software for discrimination. Sainyam Galhotra, Yuriy Brun, Alexandra Meliou, Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. the 2017 11th Joint Meeting on Foundations of Software EngineeringSainyam Galhotra, Yuriy Brun, and Alexandra Meliou. Fairness testing: testing software for discrimi- nation. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, pages 498-510, 2017.\n\nRui Gao, Anton J Kleywegt, arXiv:1604.02199Distributionally robust stochastic optimization with wasserstein distance. arXiv preprintRui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with wasserstein distance. arXiv preprint arXiv:1604.02199, 2016.\n\nThe case for process fairness in learning: Feature selection for fair decision making. Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, Adrian Weller, NIPS Symposium on Machine Learning and the Law. 1Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian Weller. The case for process fairness in learning: Feature selection for fair decision making. In NIPS Symposium on Machine Learning and the Law, volume 1, page 2, 2016.\n\nEquality of Opportunity in Supervised Learning. Moritz Hardt, Eric Price, Nati Srebro, Advances in Neural Information Processing Systems. Moritz Hardt, Eric Price, Nati Srebro, et al. Equality of Opportunity in Supervised Learning. In Advances in Neural Information Processing Systems, pages 3315-3323, 2016.\n\nIntroduction to online convex optimization. Elad Hazan, Foundations and Trends\u00ae in Optimization. 23-4Elad Hazan. Introduction to online convex optimization. Foundations and Trends\u00ae in Optimization, 2 (3-4):157-325, 2016.\n\nData preprocessing techniques for classification without discrimination. Faisal Kamiran, Toon Calders, Knowledge and Information Systems. 331Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1):1-33, 2012.\n\nFairness-aware learning through regularization approach. Toshihiro Kamishima, Shotaro Akaho, Jun Sakuma, 2011 IEEE 11th International Conference on Data Mining Workshops. IEEEToshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regularization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops, pages 643-650. IEEE, 2011.\n\nPreventing fairness gerrymandering: Auditing and learning for subgroup fairness. Michael Kearns, Seth Neel, Aaron Roth, Zhiwei Steven Wu, International Conference on Machine Learning. Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerrymandering: Auditing and learning for subgroup fairness. In International Conference on Machine Learning, pages 2564-2572, 2018.\n\nAvoiding discrimination through causal reasoning. Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, Bernhard Sch\u00f6lkopf, Advances in Neural Information Processing Systems. Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Sch\u00f6lkopf. Avoiding discrimination through causal reasoning. In Advances in Neural Information Processing Systems, pages 656-666, 2017.\n\nScaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. Ron Kohavi, Kdd. 96Ron Kohavi. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Kdd, volume 96, pages 202-207, 1996.\n\nCounterfactual fairness. J Matt, Joshua Kusner, Chris Loftus, Ricardo Russell, Silva, Advances in Neural Information Processing Systems. Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances in Neural Information Processing Systems, pages 4066-4076, 2017.\n\nStochastic gradient methods for distributionally robust optimization with f-divergences. Hongseok Namkoong, C John, Duchi, Advances in Neural Information Processing Systems. Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. In Advances in Neural Information Processing Systems, pages 2208- 2216, 2016.\n\nScikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825-2830, 2011.\n\nOn fairness and calibration. Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, Kilian Q Weinberger, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness and calibration. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5680-5689. Curran Associates, Inc., 2017.\n\nA data-driven software tool for enabling cooperative information sharing among police departments. Michael Redmond, Alok Baveja, European Journal of Operational Research. 1413Michael Redmond and Alok Baveja. A data-driven software tool for enabling cooperative information sharing among police departments. European Journal of Operational Research, 141(3):660-678, 2002.\n\nOnline learning: Theory, algorithms, and applications. Shai Shalev-Shwartz, The Hebrew University of JerusalemPhD thesisShai Shalev-Shwartz. Online learning: Theory, algorithms, and applications. PhD thesis, The Hebrew University of Jerusalem, 2007.\n\nAverage individual fairness: Algorithms, generalization and experiments. Saeed Sharifi-Malvajerdi, Michael Kearns, Aaron Roth, Advances in Neural Information Processing Systems. Saeed Sharifi-Malvajerdi, Michael Kearns, and Aaron Roth. Average individual fairness: Algorithms, generalization and experiments. In Advances in Neural Information Processing Systems, pages 8240-8249, 2019.\n\nFairtest: Discovering unwarranted associations in data-driven applications. Florian Tramer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Jean-Pierre Hubaux, Mathias Humbert, Ari Juels, Huang Lin, 2017 IEEE European Symposium on Security and Privacy (EuroS&P). IEEEFlorian Tramer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Jean-Pierre Hubaux, Mathias Humbert, Ari Juels, and Huang Lin. Fairtest: Discovering unwarranted associations in data-driven applications. In 2017 IEEE European Symposium on Security and Privacy (EuroS&P), pages 401-416. IEEE, 2017.\n\nLSAC national longitudinal bar passage study. Linda F Wightman, LSAC Research Report Series. Technical reportLinda F Wightman. LSAC national longitudinal bar passage study. Technical report, LSAC Research Report Series, 1998.\n\nFairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna P Gummadi, Proceedings of the 26th International Conference on World Wide Web. the 26th International Conference on World Wide WebMuhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web, pages 1171-1180, 2017.\n\nLearning Fair Representations. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, Cynthia Dwork, International Conference on Machine Learning. Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning Fair Representations. In International Conference on Machine Learning, pages 325-333, 2013.\n\nFairness in decision-making-the causal explanation formula. Junzhe Zhang, Elias Bareinboim, Thirty-Second AAAI Conference on Artificial Intelligence. Junzhe Zhang and Elias Bareinboim. Fairness in decision-making-the causal explanation formula. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nR(w, a , h) over all weights w and subgroups a, a is larger than \u03b5 + \u03b3 2 , the value of the corresponding LP will be larger than \u03b5 and the algorithm will set the correct coordinate of \u03bb to B. On the other hand, if the maximum value of R(w, a, h) \u2212 R(w, a , h) is between \u03b5 \u2212 4\u03b3 1 and \u03b5 + \u03b3 2. Therefore, if the maximum value of R(w, a, h) \u2212. In that case, the algorithm might return the zero vector with value zero. However, the optimal value in that case can be as large as B \u00d7 (4\u03b3 1 + \u03b3 2Therefore, if the maximum value of R(w, a, h) \u2212 R(w, a , h) over all weights w and subgroups a, a is larger than \u03b5 + \u03b3 2 , the value of the corresponding LP will be larger than \u03b5 and the algorithm will set the correct coordinate of \u03bb to B. On the other hand, if the maximum value of R(w, a, h) \u2212 R(w, a , h) is between \u03b5 \u2212 4\u03b3 1 and \u03b5 + \u03b3 2 . In that case, the algorithm might return the zero vector with value zero. However, the optimal value in that case can be as large as B \u00d7 (4\u03b3 1 + \u03b3 2 ).\n", "annotations": {"author": "[{\"end\":155,\"start\":59},{\"end\":248,\"start\":156},{\"end\":340,\"start\":249},{\"end\":438,\"start\":341},{\"end\":530,\"start\":439}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":68},{\"end\":167,\"start\":163},{\"end\":259,\"start\":255},{\"end\":357,\"start\":353},{\"end\":449,\"start\":446}]", "author_first_name": "[{\"end\":67,\"start\":59},{\"end\":162,\"start\":156},{\"end\":254,\"start\":249},{\"end\":350,\"start\":341},{\"end\":352,\"start\":351},{\"end\":445,\"start\":439}]", "author_affiliation": "[{\"end\":154,\"start\":76},{\"end\":247,\"start\":169},{\"end\":339,\"start\":261},{\"end\":437,\"start\":359},{\"end\":529,\"start\":451}]", "title": "[{\"end\":43,\"start\":1},{\"end\":573,\"start\":531}]", "venue": null, "abstract": "[{\"end\":1825,\"start\":588}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2143,\"start\":2140},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2271,\"start\":2267},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2514,\"start\":2511},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2517,\"start\":2514},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2520,\"start\":2517},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3039,\"start\":3035},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3083,\"start\":3080},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3365,\"start\":3361},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5237,\"start\":5233},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5240,\"start\":5237},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5243,\"start\":5240},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5246,\"start\":5243},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5249,\"start\":5246},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5266,\"start\":5262},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5269,\"start\":5266},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5282,\"start\":5278},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5285,\"start\":5282},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5288,\"start\":5285},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5314,\"start\":5310},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5563,\"start\":5559},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5566,\"start\":5563},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5569,\"start\":5566},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5593,\"start\":5589},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5901,\"start\":5897},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5904,\"start\":5901},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5907,\"start\":5904},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5910,\"start\":5907},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6033,\"start\":6030},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6036,\"start\":6033},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6039,\"start\":6036},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6042,\"start\":6039},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6126,\"start\":6122},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6129,\"start\":6126},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6206,\"start\":6203},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6208,\"start\":6206},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6211,\"start\":6208},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6231,\"start\":6228},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6252,\"start\":6249},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6414,\"start\":6410},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6532,\"start\":6529},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6968,\"start\":6965},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6971,\"start\":6968},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6997,\"start\":6994},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7000,\"start\":6997},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7280,\"start\":7277},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7283,\"start\":7280},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7286,\"start\":7283},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11463,\"start\":11459},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12877,\"start\":12873},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13749,\"start\":13746},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14000,\"start\":13997},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14154,\"start\":14151},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14266,\"start\":14263},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14268,\"start\":14266},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14271,\"start\":14268},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18473,\"start\":18470},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18476,\"start\":18473},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21112,\"start\":21109},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21280,\"start\":21277},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21283,\"start\":21280},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23339,\"start\":23338},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24469,\"start\":24466},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24481,\"start\":24477},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24509,\"start\":24505},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":24530,\"start\":24526},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24639,\"start\":24635},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24671,\"start\":24668},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24705,\"start\":24701},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24772,\"start\":24768},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32375,\"start\":32371},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32442,\"start\":32441},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32565,\"start\":32561},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32923,\"start\":32919},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33515,\"start\":33511},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":36155,\"start\":36151},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":36329,\"start\":36328}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35773,\"start\":35563},{\"attributes\":{\"id\":\"fig_1\"},\"end\":36002,\"start\":35774}]", "paragraph": "[{\"end\":2521,\"start\":1841},{\"end\":4416,\"start\":2523},{\"end\":4666,\"start\":4418},{\"end\":5716,\"start\":4668},{\"end\":6644,\"start\":5718},{\"end\":7487,\"start\":6646},{\"end\":8088,\"start\":7515},{\"end\":8644,\"start\":8090},{\"end\":8970,\"start\":8772},{\"end\":9480,\"start\":9176},{\"end\":9743,\"start\":9482},{\"end\":10335,\"start\":9772},{\"end\":10758,\"start\":10370},{\"end\":10925,\"start\":10777},{\"end\":12117,\"start\":11309},{\"end\":12250,\"start\":12166},{\"end\":12571,\"start\":12469},{\"end\":12759,\"start\":12573},{\"end\":12938,\"start\":12847},{\"end\":13438,\"start\":12970},{\"end\":13711,\"start\":13440},{\"end\":14614,\"start\":13713},{\"end\":14720,\"start\":14648},{\"end\":14987,\"start\":14850},{\"end\":15012,\"start\":15011},{\"end\":15287,\"start\":15014},{\"end\":15869,\"start\":15383},{\"end\":16210,\"start\":15911},{\"end\":16356,\"start\":16323},{\"end\":17218,\"start\":16474},{\"end\":17620,\"start\":17331},{\"end\":17706,\"start\":17622},{\"end\":17806,\"start\":17768},{\"end\":17950,\"start\":17913},{\"end\":18168,\"start\":18074},{\"end\":18701,\"start\":18242},{\"end\":18814,\"start\":18703},{\"end\":20766,\"start\":18816},{\"end\":20949,\"start\":20844},{\"end\":21552,\"start\":21022},{\"end\":21605,\"start\":21554},{\"end\":21855,\"start\":21645},{\"end\":22198,\"start\":21908},{\"end\":22648,\"start\":22268},{\"end\":22981,\"start\":22834},{\"end\":23818,\"start\":23151},{\"end\":24252,\"start\":24097},{\"end\":25355,\"start\":24382},{\"end\":26162,\"start\":25357},{\"end\":26737,\"start\":26164},{\"end\":27308,\"start\":26774},{\"end\":27498,\"start\":27429},{\"end\":27911,\"start\":27724},{\"end\":28014,\"start\":27936},{\"end\":28377,\"start\":28100},{\"end\":28544,\"start\":28516},{\"end\":28751,\"start\":28684},{\"end\":28995,\"start\":28945},{\"end\":29510,\"start\":29203},{\"end\":30121,\"start\":29535},{\"end\":31009,\"start\":30207},{\"end\":31272,\"start\":31041},{\"end\":31619,\"start\":31541},{\"end\":31766,\"start\":31707},{\"end\":32088,\"start\":31859},{\"end\":32534,\"start\":32124},{\"end\":32852,\"start\":32536},{\"end\":33222,\"start\":32854},{\"end\":33516,\"start\":33224},{\"end\":34744,\"start\":33518},{\"end\":35562,\"start\":34781}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8771,\"start\":8645},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9175,\"start\":8971},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9771,\"start\":9744},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10360,\"start\":10336},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11308,\"start\":10926},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12165,\"start\":12118},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12468,\"start\":12251},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12846,\"start\":12760},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14849,\"start\":14721},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15010,\"start\":14988},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15382,\"start\":15288},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16255,\"start\":16211},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16322,\"start\":16255},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16473,\"start\":16357},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17330,\"start\":17219},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17767,\"start\":17707},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17912,\"start\":17807},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18073,\"start\":17951},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18241,\"start\":18169},{\"attributes\":{\"id\":\"formula_19\"},\"end\":20843,\"start\":20767},{\"attributes\":{\"id\":\"formula_20\"},\"end\":21021,\"start\":20950},{\"attributes\":{\"id\":\"formula_21\"},\"end\":21644,\"start\":21606},{\"attributes\":{\"id\":\"formula_22\"},\"end\":21907,\"start\":21856},{\"attributes\":{\"id\":\"formula_23\"},\"end\":22267,\"start\":22199},{\"attributes\":{\"id\":\"formula_24\"},\"end\":22833,\"start\":22649},{\"attributes\":{\"id\":\"formula_25\"},\"end\":23150,\"start\":22982},{\"attributes\":{\"id\":\"formula_26\"},\"end\":24096,\"start\":23872},{\"attributes\":{\"id\":\"formula_27\"},\"end\":24367,\"start\":24253},{\"attributes\":{\"id\":\"formula_28\"},\"end\":27428,\"start\":27309},{\"attributes\":{\"id\":\"formula_29\"},\"end\":27723,\"start\":27499},{\"attributes\":{\"id\":\"formula_30\"},\"end\":28099,\"start\":28015},{\"attributes\":{\"id\":\"formula_31\"},\"end\":28507,\"start\":28378},{\"attributes\":{\"id\":\"formula_32\"},\"end\":28515,\"start\":28507},{\"attributes\":{\"id\":\"formula_33\"},\"end\":28683,\"start\":28545},{\"attributes\":{\"id\":\"formula_34\"},\"end\":28944,\"start\":28752},{\"attributes\":{\"id\":\"formula_35\"},\"end\":29202,\"start\":28996},{\"attributes\":{\"id\":\"formula_36\"},\"end\":30206,\"start\":30122},{\"attributes\":{\"id\":\"formula_37\"},\"end\":31040,\"start\":31010},{\"attributes\":{\"id\":\"formula_38\"},\"end\":31540,\"start\":31273},{\"attributes\":{\"id\":\"formula_39\"},\"end\":31706,\"start\":31620},{\"attributes\":{\"id\":\"formula_40\"},\"end\":31858,\"start\":31767}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1839,\"start\":1827},{\"attributes\":{\"n\":\"2\"},\"end\":7513,\"start\":7490},{\"attributes\":{\"n\":\"3\"},\"end\":10368,\"start\":10362},{\"attributes\":{\"n\":\"3.1\"},\"end\":10775,\"start\":10761},{\"attributes\":{\"n\":\"3.2\"},\"end\":12968,\"start\":12941},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":14646,\"start\":14617},{\"attributes\":{\"n\":\"3.3\"},\"end\":15909,\"start\":15872},{\"end\":23871,\"start\":23821},{\"attributes\":{\"n\":\"4\"},\"end\":24380,\"start\":24369},{\"attributes\":{\"n\":\"5\"},\"end\":26772,\"start\":26740},{\"end\":27934,\"start\":27914},{\"end\":29533,\"start\":29513},{\"end\":32122,\"start\":32091},{\"end\":34779,\"start\":34747},{\"end\":35574,\"start\":35564}]", "table": null, "figure_caption": "[{\"end\":35773,\"start\":35576},{\"end\":36002,\"start\":35776}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3201,\"start\":3193},{\"end\":25374,\"start\":25366},{\"end\":25789,\"start\":25781},{\"end\":34792,\"start\":34784},{\"end\":35113,\"start\":35105}]", "bib_author_first_name": "[{\"end\":39471,\"start\":39465},{\"end\":39528,\"start\":39522},{\"end\":39885,\"start\":39880},{\"end\":39898,\"start\":39897},{\"end\":39914,\"start\":39905},{\"end\":40225,\"start\":40220},{\"end\":40240,\"start\":40235},{\"end\":40262,\"start\":40254},{\"end\":40274,\"start\":40270},{\"end\":40290,\"start\":40285},{\"end\":40622,\"start\":40616},{\"end\":40636,\"start\":40630},{\"end\":40652,\"start\":40648},{\"end\":40931,\"start\":40928},{\"end\":40953,\"start\":40946},{\"end\":40967,\"start\":40961},{\"end\":40982,\"start\":40976},{\"end\":40984,\"start\":40983},{\"end\":41365,\"start\":41364},{\"end\":41367,\"start\":41366},{\"end\":41382,\"start\":41376},{\"end\":41399,\"start\":41392},{\"end\":41412,\"start\":41411},{\"end\":41430,\"start\":41421},{\"end\":41449,\"start\":41440},{\"end\":41463,\"start\":41457},{\"end\":41481,\"start\":41472},{\"end\":41495,\"start\":41489},{\"end\":41515,\"start\":41505},{\"end\":41976,\"start\":41970},{\"end\":41990,\"start\":41986},{\"end\":41994,\"start\":41991},{\"end\":42007,\"start\":42003},{\"end\":42010,\"start\":42008},{\"end\":42031,\"start\":42023},{\"end\":42047,\"start\":42043},{\"end\":42379,\"start\":42374},{\"end\":42398,\"start\":42391},{\"end\":42407,\"start\":42406},{\"end\":42424,\"start\":42415},{\"end\":42434,\"start\":42430},{\"end\":42436,\"start\":42435},{\"end\":42842,\"start\":42839},{\"end\":42861,\"start\":42855},{\"end\":43189,\"start\":43183},{\"end\":43204,\"start\":43198},{\"end\":43220,\"start\":43210},{\"end\":43238,\"start\":43232},{\"end\":43627,\"start\":43620},{\"end\":43648,\"start\":43643},{\"end\":43664,\"start\":43657},{\"end\":43964,\"start\":43957},{\"end\":43978,\"start\":43972},{\"end\":43993,\"start\":43986},{\"end\":44007,\"start\":44003},{\"end\":44025,\"start\":44018},{\"end\":44435,\"start\":44428},{\"end\":44446,\"start\":44445},{\"end\":44460,\"start\":44456},{\"end\":44477,\"start\":44471},{\"end\":44493,\"start\":44487},{\"end\":45052,\"start\":45044},{\"end\":45065,\"start\":45059},{\"end\":45077,\"start\":45071},{\"end\":45481,\"start\":45477},{\"end\":45496,\"start\":45490},{\"end\":45498,\"start\":45497},{\"end\":45744,\"start\":45737},{\"end\":45760,\"start\":45755},{\"end\":45776,\"start\":45767},{\"end\":46154,\"start\":46151},{\"end\":46165,\"start\":46160},{\"end\":46167,\"start\":46166},{\"end\":46519,\"start\":46515},{\"end\":46541,\"start\":46533},{\"end\":46562,\"start\":46555},{\"end\":46564,\"start\":46563},{\"end\":46580,\"start\":46574},{\"end\":46933,\"start\":46927},{\"end\":46945,\"start\":46941},{\"end\":46957,\"start\":46953},{\"end\":47237,\"start\":47233},{\"end\":47490,\"start\":47484},{\"end\":47504,\"start\":47500},{\"end\":47778,\"start\":47769},{\"end\":47797,\"start\":47790},{\"end\":47808,\"start\":47805},{\"end\":48181,\"start\":48174},{\"end\":48194,\"start\":48190},{\"end\":48206,\"start\":48201},{\"end\":48226,\"start\":48213},{\"end\":48546,\"start\":48542},{\"end\":48563,\"start\":48558},{\"end\":48569,\"start\":48564},{\"end\":48591,\"start\":48579},{\"end\":48612,\"start\":48606},{\"end\":48627,\"start\":48620},{\"end\":48645,\"start\":48637},{\"end\":49032,\"start\":49029},{\"end\":49203,\"start\":49202},{\"end\":49216,\"start\":49210},{\"end\":49230,\"start\":49225},{\"end\":49246,\"start\":49239},{\"end\":49578,\"start\":49570},{\"end\":49590,\"start\":49589},{\"end\":49902,\"start\":49901},{\"end\":49915,\"start\":49914},{\"end\":49928,\"start\":49927},{\"end\":49940,\"start\":49939},{\"end\":49950,\"start\":49949},{\"end\":49961,\"start\":49960},{\"end\":49971,\"start\":49970},{\"end\":49982,\"start\":49981},{\"end\":49998,\"start\":49997},{\"end\":50007,\"start\":50006},{\"end\":50018,\"start\":50017},{\"end\":50032,\"start\":50031},{\"end\":50042,\"start\":50041},{\"end\":50056,\"start\":50055},{\"end\":50067,\"start\":50066},{\"end\":50077,\"start\":50076},{\"end\":50472,\"start\":50467},{\"end\":50487,\"start\":50481},{\"end\":50503,\"start\":50498},{\"end\":50511,\"start\":50508},{\"end\":50531,\"start\":50523},{\"end\":51132,\"start\":51125},{\"end\":51146,\"start\":51142},{\"end\":51457,\"start\":51453},{\"end\":51727,\"start\":51722},{\"end\":51755,\"start\":51748},{\"end\":51769,\"start\":51764},{\"end\":52119,\"start\":52112},{\"end\":52136,\"start\":52128},{\"end\":52154,\"start\":52148},{\"end\":52171,\"start\":52165},{\"end\":52188,\"start\":52177},{\"end\":52204,\"start\":52197},{\"end\":52217,\"start\":52214},{\"end\":52230,\"start\":52225},{\"end\":52950,\"start\":52942},{\"end\":52970,\"start\":52964},{\"end\":52985,\"start\":52979},{\"end\":52991,\"start\":52986},{\"end\":53010,\"start\":53003},{\"end\":53012,\"start\":53011},{\"end\":53467,\"start\":53463},{\"end\":53477,\"start\":53475},{\"end\":53487,\"start\":53482},{\"end\":53501,\"start\":53497},{\"end\":53518,\"start\":53511},{\"end\":53807,\"start\":53801},{\"end\":53820,\"start\":53815}]", "bib_author_last_name": "[{\"end\":39499,\"start\":39472},{\"end\":39520,\"start\":39501},{\"end\":39537,\"start\":39529},{\"end\":39543,\"start\":39539},{\"end\":39895,\"start\":39886},{\"end\":39903,\"start\":39899},{\"end\":39920,\"start\":39915},{\"end\":39929,\"start\":39922},{\"end\":40233,\"start\":40226},{\"end\":40252,\"start\":40241},{\"end\":40268,\"start\":40263},{\"end\":40283,\"start\":40275},{\"end\":40298,\"start\":40291},{\"end\":40628,\"start\":40623},{\"end\":40646,\"start\":40637},{\"end\":40658,\"start\":40653},{\"end\":40944,\"start\":40932},{\"end\":40959,\"start\":40954},{\"end\":40974,\"start\":40968},{\"end\":40990,\"start\":40985},{\"end\":40996,\"start\":40992},{\"end\":41374,\"start\":41368},{\"end\":41390,\"start\":41383},{\"end\":41403,\"start\":41400},{\"end\":41409,\"start\":41405},{\"end\":41419,\"start\":41413},{\"end\":41438,\"start\":41431},{\"end\":41455,\"start\":41450},{\"end\":41470,\"start\":41464},{\"end\":41487,\"start\":41482},{\"end\":41503,\"start\":41496},{\"end\":41521,\"start\":41516},{\"end\":41533,\"start\":41523},{\"end\":41984,\"start\":41977},{\"end\":42001,\"start\":41995},{\"end\":42021,\"start\":42011},{\"end\":42041,\"start\":42032},{\"end\":42054,\"start\":42048},{\"end\":42389,\"start\":42380},{\"end\":42404,\"start\":42399},{\"end\":42413,\"start\":42408},{\"end\":42428,\"start\":42425},{\"end\":42446,\"start\":42437},{\"end\":42453,\"start\":42448},{\"end\":42853,\"start\":42843},{\"end\":42867,\"start\":42862},{\"end\":43196,\"start\":43190},{\"end\":43208,\"start\":43205},{\"end\":43230,\"start\":43221},{\"end\":43269,\"start\":43239},{\"end\":43279,\"start\":43271},{\"end\":43641,\"start\":43628},{\"end\":43655,\"start\":43649},{\"end\":43671,\"start\":43665},{\"end\":43682,\"start\":43673},{\"end\":43970,\"start\":43965},{\"end\":43984,\"start\":43979},{\"end\":44001,\"start\":43994},{\"end\":44016,\"start\":44008},{\"end\":44031,\"start\":44026},{\"end\":44443,\"start\":44436},{\"end\":44454,\"start\":44447},{\"end\":44469,\"start\":44461},{\"end\":44485,\"start\":44478},{\"end\":44505,\"start\":44494},{\"end\":44525,\"start\":44507},{\"end\":45057,\"start\":45053},{\"end\":45069,\"start\":45066},{\"end\":45084,\"start\":45078},{\"end\":45488,\"start\":45482},{\"end\":45507,\"start\":45499},{\"end\":45753,\"start\":45745},{\"end\":45765,\"start\":45761},{\"end\":45783,\"start\":45777},{\"end\":46158,\"start\":46155},{\"end\":46176,\"start\":46168},{\"end\":46531,\"start\":46520},{\"end\":46553,\"start\":46542},{\"end\":46572,\"start\":46565},{\"end\":46587,\"start\":46581},{\"end\":46939,\"start\":46934},{\"end\":46951,\"start\":46946},{\"end\":46964,\"start\":46958},{\"end\":47243,\"start\":47238},{\"end\":47498,\"start\":47491},{\"end\":47512,\"start\":47505},{\"end\":47788,\"start\":47779},{\"end\":47803,\"start\":47798},{\"end\":47815,\"start\":47809},{\"end\":48188,\"start\":48182},{\"end\":48199,\"start\":48195},{\"end\":48211,\"start\":48207},{\"end\":48229,\"start\":48227},{\"end\":48556,\"start\":48547},{\"end\":48577,\"start\":48570},{\"end\":48604,\"start\":48592},{\"end\":48618,\"start\":48613},{\"end\":48635,\"start\":48628},{\"end\":48655,\"start\":48646},{\"end\":49039,\"start\":49033},{\"end\":49208,\"start\":49204},{\"end\":49223,\"start\":49217},{\"end\":49237,\"start\":49231},{\"end\":49254,\"start\":49247},{\"end\":49261,\"start\":49256},{\"end\":49587,\"start\":49579},{\"end\":49595,\"start\":49591},{\"end\":49602,\"start\":49597},{\"end\":49912,\"start\":49903},{\"end\":49925,\"start\":49916},{\"end\":49937,\"start\":49929},{\"end\":49947,\"start\":49941},{\"end\":49958,\"start\":49951},{\"end\":49968,\"start\":49962},{\"end\":49979,\"start\":49972},{\"end\":49995,\"start\":49983},{\"end\":50004,\"start\":49999},{\"end\":50015,\"start\":50008},{\"end\":50029,\"start\":50019},{\"end\":50039,\"start\":50033},{\"end\":50053,\"start\":50043},{\"end\":50064,\"start\":50057},{\"end\":50074,\"start\":50068},{\"end\":50087,\"start\":50078},{\"end\":50479,\"start\":50473},{\"end\":50496,\"start\":50488},{\"end\":50506,\"start\":50504},{\"end\":50521,\"start\":50512},{\"end\":50542,\"start\":50532},{\"end\":51140,\"start\":51133},{\"end\":51153,\"start\":51147},{\"end\":51472,\"start\":51458},{\"end\":51746,\"start\":51728},{\"end\":51762,\"start\":51756},{\"end\":51774,\"start\":51770},{\"end\":52126,\"start\":52120},{\"end\":52146,\"start\":52137},{\"end\":52163,\"start\":52155},{\"end\":52175,\"start\":52172},{\"end\":52195,\"start\":52189},{\"end\":52212,\"start\":52205},{\"end\":52223,\"start\":52218},{\"end\":52234,\"start\":52231},{\"end\":52665,\"start\":52649},{\"end\":52962,\"start\":52951},{\"end\":52977,\"start\":52971},{\"end\":53001,\"start\":52992},{\"end\":53020,\"start\":53013},{\"end\":53473,\"start\":53468},{\"end\":53480,\"start\":53478},{\"end\":53495,\"start\":53488},{\"end\":53509,\"start\":53502},{\"end\":53524,\"start\":53519},{\"end\":53813,\"start\":53808},{\"end\":53831,\"start\":53821}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":39418,\"start\":39265},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15024318},\"end\":39800,\"start\":39420},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":8547150},\"end\":40172,\"start\":39802},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4725675},\"end\":40543,\"start\":40174},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":46926476},\"end\":40866,\"start\":40545},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7013463},\"end\":41251,\"start\":40868},{\"attributes\":{\"doi\":\"arXiv:1810.01943\",\"id\":\"b6\"},\"end\":41889,\"start\":41253},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":761793},\"end\":42289,\"start\":41891},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1704893},\"end\":42749,\"start\":42291},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3298854},\"end\":43125,\"start\":42751},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3801798},\"end\":43571,\"start\":43127},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":32380576},\"end\":43927,\"start\":43573},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":13496699},\"end\":44384,\"start\":43929},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2077168},\"end\":44977,\"start\":44386},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":16135452},\"end\":45423,\"start\":44979},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15295656},\"end\":45680,\"start\":45425},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6324652},\"end\":46149,\"start\":45682},{\"attributes\":{\"doi\":\"arXiv:1604.02199\",\"id\":\"b17\"},\"end\":46426,\"start\":46151},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13633339},\"end\":46877,\"start\":46428},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":7567061},\"end\":47187,\"start\":46879},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":30482768},\"end\":47409,\"start\":47189},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14637938},\"end\":47710,\"start\":47411},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":12882511},\"end\":48091,\"start\":47712},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":29169376},\"end\":48490,\"start\":48093},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3352595},\"end\":48951,\"start\":48492},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8314975},\"end\":49175,\"start\":48953},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":2014883},\"end\":49479,\"start\":49177},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7481496},\"end\":49857,\"start\":49481},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10659969},\"end\":50436,\"start\":49859},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":75455},\"end\":51024,\"start\":50438},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7357922},\"end\":51396,\"start\":51026},{\"attributes\":{\"id\":\"b31\"},\"end\":51647,\"start\":51398},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":166228088},\"end\":52034,\"start\":51649},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":16796732},\"end\":52601,\"start\":52036},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":151073942},\"end\":52828,\"start\":52603},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1911971},\"end\":53430,\"start\":52830},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":490669},\"end\":53739,\"start\":53432},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":19117312},\"end\":54052,\"start\":53741},{\"attributes\":{\"id\":\"b38\"},\"end\":55037,\"start\":54054}]", "bib_title": "[{\"end\":39463,\"start\":39420},{\"end\":39878,\"start\":39802},{\"end\":40218,\"start\":40174},{\"end\":40614,\"start\":40545},{\"end\":40926,\"start\":40868},{\"end\":41968,\"start\":41891},{\"end\":42372,\"start\":42291},{\"end\":42837,\"start\":42751},{\"end\":43181,\"start\":43127},{\"end\":43618,\"start\":43573},{\"end\":43955,\"start\":43929},{\"end\":44426,\"start\":44386},{\"end\":45042,\"start\":44979},{\"end\":45475,\"start\":45425},{\"end\":45735,\"start\":45682},{\"end\":46513,\"start\":46428},{\"end\":46925,\"start\":46879},{\"end\":47231,\"start\":47189},{\"end\":47482,\"start\":47411},{\"end\":47767,\"start\":47712},{\"end\":48172,\"start\":48093},{\"end\":48540,\"start\":48492},{\"end\":49027,\"start\":48953},{\"end\":49200,\"start\":49177},{\"end\":49568,\"start\":49481},{\"end\":49899,\"start\":49859},{\"end\":50465,\"start\":50438},{\"end\":51123,\"start\":51026},{\"end\":51720,\"start\":51649},{\"end\":52110,\"start\":52036},{\"end\":52647,\"start\":52603},{\"end\":52940,\"start\":52830},{\"end\":53461,\"start\":53432},{\"end\":53799,\"start\":53741},{\"end\":54345,\"start\":54054}]", "bib_author": "[{\"end\":39501,\"start\":39465},{\"end\":39522,\"start\":39501},{\"end\":39539,\"start\":39522},{\"end\":39545,\"start\":39539},{\"end\":39897,\"start\":39880},{\"end\":39905,\"start\":39897},{\"end\":39922,\"start\":39905},{\"end\":39931,\"start\":39922},{\"end\":40235,\"start\":40220},{\"end\":40254,\"start\":40235},{\"end\":40270,\"start\":40254},{\"end\":40285,\"start\":40270},{\"end\":40300,\"start\":40285},{\"end\":40630,\"start\":40616},{\"end\":40648,\"start\":40630},{\"end\":40660,\"start\":40648},{\"end\":40946,\"start\":40928},{\"end\":40961,\"start\":40946},{\"end\":40976,\"start\":40961},{\"end\":40992,\"start\":40976},{\"end\":40998,\"start\":40992},{\"end\":41376,\"start\":41364},{\"end\":41392,\"start\":41376},{\"end\":41405,\"start\":41392},{\"end\":41411,\"start\":41405},{\"end\":41421,\"start\":41411},{\"end\":41440,\"start\":41421},{\"end\":41457,\"start\":41440},{\"end\":41472,\"start\":41457},{\"end\":41489,\"start\":41472},{\"end\":41505,\"start\":41489},{\"end\":41523,\"start\":41505},{\"end\":41535,\"start\":41523},{\"end\":41986,\"start\":41970},{\"end\":42003,\"start\":41986},{\"end\":42023,\"start\":42003},{\"end\":42043,\"start\":42023},{\"end\":42056,\"start\":42043},{\"end\":42391,\"start\":42374},{\"end\":42406,\"start\":42391},{\"end\":42415,\"start\":42406},{\"end\":42430,\"start\":42415},{\"end\":42448,\"start\":42430},{\"end\":42455,\"start\":42448},{\"end\":42855,\"start\":42839},{\"end\":42869,\"start\":42855},{\"end\":43198,\"start\":43183},{\"end\":43210,\"start\":43198},{\"end\":43232,\"start\":43210},{\"end\":43271,\"start\":43232},{\"end\":43281,\"start\":43271},{\"end\":43643,\"start\":43620},{\"end\":43657,\"start\":43643},{\"end\":43673,\"start\":43657},{\"end\":43684,\"start\":43673},{\"end\":43972,\"start\":43957},{\"end\":43986,\"start\":43972},{\"end\":44003,\"start\":43986},{\"end\":44018,\"start\":44003},{\"end\":44033,\"start\":44018},{\"end\":44445,\"start\":44428},{\"end\":44456,\"start\":44445},{\"end\":44471,\"start\":44456},{\"end\":44487,\"start\":44471},{\"end\":44507,\"start\":44487},{\"end\":44527,\"start\":44507},{\"end\":45059,\"start\":45044},{\"end\":45071,\"start\":45059},{\"end\":45086,\"start\":45071},{\"end\":45490,\"start\":45477},{\"end\":45509,\"start\":45490},{\"end\":45755,\"start\":45737},{\"end\":45767,\"start\":45755},{\"end\":45785,\"start\":45767},{\"end\":46160,\"start\":46151},{\"end\":46178,\"start\":46160},{\"end\":46533,\"start\":46515},{\"end\":46555,\"start\":46533},{\"end\":46574,\"start\":46555},{\"end\":46589,\"start\":46574},{\"end\":46941,\"start\":46927},{\"end\":46953,\"start\":46941},{\"end\":46966,\"start\":46953},{\"end\":47245,\"start\":47233},{\"end\":47500,\"start\":47484},{\"end\":47514,\"start\":47500},{\"end\":47790,\"start\":47769},{\"end\":47805,\"start\":47790},{\"end\":47817,\"start\":47805},{\"end\":48190,\"start\":48174},{\"end\":48201,\"start\":48190},{\"end\":48213,\"start\":48201},{\"end\":48231,\"start\":48213},{\"end\":48558,\"start\":48542},{\"end\":48579,\"start\":48558},{\"end\":48606,\"start\":48579},{\"end\":48620,\"start\":48606},{\"end\":48637,\"start\":48620},{\"end\":48657,\"start\":48637},{\"end\":49041,\"start\":49029},{\"end\":49210,\"start\":49202},{\"end\":49225,\"start\":49210},{\"end\":49239,\"start\":49225},{\"end\":49256,\"start\":49239},{\"end\":49263,\"start\":49256},{\"end\":49589,\"start\":49570},{\"end\":49597,\"start\":49589},{\"end\":49604,\"start\":49597},{\"end\":49914,\"start\":49901},{\"end\":49927,\"start\":49914},{\"end\":49939,\"start\":49927},{\"end\":49949,\"start\":49939},{\"end\":49960,\"start\":49949},{\"end\":49970,\"start\":49960},{\"end\":49981,\"start\":49970},{\"end\":49997,\"start\":49981},{\"end\":50006,\"start\":49997},{\"end\":50017,\"start\":50006},{\"end\":50031,\"start\":50017},{\"end\":50041,\"start\":50031},{\"end\":50055,\"start\":50041},{\"end\":50066,\"start\":50055},{\"end\":50076,\"start\":50066},{\"end\":50089,\"start\":50076},{\"end\":50481,\"start\":50467},{\"end\":50498,\"start\":50481},{\"end\":50508,\"start\":50498},{\"end\":50523,\"start\":50508},{\"end\":50544,\"start\":50523},{\"end\":51142,\"start\":51125},{\"end\":51155,\"start\":51142},{\"end\":51474,\"start\":51453},{\"end\":51748,\"start\":51722},{\"end\":51764,\"start\":51748},{\"end\":51776,\"start\":51764},{\"end\":52128,\"start\":52112},{\"end\":52148,\"start\":52128},{\"end\":52165,\"start\":52148},{\"end\":52177,\"start\":52165},{\"end\":52197,\"start\":52177},{\"end\":52214,\"start\":52197},{\"end\":52225,\"start\":52214},{\"end\":52236,\"start\":52225},{\"end\":52667,\"start\":52649},{\"end\":52964,\"start\":52942},{\"end\":52979,\"start\":52964},{\"end\":53003,\"start\":52979},{\"end\":53022,\"start\":53003},{\"end\":53475,\"start\":53463},{\"end\":53482,\"start\":53475},{\"end\":53497,\"start\":53482},{\"end\":53511,\"start\":53497},{\"end\":53526,\"start\":53511},{\"end\":53815,\"start\":53801},{\"end\":53833,\"start\":53815}]", "bib_venue": "[{\"end\":39279,\"start\":39265},{\"end\":39594,\"start\":39545},{\"end\":39972,\"start\":39931},{\"end\":40344,\"start\":40300},{\"end\":40689,\"start\":40660},{\"end\":41045,\"start\":40998},{\"end\":41362,\"start\":41253},{\"end\":42074,\"start\":42056},{\"end\":42504,\"start\":42455},{\"end\":42924,\"start\":42869},{\"end\":43330,\"start\":43281},{\"end\":43733,\"start\":43684},{\"end\":44110,\"start\":44033},{\"end\":44625,\"start\":44527},{\"end\":45154,\"start\":45086},{\"end\":45536,\"start\":45509},{\"end\":45866,\"start\":45785},{\"end\":46267,\"start\":46194},{\"end\":46635,\"start\":46589},{\"end\":47015,\"start\":46966},{\"end\":47284,\"start\":47245},{\"end\":47547,\"start\":47514},{\"end\":47881,\"start\":47817},{\"end\":48275,\"start\":48231},{\"end\":48706,\"start\":48657},{\"end\":49044,\"start\":49041},{\"end\":49312,\"start\":49263},{\"end\":49653,\"start\":49604},{\"end\":50125,\"start\":50089},{\"end\":50593,\"start\":50544},{\"end\":51195,\"start\":51155},{\"end\":51451,\"start\":51398},{\"end\":51825,\"start\":51776},{\"end\":52298,\"start\":52236},{\"end\":52694,\"start\":52667},{\"end\":53088,\"start\":53022},{\"end\":53570,\"start\":53526},{\"end\":53889,\"start\":53833},{\"end\":54394,\"start\":54347},{\"end\":44174,\"start\":44112},{\"end\":44710,\"start\":44627},{\"end\":45209,\"start\":45156},{\"end\":45934,\"start\":45868},{\"end\":53141,\"start\":53090}]"}}}, "year": 2023, "month": 12, "day": 17}