{"id": 237417284, "updated": "2023-10-05 23:04:24.456", "metadata": {"title": "CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge", "authors": "[{\"first\":\"Yasumasa\",\"last\":\"Onoe\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Zhang\",\"middle\":[\"J.Q.\"]},{\"first\":\"Eunsol\",\"last\":\"Choi\",\"middle\":[]},{\"first\":\"Greg\",\"last\":\"Durrett\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 9, "day": 3}, "abstract": "Most benchmark datasets targeting commonsense reasoning focus on everyday scenarios: physical knowledge like knowing that you could fill a cup under a waterfall [Talmor et al., 2019], social knowledge like bumping into someone is awkward [Sap et al., 2019], and other generic situations. However, there is a rich space of commonsense inferences anchored to knowledge about specific entities: for example, deciding the truthfulness of a claim\"Harry Potter can teach classes on how to fly on a broomstick.\"Can models learn to combine entity knowledge with commonsense reasoning in this fashion? We introduce CREAK, a testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities (Harry Potter is a wizard and is skilled at riding a broomstick) with commonsense inferences (if you're good at a skill you can teach others how to do it). Our dataset consists of 13k human-authored English claims about entities that are either true or false, in addition to a small contrast set. Crowdworkers can easily come up with these statements and human performance on the dataset is high (high 90s); we argue that models should be able to blend entity knowledge and commonsense reasoning to do well here. In our experiments, we focus on the closed-book setting and observe that a baseline model finetuned on existing fact verification benchmark struggles on CREAK. Training a model on CREAK improves accuracy by a substantial margin, but still falls short of human performance. Our benchmark provides a unique probe into natural language understanding models, testing both its ability to retrieve facts (e.g., who teaches at the University of Chicago?) and unstated commonsense knowledge (e.g., butlers do not yell at guests).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2109.01653", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/OnoeZCD21", "doi": null}}, "content": {"source": {"pdf_hash": "0c4b2ae22f08425563a69527a3433516fc9737a1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2109.01653v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7e6606f7abbf403cab99dfe915b2ce448a9d0dcc", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0c4b2ae22f08425563a69527a3433516fc9737a1.txt", "contents": "\nCREAK: A Dataset for Commonsense Reasoning over Entity Knowledge\n\n\nYasumasa Onoe yasumasa@cs.utexas.edu \nThe University of Texas at Austin\n\n\nMichael J Q Zhang mjqzhang@cs.utexas.edu \nThe University of Texas at Austin\n\n\nEunsol Choi eunsol@cs.utexas.edu \nThe University of Texas at Austin\n\n\nGreg Durrett gdurrett@cs.utexas.edu \nThe University of Texas at Austin\n\n\nCREAK: A Dataset for Commonsense Reasoning over Entity Knowledge\n\nMost benchmark datasets targeting commonsense reasoning focus on everyday scenarios: physical knowledge like knowing that you could fill a cup under a waterfall[Talmor et al., 2019], social knowledge like bumping into someone is awkward[Sap et al., 2019], and other generic situations. However, there is a rich space of commonsense inferences anchored to knowledge about specific entities: for example, deciding the truthfulness of a claim Harry Potter can teach classes on how to fly on a broomstick. Can models learn to combine entity knowledge with commonsense reasoning in this fashion? We introduce CREAK, a testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities (Harry Potter is a wizard and is skilled at riding a broomstick) with commonsense inferences (if you're good at a skill you can teach others how to do it). Our dataset consists of 13k human-authored English claims about entities that are either true or false, in addition to a small contrast set. Crowdworkers can easily come up with these statements and human performance on the dataset is high (high 90s); we argue that models should be able to blend entity knowledge and commonsense reasoning to do well here. In our experiments, we focus on the closed-book setting and observe that a baseline model finetuned on existing fact verification benchmark struggles on CREAK. Training a model on CREAK improves accuracy by a substantial margin, but still falls short of human performance. Our benchmark provides a unique probe into natural language understanding models, testing both its ability to retrieve facts (e.g., who teaches at the University of Chicago?) and unstated commonsense knowledge (e.g., butlers do not yell at guests). Preprint. Under review. arXiv:2109.01653v1 [cs.CL] 3 Sep 2021 ENT Claim: Harry Potter can teach classes on how to fly on a broomstick. Claim: One can drive La Jolla to New York City in less than two hours. Claim: Fran\u00e7ois Mitterrand became a Texas Senator in 2001.\n\nIntroduction\n\nTo understand text, humans use rich background knowledge about the world. Despite the impressive ability of large-scale pretrained models, models often generate sentences that violate a reader's expectations, particularly in terms of common sense. As these models are increasingly employed in settings like generative question answering [Fan et al., 2019, Lewis et al., 2020 and fact verification [Vlachos and Riedel, 2014, Wang, 2017, Thorne et al., 2018, they should exhibit not just commonsense about everyday scenarios (physical, social, etc.), but factual knowledge about entities as well. These concepts overlap in a set of inferences involving entities that we call entity commonsense. For example, to recognize that \"Many business owners rely on WordPress to create their websites.\" is true requires both knowledge about the entity (WordPress is a website hosting service) and a more nebulous piece of commonsense information (famous products like WordPress are widely used).\n\nWe present CREAK, a dataset aiming to evaluate two major desiderata of NLP models: entity understanding and commonsense inference. Figure 1 shows how these concepts interact in examples from CREAK. Building LMs with a stronger ability to perform this type of inference can help make NLP systems more effective and reliable.\n\nFran\u00e7ois Mitterrand (26 Oct 1916-8 Jan 1996 was a French statesman. Figure 1: CREAK claims with different reasoning types. Datasets like FEVER focus on retrieval (as in the last case); our dataset also features many claims that involve both retrieval and also commonsense reasoning (54% of the data according our manual study in Section 3.3).\n\n\nTRUE\n\n\nFALSE FALSE\n\nOur dataset consists of 13k English claims covering 2.7k entities, each labeled as true or false. Each claim is generated by a crowdworker based on a Wikipedia entity, which can be named entities (e.g., John Dewey), common nouns (e.g., penguins), and abstract concepts (e.g., freedom of speech). Our lightweight task design provides annotators with a set of popular entity topics, and by not including explicit evidence documents to copy text from, annotators are encouraged to create examples fully from scratch. This results in sentences where annotators combine their knowledge about entities with common sense to generate claims. Even without resorting to adversarial filtering, which artificially biases a dataset against existing model checkpoints [Bowman and Dahl, 2021], we find our annotation protocol leads to challenging claims for existing models. We provide in-depth analysis on what makes our dataset uniquely challenging: for example, 18% of claims in CREAK contain quantifiers (e.g., enough, always, rarely etc.) that necessitate subtle commonse reasoning, compared to existing fact verification datasets [Thorne et al., 2018] where only 5% of claims contain the quantifiers.\n\nAsking crowdworkers to generate free-form sentences can introduce dataset artifacts [Gururangan et al., 2018, Geva et al., 2019. We carefully examine such artifacts in our datasets using quantitative tools [Swayamdipta et al., 2020, Gardner et al., 2020 as well as qualitative inspection. We also provide a small set of expert-written contrast examples [Kaushik et al., 2019, Gardner et al., 2020 which pair true and false claims sharing almost identical context. To establish an initial performance level on CREAK, we evaluate state-of-the-art pre-trained language models [Liu et al., 2019, Raffel et al., 2020. Our experiments shows that CREAK is challenging even for a large model, with a gap between model and human accuracy of 10 points on the development set and about 27 points on the contrast set for the largest model. Moreover, the model trained on CREAK outperforms the model trained on other claim verification datasets [Thorne et al., 2018, Eisenschlos et al., 2021, Park et al., 2021, suggesting that CREAK tests different reasoning capabilities compared to existing datasets. We further characterize the performance based on model size, entity type, and the whether external knowledge is used. Our analysis supports that to achieve high performance on our dataset, models should possess not only entity knowledge but also complex reasoning skills. The code and data are publicly available at https://www.cs.utexas.edu/~yasumasa/creak.\n\n\nRelated Work\n\nClaim Verification Our task is formulated as claim verification, which has seen increasing work in recent years. The largest claim verification dataset, FEVER [Thorne et al., 2018], has claims designed to be verifiable with a passage from English Wikipedia and typically covers simple facts such as attributes and records (e.g., \"Benjamin Franklin was a person,\" and \"Spider-Man 2 was released in 2004.\"). In fact, 58% of FEVER claims contain a simple copular verb (is, are, was, were) and many claims contain a definition. Prior work [Eisenschlos et al., 2021] observed high lexical overlap between claims and corresponding entity definitions in Wikipedia, and collected more complex claims using a human-in-the-loop adversarial approach. Similarly, recent work [Park et al., 2021] derives a challenging verification dataset from ambiguous questions [Min et al., 2020] and their different interpretations. However, both datasets focus on a retrieval setting where there is a single paragraph in Wikipedia from which the claim can be easily verified. In contrast, our dataset contains claims where it is not easy to find a single paragraph that can verify them, testing models' intrinsic abilities.\n\nQuestion Answering Question answering and claim verification are closely related, particularly when it comes to binary questions [Clark et al., 2019]. Our dataset is purposely constructed to go beyond basic factoid information like that tested in open QA benchmarks like NaturalQuestions  and focuses on information that is less likely to have textual support on the web. The recently proposed StrategyQA dataset [Geva et al., 2021] which contains binary questions requiring implicit reasoning that goes beyond evidence retrieval (e.g., \"would it be common to find a penguin in Miami?\") captures a similar type of reasoning and knowledge as in our work. However, our annotation process does not require authoring strategies, allowing us to scale to a larger dataset (13K vs. 2.8K) while capturing a wide range of inference types. Finally, some QA datasets have been adapted for evaluating differentiable commonsense reasoning models [Lin et al., 2021], but these benchmarks still test very different knowledge from ours.\n\nCommonsense Reasoning Commonsense reasoning tasks [Levesque et al., 2011, Zellers et al., 2018, Talmor et al., 2019, Lourie et al., 2021 evaluate models' reasoning skills in the physical world, with reporting bias being a principal challenge [Gordon and Van Durme, 2013]. Yet, most datasets assume hypothetical environments and do not address real-world entities. Our work relates to judging plausibility of events [Forbes andChoi, 2017, Wang et al., 2018], closely tied to inferences accessible from feature norms [McRae et al., 2005], but again these past efforts do not focus on judgments around specific entities.\n\nKnowledge Probing The LAMA benchmark [Petroni et al., 2019] was proposed to query factual knowledge covered in language models. Our dataset also covers such factual knowledge but also requires commonsense reasoning capabilities. Our work also creates a moderately sized training dataset. Other datasets in the KILT benchmark [Petroni et al., 2020], an aggregate suite focusing on knowledge intensive tasks, are more focused on recognizing entities and relations, \"low-level\" factual knowledge which does not require the kinds of commonsense inferences in our dataset. Another recent commonsense-focused dataset , focuses on probing numeric claims.\n\n\nCREAK\n\n\nTask Definition\n\nProblem Scope Our benchmark covers claims that are typically quite easy for humans to verify but challenging for language models. We focus on factual claims about real-world entities, but our claims are more complex than existing fact verification examples which tend to state relatively simple facts (i.e., definitive sentences, X is a Y, or sentences expressing simple relations, like X is CEO of Z). To the extent possible, we avoid information that is obscure or requires computation, such as asking about the time between two arbitrary events or how many copies of an album were sold, which test either retrieval or memorization rather than commonsense reasoning. We found that our claims can often be verified with minimum knowledge of the entities combined with common sense (i.e., you can guess the answer accurately even if you do not know the entity very well). 1 We argue that this knowledge is what pre-trained LMs should possess about moderately well-known entities after seeing a few occurrences of them during pre-training. Therefore, our claims should be solvable in the closed-book setting where we can purely evaluate LMs' commonsense reasoning skills, isolated from the performance of retrieval models.\n\nWe formally define the CREAK task as follows. Given a single sentence claim c containing at least one entity mention, the task is to assign a binary label y \u2208 {TRUE, FALSE} indicating whether the claim is true or false. Dataset statistics can be found in Table 1.\n\nDataset Properties Our dataset has following key properties. The claims are diverse covering various types of entities: they are written by 684 distinct crowdworkers 2 only based on the entity names and their minimal information. We rarely find lexical overlap between the claims and publicly available knowledge sources (e.g., the first paragraph of English Wikipedia). As a result, the claims contain a variety of reasoning types, but nevertheless are typically not subjective and easily verifiable. As discussed in Section 3.3, a majority of our examples do involve a combination of commonsense reasoning and knowledge. Finally, Sections 3.3 and 4 show that the dataset is relatively robust to spurious correlations.\n\n\nData Collection\n\nWe collect our data on the Amazon Mechanical Turk platform. Open-ended text generation is challenging to crowdsource, so we take several steps in our task design to ensure quality. First, we ask crowdworkers to write down the reason why the generated claim is true or false; although past work observes that this does not improve example quality in isolation [Nangia et al., 2021], we found it helpful for our task, and it additionally helped us spot workers who misunderstood the task. To keep the sentences natural, we use a minimal set of requirements and encourage crowdworkers to produce creative and diverse sentences. One key requirement is to use action verbs instead of copula, which prevent crowdworkers from writing simple definitive sentences. See Appendix A for more details about the annotation instructions.\n\nWe do not take a model-in-the-loop approach [Zellers et al., 2018, Nie et al., 2020, Bras et al., 2020 during data collection in order to keep our dataset organic, meaning that sentences preserve the original data distribution given by annotators. Therefore, this benchmark does not favor or disfavor particular LM checkpoints, providing a fair and comparable testbed [Bowman and Dahl, 2021].\n\nSeed Entities Curation Entity selection plays a crucial role in this task, since authoring sentences is a much easier task if a crowdworker is familiar with the entity. We take two steps to enable crowdworkers to focus on known entities. First, we use the entity list created by Geva et al. [2021] as part of StrategyQA, which aligns with our needs; the authors select entities based on some popularity measures such as the number of contributors and the number of backward links from other pages. Second, we present five entities to each annotator and let them pick from that set of five when authoring their sentences. We manually inspect the seed entities to maintain the diversity of the types of entities so that the generated claims cover diverse topics (e.g., we want to avoid too many location entities that occur in English Wikipedia frequently). We finally obtain 6.4k entities after this process.\n\nWe split the seed entity list into two parts; one for the training instances and one for the development and test instances. In both sets, roughly 80% of entities are named entities. The 5 most popular entities in the train set are Sloth, Giraffe, George Orwell, 50 Cent, Mattel. In the development and test sets, Butterfly, Ray Charles, Whole Foods Market, Internet troll, and Bigfoot are the top 5 popular entities. As can be seen, crowdworkers prefer to select relatively common entities.\n\n\nQuality Control\n\nWe split the data curation into two separate tasks such that no annotator contributed to both training and evaluation datasets. This mitigates the issue of learning to model the behavior of specific annotators [Geva et al., 2019] and annotation artifacts from annotator developing a template (e.g., ENTITY created ENTITY) across many instances. In total, CREAK is created by a large number of annotators: 153 crowdworkers annotated the development and test instances, and 531 crowdworkers worked on the training instances. We also use disjoint sets of entities between training and dev/test data so a model trained on the dataset is not simply learning properties of the entities under discussion here. We discuss more in Section 3.3.\n\nDuring the annotation process, we monitored the sentence quality and barred crowdworkers who repeatedly produced low-quality sentences or examples following a single pattern. We then inspected the examples included in our evaluation dataset. During the inspection, we found some claims that are subjective, ambiguous, or non-factual (see Appendix B). These errors potentially lower the human performance on the development and test sets. Since automatically detecting these errors is non-trivial, the authors manually filtered all claims in the evaluation dataset. This process removed roughly 18% of crowdsourced claims. This process was crucial for very high human performance (99% majority human performance), as we will see in the experiments.\n\nContrast Set The authors of the paper created a small subset of contrastive examples [Kaushik et al., 2019, Gardner et al., 2020. We select 100 seed claims from the evaluation set, then annotate true and false claims based on the seed claims by applying minimal modification (e.g., replacing a word with a similar one that changes the truth of the claim). Examples can be found in Appendix B.\n\n\nDataset Analysis\n\nIn this section, we examine the quality of our dataset. We first manually examine what types of reasoning are required to verify our claims. Then, we study potential lexical and syntactic artifacts in human-generated claims through statistical tests and training dynamics to identify word-level artifacts and learnability of the training instances.\n\nManual Analysis of Reasoning Types We manually validate whether the CREAK claims truly require both knowledge and commonsense. We classify reasoning types into three categories: 1) retrieval, 2) common sense, and 3) a mix of retrieval and common sense. These distinctions are somewhat subjective based on the background knowledge of the reader (i.e., is it common sense that NYC is a major city?); we use our own judgments as authors. The first category, retrieval, asks simple facts about entities which can be found in some knowledge sources such as English Wikipedia (e.g., The Harry Potter series originally began with the books.). The second category, common sense, requires more complex reasoning but are verifiable with the basic knowledge of the entities (e.g., Only trumpet players can perform a solo.). The third category is a mix of retrieval and common sense, meaning that it involves some degree of retrieval and commonsense reasoning. For example, the claim One can drive from La Jolla to New York City in less than two hours. requires knowing the locations of La Jolla and New York City (retrieval) and reasoning about driving times (common sense). We randomly sample 100 claims from the evaluation instances and classify them into the three categories. The proportion of the retrieval, common sense, and a mix of the two categories is 18%, 28%, and 54% respectively. You can find examples for each reasoning type in Appendix B.\n\nDataset Artifacts Past work on natural language inference has noted that \"artifacts,\" or spurious correlations with surface properties of text, may arise during the annotation process [Gururangan et al., 2018, Poliak et al., 2018. The low performance of a bag-of-words model in our setting (see Table 2) gives some confidence that such correlations are not a dominant factor in performance on our data, but we undertake quantitative analysis to explore this further.\n\nWe identify the word-level artifacts in CREAK by computing the artifact statistics described in Gardner et al. [2021]. These statistics tell us, given a balanced dataset, if some words are highly correlated with either true or false claims in a way that a model can exploit. This boils down to a one-side binomial hypothesis test with the null hypothesis p(y|x i ) = 0.5, where y \u2208 {TRUE, FALSE} is a label and x i is a word in the vocabulary. We first count the occurrence of all words 3 in CREAK . For each word x i that appears in the n i claims, we count the number of the target label y in the n i claims. We estimate p(y|x i ) with the observed probabilityp(y|x i ), which is given by a fraction of the count of y over n i . Following Gardner et al. [2021], we then compute a z-statistic and reject/accept the null hypothesis using \u03b1 = 0.01 with the Bonferroni correction. Figure 2 plots the word counts n i (x-axis) against the observed probabilityp(y|x i ) (y-axis) for CREAK and FEVER dataset. We additionally draw the curve that represents the corresponding probability of \u03b1 = 0.01/13k (for CREAK) and \u03b1 = 0.01/10k (for FEVER) at each n i . Any words above this line are considered to be artifacts in the dataset. We find 14 words (out of 13k words in the vocabulary) sit above the line. We label the most frequent words in the plot. Surprisingly, and (n = 1973) is the most frequent artifact that signals the true label, followed by some quantifiers (many, n = 483, and several, n = 119). not (n = 274) and only (n = 186) suggest the false label in both datasets. Overall, CREAK contains relatively few artifacts, and they do not impact the data quality significantly since their frequency is not very high. We observe fewer artifacts compared to FEVER dataset (14 words vs. 28 words above the threshold).\n\nTraining Dynamics We analyze training dynamics using the framework proposed by Swayamdipta et al. [2020]. The training dynamics of a training instance are defined by confidence and variability,the mean and the standard deviation of model predictions (probability) on the gold label over training epochs. Additionally, correctness is computed by the number of times a training instance is correctly predicted over the number of epochs. Figure 3 shows the histograms of those measurements 4 for CREAK (10k instances) and FEVER (105k instances). We use ROBERTA Large 5 for all experiments. In the confidence plots, CREAK has a fatter distribution (i.e., certain instances get low probability on their gold labels) compared to FEVER's skewed distribution where the majority of instances get very high probability (e.g., > 0.9) on the gold labels. CREAK's variability histogram is nearly bell-shaped while FEVER's histogram skews towards zero. As can be seen in the correctness plots, some training instances in CREAK are not always predicted correctly during training, as its distribution suggests. However, the most of training instances of FEVER are correctly predicted consistently through the training epochs. By aggregating these observations, we hypothesize that CREAK contains training instances with different difficulty levels compared to FEVER.\n\n\nExperiments\n\nWe focus on the closed-book setting where models are ask to make decisions based solely on claims without any additional retrieved evidence. To see if existing claim verification datasets provide entity commonsense supervision, we train claim-only baseline models on FEVER [Thorne et al., 2018], FAVIQ [Park et al., 2021], and FOOLMETWICE (FM2) [Eisenschlos et al., 2021] and then evaluate them on CREAK . Next, we train models on the CREAK training set and measure the improvements over the baselines. We also investigate the impacts of model sizes and external knowledge.\n\n\nExperimental Setup\n\nWe investigate three training data settings. In the Zero-Shot setting, we train models on the train sets of FEVER KILT , FAVIQ-A, FAVIQ-R, 6 and FOOLMETWICE (FM2). In the In-Domain setting, we train models on the CREAK train set in a standard fashion. The Finetuning setting means that we train models on FEVER and then further finetune on CREAK.\n\nWe evaluate all models on the CREAK balanced development, test, and contrast sets and report accuracy. As we discussed in Section 3, these evaluation sets use distinct entities from the train set and are authored by a different set of crowdworkers.\n\n\nComparison Systems\n\n\nClosed-book (Claim-only) Models\n\nIn what we consider our standard setting, these models take a claim as input and predict if the claim is true or false. We use a RoBERTa encoder [Liu et al., 2019] with a MLP classifier for baseline models: ROBERTA Large and ROBERTA Base . We also train SVM with TF-IDF, which gives a linear baseline using far fewer parameters than the LM-based models. We further employ T5-3B to see if more parameters help to learn the complex reasoning in CREAK.\n\nRetrieval-based Models These models are augmented with knowledge retrieved from Wikipedia. We feed a claim and k retrieved passages to a model, which can use the information in the passages to influence the decision. We use Dense Passage Retrieval (DPR) [Karpukhin et al., 2020] 7 , a dualencoder based model, as a retriever and English Wikipedia as a knowledge base. Specifically, we use the DPR model trained on the KILT benchmark, which includes FEVER. We use this configuration for the open-book experiments, where we finetune models on our training set as well as on FEVER. For a claim classifier, we use the ROBERTA Large model and denote this retrieval-based model as ROBERTA Large-DPR . We retrieve k = 3 passages for all experiments.\n\nHuman Performance To estimate human performance on the development set, we sample 100 examples and ask the authors of this paper to predict the corresponding labels. For the contrast set, three of the authors predict labels for claims that they did not annotate. We report the averaged human accuracy and the ensemble accuracy which we use the majority label as the final prediction to computer human performance.  Table 3 presents results for retrieval augmented approaches. We observe that all baseline models fall behind our estimated human performance by a substantial margin.\n\n\nResults and Discussion\n\nTransfer from existing datasets The zero-shot block of Table 2 compares performance of RoBERTa models trained on four prior claim verification datasets. The models trained on FAVIQ-R and FAVIQ-A perform similarly with the majority label baseline. The model trained on FM2 shows better performance than the FAVIQ models, but the accuracy is still very low. We see much improved transfer from FEVER KILT dataset, reaching an accuracy of 70%. Although designed to be more challenging than FEVER, FAVIQ and FM2 may result in models that transfer less well because these  datasets are more dependent on retrieving specific passages to judge claims, containing fewer claims resolvable with commonsense reasoning. Additionally, T5-3B trained on FEVER KILT is only better than ROBERTA Large by 3 points on the development set and 6.5 points on the test set although it is 8 times larger, suggesting that FEVER KILT is bounded in terms of how useful it can be for CREAK .\n\nIn the Finetuning block of Table 2, we report the performance of ROBERTA Large first trained on FEVER KILT and then on CREAK. Compared to ROBERTA Large trained only CREAK, additional pre-training does not bring meaningful gains.\n\nAre larger models better? The In-Domain block of Table 2 lists performance by models with different sizes ranging from 13k to 3B parameters. All models are trained on CREAK. ROBERTA Base , outperforms SVM with TF-IDF by 11 points on the test set, suggesting that a larger, knowledge-rich model can do better. But its advantage shrinks on the contrast set, only gaining 4 points over SVM with TF-IDF. A larger model ROBERTA Large , 355M parameters, further improves the performance, and this trend continues to an even larger model T5-3B, which outperforms ROBERTA Large by 5 points on the test set and 8.5 points on the contrast set. T5-3B achieves the highest accuracy in the closed-book setting. Given how the contrast set was constructed, the fact that higher-capacity models work better suggests that having more entity knowledge is a key to doing better on this task.\n\nPerformance breakdown by entity types We examine whether models are better equipped at verify claims about different entities depending on their popularity and type, as given by an NER tagger. We use ROBERTA Large as an in-domain baseline model for this analysis. To compare entity popularity, we partition our dataset into equally sized quartiles based on total number of views the entity's Wikipedia page has received since Jan. 1, 2016. For entity types, we use an off-the-shelf NER tagger from spaCy [Honnibal et al., 2020] to group examples by the entity type. In Figure 4, we plot the performance on each partition of our dataset. We observe that the model performs comparably regardless of entity popularity, partially because we sampled from popular entities, and that entity type has a greater affect on accuracy.\n\nRetrieval-based models with external knowledge To investigate the importance of entity knowledge in CREAK , we experiment in two additional settings. First, to confirm that entities are important, we experiment with the closed-book setting where all entities are dropped from the claims; this data is denoted as CREAK ENT_LESS . Second, we explore the retrieval setting, where we append three English Wikipedia passages retrieved by DPR to the claims. Similar to the main experiments, we use three data settings: Zero-Shot, In-Domain, and Finetuning. Table 3 shows the results of all models. ROBERTA Large trained on CREAK ENT_LESS loses 10 points compared to the model trained on the standard CREAK training set (In-Domain ROBERTA Large in Table 2). This shows that seeing the entity mention in the claim is important. For open-book models, we again see that In-Domain models are better than Zero-Shot models. One distinction from the closed-book setting is that the additional finetuning on FEVER KILT improves performance. If we compare the In-Domain model from the closed and retrieval settings, the additional passages bring 4 points of improvement. Although adding more entity knowledge improves performance on CREAK, there is still a gap from the human performance, particularly on the contrast set. This shows that there are some facts immediately retrievable from Wikipedia; however, our analysis of the dataset also shows that significant additional reasoning is required as well. Moreover, we believe that this kind of knowledge should be accessible to models in a closed-book way, as annotators were able to create these examples without consulting Wikipedia or other knowledge sources.\n\n\nConclusion\n\nWe have presented a dataset CREAK of binary claims involving \"entity commonsense,\" a combination of entity knowledge and commonsense reasoning. This dataset is useful both as a training set for instilling this kind of reasoning into models as well as a test set for probing whether models can recognize factually incorrect statements about entities. We believe this can be a useful proving ground for models infused with entity knowledge (e.g., entities-as-experts [F\u00e9vry et al., 2020] or interpretable entity embeddings [Onoe and Durrett, 2020]) and contribute to development of these techniques.\n\n\nLimitations and Ethical Concerns\n\nWe emphasize that our dataset is not intended for training general fact-checking models; we do not support large-scale deployment of models trained on CREAK for this purpose. Furthermore, while we have tried to measure artifacts in this dataset and found them to be minimal, our claims are artificially generated and the nature of the dataset can differ significantly from claims naturally occurring in social media or web. Large language models fine-tuned on our dataset may preserve biases learned from the web text during pre-training or biases of our annotators and make biased judgments as a result. See the datasheet in the Supplementary Material for more information about specific harms that could arise from this dataset.\n\n\nAppendix A Annotation Interface\n\nWe compensate crowdworkers $0.60 USD per HIT. Each HIT is composed of generating one true and one false claim, along with a short explanation for each claim. Compensation was determined to approximate at least a $12 USD hourly wage. The total amount spent on compensating crowdworkers was roughly $4,000 USD. Our annotation instructions and interface are given in Figure 5.    Koi typically costs someone hundreds of dollars. A nun takes a vow to remain unmarried and have no children.\n\nA nun takes a vow to marry a priest and raise their children in the church. commonsense reasoning within the context of everyday scenarios, there is a rich, unexplored space of commonsense inferences that are anchored in knowledge about specific entities. We therefore create this dataset to benchmark how well current systems are able to perform this type of reasoning and to promote the development of systems that can handle these challenges.\n\nHas the dataset been used already? We require all papers reporting on our dataset to submit their results to our dataset website (https://www.cs.utexas.edu/~yasumasa/creak).\n\nWho funded the dataset? This dataset was partially funded by the US National Science Foundation (NSF Grant IIS-1814522).\n\n\nB Dataset Composition\n\nWhat are the instances? Each instance is a claim about an entity which may be either true or false. These claims are constructed such that validating them requires specific knowledge of each entity, with many also requiring commonsense reasoning incorporating these facts. All claims are written in English.\n\nHow many instances are there? Our dataset consists of 13K claims, some of which form a smallscale contrastive evaluation set. A detailed breakdown of the number of instances can be seen in Table 1 of the main paper.\n\nWhat data does each instance consist of? Each instance is a human-written claim about a given Wikipedia entity with an associated TRUE / FALSE label of its factually.\n\nDoes the data rely on external resources? No, all resources are included in our release.\n\nAre there recommended data splits or evaluation measures? We include the recommended train, development, and test sets for our datasets. Each split is constructed such that there are no overlapping annotators nor entities between each set. We also include a small contrast set containing minimally edited pairs of examples with opposing labels of factually. The distribution of examples across splits can be seen in Table 1.\n\n\nC Data Collection Process\n\nHow was the data collected? We use crowdsourcing to collect claims. Each worker is presented with 5 entities and are instructed to select one to generate two claims for, one true and one false. For each of these claims, workers are also instructed to provided a short explanation for why the claim is true or false.\n\nWho was involved in the collection process and what were their roles? We recruit crowdworkers from Amazon Mechanical Turk to perform the all the annotation steps outlined above.\n\nOver what time frame was the data collected? The dataset was collected over a period of April to August 2021.\n\nDoes the dataset contain all possible instances? We source our list of popular Wikipedia entities, as measured by number of contributors and backlinks, from Geva et al. [2021]. Annotators are also instructed to select one of five entities to construct an example for. Our sampling process, therefore, selects for popular entities that exist in Wikipedia.\n\nWhile we do not cover the entire space of possible entity-centric claims, we promote diversity in our dataset by limiting the total number of claims a single worker can generate to 7% of any single split and by sampling from a large pool of entities. In total, our dataset is comprised of claims that were generated from 684 total crowdworkers covering over 3,000 unique entities.\n\nIf the dataset is a sample, then what is the population? CREAK represents a subset of all possible entity-centric claims, including those which require commonsense in addition to retrievable facts to verify. Our dataset also only includes claims written in English.\n\n\nD Data Preprocessing\n\nWhat preprocessing / cleaning was done? We do minimal preprocessing on the collected claims; however, we monitor crowdworker performance for sentence quality and remove repetitive examples produced by the same crowdworker. We also manually filter and clean our development and test sets for grammatically. This process removed roughly 18% of crowdsourced claims and high human performance (99% majority human performance) on 100 randomly sampled examples from our development set.\n\nWas the raw data saved in addition to the cleaned data? We maintain a record of all the original authored claims, as well as the explanations written by each claim's author. This data will be made available upon request.\n\nDoes this dataset collection/preprocessing procedure achieve the initial motivation? Our collection process indeed achieves our initial goals of creating a diverse dataset of entity-centric claims requiring commonsense reasoning. Using this data, we are able to evaluate how models that are trained on past data generalize to answering questions in the future, asked at the time of our data collection.\n\n\nE Dataset Distribution\n\nHow is the dataset distributed? We make our dataset available at https://www.cs.utexas. edu/~yasumasa/creak.\n\nWhen was it released? Our data and code is currently available.\n\nWhat license (if any) is it distributed under? CREAK is distributed under the CC BY-SA 4.0 license. 10 Who is supporting and maintaining the dataset? This dataset will be maintained by the authors of this paper. Updates will be posted on the dataset website.\n\n\nF Legal and Ethical Considerations\n\nWere workers told what the dataset would be used for and did they consent? Crowd workers informed of the goals we sought to achieve through data collection. They also consented to have their responses used in this way through the Amazon Mechanical Turk Participation Agreement.\n\nIf it relates to people, could this dataset expose people to harm or legal action? Our dataset does not contain any personal information of crowd workers; however, our dataset can include incorrect information. We perform extensive quality control and error analysis to minimize the risk due to incorrect labels. We bear all responsibility in case of violation of rights.\n\nNote that our dataset may, by design, contain false claims about real people or organizations. Most of the claims we saw are harmless in their incorrect nature rather than libelous; this includes all claims in the development and test data, which we manually inspected. However, there could be claims in the training set which are mislabeled and which could impart false \"knowledge\" to trained models.\n\nWe removed one entity from our dataset which was a deadname. 10 https://creativecommons.org/licenses/by-sa/4.0/legalcode If it relates to people, does it unfairly advantage or disadvantage a particular social group? We acknowledge that, because our dataset only covers English and annotators are required to be located in the US, our dataset lacks representation of claims that are relevant in other languages and to people around the world.\n\nThe data itself could possibly contain generalizations about groups of people; for example, one of the entities is Hopi people. As above, we audited all claims in the development and test set (20% of the data) and uniformly found claims to be respectful even when incorrect. However, incorrectly labeled claims in the training data could potentially teach false associations to trained models.\n\nFigure 2 :Figure 3 :\n23Artifact statistics of CREAK and FEVER train sets. Words (colored dots) above the green line have detectable correlation with class labels. CREAK contains relatively fewer artifacts, with low severity and frequency. Training dynamics for CREAK and FEVER train sets. This figure shows histograms of CREAK or FEVER training instances bucketed by confidence (mean), variability (std.), or correctness. On all three measures, CREAK shows fatter distributions compared to FEVER, implying that CREAK consists of instances with different difficulties.\n\nFigure 4 :\n4Performance breakdown on partitions of the development set, split by entity popularity and type, using the four most common entity types, which comprise 86% of examples (includng \"NONE\" for non-named entities).\n\nFigure 5 :\n5Annotation interface.\n\nTable 1 :\n1Data statistics of CREAK.Split \n\n# Claims \nAverage Length \n# Unique Entities \nVocab Size \nTotal \nTrue False \n(# tokens) \n\nTrain \n10,176 5,088 5,088 \n10.8 \n2,096 \n19,006 \nDev \n1,371 \n691 \n680 \n9.7 \n531 \n4,520 \nTest \n1,371 \n707 \n664 \n9.9 \n538 \n4,620 \nTest (Contrast) \n200 \n100 \n100 \n11.0 \n100 \n800 \n\n\n\nTable 2\n2presents our main experimental results for closed book systems, and\n\nTable 2 :\n2Performance of closed-book approaches on CREAK. Transfer results from prior datasets show that our dataset is distnct from these. Larger models trained with in-domain data perform the best out of all models we consider, but still lag behind human performance.Model \n#Params \nTraining Data \nAccuracy \nType \nSize \nDev Test Contrast \n\nMajority Label \n-\n-\n-\n51.6 51.6 \n50.0 \n\nZero-Shot \n\nROBERTA Large \n355M \nFAVIQ-R \n141k \n49.6 48.4 \n50.0 \nROBERTA Large \n355M \nFAVIQ-A \n17k \n52.3 52.6 \n52.0 \nROBERTA Large \n355M \nFM2 \n10k \n59.2 58.2 \n52.0 \nROBERTA Large \n355M \nFEVER KILT \n105k \n69.6 70.2 \n59.0 \nT5-3B \n3B \nFEVER KILT \n105k \n72.9 76.7 \n61.5 \n\nIn-Domain \n\nSVM + TF-IDF \n13k \nCREAK \n10k \n60.2 60.3 \n52.0 \nROBERTA Base \n125M \nCREAK \n10k \n72.2 71.6 \n56.0 \nROBERTA Large \n355M \nCREAK \n10k \n80.6 80.3 \n61.5 \nT5-3B \n3B \nCREAK \n10k \n85.6 85.1 \n70.0 \n\nFinetuning ROBERTA Large \n355M \nFEV \u2192 CREAK 115k \n80.5 81.1 \n64.0 \n\nHuman (averaged) \n-\n-\n-\n96.3 \n-\n92.2 \nHuman (ensemble) \n-\n-\n-\n99.0 \n-\n99.0 \n\n0 -12 \n12 -28 \n28 -54 \n54 -406 \n\nPageviews (in 100Ks) \n\n\n\nTable 3 :\n3Performance of retrieval-augmented approaches on CREAK. Large models retrieving from Wikipedia can do better, although the performance on the contrast set is still low.Model \n#Params \nTraining Data \nAccuracy \nType \nSize \nDev Test Contrast \n\nMajority Label \n-\n-\n-\n51.6 51.6 \n50.0 \n\nZero-Shot ROBERTA Large+DPR \n575M \nFEVER KILT \n105k \n79.9 80.7 \n68.0 \n\nIn-Domain \nROBERTA Large \n355M \nCREAKENT_LESS \n10k \n69.2 67.3 \n52.0 \nROBERTA Large+DPR \n575M \nCREAK \n10k \n84.0 84.3 \n70.0 \n\nFinetuning ROBERTA Large+DPR \n575M \nFEV \u2192 CREAK 115k \n88.7 86.8 \n72.0 \n\nHuman (averaged) \n-\n-\n-\n96.3 \n-\n92.2 \nHuman (ensemble) \n-\n-\n-\n99.0 \n-\n99.0 \n\n\n\nTable 4 :\n4CREAK claims with different reasoning types. The Atmosphere of Earth includes many types of gases. Common Sense + Retrieval TRUE One can drive from La Jolla to New York City in less than two hours. Common Sense + Retrieval FALSE J. P. Morgan restored the US Treasury surplus.Claim \nReasoning Type \nLabel \n\nHarry Potter can teach classes on how to fly on a broomstick. \nCommon Sense \nTRUE \nGrizzly bear live in danger of being hunted by other animals. \nCommon Sense \nFALSE \nRetrieval \nTRUE \nFran\u00e7ois Mitterrand became a Texas Senator in 2001. \nRetrieval \nFALSE \n\n\n\nTable 5 :\n5Unusable claims generated by crowdworkers.Claim \nRejection Rationale Label \n\nIt is alleged that a Nerd are computer geeks. \nSubjective \nTRUE \nGreen Day radiates a folksy vibe. \nSubjective \nFALSE \nDan Brown died in 2019 of heart failure. \nOffensive \nFALSE \nIt is very fun to be audited. \nAmbiguous \nFALSE \nDuring the holidays people create performances. \nAmbiguous \nFALSE \nYou can tell that a Goose is an alligator. \nOutlandish \nFALSE \n\n\n\nTable 6 :\n6Examples from contrast set. U.S. Route 1 connects New York to Florida. U.S. Route 1 connects New York to California. The Beatles released their first album on vinyl. The Beatles released their first album on Spotify. Koi can cost someone hundreds of dollars.True Claim \nFalse Claim \n\n\nDuring our validation, we could confidently judge about 30-50% of claims without searching the web.2  We limit the number of claims that a single crowdworker can generate (no more than 7% of any split.\nWe drop punctuation and lower all words.\nAll values are normalized between 0 and 1 then bucketed into subgroups. 5 Following the suggestions bySwayamdipta et al. [2020], we train models with early stopping with the patience = 3, resulting in 7 epochs for CREAK and 6 epochs for FEVER.\nThe FAVIQ benchmark consists of two datasets based on the same source QA dataset. 7 DPR is licensed under CC BY-NC 4.0\ntransformers is licensed under the Apache-2.0 License 9 DeepSpeed is licensed under the MIT License\nAcknowledgmentsThis work was partially supported by NSF Grant IIS-1814522; this support included funding for the data annotation. The authors would like to thank Mor Geva for providing the raw list of entities used in StrategyQA, as well as the Mechanical Turk annotators who participated in our task.Appendix B ExamplesAppendix C Implementation DetailsWe train all models for a maximum of 10 epochs, with the exception of our T5-3B baseline finetuned on FEVER KILT which was trained for a maximum of 6. We select the best checkpoint, evaluated on development data after each epoch. All models are trained using the AdamW optimizer with no warmup steps. ROBERTA and T5-3B based models were trained with a learning rate of 5 \u00d7 10 \u22126 and 3 \u00d7 10 \u22125 , respectively. Our closed-book ROBERTA models and T5-3B model finetuned on FEVER KILT were trained with a batch size of 32 and our ROBERTA Large + DPR model with a batch size of 16. We use the transformers library Wolf et al. All experiments were run on four RTX 8000 GPUs, with our longest experiment taking three days. All our implementation details, including scripts for training/running each of our baselines are made available at https://www.cs.utexas.edu/~yasumasa/creak.Appendix D Datasheet for CREAK A Motivation for Datasheet CreationWhy was the dataset created? Despite their impressive abilities, large-scale pretrained models often fail at performing simple commonsense reasoning. While most benchmark datasets target\nWhat Will it Take to Fix Benchmarking in Natural Language Understanding?. R Samuel, George Bowman, Dahl, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)2021Samuel R. Bowman and George Dahl. What Will it Take to Fix Benchmarking in Natural Language Understand- ing? In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2021.\n\nAdversarial Filters of Dataset Biases. Swabha Ronan Le Bras, Chandra Swayamdipta, Rowan Bhagavatula, Matthew E Zellers, Ashish Peters, Yejin Sabharwal, Choi, Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew E. Peters, Ashish Sabharwal, and Yejin Choi. Adversarial Filters of Dataset Biases. 2020.\n\nBoolQ: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, 10.18653/v1/N19-1300Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Con- ference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https: //aclanthology.org/N19-1300.\n\nFool Me Twice: Entailment from Wikipedia Gamification. Julian Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin B\u00f6rschinger, Jordan Boyd-Graber, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)2021Julian Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin B\u00f6rschinger, and Jordan Boyd-Graber. Fool Me Twice: Entailment from Wikipedia Gamification. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2021.\n\nELI5: Long Form Question Answering. Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, Michael Auli, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long Form Question Answering. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2019.\n\nEntities as Experts: Sparse Memory Access with Entity Supervision. Thibault F\u00e9vry, Baldini Livio, Nicholas Soares, Eunsol Fitzgerald, Tom Choi, Kwiatkowski, 10.18653/v1/2020.emnlp-main.400Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsThibault F\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as Experts: Sparse Memory Access with Entity Supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4937-4951, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.400. URL https: //aclanthology.org/2020.emnlp-main.400.\n\nVerb Physics: Relative Physical Knowledge of Actions and Objects. Maxwell Forbes, Yejin Choi, 10.18653/v1/P17-1025Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Long Papers)Maxwell Forbes and Yejin Choi. Verb Physics: Relative Physical Knowledge of Actions and Objects. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 266-276, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1025. URL https://aclanthology.org/P17-1025.\n\nEvaluating Models' Local Decision Boundaries via Contrast Sets. Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A Smith, Sanjay Subramanian, Findings of the Association for Computational Linguistics: EMNLP. Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben ZhouMatt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. Evaluating Models' Local Decision Boundaries via Contrast Sets. In Findings of the Association for Computational Linguistics: EMNLP, 2020.\n\nCompetency Problems: On Finding and Removing Artifacts in Language Data. Matt Gardner, William Merrill, Jesse Dodge, Matthew E Peters, Alexis Ross, Sameer Singh, Noah A Smith, abs/2104.08646ArXiv. Matt Gardner, William Merrill, Jesse Dodge, Matthew E. Peters, Alexis Ross, Sameer Singh, and Noah A. Smith. Competency Problems: On Finding and Removing Artifacts in Language Data. ArXiv, abs/2104.08646, 2021.\n\nAre We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets. Mor Geva, Yoav Goldberg, Jonathan Berant, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)Mor Geva, Yoav Goldberg, and Jonathan Berant. Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\n\nDid Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 2021Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.\n\nReporting bias and knowledge acquisition. Jonathan Gordon, Benjamin Van Durme, AKBC '13. Jonathan Gordon and Benjamin Van Durme. Reporting bias and knowledge acquisition. In AKBC '13, 2013.\n\nAnnotation Artifacts in Natural Language Inference Data. Swabha Suchin Gururangan, Omer Swayamdipta, Roy Levy, Samuel Schwartz, Noah A Bowman, Smith, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation Artifacts in Natural Language Inference Data. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL- HLT), 2018.\n\nspaCy: Industrial-strength Natural Language Processing in Python. Matthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, 10.5281/zenodo.1212303Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-strength Natural Language Processing in Python, 2020. URL https://doi.org/10.5281/zenodo.1212303.\n\nDense Passage Retrieval for Open-Domain Question Answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wentau Yih, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2020Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen- tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\n\nLearning the Difference that Makes a Difference with Counterfactually-Augmented Data. Divyansh Kaushik, Eduard H Hovy, Zachary C Lipton, International Conference on Learning Representations (ICLR). Divyansh Kaushik, Eduard H. Hovy, and Zachary C. Lipton. Learning the Difference that Makes a Difference with Counterfactually-Augmented Data. In International Conference on Learning Representations (ICLR), 2019.\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics (TACL). Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics (TACL), 2019.\n\nThe Winograd Schema Challenge. Hector J Levesque, Ernest Davis, Leora Morgenstern, AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning. Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, 2011.\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-Tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, Douwe Kiela, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)2020Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nBirds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models. Seyeon Bill Yuchen Lin, Rahul Lee, Xiang Khanna, Ren, 10.18653/v1/2020.emnlp-main.557Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. Birds have four legs?! NumerSense: Prob- ing Numerical Commonsense Knowledge of Pre-Trained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6862-6868, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.557. URL https://aclanthology.org/2020.emnlp-main.557.\n\nDifferentiable Open-Ended Commonsense Reasoning. Haitian Bill Yuchen Lin, Bhuwan Sun, Manzil Dhingra, Xiang Zaheer, William Ren, Cohen, 10.18653/v1/2021.naacl-main.366Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsBill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Xiang Ren, and William Cohen. Differentiable Open-Ended Commonsense Reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4611-4625, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.366. URL https://aclanthology.org/2021.naacl-main.366.\n\n. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, abs/1907.11692RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv, abs/1907.11692, 2019.\n\nUNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark. Nicholas Lourie, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceNicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.\n\nSemantic feature production norms for a large set of living and nonliving things. Ken Mcrae, George Cree, Mark Seidenberg, Chris Mcnorgan, 10.3758/BF03192726Behavior research methods. 372005Ken McRae, George Cree, Mark Seidenberg, and Chris Mcnorgan. Semantic feature production norms for a large set of living and nonliving things. Behavior research methods, 37:547-59, 12 2005. doi: 10.3758/BF03192726.\n\nAmbigQA: Answering ambiguous open-domain questions. Sewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2020Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. AmbigQA: Answering ambiguous open-domain questions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\n\nWhat Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?. Nikita Nangia, Saku Sugawara, Harsh Trivedi, Alex Warstadt, Clara Vania, Samuel R Bowman, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2021Nikita Nangia, Saku Sugawara, Harsh Trivedi, Alex Warstadt, Clara Vania, and Samuel R. Bowman. What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks? In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2021.\n\nAdversarial NLI: A New Benchmark for Natural Language Understanding. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2020Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A New Benchmark for Natural Language Understanding. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2020.\n\nInterpretable Entity Representations through Large-Scale Typing. Yasumasa Onoe, Greg Durrett, 10.18653/v1/2020.findings-emnlp.54Findings of the Association for Computational Linguistics: EMNLP 2020. OnlineAssociation for Computational LinguisticsYasumasa Onoe and Greg Durrett. Interpretable Entity Representations through Large-Scale Typing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 612-624, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.54. URL https: //aclanthology.org/2020.findings-emnlp.54.\n\nFaVIQ: FAct Verification from Information-seeking Questions. Jungsoo Park, Sewon Min, Jaewoo Kang, Luke Zettlemoyer, Hannaneh Hajishirzi, abs/2107.02153ArXiv. Jungsoo Park, Sewon Min, Jaewoo Kang, Luke Zettlemoyer, and Hannaneh Hajishirzi. FaVIQ: FAct Verification from Information-seeking Questions. ArXiv, abs/2107.02153, 2021.\n\nLanguage Models as Knowledge Bases?. Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language Models as Knowledge Bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.org/D19-1250.\n\nKILT: a Benchmark for Knowledge Intensive Language Tasks. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rockt\u00e4schel, Sebastian Riedel, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)2020Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. KILT: a Benchmark for Knowl- edge Intensive Language Tasks. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2020.\n\nHypothesis Only Baselines in Natural Language Inference. Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, Benjamin Van Durme, 10.18653/v1/S18-2023Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics. the Seventh Joint Conference on Lexical and Computational SemanticsNew Orleans, LouisianaAssociation for Computational LinguisticsAdam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis Only Baselines in Natural Language Inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/S18-2023. URL https://aclanthology.org/S18-2023.\n\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n\nDeepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data MiningJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505--3506, 2020.\n\nSocial IQa: Commonsense reasoning about social interactions. Maarten Sap, Hannah Rashkin, Derek Chen, Yejin Ronan Le Bras, Choi, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019.\n\nDataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics. Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, Yejin Choi, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2020Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, and Yejin Choi. Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.\n\nCommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019.\n\nFEVER: a large-scale dataset for fact extraction and VERification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2018.\n\nFact Checking: Task definition and dataset construction. Andreas Vlachos, Sebastian Riedel, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)Andreas Vlachos and Sebastian Riedel. Fact Checking: Task definition and dataset construction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2014.\n\nModeling Semantic Plausibility by Injecting World Knowledge. Su Wang, Greg Durrett, Katrin Erk, 10.18653/v1/N18-2049Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics2Short PapersSu Wang, Greg Durrett, and Katrin Erk. Modeling Semantic Plausibility by Injecting World Knowledge. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 303-308, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2049. URL https://aclanthology.org/N18-2049.\n\nliar, liar pants on fire\": A new benchmark dataset for fake news detection. William Yang, Wang , Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)William Yang Wang. \"liar, liar pants on fire\": A new benchmark dataset for fake news detection. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2017.\n\nTransformers: State-of-the-Art Natural Language Processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnlineAssociation for Computational LinguisticsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6.\n\nSWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. Rowan Zellers, Yonatan Bisk, Roy Schwartz, Yejin Choi, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.\n\nHellaSwag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2019.\n", "annotations": {"author": "[{\"end\":141,\"start\":68},{\"end\":219,\"start\":142},{\"end\":289,\"start\":220},{\"end\":362,\"start\":290}]", "publisher": null, "author_last_name": "[{\"end\":81,\"start\":77},{\"end\":159,\"start\":154},{\"end\":231,\"start\":227},{\"end\":302,\"start\":295}]", "author_first_name": "[{\"end\":76,\"start\":68},{\"end\":149,\"start\":142},{\"end\":153,\"start\":150},{\"end\":226,\"start\":220},{\"end\":294,\"start\":290}]", "author_affiliation": "[{\"end\":140,\"start\":106},{\"end\":218,\"start\":184},{\"end\":288,\"start\":254},{\"end\":361,\"start\":327}]", "title": "[{\"end\":65,\"start\":1},{\"end\":427,\"start\":363}]", "venue": null, "abstract": "[{\"end\":2437,\"start\":429}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2807,\"start\":2790},{\"end\":2827,\"start\":2807},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2875,\"start\":2850},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2887,\"start\":2875},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2908,\"start\":2887},{\"end\":3000,\"start\":2976},{\"end\":3795,\"start\":3772},{\"end\":3806,\"start\":3795},{\"end\":4905,\"start\":4882},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5270,\"start\":5249},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5429,\"start\":5405},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5448,\"start\":5429},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5552,\"start\":5527},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5574,\"start\":5552},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5695,\"start\":5674},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5717,\"start\":5695},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5911,\"start\":5894},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5932,\"start\":5911},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6273,\"start\":6253},{\"end\":6299,\"start\":6273},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6318,\"start\":6299},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6967,\"start\":6946},{\"end\":7348,\"start\":7322},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7568,\"start\":7550},{\"end\":7656,\"start\":7638},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8136,\"start\":8116},{\"end\":8419,\"start\":8400},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8938,\"start\":8920},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9081,\"start\":9059},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9103,\"start\":9081},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9124,\"start\":9103},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9145,\"start\":9124},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9278,\"start\":9251},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9435,\"start\":9424},{\"end\":9465,\"start\":9435},{\"end\":9544,\"start\":9524},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9687,\"start\":9665},{\"end\":9975,\"start\":9953},{\"end\":11176,\"start\":11175},{\"end\":12910,\"start\":12889},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13419,\"start\":13398},{\"end\":13437,\"start\":13419},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13456,\"start\":13437},{\"end\":13745,\"start\":13722},{\"end\":14045,\"start\":14027},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15397,\"start\":15378},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16759,\"start\":16738},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16781,\"start\":16759},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19069,\"start\":19045},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19090,\"start\":19069},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19446,\"start\":19425},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20091,\"start\":20070},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21251,\"start\":21226},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22807,\"start\":22786},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22834,\"start\":22815},{\"end\":22884,\"start\":22858},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23925,\"start\":23907},{\"end\":24491,\"start\":24467},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28159,\"start\":28136},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30654,\"start\":30634},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30714,\"start\":30690},{\"end\":34844,\"start\":34826},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":43794,\"start\":43769}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39762,\"start\":39194},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39986,\"start\":39763},{\"attributes\":{\"id\":\"fig_2\"},\"end\":40021,\"start\":39987},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":40332,\"start\":40022},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40410,\"start\":40333},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41464,\"start\":40411},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42102,\"start\":41465},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":42677,\"start\":42103},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":43126,\"start\":42678},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43423,\"start\":43127}]", "paragraph": "[{\"end\":3436,\"start\":2453},{\"end\":3761,\"start\":3438},{\"end\":4105,\"start\":3763},{\"end\":5319,\"start\":4128},{\"end\":6770,\"start\":5321},{\"end\":7985,\"start\":6787},{\"end\":9007,\"start\":7987},{\"end\":9626,\"start\":9009},{\"end\":10275,\"start\":9628},{\"end\":11524,\"start\":10303},{\"end\":11789,\"start\":11526},{\"end\":12510,\"start\":11791},{\"end\":13352,\"start\":12530},{\"end\":13746,\"start\":13354},{\"end\":14655,\"start\":13748},{\"end\":15148,\"start\":14657},{\"end\":15902,\"start\":15168},{\"end\":16651,\"start\":15904},{\"end\":17045,\"start\":16653},{\"end\":17414,\"start\":17066},{\"end\":18859,\"start\":17416},{\"end\":19327,\"start\":18861},{\"end\":21145,\"start\":19329},{\"end\":22497,\"start\":21147},{\"end\":23086,\"start\":22513},{\"end\":23455,\"start\":23109},{\"end\":23705,\"start\":23457},{\"end\":24211,\"start\":23762},{\"end\":24955,\"start\":24213},{\"end\":25537,\"start\":24957},{\"end\":26526,\"start\":25564},{\"end\":26756,\"start\":26528},{\"end\":27630,\"start\":26758},{\"end\":28454,\"start\":27632},{\"end\":30154,\"start\":28456},{\"end\":30766,\"start\":30169},{\"end\":31533,\"start\":30803},{\"end\":32054,\"start\":31569},{\"end\":32501,\"start\":32056},{\"end\":32676,\"start\":32503},{\"end\":32798,\"start\":32678},{\"end\":33131,\"start\":32824},{\"end\":33348,\"start\":33133},{\"end\":33516,\"start\":33350},{\"end\":33606,\"start\":33518},{\"end\":34032,\"start\":33608},{\"end\":34377,\"start\":34062},{\"end\":34556,\"start\":34379},{\"end\":34667,\"start\":34558},{\"end\":35023,\"start\":34669},{\"end\":35405,\"start\":35025},{\"end\":35672,\"start\":35407},{\"end\":36177,\"start\":35697},{\"end\":36399,\"start\":36179},{\"end\":36803,\"start\":36401},{\"end\":36938,\"start\":36830},{\"end\":37003,\"start\":36940},{\"end\":37263,\"start\":37005},{\"end\":37579,\"start\":37302},{\"end\":37952,\"start\":37581},{\"end\":38355,\"start\":37954},{\"end\":38798,\"start\":38357},{\"end\":39193,\"start\":38800}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11788,\"start\":11781},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19163,\"start\":19156},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25379,\"start\":25372},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25626,\"start\":25619},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26562,\"start\":26555},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26814,\"start\":26807},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":29014,\"start\":29007},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29204,\"start\":29197},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33329,\"start\":33322},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34031,\"start\":34024}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2451,\"start\":2439},{\"end\":4112,\"start\":4108},{\"end\":4126,\"start\":4115},{\"attributes\":{\"n\":\"2\"},\"end\":6785,\"start\":6773},{\"attributes\":{\"n\":\"3\"},\"end\":10283,\"start\":10278},{\"attributes\":{\"n\":\"3.1\"},\"end\":10301,\"start\":10286},{\"attributes\":{\"n\":\"3.2\"},\"end\":12528,\"start\":12513},{\"end\":15166,\"start\":15151},{\"attributes\":{\"n\":\"3.3\"},\"end\":17064,\"start\":17048},{\"attributes\":{\"n\":\"4\"},\"end\":22511,\"start\":22500},{\"attributes\":{\"n\":\"4.1\"},\"end\":23107,\"start\":23089},{\"attributes\":{\"n\":\"4.2\"},\"end\":23726,\"start\":23708},{\"end\":23760,\"start\":23729},{\"attributes\":{\"n\":\"5\"},\"end\":25562,\"start\":25540},{\"attributes\":{\"n\":\"6\"},\"end\":30167,\"start\":30157},{\"end\":30801,\"start\":30769},{\"end\":31567,\"start\":31536},{\"end\":32822,\"start\":32801},{\"end\":34060,\"start\":34035},{\"end\":35695,\"start\":35675},{\"end\":36828,\"start\":36806},{\"end\":37300,\"start\":37266},{\"end\":39215,\"start\":39195},{\"end\":39774,\"start\":39764},{\"end\":39998,\"start\":39988},{\"end\":40032,\"start\":40023},{\"end\":40341,\"start\":40334},{\"end\":40421,\"start\":40412},{\"end\":41475,\"start\":41466},{\"end\":42113,\"start\":42104},{\"end\":42688,\"start\":42679},{\"end\":43137,\"start\":43128}]", "table": "[{\"end\":40332,\"start\":40059},{\"end\":41464,\"start\":40682},{\"end\":42102,\"start\":41645},{\"end\":42677,\"start\":42390},{\"end\":43126,\"start\":42732},{\"end\":43423,\"start\":43397}]", "figure_caption": "[{\"end\":39762,\"start\":39218},{\"end\":39986,\"start\":39776},{\"end\":40021,\"start\":40000},{\"end\":40059,\"start\":40034},{\"end\":40410,\"start\":40343},{\"end\":40682,\"start\":40423},{\"end\":41645,\"start\":41477},{\"end\":42390,\"start\":42115},{\"end\":42732,\"start\":42690},{\"end\":43397,\"start\":43139}]", "figure_ref": "[{\"end\":3577,\"start\":3569},{\"end\":3839,\"start\":3831},{\"end\":20216,\"start\":20208},{\"end\":21590,\"start\":21582},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28209,\"start\":28201},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31941,\"start\":31933}]", "bib_author_first_name": "[{\"end\":45683,\"start\":45682},{\"end\":45698,\"start\":45692},{\"end\":46317,\"start\":46311},{\"end\":46340,\"start\":46333},{\"end\":46359,\"start\":46354},{\"end\":46380,\"start\":46373},{\"end\":46382,\"start\":46381},{\"end\":46398,\"start\":46392},{\"end\":46412,\"start\":46407},{\"end\":46683,\"start\":46672},{\"end\":46697,\"start\":46691},{\"end\":46711,\"start\":46703},{\"end\":46722,\"start\":46719},{\"end\":46743,\"start\":46736},{\"end\":46761,\"start\":46753},{\"end\":47714,\"start\":47708},{\"end\":47734,\"start\":47728},{\"end\":47750,\"start\":47744},{\"end\":47767,\"start\":47759},{\"end\":47787,\"start\":47781},{\"end\":48445,\"start\":48439},{\"end\":48457,\"start\":48451},{\"end\":48472,\"start\":48467},{\"end\":48485,\"start\":48480},{\"end\":48501,\"start\":48496},{\"end\":48517,\"start\":48510},{\"end\":48987,\"start\":48979},{\"end\":49002,\"start\":48995},{\"end\":49018,\"start\":49010},{\"end\":49033,\"start\":49027},{\"end\":49049,\"start\":49046},{\"end\":49821,\"start\":49814},{\"end\":49835,\"start\":49830},{\"end\":50530,\"start\":50526},{\"end\":50544,\"start\":50540},{\"end\":50560,\"start\":50552},{\"end\":50577,\"start\":50569},{\"end\":50589,\"start\":50586},{\"end\":50602,\"start\":50597},{\"end\":50616,\"start\":50609},{\"end\":50631,\"start\":50625},{\"end\":50642,\"start\":50637},{\"end\":50657,\"start\":50651},{\"end\":50678,\"start\":50672},{\"end\":50694,\"start\":50686},{\"end\":50714,\"start\":50707},{\"end\":50730,\"start\":50724},{\"end\":50746,\"start\":50741},{\"end\":50761,\"start\":50752},{\"end\":50773,\"start\":50767},{\"end\":50775,\"start\":50774},{\"end\":50787,\"start\":50781},{\"end\":50803,\"start\":50798},{\"end\":50816,\"start\":50810},{\"end\":50828,\"start\":50824},{\"end\":50830,\"start\":50829},{\"end\":50844,\"start\":50838},{\"end\":51583,\"start\":51579},{\"end\":51600,\"start\":51593},{\"end\":51615,\"start\":51610},{\"end\":51630,\"start\":51623},{\"end\":51632,\"start\":51631},{\"end\":51647,\"start\":51641},{\"end\":51660,\"start\":51654},{\"end\":51672,\"start\":51668},{\"end\":51674,\"start\":51673},{\"end\":52040,\"start\":52037},{\"end\":52051,\"start\":52047},{\"end\":52070,\"start\":52062},{\"end\":52611,\"start\":52608},{\"end\":52624,\"start\":52618},{\"end\":52639,\"start\":52635},{\"end\":52653,\"start\":52647},{\"end\":52663,\"start\":52660},{\"end\":52678,\"start\":52670},{\"end\":53059,\"start\":53051},{\"end\":53076,\"start\":53068},{\"end\":53263,\"start\":53257},{\"end\":53287,\"start\":53283},{\"end\":53304,\"start\":53301},{\"end\":53317,\"start\":53311},{\"end\":53332,\"start\":53328},{\"end\":53334,\"start\":53333},{\"end\":54025,\"start\":54018},{\"end\":54040,\"start\":54036},{\"end\":54055,\"start\":54050},{\"end\":54078,\"start\":54071},{\"end\":54363,\"start\":54355},{\"end\":54381,\"start\":54375},{\"end\":54393,\"start\":54388},{\"end\":54406,\"start\":54399},{\"end\":54420,\"start\":54414},{\"end\":54431,\"start\":54425},{\"end\":54445,\"start\":54440},{\"end\":54458,\"start\":54452},{\"end\":54987,\"start\":54979},{\"end\":55003,\"start\":54997},{\"end\":55005,\"start\":55004},{\"end\":55019,\"start\":55012},{\"end\":55021,\"start\":55020},{\"end\":55308,\"start\":55305},{\"end\":55332,\"start\":55322},{\"end\":55349,\"start\":55343},{\"end\":55367,\"start\":55360},{\"end\":55382,\"start\":55377},{\"end\":55396,\"start\":55391},{\"end\":55414,\"start\":55406},{\"end\":55429,\"start\":55424},{\"end\":55449,\"start\":55442},{\"end\":55463,\"start\":55458},{\"end\":55478,\"start\":55472},{\"end\":55492,\"start\":55484},{\"end\":55494,\"start\":55493},{\"end\":55511,\"start\":55506},{\"end\":55527,\"start\":55519},{\"end\":55541,\"start\":55535},{\"end\":55552,\"start\":55547},{\"end\":55568,\"start\":55564},{\"end\":55577,\"start\":55573},{\"end\":56184,\"start\":56178},{\"end\":56186,\"start\":56185},{\"end\":56203,\"start\":56197},{\"end\":56216,\"start\":56211},{\"end\":56479,\"start\":56472},{\"end\":56492,\"start\":56487},{\"end\":56510,\"start\":56500},{\"end\":56524,\"start\":56519},{\"end\":56542,\"start\":56534},{\"end\":56559,\"start\":56554},{\"end\":56575,\"start\":56567},{\"end\":56589,\"start\":56585},{\"end\":56604,\"start\":56597},{\"end\":56613,\"start\":56610},{\"end\":56636,\"start\":56627},{\"end\":56650,\"start\":56645},{\"end\":57186,\"start\":57180},{\"end\":57209,\"start\":57204},{\"end\":57220,\"start\":57215},{\"end\":57975,\"start\":57968},{\"end\":57999,\"start\":57993},{\"end\":58011,\"start\":58005},{\"end\":58026,\"start\":58021},{\"end\":58042,\"start\":58035},{\"end\":58867,\"start\":58861},{\"end\":58877,\"start\":58873},{\"end\":58888,\"start\":58883},{\"end\":58903,\"start\":58896},{\"end\":58914,\"start\":58908},{\"end\":58927,\"start\":58922},{\"end\":58938,\"start\":58934},{\"end\":58949,\"start\":58945},{\"end\":58961,\"start\":58957},{\"end\":58982,\"start\":58975},{\"end\":59392,\"start\":59384},{\"end\":59403,\"start\":59401},{\"end\":59418,\"start\":59411},{\"end\":59430,\"start\":59425},{\"end\":59876,\"start\":59873},{\"end\":59890,\"start\":59884},{\"end\":59901,\"start\":59897},{\"end\":59919,\"start\":59914},{\"end\":60254,\"start\":60249},{\"end\":60266,\"start\":60260},{\"end\":60284,\"start\":60276},{\"end\":60301,\"start\":60297},{\"end\":60801,\"start\":60795},{\"end\":60814,\"start\":60810},{\"end\":60830,\"start\":60825},{\"end\":60844,\"start\":60840},{\"end\":60860,\"start\":60855},{\"end\":60874,\"start\":60868},{\"end\":60876,\"start\":60875},{\"end\":61424,\"start\":61419},{\"end\":61435,\"start\":61430},{\"end\":61451,\"start\":61446},{\"end\":61464,\"start\":61459},{\"end\":61478,\"start\":61473},{\"end\":61492,\"start\":61487},{\"end\":61994,\"start\":61986},{\"end\":62005,\"start\":62001},{\"end\":62583,\"start\":62576},{\"end\":62595,\"start\":62590},{\"end\":62607,\"start\":62601},{\"end\":62618,\"start\":62614},{\"end\":62640,\"start\":62632},{\"end\":62888,\"start\":62883},{\"end\":62901,\"start\":62898},{\"end\":62924,\"start\":62915},{\"end\":62940,\"start\":62933},{\"end\":62953,\"start\":62948},{\"end\":62970,\"start\":62963},{\"end\":62984,\"start\":62975},{\"end\":63960,\"start\":63955},{\"end\":63980,\"start\":63970},{\"end\":63995,\"start\":63989},{\"end\":64008,\"start\":64001},{\"end\":64021,\"start\":64016},{\"end\":64037,\"start\":64031},{\"end\":64040,\"start\":64038},{\"end\":64051,\"start\":64046},{\"end\":64066,\"start\":64060},{\"end\":64084,\"start\":64076},{\"end\":64100,\"start\":64097},{\"end\":64123,\"start\":64114},{\"end\":64884,\"start\":64880},{\"end\":64898,\"start\":64893},{\"end\":64920,\"start\":64911},{\"end\":64935,\"start\":64929},{\"end\":64954,\"start\":64946},{\"end\":65682,\"start\":65677},{\"end\":65695,\"start\":65691},{\"end\":65709,\"start\":65705},{\"end\":65728,\"start\":65719},{\"end\":65740,\"start\":65734},{\"end\":65756,\"start\":65749},{\"end\":65770,\"start\":65765},{\"end\":65780,\"start\":65777},{\"end\":65790,\"start\":65785},{\"end\":65792,\"start\":65791},{\"end\":66260,\"start\":66256},{\"end\":66275,\"start\":66269},{\"end\":66297,\"start\":66289},{\"end\":66313,\"start\":66306},{\"end\":66863,\"start\":66856},{\"end\":66875,\"start\":66869},{\"end\":66890,\"start\":66885},{\"end\":66902,\"start\":66897},{\"end\":67406,\"start\":67400},{\"end\":67423,\"start\":67420},{\"end\":67442,\"start\":67434},{\"end\":67458,\"start\":67451},{\"end\":67473,\"start\":67465},{\"end\":67490,\"start\":67486},{\"end\":67492,\"start\":67491},{\"end\":67505,\"start\":67500},{\"end\":68043,\"start\":68039},{\"end\":68060,\"start\":68052},{\"end\":68077,\"start\":68069},{\"end\":68094,\"start\":68086},{\"end\":68768,\"start\":68763},{\"end\":68784,\"start\":68777},{\"end\":68802,\"start\":68794},{\"end\":68828,\"start\":68823},{\"end\":69492,\"start\":69485},{\"end\":69511,\"start\":69502},{\"end\":69941,\"start\":69939},{\"end\":69952,\"start\":69948},{\"end\":69968,\"start\":69962},{\"end\":70858,\"start\":70851},{\"end\":70869,\"start\":70865},{\"end\":71297,\"start\":71291},{\"end\":71312,\"start\":71304},{\"end\":71326,\"start\":71320},{\"end\":71339,\"start\":71333},{\"end\":71357,\"start\":71350},{\"end\":71375,\"start\":71368},{\"end\":71388,\"start\":71381},{\"end\":71400,\"start\":71397},{\"end\":71412,\"start\":71408},{\"end\":71425,\"start\":71419},{\"end\":71440,\"start\":71437},{\"end\":71453,\"start\":71450},{\"end\":71469,\"start\":71464},{\"end\":71496,\"start\":71490},{\"end\":71507,\"start\":71501},{\"end\":71523,\"start\":71517},{\"end\":71534,\"start\":71529},{\"end\":71537,\"start\":71535},{\"end\":71549,\"start\":71542},{\"end\":71563,\"start\":71556},{\"end\":71579,\"start\":71572},{\"end\":71596,\"start\":71587},{\"end\":72637,\"start\":72632},{\"end\":72654,\"start\":72647},{\"end\":72664,\"start\":72661},{\"end\":72680,\"start\":72675},{\"end\":73132,\"start\":73127},{\"end\":73145,\"start\":73142},{\"end\":73163,\"start\":73156},{\"end\":73173,\"start\":73170},{\"end\":73188,\"start\":73183}]", "bib_author_last_name": "[{\"end\":45690,\"start\":45684},{\"end\":45705,\"start\":45699},{\"end\":45711,\"start\":45707},{\"end\":46331,\"start\":46318},{\"end\":46352,\"start\":46341},{\"end\":46371,\"start\":46360},{\"end\":46390,\"start\":46383},{\"end\":46405,\"start\":46399},{\"end\":46422,\"start\":46413},{\"end\":46428,\"start\":46424},{\"end\":46689,\"start\":46684},{\"end\":46701,\"start\":46698},{\"end\":46717,\"start\":46712},{\"end\":46734,\"start\":46723},{\"end\":46751,\"start\":46744},{\"end\":46771,\"start\":46762},{\"end\":47726,\"start\":47715},{\"end\":47742,\"start\":47735},{\"end\":47757,\"start\":47751},{\"end\":47779,\"start\":47768},{\"end\":47799,\"start\":47788},{\"end\":48449,\"start\":48446},{\"end\":48465,\"start\":48458},{\"end\":48478,\"start\":48473},{\"end\":48494,\"start\":48486},{\"end\":48508,\"start\":48502},{\"end\":48522,\"start\":48518},{\"end\":48993,\"start\":48988},{\"end\":49008,\"start\":49003},{\"end\":49025,\"start\":49019},{\"end\":49044,\"start\":49034},{\"end\":49054,\"start\":49050},{\"end\":49067,\"start\":49056},{\"end\":49828,\"start\":49822},{\"end\":49840,\"start\":49836},{\"end\":50538,\"start\":50531},{\"end\":50550,\"start\":50545},{\"end\":50567,\"start\":50561},{\"end\":50584,\"start\":50578},{\"end\":50595,\"start\":50590},{\"end\":50607,\"start\":50603},{\"end\":50623,\"start\":50617},{\"end\":50635,\"start\":50632},{\"end\":50649,\"start\":50643},{\"end\":50670,\"start\":50658},{\"end\":50684,\"start\":50679},{\"end\":50705,\"start\":50695},{\"end\":50722,\"start\":50715},{\"end\":50739,\"start\":50731},{\"end\":50750,\"start\":50747},{\"end\":50765,\"start\":50762},{\"end\":50779,\"start\":50776},{\"end\":50796,\"start\":50788},{\"end\":50808,\"start\":50804},{\"end\":50822,\"start\":50817},{\"end\":50836,\"start\":50831},{\"end\":50856,\"start\":50845},{\"end\":51591,\"start\":51584},{\"end\":51608,\"start\":51601},{\"end\":51621,\"start\":51616},{\"end\":51639,\"start\":51633},{\"end\":51652,\"start\":51648},{\"end\":51666,\"start\":51661},{\"end\":51680,\"start\":51675},{\"end\":52045,\"start\":52041},{\"end\":52060,\"start\":52052},{\"end\":52077,\"start\":52071},{\"end\":52616,\"start\":52612},{\"end\":52633,\"start\":52625},{\"end\":52645,\"start\":52640},{\"end\":52658,\"start\":52654},{\"end\":52668,\"start\":52664},{\"end\":52685,\"start\":52679},{\"end\":53066,\"start\":53060},{\"end\":53086,\"start\":53077},{\"end\":53281,\"start\":53264},{\"end\":53299,\"start\":53288},{\"end\":53309,\"start\":53305},{\"end\":53326,\"start\":53318},{\"end\":53341,\"start\":53335},{\"end\":53348,\"start\":53343},{\"end\":54034,\"start\":54026},{\"end\":54048,\"start\":54041},{\"end\":54069,\"start\":54056},{\"end\":54083,\"start\":54079},{\"end\":54373,\"start\":54364},{\"end\":54386,\"start\":54382},{\"end\":54397,\"start\":54394},{\"end\":54412,\"start\":54407},{\"end\":54423,\"start\":54421},{\"end\":54438,\"start\":54432},{\"end\":54450,\"start\":54446},{\"end\":54462,\"start\":54459},{\"end\":54995,\"start\":54988},{\"end\":55010,\"start\":55006},{\"end\":55028,\"start\":55022},{\"end\":55320,\"start\":55309},{\"end\":55341,\"start\":55333},{\"end\":55358,\"start\":55350},{\"end\":55375,\"start\":55368},{\"end\":55389,\"start\":55383},{\"end\":55404,\"start\":55397},{\"end\":55422,\"start\":55415},{\"end\":55440,\"start\":55430},{\"end\":55456,\"start\":55450},{\"end\":55470,\"start\":55464},{\"end\":55482,\"start\":55479},{\"end\":55504,\"start\":55495},{\"end\":55517,\"start\":55512},{\"end\":55533,\"start\":55528},{\"end\":55545,\"start\":55542},{\"end\":55562,\"start\":55553},{\"end\":55571,\"start\":55569},{\"end\":55584,\"start\":55578},{\"end\":56195,\"start\":56187},{\"end\":56209,\"start\":56204},{\"end\":56228,\"start\":56217},{\"end\":56485,\"start\":56480},{\"end\":56498,\"start\":56493},{\"end\":56517,\"start\":56511},{\"end\":56532,\"start\":56525},{\"end\":56552,\"start\":56543},{\"end\":56565,\"start\":56560},{\"end\":56583,\"start\":56576},{\"end\":56595,\"start\":56590},{\"end\":56608,\"start\":56605},{\"end\":56625,\"start\":56614},{\"end\":56643,\"start\":56637},{\"end\":56656,\"start\":56651},{\"end\":57202,\"start\":57187},{\"end\":57213,\"start\":57210},{\"end\":57227,\"start\":57221},{\"end\":57232,\"start\":57229},{\"end\":57991,\"start\":57976},{\"end\":58003,\"start\":58000},{\"end\":58019,\"start\":58012},{\"end\":58033,\"start\":58027},{\"end\":58046,\"start\":58043},{\"end\":58053,\"start\":58048},{\"end\":58871,\"start\":58868},{\"end\":58881,\"start\":58878},{\"end\":58894,\"start\":58889},{\"end\":58906,\"start\":58904},{\"end\":58920,\"start\":58915},{\"end\":58932,\"start\":58928},{\"end\":58943,\"start\":58939},{\"end\":58955,\"start\":58950},{\"end\":58973,\"start\":58962},{\"end\":58991,\"start\":58983},{\"end\":59399,\"start\":59393},{\"end\":59409,\"start\":59404},{\"end\":59423,\"start\":59419},{\"end\":59442,\"start\":59431},{\"end\":59448,\"start\":59444},{\"end\":59882,\"start\":59877},{\"end\":59895,\"start\":59891},{\"end\":59912,\"start\":59902},{\"end\":59928,\"start\":59920},{\"end\":60258,\"start\":60255},{\"end\":60274,\"start\":60267},{\"end\":60295,\"start\":60285},{\"end\":60313,\"start\":60302},{\"end\":60808,\"start\":60802},{\"end\":60823,\"start\":60815},{\"end\":60838,\"start\":60831},{\"end\":60853,\"start\":60845},{\"end\":60866,\"start\":60861},{\"end\":60883,\"start\":60877},{\"end\":61428,\"start\":61425},{\"end\":61444,\"start\":61436},{\"end\":61457,\"start\":61452},{\"end\":61471,\"start\":61465},{\"end\":61485,\"start\":61479},{\"end\":61498,\"start\":61493},{\"end\":61999,\"start\":61995},{\"end\":62013,\"start\":62006},{\"end\":62588,\"start\":62584},{\"end\":62599,\"start\":62596},{\"end\":62612,\"start\":62608},{\"end\":62630,\"start\":62619},{\"end\":62651,\"start\":62641},{\"end\":62896,\"start\":62889},{\"end\":62913,\"start\":62902},{\"end\":62931,\"start\":62925},{\"end\":62946,\"start\":62941},{\"end\":62961,\"start\":62954},{\"end\":62973,\"start\":62971},{\"end\":62991,\"start\":62985},{\"end\":63968,\"start\":63961},{\"end\":63987,\"start\":63981},{\"end\":63999,\"start\":63996},{\"end\":64014,\"start\":64009},{\"end\":64029,\"start\":64022},{\"end\":64044,\"start\":64041},{\"end\":64058,\"start\":64052},{\"end\":64074,\"start\":64067},{\"end\":64095,\"start\":64085},{\"end\":64112,\"start\":64101},{\"end\":64130,\"start\":64124},{\"end\":64891,\"start\":64885},{\"end\":64909,\"start\":64899},{\"end\":64927,\"start\":64921},{\"end\":64944,\"start\":64936},{\"end\":64964,\"start\":64955},{\"end\":65689,\"start\":65683},{\"end\":65703,\"start\":65696},{\"end\":65717,\"start\":65710},{\"end\":65732,\"start\":65729},{\"end\":65747,\"start\":65741},{\"end\":65763,\"start\":65757},{\"end\":65775,\"start\":65771},{\"end\":65783,\"start\":65781},{\"end\":65796,\"start\":65793},{\"end\":66267,\"start\":66261},{\"end\":66287,\"start\":66276},{\"end\":66304,\"start\":66298},{\"end\":66316,\"start\":66314},{\"end\":66867,\"start\":66864},{\"end\":66883,\"start\":66876},{\"end\":66895,\"start\":66891},{\"end\":66916,\"start\":66903},{\"end\":66922,\"start\":66918},{\"end\":67418,\"start\":67407},{\"end\":67432,\"start\":67424},{\"end\":67449,\"start\":67443},{\"end\":67463,\"start\":67459},{\"end\":67484,\"start\":67474},{\"end\":67498,\"start\":67493},{\"end\":67510,\"start\":67506},{\"end\":68050,\"start\":68044},{\"end\":68067,\"start\":68061},{\"end\":68084,\"start\":68078},{\"end\":68101,\"start\":68095},{\"end\":68775,\"start\":68769},{\"end\":68792,\"start\":68785},{\"end\":68821,\"start\":68803},{\"end\":68835,\"start\":68829},{\"end\":69500,\"start\":69493},{\"end\":69518,\"start\":69512},{\"end\":69946,\"start\":69942},{\"end\":69960,\"start\":69953},{\"end\":69972,\"start\":69969},{\"end\":70863,\"start\":70859},{\"end\":71302,\"start\":71298},{\"end\":71318,\"start\":71313},{\"end\":71331,\"start\":71327},{\"end\":71348,\"start\":71340},{\"end\":71366,\"start\":71358},{\"end\":71379,\"start\":71376},{\"end\":71395,\"start\":71389},{\"end\":71406,\"start\":71401},{\"end\":71417,\"start\":71413},{\"end\":71435,\"start\":71426},{\"end\":71448,\"start\":71441},{\"end\":71462,\"start\":71454},{\"end\":71488,\"start\":71470},{\"end\":71499,\"start\":71497},{\"end\":71515,\"start\":71508},{\"end\":71527,\"start\":71524},{\"end\":71540,\"start\":71538},{\"end\":71554,\"start\":71550},{\"end\":71570,\"start\":71564},{\"end\":71585,\"start\":71580},{\"end\":71603,\"start\":71597},{\"end\":71609,\"start\":71605},{\"end\":72645,\"start\":72638},{\"end\":72659,\"start\":72655},{\"end\":72673,\"start\":72665},{\"end\":72685,\"start\":72681},{\"end\":73140,\"start\":73133},{\"end\":73154,\"start\":73146},{\"end\":73168,\"start\":73164},{\"end\":73181,\"start\":73174},{\"end\":73193,\"start\":73189}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":233033916},\"end\":46270,\"start\":45608},{\"attributes\":{\"id\":\"b1\"},\"end\":46598,\"start\":46272},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1300\",\"id\":\"b2\",\"matched_paper_id\":165163607},\"end\":47651,\"start\":46600},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":233210218},\"end\":48401,\"start\":47653},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":196170479},\"end\":48910,\"start\":48403},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.400\",\"id\":\"b5\",\"matched_paper_id\":215768768},\"end\":49746,\"start\":48912},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1025\",\"id\":\"b6\",\"matched_paper_id\":10543068},\"end\":50460,\"start\":49748},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":222124366},\"end\":51504,\"start\":50462},{\"attributes\":{\"doi\":\"abs/2104.08646\",\"id\":\"b8\",\"matched_paper_id\":233296459},\"end\":51913,\"start\":51506},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":201124736},\"end\":52511,\"start\":51915},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":230799347},\"end\":53007,\"start\":52513},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":16567195},\"end\":53198,\"start\":53009},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4537113},\"end\":53950,\"start\":53200},{\"attributes\":{\"doi\":\"10.5281/zenodo.1212303\",\"id\":\"b13\"},\"end\":54293,\"start\":53952},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":215737187},\"end\":54891,\"start\":54295},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":203591519},\"end\":55303,\"start\":54893},{\"attributes\":{\"id\":\"b16\"},\"end\":56145,\"start\":55305},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15710851},\"end\":56470,\"start\":56147},{\"attributes\":{\"id\":\"b18\"},\"end\":57071,\"start\":56472},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.557\",\"id\":\"b19\",\"matched_paper_id\":218486812},\"end\":57917,\"start\":57073},{\"attributes\":{\"doi\":\"10.18653/v1/2021.naacl-main.366\",\"id\":\"b20\",\"matched_paper_id\":225075843},\"end\":58857,\"start\":57919},{\"attributes\":{\"doi\":\"abs/1907.11692\",\"id\":\"b21\"},\"end\":59292,\"start\":58859},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232335877},\"end\":59789,\"start\":59294},{\"attributes\":{\"doi\":\"10.3758/BF03192726\",\"id\":\"b23\",\"matched_paper_id\":12526452},\"end\":60195,\"start\":59791},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":216056269},\"end\":60689,\"start\":60197},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":235294252},\"end\":61348,\"start\":60691},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":207756753},\"end\":61919,\"start\":61350},{\"attributes\":{\"doi\":\"10.18653/v1/2020.findings-emnlp.54\",\"id\":\"b27\",\"matched_paper_id\":218470189},\"end\":62513,\"start\":61921},{\"attributes\":{\"doi\":\"abs/2107.02153\",\"id\":\"b28\",\"matched_paper_id\":235731930},\"end\":62844,\"start\":62515},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1250\",\"id\":\"b29\",\"matched_paper_id\":202539551},\"end\":63895,\"start\":62846},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221507798},\"end\":64821,\"start\":63897},{\"attributes\":{\"doi\":\"10.18653/v1/S18-2023\",\"id\":\"b31\",\"matched_paper_id\":21382535},\"end\":65592,\"start\":64823},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":204838007},\"end\":66151,\"start\":65594},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":221191193},\"end\":66793,\"start\":66153},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":128296356},\"end\":67321,\"start\":66795},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":221856637},\"end\":67958,\"start\":67323},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":53296520},\"end\":68694,\"start\":67960},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4711425},\"end\":69426,\"start\":68696},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1669264},\"end\":69876,\"start\":69428},{\"attributes\":{\"doi\":\"10.18653/v1/N18-2049\",\"id\":\"b39\",\"matched_paper_id\":4731156},\"end\":70773,\"start\":69878},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":10326133},\"end\":71229,\"start\":70775},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-demos.6\",\"id\":\"b41\",\"matched_paper_id\":208117506},\"end\":72554,\"start\":71231},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":52019251},\"end\":73070,\"start\":72556},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":159041722},\"end\":73582,\"start\":73072}]", "bib_title": "[{\"end\":45680,\"start\":45608},{\"end\":46670,\"start\":46600},{\"end\":47706,\"start\":47653},{\"end\":48437,\"start\":48403},{\"end\":48977,\"start\":48912},{\"end\":49812,\"start\":49748},{\"end\":50524,\"start\":50462},{\"end\":51577,\"start\":51506},{\"end\":52035,\"start\":51915},{\"end\":52606,\"start\":52513},{\"end\":53049,\"start\":53009},{\"end\":53255,\"start\":53200},{\"end\":54353,\"start\":54295},{\"end\":54977,\"start\":54893},{\"end\":56176,\"start\":56147},{\"end\":57178,\"start\":57073},{\"end\":57966,\"start\":57919},{\"end\":59382,\"start\":59294},{\"end\":59871,\"start\":59791},{\"end\":60247,\"start\":60197},{\"end\":60793,\"start\":60691},{\"end\":61417,\"start\":61350},{\"end\":61984,\"start\":61921},{\"end\":62574,\"start\":62515},{\"end\":62881,\"start\":62846},{\"end\":63953,\"start\":63897},{\"end\":64878,\"start\":64823},{\"end\":65675,\"start\":65594},{\"end\":66254,\"start\":66153},{\"end\":66854,\"start\":66795},{\"end\":67398,\"start\":67323},{\"end\":68037,\"start\":67960},{\"end\":68761,\"start\":68696},{\"end\":69483,\"start\":69428},{\"end\":69937,\"start\":69878},{\"end\":70849,\"start\":70775},{\"end\":71289,\"start\":71231},{\"end\":72630,\"start\":72556},{\"end\":73125,\"start\":73072}]", "bib_author": "[{\"end\":45692,\"start\":45682},{\"end\":45707,\"start\":45692},{\"end\":45713,\"start\":45707},{\"end\":46333,\"start\":46311},{\"end\":46354,\"start\":46333},{\"end\":46373,\"start\":46354},{\"end\":46392,\"start\":46373},{\"end\":46407,\"start\":46392},{\"end\":46424,\"start\":46407},{\"end\":46430,\"start\":46424},{\"end\":46691,\"start\":46672},{\"end\":46703,\"start\":46691},{\"end\":46719,\"start\":46703},{\"end\":46736,\"start\":46719},{\"end\":46753,\"start\":46736},{\"end\":46773,\"start\":46753},{\"end\":47728,\"start\":47708},{\"end\":47744,\"start\":47728},{\"end\":47759,\"start\":47744},{\"end\":47781,\"start\":47759},{\"end\":47801,\"start\":47781},{\"end\":48451,\"start\":48439},{\"end\":48467,\"start\":48451},{\"end\":48480,\"start\":48467},{\"end\":48496,\"start\":48480},{\"end\":48510,\"start\":48496},{\"end\":48524,\"start\":48510},{\"end\":48995,\"start\":48979},{\"end\":49010,\"start\":48995},{\"end\":49027,\"start\":49010},{\"end\":49046,\"start\":49027},{\"end\":49056,\"start\":49046},{\"end\":49069,\"start\":49056},{\"end\":49830,\"start\":49814},{\"end\":49842,\"start\":49830},{\"end\":50540,\"start\":50526},{\"end\":50552,\"start\":50540},{\"end\":50569,\"start\":50552},{\"end\":50586,\"start\":50569},{\"end\":50597,\"start\":50586},{\"end\":50609,\"start\":50597},{\"end\":50625,\"start\":50609},{\"end\":50637,\"start\":50625},{\"end\":50651,\"start\":50637},{\"end\":50672,\"start\":50651},{\"end\":50686,\"start\":50672},{\"end\":50707,\"start\":50686},{\"end\":50724,\"start\":50707},{\"end\":50741,\"start\":50724},{\"end\":50752,\"start\":50741},{\"end\":50767,\"start\":50752},{\"end\":50781,\"start\":50767},{\"end\":50798,\"start\":50781},{\"end\":50810,\"start\":50798},{\"end\":50824,\"start\":50810},{\"end\":50838,\"start\":50824},{\"end\":50858,\"start\":50838},{\"end\":51593,\"start\":51579},{\"end\":51610,\"start\":51593},{\"end\":51623,\"start\":51610},{\"end\":51641,\"start\":51623},{\"end\":51654,\"start\":51641},{\"end\":51668,\"start\":51654},{\"end\":51682,\"start\":51668},{\"end\":52047,\"start\":52037},{\"end\":52062,\"start\":52047},{\"end\":52079,\"start\":52062},{\"end\":52618,\"start\":52608},{\"end\":52635,\"start\":52618},{\"end\":52647,\"start\":52635},{\"end\":52660,\"start\":52647},{\"end\":52670,\"start\":52660},{\"end\":52687,\"start\":52670},{\"end\":53068,\"start\":53051},{\"end\":53088,\"start\":53068},{\"end\":53283,\"start\":53257},{\"end\":53301,\"start\":53283},{\"end\":53311,\"start\":53301},{\"end\":53328,\"start\":53311},{\"end\":53343,\"start\":53328},{\"end\":53350,\"start\":53343},{\"end\":54036,\"start\":54018},{\"end\":54050,\"start\":54036},{\"end\":54071,\"start\":54050},{\"end\":54085,\"start\":54071},{\"end\":54375,\"start\":54355},{\"end\":54388,\"start\":54375},{\"end\":54399,\"start\":54388},{\"end\":54414,\"start\":54399},{\"end\":54425,\"start\":54414},{\"end\":54440,\"start\":54425},{\"end\":54452,\"start\":54440},{\"end\":54464,\"start\":54452},{\"end\":54997,\"start\":54979},{\"end\":55012,\"start\":54997},{\"end\":55030,\"start\":55012},{\"end\":55322,\"start\":55305},{\"end\":55343,\"start\":55322},{\"end\":55360,\"start\":55343},{\"end\":55377,\"start\":55360},{\"end\":55391,\"start\":55377},{\"end\":55406,\"start\":55391},{\"end\":55424,\"start\":55406},{\"end\":55442,\"start\":55424},{\"end\":55458,\"start\":55442},{\"end\":55472,\"start\":55458},{\"end\":55484,\"start\":55472},{\"end\":55506,\"start\":55484},{\"end\":55519,\"start\":55506},{\"end\":55535,\"start\":55519},{\"end\":55547,\"start\":55535},{\"end\":55564,\"start\":55547},{\"end\":55573,\"start\":55564},{\"end\":55586,\"start\":55573},{\"end\":56197,\"start\":56178},{\"end\":56211,\"start\":56197},{\"end\":56230,\"start\":56211},{\"end\":56487,\"start\":56472},{\"end\":56500,\"start\":56487},{\"end\":56519,\"start\":56500},{\"end\":56534,\"start\":56519},{\"end\":56554,\"start\":56534},{\"end\":56567,\"start\":56554},{\"end\":56585,\"start\":56567},{\"end\":56597,\"start\":56585},{\"end\":56610,\"start\":56597},{\"end\":56627,\"start\":56610},{\"end\":56645,\"start\":56627},{\"end\":56658,\"start\":56645},{\"end\":57204,\"start\":57180},{\"end\":57215,\"start\":57204},{\"end\":57229,\"start\":57215},{\"end\":57234,\"start\":57229},{\"end\":57993,\"start\":57968},{\"end\":58005,\"start\":57993},{\"end\":58021,\"start\":58005},{\"end\":58035,\"start\":58021},{\"end\":58048,\"start\":58035},{\"end\":58055,\"start\":58048},{\"end\":58873,\"start\":58861},{\"end\":58883,\"start\":58873},{\"end\":58896,\"start\":58883},{\"end\":58908,\"start\":58896},{\"end\":58922,\"start\":58908},{\"end\":58934,\"start\":58922},{\"end\":58945,\"start\":58934},{\"end\":58957,\"start\":58945},{\"end\":58975,\"start\":58957},{\"end\":58993,\"start\":58975},{\"end\":59401,\"start\":59384},{\"end\":59411,\"start\":59401},{\"end\":59425,\"start\":59411},{\"end\":59444,\"start\":59425},{\"end\":59450,\"start\":59444},{\"end\":59884,\"start\":59873},{\"end\":59897,\"start\":59884},{\"end\":59914,\"start\":59897},{\"end\":59930,\"start\":59914},{\"end\":60260,\"start\":60249},{\"end\":60276,\"start\":60260},{\"end\":60297,\"start\":60276},{\"end\":60315,\"start\":60297},{\"end\":60810,\"start\":60795},{\"end\":60825,\"start\":60810},{\"end\":60840,\"start\":60825},{\"end\":60855,\"start\":60840},{\"end\":60868,\"start\":60855},{\"end\":60885,\"start\":60868},{\"end\":61430,\"start\":61419},{\"end\":61446,\"start\":61430},{\"end\":61459,\"start\":61446},{\"end\":61473,\"start\":61459},{\"end\":61487,\"start\":61473},{\"end\":61500,\"start\":61487},{\"end\":62001,\"start\":61986},{\"end\":62015,\"start\":62001},{\"end\":62590,\"start\":62576},{\"end\":62601,\"start\":62590},{\"end\":62614,\"start\":62601},{\"end\":62632,\"start\":62614},{\"end\":62653,\"start\":62632},{\"end\":62898,\"start\":62883},{\"end\":62915,\"start\":62898},{\"end\":62933,\"start\":62915},{\"end\":62948,\"start\":62933},{\"end\":62963,\"start\":62948},{\"end\":62975,\"start\":62963},{\"end\":62993,\"start\":62975},{\"end\":63970,\"start\":63955},{\"end\":63989,\"start\":63970},{\"end\":64001,\"start\":63989},{\"end\":64016,\"start\":64001},{\"end\":64031,\"start\":64016},{\"end\":64046,\"start\":64031},{\"end\":64060,\"start\":64046},{\"end\":64076,\"start\":64060},{\"end\":64097,\"start\":64076},{\"end\":64114,\"start\":64097},{\"end\":64132,\"start\":64114},{\"end\":64893,\"start\":64880},{\"end\":64911,\"start\":64893},{\"end\":64929,\"start\":64911},{\"end\":64946,\"start\":64929},{\"end\":64966,\"start\":64946},{\"end\":65691,\"start\":65677},{\"end\":65705,\"start\":65691},{\"end\":65719,\"start\":65705},{\"end\":65734,\"start\":65719},{\"end\":65749,\"start\":65734},{\"end\":65765,\"start\":65749},{\"end\":65777,\"start\":65765},{\"end\":65785,\"start\":65777},{\"end\":65798,\"start\":65785},{\"end\":66269,\"start\":66256},{\"end\":66289,\"start\":66269},{\"end\":66306,\"start\":66289},{\"end\":66318,\"start\":66306},{\"end\":66869,\"start\":66856},{\"end\":66885,\"start\":66869},{\"end\":66897,\"start\":66885},{\"end\":66918,\"start\":66897},{\"end\":66924,\"start\":66918},{\"end\":67420,\"start\":67400},{\"end\":67434,\"start\":67420},{\"end\":67451,\"start\":67434},{\"end\":67465,\"start\":67451},{\"end\":67486,\"start\":67465},{\"end\":67500,\"start\":67486},{\"end\":67512,\"start\":67500},{\"end\":68052,\"start\":68039},{\"end\":68069,\"start\":68052},{\"end\":68086,\"start\":68069},{\"end\":68103,\"start\":68086},{\"end\":68777,\"start\":68763},{\"end\":68794,\"start\":68777},{\"end\":68823,\"start\":68794},{\"end\":68837,\"start\":68823},{\"end\":69502,\"start\":69485},{\"end\":69520,\"start\":69502},{\"end\":69948,\"start\":69939},{\"end\":69962,\"start\":69948},{\"end\":69974,\"start\":69962},{\"end\":70865,\"start\":70851},{\"end\":70872,\"start\":70865},{\"end\":71304,\"start\":71291},{\"end\":71320,\"start\":71304},{\"end\":71333,\"start\":71320},{\"end\":71350,\"start\":71333},{\"end\":71368,\"start\":71350},{\"end\":71381,\"start\":71368},{\"end\":71397,\"start\":71381},{\"end\":71408,\"start\":71397},{\"end\":71419,\"start\":71408},{\"end\":71437,\"start\":71419},{\"end\":71450,\"start\":71437},{\"end\":71464,\"start\":71450},{\"end\":71490,\"start\":71464},{\"end\":71501,\"start\":71490},{\"end\":71517,\"start\":71501},{\"end\":71529,\"start\":71517},{\"end\":71542,\"start\":71529},{\"end\":71556,\"start\":71542},{\"end\":71572,\"start\":71556},{\"end\":71587,\"start\":71572},{\"end\":71605,\"start\":71587},{\"end\":71611,\"start\":71605},{\"end\":72647,\"start\":72632},{\"end\":72661,\"start\":72647},{\"end\":72675,\"start\":72661},{\"end\":72687,\"start\":72675},{\"end\":73142,\"start\":73127},{\"end\":73156,\"start\":73142},{\"end\":73170,\"start\":73156},{\"end\":73183,\"start\":73170},{\"end\":73195,\"start\":73183}]", "bib_venue": "[{\"end\":45862,\"start\":45713},{\"end\":46309,\"start\":46272},{\"end\":46935,\"start\":46793},{\"end\":47950,\"start\":47801},{\"end\":48612,\"start\":48524},{\"end\":49194,\"start\":49100},{\"end\":49949,\"start\":49862},{\"end\":50922,\"start\":50858},{\"end\":51701,\"start\":51696},{\"end\":52168,\"start\":52079},{\"end\":52748,\"start\":52687},{\"end\":53096,\"start\":53088},{\"end\":53499,\"start\":53350},{\"end\":54016,\"start\":53952},{\"end\":54545,\"start\":54464},{\"end\":55089,\"start\":55030},{\"end\":55718,\"start\":55586},{\"end\":56300,\"start\":56230},{\"end\":56732,\"start\":56658},{\"end\":57359,\"start\":57265},{\"end\":58228,\"start\":58086},{\"end\":59069,\"start\":59007},{\"end\":59511,\"start\":59450},{\"end\":59973,\"start\":59948},{\"end\":60396,\"start\":60315},{\"end\":60973,\"start\":60885},{\"end\":61588,\"start\":61500},{\"end\":62118,\"start\":62049},{\"end\":62672,\"start\":62667},{\"end\":63188,\"start\":63013},{\"end\":64281,\"start\":64132},{\"end\":65068,\"start\":64986},{\"end\":65834,\"start\":65798},{\"end\":66414,\"start\":66318},{\"end\":67013,\"start\":66924},{\"end\":67593,\"start\":67512},{\"end\":68252,\"start\":68103},{\"end\":68986,\"start\":68837},{\"end\":69608,\"start\":69520},{\"end\":70136,\"start\":69994},{\"end\":70960,\"start\":70872},{\"end\":71750,\"start\":71641},{\"end\":72768,\"start\":72687},{\"end\":73283,\"start\":73195},{\"end\":45998,\"start\":45864},{\"end\":47086,\"start\":46937},{\"end\":48086,\"start\":47952},{\"end\":48687,\"start\":48614},{\"end\":49275,\"start\":49196},{\"end\":50040,\"start\":49951},{\"end\":52244,\"start\":52170},{\"end\":53635,\"start\":53501},{\"end\":54613,\"start\":54547},{\"end\":56793,\"start\":56734},{\"end\":57440,\"start\":57361},{\"end\":58363,\"start\":58230},{\"end\":59559,\"start\":59513},{\"end\":60464,\"start\":60398},{\"end\":61048,\"start\":60975},{\"end\":61663,\"start\":61590},{\"end\":62126,\"start\":62120},{\"end\":63366,\"start\":63190},{\"end\":64417,\"start\":64283},{\"end\":65159,\"start\":65070},{\"end\":66497,\"start\":66416},{\"end\":67089,\"start\":67015},{\"end\":67661,\"start\":67595},{\"end\":68388,\"start\":68254},{\"end\":69122,\"start\":68988},{\"end\":69683,\"start\":69610},{\"end\":70287,\"start\":70138},{\"end\":71035,\"start\":70962},{\"end\":71852,\"start\":71752},{\"end\":72836,\"start\":72770},{\"end\":73358,\"start\":73285}]"}}}, "year": 2023, "month": 12, "day": 17}