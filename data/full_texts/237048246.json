{"id": 237048246, "updated": "2023-11-08 02:37:01.358", "metadata": {"title": "Evaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks", "authors": "[{\"first\":\"Federico\",\"last\":\"Nesti\",\"middle\":[]},{\"first\":\"Giulio\",\"last\":\"Rossolini\",\"middle\":[]},{\"first\":\"Saasha\",\"last\":\"Nair\",\"middle\":[]},{\"first\":\"Alessandro\",\"last\":\"Biondi\",\"middle\":[]},{\"first\":\"Giorgio\",\"last\":\"Buttazzo\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 8, "day": 13}, "abstract": "Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2108.06179", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/wacv/NestiRNBB22", "doi": "10.1109/wacv51458.2022.00288"}}, "content": {"source": {"pdf_hash": "fc8a115c92896d0aaa5a84e6fc776630af7ade22", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2108.06179v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "68a8d443fefef78673fe0331fe7d06f215468195", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fc8a115c92896d0aaa5a84e6fc776630af7ade22.txt", "contents": "\nEvaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks *\n\n\nFederico Nesti \nDepartment of Excellence in Robotics & AI\nScuola Superiore Sant'Anna\n\n\nGiulio Rossolini \nDepartment of Excellence in Robotics & AI\nScuola Superiore Sant'Anna\n\n\nSaasha Nair \nDepartment of Excellence in Robotics & AI\nScuola Superiore Sant'Anna\n\n\nAlessandro Biondi \nDepartment of Excellence in Robotics & AI\nScuola Superiore Sant'Anna\n\n\nGiorgio Buttazzo \nDepartment of Excellence in Robotics & AI\nScuola Superiore Sant'Anna\n\n\nEvaluating the Robustness of Semantic Segmentation for Autonomous Driving against Real-World Adversarial Patch Attacks *\n\nDeep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a realworld scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scenespecific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.\n\nIntroduction\n\nThe rise of deep learning unlocked unprecedented performance in several scientific areas [24]. Convolutional (a) (b) (c) (d) (e) (f) Figure 1: Proposed adversarial patches on Cityscapes [5] (b) and CARLA Simulator [6] (e); (c/f) show the corresponding SS predicted by BiSeNet [40]; (a/d) show the corresponding predictions obtained using random patches instead of adversarial ones.\n\nneural networks [16] (CNNs) yielded super-human performance for many different computer vision tasks, such as image recognition [9], object detection [27] [26], and image segmentation [20]. Image segmentation, and semantic segmentation (SS) in particular, is used in autonomous driving perception pipelines [30], mainly for object detection [20]. Despite their high performance, CNNs are prone to adversarial attacks [31]. Most of the literature on adversarial attacks focuses on directly manipulating the pixels of the whole image, hence making the assumption that the attacker has control over the digital representation of the environment obtained by the on-board cameras. This kind of unsafe inputs are called digital adversarial examples.\n\nAlthough such digital attacks do not transfer well into the real world, they continue to be used to evaluate the robustness of models in safety-critical systems [12,3,19]. Realworld adversarial examples (RWAEs), on the other hand, are physical objects that can be placed in the field of view of a camera, such that the resulting image acts as an adversarial example for the neural network under attack [17]. Thus, RWAEs can induce errors in neural networks without requiring the attacker to access the digital representation of the image, thereby making them a more realistic and dangerous threat to safety-critical systems. This paper. This work focuses on RWAEs, as they repre-sent a potential threat to tasks in autonomous driving today. Although the effects of RWAEs have been studied extensively in the literature for classification and object detection, those on SS remain relatively unexplored. However, SS is an integral part of autonomous driving pipelines [30]. Thus, this paper examines various state-of-the-art models for real-time SS aiming at benchmarking their robustness to RWAEs in autonomous driving scenarios.\n\nOf the several types of RWAEs proposed in the literature [32], the form of attack used in this paper is adversarial patches [4]. This is because attacks that perturb the whole image are not practically feasible in the real world. Conversely, such patches can be easily printed and attached to any visible 2D surface in the driving environment, such as billboards and road signs, thus making them a simple, yet effective attack strategy.\n\nThe paper starts by recognizing the shortcomings of the standard cross-entropy loss for optimizing adversarial patches for SS. Thus, an extension to the cross-entropy loss is proposed and integrated in all the performed attacks. This extension forces the optimization to focus on pixels that are not yet misclassified, thus obtaining patches that are more powerful compared to those generated with the standard cross-entropy-based setting [21].\n\nFollowing this rationale, the robustness of real-time SS models to RWAEs attacks is benchmarked. The paper starts by first examining the case of driving images, crafting adversarial patches on the Cityscapes dataset [5], a popular benchmark of high-resolution images of urban driving. Robust real-world patches are crafted by following the Expectation Over Transformation (EOT) [2] paradigm, which has been extended in this work to attack SS models. Furthermore, a comparison against non-robust patches (without EOT) is presented to question their effectiveness on driving scenes.\n\nAnother set of experiments targeted a virtual 3D scenario, for which a stronger adversarial attack is presented and tested. The proposed scene-specific attack, defined in Section 3.4, is a more practical tool for crafting adversarial patches in a realistic autonomous driving scenario. It assumes that the attacker is interested in targeting an autonomous driving scene at a particular corner of a specific town, where information about the position of the attackable 2D surface (in our case, a billboard) is available. To satisfy such requirements we developed and tested this attack using the CARLA simulator, which provides all the needed geometric information. These experiments include a comparison with the EOT-based and non-robust patches, performed by importing them into the CARLA world and placing them on billboards to simulate a realistic study. Figure 1 provides some examples of the effect of our patches on Cityscapes and CARLA.\n\nThe last set of experiments were conducted on a real-world driving scenario, which required collecting a dataset within the city, optimizing a patch on it, physically printing said patch on a billboard, and finally evaluating SS models on images containing the printed patch.\n\nTo the best of our knowledge, this work represents the first exhaustive evaluation of the robustness of SS models against RWAEs for autonomous driving systems. The results of the experiments state important observations that should be taken into consideration while evaluating the trustworthiness of SS models in autonomous driving. First, they demonstrate that non-robust patches are not good candidates for assessing the practical robustness of an SS model to adversarial examples. Indeed, while they proved to be effective in attacking images related to driving scenes (from Cityscapes), they do not induce any real-world adversarial effect when crafted and tested in a virtual 3D world (based on CARLA). Conversely, robust patches, crafted with EOT or the proposed scene-specific approach, resulted to be less effective than non-robust ones on Cityscapes images, but were capable to accomplish the attack in both virtual 3D world and the real world. Nevertheless, their effectiveness in the latter two cases still resulted to be quite limited, hence questioning the practical relevance of RWAEs.\n\nIn summary, the paper makes the following contributions:\n\n\u2022 It proposes an extension to the pixel-wise crossentropy loss to enable crafting strong patches for the semantic segmentation setting. \u2022 It proposes a novel technique for crafting adversarial patches for autonomous driving scenarios that utilize geometric information of the 3D world. \u2022 It finally reports an extensive evaluation of RWAEbased attacks on a set of real-time semantic segmentation models using data from the Cityscapes dataset, CARLA, and the real world. The remainder of this paper is organized as follows: Section 2 provides a brief overview of related work existing in the literature, Section 3 formalizes the proposed loss function, pipeline, and attack strategy, Section 4 reports the experimental results, and Section 5 states the conclusions and proposes ideas for future work.\n\n\nRelated Work\n\nSzegedy et al. [33] showed that small well-crafted perturbations when added to the input image were sufficient to fool strong classification networks. [3,19,37,14,13,1,21,29] have studied such attacks for the specific use case of fooling SS models. However, these attacks directly manipulate the pixels of the image. Although such digital perturbations provide a convenient way to provide benchmarks in research, they do no extend well to real-world applications.\n\nA more realistic threat model led to the introduction of RWAEs by Kurakin et. al. [17]. The attacker here is as-sumed to be able to craft adversarial pictures in the physical world, without the ability to manipulate the digital representation of inputs to the neural network. However, this work did not account for factors that affect images of objects in the real-world (e.g., varying viewpoints from which input images could be captured, changes in lighting conditions and so on). Athalye et. al. [2] address this issue by introducing the EOT algorithm. EOT accounts for such factors in the optimisation by modeling them as a distribution of transformation functions applied to the adversarial input. These transformations can be in the form of rotation, scaling, noise, brightening and so on. Then, the idea is to optimize the loss function in expectation across the range of selected transformation functions.\n\nThe EOT formulation led to the development of adversarial patches, introduced by Brown et al. [4] to fool image classifiers. They are robust, localized, image-agnostic perturbations, crafted with the EOT paradigm, capable of fooling neural networks when placed within the input scene or added digitally on images.\n\nAlthough extensive prior work exists to construct such physical attacks for classification [4,28,8], object detection [36,35,18,41], optical flow [25], LiDAR object detection [34], and depth estimation [39], only a few focus on autonomous driving tasks, since testing the adversarial robustness is more challenging, as it requires controlling the 3D outdoor environments. Other works [41,35] have shown CARLA to be a viable solution in alleviating this issue by crafting and evaluating adversarial situations in virtual 3D environments. This paper also heavily relies on CARLA to evaluate how the optimized adversarial patches translate to a 3D world.\n\nThe work closest to ours is the one by Nakka et. al. [21], who attempted to fool a variety of SS models via local attacks (i.e., creating pixel perturbation in a specific area of the image). Despite the attacks being local, the objective of their study was not to evaluate the robustness to real-world attacks, which is instead the main focus of this paper. To the best of our knowledge, such a study is missing in the literature for the case of SS models, which represent essential components in an autonomous driving perception pipeline [30].\n\nAdditionally, this paper also improves the loss function used for generating patches. Section 3.5 presents a more general formulation of the cross-entropy loss for the SS setting, designed to optimize more powerful and effective adversarial examples, while all the previously mentioned papers use the standard pixel-wise cross-entropy loss.\n\n\nAttack Formulation\n\nThis section presents the design of adversarial patches for semantic segmentation (SS), starting with a short recap of the basic notions behind SS. The patch optimization scheme for both the EOT-based and the scene-specific attacks is then presented. Finally, the proposed loss function is introduced.\n\n\nBackground on SS\n\nAn image with height H and width W can be represented as x \u2208 [0, 1] H\u00d7W \u00d7C , where C is the number of channels. An SS model returns f (x) \u2208 [0, 1] H\u00d7W \u00d7Nc , where N c is the number of classes. This output represents the predicted class-probability scores associated to each image pixel. In particular, f j i (x) \u2208 [0, 1] denotes the predicted probability score for the i-th pixel of the image corresponding to the class j. Consequently, the predicted semantic segmentation SS(x) \u2208 N H\u00d7W is computed by extracting those classes with the highest probability score in each pixel:\nSS(x) = argmax j\u2208{1,...,Nc} f j i (x) , \u2200i \u2208 {1, ..., H \u00d7 W }.\nThe ground truth for the SS of x is defined as y \u2208 N H\u00d7W , and assigns the correct class (in {1, ..., N c }) to each pixel. The performance of the SS models is evaluated by computing the cross-entropy loss L CE (f i (x), y i ) = \u2212log(f yi i (x)). Thus, for each pixel i, the model's prediction f i is compared against the ground truth class y i .\n\n\nPatch-based attack pipeline\n\nBoth the EOT-based and the scene-specific attacks share a similar pipeline, which is explained in the following paragraph and illustrated in Figure 2.\n\nAn adversarial patch of heightH and widthW is denoted as \u03b4 \u2208 [0, 1]H \u00d7W \u00d7C , whereH < H andW < W . This patch is then added to the original image x to obtain a patched imagex. Thus, the output of the SS model on this patched image would now be f (x).\n\nThe attacks considered are both untargeted. This means that the objective is to maximize a certain loss function L, without forcing the classification of pixels towards any specific class.\n\nInspired by the EOT method [2], the idea is to find an optimal patch \u03b4 * (starting from a random patch) that maximizes the loss L for all the patched images in expectation according to the distribution of transformations used to apply the patch \u03b4 on the image set X.\n\nFormally, we need to define: \u2022 A set of appearance-changing transformations \u0393 a , for instance changes in illumination (brightness, contrast) and noise (uniform or gaussian). These transformations are directly applied to the patch, so obtaining a transformed patch \u03b6 a (\u03b4), where \u03b6 a \u2208 \u0393 a . They are used to make the patch robust to illumination changes and acquisition noise. \u2022 A patch placement function \u03b7 that defines which portion of the original image x is occupied by the patch. This is the only part of the pipeline that differs between the two proposed attacks, and is discussed further in the following subsections. \u2022 A patch application function g(x, \u03b6 a (\u03b4), \u03b7) that replaces a certain area of x with \u03b6 a (\u03b4) according to \u03b7 and returns the patched imagex. These functions are sufficient to define both the EOTbased attack, which uses randomized spatial transformations to place the patch onto the image, and the scenespecific attack, which uses a precise projective transformation to enhance the accuracy of the patch placement.\n\n\nEOT-based patch attack\n\nThe classic EOT-based attack, as in previous work [4] [38], uses a set of combinations of spatial transformations \u0393, including translation and scaling, from which the patch placement function \u03b7 is selected.\n\nThe parameters for each transformation are randomized within a pre-defined range. Section 4 provides a more detailed explanation of the set of transformations used. The optimal patch is then defined as\n\u03b4 * = argmax \u03b4 E x\u2208X,\u03b6a\u2208\u0393a,\u03b7\u2208\u0393 L(f (x), y)(1)\nIn practice, the optimal patch is computed via an iterative optimization process. At each iteration t, the pixels values of the patch are modified in the direction of the gradient of the loss function computed with respect to the patch:\n\u03b4 t+1 = clip [0,1] \uf8eb \uf8ec \uf8ec \uf8ed \u03b4 t + \u00b7 x\u2208X \u03b6a\u2208\u0393a \u03b7\u2208\u0393 \u2207 \u03b4t L(f (x), y) \uf8f6 \uf8f7 \uf8f7 \uf8f8 ,(2)\nwhere represents the step size. L consists of a weighted sum of multiple loss functions. The adversarial patch effect is obtained through the optimization of the adversarial loss L adv (discussed further in subsection 3.5). Additionally, to ensure that the patch transfers well to the real world, two losses are added to account for the physical realizability of the patch (see supplementary material 1 ): smoothness loss L S and non-printability score L N .\n\n\nScene-specific patch attack\n\nTo provide a more realistic approach for autonomous driving environments, this work proposes an alternative attack methodology that exploits the geometrical information provided by the CARLA Simulator [6].\n\nHere, the key assumption is the availability of an attackable 2D surface, e.g., a billboard, with a fixed location in close proximity to the road. The CARLA simulator features the possibility to extract camera extrinsic and intrinsic matrices (details in supplementary material), and the pose of the attackable surface. The billboard-to-image transformations can be computed using a 3D roto-translation composition, which allows the patch to be warped accordingly, thus obtaining higher precision in applying the patch to the attack surface.\n\nThis attack uses the same optimization pipeline as before, with one major difference: instead of placing the patch randomly, as in the previous attack, correct projective transformations are used to determine the placement of the patch on the attackable surface. Hence, \u03b7 is no longer randomized, but is computed for each image in the dataset.\n\nThis method allows crafting precise attacks that are optimised for the region of the town that the attacker is interested in. The attacker would need to collect several images, from different viewpoints, of the desired attackable surface, along with the corresponding intrinsic and extrinsic matrices. This approach to image collection also implies that EOT is no longer needed for patch placement, thereby simplifying the optimization process.\n\nThe downside of this approach is that a digital representation of the target scene is required to accurately capture the required matrices. Although CARLA provides the possibility to import cities via OpenStreetMaps (https:// www.openstreetmap.org/), it requires some amount of manual effort to properly model 3D meshes to include objects in this simulated world. These objects need to be properly designed to ensure that the patches optimised in simulation transfer well to the real-world. Although this paper does not investigate CARLA-to-real-world transfer issues, future work will address this problem to improve the proposed methodology and adapt it for real-world attacks. Section 4 provides a comparison of this method against the EOT-based attack.\n\n\nProposed loss function\n\nCross-entropy (CE) is a popular choice as adversarial loss. Pixel-wise CE has been shown to work well when crafting an untargeted digital attack (i.e., by directly adding a perturbation r to the pixels of a digital image) [21] [3]. This is formulated as: L adv (f (x + r), y) = 1 |N | \u00b7 i\u2208N L CE (f i (x+r), y i ), where N = {1, ..., H \u00d7W } denotes the whole set of pixels inx. However, modifications can be introduced to this formulation to allow crafting stronger attacks for fooling SS models.\n\nFollowing previous notation, let\u00d1 = {1, ...,H \u00d7W } \u2286 N denote only the pixels that correspond to the patch \u03b4. Then, \u03a5 defines a subset of image pixels that do not belong to the patch and are still predicted correctly by the model with respect to the trusted ground truth label y:\n\u03a5 = {i \u2208 N \\\u00d1 | SS i (x) = y i }.(3)\nUsing \u03a5, the previous pixel-wise CE loss computed on N \\\u00d1 can be split into two distinct terms:\nLx M = i\u2208\u03a5 L CE (f i (x), y i ), Lx M = i / \u2208\u03a5 L CE (f i (x), y i ) .\n(4) Lx M describes the cumulative CE for those pixels that have been misclassified with respect to the ground truth y, while Lx M refers to all the others.\n\nNote that both Lx M and Lx M do not consider pixels of the patch, which have been discarded to focus the optimization on attacking portions of the image away from the patch. By computing these separate contribution to the total loss, we avoid that the contribution of the non-misclassified pixels gets obscured by the other term, which is a problem we found during preliminary tests. Hence, the adversarial loss function gradient is redefined as follows:\n\u2207 \u03b4 L(f (x), y) = \u03b3 \u00b7 \u2207 \u03b4 Lx M ||\u2207 \u03b4 Lx M || 2 + (1 \u2212 \u03b3) \u00b7 \u2207 \u03b4 Lx M ||\u2207 \u03b4 Lx M )|| 2 ,(5)\nwhere \u03b3 \u2208 [0, 1] is a parameter that determines whether the optimization should focus on decreasing the number of correctly classified pixels or improving the adversarial strength for the currently misclassified pixels. The rationale of \u03b3 is to provide an empirical balancing between the importance of L M and L M at each iteration t depending on the number of pixels not yet misclassified.\n\nMoreover, an adaptive value of \u03b3 = |\u03a5| |N \\\u00d1 | has been proposed to provide an automatic tuning of \u03b3 at each iteration. The idea is to initially focus on boosting the number of misclassified points. Over time, as this number increases, the focus of the loss function gradually shifts toward improving the adversarial strength of the patch on these wrongly classified pixels.\n\nSection 4 provides an extensive analysis of the proposed loss function by comparing multiple values of \u03b3 with the standard pixel wise CE measured both on N \\\u00d1 and N (which is used by [21]), suggesting that our formulation is indeed more general and effective for this kind of attack.\n\n\nExperimental results\n\nThis section describes the experimental setup and the results achieved with the proposed attacks. First, the proposed loss function is evaluated for different values of \u03b3 comparing its effectiveness against the standard pixel-wise CE, showing that it is a better alternative for this kind of problems. Following this, the results of patches crafted with and without EOT are presented on the Cityscapes dataset.\n\nSubsequently, the results obtained with the scenespecific attack on three CARLA-generated datasets are presented. These results are compared against the EOT-based attack to show the improved effectiveness of this formulation. Finally, some preliminary results of real-world adversarial patches are presented. A more detailed analysis of all models tested against these attacks can be found in the supplementary material.\n\n\nExperimental setup\n\nThe experiments were performed on a set of 8 NVIDIA-A100 GPUs, while the CARLA simulator was run on a system powered by an Intel Core i7 with 12GB RAM and a GeForce GTX 1080 Ti GPU.\n\nAll experiments were performed in PyTorch [23]. The optimizer of choice was Adam [15], with learning rate set to 0.5 empirically. The effect of the adversarial patches on the SS models was evaluated using the mean Intersectionover-Union (mIoU) and mean Accuracy (mAcc) [20].\n\nThe repository link with the code used for all the experiments was not included for double-blind requirements, but it will be inserted upon request or acceptance.\n\nDatasets The experiments in the case of driving images were carried out using the Cityscapes dataset [5], a popular benchmark for urban scene SS. The dataset consists of 2975 and 500 high resolution images (1024 \u00d7 2048) for training and validation, respectively. The experiments reported in this paper were conducted on 250 images randomly sampled from the training set. Conversely, the entire validation set was used to evaluate the effectiveness of the patches.\n\nThe CARLA simulator was used to provide a 3D virtual scenario. This set of experiments was performed in Town01, one of the built-in maps provided with the simulator, with 'CloudyNoon' as the preset weather. To mimic the settings of the Cityscapes dataset, RGB images of size 1024 \u00d7 2048, along with their corresponding SS tags, were collected by placing a camera on-board the ego vehicle.\n\nThe SS models trained on Cityscapes had to be finetuned to ensure good performance on CARLA images. 600 images for fine-tuning, 100 for validation, and 100 for testing were collected by spawning the ego vehicle at random positions in Town01. Additional details about the finetuning can be found in the supplementary material.\n\nTo study the effectiveness of the patches in CARLA, the map of Town01 was manually edited to include three billboards. Thus, without-EOT, EOT-based, and the scenespecific attacks were carried out at three different locations within Town01. The datasets for the attacks were collected by spawning the ego vehicle at random locations within the proximity of each billboard to emulate varying viewpoints  Table 1: mIoU and mAcc of the tested models on Cityscapes (pre-trained) and our CARLA dataset (finetuned).\n\nfrom which the patch might be captured in the real-world. For each of the three billboards, 150 training images, 100 validation, and 100 test images were gathered. Details about the position and orientation of the billboard and the camera were stored to compute the roto-translations used in the scene-specific attack. Lastly, to study the effects of adversarial patches in the real world, an additional dataset of 1000 images, hereafter referred to as Patches-scapes, was collected by mounting an action camera on the dashboard of a vehicle using a setup similar that of the Cityscapes dataset, and then driving the vehicle within the streets of our city. Models The attacks studied in this paper were evaluated using DDRNet [11], BiSeNet [40], and ICNet [42], which represent the state-of-the-art in real-time SS, making them preferable for the use case of autonomous driving. Additionally, PSPNet [43] was included in the study for the EOT-based attack on the CityScapes dataset, but not for the scene-specific attack on CARLA, since we are interested in real-time performance.\n\nAll the models were loaded with the pre-trained weights provided by the authors (further specifications are provided in the supplementary material. Table 1 summarizes the performance of these models on both the Cityscapes and CARLA validation sets.\n\n\nEOT-based patches on Cityscapes\n\nThe Cityscapes dataset is used to optimize two types of patches on the same training images, one with EOT and the other without EOT (non-robust). Three different patch sizes are studied: 150 \u00d7 300, 200 \u00d7 400, and 300 \u00d7 600 pixels.\n\nThe non-robust patches (without EOT) were optimized by placing them at the center of the image at each training iteration (i.e., \u03b7(\u00b7) = fixed position) and applying no appearance transformations (i.e., \u0393 a = \u2205).\n\nConversely, the robust optimizations with EOT apply multiple digital transformations. \u0393 a includes only Gaussian noise with standard deviation 5% of the image range. \u0393 includes random scaling (80% \u2212 120% of the initial patch size) and random translation defined as follows: if (c x , c y ) is the center of the image, the position of the patch is randomized within the range (c x \u00b1r \u00b7W /2 , c y \u00b1r \u00b7H/2), wherer \u2208 [0, 1] is a uniform random value. The translation range was kept limited, rather than considering the full image space, to ensure a greater stability and faster convergence. The patches were optimized over 200 epochs. Figure 3 reports the mIoU obtained by training patches with the pixel-wise CE computed on N (used by [21]) and N \\\u00d1 compared to the extended CE loss proposed in this paper, with \u03b3 \u2208 {0.5, 0.6, 0.7, 0.8, 0.95, 1.0, |\u03a5| |N \\\u00d1 | }. Among the models evaluated in the paper, ICNet [42] appears to be most robust on the Cityscapes dataset. Thus, the loss functions are studied by optimizing a 200 \u00d7 400 patch with and without EOT on ICNet.\n\n\nLoss functions analysis\n\nFor all the tested values of \u03b3, our formulation converges to a higher attack effect (i.e., smaller mIoU) with lesser number of epochs than the one based on the pixel-wise CE. Experiments without EOT show that all the compared implementations converge after only 10 epochs. In the EOT case, the advantages are even more evident: our proposed formulation converges at almost 25 epochs, while the CE cases still reduce slowly at 200 epochs (nearly 6 hours of optimization time).\n\nThe same study was performed for the scene-specific attack in the CARLA virtual world, and produced similar results, reported in the supplementary material.\n\nAdversarial patch effects. Table 2 reports how varying the patch size affects each of the SS models. We used the adaptive \u03b3 (i.e., \u03b3 = |\u03a5| |N \\\u00d1 | ) that has shown the best overall effect among multiple experimental tests. Figure 4 illustrates the effects of the optimized patches on the BiSeNet model. As expected, the non-robust patches (without EOT) obtain better attack performance with respect to the ones optimized with EOT. This is because the optimization process is simpler when not considering the randomized transformations. However, it is important to note that these patches would not be transferrable to the real world, and are not robust even to simple transformations [10,22].\n\n\nScene-specific patches on CARLA\n\nThe scene-specific attack was performed on the same set of models as defined earlier. Each of these models were first fine-tuned on images generated via CARLA. The performance of these fine-tuned models on the CARLA datasets is summarized in Table 1. Please note that the mIoU score is computed as an average of the per-class IoU scores, which, for CARLA, might get to 0 for some classes due to the presence of a few pixels belonging to non-common objects.\n\nAs described in Section 3.4, the patch is optimized to be adversarial for a specific urban scene by reprojecting it on the attackable 2D surface, which, in this work, is a billboard placed in three different spots in the Town01 map of CARLA. This section reports the effect of the scene-specific attack compared against the non-robust (without-EOT) and   Since the objective of this work is to craft RWAEs, the performance of the attacks is evaluated by measuring the mIoU and mAcc scores on additional scene-specific datasets. These additional datasets are produced by collecting the same images of the validation set of each scenespecific dataset, but with a single major modification: the billboard object is modified in the Unreal Editor [7] by applying the optimized patch as a decal object, which is a way to stick an image on a surface in the virtual environment. This method should provide a simulated real-world application of the patches, since they are no more applied directly on the image, but the image itself includes the patched billboard. Table 3 summarizes the results obtained on these three additional scene-specific datasets, with a random patch, a non-robust patch, an EOT-based patch, and a scene-specific patch. Figure 5 shows a comparison of all the discussed attacks on DDRNet.\n\nFor almost all the combinations of scene and network, the scene-specific attack outperforms the EOT-based attack, confirming that the scene-specific attack, for this kind of problems, is a better alternative to the EOT formulation for the placement of the patch within the image. The only case where the the two attacks show comparable performance is for scene 3, where the billboard is almost perpendicular to the camera plane, allowing the EOT method to cover realistic patch placing functions.\n\nIt is also worth noting that SS models are rather robust to adversarial patch attacks in general. Although it is possible to craft adversarial patches that cause a section of the image to be wrongly segmented, it tends to be more difficult than fooling models for tasks such as classification. Additional details are reported in the supplementary material.\n\n\nReal-world patches\n\nIn order to prove that the proposed pipeline can be used for a real-world attack, we use the Patches-scapes dataset (described in Section 4.1) to craft an adversarial real-world  Table 3: Adversarial patch results on the three scene CARLA datasets. The Table reports the mIoU and mAcc obtained with random, non-robust (without EOT), EOT-based and scene-specific patches.  patch using the EOT-based patch attack. This section presents the results of an attack in Figure 6. Although the presence of the optimized patch does alter a significant area of the predicted SS (while the random patch does not), portions of the image far from its position are not affected. Furthermore, the attack performance decreases as we move the patch away from the camera (details provided in the supplementary material.The patch is optimized for 200 epochs on the pre-trained version of ICnet (since it showed good performance on the Patches-scapes dataset) and printed as a 1m \u00d7 2m poster. Testing adversarial patches for autonomous driving in the real world poses a series of difficulties that heavily limited the tests. First, it is not easy to find a urban corner with good prediction accuracy, and which is not crowded with moving vehicles (which might be dangerous). Second, the patch must be printed in the highest resolution possible on a large rigid surface, which might get expensive. Furthermore, since weather conditions are not controllable and change throughout the day, results can diverge from what is expected.\n\nThe scene-specific attack, which requires additional geometric information, could not be implemented at the time of writing, but will be considered in future work.\n\n\nConclusions and future work\n\nThis paper presented an extensive study of the adversarial robustness of semantic segmentation models. This was accomplished by extensively evaluating the effect introduced by adversarial patches, to investigate the limits of real-world attacks for segmentation neural networks in an autonomous driving scenario. Carrying out the investigations with increasingly \"real-world\" benchmarks, we studied the effect of non-robust and EOT-based patches on the Cityscapes dataset, on a virtual 3D scenario, and in a realworld setting. We also introduced a new method called scene-specific attack, which improves the EOT formulation for a more realistic and effective patch placement.\n\nThe novel loss function introduced in the paper enabled to advance the state-of-the-art for adversarial patches optimization methods, as it proved to be a more general and efficient alternative to the classic cross-entropy function for this kind of problems.\n\nThis exhaustive set of experiments practically opens to a new point of view for studying SS models in autonomous driving. Although the proposed attacks were able to reduce the baseline model accuracy, the SS models proved to be somehow robust to real-world patch-based attacks. This was especially noticeable when the tests were performed in more realistic settings using CARLA and the real world, where, in most cases, the patch only affected the proximity of the attacked surface.\n\nNevertheless, this is a promising result, since it shows how the prediction provided by these models is not easily corruptible, especially in real-world scenarios. This is in contrast with previous work on patch-based adversarial attacks against classification and object detection models.\n\nFuture work will further investigate the robustness properties of these models, introducing defense mechanisms and trying to enhance the robustness of SS model by adding a temporal dimension.\n\nFigure 2 :\n2Outline of the proposed approach for crafting both the EOT-based and the scene-specific patches.\n\nFigure 3 :\n3Comparison of adversarial patch optimizations (200 \u00d7 400) on ICNet and Cityscapes using different loss functions: two versions of the standard pixel-wise cross-entropy and our formulation with multiple values of \u03b3. L CE on N is the original version used by[21], while L CE on N \\\u00d1 is an improved version based on the rationale presented in Section 3\n\nFigure 4 :\n4Semantic segmentations obtained from BiSeNet with no patch (b), a random patch (c), an EOT-based patch (d), and non-robust patch (without EOT) (e) added into a original image (a) of the Cityscapes validation set.the EOT-based attacks. The optimized patch is composed of 150 \u00d7 300 pixels, imposing a real-world dimension of 3.75m \u00d7 7.5m. Additional experiments on the effect of the real-world dimension of the patch and the number of pixels used are presented in the supplementary material.For all the following experiments, \u0393 a includes contrast and brightness changes (both 10% of the image range), and Gaussian noise (standard deviation 10% of the image range).\n\nFigure 5 :\n5(a) is an image extracted from the scene-1 test dataset augmented with a scene-specific patch optimized on DDRNet, while (e) is its corresponding SS; (b), (c), and (d) are predictions obtained by augmenting the same test image with a random, non-robust and EOT-based patches, respectively.\n\nFigure 6 :\n6Real-world predictions on ICNet obtained with a printed random patch (left) and an optimized patch (right).\n\nTable 2 :\n2Adversarial patch results on the Cityscapes dataset. Each cell reports the final mIoU obtained with a random patch \n(no optimization), with EOT, and without EOT. \n\n(a) \n(b) \n(c) \n(d) \n(e) \n\n\n\n\nModel mIoU -mAcc (rand / without EOT / EOT / scene-specific) BiSeNet 0.44 / 0.42 / 0.36 / 0.31 0.63 / 0.61 / 0.55 / 0.49 0.60 / 0.60 / 0.58 / 0.58 0.76 / 0.75 / 0.74 / 0.74 0.47 / 0.46 / 0.46 / 0.45 0.74 / 0.73 / 0.73 / 0.73 DDRNet 0.51 / 0.50 / 0.46 / 0.46 0.70 / 0.69 / 0.69 / 0.69 0.62 / 0.62 / 0.52 / 0.49 0.76 / 0.75 / 0.71 / 0.66 0.65 / 0.65 / 0.58 / 0.59 0.78 / 0.78 / 0.76 / 0.76Scene1 \nScene2 \nScene3 \nICNet \n0.51 / 0.51 / 0.49 / 0.48 0.60 / 0.60 /0.56 / 0.54 0.64 / 0.64 / 0.61 / 0.61 0.74 / 0.74 / 0.73 / 0.73 0.63 / 0.63 / 0.59 / 0.59 0.76 / 0.76 / 0.73 / 0.74 \n\nThe paper includes references to additional material, that can be provided upon request.\n\nOn the robustness of semantic segmentation models to adversarial attacks. Anurag Arnab, Ondrej Miksik, Philip Hs Torr, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAnurag Arnab, Ondrej Miksik, and Philip HS Torr. On the robustness of semantic segmentation models to adversarial attacks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 888-897, 2018.\n\nSynthesizing robust adversarial examples. Anish Athalye, Logan Engstrom, Andrew Ilyas, Kevin Kwok, PMLRProceedings of the 35th International Conference on Machine Learning. Jennifer Dy and Andreas Krausethe 35th International Conference on Machine Learning80Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th In- ternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 284-293. PMLR, 10-15 Jul 2018.\n\nThe vulnerability of semantic segmentation networks to adversarial attacks in autonomous driving: Enhancing extensive environment sensing. Andreas B\u00e4r, Jonas L\u00f6hdefink, Nikhil Kapoor, Serin Varghese, Fabian H\u00fcger, Peter Schlicht, Tim Fingscheidt, IEEE Signal Process. Mag. 381Andreas B\u00e4r, Jonas L\u00f6hdefink, Nikhil Kapoor, Serin Vargh- ese, Fabian H\u00fcger, Peter Schlicht, and Tim Fingscheidt. The vulnerability of semantic segmentation networks to adver- sarial attacks in autonomous driving: Enhancing extensive environment sensing. IEEE Signal Process. Mag., 38(1):42- 52, 2021.\n\n. Tom B Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, Justin Gilmer, arXiv:1712.09665arXiv: 1712.09665Adversarial Patch. Tom B. Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, and Justin Gilmer. Adversarial Patch. arXiv:1712.09665 [cs], May 2018. arXiv: 1712.09665.\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016. Las Vegas, NV, USAIEEE Computer SocietyMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In 2016 IEEE Conference on Computer Vision and Pattern Recog- nition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 3213-3223. IEEE Computer Society, 2016.\n\nCARLA: an open urban driving simulator. Alexey Dosovitskiy, Germ\u00e1n Ros, Felipe Codevilla, Antonio M L\u00f3pez, Vladlen Koltun, 1st Annual Conference on Robot Learning. Mountain View, California, USAPMLR78ProceedingsAlexey Dosovitskiy, Germ\u00e1n Ros, Felipe Codevilla, Anto- nio M. L\u00f3pez, and Vladlen Koltun. CARLA: an open urban driving simulator. In 1st Annual Conference on Robot Learn- ing, CoRL 2017, Mountain View, California, USA, November 13-15, 2017, Proceedings, volume 78 of Proceedings of Ma- chine Learning Research, pages 1-16. PMLR, 2017.\n\nUnreal engine. Epic GamesEpic Games. Unreal engine.\n\nRobust physical-world attacks on deep learning visual classification. Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, Dawn Song, 2018 IEEE Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT, USAIEEE Computer SocietyKevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1625- 1634. IEEE Computer Society, 2018.\n\nRecent advances in convolutional neural networks. Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang, Jianfei Cai, Tsuhan Chen, Pattern Recognit. 77Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang, Gang Wang, Jianfei Cai, and Tsuhan Chen. Recent advances in convolutional neural networks. Pattern Recognit., 77:354- 377, 2018.\n\nChuan Guo, Mayank Rana, arXiv:1711.00117Moustapha Cisse, and Laurens van der Maaten. Countering Adversarial Images using Input Transformations. arXiv e-prints. Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering Adversarial Images using In- put Transformations. arXiv e-prints, page arXiv:1711.00117, Oct. 2017.\n\nDeep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes. Yuanduo Hong, Huihui Pan, Weichao Sun, Senior Member, Ieee , Yisong Jia, arXiv:2101.06085arXiv e-printsYuanduo Hong, Huihui Pan, Weichao Sun, Senior Mem- ber, IEEE, and Yisong Jia. Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes. arXiv e-prints, page arXiv:2101.06085, Jan. 2021.\n\nA survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability. Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, Xinping Yi, Computer Science Review. 37100270Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neural net- works: Verification, testing, adversarial attack and defence, and interpretability. Computer Science Review, 37:100270, 2020.\n\nBenchmarking the Robustness of Semantic Segmentation Models. Christoph Kamann, Carsten Rother, arXiv:1908.05005arXiv eprintsChristoph Kamann and Carsten Rother. Benchmarking the Robustness of Semantic Segmentation Models. arXiv e- prints, page arXiv:1908.05005, Aug. 2019.\n\nAdversarial attacks for image segmentation on multiple lightweight models. Xu Kang, Bin Song, Xiaojiang Du, Mohsen Guizani, IEEE Access. 8Xu Kang, Bin Song, Xiaojiang Du, and Mohsen Guizani. Adversarial attacks for image segmentation on multiple lightweight models. IEEE Access, 8:31359-31370, 2020.\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv e-printsDiederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv e-prints, page arXiv:1412.6980, Dec. 2014.\n\nImageNet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Communications of the ACM. 606Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural net- works. Communications of the ACM, 60(6):84-90, May 2017.\n\nAdversarial examples in the physical world. Alexey Kurakin, Ian J Goodfellow, Samy Bengio, 5th International Conference on Learning Representations. Toulon, FranceWorkshop Track Proceedings. OpenReview.netAlexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Ad- versarial examples in the physical world. In 5th Interna- tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop Track Pro- ceedings. OpenReview.net, 2017.\n\nOn Physical Adversarial Patches for Object Detection. Mark Lee, Zico Kolter, arXiv:1906.11897arXiv: 1906.11897cs, statMark Lee and Zico Kolter. On Physical Adversarial Patches for Object Detection. arXiv:1906.11897 [cs, stat], June 2019. arXiv: 1906.11897.\n\nUniversal adversarial perturbations against semantic image segmentation. Jan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, Volker Fischer, IEEE International Conference on Computer Vision. Venice, ItalyIEEE Computer SocietyJan Hendrik Metzen, Mummadi Chaithanya Kumar, Thomas Brox, and Volker Fischer. Universal adversarial perturba- tions against semantic image segmentation. In IEEE Interna- tional Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 2774-2783. IEEE Com- puter Society, 2017.\n\nShervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, arXiv:2001.05566arXiv: 2001.05566Nasser Kehtarnavaz, and Demetri Terzopoulos. Image Segmentation Using Deep Learning: A Survey. Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image Segmentation Using Deep Learning: A Survey. arXiv:2001.05566 [cs], Nov. 2020. arXiv: 2001.05566.\n\nIndirect local attacks for context-aware semantic segmentation networks. Krishna Kanth Nakka, Mathieu Salzmann, Computer Vision -ECCV 2020 -16th European Conference. Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael FrahmGlasgow, UKSpringer12350Proceedings, Part VKrishna Kanth Nakka and Mathieu Salzmann. Indirect local attacks for context-aware semantic segmentation networks. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan- Michael Frahm, editors, Computer Vision -ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V, volume 12350 of Lecture Notes in Computer Science, pages 611-628. Springer, 2020.\n\nDetecting Adversarial Examples by Input Transformations. Federico Nesti, Alessandro Biondi, Giorgio Buttazzo, arXiv:2101.11466Defense Perturbations, and Voting. arXiv e-prints. Federico Nesti, Alessandro Biondi, and Giorgio Buttazzo. Detecting Adversarial Examples by Input Transformations, Defense Perturbations, and Voting. arXiv e-prints, page arXiv:2101.11466, Jan. 2021.\n\nPytorch: An imperative style, high-performance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch'e-Buc, E. Fox, and R. GarnettCurran Associates, Inc32Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im- perative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch'e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nA survey on deep learning: Algorithms, techniques, and applications. Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, S S Iyengar, ACM Comput. Surv. 515SeptSamira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei-Ling Shyu, Shu-Ching Chen, and S. S. Iyengar. A survey on deep learning: Algo- rithms, techniques, and applications. ACM Comput. Surv., 51(5), Sept. 2018.\n\nAttacking Optical Flow. Anurag Ranjan, Joel Janai, Andreas Geiger, Michael J Black, arXiv:1910.10053arXiv e-printsAnurag Ranjan, Joel Janai, Andreas Geiger, and Michael J. Black. Attacking Optical Flow. arXiv e-prints, page arXiv:1910.10053, Oct. 2019.\n\nYou only look once: Unified, real-time object detection. Joseph Redmon, Santosh Kumar Divvala, Ross B Girshick, Ali Farhadi, 2016 IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USAIEEE Computer SocietyJoseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. You only look once: Unified, real-time ob- ject detection. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 779-788. IEEE Computer Society, 2016.\n\nFaster R-CNN: towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross B He, Jian Girshick, Sun, Corinna Cortes, Neil DShaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks. In Corinna Cortes, Neil D.\n\nDaniel D Lawrence, Masashi Lee, Roman Sugiyama, Garnett, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Montreal, Quebec, CanadaLawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Process- ing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 91-99, 2015.\n\nAccessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition. Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, Michael K Reiter, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. the 2016 ACM SIGSAC Conference on Computer and Communications SecurityVienna AustriaACMMahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition. In Proceed- ings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, pages 1528-1540, Vienna Aus- tria, Oct. 2016. ACM.\n\nAdvspade: Realistic unrestricted attacks for semantic segmentation. Guangyu Shen, Chengzhi Mao, Junfeng Yang, Baishakhi Ray, arXiv:1910.02354arXiv preprintGuangyu Shen, Chengzhi Mao, Junfeng Yang, and Baishakhi Ray. Advspade: Realistic unrestricted attacks for semantic segmentation. arXiv preprint arXiv:1910.02354, 2019.\n\nA comparative study of real-time semantic segmentation for autonomous driving. Mennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand, Hong Zhang, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsMennatullah Siam, Mostafa Gamal, Moemen Abdel-Razek, Senthil Yogamani, Martin Jagersand, and Hong Zhang. A comparative study of real-time semantic segmentation for autonomous driving. In Proceedings of the IEEE confer- ence on computer vision and pattern recognition workshops, pages 587-597, 2018.\n\nHenrique Samuel, Peyman Silva, Najafirad, arXiv:2007.00753arXiv: 2007.00753Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey. cs, statSamuel Henrique Silva and Peyman Najafirad. Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey. arXiv:2007.00753 [cs, stat], July 2020. arXiv: 2007.00753.\n\nA survey of practical adversarial example attacks. Lu Sun, Mingtian Tan, Zhe Zhou, Cybersecurity. 11Lu Sun, Mingtian Tan, and Zhe Zhou. A survey of practi- cal adversarial example attacks. Cybersecurity, 1(1):9, Dec. 2018.\n\nIntriguing properties of neural networks. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J Goodfellow, Rob Fergus, 2nd International Conference on Learning Representations. Banff, AB, CanadaConference Track ProceedingsChristian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.\n\nPhysically realizable adversarial examples for lidar object detection. J Tu, Mengye Ren, Sivabalan Manivasagam, Ming Liang, Binh Yang, Richard Du, Frank Cheng, R Urtasun, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). J. Tu, Mengye Ren, Sivabalan Manivasagam, Ming Liang, Binh Yang, Richard Du, Frank Cheng, and R. Urtasun. Phys- ically realizable adversarial examples for lidar object detec- tion. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13713-13722, 2020.\n\nPhysical adversarial attack on vehicle detector in the carla simulator. CoRR, abs. Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, Yu Wang, Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, and Yu Wang. Physical adversarial at- tack on vehicle detector in the carla simulator. CoRR, abs/2007.16118, 2020.\n\nMaking an invisibility cloak: Real world adversarial attacks on object detectors. Zuxuan Wu, Ser-Nam Lim, Larry S Davis, Tom Goldstein, Computer Vision -ECCV 2020 -16th European Conference. Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael FrahmGlasgow, UKSpringer12349Proceedings, Part IVZuxuan Wu, Ser-Nam Lim, Larry S. Davis, and Tom Gold- stein. Making an invisibility cloak: Real world adversar- ial attacks on object detectors. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision -ECCV 2020 -16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IV, volume 12349 of Lecture Notes in Computer Science, pages 1-17. Springer, 2020.\n\nAdversarial examples for semantic segmentation and object detection. Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, Alan L Yuille, IEEE International Conference on Computer Vision. Venice, ItalyIEEE Computer SocietyCihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan L. Yuille. Adversarial examples for se- mantic segmentation and object detection. In IEEE Interna- tional Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pages 1378-1387. IEEE Com- puter Society, 2017.\n\nAdversarial t-shirt! evading person detectors in a physical world. Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, Xue Lin, Computer Vision -ECCV 2020 -16th European Conference. Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael FrahmGlasgow, UKSpringer12350Proceedings, Part VKaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen, Pin-Yu Chen, Yanzhi Wang, and Xue Lin. Adversarial t-shirt! evading person detectors in a phys- ical world. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision -ECCV 2020 -16th European Conference, Glasgow, UK, August 23- 28, 2020, Proceedings, Part V, volume 12350 of Lecture Notes in Computer Science, pages 665-681. Springer, 2020.\n\nKoichiro Yamanaka, Ryutaroh Matsumoto, Keita Takahashi, Toshiaki Fujii, arXiv:2010.03072Adversarial Patch Attacks on Monocular Depth Estimation Networks. arXiv e-prints. Koichiro Yamanaka, Ryutaroh Matsumoto, Keita Takahashi, and Toshiaki Fujii. Adversarial Patch Attacks on Monoc- ular Depth Estimation Networks. arXiv e-prints, page arXiv:2010.03072, Oct. 2020.\n\nChangqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang, Bisenet, arXiv:1808.00897Bilateral Segmentation Network for Real-time Semantic Segmentation. arXiv e-prints. Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. BiSeNet: Bilateral Segmenta- tion Network for Real-time Semantic Segmentation. arXiv e-prints, page arXiv:1808.00897, Aug. 2018.\n\nCAMOU: LEARNING A VEHICLE CAMOU-FLAGE FOR PHYSICAL ADVERSARIAL ATTACK ON OBJECT DETECTORS IN THE WILD. Yang Zhang, Hassan Foroosh, Philip David, Boqing Gong, 20Yang Zhang, Hassan Foroosh, Philip David, and Boqing Gong. CAMOU: LEARNING A VEHICLE CAMOU- FLAGE FOR PHYSICAL ADVERSARIAL ATTACK ON OBJECT DETECTORS IN THE WILD. page 20, 2019.\n\nHengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, Jiaya Jia, arXiv:1704.08545ICNet for Real-Time Semantic Segmentation on High-Resolution Images. arXiv e-prints. Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. ICNet for Real-Time Semantic Seg- mentation on High-Resolution Images. arXiv e-prints, page arXiv:1704.08545, Apr. 2017.\n\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia, arXiv:1612.01105Pyramid Scene Parsing Network. arXiv e-prints. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid Scene Parsing Network. arXiv e-prints, page arXiv:1612.01105, Dec. 2016.\n", "annotations": {"author": "[{\"end\":210,\"start\":124},{\"end\":299,\"start\":211},{\"end\":383,\"start\":300},{\"end\":473,\"start\":384},{\"end\":562,\"start\":474}]", "publisher": null, "author_last_name": "[{\"end\":138,\"start\":133},{\"end\":227,\"start\":218},{\"end\":311,\"start\":307},{\"end\":401,\"start\":395},{\"end\":490,\"start\":482}]", "author_first_name": "[{\"end\":132,\"start\":124},{\"end\":217,\"start\":211},{\"end\":306,\"start\":300},{\"end\":394,\"start\":384},{\"end\":481,\"start\":474}]", "author_affiliation": "[{\"end\":209,\"start\":140},{\"end\":298,\"start\":229},{\"end\":382,\"start\":313},{\"end\":472,\"start\":403},{\"end\":561,\"start\":492}]", "title": "[{\"end\":121,\"start\":1},{\"end\":683,\"start\":563}]", "venue": null, "abstract": "[{\"end\":2397,\"start\":685}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2506,\"start\":2502},{\"end\":2525,\"start\":2522},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2602,\"start\":2599},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2630,\"start\":2627},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2693,\"start\":2689},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2816,\"start\":2812},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2927,\"start\":2924},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2950,\"start\":2946},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2955,\"start\":2951},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2984,\"start\":2980},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3107,\"start\":3103},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3141,\"start\":3137},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3217,\"start\":3213},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3706,\"start\":3702},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3708,\"start\":3706},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3711,\"start\":3708},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3947,\"start\":3943},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4511,\"start\":4507},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4732,\"start\":4728},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4798,\"start\":4795},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5552,\"start\":5548},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5774,\"start\":5771},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5936,\"start\":5933},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9353,\"start\":9349},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9488,\"start\":9485},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9491,\"start\":9488},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9494,\"start\":9491},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9497,\"start\":9494},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9500,\"start\":9497},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9502,\"start\":9500},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9505,\"start\":9502},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9508,\"start\":9505},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9885,\"start\":9881},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10301,\"start\":10298},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10811,\"start\":10808},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11123,\"start\":11120},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11126,\"start\":11123},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11128,\"start\":11126},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11151,\"start\":11147},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11154,\"start\":11151},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11157,\"start\":11154},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11160,\"start\":11157},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11179,\"start\":11175},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11208,\"start\":11204},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11235,\"start\":11231},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11417,\"start\":11413},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11420,\"start\":11417},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11739,\"start\":11735},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12225,\"start\":12221},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14555,\"start\":14552},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15913,\"start\":15910},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15918,\"start\":15914},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17326,\"start\":17323},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19672,\"start\":19668},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19676,\"start\":19673},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22084,\"start\":22080},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23289,\"start\":23285},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23328,\"start\":23324},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23516,\"start\":23512},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23787,\"start\":23784},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26105,\"start\":26101},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26119,\"start\":26115},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":26135,\"start\":26131},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26279,\"start\":26275},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27923,\"start\":27919},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28098,\"start\":28094},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29602,\"start\":29598},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29605,\"start\":29602},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30845,\"start\":30842},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":36274,\"start\":36270}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36000,\"start\":35891},{\"attributes\":{\"id\":\"fig_2\"},\"end\":36363,\"start\":36001},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37040,\"start\":36364},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37343,\"start\":37041},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37464,\"start\":37344},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":37667,\"start\":37465},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38244,\"start\":37668}]", "paragraph": "[{\"end\":2794,\"start\":2413},{\"end\":3539,\"start\":2796},{\"end\":4669,\"start\":3541},{\"end\":5107,\"start\":4671},{\"end\":5553,\"start\":5109},{\"end\":6135,\"start\":5555},{\"end\":7080,\"start\":6137},{\"end\":7357,\"start\":7082},{\"end\":8458,\"start\":7359},{\"end\":8516,\"start\":8460},{\"end\":9317,\"start\":8518},{\"end\":9797,\"start\":9334},{\"end\":10712,\"start\":9799},{\"end\":11027,\"start\":10714},{\"end\":11680,\"start\":11029},{\"end\":12226,\"start\":11682},{\"end\":12568,\"start\":12228},{\"end\":12892,\"start\":12591},{\"end\":13489,\"start\":12913},{\"end\":13899,\"start\":13553},{\"end\":14081,\"start\":13931},{\"end\":14333,\"start\":14083},{\"end\":14523,\"start\":14335},{\"end\":14791,\"start\":14525},{\"end\":15833,\"start\":14793},{\"end\":16066,\"start\":15860},{\"end\":16269,\"start\":16068},{\"end\":16552,\"start\":16316},{\"end\":17090,\"start\":16632},{\"end\":17327,\"start\":17122},{\"end\":17870,\"start\":17329},{\"end\":18215,\"start\":17872},{\"end\":18661,\"start\":18217},{\"end\":19419,\"start\":18663},{\"end\":19942,\"start\":19446},{\"end\":20223,\"start\":19944},{\"end\":20356,\"start\":20261},{\"end\":20582,\"start\":20427},{\"end\":21038,\"start\":20584},{\"end\":21519,\"start\":21129},{\"end\":21895,\"start\":21521},{\"end\":22180,\"start\":21897},{\"end\":22615,\"start\":22205},{\"end\":23037,\"start\":22617},{\"end\":23241,\"start\":23060},{\"end\":23517,\"start\":23243},{\"end\":23681,\"start\":23519},{\"end\":24146,\"start\":23683},{\"end\":24536,\"start\":24148},{\"end\":24863,\"start\":24538},{\"end\":25373,\"start\":24865},{\"end\":26455,\"start\":25375},{\"end\":26705,\"start\":26457},{\"end\":26971,\"start\":26741},{\"end\":27184,\"start\":26973},{\"end\":28251,\"start\":27186},{\"end\":28754,\"start\":28279},{\"end\":28912,\"start\":28756},{\"end\":29606,\"start\":28914},{\"end\":30098,\"start\":29642},{\"end\":31403,\"start\":30100},{\"end\":31901,\"start\":31405},{\"end\":32259,\"start\":31903},{\"end\":33790,\"start\":32282},{\"end\":33955,\"start\":33792},{\"end\":34662,\"start\":33987},{\"end\":34922,\"start\":34664},{\"end\":35406,\"start\":34924},{\"end\":35697,\"start\":35408},{\"end\":35890,\"start\":35699}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13552,\"start\":13490},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16315,\"start\":16270},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16631,\"start\":16553},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20260,\"start\":20224},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20426,\"start\":20357},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21128,\"start\":21039}]", "table_ref": "[{\"end\":25274,\"start\":25267},{\"end\":26612,\"start\":26605},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28948,\"start\":28941},{\"end\":29891,\"start\":29884},{\"end\":31163,\"start\":31156},{\"end\":32468,\"start\":32461}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2411,\"start\":2399},{\"attributes\":{\"n\":\"2.\"},\"end\":9332,\"start\":9320},{\"attributes\":{\"n\":\"3.\"},\"end\":12589,\"start\":12571},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12911,\"start\":12895},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13929,\"start\":13902},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15858,\"start\":15836},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17120,\"start\":17093},{\"attributes\":{\"n\":\"3.5.\"},\"end\":19444,\"start\":19422},{\"attributes\":{\"n\":\"4.\"},\"end\":22203,\"start\":22183},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23058,\"start\":23040},{\"attributes\":{\"n\":\"4.2.\"},\"end\":26739,\"start\":26708},{\"end\":28277,\"start\":28254},{\"attributes\":{\"n\":\"4.3.\"},\"end\":29640,\"start\":29609},{\"attributes\":{\"n\":\"4.4.\"},\"end\":32280,\"start\":32262},{\"attributes\":{\"n\":\"5.\"},\"end\":33985,\"start\":33958},{\"end\":35902,\"start\":35892},{\"end\":36012,\"start\":36002},{\"end\":36375,\"start\":36365},{\"end\":37052,\"start\":37042},{\"end\":37355,\"start\":37345},{\"end\":37475,\"start\":37466}]", "table": "[{\"end\":37667,\"start\":37477},{\"end\":38244,\"start\":38057}]", "figure_caption": "[{\"end\":36000,\"start\":35904},{\"end\":36363,\"start\":36014},{\"end\":37040,\"start\":36377},{\"end\":37343,\"start\":37054},{\"end\":37464,\"start\":37357},{\"end\":38057,\"start\":37670}]", "figure_ref": "[{\"end\":2554,\"start\":2546},{\"end\":7003,\"start\":6995},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14080,\"start\":14072},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27826,\"start\":27818},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29145,\"start\":29137},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":31344,\"start\":31336},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":32752,\"start\":32744}]", "bib_author_first_name": "[{\"end\":38415,\"start\":38409},{\"end\":38429,\"start\":38423},{\"end\":38447,\"start\":38438},{\"end\":38869,\"start\":38864},{\"end\":38884,\"start\":38879},{\"end\":38901,\"start\":38895},{\"end\":38914,\"start\":38909},{\"end\":39538,\"start\":39531},{\"end\":39549,\"start\":39544},{\"end\":39567,\"start\":39561},{\"end\":39581,\"start\":39576},{\"end\":39598,\"start\":39592},{\"end\":39611,\"start\":39606},{\"end\":39625,\"start\":39622},{\"end\":39976,\"start\":39973},{\"end\":39978,\"start\":39977},{\"end\":39995,\"start\":39986},{\"end\":40007,\"start\":40002},{\"end\":40019,\"start\":40013},{\"end\":40033,\"start\":40027},{\"end\":40309,\"start\":40303},{\"end\":40325,\"start\":40318},{\"end\":40342,\"start\":40333},{\"end\":40354,\"start\":40350},{\"end\":40370,\"start\":40364},{\"end\":40389,\"start\":40382},{\"end\":40403,\"start\":40400},{\"end\":40418,\"start\":40412},{\"end\":40430,\"start\":40425},{\"end\":40971,\"start\":40965},{\"end\":40991,\"start\":40985},{\"end\":41003,\"start\":40997},{\"end\":41022,\"start\":41015},{\"end\":41024,\"start\":41023},{\"end\":41039,\"start\":41032},{\"end\":41600,\"start\":41595},{\"end\":41614,\"start\":41610},{\"end\":41632,\"start\":41624},{\"end\":41646,\"start\":41644},{\"end\":41655,\"start\":41651},{\"end\":41672,\"start\":41665},{\"end\":41683,\"start\":41679},{\"end\":41702,\"start\":41693},{\"end\":41714,\"start\":41710},{\"end\":42258,\"start\":42250},{\"end\":42270,\"start\":42263},{\"end\":42282,\"start\":42277},{\"end\":42297,\"start\":42289},{\"end\":42306,\"start\":42302},{\"end\":42322,\"start\":42318},{\"end\":42334,\"start\":42330},{\"end\":42348,\"start\":42340},{\"end\":42359,\"start\":42355},{\"end\":42373,\"start\":42366},{\"end\":42385,\"start\":42379},{\"end\":42652,\"start\":42647},{\"end\":42664,\"start\":42658},{\"end\":43090,\"start\":43083},{\"end\":43103,\"start\":43097},{\"end\":43116,\"start\":43109},{\"end\":43128,\"start\":43122},{\"end\":43141,\"start\":43137},{\"end\":43150,\"start\":43144},{\"end\":43558,\"start\":43551},{\"end\":43572,\"start\":43566},{\"end\":43589,\"start\":43583},{\"end\":43601,\"start\":43596},{\"end\":43617,\"start\":43609},{\"end\":43628,\"start\":43623},{\"end\":43639,\"start\":43636},{\"end\":43651,\"start\":43644},{\"end\":44054,\"start\":44045},{\"end\":44070,\"start\":44063},{\"end\":44335,\"start\":44333},{\"end\":44345,\"start\":44342},{\"end\":44361,\"start\":44352},{\"end\":44372,\"start\":44366},{\"end\":44604,\"start\":44603},{\"end\":44620,\"start\":44615},{\"end\":44858,\"start\":44854},{\"end\":44875,\"start\":44871},{\"end\":44895,\"start\":44887},{\"end\":44897,\"start\":44896},{\"end\":45161,\"start\":45155},{\"end\":45174,\"start\":45171},{\"end\":45176,\"start\":45175},{\"end\":45193,\"start\":45189},{\"end\":45634,\"start\":45630},{\"end\":45644,\"start\":45640},{\"end\":45910,\"start\":45907},{\"end\":45945,\"start\":45927},{\"end\":45959,\"start\":45953},{\"end\":45972,\"start\":45966},{\"end\":46378,\"start\":46371},{\"end\":46391,\"start\":46387},{\"end\":46405,\"start\":46400},{\"end\":46422,\"start\":46415},{\"end\":46846,\"start\":46839},{\"end\":46867,\"start\":46860},{\"end\":47492,\"start\":47484},{\"end\":47510,\"start\":47500},{\"end\":47526,\"start\":47519},{\"end\":47878,\"start\":47874},{\"end\":47890,\"start\":47887},{\"end\":47907,\"start\":47898},{\"end\":47919,\"start\":47915},{\"end\":47932,\"start\":47927},{\"end\":47950,\"start\":47943},{\"end\":47965,\"start\":47959},{\"end\":47981,\"start\":47975},{\"end\":47994,\"start\":47987},{\"end\":48011,\"start\":48007},{\"end\":48025,\"start\":48020},{\"end\":48044,\"start\":48037},{\"end\":48057,\"start\":48051},{\"end\":48071,\"start\":48064},{\"end\":48086,\"start\":48080},{\"end\":48102,\"start\":48095},{\"end\":48117,\"start\":48111},{\"end\":48138,\"start\":48132},{\"end\":48150,\"start\":48148},{\"end\":48163,\"start\":48157},{\"end\":48176,\"start\":48169},{\"end\":49002,\"start\":48996},{\"end\":49018,\"start\":49014},{\"end\":49031,\"start\":49026},{\"end\":49043,\"start\":49037},{\"end\":49056,\"start\":49050},{\"end\":49067,\"start\":49062},{\"end\":49073,\"start\":49068},{\"end\":49089,\"start\":49081},{\"end\":49105,\"start\":49096},{\"end\":49113,\"start\":49112},{\"end\":49115,\"start\":49114},{\"end\":49425,\"start\":49419},{\"end\":49438,\"start\":49434},{\"end\":49453,\"start\":49446},{\"end\":49469,\"start\":49462},{\"end\":49471,\"start\":49470},{\"end\":49712,\"start\":49706},{\"end\":49728,\"start\":49721},{\"end\":49748,\"start\":49744},{\"end\":49750,\"start\":49749},{\"end\":49764,\"start\":49761},{\"end\":50259,\"start\":50252},{\"end\":50278,\"start\":50274},{\"end\":50280,\"start\":50279},{\"end\":50289,\"start\":50285},{\"end\":50499,\"start\":50493},{\"end\":50501,\"start\":50500},{\"end\":50519,\"start\":50512},{\"end\":50530,\"start\":50525},{\"end\":51042,\"start\":51035},{\"end\":51056,\"start\":51051},{\"end\":51074,\"start\":51070},{\"end\":51089,\"start\":51082},{\"end\":51091,\"start\":51090},{\"end\":51651,\"start\":51644},{\"end\":51666,\"start\":51658},{\"end\":51679,\"start\":51672},{\"end\":51695,\"start\":51686},{\"end\":51990,\"start\":51979},{\"end\":52004,\"start\":51997},{\"end\":52018,\"start\":52012},{\"end\":52039,\"start\":52032},{\"end\":52056,\"start\":52050},{\"end\":52072,\"start\":52068},{\"end\":52549,\"start\":52541},{\"end\":52564,\"start\":52558},{\"end\":52941,\"start\":52939},{\"end\":52955,\"start\":52947},{\"end\":52964,\"start\":52961},{\"end\":53163,\"start\":53154},{\"end\":53181,\"start\":53173},{\"end\":53195,\"start\":53191},{\"end\":53211,\"start\":53207},{\"end\":53226,\"start\":53219},{\"end\":53237,\"start\":53234},{\"end\":53239,\"start\":53238},{\"end\":53255,\"start\":53252},{\"end\":53782,\"start\":53781},{\"end\":53793,\"start\":53787},{\"end\":53808,\"start\":53799},{\"end\":53826,\"start\":53822},{\"end\":53838,\"start\":53834},{\"end\":53852,\"start\":53845},{\"end\":53862,\"start\":53857},{\"end\":53871,\"start\":53870},{\"end\":54327,\"start\":54323},{\"end\":54338,\"start\":54332},{\"end\":54352,\"start\":54345},{\"end\":54363,\"start\":54357},{\"end\":54379,\"start\":54371},{\"end\":54388,\"start\":54386},{\"end\":54662,\"start\":54656},{\"end\":54674,\"start\":54667},{\"end\":54685,\"start\":54680},{\"end\":54687,\"start\":54686},{\"end\":54698,\"start\":54695},{\"end\":55361,\"start\":55355},{\"end\":55373,\"start\":55367},{\"end\":55388,\"start\":55380},{\"end\":55401,\"start\":55396},{\"end\":55414,\"start\":55408},{\"end\":55424,\"start\":55420},{\"end\":55426,\"start\":55425},{\"end\":55898,\"start\":55893},{\"end\":55910,\"start\":55903},{\"end\":55923,\"start\":55918},{\"end\":55935,\"start\":55929},{\"end\":55948,\"start\":55941},{\"end\":55960,\"start\":55954},{\"end\":55973,\"start\":55967},{\"end\":55986,\"start\":55980},{\"end\":55996,\"start\":55993},{\"end\":56625,\"start\":56617},{\"end\":56644,\"start\":56636},{\"end\":56661,\"start\":56656},{\"end\":56681,\"start\":56673},{\"end\":56991,\"start\":56982},{\"end\":57002,\"start\":56996},{\"end\":57013,\"start\":57009},{\"end\":57028,\"start\":57020},{\"end\":57038,\"start\":57034},{\"end\":57047,\"start\":57043},{\"end\":57476,\"start\":57472},{\"end\":57490,\"start\":57484},{\"end\":57506,\"start\":57500},{\"end\":57520,\"start\":57514},{\"end\":57718,\"start\":57708},{\"end\":57733,\"start\":57725},{\"end\":57746,\"start\":57738},{\"end\":57761,\"start\":57753},{\"end\":57772,\"start\":57767},{\"end\":58085,\"start\":58075},{\"end\":58100,\"start\":58092},{\"end\":58114,\"start\":58106},{\"end\":58127,\"start\":58119},{\"end\":58139,\"start\":58134}]", "bib_author_last_name": "[{\"end\":38421,\"start\":38416},{\"end\":38436,\"start\":38430},{\"end\":38452,\"start\":38448},{\"end\":38877,\"start\":38870},{\"end\":38893,\"start\":38885},{\"end\":38907,\"start\":38902},{\"end\":38919,\"start\":38915},{\"end\":39542,\"start\":39539},{\"end\":39559,\"start\":39550},{\"end\":39574,\"start\":39568},{\"end\":39590,\"start\":39582},{\"end\":39604,\"start\":39599},{\"end\":39620,\"start\":39612},{\"end\":39637,\"start\":39626},{\"end\":39984,\"start\":39979},{\"end\":40000,\"start\":39996},{\"end\":40011,\"start\":40008},{\"end\":40025,\"start\":40020},{\"end\":40040,\"start\":40034},{\"end\":40316,\"start\":40310},{\"end\":40331,\"start\":40326},{\"end\":40348,\"start\":40343},{\"end\":40362,\"start\":40355},{\"end\":40380,\"start\":40371},{\"end\":40398,\"start\":40390},{\"end\":40410,\"start\":40404},{\"end\":40423,\"start\":40419},{\"end\":40438,\"start\":40431},{\"end\":40983,\"start\":40972},{\"end\":40995,\"start\":40992},{\"end\":41013,\"start\":41004},{\"end\":41030,\"start\":41025},{\"end\":41046,\"start\":41040},{\"end\":41608,\"start\":41601},{\"end\":41622,\"start\":41615},{\"end\":41642,\"start\":41633},{\"end\":41649,\"start\":41647},{\"end\":41663,\"start\":41656},{\"end\":41677,\"start\":41673},{\"end\":41691,\"start\":41684},{\"end\":41708,\"start\":41703},{\"end\":41719,\"start\":41715},{\"end\":42261,\"start\":42259},{\"end\":42275,\"start\":42271},{\"end\":42287,\"start\":42283},{\"end\":42300,\"start\":42298},{\"end\":42316,\"start\":42307},{\"end\":42328,\"start\":42323},{\"end\":42338,\"start\":42335},{\"end\":42353,\"start\":42349},{\"end\":42364,\"start\":42360},{\"end\":42377,\"start\":42374},{\"end\":42390,\"start\":42386},{\"end\":42656,\"start\":42653},{\"end\":42669,\"start\":42665},{\"end\":43095,\"start\":43091},{\"end\":43107,\"start\":43104},{\"end\":43120,\"start\":43117},{\"end\":43135,\"start\":43129},{\"end\":43154,\"start\":43151},{\"end\":43564,\"start\":43559},{\"end\":43581,\"start\":43573},{\"end\":43594,\"start\":43590},{\"end\":43607,\"start\":43602},{\"end\":43621,\"start\":43618},{\"end\":43634,\"start\":43629},{\"end\":43642,\"start\":43640},{\"end\":43654,\"start\":43652},{\"end\":44061,\"start\":44055},{\"end\":44077,\"start\":44071},{\"end\":44340,\"start\":44336},{\"end\":44350,\"start\":44346},{\"end\":44364,\"start\":44362},{\"end\":44380,\"start\":44373},{\"end\":44613,\"start\":44605},{\"end\":44627,\"start\":44621},{\"end\":44631,\"start\":44629},{\"end\":44869,\"start\":44859},{\"end\":44885,\"start\":44876},{\"end\":44904,\"start\":44898},{\"end\":45169,\"start\":45162},{\"end\":45187,\"start\":45177},{\"end\":45200,\"start\":45194},{\"end\":45638,\"start\":45635},{\"end\":45651,\"start\":45645},{\"end\":45925,\"start\":45911},{\"end\":45951,\"start\":45946},{\"end\":45964,\"start\":45960},{\"end\":45980,\"start\":45973},{\"end\":46385,\"start\":46379},{\"end\":46398,\"start\":46392},{\"end\":46413,\"start\":46406},{\"end\":46428,\"start\":46423},{\"end\":46858,\"start\":46847},{\"end\":46876,\"start\":46868},{\"end\":47498,\"start\":47493},{\"end\":47517,\"start\":47511},{\"end\":47535,\"start\":47527},{\"end\":47885,\"start\":47879},{\"end\":47896,\"start\":47891},{\"end\":47913,\"start\":47908},{\"end\":47925,\"start\":47920},{\"end\":47941,\"start\":47933},{\"end\":47957,\"start\":47951},{\"end\":47973,\"start\":47966},{\"end\":47985,\"start\":47982},{\"end\":48005,\"start\":47995},{\"end\":48018,\"start\":48012},{\"end\":48035,\"start\":48026},{\"end\":48049,\"start\":48045},{\"end\":48062,\"start\":48058},{\"end\":48078,\"start\":48072},{\"end\":48093,\"start\":48087},{\"end\":48109,\"start\":48103},{\"end\":48130,\"start\":48118},{\"end\":48146,\"start\":48139},{\"end\":48155,\"start\":48151},{\"end\":48167,\"start\":48164},{\"end\":48185,\"start\":48177},{\"end\":49012,\"start\":49003},{\"end\":49024,\"start\":49019},{\"end\":49035,\"start\":49032},{\"end\":49048,\"start\":49044},{\"end\":49060,\"start\":49057},{\"end\":49079,\"start\":49074},{\"end\":49094,\"start\":49090},{\"end\":49110,\"start\":49106},{\"end\":49123,\"start\":49116},{\"end\":49432,\"start\":49426},{\"end\":49444,\"start\":49439},{\"end\":49460,\"start\":49454},{\"end\":49477,\"start\":49472},{\"end\":49719,\"start\":49713},{\"end\":49742,\"start\":49729},{\"end\":49759,\"start\":49751},{\"end\":49772,\"start\":49765},{\"end\":50272,\"start\":50260},{\"end\":50283,\"start\":50281},{\"end\":50298,\"start\":50290},{\"end\":50303,\"start\":50300},{\"end\":50510,\"start\":50502},{\"end\":50523,\"start\":50520},{\"end\":50539,\"start\":50531},{\"end\":50548,\"start\":50541},{\"end\":51049,\"start\":51043},{\"end\":51068,\"start\":51057},{\"end\":51080,\"start\":51075},{\"end\":51098,\"start\":51092},{\"end\":51656,\"start\":51652},{\"end\":51670,\"start\":51667},{\"end\":51684,\"start\":51680},{\"end\":51699,\"start\":51696},{\"end\":51995,\"start\":51991},{\"end\":52010,\"start\":52005},{\"end\":52030,\"start\":52019},{\"end\":52048,\"start\":52040},{\"end\":52066,\"start\":52057},{\"end\":52078,\"start\":52073},{\"end\":52556,\"start\":52550},{\"end\":52570,\"start\":52565},{\"end\":52581,\"start\":52572},{\"end\":52945,\"start\":52942},{\"end\":52959,\"start\":52956},{\"end\":52969,\"start\":52965},{\"end\":53171,\"start\":53164},{\"end\":53189,\"start\":53182},{\"end\":53205,\"start\":53196},{\"end\":53217,\"start\":53212},{\"end\":53232,\"start\":53227},{\"end\":53250,\"start\":53240},{\"end\":53262,\"start\":53256},{\"end\":53785,\"start\":53783},{\"end\":53797,\"start\":53794},{\"end\":53820,\"start\":53809},{\"end\":53832,\"start\":53827},{\"end\":53843,\"start\":53839},{\"end\":53855,\"start\":53853},{\"end\":53868,\"start\":53863},{\"end\":53879,\"start\":53872},{\"end\":54330,\"start\":54328},{\"end\":54343,\"start\":54339},{\"end\":54355,\"start\":54353},{\"end\":54369,\"start\":54364},{\"end\":54384,\"start\":54380},{\"end\":54393,\"start\":54389},{\"end\":54665,\"start\":54663},{\"end\":54678,\"start\":54675},{\"end\":54693,\"start\":54688},{\"end\":54708,\"start\":54699},{\"end\":55365,\"start\":55362},{\"end\":55378,\"start\":55374},{\"end\":55394,\"start\":55389},{\"end\":55406,\"start\":55402},{\"end\":55418,\"start\":55415},{\"end\":55433,\"start\":55427},{\"end\":55901,\"start\":55899},{\"end\":55916,\"start\":55911},{\"end\":55927,\"start\":55924},{\"end\":55939,\"start\":55936},{\"end\":55952,\"start\":55949},{\"end\":55965,\"start\":55961},{\"end\":55978,\"start\":55974},{\"end\":55991,\"start\":55987},{\"end\":56000,\"start\":55997},{\"end\":56634,\"start\":56626},{\"end\":56654,\"start\":56645},{\"end\":56671,\"start\":56662},{\"end\":56687,\"start\":56682},{\"end\":56994,\"start\":56992},{\"end\":57007,\"start\":57003},{\"end\":57018,\"start\":57014},{\"end\":57032,\"start\":57029},{\"end\":57041,\"start\":57039},{\"end\":57052,\"start\":57048},{\"end\":57061,\"start\":57054},{\"end\":57482,\"start\":57477},{\"end\":57498,\"start\":57491},{\"end\":57512,\"start\":57507},{\"end\":57525,\"start\":57521},{\"end\":57723,\"start\":57719},{\"end\":57736,\"start\":57734},{\"end\":57751,\"start\":57747},{\"end\":57765,\"start\":57762},{\"end\":57776,\"start\":57773},{\"end\":58090,\"start\":58086},{\"end\":58104,\"start\":58101},{\"end\":58117,\"start\":58115},{\"end\":58132,\"start\":58128},{\"end\":58143,\"start\":58140}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4670132},\"end\":38820,\"start\":38335},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b1\",\"matched_paper_id\":2645819},\"end\":39390,\"start\":38822},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":229702457},\"end\":39969,\"start\":39392},{\"attributes\":{\"doi\":\"arXiv:1712.09665\",\"id\":\"b3\"},\"end\":40238,\"start\":39971},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":502946},\"end\":40923,\"start\":40240},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":5550767},\"end\":41470,\"start\":40925},{\"attributes\":{\"id\":\"b6\"},\"end\":41523,\"start\":41472},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":29162614},\"end\":42198,\"start\":41525},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3879949},\"end\":42645,\"start\":42200},{\"attributes\":{\"id\":\"b9\"},\"end\":42986,\"start\":42647},{\"attributes\":{\"id\":\"b10\"},\"end\":43408,\"start\":42988},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":198967636},\"end\":43982,\"start\":43410},{\"attributes\":{\"id\":\"b12\"},\"end\":44256,\"start\":43984},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":211244412},\"end\":44557,\"start\":44258},{\"attributes\":{\"id\":\"b14\"},\"end\":44787,\"start\":44559},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":195908774},\"end\":45109,\"start\":44789},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1257772},\"end\":45574,\"start\":45111},{\"attributes\":{\"id\":\"b17\"},\"end\":45832,\"start\":45576},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5737015},\"end\":46369,\"start\":45834},{\"attributes\":{\"id\":\"b19\"},\"end\":46764,\"start\":46371},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":208512887},\"end\":47425,\"start\":46766},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":231718738},\"end\":47802,\"start\":47427},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202786778},\"end\":48925,\"start\":47804},{\"attributes\":{\"id\":\"b23\"},\"end\":49393,\"start\":48927},{\"attributes\":{\"id\":\"b24\"},\"end\":49647,\"start\":49395},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206594738},\"end\":50170,\"start\":49649},{\"attributes\":{\"id\":\"b26\"},\"end\":50491,\"start\":50172},{\"attributes\":{\"id\":\"b27\"},\"end\":50945,\"start\":50493},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":207241700},\"end\":51574,\"start\":50947},{\"attributes\":{\"id\":\"b29\"},\"end\":51898,\"start\":51576},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":53533016},\"end\":52539,\"start\":51900},{\"attributes\":{\"id\":\"b31\"},\"end\":52886,\"start\":52541},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":53617362},\"end\":53110,\"start\":52888},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":604334},\"end\":53708,\"start\":53112},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":214743069},\"end\":54238,\"start\":53710},{\"attributes\":{\"id\":\"b35\"},\"end\":54572,\"start\":54240},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":207757900},\"end\":55284,\"start\":54574},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3350285},\"end\":55824,\"start\":55286},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":208310168},\"end\":56615,\"start\":55826},{\"attributes\":{\"id\":\"b39\"},\"end\":56980,\"start\":56617},{\"attributes\":{\"id\":\"b40\"},\"end\":57367,\"start\":56982},{\"attributes\":{\"id\":\"b41\"},\"end\":57706,\"start\":57369},{\"attributes\":{\"id\":\"b42\"},\"end\":58073,\"start\":57708},{\"attributes\":{\"id\":\"b43\"},\"end\":58362,\"start\":58075}]", "bib_title": "[{\"end\":38407,\"start\":38335},{\"end\":38862,\"start\":38822},{\"end\":39529,\"start\":39392},{\"end\":40301,\"start\":40240},{\"end\":40963,\"start\":40925},{\"end\":41593,\"start\":41525},{\"end\":42248,\"start\":42200},{\"end\":43549,\"start\":43410},{\"end\":44331,\"start\":44258},{\"end\":44852,\"start\":44789},{\"end\":45153,\"start\":45111},{\"end\":45905,\"start\":45834},{\"end\":46837,\"start\":46766},{\"end\":47482,\"start\":47427},{\"end\":47872,\"start\":47804},{\"end\":48994,\"start\":48927},{\"end\":49704,\"start\":49649},{\"end\":51033,\"start\":50947},{\"end\":51977,\"start\":51900},{\"end\":52937,\"start\":52888},{\"end\":53152,\"start\":53112},{\"end\":53779,\"start\":53710},{\"end\":54654,\"start\":54574},{\"end\":55353,\"start\":55286},{\"end\":55891,\"start\":55826}]", "bib_author": "[{\"end\":38423,\"start\":38409},{\"end\":38438,\"start\":38423},{\"end\":38454,\"start\":38438},{\"end\":38879,\"start\":38864},{\"end\":38895,\"start\":38879},{\"end\":38909,\"start\":38895},{\"end\":38921,\"start\":38909},{\"end\":39544,\"start\":39531},{\"end\":39561,\"start\":39544},{\"end\":39576,\"start\":39561},{\"end\":39592,\"start\":39576},{\"end\":39606,\"start\":39592},{\"end\":39622,\"start\":39606},{\"end\":39639,\"start\":39622},{\"end\":39986,\"start\":39973},{\"end\":40002,\"start\":39986},{\"end\":40013,\"start\":40002},{\"end\":40027,\"start\":40013},{\"end\":40042,\"start\":40027},{\"end\":40318,\"start\":40303},{\"end\":40333,\"start\":40318},{\"end\":40350,\"start\":40333},{\"end\":40364,\"start\":40350},{\"end\":40382,\"start\":40364},{\"end\":40400,\"start\":40382},{\"end\":40412,\"start\":40400},{\"end\":40425,\"start\":40412},{\"end\":40440,\"start\":40425},{\"end\":40985,\"start\":40965},{\"end\":40997,\"start\":40985},{\"end\":41015,\"start\":40997},{\"end\":41032,\"start\":41015},{\"end\":41048,\"start\":41032},{\"end\":41610,\"start\":41595},{\"end\":41624,\"start\":41610},{\"end\":41644,\"start\":41624},{\"end\":41651,\"start\":41644},{\"end\":41665,\"start\":41651},{\"end\":41679,\"start\":41665},{\"end\":41693,\"start\":41679},{\"end\":41710,\"start\":41693},{\"end\":41721,\"start\":41710},{\"end\":42263,\"start\":42250},{\"end\":42277,\"start\":42263},{\"end\":42289,\"start\":42277},{\"end\":42302,\"start\":42289},{\"end\":42318,\"start\":42302},{\"end\":42330,\"start\":42318},{\"end\":42340,\"start\":42330},{\"end\":42355,\"start\":42340},{\"end\":42366,\"start\":42355},{\"end\":42379,\"start\":42366},{\"end\":42392,\"start\":42379},{\"end\":42658,\"start\":42647},{\"end\":42671,\"start\":42658},{\"end\":43097,\"start\":43083},{\"end\":43109,\"start\":43097},{\"end\":43122,\"start\":43109},{\"end\":43137,\"start\":43122},{\"end\":43144,\"start\":43137},{\"end\":43156,\"start\":43144},{\"end\":43566,\"start\":43551},{\"end\":43583,\"start\":43566},{\"end\":43596,\"start\":43583},{\"end\":43609,\"start\":43596},{\"end\":43623,\"start\":43609},{\"end\":43636,\"start\":43623},{\"end\":43644,\"start\":43636},{\"end\":43656,\"start\":43644},{\"end\":44063,\"start\":44045},{\"end\":44079,\"start\":44063},{\"end\":44342,\"start\":44333},{\"end\":44352,\"start\":44342},{\"end\":44366,\"start\":44352},{\"end\":44382,\"start\":44366},{\"end\":44615,\"start\":44603},{\"end\":44629,\"start\":44615},{\"end\":44633,\"start\":44629},{\"end\":44871,\"start\":44854},{\"end\":44887,\"start\":44871},{\"end\":44906,\"start\":44887},{\"end\":45171,\"start\":45155},{\"end\":45189,\"start\":45171},{\"end\":45202,\"start\":45189},{\"end\":45640,\"start\":45630},{\"end\":45653,\"start\":45640},{\"end\":45927,\"start\":45907},{\"end\":45953,\"start\":45927},{\"end\":45966,\"start\":45953},{\"end\":45982,\"start\":45966},{\"end\":46387,\"start\":46371},{\"end\":46400,\"start\":46387},{\"end\":46415,\"start\":46400},{\"end\":46430,\"start\":46415},{\"end\":46860,\"start\":46839},{\"end\":46878,\"start\":46860},{\"end\":47500,\"start\":47484},{\"end\":47519,\"start\":47500},{\"end\":47537,\"start\":47519},{\"end\":47887,\"start\":47874},{\"end\":47898,\"start\":47887},{\"end\":47915,\"start\":47898},{\"end\":47927,\"start\":47915},{\"end\":47943,\"start\":47927},{\"end\":47959,\"start\":47943},{\"end\":47975,\"start\":47959},{\"end\":47987,\"start\":47975},{\"end\":48007,\"start\":47987},{\"end\":48020,\"start\":48007},{\"end\":48037,\"start\":48020},{\"end\":48051,\"start\":48037},{\"end\":48064,\"start\":48051},{\"end\":48080,\"start\":48064},{\"end\":48095,\"start\":48080},{\"end\":48111,\"start\":48095},{\"end\":48132,\"start\":48111},{\"end\":48148,\"start\":48132},{\"end\":48157,\"start\":48148},{\"end\":48169,\"start\":48157},{\"end\":48187,\"start\":48169},{\"end\":49014,\"start\":48996},{\"end\":49026,\"start\":49014},{\"end\":49037,\"start\":49026},{\"end\":49050,\"start\":49037},{\"end\":49062,\"start\":49050},{\"end\":49081,\"start\":49062},{\"end\":49096,\"start\":49081},{\"end\":49112,\"start\":49096},{\"end\":49125,\"start\":49112},{\"end\":49434,\"start\":49419},{\"end\":49446,\"start\":49434},{\"end\":49462,\"start\":49446},{\"end\":49479,\"start\":49462},{\"end\":49721,\"start\":49706},{\"end\":49744,\"start\":49721},{\"end\":49761,\"start\":49744},{\"end\":49774,\"start\":49761},{\"end\":50274,\"start\":50252},{\"end\":50285,\"start\":50274},{\"end\":50300,\"start\":50285},{\"end\":50305,\"start\":50300},{\"end\":50512,\"start\":50493},{\"end\":50525,\"start\":50512},{\"end\":50541,\"start\":50525},{\"end\":50550,\"start\":50541},{\"end\":51051,\"start\":51035},{\"end\":51070,\"start\":51051},{\"end\":51082,\"start\":51070},{\"end\":51100,\"start\":51082},{\"end\":51658,\"start\":51644},{\"end\":51672,\"start\":51658},{\"end\":51686,\"start\":51672},{\"end\":51701,\"start\":51686},{\"end\":51997,\"start\":51979},{\"end\":52012,\"start\":51997},{\"end\":52032,\"start\":52012},{\"end\":52050,\"start\":52032},{\"end\":52068,\"start\":52050},{\"end\":52080,\"start\":52068},{\"end\":52558,\"start\":52541},{\"end\":52572,\"start\":52558},{\"end\":52583,\"start\":52572},{\"end\":52947,\"start\":52939},{\"end\":52961,\"start\":52947},{\"end\":52971,\"start\":52961},{\"end\":53173,\"start\":53154},{\"end\":53191,\"start\":53173},{\"end\":53207,\"start\":53191},{\"end\":53219,\"start\":53207},{\"end\":53234,\"start\":53219},{\"end\":53252,\"start\":53234},{\"end\":53264,\"start\":53252},{\"end\":53787,\"start\":53781},{\"end\":53799,\"start\":53787},{\"end\":53822,\"start\":53799},{\"end\":53834,\"start\":53822},{\"end\":53845,\"start\":53834},{\"end\":53857,\"start\":53845},{\"end\":53870,\"start\":53857},{\"end\":53881,\"start\":53870},{\"end\":54332,\"start\":54323},{\"end\":54345,\"start\":54332},{\"end\":54357,\"start\":54345},{\"end\":54371,\"start\":54357},{\"end\":54386,\"start\":54371},{\"end\":54395,\"start\":54386},{\"end\":54667,\"start\":54656},{\"end\":54680,\"start\":54667},{\"end\":54695,\"start\":54680},{\"end\":54710,\"start\":54695},{\"end\":55367,\"start\":55355},{\"end\":55380,\"start\":55367},{\"end\":55396,\"start\":55380},{\"end\":55408,\"start\":55396},{\"end\":55420,\"start\":55408},{\"end\":55435,\"start\":55420},{\"end\":55903,\"start\":55893},{\"end\":55918,\"start\":55903},{\"end\":55929,\"start\":55918},{\"end\":55941,\"start\":55929},{\"end\":55954,\"start\":55941},{\"end\":55967,\"start\":55954},{\"end\":55980,\"start\":55967},{\"end\":55993,\"start\":55980},{\"end\":56002,\"start\":55993},{\"end\":56636,\"start\":56617},{\"end\":56656,\"start\":56636},{\"end\":56673,\"start\":56656},{\"end\":56689,\"start\":56673},{\"end\":56996,\"start\":56982},{\"end\":57009,\"start\":56996},{\"end\":57020,\"start\":57009},{\"end\":57034,\"start\":57020},{\"end\":57043,\"start\":57034},{\"end\":57054,\"start\":57043},{\"end\":57063,\"start\":57054},{\"end\":57484,\"start\":57472},{\"end\":57500,\"start\":57484},{\"end\":57514,\"start\":57500},{\"end\":57527,\"start\":57514},{\"end\":57725,\"start\":57708},{\"end\":57738,\"start\":57725},{\"end\":57753,\"start\":57738},{\"end\":57767,\"start\":57753},{\"end\":57778,\"start\":57767},{\"end\":58092,\"start\":58075},{\"end\":58106,\"start\":58092},{\"end\":58119,\"start\":58106},{\"end\":58134,\"start\":58119},{\"end\":58145,\"start\":58134}]", "bib_venue": "[{\"end\":38595,\"start\":38533},{\"end\":39078,\"start\":39025},{\"end\":40534,\"start\":40516},{\"end\":41119,\"start\":41089},{\"end\":41809,\"start\":41786},{\"end\":45274,\"start\":45260},{\"end\":46045,\"start\":46032},{\"end\":47008,\"start\":46997},{\"end\":49857,\"start\":49839},{\"end\":50688,\"start\":50664},{\"end\":51271,\"start\":51187},{\"end\":52241,\"start\":52169},{\"end\":53339,\"start\":53322},{\"end\":54840,\"start\":54829},{\"end\":55498,\"start\":55485},{\"end\":56132,\"start\":56121},{\"end\":38531,\"start\":38454},{\"end\":38993,\"start\":38925},{\"end\":39663,\"start\":39639},{\"end\":40092,\"start\":40075},{\"end\":40514,\"start\":40440},{\"end\":41087,\"start\":41048},{\"end\":41485,\"start\":41472},{\"end\":41784,\"start\":41721},{\"end\":42408,\"start\":42392},{\"end\":42805,\"start\":42687},{\"end\":43081,\"start\":42988},{\"end\":43679,\"start\":43656},{\"end\":44043,\"start\":43984},{\"end\":44393,\"start\":44382},{\"end\":44601,\"start\":44559},{\"end\":44931,\"start\":44906},{\"end\":45258,\"start\":45202},{\"end\":45628,\"start\":45576},{\"end\":46030,\"start\":45982},{\"end\":46556,\"start\":46463},{\"end\":46930,\"start\":46878},{\"end\":47602,\"start\":47553},{\"end\":48236,\"start\":48187},{\"end\":49141,\"start\":49125},{\"end\":49417,\"start\":49395},{\"end\":49837,\"start\":49774},{\"end\":50250,\"start\":50172},{\"end\":50662,\"start\":50550},{\"end\":51185,\"start\":51100},{\"end\":51642,\"start\":51576},{\"end\":52167,\"start\":52080},{\"end\":52694,\"start\":52616},{\"end\":52984,\"start\":52971},{\"end\":53320,\"start\":53264},{\"end\":53955,\"start\":53881},{\"end\":54321,\"start\":54240},{\"end\":54762,\"start\":54710},{\"end\":55483,\"start\":55435},{\"end\":56054,\"start\":56002},{\"end\":56785,\"start\":56705},{\"end\":57161,\"start\":57079},{\"end\":57470,\"start\":57369},{\"end\":57877,\"start\":57794},{\"end\":58206,\"start\":58161}]"}}}, "year": 2023, "month": 12, "day": 17}