{"id": 263869520, "updated": "2023-10-13 03:14:28.002", "metadata": {"title": "Resource-aware Federated Learning using Knowledge Extraction and Multi-model Fusion", "authors": "[{\"first\":\"Sixing\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Qian\",\"middle\":[]},{\"first\":\"Ali\",\"last\":\"Jannesari\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "With increasing concern about user data privacy, federated learning (FL) has been developed as a unique training paradigm for training machine learning models on edge devices without access to sensitive data. Traditional FL and existing methods directly employ aggregation methods on all edges of the same models and training devices for a cloud server. Although these methods protect data privacy, they are not capable of model heterogeneity, even ignore the heterogeneous computing power, and incur steep communication costs. In this paper, we purpose a resource-aware FL to aggregate an ensemble of local knowledge extracted from edge models, instead of aggregating the weights of each local model, which is then distilled into a robust global knowledge as the server model through knowledge distillation. The local model and the global knowledge are extracted into a tiny size knowledge network by deep mutual learning. Such knowledge extraction allows the edge client to deploy a resource-aware model and perform multi-model knowledge fusion while maintaining communication ef\ufb01ciency and model heterogeneity. Empirical results show that our approach has sig-ni\ufb01cantly improved over existing FL algorithms in terms of communication cost and generalization performance in heterogeneous data and models. Our approach reduces the communication cost of VGG-11 by up to 102 \u00d7 and ResNet-32 by up to 30 \u00d7 when training ResNet-20 as the knowledge network.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2208-07978", "doi": "10.48550/arxiv.2208.07978"}}, "content": {"source": {"pdf_hash": "98fd8b8476bfa9da924c86543141a6fb920cfc4c", "pdf_src": "ScienceParsePlus", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8383b8b7d169d8ab9444573b024ab965602f548b", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/98fd8b8476bfa9da924c86543141a6fb920cfc4c.txt", "contents": "\nResource-aware Federated Learning using Knowledge Extraction and Multi-model Fusion\n16 Aug 2022\n\nSixing Yu \nIowa State University\n\n\nWei Qian \nIowa State University\n\n\nAli Jannesari jannesar@iastate.edu \nIowa State University\n\n\nResource-aware Federated Learning using Knowledge Extraction and Multi-model Fusion\n16 Aug 2022CB38E7B7CE8CC93564F439877F56FA5EarXiv:2208.07978v1[cs.DC]\nWith increasing concern about user data privacy, federated learning (FL) has been developed as a unique training paradigm for training machine learning models on edge devices without access to sensitive data.Traditional FL and existing methods directly employ aggregation methods on all edges of the same models and training devices for a cloud server.Although these methods protect data privacy, they are not capable of model heterogeneity, even ignore the heterogeneous computing power, and incur steep communication costs.In this paper, we purpose a resource-aware FL to aggregate an ensemble of local knowledge extracted from edge models, instead of aggregating the weights of each local model, which is then distilled into a robust global knowledge as the server model through knowledge distillation.The local model and the global knowledge are extracted into a tiny size knowledge network by deep mutual learning.Such knowledge extraction allows the edge client to deploy a resourceaware model and perform multi-model knowledge fusion while maintaining communication efficiency and model heterogeneity.Empirical results show that our approach has significantly improved over existing FL algorithms in terms of communication cost and generalization performance in heterogeneous data and models.Our approach reduces the communication cost of VGG-11 by up to 102\u00d7 and ResNet-32 by up to 30\u00d7 when training ResNet-20 as the knowledge network.\n\nIntroduction\n\nFederated learning (FL) has emerged as a novel machine learning paradigm for distributed clients to participate in the collaborative training of a centralized model.FL brings model asynchronous training on edge, where devices (e.g., mobile phones and IoT devices) extract the knowledge on the private-sensitive training data and then upload the learned models to the cloud for aggregation.FL stores user data locally and restricts direct access to it from cloud servers; thereby, this paradigm not only enhances privacypreserving but also introduces several inherent advantages, including model accuracy, cost efficiency, and diversity.With the massive demand for data in today's machine learning models and the social considerations of artificial intelligence (e.g., privacy and security (Yang et al. 2019;Curzon et al. 2021)), federated learning has great potential and role in counterpoising this trade-off.\n\nFederated learning has already shown its potential in practical applications, including health care (Sheller et al. 2020), environment protection (Hu et al. 2018), electrical vehicles (Saputra et al. 2019), and etc. Google, Apple, and NVIDIA are using FL for their applications nowadays (e.g., Google Keyboard (Yang et al. 2018;Chen et al. 2019), Apple Siri (Freudiger 2019;Paulik et al. 2021), NVIDIA medical imaging (Li et al. 2019)).In consequence, designing efficient FL models and deploying them effectively and fairly on edge devices is crucial for improving the performance of edge computing in the future.\n\nTraditional FL, represented by FedAvg (McMahan et al. 2017) in Figure 1 (b), broadcasts global model parameters to selected edge devices, performs a weighted average of the trained local model parameters based on their local data and updates the global model.This process is then iterated through many communication rounds to achieve good performance across all edge devices.From the framework of traditional FL, we observe the following limitations: First, iterative model weights sharing between network edges and the servers introduces excessive communication overhead.Second, the ever-increasing computational and memory requirements of AI models (e.g., deep neural networks -DNNs) and the heterogeneous computing power of edge devices make deploying the same models on resource-constrained edge devices problematic.For example, it is infeasible to deploy a large model on a resourcelimited edge device.In addition, local data from the realworld is typically imbalance/non-independent identically distributed (non-IID), which can easily lead to training failure in a decentralized situation (Michieli and Ozay 2021).Deep learning models are generally over-parameterized, and when local data are heterogeneous, aggregating local models tends to cause over-fitting, leading to high variance in learning and prediction (Jiang et al. 2019;Nishio and Yonetani 2019).Therefore, performing weight-average aggregating of local models, or even deploying the same model on devices with different computing power, as employed by most existing FL methods, may produce an unfair, ineffective global model and fail to deploy it efficiently.\n\nTo overcome the above limitations of previous FL methods, we came up with the idea of performing resource-aware federated learning using knowledge extraction and multimodel fusion (FedKEMF), as illustrated in Figure 1 (a).The benefits of our approach FedKEMF are: i) It distills the client's knowledge before aggregating it to the server, which prevents such large edge models from being over-parameterized while reducing the resource constraints of edge devices.ii) It effectively reduces communication costs by exchanging distilled tiny size neural networks between edges and clouds instead of the original large models.iii) Ensembling knowledge from edges efficiently robust the global model, reducing the risk of over-fitting and variance, and achieving better generalization performance in FL.Furthermore, our purposed approach is aware of resourceconstraint and applies multi-model fusion to break the limitations of model structure, deploy models fairly on edge devices, and enable federated learning in a more realistic setting.\n\nWe conduct extensive experiments on non-IID data settings and heterogeneous client models to validate the performance of the purposed approach and compare it with existing FL methods.Our results show that, when optimizing a small neural network as the knowledge network, FedKEMF significantly reduces communication cost and achieves better performance using fewer communication rounds.Another line in FL is personalized FL, which focuses on the problem of statistical heterogeneity.Personalized FL aims to personalize the global model for each client in FL and find how to develop improved personalized models that can benefit a large majority of clients (Kulkarni, Kulkarni, and Pant 2020).Although we have the same consideration of device heterogeneity (memory storage and computation power), data heterogeneity (non-IID data), and model heterogeneity (model structure and size), we focus on how to extract knowledge from different types of models and their corresponding training devices to build robust global knowledge.\n\n\nRelated work Federated Learning\n\n\nEnsemble Learning and Knowledge Distillation in Federated Learning\n\nKnowledge distillation is first introduced as a model compression technique for neural networks to transfer knowledge from a large teacher model to a small student model (Buciluundefined, Caruana, and Niculescu-Mizil 2006;Hinton, Vinyals, and Dean 2015).Ensemble learning is a promising technique to combine several individual models for better generalization performance (Ganaie et al. 2022).In light of these two ideas, such methods (You et al. 2017;Fukuda et al. 2017;Tung and Mori 2019; Park and Kwak  2020; Asif, Tang, and Harrer 2020) purpose efficient knowledge distillation with ensemble of teachers to further improve the student performance.\n\nRecently, ensemble learning and knowledge distillation has emerged as an effective approach to address heterogeneity issue, resource-constrained of edge devices, and communication efficiency in FL (Li and Wang 2019;He, Annavaram, and Avestimehr 2020;Seo et al. 2020;Sui et al. 2020;Lin et al. 2020).For example, Fed-ensemble (Sui et al. 2020) ensembles the prediction output of all client models; FedKD (Wu et al. 2021) proposes an adaptive mutual distillation to learn a student and a teacher model simultaneously on the client side; FedDF (Lin et al. 2020) distills the ensemble of client teacher models to a server student model.In contrast, our approach utilizes knowledge distillation to encode the ensemble knowledge from clients into global knowledge.The novelty of our approach is that FedKEMF extracts the local model knowledge that is being learned from the global knowledge into a tiny size neural network by deep mutual learning (Zhang et al. 2018), and then ensembles the tiny size neural networks for multi-model fusion.\n\n\nMethodology\n\nFigure 2 shows the local updates of FedKEMF and Figure 3 shows the cloud updates of FedKEMF.In local updates, the client first downloads the knowledge network from the server, then mutually trains the knowledge network with the local model to extract an updated knowledge network, and finally uploads it back to the server.In cloud updates, the server first collects local knowledge (tiny size network) uploaded from clients, then ensembles all the tiny size networks, distills them into global knowledge, and finally transfers it to clients.In this section, we will explain our approach in detail.\n\n\nKnowledge Extraction using Deep Mutual Learning\n\nTraditional FL and its variants (Li et al. 2020a;Wang et al. 2020;Karimireddy et al. 2020) keep model up-to-date by sharing model weights/gradients between server and edge clients.Simply aggregating weights might raise unexpected training failures.We aim to fusion the local model's knowledge to keep the model updated.Hence, we use deep mutual learning (Zhang et al. 2018), to extract the knowledge.\n\nThe Key idea of deep mutual learning is to train multiple neural networks (NNs) synchronously while minimizing the Kullback Leibler (KL) divergence among the output of the  To intuitively explain the knowledge extraction process, we explain it in an image classification task.Formally, in an edge client, we have a local model \u03b8 and a knowledge network (tiny size network) \u03b8 g .First, to update the local model \u03b8.For any input batch of data x, we calculate the crossentropy loss of predictions and ground truth as equation 1.\nL c = \u2212 N i=1 y T log(\u03c3(\u03b8(x i )))(1)\n, where N is the mini-batch size, y is the ground truth label, and \u03c3 is the softmax function.\n\nThen, we compute the KL divergence from \u03b8(x) to \u03b8 g (x)\n\nAlgorithm\n\u03b8 \u2190 \u03b8 \u2212 \u03b7 (L(\u03b8; b) + D KL (\u03b8 g ||\u03b8))\n7:\n\u03b8 g \u2190 \u03b8 g \u2212 \u03b7 (L(\u03b8 g ; b) + D KL (\u03b8||\u03b8 g )) 8:\nend for 9: end for 10: Deploy \u03b8 on local application.11: Communicate \u03b8 g .as equation 2.\nD KL (\u03b8 g ||\u03b8) = N i=1 \u03c3(\u03b8 g (x) T )log( \u03c3(\u03b8 g (x)) \u03c3(\u03b8(x)) )(2)\n, where we do element-wise division on \u03c3(\u03b8g(x)) \u03c3(\u03b8(x)) .We update the \u03b8 by using the total loss as equation 3.\nL \u03b8 = L c + D KL (\u03b8 g ||\u03b8)(3)\nSimilar steps are followed by \u03b8 g .\n\n\nLocal Updates Through Deep Mutual Learning\n\nAs depicted in Figure 2\n\n\nMulti-model Knowledge Fusion\n\nIn FedKEMF, we provide two model fusion methods for server fusion of the knowledge from the edge.The first one is similar to the traditional FL in that we aggregate the weight.Second, inspired by (Lin et al. 2020), we ensemble all received client models and distillate the ensemble knowledge into a global knowledge network.In this section and the experiments, we mainly focus on ensemble the client's knowledge.However, FedKEMF can also use traditional fusion method, such as (McMahan et al. 2017;Karimireddy et al. 2020), to aggregate model.\n\nWe define the ensemble model as \u0398 = {\u03b8 k g } k\u2208S , where the \u03b8 k g is the k th client's knowledge network and the S is the set of clients that communicate with the server in current communication round.Then we distillate the knowledge of ensemble \u0398 to a global knowledge network \u03b8 g by using unlabeled data, generative data, or public data in the server.The distillation loss for \u03b8 g is defined in equation 4.\nL d = D KL (\u0398, \u03b8 g )(4)\nThe server update process is shown in the Algorithm 2.\n\nAlgorithm 2: FedKEMF Server Update Server executes:\n\n1: Initialize \u03b8 g .2: for each round t = 1, 2, . . ., T do 3:\n\nS \u2190 random set of clients \u2208 N 4:\n\nfor each client k \u2208 S in parallel do 5:\n\nCommunicate with client k.\n\n6:\n\u03b8 k g \u2190 ClientUpdate(\u03b8 g ) 7:\nend for 8: end for\n9: \u0398 = ensemble({\u03b8 k g } k\u2208S ) 10: \u03b8 g \u2190 \u03b8 g \u2212 \u03b7 (L d (\u0398, \u03b8 g ))\n\nEnsemble Knowledge\n\nIn FedKEMF, we investigate three ensemble strategies, i.e., max logits, average logits, and majority vote.We adopt the max logits as the ensemble strategy since the max logits get the best results in practice.For a given input instance x, the ensemble model computes as the following equation:\n\u0398(x) = Ensemble M ax ({\u03b8 k g (x)} k\u2208S )(5)\n, where the Ensemble M ax compares all output vectors and returns a new vector containing the element-wise maxima.\n\n\nExperiments\n\nWe\n\n\nLearning efficiency\n\nIn this section, we evaluate the learning efficiency and optimization ability of FedKEMF (i.e., analysis of the relationship between communication rounds and the accuracy of the target model).We train VGG-11 ( As shown in Figure 4, FedKEMF achieves outstanding results with strong FL baselines in most benchmark settings, while producing a stable training process.In overparameterized neural networks, such as ResNet-20 and ResNet-32, FedKEMF outperforms baselines with a large margin.It's worth mentioning that FedKEMF shows superiority in dealing with heterogeneous settings than other baselines.We train FedKEMF and baselines with 30 clients, 50 clients, and 100 clients with different sample ratios and different heterogeneous noises.As the heterogeneity increases along with the number of clients increases, FedKEMF remains constantly stable across different FL scale settings.However, our baselines show significant fluctuation.That means our method has the ability to deal with large-scale federated learning training systems.\n\nThe baseline methods mainly focus on parameter and gradient aggregating when performing model fusion in the cloud, such as FedAvg (McMahan et al. 2017) simply weighted averaging edge model parameters.Although great efforts have been devoted to a fairly aggregate edge model, the summation over the parameters/gradients still introduces biases, because the contribution of a single edge model to the FL system is a black box.FedKEMF leverages ensemble distillation for model fusion.The ensemble edge model can significantly generalize heterogeneous edge models and guide the model towards a general optimized direction.Hence, FedKEMF shows a much more stable optimizing process across various non-IID FL settings.\n\nBesides, in the 2-layer CNN of Figure 4, FedKEMF shows less advantage when dealing with tiny structure neural networks.However, when we increase the number of communication rounds.In this way, FedKEMF can still achieve comparable final convergence accuracy as baselines.\n\nTo fully evaluate the optimization ability of FedKEMF, we further evaluate the model's accuracy overhead.Figure 5 shows the convergence accuracy comparison.The results show that the model trained by FedKEMF has a higher convergence overhead.Additionally, we investigate the communication rounds to achieve target accuracy.As Figure 6 shows, under different FL settings, FedKEMF achieves target model accuracy with remarkably fewer communication rounds.\n\n\nCommunication efficiency\n\nIn FedKEMF, a key attribute is that we introduce a knowledge network independent of the edge models.Hence,    1 shows the results of communication cost when we train the models to the target accuracy.The communication and stable training process of FedKEMF with knowledge network count up to 102\u00d7 less communication cost than baselines.For instance, when optimizing VGG-11 with 30 clients, FedKEMF uses 80.10 GB less (51.08\u00d7 less) communication overhead than FedAvg to achieve 65% accuracy and 161.8 GB less (102\u00d7) less communication overhead than FedNova.\n\nAdditionally, we evaluate the communication cost when we train the models to converge.We consider a performance upper bound by creating a hypothetical centralized case where images are heterogeneously distributed across 30, 50, and 100 clients.Table 2 shows the corresponding results.FedKEMF achieves remarkably higher convergence accuracy with negligible extra communication burdens and in some cases even less communication cost.For example, when training the ResNet-32 with 50 clients, FedKEMF achieves 38.68% higher accuracy with 1.51\u00d7 less communication cost.\n\n\nMulti-model federated learning\n\nFL systems not only suffer data heterogeneity but also have resource heterogeneity issues.Existing FL works mainly fo-cus on addressing data heterogeneity and improving training performance overhead.However, barely has a work target on resource heterogeneity.Simply deploying a uniform model to all resource-heterogeneous edge clients is inefficient, since some resource-poor clients will limit the FL system's computational overhead.Unlike traditional FL methods, which share an identical model among edge devices after optimization, FedKEMF with knowledge extraction enables multi-model deployment on heterogeneous edge devices.Specifically, with multi-model deployment, we can reasonably deploy models to edge clients according to their computational resources.\n\nTo evaluate FedKEMF on multi-model deployment, we deploy ResNet-20/32/44 in the same FL system and update the multi-model edge clients using FedKEMF.However, the customized model on edge performs differently.Hence, instead of evaluating a global test accuracy on the server side like the existing FL method.In the multi-model experiment, we allocate each client a local dataset and evaluate the average accuracy among all edge clients.\n\nTable 3 shows the multi-model results.We apply baseline models to train ResNet-20 only, while FedKEMF trains multi-model system containing ResNet-20/32/44.FedKEMF achieves the highest average accuracy on the multi-model system and outperforms baselines.\n\n\nAblation study\n\nIn this section, we investigate the optimization ability of FedKEMF under different FL settings and scales.As shown in Figure 7, we run FedKEMF on different FL settings heterogeneous noises.The results further demonstrate the superiority of FedKEMF in large-scale and heterogeneous FL environments.When the heterogeneity increases, FedKEMF constantly remains a stable training process.\n\n\nConclusion\n\nIn this paper, we purpose FedKEMF, a resource-aware FL paradigm for efficient federated learning using knowledge extraction and multi-model fusion.To address the resourceconstrained and computing power heterogeneity of edge devices in federated learning, we introduce a tiny size network to extract the local knowledge by mutually training the local model trains with the global knowledge, then transmit the tiny size network to the service for multi-model fusion and global knowledge distillation.We evaluate FedKEMF on image datasets (CIFAR-10 and MNIST) and their non-IID variants.Our experiments show that FedKEMF generates better performance in terms of communication efficiency and is capable of multi-model federated learning.In the future, we will continue to investigate maximizing the efficiency of multi-model fusion on edge devices.\n\nFigure 1 :\n1\nFigure 1: (a) The proposed knowledge extraction and multimodel fusion FL builds global knowledge by extracting local knowledge from different models using the corresponding computing power device and fuses it in, then transfers the local knowledge to the edge.It can thus be aware of resource constraints and serve as a robust general-purpose FL for practical applications.(b) In contrast, traditional FL (McMahan et al. 2017) produces aggregation of local model weights and distributes global model to edges.It treats edge devices with the same model and computing power, and has complex communication costs.\n\n\nFedAvg(\n\nMcMahan et al. 2017) is the original implementation for training decentralized data and preserving privacy in FL.Based on FedAvg, numerous variants have been proposed to optimize FL (Li et al. 2020a; Wang et al. 2020; Karimireddy et al. 2020), especially to track the heterogeneity issue in FL (Karimireddy et al. 2020; Li et al. 2020b).For example, FedProx (Li et al. 2020a) is proposed to improve local client training by adding a proximal term to the local loss, FedNova (Wang et al. 2020) introduces weight modification to avoid gradient bias by normalizing and scaling local updates, SCAFFOLD (Karimireddy et al. 2020) corrects the update direction to prevent client drift problem by maintaining drift variates.It is worth mentioning that these FL methods aggregate single-model weights for server from edges and incurs extra communication overhead.Other works (So, Guler, and Avestimehr 2020; Elkordy and Avestimehr 2022; Yuan and Ma 2020) optimize the communication cost but have no consideration of computation power heterogeneity of edge devices.Unlike prior works, we aggregate knowledge from the edge, and we only communicate these tiny knowledge networks among the edge and the server.\n\n\nFigure 2 :\n2\nFigure 2: Local updates.The client downloads the knowledge network from the server, trains mutually with the local model, extracts the updated knowledge network, and uploads it to the server.\n\n\nFigure 3 :\n3\nFigure 3: Cloud updates.The cloud collects local knowledge (tiny size network) from clients, ensembles all tiny size networks, distills into global knowledge, and transfers it to clients.\n\n\n\n\n, we mutually train the local model and knowledge network, transmitting the knowledge network back and forcing it to update the global knowledge.Knowledge extraction and communication allow edge clients to deploy resource-aware models for the application while keeping the communication efficiency and model heterogeneity.Algorithm 1 shows the local update process.\n\n\nFigure 4 :\n4\nFigure 4: Comparison of FedKEMF with SoTAs: the top-1 average test accuracy vs. communication rounds.2-layer CNN is trained on MNIST (Caldas et al. 2019).The VGG-11 (Simonyan and Zisserman 2015) and ResNet-20/32 (He et al. 2016) are trained on CIFAR-10 (Krizhevsky 2009).\n\n\nFigure 6 :\n6\nFigure 6: Communication rounds to achieve target accuracy.The lower the better.\n\n\n\n\n1: FedKEMF Edge Client Update ClientUpdate(\u03b8 g ): 1: \u03b8 \u2190 local deployed model 2: B \u2190 split local dataset into batches 3: for epoch = 1, 2, . . ., E do\n4:for batch b \u2208 B do5:Perform Deep Mutual Learning:6:\n\n\n\n(Li et al. 2021)We implement the non-IID benchmark federated learning setting(Li et al. 2021)for CIFAR-10 and MNIST.In the non-IID setting, each client is assigned a sample proportion of each label based on the Dirichlet distribution (with concentration \u03b1).Specifically, we sample p k \u223c Dir N (\u03b1) and assign a p k,j proportion of the instances to client j.Here we choose the \u03b1 = 0.1.\nExperimental setupDatasets and models. We conduct experiments ontwo image datasets: CIFAR-10 (Krizhevsky 2009) andMNIST (Caldas et al. 2019), with deep learning mod-els: VGG-11 (Simonyan and Zisserman 2015) and ResNet-20/32 (Federated learning settings. We implement federatedlearning following the non-IID benchmark setting (Li et al.2021). The server execution of the FL algorithm choosesa random sample ratio of clients for local training in eachcommunication round. We experiment on different clientsfrom 30 to 100 and the sample ratio from 0.4 to 1.0.\n(Wang et al. 2020), and experiments to evaluate the performance of FedKEMF.We separated our experiments into several sections: learning efficiency, communication cost, and multi-model federated learning.In addition, we performed an ablation study with different ensemble methods for FedKEMF.We compared FedKEMF with strong FL baselines, such as FedAvg(McMahan et al. 2017), Fed-Nova(Wang et al. 2020), FedProx(Li et al. 2020a), and  SCAFFOLD (Karimireddy et al. 2020).Baselines.We compare FedKEMF with state-of-the-art (SoTA) FL algorithms, including FedAvg(McMahan et al. 2017), FedProx(Li et al. 2020a), FedNova(Wang et al. 2020), and SCAFFOLD (Karimireddy et al. 2020).\n\n\nTable 1 :\n1\nComparison of communication cost with SoTAs to achieve target accuracy.The model marked with '*' failed to achieve target accuracy.\nMethodModelTarget AccuracyClients Communication Rounds Round/ClientCommunication Cost Total \u2206 CostSpeed UpResNet-201632.1MB4.01GB0GB(1 \u00d7)ResNet-3265%301833.2MB6.86GB0GB(1 \u00d7)VGG-1116642MB81.70GB0GB(1 \u00d7)FedAvgResNet-20  *  ResNet-32  *57%50400 4002.1MB 3.2MB28.71GB 43.75GB0GB 0GB(1 \u00d7) (1 \u00d7)ResNet-20 ResNet-3260%100109 1092.1MB 3.2MB11.18GB 17.03GB0GB 0GB(1 \u00d7) (1 \u00d7)ResNet-201474.2MB7.24GB+3.22GB(0.55 \u00d7)ResNet-3265%301476.4MB11.03GB+4.16GB(0.62 \u00d7)VGG-1116684MB163.41GB +81.70GB (0.50 \u00d7)FedNovaResNet-20  *  ResNet-32  *57%50400 4004.2MB 6.4MB57.42GB +28.71GB (0.50 \u00d7) 87.50GB +43.75GB (0.50 \u00d7)ResNet-20 ResNet-3260%100182 1554.2MB 6.4MB37.32GB +26.15GB (0.30 \u00d7) 48.44GB +31.41GB (0.35 \u00d7)30 Clients 50 Clients 100 ClientsFedAvg60.58 32.71 61.49ResNet-20 ResNet-32 FedNova FedProx 64.50 VGG-11 28.19 64.0860.2 30.97 57.765% SCAFFOLD16.37 22.52 48.69FedKEMF30 73.35 57.92 68.78200 195 20030(65%)FedAvg2.1MB 3.2MB 42MB FedNova4.92GB 7.31GB \u8868\u683c 1 FedProx 98.44GB +16.73GB (0.83 \u00d7) +0.91GB (0.82 \u00d7) +0.45GB (0.94 \u00d7) SCAFFOLD FedKEMF30 Clients 50 Clients 100 Clients ResNet-20 ResNet-32 VGG-11FedProx61.91 32.62 61.38 78.48 78.67 79.9ResNet-20  *  ResNet-32  *  67.84 28.32 ResNet-20 67.27 ResNet-32 ResNet-20 ResNet-32 76.87 77.48 80.4160.64 31.64 63.69 76.82 76.99 79.4257% 60% 65%10 10 43.67 81.82 81.62 82.450 72.47 71.87 72.01 100 30 84.269 83.669 84.07400 400 109 109 76 87ResNet-20 ResNet-32 30 (60%) ResNet-20 ResNet-32 100 (60%) ResNet-20 ResNet-32 VGG-112.1MB 164 3.2MB 250 2.1MB 140 140 3.2MB 2.1MB 2.1MB 109 109 181147 147 123 149 182 155 15928.71GB 250 43.75GB 250 11.18GB 140 151 17.03GB 1.87GB 2.14GB 110 109 2960GB 250 0GB 250 0GB 250 250 0GB -2.14GB -4.72GB 250 250 5481 91 49 56 59 47 46(1 \u00d7) (1 \u00d7) (1 \u00d7) (1 \u00d7) (2.14 \u00d7) (3.21 \u00d7)FedKEMF ResNet-20 61.29 ResNet-32 61.38 VGG-11 73.26VGG-11 76.76 ResNet-20 ResNet-32 68.25 71.1962.55 63.69 62.3757%56.565065 188 40100 (50%) ResNet-20 ResNet-32 VGG-112.1MB 2.1MB 2.1MB 71 72 69128 131 561.60GB 13.49GB 2.87GB 72 72 87-80.10GB (51.08 \u00d7) -15.22GB (2.13 \u00d7) 250 21 250 16 -40.88GB (15.24 \u00d7) 46 35ResNet-20 ResNet-3217.28 10ResNet-20 ResNet-32 19.43 1010 1060%10 10100 39 4153 45ResNet-20 ResNet-32502.1MB 2.1MB 1842005.43GB 4.61GB 193-5.74GB -12.42GB171(2.06 \u00d7) (3.69 \u00d7)VGG-1128.3322.1231.2131.7844.23VGG-11171193195156FedAvgFedNovaFedProxSCAFFOLDFedKEMFResNet-20ResNet-327580FedAvgFedNovaFedProxSCAFFOLDFedKEMF30 clients 60% target accuracy30 clients 65% target accuracyAccuracy %0 25 5030 Clients 50 Clients 100 Clients0 4030 Clients 50 Clients 100 ClientsNumber of rounds150 300150 30010 clients, sample ratio 1 Figure 5: Convergence accuracy overhead. The higher the 85 50 better. 100 clients, sample ratio 0.40100 clients 60% target accuracy ResNet-20 ResNet-320100 clients 50% target accuracy ResNet-20 ResNet-32300300Accuracy % it only communicates the tiny size knowledge network 76 80.5 ResNet-20 ResNet-32 VGG-11 0 25 ResNet-20 through training and inference, resulting in lower commu-nication cost than SoTA FL algorithms that use models for aggregation and fusion. For example, FedNova (Wang VGG-11 et al. 2020) and SCAFFOLD (Karimireddy et al. 2020) canNumber of rounds150 0ResNet-20ResNet-32150 0ResNet-20ResNet-32achieve stable training but cost double average communica-tion cost compared to FedAvg (McMahan et al. 2017) as aresult of sharing the extra gradient information. We evaluatethe communication cost in two ways. First, we train all themodels to a target accuracy and calculate the communicationcost. Second, we train all the models to converge and then\n\nTable 2 :\n2\nComparison of communication cost with SoTAs to achieve model convergence.\nMethodClients ModelSample RatioConverge RoundsCommunication Cost Round/Client TotalSpeedupConverge Acc. \u2206 Acc.ResNet-20 0.41632.1MB4.01GB(1 \u00d7)64.95%0%30ResNet-32 0.41823.2MB6.83GB(1 \u00d7)64.92%0%VGG-110.416342MB80.23GB(1 \u00d7)64.69%0%FedAvg50ResNet-20 0.7 ResNet-32 0.7195 1952.1MB 3.2MB14.00GB 21.33GB(1 \u00d7) (1 \u00d7)33.22% 33.19%0% 0%100ResNet-20 0.5 ResNet-32 0.5111 1222.1MB 3.2MB11.38GB 19.06GB(1 \u00d7) (1 \u00d7)61.39% 61.38%0% 0%ResNet-20 0.41954.2MB9.60GB(0.42 \u00d7)69.28%+4.33%30ResNet-32 0.41966.4MB14.70GB(0.46 \u00d7)69.13%+4.21%VGG-110.419684MB192.94GB (0.42 \u00d7)69.15%+4.46%FedNova50ResNet-20 0.7 ResNet-32 0.7167 1834.2MB 6.4MB23.97GB 40.03GB(0.58 \u00d7) (0.53 \u00d7)31.27% 31.87%-1.95% -1.32%100ResNet-20 0.5 ResNet-32 0.5191 1924.2MB 6.4MB39.17GB 60.00GB(0.29 \u00d7) (0.32 \u00d7)68.30% 67.27%+6.91% +5.89%ResNet-20 0.41632.1MB4.01GB(1 \u00d7)64.00%-0.95%30ResNet-32 0.41953.2MB7.31GB(0.93 \u00d7)64.75%-0.17%VGG-110.418842MB92.53GB(0.87 \u00d7)64.13%-0.56%FedProx50ResNet-20 0.7 ResNet-32 0.7195 1952.1MB 3.2MB14.00GB 21.33GB(1 \u00d7) (1 \u00d7)32.43% 32.89%-0.79% -0.3%100ResNet-20 0.5 ResNet-32 0.5118 1282.1MB 3.2MB12.10GB 20.00GB(0.94 \u00d7) (0.95 \u00d7)62.55% 63.69%+1.16% +2.31%ResNet-20 0.41932.1MB4.75GB(0.84 \u00d7)73.35%+8.40%30ResNet-32 0.41992.1MB4.90GB(1.39 \u00d7)72.47%+7.55%VGG-110.41912.1MB4.70GB(17.07 \u00d7) 74.58%+9.89%FedKEMF50ResNet-20 0.7 ResNet-32 0.7199 1972.1MB 2.1MB14.28GB 14.14GB(0.98 \u00d7) (1.51 \u00d7)57.92% 71.87%+24.70% +38.68%100ResNet-20 0.5 ResNet-32 0.5127 1752.1MB 2.1MB13.02GB 17.94GB(0.87 \u00d7) (1.06 \u00d7)68.78% 72.01%+7.39% +10.63%compare the communication cost for each FL algorithm. Thecommunication cost is represented by:\n# rounds \u00d7 round cost per client \u00d7 # sampled clients Table\n\n\nTable 3 :\n3\nMulti-model federated learning Figure 7: FedKEMF under different FL settings.FedKEMF provides a stable optimizing process among various FL settings.\nMethodModelClientsSample RatioAverage Acc.FedAvgResNet-2032.71%FedNova FedProxResNet-20 ResNet-20500.531.72% 32.43%FedKEMF Multi-model58.55%60ResNet-44 under different FL settingsResNet-32 under different FL settings70Accuracy (%)30 40 50ResNet-44 (50 Clients, r=0.5)40 50 60ResNet-32 (30 Clients, r=0.4)20ResNet-44 (100 Clients, r=0.5) ResNet-44 (100 Clients, r=0.7)30ResNet-32 (50 Clients, r=0,7) ResNet-32 (100 Clients, r=0.5)0 25 50 75 100 125 150 175 200 Round0 25 50 75 100 125 150 175 200 Round\n\nEnsemble Knowledge Distillation for Learning Improved and Efficient Networks. U Asif, J Tang, S Harrer, Santiago de Compostela. 82020\n\nLEAF: A Benchmark for Federated Settings. C Buciluundefined, R Caruana, A Niculescu-Mizil, S Caldas, S M K Duddu, P Wu, T Li, J Kone\u010dn\u1ef3, H B Mcmahan, V Smith, A Talwalkar, Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06. the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '06New York, NY, USAAssociation for Computing Machinery2006. 2019Proc. of Advances in Neural Information Processing Systems (NeurIPS)\n\nM Chen, R Mathews, T Ouyang, F Beaufays, arXiv:1903.10635Federated Learning Of Out-Of-Vocabulary Words. 2019arXiv preprint\n\nPrivacy and Artificial Intelligence. J Curzon, T A Kosa, R Akalu, K El-Khatib, IEEE Transactions on Artificial Intelligence. 222021\n\nHeteroSAg: Secure Aggregation With Heterogeneous Quantization in Federated Learning. A R Elkordy, A S Avestimehr, IEEE Transactions on Communications. 7042022\n\nPrivate Federated Learning. J Freudiger, NeurIPS Expo Talk. 2019\n\nFederated Region-Learning: An Edge Computing Based Framework for Urban Environment Sensing. T Fukuda, M Suzuki, G Kurata, S Thomas, J Cui, B Ramabhadran, Isca, M A Ganaie, M Hu, A K Malik, M Tanveer, P N Suganthan, C He, M Annavaram, S Avestimehr, K Zhang, X Ren, S Sun, J Dean, J Liu, L Ma, H , arXiv:2104.02395ArXiv: 1503.025312016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Hinton, G; Gao, YCurran Associates, Inc. He2017. 2017. 2022. 2020. 2016. 2015. 2018332018 IEEE Global Communications Conference (GLOBECOM)\n\nImproving Federated Learning Personalization via Model Agnostic Meta Learning. Y Jiang, J Kone\u010dn\u00fd, K Rush, S Kannan, S P Karimireddy, S Kale, M Mohri, S J Reddi, S U Stich, A T Suresh, arXiv:1909.12488ArXiv: 1909.12488Proc. of International Conference on Machine Learning (ICML). of International Conference on Machine Learning (ICML)2019. 2020SCAFFOLD: Stochastic Controlled Averaging for Federated Learning\n\nLearning multiple layers of features from tiny images. A Krizhevsky, V Kulkarni, M Kulkarni, A Pant, arXiv:2003.08673ArXiv: 2003.08673Survey of Personalization Techniques for Federated Learning. 2009. 2020Technical reportcs, stat\n\nD Li, J Wang, arXiv:1910.03581FedMD: Heterogenous Federated Learning via Model Distillation. 2019cs, stat\n\nFederated Learning on Non-IID Data Silos: An Experimental Study. Q Li, Y Diao, Q Chen, B He, ArXiv, abs/2102.020792021\n\nFederated Optimization in Heterogeneous Networks. T Li, A K Sahu, M Zaheer, M Sanjabi, A Talwalkar, V Smith, Proc. of Conference on Machine Learning and Systems (MLSys). of Conference on Machine Learning and Systems (MLSys)2020a\n\nPrivacy-preserving Federated Brain Tumour Segmentation. W Li, F Milletar\u00ec, D Xu, N Rieke, J Hancox, W Zhu, M Baust, Y Cheng, S Ourselin, M J Cardoso, A Feng, Proc. of MICCAI Workshop on Machine Learning in Medical Imaging (MLMI). of MICCAI Workshop on Machine Learning in Medical Imaging (MLMI)2019\n\nOn the Convergence of FedAvg on Non-IID Data. X Li, K Huang, W Yang, S Wang, Z Zhang, Proc, of International Conference on Learning Representations (ICML). of International Conference on Learning Representations (ICML)2020b\n\nEnsemble Distillation for Robust Model Fusion in Federated Learning. T Lin, L Kong, S U Stich, M Jaggi, Advances in Neural Information Processing Systems. Curran Associates, Inc202033\n\nCommunication-Efficient Learning of Deep Networks from Decentralized Data. H B Mcmahan, E Moore, D Ramage, S Hampson, B A Arcas, Proc. of International Conference on Artificial Intelligence and Statistics (AISTATS). of International Conference on Artificial Intelligence and Statistics (AISTATS)2017\n\nAre All Users Treated Fairly in Federated Learning Systems?. U Michieli, M Ozay, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops2021\n\nFeature-level Ensemble Knowledge Distillation for Aggregating Knowledge from Multiple Networks. T Nishio, R Yonetani, S Park, N Kwak, ArXiv: 1804.08333ICC 2019 -2019 IEEE International Conference on Communications (ICC). Santiago de Compostela. 2019. 20208Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge\n\nM Paulik, M Seigel, H Mason, D Telaar, J Kluivers, R Van Dalen, C W Lau, L Carlson, F Granqvist, C Vandevelde, S Agarwal, J Freudiger, A Byde, A Bhowmick, G Kapoor, S Beaumont, D \u00c1ine Cahill; Hughes, O Javidbakht, F Dong, R Rishi, S Hung, arXiv:2102.08503Federated Evaluation and Tuning for On-Device Personalization: System Design & Applications. 2021arXiv preprint\n\nEnergy Demand Prediction with Federated Learning for Electric Vehicle Networks. Y M Saputra, D T Hoang, D N Nguyen, E Dutkiewicz, M D Mueck, S Srikanteswara, 2019 IEEE Global Communications Conference (GLOBECOM). 2019\n\n. H Seo, J Park, S Oh, M Bennis, S.-L Kim, arXiv:2011.023672020Federated Knowledge Distillation. cs, math\n\nFederated learning in medicine: facilitating multi-institutional collaborations without sharing patient data. M J Sheller, B Edwards, G A Reina, J Martin, S Pati, A Kotrotsou, M Milchenko, W Xu, D Marcus, R R Colen, S Bakas, ArXiv: 2011.02367Scientific reports. 1012020\n\nTurbo-Aggregate: Breaking the Quadratic Aggregation Barrier in Secure Federated Learning. K Simonyan, A Zisserman, J So, B Guler, A S Avestimehr, D Sui, Y Chen, J Zhao, Y Jia, Y Xie, W Sun, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2015. 2020. 2020/167. 2020Online: Association for Computational Linguistics\n\nSimilarity-Preserving Knowledge Distillation. F Tung, G Mori, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Seoul, Korea (South2019\n\nTackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization. J Wang, Q Liu, H Liang, G Joshi, H V Poor, Proc. of Advances in Neural Information Processing Systems (NeurIPS). of Advances in Neural Information essing Systems (NeurIPS)2020\n\nLearning from Multiple Teacher Networks. C Wu, F Wu, R Liu, L Lyu, Y Huang, X Xie, Q Yang, Y Liu, T Chen, Y Tong, T Yang, G Andrew, H Eichner, H Sun, W Li, N Kong, D Ramage, F Beaufays, S You, C Xu, C Xu, D Tao, arXiv:2108.13323Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningHalifax NS CanadaACM2021. 2019. 2018. 2017arXiv preprintFedKD: Communication Efficient Federated Learning via Knowledge Distillation\n\nFederated Accelerated Stochastic Gradient Descent. H Yuan, T Ma, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033\n\nDeep Mutual Learning. Y Zhang, T Xiang, T M Hospedales, H Lu, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Salt Lake City, UT2018. 2018\n", "annotations": {"author": "[{\"end\":132,\"start\":98},{\"end\":166,\"start\":133},{\"end\":226,\"start\":167}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":105},{\"end\":141,\"start\":137},{\"end\":180,\"start\":171}]", "author_first_name": "[{\"end\":104,\"start\":98},{\"end\":136,\"start\":133},{\"end\":170,\"start\":167}]", "author_affiliation": "[{\"end\":131,\"start\":109},{\"end\":165,\"start\":143},{\"end\":225,\"start\":203}]", "title": "[{\"end\":84,\"start\":1},{\"end\":310,\"start\":227}]", "venue": null, "abstract": "[{\"end\":1823,\"start\":380}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2646,\"start\":2628},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2666,\"start\":2646},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2872,\"start\":2851},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2913,\"start\":2897},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2956,\"start\":2935},{\"end\":3079,\"start\":3052},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3096,\"start\":3079},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3125,\"start\":3109},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3144,\"start\":3125},{\"end\":3185,\"start\":3169},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3425,\"start\":3404},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4485,\"start\":4461},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4705,\"start\":4686},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4730,\"start\":4705},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7387,\"start\":7335},{\"end\":7418,\"start\":7387},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7557,\"start\":7537},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7617,\"start\":7600},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7636,\"start\":7617},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7644,\"start\":7636},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8033,\"start\":8015},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8068,\"start\":8033},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8084,\"start\":8068},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8100,\"start\":8084},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8116,\"start\":8100},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8237,\"start\":8221},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8375,\"start\":8359},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8778,\"start\":8759},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9567,\"start\":9550},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9584,\"start\":9567},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9608,\"start\":9584},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9891,\"start\":9872},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11379,\"start\":11362},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11664,\"start\":11643},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11688,\"start\":11664},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14237,\"start\":14216}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20054,\"start\":19430},{\"attributes\":{\"id\":\"fig_1\"},\"end\":21263,\"start\":20055},{\"attributes\":{\"id\":\"fig_2\"},\"end\":21470,\"start\":21264},{\"attributes\":{\"id\":\"fig_3\"},\"end\":21673,\"start\":21471},{\"attributes\":{\"id\":\"fig_4\"},\"end\":22043,\"start\":21674},{\"attributes\":{\"id\":\"fig_5\"},\"end\":22330,\"start\":22044},{\"attributes\":{\"id\":\"fig_6\"},\"end\":22425,\"start\":22331},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":22633,\"start\":22426},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":24251,\"start\":22634},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27926,\"start\":24252},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":29654,\"start\":27927},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30318,\"start\":29655}]", "paragraph": "[{\"end\":2749,\"start\":1839},{\"end\":3364,\"start\":2751},{\"end\":4996,\"start\":3366},{\"end\":6034,\"start\":4998},{\"end\":7060,\"start\":6036},{\"end\":7816,\"start\":7165},{\"end\":8852,\"start\":7818},{\"end\":9466,\"start\":8868},{\"end\":9918,\"start\":9518},{\"end\":10445,\"start\":9920},{\"end\":10576,\"start\":10483},{\"end\":10633,\"start\":10578},{\"end\":10644,\"start\":10635},{\"end\":10684,\"start\":10682},{\"end\":10820,\"start\":10732},{\"end\":10997,\"start\":10886},{\"end\":11063,\"start\":11028},{\"end\":11133,\"start\":11110},{\"end\":11709,\"start\":11166},{\"end\":12120,\"start\":11711},{\"end\":12199,\"start\":12145},{\"end\":12252,\"start\":12201},{\"end\":12315,\"start\":12254},{\"end\":12349,\"start\":12317},{\"end\":12390,\"start\":12351},{\"end\":12418,\"start\":12392},{\"end\":12422,\"start\":12420},{\"end\":12471,\"start\":12453},{\"end\":12851,\"start\":12558},{\"end\":13009,\"start\":12895},{\"end\":13027,\"start\":13025},{\"end\":14084,\"start\":13051},{\"end\":14798,\"start\":14086},{\"end\":15070,\"start\":14800},{\"end\":15524,\"start\":15072},{\"end\":16109,\"start\":15553},{\"end\":16675,\"start\":16111},{\"end\":17474,\"start\":16710},{\"end\":17911,\"start\":17476},{\"end\":18166,\"start\":17913},{\"end\":18570,\"start\":18185},{\"end\":19429,\"start\":18585},{\"end\":20053,\"start\":19444},{\"end\":21262,\"start\":20065},{\"end\":21469,\"start\":21278},{\"end\":21672,\"start\":21485},{\"end\":22042,\"start\":21677},{\"end\":22329,\"start\":22058},{\"end\":22424,\"start\":22345},{\"end\":22579,\"start\":22429},{\"end\":23020,\"start\":22637},{\"end\":24250,\"start\":23578},{\"end\":24396,\"start\":24265},{\"end\":28013,\"start\":27940},{\"end\":29653,\"start\":29595},{\"end\":29816,\"start\":29668}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10482,\"start\":10446},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10681,\"start\":10645},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10731,\"start\":10685},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10885,\"start\":10821},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11027,\"start\":10998},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12144,\"start\":12121},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12452,\"start\":12423},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12536,\"start\":12472},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12894,\"start\":12852}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":7704,\"start\":7645},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":15664,\"start\":15663},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16362,\"start\":16361},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17920,\"start\":17919}]", "section_header": "[{\"end\":1837,\"start\":1825},{\"end\":7094,\"start\":7063},{\"end\":7163,\"start\":7097},{\"end\":8866,\"start\":8855},{\"end\":9516,\"start\":9469},{\"end\":11108,\"start\":11066},{\"end\":11164,\"start\":11136},{\"end\":12556,\"start\":12538},{\"end\":13023,\"start\":13012},{\"end\":13049,\"start\":13030},{\"end\":15551,\"start\":15527},{\"end\":16708,\"start\":16678},{\"end\":18183,\"start\":18169},{\"end\":18583,\"start\":18573},{\"end\":19441,\"start\":19431},{\"end\":20063,\"start\":20056},{\"end\":21275,\"start\":21265},{\"end\":21482,\"start\":21472},{\"end\":22055,\"start\":22045},{\"end\":22342,\"start\":22332},{\"end\":24262,\"start\":24253},{\"end\":27937,\"start\":27928},{\"end\":29665,\"start\":29656}]", "table": "[{\"end\":22633,\"start\":22580},{\"end\":23577,\"start\":23021},{\"end\":27926,\"start\":24397},{\"end\":29594,\"start\":28014},{\"end\":30318,\"start\":29817}]", "figure_caption": "[{\"end\":20054,\"start\":19443},{\"end\":21263,\"start\":20064},{\"end\":21470,\"start\":21277},{\"end\":21673,\"start\":21484},{\"end\":22043,\"start\":21676},{\"end\":22330,\"start\":22057},{\"end\":22425,\"start\":22344},{\"end\":22580,\"start\":22428},{\"end\":23021,\"start\":22636},{\"end\":24397,\"start\":24264},{\"end\":28014,\"start\":27939},{\"end\":29817,\"start\":29667}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3437,\"start\":3436},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5219,\"start\":5214},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":8876,\"start\":8875},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":8924,\"start\":8923},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11133,\"start\":11132},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":13281,\"start\":13280},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":14839,\"start\":14838},{\"end\":15185,\"start\":15184},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":15405,\"start\":15404},{\"end\":18312,\"start\":18311}]", "bib_author_first_name": "[{\"end\":30399,\"start\":30398},{\"end\":30407,\"start\":30406},{\"end\":30415,\"start\":30414},{\"end\":30498,\"start\":30497},{\"end\":30517,\"start\":30516},{\"end\":30528,\"start\":30527},{\"end\":30547,\"start\":30546},{\"end\":30557,\"start\":30556},{\"end\":30561,\"start\":30558},{\"end\":30570,\"start\":30569},{\"end\":30576,\"start\":30575},{\"end\":30582,\"start\":30581},{\"end\":30593,\"start\":30592},{\"end\":30595,\"start\":30594},{\"end\":30606,\"start\":30605},{\"end\":30615,\"start\":30614},{\"end\":30961,\"start\":30960},{\"end\":30969,\"start\":30968},{\"end\":30980,\"start\":30979},{\"end\":30990,\"start\":30989},{\"end\":31122,\"start\":31121},{\"end\":31132,\"start\":31131},{\"end\":31134,\"start\":31133},{\"end\":31142,\"start\":31141},{\"end\":31151,\"start\":31150},{\"end\":31303,\"start\":31302},{\"end\":31305,\"start\":31304},{\"end\":31316,\"start\":31315},{\"end\":31318,\"start\":31317},{\"end\":31406,\"start\":31405},{\"end\":31536,\"start\":31535},{\"end\":31546,\"start\":31545},{\"end\":31556,\"start\":31555},{\"end\":31566,\"start\":31565},{\"end\":31576,\"start\":31575},{\"end\":31583,\"start\":31582},{\"end\":31604,\"start\":31603},{\"end\":31606,\"start\":31605},{\"end\":31616,\"start\":31615},{\"end\":31622,\"start\":31621},{\"end\":31624,\"start\":31623},{\"end\":31633,\"start\":31632},{\"end\":31644,\"start\":31643},{\"end\":31646,\"start\":31645},{\"end\":31659,\"start\":31658},{\"end\":31665,\"start\":31664},{\"end\":31678,\"start\":31677},{\"end\":31692,\"start\":31691},{\"end\":31701,\"start\":31700},{\"end\":31708,\"start\":31707},{\"end\":31715,\"start\":31714},{\"end\":31723,\"start\":31722},{\"end\":31730,\"start\":31729},{\"end\":31736,\"start\":31735},{\"end\":32064,\"start\":32063},{\"end\":32073,\"start\":32072},{\"end\":32084,\"start\":32083},{\"end\":32092,\"start\":32091},{\"end\":32102,\"start\":32101},{\"end\":32104,\"start\":32103},{\"end\":32119,\"start\":32118},{\"end\":32127,\"start\":32126},{\"end\":32136,\"start\":32135},{\"end\":32138,\"start\":32137},{\"end\":32147,\"start\":32146},{\"end\":32149,\"start\":32148},{\"end\":32158,\"start\":32157},{\"end\":32160,\"start\":32159},{\"end\":32450,\"start\":32449},{\"end\":32464,\"start\":32463},{\"end\":32476,\"start\":32475},{\"end\":32488,\"start\":32487},{\"end\":32626,\"start\":32625},{\"end\":32632,\"start\":32631},{\"end\":32798,\"start\":32797},{\"end\":32804,\"start\":32803},{\"end\":32812,\"start\":32811},{\"end\":32820,\"start\":32819},{\"end\":32903,\"start\":32902},{\"end\":32909,\"start\":32908},{\"end\":32911,\"start\":32910},{\"end\":32919,\"start\":32918},{\"end\":32929,\"start\":32928},{\"end\":32940,\"start\":32939},{\"end\":32953,\"start\":32952},{\"end\":33139,\"start\":33138},{\"end\":33145,\"start\":33144},{\"end\":33158,\"start\":33157},{\"end\":33164,\"start\":33163},{\"end\":33173,\"start\":33172},{\"end\":33183,\"start\":33182},{\"end\":33190,\"start\":33189},{\"end\":33199,\"start\":33198},{\"end\":33208,\"start\":33207},{\"end\":33220,\"start\":33219},{\"end\":33222,\"start\":33221},{\"end\":33233,\"start\":33232},{\"end\":33429,\"start\":33428},{\"end\":33435,\"start\":33434},{\"end\":33444,\"start\":33443},{\"end\":33452,\"start\":33451},{\"end\":33460,\"start\":33459},{\"end\":33677,\"start\":33676},{\"end\":33684,\"start\":33683},{\"end\":33692,\"start\":33691},{\"end\":33694,\"start\":33693},{\"end\":33703,\"start\":33702},{\"end\":33868,\"start\":33867},{\"end\":33870,\"start\":33869},{\"end\":33881,\"start\":33880},{\"end\":33890,\"start\":33889},{\"end\":33900,\"start\":33899},{\"end\":33911,\"start\":33910},{\"end\":33913,\"start\":33912},{\"end\":34155,\"start\":34154},{\"end\":34167,\"start\":34166},{\"end\":34460,\"start\":34459},{\"end\":34470,\"start\":34469},{\"end\":34482,\"start\":34481},{\"end\":34490,\"start\":34489},{\"end\":34705,\"start\":34704},{\"end\":34715,\"start\":34714},{\"end\":34725,\"start\":34724},{\"end\":34734,\"start\":34733},{\"end\":34744,\"start\":34743},{\"end\":34756,\"start\":34755},{\"end\":34769,\"start\":34768},{\"end\":34771,\"start\":34770},{\"end\":34778,\"start\":34777},{\"end\":34789,\"start\":34788},{\"end\":34802,\"start\":34801},{\"end\":34816,\"start\":34815},{\"end\":34827,\"start\":34826},{\"end\":34840,\"start\":34839},{\"end\":34848,\"start\":34847},{\"end\":34860,\"start\":34859},{\"end\":34870,\"start\":34869},{\"end\":34882,\"start\":34881},{\"end\":34905,\"start\":34904},{\"end\":34919,\"start\":34918},{\"end\":34927,\"start\":34926},{\"end\":34936,\"start\":34935},{\"end\":35153,\"start\":35152},{\"end\":35155,\"start\":35154},{\"end\":35166,\"start\":35165},{\"end\":35168,\"start\":35167},{\"end\":35177,\"start\":35176},{\"end\":35179,\"start\":35178},{\"end\":35189,\"start\":35188},{\"end\":35203,\"start\":35202},{\"end\":35205,\"start\":35204},{\"end\":35214,\"start\":35213},{\"end\":35294,\"start\":35293},{\"end\":35301,\"start\":35300},{\"end\":35309,\"start\":35308},{\"end\":35315,\"start\":35314},{\"end\":35328,\"start\":35324},{\"end\":35509,\"start\":35508},{\"end\":35511,\"start\":35510},{\"end\":35522,\"start\":35521},{\"end\":35533,\"start\":35532},{\"end\":35535,\"start\":35534},{\"end\":35544,\"start\":35543},{\"end\":35554,\"start\":35553},{\"end\":35562,\"start\":35561},{\"end\":35575,\"start\":35574},{\"end\":35588,\"start\":35587},{\"end\":35594,\"start\":35593},{\"end\":35604,\"start\":35603},{\"end\":35606,\"start\":35605},{\"end\":35615,\"start\":35614},{\"end\":35760,\"start\":35759},{\"end\":35772,\"start\":35771},{\"end\":35785,\"start\":35784},{\"end\":35791,\"start\":35790},{\"end\":35800,\"start\":35799},{\"end\":35802,\"start\":35801},{\"end\":35816,\"start\":35815},{\"end\":35823,\"start\":35822},{\"end\":35831,\"start\":35830},{\"end\":35839,\"start\":35838},{\"end\":35846,\"start\":35845},{\"end\":35853,\"start\":35852},{\"end\":36142,\"start\":36141},{\"end\":36150,\"start\":36149},{\"end\":36335,\"start\":36334},{\"end\":36343,\"start\":36342},{\"end\":36350,\"start\":36349},{\"end\":36359,\"start\":36358},{\"end\":36368,\"start\":36367},{\"end\":36370,\"start\":36369},{\"end\":36553,\"start\":36552},{\"end\":36559,\"start\":36558},{\"end\":36565,\"start\":36564},{\"end\":36572,\"start\":36571},{\"end\":36579,\"start\":36578},{\"end\":36588,\"start\":36587},{\"end\":36595,\"start\":36594},{\"end\":36603,\"start\":36602},{\"end\":36610,\"start\":36609},{\"end\":36618,\"start\":36617},{\"end\":36626,\"start\":36625},{\"end\":36634,\"start\":36633},{\"end\":36644,\"start\":36643},{\"end\":36655,\"start\":36654},{\"end\":36662,\"start\":36661},{\"end\":36668,\"start\":36667},{\"end\":36676,\"start\":36675},{\"end\":36686,\"start\":36685},{\"end\":36698,\"start\":36697},{\"end\":36705,\"start\":36704},{\"end\":36711,\"start\":36710},{\"end\":36717,\"start\":36716},{\"end\":37108,\"start\":37107},{\"end\":37116,\"start\":37115},{\"end\":37173,\"start\":37172},{\"end\":37187,\"start\":37186},{\"end\":37198,\"start\":37197},{\"end\":37209,\"start\":37208},{\"end\":37219,\"start\":37218},{\"end\":37278,\"start\":37277},{\"end\":37287,\"start\":37286},{\"end\":37296,\"start\":37295},{\"end\":37298,\"start\":37297},{\"end\":37312,\"start\":37311}]", "bib_author_last_name": "[{\"end\":30404,\"start\":30400},{\"end\":30412,\"start\":30408},{\"end\":30422,\"start\":30416},{\"end\":30514,\"start\":30499},{\"end\":30525,\"start\":30518},{\"end\":30544,\"start\":30529},{\"end\":30554,\"start\":30548},{\"end\":30567,\"start\":30562},{\"end\":30573,\"start\":30571},{\"end\":30579,\"start\":30577},{\"end\":30590,\"start\":30583},{\"end\":30603,\"start\":30596},{\"end\":30612,\"start\":30607},{\"end\":30625,\"start\":30616},{\"end\":30966,\"start\":30962},{\"end\":30977,\"start\":30970},{\"end\":30987,\"start\":30981},{\"end\":30999,\"start\":30991},{\"end\":31129,\"start\":31123},{\"end\":31139,\"start\":31135},{\"end\":31148,\"start\":31143},{\"end\":31161,\"start\":31152},{\"end\":31313,\"start\":31306},{\"end\":31329,\"start\":31319},{\"end\":31416,\"start\":31407},{\"end\":31543,\"start\":31537},{\"end\":31553,\"start\":31547},{\"end\":31563,\"start\":31557},{\"end\":31573,\"start\":31567},{\"end\":31580,\"start\":31577},{\"end\":31595,\"start\":31584},{\"end\":31601,\"start\":31597},{\"end\":31613,\"start\":31607},{\"end\":31619,\"start\":31617},{\"end\":31630,\"start\":31625},{\"end\":31641,\"start\":31634},{\"end\":31656,\"start\":31647},{\"end\":31662,\"start\":31660},{\"end\":31675,\"start\":31666},{\"end\":31689,\"start\":31679},{\"end\":31698,\"start\":31693},{\"end\":31705,\"start\":31702},{\"end\":31712,\"start\":31709},{\"end\":31720,\"start\":31716},{\"end\":31727,\"start\":31724},{\"end\":31733,\"start\":31731},{\"end\":32070,\"start\":32065},{\"end\":32081,\"start\":32074},{\"end\":32089,\"start\":32085},{\"end\":32099,\"start\":32093},{\"end\":32116,\"start\":32105},{\"end\":32124,\"start\":32120},{\"end\":32133,\"start\":32128},{\"end\":32144,\"start\":32139},{\"end\":32155,\"start\":32150},{\"end\":32167,\"start\":32161},{\"end\":32461,\"start\":32451},{\"end\":32473,\"start\":32465},{\"end\":32485,\"start\":32477},{\"end\":32493,\"start\":32489},{\"end\":32629,\"start\":32627},{\"end\":32637,\"start\":32633},{\"end\":32801,\"start\":32799},{\"end\":32809,\"start\":32805},{\"end\":32817,\"start\":32813},{\"end\":32823,\"start\":32821},{\"end\":32906,\"start\":32904},{\"end\":32916,\"start\":32912},{\"end\":32926,\"start\":32920},{\"end\":32937,\"start\":32930},{\"end\":32950,\"start\":32941},{\"end\":32959,\"start\":32954},{\"end\":33142,\"start\":33140},{\"end\":33155,\"start\":33146},{\"end\":33161,\"start\":33159},{\"end\":33170,\"start\":33165},{\"end\":33180,\"start\":33174},{\"end\":33187,\"start\":33184},{\"end\":33196,\"start\":33191},{\"end\":33205,\"start\":33200},{\"end\":33217,\"start\":33209},{\"end\":33230,\"start\":33223},{\"end\":33238,\"start\":33234},{\"end\":33432,\"start\":33430},{\"end\":33441,\"start\":33436},{\"end\":33449,\"start\":33445},{\"end\":33457,\"start\":33453},{\"end\":33466,\"start\":33461},{\"end\":33681,\"start\":33678},{\"end\":33689,\"start\":33685},{\"end\":33700,\"start\":33695},{\"end\":33709,\"start\":33704},{\"end\":33878,\"start\":33871},{\"end\":33887,\"start\":33882},{\"end\":33897,\"start\":33891},{\"end\":33908,\"start\":33901},{\"end\":33919,\"start\":33914},{\"end\":34164,\"start\":34156},{\"end\":34172,\"start\":34168},{\"end\":34467,\"start\":34461},{\"end\":34479,\"start\":34471},{\"end\":34487,\"start\":34483},{\"end\":34495,\"start\":34491},{\"end\":34712,\"start\":34706},{\"end\":34722,\"start\":34716},{\"end\":34731,\"start\":34726},{\"end\":34741,\"start\":34735},{\"end\":34753,\"start\":34745},{\"end\":34766,\"start\":34757},{\"end\":34775,\"start\":34772},{\"end\":34786,\"start\":34779},{\"end\":34799,\"start\":34790},{\"end\":34813,\"start\":34803},{\"end\":34824,\"start\":34817},{\"end\":34837,\"start\":34828},{\"end\":34845,\"start\":34841},{\"end\":34857,\"start\":34849},{\"end\":34867,\"start\":34861},{\"end\":34879,\"start\":34871},{\"end\":34902,\"start\":34883},{\"end\":34916,\"start\":34906},{\"end\":34924,\"start\":34920},{\"end\":34933,\"start\":34928},{\"end\":34941,\"start\":34937},{\"end\":35163,\"start\":35156},{\"end\":35174,\"start\":35169},{\"end\":35186,\"start\":35180},{\"end\":35200,\"start\":35190},{\"end\":35211,\"start\":35206},{\"end\":35228,\"start\":35215},{\"end\":35298,\"start\":35295},{\"end\":35306,\"start\":35302},{\"end\":35312,\"start\":35310},{\"end\":35322,\"start\":35316},{\"end\":35332,\"start\":35329},{\"end\":35519,\"start\":35512},{\"end\":35530,\"start\":35523},{\"end\":35541,\"start\":35536},{\"end\":35551,\"start\":35545},{\"end\":35559,\"start\":35555},{\"end\":35572,\"start\":35563},{\"end\":35585,\"start\":35576},{\"end\":35591,\"start\":35589},{\"end\":35601,\"start\":35595},{\"end\":35612,\"start\":35607},{\"end\":35621,\"start\":35616},{\"end\":35769,\"start\":35761},{\"end\":35782,\"start\":35773},{\"end\":35788,\"start\":35786},{\"end\":35797,\"start\":35792},{\"end\":35813,\"start\":35803},{\"end\":35820,\"start\":35817},{\"end\":35828,\"start\":35824},{\"end\":35836,\"start\":35832},{\"end\":35843,\"start\":35840},{\"end\":35850,\"start\":35847},{\"end\":35857,\"start\":35854},{\"end\":36147,\"start\":36143},{\"end\":36155,\"start\":36151},{\"end\":36340,\"start\":36336},{\"end\":36347,\"start\":36344},{\"end\":36356,\"start\":36351},{\"end\":36365,\"start\":36360},{\"end\":36375,\"start\":36371},{\"end\":36556,\"start\":36554},{\"end\":36562,\"start\":36560},{\"end\":36569,\"start\":36566},{\"end\":36576,\"start\":36573},{\"end\":36585,\"start\":36580},{\"end\":36592,\"start\":36589},{\"end\":36600,\"start\":36596},{\"end\":36607,\"start\":36604},{\"end\":36615,\"start\":36611},{\"end\":36623,\"start\":36619},{\"end\":36631,\"start\":36627},{\"end\":36641,\"start\":36635},{\"end\":36652,\"start\":36645},{\"end\":36659,\"start\":36656},{\"end\":36665,\"start\":36663},{\"end\":36673,\"start\":36669},{\"end\":36683,\"start\":36677},{\"end\":36695,\"start\":36687},{\"end\":36702,\"start\":36699},{\"end\":36708,\"start\":36706},{\"end\":36714,\"start\":36712},{\"end\":36721,\"start\":36718},{\"end\":37113,\"start\":37109},{\"end\":37119,\"start\":37117},{\"end\":37184,\"start\":37174},{\"end\":37195,\"start\":37188},{\"end\":37206,\"start\":37199},{\"end\":37216,\"start\":37210},{\"end\":37223,\"start\":37220},{\"end\":37284,\"start\":37279},{\"end\":37293,\"start\":37288},{\"end\":37309,\"start\":37299},{\"end\":37315,\"start\":37313}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":202660953},\"end\":30453,\"start\":30320},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":53701546},\"end\":30958,\"start\":30455},{\"attributes\":{\"doi\":\"arXiv:1903.10635\",\"id\":\"b2\"},\"end\":31082,\"start\":30960},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":237332803},\"end\":31215,\"start\":31084},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":222066969},\"end\":31375,\"start\":31217},{\"attributes\":{\"id\":\"b5\"},\"end\":31441,\"start\":31377},{\"attributes\":{\"doi\":\"arXiv:2104.02395\",\"id\":\"b6\",\"matched_paper_id\":67864283},\"end\":31982,\"start\":31443},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":203591432},\"end\":32392,\"start\":31984},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18268744},\"end\":32623,\"start\":32394},{\"attributes\":{\"id\":\"b9\"},\"end\":32730,\"start\":32625},{\"attributes\":{\"id\":\"b10\"},\"end\":32850,\"start\":32732},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":59316566},\"end\":33080,\"start\":32852},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":203627027},\"end\":33380,\"start\":33082},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":195798643},\"end\":33605,\"start\":33382},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":219636007},\"end\":33790,\"start\":33607},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14955348},\"end\":34091,\"start\":33792},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":235703165},\"end\":34361,\"start\":34093},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":220378802},\"end\":34702,\"start\":34363},{\"attributes\":{\"id\":\"b18\"},\"end\":35070,\"start\":34704},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":202537812},\"end\":35289,\"start\":35072},{\"attributes\":{\"id\":\"b20\"},\"end\":35396,\"start\":35291},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":220812287},\"end\":35667,\"start\":35398},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":211076217},\"end\":36093,\"start\":35669},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":198179476},\"end\":36246,\"start\":36095},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":220525591},\"end\":36509,\"start\":36248},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":26021416},\"end\":37054,\"start\":36511},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":219708203},\"end\":37253,\"start\":37056},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":26071966},\"end\":37409,\"start\":37255}]", "bib_title": "[{\"end\":30396,\"start\":30320},{\"end\":30495,\"start\":30455},{\"end\":31119,\"start\":31084},{\"end\":31300,\"start\":31217},{\"end\":31403,\"start\":31377},{\"end\":31533,\"start\":31443},{\"end\":32061,\"start\":31984},{\"end\":32447,\"start\":32394},{\"end\":32900,\"start\":32852},{\"end\":33136,\"start\":33082},{\"end\":33426,\"start\":33382},{\"end\":33674,\"start\":33607},{\"end\":33865,\"start\":33792},{\"end\":34152,\"start\":34093},{\"end\":34457,\"start\":34363},{\"end\":35150,\"start\":35072},{\"end\":35506,\"start\":35398},{\"end\":35757,\"start\":35669},{\"end\":36139,\"start\":36095},{\"end\":36332,\"start\":36248},{\"end\":36550,\"start\":36511},{\"end\":37105,\"start\":37056},{\"end\":37275,\"start\":37255}]", "bib_author": "[{\"end\":30406,\"start\":30398},{\"end\":30414,\"start\":30406},{\"end\":30424,\"start\":30414},{\"end\":30516,\"start\":30497},{\"end\":30527,\"start\":30516},{\"end\":30546,\"start\":30527},{\"end\":30556,\"start\":30546},{\"end\":30569,\"start\":30556},{\"end\":30575,\"start\":30569},{\"end\":30581,\"start\":30575},{\"end\":30592,\"start\":30581},{\"end\":30605,\"start\":30592},{\"end\":30614,\"start\":30605},{\"end\":30627,\"start\":30614},{\"end\":30968,\"start\":30960},{\"end\":30979,\"start\":30968},{\"end\":30989,\"start\":30979},{\"end\":31001,\"start\":30989},{\"end\":31131,\"start\":31121},{\"end\":31141,\"start\":31131},{\"end\":31150,\"start\":31141},{\"end\":31163,\"start\":31150},{\"end\":31315,\"start\":31302},{\"end\":31331,\"start\":31315},{\"end\":31418,\"start\":31405},{\"end\":31545,\"start\":31535},{\"end\":31555,\"start\":31545},{\"end\":31565,\"start\":31555},{\"end\":31575,\"start\":31565},{\"end\":31582,\"start\":31575},{\"end\":31597,\"start\":31582},{\"end\":31603,\"start\":31597},{\"end\":31615,\"start\":31603},{\"end\":31621,\"start\":31615},{\"end\":31632,\"start\":31621},{\"end\":31643,\"start\":31632},{\"end\":31658,\"start\":31643},{\"end\":31664,\"start\":31658},{\"end\":31677,\"start\":31664},{\"end\":31691,\"start\":31677},{\"end\":31700,\"start\":31691},{\"end\":31707,\"start\":31700},{\"end\":31714,\"start\":31707},{\"end\":31722,\"start\":31714},{\"end\":31729,\"start\":31722},{\"end\":31735,\"start\":31729},{\"end\":31739,\"start\":31735},{\"end\":32072,\"start\":32063},{\"end\":32083,\"start\":32072},{\"end\":32091,\"start\":32083},{\"end\":32101,\"start\":32091},{\"end\":32118,\"start\":32101},{\"end\":32126,\"start\":32118},{\"end\":32135,\"start\":32126},{\"end\":32146,\"start\":32135},{\"end\":32157,\"start\":32146},{\"end\":32169,\"start\":32157},{\"end\":32463,\"start\":32449},{\"end\":32475,\"start\":32463},{\"end\":32487,\"start\":32475},{\"end\":32495,\"start\":32487},{\"end\":32631,\"start\":32625},{\"end\":32639,\"start\":32631},{\"end\":32803,\"start\":32797},{\"end\":32811,\"start\":32803},{\"end\":32819,\"start\":32811},{\"end\":32825,\"start\":32819},{\"end\":32908,\"start\":32902},{\"end\":32918,\"start\":32908},{\"end\":32928,\"start\":32918},{\"end\":32939,\"start\":32928},{\"end\":32952,\"start\":32939},{\"end\":32961,\"start\":32952},{\"end\":33144,\"start\":33138},{\"end\":33157,\"start\":33144},{\"end\":33163,\"start\":33157},{\"end\":33172,\"start\":33163},{\"end\":33182,\"start\":33172},{\"end\":33189,\"start\":33182},{\"end\":33198,\"start\":33189},{\"end\":33207,\"start\":33198},{\"end\":33219,\"start\":33207},{\"end\":33232,\"start\":33219},{\"end\":33240,\"start\":33232},{\"end\":33434,\"start\":33428},{\"end\":33443,\"start\":33434},{\"end\":33451,\"start\":33443},{\"end\":33459,\"start\":33451},{\"end\":33468,\"start\":33459},{\"end\":33683,\"start\":33676},{\"end\":33691,\"start\":33683},{\"end\":33702,\"start\":33691},{\"end\":33711,\"start\":33702},{\"end\":33880,\"start\":33867},{\"end\":33889,\"start\":33880},{\"end\":33899,\"start\":33889},{\"end\":33910,\"start\":33899},{\"end\":33921,\"start\":33910},{\"end\":34166,\"start\":34154},{\"end\":34174,\"start\":34166},{\"end\":34469,\"start\":34459},{\"end\":34481,\"start\":34469},{\"end\":34489,\"start\":34481},{\"end\":34497,\"start\":34489},{\"end\":34714,\"start\":34704},{\"end\":34724,\"start\":34714},{\"end\":34733,\"start\":34724},{\"end\":34743,\"start\":34733},{\"end\":34755,\"start\":34743},{\"end\":34768,\"start\":34755},{\"end\":34777,\"start\":34768},{\"end\":34788,\"start\":34777},{\"end\":34801,\"start\":34788},{\"end\":34815,\"start\":34801},{\"end\":34826,\"start\":34815},{\"end\":34839,\"start\":34826},{\"end\":34847,\"start\":34839},{\"end\":34859,\"start\":34847},{\"end\":34869,\"start\":34859},{\"end\":34881,\"start\":34869},{\"end\":34904,\"start\":34881},{\"end\":34918,\"start\":34904},{\"end\":34926,\"start\":34918},{\"end\":34935,\"start\":34926},{\"end\":34943,\"start\":34935},{\"end\":35165,\"start\":35152},{\"end\":35176,\"start\":35165},{\"end\":35188,\"start\":35176},{\"end\":35202,\"start\":35188},{\"end\":35213,\"start\":35202},{\"end\":35230,\"start\":35213},{\"end\":35300,\"start\":35293},{\"end\":35308,\"start\":35300},{\"end\":35314,\"start\":35308},{\"end\":35324,\"start\":35314},{\"end\":35334,\"start\":35324},{\"end\":35521,\"start\":35508},{\"end\":35532,\"start\":35521},{\"end\":35543,\"start\":35532},{\"end\":35553,\"start\":35543},{\"end\":35561,\"start\":35553},{\"end\":35574,\"start\":35561},{\"end\":35587,\"start\":35574},{\"end\":35593,\"start\":35587},{\"end\":35603,\"start\":35593},{\"end\":35614,\"start\":35603},{\"end\":35623,\"start\":35614},{\"end\":35771,\"start\":35759},{\"end\":35784,\"start\":35771},{\"end\":35790,\"start\":35784},{\"end\":35799,\"start\":35790},{\"end\":35815,\"start\":35799},{\"end\":35822,\"start\":35815},{\"end\":35830,\"start\":35822},{\"end\":35838,\"start\":35830},{\"end\":35845,\"start\":35838},{\"end\":35852,\"start\":35845},{\"end\":35859,\"start\":35852},{\"end\":36149,\"start\":36141},{\"end\":36157,\"start\":36149},{\"end\":36342,\"start\":36334},{\"end\":36349,\"start\":36342},{\"end\":36358,\"start\":36349},{\"end\":36367,\"start\":36358},{\"end\":36377,\"start\":36367},{\"end\":36558,\"start\":36552},{\"end\":36564,\"start\":36558},{\"end\":36571,\"start\":36564},{\"end\":36578,\"start\":36571},{\"end\":36587,\"start\":36578},{\"end\":36594,\"start\":36587},{\"end\":36602,\"start\":36594},{\"end\":36609,\"start\":36602},{\"end\":36617,\"start\":36609},{\"end\":36625,\"start\":36617},{\"end\":36633,\"start\":36625},{\"end\":36643,\"start\":36633},{\"end\":36654,\"start\":36643},{\"end\":36661,\"start\":36654},{\"end\":36667,\"start\":36661},{\"end\":36675,\"start\":36667},{\"end\":36685,\"start\":36675},{\"end\":36697,\"start\":36685},{\"end\":36704,\"start\":36697},{\"end\":36710,\"start\":36704},{\"end\":36716,\"start\":36710},{\"end\":36723,\"start\":36716},{\"end\":37115,\"start\":37107},{\"end\":37121,\"start\":37115},{\"end\":37286,\"start\":37277},{\"end\":37295,\"start\":37286},{\"end\":37311,\"start\":37295},{\"end\":37317,\"start\":37311}]", "bib_venue": "[{\"end\":30845,\"start\":30736},{\"end\":31861,\"start\":31844},{\"end\":32318,\"start\":32264},{\"end\":33075,\"start\":33022},{\"end\":33376,\"start\":33312},{\"end\":33600,\"start\":33538},{\"end\":34087,\"start\":34008},{\"end\":34357,\"start\":34274},{\"end\":36018,\"start\":35947},{\"end\":36242,\"start\":36223},{\"end\":36505,\"start\":36447},{\"end\":36939,\"start\":36839},{\"end\":37399,\"start\":37381},{\"end\":30446,\"start\":30424},{\"end\":30734,\"start\":30627},{\"end\":31062,\"start\":31017},{\"end\":31207,\"start\":31163},{\"end\":31366,\"start\":31331},{\"end\":31435,\"start\":31418},{\"end\":31842,\"start\":31772},{\"end\":32262,\"start\":32202},{\"end\":32587,\"start\":32528},{\"end\":32716,\"start\":32655},{\"end\":32795,\"start\":32732},{\"end\":33020,\"start\":32961},{\"end\":33310,\"start\":33240},{\"end\":33536,\"start\":33468},{\"end\":33760,\"start\":33711},{\"end\":34006,\"start\":33921},{\"end\":34272,\"start\":34174},{\"end\":34582,\"start\":34514},{\"end\":34606,\"start\":34584},{\"end\":35050,\"start\":34959},{\"end\":35283,\"start\":35230},{\"end\":35658,\"start\":35640},{\"end\":35945,\"start\":35859},{\"end\":36221,\"start\":36157},{\"end\":36445,\"start\":36377},{\"end\":36837,\"start\":36739},{\"end\":37170,\"start\":37121},{\"end\":37379,\"start\":37317}]"}}}, "year": 2023, "month": 12, "day": 17}