{"id": 252819090, "updated": "2023-06-08 17:19:42.869", "metadata": {"title": "Generating Temporally-ordered Event Sequences via Event Optimal Transport", "authors": "[{\"first\":\"Bo\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Yubo\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Kang\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Jiexin\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Xiaojian\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Qiuxia\",\"last\":\"Li\",\"middle\":[]}]", "venue": "COLING", "journal": "Proceedings of the 29th International Conference on Computational Linguistics", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Generating temporally-ordered event sequences in texts is important to natural language processing. Two emerging tasks in this direction are temporal event ordering (rearranging the set of events to correct order) and event infilling (generating an event at a specified position). To tackle the two related tasks, the existing method adopts a vanilla sequence-to-sequence model via maximum likelihood estimation (MLE). However, applying this approach to these tasks will cause two issues. One issue is that the MLE loss emphasizes strict local alignment and ignores the global semantics of the event. The other issue is that the model adopts a word-level objective to model events in texts, failing to evaluate the predicted results of the model from the perspective of event sequence. To alleviate these issues, we present a novel model to tackle the generation of temporally-ordered event sequences via Event Optimal Transport (EOT). First, we treat the events in the sequence as modeling units and explicitly extract the semantics of the events. Second, to provide event sequence-level evaluation of the predicted results of the model, we directly match events in sequences. Extensive experimental results show that our approach outperforms previous models on all evaluation datasets. In particular, the accuracy is improved by 7.7%, and the Macro F1 is improved by 7.2% on one of the datasets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2022.coling-1.162", "pubmed": null, "pubmedcentral": null, "dblp": "conf/coling/Zhou000XJL22", "doi": null}}, "content": {"source": {"pdf_hash": "624c6580cf5abd31db63b38596ffa3622c731b00", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.coling-1.162.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6b087cdb645cd5971789da58520b080007aced28", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/624c6580cf5abd31db63b38596ffa3622c731b00.txt", "contents": "\nGenerating Temporally-ordered Event Sequences via Event Optimal Transport\n1884 October 12-17, 2022\n\nBo Zhou bo.zhou@nlpr.ia.ac.cn \nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nNational Laboratory of Pattern Recognition\nCASIA\n\n\nYubo Chen yubo.chen@nlpr.ia.ac.cn \nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nNational Laboratory of Pattern Recognition\nCASIA\n\n\nKang Liu kliu@nlpr.ia.ac.cn \nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nNational Laboratory of Pattern Recognition\nCASIA\n\n\nBeijing Academy of Artificial Intelligence\n\n\nJun Zhao jzhao@nlpr.ia.ac.cn \nSchool of Artificial Intelligence\nUniversity of Chinese Academy of Sciences\n\n\nNational Laboratory of Pattern Recognition\nCASIA\n\n\nJiexin Xu jiexinx@cmbchina.com \nChina Merchants Bank\n\n\nXiaojian Jiang jiangxiaojian@cmbchina.com \nChina Merchants Bank\n\n\nQiuxia Li \nChina Merchants Bank\n\n\nGenerating Temporally-ordered Event Sequences via Event Optimal Transport\n\nProceedings of the 29th International Conference on Computational Linguistic\nthe 29th International Conference on Computational Linguistic18751884 October 12-17, 20221875\nGenerating temporally-ordered event sequences in texts is important to natural language processing. Two emerging tasks in this direction are temporal event ordering (rearranging the set of events to correct order) and event infilling (generating an event at a specified position). To tackle the two related tasks, the existing method adopts a vanilla sequence-to-sequence model via maximum likelihood estimation (MLE). However, applying this approach to these tasks will cause two issues. One issue is that the MLE loss emphasizes strict local alignment and ignores the global semantics of the event. The other issue is that the model adopts a word-level objective to model events in texts, failing to evaluate the predicted results of the model from the perspective of event sequence. To alleviate these issues, we present a novel model to tackle the generation of temporally-ordered event sequences via Event Optimal Transport (EOT). First, we treat the events in the sequence as modeling units and explicitly extract the semantics of the events. Second, to provide event sequence-level evaluation of the predicted results of the model, we directly match events in sequences. Extensive experimental results show that our approach outperforms previous models on all evaluation datasets. In particular, the accuracy is improved by 7.7%, and the Macro F1 is improved by 7.2% on one of the datasets.\n\nIntroduction\n\nGenerating temporally-ordered event sequences in texts is crucial to many artificial intelligence applications, such as discourse understanding (Nie et al., 2019), dialog generation  and stock prediction (Ding et al., 2016). Temporal event ordering and event infilling are two challenging tasks in this line of work. The former refers to the rearrangement of an unordered sequence of events to an ordered sequence of events, and the  Figure 1: The temporal event ordering task, event infilling task and a single generation model handling these two tasks.\n\nlatter refers to inferring missing events in an incomplete sequence of events. Figure 1 shows an example of the temporal event ordering task and event infilling task in (a) and (b), respectively. For the temporal event ordering, given an unordered sequence (e 3 , e 2 , e 4 , e 1 ) as input, the output is ordered sequence (e 1 , e 2 , e 3 , e 4 ). For the event infilling, given an incomplete sequence (e 1 , e 2 , e 4 ) as input, the output is complete sequence (e 1 , e 2 , e 3 , e 4 ) with the infilled event e 3 . To handle these two related tasks, the currently existing method (Lin et al., 2021) employs a unified generation model which is shown in Figure 1 (c). Their model takes incomplete unordered events as input sequence and outputs a complete ordered event sequence, based on the sequence-to-sequence (Seq2Seq) model via maximum likelihood estimation (MLE), which maximizes the likelihood of the next word conditioned on its previous ground-truth words. Such an approach adopts cross-entropy loss as the objective, essentially measuring the word difference at each position of the target sequence and providing a word-level training loss. Although their model has shown good performance in handling the two tasks, there are still two issues.\n\nFirst, the MLE loss emphasizes strict local alignment and ignores the global semantics of the event. For example in Figure 2, the MLE loss will give the Figure 2: Comparison between word-level score and event-level score on two predicted events.\n\npredicted event A a high score and B a low score because all words except one in event A are aligned with the words in the answer event yet no word event B is aligned with the words in the answer event. However, the loss is expected to give event A a low score and event B a high score because from the perspective of event semantics, event A is completely different from the answer event and event B is the same as the answer event. Therefore, we should consider the overall semantics of the event, rather than the strict alignment of words within the event.\n\nSecond, the goal of the model is to infer events in texts, but the sequence-to-sequence model uses a word-level objective so it fails to evaluate the result from the perspective of event sequence. For example in Figure 1, the correct predicted event sequence is (e 1 , e 2 , e 3 , e 4 ). If the model predicts an event sequence (e 4 , e 1 , e 2 , e 3 ), the MLE loss will give it a low score because no event is predicted correctly compared to the answer event sequence. However, the sequence (e 4 , e 1 , e 2 , e 3 ) should not get a low score, because at least the orders of 3 events are predicted correctly. Therefore, how to evaluate the predicted results of the model from the perspective of event sequence is a challenging problem.\n\nTo tackle the above issues, we introduce a novel method for the generation of temporallyordered event sequences via Event Optimal Transport (EOT). Specifically, for the first issue, we treat the events in the sequence as modeling units and explicitly extract the semantics of the events. For the second issue, we propose to use optimal transport to directly match events in sequences. The EOT allows end-to-end supervised training and acts as an effective sequence-level regularization to the MLE loss.\n\nIn summary, our contributions can be summarized as follows:\n\n\u2022 We introduce a novel method for the generation of temporally-ordered event sequences via Event Optimal Transport (EOT), which treats the events in the sequence as modeling units and explicitly extracts the semantics of the events.\n\n\u2022 We directly match events in sequences to provide event sequence-level evaluation of the predicted results of the model.\n\n\u2022 Extensive experimental results demonstrate the superiority of the proposed method on all evaluation datasets. Specifically, the accuracy is improved by 7.7%, and the Macro F1 is improved by 7.2% on one of the datasets.\n\n\nRelated Work\n\nScript Event Prediction Given context event sequence, script event prediction aims at predicting the subsequent event from a candidate list. The task is first proposed by Chambers and Jurafsky (2008), and a statistical model is proposed to learn the cooccurrence between events. Jans et al. (2012) leverages a bigram model to model the temporal order between events explicitly. The above two methods are count-based, and then researchers have proposed methods based on neural networks. Granroth-Wilding and Clark (2016) proposes a neural network based model for simultaneously learning word embedding and composition function. In Wang et al. (2017), they propose an LSTM based model to integrate order information and event relation. Li et al. (2018) treats event chain as a subgraph and leverages recurrent networks to better model relatedness between events in the candidate list with events in the graph. Lv et al. (2019) proposes a model that integrates event-level and chainlevel attentions to better leverage information contained in event chain. Zhou et al. (2021) proposes a multi-task self-supervised model to cope with the problem of lack of training data in script event prediction. To incorporate event circumstances into the narrative event prediction, Wang et al. (2021) adopts the two multi-head attention to retrieve circumstances at the local and global levels. In this paper, we consider two related tasks of temporal event ordering and event infilling and leverage a single generation model to tackle these two tasks.\n\nOptimal Transport in NLP Optimal transport has been applied in NLP for a variety of tasks recently. In Kusner et al. (2015), the author proposes the Word Mover's Distance (WMD), a \n\n\n[E3] proposed [A] The British Prime Minister proposed a referendum [E2] held [A] Britain held a referendum on Brexit [E] triggered [A] The referendum triggered panic in the stock market [E1] sold [A] Investors sold stocks [E1] Investors sold stocks [E2] Britain held a referendum on Brexit [E3] The British Prime Minister proposed a referendum\n\nC 11 C 12 C 13 C 14 C 21 C 22 C 23 C 24 C 31 C 32 C 33 C 34 T 11 T 12 T 13 T 14 T 21 T 22 T 23 T 24 T 31 T 32 T 33 T 34 novel distance function between text documents that measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to \"travel\" to reach the embedded words of another document. Later in Xu et al. (2018), a novel Wasserstein method with a distillation mechanism is proposed, yielding joint learning of word embeddings and topics. The cross-lingual correspondence problem is cast as an optimal transport problem in Alvarez-Melis and Jaakkola (2018), and the Gromov-Wasserstein distance is exploited for the alignment of word embedding spaces. A content-aware sparse attention module based on optimal transport is proposed in  to deal with textual network embedding problem. Li et al. (2020) proposes student-forcing optimal transport to tackle the exposure bias problem in text-generation models trained by maximum likelihood estimation.  formulates the quest of vocabularization -finding the best token dictionary with a proper size -as an optimal transport problem and proposes a simple and efficient solution without trial training. A time-aware optimal transport distance is introduced  for learning the model to compress the eventgraphs of news articles in an unsupervised manner. In this paper, we leverage optimal transport and propose event OT to directly match events in event sequences to provide sequence-level supervision of the predicted results.\n\n\nMethodology\n\nThe overall structure of our model is illustrated in Figure 3. The input of the model is a sequence of events x = (e 1 , ..., e n ), which is incomplete and unordered, and the output of the model is a complete and ordered sequence of events y = (e 1 , ..., e m ). We represent e in the event sequence as the concatenation of its predicate with all its arguments.\n\n\nGeneration Model\n\nTo leverage the power of pretrained transformers, we base the underlying architecture for our model on BART (Lewis et al., 2020). BART is a denoising autoencoder for pretraining sequence-to-sequence models which can be seen as generalizing BERT (Kenton and Toutanova, 2019) (due to the bidirectional encoder), GPT (Radford and Narasimhan, 2018) (with the left-to-right decoder), and other recent pretraining schemes.\n\nSimilar to previous work (Lin et al., 2021), we then prepend a special word [Ei] in front of each event in input event sequence x. For the output, if e j in y is one of the input events e i in x, then we prepend special words [Ei]v e j [A] before e j , where v e j is the predicate of event e j . Otherwise, the special words [E]v e j [A] are used in front of e j . An example of the above is shown in Figure 3. The use of [Ei] helps the model differentiate between events in the input and output sequences, which facilitates optimal transport between events.\n\n\nEvent Optimal Transport\n\nWe first briefly introduce the optimal transport. Given two spaces X and Y , the Kantorovich formulation of optimal transport aims to find a probability measure \u03b3 on X \u00d7 Y that attains the following infimum:\ninf X\u00d7Y c(x, y)d\u03b3(x, y) | \u03b3 \u2208 \u0393(\u00b5, \u03bd) (1) where \u0393(\u00b5, \u03bd) is the set of probability measures on X \u00d7 Y with marginal \u00b5 on X and \u03bd on Y .\nNow given two discrete probability measures \u00b5 and \u03bd, which can be represented by Dirac measure as below:\n\u00b5 = n i=1 u i \u03b4 x i , \u03bd = m j=1 v i \u03b4 y j(2)\nwhere \u03b4 x is the Dirac measure sitting at x. The\ncoefficients u = {u i } n i=1 and v = {v j } m j=1 satisfy the constraints n i=1 u i = m j=1 v i = 1\nas both \u00b5 and \u03bd are probability measures. Under such a setting, the objective function and the constraint in the primal Kantorovich problem are then:\nmin T i,j T ij \u00b7 c x i , y j = min T T , C s.t. j T ij = u i , i = 1, ..., n, i T ij = v j , j = 1, ..., m, T \u2208 R n\u00d7m + .(3)\nwhere C is the cost matrix with C ij = c x i , y j and c is the cost function. T , C = Tr(T C) denotes the Frobenius dot-product, here Tr denotes the trace of a matrix.\n\nBecause the exact solution of (3) is is highly intractable (Arjovsky et al., 2017), researchers have proposed some approximate algorithms, such as the Sinkhorn (Cuturi, 2013) and IPOT (Xie et al., 2020) algorithms. Here we choose the IPOT algorithm to approximate the minimizer of (3). The details of the IPOT algorithm are shown in Algorithm 1.\n\nAssume the output (the last hidden state) of the BART encoder and decoder are H e \u2208 R Le\u00d7d and H d \u2208 R L d \u00d7d respectively, where L e and L d are T (t+1) = diag(\u03b4)Qdiag(\u03c3). 9: end for 10: return T , C lengths of input and output sequences. We first partition H e into n groups where each group corresponds to features of words in an event (note that the input to our model is x = {e 1 , ..., e n }), then features of words in a group are averaged to obtain a vector sequence S = {h i } n i=1 . Note that besides average, we also tried other aggregation functions, such as max pooling, attention, etc., but they all performed worse than average. The possible reason is that there are already more complex aggregation functions such as attention in BART, so it is enough to use average as the aggregation function of the BART output.\nAlgorithm 1 IPOT algorithm Input: Feature vectors S = {x i } n 1 ,S = {x i } m 1 Parameter: Generalized stepsize 1 \u03b2 Output: T , C 1: \u03c3 = 1 m 1 m ,T (1) = 1 n 1 m . 2: C ij = c(x i , x j ),A ij = e \u2212 C ij\nFor H d , it's first input to a linear layer:\nH l = H d W + b(4)\nwhere W \u2208 R d\u00d7V and V is the vocabulary size. We then use the argmax function over H l to obtain the predicted indices i \u2208 Z L d + . We identify indices of special word [E i ] in i and these identified indices are used to partition H d into k groups where each group corresponds to features of words in an event. Note that there may be cases where there are no identified indices, which are most likely to occur at the beginning of training. To solve possible errors, we partition H d into L d groups when this happens. Finally after averaging, we obtain another vector sequence S = {h j } k j=1 .\n\n\nTraining and Optimization\n\nIn order to use a unified generation model to handle temporal event ordering and event infilling tasks, we first construct input-output data pairs related to these two tasks. Specifically, given an ordered Compute the cost matrix C based on S and S via cosine distance.\n\n\n8:\n\nCompute the EOT loss defined in (4).\n\n\n9:\n\nend for 10:\n\nUpdate model parameters by optimizing loss in (6). 11: end for event sequence y as defined above, we follow previous work (Lewis et al., 2020;Lin et al., 2021) to corrupt it to obtain the required input x by two steps. The first step is to shuffle events: performing a random shuffling of the complete ordered event sequence y to produce an unordered event sequence x . The second step is to delete events: randomly deleting each event in x with probability p to produce the incomplete unordered input event sequence x.\n\nTake Figure 1 (c) for example, the complete ordered event sequence on the right is y. After performing event shuffling and event deletion, we obtain incomplete unordered input event sequence x on the left. Now we introduce the optimization process of the model. After obtaining two vector sequences S = {h i } n i=1 and S = {h j } k j=1 as described in the previous section, we can compute the eventlevel OT loss using the IPOT algorithm described above:\nL EOT = IPOT(S, S )(5)\nMeanwhile, suppose the input sequence is x = (w 1 , ..., w L ) and the gold output sequence is y = (w 1 , ..., w L ) , where L and L are numbers of words. We also have the following maximum likelihood estimation (MLE) loss:\nL M LE = L t=1 log p \u03b8 (w t |w <t , x)(6)\nWe then combine the two loss functions to obtain the following loss:\nL = 1 M M n=1\n(\u2212L M LE (x n , y n )+\u03b1L EOT (S n , S n )) (7) where M is the number of training pairs. The parameters are updated by minimizing this loss and the full algorithm is summarized in Algorithm 2.\n\n\nExperiment\n\n\nExperimental Setup\n\nDataset Following Lin et al. (2021), the temporal event sequences are extracted from the EventsNarratives corpus (Yao and Huang, 2018). The SRL model from AllenNLP (Gardner et al., 2018) is used to extract verbs (events) and their arguments. Then, only events in different sentences are connected to construct temporal event sequences, and only event chains associated with a common entity are included. We train our model on 100,000 sequences extracted by the above procedure. Two different orders are used to scramble each sequence, resulting in a total of 200,000 training data.\n\nFor testing, two out-of-domain English datasets CaTeRS (Mostafazadeh et al., 2016) and MCTaco (Zhou et al., 2019) are used to extract the test temporal event sequences. CaTeRS includes annotations of events and their causal and temporal relations on short stories. MCTaco is a question answering dataset for evaluating the model's capability of understanding temporal commonsense. Following Lin et al. (2021), 842 event sequences are extracted for CaTeRS and after applying two different permutations to each sequence, 1684 CaTeRS examples are finally obtained. For MCTaco, 585 test sequences are extracted.\n\nTraining Details We choose the cosine distance as the cost function in the event optimal transport and the hyper-parameter \u03b1 is set to 0.1. The learning rate of our model is 1e-5, and a polynomial decay scheduling with 500 steps of warm-up is used. We set the batch size to 64, the models are trained for 10 epochs, with 2000 updates each epoch. We set the event deletion probability to 0.15 for the deletion training strategy. The BART-large pretrained model from Hugging-Face's Transformers library (Wolf et al., 2020) is used as the underlying structure which is the same as previous work. Beam search with the beam size 4 is used when decoding the output event sequences during the evaluation for temporal event ordering. Baselines We compare our model with the following baseline methods for temporal event ordering:\n\n\u2022 BERT-based Pairwise Model + SSVM  leverages a BERT-based model (Kenton and Toutanova, 2019) to compute pairwise scores for two events in the output, and the final output is then obtained by solving an ILP over all the pairwise scores.\n\n\u2022 BERT-based Pointer Network first uses BERT to extract representations for events that are fed into an LSTM-based pointer network to compute the probability for ordering.\n\n\u2022 TemporalBART (Lin et al., 2021) is based on BART (Lewis et al., 2020), and special words are prepended in front of events in input and output sequences to provide extra clues.\n\n\u2022 TemporalBART-indexed (Lin et al., 2021) is the same as TemporalBART except that the indices of the special words prepended before events are considered.\n\nFor event infilling, we compare our model with these extra baselines:\n\n\u2022 HAQAE (Weber et al., 2018) is a vector quantized variational autoencoder with a latent space defined by a hierarchy of categorical variables which encodes schema knowledge.\n\n\u2022 GPT-2 ) is a transformerbased pretrained language model which is used as the underlying structure in many generation tasks.\n\n\u2022 Infilling GPT-2 (Qin et al., 2020) generates the infilling events conditioned on both the prefix events and the events after the insertion position.\n\n\nResults on Temporal Event Ordering\n\nCaTeRS Dataset Experimental results on CaTeRS are shown in Table 1, pairwise accuracy is used to calculate the proportion of ordered event pairs in the predicted sequence. Among all the approaches, our method performs best. It achieves the best performance on both all sequences and long sequences. The reasons may be that compared with the BERT-based pointer network, our model can condition the word-level embeddings of the events when generating the output events instead of condensed event embeddings. Compared with BART-based models, our model treats the events in the sequence as modeling units and explicitly extracts the semantics of the events instead of emphasizing strict local alignment of words.\n\n\nMCTaco Dataset\n\nThe accuracy on predicting the temporal relation of event in question and event in answer is computed, since only gold temporal relation of event in question and event in answer is known for each test sequence. The macro F1 score is also computed because the proportion of before/after questions is unbalanced in MCTaco.\n\nOur EOT model outperforms all the baselines, in particular, our model outperforms TemporalBARTindexed by 7.7% in accuracy and 7.2% in Macro F1. We attribute the significant improvement to the direct events matching in sequences by our model. We will further demonstrate the effectiveness of direct events matching in the following experiments.  event ordering which better tests its capability as a generative model. Specifically, for each event sequence in CaTeRS, we first randomly delete an event e * in the sequence, and let the remaining events be denoted as (e 1 , ..., e N ). We want to test whether the model can insert the deleted event e * to its original position, thus we use the generation probability of the model to rank the N + 1 sequences which are obtained by inserting e * to N + 1 different positions. The higher the model ranks the original sequence, the better the model is able to capture the relationship between seen events and possibly generated unseen events. Table 2 shows the results, and the top-1 and top-2 exact match (EM) are used to evaluate the results, which calculate the proportion of gold sequences that the model ranks first and second above. Our model outperforms all the baselines on both all the event sequences and long sequences, which again demonstrates the effectiveness of considering event-level semantics and direct events matching in sequences. Another observation is that compared with the TenporalBART-indexed model, the performance of our EOT model on longer sequences is more significant than on all the sequences. One possible reason is that longer sequences have more events which are more helpful for the model to do the event-level matching.\n\n\nOrdering Unseen Events for CaTeRS\n\n\nResults on Event Infilling\n\nNow we consider temporal event infilling and the dataset CaTeRS is used. Given an event sequence from CaTeRS, we first randomly delete an event to obtain (e 1 , ..., e i\u22121 , e i+1 , ..., e n ). Then the model should generate an infilled event e * at position i and the new sequence (e 1 , ..., e i\u22121 , e i , e i+1 , ..., e n ) is expected to be temporally ordered.\n\nWe measure the quality of generated events through human evaluation. Given a sequence with a generated event at some position, 3 raters are asked to score this sequence in terms of the coherence (How coherent the generated event is with the context?) and temporality (Does the generated event occur in the right order concerning the context?) for the generated event and both scores are from {0, 1, 2}. The final scores are the majority scores of the 3 raters for both coherence and temporality. We then randomly sample 15 sequences from CaTeRS and the averaged scores are took as the metric.\n\nThe result is shown in Table 3, and our EOT model achieves better performance than all the baseline models in terms of coherence and temporality. The reason may be that by explicitly matching events in input and output sequence, the model can generate an event that is more relevant to the scenario of events in the input sequence.  We show two examples of events generated by different models in Figure 4. As we can see in Figure 4 (a), the event generated by infilling GPT-2 is less relevant to the context events. The order of the event generated by TemporalBART-indexed is inappropriate, although it's coherent with the scenario of the input events. The event generated by our model EOT is both coherent and temporally ordered. Another example is shown in 4 (b).\n\n\nFurther Demonstration of EOT's Effectiveness\n\nTo further demonstrate the effectiveness of our event optimal transport, we compare EOT with two extra models. Recall that in our model, the output (the last hidden state) of the BART encoder and decoder are H e \u2208 R Le\u00d7d and H d \u2208 R L d \u00d7d respectively, then two vector sequences S = {h i } n i=1 and S = {h j } k j=1 corresponding to events in input and output are extracted to construct event OT loss. Now, we directly treat vectors in H e and H d as S and S respectively to construct word-level loss, and this model is called Word Optimal Transport (WOT). We also test the model which com-   Table 1 and 2. The TBART-idx represents the TemporalBARTindexed model. Pacc means Pairwise Accuracy and 3+ means sequences with 3 or more events. bines event and word-level OT loss and is called EOT+WOT. Experimental results are shown in Figure 5, from which we can make the following observations:\n\n(1) The WOT model achieves comparable results with the TemporalBART-indexed model and performs better on some metrics. This shows that imposing global sequence-level guidance via new supervision is effective, although the effect is not obvious, because it did not take into account the event-level matching.\n\n(2) When event-level matching is considered, the EOT model achieves performance improvements on almost all metrics compared to the WOT model, and the EOT model outperforms the TemporalBART-indexed model on all metrics. This demonstrates that incorporating event-level matching can further improve the performance of optimal transport and outperform the baseline models.\n\n(3) When we combine word-level with event-level OT, the performance is expected to get better. Unfortunately, the performance of EOT+WOT decreases compared with EOT. One possible reason is that word-level matching and event-level matching conflict with each other to some extent: two events may match well, but the words in the two events may not match each other.\n\n\nConclusion\n\nIn this paper, we consider a single generation model which can support inferences in the two related tasks. We introduce a novel method for the generation of temporally-ordered event sequences via Event Optimal Transport (EOT). Compared with the MLE-based Seq2Seq model, our approach has two advantages: (i) we treat the events in the sequence as modeling units and explicitly extract the semantics of the events; (ii) we directly match events in sequences to provide event sequence-level evaluation of the predicted results of the model. Experimental results show the superiority of our model on all evaluation datasets. Specifically, the accuracy is improved by 7.7%, and the Macro F1 is improved by 7.2% on one of the datasets.\n\n\ne1:The British Prime Minister proposed a referendum. e2:Britain held a referendum on Brexit. e3:The referendum triggered panic in the stock market. e4:Investors sold stocks.\n\nFigure 3 :\n3Overall architecture of the proposed generation model based on event optimal transport.\n\nFigure 4 :Figure 5 :\n45Two examples of events generated by infilling GPT-2, TemporalBART-indexed and our model EOT. The input events are in green and orange, while events generated by the models are in blue and purple.Pacc. 3+ Pacc. Acc. Macro F1 EM Top2 EM 3+ EM 3+ The performances of the 4 models on the 8 evaluation metrics which correspond to\n\nTable 2 :\n2The results of ordering unseen events on se-\nquences from dataset CaTeRS. Longer seq means re-\nsults for sequences containing more than three events. \nThe symbol  \u2020 has same meaning as Table 1. \n\n\n\nTable 3 :\n3The human evaluation result for event infilling on dataset CaTeRS.\n\n\n[INSERTED EVENT]e2: Her mom told her if she gets all good grades, she could get one.e3: Maddie's mom took her to get her puppy as her reward.Infilling GPT-2: The first time I saw the movie, I was in my early 20s.TemporalBART-indexed: Maddie's mom was a teacher. EOT: Maddie wanted to get a puppy. e1: Ashley went to the cabinet. e2: Ashley pulled out a bottle of wine. e4: She opened the wine.Infilling GPT-2: She poured it into her glass. TemporalBART-indexed: She drank the wine. EOT: The wine was good.\nAcknowledgementsThis work is supported by the National Key Research and Development Program of China (No.2020AAA0106400), the National Natural Science Foundation of China (No.62176257,  61976211, 61922085). This work is also supported by the Strategic Priority Research Program of Chinese Academy of Sciences (Grant No.XDA27020200), the Youth Innovation Promotion Association CAS, and Yunnan Provincial Major Science and Technology Special Plan Projects (No.202202AD080004).\nGromov-wasserstein alignment of word embedding spaces. David Alvarez-Melis, Tommi Jaakkola, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingDavid Alvarez-Melis and Tommi Jaakkola. 2018. Gromov-wasserstein alignment of word embedding spaces. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1881-1890.\n\nWasserstein generative adversarial networks. Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou, PMLRInternational conference on machine learning. Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. 2017. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214-223. PMLR.\n\nUnsupervised learning of narrative event chains. Nathanael Chambers, Dan Jurafsky, Proceedings of ACL-08: HLT. ACL-08: HLTNathanael Chambers and Dan Jurafsky. 2008. Unsu- pervised learning of narrative event chains. In Pro- ceedings of ACL-08: HLT, pages 789-797.\n\nImproving textual network embedding with global attention via optimal transport. Liqun Chen, Guoyin Wang, Chenyang Tao, Dinghan Shen, Pengyu Cheng, Xinyuan Zhang, Wenlin Wang, Yizhe Zhang, Lawrence Carin, ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference. Association for Computational Linguistics (ACLLiqun Chen, Guoyin Wang, Chenyang Tao, Ding- han Shen, Pengyu Cheng, Xinyuan Zhang, Wenlin Wang, Yizhe Zhang, and Lawrence Carin. 2020. Im- proving textual network embedding with global at- tention via optimal transport. In ACL 2019-57th An- nual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 5193-5202. Association for Computational Linguis- tics (ACL).\n\nSinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems. Marco Cuturi, 26Marco Cuturi. 2013. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neu- ral information processing systems, 26.\n\nKnowledge-driven event embedding for stock prediction. Xiao Ding, Yue Zhang, Ting Liu, Junwen Duan, Proceedings of coling 2016, the 26th international conference on computational linguistics: Technical papers. coling 2016, the 26th international conference on computational linguistics: Technical papersXiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. 2016. Knowledge-driven event embedding for stock prediction. In Proceedings of coling 2016, the 26th international conference on computational linguis- tics: Technical papers, pages 2133-2142.\n\nAllennlp: A deep semantic natural language processing platform. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, F Nelson, Matthew Liu, Michael Peters, Luke Schmitz, Zettlemoyer, Proceedings of Workshop for NLP Open Source Software (NLP-OSS). Workshop for NLP Open Source Software (NLP-OSS)Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F Liu, Matthew Pe- ters, Michael Schmitz, and Luke Zettlemoyer. 2018. Allennlp: A deep semantic natural language process- ing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 1-6.\n\nWhat happens next? event prediction using a compositional neural network model. Mark Granroth, - Wilding, Stephen Clark, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence30Mark Granroth-Wilding and Stephen Clark. 2016. What happens next? event prediction using a com- positional neural network model. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 30.\n\nDeep structured neural network for event temporal relation extraction. Rujun Han, I-Hung Hsu, Mu Yang, Aram Galstyan, Ralph Weischedel, Nanyun Peng, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)Rujun Han, I-Hung Hsu, Mu Yang, Aram Galstyan, Ralph Weischedel, and Nanyun Peng. 2019. Deep structured neural network for event temporal rela- tion extraction. In Proceedings of the 23rd Confer- ence on Computational Natural Language Learning (CoNLL), pages 666-106.\n\nSkip n-grams and ranking functions for predicting script events. Bram Jans, Steven Bethard, Ivan Vuli\u0107, Marie Francine Moens, Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. the 13th Conference of the European Chapter of the Association for Computational LinguisticsBram Jans, Steven Bethard, Ivan Vuli\u0107, and Marie Francine Moens. 2012. Skip n-grams and ranking functions for predicting script events. In Proceedings of the 13th Conference of the Euro- pean Chapter of the Association for Computational Linguistics, pages 336-344.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of NAACL-HLT. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina ToutanovaNAACL-HLTJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171-4186.\n\nFrom word embeddings to document distances. Matt Kusner, Yu Sun, Nicholas Kolkin, Kilian Weinberger, PMLRInternational conference on machine learning. Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word embeddings to doc- ument distances. In International conference on ma- chine learning, pages 957-966. PMLR.\n\nBart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871-7880.\n\nImproving text generation with student-forcing optimal transport. Jianqiao Li, Chunyuan Li, Guoyin Wang, Hao Fu, Yuhchen Lin, Liqun Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Jianqiao Li, Chunyuan Li, Guoyin Wang, Hao Fu, Yuhchen Lin, Liqun Chen, Yizhe Zhang, Chenyang Tao, Ruiyi Zhang, Wenlin Wang, et al. 2020. Im- proving text generation with student-forcing optimal transport. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9144-9156.\n\nTimeline summarization based on event graph compression via time-aware optimal transport. Manling Li, Tengfei Ma, Mo Yu, Lingfei Wu, Tian Gao, Ji Heng, Kathleen Mckeown, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingManling Li, Tengfei Ma, Mo Yu, Lingfei Wu, Tian Gao, Heng Ji, and Kathleen McKeown. 2021. Timeline summarization based on event graph compression via time-aware optimal transport. In Proceedings of the 2021 Conference on Empirical Methods in Natu- ral Language Processing, pages 6443-6456.\n\nConstructing narrative event evolutionary graph for script event prediction. Zhongyang Li, Xiao Ding, Ting Liu, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial IntelligenceZhongyang Li, Xiao Ding, and Ting Liu. 2018. Con- structing narrative event evolutionary graph for script event prediction. In Proceedings of the 27th International Joint Conference on Artificial Intelli- gence, pages 4201-4207.\n\nConditional generation of temporally-ordered event sequences. Nathanael Shih-Ting Lin, Greg Chambers, Durrett, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics1Shih-Ting Lin, Nathanael Chambers, and Greg Durrett. 2021. Conditional generation of temporally-ordered event sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7142-7157, Online. Association for Computational Linguistics.\n\nSam-net: Integrating event-level and chain-level attentions to predict what happens next. Shangwen Lv, Wanhui Qian, Longtao Huang, Jizhong Han, Songlin Hu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Shangwen Lv, Wanhui Qian, Longtao Huang, Jizhong Han, and Songlin Hu. 2019. Sam-net: Integrating event-level and chain-level attentions to predict what happens next. In Proceedings of the AAAI Con- ference on Artificial Intelligence, volume 33, pages 6802-6809.\n\nCaters: Causal and temporal relation scheme for semantic annotation of event structures. Nasrin Mostafazadeh, Alyson Grealish, Nathanael Chambers, James Allen, Lucy Vanderwende, Proceedings of the Fourth Workshop on Events. the Fourth Workshop on EventsNasrin Mostafazadeh, Alyson Grealish, Nathanael Chambers, James Allen, and Lucy Vanderwende. 2016. Caters: Causal and temporal relation scheme for semantic annotation of event structures. In Pro- ceedings of the Fourth Workshop on Events, pages 51-61.\n\nDissent: Learning sentence representations from explicit discourse relations. Allen Nie, Erin Bennett, Noah Goodman, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAllen Nie, Erin Bennett, and Noah Goodman. 2019. Dissent: Learning sentence representations from ex- plicit discourse relations. In Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics, pages 4497-4510.\n\nBack to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning. Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena Hwang, Ronan Bras, Antoine Bosselut, Choi Yejin, Lianhui Qin, Vered Shwartz, Peter West, Chandra Bha- gavatula, Jena Hwang, Ronan Bras, Antoine Bosse- lut, and Choi Yejin. 2020. Back to the future: Un- supervised backprop-based decoding for counterfac- tual and abductive commonsense reasoning. pages 794-805.\n\nImproving language understanding by generative pretraining. Alec Radford, Karthik Narasimhan, Alec Radford and Karthik Narasimhan. 2018. Im- proving language understanding by generative pre- training.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2018. Language models are unsupervised multitask learners.\n\nIncorporating circumstances into narrative event prediction. Shichao Wang, Xiangrui Cai, Hongbin Wang, Xiaojie Yuan, Findings of the Association for Computational Linguistics: EMNLP 2021. Shichao Wang, Xiangrui Cai, Hongbin Wang, and Xi- aojie Yuan. 2021. Incorporating circumstances into narrative event prediction. In Findings of the Associ- ation for Computational Linguistics: EMNLP 2021, pages 4840-4849.\n\nIntegrating order information and event relation for script event prediction. Zhongqing Wang, Yue Zhang, Ching Yun Chang, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingZhongqing Wang, Yue Zhang, and Ching Yun Chang. 2017. Integrating order information and event rela- tion for script event prediction. In Proceedings of the 2017 Conference on Empirical Methods in Natu- ral Language Processing, pages 57-67.\n\nHierarchical quantized representations for script generation. Noah Weber, Leena Shekhar, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingNiranjan Balasubramanian, and Nathanael ChambersNoah Weber, Leena Shekhar, Niranjan Balasubrama- nian, and Nathanael Chambers. 2018. Hierarchi- cal quantized representations for script generation. In Proceedings of the 2018 Conference on Empiri- cal Methods in Natural Language Processing, pages 3783-3792.\n\nTransformers: State-of-theart natural language processing. Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony Moi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsThomas Wolf, Julien Chaumond, Lysandre Debut, Vic- tor Sanh, Clement Delangue, Anthony Moi, Pier- ric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, et al. 2020. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Nat- ural Language Processing: System Demonstrations, pages 38-45.\n\nDialog generation using multi-turn reasoning neural networks. Xianchao Wu, Ander Martinez, Momo Klyen, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Long PapersXianchao Wu, Ander Martinez, and Momo Klyen. 2018. Dialog generation using multi-turn reasoning neural networks. In Proceedings of the 2018 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2049-2059.\n\nA fast proximal point method for computing exact wasserstein distance. Yujia Xie, Xiangfeng Wang, Ruijia Wang, Hongyuan Zha, PMLRUncertainty in Artificial Intelligence. Yujia Xie, Xiangfeng Wang, Ruijia Wang, and Hongyuan Zha. 2020. A fast proximal point method for computing exact wasserstein distance. In Un- certainty in Artificial Intelligence, pages 433-453. PMLR.\n\nDistilled wasserstein learning for word embedding and topic modeling. Hongteng Xu, Wenlin Wang, Wei Liu, Lawrence Carin, Proceedings of the 32nd International Conference on Neural Information Processing Systems. the 32nd International Conference on Neural Information Processing SystemsHongteng Xu, Wenlin Wang, Wei Liu, and Lawrence Carin. 2018. Distilled wasserstein learning for word embedding and topic modeling. In Proceedings of the 32nd International Conference on Neural Infor- mation Processing Systems, pages 1723-1732.\n\nVocabulary learning via optimal transport for neural machine translation. Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng, Lei Li, Proceedings of ACL 2021. ACL 2021Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng, and Lei Li. 2021. Vocabulary learning via optimal transport for neural machine translation. In Proceed- ings of ACL 2021.\n\nTemporal event knowledge acquisition via identifying narratives. Wenlin Yao, Ruihong Huang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers)Wenlin Yao and Ruihong Huang. 2018. Temporal event knowledge acquisition via identifying narra- tives. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 537-547.\n\ngoing on a vacation\" takes longer than \"going for a walk\": A study of temporal commonsense understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019. \"going on a vacation\" takes longer than \"going for a walk\": A study of temporal com- monsense understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3363-3369.\n\nMulti-task self-supervised learning for script event prediction. Bo Zhou, Yubo Chen, Kang Liu, Jun Zhao, Jiexin Xu, Xiaojian Jiang, Jinlong Li, Proceedings of the 30th ACM International Conference on Information & Knowledge Management. the 30th ACM International Conference on Information & Knowledge ManagementBo Zhou, Yubo Chen, Kang Liu, Jun Zhao, Jiexin Xu, Xiaojian Jiang, and Jinlong Li. 2021. Multi-task self-supervised learning for script event prediction. In Proceedings of the 30th ACM International Con- ference on Information & Knowledge Management, pages 3662-3666.\n", "annotations": {"author": "[{\"end\":260,\"start\":101},{\"end\":424,\"start\":261},{\"end\":627,\"start\":425},{\"end\":786,\"start\":628},{\"end\":841,\"start\":787},{\"end\":907,\"start\":842},{\"end\":941,\"start\":908}]", "publisher": null, "author_last_name": "[{\"end\":108,\"start\":104},{\"end\":270,\"start\":266},{\"end\":433,\"start\":430},{\"end\":636,\"start\":632},{\"end\":796,\"start\":794},{\"end\":856,\"start\":851},{\"end\":917,\"start\":915}]", "author_first_name": "[{\"end\":103,\"start\":101},{\"end\":265,\"start\":261},{\"end\":429,\"start\":425},{\"end\":631,\"start\":628},{\"end\":793,\"start\":787},{\"end\":850,\"start\":842},{\"end\":914,\"start\":908}]", "author_affiliation": "[{\"end\":208,\"start\":132},{\"end\":259,\"start\":210},{\"end\":372,\"start\":296},{\"end\":423,\"start\":374},{\"end\":530,\"start\":454},{\"end\":581,\"start\":532},{\"end\":626,\"start\":583},{\"end\":734,\"start\":658},{\"end\":785,\"start\":736},{\"end\":840,\"start\":819},{\"end\":906,\"start\":885},{\"end\":940,\"start\":919}]", "title": "[{\"end\":74,\"start\":1},{\"end\":1015,\"start\":942}]", "venue": "[{\"end\":1093,\"start\":1017}]", "abstract": "[{\"end\":2585,\"start\":1188}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2763,\"start\":2745},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2824,\"start\":2805},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7319,\"start\":7291},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7417,\"start\":7399},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7639,\"start\":7606},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7768,\"start\":7750},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7870,\"start\":7854},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8044,\"start\":8028},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8404,\"start\":8386},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8781,\"start\":8761},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9577,\"start\":9561},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":10063,\"start\":10047},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11259,\"start\":11239},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11474,\"start\":11445},{\"end\":11629,\"start\":11625},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":13305,\"start\":13282},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":13397,\"start\":13383},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13425,\"start\":13407},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15773,\"start\":15753},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15790,\"start\":15773},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17339,\"start\":17319},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17391,\"start\":17370},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17871,\"start\":17844},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17902,\"start\":17883},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18918,\"start\":18899},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19703,\"start\":19683},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20065,\"start\":20046},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20377,\"start\":20359}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27981,\"start\":27806},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28082,\"start\":27982},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28431,\"start\":28083},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":28640,\"start\":28432},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":28719,\"start\":28641},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":29227,\"start\":28720}]", "paragraph": "[{\"end\":3155,\"start\":2601},{\"end\":4412,\"start\":3157},{\"end\":4659,\"start\":4414},{\"end\":5220,\"start\":4661},{\"end\":5959,\"start\":5222},{\"end\":6463,\"start\":5961},{\"end\":6524,\"start\":6465},{\"end\":6758,\"start\":6526},{\"end\":6881,\"start\":6760},{\"end\":7103,\"start\":6883},{\"end\":8656,\"start\":7120},{\"end\":8838,\"start\":8658},{\"end\":10732,\"start\":9186},{\"end\":11110,\"start\":10748},{\"end\":11547,\"start\":11131},{\"end\":12108,\"start\":11549},{\"end\":12343,\"start\":12136},{\"end\":12582,\"start\":12478},{\"end\":12676,\"start\":12628},{\"end\":12927,\"start\":12778},{\"end\":13221,\"start\":13053},{\"end\":13568,\"start\":13223},{\"end\":14401,\"start\":13570},{\"end\":14652,\"start\":14607},{\"end\":15269,\"start\":14672},{\"end\":15568,\"start\":15299},{\"end\":15611,\"start\":15575},{\"end\":15629,\"start\":15618},{\"end\":16150,\"start\":15631},{\"end\":16606,\"start\":16152},{\"end\":16853,\"start\":16630},{\"end\":16964,\"start\":16896},{\"end\":17170,\"start\":16979},{\"end\":17787,\"start\":17206},{\"end\":18396,\"start\":17789},{\"end\":19219,\"start\":18398},{\"end\":19457,\"start\":19221},{\"end\":19630,\"start\":19459},{\"end\":19809,\"start\":19632},{\"end\":19965,\"start\":19811},{\"end\":20036,\"start\":19967},{\"end\":20212,\"start\":20038},{\"end\":20339,\"start\":20214},{\"end\":20491,\"start\":20341},{\"end\":21238,\"start\":20530},{\"end\":21577,\"start\":21257},{\"end\":23279,\"start\":21579},{\"end\":23710,\"start\":23346},{\"end\":24304,\"start\":23712},{\"end\":25072,\"start\":24306},{\"end\":26014,\"start\":25121},{\"end\":26323,\"start\":26016},{\"end\":26694,\"start\":26325},{\"end\":27060,\"start\":26696},{\"end\":27805,\"start\":27075}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12477,\"start\":12344},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12627,\"start\":12583},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12777,\"start\":12677},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13052,\"start\":12928},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14606,\"start\":14402},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14671,\"start\":14653},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16629,\"start\":16607},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16895,\"start\":16854},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16978,\"start\":16965}]", "table_ref": "[{\"end\":20596,\"start\":20589},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22573,\"start\":22566},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":24336,\"start\":24329},{\"end\":25723,\"start\":25716}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2599,\"start\":2587},{\"attributes\":{\"n\":\"2\"},\"end\":7118,\"start\":7106},{\"end\":9184,\"start\":8841},{\"attributes\":{\"n\":\"3\"},\"end\":10746,\"start\":10735},{\"attributes\":{\"n\":\"3.1\"},\"end\":11129,\"start\":11113},{\"attributes\":{\"n\":\"3.2\"},\"end\":12134,\"start\":12111},{\"attributes\":{\"n\":\"3.3\"},\"end\":15297,\"start\":15272},{\"end\":15573,\"start\":15571},{\"end\":15616,\"start\":15614},{\"attributes\":{\"n\":\"4\"},\"end\":17183,\"start\":17173},{\"attributes\":{\"n\":\"4.1\"},\"end\":17204,\"start\":17186},{\"attributes\":{\"n\":\"4.2\"},\"end\":20528,\"start\":20494},{\"end\":21255,\"start\":21241},{\"end\":23315,\"start\":23282},{\"attributes\":{\"n\":\"4.3\"},\"end\":23344,\"start\":23318},{\"attributes\":{\"n\":\"4.4\"},\"end\":25119,\"start\":25075},{\"attributes\":{\"n\":\"5\"},\"end\":27073,\"start\":27063},{\"end\":27993,\"start\":27983},{\"end\":28104,\"start\":28084},{\"end\":28442,\"start\":28433},{\"end\":28651,\"start\":28642}]", "table": "[{\"end\":28640,\"start\":28444}]", "figure_caption": "[{\"end\":27981,\"start\":27808},{\"end\":28082,\"start\":27995},{\"end\":28431,\"start\":28107},{\"end\":28719,\"start\":28653},{\"end\":29227,\"start\":28722}]", "figure_ref": "[{\"end\":3043,\"start\":3035},{\"end\":3244,\"start\":3236},{\"end\":3821,\"start\":3813},{\"end\":4538,\"start\":4530},{\"end\":5442,\"start\":5434},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10809,\"start\":10801},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11959,\"start\":11951},{\"end\":16165,\"start\":16157},{\"end\":24711,\"start\":24703},{\"end\":24738,\"start\":24730},{\"end\":25962,\"start\":25954}]", "bib_author_first_name": "[{\"end\":29763,\"start\":29758},{\"end\":29784,\"start\":29779},{\"end\":30215,\"start\":30209},{\"end\":30233,\"start\":30226},{\"end\":30248,\"start\":30244},{\"end\":30539,\"start\":30530},{\"end\":30553,\"start\":30550},{\"end\":30832,\"start\":30827},{\"end\":30845,\"start\":30839},{\"end\":30860,\"start\":30852},{\"end\":30873,\"start\":30866},{\"end\":30886,\"start\":30880},{\"end\":30901,\"start\":30894},{\"end\":30915,\"start\":30909},{\"end\":30927,\"start\":30922},{\"end\":30943,\"start\":30935},{\"end\":31633,\"start\":31628},{\"end\":31846,\"start\":31842},{\"end\":31856,\"start\":31853},{\"end\":31868,\"start\":31864},{\"end\":31880,\"start\":31874},{\"end\":32401,\"start\":32397},{\"end\":32415,\"start\":32411},{\"end\":32426,\"start\":32422},{\"end\":32442,\"start\":32436},{\"end\":32459,\"start\":32452},{\"end\":32469,\"start\":32468},{\"end\":32485,\"start\":32478},{\"end\":32498,\"start\":32491},{\"end\":32511,\"start\":32507},{\"end\":33022,\"start\":33018},{\"end\":33034,\"start\":33033},{\"end\":33051,\"start\":33044},{\"end\":33455,\"start\":33450},{\"end\":33467,\"start\":33461},{\"end\":33475,\"start\":33473},{\"end\":33486,\"start\":33482},{\"end\":33502,\"start\":33497},{\"end\":33521,\"start\":33515},{\"end\":34023,\"start\":34019},{\"end\":34036,\"start\":34030},{\"end\":34050,\"start\":34046},{\"end\":34063,\"start\":34058},{\"end\":34072,\"start\":34064},{\"end\":34971,\"start\":34967},{\"end\":34982,\"start\":34980},{\"end\":34996,\"start\":34988},{\"end\":35011,\"start\":35005},{\"end\":35378,\"start\":35374},{\"end\":35392,\"start\":35386},{\"end\":35403,\"start\":35398},{\"end\":35437,\"start\":35433},{\"end\":35451,\"start\":35444},{\"end\":35466,\"start\":35462},{\"end\":36082,\"start\":36074},{\"end\":36095,\"start\":36087},{\"end\":36106,\"start\":36100},{\"end\":36116,\"start\":36113},{\"end\":36128,\"start\":36121},{\"end\":36139,\"start\":36134},{\"end\":36151,\"start\":36146},{\"end\":36167,\"start\":36159},{\"end\":36178,\"start\":36173},{\"end\":36192,\"start\":36186},{\"end\":36794,\"start\":36787},{\"end\":36806,\"start\":36799},{\"end\":36813,\"start\":36811},{\"end\":36825,\"start\":36818},{\"end\":36834,\"start\":36830},{\"end\":36842,\"start\":36840},{\"end\":36857,\"start\":36849},{\"end\":37403,\"start\":37394},{\"end\":37412,\"start\":37408},{\"end\":37423,\"start\":37419},{\"end\":37879,\"start\":37870},{\"end\":37899,\"start\":37895},{\"end\":38761,\"start\":38753},{\"end\":38772,\"start\":38766},{\"end\":38786,\"start\":38779},{\"end\":38801,\"start\":38794},{\"end\":38814,\"start\":38807},{\"end\":39288,\"start\":39282},{\"end\":39309,\"start\":39303},{\"end\":39329,\"start\":39320},{\"end\":39345,\"start\":39340},{\"end\":39357,\"start\":39353},{\"end\":39782,\"start\":39777},{\"end\":39792,\"start\":39788},{\"end\":39806,\"start\":39802},{\"end\":40338,\"start\":40331},{\"end\":40349,\"start\":40344},{\"end\":40364,\"start\":40359},{\"end\":40378,\"start\":40371},{\"end\":40396,\"start\":40392},{\"end\":40409,\"start\":40404},{\"end\":40423,\"start\":40416},{\"end\":40438,\"start\":40434},{\"end\":40772,\"start\":40768},{\"end\":40789,\"start\":40782},{\"end\":40967,\"start\":40963},{\"end\":40984,\"start\":40977},{\"end\":40994,\"start\":40989},{\"end\":41007,\"start\":41002},{\"end\":41019,\"start\":41014},{\"end\":41032,\"start\":41028},{\"end\":41257,\"start\":41250},{\"end\":41272,\"start\":41264},{\"end\":41285,\"start\":41278},{\"end\":41299,\"start\":41292},{\"end\":41687,\"start\":41678},{\"end\":41697,\"start\":41694},{\"end\":41710,\"start\":41705},{\"end\":41714,\"start\":41711},{\"end\":42188,\"start\":42184},{\"end\":42201,\"start\":42196},{\"end\":42743,\"start\":42737},{\"end\":42756,\"start\":42750},{\"end\":42775,\"start\":42767},{\"end\":42789,\"start\":42783},{\"end\":42803,\"start\":42796},{\"end\":42821,\"start\":42814},{\"end\":42834,\"start\":42827},{\"end\":42849,\"start\":42843},{\"end\":42864,\"start\":42861},{\"end\":42877,\"start\":42874},{\"end\":43522,\"start\":43514},{\"end\":43532,\"start\":43527},{\"end\":43547,\"start\":43543},{\"end\":44222,\"start\":44217},{\"end\":44237,\"start\":44228},{\"end\":44250,\"start\":44244},{\"end\":44265,\"start\":44257},{\"end\":44595,\"start\":44587},{\"end\":44606,\"start\":44600},{\"end\":44616,\"start\":44613},{\"end\":44630,\"start\":44622},{\"end\":45130,\"start\":45122},{\"end\":45138,\"start\":45135},{\"end\":45149,\"start\":45145},{\"end\":45163,\"start\":45155},{\"end\":45174,\"start\":45171},{\"end\":45455,\"start\":45449},{\"end\":45468,\"start\":45461},{\"end\":45995,\"start\":45992},{\"end\":46008,\"start\":46002},{\"end\":46024,\"start\":46019},{\"end\":46034,\"start\":46031},{\"end\":46813,\"start\":46811},{\"end\":46824,\"start\":46820},{\"end\":46835,\"start\":46831},{\"end\":46844,\"start\":46841},{\"end\":46857,\"start\":46851},{\"end\":46870,\"start\":46862},{\"end\":46885,\"start\":46878}]", "bib_author_last_name": "[{\"end\":29777,\"start\":29764},{\"end\":29793,\"start\":29785},{\"end\":30224,\"start\":30216},{\"end\":30242,\"start\":30234},{\"end\":30255,\"start\":30249},{\"end\":30548,\"start\":30540},{\"end\":30562,\"start\":30554},{\"end\":30837,\"start\":30833},{\"end\":30850,\"start\":30846},{\"end\":30864,\"start\":30861},{\"end\":30878,\"start\":30874},{\"end\":30892,\"start\":30887},{\"end\":30907,\"start\":30902},{\"end\":30920,\"start\":30916},{\"end\":30933,\"start\":30928},{\"end\":30949,\"start\":30944},{\"end\":31640,\"start\":31634},{\"end\":31851,\"start\":31847},{\"end\":31862,\"start\":31857},{\"end\":31872,\"start\":31869},{\"end\":31885,\"start\":31881},{\"end\":32409,\"start\":32402},{\"end\":32420,\"start\":32416},{\"end\":32434,\"start\":32427},{\"end\":32450,\"start\":32443},{\"end\":32466,\"start\":32460},{\"end\":32476,\"start\":32470},{\"end\":32489,\"start\":32486},{\"end\":32505,\"start\":32499},{\"end\":32519,\"start\":32512},{\"end\":32532,\"start\":32521},{\"end\":33031,\"start\":33023},{\"end\":33042,\"start\":33035},{\"end\":33057,\"start\":33052},{\"end\":33459,\"start\":33456},{\"end\":33471,\"start\":33468},{\"end\":33480,\"start\":33476},{\"end\":33495,\"start\":33487},{\"end\":33513,\"start\":33503},{\"end\":33526,\"start\":33522},{\"end\":34028,\"start\":34024},{\"end\":34044,\"start\":34037},{\"end\":34056,\"start\":34051},{\"end\":34078,\"start\":34073},{\"end\":34978,\"start\":34972},{\"end\":34986,\"start\":34983},{\"end\":35003,\"start\":34997},{\"end\":35022,\"start\":35012},{\"end\":35384,\"start\":35379},{\"end\":35396,\"start\":35393},{\"end\":35431,\"start\":35404},{\"end\":35442,\"start\":35438},{\"end\":35460,\"start\":35452},{\"end\":35478,\"start\":35467},{\"end\":36085,\"start\":36083},{\"end\":36098,\"start\":36096},{\"end\":36111,\"start\":36107},{\"end\":36119,\"start\":36117},{\"end\":36132,\"start\":36129},{\"end\":36144,\"start\":36140},{\"end\":36157,\"start\":36152},{\"end\":36171,\"start\":36168},{\"end\":36184,\"start\":36179},{\"end\":36197,\"start\":36193},{\"end\":36797,\"start\":36795},{\"end\":36809,\"start\":36807},{\"end\":36816,\"start\":36814},{\"end\":36828,\"start\":36826},{\"end\":36838,\"start\":36835},{\"end\":36847,\"start\":36843},{\"end\":36865,\"start\":36858},{\"end\":37406,\"start\":37404},{\"end\":37417,\"start\":37413},{\"end\":37427,\"start\":37424},{\"end\":37893,\"start\":37880},{\"end\":37908,\"start\":37900},{\"end\":37917,\"start\":37910},{\"end\":38764,\"start\":38762},{\"end\":38777,\"start\":38773},{\"end\":38792,\"start\":38787},{\"end\":38805,\"start\":38802},{\"end\":38817,\"start\":38815},{\"end\":39301,\"start\":39289},{\"end\":39318,\"start\":39310},{\"end\":39338,\"start\":39330},{\"end\":39351,\"start\":39346},{\"end\":39369,\"start\":39358},{\"end\":39786,\"start\":39783},{\"end\":39800,\"start\":39793},{\"end\":39814,\"start\":39807},{\"end\":40342,\"start\":40339},{\"end\":40357,\"start\":40350},{\"end\":40369,\"start\":40365},{\"end\":40390,\"start\":40379},{\"end\":40402,\"start\":40397},{\"end\":40414,\"start\":40410},{\"end\":40432,\"start\":40424},{\"end\":40444,\"start\":40439},{\"end\":40780,\"start\":40773},{\"end\":40800,\"start\":40790},{\"end\":40975,\"start\":40968},{\"end\":40987,\"start\":40985},{\"end\":41000,\"start\":40995},{\"end\":41012,\"start\":41008},{\"end\":41026,\"start\":41020},{\"end\":41042,\"start\":41033},{\"end\":41262,\"start\":41258},{\"end\":41276,\"start\":41273},{\"end\":41290,\"start\":41286},{\"end\":41304,\"start\":41300},{\"end\":41692,\"start\":41688},{\"end\":41703,\"start\":41698},{\"end\":41720,\"start\":41715},{\"end\":42194,\"start\":42189},{\"end\":42209,\"start\":42202},{\"end\":42748,\"start\":42744},{\"end\":42765,\"start\":42757},{\"end\":42781,\"start\":42776},{\"end\":42794,\"start\":42790},{\"end\":42812,\"start\":42804},{\"end\":42825,\"start\":42822},{\"end\":42841,\"start\":42835},{\"end\":42859,\"start\":42850},{\"end\":42872,\"start\":42865},{\"end\":42886,\"start\":42878},{\"end\":43525,\"start\":43523},{\"end\":43541,\"start\":43533},{\"end\":43553,\"start\":43548},{\"end\":44226,\"start\":44223},{\"end\":44242,\"start\":44238},{\"end\":44255,\"start\":44251},{\"end\":44269,\"start\":44266},{\"end\":44598,\"start\":44596},{\"end\":44611,\"start\":44607},{\"end\":44620,\"start\":44617},{\"end\":44636,\"start\":44631},{\"end\":45133,\"start\":45131},{\"end\":45143,\"start\":45139},{\"end\":45153,\"start\":45150},{\"end\":45169,\"start\":45164},{\"end\":45177,\"start\":45175},{\"end\":45459,\"start\":45456},{\"end\":45474,\"start\":45469},{\"end\":46000,\"start\":45996},{\"end\":46017,\"start\":46009},{\"end\":46029,\"start\":46025},{\"end\":46039,\"start\":46035},{\"end\":46818,\"start\":46814},{\"end\":46829,\"start\":46825},{\"end\":46839,\"start\":46836},{\"end\":46849,\"start\":46845},{\"end\":46860,\"start\":46858},{\"end\":46876,\"start\":46871},{\"end\":46888,\"start\":46886}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52156206},\"end\":30162,\"start\":29703},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b1\",\"matched_paper_id\":2057420},\"end\":30479,\"start\":30164},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":529375},\"end\":30744,\"start\":30481},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":174799551},\"end\":31510,\"start\":30746},{\"attributes\":{\"id\":\"b4\"},\"end\":31785,\"start\":31512},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":10638526},\"end\":32331,\"start\":31787},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3994096},\"end\":32936,\"start\":32333},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":35860340},\"end\":33377,\"start\":32938},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":202718810},\"end\":33952,\"start\":33379},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5816453},\"end\":34545,\"start\":33954},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52967399},\"end\":34921,\"start\":34547},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b11\",\"matched_paper_id\":14674248},\"end\":35258,\"start\":34923},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":204960716},\"end\":36006,\"start\":35260},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":222310161},\"end\":36695,\"start\":36008},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":243865670},\"end\":37315,\"start\":36697},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":21723549},\"end\":37806,\"start\":37317},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":229924289},\"end\":38661,\"start\":37808},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":57983897},\"end\":39191,\"start\":38663},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8387007},\"end\":39697,\"start\":39193},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":196181734},\"end\":40216,\"start\":39699},{\"attributes\":{\"id\":\"b20\"},\"end\":40706,\"start\":40218},{\"attributes\":{\"id\":\"b21\"},\"end\":40908,\"start\":40708},{\"attributes\":{\"id\":\"b22\"},\"end\":41187,\"start\":40910},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":244119075},\"end\":41598,\"start\":41189},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":38934160},\"end\":42120,\"start\":41600},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":52124023},\"end\":42676,\"start\":42122},{\"attributes\":{\"id\":\"b26\"},\"end\":43450,\"start\":42678},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":44113253},\"end\":44144,\"start\":43452},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b28\",\"matched_paper_id\":195849235},\"end\":44515,\"start\":44146},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":52274975},\"end\":45046,\"start\":44517},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":235377446},\"end\":45382,\"start\":45048},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":43987349},\"end\":45884,\"start\":45384},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":202541184},\"end\":46744,\"start\":45886},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":240230725},\"end\":47324,\"start\":46746}]", "bib_title": "[{\"end\":29756,\"start\":29703},{\"end\":30207,\"start\":30164},{\"end\":30528,\"start\":30481},{\"end\":30825,\"start\":30746},{\"end\":31840,\"start\":31787},{\"end\":32395,\"start\":32333},{\"end\":33016,\"start\":32938},{\"end\":33448,\"start\":33379},{\"end\":34017,\"start\":33954},{\"end\":34627,\"start\":34547},{\"end\":34965,\"start\":34923},{\"end\":35372,\"start\":35260},{\"end\":36072,\"start\":36008},{\"end\":36785,\"start\":36697},{\"end\":37392,\"start\":37317},{\"end\":37868,\"start\":37808},{\"end\":38751,\"start\":38663},{\"end\":39280,\"start\":39193},{\"end\":39775,\"start\":39699},{\"end\":41248,\"start\":41189},{\"end\":41676,\"start\":41600},{\"end\":42182,\"start\":42122},{\"end\":42735,\"start\":42678},{\"end\":43512,\"start\":43452},{\"end\":44215,\"start\":44146},{\"end\":44585,\"start\":44517},{\"end\":45120,\"start\":45048},{\"end\":45447,\"start\":45384},{\"end\":45990,\"start\":45886},{\"end\":46809,\"start\":46746}]", "bib_author": "[{\"end\":29779,\"start\":29758},{\"end\":29795,\"start\":29779},{\"end\":30226,\"start\":30209},{\"end\":30244,\"start\":30226},{\"end\":30257,\"start\":30244},{\"end\":30550,\"start\":30530},{\"end\":30564,\"start\":30550},{\"end\":30839,\"start\":30827},{\"end\":30852,\"start\":30839},{\"end\":30866,\"start\":30852},{\"end\":30880,\"start\":30866},{\"end\":30894,\"start\":30880},{\"end\":30909,\"start\":30894},{\"end\":30922,\"start\":30909},{\"end\":30935,\"start\":30922},{\"end\":30951,\"start\":30935},{\"end\":31642,\"start\":31628},{\"end\":31853,\"start\":31842},{\"end\":31864,\"start\":31853},{\"end\":31874,\"start\":31864},{\"end\":31887,\"start\":31874},{\"end\":32411,\"start\":32397},{\"end\":32422,\"start\":32411},{\"end\":32436,\"start\":32422},{\"end\":32452,\"start\":32436},{\"end\":32468,\"start\":32452},{\"end\":32478,\"start\":32468},{\"end\":32491,\"start\":32478},{\"end\":32507,\"start\":32491},{\"end\":32521,\"start\":32507},{\"end\":32534,\"start\":32521},{\"end\":33033,\"start\":33018},{\"end\":33044,\"start\":33033},{\"end\":33059,\"start\":33044},{\"end\":33461,\"start\":33450},{\"end\":33473,\"start\":33461},{\"end\":33482,\"start\":33473},{\"end\":33497,\"start\":33482},{\"end\":33515,\"start\":33497},{\"end\":33528,\"start\":33515},{\"end\":34030,\"start\":34019},{\"end\":34046,\"start\":34030},{\"end\":34058,\"start\":34046},{\"end\":34080,\"start\":34058},{\"end\":34980,\"start\":34967},{\"end\":34988,\"start\":34980},{\"end\":35005,\"start\":34988},{\"end\":35024,\"start\":35005},{\"end\":35386,\"start\":35374},{\"end\":35398,\"start\":35386},{\"end\":35433,\"start\":35398},{\"end\":35444,\"start\":35433},{\"end\":35462,\"start\":35444},{\"end\":35480,\"start\":35462},{\"end\":36087,\"start\":36074},{\"end\":36100,\"start\":36087},{\"end\":36113,\"start\":36100},{\"end\":36121,\"start\":36113},{\"end\":36134,\"start\":36121},{\"end\":36146,\"start\":36134},{\"end\":36159,\"start\":36146},{\"end\":36173,\"start\":36159},{\"end\":36186,\"start\":36173},{\"end\":36199,\"start\":36186},{\"end\":36799,\"start\":36787},{\"end\":36811,\"start\":36799},{\"end\":36818,\"start\":36811},{\"end\":36830,\"start\":36818},{\"end\":36840,\"start\":36830},{\"end\":36849,\"start\":36840},{\"end\":36867,\"start\":36849},{\"end\":37408,\"start\":37394},{\"end\":37419,\"start\":37408},{\"end\":37429,\"start\":37419},{\"end\":37895,\"start\":37870},{\"end\":37910,\"start\":37895},{\"end\":37919,\"start\":37910},{\"end\":38766,\"start\":38753},{\"end\":38779,\"start\":38766},{\"end\":38794,\"start\":38779},{\"end\":38807,\"start\":38794},{\"end\":38819,\"start\":38807},{\"end\":39303,\"start\":39282},{\"end\":39320,\"start\":39303},{\"end\":39340,\"start\":39320},{\"end\":39353,\"start\":39340},{\"end\":39371,\"start\":39353},{\"end\":39788,\"start\":39777},{\"end\":39802,\"start\":39788},{\"end\":39816,\"start\":39802},{\"end\":40344,\"start\":40331},{\"end\":40359,\"start\":40344},{\"end\":40371,\"start\":40359},{\"end\":40392,\"start\":40371},{\"end\":40404,\"start\":40392},{\"end\":40416,\"start\":40404},{\"end\":40434,\"start\":40416},{\"end\":40446,\"start\":40434},{\"end\":40782,\"start\":40768},{\"end\":40802,\"start\":40782},{\"end\":40977,\"start\":40963},{\"end\":40989,\"start\":40977},{\"end\":41002,\"start\":40989},{\"end\":41014,\"start\":41002},{\"end\":41028,\"start\":41014},{\"end\":41044,\"start\":41028},{\"end\":41264,\"start\":41250},{\"end\":41278,\"start\":41264},{\"end\":41292,\"start\":41278},{\"end\":41306,\"start\":41292},{\"end\":41694,\"start\":41678},{\"end\":41705,\"start\":41694},{\"end\":41722,\"start\":41705},{\"end\":42196,\"start\":42184},{\"end\":42211,\"start\":42196},{\"end\":42750,\"start\":42737},{\"end\":42767,\"start\":42750},{\"end\":42783,\"start\":42767},{\"end\":42796,\"start\":42783},{\"end\":42814,\"start\":42796},{\"end\":42827,\"start\":42814},{\"end\":42843,\"start\":42827},{\"end\":42861,\"start\":42843},{\"end\":42874,\"start\":42861},{\"end\":42888,\"start\":42874},{\"end\":43527,\"start\":43514},{\"end\":43543,\"start\":43527},{\"end\":43555,\"start\":43543},{\"end\":44228,\"start\":44217},{\"end\":44244,\"start\":44228},{\"end\":44257,\"start\":44244},{\"end\":44271,\"start\":44257},{\"end\":44600,\"start\":44587},{\"end\":44613,\"start\":44600},{\"end\":44622,\"start\":44613},{\"end\":44638,\"start\":44622},{\"end\":45135,\"start\":45122},{\"end\":45145,\"start\":45135},{\"end\":45155,\"start\":45145},{\"end\":45171,\"start\":45155},{\"end\":45179,\"start\":45171},{\"end\":45461,\"start\":45449},{\"end\":45476,\"start\":45461},{\"end\":46002,\"start\":45992},{\"end\":46019,\"start\":46002},{\"end\":46031,\"start\":46019},{\"end\":46041,\"start\":46031},{\"end\":46820,\"start\":46811},{\"end\":46831,\"start\":46820},{\"end\":46841,\"start\":46831},{\"end\":46851,\"start\":46841},{\"end\":46862,\"start\":46851},{\"end\":46878,\"start\":46862},{\"end\":46890,\"start\":46878}]", "bib_venue": "[{\"end\":29881,\"start\":29795},{\"end\":30305,\"start\":30261},{\"end\":30590,\"start\":30564},{\"end\":31059,\"start\":30951},{\"end\":31626,\"start\":31512},{\"end\":31995,\"start\":31887},{\"end\":32596,\"start\":32534},{\"end\":33120,\"start\":33059},{\"end\":33613,\"start\":33528},{\"end\":34187,\"start\":34080},{\"end\":34653,\"start\":34629},{\"end\":35072,\"start\":35028},{\"end\":35567,\"start\":35480},{\"end\":36293,\"start\":36199},{\"end\":36953,\"start\":36867},{\"end\":37510,\"start\":37429},{\"end\":38081,\"start\":37919},{\"end\":38880,\"start\":38819},{\"end\":39415,\"start\":39371},{\"end\":39903,\"start\":39816},{\"end\":40329,\"start\":40218},{\"end\":40766,\"start\":40708},{\"end\":40961,\"start\":40910},{\"end\":41375,\"start\":41306},{\"end\":41808,\"start\":41722},{\"end\":42297,\"start\":42211},{\"end\":42997,\"start\":42888},{\"end\":43697,\"start\":43555},{\"end\":44313,\"start\":44275},{\"end\":44727,\"start\":44638},{\"end\":45202,\"start\":45179},{\"end\":45563,\"start\":45476},{\"end\":46216,\"start\":46041},{\"end\":46980,\"start\":46890},{\"end\":29954,\"start\":29883},{\"end\":30603,\"start\":30592},{\"end\":32090,\"start\":31997},{\"end\":32645,\"start\":32598},{\"end\":33168,\"start\":33122},{\"end\":33685,\"start\":33615},{\"end\":34281,\"start\":34189},{\"end\":34725,\"start\":34716},{\"end\":35641,\"start\":35569},{\"end\":36374,\"start\":36295},{\"end\":37026,\"start\":36955},{\"end\":37578,\"start\":37512},{\"end\":38230,\"start\":38083},{\"end\":38928,\"start\":38882},{\"end\":39446,\"start\":39417},{\"end\":39977,\"start\":39905},{\"end\":41881,\"start\":41810},{\"end\":42370,\"start\":42299},{\"end\":43093,\"start\":42999},{\"end\":43826,\"start\":43699},{\"end\":44803,\"start\":44729},{\"end\":45212,\"start\":45204},{\"end\":45637,\"start\":45565},{\"end\":46378,\"start\":46218},{\"end\":47057,\"start\":46982}]"}}}, "year": 2023, "month": 12, "day": 17}