{"id": 256808576, "updated": "2023-10-05 04:18:57.349", "metadata": {"title": "LCDnet: A Lightweight Crowd Density Estimation Model for Real-time Video Surveillance", "authors": "[{\"first\":\"Muhammad\",\"last\":\"Khan\",\"middle\":[\"Asif\"]},{\"first\":\"Hamid\",\"last\":\"Menouar\",\"middle\":[]},{\"first\":\"Ridha\",\"last\":\"Hamila\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Automatic crowd counting using density estimation has gained significant attention in computer vision research. As a result, a large number of crowd counting and density estimation models using convolution neural networks (CNN) have been published in the last few years. These models have achieved good accuracy over benchmark datasets. However, attempts to improve the accuracy often lead to higher complexity in these models. In real-time video surveillance applications using drones with limited computing resources, deep models incur intolerable higher inference delay. In this paper, we propose (i) a Lightweight Crowd Density estimation model (LCDnet) for real-time video surveillance, and (ii) an improved training method using curriculum learning (CL). LCDnet is trained using CL and evaluated over two benchmark datasets i.e., DroneRGBT and CARPK. Results are compared with existing crowd models. Our evaluation shows that the LCDnet achieves a reasonably good accuracy while significantly reducing the inference time and memory requirement and thus can be deployed over edge devices with very limited computing resources.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2302.05374", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/jrtip/KhanMH23", "doi": "10.1007/s11554-023-01286-8"}}, "content": {"source": {"pdf_hash": "360f3defda47bf2d5f6edd10eb1193074a70d8c2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.05374v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d2fa56c33e222441565706f85e6d3447328982bb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/360f3defda47bf2d5f6edd10eb1193074a70d8c2.txt", "contents": "\nLCDnet: A Lightweight Crowd Density Estimation Model for Real-time Video Surveillance\n10 Feb 2023\n\nMuhammad Asif Khan mkhan@qu.edu.qa \nQatar Mobility Innovations Center (QMIC)\nQatar University\n\nHamid Menouar hamidm@qmic.com \nQatar Mobility Innovations Center (QMIC)\nQatar University\n\nRidha Hamila hamila@qu.edu.qa \nElectrical Engineering\nQatar University\n\nLCDnet: A Lightweight Crowd Density Estimation Model for Real-time Video Surveillance\n10 Feb 2023Crowd countingCNNdensity estimationlightweightreal- time\nAutomatic crowd counting using density estimation has gained significant attention in computer vision research. As a result, a large number of crowd counting and density estimation models using convolution neural networks (CNN) have been published in the last few years. These models have achieved good accuracy over benchmark datasets. However, attempts to improve the accuracy often lead to higher complexity in these models. In real-time video surveillance applications using drones with limited computing resources, deep models incur intolerable higher inference delay. In this paper, we propose (i) a Lightweight Crowd Density estimation model (LCDnet) for real-time video surveillance, and (ii) an improved training method using curriculum learning (CL). LCDnet is trained using CL and evaluated over two benchmark datasets i.e., DroneRGBT and CARPK. Results are compared with existing crowd models. Our evaluation shows that the LCDnet achieves a reasonably good accuracy while significantly reducing the inference time and memory requirement and thus can be deployed over edge devices with very limited computing resources. . . .Crowd counting is an interesting research area that involves computer vision and deep learning to estimate the number of people in an images or video frames.Recently it has gain significant attention in the computer vision community due to the significance of the problem. Crowd counting is generally implemented in two ways: (i) counting objects (input is an image and the output is a number i.e., total head count in the image.), and (ii) density map estimation (input is an image and the output is the density map of the crowd which is then integrated to get the total head count.). Traditional methods for crowd counting were all based on detecting hand-crafted local features such as full body [42,43], body parts [12, 24, 44], shapes [27], or global features such as foreground [11], edge [47], texture [6] and gradient features [10,41] and then use machine learning models such as linear regression, ridge regression, Gaussian process, support vector machines (SVMs), random forest, gradient boost, and neural networks to arXiv:2302.05374v1 [cs.CV]\n\nprovide the total count or a density map of the image. However, all the accuracy of these methods significantly degrades on images with dense crowds due to challenges such as occlusions, low resolution, foreshortening and perspectives.\n\nRecent research on crowd counting shows the efficacy of deep learning methods for crowd counting [23]. Convolution neural networks (CNNs) have a strong capability of auto feature extraction [9,51] and perform very well in learning spatial features. Although even small CNN models [49] outperform traditional counting methods, their accuracy degrade on high density scenes.\n\nTo achieve higher accuracy in dense scenes, deeper models with a large size of parameters are developed [4,26,35,40]. These deep models although achieve good accuracy create performance bottlenecks in real-time applications due to large memory requirement, higher training complexity, and large inference delay. In contrast, small-sized CNN models offer several benefits in real-time video surveillance e.g., they incur low inference delay, require low memory for deployment on embedded devices, quickly update over-the-air, and can be trained, fine-tuned and run in distributed manner [18]. However, lightweight and shallow CNN models are usually disregarded by some due to their limited accuracy. Contrary to that, we believe that leveraging best practices such as carefully designing the model architecture, the use of accurately annotated training data, and efficient learning strategies can jointly improve the accuracy of shallow models to a greater extent [37].\n\n\nModel Design Strategy\n\nGenerally, a good choice of convolution filters play a crucial role in feature learning and contribute to reducing the size of the model. In essence, larger filters (5 \u00d7 5) are more expensive and thus should be replaced by smaller filters (3 \u00d7 3). As an example, a stack of three (3 \u00d7 3) convolution layers is preferred over a single convolution layer of larger receptive fields such as 7 \u00d7 7 or 9 \u00d7 9 because of the non-linear activations between them. It also has less number of parameters (3 \u00d7 3 2 \u00d7 C 2 < 1 \u00d7 7 2 \u00d7 C 2 i.e., 81% less parameters) and are computed faster. Further dimension reduction can be applied by using a \u00d7 1 convolution before expensive convolutions (e.g., 5 \u00d7 5 or 3 \u00d7 3) [38]. Furthermore, spatially separable convolutions are preferred i.e., a 3 \u00d7 3 convolution can be decomposed in two sequential 1 \u00d7 3 and 3 \u00d7 1 convolutions which leads to the same number of parameters and can even achieve better learning.\n\n\nData Annotation Strategy\n\nIn crowd density estimation, the point annotations on top of the heads in crowd image form a sparse \"dot-map\" or \"localization-map\" which is converted into a density map by convolving the head position with a Gaussian kernel. The scale parameter in the Gaussian kernel visually creates a blob around the point (pixel). A good density map is more accurate if the blob size covers the entire head and do not non-overlap with neighboring heads. However, due to the camera perspective effect, the size of heads vary in the same image. Adaptive kernels are used to select different values of the scale parameter which solve this problem to some extent. To cope with the perspective distortion and scale variations in crowd images, recent works propose often very deep models with complex architectures. However, unlike images captured with CCTV camera, in aerial images captured from drones, the perspective distortion is minimum and the scale variation is directly related to the drone altitude. If the drone-altitude is known, it can be used to generate accurate density maps.\n\n\nModel Learning Strategy\n\nThe idea of curriculum learning (CL) in neural network was first presented in [2]. The idea behind CL is the natural learning process in humans and animals i.e., humans learn better when concepts are presented in specific order of complexity i.e., from simpler to difficult tasks. Unlike traditional learning methods, in curriculum learning the training samples are sorted by order of (typically increasing) complexity. CL has been demonstrated as an effective strategy to improve the learning capability and faster convergence in various tasks e.g., computer vision [15,16,21], natural language processing (NLP) [31,39], reinforcement learning [13,29,32] etc. Recent studies [25,45] inspire to adopt CL in our research.\n\nThis work leverages the aforementioned three strategies for generating ground truth density maps and to design and train an extremely lightweight crowd density estimation model. In the first step, we designed the model (LCDnet) by carefully choosing convolution filters of different sizes in different layers. To keep the model more compact, we used less number of filters in the initial layers so even with the larger input feature maps, the computational load is controlled. To further alleviate the computational complexity, we used rectangular filters of size (1 \u00d7 3) and (3 \u00d7 1) instead of square filters. This also improves the learning performance as indicated in [38]. The resultant CNN model is a shallow network with only 0.05 Million parameters. Next, to train the shallow model with dronecaptured aerial images, we generated high quality (accurate) density maps. We tested different scale values of the Gaussian function and empirically found the most accurate values for each image. Lastly, the curriculum learning technique is employed to improve the learning performance of the model.\n\nThe contribution of our work is as follows:\n\n-A lightweight CNN model (LCDnet) with fewer parameters, low memory requirement, and faster run-time than existing models. -Generate density maps from sparse localization maps by considering dronealtitude to create adaptive Gaussian kernels to improve learning. -Propose an efficient strategy based on curriculum-learning approach to further improve model training and convergence. -Experimental demonstration of reasonably good performance using LCDnet on benchmark datasets. The accuracy is comparable to existing models of double-size than LCDnet.\n\n\nRelated Work\n\nA number of crowd counting datasets exists each of which can be divided into three categories. Surveillance-view datasets containing indoor or outdoor images collected by surveillance cameras (e.g., Mall [7], UCSD [5], WorldExpo'10 [49], ShanghaiTech Part B [50], Free-view dataset containing images from different sources including Internet (e.g., UCF-CC-50 [19], UCF-QNRF [20], Shang-haiTech Part A [50]), and Drone view datasets containing images collected using drones (e.g., DroneRGBT [30], CARPK [17]). Most of the earlier research works on crowd counting have been using the datasets in the first two categories. The drone-view datasets have been available recently. While there has been different approaches for crowd counting, density estimation using CNN is the most widely used method. The first known CNN model for density estimation and counting is CrowdCNN [49]. The CrowdCNN model consists of three convolution layers followed by three fully connected layers. Following this, numerous works proposed different CNN-based models for crowd counting.\n\nA multi-column CNN (MCNN) model is proposed in [50] which consists of three CNN columns, each containing filters of receptive fields of different sizes (small, medium large). The outputs of the three columns are combined to predict the final density map. MCNN exhibits good performance to adapt to the scale variations in images due to perspective effects or different image resolutions. A two column CNN model (CrowdNet) is proposed in [3]. The model consists of two CNN columns i.e., a deep network (five CNN layers) and a shallow network (three CNN layers). The outputs of both networks are combined to predict the final density map. A Switched CNN (SCNN) is proposed in [33], which consists of two parts, a Switch and a CNN regressor. The CNN regressor consists of three independent columns each with different size receptive fields. Patches from the input image are first fed to the Switch network, which relay it to one of the CNN regressor to predict the density map. The intuition in SCNN is to build CNN model which can adapt to the large scale variations without increasing the model computational complexity i.e., a patch is passed through only one column in the regressor. This reduces the computational complexity ascompared to other multi column CNN models e.g., MCNN and CrowdNet.\n\nUnlike multi-column CNN models, authors in [48] proposed a single column network called multi scale CNN (MSCNN) to learn the scale variations. MSCNN uses three Inception modules [38] called multi-scale blobs (MSBs). Each MSB consists of multiple filters with different kernel size and is able to extract scalerelevant features. The aforementioned CNN models can adapt to the scale variations introduced in the training data but may fail to generalize well [35]. A cascaded multi-task learning (CMTL) model [35] is proposed to adapt to the wide variations of density levels in images. CMTL is also a two column network. The first column is a high-level prior that classify an input image into groups based on the total count in the image. The features learned by the high level prior are shared with the second column that estimates the respective density map.\n\nThe previous models mostly used multi-column architectures to learn scalerelevant features in crowd images achieves good results. To further improve the counting accuracy in highly congested scenes, authors in [26] propose deeper architecture by utilizing transfer learning. The Congested Scene Recognition (CSRNet) model [26] uses VGG16 [34] (first 10 layers) as the front-end to extract features, and a back-end network with dilated convolution to substitute the pooling layers thus avoiding the loss of spatial information. Transfer learning an improve the feature extraction capability of the crowd counting model and has been recently adopted in CANNet [28], GSP [1], TEDnet [22], Deepcount [8], SASNet [36], M-SFANet [40], and SGANet [46].\n\nThe aforementioned models often produces output density maps of lower resolution than the input image and uses patch-based training. The Trellis Encoder-Decoder Network (TEDnet) is proposed [22] which uses whole image as the model input and preserve the size of the density map to the actual resolution. Another encoder-decoder model with special Inception [38]-like modules is scale aggregation network (SANet) proposed in [4]. Like TEDnet, SANet also produces high resolution density map but has a simpler model structure.\n\nOur proposed model LCDnet simultaneously achieves two benefits as compared to the aforementioned models. First, it is extremely lightweight as compared to other models and can run faster even at edge devices with limited compute resources (a comparison is shown later in the paper). Secondly, it also produces density maps of better quality of size ( 1 2 ) of input size as compared to other methods e.g., CSRNet ( 1 8 ) and MCNN ( 1 4 ). It also requires least amount of memory to fit in on-chip caches.\n\n\nProposed Method\n\nThe aforementioned CNN-based crowd models are designed to improve counting accuracy. However, in a typical crowd monitoring system, the user may be interested in a rough estimation of crowd densities (e.g., low, medium, high, very high etc.) located in a geographical area rather than the exact count. Our goal in this paper is to develop a very lightweight model which can reasonably detect the presence of crowd scenes to an extent useful in analyzing crowds but essentially run faster on edge devices with limited computing resource. To this end, we propose a crowd density estimation model (LCDnet).\n\n\nNetwork Architecture\n\nThe architecture of LCDnet is shown in Fig. 1. It consists of six convolution (Conv) layers. Conv1 consists of 64 (5 \u00d7 5) filters. The output of Conv1 layer is fed to Conv2 and Conv3, each having 32 (3 \u00d7 3) filters. The outputs from Conv2 and Conv3 is fed to Conv4 and Conv5 both has 64 (3 \u00d7 3)filters respectively. The outputs of Conv4 and Conv5 is concatenated and fed to Conv6 which consists of 128 (1 \u00d7 1) filters to predict the final density map. It is worthy to note that LCDnet returns the density map rather than the crowd density class. There are two benefits for this. First, the density map preserves the location information of crowd which can be used to localize the crowd in the real-world. Second, it is easy to determine the total count (whole scene) or local count (specific part of the scene) from the predicted density map and even use the count information to configure user-defined crowd densities based on the application requirement.\n\nIn the proposed LCDnet architecture, the first convolution layer is used to detect features such as edges. These detected features are then used in two columns. Both columns contain three layers i.e., two layers of 32 filters of size 1 \u00d7 3, and 3 \u00d7 1 (in reverse order in column 2), respectively, which is followed by a layer of 64 filters of 3 \u00d7 3 size. The outputs of both columns are concatenated and fed to a 1 \u00d7 1 conv layer of size 128, which generates a density map. The output density map is half size of the input image. \n\n\nGround Truth Generation.\n\nIf x i is a pixel containing the the head position, it can be represented by a delta function \u03b4(x\u2212x i ). The density map is generated by convolving the delta function with a Gaussian kernel G \u03c3 .\nY = N i=1 \u03b4(x \u2212 x i ) * G \u03c3(1)\nwhere, N is the total number of annotated points (i.e., total count of heads) in the image. The integral of density map Y is equal to the total head count in the image. Visually, this operation creates a blurring of each head annotation using the scale parameter \u03c3. There are various kernel settings to generate a variety of density maps. The most basic approach is to keep \u03c3 fixed value, which means that the density map will apply same kernel to all head positions irrespective of their scale in the image [4]. As head sizes in image can vary due to camera prospective, a single value of \u03c3 may not be a good choice. Hence, some recent works propose to use adaptive Gaussian kernels to create density maps. The value of \u03c3 is calculated as the average distance to k-nearest neighboring head annotations. Visually, it generates lower degree of Gaussian blur for dense crowds and higher degree for region of sparse density in crowd scene. Typical settings includes k = 1 [20], k = 10 [4,48]. Although adaptive Gaussian kernel may produce better results on images with large scale variations, our intuition is that drone images typically have less scale variations as compared to surveillance images e.g., from CCTV. The scale variations in drone images result from drone flying altitudes which do not vary too much due to regulatory measures. Thus, the scale variations are limited and a single value of \u03c3 can be experimentally determined to produce density maps. In our experiments, we empirically determined the value of \u03c3 based on the drone altitudes. The datasets used in this study contain images captured from different altitudes. Thus, we first segregated images into different groups by drone altitudes and empirically found separate values of \u03c3 for each group.\n\n\nTraining.\n\nThe image resolutions in the datasets used in this study are not very high, thus we use whole image-based training operations without extracting patches or downsampling operations on training images. However, to avoid model overfitting, data augmentations techniques such as horizontal flipping, and random brightness and contrast are applied. The kernels in all Conv layers are randomly initialized using Gaussian distribution with the value of standard deviations 0.01. We used Adam optimizer with a base learning rate 0.0001. The loss function used is pixel-wise euclidean distance between the target and predicted density maps which is defined in Eq. 2.\nL(\u0398) = 1 N N 1 ||D(X i ; \u0398) \u2212 D gt i || 2 2 (2)\nwhere N is the number of samples in training data, D(X i ; \u0398) is the predicted density map with parameters \u0398 for the input image X i , and D gt i is the ground truth density map.\n\nWe further applied curriculum learning technique to improve the learning performance of our model. In CL settings, we used transfer learning using CSR-Net to determine the difficulty level of each image. Based on the counting error, images are sorted in ascending order before packing them into mini-batches. Thus, mini-batches are created in the order of their cumulative complexity.\n\n\nExperiments and Results\n\nThe proposed model (LCDnet) was trained on a single GPU (Nvidia RTX-8000) using PyTorch deep learning framework. We also implemented and trained other models used in this study from the scratch for fair comparison.\n\n\nDatasets\n\nWe evaluate the proposed model on two benchmark datasets i.e., DroneRGBT and CARPK. The DroneRGBT dataset contains images of people whereas the CARPK dataset contains images of cars, both captured from drones.\n\nDroneRGBT. The dataset contains 3600 RGB and thermal image pairs with a spatial resolution of 512 \u00d7 640 pixels. The images cover a wide range of scenes e.g., campus, streets, parks, parking lots, playgrounds, and plazas. The dataset is divided into training set (1807 samples) and test set (912 samples) in such a way that both the training and test set include diverse images (i.e., different scenes, crowd densities, illumination, and scales) to avoid overfitting. The dataset provides head annotations of people. The count distribution and sample images from the dataset are presented in Fig. 3 and Fig. 2, respectively.\n\n\nCount 19\n\nCount 31 Count 153 \n\n\nEvaluation Metrics\n\nMost of the existing works on crowd counting use mean absolute error (MAE) and Grid average mean absolute error (GAME) to evaluate the accuracy of the model. MAE is the average of absolute error in predicted counts and actual counts of all images. It is calculated using 3:\nM AE = 1 N N 1 (e n \u2212\u011d n )(3)\nwhere, N is the total number of images in the dataset, g n is the ground truth (actual count) and\u00ea n is the prediction (estimated count) in the n th image. While MAE is the most widely used metric in crowd counting research and is often used to compare various models, MAE provide image-wide counting and does not provide where the estimations have been done in the image. Owing to the possible estimation errors in MAE, authors in [14] proposed Grid Average Mean absolute Error (GAME). In GAME, an image is divided into 4 L nonoverlapping patches and compute MAE separately for each patch. Thus, GAME poses a more robust and accurate estimation for crowd counting applications. It is defined in q. 4.\nGAM E = 1 N N n=1 ( 4 L l=1 |e l n \u2212 g l n |)(4)\nThe GAME metric is more robust to localisation errors in density estimation by calculating localized error among the target and predicted density maps. We set the value of L = 4, thus each density map is divided into a grid size of 4 \u00d7 4 creating 16 patches. The absolute difference in the head count for each patch is measured and summed for all patches of the same density map, then averaged over the whole dataset. We compared the LCDnet model against existing models over the two metrics to provide a fair evaluation of the model accuracy. However, the true benefit of LCDnet is the lower model complexity at the cost of tolerable counting error. In addition, the performance is measured over two other metrics i.e., structural similarity index (SSIM), peak signal-to-noise ratio (PSNR). Both SSIM and PSNR evaluate the quality of the predicted density maps and are measured in Eq. 5 and 6 as follows:\nSSIM (x, y) = (2\u00b5 x \u00b5 y + C 1 )(2\u03c3 x \u03c3 y C 2 ) (\u00b5 2 z \u00b5 2 y + C 1 )(\u00b5 2 z \u00b5 2 y + C 2 )(5)\nwhere \u00b5 x , \u00b5 y , \u03c3 x , \u03c3 y represents the means and standard deviations of the actual and predicted density maps, respectively.\nP SN R = 10log 10 M ax(I 2 ) M SE(6)\nwhere M ax(I 2 ) the maximal in the image data. If it is an 8-bit unsigned integer data type, the M ax(I 2 ) = 255.\n\n\nEvaluation Results\n\nWe compared the proposed model (LCDnet) against two mainstream crowd density estimation models i.e., MCNN [50], and CSRNet [26]. MCNN is a relatively small-sized CNN model which has gained good counting accuracy over several benchmark datasets as compared to other models of similar size. CSRNet on the other hand is a deep CNN model that uses VGG-16 [34] as a front-end and has shown high accuracy in dense crowd scenes. Both models have been used for comparison in many crowd counting studies and thus we chose them as candidate models for small-sized and large-sized models in this study. We compare the performance of LCDnet in terms of counting accuracy at the cost of model size and complexity against the MCNN and CSRNet on DroneRGBT and CARPK datasets. While the primay comparison is done against MCNN and CSRNet, we additionally provide complexity comparison against some other well-known counting models to highlight the benefit of the proposed model (LCDnet). The model complexity comparison is shown in Table  1 whereas the accuracy comparison is depicted in Table 2. The inference time is computed on GPU server (Nvidia RTX 8GB), and two different edge devices (Nvidia Jetson Xavier and Jetson Nano). The system details of these devices are as follows:\n\n-Server: GPU (Nvidia Quadro RTX-8000).\n\n-Jetson Xavior NX: 64bit system with Processor (6-core NVIDIA Carmel ARM), Memory (8GB), GPU (NVIDIA Volta architecture with 384 NVIDIA CUDA cores and 48 Tensor cores). -Jetson Nano: 64bit system with Quad-Core Arm Cortex-A57 MPCore, Memory (4GB), GPU (128-core NVIDIA Maxwell GPU).  (17.9). In terms of complexity, LCDnet has almost half the number of parameters and half number of multiply-add-calculations (GMACs). LCDnet also incurs much lower ( 1 2 \u00d7) inference delay as compared to MCNN.\n\nAlthough the accuracy of CSRNet is much higher than both LCDnet (3\u00d7) and MCNN (2.2\u00d7), it has a very huge size requiring large memory size and much higher (20\u00d7) inference delay than LCDnet. On CARPK dataset, LCDnet achieves better results. It achieves MAE 13.1, which is close to MCNN (10.1) and slighltly less than CSRNet (6.12). Some sample predictions using MCNN, CSRNet and the proposed LCDnet models over DroneRGBT and CARPK datasets, respectively, are shown in Fig. 6 and Fig. 7. It can be visualized that LCDnet has good detection capability and produces better quality density maps than MCNN for DroneRGBT dataset. We believe this is due to the use of small sized filters (1 \u00d7 3 and 3 \u00d7 1). The better quality of density map is expected and evident from the higher values of SSIM and PSNR.\n\n\nImage\n\nGT  6. Comparison of predictions on on DroneRGBT dataset. The first two columns shows crowd images and their corresponding ground truth. Columns 3-4 shows predictions using MCNN [50] and CSRNet [26] without curriculum learning. Column 5 shows predictions using LCDnet (ours), respectively.\n\n\nConclusion\n\nThis paper proposes a lightweight crowd density estimation model (LCDnet) for deployment over resource-constrained embedded devices (e.g., drones) suitable for real-time applications (e.g., surveillance) scenarios.  7. Comparison of predictions on on CARPK dataset. The first two columns shows crowd images and their corresponding ground truth. Columns 3-4 shows predictions using MCNN [50] and CSRNet [26]. without curriculum learning. Column 5 shows predictions using LCDnet (ours), respectively.\n\nCNN model (ii) improved ground truth generation from head annotations and drone altitudes, and (iii) improved training mechanism using curriculum learning. LCDnet is evaluated on two different datasets of drone-captured images i.e., DroneRGBT, CARPK. Our experimental analysis shows that LCDnet achieves reasonably good accuracy at much lower computational cost. The small memory footprint and lower inference time makes LCDnet a good fit for drone-based video surveillance.\n\nFig. 1 .\n1LCDnet with curriculum learning.\n\nFig. 2 .Fig. 3 .Fig. 4 .Fig. 5 .\n2345Sample images (top) and their corresponding density maps (bottom) from DroneRGBT dataset.CARPK. The dataset contains 1 images of cars from from 4 different parking lots captured with a drone. The dataset is divided into a training set containing Count distribution in DroneRGBT dataset.989 images and a test set containing 459 images. The dataset has a total number of 89, 777 cars. The original dataset contains bounding box annotations, however we transformed the original annotations to dot annotation by taking the center of the bounding box. The count distribution and sample images from the dataset are presented inFig. 5andFig. 4, respectively. Sample images (top) and their corresponding density maps (bottom) from CARPK dataset. Count distribution in CARPK dataset.\n\nTable 1 .Table 2 .\n12Comparison of proposed scheme (LCDnet trained with curriculum learning) against SOTA models for number of parameters (in Million), GMACs, size (in MB), and inference time (in mili seconds) for fixed input size. Accuracy comparison of the proposed scheme (LCDnet trained with curriculum learning) against SOTA models over DroneRGBT dataset[30] and CARPK dataset[17].MAE GAME SSIM PSNR MAE GAME SSIM PSNRLCDnet (ours) 21.4 46.9 0.60 21.39 13.1 45.2 0.79 20.14 On DroneRGBT dataset, LCDnet achieves MAE of 21.4 which is comparable with that of MCNNModel \nOutput Parameters (M) Size (MB) GMACs \nInference Time (s) \n\nServer Jetson Xavier Jetson Nano \n\nCrowdCNN [49] \n1/4 \n1.66 \n6.65 \n3.96 0.071 \n0.073 \n0.23 \n\nMCNN [50] \n1/4 \n0.13 \n0.53 \n8.82 \n0.05 \n0.10 \n0.21 \n\nCMTL [35] \n1/4 \n2.45 \n9.82 \n39.82 0.098 \n0.31 \n0.62 \n\nCSRNet [26] \n1/8 \n16.26 \n65.05 \n135.4 0.19 \n1.01 \n1.88 \n\nSANet [4] \n1/4 \n0.25 \n1.02 \n8.97 0.075 \n0.12 \n0.23 \n\nLCDnet (ours) 1/2 \n0.05 \n0.21 \n4.85 0.006 \n0.05 \n0.10 \n\nMethod \nDroneRGBT \nCARPK \n\nCrowdCNN [49] 26.6 48.2 0.52 17.3 15.6 49.1 0.63 18.8 \nMCNN [50] \n17.9 44.49 0.54 18.2 10.3 42.40 0.76 19.21 \nCMTL [35] \n18.1 40.5 0.53 17.1 10.2 41.3 0.73 19.0 \nCSRNet [26] \n7.6 \n25.7 0.72 21.70 6.12 21.8 0.82 20.52 \nSANet [4] \n16.2 34.5 0.59 19.4 \n9.8 \n27.2 0.76 19.8 \n\n\n\nThe paper outlines various design principles and best practices used to develop efficient CNN architectures. LCDnet is designed by adopting three efficient strategies; (i) compactRGB image \nGT Count 56 \nCount 68 \nCount 47 \nCount 69 \n\nRGB image \nGT Count 125 \nCount 117 \nCount 117 \nCount 122 \n\nRGB image \nGT Count 113 \nCount 107 \nCount 115 \nCount 99 \n\nFig. \n\nGlobal sum pooling: A generalization trick for object counting with small datasets of large images. S Aich, I Stavness, arXiv:1805.11123arXiv preprintAich, S., Stavness, I.: Global sum pooling: A generalization trick for object counting with small datasets of large images. arXiv preprint arXiv:1805.11123 (2018)\n\nCurriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, 10.1145/1553374.1553380Proceedings of the 26th Annual International Conference on Machine Learning. p. 41-48. ICML '09. the 26th Annual International Conference on Machine Learning. p. 41-48. ICML '09New York, NY, USAAssociation for Computing MachineryBengio, Y., Louradour, J., Collobert, R., Weston, J.: Curriculum learning. In: Proceedings of the 26th Annual International Conference on Machine Learn- ing. p. 41-48. ICML '09, Association for Computing Machinery, New York, NY, USA (2009). https://doi.org/10.1145/1553374.1553380, https://doi.org/ 10.1145/1553374.1553380\n\nCrowdnet: A deep convolutional network for dense crowd counting. L Boominathan, S S S Kruthiventi, R V Babu, Proceedings of the 24th ACM international conference on Multimedia. the 24th ACM international conference on MultimediaBoominathan, L., Kruthiventi, S.S.S., Babu, R.V.: Crowdnet: A deep convolutional network for dense crowd counting. Proceedings of the 24th ACM international conference on Multimedia (2016)\n\nScale aggregation network for accurate and efficient crowd counting. X Cao, Z Wang, Y Zhao, F Su, ECCVCao, X., Wang, Z., Zhao, Y., Su, F.: Scale aggregation network for accurate and efficient crowd counting. In: ECCV (2018)\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. A B Chan, Z S J Liang, N Vasconcelos, IEEE Conference on Computer Vision and Pattern Recognition. Chan, A.B., Liang, Z.S.J., Vasconcelos, N.: Privacy preserving crowd monitoring: Counting people without people models or tracking. 2008 IEEE Conference on Computer Vision and Pattern Recognition pp. 1-7 (2008)\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, BMVCChen, K., Loy, C.C., Gong, S., Xiang, T.: Feature mining for localised crowd count- ing. In: BMVC (2012)\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, 21.1-21.11Proceedings of the British Machine Vision Conference. the British Machine Vision ConferenceBMVA PressChen, K., Loy, C.C., Gong, S., Xiang, T.: Feature mining for localised crowd count- ing. In: Proceedings of the British Machine Vision Conference. pp. 21.1-21.11. BMVA Press (2012)\n\nDeep density-aware count regressor. Z Chen, J Cheng, Y Yuan, D Liao, Y Li, J Lv, ECAIChen, Z., Cheng, J., Yuan, Y., Liao, D., Li, Y., Lv, J.: Deep density-aware count regressor. In: ECAI (2020)\n\nObject detection based on multi-layer convolution feature fusion and online hard example mining. J Chu, Z Guo, L Leng, 10.1109/ACCESS.2018.2815149IEEE Access. 6Chu, J., Guo, Z., Leng, L.: Object detection based on multi-layer convolution fea- ture fusion and online hard example mining. IEEE Access 6, 19959-19967 (2018). https://doi.org/10.1109/ACCESS.2018.2815149\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). 1Dalal, N., Triggs, B.: Histograms of oriented gradients for human detec- tion. In: 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). vol. 1, pp. 886-893 vol. 1 (2005).\n\n. 10.1109/CVPR.2005.177https://doi.org/10.1109/CVPR.2005.177\n\nCrowd monitoring using image processing. A C Davies, J H Yin, S A Velast\u00edn, Electronics & Communication Engineering Journal. 7Davies, A.C., Yin, J.H., Velast\u00edn, S.A.: Crowd monitoring using image processing. Electronics & Communication Engineering Journal 7, 37-47 (1995)\n\nObject detection with discriminatively trained part-based models. P F Felzenszwalb, R B Girshick, D Mcallester, D Ramanan, 10.1109/TPAMI.2009.167IEEE Transactions on Pattern Analysis and Machine Intelligence. 329Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection with discriminatively trained part-based models. IEEE Transac- tions on Pattern Analysis and Machine Intelligence 32(9), 1627-1645 (2010). https://doi.org/10.1109/TPAMI.2009.167\n\nReverse curriculum generation for reinforcement learning. C Florensa, D Held, M Wulfmeier, M Zhang, P Abbeel, CoRLFlorensa, C., Held, D., Wulfmeier, M., Zhang, M., Abbeel, P.: Reverse curriculum generation for reinforcement learning. In: CoRL (2017)\n\nExtremely overlapping vehicle counting. R Guerrero-G\u00f3mez-Olmedo, B Torre-Jim\u00e9nez, R J L\u00f3pez-Sastre, S Maldonado-Basc\u00f3n, D O\u00f1oro-Rubio, IbPRIAGuerrero-G\u00f3mez-Olmedo, R., Torre-Jim\u00e9nez, B., L\u00f3pez-Sastre, R.J., Maldonado- Basc\u00f3n, S., O\u00f1oro-Rubio, D.: Extremely overlapping vehicle counting. In: IbPRIA (2015)\n\nCurriculumnet: Weakly supervised learning from large-scale web images. S Guo, W Huang, H Zhang, C Zhuang, D Dong, M R Scott, D Huang, ArXiv abs/1808.01097Guo, S., Huang, W., Zhang, H., Zhuang, C., Dong, D., Scott, M.R., Huang, D.: Curriculumnet: Weakly supervised learning from large-scale web images. ArXiv abs/1808.01097 (2018)\n\nOn the power of curriculum learning in training deep networks. G Hacohen, D Weinshall, ArXiv abs/1904.03626Hacohen, G., Weinshall, D.: On the power of curriculum learning in training deep networks. ArXiv abs/1904.03626 (2019)\n\nDrone-based object counting by spatially regularized regional proposal network. M R Hsieh, Y L Lin, W H Hsu, IEEE International Conference on Computer Vision (ICCV. Hsieh, M.R., Lin, Y.L., Hsu, W.H.: Drone-based object counting by spatially reg- ularized regional proposal network. 2017 IEEE International Conference on Com- puter Vision (ICCV) pp. 4165-4173 (2017)\n\nSqueezenet: Alexnet-level accuracy with 50x fewer parameters and \u00a11mb model size. F N Iandola, M W Moskewicz, K Ashraf, S Han, W J Dally, K Keutzer, ArXiv abs/1602.07360Iandola, F.N., Moskewicz, M.W., Ashraf, K., Han, S., Dally, W.J., Keutzer, K.: Squeezenet: Alexnet-level accuracy with 50x fewer parameters and \u00a11mb model size. ArXiv abs/1602.07360 (2016)\n\nMulti-source multi-scale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, IEEE Conference on Computer Vision and Pattern Recognition. Idrees, H., Saleemi, I., Seibert, C., Shah, M.: Multi-source multi-scale counting in extremely dense crowd images. 2013 IEEE Conference on Computer Vision and Pattern Recognition pp. 2547-2554 (2013)\n\nComposition loss for counting, density map estimation and localization in dense crowds. H Idrees, M Tayyab, K Athrey, D Zhang, S A Al-Maadeed, N M Rajpoot, M Shah, ArXiv abs/1808.01050Idrees, H., Tayyab, M., Athrey, K., Zhang, D., Al-Maadeed, S.A., Rajpoot, N.M., Shah, M.: Composition loss for counting, density map estimation and localization in dense crowds. ArXiv abs/1808.01050 (2018)\n\nEasy samples first: Self-paced reranking for zero-example multimedia search. L Jiang, D Meng, T Mitamura, A Hauptmann, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaJiang, L., Meng, D., Mitamura, T., Hauptmann, A.: Easy samples first: Self-paced reranking for zero-example multimedia search. Proceedings of the 22nd ACM in- ternational conference on Multimedia (2014)\n\nCrowd counting and density estimation by trellis encoder-decoder networks. X Jiang, Z Xiao, B Zhang, X Zhen, X Cao, D S Doermann, L Shao, Jiang, X., Xiao, Z., Zhang, B., Zhen, X., Cao, X., Doermann, D.S., Shao, L.: Crowd counting and density estimation by trellis encoder-decoder networks. 2019\n\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 6126-6135 (2019)\n\nRevisiting crowd counting: State-of-the-art, trends, and future perspectives. M A Khan, H Menouar, R Hamila, ArXiv abs/2209.07271Khan, M.A., Menouar, H., Hamila, R.: Revisiting crowd counting: State-of-the-art, trends, and future perspectives. ArXiv abs/2209.07271 (2022)\n\nEstimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection. M Li, Z Zhang, K Huang, T Tan, 10.1109/ICPR.2008.476170519th International Conference on Pattern Recognition. Li, M., Zhang, Z., Huang, K., Tan, T.: Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder de- tection. In: 2008 19th International Conference on Pattern Recognition. pp. 1-4 (2008). https://doi.org/10.1109/ICPR.2008.4761705\n\nLearning error-driven curriculum for crowd counting. W Li, Z Cao, Q Wang, S Chen, R Feng, 25th International Conference on Pattern Recognition (ICPR. Li, W., Cao, Z., Wang, Q., Chen, S., Feng, R.: Learning error-driven curriculum for crowd counting. 2020 25th International Conference on Pattern Recognition (ICPR) pp. 843-849 (2021)\n\nCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes. Y Li, X Zhang, D Chen, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Li, Y., Zhang, X., Chen, D.: Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes. 2018 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition pp. 1091-1100 (2018)\n\nShape-based human detection and segmentation via hierarchical part-template matching. Z Lin, L S Davis, 10.1109/TPAMI.2009.204IEEE Transactions on Pattern Analysis and Machine Intelligence. 324Lin, Z., Davis, L.S.: Shape-based human detection and segmentation via hierarchi- cal part-template matching. IEEE Transactions on Pattern Analysis and Machine Intelligence 32(4), 604-618 (2010). https://doi.org/10.1109/TPAMI.2009.204\n\nContext-aware crowd counting. W Liu, M Salzmann, P V Fua, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR. Liu, W., Salzmann, M., Fua, P.V.: Context-aware crowd counting. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 5094-5103 (2019)\n\nAutonomous task sequencing for customized curriculum design in reinforcement learning. S Narvekar, J Sinapov, P Stone, Narvekar, S., Sinapov, J., Stone, P.: Autonomous task sequencing for customized curriculum design in reinforcement learning. In: IJCAI (2017)\n\nRgb-t crowd counting from drone: A benchmark and mmccn network. T Peng, Q Li, P Zhu, Computer Vision -ACCV 2020: 15th Asian Conference on Computer Vision. Kyoto, Japan; Berlin, HeidelbergSpringer-VerlagRevised Selected Papers, Part VIPeng, T., Li, Q., Zhu, P.: Rgb-t crowd counting from drone: A benchmark and mmccn network. In: Computer Vision -ACCV 2020: 15th Asian Conference on Computer Vision, Kyoto, Japan, November 30 -December 4, 2020, Revised Se- lected Papers, Part VI. p. 497-513. Springer-Verlag, Berlin, Heidelberg (2020)\n\nCompetencebased curriculum learning for neural machine translation. E A Platanios, O Stretcu, G Neubig, B P\u00f3czos, T M Mitchell, ArXiv abs/1903.09848Platanios, E.A., Stretcu, O., Neubig, G., P\u00f3czos, B., Mitchell, T.M.: Competence- based curriculum learning for neural machine translation. ArXiv abs/1903.09848 (2019)\n\nSelf-paced prioritized curriculum learning with coverage penalty in deep reinforcement learning. Z Ren, D Dong, H Li, C Chen, IEEE Transactions on Neural Networks and Learning Systems. 29Ren, Z., Dong, D., Li, H., Chen, C.: Self-paced prioritized curriculum learning with coverage penalty in deep reinforcement learning. IEEE Transactions on Neural Networks and Learning Systems 29, 2216-2226 (2018)\n\nSwitching convolutional neural network for crowd counting. D Sam, S Surya, R Babu, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Los Alamitos, CA, USAIEEE Computer SocietySam, D., Surya, S., Babu, R.: Switching convolutional neural network for crowd counting. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4031-4039. IEEE Computer Society, Los Alamitos, CA, USA (jul 2017)\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsSimonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (2015), http://arxiv.org/abs/1409.1556\n\nCnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. V A Sindagi, V M Patel, 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). Sindagi, V.A., Patel, V.M.: Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting. 2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS) pp. 1-6 (2017)\n\nTo choose or to fuse? scale selection for crowd counting. Q Song, C Wang, Y Wang, Y Tai, C Wang, J Li, J Wu, J Ma, AAAISong, Q., Wang, C., Wang, Y., Tai, Y., Wang, C., Li, J., Wu, J., Ma, J.: To choose or to fuse? scale selection for crowd counting. In: AAAI (2021)\n\nStriving for simplicity: The all convolutional net. J T Springenberg, A Dosovitskiy, T Brox, M A Riedmiller, CoRR abs/1412.6806Springenberg, J.T., Dosovitskiy, A., Brox, T., Riedmiller, M.A.: Striving for sim- plicity: The all convolutional net. CoRR abs/1412.6806 (2015)\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S E Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.E., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 1-9 (2015)\n\nSimple and effective curriculum pointer-generator networks for reading comprehension over long narratives. Y Tay, S Wang, A T Luu, J Fu, M C Phan, X Yuan, J Rao, S C Hui, A Zhang, ACLTay, Y., Wang, S., Luu, A.T., Fu, J., Phan, M.C., Yuan, X., Rao, J., Hui, S.C., Zhang, A.: Simple and effective curriculum pointer-generator networks for reading comprehension over long narratives. In: ACL (2019)\n\nEncoder-decoder based convolutional neural networks with multi-scale-aware modules for crowd counting. P Thanasutives, K Ichi Fukui, M Numao, B Kijsirikul, 25th International Conference on Pattern Recognition (ICPR. Thanasutives, P., ichi Fukui, K., Numao, M., Kijsirikul, B.: Encoder-decoder based convolutional neural networks with multi-scale-aware modules for crowd counting. 2020 25th International Conference on Pattern Recognition (ICPR) pp. 2382-2389 (2021)\n\nLatent gaussian mixture regression for human pose estimation. Y Tian, L Sigal, H Badino, F D La Torre, Y Liu, ACCVTian, Y., Sigal, L., Badino, H., la Torre, F.D., Liu, Y.: Latent gaussian mixture regression for human pose estimation. In: ACCV (2010)\n\nCounting people by clustering person detector outputs. I S Topkaya, H Erdogan, F Porikli, 11th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS). Topkaya, I.S., Erdogan, H., Porikli, F.: Counting people by clustering per- son detector outputs. In: 2014 11th IEEE International Conference on Ad- vanced Video and Signal Based Surveillance (AVSS). pp. 313-318 (2014).\n\n. 10.1109/AVSS.2014.6918687https://doi.org/10.1109/AVSS.2014.6918687\n\nPedestrian detection via classification on riemannian manifolds. O Tuzel, F Porikli, P Meer, 10.1109/TPAMI.2008.75IEEE Transactions on Pattern Analysis and Machine Intelligence. 3010Tuzel, O., Porikli, F., Meer, P.: Pedestrian detection via classification on rieman- nian manifolds. IEEE Transactions on Pattern Analysis and Machine Intelligence 30(10), 1713-1727 (2008). https://doi.org/10.1109/TPAMI.2008.75\n\nRobust real-time face detection. P Viola, M Jones, 10.1109/ICCV.2001.937709Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001. Eighth IEEE International Conference on Computer Vision. ICCV 20012Viola, P., Jones, M.: Robust real-time face detection. In: Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001. vol. 2, pp. 747-747 (2001). https://doi.org/10.1109/ICCV.2001.937709\n\nDensity-aware curriculum learning for crowd counting. Q Wang, W Lin, J Gao, X Li, IEEE Transactions on Cybernetics. 52Wang, Q., Lin, W., Gao, J., Li, X.: Density-aware curriculum learning for crowd counting. IEEE Transactions on Cybernetics 52, 4675-4687 (2022)\n\nCrowd counting via segmentation guided attention networks and curriculum loss. Q Wang, T Breckon, IEEE Transactions on Intelligent Transportation Systems. Wang, Q., Breckon, T.: Crowd counting via segmentation guided attention net- works and curriculum loss. IEEE Transactions on Intelligent Transportation Sys- tems (2022)\n\nDetection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors. B Wu, R Nevatia, International Journal of Computer Vision. 75Wu, B., Nevatia, R.: Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors. International Journal of Computer Vision 75, 247-266 (2006)\n\nMulti-scale convolutional neural networks for crowd counting. L Zeng, X Xu, B Cai, S Qiu, T Zhang, IEEE International Conference on Image Processing (ICIP. Zeng, L., Xu, X., Cai, B., Qiu, S., Zhang, T.: Multi-scale convolutional neural networks for crowd counting. 2017 IEEE International Conference on Image Pro- cessing (ICIP) pp. 465-469 (2017)\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Zhang, C., Li, H., Wang, X., Yang, X.: Cross-scene crowd counting via deep convo- lutional neural networks. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 833-841 (2015)\n\nSingle-image crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Zhang, Y., Zhou, D., Chen, S., Gao, S., Ma, Y.: Single-image crowd count- ing via multi-column convolutional neural network. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 589-597 (2016).\n\n. 10.1109/CVPR.2016.70https://doi.org/10.1109/CVPR.2016.70\n\nMask-refined r-cnn: A network for refining object details in instance segmentation. Y Zhang, J Chu, L Leng, J Miao, Sensors. 20Zhang, Y., Chu, J., Leng, L., Miao, J.: Mask-refined r-cnn: A network for refining object details in instance segmentation. Sensors (Basel, Switzerland) 20 (2020)\n", "annotations": {"author": "[{\"end\":194,\"start\":100},{\"end\":284,\"start\":195},{\"end\":356,\"start\":285}]", "publisher": null, "author_last_name": "[{\"end\":118,\"start\":114},{\"end\":208,\"start\":201},{\"end\":297,\"start\":291}]", "author_first_name": "[{\"end\":108,\"start\":100},{\"end\":113,\"start\":109},{\"end\":200,\"start\":195},{\"end\":290,\"start\":285}]", "author_affiliation": "[{\"end\":193,\"start\":136},{\"end\":283,\"start\":226},{\"end\":355,\"start\":316}]", "title": "[{\"end\":86,\"start\":1},{\"end\":442,\"start\":357}]", "venue": null, "abstract": "[{\"end\":2703,\"start\":511}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3043,\"start\":3039},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3135,\"start\":3132},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":3138,\"start\":3135},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3226,\"start\":3222},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3423,\"start\":3420},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3426,\"start\":3423},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3429,\"start\":3426},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3432,\"start\":3429},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3906,\"start\":3902},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4283,\"start\":4279},{\"end\":4482,\"start\":4475},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5012,\"start\":5008},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6458,\"start\":6455},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6948,\"start\":6944},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6951,\"start\":6948},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6954,\"start\":6951},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6994,\"start\":6990},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6997,\"start\":6994},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7026,\"start\":7022},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7029,\"start\":7026},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7032,\"start\":7029},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7057,\"start\":7053},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7060,\"start\":7057},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7774,\"start\":7770},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9019,\"start\":9016},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9029,\"start\":9026},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9048,\"start\":9044},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9074,\"start\":9070},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9175,\"start\":9171},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9190,\"start\":9186},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9217,\"start\":9213},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9306,\"start\":9302},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9318,\"start\":9314},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9687,\"start\":9683},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9926,\"start\":9922},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10315,\"start\":10312},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10553,\"start\":10549},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11219,\"start\":11215},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11354,\"start\":11350},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11632,\"start\":11628},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11682,\"start\":11678},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12247,\"start\":12243},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12359,\"start\":12355},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12375,\"start\":12371},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12695,\"start\":12691},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12704,\"start\":12701},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12717,\"start\":12713},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12732,\"start\":12729},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12745,\"start\":12741},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12760,\"start\":12756},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12777,\"start\":12773},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12974,\"start\":12970},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":13141,\"start\":13137},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13207,\"start\":13204},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16713,\"start\":16710},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17175,\"start\":17171},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":17187,\"start\":17184},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":17190,\"start\":17187},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":21137,\"start\":21133},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":22863,\"start\":22859},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22880,\"start\":22876},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23108,\"start\":23104},{\"end\":24350,\"start\":24344},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25543,\"start\":25539},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":25559,\"start\":25555},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26055,\"start\":26051},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26071,\"start\":26067},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27861,\"start\":27857},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27883,\"start\":27879}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26683,\"start\":26640},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27496,\"start\":26684},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":28795,\"start\":27497},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29154,\"start\":28796}]", "paragraph": "[{\"end\":2940,\"start\":2705},{\"end\":3314,\"start\":2942},{\"end\":4284,\"start\":3316},{\"end\":5247,\"start\":4310},{\"end\":6349,\"start\":5276},{\"end\":7097,\"start\":6377},{\"end\":8198,\"start\":7099},{\"end\":8243,\"start\":8200},{\"end\":8795,\"start\":8245},{\"end\":9873,\"start\":8812},{\"end\":11170,\"start\":9875},{\"end\":12031,\"start\":11172},{\"end\":12778,\"start\":12033},{\"end\":13304,\"start\":12780},{\"end\":13810,\"start\":13306},{\"end\":14433,\"start\":13830},{\"end\":15414,\"start\":14458},{\"end\":15946,\"start\":15416},{\"end\":16170,\"start\":15975},{\"end\":17969,\"start\":16202},{\"end\":18640,\"start\":17983},{\"end\":18867,\"start\":18689},{\"end\":19253,\"start\":18869},{\"end\":19495,\"start\":19281},{\"end\":19717,\"start\":19508},{\"end\":20342,\"start\":19719},{\"end\":20374,\"start\":20355},{\"end\":20670,\"start\":20397},{\"end\":21402,\"start\":20701},{\"end\":22357,\"start\":21452},{\"end\":22577,\"start\":22449},{\"end\":22730,\"start\":22615},{\"end\":24018,\"start\":22753},{\"end\":24058,\"start\":24020},{\"end\":24553,\"start\":24060},{\"end\":25351,\"start\":24555},{\"end\":25650,\"start\":25361},{\"end\":26163,\"start\":25665},{\"end\":26639,\"start\":26165}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16201,\"start\":16171},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18688,\"start\":18641},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20700,\"start\":20671},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21451,\"start\":21403},{\"attributes\":{\"id\":\"formula_4\"},\"end\":22448,\"start\":22358},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22614,\"start\":22578}]", "table_ref": "[{\"end\":23776,\"start\":23768},{\"end\":23831,\"start\":23824}]", "section_header": "[{\"attributes\":{\"n\":\"0.1\"},\"end\":4308,\"start\":4287},{\"attributes\":{\"n\":\"0.2\"},\"end\":5274,\"start\":5250},{\"attributes\":{\"n\":\"0.3\"},\"end\":6375,\"start\":6352},{\"attributes\":{\"n\":\"1\"},\"end\":8810,\"start\":8798},{\"attributes\":{\"n\":\"2\"},\"end\":13828,\"start\":13813},{\"attributes\":{\"n\":\"2.1\"},\"end\":14456,\"start\":14436},{\"attributes\":{\"n\":\"2.2\"},\"end\":15973,\"start\":15949},{\"attributes\":{\"n\":\"2.3\"},\"end\":17981,\"start\":17972},{\"attributes\":{\"n\":\"3\"},\"end\":19279,\"start\":19256},{\"attributes\":{\"n\":\"3.1\"},\"end\":19506,\"start\":19498},{\"end\":20353,\"start\":20345},{\"attributes\":{\"n\":\"3.2\"},\"end\":20395,\"start\":20377},{\"attributes\":{\"n\":\"3.3\"},\"end\":22751,\"start\":22733},{\"end\":25359,\"start\":25354},{\"attributes\":{\"n\":\"4\"},\"end\":25663,\"start\":25653},{\"end\":26649,\"start\":26641},{\"end\":26717,\"start\":26685},{\"end\":27516,\"start\":27498}]", "table": "[{\"end\":28795,\"start\":28064},{\"end\":29154,\"start\":28977}]", "figure_caption": "[{\"end\":26683,\"start\":26651},{\"end\":27496,\"start\":26722},{\"end\":28064,\"start\":27519},{\"end\":28977,\"start\":28798}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14503,\"start\":14497},{\"end\":20316,\"start\":20310},{\"end\":20327,\"start\":20321},{\"end\":25027,\"start\":25021},{\"end\":25038,\"start\":25032},{\"end\":25366,\"start\":25365},{\"end\":25882,\"start\":25881}]", "bib_author_first_name": "[{\"end\":29257,\"start\":29256},{\"end\":29265,\"start\":29264},{\"end\":29492,\"start\":29491},{\"end\":29502,\"start\":29501},{\"end\":29515,\"start\":29514},{\"end\":29528,\"start\":29527},{\"end\":30179,\"start\":30178},{\"end\":30194,\"start\":30193},{\"end\":30198,\"start\":30195},{\"end\":30213,\"start\":30212},{\"end\":30215,\"start\":30214},{\"end\":30601,\"start\":30600},{\"end\":30608,\"start\":30607},{\"end\":30616,\"start\":30615},{\"end\":30624,\"start\":30623},{\"end\":30845,\"start\":30844},{\"end\":30847,\"start\":30846},{\"end\":30855,\"start\":30854},{\"end\":30859,\"start\":30856},{\"end\":30868,\"start\":30867},{\"end\":31200,\"start\":31199},{\"end\":31208,\"start\":31207},{\"end\":31210,\"start\":31209},{\"end\":31217,\"start\":31216},{\"end\":31225,\"start\":31224},{\"end\":31389,\"start\":31388},{\"end\":31397,\"start\":31396},{\"end\":31399,\"start\":31398},{\"end\":31406,\"start\":31405},{\"end\":31414,\"start\":31413},{\"end\":31752,\"start\":31751},{\"end\":31760,\"start\":31759},{\"end\":31769,\"start\":31768},{\"end\":31777,\"start\":31776},{\"end\":31785,\"start\":31784},{\"end\":31791,\"start\":31790},{\"end\":32008,\"start\":32007},{\"end\":32015,\"start\":32014},{\"end\":32022,\"start\":32021},{\"end\":32332,\"start\":32331},{\"end\":32341,\"start\":32340},{\"end\":32758,\"start\":32757},{\"end\":32760,\"start\":32759},{\"end\":32770,\"start\":32769},{\"end\":32772,\"start\":32771},{\"end\":32779,\"start\":32778},{\"end\":32781,\"start\":32780},{\"end\":33056,\"start\":33055},{\"end\":33058,\"start\":33057},{\"end\":33074,\"start\":33073},{\"end\":33076,\"start\":33075},{\"end\":33088,\"start\":33087},{\"end\":33102,\"start\":33101},{\"end\":33521,\"start\":33520},{\"end\":33533,\"start\":33532},{\"end\":33541,\"start\":33540},{\"end\":33554,\"start\":33553},{\"end\":33563,\"start\":33562},{\"end\":33754,\"start\":33753},{\"end\":33779,\"start\":33778},{\"end\":33796,\"start\":33795},{\"end\":33798,\"start\":33797},{\"end\":33814,\"start\":33813},{\"end\":33834,\"start\":33833},{\"end\":34091,\"start\":34090},{\"end\":34098,\"start\":34097},{\"end\":34107,\"start\":34106},{\"end\":34116,\"start\":34115},{\"end\":34126,\"start\":34125},{\"end\":34134,\"start\":34133},{\"end\":34136,\"start\":34135},{\"end\":34145,\"start\":34144},{\"end\":34414,\"start\":34413},{\"end\":34425,\"start\":34424},{\"end\":34658,\"start\":34657},{\"end\":34660,\"start\":34659},{\"end\":34669,\"start\":34668},{\"end\":34671,\"start\":34670},{\"end\":34678,\"start\":34677},{\"end\":34680,\"start\":34679},{\"end\":35027,\"start\":35026},{\"end\":35029,\"start\":35028},{\"end\":35040,\"start\":35039},{\"end\":35042,\"start\":35041},{\"end\":35055,\"start\":35054},{\"end\":35065,\"start\":35064},{\"end\":35072,\"start\":35071},{\"end\":35074,\"start\":35073},{\"end\":35083,\"start\":35082},{\"end\":35371,\"start\":35370},{\"end\":35381,\"start\":35380},{\"end\":35392,\"start\":35391},{\"end\":35403,\"start\":35402},{\"end\":35760,\"start\":35759},{\"end\":35770,\"start\":35769},{\"end\":35780,\"start\":35779},{\"end\":35790,\"start\":35789},{\"end\":35799,\"start\":35798},{\"end\":35801,\"start\":35800},{\"end\":35815,\"start\":35814},{\"end\":35817,\"start\":35816},{\"end\":35828,\"start\":35827},{\"end\":36140,\"start\":36139},{\"end\":36149,\"start\":36148},{\"end\":36157,\"start\":36156},{\"end\":36169,\"start\":36168},{\"end\":36580,\"start\":36579},{\"end\":36589,\"start\":36588},{\"end\":36597,\"start\":36596},{\"end\":36606,\"start\":36605},{\"end\":36614,\"start\":36613},{\"end\":36621,\"start\":36620},{\"end\":36623,\"start\":36622},{\"end\":36635,\"start\":36634},{\"end\":37041,\"start\":37040},{\"end\":37043,\"start\":37042},{\"end\":37051,\"start\":37050},{\"end\":37062,\"start\":37061},{\"end\":37352,\"start\":37351},{\"end\":37358,\"start\":37357},{\"end\":37367,\"start\":37366},{\"end\":37376,\"start\":37375},{\"end\":37794,\"start\":37793},{\"end\":37800,\"start\":37799},{\"end\":37807,\"start\":37806},{\"end\":37815,\"start\":37814},{\"end\":37823,\"start\":37822},{\"end\":38169,\"start\":38168},{\"end\":38175,\"start\":38174},{\"end\":38184,\"start\":38183},{\"end\":38556,\"start\":38555},{\"end\":38563,\"start\":38562},{\"end\":38565,\"start\":38564},{\"end\":38929,\"start\":38928},{\"end\":38936,\"start\":38935},{\"end\":38948,\"start\":38947},{\"end\":38950,\"start\":38949},{\"end\":39275,\"start\":39274},{\"end\":39287,\"start\":39286},{\"end\":39298,\"start\":39297},{\"end\":39514,\"start\":39513},{\"end\":39522,\"start\":39521},{\"end\":39528,\"start\":39527},{\"end\":40054,\"start\":40053},{\"end\":40056,\"start\":40055},{\"end\":40069,\"start\":40068},{\"end\":40080,\"start\":40079},{\"end\":40090,\"start\":40089},{\"end\":40100,\"start\":40099},{\"end\":40102,\"start\":40101},{\"end\":40400,\"start\":40399},{\"end\":40407,\"start\":40406},{\"end\":40415,\"start\":40414},{\"end\":40421,\"start\":40420},{\"end\":40763,\"start\":40762},{\"end\":40770,\"start\":40769},{\"end\":40779,\"start\":40778},{\"end\":41206,\"start\":41205},{\"end\":41218,\"start\":41217},{\"end\":41711,\"start\":41710},{\"end\":41713,\"start\":41712},{\"end\":41724,\"start\":41723},{\"end\":41726,\"start\":41725},{\"end\":42125,\"start\":42124},{\"end\":42133,\"start\":42132},{\"end\":42141,\"start\":42140},{\"end\":42149,\"start\":42148},{\"end\":42156,\"start\":42155},{\"end\":42164,\"start\":42163},{\"end\":42170,\"start\":42169},{\"end\":42176,\"start\":42175},{\"end\":42386,\"start\":42385},{\"end\":42388,\"start\":42387},{\"end\":42404,\"start\":42403},{\"end\":42419,\"start\":42418},{\"end\":42427,\"start\":42426},{\"end\":42429,\"start\":42428},{\"end\":42639,\"start\":42638},{\"end\":42650,\"start\":42649},{\"end\":42657,\"start\":42656},{\"end\":42664,\"start\":42663},{\"end\":42676,\"start\":42675},{\"end\":42678,\"start\":42677},{\"end\":42686,\"start\":42685},{\"end\":42698,\"start\":42697},{\"end\":42707,\"start\":42706},{\"end\":42720,\"start\":42719},{\"end\":43139,\"start\":43138},{\"end\":43146,\"start\":43145},{\"end\":43154,\"start\":43153},{\"end\":43156,\"start\":43155},{\"end\":43163,\"start\":43162},{\"end\":43169,\"start\":43168},{\"end\":43171,\"start\":43170},{\"end\":43179,\"start\":43178},{\"end\":43187,\"start\":43186},{\"end\":43194,\"start\":43193},{\"end\":43196,\"start\":43195},{\"end\":43203,\"start\":43202},{\"end\":43532,\"start\":43531},{\"end\":43548,\"start\":43547},{\"end\":43562,\"start\":43561},{\"end\":43571,\"start\":43570},{\"end\":43958,\"start\":43957},{\"end\":43966,\"start\":43965},{\"end\":43975,\"start\":43974},{\"end\":43985,\"start\":43984},{\"end\":43987,\"start\":43986},{\"end\":43999,\"start\":43998},{\"end\":44202,\"start\":44201},{\"end\":44204,\"start\":44203},{\"end\":44215,\"start\":44214},{\"end\":44226,\"start\":44225},{\"end\":44684,\"start\":44683},{\"end\":44693,\"start\":44692},{\"end\":44704,\"start\":44703},{\"end\":45063,\"start\":45062},{\"end\":45072,\"start\":45071},{\"end\":45515,\"start\":45514},{\"end\":45523,\"start\":45522},{\"end\":45530,\"start\":45529},{\"end\":45537,\"start\":45536},{\"end\":45803,\"start\":45802},{\"end\":45811,\"start\":45810},{\"end\":46168,\"start\":46167},{\"end\":46174,\"start\":46173},{\"end\":46492,\"start\":46491},{\"end\":46500,\"start\":46499},{\"end\":46506,\"start\":46505},{\"end\":46513,\"start\":46512},{\"end\":46520,\"start\":46519},{\"end\":46846,\"start\":46845},{\"end\":46855,\"start\":46854},{\"end\":46861,\"start\":46860},{\"end\":46869,\"start\":46868},{\"end\":47217,\"start\":47216},{\"end\":47226,\"start\":47225},{\"end\":47234,\"start\":47233},{\"end\":47242,\"start\":47241},{\"end\":47249,\"start\":47248},{\"end\":47693,\"start\":47692},{\"end\":47702,\"start\":47701},{\"end\":47709,\"start\":47708},{\"end\":47717,\"start\":47716}]", "bib_author_last_name": "[{\"end\":29262,\"start\":29258},{\"end\":29274,\"start\":29266},{\"end\":29499,\"start\":29493},{\"end\":29512,\"start\":29503},{\"end\":29525,\"start\":29516},{\"end\":29535,\"start\":29529},{\"end\":30191,\"start\":30180},{\"end\":30210,\"start\":30199},{\"end\":30220,\"start\":30216},{\"end\":30605,\"start\":30602},{\"end\":30613,\"start\":30609},{\"end\":30621,\"start\":30617},{\"end\":30627,\"start\":30625},{\"end\":30852,\"start\":30848},{\"end\":30865,\"start\":30860},{\"end\":30880,\"start\":30869},{\"end\":31205,\"start\":31201},{\"end\":31214,\"start\":31211},{\"end\":31222,\"start\":31218},{\"end\":31231,\"start\":31226},{\"end\":31394,\"start\":31390},{\"end\":31403,\"start\":31400},{\"end\":31411,\"start\":31407},{\"end\":31420,\"start\":31415},{\"end\":31757,\"start\":31753},{\"end\":31766,\"start\":31761},{\"end\":31774,\"start\":31770},{\"end\":31782,\"start\":31778},{\"end\":31788,\"start\":31786},{\"end\":31794,\"start\":31792},{\"end\":32012,\"start\":32009},{\"end\":32019,\"start\":32016},{\"end\":32027,\"start\":32023},{\"end\":32338,\"start\":32333},{\"end\":32348,\"start\":32342},{\"end\":32767,\"start\":32761},{\"end\":32776,\"start\":32773},{\"end\":32790,\"start\":32782},{\"end\":33071,\"start\":33059},{\"end\":33085,\"start\":33077},{\"end\":33099,\"start\":33089},{\"end\":33110,\"start\":33103},{\"end\":33530,\"start\":33522},{\"end\":33538,\"start\":33534},{\"end\":33551,\"start\":33542},{\"end\":33560,\"start\":33555},{\"end\":33570,\"start\":33564},{\"end\":33776,\"start\":33755},{\"end\":33793,\"start\":33780},{\"end\":33811,\"start\":33799},{\"end\":33831,\"start\":33815},{\"end\":33846,\"start\":33835},{\"end\":34095,\"start\":34092},{\"end\":34104,\"start\":34099},{\"end\":34113,\"start\":34108},{\"end\":34123,\"start\":34117},{\"end\":34131,\"start\":34127},{\"end\":34142,\"start\":34137},{\"end\":34151,\"start\":34146},{\"end\":34422,\"start\":34415},{\"end\":34435,\"start\":34426},{\"end\":34666,\"start\":34661},{\"end\":34675,\"start\":34672},{\"end\":34684,\"start\":34681},{\"end\":35037,\"start\":35030},{\"end\":35052,\"start\":35043},{\"end\":35062,\"start\":35056},{\"end\":35069,\"start\":35066},{\"end\":35080,\"start\":35075},{\"end\":35091,\"start\":35084},{\"end\":35378,\"start\":35372},{\"end\":35389,\"start\":35382},{\"end\":35400,\"start\":35393},{\"end\":35408,\"start\":35404},{\"end\":35767,\"start\":35761},{\"end\":35777,\"start\":35771},{\"end\":35787,\"start\":35781},{\"end\":35796,\"start\":35791},{\"end\":35812,\"start\":35802},{\"end\":35825,\"start\":35818},{\"end\":35833,\"start\":35829},{\"end\":36146,\"start\":36141},{\"end\":36154,\"start\":36150},{\"end\":36166,\"start\":36158},{\"end\":36179,\"start\":36170},{\"end\":36586,\"start\":36581},{\"end\":36594,\"start\":36590},{\"end\":36603,\"start\":36598},{\"end\":36611,\"start\":36607},{\"end\":36618,\"start\":36615},{\"end\":36632,\"start\":36624},{\"end\":36640,\"start\":36636},{\"end\":37048,\"start\":37044},{\"end\":37059,\"start\":37052},{\"end\":37069,\"start\":37063},{\"end\":37355,\"start\":37353},{\"end\":37364,\"start\":37359},{\"end\":37373,\"start\":37368},{\"end\":37380,\"start\":37377},{\"end\":37797,\"start\":37795},{\"end\":37804,\"start\":37801},{\"end\":37812,\"start\":37808},{\"end\":37820,\"start\":37816},{\"end\":37828,\"start\":37824},{\"end\":38172,\"start\":38170},{\"end\":38181,\"start\":38176},{\"end\":38189,\"start\":38185},{\"end\":38560,\"start\":38557},{\"end\":38571,\"start\":38566},{\"end\":38933,\"start\":38930},{\"end\":38945,\"start\":38937},{\"end\":38954,\"start\":38951},{\"end\":39284,\"start\":39276},{\"end\":39295,\"start\":39288},{\"end\":39304,\"start\":39299},{\"end\":39519,\"start\":39515},{\"end\":39525,\"start\":39523},{\"end\":39532,\"start\":39529},{\"end\":40066,\"start\":40057},{\"end\":40077,\"start\":40070},{\"end\":40087,\"start\":40081},{\"end\":40097,\"start\":40091},{\"end\":40111,\"start\":40103},{\"end\":40404,\"start\":40401},{\"end\":40412,\"start\":40408},{\"end\":40418,\"start\":40416},{\"end\":40426,\"start\":40422},{\"end\":40767,\"start\":40764},{\"end\":40776,\"start\":40771},{\"end\":40784,\"start\":40780},{\"end\":41215,\"start\":41207},{\"end\":41228,\"start\":41219},{\"end\":41721,\"start\":41714},{\"end\":41732,\"start\":41727},{\"end\":42130,\"start\":42126},{\"end\":42138,\"start\":42134},{\"end\":42146,\"start\":42142},{\"end\":42153,\"start\":42150},{\"end\":42161,\"start\":42157},{\"end\":42167,\"start\":42165},{\"end\":42173,\"start\":42171},{\"end\":42179,\"start\":42177},{\"end\":42401,\"start\":42389},{\"end\":42416,\"start\":42405},{\"end\":42424,\"start\":42420},{\"end\":42440,\"start\":42430},{\"end\":42647,\"start\":42640},{\"end\":42654,\"start\":42651},{\"end\":42661,\"start\":42658},{\"end\":42673,\"start\":42665},{\"end\":42683,\"start\":42679},{\"end\":42695,\"start\":42687},{\"end\":42704,\"start\":42699},{\"end\":42717,\"start\":42708},{\"end\":42731,\"start\":42721},{\"end\":43143,\"start\":43140},{\"end\":43151,\"start\":43147},{\"end\":43160,\"start\":43157},{\"end\":43166,\"start\":43164},{\"end\":43176,\"start\":43172},{\"end\":43184,\"start\":43180},{\"end\":43191,\"start\":43188},{\"end\":43200,\"start\":43197},{\"end\":43209,\"start\":43204},{\"end\":43545,\"start\":43533},{\"end\":43559,\"start\":43549},{\"end\":43568,\"start\":43563},{\"end\":43582,\"start\":43572},{\"end\":43963,\"start\":43959},{\"end\":43972,\"start\":43967},{\"end\":43982,\"start\":43976},{\"end\":43996,\"start\":43988},{\"end\":44003,\"start\":44000},{\"end\":44212,\"start\":44205},{\"end\":44223,\"start\":44216},{\"end\":44234,\"start\":44227},{\"end\":44690,\"start\":44685},{\"end\":44701,\"start\":44694},{\"end\":44709,\"start\":44705},{\"end\":45069,\"start\":45064},{\"end\":45078,\"start\":45073},{\"end\":45520,\"start\":45516},{\"end\":45527,\"start\":45524},{\"end\":45534,\"start\":45531},{\"end\":45540,\"start\":45538},{\"end\":45808,\"start\":45804},{\"end\":45819,\"start\":45812},{\"end\":46171,\"start\":46169},{\"end\":46182,\"start\":46175},{\"end\":46497,\"start\":46493},{\"end\":46503,\"start\":46501},{\"end\":46510,\"start\":46507},{\"end\":46517,\"start\":46514},{\"end\":46526,\"start\":46521},{\"end\":46852,\"start\":46847},{\"end\":46858,\"start\":46856},{\"end\":46866,\"start\":46862},{\"end\":46874,\"start\":46870},{\"end\":47223,\"start\":47218},{\"end\":47231,\"start\":47227},{\"end\":47239,\"start\":47235},{\"end\":47246,\"start\":47243},{\"end\":47252,\"start\":47250},{\"end\":47699,\"start\":47694},{\"end\":47706,\"start\":47703},{\"end\":47714,\"start\":47710},{\"end\":47722,\"start\":47718}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1805.11123\",\"id\":\"b0\"},\"end\":29468,\"start\":29156},{\"attributes\":{\"doi\":\"10.1145/1553374.1553380\",\"id\":\"b1\",\"matched_paper_id\":873046},\"end\":30111,\"start\":29470},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":697405},\"end\":30529,\"start\":30113},{\"attributes\":{\"id\":\"b3\"},\"end\":30754,\"start\":30531},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9059102},\"end\":31152,\"start\":30756},{\"attributes\":{\"id\":\"b5\"},\"end\":31341,\"start\":31154},{\"attributes\":{\"doi\":\"21.1-21.11\",\"id\":\"b6\",\"matched_paper_id\":1910869},\"end\":31713,\"start\":31343},{\"attributes\":{\"id\":\"b7\"},\"end\":31908,\"start\":31715},{\"attributes\":{\"doi\":\"10.1109/ACCESS.2018.2815149\",\"id\":\"b8\",\"matched_paper_id\":5039973},\"end\":32275,\"start\":31910},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206590483},\"end\":32652,\"start\":32277},{\"attributes\":{\"doi\":\"10.1109/CVPR.2005.177\",\"id\":\"b10\"},\"end\":32714,\"start\":32654},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18502218},\"end\":32987,\"start\":32716},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2009.167\",\"id\":\"b12\",\"matched_paper_id\":3198903},\"end\":33460,\"start\":32989},{\"attributes\":{\"id\":\"b13\"},\"end\":33711,\"start\":33462},{\"attributes\":{\"id\":\"b14\"},\"end\":34017,\"start\":33713},{\"attributes\":{\"doi\":\"ArXiv abs/1808.01097\",\"id\":\"b15\"},\"end\":34348,\"start\":34019},{\"attributes\":{\"doi\":\"ArXiv abs/1904.03626\",\"id\":\"b16\"},\"end\":34575,\"start\":34350},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11908837},\"end\":34942,\"start\":34577},{\"attributes\":{\"doi\":\"ArXiv abs/1602.07360\",\"id\":\"b18\"},\"end\":35301,\"start\":34944},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9749221},\"end\":35669,\"start\":35303},{\"attributes\":{\"doi\":\"ArXiv abs/1808.01050\",\"id\":\"b20\"},\"end\":36060,\"start\":35671},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":207216972},\"end\":36502,\"start\":36062},{\"attributes\":{\"id\":\"b22\"},\"end\":36798,\"start\":36504},{\"attributes\":{\"id\":\"b23\"},\"end\":36960,\"start\":36800},{\"attributes\":{\"doi\":\"ArXiv abs/2209.07271\",\"id\":\"b24\"},\"end\":37233,\"start\":36962},{\"attributes\":{\"doi\":\"10.1109/ICPR.2008.4761705\",\"id\":\"b25\",\"matched_paper_id\":5157313},\"end\":37738,\"start\":37235},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":220647241},\"end\":38073,\"start\":37740},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3645757},\"end\":38467,\"start\":38075},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2009.204\",\"id\":\"b28\",\"matched_paper_id\":17682066},\"end\":38896,\"start\":38469},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53783843},\"end\":39185,\"start\":38898},{\"attributes\":{\"id\":\"b30\"},\"end\":39447,\"start\":39187},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":229644676},\"end\":39983,\"start\":39449},{\"attributes\":{\"doi\":\"ArXiv abs/1903.09848\",\"id\":\"b32\"},\"end\":40300,\"start\":39985},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":21697434},\"end\":40701,\"start\":40302},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1089358},\"end\":41135,\"start\":40703},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":14124313},\"end\":41606,\"start\":41137},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3003101},\"end\":42064,\"start\":41608},{\"attributes\":{\"id\":\"b37\"},\"end\":42331,\"start\":42066},{\"attributes\":{\"doi\":\"CoRR abs/1412.6806\",\"id\":\"b38\"},\"end\":42604,\"start\":42333},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206592484},\"end\":43029,\"start\":42606},{\"attributes\":{\"id\":\"b40\"},\"end\":43426,\"start\":43031},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":212675106},\"end\":43893,\"start\":43428},{\"attributes\":{\"id\":\"b42\"},\"end\":44144,\"start\":43895},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":7454625},\"end\":44546,\"start\":44146},{\"attributes\":{\"doi\":\"10.1109/AVSS.2014.6918687\",\"id\":\"b44\"},\"end\":44616,\"start\":44548},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2008.75\",\"id\":\"b45\",\"matched_paper_id\":7817555},\"end\":45027,\"start\":44618},{\"attributes\":{\"doi\":\"10.1109/ICCV.2001.937709\",\"id\":\"b46\",\"matched_paper_id\":2796017},\"end\":45458,\"start\":45029},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":226962392},\"end\":45721,\"start\":45460},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":220961550},\"end\":46046,\"start\":45723},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":2536395},\"end\":46427,\"start\":46048},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":206814921},\"end\":46776,\"start\":46429},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":2131202},\"end\":47139,\"start\":46778},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4545310},\"end\":47546,\"start\":47141},{\"attributes\":{\"doi\":\"10.1109/CVPR.2016.70\",\"id\":\"b53\"},\"end\":47606,\"start\":47548},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":211191810},\"end\":47897,\"start\":47608}]", "bib_title": "[{\"end\":29489,\"start\":29470},{\"end\":30176,\"start\":30113},{\"end\":30842,\"start\":30756},{\"end\":31386,\"start\":31343},{\"end\":32005,\"start\":31910},{\"end\":32329,\"start\":32277},{\"end\":32755,\"start\":32716},{\"end\":33053,\"start\":32989},{\"end\":34655,\"start\":34577},{\"end\":35368,\"start\":35303},{\"end\":36137,\"start\":36062},{\"end\":37349,\"start\":37235},{\"end\":37791,\"start\":37740},{\"end\":38166,\"start\":38075},{\"end\":38553,\"start\":38469},{\"end\":38926,\"start\":38898},{\"end\":39511,\"start\":39449},{\"end\":40397,\"start\":40302},{\"end\":40760,\"start\":40703},{\"end\":41203,\"start\":41137},{\"end\":41708,\"start\":41608},{\"end\":42636,\"start\":42606},{\"end\":43529,\"start\":43428},{\"end\":44199,\"start\":44146},{\"end\":44681,\"start\":44618},{\"end\":45060,\"start\":45029},{\"end\":45512,\"start\":45460},{\"end\":45800,\"start\":45723},{\"end\":46165,\"start\":46048},{\"end\":46489,\"start\":46429},{\"end\":46843,\"start\":46778},{\"end\":47214,\"start\":47141},{\"end\":47690,\"start\":47608}]", "bib_author": "[{\"end\":29264,\"start\":29256},{\"end\":29276,\"start\":29264},{\"end\":29501,\"start\":29491},{\"end\":29514,\"start\":29501},{\"end\":29527,\"start\":29514},{\"end\":29537,\"start\":29527},{\"end\":30193,\"start\":30178},{\"end\":30212,\"start\":30193},{\"end\":30222,\"start\":30212},{\"end\":30607,\"start\":30600},{\"end\":30615,\"start\":30607},{\"end\":30623,\"start\":30615},{\"end\":30629,\"start\":30623},{\"end\":30854,\"start\":30844},{\"end\":30867,\"start\":30854},{\"end\":30882,\"start\":30867},{\"end\":31207,\"start\":31199},{\"end\":31216,\"start\":31207},{\"end\":31224,\"start\":31216},{\"end\":31233,\"start\":31224},{\"end\":31396,\"start\":31388},{\"end\":31405,\"start\":31396},{\"end\":31413,\"start\":31405},{\"end\":31422,\"start\":31413},{\"end\":31759,\"start\":31751},{\"end\":31768,\"start\":31759},{\"end\":31776,\"start\":31768},{\"end\":31784,\"start\":31776},{\"end\":31790,\"start\":31784},{\"end\":31796,\"start\":31790},{\"end\":32014,\"start\":32007},{\"end\":32021,\"start\":32014},{\"end\":32029,\"start\":32021},{\"end\":32340,\"start\":32331},{\"end\":32350,\"start\":32340},{\"end\":32769,\"start\":32757},{\"end\":32778,\"start\":32769},{\"end\":32792,\"start\":32778},{\"end\":33073,\"start\":33055},{\"end\":33087,\"start\":33073},{\"end\":33101,\"start\":33087},{\"end\":33112,\"start\":33101},{\"end\":33532,\"start\":33520},{\"end\":33540,\"start\":33532},{\"end\":33553,\"start\":33540},{\"end\":33562,\"start\":33553},{\"end\":33572,\"start\":33562},{\"end\":33778,\"start\":33753},{\"end\":33795,\"start\":33778},{\"end\":33813,\"start\":33795},{\"end\":33833,\"start\":33813},{\"end\":33848,\"start\":33833},{\"end\":34097,\"start\":34090},{\"end\":34106,\"start\":34097},{\"end\":34115,\"start\":34106},{\"end\":34125,\"start\":34115},{\"end\":34133,\"start\":34125},{\"end\":34144,\"start\":34133},{\"end\":34153,\"start\":34144},{\"end\":34424,\"start\":34413},{\"end\":34437,\"start\":34424},{\"end\":34668,\"start\":34657},{\"end\":34677,\"start\":34668},{\"end\":34686,\"start\":34677},{\"end\":35039,\"start\":35026},{\"end\":35054,\"start\":35039},{\"end\":35064,\"start\":35054},{\"end\":35071,\"start\":35064},{\"end\":35082,\"start\":35071},{\"end\":35093,\"start\":35082},{\"end\":35380,\"start\":35370},{\"end\":35391,\"start\":35380},{\"end\":35402,\"start\":35391},{\"end\":35410,\"start\":35402},{\"end\":35769,\"start\":35759},{\"end\":35779,\"start\":35769},{\"end\":35789,\"start\":35779},{\"end\":35798,\"start\":35789},{\"end\":35814,\"start\":35798},{\"end\":35827,\"start\":35814},{\"end\":35835,\"start\":35827},{\"end\":36148,\"start\":36139},{\"end\":36156,\"start\":36148},{\"end\":36168,\"start\":36156},{\"end\":36181,\"start\":36168},{\"end\":36588,\"start\":36579},{\"end\":36596,\"start\":36588},{\"end\":36605,\"start\":36596},{\"end\":36613,\"start\":36605},{\"end\":36620,\"start\":36613},{\"end\":36634,\"start\":36620},{\"end\":36642,\"start\":36634},{\"end\":37050,\"start\":37040},{\"end\":37061,\"start\":37050},{\"end\":37071,\"start\":37061},{\"end\":37357,\"start\":37351},{\"end\":37366,\"start\":37357},{\"end\":37375,\"start\":37366},{\"end\":37382,\"start\":37375},{\"end\":37799,\"start\":37793},{\"end\":37806,\"start\":37799},{\"end\":37814,\"start\":37806},{\"end\":37822,\"start\":37814},{\"end\":37830,\"start\":37822},{\"end\":38174,\"start\":38168},{\"end\":38183,\"start\":38174},{\"end\":38191,\"start\":38183},{\"end\":38562,\"start\":38555},{\"end\":38573,\"start\":38562},{\"end\":38935,\"start\":38928},{\"end\":38947,\"start\":38935},{\"end\":38956,\"start\":38947},{\"end\":39286,\"start\":39274},{\"end\":39297,\"start\":39286},{\"end\":39306,\"start\":39297},{\"end\":39521,\"start\":39513},{\"end\":39527,\"start\":39521},{\"end\":39534,\"start\":39527},{\"end\":40068,\"start\":40053},{\"end\":40079,\"start\":40068},{\"end\":40089,\"start\":40079},{\"end\":40099,\"start\":40089},{\"end\":40113,\"start\":40099},{\"end\":40406,\"start\":40399},{\"end\":40414,\"start\":40406},{\"end\":40420,\"start\":40414},{\"end\":40428,\"start\":40420},{\"end\":40769,\"start\":40762},{\"end\":40778,\"start\":40769},{\"end\":40786,\"start\":40778},{\"end\":41217,\"start\":41205},{\"end\":41230,\"start\":41217},{\"end\":41723,\"start\":41710},{\"end\":41734,\"start\":41723},{\"end\":42132,\"start\":42124},{\"end\":42140,\"start\":42132},{\"end\":42148,\"start\":42140},{\"end\":42155,\"start\":42148},{\"end\":42163,\"start\":42155},{\"end\":42169,\"start\":42163},{\"end\":42175,\"start\":42169},{\"end\":42181,\"start\":42175},{\"end\":42403,\"start\":42385},{\"end\":42418,\"start\":42403},{\"end\":42426,\"start\":42418},{\"end\":42442,\"start\":42426},{\"end\":42649,\"start\":42638},{\"end\":42656,\"start\":42649},{\"end\":42663,\"start\":42656},{\"end\":42675,\"start\":42663},{\"end\":42685,\"start\":42675},{\"end\":42697,\"start\":42685},{\"end\":42706,\"start\":42697},{\"end\":42719,\"start\":42706},{\"end\":42733,\"start\":42719},{\"end\":43145,\"start\":43138},{\"end\":43153,\"start\":43145},{\"end\":43162,\"start\":43153},{\"end\":43168,\"start\":43162},{\"end\":43178,\"start\":43168},{\"end\":43186,\"start\":43178},{\"end\":43193,\"start\":43186},{\"end\":43202,\"start\":43193},{\"end\":43211,\"start\":43202},{\"end\":43547,\"start\":43531},{\"end\":43561,\"start\":43547},{\"end\":43570,\"start\":43561},{\"end\":43584,\"start\":43570},{\"end\":43965,\"start\":43957},{\"end\":43974,\"start\":43965},{\"end\":43984,\"start\":43974},{\"end\":43998,\"start\":43984},{\"end\":44005,\"start\":43998},{\"end\":44214,\"start\":44201},{\"end\":44225,\"start\":44214},{\"end\":44236,\"start\":44225},{\"end\":44692,\"start\":44683},{\"end\":44703,\"start\":44692},{\"end\":44711,\"start\":44703},{\"end\":45071,\"start\":45062},{\"end\":45080,\"start\":45071},{\"end\":45522,\"start\":45514},{\"end\":45529,\"start\":45522},{\"end\":45536,\"start\":45529},{\"end\":45542,\"start\":45536},{\"end\":45810,\"start\":45802},{\"end\":45821,\"start\":45810},{\"end\":46173,\"start\":46167},{\"end\":46184,\"start\":46173},{\"end\":46499,\"start\":46491},{\"end\":46505,\"start\":46499},{\"end\":46512,\"start\":46505},{\"end\":46519,\"start\":46512},{\"end\":46528,\"start\":46519},{\"end\":46854,\"start\":46845},{\"end\":46860,\"start\":46854},{\"end\":46868,\"start\":46860},{\"end\":46876,\"start\":46868},{\"end\":47225,\"start\":47216},{\"end\":47233,\"start\":47225},{\"end\":47241,\"start\":47233},{\"end\":47248,\"start\":47241},{\"end\":47254,\"start\":47248},{\"end\":47701,\"start\":47692},{\"end\":47708,\"start\":47701},{\"end\":47716,\"start\":47708},{\"end\":47724,\"start\":47716}]", "bib_venue": "[{\"end\":29254,\"start\":29156},{\"end\":29655,\"start\":29560},{\"end\":30288,\"start\":30222},{\"end\":30598,\"start\":30531},{\"end\":30940,\"start\":30882},{\"end\":31197,\"start\":31154},{\"end\":31484,\"start\":31432},{\"end\":31749,\"start\":31715},{\"end\":32067,\"start\":32056},{\"end\":32440,\"start\":32350},{\"end\":32839,\"start\":32792},{\"end\":33196,\"start\":33134},{\"end\":33518,\"start\":33462},{\"end\":33751,\"start\":33713},{\"end\":34088,\"start\":34019},{\"end\":34411,\"start\":34350},{\"end\":34740,\"start\":34686},{\"end\":35024,\"start\":34944},{\"end\":35468,\"start\":35410},{\"end\":35757,\"start\":35671},{\"end\":36247,\"start\":36181},{\"end\":36577,\"start\":36504},{\"end\":36868,\"start\":36800},{\"end\":37038,\"start\":36962},{\"end\":37459,\"start\":37407},{\"end\":37888,\"start\":37830},{\"end\":38253,\"start\":38191},{\"end\":38657,\"start\":38595},{\"end\":39024,\"start\":38956},{\"end\":39272,\"start\":39187},{\"end\":39602,\"start\":39534},{\"end\":40051,\"start\":39985},{\"end\":40485,\"start\":40428},{\"end\":40856,\"start\":40786},{\"end\":41286,\"start\":41230},{\"end\":41823,\"start\":41734},{\"end\":42122,\"start\":42066},{\"end\":42383,\"start\":42333},{\"end\":42797,\"start\":42733},{\"end\":43136,\"start\":43031},{\"end\":43642,\"start\":43584},{\"end\":43955,\"start\":43895},{\"end\":44325,\"start\":44236},{\"end\":44794,\"start\":44732},{\"end\":45182,\"start\":45104},{\"end\":45574,\"start\":45542},{\"end\":45876,\"start\":45821},{\"end\":46224,\"start\":46184},{\"end\":46583,\"start\":46528},{\"end\":46940,\"start\":46876},{\"end\":47324,\"start\":47254},{\"end\":47731,\"start\":47724},{\"end\":29754,\"start\":29657},{\"end\":30341,\"start\":30290},{\"end\":31523,\"start\":31486},{\"end\":36300,\"start\":36249},{\"end\":39636,\"start\":39604},{\"end\":40879,\"start\":40858},{\"end\":41306,\"start\":41288},{\"end\":45250,\"start\":45184}]"}}}, "year": 2023, "month": 12, "day": 17}