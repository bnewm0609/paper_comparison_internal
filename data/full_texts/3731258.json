{"id": 3731258, "updated": "2023-11-11 03:02:23.27", "metadata": {"title": "Learning Monocular Visual Odometry with Dense 3D Mapping from Dense 3D Flow", "authors": "[{\"first\":\"Cheng\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Pulak\",\"last\":\"Purkait\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Duckett\",\"middle\":[]},{\"first\":\"Rustam\",\"last\":\"Stolkin\",\"middle\":[]}]", "venue": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "journal": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "This paper introduces a fully deep learning approach to monocular SLAM, which can perform simultaneous localization using a neural network for learning visual odometry (L-VO) and dense 3D mapping. Dense 2D flow and a depth image are generated from monocular images by sub-networks, which are then used by a 3D flow associated layer in the L-VO network to generate dense 3D flow. Given this 3D flow, the dual-stream L-VO network can then predict the 6DOF relative pose and furthermore reconstruct the vehicle trajectory. In order to learn the correlation between motion directions, the Bivariate Gaussian modeling is employed in the loss function. The L-VO network achieves an overall performance of 2.68 % for average translational error and 0.0143\u00b0/m for average rotational error on the KITTI odometry benchmark. Moreover, the learned depth is leveraged to generate a dense 3D map. As a result, an entire visual SLAM system, that is, learning monocular odometry combined with dense 3D mapping, is achieved.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2962891637", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iros/ZhaoSPDS18", "doi": "10.1109/iros.2018.8594151"}}, "content": {"source": {"pdf_hash": "53c363097b48c2a5f39e90b1f02936c555315ec0", "pdf_src": "ArXiv", "pdf_uri": "[\"https://arxiv.org/pdf/1803.02286v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/1803.02286", "status": "GREEN"}}, "grobid": {"id": "68505e080818baecbf59ba5c2992802117734b66", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/53c363097b48c2a5f39e90b1f02936c555315ec0.txt", "contents": "\nLearning monocular visual odometry with dense 3D mapping from dense 3D flow\n25 Jul 2018\n\nCheng Zhao \nSun Li \nPulak Purkait \nTom Duckett \nRustam Stolkin \nLearning monocular visual odometry with dense 3D mapping from dense 3D flow\n25 Jul 2018EE3D415C6C85AB5950203B18A6B4ECD3arXiv:1803.02286v2[cs.RO]\nThis paper introduces a fully deep learning approach to monocular SLAM, which can perform simultaneous localization using a neural network for learning visual odometry (L-VO) and dense 3D mapping.Dense 2D flow and a depth image are generated from monocular images by sub-networks, which are then used by a 3D flow associated layer in the L-VO network to generate dense 3D flow.Given this 3D flow, the dualstream L-VO network can then predict the 6DOF relative pose and furthermore reconstruct the vehicle trajectory.In order to learn the correlation between motion directions, the Bivariate Gaussian modeling is employed in the loss function.The L-VO network achieves an overall performance of 2.68% for average translational error and 0.0143 \u2022 /m for average rotational error on the KITTI odometry benchmark.Moreover, the learned depth is leveraged to generate a dense 3D map.As a result, an entire visual SLAM system, that is, learning monocular odometry combined with dense 3D mapping, is achieved.\n\nI. INTRODUCTION\n\nSimultaneous localization and mapping (SLAM) is an essential technique for mobile robot applications.In the past few decades, a substantial amount of research has been devoted to visual SLAM systems that enable robots to localize robustly and accurately in different environments.One of the most challenging branches of visual SLAM is monocular SLAM, which often suffers critically from absolute scale drift.Usually, some prior knowledge such as the height of the camera is necessary to alleviate scale drift.Moreover, these methods require hand-coded engineering efforts and excellent parameter tuning skills.\n\nIn recent years, deep learning techniques for visual odometry and SLAM have attracted considerable attention in the SLAM community.These methods not only provide good performance in challenging environments but also rectify the incorrect scale estimation of monocular SLAM.Supervised learning approaches formulate visual odometry (VO) as a regression problem.They explore the ability of CNN [1] or RNN [2][3] to learn ego-motion estimation using the change of RGB value features [4], deep flow [5] and nondeep flow [6] features.These methods are calibration-free but require a lot of expensive ground truth data for training.\n\nOn the other hand, some networks for predicting VO take advantage of geometric constraints, e.g.similarity constraints, epipolar constraints, etc., by integrating them into the loss function and training the network in an unsupervised manner.Although the trajectory ground truth is not required Fig. 1: The pipeline of the proposed learning monocular SLAM system.More detail can be found in Sec.III-A.\n\nfor these methods, consecutive frames [7][8] [9] or stereo image pairs [10] along with the above geometric constraints are enough to provide sufficient supervision to train the network.However, these methods usually require the intrinsic parameters of the camera.\n\nThe main limitation of the above methods is that they all suffer from high dataset bias and require domain similarity between the training and testing sequences.Moreover, most of the deep learning geometry research only focus on visual odometry for localization without mapping.CNN-SLAM [11] is the forerunner to integrate learning of depth prediction with monocular SLAM to generate an accurate dense 3D map.But the odometry in CNN-SLAM is still based on the conventional method.Therefore, it is still not a pure deep learning SLAM method.In addition, some researches [12][13][14] [15] integrate deep semantic information into a conventional SLAM system.\n\nIn this paper, a learning system for monocular SLAM is developed, which can simultaneously perform localization and dense 3D mapping through an end-to-end neural network.A learning visual odometry (L-VO) network with a 3D association layer is proposed for ego-motion estimation, which achieves an overall performance of 2.68% for average translational error and 0.0143 \u2022 /m for average rotational error on the KITTI1 odometry benchmark.The main contributions can be briefly summarized as follows: i) A new baseline L-VO method with a 3D association layer is proposed for ego-motion estimation, ii) a Bivariate Gaussian loss function is used to learn the correlation between motion directions, iii) L-VO is extended to a learning monocular SLAM system.An overview of the proposed architecture is shown in Fig. 1.\n\n\nII. RELATED WORK\n\nA. Learning based visual odometry (Pre-deep learning era)\n\nIn the recent past, some learning-based visual odometry estimation methods [16][17] [18][19] [20] were explored, before deep learning began to dominate many computer vision and robotics tasks.These learning-based methods mainly explored different pre-deep learning methods such as SVM, Gaussian Processes, etc. using sparse optical flow features for camera localisation and motion estimation.\n\n\nB. Supervised deep learning for visual odometry\n\nOne of the pioneering works on deep learning for visual odometry estimation was proposed by Costante et al. [6].They employed convolutional neural networks (CNNs) for ego-motion estimation from dense optical flow obtained by a non-deep method [21].Then, Muller et al. [5] proposed Flowdometry, which combines FlowNet [22] and CNNs to obtain an end-to-end odometry system.Gabriele et al. [1] proposed Latent Space Visual Odometry (LS-VO) to find a non-linear representation of the optical flow manifold.\n\nTuomas et al. [4] explored LSTM for visual odometry.They utilized CNNs on the temporal change of RGB values (temporal derivatives) between two adjacent images.They utilized LSTM as a baseline in their work and proposed a back-propagation method for a Kalman Filter to learn the discriminative deterministic state estimators.Another seminal work on learning visual odometry was proposed by Wang et al. [2] [23].They utilized FlowNet features with LSTM for an end-to-end visual odometry system.Clark et.al [3] used the same network but fused the features of the monocular RGB camera with additional IMU readings for improved performance.Mehmet et al. [24] adopted a similar architecture -CNNs with LSTM -to develop a visual odometry system for endoscopic capsule robots.\n\n\nC. Unsupervised deep learning for visual odometry\n\nMost of the unsupervised visual odometry estimation methods predict the depth and ego-motion simultaneously.These methods do not require the trajectory ground truth but need camera parameters and often some additional information such as stereo images for training.Benjamin et al. [8] proposed the DeMoN architecture, which estimates not only depth and motion but also the surface normals and optical flow from a pair of images.They employed an unsupervised training loss function based on the relative spatial differences.Zhou et al. [7] also used a training loss function which minimizes image warping error of an image sequence for unsupervised depth prediction and egomotion estimation.SfM-Net [9] predicts depth, segmentation, camera and rigid object motions, and transforms these to obtain frame-to-frame dense optical flow.Li et al. [10] combined the loss functions from [7] and [25] to obtain an unsupervised visual odometry method that can recover the absolute scale.\n\n\nD. Learning visual SLAM\n\nMost of the deep learning geometry research only focuses on VO for localization without mapping.The only forerunner of deep learning SLAM, CNN-SLAM [11], integrates CNNstyle depth prediction with monocular SLAM to recover the absolute scale, and meanwhile generates a dense 3D map.However the odometry in CNN-SLAM is still based on the conventional method.As the estimated odometry of CNN-SLAM is not based on learning methods, it is not a complete end-to-end approach for learning SLAM.\n\n\nE. Discussion\n\nConventional monocular visual odometry suffers from scale drift.Pioneering researchers [5][23][3] [1] show that this problem can be mitigated via learning from 2D flow features.Inspired by RGBD-SLAM, the relative transform can be estimated directly from solving the PnP problem when the depth is given.In this paper, we model the visual odometry problem as a probabilistic regression problem.Multi-modal features, i.e. 3D flow (derived from the 2D flow and depth flow), are used to enhance the observation of the learning visual odometry.We further explore the correlation of motion directions and learn the translation with a multi-variate Gaussian rather than isotropic Gaussian [2].Moreover, the learned depth is leveraged to generate a dense 3D map.As a result, an entire visual SLAM system, that is, learning monocular odometry combined with dense 3D mapping, is achieved.\n\n\nIII. METHODOLOGY A. Overview\n\nThe pipeline of the proposed learning monocular SLAM is shown in Fig. 1.The proposed L-VO network is an endto-end neural network for simultaneous monocular visual odometry and dense 3D mapping.To be more specific, L-VO Net takes a pair of consecutive images as input and predicts Ego-motion.The dense 2D flow and depth are obtained with FlowNet2 [26] and DepthNet [25] respectively.The estimated dense 2D flow and depth are further associated to obtain the 3D flow.Next, the 3D flow is fed into two separate regressors to predict the 6DOF relative pose (including scale) transform between each pair of images.As a consequence, the 6DOF camera trajectory can be obtained by accumulating relative poses.The point cloud is simultaneously generated and mapped incrementally from the given RGB image and the estimated depth.Furthermore, a 3D refinement is employed to remove the outliers and incorrect predictions.Finally, a dense 3D map is generated.\n\n\nB. 2D optical flow and depth prediction\n\nFor dense 2D optical flow prediction, the state-of-theart approach FlowNet2 [26] is employed.FlowNet2 is a stacked architecture composed of a series of FlowNet-S [22], FlowNet-C [22] and FlowNet-SD [26].It can deliver robust 2D dense optical flow, which is of significant importance for learning odometry.We fine-tune this network using the training KITTI data (as described in IV-C) and then transplant the network for our task.\n\nFor depth prediction, any of the state-of-the-art methods [8], [7] and [25] can be adapted to our approach.In this paper, [25] is employed because of its good performance in outdoor scenes.[25] is an encoder-decoder architecture with appearance matching loss, disparity smoothness loss and left-right disparity consistency loss, which can be trained in unsupervised fashion.The training objective enables the network to perform the depth estimation from a monocular image.The network is also fine-tuned using the training KITTI data (as described in IV-C).\n\n\nC. 3D flow association layer\n\nWe propose a 3D flow association layer which generates dense 3D flow from 2D flow and the corresponding depth maps.Assuming F k:k+1 XY \u2208 R h\u00d7w\u00d72 is the predicted dense 2D flow (on X-Y image plane) between frame k and k + 1, and D k \u2208 R h\u00d7w is the predicted depth map of frame k, the 3D flow association layer can be defined as:\nF k:k+1 Z (x, y) = D k+1 (x, y) + F k:k+1 XY (x, y) \u2212 D k (x, y)(1)F k:k+1 3D = C(F k:k+1 XY , F k:k+1 Z )(2)\nwhere F k:k+1 3D (x, y) \u2208 R 3 refers to the 3D flow at pixel coordinate (x, y) and C is the concatenation operation.If the depth value in frame k + 1 cannot be associated with the corresponding depth value in frame k, the missing flow pixels between two adjacent frames can be interpolated through bilinear filtering.It is worth noting that the inverse depth (i.e.disparity) is more sensitive to the motion of surroundings and objects close to the camera.Hence, the inverse depth is used instead of the depth value in our approach.We still use the term \"depth\" in order to make the following description more readable.\n\n\nD. Learning odometry\n\nAs shown in Fig. 2, our learning odometry network is a dual stream architecture network, composed of two branches of convolution stacks followed by a squeeze layer [27] and two fully connected regressors.The convolution layers are composed of 3 \u00d7 3 filters and are of stride 2. The numbers of channels in the two branches are 64, 128, 256 and 512.In order to keep the spatial geometry information, the pooling layer is abandoned in these two CNN stacks.In the end, the feature maps of the two branches are concatenated together and squeezed using a 1 \u00d7 1 filter:\nF 3D = S(F XY , F Z ) (3)\nwhere S is the squeeze operation, F XY \u2208 R h\u00d7w\u00d7n and F Z \u2208 R h\u00d7w\u00d7n are the feature maps of 2D flow and depth flow respectively, F 3D \u2208 R h\u00d7w\u00d7n/4 is the squeezed feature, and n is the number of feature channels.The squeeze layer embeds the 3D feature map into a lower dimensional space, thereby reducing the input dimension of the regressors.A triple-layer fully-connected network is used for regression.We set the hidden layers of the regressors to size 128 with relu activation function.The output of the translation regressor is 6 for bivariate Gaussian loss and that of the rotation regressor is 3, which is trained through a 2 loss.The details of the loss function are described as follows.\n\n\nE. Bivariate Gaussian loss function\n\nFor most of the outdoor on-road driving data, e.g.KITTI dataset, there is a strong correlation between the translations along different axes in the horizontal plane.In contrast with the previous loss functions used in learning odometry, we aim to let our network learn the correlation along the forward and left/right translation directions.In this paper, this correlation is modeled as a multivariate Gaussian distribution.\n\nThe same camera configuration (axes definitions) as in the KITTI dataset is used, i.e. x : right (horizontal), y : down (vertical), z : forward (horizontal), then the translation variation along y coordinate is small compared to the other axes.Therefore, we only need to find the correlations between translation x and translation z.In our approach, the Bivariate Gaussian Probabilistic-Density-Function (P DF ) [28] is employed as the likelihood function for x (left/right) and z (forward) translation prediction.For the translation in y direction and orientations, 2 loss is used for optimization.Similar to [29], the Euler angles rather than quaternion are used to represent the orientation, as the quaternion representation opens up the possibility of over-fitting in the rotation regression.We further include a 2 regularization term for all weights to mitigate over-fitting.Our loss function is defined as:\nloss = N i \u2212 log P DF ((x i gt , z i gt ), N i (\u00b5, \u03a3) +\u03bb 1 N i y i p \u2212 y i gt 2 + \u03bb 2 N i r i p \u2212 r i gt 2 + \u03bb 3 W 2 (4)\nwhere N is the number of training pair images, (x i gt , y i gt , z i gt ) is the ground truth camera translation, and (x i p , y i p , z i p ) is the predicted translation of the i th image/camera.r i p := (e z p , e y p , e x p ) i and r i gt := (e z gt , e y gt , e x gt ) i are the predicted and ground-truth Euler angles, respectively.W are the trainable weights of the neural network.\u03bb 1 , \u03bb 2 and \u03bb 3 are the scale factors to balance the weights of translation and orientations.The Gaussian Density Function P DF is defined as:\nP DF (x i gt , z i gt ), N i (\u00b5, \u03a3) = exp(\u2212 1 2 ((x i gt , z i gt ) \u2212 \u00b5)\u03a3 \u22121 ((x i gt , z i gt ) \u2212 \u00b5) T ) ((2\u03c0) 2 |\u03a3|) \u22121/2 (5)\nwhere the bivariate Gaussian distribution N is:\n\u00b5 = (\u00b5x, \u00b5z) i , \u03a3 = \u03c3 2 x \u03c1\u03c3x\u03c3z \u03c1\u03c3x\u03c3z \u03c3 2 z , i .(6)\nwhere \u00b5 x and \u00b5 z are two mean variables in the left/right and forward direction, \u03c3 x , \u03c3 z are the corresponding standard deviations and \u03c1 is the correlation coefficient of the translation between left/right and forward direction in the horizontal plane.Our neural network is expected to learn (\u00b5 x , \u00b5 z , \u03c3 x , \u03c3 z , \u03c1, y p ), and (e z p , e y p , e x p ), corresponding to the 6-dimensional and 3-dimensional outputs of two regression neural networks.\n\nOnce the network is trained, i.e. the translation (\u00b5 x , \u00b5 z , \u03c3 x , \u03c3 z , \u03c1, y p ) and rotation (e z p , e y p , e x p ) can be estimated from the network, the predicted translation in the horizontal plane is obtained through sampling within the bivariant Gaussian distribution using:\nx, z = 1 N s Ns k (x s , z s ) k \u223c N p (\u00b5, \u03a3),(7)\nwhere N p is obtained from (\u00b5 x , \u00b5 z , \u03c3 x , \u03c3 z , \u03c1), (x s , z s ) k is the kth sample, and N s is the number of samples.\n\n\nF. Octree depth fusion for mapping\n\nWe also proposed a dense 3D mapping method using the learned odometry and depth.Given the RGB image and the corresponding predicted depth image, the 3D point cloud (X, Y, Z) can be obtained through:\nd u,v \uf8ee \uf8f0 u v 1 \uf8f9 \uf8fb = \uf8ee \uf8f0 f x s c x 0 f y c y 0 0 1 \uf8f9 \uf8fb \uf8ee \uf8f0 X Y Z \uf8f9 \uf8fb (8)\nwhere f x , f y are the focal lengths, (c x , c y ) is the principal point offset and s is the axis skew.(u, v) is the pixel position in the image plane.\n\nUnfortunately, the depth prediction usually suffers from blur around the depth borders.The predicted depth is not accurate enough to be utilized directly for 3D mapping.In our approach, the OctoMap representation [30] is used to refine and maintain the 3D map.In order to build a robust, accurate dense 3D map, depth fusion using measurements from multiple views is employed.In OctoMap, each leaf node n stores the occupancy probability P (n|o 1:t ).Given the 3D point measurements o 1:t , the probability P (n|o 1:t ) can be updated as:\nP (n|o1:t) = 1 + 1 \u2212 P (n|ot) P (n|ot)\n1 \u2212 P (n|o1:t\u22121) P (n|o1:t\u22121)\nP (n) 1 \u2212 P (n) \u22121 (9)\nhere, P (n|o t ) can be obtained by a beam tracing sensor model.If the probability P (n|o 1:t ) of the leaf node is beyond a threshold, this node will be marked as occupied in the dense 3D map.This probabilistic occupancy fusion can fuse the depth estimations from multiple views, and remove points arising from inaccurate depth predictions.\n\n\nIV. EXPERIMENTS A. Dataset\n\nThe proposed L-VO Net is evaluated on the most popular KITTI VO/SLAM benchmark.The KITTI VO/SLAM benchmark consists of 22 sequences saved in PNG format.Sequences 00 \u2212 10 provide the sensor data with the accurate ground truth (< 10cm) from a GPS/IMU system, while sequences 11 \u2212 21 only provide the raw sensor data.The large number of dynamic objects such as cars means that visual odometry could easily fail on this challenging dataset.\n\n\nB. Network training and testing\n\nThe network is trained with Adam optimization.The batch size is set to 100, the momentum is fixed to (0.9, 0.999), and the starting learning rate is 0.0001.The step learning policy is adopted and the learning rate decay is set to 0.95.The network is trained by 100 epochs.In order to reduce the GPU memory requirement and training time, the raw images from the KITTI dataset are down-sampled 4 times to 320\u00d796.But using this small image size for training can definitely degrade the performance.The whole network is end-to-end trainable.Considering the GPU limitation, training the network stepby-step is more practicable.The pre-trained model (without training on KITTI dataset) from [26] and [25] is adapted and then fine-tuned using the training KITTI data (as described in IV-C).In order to enhance the performance and avoid overfitting, both geometric augmentation (translation, rotation, scaling) and image augmentation (color, brightness, gamma)\n\n\nTABLE I:\n\nThe comparison of the performance of L-VO against the baselines on the KITTI dataset according to the evaluation method [2].Note that VISO-S is a stereo VO and the other methods are monocular VO.The L-VO model is trained on the sequences 00, 02, 08 and 09, and evaluated on the rest.\n\nSeq.\n\n\nVISO-S[31] VISO-M[31] ESP-VO\n[2] L-VO(2D Flow) L-VO(3D Flow) (1242 \u00d7 376) (1242 \u00d7 376) (1242 \u00d7 376) (320 \u00d7 92) (320 \u00d7 92) t rel (%) r rel ( \u2022 ) t rel (%) r rel ( \u2022 ) t rel (%) r rel ( \u2022 ) t rel (%) r rel ( \u2022 ) t rel (%) r rel ( \u2022 ) 03 1.\n\nC. Visual odometry performance\n\nWe perform two kinds of evaluation for the proposed methods.The first evaluation is based on sequence 00 \u2212 10.Both the qualitative and quantitative results are reported for analysis.For fair comparison, we follow the same partition proposed by [23]  recover the absolute scale.We evaluate the learning odometry using the KITTI VO evaluation metrics, computing the average translational and rotational RMSE for all possible sub-sequences of length (100, . . ., 800) meters.Note that the same evaluation metric was employed in [2].\n\nFor the first evaluation, the overall performance of average translational and rotational errors of L-VO based on 2D flow and 3D flow can reach 4.71%, 0.0241 \u2022 /m and 2.68%, 0.0143 \u2022 /m, respectively, using the standard KITTI evaluation metrics.The detailed comparison of performances (some entries are copied from [2]) is shown in Table I.It is clear that the performance of both L-VO (2D) and L-VO (3D) is much better than conventional monocular VO.L-VO (3D) performs slightly worse than conventional stereo VO.This can also be seen in the predicted trajectory Fig. 3.Most of the time, the average drift distances of the red line (L-VO 3D) and green one (L-VO 2D) are between that of the light blue line (stereo VO) and dark blue line (monocular VO).The red line is much closer to the light blue line.\n\nThe main limitation of monocular VO and SLAM is the absolute scale estimation.However, with a deep learning method, the scale can be estimated more accurately without any scene-based geometric constraints such as camera height.This is one of the main reasons why the proposed L-VO(2D) and L-VO(3D) outperform the conventional monocular VO.\n\nAs we formulate VO prediction as a regression problem, multi-modal features can enhance the prediction.That is the reason why the result of L-VO(3D) is better than L-VO(2D) and closer to the performance of stereo VO.Another reason why L-VO(3D) can be close to stereo VO is that the (x, z) constraint in the Bivariate Gaussian loss function can learn the translation correlation between the left/right and forward direction.This learned constraint can make the trajectory more accurate -see, for example, the straight line in Fig. 3(b) and corner in Fig. 3(c).\n\nFor low-speed scenarios, the magnitude of 2D flow is insignificant and thus provides a weak feature response to the network, while the magnitude of the depth flow is still quite strong even in a low-speed situation.Thus, the depth flow feature is a good complement to 2D flow in low-speed situations, which is further observed in Fig. 4(c) and Fig. 4(d).Moreover, because the training data is only provided by 4 sequences, multi-modal features, i.e., 3D flow can enhance the robustness of 6DOF relative pose regression.\n\nFor the second evaluation, the L-VO network is trained using more data, i.e. sequence 00-10.Due to the lack of ground truth, only qualitative results are shown in Fig. 6.It can be seen that the L-VO network can also give a highquality prediction in the new scenarios.Both L-VO(2D) and L-VO(3D) outperform monocular VO thanks to better scale estimation.The trajectory of L-VO(3D) is closer to stereo VO than L-VO(2D).However, the performance of L-VO(2D) is boosted more than L-VO(3D) by using more training data.\n\nDuring testing, we observe that L-VO cannot give a similar prediction to stereo VO for sequence 21 (Fig. 6(i)).This sequence is very challenging as it is captured over a long distance in a high-speed scenario (up to 80km/h).The main difficulty L-VO encounters is the high number of moving objects such as fast-moving cars in this street.As displayed in Fig. 5, the main flow feature is extracted from the fastmoving cars.Therefore, the main challenge for flow-based learning VO is to remove the effects of dynamic objects.\n\n\nD. Dense 3D mapping\n\nA learning monocular SLAM system integrated with L-VO(3D) is deployed in this paper.The whole system is implemented under ROS and the neural network is implemented using Tensorflow trained on an NVIDIA Titan GPU.Compared to LSD and ORB monocular SLAM, our system\n\nFig. 2 :\n2\nFig. 2: The architecture of the proposed learning visual odometry (L-VO) network.\n\n\nFig. 3 :\n3\nFig. 3: The predicted trajectories of the proposed L-VO Net on Sequences 03, 04, 05, 06, 07 and 10.The network is trained on Sequence 00, 02, 08 and 09.\n\n\n[ 2 ]\n2\nand split the sequences 00-10 to 00, 02, 08, 09 for training and 03, 04, 05, 06, 07, 10 for testing.The second evaluation is based on sequence 00-21.The sequence 00-10 is employed for training and sequence 11-21 for testing.Only the qualitative results are provided because the ground truth of sequence 11-21 are not provided.The open-source visual odometry library VISO2[31] is employed as the baseline method.It provides both monocular visual odometry and stereo odometry.For monocular VO, the fixed height (1.7) and pitch (-0.03) are employed in order to Rotation vs speed.\n\n\nFig. 4 :Fig. 5 :\n45\nFig. 4: Average translational and rotational errors of the baselines against different path lengths and speeds.The L-VO model is trained on the sequences 00, 02, 08 and 09, and evaluated on the rest.\n\n\nFig. 7 :\n7\nFig. 7: The center image is the global dense 3D map of sequence 07 in the KITTI dataset.The small images in the surrounding show enlarged local areas of the global map.\n\n\n\n\n\n\n\n\n\nrel and r rel are average translational RMSE(%) and rotational RMSE( \u2022 /100m) over 100m \u2212 800m intervals.\n711.129.022.836.726.463.351.623.181.31041.540.844.331.636.336.084.152.532.040.81052.361.2019.163.623.354.932.491.192.590.99061.470.876.641.967.247.293.191.541.390.95072.371.7826.545.923.525.0217.210.42.812.54101.511.1548.293.439.7710.27.243.064.383.12Mean1.831.1619.003.236.156.666.273.392.731.62t are employed. As mentioned in [22][25] and [2], we alsoobserve that these data augmentation techniques are crucialto improve the 2D flow estimation, depth prediction, andespecially VO prediction, because of the limited number oftraining examples. During testing, the number of Gaussiansamples is set to 10000.\nExtreme Robotics Lab, University of Birmingham, Birmingham, UK, B15\n2TT. IRobotCheng@gmail.com.2 Lincoln Centre for Autonomous Systems (L-CAS), University of Lincoln, UK, LN6\n7TS.3 Cambridge Research Lab, Toshiba Research Europe, Cambridge, UK, CB4 0GZ.\nhttp://www.cvlibs.net/datasets/kitti\ncan generate a significantly denser 3D map.In order to alleviate the border blur and wrong prediction of depth, depth fusion from multiple frames is employed during mapping.In order to reduce the hardware resource requirement, OctoMap is used for the map representation instead of the point cloud.Given the dense refinement of depth information, a dense 3D map can be generated online.In Fig.7, the center image is the dense 3D map of the sequence 07 in the KITTI dataset and the small images in the surrounding show enlarged local areas of the global map.It can be seen that after depth fusion, sharply defined shapes such as the car, trees and building are obtained.Moreover, a lot of outliers and noise are removed to make the map cleaner.V. CONCLUSIONIn this paper, a learning system for monocular SLAM is proposed, which can deploy simultaneous localization using a L-VO neural network and the dense 3D mapping.Its performance exceeds most of the monocular SLAM approaches and is even comparable with some stereo SLAM approaches.Compared with conventional SLAM, its main limitations are the high computational requirements and high dataset bias.A demo can be found on the first author's Youtube channel 2 .VI. ACKNOWLEDGEMENTThe authors was funded by a DISTINCTIVE scholarship, EU H2020 projects: RoMaNS (645582) & ILIAD (732737).\nLS-VO: Learning dense optical subspace for robust visual odometry estimation. G Costante, T A Ciarfuglia, arXiv:1709.060192017arXiv preprint\n\nEnd-to-end, sequence-tosequence probabilistic visual odometry through deep neural networks. S Wang, R Clark, H Wen, N Trigoni, The International Journal of Robotics Research. 2017\n\nVINet: Visual-inertial odometry as a sequence-to-sequence learning problem. R Clark, S Wang, H Wen, A Markham, N Trigoni, AAAI. 2017\n\nBackprop KF: Learning discriminative deterministic state estimators. T Haarnoja, A Ajay, S Levine, P Abbeel, Advances in Neural Information Processing Systems. 2016\n\nFlowdometry: An optical flow and deep learning based approach to visual odometry. P Muller, A Savakis, Proc. WACV. WACV2017\n\nExploring representation learning with CNNs for frame-to-frame ego-motion estimation. G Costante, M Mancini, P Valigi, T A Ciarfuglia, IEEE robotics and automation letters. 112016\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, CVPR. 2672017\n\nDeMoN: Depth and motion network for learning monocular stereo. B Ummenhofer, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, CVPR. 20175\n\nS Vijayanarasimhan, S Ricco, C Schmid, R Sukthankar, K Fragkiadaki, arXiv:1704.07804Sfm-Net: Learning of structure and motion from video. 2017arXiv preprint\n\nUnDeepVO: Monocular visual odometry through unsupervised deep learning. R Li, S Wang, Z Long, D Gu, arXiv:1709.068412017arXiv preprint\n\nCNN-SLAM: Realtime dense monocular slam with learned depth prediction. K Tateno, F Tombari, I Laina, N Navab, arXiv:1704.034892017arXiv preprint\n\nA fully end-to-end deep learning approach for real-time simultaneous 3d reconstruction and material recognition. C Zhao, L Sun, R Stolkin, Advanced Robotics (ICAR). 2017. 2017\n\nDense RGB-D semantic mapping with pixel-voxel neural network. C Zhao, L Sun, B Shuai, P Purkait, R Stolkin, arXiv:1710.001322017arXiv preprint\n\nWeakly-supervised DCNN for RGB-D object recognition in real-world applications which lack large-scale annotated training data. L Sun, C Zhao, R Stolkin, arXiv:1703.063702017arXiv preprint\n\nRecurrent-OctoMap: Learning state-based map refinement for long-term semantic mapping with 3d-lidar data. L Sun, Z Yan, A Zaganidis, C Zhao, T Duckett, arXiv:1807.009252018arXiv preprint\n\nMemorybased learning for visual odometry. R Roberts, H Nguyen, N Krishnamurthi, T Balch, ICRA. 2008IEEE\n\nLearning general optical flow subspaces for egomotion estimation and detection of motion anomalies. R Roberts, C Potthast, F Dellaert, CVPR. IEEE2009\n\nVisual odometry learning for unmanned aerial vehicles. V Guizilini, F Ramos, ICRA. IEEE2011\n\nSemi-parametric models for visual odometry. F Guizilini, Ramos, ICRA. IEEE2012\n\nEvaluation of non-geometric methods for visual odometry. T A Ciarfuglia, G Costante, P Valigi, E Ricci, Robotics and Autonomous Systems. 62122014\n\nHigh accuracy optical flow estimation based on a theory for warping. T Brox, A Bruhn, N Papenberg, J Weickert, ECCV. Springer2004\n\nFlowNet: Learning optical flow with convolutional networks. A Dosovitskiy, P Fischer, E Ilg, P Hausser, C Hazirbas, V Golkov, P Van Der Smagt, D Cremers, T Brox, ICCV. 2015\n\nDeepVO: Towards end-to-end visual odometry with deep recurrent convolutional neural networks,\" in ICRA. S Wang, R Clark, H Wen, N Trigoni, 2017IEEE\n\nDeep EndoVO: A recurrent convolutional neural network (RCNN) based visual odometry approach for endoscopic capsule robots. M Turan, Y Almalioglu, H Araujo, E Konukoglu, M Sitti, Neurocomputing. 2752018\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, CVPR. 2017\n\nFlownet 2.0: Evolution of optical flow estimation with deep networks. E Ilg, N Mayer, T Saikia, M Keuper, A Dosovitskiy, T Brox, CVPR. 22017\n\nSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and\u00a1 0.5 mb model size. F N Iandola, Others, arXiv:1602.073602016arXiv preprint\n\nGenerating sequences with recurrent neural networks. A Graves, arXiv:1308.08502013arXiv preprint\n\nInferring 3d body pose from silhouettes using activity manifold learning. A Elgammal, C.-S Lee, CVPR. 2004\n\nOctoMap: An efficient probabilistic 3d mapping framework based on octrees. A Hornung, K M Wurm, M Bennewitz, C Stachniss, W Burgard, Autonomous Robots. 3432013\n\nStereoScan: Dense 3d reconstruction in real-time. A Geiger, J Ziegler, C Stiller, Intelligent Vehicles Symposium (IV). IEEE2011\n", "annotations": {"author": "[{\"end\":101,\"start\":90},{\"end\":109,\"start\":102},{\"end\":124,\"start\":110},{\"end\":137,\"start\":125},{\"end\":153,\"start\":138}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":96},{\"end\":108,\"start\":106},{\"end\":123,\"start\":116},{\"end\":136,\"start\":129},{\"end\":152,\"start\":145}]", "author_first_name": "[{\"end\":95,\"start\":90},{\"end\":105,\"start\":102},{\"end\":115,\"start\":110},{\"end\":128,\"start\":125},{\"end\":144,\"start\":138}]", "author_affiliation": null, "title": "[{\"end\":76,\"start\":1},{\"end\":229,\"start\":154}]", "venue": null, "abstract": "[{\"end\":1300,\"start\":299}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2325,\"start\":2322},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2336,\"start\":2333},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2413,\"start\":2410},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2428,\"start\":2425},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2449,\"start\":2446},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3002,\"start\":2999},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3009,\"start\":3006},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3036,\"start\":3032},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3517,\"start\":3513},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3799,\"start\":3795},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3812,\"start\":3808},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4853,\"start\":4849},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4862,\"start\":4858},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4871,\"start\":4867},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5329,\"start\":5326},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5465,\"start\":5461},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5489,\"start\":5486},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5539,\"start\":5535},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5608,\"start\":5605},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5739,\"start\":5736},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6126,\"start\":6123},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6131,\"start\":6127},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6229,\"start\":6226},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6375,\"start\":6371},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6828,\"start\":6825},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7082,\"start\":7079},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7245,\"start\":7242},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7388,\"start\":7384},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7425,\"start\":7422},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7434,\"start\":7430},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7700,\"start\":7696},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8143,\"start\":8140},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8154,\"start\":8151},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8737,\"start\":8734},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9313,\"start\":9309},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9331,\"start\":9327},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10033,\"start\":10029},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10119,\"start\":10115},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10135,\"start\":10131},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10155,\"start\":10151},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10445,\"start\":10442},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10450,\"start\":10447},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10459,\"start\":10455},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10510,\"start\":10506},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10577,\"start\":10573},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12222,\"start\":12218},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14219,\"start\":14215},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14417,\"start\":14413},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17202,\"start\":17198},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19147,\"start\":19143},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19156,\"start\":19152},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19546,\"start\":19543},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20234,\"start\":20230},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20514,\"start\":20511},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20835,\"start\":20832},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24712,\"start\":24708},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26137,\"start\":26136},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26221,\"start\":26220}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":24161,\"start\":24067},{\"attributes\":{\"id\":\"fig_2\"},\"end\":24327,\"start\":24162},{\"attributes\":{\"id\":\"fig_3\"},\"end\":24914,\"start\":24328},{\"attributes\":{\"id\":\"fig_4\"},\"end\":25136,\"start\":24915},{\"attributes\":{\"id\":\"fig_5\"},\"end\":25318,\"start\":25137},{\"end\":25323,\"start\":25319},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26040,\"start\":25324}]", "paragraph": "[{\"end\":1929,\"start\":1319},{\"end\":2556,\"start\":1931},{\"end\":2959,\"start\":2558},{\"end\":3224,\"start\":2961},{\"end\":3881,\"start\":3226},{\"end\":4694,\"start\":3883},{\"end\":4772,\"start\":4715},{\"end\":5166,\"start\":4774},{\"end\":5720,\"start\":5218},{\"end\":6490,\"start\":5722},{\"end\":7520,\"start\":6544},{\"end\":8035,\"start\":7548},{\"end\":8930,\"start\":8053},{\"end\":9909,\"start\":8963},{\"end\":10382,\"start\":9953},{\"end\":10940,\"start\":10384},{\"end\":11300,\"start\":10973},{\"end\":12029,\"start\":11411},{\"end\":12616,\"start\":12054},{\"end\":13337,\"start\":12643},{\"end\":13801,\"start\":13377},{\"end\":14715,\"start\":13803},{\"end\":15371,\"start\":14837},{\"end\":15547,\"start\":15500},{\"end\":16057,\"start\":15602},{\"end\":16344,\"start\":16059},{\"end\":16518,\"start\":16395},{\"end\":16755,\"start\":16557},{\"end\":16983,\"start\":16830},{\"end\":17522,\"start\":16985},{\"end\":17591,\"start\":17562},{\"end\":17956,\"start\":17615},{\"end\":18423,\"start\":17987},{\"end\":19410,\"start\":18459},{\"end\":19706,\"start\":19423},{\"end\":19712,\"start\":19708},{\"end\":20515,\"start\":19986},{\"end\":21320,\"start\":20517},{\"end\":21661,\"start\":21322},{\"end\":22222,\"start\":21663},{\"end\":22743,\"start\":22224},{\"end\":23256,\"start\":22745},{\"end\":23780,\"start\":23258},{\"end\":24066,\"start\":23804},{\"end\":24160,\"start\":24079},{\"end\":24326,\"start\":24174},{\"end\":24913,\"start\":24337},{\"end\":25135,\"start\":24936},{\"end\":25317,\"start\":25149},{\"end\":25432,\"start\":25327}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11368,\"start\":11301},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11410,\"start\":11368},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12641,\"start\":12617},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12642,\"start\":12641},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14835,\"start\":14716},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14836,\"start\":14835},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15499,\"start\":15372},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15601,\"start\":15548},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16394,\"start\":16345},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16829,\"start\":16756},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17561,\"start\":17523},{\"attributes\":{\"id\":\"formula_11\"},\"end\":17614,\"start\":17592},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19952,\"start\":19744}]", "table_ref": "[{\"end\":20856,\"start\":20855}]", "section_header": "[{\"end\":1317,\"start\":1302},{\"end\":4713,\"start\":4697},{\"end\":5216,\"start\":5169},{\"end\":6542,\"start\":6493},{\"end\":7546,\"start\":7523},{\"end\":8051,\"start\":8038},{\"end\":8961,\"start\":8933},{\"end\":9951,\"start\":9912},{\"end\":10971,\"start\":10943},{\"end\":12052,\"start\":12032},{\"end\":13375,\"start\":13340},{\"end\":16555,\"start\":16521},{\"end\":17985,\"start\":17959},{\"end\":18457,\"start\":18426},{\"end\":19421,\"start\":19413},{\"end\":19743,\"start\":19715},{\"end\":19984,\"start\":19954},{\"end\":23802,\"start\":23783},{\"end\":24076,\"start\":24068},{\"end\":24171,\"start\":24163},{\"end\":24334,\"start\":24329},{\"end\":24932,\"start\":24916},{\"end\":25146,\"start\":25138}]", "table": "[{\"end\":26040,\"start\":25433}]", "figure_caption": "[{\"end\":24161,\"start\":24078},{\"end\":24327,\"start\":24173},{\"end\":24914,\"start\":24336},{\"end\":25136,\"start\":24935},{\"end\":25318,\"start\":25148},{\"end\":25323,\"start\":25321},{\"end\":25433,\"start\":25326}]", "figure_ref": "[{\"end\":2859,\"start\":2858},{\"end\":4693,\"start\":4692},{\"end\":9034,\"start\":9033},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12072,\"start\":12071},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21086,\"start\":21085},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22197,\"start\":22193},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22221,\"start\":22217},{\"end\":22560,\"start\":22559},{\"end\":22577,\"start\":22573},{\"end\":22914,\"start\":22913},{\"end\":23367,\"start\":23363},{\"end\":23617,\"start\":23616}]", "bib_author_first_name": "[{\"end\":27747,\"start\":27746},{\"end\":27759,\"start\":27758},{\"end\":27761,\"start\":27760},{\"end\":27903,\"start\":27902},{\"end\":27911,\"start\":27910},{\"end\":27920,\"start\":27919},{\"end\":27927,\"start\":27926},{\"end\":28068,\"start\":28067},{\"end\":28077,\"start\":28076},{\"end\":28085,\"start\":28084},{\"end\":28092,\"start\":28091},{\"end\":28103,\"start\":28102},{\"end\":28195,\"start\":28194},{\"end\":28207,\"start\":28206},{\"end\":28215,\"start\":28214},{\"end\":28225,\"start\":28224},{\"end\":28374,\"start\":28373},{\"end\":28384,\"start\":28383},{\"end\":28503,\"start\":28502},{\"end\":28515,\"start\":28514},{\"end\":28526,\"start\":28525},{\"end\":28536,\"start\":28535},{\"end\":28538,\"start\":28537},{\"end\":28656,\"start\":28655},{\"end\":28664,\"start\":28663},{\"end\":28673,\"start\":28672},{\"end\":28684,\"start\":28683},{\"end\":28686,\"start\":28685},{\"end\":28772,\"start\":28771},{\"end\":28786,\"start\":28785},{\"end\":28794,\"start\":28793},{\"end\":28803,\"start\":28802},{\"end\":28812,\"start\":28811},{\"end\":28819,\"start\":28818},{\"end\":28834,\"start\":28833},{\"end\":28855,\"start\":28854},{\"end\":28875,\"start\":28874},{\"end\":28884,\"start\":28883},{\"end\":28894,\"start\":28893},{\"end\":28908,\"start\":28907},{\"end\":29085,\"start\":29084},{\"end\":29091,\"start\":29090},{\"end\":29099,\"start\":29098},{\"end\":29107,\"start\":29106},{\"end\":29220,\"start\":29219},{\"end\":29230,\"start\":29229},{\"end\":29241,\"start\":29240},{\"end\":29250,\"start\":29249},{\"end\":29408,\"start\":29407},{\"end\":29416,\"start\":29415},{\"end\":29423,\"start\":29422},{\"end\":29534,\"start\":29533},{\"end\":29542,\"start\":29541},{\"end\":29549,\"start\":29548},{\"end\":29558,\"start\":29557},{\"end\":29569,\"start\":29568},{\"end\":29743,\"start\":29742},{\"end\":29750,\"start\":29749},{\"end\":29758,\"start\":29757},{\"end\":29911,\"start\":29910},{\"end\":29918,\"start\":29917},{\"end\":29925,\"start\":29924},{\"end\":29938,\"start\":29937},{\"end\":29946,\"start\":29945},{\"end\":30035,\"start\":30034},{\"end\":30046,\"start\":30045},{\"end\":30056,\"start\":30055},{\"end\":30073,\"start\":30072},{\"end\":30198,\"start\":30197},{\"end\":30209,\"start\":30208},{\"end\":30221,\"start\":30220},{\"end\":30304,\"start\":30303},{\"end\":30317,\"start\":30316},{\"end\":30386,\"start\":30385},{\"end\":30479,\"start\":30478},{\"end\":30481,\"start\":30480},{\"end\":30495,\"start\":30494},{\"end\":30507,\"start\":30506},{\"end\":30517,\"start\":30516},{\"end\":30638,\"start\":30637},{\"end\":30646,\"start\":30645},{\"end\":30655,\"start\":30654},{\"end\":30668,\"start\":30667},{\"end\":30760,\"start\":30759},{\"end\":30775,\"start\":30774},{\"end\":30786,\"start\":30785},{\"end\":30793,\"start\":30792},{\"end\":30804,\"start\":30803},{\"end\":30816,\"start\":30815},{\"end\":30826,\"start\":30825},{\"end\":30843,\"start\":30842},{\"end\":30854,\"start\":30853},{\"end\":30978,\"start\":30977},{\"end\":30986,\"start\":30985},{\"end\":30995,\"start\":30994},{\"end\":31002,\"start\":31001},{\"end\":31146,\"start\":31145},{\"end\":31155,\"start\":31154},{\"end\":31169,\"start\":31168},{\"end\":31179,\"start\":31178},{\"end\":31192,\"start\":31191},{\"end\":31295,\"start\":31294},{\"end\":31305,\"start\":31304},{\"end\":31309,\"start\":31306},{\"end\":31318,\"start\":31317},{\"end\":31320,\"start\":31319},{\"end\":31413,\"start\":31412},{\"end\":31420,\"start\":31419},{\"end\":31429,\"start\":31428},{\"end\":31439,\"start\":31438},{\"end\":31449,\"start\":31448},{\"end\":31464,\"start\":31463},{\"end\":31570,\"start\":31569},{\"end\":31572,\"start\":31571},{\"end\":31680,\"start\":31679},{\"end\":31799,\"start\":31798},{\"end\":31814,\"start\":31810},{\"end\":31908,\"start\":31907},{\"end\":31919,\"start\":31918},{\"end\":31921,\"start\":31920},{\"end\":31929,\"start\":31928},{\"end\":31942,\"start\":31941},{\"end\":31955,\"start\":31954},{\"end\":32044,\"start\":32043},{\"end\":32054,\"start\":32053},{\"end\":32065,\"start\":32064}]", "bib_author_last_name": "[{\"end\":27756,\"start\":27748},{\"end\":27772,\"start\":27762},{\"end\":27908,\"start\":27904},{\"end\":27917,\"start\":27912},{\"end\":27924,\"start\":27921},{\"end\":27935,\"start\":27928},{\"end\":28074,\"start\":28069},{\"end\":28082,\"start\":28078},{\"end\":28089,\"start\":28086},{\"end\":28100,\"start\":28093},{\"end\":28111,\"start\":28104},{\"end\":28204,\"start\":28196},{\"end\":28212,\"start\":28208},{\"end\":28222,\"start\":28216},{\"end\":28232,\"start\":28226},{\"end\":28381,\"start\":28375},{\"end\":28392,\"start\":28385},{\"end\":28512,\"start\":28504},{\"end\":28523,\"start\":28516},{\"end\":28533,\"start\":28527},{\"end\":28549,\"start\":28539},{\"end\":28661,\"start\":28657},{\"end\":28670,\"start\":28665},{\"end\":28681,\"start\":28674},{\"end\":28691,\"start\":28687},{\"end\":28783,\"start\":28773},{\"end\":28791,\"start\":28787},{\"end\":28800,\"start\":28795},{\"end\":28809,\"start\":28804},{\"end\":28816,\"start\":28813},{\"end\":28831,\"start\":28820},{\"end\":28839,\"start\":28835},{\"end\":28872,\"start\":28856},{\"end\":28881,\"start\":28876},{\"end\":28891,\"start\":28885},{\"end\":28905,\"start\":28895},{\"end\":28920,\"start\":28909},{\"end\":29088,\"start\":29086},{\"end\":29096,\"start\":29092},{\"end\":29104,\"start\":29100},{\"end\":29110,\"start\":29108},{\"end\":29227,\"start\":29221},{\"end\":29238,\"start\":29231},{\"end\":29247,\"start\":29242},{\"end\":29256,\"start\":29251},{\"end\":29413,\"start\":29409},{\"end\":29420,\"start\":29417},{\"end\":29431,\"start\":29424},{\"end\":29539,\"start\":29535},{\"end\":29546,\"start\":29543},{\"end\":29555,\"start\":29550},{\"end\":29566,\"start\":29559},{\"end\":29577,\"start\":29570},{\"end\":29747,\"start\":29744},{\"end\":29755,\"start\":29751},{\"end\":29766,\"start\":29759},{\"end\":29915,\"start\":29912},{\"end\":29922,\"start\":29919},{\"end\":29935,\"start\":29926},{\"end\":29943,\"start\":29939},{\"end\":29954,\"start\":29947},{\"end\":30043,\"start\":30036},{\"end\":30053,\"start\":30047},{\"end\":30070,\"start\":30057},{\"end\":30079,\"start\":30074},{\"end\":30206,\"start\":30199},{\"end\":30218,\"start\":30210},{\"end\":30230,\"start\":30222},{\"end\":30314,\"start\":30305},{\"end\":30323,\"start\":30318},{\"end\":30396,\"start\":30387},{\"end\":30403,\"start\":30398},{\"end\":30492,\"start\":30482},{\"end\":30504,\"start\":30496},{\"end\":30514,\"start\":30508},{\"end\":30523,\"start\":30518},{\"end\":30643,\"start\":30639},{\"end\":30652,\"start\":30647},{\"end\":30665,\"start\":30656},{\"end\":30677,\"start\":30669},{\"end\":30772,\"start\":30761},{\"end\":30783,\"start\":30776},{\"end\":30790,\"start\":30787},{\"end\":30801,\"start\":30794},{\"end\":30813,\"start\":30805},{\"end\":30823,\"start\":30817},{\"end\":30840,\"start\":30827},{\"end\":30851,\"start\":30844},{\"end\":30859,\"start\":30855},{\"end\":30983,\"start\":30979},{\"end\":30992,\"start\":30987},{\"end\":30999,\"start\":30996},{\"end\":31010,\"start\":31003},{\"end\":31152,\"start\":31147},{\"end\":31166,\"start\":31156},{\"end\":31176,\"start\":31170},{\"end\":31189,\"start\":31180},{\"end\":31198,\"start\":31193},{\"end\":31302,\"start\":31296},{\"end\":31315,\"start\":31310},{\"end\":31328,\"start\":31321},{\"end\":31417,\"start\":31414},{\"end\":31426,\"start\":31421},{\"end\":31436,\"start\":31430},{\"end\":31446,\"start\":31440},{\"end\":31461,\"start\":31450},{\"end\":31469,\"start\":31465},{\"end\":31580,\"start\":31573},{\"end\":31588,\"start\":31582},{\"end\":31687,\"start\":31681},{\"end\":31808,\"start\":31800},{\"end\":31818,\"start\":31815},{\"end\":31916,\"start\":31909},{\"end\":31926,\"start\":31922},{\"end\":31939,\"start\":31930},{\"end\":31952,\"start\":31943},{\"end\":31963,\"start\":31956},{\"end\":32051,\"start\":32045},{\"end\":32062,\"start\":32055},{\"end\":32073,\"start\":32066}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1709.06019\",\"id\":\"b0\"},\"end\":27808,\"start\":27668},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":21689894},\"end\":27989,\"start\":27810},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":17112447},\"end\":28123,\"start\":27991},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":5670914},\"end\":28289,\"start\":28125},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206858678},\"end\":28414,\"start\":28291},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7183629},\"end\":28595,\"start\":28416},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":11977588},\"end\":28706,\"start\":28597},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6159584},\"end\":28852,\"start\":28708},{\"attributes\":{\"doi\":\"arXiv:1704.07804\",\"id\":\"b8\"},\"end\":29010,\"start\":28854},{\"attributes\":{\"doi\":\"arXiv:1709.06841\",\"id\":\"b9\"},\"end\":29146,\"start\":29012},{\"attributes\":{\"doi\":\"arXiv:1704.03489\",\"id\":\"b10\"},\"end\":29292,\"start\":29148},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1744396},\"end\":29469,\"start\":29294},{\"attributes\":{\"doi\":\"arXiv:1710.00132\",\"id\":\"b12\"},\"end\":29613,\"start\":29471},{\"attributes\":{\"doi\":\"arXiv:1703.06370\",\"id\":\"b13\"},\"end\":29802,\"start\":29615},{\"attributes\":{\"doi\":\"arXiv:1807.00925\",\"id\":\"b14\"},\"end\":29990,\"start\":29804},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9955291},\"end\":30095,\"start\":29992},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11498796},\"end\":30246,\"start\":30097},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1913580},\"end\":30339,\"start\":30248},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15168299},\"end\":30419,\"start\":30341},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16060445},\"end\":30566,\"start\":30421},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":76390},\"end\":30697,\"start\":30568},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":12552176},\"end\":30871,\"start\":30699},{\"attributes\":{\"id\":\"b22\"},\"end\":31020,\"start\":30873},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10416499},\"end\":31223,\"start\":31022},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":206596513},\"end\":31340,\"start\":31225},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3759573},\"end\":31482,\"start\":31342},{\"attributes\":{\"doi\":\"arXiv:1602.07360\",\"id\":\"b26\"},\"end\":31624,\"start\":31484},{\"attributes\":{\"doi\":\"arXiv:1308.0850\",\"id\":\"b27\"},\"end\":31722,\"start\":31626},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":659482},\"end\":31830,\"start\":31724},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8655888},\"end\":31991,\"start\":31832},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":16284071},\"end\":32120,\"start\":31993}]", "bib_title": "[{\"end\":27900,\"start\":27810},{\"end\":28065,\"start\":27991},{\"end\":28192,\"start\":28125},{\"end\":28371,\"start\":28291},{\"end\":28500,\"start\":28416},{\"end\":28653,\"start\":28597},{\"end\":28769,\"start\":28708},{\"end\":29405,\"start\":29294},{\"end\":30032,\"start\":29992},{\"end\":30195,\"start\":30097},{\"end\":30301,\"start\":30248},{\"end\":30383,\"start\":30341},{\"end\":30476,\"start\":30421},{\"end\":30635,\"start\":30568},{\"end\":30757,\"start\":30699},{\"end\":31143,\"start\":31022},{\"end\":31292,\"start\":31225},{\"end\":31410,\"start\":31342},{\"end\":31796,\"start\":31724},{\"end\":31905,\"start\":31832},{\"end\":32041,\"start\":31993}]", "bib_author": "[{\"end\":27758,\"start\":27746},{\"end\":27774,\"start\":27758},{\"end\":27910,\"start\":27902},{\"end\":27919,\"start\":27910},{\"end\":27926,\"start\":27919},{\"end\":27937,\"start\":27926},{\"end\":28076,\"start\":28067},{\"end\":28084,\"start\":28076},{\"end\":28091,\"start\":28084},{\"end\":28102,\"start\":28091},{\"end\":28113,\"start\":28102},{\"end\":28206,\"start\":28194},{\"end\":28214,\"start\":28206},{\"end\":28224,\"start\":28214},{\"end\":28234,\"start\":28224},{\"end\":28383,\"start\":28373},{\"end\":28394,\"start\":28383},{\"end\":28514,\"start\":28502},{\"end\":28525,\"start\":28514},{\"end\":28535,\"start\":28525},{\"end\":28551,\"start\":28535},{\"end\":28663,\"start\":28655},{\"end\":28672,\"start\":28663},{\"end\":28683,\"start\":28672},{\"end\":28693,\"start\":28683},{\"end\":28785,\"start\":28771},{\"end\":28793,\"start\":28785},{\"end\":28802,\"start\":28793},{\"end\":28811,\"start\":28802},{\"end\":28818,\"start\":28811},{\"end\":28833,\"start\":28818},{\"end\":28841,\"start\":28833},{\"end\":28874,\"start\":28854},{\"end\":28883,\"start\":28874},{\"end\":28893,\"start\":28883},{\"end\":28907,\"start\":28893},{\"end\":28922,\"start\":28907},{\"end\":29090,\"start\":29084},{\"end\":29098,\"start\":29090},{\"end\":29106,\"start\":29098},{\"end\":29112,\"start\":29106},{\"end\":29229,\"start\":29219},{\"end\":29240,\"start\":29229},{\"end\":29249,\"start\":29240},{\"end\":29258,\"start\":29249},{\"end\":29415,\"start\":29407},{\"end\":29422,\"start\":29415},{\"end\":29433,\"start\":29422},{\"end\":29541,\"start\":29533},{\"end\":29548,\"start\":29541},{\"end\":29557,\"start\":29548},{\"end\":29568,\"start\":29557},{\"end\":29579,\"start\":29568},{\"end\":29749,\"start\":29742},{\"end\":29757,\"start\":29749},{\"end\":29768,\"start\":29757},{\"end\":29917,\"start\":29910},{\"end\":29924,\"start\":29917},{\"end\":29937,\"start\":29924},{\"end\":29945,\"start\":29937},{\"end\":29956,\"start\":29945},{\"end\":30045,\"start\":30034},{\"end\":30055,\"start\":30045},{\"end\":30072,\"start\":30055},{\"end\":30081,\"start\":30072},{\"end\":30208,\"start\":30197},{\"end\":30220,\"start\":30208},{\"end\":30232,\"start\":30220},{\"end\":30316,\"start\":30303},{\"end\":30325,\"start\":30316},{\"end\":30398,\"start\":30385},{\"end\":30405,\"start\":30398},{\"end\":30494,\"start\":30478},{\"end\":30506,\"start\":30494},{\"end\":30516,\"start\":30506},{\"end\":30525,\"start\":30516},{\"end\":30645,\"start\":30637},{\"end\":30654,\"start\":30645},{\"end\":30667,\"start\":30654},{\"end\":30679,\"start\":30667},{\"end\":30774,\"start\":30759},{\"end\":30785,\"start\":30774},{\"end\":30792,\"start\":30785},{\"end\":30803,\"start\":30792},{\"end\":30815,\"start\":30803},{\"end\":30825,\"start\":30815},{\"end\":30842,\"start\":30825},{\"end\":30853,\"start\":30842},{\"end\":30861,\"start\":30853},{\"end\":30985,\"start\":30977},{\"end\":30994,\"start\":30985},{\"end\":31001,\"start\":30994},{\"end\":31012,\"start\":31001},{\"end\":31154,\"start\":31145},{\"end\":31168,\"start\":31154},{\"end\":31178,\"start\":31168},{\"end\":31191,\"start\":31178},{\"end\":31200,\"start\":31191},{\"end\":31304,\"start\":31294},{\"end\":31317,\"start\":31304},{\"end\":31330,\"start\":31317},{\"end\":31419,\"start\":31412},{\"end\":31428,\"start\":31419},{\"end\":31438,\"start\":31428},{\"end\":31448,\"start\":31438},{\"end\":31463,\"start\":31448},{\"end\":31471,\"start\":31463},{\"end\":31582,\"start\":31569},{\"end\":31590,\"start\":31582},{\"end\":31689,\"start\":31679},{\"end\":31810,\"start\":31798},{\"end\":31820,\"start\":31810},{\"end\":31918,\"start\":31907},{\"end\":31928,\"start\":31918},{\"end\":31941,\"start\":31928},{\"end\":31954,\"start\":31941},{\"end\":31965,\"start\":31954},{\"end\":32053,\"start\":32043},{\"end\":32064,\"start\":32053},{\"end\":32075,\"start\":32064}]", "bib_venue": "[{\"end\":27744,\"start\":27668},{\"end\":27983,\"start\":27937},{\"end\":28117,\"start\":28113},{\"end\":28283,\"start\":28234},{\"end\":28404,\"start\":28394},{\"end\":28587,\"start\":28551},{\"end\":28697,\"start\":28693},{\"end\":28845,\"start\":28841},{\"end\":28990,\"start\":28938},{\"end\":29082,\"start\":29012},{\"end\":29217,\"start\":29148},{\"end\":29457,\"start\":29433},{\"end\":29531,\"start\":29471},{\"end\":29740,\"start\":29615},{\"end\":29908,\"start\":29804},{\"end\":30085,\"start\":30081},{\"end\":30236,\"start\":30232},{\"end\":30329,\"start\":30325},{\"end\":30409,\"start\":30405},{\"end\":30556,\"start\":30525},{\"end\":30683,\"start\":30679},{\"end\":30865,\"start\":30861},{\"end\":30975,\"start\":30873},{\"end\":31214,\"start\":31200},{\"end\":31334,\"start\":31330},{\"end\":31475,\"start\":31471},{\"end\":31567,\"start\":31484},{\"end\":31677,\"start\":31626},{\"end\":31824,\"start\":31820},{\"end\":31982,\"start\":31965},{\"end\":32110,\"start\":32075},{\"end\":28410,\"start\":28406}]"}}}, "year": 2023, "month": 12, "day": 17}