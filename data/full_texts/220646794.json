{"id": 220646794, "updated": "2023-10-06 13:25:32.778", "metadata": {"title": "CATCH: Context-based Meta Reinforcement Learning for Transferrable Architecture Search", "authors": "[{\"first\":\"Xin\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yawen\",\"last\":\"Duan\",\"middle\":[]},{\"first\":\"Zewei\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Hang\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Zihao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xiaodan\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Tong\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zhenguo\",\"last\":\"Li\",\"middle\":[]}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Neural Architecture Search (NAS) achieved many breakthroughs in recent years. In spite of its remarkable progress, many algorithms are restricted to particular search spaces. They also lack efficient mechanisms to reuse knowledge when confronting multiple tasks. These challenges preclude their applicability, and motivate our proposal of CATCH, a novel Context-bAsed meTa reinforcement learning (RL) algorithm for transferrable arChitecture searcH. The combination of meta-learning and RL allows CATCH to efficiently adapt to new tasks while being agnostic to search spaces. CATCH utilizes a probabilistic encoder to encode task properties into latent context variables, which then guide CATCH's controller to quickly\"catch\"top-performing networks. The contexts also assist a network evaluator in filtering inferior candidates and speed up learning. Extensive experiments demonstrate CATCH's universality and search efficiency over many other widely-recognized algorithms. It is also capable of handling cross-domain architecture search as competitive networks on ImageNet, COCO, and Cityscapes are identified. This is the first work to our knowledge that proposes an efficient transferrable NAS solution while maintaining robustness across various settings.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.09380", "mag": "3109568930", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/ChenDCXCLZL20", "doi": "10.1007/978-3-030-58529-7_12"}}, "content": {"source": {"pdf_hash": "fc551bdb4046585b9ac834b999e8518c2da1be06", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.09380", "status": "GREEN"}}, "grobid": {"id": "825c5b228d7f2bff18761956ac26ff049cca3e9e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fc551bdb4046585b9ac834b999e8518c2da1be06.txt", "contents": "\nCATCH: Context-based Meta Reinforcement Learning for Transferrable Architecture Search\n\n\nXin Chen \nThe University of Hong Kong\n\n\nYawen Duan \nThe University of Hong Kong\n\n\nZewei Chen \nHuawei Noah's Ark Lab\n\n\nHang Xu \nHuawei Noah's Ark Lab\n\n\nZihao Chen \nXiaodan Liang \nHuawei Noah's Ark Lab\n\n\nSun Yat-sen University\n\n\nTong Zhang \nThe Hong Kong University of Science and Technology\n\n\nZhenguo Li \nHuawei Noah's Ark Lab\n\n\nCATCH: Context-based Meta Reinforcement Learning for Transferrable Architecture Search\nNeural Architecture Search, Meta Reinforcement Learning\nNeural Architecture Search (NAS) achieved many breakthroughs in recent years. In spite of its remarkable progress, many algorithms are restricted to particular search spaces. They also lack efficient mechanisms to reuse knowledge when confronting multiple tasks. These challenges preclude their applicability, and motivate our proposal of CATCH, a novel Context-bAsed meTa reinforcement learning (RL) algorithm for transferrable arChitecture searcH. The combination of metalearning and RL allows CATCH to efficiently adapt to new tasks while being agnostic to search spaces. CATCH utilizes a probabilistic encoder to encode task properties into latent context variables, which then guide CATCH's controller to quickly \"catch\" top-performing networks. The contexts also assist a network evaluator in filtering inferior candidates and speed up learning. Extensive experiments demonstrate CATCH's universality and search efficiency over many other widely-recognized algorithms. It is also capable of handling cross-domain architecture search as competitive networks on ImageNet, COCO, and Cityscapes are identified. This is the first work to our knowledge that proposes an efficient transferrable NAS solution while maintaining robustness across various settings.\n\nIntroduction\n\nThe emergence of many high-performance neural networks has been one of the pivotal forces pushing forward the progress of deep learning research and production. Recently, many neural networks discovered by Neural Architecture Search (NAS) methods have surpassed manually designed ones on a variety of domains including image classification [47,61], object detection [61], semantic segmentation [5], and recommendation systems [31]. Many potential applications of Equal contribution. Correspondence to: tongzhang@tongzhang-ml.org Fig. 1: Upper: drawbacks of current NAS schemes. Lower: the overall framework of CATCH. Our search agent, CATCHer, consists of three core components: context encoder, RL controller and network evaluator. CATCHer first goes through the meta-training phase to learn an initial search policy, then it adapts to target tasks efficiently.\n\npractical interests are calling for solutions that can (1) efficiently handle a myriad of tasks, (2) be widely applicable to different search spaces, and (3) maintain their levels of competency across various settings. We believe these are important yet somewhat neglected aspects in the past research, and a transformative NAS algorithm should be able to respond to these needs to make a real influence.\n\nMany algorithms [33,37] have been proposed to improve the efficiency of NAS. However, they lack mechanisms to seek and preserve information that can be meaningfully reused. Hence, these algorithms can only repeatedly and inefficiently search from scratch when encountering new tasks. To tackle this problem, a rising direction of NAS attempts to create efficient transferrable algorithms. Several works [23,36] try to search for architectures that perform well across tasks, but the solutions may not be optimal on the target tasks, especially when the target task distributions are distant from the training task distributions. Some recent works [28,15] use meta-learning [16,27] for one-shot NAS instead. With recent critiques [56,26] pointing out some one-shot solutions' dependence on particular search spaces and sensitivity to hyperparameters, many concerns arise on the practicality of these meta NAS works based on one-shot methods. To avoid ambiguity, throughout this paper, tasks are defined as problems that share the same action space, but differ in reward functions. In NAS, the change of either the dataset or domain (e.g. from classification to detection) alters the underlying reward function, and thus can be treated as different tasks.\n\nStriking a balance between universality and efficiency is hard. Solving the universality problem needs a policy to disentangle from specifics of search spaces, which uproots an important foundation of many efficient algorithms. The aim to improve efficiency on multiple tasks naturally links us to a transfer/meta learning paradigm. Meta Reinforcement Learning (RL) [38,25] offers a solution to achieving both efficiency and universality, which largely inspired our proposal of CATCH, a novel context-guided meta reinforcement learning framework that is both search space-agnostic and swiftly adaptive to new tasks.\n\nThe search agent in our framework, namely CATCHer, acts as the decisionmaker to quickly \"catch\" top-performing networks on a task. As is shown in Figure 1, it is first trained on a set of meta-training tasks then deployed to target tasks for fast adaptation. CATCHer leverages three core components: context encoder, RL controller, and network evaluator. The context encoder adopts an amortized variational inference approach [1,38,24] to encode task properties into latent context variables that guide the controller and evaluator. The RL controller makes sequential decisions to generate candidate networks in a stochastic manner. The network evaluator predicts the performance of the candidate networks and decides which nets are valuable for training. All three components are optimized in an end-to-end manner.\n\nWe test the method's universality and adaptation efficiency on two fundamentally different search spaces: cell-based search space [13] and Residual blockbased [19,57] search space. The former focuses on cell structure design, while the latter targets macro skeleton search. With NAS-Bench-201 [13], we can compare CATCH fairly with other algorithms by eliminating performance fluctuations rising from different search spaces and training settings. Our experiments demonstrate CATCH's superiority over various other works, including R-EA [40] and DARTS [33]. On Residual block-based search space, we use image classification tasks on sub-datasets of ImageNet [10] as meta-training tasks, and then adapt the CATCHer to target tasks, such as image classification on full ImageNet, object detection on COCO [30], and semantic segmentation on Cityscapes [9]. CATCH discovered networks on these tasks with competitive performance and inference latency. Our results demonstrated CATCH's robustness across various settings, easing previously raised concerns of NAS algorithms' sensitivity to search space, random seeds, and tendencies to overfit to only one or two reported tasks.\n\nOur key contribution is the first attempt to design an efficient and universal transferrable NAS framework. It swiftly handles various tasks through fast adaptation, and robustly maintains competitive performance across different settings. Our work brings along new perspectives on solving NAS problems, including using amortized variational inference to generate task characteristics that inform network designs. It also demonstrates the possibility of creating efficient sample-based NAS solutions that are comparable with widely-recognized one-shot methods. With competitive networks identified across classification, detection, and segmentation domains, it further opens the investigation on the feasibility of cross-domain architecture search.\n\n\nRelated Work\n\nNAS is an algorithmic approach to design neural networks through searching over candidate architectures. Many harness the power of Reinforcement Learning (RL) [60], Bayesian Optimization [3,4], Evolutionary Algorithm [14,39], and Monte Carlo Tree Search [35,52]. The field gradually gains its tractions with the emergence of highly-efficient algorithms [33,37,39] and architectures [40,47] with remarkable performance.\n\nOur method is inspired by PEARL [38], a recent work in context-based meta reinforcement learning, which captures knowledge about a task with probabilistic latent contexts. The knowledge is then leveraged for informed policy training.\n\nThere are a few key challenges in efficiently applying it to NAS: (1) PEARL models the latent context embeddings of RL tasks as distributions over Markov Decision Processes (MDP), but it is less clear how a task in NAS can be meaningfully encoded. (2) RL is notoriously famous for its sample inefficiency, but it is extremely expensive to obtain reward signals on NAS. We address these challenges by (1) proposing the use of network-reward pairs to represent a task, (2) introducing meta-training tasks that can be cheaply evaluated to obtain more data for learning, and including a network evaluator that acts like Q-learning agents to speed up learning.\n\nPrevious works also explored the possibility of using meta-learning for NAS. Some [23,36] aimed to identify a single architecture that simultaneously works well on all considered tasks. These solutions may not be scalable when confronting a large pool of target tasks. An early work [53] aimed to learn a general policy across tasks. However, it generates task embeddings from images, which may fail at datasets with the same images, and is unable to differentiate among classification, detection, and segmentation tasks on the same dataset. A few recent papers [28,15] combined gradient-based meta-learning with DARTS, but the algorithms are only applicable to search spaces compatible with DARTS. Additionally, none of the above proposals reported their performance on largescale tasks like ImageNet full dataset. This leaves questions on these proposals' generalizability and adaptation efficiency on more challenging datasets, where scientists expect meta-NAS algorithms should have an edge over typical NAS methods. CATCH is the first NAS algorithm to our knowledge that deploys meta-learning while maintaining universality, robustness across different search spaces, and capability to handle large-scale tasks.\n\n\nCATCH Framework\n\nIn NAS, the change of dataset (e.g. CIFAR-10 vs. ImageNet) or domain (e.g. image classification vs. object detection) essentially indicates the shift of underlying reward distribution. The goal of a cross-task transfer algorithm is hence to quickly identify the best actions under the changed reward dynamics. To handle this challenge, the CATCH framework consists of two phases: a meta-training phase and an adaptation phase, as is presented in Algorithm 1. In the metatraining phase, we train the CATCHer on a pool of meta-training tasks that can be cheaply evaluated. A key goal of this phase is to present the context encoder with sufficiently diversified tasks, and encourage it to consistently encode meaningful information for different tasks. Meanwhile, both the controller and the evaluator may gain a good initialization for adaptation. In the adaptation   The search procedure of CATCH on a given task. The procedure starts from initializing the search history by storing a randomly selected network m and its reward r. The encoder applies amortized variational inference approach to generate latent context encoding z by encoding network-reward pairs from the search history. The controller then generates candidate networks for the evaluator to choose the most promising ones to train and evaluate. Newly selected networks and their rewards will be stored in the search history. The loop continues after the three components are optimized.\n\nphase, the meta-trained CATCHer then learns to find networks on the target task efficiently through the guidance of the latent context encoding. We show the search procedure on any single task in Figure 2, which corresponds to line 3-13 of Algorithm 1.\n\n\nContext Encoding\n\nThe use of latent context encoding is a crucial part of CATCH. The question is what information about the task is reliable to construct such latent contexts. Directly extracting feature maps of images of the dataset is an intuitive solution. However, for the same dataset, the best network configurations to perform different tasks like object detection and semantic segmentation may differ a lot. Hence, simply extracting information directly from images may not be a viable approach.\n\nWe instead believe that the task-specific contextual knowledge can be mined from the search history (i.e. sets of network-reward pairs). If the same group of networks have similar relative strengths on two tasks, it might mean these tasks are \"close\" to each other. It is also helpful to break the barriers for cross-task architecture search, since the network-reward pair of information is universal across tasks. Before searching on a task, we randomly form a few networks m and evaluate their performance r to initialize the search history. The retrieved network-reward pairs are stored in the search history for its initialization. To start the search, we sample a number of network-reward pairs {(m, r) i } N 1 (denoted by c 1:N for simplicity) from the search history, which will be fed into the encoder to generate a latent context vector z representing the salient knowledge about the task.\n\nWe model the latent context encoding process in a probabilistic manner, because it allows the context encoder to model a distribution over tasks and conduct exploration via posterior sampling. Following the amortized variational inference approach used in [38, 1,24], we aim to estimate the posterior p(z|c 1:N ) with the encoder q \u03c6 (z|c 1:N ), parametrized by \u03c6. We assume the prior p(z) is a unit multivariate Gaussian distribution with diagonal covariance matrix N (0, diag(1)), and hence, the posterior p(z|c) conditioning on c is Gaussian. Since the network-reward pairs c 1:N are independent on a task, we could factor q \u03c6 (z|c 1:N ) into the product of Gaussian factors conditioning on each piece of contexts c i ,\nq \u03c6 (z|c 1:N ) \u221d N i=1 N (f\u03bc \u03c6 (c i ), diag(f\u03c3 \u03c6 (c i )),(1)\nwhere f \u03c6 is an inference network parametrized by \u03c6, which predicts the mea\u00f1 \u00b5 i and the standard deviation\u03c3 i of q \u03c6 (z|c i ) as a function of c i to approximate Gaussian p(z|c i ).\n\nDuring the forward pass, the encoder network f \u03c6 outputs\u03bc i ,\u03c3 i of the Gaussian posterior q \u03c6 (z|c i ) conditioning on each context, then we take their product q \u03c6 (z|c 1:N ). Each context c i is (m, r) i , where r is normalized among {r} 1:N to reflect the relative advantage of each network. All the network-reward pairs in the search history are utilized. We then sample z from q \u03c6 (z|c 1:N ). Further implementation details can be found in the Appendix.\n\n\nNetwork Sampling\n\nThe generation of a network can be treated as a decision-making problem, where each of the RL controller's actions determines one attribute of the resulting architecture. The attribute can be an operation type to form a certain edge in a cell-based search (e.g. skip-connect, convolution operations, etc.), or the shape of a network in a macro-skeleton search (e.g. width, depth, etc.). Both ways are explored in our work.\n\nA network, denoted by m, is represented as a list of actions [a 1 , a 2 , ..., a L ] taken by the controller in a sequential manner. At each time step l, the controller makes a decision a l according to its policy \u03c0 \u03b8c , parametrized by \u03b8 c . The controller policy takes z and the previous actions [a 1 ...a l\u22121 , 0, ..., 0] as inputs, and outputs the probability distribution of choosing a certain action \u03c0 \u03b8c (a l |[a 1 ...a l\u22121 , 0, ..., 0], z), where the actions will be sampled accordingly. z is the latent context vector generated by the encoder, and [a 1 ...a l\u22121 , 0, ..., 0] is a collection of one-hot vectors indicating all the actions taken so far at l-th timestep, leaving untaken actions [a l , ..., a L ] as zero vectors. The reward for each action is the normalized performance score of the network. The controller samples M networks stochastically as candidates for the network evaluator.\n\n\nAlgorithm 1 Context-based Meta Architecture Search (CATCH)\n\nInputs: {Tmeta} (meta-training task pool), {Ttarget} (target task pool), Nmeta (# of meta epochs), N search (# of search epochs), C (# of contexts to sample), M (# of models to sample) Meta-training Phase: 1: for Nmeta meta epochs do 2:\n\nSelect meta-training task T from {Tmeta} 3:\n\nInitialize SearchHistory 4:\n\nfor n = 1 to N search do 5:\n{(m, r)i} C 1 = SearchHistory.sample contexts(C) 6: z = Encoder.encode({(m, r)i} C 1 ) 7: {m} M 1 \u2190 Controller.sample networks(z, M ) 8: m \u2190 Evaluator.choose best({mj} M 1 , z) 9:\nr \u2190 train and evaluate(m , T ) 10:\n\nSearchHistory.add((m , z, r)) 11:\n\nEncoder, Controller, Evaluator optimization 12:\n\nend for 13: end for Adaptation Phase: 14: Select target task T from {Ttarget} 15: Repeat Line 3-13 16: BestModel \u2190 SearchHistory.best model() 17: return BestModel\n\n\nNetwork Scoring and Evaluation\n\nSince the candidate networks are sampled stochastically by the controller, it is almost inevitable that some inferior models will be generated. We set up a filtering mechanism, namely network evaluator, which acts like a Q-learning agent that predicts the actual performance of each network, and selects the top one for training. The predicted value is not necessarily an accurate prediction of the training performance, but should be able to provide a ranking among candidate models roughly similar to their true performance.\n\nThe evaluator f \u03b8e (m, z) is parameterized by \u03b8 e . It takes M tuples of networkcontext pairs (m, z) as inputs, and outputs the predicted performance of input architectures. The network with the highest predicted performance score will be trained to obtain the true reward r. The network-context-reward tuple (m, z, r) is then stored in the evaluator's local memory for future gradient updates.\n\n\nOptimization of CATCHer\n\nTo optimize the controller policy, we maximize the expected reward for the task it is performed on. The controller is trained using Proximal Policy Optimization (PPO) [  and thus improves sample efficiency. The loss of the evaluator L e is the Huber loss [22] between the evaluator's predictionr and the normalized true performance score. Further details of L c and L e can be found in the Appendix.\n\nTo optimize the encoder, we take L c and L e as part of the objective. The resulting variational lower bound for each task T is\nL = E z\u223cq \u03c6 (z|c T ) [L c + L e + \u03b2D KL (q \u03c6 (z|c T )||p(z))],(2)\nwhere D KL serves as an approximation to a variational information bottleneck that constrains the mutual information between z and c, as is shown in [1,38]. This information bottleneck acts as a regularizer to avoid overfitting to training tasks. \u03b2 is the weight of D KL in the objective, and p(z) is a unit Gaussian prior. Since (1) the latent context z serves as input to both controller and evaluator, and (2) q \u03c6 (z|c) and p(z) are Gaussian, with D KL computed using their mean and variance, gradient of Eq. 2 can be back-propagated end-to-end to the encoder with the reparameterization trick.\n\n\nExperiments\n\n\nImplementation Details\n\nWe use Multi-layer Perceptrons (MLP) as the controller policy network to generate the probability of choosing a certain action. The parameters \u03b8 c of the controller is trained on-policy via the PPO algorithm. We mask invalid actions by zeroing out their probabilities in the controller's outputs, then softmax the remaining probabilities and sample actions accordingly.\n\nThe evaluator is an MLP to generate the predicted score of a network. In the meta-training phase, we reset in the -greedy exploration strategy each time when the agent initializes a new task. We sample 80% of the entries as a batch from the replay buffer using PER. The encoder MLP outputs a 10-dim latent context vector z, and the weight of the KL-Divergence \u03b2 in the combined loss is set to be 0.1. More details of the components' hyperparameters can be found in the Appendix.\n\n\nBenchmark on NAS-Bench-201\n\nAs recent work [56] indicated, NAS algorithms are usually compared unfairly under different settings. To mitigate such problems, we first tested CATCH on NAS-Bench-201. It is a benchmark dataset that enables fair comparisons among NAS methods under the same configurations. It supports searching over cellbased architectures, where a directed acyclic graph represents each cell with 4 nodes and 5 possible connection operations on each edge. It provides the validation and test accuracies of 15,625 architectures on CIFAR-10, CIFAR-100, and ImageNet16-120 datasets. ImageNet16-120 is a subdataset for ImageNet, which downsampled all its images to 16 \u00d7 16, and contains only the first 120 classes of ImageNet.\n\nExperiment Settings. In the meta-training phase, each task is formed as a classification task on an X-class sub-dataset of ImageNet16 (ImageNet downsampled to 16 \u00d7 16) to maintain consistency with the configurations in NAS-Bench-201. The number of classes X \u2208 [10,20,30]. In each meta-epoch, the agent searches 20 networks whose validation accuracies after 12 training epochs are used as the reward signals. The hyperparameters used for training the networks in both phases are identical to those in NAS-Bench-201. In the following experiments, CATCH-meta is meta-trained with 25 meta epochs for 10.5 GPU hours on Tesla V100. We apply the same configurations as those in NAS-Bench-201.\n\nComparison with Sample-based Algorithms. We display the search results of the meta-trained version (CATCH-meta) and the search-from-scratch Table 1: Comparison of CATCH with one-shot algorithms. The top accuracies of identified models, standard deviations, search time (hour), total search time (hour), and the highest validation accuracies among all the networks in NAS-Bench-201 are reported. The same three random seeds are used to run through each algorithm. The time budget for search on CIFAR-10, CIFAR-100, and ImageNet16-120 are 3, 4, and 5 hours respectively.  Figure 3 presents the search results on CIFAR-10, CIFAR-100, ImageNet16-120, with the highest validation accuracy on each task. The reproduced results are consistent with the experiments performed in NAS-Bench-201. The performance of CATCH-sfs is similar to the other four methods, but CATCH-meta dominates all other algorithms in the searched network accuracies. On CIFAR-10, CATCH-meta finds the best model in 280/500 trials. On CIFAR-100, over half of them find top-3 performance networks within 50 samples, while other algorithms barely touch the roof. On ImageNet16-120, CATCH reaches the best network for more than 22% trials. We can see tremendous benefits for using the meta-trained CATCH to reduce time and cost.\n\nComparison with One-shot Algorithms. One of the central controversies around meta-NAS algorithms is: given the high searching efficiency of one-shot methods, can sample-based algorithms outperform them? We therefore compare the performance of CATCH with many state-of-the-art one-shot NAS solutions. For fair comparisons, instead of querying the NAS-Bench-201 network database, we train each child network for 12 epochs and obtain their early-stop validation accuracies as training feedbacks. The early-stop training setup is the same as the one in the meta-training phase. The one-shot algorithms involved are first-order DARTS (DARTS-V1) [33], second-order DARTS (DARTS-V2), GDAS [12], Random NAS (R-NAS) [26], ENAS [37], and SETN [11]. We run the algorithms with  Figure 4 presents the learning curves of each algorithm in the first 100 search epochs. For CATCH, at each search epoch, we identify networks with the best partially trained accuracy found so far, and report their fully trained accuracies. Both DARTS and ENAS have a relatively strong performance at the beginning, but the curves drop significantly afterward. SETN resembles Random NAS a lot. GDAS is among the best one-shot algorithms, but it seems to plateau at local maximums after a few search epochs. CATCH has the best performance among all, as it quickly adapts and identifies promising architectures that are beyond other algorithms' search capacity.\n\nIn Table 1, we report the best fully trained accuracy of networks that each algorithm identifies over their complete training process. We set the time budget for CATCH to search on CIFAR-10, CIFAR-100, and ImageNet16-120 as 3, 4, and 5 hours. It is roughly equivalent to cutting the search on these tasks at 70, 50, and 40 search epochs, respectively. Although DARTS-V1, R-NAS, and ENAS spend less time in total, they are highly unstable and the performance of DARTS and ENAS tends to deteriorate over time. CATCH spends 22.5 (10.5 meta + 12 adaptation) hours on all three tasks, and its searched networks surpass all other algorithms. The presented results have proved that CATCH is swiftly adaptive, and it is able to identify networks beyond many one-shot algorithms' reach within a reasonable time.\n\n\nExperiments on Residual Block-based Search Space\n\nHaving proved that CATCH can adapt to new tasks efficiently with metatraining, we further inquire whether CATCH has the ability to transfer across different domains including image classification, objection detection, and semantic segmentation. In this section, we consider a more challenging setting where  [15,20,25,30] , where P (m) denotes the model 's performance (e.g. validation accuracy for classification, mAP for object detection or mIoU for semantic segmentation), LAT (m) measures the model's inference latency, and T target is the target latency. w serves as a hyperparameter adjusting the performance-latency tradeoff. In our experiments, we set w = \u22120.05. With this reward, we hope to find models that excel not only in performance but also in inference speed. We meta train CATCHer for 5 GPU days, and adapt on each target task to search for 10 architectures. We target ImageNet dataset for image classification, COCO dataset for object detection and Cityscapes dataset for semantic segmentation. The detailed settings can be found in the Appendix.  Table 3 and Table 4. Our network again shows faster inference time and competitive performance. We also transfer CATCH-Net-B found during the search on ImageNet to COCO and Cityscapes, which yield 42% mAP with 136ms inference time and 80.87% mIoU (MS) with 52ms latency, respectively. Our results again show that directly transferring top architectures from one task to another cannot guarantee optimality. It also reveals CATCH's potentials to transfer across tasks even when they are distant from the meta-training ones.\n\n\nAblation Study\n\nThe context encoder is the spotlight component of our algorithm. We are especially curious about: (1) Is the encoder actually helpful for adaptation (compared with simply plugging in the meta-learned controller and evaluator priors)? (2) If so, does the improvement come from good estimates of the posterior, or is it from the stochastic generation of z that encourages exploration and benefits generalization?\n\nTo answer these questions, we designed two extra sets of experiments: (1) CATCH-zero: We set z = 0, and thereby completely eliminate the encoder's effect on both the controller and the evaluator; (2) CATCH-random: We sample each z from a unit Gaussian prior N (0, diag(1)) during the search as random inputs. The results are presented in Figure 5 (a)-(c). In both settings, the agents are still meta-trained for 10.5 hours before they are plugged in for adaptation.\n\nThe gaps among the lines in Figure 5 answered our questions. The encoder not only helps with adaptation (through comparing CATCH-meta and CATCHzero), but also provides assistance in a much more meaningful way than using random inputs for exploration, as CATCH-meta outperforms CATCH-random on both CIFAR-10 and CIFAR-100. Interestingly, we observe less significant improvement on ImageNet16-120. One hypothesis is since we perform the metatraining phase on sub-datasets of ImageNet16, the meta-trained controller and evaluator are already tuned towards policies that fit the search on ImageNet16. Hence, the transferred policies require less adaptation assistance from the encoder. More ablation studies can be found in the Appendix.\n\n\nConclusion and Discussion\n\nIn this work, we propose CATCH, a transferrable NAS approach, by designing an efficient learning framework that leverages the benefits of context-based meta reinforcement learning. The key contribution of CATCH is to boost NAS efficiency by extracting and utilizing task-specific latent contexts, while maintaining universality and robustness in various settings. Experiments and ablation studies show its dominant position in search efficiency and performance over non-transferrable schemes on NAS-Bench-201. Extensive experiments on residual block-based search space also demonstrate its capability in handling cross-task architecture search. As a task-agnostic transferrable NAS framework, CATCH possesses great potentials in scaling NAS to large datasets and various domains efficiently. During our research into transferrable NAS frameworks, we identified many potentially valuable questions to be explored. Efficient adaptation among domains is challenging, and we demonstrated a first attempt to simplify it by searching for backbones with a shared search space. A possible future investigation would be to generalize cross-task architecture search to flexibly include more decisions, such as searching for detection and segmentation heads. Meanwhile, our meta-training tasks involve only classification tasks, but it is also possible to diversify the pool and explore whether it leads to further performance boosts.  , CIFAR-100 [6], and ImageNet16-120 [3].\n\nWe compare the learning curve of CATCH with other sample-based algorithms in Figure 1. We plot each curve with the highest fully-train validation accuracy the agent has seen at each search epoch. Each curve is plotted with an average of 500 trials. The shaded area shows the mean \u00b1 standard deviation among all trials at each search epoch. CATCH stands out among others with higher performance and lower variation on all three datasets (CIFAR-10, CIFAR-100, and ImageNet16-120). It is also on average a magnitude faster than other algorithms to find their best architectures after 500 searching epochs. On ImageNet16-120, none of the algorithms except CATCH could even identify the best architecture within 500 searching epochs across all 500 trials. CATCH is also more stable, as is indicated by its much lower variation compared with other algorithms. Its variance tends to shrink over time, while R-EA and REINFORCE policies are almost as unstable as random search. Through this comparison, we further prove the adaptation speed and stability of CATCH, along with its competency across various datasets and random seeds.\n\n\nEncoder's Adaptation Result\n\nThroughout the adaptation process, we hypothesize that the encoder can provide dataset-specific guidance to the controller and the evaluator. To test this hypothesis, we visualize the encoded latent context vector z of each dataset through Principle Component Analysis, with the results presented in Figure 2. Each point is generated by randomly selecting and encoding 80% network-reward pairs from the search history. We freeze the weights of the meta-trained controller and evaluator policy, and only allow gradient updates for the encoder. This operation eliminates influence from the changing controller and evaluator policies, and thus enables us to closely observe just the behaviors of the encoder. When the encoder is first adapted to CIFAR-10, CIFAR-100, and ImageNet16-120, the generated context vectors are not distinguishable across the three datasets. However, after just 10 search epochs of adaptation, we can already identify a cluster of ImageNet16-120 context vectors. The clusters then quickly evolve as the encoder sees more architectures. By the 50-th search epoch, we can see three distinctive clusters as a result of the encoder's fast adaptation towards the three datasets. This observation is consistent with the results of NAS-Bench-201 [3]. In the original paper, the network-performance pairs have higher correlation between CIFAR-10 and CIFAR-100 (0.968) than that between CIFAR-10 and ImageNet16-120 (0.827). This correlation is also higher than the correlation between CIFAR-100 and ImageNet16-120 (0.91). This attributes to the reason why the encoder takes more search epochs to distinguish CIFAR-10 from CIFAR-100. The results are in support of our hypothesis, and show the encoder's capability to learn and express dataset-specific information effectively. \n\n\nAblation Study on the Evaluator\n\nWe also explored the effects of the evaluator by eliminating it from both the meta-training and adaptation phase, and its performance is presented in Figure  3 (a)-(c). As the figure shows, the evaluator lifts the performance by a large margin, making it a crucial component in the search algorithm. Table 1 provides further information on the evaluator when comparing it with CATCH using ground truth as the evaluator (CATCH-GT). CATCH-GT is a hard-to-defeat baseline, but CATCH-meta managed to get very close to it and the global max accuracy. 4 CATCHer Training Details\n\n\nController Settings and Hyperparameters\n\nThe controller is trained with Proximal Policy Optimization (PPO) [10] algorithm, and its loss L c is defined following the original PPO loss:\nL c =\u00ca t min r t (\u03b8 c )\u00c2 t , clip (r t (\u03b8 c ) , 1 \u2212 , 1 + )\u00c2 t\nis the PPO clipping parameter, r t (\u03b8 c ) = \u03c0 \u03b8c (a l |st) \u03c0 \u03b8 old (a l |st) is the probability ratio, and\u00c2 t is the General Advantage Estimate (GAE) [9] estimate:\nA t = t l=0 (\u03b3\u03bb) l \u03b4 V l where \u03b4 V l = r t + \u03b3V (s l+1 ) \u2212 V (s l )\nis the Bellman residual term. The definition of s l can be found in Table 3. We show the training hyperparameters and our settings on translating architecture search elements as Markov Decision Processes (MDP) in the following tables.  Latent context and the current network design.\n\n\nEncoder and Evaluator Settings\n\nThe encoder generates the latent conext through the network-reward information (m, r). This is done by taking the encoder output as the means and variances of a D-dimensional Gaussian distribution, from which we sample z. We provide pseudocode for this process in Algorithm 1. The evaluator uses the Huber loss [5] to close the gap between its predicted network performancer and the actual performance r. L e = 1 n i loss(r i ,r i ), where loss(r,r) = 0.5(r i \u2212r i ) 2 if | r i \u2212r i |< 1, | r i \u2212r i | \u22120.5 otherwise.\n\n(1)  5 ImageNet, COCO, and Cityscapes Training Settings Table 6-8 shows our training configurations on ImageNet [2] , COCO [7], and Cityscapes [1] . On COCO, Faster R-CNN with the ResNet backbone and Cascade FPN is used as our baseline. It is extremely costly to perform ImageNet pretrain for search, but training detection networks without ImageNet pretrain was made possible by [4]. For COCO and Cityscapes, we use Group Normalization with halved-base-channel groups instead of Batch Normalization. Conv2D with weight standardization (ConvWS2D) is also applied.  \n\n\nFig. 2: The search procedure of CATCH on a given task. The procedure starts from initializing the search history by storing a randomly selected network m and its reward r. The encoder applies amortized variational inference approach to generate latent context encoding z by encoding network-reward pairs from the search history. The controller then generates candidate networks for the evaluator to choose the most promising ones to train and evaluate. Newly selected networks and their rewards will be stored in the search history. The loop continues after the three components are optimized.\n\nFig. 3 :\n3(a)-(c) show the results of 500 trials for CATCH-meta, CATCH-sfs(search from scratch) and other sample-based algorithms. Each individual trial is sorted by the final validation accuracy of the searched network.\n\nFig. 4 :\n4Learning curves of one-shot algorithms and CATCH. Each curve is an average of three runs. We plot the first 100 search epochs for algorithms except for DARTS, which is trained only for 50 search epochs.\n\n\n, (3) choose the number of stages s, which is either 4 or 5, (4) schedule the number of blocks contained in each stage, and (5) arrange the distribution of blocks holding different channels. Details of the Residual block-based search space can be found in the Appendix.Experiment Settings. We use the same meta-training settings as the ones we used in NAS-Bench-201. For each meta epoch, an ImageNet sub-dataset is created. To form such sub-datasets, we sample X classes from all classes of ImageNet, where X \u2208[10,20,30]. Then the images are resize to 16 \u00d7 16, 32 \u00d7 32, or 224 \u00d7 224. Thus there are 3 sub-datasets.To achieve the balance between inference latency and network performance, we adopt the multi-objectve reward function R = P (m) \u00d7 [ LAT (m) Ttarget ] w in[46]\n\nFig. 5 :\n5(a)-(c) compare results of 500 trials for CATCH-meta, CATCH-sfs(search from scratch), CATCH-zero, CATCH-random.\n\nFig. 1 :\n1Comparison of CATCH with other sample-based algorithms on CIFAR-10[6]\n\nFig. 2 :\n2The encoder's adaptation process. It learns to distinguish different datasets throughout the learning process, and thus provide informed input to the controller and the evaluator.\n\nFig. 3 :\n3Comparison of CATCH-meta, CATCH-sfs with CATCH-withoutevaluator. Including the evaluator significantly raises the performance.\n\nAlgorithm 1\n1Pseudocode of Latent Context Encoding Procedure in a PyTorchlike style. def encode_z(B, D, Contexts, Encoder): # Contexts: a batch of contexts {(m, r)} use for encoding # B: len(Contexts), batch # D: the dimension of latent context variable z # Encoder: 3-layer MLP mapping (m, r) to (mean, var) of z_i # encode each (m, r) to (mean, var) of z context_batch.rewards = normalize(context_batch.rewards) params = Encoder.forward(context_batch) # shape: [B, 2 * D] # get mean and var; t(): matrix transpose means = params[..., :D].t() # shape: [D, B] vars = F.softplus(params[..., D:].t()) # shape: [D, B] # get mean & var of each z_i; ds: torch.distributions posteriors = [] for ms, vs in zip(unbind(means), unbind(vars)): z_i_mean, z_i_var = _product_of_gaussian(ms, vs) # form a Gaussian Posterior from z_i_mean, sqrt(z_i_var) z_i_posterior = ds.Gaussian(z_i_mean, sqrt(z_i_var)) posteriors.append(z_i_posterior) # sample z from q(z|Contexts); rsample(): random sample z = [d.rsample() for d in posteriors] return torch.stack(z)\n\n\n43] with a clipped surrogate objective L c . To optimize the evaluator, we deploy Prioritized Experience Replay (PER) [42], a Deep Q-learning [34] optimization technique. During the update, it prompts the evaluator to prioritize sampling entries that it makes the most mistakes on,0 \n100 \n200 \n300 \n400 \n500 \n\ntrial number \n\n89.0 \n\n89.5 \n\n90.0 \n\n90.5 \n\n91.0 \n\n91.5 \n\n92.0 \n\nvalidation accuracy \n\nCIFAR-10 \n\nGlobal Max \nRS \nREINFORCE \nR-EA \nCATCH-sfs \nCATCH-meta \n\n(a) CIFAR-10 \n\n0 \n100 \n200 \n300 \n400 \n500 \n\ntrial number \n\n68 \n\n69 \n\n70 \n\n71 \n\n72 \n\n73 \n\n74 \n\nvalidation accuracy \n\nCIFAR-100 \n\nGlobal Max \nRS \nREINFORCE \nR-EA \nCATCH-sfs \nCATCH-meta \n\n(b) CIFAR-100 \n\n0 \n100 \n200 \n300 \n400 \n500 \n\ntrial number \n\n41 \n\n42 \n\n43 \n\n44 \n\n45 \n\n46 \n\n47 \n\n48 \n\nvalidation accuracy \n\nImageNet16-120 \n\nGlobal Max \nRS \nREINFORCE \nR-EA \nCATCH-sfs \nCATCH-meta \n\n(c) ImageNet16-120 \n\n\n\n\nversion (CATCH-sfs where the meta-training phase is skipped) of our method, and compare them with other sample-based algorithms: Random Search (RS) [2], Regularized Evolution Algorithm (R-EA) [40], and REINFORCE [51]. The results of other methods are reproduced by running the code and configurations originally provided by NAS-bench-201. Each experiment is repeated for 500 trials with different seeds. The algorithms are trained for 50 search epochs in each trial.Algorithm \nCIFAR-10 \nCIFAR-100 \nImageNet16-120 Total Time \nAcc \u00b1std Time Acc\u00b1std Time Acc\u00b1std Time \nDARTS-V1 [33] 88.08\u00b11.89 2.46 68.99\u00b11.93 2.44 23.66\u00b10 4.55 \n9.45 \nDARTS-V2 [33] 87.16\u00b10.39 \n9 \n65.06\u00b12.95 7.91 26.29\u00b10 22.14 \n39.05 \nGDAS [12] \n90.32\u00b10.08 \n6 \n70.33\u00b10.85 6.23 44.81\u00b10.97 17 \n29.23 \nR-NAS [26] \n90.45\u00b10.43 2.19 70.39\u00b11.36 2.26 44.12\u00b11.04 5.94 \n10.39 \nENAS [37] \n90.2\u00b10.63 4.22 69.99\u00b11.03 4.26 44.92\u00b10.51 5.18 \n13.66 \nSETN [11] \n90.26\u00b10.75 7.62 68.01\u00b10.21 7.74 41.04\u00b11.64 20.33 \n35.69 \nCATCH-meta 91.33\u00b10.07 3 72.57\u00b10.81 4 46.07\u00b10.6 5 \n22.5 \nMax Acc. \n91.719 \n73.45 \n47.19 \n-\n\n\n\nTable 2 :\n2Results on ImageNet compared to manually designed and NAS searched architectures. Latency is measured on one Tesla V100 with one image with shape (3, 720, 1080).Network \nTop-1 Acc (%) Top-5 Acc (%) Latency (ms) \nResNet50 [19] \n77.15 \n93.29 \n16.4 \nDenseNet201 [20] \n77.42 \n93.66 \n31.6 \nResNext101 [54] \n79.31 \n94.5 \n76.7 \nInception-V3 [45] \n78.8 \n94.4 \n16.4 \nEfficientNet-B1 [47] \n77.3 \n93.5 \n29.5 \nEfficientNet-B2 \n79.2 \n94.5 \n47.6 \nNASNet-A [61] \n78.6 \n94.2 \n-\nBASE [44] \n74.3 \n91.9 \n-\nCATCH-Net-A \n79.04 \n94.43 \n16.9 \nCATCH-Net-B \n79.46 \n94.7 \n33.7 \n\nthe original code and configurations released from NAS-Bench-201. DARTS-V1 \nand DARTS-V2 are run for 50 search epochs, and other algorithms are trained \nfor 250 search epochs. \n\n\nTable 3 :\n3Results on COCO compared to manually designed and NAS searched backbones. Latency results of networks except CATCH are referred from[57].the meta-training phase contain only image classification tasks while tasks in all the three domains are targeted in the adaptation phase. The architectures are very different among these domains, so we search for their common component -the feature extractor (backbone). ResNet is one popular backbone for these tasks, thus we design the search space following[49,57].Constructing a model in the Residual block-based search space requires the controller to make several decisions: (1) select the network's base channel from [48, 56, 64, 72], (2) decide the network's depth withinMethod \nBackbone \nInput size Latency (ms) mAP \nRetinaNet [29] \nResNet101-FPN 1333x800 91.7 (V100) 39.1 \nFSAF [59] \nResNet101-FPN 1333x800 92.5 (V100) 40.9 \nGA-Faster RCNN [48] ResNet50-FPN 1333x800 104.2 (V100) 39.8 \nFaster-RCNN [41] \nResNet101-FPN 1333x800 84.0 (V100) 39.4 \nMask-RCNN [18] \nResNet101-FPN 1333x800 105.0 (V100) 40.2 \nDetNAS [8] \nSearched Backbone 1333x800 \n-\n42.0 \nSM-NAS: E3 \nSearched Backbone 800x600 50.7(V100) 42.8 \nSM-NAS: E5 \nSearched Backbone 1333x800 108.1(V100) 45.9 \nAuto-FPN [55] \nSearched Backbone 1333x800 \n-\n40.5 \nCATCH \nCATCH-Net-C 1333x800 123.5 (V100) 43.2 \n\n\n\nTable 4 :\n4Results on Cityscapes compared to manually designed and NAS searched backbones. Latency is measured on Tesla V100 with one image with shape (3, 1024, 1024). SS and MS denote for single scale and multiple scale testing respectively. 82X and 4.54X faster. CATCH-Net-B outperforms ResNext-101 while shortens the latency by 2.28X. The network comparison on COCO and Cityscapes is presented inMethod \nBackbone \nLatency (ms) mIoU (SS) mIoU (MS) \nBiSeNet [58] \nResNet101 \n41 \n-\n80.3 \nDeepLabv3+ [7] \nXception-65 \n85 \n77.82 \n79.3 \nCCNet [21] \nResNet50 \n175 \n-\n78.5 \nDUC [50] \nResNet152 \n-\n76.7 \n-\nDANet [17] \nResNet50 \n-\n76.34 \n-\nAuto-DeepLab [32] Searched Backbone \n-\n79.94 \n-\nDPC [6] \nXception-71 \n-\n80.1 \n-\nCATCH \nCATCH-Net-D \n27 \n79.52 \n81.12 \n\nSearch Results. Table 2 compares the searched architectures with other widely-\nrecognized networks on ImageNet. CATCH-Net-A outperforms many listed net-\nworks. Its accuracy is comparable with EfficientNet-B1 and ResNext-101, yet it \nis 2.\n\n\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015. 43. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 44. Albert Shaw, Wei Wei, Weiyang Liu, Le Song, and Bo Dai. Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946, 2019. 48. Jiaqi Wang, Kai Chen, Shuo Yang, Chen Change Loy, and Dahua Lin.34. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis \nAntonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep rein-\nforcement learning. arXiv preprint arXiv:1312.5602, 2013. \n35. Renato Negrinho and Geoff Gordon. Deeparchitect: Automatically designing and \ntraining deep architectures. arXiv preprint arXiv:1704.08792, 2017. \n36. Ramakanth Pasunuru and Mohit Bansal. Continual and multi-task architecture \nsearch. arXiv preprint arXiv:1906.05226, 2019. \n37. Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient \nneural architecture search via parameter sharing. arXiv preprint arXiv:1802.03268, \n2018. \n38. Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. \nEfficient off-policy meta-reinforcement learning via probabilistic context variables. \narXiv preprint arXiv:1903.08254, 2019. \n39. E Real, A Aggarwal, Y Huang, and QV Le. Aging evolution for image classifier \narchitecture search. In AAAI Conference on Artificial Intelligence, 2019. \n40. Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolu-\ntion for image classifier architecture search. In Proceedings of the AAAI Conference \non Artificial Intelligence, volume 33, pages 4780-4789, 2019. \n41. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards \nreal-time object detection with region proposal networks. In Advances in neural \ninformation processing systems, pages 91-99, 2015. \n42. Meta architecture \nsearch. In Advances in Neural Information Processing Systems, pages 11225-11235, \n2019. \n45. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew \nWojna. Rethinking the inception architecture for computer vision. In Proceedings \nof the IEEE conference on computer vision and pattern recognition, pages 2818-\n2826, 2016. \n46. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew \nHoward, and Quoc V. Le. Mnasnet: Platform-aware neural architecture search for \nmobile. In The IEEE Conference on Computer Vision and Pattern Recognition \n(CVPR), June 2019. \n47. Region \nproposal by guided anchoring. In Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition, pages 2965-2974, 2019. \n49. Ning Wang, Yang Gao, Hao Chen, Peng Wang, Zhi Tian, and Chunhua Shen. NAS-\nFCOS: fast neural architecture search for object detection. CoRR, abs/1906.04423, \n2019. \n50. P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and G. Cottrell. Under-\nstanding convolution for semantic segmentation. In 2018 IEEE Winter Conference \non Applications of Computer Vision (WACV), pages 1451-1460, March 2018. \n51. Ronald J Williams. Simple statistical gradient-following algorithms for connec-\ntionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992. \n52. Martin Wistuba. Finding competitive network architectures within a day using \nuct. arXiv preprint arXiv:1712.07420, 2017. \n1 The University of Hong Kong \n2 Huawei Noah's Ark Lab \n3 Sun Yat-sen University \n4 The Hong Kong University of Science and Technology \n\n1 Learning Curve Comparison with Sample-based \nAlgorithms \n\n0 \n100 \n200 \n300 \n400 \n500 \n\nepoch \n\n90.6 \n\n90.8 \n\n91.0 \n\n91.2 \n\n91.4 \n\n91.6 \n\n91.8 \n\naccuracy \n\nCIFAR-10 \n\nGlobal Max \nRS \nREINFORCE \nR-EA \nCATCH-meta \n\n\n\nTable 1 :\n1Comparison of CATCH when using ground truth as the evaluator (CATCH-GT), CATCH without evaluator (CATCH-w/o-evaluator), and CATCH-meta. The results are taken from 100 trials where each trail contains 50 search epochs. We report the mean \u00b1 std for each setting in thetable. CIFAR-10 CIFAR-100 ImageNet16-120 CATCH-GT 91.64\u00b10.09 73.31\u00b10.16 47.18\u00b10.09 CATCH-w/o-evaluator 91.17\u00b10.25 72.08\u00b10.6845.86\u00b10.54 \n\n\nTable 2 :\n2Controller hyperparametersHyperparameter \nValue \nNAS-Bench-201 [3] \nResidual Block \n(meta-train) \n(adaptation) \nSearch Space (adaptation) \nLearning rate \n0.001 \n0.001 \n0.0001 \nAdam scheduler step size \n20 \n20 \n20 \nAdam scheduler gamma \n0.99 \n0.99 \n0.99 \nUpdate frequency \n1 epoch \n1 epoch \n1 epoch \nClipping parameter \n0.2 \n0.2 \n0.2 \nMemory size \n200 \n200 \n200 \nDiscount \u03b3 \n0.99 \n0.99 \n0.99 \nGAE parameter \u03bb \n0.95 \n0.95 \n0.95 \nValue Function coeff. \n1 \n1 \n1 \nEntropy coeff. \n0.01 \n0.03 \n0.05 \n\n\n\nTable 3 :\n3A mapping of Neural Architecture Search elements to MDP factors for controller training. l denotes the current timestep. Invalid actions are masked by zeroing out their probabilities in the outputs, then softmax the remaining probabilities and sample accordingly. Current state s l (z, [a1...a l\u22121 ]) Latent context and the current network design. Current action a a l A one-hot vector of the current design choice. Reward r R A function of the evaluated network's performance. Next state s l+1 (z, [a1...a l ])MDP Factor \nValue \nExplanation \n\n\nTable 4 :\n4Encoder hyperparametersHyperparameter Value \nLearning rate 0.01 \nDimension of z 10 \nKL weight \u03b2 \n0.1 \n\n\nTable 5 :\n5Evaluator hyperparametersHyperparameter \nValue \nValue \n(meta-train) (adaptation) \nLearning rate \n0.0001 \n0.0001 \nExploration factor initial value \n1.0 \n0.5 \nExploration factor decay rate \n0.025 \n0.025 \nExploration factor decay step \n20 \n20 \nNumber of networks evaluated per epoch \n25 \n25 \nPER [8] prioritization factor \u03b1 \n0.5 \n0.5 \nPER bias correction factor \u03b2 \n0.575 \n0.575 \nPER \u03b2 annealing step size \n0.01 \n0.01 \n\n\n\nTable 6 :\n6ImageNet training hyperparameters with 8 GPUs. Learning rate warmup linear for 3 epochs linear for 3 epochs Learning rate decay policyHyperparameter \nValue \nValue \n(partial-train) \n(fully-train) \nLearning rate \n0.1 \n0.1 \nLearning rate momentum \n0.9 \n0.9 \nWeight decay \n1 \u00d7 10 \u22123 \n4 \u00d7 10 \u22125 \ncosine \ncosine \nTotal epoch \n40 \n240 \nBatch size \n1024 \n512 \n\n\n\nTable 7 :\n7COCO training hyperparameters with 8 GPUs.Hyperparameters \nValue \nValue \n(partial-train) \n(fully-train) \nNormalization \nGroup Normalization Batch Normalization \nBatch size \n16 \n16 \nLearning rate \n0.18 \n0.02 \nLearning rate momentum \n0.9 \n0.9 \nWeight decay \n0.0001 \n0.0001 \nLearning rate decay policy \ncosine \nstep \nTotal epoch \n9 \n24 \n\nXin Chen 1 , Yawen Duan 1 , Zewei Chen 2 , Hang Xu 2 , Zihao Chen 2 , Xiaodan Liang 3 , Tong Zhang 4 , Zhenguo Li 2\nAppendix for CATCH: Context-based MetaReinforcement Learning for Transferrable Architecture Search We show an example model in our Residual Block search space inFigure 3. It consists of 5 stages, with depth=15, stage distribution=[3,3,4,5], and channel distribution=[2,2,4,7]. We use the same notation format to show the searched models inTable 9.[12,11]. C-N-R stands for a combination of Convolution layer, Normalization layer, and a ReLU operation.\nIan Alexander A Alemi, Joshua V Fischer, Kevin Dillon, Murphy, arXiv:1612.00410Deep variational information bottleneck. arXiv preprintAlexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep vari- ational information bottleneck. arXiv preprint arXiv:1612.00410, 2016.\n\nRandom search for hyper-parameter optimization. James Bergstra, Yoshua Bengio, Journal of machine learning research. 13James Bergstra and Yoshua Bengio. Random search for hyper-parameter opti- mization. Journal of machine learning research, 13(Feb):281-305, 2012.\n\nMaking a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. James Bergstra, Daniel Yamins, David Cox, International conference on machine learning. James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International conference on machine learning, pages 115-123, 2013.\n\nAlgorithms for hyper-parameter optimization. S James, R\u00e9mi Bergstra, Yoshua Bardenet, Bal\u00e1zs Bengio, K\u00e9gl, Advances in neural information processing systems. James S Bergstra, R\u00e9mi Bardenet, Yoshua Bengio, and Bal\u00e1zs K\u00e9gl. Algorithms for hyper-parameter optimization. In Advances in neural information processing systems, pages 2546-2554, 2011.\n\nSearching for efficient multiscale architectures for dense image prediction. Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, Jon Shlens, Advances in Neural Information Processing Systems. Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, and Jon Shlens. Searching for efficient multi- scale architectures for dense image prediction. In Advances in Neural Information Processing Systems, pages 8699-8710, 2018.\n\nSearching for efficient multi-scale architectures for dense image prediction. Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, Jon Shlens, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, and Jon Shlens. Searching for efficient multi-scale architectures for dense image prediction. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems 31, pages 8699-8710. Curran Associates, Inc., 2018.\n\nEncoder-decoder with atrous separable convolution for semantic image segmentation. Yukun Liang-Chieh Chen, George Zhu, Florian Papandreou, Hartwig Schroff, Adam, The European Conference on Computer Vision (ECCV). Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image seg- mentation. In The European Conference on Computer Vision (ECCV), September 2018.\n\nYukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng, Chunhong Pan, Jian Sun, Detnas, arXiv:1903.10979Neural architecture search on object detection. arXiv preprintYukang Chen, Tong Yang, Xiangyu Zhang, Gaofeng Meng, Chunhong Pan, and Jian Sun. Detnas: Neural architecture search on object detection. arXiv preprint arXiv:1903.10979, 2019.\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus En- zweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.\n\nOne-shot neural architecture search via self-evaluated template network. Xuanyi Dong, Yi Yang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionXuanyi Dong and Yi Yang. One-shot neural architecture search via self-evaluated template network. In Proceedings of the IEEE International Conference on Com- puter Vision, pages 3681-3690, 2019.\n\nSearching for a robust neural architecture in four gpu hours. Xuanyi Dong, Yi Yang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXuanyi Dong and Yi Yang. Searching for a robust neural architecture in four gpu hours. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1761-1770, 2019.\n\nNas-bench-201: Extending the scope of reproducible neural architecture search. Xuanyi Dong, Yi Yang, arXiv:2001.00326arXiv preprintXuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. arXiv preprint arXiv:2001.00326, 2020.\n\nEfficient multiobjective neural architecture search via lamarckian evolution. Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, arXiv:1804.09081arXiv preprintThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efficient multi- objective neural architecture search via lamarckian evolution. arXiv preprint arXiv:1804.09081, 2018.\n\nMeta-learning of neural architectures for few-shot learning. Thomas Elsken, Benedikt Staffler, Jan Hendrik Metzen, Frank Hutter, arXiv:1911.11090arXiv preprintThomas Elsken, Benedikt Staffler, Jan Hendrik Metzen, and Frank Hutter. Meta-learning of neural architectures for few-shot learning. arXiv preprint arXiv:1911.11090, 2019.\n\nModel-agnostic meta-learning for fast adaptation of deep networks. Chelsea Finn, Pieter Abbeel, Sergey Levine, arXiv:1703.03400arXiv preprintChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.\n\nDual attention network for scene segmentation. Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\nMask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961- 2969, 2017.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Q Weinberger, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700-4708, 2017.\n\nCcnet: Criss-cross attention for semantic segmentation. Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu, The IEEE International Conference on Computer Vision (ICCV). Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\n\nRobust estimation of a location parameter. J Peter, Huber, Breakthroughs in statistics. SpringerPeter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pages 492-518. Springer, 1992.\n\nAuto-meta: Automated gradient based meta learner search. Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee, Youngduck Choi, Yongseok Choi, Dong-Yeon Cho, Jiwon Kim, arXiv:1806.06927arXiv preprintJaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee, Young- duck Choi, Yongseok Choi, Dong-Yeon Cho, and Jiwon Kim. Auto-meta: Auto- mated gradient based meta learner search. arXiv preprint arXiv:1806.06927, 2018.\n\nAuto-encoding variational bayes. P Diederik, Max Kingma, Welling, arXiv:1312.6114arXiv preprintDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\nLin Lan, Zhenguo Li, Xiaohong Guan, Pinghui Wang, arXiv:1905.06527Meta reinforcement learning with task embedding and shared policy. arXiv preprintLin Lan, Zhenguo Li, Xiaohong Guan, and Pinghui Wang. Meta reinforcement learning with task embedding and shared policy. arXiv preprint arXiv:1905.06527, 2019.\n\nRandom search and reproducibility for neural architecture search. Liam Li, Ameet Talwalkar, arXiv:1902.07638arXiv preprintLiam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. arXiv preprint arXiv:1902.07638, 2019.\n\nMeta-sgd: Learning to learn quickly for few-shot learning. Zhenguo Li, Fengwei Zhou, Fei Chen, Hang Li, arXiv:1707.09835arXiv preprintZhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017.\n\nTowards fast adaptation of neural architectures with meta learning. Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang, Shenghua Gao, International Conference on Learning Representations. Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang, and Shenghua Gao. Towards fast adaptation of neural architectures with meta learning. In International Conference on Learning Representations, 2020.\n\nKaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980-2988, 2017.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, Larry Zitnick, ECCV. European Conference on Computer Vision. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Larry Zitnick. Microsoft coco: Common objects in context. In ECCV. European Conference on Computer Vision, September 2014.\n\nAutofis: Automatic feature interaction selection in factorization models for click-through rate prediction. Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xiuqiang He, Zhenguo Li, Yong Yu, arXiv:2003.11235arXiv preprintBin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming Tang, Xi- uqiang He, Zhenguo Li, and Yong Yu. Autofis: Automatic feature interaction selection in factorization models for click-through rate prediction. arXiv preprint arXiv:2003.11235, 2020.\n\nAuto-deeplab: Hierarchical neural architecture search for semantic image segmentation. Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, Li Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li Fei-Fei. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 82-92, 2019.\n\nHanxiao Liu, Karen Simonyan, Yiming Yang, arXiv:1806.09055Darts: Differentiable architecture search. arXiv preprintHanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018.\n\nTransfer learning with neural automl. Catherine Wong, Neil Houlsby, Yifeng Lu, Andrea Gesmundo, Advances in Neural Information Processing Systems. Catherine Wong, Neil Houlsby, Yifeng Lu, and Andrea Gesmundo. Transfer learn- ing with neural automl. In Advances in Neural Information Processing Systems, pages 8356-8365, 2018.\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggre- gated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1492-1500, 2017.\n\nAuto-fpn: Automatic network architecture adaptation for object detection beyond classification. Hang Xu, Lewei Yao, Wei Zhang, Xiaodan Liang, Zhenguo Li, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionHang Xu, Lewei Yao, Wei Zhang, Xiaodan Liang, and Zhenguo Li. Auto-fpn: Au- tomatic network architecture adaptation for object detection beyond classification. In Proceedings of the IEEE International Conference on Computer Vision, pages 6649-6658, 2019.\n\nNas evaluation is frustratingly hard. Antoine Yang, Pedro M Esperan\u00e3, Fabio M Carlucci, International Conference on Learning Representations. Antoine Yang, Pedro M. Esperan\u00c3 \u00a7a, and Fabio M. Carlucci. Nas evaluation is frustratingly hard. In International Conference on Learning Representations, 2020.\n\nSm-nas: Structural-to-modular neural architecture search for object detection. Lewei Yao, Hang Xu, Wei Zhang, Xiaodan Liang, Zhenguo Li, arXiv:1911.09929arXiv preprintLewei Yao, Hang Xu, Wei Zhang, Xiaodan Liang, and Zhenguo Li. Sm-nas: Structural-to-modular neural architecture search for object detection. arXiv preprint arXiv:1911.09929, 2019.\n\nBisenet: Bilateral segmentation network for real-time semantic segmentation. Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang, The European Conference on Computer Vision (ECCV). Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In The European Conference on Computer Vision (ECCV), September 2018.\n\nFeature selective anchor-free module for single-shot object detection. Chenchen Zhu, Yihui He, Marios Savvides, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChenchen Zhu, Yihui He, and Marios Savvides. Feature selective anchor-free mod- ule for single-shot object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 840-849, 2019.\n\nBarret Zoph, V Quoc, Le, arXiv:1611.01578Neural architecture search with reinforcement learning. arXiv preprintBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learn- ing. arXiv preprint arXiv:1611.01578, 2016.\n\nSearched Model Input Depth Stage Channel FLOPS(G) Params(MB) Channel Distribution Distribution CATCH-Net-A. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V Le, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition64Learning transferable architectures for scalable image recognition. 2, 7, 8, 3. 5, 4, 8, 3] 4.45 25.96 CATCH-Net-B 64 25 [8, 5, 8, 4] [3, 10, 8, 4. 5, 4, 5, 6] [1, 8, 5, 6] 8.08 37.03 CATCH-Net-D 64 20 [1, 8, 5, 6] [2, 7, 7, 4Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning trans- ferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8697-8710, 2018. Searched Model Input Depth Stage Channel FLOPS(G) Params(MB) Channel Distribution Distribution CATCH-Net-A 64 20 [2, 7, 8, 3] [5, 4, 8, 3] 4.45 25.96 CATCH-Net-B 64 25 [8, 5, 8, 4] [3, 10, 8, 4] 9.84 32.16 CATCH-Net-C 64 20 [5, 4, 5, 6] [1, 8, 5, 6] 8.08 37.03 CATCH-Net-D 64 20 [1, 8, 5, 6] [2, 7, 7, 4]\n\nThe cityscapes dataset for semantic urban scene understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, Bernt Schiele, Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus En- zweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.\n\nNas-bench-201: Extending the scope of reproducible neural architecture search. Xuanyi Dong, Yi Yang, arXiv:2001.00326arXiv preprintXuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. arXiv preprint arXiv:2001.00326, 2020.\n\nRethinking imagenet pre-training. Kaiming He, Ross B Girshick, Piotr Doll\u00e1r, abs/1811.08883CoRRKaiming He, Ross B. Girshick, and Piotr Doll\u00e1r. Rethinking imagenet pre-training. CoRR, abs/1811.08883, 2018.\n\nRobust estimation of a location parameter. J Peter, Huber, Breakthroughs in statistics. SpringerPeter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pages 492-518. Springer, 1992.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, Larry Zitnick, ECCV. European Conference on Computer Vision. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Larry Zitnick. Microsoft coco: Common objects in context. In ECCV. European Conference on Computer Vision, September 2014.\n\n. Tom Schaul, John Quan, Ioannis Antonoglou, David Silver, arXiv:1511.05952Prioritized experience replay. arXiv preprintTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized expe- rience replay. arXiv preprint arXiv:1511.05952, 2015.\n\nHigh-dimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.02438arXiv preprintJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nNAS-FCOS: fast neural architecture search for object detection. Ning Wang, Yang Gao, Hao Chen, Peng Wang, Zhi Tian, Chunhua Shen, abs/1906.04423CoRRNing Wang, Yang Gao, Hao Chen, Peng Wang, Zhi Tian, and Chunhua Shen. NAS- FCOS: fast neural architecture search for object detection. CoRR, abs/1906.04423, 2019.\n\nSm-nas: Structural-to-modular neural architecture search for object detection. Lewei Yao, Hang Xu, Wei Zhang, Xiaodan Liang, Zhenguo Li, arXiv:1911.09929arXiv preprintLewei Yao, Hang Xu, Wei Zhang, Xiaodan Liang, and Zhenguo Li. Sm-nas: Structural-to-modular neural architecture search for object detection. arXiv preprint arXiv:1911.09929, 2019.\n\nBisenet: Bilateral segmentation network for real-time semantic segmentation. Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang, The European Conference on Computer Vision (ECCV). Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In The European Conference on Computer Vision (ECCV), September 2018.\n", "annotations": {"author": "[{\"end\":129,\"start\":90},{\"end\":171,\"start\":130},{\"end\":207,\"start\":172},{\"end\":240,\"start\":208},{\"end\":252,\"start\":241},{\"end\":316,\"start\":253},{\"end\":381,\"start\":317},{\"end\":417,\"start\":382}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":94},{\"end\":140,\"start\":136},{\"end\":182,\"start\":178},{\"end\":215,\"start\":213},{\"end\":251,\"start\":247},{\"end\":266,\"start\":261},{\"end\":327,\"start\":322},{\"end\":392,\"start\":390}]", "author_first_name": "[{\"end\":93,\"start\":90},{\"end\":135,\"start\":130},{\"end\":177,\"start\":172},{\"end\":212,\"start\":208},{\"end\":246,\"start\":241},{\"end\":260,\"start\":253},{\"end\":321,\"start\":317},{\"end\":389,\"start\":382}]", "author_affiliation": "[{\"end\":128,\"start\":100},{\"end\":170,\"start\":142},{\"end\":206,\"start\":184},{\"end\":239,\"start\":217},{\"end\":290,\"start\":268},{\"end\":315,\"start\":292},{\"end\":380,\"start\":329},{\"end\":416,\"start\":394}]", "title": "[{\"end\":87,\"start\":1},{\"end\":504,\"start\":418}]", "venue": null, "abstract": "[{\"end\":1821,\"start\":561}]", "bib_ref": "[{\"end\":2181,\"start\":2177},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2184,\"start\":2181},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2207,\"start\":2203},{\"end\":2234,\"start\":2231},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2267,\"start\":2263},{\"end\":2801,\"start\":2798},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3127,\"start\":3123},{\"end\":3130,\"start\":3127},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3514,\"start\":3510},{\"end\":3517,\"start\":3514},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3758,\"start\":3754},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3761,\"start\":3758},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3784,\"start\":3780},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3787,\"start\":3784},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3840,\"start\":3836},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3843,\"start\":3840},{\"end\":4732,\"start\":4728},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4735,\"start\":4732},{\"end\":5408,\"start\":5405},{\"end\":5411,\"start\":5408},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5414,\"start\":5411},{\"end\":5930,\"start\":5926},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5959,\"start\":5955},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5962,\"start\":5959},{\"end\":6093,\"start\":6089},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6352,\"start\":6348},{\"end\":6458,\"start\":6454},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6603,\"start\":6599},{\"end\":6648,\"start\":6645},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7898,\"start\":7894},{\"end\":7925,\"start\":7922},{\"end\":7927,\"start\":7925},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7956,\"start\":7952},{\"end\":7959,\"start\":7956},{\"end\":7993,\"start\":7989},{\"end\":7996,\"start\":7993},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8092,\"start\":8088},{\"end\":8095,\"start\":8092},{\"end\":8098,\"start\":8095},{\"end\":8121,\"start\":8117},{\"end\":8124,\"start\":8121},{\"end\":8793,\"start\":8790},{\"end\":8860,\"start\":8857},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9133,\"start\":9129},{\"end\":9136,\"start\":9133},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9334,\"start\":9330},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9613,\"start\":9609},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9616,\"start\":9613},{\"end\":13660,\"start\":13658},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13663,\"start\":13660},{\"end\":18190,\"start\":18189},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18281,\"start\":18277},{\"end\":18769,\"start\":18766},{\"end\":18772,\"start\":18769},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20154,\"start\":20150},{\"end\":21012,\"start\":21004},{\"end\":21109,\"start\":21105},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21112,\"start\":21109},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21115,\"start\":21112},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23469,\"start\":23465},{\"end\":23511,\"start\":23507},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23536,\"start\":23532},{\"end\":23562,\"start\":23558},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25419,\"start\":25415},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25422,\"start\":25419},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25425,\"start\":25422},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":25428,\"start\":25425},{\"end\":26951,\"start\":26948},{\"end\":29796,\"start\":29793},{\"end\":29820,\"start\":29817},{\"end\":32243,\"start\":32240},{\"end\":33490,\"start\":33486},{\"end\":33779,\"start\":33776},{\"end\":34489,\"start\":34486},{\"end\":34809,\"start\":34806},{\"end\":34820,\"start\":34817},{\"end\":34840,\"start\":34837},{\"end\":35077,\"start\":35074},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36811,\"start\":36808},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":36814,\"start\":36811},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41461,\"start\":41457},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":41830,\"start\":41827}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":35855,\"start\":35260},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36077,\"start\":35856},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36291,\"start\":36078},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37066,\"start\":36292},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37189,\"start\":37067},{\"attributes\":{\"id\":\"fig_7\"},\"end\":37270,\"start\":37190},{\"attributes\":{\"id\":\"fig_8\"},\"end\":37461,\"start\":37271},{\"attributes\":{\"id\":\"fig_9\"},\"end\":37599,\"start\":37462},{\"attributes\":{\"id\":\"fig_10\"},\"end\":38641,\"start\":37600},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39510,\"start\":38642},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40569,\"start\":39511},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41312,\"start\":40570},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42635,\"start\":41313},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43627,\"start\":42636},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47479,\"start\":43628},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47894,\"start\":47480},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":48401,\"start\":47895},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":48957,\"start\":48402},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":49072,\"start\":48958},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":49501,\"start\":49073},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":49867,\"start\":49502},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":50214,\"start\":49868}]", "paragraph": "[{\"end\":2699,\"start\":1837},{\"end\":3105,\"start\":2701},{\"end\":4360,\"start\":3107},{\"end\":4977,\"start\":4362},{\"end\":5794,\"start\":4979},{\"end\":6968,\"start\":5796},{\"end\":7718,\"start\":6970},{\"end\":8153,\"start\":7735},{\"end\":8388,\"start\":8155},{\"end\":9045,\"start\":8390},{\"end\":10263,\"start\":9047},{\"end\":11735,\"start\":10283},{\"end\":11989,\"start\":11737},{\"end\":12495,\"start\":12010},{\"end\":13395,\"start\":12497},{\"end\":14119,\"start\":13397},{\"end\":14363,\"start\":14181},{\"end\":14823,\"start\":14365},{\"end\":15266,\"start\":14844},{\"end\":16172,\"start\":15268},{\"end\":16471,\"start\":16235},{\"end\":16516,\"start\":16473},{\"end\":16545,\"start\":16518},{\"end\":16574,\"start\":16547},{\"end\":16789,\"start\":16755},{\"end\":16824,\"start\":16791},{\"end\":16873,\"start\":16826},{\"end\":17037,\"start\":16875},{\"end\":17598,\"start\":17072},{\"end\":17994,\"start\":17600},{\"end\":18421,\"start\":18022},{\"end\":18550,\"start\":18423},{\"end\":19214,\"start\":18617},{\"end\":19624,\"start\":19255},{\"end\":20104,\"start\":19626},{\"end\":20843,\"start\":20135},{\"end\":21530,\"start\":20845},{\"end\":22823,\"start\":21532},{\"end\":24250,\"start\":22825},{\"end\":25054,\"start\":24252},{\"end\":26695,\"start\":25107},{\"end\":27124,\"start\":26714},{\"end\":27591,\"start\":27126},{\"end\":28326,\"start\":27593},{\"end\":29821,\"start\":28356},{\"end\":30946,\"start\":29823},{\"end\":32768,\"start\":30978},{\"end\":33376,\"start\":32804},{\"end\":33562,\"start\":33420},{\"end\":33789,\"start\":33626},{\"end\":34140,\"start\":33858},{\"end\":34692,\"start\":34175},{\"end\":35259,\"start\":34694}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14180,\"start\":14120},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16754,\"start\":16575},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18616,\"start\":18551},{\"attributes\":{\"id\":\"formula_3\"},\"end\":33625,\"start\":33563},{\"attributes\":{\"id\":\"formula_4\"},\"end\":33857,\"start\":33790}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":21679,\"start\":21672},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24262,\"start\":24255},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26180,\"start\":26173},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":26192,\"start\":26185},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":33111,\"start\":33104},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33933,\"start\":33926},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":34757,\"start\":34750}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1835,\"start\":1823},{\"attributes\":{\"n\":\"2\"},\"end\":7733,\"start\":7721},{\"attributes\":{\"n\":\"3\"},\"end\":10281,\"start\":10266},{\"attributes\":{\"n\":\"3.1\"},\"end\":12008,\"start\":11992},{\"attributes\":{\"n\":\"3.2\"},\"end\":14842,\"start\":14826},{\"end\":16233,\"start\":16175},{\"attributes\":{\"n\":\"3.3\"},\"end\":17070,\"start\":17040},{\"attributes\":{\"n\":\"3.4\"},\"end\":18020,\"start\":17997},{\"attributes\":{\"n\":\"4\"},\"end\":19228,\"start\":19217},{\"attributes\":{\"n\":\"4.1\"},\"end\":19253,\"start\":19231},{\"attributes\":{\"n\":\"4.2\"},\"end\":20133,\"start\":20107},{\"attributes\":{\"n\":\"4.3\"},\"end\":25105,\"start\":25057},{\"attributes\":{\"n\":\"5\"},\"end\":26712,\"start\":26698},{\"attributes\":{\"n\":\"6\"},\"end\":28354,\"start\":28329},{\"attributes\":{\"n\":\"2\"},\"end\":30976,\"start\":30949},{\"attributes\":{\"n\":\"3\"},\"end\":32802,\"start\":32771},{\"attributes\":{\"n\":\"4.1\"},\"end\":33418,\"start\":33379},{\"attributes\":{\"n\":\"4.2\"},\"end\":34173,\"start\":34143},{\"end\":35865,\"start\":35857},{\"end\":36087,\"start\":36079},{\"end\":37076,\"start\":37068},{\"end\":37199,\"start\":37191},{\"end\":37280,\"start\":37272},{\"end\":37471,\"start\":37463},{\"end\":37612,\"start\":37601},{\"end\":40580,\"start\":40571},{\"end\":41323,\"start\":41314},{\"end\":42646,\"start\":42637},{\"end\":47490,\"start\":47481},{\"end\":47905,\"start\":47896},{\"end\":48412,\"start\":48403},{\"end\":48968,\"start\":48959},{\"end\":49083,\"start\":49074},{\"end\":49512,\"start\":49503},{\"end\":49878,\"start\":49869}]", "table": "[{\"end\":39510,\"start\":38925},{\"end\":40569,\"start\":39979},{\"end\":41312,\"start\":40743},{\"end\":42635,\"start\":42042},{\"end\":43627,\"start\":43036},{\"end\":47479,\"start\":44193},{\"end\":47894,\"start\":47882},{\"end\":48401,\"start\":47933},{\"end\":48957,\"start\":48925},{\"end\":49072,\"start\":48993},{\"end\":49501,\"start\":49110},{\"end\":49867,\"start\":49648},{\"end\":50214,\"start\":49922}]", "figure_caption": "[{\"end\":35855,\"start\":35262},{\"end\":36077,\"start\":35867},{\"end\":36291,\"start\":36089},{\"end\":37066,\"start\":36294},{\"end\":37189,\"start\":37078},{\"end\":37270,\"start\":37201},{\"end\":37461,\"start\":37282},{\"end\":37599,\"start\":37473},{\"end\":38641,\"start\":37614},{\"end\":38925,\"start\":38644},{\"end\":39979,\"start\":39513},{\"end\":40743,\"start\":40582},{\"end\":42042,\"start\":41325},{\"end\":43036,\"start\":42648},{\"end\":44193,\"start\":43630},{\"end\":47882,\"start\":47492},{\"end\":47933,\"start\":47907},{\"end\":48925,\"start\":48414},{\"end\":48993,\"start\":48970},{\"end\":49110,\"start\":49085},{\"end\":49648,\"start\":49514},{\"end\":49922,\"start\":49880}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":2372,\"start\":2366},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":5133,\"start\":5125},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":11941,\"start\":11933},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22110,\"start\":22102},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23600,\"start\":23592},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27472,\"start\":27464},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27629,\"start\":27621},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29908,\"start\":29900},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":31286,\"start\":31278},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32963,\"start\":32954}]", "bib_author_first_name": "[{\"end\":50786,\"start\":50783},{\"end\":50812,\"start\":50806},{\"end\":50814,\"start\":50813},{\"end\":50829,\"start\":50824},{\"end\":51120,\"start\":51115},{\"end\":51137,\"start\":51131},{\"end\":51451,\"start\":51446},{\"end\":51468,\"start\":51462},{\"end\":51482,\"start\":51477},{\"end\":51811,\"start\":51810},{\"end\":51823,\"start\":51819},{\"end\":51840,\"start\":51834},{\"end\":51857,\"start\":51851},{\"end\":52199,\"start\":52188},{\"end\":52213,\"start\":52206},{\"end\":52228,\"start\":52223},{\"end\":52240,\"start\":52234},{\"end\":52259,\"start\":52253},{\"end\":52273,\"start\":52266},{\"end\":52290,\"start\":52283},{\"end\":52300,\"start\":52297},{\"end\":52731,\"start\":52720},{\"end\":52745,\"start\":52738},{\"end\":52760,\"start\":52755},{\"end\":52772,\"start\":52766},{\"end\":52791,\"start\":52785},{\"end\":52805,\"start\":52798},{\"end\":52822,\"start\":52815},{\"end\":52832,\"start\":52829},{\"end\":53488,\"start\":53483},{\"end\":53513,\"start\":53507},{\"end\":53526,\"start\":53519},{\"end\":53546,\"start\":53539},{\"end\":53858,\"start\":53852},{\"end\":53869,\"start\":53865},{\"end\":53883,\"start\":53876},{\"end\":53898,\"start\":53891},{\"end\":53913,\"start\":53905},{\"end\":53923,\"start\":53919},{\"end\":54261,\"start\":54255},{\"end\":54277,\"start\":54270},{\"end\":54294,\"start\":54285},{\"end\":54306,\"start\":54302},{\"end\":54322,\"start\":54316},{\"end\":54341,\"start\":54334},{\"end\":54355,\"start\":54352},{\"end\":54370,\"start\":54364},{\"end\":54382,\"start\":54377},{\"end\":54896,\"start\":54893},{\"end\":54906,\"start\":54903},{\"end\":54920,\"start\":54913},{\"end\":54935,\"start\":54929},{\"end\":54943,\"start\":54940},{\"end\":54950,\"start\":54948},{\"end\":55328,\"start\":55322},{\"end\":55337,\"start\":55335},{\"end\":55729,\"start\":55723},{\"end\":55738,\"start\":55736},{\"end\":56164,\"start\":56158},{\"end\":56173,\"start\":56171},{\"end\":56438,\"start\":56432},{\"end\":56450,\"start\":56447},{\"end\":56458,\"start\":56451},{\"end\":56472,\"start\":56467},{\"end\":56751,\"start\":56745},{\"end\":56768,\"start\":56760},{\"end\":56782,\"start\":56779},{\"end\":56790,\"start\":56783},{\"end\":56804,\"start\":56799},{\"end\":57090,\"start\":57083},{\"end\":57103,\"start\":57097},{\"end\":57118,\"start\":57112},{\"end\":57362,\"start\":57359},{\"end\":57371,\"start\":57367},{\"end\":57383,\"start\":57377},{\"end\":57394,\"start\":57390},{\"end\":57406,\"start\":57399},{\"end\":57418,\"start\":57412},{\"end\":57432,\"start\":57425},{\"end\":57742,\"start\":57735},{\"end\":57754,\"start\":57747},{\"end\":57770,\"start\":57765},{\"end\":57783,\"start\":57779},{\"end\":58140,\"start\":58133},{\"end\":58152,\"start\":58145},{\"end\":58168,\"start\":58160},{\"end\":58178,\"start\":58174},{\"end\":58575,\"start\":58572},{\"end\":58589,\"start\":58583},{\"end\":58602,\"start\":58595},{\"end\":58627,\"start\":58619},{\"end\":59063,\"start\":59057},{\"end\":59079,\"start\":59071},{\"end\":59092,\"start\":59086},{\"end\":59105,\"start\":59100},{\"end\":59120,\"start\":59113},{\"end\":59131,\"start\":59126},{\"end\":59461,\"start\":59460},{\"end\":59699,\"start\":59692},{\"end\":59713,\"start\":59705},{\"end\":59726,\"start\":59719},{\"end\":59738,\"start\":59732},{\"end\":59748,\"start\":59744},{\"end\":59753,\"start\":59749},{\"end\":59768,\"start\":59759},{\"end\":59783,\"start\":59775},{\"end\":59799,\"start\":59790},{\"end\":59810,\"start\":59805},{\"end\":60109,\"start\":60108},{\"end\":60123,\"start\":60120},{\"end\":60280,\"start\":60277},{\"end\":60293,\"start\":60286},{\"end\":60306,\"start\":60298},{\"end\":60320,\"start\":60313},{\"end\":60655,\"start\":60651},{\"end\":60665,\"start\":60660},{\"end\":60908,\"start\":60901},{\"end\":60920,\"start\":60913},{\"end\":60930,\"start\":60927},{\"end\":60941,\"start\":60937},{\"end\":61198,\"start\":61192},{\"end\":61208,\"start\":61205},{\"end\":61222,\"start\":61216},{\"end\":61235,\"start\":61227},{\"end\":61244,\"start\":61240},{\"end\":61256,\"start\":61250},{\"end\":61270,\"start\":61263},{\"end\":61286,\"start\":61278},{\"end\":61659,\"start\":61651},{\"end\":61670,\"start\":61665},{\"end\":61682,\"start\":61678},{\"end\":62072,\"start\":62064},{\"end\":62085,\"start\":62078},{\"end\":62098,\"start\":62093},{\"end\":62114,\"start\":62109},{\"end\":62127,\"start\":62121},{\"end\":62140,\"start\":62136},{\"end\":62155,\"start\":62150},{\"end\":62169,\"start\":62164},{\"end\":62564,\"start\":62561},{\"end\":62576,\"start\":62570},{\"end\":62588,\"start\":62582},{\"end\":62599,\"start\":62593},{\"end\":62613,\"start\":62607},{\"end\":62626,\"start\":62619},{\"end\":62641,\"start\":62633},{\"end\":62653,\"start\":62646},{\"end\":62662,\"start\":62658},{\"end\":63050,\"start\":63044},{\"end\":63067,\"start\":63056},{\"end\":63081,\"start\":63074},{\"end\":63098,\"start\":63091},{\"end\":63108,\"start\":63105},{\"end\":63118,\"start\":63114},{\"end\":63120,\"start\":63119},{\"end\":63131,\"start\":63129},{\"end\":63579,\"start\":63572},{\"end\":63590,\"start\":63585},{\"end\":63607,\"start\":63601},{\"end\":63863,\"start\":63854},{\"end\":63874,\"start\":63870},{\"end\":63890,\"start\":63884},{\"end\":63901,\"start\":63895},{\"end\":64212,\"start\":64205},{\"end\":64222,\"start\":64218},{\"end\":64238,\"start\":64233},{\"end\":64254,\"start\":64247},{\"end\":64266,\"start\":64259},{\"end\":64752,\"start\":64748},{\"end\":64762,\"start\":64757},{\"end\":64771,\"start\":64768},{\"end\":64786,\"start\":64779},{\"end\":64801,\"start\":64794},{\"end\":65228,\"start\":65221},{\"end\":65240,\"start\":65235},{\"end\":65242,\"start\":65241},{\"end\":65258,\"start\":65253},{\"end\":65260,\"start\":65259},{\"end\":65570,\"start\":65565},{\"end\":65580,\"start\":65576},{\"end\":65588,\"start\":65585},{\"end\":65603,\"start\":65596},{\"end\":65618,\"start\":65611},{\"end\":65920,\"start\":65911},{\"end\":65931,\"start\":65925},{\"end\":65942,\"start\":65938},{\"end\":65957,\"start\":65949},{\"end\":65967,\"start\":65963},{\"end\":65976,\"start\":65972},{\"end\":66337,\"start\":66329},{\"end\":66348,\"start\":66343},{\"end\":66359,\"start\":66353},{\"end\":66739,\"start\":66733},{\"end\":66747,\"start\":66746},{\"end\":67083,\"start\":67077},{\"end\":67095,\"start\":67090},{\"end\":67115,\"start\":67107},{\"end\":67130,\"start\":67124},{\"end\":68116,\"start\":68110},{\"end\":68132,\"start\":68125},{\"end\":68149,\"start\":68140},{\"end\":68161,\"start\":68157},{\"end\":68177,\"start\":68171},{\"end\":68196,\"start\":68189},{\"end\":68210,\"start\":68207},{\"end\":68225,\"start\":68219},{\"end\":68237,\"start\":68232},{\"end\":68751,\"start\":68748},{\"end\":68761,\"start\":68758},{\"end\":68775,\"start\":68768},{\"end\":68790,\"start\":68784},{\"end\":68798,\"start\":68795},{\"end\":68805,\"start\":68803},{\"end\":69189,\"start\":69183},{\"end\":69198,\"start\":69196},{\"end\":69420,\"start\":69413},{\"end\":69429,\"start\":69425},{\"end\":69431,\"start\":69430},{\"end\":69447,\"start\":69442},{\"end\":69629,\"start\":69628},{\"end\":69862,\"start\":69858},{\"end\":69883,\"start\":69875},{\"end\":70046,\"start\":70038},{\"end\":70059,\"start\":70052},{\"end\":70072,\"start\":70067},{\"end\":70088,\"start\":70083},{\"end\":70101,\"start\":70095},{\"end\":70114,\"start\":70110},{\"end\":70129,\"start\":70124},{\"end\":70143,\"start\":70138},{\"end\":70432,\"start\":70429},{\"end\":70445,\"start\":70441},{\"end\":70459,\"start\":70452},{\"end\":70477,\"start\":70472},{\"end\":70761,\"start\":70757},{\"end\":70779,\"start\":70772},{\"end\":70794,\"start\":70788},{\"end\":70810,\"start\":70803},{\"end\":70825,\"start\":70819},{\"end\":71065,\"start\":71061},{\"end\":71081,\"start\":71076},{\"end\":71098,\"start\":71090},{\"end\":71113,\"start\":71109},{\"end\":71127,\"start\":71123},{\"end\":71435,\"start\":71431},{\"end\":71446,\"start\":71442},{\"end\":71455,\"start\":71452},{\"end\":71466,\"start\":71462},{\"end\":71476,\"start\":71473},{\"end\":71490,\"start\":71483},{\"end\":71763,\"start\":71758},{\"end\":71773,\"start\":71769},{\"end\":71781,\"start\":71778},{\"end\":71796,\"start\":71789},{\"end\":71811,\"start\":71804},{\"end\":72113,\"start\":72104},{\"end\":72124,\"start\":72118},{\"end\":72135,\"start\":72131},{\"end\":72150,\"start\":72142},{\"end\":72160,\"start\":72156},{\"end\":72169,\"start\":72165}]", "bib_author_last_name": "[{\"end\":50804,\"start\":50787},{\"end\":50822,\"start\":50815},{\"end\":50836,\"start\":50830},{\"end\":50844,\"start\":50838},{\"end\":51129,\"start\":51121},{\"end\":51144,\"start\":51138},{\"end\":51460,\"start\":51452},{\"end\":51475,\"start\":51469},{\"end\":51486,\"start\":51483},{\"end\":51817,\"start\":51812},{\"end\":51832,\"start\":51824},{\"end\":51849,\"start\":51841},{\"end\":51864,\"start\":51858},{\"end\":51870,\"start\":51866},{\"end\":52204,\"start\":52200},{\"end\":52221,\"start\":52214},{\"end\":52232,\"start\":52229},{\"end\":52251,\"start\":52241},{\"end\":52264,\"start\":52260},{\"end\":52281,\"start\":52274},{\"end\":52295,\"start\":52291},{\"end\":52307,\"start\":52301},{\"end\":52736,\"start\":52732},{\"end\":52753,\"start\":52746},{\"end\":52764,\"start\":52761},{\"end\":52783,\"start\":52773},{\"end\":52796,\"start\":52792},{\"end\":52813,\"start\":52806},{\"end\":52827,\"start\":52823},{\"end\":52839,\"start\":52833},{\"end\":53505,\"start\":53489},{\"end\":53517,\"start\":53514},{\"end\":53537,\"start\":53527},{\"end\":53554,\"start\":53547},{\"end\":53560,\"start\":53556},{\"end\":53863,\"start\":53859},{\"end\":53874,\"start\":53870},{\"end\":53889,\"start\":53884},{\"end\":53903,\"start\":53899},{\"end\":53917,\"start\":53914},{\"end\":53927,\"start\":53924},{\"end\":53935,\"start\":53929},{\"end\":54268,\"start\":54262},{\"end\":54283,\"start\":54278},{\"end\":54300,\"start\":54295},{\"end\":54314,\"start\":54307},{\"end\":54332,\"start\":54323},{\"end\":54350,\"start\":54342},{\"end\":54362,\"start\":54356},{\"end\":54375,\"start\":54371},{\"end\":54390,\"start\":54383},{\"end\":54901,\"start\":54897},{\"end\":54911,\"start\":54907},{\"end\":54927,\"start\":54921},{\"end\":54938,\"start\":54936},{\"end\":54946,\"start\":54944},{\"end\":54958,\"start\":54951},{\"end\":55333,\"start\":55329},{\"end\":55342,\"start\":55338},{\"end\":55734,\"start\":55730},{\"end\":55743,\"start\":55739},{\"end\":56169,\"start\":56165},{\"end\":56178,\"start\":56174},{\"end\":56445,\"start\":56439},{\"end\":56465,\"start\":56459},{\"end\":56479,\"start\":56473},{\"end\":56758,\"start\":56752},{\"end\":56777,\"start\":56769},{\"end\":56797,\"start\":56791},{\"end\":56811,\"start\":56805},{\"end\":57095,\"start\":57091},{\"end\":57110,\"start\":57104},{\"end\":57125,\"start\":57119},{\"end\":57365,\"start\":57363},{\"end\":57375,\"start\":57372},{\"end\":57388,\"start\":57384},{\"end\":57397,\"start\":57395},{\"end\":57410,\"start\":57407},{\"end\":57423,\"start\":57419},{\"end\":57435,\"start\":57433},{\"end\":57745,\"start\":57743},{\"end\":57763,\"start\":57755},{\"end\":57777,\"start\":57771},{\"end\":57792,\"start\":57784},{\"end\":58143,\"start\":58141},{\"end\":58158,\"start\":58153},{\"end\":58172,\"start\":58169},{\"end\":58182,\"start\":58179},{\"end\":58581,\"start\":58576},{\"end\":58593,\"start\":58590},{\"end\":58617,\"start\":58603},{\"end\":58638,\"start\":58628},{\"end\":59069,\"start\":59064},{\"end\":59084,\"start\":59080},{\"end\":59098,\"start\":59093},{\"end\":59111,\"start\":59106},{\"end\":59124,\"start\":59121},{\"end\":59135,\"start\":59132},{\"end\":59467,\"start\":59462},{\"end\":59474,\"start\":59469},{\"end\":59703,\"start\":59700},{\"end\":59717,\"start\":59714},{\"end\":59730,\"start\":59727},{\"end\":59742,\"start\":59739},{\"end\":59757,\"start\":59754},{\"end\":59773,\"start\":59769},{\"end\":59788,\"start\":59784},{\"end\":59803,\"start\":59800},{\"end\":59814,\"start\":59811},{\"end\":60118,\"start\":60110},{\"end\":60130,\"start\":60124},{\"end\":60139,\"start\":60132},{\"end\":60284,\"start\":60281},{\"end\":60296,\"start\":60294},{\"end\":60311,\"start\":60307},{\"end\":60325,\"start\":60321},{\"end\":60658,\"start\":60656},{\"end\":60675,\"start\":60666},{\"end\":60911,\"start\":60909},{\"end\":60925,\"start\":60921},{\"end\":60935,\"start\":60931},{\"end\":60944,\"start\":60942},{\"end\":61203,\"start\":61199},{\"end\":61214,\"start\":61209},{\"end\":61225,\"start\":61223},{\"end\":61238,\"start\":61236},{\"end\":61248,\"start\":61245},{\"end\":61261,\"start\":61257},{\"end\":61276,\"start\":61271},{\"end\":61290,\"start\":61287},{\"end\":61663,\"start\":61660},{\"end\":61676,\"start\":61671},{\"end\":61691,\"start\":61683},{\"end\":62076,\"start\":62073},{\"end\":62091,\"start\":62086},{\"end\":62107,\"start\":62099},{\"end\":62119,\"start\":62115},{\"end\":62134,\"start\":62128},{\"end\":62148,\"start\":62141},{\"end\":62162,\"start\":62156},{\"end\":62177,\"start\":62170},{\"end\":62568,\"start\":62565},{\"end\":62580,\"start\":62577},{\"end\":62591,\"start\":62589},{\"end\":62605,\"start\":62600},{\"end\":62617,\"start\":62614},{\"end\":62631,\"start\":62627},{\"end\":62644,\"start\":62642},{\"end\":62656,\"start\":62654},{\"end\":62665,\"start\":62663},{\"end\":63054,\"start\":63051},{\"end\":63072,\"start\":63068},{\"end\":63089,\"start\":63082},{\"end\":63103,\"start\":63099},{\"end\":63112,\"start\":63109},{\"end\":63127,\"start\":63121},{\"end\":63139,\"start\":63132},{\"end\":63583,\"start\":63580},{\"end\":63599,\"start\":63591},{\"end\":63612,\"start\":63608},{\"end\":63868,\"start\":63864},{\"end\":63882,\"start\":63875},{\"end\":63893,\"start\":63891},{\"end\":63910,\"start\":63902},{\"end\":64216,\"start\":64213},{\"end\":64231,\"start\":64223},{\"end\":64245,\"start\":64239},{\"end\":64257,\"start\":64255},{\"end\":64269,\"start\":64267},{\"end\":64755,\"start\":64753},{\"end\":64766,\"start\":64763},{\"end\":64777,\"start\":64772},{\"end\":64792,\"start\":64787},{\"end\":64804,\"start\":64802},{\"end\":65233,\"start\":65229},{\"end\":65251,\"start\":65243},{\"end\":65269,\"start\":65261},{\"end\":65574,\"start\":65571},{\"end\":65583,\"start\":65581},{\"end\":65594,\"start\":65589},{\"end\":65609,\"start\":65604},{\"end\":65621,\"start\":65619},{\"end\":65923,\"start\":65921},{\"end\":65936,\"start\":65932},{\"end\":65947,\"start\":65943},{\"end\":65961,\"start\":65958},{\"end\":65970,\"start\":65968},{\"end\":65981,\"start\":65977},{\"end\":66341,\"start\":66338},{\"end\":66351,\"start\":66349},{\"end\":66368,\"start\":66360},{\"end\":66744,\"start\":66740},{\"end\":66752,\"start\":66748},{\"end\":66756,\"start\":66754},{\"end\":67088,\"start\":67084},{\"end\":67105,\"start\":67096},{\"end\":67122,\"start\":67116},{\"end\":67133,\"start\":67131},{\"end\":68123,\"start\":68117},{\"end\":68138,\"start\":68133},{\"end\":68155,\"start\":68150},{\"end\":68169,\"start\":68162},{\"end\":68187,\"start\":68178},{\"end\":68205,\"start\":68197},{\"end\":68217,\"start\":68211},{\"end\":68230,\"start\":68226},{\"end\":68245,\"start\":68238},{\"end\":68756,\"start\":68752},{\"end\":68766,\"start\":68762},{\"end\":68782,\"start\":68776},{\"end\":68793,\"start\":68791},{\"end\":68801,\"start\":68799},{\"end\":68813,\"start\":68806},{\"end\":69194,\"start\":69190},{\"end\":69203,\"start\":69199},{\"end\":69423,\"start\":69421},{\"end\":69440,\"start\":69432},{\"end\":69454,\"start\":69448},{\"end\":69635,\"start\":69630},{\"end\":69642,\"start\":69637},{\"end\":69873,\"start\":69863},{\"end\":69890,\"start\":69884},{\"end\":70050,\"start\":70047},{\"end\":70065,\"start\":70060},{\"end\":70081,\"start\":70073},{\"end\":70093,\"start\":70089},{\"end\":70108,\"start\":70102},{\"end\":70122,\"start\":70115},{\"end\":70136,\"start\":70130},{\"end\":70151,\"start\":70144},{\"end\":70439,\"start\":70433},{\"end\":70450,\"start\":70446},{\"end\":70470,\"start\":70460},{\"end\":70484,\"start\":70478},{\"end\":70770,\"start\":70762},{\"end\":70786,\"start\":70780},{\"end\":70801,\"start\":70795},{\"end\":70817,\"start\":70811},{\"end\":70832,\"start\":70826},{\"end\":71074,\"start\":71066},{\"end\":71088,\"start\":71082},{\"end\":71107,\"start\":71099},{\"end\":71121,\"start\":71114},{\"end\":71134,\"start\":71128},{\"end\":71440,\"start\":71436},{\"end\":71450,\"start\":71447},{\"end\":71460,\"start\":71456},{\"end\":71471,\"start\":71467},{\"end\":71481,\"start\":71477},{\"end\":71495,\"start\":71491},{\"end\":71767,\"start\":71764},{\"end\":71776,\"start\":71774},{\"end\":71787,\"start\":71782},{\"end\":71802,\"start\":71797},{\"end\":71814,\"start\":71812},{\"end\":72116,\"start\":72114},{\"end\":72129,\"start\":72125},{\"end\":72140,\"start\":72136},{\"end\":72154,\"start\":72151},{\"end\":72163,\"start\":72161},{\"end\":72174,\"start\":72170}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1612.00410\",\"id\":\"b0\"},\"end\":51065,\"start\":50783},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15700257},\"end\":51330,\"start\":51067},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3356163},\"end\":51763,\"start\":51332},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":11688126},\"end\":52109,\"start\":51765},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52195590},\"end\":52640,\"start\":52111},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52195590},\"end\":53398,\"start\":52642},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3638670},\"end\":53850,\"start\":53400},{\"attributes\":{\"doi\":\"arXiv:1903.10979\",\"id\":\"b7\"},\"end\":54190,\"start\":53852},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":502946},\"end\":54838,\"start\":54192},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":55247,\"start\":54840},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":204509097},\"end\":55659,\"start\":55249},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":198903996},\"end\":56077,\"start\":55661},{\"attributes\":{\"doi\":\"arXiv:2001.00326\",\"id\":\"b12\"},\"end\":56352,\"start\":56079},{\"attributes\":{\"doi\":\"arXiv:1804.09081\",\"id\":\"b13\"},\"end\":56682,\"start\":56354},{\"attributes\":{\"doi\":\"arXiv:1911.11090\",\"id\":\"b14\"},\"end\":57014,\"start\":56684},{\"attributes\":{\"doi\":\"arXiv:1703.03400\",\"id\":\"b15\"},\"end\":57310,\"start\":57016},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52180375},\"end\":57721,\"start\":57312},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":54465873},\"end\":58085,\"start\":57723},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":58528,\"start\":58087},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9433631},\"end\":58999,\"start\":58530},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":53846561},\"end\":59415,\"start\":59001},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":61846277},\"end\":59633,\"start\":59417},{\"attributes\":{\"doi\":\"arXiv:1806.06927\",\"id\":\"b22\"},\"end\":60073,\"start\":59635},{\"attributes\":{\"doi\":\"arXiv:1312.6114\",\"id\":\"b23\"},\"end\":60275,\"start\":60075},{\"attributes\":{\"doi\":\"arXiv:1905.06527\",\"id\":\"b24\"},\"end\":60583,\"start\":60277},{\"attributes\":{\"doi\":\"arXiv:1902.07638\",\"id\":\"b25\"},\"end\":60840,\"start\":60585},{\"attributes\":{\"doi\":\"arXiv:1707.09835\",\"id\":\"b26\"},\"end\":61122,\"start\":60842},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":213025655},\"end\":61580,\"start\":61124},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":206771220},\"end\":62019,\"start\":61582},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":14113767},\"end\":62451,\"start\":62021},{\"attributes\":{\"doi\":\"arXiv:2003.11235\",\"id\":\"b30\"},\"end\":62955,\"start\":62453},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":57761158},\"end\":63570,\"start\":62957},{\"attributes\":{\"doi\":\"arXiv:1806.09055\",\"id\":\"b32\"},\"end\":63814,\"start\":63572},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":52192622},\"end\":64141,\"start\":63816},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8485068},\"end\":64650,\"start\":64143},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":207975574},\"end\":65181,\"start\":64652},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":209516112},\"end\":65484,\"start\":65183},{\"attributes\":{\"doi\":\"arXiv:1911.09929\",\"id\":\"b37\"},\"end\":65832,\"start\":65486},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":51906325},\"end\":66256,\"start\":65834},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":67855967},\"end\":66731,\"start\":66258},{\"attributes\":{\"doi\":\"arXiv:1611.01578\",\"id\":\"b40\"},\"end\":66967,\"start\":66733},{\"attributes\":{\"id\":\"b41\"},\"end\":68045,\"start\":66969},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":502946},\"end\":68693,\"start\":68047},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":57246310},\"end\":69102,\"start\":68695},{\"attributes\":{\"doi\":\"arXiv:2001.00326\",\"id\":\"b44\"},\"end\":69377,\"start\":69104},{\"attributes\":{\"doi\":\"abs/1811.08883\",\"id\":\"b45\"},\"end\":69583,\"start\":69379},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":61846277},\"end\":69801,\"start\":69585},{\"attributes\":{\"id\":\"b47\"},\"end\":69993,\"start\":69803},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14113767},\"end\":70425,\"start\":69995},{\"attributes\":{\"doi\":\"arXiv:1511.05952\",\"id\":\"b49\"},\"end\":70679,\"start\":70427},{\"attributes\":{\"doi\":\"arXiv:1506.02438\",\"id\":\"b50\"},\"end\":71059,\"start\":70681},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b51\"},\"end\":71365,\"start\":71061},{\"attributes\":{\"doi\":\"abs/1906.04423\",\"id\":\"b52\"},\"end\":71677,\"start\":71367},{\"attributes\":{\"doi\":\"arXiv:1911.09929\",\"id\":\"b53\"},\"end\":72025,\"start\":71679},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":51906325},\"end\":72449,\"start\":72027}]", "bib_title": "[{\"end\":51113,\"start\":51067},{\"end\":51444,\"start\":51332},{\"end\":51808,\"start\":51765},{\"end\":52186,\"start\":52111},{\"end\":52718,\"start\":52642},{\"end\":53481,\"start\":53400},{\"end\":54253,\"start\":54192},{\"end\":54891,\"start\":54840},{\"end\":55320,\"start\":55249},{\"end\":55721,\"start\":55661},{\"end\":57357,\"start\":57312},{\"end\":57733,\"start\":57723},{\"end\":58131,\"start\":58087},{\"end\":58570,\"start\":58530},{\"end\":59055,\"start\":59001},{\"end\":59458,\"start\":59417},{\"end\":61190,\"start\":61124},{\"end\":61649,\"start\":61582},{\"end\":62062,\"start\":62021},{\"end\":63042,\"start\":62957},{\"end\":63852,\"start\":63816},{\"end\":64203,\"start\":64143},{\"end\":64746,\"start\":64652},{\"end\":65219,\"start\":65183},{\"end\":65909,\"start\":65834},{\"end\":66327,\"start\":66258},{\"end\":67075,\"start\":66969},{\"end\":68108,\"start\":68047},{\"end\":68746,\"start\":68695},{\"end\":69626,\"start\":69585},{\"end\":70036,\"start\":69995},{\"end\":72102,\"start\":72027}]", "bib_author": "[{\"end\":50806,\"start\":50783},{\"end\":50824,\"start\":50806},{\"end\":50838,\"start\":50824},{\"end\":50846,\"start\":50838},{\"end\":51131,\"start\":51115},{\"end\":51146,\"start\":51131},{\"end\":51462,\"start\":51446},{\"end\":51477,\"start\":51462},{\"end\":51488,\"start\":51477},{\"end\":51819,\"start\":51810},{\"end\":51834,\"start\":51819},{\"end\":51851,\"start\":51834},{\"end\":51866,\"start\":51851},{\"end\":51872,\"start\":51866},{\"end\":52206,\"start\":52188},{\"end\":52223,\"start\":52206},{\"end\":52234,\"start\":52223},{\"end\":52253,\"start\":52234},{\"end\":52266,\"start\":52253},{\"end\":52283,\"start\":52266},{\"end\":52297,\"start\":52283},{\"end\":52309,\"start\":52297},{\"end\":52738,\"start\":52720},{\"end\":52755,\"start\":52738},{\"end\":52766,\"start\":52755},{\"end\":52785,\"start\":52766},{\"end\":52798,\"start\":52785},{\"end\":52815,\"start\":52798},{\"end\":52829,\"start\":52815},{\"end\":52841,\"start\":52829},{\"end\":53507,\"start\":53483},{\"end\":53519,\"start\":53507},{\"end\":53539,\"start\":53519},{\"end\":53556,\"start\":53539},{\"end\":53562,\"start\":53556},{\"end\":53865,\"start\":53852},{\"end\":53876,\"start\":53865},{\"end\":53891,\"start\":53876},{\"end\":53905,\"start\":53891},{\"end\":53919,\"start\":53905},{\"end\":53929,\"start\":53919},{\"end\":53937,\"start\":53929},{\"end\":54270,\"start\":54255},{\"end\":54285,\"start\":54270},{\"end\":54302,\"start\":54285},{\"end\":54316,\"start\":54302},{\"end\":54334,\"start\":54316},{\"end\":54352,\"start\":54334},{\"end\":54364,\"start\":54352},{\"end\":54377,\"start\":54364},{\"end\":54392,\"start\":54377},{\"end\":54903,\"start\":54893},{\"end\":54913,\"start\":54903},{\"end\":54929,\"start\":54913},{\"end\":54940,\"start\":54929},{\"end\":54948,\"start\":54940},{\"end\":54960,\"start\":54948},{\"end\":55335,\"start\":55322},{\"end\":55344,\"start\":55335},{\"end\":55736,\"start\":55723},{\"end\":55745,\"start\":55736},{\"end\":56171,\"start\":56158},{\"end\":56180,\"start\":56171},{\"end\":56447,\"start\":56432},{\"end\":56467,\"start\":56447},{\"end\":56481,\"start\":56467},{\"end\":56760,\"start\":56745},{\"end\":56779,\"start\":56760},{\"end\":56799,\"start\":56779},{\"end\":56813,\"start\":56799},{\"end\":57097,\"start\":57083},{\"end\":57112,\"start\":57097},{\"end\":57127,\"start\":57112},{\"end\":57367,\"start\":57359},{\"end\":57377,\"start\":57367},{\"end\":57390,\"start\":57377},{\"end\":57399,\"start\":57390},{\"end\":57412,\"start\":57399},{\"end\":57425,\"start\":57412},{\"end\":57437,\"start\":57425},{\"end\":57747,\"start\":57735},{\"end\":57765,\"start\":57747},{\"end\":57779,\"start\":57765},{\"end\":57794,\"start\":57779},{\"end\":58145,\"start\":58133},{\"end\":58160,\"start\":58145},{\"end\":58174,\"start\":58160},{\"end\":58184,\"start\":58174},{\"end\":58583,\"start\":58572},{\"end\":58595,\"start\":58583},{\"end\":58619,\"start\":58595},{\"end\":58640,\"start\":58619},{\"end\":59071,\"start\":59057},{\"end\":59086,\"start\":59071},{\"end\":59100,\"start\":59086},{\"end\":59113,\"start\":59100},{\"end\":59126,\"start\":59113},{\"end\":59137,\"start\":59126},{\"end\":59469,\"start\":59460},{\"end\":59476,\"start\":59469},{\"end\":59705,\"start\":59692},{\"end\":59719,\"start\":59705},{\"end\":59732,\"start\":59719},{\"end\":59744,\"start\":59732},{\"end\":59759,\"start\":59744},{\"end\":59775,\"start\":59759},{\"end\":59790,\"start\":59775},{\"end\":59805,\"start\":59790},{\"end\":59816,\"start\":59805},{\"end\":60120,\"start\":60108},{\"end\":60132,\"start\":60120},{\"end\":60141,\"start\":60132},{\"end\":60286,\"start\":60277},{\"end\":60298,\"start\":60286},{\"end\":60313,\"start\":60298},{\"end\":60327,\"start\":60313},{\"end\":60660,\"start\":60651},{\"end\":60677,\"start\":60660},{\"end\":60913,\"start\":60901},{\"end\":60927,\"start\":60913},{\"end\":60937,\"start\":60927},{\"end\":60946,\"start\":60937},{\"end\":61205,\"start\":61192},{\"end\":61216,\"start\":61205},{\"end\":61227,\"start\":61216},{\"end\":61240,\"start\":61227},{\"end\":61250,\"start\":61240},{\"end\":61263,\"start\":61250},{\"end\":61278,\"start\":61263},{\"end\":61292,\"start\":61278},{\"end\":61665,\"start\":61651},{\"end\":61678,\"start\":61665},{\"end\":61693,\"start\":61678},{\"end\":62078,\"start\":62064},{\"end\":62093,\"start\":62078},{\"end\":62109,\"start\":62093},{\"end\":62121,\"start\":62109},{\"end\":62136,\"start\":62121},{\"end\":62150,\"start\":62136},{\"end\":62164,\"start\":62150},{\"end\":62179,\"start\":62164},{\"end\":62570,\"start\":62561},{\"end\":62582,\"start\":62570},{\"end\":62593,\"start\":62582},{\"end\":62607,\"start\":62593},{\"end\":62619,\"start\":62607},{\"end\":62633,\"start\":62619},{\"end\":62646,\"start\":62633},{\"end\":62658,\"start\":62646},{\"end\":62667,\"start\":62658},{\"end\":63056,\"start\":63044},{\"end\":63074,\"start\":63056},{\"end\":63091,\"start\":63074},{\"end\":63105,\"start\":63091},{\"end\":63114,\"start\":63105},{\"end\":63129,\"start\":63114},{\"end\":63141,\"start\":63129},{\"end\":63585,\"start\":63572},{\"end\":63601,\"start\":63585},{\"end\":63614,\"start\":63601},{\"end\":63870,\"start\":63854},{\"end\":63884,\"start\":63870},{\"end\":63895,\"start\":63884},{\"end\":63912,\"start\":63895},{\"end\":64218,\"start\":64205},{\"end\":64233,\"start\":64218},{\"end\":64247,\"start\":64233},{\"end\":64259,\"start\":64247},{\"end\":64271,\"start\":64259},{\"end\":64757,\"start\":64748},{\"end\":64768,\"start\":64757},{\"end\":64779,\"start\":64768},{\"end\":64794,\"start\":64779},{\"end\":64806,\"start\":64794},{\"end\":65235,\"start\":65221},{\"end\":65253,\"start\":65235},{\"end\":65271,\"start\":65253},{\"end\":65576,\"start\":65565},{\"end\":65585,\"start\":65576},{\"end\":65596,\"start\":65585},{\"end\":65611,\"start\":65596},{\"end\":65623,\"start\":65611},{\"end\":65925,\"start\":65911},{\"end\":65938,\"start\":65925},{\"end\":65949,\"start\":65938},{\"end\":65963,\"start\":65949},{\"end\":65972,\"start\":65963},{\"end\":65983,\"start\":65972},{\"end\":66343,\"start\":66329},{\"end\":66353,\"start\":66343},{\"end\":66370,\"start\":66353},{\"end\":66746,\"start\":66733},{\"end\":66754,\"start\":66746},{\"end\":66758,\"start\":66754},{\"end\":67090,\"start\":67077},{\"end\":67107,\"start\":67090},{\"end\":67124,\"start\":67107},{\"end\":67135,\"start\":67124},{\"end\":68125,\"start\":68110},{\"end\":68140,\"start\":68125},{\"end\":68157,\"start\":68140},{\"end\":68171,\"start\":68157},{\"end\":68189,\"start\":68171},{\"end\":68207,\"start\":68189},{\"end\":68219,\"start\":68207},{\"end\":68232,\"start\":68219},{\"end\":68247,\"start\":68232},{\"end\":68758,\"start\":68748},{\"end\":68768,\"start\":68758},{\"end\":68784,\"start\":68768},{\"end\":68795,\"start\":68784},{\"end\":68803,\"start\":68795},{\"end\":68815,\"start\":68803},{\"end\":69196,\"start\":69183},{\"end\":69205,\"start\":69196},{\"end\":69425,\"start\":69413},{\"end\":69442,\"start\":69425},{\"end\":69456,\"start\":69442},{\"end\":69637,\"start\":69628},{\"end\":69644,\"start\":69637},{\"end\":69875,\"start\":69858},{\"end\":69892,\"start\":69875},{\"end\":70052,\"start\":70038},{\"end\":70067,\"start\":70052},{\"end\":70083,\"start\":70067},{\"end\":70095,\"start\":70083},{\"end\":70110,\"start\":70095},{\"end\":70124,\"start\":70110},{\"end\":70138,\"start\":70124},{\"end\":70153,\"start\":70138},{\"end\":70441,\"start\":70429},{\"end\":70452,\"start\":70441},{\"end\":70472,\"start\":70452},{\"end\":70486,\"start\":70472},{\"end\":70772,\"start\":70757},{\"end\":70788,\"start\":70772},{\"end\":70803,\"start\":70788},{\"end\":70819,\"start\":70803},{\"end\":70834,\"start\":70819},{\"end\":71076,\"start\":71061},{\"end\":71090,\"start\":71076},{\"end\":71109,\"start\":71090},{\"end\":71123,\"start\":71109},{\"end\":71136,\"start\":71123},{\"end\":71442,\"start\":71431},{\"end\":71452,\"start\":71442},{\"end\":71462,\"start\":71452},{\"end\":71473,\"start\":71462},{\"end\":71483,\"start\":71473},{\"end\":71497,\"start\":71483},{\"end\":71769,\"start\":71758},{\"end\":71778,\"start\":71769},{\"end\":71789,\"start\":71778},{\"end\":71804,\"start\":71789},{\"end\":71816,\"start\":71804},{\"end\":72118,\"start\":72104},{\"end\":72131,\"start\":72118},{\"end\":72142,\"start\":72131},{\"end\":72156,\"start\":72142},{\"end\":72165,\"start\":72156},{\"end\":72176,\"start\":72165}]", "bib_venue": "[{\"end\":50901,\"start\":50862},{\"end\":51182,\"start\":51146},{\"end\":51532,\"start\":51488},{\"end\":51921,\"start\":51872},{\"end\":52358,\"start\":52309},{\"end\":52890,\"start\":52841},{\"end\":53611,\"start\":53562},{\"end\":53999,\"start\":53953},{\"end\":54470,\"start\":54392},{\"end\":55023,\"start\":54960},{\"end\":55411,\"start\":55344},{\"end\":55822,\"start\":55745},{\"end\":56156,\"start\":56079},{\"end\":56430,\"start\":56354},{\"end\":56743,\"start\":56684},{\"end\":57081,\"start\":57016},{\"end\":57506,\"start\":57437},{\"end\":57861,\"start\":57794},{\"end\":58261,\"start\":58184},{\"end\":58717,\"start\":58640},{\"end\":59196,\"start\":59137},{\"end\":59503,\"start\":59476},{\"end\":59690,\"start\":59635},{\"end\":60106,\"start\":60075},{\"end\":60408,\"start\":60343},{\"end\":60649,\"start\":60585},{\"end\":60899,\"start\":60842},{\"end\":61344,\"start\":61292},{\"end\":61760,\"start\":61693},{\"end\":62223,\"start\":62179},{\"end\":62559,\"start\":62453},{\"end\":63218,\"start\":63141},{\"end\":63671,\"start\":63630},{\"end\":63961,\"start\":63912},{\"end\":64348,\"start\":64271},{\"end\":64873,\"start\":64806},{\"end\":65323,\"start\":65271},{\"end\":65563,\"start\":65486},{\"end\":66032,\"start\":65983},{\"end\":66447,\"start\":66370},{\"end\":66828,\"start\":66774},{\"end\":67212,\"start\":67135},{\"end\":68325,\"start\":68247},{\"end\":68878,\"start\":68815},{\"end\":69181,\"start\":69104},{\"end\":69411,\"start\":69379},{\"end\":69671,\"start\":69644},{\"end\":69856,\"start\":69803},{\"end\":70197,\"start\":70153},{\"end\":70755,\"start\":70681},{\"end\":71191,\"start\":71152},{\"end\":71429,\"start\":71367},{\"end\":71756,\"start\":71679},{\"end\":72225,\"start\":72176},{\"end\":54544,\"start\":54472},{\"end\":55465,\"start\":55413},{\"end\":55886,\"start\":55824},{\"end\":57915,\"start\":57863},{\"end\":58325,\"start\":58263},{\"end\":58781,\"start\":58719},{\"end\":61814,\"start\":61762},{\"end\":63282,\"start\":63220},{\"end\":64412,\"start\":64350},{\"end\":64927,\"start\":64875},{\"end\":66511,\"start\":66449},{\"end\":67276,\"start\":67214},{\"end\":68399,\"start\":68327}]"}}}, "year": 2023, "month": 12, "day": 17}