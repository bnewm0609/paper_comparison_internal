{"id": 15636415, "updated": "2023-09-28 15:14:44.555", "metadata": {"title": "Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control", "authors": "[{\"first\":\"Natasha\",\"last\":\"Jaques\",\"middle\":[]},{\"first\":\"Shixiang\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Dzmitry\",\"last\":\"Bahdanau\",\"middle\":[]},{\"first\":\"Jos'e\",\"last\":\"Hern'andez-Lobato\",\"middle\":[\"Miguel\"]},{\"first\":\"Richard\",\"last\":\"Turner\",\"middle\":[\"E.\"]},{\"first\":\"Douglas\",\"last\":\"Eck\",\"middle\":[]}]", "venue": "ICML", "journal": "arXiv: Learning", "publication_date": {"year": 2016, "month": 11, "day": 9}, "abstract": "This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1611.02796", "mag": "2953113026", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/JaquesGBHTE17", "doi": "10.17863/cam.21343"}}, "content": {"source": {"pdf_hash": "a870df7e7d43c9144e2520ef4e4779f1672dd654", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1611.02796v9.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c6d6fd9c754d693b8af30177f033ca53cd713ec3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a870df7e7d43c9144e2520ef4e4779f1672dd654.txt", "contents": "\nSequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control\n\n\nNatasha Jaques \nShixiang Gu \nDzmitry Bahdanau \nJos\u00e9 Miguel Hern\u00e1ndez-Lobato \nRichard E Turner \nDouglas Eck \nSequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control\n\nThis paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.\n\nIntroduction\n\nThe approach of training sequence generation models using likelihood maximization suffers from known failure modes, and it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. For example, long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) networks trained to predict the next character in sequences of text may produce text that has correct spelling, punctuation, and even a semblance of grammar, but the generated text shifts so rapidly from topic to topic, that it is almost completely nonsensical (see (Graves, 2013) for an example). Similar networks trained to predict the next note in a melody suffer from the same problem; the generated music has no consistent theme or structure, and appears wandering and random. In addition, these models are prone to excessively repeating the same output token, a problem that has also been noted in the context of recurrent dialog generation models (Li et al., 2016).\n\nTo ameliorate these problems we propose Sequence Tutor, a novel approach which uses RL to impose structure on a sequence generation RNN via task-specific rewards, while simultaneously ensuring that information learned from data is retained. This is accomplished by maintaining a fixed copy of a sequence generation RNN pre-trained on data, which is termed the Reward RNN. Rather than simply using the Reward RNN to supply part of the rewards to our model, we derive novel off-policy RL methods for sequence generation from KL-control that allow us to directly penalize Kullback Leibler (KL) divergence from the policy defined by the Reward RNN. As a byproduct of minimizing KL our objective includes an entropy regularization term that encourages high entropy in the distribution of the RL model. This is ideal for sequence generation tasks such as text, music, or molecule generation, in which maintaining diversity in the samples generated by the model is critical.\n\nSequence Tutor effectively combines both data and taskrelated goals, without relying on either as a perfect metric of task success. This is an important novel direction of research. Much previous work on combining RL and MLE has used MLE training simply as a way to bootstrap the training of an RL model (Ranzato et al., 2015;Bahdanau et al., 2016;Li et al., 2016), since training with RL from scratch is difficult. However, this approach does not encourage diversity of the generated samples, and can be problematic when task-specific rewards are incomplete or imperfect. Designing an appropriate reward definition is highly non-trivial, and often the hand-crafted rewards cannot be fully trusted (Vedantam et al., 2015;Liu et al., 2016). And yet, relying on data alone can be insufficient when the data itself contains biases, as has been shown for text data (Caliskan-Islam et al., 2016), or when domain-specific constraints cannot be encoded directly into MLE training. By learning a policy that trades off staying close to the data distribution while improving performance on specific metrics, Sequence Tutor reduces both of these problems.\n\nThis paper contributes to the sequence training and RL literature by a) proposing a novel method for combining MLE and RL training; b) showing the connection between KL control and sequence generation; c) deriving the explicit relationships among a generalized variant of \u03a8-learning (Rawlik et al., 2012), G-learning (Fox et al., 2015), and Q-learning with log prior augmentation, and being the first to empirically compare these methods and use them with deep neural networks.\n\nWe explore the usefulness of our approach for two sequence generation applications. The first, music generation, is a difficult problem in which the aesthetic beauty of generated sequences cannot be fully captured in a known reward function, but in which models trained purely on data cannot produce well-structured sequences. Through an empirical study, we show that by imposing rules of music theory on a melody generation model, Sequence Tutor is able to produce melodies which are varied, yet more harmonious, interesting, and rated as significantly more subjectively pleasing than those of the MLE model. Further, Sequence Tutor is able to significantly reduce unwanted behaviors and failure modes of the original RNN. The effectiveness of Sequence Tutor is also demonstrated for computational molecular generation, a task in which the goal is to generate novel drug-like molecules with desirable properties by outputting a string representation of the molecule encoding. However, generating valid molecules can prove difficult, as it is hard for probabilistic models to learn all the constraints that define physically realizable molecules directly from data (G\u00f3mez-Bombarelli et al., 2016). We show that Sequence Tutor is able to yield a higher percentage of valid molecules than the baseline MLE RNN, and the generated molecules score higher on metrics of druglikeness and ease of synthesis.\n\n\nRelated Work\n\nRecent work has attempted to use both MLE and RL in the context of structured prediction. While the attempts were successful, the problems of maintaining information about the data distribution and diversity in the generated samples were not addressed. MIXER (Mixed Incremental Cross-Entropy Reinforce) (Ranzato et al., 2015) uses BLEU score as a reward signal to gradually introduce a RL loss to a text translation model. Bahdanau et al. (2016) applies an actor-critic method and uses BLEU score directly to train a critic network to output the value of each word, where the actor is again initialized with the policy of an RNN trained with next-step prediction. Li et al. (2016) use RL to improve a pre-trained dialog model with heuristic rewards. These approaches assume that the complete task reward specification is available. They pre-train a good policy with supervised learning so that RL can be used to learn the true task objective, since it can be difficult to reach convergence when training with pure RL. However, the original MLE policy of these models is overwritten by the RL training process. In contrast, Sequence Tutor uses rewards to correct certain properties of the generated data, while learning most information from data and maintaining this information; an important ability when the true reward function is not available or imperfect.\n\nReward augmented maximum likelihood (RAML) (Norouzi et al., 2016) is an approach designed to improve MLE training of a translation model by augmenting the ground truth targets with additional outputs that are within a small edit distance, and performing MLE training against those as well. The authors show that their approach is equivalent to minimizing KL-divergence between an RL exponentiated payoff distribution based on edit distance, and the MLE distribution. In contrast, our goal is generation rather than prediction, and we train an RL rather than MLE model. The RAML approach, while an important contribution, is only viable if it is possible to generate additional MLE training samples that are similar in terms of the reward function to the ground truth (i.e. samples within a small edit distance). However in some domains, including the two explored in this paper, generating similar samples with high reward is not only not possible, but in fact constitutes the entire problem under investigation.\n\nFinally, our approach is related to KL control (Todorov, 2007;Kappen et al., 2012;Rawlik et al., 2012), a branch of stochastic optimal control (SOC) (Stengel, 1986). There is also a connection between this work and Maximum Entropy Inverse RL (Ziebart et al., 2008), which can be seen as KL control with a flat, improper prior. From KL control, we take inspiration from two off-policy, model-free methods, \u03a8-learning (Rawlik et al., 2012) and G-learning (Fox et al., 2015). Both approaches are derived from a KLregularized RL objective, where an agent maximizes the reward while incurring additional penalty for divergence from some prior policy. While our methods rely on similar derivations presented in these papers, our methods have different motivations and forms from the original papers. The original \u03a8-learning (Rawlik et al., 2012) restricts the prior policy to be the policy at the previous iteration and solves the original RL objective with conservative, KLregularized policy updates, similar to conservative policy gradient methods (? Peters et al., 2010;Schulman et al., 2015). The original G-learning (Fox et al., 2015) penalizes divergence from a simple uniform prior policy in order to cope with over-estimation of target Q values. These tech-niques have not been applied to deep learning techniques or with RNNs, or as a way to improve a pre-trained MLE model. Our work is the first to explore these methods in such a context, and includes a Q-learning model with additional cross-entropy reward as a comparable alternative. To the best of our knowledge, our work is the first to provide comparisons among these three approaches.\n\nThere has also been prior work in the domain of generative modeling of music. Using RNNs for this purpose has been explored in a variety of contexts, including generating Celtic folk music (Sturm et al., 2016), or improvising the blues (Eck & Schmidhuber, 2002). Often, this involves training the RNN to predict the next note in a monophonic melody; however, as mentioned above, the melodies generated by this model tend to wander and lack musical structure. Some authors have experimented with encoding musical structure into a hierarchical RNN with layers dedicated to generated the melody, drums, and chords (Chu et al., 2016). Other approaches have examined RNNs with richer expressivity, latent-variables for notes, or raw audio synthesis (Boulanger-Lewandowski et al., 2012;Gu et al., 2015;Chung et al., 2015). Recently, Wavenet produced impressive performance in generating music from raw audio using convolutional neural networks with receptive fields at various time scales (van den Oord et al., 2016). However, the authors themselves note that \"even with a receptive field of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, and sound quality\" (p. 8).\n\nFinally, prior work has successfully performed computational molecular generation with deep neural networks. Segler et al. (2017) demonstrated that an LSTM trained on sets of biologically active molecules can be used to generate novel molecules with similar properties. G\u00f3mez-Bombarelli et al. (2016) trained a variational autoencoder to learn a compact embedding of molecules encoded using the SMILES notation. By interpolating in the embedding space and optimizing for desirable metrics of drug quality, the authors were able to decode molecules with high scores on these metrics. However, producing embeddings that led to valid molecules was difficult; in some cases, as little as 1% of generated sequences proved to be a valid molecule encoding.\n\n\nBackground\n\nIn RL, an agent interacts with an environment. Given the state of the environment at time t, s t , the agent takes an action a t according to its policy \u03c0(a t |s t ), receives a reward r(s t , a t ), and the environment transitions to state, s t+1 .The agent's goal is to maximize reward over a sequence of actions, with a discount factor of \u03b3 applied to future rewards. The optimal deterministic policy \u03c0 * is known to satisfy the following Bellman optimality equation,\nQ(s t , a t ; \u03c0 * ) = r(s t , a t ) (1) + \u03b3E p(st+1|st,at) [max at+1 Q(s t+1 , a t+1 ; \u03c0 * )] where Q \u03c0 (s t , a t ) = E \u03c0 [ \u221e t =t \u03b3 t \u2212t r(s t , a t )]\nis the Q function of a policy \u03c0. In Deep Q-learning (Mnih et al., 2013), a neural network called the deep Q-network (DQN) is trained to approximate Q(s, a; \u03b8), using the following objective,\nL(\u03b8) = E \u03b2 [(r(s, a) + \u03b3 max a Q(s , a ; \u03b8 \u2212 ) \u2212 Q(s, a; \u03b8)) 2 ](2)\nwhere \u03b2 is the exploration policy, and \u03b8 \u2212 is the parameters of the target Q-network (Mnih et al., 2013) that is held fixed during the gradient computation. The target Q-network is updated more slowly than the Q-network; for example the moving average of \u03b8 can be used as \u03b8 \u2212 , as proposed by Lillicrap et al. (2015). Exploration can be performed with either the -greedy method or Boltzmann sampling. Additional techniques such as a replay memory (Mnih et al., 2013) are used to stabilize and improve learning.\n\n\nSequence Tutor\n\nGiven a trained sequence generation RNN, we would like to impose domain-specific rewards based on the structure and quality of generated sequences, while still maintaining information about typical sequences learned from data. Therefore, we treat the trained model as a black-box prior policy, and focus on developing a method that can tune some properties of the model without interfering with the original probability distribution learned from data. The separation between the trained sequence model and the tuning method is important, as it prevents RL training from overwriting the original policy. To accomplish this task, we propose Sequence Tutor. An LSTM trained on data supplies the initial weights for three networks in the model: a recurrent Q-network and target Q-network, and a Reward RNN. The Reward RNN is held fixed during training, and treated as a prior policy which can supply the probability of a given token in a sequence as originally learned from data.\n\nTo apply RL to sequence generation, generating the next token in the sequence is treated as an action a. The state of the environment consists of all of the tokens generated so far, i.e. s t = {a 1 , a 2 , ...a t\u22121 }. Given action a t , we would like the reward r t to combine information about the prior policy p(a t |s t ) as output by the Reward RNN, as well as some domain-or task-specific rewards r T . Figure 1 illustrates these ideas. Figure 1: An RNN pre-trained on data using MLE supplies the initial weights for the Q-network and target Q-network, and a fixed copy is used as the Reward RNN.\n\n\nQ-learning with log prior augmentation\n\nThe simplest and most na\u00efve way to incorporate information about the prior policy is to directly augment the taskspecific rewards with the output of the Reward RNN. In this case, the total reward given at time t becomes:\nr(s, a) = log p(a|s) + r T (a, s)/c (3)\nwhere c is a constant controlling the emphasis placed on the task-specific rewards. Given the DQN objective in Eq. 2 and modified reward function in Eq. 3, the objective and learned policy are:\nL(\u03b8) = E \u03b2 [(log p(a|s) + r M T (a, s)/c (4) + \u03b3 max a Q(s , a ; \u03b8 \u2212 ) \u2212 Q(s, a; \u03b8)) 2 ]\n\u03c0 \u03b8 (a|s) = \u03b4(a = arg max a Q(s, a; \u03b8)).\n\nThis modified objective forces the model to learn that the most valuable actions are those that conform to the music theory rules, but still have high probability in the original data. However, the DQN learns a deterministic policy (as shown in Eq. 5), which is not ideal for sequence generation. Therefore, after the model is trained, we generate sequences by sampling from the softmax function applied to the predicted Q-values.\n\n\nKL Control for Sequence Generation\n\nIf we cast sequence generation as a sequential decisionmaking problem and the desired sequence properties in terms of target rewards, the problem can be expressed as a KL control problem for a non-Markovian system. KL control (Todorov, 2007;Kappen et al., 2012;Rawlik et al., 2012) is a branch of stochastic optimal control (SOC) (Stengel, 1986), which studies an RL, or control, problem in which the agent tries maximizing its task reward while minimizing deviation from a prior policy. For our purposes, we treat a trained MLE sequence model as the prior policy, and thus the objective is to train a new policy, or sequence model, to maximize some rewards while keeping close to the original MLE model. We show that such KL control formulation allows us to derive additional variants of Q-learning with minimal modifications, which give rise to different properties. Let \u03c4 = {a 1 , a 2 , ..., a t\u22121 } represent the sequence, r(\u03c4 ) the reward of the sequence, p(\u03c4 ) be the prior distribution over \u03c4 given by the trained sequence model, and q(\u03c4 ) be the policy of the Sequence Tutor model. The objective is then to maximize the following expression with respect to q(\u03c4 ), where D KL represents the KL divergence of distributions:\nL(q) = E q(\u03c4 ) [r(\u03c4 )]/c \u2212 D KL [q(\u03c4 )||p(\u03c4 )].(6)\nWe express q(\u03c4 ) in terms of a parametrized recurrent policy \u03c0 \u03b8 (a t |s t ), i.e. q(\u03c4 ) = T t=1 \u03c0 \u03b8 (a t |s t ) where s t = {a 1 , a 2 , ..., a t\u22121 }, indicates that the system is non-Markovian. The prior policy is expressed similarly p(\u03c4 ) = T t=1 p(a t |s t ). The reinforcement learning objective is the following, where E \u03c0 [\u00b7] below indicates expectation with respect to sequences sampled from \u03c0,\nL(\u03b8) = E \u03c0 [ t r(s t , a t )/c + log p(a t |s t ) \u2212 log \u03c0 \u03b8 (a t |s t )]\nThe difference between this equation and Eq. 4 is that an entropy regularizer is now included, and thus the optimal policy is no longer deterministic. Below, we derive general temporal-difference based methods for the KL-control problem for sequence generation.\n\n\nRecurrent Generalized \u03a8-learning\n\nLet V \u03c0 (s t ) define the recurrent value function of the policy \u03c0 \u03b8 , given by,\nV \u03c0 (s t ) = E \u03c0 [ \u221e t =t r(s t , a t )/c + log p(a t |s t ) (7) \u2212 log \u03c0(a t |s t )]\nWe define the generalized \u03a8 function, analogous to Q function for KL control, as below. We call this generalized \u03a8 function, as it was introduced in deriving \u03a8-learning (Rawlik et al., 2012), and the following derivation is a generalization to the \u03a8-learning algorithm.\n\n\u03a8 \u03c0 (s t , a t ) = r(s t , a t )/c + log p(a t |s t ) + V \u03c0 (s t+1 ) (8) Note that the state s t+1 is given deterministically by s t = {a 1 , a 2 , ..., a t\u22121 } and a t for sequence modeling, and thus the expressions do not contain the usual stochastic dynamics p(s t+1 |s t , a t ). The value function V \u03c0 (s t ) can be recursively expressed in terms of \u03a8 \u03c0 ,\nV \u03c0 (s t ) = E \u03c0 [\u03a8 \u03c0 (s t , a t )] + H[\u03c0(.|s t )] (9) = E \u03c0 [\u03a8 \u03c0 (s t , a t ) \u2212 log \u03c0(a t |s t )](10)\nFixing \u03a8(s t , a t ) = \u03a8 \u03c0 (s t , a t ) and constraining \u03c0 to be a probability distribution, the optimal greedy policy update \u03c0 * can be derived, along with the corresponding optimal value function, \u03c0 * (a t |s t ) \u221d e \u03a8(st,at) (11)\nV * (s t ) = log at e \u03a8(st,at)(12)\nGiven Eq. 8 and 12, the following Bellman optimality equation for generalized \u03a8 function is derived.\n\u03a8 * (s t , a t ) = r(s t , a t )/c + log p(a t |s t ) + log at+1 exp(\u03a8 * (s t+1 , a t+1 ))(13)\nThe \u03a8-learning loss directly follows:\nL \u03a8 (\u03b8) = E \u03b2 [(\u03a8 \u03b8 (s t , a t ) \u2212 y t ) 2 ] where(14)\ny t = log p(a t |s t ) + r(s t , a t )/c + \u03b3 log a e \u03a8 \u2212 (st+1,a ) \u03b2 corresponds to sampling sequence trajectories from an arbitrary distribution; in practice, the experience replay (Mnih et al., 2013). \u03a8 \u2212 indicates that it uses the target network. \u03a8 \u03b8 , i.e. \u03c0 \u03b8 , is parametrized with recurrent neural networks, and for discrete actions, \u03c0 \u03b8 is effectively a softmax layer on top of \u03a8 \u03b8 .\n\n\nRecurrent G-learning\n\nWe can derive another algorithm by parametrizing \u03a8 \u03b8 indirectly by \u03a8 \u03b8 (s t , a t ) = log p(a t |s t ) + G \u03b8 (s t , a t ). Substituting into above equations, we get a different temporaldifference method:\nL G (\u03b8) = E \u03b2 [(G \u03b8 (s t , a t ) \u2212 y t ) 2 ] where(15)\ny t = r(s t , a t )/c + \u03b3 log a p(a |s t+1 )e G \u2212 (st+1,a ) and \u03c0 \u03b8 (a t |s t ) \u221d p(a t |s t ) exp(G \u03b8 (s t , a t ))\n\nThis formulation corresponds to G-learning (Fox et al., 2015), which can thus be seen as a special case of generalized \u03a8-learning. Unlike \u03a8 learning, which directly builds knowledge about the prior policy into the \u03a8 function, the G-function does not give the policy directly but instead needs to be dynamically mixed with the prior policy probabilities. While this computation is straight-forward for discrete action domains as here, extensions to continuous action domains require additional considerations such as normalizability of \u03a8-function parametrization (Gu et al., 2016).\n\nThe KL control-based derivation also has another benefit in that the stochastic policies can be directly used as an exploration strategy, instead of heuristics such as -greedy or additive noise (Mnih et al., 2013;Lillicrap et al., 2015).\n\n\nSequence Tutor implementation\n\nFollowing from the above derivations, we compare three methods for implementing Sequence Tutor: Q-learning with log prior augmentation (based on Eq. 4), generalized \u03a8-learning (based on Eq. 14), and G-learning (based on Eq. 15). A pre-trained sequence generation LSTM is used as the Reward RNN, to supply the cross entropy reward in Q-learning and the prior policy in G-and generalized \u03a8learning. These approaches are compared to both the original performance of the MLE RNN, and a model trained using only RL and no prior policy. Model evaluation is performed every 100,000 training epochs, by generating 100 sequences and assessing the average r T and log p(a|s).\n\nThe code for Sequence Tutor, including a checkpointed version of the trained melody RNN is available at https://github.com/tensorflow/magenta/ tree/master/magenta/models/rl_tuner.\n\n\nExperiment I: Melody Generation\n\nMusic compositions adhere to relatively well-defined structural rules, making music an interesting sequence generation challenge. For example, music theory tells that groups of notes belong to keys, chords follow progressions, and songs have consistent structures made up of musical phrases. Our research question is therefore whether such constraints can be learned by an RNN, while still allowing it to maintain note probabilities learned from data.\n\nTo test this hypothesis, we developed several rules that we believe describe pleasant-sounding melodies, taking inspiration from a text on melodic composition (Gauldin, 1995). We do not claim these characteristics are exhaustive or strictly necessary for good composition; rather, they are an incomplete measure of task success that can simply guide the model towards traditional composition structure. It is therefore crucial that the Sequence Tutor approach allows the model to retain knowledge learned from real songs in the training data. The rules comprising the music-specific reward function r T (a, s) encourage melodies to: stay in key, start with the tonic note, resolve melodic leaps, have a unique maximum and minimum note, prefer harmonious intervals, play motifs and repeat them, have a low autocorrelation at a lag of 1, 2, and 3 beats, and avoid excessively repeating notes. Interestingly, while excessively repeating tokens is a common problem in RNN sequence generation models, avoiding this behavior is also Gauldin's first rule of melodic composition (p. 42).\n\nTo train the model, we begin by extracting monophonic melodies from a corpus of 30,000 MIDI songs and encoding them as one-hot sequences of notes 1 . These melodies are then used to train an LSTM with one layer of 100 cells. Optimization was performed with Adam (Kingma & Ba, 2014), a batch size of 128, initial learning rate of .5, and a stepwise learning rate decay of 0.85 every 1000 steps. Gradients were clipped to ensure the L2 norm was less than 5, and weight regularization was applied with \u03b2 = 2.5\u00d710 \u22125 . Finally, the losses for the first 8 notes of each sequence were not used to train the model, since it cannot reasonably be expected to accurately predict them with no context. The trained RNN eventually obtained a validation accuracy of 92% and a log perplexity score of .2536. This model was used as described above to initialize the three sub-networks in the Sequence Tutor model. The Sequence Tutor model was trained using a similar configuration to the one above, except with a batch size of 32, and a reward discount factor of \u03b3=.5. The Target-Q-network's weights \u03b8 \u2212 were gradually updated towards those of the Q-network (\u03b8) according to the formula (1 \u2212 \u03b7)\u03b8 \u2212 + \u03b7\u03b8, where \u03b7 = .01 is the Target-Q-network update rate. A strength of our model is that the influence of data and task-specific rewards can be explicitly controlled by adjusting the temperature parameter c. We replicated our results for a number of settings for c; we present results for c=.5 below because we believe them to be most musically pleasing, however additional results are available at https://goo.gl/cTZy8r. Similarly, we replicated the results using both -greedy and Boltzmann exploration, and present the results using -greedy exploration below.\n\n\nResults\n\nTable 1 provides quantitative results in the form of performance on the music theory rules to which we trained the model to adhere; for example, we can assess the fraction of notes played by the model which belonged to the correct key, or the fraction of melodic leaps that were resolved. The statistics were computed by randomly generating 100,000 melodies from each model.\n\nThe results above demonstrate that the application of RL is able to correct almost all of the targeted \"bad behaviors\" of the MLE RNN, while improving performance on the desired metrics. For example, the original LSTM model was extremely prone to repeating the same note; after applying RL, we see that the number of notes belonging to some excessively repeated segment has dropped from 63% to nearly 0% in all of the Sequence Tutor models. While the metrics for the G model did not improve as consistently, the Q and \u03a8 models successfully learned to adhere to most of the imposed rules. The degree of improvement on these metrics is related to the magnitude of the reward given for the behavior. For example, a strong penalty of -100 was applied each time a note was excessively repeated, while a reward of only 3 was applied at the end of a melody for unique extrema notes (which most likely explains the lack of improvement on this metric). The reward values could be adjusted to improve the metrics further, however we found that these values produced pleasant melodies.\n\nWhile the metrics indicate that the targeted behaviors of the RNN have improved, it is not clear whether the models have retained information about the training data. Figure  2a plots the average log p(a|s) as produced by the Reward RNN for melodies generated by the models every 100,000 training epochs; Figure 2b plots the average r T . Included in the plot is an RL only model trained using only the music theory rewards, with no information about log p(a|s). Since each model is initialized with the weights of the trained MLE RNN, we see that as the models quickly learn to adhere to the music theory constraints, log p(a|s) falls from its initial point. For the RL only model, log p(a|s) reaches an average of -3.65, which is equivalent to an average p(a|s) of approximately 0.026, or essentially a random policy over the 38 actions with respect to the distribution defined by the Reward RNN. Figure 2a shows that each of our models (Q, \u03a8, and G) attain higher log p(a|s) values than this baseline, indicating they have maintained information about the data distribution, even over 3,000,000 training steps. The G-learning implementation scores highest on this metric, at the cost of slightly lower average r T . This compromise between data probability and adherence to music theory could explain the difference in the G model's performance on the music theory metrics in Table  1. Finally, we have verified that by increasing the c parameter it is possible to train all the models to have even higher average log p(a|s), but found that c = 0.5 produced melodies that sounded better subjectively.\n\nThe question remains whether the RL-tutored models actually produce more pleasing melodies. The sample melodies used for the study are available here: goo.gl/XIYt9m; we encourage readers to judge their quality for themselves.\n\nTo more formally answer this question, we conducted a user study via Amazon Mechanical Turk in which participants were asked to rate which of two randomly selected melodies they preferred on a Likert scale. A total of 192 ratings were collected; each model was involved in 92 of these comparisons. Figure 3 plots the number of comparisons in which a melody from each model was selected as the most musically pleasing. A Kruskal-Wallis H test of the ratings showed that there was a statistically significant difference between the models, \u03c7 2 (3) = 109.480, p < 0.001. Mann-Whitney U post-hoc tests revealed that the melodies from all three Sequence Tuner models (Q, \u03a8, and G) had significantly higher ratings than the melodies of the MLE RNN, p < .001. The Q and \u03a8 melodies were also rated as significantly more pleasing than those of the G model, but did not differ significantly from each other.\n\n\nDiscussion\n\nListening to the samples produced by the MLE RNN reveals that they are sometimes dischordant and usually dull; the model tends to place rests frequently, repeat the same  Figure 3: The number of times a melody from each model was selected as most musically pleasing. Error bars reflect the std. dev. of a binomial distribution fit to the binary win/loss data from each model. note, and produce melodies with little variation. In contrast, the melodies produced by the Sequence Tutor models are more varied and interesting. The G model tends to produce energetic and chaotic melodies, which include sequences of repeated notes. This repetition is likely because the G policy as defined in Eq. 15 directly mixes p(a|s) with the output of the G network, and the MLE RNN strongly favours repeating notes. The most pleasant melodies are generated by the Q and \u03a8 models. These melodies stay firmly in key and frequently choose more harmonious interval steps, leading to melodic and pleasant melodies. However, it is clear they have retained information about the training data; for example, the sample q2.wav in the sample directory ends with a seemingly familiar riff.\n\nWhile we acknowledge that the monophonic melodies generated by these models -which are based on highly simplistic rules of melodic composition -do not approach the level of artistic merit of human composers, we believe this study provides a proof-of-concept that encoding even incomplete and partially specified domain knowledge using our method can help the outputs of an LSTM adhere to a more consistent structure. The musical complexity of the songs is limited not just by the heuristic rules, but also by the simple monophonic encoding, which cannot represent the dynamics and expressivity of a musical performance. Although these melodies cannot surpass those of human musicians, attempting to train a model to generate aesthetically pleasing outputs in the absence of a better metric of human taste than log-likelihood is a problem of broader interest to the artificial intelligence community.\n\n\nExperiment II: Computational Molecular Generation\n\nAs a follow-on experiment, we tested the effectiveness of Sequence Tutor for generating a higher yield of synthet-ically accessible drug-like molecules. Organic molecules can be encoded using the commonly used SMILES representation (Weininger, 1970). For example, amphetamine can be encoded as 'CC(N)Cc1ccccc1', while creatine is 'CN(CC(=O)O)C(=N)N'. Using this character encoding, it is straightforward to train an MLE RNN to generate sequences of SMILES characters; we trained such a model using the same settings as described above for the melody MLE RNN. However, only about a third of the molecules generated using this simple approach are actually valid SMILES encodings. Further, this approach does not directly optimize for metrics of molecule or drug quality. These metrics include: a) the water-octanol partition coefficient (logP), which is important in assessing the druglikeness of a molecule; b) synthetic accessibility (SA) (Ertl & Schuffenhauer, 2009), a score from 1-10 that is lower if the molecule is easier to synthesize; and c) Quantitative Estimation of Drug-likeness (QED) (Bickerton et al., 2012), a more subjective measure of drug-likeness based on abstract ideas of medicinal aesthetics.\n\nTo optimize for these metrics, while simultaneously improving the percent yield of valid molecules from the RNN, we constructed a reward function that incentivizes validity, logP, SA, and QED using an open-source library called RDkit (http://www.rdkit.org/). Included in the reward function was a penalty for molecules with unrealistically large carbon rings (size larger than 6), as per previous work (G\u00f3mez-Bombarelli et al., 2016). Finally, after observing that the model could exploit the reward function by generating the simple molecule 'N' repeatedly, or 'CCCCC...' (which produces an unrealistically high logP value), we added penalties for sequences shorter than, or with more consecutive carbon atoms than, any sequence in the training data. Sequence Tutor was then trained using these rewards, the pre-trained MLE RNN, and similar settings to the first experiment, except with -greedy exploration with = .01, a batch size of 512, and discount factor \u03b3 = .95. For this experiment, we also made use of prioritized experience replay (Schaul et al., 2015) to allow the model to more frequently learn from relatively rare valid samples. A value of c = 2.85 led to a higher yield of valid molecules with high metrics, but still encouraged the diversity of generated samples.\n\n\nResults and discussion\n\nAs the \u03a8 algorithm produced the best results for the music generation task, we focused on using this technique for generating molecules. Table 2 shows the performance of this model against the original MLE model according to metrics of validity, drug-likeness, and synthetic accessibility. Once again, Sequence Tutor is able to significantly improve almost all of the targeted metrics. However, it should be noted that the Sequence Tutor model tends to produce simplistic molecules involving more carbon atoms than the MLE baseline; e.g. Sequence Tutor may produce 'SNCc1ccccc1', while the MLE produces 'C(=O)c1ccc(S(=O)(=O)N(C)C)c(Cl)c1', which is the reason for the Sequence Tutor model's lower QED scores. This effect is due to the fact that simple sequences are more likely to be valid, have high logP and SA scores, and carbon is highly likely under the distribution learned by the MLE model. A higher reward for QED and further improvement of the task-specific rewards based on domain knowledge could help to alleviate these problems. Overall, the fact that Sequence Tutor can improve the percentage of valid molecules produced as well as the logP and synthetic accessibility scores serves as a proof-of-concept that Sequence Tutor may be valuable in a number of domains for imparting domain knowledge onto a sequence predictor.  \n\n\nConclusion and Future Work\n\nWe have derived a novel sequence learning framework which uses RL to correct properties of sequences generated by an RNN, while maintaining information learned from MLE training on data, and ensuring the diversity of generated samples. By demonstrating a connection between our sequence generation approach and KL-control, we have derived three novel RL-based methods for optimizing sequence generation models. These methods were empirically compared in the context of a music generation task, and further demonstrated on a computational molecular generation task. Sequence Tutor showed promising results in terms of both adherence to task-specific rules, and subjective quality of the generated sequences.\n\nWe believe the Sequence Tutor approach of using RL to refine RNN models could be promising for a number of applications, including the reduction of bias in deep learning models. While manually writing a domain-specific reward function may seem unappealing, that approach is limited by the quality of the data that can be collected, and besides, even state-of-the-art sequence models often fail to learn all the aspects of high-level structure (van den Oord et al., 2016;Graves, 2013). Further, the data may contain hidden biases, as has been demonstrated for popular language models (Caliskan-Islam et al., 2016). In contrast to relying solely on possibly biased data, our approach allows for encoding high-level domain knowledge into the RNN, providing a general, alternative tool for training sequence models.\n\nFigure 2 :\n2Average reward obtained by sampling 100 melodies every 100,000 training epochs. The three models are compared to a model trained using only the music theory rewards r T .\n\nTable 2 :\n2Statistics of molecule validity and quality based on 100,000 randomly initialized samples. Bolded entries represent significant improvements over the MLE baseline.\nMore information about both the note encoding and the reward metrics is available in the supplementary material.\nACKNOWLEDGMENTSThis work was supported by Google Brain, the MIT Media Lab Consortium, and Canada's Natural Sciences and Engineering Research Council (NSERC). We thank Greg Wayne, Sergey Levine, and Timothy Lillicrap for helpful discussions on RL and stochastic optimal control and Kyle Kastner and Tim Cooijmans for valuable insight into training RNNs.\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, arXiv:1607.07086arXiv preprintDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086, 2016.\n\nQuantifying the chemical beauty of drugs. G Richard Bickerton, V Gaia, J\u00e9r\u00e9my Paolini, Sorel Besnard, Andrew L Muresan, Hopkins, Nature chemistry. 42G Richard Bickerton, Gaia V Paolini, J\u00e9r\u00e9my Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90-98, 2012.\n\nModeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. Nicolas Boulanger-Lewandowski, Yoshua Bengio, Pascal Vincent, arXiv:1206.6392arXiv preprintNicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. arXiv preprint arXiv:1206.6392, 2012.\n\nAylin Caliskan-Islam, Joanna J Bryson, Arvind Narayanan, arXiv:1608.07187Semantics derived automatically from language corpora necessarily contain human biases. arXiv preprintAylin Caliskan-Islam, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from lan- guage corpora necessarily contain human biases. arXiv preprint arXiv:1608.07187, 2016.\n\nSong from pi: A musically plausible network for pop music generation. Hang Chu, Raquel Urtasun, Sanja Fidler, arXiv:1611.03477arXiv preprintHang Chu, Raquel Urtasun, and Sanja Fidler. Song from pi: A musically plausible network for pop music generation. arXiv preprint arXiv:1611.03477, 2016.\n\nA recurrent latent variable model for sequential data. Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, C Aaron, Yoshua Courville, Bengio, Advances in neural information processing systems (NIPS). Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A re- current latent variable model for sequential data. In Ad- vances in neural information processing systems (NIPS), pp. 2980-2988, 2015.\n\nFinding temporal structure in music: Blues improvisation with lstm recurrent networks. Douglas Eck, Juergen Schmidhuber, Proceedings of the 2002 12th IEEE Workshop on. the 2002 12th IEEE Workshop onIEEENeural Networks for Signal ProcessingDouglas Eck and Juergen Schmidhuber. Finding temporal structure in music: Blues improvisation with lstm recur- rent networks. In Neural Networks for Signal Process- ing, 2002. Proceedings of the 2002 12th IEEE Workshop on, pp. 747-756. IEEE, 2002.\n\nEstimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Peter Ertl, Ansgar Schuffenhauer, Journal of cheminformatics. 11Peter Ertl and Ansgar Schuffenhauer. Estimation of syn- thetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Jour- nal of cheminformatics, 1(1):8, 2009.\n\nTaming the noise in reinforcement learning via soft updates. Roy Fox, Ari Pakman, Naftali Tishby, arXiv:1512.08562arXiv preprintRoy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. arXiv preprint arXiv:1512.08562, 2015.\n\nA practical approach to eighteenth-century counterpoint. Gauldin, Waveland Pr IncGauldin. A practical approach to eighteenth-century coun- terpoint. Waveland Pr Inc, 1995.\n\nAutomatic chemical design using a data-driven continuous representation of molecules. Rafael G\u00f3mez-Bombarelli, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, P Ryan, Al\u00e1n Adams, Aspuru-Guzik, arXiv:1610.02415arXiv preprintRafael G\u00f3mez-Bombarelli, David Duvenaud, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Jorge Aguilera-Iparraguirre, Timo- thy D Hirzel, Ryan P Adams, and Al\u00e1n Aspuru-Guzik. Automatic chemical design using a data-driven con- tinuous representation of molecules. arXiv preprint arXiv:1610.02415, 2016.\n\nGenerating sequences with recurrent neural networks. Alex Graves, arXiv preprint:1308.0850Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint:1308.0850, 2013.\n\nNeural adaptive sequential monte carlo. Shixiang Gu, Zoubin Ghahramani, Richard E Turner, Advances in Neural Information Processing Systems (NIPS). Shixiang Gu, Zoubin Ghahramani, and Richard E Turner. Neural adaptive sequential monte carlo. In Advances in Neural Information Processing Systems (NIPS), pp. 2629-2637, 2015.\n\nContinuous Deep Q-Learning with model-based acceleration. Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, Sergey Levine, ICML. Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous Deep Q-Learning with model-based acceleration. In ICML, 2016.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\n\nOptimal control as a graphical model inference problem. Vicen\u00e7 Hilbert J Kappen, Manfred G\u00f3mez, Opper, Machine learning. 872Hilbert J Kappen, Vicen\u00e7 G\u00f3mez, and Manfred Opper. Op- timal control as a graphical model inference problem. Machine learning, 87(2):159-182, 2012.\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, arXiv:1412.6980arXiv preprintDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nDeep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, arXiv:1606.01541arXiv preprintJiwei Li, Will Monroe, Alan Ritter, and Dan Jurafsky. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.\n\nP Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforce- ment learning. arXiv preprint arXiv:1509.02971, 2015.\n\nHow not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. Chia-Wei Liu, Ryan Lowe, V Iulian, Michael Serban, Laurent Noseworthy, Joelle Charlin, Pineau, EMNLP. Chia-Wei Liu, Ryan Lowe, Iulian V Serban, Michael Nose- worthy, Laurent Charlin, and Joelle Pineau. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In EMNLP, 2016.\n\nEmotional response to musical repetition. Caroline Steven R Livingstone, Emery Palmer, Schubert, Emotion. 123552Steven R Livingstone, Caroline Palmer, and Emery Schu- bert. Emotional response to musical repetition. Emotion, 12(3):552, 2012.\n\nPlaying atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, arXiv:1312.5602arXiv preprintVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learn- ing. arXiv preprint arXiv:1312.5602, 2013.\n\nSequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control. Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control\n\nReward augmented maximum likelihood for neural structured prediction. Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, Advances In Neural Information Processing Systems. Mohammad Norouzi, Samy Bengio, Navdeep Jaitly, Mike Schuster, Yonghui Wu, Dale Schuurmans, et al. Re- ward augmented maximum likelihood for neural struc- tured prediction. In Advances In Neural Information Processing Systems, pp. 1723-1731, 2016.\n\nRelative entropy policy search. Jan Peters, Katharina M\u00fclling, Yasemin Altun, AAAI. Jan Peters, Katharina M\u00fclling, and Yasemin Altun. Rel- ative entropy policy search. In AAAI, pp. 1607-1612.\n\n. Atlanta. Atlanta, 2010.\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recur- rent neural networks. arXiv preprint arXiv:1511.06732, 2015.\n\nOn stochastic optimal control and reinforcement learning by approximate inference. Konrad Rawlik, Marc Toussaint, Sethu Vijayakumar, Robotics: science and systems. Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement learn- ing by approximate inference. In Robotics: science and systems, 2012.\n\n. Tom Schaul, John Quan, Ioannis Antonoglou, David Silver, arXiv:1511.05952Prioritized experience replay. arXiv preprintTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, Proceedings of the 32nd International Conference on Machine Learning (ICML-15). the 32nd International Conference on Machine Learning (ICML-15)John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy op- timization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 1889- 1897, 2015.\n\nGenerating focussed molecule libraries for drug discovery with recurrent neural networks. H S Marwin, Thierry Segler, Christian Kogej, Mark P Tyrchan, Waller, arXiv:1701.01329arXiv preprintMarwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focussed molecule libraries for drug discovery with recurrent neural networks. arXiv preprint arXiv:1701.01329, 2017.\n\nStochastic optimal control. F Robert, Stengel, John Wiley and SonsNew York, New YorkRobert F Stengel. Stochastic optimal control. John Wiley and Sons New York, New York, 1986.\n\nMusic transcription modelling and composition using deep learning. L Bob, Joao Felipe Sturm, Oded Santos, Iryna Ben-Tal, Korshunova, arXiv:1604.08723arXiv preprintBob L Sturm, Joao Felipe Santos, Oded Ben-Tal, and Iryna Korshunova. Music transcription modelling and composition using deep learning. arXiv preprint arXiv:1604.08723, 2016.\n\nLinearly-solvable markov decision problems. Emanuel Todorov, Advances in neural information processing systems (NIPS). Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information process- ing systems (NIPS), pp. 1369-1376, 2007.\n\nWavenet: A generative model for raw audio. A\u00e4ron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray Kavukcuoglu, CoRR abs/1609.03499A\u00e4ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. CoRR abs/1609.03499, 2016.\n\nCider: Consensus-based image description evaluation. Ramakrishna Vedantam, Lawrence Zitnick, Devi Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description eval- uation. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pp. 4566-4575, 2015.\n\nSmiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Proc. Edinburgh Math. SOC. Edinburgh Math. SOC17David Weininger. Smiles, a chemical language and infor- mation system. 1. introduction to methodology and en- coding rules. In Proc. Edinburgh Math. SOC, volume 17, pp. 1-14, 1970.\n\nMaximum entropy inverse reinforcement learning. D Brian, Andrew L Ziebart, Andrew Maas, Anind K Bagnell, Dey, AAAI. Chicago, IL, USA8Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.\n", "annotations": {"author": "[{\"end\":105,\"start\":90},{\"end\":118,\"start\":106},{\"end\":136,\"start\":119},{\"end\":166,\"start\":137},{\"end\":184,\"start\":167},{\"end\":197,\"start\":185}]", "publisher": null, "author_last_name": "[{\"end\":104,\"start\":98},{\"end\":117,\"start\":115},{\"end\":135,\"start\":127},{\"end\":165,\"start\":142},{\"end\":183,\"start\":177},{\"end\":196,\"start\":193}]", "author_first_name": "[{\"end\":97,\"start\":90},{\"end\":114,\"start\":106},{\"end\":126,\"start\":119},{\"end\":141,\"start\":137},{\"end\":174,\"start\":167},{\"end\":176,\"start\":175},{\"end\":192,\"start\":185}]", "author_affiliation": null, "title": "[{\"end\":87,\"start\":1},{\"end\":284,\"start\":198}]", "venue": null, "abstract": "[{\"end\":1334,\"start\":286}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1645,\"start\":1614},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1927,\"start\":1913},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2318,\"start\":2301},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3616,\"start\":3594},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3638,\"start\":3616},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3654,\"start\":3638},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4011,\"start\":3988},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4028,\"start\":4011},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4180,\"start\":4151},{\"end\":4571,\"start\":4548},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4741,\"start\":4720},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4772,\"start\":4754},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6112,\"start\":6081},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6657,\"start\":6635},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6777,\"start\":6755},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7012,\"start\":6996},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7760,\"start\":7738},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8771,\"start\":8756},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8791,\"start\":8771},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8811,\"start\":8791},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8873,\"start\":8858},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8973,\"start\":8951},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9146,\"start\":9125},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9180,\"start\":9162},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9548,\"start\":9527},{\"end\":9776,\"start\":9756},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9798,\"start\":9776},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9842,\"start\":9824},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10566,\"start\":10546},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10618,\"start\":10593},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10986,\"start\":10968},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11137,\"start\":11101},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11153,\"start\":11137},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11172,\"start\":11153},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11367,\"start\":11340},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11740,\"start\":11720},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13071,\"start\":13052},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13363,\"start\":13344},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13575,\"start\":13552},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13725,\"start\":13706},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16705,\"start\":16690},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16725,\"start\":16705},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16744,\"start\":16725},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16809,\"start\":16794},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18875,\"start\":18854},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20178,\"start\":20159},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20831,\"start\":20813},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21349,\"start\":21332},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21565,\"start\":21546},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21588,\"start\":21565},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23132,\"start\":23117},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24320,\"start\":24301},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32357,\"start\":32340},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33074,\"start\":33047},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33756,\"start\":33725},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":34385,\"start\":34364},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":37174,\"start\":37156},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37187,\"start\":37174},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":37316,\"start\":37287}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37699,\"start\":37516},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":37875,\"start\":37700}]", "paragraph": "[{\"end\":2319,\"start\":1350},{\"end\":3288,\"start\":2321},{\"end\":4435,\"start\":3290},{\"end\":4914,\"start\":4437},{\"end\":6315,\"start\":4916},{\"end\":7693,\"start\":6332},{\"end\":8707,\"start\":7695},{\"end\":10355,\"start\":8709},{\"end\":11609,\"start\":10357},{\"end\":12360,\"start\":11611},{\"end\":12845,\"start\":12375},{\"end\":13190,\"start\":13000},{\"end\":13769,\"start\":13259},{\"end\":14763,\"start\":13788},{\"end\":15366,\"start\":14765},{\"end\":15629,\"start\":15409},{\"end\":15863,\"start\":15670},{\"end\":15993,\"start\":15953},{\"end\":16425,\"start\":15995},{\"end\":17693,\"start\":16464},{\"end\":18147,\"start\":17745},{\"end\":18482,\"start\":18221},{\"end\":18599,\"start\":18519},{\"end\":18954,\"start\":18685},{\"end\":19316,\"start\":18956},{\"end\":19652,\"start\":19420},{\"end\":19788,\"start\":19688},{\"end\":19921,\"start\":19884},{\"end\":20368,\"start\":19977},{\"end\":20596,\"start\":20393},{\"end\":20768,\"start\":20652},{\"end\":21350,\"start\":20770},{\"end\":21589,\"start\":21352},{\"end\":22288,\"start\":21623},{\"end\":22469,\"start\":22290},{\"end\":22956,\"start\":22505},{\"end\":24037,\"start\":22958},{\"end\":25782,\"start\":24039},{\"end\":26168,\"start\":25794},{\"end\":27244,\"start\":26170},{\"end\":28849,\"start\":27246},{\"end\":29076,\"start\":28851},{\"end\":29975,\"start\":29078},{\"end\":31153,\"start\":29990},{\"end\":32054,\"start\":31155},{\"end\":33321,\"start\":32108},{\"end\":34602,\"start\":33323},{\"end\":35965,\"start\":34629},{\"end\":36702,\"start\":35996},{\"end\":37515,\"start\":36704}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12999,\"start\":12846},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13258,\"start\":13191},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15669,\"start\":15630},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15952,\"start\":15864},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17744,\"start\":17694},{\"attributes\":{\"id\":\"formula_6\"},\"end\":18220,\"start\":18148},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18684,\"start\":18600},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19419,\"start\":19317},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19687,\"start\":19653},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19883,\"start\":19789},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19976,\"start\":19922},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20651,\"start\":20597}]", "table_ref": "[{\"end\":28633,\"start\":28625},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34773,\"start\":34766}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1348,\"start\":1336},{\"attributes\":{\"n\":\"2.\"},\"end\":6330,\"start\":6318},{\"attributes\":{\"n\":\"3.\"},\"end\":12373,\"start\":12363},{\"attributes\":{\"n\":\"4.\"},\"end\":13786,\"start\":13772},{\"attributes\":{\"n\":\"4.1.\"},\"end\":15407,\"start\":15369},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16462,\"start\":16428},{\"attributes\":{\"n\":\"4.3.\"},\"end\":18517,\"start\":18485},{\"attributes\":{\"n\":\"4.4.\"},\"end\":20391,\"start\":20371},{\"attributes\":{\"n\":\"4.5.\"},\"end\":21621,\"start\":21592},{\"attributes\":{\"n\":\"5.\"},\"end\":22503,\"start\":22472},{\"attributes\":{\"n\":\"5.1.\"},\"end\":25792,\"start\":25785},{\"attributes\":{\"n\":\"5.2.\"},\"end\":29988,\"start\":29978},{\"attributes\":{\"n\":\"6.\"},\"end\":32106,\"start\":32057},{\"attributes\":{\"n\":\"6.1.\"},\"end\":34627,\"start\":34605},{\"attributes\":{\"n\":\"7.\"},\"end\":35994,\"start\":35968},{\"end\":37527,\"start\":37517},{\"end\":37710,\"start\":37701}]", "table": null, "figure_caption": "[{\"end\":37699,\"start\":37529},{\"end\":37875,\"start\":37712}]", "figure_ref": "[{\"end\":15181,\"start\":15173},{\"end\":15215,\"start\":15207},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27423,\"start\":27413},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27560,\"start\":27551},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28154,\"start\":28145},{\"end\":29384,\"start\":29376},{\"end\":30169,\"start\":30161}]", "bib_author_first_name": "[{\"end\":38400,\"start\":38393},{\"end\":38419,\"start\":38411},{\"end\":38434,\"start\":38428},{\"end\":38446,\"start\":38439},{\"end\":38458,\"start\":38454},{\"end\":38471,\"start\":38465},{\"end\":38485,\"start\":38480},{\"end\":38503,\"start\":38497},{\"end\":38820,\"start\":38819},{\"end\":38833,\"start\":38827},{\"end\":38848,\"start\":38843},{\"end\":38866,\"start\":38858},{\"end\":39205,\"start\":39198},{\"end\":39235,\"start\":39229},{\"end\":39250,\"start\":39244},{\"end\":39523,\"start\":39518},{\"end\":39546,\"start\":39540},{\"end\":39548,\"start\":39547},{\"end\":39563,\"start\":39557},{\"end\":39958,\"start\":39954},{\"end\":39970,\"start\":39964},{\"end\":39985,\"start\":39980},{\"end\":40241,\"start\":40233},{\"end\":40253,\"start\":40249},{\"end\":40270,\"start\":40263},{\"end\":40285,\"start\":40277},{\"end\":40293,\"start\":40292},{\"end\":40307,\"start\":40301},{\"end\":40718,\"start\":40711},{\"end\":40731,\"start\":40724},{\"end\":41242,\"start\":41237},{\"end\":41255,\"start\":41249},{\"end\":41574,\"start\":41571},{\"end\":41583,\"start\":41580},{\"end\":41599,\"start\":41592},{\"end\":42045,\"start\":42039},{\"end\":42069,\"start\":42064},{\"end\":42084,\"start\":42080},{\"end\":42091,\"start\":42085},{\"end\":42115,\"start\":42110},{\"end\":42146,\"start\":42139},{\"end\":42148,\"start\":42147},{\"end\":42158,\"start\":42157},{\"end\":42169,\"start\":42165},{\"end\":42563,\"start\":42559},{\"end\":42743,\"start\":42735},{\"end\":42754,\"start\":42748},{\"end\":42774,\"start\":42767},{\"end\":42776,\"start\":42775},{\"end\":43086,\"start\":43078},{\"end\":43098,\"start\":43091},{\"end\":43114,\"start\":43110},{\"end\":43132,\"start\":43126},{\"end\":43316,\"start\":43312},{\"end\":43335,\"start\":43329},{\"end\":43540,\"start\":43534},{\"end\":43566,\"start\":43559},{\"end\":43803,\"start\":43795},{\"end\":43817,\"start\":43812},{\"end\":44022,\"start\":44017},{\"end\":44031,\"start\":44027},{\"end\":44044,\"start\":44040},{\"end\":44056,\"start\":44053},{\"end\":44245,\"start\":44244},{\"end\":44263,\"start\":44255},{\"end\":44265,\"start\":44264},{\"end\":44286,\"start\":44277},{\"end\":44300,\"start\":44293},{\"end\":44313,\"start\":44310},{\"end\":44326,\"start\":44321},{\"end\":44338,\"start\":44333},{\"end\":44350,\"start\":44346},{\"end\":44813,\"start\":44805},{\"end\":44823,\"start\":44819},{\"end\":44831,\"start\":44830},{\"end\":44847,\"start\":44840},{\"end\":44863,\"start\":44856},{\"end\":44882,\"start\":44876},{\"end\":45204,\"start\":45196},{\"end\":45232,\"start\":45227},{\"end\":45453,\"start\":45444},{\"end\":45465,\"start\":45460},{\"end\":45484,\"start\":45479},{\"end\":45497,\"start\":45493},{\"end\":45513,\"start\":45506},{\"end\":45530,\"start\":45526},{\"end\":45547,\"start\":45541},{\"end\":46052,\"start\":46044},{\"end\":46066,\"start\":46062},{\"end\":46082,\"start\":46075},{\"end\":46095,\"start\":46091},{\"end\":46113,\"start\":46106},{\"end\":46122,\"start\":46118},{\"end\":46469,\"start\":46466},{\"end\":46487,\"start\":46478},{\"end\":46504,\"start\":46497},{\"end\":46661,\"start\":46654},{\"end\":46673,\"start\":46668},{\"end\":46690,\"start\":46683},{\"end\":46707,\"start\":46699},{\"end\":47068,\"start\":47062},{\"end\":47081,\"start\":47077},{\"end\":47098,\"start\":47093},{\"end\":47328,\"start\":47325},{\"end\":47341,\"start\":47337},{\"end\":47355,\"start\":47348},{\"end\":47373,\"start\":47368},{\"end\":47613,\"start\":47609},{\"end\":47630,\"start\":47624},{\"end\":47645,\"start\":47639},{\"end\":47661,\"start\":47654},{\"end\":47677,\"start\":47670},{\"end\":48143,\"start\":48142},{\"end\":48145,\"start\":48144},{\"end\":48161,\"start\":48154},{\"end\":48179,\"start\":48170},{\"end\":48191,\"start\":48187},{\"end\":48193,\"start\":48192},{\"end\":48471,\"start\":48470},{\"end\":48687,\"start\":48686},{\"end\":48697,\"start\":48693},{\"end\":48704,\"start\":48698},{\"end\":48716,\"start\":48712},{\"end\":48730,\"start\":48725},{\"end\":49009,\"start\":49002},{\"end\":49271,\"start\":49266},{\"end\":49292,\"start\":49286},{\"end\":49308,\"start\":49303},{\"end\":49319,\"start\":49314},{\"end\":49335,\"start\":49330},{\"end\":49349,\"start\":49345},{\"end\":49361,\"start\":49358},{\"end\":49382,\"start\":49376},{\"end\":49396,\"start\":49391},{\"end\":49712,\"start\":49701},{\"end\":49731,\"start\":49723},{\"end\":49745,\"start\":49741},{\"end\":50223,\"start\":50218},{\"end\":50514,\"start\":50513},{\"end\":50528,\"start\":50522},{\"end\":50530,\"start\":50529},{\"end\":50546,\"start\":50540},{\"end\":50560,\"start\":50553}]", "bib_author_last_name": "[{\"end\":38409,\"start\":38401},{\"end\":38426,\"start\":38420},{\"end\":38437,\"start\":38435},{\"end\":38452,\"start\":38447},{\"end\":38463,\"start\":38459},{\"end\":38478,\"start\":38472},{\"end\":38495,\"start\":38486},{\"end\":38510,\"start\":38504},{\"end\":38817,\"start\":38798},{\"end\":38825,\"start\":38821},{\"end\":38841,\"start\":38834},{\"end\":38856,\"start\":38849},{\"end\":38874,\"start\":38867},{\"end\":38883,\"start\":38876},{\"end\":39227,\"start\":39206},{\"end\":39242,\"start\":39236},{\"end\":39258,\"start\":39251},{\"end\":39538,\"start\":39524},{\"end\":39555,\"start\":39549},{\"end\":39573,\"start\":39564},{\"end\":39962,\"start\":39959},{\"end\":39978,\"start\":39971},{\"end\":39992,\"start\":39986},{\"end\":40247,\"start\":40242},{\"end\":40261,\"start\":40254},{\"end\":40275,\"start\":40271},{\"end\":40290,\"start\":40286},{\"end\":40299,\"start\":40294},{\"end\":40317,\"start\":40308},{\"end\":40325,\"start\":40319},{\"end\":40722,\"start\":40719},{\"end\":40743,\"start\":40732},{\"end\":41247,\"start\":41243},{\"end\":41269,\"start\":41256},{\"end\":41578,\"start\":41575},{\"end\":41590,\"start\":41584},{\"end\":41606,\"start\":41600},{\"end\":41844,\"start\":41837},{\"end\":42062,\"start\":42046},{\"end\":42078,\"start\":42070},{\"end\":42108,\"start\":42092},{\"end\":42137,\"start\":42116},{\"end\":42155,\"start\":42149},{\"end\":42163,\"start\":42159},{\"end\":42175,\"start\":42170},{\"end\":42189,\"start\":42177},{\"end\":42570,\"start\":42564},{\"end\":42746,\"start\":42744},{\"end\":42765,\"start\":42755},{\"end\":42783,\"start\":42777},{\"end\":43089,\"start\":43087},{\"end\":43108,\"start\":43099},{\"end\":43124,\"start\":43115},{\"end\":43139,\"start\":43133},{\"end\":43327,\"start\":43317},{\"end\":43347,\"start\":43336},{\"end\":43557,\"start\":43541},{\"end\":43572,\"start\":43567},{\"end\":43579,\"start\":43574},{\"end\":43810,\"start\":43804},{\"end\":43820,\"start\":43818},{\"end\":44025,\"start\":44023},{\"end\":44038,\"start\":44032},{\"end\":44051,\"start\":44045},{\"end\":44065,\"start\":44057},{\"end\":44253,\"start\":44246},{\"end\":44275,\"start\":44266},{\"end\":44291,\"start\":44287},{\"end\":44308,\"start\":44301},{\"end\":44319,\"start\":44314},{\"end\":44331,\"start\":44327},{\"end\":44344,\"start\":44339},{\"end\":44357,\"start\":44351},{\"end\":44367,\"start\":44359},{\"end\":44817,\"start\":44814},{\"end\":44828,\"start\":44824},{\"end\":44838,\"start\":44832},{\"end\":44854,\"start\":44848},{\"end\":44874,\"start\":44864},{\"end\":44890,\"start\":44883},{\"end\":44898,\"start\":44892},{\"end\":45225,\"start\":45205},{\"end\":45239,\"start\":45233},{\"end\":45249,\"start\":45241},{\"end\":45458,\"start\":45454},{\"end\":45477,\"start\":45466},{\"end\":45491,\"start\":45485},{\"end\":45504,\"start\":45498},{\"end\":45524,\"start\":45514},{\"end\":45539,\"start\":45531},{\"end\":45558,\"start\":45548},{\"end\":46060,\"start\":46053},{\"end\":46073,\"start\":46067},{\"end\":46089,\"start\":46083},{\"end\":46104,\"start\":46096},{\"end\":46116,\"start\":46114},{\"end\":46133,\"start\":46123},{\"end\":46476,\"start\":46470},{\"end\":46495,\"start\":46488},{\"end\":46510,\"start\":46505},{\"end\":46666,\"start\":46662},{\"end\":46681,\"start\":46674},{\"end\":46697,\"start\":46691},{\"end\":46712,\"start\":46708},{\"end\":46721,\"start\":46714},{\"end\":47075,\"start\":47069},{\"end\":47091,\"start\":47082},{\"end\":47110,\"start\":47099},{\"end\":47335,\"start\":47329},{\"end\":47346,\"start\":47342},{\"end\":47366,\"start\":47356},{\"end\":47380,\"start\":47374},{\"end\":47622,\"start\":47614},{\"end\":47637,\"start\":47631},{\"end\":47652,\"start\":47646},{\"end\":47668,\"start\":47662},{\"end\":47684,\"start\":47678},{\"end\":48152,\"start\":48146},{\"end\":48168,\"start\":48162},{\"end\":48185,\"start\":48180},{\"end\":48201,\"start\":48194},{\"end\":48209,\"start\":48203},{\"end\":48478,\"start\":48472},{\"end\":48487,\"start\":48480},{\"end\":48691,\"start\":48688},{\"end\":48710,\"start\":48705},{\"end\":48723,\"start\":48717},{\"end\":48738,\"start\":48731},{\"end\":48750,\"start\":48740},{\"end\":49017,\"start\":49010},{\"end\":49284,\"start\":49272},{\"end\":49301,\"start\":49293},{\"end\":49312,\"start\":49309},{\"end\":49328,\"start\":49320},{\"end\":49343,\"start\":49336},{\"end\":49356,\"start\":49350},{\"end\":49374,\"start\":49362},{\"end\":49389,\"start\":49383},{\"end\":49408,\"start\":49397},{\"end\":49721,\"start\":49713},{\"end\":49739,\"start\":49732},{\"end\":49752,\"start\":49746},{\"end\":50233,\"start\":50224},{\"end\":50520,\"start\":50515},{\"end\":50538,\"start\":50531},{\"end\":50551,\"start\":50547},{\"end\":50568,\"start\":50561},{\"end\":50573,\"start\":50570}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1607.07086\",\"id\":\"b0\"},\"end\":38754,\"start\":38342},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":205289650},\"end\":39072,\"start\":38756},{\"attributes\":{\"doi\":\"arXiv:1206.6392\",\"id\":\"b2\"},\"end\":39516,\"start\":39074},{\"attributes\":{\"doi\":\"arXiv:1608.07187\",\"id\":\"b3\"},\"end\":39882,\"start\":39518},{\"attributes\":{\"doi\":\"arXiv:1611.03477\",\"id\":\"b4\"},\"end\":40176,\"start\":39884},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1594370},\"end\":40622,\"start\":40178},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":579926},\"end\":41110,\"start\":40624},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7423230},\"end\":41508,\"start\":41112},{\"attributes\":{\"doi\":\"arXiv:1512.08562\",\"id\":\"b8\"},\"end\":41778,\"start\":41510},{\"attributes\":{\"id\":\"b9\"},\"end\":41951,\"start\":41780},{\"attributes\":{\"doi\":\"arXiv:1610.02415\",\"id\":\"b10\"},\"end\":42504,\"start\":41953},{\"attributes\":{\"doi\":\"arXiv preprint:1308.0850\",\"id\":\"b11\"},\"end\":42693,\"start\":42506},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8769938},\"end\":43018,\"start\":42695},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":890737},\"end\":43286,\"start\":43020},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1915014},\"end\":43476,\"start\":43288},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7522435},\"end\":43749,\"start\":43478},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b16\"},\"end\":43962,\"start\":43751},{\"attributes\":{\"doi\":\"arXiv:1606.01541\",\"id\":\"b17\"},\"end\":44242,\"start\":43964},{\"attributes\":{\"doi\":\"arXiv:1509.02971\",\"id\":\"b18\"},\"end\":44673,\"start\":44244},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9197196},\"end\":45152,\"start\":44675},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":20724976},\"end\":45394,\"start\":45154},{\"attributes\":{\"doi\":\"arXiv:1312.5602\",\"id\":\"b21\"},\"end\":45796,\"start\":45396},{\"attributes\":{\"id\":\"b22\"},\"end\":45972,\"start\":45798},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3631537},\"end\":46432,\"start\":45974},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2984847},\"end\":46625,\"start\":46434},{\"attributes\":{\"id\":\"b25\"},\"end\":46652,\"start\":46627},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b26\"},\"end\":46977,\"start\":46654},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":2168740},\"end\":47321,\"start\":46979},{\"attributes\":{\"doi\":\"arXiv:1511.05952\",\"id\":\"b28\"},\"end\":47573,\"start\":47323},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16046818},\"end\":48050,\"start\":47575},{\"attributes\":{\"doi\":\"arXiv:1701.01329\",\"id\":\"b30\"},\"end\":48440,\"start\":48052},{\"attributes\":{\"id\":\"b31\"},\"end\":48617,\"start\":48442},{\"attributes\":{\"doi\":\"arXiv:1604.08723\",\"id\":\"b32\"},\"end\":48956,\"start\":48619},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":393501},\"end\":49221,\"start\":48958},{\"attributes\":{\"doi\":\"CoRR abs/1609.03499\",\"id\":\"b34\"},\"end\":49646,\"start\":49223},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":9026666},\"end\":50113,\"start\":49648},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5445756},\"end\":50463,\"start\":50115},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":336219},\"end\":50770,\"start\":50465}]", "bib_title": "[{\"end\":38796,\"start\":38756},{\"end\":40231,\"start\":40178},{\"end\":40709,\"start\":40624},{\"end\":41235,\"start\":41112},{\"end\":42733,\"start\":42695},{\"end\":43076,\"start\":43020},{\"end\":43310,\"start\":43288},{\"end\":43532,\"start\":43478},{\"end\":44803,\"start\":44675},{\"end\":45194,\"start\":45154},{\"end\":46042,\"start\":45974},{\"end\":46464,\"start\":46434},{\"end\":47060,\"start\":46979},{\"end\":47607,\"start\":47575},{\"end\":49000,\"start\":48958},{\"end\":49699,\"start\":49648},{\"end\":50216,\"start\":50115},{\"end\":50511,\"start\":50465}]", "bib_author": "[{\"end\":38411,\"start\":38393},{\"end\":38428,\"start\":38411},{\"end\":38439,\"start\":38428},{\"end\":38454,\"start\":38439},{\"end\":38465,\"start\":38454},{\"end\":38480,\"start\":38465},{\"end\":38497,\"start\":38480},{\"end\":38512,\"start\":38497},{\"end\":38819,\"start\":38798},{\"end\":38827,\"start\":38819},{\"end\":38843,\"start\":38827},{\"end\":38858,\"start\":38843},{\"end\":38876,\"start\":38858},{\"end\":38885,\"start\":38876},{\"end\":39229,\"start\":39198},{\"end\":39244,\"start\":39229},{\"end\":39260,\"start\":39244},{\"end\":39540,\"start\":39518},{\"end\":39557,\"start\":39540},{\"end\":39575,\"start\":39557},{\"end\":39964,\"start\":39954},{\"end\":39980,\"start\":39964},{\"end\":39994,\"start\":39980},{\"end\":40249,\"start\":40233},{\"end\":40263,\"start\":40249},{\"end\":40277,\"start\":40263},{\"end\":40292,\"start\":40277},{\"end\":40301,\"start\":40292},{\"end\":40319,\"start\":40301},{\"end\":40327,\"start\":40319},{\"end\":40724,\"start\":40711},{\"end\":40745,\"start\":40724},{\"end\":41249,\"start\":41237},{\"end\":41271,\"start\":41249},{\"end\":41580,\"start\":41571},{\"end\":41592,\"start\":41580},{\"end\":41608,\"start\":41592},{\"end\":41846,\"start\":41837},{\"end\":42064,\"start\":42039},{\"end\":42080,\"start\":42064},{\"end\":42110,\"start\":42080},{\"end\":42139,\"start\":42110},{\"end\":42157,\"start\":42139},{\"end\":42165,\"start\":42157},{\"end\":42177,\"start\":42165},{\"end\":42191,\"start\":42177},{\"end\":42572,\"start\":42559},{\"end\":42748,\"start\":42735},{\"end\":42767,\"start\":42748},{\"end\":42785,\"start\":42767},{\"end\":43091,\"start\":43078},{\"end\":43110,\"start\":43091},{\"end\":43126,\"start\":43110},{\"end\":43141,\"start\":43126},{\"end\":43329,\"start\":43312},{\"end\":43349,\"start\":43329},{\"end\":43559,\"start\":43534},{\"end\":43574,\"start\":43559},{\"end\":43581,\"start\":43574},{\"end\":43812,\"start\":43795},{\"end\":43822,\"start\":43812},{\"end\":44027,\"start\":44017},{\"end\":44040,\"start\":44027},{\"end\":44053,\"start\":44040},{\"end\":44067,\"start\":44053},{\"end\":44255,\"start\":44244},{\"end\":44277,\"start\":44255},{\"end\":44293,\"start\":44277},{\"end\":44310,\"start\":44293},{\"end\":44321,\"start\":44310},{\"end\":44333,\"start\":44321},{\"end\":44346,\"start\":44333},{\"end\":44359,\"start\":44346},{\"end\":44369,\"start\":44359},{\"end\":44819,\"start\":44805},{\"end\":44830,\"start\":44819},{\"end\":44840,\"start\":44830},{\"end\":44856,\"start\":44840},{\"end\":44876,\"start\":44856},{\"end\":44892,\"start\":44876},{\"end\":44900,\"start\":44892},{\"end\":45227,\"start\":45196},{\"end\":45241,\"start\":45227},{\"end\":45251,\"start\":45241},{\"end\":45460,\"start\":45444},{\"end\":45479,\"start\":45460},{\"end\":45493,\"start\":45479},{\"end\":45506,\"start\":45493},{\"end\":45526,\"start\":45506},{\"end\":45541,\"start\":45526},{\"end\":45560,\"start\":45541},{\"end\":46062,\"start\":46044},{\"end\":46075,\"start\":46062},{\"end\":46091,\"start\":46075},{\"end\":46106,\"start\":46091},{\"end\":46118,\"start\":46106},{\"end\":46135,\"start\":46118},{\"end\":46478,\"start\":46466},{\"end\":46497,\"start\":46478},{\"end\":46512,\"start\":46497},{\"end\":46668,\"start\":46654},{\"end\":46683,\"start\":46668},{\"end\":46699,\"start\":46683},{\"end\":46714,\"start\":46699},{\"end\":46723,\"start\":46714},{\"end\":47077,\"start\":47062},{\"end\":47093,\"start\":47077},{\"end\":47112,\"start\":47093},{\"end\":47337,\"start\":47325},{\"end\":47348,\"start\":47337},{\"end\":47368,\"start\":47348},{\"end\":47382,\"start\":47368},{\"end\":47624,\"start\":47609},{\"end\":47639,\"start\":47624},{\"end\":47654,\"start\":47639},{\"end\":47670,\"start\":47654},{\"end\":47686,\"start\":47670},{\"end\":48154,\"start\":48142},{\"end\":48170,\"start\":48154},{\"end\":48187,\"start\":48170},{\"end\":48203,\"start\":48187},{\"end\":48211,\"start\":48203},{\"end\":48480,\"start\":48470},{\"end\":48489,\"start\":48480},{\"end\":48693,\"start\":48686},{\"end\":48712,\"start\":48693},{\"end\":48725,\"start\":48712},{\"end\":48740,\"start\":48725},{\"end\":48752,\"start\":48740},{\"end\":49019,\"start\":49002},{\"end\":49286,\"start\":49266},{\"end\":49303,\"start\":49286},{\"end\":49314,\"start\":49303},{\"end\":49330,\"start\":49314},{\"end\":49345,\"start\":49330},{\"end\":49358,\"start\":49345},{\"end\":49376,\"start\":49358},{\"end\":49391,\"start\":49376},{\"end\":49410,\"start\":49391},{\"end\":49723,\"start\":49701},{\"end\":49741,\"start\":49723},{\"end\":49754,\"start\":49741},{\"end\":50235,\"start\":50218},{\"end\":50522,\"start\":50513},{\"end\":50540,\"start\":50522},{\"end\":50553,\"start\":50540},{\"end\":50570,\"start\":50553},{\"end\":50575,\"start\":50570}]", "bib_venue": "[{\"end\":40822,\"start\":40792},{\"end\":47829,\"start\":47766},{\"end\":49895,\"start\":49833},{\"end\":50281,\"start\":50262},{\"end\":50597,\"start\":50581},{\"end\":38391,\"start\":38342},{\"end\":38901,\"start\":38885},{\"end\":39196,\"start\":39074},{\"end\":39677,\"start\":39591},{\"end\":39952,\"start\":39884},{\"end\":40383,\"start\":40327},{\"end\":40790,\"start\":40745},{\"end\":41297,\"start\":41271},{\"end\":41569,\"start\":41510},{\"end\":41835,\"start\":41780},{\"end\":42037,\"start\":41953},{\"end\":42557,\"start\":42506},{\"end\":42841,\"start\":42785},{\"end\":43145,\"start\":43141},{\"end\":43367,\"start\":43349},{\"end\":43597,\"start\":43581},{\"end\":43793,\"start\":43751},{\"end\":44015,\"start\":43964},{\"end\":44436,\"start\":44385},{\"end\":44905,\"start\":44900},{\"end\":45258,\"start\":45251},{\"end\":45442,\"start\":45396},{\"end\":45884,\"start\":45798},{\"end\":46184,\"start\":46135},{\"end\":46516,\"start\":46512},{\"end\":46636,\"start\":46629},{\"end\":46793,\"start\":46739},{\"end\":47141,\"start\":47112},{\"end\":47764,\"start\":47686},{\"end\":48140,\"start\":48052},{\"end\":48468,\"start\":48442},{\"end\":48684,\"start\":48619},{\"end\":49075,\"start\":49019},{\"end\":49264,\"start\":49223},{\"end\":49831,\"start\":49754},{\"end\":50260,\"start\":50235},{\"end\":50579,\"start\":50575}]"}}}, "year": 2023, "month": 12, "day": 17}