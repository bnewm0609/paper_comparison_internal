{"id": 5437340, "updated": "2022-07-14 04:47:46.038", "metadata": {"title": "Place Recognition with ConvNet Landmarks: Viewpoint-Robust, Condition-Robust, Training-Free", "authors": "[{\"first\":\"Niko\",\"last\":\"S\u00fcnderhauf\",\"middle\":[]},{\"first\":\"Sareh\",\"last\":\"Shirazi\",\"middle\":[]},{\"first\":\"Adam\",\"last\":\"Jacobson\",\"middle\":[]},{\"first\":\"Feras\",\"last\":\"Dayoub\",\"middle\":[]},{\"first\":\"Edward\",\"last\":\"Pepperell\",\"middle\":[]},{\"first\":\"Ben\",\"last\":\"Upcroft\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Milford\",\"middle\":[]}]", "venue": "Robotics: Science and Systems", "journal": null, "publication_date": {"year": 2015, "month": null, "day": null}, "abstract": "\u2014Place recognition has long been an incompletely solved problem in that all approaches involve signi\ufb01cant com-promises. Current methods address many but never all of the critical challenges of place recognition \u2013 viewpoint-invariance, condition-invariance and minimizing training requirements. Here we present an approach that adapts state-of-the-art object proposal techniques to identify potential landmarks within an image for place recognition. We use the astonishing power of convolutional neural network features to identify matching landmark proposals between images to perform place recognition over extreme appearance and viewpoint variations. Our system does not require any form of training, all components are generic enough to be used off-the-shelf. We present a range of challenging experiments in varied viewpoint and environmental conditions. We demonstrate superior performance to current state-of-the- art techniques. Furthermore, by building on existing and widely used recognition frameworks, this approach provides a highly compatible place recognition system with the potential for easy integration of other techniques such as object detection and semantic scene interpretation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2296602078", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/rss/SunderhaufSJDPU15", "doi": "10.15607/rss.2015.xi.022"}}, "content": {"source": {"pdf_hash": "d07339520fecaaaa4faa18af1aaaae4755516a0d", "pdf_src": "Adhoc", "pdf_uri": "[\"https://web.archive.org/web/20200505083113/https:/eprints.qut.edu.au/84931/21/rss15_placeRec.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://doi.org/10.15607/rss.2015.xi.022", "status": "BRONZE"}}, "grobid": {"id": "b4ce0f67701949425dcb49e1d06002ae2142a488", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d07339520fecaaaa4faa18af1aaaae4755516a0d.txt", "contents": "\n\n\n\n\nARC Centre of Excellence for Robotic Vision\nQueensland University of Technology\n4001BrisbaneQLDAustralia\nThis may be the author's version of a work that was submitted/accepted for publication in the following source: Suenderhauf, Niko, Shirazi, Sareh, Jacobson, Adam, Dayoub, Feras, Pep-perell, Edward, Upcroft, Ben, & Milford, Michael (2015) Place recognition with ConvNet landmarks: Viewpoint-robust, condition-robust, training-free. In Hsu, D (Ed.) Robotics: Science and Systems XI:. Robotics: Science and Systems Conference, http://www.roboticsproceedings.org/, pp. 1-10. This file was downloaded from: https://eprints.qut.edu.au/84931/\n\n\nI. INTRODUCTION\n\nVisual place recognition research has been dominated by sophisticated local feature-based techniques such as SIFT and SURF keypoints, hand-crafted global image descriptors such as GIST and bag-of-words approaches. However, as robots operate for longer periods of time in real-world environments, the problem of changing environmental conditions has come to the fore, where conventional recognition approaches fail. To address this problem, a number of techniques have been adopted -matching image sequences [27,29,33,28], creating shadow-invariant images [6,43,25,24], learning salient image regions [26] or learning temporal models that allow the prediction of occurring changes [31]. Recent research has also demonstrated how generic deep learning-based features trained for object recognition can be successfully applied in the domain of place recognition [41,3]. However, all current approaches have introduced at least one significant performance or usability compromise, whether it be a lack of invariance to camera viewpoint changes [27,28], extensive environment-specific training requirements [26], or the lack of appearance change robustness [7]. If visual place recognition is to be truly robust, it must simultaneously address three critical challenges: 1) condition invariance; 2) viewpoint invariance; and 3) generality (no environment-specific training requirements).\n\nIn this paper, we present a unified approach that addresses all three of these challenges. We use a state-of-the-art object proposal method to discover potential landmarks in the images. A convolutional network (ConvNet) is then used to extract general purpose features for each of these landmark proposals. We show that the ConvNet features are robust to both appearance and viewpoint change; the first two critical challenges. We also emphasize that landmark proposals require no training and the ConvNet is pre-trained on ImageNet, a generic image database; the third critical challenge. By conducting experiments on a number of datasets we show that our system is training-free in that no task-specific or even site-specific training is required. We also highlight that only single images are required for matching and the system does not require image sequences. We demonstrate the generality of our system on a number of existing datasets and introduce new challenging place recognition datasets, while comparing to state of the art methods.\n\nThe novel contributions of this paper are: 1) A place recognition system that is robust to viewpoint and appearance variation, requiring no environment specific training, and 2) The introduction of new challenging datasets exhibiting extreme viewpoint and appearance variation. The paper proceeds as follows. Section II provides a brief overview of related work. The method is described in detail in Section III followed by an overview of the four sets of experiments. We present results in Section V before concluding with a discussion and outlining future work.\n\n\nII. RELATED WORK\n\nThe focus of research in place recognition has recently moved from recognizing scenes without significant appearance changes to more challenging, but also more realistic changing environments.\n\nPlace Recognition: Methods that address the place recognition problem span from matching sequences of images [27,17,40,33,29], transforming images to become invariant against common scene changes such as shadows [6,43,25,24,21], learning how environments change over time and predicting these changes in image space [30,21,31], particle filterbased approaches that build up place recognition hypotheses over time [23,39,22], or build a map of experiences that cover the different appearances of a place over time [5].\n\nLearning how the appearance of the environment changes generally requires training data with known frame correspondences. [17] builds a database of observed features over the course of a day and night. [30,31] presents an approach that learns systematic scene changes in order to improve performance on a seasonal change dataset. [26] learns salient regions in images of the same place with different environmental conditions. Beyond the limitation of requiring training data, the generality of these methods is also currently unknown; these methods have only been demonstrated to work in the same environment and on the same or very similar types of environmental change to that encountered in the training datasets.\n\nAlthough point feature-based methods were shown to be robust against viewpoint changes [7,8,38], to the authors' knowledge, significant changes in both viewpoint and environmental conditions have not been addressed in the literature. We show that robustness to variation in both cases can be addressed without site-specific training.\n\nFeature-based Approaches: SIFT [20], SURF [1] and a number of subsequent feature detectors have been demonstrated to display a significant degree of pose invariance but only a limited degree of condition-invariance (illumination, atmospheric conditions, shadows, seasons). Perceptual change as drastic as that illustrated in Fig. 1 has been shown to be challenging for conventional feature detectors [27,44] and while FAB-MAP [7] is robust with respect to viewpoint changes, it is known to fail in conditions with severe appearance changes [29,31,13]. Furthermore, [11,34] argued that FAB-MAP does not generalize well to new environments without learning a new site-specific vocabulary. [26] shows that patches and region-based methods within an image can exhibit the same robustness as whole-image techniques while retaining some robustness to scale variation, and thus achieve some of the advantages of both point and wholeimage features. However, extensive site-specific training was required. In this research we extend the advantages of regionbased methods to address both viewpoint and environmental changes without the requirement for site-specific training.\n\nA commonality between all these approaches is that they rely on a fixed set of hand-crafted traditional features or operate on the raw pixel level. A recent trend in computer vision, and especially in the field of object recognition and detection, is to exploit learned features using deep convolutional networks (ConvNets). The most prominent example of this trend is the annual ImageNet Large Scale Visual Recognition Challenge where for the past two years many of the participants have used ConvNet features [36].\n\nSeveral research groups have shown that ConvNets outperform classical approaches for object classification or detection that are based on hand-crafted features [19,37,10,12,35]. The availability of pre-trained network models makes it easy to experiment with such approaches for different tasks: the software packages Overfeat [37] and Decaf [10] or its successor Caffe [16], provide similar network architectures that were pre-trained on the ImageNet ILSVRC dataset [36].\n\nRecent studies have shown that state-of-the-art performance in place recognition can be achieved with networks trained using generic data: [41] demonstrated that ConvNet features representing the whole image outperform current methods for changing environmental conditions. However, whole-image features suffer from sensitivity to viewpoint change. We show that by combining the power of ConvNets and region-based features rather than using whole-image representations, a large degree of robustness to viewpoint change can be achieved.\n\nConsequently in this research we build on the best performing aspects of the state of the art; the recognition performance of ConvNet approaches [41], and the robustness of regionbased methods to viewpoint change [26].\n\n\nIII. PROPOSED SYSTEM\n\nIn this section we describe the five key components of our proposed place recognition system: 1) landmark proposal extraction from the current image 2) calculation of a ConvNet feature for each proposal 3) projection of the features into a lower dimensional space 4) calculation of a matching score for each previously seen image 5) calculation of the best match Fig. 2 illustrates our system. The approach has several properties that distinguishes it from previous work: volutional network [16] to calculate features and a generic object proposal system to extract landmark proposals from images. \u2022 By using a landmark proposal system (Edge Boxes [45]) that was designed to find arbitrary objects in scenes, we extract recognizable and stable regions in the images that automatically tend to be reliable landmarks. \u2022 Relying on landmark regions rather than the whole image to describe a scene significantly improves the robustness against view point changes or partial occlusions in the scenes. \u2022 ConvNet features have been shown to be more stable against appearance and condition changes than other methods [41]. Since we use these robust features as descriptors for the extracted landmarks, we inherit their robustness against appearance changes such as induced by weather, seasons, or the time of day. \u2022 Since both the landmark proposal and the feature extraction system are used as exchangeable black boxes, any future improvement on these methods by the robotics or computer vision community can be immediately exploited by exchanging the currently used algorithms and network architecture with improved future versions. \u2022 The incorporation of a complete object detection pipeline readily enables future enhancements, such as using the output of the object classifier layer of the ConvNet to discard those proposed landmarks that are likely to contain dynamic objects or scene structures that are otherwise known to be unsuitable as landmarks.\n\n\nA. Bottom-Up Object Proposals as Region Landmarks\n\nIn contrast to previous work we exploit the bottom-up object proposal methods that have been developed in the computer vision community. These methods usually serve as a first step in a general purpose object detection pipeline and extract bounding boxes from an image that are likely to contain an interesting object. R-CNN [12] is a prominent recent example of such a system that extracts approximately 2000 proposal regions per image and passes all of them through a Convolutional Network classifier that determines if an object is present and which of the known classes it belongs to.\n\nTo extract landmarks we apply Edge Boxes, an object proposal method developed by Zitnick and Doll\u00e1r [45]. It has been shown to outperform other recent proposal methods such as BING [4] or Selective Search [42] in the context of object detection and is considered the current state of the art by the computer vision community [15]. In our experiments, we extract 50 or 100 landmark proposals per image.\n\nEdge Boxes mainly relies on the observation that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. It measures an objectness score by comparing the number of edges within each bounding box with the number of edges passing through it. \n\n\nB. ConvNet Features as Robust Landmark Descriptors\n\nAfter landmark proposals have been extracted, we calculate an individual feature vector to describe their appearance. Ideally, this feature vector should be robust against changes in appearance induced by weather, seasonal effects, time of day, and -to a certain degree -occlusions and changes in perspective. While previous work often relied on standard feature descriptors like SIFT or SURF [7] or hand-crafted features to describe landmarks for place recognition, we pass each landmark proposal through a Convolutional Network to extract a feature vector that is stable under the conditions mentioned above.\n\nWe build upon the astonishing results from the computer vision community where ConvNets have been shown to outperform all previous methods in the area of object detection and recognition [19,37,10,12,35]. In robotics, the first publications that exploited the beneficial properties of these generic pre-trained features for place recognition appeared very recently [3,41]. Most available ConvNet frameworks (Overfeat [37], Decaf [10] and its successor Caffe [16]) follow the same principled architecture that was introduced by AlexNet [19]. The network consists of 5 combined layers that perform a convolution, followed by a nonlinear activation function (rectified linear units), and spatial pooling. Three fully connected layers plus a subsequent soft-max layer form the upper parts in the network hierarchy. [41] has shown that the features of the mid-level features from the 3rd convolutional layer (hereafter called conv3) are highly invariant against the appearance changes that are caused by different weather conditions, seasons or the time of day. They found the cosine distance to be a suitable measure between two of these features. We follow their results and extract a conv3 feature for each of the landmark proposals in an image, utilizing the AlexNet network [19] as implemented by Caffe [16]. It was pre-trained on the ImageNet dataset. We modified the ConvNet so that only the layers up to conv3 are calculated. This allows us to extract a feature within 15 ms (a speed-up of 6.7\u00d7). The landmarks are resized to the expected input size of 231 \u00d7 231 \u00d7 3 pixels. This procedure has been suggested in the object detection literature [12] and does not seem to degrade the performance.\n\n\nC. Random Projections for Dimensionality Reduction\n\nThe features produced by the conv3 layer of the convolutional network are of size 384 \u00d7 13 \u00d7 13, i.e. for each proposed landmark we calculate a 64,896 dimensional feature. Calculating the pairwise cosine distances between 50 or 100 of those high-dimensional features per image during the image matching process is an expensive operation.\n\nTo make the matching process more efficient, we apply dimensionality reduction. According to the Johnson-Lindenstrauss-Lemma [18] a set of points in a highdimensional space can be linearly embedded in a lower dimensional space while maintaining the pairwise euclidean distances between the points up to an epsilon factor:\n(1 \u2212 ) u \u2212 v 2 \u2264 Au \u2212 Av 2 \u2264 (1 + ) u \u2212 v 2 (1)\nwhere u, v \u2208 R n are vectors of the original high dimensional space and A \u2208 R m\u00d7n is a projection into a lower dimensional space R m with m n. We leverage this lemma and apply a Gaussian Random Projection [9,2] to transfer the original features to a space of much lower dimension. The elements of the projection matrix A are drawn from a Gaussian distribution. In our experiments, we project the 64,896 original dimensions to 512, 1024, and 4096 dimensions and compare their relative performances.\n\nThe Johnson-Lindenstrauss-Lemma is formulated in terms of the euclidean distance between points, but due to the spherical distribution of the features in their high dimensional space, we found it to be applicable for the cosine distance too.\n\n\nD. Image Matching\n\nTo determine the similarity between two images I a and I b , we perform matching between all landmark proposals l a i and l b j that were extracted from both images. The landmark matching is performed using a nearest neighbor search based on the cosine distance d ij of their descriptors (after applying the dimensionality reduction as described above) and applies crosschecking, i.e. only mutual matches are accepted.\n\nAs a second step, we score each matched landmark pair (l a i , l b j ) by the similarity of the shape of their bounding boxes. Let w i , h i , w j , and h j be the width and height of the matched landmark proposals. We then calculate their shape similarity measure s ij :\ns ij = exp 1 2 |w i \u2212 w j | max(w i , w j ) + |h i \u2212 h j | max(h i , h j )(2)\nThe overall similarity between both images I a and I b is then calculated as\nS a,b = 1 \u221a n a \u00b7 n b ij 1 \u2212 (d ij \u00b7 s ij )(3)\nwhere d ij is the cosine distance between both landmarks and n a , n b are the number of extracted landmarks proposals in both images, including the non-matched ones. In our experiments, n a = n b = 50 or 100. The shape similarity score s ij penalizes false positive matches between landmarks that have a similar conv3 descriptor but are of significantly different shape. Empirically we found this improves overall performance by a small but notable margin. The matched landmarks can still vary in size and aspect ratio, since the appearance-based cosine distance between two landmarks has a bigger influence on the overall similarity score.\n\nTo retrieve the best matching database image I a for a query image I b , we search for the database image with the highest similarity score, i.e. argmax a S a,b . The matching image is found using the single best match only.\n\n\nIV. EVALUATION AND RESULTS\n\nThis section describes the conducted experiments and their results. We compare the performance of the proposed system against several state of the art methods using precision-recall plots. Concretely, we compare against the whole image, single ConvNet feature system proposed by [41], the featurebased method FAB-MAP [7] (using OpenFABMAP [14]) and the sequence-based approaches SeqSLAM [27] (using the OpenSeqSLAM implementation [40]) and SMART [33]. FAB-MAP's vocabulary was trained on the St. Lucia dataset 1 . We found that training a specific vocabulary on the test dataset for each of the individual experiments in the following increased FAB-MAP's performance slightly, but such an approach would be infeasible in practice.\n\n\nA. Place Recognition with Viewpoint Variations\n\nIn this experiment we evaluate the robustness to viewpoint variations on the Gardens Point dataset that consists of footage recorded by a pedestrian. It exhibits viewpoint variations that occur from walking on the left or right side of a pathway and mild appearance changes mainly caused by dynamic objects We clearly outperform the whole-image ConvNet-based method proposed by [41] and OpenFABMAP [14]. such as pedestrians. The dataset has been used in a number of place recognition evaluations before (e.g. [41]) and is available online 2 . Fig. 4 shows two example frames along with the extracted and matched landmarks. When comparing to the results obtained by the FAB-MAP [7,14] and the wholeimage ConvNet based method of [41], we see in Fig. 3 that our method outperforms both approaches significantly, coming close to perfect performance.\n\n\nB. Place Recognition with Viewpoint and Appearance Changes -The Mapillary Dataset\n\nThese experiments introduce a new dataset exhibiting significant changes in viewpoint and moderate changes in appearance. Mapillary 3 is a crowdsourced alternative to Google Street View. It is a collaborative photo mapping initiative that allows users to upload sequences of GPS-tagged photos and provides an API interface to download these sequences along with their meta data. Since many roads have been mapped by more than one user, Mapillary is an ideal platform to harvest datasets for place recognition under every-day conditions. We downloaded three sequences of images that exerted significant viewpoint changes and make these available to the community along with ground truth data. For example, the images of the Berlin Kurf\u00fcrstendamm (201 + 222 frames, 3 km) and Berlin Halenseestra\u00dfe sequence (157 + 67 frames, 3 km) have been recorded by a bike rider on the bike lane and from the upper deck of a tourist bus, or a dashboard camera in a car respectively. This results in a large variation of viewpoint, as can be seen in Figs. 1, 5, and 6. These figures also illustrate some of the landmark proposals that were correctly matched between the image pairs and demonstrate that the matching process is robust against a reasonable amount of scaling, occlusion, illumination changes, and perspective distortion.\n\nAs we can see from the precision recall plots in Fig. 7, our proposed method outperforms the approach of [41] (using a single ConvNet feature over the whole image), FAB-MAP [7] (utilizing the OpenFABMAP implementation [14]) and SMART [33] by a large margin. This underlines the increased robustness against viewpoint changes.\n\nThe results also illustrate the effect of different parameters on system performance. Using more landmark proposals (i.e. 100 instead of 50) leads to more accurate place recognition. Performance also does not appear to drop significantly when applying dimensionality reduction using Gaussian Random Projections. The differently colored lines represent the exact cosine distance over all 64,896 dimensions (red), and the reduced feature spaces of 4096 (black), 1024 (green), and 512 (cyan) dimensions. As we can see, the performance drops slightly with reduced dimensionality, but the difference between the exact cosine distance and the reduced spaces of 1024 and 4096 is marginal, especially when using 100 landmarks per image.\n\nAnother sequence (Malm\u00f6 John Ericssons V\u00e4g, 221 + 378 frames, 4 km) from Mapillary contains only mild viewpoint changes; both traverses were recorded from the same lane on a road. However, the weather conditions between the two recordings were very different, changing from a bright sunny day to a murky overcast day with wet roads after a rain. Fig. 8 shows two example scenes. In the results illustrated in Fig. 9 we see that when using only 50 landmarks, the performance is worse than the whole-image system of [41], for The changes in viewpoint are severe but our proposed method is able to extract landmarks and correctly match them between a large number of scenes.\n\nboth the exact cosine distance and the reduced feature space of 1024 dimensions. When the number of landmark proposals is increased to 100 the performance is superior to [41], even when applying dimensionality reduction. Our approach outperforms FAB-MAP [7,14] for all tested parameters.\n\n\nC. The Library Robot Indoor Dataset\n\nIn this experiment we evaluate our approach on a dataset captured by a service robot in a public library. The robot traversed the library once during the day and a second time during the night. Appearance changes were induced by the different external and internal lighting conditions, while people and moved furniture caused structural changes. In contrast to the previous datasets, this experiment is more realistic since the robot did not revisit all places, i.e. there are true negative non-matching scenes. Furthermore, the robot encounters many weakly textured areas and large parts of the environment suffer To better quantify the robustness to viewpoint changes, we systematically translate a camera in a complex scene with both close and distant objects. We use the image from the original position as the reference image and move the camera sideways in 10 cm increments under changed illumination conditions. The images from the changed conditions are used as query images, i.e. we attempt to match them with the original image. To make the experiment more significant, we repeat it in 8 different scenes and add 678 unrelated indoor scenes to the dataset. Fig. 11 plots the average accuracy over the sideways camera displacement. Our approach outperforms FAB-MAP [7,14] under this combination of appearance and viewpoint change.\n\nb) Simulated Viewpoint Changes: For this experiment we use 2289 images of the spring and winter season of the Nordland train dataset (see [31] for an elaborate discussion of the dataset) and crop them to half of their original width. We simulate viewpoint changes between two traverses by shifting  [41] and OpenFABMAP [14,7] on the Ericssons V\u00e4g sequence. the images of the second traverse to the right, so that the overlap between the images are 90%, 75% and 65%.\n\nThe results of this experiment are illustrated in Fig. 12. Our proposed method based on landmark proposals and robust ConvNet features (solid lines) again clearly outperforms the whole-image based method (dashed lines) except for the idealized case of 100% overlap. However, such a scenario is not realistic for real-world applications. Fig. 12 illustrates how the system can pick the same objects as landmark proposals across changing environmental conditions, and match them between images despite the changes in viewpoint and appearance.\n\n\nE. Runtime Considerations\n\nLike other state-of-the-art methods [26] the system in its current stage is not capable of processing images in real time. Finding landmark proposals using the Matlab implementation of Edge Boxes [45] takes around 1.8 seconds per image on a standard desktop machine. In addition, calculating a single ConvNet feature up to layer conv3 requires approximately 15 ms using Caffe on a NVIDIA Quadro K4000 GPU.\n\nIn future work we will adapt the system so that only one forward pass through the ConvNet is performed per image, instead of one individual pass for each of the 100 landmarks. This will result in a 100\u00d7 speed-up of the feature extraction.\n\n\nF. Improving the Absolute Performance by Sequence Search\n\nWhile single image matching performance can serve as a good evaluation of performance, in many practical robotic applications it is feasible to exploit the inherent temporal information available to a navigating robot. Consequently we replace the simple nearest neighbor search with a state-ofthe-art sequence search technique from [33], while keeping the preceding parts of our approach. Performance improves significantly even for short sequence search lengths of 6 images. Fig. 13 summarizes these results and compares the vanilla single-image precision recall curves from above with the results obtained by the addition of the sequence search.\n\n\nV. CONCLUSION AND DISCUSSION\n\nWe have presented a novel place recognition system that builds on state-of-the-art object detection methods and con-   volutional visual features. The system generates a sufficient number of landmark proposals that are both stable and repeatable over significant viewpoint and condition changes and hence perfectly suited for place recognition. Perhaps most importantly, the method does not require any environmentspecific training, instead utilizing a generic ConvNet pretrained on a large computer vision image dataset. The validity of using such an approach is confirmed by the technique's consistent place recognition performance over a wide range of datasets. When coupled with short (and hence practical) sequence-based matching methods, the performance improves even further.\n\nAs well as demonstrating state-of-the-art performance without environment-specific training, the results have also revealed further insights: Mid-level ConvNet features appear to be highly suitable as descriptors for landmarks of various sizes in a place recognition context; they are stable under appearance changes and can be successfully matched even when the landmark is partially occluded or changes its size and  with Sequence Search  Berlin Halensee, 100 regions, GRP 1024  with sequence search  Berlin Kurf\u00fcrstendamm, 100 regions, GRP 1024  with sequence search  Malm\u00f6 Ericssons V\u00e4g, 100 regions, GRP 1024 with sequence search Fig. 13: The proposed method can be combined with sequence search techniques such as [33,27] to boost absolute performance. Here we used the sequence search part of SMART [33] with a sequence length of 6 images.\n\naspect ratio. Several examples of this impressive performance have been provided in the paper -for example the landmarks shown in Fig. 1.\n\nThere are many promising avenues for future research: First and foremost, state-of-the-art place recognition performance is currently achieved using convolutional networks trained on generic computer vision classification datasets. We are investigating whether fine tuning the network for the specific task of place recognition will result in further performance gains for our approach, but also for methods such as [41,3]. Significant further improvements may be possible by introducing several quality measures. The repeatability of different landmark proposal methods can be quantified using static camera databases such as AMOS [32]. By quantifying the relative viewpoint changes in datasets like Mapillary (GPSlocalization alone is too coarse), we will be able to analyze the system's sensitivity to occlusion and large perspective changes. Building on the idea of landmark quality assessment, the semantic expressiveness of the ConvNet's object recognition layer (fc8/prob) can be used to learn and to discard \"bad\" landmarks (e.g. things known to be moving, such as cars, or people) or generate weights for \"good\" landmarks (e.g. buildings), perhaps using feedback from the place recognition performance. Finally, using temporal information will enable us to filter landmark proposals over short periods of time and discard unstable ones.\n\nThe current system has no explicit or implicit metricity. Following on the success in metric localization of landmarks presented in [26], we will investigate whether the camera position can be estimated relative to observed landmarks. Other geometry-within-the-image techniques like geometric verification [8,28] may also improve performance. In subsequent work we replaced the Gaussian Random Projection by a binary locality-sensitive hashing method that compresses the original 64,896 dimensional conv3 feature vectors to merely 8192 bits. First results indicate this approach regains more than 95% of place recognition performance while achieving a 250\u00d7 speed-up for the feature matching.\n\nFig. 2 :\n2Summary of the proposed place recognition approach based on ConvNet landmarks.\n\nFig. 3 :\n3Results for the Gardens Point Campus dataset.\n\nFig. 4 :\n4Two example scenes from the Gardens Point Campus dataset with extracted and matched ConvNet landmarks. Notice the lateral camera displacement of several meters.\n\nFig. 5 :\n5Examples of successfully matched scenes from the Berlin Kurf\u00fcrstendamm sequence of the Mapillary dataset. Images in a row belong to the same place but have been taken from different viewpoints, i.e. from the bike lane and from the upper deck of a tourist bus. The colored boxes illustrate some of the extracted and correctly matched landmarks.\n\nFig. 6 :\n6The images in the Berlin Halenseestra\u00dfe sequence have been recorded by a biker riding on the bike lane (left column) and a dashboard camera in the front of a car (right column).\n\nFig. 7 :\n7Our proposed method outperforms the approaches by[41] (blue solid), SMART[33] (blue dashed) and FAB-MAP[7,14] (blue dash dot) on the Berlin datasets. The precision recall curves compare the performance using different parameters of our system. from perceptual aliasing. The results and example scenes are depicted inFig. 10. Our approach again outperforms FAB-MAP in this scenario.D. Quantifying Viewpoint Robustness a) Real World Scenes:\n\nFig. 8 :Fig. 9 :\n89Example scenes from the John Ericssons V\u00e4g sequence. Despite different weather conditions (sunny vs. overcast) the ConvNet landmarks allow for successful place recognition. With 100 landmarks, our proposed approach performs better than the whole image ConvNet-based method of\n\nFig. 10 :Fig. 11 :\n1011Example scenes (top) and precision-recall plot (bottom) for the Library dataset. This dataset was collected by a mobile service robot roaming through the university library both during the day and night. Top: Example scenes from the viewpoint evaluation experiment: Daytime reference (left) and three translated nighttime query scenes (right). Bottom: Results show our proposed method is more robust than FAB-MAP[7,14].\n\nFig. 12 :\n12Synthetic viewpoint change experiments using cropped and shifted images of the Nordland spring and winter dataset. Top: Although large parts of this example scene are occluded by a train in the left image (spring season) and the right image (winter) was taken from a different simulated viewpoint, our method extracts and successfully matches the landmark regions illustrated by the colored boxes. Bottom: Precision recall plot showing the proposed region-based method (solid) outperforms the whole image based method[41] (dashed) and SeqSLAM[27] (dotted) significantly for different values of overlap between the query and database images.\n\n\nNordland Spring vs. Winter -Synthetic Viewpoint Change 100% overlap, 50 regions, GRP 1024 dimensions 100% overlap, whole image, exact cosine dist100% overlap, SeqSLAM \n90% overlap \n75% overlap \n65% overlap \n\n\nhttp://tinyurl.com/stluciadataset\nhttp://tinyurl.com/gardenspointdataset 3 http://www.mapillary.com\n\nSurf: Speeded up robust features. Herbert Bay, Tinne Tuytelaars, Luc Van Gool, Proceedings of the ninth European Conference on Computer Vision. the ninth European Conference on Computer VisionHerbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf: Speeded up robust features. In Proceedings of the ninth European Conference on Computer Vision, May 2006.\n\nRandom projection in dimensionality reduction: applications to image and text data. Ella Bingham, Heikki Mannila, Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. the seventh ACM SIGKDD international conference on Knowledge discovery and data miningElla Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 245-250, 2001.\n\nConvolutional Neural Network-based Place Recognition. Zetao Chen, Obadiah Lam, Adam Jacobson, Michael Milford, Proceedings of Australasian Conference on Robotics and Automation (ACRA). Australasian Conference on Robotics and Automation (ACRA)Zetao Chen, Obadiah Lam, Adam Jacobson, and Michael Milford. Convolutional Neural Network-based Place Recognition. In Proceedings of Australasian Conference on Robotics and Automation (ACRA), 2014.\n\nBING: Binarized normed gradients for objectness estimation at 300fps. Ming-Ming Cheng, Ziming Zhang, Wen-Yan Lin, Philip H S Torr, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Ming-Ming Cheng, Ziming Zhang, Wen-Yan Lin, and Philip H. S. Torr. BING: Binarized normed gradients for objectness estimation at 300fps. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.\n\nPractice makes perfect? Managing and leveraging visual experiences for lifelong navigation. Winston Churchill, Paul M Newman, 10.1109/ICRA.2012.6224596Proceedings of International Conference on Robotics and Automation (ICRA). International Conference on Robotics and Automation (ICRA)Winston Churchill and Paul M. Newman. Practice makes perfect? Managing and leveraging visual experiences for lifelong navigation. In Proceedings of International Conference on Robotics and Automation (ICRA), 2012.\n\nDealing with shadows: Capturing intrinsic scene appearance for image-based outdoor localisation. Peter Corke, Rohan Paul, Winston Churchill, Paul Newman, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Peter Corke, Rohan Paul, Winston Churchill, and Paul Newman. Dealing with shadows: Capturing intrinsic scene appearance for image-based outdoor localisation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2085-2092, Nov 2013.\n\nFAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance. Mark Cummins, Paul Newman, The International Journal of Robotics Research. 276Mark Cummins and Paul Newman. FAB-MAP: Prob- abilistic Localization and Mapping in the Space of Appearance. The International Journal of Robotics Research, 27(6):647-665, 2008.\n\nAppearance-only slam at large scale with fab-map 2.0. Mark Cummins, Paul Newman, The International Journal of Robotics Research. 309Mark Cummins and Paul Newman. Appearance-only slam at large scale with fab-map 2.0. The International Journal of Robotics Research, 30(9):1100-1123, 2011.\n\nExperiments with random projection. Sanjoy Dasgupta, Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence. the Sixteenth conference on Uncertainty in artificial intelligenceSanjoy Dasgupta. Experiments with random projection. In Proceedings of the Sixteenth conference on Uncer- tainty in artificial intelligence, pages 143-151, 2000.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, Trevor Darrell, arXiv:1310.1531arXiv preprintJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoff- man, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531, 2013.\n\nIndoor scene recognition through object detection. Pablo Espinace, Thomas Kollar, Alvaro Soto, Nicholas Roy, IEEE International Conference on Robotics and Automation (ICRA). IEEEPablo Espinace, Thomas Kollar, Alvaro Soto, and Nicholas Roy. Indoor scene recognition through object detection. In IEEE International Conference on Robotics and Automation (ICRA), pages 1406-1413. IEEE, 2010.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.\n\nAppearance-Based SLAM for Multiple Times of Day. Arren Glover, William Maddern, Michael Milford, Gordon Wyeth, Fab-Map + Ratslam, Proceedings of IEEE International Conference on Robotics and Automation (ICRA). IEEE International Conference on Robotics and Automation (ICRA)Anchorage, AlaskaArren Glover, William Maddern, Michael Milford, and Gordon Wyeth. FAB-MAP + RatSLAM : Appearance- Based SLAM for Multiple Times of Day. In Proceedings of IEEE International Conference on Robotics and Au- tomation (ICRA), Anchorage, Alaska, May 2010.\n\nOpenfabmap: An open source toolbox for appearancebased loop closure detection. Arren Glover, William Maddern, Michael Warren, Stephanie Reid, Michael Milford, Gordon Wyeth, Robotics and Automation (ICRA), 2012 IEEE International Conference on. IEEEArren Glover, William Maddern, Michael Warren, Stephanie Reid, Michael Milford, and Gordon Wyeth. Openfabmap: An open source toolbox for appearance- based loop closure detection. In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pages 4730-4735. IEEE, 2012.\n\nHow good are detection proposals, really?. Jan Hosang, Rodrigo Benenson, Bernt Schiele, British Machine Vision Conference, (BMVC). Jan Hosang, Rodrigo Benenson, and Bernt Schiele. How good are detection proposals, really? In British Machine Vision Conference, (BMVC), 2014.\n\nCaffe: Convolutional Architecture for Fast Feature Embedding. Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell, Proc. of ACM International Conference on Multimedia. of ACM International Conference on MultimediaYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadar- rama, and Trevor Darrell. Caffe: Convolutional Archi- tecture for Fast Feature Embedding. In Proc. of ACM International Conference on Multimedia., 2014.\n\nFeature co-occurrence maps: Appearance-based localisation throughout the day. Edward Johns, Guang-Zhong Yang, Proceedings of International Conference on Robotics and Automation (ICRA). International Conference on Robotics and Automation (ICRA)Edward Johns and Guang-Zhong Yang. Fea- ture co-occurrence maps: Appearance-based localisation throughout the day. In Proceedings of International Conference on Robotics and Automation (ICRA), 2013.\n\nExtensions of lipschitz mappings into a hilbert space. B William, Joram Johnson, Lindenstrauss, Contemporary mathematics. 261William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Contemporary mathematics, 26(189-206):1, 1984.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems 25. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25. 2012.\n\nDistinctive Image Features from Scale-Invariant Keypoints. David G Lowe, In International Journal of Computer Vision. 602David G. Lowe. Distinctive Image Features from Scale- Invariant Keypoints. In International Journal of Com- puter Vision, 60, 2, 2004.\n\nTransforming morning to afternoon using linear regression techniques. Stephanie Lowry, Michael Milford, Gordon Wyeth, Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEEStephanie Lowry, Michael Milford, and Gordon Wyeth. Transforming morning to afternoon using linear regres- sion techniques. In Robotics and Automation (ICRA), 2014 IEEE International Conference on, pages 3950- 3955. IEEE, 2014.\n\nTowards training-free appearance-based localization: probabilistic models for whole-image descriptors. Stephanie Lowry, Gordon Wyeth, Michael Milford, Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEEStephanie Lowry, Gordon Wyeth, and Michael Mil- ford. Towards training-free appearance-based localiza- tion: probabilistic models for whole-image descriptors. In Robotics and Automation (ICRA), 2014 IEEE Inter- national Conference on, pages 711-717. IEEE, 2014.\n\nContinuous Appearance-based Trajectory SLAM. Will Maddern, Michael Milford, Gordon Wyeth, International Conference on Robotics and Automation (ICRA). Will Maddern, Michael Milford, and Gordon Wyeth. Continuous Appearance-based Trajectory SLAM. In International Conference on Robotics and Automation (ICRA), 2011.\n\nIllumination invariant imaging: Applications in robust vision-based localisation, mapping and classification for autonomous vehicles. Will Maddern, Alex Stewart, Colin Mcmanus, Ben Upcroft, Winston Churchill, Paul Newman, Proceedings of the Visual Place Recognition in Changing Environments Workshop, IEEE International Conference on Robotics and Automation (ICRA). the Visual Place Recognition in Changing Environments Workshop, IEEE International Conference on Robotics and Automation (ICRA)Hong Kong, ChinaWill Maddern, Alex Stewart, Colin McManus, Ben Up- croft, Winston Churchill, and Paul Newman. Illumination invariant imaging: Applications in robust vision-based localisation, mapping and classification for autonomous vehicles. In Proceedings of the Visual Place Recognition in Changing Environments Workshop, IEEE International Conference on Robotics and Automation (ICRA), Hong Kong, China, May 2014.\n\nShady dealings: Robust, long-term visual localisation using illumination invariance. Colin Mcmanus, Winston Churchill, Will Maddern, Alex Stewart, Paul Newman, Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). the IEEE International Conference on Robotics and Automation (ICRA)Hong Kong, ChinaColin McManus, Winston Churchill, Will Maddern, Alex Stewart, and Paul Newman. Shady dealings: Robust, long-term visual localisation using illumination invari- ance. In Proceedings of the IEEE International Confer- ence on Robotics and Automation (ICRA), Hong Kong, China, May 2014.\n\nScene signatures: Localised and point-less features for localisation. Colin Mcmanus, Ben Upcroft, Paul Newman, Proceedings of Robotics Science and Systems (RSS). Robotics Science and Systems (RSS)Berkeley, CA, USAColin McManus, Ben Upcroft, and Paul Newman. Scene signatures: Localised and point-less features for locali- sation. In Proceedings of Robotics Science and Systems (RSS), Berkeley, CA, USA, July 2014.\n\nSeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights. Michael Milford, Gordon F Wyeth, 10.1109/ICRA.2012.6224623Proc. of Intl. Conf. on Robotics and Automation (ICRA). of Intl. Conf. on Robotics and Automation (ICRA)Michael Milford and Gordon F. Wyeth. SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights. In Proc. of Intl. Conf. on Robotics and Automation (ICRA), 2012.\n\nCondition-invariant, top-down visual place recognition. Michael Milford, Walter Scheirer, Eleonora Vig, Arren Glover, Oliver Baumann, Jason Mattingley, David Cox, Proc. of IEEE Intl. Conf. on Robotics and Automation (ICRA). of IEEE Intl. Conf. on Robotics and Automation (ICRA)Michael Milford, Walter Scheirer, Eleonora Vig, Arren Glover, Oliver Baumann, Jason Mattingley, and David Cox. Condition-invariant, top-down visual place recog- nition. In In Proc. of IEEE Intl. Conf. on Robotics and Automation (ICRA), 2014.\n\nRobust visual robot localization across seasons using network flows. Tayyab Naseer, Luciano Spinello, Wolfram Burgard, Cyrill Stachniss, Tayyab Naseer, Luciano Spinello, Wolfram Burgard, and Cyrill Stachniss. Robust visual robot localization across seasons using network flows. 2014.\n\nAppearance Change Prediction for Long-Term Navigation Across Seasons. Peer Neubert, Niko S\u00fcnderhauf, Peter Protzel, Proceedings of European Conference on Mobile Robotics (ECMR). European Conference on Mobile Robotics (ECMR)Peer Neubert, Niko S\u00fcnderhauf, and Peter Protzel. Ap- pearance Change Prediction for Long-Term Navigation Across Seasons. In Proceedings of European Conference on Mobile Robotics (ECMR), 2013.\n\nSuperpixel-based appearance change prediction for longterm navigation across seasons. Robotics and Autonomous Systems. Peer Neubert, Niko S\u00fcnderhauf, Peter Protzel, Peer Neubert, Niko S\u00fcnderhauf, and Peter Protzel. Superpixel-based appearance change prediction for long- term navigation across seasons. Robotics and Au- tonomous Systems, 2014.\n\nDemocratizing the visualization of 500 million webcam images. O&apos; Joseph, Abby Sullivan, Robert Stylianou, Pless, Applied Imagery Pattern Recognition Workshop (AIPR). Joseph O'Sullivan, Abby Stylianou, and Robert Pless. Democratizing the visualization of 500 million webcam images. In Applied Imagery Pattern Recognition Work- shop (AIPR), 2014.\n\nAll-environment visual place recognition with SMART. Edward Pepperell, Michael J Peter I Corke, Milford, IEEE International Conference on Robotics and Automation (ICRA). IEEEEdward Pepperell, Peter I Corke, and Michael J Milford. All-environment visual place recognition with SMART. In IEEE International Conference on Robotics and Au- tomation (ICRA), pages 1612-1618. IEEE, 2014.\n\nPliss: labeling places using online changepoint detection. Ananth Ranganathan, Autonomous Robots. 324Ananth Ranganathan. Pliss: labeling places using online changepoint detection. Autonomous Robots, 32(4):351- 368, 2012.\n\nCNN Features off-the-shelf: an Astounding Baseline for Recognition. Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, Stefan Carlsson, Computer Vision and Pattern Recognition Workshops (CVPRW). Ali Sharif Razavian, Hossein Azizpour, Josephine Sulli- van, and Stefan Carlsson. CNN Features off-the-shelf: an Astounding Baseline for Recognition. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2014.\n\n. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, arXiv:1409.0575arXiv preprintImagenet large scale visual recognition challengeOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan- der C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. arXiv preprint arXiv:1409.0575, 2014.\n\nOverfeat: Integrated recognition, localization and detection using convolutional networks. Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann Lecun, arXiv:1312.6229arXiv preprintPierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229, 2013.\n\nSliding Window Filter with Application to Planetary Landing. Gabe Sibley, Larry Matthies, Gaurav Sukhatme, 10.1002/rob.20360J. Field Robotics. 275Gabe Sibley, Larry Matthies, and Gaurav Sukhatme. Sliding Window Filter with Application to Planetary Landing. J. Field Robotics, 27(5):587-608, 2010. doi: 10.1002/rob.20360.\n\nParticle filters for robot navigation. Cyrill Stachniss, Wolfram Burgard, Foundations and Trends in Robotics. 34Cyrill Stachniss and Wolfram Burgard. Particle filters for robot navigation. Foundations and Trends in Robotics, 3 (4):211-282, 2014.\n\nAre We There Yet? Challenging SeqSLAM on a 3000 km Journey Across All Four Seasons. Niko S\u00fcnderhauf, Peer Neubert, Peter Protzel, Proceedings of Workshop on Long-Term Autonomy, IEEE International Conference on Robotics and Automation (ICRA). Workshop on Long-Term Autonomy, IEEE International Conference on Robotics and Automation (ICRA)Niko S\u00fcnderhauf, Peer Neubert, and Peter Protzel. Are We There Yet? Challenging SeqSLAM on a 3000 km Journey Across All Four Seasons. In Proceedings of Workshop on Long-Term Autonomy, IEEE International Conference on Robotics and Automation (ICRA), 2013.\n\nOn the Performance of ConvNet Features for Place Recognition. Niko S\u00fcnderhauf, Feras Dayoub, Sareh Shirazi, Ben Upcroft, Michael Milford, arXiv:1501.04158Niko S\u00fcnderhauf, Feras Dayoub, Sareh Shirazi, Ben Upcroft, and Michael Milford. On the Performance of ConvNet Features for Place Recognition. In preprint arXiv:1501.04158, 2015.\n\nSelective search for object recognition. J R R Uijlings, K E A Van De Sande, T Gevers, A W M Smeulders, International Journal of Computer Vision. J.R.R. Uijlings, K.E.A. van de Sande, T. Gevers, and A.W.M. Smeulders. Selective search for object recogni- tion. International Journal of Computer Vision, 2013.\n\nLighting invariant urban street classification. Ben Upcroft, Colin Mcmanus, Winston Churchill, Will Maddern, Paul Newman, Robotics and Automation (ICRA), 2014 IEEE International Conference on. Ben Upcroft, Colin McManus, Winston Churchill, Will Maddern, and Paul Newman. Lighting invariant urban street classification. In Robotics and Automation (ICRA), 2014 IEEE International Conference on, pages 1712- 1718, May 2014.\n\nSift, surf and seasons: Long-term outdoor localization using local features. Christoffer Valgren, European Conference on Mobile Robots (ECMR). Christoffer Valgren and Achim J Lilienthal. Sift, surf and seasons: Long-term outdoor localization using local features. In European Conference on Mobile Robots (ECMR), 2007.\n\nEdge boxes: Locating object proposals from edges. C , Lawrence Zitnick, Piotr Doll\u00e1r, ECCV. European Conference on Computer Vision. C. Lawrence Zitnick and Piotr Doll\u00e1r. Edge boxes: Lo- cating object proposals from edges. In ECCV. European Conference on Computer Vision, September 2014.\n", "annotations": {"author": "[{\"end\":110,\"start\":4}]", "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": "[{\"end\":109,\"start\":5}]", "title": null, "venue": null, "abstract": null, "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1176,\"start\":1172},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1179,\"start\":1176},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1182,\"start\":1179},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1185,\"start\":1182},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1223,\"start\":1220},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":1226,\"start\":1223},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1229,\"start\":1226},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1232,\"start\":1229},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1269,\"start\":1265},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1349,\"start\":1345},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1528,\"start\":1524},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1530,\"start\":1528},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1709,\"start\":1705},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1712,\"start\":1709},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1771,\"start\":1767},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1820,\"start\":1817},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3989,\"start\":3985},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3992,\"start\":3989},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3995,\"start\":3992},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3998,\"start\":3995},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4001,\"start\":3998},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4091,\"start\":4088},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4094,\"start\":4091},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4097,\"start\":4094},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4100,\"start\":4097},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4103,\"start\":4100},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4196,\"start\":4192},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4199,\"start\":4196},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4202,\"start\":4199},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4293,\"start\":4289},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4296,\"start\":4293},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4299,\"start\":4296},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4392,\"start\":4389},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4521,\"start\":4517},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4601,\"start\":4597},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4604,\"start\":4601},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4729,\"start\":4725},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5204,\"start\":5201},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5206,\"start\":5204},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5209,\"start\":5206},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5484,\"start\":5480},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5494,\"start\":5491},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5853,\"start\":5849},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5856,\"start\":5853},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5878,\"start\":5875},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5993,\"start\":5989},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5996,\"start\":5993},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5999,\"start\":5996},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6018,\"start\":6014},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6021,\"start\":6018},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6140,\"start\":6136},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7131,\"start\":7127},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7298,\"start\":7294},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7301,\"start\":7298},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7304,\"start\":7301},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7307,\"start\":7304},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7310,\"start\":7307},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7464,\"start\":7460},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7479,\"start\":7475},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7507,\"start\":7503},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7604,\"start\":7600},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7750,\"start\":7746},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8293,\"start\":8289},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8361,\"start\":8357},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8882,\"start\":8878},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9039,\"start\":9035},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9500,\"start\":9496},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10719,\"start\":10715},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11084,\"start\":11080},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11164,\"start\":11161},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11189,\"start\":11185},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11309,\"start\":11305},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12150,\"start\":12147},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12557,\"start\":12553},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12560,\"start\":12557},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12563,\"start\":12560},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12566,\"start\":12563},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12569,\"start\":12566},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12734,\"start\":12731},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12737,\"start\":12734},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12787,\"start\":12783},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12799,\"start\":12795},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12828,\"start\":12824},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12905,\"start\":12901},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13181,\"start\":13177},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13644,\"start\":13640},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13673,\"start\":13669},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14017,\"start\":14013},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14586,\"start\":14582},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15035,\"start\":15032},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15037,\"start\":15035},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17664,\"start\":17660},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17701,\"start\":17698},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17724,\"start\":17720},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17772,\"start\":17768},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":17815,\"start\":17811},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17831,\"start\":17827},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18544,\"start\":18540},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18564,\"start\":18560},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18675,\"start\":18671},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18842,\"start\":18839},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18845,\"start\":18842},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18893,\"start\":18889},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20522,\"start\":20518},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20589,\"start\":20586},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20635,\"start\":20631},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20651,\"start\":20647},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21988,\"start\":21984},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22317,\"start\":22313},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22400,\"start\":22397},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22403,\"start\":22400},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23747,\"start\":23744},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23750,\"start\":23747},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23953,\"start\":23949},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24114,\"start\":24110},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24134,\"start\":24130},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":24136,\"start\":24134},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24888,\"start\":24884},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25048,\"start\":25044},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25890,\"start\":25886},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27742,\"start\":27738},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27745,\"start\":27742},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27828,\"start\":27824},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28425,\"start\":28421},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28427,\"start\":28425},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28641,\"start\":28637},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29488,\"start\":29484},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29661,\"start\":29658},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29664,\"start\":29661},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30971,\"start\":30967},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30995,\"start\":30991},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31024,\"start\":31021},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31027,\"start\":31024},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32092,\"start\":32089},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32095,\"start\":32092},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32631,\"start\":32627},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32656,\"start\":32652}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30133,\"start\":30044},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30190,\"start\":30134},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30362,\"start\":30191},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30717,\"start\":30363},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30906,\"start\":30718},{\"attributes\":{\"id\":\"fig_5\"},\"end\":31356,\"start\":30907},{\"attributes\":{\"id\":\"fig_6\"},\"end\":31652,\"start\":31357},{\"attributes\":{\"id\":\"fig_7\"},\"end\":32096,\"start\":31653},{\"attributes\":{\"id\":\"fig_9\"},\"end\":32750,\"start\":32097},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32961,\"start\":32751}]", "paragraph": "[{\"end\":2047,\"start\":665},{\"end\":3096,\"start\":2049},{\"end\":3661,\"start\":3098},{\"end\":3874,\"start\":3682},{\"end\":4393,\"start\":3876},{\"end\":5112,\"start\":4395},{\"end\":5447,\"start\":5114},{\"end\":6614,\"start\":5449},{\"end\":7132,\"start\":6616},{\"end\":7605,\"start\":7134},{\"end\":8142,\"start\":7607},{\"end\":8362,\"start\":8144},{\"end\":10336,\"start\":8387},{\"end\":10978,\"start\":10390},{\"end\":11381,\"start\":10980},{\"end\":11699,\"start\":11383},{\"end\":12364,\"start\":11754},{\"end\":14063,\"start\":12366},{\"end\":14455,\"start\":14118},{\"end\":14778,\"start\":14457},{\"end\":15324,\"start\":14827},{\"end\":15567,\"start\":15326},{\"end\":16007,\"start\":15589},{\"end\":16280,\"start\":16009},{\"end\":16435,\"start\":16359},{\"end\":17124,\"start\":16483},{\"end\":17350,\"start\":17126},{\"end\":18111,\"start\":17381},{\"end\":19007,\"start\":18162},{\"end\":20411,\"start\":19093},{\"end\":20738,\"start\":20413},{\"end\":21468,\"start\":20740},{\"end\":22141,\"start\":21470},{\"end\":22430,\"start\":22143},{\"end\":23809,\"start\":22470},{\"end\":24276,\"start\":23811},{\"end\":24818,\"start\":24278},{\"end\":25253,\"start\":24848},{\"end\":25493,\"start\":25255},{\"end\":26201,\"start\":25554},{\"end\":27016,\"start\":26234},{\"end\":27864,\"start\":27018},{\"end\":28003,\"start\":27866},{\"end\":29350,\"start\":28005},{\"end\":30043,\"start\":29352}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14826,\"start\":14779},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16358,\"start\":16281},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16482,\"start\":16436}]", "table_ref": "[{\"end\":27631,\"start\":27437}]", "section_header": "[{\"end\":663,\"start\":648},{\"end\":3680,\"start\":3664},{\"end\":8385,\"start\":8365},{\"end\":10388,\"start\":10339},{\"end\":11752,\"start\":11702},{\"end\":14116,\"start\":14066},{\"end\":15587,\"start\":15570},{\"end\":17379,\"start\":17353},{\"end\":18160,\"start\":18114},{\"end\":19091,\"start\":19010},{\"end\":22468,\"start\":22433},{\"end\":24846,\"start\":24821},{\"end\":25552,\"start\":25496},{\"end\":26232,\"start\":26204},{\"end\":30053,\"start\":30045},{\"end\":30143,\"start\":30135},{\"end\":30200,\"start\":30192},{\"end\":30372,\"start\":30364},{\"end\":30727,\"start\":30719},{\"end\":30916,\"start\":30908},{\"end\":31374,\"start\":31358},{\"end\":31672,\"start\":31654},{\"end\":32107,\"start\":32098}]", "table": "[{\"end\":32961,\"start\":32898}]", "figure_caption": "[{\"end\":30133,\"start\":30055},{\"end\":30190,\"start\":30145},{\"end\":30362,\"start\":30202},{\"end\":30717,\"start\":30374},{\"end\":30906,\"start\":30729},{\"end\":31356,\"start\":30918},{\"end\":31652,\"start\":31377},{\"end\":32096,\"start\":31677},{\"end\":32750,\"start\":32110},{\"end\":32898,\"start\":32753}]", "figure_ref": "[{\"end\":5780,\"start\":5774},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8756,\"start\":8750},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18711,\"start\":18705},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18911,\"start\":18905},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20144,\"start\":20127},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20468,\"start\":20462},{\"end\":21822,\"start\":21816},{\"end\":21885,\"start\":21879},{\"end\":23644,\"start\":23637},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24335,\"start\":24328},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24622,\"start\":24615},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26037,\"start\":26030},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27660,\"start\":27653},{\"end\":28002,\"start\":27996}]", "bib_author_first_name": "[{\"end\":33104,\"start\":33097},{\"end\":33115,\"start\":33110},{\"end\":33131,\"start\":33128},{\"end\":33505,\"start\":33501},{\"end\":33521,\"start\":33515},{\"end\":34024,\"start\":34019},{\"end\":34038,\"start\":34031},{\"end\":34048,\"start\":34044},{\"end\":34066,\"start\":34059},{\"end\":34485,\"start\":34476},{\"end\":34499,\"start\":34493},{\"end\":34514,\"start\":34507},{\"end\":34526,\"start\":34520},{\"end\":34530,\"start\":34527},{\"end\":34917,\"start\":34910},{\"end\":34933,\"start\":34929},{\"end\":34935,\"start\":34934},{\"end\":35419,\"start\":35414},{\"end\":35432,\"start\":35427},{\"end\":35446,\"start\":35439},{\"end\":35462,\"start\":35458},{\"end\":35892,\"start\":35888},{\"end\":35906,\"start\":35902},{\"end\":36202,\"start\":36198},{\"end\":36216,\"start\":36212},{\"end\":36474,\"start\":36468},{\"end\":36880,\"start\":36876},{\"end\":36898,\"start\":36890},{\"end\":36909,\"start\":36904},{\"end\":36923,\"start\":36919},{\"end\":36937,\"start\":36933},{\"end\":36949,\"start\":36945},{\"end\":36963,\"start\":36957},{\"end\":37279,\"start\":37274},{\"end\":37296,\"start\":37290},{\"end\":37311,\"start\":37305},{\"end\":37326,\"start\":37318},{\"end\":37698,\"start\":37694},{\"end\":37713,\"start\":37709},{\"end\":37729,\"start\":37723},{\"end\":37747,\"start\":37739},{\"end\":38207,\"start\":38202},{\"end\":38223,\"start\":38216},{\"end\":38240,\"start\":38233},{\"end\":38256,\"start\":38250},{\"end\":38778,\"start\":38773},{\"end\":38794,\"start\":38787},{\"end\":38811,\"start\":38804},{\"end\":38829,\"start\":38820},{\"end\":38843,\"start\":38836},{\"end\":38859,\"start\":38853},{\"end\":39271,\"start\":39268},{\"end\":39287,\"start\":39280},{\"end\":39303,\"start\":39298},{\"end\":39570,\"start\":39562},{\"end\":39580,\"start\":39576},{\"end\":39596,\"start\":39592},{\"end\":39612,\"start\":39606},{\"end\":39630,\"start\":39622},{\"end\":39641,\"start\":39637},{\"end\":39658,\"start\":39652},{\"end\":39677,\"start\":39671},{\"end\":40128,\"start\":40122},{\"end\":40147,\"start\":40136},{\"end\":40543,\"start\":40542},{\"end\":40558,\"start\":40553},{\"end\":40827,\"start\":40823},{\"end\":40844,\"start\":40840},{\"end\":40864,\"start\":40856},{\"end\":40866,\"start\":40865},{\"end\":41179,\"start\":41174},{\"end\":41181,\"start\":41180},{\"end\":41451,\"start\":41442},{\"end\":41466,\"start\":41459},{\"end\":41482,\"start\":41476},{\"end\":41906,\"start\":41897},{\"end\":41920,\"start\":41914},{\"end\":41935,\"start\":41928},{\"end\":42332,\"start\":42328},{\"end\":42349,\"start\":42342},{\"end\":42365,\"start\":42359},{\"end\":42735,\"start\":42731},{\"end\":42749,\"start\":42745},{\"end\":42764,\"start\":42759},{\"end\":42777,\"start\":42774},{\"end\":42794,\"start\":42787},{\"end\":42810,\"start\":42806},{\"end\":43600,\"start\":43595},{\"end\":43617,\"start\":43610},{\"end\":43633,\"start\":43629},{\"end\":43647,\"start\":43643},{\"end\":43661,\"start\":43657},{\"end\":44196,\"start\":44191},{\"end\":44209,\"start\":44206},{\"end\":44223,\"start\":44219},{\"end\":44630,\"start\":44623},{\"end\":44646,\"start\":44640},{\"end\":44648,\"start\":44647},{\"end\":45038,\"start\":45031},{\"end\":45054,\"start\":45048},{\"end\":45073,\"start\":45065},{\"end\":45084,\"start\":45079},{\"end\":45099,\"start\":45093},{\"end\":45114,\"start\":45109},{\"end\":45132,\"start\":45127},{\"end\":45570,\"start\":45564},{\"end\":45586,\"start\":45579},{\"end\":45604,\"start\":45597},{\"end\":45620,\"start\":45614},{\"end\":45854,\"start\":45850},{\"end\":45868,\"start\":45864},{\"end\":45886,\"start\":45881},{\"end\":46320,\"start\":46316},{\"end\":46334,\"start\":46330},{\"end\":46352,\"start\":46347},{\"end\":46611,\"start\":46604},{\"end\":46624,\"start\":46620},{\"end\":46641,\"start\":46635},{\"end\":46952,\"start\":46946},{\"end\":46973,\"start\":46964},{\"end\":47341,\"start\":47335},{\"end\":47569,\"start\":47566},{\"end\":47594,\"start\":47587},{\"end\":47614,\"start\":47605},{\"end\":47631,\"start\":47625},{\"end\":47926,\"start\":47922},{\"end\":47943,\"start\":47940},{\"end\":47953,\"start\":47950},{\"end\":47966,\"start\":47958},{\"end\":47982,\"start\":47975},{\"end\":47997,\"start\":47993},{\"end\":48009,\"start\":48002},{\"end\":48023,\"start\":48017},{\"end\":48040,\"start\":48034},{\"end\":48056,\"start\":48049},{\"end\":48077,\"start\":48068},{\"end\":48079,\"start\":48078},{\"end\":48088,\"start\":48086},{\"end\":48546,\"start\":48540},{\"end\":48562,\"start\":48557},{\"end\":48575,\"start\":48570},{\"end\":48590,\"start\":48583},{\"end\":48603,\"start\":48600},{\"end\":48616,\"start\":48612},{\"end\":48936,\"start\":48932},{\"end\":48950,\"start\":48945},{\"end\":48967,\"start\":48961},{\"end\":49238,\"start\":49232},{\"end\":49257,\"start\":49250},{\"end\":49528,\"start\":49524},{\"end\":49545,\"start\":49541},{\"end\":49560,\"start\":49555},{\"end\":50099,\"start\":50095},{\"end\":50117,\"start\":50112},{\"end\":50131,\"start\":50126},{\"end\":50144,\"start\":50141},{\"end\":50161,\"start\":50154},{\"end\":50408,\"start\":50407},{\"end\":50412,\"start\":50409},{\"end\":50424,\"start\":50423},{\"end\":50428,\"start\":50425},{\"end\":50444,\"start\":50443},{\"end\":50454,\"start\":50453},{\"end\":50458,\"start\":50455},{\"end\":50726,\"start\":50723},{\"end\":50741,\"start\":50736},{\"end\":50758,\"start\":50751},{\"end\":50774,\"start\":50770},{\"end\":50788,\"start\":50784},{\"end\":51185,\"start\":51174},{\"end\":51467,\"start\":51466},{\"end\":51478,\"start\":51470},{\"end\":51493,\"start\":51488}]", "bib_author_last_name": "[{\"end\":33108,\"start\":33105},{\"end\":33126,\"start\":33116},{\"end\":33140,\"start\":33132},{\"end\":33513,\"start\":33506},{\"end\":33529,\"start\":33522},{\"end\":34029,\"start\":34025},{\"end\":34042,\"start\":34039},{\"end\":34057,\"start\":34049},{\"end\":34074,\"start\":34067},{\"end\":34491,\"start\":34486},{\"end\":34505,\"start\":34500},{\"end\":34518,\"start\":34515},{\"end\":34535,\"start\":34531},{\"end\":34927,\"start\":34918},{\"end\":34942,\"start\":34936},{\"end\":35425,\"start\":35420},{\"end\":35437,\"start\":35433},{\"end\":35456,\"start\":35447},{\"end\":35469,\"start\":35463},{\"end\":35900,\"start\":35893},{\"end\":35913,\"start\":35907},{\"end\":36210,\"start\":36203},{\"end\":36223,\"start\":36217},{\"end\":36483,\"start\":36475},{\"end\":36888,\"start\":36881},{\"end\":36902,\"start\":36899},{\"end\":36917,\"start\":36910},{\"end\":36931,\"start\":36924},{\"end\":36943,\"start\":36938},{\"end\":36955,\"start\":36950},{\"end\":36971,\"start\":36964},{\"end\":37288,\"start\":37280},{\"end\":37303,\"start\":37297},{\"end\":37316,\"start\":37312},{\"end\":37330,\"start\":37327},{\"end\":37707,\"start\":37699},{\"end\":37721,\"start\":37714},{\"end\":37737,\"start\":37730},{\"end\":37753,\"start\":37748},{\"end\":38214,\"start\":38208},{\"end\":38231,\"start\":38224},{\"end\":38248,\"start\":38241},{\"end\":38262,\"start\":38257},{\"end\":38281,\"start\":38264},{\"end\":38785,\"start\":38779},{\"end\":38802,\"start\":38795},{\"end\":38818,\"start\":38812},{\"end\":38834,\"start\":38830},{\"end\":38851,\"start\":38844},{\"end\":38865,\"start\":38860},{\"end\":39278,\"start\":39272},{\"end\":39296,\"start\":39288},{\"end\":39311,\"start\":39304},{\"end\":39574,\"start\":39571},{\"end\":39590,\"start\":39581},{\"end\":39604,\"start\":39597},{\"end\":39620,\"start\":39613},{\"end\":39635,\"start\":39631},{\"end\":39650,\"start\":39642},{\"end\":39669,\"start\":39659},{\"end\":39685,\"start\":39678},{\"end\":40134,\"start\":40129},{\"end\":40152,\"start\":40148},{\"end\":40551,\"start\":40544},{\"end\":40566,\"start\":40559},{\"end\":40581,\"start\":40568},{\"end\":40838,\"start\":40828},{\"end\":40854,\"start\":40845},{\"end\":40873,\"start\":40867},{\"end\":41186,\"start\":41182},{\"end\":41457,\"start\":41452},{\"end\":41474,\"start\":41467},{\"end\":41488,\"start\":41483},{\"end\":41912,\"start\":41907},{\"end\":41926,\"start\":41921},{\"end\":41943,\"start\":41936},{\"end\":42340,\"start\":42333},{\"end\":42357,\"start\":42350},{\"end\":42371,\"start\":42366},{\"end\":42743,\"start\":42736},{\"end\":42757,\"start\":42750},{\"end\":42772,\"start\":42765},{\"end\":42785,\"start\":42778},{\"end\":42804,\"start\":42795},{\"end\":42817,\"start\":42811},{\"end\":43608,\"start\":43601},{\"end\":43627,\"start\":43618},{\"end\":43641,\"start\":43634},{\"end\":43655,\"start\":43648},{\"end\":43668,\"start\":43662},{\"end\":44204,\"start\":44197},{\"end\":44217,\"start\":44210},{\"end\":44230,\"start\":44224},{\"end\":44638,\"start\":44631},{\"end\":44654,\"start\":44649},{\"end\":45046,\"start\":45039},{\"end\":45063,\"start\":45055},{\"end\":45077,\"start\":45074},{\"end\":45091,\"start\":45085},{\"end\":45107,\"start\":45100},{\"end\":45125,\"start\":45115},{\"end\":45136,\"start\":45133},{\"end\":45577,\"start\":45571},{\"end\":45595,\"start\":45587},{\"end\":45612,\"start\":45605},{\"end\":45630,\"start\":45621},{\"end\":45862,\"start\":45855},{\"end\":45879,\"start\":45869},{\"end\":45894,\"start\":45887},{\"end\":46328,\"start\":46321},{\"end\":46345,\"start\":46335},{\"end\":46360,\"start\":46353},{\"end\":46618,\"start\":46612},{\"end\":46633,\"start\":46625},{\"end\":46651,\"start\":46642},{\"end\":46658,\"start\":46653},{\"end\":46962,\"start\":46953},{\"end\":46987,\"start\":46974},{\"end\":46996,\"start\":46989},{\"end\":47353,\"start\":47342},{\"end\":47585,\"start\":47570},{\"end\":47603,\"start\":47595},{\"end\":47623,\"start\":47615},{\"end\":47640,\"start\":47632},{\"end\":47938,\"start\":47927},{\"end\":47948,\"start\":47944},{\"end\":47956,\"start\":47954},{\"end\":47973,\"start\":47967},{\"end\":47991,\"start\":47983},{\"end\":48000,\"start\":47998},{\"end\":48015,\"start\":48010},{\"end\":48032,\"start\":48024},{\"end\":48047,\"start\":48041},{\"end\":48066,\"start\":48057},{\"end\":48084,\"start\":48080},{\"end\":48096,\"start\":48089},{\"end\":48555,\"start\":48547},{\"end\":48568,\"start\":48563},{\"end\":48581,\"start\":48576},{\"end\":48598,\"start\":48591},{\"end\":48610,\"start\":48604},{\"end\":48622,\"start\":48617},{\"end\":48943,\"start\":48937},{\"end\":48959,\"start\":48951},{\"end\":48976,\"start\":48968},{\"end\":49248,\"start\":49239},{\"end\":49265,\"start\":49258},{\"end\":49539,\"start\":49529},{\"end\":49553,\"start\":49546},{\"end\":49568,\"start\":49561},{\"end\":50110,\"start\":50100},{\"end\":50124,\"start\":50118},{\"end\":50139,\"start\":50132},{\"end\":50152,\"start\":50145},{\"end\":50169,\"start\":50162},{\"end\":50421,\"start\":50413},{\"end\":50441,\"start\":50429},{\"end\":50451,\"start\":50445},{\"end\":50468,\"start\":50459},{\"end\":50734,\"start\":50727},{\"end\":50749,\"start\":50742},{\"end\":50768,\"start\":50759},{\"end\":50782,\"start\":50775},{\"end\":50795,\"start\":50789},{\"end\":51193,\"start\":51186},{\"end\":51486,\"start\":51479},{\"end\":51500,\"start\":51494}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":161878},\"end\":33415,\"start\":33063},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1854295},\"end\":33963,\"start\":33417},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":18130455},\"end\":34404,\"start\":33965},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9823657},\"end\":34816,\"start\":34406},{\"attributes\":{\"doi\":\"10.1109/ICRA.2012.6224596\",\"id\":\"b4\",\"matched_paper_id\":15007579},\"end\":35315,\"start\":34818},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":12975004},\"end\":35810,\"start\":35317},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":17969052},\"end\":36142,\"start\":35812},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":18537011},\"end\":36430,\"start\":36144},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8454405},\"end\":36795,\"start\":36432},{\"attributes\":{\"doi\":\"arXiv:1310.1531\",\"id\":\"b9\"},\"end\":37221,\"start\":36797},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14189501},\"end\":37610,\"start\":37223},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":215827080},\"end\":38151,\"start\":37612},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6168903},\"end\":38692,\"start\":38153},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":5420500},\"end\":39223,\"start\":38694},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5819909},\"end\":39498,\"start\":39225},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1799558},\"end\":40042,\"start\":39500},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":18762722},\"end\":40485,\"start\":40044},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":117819162},\"end\":40756,\"start\":40487},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":195908774},\"end\":41113,\"start\":40758},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":174065},\"end\":41370,\"start\":41115},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11771889},\"end\":41792,\"start\":41372},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15475898},\"end\":42281,\"start\":41794},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18901135},\"end\":42595,\"start\":42283},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17816426},\"end\":43508,\"start\":42597},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11097039},\"end\":44119,\"start\":43510},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":12130346},\"end\":44534,\"start\":44121},{\"attributes\":{\"doi\":\"10.1109/ICRA.2012.6224623\",\"id\":\"b26\",\"matched_paper_id\":14700600},\"end\":44973,\"start\":44536},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14999834},\"end\":45493,\"start\":44975},{\"attributes\":{\"id\":\"b28\"},\"end\":45778,\"start\":45495},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2432984},\"end\":46195,\"start\":45780},{\"attributes\":{\"id\":\"b30\"},\"end\":46540,\"start\":46197},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":10967639},\"end\":46891,\"start\":46542},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":986566},\"end\":47274,\"start\":46893},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":33622219},\"end\":47496,\"start\":47276},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6383532},\"end\":47918,\"start\":47498},{\"attributes\":{\"doi\":\"arXiv:1409.0575\",\"id\":\"b35\"},\"end\":48447,\"start\":47920},{\"attributes\":{\"doi\":\"arXiv:1312.6229\",\"id\":\"b36\"},\"end\":48869,\"start\":48449},{\"attributes\":{\"doi\":\"10.1002/rob.20360\",\"id\":\"b37\",\"matched_paper_id\":12663958},\"end\":49191,\"start\":48871},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":51760623},\"end\":49438,\"start\":49193},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":15204633},\"end\":50031,\"start\":49440},{\"attributes\":{\"doi\":\"arXiv:1501.04158\",\"id\":\"b40\"},\"end\":50364,\"start\":50033},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":216077384},\"end\":50673,\"start\":50366},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":9538705},\"end\":51095,\"start\":50675},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":18900131},\"end\":51414,\"start\":51097},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":5984060},\"end\":51702,\"start\":51416}]", "bib_title": "[{\"end\":33095,\"start\":33063},{\"end\":33499,\"start\":33417},{\"end\":34017,\"start\":33965},{\"end\":34474,\"start\":34406},{\"end\":34908,\"start\":34818},{\"end\":35412,\"start\":35317},{\"end\":35886,\"start\":35812},{\"end\":36196,\"start\":36144},{\"end\":36466,\"start\":36432},{\"end\":37272,\"start\":37223},{\"end\":37692,\"start\":37612},{\"end\":38200,\"start\":38153},{\"end\":38771,\"start\":38694},{\"end\":39266,\"start\":39225},{\"end\":39560,\"start\":39500},{\"end\":40120,\"start\":40044},{\"end\":40540,\"start\":40487},{\"end\":40821,\"start\":40758},{\"end\":41172,\"start\":41115},{\"end\":41440,\"start\":41372},{\"end\":41895,\"start\":41794},{\"end\":42326,\"start\":42283},{\"end\":42729,\"start\":42597},{\"end\":43593,\"start\":43510},{\"end\":44189,\"start\":44121},{\"end\":44621,\"start\":44536},{\"end\":45029,\"start\":44975},{\"end\":45848,\"start\":45780},{\"end\":46602,\"start\":46542},{\"end\":46944,\"start\":46893},{\"end\":47333,\"start\":47276},{\"end\":47564,\"start\":47498},{\"end\":48930,\"start\":48871},{\"end\":49230,\"start\":49193},{\"end\":49522,\"start\":49440},{\"end\":50405,\"start\":50366},{\"end\":50721,\"start\":50675},{\"end\":51172,\"start\":51097},{\"end\":51464,\"start\":51416}]", "bib_author": "[{\"end\":33110,\"start\":33097},{\"end\":33128,\"start\":33110},{\"end\":33142,\"start\":33128},{\"end\":33515,\"start\":33501},{\"end\":33531,\"start\":33515},{\"end\":34031,\"start\":34019},{\"end\":34044,\"start\":34031},{\"end\":34059,\"start\":34044},{\"end\":34076,\"start\":34059},{\"end\":34493,\"start\":34476},{\"end\":34507,\"start\":34493},{\"end\":34520,\"start\":34507},{\"end\":34537,\"start\":34520},{\"end\":34929,\"start\":34910},{\"end\":34944,\"start\":34929},{\"end\":35427,\"start\":35414},{\"end\":35439,\"start\":35427},{\"end\":35458,\"start\":35439},{\"end\":35471,\"start\":35458},{\"end\":35902,\"start\":35888},{\"end\":35915,\"start\":35902},{\"end\":36212,\"start\":36198},{\"end\":36225,\"start\":36212},{\"end\":36485,\"start\":36468},{\"end\":36890,\"start\":36876},{\"end\":36904,\"start\":36890},{\"end\":36919,\"start\":36904},{\"end\":36933,\"start\":36919},{\"end\":36945,\"start\":36933},{\"end\":36957,\"start\":36945},{\"end\":36973,\"start\":36957},{\"end\":37290,\"start\":37274},{\"end\":37305,\"start\":37290},{\"end\":37318,\"start\":37305},{\"end\":37332,\"start\":37318},{\"end\":37709,\"start\":37694},{\"end\":37723,\"start\":37709},{\"end\":37739,\"start\":37723},{\"end\":37755,\"start\":37739},{\"end\":38216,\"start\":38202},{\"end\":38233,\"start\":38216},{\"end\":38250,\"start\":38233},{\"end\":38264,\"start\":38250},{\"end\":38283,\"start\":38264},{\"end\":38787,\"start\":38773},{\"end\":38804,\"start\":38787},{\"end\":38820,\"start\":38804},{\"end\":38836,\"start\":38820},{\"end\":38853,\"start\":38836},{\"end\":38867,\"start\":38853},{\"end\":39280,\"start\":39268},{\"end\":39298,\"start\":39280},{\"end\":39313,\"start\":39298},{\"end\":39576,\"start\":39562},{\"end\":39592,\"start\":39576},{\"end\":39606,\"start\":39592},{\"end\":39622,\"start\":39606},{\"end\":39637,\"start\":39622},{\"end\":39652,\"start\":39637},{\"end\":39671,\"start\":39652},{\"end\":39687,\"start\":39671},{\"end\":40136,\"start\":40122},{\"end\":40154,\"start\":40136},{\"end\":40553,\"start\":40542},{\"end\":40568,\"start\":40553},{\"end\":40583,\"start\":40568},{\"end\":40840,\"start\":40823},{\"end\":40856,\"start\":40840},{\"end\":40875,\"start\":40856},{\"end\":41188,\"start\":41174},{\"end\":41459,\"start\":41442},{\"end\":41476,\"start\":41459},{\"end\":41490,\"start\":41476},{\"end\":41914,\"start\":41897},{\"end\":41928,\"start\":41914},{\"end\":41945,\"start\":41928},{\"end\":42342,\"start\":42328},{\"end\":42359,\"start\":42342},{\"end\":42373,\"start\":42359},{\"end\":42745,\"start\":42731},{\"end\":42759,\"start\":42745},{\"end\":42774,\"start\":42759},{\"end\":42787,\"start\":42774},{\"end\":42806,\"start\":42787},{\"end\":42819,\"start\":42806},{\"end\":43610,\"start\":43595},{\"end\":43629,\"start\":43610},{\"end\":43643,\"start\":43629},{\"end\":43657,\"start\":43643},{\"end\":43670,\"start\":43657},{\"end\":44206,\"start\":44191},{\"end\":44219,\"start\":44206},{\"end\":44232,\"start\":44219},{\"end\":44640,\"start\":44623},{\"end\":44656,\"start\":44640},{\"end\":45048,\"start\":45031},{\"end\":45065,\"start\":45048},{\"end\":45079,\"start\":45065},{\"end\":45093,\"start\":45079},{\"end\":45109,\"start\":45093},{\"end\":45127,\"start\":45109},{\"end\":45138,\"start\":45127},{\"end\":45579,\"start\":45564},{\"end\":45597,\"start\":45579},{\"end\":45614,\"start\":45597},{\"end\":45632,\"start\":45614},{\"end\":45864,\"start\":45850},{\"end\":45881,\"start\":45864},{\"end\":45896,\"start\":45881},{\"end\":46330,\"start\":46316},{\"end\":46347,\"start\":46330},{\"end\":46362,\"start\":46347},{\"end\":46620,\"start\":46604},{\"end\":46635,\"start\":46620},{\"end\":46653,\"start\":46635},{\"end\":46660,\"start\":46653},{\"end\":46964,\"start\":46946},{\"end\":46989,\"start\":46964},{\"end\":46998,\"start\":46989},{\"end\":47355,\"start\":47335},{\"end\":47587,\"start\":47566},{\"end\":47605,\"start\":47587},{\"end\":47625,\"start\":47605},{\"end\":47642,\"start\":47625},{\"end\":47940,\"start\":47922},{\"end\":47950,\"start\":47940},{\"end\":47958,\"start\":47950},{\"end\":47975,\"start\":47958},{\"end\":47993,\"start\":47975},{\"end\":48002,\"start\":47993},{\"end\":48017,\"start\":48002},{\"end\":48034,\"start\":48017},{\"end\":48049,\"start\":48034},{\"end\":48068,\"start\":48049},{\"end\":48086,\"start\":48068},{\"end\":48098,\"start\":48086},{\"end\":48557,\"start\":48540},{\"end\":48570,\"start\":48557},{\"end\":48583,\"start\":48570},{\"end\":48600,\"start\":48583},{\"end\":48612,\"start\":48600},{\"end\":48624,\"start\":48612},{\"end\":48945,\"start\":48932},{\"end\":48961,\"start\":48945},{\"end\":48978,\"start\":48961},{\"end\":49250,\"start\":49232},{\"end\":49267,\"start\":49250},{\"end\":49541,\"start\":49524},{\"end\":49555,\"start\":49541},{\"end\":49570,\"start\":49555},{\"end\":50112,\"start\":50095},{\"end\":50126,\"start\":50112},{\"end\":50141,\"start\":50126},{\"end\":50154,\"start\":50141},{\"end\":50171,\"start\":50154},{\"end\":50423,\"start\":50407},{\"end\":50443,\"start\":50423},{\"end\":50453,\"start\":50443},{\"end\":50470,\"start\":50453},{\"end\":50736,\"start\":50723},{\"end\":50751,\"start\":50736},{\"end\":50770,\"start\":50751},{\"end\":50784,\"start\":50770},{\"end\":50797,\"start\":50784},{\"end\":51195,\"start\":51174},{\"end\":51470,\"start\":51466},{\"end\":51488,\"start\":51470},{\"end\":51502,\"start\":51488}]", "bib_venue": "[{\"end\":33255,\"start\":33207},{\"end\":33720,\"start\":33634},{\"end\":34207,\"start\":34150},{\"end\":35102,\"start\":35044},{\"end\":36634,\"start\":36568},{\"end\":37910,\"start\":37841},{\"end\":38443,\"start\":38363},{\"end\":39785,\"start\":39740},{\"end\":40287,\"start\":40229},{\"end\":43106,\"start\":42963},{\"end\":43837,\"start\":43754},{\"end\":44334,\"start\":44283},{\"end\":44785,\"start\":44737},{\"end\":45252,\"start\":45199},{\"end\":46003,\"start\":45958},{\"end\":49777,\"start\":49682},{\"end\":33205,\"start\":33142},{\"end\":33632,\"start\":33531},{\"end\":34148,\"start\":34076},{\"end\":34602,\"start\":34537},{\"end\":35042,\"start\":34969},{\"end\":35545,\"start\":35471},{\"end\":35961,\"start\":35915},{\"end\":36271,\"start\":36225},{\"end\":36566,\"start\":36485},{\"end\":36874,\"start\":36797},{\"end\":37395,\"start\":37332},{\"end\":37839,\"start\":37755},{\"end\":38361,\"start\":38283},{\"end\":38936,\"start\":38867},{\"end\":39354,\"start\":39313},{\"end\":39738,\"start\":39687},{\"end\":40227,\"start\":40154},{\"end\":40607,\"start\":40583},{\"end\":40927,\"start\":40875},{\"end\":41231,\"start\":41188},{\"end\":41559,\"start\":41490},{\"end\":42014,\"start\":41945},{\"end\":42431,\"start\":42373},{\"end\":42961,\"start\":42819},{\"end\":43752,\"start\":43670},{\"end\":44281,\"start\":44232},{\"end\":44735,\"start\":44681},{\"end\":45197,\"start\":45138},{\"end\":45562,\"start\":45495},{\"end\":45956,\"start\":45896},{\"end\":46314,\"start\":46197},{\"end\":46711,\"start\":46660},{\"end\":47061,\"start\":46998},{\"end\":47372,\"start\":47355},{\"end\":47699,\"start\":47642},{\"end\":48538,\"start\":48449},{\"end\":49012,\"start\":48995},{\"end\":49301,\"start\":49267},{\"end\":49680,\"start\":49570},{\"end\":50093,\"start\":50033},{\"end\":50510,\"start\":50470},{\"end\":50866,\"start\":50797},{\"end\":51238,\"start\":51195},{\"end\":51546,\"start\":51502}]"}}}, "year": 2023, "month": 12, "day": 17}