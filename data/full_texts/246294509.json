{"id": 246294509, "updated": "2023-11-08 15:12:56.267", "metadata": {"title": "SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders", "authors": "[{\"first\":\"Tianshuo\",\"last\":\"Cong\",\"middle\":[]},{\"first\":\"Xinlei\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Self-supervised learning is an emerging machine learning paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the machine learning model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2201.11692", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ccs/CongHZ22", "doi": "10.1145/3548606.3559355"}}, "content": {"source": {"pdf_hash": "a6cb46a2d7549d82abe893602b9a22b406859ebb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2201.11692v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9e571533d0959c2442c73ce9fc01b077ea306f79", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a6cb46a2d7549d82abe893602b9a22b406859ebb.txt", "contents": "\nSSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders\nNovember 2022\n\nTianshuo Cong \nInstitute for Advanced Study\nTsinghua University\nBNRist\n\nXinlei He \nCISPA Helmholtz Center for Information Security\n\n\nYang Zhang \nCISPA Helmholtz Center for Information Security\n\n\nSSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders\n\n2022 ACM SIGSAC Conference on Computer and Communications Security\nNovember 2022\nSelf-supervised learning is an emerging machine learning paradigm. Compared to supervised learning which leverages high-quality labeled datasets, self-supervised learning relies on unlabeled datasets to pre-train powerful encoders which can then be treated as feature extractors for various downstream tasks. The huge amount of data and computational resources consumption makes the encoders themselves become the valuable intellectual property of the model owner. Recent research has shown that the machine learning model's copyright is threatened by model stealing attacks, which aim to train a surrogate model to mimic the behavior of a given model. We empirically show that pre-trained encoders are highly vulnerable to model stealing attacks. However, most of the current efforts of copyright protection algorithms such as watermarking concentrate on classifiers. Meanwhile, the intrinsic challenges of pre-trained encoder's copyright protection remain largely unstudied. We fill the gap by proposing SSLGuard, the first watermarking scheme for pre-trained encoders. Given a clean pre-trained encoder, SSLGuard injects a watermark into it and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Our extensive evaluation shows that SSLGuard is effective in watermark injection and verification, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.\n\nIntroduction\n\nDeep learning, in particular supervised learning (SL), has gained tremendous success during the past decade, and the development of SL relies on a large amount of high-quality labeled data. However, high-quality data is often difficult to collect and the cost of labeling is expensive. Self-supervised learning (SSL) is proposed to resolve such restrictions by generating \"labels\" from the unlabeled dataset (called pretraining dataset) and uses the derived \"labels\" to pre-train an encoder which can output informative embeddings. SSL encoders have shown great promise in various downstream tasks. For instance, on the ImageNet dataset [46], Chen et al. [13] show that, by using SimCLR pre-trained with Im-ageNet (unlabeled), the downstream classifier can achieve 85.8% top-5 accuracy with only 1% labels, which outperforms a supervised AlexNet but uses 100\u00d7 fewer labels. He et al. [22] show that SSL can surpass SL under 7 downstream tasks including segmentation and detection. Therefore, compared to the SL-based classifier which only suits a specific classification task, the SSL pre-trained encoder can achieve remarkable performance on different downstream tasks.\n\nHowever, the data collection and training process of SSL encoders are also expensive as they benefit from larger datasets and more powerful computing devices. For example, the performance of MoCo [22] pre-trained with the Instagram-1B dataset (\u223c 1 billion images) outperforms that of the encoder pre-trained with the ImageNet-1M dataset (1.28 million images), and SimCLR requires 32 TPU v3 cores to train a ResNet-50 due to the large batch size setting (i.e., 4096) [13]. Therefore, the cost to train a powerful encoder by SSL is prohibitive for individuals, and the highperformance encoders are usually pre-trained by leading AI companies with sufficient computing resources and shared via cloud platforms for commercial usage, i.e., Encoder-asa-Service (EaaS) [1,2]. For instance, Clarifai [2] provides image encoders for different downstream services. OpenAI provides access to GPT-3 [6] which can be considered as a powerful encoder for a variety of natural language processing (NLP) downstream tasks, such as code generation, style transfer, etc.\n\nOnce deployed on the cloud platform, the encoders are not only accessible to legitimate users but also threatened by potential adversaries. As illustrated in Figure 1, for the legitimate user, the encoder is used to train a downstream classifier. On the other hand, an adversary may perform model stealing attacks [32,42,49,53] which aim to learn a surrogate encoder that has similar functionality. Such attacks may not only compromise the intellectual property of the service provider but also serve as a stepping stone for further attacks such as membership inference attacks (MIA) [37,48,50] (i.e., mount MIA offline by using surrogate encoders), backdoor attacks [30] (i.e., publish another backdoored encoder), and adversarial attacks [43]. The security and privacy of SSL encoders are threatened by these attacks, which call for effective defenses.\n\nAs one major technique to protect the machine learning model's copyright, model watermarking [28,34] inserts a secret pattern into the model. Then, the ownership can be claimed if a similar or the same pattern is successfully extracted from the model. Recent studies on model watermarking mainly focus on the classifier that targeted a specific task [5,28,61]. However, watermarking SSL encoders may face several intrinsic challenges. First, model watermarking against the classifier usually needs to specify a target class, while the SSL encoder does not have such information. Second, downstream tasks for SSL encoders are flexible, which challenges the traditional model watermarking scheme that is only suitable for one specific downstream task. Therefore, a new watermarking scheme should be designed to overcome those challenges to protect the copyright of SSL encoders. To the best of our knowledge, this has been left largely unstudied.\n\nOur Work. In this paper, we first quantify the copyright breaching threat against SSL encoders through the lens of model stealing attacks. Then, we introduce SSLGuard, the first watermarking scheme for the SSL encoders to protect their copyrights. Note that in this work, we consider image encoders only.\n\nFor model stealing attacks, we first assume that the adversary only has black-box access to the victim encoder. We then characterize the adversary's background knowledge into two dimensions, i.e., the surrogate dataset and the surrogate encoder's architecture. Regarding the surrogate dataset which is used to train the surrogate encoder, we consider the adversary may or may not know the victim encoder's pretraining dataset. Regarding the surrogate encoder's architecture, we first assume that it shares the same architecture as the victim encoder. Then, we relax this assumption and find that the effectiveness of model stealing attacks can even increase by leveraging a larger model architecture. We empirically show that the model stealing attacks achieve remarkable performance. For instance, given a ResNet-50 encoder pretrained on ImageNet by SimCLR, the ResNet-101 surrogate encoder can achieve 0.944 accuracy on STL-10 while the accuracy for the victim encoder is 0.948. We also show that the cost of stealing an encoder is much smaller than pre-training it from scratch, e.g., pre-training a BYOL ResNet-50 encoder costs $5,713.92 while stealing it with ResNet-101 only costs $72.49 (see Table 3 for the detailed comparison). Such observation emphasizes the underlying threat of jeopardizing the model owner's intellectual property and the emergence of copyright protection.\n\nTo protect the copyright of SSL encoders, we propose a robust black-box watermarking scheme named SSLGuard. Concretely, the goal of SSLGuard is to inject a watermark based on a given secret vector into a clean SSL encoder. The output of SSLGuard contains a watermarked encoder and a key-tuple. To be specific, the key-tuple consists of the secret vector, a verification dataset, and a decoder. SSLGuard finetunes a clean encoder to a watermarked encoder which can keep the utility and map samples in the verification dataset to secret embeddings. We further introduce a decoder to transform these secret embeddings into the secret vector. For other encoders, the decoder only transforms the embeddings generated from the verification dataset into random vectors. Recent research has shown that if a watermarked model is stolen, its corresponding watermark usually vanishes [40]. To remedy this situation, SSLGuard adopts a shadow dataset and a shadow encoder to locally simulate model stealing attacks. Meanwhile, SSLGuard optimizes a trigger that can be recognized by both the watermarked encoder and the shadow encoder. We later show in Section 5 that such a design can strongly preserve the watermark even in the surrogate encoder stolen by the adversary.\n\nEmpirical evaluations over 7 datasets (i.e., ImageNet, CIFAR-10, CIFAR-100, STL-10, GTSRB, MNIST, and FashionMNIST) and 3 encoder pre-training algorithms (i.e., SimCLR, MoCo v2, and BYOL) show that SSLGuard can successfully inject/extract the watermark to/from the SSL encoder without sacrificing its performance and is robust to model stealing attacks. Moreover, we consider various types of watermark removal attacks including input preprocessing (noising), output perturbing (noising and truncation), and model modification (overwriting, pruning, and fine-tuning). We empirically show that SSLGuard is still effective in such a scenario.\n\nIn summary, we make the following contributions:\n\n\u2022 We unveil that the SSL pre-trained encoders are highly vulnerable to model stealing attacks.\n\n\u2022 We propose SSLGuard, the first watermarking scheme against SSL pre-trained encoders, which can protect the intellectual property of published encoders.\n\n\u2022 Extensive evaluations show that SSLGuard is effective in injecting and extracting watermarks, and it is robust against model stealing and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.\n\n2 Background 2.1 Self-supervised Learning Self-supervised learning is a rising AI paradigm that aims to train an encoder by a large scale of unlabeled data. A high-performance pre-trained encoder can be shared into the public platform as an upstream service. In downstream tasks, customers can use the embeddings output from the pre-trained encoder to train their classifiers with limited labeled data [13] or even no data [44]. One of the most remarkable self-supervised learning paradigms is contrastive learning [13,15,21,22,44]. In general, encoders are pretrained through contrastive losses which calculate the similarities of embeddings in a latent space. In this paper, we consider three representative contrastive learning algorithms, i.e., SimCLR [13], MoCo v2 [15], and BYOL [44].\n\nSimCLR [13]. SimCLR is a simple framework for contrastive learning. It consists of 4 components, including Data augmentation, Base encoder f (\u00b7), Projection head g(\u00b7) and Contrastive loss function.\n\nThe data augmentation module is used to transform a data sample x randomly into two augmented views. Specifically, the augmentations include random cropping, random color distortions, and random Gaussian blur. If two augmented views are generated from the same data sample x, we treat them as a positive pair, otherwise, they are considered a negative pair. Positive pairs of x are denoted asx i andx j .\n\nBase encoder f (\u00b7) extracts feature vectors h i = f (x i ) from augmented inputs. Projection head g(\u00b7) is a small neural network that maps feature vectors to a latent space where contrastive loss is applied. SimCLR uses a multilayer perceptron (MLP) as the projection head g(\u00b7) to obtain the output\nz i = g(h i ).\nFor a set of samples {x k } including both positive and negative pairs, contrastive loss aims to maximize the similarity between the feature vectors of positive pairs and minimize those of negative pairs. Given N samples in each mini-batch, we could get 2N augmented samples. Formally, the loss function for a positive pairx i andx j can be formulated as:\nl(i, j) = \u2212 log exp(sim(z i , z j )/\u03c4) \u2211 2N k=1,k =i exp(sim(z i , z k )/\u03c4) ,\nwhere sim(\u00b7, \u00b7) denotes the cosine similarity function and \u03c4 denotes a temperature parameter. SimCLR jointly trains the base encoder and projection head by minimizing the final loss function:\nL SimCLR = 1 2N N \u2211 k=1 [l(2k \u2212 1, 2k) + l(2k, 2k \u2212 1)],\nwhere 2k \u2212 1 and 2k are the indexes for each positive pair. Once the model is trained, SimCLR discards the projection head and keeps the base encoder f (\u00b7) only, which serves as the pre-trained encoder. MoCo v2 [15]. Momentum Contrast (MoCo) [22] is a famous contrastive learning algorithm, and MoCo v2 is the modified version (using a projection head and more data augmentations). MoCo points out that contrastive learning can be regarded as a dictionary lookup task. The \"keys\" in the dictionary are the embeddings output from the encoder. A \"query\" matches a key if they are encoded from the same image. MoCo aims to train an encoder that outputs similar embeddings for a query and its matching key, and dissimilar embeddings for others. The dictionary is desirable to be large and consistent, which contains rich negative images and helps to learn good embeddings. MoCo aims to build such a dictionary with a queue and momentum encoder.\n\nMoCo contains two parts: query encoder f q (x; \u03b8 q ) and key encoder f k (x; \u03b8 k ). Given a query sample x q , MoCo gets an encoded query q = f q (x q ). For other samples x k , MoCo builds a dictionary whose keys are {k 0 , k 1 , ...}, k i = f k (x k i ). The dictionary is a dynamic queue that keeps the current mini-batch encoded embeddings and discards the ones in the oldest mini-batch. The benefit of using a queue is decoupling the dictionary size from the mini-batch size, so the dictionary size can be set as a hyper-parameter. Assume k + is the key that q matches, the loss function will be defined as:\nL MoCo = \u2212 log exp(q \u00b7 k + /\u03c4) \u2211 K i=0 exp(q \u00b7 k i /\u03c4) .\nHere \u03c4 is a temperature hyper-parameter. MoCo trains f q by minimizing contrastive loss and updates \u03b8 q by gradient descent. However, it is difficult to update \u03b8 k by backpropagation because of the queue, so f k is updated by moving-averaged as:\n\u03b8 k \u2190 m\u03b8 k + (1 \u2212 m)\u03b8 q ,\nwhere m \u2208 [0, 1) denotes a momentum coefficient. Finally, we keep the f q as the final pre-trained encoder. BYOL [21]. Bootstrap Your Own Latent (BYOL) is a novel self-supervised learning algorithm. Different from previous methods, BYOL does not rely on negative pairs, and it has a more robust selection of image augmentations. BYOL's architecture consists of two neural networks: online networks and target networks. The online networks, with parameters \u03b8, consist of an encoder f \u03b8 , a projector g \u03b8 and a predictor q \u03b8 . The target networks are made up of an encoder f \u03be and a projector g \u03be . The two networks bootstrap the embeddings and learn from each other.\n\nGiven an input sample x, BYOL produces two augmented views v \u2190 t(x) and v \u2190 t (x) by using image augmentations t and t , respectively. The online networks output a projection z \u03b8 \u2190 g \u03b8 ( f \u03b8 (v)) and target networks output a target projection z \u03be \u2190 g \u03be ( f \u03be (v )). The online networks' goal is to make the prediction q \u03b8 (z \u03b8 ) similar to z \u03be . Formally, the similarity can be defined as the following:\nL \u03b8,\u03be = 2 \u2212 2 \u00b7 q \u03b8 (z \u03b8 ), z \u03be q \u03b8 (z \u03b8 ) 2 \u00b7 z \u03be 2 .\nConversely, BYOL feeds v to the online networks and v to the target networks separately and gets L \u03b8,\u03be . The final loss function can be formulated as:\nL BY OL = L \u03b8,\u03be + L \u03b8,\u03be .\nBYOL updates the weights of the online and target networks by:\n\u03b8 \u2190 optimizer(\u03b8, \u03b8 L BY OL \u03b8,\u03be , \u03b7), \u03be \u2190 \u03c4\u03be + (1 \u2212 \u03c4)\u03b8,\nwhere \u03b7 is the learning rate of the online networks. The target networks' weight \u03be is updated in a weighted average way, and \u03c4 \u2208 [0, 1] denotes the decay rate of the target encoder. Once the model is trained, we treat the online networks' encoder f \u03b8 as the pre-trained encoder.\n\n\nModel Stealing Attacks\n\nModel stealing attacks [10,11,17,27,32,42,49,53,56] aim to steal the parameters or the functionality of the victim model. To achieve this goal, given a victim model f (x; \u03b8), the adversary can issue a bunch of queries to the victim model and obtain the corresponding responses. Then the queries and responses serve as the inputs and \"labels\" to train the surrogate model, denoted as f (x; \u03b8 ). Formally, given a query dataset D, the adversary can train f (x; \u03b8 ) by\nL steal = E x\u223cD [sim( f (x; \u03b8), f (x; \u03b8 ))],(1)\nwhere sim(\u00b7, \u00b7) is a similarity function. Note that if the victim model is a classifier, the response can be the prediction probability of each class. If the victim model is an encoder, the response can be the embeddings. A successful model stealing attack may not only breach the intellectual property of the victim model but also serve as a springboard for further attacks such as MIA [23-26, 35-37, 48, 50, 51], backdoor attacks [14,30,47,59] and adversarial attacks [9,20,33,39,43]. Previous work has demonstrated that neural networks are vulnerable to model stealing attacks. In this paper, we concentrate on model stealing attacks on SSL encoders, which have not been studied yet.\n\n\nDNNs Watermarking\n\nConsidering the cost of training deep neural networks (DNNs), DNNs watermarking algorithms have received wide attention as it is an effective method to protect the copyright of the DNNs. Watermarking is a traditional concept for media such as audio and video, and it has been extended to protect the intellectual property of machine learning models recently [5,28,41,45,54]. Concretely, the watermarking procedure can be divided into two steps, i.e., injection and verification. In the injection step, the model owner injects a watermark and a pre-defined behavior into the model in the training process. The watermark is usually secret, such as a trigger that is only known to the model owner [34]. In the verification step, the ownership of a suspect model can be claimed if the watermarked encoder has the pre-defined behavior when the input samples contain the trigger.\n\nSo far, the watermarking algorithms mainly focus on the classifiers in a specific task. However, how to design a watermarking algorithm for SSL pre-trained encoders that can fit various downstream tasks remains largely unexplored.\n\n\nThreat Model\n\nIn this paper, we consider two parties: the defender and the adversary. The defender is the owner of the victim encoder, whose goal is to protect the copyright of the victim encoder when publishing it as an online service. The adversary, on the contrary, aims to steal the victim encoder, i.e., by model stealing attacks or directly obtaining the model (insider threat), and bypass the copyright protection method for the victim encoder.\n\nAdversary's Motivation. Adversary's motivation lies in two areas: Firstly, EaaS is being popular and high-performance SSL encoders are often pre-trained by top AI companies [1, 2]. Pre-training an encoder requires collecting a huge amount of data, expert knowledge for designing architectures/algorithms, and many failure trials, which are expensive. This makes the model architectures or training algorithms regarded as trade secrets and will not be publicly available, which makes it less possible for the adversary to directly train a comparable performance SSL encoder from scratch. Secondly, the cost of stealing an SSL encoder is quite less than training an SSL encoder from scratch. For instance, pre-training a ResNet-50 by BYOL needs $5,713.92 while generating a surrogate encoder with similar performance only needs $72.49 (see Table 3 for more details). Once the adversary steals the victim encoder successfully, they can resell it or deploy it on the cloud platform to be a commercial competitor.\n\nAdversary's Background Knowledge. For the adversary, we first assume that they only have black-box access to the victim encoder, which is the most challenging setting for the model stealing attacks [27,32,42,49]. In this setting, the adversary can only query the victim encoder with data samples and obtain their corresponding responses, i.e., the embeddings, to train the surrogate encoders. We categorize the adversary's background knowledge into two dimensions, i.e., the pre-training dataset and the victim encoder's architecture. Concretely, we assume that the adversary has a query dataset to perform the attack. Note that the query dataset does not need to be in the same distribution as the victim encoder's pre-training dataset. Regarding the victim encoder's architecture, we first assume that the adversary can obtain it since such information is usually publicly accessible. Then we empirically show that this assumption can be relaxed, and the attack is even more effective when the adversary leverages a deeper model architecture.\n\nAdaptive Adversary. We then consider an adaptive adversary who knows that the victim encoder has already been watermarked. This means they can leverage watermark removal techniques including input preprocessing (noising), output perturbing (noising and truncation), and model modification (overwriting, pruning, and fine-tuning) on the encoder to bypass the watermark verification.\n\n\nDesign of Watermarking Scheme\n\nIn this section, we present SSLGuard, a watermarking scheme to preserve the copyright of the SSL pre-trained encoders. SSLGuard should have the following properties:\n\n\u2022 Fidelity: To minimize the impact of SSLGuard on the legitimate users, the influence of SSLGuard on the clean  Figure 2: The workflow of SSLGuard. Given a clean SSL pretrained encoder (colored in green), SSLGuard outputs a keytuple and a watermarked encoder (colored in yellow). The defender can employ the watermarked encoder on the cloud platform or adopt the key-tuple to extract the watermark from a suspect encoder.\n\npre-trained encoders should be negligible, which means SSLGuard should keep the utility of downstream tasks.\n\n\u2022 Effectiveness: SSLGuard should judge whether a suspect model is a watermarked (or a clean) model with high precision. In other words, SSLGuard should extract watermarks from watermarked encoders effectively.\n\n\u2022 Undetectability: The watermark cannot be extracted by a no-matching secret key-tuple. Undetectability ensures that ownership of the SSL pre-trained encoder could not be misrepresented.\n\n\u2022 Efficiency: SSLGuard should inject and extract watermark efficiently. For instance, the time cost for the watermark injection and extraction process should be less than pre-training an SSL model.\n\n\u2022 Robustness: SSLGuard should be robust against model stealing attacks and other watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.\n\nIn the following subsections, we will introduce the design methods for SSLGuard. Table 1 summarizes the notations used in this paper.\n\n\nOverview\n\nAs shown by Cai et al. [7], in space R n , given two random vectors which are independently chosen with the uniform distribution on the unit sphere, the empirical distribution of angles \u03b8 between these two random vectors converges to a distribution with the following probability density function:\nf (\u03b8) = 1 \u221a \u03c0 \u00b7 \u0393( n 2 ) \u0393( n\u22121 2 ) \u00b7 (sin \u03b8) n\u22122 , \u03b8 \u2208 [0, \u03c0].\nThe distribution f (\u03b8) will be very close to normal distribution if n \u2265 5. The equation above implies that two random vectors in high-dimensional space (such as R 256 ) are almost orthogonal. The inspiration for SSLGuard is based on the above mathematical fact: Given a vector that has the same dimension as embeddings, if the vector is randomly initialized, the average cosine similarity between these embeddings and the vector should be concentrated around 0. However, if the average cosine similarity is much bigger than 0 or even close to 1, this can be considered as a signal that those embeddings are strongly related to this vector. Therefore, the defender can generate a verification dataset D v and a secret vector sk \u2208 R m . Then, the defender can fine-tune a clean encoder to transform samples from D v to the embeddings and train a decoder to further transform the embeddings to the decoded vectors that have high cosine similarity with sk. Meanwhile, if the defender input these verification samples to a clean encoder, the distribution of cosine similarity between decoded vectors and sk should be a normal distribution with 0 as its mean value. We leverage this mechanism to design SSLGuard.\n\nThe workflow of SSLGuard is shown in Figure 2. Concretely, given a clean encoder F which is pre-trained by a certain SSL algorithm, SSLGuard will output a watermarked encoder F * and a secret key-tuple \u03ba as:\nF * , \u03ba \u2190 SSLGuard(F), \u03ba = {D v , G, sk}.\nThe secret key-tuple \u03ba consists of three items: a verification dataset D v , a decoder G, and a secret vector sk. G is an MLP that maps the embeddings generated from the encoder to a new latent space (same dimension as sk) to calculate the cosine similarity with sk. Concretely, given an input image x, the decoded vector sk x can be defined as:\nsk x = G(E(x)), x \u2208 D,\nwhere sk x \u2208 R m is a vector whose dimension is the same as the secret vector sk, D is a given dataset, and E is an encoder (i.e., F or F * , etc).\n\nSSLGuard contains two processes, i.e., watermark injection and extraction. For the injection process, SSLGuard uses a secret key-tuple \u03ba to inject the watermark into a clean encoder F and outputs watermarked encoder F * as: F * \u2190 Inject(F, \u03ba). The defender can release F * to the cloud platform and keep \u03ba secret. For the extraction process, given a suspect encoder F , the defender can use \u03ba to extract decoded\nvectors sk x from F by: {sk x } \u2190 Extract(F , \u03ba), x \u2208 D v ,\nwhere {sk x } is a set of decoded vectors. Then, the defender can measure the cosine similarity between {sk x } and sk, and judge if a suspect encoder F is a copy by:\nVerify(F ) = 1, WR > th v 0, otherwise ,\nhere we adopt watermark rate (WR) as the metric to denote the ratio of the verified samples whose outputs sk x are close to sk. Concretely, WR is defined as:\nWR = 1 |D v | \u2211 x\u2208D v 1(sim(sk x , sk) > th w ).\nIn summary, we need two thresholds here: th v and th w . th w is used to calculate WR, and th v is a threshold to verify the copyright. We set th w = 0.5 and th v = 0.5 by default. Note that the th w can be set to a smaller value as we show in Section 5 that the WR is 0 for the clean encoders. The overview of SSLGuard is depicted in Figure 3. Concretely, we first train a watermarked encoder that contains the information of the verification dataset and the secret vector. The clean encoder serves as a query-based API to guide the training process. The shadow encoder is used to simulate the model stealing process to better preserve the watermark under model stealing attacks. The watermarked encoder should keep the utility of the clean encoder while preserving the watermark injected in it.  \nDataset: ! Model: * # # $ Embedding Secret vector Decoded vector \u2112 %&!'( \u2112 )*'+,, \u2112 '+,,Notation Watermark\n\nPreparation\n\nTo watermark a pre-trained encoder, the defender should prepare a private dataset D p , a mask M, and a random trigger T .\n\nThe mask M is a binary matrix that contains the position information of trigger T , which means M and T have the same size as the private samples x p . Following [18,59], we inject the trigger into x p by:\nP (x p , T ) = (1 \u2212 M) \u2022 x p + M \u2022 T, x p \u2208 D p ,\nwhere \u2022 denotes the element-wise product. Therefore, given the trigger T , we can generate the verification dataset as:\nD v = {x v |x v = P (x p , T ), x p \u2208 D p }.\nHere we define three loss functions, i.e., correlated loss L corr , uncorrelated loss L uncorr , and embedding match loss L match to achieve three goals. Our first goal is to let the decoded vectors transformed from the verification dataset D v to be similar to the secret vector sk, and we define correlated loss function as:\nL corr (D v , E) = \u2212 \u2211 x\u223cD v sim(sk x , sk) |D v | ,(2)\nwhere sim(\u00b7, \u00b7) is a similarity function. If not otherwise specified, we use cosine similarity as the similarity function. The goal of L corr is to train an encoder and an decoder together to transform x into sk x , where sk x is correlated with sk. The more similar sk x and sk are, the smaller L corr will be. Secondly, given a clean dataset D and an encoder E, the decoder G transforms embeddings to the orthogonal direction of sk for uncorrelated samples x \u2208 D. Therefore, we could get another loss function, uncorrelated loss function, as:\nL uncorr (D, E) = ( \u2211 x\u223cD sim(sk x , sk) |D| ) 2 .(3)\nFinally, we here define an embedding match loss function to match the embeddings generated from two encoders E and E :\nL match (D, E , E ) = \u2212 \u2211 x\u223cD sim(E (x), E (x)) |D| .(4)\nSSLGuard leverages L match to maintain the utility of the watermarked encoder and simulate the model stealing attacks.\n\n\nWatermark Injection\n\nAs shown in Figure 3, SSLGuard adopts three encoders: a clean encoder F(x; \u03b8), a watermarked encoder F * (x; \u03b8 w ) and a shadow encoder F s (x; \u03b8 s ). Meanwhile, SSLGuard also uses three datasets: a target dataset D t , a shadow dataset D s , and a verification dataset D v . In the following part, we will introduce our loss functions for each module. Shadow Encoder. For the shadow encoder, its task is to mimic the model stealing attacks. Here we use D s to simulate the query process. The loss function of the shadow encoder is:\nL s = L match (D s , F * , F s ).(5)\nTrigger and Decoder. Given a verification dataset, we aim to optimize a trigger T and a decoder G to extract sk from both the watermarked encoder and the shadow encoder, but not the clean encoder. The corresponding loss can be defined as:\nL 1 = L uncorr (D v , F) + L corr (D v , F * ) + L corr (D v , F s ). (6)\nBesides, for the clean encoder F, watermarked encoder F * , and the shadow encoder F s , the decoder should not map the decoded keys closely to sk from the target dataset, the loss to achieve this goal can be defined as:\nL 2 = L uncorr (D t , F) + L uncorr (D t , F * ) + L uncorr (D t , F s ). (7)\nGiven the above losses, the final loss function for trigger and decoder can be defined as:\nL T D = L 1 + L 2 .(8)\nWatermarked Encoder. For the watermarked encoder, we want it to keep the utility of the clean encoder. Therefore, for the samples from D t , we force the embeddings from F and F * to become similar through L match . The loss L 3 can be defined as:\nL 3 = L match (D t , F, F * ).(9)\nMeanwhile, the decoder G should successfully extract sk from the verification dataset D v instead of the target dataset D t . The corresponding loss L 1 to achieve this goal is defined as:\nL 4 = L uncorr (D t , F * ) + L corr (D v , F * ).(10)\nThe final loss function for the watermarked encoder is:\nL w = L 3 + L 4 .(11)\nOptimization Problem. After designing all loss functions, we formulate SSLGuard as an optimization problem. Concretely, we update the parameters as follows:\n\u03b8 s \u2190 Optimizer(\u03b8 s , \u03b8 s L s , \u03b7 s ), T, G \u2190 Optimizer(T, G, T,G L T D , \u03b7 T D ), \u03b8 w \u2190 Optimizer(\u03b8 w , \u03b8 w L w , \u03b7 w ),(12)\nwhere \u03b7 s , \u03b7 T D , and \u03b7 w are learning rates of shadow encoder, watermarked encoder, trigger, and decoder, respectively. We note that we update \u03b8 s , T , G, and \u03b8 w sequentially in one iteration, and we stop the optimization until the iteration reaches the max iteration number.\n\n\nEvaluation\n\n\nExperimental Setup\n\nDatasets. We use the following 7 datasets to conduct our experiments.\n\n\u2022 ImageNet [46]. The ImageNet dataset contains 1.2 million training images distributed in 1,000 classes. Each image has size 224 \u00d7 224 \u00d7 3.\n\n\u2022 CIFAR-10 [3] The CIFAR-10 dataset has 60, 000 images in 10 classes. Among them, there are 50, 000 images for training and 10, 000 images for testing. The size of each image is 32 \u00d7 32 \u00d7 3.\n\n\u2022 CIFAR-100 [3]. Similar to CIFAR-10, The CIFAR-100 dataset contains 60, 000 images with size 32 \u00d7 32 \u00d7 3 in 100 classes, and there are 500 training images and 100 testing images in each class.\n\n\u2022 STL-10 [16]. The STL-10 dataset consists of 5, 000 training images and 8, 000 testing images in 10 classes. Besides, it also contains 100, 000 unlabeled images. Note that the images on STL-10 are acquired from labeled images on ImageNet. 2 The size of each image is 96 \u00d7 96 \u00d7 3.\n\n\u2022 GTSRB [52]. German Traffic Sign Recognition Benchmark (GTSRB) contains 39, 209 training images and 12, 630 testing images. It contains 43-category traffic signs.\n\n\u2022 MNIST [4]. MNIST is a handwritten digits dataset that contains 60,000 training images and 10,000 testing images in 10 classes. Each image has size 28 \u00d7 28 \u00d7 1.\n\n\u2022 FashionMNIST [58]. FashionMNIST (F-MNIST) is a Zalando's article image dataset that has 10 classes. It has 60,000 training images and 10,000 testing images. Each sample is a grayscale image with size 28 \u00d7 28 \u00d7 1.\n\nWe resize images of all datasets to 224 \u00d7 224 \u00d7 3 in our experiments. We use ImageNet as the pre-training dataset; STL-10, CIFAR-10, F-MNIST, and MNIST as the downstream datasets; and STL-10, CIFAR-10, CIFAR-100, and GTSRB as the query dataset (to launch model stealing attacks). Note that for the STL-10 dataset, we randomly split the unlabeled samples (100,000) of it into two parts (each containing 50,000 samples). We consider the first part as the unlabeled STL-10 dataset and the second part as the same distribution unlabeled STL-10 dataset which is denoted as STL-10 (s).\n\nPre-trained Encoder. In our experiments, we adopt realworld contrastive learning pre-trained encoders as the victim encoders. Concretely, we download the checkpoints of the encoders from the official website (i.e., SimCLR 3 and MoCo v2 4 ) or the public platform (i.e., BYOL 5 ). All the encoders are ResNet-50 pre-trained on ImageNet. Downstream Classifier. We use a 3-layer MLP as the downstream classifier with 512 and 256 neurons in its hidden layer. For each downstream task, we freeze the parameters of the pre-trained encoders and train the downstream classifier for 20 epochs using Adam optimizer [31] with 0.005 learning rate. SSLGuard. We reload the clean encoder and fine-tune it to be the watermarked encoder. Note that we freeze the weights in batch normalization layers following the settings by Jia et al. [30]. We consider the unlabeled STL-10 dataset (with only 50,000 images as mentioned above) as both D s and D t , and adopt a ResNet-50 as the shadow encoder's architecture. We sample 100 images from 5 random classes on ImageNet as our D p . Note that each class contains 20 images and the D p for watermarking SimCLR, MoCo v2, and BYOL are non-overlapping. For each sample in D p , 35% space will be patterned by the trigger. We leverage the SGD optimizer with 0.01 learning rate to train both the watermarked encoder and shadow encoder for 50 epochs. The batch size in our experiment is 8. The dimension of sk is 256. For the trigger, we randomly generate a 224 \u00d7 224 \u00d7 3 tensor from a uniform distribution in [0, 1] as the initial trigger. We use a 3-layer MLP as the decoder G. The numbers of G's neurons are 512, 256, and 256, respectively. We use the SGD optimizer with 0.005 learning rate to update both the decoder and the trigger.\n\n\nClean Downstream Accuracy\n\nGiven three clean SSL pre-trained encoders (i.e., pre-trained by SimCLR, MoCo v2, and BYOL on ImageNet), we first measure their downstream accuracy, denoted as clean downstream accuracy (CDA), for different tasks. We consider 4 downstream classification tasks, i.e., STL-10, CIFAR-10, MNIST, and F-MNIST. The CDA are shown in Table 2. We observe that the SSL pre-trained encoders can achieve remarkable performance on different downstream tasks, which means the SSL pre-trained encoders can learn high-level semantic information from one task (i.e., ImageNet), and the informative embeddings can generalize to other tasks (i.e., STL-10 and CIFAR-10). Meanwhile, the cost of pre-training SSL encoders is expensive (see Table 3), such observation further demonstrates the necessity of protecting the copyright of the SSL pre-trained encoders. Note that we adopt CDA as our baseline accuracy. Later we measure an encoder's performance by comparing its DA with CDA. \n\n\nModel Stealing Attacks\n\nSince the SSL pre-trained encoders (clean encoders) are powerful, we then evaluate whether they are vulnerable to model stealing attacks. To build a surrogate encoder, we consider three key information, i.e., the surrogate encoder's architecture, the distribution of the query dataset, and the similarity function used to \"copy\" the victim encoder. Surrogate Encoder's Architecture. We first investigate the impact of the surrogate encoder's architecture. Note that here we adopt the unlabeled STL-10 dataset (with 50,000 unlabeled samples) as the query dataset and cosine similarity as the similarity function to measure the difference between the victim and surrogate encoders' embeddings.\n\nSince the architecture of the victim encoder can be nonpublic, the adversaries may try different surrogate encoder architectures to perform the model stealing attacks.  Figure 4. A general trend is that the deeper the surrogate encoder's architecture, the better performance it can achieve on the downstream tasks. For instance, for SimCLR (Figure 4a), the DA on STL-10 and CIFAR-10 are 0.728 and 0.657 when the surrogate encoder's architecture is ResNet-18, while the DA increases to 0.759 and 0.697 when the surrogate encoder's architecture is changed to ResNet-50. This may be because a deeper model architecture can provide a wider parameter space and greater representation ability. Therefore, in general, deeper surrogate encoder's architectures can better \"copy\" the functionality from victim encoders. Note that in the following experiments, the adversary uses ResNet-50 as the surrogate encoder's architecture by default as it has comparable performance to ResNet-101 while requiring fewer resources. Distribution of the Query Dataset. Secondly, we evaluate the impact of the query dataset's distribution. In the real-world scenario, the adversary may or may not have the query dataset that is from the same distribution as the victim encoder's pre-training dataset. Here the adversary leverages ResNet-50 as the surrogate model's architecture and cosine similarity as the similarity function. Regarding the query dataset, the adversary may leverage the training dataset of CIFAR-10, CIFAR-100, and GTSRB and the unlabeled dataset of STL-10 to perform the attacks. The results are shown in Figure 5. First, we observe that the model stealing attack is more effective with querying by the same distribution dataset as the pre-training dataset. For instance, given the victim model trained by SimCLR (Figure 5a), when the downstream task is STL-10 classification, the DA for the surrogate encoders are 0.759, 0.646, 0.651, and 0.538 when the query dataset is STL-10, CIFAR-10, CIFAR-100, and GT-SRB, respectively. This demonstrates that the same distribution query dataset can better steal the functionality of the victim encoder. Another observation is that the distribution of the surrogate dataset may also influence DA on different tasks. For instance, given the victim model trained by BYOL (Figure 5c), when the downstream task is CIFAR-10 classification, the DA is 0.814 with CIFAR-10 as the query dataset, while only 0.769 with STL-10 as the query dataset. However, when the downstream task is STL-10 classification, the DA is 0.799 with CIFAR-10 as the query dataset but increases to 0.946 with STL-10 as the query dataset. Therefore, if the adversary is aware of the downstream task, they can construct a query dataset that is close to the downstream task to improve the stealing performance.\n\nSimilarity Function. Finally, we investigate the effect of similarity functions used in model stealing attacks. Besides cosine similarity, the adversary can also use mean absolute error (MAE) and mean square error (MSE) to match the victim encoder's embeddings. Here we assume that the adversary leverages ResNet-50 as the surrogate model's architecture and STL-10 as the query dataset. The results are shown in Figure 6. We can see that cosine similarity outperforms MAE and MSE in most settings. For instance, given the victim model trained by MoCo v2 (Figure 6b), the DA are all below 0.5 when using MAE and MSE. This can be credited to the normalization effect of cosine similarity, which helps to better learn the embeddings [21]. This indicates that co-sine similarity may better facilitate the stealing process.\n\nMonetary Cost. We compare the monetary costs of pretraining an SSL encoder from scratch and stealing an SSL encoder. We first measure the training cost of the encoders. To pre-train a ResNet-50 encoder, SimCLR needs 60 hours with 32 TPU v3s, MoCo v2 uses 212 hours with 8 NVIDIA V100 GPUs, and BYOL takes 72 hours with 32 NVIDIA V100 GPUs (the training information is from the official or open-source implementation as mentioned in Section 5.1). The cost of model stealing contains two parts: querying the victim encoders and training the surrogate encoders locally. We use the GPU price from Google cloud 6 to calculate the price for pre-training (i.e., We run our experiments on one NVIDIA A100 GPU whose price is $2.934 per hour). Meanwhile, we refer to the querying price, i.e., $1 per 1,000 queries, from AWS. 7 We adopt the unlabeled STL-10 dataset (50,000 samples), cosine similarity, and different architectures to launch model stealing attacks. The monetary costs are shown in Table 3. We observe that the cost of stealing the pre-trained encoder is much smaller than pre-training it from scratch. For instance, pre-trains a BYOL ResNet-50 encoder takes $5, 713.92 while stealing it with a ResNet-101 encoder only takes $72. 49. This indicates that an adversary can \"copy\" the victim encoder with much less cost. \n\n\nSSLGuard\n\nIn this section, we adopt SSLGuard to inject the watermarks into the clean encoders pre-trained by SimCLR, MoCo v2, and BYOL. We aim to validate four properties of SSLGuard, i.e., effectiveness, utility, undetectability, and efficiency. We will discuss the robustness of SSLGuard separately in Section 5.5.\n\nEffectiveness. We first evaluate the effectiveness of SSL-Guard. Concretely, we check whether the model owner can extract the watermark from the watermarked encoders. Ideally, the watermark should be successfully extracted from the watermarked encoder F * and shadow encoder F s , but not the clean encoder F. We use the generated key-tuple \u03ba to measure the watermark rate (WR) for F, F * , and F s on three SSL algorithms. As shown in Table 4, the WR of F * and F s are all 1.00, which means encoder F * and F s both contain the information of D v and sk. Meanwhile, the WR of F is 0.00.\n\nThis means SSLGuard is generic and does not judge a clean encoder to be a watermarked encoder. Fidelity. One of the initial intentions of SSLGuard is to maintain the utility of the original downstream task. To verify its fidelity, we first take BYOL as an example and visualize embeddings output from F byol (the clean encoder pre-trained by BYOL) and F byol * using t-Distributed Neighbor Embedding (t-SNE) [55], which is depicted in Figure 7. We observe that the t-SNE results of F byol and F byol * are almost identical and the embeddings are successfully separated by both encoders. This demonstrates that watermarked encoder trained by SSLGuard can faithfully reproduce the embeddings generated from the clean encoder. Also, we train downstream classifiers by using three watermarked encoders F simclr * , F moco * and F byol * on STL-10, CIFAR-10, F-MNIST, and MNIST. Table 5 shows the DA in different scenarios. We observe that the DA of the watermarked encoders are almost the same as that of the clean encoders. For instance, compared to F simclr , the DA for F simclr * only drops up to 0.009 Figure 7: The t-SNE visualizations of features output from F byol and F byol * when we input 800 samples in 10 classes randomly chosen from the STL-10 training dataset. Each point represents an embedding. Each color represents one class.  Undetectability. We then check if the watermark can be extracted by a no-matching key-tuple. Through SSLGuard, we generate three key-tuples: \u03ba simclr , \u03ba moco and \u03ba byol . We use one of the key-tuples to verify other watermarked encoders, such as using \u03ba simclr to judge F moco * . As shown in Table 6, we see that the WR are all 0.00 in no-match pairs, which means we cannot use a non-matching \u03ba to verify a watermarked encoder. Note also that we use only a single GPU (A100) in the watermark injection process, which is much less than the requirement for pre-training the SSL encoders. This demonstrates that SSLGuard can inject and extract watermarks efficiently.\n(a) F byol (b) F byol *\n\nRobustness\n\nWe now quantify the robustness of SSLGuard. Concretely, we evaluate SSLGuard against model stealing and the follow- ing watermark removal attacks: Input preprocessing, output perturbing, and model modification. For instance, the adversary can add noise to the input samples or output embeddings. Also, the adversary can modify the parameters of the encoder by overwriting, pruning, and fine-tuning. Since watermark removal attacks may affect the performance of the encoders, and the adversary aims to \"clean\" the encoder but keep its functionality, we measure DA and WR simultaneously of these surrogate encoders. We note that the victim encoders are the watermarked encoders, and we leverage Sim-CLR, MoCo, and BYOL to denote F simclr * , F moco * , and F byol * in this subsection. Regarding the downstream accuracy, we only show the results on BYOL (SimCLR and MoCo have similar trends).\n\n\nInput Preprocessing.\n\nHere we consider that the adversary may add i.i.d.Gaussian noise to each input image by x = x + \u03b5 1 \u00b7 N (0, 1). We evaluate DA on four downstream tasks and WR when we use different \u03b5 1 . The results of WR are shown in Figure 8a and DA are shown in Figure 9a. We first observe that DA drops as \u03b5 1 increases. For instance, the DA on CIFAR-10 drops from 0.932 to 0.865 when \u03b5 1 increases from 0.05 to 0.15. On the other hand, the WR are all 1.00 for different \u03b5 1 on Sim-CLR, MoCo, and BYOL, respectively. This may be because when we inject the trigger into D p , the distribution of D v is too special, so our watermarked encoder can remember these special samples, which is robust to the input noising attacks.\n\n\nOutput Perturbing.\n\nThe adversary can also add some perturbations to the embeddings before returning them as the outputs. Here we consider two kinds of perturbations, i.e., random noising and truncation.\n\nOutput Noising. The adversary may return the perturbed embeddings by adding i.i.d.Gaussian noise as h = h + \u03b5 2 \u00b7 N (0, 1), where h is the original embedding, h is the perturbed embedding, and \u03b5 2 is a hyper-parameter to control the noise level. Then, we evaluate DA and WR on different \u03b5 2 . From Figure 9b, we observe that DA decreases when \u03b5 2 increases. For instance, when \u03b5 2 increases from 0.05 to 0.15, DA on STL-10 drops from 0.940 to 0.905. However, the WR remains above 0.50 for all watermarked encoders (see Figure 8b), which means when we feed the embeddings with noise into the decoders, the secret vector can still be successfully extracted. Therefore, the adversaries cannot remove the watermark even if they add random noise to the embeddings at the expense of decreasing the model's performance.\n\nTruncation. The adversary may decrease the precision of the embeddings by leveraging truncation. For instance, the adversary retains k decimal places for each value in the embeddings, e.g., if k = 3, the adversary modifies the value 1.2368 to 1.236, and changes 1.2368 to 1 when k = 0. Figure 8c and Figure 9c shows WR and DA under different k. We observe that DA has a sharp drop when k decreases from 1 to 0. Meanwhile, WR are all above 0.5 instead of MoCo, i.e., WR of MoCo drops to 0.00 when k = 0, but the DA on STL-10 is only 0.10. Therefore, adversaries cannot remove the watermark from the encoder while remaining its functionality.\n\n\nModel Modification.\n\nWhen adversaries have white-box access to the encoder, they can try to remove the watermark by modifying the encoder's parameters. In this section, we consider three methods of model modification: watermark overwriting, model pruning, and fine-tuning.\n\nOverwriting. The adversary can also leverage SSLGuard to inject a new watermark into an SSL encoder whether or not they know that the encoder has already been injected with a watermark. The adversary aims to generate a new watermarked encoder F * from F * with a different key-tuple. We want to confirm if our original watermark can remain in F * as well. For each F * , we measure the DA on different downstream tasks and the WR of the original key-tuple. The results are shown in Table 7. We observe that although we overwrite the watermarked encoder with a new key-tuple to generate a new encoder, the original watermark is still preserved, i.e., the WR of the original watermark in the new encoder is 1.00. This indicates that the original watermark can still be preserved even if the adversary overwrites a new watermark into the model. Pruning. Pruning is an effective technology for model compression [62]. It is also considered a watermark removal attack since many neurons may be disabled which reduces the effectiveness of the watermark [40]. In this part, we leverage global and local unstructured pruning methods to the watermarked encoders. In the global pruning setting, we set r fraction of weights in the convolutional layers which have the smallest absolute values in all layers to 0. Compared to global pruning, i.e., putting together all the connections across different layers and comparing them, local pruning aims to prune a proportion of connections with the smallest absolute values in the same layer. We show the WR and DA in the first two sub-figures of Figure 10 and Figure 11, respectively. We observe that DA and WR drop a little as the ratio increases in global pruning. However, for local pruning, there is a larger downward trend in DA. For instance, DA is 0.954 when r = 0.1 and 0.871 when r = 0.5, this is because local pruning cannot preserve the global information in the model properly. In general, most of the WR are 1.0, which means SSLGuard is robust to different pruning settings. We also notice a special case here, i.e., on BYOL, when r = 0.4, the WR is 0.50. This is the worst case in our experiment, which demonstrates that we use watermark verification threshold th w = 0.5 in SSLGuard is reasonable. Also, note that for all clean encoders we evaluate in this paper, the WR is 0. This means the th w can be set to a smaller value to better verify the watermarked encoder as we discussed in Section 4.1. Fine-tuning. After pruning, the adversary can fine-tune the surrogate encoders under the victim encoder's supervision, which is following the setting in [28]. This process is also called fine-pruning [38]. The goal of fine-tuning is to regain DA's drop. We fine-tune all the weights of the pruned encoders (global and local) by the MSE loss function. We note that we freeze the BatchNorm layers of the pruned encoders due to reducing inaccurate batch statistics estimation caused by a small batchsize [57]. The WR are shown in Figure 10c and Figure 10d, and the DA are shown in Figure 11c and Figure 11d. We observe that fine-tuning can recover lost information from the victim encoder. For instance, when r = 0.3 in the local pruned model, DA on STL-10 is 0.917. After fine-tuning the pruned model, DA comes to 0.954. Meanwhile, WR increases as DA recovers. This means SSLGuard is robust to fine-tuning.\n\n\nModel Stealing.\n\nWe then quantify the robustness of SSLGuard through the lens of model stealing attacks. Note that we only consider the most powerful surrogate encoder's architectures and most effective query datasets. Concretely, based on the evaluation in Section 5.2, we consider ResNet-50 and ResNet-101 as the surrogate encoder's architectures and STL-10 as the query dataset. We name the three attacks Steal-1, Steal-2, and Steal-3. The details of each attack are shown in Table 8. The WR and DA for different attacks are shown in Table 9. We observe that although the model stealing attack is effective against the watermarked encoder, we can still verify the ownership of the surrogate model as the WR is also high. For instance, for Steal-2 against the watermarked encoder pretrained by BYOL, the DA is 0.937 and 0.815 on STL-10 and CIFAR-10, while the WR is 1.00, which indicates that the watermark injected by SSLGuard can still preserve in the surrogate encoder stolen by the adversary. We also have similar observations on Steal-1 and Steal-3, which demonstrate the robustness of SSLGuard under model stealing attacks.\n\n\nDiscussion\n\nThe Necessity of the Shadow Encoder. The reason why SSLGuard can extract watermarks from the surrogate encoder is that it locally simulates a model stealing process by using a shadow dataset and shadow encoder. In this part, we aim to demonstrate the need for such a design. We discard the shadow encoder and inject the watermark into a clean pre-trained encoder on SimCLR, MoCo v2, and BYOL. Then we get the corresponding key-tuples. The key-tuples can extract watermarks successfully. However, when We mount  Steal-1 to the watermarked encoders to generate three surrogate encoders (i.e., S simclr , S moco , and S byol ), the WR are all 0.00, which means the watermark may not be verified. Meanwhile, DA for S byol are 0.945, 0.735, 0.843, and 0.926 on STL-10, CIFAR-10, F-MNIST, and MNIST, respectively. This indicates that the adversary can successfully steal the victim encoder as the DA for the surrogate encoder are close to the target encoder. In conclusion, SSLGuard cannot work well without the shadow encoder as the adversary can steal a surrogate encoder with high utility while bypassing the watermark verification process. Therefore, the shadow encoder is crucial for defending against model stealing attacks.\n\nThe Choice of Mask. In our experiments, we set the covering space of the mask as 35%. We also leverage different masks M, i.e., 5% and 50% to inject watermark into BYOL, then we mount Steal-1 to the watermarked encoders, the WR are 0.99 and 1.00. The results show that the WR is similar when we leverage different covering spaces of the masks, which indicates that SSLGuard is effective under different masks.\n\nExtension to Other Types of Datasets. In this paper, we only focus on encoders pre-trained on image datasets. To extend SSLGuard into encoders pre-trained on other types of datasets such as texts or graphs [19,60], the main challenge is to define a suitable trigger pattern in the language or graph domain. Then we can apply a similar method to watermark those models. We leave it as our future work to further explore the effectiveness of SSLGuard on other domains such as texts or graphs.\n\n\nRelated Work\n\nPrivacy and Security for SSL. There have been more and more studies on the privacy and security of self-supervised learning. Jia et al. [29] sum up 10 security and privacy problems for SSL. Among them, only a small part has been studied. Liu et al. [37] study MIA against contrastive learningbased pre-train encoder. Concretely, Liu et al. [37] leverage data augmentations over the original samples to generate multiple augmented views. Then, the authors measure the similarities among the embeddings of the augmented samples. The intuition is that, if the sample is a member, then the similarities should be higher than a non-member. He and Zhang [26] perform the first privacy analysis of contrastive learning. Concretely, the authors observe that the contrastive models are less vulnerable to membership inference attacks, while more vulnerable to attribute inference attacks. The reason is that contrastive models are more generalized with less overfitting level, which leads to fewer membership inference risks, but the representations learned by contrastive learning are more informative, thus leaking more attribute informa-tion. Jia et al. [30] propose the first backdoor attack against SSL pre-trained encoders. By injecting the trigger pattern in the pre-training process of an encoder that correlated to a specific downstream task, the backdoored encoder can behave abnormally for this downstream task. The author further shows that triggers for multiple tasks can be simultaneously injected into the encoder.\n\nDNNs Copyright Protection. In recent years, several techniques for DNNs copyright protection have been proposed. Among them, DNNs watermarking is one of the most representative algorithms. Jia et al. [28] propose an entangled watermarking algorithm that encourages the classifiers to represent training data and watermarks similarly. The goal of the entanglement is to force the adversary to learn the knowledge of the watermarks when he steals the model. DNN fingerprinting is another protection method. Unlike watermarking, the goal of fingerprinting is to extract a specific property from the model. Cao et al. [8] introduce a fingerprinting extraction algorithm, namely IPGuard. IPGuard regards the data points near the classification boundary as the model's fingerprint. If a suspect classifier predicts the same labels for these points, then it will be judged as a surrogate classifier. Chen et al. [12] propose a testing framework for supervised learning models. They propose six metrics to measure whether a suspect model is a copy of the victim model. Among these metrics, four of them need white-box access, and black-box access is enough for the rest.\n\n\nConclusion\n\nIn this paper, we first quantify the copyright breaching threats of SSL pre-trained encoders through the lens of model stealing attacks. We empirically show that the SSL pre-trained encoders are highly vulnerable to model stealing attacks. This is because the rich information in the embeddings can be leveraged to better capture the behavior of the victim encoder. To protect the copyright of the SSL pre-trained encoder, we propose SSLGuard, a robust black-box watermarking scheme for the SSL pre-trained encoders. Concretely, given a secret vector, SSLGuard injects a watermark into a clean pre-trained encoder and outputs a watermarked version. The shadow training technique is also applied to preserve the watermark under potential model stealing attacks. Extensive evaluations show that SSLGuard is effective in embedding and extracting watermarks and robust against model stealing and different types of watermark removal attacks such as input noising, output perturbing, overwriting, model pruning, and fine-tuning.\n\nFigure 1 :\n1An illustration of deploying an SSL pre-trained encoder as a service. The legitimate user aims to train downstream classifiers while the adversary tries to generate a surrogate encoder.\n\n\"Figure 3 :\n3Update $ and of the key-tuple\" \"Improve utility & effectiveness of * \" The overview of SSLGuard.\n\nFigure 4 :Figure 5 :Figure 6 :\n456The performance of surrogate encoders trained with different architectures. The performance of surrogate encoders trained with different query datasets. The performance of surrogate encoders trained with different loss functions.\n\n\n. The evaluation shows that SSLGuard does not sacrifice the utility of the clean encoders.\n\nFigure 8 :Figure 9 :\n89The WR on different watermark removal attacks. The DA on different watermark removal attacks. The victim encoder is BYOL.\n\nFigure 10 :Figure 11 :\n1011The WR of pruned and fine-tuned encoders. The DA of pruned and fine-tuned encoders. The victim encoder is BYOL.\n\nTable 1 :\n1List of notations. Notation Description F, F * , F s Clean/Watermarked/Shadow encoder D t , D s Target/Shadow dataset D p , D v Private/Verification datasetT , M Trigger, Mask \n\u03ba, G Key-tuple, Decoder \nsk, sk x Secret vector, Decoded vector \nDA Downstream accuracy \nWR Watermark rate \n\n\n\nTable 2 :\n2Clean downstream accuracy (CDA).Downstream Task SimCLR MoCo v2 BYOL \n\nSTL-10 \n0.783 \n0.889 \n0.948 \nCIFAR-10 \n0.766 \n0.712 \n0.855 \nMNIST \n0.974 \n0.940 \n0.974 \nF-MNIST \n0.874 \n0.852 \n0.894 \n\n\nTable 3 :\n3Monetary Cost ($). Here Res denotes ResNet.Pre-training \nStealing \nRes-18 Res-34 Res-50 Res-101 \n\nSimCLR 1,920.00 \n58.24 61.10 66.67 74.50 \nMoCo v2 4,206.08 \n58.13 61.09 66.55 74.37 \nBYOL \n5,713.92 \n58.16 60.84 64.28 72.49 \n\n\n\nTable 4 :\n4Effectiveness.Encoder SimCLR MoCo v2 BYOL \n\nF \n0.00 \n0.00 \n0.00 \nF  *  \n1.00 \n1.00 \n1.00 \nF s \n1.00 \n1.00 \n1.00 \n\n\n\nTable 5 :\n5Fidelity (DA). The value in the parenthesis denotes the difference between CDA.Task \nF simclr \n\n *  \n\nF moco \n\n *  \n\n\n\nTable 6 :\n6Undetectability.Key-tuple F simclr \n\n *  \n\nF moco \n\n *  \n\nF \n\nbyol \n *  \n\n\u03ba simclr \n1.00 \n0.00 \n0.00 \n\u03ba moco \n0.00 \n1.00 \n0.00 \n\u03ba byol \n0.00 \n0.00 \n1.00 \n\nEfficiency. SSLGuard injects watermark into SimCLR, \nMoCo v2, and BYOL using 17.5hrs, 17.36hrs, and 10.70hrs, \nrespectively, which are only 29.17%, 8.19%, and 14.86% of \nthe time cost to pre-train SSL encoders, and the watermark \nextraction time is only 1.51s, 2.08s, and 1.82s, respectively. \n\n\nTable 7 :\n7Overwriting.SimCLR MoCo v2 BYOL \n\nDA \n\nSTL-10 \n0.785 \n0.888 \n0.954 \nCIFAR-10 \n0.765 \n0.685 \n0.863 \nMNIST \n0.962 \n0.955 \n0.977 \nF-MNIST \n0.885 \n0.837 \n0.905 \n\nWR \nOverwriting key \n1.00 \n1.00 \n0.98 \nOriginal key \n1.00 \n1.00 \n1.00 \n\n\n\nTable 8 :\n8Details of different model stealing attacks.Attacks \nQuery dataset Architecture Loss function \n\nSteal-1 \nSTL-10 \nResNet-50 \nCosine \nSteal-2 \nSTL-10 \nResNet-101 \nCosine \nSteal-3 \nSTL-10 (s) \nResNet-50 \nCosine \n\n\n\nTable 9 :\n9The DA and WR of model stealing attacks against the watermarked encoders.Attacks \nMetric \nSimCLR MoCo BYOL \n\nSteal-1 \nDA \n\nSTL-10 \n0.721 0.890 0.938 \nCIFAR-10 0.685 0.628 0.791 \nF-MNIST \n0.832 0.809 0.830 \nMNIST \n0.928 0.923 0.915 \n\nWR \n1.00 \n0.96 1.00 \n\nSteal-2 \nDA \n\nSTL-10 \n0.727 0.871 0.937 \nCIFAR-10 0.677 0.628 0.815 \nF-MNIST \n0.840 0.827 0.865 \nMNIST \n0.935 0.919 0.961 \n\nWR \n0.99 \n0.90 1.00 \n\nSteal-3 \nDA \n\nSTL-10 \n0.732 0.874 0.923 \nCIFAR-10 0.677 0.658 0.784 \nF-MNIST \n0.827 0.823 0.851 \nMNIST \n0.932 0.940 0.922 \n\nWR \n1.00 \n0.95 0.98 \n\n\nOur code is available at https://github.com/tianshuocong/ SSLGuard.\nhttps://cs.stanford.edu/~acoates/stl10/ 3 https://github.com/google-research/simclr\nhttps://github.com/facebookresearch/moco 5 https://github.com/yaox12/BYOL-PyTorch\nhttps://cloud.google.com/compute/gpus-pricing 7 https://aws.amazon.com/rekognition/pricing\nAcknowledgementWe thank all anonymous reviewers for their constructive comments. This work is partially funded by the Helmholtz Association within the project \"Trustworthy Fed-\nTurning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring. Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, Joseph Keshet, USENIX Security Symposium (USENIX Security). 24Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring. In USENIX Security Symposium (USENIX Security), pages 1615-1631. USENIX, 2018. 2, 4\n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Annual Conference on Neural Information Processing Systems (NeurIPS). NeurIPS. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordIlya Sutskever, and Dario Amodei. Language Models are Few-Shot LearnersTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert- Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In Annual Conference on Neural Information Processing Systems (NeurIPS). NeurIPS, 2020. 1\n\nT , Tony Cai, Jianqing Fan, and Tiefeng Jiang. Distributions of Angles in Random Packing on Spheres. Journal of Machine Learning Research. T. Tony Cai, Jianqing Fan, and Tiefeng Jiang. Distribu- tions of Angles in Random Packing on Spheres. Jour- nal of Machine Learning Research, 2013. 5\n\nProtecting Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Boundary. Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong, Ip-Guard, ACM Asia Conference on Computer and Communications Security (ASIACCS). ACM14Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. IP- Guard: Protecting Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Bound- ary. In ACM Asia Conference on Computer and Com- munications Security (ASIACCS), pages 14-25. ACM, 2021. 14\n\nAdversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods. Nicholas Carlini, David Wagner, CoRR abs/1705.07263Nicholas Carlini and David Wagner. Adversarial Exam- ples Are Not Easily Detected: Bypassing Ten Detection Methods. CoRR abs/1705.07263, 2017. 4\n\nModel Extraction and Active Learning. Kamalika Varun Chandrasekaran, Irene Chaudhuri, Somesh Giacomelli, Songbai Jha, Yan, CoRR abs/1811.02054Varun Chandrasekaran, Kamalika Chaudhuri, Irene Gi- acomelli, Somesh Jha, and Songbai Yan. Model Ex- traction and Active Learning. CoRR abs/1811.02054, 2018. 4\n\nExploring Connections Between Active Learning and Model Extraction. Kamalika Varun Chandrasekaran, Irene Chaudhuri, Somesh Giacomelli, Songbai Jha, Yan, 2020. 4USENIX Security Symposium (USENIX Security). Varun Chandrasekaran, Kamalika Chaudhuri, Irene Gi- acomelli, Somesh Jha, and Songbai Yan. Exploring Connections Between Active Learning and Model Ex- traction. In USENIX Security Symposium (USENIX Se- curity), pages 1309-1326. USENIX, 2020. 4\n\nCopy, Right? A Testing Framework for Copyright Protection of Deep Learning Models. Jialuo Chen, Jingyi Wang, Tinglan Peng, Youcheng Sun, Peng Cheng, Shouling Ji, Xingjun Ma, Bo Li, Dawn Song, 2022. 14IEEE Symposium on Security and Privacy (S&P). IEEEJialuo Chen, Jingyi Wang, Tinglan Peng, Youcheng Sun, Peng Cheng, Shouling Ji, Xingjun Ma, Bo Li, and Dawn Song. Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models. In IEEE Symposium on Security and Privacy (S&P). IEEE, 2022. 14\n\nA Simple Framework for Contrastive Learning of Visual Representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey E Hinton, PMLRInternational Conference on Machine Learning (ICML). 13Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A Simple Framework for Con- trastive Learning of Visual Representations. In Interna- tional Conference on Machine Learning (ICML), pages 1597-1607. PMLR, 2020. 1, 3\n\nBadNL: Backdoor Attacks Against NLP Models with Semantic-preserving Improvements. Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, Yang Zhang, Annual Computer Security Applications Conference (ACSAC). ACSACXiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. BadNL: Backdoor Attacks Against NLP Models with Semantic-preserving Improvements. In Annual Computer Security Applications Conference (ACSAC), pages 554-569. ACSAC, 2021. 4\n\nImproved Baselines with Momentum Contrastive Learning. Xinlei Chen, Haoqi Fan, Ross B Girshick, Kaiming He, CoRR abs/2003.04297, 2020. 3Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaim- ing He. Improved Baselines with Momentum Con- trastive Learning. CoRR abs/2003.04297, 2020. 3\n\nAn Analysis of Single-Layer Networks in Unsupervised Feature Learning. Adam Coates, Andrew Y Ng, Honglak Lee, International Conference on Artificial Intelligence and Statistics (AISTATS). Adam Coates, Andrew Y. Ng, and Honglak Lee. An Analysis of Single-Layer Networks in Unsupervised Feature Learning. In International Conference on Arti- ficial Intelligence and Statistics (AISTATS), pages 215- 223. JMLR, 2011. 7\n\nAdversarial Model Extraction on Graph Neural Networks. David Defazio, Arti Ramesh, CoRR abs/1912.07721David DeFazio and Arti Ramesh. Adversarial Model Extraction on Graph Neural Networks. CoRR abs/1912.07721, 2019. 4\n\nAnti-Distillation Backdoor Attacks: Backdoors Can Really Survive in Knowledge Distillation. Yunjie Ge, Qian Wang, Baolin Zheng, Xinlu Zhuang, Qi Li, Chao Shen, Cong Wang, ACM International Conference on Multimedia (MM). ACMYunjie Ge, Qian Wang, Baolin Zheng, Xinlu Zhuang, Qi Li, Chao Shen, and Cong Wang. Anti-Distillation Backdoor Attacks: Backdoors Can Really Survive in Knowledge Distillation. In ACM International Con- ference on Multimedia (MM), pages 826-834. ACM, 2021. 6\n\nDeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations. John M Giorgi, Osvald Nitski, Bo Wang, Gary D Bader, Annual Meeting of the Association for Computational Linguistics (ACL). John M. Giorgi, Osvald Nitski, Bo Wang, and Gary D. Bader. DeCLUTR: Deep Contrastive Learning for Un- supervised Textual Representations. In Annual Meet- ing of the Association for Computational Linguistics (ACL), pages 879-895. ACL, 2021. 13\n\nExplaining and Harnessing Adversarial Examples. Ian Goodfellow, Jonathon Shlens, Christian Szegedy, International Conference on Learning Representations (ICLR). Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Ex- amples. In International Conference on Learning Rep- resentations (ICLR), 2015. 4\n\nBootstrap Your Own Latent -A New Approach to Self-Supervised Learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Zhaohan Bernardo \u00c1vila Pires, Mohammad Gheshlaghi Guo, Bilal Azar, Koray Piot, R\u00e9mi Kavukcuoglu, Michal Munos, Valko, Annual Conference on Neural Information Processing Systems (NeurIPS). NeurIPS. 39Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo \u00c1vila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, R\u00e9mi Munos, and Michal Valko. Bootstrap Your Own Latent -A New Approach to Self-Supervised Learning. In Annual Conference on Neural Information Processing Systems (NeurIPS). NeurIPS, 2020. 3, 9\n\nMomentum Contrast for Unsupervised Visual Representation Learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross B Girshick, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE13Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum Contrast for Unsuper- vised Visual Representation Learning. In IEEE Con- ference on Computer Vision and Pattern Recognition (CVPR), pages 9726-9735. IEEE, 2020. 1, 3\n\nMembership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models. Xinlei He, Zheng Li, Weilin Xu, Cory Cornelius, Yang Zhang, CoRR abs/2208.10445, 2022. 4Xinlei He, Zheng Li, Weilin Xu, Cory Cornelius, and Yang Zhang. Membership-Doctor: Comprehensive As- sessment of Membership Inference Against Machine Learning Models. CoRR abs/2208.10445, 2022. 4\n\nSemi-Leak: Membership Inference Attacks Against Semi-supervised Learning. Xinlei He, Hongbin Liu, Neil Zhenqiang Gong, Yang Zhang, 2022. 4European Conference on Computer Vision (ECCV). SpringerXinlei He, Hongbin Liu, Neil Zhenqiang Gong, and Yang Zhang. Semi-Leak: Membership Inference At- tacks Against Semi-supervised Learning. In European Conference on Computer Vision (ECCV). Springer, 2022. 4\n\nNode-Level Membership Inference Attacks Against Graph Neural Networks. Xinlei He, Rui Wen, Yixin Wu, Michael Backes, Yun Shen, Yang Zhang, CoRR abs/2102.05429, 2021. 4Xinlei He, Rui Wen, Yixin Wu, Michael Backes, Yun Shen, and Yang Zhang. Node-Level Membership Infer- ence Attacks Against Graph Neural Networks. CoRR abs/2102.05429, 2021. 4\n\nQuantifying and Mitigating Privacy Risks of Contrastive Learning. Xinlei He, Yang Zhang, ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM413Xinlei He and Yang Zhang. Quantifying and Mitigat- ing Privacy Risks of Contrastive Learning. In ACM SIGSAC Conference on Computer and Communica- tions Security (CCS), pages 845-863. ACM, 2021. 4, 13\n\nHigh Accuracy and High Fidelity Extraction of Neural Networks. Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, Nicolas Papernot, 2020. 4USENIX Security Symposium (USENIX Security). Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot. High Accu- racy and High Fidelity Extraction of Neural Networks. In USENIX Security Symposium (USENIX Security), pages 1345-1362. USENIX, 2020. 4\n\nEntangled Watermarks as a Defense against Model Extraction. Hengrui Jia, Christopher A Choquette-Choo, Varun Chandrasekaran, Nicolas Papernot, USENIX Security Symposium (USENIX Security). 1214Hengrui Jia, Christopher A. Choquette-Choo, Varun Chandrasekaran, and Nicolas Papernot. Entangled Watermarks as a Defense against Model Extraction. In USENIX Security Symposium (USENIX Security), pages 1937-1954. USENIX, 2021. 2, 4, 12, 14\n\n10 Security and Privacy Problems in Self-Supervised Learning. Jinyuan Jia, Hongbin Liu, Neil Zhenqiang Gong, CoRR abs/2110.15444Jinyuan Jia, Hongbin Liu, and Neil Zhenqiang Gong. 10 Security and Privacy Problems in Self-Supervised Learning. CoRR abs/2110.15444, 2021. 13\n\nBadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. Jinyuan Jia, Yupei Liu, Neil Zhenqiang Gong, IEEE Symposium on Security and Privacy (S&P). 814Jinyuan Jia, Yupei Liu, and Neil Zhenqiang Gong. BadEncoder: Backdoor Attacks to Pre-trained En- coders in Self-Supervised Learning. In IEEE Sympo- sium on Security and Privacy (S&P). IEEE, 2022. 2, 4, 8, 14\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, International Conference on Learning Representations (ICLR). Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International Confer- ence on Learning Representations (ICLR), 2015. 8\n\nThieves on Sesame Street! Model Extraction of BERT-based APIs. Kalpesh Krishna, Gaurav Singh Tomar, Ankur P Parikh, Nicolas Papernot, Mohit Iyyer, International Conference on Learning Representations (ICLR), 2020. 24Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot, and Mohit Iyyer. Thieves on Sesame Street! Model Extraction of BERT-based APIs. In International Conference on Learning Representa- tions (ICLR), 2020. 2, 4\n\nAdversarial Examples in the Physical World. Alexey Kurakin, Ian Goodfellow, Samy Bengio, CoRR abs/1607.02533Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial Examples in the Physical World. CoRR abs/1607.02533, 2016. 4\n\nHow to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellectual Property of DNN. Zheng Li, Chengyu Hu, Yang Zhang, Shanqing Guo, Annual Computer Security Applications Conference (ACSAC). ACM24Zheng Li, Chengyu Hu, Yang Zhang, and Shanqing Guo. How to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellec- tual Property of DNN. In Annual Computer Secu- rity Applications Conference (ACSAC), pages 126-137. ACM, 2019. 2, 4\n\nAuditing Membership Leakages of Multi-Exit Networks. Zheng Li, Yiyong Liu, Xinlei He, Ning Yu, Michael Backes, Yang Zhang, CoRR abs/2208.11180, 2022. 4Zheng Li, Yiyong Liu, Xinlei He, Ning Yu, Michael Backes, and Yang Zhang. Auditing Membership Leak- ages of Multi-Exit Networks. CoRR abs/2208.11180, 2022. 4\n\nMembership Leakage in Label-Only Exposures. Zheng Li, Yang Zhang, 2021. 4ACM SIGSAC Conference on Computer and Communications Security (CCS). ACMZheng Li and Yang Zhang. Membership Leakage in Label-Only Exposures. In ACM SIGSAC Conference on Computer and Communications Security (CCS), pages 880-895. ACM, 2021. 4\n\nEncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning. Hongbin Liu, Jinyuan Jia, Wenjie Qu, Neil Zhenqiang Gong, ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM13Hongbin Liu, Jinyuan Jia, Wenjie Qu, and Neil Zhen- qiang Gong. EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning. In ACM SIGSAC Conference on Computer and Com- munications Security (CCS). ACM, 2021. 2, 4, 13\n\nFine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks. Kang Liu, Brendan Dolan-Gavitt, Siddharth Garg, Research in Attacks, Intrusions, and Defenses (RAID). Springer12Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-Pruning: Defending Against Backdooring At- tacks on Deep Neural Networks. In Research in At- tacks, Intrusions, and Defenses (RAID), pages 273- 294. Springer, 2018. 12\n\nDelving into Transferable Adversarial Examples and Black-box Attacks. Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song, CoRR abs/1611.02770Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into Transferable Adversarial Examples and Black-box Attacks. CoRR abs/1611.02770, 2016. 4\n\nSoK: How Robust is Image Classification Deep Neural Network Watermarking?. Nils Lukas, Edward Jiang, Xinda Li, Florian Kerschbaum, IEEE Symposium on Security and Privacy (S&P). 212Nils Lukas, Edward Jiang, Xinda Li, and Florian Ker- schbaum. SoK: How Robust is Image Classification Deep Neural Network Watermarking? In IEEE Sym- posium on Security and Privacy (S&P). IEEE, 2022. 2, 12\n\nAdversarial Frontier Stitching for Remote Neural Network Watermarking. Patrick Erwan Le Merrer, Gilles Perez, Tr\u00e9dan, CoRR abs/1711.01894Erwan Le Merrer, Patrick Perez, and Gilles Tr\u00e9dan. Ad- versarial Frontier Stitching for Remote Neural Network Watermarking. CoRR abs/1711.01894, 2017. 4\n\nKnockoff Nets: Stealing Functionality of Black-Box Models. Tribhuvanesh Orekondy, Bernt Schiele, Mario Fritz, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 24Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff Nets: Stealing Functionality of Black- Box Models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4954-4963. IEEE, 2019. 2, 4\n\nPractical Black-Box Attacks Against Machine Learning. Nicolas Papernot, Patrick D Mcdaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, Ananthram Swami, ACM Asia Conference on Computer and Communications Security (ASIACCS). ACM24Nicolas Papernot, Patrick D. McDaniel, Ian Goodfel- low, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical Black-Box Attacks Against Machine Learning. In ACM Asia Conference on Computer and Communications Security (ASIACCS), pages 506-519. ACM, 2017. 2, 4\n\nLearning Transferable Visual Models From Natural Language Supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, PMLR, 2021. 3International Conference on Machine Learning (ICML). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas- try, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Trans- ferable Visual Models From Natural Language Super- vision. In International Conference on Machine Learn- ing (ICML), pages 8748-8763. PMLR, 2021. 3\n\nDeepSigns: A Generic Watermarking Framework for IP Protection of Deep Learning Models. Huili Bita Darvish Rouhani, Farinaz Chen, Koushanfar, CoRR abs/1804.00750Bita Darvish Rouhani, Huili Chen, and Farinaz Koushanfar. DeepSigns: A Generic Watermarking Framework for IP Protection of Deep Learning Mod- els. CoRR abs/1804.00750, 2018. 4\n\nImageNet Large Scale Visual Recognition Challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, Li Fei-Fei, CoRR abs/1409.057517Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan- der C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. CoRR abs/1409.0575, 2015. 1, 7\n\nHidden Trigger Backdoor Attacks. Aniruddha Saha, Akshayvarun Subramanya, Hamed Pirsiavash, AAAI Conference on Artificial Intelligence (AAAI). AAAIAniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden Trigger Backdoor Attacks. In AAAI Conference on Artificial Intelligence (AAAI), pages 11957-11965. AAAI, 2020. 4\n\nML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models. Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, Michael Backes, Network and Distributed System Security Symposium (NDSS). Internet Society. 24Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes. ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models. In Network and Distributed System Security Symposium (NDSS). Internet Society, 2019. 2, 4\n\nModel Stealing Attacks Against Inductive Graph Neural Networks. Yun Shen, Xinlei He, Yufei Han, Yang Zhang, IEEE Symposium on Security and Privacy (S&P). IEEE, 2022. 24Yun Shen, Xinlei He, Yufei Han, and Yang Zhang. Model Stealing Attacks Against Inductive Graph Neu- ral Networks. In IEEE Symposium on Security and Pri- vacy (S&P). IEEE, 2022. 2, 4\n\nMembership Inference Attacks Against Machine Learning Models. Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov, IEEE Symposium on Security and Privacy (S&P). 24Reza Shokri, Marco Stronati, Congzheng Song, and Vi- taly Shmatikov. Membership Inference Attacks Against Machine Learning Models. In IEEE Symposium on Se- curity and Privacy (S&P), pages 3-18. IEEE, 2017. 2, 4\n\nSystematic Evaluation of Privacy Risks of Machine Learning Models. Liwei Song, Prateek Mittal, 2021. 4USENIX Security Symposium (USENIX Security). USENIX. Liwei Song and Prateek Mittal. Systematic Evalua- tion of Privacy Risks of Machine Learning Models. In USENIX Security Symposium (USENIX Security). USENIX, 2021. 4\n\nThe German Traffic Sign Recognition Benchmark: A Multi-class Classification Competition. Johannes Stallkamp, Marc Schlipsing, Jan Salmen, Christian Igel, International Joint Conference on Neural Networks (IJCNN). IEEEJohannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The German Traffic Sign Recognition Benchmark: A Multi-class Classification Competition. In International Joint Conference on Neural Networks (IJCNN), pages 1453-1460. IEEE, 2011. 7\n\nStealing Machine Learning Models via Prediction APIs. Florian Tram\u00e8r, Fan Zhang, Ari Juels, Michael K Reiter, Thomas Ristenpart, USENIX Security Symposium (USENIX Security). 24Florian Tram\u00e8r, Fan Zhang, Ari Juels, Michael K. Re- iter, and Thomas Ristenpart. Stealing Machine Learn- ing Models via Prediction APIs. In USENIX Secu- rity Symposium (USENIX Security), pages 601-618. USENIX, 2016. 2, 4\n\nEmbedding Watermarks into Deep Neural Networks. Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, Shin&apos;ichi Satoh, International Conference on Multimedia Retrieval (ICMR). ACMYusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin'ichi Satoh. Embedding Watermarks into Deep Neural Networks. In International Conference on Mul- timedia Retrieval (ICMR), pages 269-277. ACM, 2017. 4\n\nVisualizing Data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of Machine Learning Research. 10Laurens van der Maaten and Geoffrey Hinton. Visual- izing Data using t-SNE. Journal of Machine Learning Research, 2008. 10\n\nModel Extraction Attacks on Graph Neural Networks: Taxonomy and Realization. Bang Wu, Xiangwen Yang, Shirui Pan, Xingliang Yuan, CoRR abs/2010.12751, 2020. 4Bang Wu, Xiangwen Yang, Shirui Pan, and Xingliang Yuan. Model Extraction Attacks on Graph Neu- ral Networks: Taxonomy and Realization. CoRR abs/2010.12751, 2020. 4\n\nGroup Normalization. Yuxin Wu, Kaiming He, European Conference on Computer Vision (ECCV). Springer12Yuxin Wu and Kaiming He. Group Normalization. In European Conference on Computer Vision (ECCV), pages 3-19. Springer, 2018. 12\n\nFashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. Han Xiao, Kashif Rasul, Roland Vollgraf, CoRR abs/1708.07747Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion- MNIST: a Novel Image Dataset for Benchmarking Ma- chine Learning Algorithms. CoRR abs/1708.07747, 2017. 7\n\nLatent Backdoor Attacks on Deep Neural Networks. Yuanshun Yao, Huiying Li, Haitao Zheng, Ben Y Zhao, ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM46Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. Latent Backdoor Attacks on Deep Neural Net- works. In ACM SIGSAC Conference on Computer and Communications Security (CCS), pages 2041-2055. ACM, 2019. 4, 6\n\nGraph Contrastive Learning with Augmentations. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, Yang Shen, Annual Conference on Neural Information Processing Systems (NeurIPS). NeurIPS. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph Contrastive Learning with Augmentations. In Annual Conference on Neural Information Processing Systems (NeurIPS). NeurIPS, 2020. 13\n\nProtecting Intellectual Property of Deep Neural Networks with Watermarking. Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing Huang, Ian Molloy, ACM Asia Conference on Computer and Communications Security (ASIACCS). ACMJialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph. Stoecklin, Heqing Huang, and Ian Molloy. Protecting Intellectual Property of Deep Neural Net- works with Watermarking. In ACM Asia Conference on Computer and Communications Security (ASIACCS), pages 159-172. ACM, 2018. 2\n\nTo Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression. Michael Zhu, Suyog Gupta, International Conference on Learning Representations (ICLR). 12Michael Zhu and Suyog Gupta. To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression. In International Conference on Learn- ing Representations (ICLR), 2018. 12\n", "annotations": {"author": "[{\"end\":169,\"start\":98},{\"end\":230,\"start\":170},{\"end\":292,\"start\":231}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":107},{\"end\":179,\"start\":177},{\"end\":241,\"start\":236}]", "author_first_name": "[{\"end\":106,\"start\":98},{\"end\":176,\"start\":170},{\"end\":235,\"start\":231}]", "author_affiliation": "[{\"end\":168,\"start\":113},{\"end\":229,\"start\":181},{\"end\":291,\"start\":243}]", "title": "[{\"end\":82,\"start\":1},{\"end\":374,\"start\":293}]", "venue": "[{\"end\":442,\"start\":376}]", "abstract": "[{\"end\":2009,\"start\":457}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2666,\"start\":2662},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2684,\"start\":2680},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2913,\"start\":2909},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3397,\"start\":3393},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3667,\"start\":3663},{\"end\":3962,\"start\":3959},{\"end\":3964,\"start\":3962},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4087,\"start\":4084},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4568,\"start\":4564},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4571,\"start\":4568},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4574,\"start\":4571},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4577,\"start\":4574},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4838,\"start\":4834},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4841,\"start\":4838},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4844,\"start\":4841},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4921,\"start\":4917},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4994,\"start\":4990},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5203,\"start\":5199},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5206,\"start\":5203},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5459,\"start\":5456},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5462,\"start\":5459},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5465,\"start\":5462},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8622,\"start\":8618},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10614,\"start\":10610},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10635,\"start\":10631},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10727,\"start\":10723},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10730,\"start\":10727},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10733,\"start\":10730},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10736,\"start\":10733},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10739,\"start\":10736},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10968,\"start\":10964},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10982,\"start\":10978},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10997,\"start\":10993},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11011,\"start\":11007},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12817,\"start\":12813},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12848,\"start\":12844},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14603,\"start\":14599},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16240,\"start\":16236},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16243,\"start\":16240},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16246,\"start\":16243},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16249,\"start\":16246},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16252,\"start\":16249},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":16255,\"start\":16252},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16258,\"start\":16255},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":16261,\"start\":16258},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":16264,\"start\":16261},{\"end\":17140,\"start\":17114},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17163,\"start\":17159},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17166,\"start\":17163},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17169,\"start\":17166},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":17172,\"start\":17169},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17200,\"start\":17197},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17203,\"start\":17200},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17206,\"start\":17203},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17209,\"start\":17206},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17212,\"start\":17209},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17796,\"start\":17793},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":17799,\"start\":17796},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17802,\"start\":17799},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":17805,\"start\":17802},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17808,\"start\":17805},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18133,\"start\":18129},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20208,\"start\":20204},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20211,\"start\":20208},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20214,\"start\":20211},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20217,\"start\":20214},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":23132,\"start\":23129},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27541,\"start\":27537},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":27544,\"start\":27541},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31681,\"start\":31677},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32207,\"start\":32203},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":32488,\"start\":32484},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":32823,\"start\":32819},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34210,\"start\":34206},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":34426,\"start\":34422},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":40618,\"start\":40614},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":41520,\"start\":41519},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":41940,\"start\":41938},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":43349,\"start\":43345},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":49459,\"start\":49455},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":49598,\"start\":49594},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":51153,\"start\":51149},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":51200,\"start\":51196},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":51501,\"start\":51497},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":54896,\"start\":54892},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":54899,\"start\":54896},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":55333,\"start\":55329},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":55446,\"start\":55442},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":55537,\"start\":55533},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":55845,\"start\":55841},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":56345,\"start\":56341},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":56919,\"start\":56915},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":57332,\"start\":57329},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":57624,\"start\":57620}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":59114,\"start\":58916},{\"attributes\":{\"id\":\"fig_1\"},\"end\":59225,\"start\":59115},{\"attributes\":{\"id\":\"fig_2\"},\"end\":59490,\"start\":59226},{\"attributes\":{\"id\":\"fig_3\"},\"end\":59583,\"start\":59491},{\"attributes\":{\"id\":\"fig_4\"},\"end\":59729,\"start\":59584},{\"attributes\":{\"id\":\"fig_5\"},\"end\":59869,\"start\":59730},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":60168,\"start\":59870},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":60369,\"start\":60169},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":60607,\"start\":60370},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":60734,\"start\":60608},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":60864,\"start\":60735},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":61326,\"start\":60865},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":61569,\"start\":61327},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":61792,\"start\":61570},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":62352,\"start\":61793}]", "paragraph": "[{\"end\":3195,\"start\":2025},{\"end\":4248,\"start\":3197},{\"end\":5104,\"start\":4250},{\"end\":6050,\"start\":5106},{\"end\":6356,\"start\":6052},{\"end\":7743,\"start\":6358},{\"end\":9003,\"start\":7745},{\"end\":9645,\"start\":9005},{\"end\":9695,\"start\":9647},{\"end\":9791,\"start\":9697},{\"end\":9946,\"start\":9793},{\"end\":10206,\"start\":9948},{\"end\":10998,\"start\":10208},{\"end\":11197,\"start\":11000},{\"end\":11603,\"start\":11199},{\"end\":11903,\"start\":11605},{\"end\":12274,\"start\":11919},{\"end\":12544,\"start\":12353},{\"end\":13542,\"start\":12602},{\"end\":14156,\"start\":13544},{\"end\":14459,\"start\":14214},{\"end\":15151,\"start\":14486},{\"end\":15556,\"start\":15153},{\"end\":15762,\"start\":15612},{\"end\":15851,\"start\":15789},{\"end\":16186,\"start\":15908},{\"end\":16678,\"start\":16213},{\"end\":17413,\"start\":16727},{\"end\":18308,\"start\":17435},{\"end\":18540,\"start\":18310},{\"end\":18994,\"start\":18557},{\"end\":20004,\"start\":18996},{\"end\":21050,\"start\":20006},{\"end\":21433,\"start\":21052},{\"end\":21632,\"start\":21467},{\"end\":22055,\"start\":21634},{\"end\":22165,\"start\":22057},{\"end\":22376,\"start\":22167},{\"end\":22564,\"start\":22378},{\"end\":22763,\"start\":22566},{\"end\":22958,\"start\":22765},{\"end\":23093,\"start\":22960},{\"end\":23403,\"start\":23106},{\"end\":24674,\"start\":23468},{\"end\":24883,\"start\":24676},{\"end\":25271,\"start\":24926},{\"end\":25442,\"start\":25295},{\"end\":25855,\"start\":25444},{\"end\":26082,\"start\":25916},{\"end\":26281,\"start\":26124},{\"end\":27129,\"start\":26331},{\"end\":27373,\"start\":27251},{\"end\":27580,\"start\":27375},{\"end\":27750,\"start\":27631},{\"end\":28122,\"start\":27796},{\"end\":28723,\"start\":28179},{\"end\":28896,\"start\":28778},{\"end\":29072,\"start\":28954},{\"end\":29628,\"start\":29096},{\"end\":29904,\"start\":29666},{\"end\":30199,\"start\":29979},{\"end\":30368,\"start\":30278},{\"end\":30639,\"start\":30392},{\"end\":30862,\"start\":30674},{\"end\":30973,\"start\":30918},{\"end\":31152,\"start\":30996},{\"end\":31559,\"start\":31279},{\"end\":31664,\"start\":31595},{\"end\":31805,\"start\":31666},{\"end\":31997,\"start\":31807},{\"end\":32192,\"start\":31999},{\"end\":32474,\"start\":32194},{\"end\":32639,\"start\":32476},{\"end\":32802,\"start\":32641},{\"end\":33018,\"start\":32804},{\"end\":33599,\"start\":33020},{\"end\":35361,\"start\":33601},{\"end\":36353,\"start\":35391},{\"end\":37071,\"start\":36380},{\"end\":39882,\"start\":37073},{\"end\":40702,\"start\":39884},{\"end\":42026,\"start\":40704},{\"end\":42345,\"start\":42039},{\"end\":42935,\"start\":42347},{\"end\":44945,\"start\":42937},{\"end\":45873,\"start\":44983},{\"end\":46608,\"start\":45898},{\"end\":46814,\"start\":46631},{\"end\":47628,\"start\":46816},{\"end\":48270,\"start\":47630},{\"end\":48545,\"start\":48294},{\"end\":51900,\"start\":48547},{\"end\":53034,\"start\":51920},{\"end\":54273,\"start\":53049},{\"end\":54684,\"start\":54275},{\"end\":55176,\"start\":54686},{\"end\":56713,\"start\":55193},{\"end\":57877,\"start\":56715},{\"end\":58915,\"start\":57892}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11918,\"start\":11904},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12352,\"start\":12275},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12601,\"start\":12545},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14213,\"start\":14157},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14485,\"start\":14460},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15611,\"start\":15557},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15788,\"start\":15763},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15907,\"start\":15852},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16726,\"start\":16679},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23467,\"start\":23404},{\"attributes\":{\"id\":\"formula_10\"},\"end\":24925,\"start\":24884},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25294,\"start\":25272},{\"attributes\":{\"id\":\"formula_12\"},\"end\":25915,\"start\":25856},{\"attributes\":{\"id\":\"formula_13\"},\"end\":26123,\"start\":26083},{\"attributes\":{\"id\":\"formula_14\"},\"end\":26330,\"start\":26282},{\"attributes\":{\"id\":\"formula_15\"},\"end\":27236,\"start\":27130},{\"attributes\":{\"id\":\"formula_16\"},\"end\":27630,\"start\":27581},{\"attributes\":{\"id\":\"formula_17\"},\"end\":27795,\"start\":27751},{\"attributes\":{\"id\":\"formula_18\"},\"end\":28178,\"start\":28123},{\"attributes\":{\"id\":\"formula_19\"},\"end\":28777,\"start\":28724},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28953,\"start\":28897},{\"attributes\":{\"id\":\"formula_21\"},\"end\":29665,\"start\":29629},{\"attributes\":{\"id\":\"formula_22\"},\"end\":29978,\"start\":29905},{\"attributes\":{\"id\":\"formula_23\"},\"end\":30277,\"start\":30200},{\"attributes\":{\"id\":\"formula_24\"},\"end\":30391,\"start\":30369},{\"attributes\":{\"id\":\"formula_25\"},\"end\":30673,\"start\":30640},{\"attributes\":{\"id\":\"formula_26\"},\"end\":30917,\"start\":30863},{\"attributes\":{\"id\":\"formula_27\"},\"end\":30995,\"start\":30974},{\"attributes\":{\"id\":\"formula_28\"},\"end\":31278,\"start\":31153},{\"attributes\":{\"id\":\"formula_29\"},\"end\":44969,\"start\":44946}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":7564,\"start\":7557},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":19841,\"start\":19834},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":23048,\"start\":23041},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":35724,\"start\":35717},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":36116,\"start\":36109},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":41697,\"start\":41690},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":42790,\"start\":42783},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":43818,\"start\":43811},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":44580,\"start\":44573},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":49036,\"start\":49029},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":52389,\"start\":52382},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":52447,\"start\":52440}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2023,\"start\":2011},{\"attributes\":{\"n\":\"2.2\"},\"end\":16211,\"start\":16189},{\"attributes\":{\"n\":\"2.3\"},\"end\":17433,\"start\":17416},{\"attributes\":{\"n\":\"3\"},\"end\":18555,\"start\":18543},{\"attributes\":{\"n\":\"4\"},\"end\":21465,\"start\":21436},{\"attributes\":{\"n\":\"4.1\"},\"end\":23104,\"start\":23096},{\"attributes\":{\"n\":\"4.2\"},\"end\":27249,\"start\":27238},{\"attributes\":{\"n\":\"4.3\"},\"end\":29094,\"start\":29075},{\"attributes\":{\"n\":\"5\"},\"end\":31572,\"start\":31562},{\"attributes\":{\"n\":\"5.1\"},\"end\":31593,\"start\":31575},{\"attributes\":{\"n\":\"5.2\"},\"end\":35389,\"start\":35364},{\"attributes\":{\"n\":\"5.3\"},\"end\":36378,\"start\":36356},{\"attributes\":{\"n\":\"5.4\"},\"end\":42037,\"start\":42029},{\"attributes\":{\"n\":\"5.5\"},\"end\":44981,\"start\":44971},{\"attributes\":{\"n\":\"5.5.1\"},\"end\":45896,\"start\":45876},{\"attributes\":{\"n\":\"5.5.2\"},\"end\":46629,\"start\":46611},{\"attributes\":{\"n\":\"5.5.3\"},\"end\":48292,\"start\":48273},{\"attributes\":{\"n\":\"5.5.4\"},\"end\":51918,\"start\":51903},{\"attributes\":{\"n\":\"6\"},\"end\":53047,\"start\":53037},{\"attributes\":{\"n\":\"7\"},\"end\":55191,\"start\":55179},{\"attributes\":{\"n\":\"8\"},\"end\":57890,\"start\":57880},{\"end\":58927,\"start\":58917},{\"end\":59127,\"start\":59116},{\"end\":59257,\"start\":59227},{\"end\":59605,\"start\":59585},{\"end\":59753,\"start\":59731},{\"end\":59880,\"start\":59871},{\"end\":60179,\"start\":60170},{\"end\":60380,\"start\":60371},{\"end\":60618,\"start\":60609},{\"end\":60745,\"start\":60736},{\"end\":60875,\"start\":60866},{\"end\":61337,\"start\":61328},{\"end\":61580,\"start\":61571},{\"end\":61803,\"start\":61794}]", "table": "[{\"end\":60168,\"start\":60038},{\"end\":60369,\"start\":60213},{\"end\":60607,\"start\":60425},{\"end\":60734,\"start\":60634},{\"end\":60864,\"start\":60826},{\"end\":61326,\"start\":60893},{\"end\":61569,\"start\":61351},{\"end\":61792,\"start\":61626},{\"end\":62352,\"start\":61878}]", "figure_caption": "[{\"end\":59114,\"start\":58929},{\"end\":59225,\"start\":59129},{\"end\":59490,\"start\":59261},{\"end\":59583,\"start\":59493},{\"end\":59729,\"start\":59608},{\"end\":59869,\"start\":59758},{\"end\":60038,\"start\":59882},{\"end\":60213,\"start\":60181},{\"end\":60425,\"start\":60382},{\"end\":60634,\"start\":60620},{\"end\":60826,\"start\":60747},{\"end\":60893,\"start\":60877},{\"end\":61351,\"start\":61339},{\"end\":61626,\"start\":61582},{\"end\":61878,\"start\":61805}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4416,\"start\":4408},{\"end\":21754,\"start\":21746},{\"end\":24721,\"start\":24713},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26674,\"start\":26666},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":29116,\"start\":29108},{\"end\":37250,\"start\":37242},{\"end\":37424,\"start\":37413},{\"end\":38680,\"start\":38672},{\"end\":38890,\"start\":38880},{\"end\":39386,\"start\":39376},{\"end\":40304,\"start\":40296},{\"end\":40449,\"start\":40438},{\"end\":43380,\"start\":43372},{\"end\":44048,\"start\":44040},{\"end\":46125,\"start\":46116},{\"end\":46155,\"start\":46146},{\"end\":47123,\"start\":47114},{\"end\":47345,\"start\":47335},{\"end\":47925,\"start\":47916},{\"end\":47939,\"start\":47930},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50136,\"start\":50127},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50150,\"start\":50141},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51533,\"start\":51523},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51548,\"start\":51538},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51584,\"start\":51574},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51599,\"start\":51589}]", "bib_author_first_name": "[{\"end\":62949,\"start\":62944},{\"end\":62962,\"start\":62955},{\"end\":62978,\"start\":62969},{\"end\":62991,\"start\":62986},{\"end\":63006,\"start\":63000},{\"end\":63314,\"start\":63311},{\"end\":63316,\"start\":63315},{\"end\":63332,\"start\":63324},{\"end\":63343,\"start\":63339},{\"end\":63358,\"start\":63351},{\"end\":63373,\"start\":63368},{\"end\":63390,\"start\":63382},{\"end\":63407,\"start\":63401},{\"end\":63427,\"start\":63421},{\"end\":63441,\"start\":63435},{\"end\":63456,\"start\":63450},{\"end\":63473,\"start\":63465},{\"end\":63488,\"start\":63483},{\"end\":63511,\"start\":63503},{\"end\":63524,\"start\":63521},{\"end\":63540,\"start\":63535},{\"end\":63554,\"start\":63548},{\"end\":63569,\"start\":63563},{\"end\":63571,\"start\":63570},{\"end\":63588,\"start\":63581},{\"end\":63600,\"start\":63593},{\"end\":63620,\"start\":63609},{\"end\":63632,\"start\":63628},{\"end\":63643,\"start\":63639},{\"end\":63659,\"start\":63652},{\"end\":64521,\"start\":64520},{\"end\":64528,\"start\":64524},{\"end\":64921,\"start\":64915},{\"end\":64934,\"start\":64927},{\"end\":64944,\"start\":64940},{\"end\":64954,\"start\":64945},{\"end\":65406,\"start\":65398},{\"end\":65421,\"start\":65416},{\"end\":65641,\"start\":65633},{\"end\":65669,\"start\":65664},{\"end\":65687,\"start\":65681},{\"end\":65707,\"start\":65700},{\"end\":65974,\"start\":65966},{\"end\":66002,\"start\":65997},{\"end\":66020,\"start\":66014},{\"end\":66040,\"start\":66033},{\"end\":66437,\"start\":66431},{\"end\":66450,\"start\":66444},{\"end\":66464,\"start\":66457},{\"end\":66479,\"start\":66471},{\"end\":66489,\"start\":66485},{\"end\":66505,\"start\":66497},{\"end\":66517,\"start\":66510},{\"end\":66524,\"start\":66522},{\"end\":66533,\"start\":66529},{\"end\":66934,\"start\":66930},{\"end\":66946,\"start\":66941},{\"end\":66966,\"start\":66958},{\"end\":66984,\"start\":66976},{\"end\":66986,\"start\":66985},{\"end\":67378,\"start\":67372},{\"end\":67390,\"start\":67385},{\"end\":67405,\"start\":67398},{\"end\":67421,\"start\":67414},{\"end\":67432,\"start\":67426},{\"end\":67447,\"start\":67439},{\"end\":67456,\"start\":67452},{\"end\":67858,\"start\":67852},{\"end\":67870,\"start\":67865},{\"end\":67880,\"start\":67876},{\"end\":67882,\"start\":67881},{\"end\":67900,\"start\":67893},{\"end\":68155,\"start\":68151},{\"end\":68170,\"start\":68164},{\"end\":68172,\"start\":68171},{\"end\":68184,\"start\":68177},{\"end\":68557,\"start\":68552},{\"end\":68571,\"start\":68567},{\"end\":68813,\"start\":68807},{\"end\":68822,\"start\":68818},{\"end\":68835,\"start\":68829},{\"end\":68848,\"start\":68843},{\"end\":68859,\"start\":68857},{\"end\":68868,\"start\":68864},{\"end\":68879,\"start\":68875},{\"end\":69277,\"start\":69273},{\"end\":69279,\"start\":69278},{\"end\":69294,\"start\":69288},{\"end\":69305,\"start\":69303},{\"end\":69316,\"start\":69312},{\"end\":69318,\"start\":69317},{\"end\":69692,\"start\":69689},{\"end\":69713,\"start\":69705},{\"end\":69731,\"start\":69722},{\"end\":70066,\"start\":70054},{\"end\":70081,\"start\":70074},{\"end\":70096,\"start\":70089},{\"end\":70113,\"start\":70105},{\"end\":70128,\"start\":70122},{\"end\":70130,\"start\":70129},{\"end\":70147,\"start\":70142},{\"end\":70165,\"start\":70161},{\"end\":70182,\"start\":70175},{\"end\":70213,\"start\":70205},{\"end\":70224,\"start\":70214},{\"end\":70235,\"start\":70230},{\"end\":70247,\"start\":70242},{\"end\":70258,\"start\":70254},{\"end\":70278,\"start\":70272},{\"end\":70857,\"start\":70850},{\"end\":70867,\"start\":70862},{\"end\":70878,\"start\":70873},{\"end\":70890,\"start\":70883},{\"end\":70900,\"start\":70896},{\"end\":70902,\"start\":70901},{\"end\":71337,\"start\":71331},{\"end\":71347,\"start\":71342},{\"end\":71358,\"start\":71352},{\"end\":71367,\"start\":71363},{\"end\":71383,\"start\":71379},{\"end\":71696,\"start\":71690},{\"end\":71708,\"start\":71701},{\"end\":71718,\"start\":71714},{\"end\":71728,\"start\":71719},{\"end\":71739,\"start\":71735},{\"end\":72092,\"start\":72086},{\"end\":72100,\"start\":72097},{\"end\":72111,\"start\":72106},{\"end\":72123,\"start\":72116},{\"end\":72135,\"start\":72132},{\"end\":72146,\"start\":72142},{\"end\":72429,\"start\":72423},{\"end\":72438,\"start\":72434},{\"end\":72792,\"start\":72785},{\"end\":72812,\"start\":72804},{\"end\":72827,\"start\":72822},{\"end\":72843,\"start\":72839},{\"end\":72860,\"start\":72853},{\"end\":73227,\"start\":73220},{\"end\":73244,\"start\":73233},{\"end\":73246,\"start\":73245},{\"end\":73268,\"start\":73263},{\"end\":73292,\"start\":73285},{\"end\":73662,\"start\":73655},{\"end\":73675,\"start\":73668},{\"end\":73695,\"start\":73681},{\"end\":73954,\"start\":73947},{\"end\":73965,\"start\":73960},{\"end\":73985,\"start\":73971},{\"end\":74295,\"start\":74294},{\"end\":74311,\"start\":74306},{\"end\":74607,\"start\":74600},{\"end\":74623,\"start\":74617},{\"end\":74642,\"start\":74637},{\"end\":74644,\"start\":74643},{\"end\":74660,\"start\":74653},{\"end\":74676,\"start\":74671},{\"end\":75033,\"start\":75027},{\"end\":75046,\"start\":75043},{\"end\":75063,\"start\":75059},{\"end\":75334,\"start\":75329},{\"end\":75346,\"start\":75339},{\"end\":75355,\"start\":75351},{\"end\":75371,\"start\":75363},{\"end\":75762,\"start\":75757},{\"end\":75773,\"start\":75767},{\"end\":75785,\"start\":75779},{\"end\":75794,\"start\":75790},{\"end\":75806,\"start\":75799},{\"end\":75819,\"start\":75815},{\"end\":76063,\"start\":76058},{\"end\":76072,\"start\":76068},{\"end\":76422,\"start\":76415},{\"end\":76435,\"start\":76428},{\"end\":76447,\"start\":76441},{\"end\":76466,\"start\":76452},{\"end\":76873,\"start\":76869},{\"end\":76886,\"start\":76879},{\"end\":76910,\"start\":76901},{\"end\":77283,\"start\":77277},{\"end\":77295,\"start\":77289},{\"end\":77307,\"start\":77302},{\"end\":77317,\"start\":77313},{\"end\":77573,\"start\":77569},{\"end\":77587,\"start\":77581},{\"end\":77600,\"start\":77595},{\"end\":77612,\"start\":77605},{\"end\":77958,\"start\":77951},{\"end\":77982,\"start\":77976},{\"end\":78242,\"start\":78230},{\"end\":78258,\"start\":78253},{\"end\":78273,\"start\":78268},{\"end\":78631,\"start\":78624},{\"end\":78649,\"start\":78642},{\"end\":78651,\"start\":78650},{\"end\":78665,\"start\":78662},{\"end\":78684,\"start\":78678},{\"end\":78691,\"start\":78690},{\"end\":78715,\"start\":78706},{\"end\":79141,\"start\":79137},{\"end\":79155,\"start\":79151},{\"end\":79160,\"start\":79156},{\"end\":79171,\"start\":79166},{\"end\":79187,\"start\":79181},{\"end\":79203,\"start\":79196},{\"end\":79217,\"start\":79209},{\"end\":79233,\"start\":79227},{\"end\":79248,\"start\":79242},{\"end\":79263,\"start\":79257},{\"end\":79277,\"start\":79273},{\"end\":79293,\"start\":79285},{\"end\":79307,\"start\":79303},{\"end\":79830,\"start\":79825},{\"end\":79860,\"start\":79853},{\"end\":80130,\"start\":80126},{\"end\":80147,\"start\":80144},{\"end\":80157,\"start\":80154},{\"end\":80170,\"start\":80162},{\"end\":80186,\"start\":80179},{\"end\":80201,\"start\":80197},{\"end\":80213,\"start\":80206},{\"end\":80227,\"start\":80221},{\"end\":80244,\"start\":80238},{\"end\":80260,\"start\":80253},{\"end\":80281,\"start\":80272},{\"end\":80283,\"start\":80282},{\"end\":80292,\"start\":80290},{\"end\":80630,\"start\":80621},{\"end\":80648,\"start\":80637},{\"end\":80666,\"start\":80661},{\"end\":81029,\"start\":81024},{\"end\":81041,\"start\":81037},{\"end\":81056,\"start\":81049},{\"end\":81072,\"start\":81066},{\"end\":81087,\"start\":81082},{\"end\":81102,\"start\":81095},{\"end\":81545,\"start\":81542},{\"end\":81558,\"start\":81552},{\"end\":81568,\"start\":81563},{\"end\":81578,\"start\":81574},{\"end\":81895,\"start\":81891},{\"end\":81909,\"start\":81904},{\"end\":81929,\"start\":81920},{\"end\":81942,\"start\":81936},{\"end\":82286,\"start\":82281},{\"end\":82300,\"start\":82293},{\"end\":82631,\"start\":82623},{\"end\":82647,\"start\":82643},{\"end\":82663,\"start\":82660},{\"end\":82681,\"start\":82672},{\"end\":83064,\"start\":83057},{\"end\":83076,\"start\":83073},{\"end\":83087,\"start\":83084},{\"end\":83102,\"start\":83095},{\"end\":83104,\"start\":83103},{\"end\":83119,\"start\":83113},{\"end\":83456,\"start\":83450},{\"end\":83469,\"start\":83465},{\"end\":83486,\"start\":83477},{\"end\":83511,\"start\":83497},{\"end\":83823,\"start\":83816},{\"end\":83848,\"start\":83840},{\"end\":84102,\"start\":84098},{\"end\":84115,\"start\":84107},{\"end\":84128,\"start\":84122},{\"end\":84143,\"start\":84134},{\"end\":84369,\"start\":84364},{\"end\":84381,\"start\":84374},{\"end\":84657,\"start\":84654},{\"end\":84670,\"start\":84664},{\"end\":84684,\"start\":84678},{\"end\":84932,\"start\":84924},{\"end\":84945,\"start\":84938},{\"end\":84956,\"start\":84950},{\"end\":84967,\"start\":84964},{\"end\":84969,\"start\":84968},{\"end\":85317,\"start\":85311},{\"end\":85331,\"start\":85323},{\"end\":85345,\"start\":85338},{\"end\":85355,\"start\":85351},{\"end\":85371,\"start\":85362},{\"end\":85382,\"start\":85378},{\"end\":85772,\"start\":85765},{\"end\":85788,\"start\":85780},{\"end\":85799,\"start\":85793},{\"end\":85809,\"start\":85806},{\"end\":85818,\"start\":85814},{\"end\":85821,\"start\":85819},{\"end\":85839,\"start\":85833},{\"end\":85850,\"start\":85847},{\"end\":86304,\"start\":86297},{\"end\":86315,\"start\":86310}]", "bib_author_last_name": "[{\"end\":62953,\"start\":62950},{\"end\":62967,\"start\":62963},{\"end\":62984,\"start\":62979},{\"end\":62998,\"start\":62992},{\"end\":63013,\"start\":63007},{\"end\":63322,\"start\":63317},{\"end\":63337,\"start\":63333},{\"end\":63349,\"start\":63344},{\"end\":63366,\"start\":63359},{\"end\":63380,\"start\":63374},{\"end\":63399,\"start\":63391},{\"end\":63419,\"start\":63408},{\"end\":63433,\"start\":63428},{\"end\":63448,\"start\":63442},{\"end\":63463,\"start\":63457},{\"end\":63481,\"start\":63474},{\"end\":63501,\"start\":63489},{\"end\":63519,\"start\":63512},{\"end\":63533,\"start\":63525},{\"end\":63546,\"start\":63541},{\"end\":63561,\"start\":63555},{\"end\":63579,\"start\":63572},{\"end\":63591,\"start\":63589},{\"end\":63607,\"start\":63601},{\"end\":63626,\"start\":63621},{\"end\":63637,\"start\":63633},{\"end\":63650,\"start\":63644},{\"end\":63666,\"start\":63660},{\"end\":64532,\"start\":64529},{\"end\":64925,\"start\":64922},{\"end\":64938,\"start\":64935},{\"end\":64959,\"start\":64955},{\"end\":64969,\"start\":64961},{\"end\":65414,\"start\":65407},{\"end\":65428,\"start\":65422},{\"end\":65662,\"start\":65642},{\"end\":65679,\"start\":65670},{\"end\":65698,\"start\":65688},{\"end\":65711,\"start\":65708},{\"end\":65716,\"start\":65713},{\"end\":65995,\"start\":65975},{\"end\":66012,\"start\":66003},{\"end\":66031,\"start\":66021},{\"end\":66044,\"start\":66041},{\"end\":66049,\"start\":66046},{\"end\":66442,\"start\":66438},{\"end\":66455,\"start\":66451},{\"end\":66469,\"start\":66465},{\"end\":66483,\"start\":66480},{\"end\":66495,\"start\":66490},{\"end\":66508,\"start\":66506},{\"end\":66520,\"start\":66518},{\"end\":66527,\"start\":66525},{\"end\":66538,\"start\":66534},{\"end\":66939,\"start\":66935},{\"end\":66956,\"start\":66947},{\"end\":66974,\"start\":66967},{\"end\":66993,\"start\":66987},{\"end\":67383,\"start\":67379},{\"end\":67396,\"start\":67391},{\"end\":67412,\"start\":67406},{\"end\":67424,\"start\":67422},{\"end\":67437,\"start\":67433},{\"end\":67450,\"start\":67448},{\"end\":67462,\"start\":67457},{\"end\":67863,\"start\":67859},{\"end\":67874,\"start\":67871},{\"end\":67891,\"start\":67883},{\"end\":67903,\"start\":67901},{\"end\":68162,\"start\":68156},{\"end\":68175,\"start\":68173},{\"end\":68188,\"start\":68185},{\"end\":68565,\"start\":68558},{\"end\":68578,\"start\":68572},{\"end\":68816,\"start\":68814},{\"end\":68827,\"start\":68823},{\"end\":68841,\"start\":68836},{\"end\":68855,\"start\":68849},{\"end\":68862,\"start\":68860},{\"end\":68873,\"start\":68869},{\"end\":68884,\"start\":68880},{\"end\":69286,\"start\":69280},{\"end\":69301,\"start\":69295},{\"end\":69310,\"start\":69306},{\"end\":69324,\"start\":69319},{\"end\":69703,\"start\":69693},{\"end\":69720,\"start\":69714},{\"end\":69739,\"start\":69732},{\"end\":70072,\"start\":70067},{\"end\":70087,\"start\":70082},{\"end\":70103,\"start\":70097},{\"end\":70120,\"start\":70114},{\"end\":70140,\"start\":70131},{\"end\":70159,\"start\":70148},{\"end\":70173,\"start\":70166},{\"end\":70203,\"start\":70183},{\"end\":70228,\"start\":70225},{\"end\":70240,\"start\":70236},{\"end\":70252,\"start\":70248},{\"end\":70270,\"start\":70259},{\"end\":70284,\"start\":70279},{\"end\":70291,\"start\":70286},{\"end\":70860,\"start\":70858},{\"end\":70871,\"start\":70868},{\"end\":70881,\"start\":70879},{\"end\":70894,\"start\":70891},{\"end\":70911,\"start\":70903},{\"end\":71340,\"start\":71338},{\"end\":71350,\"start\":71348},{\"end\":71361,\"start\":71359},{\"end\":71377,\"start\":71368},{\"end\":71389,\"start\":71384},{\"end\":71699,\"start\":71697},{\"end\":71712,\"start\":71709},{\"end\":71733,\"start\":71729},{\"end\":71745,\"start\":71740},{\"end\":72095,\"start\":72093},{\"end\":72104,\"start\":72101},{\"end\":72114,\"start\":72112},{\"end\":72130,\"start\":72124},{\"end\":72140,\"start\":72136},{\"end\":72152,\"start\":72147},{\"end\":72432,\"start\":72430},{\"end\":72444,\"start\":72439},{\"end\":72802,\"start\":72793},{\"end\":72820,\"start\":72813},{\"end\":72837,\"start\":72828},{\"end\":72851,\"start\":72844},{\"end\":72869,\"start\":72861},{\"end\":73231,\"start\":73228},{\"end\":73261,\"start\":73247},{\"end\":73283,\"start\":73269},{\"end\":73301,\"start\":73293},{\"end\":73666,\"start\":73663},{\"end\":73679,\"start\":73676},{\"end\":73700,\"start\":73696},{\"end\":73958,\"start\":73955},{\"end\":73969,\"start\":73966},{\"end\":73990,\"start\":73986},{\"end\":74304,\"start\":74296},{\"end\":74318,\"start\":74312},{\"end\":74322,\"start\":74320},{\"end\":74615,\"start\":74608},{\"end\":74635,\"start\":74624},{\"end\":74651,\"start\":74645},{\"end\":74669,\"start\":74661},{\"end\":74682,\"start\":74677},{\"end\":75041,\"start\":75034},{\"end\":75057,\"start\":75047},{\"end\":75070,\"start\":75064},{\"end\":75337,\"start\":75335},{\"end\":75349,\"start\":75347},{\"end\":75361,\"start\":75356},{\"end\":75375,\"start\":75372},{\"end\":75765,\"start\":75763},{\"end\":75777,\"start\":75774},{\"end\":75788,\"start\":75786},{\"end\":75797,\"start\":75795},{\"end\":75813,\"start\":75807},{\"end\":75825,\"start\":75820},{\"end\":76066,\"start\":76064},{\"end\":76078,\"start\":76073},{\"end\":76426,\"start\":76423},{\"end\":76439,\"start\":76436},{\"end\":76450,\"start\":76448},{\"end\":76471,\"start\":76467},{\"end\":76877,\"start\":76874},{\"end\":76899,\"start\":76887},{\"end\":76915,\"start\":76911},{\"end\":77287,\"start\":77284},{\"end\":77300,\"start\":77296},{\"end\":77311,\"start\":77308},{\"end\":77322,\"start\":77318},{\"end\":77579,\"start\":77574},{\"end\":77593,\"start\":77588},{\"end\":77603,\"start\":77601},{\"end\":77623,\"start\":77613},{\"end\":77974,\"start\":77959},{\"end\":77988,\"start\":77983},{\"end\":77996,\"start\":77990},{\"end\":78251,\"start\":78243},{\"end\":78266,\"start\":78259},{\"end\":78279,\"start\":78274},{\"end\":78640,\"start\":78632},{\"end\":78660,\"start\":78652},{\"end\":78676,\"start\":78666},{\"end\":78688,\"start\":78685},{\"end\":78704,\"start\":78692},{\"end\":78721,\"start\":78716},{\"end\":79149,\"start\":79142},{\"end\":79164,\"start\":79161},{\"end\":79179,\"start\":79172},{\"end\":79194,\"start\":79188},{\"end\":79207,\"start\":79204},{\"end\":79225,\"start\":79218},{\"end\":79240,\"start\":79234},{\"end\":79255,\"start\":79249},{\"end\":79271,\"start\":79264},{\"end\":79283,\"start\":79278},{\"end\":79301,\"start\":79294},{\"end\":79317,\"start\":79308},{\"end\":79851,\"start\":79831},{\"end\":79865,\"start\":79861},{\"end\":79877,\"start\":79867},{\"end\":80142,\"start\":80131},{\"end\":80152,\"start\":80148},{\"end\":80160,\"start\":80158},{\"end\":80177,\"start\":80171},{\"end\":80195,\"start\":80187},{\"end\":80204,\"start\":80202},{\"end\":80219,\"start\":80214},{\"end\":80236,\"start\":80228},{\"end\":80251,\"start\":80245},{\"end\":80270,\"start\":80261},{\"end\":80288,\"start\":80284},{\"end\":80300,\"start\":80293},{\"end\":80635,\"start\":80631},{\"end\":80659,\"start\":80649},{\"end\":80677,\"start\":80667},{\"end\":81035,\"start\":81030},{\"end\":81047,\"start\":81042},{\"end\":81064,\"start\":81057},{\"end\":81080,\"start\":81073},{\"end\":81093,\"start\":81088},{\"end\":81109,\"start\":81103},{\"end\":81550,\"start\":81546},{\"end\":81561,\"start\":81559},{\"end\":81572,\"start\":81569},{\"end\":81584,\"start\":81579},{\"end\":81902,\"start\":81896},{\"end\":81918,\"start\":81910},{\"end\":81934,\"start\":81930},{\"end\":81952,\"start\":81943},{\"end\":82291,\"start\":82287},{\"end\":82307,\"start\":82301},{\"end\":82641,\"start\":82632},{\"end\":82658,\"start\":82648},{\"end\":82670,\"start\":82664},{\"end\":82686,\"start\":82682},{\"end\":83071,\"start\":83065},{\"end\":83082,\"start\":83077},{\"end\":83093,\"start\":83088},{\"end\":83111,\"start\":83105},{\"end\":83130,\"start\":83120},{\"end\":83463,\"start\":83457},{\"end\":83475,\"start\":83470},{\"end\":83495,\"start\":83487},{\"end\":83517,\"start\":83512},{\"end\":83838,\"start\":83824},{\"end\":83855,\"start\":83849},{\"end\":84105,\"start\":84103},{\"end\":84120,\"start\":84116},{\"end\":84132,\"start\":84129},{\"end\":84148,\"start\":84144},{\"end\":84372,\"start\":84370},{\"end\":84384,\"start\":84382},{\"end\":84662,\"start\":84658},{\"end\":84676,\"start\":84671},{\"end\":84693,\"start\":84685},{\"end\":84936,\"start\":84933},{\"end\":84948,\"start\":84946},{\"end\":84962,\"start\":84957},{\"end\":84974,\"start\":84970},{\"end\":85321,\"start\":85318},{\"end\":85336,\"start\":85332},{\"end\":85349,\"start\":85346},{\"end\":85360,\"start\":85356},{\"end\":85376,\"start\":85372},{\"end\":85387,\"start\":85383},{\"end\":85778,\"start\":85773},{\"end\":85791,\"start\":85789},{\"end\":85804,\"start\":85800},{\"end\":85812,\"start\":85810},{\"end\":85831,\"start\":85822},{\"end\":85845,\"start\":85840},{\"end\":85857,\"start\":85851},{\"end\":86308,\"start\":86305},{\"end\":86321,\"start\":86316}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3322503},\"end\":63309,\"start\":62855},{\"attributes\":{\"id\":\"b1\"},\"end\":64518,\"start\":63311},{\"attributes\":{\"id\":\"b2\"},\"end\":64808,\"start\":64520},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":204960658},\"end\":65317,\"start\":64810},{\"attributes\":{\"doi\":\"CoRR abs/1705.07263\",\"id\":\"b4\"},\"end\":65593,\"start\":65319},{\"attributes\":{\"doi\":\"CoRR abs/1811.02054\",\"id\":\"b5\"},\"end\":65896,\"start\":65595},{\"attributes\":{\"doi\":\"2020. 4\",\"id\":\"b6\",\"matched_paper_id\":67876983},\"end\":66346,\"start\":65898},{\"attributes\":{\"doi\":\"2022. 14\",\"id\":\"b7\",\"matched_paper_id\":245117504},\"end\":66857,\"start\":66348},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b8\",\"matched_paper_id\":211096730},\"end\":67288,\"start\":66859},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":238354397},\"end\":67795,\"start\":67290},{\"attributes\":{\"doi\":\"CoRR abs/2003.04297, 2020. 3\",\"id\":\"b10\"},\"end\":68078,\"start\":67797},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":308212},\"end\":68495,\"start\":68080},{\"attributes\":{\"doi\":\"CoRR abs/1912.07721\",\"id\":\"b12\"},\"end\":68713,\"start\":68497},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":239012191},\"end\":69194,\"start\":68715},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":219530980},\"end\":69639,\"start\":69196},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6706414},\"end\":69981,\"start\":69641},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":219687798},\"end\":70781,\"start\":69983},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207930212},\"end\":71228,\"start\":70783},{\"attributes\":{\"doi\":\"CoRR abs/2208.10445, 2022. 4\",\"id\":\"b18\"},\"end\":71614,\"start\":71230},{\"attributes\":{\"doi\":\"2022. 4\",\"id\":\"b19\",\"matched_paper_id\":251066729},\"end\":72013,\"start\":71616},{\"attributes\":{\"doi\":\"CoRR abs/2102.05429, 2021. 4\",\"id\":\"b20\"},\"end\":72355,\"start\":72015},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":231846491},\"end\":72720,\"start\":72357},{\"attributes\":{\"doi\":\"2020. 4\",\"id\":\"b22\",\"matched_paper_id\":211858541},\"end\":73158,\"start\":72722},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":211532649},\"end\":73591,\"start\":73160},{\"attributes\":{\"doi\":\"CoRR abs/2110.15444\",\"id\":\"b24\"},\"end\":73863,\"start\":73593},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":236772198},\"end\":74248,\"start\":73865},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6628106},\"end\":74535,\"start\":74250},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":204907203},\"end\":74981,\"start\":74537},{\"attributes\":{\"doi\":\"CoRR abs/1607.02533\",\"id\":\"b28\"},\"end\":75212,\"start\":74983},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":207847538},\"end\":75702,\"start\":75214},{\"attributes\":{\"doi\":\"CoRR abs/2208.11180, 2022. 4\",\"id\":\"b30\"},\"end\":76012,\"start\":75704},{\"attributes\":{\"doi\":\"2021. 4\",\"id\":\"b31\",\"matched_paper_id\":237563320},\"end\":76327,\"start\":76014},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":237290083},\"end\":76790,\"start\":76329},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":44096776},\"end\":77205,\"start\":76792},{\"attributes\":{\"doi\":\"CoRR abs/1611.02770\",\"id\":\"b34\"},\"end\":77492,\"start\":77207},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":236975869},\"end\":77878,\"start\":77494},{\"attributes\":{\"doi\":\"CoRR abs/1711.01894\",\"id\":\"b36\"},\"end\":78169,\"start\":77880},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":54457412},\"end\":78568,\"start\":78171},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1090603},\"end\":79064,\"start\":78570},{\"attributes\":{\"doi\":\"PMLR, 2021. 3\",\"id\":\"b39\",\"matched_paper_id\":231591445},\"end\":79736,\"start\":79066},{\"attributes\":{\"doi\":\"CoRR abs/1804.00750\",\"id\":\"b40\"},\"end\":80073,\"start\":79738},{\"attributes\":{\"doi\":\"CoRR abs/1409.0575\",\"id\":\"b41\"},\"end\":80586,\"start\":80075},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":203610516},\"end\":80915,\"start\":80588},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":46933970},\"end\":81476,\"start\":80917},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":245144803},\"end\":81827,\"start\":81478},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10488675},\"end\":82212,\"start\":81829},{\"attributes\":{\"doi\":\"2021. 4\",\"id\":\"b46\",\"matched_paper_id\":214623088},\"end\":82532,\"start\":82214},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":15926837},\"end\":83001,\"start\":82534},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2984526},\"end\":83400,\"start\":83003},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":13060737},\"end\":83784,\"start\":83402},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":5855042},\"end\":84019,\"start\":83786},{\"attributes\":{\"doi\":\"CoRR abs/2010.12751, 2020. 4\",\"id\":\"b51\"},\"end\":84341,\"start\":84021},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4076251},\"end\":84569,\"start\":84343},{\"attributes\":{\"doi\":\"CoRR abs/1708.07747\",\"id\":\"b53\"},\"end\":84873,\"start\":84571},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":202235190},\"end\":85262,\"start\":84875},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":225076220},\"end\":85687,\"start\":85264},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":44085059},\"end\":86211,\"start\":85689},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":27494814},\"end\":86573,\"start\":86213}]", "bib_title": "[{\"end\":62942,\"start\":62855},{\"end\":64913,\"start\":64810},{\"end\":65964,\"start\":65898},{\"end\":66429,\"start\":66348},{\"end\":66928,\"start\":66859},{\"end\":67370,\"start\":67290},{\"end\":68149,\"start\":68080},{\"end\":68805,\"start\":68715},{\"end\":69271,\"start\":69196},{\"end\":69687,\"start\":69641},{\"end\":70052,\"start\":69983},{\"end\":70848,\"start\":70783},{\"end\":71688,\"start\":71616},{\"end\":72421,\"start\":72357},{\"end\":72783,\"start\":72722},{\"end\":73218,\"start\":73160},{\"end\":73945,\"start\":73865},{\"end\":74292,\"start\":74250},{\"end\":74598,\"start\":74537},{\"end\":75327,\"start\":75214},{\"end\":76056,\"start\":76014},{\"end\":76413,\"start\":76329},{\"end\":76867,\"start\":76792},{\"end\":77567,\"start\":77494},{\"end\":78228,\"start\":78171},{\"end\":78622,\"start\":78570},{\"end\":79135,\"start\":79066},{\"end\":80619,\"start\":80588},{\"end\":81022,\"start\":80917},{\"end\":81540,\"start\":81478},{\"end\":81889,\"start\":81829},{\"end\":82279,\"start\":82214},{\"end\":82621,\"start\":82534},{\"end\":83055,\"start\":83003},{\"end\":83448,\"start\":83402},{\"end\":83814,\"start\":83786},{\"end\":84362,\"start\":84343},{\"end\":84922,\"start\":84875},{\"end\":85309,\"start\":85264},{\"end\":85763,\"start\":85689},{\"end\":86295,\"start\":86213}]", "bib_author": "[{\"end\":62955,\"start\":62944},{\"end\":62969,\"start\":62955},{\"end\":62986,\"start\":62969},{\"end\":63000,\"start\":62986},{\"end\":63015,\"start\":63000},{\"end\":63324,\"start\":63311},{\"end\":63339,\"start\":63324},{\"end\":63351,\"start\":63339},{\"end\":63368,\"start\":63351},{\"end\":63382,\"start\":63368},{\"end\":63401,\"start\":63382},{\"end\":63421,\"start\":63401},{\"end\":63435,\"start\":63421},{\"end\":63450,\"start\":63435},{\"end\":63465,\"start\":63450},{\"end\":63483,\"start\":63465},{\"end\":63503,\"start\":63483},{\"end\":63521,\"start\":63503},{\"end\":63535,\"start\":63521},{\"end\":63548,\"start\":63535},{\"end\":63563,\"start\":63548},{\"end\":63581,\"start\":63563},{\"end\":63593,\"start\":63581},{\"end\":63609,\"start\":63593},{\"end\":63628,\"start\":63609},{\"end\":63639,\"start\":63628},{\"end\":63652,\"start\":63639},{\"end\":63668,\"start\":63652},{\"end\":64524,\"start\":64520},{\"end\":64534,\"start\":64524},{\"end\":64927,\"start\":64915},{\"end\":64940,\"start\":64927},{\"end\":64961,\"start\":64940},{\"end\":64971,\"start\":64961},{\"end\":65416,\"start\":65398},{\"end\":65430,\"start\":65416},{\"end\":65664,\"start\":65633},{\"end\":65681,\"start\":65664},{\"end\":65700,\"start\":65681},{\"end\":65713,\"start\":65700},{\"end\":65718,\"start\":65713},{\"end\":65997,\"start\":65966},{\"end\":66014,\"start\":65997},{\"end\":66033,\"start\":66014},{\"end\":66046,\"start\":66033},{\"end\":66051,\"start\":66046},{\"end\":66444,\"start\":66431},{\"end\":66457,\"start\":66444},{\"end\":66471,\"start\":66457},{\"end\":66485,\"start\":66471},{\"end\":66497,\"start\":66485},{\"end\":66510,\"start\":66497},{\"end\":66522,\"start\":66510},{\"end\":66529,\"start\":66522},{\"end\":66540,\"start\":66529},{\"end\":66941,\"start\":66930},{\"end\":66958,\"start\":66941},{\"end\":66976,\"start\":66958},{\"end\":66995,\"start\":66976},{\"end\":67385,\"start\":67372},{\"end\":67398,\"start\":67385},{\"end\":67414,\"start\":67398},{\"end\":67426,\"start\":67414},{\"end\":67439,\"start\":67426},{\"end\":67452,\"start\":67439},{\"end\":67464,\"start\":67452},{\"end\":67865,\"start\":67852},{\"end\":67876,\"start\":67865},{\"end\":67893,\"start\":67876},{\"end\":67905,\"start\":67893},{\"end\":68164,\"start\":68151},{\"end\":68177,\"start\":68164},{\"end\":68190,\"start\":68177},{\"end\":68567,\"start\":68552},{\"end\":68580,\"start\":68567},{\"end\":68818,\"start\":68807},{\"end\":68829,\"start\":68818},{\"end\":68843,\"start\":68829},{\"end\":68857,\"start\":68843},{\"end\":68864,\"start\":68857},{\"end\":68875,\"start\":68864},{\"end\":68886,\"start\":68875},{\"end\":69288,\"start\":69273},{\"end\":69303,\"start\":69288},{\"end\":69312,\"start\":69303},{\"end\":69326,\"start\":69312},{\"end\":69705,\"start\":69689},{\"end\":69722,\"start\":69705},{\"end\":69741,\"start\":69722},{\"end\":70074,\"start\":70054},{\"end\":70089,\"start\":70074},{\"end\":70105,\"start\":70089},{\"end\":70122,\"start\":70105},{\"end\":70142,\"start\":70122},{\"end\":70161,\"start\":70142},{\"end\":70175,\"start\":70161},{\"end\":70205,\"start\":70175},{\"end\":70230,\"start\":70205},{\"end\":70242,\"start\":70230},{\"end\":70254,\"start\":70242},{\"end\":70272,\"start\":70254},{\"end\":70286,\"start\":70272},{\"end\":70293,\"start\":70286},{\"end\":70862,\"start\":70850},{\"end\":70873,\"start\":70862},{\"end\":70883,\"start\":70873},{\"end\":70896,\"start\":70883},{\"end\":70913,\"start\":70896},{\"end\":71342,\"start\":71331},{\"end\":71352,\"start\":71342},{\"end\":71363,\"start\":71352},{\"end\":71379,\"start\":71363},{\"end\":71391,\"start\":71379},{\"end\":71701,\"start\":71690},{\"end\":71714,\"start\":71701},{\"end\":71735,\"start\":71714},{\"end\":71747,\"start\":71735},{\"end\":72097,\"start\":72086},{\"end\":72106,\"start\":72097},{\"end\":72116,\"start\":72106},{\"end\":72132,\"start\":72116},{\"end\":72142,\"start\":72132},{\"end\":72154,\"start\":72142},{\"end\":72434,\"start\":72423},{\"end\":72446,\"start\":72434},{\"end\":72804,\"start\":72785},{\"end\":72822,\"start\":72804},{\"end\":72839,\"start\":72822},{\"end\":72853,\"start\":72839},{\"end\":72871,\"start\":72853},{\"end\":73233,\"start\":73220},{\"end\":73263,\"start\":73233},{\"end\":73285,\"start\":73263},{\"end\":73303,\"start\":73285},{\"end\":73668,\"start\":73655},{\"end\":73681,\"start\":73668},{\"end\":73702,\"start\":73681},{\"end\":73960,\"start\":73947},{\"end\":73971,\"start\":73960},{\"end\":73992,\"start\":73971},{\"end\":74306,\"start\":74294},{\"end\":74320,\"start\":74306},{\"end\":74324,\"start\":74320},{\"end\":74617,\"start\":74600},{\"end\":74637,\"start\":74617},{\"end\":74653,\"start\":74637},{\"end\":74671,\"start\":74653},{\"end\":74684,\"start\":74671},{\"end\":75043,\"start\":75027},{\"end\":75059,\"start\":75043},{\"end\":75072,\"start\":75059},{\"end\":75339,\"start\":75329},{\"end\":75351,\"start\":75339},{\"end\":75363,\"start\":75351},{\"end\":75377,\"start\":75363},{\"end\":75767,\"start\":75757},{\"end\":75779,\"start\":75767},{\"end\":75790,\"start\":75779},{\"end\":75799,\"start\":75790},{\"end\":75815,\"start\":75799},{\"end\":75827,\"start\":75815},{\"end\":76068,\"start\":76058},{\"end\":76080,\"start\":76068},{\"end\":76428,\"start\":76415},{\"end\":76441,\"start\":76428},{\"end\":76452,\"start\":76441},{\"end\":76473,\"start\":76452},{\"end\":76879,\"start\":76869},{\"end\":76901,\"start\":76879},{\"end\":76917,\"start\":76901},{\"end\":77289,\"start\":77277},{\"end\":77302,\"start\":77289},{\"end\":77313,\"start\":77302},{\"end\":77324,\"start\":77313},{\"end\":77581,\"start\":77569},{\"end\":77595,\"start\":77581},{\"end\":77605,\"start\":77595},{\"end\":77625,\"start\":77605},{\"end\":77976,\"start\":77951},{\"end\":77990,\"start\":77976},{\"end\":77998,\"start\":77990},{\"end\":78253,\"start\":78230},{\"end\":78268,\"start\":78253},{\"end\":78281,\"start\":78268},{\"end\":78642,\"start\":78624},{\"end\":78662,\"start\":78642},{\"end\":78678,\"start\":78662},{\"end\":78690,\"start\":78678},{\"end\":78706,\"start\":78690},{\"end\":78723,\"start\":78706},{\"end\":79151,\"start\":79137},{\"end\":79166,\"start\":79151},{\"end\":79181,\"start\":79166},{\"end\":79196,\"start\":79181},{\"end\":79209,\"start\":79196},{\"end\":79227,\"start\":79209},{\"end\":79242,\"start\":79227},{\"end\":79257,\"start\":79242},{\"end\":79273,\"start\":79257},{\"end\":79285,\"start\":79273},{\"end\":79303,\"start\":79285},{\"end\":79319,\"start\":79303},{\"end\":79853,\"start\":79825},{\"end\":79867,\"start\":79853},{\"end\":79879,\"start\":79867},{\"end\":80144,\"start\":80126},{\"end\":80154,\"start\":80144},{\"end\":80162,\"start\":80154},{\"end\":80179,\"start\":80162},{\"end\":80197,\"start\":80179},{\"end\":80206,\"start\":80197},{\"end\":80221,\"start\":80206},{\"end\":80238,\"start\":80221},{\"end\":80253,\"start\":80238},{\"end\":80272,\"start\":80253},{\"end\":80290,\"start\":80272},{\"end\":80302,\"start\":80290},{\"end\":80637,\"start\":80621},{\"end\":80661,\"start\":80637},{\"end\":80679,\"start\":80661},{\"end\":81037,\"start\":81024},{\"end\":81049,\"start\":81037},{\"end\":81066,\"start\":81049},{\"end\":81082,\"start\":81066},{\"end\":81095,\"start\":81082},{\"end\":81111,\"start\":81095},{\"end\":81552,\"start\":81542},{\"end\":81563,\"start\":81552},{\"end\":81574,\"start\":81563},{\"end\":81586,\"start\":81574},{\"end\":81904,\"start\":81891},{\"end\":81920,\"start\":81904},{\"end\":81936,\"start\":81920},{\"end\":81954,\"start\":81936},{\"end\":82293,\"start\":82281},{\"end\":82309,\"start\":82293},{\"end\":82643,\"start\":82623},{\"end\":82660,\"start\":82643},{\"end\":82672,\"start\":82660},{\"end\":82688,\"start\":82672},{\"end\":83073,\"start\":83057},{\"end\":83084,\"start\":83073},{\"end\":83095,\"start\":83084},{\"end\":83113,\"start\":83095},{\"end\":83132,\"start\":83113},{\"end\":83465,\"start\":83450},{\"end\":83477,\"start\":83465},{\"end\":83497,\"start\":83477},{\"end\":83519,\"start\":83497},{\"end\":83840,\"start\":83816},{\"end\":83857,\"start\":83840},{\"end\":84107,\"start\":84098},{\"end\":84122,\"start\":84107},{\"end\":84134,\"start\":84122},{\"end\":84150,\"start\":84134},{\"end\":84374,\"start\":84364},{\"end\":84386,\"start\":84374},{\"end\":84664,\"start\":84654},{\"end\":84678,\"start\":84664},{\"end\":84695,\"start\":84678},{\"end\":84938,\"start\":84924},{\"end\":84950,\"start\":84938},{\"end\":84964,\"start\":84950},{\"end\":84976,\"start\":84964},{\"end\":85323,\"start\":85311},{\"end\":85338,\"start\":85323},{\"end\":85351,\"start\":85338},{\"end\":85362,\"start\":85351},{\"end\":85378,\"start\":85362},{\"end\":85389,\"start\":85378},{\"end\":85780,\"start\":85765},{\"end\":85793,\"start\":85780},{\"end\":85806,\"start\":85793},{\"end\":85814,\"start\":85806},{\"end\":85833,\"start\":85814},{\"end\":85847,\"start\":85833},{\"end\":85859,\"start\":85847},{\"end\":86310,\"start\":86297},{\"end\":86323,\"start\":86310}]", "bib_venue": "[{\"end\":63835,\"start\":63747},{\"end\":63058,\"start\":63015},{\"end\":63745,\"start\":63668},{\"end\":64657,\"start\":64534},{\"end\":65040,\"start\":64971},{\"end\":65396,\"start\":65319},{\"end\":65631,\"start\":65595},{\"end\":66101,\"start\":66058},{\"end\":66592,\"start\":66548},{\"end\":67050,\"start\":66999},{\"end\":67520,\"start\":67464},{\"end\":67850,\"start\":67797},{\"end\":68266,\"start\":68190},{\"end\":68550,\"start\":68497},{\"end\":68933,\"start\":68886},{\"end\":69395,\"start\":69326},{\"end\":69800,\"start\":69741},{\"end\":70370,\"start\":70293},{\"end\":70978,\"start\":70913},{\"end\":71329,\"start\":71230},{\"end\":71799,\"start\":71754},{\"end\":72084,\"start\":72015},{\"end\":72513,\"start\":72446},{\"end\":72921,\"start\":72878},{\"end\":73346,\"start\":73303},{\"end\":73653,\"start\":73593},{\"end\":74036,\"start\":73992},{\"end\":74383,\"start\":74324},{\"end\":74749,\"start\":74684},{\"end\":75025,\"start\":74983},{\"end\":75433,\"start\":75377},{\"end\":75755,\"start\":75704},{\"end\":76154,\"start\":76087},{\"end\":76540,\"start\":76473},{\"end\":76969,\"start\":76917},{\"end\":77275,\"start\":77207},{\"end\":77669,\"start\":77625},{\"end\":77949,\"start\":77880},{\"end\":78346,\"start\":78281},{\"end\":78792,\"start\":78723},{\"end\":79383,\"start\":79332},{\"end\":79823,\"start\":79738},{\"end\":80124,\"start\":80075},{\"end\":80728,\"start\":80679},{\"end\":81185,\"start\":81111},{\"end\":81642,\"start\":81586},{\"end\":81998,\"start\":81954},{\"end\":82367,\"start\":82316},{\"end\":82745,\"start\":82688},{\"end\":83175,\"start\":83132},{\"end\":83574,\"start\":83519},{\"end\":83893,\"start\":83857},{\"end\":84096,\"start\":84021},{\"end\":84431,\"start\":84386},{\"end\":84652,\"start\":84571},{\"end\":85043,\"start\":84976},{\"end\":85466,\"start\":85389},{\"end\":85928,\"start\":85859},{\"end\":86382,\"start\":86323}]"}}}, "year": 2023, "month": 12, "day": 17}