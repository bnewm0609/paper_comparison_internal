{"id": 244729224, "updated": "2023-10-05 19:14:17.587", "metadata": {"title": "Diffusion Autoencoders: Toward a Meaningful and Decodable Representation", "authors": "[{\"first\":\"Konpat\",\"last\":\"Preechakul\",\"middle\":[]},{\"first\":\"Nattanat\",\"last\":\"Chatthee\",\"middle\":[]},{\"first\":\"Suttisak\",\"last\":\"Wizadwongsa\",\"middle\":[]},{\"first\":\"Supasorn\",\"last\":\"Suwajanakorn\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Diffusion probabilistic models (DPMs) have achieved remarkable quality in image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful representation for other tasks. This paper explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code, where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding improves denoising efficiency and naturally facilitates various downstream tasks including few-shot conditional sampling. Please visit our project page: https://Diff-AE.github.io/", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.15640", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/PreechakulCWS22", "doi": "10.1109/cvpr52688.2022.01036"}}, "content": {"source": {"pdf_hash": "b582edb16f5425642767cb6c26839111f867f4dc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.15640v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "35d5be4b9be8ebc08f77de53ee7a86197fcbae7a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b582edb16f5425642767cb6c26839111f867f4dc.txt", "contents": "\nDiffusion Autoencoders: Toward a Meaningful and Decodable Representation\n\n\nKonpat Preechakul \nNattanat Chatthee \nSuttisak Wizadwongsa \nThailandSupasorn Suwajanakorn \nDiffusion Autoencoders: Toward a Meaningful and Decodable Representation\nReal image Real image Real image Younger Older Real image Wavy hair Real image Smiling\nFigure 1. Attribute manipulation and interpolation on real images. Diffusion autoencoders can encode any image into a two-part latent code that captures both semantics and stochastic variations and allows near-exact reconstruction. This latent code can be interpolated or modified by a simple linear operation and decoded back to a highly realistic output for various downstream tasks.AbstractDiffusion probabilistic models (DPMs) have achieved remarkable quality in image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful representation for other tasks. This paper explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding improves denoising efficiency and naturally facilitates various downstream tasks including few-shot conditional sampling. Please visit our page: https\n\nIntroduction\n\nDiffusion-based (DPMs) [22,46] and score-based [49] generative models have recently succeeded in synthesizing realistic and high-resolution images, rivaling those from GANs [11,15,23]. These two models are closely related and, in practice, optimize similar objectives. Numerous applications have emerged notably in the image domain, such as image manipulation, translation, super-resolution [8,32,35,43], in speech and text domains [5,6], or 3D point cloud [34]. Recent studies have improved DPMs further in both theory and practice [25,29,31]. In this paper, however, we question whether DPMs can serve as a good representation learner. Specifically, we seek to extract a meaningful and decodable representation of an image that contains high-level semantics yet allows near-exact reconstruction of the image. Our exploration focuses on diffusion models, but the contributions are applicable also to score-based models.\n\nOne way to learn a representation is through an autoencoder. There exists a certain kind of DPM [47] that can act as an encoder-decoder that converts any input image x 0 into a spatial latent variable x T by running the generative process backward. However, the resulting latent variable lacks high-level semantics and other desirable properties, such as disentanglement, compactness, or the ability to perform meaningful linear interpolation in the latent space. Alternatively, one can use a trained GAN for extracting a representation using the so-called GAN inversion [28,58], which optimizes for a latent code that reproduces the given input. Even though the resulting code carries rich semantics, this technique struggles to faithfully reconstruct the input image. To overcome these challenges, we propose a diffusionbased autoencoder that leverages the powerful DPMs for decodable representation learning.\n\nFinding a meaningful representation that is decodable requires capturing both the high-level semantics and low-level stochastic variations. Our key idea is to learn both levels of representation by utilizing a learnable encoder for discovering high-level semantics and utilizing a DPM for decoding and modeling stochastic variations. In particular, we use our conditional variant of the Denoising Diffusion Implicit Model (DDIM) [47] as the decoder and separate the latent code into two subcodes. The first \"semantic\" subcode is compact and inferred with a CNN encoder, whereas the second \"stochastic\" subcode is inferred by reversing the generative process of our DDIM variant conditioned on the semantic subcode. In contrast to other DPMs, DDIM modifies the forward process to be non-Markovian while preserving the training objectives of DPMs. This modification allows deterministically encoding an image to its corresponding initial noise, which represents our stochastic subcode.\n\nThe implication of this framework is two-fold. First, by conditioning DDIM on the semantic information of the target output, denoising becomes easier and faster. Second, this design produces a representation that is linear, semantically meaningful, and decodable-a novel property for DPMs' latent variables. This crucial property allows harnessing DPMs for many tasks including those that are highly challenging for any GAN-based methods, such as interpolation and attribute manipulation on real images. Unlike GANs, which rely on error-prone inversion before operating on real images, our method requires no optimization to encode the input and produces high-quality output with original details preserved.\n\nDespite being an autoencoder, which is generally not designed for unconditional generation, our framework can be used to generate image samples by fitting another DPM to the semantic subcode distribution. This combination achieves competitive FID scores on unconditional generation compared to a vanilla DPM. Moreover, the ability to sample from our compact and meaningful latent space also enables few-shot conditional generation (i.e., generate images with similar semantics to those of a few examples). Compared to other DPM-based techniques for the few-shot setup, our method produces convincing results with only a handful labeled examples without additional contrastive learning used in prior work [45].\n\n\nBackground\n\nDiffusion-based (DPMs) and score-based generative models belong to a family of generative models that model the target distribution by learning a denoising process of varying noise levels. A successful process can denoise or map an arbitrary Gaussian noise map from the prior N (0, I) to a clean image sample after T successive denoising passes. Ho et al. [22] proposed to learn a function \u03b8 (x t , t) that takes a noisy image x t and predicts its noise using a UNet [40]. The model is trained with a loss function \u03b8 (x t , t) \u2212 , where is the actual noise added to x 0 to produce x t . This formulation is a simplified, reweighted version of the variational lower bound on the marginal log likelihood and has been commonly used throughout the community [11,29,36,47].\n\nMore formally, we define a Gaussian diffusion process at time t (out of T ) that increasingly adds noise to an input image\nx 0 as q(x t |x t\u22121 ) = N ( \u221a 1 \u2212 \u03b2 t x t\u22121 , \u03b2 t I),\nwhere \u03b2 t are hyperparameters representing the noise levels. With Gaussian diffusion, the noisy version of an image x 0 at time t is another Gaussian\nq(x t |x 0 ) = N ( \u221a \u03b1 t x 0 , (1 \u2212 \u03b1 t )I) where \u03b1 t = t s=1 (1 \u2212 \u03b2 s ).\nWe are interested in learning the reverse process of this, i.e., the distribution p(x t\u22121 |x t ). This probability function is likely a complex one unless the gap between t \u2212 1 and t is infinitesimally small (T = \u221e) [46]. In such a case, p(x t\u22121 |x t ) can be modeled as N (\u00b5 \u03b8 (x t , t), \u03c3 t ) [22]. There are many ways to model this distribution, one of which is via \u03b8 (x t , t) mentioned earlier.\n\nIn practice, the assumption of T = \u221e is never satisfied; hence, DPMs are only approximations.\n\nAs latent-variable models, DPMs can naturally yield the latent variables x 1:T through its forward process; however, these variables are stochastic and only representing a sequence of image degradation by Gaussian noise, which does not contain much semantics. Song et al. [47] proposed another kind of DPM called Denoising Diffusion Implicit Model (DDIM) that enjoys the following deterministic generative process:\nx t\u22121 = \u221a \u03b1 t\u22121 x t \u2212 \u221a 1\u2212\u03b1 t t \u03b8 (x t ) \u221a \u03b1 t + \u221a 1 \u2212 \u03b1 t\u22121 t \u03b8 (x t )\n(1) and the following novel inference distribution:\nq(x t\u22121 |x t , x 0 ) = N \u221a \u03b1 t\u22121 x 0 + \u221a 1 \u2212 \u03b1 t\u22121 xt\u2212 \u221a \u03b1tx0 \u221a 1\u2212\u03b1t , 0 (2) while maintaining the original DDPM marginal distribution q(x t |x 0 ) = N ( \u221a \u03b1 t x 0 , (1 \u2212 \u03b1 t )I).\nBy doing so, DDIM shares both the objective and solution with DDPM and only differs in how samples are generated. With DDIM, it is possible to run the generative process backward deterministically to obtain the noise map x T , which represents the latent variable or encoding of a given image x 0 . In this context, DDIM can be thought of as an image decoder that decodes the latent code x T back to the input image. This process can yield a very accurate reconstruction; however, x T still does not contain high-level semantics as would be expected from a meaningful representation. We show in Figure 4c that the interpolation between two latent variables x T 's does not correspond to a semantically-smooth change in the resulting images. The images only share the overall composition and background colors but do not resemble the identity of either person. This  Figure 2. Overview of our diffusion autoencoder. The autoencoder consists of a \"semantic\" encoder that maps the input image to the semantic subcode (x0 \u2192 zsem), and a conditional DDIM that acts both as a \"stochastic\" encoder (x0 \u2192 xT ) and a decoder ((zsem, xT ) \u2192 x0). Here, zsem captures the high-level semantics while xT captures low-level stochastic variations, and together they can be decoded back to the original image with high fidelity. To sample from this autoencoder, we fit a latent DDIM to the distribution of zsem and sample (zsem, xT \u223c N (0, I)) for decoding.\n\nis, perhaps, understandable as x T is heavily influenced by the pixel values of x 0 due to an implicit linear bias from the marginals q(\nx T |x 0 ) = N ( \u221a \u03b1 T x 0 , (1 \u2212 \u03b1 T )I).\nThis motivates approaches that augment DPMs with novel mechanisms to make their latent variables more meaningful, as will be proposed in this work.\n\n\nDiffusion autoencoders\n\nIn the pursuit of a meaningful latent code, we design a conditional DDIM image decoder p(x t\u22121 |x t , z sem ) that is conditioned on an additional latent variable z sem , and a semantic encoder z sem = Enc \u03c6 (x 0 ) that learns to map an input image x 0 to a semantically meaningful z sem . Here, the conditional DDIM decoder takes as input a latent variable z = (z sem , x T ), which consists of the high-level \"semantic\" subcode z sem and a low-level \"stochastic\" subcode x T , inferred by reversing the generative process of DDIM. In this framework, DDIM acts as both the decoder and the stochastic encoder. The overview is shown in Figure 2.\n\nUnlike in other conditional DPMs [23,32,45] that use spatial conditional variables (e.g., 2D latent maps), our z sem is a non-spatial vector of dimension d = 512, which resembles the style vector in StyleGAN [27,28] and allows us to encode global semantics not specific to any spatial regions. One of our goals is to learn a semantically rich latent space that allows smooth interpolation, similar to those learned by GANs, while keeping the reconstruction capability that diffusion models excel.\n\n\nDiffusion-based Decoder\n\nOur conditional DDIM decoder receives as input z = (z sem , x T ) to produce the output image. This decoder is a conditional DDIM that models p \u03b8 (x t\u22121 |x t , z sem ) to match the inference distribution q(x t\u22121 |x t , x 0 ) defined in Equation 2, with the following reverse (generative) process:\np \u03b8 (x 0:T | z sem ) = p(x T ) T t=1 p \u03b8 (x t\u22121 | x t , z sem ) (3) p \u03b8 (x t\u22121 |x t , z sem ) = N (f \u03b8 (x 1 , 1, z sem ), 0) if t = 1 q(x t\u22121 |x t , f \u03b8 (x t , t, z sem )) otherwise\n(4) Following Song et al. [47], we parameterize f \u03b8 in Equation 4 as a noise prediction network \u03b8 (x t , t, z sem ):\nf \u03b8 (x t , t, z sem ) = 1 \u221a \u03b1 t x t \u2212 \u221a 1 \u2212 \u03b1 t \u03b8 (x t , t, z sem ) (5)\nThis network is a modified version of the UNet of a recent DPM from Dhariwal et al. [11]. Training is done by optimizing L simple [22] loss function with respect to \u03b8 and \u03c6.\nL simple = T t=1 E x0, t \u03b8 (x t , t, z sem ) \u2212 t 2 2 (6) where t \u2208 R 3\u00d7h\u00d7w \u223c N (0, I), x t = \u221a \u03b1 t x 0 + \u221a 1 \u2212 \u03b1 t t ,\nand T is set to some large number, e.g., 1,000. Note that this simplified loss function has been shown to optimize both DDPM [22] and DDIM [47], though not the actual variational lower bound. For training, the stochastic subcode x T is not needed. We condition the UNet using adaptive group normalization layers (AdaGN), following Dhariwal et al. [11], which extend group normalization [56] by applying channel-wise scaling and shifting on the normalized feature maps h \u2208 R c\u00d7h\u00d7w . Our AdaGN is conditioned on t and z sem :\nAdaGN(h, t, z sem ) = z s (t s GroupNorm(h) + t b ) (7)\nwhere z s \u2208 R c = Affine(z sem ) and (t s , t b ) \u2208 R 2\u00d7c = MLP(\u03c8(t)) is the output of a multilayer perceptron with a sinusoidal encoding function \u03c8. These layers are used throughout the UNet. Please see details in Appendix A.\n\n\nSemantic encoder\n\nThe goal of the semantic encoder Enc(x 0 ) is to summarize an input image into a descriptive vector z sem = Enc(x 0 ) with necessary information to help the decoder p \u03b8 (x t\u22121 |x t , z sem ) denoise and predict the output image. We do not assume any particular architecture for this encoder; however, in our experiments, this encoder shares the same architecture as the first half of our UNet decoder. One benefit of conditioning DDIM with information-rich z sem is more efficient denoising process, which will be discussed further in Section 5.5.\n\n\nStochastic encoder\n\nBesides decoding, our conditional DDIM can also be used to encode an input image x 0 to the stochastic subcode x T by running its deterministic generative process backward (the reverse of Equation 1):\nx t+1 = \u221a \u03b1 t+1 f \u03b8 (x t , t, z sem ) + 1 \u2212 \u03b1 t+1 \u03b8 (x t , t, z sem )(8)\nWe can think of this process as a stochastic encoder because x T is encouraged to encode only the information left out by z sem , which has a limited capacity for compressing stochastic details. By utilizing both semantic and stochastic encoders, our autoencoder can capture an input image to the very last detail while also providing a high-level representation z sem for downstream tasks. Note that the stochastic encoder is not used during training (Equation 6) and is used to compute x T for tasks that require exact reconstruction or inversion, such as real-image manipulation.\n\n\nSampling with diffusion autoencoders\n\nBy conditioning the decoder on z sem , diffusion autoencoders are no longer generative models. So, to sample from our autoencoder, we need an additional mechanism to sample z sem \u2208 R d from the latent distribution. While VAE is an appealing choice for this task, balancing between retaining rich information in the latent code and maintaining the sampling quality in VAE is hard [41,42,45,52]. GAN is another choice, though it complicates training stability, which is one main strength of DPMs. Here, we choose to fit another DDIM, called latent DDIM p \u03c9 (z sem,t\u22121 |z sem,t ), to the latent distribution of z sem = Enc \u03c6 (x 0 ), x 0 \u223c p(x 0 ). Analogous to Equation 5 and 6, training the latent DDIM is done by optimizing L latent with respect to \u03c9:\nL latent = T t=1 E zsem, t \u03c9 (z sem,t , t) \u2212 t 1 (9) where t \u2208 R d \u223c N (0, I), z sem,t = \u221a \u03b1 t z sem + \u221a 1 \u2212 \u03b1 t t ,\nand T is the same as in the DDIM image decoder. For L latent , we empirically found that L 1 works better than L 2 loss. Unlike for 1D/2D images, there is no well-established DPM architecture for non-spatial data, but we have found that deep MLPs (10-20 layers) with skip connections perform reasonably well. The details are provided in Appendix A.1.\n\nWe first train the semantic encoder (\u03c6) and the image decoder (\u03b8) via Equation 6 until convergence. Then, we train the latent DDIM (\u03c9) via Equation 9 with the semantic encoder fixed. In practice, the latent distribution modeled by the latent DDIM is first normalized to have zero mean and unit variance. Unconditional sampling from a diffusion autoencoder is thus done by sampling z sem from the latent DDIM and unnormalizing it, then sampling x T \u223c N (0, I), and finally decoding z = (z sem , x T ) using the decoder.\n\nOur choice of training the latent DDIM post-hoc has a few practical reasons. First, since training the latent DDIM takes only a fraction of the full training time, post-hoc training enables quick experiments on different latent DDIMs with the same diffusion autoencoder. Another reason is to keep z sem as expressive as possible by not imposing any constraints, such as the prior loss in VAE [30], that can compromise the quality of the latent variables.\n\n\nExperiments\n\nWe now turn to assessing the properties of our learned latent space and demonstrating new capabilities, such as attribute manipulation and conditional generation. For fair comparison, the DDIM baseline in our experiments refers to our reimplementation of DDIM [47] based on an improved architecture of Dhariwal et al. [11] with the same UNet hyperparameters as our decoder. In short, the DDIM baseline is similar to our decoder except that it does not take z sem .\n\n\nLatent code captures both high-level semantics and low-level stochastic variations\n\nTo demonstrate that high-level semantics are mostly captured in z sem and very little in x T , we first compute the semantic subcode z sem = Enc(x 0 ) from an input image x 0 . For the stochastic subcode x T , instead of inferring it from the input, we will sample this subcode multiple times The result show that with a fixed z sem , the stochastic subcode x T only affects minor details, such as the hair and skin details, the eyes, or the mouth, but does not change the overall global appearance. And by varying z sem , we obtain completely different people with different facial shapes, illuminations, and overall structures. Quantitative results are discussed in Section 5.4 and Table 2.\nx i T \u223c N (0, I) and decode multiple z i = (z sem , x i T ).\n\nSemantically meaningful latent interpolation\n\nOne desirable property of a useful latent space is the ability to represent semantic changes in the image by a simple linear change in the latent space. For example, by moving along a straight line connecting any two latent codes, we expect a smooth morphing between the corresponding two images. In Figure 4d and Figure 1, we show our interpolation results by encoding two input im-\nages into (z 1 sem , x 1 T ) and (z 2 sem , x 2 T ), then decode z(t) = (Lerp(z 1 sem , z 2 sem ; t), Slerp(x 1 T , x 2 T ; t))\nfor various values of t \u2208 [0, 1], where linear interpolation is used for z sem and spherical linear interpolation is used for x T , following [47].\n\nCompared to DDIM, which produces non-smooth transitions, our method gradually changes the head pose, background, and facial attributes between the two endpoints. The interpolation results from StyleGAN in both W and   W+ spaces are smooth, but the two endpoints do not resemble the input images, whereas ours and DDIM's match the real input images almost exactly. We quantitatively evaluate how smooth our interpolation is in Appendix F.\n\n\nAttribute manipulation on real images\n\nAnother way to assess the relationship between image semantics and linear motion or separability in the latent space is by moving the latent z sem of an image in a particular direction and observing changes in the image [44]. By finding such a direction from the weight vector of a linear classifier trained on latent codes of negative and positive im-ages of a target attribute, e.g., smiling, this operation consequently changes the semantic attribute in the image. There exists specialized techniques for this task [3,37,44,57], but here we aim to showcase the quality and applicability of our latent space by using the simplest linear operation.\n\nWe trained linear classifiers using images and attribute labels from CelebA-HQ [26] and tested on CelebA-HQ and FFHQ [27] in Figure 5. Implementation details and more results can be found in Appendix G. Note that our autoencoder was trained on FFHQ but can generalize to CelebA-HQ without fine-tuning the autoencoder. Our method is able to change local features, such as the mouth for smiling, while keeping the rest of the image and details mostly stationary. For global attributes that involve changing multiple features at the same time, such as aging, our results look highly plausible and realistic. Additionally, we compare the accuracy of these linear classifiers (Appendix E) that take z sem versus those taking StyleGAN's inverted W as input. The AUROC\u2191 over 40 attributes of our method is 0.925 and of StyleGAN-W is 0.891. And we test how much the input's identity is preserved via ArcFace [10] and quantify the manipulation quality in Appendix G.\n\nOne notable advantage of diffusion autoencoders over GAN-based manipulation techniques is the ability to manipulate real images while preserving details irrelevant to the manipulation (e.g., keeping the original hair and background when manipulating facial expression). When GANs are used for such tasks, the details are often altered because real images cannot be faithfully inverted back to the GAN's latent space. Compared to a recent score-based manipulation technique SDEdit [35], which focuses on local edits or translating images from another domain using a forwardbackward sampling trick, our method solves changing semantic attributes by simply modifying the latent code. We also compare qualitatively to D2C [45], which uses NVAE [50] decoder to perform a similar task in our Appendix H.\n\n\nAutoencoding reconstruction quality\n\nAlthough good reconstruction quality of an autoencoder may not necessarily be an indicator of good representation learning, this property plays an important role in many applications, such as compression or image manipulation that requires accurate encoding-decoding abilities. For these tasks, traditional autoencoders that rely on MSE or L 1 loss functions perform poorly and produce blurry results. More advanced autoencoders combine perceptual loss and adversarial loss, e.g., VQGAN [13], or rely on a hierarchy of latent variables, e.g., NVAE [50], VQ-VAE2 [38]. Our diffusion autoencoder is an alternative design that produces a reasonable-size latent code with meaningful and compact semantic subcode and performs competitively with state-ofthe-art autoencoders. The key is our two-level encoding that delegates the reconstruction of less compressible stochastic  Figure 5. Real-image attribute manipulation results on two global attributes (gender, age) and two local attributes (smile, wavy hair) by moving zsem along the positive or negative direction found by linear classifiers. The top two are from FFHQ [27] and the bottom two are from CelebA-HQ [26]. Our method synthesizes highly-plausible and realistic results that preserve an unprecedented level of detail. details to our conditional DDIM.\n\nIn Table 1, we evaluate the reconstruction quality of 1) our diffusion autoencoder, 2) DDIM [47], 3) a pretrained StyleGAN2 [28] (via two types of inversion), 4) VQ-GAN [13], 5) VQ-VAE2 [38], 6) NVAE [50]. Both DDIM and ours were trained on 130M images and used T=100 for decoding. All these models were trained on FFHQ [27] and tested on 30k images from CelebA-HQ [26]. For our method and DDIM, we encoded downscaled test images of size 128\u00d7128 and decoded them back. For the others, we used publicly available pretrained networks for 256\u00d7256 and downscaled the results to the same 128\u00d7128 before comparison. For StyleGAN2, we performed inversion in W [28] and W+ [1,2] spaces on the test images and used the optimized codes for reconstruction. The evaluation metrics are SSIM [53] (\u2191), LPIPS [61] (\u2193), and MSE. NVAE [50] achieves the lowest LPIPS and MSE scores, though it requires orders of magnitude larger latent dimension compared to others. Besides NVAE, our diffusion autoencoders outperform other models on all metrics, and only require T =20 steps to surpass DDIM with T =100 steps (Table 2). Furthermore, we performed ablation studies to investigate 1) the reconstruction quality when only z sem is encoded from the input but x T is sampled from N (0, I) for decoding (Table 2.a), and 2) the effects of varying the dimension of z sem from 64 to 512 (Table 2.b-e) on our autoencoder trained with 48M images for expedience. All configs a)-e) produce realistic results but differ in the degree of fidelity, where higher latent dimensions are better. For config a) with 512D z sem , even though x T is random, the reconstructions still look perceptually close to the input images as measured Table 1. Autoencoding reconstruction quality of models trained on FFHQ [27] and tested on unseen CelebA-HQ [26]. Our model is competitive with state-of-the-art NVAE while producing readily useful high-level semantics in a compact 512D zsem. by LPIPS (also Figure 3). Our reconstruction with a small 64D z sem is already on par with StyleGAN2 inversion in 512D W latent space, suggesting that our diffusion autoencoders are proficient in compression.\n\n\nFaster denoising process\n\nOne useful benefit of conditioning the denoising process with semantic information from z sem is faster generation. One main reason DPMs require many generative steps is because DPMs can only use a Gaussian distribution to approximate p(x t\u22121 |x t ) when T is sufficiently large (\u223c1000). Recent attempts to improve sampling speed focus on finding a better sampling interval or noise schedule [25,29,31,36], or using more efficient solvers to solve the score-based ODE counterpart [25]. Our diffusion autoencoders do not aim to solve this problem directly, nor can they be compared in the same context as generative models that lack access to the target samples. It is, however, worth mentioning the effects Table 2. Ablation study results for a) autoencoding reconstruction quality when xT is not encoded from the input but sampled from N (0, I), and b-e) the effects of varying the dimension of zsem from 64 to 512 on our autoencoder trained with 48M images for expedience. In a), our reconstruction is perceptually close to the input images (LPIPS=0.073) even when xT is random. b-e) suggest that higher zsem dimensions lead to higher fidelity reconstruction. Our diffusion autoencoders with T=20 steps also surpass DDIM with T=100 steps. T=10 T=20 T=50 T=100 T=10 T=20 T=50 T=100 T=10 T=20 T=50 T=100 DDIM (@130M) [47] 0  they have within the DPM framework. Consider a scenario where x 0 is known to the denoising network. The noise prediction task will become trivial, and q(x t\u22121 |x t , x 0 ) is a Gaussian distribution regardless of the number of timesteps [22]. Since our diffusion autoencoders model the distribution p(x t\u22121 |x t , z sem ), it follows that p(x t\u22121 |x t , z sem ) is a better approximation to q(x t\u22121 |x t , x 0 ) than p(x t\u22121 |x t ) when z sem has captured much information about x 0 . Figure 6 shows that a diffusion autoencoder is able to predict x 0 more accurately in fewer steps than DDIM and yield better image quality on four different datasets with the same timesteps T in Table 4.\nModel SSIM \u2191 LPIPS \u2193 MSE \u2193\n\nClass-conditional sampling\n\nThis experiment demonstrates how our framework can be used for few-shot conditional generation and compares to D2C [45], a state-of-the-art DPM-based method for this setup. We follow the problem setup in D2C where the goal is to generate a diverse set of images of a target class, such as female, by utilizing a small number of labeled examples (\u2264 100). The labels can specify both the positives and negatives with respect to the target class (binary scenario) or only the positives (positive and unlabeled, or PU scenario). Given a latent classifier p \u03b3 (c|z sem ) for a target class c, one simple way to do class-conditional sampling is with rejection sampling, as used by D2C. That is, we sample z sem from our latent DDIM and accept this sample with probability p \u03b3 (c|z sem ). We followed D2C's methodology and con- Table 3. FID scores (\u2193) for class-conditional generation on CelebA 64 dataset computed between 5k sampled images and the target subset. \u00b1 represents one standard deviation (n=3). D2C [45] results come from their paper (n=1 run of FID computation on 5k samples). Binary classifier was trained with 50 positives and 50 negatives. Positive-unlabeled (PU) classifier was trained with 100 positives and 10,000 unlabeled examples (as negatives). Naive FIDs were computed between all images and the target subset. ditionally sampled 5k images, then computed FID scores between these images and all the images of the same target class in CelebA dataset (with the same crop used by D2C). We used T = 100 for both latent and image generations. Table 3 shows that our method achieves comparable FID scores to D2C, despite not using any self-supervised contrastive learning used in D2C.\n\n\nUnconditional sampling\n\nTo evaluate the quality of our unconditional samples from diffusion autoencoders, we first sample z sem from the latent DDIM, then decode z = (z sem , x T \u223c N (0, I)) using our decoder. We trained our autoencoders on FFHQ [27], LSUN Horse & Bedroom [60], and CelebA [33]. For each dataset, we computed FID scores between 50k randomly sampled images from the dataset and our 50k generated images. We also varied the timestep T = (10, 20, 50, 100) used in both latent DDIM and our main decoder.\n\nAs shown in Table 4, our diffusion autoencoders are competitive with DDIM baselines and produce higher FID scores in most cases across numbers of timesteps. We also provide as reference our diffusion autoencoders trained with Table 4. FID scores (\u2193) for unconditional generation. Our method is competitive with DDIM baselines. \"+ autoencoding\" refers to diffusion autoencoders that infer ground-truth semantic subcode from the test set and do not sample from the latent DDIM. ground-truth latent variables encoded from the test images, labeled \"+autoencoding.\" In every dataset, perhaps unsurprisingly, conditioning the DDIM decoder with z sem significantly improves the quality with small T s. In Appendix C, we show qualitative results and an additional experiment to verify that the latent DDIM does not memorize its input.\n\n\nRelated work\n\nDenoising diffusion-based generative models [22,46] are closely related to denoising score-based generative models [48]. Models under this family have been shown to produce images with high quality rivaling those of GANs [11] without using the less stable adversarial training. They are also used widely for multiple conditional generation tasks, such as image super-resolution [32,43], image conditional generation [8,35], class-conditional generation in ImageNet dataset [11], and mel-spectrogram conditional speech synthesis [6]. Similar to our work, these methods rely on conditional DPMs; however, most conditioning signals in prior work are known a priori and fixed, while our diffusion autoencoder augments the latent variable with an end-toend learnable signal that the CNN encoder discovers. This puts our work closer to VAE [30], particularly Wehenkel et al. [54] and D2C [45]. While these only utilize DPMs to model the prior distribution or latent representation for another generative model [12], our focus is on how DPMs can be augmented with meaningful latent codes.\n\nOur diffusion autoencoders share common goals with other kinds of autoencoders such as VAE [30], NVAE [50], and VQ-VAE [52] and VQ-VAE2 [38]. While VAEs provide reasonable latent quality and sample quality, they are subject to posterior collapse [52] and prior holes problems [45] , whereas DPMs are not. VQ-VAE with discrete latent variables was proposed to deal with these problems by fitting an autoregressive Pixel-CNN model to the latent variable post-hoc [51]. Fitting the latent variable post-hoc is also used in our work, but we utilize another DPM instead of an autoregressive model. Rich image representations are useful for many downstream tasks; for example, VAE are often used in model-based reinforcement learning [14,17,18] for predicting future outcomes of the environment. VQ-VAE's latent variables are used as a means for video generation tasks [59]. Our diffusion autoencoders also provide useful representations with an added ability to decode the representations back near perfectly.\n\nBesides producing impressive image samples, GANs [15] have been shown to learn meaningful latent spaces [27] with extensive studies on multiple derived spaces [24,57] and various knobs and controls for conditional human face generation [21,37,55]. Encoding an image to the GAN's latent space requires an optimization-based inversion process [28,58] or an external image encoder [39], which has limited reconstruction fidelity (or yields high-dimensional codes outside the learned manifold). This problem may be related to the GAN's limited latent size and mode-collapse problem, where the latent space only partially covers the support of training samples. Diffusion autoencoders do not have this problem and can readily encode any image without any additional error-prone optimization.\n\n\nLimitations & Discussion\n\nWhen encoding images that are out of the training distribution, our diffusion autoencoders can still reconstruct the images well, owing to the high-dimensional stochastic subcode from DDIM. However, both the inferred semantic and stochastic subcodes may fall outside the learned distributions, resulting in a poor representation that can no longer be interpreted or interpolated. While our choice of using non-spatial latent code is suitable for learning global semantics, certain image and spatial reasoning tasks may require more precise local latent variables. For these tasks, incorporating 2D latent maps can be beneficial.\n\nFor image generation, one unique feature of StyleGAN that is lacking from our diffusion autoencoders is the ability to control scale-specific generation. In terms of generation speed, our framework has significantly reduced the timesteps needed to achieve high-quality samples from our DDIM but still lacks behind GANs, which only require a single generator's pass to generate an image.\n\nIn conclusion, we have presented diffusion autoencoders that can separately infer both semantics and stochastic information from an input image. In contrast to DPMs and high-fidelity autoencoders like NVAE, our latent representation allows near-exact decoding while containing compact semantics readily useful for downstream tasks. These properties enable simple solutions to various real-image editing tasks without requiring GANs and their error-prone inversion. Our framework also improves denoising efficiency and retains competitive unconditional sampling of DPMs.\n\n\nA. Diffusion autoencoder architectures\n\nThe baseline diffusion models and our diffusion autoencoders are based on the same DDIM model [11] (publicly available at https://github.com/openai/guideddiffusion). The architecture is specified in Table 5. We selected the hyperparameters differently due to the limited computational resources. Note that we used the linear \u03b2 scheduler as in Ho et al. [22], but we do observe improvements using the cosine \u03b2 scheduler [36] in our preliminary results.\n\n\nResBlock + AdaGN\n\nInput Output  \n\n\nA.1. Latent DDIM architectures\n\nFor latent DDIMs, we experimented with multiple architectures including MLP, MLP + skip connections, and projecting z sem into a spatial vector before using a CNN or UNet. We have found that MLP + skip connection performed reasonably well while being very fast (See unconditional samples in Figure 20). The architecture is specified in Table 6. Each layer of the MLP has a skip connection from the input, which simply concatenates the input with the output from the previous layer. The network is conditioned on t by scaling the hidden representations to help denoising.\n\nThe architecture is shown in Figure 8 and the hyperparameters are shown in Table 6. We have compared different \u03b2 schedulers including Linear [22], and a constant of 0.008 schedulers. (We found that Cosine [36] scheduler underperformed during preliminary experiments for our latent DDIM.) We compared the two schedulers on the z sem of LSUN's Horse 128 diffusion autoencoder model. The latent DDIM is MLP + Skip with 10 layers and 2048 hidden nodes. The validation FID score for using linear beta schedule is 13.36, whereas for constant 0.008 scheduler is 10.50. We found that an L1 loss performed better for the latent DDIM with FID of 11.65 vs 13.36 of MSE (Though, the main autoencoder uses MSE loss). We provide the hyperparameter tuning results of the MLP + Skip network: Even though these results come from LSUN's Horse dataset, we found that similar settings worked well across datasets. We only tuned the network depth and the total training iterations for each dataset separately, a common practice in StyleGAN's training on these datasets.\n\n\nLatent model FID\n\n\nA.2. Classifiers\n\nWe always use linear classifiers (logistic regression) trained on z sem space in all relevant experiments, which are attribute manipulation and class-conditional sampling. For training, z sem is first normalized so that its entire distribution has zero mean and unit variance before putting to the classifier. For the PU classifier, we oversampled the positive data points to match the negative ones to maintain the  That is, we reject samples with the target class probabilities less than 0.5 before performing rejection sampling.\n\n\nB. Computation resources\n\nWe used four Nvidia V100s for both diffusion autoencoders and DDIM and a single Nvidia RTX 2080 Ti for the latent DDIMs. Training the latent DDIMs takes only a fraction of the computational resources compared to the diffusion autencoders. Table 7 shows the throughputs of DDIM and diffusion autoencoders. Diffusion autoencoders were around 20% slower to train than DDIM counterparts due to the additional semantic encoder. The total GPU-hours can be computed by multiplying the throughput with the number of training images for each model provided in Table 6. \n\n\nC. Does the latent DDIM memorize its input?\n\nTo verify if our diffusion autoencoder and latent DDIM can generate novel samples and do not simply memorize the input, we generate image samples and compare them to their nearest neighbors in the training set ( Figure 9). (They  should look different). To find nearest neighbors, we used three different metrics: 1) lowest LPIPS [61] in the image space, 2) lowest MSE in the image space, 3) lowest MSE in the semantic subspace (z sem ). We have found that our autoencoder can generate substantially different images from the training set, suggesting no memorization problem.\n\nD. What is encoded in the stochastic subcode? Figure 10 shows the stochastic variations induced by varying x T given the same z sem . We also compute the mean and standard deviation of these variations. All generated images look realistic and x T changes only minor details, such as the hair pattern, while keeping the overall structure the same.\n\n\nE. Predictive power of the semantic subcode\n\nWe assess the quality of our proposed z sem via linear classification performance, which has been extensively used to evaluate the quality of learned representations [4,7,16,19]. In Table 8, we measure the performance of linear classifiers trained on z sem and StyleGAN's latent code in W space (obtained from an inversion process [28]) using Area Under the Receiver Operating Characteristic (AUROC) on the CelebA-HQ's 40 attributes with 30% test data out of 30,000 total data points. The classifiers were trained on z-normalized latent vectors until convergence with Adam optimizer (learning rate 1e-3). For most classes, the linear classifiers using z sem outperform those using StyleGAN's W with weighted averages of 0.92 vs 0.89. This suggests that z sem contains attribute-specific information that is more readily predictive than that of StyleGAN's W. \n\n\nClass\n\n#Positives z sem W\n\nIn Table 9, we measure FID scores \u2193 between the manipulated (to be positive) and real positive images, as well as FID scores between real negative and real positive images as baselines for five different attributes from CelebA-HQ [26]. While we expect the manipulated images to get closer to the positive images, we also expect them to not deviate too far from the negative as some original content, such as the background, the identity, should be retained. Hence, we also provide FID scores \u2193 between the manipulated images and the real negative images. Our z sem manipulated images are closer to the real positive images for 4 out of 5 attributes than those of StyleGAN-W while better preserving the original contents in all 5 attributes.\n\nIdentity preservation. We quantitatively evaluate how well the input's identity is preserved under the manipulation by computing the cosine similarity \u2191 between the ArcFace embeddings [10] of the input and its manipulated version, following [39]. Table 10 shows our scores on CelebA-HQ images of 4 classes used in Figure 5: Male, Smiling, Wavy Hair, Young. For this experiment, we use the original W space inversion of StyleGAN that produces the same 512D latent code as our z sem . Their lower scores can be attributed partly to the poor inversion in this space. \n\n\nH. Attribute manipulation comparison to D2C\n\nWe show a qualitative comparison to D2C [45] on realimage attribute manipulation in Figure 17. These official D2C's results are from https://d2c-model.github. io/. The results of the other baselines are also borrowed from the same website.\n\n\nI. Class-conditional samples\n\nWe show our conditional samples of Blond, Non-blond, Famale, and Male classes in Figures 18, 19. This is done by training a linear classifier for each attribute using only 100 labeled examples and 10k unlabeled examples, similar to the few-shot experiment done in D2C [45]. The details are in Section 5.6 in the main paper.\n\n\nJ. Unconditional samples\n\nWe show uncurated unconditional samples from our diffusion autoencoder on FFHQ [27], LSUN-Bed [60], and LSUN-horse [60] in Figure 20, 21, 22.\n\n\nK. Encoding out-of-distribution images\n\nAs discussed in the main paper, when encoding images that are out of the training distribution, our diffusion autoencoders can still reconstruct the images well but the inferred semantic and stochastic subcodes may fall outside the learned distributions. We simulate simple out-ofdistribution samples by translating an FFHQ face image in Figure 11 and by encoding a horse image using our diffusion autoencoder trained on face images in Figure 12. The reconstruction results still look very close to the input images, but the noise maps x T show some residual details and do not look normally distributed.\n\n\nInput\n\nRecon. Figure 11. Noise maps xT when the input face image is shifted to the right to simulate out-of-distribution input image.\n\nReal Input using \"horse\" autoencoder Recon.\n\nRecon.\n\nusing \"face\" autoencoder Figure 12. We test how the noise map xT of a horse image would look if it is encoded by a diffusion autoencoder trained on face images. Both reconstructions look reasonably close to the input image, but xT from the face autoencoder does not look normally distributed and contains details from the input image.\n\n\nL. Potential negative impact\n\nThe ability to generate image samples and manipulate attributes of a real image can be used to generate synthetic media, such as deepfakes. We realize the potential negative impact and further conducted a study to determine the difficulty in distinguishing real and synthesized images from our method, as well as discussing some possible directions.\n\nTo detect fake images, we train a CNN architecture based on ResNet-50 [20], which is pretrained on ImageNet [9], followed by a linear layer used for classification. Our training dataset consists of \"real\" images from FFHQ256 [27]  and \"fake\" images from either the unconditional sampling experiment (Section 5.7) or the attribute manipulation experiment (Section 5.3). This dataset contains 20k images: 10K images for each real and fake. The dataset is randomly split into train, test, and validation class-balanced subsets with the ratios of 0.7, 0.2, and 0.1, respectively. The classifier is trained using a binary cross-entropy loss function with an SGD optimizer (learning rate 0.001, momentum 0.9, batchsize 64). Fake detection accuracy is reported here:\n\nMethod T=100 T=200 T=500\n\nUnconditional sampling 0.9551 0.9483 0.9313 Attribute manipulation 0.9950 0.9643 0.9213\n\nThe results suggest that even though the generated samples look highly realistic, there could be some certain artifacts that can be easily detected by another neural network. Diffusion-based models also do not have a mechanism to purposely fool a classifier or discriminator like GANs do, and a neural network-based technique is currently found to be > 90% effective at detecting fake images from diffusion models. Note that sampling with higher T leads to samples that are harder to detect. A further study on how easy it is to circumvent detection through adversarial training and an analysis on those giveaway artifacts will be useful for future technical safeguards.  Input Ours D2C StyleGAN2 NVAE Input Ours D2C StyleGAN2 NVAE Figure 17. Comparison on attribute manipulation (blond hair) between our method, D2C [45], StyleGAN2 [28], and NVAE [50].     \n\n\nNon-blond class Blond class\n\n\nFigure 3 shows the variations induced by varying x T given the same z sem , as well as the variations from different z sem .\n\nFigure 3 .\n3Reconstruction results and the variations induced by changing the stochastic subcode xT . Each row corresponds to a different zsem, which completely changes the person, whereas changing the stochastic subcode xT only affects minor details.(a) StyleGAN2 interpolation after W space inversion. (b) StyleGAN2 interpolation after W+ space inversion. (c) DDIM interpolation. (d) Our diffusion autoencoder interpolation.\n\nFigure 4 .\n4Interpolation between two real images. In contrast to StyleGAN2 and DDIM, our method produces smooth and consistent results with well-preserved original details from both images.\n\n\nOur diffusion autoencoder predicting x 0 .\n\nFigure 6 .\n6Predicted x0 at t9,8,7,5,2,0 (T =10). By conditioning on zsem, our method predicts images that resemble x0 much faster.\n\n\nResBlock + AdaGN. The residual path is not depicted.\n\nFigure 7 .\n7Architecture overview of our diffusion autoencoder.\n\nFigure 8 .\n8Architecture overview of our latent DDIM.\n\nFigure 9 .\n9Does latent DDIM memorize its input? For each sampled image at the top, we find its closest images from the training set in terms of LPIPS, MSE in the image space, and MSE in the semantic subcode zsem space. The sampled images do not closely resemble any of the training images, suggesting that our latent DDIM does not memorize the input samples.\n\nFigure 10 .\n10Reconstruction results and the variations induced by changing the stochastic subcode xT .\n\nFigure 13 .\n13Real-image attribute manipulation for attributes: Wavy Hair, Smiling, Male, Young.\n\nFigure 14 .Figure 15 .Figure 16 .\n141516Real-image interpolation on FFHQ dataset[27] Real-image interpolation on LSUN bedroom-128[60] Real-image interpolation on LSUN horse-128[60] \n\nFigure 18 .\n18Class-conditional generation using 100 positive labeled examples and 10k unlabeled examples on Blond and Non-blond from CelebA [26]. These results are uncurated. Please see Section 5.6 in the main paper for details.Male class Female class\n\nFigure 19 .\n19Class-conditional generation using 100 positive labeled examples and 10k unlabeled examples on Female and Male from CelebA[26]. These results are uncurated. Please see Section 5.6 in the main paper for details.\n\nFigure 20 .\n20Unconditional samples (uncurated) from our diffusion autoencoder and latent DDIM trained on FFHQ-256[27].\n\nFigure 21 .\n21Unconditional samples (uncurated) from our diffusion autoencoder and latent DDIM trained on LSUN bedroom-128[60].\n\nFigure 22 .\n22Unconditional samples (uncurated) from our diffusion autoencoder and latent DDIM trained on LSUN horse-128[60].\n\n\nModelLatent dim SSIM \u2191 LPIPS \u2193 MSE \u2193StyleGAN2 (W) [28] \n512 0.677 \n0.168 \n0.016 \nStyleGAN2 (W+) [28] \n7,168 0.827 \n0.114 \n0.006 \nVQ-GAN [13] \n65,536 0.782 \n0.109 3.61e-3 \nVQ-VAE2 [38] \n327,680 0.947 \n0.012 4.87e-4 \nNVAE [50] \n6,005,760 0.984 \n0.001 4.85e-5 \nDDIM (T=100, 128 2 ) [47] \n49,152 0.917 \n0.063 \n0.002 \nOurs (T=100, 128 2 , no x T ) \n512 0.677 \n0.073 \n0.007 \nOurs (T=100, 128 2 ) \n49,664 0.991 \n0.011 6.07e-5 \n\n\n\nTable 5 .\n5Network architecture of our diffusion autoencoder based on the improved DPM architecture of Dhariwal et al.[11].Parameter \nCelebA 64 FFHQ 64 FFHQ 128 \nHorse 128 \nBedroom 128 \nFFHQ256 \n\nBatch size \n128 \n128 \n128 \n128 \n128 \n64 \nBase channels \n64 \n64 \n128 \n128 \n128 \n128 \nChannel multipliers \n[1,2,4,8] \n[1,2,4,8] \n[1,1,2,3,4] \n[1,1,2,3,4] \n[1,1,2,3,4] \n[1,1,2,2,4,4,] \nAttention resolution \n[16] \n[16] \n[16] \n[16] \n[16] \n[16] \nImages trained \n72M \n48M \n130M \n130M \n120M \n90M \nEncoder base ch \n64 \n64 \n128 \n128 \n128 \n128 \nEnc. attn. resolution \n[16] \n[16] \n[16] \n[16] \n[16] \n[16] \nEncoder ch. mult. \n[1,2,4,8,8] [1,2,4,8,8] [1,1,2,3,4,4] [1,1,2,3,4,4] \n[1,1,2,3,4,4] \n[1,1,2,2,4,4,4] \nz sem size \n512 \n512 \n512 \n512 \n512 \n512 \n\u03b2 scheduler \nLinear \nLinear \nLinear \nLinear \nLinear \nLinear \nLearning rate \n1e-4 \nOptimizer \nAdam (no weight decay) \nTraining T \n1000 \nDiffusion loss \nMSE with noise prediction \nDiffusion var. \nNot important for DDIM \n\n\n\nTable 6 .\n6Network architecture of our latent DDIM.Parameter \nCelebA \nFFHQ \nHorse \nBedroom \n\nBatch size \n512 \n256 \n2048 \n2048 \nz sem trained \n300M \n100M \n2000M \n2000M \nMLP layers (N ) \n10 \n10 \n20 \n20 \nMLP hidden size \n2048 \nz sem size \n512 \n\u03b2 scheduler \nConstant 0.008 \nLearning rate \n1e-4 \nOptimizer \nAdamW (weight decay = 0.01) Adam (no weight decay) \nTrain Diff T \n1000 \nDiffusion loss \nL1 loss with noise prediction \nDiffusion var. \nNot important for DDIM \n\nbalance. For conditional generation, we follow D2C and \napply rejection sampling after an additional thresholding. \n\n\nTable 7 .\n7Throughputs of DDIM and diffusion autoencoders.Model \n\nDDIMs \nDiffusion autoencoders \nThroughput \nThroughput \n(imgs/sec./V100) \n(imgs/sec./V100) \n\nFFHQ-64 \n160 \n128 \nFFHQ-128 \n51 \n41.65 \nFFHQ-256 \n-\n10.08 \nHorse-128 \n51 \n41.65 \nBedroom-128 \n51 \n41.65 \n\n\n\nTable 8 .\n8Classification AUROC \u2191 on CelebA-HQ's 40 attributes of linear classifiers trained on our zsem vs. StyleGAN's latent code in W space (obtained via inversion).\n\nTable 10 .\n10Average cosine similarity \u2191 of the ArcFace embeddings[10] of the input and its manipulated version.Model \nMale Smiling Wavy Hair Young \n\nStyleGAN-W 0.4174 0.7850 \n0.8544 \n0.6955 \nOurs \n0.6247 0.8160 \n0.9821 \n0.8922 \n\n\n\nTable 9 .\n9Image manipulation FID scores \u2193.Mode \nModel \nMale \nSmiling \nWavy Hair \nYoung \nBlond Hair \n\nPositive vs negative \n95.82 \n11.15 \n25.04 \n36.75 \n39.65 \n\nManipulated vs. positive \nOurs \n52.85 \n9.19 \n20.80 \n20.68 \n33.51 \nStyleGAN-W \n42.90 \n18.52 \n27.10 \n31.15 \n33.89 \n\nManipulated vs. negative \nOurs \n23.15 \n7.25 \n4.89 \n11.81 \n6.79 \nStyleGAN-W \n66.92 \n22.15 \n20.70 \n31.15 \n27.54 \n\n\nAppendix F. Real-image interpolation resultsWe show interpolation results on real images from FFHQ[27](Figure 14), LSUN-Bedroom[60](Figure 15) and LSUN-Horse[60](Figure 16). Our method can handle challenging morphing between people with and without glasses, bedrooms from different styles and angles, or horses with different body poses.To quantify the smoothness of the interpolation, we use Perceptual Path Length (PPL) introduced in StyleGAN[27], to measure the perceptual difference in the image as we move along the interpolation path by a small = 10 \u22124 in the latent space. Specifically, we compute the following expectation over multiple sampled pairs of latent codes (z 1 , z 2 ) and t \u2208 [0, 1]:where G is the decoder, and d computes the perceptual distance based on the VGG16 network. slerp(\u00b7) denotes spherical interpolation. We compute this expected value using 200 samples (400 images) from FFHQ. Our method significantly outperforms DDIM in terms of interpolation smoothness as shown below.Model DDIM Ours PPL 2,634.14 613.73G. Real-image attribute manipulation resultsWe show real-image attribute manipulation results on FFHQ[27]and CelebA-HQ[26]inFigure 5for smiling, wavy hair, aging, and gender change. For more results, please visit https://Diff-AE.github.io/. Our generated results look highly realistic and plausible.FID between the input and its manipulated version. To assess the quality of our manipulated results, we compare their distribution with that of real images with the target positive attribute, such as smiling. Our manipulation is done by moving z sem linearly along the target direction w, found by training a linear classifier (logistic regression) y = w z + b to predict the target attribute using a labeled dataset. The stochastic subcode x T is kept intact. Given z, its manipulated version is produced by decoding z = z + sw, where s \u2208 R controls the degree of manipulation. For this experiment, each input image will be manipulated by a different s i so that the manipulated result reaches the same degree of the target attribute (e.g., similarly big smile) Specifically, we pick s i so that the logit confidence of its z i equals the median confidence of all real positive images:In our implementation, we use normalized z instead of z for this operation and unnormalize it before decoding.\nIm-age2StyleGAN: How to Embed Images Into the StyleGAN Latent Space. Rameen Abdal, Yipeng Qin, Peter Wonka, Rameen Abdal, Yipeng Qin, and Peter Wonka. Im- age2StyleGAN: How to Embed Images Into the StyleGAN Latent Space? pages 4432-4441, 2019.\n\nIm-age2StyleGAN++: How to Edit the Embedded Images. Rameen Abdal, Yipeng Qin, Peter Wonka, Rameen Abdal, Yipeng Qin, and Peter Wonka. Im- age2StyleGAN++: How to Edit the Embedded Images? pages 8296-8305, 2020.\n\nStyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated Images using Conditional Continuous Normalizing Flows. Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka, arXiv:2008.02401ACM Transactions on Graphics. 403Rameen Abdal, Peihao Zhu, Niloy Mitra, and Peter Wonka. StyleFlow: Attribute-conditioned Exploration of StyleGAN- Generated Images using Conditional Continuous Normaliz- ing Flows. ACM Transactions on Graphics, 40(3):1-21, May 2021. arXiv: 2008.02401.\n\nUnderstanding intermediate layers using linear classifier probes. Guillaume Alain, Yoshua Bengio, arXiv:1610.01644arXiv: 1610.01644Novcs, statGuillaume Alain and Yoshua Bengio. Understand- ing intermediate layers using linear classifier probes. arXiv:1610.01644 [cs, stat], Nov. 2018. arXiv: 1610.01644.\n\nDaniel Tarlow, and Rianne van den Berg. Structured Denoising Diffusion Models in Discrete State-Spaces. Jacob Austin, Daniel D Johnson, Jonathan Ho, Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tar- low, and Rianne van den Berg. Structured Denoising Diffu- sion Models in Discrete State-Spaces. May 2021.\n\nWaveGrad: Estimating Gradients for Waveform Generation. Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, William Chan, arXiv:2009.00713arXiv: 2009.00713cs, eess, statNanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Moham- mad Norouzi, and William Chan. WaveGrad: Estimating Gradients for Waveform Generation. arXiv:2009.00713 [cs, eess, stat], Oct. 2020. arXiv: 2009.00713.\n\nA Simple Framework for Contrastive Learning of Visual Representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, PMLRProceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningTing Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A Simple Framework for Contrastive Learn- ing of Visual Representations. In Proceedings of the 37th In- ternational Conference on Machine Learning, pages 1597- 1607. PMLR, Nov. 2020. ISSN: 2640-3498.\n\nJooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon, Ilvr, arXiv:2108.02938arXiv: 2108.02938Conditioning Method for Denoising Diffusion Probabilistic Models. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. ILVR: Condi- tioning Method for Denoising Diffusion Probabilistic Models. arXiv:2108.02938 [cs], Sept. 2021. arXiv: 2108.02938.\n\nImageNet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE Conference on Computer Vision and Pattern Recognition. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical im- age database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248-255, June 2009. ISSN: 1063-6919.\n\nArcFace: Additive Angular Margin Loss for Deep Face Recognition. Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou, Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep Face Recognition. pages 4690-4699, 2019.\n\nDiffusion Models Beat GANs on Image Synthesis. Prafulla Dhariwal, Alexander Quinn, Nichol , Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion Models Beat GANs on Image Synthesis. May 2021.\n\nImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis. Patrick Esser, Robin Rombach, Andreas Blattmann, Bj\u00f6rn Ommer, Patrick Esser, Robin Rombach, Andreas Blattmann, and Bj\u00f6rn Ommer. ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis. May 2021.\n\nTaming Transformers for High-Resolution Image Synthesis. Patrick Esser, Robin Rombach, Bjorn Ommer, Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming Transformers for High-Resolution Image Synthesis. pages 12873-12883, 2021.\n\nLearning to Predict Without Looking Ahead: World Models Without Forward Prediction. Daniel Freeman, David Ha, Luke Metz, Advances in Neural Information Processing Systems. Curran Associates, Inc32Daniel Freeman, David Ha, and Luke Metz. Learning to Pre- dict Without Looking Ahead: World Models Without For- ward Prediction. In Advances in Neural Information Pro- cessing Systems, volume 32. Curran Associates, Inc., 2019.\n\nGenerative Adversarial Nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Curran Associates, Inc27Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.\n\nBootstrap Your Own Latent -A New Approach to Self-Supervised Learning. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Remi Koray Kavukcuoglu, Michal Munos, Valko, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap Your Own Latent -A New Approach to Self-Supervised Learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, vol- ume 33, pages 21271-21284. Curran Associates, Inc., 2020.\n\nRecurrent World Models Facilitate Policy Evolution. David Ha, J\u00fcrgen Schmidhuber, Advances in Neural Information Processing Systems. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettCurran Associates, Inc31David Ha and J\u00fcrgen Schmidhuber. Recurrent World Models Facilitate Policy Evolution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\n\nLearning Latent Dynamics for Planning from Pixels. Danijar Hafner, Timothy P Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Vil- legas, David Ha, Honglak Lee, and James Davidson. Learn- ing Latent Dynamics for Planning from Pixels. Jan. 2019.\n\nMomentum Contrast for Unsupervised Visual Representation Learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for Unsupervised Visual Rep- resentation Learning. pages 9729-9738, 2020.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. pages 770- 778, 2016.\n\nAttGAN: Facial Attribute Editing by Only Changing What You Want. Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, Xilin Chen, Conference Name: IEEE Transactions on Image Processing. 28IEEE Transactions on Image ProcessingZhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, and Xilin Chen. AttGAN: Facial Attribute Editing by Only Changing What You Want. IEEE Transactions on Im- age Processing, 28(11):5464-5478, Nov. 2019. Conference Name: IEEE Transactions on Image Processing.\n\nDenoising Diffusion Probabilistic Models. Jonathan Ho, Ajay Jain, Pieter Abbeel, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif- fusion Probabilistic Models. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6840-6851. Curran Associates, Inc., 2020.\n\nCascaded Diffusion Models for High Fidelity Image Generation. Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, Tim Salimans, Journal of Machine Learning Research. 2347Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded Diffu- sion Models for High Fidelity Image Generation. Journal of Machine Learning Research, 23(47):1-33, 2022.\n\nGANSpace: Discovering Interpretable GAN Controls. Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. GANSpace: Discovering Interpretable GAN Controls. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 9841-9850. Curran Associates, Inc., 2020.\n\nAlexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Pich\u00e9-Taillefer, arXiv:2105.14080arXiv: 2105.14080Tal Kachman, and Ioannis Mitliagkas. Gotta Go Fast When Generating Data with Score-Based Models. cs, math, statAlexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Pich\u00e9-Taillefer, Tal Kachman, and Ioannis Mitliagkas. Gotta Go Fast When Gen- erating Data with Score-Based Models. arXiv:2105.14080 [cs, math, stat], May 2021. arXiv: 2105.14080.\n\nProgressive Growing of GANs for Improved Quality. Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen, arXiv:1710.10196arXiv: 1710.10196Stability, and Variation. cs, statTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive Growing of GANs for Improved Quality, Stabil- ity, and Variation. arXiv:1710.10196 [cs, stat], Feb. 2018. arXiv: 1710.10196.\n\nA Style-Based Generator Architecture for Generative Adversarial Networks. Tero Karras, Samuli Laine, Timo Aila, Tero Karras, Samuli Laine, and Timo Aila. A Style- Based Generator Architecture for Generative Adversarial Networks. pages 4401-4410, 2019.\n\nAnalyzing and Improving the Image Quality of StyleGAN. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and Improving the Image Quality of StyleGAN. pages 8110-8119, 2020.\n\nVariational Diffusion Models. P Diederik, Tim Kingma, Ben Salimans, Jonathan Poole, Ho, Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational Diffusion Models. May 2021.\n\nAuto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, Diederik P. Kingma and Max Welling. Auto-Encoding Vari- ational Bayes. Dec. 2013.\n\nW Y Max, Jun Lam, Rongjie Wang, Dan Huang, Dong Su, Yu, arXiv:2108.11514arXiv: 2108.11514Bilateral Denoising Diffusion Models. cs, eessMax W. Y. Lam, Jun Wang, Rongjie Huang, Dan Su, and Dong Yu. Bilateral Denoising Diffusion Mod- els. arXiv:2108.11514 [cs, eess], Sept. 2021. arXiv: 2108.11514.\n\nSRDiff: Single image super-resolution with diffusion probabilistic models. Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, Yueting Chen, Neurocomputing. 479Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. SRDiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing, 479:47-59, Mar. 2022.\n\nDeep Learning Face Attributes in the Wild. Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang, 2015 IEEE International Conference on Computer Vision (ICCV). Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 3730-3738, Dec. 2015. ISSN: 2380-7504.\n\nDiffusion Probabilistic Models for 3D Point Cloud Generation. Shitong Luo, Wei Hu, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Nashville, TN, USAIEEEShitong Luo and Wei Hu. Diffusion Probabilistic Models for 3D Point Cloud Generation. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2836-2844, Nashville, TN, USA, June 2021. IEEE.\n\nSDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon, Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided Im- age Synthesis and Editing with Stochastic Differential Equa- tions. Sept. 2021.\n\nImproved Denoising Diffusion Probabilistic Models. Alexander Quinn, Nichol , Prafulla Dhariwal, PMLRProceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningAlexander Quinn Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models. In Proceedings of the 38th International Conference on Machine Learning, pages 8162-8171. PMLR, July 2021. ISSN: 2640-3498.\n\nStyleCLIP: Text-Driven Manipulation of StyleGAN Imagery. Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski, Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery. pages 2085-2094, 2021.\n\nGenerating Diverse High-Fidelity Images with VQ-VAE-2. Ali Razavi, Aaron Van Den Oord, Oriol Vinyals, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc32Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Gener- ating Diverse High-Fidelity Images with VQ-VAE-2. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems, volume 32. Curran Associates, Inc., 2019.\n\nEncoding in Style: A StyleGAN Encoder for Image-to-Image Translation. Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, Daniel Cohen-Or, Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in Style: A StyleGAN Encoder for Image-to-Image Transla- tion. pages 2287-2296, 2021.\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015. Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. FrangiChamSpringer International PublishingOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U- Net: Convolutional Networks for Biomedical Image Seg- mentation. In Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi, editors, Medical Image Com- puting and Computer-Assisted Intervention -MICCAI 2015, Lecture Notes in Computer Science, pages 234-241, Cham, 2015. Springer International Publishing.\n\nMihaela Rosca, Balaji Lakshminarayanan, Shakir Mohamed, arXiv:1802.06847arXiv: 1802.06847Distribution Matching in Variational Inference. cs, statMihaela Rosca, Balaji Lakshminarayanan, and Shakir Mo- hamed. Distribution Matching in Variational Inference. arXiv:1802.06847 [cs, stat], June 2019. arXiv: 1802.06847.\n\nSimple and Effective VAE Training with Calibrated Decoders. Kostas Oleh Rybkin, Sergey Daniilidis, Levine, PMLRProceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningOleh Rybkin, Kostas Daniilidis, and Sergey Levine. Sim- ple and Effective VAE Training with Calibrated Decoders. In Proceedings of the 38th International Conference on Ma- chine Learning, pages 9179-9189. PMLR, July 2021. ISSN: 2640-3498.\n\nImage Super-Resolution via Iterative Refinement. Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, Mohammad Norouzi, arXiv:2104.07636arXiv: 2104.07636cs, eessChitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image Super-Resolution via Iterative Refinement. arXiv:2104.07636 [cs, eess], June 2021. arXiv: 2104.07636.\n\nInterpreting the Latent Space of GANs for Semantic Face Editing. Yujun Shen, Jinjin Gu, Xiaoou Tang, Bolei Zhou, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Inter- preting the Latent Space of GANs for Semantic Face Edit- ing. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9240-9249, June 2020. ISSN: 2575-7075.\n\nD2C: Diffusion-Decoding Models for Few-Shot Conditional Generation. Abhishek Sinha, Jiaming Song, Chenlin Meng, Stefano Ermon, Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano Ermon. D2C: Diffusion-Decoding Models for Few-Shot Conditional Generation. May 2021.\n\nDeep Unsupervised Learning using Nonequilibrium Thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, PMLRProceedings of the 32nd International Conference on Machine Learning. the 32nd International Conference on Machine LearningJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, pages 2256-2265. PMLR, June 2015. ISSN: 1938-7228.\n\nDenoising Diffusion Implicit Models. Jiaming Song, Chenlin Meng, Stefano Ermon, Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois- ing Diffusion Implicit Models. Sept. 2020.\n\nGenerative Modeling by Estimating Gradients of the Data Distribution. Yang Song, Stefano Ermon, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alch\u00e9-Buc, E. Fox, and R. GarnettCurran Associates, Inc32Yang Song and Stefano Ermon. Generative Modeling by Es- timating Gradients of the Data Distribution. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d' Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Process- ing Systems, volume 32. Curran Associates, Inc., 2019.\n\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole, arXiv:2011.13456arXiv: 2011.13456Score-Based Generative Modeling through Stochastic Differential Equations. cs, statYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab- hishek Kumar, Stefano Ermon, and Ben Poole. Score- Based Generative Modeling through Stochastic Differential Equations. arXiv:2011.13456 [cs, stat], Feb. 2021. arXiv: 2011.13456.\n\nNVAE: A Deep Hierarchical Variational Autoencoder. Arash Vahdat, Jan Kautz, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinCurran Associates, Inc33Arash Vahdat and Jan Kautz. NVAE: A Deep Hierarchi- cal Variational Autoencoder. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 19667-19679. Curran Associates, Inc., 2020.\n\nConditional Image Generation with PixelCNN Decoders. Aaron Van Den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Koray Kavukcuoglu, Alex Vinyals, Graves, Advances in Neural Information Processing Systems. D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. GarnettCurran Associates, Inc29Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, ko- ray kavukcuoglu, Oriol Vinyals, and Alex Graves. Condi- tional Image Generation with PixelCNN Decoders. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.\n\nNeural Discrete Representation Learning. Aaron Van Den Oord, Oriol Vinyals, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural Discrete Representation Learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish- wanathan, and R. Garnett, editors, Advances in Neural Infor- mation Processing Systems, volume 30. Curran Associates, Inc., 2017.\n\nImage quality assessment: from error visibility to structural similarity. A C Zhou Wang, H R Bovik, E P Sheikh, Simoncelli, Conference Name: IEEE Transactions on Image Processing. 13Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to struc- tural similarity. IEEE Transactions on Image Processing, 13(4):600-612, Apr. 2004. Conference Name: IEEE Trans- actions on Image Processing.\n\nDiffusion Priors In Variational Autoencoders. Antoine Wehenkel, Gilles Louppe, Antoine Wehenkel and Gilles Louppe. Diffusion Priors In Variational Autoencoders. June 2021.\n\nRelGAN: Multi-Domain Imageto-Image Translation via Relative Attributes. Po-Wei Wu, Yu-Jing Lin, Che-Han Chang, Edward Y Chang, Shih-Wei Liao, Po-Wei Wu, Yu-Jing Lin, Che-Han Chang, Edward Y. Chang, and Shih-Wei Liao. RelGAN: Multi-Domain Image- to-Image Translation via Relative Attributes. pages 5914- 5922, 2019.\n\nGroup Normalization. Yuxin Wu, Kaiming He, Yuxin Wu and Kaiming He. Group Normalization. pages 3-19, 2018.\n\nStyleSpace Analysis: Disentangled Controls for Style-GAN Image Generation. Zongze Wu, Dani Lischinski, Eli Shechtman, Zongze Wu, Dani Lischinski, and Eli Shechtman. StyleSpace Analysis: Disentangled Controls for Style- GAN Image Generation. pages 12863-12872, 2021.\n\nWeihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, Ming-Hsuan Yang, arXiv:2101.05278arXiv: 2101.05278GAN Inversion: A Survey. Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. GAN Inversion: A Survey. arXiv:2101.05278 [cs], Aug. 2021. arXiv: 2101.05278.\n\nVideoGPT: Video Generation using VQ-VAE and Transformers. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, Aravind Srinivas, arXiv:2104.10157arXiv: 2104.10157Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. VideoGPT: Video Generation using VQ-VAE and Transformers. arXiv:2104.10157 [cs], Sept. 2021. arXiv: 2104.10157.\n\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, Jianxiong Xiao, Lsun, arXiv:1506.03365arXiv: 1506.03365Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop. Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Hu- mans in the Loop. arXiv:1506.03365 [cs], June 2016. arXiv: 1506.03365.\n\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Salt Lake City, UTIEEERichard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht- man, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 586-595, Salt Lake City, UT, June 2018. IEEE.\n", "annotations": {"author": "[{\"end\":94,\"start\":76},{\"end\":113,\"start\":95},{\"end\":135,\"start\":114},{\"end\":166,\"start\":136}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":83},{\"end\":112,\"start\":104},{\"end\":134,\"start\":123},{\"end\":165,\"start\":153}]", "author_first_name": "[{\"end\":82,\"start\":76},{\"end\":103,\"start\":95},{\"end\":122,\"start\":114},{\"end\":152,\"start\":144}]", "author_affiliation": null, "title": "[{\"end\":73,\"start\":1},{\"end\":239,\"start\":167}]", "venue": null, "abstract": "[{\"end\":1839,\"start\":327}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1882,\"start\":1878},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":1885,\"start\":1882},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":1906,\"start\":1902},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2032,\"start\":2028},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2035,\"start\":2032},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2038,\"start\":2035},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2249,\"start\":2246},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2252,\"start\":2249},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2255,\"start\":2252},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2258,\"start\":2255},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2290,\"start\":2287},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2292,\"start\":2290},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2316,\"start\":2312},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2392,\"start\":2388},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2395,\"start\":2392},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2398,\"start\":2395},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2877,\"start\":2873},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3352,\"start\":3348},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3355,\"start\":3352},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4123,\"start\":4119},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6092,\"start\":6088},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6468,\"start\":6464},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6579,\"start\":6575},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6866,\"start\":6862},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6869,\"start\":6866},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6872,\"start\":6869},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6875,\"start\":6872},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7499,\"start\":7495},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7578,\"start\":7574},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8051,\"start\":8047},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10973,\"start\":10969},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10976,\"start\":10973},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10979,\"start\":10976},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11148,\"start\":11144},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11151,\"start\":11148},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11969,\"start\":11965},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12216,\"start\":12212},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12262,\"start\":12258},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12550,\"start\":12546},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12564,\"start\":12560},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12772,\"start\":12768},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":12811,\"start\":12807},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":15098,\"start\":15094},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15101,\"start\":15098},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":15104,\"start\":15101},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15107,\"start\":15104},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16851,\"start\":16847},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":17189,\"start\":17185},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":17247,\"start\":17243},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":18935,\"start\":18931},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19641,\"start\":19637},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19938,\"start\":19935},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19941,\"start\":19938},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19944,\"start\":19941},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":19947,\"start\":19944},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20151,\"start\":20147},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20189,\"start\":20185},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":20972,\"start\":20968},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21511,\"start\":21507},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21749,\"start\":21745},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":21771,\"start\":21767},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22355,\"start\":22351},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22416,\"start\":22412},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22430,\"start\":22426},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22985,\"start\":22981},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23028,\"start\":23024},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23270,\"start\":23266},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23302,\"start\":23298},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23347,\"start\":23343},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23364,\"start\":23360},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23378,\"start\":23374},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23498,\"start\":23494},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23543,\"start\":23539},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23831,\"start\":23827},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23842,\"start\":23839},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23844,\"start\":23842},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23956,\"start\":23952},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":23972,\"start\":23968},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23996,\"start\":23992},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24948,\"start\":24944},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24984,\"start\":24980},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25747,\"start\":25743},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":25750,\"start\":25747},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25753,\"start\":25750},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":25756,\"start\":25753},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25835,\"start\":25831},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":26672,\"start\":26668},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26918,\"start\":26914},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27541,\"start\":27537},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28430,\"start\":28426},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29370,\"start\":29366},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":29397,\"start\":29393},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29414,\"start\":29410},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30529,\"start\":30525},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30532,\"start\":30529},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30600,\"start\":30596},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30706,\"start\":30702},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30863,\"start\":30859},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":30866,\"start\":30863},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30900,\"start\":30897},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":30903,\"start\":30900},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30958,\"start\":30954},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31012,\"start\":31009},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31319,\"start\":31315},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":31354,\"start\":31350},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":31367,\"start\":31363},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31489,\"start\":31485},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31659,\"start\":31655},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":31670,\"start\":31666},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31687,\"start\":31683},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31704,\"start\":31700},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31814,\"start\":31810},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":31844,\"start\":31840},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":32029,\"start\":32025},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32296,\"start\":32292},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32299,\"start\":32296},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32302,\"start\":32299},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":32431,\"start\":32427},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32623,\"start\":32619},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32678,\"start\":32674},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32733,\"start\":32729},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":32736,\"start\":32733},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32810,\"start\":32806},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32813,\"start\":32810},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":32816,\"start\":32813},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32915,\"start\":32911},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":32918,\"start\":32915},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32952,\"start\":32948},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":35113,\"start\":35109},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35372,\"start\":35368},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":35438,\"start\":35434},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36253,\"start\":36249},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":36317,\"start\":36313},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":38698,\"start\":38694},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":39504,\"start\":39501},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":39506,\"start\":39504},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":39509,\"start\":39506},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39512,\"start\":39509},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39670,\"start\":39666},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":40457,\"start\":40453},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":41153,\"start\":41149},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":41210,\"start\":41206},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":41621,\"start\":41617},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":42121,\"start\":42117},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":42284,\"start\":42280},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":42299,\"start\":42295},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":42320,\"start\":42316},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":43972,\"start\":43968},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":44009,\"start\":44006},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":44127,\"start\":44123},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":45595,\"start\":45591},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":45611,\"start\":45607},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":45626,\"start\":45622},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":47412,\"start\":47408},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":47461,\"start\":47457},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":47508,\"start\":47504},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":47905,\"start\":47901},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":48109,\"start\":48105},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":48238,\"start\":48234},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":48365,\"start\":48361},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":48914,\"start\":48910},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":50834,\"start\":50830}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45789,\"start\":45663},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46217,\"start\":45790},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46409,\"start\":46218},{\"attributes\":{\"id\":\"fig_5\"},\"end\":46454,\"start\":46410},{\"attributes\":{\"id\":\"fig_6\"},\"end\":46587,\"start\":46455},{\"attributes\":{\"id\":\"fig_7\"},\"end\":46642,\"start\":46588},{\"attributes\":{\"id\":\"fig_8\"},\"end\":46707,\"start\":46643},{\"attributes\":{\"id\":\"fig_9\"},\"end\":46762,\"start\":46708},{\"attributes\":{\"id\":\"fig_10\"},\"end\":47123,\"start\":46763},{\"attributes\":{\"id\":\"fig_11\"},\"end\":47228,\"start\":47124},{\"attributes\":{\"id\":\"fig_12\"},\"end\":47326,\"start\":47229},{\"attributes\":{\"id\":\"fig_13\"},\"end\":47509,\"start\":47327},{\"attributes\":{\"id\":\"fig_14\"},\"end\":47763,\"start\":47510},{\"attributes\":{\"id\":\"fig_15\"},\"end\":47989,\"start\":47764},{\"attributes\":{\"id\":\"fig_16\"},\"end\":48110,\"start\":47990},{\"attributes\":{\"id\":\"fig_17\"},\"end\":48239,\"start\":48111},{\"attributes\":{\"id\":\"fig_18\"},\"end\":48366,\"start\":48240},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48790,\"start\":48367},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49746,\"start\":48791},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":50326,\"start\":49747},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":50592,\"start\":50327},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":50762,\"start\":50593},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50994,\"start\":50763},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":51382,\"start\":50995}]", "paragraph": "[{\"end\":2775,\"start\":1855},{\"end\":3688,\"start\":2777},{\"end\":4673,\"start\":3690},{\"end\":5382,\"start\":4675},{\"end\":6093,\"start\":5384},{\"end\":6876,\"start\":6108},{\"end\":7000,\"start\":6878},{\"end\":7204,\"start\":7055},{\"end\":7678,\"start\":7279},{\"end\":7773,\"start\":7680},{\"end\":8189,\"start\":7775},{\"end\":8313,\"start\":8262},{\"end\":9934,\"start\":8494},{\"end\":10072,\"start\":9936},{\"end\":10263,\"start\":10116},{\"end\":10934,\"start\":10290},{\"end\":11432,\"start\":10936},{\"end\":11756,\"start\":11460},{\"end\":12055,\"start\":11939},{\"end\":12301,\"start\":12128},{\"end\":12944,\"start\":12421},{\"end\":13227,\"start\":13001},{\"end\":13795,\"start\":13248},{\"end\":14018,\"start\":13818},{\"end\":14674,\"start\":14092},{\"end\":15465,\"start\":14715},{\"end\":15933,\"start\":15583},{\"end\":16453,\"start\":15935},{\"end\":16909,\"start\":16455},{\"end\":17389,\"start\":16925},{\"end\":18168,\"start\":17476},{\"end\":18660,\"start\":18277},{\"end\":18936,\"start\":18789},{\"end\":19375,\"start\":18938},{\"end\":20066,\"start\":19417},{\"end\":21025,\"start\":20068},{\"end\":21824,\"start\":21027},{\"end\":23172,\"start\":21864},{\"end\":25322,\"start\":23174},{\"end\":27365,\"start\":25351},{\"end\":29117,\"start\":27422},{\"end\":29636,\"start\":29144},{\"end\":30464,\"start\":29638},{\"end\":31562,\"start\":30481},{\"end\":32568,\"start\":31564},{\"end\":33356,\"start\":32570},{\"end\":34013,\"start\":33385},{\"end\":34401,\"start\":34015},{\"end\":34972,\"start\":34403},{\"end\":35466,\"start\":35015},{\"end\":35501,\"start\":35487},{\"end\":36106,\"start\":35536},{\"end\":37156,\"start\":36108},{\"end\":37727,\"start\":37196},{\"end\":38316,\"start\":37756},{\"end\":38939,\"start\":38364},{\"end\":39287,\"start\":38941},{\"end\":40193,\"start\":39335},{\"end\":40221,\"start\":40203},{\"end\":40963,\"start\":40223},{\"end\":41529,\"start\":40965},{\"end\":41816,\"start\":41577},{\"end\":42172,\"start\":41849},{\"end\":42342,\"start\":42201},{\"end\":42989,\"start\":42385},{\"end\":43125,\"start\":42999},{\"end\":43170,\"start\":43127},{\"end\":43178,\"start\":43172},{\"end\":43514,\"start\":43180},{\"end\":43896,\"start\":43547},{\"end\":44657,\"start\":43898},{\"end\":44683,\"start\":44659},{\"end\":44772,\"start\":44685},{\"end\":45632,\"start\":44774}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7054,\"start\":7001},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7278,\"start\":7205},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8261,\"start\":8190},{\"attributes\":{\"id\":\"formula_3\"},\"end\":8493,\"start\":8314},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10115,\"start\":10073},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11938,\"start\":11757},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12127,\"start\":12056},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12420,\"start\":12302},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13000,\"start\":12945},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14091,\"start\":14019},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15582,\"start\":15466},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18229,\"start\":18169},{\"attributes\":{\"id\":\"formula_12\"},\"end\":18788,\"start\":18661},{\"attributes\":{\"id\":\"formula_13\"},\"end\":27392,\"start\":27366}]", "table_ref": "[{\"end\":18167,\"start\":18160},{\"end\":23184,\"start\":23177},{\"end\":24275,\"start\":24266},{\"end\":24461,\"start\":24453},{\"end\":24542,\"start\":24534},{\"end\":24880,\"start\":24873},{\"end\":26065,\"start\":26058},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26654,\"start\":26592},{\"end\":27364,\"start\":27357},{\"end\":28250,\"start\":28243},{\"end\":28984,\"start\":28977},{\"end\":29657,\"start\":29650},{\"end\":29871,\"start\":29864},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35221,\"start\":35214},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":35879,\"start\":35872},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36190,\"start\":36183},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":38002,\"start\":37995},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":38314,\"start\":38307},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":39524,\"start\":39517},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":40233,\"start\":40226},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":41220,\"start\":41212}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1853,\"start\":1841},{\"attributes\":{\"n\":\"2.\"},\"end\":6106,\"start\":6096},{\"attributes\":{\"n\":\"3.\"},\"end\":10288,\"start\":10266},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11458,\"start\":11435},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13246,\"start\":13230},{\"attributes\":{\"n\":\"3.3.\"},\"end\":13816,\"start\":13798},{\"attributes\":{\"n\":\"4.\"},\"end\":14713,\"start\":14677},{\"attributes\":{\"n\":\"5.\"},\"end\":16923,\"start\":16912},{\"attributes\":{\"n\":\"5.1.\"},\"end\":17474,\"start\":17392},{\"attributes\":{\"n\":\"5.2.\"},\"end\":18275,\"start\":18231},{\"attributes\":{\"n\":\"5.3.\"},\"end\":19415,\"start\":19378},{\"attributes\":{\"n\":\"5.4.\"},\"end\":21862,\"start\":21827},{\"attributes\":{\"n\":\"5.5.\"},\"end\":25349,\"start\":25325},{\"attributes\":{\"n\":\"5.6.\"},\"end\":27420,\"start\":27394},{\"attributes\":{\"n\":\"5.7.\"},\"end\":29142,\"start\":29120},{\"attributes\":{\"n\":\"6.\"},\"end\":30479,\"start\":30467},{\"attributes\":{\"n\":\"7.\"},\"end\":33383,\"start\":33359},{\"end\":35013,\"start\":34975},{\"end\":35485,\"start\":35469},{\"end\":35534,\"start\":35504},{\"end\":37175,\"start\":37159},{\"end\":37194,\"start\":37178},{\"end\":37754,\"start\":37730},{\"end\":38362,\"start\":38319},{\"end\":39333,\"start\":39290},{\"end\":40201,\"start\":40196},{\"end\":41575,\"start\":41532},{\"end\":41847,\"start\":41819},{\"end\":42199,\"start\":42175},{\"end\":42383,\"start\":42345},{\"end\":42997,\"start\":42992},{\"end\":43545,\"start\":43517},{\"end\":45662,\"start\":45635},{\"end\":45801,\"start\":45791},{\"end\":46229,\"start\":46219},{\"end\":46466,\"start\":46456},{\"end\":46654,\"start\":46644},{\"end\":46719,\"start\":46709},{\"end\":46774,\"start\":46764},{\"end\":47136,\"start\":47125},{\"end\":47241,\"start\":47230},{\"end\":47361,\"start\":47328},{\"end\":47522,\"start\":47511},{\"end\":47776,\"start\":47765},{\"end\":48002,\"start\":47991},{\"end\":48123,\"start\":48112},{\"end\":48252,\"start\":48241},{\"end\":48801,\"start\":48792},{\"end\":49757,\"start\":49748},{\"end\":50337,\"start\":50328},{\"end\":50603,\"start\":50594},{\"end\":50774,\"start\":50764},{\"end\":51005,\"start\":50996}]", "table": "[{\"end\":48790,\"start\":48405},{\"end\":49746,\"start\":48915},{\"end\":50326,\"start\":49799},{\"end\":50592,\"start\":50386},{\"end\":50994,\"start\":50876},{\"end\":51382,\"start\":51039}]", "figure_caption": "[{\"end\":45789,\"start\":45665},{\"end\":46217,\"start\":45803},{\"end\":46409,\"start\":46231},{\"end\":46454,\"start\":46412},{\"end\":46587,\"start\":46468},{\"end\":46642,\"start\":46590},{\"end\":46707,\"start\":46656},{\"end\":46762,\"start\":46721},{\"end\":47123,\"start\":46776},{\"end\":47228,\"start\":47139},{\"end\":47326,\"start\":47244},{\"end\":47509,\"start\":47368},{\"end\":47763,\"start\":47525},{\"end\":47989,\"start\":47779},{\"end\":48110,\"start\":48005},{\"end\":48239,\"start\":48126},{\"end\":48366,\"start\":48255},{\"end\":48405,\"start\":48369},{\"end\":48915,\"start\":48803},{\"end\":49799,\"start\":49759},{\"end\":50386,\"start\":50339},{\"end\":50762,\"start\":50605},{\"end\":50876,\"start\":50777},{\"end\":51039,\"start\":51007}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9098,\"start\":9089},{\"end\":9368,\"start\":9360},{\"end\":10933,\"start\":10925},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18586,\"start\":18577},{\"end\":18599,\"start\":18591},{\"end\":20201,\"start\":20193},{\"end\":22743,\"start\":22735},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25137,\"start\":25129},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27170,\"start\":27162},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":35837,\"start\":35827},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":36145,\"start\":36137},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":38584,\"start\":38576},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":38996,\"start\":38987},{\"end\":41287,\"start\":41279},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":41670,\"start\":41661},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":41944,\"start\":41930},{\"attributes\":{\"ref_id\":\"fig_16\"},\"end\":42333,\"start\":42324},{\"end\":42732,\"start\":42723},{\"end\":42830,\"start\":42821},{\"end\":43015,\"start\":43006},{\"end\":43214,\"start\":43205},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":45515,\"start\":45506}]", "bib_author_first_name": "[{\"end\":53792,\"start\":53786},{\"end\":53806,\"start\":53800},{\"end\":53817,\"start\":53812},{\"end\":54020,\"start\":54014},{\"end\":54034,\"start\":54028},{\"end\":54045,\"start\":54040},{\"end\":54301,\"start\":54295},{\"end\":54315,\"start\":54309},{\"end\":54326,\"start\":54321},{\"end\":54339,\"start\":54334},{\"end\":54724,\"start\":54715},{\"end\":54738,\"start\":54732},{\"end\":55063,\"start\":55058},{\"end\":55078,\"start\":55072},{\"end\":55080,\"start\":55079},{\"end\":55098,\"start\":55090},{\"end\":55330,\"start\":55324},{\"end\":55339,\"start\":55337},{\"end\":55352,\"start\":55347},{\"end\":55361,\"start\":55358},{\"end\":55363,\"start\":55362},{\"end\":55379,\"start\":55371},{\"end\":55396,\"start\":55389},{\"end\":55733,\"start\":55729},{\"end\":55745,\"start\":55740},{\"end\":55765,\"start\":55757},{\"end\":55783,\"start\":55775},{\"end\":56197,\"start\":56189},{\"end\":56211,\"start\":56204},{\"end\":56225,\"start\":56217},{\"end\":56242,\"start\":56233},{\"end\":56256,\"start\":56249},{\"end\":56631,\"start\":56628},{\"end\":56641,\"start\":56638},{\"end\":56655,\"start\":56648},{\"end\":56670,\"start\":56664},{\"end\":56678,\"start\":56675},{\"end\":56685,\"start\":56683},{\"end\":57071,\"start\":57063},{\"end\":57081,\"start\":57078},{\"end\":57094,\"start\":57087},{\"end\":57108,\"start\":57100},{\"end\":57325,\"start\":57317},{\"end\":57345,\"start\":57336},{\"end\":57359,\"start\":57353},{\"end\":57569,\"start\":57562},{\"end\":57582,\"start\":57577},{\"end\":57599,\"start\":57592},{\"end\":57616,\"start\":57611},{\"end\":57861,\"start\":57854},{\"end\":57874,\"start\":57869},{\"end\":57889,\"start\":57884},{\"end\":58117,\"start\":58111},{\"end\":58132,\"start\":58127},{\"end\":58141,\"start\":58137},{\"end\":58483,\"start\":58480},{\"end\":58500,\"start\":58496},{\"end\":58521,\"start\":58516},{\"end\":58533,\"start\":58529},{\"end\":58543,\"start\":58538},{\"end\":58565,\"start\":58558},{\"end\":58578,\"start\":58573},{\"end\":58596,\"start\":58590},{\"end\":59018,\"start\":59006},{\"end\":59033,\"start\":59026},{\"end\":59048,\"start\":59041},{\"end\":59065,\"start\":59057},{\"end\":59080,\"start\":59074},{\"end\":59097,\"start\":59092},{\"end\":59115,\"start\":59111},{\"end\":59133,\"start\":59125},{\"end\":59154,\"start\":59147},{\"end\":59168,\"start\":59160},{\"end\":59179,\"start\":59169},{\"end\":59191,\"start\":59186},{\"end\":59202,\"start\":59198},{\"end\":59228,\"start\":59222},{\"end\":59944,\"start\":59939},{\"end\":59955,\"start\":59949},{\"end\":60457,\"start\":60450},{\"end\":60473,\"start\":60466},{\"end\":60475,\"start\":60474},{\"end\":60490,\"start\":60487},{\"end\":60505,\"start\":60500},{\"end\":60521,\"start\":60516},{\"end\":60533,\"start\":60526},{\"end\":60544,\"start\":60539},{\"end\":60806,\"start\":60799},{\"end\":60816,\"start\":60811},{\"end\":60827,\"start\":60822},{\"end\":60839,\"start\":60832},{\"end\":60849,\"start\":60845},{\"end\":61071,\"start\":61064},{\"end\":61083,\"start\":61076},{\"end\":61099,\"start\":61091},{\"end\":61109,\"start\":61105},{\"end\":61313,\"start\":61304},{\"end\":61326,\"start\":61318},{\"end\":61337,\"start\":61332},{\"end\":61351,\"start\":61343},{\"end\":61363,\"start\":61358},{\"end\":61777,\"start\":61769},{\"end\":61786,\"start\":61782},{\"end\":61799,\"start\":61793},{\"end\":62291,\"start\":62283},{\"end\":62303,\"start\":62296},{\"end\":62320,\"start\":62313},{\"end\":62332,\"start\":62327},{\"end\":62334,\"start\":62333},{\"end\":62350,\"start\":62342},{\"end\":62363,\"start\":62360},{\"end\":62688,\"start\":62684},{\"end\":62704,\"start\":62699},{\"end\":62722,\"start\":62716},{\"end\":62740,\"start\":62733},{\"end\":63198,\"start\":63192},{\"end\":63222,\"start\":63220},{\"end\":63231,\"start\":63227},{\"end\":63668,\"start\":63664},{\"end\":63681,\"start\":63677},{\"end\":63694,\"start\":63688},{\"end\":63708,\"start\":63702},{\"end\":64061,\"start\":64057},{\"end\":64076,\"start\":64070},{\"end\":64088,\"start\":64084},{\"end\":64295,\"start\":64291},{\"end\":64310,\"start\":64304},{\"end\":64323,\"start\":64318},{\"end\":64338,\"start\":64333},{\"end\":64355,\"start\":64349},{\"end\":64370,\"start\":64366},{\"end\":64577,\"start\":64576},{\"end\":64591,\"start\":64588},{\"end\":64603,\"start\":64600},{\"end\":64622,\"start\":64614},{\"end\":64771,\"start\":64770},{\"end\":64785,\"start\":64782},{\"end\":64887,\"start\":64886},{\"end\":64889,\"start\":64888},{\"end\":64898,\"start\":64895},{\"end\":64911,\"start\":64904},{\"end\":64921,\"start\":64918},{\"end\":64933,\"start\":64929},{\"end\":65265,\"start\":65258},{\"end\":65275,\"start\":65270},{\"end\":65286,\"start\":65282},{\"end\":65299,\"start\":65294},{\"end\":65312,\"start\":65306},{\"end\":65325,\"start\":65319},{\"end\":65332,\"start\":65330},{\"end\":65344,\"start\":65337},{\"end\":65629,\"start\":65624},{\"end\":65639,\"start\":65635},{\"end\":65653,\"start\":65645},{\"end\":65666,\"start\":65660},{\"end\":66011,\"start\":66004},{\"end\":66020,\"start\":66017},{\"end\":66433,\"start\":66426},{\"end\":66446,\"start\":66440},{\"end\":66455,\"start\":66451},{\"end\":66469,\"start\":66462},{\"end\":66482,\"start\":66476},{\"end\":66494,\"start\":66487},{\"end\":66507,\"start\":66500},{\"end\":66768,\"start\":66759},{\"end\":66782,\"start\":66776},{\"end\":66793,\"start\":66785},{\"end\":67212,\"start\":67210},{\"end\":67230,\"start\":67224},{\"end\":67238,\"start\":67235},{\"end\":67256,\"start\":67250},{\"end\":67271,\"start\":67267},{\"end\":67501,\"start\":67498},{\"end\":67515,\"start\":67510},{\"end\":67535,\"start\":67530},{\"end\":68076,\"start\":68072},{\"end\":68094,\"start\":68089},{\"end\":68105,\"start\":68103},{\"end\":68122,\"start\":68117},{\"end\":68136,\"start\":68131},{\"end\":68147,\"start\":68143},{\"end\":68163,\"start\":68157},{\"end\":68445,\"start\":68441},{\"end\":68466,\"start\":68459},{\"end\":68482,\"start\":68476},{\"end\":69061,\"start\":69054},{\"end\":69075,\"start\":69069},{\"end\":69100,\"start\":69094},{\"end\":69435,\"start\":69429},{\"end\":69455,\"start\":69449},{\"end\":69899,\"start\":69892},{\"end\":69917,\"start\":69909},{\"end\":69929,\"start\":69922},{\"end\":69939,\"start\":69936},{\"end\":69955,\"start\":69950},{\"end\":69957,\"start\":69956},{\"end\":69973,\"start\":69965},{\"end\":70299,\"start\":70294},{\"end\":70312,\"start\":70306},{\"end\":70323,\"start\":70317},{\"end\":70335,\"start\":70330},{\"end\":70740,\"start\":70732},{\"end\":70755,\"start\":70748},{\"end\":70769,\"start\":70762},{\"end\":70783,\"start\":70776},{\"end\":71003,\"start\":70997},{\"end\":71024,\"start\":71020},{\"end\":71036,\"start\":71032},{\"end\":71059,\"start\":71054},{\"end\":71505,\"start\":71498},{\"end\":71519,\"start\":71512},{\"end\":71533,\"start\":71526},{\"end\":71714,\"start\":71710},{\"end\":71728,\"start\":71721},{\"end\":72190,\"start\":72186},{\"end\":72203,\"start\":72197},{\"end\":72228,\"start\":72220},{\"end\":72230,\"start\":72229},{\"end\":72247,\"start\":72239},{\"end\":72262,\"start\":72255},{\"end\":72273,\"start\":72270},{\"end\":72691,\"start\":72686},{\"end\":72703,\"start\":72700},{\"end\":73178,\"start\":73173},{\"end\":73196,\"start\":73193},{\"end\":73216,\"start\":73211},{\"end\":73232,\"start\":73227},{\"end\":73256,\"start\":73252},{\"end\":73779,\"start\":73774},{\"end\":73799,\"start\":73794},{\"end\":74350,\"start\":74349},{\"end\":74352,\"start\":74351},{\"end\":74365,\"start\":74364},{\"end\":74367,\"start\":74366},{\"end\":74376,\"start\":74375},{\"end\":74378,\"start\":74377},{\"end\":74767,\"start\":74760},{\"end\":74784,\"start\":74778},{\"end\":74965,\"start\":74959},{\"end\":74977,\"start\":74970},{\"end\":74990,\"start\":74983},{\"end\":75004,\"start\":74998},{\"end\":75006,\"start\":75005},{\"end\":75022,\"start\":75014},{\"end\":75229,\"start\":75224},{\"end\":75241,\"start\":75234},{\"end\":75392,\"start\":75386},{\"end\":75401,\"start\":75397},{\"end\":75417,\"start\":75414},{\"end\":75584,\"start\":75578},{\"end\":75595,\"start\":75590},{\"end\":75608,\"start\":75603},{\"end\":75623,\"start\":75615},{\"end\":75634,\"start\":75629},{\"end\":75651,\"start\":75641},{\"end\":75943,\"start\":75937},{\"end\":75955,\"start\":75949},{\"end\":75969,\"start\":75963},{\"end\":75985,\"start\":75978},{\"end\":76211,\"start\":76205},{\"end\":76219,\"start\":76216},{\"end\":76231,\"start\":76226},{\"end\":76245,\"start\":76239},{\"end\":76258,\"start\":76252},{\"end\":76280,\"start\":76271},{\"end\":76731,\"start\":76724},{\"end\":76746,\"start\":76739},{\"end\":76760,\"start\":76754},{\"end\":76762,\"start\":76761},{\"end\":76773,\"start\":76770},{\"end\":76791,\"start\":76785}]", "bib_author_last_name": "[{\"end\":53798,\"start\":53793},{\"end\":53810,\"start\":53807},{\"end\":53823,\"start\":53818},{\"end\":54026,\"start\":54021},{\"end\":54038,\"start\":54035},{\"end\":54051,\"start\":54046},{\"end\":54307,\"start\":54302},{\"end\":54319,\"start\":54316},{\"end\":54332,\"start\":54327},{\"end\":54345,\"start\":54340},{\"end\":54730,\"start\":54725},{\"end\":54745,\"start\":54739},{\"end\":55070,\"start\":55064},{\"end\":55088,\"start\":55081},{\"end\":55101,\"start\":55099},{\"end\":55335,\"start\":55331},{\"end\":55345,\"start\":55340},{\"end\":55356,\"start\":55353},{\"end\":55369,\"start\":55364},{\"end\":55387,\"start\":55380},{\"end\":55401,\"start\":55397},{\"end\":55738,\"start\":55734},{\"end\":55755,\"start\":55746},{\"end\":55773,\"start\":55766},{\"end\":55790,\"start\":55784},{\"end\":56202,\"start\":56198},{\"end\":56215,\"start\":56212},{\"end\":56231,\"start\":56226},{\"end\":56247,\"start\":56243},{\"end\":56261,\"start\":56257},{\"end\":56267,\"start\":56263},{\"end\":56636,\"start\":56632},{\"end\":56646,\"start\":56642},{\"end\":56662,\"start\":56656},{\"end\":56673,\"start\":56671},{\"end\":56681,\"start\":56679},{\"end\":56693,\"start\":56686},{\"end\":57076,\"start\":57072},{\"end\":57085,\"start\":57082},{\"end\":57098,\"start\":57095},{\"end\":57118,\"start\":57109},{\"end\":57334,\"start\":57326},{\"end\":57351,\"start\":57346},{\"end\":57575,\"start\":57570},{\"end\":57590,\"start\":57583},{\"end\":57609,\"start\":57600},{\"end\":57622,\"start\":57617},{\"end\":57867,\"start\":57862},{\"end\":57882,\"start\":57875},{\"end\":57895,\"start\":57890},{\"end\":58125,\"start\":58118},{\"end\":58135,\"start\":58133},{\"end\":58146,\"start\":58142},{\"end\":58494,\"start\":58484},{\"end\":58514,\"start\":58501},{\"end\":58527,\"start\":58522},{\"end\":58536,\"start\":58534},{\"end\":58556,\"start\":58544},{\"end\":58571,\"start\":58566},{\"end\":58588,\"start\":58579},{\"end\":58603,\"start\":58597},{\"end\":59024,\"start\":59019},{\"end\":59039,\"start\":59034},{\"end\":59055,\"start\":59049},{\"end\":59072,\"start\":59066},{\"end\":59090,\"start\":59081},{\"end\":59109,\"start\":59098},{\"end\":59123,\"start\":59116},{\"end\":59145,\"start\":59134},{\"end\":59158,\"start\":59155},{\"end\":59184,\"start\":59180},{\"end\":59196,\"start\":59192},{\"end\":59220,\"start\":59203},{\"end\":59234,\"start\":59229},{\"end\":59241,\"start\":59236},{\"end\":59947,\"start\":59945},{\"end\":59967,\"start\":59956},{\"end\":60464,\"start\":60458},{\"end\":60485,\"start\":60476},{\"end\":60498,\"start\":60491},{\"end\":60514,\"start\":60506},{\"end\":60524,\"start\":60522},{\"end\":60537,\"start\":60534},{\"end\":60553,\"start\":60545},{\"end\":60809,\"start\":60807},{\"end\":60820,\"start\":60817},{\"end\":60830,\"start\":60828},{\"end\":60843,\"start\":60840},{\"end\":60858,\"start\":60850},{\"end\":61074,\"start\":61072},{\"end\":61089,\"start\":61084},{\"end\":61103,\"start\":61100},{\"end\":61113,\"start\":61110},{\"end\":61316,\"start\":61314},{\"end\":61330,\"start\":61327},{\"end\":61341,\"start\":61338},{\"end\":61356,\"start\":61352},{\"end\":61368,\"start\":61364},{\"end\":61780,\"start\":61778},{\"end\":61791,\"start\":61787},{\"end\":61806,\"start\":61800},{\"end\":62294,\"start\":62292},{\"end\":62311,\"start\":62304},{\"end\":62325,\"start\":62321},{\"end\":62340,\"start\":62335},{\"end\":62358,\"start\":62351},{\"end\":62372,\"start\":62364},{\"end\":62697,\"start\":62689},{\"end\":62714,\"start\":62705},{\"end\":62731,\"start\":62723},{\"end\":62746,\"start\":62741},{\"end\":63218,\"start\":63199},{\"end\":63225,\"start\":63223},{\"end\":63247,\"start\":63232},{\"end\":63675,\"start\":63669},{\"end\":63686,\"start\":63682},{\"end\":63700,\"start\":63695},{\"end\":63717,\"start\":63709},{\"end\":64068,\"start\":64062},{\"end\":64082,\"start\":64077},{\"end\":64093,\"start\":64089},{\"end\":64302,\"start\":64296},{\"end\":64316,\"start\":64311},{\"end\":64331,\"start\":64324},{\"end\":64347,\"start\":64339},{\"end\":64364,\"start\":64356},{\"end\":64375,\"start\":64371},{\"end\":64586,\"start\":64578},{\"end\":64598,\"start\":64592},{\"end\":64612,\"start\":64604},{\"end\":64628,\"start\":64623},{\"end\":64632,\"start\":64630},{\"end\":64780,\"start\":64772},{\"end\":64792,\"start\":64786},{\"end\":64801,\"start\":64794},{\"end\":64893,\"start\":64890},{\"end\":64902,\"start\":64899},{\"end\":64916,\"start\":64912},{\"end\":64927,\"start\":64922},{\"end\":64936,\"start\":64934},{\"end\":64940,\"start\":64938},{\"end\":65268,\"start\":65266},{\"end\":65280,\"start\":65276},{\"end\":65292,\"start\":65287},{\"end\":65304,\"start\":65300},{\"end\":65317,\"start\":65313},{\"end\":65328,\"start\":65326},{\"end\":65335,\"start\":65333},{\"end\":65349,\"start\":65345},{\"end\":65633,\"start\":65630},{\"end\":65643,\"start\":65640},{\"end\":65658,\"start\":65654},{\"end\":65671,\"start\":65667},{\"end\":66015,\"start\":66012},{\"end\":66023,\"start\":66021},{\"end\":66438,\"start\":66434},{\"end\":66449,\"start\":66447},{\"end\":66460,\"start\":66456},{\"end\":66474,\"start\":66470},{\"end\":66485,\"start\":66483},{\"end\":66498,\"start\":66495},{\"end\":66513,\"start\":66508},{\"end\":66774,\"start\":66769},{\"end\":66802,\"start\":66794},{\"end\":67222,\"start\":67213},{\"end\":67233,\"start\":67231},{\"end\":67248,\"start\":67239},{\"end\":67265,\"start\":67257},{\"end\":67282,\"start\":67272},{\"end\":67508,\"start\":67502},{\"end\":67528,\"start\":67516},{\"end\":67543,\"start\":67536},{\"end\":68087,\"start\":68077},{\"end\":68101,\"start\":68095},{\"end\":68115,\"start\":68106},{\"end\":68129,\"start\":68123},{\"end\":68141,\"start\":68137},{\"end\":68155,\"start\":68148},{\"end\":68172,\"start\":68164},{\"end\":68457,\"start\":68446},{\"end\":68474,\"start\":68467},{\"end\":68487,\"start\":68483},{\"end\":69067,\"start\":69062},{\"end\":69092,\"start\":69076},{\"end\":69108,\"start\":69101},{\"end\":69447,\"start\":69436},{\"end\":69466,\"start\":69456},{\"end\":69474,\"start\":69468},{\"end\":69907,\"start\":69900},{\"end\":69920,\"start\":69918},{\"end\":69934,\"start\":69930},{\"end\":69948,\"start\":69940},{\"end\":69963,\"start\":69958},{\"end\":69981,\"start\":69974},{\"end\":70304,\"start\":70300},{\"end\":70315,\"start\":70313},{\"end\":70328,\"start\":70324},{\"end\":70340,\"start\":70336},{\"end\":70746,\"start\":70741},{\"end\":70760,\"start\":70756},{\"end\":70774,\"start\":70770},{\"end\":70789,\"start\":70784},{\"end\":71018,\"start\":71004},{\"end\":71030,\"start\":71025},{\"end\":71052,\"start\":71037},{\"end\":71067,\"start\":71060},{\"end\":71510,\"start\":71506},{\"end\":71524,\"start\":71520},{\"end\":71539,\"start\":71534},{\"end\":71719,\"start\":71715},{\"end\":71734,\"start\":71729},{\"end\":72195,\"start\":72191},{\"end\":72218,\"start\":72204},{\"end\":72237,\"start\":72231},{\"end\":72253,\"start\":72248},{\"end\":72268,\"start\":72263},{\"end\":72279,\"start\":72274},{\"end\":72698,\"start\":72692},{\"end\":72709,\"start\":72704},{\"end\":73191,\"start\":73179},{\"end\":73209,\"start\":73197},{\"end\":73225,\"start\":73217},{\"end\":73250,\"start\":73233},{\"end\":73264,\"start\":73257},{\"end\":73272,\"start\":73266},{\"end\":73792,\"start\":73780},{\"end\":73807,\"start\":73800},{\"end\":74362,\"start\":74353},{\"end\":74373,\"start\":74368},{\"end\":74385,\"start\":74379},{\"end\":74397,\"start\":74387},{\"end\":74776,\"start\":74768},{\"end\":74791,\"start\":74785},{\"end\":74968,\"start\":74966},{\"end\":74981,\"start\":74978},{\"end\":74996,\"start\":74991},{\"end\":75012,\"start\":75007},{\"end\":75027,\"start\":75023},{\"end\":75232,\"start\":75230},{\"end\":75244,\"start\":75242},{\"end\":75395,\"start\":75393},{\"end\":75412,\"start\":75402},{\"end\":75427,\"start\":75418},{\"end\":75588,\"start\":75585},{\"end\":75601,\"start\":75596},{\"end\":75613,\"start\":75609},{\"end\":75627,\"start\":75624},{\"end\":75639,\"start\":75635},{\"end\":75656,\"start\":75652},{\"end\":75947,\"start\":75944},{\"end\":75961,\"start\":75956},{\"end\":75976,\"start\":75970},{\"end\":75994,\"start\":75986},{\"end\":76214,\"start\":76212},{\"end\":76224,\"start\":76220},{\"end\":76237,\"start\":76232},{\"end\":76250,\"start\":76246},{\"end\":76269,\"start\":76259},{\"end\":76285,\"start\":76281},{\"end\":76291,\"start\":76287},{\"end\":76737,\"start\":76732},{\"end\":76752,\"start\":76747},{\"end\":76768,\"start\":76763},{\"end\":76783,\"start\":76774},{\"end\":76796,\"start\":76792}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":53960,\"start\":53717},{\"attributes\":{\"id\":\"b1\"},\"end\":54171,\"start\":53962},{\"attributes\":{\"doi\":\"arXiv:2008.02401\",\"id\":\"b2\",\"matched_paper_id\":221006041},\"end\":54647,\"start\":54173},{\"attributes\":{\"doi\":\"arXiv:1610.01644\",\"id\":\"b3\"},\"end\":54952,\"start\":54649},{\"attributes\":{\"id\":\"b4\"},\"end\":55266,\"start\":54954},{\"attributes\":{\"id\":\"b5\"},\"end\":55656,\"start\":55268},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211096730},\"end\":56187,\"start\":55658},{\"attributes\":{\"id\":\"b7\"},\"end\":56573,\"start\":56189},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":57246310},\"end\":56996,\"start\":56575},{\"attributes\":{\"id\":\"b9\"},\"end\":57268,\"start\":56998},{\"attributes\":{\"id\":\"b10\"},\"end\":57464,\"start\":57270},{\"attributes\":{\"id\":\"b11\"},\"end\":57795,\"start\":57466},{\"attributes\":{\"id\":\"b12\"},\"end\":58025,\"start\":57797},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":202774726},\"end\":58449,\"start\":58027},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1033682},\"end\":58933,\"start\":58451},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":219687798},\"end\":59885,\"start\":58935},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52171619},\"end\":60397,\"start\":59887},{\"attributes\":{\"id\":\"b17\"},\"end\":60730,\"start\":60399},{\"attributes\":{\"id\":\"b18\"},\"end\":61016,\"start\":60732},{\"attributes\":{\"id\":\"b19\"},\"end\":61237,\"start\":61018},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":50785552},\"end\":61725,\"start\":61239},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":219955663},\"end\":62219,\"start\":61727},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":235619773},\"end\":62632,\"start\":62221},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":214802845},\"end\":63190,\"start\":62634},{\"attributes\":{\"id\":\"b24\"},\"end\":63612,\"start\":63192},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3568073},\"end\":63981,\"start\":63614},{\"attributes\":{\"id\":\"b26\"},\"end\":64234,\"start\":63983},{\"attributes\":{\"id\":\"b27\"},\"end\":64544,\"start\":64236},{\"attributes\":{\"id\":\"b28\"},\"end\":64735,\"start\":64546},{\"attributes\":{\"id\":\"b29\"},\"end\":64884,\"start\":64737},{\"attributes\":{\"id\":\"b30\"},\"end\":65181,\"start\":64886},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":233476433},\"end\":65579,\"start\":65183},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":459456},\"end\":65940,\"start\":65581},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":232092778},\"end\":66341,\"start\":65942},{\"attributes\":{\"id\":\"b34\"},\"end\":66706,\"start\":66343},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":231979499},\"end\":67151,\"start\":66708},{\"attributes\":{\"id\":\"b36\"},\"end\":67441,\"start\":67153},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":173990382},\"end\":68000,\"start\":67443},{\"attributes\":{\"id\":\"b38\"},\"end\":68374,\"start\":68002},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3719281},\"end\":69052,\"start\":68376},{\"attributes\":{\"id\":\"b40\"},\"end\":69367,\"start\":69054},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":219980562},\"end\":69841,\"start\":69369},{\"attributes\":{\"id\":\"b42\"},\"end\":70227,\"start\":69843},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":198897678},\"end\":70662,\"start\":70229},{\"attributes\":{\"id\":\"b44\"},\"end\":70931,\"start\":70664},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":14888175},\"end\":71459,\"start\":70933},{\"attributes\":{\"id\":\"b46\"},\"end\":71638,\"start\":71461},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":196470871},\"end\":72184,\"start\":71640},{\"attributes\":{\"id\":\"b48\"},\"end\":72633,\"start\":72186},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":220403493},\"end\":73118,\"start\":72635},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":14989939},\"end\":73731,\"start\":73120},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":20282961},\"end\":74273,\"start\":73733},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":207761262},\"end\":74712,\"start\":74275},{\"attributes\":{\"id\":\"b53\"},\"end\":74885,\"start\":74714},{\"attributes\":{\"id\":\"b54\"},\"end\":75201,\"start\":74887},{\"attributes\":{\"id\":\"b55\"},\"end\":75309,\"start\":75203},{\"attributes\":{\"id\":\"b56\"},\"end\":75576,\"start\":75311},{\"attributes\":{\"id\":\"b57\"},\"end\":75877,\"start\":75578},{\"attributes\":{\"id\":\"b58\"},\"end\":76203,\"start\":75879},{\"attributes\":{\"id\":\"b59\"},\"end\":76650,\"start\":76205},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":4766599},\"end\":77165,\"start\":76652}]", "bib_title": "[{\"end\":54293,\"start\":54173},{\"end\":55727,\"start\":55658},{\"end\":56626,\"start\":56575},{\"end\":58109,\"start\":58027},{\"end\":58478,\"start\":58451},{\"end\":59004,\"start\":58935},{\"end\":59937,\"start\":59887},{\"end\":61302,\"start\":61239},{\"end\":61767,\"start\":61727},{\"end\":62281,\"start\":62221},{\"end\":62682,\"start\":62634},{\"end\":63662,\"start\":63614},{\"end\":65256,\"start\":65183},{\"end\":65622,\"start\":65581},{\"end\":66002,\"start\":65942},{\"end\":66757,\"start\":66708},{\"end\":67496,\"start\":67443},{\"end\":68439,\"start\":68376},{\"end\":69427,\"start\":69369},{\"end\":70292,\"start\":70229},{\"end\":70995,\"start\":70933},{\"end\":71708,\"start\":71640},{\"end\":72684,\"start\":72635},{\"end\":73171,\"start\":73120},{\"end\":73772,\"start\":73733},{\"end\":74347,\"start\":74275},{\"end\":76722,\"start\":76652}]", "bib_author": "[{\"end\":53800,\"start\":53786},{\"end\":53812,\"start\":53800},{\"end\":53825,\"start\":53812},{\"end\":54028,\"start\":54014},{\"end\":54040,\"start\":54028},{\"end\":54053,\"start\":54040},{\"end\":54309,\"start\":54295},{\"end\":54321,\"start\":54309},{\"end\":54334,\"start\":54321},{\"end\":54347,\"start\":54334},{\"end\":54732,\"start\":54715},{\"end\":54747,\"start\":54732},{\"end\":55072,\"start\":55058},{\"end\":55090,\"start\":55072},{\"end\":55103,\"start\":55090},{\"end\":55337,\"start\":55324},{\"end\":55347,\"start\":55337},{\"end\":55358,\"start\":55347},{\"end\":55371,\"start\":55358},{\"end\":55389,\"start\":55371},{\"end\":55403,\"start\":55389},{\"end\":55740,\"start\":55729},{\"end\":55757,\"start\":55740},{\"end\":55775,\"start\":55757},{\"end\":55792,\"start\":55775},{\"end\":56204,\"start\":56189},{\"end\":56217,\"start\":56204},{\"end\":56233,\"start\":56217},{\"end\":56249,\"start\":56233},{\"end\":56263,\"start\":56249},{\"end\":56269,\"start\":56263},{\"end\":56638,\"start\":56628},{\"end\":56648,\"start\":56638},{\"end\":56664,\"start\":56648},{\"end\":56675,\"start\":56664},{\"end\":56683,\"start\":56675},{\"end\":56695,\"start\":56683},{\"end\":57078,\"start\":57063},{\"end\":57087,\"start\":57078},{\"end\":57100,\"start\":57087},{\"end\":57120,\"start\":57100},{\"end\":57336,\"start\":57317},{\"end\":57353,\"start\":57336},{\"end\":57362,\"start\":57353},{\"end\":57577,\"start\":57562},{\"end\":57592,\"start\":57577},{\"end\":57611,\"start\":57592},{\"end\":57624,\"start\":57611},{\"end\":57869,\"start\":57854},{\"end\":57884,\"start\":57869},{\"end\":57897,\"start\":57884},{\"end\":58127,\"start\":58111},{\"end\":58137,\"start\":58127},{\"end\":58148,\"start\":58137},{\"end\":58496,\"start\":58480},{\"end\":58516,\"start\":58496},{\"end\":58529,\"start\":58516},{\"end\":58538,\"start\":58529},{\"end\":58558,\"start\":58538},{\"end\":58573,\"start\":58558},{\"end\":58590,\"start\":58573},{\"end\":58605,\"start\":58590},{\"end\":59026,\"start\":59006},{\"end\":59041,\"start\":59026},{\"end\":59057,\"start\":59041},{\"end\":59074,\"start\":59057},{\"end\":59092,\"start\":59074},{\"end\":59111,\"start\":59092},{\"end\":59125,\"start\":59111},{\"end\":59147,\"start\":59125},{\"end\":59160,\"start\":59147},{\"end\":59186,\"start\":59160},{\"end\":59198,\"start\":59186},{\"end\":59222,\"start\":59198},{\"end\":59236,\"start\":59222},{\"end\":59243,\"start\":59236},{\"end\":59949,\"start\":59939},{\"end\":59969,\"start\":59949},{\"end\":60466,\"start\":60450},{\"end\":60487,\"start\":60466},{\"end\":60500,\"start\":60487},{\"end\":60516,\"start\":60500},{\"end\":60526,\"start\":60516},{\"end\":60539,\"start\":60526},{\"end\":60555,\"start\":60539},{\"end\":60811,\"start\":60799},{\"end\":60822,\"start\":60811},{\"end\":60832,\"start\":60822},{\"end\":60845,\"start\":60832},{\"end\":60860,\"start\":60845},{\"end\":61076,\"start\":61064},{\"end\":61091,\"start\":61076},{\"end\":61105,\"start\":61091},{\"end\":61115,\"start\":61105},{\"end\":61318,\"start\":61304},{\"end\":61332,\"start\":61318},{\"end\":61343,\"start\":61332},{\"end\":61358,\"start\":61343},{\"end\":61370,\"start\":61358},{\"end\":61782,\"start\":61769},{\"end\":61793,\"start\":61782},{\"end\":61808,\"start\":61793},{\"end\":62296,\"start\":62283},{\"end\":62313,\"start\":62296},{\"end\":62327,\"start\":62313},{\"end\":62342,\"start\":62327},{\"end\":62360,\"start\":62342},{\"end\":62374,\"start\":62360},{\"end\":62699,\"start\":62684},{\"end\":62716,\"start\":62699},{\"end\":62733,\"start\":62716},{\"end\":62748,\"start\":62733},{\"end\":63220,\"start\":63192},{\"end\":63227,\"start\":63220},{\"end\":63249,\"start\":63227},{\"end\":63677,\"start\":63664},{\"end\":63688,\"start\":63677},{\"end\":63702,\"start\":63688},{\"end\":63719,\"start\":63702},{\"end\":64070,\"start\":64057},{\"end\":64084,\"start\":64070},{\"end\":64095,\"start\":64084},{\"end\":64304,\"start\":64291},{\"end\":64318,\"start\":64304},{\"end\":64333,\"start\":64318},{\"end\":64349,\"start\":64333},{\"end\":64366,\"start\":64349},{\"end\":64377,\"start\":64366},{\"end\":64588,\"start\":64576},{\"end\":64600,\"start\":64588},{\"end\":64614,\"start\":64600},{\"end\":64630,\"start\":64614},{\"end\":64634,\"start\":64630},{\"end\":64782,\"start\":64770},{\"end\":64794,\"start\":64782},{\"end\":64803,\"start\":64794},{\"end\":64895,\"start\":64886},{\"end\":64904,\"start\":64895},{\"end\":64918,\"start\":64904},{\"end\":64929,\"start\":64918},{\"end\":64938,\"start\":64929},{\"end\":64942,\"start\":64938},{\"end\":65270,\"start\":65258},{\"end\":65282,\"start\":65270},{\"end\":65294,\"start\":65282},{\"end\":65306,\"start\":65294},{\"end\":65319,\"start\":65306},{\"end\":65330,\"start\":65319},{\"end\":65337,\"start\":65330},{\"end\":65351,\"start\":65337},{\"end\":65635,\"start\":65624},{\"end\":65645,\"start\":65635},{\"end\":65660,\"start\":65645},{\"end\":65673,\"start\":65660},{\"end\":66017,\"start\":66004},{\"end\":66025,\"start\":66017},{\"end\":66440,\"start\":66426},{\"end\":66451,\"start\":66440},{\"end\":66462,\"start\":66451},{\"end\":66476,\"start\":66462},{\"end\":66487,\"start\":66476},{\"end\":66500,\"start\":66487},{\"end\":66515,\"start\":66500},{\"end\":66776,\"start\":66759},{\"end\":66785,\"start\":66776},{\"end\":66804,\"start\":66785},{\"end\":67224,\"start\":67210},{\"end\":67235,\"start\":67224},{\"end\":67250,\"start\":67235},{\"end\":67267,\"start\":67250},{\"end\":67284,\"start\":67267},{\"end\":67510,\"start\":67498},{\"end\":67530,\"start\":67510},{\"end\":67545,\"start\":67530},{\"end\":68089,\"start\":68072},{\"end\":68103,\"start\":68089},{\"end\":68117,\"start\":68103},{\"end\":68131,\"start\":68117},{\"end\":68143,\"start\":68131},{\"end\":68157,\"start\":68143},{\"end\":68174,\"start\":68157},{\"end\":68459,\"start\":68441},{\"end\":68476,\"start\":68459},{\"end\":68489,\"start\":68476},{\"end\":69069,\"start\":69054},{\"end\":69094,\"start\":69069},{\"end\":69110,\"start\":69094},{\"end\":69449,\"start\":69429},{\"end\":69468,\"start\":69449},{\"end\":69476,\"start\":69468},{\"end\":69909,\"start\":69892},{\"end\":69922,\"start\":69909},{\"end\":69936,\"start\":69922},{\"end\":69950,\"start\":69936},{\"end\":69965,\"start\":69950},{\"end\":69983,\"start\":69965},{\"end\":70306,\"start\":70294},{\"end\":70317,\"start\":70306},{\"end\":70330,\"start\":70317},{\"end\":70342,\"start\":70330},{\"end\":70748,\"start\":70732},{\"end\":70762,\"start\":70748},{\"end\":70776,\"start\":70762},{\"end\":70791,\"start\":70776},{\"end\":71020,\"start\":70997},{\"end\":71032,\"start\":71020},{\"end\":71054,\"start\":71032},{\"end\":71069,\"start\":71054},{\"end\":71512,\"start\":71498},{\"end\":71526,\"start\":71512},{\"end\":71541,\"start\":71526},{\"end\":71721,\"start\":71710},{\"end\":71736,\"start\":71721},{\"end\":72197,\"start\":72186},{\"end\":72220,\"start\":72197},{\"end\":72239,\"start\":72220},{\"end\":72255,\"start\":72239},{\"end\":72270,\"start\":72255},{\"end\":72281,\"start\":72270},{\"end\":72700,\"start\":72686},{\"end\":72711,\"start\":72700},{\"end\":73193,\"start\":73173},{\"end\":73211,\"start\":73193},{\"end\":73227,\"start\":73211},{\"end\":73252,\"start\":73227},{\"end\":73266,\"start\":73252},{\"end\":73274,\"start\":73266},{\"end\":73794,\"start\":73774},{\"end\":73809,\"start\":73794},{\"end\":74364,\"start\":74349},{\"end\":74375,\"start\":74364},{\"end\":74387,\"start\":74375},{\"end\":74399,\"start\":74387},{\"end\":74778,\"start\":74760},{\"end\":74793,\"start\":74778},{\"end\":74970,\"start\":74959},{\"end\":74983,\"start\":74970},{\"end\":74998,\"start\":74983},{\"end\":75014,\"start\":74998},{\"end\":75029,\"start\":75014},{\"end\":75234,\"start\":75224},{\"end\":75246,\"start\":75234},{\"end\":75397,\"start\":75386},{\"end\":75414,\"start\":75397},{\"end\":75429,\"start\":75414},{\"end\":75590,\"start\":75578},{\"end\":75603,\"start\":75590},{\"end\":75615,\"start\":75603},{\"end\":75629,\"start\":75615},{\"end\":75641,\"start\":75629},{\"end\":75658,\"start\":75641},{\"end\":75949,\"start\":75937},{\"end\":75963,\"start\":75949},{\"end\":75978,\"start\":75963},{\"end\":75996,\"start\":75978},{\"end\":76216,\"start\":76205},{\"end\":76226,\"start\":76216},{\"end\":76239,\"start\":76226},{\"end\":76252,\"start\":76239},{\"end\":76271,\"start\":76252},{\"end\":76287,\"start\":76271},{\"end\":76293,\"start\":76287},{\"end\":76739,\"start\":76724},{\"end\":76754,\"start\":76739},{\"end\":76770,\"start\":76754},{\"end\":76785,\"start\":76770},{\"end\":76798,\"start\":76785}]", "bib_venue": "[{\"end\":53784,\"start\":53717},{\"end\":54012,\"start\":53962},{\"end\":54391,\"start\":54363},{\"end\":54713,\"start\":54649},{\"end\":55056,\"start\":54954},{\"end\":55322,\"start\":55268},{\"end\":55864,\"start\":55796},{\"end\":56366,\"start\":56302},{\"end\":56758,\"start\":56695},{\"end\":57061,\"start\":56998},{\"end\":57315,\"start\":57270},{\"end\":57560,\"start\":57466},{\"end\":57852,\"start\":57797},{\"end\":58197,\"start\":58148},{\"end\":58654,\"start\":58605},{\"end\":59292,\"start\":59243},{\"end\":60018,\"start\":59969},{\"end\":60448,\"start\":60399},{\"end\":60797,\"start\":60732},{\"end\":61062,\"start\":61018},{\"end\":61424,\"start\":61370},{\"end\":61857,\"start\":61808},{\"end\":62410,\"start\":62374},{\"end\":62797,\"start\":62748},{\"end\":63377,\"start\":63282},{\"end\":63776,\"start\":63752},{\"end\":64055,\"start\":63983},{\"end\":64289,\"start\":64236},{\"end\":64574,\"start\":64546},{\"end\":64768,\"start\":64737},{\"end\":65011,\"start\":64975},{\"end\":65365,\"start\":65351},{\"end\":65733,\"start\":65673},{\"end\":66099,\"start\":66025},{\"end\":66424,\"start\":66343},{\"end\":66876,\"start\":66808},{\"end\":67208,\"start\":67153},{\"end\":67594,\"start\":67545},{\"end\":68070,\"start\":68002},{\"end\":68560,\"start\":68489},{\"end\":69189,\"start\":69143},{\"end\":69548,\"start\":69480},{\"end\":69890,\"start\":69843},{\"end\":70416,\"start\":70342},{\"end\":70730,\"start\":70664},{\"end\":71141,\"start\":71073},{\"end\":71496,\"start\":71461},{\"end\":71785,\"start\":71736},{\"end\":72387,\"start\":72314},{\"end\":72760,\"start\":72711},{\"end\":73323,\"start\":73274},{\"end\":73858,\"start\":73809},{\"end\":74453,\"start\":74399},{\"end\":74758,\"start\":74714},{\"end\":74957,\"start\":74887},{\"end\":75222,\"start\":75203},{\"end\":75384,\"start\":75311},{\"end\":75714,\"start\":75691},{\"end\":75935,\"start\":75879},{\"end\":76413,\"start\":76326},{\"end\":76865,\"start\":76798},{\"end\":55919,\"start\":55866},{\"end\":66119,\"start\":66101},{\"end\":66931,\"start\":66878},{\"end\":68640,\"start\":68636},{\"end\":69603,\"start\":69550},{\"end\":71196,\"start\":71143},{\"end\":76885,\"start\":76867}]"}}}, "year": 2023, "month": 12, "day": 17}