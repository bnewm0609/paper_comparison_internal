{"id": 189926717, "updated": "2023-08-25 13:18:30.67", "metadata": {"title": "WaveEar: Exploring a mmWave-based Noise-resistant Speech Sensing for Voice-User Interface", "authors": "[{\"first\":\"Chenhan\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Zhengxiong\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Hanbin\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Aditya\",\"last\":\"Rathore\",\"middle\":[\"Singh\"]},{\"first\":\"Huining\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Chen\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Kun\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Wenyao\",\"last\":\"Xu\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Voice-user interface (VUI) has become an integral component in modern personal devices (\\textite.g., smartphones, voice assistant) by fundamentally evolving the information sharing between the user and device. Acoustic sensing for VUI is designed to sense all acoustic objects; however, the existing VUI mechanism can only offer low-quality speech sensing. This is due to the audible and inaudible interference from complex ambient noise that limits the performance of VUI by causing denial-of-service (DoS) of user requests. Therefore, it is of paramount importance to enable noise-resistant speech sensing in VUI for executing critical tasks with superior efficiency and precision in robust environments. To this end, we investigate the feasibility of employing radio-frequency signals, such as millimeter wave (mmWave) for sensing the noise-resistant voice of an individual. We first perform an in-depth study behind the rationale of voice generation and resulting vocal vibrations. From the obtained insights, we presentWaveEar, an end-to-end noise-resistant speech sensing system.WaveEar comprises a low-cost mmWave probe to localize the position of the speaker among multiple people and direct the mmWave signals towards the near-throat region of the speaker for sensing his/her vocal vibrations. The received signal, containing the speech information, is fed to our novel deep neural network for recovering the voice through exhaustive extraction. Our experimental evaluation under real-world scenarios with 21 participants shows the effectiveness ofWaveEar to precisely infer the noise-resistant voice and enable a pervasive VUI in modern electronic devices.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2953297989", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mobisys/XuLZRL0WX19", "doi": "10.1145/3307334.3326073"}}, "content": {"source": {"pdf_hash": "67f635ecee64c85bb6edbe8506bf483607b594e4", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://dl.acm.org/doi/pdf/10.1145/3307334.3326073", "status": "BRONZE"}}, "grobid": {"id": "9b9e1ad7429ba0ef91528752f23b46760f1c65ee", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/67f635ecee64c85bb6edbe8506bf483607b594e4.txt", "contents": "\nWaveEar: Exploring a mmWave-based Noise-resistant Speech Sensing for Voice-User Interface\nACMCopyright ACMJune 17-21, 2019. 2019. June 17-21, 2019\n\nChenhan Xu chenhanx@buffalo.edu \nUniversity at Buffalo\nthe State University of New York\nBuffaloNew YorkUSA\n\nZhengxiong Li \nUniversity at Buffalo\nthe State University of New York\nBuffaloNew YorkUSA\n\nHanbin Zhang hanbinzh@buffalo.edu \nUniversity at Buffalo\nthe State University of New York\nBuffaloNew YorkUSA\n\nAditya Singh Rathore \nUniversity at Buffalo\nthe State University of New York\nBuffaloNew YorkUSA\n\nHuining Li \nNanjing University of Posts and Telecommunications\n\n\nChen Song csong5@buffalo.edu \nUniversity at Buffalo\nthe State University of New York\nBuffaloNew YorkUSA\n\nKun Wang wangk@ucla.edu \nUniversity at Buffalo\nthe State University of New York\nBuffaloNew YorkUSA\n\nUniversity of California\nLos AngelesCaliforniaUSA\n\nWenyao Xu wenyaoxu@buffalo.edu \nUniversity at Buffalo\nthe State University of New York\nBuffaloNew YorkUSA\n\nChenhan Xu \nZhengxiong Li \nHanbin Zhang \nAditya Singh Rathore asrathor@buffalo.edu \nHuining Li \nChen Song \nKun Wang \nWenyao Xu \nWaveEar: Exploring a mmWave-based Noise-resistant Speech Sensing for Voice-User Interface\n\nthe 17th Annual Int'l Conference on Mobile Systems, Applications, and Services (MobiSys '19)\nSeoul, Republic of Korea; Seoul, Republic of Korea; NY, NY, USAACM19June 17-21, 2019. 2019. June 17-21, 201910.1145/3307334.3326073Corresponding Contact: wenyaoxu@buffalo.edu; wangk@ucla.edu./19/06. . . $15.00CCS CONCEPTS \u2022 Human-centered computing \u2192 Human computer interac- tion (HCI)\u2022 Computing methodologies \u2192 Speech recogni- tion\u2022 Hardware \u2192 Signal processing systemsSensor applica- tions and deployments\nVoice-user interface (VUI) has become an integral component in modern personal devices (e.g., smartphones, voice assistant) by fundamentally evolving the information sharing between the user and device. Acoustic sensing for VUI is designed to sense all acoustic objects; however, the existing VUI mechanism can only offer lowquality speech sensing. This is due to the audible and inaudible interference from complex ambient noise that limits the performance of VUI by causing denial-of-service (DoS) of user requests. Therefore, it is of paramount importance to enable noise-resistant speech sensing in VUI for executing critical tasks with superior efficiency and precision in robust environments. To this end, we investigate the feasibility of employing radio-frequency signals, such as millimeter wave (mmWave) for sensing the noise-resistant voice of an individual. We first perform an in-depth study behind the rationale of voice generation and resulting vocal vibrations. From the obtained insights, we present WaveEar, an end-to-end noise-resistant speech sensing system. WaveEar comprises a lowcost mmWave probe to localize the position of the speaker among multiple people and direct the mmWave signals towards the nearthroat region of the speaker for sensing his/her vocal vibrations. The received signal, containing the speech information, is fed to our novel deep neural network for recovering the voice through exhaustive extraction. Our experimental evaluation under real-world scenarios with 21 participants shows the effectiveness of WaveEar to precisely infer the noise-resistant voice and enable a pervasive VUI in modern electronic devices.\n\nINTRODUCTION\n\nWith the prevalence of smart devices and home automation, voice has become a popular medium of communication in the Internet of Things (IoT) environment through voice-user interfaces (VUIs). Through voice interactions, VUIs facilitate various daily tasks with superior efficiency such as private communication (e.g., phone calls, messages, emails), information sharing (e.g., voice search) and even monetary transactions [1]. Reports reveal that VUI is striving to become the primary user interface for smart homes and connected lifestyle [2][3][4]. The voice-enabled smart home market is expected to reach $1.3 billion by 2022 [5].\n\nAlthough VUI is considered as the most promising interface, a fundamental problem needs to be addressed before it can completely replace the physical user interaction. The existing VUIs rely on an acoustic-based approach of using a low-cost microphone or array of microphones for sensing the voice signals of the user [3,6].\n\nHowever, these sensors are highly sensitive to the ambient noise and unable to provide high-quality reproduction of original sound, also known as high-fidelity voice. The primary consequence of this limitation is audible interference during voice transmission in daily environments (e.g., airports, public transportation) with more severe outcomes leading to denial-of-service (DoS) during voice search and breach of VUI security through inaudible voice commands [7,8].\n\nMitigating the influence of ambient noise during voice sensing has a long and rich history in literature and is a core topic in the acoustic inference community. The classic solution is active noise cancellation (ANC) which leverages additional microphones to generate the anti-noise signals and nullify the noise from the environment [9]. However, when there are multiple noise sources in the environment, ANC requires multiple microphones (one for each noise channel) to cancel the noise interference [10]. Recent studies explore the feasibility of employing non-acoustic sensors, e.g., electromagnetic motion sensors [11,12] and bone-conduction microphones [13] for recovering the speech signals by analyzing the skin-vibrations. Nevertheless, these sensors require prolonged contact, thereby obstructing the daily activities of the user and frequently causing skin irritations. While non-contact lip reading utilizes the visual motion of the lip to infer the phoneme, i.e., the minimum unit in speech, it requires a highly directional front camera to record the lip motion which is impractical in real-world scenarios [14]. As a result, how to acquire noise-resistant voice in an unobtrusive, robust and cost-efficient manner to enable pervasive VUI in electronic devices remains an unsolved challenge.\n\nIn this paper, we take an exploratory step and unveil the opportunity of leveraging radio-frequency (RF) signals for acquiring the noise-resistant voice of an individual. We observe that when a person speaks, the vibrations of the vocal cord containing the acoustic information are propagated through the body tissue and can be measured on the skin's surface. Motivated by this observation, we focus on analyzing the unique distribution of RF signals caused by the minute vibrations near the throat region. Due to its short wavelength and strong reflectance properties, high-frequency RF signals such as millimeter wave (mmWave), can effectively infer the vocal vibrations and eventually recover the voice information. The objective of this paper is to shift from the conventional microphonebased voice recording that has been utilized for decades to a new mmWave sensing of noise-resistant voice. This will facilitate three vital advantages for voice sensing in real-world applications: 1) Noise-resistant: Due to directional beamforming of mmWave signals, the proposed system will be insensitive to the ambient noise even in robust environments and under flexible human dynamics (e.g., movement). 2) Informative: The high-frequency mmWave allows inference of comprehensive speech information regardless of the language, pitch or pronunciation of the user. 3) Secure: The mmWave sensing provides superior security for VUI due to its resilience to inaudible white noise or hidden voice commands. Naturally, to enable a pervasive VUI with these advantages, two major challenges will need to be addressed. 1) Unlike the omnidirectional sensing in microphones, mmWave has a highly directional beamforming. How can we design our system to support non-invasive voice sensing when the user begins her speaking in an unfixed position? 2) How to reconstruct noise-resistant voice from the unstable mmWave signals which comprise not only the vocal vibrations but also other interference from environmental reflections (e.g., objects, multiple users) and human motion artifacts?\n\nTo this end, we propose our system, WaveEar, to facilitate remote sensing of noise-resistant voice in robust environments as shown in Figure 1. We prototype and develop a portable (11.8 cm \u00d7 4.5 cm \u00d7 1.8 cm), light-weight (45.4 \u0434), and low-cost (less than $100) 24GHz mmWave probe that is able to localize the legitimate speaker among multiple users and non-invasively sense high-resolution vocal vibrations. Considering the fact that deep neural networks have demonstrated immense capability to learn the mappings between two different domains, we design and implement a novel network architecture, namely Wave-voice Net to recover noise-resistant voice from the received mmWave signal. The voice recovery procedure contains three steps. First, we enhance the time-and-frequency domain information by transforming the reflected 1-D time-domain signal to 2-D spectrogram. Afterward, a residual architecture based neural network is employed to learn and establish the mapping between the mmWave spectrogram and voice spectrogram. Finally, we adopt a phase reconstruction algorithm to recover the voice. As a first exploration study for achieving noise-resistant speech sensing via mmWave, we select a set of metrics, e.g., Mel-Cepstral Distortion (MCD), to better evaluate our system. Using these metrics, we perform a comprehensive evaluation with 21 participants to validate the effectiveness of WaveEar in inferring the noise-resistant voice under real-world scenarios.\n\nSummary: The contributions of our work are three-fold:\n\n\u2022 We develop WaveEar, a noise-resistant vocal sensing system that leverages the disturbance of the mmWave signal caused by near-throat skin vibration to recover the human voice. \u2022 We design a Frequency-Modulated Continuous-Wave (FMCW) based mmWave probe which is capable of localizing the speaker among multiple people and sensing the minute nearthroat vocal vibrations. A novel deep neural network, i.e., Wave-voice Net is proposed to reconstruct the noise-resistant human voice from the probe signal. \u2022 We evaluate the robustness of WaveEar under the interference from distinct ambient noises and other factors such as sensing distance, motion artifacts and emotional state of the user. Our results demonstrate that the system achieves superior performance and provides a foundation for enabling a pervasive VUI in modern electronic devices.\n\n\nSPEECH SENSING VIA MMWAVE: CONCEPT AND PRELIMINARIES\n\nIn this section, we present the background on physiology of the human voice and rationale behind obtaining noise-resistant speech through sensing the near-throat vocal vibration. We also perform a feasibility study to prove this concept.\n\n\nThe Principle of Human Voice\n\nThe human voice is generated from an integrated effort of three physiological organs, i.e., lungs, vocal cords, and articulators. During the exhalation process, the lungs create sufficient airflow that passes over the vocal cords. This air pressure causes vocal cord vibrations and produces audible pulses, commonly known as voiced and unvoiced sound. The articulatory organs (e.g., tongue, palate, thyroid, uvula) further articulate and filter the sound to strengthen or weaken it while the larynx regulates the tension on vocal cords to calibrate the pitch and tone. The vocal cords and articulatory organs are adept at producing a highly comprehensive array of sounds containing profound information.\n\n\nVocal vibration\n\nAirflow Rx Tx Figure 2: The vocal vibration distorts the skin-reflected mm-Wave signal. The distortion has a strong correlation with human speech.\n\n\nmmWave Sensing on Vocal Vibration\n\nBased on the knowledge of vocal vibration, we hypothesize that an FMCW probe can be used to sense the propagated vocal vibration on the skin's surface around the throat and mouth region. This region is in proximity to the human articulatory system to guarantee the strong correlation between vibration and human voice. As illustrated in Figure 2, the FMCW probe transmits the saw-tooth wave in the frequency domain to human throat periodically. When the person is speaking, the airflow goes through the articulatory system, issuing the vocal vibration and voice signal. The vocal vibration disturbs the skin-reflect mmWave, which will be sensed by the mmWave probe.\n\nDue to the short wavelength of the mmWave, even a small displacement at the micron-level can be detected by calculating the phase change of a few degrees. In particular, given the \u03bb as the length of transmitted wave, the minute displacement d of the skin can be calculated as:\nd = \u03bb\u2206\u03c6 4\u03c0 ,(1)\nwhere \u2206\u03c6 is the phase shift resolution. It is related with the probe's signal noise ratio (SNR) as:\n\u2206\u03c6 2 = 1/(2 \u00b7 SNR).(2)\nA typical value of the SNR is in the range of 15dB-20dB. We observe from Eq. (1) that a shorter wavelength of transmitted wave results in a higher resolution of the skin minute displacement. Therefore, taking the phase noise into consideration, we choose high-frequency mmWave at 24GHz as the transmitted wave to reach the displacement resolution at around 1 mm. \n\n\nA Feasibility Study\n\nTo validate the relationship between the received mmWave signal and the speech information, we conduct this proof-of-concept. In our experiment, we ask one subject to speak \"A, B, C, D\" two times in a normal tone. The mmWave probe is fixed in front of the subject and pointed to her throat from a distance of 40cm. Figure 3 shows the envelope of the time-domain reflected mmWave signals received by the probe when the subject is speaking together with the corresponding voice signals. We observe that the mmWave signals for the same character show a high similarity, while there is an obvious discrepancy between the signals from different characters. Based on this observation, we prove that the sensed mmWave signal has a persistent and unique relationship with the speech information, which matches our claim in Sections 2.1 and 2.2.\n\n\nSYSTEM OVERVIEW\n\nIn this paper, we propose WaveEar, a noise-resistant speech sensing system. The end-to-end system overview is shown in Figure 4.\n\nWaveEar Hardware: A new mmWave probe with an audio interface is designed to remotely and accurately acquire the human vocal vibration for voice reconstruction. Specifically, the probe first locates the human vocal vibration by the throat localization module. Then, it transmits the continuous wave and processes the reflected signal. After that, the received data are sent to the device for voice reconstruction via the line-in audio card converter.\n\nWaveEar Software: The Wave-voice Net on the device side performs voice reconstruction. Upon receiving data, the data representation module first transforms the data to enhance the temporal and spatial voice information explicitly. Then, the represented spectrogram is fed to the deep network, and our encode-decode architecture achieves the spectrogram transformation. The final voice recover is achieved by recovering the implicit phase information from the spectrogram. \n\n\nWAVEEAR VOCAL SENSING\n\nIn this section, we first introduce the design of the mmWave probe, which includes a three-board structure. Then, we present the vocal vibration locating algorithm that can facilitate spatial vocal sensing in detail.  The RF board achieves the transmission and reception of the frequency-modulated RF signal. Because the waveform is continuous, we employ two separate RF channels to minimize coupling between Tx and Rx. The RF front-end employs the phased antenna arrays for two reasons. First, mmWave attenuates rapidly out to a few meters, and antenna array can help generate high antenna gains to fight with this attenuation. Second, the antenna array can steer the beam to a specific direction and thereby reduce the interference from the background.\n\n\nmmWave Probe Design\n\nThe baseband board achieves the signal generation and modulation. It employs an operational-amplifier based circuit to generate an analog sawtooth signal and a reference pulse sequence (RPS) which is locked to the sawtooth ramp voltage signal. Afterward, the analog sawtooth signal controls a free-running voltage-controlled oscillator (VCO) to generate the desired frequency-modulated RF signal. The power splitter then divides the output signal of VCO into two parts. One part is fed to the Tx antenna, and the other part is fed to the mixer to help demodulation of the received signal.\n\nAfter being amplified by the baseband amplifiers, the received signals are modulated with the low-IF digital local oscillator. Then, the two-channel modulated signals are sampled by the audio card. \n\n\nName Description\n\nMean Value\nx = 1 N N i=1 x(i) Standard Deviation \u03c3 = 1 N \u22121 N i=1 (x(i) \u2212 x) 2 Skewness \u03b3 = 1 N N i=1 ( x (i)\u2212x \u03c3 ) 3 Kurtosis \u03b2 = 1 N N i=1 ( x (i)\u2212x \u03c3 ) 4 \u2212 3 RMS Amplitude \u03bb = 1 N N i=1 x(i) 2\n\nThroat Localization\n\nAlthough the above-illustrated probe design facilitates vibration sensing, the WaveEar cannot achieve high-resolution sensing without the localization of throat vibration. As aforementioned, the mmWave probe employs a phased directional array with a 3 dB beamwidth of 12\u00b0, out of which antenna gain attenuates rapidly. When the probe misses the throat, the system achieves a low antenna gain at the side lobe and thereby brings a low SNR. According to the Eq. (1) and Eq. (2), low SNR significantly hurts the sensing resolution and therefore impairs the performance of speech recovery.\n\nOur objective is to ensure the mmWave probe can always achieve the strongest SNR by steering beam to the location of the vocal vibration. To achieve this goal, we propose our solution based on the fact that the FMCW probe is sensitive to distance and displacement. Considering the pattern of throat vibration is significantly different from the background (e.g., static objects or slow movement), we employ a classifier to differentiate the background and throat vibration according to the different echo signals which our FMCW probe receives. First of all, the mmWave probe steers beam in all directions to sense the environment. Afterward, a feature detector extracts time domain features (see Table 1), and a classifier is employed to differentiate the throat vibration from backgrounds. These procedures can be completed within 5 ms through digital beamforming [15], a widely used technique in the mmWave systems. Moreover, adaptive beam training protocols could further reduce the localization time with 2 ms [16] (including the 1-ms inference time cost by a classifier such as decision tree). Through throat localization, WaveEar achieves the highest resolution in vibration sensing.\n\n\nWAVEEAR PROCESSING SCHEME\n\nIn this section, we introduce the voice recovery methodology. Due to the collaboration among all the vocal organs, the information contained in the voice is more than just vocal vibration. Therefore, the WaveEar leverages a deep-neural-network based processing scheme to make an end-to-end inference from reflected mmWave to voice. The scheme contains the spectrogram representation module for feature enhancement, the Wave-voice Net for spectrogram transformation, and the phase reconstruction algorithm to recover real voice from the spectrogram.\n\n\nSpectrogram Representation to Augment Features\n\nGiven the mmWave signal, intuition is to learn the mapping between mmWave and voice signals in the time domain. However, in the signal from the time domain, the frequency-information containing acoustic characteristics is implicit. Deep learning may be a choice to learn the implicit information by cascading more layers, but tremendous extra parameters make training extremely timeconsuming and prone to failure. Thereby, we utilize spectrogram as the inner presentation of mmWave and voice signals in Wave-voice Net. The mmWave signal captured by the audio interface will be segmented into a series of small windows before the data representation, which guarantees an appropriate time-domain resolution for each spectrogram. After segmentation, 1-D signals in these small windows will be transformed to 3-D spectrograms by\nSP{x(t)}(m, \u03c9) = \u221e n=\u2212\u221e x[n]w[n \u2212 m]e \u2212j\u03c9n 2 ,(3)\nwhere the Hanning window is chosen to prevent the contained human voice information from the high-frequency interference and spectral leakage:\nw[n] = 1 + cos 2\u03c0 \u00b7 n N \u22121 2 .\n(4) Figure 6: The structure of Wave-voice Net based on encoders and decoders to perform exhaustive extraction of speech information from voice and mmWave spectrogram.\n\nThe 2-D spectrogram presents more correlations and patterns, such as formant and sound intensity, which can be easily extracted by a deep learning model. We detail the novel Wave-voice Net in the next subsection.\n\n\nWave-voice Net\n\nOur objective is to solve the image transformation problem between the mmWave spectrogram and the voice spectrogram. As observed in Figure 6, the transformation between mmWave signal and voice requires changing large parts of the image to appear coherently. Therefore, it is advantageous for each pixel in the output to have a large effective receptive field in the input. Consequently, we design our neural network to include an encoder and a decoder.\n\nEncoder. The Wave-voice Net learns the mapping between the mmWave spectrogram and voice spectrogram. Without encoding, each additional 3 \u00d7 3 convolutional layer increases the effective receptive field size by 2. Instead, after encoding by a factor (e.g., D), each 3 \u00d7 3 convolution layer increases its effective receptive field size by 2D. Given the same number of convolutional layers, the encoding achieves larger effective receptive fields. Specifically, our encoder contains three 3 \u00d7 3 convolutional layers. The first layer contains 32 filters, the second layer contains 64 filters, and the third one contains 128 filters. Each of them employs a stride of 2 to achieve encoding.\n\nResidual Blocks. As aforementioned in [17], neural networks need multiple layers to learn features at various levels of abstraction, which benefits the representation. Therefore, we are motivated to add extra weighted layers besides the encoder and decoder. We introduce the residual blocks in the encoder and decoder block. The reason is that the residual architecture contains the shortcut connections which help convergence. The shortcut connections can be formulated as:\ny = F (x, {W i }) + x,(5)\nwhere F (x, {W i }) represents the multiple convolutional layers, and F (x, {W i }) + x represents element-wise addition. According to He et al. [18], shortcut connections make it easy for the network to learn the identity function. This is an appealing property for transformation between the mmWave signal and voice since in most cases the output of the voice spectrogram shares structures with the input mmWave probe spectrogram. Specifically, our network contains five residual blocks, each of which contains two 3 \u00d7 3 convolutional layers.\n\nDecoder and Constraint. The decoder contains two 3\u00d73 convolutional layers with a stride of 1/2 to enable upsampling. To facilitate the convergence, we employ the mean square error (MSE) to constrain the loss l between the predictions and the ground truths, which can be formulated as follows:\nL = {l 1 , . . . , l N } \u22a4 , l n = (x n \u2212 y n ) 2 ,(6)\nwhere N is the batch size, x is the input and the y is the target. Then the batch loss \u2113 can be calculated as:\n\u2113(x, y) = mean(L),(7)\nWave-voice Net then minimizes the loss function \u2113(x, y) through back propagation to recover the voice from the the mmWave signal. As shown in Figure 4, it achieves an end-to-end transformation from a 3 \u00d7 256 \u00d7 256 mmWave spectrogram to a 3 \u00d7 256 \u00d7 256 voice spectrogram.\n\n\nPhase Identification to Reconstruct Voice\n\nThe restored spectrogram contains amplitude and phase information of the voice that is necessary to determine the voice. However, the phase information is implicit in the spectrogram. To tackle this problem and reconstruct the final high-fidelity voice, we apply spectrogram inversion. In particular, we aim to estimate the phases from the spectrogram and thereby reconstruct the time-domain high-fidelity voice signal.\n\nPhase Initialization. In the magnitude spectrum, we identify the bins that represent peaks by comparing the magnitude of each bin j with that of neighbors, j + 1 and j \u2212 1. We consider bin j as a peak if |X (mS, \u03c9 j )| > |X (mS, \u03c9 j\u22121 )| and |X (mS, \u03c9 j )| > |X (mS, \u03c9 j+1 )|.\n\nHere m is the index of the frame, S refers to the synthesis step size, which is one quarter of the frame length N , and \u03c9 j = 2\u03c0 j N presents the frequency of bin j. Afterward, we employ a phase accumulator \u03d5 [19] to represent the phase value for bin j at frame m,\n\u03d5 m, j = \u03d5 (m\u22121), j + S\u03c9 j .(8)\nAs the phases at the peaks have been determined, the phases at the remaining bins can be determined depending on the Pi-phase alternation strategy [20].\n\nGriffin-Lim Reconstruction. Starting with the initial phase \u03d5 m estimated above, we then employ the G&L algorithm [21] to iteratively renew the estimation:\nx i+1 (n) = \u221e m=\u2212\u221e w(n \u2212 mS) 1 2\u03c0 \u222b \u03c0 \u03c9=\u2212\u03c0X i (mS, \u03c9)e \u2212j\u03c9n d\u03c9 \u221e m=\u2212\u221e w 2 (n \u2212 mS) ,(9)\nwhereX i (mS, \u03c9) is the Short-Time Fourier Transform (STFT) of x i (n) with the following magnitude constraint, which can be described as:X\ni (mS, \u03c9) = X i (mS, \u03c9) |X (mS, \u03c9)| |X i (mS, \u03c9)| ,(10)\nwhere |X (mS, \u03c9)| is the Short-Time Fourier Transform Magnitude (STFTM) of the original signal x(n). According to the results from [21], the G&L algorithm can reduce the Signal-to-Error Ratio (SER) through each iteration.\n\n6 SYSTEM IMPLEMENTATION AND EVALUATION SETUP 6.1 mmWave Probe Implementation  We integrate the baseband board into the FR4 substrate, which supports the RF board. On the Tx side, three voltage regulators help stabilize the power to generate the mmWave as our design. On the Rx side, high-speed difference amplifiers are used to restrain the noise. The amplified signal then flows to the MCU board, where the embedded MCU is MSP430F2610. The on-chip 12-bit ADC of the MCU converts the amplified signal to the digital signal and feeds it to the 3.5 mm AUX port. We place the power interface in the MCU board, and the MCU board cascades the baseband board to power the entire probe.\n\nIntegrating all of these components, the size of the probe reaches 11.8 cm (4.65 in) \u00d7 4.5 cm (4.65 in) \u00d7 1.5 cm (0.59 in) and the total weights is 45.4 g. We use a 5.9 V power supply so that all ICs work at their nominal voltage. As a result, the total power consumption is 1.23W.\n\nWe adopt the time interval (dwell time) of 1000 milliseconds for throat localization, and we achieve an average accuracy of 96.7%. We observe that as the dwell time increases, the classification accuracy is promoted rapidly when the dwell time is less than 1000 milliseconds long. If we continue to increase the dwell time, the classification accuracy becomes progressively saturated.\n\n\nWave-voice Net Implementation\n\nWe implement the proposed Wave-Net in Pytorch. To avoid loss of the voice information, we adopt the reflection padding with paddin\u0434 size = kernel size/2 in all the convolutional layers. Moreover, the size of reflection padding guarantees a fixed feature map size under convolution stride = 1, thereby supporting our sequential residual block design of Wave-voice Net. We use the nearest upsampling in the decoder to retain time correlation of the restored voice spectrogram. Adam optimizer with an initial learning rate of 0.001 is used to train the Wave-voice Net. Additionally, we use a batch size of 32 and reduce the learning rate to 10% in every 128 epochs during the training process to fine tune the Wave-voice Net.\n\n\nDataset\n\nPreparation: We conduct extensive experiments to prove the voice reconstructing capability of the proposed WaveEar. Figure 8 shows the experimental setup. A subject sits down in a normal position. We place the implemented mmWave probe in front of the subject and align it to the subject's throat. The mmWave probe connects to a 5.9V power supply, and the working current is 0.21A. A microphone is placed close to the probe to record the human voice. We attach another microphone to the subject's collar to collect near-throat voice as the reference signal. We deploy three laptops with Windows 10 and the open-source audio software Audacity to record the signals from the mmWave probe and the microphones, respectively. The three laptops are configured to start the recording at the same time so that the signals are aligned. The training and inference processes are performed by a workstation equipped with an Intel Xeon CPU E5-1620 v4 @ 3.50 GHz and an Nvidia Titan Xp 12 GB. Participants: Our experiments involve 21 participants including 11 males and 10 females. The participants are recruited by emails including both graduate students and undergraduate students. These participants include both native and non-native English speakers with ages from 21 to 35 years old. We explicitly tell the participants that the purpose of the experiments is to perform voice reconstruction.\n\nData Collection: In our experiments, we choose five reading materials which cover all the phonemes in English and being used in many phonetic tests as the reading test benchmark. Table 2 shows the five materials and their statistics information. We denote the words per sentence, characters per word, and the average reading speed (words per second) of each material by \"W/S\", \"C/W\", and \"Speed\", respectively. During the experiments, all 21 subjects read the designated materials three times under the controlled quiet environment. We apply one-second segmentation and randomly choose the samples from two male subjects and two female subjects as the test data. Thus, there are 35394 samples used for training and 8328 samples for testing. Note that we have evaluated the system with different experimental setups and data partition for the varying ambient sounds and robustness analysis which will be described in these sections in particular. \n\n\nEvaluation Metrics\n\nAs a novel voice sensing system, WaveEar is compared with the conventional acoustic voice sensing system in the evaluation. Moreover, we also adapt objective quality assessments to evaluate the reconstructed voice. The evaluation metrics include: Speaking-Signal-to-Noise Ratio (SSNR). WaveEar is a novel vocal sensing system that directly senses the vocal vibration but not the sound pressure. The typical reference signal for the SNR evaluation of the acoustic systems is the standard 1 kHz, 94 dB Sound Pressure Level (SPL) signal generated by the test instrument. However, such typical reference signal is not suitable for the proposed system. Therefore, we use the voice signal sensed by WaveEar as the reference signal and name the new measurement as Speaking-Signal-to-Noise Ratio (SSNR). Similar to the SNR, a higher SSNR indicates the system has a higher capacity for extracting the voice rather than the ground noise while sensing the voice.\n\nSensitivity. Sensitivity is measured by the same standard reference signal with SNR and can be formulated as follows:\n20 \u00d7 log 10 ( Sensitivity mV /P a Output ARE F ),(11)\nwhere Output ARE F is the 1000mV /Pa (1V /Pa) reference output ratio. We replace the reference signal with the voice signal for the same reason as we propose the SSNR. Higher sensitivity provides a better resistance to the signal attenuation.\n\nWord Error Rate (WER). In speech recognition, WER measures the difference between the reference and the recovered words (also named as hypothesis words). Specifically, the WER is formulated as follows:\nW ER = S + D + I S + D + C ,(12)\nwhere S, D, I and C represent the number of substitutions, deletions, insertions, and correct words, respectively. We compare the WER of the reconstructed voices with that of the reference signal collected from the attached microphone based on the Google Cloud Speech-to-Text API [22]. The output is from the \"Default model\" and language is set to \"English\". Besides, we remove all the punctuation in the output text and disable the Speaker diarization.\n\nA lower WER indicates that the voice reconstructed by WaveEar is considered closer to the reference signal in the Speech-to-Text application.\n\nMel-Cepstral Distortion. MCD is a psychoacoustically motivated measure, which can evaluate the voice considering the human acoustic system preference. The MCD of the k-th frame is denoted by:\nMCD = 13 i=1 [MC X (i, k) \u2212 MC Y (i, k)] 2 ,(13)\nwhere the Mel-frequency cepstral coefficients (MFCCs) is calculated as:\nMC X (i, k) = 20 i=1 X k,n cos i(n \u2212 1 2 ) \u03c0 20 , i \u2208 [1, M](14)\nand X k,n = log 10\nM |X (k, m)| 2 \u00b7 w n m .(15)\nWe use the audio signal from the attached microphone as the reference signal. A lower MCD indicates that the voice sensed by the system is more close to the original voice that humans hear.\n\n\nPERFORMANCE ANALYSIS\n\nIn this section, we analyze the results of our extensive experiments, aiming to address the following questions: 1) the overall voice reconstruction performance of WaveEar; 2) performance comparison for various noise types; 3) robustness of the WaveEar.\n\n\nWaveEar Performance\n\nWe evaluate the ability of WaveEar to reconstruct human voice in the controlled lab environment.\n\nTo give an intuitive awareness of the voice reconstruction performance of the WaveEar, we illustrate the spectrograms of the microphone-detected signals and the reconstructed voice from the Wave-voice Net in Figure 9. The overall shapes of the spectrograms are similar, indicating that the general variation of the human voice is reserved. The reason is we embedded a number of convolution layers into the Wave-voice Net, which has a strong ability to capture the high-level feature of the spectrogram. In addition, there is almost no difference in the color depth between the microphone's and the reconstructed voice signals. This observation shows that the energy and the magnitude of the human voice on a different frequency are not distorted with the help of the shortcut connection in residual blocks. The shortcut connection makes the input a part of the output, preventing the WaveEar from the data vanishing problem. Much reserved detail in low-frequency also confirms the excellent performance of the proposed WaveEar. We annotated these details, i.e., formants, of the voice with their corresponding words in both the spectrograms of the microphone's and the reconstructed voice signal. Formants are distinctive frequency components of the human voice, which help humans detect the source of the voice. Having general awareness of the performance, we deepen our performance analysis by measuring WaveEar from different aspects using the four metrics aforementioned. Then, we further evaluate the WaveEar performance by the proposed three metrics. Figure 10 shows the average SSNR of the WaveEar and the microphone over the test set. In this experiment, subjects keep silent for 5 minutes when they sit in the chair so that we can capture the ground noise from both microphone and WaveEar. We observed that both WaveEar and microphone provide high SSNR during recording of human speech in the controlled environment. Specifically, the average SSNR of WaveEar is 38.85 dB while that of the microphone is 41.18 dB, which implies that WaveEar only introduces minute ground noise into the sensed voice. The results are because the mmWave probe is able to recognize and locate the near-throat vocal vibration leveraging the proposed Algorithm 1. When subjects keep silent, WaveEar will pause the wave-voice inference and stay mute. We use the reference signal from the attached microphone to calculate the sensitivity. The results are illustrated in Figure 11. For each reading material, the sensitivities of WaveEar and microphone have no significant difference. The mean sensitivities of WaveEar and microphone are -9.516 dBV and -9.464 dBV, respectively. Even mmWave attenuates more rapidly than sound waves, WaveEar senses the voice in a different way, i.e., it learns the map from the reflected mmWave signal to speech voice directly, which compensates the sensitivity decreasing caused by the signal attenuation. Moreover, we evaluate the MCD of signals from WaveEar and microphone. As shown in Figure 12, the reported mean MCD of WaveEar is lower than 1.5, slightly higher than that of the microphone. These results imply that WaveEar is able to sense the voice as clear as the microphone. Finally, we leverage the WER to show the performance of voice reconstruction from the voice-to-text perspective. Figure 13 shows that the WER of WaveEar over the five reading materials is lower than 6%. Because Google Cloud Speech-to-Text not only leverages the voice itself but also uses the speech context to infer the text, a sequential word error happens only when a word is not reconstructed perfectly. Even WaveEar reports a low WER on Google Cloud Speech-to-Text, a joint optimization between WaveEar and the speech-to-text system would further enhance the speech-to-text application experience. In conclusion, our results demonstrate that a noise-resistant speech sensing can be achieved by WaveEar.\n\n\nResistance Test of Ambient Noise\n\nThe ambient environment can introduce different noises or even interfere with the probe hardware operation. In this subsection, we evaluate voice reconstruction performance across three different types of real-world noises with different spectral characteristics: pop music, presentation, and water flow. In the experiments, one subject wears a noise-isolation headset and reads \"The north wind and the sun\" three times while a loudspeaker plays the three types of noises. The received mmWave signals are fed to the model trained on the training set to reconstruct the speech voice. Pop music: Pop music consists of sounds coming from instruments and the singer. Figure 14(a) and (b) show the signals recorded by microphone and reconstructed by WaveEar in spectrogram. In the spectrogram of the microphone's signal, the beats of the music and the harmonics of the song are evident and mixed with the speech voice. In contrast, the spectrogram of the WaveEar outputs is clear with the speech voice reserved. Presentation: As shown in Figure 14(c) and (d), the presentation voice has a large overlap with the reading voice of the subject. Besides, the echo of the presentation is recorded by microphone, messing up the signal. Signal reconstructed by WaveEar does not contain the frequency components of the presentation voice and the echo. Water flow: The frequency of water flow noise is typically at 1-2KHz, which locates in the frequency range of human voice. Figure 14(e) demonstrates that the water flow noise forms a shallow horizon frequency band, which overlaps with the frequency components of speech voice in the spectrogram of the signal recorded by microphone. Compared to it, the spectrogram of WaveEar's output does not contain this frequency band.\n\nWith all the observations above, we draw the conclusion that the proposed WaveEar is immune from the ambient noise.\n\n\nRobustness Analysis\n\nImpact of Distance and Orientation: In practical scenarios like smart homes, users should be able to walk around with WaveEar according to their accessibility, without placing it at a fixed location. However, such a convenient practice will result in variant distance and orientation between the users' throat and the mmWave probe. Therefore, we are motivated to analyze the performance of WaveEar under different distances and orientations. Specifically, the mm-Wave probe is placed in different orientations (from 0 \u2022 to 90 \u2022 ) and different distances (from 0.5 m to 2 m) to the subject's throat when subjects read \"Comma Gets a Cure\" material. Figure 15 shows the measurement results. The MCD value remains low (less than 1.45) when the sensing distance is within 1.5 m. As the sensing distance exceeds 1.8 m, MCD value increases faster. As for the orientation,   the MCD value slightly changes when the orientation varies. As the distance and orientation increase, WER stabilizes at around 5.5%. We can conclude that WaveEar is robust to the orientation changes, and it can display superior performance within 1.5m sensing distance. Thereby, WaveEar can support robust and convenient speech sensing in real practice. Impact of Body Motion: For comfortability, users always want to communicate with the WaveEar when needed, without putting down the things in their hands, such as driving. Therefore, we are curious about how well the WaveEar can be utilized for effective speech sensing under the effect of body motion. Specifically, we conduct a set of experiments to show the impact of body motion activities to the WaveEar performance, ranging from small to largerange motions. Twenty-one subjects are involved in this experiment and each one is required to perform four groups of body motion activities when reading the first three sentences of \"Arthur the Rat\" material, including making phone calls, typing, mimic driving, and rhythmical movement with beats. The overall performance on MCD when reading \"Arthur the Rat\" material is used as a control group without body motions. The mmWave probe is placed in front of the subject's body, where the distance and orientation between the subject and the probe are set at 0.5 m and 0 \u2022 . With the current experiment setting, we examine the average MCD value of 21 subjects among different body motion activities. The results of the comparison are shown in Figure 16. We can observe that when subjects are making phone calls or typing during reading, the average MCD value almost keeps unchanged compared with the control group. While subjects perform mimic driving activities during reading, the average MCD value slightly increases. As for the large-motions such as rhythmical movement with the beats, the MCD value gains an obvious increment. The performance shows that small-range body motion artifacts can be removed via setting parameters of Wave-voice Net. Since our system is evaluated based on a training set containing a limited number of young subjects, some large-range body motions cannot be completely eliminated. A larger number of subjects with a more diverse background will help training the WaveEar system to become more robust.\n\nImpact of Emotional State: A user's emotional state is changeable and one of its external expressions is speaking volume and speaking speed. Thereby, it is interesting to verify whether the user's emotional state can affect the WaveEar performance. According to [23], music can evoke emotions, 21 subjects are arranged to listen to blues, classic music, pop music, and heavy metal music via earphones, respectively. When reading \"The Rainbow Passage\", collect subjects' vocal vibration signals in different emotional states. We observe that the average time reading \"The Rainbow Passage\" under blues is 157s while the average time under pop music is 151s, and even more than 90% subjects spend more reading time under blues. These preliminary results indicate that subjects' emotional states are changeable in our experiment settings. The mmWave probe is also positioned at a distance of 0.5m and an orientation of 0 \u2022 facing toward the subject. Since Simon [24] proposed that women are more emotional than men, we measure the average MCD values of both men and women under different emotional states. Figure 17 shows the MCD comparison results. We observe that the average MCD values are almost stable when reading under different types of music, and the average MCD values of men and women differ a little. These results show that the WaveEar performance is robust to users' different emotional states regardless of a subject's sex.\n\n\nIMPLICATIONS 8.1 Implications on Smart Device Architecture\n\nBased on the real-world development process, this is an upcoming goal to market the mmWave-capable smartphones or smart devices.\n\nSmart Device Architecture in 5G and Beyond: Due to its high frequency and board bands, mmWave can facilitate next-generation high-speed communication, i.e., 5G. Ericsson reports that the number of smart devices accessing 5G networks will reach one billion over the next 5 years. Currently, the first-generation smartphone equipping 5G baseband is introduced to the market. Therefore, we take the smartphone exploration as one illustration, as this end other devices have a similar mechanism. Compared to the model smartphone, the architecture of 5G smartphone differs mainly in the hardware, including the new phased array antenna, the extra beamforming module [25]. First, the embedded phased array antennas are separated to different parts of the entire phone body, which enables beamforming required by 5G, thereby providing spatial multiplexing. This leads to the non-physical eSIM design that provides more space inside the phone body. Second, the size of the extra beamforming model is well controlled below 10 \u00d7 10 \u00d7 1.5 mm with the help of the emerging commercial solutions. Therefore, more beamforming chipsets are embedded on the back housing of the handset to support higher order MIMO. Given these considerations, we next discuss the compatibility of WaveEar with contemporary devices.\n\nCompatibility with Smart Devices: On the hardware side, as aforementioned, the schemes of the 5G smartphone have comprised the phased array and beamforming module, which are the key components required by WaveEar front-end. On the software side, the wave-voice spectrogram and spectrogram inversion are easy to implement leveraging the high-speed DSP in the smartphone. Although Wave-voice Net is deployed on the server side and built with Pytorch, the ONNX format and TensorFlow mobile framework can deploy the Wave-voice Net to mobile devices, thereby training a more personalized model and further improving fidelity. Moreover, more and more AI chips have been applied to smart device products [26], which naturally meets the needs for Wave-voice Net. To sum, we can conclude that our system WaveEar is compatible or can be updated quickly with contemporary devices or smartphones.\n\n\nImplications on Privacy\n\nWe consider the potential privacy leakage via mmWave side-channel and the related permission management.\n\nmmWave-voice Side-channel: mmWave-voice can be a new sidechannel that has the potential to be utilized by malicious applications or attackers. Once the malicious application controls the mmWave module of the smart device in the background, it can discover the voice of the nearby victim and extract sensitive information from the audio signal, although the user does not give any audio permission to the malicious application. Even smart devices without microphones are able to sense the voice. This implies the user should be aware of the related permission management as illustrated in the next section.\n\nPermission Management: Our system empowers the ability of IoT devices (e.g., smart refrigerator) to interact with the user through physical interaction capabilities. Although such physical interactions of IoT devices could bring significant convenience to end users, they also could be potentially exploited by attackers to jeopardize IoT environments. The access control policies can be generated automatically by machine learning techniques or from users' inputs. Thus, our system could achieve runtime interaction control without modifying existing applications.\n\n\nImplications on Cocktail Party Effect\n\nIn real practice, it is not uncommon for multiple people to be present in the background of the legitimate user, hereafter Alice, who is speaking into the WaveEar. If those people are silent, our system can precisely localize Alice's position and record her vocal vibrations as previously shown in Section 4.2. However, the situation becomes more challenging when the people are also conversing which may cause WaveEar to incorrectly select the target speaker and attempt to record their vocal vibrations instead of Alice. To address this challenge, parameters such as sensing distance or body motion, which are unique to an individual, can be additionally leveraged to enhance the performance of localizing the target user's position. We plan to apply such techniques in our future versions of WaveEar.\n\n\nDISCUSSION\n\nAttenuation. Though Section 7.3 has shown that WaveEar can achieve low MCD and WER within the distance of 2m, the attenuation along the sensing distance due to the millimeter-level wavelength or caused by an object's block is still a major concern in the long-distance application of mmWave-based WaveEar. This is because the attenuation will result in a low SNR thereby decreasing the sensing quality. To further improve the sensing distance of WaveEar, advanced noise cancellation technology [27] is needed to increase the SNR. Specifically, empirical mode decomposition is applied to the received low SNR mmWave signal to decompose it into oscillatory components. Then, the mutual information entropy is calculated on each component and a threshold is applied to filter out the noise components.\n\nHuman-factor distortion. The large body motion and human orientation are the two major factors that would distort the reflected mmWave signal thereby decreasing the performance of voice reconstruction. For large body motion, while the WaveEar shows the resistance to the body motion as we show in Section 7.3, it could be vulnerable when large body motion presents because the large motion will shift the reflected signal in the frequency domain. Thus, the signal alignment [28] could be involved in the signal pre-processing stage to compensate the distortion. For human orientation, recent research [29] reveals that the vocal vibration near the throat, neck, and even ear can be used for human authentication, indicating that the vocal vibration from different near-throat areas also contains rich voice information for reconstruction. Based on this exploration, one potential extension of WaveEar is to reconstruct the voice even with the hand or jaw occlusion.\n\nEnvironmental independence. While the experiments of WaveEar do not appear to be overfitting, the reflected mmWave signal may carry latent information that is related to the environments where the experiments are conducted. Therefore, when WaveEar is applied to different environments, such as outdoors and conference rooms, removing these environmental specific information will help WaveEar to further enhance its performance. In particular, domain adversarial training [30] is a promising approach that could be integrated into WaveEar to filter out the latent environmental related information. This approach leverages a discriminator that tries to distinguish environments to indicate and suppress the latent environmental information, thereby facilitating the environmental independent voice reconstruction.\n\n\nRELATED WORK\n\nSpeech Sensing: Speech sensing has a long and rich research history. Holzrichter et al. [12] used the glottal electromagnetic motion sensor (GEMS) to measure the movement of posterior tracheal wall during voiced speech. Many researchers also used sensors to detect electrical signals from various muscles related to speech production, such as electromyograms (EMG) sensor [31], and electroglottalgrams sensor [32]. In addition, many silent speech interfaces (SSIs) exist, which mainly work for sensing bone structure vibration (e.g., bone-conduction sensors [33]) and skin vibration (e.g., throat microphone [34], skin-conduction accelerometers [35], physiological microphone [36]). Recently, quite a few researchers studied more novel approaches for voice sensing. Zhang et al. [37] re-used the smartphone as a Doppler radar to sense a user's articulatory gestures. Ding et al. [38] adopted Kinect sensor for voice sensing. Roy et al. [39] demonstrated that vibration motor embedded in phones and wearables can be used as a sound sensor. However, most of sensors and interfaces need to have close contact with users or locate in a fixed place, which may cause user discomfort.\n\nmmWave Sensing: In recent years, the development of millimeter wave (mmWave) radar makes it possible to sense biometrics and detect chemicals [40,41]. Lin et al. [42] developed a smart DC-coupled continuous-wave (CW) radar to sense the high-resolution cardiac motion in a non-contact and continuous manner. Lien et al. [43] used mmWave to sense gestures for low power human-computer interaction. Lin et al. [44] proposed a sleep monitoring system, where a Doppler radar sensor is used to collect respiration data. Liu et al. [45] developed a novel gas sensor system based on a mmWave radar operating at 60GHz, which supports higher sensitivity and lower cost. Vocal vibration sensing by mmWave, as a new remote sensing modality attracts more attention in the academic community [46]. However, vocal vibration is a part of human speech which is the result of the collaboration of all vocal organs. To facilitate modern VUI, most of which is based on voice recognition, there is still a gap from vocal vibration to voice. WaveEar, as a mmWave based speech sensing system, is inspired by these previous studies but different from them in its noise-resistance and robustness. 1 WaveEar lays the first stone for Wave-voice interaction.\n\n\nCONCLUSION\n\nIn this paper, we proposed a noise-resistant speech sensing system named WaveEar, to facilitate the human voice sensing for the widespread Voice-user interface (VUI) in a noisy environment. First, we explore the feasibility of acquiring high-quality voice by mmWave. Then, we proposed a low-cost 24 GHz mmWave probe with phased array, based on which we developed a throat localization approach. After that, the sensed signal containing speech information flows to the novel deep neural Wave-voice Net to reconstruct the speech via exhaustive extraction. Extensive experiments under real-world scenarios with 21 participants show the effectiveness of WaveEar to precisely infer the speech in noisy environments.\n\nFigure 1 :\n1WaveEar leverages the reflected mmWave signal from vocal vibrations to facilitate human-device interaction in noisy environments.\n\nFigure 3 :\n3The envelope of time-domain reflected mmWave signal (a), (b) and corresponding speech signal (c), (d) from a subject saying \"A,B,C,D\" twice. The reflected signal keeps a persistent and unique correlation with human speech.\n\nFigure 4 :\n4The system architecture for WaveEar mainly consisting of a mmWave sesing module in the front-end to locate and sense the vocal vibrations and Wave-voice Net in the back-end to reconstruct the high-quality voice even in the presence of ambient noise.\n\nFigure 5 :\n5The hardware design for the mmWave probe which comprises MCU, Baseband and RF board to infer the vocal vibrations from mmWave signals.\n\nFigure 5\n5shows the schematic design of the mmWave probe. It contains three parts, i.e., the RF board, the baseband board, and the Microprocessor Control Unit (MCU) board.\n\nFigure 7 :\n7The implementation of 24GHz mmWave probe follows the three-board architecture: (a) a down-frequency baseband board (b) an MCU board and (c) a radio-frequency (RF) Tx/Rx board.\n\nFigure 7\n7shows the implementation of the designed mmWave probe. The implementation consists of the RF board, the baseband board, and the MCU board. The implementation of the RF board adopts glass microfiber reinforced polytetrafluroethylene composites (Rogers RT/duroid 5880) as the substrate, which provides the thickness lower to 0.245 mm (0.0096 in). The size of the microstrip patch antenna is 5 mm \u00d7 3.7 mm, and the gap between two neighbor antennas is 5 mm in the x-axis and 3.5 mm in the y-axis. Both Tx and Rx consist of 16 antennas following the 4 \u00d7 4 layout. The distance between Tx and Rx of the probe is 40 mm.\n\nFigure 8 :\n8Experimental setup for speech sensing. A subject is sitting 0.5 meter away from the adjacent mmWave probe and microphone (MIC). We attach another microphone on subjects' collar to collect near-throat reference signals.\n\nFigure 9 :Figure 10 :\n910Spectrograms of (a) microphone-detect signal and (b) reconstructed voice signal. The vital voice information (refer to the annotated formants) are reconstructed by the Wave-Net. (Training loss = 0.00604357, Test loss = 0.00599584) The SSNR of WaveEar with different reading materials.\n\nFigure 11 :\n11The sensitivity of WaveEar with different reading materials.\n\nFigure 12 :\n12The MCD of WaveEar with different reading materials.\n\nFigure 13 :\n13The WER of WaveEar with different reading materials.\n\nFigure 14 :\n14Spectrograms of microphone-detected signal (left) and WaveEar-sensed signal (right) with three different noises: music, presentation, and water flow. Noises cannot break into WaveEar.\n\nFigure 15 :\n15MCD varies under different probe orientation and sensing distance.\n\nFigure 16 :\n16The average MCD comparison among different body motions.\n\nFigure 17 :\n17The average MCD comparison of men and women among different emotional states.\n\nTable 1 :\n1Time-Domain Features for Throat Localization.\n\nTable 2 :\n2The reading materials used in the experiments.Reading materials (Abbr.) \nLength W/S C/W Speed \n\nThe Rainbow Passage (Rainbow) \n330 \n17.3 \n4.3 \n1.90 \nThe Grandfather Passage (Grandfather) \n132 \n16.5 \n4.2 \n1.97 \nComma Gets a Cure (Comma) \n375 \n17.7 \n4.2 \n1.98 \nThe North Wind and the Sun (North) \n113 \n22.6 \n4.2 \n2.05 \nArthur the Rat (Arthur) \n590 \n14.6 \n3.8 \n2.80 \n\n\nACKNOWLEDGMENTSWe thank our shepherd, Dr. Romit Roy Choudhury, and all anonymous reviewers for their insightful comments on this paper. This work was supported by the National Science Foundation under grant No. 1718375 and the National Science Foundation of China under grant No. 61872195.\nvtrack: virtual trackpad interface using mm-level sound source localization for mobile interaction. S Chung, I Rhee, Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct. the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: AdjunctACMS. Chung and I. Rhee, \"vtrack: virtual trackpad interface using mm-level sound source localization for mobile interaction, \" in Proceedings of the 2016 ACM Inter- national Joint Conference on Pervasive and Ubiquitous Computing: Adjunct. ACM, 2016, pp. 41-44.\n\nWhitepaper: Top 10 consumer iot trends in 2017. \"Whitepaper: Top 10 consumer iot trends in 2017.\" [Online]. Available: https://www.parksassociates.com/whitepapers/top10-2017\n\nCross-platform support for rapid development of mobile acoustic sensing applications. Y.-C Tung, D Bui, K G Shin, Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services. the 16th Annual International Conference on Mobile Systems, Applications, and ServicesACMY.-C. Tung, D. Bui, and K. G. Shin, \"Cross-platform support for rapid develop- ment of mobile acoustic sensing applications, \" in Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services. ACM, 2018, pp. 455-467.\n\nExpansion of human-phone interface by sensing structure-borne sound propagation. Y.-C Tung, K G Shin, Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services. the 14th Annual International Conference on Mobile Systems, Applications, and ServicesACMY.-C. Tung and K. G. Shin, \"Expansion of human-phone interface by sensing structure-borne sound propagation,\" in Proceedings of the 14th Annual Interna- tional Conference on Mobile Systems, Applications, and Services. ACM, 2016, pp. 277-289.\n\nSmart Home Devices Market Forecast to Be Growing Globally at 31% Annual Clip. AccessedSmart Home Devices Market Forecast to Be Growing Globally at 31% Annual Clip. https://www.securitysales.com/research/ smart-home-devices-market-forecast/, Accessed: 2018-10-12.\n\nIt is worth to mention that Wei et al. [47] also leveraged the vibration as a side-channel to eavesdrop a target loudspeaker by Wi-Fi signal. Session 1: On the Horizon MobiSys '19. Seoul, KoreaIt is worth to mention that Wei et al. [47] also leveraged the vibration as a side-channel to eavesdrop a target loudspeaker by Wi-Fi signal. Session 1: On the Horizon MobiSys '19, June 17-21, 2019, Seoul, Korea\n\nAcoustruments: Passive, acoustically-driven, interactive controls for handheld devices. G Laput, E Brockmeyer, S E Hudson, C Harrison, Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. the 33rd Annual ACM Conference on Human Factors in Computing SystemsACMG. Laput, E. Brockmeyer, S. E. Hudson, and C. Harrison, \"Acoustruments: Passive, acoustically-driven, interactive controls for handheld devices, \" in Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, 2015, pp. 2161-2170.\n\nWivo: Enhancing the security of voice control system via wireless signal in iot environment. Y Meng, Z Wang, W Zhang, P Wu, H Zhu, X Liang, Y Liu, Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing. the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and ComputingACMY. Meng, Z. Wang, W. Zhang, P. Wu, H. Zhu, X. Liang, and Y. Liu, \"Wivo: Enhanc- ing the security of voice control system via wireless signal in iot environment, \" in Proceedings of the Eighteenth ACM International Symposium on Mobile Ad Hoc Networking and Computing. ACM, 2018, pp. 81-90.\n\nThe insecurity of home digital voice assistants-vulnerabilities, attacks and countermeasures. X Lei, G.-H Tu, A X Liu, C.-Y. Li, T Xie, 2018 IEEE Conference on Communications and Network Security (CNS). IEEEX. Lei, G.-H. Tu, A. X. Liu, C.-Y. Li, and T. Xie, \"The insecurity of home digital voice assistants-vulnerabilities, attacks and countermeasures, \" in 2018 IEEE Conference on Communications and Network Security (CNS). IEEE, 2018, pp. 1-9.\n\nActive noise control: Open problems and challenges. S M Kuo, K Kuo, W S Gan, Green Circuits and Systems (ICGCS), 2010 International Conference on. IEEES. M. Kuo, K. Kuo, and W. S. Gan, \"Active noise control: Open problems and challenges, \" in Green Circuits and Systems (ICGCS), 2010 International Conference on. IEEE, 2010, pp. 164-169.\n\nMute: Bringing iot to noise cancellation. S Shen, N Roy, J Guan, H Hassanieh, R R Choudhury, http:/doi.acm.org/10.1145/3230543.3230550Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, ser. SIGCOMM '18. the 2018 Conference of the ACM Special Interest Group on Data Communication, ser. SIGCOMM '18New York, NY, USAACMS. Shen, N. Roy, J. Guan, H. Hassanieh, and R. R. Choudhury, \"Mute: Bringing iot to noise cancellation,\" in Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication, ser. SIGCOMM '18. New York, NY, USA: ACM, 2018, pp. 282-296. [Online]. Available: http://doi.acm.org/10.1145/3230543.3230550\n\nSpeechless: Analyzing the threat to speech privacy from smartphone motion sensors. S A Anand, N Saxena, 2018 IEEE Symposium on Security and Privacy (SP). 00S. A. Anand and N. Saxena, \"Speechless: Analyzing the threat to speech privacy from smartphone motion sensors,\" in 2018 IEEE Symposium on Security and Privacy (SP). Vol. 00, 2018, pp. 116-133.\n\nSpeech articulator measurements using low power em-wave sensors. J Holzrichter, G Burnett, L Ng, W Lea, The Journal of the Acoustical Society of America. 1031J. Holzrichter, G. Burnett, L. Ng, and W. Lea, \"Speech articulator measurements using low power em-wave sensors,\" The Journal of the Acoustical Society of America, vol. 103, no. 1, pp. 622-625, 1998.\n\nAir-and bone-conductive integrated microphones for robust speech detection and enhancement. Y Zheng, Z Liu, Z Zhang, M Sinclair, J Droppo, L Deng, A Acero, X Huang, Automatic Speech Recognition and Understanding, 2003. ASRU'03. Y. Zheng, Z. Liu, Z. Zhang, M. Sinclair, J. Droppo, L. Deng, A. Acero, and X. Huang, \"Air-and bone-conductive integrated microphones for robust speech detection and enhancement,\" in Automatic Speech Recognition and Understanding, 2003. ASRU'03. 2003 IEEE Workshop on. IEEE, 2003, pp. 249-254.\n\nLip reading sentences in the wild. J S Chung, A W Senior, O Vinyals, A Zisserman, CVPR. J. S. Chung, A. W. Senior, O. Vinyals, and A. Zisserman, \"Lip reading sentences in the wild. \" in CVPR, 2017, pp. 3444-3453.\n\nMillimeter-wave beamforming: Antenna array design choices & characterization white paper. P White, G L , P. White and G. L. Reil, \"Millimeter-wave beamforming: Antenna array design choices & characterization white paper. \"\n\nFast millimeter-wave beam training with receive beamforming. J Kim, A F Molisch, Journal of Communications and Networks. 165J. Kim and A. F. Molisch, \"Fast millimeter-wave beam training with receive beamforming, \" Journal of Communications and Networks, vol. 16, no. 5, pp. 512- 522, Oct 2014.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, nature. 5217553436Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning, \" nature, vol. 521, no. 7553, p. 436, 2015.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition, \" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.\n\nPhase-vocoder: About this phasiness business. J Laroche, M Dolson, Applications of Signal Processing to Audio and Acoustics. 4J. Laroche and M. Dolson, \"Phase-vocoder: About this phasiness business,\" in Applications of Signal Processing to Audio and Acoustics, 1997. 1997 IEEE ASSP Workshop on. IEEE, 1997, pp. 4-pp.\n\nPhase-locked vocoder. M Puckette, Applications of Signal Processing to Audio and Acoustics. IEEEM. Puckette, \"Phase-locked vocoder, \" in Applications of Signal Processing to Audio and Acoustics, 1995., IEEE ASSP Workshop on. IEEE, 1995, pp. 222-225.\n\nSignal estimation from modified short-time fourier transform. D Griffin, J Lim, IEEE Transactions on Acoustics, Speech, and Signal Processing. 322D. Griffin and J. Lim, \"Signal estimation from modified short-time fourier trans- form, \" IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 2, pp. 236-243, 1984.\n\nCloud speech-to-text documentation. google cloud\"Cloud speech-to-text documentation, \" 2019, google cloud.\n\nEmotional responses to music: The need to consider underlying mechanisms. P N Juslin, D V\u00e4stfj\u00e4ll, Behavioral and brain sciences. 315P. N. Juslin and D. V\u00e4stfj\u00e4ll, \"Emotional responses to music: The need to consider underlying mechanisms, \" Behavioral and brain sciences, vol. 31, no. 5, pp. 559-575, 2008.\n\nGender and emotion in the united states: Do men and women differ in self-reports of feelings and expressive behavior?. R W Simon, L E Nath, American journal of sociology. 1095R. W. Simon and L. E. Nath, \"Gender and emotion in the united states: Do men and women differ in self-reports of feelings and expressive behavior?\" American journal of sociology, vol. 109, no. 5, pp. 1137-1176, 2004.\n\n5g cellular user equipment: From theory to practical hardware design. Y Huo, X Dong, W Xu, arXiv:1704.02540arXiv preprintY. Huo, X. Dong, and W. Xu, \"5g cellular user equipment: From theory to practical hardware design, \" arXiv preprint arXiv:1704.02540, 2017.\n\nsmartphone-artificial-intelligence-(ai)-chip-market-poised-for-growth. Smartphone Artificial Intelligence (AI) Chip Market Poised for Growth. AccessedSmartphone Artificial Intelligence (AI) Chip Market Poised for Growth. https://www.strategyanalytics.com/strategy-analytics/blogs/ components/handset-components/handset-components/2018/08/10/ smartphone-artificial-intelligence-(ai)-chip-market-poised-for-growth, Accessed: 2018-10-09.\n\nA novel method for speech acquisition and enhancement by 94 ghz millimeter-wave sensor. F Chen, S Li, C Li, M Liu, Z Li, H Xue, X Jing, J Wang, Sensors. 161F. Chen, S. Li, C. Li, M. Liu, Z. Li, H. Xue, X. Jing, and J. Wang, \"A novel method for speech acquisition and enhancement by 94 ghz millimeter-wave sensor, \" Sensors, vol. 16, no. 1, 2016. [Online]. Available: http://www.mdpi.com/1424-8220/16/1/50\n\nSubinteger range-bin alignment method for isar imaging of noncooperative targets. J M Mu\u00f1oz-Ferreras, F P\u00e9rez-Mart\u00ednez, EURASIP Journal on Advances in Signal Processing. 20101438615J. M. Mu\u00f1oz-Ferreras and F. P\u00e9rez-Mart\u00ednez, \"Subinteger range-bin alignment method for isar imaging of noncooperative targets, \" EURASIP Journal on Advances in Signal Processing, vol. 2010, no. 1, p. 438615, 2010.\n\nVocal resonance: Using internal body voice for wearable authentication. R Liu, C Cornelius, R Rawassizadeh, R Peterson, D Kotz, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies. the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies219R. Liu, C. Cornelius, R. Rawassizadeh, R. Peterson, and D. Kotz, \"Vocal resonance: Using internal body voice for wearable authentication, \" Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 2, no. 1, p. 19, 2018.\n\nTowards environment independent device free human activity recognition. W Jiang, C Miao, F Ma, S Yao, Y Wang, Y Yuan, H Xue, C Song, X Ma, D Koutsonikolas, W Xu, L Su, http:/doi.acm.org/10.1145/3241539.3241548Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, ser. MobiCom '18. the 24th Annual International Conference on Mobile Computing and Networking, ser. MobiCom '18New York, NY, USAACMW. Jiang, C. Miao, F. Ma, S. Yao, Y. Wang, Y. Yuan, H. Xue, C. Song, X. Ma, D. Koutsonikolas, W. Xu, and L. Su, \"Towards environment independent device free human activity recognition,\" in Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, ser. MobiCom '18. New York, NY, USA: ACM, 2018, pp. 289-304. [Online]. Available: http://doi.acm.org/10.1145/3241539.3241548\n\nNonlinear feature based classification of speech under stress. G Zhou, J H Hansen, J F Kaiser, IEEE Transactions on speech and audio processing. 93G. Zhou, J. H. Hansen, and J. F. Kaiser, \"Nonlinear feature based classification of speech under stress, \" IEEE Transactions on speech and audio processing, vol. 9, no. 3, pp. 201-216, 2001.\n\nComparison between electroglottography and electromagnetic glottography. I R Titze, B H Story, G C Burnett, J F Holzrichter, L C Ng, W A Lea, The Journal of the Acoustical Society of America. 1071I. R. Titze, B. H. Story, G. C. Burnett, J. F. Holzrichter, L. C. Ng, and W. A. Lea, \"Comparison between electroglottography and electromagnetic glottography,\" The Journal of the Acoustical Society of America, vol. 107, no. 1, pp. 581-588, 2000.\n\nFactors contributing to bone conduction: The outer ear. S Stenfelt, T Wild, N Hato, R L Goode, The Journal of the Acoustical Society of America. 1132S. Stenfelt, T. Wild, N. Hato, and R. L. Goode, \"Factors contributing to bone conduction: The outer ear, \" The Journal of the Acoustical Society of America, vol. 113, no. 2, pp. 902-913, 2003.\n\nSpeech intelligibility in noise using throat and acoustic microphones. B E Acker-Mills, A J Houtsma, W A Ahroon, Aviation, space, and environmental medicine. 771B. E. Acker-Mills, A. J. Houtsma, and W. A. Ahroon, \"Speech intelligibility in noise using throat and acoustic microphones, \" Aviation, space, and environmental medicine, vol. 77, no. 1, pp. 26-31, 2006.\n\nEstimation of acoustic microphone vocal tract parameters from throat microphone recordings, \" in In-Vehicle Corpus and Signal Processing for Driver Behavior. \u00dc \u00c7 Akarg\u00fcn, E Erzin, Springer\u00dc. \u00c7. Akarg\u00fcn and E. Erzin, \"Estimation of acoustic microphone vocal tract parameters from throat microphone recordings, \" in In-Vehicle Corpus and Signal Processing for Driver Behavior. Springer, 2009, pp. 161-169.\n\nThe physiological microphone (pmic): A competitive alternative for speaker assessment in stress detection and speaker verification. S A Patil, J H Hansen, Speech Communication. 524S. A. Patil and J. H. Hansen, \"The physiological microphone (pmic): A competitive alternative for speaker assessment in stress detection and speaker verification, \" Speech Communication, vol. 52, no. 4, pp. 327-340, 2010.\n\nHearing your voice is not enough: An articulatory gesture based liveness detection for voice authentication. L Zhang, S Tan, J Yang, Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. the 2017 ACM SIGSAC Conference on Computer and Communications SecurityACML. Zhang, S. Tan, and J. Yang, \"Hearing your voice is not enough: An articulatory gesture based liveness detection for voice authentication,\" in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. ACM, 2017, pp. 57-71.\n\nOperational control of the multimedia player of smart phones using a kinect voice-sensing scheme. I.-J Ding, C.-M Ruan, J.-Y Shi, Proc. Int. Conf. Inf. Eng. Int. Conf. Inf. EngI.-J. Ding, C.-M. Ruan, and J.-Y. Shi, \"Operational control of the multimedia player of smart phones using a kinect voice-sensing scheme, \" in Proc. Int. Conf. Inf. Eng. Mech. Mater. Citeseer, 2015, pp. 194-198.\n\nListening through a vibration motor. N Roy, R Roy Choudhury, Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services. the 14th Annual International Conference on Mobile Systems, Applications, and ServicesACMN. Roy and R. Roy Choudhury, \"Listening through a vibration motor, \" in Proceed- ings of the 14th Annual International Conference on Mobile Systems, Applications, and Services. ACM, 2016, pp. 57-69.\n\nA review on recent progress of portable short-range noncontact microwave radar systems. C Li, Z Peng, T Huang, T Fan, F Wang, T Horng, J M Noz Ferreras, R G\u00f3mez-Garc\u00eda, L Ran, J Lin, IEEE Transactions on Microwave Theory and Techniques. 655C. Li, Z. Peng, T. Huang, T. Fan, F. Wang, T. Horng, J. M. noz Ferreras, R. G\u00f3mez- Garc\u00eda, L. Ran, and J. Lin, \"A review on recent progress of portable short-range noncontact microwave radar systems,\" IEEE Transactions on Microwave Theory and Techniques, vol. 65, no. 5, pp. 1692-1706, May 2017.\n\nReview on advanced short-range multimode continuous-wave radar architectures for healthcare applications. J M Oz Ferreras, Z Peng, R G\u00f3mez-Garc\u00eda, C Li, IEEE Journal of Electromagnetics, RF and Microwaves in Medicine and Biology. 11J. M. oz Ferreras, Z. Peng, R. G\u00f3mez-Garc\u00eda, and C. Li, \"Review on advanced short-range multimode continuous-wave radar architectures for healthcare ap- plications, \" IEEE Journal of Electromagnetics, RF and Microwaves in Medicine and Biology, vol. 1, no. 1, pp. 14-25, June 2017.\n\nCardiac scan: A non-contact and continuous heart-based user authentication system. F Lin, C Song, Y Zhuang, W Xu, C Li, K Ren, Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking. the 23rd Annual International Conference on Mobile Computing and NetworkingACMF. Lin, C. Song, Y. Zhuang, W. Xu, C. Li, and K. Ren, \"Cardiac scan: A non-contact and continuous heart-based user authentication system, \" in Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking. ACM, 2017, pp. 315-328.\n\nSoli: Ubiquitous gesture sensing with millimeter wave radar. J Lien, N Gillian, M E Karagozler, P Amihood, C Schwesig, E Olson, H Raja, I Poupyrev, ACM Transactions on Graphics (TOG). 354142J. Lien, N. Gillian, M. E. Karagozler, P. Amihood, C. Schwesig, E. Olson, H. Raja, and I. Poupyrev, \"Soli: Ubiquitous gesture sensing with millimeter wave radar, \" ACM Transactions on Graphics (TOG), vol. 35, no. 4, p. 142, 2016.\n\nSleepsense: A noncontact and cost-effective sleep monitoring system. F Lin, Y Zhuang, C Song, A Wang, Y Li, C Gu, C Li, W Xu, IEEE transactions on biomedical circuits and systems. 111F. Lin, Y. Zhuang, C. Song, A. Wang, Y. Li, C. Gu, C. Li, and W. Xu, \"Sleepsense: A noncontact and cost-effective sleep monitoring system, \" IEEE transactions on biomedical circuits and systems, vol. 11, no. 1, pp. 189-202, 2017.\n\nLow-cost gas sensors utilizing mm-wave radars. S Liu, G Shaker, S Safavi-Naeini, J M Chong, Antennas and Propagation & USNC/URSI National Radio Science Meeting. S. Liu, G. Shaker, S. Safavi-Naeini, and J. M. Chong, \"Low-cost gas sensors utilizing mm-wave radars,\" in Antennas and Propagation & USNC/URSI National Radio Science Meeting, 2017 IEEE International Symposium on. IEEE, 2017, pp. 1853- 1854.\n\nMicrowave human vocal vibration signal detection based on doppler radar technology. C Lin, S Chang, C Chang, C Lin, IEEE Transactions on Microwave Theory and Techniques. 588C. Lin, S. Chang, C. Chang, and C. Lin, \"Microwave human vocal vibration signal detection based on doppler radar technology,\" IEEE Transactions on Microwave Theory and Techniques, vol. 58, no. 8, pp. 2299-2306, Aug 2010.\n\nAcoustic eavesdropping through wireless vibrometry. T Wei, S Wang, A Zhou, X Zhang, http:/doi.acm.org/10.1145/2789168.2790119Proceedings of the 21st Annual International Conference on Mobile Computing and Networking, ser. MobiCom '15. the 21st Annual International Conference on Mobile Computing and Networking, ser. MobiCom '15New York, NY, USAACMT. Wei, S. Wang, A. Zhou, and X. Zhang, \"Acoustic eavesdropping through wireless vibrometry, \" in Proceedings of the 21st Annual International Conference on Mobile Computing and Networking, ser. MobiCom '15. New York, NY, USA: ACM, 2015, pp. 130-141. [Online]. Available: http://doi.acm.org/10.1145/2789168. 2790119\n", "annotations": {"author": "[{\"end\":256,\"start\":149},{\"end\":346,\"start\":257},{\"end\":456,\"start\":347},{\"end\":553,\"start\":457},{\"end\":618,\"start\":554},{\"end\":723,\"start\":619},{\"end\":874,\"start\":724},{\"end\":981,\"start\":875},{\"end\":993,\"start\":982},{\"end\":1008,\"start\":994},{\"end\":1022,\"start\":1009},{\"end\":1065,\"start\":1023},{\"end\":1077,\"start\":1066},{\"end\":1088,\"start\":1078},{\"end\":1098,\"start\":1089},{\"end\":1109,\"start\":1099}]", "publisher": "[{\"end\":94,\"start\":91},{\"end\":1360,\"start\":1357}]", "author_last_name": "[{\"end\":159,\"start\":157},{\"end\":270,\"start\":268},{\"end\":359,\"start\":354},{\"end\":477,\"start\":464},{\"end\":564,\"start\":562},{\"end\":628,\"start\":624},{\"end\":732,\"start\":728},{\"end\":884,\"start\":882},{\"end\":992,\"start\":990},{\"end\":1007,\"start\":1005},{\"end\":1021,\"start\":1016},{\"end\":1043,\"start\":1036},{\"end\":1076,\"start\":1074},{\"end\":1087,\"start\":1083},{\"end\":1097,\"start\":1093}]", "author_first_name": "[{\"end\":156,\"start\":149},{\"end\":267,\"start\":257},{\"end\":353,\"start\":347},{\"end\":463,\"start\":457},{\"end\":561,\"start\":554},{\"end\":623,\"start\":619},{\"end\":727,\"start\":724},{\"end\":881,\"start\":875},{\"end\":989,\"start\":982},{\"end\":1004,\"start\":994},{\"end\":1015,\"start\":1009},{\"end\":1029,\"start\":1023},{\"end\":1035,\"start\":1030},{\"end\":1073,\"start\":1066},{\"end\":1082,\"start\":1078},{\"end\":1092,\"start\":1089},{\"end\":1105,\"start\":1099},{\"end\":1108,\"start\":1106}]", "author_affiliation": "[{\"end\":255,\"start\":182},{\"end\":345,\"start\":272},{\"end\":455,\"start\":382},{\"end\":552,\"start\":479},{\"end\":617,\"start\":566},{\"end\":722,\"start\":649},{\"end\":822,\"start\":749},{\"end\":873,\"start\":824},{\"end\":980,\"start\":907}]", "title": "[{\"end\":90,\"start\":1},{\"end\":1199,\"start\":1110}]", "venue": "[{\"end\":1293,\"start\":1201}]", "abstract": "[{\"end\":3362,\"start\":1703}]", "bib_ref": "[{\"end\":3802,\"start\":3799},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3920,\"start\":3917},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3923,\"start\":3920},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3926,\"start\":3923},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4009,\"start\":4006},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4333,\"start\":4330},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4335,\"start\":4333},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4804,\"start\":4801},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4806,\"start\":4804},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5147,\"start\":5144},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5316,\"start\":5312},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5433,\"start\":5429},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5436,\"start\":5433},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5473,\"start\":5469},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5935,\"start\":5931},{\"end\":8417,\"start\":8409},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18459,\"start\":18455},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18608,\"start\":18604},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22037,\"start\":22033},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22645,\"start\":22641},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24751,\"start\":24747},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24986,\"start\":24982},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25107,\"start\":25103},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25564,\"start\":25560},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32009,\"start\":32005},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":42653,\"start\":42649},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43349,\"start\":43345},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":44679,\"start\":44675},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":46014,\"start\":46010},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":48861,\"start\":48857},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":49641,\"start\":49637},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":49768,\"start\":49764},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":50606,\"start\":50602},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":51052,\"start\":51048},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":51336,\"start\":51332},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":51373,\"start\":51369},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":51522,\"start\":51518},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":51572,\"start\":51568},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":51609,\"start\":51605},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":51640,\"start\":51636},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":51743,\"start\":51739},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":51843,\"start\":51839},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":51900,\"start\":51896},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":52285,\"start\":52281},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":52288,\"start\":52285},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":52305,\"start\":52301},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":52462,\"start\":52458},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":52550,\"start\":52546},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":52668,\"start\":52664},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":52921,\"start\":52917},{\"end\":53312,\"start\":53311}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54237,\"start\":54095},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54473,\"start\":54238},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54736,\"start\":54474},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54884,\"start\":54737},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55057,\"start\":54885},{\"attributes\":{\"id\":\"fig_5\"},\"end\":55246,\"start\":55058},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55871,\"start\":55247},{\"attributes\":{\"id\":\"fig_7\"},\"end\":56103,\"start\":55872},{\"attributes\":{\"id\":\"fig_8\"},\"end\":56414,\"start\":56104},{\"attributes\":{\"id\":\"fig_9\"},\"end\":56490,\"start\":56415},{\"attributes\":{\"id\":\"fig_10\"},\"end\":56558,\"start\":56491},{\"attributes\":{\"id\":\"fig_11\"},\"end\":56626,\"start\":56559},{\"attributes\":{\"id\":\"fig_13\"},\"end\":56825,\"start\":56627},{\"attributes\":{\"id\":\"fig_14\"},\"end\":56907,\"start\":56826},{\"attributes\":{\"id\":\"fig_15\"},\"end\":56979,\"start\":56908},{\"attributes\":{\"id\":\"fig_16\"},\"end\":57072,\"start\":56980},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":57130,\"start\":57073},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":57508,\"start\":57131}]", "paragraph": "[{\"end\":4010,\"start\":3378},{\"end\":4336,\"start\":4012},{\"end\":4807,\"start\":4338},{\"end\":6115,\"start\":4809},{\"end\":8185,\"start\":6117},{\"end\":9658,\"start\":8187},{\"end\":9714,\"start\":9660},{\"end\":10559,\"start\":9716},{\"end\":10853,\"start\":10616},{\"end\":11589,\"start\":10886},{\"end\":11755,\"start\":11609},{\"end\":12458,\"start\":11793},{\"end\":12736,\"start\":12460},{\"end\":12852,\"start\":12753},{\"end\":13239,\"start\":12876},{\"end\":14099,\"start\":13263},{\"end\":14247,\"start\":14119},{\"end\":14698,\"start\":14249},{\"end\":15172,\"start\":14700},{\"end\":15952,\"start\":15198},{\"end\":16564,\"start\":15976},{\"end\":16764,\"start\":16566},{\"end\":16795,\"start\":16785},{\"end\":17588,\"start\":17003},{\"end\":18779,\"start\":17590},{\"end\":19357,\"start\":18809},{\"end\":20232,\"start\":19408},{\"end\":20425,\"start\":20283},{\"end\":20623,\"start\":20457},{\"end\":20837,\"start\":20625},{\"end\":21308,\"start\":20856},{\"end\":21993,\"start\":21310},{\"end\":22469,\"start\":21995},{\"end\":23040,\"start\":22496},{\"end\":23334,\"start\":23042},{\"end\":23500,\"start\":23390},{\"end\":23793,\"start\":23523},{\"end\":24258,\"start\":23839},{\"end\":24536,\"start\":24260},{\"end\":24802,\"start\":24538},{\"end\":24987,\"start\":24835},{\"end\":25144,\"start\":24989},{\"end\":25372,\"start\":25233},{\"end\":25650,\"start\":25429},{\"end\":26331,\"start\":25652},{\"end\":26614,\"start\":26333},{\"end\":27000,\"start\":26616},{\"end\":27756,\"start\":27034},{\"end\":29150,\"start\":27768},{\"end\":30098,\"start\":29152},{\"end\":31072,\"start\":30121},{\"end\":31191,\"start\":31074},{\"end\":31488,\"start\":31246},{\"end\":31691,\"start\":31490},{\"end\":32178,\"start\":31725},{\"end\":32321,\"start\":32180},{\"end\":32514,\"start\":32323},{\"end\":32635,\"start\":32564},{\"end\":32719,\"start\":32701},{\"end\":32938,\"start\":32749},{\"end\":33216,\"start\":32963},{\"end\":33336,\"start\":33240},{\"end\":37246,\"start\":33338},{\"end\":39044,\"start\":37283},{\"end\":39161,\"start\":39046},{\"end\":42385,\"start\":39185},{\"end\":43821,\"start\":42387},{\"end\":44012,\"start\":43884},{\"end\":45311,\"start\":44014},{\"end\":46197,\"start\":45313},{\"end\":46329,\"start\":46225},{\"end\":46936,\"start\":46331},{\"end\":47503,\"start\":46938},{\"end\":48348,\"start\":47545},{\"end\":49161,\"start\":48363},{\"end\":50128,\"start\":49163},{\"end\":50943,\"start\":50130},{\"end\":52137,\"start\":50960},{\"end\":53369,\"start\":52139},{\"end\":54094,\"start\":53384}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12752,\"start\":12737},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12875,\"start\":12853},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16980,\"start\":16796},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20282,\"start\":20233},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20456,\"start\":20426},{\"attributes\":{\"id\":\"formula_5\"},\"end\":22495,\"start\":22470},{\"attributes\":{\"id\":\"formula_6\"},\"end\":23389,\"start\":23335},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23522,\"start\":23501},{\"attributes\":{\"id\":\"formula_8\"},\"end\":24834,\"start\":24803},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25232,\"start\":25145},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25428,\"start\":25373},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31245,\"start\":31192},{\"attributes\":{\"id\":\"formula_12\"},\"end\":31724,\"start\":31692},{\"attributes\":{\"id\":\"formula_13\"},\"end\":32563,\"start\":32515},{\"attributes\":{\"id\":\"formula_14\"},\"end\":32700,\"start\":32636},{\"attributes\":{\"id\":\"formula_15\"},\"end\":32748,\"start\":32720}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18293,\"start\":18286},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29338,\"start\":29331}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3376,\"start\":3364},{\"attributes\":{\"n\":\"2\"},\"end\":10614,\"start\":10562},{\"attributes\":{\"n\":\"2.1\"},\"end\":10884,\"start\":10856},{\"end\":11607,\"start\":11592},{\"attributes\":{\"n\":\"2.2\"},\"end\":11791,\"start\":11758},{\"attributes\":{\"n\":\"2.3\"},\"end\":13261,\"start\":13242},{\"attributes\":{\"n\":\"3\"},\"end\":14117,\"start\":14102},{\"attributes\":{\"n\":\"4\"},\"end\":15196,\"start\":15175},{\"attributes\":{\"n\":\"4.1\"},\"end\":15974,\"start\":15955},{\"end\":16783,\"start\":16767},{\"attributes\":{\"n\":\"4.2\"},\"end\":17001,\"start\":16982},{\"attributes\":{\"n\":\"5\"},\"end\":18807,\"start\":18782},{\"attributes\":{\"n\":\"5.1\"},\"end\":19406,\"start\":19360},{\"attributes\":{\"n\":\"5.2\"},\"end\":20854,\"start\":20840},{\"attributes\":{\"n\":\"5.3\"},\"end\":23837,\"start\":23796},{\"attributes\":{\"n\":\"6.2\"},\"end\":27032,\"start\":27003},{\"attributes\":{\"n\":\"6.3\"},\"end\":27766,\"start\":27759},{\"attributes\":{\"n\":\"6.4\"},\"end\":30119,\"start\":30101},{\"attributes\":{\"n\":\"7\"},\"end\":32961,\"start\":32941},{\"attributes\":{\"n\":\"7.1\"},\"end\":33238,\"start\":33219},{\"attributes\":{\"n\":\"7.2\"},\"end\":37281,\"start\":37249},{\"attributes\":{\"n\":\"7.3\"},\"end\":39183,\"start\":39164},{\"attributes\":{\"n\":\"8\"},\"end\":43882,\"start\":43824},{\"attributes\":{\"n\":\"8.2\"},\"end\":46223,\"start\":46200},{\"attributes\":{\"n\":\"8.3\"},\"end\":47543,\"start\":47506},{\"attributes\":{\"n\":\"9\"},\"end\":48361,\"start\":48351},{\"attributes\":{\"n\":\"10\"},\"end\":50958,\"start\":50946},{\"attributes\":{\"n\":\"11\"},\"end\":53382,\"start\":53372},{\"end\":54106,\"start\":54096},{\"end\":54249,\"start\":54239},{\"end\":54485,\"start\":54475},{\"end\":54748,\"start\":54738},{\"end\":54894,\"start\":54886},{\"end\":55069,\"start\":55059},{\"end\":55256,\"start\":55248},{\"end\":55883,\"start\":55873},{\"end\":56126,\"start\":56105},{\"end\":56427,\"start\":56416},{\"end\":56503,\"start\":56492},{\"end\":56571,\"start\":56560},{\"end\":56639,\"start\":56628},{\"end\":56838,\"start\":56827},{\"end\":56920,\"start\":56909},{\"end\":56992,\"start\":56981},{\"end\":57083,\"start\":57074},{\"end\":57141,\"start\":57132}]", "table": "[{\"end\":57508,\"start\":57189}]", "figure_caption": "[{\"end\":54237,\"start\":54108},{\"end\":54473,\"start\":54251},{\"end\":54736,\"start\":54487},{\"end\":54884,\"start\":54750},{\"end\":55057,\"start\":54896},{\"end\":55246,\"start\":55071},{\"end\":55871,\"start\":55258},{\"end\":56103,\"start\":55885},{\"end\":56414,\"start\":56130},{\"end\":56490,\"start\":56430},{\"end\":56558,\"start\":56506},{\"end\":56626,\"start\":56574},{\"end\":56825,\"start\":56642},{\"end\":56907,\"start\":56841},{\"end\":56979,\"start\":56923},{\"end\":57072,\"start\":56995},{\"end\":57130,\"start\":57085},{\"end\":57189,\"start\":57143}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8329,\"start\":8321},{\"end\":11631,\"start\":11623},{\"end\":12138,\"start\":12130},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13586,\"start\":13578},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14246,\"start\":14238},{\"end\":20469,\"start\":20461},{\"end\":20996,\"start\":20988},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23673,\"start\":23665},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27892,\"start\":27884},{\"end\":33554,\"start\":33546},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34904,\"start\":34895},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35801,\"start\":35792},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36352,\"start\":36343},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36661,\"start\":36652},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37955,\"start\":37946},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38325,\"start\":38316},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38754,\"start\":38745},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39841,\"start\":39832},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41604,\"start\":41595},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43498,\"start\":43489}]", "bib_author_first_name": "[{\"end\":57900,\"start\":57899},{\"end\":57909,\"start\":57908},{\"end\":58641,\"start\":58637},{\"end\":58649,\"start\":58648},{\"end\":58656,\"start\":58655},{\"end\":58658,\"start\":58657},{\"end\":59200,\"start\":59196},{\"end\":59208,\"start\":59207},{\"end\":59210,\"start\":59209},{\"end\":60411,\"start\":60410},{\"end\":60420,\"start\":60419},{\"end\":60434,\"start\":60433},{\"end\":60436,\"start\":60435},{\"end\":60446,\"start\":60445},{\"end\":60969,\"start\":60968},{\"end\":60977,\"start\":60976},{\"end\":60985,\"start\":60984},{\"end\":60994,\"start\":60993},{\"end\":61000,\"start\":60999},{\"end\":61007,\"start\":61006},{\"end\":61016,\"start\":61015},{\"end\":61595,\"start\":61594},{\"end\":61605,\"start\":61601},{\"end\":61611,\"start\":61610},{\"end\":61613,\"start\":61612},{\"end\":61624,\"start\":61619},{\"end\":61630,\"start\":61629},{\"end\":62000,\"start\":61999},{\"end\":62002,\"start\":62001},{\"end\":62009,\"start\":62008},{\"end\":62016,\"start\":62015},{\"end\":62018,\"start\":62017},{\"end\":62329,\"start\":62328},{\"end\":62337,\"start\":62336},{\"end\":62344,\"start\":62343},{\"end\":62352,\"start\":62351},{\"end\":62365,\"start\":62364},{\"end\":62367,\"start\":62366},{\"end\":63053,\"start\":63052},{\"end\":63055,\"start\":63054},{\"end\":63064,\"start\":63063},{\"end\":63385,\"start\":63384},{\"end\":63400,\"start\":63399},{\"end\":63411,\"start\":63410},{\"end\":63417,\"start\":63416},{\"end\":63771,\"start\":63770},{\"end\":63780,\"start\":63779},{\"end\":63787,\"start\":63786},{\"end\":63796,\"start\":63795},{\"end\":63808,\"start\":63807},{\"end\":63818,\"start\":63817},{\"end\":63826,\"start\":63825},{\"end\":63835,\"start\":63834},{\"end\":64236,\"start\":64235},{\"end\":64238,\"start\":64237},{\"end\":64247,\"start\":64246},{\"end\":64249,\"start\":64248},{\"end\":64259,\"start\":64258},{\"end\":64270,\"start\":64269},{\"end\":64505,\"start\":64504},{\"end\":64514,\"start\":64513},{\"end\":64516,\"start\":64515},{\"end\":64700,\"start\":64699},{\"end\":64707,\"start\":64706},{\"end\":64709,\"start\":64708},{\"end\":64949,\"start\":64948},{\"end\":64958,\"start\":64957},{\"end\":64968,\"start\":64967},{\"end\":65139,\"start\":65138},{\"end\":65145,\"start\":65144},{\"end\":65154,\"start\":65153},{\"end\":65161,\"start\":65160},{\"end\":65543,\"start\":65542},{\"end\":65554,\"start\":65553},{\"end\":65837,\"start\":65836},{\"end\":66128,\"start\":66127},{\"end\":66139,\"start\":66138},{\"end\":66583,\"start\":66582},{\"end\":66585,\"start\":66584},{\"end\":66595,\"start\":66594},{\"end\":66936,\"start\":66935},{\"end\":66938,\"start\":66937},{\"end\":66947,\"start\":66946},{\"end\":66949,\"start\":66948},{\"end\":67280,\"start\":67279},{\"end\":67287,\"start\":67286},{\"end\":67295,\"start\":67294},{\"end\":67996,\"start\":67995},{\"end\":68004,\"start\":68003},{\"end\":68010,\"start\":68009},{\"end\":68016,\"start\":68015},{\"end\":68023,\"start\":68022},{\"end\":68029,\"start\":68028},{\"end\":68036,\"start\":68035},{\"end\":68044,\"start\":68043},{\"end\":68396,\"start\":68395},{\"end\":68398,\"start\":68397},{\"end\":68416,\"start\":68415},{\"end\":68782,\"start\":68781},{\"end\":68789,\"start\":68788},{\"end\":68802,\"start\":68801},{\"end\":68818,\"start\":68817},{\"end\":68830,\"start\":68829},{\"end\":69320,\"start\":69319},{\"end\":69329,\"start\":69328},{\"end\":69337,\"start\":69336},{\"end\":69343,\"start\":69342},{\"end\":69350,\"start\":69349},{\"end\":69358,\"start\":69357},{\"end\":69366,\"start\":69365},{\"end\":69373,\"start\":69372},{\"end\":69381,\"start\":69380},{\"end\":69387,\"start\":69386},{\"end\":69404,\"start\":69403},{\"end\":69410,\"start\":69409},{\"end\":70150,\"start\":70149},{\"end\":70158,\"start\":70157},{\"end\":70160,\"start\":70159},{\"end\":70170,\"start\":70169},{\"end\":70172,\"start\":70171},{\"end\":70499,\"start\":70498},{\"end\":70501,\"start\":70500},{\"end\":70510,\"start\":70509},{\"end\":70512,\"start\":70511},{\"end\":70521,\"start\":70520},{\"end\":70523,\"start\":70522},{\"end\":70534,\"start\":70533},{\"end\":70536,\"start\":70535},{\"end\":70551,\"start\":70550},{\"end\":70553,\"start\":70552},{\"end\":70559,\"start\":70558},{\"end\":70561,\"start\":70560},{\"end\":70925,\"start\":70924},{\"end\":70937,\"start\":70936},{\"end\":70945,\"start\":70944},{\"end\":70953,\"start\":70952},{\"end\":70955,\"start\":70954},{\"end\":71283,\"start\":71282},{\"end\":71285,\"start\":71284},{\"end\":71300,\"start\":71299},{\"end\":71302,\"start\":71301},{\"end\":71313,\"start\":71312},{\"end\":71315,\"start\":71314},{\"end\":71736,\"start\":71735},{\"end\":71738,\"start\":71737},{\"end\":71749,\"start\":71748},{\"end\":72115,\"start\":72114},{\"end\":72117,\"start\":72116},{\"end\":72126,\"start\":72125},{\"end\":72128,\"start\":72127},{\"end\":72495,\"start\":72494},{\"end\":72504,\"start\":72503},{\"end\":72511,\"start\":72510},{\"end\":73035,\"start\":73031},{\"end\":73046,\"start\":73042},{\"end\":73057,\"start\":73053},{\"end\":73360,\"start\":73359},{\"end\":73367,\"start\":73366},{\"end\":73371,\"start\":73368},{\"end\":73864,\"start\":73863},{\"end\":73870,\"start\":73869},{\"end\":73878,\"start\":73877},{\"end\":73887,\"start\":73886},{\"end\":73894,\"start\":73893},{\"end\":73902,\"start\":73901},{\"end\":73911,\"start\":73910},{\"end\":73913,\"start\":73912},{\"end\":73929,\"start\":73928},{\"end\":73945,\"start\":73944},{\"end\":73952,\"start\":73951},{\"end\":74419,\"start\":74418},{\"end\":74421,\"start\":74420},{\"end\":74436,\"start\":74435},{\"end\":74444,\"start\":74443},{\"end\":74460,\"start\":74459},{\"end\":74910,\"start\":74909},{\"end\":74917,\"start\":74916},{\"end\":74925,\"start\":74924},{\"end\":74935,\"start\":74934},{\"end\":74941,\"start\":74940},{\"end\":74947,\"start\":74946},{\"end\":75445,\"start\":75444},{\"end\":75453,\"start\":75452},{\"end\":75464,\"start\":75463},{\"end\":75466,\"start\":75465},{\"end\":75480,\"start\":75479},{\"end\":75491,\"start\":75490},{\"end\":75503,\"start\":75502},{\"end\":75512,\"start\":75511},{\"end\":75520,\"start\":75519},{\"end\":75874,\"start\":75873},{\"end\":75881,\"start\":75880},{\"end\":75891,\"start\":75890},{\"end\":75899,\"start\":75898},{\"end\":75907,\"start\":75906},{\"end\":75913,\"start\":75912},{\"end\":75919,\"start\":75918},{\"end\":75925,\"start\":75924},{\"end\":76266,\"start\":76265},{\"end\":76273,\"start\":76272},{\"end\":76283,\"start\":76282},{\"end\":76300,\"start\":76299},{\"end\":76302,\"start\":76301},{\"end\":76706,\"start\":76705},{\"end\":76713,\"start\":76712},{\"end\":76722,\"start\":76721},{\"end\":76731,\"start\":76730},{\"end\":77069,\"start\":77068},{\"end\":77076,\"start\":77075},{\"end\":77084,\"start\":77083},{\"end\":77092,\"start\":77091}]", "bib_author_last_name": "[{\"end\":57906,\"start\":57901},{\"end\":57914,\"start\":57910},{\"end\":58646,\"start\":58642},{\"end\":58653,\"start\":58650},{\"end\":58663,\"start\":58659},{\"end\":59205,\"start\":59201},{\"end\":59215,\"start\":59211},{\"end\":60417,\"start\":60412},{\"end\":60431,\"start\":60421},{\"end\":60443,\"start\":60437},{\"end\":60455,\"start\":60447},{\"end\":60974,\"start\":60970},{\"end\":60982,\"start\":60978},{\"end\":60991,\"start\":60986},{\"end\":60997,\"start\":60995},{\"end\":61004,\"start\":61001},{\"end\":61013,\"start\":61008},{\"end\":61020,\"start\":61017},{\"end\":61599,\"start\":61596},{\"end\":61608,\"start\":61606},{\"end\":61617,\"start\":61614},{\"end\":61627,\"start\":61625},{\"end\":61634,\"start\":61631},{\"end\":62006,\"start\":62003},{\"end\":62013,\"start\":62010},{\"end\":62022,\"start\":62019},{\"end\":62334,\"start\":62330},{\"end\":62341,\"start\":62338},{\"end\":62349,\"start\":62345},{\"end\":62362,\"start\":62353},{\"end\":62377,\"start\":62368},{\"end\":63061,\"start\":63056},{\"end\":63071,\"start\":63065},{\"end\":63397,\"start\":63386},{\"end\":63408,\"start\":63401},{\"end\":63414,\"start\":63412},{\"end\":63421,\"start\":63418},{\"end\":63777,\"start\":63772},{\"end\":63784,\"start\":63781},{\"end\":63793,\"start\":63788},{\"end\":63805,\"start\":63797},{\"end\":63815,\"start\":63809},{\"end\":63823,\"start\":63819},{\"end\":63832,\"start\":63827},{\"end\":63841,\"start\":63836},{\"end\":64244,\"start\":64239},{\"end\":64256,\"start\":64250},{\"end\":64267,\"start\":64260},{\"end\":64280,\"start\":64271},{\"end\":64511,\"start\":64506},{\"end\":64704,\"start\":64701},{\"end\":64717,\"start\":64710},{\"end\":64955,\"start\":64950},{\"end\":64965,\"start\":64959},{\"end\":64975,\"start\":64969},{\"end\":65142,\"start\":65140},{\"end\":65151,\"start\":65146},{\"end\":65158,\"start\":65155},{\"end\":65165,\"start\":65162},{\"end\":65551,\"start\":65544},{\"end\":65561,\"start\":65555},{\"end\":65846,\"start\":65838},{\"end\":66136,\"start\":66129},{\"end\":66143,\"start\":66140},{\"end\":66592,\"start\":66586},{\"end\":66605,\"start\":66596},{\"end\":66944,\"start\":66939},{\"end\":66954,\"start\":66950},{\"end\":67284,\"start\":67281},{\"end\":67292,\"start\":67288},{\"end\":67298,\"start\":67296},{\"end\":68001,\"start\":67997},{\"end\":68007,\"start\":68005},{\"end\":68013,\"start\":68011},{\"end\":68020,\"start\":68017},{\"end\":68026,\"start\":68024},{\"end\":68033,\"start\":68030},{\"end\":68041,\"start\":68037},{\"end\":68049,\"start\":68045},{\"end\":68413,\"start\":68399},{\"end\":68431,\"start\":68417},{\"end\":68786,\"start\":68783},{\"end\":68799,\"start\":68790},{\"end\":68815,\"start\":68803},{\"end\":68827,\"start\":68819},{\"end\":68835,\"start\":68831},{\"end\":69326,\"start\":69321},{\"end\":69334,\"start\":69330},{\"end\":69340,\"start\":69338},{\"end\":69347,\"start\":69344},{\"end\":69355,\"start\":69351},{\"end\":69363,\"start\":69359},{\"end\":69370,\"start\":69367},{\"end\":69378,\"start\":69374},{\"end\":69384,\"start\":69382},{\"end\":69401,\"start\":69388},{\"end\":69407,\"start\":69405},{\"end\":69413,\"start\":69411},{\"end\":70155,\"start\":70151},{\"end\":70167,\"start\":70161},{\"end\":70179,\"start\":70173},{\"end\":70507,\"start\":70502},{\"end\":70518,\"start\":70513},{\"end\":70531,\"start\":70524},{\"end\":70548,\"start\":70537},{\"end\":70556,\"start\":70554},{\"end\":70565,\"start\":70562},{\"end\":70934,\"start\":70926},{\"end\":70942,\"start\":70938},{\"end\":70950,\"start\":70946},{\"end\":70961,\"start\":70956},{\"end\":71297,\"start\":71286},{\"end\":71310,\"start\":71303},{\"end\":71322,\"start\":71316},{\"end\":71746,\"start\":71739},{\"end\":71755,\"start\":71750},{\"end\":72123,\"start\":72118},{\"end\":72135,\"start\":72129},{\"end\":72501,\"start\":72496},{\"end\":72508,\"start\":72505},{\"end\":72516,\"start\":72512},{\"end\":73040,\"start\":73036},{\"end\":73051,\"start\":73047},{\"end\":73061,\"start\":73058},{\"end\":73364,\"start\":73361},{\"end\":73381,\"start\":73372},{\"end\":73867,\"start\":73865},{\"end\":73875,\"start\":73871},{\"end\":73884,\"start\":73879},{\"end\":73891,\"start\":73888},{\"end\":73899,\"start\":73895},{\"end\":73908,\"start\":73903},{\"end\":73926,\"start\":73914},{\"end\":73942,\"start\":73930},{\"end\":73949,\"start\":73946},{\"end\":73956,\"start\":73953},{\"end\":74433,\"start\":74422},{\"end\":74441,\"start\":74437},{\"end\":74457,\"start\":74445},{\"end\":74463,\"start\":74461},{\"end\":74914,\"start\":74911},{\"end\":74922,\"start\":74918},{\"end\":74932,\"start\":74926},{\"end\":74938,\"start\":74936},{\"end\":74944,\"start\":74942},{\"end\":74951,\"start\":74948},{\"end\":75450,\"start\":75446},{\"end\":75461,\"start\":75454},{\"end\":75477,\"start\":75467},{\"end\":75488,\"start\":75481},{\"end\":75500,\"start\":75492},{\"end\":75509,\"start\":75504},{\"end\":75517,\"start\":75513},{\"end\":75529,\"start\":75521},{\"end\":75878,\"start\":75875},{\"end\":75888,\"start\":75882},{\"end\":75896,\"start\":75892},{\"end\":75904,\"start\":75900},{\"end\":75910,\"start\":75908},{\"end\":75916,\"start\":75914},{\"end\":75922,\"start\":75920},{\"end\":75928,\"start\":75926},{\"end\":76270,\"start\":76267},{\"end\":76280,\"start\":76274},{\"end\":76297,\"start\":76284},{\"end\":76308,\"start\":76303},{\"end\":76710,\"start\":76707},{\"end\":76719,\"start\":76714},{\"end\":76728,\"start\":76723},{\"end\":76735,\"start\":76732},{\"end\":77073,\"start\":77070},{\"end\":77081,\"start\":77077},{\"end\":77089,\"start\":77085},{\"end\":77098,\"start\":77093}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17069426},\"end\":58374,\"start\":57799},{\"attributes\":{\"id\":\"b1\"},\"end\":58549,\"start\":58376},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":49664738},\"end\":59113,\"start\":58551},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18301965},\"end\":59650,\"start\":59115},{\"attributes\":{\"id\":\"b4\"},\"end\":59914,\"start\":59652},{\"attributes\":{\"id\":\"b5\"},\"end\":60320,\"start\":59916},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":15902504},\"end\":60873,\"start\":60322},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49354919},\"end\":61498,\"start\":60875},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52005243},\"end\":61945,\"start\":61500},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":14260679},\"end\":62284,\"start\":61947},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/3230543.3230550\",\"id\":\"b10\",\"matched_paper_id\":49485452},\"end\":62967,\"start\":62286},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49230159},\"end\":63317,\"start\":62969},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":23159259},\"end\":63676,\"start\":63319},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16143360},\"end\":64198,\"start\":63678},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1662180},\"end\":64412,\"start\":64200},{\"attributes\":{\"id\":\"b15\"},\"end\":64636,\"start\":64414},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17872364},\"end\":64931,\"start\":64638},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1779661},\"end\":65090,\"start\":64933},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206594692},\"end\":65494,\"start\":65092},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2571846},\"end\":65812,\"start\":65496},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":61329543},\"end\":66063,\"start\":65814},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53067},\"end\":66398,\"start\":66065},{\"attributes\":{\"id\":\"b22\"},\"end\":66506,\"start\":66400},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3019262},\"end\":66814,\"start\":66508},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":142510313},\"end\":67207,\"start\":66816},{\"attributes\":{\"doi\":\"arXiv:1704.02540\",\"id\":\"b25\"},\"end\":67469,\"start\":67209},{\"attributes\":{\"id\":\"b26\"},\"end\":67905,\"start\":67471},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15237098},\"end\":68311,\"start\":67907},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":21095870},\"end\":68707,\"start\":68313},{\"attributes\":{\"id\":\"b29\"},\"end\":69245,\"start\":68709},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/3241539.3241548\",\"id\":\"b30\",\"matched_paper_id\":51919046},\"end\":70084,\"start\":69247},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":17885419},\"end\":70423,\"start\":70086},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":20121766},\"end\":70866,\"start\":70425},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":26266550},\"end\":71209,\"start\":70868},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":18989950},\"end\":71575,\"start\":71211},{\"attributes\":{\"id\":\"b35\"},\"end\":71980,\"start\":71577},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":12889062},\"end\":72383,\"start\":71982},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1295944},\"end\":72931,\"start\":72385},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":55636845},\"end\":73320,\"start\":72933},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":6252639},\"end\":73773,\"start\":73322},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":32308527},\"end\":74310,\"start\":73775},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":32467355},\"end\":74824,\"start\":74312},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":29565200},\"end\":75381,\"start\":74826},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6601027},\"end\":75802,\"start\":75383},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3355455},\"end\":76216,\"start\":75804},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":35624090},\"end\":76619,\"start\":76218},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":17402394},\"end\":77014,\"start\":76621},{\"attributes\":{\"doi\":\"http:/doi.acm.org/10.1145/2789168.2790119\",\"id\":\"b47\",\"matched_paper_id\":1875264},\"end\":77679,\"start\":77016}]", "bib_title": "[{\"end\":57897,\"start\":57799},{\"end\":58635,\"start\":58551},{\"end\":59194,\"start\":59115},{\"end\":60056,\"start\":59916},{\"end\":60408,\"start\":60322},{\"end\":60966,\"start\":60875},{\"end\":61592,\"start\":61500},{\"end\":61997,\"start\":61947},{\"end\":62326,\"start\":62286},{\"end\":63050,\"start\":62969},{\"end\":63382,\"start\":63319},{\"end\":63768,\"start\":63678},{\"end\":64233,\"start\":64200},{\"end\":64697,\"start\":64638},{\"end\":64946,\"start\":64933},{\"end\":65136,\"start\":65092},{\"end\":65540,\"start\":65496},{\"end\":65834,\"start\":65814},{\"end\":66125,\"start\":66065},{\"end\":66580,\"start\":66508},{\"end\":66933,\"start\":66816},{\"end\":67540,\"start\":67471},{\"end\":67993,\"start\":67907},{\"end\":68393,\"start\":68313},{\"end\":68779,\"start\":68709},{\"end\":69317,\"start\":69247},{\"end\":70147,\"start\":70086},{\"end\":70496,\"start\":70425},{\"end\":70922,\"start\":70868},{\"end\":71280,\"start\":71211},{\"end\":72112,\"start\":71982},{\"end\":72492,\"start\":72385},{\"end\":73029,\"start\":72933},{\"end\":73357,\"start\":73322},{\"end\":73861,\"start\":73775},{\"end\":74416,\"start\":74312},{\"end\":74907,\"start\":74826},{\"end\":75442,\"start\":75383},{\"end\":75871,\"start\":75804},{\"end\":76263,\"start\":76218},{\"end\":76703,\"start\":76621},{\"end\":77066,\"start\":77016}]", "bib_author": "[{\"end\":57908,\"start\":57899},{\"end\":57916,\"start\":57908},{\"end\":58648,\"start\":58637},{\"end\":58655,\"start\":58648},{\"end\":58665,\"start\":58655},{\"end\":59207,\"start\":59196},{\"end\":59217,\"start\":59207},{\"end\":60419,\"start\":60410},{\"end\":60433,\"start\":60419},{\"end\":60445,\"start\":60433},{\"end\":60457,\"start\":60445},{\"end\":60976,\"start\":60968},{\"end\":60984,\"start\":60976},{\"end\":60993,\"start\":60984},{\"end\":60999,\"start\":60993},{\"end\":61006,\"start\":60999},{\"end\":61015,\"start\":61006},{\"end\":61022,\"start\":61015},{\"end\":61601,\"start\":61594},{\"end\":61610,\"start\":61601},{\"end\":61619,\"start\":61610},{\"end\":61629,\"start\":61619},{\"end\":61636,\"start\":61629},{\"end\":62008,\"start\":61999},{\"end\":62015,\"start\":62008},{\"end\":62024,\"start\":62015},{\"end\":62336,\"start\":62328},{\"end\":62343,\"start\":62336},{\"end\":62351,\"start\":62343},{\"end\":62364,\"start\":62351},{\"end\":62379,\"start\":62364},{\"end\":63063,\"start\":63052},{\"end\":63073,\"start\":63063},{\"end\":63399,\"start\":63384},{\"end\":63410,\"start\":63399},{\"end\":63416,\"start\":63410},{\"end\":63423,\"start\":63416},{\"end\":63779,\"start\":63770},{\"end\":63786,\"start\":63779},{\"end\":63795,\"start\":63786},{\"end\":63807,\"start\":63795},{\"end\":63817,\"start\":63807},{\"end\":63825,\"start\":63817},{\"end\":63834,\"start\":63825},{\"end\":63843,\"start\":63834},{\"end\":64246,\"start\":64235},{\"end\":64258,\"start\":64246},{\"end\":64269,\"start\":64258},{\"end\":64282,\"start\":64269},{\"end\":64513,\"start\":64504},{\"end\":64519,\"start\":64513},{\"end\":64706,\"start\":64699},{\"end\":64719,\"start\":64706},{\"end\":64957,\"start\":64948},{\"end\":64967,\"start\":64957},{\"end\":64977,\"start\":64967},{\"end\":65144,\"start\":65138},{\"end\":65153,\"start\":65144},{\"end\":65160,\"start\":65153},{\"end\":65167,\"start\":65160},{\"end\":65553,\"start\":65542},{\"end\":65563,\"start\":65553},{\"end\":65848,\"start\":65836},{\"end\":66138,\"start\":66127},{\"end\":66145,\"start\":66138},{\"end\":66594,\"start\":66582},{\"end\":66607,\"start\":66594},{\"end\":66946,\"start\":66935},{\"end\":66956,\"start\":66946},{\"end\":67286,\"start\":67279},{\"end\":67294,\"start\":67286},{\"end\":67300,\"start\":67294},{\"end\":68003,\"start\":67995},{\"end\":68009,\"start\":68003},{\"end\":68015,\"start\":68009},{\"end\":68022,\"start\":68015},{\"end\":68028,\"start\":68022},{\"end\":68035,\"start\":68028},{\"end\":68043,\"start\":68035},{\"end\":68051,\"start\":68043},{\"end\":68415,\"start\":68395},{\"end\":68433,\"start\":68415},{\"end\":68788,\"start\":68781},{\"end\":68801,\"start\":68788},{\"end\":68817,\"start\":68801},{\"end\":68829,\"start\":68817},{\"end\":68837,\"start\":68829},{\"end\":69328,\"start\":69319},{\"end\":69336,\"start\":69328},{\"end\":69342,\"start\":69336},{\"end\":69349,\"start\":69342},{\"end\":69357,\"start\":69349},{\"end\":69365,\"start\":69357},{\"end\":69372,\"start\":69365},{\"end\":69380,\"start\":69372},{\"end\":69386,\"start\":69380},{\"end\":69403,\"start\":69386},{\"end\":69409,\"start\":69403},{\"end\":69415,\"start\":69409},{\"end\":70157,\"start\":70149},{\"end\":70169,\"start\":70157},{\"end\":70181,\"start\":70169},{\"end\":70509,\"start\":70498},{\"end\":70520,\"start\":70509},{\"end\":70533,\"start\":70520},{\"end\":70550,\"start\":70533},{\"end\":70558,\"start\":70550},{\"end\":70567,\"start\":70558},{\"end\":70936,\"start\":70924},{\"end\":70944,\"start\":70936},{\"end\":70952,\"start\":70944},{\"end\":70963,\"start\":70952},{\"end\":71299,\"start\":71282},{\"end\":71312,\"start\":71299},{\"end\":71324,\"start\":71312},{\"end\":71748,\"start\":71735},{\"end\":71757,\"start\":71748},{\"end\":72125,\"start\":72114},{\"end\":72137,\"start\":72125},{\"end\":72503,\"start\":72494},{\"end\":72510,\"start\":72503},{\"end\":72518,\"start\":72510},{\"end\":73042,\"start\":73031},{\"end\":73053,\"start\":73042},{\"end\":73063,\"start\":73053},{\"end\":73366,\"start\":73359},{\"end\":73383,\"start\":73366},{\"end\":73869,\"start\":73863},{\"end\":73877,\"start\":73869},{\"end\":73886,\"start\":73877},{\"end\":73893,\"start\":73886},{\"end\":73901,\"start\":73893},{\"end\":73910,\"start\":73901},{\"end\":73928,\"start\":73910},{\"end\":73944,\"start\":73928},{\"end\":73951,\"start\":73944},{\"end\":73958,\"start\":73951},{\"end\":74435,\"start\":74418},{\"end\":74443,\"start\":74435},{\"end\":74459,\"start\":74443},{\"end\":74465,\"start\":74459},{\"end\":74916,\"start\":74909},{\"end\":74924,\"start\":74916},{\"end\":74934,\"start\":74924},{\"end\":74940,\"start\":74934},{\"end\":74946,\"start\":74940},{\"end\":74953,\"start\":74946},{\"end\":75452,\"start\":75444},{\"end\":75463,\"start\":75452},{\"end\":75479,\"start\":75463},{\"end\":75490,\"start\":75479},{\"end\":75502,\"start\":75490},{\"end\":75511,\"start\":75502},{\"end\":75519,\"start\":75511},{\"end\":75531,\"start\":75519},{\"end\":75880,\"start\":75873},{\"end\":75890,\"start\":75880},{\"end\":75898,\"start\":75890},{\"end\":75906,\"start\":75898},{\"end\":75912,\"start\":75906},{\"end\":75918,\"start\":75912},{\"end\":75924,\"start\":75918},{\"end\":75930,\"start\":75924},{\"end\":76272,\"start\":76265},{\"end\":76282,\"start\":76272},{\"end\":76299,\"start\":76282},{\"end\":76310,\"start\":76299},{\"end\":76712,\"start\":76705},{\"end\":76721,\"start\":76712},{\"end\":76730,\"start\":76721},{\"end\":76737,\"start\":76730},{\"end\":77075,\"start\":77068},{\"end\":77083,\"start\":77075},{\"end\":77091,\"start\":77083},{\"end\":77100,\"start\":77091}]", "bib_venue": "[{\"end\":58113,\"start\":58023},{\"end\":58854,\"start\":58768},{\"end\":59406,\"start\":59320},{\"end\":60109,\"start\":60097},{\"end\":60610,\"start\":60542},{\"end\":61207,\"start\":61123},{\"end\":62640,\"start\":62530},{\"end\":65308,\"start\":65246},{\"end\":67621,\"start\":67613},{\"end\":68990,\"start\":68922},{\"end\":69676,\"start\":69566},{\"end\":72675,\"start\":72605},{\"end\":73109,\"start\":73090},{\"end\":73572,\"start\":73486},{\"end\":75120,\"start\":75045},{\"end\":77361,\"start\":77251},{\"end\":58021,\"start\":57916},{\"end\":58422,\"start\":58376},{\"end\":58766,\"start\":58665},{\"end\":59318,\"start\":59217},{\"end\":59728,\"start\":59652},{\"end\":60095,\"start\":60058},{\"end\":60540,\"start\":60457},{\"end\":61121,\"start\":61022},{\"end\":61701,\"start\":61636},{\"end\":62092,\"start\":62024},{\"end\":62528,\"start\":62420},{\"end\":63121,\"start\":63073},{\"end\":63471,\"start\":63423},{\"end\":63904,\"start\":63843},{\"end\":64286,\"start\":64282},{\"end\":64502,\"start\":64414},{\"end\":64757,\"start\":64719},{\"end\":64983,\"start\":64977},{\"end\":65244,\"start\":65167},{\"end\":65619,\"start\":65563},{\"end\":65904,\"start\":65848},{\"end\":66206,\"start\":66145},{\"end\":66434,\"start\":66400},{\"end\":66636,\"start\":66607},{\"end\":66985,\"start\":66956},{\"end\":67277,\"start\":67209},{\"end\":67611,\"start\":67542},{\"end\":68058,\"start\":68051},{\"end\":68481,\"start\":68433},{\"end\":68920,\"start\":68837},{\"end\":69564,\"start\":69456},{\"end\":70229,\"start\":70181},{\"end\":70615,\"start\":70567},{\"end\":71011,\"start\":70963},{\"end\":71367,\"start\":71324},{\"end\":71733,\"start\":71577},{\"end\":72157,\"start\":72137},{\"end\":72603,\"start\":72518},{\"end\":73088,\"start\":73063},{\"end\":73484,\"start\":73383},{\"end\":74010,\"start\":73958},{\"end\":74540,\"start\":74465},{\"end\":75043,\"start\":74953},{\"end\":75565,\"start\":75531},{\"end\":75982,\"start\":75930},{\"end\":76377,\"start\":76310},{\"end\":76789,\"start\":76737},{\"end\":77249,\"start\":77141}]"}}}, "year": 2023, "month": 12, "day": 17}