{"id": 14542261, "updated": "2022-04-29 00:20:11.087", "metadata": {"title": "Gradient-Based Learning Applied To Document Recognition", "authors": "[{\"first\":\"Yann\",\"last\":\"LeCun\",\"middle\":[]},{\"first\":\"L\u00e9on\",\"last\":\"Bottou\",\"middle\":[]},{\"first\":\"Yoshua\",\"last\":\"Bengio\",\"middle\":[]},{\"first\":\"Patrick\",\"last\":\"Haffner\",\"middle\":[]}]", "venue": "Proc. IEEE", "journal": "Proc. IEEE", "publication_date": {"year": 1998, "month": null, "day": null}, "abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2112796928", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/pieee/LeCunBBH98", "doi": "10.1109/5.726791"}}, "content": {"source": {"pdf_hash": "c535e5c2d6ac03e99b528438aaec6a547de8098c", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "168d95c2942531b39bd18e3d8df3be82c6fc938a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c535e5c2d6ac03e99b528438aaec6a547de8098c.txt", "contents": "\n\n\n15 E/15/076\n\n\nINTRODUCTION\n\n\u2022 Why Gradient based learning?\n\n\u25cb easier to minimize a reasonably smooth,continuous function than a discrete function! \u2022 Two tokens sitting each on the start nodes of the input acceptor graph and the transducer graph.\n\n\u2022 Trajectory represents a sequence of input symbols that complies with both the acceptor and the transducer.\n\nEg:- \n\n\u2022 backpropagation \u2022\nbackpropagationLearning In Real Handwriting Recognition Systems \u25cb Heuristic Over Segmentation\u2022 Solutions for tasks such as object detection, face detection, pose estimation and more all have CNN architecture variants. Each input pixel value contributes to a weighted sum for each output unit. The output unit with the highest sum (including the contribution of a bias constant) indicates the class of the input character.\u2022 A K-NN classifier with a Euclidean distance measure between input images\u25cb No training time \u25cb No thought on the part of the designer \u25cb But, memory requirement and recognition time are large \u25cb 60000 , 20 x 20 pixel images Comparison with Other Classifiers cont. C.3.Principle Component Analysis(PCA) and Polynomial Classifier \u2022 To compute the principal components \u25cb First, compute the mean of each input component \u25cb Second, subtract from the training vectors \u25cb Next, compute covariance matrix of the resulting vectors \u25cb Then, diagonalize using singular value decomposition \u2022 Second degree polynomial classifier Comparison with Other Classifiers cont. C.4.Radial Basis Function Network \u2022 An RBF network \u25cb First layer -1000 Gaussian RBF units with 28 28 inputs \u25cb Second layer -simple 1000 inputs/ten outputs linear classifier Comparison with Other Classifiers cont. Develop-own version of the USPS (U.S. Postal Service zip codes) database Comparison with Other Classifiers cont. Comparison with Other Classifiers cont. Comparison with Other Classifiers cont. Comparison with Other Classifiers cont. Upper and lower profiles of entire fields are detected and used to normalize the image to a fixed height. \u2022 MNIST training set with salt and pepper noise \u2022 Each pixel -randomly inverted with probability 0.1 2 .\u2022 Globally Trainable Systems \n\n\u25cb Most practical pattern recognition systems are composed of \nmultiple modules. \n\n3 \n\nE/15/279 \n\nCONVOLUTIONAL NEURAL NETWORKS FOR \nISOLATED CHARACTER RECOGNITION \n\n4 \n\nE/15/279 \n\nConvolutional Networks \n\n5 \n\n\u2022 Convolutional Neural Networks is the standard form of neural network \narchitecture for solving tasks associated with images. \n\nE/15/279 \n\n6 \n\n\u2022 A few characteristics of the CNN architecture makes them more favourable in several \ncomputer vision tasks. \n\u2022 Local Receptive Fields \n\u2022 Sub-Sampling \n\u2022 Weight Sharing \n\nE/15/279 \nFig01 \nFig02 \n\nLeNet-5 \n\n7 \n\n\u2022 LeNet-5 CNN architecture is made up of 7 layers. \n-3 convolutional layers \n-2 subsampling layers \n-2 fully connected layers \n\n8 \n\n\u2022 The LeNet5 construct two significant types of layer construct \n-Convolutional Layers \n-Sub-Sampling Layers \n\nLayer C1 \n\n-A convolutional layer with 6 feature maps of size 28x28 \n-The size of the feature maps prevents connections from the input from falling off the boundary \n-C1 contains 156 trainable parameters and 122,304 connections. \n\nLayer S2 \n\n-A sub-sampling layer with 6 feature maps of size 14x14 \n-Each unit in each feature map is connected to a 2x2 neighbourhood in the corresponding feature map in \nC1 \n-do downsampling \n\nLayer C3 \n\n-A convolutional layer with 16 feature maps of size 28x28 \n-Each unit is connected to several 5x5 neighbourhoods \nst identical locations \n\nE/15/279 \nLayer S4 \n\n-A sub-sampling layer with 16 feature maps of size 5x5 \n-Each unit in each feature map is connected in a similar way as C1 and S2 \n-S4 contains 32 trainable parameters and 2000 connections. \n\nLayer C5 \n\n-A convolutional layer with 120 feature maps of size 1x1 \n-S4 contains 48,120 trainable connections. \n\nE/15/279 \n\nLoss Function \n\n10 \n\nE/15/279 \n\n\u2022 The Loss is used to calculate the gradients. And gradients are used to \nupdate the weights of the Neural Net. This is how a Neural Net is \ntrained. \nB. Results \n\n\u2022 LeNet-5 \n\n\u25cb 20 iterations \n\u25cb Diagonal hessian approximation was re-evaluated on 500 samples \n\u25cb \nis a hand-picked constant ( = 0.02 ) \n\u25cb h kk is is an estimate of the second derivative of the loss function with respect to \u2375 k \n\u25cb \n-global learning rate \n\n14 \n\nIteration \n\n1-2 \n0.0005 \n\n3-5 \n0.0002 \n\n6-8 \n0.0001 \n\n9-12 \n0.00005 \n\n13=20 \n0.00001 \n\nE/15/076 \n\nResults cont. \n\n\u2022 Common phenomenon \n\n\u25cb \nWhen overtraining occurs, the training error keeps decreasing over time but the test error \ngoes through a minimum and starts increasing after a certain number of iterations \n\n\u2022 It was not observed in this case \n\n\u25cb Because learning rate was kept relatively large. \n\n\u2022 The test error rate through the training set \nat 0.95%. (10 iterations) \n\u2022 The error rate on the training set \n0.35% (19 iterations) \nResults cont. \n\n\u2022 Influence of training set \n\n\u25cb artificially generated more training examples \n\u25a0 \n60 000 original patterns \n\u25a0 540 000 instances of distorted patterns \n\u2022 \nhorizontal and vertical translations \n\u2022 Scaling \n\u2022 Squeezing \n\u25cb \nTest error rate dropped to 0.8% (from 0.95%) \n\n16 \n\nE/15/076 \n\nC. Comparison with Other Classifiers \n\nC.1.Linear Classifier and Pairwise Linear Classifier \n\n\u2022 Simplest classifier \n\n\u2022 \n\u2022 \nsimple improvement of the basic linear classifier \n\n\u25cb train each unit of a single layer network to \n\nseparate each class from each other class \n\n17 \n\nDatabase \nError rate Free \nparameters \n\nRegular data \n12% \n7850 \n\nDeslanted \nimages \n\n8.4% \n4010 \n\nE/15/076 \n\nComparison with Other Classifiers cont. \n\nC.2. Baseline Nearest Neighbour Classifier \n\n\u2022 K = 3 \n\n18 \n\nDatabase \nError rate \n\nRegular data \n5% \n\nDeslanted images \n2.4% \n\nE/15/076 \n\n\u25cb Input -40-dimensional feature vector \n\u25cb A linear classifier with 821 inputs \n\n19 \n\nDatabase \nError rate \n\nRegular data \n3.3% \n\nE/15/076 \n\n\u25cb \nRBF units were divided into ten groups of 100 \n\u25a0 using the adaptive K-means algorithm \n\u25cb Second-layer weights \n\u25a0 compute using a regularized pseudo inverse method \n\n20 \n\nDatabase \nError rate \n\nRegular data \n3.6% \n\nE/15/076 \n\nComparison with Other Classifiers cont. \n\nC.5.One-Hidden Layer Fully Connected MultiLayer Neural Network \n\n\u2022 A fully connected multilayer NN \n\n\u25cb Two layers of weights \n\u25cb Trained with the version of back-propagation \n\n21 \n\nDatabase \nError rate No.of hidden \nunits \n\nRegular data \n4.7% \n300 \n\n4.5% \n1000 \n\nDeslanted \nimages \n\n1.6% \n300 \n\nE/15/076 \n\nComparison with Other Classifiers cont. \n\nC.6.Two-Hidden Layer Fully Connected MultiLayer Neural Network \n\n\u2022 A two-hidden-layer multilayer NN \n\n\u25cb A much better result than the one-hidden-layer network \n\n22 \n\nNetwork \nError rate \n\n28x28-300-100-10 network \n3.05% \n\n28x28-1000-150-10 network \n2.95% \n\n28x28-300-100-10 network \n2.50% \n\n28x28-1000-150-10 network \n2.45% \n\nE/15/076 \n\nC.7.A Small Convolutional Network:LeNet-1 \n\n\u2022 LeNet-1 \n\n\u25cb For comparison purposes \n\n\u25cb Images -16x16 pixels & centered in the 28 28 input layer \n\n\u25cb \u25cb Test error -1.7% \n\n23 \n\nE/15/076 \n\nC.8.LeNet-4 \n\n\u2022 LeNet-4 \n\n\u25cb For large size of the training set \n\n\u25cb Test error -1.1% \n\n\u25cb Replace the last layer -a Euclidean nearest-neighbor classifier \n\n\u25cb Improve rejection performance \n\n24 \n\nE/15/076 \n\nC.9.Boosted LeNet-4 \n\n\u2022 Three LeNet-4 \n\n\u25cb First -train usually \n\u25cb Second -train on patterns that are filtered by the first net \n\u25cb Third -train on new patterns \n\n\u2022 Test error rate -0.7% \n\n25 \n\nE/15/076 \n\nC.10.Tangent Distance Classifier(TDC) \n\n\u2022 Tangent distance classifier -a nearest-neighbor method \n\u2022 Test error rate -1.1% \n\n\u25cb Using 16x16 pixel images \n\n26 \n\nE/15/076 \n\nC.11.Support Vector Machine(SVM) \n\n\u2022 \n\n27 \n\nMethod \nError rate \n\nRegular SVM \n1.4% \n\nScholkopf's V-SVM \n1.0% \n\nmodified V-SVM \n0.8% \n\nBurges's RS-SVM technique \n1.1% \n\nE/15/076 \n\nDiscussion \n\n\u2022 Raw error rate of the \n\nClassifiers on the 10 000 \n\nexample test set. \n\n\u2022 Boosted LeNet-4 \n\nperformed best (0.7%) \n\n\u2022 Closely followed by \n\nLeNet-5 at 0.8%. \n\n28 \n\nE/15/076 \n\nDiscussion cont. \n\n\u2022 Rejection performance \n\n\u25cb Again, Boosted LeNet-4 has the best \nperformance. \n\u25cb The enhanced versions of LeNet4 \ndid better than the original LeNet-4 \n\n\u2022 Memory requirements \n\n\u25cb Most methods -1 byte , nearest \nneighbor methods-4 bits per pixel \n\n\u2022 Boosting gives a substantial \nimprovement in accuracy, SVM \nhas excellent accuracy. \n\n29 \n\nE/15/076 \n\nInvariance and Noise Resistance \n\n\u2022 Important is not obvious \n\u2022 30 \n\nE/15/076 \n\nMULTI-MODULE SYSTEMS AND GRAPH \nTRANSFORMER NETWORKS \n\n\u2022 Train systems composed of multiple heterogeneous modules? \n\n\u25cb \nlarge and complex trainable systems need to be built out of simple, specialized modules \n\u25a0 LeNet-5 \n\u25a0 system for recognizing words \n\n31 \n\nE/15/076 \n\nA. An Object-Oriented Approach \n\n\u2022 Multi-module systems \n\n\u25cb Each module is an instance of a class \n\u25cb Module classes have a fprop method \n\u25cb bprop method for computing derivatives \n\u25cb Duality property between fprop & bprop \n\n\u2022 SN3.1 \n\n\u25cb Software environment used to obtain the results \n\u25cb Based on a home-grown object-oriented dialect \n\n32 \n\nE/15/076 \n\nB. Special Modules \n\n\u2022 Multiplexer module \n\n\u25cb Two (or more) regular inputs, \n\u25cb One switching input \n\u25cb One output \n\n\u2022 Min module \n\n\u25cb Two inputs \n\u25cb One output \nA. Previous work \n\n\u2022 Handwriting Recognition \nUse Graped-Based learning methods \n\u2022 Speech Recognition \nintegrate Graph based statistical models \nwith acoustic recognition modules mainly \nGaussian mixture models \n\n\u274f Problem \n\nNo proposal for a systematic approach to multi-layer graph-based Trainable \nsystems. \n\n43 \n\nE/15/211 \n\n\u2022 Concept of weighted finite-state transducers \n\n\u274f Mainly focused on \n\n>> Efficient search algorithms \n\n>> Algebraic aspects of combining transducers and graphs \n\n44 \n\nSpeech Recognition \n\nLanguage Transition \n\nHandwriting Recognition \n\nTransducers \n\nE/15/211 \n\nB. Standard Transduction \n\n\u2022 Transduction Operation \n\n45 \n\nInput Acceptor Graph \n\nInput Transducer Graph \n\nComposition \nOperation \nOutput Acceptor Graph \n\n\u27a2 Each path in this output graph (Sout) corresponds to one path (Sin) in the input \nacceptor graph and one path and a corresponding pair of input/output sequences \n(Sout,Sin) in the transducer graph. \n\nE/15/211 \n\nComposition of the Recognition Graph with the Grammar Graph \n\n46 \n\n\n\n\nThe incorporation of linguistic constraints when recognizing words or other character strings.-Compares the data structures pointed to by arcs arc1 and arc2 -Returns a boolean 2. fprop(ngraph, upnode, downnode, arc1, arc2) -Called when check(arc1, arc2) returns true. -Creates new arcs and nodes between nodes upnode and downnode -Computes the information attached to these newly created arcs 3. bprop(ngraph, upnode, downnode, arc1, arc2) -Called during training -Used in the fprop call with the same arguments -Compute the values attached to its output arcs is differentiable D. Notes On The Graph Structure \u27a2 The structure of the graph depends on -the nature of the Graph Transformer -the value of the parameters and on the input This might be considered a combinatorial problem and not Training proceeded in two phases 1. Kept the centers of the RBFs fixed 2. All the parameters, network weights, RBF centers were trained globallyE/15/211 \n\nC. Generalized Transduction \n\n\u2022 Composition Transformer \n\nThree methods : \n\n1. check(arc1, arc2) \n\n47 \n\nE/15/211 \n\nSimplified generalized graph composition algorithm \n\n48 \n\nE/15/211 \n\n49 \n\nbprop( ) \ncheck( ) \nfprop( ) \n\nBasis of the backpropagation \nalgorithm for generic graph \ntransformers \n\nEstablishes the structure of the \nephemeral network inside the \ncomposition transformer \n\nImplements the numerical \nrelationship \n\nAn online handwriting recognition \nGTN based on heuristic \nover-segmentation \n\nAn online handwriting recognition \nGTN based on Space Displacement \nNeural Network \n\nE/15/211 \n\nSystem Flow \n\n54 \n\nPreprocessing \nNetwork \nArchitecture \n\nNetwork \nTraining \n\nExperimental \nResults \n\n\u2022 \nReduces intra-character \nvariability, simplifying \ncharacter recognition. \n\n\u2022 \nDesign AMAP where pen \ntrajectories are represented \nby low resolution images \n\n\u2022 \nBest online/offline network is a \n5-layer convolutional network \n\n\u2022 \nDetermine that the resolution was \nsufficient for representing \nhandwritten characters \n\n\u2022 \n\u2022 \nTwo approaches \n\nA. A GTN for Check Amount RecognitionProcedure :Input -A trivial graph with a single arc that carries the image of the whole check\nHeuristic Over-Segmentation approach 2. SDNN approach. Heuristic Over-Segmentation approach 2. SDNN approach\n", "annotations": {"author": null, "publisher": null, "author_last_name": null, "author_first_name": null, "author_affiliation": null, "title": null, "venue": null, "abstract": null, "bib_ref": null, "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":10200,\"start\":366},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":12195,\"start\":10201}]", "paragraph": "[{\"end\":61,\"start\":31},{\"end\":248,\"start\":63},{\"end\":358,\"start\":250},{\"end\":365,\"start\":360}]", "formula": null, "table_ref": null, "section_header": "[{\"end\":29,\"start\":17},{\"end\":386,\"start\":367}]", "table": "[{\"end\":10200,\"start\":2115},{\"end\":12195,\"start\":11137}]", "figure_caption": "[{\"end\":2115,\"start\":402},{\"end\":11137,\"start\":10203}]", "figure_ref": null, "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":12435,\"start\":12327}]", "bib_title": null, "bib_author": null, "bib_venue": "[{\"end\":12380,\"start\":12327}]"}}}, "year": 2023, "month": 12, "day": 17}