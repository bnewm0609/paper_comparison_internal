{"id": 203651142, "updated": "2022-02-04 03:36:29.839", "metadata": {"title": "AdaptHD: Adaptive Efficient Training for Brain-Inspired Hyperdimensional Computing", "authors": "[{\"middle\":[],\"last\":\"Imani\",\"first\":\"Mohsen\"},{\"middle\":[],\"last\":\"Morris\",\"first\":\"Justin\"},{\"middle\":[],\"last\":\"Bosch\",\"first\":\"Samuel\"},{\"middle\":[],\"last\":\"Shu\",\"first\":\"Helen\"},{\"middle\":[\"De\"],\"last\":\"Micheli\",\"first\":\"Giovanni\"},{\"middle\":[],\"last\":\"Rosing\",\"first\":\"Tajana\"}]", "venue": "2019 IEEE Biomedical Circuits and Systems Conference (BioCAS)", "journal": "2019 IEEE Biomedical Circuits and Systems Conference (BioCAS)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Brain-inspired Hyperdimensional (HD) computing is a promising solution for energy-efficient classification. HD emulates cognition tasks by exploiting long-size vectors instead of working with numeric values used in contemporary processors. However, the existing HD computing algorithms have lack of controllability on the training iterations which often results in slow training or divergence. In this work, we propose AdaptHD, an adaptive learning approach based on HD computing to address the HD training issues. AdaptHD introduces the definition of learning rate in HD computing and proposes two approaches for adaptive training: iteration-dependent and data-dependent. In the iteration-dependent approach, AdaptHD uses a large learning rate to speedup the training procedure in the first iterations, and then adaptively reduces the learning rate depending on the slope of the error rate. In the data-dependent approach, AdaptHD changes the learning rate for each data point depending on how far off the data was misclassified. Our evaluations on a wide range of classification applications show that AdaptHD achieves 6.9\u00d7 speedup and 6.3\u00d7 energy efficiency improvement during training as compared to the state-of-the-art HD computing algorithm.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2993412634", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/biocas/ImaniMBSMR19", "doi": "10.1109/biocas.2019.8918974"}}, "content": {"source": {"pdf_hash": "6ce217e430ae1ea2b67018c1f3ca7fbb005437c3", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://infoscience.epfl.ch/record/271523/files/BioCAS_AdaptHD.pdf", "status": "GREEN"}}, "grobid": {"id": "e65f5b20ce675f91627e9ff0a2324c157bad0c8c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6ce217e430ae1ea2b67018c1f3ca7fbb005437c3.txt", "contents": "\nAdaptHD: Adaptive Efficient Training for Brain-Inspired Hyperdimensional Computing\n\n\nMohsen Imani moimani@ucsd.edu \nComputer Science and Engineering\nUC San Diego\nLa JollaCAUSA\n\nJustin Morris j1morris@ucsd.edu \nComputer Science and Engineering\nUC San Diego\nLa JollaCAUSA\n\nSamuel Bosch samuel.bosch@epfl.ch \nIntegrated Systems Laboratory\nEPFL\nLausanneSwitzerland\n\nHelen Shu hsshu@ucsd.edu \nComputer Science and Engineering\nUC San Diego\nLa JollaCAUSA\n\nGiovanni De Micheli giovanni.demicheli@epfl.ch \nIntegrated Systems Laboratory\nEPFL\nLausanneSwitzerland\n\nTajana Rosing tajana@ucsd.edu \nComputer Science and Engineering\nUC San Diego\nLa JollaCAUSA\n\nAdaptHD: Adaptive Efficient Training for Brain-Inspired Hyperdimensional Computing\n\nBrain-inspired Hyperdimensional (HD) computing is a promising solution for energy-efficient classification. HD emulates cognition tasks by exploiting long-size vectors instead of working with numeric values used in contemporary processors. However, the existing HD computing algorithms have lack of controllability on the training iterations which often results in slow training or divergence. In this work, we propose AdaptHD, an adaptive learning approach based on HD computing to address the HD training issues. AdaptHD introduces the definition of learning rate in HD computing and proposes two approaches for adaptive training: iteration-dependent and data-dependent. In the iteration-dependent approach, AdaptHD uses a large learning rate to speedup the training procedure in the first iterations, and then adaptively reduces the learning rate depending on the slope of the error rate. In the data-dependent approach, AdaptHD changes the learning rate for each data point depending on how far off the data was misclassified. Our evaluations on a wide range of classification applications show that AdaptHD achieves 6.9\u00d7 speedup and 6.3\u00d7 energy efficiency improvement during training as compared to the state-of-the-art HD computing algorithm.\n\nI. INTRODUCTION\n\nHyperdimensional (HD) computing is a brain-inspired computational approach which can go a long way in addressing the energy bounds that plague deterministic computing [1]. HD computing is based on the understanding that brains compute with patterns of neural activity that are not readily associated with numbers. In fact, the brain's ability to calculate with numbers is feeble. However, we can model neural activity patterns with points in a high-dimensional space. When the dimensionality is in the thousands (e.g., D = 10,000), it is called hyperdimensional. Operations on hypervectors can be combined into interesting computational behaviors with unique features that make them robust and efficient. HD has been used in different domains, including analogy-based reasoning [2], language recognition [3], [4], prediction from multimodal sensor fusion [5], [6], speech recognition [7], and braincomputer interfaces [8]. HD computing supports single-pass training, where the training can happen by only going once through a training data [4]. However, in this work, we argue that HD using single-pass training provides significantly low accuracy on practical classification problems, e.g., speech recognition and face detection. For example, for face detection application [9], singlepass learning provides 40% lower classification accuracy than the model which has been retrained. One efficient way to improve the HD classification accuracy is performing perceptron-like iterative training. However, without a clear definition of learning rate the retraining process often diverge or may take long time to converge. This makes the HD computing less desirable, as low-cost training is essential for embedded devices with limited battery and resources.\n\nIn this paper, we propose AdaptHD, a novel adaptive retraining method for HD computing. For the first time, AdaptHD introduces the definition of learning rate in HD computing and retrains a model using two adaptive approaches: iteration-dependent and data-dependent. In the iteration-dependent approach, AdaptHD changes the learning rate based on the changes in the classification accuracy during retraining. In the first iterations, this approach uses a large learning rate to speedup the training procedure, and then adaptively reduces the learning rate depending on the slope of the error rate. A second approach is a data-dependent approach, where AdaptHD changes the learning rate for each data point, depending on how far off the data was misclassified. Finally, AdaptHD proposes a hybrid approach which updates the learning rate considering both iteration and data dependency. We have evaluated the efficiency of AdaptHD on a wide range of classification applications. Our evaluation shows that AdaptHD using iteration-dependent (data-dependent) approach can achieve 6.9\u00d7 speedup and 6.3\u00d7 energy efficiency improvement during training as compared to the state-of-the-art HD computing algorithm.\n\n\nII. ADAPTHD FRAMEWORK A. HD Classification\n\nIn this section, we propose AdaptHD, an adaptive training framework for high-efficient and high-accurate Hyperdimensional computing. Figure 1 shows an overview of the AdaptHD performing the classification task. In AdaptHD, the encoder block maps input data into high-dimensional vectors, e.g., D = 10, 000 dimensions, and combines them in order to generate a hypervector representing each class. During inference, the classifier performs the classification task by looking at the similarity of the input hypervector to each of the stored class hypervectors. In the following, we explain the functionality of each module.\n\nEncoding: In HD computing, the first step is to map/encode all data points from original to a hypervector( \u2022 A ). Here, we consider a general encoding approach which maps a feature vector [7]. This encoding finds the minimum and maximum feature values and quantizes that range into m levels.  where H is the (non-binary) encoded hypervector, \u2295 denotes the XOR operation, and L i is the (binary) hypervector corresponding to the i-th feature of vector F . AdaptHD binarizes this hypervector by comparing each dimension of H with a threshold value, which is half of the number of features (T HR = n/2). Training: The encoded hypervectors are combined in a AdaptHD training module in order to create a single hypervector representing each class [4]. This combination is a simple addition of all encoded data points belong to a particular class. Then, the class hypervectors are binarized. For an application with k classes, AdaptHD results in generating k class hypervec- This single-pass model provides good classification accuracy in a short amount of training time, but in practical applications, this accuracy is significantly lower than state-of-the-art (as explained in Section I).\nF = {f 1 , f 2 , . . . , fn}, with n features (f i \u2208 N ) to high- dimensional vector Q = {q 1 , q 2 , . . . , q D } with D dimensions (q i \u2208 {0, 1} D ) [1],H = ID 1 \u2295 L 1 + ID 2 \u2295 L 2 + . . . + IDn \u2295 Ln.tors, {C 1 , C 2 , . . . , C k } where C i \u2208 {0, 1} D .\nInference: In order to classify the input data, AdaptHD uses the same encoding module to map a test data into a high-dimensional vector, called query hypervector. AdaptHD calculates the Hamming distance similarity of each class hypervector with a query hypervector. Finally, a data point is assigned to a class which it has the highest similarity with it.\n\n\nB. AdaptHD Learning Rate\n\nTo improve the HD model, we use a retraining approach which adjusts the class hypervectors by iterating over the training dataset ( \u2022 C ). After the initial training, AdaptHD saves a nonbinary training model, C i \u2208 N D . AdaptHD starts retraining by iteratively checking the similarity of each training data point with all class hypervector. Since the class hypervectors are non-binary, this similarity check happens using cosine metric. Our exportation is that all training data points should be correctly classified by the model. In case of a correct classification, we do not make any changes on the model. However, if Q i is misclassified, we subtract Q i from the incorrect class and add it to a class that it should have been matched with it (the label of the Q i data). For the first time, AdaptHD introduces the definition of learning rate in HD computing. Learning rate, \u03b1, specifies the amount of changes that we make into a model during each retraining iteration. For example, for each misclassified data during retraining iteration, AdaptHD updates correct and incorrect class hypervectors using: Cwrong = Cwrong \u2212 \u03b1Q i and C correct = C correct + \u03b1Q i . This process continues iteratively through training dataset until the algorithm converges ( \u2022 D ). The convergence defines as a time that the HD classification accuracy in the last three iterations changes less than 0.1%. After the convergence, AdaptHD binarizes the retrained model in order to enable the inference task to be performed using as efficient Hamming distance similarity check. Figure 2 shows the impact of using different learning rates on the accuracy of speech recognition application [10]. Our evaluation shows that using a small learning rate, e.g., \u03b1 = 2, HD requires many retraining iterations to get near the best accuracy. Lower learning rates are better for fine-tuning the model towards the end because the accuracy does not fluctuate as much. On the other hand, with higher learning rates, we can get near the best accuracy faster. Higher learning rates are not suitable for finetuning the model once we are near the best accuracy. In fact, large learning rate results in the accuracy fluctuation which increases the possibility of divergence (as depicted in Figure 2). Often time HD using high learning rates do not achieve as high of an accuracy as lower learning rates. This is also due to the fact the lower learning rates are better at fine-tuning the HD model once it is near the best accuracy it can achieve.\n\nWhen adaptively changing \u03b1, we want to take advantage of higher learning rates (\u03b1 >>) to get near the best accuracy faster. Once we are near the best accuracy, we want to take advantage of lower learning (\u03b1 <<) rates to fine-tune the model. In this work, AdaptHD goal is to get the advantage of both small and larger learning rates in order to provide high classification accuracy as well as fast training. The black line in Figure 2 shows the AdaptHD classification accuracy when using the adaptive learning rate. This approach uses a large learning rate in the first iterations of training in order to accelerate the training process but eventually reduces the learning rate in order to converge.\n\n\nC. AdaptHD Iterative Training\n\nAdaptHD proposes two methods for adaptive retraining: iteration-dependent and data-dependent. In the following, we explain the details of each approach.\n\nAdaptHD Iteration-Dependent Learning: The iteration dependent approach increases the learning rate (\u03b1) adaptively during the retraining iterating. AdaptHD starts the training with a large learning rate, \u03b1max, in order to speedup the training. Then, it reduces the learning rate depending on the changes in the classification accuracy. AdaptHD exploits the error rates (percentages of wrong classifications) of the last \u03b2 iterations in order to decide the learning rate on the next iteration. Where there is larger the error rates on the last \u03b2 iterations, AdaptHD selects a larger learning rate in order to faster adjust the model. As the changes in the error rate decreases, the accuracy gets closer to converging, so AdaptHD uses lower learning rates. Figure 3 shows the overview of the AdaptHD using iterationdependent adaptive learning framework. After initial training, the HD model is retrained iteratively on every training data points. If the HD model misclassifies a training data, the HD model is updated by adding and subtracting \u03b1Q from two hypervectors in the model. Once the HD model has been retrained on the entire training set, the previous error rates are sent to the Learning Rate Decider LRDecider to set \u03b1 for the next iteration. Figure 3 also shows how the LRDecider sets the \u03b1 value on the next iterations. Based on the data distribution of error rates, AdaptHD uses a linear function to determine \u03b1 value. To avoid floating point operations, we use a step function to set the learning rate depending on the average error rate. As the error rate decreases, \u03b1 also decreases. This is because as the accuracy of our HD model gets closer to converging, the error rate will decrease. Thus, \u03b1 should be decreased to help the model converge faster. AdaptHD Data-Dependent Learning: Unlike the iteration depended approach which uses the same learning rate for all data points, data-dependent approach adaptively modifies the learning rate for each sample data. The goal of this approach is to make higher changes to the model if a data point is misclassified with a far distance. Figure 4 shows the histogram of the cosine similarity difference in speech recognition application [10] during a single retraining iteration. Our evaluation shows that most of the data points have small cosine similarity difference. Meanwhile, there are some outliers (highlighted in the figure) that have much larger similarities differences. Since the model is further off from these training hypervectors, the HD model should be adjusted more aggressively to address that. To this end, AdaptHD uses a large learning rate for data points which are misclassified with the HD model with a far distance, while using smaller learning rates for marginal misclassifications. AdaptHD uses the difference of the cosine similarity of a query with the correct class, C correct , and misclassified class, Cwrong, as a similarity metric to identify how far a misclassification occurs. When the difference between the cosine similarities is larger, we need to adjust both class hyperevectors more to achieve this goal. When the difference is smaller, the model adjustment can happen with smaller learning rate. Figure 5 shows the overview of AdaptHD framework using datadependent learning. First, AdaptHD checks the similarity of each training data point with the current HD model. If a data point is misclassified, the difference of cosine similarities of a correct and incorrect class, \u0394\u03b4 = \u03b4(Q, Cwrong)\u2212\u03b4(Q, C correct ), is sent to the LRDecider, where \u03b4( * ) denotes cosine similarity. The LRDecider uses this similarity difference to decide what \u03b1 should be for that data point. The HD model is then updated with the data point, Q, multiplied by \u03b1. Once the HD model has been updated, AdaptHD sends a next data point to check the quality of the model. Figure 5 shows how the LRDecider block determines the learning rate for each data point. As shown, \u03b1 is determined with a linear function based on the similarity difference. We use a step function to set the learning rate depending on the similarity difference. As the similarity difference increases, \u03b1 also increases in order to make a larger modification on the current model. \n\n\nIII. EVALUATION A. Experimental Setup\n\nWe describe the functionality of the proposed AdaptHD using C++ implementation. We run AdaptHD on an embedded device (Raspberry Pi 3) using ARM Cortex A53 CPU. For the measurement of CPU power, we use a Hioki 3334 power meter. All experiments are performed for a case of using D = 10, 000 dimensions. We evaluate the efficiency of the proposed AdaptHD on four practical classification problems: Speech Recognition (ISO-LET) [10], Activity Recognition (UCIHAR) [11], Face Recognition (FACE) [9], and Cardiotocography (CARDIO) [12].\n\n\nB. AdaptHD & Maximum Learning Rate\n\nTraining Iterations: There is an optimal \u03b1max that the LRDecider function should saturate at. This is because if the function saturates at too low of an \u03b1max, retraining may still take many iterations. In this case, our method is closer to having a low constant learning rate as discussed in Section II-B. However, if \u03b1max is large, we may run into the same problem of having a large learning rate. This fluctuates the accuracy of the HD model and result in a low accuracy or divergence.\n\nWe explored different values of \u03b1max to find the optimal value. Figure 6 shows the effect of increasing \u03b1max for iterationdependent learning. As we predicted, a small \u03b1max of 2 does not significantly reduce the number of training iterations. Additionally, AdaptHD using a large \u03b1max of 20 takes many iterations to converge because the HD model is overadjusted. In some cases, the HD model diverges because it is overadjusted, such as when \u03b1max is 20 for the ISOLET dataset. By looking at Table I, an \u03b1max of 5 is the optimal value for the iteration-dependent approach. Figure 6 shows the impact of \u03b1max on the number of retraining iterations of data-dependent approach. Our results show that the optimal \u03b1max for the data-dependent approach is 15, which is slightly higher than the optimal value of the iteration-dependent approach. In fact, the data-dependent approach needs to make higher modification on each individual data, while still keeping the learning rate of marginally misclassified data small. However, iteration-dependent needs to be more conservative since it uses the same learning rate for all data points.\n\nAdaptHD Accuracy: Table I shows the classification accuracy of AdaptHD using different \u03b1max. The results are reported for four different cases: AdaptHD with iteration-based and data-dependent learning rates, AdaptHD using fixed learning rate of \u03b1max, and the baseline HD (\u03b1 = 1). In terms of accuracy, AdaptHD provides higher accuracy using small \u03b1max, e.g., 5 or 10. Using a very large learning rate, the fluctuation in the accuracy eliminates AdaptHD accuracy to converge into a stable value. Our evaluation shows   that AdaptHD trained with iteration-dependent and data-dependent approaches can achieve on average 2.0% and 1.2% higher classification accuracy as compared to the baseline HD (with a fixed \u03b1 of 1) in a significantly lower number of retraining iterations. Figure 7 visualizes the classification accuracy of AdaptHD using iteratingdependent and data-dependent approaches for ISOLET and FACE applications. Our evaluation shows that HD computing using a fixed and small learning rate takes a very long time to converge. However, AdaptHD exploits a large learning rate to accelerate the first training iterations and a small learning rate for convergence. Table II shows AdaptHD efficiency and accuracy exploiting both iteration and data-dependent approaches. Before each retraining iteration, AdaptHD first sets the learning rate using the iterationdependent approach. Then, it exploits the data-dependent approach to find a learning rate for each individual data point in an iteration. AdaptHD selects the average learning rate of the iteration and datadependent approach as a learning rate for each data point. The results (on ARM Cortex A53 CPU) show that AdaptHD in hybrid mode can achieve the high classification accuracy of iterationdependent and the fast training of data-dependent approach. For example, AdaptHD using iteration-dependent (data-dependent) approach can achieve 4.6\u00d7 and 4.2\u00d7 (6.7\u00d7 and 6.2\u00d7) speedup and energy efficiency improvement during training as compared to the baseline HD computing algorithm.\n\n\nC. Hybrid AdaptHD\n\n\nD. AdaptHD Comparison with Other Classifiers\n\nFinally, we compare the efficiency and accuracy of AdaptHD with other light-weight classification algorithms including SVM, Random Forest, Naive Bayes, Gradient Boosting (GBoosting), and Perceptron. We exploited Scikit-learn library [13] for the model training and testing and used grid search to find the best hyperparameters. Table III lists the classification accuracy and the training/test execution time of all algorithms running on an embedded device (Raspberry Pi 3) using ARM Cortex A53 CPU. Our evaluation shows that AdaptHD can provide better or comparable accuracy to other algorithms. For example, thanks to our non-linear encoding, AdaptHD can provide about 4-5% higher accuracy than perceptron algorithm which performs iterative training on original data. In terms of efficiency, AdaptHD can provide much faster computation in both training and testing. This higher AdaptHD efficiency comes from the following points: (i) AdaptHD starts the retraining process from an initial model which significantly reduces the number of required iterations. (ii) AdaptHD represents data points using a binary vectors which can process with significantly higher efficiency as compared to non-binary vectors.\n\n(iii) AdaptHD simplifies the inference task to Hamming distance similarity check, which can be implemented with an order of magnitude higher efficiency than dot product. Our evaluation shows that AdaptHD can achieve 2.1\u00d7 faster training and 5.5\u00d7 testing as compared to learning algorithms such as SVM.\n\n\nIV. CONCLUSION\n\nIn this paper, we propose two adaptive HD retraining approaches which enable retraining the HD model to be more efficient while maintaining the same accuracy. AdaptHD changes the learning rate to converge the accuracy faster. The iteration-dependent changes the learning rate through the retraining iterations, while datadependent approach adaptively changes learning rate depending on each data. Our evaluation shows that AdaptHD can achieve 6.9\u00d7 speedup and 6.3\u00d7 energy efficiency improvement during training as compared to the state-of-the-art HD computing algorithm.\n\n\nV. ACKNOWLEDGEMENTS\n\nThis work was partially supported by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, and also NSF grants #1911095, #1826967, #1730158 and #1527034.\n\nFig. 2 .\n2HD classification accuracy using low and high learning rates.\n\n\nThis training can happen by a single time passing through a training dataset ( \u2022 B ).\n\nFig. 3 .\n3AdaptHD framework supporting iteration-dependent training.\n\nFig. 4 .Fig. 5 .\n45Cosine difference (\u0394\u03b4) distribution in data-dependent. AdaptHD framework supporting data-dependent training.\n\nFig. 6 .\n6Number of required training iterations of the baseline HD and AdaptHD using iteration-dependent and data-dependent approaches.\n\nFig. 7 .\n7Classification accuracy of the baseline HD and AdaptHD using iteration and data-dependent approaches.\n\nTABLE I THE\nIIMPACT OF THE MAXIMUM LEARNING RATE (\u03b1max) ON THE ADAPTHD CLASSIFICATION ACCURACY.Learning Rate \n\u03b1max = 2 \n\u03b1max = 5 \n\u03b1max = 10 \n\u03b1max = 15 \n\u03b1max = 20 \n\u03b1 = 1 \nIteration \nData \n\u03b1max \nIteration \nData \n\u03b1max \nIteration \nData \n\u03b1max \nIteration \nData \n\u03b1max \nIteration \nData \n\u03b1max \n\nISOLET \n91.28 \n91.34 \n91.66 \n91.34 \n91.89 \n90.21 \n91.85 \n90.83 \n89.05 \n92.24 \n91.15 \n88.17 \n90.89 \n90.13 \nN/A \n91.10 \nUCIHAR \n93.44 \n94.63 \n93.96 \n96.20 \n94.40 \n93.9 \n95.62 \n94.41 \n93.65 \n94.66 \n94.34 \n92.12 \n94.34 \n92.66 \nNA \n93.82 \nFACE \n94.35 \n94.69 \n94.91 \n95.97 \n95.83 \n95.81 \n95.75 \n95.87 \n95.03 \n95.15 \n95.03 \n94.11 \n94.23 \n95.71 \n93.2 \n94.38 \nCARDIO \n99.88 \n98.34 \n98.56 \n99.90 \n99.53 \n99.30 \n100 \n98.59 \n99.21 \n100 \n98.12 \n98.12 \n99.06 \n98.59 \n97.84 \n98.17 \n\n\n\nTABLE II ACCURACY\nIIAND TRAINING EFFICIENCY OF ADAPTHD USING SIMULTANEOUS USE OF ITERATION AND DATA-DEPENDENT APPROACH.ISOLET \nUCIHAR \nFACE \nCARDIO \nIteration \nDependent \n\nEnergy Improv. \n2.83\u00d7 \n3.93\u00d7 \n5.20\u00d7 \n4.92\u00d7 \nSpeedup \n3.08\u00d7 \n4.41\u00d7 \n5.71\u00d7 \n5.29\u00d7 \nData \nDependent \n\nEnergy Improv. \n6.28\u00d7 \n4.90\u00d7 \n7.63\u00d7 \n5.96\u00d7 \nSpeedup \n6.83\u00d7 \n5.33\u00d7 \n8.29\u00d7 \n6.51\u00d7 \n\nHybrid \n\nEnergy Improv. \n6.25\u00d7 \n5.61\u00d7 \n7.58\u00d7 \n5.94\u00d7 \nSpeedup \n6.78\u00d7 \n6.09\u00d7 \n8.24\u00d7 \n6.48\u00d7 \nAccuracy \n92.4% \n96.0% \n95.9% \n99.9% \n\n\n\nTABLE III ACCURACY\nIII, TRAINING AND TESTING EXECUTION TIME OF ADAPTHD WITH OTHER LIGHT-WEIGHT ALGORITHMS.Training Execution (s) \nAccuracy \nInference \nExecution (ms) \nISOLET \nUCIHAR FACE CARDIO \n\nSVM \n4.82 \n2.81 \n0.53 \n0.03 \n95.69% \n6.91 \nRandom Forest \n1.04 \n1.10 \n2.96 \n0.02 \n93.50% \n5.66 \nNaive Bayes \n0.21 \n0.78 \n0.50 \n0.06 \n87.92% \n1.85 \nGBoosting \n53.15 \n7.36 \n77.41 \n0.21 \n96.05% \n4.38 \nPerceptron \n1.05 \n1.89 \n1.21 \n0.06 \n91.78% \n0.80 \nIteration (AdaptHD) \n1.36 \n1.40 \n0.73 \n0.01 \n96.15% \n1.24 \nData (AdaptHD) \n0.68 \n1.26 \n0.41 \n0.009 \n95.38% \nHybrid (AdaptHD) \n0.66 \n1.16 \n0.40 \n0.008 \n96.05% \n\n\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cognitive Com- putation, vol. 1, no. 2, pp. 139-159, 2009.\n\nWhat we mean when we say \"whats the dollar of mexico?\": Prototypes and mapping in concept space. P Kanerva, AAAI Fall Symposium. P. Kanerva, \"What we mean when we say \"whats the dollar of mexico?\": Prototypes and mapping in concept space,\" in AAAI Fall Symposium, pp. 2-6, 2010.\n\nLanguage geometry using random indexing. A Joshi, Quantum Interaction 2016 Conference Proceedings. In pressA. Joshi et al., \"Language geometry using random indexing,\" Quantum Interaction 2016 Conference Proceedings, In press.\n\nA robust and energy-efficient classifier using brain-inspired hyperdimensional computing. A Rahimi, ACM ISLPED. ACMA. Rahimi et al., \"A robust and energy-efficient classifier using brain-inspired hyperdimensional computing,\" in ACM ISLPED, pp. 64-69, ACM, 2016.\n\nModeling dependencies in multiple parallel data streams with hyperdimensional computing. O R\u00e4s\u00e4nen, IEEE Signal Processing Letters. 217O. R\u00e4s\u00e4nen et al., \"Modeling dependencies in multiple parallel data streams with hyperdimensional computing,\" IEEE Signal Processing Letters, vol. 21, no. 7, pp. 899-903, 2014.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, IEEE TNNLS. 99O. Rasanen et al., \"Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns,\" IEEE TNNLS, vol. PP, no. 99, pp. 1-12, 2015.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, DAC. ACM108M. Imani et al., \"Hierarchical hyperdimensional computing for energy efficient classification,\" in DAC, p. 108, ACM, 2018.\n\nHyperdimensional computing for noninvasive brain-computer interfaces: Blind and one-shot classification of eeg error-related potentials. A Rahimi, BICTA. Rahimi et al., \"Hyperdimensional computing for noninvasive brain-computer interfaces: Blind and one-shot classification of eeg error-related potentials,\" in BICT, 2017.\n\nCaltech-256 object category dataset. G Griffin, G. Griffin et al., \"Caltech-256 object category dataset,\" 2007.\n\nUci machine learning repository. \"Uci machine learning repository.\" http://archive.ics.uci.edu/ml/datasets/ISOLET.\n\nUci machine learning repository. \"Uci machine learning repository.\" https://archive.ics.uci.edu/ml/datasets/human+ activity+recognition+using+smartphones.\n\nUci machine learning repository. \"Uci machine learning repository.\" https://archive.ics.uci.edu/ml/datasets/ cardiotocography.\n\nScikit-learn: Machine learning in python. F Pedregosa, Journal of Machine Learning Research. 12F. Pedregosa et al., \"Scikit-learn: Machine learning in python,\" Journal of Machine Learning Research, vol. 12, no. Oct, pp. 2825-2830, 2011.\n", "annotations": {"author": "[{\"start\":\"86\",\"end\":\"177\"},{\"start\":\"178\",\"end\":\"271\"},{\"start\":\"272\",\"end\":\"362\"},{\"start\":\"363\",\"end\":\"449\"},{\"start\":\"450\",\"end\":\"553\"},{\"start\":\"554\",\"end\":\"645\"}]", "publisher": null, "author_last_name": "[{\"start\":\"93\",\"end\":\"98\"},{\"start\":\"185\",\"end\":\"191\"},{\"start\":\"279\",\"end\":\"284\"},{\"start\":\"369\",\"end\":\"372\"},{\"start\":\"459\",\"end\":\"469\"},{\"start\":\"561\",\"end\":\"567\"}]", "author_first_name": "[{\"start\":\"86\",\"end\":\"92\"},{\"start\":\"178\",\"end\":\"184\"},{\"start\":\"272\",\"end\":\"278\"},{\"start\":\"363\",\"end\":\"368\"},{\"start\":\"450\",\"end\":\"458\"},{\"start\":\"554\",\"end\":\"560\"}]", "author_affiliation": "[{\"start\":\"117\",\"end\":\"176\"},{\"start\":\"211\",\"end\":\"270\"},{\"start\":\"307\",\"end\":\"361\"},{\"start\":\"389\",\"end\":\"448\"},{\"start\":\"498\",\"end\":\"552\"},{\"start\":\"585\",\"end\":\"644\"}]", "title": "[{\"start\":\"1\",\"end\":\"83\"},{\"start\":\"646\",\"end\":\"728\"}]", "venue": null, "abstract": "[{\"start\":\"730\",\"end\":\"1978\"}]", "bib_ref": "[{\"start\":\"2164\",\"end\":\"2167\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"2775\",\"end\":\"2778\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"2801\",\"end\":\"2804\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"2806\",\"end\":\"2809\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"2852\",\"end\":\"2855\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"2857\",\"end\":\"2860\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"2881\",\"end\":\"2884\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"2915\",\"end\":\"2918\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"3037\",\"end\":\"3040\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3272\",\"end\":\"3275\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"5810\",\"end\":\"5813\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"6364\",\"end\":\"6367\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"9118\",\"end\":\"9122\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"13040\",\"end\":\"13044\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"15533\",\"end\":\"15537\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"15569\",\"end\":\"15573\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"15599\",\"end\":\"15602\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"15634\",\"end\":\"15638\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"19631\",\"end\":\"19635\",\"attributes\":{\"ref_id\":\"b12\"}}]", "figure": "[{\"start\":\"21691\",\"end\":\"21763\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"21764\",\"end\":\"21851\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"21852\",\"end\":\"21921\",\"attributes\":{\"id\":\"fig_2\"}},{\"start\":\"21922\",\"end\":\"22050\",\"attributes\":{\"id\":\"fig_3\"}},{\"start\":\"22051\",\"end\":\"22188\",\"attributes\":{\"id\":\"fig_4\"}},{\"start\":\"22189\",\"end\":\"22301\",\"attributes\":{\"id\":\"fig_5\"}},{\"start\":\"22302\",\"end\":\"23057\",\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"}},{\"start\":\"23058\",\"end\":\"23541\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"23542\",\"end\":\"24147\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"1997\",\"end\":\"3750\"},{\"start\":\"3752\",\"end\":\"4953\"},{\"start\":\"5000\",\"end\":\"5620\"},{\"start\":\"5622\",\"end\":\"6806\"},{\"start\":\"7066\",\"end\":\"7421\"},{\"start\":\"7450\",\"end\":\"9957\"},{\"start\":\"9959\",\"end\":\"10657\"},{\"start\":\"10691\",\"end\":\"10843\"},{\"start\":\"10845\",\"end\":\"15067\"},{\"start\":\"15109\",\"end\":\"15639\"},{\"start\":\"15678\",\"end\":\"16165\"},{\"start\":\"16167\",\"end\":\"17290\"},{\"start\":\"17292\",\"end\":\"19329\"},{\"start\":\"19398\",\"end\":\"20605\"},{\"start\":\"20607\",\"end\":\"20908\"},{\"start\":\"20927\",\"end\":\"21497\"},{\"start\":\"21521\",\"end\":\"21690\"}]", "formula": "[{\"start\":\"6807\",\"end\":\"6963\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"6963\",\"end\":\"7010\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"7010\",\"end\":\"7065\",\"attributes\":{\"id\":\"formula_2\"}}]", "table_ref": "[{\"start\":\"16655\",\"end\":\"16662\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"17310\",\"end\":\"17317\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"18461\",\"end\":\"18469\",\"attributes\":{\"ref_id\":\"tab_2\"}},{\"start\":\"19726\",\"end\":\"19735\",\"attributes\":{\"ref_id\":\"tab_2\"}}]", "section_header": "[{\"start\":\"1980\",\"end\":\"1995\"},{\"start\":\"4956\",\"end\":\"4998\"},{\"start\":\"7424\",\"end\":\"7448\"},{\"start\":\"10660\",\"end\":\"10689\"},{\"start\":\"15070\",\"end\":\"15107\"},{\"start\":\"15642\",\"end\":\"15676\"},{\"start\":\"19332\",\"end\":\"19349\"},{\"start\":\"19352\",\"end\":\"19396\"},{\"start\":\"20911\",\"end\":\"20925\"},{\"start\":\"21500\",\"end\":\"21519\"},{\"start\":\"21692\",\"end\":\"21700\"},{\"start\":\"21853\",\"end\":\"21861\"},{\"start\":\"21923\",\"end\":\"21939\"},{\"start\":\"22052\",\"end\":\"22060\"},{\"start\":\"22190\",\"end\":\"22198\"},{\"start\":\"22303\",\"end\":\"22314\"},{\"start\":\"23059\",\"end\":\"23076\"},{\"start\":\"23543\",\"end\":\"23561\"}]", "table": "[{\"start\":\"22398\",\"end\":\"23057\"},{\"start\":\"23178\",\"end\":\"23541\"},{\"start\":\"23649\",\"end\":\"24147\"}]", "figure_caption": "[{\"start\":\"21702\",\"end\":\"21763\"},{\"start\":\"21766\",\"end\":\"21851\"},{\"start\":\"21863\",\"end\":\"21921\"},{\"start\":\"21942\",\"end\":\"22050\"},{\"start\":\"22062\",\"end\":\"22188\"},{\"start\":\"22200\",\"end\":\"22301\"},{\"start\":\"22316\",\"end\":\"22398\"},{\"start\":\"23079\",\"end\":\"23178\"},{\"start\":\"23565\",\"end\":\"23649\"}]", "figure_ref": "[{\"start\":\"5133\",\"end\":\"5141\"},{\"start\":\"9008\",\"end\":\"9016\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"9701\",\"end\":\"9709\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"10384\",\"end\":\"10392\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"11599\",\"end\":\"11607\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"12096\",\"end\":\"12104\",\"attributes\":{\"ref_id\":\"fig_2\"}},{\"start\":\"12941\",\"end\":\"12949\"},{\"start\":\"14041\",\"end\":\"14049\"},{\"start\":\"14687\",\"end\":\"14695\"},{\"start\":\"16231\",\"end\":\"16239\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"16736\",\"end\":\"16744\",\"attributes\":{\"ref_id\":\"fig_4\"}},{\"start\":\"18065\",\"end\":\"18073\",\"attributes\":{\"ref_id\":\"fig_5\"}}]", "bib_author_first_name": "[{\"start\":\"24274\",\"end\":\"24275\"},{\"start\":\"24606\",\"end\":\"24607\"},{\"start\":\"24830\",\"end\":\"24831\"},{\"start\":\"25106\",\"end\":\"25107\"},{\"start\":\"25368\",\"end\":\"25369\"},{\"start\":\"25714\",\"end\":\"25715\"},{\"start\":\"26005\",\"end\":\"26006\"},{\"start\":\"26286\",\"end\":\"26287\"},{\"start\":\"26510\",\"end\":\"26511\"},{\"start\":\"27028\",\"end\":\"27029\"}]", "bib_author_last_name": "[{\"start\":\"24276\",\"end\":\"24283\"},{\"start\":\"24608\",\"end\":\"24615\"},{\"start\":\"24832\",\"end\":\"24837\"},{\"start\":\"25108\",\"end\":\"25114\"},{\"start\":\"25370\",\"end\":\"25377\"},{\"start\":\"25716\",\"end\":\"25723\"},{\"start\":\"26007\",\"end\":\"26012\"},{\"start\":\"26288\",\"end\":\"26294\"},{\"start\":\"26512\",\"end\":\"26519\"},{\"start\":\"27030\",\"end\":\"27039\"}]", "bib_entry": "[{\"start\":\"24149\",\"end\":\"24507\",\"attributes\":{\"matched_paper_id\":\"733980\",\"id\":\"b0\"}},{\"start\":\"24509\",\"end\":\"24787\",\"attributes\":{\"matched_paper_id\":\"7149851\",\"id\":\"b1\"}},{\"start\":\"24789\",\"end\":\"25014\",\"attributes\":{\"matched_paper_id\":\"39020350\",\"id\":\"b2\"}},{\"start\":\"25016\",\"end\":\"25277\",\"attributes\":{\"matched_paper_id\":\"9812826\",\"id\":\"b3\"}},{\"start\":\"25279\",\"end\":\"25590\",\"attributes\":{\"matched_paper_id\":\"1690456\",\"id\":\"b4\"}},{\"start\":\"25592\",\"end\":\"25926\",\"attributes\":{\"matched_paper_id\":\"15258913\",\"id\":\"b5\"}},{\"start\":\"25928\",\"end\":\"26147\",\"attributes\":{\"matched_paper_id\":\"49301394\",\"id\":\"b6\"}},{\"start\":\"26149\",\"end\":\"26471\",\"attributes\":{\"id\":\"b7\"}},{\"start\":\"26473\",\"end\":\"26584\",\"attributes\":{\"id\":\"b8\"}},{\"start\":\"26586\",\"end\":\"26700\",\"attributes\":{\"id\":\"b9\"}},{\"start\":\"26702\",\"end\":\"26856\",\"attributes\":{\"id\":\"b10\"}},{\"start\":\"26858\",\"end\":\"26984\",\"attributes\":{\"id\":\"b11\"}},{\"start\":\"26986\",\"end\":\"27222\",\"attributes\":{\"matched_paper_id\":\"10659969\",\"id\":\"b12\"}}]", "bib_title": "[{\"start\":\"24149\",\"end\":\"24272\"},{\"start\":\"24509\",\"end\":\"24604\"},{\"start\":\"24789\",\"end\":\"24828\"},{\"start\":\"25016\",\"end\":\"25104\"},{\"start\":\"25279\",\"end\":\"25366\"},{\"start\":\"25592\",\"end\":\"25712\"},{\"start\":\"25928\",\"end\":\"26003\"},{\"start\":\"26986\",\"end\":\"27026\"}]", "bib_author": "[{\"start\":\"24274\",\"end\":\"24285\"},{\"start\":\"24606\",\"end\":\"24617\"},{\"start\":\"24830\",\"end\":\"24839\"},{\"start\":\"25106\",\"end\":\"25116\"},{\"start\":\"25368\",\"end\":\"25379\"},{\"start\":\"25714\",\"end\":\"25725\"},{\"start\":\"26005\",\"end\":\"26014\"},{\"start\":\"26286\",\"end\":\"26296\"},{\"start\":\"26510\",\"end\":\"26521\"},{\"start\":\"27028\",\"end\":\"27041\"}]", "bib_venue": "[{\"start\":\"24285\",\"end\":\"24306\"},{\"start\":\"24617\",\"end\":\"24636\"},{\"start\":\"24839\",\"end\":\"24886\"},{\"start\":\"25116\",\"end\":\"25126\"},{\"start\":\"25379\",\"end\":\"25409\"},{\"start\":\"25725\",\"end\":\"25735\"},{\"start\":\"26014\",\"end\":\"26017\"},{\"start\":\"26149\",\"end\":\"26284\"},{\"start\":\"26473\",\"end\":\"26508\"},{\"start\":\"26586\",\"end\":\"26617\"},{\"start\":\"26702\",\"end\":\"26733\"},{\"start\":\"26858\",\"end\":\"26889\"},{\"start\":\"27041\",\"end\":\"27077\"}]"}}}, "year": 2023, "month": 12, "day": 17}