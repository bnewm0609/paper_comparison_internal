{"id": 247996703, "updated": "2023-10-05 15:46:53.943", "metadata": {"title": "Text2LIVE: Text-Driven Layered Image and Video Editing", "authors": "[{\"first\":\"Omer\",\"last\":\"Bar-Tal\",\"middle\":[]},{\"first\":\"Dolev\",\"last\":\"Ofri-Amar\",\"middle\":[]},{\"first\":\"Rafail\",\"last\":\"Fridman\",\"middle\":[]},{\"first\":\"Yoni\",\"last\":\"Kasten\",\"middle\":[]},{\"first\":\"Tali\",\"last\":\"Dekel\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with visual effects (e.g., smoke, fire) in a semantically meaningful manner. We train a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. We demonstrate localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.02491", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/Bar-TalOFKD22", "doi": "10.1007/978-3-031-19784-0_41"}}, "content": {"source": {"pdf_hash": "d7028c804d0dcd382d8cfcdae436268b61a457c2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.02491v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2ae7baf8f60bba8864532700e54050ce0fcbddd9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d7028c804d0dcd382d8cfcdae436268b61a457c2.txt", "contents": "\nText2LIVE: Text-Driven Layered Image and Video Editing (a) Image text-guided layered editing (b) Video text-guided layered editing\n\n\nOmer Bar-Tal \nWeizmann Institute of Science\n\n\nDolev Ofri-Amar \nWeizmann Institute of Science\n\n\nRafail Fridman \nWeizmann Institute of Science\n\n\nYoni Kasten \nNVIDIA Research\n\n\nTali Dekel \nWeizmann Institute of Science\n\n\nText2LIVE: Text-Driven Layered Image and Video Editing (a) Image text-guided layered editing (b) Video text-guided layered editing\n2 O. Bar-Tal, D. Ofri-Amar , R. Fridman et al.text-guided image and video editingappearance editingCLIP * Denotes equal contribution\nb) Image editing results (c) Video editing results \"smoke\" \"smoking cigar\" \"spinach moss cake\" \"ice\"\"gira e with a neck warmer\" \"colorful stained glass gira e\" \"brioche\" \"oreo cake\"Input image Input image Input videoText2LIVE(a) Our text to layer approachFig. 1. Text2LIVE performs semantic, localized edits to real-world images (b), or videos (c). Our key idea is to generate an edit layer -RGBA image representing the target edit when composited over the original input (a). This allows us to use text to guide not only the final composite, but also the edit layer itself (target text prompts are shown above each image). Our edit layers are synthesized by training a generator on a single input, without relying on user-provided masks or a pre-trained generator.Abstract. We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with visual effects (e.g., smoke, fire) in a semantically meaningful manner. We train a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. We demonstrate localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes. Project page: https://text2live.github.io/\n\n(b) Image editing results (c) Video editing results\n\n\"smoke\" \"smoking cigar\" \"spinach moss cake\" \"ice\"\n\n\"gira e with a neck warmer\" \"colorful stained glass gira e\" \"brioche\" \"oreo cake\"\n\n\nInput image Input image Input video\n\n\nText2LIVE\n\n(a) Our text to layer approach Fig. 1. Text2LIVE performs semantic, localized edits to real-world images (b), or videos (c). Our key idea is to generate an edit layer -RGBA image representing the target edit when composited over the original input (a). This allows us to use text to guide not only the final composite, but also the edit layer itself (target text prompts are shown above each image). Our edit layers are synthesized by training a generator on a single input, without relying on user-provided masks or a pre-trained generator.\n\nAbstract. We present a method for zero-shot, text-driven appearance manipulation in natural images and videos. Given an input image or video and a target text prompt, our goal is to edit the appearance of existing objects (e.g., object's texture) or augment the scene with visual effects (e.g., smoke, fire) in a semantically meaningful manner. We train a generator using an internal dataset of training examples, extracted from a single input (image or video and target text prompt), while leveraging an external pre-trained CLIP model to establish our losses. Rather than directly generating the edited output, our key idea is to generate an edit layer (color+opacity) that is composited over the original input. This allows us to constrain the generation process and maintain high fidelity to the original input via novel text-driven losses that are applied directly to the edit layer. Our method neither relies on a pre-trained generator nor requires user-provided edit masks. We demonstrate localized, semantic edits on high-resolution natural images and videos across a variety of objects and scenes. Project page: https://text2live.github.io/ Keywords: text-guided image and video editing, appearance editing, CLIP (a) Image text-guided layered editing (b) Video text-guided layered editing \"fire out of bear's mouth\" \"fire\" \"latte art heart pattern\" \"monarch butterfly\" \"orca\" \"snow\" \"latte art heart pattern\" \"monarch butterfly\" \"orca\" \"snowy countryside scene\" \n\n\nInput\n\n\nIntroduction\n\nComputational methods for manipulating the appearance and style of objects in natural images and videos have seen tremendous progress, facilitating a variety of editing effects to be achieved by novice users. Nevertheless, research in this area has been mostly focused in the Style-Transfer setting where the target appearance is given by a reference image (or domain of images), and the original image is edited in a global manner [16]. Controlling the localization of the edits typically involves additional input guidance such as segmentation masks. Thus, appearance transfer has been mostly restricted to global artistic stylization or to specific image domains or styles (e.g., faces, day-to-night, summer-to-winter). In this work, we seek to eliminate these requirements and enable more flexible and creative semantic appearance manipulation of real-world images and videos. Inspired by the unprecedented power of recent Vision-Language models, we use simple text prompts to express the target edit. This allows the user to easily and intuitively specify the target appearance and the object/region to be edited. Specifically, our method enables local, semantic editing that satisfies a given target text prompt (e.g., Fig. 1 and Fig. 2). For example, given the cake image in Fig. 1(b), and the target text: \"oreo cake\", our method automatically locates the cake region and synthesizes realistic, high-quality texture that combines naturally with the original image -the cream filling and the cookie crumbs \"paint\" the full cake and the sliced piece in a semantically-aware manner. As seen, these properties hold across a variety of different edits.\n\nOur framework leverages the representation learned by a Contrastive Language-Image Pretraining (CLIP) model, which has been pre-trained on 400 million text-image examples [35]. The richness of the enormous visual and textual space spanned by CLIP has been demonstrated by various recent image editing methods (e.g., [2,3,11,12,33]). However, the task of editing existing objects in arbi-trary, real-world images remains challenging. Most existing methods combine a pre-trained generator (e.g., a GAN or a Diffusion model) in conjunction with CLIP. With GANs, the domain of images is restricted and requires to invert the input image to the GAN's latent space -a challenging task by itself [49]. Diffusion models [13,45] overcome these barriers but face an inherent trade-off between satisfying the target edit and maintaining high-fidelity to the original content [2]. Furthermore, it is not straightforward to extend these methods to videos. In this work, we take a different route and propose to learn a generator from a single input-image or video and text prompts.\n\nIf no external generative prior is used, how can we steer the generation towards meaningful, high-quality edits? We achieve this via the following two key components: (i) we propose a novel text-guided layered editing, i.e., rather than directly generating the edited image, we represent the edit via an RGBA layer (color and opacity) that is composited over the input. This allows us to guide the content and localization of the generated edit via a novel objective function, including text-driven losses applied directly to the edit layer. For example, as seen in Fig. 2, we use text prompts to express not only the final edited image but also a target effect (e.g., fire) represented by the edit layer. (ii) We train our generator on an internal dataset of diverse image-text training examples by applying various augmentations to the input image and text. We demonstrate that our internal learning approach serves as a strong regularization, enabling high quality generation of complex textures and semi-transparent effects.\n\nWe further take our framework to the realm of text-guided video editing. Realworld videos often consist of complex object and camera motion, which provide abundant information about the scene. Nevertheless, achieving consistent video editing is difficult and cannot be accomplished na\u00efvely. We thus propose to decompose the video into a set of 2D atlases using [18]. Each atlas can be treated as a unified 2D image representing either a foreground object or the background throughout the video. This representation significantly simplifies the task of video editing: edits applied to a single 2D atlas are automatically mapped back to the entire video in a consistent manner. We demonstrate how to extend our framework to perform edits in the atlas space while harnessing the rich information readily available in videos.\n\nIn summary, we present the following contributions:\n\n-An end-to-end text-guided framework for performing localized, semantic edits of existing objects in real-world images. -A novel layered editing approach and objective function that automatically guides the content and localization of the generated edit. -We demonstrate the effectiveness of internal learning for training a generator on a single input in a zero-shot manner. -An extension to video which harnesses the richness of information across time, and can perform consistent text-guided editing. -We demonstrate various edits, ranging from changing objects' texture to generating complex semi-transparent effects, all achieved fully automatically across a wide-range of objects and scenes.\n\n\nRelated Work\n\nText-guided image manipulation and synthesis. There has been remarkable progress since the use of conditional GANs in both text-guided image generation [38,50,51,52], and editing [9,22,29]. ManiGAN [22] proposed a textconditioned GAN for editing an object's appearance while preserving the image content. However, such multi-modal GAN-based methods are restricted to specific image domains and limited in the expressiveness of the text (e.g., trained on COCO [24]). DALL-E [36] addresses this by learning a joint image-text distribution over a massive dataset. While achieving remarkable text-to-image generation, DALL-E is not designed for editing existing images. GLIDE [30] takes this approach further, supporting both text-to-image generation and inpainting. Instead of directly training a text-to-image generator, a recent surge of methods leverage a pre-trained generator, and use a pre-trained CLIP [35] to guide the generation process by text [3,12,25,33]. StyleCLIP [33] and StyleGAN-NADA [12] use a pre-trained StyleGAN2 [17] for image manipulation, by either controlling the GAN's latent code [33], or by fine-tuning the StyleGAN's output domain [12]. However, editing a real input image using these methods requires first tackling the GAN-inversion challenge [39,47]. Furthermore, these methods can edit images from a few specific domains, and edit images in a global fashion. In contrast, we consider a different problem setting -localized edits that can be applied to real-world images spanning a variety of object and scene categories.\n\nA recent exploratory and artistic trend in the online AI community has demonstrated impressive text-guided image generation. CLIP is used to guide the generation process of a pre-trained generator, e.g., VQ-GAN [10], or diffusion models [13,45]. [19] takes this approach a step forward by optimizing the diffusion process itself. However, since the generation is globally controlled by the diffusion process, this method is not designed to support localized edits that are applied only to selected objects.\n\nTo enable region-based editing, user-provided masks are used to control the diffusion process for image inpainting [2]. In contrast, our goal is not to generate new objects but rather to manipulate the appearance of existing ones, while preserving the original content. Furthermore, our method is fully automatic and performs the edits directly from the text, without user edit masks.\n\nSeveral works [11,14,21,28] take a test-time optimization approach and leverage CLIP without using a pre-trained generator. For example, CLIPDraw [11] renders a drawing that matches a target text by directly optimizing a set of vector strokes. To prevent adversarial solutions, various augmentations are applied to the output image, all of which are required to align with the target text in CLIP embedding space. CLIPStyler [21] takes a similar approach for global stylization. Our goal is to perform localized edits, which are applied only to specific objects. Furthermore, CLIPStyler optimizes a CNN that observes only the source image. In contrast, our generator is trained on an internal dataset, extracted from the input image and text. We draw inspiration from previous works that show the effectiveness of internal learning in the context of generation [42,44,48]. Right: Our generator takes as input an image and outputs an edit RGBA layer (color+opacity), which is composited over the input to form the final edited image. The generator is trained by minimizing several loss terms that are defined in CLIP space, and include: Lcomp, applied to the composite, and Lscreen, applied to the edit layer (when composited over a green background). We apply additional augmentations before CLIP (Sec. 3.1)\n\nOther works use CLIP to synthesize [14] or edit [28] a single 3D representation (NeRF or mesh). The unified 3D representation is optimized through a differentiable renderer: CLIP loss is applied across different 2D rendered viewpoints. Inspired by this approach, we use a similar concept to edit videos. In our case, the \"renderer\" is a layered neural atlas representation of the video [18].\n\nConsistent Video Editing. Existing approaches for consistent video editing can be roughly divided into: (i) propagation-based methods, which use keyframes [15,46] or optical flow [41] to propagate edits through the video, and (ii) video layering-based methods, in which a layered representation of the video is estimated and then edited [18,23,26,27,37]. For example, Lu et al. [26,27] estimate omnimattes -RGBA layers that contain a target subject along with their associated scene effects. Omnimattes facilitate a variety of video effects (e.g., object removal or retiming). However, since the layers are computed independently for each frame, it cannot support consistent propagation of edits across time. Kasten et al. [18] address this challenge by decomposing the video into unified 2D atlas layers (foreground and background). Edits applied to the 2D atlases are automatically mapped back to the video, thus achieving temporal consistency with minimal effort. In our work, we treat a pre-trained neural layered atlas model as a video renderer and leverage it for the task of text-guided video editing.\n\n\nText-Guided Layered Image and Video Editing\n\nWe focus on semantic, localized edits expressed by simple text prompts. Such edits include changing objects' texture or semantically augmenting the scene with complex semi-transparent effects (e.g., smoke, fire). To this end, we harness the potential of learning a generator from a single input image or video while leveraging a pre-trained CLIP model, which is kept fixed and used to establish our losses [35]. Our task is ill-posed -numerous possible edits can satisfy the target text according to CLIP, some of which include noisy or undesired solutions [11,25]. Thus, controlling edits' localization and preserving the original content are both pivotal components for achieving high-quality editing results. We tackle these challenges through the following key components:\n\n1. Layered editing. Our generator outputs an RGBA layer that is composited over the input image. This allows us to control the content and spatial extent of the edit via dedicated losses applied directly to the edit layer. 2. Explicit content preservation and localization losses. We devise new losses using the internal spatial features in CLIP space to preserve the original content, and to guide the localization of the edits. 3. Internal generative prior. We construct an internal dataset of examples by applying augmentations to the input image/video and text. These augmented examples are used to train our generator, whose task is to perform text-guided editing on a larger and more diverse set of examples.\n\n\nText to Image Edit Layer\n\nAs illustrated in Fig. 3, our framework consists of a generator G \u03b8 that takes as input a source image I s and synthesizes an edit layer, E = {C, \u03b1}, which consists of a color image C and an opacity map \u03b1. The final edited image I o is given by compositing the edit layer over I s :\nIo = \u03b1 \u00b7 C + (1 \u2212 \u03b1) \u00b7 Is (1)\nOur main goal is to generate E such that the final composite I o would comply with a target text prompt T . In addition, generating an RGBA layer allows us to use text to further guide the generated content and its localization. To this end, we consider a couple of auxiliary text prompts: T screen which expresses the target edit layer, when composited over a green background, and T ROI which specifies a region-of-interest in the source image, and is used to initialize the localization of the edit. For example, in the Bear edit in Fig. 2, T =\"fire out of the bear's mouth\", T screen =\"fire over a green screen\", and T ROI =\"mouth\". We next describe in detail how these are used in our objective function.\n\nObjective function. Our novel objective function incorporates three main loss terms, all defined in CLIP's feature space: (i) L comp , which is the driving loss and encourages I o to conform with T , (ii) L screen , which serves as a direct supervision on the edit layer, and (iii) L structure , a structure preservation loss w.r.t. I s . Additionally, a regularization term L reg is used for controlling the extent of the edit by encouraging sparse alpha matte \u03b1. Formally,\nL Text2LIVE = Lcomp + \u03bbgLscreen + \u03bbsLstructure + \u03bbrLreg,(2)\nwhere \u03bb g , \u03bb s , and \u03bb r control the relative weights between the terms, and are fixed throughout all our experiments (see Appendix A.3).\n\nComposition loss. L comp reflects our primary objective of generating an image that matches the target text prompt and is given by a combination of a cosine distance loss and a directional loss [33]:\nLcomp = Lcos (Io, T ) + L dir (Is, Io, T ROI , T ),(3)\nwhere Lcos = Dcos (Eim(Io), Etxt(T )) is the cosine distance between the CLIP embeddings for I o and T . Here, Eim, Etxt denote CLIP's image and text encoders, respectively. The second term controls the direction of edit in CLIP space [12,33] and is given by\n: L dir = Dcos(Eim(Io)\u2212Eim(Is), Etxt(T ) \u2212 Etxt(T ROI )) .\nSimilar to most CLIP-based editing methods, we first augment each image to get several different views and calculate the CLIP losses w.r.t. each of them separately, as in [2]. This holds for all our CLIP-based losses. See Appendix A.2 for details.\n\nScreen loss. The term L screen serves as a direct text supervision on the generated edit layer E. We draw inspiration from chroma keying [4]-a well-known technique by which a solid background (often green) is replaced by an image in a post-process. Chroma keying is extensively used in image and video postproduction, and there is high prevalence of online images depicting various visual elements over a green background. We thus composite the edit layer over a green background I green and encourage it to match the text-template T screen := \" { } over a green screen\", ( where I screen = \u03b1 \u00b7 C + (1 \u2212 \u03b1) \u00b7 I green .\n\nA nice property of this loss is that it allows intuitive supervision on a desired effect. For example, when generating semi-transparent effects, e.g., Bear in Fig. 2, we can use this loss to focus on the fire regardless of the image content by using T screen =\"fire over a green screen\". Unless specified otherwise, we plug in T to our screen text template in all our experiments. Similar to the composition loss, we first apply augmentations on the images before feeding to CLIP.\n\nStructure loss. We want to allow substantial texture and appearance changes while preserving the objects' original spatial layout, shape, and perceived semantics. While various perceptual content losses have been proposed in the context of style transfer, most of them use features extracted from a pre-trained VGG model. Instead, we define our loss in CLIP feature space. This allows us to impose additional constraints to the resulting internal CLIP representation of I o . Inspired by classical and recent works [20,43,48], we adopt the self-similarity measure. Specifically, we feed an image into CLIP's ViT encoder and extract its K spatial tokens from the deepest layer. The self-similarity matrix, denoted by S(I) \u2208 R K\u00d7K , is used as structure representation. Each matrix element S(I) ij is defined by:\nS(I)ij = 1 \u2212 Dcos t i (I), t j (I)(5)\nwhere t i (I) \u2208 R 768 is the i th token of image I. The term L structure is defined as the Frobenius norm distance between the self-similarity matrices of I s , and I o :\nLstructure = \u2225S(Is) \u2212 S(Io)\u2225 F(6)\nSparsity regularization. To control the spatial extent of the edit, we encourage the output opacity map to be sparse. We follow [26,27] and define the sparsity loss term as a combination of L 1 -and L 0 -approximation regularization terms:\nLreg = \u03b3 \u2225\u03b1\u2225 1 + \u03a80(\u03b1)(7)\nwhere \u03a8 0 (x) \u2261 2Sigmoid(5x) \u2212 1 is a smooth L 0 approximation that penalizes non zero elements. We fix \u03b3 in all our experiments.\n\nBootstrapping. To achieve accurate localized effects without user-provided edit mask, we apply a text-driven relevancy loss to initialize our opacity map. Specifically, we use Chefer et al. [6] to automatically estimate a relevancy map 1 R(I s ) \u2208 [0, 1] 224\u00d7224 which roughly highlights the image regions that are most relevant to a given text T ROI . We use the relevancy map to initialize \u03b1 by minimizing:\nL init = MSE (R(Is), \u03b1)(8)\nNote that the relevancy maps are noisy, and only provide a rough estimation for the region of interest (Fig. 8(c)). Thus, we anneal this loss during training (see implementation details in Appendix A.3). By training on diverse internal examples along with the rest of our losses, our framework dramatically refines this rough initialization, and produces accurate and clean opacity ( Fig. 8(d)).\n\nTraining data. Our generator is trained from scratch for each input (I s , T )\nusing an internal dataset of diverse image-text training examples {(I i s , T i )} N i=1\nthat are derived from the input (Fig. 3 left). Specifically, each training example (I i s , T i ) is generated by randomly applying a set of augmentations to I s and to T . The image augmentations include global crops, color jittering, and flip, while text augmentations are randomly sampled from a predefined text template (e.g., \"a photo of \"+T ); see Appendix A.2 for details. The vast space of all combinations between these augmentations provides us with a rich and diverse dataset for training. The task is now to learn one mapping function G \u03b8 for the entire dataset, which poses a strong regularization on the task. Specifically, for each individual example, G \u03b8 has to generate a plausible edit layer E i from I i s such that the composited image is well described by T i . We demonstrate the effectiveness of our internal learning approach compared to the test-time optimization approach in Sec. 4.\n\n\nText to Video Edit Layer\n\nA natural question is whether our image framework can be applied to videos.\n\nThe key additional challenge is achieving a temporally consistent result. Na\u00efvely applying our image framework on each frame independently yields unsatisfactory jittery results (see Sec. 4). To enforce temporal consistency, we utilize the Neural Layered Atlases (NLA) method [18], as illustrated in Fig. 4(a). We next provide a brief review of NLA and discuss in detail how our extension to videos. can be treated as a 2D image, representing either one foreground object or the background throughout the entire video. An example of foreground and background atlases are shown in Fig. 4. For each video location p = (x, y, t), NLA computes a corresponding 2D location (UV) in each atlas, and a foreground opacity value. This allows to reconstruct the original video from the set atlases. NLA comprises of several Multi-Layered Perceptrons (MLPs), representing the atlases, the mappings from pixels to atlases and their opacity. More specifically, each video location p is first fed into two mapping networks, M b and M f :\nM b (p) = (u p b , v p b ), M f (p) = (u p f , v p f )(9)\nwhere (u p * , v p * ) are the 2D coordinates in the background/foreground atlas space. Each pixel is also fed to an MLP that predicts the opacity value of the foreground in each position. The predicted UV coordinates are then fed into an atlas network A, which outputs the RGB colors in each location. Thus, the original RGB value of p can be reconstructed by mapping p to the atlases, extracting the corresponding atlas colors, and blending them according to the predicted opacity. We refer the reader to [18] for full details.\n\nImportantly, NLA enables consistent video editing: the continuous atlas (foreground or background) is first discretized to a fixed resolution image (e.g., 1000\u00d71000 px). The user can directly edit the discretized atlas using image editing tools (e.g., Photoshop). The atlas edit is then mapped back to the video, and blended with the original frames, using the predicted UV mappings and foreground opacity. In this work, we are interested in generating atlas edits in a fully automatic manner, solely guided by text.\n\nText to Atlas Edit Layer. Our video framework leverages NLA as a \"video renderer\", as illustrated in Fig. 4. Specifically, given a pre-trained and fixed NLA model for a video, our goal is to generate a 2D atlas edit layer, either for the background or foreground, such that when mapped back to the video, each of the rendered frames would comply with the target text.\n\nSimilar to the image framework, we train a generator G \u03b8 that takes a 2D atlas as input and generates an atlas edit layer E A = {C A , \u03b1 A }. Note that since\n\n\n(a) Input image (b) Editing results (RGBA edit layer composited over the input image)\n\n\"wooden\" \"golden\" \"stained glass\" \"crochet\" \"brioche\" \"red velvet\"\n\n\"ice\" \"melted cheese\" \"snow\" \"volcano\" \"ocean\" \"sahara\" G \u03b8 is a CNN, we work with a discretized atlas, denoted as I A . The pre-trained UV mapping, denoted by M, is used to bilinearly sample E A to map it to each frame:\nEt = Sampler(EA, S)(10)\nwhere S = {M(p) | p = (\u00b7, \u00b7, t)} is the set of UV coordinates that correspond to frame t. The final edited video is obtained by blending E t with the original frames, following the same process as done in [18].\n\nTraining. A straightforward approach for training G \u03b8 is to treat I A as an image and plug it into our image framework (Sec. 3.1). This approach will result in a temporally consistent result, yet it has two main drawbacks: (i) the atlas often non-uniformly distorts the original structures (see Fig. 4), which may lead to low-quality edits , (ii) solely using the atlas, while ignoring the video frames, disregards the abundant, diverse information available in the video such as different viewpoints, or non-rigid object deformations, which can serve as \"natural augmentations\" to our generator. We overcome these drawbacks by mapping the atlas edit back to the video and applying our losses on the resulting edited frames. Similar to the image case, we use the same objective function (Eq. 2), and construct an internal dataset directly from the atlas for training.  More specifically, a training example is constructed by first extracting a crop from I A . To ensure we sample informative atlas regions, we first randomly crop a video segment in both space and time, and then map it to a corresponding atlas crop I Ac using M (see Appendix A.4 for full technical details). We then apply additional augmentations to I Ac and feed it into the generator, resulting in an edit layer E Ac = G \u03b8 (I Ac ). We then map E Ac and I Ac back to the video, resulting in frame edit layer E t , and a reconstructed foreground/background crop I t . This is done by bilinearly sampling E Ac and I Ac using Eq. (10), with S as the set of UV coordinates corresponding to the frame crop. Finally, we apply L Text2LIVE from Eq. 2, where I s = I t and E = E t . Fig. 8. Top: We illustrate the effect of our relevancy-based bootstrapping for image (a) using \"red hat\" as the target edit. (b) w/o bootstrapping our edited image suffers from color bleeding. When initializing our alpha-matte to capture the hat (T ROI =\"hat\"), an accurate matting is achieved (d-e). Notably, the raw relevancy map provides very rough supervision (c); during training, our method dramatically refines it (d). Bottom: We ablate each of our loss terms and the effect of internal learning (\"mango\" to \"golden mango\"). See Sec. 4.4.\n\n\n(a) Input image (c) Relevancy map (b) w/o bootstrapping (e) w/ bootstrapping (d) Our output matte (a) Input image (c) w/o structure (b) w/o sparsity (e) w/o screen (e) full objective (e) w/o internal dataset\n\n\nResults\n\n\nQualitative evaluation\n\nWe tested our method across various real-world, high-resolution images and videos. The image set contains 35 images collected from the web, spanning various object categories, including animals, food, landscapes and others. The video set contains seven videos from DAVIS dataset [34]. We applied our method using various target edits, ranging from text prompts that describe the texture/materials of specific objects, to edits that express complex scene effects such as smoke, fire, or clouds. Sample examples for the inputs along with our results can be seen in Fig. 1, Fig. 2 As can be seen, in all examples, our method successfully generates photorealistic textures that are \"painted\" over the target objects in a semantically aware manner. For example, in red velvet edit (first row in Fig. 5), the frosting is naturally placed on the top. In car-turn example (Fig. 6), the neon lights nicely follow the car's framing. In all examples, the edits are accurately localized, even under partial occlusions, multiple objects (last row and third row of Fig. 5) and complex scene composition (the dog in Fig. 2). Our method successfully augments the input scene with complex semi-transparent effects without changing irrelevant content in the image (see Fig. 1).\n\n\nComparison to Prior Work\n\nTo the best of our knowledge, there is no existing method tailored for solving our task: text-driven semantic, localized editing of existing objects in real-world images and videos. We illustrate the key differences between our method and several prominent text-driven image editing methods. We consider those that can be applied to a similar setting to ours: editing real-world images that are not restricted to specific domains. Inpainting methods: Blended-Diffusion [2] and GLIDE [30], both require user-provided editing mask. CLIPStyler, which performs image stylization, and Diffusion+CLIP [1], and VQ-GAN+CLIP [7]: two baselines that combine CLIP with either a pre-trained VQ-GAN or a Diffusion model. In the SM, we also include additional qualitative comparison to the StyleGAN text-guided editing methods [33,12]. Fig. 7 shows representative results, and the rest are included in the SM. As can be seen, none of these methods are designed for our task. The inpainting methods (b-c), even when supplied with tight edit masks, generate new content in the masked region rather than changing the texture of the existing one. CLIP-Styler modifies the image in a global artistic manner, rather than performing local semantic editing (e.g., the background in both examples is entirely changed, regardless of the image content). For the baselines (d-f), Diffusion+CLIP [1] can often synthesize high-quality images, but with either low-fidelity to the target text (e), or with low-fidelity to the input image content (see many examples in SM). VQ-GAN+CLIP [7] fails to maintain fidelity to the input image and produces non-realistic images (f). Our method automatically locates the cake region and generates high-quality texture that naturally combines with the original content.\n\n\nQuantitative evaluation\n\nComparison to image baselines. We conduct an extensive human perceptual evaluation on Amazon Mechanical Turk (AMT). We adopt the Two-alternative Forced Choice (2AFC) protocol suggested in [20,31]. Participants are shown a reference image and a target editing prompt, along with two alternatives: our result and another baseline result. We consider from the above baselines those not requiring user-masks. The participants are asked: \"Which image better shows objects in the reference image edited according to the text\". We perform the survey using a total of 82 image-text combinations. We collected 12,450 user judgments w.r.t. prominent text-guided image editing methods. Table 1 reports the percentage of votes in our favor. As seen, our method outperforms all baselines by a large margin, including those using a strong generative prior.\n\nComparison to video baselines. We quantify the effectiveness of our key design choices for the video-editing by comparing our video method against: (i) Atlas Baseline: feeding the discretized 2D Atlas to our single-image method (Sec. 3.1), and using the same inference pipeline illustrated in Fig. 4 to map the edited atlas back to frames. (ii) Frames Baseline: treating all video frames as part of a single internal dataset, used to train our generator; at inference, we apply the trained generator independently to each frame.\n\nWe conduct a human perceptual evaluation in which we provide participants a target editing prompt and two video alternatives: our result and a baseline. The participants are asked \"Choose the video that has better quality and better\n\n\nImage baselines\n\nVideo baselines CLIPStyler VQ-GAN+CLIP Diffusion+CLIP Atlas baseline Frames baseline 0.85 \u00b1 0.12 0.86 \u00b1 0.14 0.82 \u00b1 0.11 0.73 \u00b1 0.14 0.74 \u00b1 0.15 Table 1. AMT surveys evaluation (see Sec. 4). We compare to prominent (maskfree) image baselines (left), and demonstrate the effectiveness of our design choices in the video framework compared to alternatives (right). We report the percentage of judgments in our favor (mean, std). Our method outperforms all baselines.\n\n\"moon\" \"a bright full moon\" Input Image \"chess cake\" \"birthday cake\" Input Image Fig. 9. Limitations. CLIP often exhibit strong association between text and certain visual elements such as the shape of objects (e.g., \"moon\" with crescent shape), or additional new objects (e.g., \"birthday cake\" with candles). As our method is designed to edit existing objects, generating new ones may not lead to a visually pleasing result. However, often the desired edit can be achieved by using more specific text (left).\n\nrepresents the text\". We collected 2,400 user judgments over 19 video-text combinations and report the percentage of votes in favor of the complete model in table 1. We first note that the Frames baseline produces temporally inconsistent edits. As expected, the Atlas baseline produces temporally consistent results. However, it struggles to generate high-quality textures and often produces blurry results. These observations support our hypotheses mentioned in Sec. 3.2. We refer the reader to the SM for visual comparisons. Fig. 8(top) illustrates the effect of our relevancy-based bootstrapping (Sec. 3.1). As seen, this component allows us to achieve accurate object mattes, which significantly improves the rough, inaccurate relevancy maps.\n\n\nAblation Study\n\nWe ablate the different loss terms in our objective by qualitatively comparing our results when training with our full objective (Eq. 2) and with a specific loss removed. The results are shown in Fig. 8. As can be seen, without L reg (w/o sparsity), the output matte does not accurately capture the mango, resulting in a global color shift around it. Without L structure (w/o structure), the model outputs an image with the desired appearance but fails to preserve the mango shape fully. Without L screen (w/o screen), the segmentation of the object is noisy (color bleeding from the mango), and the overall quality of the texture is degraded (see SM for additional illustration). Lastly, we consider a test-time optimization baseline by not using our internal dataset but rather inputting to G \u03b8 the same input at each training step. As seen, this baseline results in lower-quality edits.\n\n\nLimitations\n\nWe noticed that for some edits, CLIP exhibits a very strong bias towards a specific solution. For example, as seen in Fig. 9, given an image of a cake, the text \"birthday cake\" is strongly associated with candles. Our method is not designed to significantly deviate from the input image layout and to create new objects, and generates unrealistic candles. Nevertheless, in many cases the desired edit can be achieved by using more specific text. For example, the text \"moon\" guides the generation towards a crescent. By using the text \"a bright full moon\" we can steer the generation towards a full moon (Fig. 9 left). Finally, as acknowledged by prior works (e.g., [28]), we also noticed that slightly different text prompts describing similar concepts may lead to slightly different flavors of edits.\n\nOn the video side, our method assumes that the pre-trained NLA model accurately represents the original video. Thus, we are restricted to examples where NLA works well, as artifacts in the atlas representation can propagate to our edited video. An exciting avenue of future research may include fine-tuning the NLA representation jointly with our model.\n\n\nConclusion\n\nWe considered a new problem setting in the context of zero-shot text-guided editing: semantic, localized editing of existing objects within real-world images and videos. Addressing this task requires careful control of several aspects of the editing: the edit localization, the preservation of the original content, and visual quality. We proposed to generate text-driven edit layers that allow us to tackle these challenges, without using a pre-trained generator in the loop. We further demonstrated how to adopt our image framework, with only minimal changes, to perform consistent text-guided video editing. We believe that the key principles exhibited in the paper hold promise for leveraging large-scale multi-modal networks in tandem with an internal learning approach.\n\n\nAcknowledgments\n\nWe thank Kfir Aberman, Lior Yariv, Shai Bagon, and Narek Tumanayan for their insightful comments. We thank Narek Tumanayan for his help with the baselines comparison. This project received funding from the Israeli Science Foundation (grant 2303/20).\n\n\nA Implementation Details\n\nWe provide implementation details for our architecture and training regime.\n\n\nA.1 Generator Network Architecture\n\nWe base our generator G \u03b8 network on the U-Net architecture [40], with a 7-layer encoder and a symmetrical decoder. All layers comprise 3\u00d73 Convolutional layers, followed by BatchNorm, and LeakyReLU activation. The intermediate channels dimensions is 128. In each level of the encoder, we add an additional 1\u00d71 Convolutional layer and concatenate the output features to the corresponding level of the decoder. Lastly, we add a 1\u00d71 Convolutional layer followed by Sigmoid activation to get the final RGB output.\n\n\nA.2 Internal Dataset (Sec. 3.1)\n\nWe apply data augmentations to the source image and target text (I s , T ) to cre-\nate multiple internal examples {(I i s , T i )} N i=1\n. Specifically, at each training step, we apply a random set of image augmentations to I s , and augment T using a pre-defined set of text templates, as follows: Text augmentations and the target text prompt T We compose T with a random text template, sampled from of a pre-defined list of 14 templates. We designed our text-templates that does not change the semantics of the prompt, yet provide variability in the resulting CLIP embedding e.g.: At each step, one of the above templates is chosen at random and the target text prompt T is plugged in to it and forms our augmented text. By default, our framework uses a single text prompt T , but can also support multiple input text prompts describing the same edit, which effectively serve as additional text augmentations (e.g., \"crochet swan\", and \"knitted swan\" can both be used to describe the same edit).\n\n\nA.3 Training Details\n\nWe implement our framework in PyTorch [32] (code will be made available). As described in Sec. 3, we leverage a pre-trained CLIP model [35] to establish our losses. We use the ViT-B/32 pretrained model (12 layers, 32x32 patches), downloaded from the official implementation at GitHub. We optimize our full objective (Eq. 2, Sec. 3.1), with relative weights: \u03bb g = 1, \u03bb s = 2 (3 for videos), \u03bb r = 5 \u00b7 10 \u22122 , (5 \u00b7 10 \u22124 for videos) and \u03b3 = 2. For bootstrapping, we set the relative weight to be 10, and for the image framework we anneal it linearly throughout the training. We use the MADGRAD optimizer [8] with an initial learning rate of 2.5\u00b710 \u22123 , weight decay of 0.01 and momentum 0.9. We decay the learning rate with an exponential learning rate scheduler with gamma = 0.99 (gamma = 0.999 for videos), limiting the learning rate to be no less than 10 \u22125 . Each batch contains (I i s , T i ) (see Sec. 3.1), the augmented source image and target text respectively. Every 75 iterations, we add {I s , T } to the batch (i.e., do not apply augmentations). The output of G \u03b8 is then resized down to 224[px] maintaining aspect ratio and augmented (e.g., geometrical augmentations) before extracting CLIP features for establishing the losses. We enable feeding to CLIP arbitrary resolution images (i.e., non-square images) by interpolating the position embeddings (to match the size of spatial tokens of a the given image) using bicubic interpolation, similarly to [5].\n\nTraining on an input image of size 512\u00d7512 takes \u223c 9 minutes to train on a single GPU (NVIDIA RTX 6000) for a total of 1000 iterations. Training on one video layer (foreground/background) of 70 frames with resolution 432 \u00d7 768 takes \u223c60 minutes on a single GPU (NVIDIA RTX 8000) for a total of 3000 iterations.\n\n\nA.4 Video Framework\n\nWe further elaborate on the framework's details described in Sec. 3.2 of the paper.\n\nAtlas Pre-processing. Our framework works on a discretized atlas, which we obtain by rendering the atlas to a resolution of 2000\u00d72000 px. This is done as in [18], by querying the pre-trained atlas network in uniformly sampled UV locations. The neural atlas representation is defined within the [-1,1] continuous space, yet the video content may not occupy the entire space. To focus only on the used atlas regions, we crop the atlas prior to training, by mapping all video locations to the atlas and taking their bounding box. Note that for foreground atlas, we map only the foreground pixels in each frame, i.e., pixels for which the foreground opacity is above 0.95; the foreground/background opacity is estimated by the pre-trained neural atlas representation.\n\nTraining. As discussed in Sec. 3.2 in the paper, our generator is trained on atlas crops, yet our losses are applied to the resulting edited frames. In each iteration, we crop the atlas by first sampling a video segment of 3 frames and mapping it to the atlas. Formally, we sample a random frame t and a random spatial crop size (W, H) where its top left coordinate is at (x, y). As a result we get a set of cropped (spatially and temporally) video locations: V = {p = (x + j, y + i, t + m) s.t. 0 \u2264 j < W, 0 \u2264 i < H, m \u2208 {\u2212k, 0, k}} (11) where k = 2 is the offset between frames.\n\nThe video locations set V is then mapped to its corresponding UV atlas locations: S V = M(V), where M is a pre-trained mapping network. We define the atlas crop I Ac as the minimal crop in the atlas space that contains all the mapped UV locations:\nIAc = IA[u, v] s.t. min(SV .u) \u2264 u \u2264 max(SV .u) min(SV .v) \u2264 v \u2264 max(SV .v),(12)\nWe augment the atlas crop I Ac as well as the target text T , as described in Sec. A.2 herein to generate an internal training dataset. To apply our losses, we map back the atlas edit layer to the original video segment and process the edited frames the same way as in the image framework: resizing, applying CLIP augmentations, and applying the final loss function of Eq. 2 in Sec. 3.1 in the paper. To enrich the data, we also include one of the sampled frame crops as a direct input to G and apply the losses directly on the output (as in the image case). Similarly to the image framework, every 75 iterations we additionally pass the pair {I A , T }, where I A is the entire atlas (without augmentations, and without mapping back to frames). For the background atlas, we first downscale it by three due to memory limitations.\n\nInference. As described in Sec. 3.2, at inference time, the entire atlas I A is fed into G \u03b8 results in E A . The edit is mapped and combined with the original frames using the process that is described in [18](Sec. 3.4, Eq. (15), (16)). Note that our generator operates on a single atlas. To produce foreground and background edits, we train two separate generators for each atlas.\n\nFig. 2 .\n2Text2LIVE generates an edit layer (middle row), which is composited over the original input (bottom row). The text prompts expressing the target layer and the final composite are shown above each image. Our layered editing facilities a variety of effects including changing objects' texture or augmenting the scene with complex semi-transparent effects.\n\nFig. 3 .\n3Image pipeline. Our method consists of a generator trained on a single input image and target text prompts. Left: an internal image-text dataset of diverse training examples is created by augmenting both image and text (see Sec. 3.1).\n\nFig. 4 .\n4Preliminary: Neural Layered Atlases. NLA provides a unified 2D parameterization of a video: the video is decomposed into a set of 2D atlases, each Video pipeline. (a) a pre-trained and fixed layered neural atlas model[18] is used as a \"video renderer\", which consists of: a set of 2D atlases, mapping functions from pixels to the atlases (and per-pixel fg/bg opacity values). (b) Our framework trains a generator that takes a chosen (discretized) atlas IA as input and a target text prompt (e.g., \"rusty car\"), and outputs (c) an atlas edit layer EA. (d) The edited atlas is rendered back to frames using the pre-trained mapping network M, and then (e) composited over the original video.\n\nFig. 5 .\n5Text2LIVE image results. Across rows: different images, across columns: different target edits. All results are produced fully automatically w/o any input masks.\n\nFig. 6 .\n6Black-swan\"cyberpunk neon car\" + \"countryside at nighttime\" Text2LIVE video results. A representative frame from the original and edited videos are shown for each example, along with the target text prompt. In car-turn, both foreground and background atlases are edited sequentially (see Sec. 4). The original and edited atlases are shown on the right. Full video results are included in the SM.\n\nFig. 7 .\n7a) Edit mask (overlay) (d) CLIPStyler (c) Blended-Diffusion (f) VQ-GAN + CLIP (b) GLIDE (e) Diffusion + CLIP Comparison to baselines. A couple of inputs are plugged into different image manipulation methods: cake image, shown in Fig. 1, using \"oreo cake\"; and birds, shown in Fig. 5, using \"golden birds\". (a) manually created masks (shown in red over the input) are provided to (b-c) the inpainting methods as additional inputs, while the rest of the methods are mask-free. Our results are shown in Fig. 1, and Fig. 5.\n\n\n, and Fig. 5 for images, and Fig. 6 for videos. The full set of examples and results are included in the Supplementary Materials (SM).\n\n-\nRandom spatial crops: 0.85 and 0.95 of the image size in our image and video frameworks respectively. -Random scaling: aspect ratio preserved scaling, of both spatial dimensions by a random factor, sampled uniformly from the range [0.8, 1.2]. -Random horizontal-flipping is applied with probability p=0.5. -Random color jittering: we jitter the global brightness, contrast, saturation and hue of the image.\n\n-\n\"photo of {}.\" -\"high quality photo of {}.\" -\"a photo of {}.\" -\"the photo of {}.\" -\"image of {}.\" -\"an image of {}.\" -\"high quality image of {}.\" -\"a high quality image of {}.\" -\"the {}.\" -\"a {}.\" -\"{}.\" -\"{}\" -\"{}!\" -\"{}...\"\n[6] can only work with 224 \u00d7 224 images, so we resize both Is and \u03b1 to 224 \u00d7 224 before applying the loss of (8)\n\nBlended diffusion for text-driven editing of natural images. O Avrahami, D Lischinski, O Fried, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)Avrahami, O., Lischinski, D., Fried, O.: Blended diffusion for text-driven editing of natural images. In: Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR) (2022)\n\n. D Bau, A Andonian, A Cui, Y Park, A Jahanian, A Oliva, A Torralba, arXiv:2103.10951Paint by word. arXiv preprintBau, D., Andonian, A., Cui, A., Park, Y., Jahanian, A., Oliva, A., Torralba, A.: Paint by word. arXiv preprint arXiv:2103.10951 (2021)\n\nThe art and science of digital compositing: Techniques for visual effects, animation and motion graphics. R Brinkmann, Morgan KaufmannBrinkmann, R.: The art and science of digital compositing: Techniques for visual effects, animation and motion graphics. Morgan Kaufmann (2008)\n\nEmerging properties in self-supervised vision transformers. M Caron, H Touvron, I Misra, H Jegou, J Mairal, P Bojanowski, A Joulin, ICCVCaron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. ICCV (2021)\n\nGeneric attention-model explainability for interpreting bi-modal and encoder-decoder transformers. H Chefer, S Gur, L Wolf, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Chefer, H., Gur, S., Wolf, L.: Generic attention-model explainability for interpret- ing bi-modal and encoder-decoder transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\n. K Crowson, Crowson, K.: VQGAN+CLIP, https://colab.research.google.com/github/ justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP(Updated).ipynb\n\nAdaptivity without compromise: a momentumized, adaptive, dual averaged gradient method for stochastic optimization. A Defazio, S Jelassi, arXiv:2101.11075arXiv preprintDefazio, A., Jelassi, S.: Adaptivity without compromise: a momentumized, adap- tive, dual averaged gradient method for stochastic optimization. arXiv preprint arXiv:2101.11075 (2021)\n\nSemantic image synthesis via adversarial learning. H Dong, S Yu, C Wu, Y Guo, Proceedings of the IEEE International Conference on Computer Vision, ICCV. the IEEE International Conference on Computer Vision, ICCVDong, H., Yu, S., Wu, C., Guo, Y.: Semantic image synthesis via adversarial learn- ing. In: Proceedings of the IEEE International Conference on Computer Vision, ICCV (2017)\n\nTaming transformers for high-resolution image synthesis. P Esser, R Rombach, B Ommer, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution im- age synthesis. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2021)\n\nClipdraw: Exploring text-to-drawing synthesis through language-image encoders. K Frans, L Soros, O Witkowski, arXiv:2106.14843arXiv preprintFrans, K., Soros, L., Witkowski, O.: Clipdraw: Exploring text-to-drawing synthesis through language-image encoders. arXiv preprint arXiv:2106.14843 (2021)\n\nStylegan-nada: Clipguided domain adaptation of image generators. R Gal, O Patashnik, H Maron, G Chechik, D Cohen-Or, arXiv:2108.00946arXiv preprintGal, R., Patashnik, O., Maron, H., Chechik, G., Cohen-Or, D.: Stylegan-nada: Clip- guided domain adaptation of image generators. arXiv preprint arXiv:2108.00946 (2021)\n\nDenoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, Advances in Neural Information Processing Systems (NeurIPS). Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: Advances in Neural Information Processing Systems (NeurIPS) (2020)\n\nZero-shot text-guided object generation with dream fields. A Jain, B Mildenhall, J T Barron, P Abbeel, B Poole, Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR). the Conference on Computer Vision and Pattern Recognition (CVPR)Jain, A., Mildenhall, B., Barron, J.T., Abbeel, P., Poole, B.: Zero-shot text-guided object generation with dream fields. In: Proceedings of the Conference on Com- puter Vision and Pattern Recognition (CVPR) (2022)\n\nStylizing video by example. O Jamri\u0161ka, \u0160\u00e1rka Sochorov\u00e1, O Texler, M Luk\u00e1\u010d, J Fi\u0161er, J Lu, E Shechtman, D S\u00fdkora, ACM Transactions on Graphics. Jamri\u0161ka, O.,\u0160\u00e1rka Sochorov\u00e1, Texler, O., Luk\u00e1\u010d, M., Fi\u0161er, J., Lu, J., Shechtman, E., S\u00fdkora, D.: Stylizing video by example. ACM Transactions on Graphics (2019)\n\nY Jing, Y Yang, Z Feng, J Ye, Y Yu, M Song, Neural style transfer: A review. IEEE transactions on visualization and computer graphics. Jing, Y., Yang, Y., Feng, Z., Ye, J., Yu, Y., Song, M.: Neural style transfer: A review. IEEE transactions on visualization and computer graphics (2019)\n\nAnalyzing and improving the image quality of stylegan. T Karras, S Laine, M Aittala, J Hellsten, J Lehtinen, T Aila, IEEE/CVF Conference on Computer Vision and Pattern Recognition. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of stylegan. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition, (CVPR) (2020)\n\nLayered neural atlases for consistent video editing. Y Kasten, D Ofri, O Wang, T Dekel, ACM Transactions on Graphics. Kasten, Y., Ofri, D., Wang, O., Dekel, T.: Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG) (2021)\n\nG Kim, J C Ye, arXiv:2110.02711Diffusionclip: Text-guided image manipulation using diffusion models. arXiv preprintKim, G., Ye, J.C.: Diffusionclip: Text-guided image manipulation using diffusion models. arXiv preprint arXiv:2110.02711 (2021)\n\nStyle transfer by relaxed optimal transport and self-similarity. N I Kolkin, J Salavon, G Shakhnarovich, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Kolkin, N.I., Salavon, J., Shakhnarovich, G.: Style transfer by relaxed optimal transport and self-similarity. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)\n\nG Kwon, J C Ye, arXiv:2112.00374Clipstyler: Image style transfer with a single text condition. arXiv preprintKwon, G., Ye, J.C.: Clipstyler: Image style transfer with a single text condition. arXiv preprint arXiv:2112.00374 (2021)\n\nManigan: Text-guided image manipulation. B Li, X Qi, T Lukasiewicz, P H Torr, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Li, B., Qi, X., Lukasiewicz, T., Torr, P.H.: Manigan: Text-guided image manip- ulation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020)\n\nLayerbuilder: Layer decomposition for interactive image and video color editing. S Lin, M Fisher, A Dai, P Hanrahan, arXiv:1701.03754arXiv preprintLin, S., Fisher, M., Dai, A., Hanrahan, P.: Layerbuilder: Layer decomposition for interactive image and video color editing. arXiv preprint arXiv:1701.03754 (2017)\n\nMicrosoft coco: Common objects in context. T Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European conference on computer vision (ECCV). Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision (ECCV) (2014)\n\nFusedream: Training-free textto-image generation with improved clip+ gan space optimization. X Liu, C Gong, L Wu, S Zhang, H Su, Q Liu, arXiv:2112.01573arXiv preprintLiu, X., Gong, C., Wu, L., Zhang, S., Su, H., Liu, Q.: Fusedream: Training-free text- to-image generation with improved clip+ gan space optimization. arXiv preprint arXiv:2112.01573 (2021)\n\nLayered neural rendering for retiming people in video. E Lu, F Cole, T Dekel, W Xie, A Zisserman, D Salesin, W T Freeman, M Rubinstein, ACM Trans. Graph. Lu, E., Cole, F., Dekel, T., Xie, W., Zisserman, A., Salesin, D., Freeman, W.T., Rubinstein, M.: Layered neural rendering for retiming people in video. ACM Trans. Graph. (2020)\n\nOmnimatte: Associating objects and their effects in video. E Lu, F Cole, T Dekel, A Zisserman, W T Freeman, M Rubinstein, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Lu, E., Cole, F., Dekel, T., Zisserman, A., Freeman, W.T., Rubinstein, M.: Om- nimatte: Associating objects and their effects in video. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)\n\nO Michel, R Bar-On, R Liu, S Benaim, R Hanocka, arXiv:2112.03221Text2mesh: Text-driven neural stylization for meshes. arXiv preprintMichel, O., Bar-On, R., Liu, R., Benaim, S., Hanocka, R.: Text2mesh: Text-driven neural stylization for meshes. arXiv preprint arXiv:2112.03221 (2021)\n\nText-adaptive generative adversarial networks: Manipulating images with natural language. S Nam, Y Kim, S J Kim, Advances in Neural Information Processing Systems (NeurIPS). Nam, S., Kim, Y., Kim, S.J.: Text-adaptive generative adversarial networks: Ma- nipulating images with natural language. In: Advances in Neural Information Pro- cessing Systems (NeurIPS) (2018)\n\nGlide: Towards photorealistic image generation and editing with text-guided diffusion models. A Nichol, P Dhariwal, A Ramesh, P Shyam, P Mishkin, B Mcgrew, I Sutskever, M Chen, arXiv:2112.10741arXiv preprintNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021)\n\nSwapping autoencoder for deep image manipulation. T Park, J Zhu, O Wang, J Lu, E Shechtman, A A Efros, R Zhang, Advances in Neural Information Processing Systems (NeurIPS). Park, T., Zhu, J., Wang, O., Lu, J., Shechtman, E., Efros, A.A., Zhang, R.: Swap- ping autoencoder for deep image manipulation. In: Advances in Neural Information Processing Systems (NeurIPS) (2020)\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems. Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E., Garnett, R.Curran Associates, Inc32Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.: Pytorch: An imperative style, high-performance deep learning library. In: Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\u00e9-Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran Associates, Inc. (2019), http://papers.neurips.cc/paper/ 9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\n\nStyleclip: Text-driven manipulation of stylegan imagery. O Patashnik, Z Wu, E Shechtman, D Cohen-Or, D Lischinski, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., Lischinski, D.: Styleclip: Text-driven manipulation of stylegan imagery. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2021)\n\nJ Pont-Tuset, F Perazzi, S Caelles, P Arbel\u00e1ez, A Sorkine-Hornung, L Van Gool, arXiv:1704.00675The 2017 davis challenge on video object segmentation. arXiv preprintPont-Tuset, J., Perazzi, F., Caelles, S., Arbel\u00e1ez, P., Sorkine-Hornung, A., Van Gool, L.: The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675 (2017)\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, Proceedings of the 38th International Conference on Machine Learning (ICML). the 38th International Conference on Machine Learning (ICML)Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transfer- able visual models from natural language supervision. In: Proceedings of the 38th International Conference on Machine Learning (ICML) (2021)\n\nZero-shot text-to-image generation. A Ramesh, M Pavlov, G Goh, S Gray, C Voss, A Radford, M Chen, I Sutskever, Proceedings of the 38th International Conference on Machine Learning (ICML). the 38th International Conference on Machine Learning (ICML)Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., Sutskever, I.: Zero-shot text-to-image generation. In: Proceedings of the 38th In- ternational Conference on Machine Learning (ICML) (2021)\n\nUnwrap mosaics: a new representation for video editing. A Rav-Acha, P Kohli, C Rother, A W Fitzgibbon, ACM Trans. Graph. Rav-Acha, A., Kohli, P., Rother, C., Fitzgibbon, A.W.: Unwrap mosaics: a new representation for video editing. ACM Trans. Graph. (2008)\n\nGenerative adversarial text to image synthesis. S E Reed, Z Akata, X Yan, L Logeswaran, B Schiele, H Lee, Proceedings of the 33rd International Conference on Machine Learning (ICML). the 33rd International Conference on Machine Learning (ICML)Reed, S.E., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative adversarial text to image synthesis. In: Proceedings of the 33rd International Con- ference on Machine Learning (ICML) (2016)\n\nEncoding in style: a stylegan encoder for image-to-image translation. E Richardson, Y Alaluf, O Patashnik, Y Nitzan, Y Azar, S Shapiro, D Cohen-Or, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Richardson, E., Alaluf, Y., Patashnik, O., Nitzan, Y., Azar, Y., Shapiro, S., Cohen- Or, D.: Encoding in style: a stylegan encoder for image-to-image translation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition (CVPR) (2021)\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerRonneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi- cal image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. Springer (2015)\n\nArtistic style transfer for videos. M Ruder, A Dosovitskiy, T Brox, Pattern Recognition -38th German Conference (GCPR). Ruder, M., Dosovitskiy, A., Brox, T.: Artistic style transfer for videos. In: Pattern Recognition -38th German Conference (GCPR) (2016)\n\nSingan: Learning a generative model from a single natural image. T R Shaham, T Dekel, T Michaeli, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Shaham, T.R., Dekel, T., Michaeli, T.: Singan: Learning a generative model from a single natural image. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (2019)\n\nMatching local self-similarities across images and videos. E Shechtman, M Irani, 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR). Shechtman, E., Irani, M.: Matching local self-similarities across images and videos. In: 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR) (2007)\n\nIngan: Capturing and retargeting the \"dna\" of a natural image. A Shocher, S Bagon, P Isola, M Irani, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Shocher, A., Bagon, S., Isola, P., Irani, M.: Ingan: Capturing and retargeting the \"dna\" of a natural image. In: 2019 IEEE/CVF International Conference on Com- puter Vision (ICCV) (2019)\n\nDenoising diffusion implicit models. J Song, C Meng, S Ermon, 9th International Conference on Learning Representations. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: 9th Inter- national Conference on Learning Representations, (ICLR) (2021)\n\nInteractive video stylization using few-shot patch-based training. O Texler, D Futschik, M Ku\u010dera, O Jamri\u0161ka, \u0160\u00e1rka Sochorov\u00e1, M Chai, S Tulyakov, D S\u00fdkora, ACM Transactions on Graphics. Texler, O., Futschik, D., Ku\u010dera, M., Jamri\u0161ka, O.,\u0160\u00e1rka Sochorov\u00e1, Chai, M., Tulyakov, S., S\u00fdkora, D.: Interactive video stylization using few-shot patch-based training. ACM Transactions on Graphics (2020)\n\nDesigning an encoder for stylegan image manipulation. O Tov, Y Alaluf, Y Nitzan, O Patashnik, D Cohen-Or, ACM Transactions on Graphics. Tov, O., Alaluf, Y., Nitzan, Y., Patashnik, O., Cohen-Or, D.: Designing an encoder for stylegan image manipulation. ACM Transactions on Graphics (TOG) (2021)\n\nSplicing vit features for semantic appearance transfer. N Tumanyan, O Bar-Tal, S Bagon, T Dekel, arXiv:2201.00424arXiv preprintTumanyan, N., Bar-Tal, O., Bagon, S., Dekel, T.: Splicing vit features for semantic appearance transfer. arXiv preprint arXiv:2201.00424 (2022)\n\nW Xia, Y Zhang, Y Yang, J H Xue, B Zhou, M H Yang, arXiv:2101.05278Gan inversion: A survey. arXiv preprintXia, W., Zhang, Y., Yang, Y., Xue, J.H., Zhou, B., Yang, M.H.: Gan inversion: A survey. arXiv preprint arXiv:2101.05278 (2021)\n\nAttngan: Fine-grained text to image generation with attentional generative adversarial networks. T Xu, P Zhang, Q Huang, H Zhang, Z Gan, X Huang, X He, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X.: Attngan: Fine-grained text to image generation with attentional generative adversarial net- works. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)\n\nStackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. H Zhang, T Xu, H Li, S Zhang, X Wang, X Huang, D N Metaxas, Proceedings of the IEEE International Conference on Computer Vision (ICCV. the IEEE International Conference on Computer Vision (ICCVZhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.N.: Stack- gan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In: Proceedings of the IEEE International Conference on Computer Vi- sion (ICCV) (2017)\n\nStack-gan++: Realistic image synthesis with stacked generative adversarial networks. H Zhang, T Xu, H Li, S Zhang, X Wang, X Huang, D N Metaxas, IEEE Transactions on Pattern Analysis and Machine Intelligence. Zhang, H., Xu, T., Li, H., Zhang, S., Wang, X., Huang, X., Metaxas, D.N.: Stack- gan++: Realistic image synthesis with stacked generative adversarial networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019)\n", "annotations": {"author": "[{\"end\":179,\"start\":134},{\"end\":228,\"start\":180},{\"end\":276,\"start\":229},{\"end\":307,\"start\":277},{\"end\":351,\"start\":308}]", "publisher": null, "author_last_name": "[{\"end\":146,\"start\":139},{\"end\":195,\"start\":186},{\"end\":243,\"start\":236},{\"end\":288,\"start\":282},{\"end\":318,\"start\":313}]", "author_first_name": "[{\"end\":138,\"start\":134},{\"end\":185,\"start\":180},{\"end\":235,\"start\":229},{\"end\":281,\"start\":277},{\"end\":312,\"start\":308}]", "author_affiliation": "[{\"end\":178,\"start\":148},{\"end\":227,\"start\":197},{\"end\":275,\"start\":245},{\"end\":306,\"start\":290},{\"end\":350,\"start\":320}]", "title": "[{\"end\":131,\"start\":1},{\"end\":482,\"start\":352}]", "venue": null, "abstract": "[{\"end\":2530,\"start\":616}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5244,\"start\":5240},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6640,\"start\":6636},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6784,\"start\":6781},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6786,\"start\":6784},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6789,\"start\":6786},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6792,\"start\":6789},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6795,\"start\":6792},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7158,\"start\":7154},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7181,\"start\":7177},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7184,\"start\":7181},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7332,\"start\":7329},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8930,\"start\":8926},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10311,\"start\":10307},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10314,\"start\":10311},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10317,\"start\":10314},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10320,\"start\":10317},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10337,\"start\":10334},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10340,\"start\":10337},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10343,\"start\":10340},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10357,\"start\":10353},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10618,\"start\":10614},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10632,\"start\":10628},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10831,\"start\":10827},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11065,\"start\":11061},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11109,\"start\":11106},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11112,\"start\":11109},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11115,\"start\":11112},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11118,\"start\":11115},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11134,\"start\":11130},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11157,\"start\":11153},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11190,\"start\":11186},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11263,\"start\":11259},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11316,\"start\":11312},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11430,\"start\":11426},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11433,\"start\":11430},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11922,\"start\":11918},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11948,\"start\":11944},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11951,\"start\":11948},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11957,\"start\":11953},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12333,\"start\":12330},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12619,\"start\":12615},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12622,\"start\":12619},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12625,\"start\":12622},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12628,\"start\":12625},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12751,\"start\":12747},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13030,\"start\":13026},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13466,\"start\":13462},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":13469,\"start\":13466},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":13472,\"start\":13469},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13949,\"start\":13945},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13962,\"start\":13958},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14300,\"start\":14296},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14462,\"start\":14458},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14465,\"start\":14462},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14486,\"start\":14482},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14644,\"start\":14640},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":14647,\"start\":14644},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14650,\"start\":14647},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14653,\"start\":14650},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14656,\"start\":14653},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14685,\"start\":14681},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14688,\"start\":14685},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15030,\"start\":15026},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15869,\"start\":15865},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16020,\"start\":16016},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16023,\"start\":16020},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18877,\"start\":18873},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19173,\"start\":19169},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19176,\"start\":19173},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19426,\"start\":19423},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21122,\"start\":21118},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21125,\"start\":21122},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21128,\"start\":21125},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21789,\"start\":21785},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21792,\"start\":21789},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22247,\"start\":22244},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24348,\"start\":24344},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25660,\"start\":25656},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27336,\"start\":27332},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30057,\"start\":30053},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31534,\"start\":31531},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31549,\"start\":31545},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31681,\"start\":31678},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31879,\"start\":31875},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31882,\"start\":31879},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32620,\"start\":32617},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33060,\"start\":33056},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33063,\"start\":33060},{\"end\":34683,\"start\":34676},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":37811,\"start\":37807},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":39564,\"start\":39560},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":41111,\"start\":41107},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":41208,\"start\":41204},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":41675,\"start\":41672},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":42536,\"start\":42533},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":43119,\"start\":43115},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":44261,\"start\":44257},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":45675,\"start\":45671},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":45700,\"start\":45696},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":46691,\"start\":46687},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":49049,\"start\":49046}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":46212,\"start\":45848},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46458,\"start\":46213},{\"attributes\":{\"id\":\"fig_3\"},\"end\":47158,\"start\":46459},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47331,\"start\":47159},{\"attributes\":{\"id\":\"fig_5\"},\"end\":47738,\"start\":47332},{\"attributes\":{\"id\":\"fig_6\"},\"end\":48269,\"start\":47739},{\"attributes\":{\"id\":\"fig_7\"},\"end\":48406,\"start\":48270},{\"attributes\":{\"id\":\"fig_8\"},\"end\":48816,\"start\":48407},{\"attributes\":{\"id\":\"fig_9\"},\"end\":49045,\"start\":48817}]", "paragraph": "[{\"end\":2634,\"start\":2585},{\"end\":2717,\"start\":2636},{\"end\":3310,\"start\":2769},{\"end\":4783,\"start\":3312},{\"end\":6463,\"start\":4808},{\"end\":7533,\"start\":6465},{\"end\":8563,\"start\":7535},{\"end\":9386,\"start\":8565},{\"end\":9439,\"start\":9388},{\"end\":10138,\"start\":9441},{\"end\":11705,\"start\":10155},{\"end\":12213,\"start\":11707},{\"end\":12599,\"start\":12215},{\"end\":13908,\"start\":12601},{\"end\":14301,\"start\":13910},{\"end\":15411,\"start\":14303},{\"end\":16235,\"start\":15459},{\"end\":16951,\"start\":16237},{\"end\":17262,\"start\":16980},{\"end\":18002,\"start\":17293},{\"end\":18478,\"start\":18004},{\"end\":18677,\"start\":18539},{\"end\":18878,\"start\":18679},{\"end\":19192,\"start\":18934},{\"end\":19499,\"start\":19252},{\"end\":20119,\"start\":19501},{\"end\":20601,\"start\":20121},{\"end\":21413,\"start\":20603},{\"end\":21622,\"start\":21452},{\"end\":21896,\"start\":21657},{\"end\":22052,\"start\":21923},{\"end\":22462,\"start\":22054},{\"end\":22885,\"start\":22490},{\"end\":22965,\"start\":22887},{\"end\":23963,\"start\":23055},{\"end\":24067,\"start\":23992},{\"end\":25090,\"start\":24069},{\"end\":25678,\"start\":25149},{\"end\":26196,\"start\":25680},{\"end\":26565,\"start\":26198},{\"end\":26724,\"start\":26567},{\"end\":26880,\"start\":26814},{\"end\":27102,\"start\":26882},{\"end\":27337,\"start\":27127},{\"end\":29527,\"start\":27339},{\"end\":31033,\"start\":29774},{\"end\":32840,\"start\":31062},{\"end\":33710,\"start\":32868},{\"end\":34240,\"start\":33712},{\"end\":34474,\"start\":34242},{\"end\":34958,\"start\":34494},{\"end\":35469,\"start\":34960},{\"end\":36217,\"start\":35471},{\"end\":37125,\"start\":36236},{\"end\":37943,\"start\":37141},{\"end\":38298,\"start\":37945},{\"end\":39088,\"start\":38313},{\"end\":39357,\"start\":39108},{\"end\":39461,\"start\":39386},{\"end\":40010,\"start\":39500},{\"end\":40128,\"start\":40046},{\"end\":41044,\"start\":40183},{\"end\":42537,\"start\":41069},{\"end\":42849,\"start\":42539},{\"end\":42956,\"start\":42873},{\"end\":43721,\"start\":42958},{\"end\":44303,\"start\":43723},{\"end\":44552,\"start\":44305},{\"end\":45463,\"start\":44634},{\"end\":45847,\"start\":45465}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17292,\"start\":17263},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18538,\"start\":18479},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18933,\"start\":18879},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19251,\"start\":19193},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21451,\"start\":21414},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21656,\"start\":21623},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21922,\"start\":21897},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22489,\"start\":22463},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23054,\"start\":22966},{\"attributes\":{\"id\":\"formula_9\"},\"end\":25148,\"start\":25091},{\"attributes\":{\"id\":\"formula_10\"},\"end\":27126,\"start\":27103},{\"attributes\":{\"id\":\"formula_11\"},\"end\":40182,\"start\":40129},{\"attributes\":{\"id\":\"formula_12\"},\"end\":44633,\"start\":44553}]", "table_ref": "[{\"end\":33550,\"start\":33543},{\"end\":34646,\"start\":34639}]", "section_header": "[{\"end\":2583,\"start\":2532},{\"end\":2755,\"start\":2720},{\"end\":2767,\"start\":2758},{\"end\":4791,\"start\":4786},{\"attributes\":{\"n\":\"1\"},\"end\":4806,\"start\":4794},{\"attributes\":{\"n\":\"2\"},\"end\":10153,\"start\":10141},{\"attributes\":{\"n\":\"3\"},\"end\":15457,\"start\":15414},{\"attributes\":{\"n\":\"3.1\"},\"end\":16978,\"start\":16954},{\"attributes\":{\"n\":\"3.2\"},\"end\":23990,\"start\":23966},{\"end\":26812,\"start\":26727},{\"end\":29737,\"start\":29530},{\"attributes\":{\"n\":\"4\"},\"end\":29747,\"start\":29740},{\"attributes\":{\"n\":\"4.1\"},\"end\":29772,\"start\":29750},{\"attributes\":{\"n\":\"4.2\"},\"end\":31060,\"start\":31036},{\"attributes\":{\"n\":\"4.3\"},\"end\":32866,\"start\":32843},{\"end\":34492,\"start\":34477},{\"attributes\":{\"n\":\"4.4\"},\"end\":36234,\"start\":36220},{\"attributes\":{\"n\":\"4.5\"},\"end\":37139,\"start\":37128},{\"attributes\":{\"n\":\"5\"},\"end\":38311,\"start\":38301},{\"attributes\":{\"n\":\"6\"},\"end\":39106,\"start\":39091},{\"end\":39384,\"start\":39360},{\"end\":39498,\"start\":39464},{\"end\":40044,\"start\":40013},{\"end\":41067,\"start\":41047},{\"end\":42871,\"start\":42852},{\"end\":45857,\"start\":45849},{\"end\":46222,\"start\":46214},{\"end\":46468,\"start\":46460},{\"end\":47168,\"start\":47160},{\"end\":47341,\"start\":47333},{\"end\":47748,\"start\":47740},{\"end\":48409,\"start\":48408},{\"end\":48819,\"start\":48818}]", "table": null, "figure_caption": "[{\"end\":46212,\"start\":45859},{\"end\":46458,\"start\":46224},{\"end\":47158,\"start\":46470},{\"end\":47331,\"start\":47170},{\"end\":47738,\"start\":47343},{\"end\":48269,\"start\":47750},{\"end\":48406,\"start\":48272},{\"end\":48816,\"start\":48410},{\"end\":49045,\"start\":48820}]", "figure_ref": "[{\"end\":2806,\"start\":2800},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6050,\"start\":6033},{\"end\":6099,\"start\":6090},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8107,\"start\":8101},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17004,\"start\":16998},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17835,\"start\":17829},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20286,\"start\":20280},{\"end\":22603,\"start\":22593},{\"end\":22883,\"start\":22874},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23100,\"start\":23087},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24377,\"start\":24368},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24654,\"start\":24648},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26305,\"start\":26299},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27640,\"start\":27634},{\"end\":28988,\"start\":28982},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30351,\"start\":30337},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30571,\"start\":30564},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30646,\"start\":30638},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30831,\"start\":30825},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30881,\"start\":30875},{\"end\":31031,\"start\":31025},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31890,\"start\":31884},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":34011,\"start\":34005},{\"end\":35047,\"start\":35041},{\"end\":36009,\"start\":35998},{\"end\":36438,\"start\":36432},{\"end\":37265,\"start\":37259},{\"end\":37758,\"start\":37745}]", "bib_author_first_name": "[{\"end\":49222,\"start\":49221},{\"end\":49234,\"start\":49233},{\"end\":49248,\"start\":49247},{\"end\":49598,\"start\":49597},{\"end\":49605,\"start\":49604},{\"end\":49617,\"start\":49616},{\"end\":49624,\"start\":49623},{\"end\":49632,\"start\":49631},{\"end\":49644,\"start\":49643},{\"end\":49653,\"start\":49652},{\"end\":49952,\"start\":49951},{\"end\":50185,\"start\":50184},{\"end\":50194,\"start\":50193},{\"end\":50205,\"start\":50204},{\"end\":50214,\"start\":50213},{\"end\":50223,\"start\":50222},{\"end\":50233,\"start\":50232},{\"end\":50247,\"start\":50246},{\"end\":50519,\"start\":50518},{\"end\":50529,\"start\":50528},{\"end\":50536,\"start\":50535},{\"end\":50912,\"start\":50911},{\"end\":51171,\"start\":51170},{\"end\":51182,\"start\":51181},{\"end\":51458,\"start\":51457},{\"end\":51466,\"start\":51465},{\"end\":51472,\"start\":51471},{\"end\":51478,\"start\":51477},{\"end\":51849,\"start\":51848},{\"end\":51858,\"start\":51857},{\"end\":51869,\"start\":51868},{\"end\":52196,\"start\":52195},{\"end\":52205,\"start\":52204},{\"end\":52214,\"start\":52213},{\"end\":52478,\"start\":52477},{\"end\":52485,\"start\":52484},{\"end\":52498,\"start\":52497},{\"end\":52507,\"start\":52506},{\"end\":52518,\"start\":52517},{\"end\":52771,\"start\":52770},{\"end\":52777,\"start\":52776},{\"end\":52785,\"start\":52784},{\"end\":53059,\"start\":53058},{\"end\":53067,\"start\":53066},{\"end\":53081,\"start\":53080},{\"end\":53083,\"start\":53082},{\"end\":53093,\"start\":53092},{\"end\":53103,\"start\":53102},{\"end\":53501,\"start\":53500},{\"end\":53517,\"start\":53512},{\"end\":53530,\"start\":53529},{\"end\":53540,\"start\":53539},{\"end\":53549,\"start\":53548},{\"end\":53558,\"start\":53557},{\"end\":53564,\"start\":53563},{\"end\":53577,\"start\":53576},{\"end\":53781,\"start\":53780},{\"end\":53789,\"start\":53788},{\"end\":53797,\"start\":53796},{\"end\":53805,\"start\":53804},{\"end\":53811,\"start\":53810},{\"end\":53817,\"start\":53816},{\"end\":54125,\"start\":54124},{\"end\":54135,\"start\":54134},{\"end\":54144,\"start\":54143},{\"end\":54155,\"start\":54154},{\"end\":54167,\"start\":54166},{\"end\":54179,\"start\":54178},{\"end\":54516,\"start\":54515},{\"end\":54526,\"start\":54525},{\"end\":54534,\"start\":54533},{\"end\":54542,\"start\":54541},{\"end\":54720,\"start\":54719},{\"end\":54727,\"start\":54726},{\"end\":54729,\"start\":54728},{\"end\":55029,\"start\":55028},{\"end\":55031,\"start\":55030},{\"end\":55041,\"start\":55040},{\"end\":55052,\"start\":55051},{\"end\":55325,\"start\":55324},{\"end\":55333,\"start\":55332},{\"end\":55335,\"start\":55334},{\"end\":55598,\"start\":55597},{\"end\":55604,\"start\":55603},{\"end\":55610,\"start\":55609},{\"end\":55625,\"start\":55624},{\"end\":55627,\"start\":55626},{\"end\":56068,\"start\":56067},{\"end\":56075,\"start\":56074},{\"end\":56085,\"start\":56084},{\"end\":56092,\"start\":56091},{\"end\":56342,\"start\":56341},{\"end\":56344,\"start\":56343},{\"end\":56351,\"start\":56350},{\"end\":56360,\"start\":56359},{\"end\":56372,\"start\":56371},{\"end\":56380,\"start\":56379},{\"end\":56390,\"start\":56389},{\"end\":56401,\"start\":56400},{\"end\":56411,\"start\":56410},{\"end\":56413,\"start\":56412},{\"end\":56763,\"start\":56762},{\"end\":56770,\"start\":56769},{\"end\":56778,\"start\":56777},{\"end\":56784,\"start\":56783},{\"end\":56793,\"start\":56792},{\"end\":56799,\"start\":56798},{\"end\":57081,\"start\":57080},{\"end\":57087,\"start\":57086},{\"end\":57095,\"start\":57094},{\"end\":57104,\"start\":57103},{\"end\":57111,\"start\":57110},{\"end\":57124,\"start\":57123},{\"end\":57135,\"start\":57134},{\"end\":57137,\"start\":57136},{\"end\":57148,\"start\":57147},{\"end\":57417,\"start\":57416},{\"end\":57423,\"start\":57422},{\"end\":57431,\"start\":57430},{\"end\":57440,\"start\":57439},{\"end\":57453,\"start\":57452},{\"end\":57455,\"start\":57454},{\"end\":57466,\"start\":57465},{\"end\":57880,\"start\":57879},{\"end\":57890,\"start\":57889},{\"end\":57900,\"start\":57899},{\"end\":57907,\"start\":57906},{\"end\":57917,\"start\":57916},{\"end\":58254,\"start\":58253},{\"end\":58261,\"start\":58260},{\"end\":58268,\"start\":58267},{\"end\":58270,\"start\":58269},{\"end\":58627,\"start\":58626},{\"end\":58637,\"start\":58636},{\"end\":58649,\"start\":58648},{\"end\":58659,\"start\":58658},{\"end\":58668,\"start\":58667},{\"end\":58679,\"start\":58678},{\"end\":58689,\"start\":58688},{\"end\":58702,\"start\":58701},{\"end\":59023,\"start\":59022},{\"end\":59031,\"start\":59030},{\"end\":59038,\"start\":59037},{\"end\":59046,\"start\":59045},{\"end\":59052,\"start\":59051},{\"end\":59065,\"start\":59064},{\"end\":59067,\"start\":59066},{\"end\":59076,\"start\":59075},{\"end\":59416,\"start\":59415},{\"end\":59426,\"start\":59425},{\"end\":59435,\"start\":59434},{\"end\":59444,\"start\":59443},{\"end\":59453,\"start\":59452},{\"end\":59465,\"start\":59464},{\"end\":59475,\"start\":59474},{\"end\":59486,\"start\":59485},{\"end\":59493,\"start\":59492},{\"end\":59507,\"start\":59506},{\"end\":59517,\"start\":59516},{\"end\":59530,\"start\":59529},{\"end\":59538,\"start\":59537},{\"end\":59546,\"start\":59545},{\"end\":59556,\"start\":59555},{\"end\":59566,\"start\":59565},{\"end\":59576,\"start\":59575},{\"end\":59592,\"start\":59591},{\"end\":59603,\"start\":59602},{\"end\":59611,\"start\":59610},{\"end\":59618,\"start\":59617},{\"end\":60476,\"start\":60475},{\"end\":60489,\"start\":60488},{\"end\":60495,\"start\":60494},{\"end\":60508,\"start\":60507},{\"end\":60520,\"start\":60519},{\"end\":60893,\"start\":60892},{\"end\":60907,\"start\":60906},{\"end\":60918,\"start\":60917},{\"end\":60929,\"start\":60928},{\"end\":60941,\"start\":60940},{\"end\":60960,\"start\":60959},{\"end\":61314,\"start\":61313},{\"end\":61325,\"start\":61324},{\"end\":61327,\"start\":61326},{\"end\":61334,\"start\":61333},{\"end\":61345,\"start\":61344},{\"end\":61355,\"start\":61354},{\"end\":61362,\"start\":61361},{\"end\":61373,\"start\":61372},{\"end\":61383,\"start\":61382},{\"end\":61393,\"start\":61392},{\"end\":61404,\"start\":61403},{\"end\":61413,\"start\":61412},{\"end\":61424,\"start\":61423},{\"end\":61918,\"start\":61917},{\"end\":61928,\"start\":61927},{\"end\":61938,\"start\":61937},{\"end\":61945,\"start\":61944},{\"end\":61953,\"start\":61952},{\"end\":61961,\"start\":61960},{\"end\":61972,\"start\":61971},{\"end\":61980,\"start\":61979},{\"end\":62403,\"start\":62402},{\"end\":62415,\"start\":62414},{\"end\":62424,\"start\":62423},{\"end\":62434,\"start\":62433},{\"end\":62436,\"start\":62435},{\"end\":62653,\"start\":62652},{\"end\":62655,\"start\":62654},{\"end\":62663,\"start\":62662},{\"end\":62672,\"start\":62671},{\"end\":62679,\"start\":62678},{\"end\":62693,\"start\":62692},{\"end\":62704,\"start\":62703},{\"end\":63126,\"start\":63125},{\"end\":63140,\"start\":63139},{\"end\":63150,\"start\":63149},{\"end\":63163,\"start\":63162},{\"end\":63173,\"start\":63172},{\"end\":63181,\"start\":63180},{\"end\":63192,\"start\":63191},{\"end\":63698,\"start\":63697},{\"end\":63713,\"start\":63712},{\"end\":63724,\"start\":63723},{\"end\":64080,\"start\":64079},{\"end\":64089,\"start\":64088},{\"end\":64104,\"start\":64103},{\"end\":64366,\"start\":64365},{\"end\":64368,\"start\":64367},{\"end\":64378,\"start\":64377},{\"end\":64387,\"start\":64386},{\"end\":64796,\"start\":64795},{\"end\":64809,\"start\":64808},{\"end\":65155,\"start\":65154},{\"end\":65166,\"start\":65165},{\"end\":65175,\"start\":65174},{\"end\":65184,\"start\":65183},{\"end\":65484,\"start\":65483},{\"end\":65492,\"start\":65491},{\"end\":65500,\"start\":65499},{\"end\":65781,\"start\":65780},{\"end\":65791,\"start\":65790},{\"end\":65803,\"start\":65802},{\"end\":65813,\"start\":65812},{\"end\":65829,\"start\":65824},{\"end\":65842,\"start\":65841},{\"end\":65850,\"start\":65849},{\"end\":65862,\"start\":65861},{\"end\":66164,\"start\":66163},{\"end\":66171,\"start\":66170},{\"end\":66181,\"start\":66180},{\"end\":66191,\"start\":66190},{\"end\":66204,\"start\":66203},{\"end\":66461,\"start\":66460},{\"end\":66473,\"start\":66472},{\"end\":66484,\"start\":66483},{\"end\":66493,\"start\":66492},{\"end\":66677,\"start\":66676},{\"end\":66684,\"start\":66683},{\"end\":66693,\"start\":66692},{\"end\":66701,\"start\":66700},{\"end\":66703,\"start\":66702},{\"end\":66710,\"start\":66709},{\"end\":66718,\"start\":66717},{\"end\":66720,\"start\":66719},{\"end\":67008,\"start\":67007},{\"end\":67014,\"start\":67013},{\"end\":67023,\"start\":67022},{\"end\":67032,\"start\":67031},{\"end\":67041,\"start\":67040},{\"end\":67048,\"start\":67047},{\"end\":67057,\"start\":67056},{\"end\":67579,\"start\":67578},{\"end\":67588,\"start\":67587},{\"end\":67594,\"start\":67593},{\"end\":67600,\"start\":67599},{\"end\":67609,\"start\":67608},{\"end\":67617,\"start\":67616},{\"end\":67626,\"start\":67625},{\"end\":67628,\"start\":67627},{\"end\":68118,\"start\":68117},{\"end\":68127,\"start\":68126},{\"end\":68133,\"start\":68132},{\"end\":68139,\"start\":68138},{\"end\":68148,\"start\":68147},{\"end\":68156,\"start\":68155},{\"end\":68165,\"start\":68164},{\"end\":68167,\"start\":68166}]", "bib_author_last_name": "[{\"end\":49231,\"start\":49223},{\"end\":49245,\"start\":49235},{\"end\":49254,\"start\":49249},{\"end\":49602,\"start\":49599},{\"end\":49614,\"start\":49606},{\"end\":49621,\"start\":49618},{\"end\":49629,\"start\":49625},{\"end\":49641,\"start\":49633},{\"end\":49650,\"start\":49645},{\"end\":49662,\"start\":49654},{\"end\":49962,\"start\":49953},{\"end\":50191,\"start\":50186},{\"end\":50202,\"start\":50195},{\"end\":50211,\"start\":50206},{\"end\":50220,\"start\":50215},{\"end\":50230,\"start\":50224},{\"end\":50244,\"start\":50234},{\"end\":50254,\"start\":50248},{\"end\":50526,\"start\":50520},{\"end\":50533,\"start\":50530},{\"end\":50541,\"start\":50537},{\"end\":50920,\"start\":50913},{\"end\":51179,\"start\":51172},{\"end\":51190,\"start\":51183},{\"end\":51463,\"start\":51459},{\"end\":51469,\"start\":51467},{\"end\":51475,\"start\":51473},{\"end\":51482,\"start\":51479},{\"end\":51855,\"start\":51850},{\"end\":51866,\"start\":51859},{\"end\":51875,\"start\":51870},{\"end\":52202,\"start\":52197},{\"end\":52211,\"start\":52206},{\"end\":52224,\"start\":52215},{\"end\":52482,\"start\":52479},{\"end\":52495,\"start\":52486},{\"end\":52504,\"start\":52499},{\"end\":52515,\"start\":52508},{\"end\":52527,\"start\":52519},{\"end\":52774,\"start\":52772},{\"end\":52782,\"start\":52778},{\"end\":52792,\"start\":52786},{\"end\":53064,\"start\":53060},{\"end\":53078,\"start\":53068},{\"end\":53090,\"start\":53084},{\"end\":53100,\"start\":53094},{\"end\":53109,\"start\":53104},{\"end\":53510,\"start\":53502},{\"end\":53527,\"start\":53518},{\"end\":53537,\"start\":53531},{\"end\":53546,\"start\":53541},{\"end\":53555,\"start\":53550},{\"end\":53561,\"start\":53559},{\"end\":53574,\"start\":53565},{\"end\":53584,\"start\":53578},{\"end\":53786,\"start\":53782},{\"end\":53794,\"start\":53790},{\"end\":53802,\"start\":53798},{\"end\":53808,\"start\":53806},{\"end\":53814,\"start\":53812},{\"end\":53822,\"start\":53818},{\"end\":54132,\"start\":54126},{\"end\":54141,\"start\":54136},{\"end\":54152,\"start\":54145},{\"end\":54164,\"start\":54156},{\"end\":54176,\"start\":54168},{\"end\":54184,\"start\":54180},{\"end\":54523,\"start\":54517},{\"end\":54531,\"start\":54527},{\"end\":54539,\"start\":54535},{\"end\":54548,\"start\":54543},{\"end\":54724,\"start\":54721},{\"end\":54732,\"start\":54730},{\"end\":55038,\"start\":55032},{\"end\":55049,\"start\":55042},{\"end\":55066,\"start\":55053},{\"end\":55330,\"start\":55326},{\"end\":55338,\"start\":55336},{\"end\":55601,\"start\":55599},{\"end\":55607,\"start\":55605},{\"end\":55622,\"start\":55611},{\"end\":55632,\"start\":55628},{\"end\":56072,\"start\":56069},{\"end\":56082,\"start\":56076},{\"end\":56089,\"start\":56086},{\"end\":56101,\"start\":56093},{\"end\":56348,\"start\":56345},{\"end\":56357,\"start\":56352},{\"end\":56369,\"start\":56361},{\"end\":56377,\"start\":56373},{\"end\":56387,\"start\":56381},{\"end\":56398,\"start\":56391},{\"end\":56408,\"start\":56402},{\"end\":56421,\"start\":56414},{\"end\":56767,\"start\":56764},{\"end\":56775,\"start\":56771},{\"end\":56781,\"start\":56779},{\"end\":56790,\"start\":56785},{\"end\":56796,\"start\":56794},{\"end\":56803,\"start\":56800},{\"end\":57084,\"start\":57082},{\"end\":57092,\"start\":57088},{\"end\":57101,\"start\":57096},{\"end\":57108,\"start\":57105},{\"end\":57121,\"start\":57112},{\"end\":57132,\"start\":57125},{\"end\":57145,\"start\":57138},{\"end\":57159,\"start\":57149},{\"end\":57420,\"start\":57418},{\"end\":57428,\"start\":57424},{\"end\":57437,\"start\":57432},{\"end\":57450,\"start\":57441},{\"end\":57463,\"start\":57456},{\"end\":57477,\"start\":57467},{\"end\":57887,\"start\":57881},{\"end\":57897,\"start\":57891},{\"end\":57904,\"start\":57901},{\"end\":57914,\"start\":57908},{\"end\":57925,\"start\":57918},{\"end\":58258,\"start\":58255},{\"end\":58265,\"start\":58262},{\"end\":58274,\"start\":58271},{\"end\":58634,\"start\":58628},{\"end\":58646,\"start\":58638},{\"end\":58656,\"start\":58650},{\"end\":58665,\"start\":58660},{\"end\":58676,\"start\":58669},{\"end\":58686,\"start\":58680},{\"end\":58699,\"start\":58690},{\"end\":58707,\"start\":58703},{\"end\":59028,\"start\":59024},{\"end\":59035,\"start\":59032},{\"end\":59043,\"start\":59039},{\"end\":59049,\"start\":59047},{\"end\":59062,\"start\":59053},{\"end\":59073,\"start\":59068},{\"end\":59082,\"start\":59077},{\"end\":59423,\"start\":59417},{\"end\":59432,\"start\":59427},{\"end\":59441,\"start\":59436},{\"end\":59450,\"start\":59445},{\"end\":59462,\"start\":59454},{\"end\":59472,\"start\":59466},{\"end\":59483,\"start\":59476},{\"end\":59490,\"start\":59487},{\"end\":59504,\"start\":59494},{\"end\":59514,\"start\":59508},{\"end\":59527,\"start\":59518},{\"end\":59535,\"start\":59531},{\"end\":59543,\"start\":59539},{\"end\":59553,\"start\":59547},{\"end\":59563,\"start\":59557},{\"end\":59573,\"start\":59567},{\"end\":59589,\"start\":59577},{\"end\":59600,\"start\":59593},{\"end\":59608,\"start\":59604},{\"end\":59615,\"start\":59612},{\"end\":59627,\"start\":59619},{\"end\":60486,\"start\":60477},{\"end\":60492,\"start\":60490},{\"end\":60505,\"start\":60496},{\"end\":60517,\"start\":60509},{\"end\":60531,\"start\":60521},{\"end\":60904,\"start\":60894},{\"end\":60915,\"start\":60908},{\"end\":60926,\"start\":60919},{\"end\":60938,\"start\":60930},{\"end\":60957,\"start\":60942},{\"end\":60969,\"start\":60961},{\"end\":61322,\"start\":61315},{\"end\":61331,\"start\":61328},{\"end\":61342,\"start\":61335},{\"end\":61352,\"start\":61346},{\"end\":61359,\"start\":61356},{\"end\":61370,\"start\":61363},{\"end\":61380,\"start\":61374},{\"end\":61390,\"start\":61384},{\"end\":61401,\"start\":61394},{\"end\":61410,\"start\":61405},{\"end\":61421,\"start\":61414},{\"end\":61434,\"start\":61425},{\"end\":61925,\"start\":61919},{\"end\":61935,\"start\":61929},{\"end\":61942,\"start\":61939},{\"end\":61950,\"start\":61946},{\"end\":61958,\"start\":61954},{\"end\":61969,\"start\":61962},{\"end\":61977,\"start\":61973},{\"end\":61990,\"start\":61981},{\"end\":62412,\"start\":62404},{\"end\":62421,\"start\":62416},{\"end\":62431,\"start\":62425},{\"end\":62447,\"start\":62437},{\"end\":62660,\"start\":62656},{\"end\":62669,\"start\":62664},{\"end\":62676,\"start\":62673},{\"end\":62690,\"start\":62680},{\"end\":62701,\"start\":62694},{\"end\":62708,\"start\":62705},{\"end\":63137,\"start\":63127},{\"end\":63147,\"start\":63141},{\"end\":63160,\"start\":63151},{\"end\":63170,\"start\":63164},{\"end\":63178,\"start\":63174},{\"end\":63189,\"start\":63182},{\"end\":63201,\"start\":63193},{\"end\":63710,\"start\":63699},{\"end\":63721,\"start\":63714},{\"end\":63729,\"start\":63725},{\"end\":64086,\"start\":64081},{\"end\":64101,\"start\":64090},{\"end\":64109,\"start\":64105},{\"end\":64375,\"start\":64369},{\"end\":64384,\"start\":64379},{\"end\":64396,\"start\":64388},{\"end\":64806,\"start\":64797},{\"end\":64815,\"start\":64810},{\"end\":65163,\"start\":65156},{\"end\":65172,\"start\":65167},{\"end\":65181,\"start\":65176},{\"end\":65190,\"start\":65185},{\"end\":65489,\"start\":65485},{\"end\":65497,\"start\":65493},{\"end\":65506,\"start\":65501},{\"end\":65788,\"start\":65782},{\"end\":65800,\"start\":65792},{\"end\":65810,\"start\":65804},{\"end\":65822,\"start\":65814},{\"end\":65839,\"start\":65830},{\"end\":65847,\"start\":65843},{\"end\":65859,\"start\":65851},{\"end\":65869,\"start\":65863},{\"end\":66168,\"start\":66165},{\"end\":66178,\"start\":66172},{\"end\":66188,\"start\":66182},{\"end\":66201,\"start\":66192},{\"end\":66213,\"start\":66205},{\"end\":66470,\"start\":66462},{\"end\":66481,\"start\":66474},{\"end\":66490,\"start\":66485},{\"end\":66499,\"start\":66494},{\"end\":66681,\"start\":66678},{\"end\":66690,\"start\":66685},{\"end\":66698,\"start\":66694},{\"end\":66707,\"start\":66704},{\"end\":66715,\"start\":66711},{\"end\":66725,\"start\":66721},{\"end\":67011,\"start\":67009},{\"end\":67020,\"start\":67015},{\"end\":67029,\"start\":67024},{\"end\":67038,\"start\":67033},{\"end\":67045,\"start\":67042},{\"end\":67054,\"start\":67049},{\"end\":67060,\"start\":67058},{\"end\":67585,\"start\":67580},{\"end\":67591,\"start\":67589},{\"end\":67597,\"start\":67595},{\"end\":67606,\"start\":67601},{\"end\":67614,\"start\":67610},{\"end\":67623,\"start\":67618},{\"end\":67636,\"start\":67629},{\"end\":68124,\"start\":68119},{\"end\":68130,\"start\":68128},{\"end\":68136,\"start\":68134},{\"end\":68145,\"start\":68140},{\"end\":68153,\"start\":68149},{\"end\":68162,\"start\":68157},{\"end\":68175,\"start\":68168}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":244714366},\"end\":49593,\"start\":49160},{\"attributes\":{\"doi\":\"arXiv:2103.10951\",\"id\":\"b1\"},\"end\":49843,\"start\":49595},{\"attributes\":{\"id\":\"b2\"},\"end\":50122,\"start\":49845},{\"attributes\":{\"id\":\"b3\"},\"end\":50417,\"start\":50124},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":232417173},\"end\":50907,\"start\":50419},{\"attributes\":{\"id\":\"b5\"},\"end\":51052,\"start\":50909},{\"attributes\":{\"doi\":\"arXiv:2101.11075\",\"id\":\"b6\"},\"end\":51404,\"start\":51054},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4855369},\"end\":51789,\"start\":51406},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":229297973},\"end\":52114,\"start\":51791},{\"attributes\":{\"doi\":\"arXiv:2106.14843\",\"id\":\"b9\"},\"end\":52410,\"start\":52116},{\"attributes\":{\"doi\":\"arXiv:2108.00946\",\"id\":\"b10\"},\"end\":52726,\"start\":52412},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":219955663},\"end\":52997,\"start\":52728},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":244799255},\"end\":53470,\"start\":52999},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":196834800},\"end\":53778,\"start\":53472},{\"attributes\":{\"id\":\"b14\"},\"end\":54067,\"start\":53780},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":209202273},\"end\":54460,\"start\":54069},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":237605410},\"end\":54717,\"start\":54462},{\"attributes\":{\"doi\":\"arXiv:2110.02711\",\"id\":\"b17\"},\"end\":54961,\"start\":54719},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":216622033},\"end\":55322,\"start\":54963},{\"attributes\":{\"doi\":\"arXiv:2112.00374\",\"id\":\"b19\"},\"end\":55554,\"start\":55324},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":209077411},\"end\":55984,\"start\":55556},{\"attributes\":{\"doi\":\"arXiv:1701.03754\",\"id\":\"b21\"},\"end\":56296,\"start\":55986},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14113767},\"end\":56667,\"start\":56298},{\"attributes\":{\"doi\":\"arXiv:2112.01573\",\"id\":\"b23\"},\"end\":57023,\"start\":56669},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":221737172},\"end\":57355,\"start\":57025},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":234680114},\"end\":57877,\"start\":57357},{\"attributes\":{\"doi\":\"arXiv:2112.03221\",\"id\":\"b26\"},\"end\":58161,\"start\":57879},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":53103701},\"end\":58530,\"start\":58163},{\"attributes\":{\"doi\":\"arXiv:2112.10741\",\"id\":\"b28\"},\"end\":58970,\"start\":58532},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":220280381},\"end\":59343,\"start\":58972},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202786778},\"end\":60416,\"start\":59345},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":232428282},\"end\":60890,\"start\":60418},{\"attributes\":{\"doi\":\"arXiv:1704.00675\",\"id\":\"b32\"},\"end\":61240,\"start\":60892},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":231591445},\"end\":61879,\"start\":61242},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":232035663},\"end\":62344,\"start\":61881},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":15809974},\"end\":62602,\"start\":62346},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1563370},\"end\":63053,\"start\":62604},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":220936362},\"end\":63630,\"start\":63055},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3719281},\"end\":64041,\"start\":63632},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":47476652},\"end\":64298,\"start\":64043},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":145052179},\"end\":64734,\"start\":64300},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2341530},\"end\":65089,\"start\":64736},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":208002447},\"end\":65444,\"start\":65091},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":222140788},\"end\":65711,\"start\":65446},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":216868355},\"end\":66107,\"start\":65713},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":231802331},\"end\":66402,\"start\":66109},{\"attributes\":{\"doi\":\"arXiv:2201.00424\",\"id\":\"b46\"},\"end\":66674,\"start\":66404},{\"attributes\":{\"doi\":\"arXiv:2101.05278\",\"id\":\"b47\"},\"end\":66908,\"start\":66676},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":8858625},\"end\":67480,\"start\":66910},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":1277217},\"end\":68030,\"start\":67482},{\"attributes\":{\"id\":\"b50\"},\"end\":68470,\"start\":68032}]", "bib_title": "[{\"end\":49219,\"start\":49160},{\"end\":50516,\"start\":50419},{\"end\":51455,\"start\":51406},{\"end\":51846,\"start\":51791},{\"end\":52768,\"start\":52728},{\"end\":53056,\"start\":52999},{\"end\":53498,\"start\":53472},{\"end\":54122,\"start\":54069},{\"end\":54513,\"start\":54462},{\"end\":55026,\"start\":54963},{\"end\":55595,\"start\":55556},{\"end\":56339,\"start\":56298},{\"end\":57078,\"start\":57025},{\"end\":57414,\"start\":57357},{\"end\":58251,\"start\":58163},{\"end\":59020,\"start\":58972},{\"end\":59413,\"start\":59345},{\"end\":60473,\"start\":60418},{\"end\":61311,\"start\":61242},{\"end\":61915,\"start\":61881},{\"end\":62400,\"start\":62346},{\"end\":62650,\"start\":62604},{\"end\":63123,\"start\":63055},{\"end\":63695,\"start\":63632},{\"end\":64077,\"start\":64043},{\"end\":64363,\"start\":64300},{\"end\":64793,\"start\":64736},{\"end\":65152,\"start\":65091},{\"end\":65481,\"start\":65446},{\"end\":65778,\"start\":65713},{\"end\":66161,\"start\":66109},{\"end\":67005,\"start\":66910},{\"end\":67576,\"start\":67482},{\"end\":68115,\"start\":68032}]", "bib_author": "[{\"end\":49233,\"start\":49221},{\"end\":49247,\"start\":49233},{\"end\":49256,\"start\":49247},{\"end\":49604,\"start\":49597},{\"end\":49616,\"start\":49604},{\"end\":49623,\"start\":49616},{\"end\":49631,\"start\":49623},{\"end\":49643,\"start\":49631},{\"end\":49652,\"start\":49643},{\"end\":49664,\"start\":49652},{\"end\":49964,\"start\":49951},{\"end\":50193,\"start\":50184},{\"end\":50204,\"start\":50193},{\"end\":50213,\"start\":50204},{\"end\":50222,\"start\":50213},{\"end\":50232,\"start\":50222},{\"end\":50246,\"start\":50232},{\"end\":50256,\"start\":50246},{\"end\":50528,\"start\":50518},{\"end\":50535,\"start\":50528},{\"end\":50543,\"start\":50535},{\"end\":50922,\"start\":50911},{\"end\":51181,\"start\":51170},{\"end\":51192,\"start\":51181},{\"end\":51465,\"start\":51457},{\"end\":51471,\"start\":51465},{\"end\":51477,\"start\":51471},{\"end\":51484,\"start\":51477},{\"end\":51857,\"start\":51848},{\"end\":51868,\"start\":51857},{\"end\":51877,\"start\":51868},{\"end\":52204,\"start\":52195},{\"end\":52213,\"start\":52204},{\"end\":52226,\"start\":52213},{\"end\":52484,\"start\":52477},{\"end\":52497,\"start\":52484},{\"end\":52506,\"start\":52497},{\"end\":52517,\"start\":52506},{\"end\":52529,\"start\":52517},{\"end\":52776,\"start\":52770},{\"end\":52784,\"start\":52776},{\"end\":52794,\"start\":52784},{\"end\":53066,\"start\":53058},{\"end\":53080,\"start\":53066},{\"end\":53092,\"start\":53080},{\"end\":53102,\"start\":53092},{\"end\":53111,\"start\":53102},{\"end\":53512,\"start\":53500},{\"end\":53529,\"start\":53512},{\"end\":53539,\"start\":53529},{\"end\":53548,\"start\":53539},{\"end\":53557,\"start\":53548},{\"end\":53563,\"start\":53557},{\"end\":53576,\"start\":53563},{\"end\":53586,\"start\":53576},{\"end\":53788,\"start\":53780},{\"end\":53796,\"start\":53788},{\"end\":53804,\"start\":53796},{\"end\":53810,\"start\":53804},{\"end\":53816,\"start\":53810},{\"end\":53824,\"start\":53816},{\"end\":54134,\"start\":54124},{\"end\":54143,\"start\":54134},{\"end\":54154,\"start\":54143},{\"end\":54166,\"start\":54154},{\"end\":54178,\"start\":54166},{\"end\":54186,\"start\":54178},{\"end\":54525,\"start\":54515},{\"end\":54533,\"start\":54525},{\"end\":54541,\"start\":54533},{\"end\":54550,\"start\":54541},{\"end\":54726,\"start\":54719},{\"end\":54734,\"start\":54726},{\"end\":55040,\"start\":55028},{\"end\":55051,\"start\":55040},{\"end\":55068,\"start\":55051},{\"end\":55332,\"start\":55324},{\"end\":55340,\"start\":55332},{\"end\":55603,\"start\":55597},{\"end\":55609,\"start\":55603},{\"end\":55624,\"start\":55609},{\"end\":55634,\"start\":55624},{\"end\":56074,\"start\":56067},{\"end\":56084,\"start\":56074},{\"end\":56091,\"start\":56084},{\"end\":56103,\"start\":56091},{\"end\":56350,\"start\":56341},{\"end\":56359,\"start\":56350},{\"end\":56371,\"start\":56359},{\"end\":56379,\"start\":56371},{\"end\":56389,\"start\":56379},{\"end\":56400,\"start\":56389},{\"end\":56410,\"start\":56400},{\"end\":56423,\"start\":56410},{\"end\":56769,\"start\":56762},{\"end\":56777,\"start\":56769},{\"end\":56783,\"start\":56777},{\"end\":56792,\"start\":56783},{\"end\":56798,\"start\":56792},{\"end\":56805,\"start\":56798},{\"end\":57086,\"start\":57080},{\"end\":57094,\"start\":57086},{\"end\":57103,\"start\":57094},{\"end\":57110,\"start\":57103},{\"end\":57123,\"start\":57110},{\"end\":57134,\"start\":57123},{\"end\":57147,\"start\":57134},{\"end\":57161,\"start\":57147},{\"end\":57422,\"start\":57416},{\"end\":57430,\"start\":57422},{\"end\":57439,\"start\":57430},{\"end\":57452,\"start\":57439},{\"end\":57465,\"start\":57452},{\"end\":57479,\"start\":57465},{\"end\":57889,\"start\":57879},{\"end\":57899,\"start\":57889},{\"end\":57906,\"start\":57899},{\"end\":57916,\"start\":57906},{\"end\":57927,\"start\":57916},{\"end\":58260,\"start\":58253},{\"end\":58267,\"start\":58260},{\"end\":58276,\"start\":58267},{\"end\":58636,\"start\":58626},{\"end\":58648,\"start\":58636},{\"end\":58658,\"start\":58648},{\"end\":58667,\"start\":58658},{\"end\":58678,\"start\":58667},{\"end\":58688,\"start\":58678},{\"end\":58701,\"start\":58688},{\"end\":58709,\"start\":58701},{\"end\":59030,\"start\":59022},{\"end\":59037,\"start\":59030},{\"end\":59045,\"start\":59037},{\"end\":59051,\"start\":59045},{\"end\":59064,\"start\":59051},{\"end\":59075,\"start\":59064},{\"end\":59084,\"start\":59075},{\"end\":59425,\"start\":59415},{\"end\":59434,\"start\":59425},{\"end\":59443,\"start\":59434},{\"end\":59452,\"start\":59443},{\"end\":59464,\"start\":59452},{\"end\":59474,\"start\":59464},{\"end\":59485,\"start\":59474},{\"end\":59492,\"start\":59485},{\"end\":59506,\"start\":59492},{\"end\":59516,\"start\":59506},{\"end\":59529,\"start\":59516},{\"end\":59537,\"start\":59529},{\"end\":59545,\"start\":59537},{\"end\":59555,\"start\":59545},{\"end\":59565,\"start\":59555},{\"end\":59575,\"start\":59565},{\"end\":59591,\"start\":59575},{\"end\":59602,\"start\":59591},{\"end\":59610,\"start\":59602},{\"end\":59617,\"start\":59610},{\"end\":59629,\"start\":59617},{\"end\":60488,\"start\":60475},{\"end\":60494,\"start\":60488},{\"end\":60507,\"start\":60494},{\"end\":60519,\"start\":60507},{\"end\":60533,\"start\":60519},{\"end\":60906,\"start\":60892},{\"end\":60917,\"start\":60906},{\"end\":60928,\"start\":60917},{\"end\":60940,\"start\":60928},{\"end\":60959,\"start\":60940},{\"end\":60971,\"start\":60959},{\"end\":61324,\"start\":61313},{\"end\":61333,\"start\":61324},{\"end\":61344,\"start\":61333},{\"end\":61354,\"start\":61344},{\"end\":61361,\"start\":61354},{\"end\":61372,\"start\":61361},{\"end\":61382,\"start\":61372},{\"end\":61392,\"start\":61382},{\"end\":61403,\"start\":61392},{\"end\":61412,\"start\":61403},{\"end\":61423,\"start\":61412},{\"end\":61436,\"start\":61423},{\"end\":61927,\"start\":61917},{\"end\":61937,\"start\":61927},{\"end\":61944,\"start\":61937},{\"end\":61952,\"start\":61944},{\"end\":61960,\"start\":61952},{\"end\":61971,\"start\":61960},{\"end\":61979,\"start\":61971},{\"end\":61992,\"start\":61979},{\"end\":62414,\"start\":62402},{\"end\":62423,\"start\":62414},{\"end\":62433,\"start\":62423},{\"end\":62449,\"start\":62433},{\"end\":62662,\"start\":62652},{\"end\":62671,\"start\":62662},{\"end\":62678,\"start\":62671},{\"end\":62692,\"start\":62678},{\"end\":62703,\"start\":62692},{\"end\":62710,\"start\":62703},{\"end\":63139,\"start\":63125},{\"end\":63149,\"start\":63139},{\"end\":63162,\"start\":63149},{\"end\":63172,\"start\":63162},{\"end\":63180,\"start\":63172},{\"end\":63191,\"start\":63180},{\"end\":63203,\"start\":63191},{\"end\":63712,\"start\":63697},{\"end\":63723,\"start\":63712},{\"end\":63731,\"start\":63723},{\"end\":64088,\"start\":64079},{\"end\":64103,\"start\":64088},{\"end\":64111,\"start\":64103},{\"end\":64377,\"start\":64365},{\"end\":64386,\"start\":64377},{\"end\":64398,\"start\":64386},{\"end\":64808,\"start\":64795},{\"end\":64817,\"start\":64808},{\"end\":65165,\"start\":65154},{\"end\":65174,\"start\":65165},{\"end\":65183,\"start\":65174},{\"end\":65192,\"start\":65183},{\"end\":65491,\"start\":65483},{\"end\":65499,\"start\":65491},{\"end\":65508,\"start\":65499},{\"end\":65790,\"start\":65780},{\"end\":65802,\"start\":65790},{\"end\":65812,\"start\":65802},{\"end\":65824,\"start\":65812},{\"end\":65841,\"start\":65824},{\"end\":65849,\"start\":65841},{\"end\":65861,\"start\":65849},{\"end\":65871,\"start\":65861},{\"end\":66170,\"start\":66163},{\"end\":66180,\"start\":66170},{\"end\":66190,\"start\":66180},{\"end\":66203,\"start\":66190},{\"end\":66215,\"start\":66203},{\"end\":66472,\"start\":66460},{\"end\":66483,\"start\":66472},{\"end\":66492,\"start\":66483},{\"end\":66501,\"start\":66492},{\"end\":66683,\"start\":66676},{\"end\":66692,\"start\":66683},{\"end\":66700,\"start\":66692},{\"end\":66709,\"start\":66700},{\"end\":66717,\"start\":66709},{\"end\":66727,\"start\":66717},{\"end\":67013,\"start\":67007},{\"end\":67022,\"start\":67013},{\"end\":67031,\"start\":67022},{\"end\":67040,\"start\":67031},{\"end\":67047,\"start\":67040},{\"end\":67056,\"start\":67047},{\"end\":67062,\"start\":67056},{\"end\":67587,\"start\":67578},{\"end\":67593,\"start\":67587},{\"end\":67599,\"start\":67593},{\"end\":67608,\"start\":67599},{\"end\":67616,\"start\":67608},{\"end\":67625,\"start\":67616},{\"end\":67638,\"start\":67625},{\"end\":68126,\"start\":68117},{\"end\":68132,\"start\":68126},{\"end\":68138,\"start\":68132},{\"end\":68147,\"start\":68138},{\"end\":68155,\"start\":68147},{\"end\":68164,\"start\":68155},{\"end\":68177,\"start\":68164}]", "bib_venue": "[{\"end\":49335,\"start\":49256},{\"end\":49949,\"start\":49845},{\"end\":50182,\"start\":50124},{\"end\":50621,\"start\":50543},{\"end\":51168,\"start\":51054},{\"end\":51557,\"start\":51484},{\"end\":51942,\"start\":51877},{\"end\":52193,\"start\":52116},{\"end\":52475,\"start\":52412},{\"end\":52853,\"start\":52794},{\"end\":53190,\"start\":53111},{\"end\":53614,\"start\":53586},{\"end\":53913,\"start\":53824},{\"end\":54248,\"start\":54186},{\"end\":54578,\"start\":54550},{\"end\":54818,\"start\":54750},{\"end\":55133,\"start\":55068},{\"end\":55417,\"start\":55356},{\"end\":55722,\"start\":55634},{\"end\":56065,\"start\":55986},{\"end\":56468,\"start\":56423},{\"end\":56760,\"start\":56669},{\"end\":57177,\"start\":57161},{\"end\":57567,\"start\":57479},{\"end\":57995,\"start\":57943},{\"end\":58335,\"start\":58276},{\"end\":58624,\"start\":58532},{\"end\":59143,\"start\":59084},{\"end\":59678,\"start\":59629},{\"end\":60611,\"start\":60533},{\"end\":61040,\"start\":60987},{\"end\":61511,\"start\":61436},{\"end\":62067,\"start\":61992},{\"end\":62465,\"start\":62449},{\"end\":62785,\"start\":62710},{\"end\":63291,\"start\":63203},{\"end\":63817,\"start\":63731},{\"end\":64161,\"start\":64111},{\"end\":64476,\"start\":64398},{\"end\":64904,\"start\":64817},{\"end\":65256,\"start\":65192},{\"end\":65564,\"start\":65508},{\"end\":65899,\"start\":65871},{\"end\":66243,\"start\":66215},{\"end\":66458,\"start\":66404},{\"end\":66766,\"start\":66743},{\"end\":67146,\"start\":67062},{\"end\":67711,\"start\":67638},{\"end\":68239,\"start\":68177},{\"end\":49401,\"start\":49337},{\"end\":50686,\"start\":50623},{\"end\":51617,\"start\":51559},{\"end\":53256,\"start\":53192},{\"end\":55797,\"start\":55724},{\"end\":57642,\"start\":57569},{\"end\":60676,\"start\":60613},{\"end\":61573,\"start\":61513},{\"end\":62129,\"start\":62069},{\"end\":62847,\"start\":62787},{\"end\":63366,\"start\":63293},{\"end\":64541,\"start\":64478},{\"end\":67217,\"start\":67148},{\"end\":67771,\"start\":67713}]"}}}, "year": 2023, "month": 12, "day": 17}