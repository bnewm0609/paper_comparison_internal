{"id": 251371732, "updated": "2023-10-20 13:19:05.19", "metadata": {"title": "Atlas: Few-shot Learning with Retrieval Augmented Language Models", "authors": "[{\"first\":\"Gautier\",\"last\":\"Izacard\",\"middle\":[]},{\"first\":\"Patrick\",\"last\":\"Lewis\",\"middle\":[]},{\"first\":\"Maria\",\"last\":\"Lomeli\",\"middle\":[]},{\"first\":\"Lucas\",\"last\":\"Hosseini\",\"middle\":[]},{\"first\":\"Fabio\",\"last\":\"Petroni\",\"middle\":[]},{\"first\":\"Timo\",\"last\":\"Schick\",\"middle\":[]},{\"first\":\"Jane\",\"last\":\"Dwivedi-Yu\",\"middle\":[]},{\"first\":\"Armand\",\"last\":\"Joulin\",\"middle\":[]},{\"first\":\"Sebastian\",\"last\":\"Riedel\",\"middle\":[]},{\"first\":\"Edouard\",\"last\":\"Grave\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2208.03299", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/jmlr/IzacardLLHPSDJRG23", "doi": "10.48550/arxiv.2208.03299"}}, "content": {"source": {"pdf_hash": "916be31cbf847faa65cad0549e153f0c25b9f424", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2208.03299v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "10605aea0a65d409d87340b298b2a25860e3a962", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/916be31cbf847faa65cad0549e153f0c25b9f424.txt", "contents": "\nAtlas: Few-shot Learning with Retrieval Augmented Language Models\n\n\nGautier Izacard gizacard@fb.com \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nPatrick Lewis plewis@fb.com \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nMaria Lomeli marialomeli@fb.com \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nLucas Hosseini \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nFabio Petroni fabiopetroni@fb.com \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nTimo Schick schick@fb.com \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nJane Dwivedi-Yu janeyu@fb.com \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nArmand Joulin ajoulin@fb.com \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nSebastian Riedel sriedel@fb.com \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nEdouard Grave egrave@fb.com \nMeta AI Research, \u2663 ENS\nPSL University\n\u2665 Inria\n\n\u2660 University College London\n\n\nAtlas: Few-shot Learning with Retrieval Augmented Language Models\n\nLarge language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters. * equal contribution\n\nIntroduction\n\nLarge language models (LLMs) are impressive few-shot learners Rae et al., 2021;Hoffmann et al., 2022;Chowdhery et al., 2022). They are able to learn new tasks with very few examples or even from instructions alone. For this generalisation ability to emerge, the key ingredients are scaling both the parameter count of the model, and the size of the training data. Large language models owe this improvement to both a larger computational budget, enabling more complex reasoning, and the ability to memorize more information related to downstream tasks from the larger training data. While it is intuitive to assume that increased reasoning abilities lead to better generalisation, and hence few-shot learning, the same is not true for in-parameter memorisation. Specifically, it is unclear to what extent effective few-shot learning requires vast knowledge in the parameters of the model.\n\nIn this paper, we investigate whether few-shot learning requires models to store a large amount of information in their parameters, and if memorisation can be decoupled from generalisation. To do so, we leverage the fact that memory can be outsourced and replaced by an external non-parametric knowledge source by employing a retrieval-augmented architecture. These models employ a non-parametric memory, e.g. a neural retriever over a large, external, potentially non-static knowledge source to enhance a parametric language model. In addition to their memorisation abilities, such architectures are attractive due to a number of other established advantages in terms of adaptability, interpretability and efficiency (Guu et al., 2020;Borgeaud et al., 2021, inter alia). However, retrieval-augmented models have yet to \u2026 \u2026 Figure 1: We introduce Atlas, a retrieval-augmented language model that exhibits strong few-shot performance on knowledge tasks, and uses retrieval during both pre-training and fine-tuning.\n\ndemonstrate compelling few-shot learning capabilities. In this work we address this gap, and present Atlas, a retrieval-augmented language model capable of strong few-shot learning, despite having lower parameter counts than other powerful recent few-shot learners.\n\nAtlas retrieves relevant documents based on the current context by using a general-purpose dense retriever using a dual-encoder architecture, based on the Contriever . The retrieved documents are processed, along with the current context, by a sequence-to-sequence model using the Fusion-in-Decoder architecture ) that generates the corresponding output. We study the impact of different techniques to train Atlas on its few-shot performance on a range of downstream tasks, including question answering and fact checking. We find that jointly pre-training the components is crucial for few-shot performance, and we carefully evaluate a number of existing and novel pre-training tasks and schemes for this purpose. Atlas achieves strong downstream performance in both few-shot and resource-rich settings. For example, with only 11B parameters, Atlas achieves an accuracy of 42.4% on NaturalQuestions using 64 training examples (45.1% with a Wikipedia-only index), outperforming PaLM (Chowdhery et al., 2022), a 540B parameter model by almost 3 points, and 64.0% in a full-dataset setting with a Wikipedia index, establishing a new state of the art by 8 points.\n\nIn summary we make the following contributions:\n\n\u2022 A thorough study on how to design and train retrieval-augmented language models, with a focus on downstream few-shot learning and sample efficiency.\n\n\u2022 The findings of this study lead to a retrieval-augmented language model, called Atlas, that exhibits few-shot abilities that emerge at lower scale than standard LLM.\n\n\u2022 We provide an exploration of fine-tuning strategies to efficiently adapt both the retriever and the language model to the task at hand.\n\n\u2022 Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art results on few-shot NaturalQuestions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%), and results on par or stronger than models with 15\u00d7 more parameters on MMLU.\n\n\u2022 Experiments investigating full-dataset finetuning, setting new state-of-the-art results in NaturalQuestions (+8.1%), TriviaQA (+9.3%) and 5 KILT Tasks.\n\n\u2022 Experiments demonstrating the updatability and interpretability characteristics of Atlas.\n\n\u2022 Experiments demonstrating that a compressed index using product quantisation achieves comparable performance as an uncompressed index while resulting in a 5x memory reduction.\n\nOur code, pretrained Atlas checkpoints, and various supporting data are available at https://github.com/ facebookresearch/atlas\n\n\nTraining objectives for the retriever\n\nIn this section, we discuss four different loss functions to train the retriever jointly with the language model. We consider loss functions that leverage the language model to provide supervisory signal to train the retriever. In other words, if the language model finds a document useful when generating the output, the retriever objective should encourage the retriever to rank said document higher. This allows us to train models using only query and output pairs from the task of interest, without relying on document annotations. For example, in the case of fact checking, a model only requires pairs of claims and corresponding verdicts but no documents containing the evidence to back up the verdict. In practice, we can apply this approach on any task, including self-supervised pre-training. As shown in the experimental section, pre-training is critical for obtaining models that exhibit few-shot learning abilities.\n\nAttention Distillation (ADist). The first loss that we consider is based on the attention scores of the language model, and is heavily inspired by . The main idea is that the cross-attention scores between the input documents and the output, can be used as a proxy of the importance of each input document when generating the output. In particular,  showed that these scores can be aggregated across attention heads, layers and tokens for a given document to obtain a single score for each document. Then, these scores can be distilled into the retriever by minimizing the KL-divergence with the probability distribution p retr over the top-K documents {d k } 1,...,K obtained from the retriever:\np retr (d | q) = exp(s(d, q)/\u03b8) K k=1 exp(s(d k , q)/\u03b8) ,(1)\nwhere s is the dot-product between the query and documents vectors and \u03b8 is a temperature hyper-parameter.\n\nIn the original paper, it was proposed to use the pre-softmax scores from the decoder cross-attentions, and average across heads, layers and tokens. Here, we propose an alternative which gives slightly stronger results, which relies on the following observation. In the attention mechanism, as defined by y = N n=1 \u03b1 n v n , the contribution to the output y of a particular token n cannot be evaluated from the attention score \u03b1 n alone, but should also take the norm of the value v n into account. Hence, we use the quantity \u03b1 n v n 2 as the measure of relevance for token n. Following , we average these scores over all attention heads, layers, and tokens to obtain a score for each document. We apply the Softmax operator over the resulting scores, to obtain a distribution p attn (d k ) over the top-K retrieved documents. We then minimize the KL-divergence between p attn (d k ), and the distribution p retr from the retriever defined in Equation 1:\nKL(p attn p retr ) = K k=1 p attn (d k ) log p attn (d k ) p retr (d k ) .\nHere, this loss is only used to optimize the parameters of the retriever, and not the language model. When using recent deep learning frameworks, this is achieved by applying a StopGradient operator on p attn .\n\nEnd-to-end training of Multi-Document Reader and Retriever (EMDR 2 ). Next, we consider the method introduced by Sachan et al. (2021), which is inspired by the expectation-maximization algorithm, treating retrieved documents as latent variables. Given a query q, the corresponding output a and the set D K of top-K retrieved documents with the current retriever, the EMDR 2 loss to train the retriever is\nlog K k=1 p lm (a | q, d k )p retr (d k | q) ,\nwhere p retr is again the probability over the top-K documents obtained with the retriever, as defined by Equation 1. Again, only the parameters of the retriever are updated by applying a StopGradient operator around p lm . One should note that the probability distribution over documents that maximizes this loss function is an indicator of the document corresponding to the highest probability of the output according to the language model. Finally, in practice, the EMDR 2 loss function is applied at the token level, and not at the sequence level.\n\nPerplexity Distillation (PDist). Third, we discuss a simpler loss function which is loosely inspired by the objectives from the attention distillation and EMDR 2 methods Sachan et al., 2021). More precisely, we want to train the retriever to predict how much each document would improve the language model perplexity of the output, given the query. To this end, we minimize the KL-divergence between the documents distribution of the retriever (Eqn. 1), and the documents posterior distribution according to the language model, using a uniform prior:\np k \u221d p LM (a | d k , q).\nUsing the Softmax operator, we have that\np k = exp(log p LM (a | d k , q)) K i=1 exp(log p LM (a | d i , q))\n.\n\nLeave-one-out Perplexity Distillation (LOOP). Finally, we propose an objective based on how much worse the prediction of the language model gets, when removing one of the top-k retrieved documents. To do so, we compute the log probability of the output for each subset of k-1 documents, and use the negative value as relevance score for each document. Following the previous loss function, we use the softmax operator to obtain a probability distribution over documents:\np loop (d k ) = exp(\u2212 log p LM (a | D K \\ {d k }, q)) K i=1 exp(\u2212 log p LM (a | D K \\ {d i }, q))\n.\n\nAs before, we then minimize the KL-divergence between this distribution, and the one obtained with retriever. This loss is more expensive to compute than PDist and EMDR, but, like ADist, employs the language model more closely to the way it is trained i.e. the LM is trained to be conditioned on a set of K documents. For LOOP, the language model is conditioned on (K \u2212 1) documents, rather than a single document as in EMDR 2 and PDist.\n\nFor all losses, we can also use a temperature hyper-parameter when computing the target or retriever distributions to control the distribution's peakiness of, which might be important for some tasks or losses. Indeed, for PDist and LOOP, the perplexity of the output may not vary much when conditioning on different documents, especially in the case of long outputs.\n\n\nPretext tasks\n\nIn this section, we describe pretext tasks that can be used to jointly pre-train the retriever and the language model using only unsupervised data.\n\nPrefix language modeling. First, we consider a standard language modeling task as potential pre-training objective. To cast language modeling in the text-to-text framework, we consider a chunk of N words, and split this chunk in two sub-sequences of equal length N/2. Then, the first sub-sequence is used as the query, and the second corresponds to the output. We thus retrieve relevant documents by using the first sub-sequence of N/2 tokens, to generate the output.\n\nMasked language modeling. Second, we consider masked language modeling, as formulated by Raffel et al. (2019). Again, starting from a chunk of N words, we sample k spans of average length 3 tokens, leading to a masking ratio of 15%. We then replace each span by a different special token. The model is then trained to generate the masked spans, each span beginning with the special sentinel mask token that was inserted in the input sequence. We retrieve documents using the masked query, but replace the special mask tokens with a mask token supported by the retriever vocabulary.\n\nTitle to section generation. Finally, we consider a more abstractive generation task, generating sections from Wikipedia articles, given the article and section title. Here, the query corresponds to the title of the article, together with the title of the section, and the output corresponds to the text of the section. We exclude sections \"See also\", \"References\", \"Further reading\" and \"External links\".\n\n\nEfficient retriever fine-tuning\n\nRetrieval is facilitated by using a document index, which is a pre-computed collection of the document embeddings for all the documents in the retrieval corpus. When jointly training the retriever and language model, the index needs to be updated regularly, otherwise, the embeddings of the documents stored in the index become stale relative to the updated retriever. This means that we need to recompute the embeddings for the full collection of documents regularly during training to keep the index fresh, which can be computationally expensive for large indices. This is particularly true at fine-tuning time, where the number of training examples could be small relative to the number of documents in the index. Training the retriever could thus add an important computational overhead compared to standard language model finetuning. In this section, we analyse strategies that might make this process more efficient, alleviating the need to re-compute the embeddings of all the documents too often.\n\nFull index update. Let us start by analysing the overhead due to updating the index, compared to using a fixed retriever. To compare the computation time of different models, we will make the following assumption: the time required to perform a forward pass on a document with a model of P parameters is O(P ). While this computation model may seem naive, the main assumption is that document sizes are constant. 1 Since we split long documents into passages with similar number of words, and use padding when processing documents of different sizes, this assumption is reasonable in practice. Let K be the number of documents that are retrieved and processed by the language model, P lm be the number of parameters of the language model and B the batch size. Each training step has a complexity of 4 \u00d7 B \u00d7 K \u00d7 P lm . 2\n\nNext, let N be the number of documents in the index, and P retr be the number of parameters of the retriever. Then, re-computing the full index has a complexity of N \u00d7 P retr . If we refresh the index every R training steps, we obtain the following overhead:\nN \u00d7 P retr 4 \u00d7 B \u00d7 K \u00d7 P lm \u00d7 R .\nIf we use the BERT-base architecture for our retriever and T5-XL for our language model, we get Pretr Plm \u2248 1 25 , lading to the overhead:\nN 100 \u00d7 B \u00d7 K \u00d7 R .\nIf we use an index containing 37M documents (the size of our Wikipedia index), train with a batch size of 64 with 20 retrieved documents and refresh the index every 1000 steps, this results in an overhead of \u223c 30%.\n\n\nRe-ranking.\n\nA second strategy is to retrieve a larger number of documents L with the retriever, and to re-embed and rerank these documents with the up-to-date retriever, and pass the resulting top-K to the language model. In that case, the overhead of reranking the top-L documents is equal to B \u00d7 L \u00d7 P retr . Since we perform this operation at every time step, the overhead is equal to\nL \u00d7 P retr 4 \u00d7 K \u00d7 P lm .\nUsing the same assumption as before, we finally get that the overhead is of the order of L 100\u00d7K . If we re-rank 10x more documents than what the language model processes (i.e., L = 10 \u00d7 K), we get an overhead of 10%. However, note that if many updates are performed on the retriever, the index might still need to be fully updated, as the true top-k documents may not be retrieved in the top-L results from the stale index. In practice, it is possible to track the positions of the top-K re-ranked documents in the top-L, and estimate when the index needs to be updated.\n\nQuery-side fine-tuning. Finally, the last strategy is to decouple the encoding of the queries and documents. In this case, we fix the parameters corresponding to the document encoder, and only train the parameters corresponding to the query encoder. Thus, the embeddings of documents are fixed, and we do not need to refresh the index, and thus there is no computational overhead. As we will see in practice, the impact of fixing the documents encoder varies greatly for different tasks when a large training dataset is available. For most of the few-shot settings that we consider, query-side finetuning does not have large performance impact, and sometimes even slightly improves performance.\n\n\nRelated work\n\n\nRetrieval in natural language processing\n\nRetrieval for knowledge intensive tasks. Previous work has shown that retrieval improves performance across a variety of tasks such as question answering (Voorhees et al., 1999;Chen et al., 2017;Kwiatkowski et al., 2019), fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommendation . Historically, this information retrieval step was implemented using term-matching methods, such as TF-IDF or BM25 (Jones, 1972;Robertson et al., 1995). For open-domain question answering (Voorhees et al., 1999), documents are often retrieved from Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have become popular. These usually follow a dual-encoder architecture (Yih et al., 2011;Huang et al., 2013;Shen et al., 2014), where queries and passages are encoded independently as vectors, and relevance is computed using the inner product or Euclidean distance. Popular supervised retrievers include DPR , which is trained to discriminate the relevant passage among negative passages, and extensions such as ANCE (Xiong et al., 2020) which improved the hard negatives mining process. We refer the reader to Yates et al. (2021) for a survey of dense retrieval techniques.\n\nAfter retrieval, the relevant documents are processed to produce the final output. In open-domain QA, models can extract a span of text from retrieved documents as the answer (Chen et al., 2017;Clark & Gardner, 2018;Wang et al., 2019;, a method inspired by reading comprehension (Richardson, 2013;Rajpurkar et al., 2016). Recently, generating the answer as free-form text, using a seq2seq model conditioned on retrieved documents have become prevalent . These architectures have also been shown to reduce hallucination in dialogue agents (Shuster et al., 2021).\n\n\nRetriever training.\n\nThe need for expensive query-document annotations for training the retriever can be bypassed, by leveraging signals from the language model, or using unsupervised learning. REALM (Guu et al., 2020) and RAG  jointly train the retriever and language model by modelling documents as latent variable, and minimizing the objective with gradient descent. REALM pre-trains end-to-end with an MLM approach but uses an extractive BERT-style model . Guu et al. (2020) also explore a query-side finetuning at finetuning time to avoid index refreshes, which is also explored in the context of phrase-based retrieval by Lee et al. (2021b).  proposed to use cross-attention scores as supervision with knowledge distillation. Sachan et al. (2021) perform joint training of the reader and the retriever by leveraging the perplexity of the output generated by the reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to pre-train retrievers, leveraging the perplexity and attention scores from the language model. The inverse cloze task was proposed by  to pre-train dense retrievers in an unsupervised way. Paranjape et al. (2021) propose a method to train retrieval-augmented generators using a second \"informed\" retriever with access to the output, which the test-time retriever can be distilled from, and Hofst\u00e4tter et al. (2022) recently proposed a training set filtering/weighting approach to train stronger retrieval-augmented generators.  explored different contrastive learning methods to train retrievers, while Ram et al. (2022) used recurring spans within a document to create pseudo-positive query-document pairs. Retrieval-augmented language models. Continuous cache models (Grave et al., 2017b) defines a probability distribution over recent tokens, by computing the similarity between previous and current representations of tokens. This distribution is then interpolated with the distribution of the language model, to improve predictions. Later, the amount of tokens used to compute this distribution was extended to a much larger memory by leveraging approximate nearest neighbors search (Grave et al., 2017a). The related kNN-LM model (Khandelwal et al., 2020) replaced LSTMs by transformer networks, and scaled the memory to billions of tokens, leading to strong performance improvements. More recently, RETRO (Borgeaud et al., 2021) extended these by scaling the retrieval memory to trillions of tokens, and changing the model architecture to take retrieved documents as input.\n\n\nRetrieval-Augmentation with Search Engines.\n\nRecently, different works have proposed to train large language models to interact with a search engine, by generating text queries, and using the retrieved documents as additional context (Nakano et al., 2021;Thoppilan et al., 2022;Shuster et al., 2022). In the context of few-shot question answering, Lazaridou et al. (2022) used the question to perform a search query, and retrieved documents are added to the prompt of a large language model performing in-context learning.\n\n\nFew-shot learning\n\nFew-shot learning, the task of learning from very few examples, has been studied for decades (Thrun & Pratt, 1998;Fink, 2005;Vinyals et al., 2016), but has recently seen an explosion of interest in NLP with the arrival of large pre-trained models, which exhibit emergent few-shot learning abilities (Wei et al., 2022).\n\nIn-context Learning with large Language models. Providing language models with natural language descriptions of tasks, as proposed by Radford et al. (2019) has led to significant developments in few-shot learning. GPT-3  demonstrated the ability of large language models to perform few-shot predictions, where the model is given a description of the task in natural language with few examples. Scaling model size, data and compute is crucial to enable this learning ability, leading to the further development of large models (Lieber et al., 2021;Rae et al., 2021;Smith et al., 2022;Chowdhery et al., 2022;Smith et al., 2022). Hoffmann et al. (2022) revisited the scaling law from , suggesting that training on more data with a smaller model may be more effective, resulting in Chinchilla, a 70B parameter model with improved parameter efficiency.\n\nFew-shot finetuning and prompt-based learning. The above models perform few-shot learning with in-context instructions without training the parameters of the language model. Few-shot learning can also be accomplished by combining textual templates (\"prompts\") and various forms of model finetuning, either fully updating a model's parameters, e.g. for classification (Schick & Sch\u00fctze, 2021a;Schick & Schutze, 2021;Gao et al., 2021;Tam et al., 2021) or generation (Schick & Sch\u00fctze, 2021b). Prompts themselves can be optimized, for example by search (Jiang et al., 2020;Shin et al., 2020) or by only updating parts of the model (Logan et al., 2021), or learning \"soft-prompts\" (Lester et al., 2021;Li & Liang, 2021). Due to its simplicity, in this work we either employ simple prompts or simply feed in inputs without preprocessing, and perform full-model finetuning, a method similar to Le Scao & Rush (2021).\n\n\nExperiments\n\nIn this section, we report empirical evaluations of our language models on few-shot learning. We start by introducing our experimental setup, describing our evaluation benchmarks in section 4.1, and giving the training details of our models in section 4.2. Then, we perform an ablation study to compare the different technical choices leading to our main model. We finally evaluate this model, called Atlas, on different natural language understanding tasks in few-shot and full dataset settings.\n\n\nBenchmarks\n\nTo evaluate our retrieval-augmented language models we consider the following benchmarks, which include different tasks.\n\n\nKnowledge-Intensive Language Tasks (KILT).\n\nFirst, we use the KILT evaluation suite , containing 11 datasets corresponding to 5 tasks: fact checking, question answering, dialog generation, entity linking and slot-filling. These different tasks require knowledge about the world to be solved, which can be found in Wikipedia. We evaluate our model on the following tasks and datasets included in KILT: question answering: NaturalQuestions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018); slot filling: Zero Shot RE (Levy et al., 2017) and T-REx (Elsahar et al., 2018); entity linking: AIDA CoNLL-YAGO (Hoffart et al., 2011); dialogue: Wizard of Wikipedia (Dinan et al., 2019); and fact checking: FEVER (Thorne et al., 2018). The KILT versions of these datasets differ from their original versions, as instances requiring knowledge not present in the August 2019 Wikipedia dump have been removed.\n\n\nMassively-Multitask Language Understanding (MMLU).\n\nOur second main evaluation benchmark is MMLU (Hendrycks et al., 2021), which contains 57 multi-choice question answering datasets (referred to as domains), sourced from real examinations designed for humans. These cover a very broad range of topics, e.g. high school mathematics, professional law, logical fallacies and clinical knowledge and can be broadly categorized in four subsets: humanities, social sciences, STEM and \"other\". We focus on few-shot learning, and the authors of the benchmarks suggest to use 5 training examples per domain. Beyond the 5-shot setting, We also consider three additional settings. The first is a zero-shot setting, with no training data at all. The second, which we call multi-task few-shot, is where we train a single model on the 6-shot data from all tasks, hence leading to a training set of 285 examples. The last, which we call transfer learning, leverages additional training examples from other multiple-choice QA tasks provided by the MMLU authors, namely MCTest (Richardson, 2013), RACE (Lai et al., 2017), ARC  and OBQA (Mihaylov et al., 2018) leading to a training set of 95k examples.\n\nAdditional benchmarks. Additionally, we report results on the original open-domain versions of the popular NaturalQuestions (Kwiatkowski et al., 2019), and TriviaQA (Joshi et al., 2017) datasets. We also evaluate our model on the original version of FEVER (Thorne et al., 2018), which presents fact checking as a three-way classification problem for textual claims (either \"Supported\": the text is supported by evidence in Wikipedia, \"refuted\": the claim is not consistent with evidence in Wikipedia, or \"not enough info\", where there is insufficient evidence to make a judgement). We also perform experiments to assess temporal sensitivity of our models. Here, we construct a dataset from TempLAMA (Dhingra et al., 2022), consisting of a set of time-sensitive cloze questions on a range of topics, where the answer changes from 2017 to 2020. We assess the accuracy of our models when supplied with a index from 2017 vs 2020 to assess to what degree models faithfully reflect the content of the index supplied to them at test time, and how effective updating the index is as a continual learning or model updateability method.\n\n\nTechnical details\n\nWe now describe the procedure for pre-training and fine-tuning our models. We focus on the setting used for the ablation studies performed in Section 4.3 and Section 4.4. We give more details about the hyperparameters used for our final model later.\n\n\nPre-training.\n\nFor the pre-training, we initialize the retriever module using the unsupervised Contriever model, which uses the BERT-base architecture. We initialize the language model with the T5 pre-trained weight. As the original T5 pre-trained model included supervised data in the training set, we use the version 1.1 models which were trained on unlabeled text only. Specifically, we initialize from the T5-lm-adapt variants due to their improved stability.\n\nFor the ablation studies performed in Section 4.3 and Section 4.4, we use T5-XL which contains 3B weights. We pre-train all our models for 10,000 iterations, using AdamW with a batch size of 64 and a learning rate of 10 \u22124 for the reader and 10 \u22125 for the retriever with linear decay and 1,000 warmup steps. We refresh the index every 1,000 steps. This means that the index is recomputed 10 times during the pre-training, leading to an overhead of around 30%, compared to training with a fixed retriever. We set the number of retrieved documents to 20. We detail the hyperparameters used for the training of our final model at the beginning of Section 4.5. Fine-tuning. When performing a downstream task, either in a few-shot setting or with a large training set, we employ fine-tuning to adapt our models to these tasks. For the few-shot KILT ablation experiments, we perform a fixed number of fine-tuning iterations, instead of using early-stopping. More precisely, we decided to use 50 iterations for the 64-shot setting and 200 iterations in the 1024-shot setting. In both cases, we use a batch size of 32 examples, a learning rate of 4 \u00d7 10 \u22125 with linear decay and 5 warmup steps for both the reader and the retriever.\n\nUnlabeled datasets. Finally, we discuss the unlabeled text datasets that we use to train our models, which form the retrieval index. First, we consider the Dec. 20, 2021 Wikipedia dump, for which we keep the lists and infoboxes, which are linearized by adding a semi-colon separator between the entries. We split articles by section, and split long sections into passages of equal sizes and containing less than 200 words. This leads to a total of 37M passages, containing 78 words in average. We also use documents from the 2020-10 common crawl dump, preprocessed with the CCNet pipeline (Wenzek et al., 2020). We perform additional document filtering, in a similar fashion to Gopher . More precisely, we filter documents based on document length, average word length, ratio of alphanumeric characters and number of repeated tokens. This leads to a total of 350M passages. The same passages are used for the index and model pre-training. During pre-training, we ensure the passage we are training on is filtered out from the retrieved documents, to prevent the model from simply retrieving the passage it is de-nosing/generating, and trivially using it to solve the pre-training task.\n\n\nPre-training loss and tasks\n\nWe start our ablation study by comparing different pre-training tasks, and objective functions to jointly train the retriever and the language model. Our goal here is to answer the following research questions:\n\n(RQ 1) Does jointly pre-training the whole model lead to better few-shot performance?\n\n(RQ 2) What is the best objective function for the retriever, and the best pretext task?\n\nWe start by comparing the training objectives of the retriever, introduced in Section 2.2, by pre-training models using the masked language modelling task. We evaluate these models on a subset of the 64-shot and 1024-shot KILT benchmark: NaturalQuestions, FEVER and Wizard of Wikipedia, along with two baselines: a 'closed-book\" (i.e. non-augmented T5) baseline, pre-trained on the same data, and initialized from Contriever and T5-lm-adapt. We report results in Table 1. First, we note the poor performance of the closed-book baseline, indicating the importance of augmentation. Next, we observe that pre-training our model with retrieval is important to obtain good performance on few-shot tasks. Indeed, all models that include retrieval during pre-training strongly outperform the baseline without joint pre-training. Next,  we compare a model that was pre-trained with a fixed retriever, and models using the various retriever training objectives. On the MLM validation metric corresponding to the pre-training objective, we observe that jointly training the retriever leads to strong improvements. This effect tends to be less marked on 64-shot downstream tasks, and almost non-existent for 1024-shot. We believe that this is evidence that the biggest impact of pre-training is on the language model, which learns to use and aggregate information from the retrieved documents. Lastly, we do not observe significant systematic differences between the different retriever training objectives. We thus decide adopt use Perplexity Distillation for subsequent experiments, as it tends to be more stable than EMDR 2 or ADist, and more computationally efficient than LOOP.\n\nNext, we compare the different self-supervised pretext tasks introduced in Section 2.3 in Table 2. Here we observe similar results for all three tasks, with a small advantage for masked language modelling. Thus, in what follows, we adopt masked language modelling for pre-training.\n\nFinally, we consider different combinations of data sources-Wikipedia and common crawl-for the index and training data during pre-training. In all cases, we use the Wikipedia 2021 dump as the index when performing few-shot fine-tuning. We report results in Table 3. First, we observe that using a Wikipedia-based index leads to better downstream performance. There could be two explanations for this: first, as we use Wikipedia for the few-shot tasks, the model might be better adapted when trained using the same data. Another explanation might be that Wikipedia is a higher-quality and denser source of knowledge than common crawl. Second, when using a common crawl index, we observe that pre-training on Wikipedia data leads to lower performance than using common crawl data. We believe that the primary reason is that the distribution mismatch between the two domains leads to generally-less relevant retrieved documents. In turn, this probably means that the pre-training is less efficient, because the language model does not leverage as much information from the documents. In the following, we thus decide to combine the data from both domains for both the index and the pre-training data.\n\n\nFine-tuning\n\nIn this section, we perform an ablation study on how to apply our models on downstream tasks, which relies on fine-tuning. In particular, we want to investigate the following research question:\n\n(RQ 3) How to efficiently fine-tune Atlas on tasks with limited training data? To answer this question, we compare the different strategies to fine-tune the retriever module, described in Section 2.4. We report results in Table 4. First, as for pre-training, we observe that keeping the retriever fixed during fine-tuning leads to a significant performance drops, for both 64-and 1024-shot settings. Second, the re-ranking strategy (row 2) leads to very similar results to fully updating the index (row 1), while being significantly more efficient. Lastly, fine-tuning only the query encoder also leads to strong results: in particular, in the 64-shot setup, this is slightly stronger than performing full fine-tuning, which we attribute to there being less opportunity for over-fitting. On the other hand, on the 1024-shot setting, performing a full fine-tuning leads to stronger results, especially on NaturalQuestions. In the following, we thus use query-side fine-tuning for experiments with small numbers of examples, and standard fine-tuning for larger datasets.\n\n\nTraining and evaluating Atlas\n\nIn this section, we apply the findings from the ablations of the previous sections to train a family of Atlas models, ranging from 770M to 11B parameters. More specifically, we use the Perplexity Distillation objective function, along with the masked language modelling pretext task. We pre-train these models using a mix of Wikipedia and Common Crawl data, for both the training data and content of the index. We retrieve 20 documents, and update the index every 2,500 steps and perform re-ranking of the top-100 documents. We pre-train models for 10,000 iterations using AdamW with a batch size of 128.\n\n\nMMLU Results\n\nAs mentioned in section 4.1, we consider four setting for MMLU: 1) a zero-shot setting where we directly apply the pretrained model with no few-shot finetuning 2) a 5-shot setting, where we finetune a model using 5 training examples for each of the 57 domains 3) a 5-shot multitask setting, where, rather than finetuning a model independently for each domain, we train a single model to perform all tasks and 4) a setting with access to a number of auxiliary datasets, with 95K total training examples. We train the models to generate the letter corresponding to the correct answer option ('A', 'B', 'C' or 'D'), and pick the answer with the most likely of the 4 letters at test time. Full technical details can be found in appendix A.1.\n\n\nPerformance vs Parameters.\n\nWe start by comparing Atlas to closed-book models of different sizes for 5-shot, 5-shot multitask and the full setting, and report results in Table 5. Across these settings, Atlas outperforms the closed-book baselines by between 6.6 and 15.6 points, demonstrating consistent utility of retrieval for few-shot language understanding across 57 domains. The closed-book T5 struggles to perform significantly better than random (25%) in few-shot settings with 770M parameters, whereas the equivalent Atlas achieves around 40%, significantly better than random, despite its small size. All models improve with more data, but interestingly, the 770M models do not benefit as much from few-shot multitask learning compared to larger models (for closed-book, it actually loses 3 points) suggesting smaller models struggle to grasp the synergies between the tasks in the few-shot setting. Larger models exploit the multi-task setting well, with Atlas improving more than closed-book. For example, Atlas-11B improves by 13 points (43.4 \u2192 56.4), but equivalent closed-book only improves by 7 (36.1 \u2192 43.5). Finally, on the transfer learning setting, all models improve, but the relative gaps between closed-book at Atlas models remain similar.\n\nDe-biasing. When finetuning, we permute which answer option appears with which answer letter to reduce over-fitting and encourage a uniform prior over answer letters. However, the model may still exhibit a bias towards some letters, especially in few-shot settings, so we also include a second 'de-biased' inference mode in addition the standard inference used above. Here, we run 4 forward passes, one for each cyclic permutation of the answer letter-answer option assignment in the question, e.g. the answer option assigned to letter 'A' becomes 'B', what was 'B' becomes 'C' etc. 3 We then sum the 4 probabilities to obtain the final prediction, which reduces spurious bias towards one of the answer letters (further details in appendix A.1). The results are shown in Table 6. We find that in zero-shot and 5-shot settings, de-biasing is very effective, improving results by 10.3 and 4.5 points respectively. When more training data is available, the need for de-biasing decreases, leading to only 0.2 point improvement in the multi-task and full data settings.\n\nComparison to published works Next, we compare our Atlas-11B results with de-biasing to recently reported results with state-of-the-art large language models such as GPT-3 or Chinchilla, which required significantly more amount of computation to train. We report results in Table 7. We find that Atlas is able to perform significantly better than random in zero-shot, and in conjunction with de-biased inference, achieves zero-shot scores that exceed 5-shot results reported with GPT3 in the literature (47.1% vs 43.9%) (Hendrycks et al., 2021). For the 5-shot setting, Atlas outperforms GPT-3 by 4%, while using 15\u00d7 less parameters, and 10\u00d7 less pre-training compute. 4 When multitask-training on the combined 5-shot data, Atlas improves to 56.6% close to the 5-shot performance of Gopher (60.0%). Finally, on the full data setting, where we train on auxiliary data recommended by the MMLU authors, Atlas reaches an overall accuracy of 65.6%, close to the state-of-the-art. Interestingly, in this setup, Atlas significantly outperforms GPT-3, while on the 5-shot setting, their performance is similar.\n\n\nOpen-domain Question Answering Results\n\nNext we evaluate Atlas on two open-domain question answering benchmarks: NaturalQuestions and TriviaQA. We compare to prior work, both in a few-shot setting using 64 examples, and using the full training set, and report results in Table 8. On these benchmarks, which require high-degree of memorisation, we clearly see the benefits of retrieval-augmentation. Atlas-11B obtains state-of-the-art results on 64-shot question answering, for both NaturalQuestions and TriviaQA. In particular, it outperforms significantly larger models, such as PaLM, or models that required significantly more training compute such as Chinchilla. When using the full training set, Atlas also obtains state-of-the-art results, for example improving the accuracy on NaturalQuestions from 55.9% to 60.4%. This result is obtained using an index comprised of CCNet and  the December 2021 Wikipedia corpora, our default setting for the index. In section 5.2 we consider using indexes composed of Wikipedia corpus archived at different dates, and demonstrate an additional +3.6% on NaturalQuestions when using an index which is temporally matched to NaturalQuestions. We report performance as a function of model size as well as detailed hyperparameters in Appendix A.2.\n\nAtlas also compares favorably to recent work exploring retrieval-augmented few-shot question answering with very large models. Lazaridou et al. (2022) explore NaturalQuestions in a 15-shot setup using Gopher, augmenting questions with 50 passages retrieved using Google Search. This method consists of generating 4 candidate answers from each retrieved passages, and then re-ranking using either a score inspired by RAG   \n\n\nFEVER Results\n\nWe report results on the original 3-class FEVER fact checking test set in Table 9. We consider a 64-shot setting, with training examples uniformly sampled from the full training set. Unlike the development and test sets, the train set is imbalanced, with more positive labels than negative, posing a challenge for few-shot  learning. In this setting, we achieve an accuracy of 64.3%. We also report a 15-shot setting, with 5 examples uniformly sampled from each class to compare with published results from Gopher , where Atlas scores 56.2%, outperforming Gopher by 5.1 points. Lastly we fine-tune our model on the full training set, and achieve a score of 78%, within 1.5% of the ProoFVer, which uses a specialized architecture, a retriever trained with sentence-level annotations, and is supplied with the Wikipedia corpus released with FEVER, whereas Atlas retrieves from CCNet and the December 2021 Wikipedia dump. If we give Atlas an index comprised of the FEVER Wikipedia corpus, we set a new state-of-the-art of 80.1%\n\n\nKILT Results\n\nFinally we evaluate Atlas on KILT, a benchmark composed of several different knowledge intensive tasks, which was described in section 4.1. We report results on test sets in Table 10 for which evaluation is available online 5 . The KILT versions of datasets are filtered, and thus results for datasets we have evaluated elsewhere are not directly comparable on KILT (i.e. FEVER, NQ and TQA). We consider both a 64-shot setting and a full fine-tuning setting, in both cases we train Atlas individually on each dataset. More details on the hyperparameters and development set results are reported in Appendix A.3. For 64-shot, we greatly exceed random performance, and are even competitive with some fully-finetuned models on the leaderboard, such as for FEVER, where our 64-shot Atlas is only 2-2.5 points behind Sphere, SEAL and Re2G, and outperforms Sphere and SEAL on zero-shot RE. In the full dataset setting, Atlas is within 3% to the state-of-the-art for 3 datasets, and sets the state-of-the-art in the remaining five datasets. \n\n\nInterpretability and Leakage\n\nAn advantage of semi-parametric models like Atlas is the ability to inspect retrieved items to aid interpretability. To better understand how well Atlas retrieves, and how it uses retrieved passages, we examine the retrieved passages for multi-task few-shot MMLU. As shown in the left panel of Figure 3, the model retrieves the majority of its passages from CCNet (85% on average). Wikipedia makes up about 15% of retrieved passages, which is higher than we would expect under a uniform prior, given Wikipedia only makes up about 10% of the index. The fraction of Wikipedia retrieval varies between MMLU domains, with the model using Wikipedia to a greater extent for STEM domains, and least for social sciences. The domain making the greatest use of Wikipedia is \"abstract algebra\" (73%), and the least is \"moral scenarios\" (3%). We also note that the MMLU-finetuned Atlas does not make significant use of Wikipedia infobox passages.\n\nWe can also analyse the content of passages to assess how they may useful for accomplishing the downstream task. The middle panel of Figure 3 shows how often retrieved documents contain the text of the correct answer option. There being at least one mention of the correct answer choice in 30% of test questions in the top 25 passages. 6 The right panel shows that the accuracy on MMLU increases when the correct answer option text occurs more frequently in retrieved passages, rising from 55% for questions when the answer option does not appear, to 77% for questions mentioned more than 15 times.\n\nA human analysis of retrieved documents revealed that documents are helpful for answering questions in a number of different ways. Manual inspection of a sample of 50 correctly-answered questions revealed that 44% contained at least partially useful background information. These are documents that would improve the likelihood of a non-expert human answering correctly, such as contextual clues surrounding a quotation from a question, or helpful numerical figures for quantity-based questions, which help to narrow down the answer options to a smaller range. In a further 26% of cases, a passage contained all the necessary information to answer the question, stated in a straightforward way. If read competently, such passages make the question simple to answer, and often include information such as canonical definitions, or the exact numerical answer requested in the question. 28% of retrieval sets did not contain obvious information which would make the question easier. Finally, 2% contained the verbatim question in a passage, together with its answer.\n\nGiven that MMLU has been created from pre-existing exams, it is possible that these questions appear on the open web. Models trained on web data (or, in our case, retrieving from it) run the risk of answering correctly not through generalisation, but by verbatim memorisation, which could lead to misleadingly high scores. In some very large language models, which can verbatim memorize and recall large parts of their pre-training data (Carlini et al., 2021), efforts have sometimes been made to filter occurrences of downstream instances from pre-training data, but this has not been performed for MMLU in the literature. In order to assess the prevalence of MMLU leakage in our index, we manually checked retrieval results for questions where the longest n-gram overlap between the question (without answer options) and a passage was at least 75% the length of the question. This resulted in an estimate of leakage of 2.8% of questions from our CC-Net corpus.\n\nA benefit of retrieval-augmented models such as Atlas is the editability of its knowledge (see section 5.2 for additional analysis). To estimate pure, non-leaked performance, we can filter out any potentially-leaked passages from retrieved results and rerun the language model. The MMLU score drops slightly when controlling for this leakage from 56.4 to 55.8% (-.5%).We note that our CC-net corpus is relatively small compared to the pre-trained corpora of recent very large models, which are trained on up to 1.4 trillion tokens (Hoffmann et al., 2022), 35x the size of our index, making it likely that models trained on corpora of that size would observe more MMLU leaked examples, but detecting such leakage is challenging in non-retrieval augmented models.\n\n\nTemporal Sensitivity and Updateability\n\nA benefit of retrieval-augmented models is that they can be kept up-to-date without retraining, by updating or swapping their index at test time. To assess the effectiveness of this mechanism in Atlas, we first construct a dataset of time-sensitive questions derived from TempLAMA (Dhingra et al., 2022). TempLAMA is a collection of templated cloze questions derived from Wikidata and Wikidata where the correct answer changes over time. We select a subset of questions from this dataset which have a different answer in 2017 and 2020, for example, Question: Theo Walcott plays for ___ Answer: Arsenal F.C. ( ), Everton F.C. (2020, and form a small training set of 248 training, 112 development and 806 test questions.\n\nUsing this dataset, we finetune closed-book T5-XXL and Atlas using the questions and the 2017 answers, supplying Atlas with a 2017 Wikipedia index, and then measure exact match accuracy on the 2017 test set. The results can be found in the first row and first two columns of Table 11. We first observe that, as expected, Atlas greatly outperforms T5 (57.7% c.f. 12.1%). We also note that, as desired, both T5 and Atlas almost never generate an answer from 2020 when trained with the 2017 answers, scoring 2.8% and 1.5% respectively (first row, second two columns of Table 11). However, as shown in row 2, we can swap the Atlas index to a 2020 Wikipedia index, without retraining, and find that Atlas updates its predictions accordingly, with 2020 accuracy rising to a similar level to its 2017 performance (53.1%), whereas the purely parametric T5 has no such updateability mechanism.\n\nThis demonstrates that Atlas can be faithful and condition strongly on its supplied index. Furthermore, this zero-shot updateability mechanism has the useful property of staying up-to-date without requiring up-to-date annotated data, or continuous, lifelong pre-training, as would be may required for a large parametric-only model. Rows 3 and 4 of Table 11 complete the picture, where this time we train with 2020 answers, and demonstrate Atlas can zero-shot transfer backwards in time to 2017 effectively too (50.1%). Interestingly, T5 is unable answer questions from 2020 well, even when trained with 2020 answers (3.6%), likely because it was pre-trained on data pre-dating 2020 (Dodge et al., 2021).\n\nWe also examine temporal effects for NaturalQuestions. NaturalQuestions is a dataset composed of search queries collected via the Google search engine in a short period of time. Thus data have a strong temporal bias, with a lot of questions about the 2018 World Cup for example. Moreover some questions are ambiguous without specification of the temporal context. For instance, for the question \"when did ireland last beat england at twickenham\", the expected answer is 2018 in NaturalQuestions, while Ireland also beat England at Twickenham in 2022 as well as many other times before. In Table 12, we report results obtained by finetuning Atlas using different Wikipedia dumps for the index. We observe that the 2018 December Wikipedia dump, which is close to the date of data collection, leads to the best results for both few-shot and full fine-tuning. In particular, it leads to a new state-of-the-art of 64 EM on NaturalQuestions. Table 11: Results on our TempLAMA-derived dataset. We report performance for a static, closed-book T5-11B, as well as Atlas-11B supplied with a test-time Wikipedia index from 2017 or 2020. We evaluate models finetuned on a small training set of 248 time-sensitive cloze-question-answer pairs, using answers either from 2017 or 2020. Good models should score highly when the test set year matches the year of the test-time index, and score low otherwise.  \n\n\nIndex Compression\n\nMaintaining dense retrieval indices can be memory-intensive, especially as the number of indexed items is scaled. In this section, we briefly analyse the memory requirements of Atlas's index in the case of a) a Wikipedia index and b) the combined CC and Wikipedia index used in most of the experiments above.\n\nThere are two sources of memory pressure for Atlas's retrieval component -the passages themselves, and the document embedding index. The tokenized passages, once binarized, require 11GB and 130GB of storage for the Wikipedia and combined indices respectively. These passages do not need to be stored in expensive GPU RAM, and could even be memory-mapped to disk, sharded across nodes or compressed if required, and thus do not represent a limiting hardware challenge in this context. The embedding index itself, however, must be stored in GPU RAM for fast search, and thus its size is more sensitive. In the above experiments, we perform exact search over our index, which is achieved by sharding the index over all the the available GPUs, and computing the search in parallel. The index is stored at fp16 precision, resulting in a total GPU memory requirement of 49 GB and 587 GB for the Wikipedia and combined indices, respectively.\n\nThis large GPU memory requirement for the index limits accessibility and ease of deployment. However, many index compression techniques are available for nearest neighbour search, which can often dramatically reduce memory requirements at the cost of some retrieval accuracy. Following , we explore the effect of Product Quantization (PQ, J\u00e9gou et al., 2011), a popular lossy compression technique on Atlas-3B's accuracy for the 64-shot NQ task at different compression levels.\n\nThe results are shown in Figure 4. We find that substantial compression is possible before the onset of significant performance degradation. Namely, the Wikipedia index can be compressed from 49GB to 4GB with negligible drop in retrieval precision and exact match. Likewise, the combined index can be compressed from 587GB to 50GB without serious degradation, indicating that the combined index could be loaded onto a single 80GB GPU.  \n\n\nDiscussion\n\nIn this paper, we introduce Atlas, a large retrieval-augmented language model. By jointly pre-training the retriever module and the language model, we show that Atlas has strong few-shot learning capabilities on a wide range of knowledge intensive tasks, including NaturalQuestions, TriviaQA, FEVER, 8 KILT tasks and 57 MMLU tasks. For example, Atlas-11B reaches more than 42% accuracy on NaturalQuestions and 84.7% on TriviaQA when training on 64 examples, which is an improvement of almost 3 points compared to PaLM, a 540B parameters model, which required 50x more pre-training compute. We also provided detailed ablations and analyses for what factors are important when training such retrieval-augmented models, and demonstrated Atlas's updateability, interpretability and controlability capabilities. Lastly, we demonstrated that Atlas is also powerful in full-dataset settings obtaining a new state-of-the-art results on NaturalQuestions, TriviaQA, FEVER, and 5 KILT tasks. \n\n\nA Training details and additional results\n\n\nA.1 MMLU\n\n\nA.1.1 Training Details\n\nFeaturization MMLU consists of multiple choice questions with four possible lexicalized answer options. We represent the input using the following template:\n\n\nquestion: {question text} options: (A) {answer 1 text} (B) {answer 2 text} (C) {answer 3 text} (D) {answer 4 text} answer: [MASK_0]\n\nand train the model to generate the mask token followed by the letter of the correct answer:\n\n\n[MASK_0] {correct answer option letter}\n\nThis format closely matches the format of MLM pre-training objective, aiding few-shot learning. When training, we permute the order of the answer options, i.e. shuffling which answer option appears as letter A etc. This helps reduce overfitting, and encourages a uniform prior on the letters.\n\n\nStandard inference\n\nOnce trained we obtain predictions from the model by selecting the pre-softmax logits for the tokens A, B, C and D, and performing a softmax over them to obtain a distribution over the 4 answer options. For standard inference, we then simply return the answer corresponding to the argmax of this distribution.\n\n\nDe-biased Inference\n\nAs mentioned in the main text, even though our model is finetuned with data that encourages a uniform prior over answer letters (by permuting which answer option letter is used with which lexical answer option text in training data), this may not be enough to ensure the model has no residual bias towards specific letters. Consider answers a, questions q and a nuisance variable z \u2208 Z, which represents the ordering of the answer options or, equivalently, which answer letter gets assigned to which answer option text. There are 4 answer options in MMLU, and thus |Z| = 24 unique ways they can be ordered, or assigned to given letters. Running our model with our standard inference for a question q, corresponds to calculating p(a|q = q, z = z) for the answer ordering z that happens to appear in the dataset. We can control for z by running the model with all possible answer orderings in the input, and marginalizing: p(a|q = q) = z \u2208Z p(a|q = q, z = z )p(z = z |q = q), and assuming p(z = z |q = q) is uniform (no answer ordering is more likely than another), this reduces to simply p(a|q = q) \u221d z \u2208Z p(a|q = q, z = z ). This procedure requires 24 forward passes, one for each answer ordering, so is 24\u00d7 slower than standard inference. Table 13 shows the result of applying the full permutation de-biasing, which leads to an 12% improvement zero-shot and 6% in 5-shot performance overall. Empirically, using only the cyclic permutations of the answer order provided in the original dataset (of which there are 4) works nearly as well, which is what we report in the main paper, and only increases inference compute by a factor of 4, rather than 24. Cyclic permutation de-biasing improves over standard inference by 10% in zero-shot and 5% in 5-shot. Empirically, de-biased inference is largely unnecessary when training in the 5-shot multitask or full dataset setting, as there is enough data for the model to learn a more uniform prior over the letters. Evaluation We evaluate by following the method of Hendrycks et al. (2021), namely, micro-averaging across all 57 domains to obtain overall accuracy. We quote the results of GPT3  and UnifiedQA (Khashabi et al., 2020) from the MMLU leaderboard at https://github.com/hendrycks/test. For Chinchilla and Gopher, we calculate the scores on the categories using the full MMLU results from Hoffmann et al. (2022).\n\n\nIndex\n\nThe index used for MMLU for all MMLU experiments in the main paper comprised of concatenation of the Wikipedia passages, Wikipedia info boxes and Common Crawl indices, for a total of 387M passages. We can assess the importance of the index by running a model without the common crawl data, leading to a 5-shot multitask result of 52.8%, compared to 56.4% for the full model, a drop of 3.6%. This indicates that whilst the Wikipedia data is sufficient do well on the task, the addition of the CC data improves results further.\n\nHyperparameters and development data Selecting hyperparameters is challenging in few-shot settings. We do not assume access to an in-domain development set for the 5-shot task. Instead, we determine a set of hyperparameters for the 5-shot task using data from RACE, one of the auxiliary datasets provided by MMLU. Here, we sample 5 sets of 5-shot training data, and for each model size, we explore batch size {32, 64}, learning rates for the language model and retriever {(5e-5, 1e-5), (4e-5, 4e-5)}, retriever temperature {0.1, 0.01} and a fixed number of training steps {16, 32, 64, 128}, picking the setting that achieves strongest RACE validation scores. Having determined these hyperparameters, we apply them directly to the 5-shot MMLU task. For the 5-shot multi-task and full/transfer settings, we use the same batch size, temperatures and learning rates as the 5-shot task, but use a set of 285 MMLU validation examples (5 per domain) in order to determine the total number of training steps and for early stopping. The hyperparameters selected in the MMLU experiments can be found in table 14. We use query-side finetuning for the 5-shot and 5-shot multitask settings, and top-128 reranking for the full setting. For all MMLU runs we retrieve 30 documents\n\nInter-run Variance few-shot learning is well-known to suffer from high variance. In the main paper, we quote the result obtained with our first run. In order to assess the effect of noise and variance, we ran the 5-shot experiment with Atlas 5 times. 7 We observe high variance for individual domains, sometimes as high as 20%, however, once aggregated across all 57 domains, the inter-run variance is low. The overall scores for these different runs, when using the same hyperparameters are shown in table 15. Due the effects of averaging over the many domains that comprise MMLU, the inter-run variance is quite modest on the aggregated metrics, with a std deviation of 0.5 in this experiment.\n\n\nClosed-Book Baselines\n\nThe closed book baselines we compare Atlas to in table 5 are initialized from the same T5 model as their respective Atlas, and then pre-trained with MLM for the same number of steps (10K) using the same pre-training data as Atlas, for fairer comparison. The same procedure as for Atlas was used to determine hyperparameters for MMLU for the closed-book model.s 45.0 \u00b1 0.5 40.5 \u00b1 0.6 53.6 \u00b1 1.6 37.4 \u00b1 0.5 51.1 \u00b1 0.8\n\n\nA.1.2 Full results\n\nTables 16 and 17 shows the full MMLU scores for each domain for Atlas and the closed book T5 respectively. The full results for the cyclic-permutation-de-biased Atlas-XXL can be found in Table 18.\n\n\nA.2 Question answering\n\n\nA.2.1 Training Details\n\nFor question answering, similarly to the MMLU experiments, we format the input using the following template:\n\n\nquestion: {question text} answer: [MASK_0]\n\nand train the model to generate the mask token followed by the answer:\n\n[MASK_0] {answer}.\n\nWe generate answers using greedy decoding. For both training and testing, we retrieve 40 passages, and truncate the result of the concatenation between the query and the passages to 384 tokens.\n\nFor few-shot fine-tuning we train Atlas for 30 steps using 64 random samples from the train sets. The retriever is trained using query-side fine-tuning. We select the model after 30 training steps. We use AdamW with a batch size of 32 and a learning rate of 4 \u00d7 10 \u22125 with linear decay and 5 iterations of warmup for both the language model and the retriever.\n\nFor the fine-tuning on the full datasets, we train the model for 5k gradient steps and refresh the index every 500 steps for the first 1,000 training steps and every 2k training steps afterwards. We use AdamW with a batch size of 64 and a learning rate of 4 \u00d7 10 \u22125 with linear decay and 5 iterations of warmup for both the language model and the retriever. We evaluate models every 500 steps and select the best one on the validation set based on the exact match score.\n\n\nA.2.2 Impact of scaling\n\nIn Table 19, we report performance on NaturalQuestions and TriviaQA as a function of the number of parameters in the reader module. Both for few-shot learning and full fine-tuning we observe strong improvements by scaling the size of the reader module. However we can notice sign of saturation when finetuning on full datasets, with limited gains when scaling from 3B to 11B parameters (+0.6% on NaturalQuestions, +0.5% on TriviaQA). While performance improves substantially when scaling from 3B to 11B parameters with 64 training samples, with +3.7% and +1.2% improvement on NaturalQuestions and TriviaQA respectively. For these experiments we use a setup similar to the one use in Table 8, except that we use an index composed of the December 2018 Wikipedia dump processed as described in section 4.2.\n\n\nA.3 KILT\n\nFor the results on KILT reported in Table 10 we fine-tune Atlas individually on each dataset. We format the input using a template similar to the one used for question answering:     and train the model to generate the mask token followed by the expected output:\n\n[MASK_0] {output}.\n\nWe retrieve 20 passages and generate answer using greedy decoding. In KILT, FEVER is a two-way classification task of claims. We lexicalize the \"SUPPORTS\" (resp. 'REFUTES\") label into \"true\" (respectively \"false\").\n\nFor few-shot fine-tuning we train Atlas for 30 steps using 64 random samples from the train sets. The retriever is trained using query-side fine-tuning. We evaluate models every 5 steps and select the best one on the development set based on the reported metric. We use AdamW with a batch size of 32 and a learning rate of 4 \u00d7 10 \u22125 with linear decay and 5 iterations of warmup for both the language model and the retriever.\n\nFor the fine-tuning on the full datasets, the model is trained for 5k gradient steps. We evaluate models every 500 steps and select the best one on the development set based on the reported metric. The index is refreshed every 500 step for the first 1000 iterations, and every 2k steps afterwards. We use AdamW with a batch size of 64 and a learning rate of 4 \u00d7 10 \u22125 with linear decay and 500 iterations of warmup for both the language model and the retriever.\n\nWe report results on the development sets in Table 20.\n\nFigure 3 :\n3MMLU Retrieval Analysis. Left: Fraction of sources of top 30 retrieved passages for MMLU from CCNet, Wikipedia passages and info boxes for the 5-shot multitask Atlas. Center: How often the text of the correct MMLU answer option appears in retrieved passages, as a function of the number of retrieved passages. Right: MMLU accuracy as a function of answer occurrence frequency in retrieved passages set 5 Analysis\n\nFigure 4 :\n4Index Compression: Atlas-3B 64-shot NQ performance (left column: Retrieval Recall@50, right column: QA Exact Match score), as a function of index size, for different levels of quantisation. The right-most point in each plot represents the uncompressed index. Top Row: Wikipedia + CC Index. Bottom Row: Wikipedia Index.\n\nTable 1 :\n1Retriever loss ablation. We compare different loss functions to pre-train the retriever jointly with the language model. We use the prefix MLM task, and the December 2021 Wikipedia dump for both the index and pre-training data. Fine-tuning is performed with query-side fine-tuning and the loss used for pre-training. Best result is bold, second highest underlined.64-shot \n1024-shot \n\nMLM \nNQ WoW FEVER Avg. \nNQ WoW FEVER Avg. \n\nClosed-book \n1.083 \n6.5 \n14.1 \n59.0 \n26.5 10.7 \n16.5 \n75.3 \n34.2 \nNo Joint pre-training \n-\n9.0 \n14.1 \n67.0 \n30.0 \n9.9 \n16.6 \n78.3 \n34.9 \nFixed retriever \n0.823 \n39.9 \n14.3 \n72.4 \n42.2 45.3 \n17.9 \n90.0 \n51.1 \nADist \n0.780 \n40.9 \n14.4 \n73.8 \n43.0 46.2 \n17.2 \n90.9 \n51.4 \nEMDR 2 \n0.783 \n43.3 \n14.6 \n72.1 \n43.3 44.9 \n18.3 \n85.7 \n49.6 \nPDist \n0.783 45.0 15.0 \n77.0 \n45.7 44.9 \n17.9 \n90.2 \n51.0 \nLOOP \n0.766 41.8 \n15.0 \n74.4 \n43.7 47.1 \n17.9 \n87.5 \n50.8 \n\n\n\nTable 2 :\n2Pretext task ablation. We compare different pretext tasks, used to jointly pre-train our models. Examples are randomly sampled from the training set of the KILT version of the dataset. We report the exact match on NaturalQuestions, the F1 score on Wizard of Wikipedia and the accuracy on FEVER.64-shot \n1024-shot \n\nNQ WoW FEVER Avg. \nNQ WoW FEVER Avg. \n\nPrefix Language Modelling \n41.0 \n14.5 \n64.9 \n40.1 44.7 \n17.9 \n86.0 \n49.5 \nMasked Language Modelling 42.7 14.9 \n69.7 \n42.4 44.7 18.3 \n88.8 \n50.6 \nTitle-to-section generation \n41.1 \n15.2 \n66.1 \n40.8 45.4 \n17.9 \n84.6 \n49.3 \n\n\n\nTable 3 :\n3Index content ablation. In this table, we report results for models where the content of the index was changed between the pre-training and the fine-tuning.64-shot \n1024-shot \n\nIndex Training data NQ WoW FEVER Avg. \nNQ WoW FEVER Avg. \n\nWiki \nWiki \n42.7 \n14.9 \n69.7 \n42.4 44.7 \n18.3 \n88.8 \n50.6 \nWiki \nCC \n40.9 \n15.3 \n67.3 \n41.2 44.8 18.4 \n88.1 \n50.4 \nCC \nWiki \n32.9 \n14.5 \n72.1 \n39.8 37.8 \n17.1 \n85.8 \n46.9 \nCC \nCC \n38.4 \n14.9 \n70.1 \n41.1 42.0 \n17.3 \n88.9 \n49.4 \n\n\n\nTable 4 :\n4Retriever fine-tuning ablation. Here, we compare different strategies to fine-tune the retriever in a few-shot setting.64-shot \n1024-shot \n\nNQ WoW FEVER Avg. \nNQ WoW FEVER Avg. \n\nStandard fine-tuning \n44.3 \n14.9 \n73.2 \n44.1 47.0 \n18.4 \n89.7 \n51.7 \nTop-100 re-ranking \n44.2 \n14.6 \n75.4 \n44.7 47.1 18.7 \n88.9 \n51.6 \nQuery-side fine-tuning 45.0 15.0 \n77.0 \n45.7 44.9 \n17.9 \n90.2 \n51.0 \nFixed retriever \n36.8 \n14.5 \n72.0 \n41.1 38.0 \n17.7 \n89.3 \n48.3 \n\nTable 5: Performance on MMLU as a function of model size. \n\n5-shot \n5-shot (multi-task) \nFull / Transfer \n\n770M \n3B \n11B 770M \n3B \n11B \n770M \n3B \n11B \n\nClosed-book T5 \n29.2 \n35.7 36.1 \n26.5 \n40.0 \n43.5 \n42.4 \n50.4 \n54.0 \nAtlas \n38.9 \n42.3 43.4 \n42.1 \n48.7 \n56.4 \n56.3 \n59.9 \n65.8 \n\n\u2206 \n+9.8 +6.6 +7.3 +15.6 +8.7 +12.9 +13.9 +9.5 +11.8 \n\n\n\nTable 6 :\n6Standard vs de-biased inference for MMLU These results are reported for Atlas-11B, using cyclic permutations for de-biasing, which increases inference costs by a factor of 4\u00d7.Zero-shot 5-shot 5-shot (multi-task) Full / Transfer \n\nStandard Inference \n36.8 \n43.4 \n56.4 \n65.8 \nDe-biased Inference \n47.1 \n47.9 \n56.6 \n66.0 \n\n\n\nTable 7 :\n7Comparison to state-of-the-art on MMLU. * For the 5-shot setting, Atlas uses fine-tuning, while previous works use in-context learning. The Atlas model uses de-biased inference. Train FLOPS refers to total the amount of computation necessary to train the model, including pre-training and/or fine-tuning.Setting \nModel \nParams Train FLOPS \nAll \nHum. Soc. Sci. STEM Other \n\nzero-shot \nAtlas \n11B \n3.5e22 \n47.1 \n43.6 \n54.1 \n38.0 \n54.4 \n\n5-shot \n\nGPT-3 \n175B \n3.1e23 \n43.9 \n40.8 \n50.4 \n36.7 \n48.8 \nGopher \n280B \n5.0e23 \n60.0 \n56.2 \n71.9 \n47.4 \n66.1 \nChinchilla \n70B \n5.0e23 \n67.5 \n63.6 \n79.3 \n55.0 \n73.9 \nAtlas  *  \n11B \n3.5e22 \n47.9 \n46.1 \n54.6 \n38.8 \n52.8 \n\n5-shot (multi-task) Atlas \n11B \n3.5e22 \n56.6 \n50.1 \n66.4 \n46.4 \n66.2 \n\nFull / Transfer \n\nUnifiedQA \n11B \n3.3e22 \n48.9 \n45.6 \n56.6 \n40.2 \n54.6 \nGPT-3 \n175B \n3.1e23 \n53.9 \n52.5 \n63.9 \n41.4 \n57.9 \nAtlas \n11B \n3.5e22 \n66.0 \n61.1 \n77.2 \n53.2 \n74.4 \n\n\n\nTable 8 :\n8Comparison to state-of-the-art on question answering. We report results on NaturalQuestions, and on TriviaQA for both the filtered set, commonly used for open-domain question answering and the unfiltered hidden set for which evaluation is accessible online: https://competitions.codalab.org/ competitions/17208. For the 64-shot setting, our model uses fine-tuning, while the other models use prompting.NQ \nTriviaQA filtered TriviaQA unfiltered \n\n\n\nTable 9 :\n9Comparison to state-of-the-art on FEVER. We report accuracy on FEVER test set, for which evaluation is available here: https://competitions.codalab.org/competitions/18814. For the few-shot settings, our model uses fine-tuning while other models use prompting. \u2020 uses an index composed of the FEVER Wikipedia corpus.15-shot 65-shot Full dataset \n\nGopher (Rae et al., 2021) \n51.1 \n-\n-\nProoFVer (Krishna et al., 2021) \n-\n-\n79.5 \nAtlas \n56.2 \n64.3 \n78.0 / 80.1  \u2020 \n\n\n\nTable 10 :\n10Downstream results on the KILT hidden test setsDownstream metrics are accuracy (AIDA \n\n\nTable 12 :\n12Impact of index data temporality on NaturalQuestions.We report exact match performance \n\n\nTable 13 :\n13MMLU scores with de-biasing:Setting \nModel \nAll \nHum. Soc. Sci. STEM Other \n\nzero-shot \n\nStandard \n36.8 \n37.5 \n39.0 \n30.2 \n39.7 \nAll permutations \n48.5 \n45.7 \n55.2 \n39.4 \n54.4 \nCyclic Permutations 47.1 \n43.6 \n54.1 \n38.0 \n54.9 \n\n5-shot \n\nStandard \n43.4 \n41.8 \n49.3 \n33.9 \n48.8 \nAll permutations \n49.0 \n46.0 \n56.1 \n40.5 \n54.6 \nCyclic Permutations 47.9 \n46.1 \n54.6 \n38.8 \n52.8 \n\n\n\nTable 14 :\n14Hyperparameters for MMLU770M \n3B \n11B \n\n\n\nTable 15 :\n15Interrun Variance for 5-shot MMLU using Atlas-11BRun # \nAll \nHum. \nSoc. Sci. \nSTEM \nOther \n\n1 \n45.2 \n40.6 \n54.1 \n37.1 \n51.1 \n2 \n45.1 \n39.8 \n54.4 \n37.1 \n52.0 \n3 \n45.0 \n40.0 \n54.1 \n37.7 \n51.1 \n4 \n45.6 \n41.3 \n54.7 \n37.0 \n51.6 \n5 \n44.3 \n40.6 \n50.7 \n38.1 \n49.8 \n\nAve: \n\n\nTable 16 :\n16MMLU Test set scores for Atlas for each model size and each of the 57 domains.5-shot \n\n5-shot (multi-task) \nFull / Transfer \n\n770M 3B \n11B 770M 3B \n11B 770M 3B \n11B \nAll \n29.2 \n35.7 36.1 \n26.5 \n40.0 43.5 \n42.4 \n50.4 54.0 \n\nHumanities \n30.5 \n35.4 35.5 \n27.3 \n38.5 41.6 \n41.0 \n48.6 51.3 \nSocial Sciences \n29.7 \n38.0 39.4 \n24.8 \n43.8 48.9 \n48.6 \n57.8 64.7 \nSTEM \n29.0 \n31.4 30.8 \n26.5 \n32.8 35.8 \n33.4 \n40.6 41.7 \nOther \n26.7 \n37.7 38.6 \n27.0 \n45.0 48.5 \n46.8 \n55.2 59.1 \n\nabstract algebra \n26.0 \n23.0 21.0 \n29.0 \n30.0 26.0 \n23.0 \n29.0 26.0 \nanatomy \n21.5 \n40.0 40.7 \n27.4 \n39.3 45.9 \n35.6 \n43.7 42.2 \nastronomy \n37.5 \n38.8 37.5 \n27.6 \n39.5 41.4 \n36.2 \n50.7 55.3 \nbusiness ethics \n29.0 \n54.0 42.0 \n26.0 \n47.0 55.0 \n53.0 \n64.0 60.0 \nclinical knowledge \n32.5 \n33.6 40.0 \n28.7 \n44.2 47.9 \n45.3 \n52.8 57.7 \ncollege biology \n29.9 \n34.7 34.0 \n29.9 \n34.7 40.3 \n38.2 \n46.5 52.1 \ncollege chemistry \n37.0 \n22.0 32.0 \n20.0 \n35.0 33.0 \n36.0 \n34.0 36.0 \ncollege computer science \n28.0 \n35.0 34.0 \n28.0 \n27.0 36.0 \n31.0 \n44.0 35.0 \ncollege mathematics \n31.0 \n29.0 27.0 \n22.0 \n34.0 27.0 \n30.0 \n33.0 32.0 \ncollege medicine \n24.3 \n34.7 34.1 \n27.2 \n40.5 40.5 \n35.8 \n41.6 48.6 \ncollege physics \n33.3 \n23.5 23.5 \n22.5 \n19.6 26.5 \n22.5 \n32.4 24.5 \ncomputer security \n36.0 \n42.0 46.0 \n31.0 \n49.0 52.0 \n50.0 \n65.0 61.0 \nconceptual physics \n26.4 \n35.7 30.2 \n23.4 \n30.6 32.8 \n34.5 \n37.4 43.8 \neconometrics \n26.3 \n21.9 28.9 \n17.5 \n19.3 24.6 \n29.8 \n25.4 29.8 \nelectrical engineering \n31.0 \n33.1 31.7 \n31.0 \n31.0 36.6 \n41.4 \n47.6 51.7 \nelementary mathematics \n26.2 \n27.5 28.0 \n27.0 \n31.2 33.3 \n25.9 \n31.2 35.5 \nformal logic \n34.1 \n34.1 31.7 \n15.1 \n34.9 31.0 \n31.7 \n38.1 42.1 \nglobal facts \n32.0 \n30.0 25.0 \n34.0 \n34.0 27.0 \n28.0 \n34.0 30.0 \nhigh school biology \n22.6 \n31.9 29.7 \n27.1 \n41.6 50.0 \n43.5 \n57.7 60.6 \nhigh school chemistry \n27.1 \n26.6 27.6 \n28.6 \n31.5 29.1 \n30.5 \n36.5 38.9 \nhigh school computer science \n26.0 \n32.0 25.0 \n33.0 \n37.0 45.0 \n45.0 \n55.0 48.0 \nhigh school european history \n34.5 \n43.0 42.4 \n24.2 \n60.0 59.4 \n58.2 \n69.1 76.4 \nhigh school geography \n31.3 \n40.4 36.9 \n24.7 \n45.5 50.5 \n56.1 \n66.7 74.2 \nhigh school gov. and pol. \n28.0 \n49.2 51.3 \n19.2 \n56.0 59.6 \n55.4 \n70.5 75.6 \nhigh school macroeconomics \n25.6 \n37.7 32.1 \n26.7 \n42.3 43.6 \n41.0 \n51.5 56.4 \nhigh school mathematics \n35.9 \n35.2 35.9 \n28.1 \n26.7 31.1 \n27.8 \n36.7 31.9 \nhigh school microeconomics \n27.3 \n29.8 36.1 \n20.6 \n35.7 42.9 \n42.9 \n50.8 60.5 \nhigh school physics \n21.9 \n25.2 22.5 \n24.5 \n28.5 29.1 \n27.8 \n31.1 27.8 \nhigh school psychology \n26.1 \n46.4 51.0 \n24.8 \n54.3 60.2 \n56.3 \n67.3 76.1 \nhigh school statistics \n27.8 \n33.3 33.3 \n17.6 \n30.6 33.8 \n32.9 \n33.3 37.0 \nhigh school us history \n30.4 \n39.7 45.6 \n27.5 \n46.1 58.3 \n51.0 \n63.2 72.5 \nhigh school world history \n42.6 \n50.6 41.8 \n29.1 \n54.0 64.6 \n66.7 \n72.2 73.8 \nhuman aging \n28.3 \n37.2 29.6 \n26.0 \n45.3 46.2 \n46.6 \n57.0 62.8 \nhuman sexuality \n29.8 \n34.4 41.2 \n25.2 \n42.0 44.3 \n51.1 \n58.0 59.5 \ninternational law \n57.9 \n57.9 41.3 \n44.6 \n57.9 58.7 \n62.8 \n71.9 71.1 \njurisprudence \n30.6 \n33.3 34.3 \n32.4 \n49.1 52.8 \n55.6 \n67.6 74.1 \nlogical fallacies \n40.5 \n55.8 46.6 \n25.8 \n51.5 62.0 \n43.6 \n69.3 71.2 \nmachine learning \n33.0 \n34.8 36.6 \n29.5 \n35.7 37.5 \n32.1 \n37.5 42.9 \nmanagement \n21.4 \n29.1 40.8 \n24.3 \n47.6 50.5 \n60.2 \n69.9 70.9 \nmarketing \n38.9 \n58.5 60.7 \n31.2 \n67.9 75.6 \n69.2 \n79.9 85.9 \nmedical genetics \n26.0 \n36.0 36.0 \n29.0 \n43.0 44.0 \n40.0 \n54.0 50.0 \nmiscellaneous \n24.5 \n45.2 46.4 \n27.1 \n52.2 58.2 \n51.3 \n64.6 72.7 \nmoral disputes \n32.4 \n37.3 38.7 \n28.6 \n43.4 43.4 \n49.7 \n64.7 64.7 \nmoral scenarios \n24.7 \n24.7 24.7 \n23.0 \n23.9 24.7 \n23.8 \n24.0 23.8 \nnutrition \n30.1 \n33.0 34.6 \n25.8 \n42.5 44.1 \n50.3 \n55.6 61.1 \nphilosophy \n28.6 \n32.5 37.3 \n31.2 \n38.9 45.0 \n44.1 \n56.6 59.2 \nprehistory \n33.6 \n37.0 41.4 \n27.5 \n39.8 50.6 \n41.0 \n51.5 57.7 \nprofessional accounting \n21.3 \n28.0 30.5 \n25.9 \n35.5 34.0 \n37.2 \n41.5 42.2 \nprofessional law \n28.2 \n33.4 34.0 \n27.6 \n35.4 35.5 \n38.3 \n43.0 45.6 \nprofessional medicine \n19.5 \n26.5 24.3 \n20.2 \n32.0 37.9 \n38.6 \n40.8 46.0 \nprofessional psychology \n27.8 \n32.8 32.8 \n26.6 \n39.5 43.6 \n38.4 \n48.0 58.3 \npublic relations \n22.7 \n43.6 40.0 \n21.8 \n47.3 56.4 \n50.0 \n55.5 60.0 \nsecurity studies \n37.6 \n26.1 31.0 \n20.4 \n34.7 44.1 \n56.3 \n61.6 66.9 \nsociology \n43.3 \n41.8 38.8 \n30.8 \n45.8 52.7 \n60.2 \n66.7 72.1 \nus foreign policy \n49.0 \n57.0 66.0 \n38.0 \n56.0 61.0 \n59.0 \n75.0 76.0 \nvirology \n29.5 \n26.5 34.3 \n30.1 \n36.1 39.8 \n44.0 \n46.4 41.6 \nworld religions \n24.0 \n40.9 47.4 \n32.7 \n49.1 57.3 \n48.0 \n63.7 70.2 \n\n\n\nTable 17 :\n17MMLU Test scores for the T5 closed book baseline for each model size and each of the 57 domains.Domain \n\nzero-shot \n5-shot \n5-shot (multi-task) Full / Transfer \n\nAll \n47.1 \n47.9 \n56.6 \n66.0 \n\nHumanities \n43.6 \n46.1 \n50.1 \n61.1 \nSocial Sciences \n54.1 \n54.6 \n66.4 \n77.2 \nSTEM \n38.0 \n38.8 \n46.4 \n53.2 \nOther \n53.9 \n52.8 \n66.2 \n74.4 \n\nabstract algebra \n22.0 \n26.0 \n31.0 \n31.0 \nanatomy \n48.9 \n47.4 \n62.2 \n70.4 \nastronomy \n61.8 \n62.5 \n68.4 \n81.6 \nbusiness ethics \n60.0 \n57.0 \n62.0 \n70.0 \nclinical knowledge \n50.6 \n49.4 \n66.4 \n72.8 \ncollege biology \n51.4 \n53.5 \n61.1 \n77.8 \ncollege chemistry \n36.0 \n39.0 \n39.0 \n45.0 \ncollege computer science \n32.0 \n32.0 \n33.0 \n49.0 \ncollege mathematics \n30.0 \n35.0 \n35.0 \n34.0 \ncollege medicine \n44.5 \n41.0 \n52.6 \n67.6 \ncollege physics \n24.5 \n26.5 \n37.3 \n42.2 \ncomputer security \n59.0 \n59.0 \n68.0 \n76.0 \nconceptual physics \n37.0 \n41.3 \n57.0 \n60.0 \neconometrics \n20.2 \n20.2 \n36.8 \n37.7 \nelectrical engineering \n37.9 \n40.0 \n50.3 \n65.5 \nelementary mathematics \n31.2 \n28.0 \n30.7 \n36.5 \nformal logic \n27.8 \n27.0 \n32.5 \n35.7 \nglobal facts \n41.0 \n43.0 \n51.0 \n53.0 \nhigh school biology \n53.2 \n56.5 \n68.7 \n83.2 \nhigh school chemistry \n41.9 \n41.4 \n49.3 \n51.2 \nhigh school computer science \n40.0 \n36.0 \n46.0 \n60.0 \nhigh school european history \n56.4 \n58.8 \n68.5 \n80.6 \nhigh school geography \n57.1 \n59.6 \n71.2 \n81.3 \nhigh school gov. and pol. \n67.9 \n67.9 \n77.2 \n90.2 \nhigh school macroeconomics \n46.9 \n48.5 \n57.9 \n65.9 \nhigh school mathematics \n28.1 \n28.9 \n34.1 \n31.5 \nhigh school microeconomics \n51.7 \n51.7 \n68.9 \n82.4 \nhigh school physics \n26.5 \n25.8 \n32.5 \n41.1 \nhigh school psychology \n66.2 \n65.5 \n78.9 \n86.8 \nhigh school statistics \n31.5 \n30.1 \n43.1 \n45.8 \nhigh school us history \n57.8 \n54.9 \n64.7 \n77.5 \nhigh school world history \n59.1 \n62.9 \n65.4 \n79.3 \nhuman aging \n48.4 \n50.7 \n60.5 \n70.4 \nhuman sexuality \n55.7 \n54.2 \n61.8 \n84.0 \ninternational law \n66.1 \n72.7 \n71.9 \n84.3 \njurisprudence \n61.1 \n64.8 \n72.2 \n81.5 \nlogical fallacies \n54.6 \n57.7 \n71.2 \n77.9 \nmachine learning \n37.5 \n39.3 \n43.8 \n44.6 \nmanagement \n56.3 \n56.3 \n79.6 \n89.3 \nmarketing \n72.2 \n73.1 \n84.6 \n91.9 \nmedical genetics \n55.0 \n58.0 \n71.0 \n81.0 \nmiscellaneous \n69.7 \n67.8 \n83.8 \n90.4 \nmoral disputes \n45.1 \n46.8 \n60.1 \n72.3 \nmoral scenarios \n24.5 \n30.3 \n25.8 \n38.5 \nnutrition \n56.5 \n53.9 \n67.0 \n77.1 \nphilosophy \n56.3 \n57.6 \n70.7 \n77.2 \nprehistory \n59.3 \n60.5 \n71.6 \n78.7 \nprofessional accounting \n35.1 \n33.0 \n42.2 \n50.7 \nprofessional law \n36.3 \n38.4 \n39.4 \n51.7 \nprofessional medicine \n35.7 \n33.1 \n52.2 \n60.7 \nprofessional psychology \n47.7 \n49.3 \n60.9 \n74.0 \npublic relations \n54.5 \n53.6 \n68.2 \n68.2 \nsecurity studies \n47.3 \n45.7 \n59.2 \n73.9 \nsociology \n62.2 \n62.7 \n71.6 \n84.6 \nus foreign policy \n64.0 \n68.0 \n73.0 \n83.0 \nvirology \n39.8 \n40.4 \n44.6 \n51.8 \nworld religions \n77.2 \n74.9 \n80.7 \n87.1 \n\n\n\nTable 18 :Table 19 :\n1819MMLU Test set scores for the de-biased Atlas-XXL using cyclic permutations for each of the 57 domains for zero-shot, 5 shot, 5-shot-multitask and the transfer setting. Impact of model size on question answering datasets. We report exact match performance on the test sets of NaturalQuestions and TriviaQA filtered depending on the number of parameters in the reader module. For these experiments the index contains the December 2018 Wikipedia dump.Number of parameters \n\n220M 770M 3B \n11B \n\nNaturalQuestions 64-shot \n27.0 \n35.4 \n41.3 45.1 \nNaturalQuestions full \n54.1 \n60.8 \n63.4 64.0 \n\nTriviaQA 64-shot \n55.3 \n65.0 \n70.2 71.4 \nTriviaQA full \n71.8 \n74.9 \n77.5 78.0 \n\nModel \nAIDA FEV T-REx zsRE NQ HoPo TQA WoW \nacc acc \nacc \nacc em \nem \nem \nf1 \n\nAtlas 64-shot \n69.0 88.1 \n58.5 \n60.2 44.2 34.1 77.1 15.4 \nAtlas full dataset 92.7 94.4 \n84.8 \n80.9 63.4 51.4 84.4 21.0 \n\n\n\nTable 20 :\n20Downstream results on KILT dev sets. question: {query text} answer: [MASK_0]\n See Hoffmann et al. (2022)  for more details about the computation of the FLOPS corresponding to the forward and backward passes of transformer networks.2 There is a factor 4 to account for the backward pass and activation checkpointing.\nExploring all answer option permutations would involve 24 forward passes, which improves results by an additional \u223c1% over the 4 cyclic permutations, but requires much more compute, so we exclude it here, see Appendix A.1 4 Atlas's pre-training compute is dominated by the T5 pre-training. The computational requirements for the retrievalaugmented pre-train is orders of magnitude lower\nhttps://eval.ai/web/challenges/challenge-page/689\nNote: Depending on the question, it may not be important or useful to retrieve the exact text of the answer in MMLU, and as such, a hits@k value of 30% does not imply that retrieval fails to surface useful information in 70% of cases\nThis experiment was performed with a slightly different index to the main experiments, which achieves a stronger result\n\nAutoregressive search engines: Generating substrings as document identifiers. Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-Tau Yih, Sebastian Riedel, Fabio Petroni, Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, and Fabio Petroni. Autoregressive search engines: Generating substrings as document identifiers, 2022. URL https://arxiv. org/abs/2204.10628. 15\n\nDiego de Las Casas. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Oriol Vinyals. Simon OsinderoSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W.\n\nImproving language models by retrieving from trillions of tokens. Erich Rae, Laurent Elsen, Sifre, 814Rae, Erich Elsen, and Laurent Sifre. Improving language models by retrieving from trillions of tokens, 2021. URL https://arxiv.org/abs/2112.04426. 1, 8, 14\n\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, 1428Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. 1, 8, 14, 28\n\nAutoregressive entity retrieval. Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni, International Conference on Learning Representations. Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= 5k8F6UU39V. 15\n\nExtracting training data from large language models. Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, USENIX Security Symposium. \u00dalfar Erlingsson, Alina Oprea, and Colin RaffelNicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In USENIX Security Symposium, pp. 2633-2650, 2021. URL https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting. 17\n\nReading Wikipedia to answer open-domain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, Proc. ACL. ACLDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-domain questions. In Proc. ACL, 2017. 7\n\nPalm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Parker Gehrmann, Kensen Schuh, Sasha Shi, Joshua Tsvyashchenko, Abhishek Maynez, Parker Rao, Yi Barnes, Noam Tay, Vinodkumar Shazeer, Emily Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Lim, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern814Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick; Douglas Eck, Jeff Dean, Slav Petrov, and Noah FiedelAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311. 1, 2, 8, 14\n\nSimple and effective multi-paragraph reading comprehension. Christopher Clark, Matt Gardner, Proc. ACL. ACLChristopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. In Proc. ACL, 2018. 7\n\nThink you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, abs/1803.05457ArXiv. 9Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. 9\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proc. NAACL. NAACLJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL, 2019. 7\n\nTime-aware language models as temporal knowledge bases. Bhuwan Dhingra, Jeremy R Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, William W Cohen, 10.1162/tacl_a_00459Transactions of the Association for Computational Linguistics. 1017Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10:257-273, 2022. doi: 10.1162/tacl_a_00459. URL https://aclanthology. org/2022.tacl-1.15. 9, 17\n\nWizard of wikipedia: Knowledge-powered conversational agents. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, International Conference on Learning Representations. 79Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm. 7, 9\n\nDocumenting large webtext corpora: A case study on the colossal clean crawled corpus. Jesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner, 10.18653/v1/2021.emnlp-main.98Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsOnline and Punta CanaJesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1286-1305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.98. URL https://aclanthology.org/2021.emnlp-main. 98. 17\n\nA large scale alignment of natural language with knowledge base triples. Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, Elena Simperl, T-Rex, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. T-REx: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), 2018. URL https://aclanthology.org/L18-1544. 9\n\nR2-D2: A modular baseline for open-domain question answering. Martin Fajcik, Martin Docekal, Karel Ondrej, Pavel Smrz, 10.18653/v1/2021.findings-emnlp.73Findings of the Association for Computational Linguistics: EMNLP 2021. Martin Fajcik, Martin Docekal, Karel Ondrej, and Pavel Smrz. R2-D2: A modular baseline for open-domain question answering. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 854-870, November 2021. doi: 10.18653/v1/2021.findings-emnlp.73. URL https://aclanthology.org/ 2021.findings-emnlp.73. 14\n\nObject classification from a single example utilizing class relevance metrics. Michael Fink, Advances in Neural Information Processing Systems. L. Saul, Y. Weiss, and L. BottouMIT Press17Michael Fink. Object classification from a single example utilizing class relevance metrics. In L. Saul, Y. Weiss, and L. Bottou (eds.), Advances in Neural Information Processing Sys- tems, volume 17. MIT Press, 2005. URL https://proceedings.neurips.cc/paper/2004/file/ ef1e491a766ce3127556063d49bc2f98-Paper.pdf. 8\n\nMaking pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, 10.18653/v1/2021.acl-long.295Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3816-3830, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021.acl-long.295. 8\n\nMd Faisal Mahbub Chowdhury, Ankita Rajaram Naik, Pengshan Cai, and Alfio Gliozzo. Re2g: Retrieve, rerank, generate. Michael Glass, Gaetano Rossiello, Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Rajaram Naik, Pengshan Cai, and Alfio Gliozzo. Re2g: Retrieve, rerank, generate, 2022. URL https://arxiv.org/abs/2207.06300. 15\n\nUnbounded cache model for online language modeling with open vocabulary. Edouard Grave, Moustapha Cisse, Armand Joulin, Edouard Grave, Moustapha Cisse, and Armand Joulin. Unbounded cache model for online language modeling with open vocabulary, 2017a. URL https://arxiv.org/abs/1711.02604. 8\n\nImproving neural language models with a continuous cache. Edouard Grave, Armand Joulin, Nicolas Usunier, International Conference on Learning Representations. Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In International Conference on Learning Representations, 2017b. URL https://openreview.net/ forum?id=B184E5qee. 8\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, arXiv:2002.08909Realm: Retrieval-augmented language model pre-training. 17arXiv preprintKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020. 1, 7\n\nMomentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9729-9738, 2020. 3\n\nMeasuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR), 2021. 9. the International Conference on Learning Representations (ICLR), 2021. 91328Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. 9, 13, 28\n\nRobust disambiguation of named entities in text. Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F\u00fcrstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, Gerhard Weikum, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingJohannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F\u00fcrstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. Robust disambiguation of named entities in text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pp. 782-792, 2011. URL https://aclanthology.org/D11-1072. 9\n\nTraining compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Aurelia Damoc, Simon Guy, Karen Osindero, Simonyan, Oriol Vinyals, and Laurent Sifre. Erich Elsen, Jack W. Rae1728. 1, 6, 8, 14Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203.15556. 1, 6, 8, 14, 17, 28\n\nMulti-task retrieval-augmented text generation with relevance sampling. Sebastian Hofst\u00e4tter, Jiecao Chen, Karthik Raman, Hamed Zamani, abs/2207.03030ArXiv. 715Sebastian Hofst\u00e4tter, Jiecao Chen, Karthik Raman, and Hamed Zamani. Multi-task retrieval-augmented text generation with relevance sampling. ArXiv, abs/2207.03030, 2022. 7, 15\n\nLearning deep structured semantic models for web search using clickthrough data. Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, Larry Heck, Proceedings of the 22nd ACM international conference on Information & Knowledge Management. the 22nd ACM international conference on Information & Knowledge Management7Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management, pp. 2333-2338, 2013. 3, 7\n\nLeveraging passage retrieval with generative models for open domain question answering. Gautier Izacard, Edouard Grave, arXiv:2007.01282714arXiv preprintGautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282, 2020. 2, 3, 7, 14\n\nDistilling knowledge from reader to retriever for question answering. Gautier Izacard, Edouard Grave, International Conference on Learning Representations. 514Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= NTEz-6wysdb. 4, 5, 14\n\nA memory efficient baseline for open domain question answering. Gautier Izacard, Fabio Petroni, Lucas Hosseini, Nicola De Cao, Sebastian Riedel, Edouard Grave, arXiv:2012.1515618arXiv preprintGautier Izacard, Fabio Petroni, Lucas Hosseini, Nicola De Cao, Sebastian Riedel, and Edouard Grave. A memory efficient baseline for open domain question answering. arXiv preprint arXiv:2012.15156, 2020. 18\n\nUnsupervised dense information retrieval with contrastive learning. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave, 37Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning, 2022. URL https://arxiv.org/abs/2112.09118. 2, 3, 7\n\nHow can we know what language models know?. Zhengbao Jiang, Frank F Xu, Jun Araki, Graham Neubig, 10.1162/tacl_a_00324Transactions of the Association for Computational Linguistics. 8Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020. doi: 10.1162/tacl_a_00324. URL https://aclanthology.org/2020.tacl-1.28. 8\n\nA statistical interpretation of term specificity and its application in retrieval. Karen Sparck, Jones , Journal of documentation. 7Karen Sparck Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation, 1972. 7\n\nTriviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, Proc. ACL. ACLMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proc. ACL, 2017. 9\n\nProduct quantization for nearest neighbor search. Herve J\u00e9gou, Matthijs Douze, Cordelia Schmid, 10.1109/TPAMI.2010.57.18IEEE Transactions on Pattern Analysis and Machine Intelligence. 331Herve J\u00e9gou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(1):117-128, 2011. doi: 10.1109/TPAMI. 2010.57. 18\n\nScaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. 8\n\nDense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, arXiv:2004.0490637arXiv preprintVladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. 3, 7\n\nGeneralization through memorization: Nearest neighbor language models. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, International Conference on Learning Representations. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH. 8\n\nUNIFIEDQA: Crossing format boundaries with a single QA system. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, 10.18653/v1/2020.findings-emnlp.171Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896-1907, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.171. URL https://aclanthology.org/ 2020.findings-emnlp.171. 28\n\nProofver: Natural logic theorem proving for fact verification. Amrith Krishna, Sebastian Riedel, Andreas Vlachos, Amrith Krishna, Sebastian Riedel, and Andreas Vlachos. Proofver: Natural logic theorem proving for fact verification, 2021. URL https://arxiv.org/abs/2108.11357. 15\n\nNatural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 10.1162/tacl_a_00276Transactions of the Association for Computational Linguistics. 79Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466, 2019. doi: 10.1162/tacl_a_00276. URL https://aclanthology.org/Q19-1026. 7, 9\n\nRACE: Large-scale ReAding comprehension dataset from examinations. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, Eduard Hovy, 10.18653/v1/D17-1082Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785-794, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082. 9\n\nInternet-augmented language models through few-shot prompting for open-domain question answering. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, Nikolai Grigorev, abs/2203.05115ArXiv. 814Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering. ArXiv, abs/2203.05115, 2022. 8, 14\n\nHow many data points is a prompt worth?. Le Teven, Alexander Scao, Rush, 10.18653/v1/2021.naacl-main.208Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsTeven Le Scao and Alexander Rush. How many data points is a prompt worth? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2627-2636, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.208. URL https://aclanthology.org/2021.naacl-main.208. 8\n\nYou only need one model for open-domain question answering. Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher D Manning, Kyoung-Gu Woo, Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher D. Manning, and Kyoung-Gu Woo. You only need one model for open-domain question answering, 2021a. URL https://arxiv.org/abs/ 2112.07381. 7\n\nLearning dense representations of phrases at scale. Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen, 10.18653/v1/2021.acl-long.518Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. Learning dense representations of phrases at scale. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 6634-6647, Online, August 2021b. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.518. URL https://aclanthology.org/2021.acl-long.518. 7\n\nLatent retrieval for weakly supervised open domain question answering. Kenton Lee, Ming-Wei Chang, Kristina Toutanova, Proc. ACL. ACLKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proc. ACL, 2019. 7\n\nThe power of scale for parameter-efficient prompt tuning. CoRR, abs/2104.08691, 2021. Brian Lester, Rami Al-Rfou, Noah Constant, Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. CoRR, abs/2104.08691, 2021. URL https://arxiv.org/abs/2104.08691. 8\n\nZero-shot relation extraction via reading comprehension. Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer, 10.18653/v1/K17-1034Proceedings of the 21st Conference on Computational Natural Language Learning. the 21st Conference on Computational Natural Language LearningOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pp. 333-342, 2017. doi: 10.18653/v1/K17-1034. URL https://aclanthology.org/K17-1034. 9\n\nRetrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-Tau Yih, Tim Rockt\u00e4schel, arXiv:2005.11401714arXiv preprintPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge- intensive nlp tasks. arXiv preprint arXiv:2005.11401, 2020. 1, 7, 14\n\nPrefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, 10.18653/v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics1Long Papers)Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4582-4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353. 8\n\nJurassic-1: Technical details and evaluation. Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham, AI21 LabsTechnical reportOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. Technical report, AI21 Labs, August 2021. 8\n\nCutting down on prompts and parameters: Simple few-shot learning with language models. L Iv Robert, Ivana Logan, Eric Balavzevi&apos;c, Fabio Wallace, Sameer Petroni, Sebastian Singh, Riedel, abs/2106.13353ArXiv. 8IV Robert L. Logan, Ivana Balavzevi'c, Eric Wallace, Fabio Petroni, Sameer Singh, and Sebastian Riedel. Cutting down on prompts and parameters: Simple few-shot learning with language models. ArXiv, abs/2106.13353, 2021. 8\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, 10.18653/v1/D18-1260Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2381-2391, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260.\n\nSewon Min, Julian Michael, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2004.10645Ambigqa: Answering ambiguous open-domain questions. arXiv preprintSewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. Ambigqa: Answering ambiguous open-domain questions. arXiv preprint arXiv:2004.10645, 2020. 7\n\nWebgpt: Browser-assisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, S Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, abs/2112.09332ArXiv. Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman8Reiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. ArXiv, abs/2112.09332, 2021. 8\n\nHindsight: Posterior-guided training of retrievers for improved open-ended generation. Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, Christopher D Manning, Ashwin Paranjape, Omar Khattab, Christopher Potts, Matei Zaharia, and Christopher D. Manning. Hindsight: Posterior-guided training of retrievers for improved open-ended generation, 2021. URL https://arxiv. org/abs/2110.07752. 7\n\nHow context affects language models' factual predictions. Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, arXiv:2005.04611arXiv preprintFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. How context affects language models' factual predictions. arXiv preprint arXiv:2005.04611, 2020. 9\n\nImproving wikipedia verifiability with ai. Fabio Petroni, Samuel Broscheit, Aleksandra Piktus, Patrick Lewis, Gautier Izacard, Lucas Hosseini, Jane Dwivedi-Yu, Maria Lomeli, Timo Schick, Pierre-Emmanuel Mazar\u00e9, Armand Joulin, Edouard Grave, Sebastian Riedel, Fabio Petroni, Samuel Broscheit, Aleksandra Piktus, Patrick Lewis, Gautier Izacard, Lucas Hosseini, Jane Dwivedi-Yu, Maria Lomeli, Timo Schick, Pierre-Emmanuel Mazar\u00e9, Armand Joulin, Edouard Grave, and Sebastian Riedel. Improving wikipedia verifiability with ai, 2022. URL https://arxiv.org/abs/2207. 06220. 7\n\nThe web is your oysterknowledge-intensive nlp against a very large web corpus. Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas O\u011fuz, Edouard Grave, Yih Wen-Tau, Sebastian Riedel, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas O\u011fuz, Edouard Grave, Wen-tau Yih, and Sebastian Riedel. The web is your oyster - knowledge-intensive nlp against a very large web corpus, 2021. URL https://arxiv.org/abs/2112.09924. 15\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Technical Report. 8Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Technical Report, 2019. 8\n\nScaling language models: Methods, analysis & insights from training gopher. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George Van Den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat Mcaleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero1415Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama; Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey IrvingJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2021. URL https://arxiv.org/abs/2112.11446. 1, 8, 10, 14, 15\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.1068335arXiv preprintColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. 3, 5\n\nSQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Proc. EMNLP. EMNLPPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proc. EMNLP, 2016. 7\n\nLearning to retrieve passages without supervision. Ori Ram, Omer Shachaf, Jonathan Levy, Amir Berant, Globerson, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsOri Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. Learning to retrieve passages without supervision. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2687-2700, Seattle, United States, July 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.naacl-main. 193. 7\n\nMCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text. Matthew Richardson, Proceedings of the 2013 Conference on Emprical Methods in Natural Language Processing (EMNLP 2013). the 2013 Conference on Emprical Methods in Natural Language Processing (EMNLP 2013)79Matthew Richardson. MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text. In Proceedings of the 2013 Conference on Emprical Methods in Natural Language Process- ing (EMNLP 2013), October 2013. URL https://www.microsoft.com/en-us/research/publication/ mctest-challenge-dataset-open-domain-machine-comprehension-text/. 7, 9\n\nOkapi at TREC-3. NIST Special Publication Sp. Steve Stephen E Robertson, Susan Walker, Micheline M Jones, Mike Hancock-Beaulieu, Gatford, Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at TREC-3. NIST Special Publication Sp, 1995. 7\n\nEnd-to-end training of multi-document reader and retriever for open-domain question answering. Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, Dani Yogatama, 57Devendra Singh Sachan, Siva Reddy, William Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of multi-document reader and retriever for open-domain question answering, 2021. URL https: //arxiv.org/abs/2106.05346. 4, 5, 7\n\nIt's not just size that matters: Small language models are also few-shot learners. Timo Schick, H Schutze, abs/2009.07118ArXiv. 8Timo Schick and H. Schutze. It's not just size that matters: Small language models are also few-shot learners. ArXiv, abs/2009.07118, 2021. 8\n\nExploiting cloze-questions for few-shot text classification and natural language inference. Timo Schick, Hinrich Sch\u00fctze, 10.18653/v1/2021.eacl-main.20Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeAssociation for Computational LinguisticsTimo Schick and Hinrich Sch\u00fctze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255-269. Association for Computational Linguistics, 2021a. doi: 10.18653/v1/2021.eacl-main.20. URL https://aclanthology.org/2021.eacl-main.20. 8\n\nFew-shot text generation with natural language instructions. Timo Schick, Hinrich Sch\u00fctze, 10.18653/v1/2021.emnlp-main.32Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsTimo Schick and Hinrich Sch\u00fctze. Few-shot text generation with natural language instructions. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 390-402. Association for Computational Linguistics, November 2021b. doi: 10.18653/v1/2021.emnlp-main.32. URL https: //aclanthology.org/2021.emnlp-main.32. 8\n\nLearning semantic representations using convolutional neural networks for web search. Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, Gr\u00e9goire Mesnil, 10.1145/2567948.2577348Proceedings of the 23rd International Conference on World Wide Web. the 23rd International Conference on World Wide WebYelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr\u00e9goire Mesnil. Learning semantic representations using convolutional neural networks for web search. In Proceedings of the 23rd International Conference on World Wide Web, pp. 373-374, 2014. doi: 10.1145/2567948.2577348. URL https://doi.org/10.1145/ 2567948.2577348. 7\n\nAutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, 10.18653/v1/2020.emnlp-main.346Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4222-4235, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.346. URL https://aclanthology.org/2020.emnlp-main.346. 8\n\nRetrieval augmentation reduces hallucination in conversation. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston, Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation, 2021. URL https://arxiv.org/abs/2104.07567. 7\n\nLanguage models that seek for knowledge: Modular search & generation for dialogue and prompt completion. Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur D Szlam, Jason Weston, abs/2203.13224ArXiv. 8Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur D. Szlam, and Jason Weston. Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion. ArXiv, abs/2203.13224, 2022. 8\n\nUsing deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick Legresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro, Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yaz- dani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model, 2022. URL https://arxiv.org/abs/2201.11990. 8\n\nImproving and simplifying pattern exploiting training. ArXiv, abs/2103.11955. Derek Tam, Mohit Rakesh R Menon, Shashank Bansal, Colin Srivastava, Raffel, Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. Improving and simplifying pattern exploiting training. ArXiv, abs/2103.11955, 2021. 8\n\n. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Amin Zheng, Marcelo Ghafouri, Yanping Menegali, Maxim Huang, Dmitry Krikun, James Lepikhin, Dehao Qin, Yuanzhong Chen, Zhifeng Xu, Adam Chen, Maarten Roberts, Yanqi Bosma, Chung-Ching Zhou, I A Chang, Willard James Krivokon, Marc Rusch, Kathleen S Pickett, Meier-Hellstern, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc LeMeredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Hartz S\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark D\u00edaz, Ben HutchinsonLamda: Language models for dialog applications. ArXiv, abs/2201.08239, 2022. 8Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Hartz S\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark D\u00edaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravindran Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. ArXiv, abs/2201.08239, 2022. 8\n\nFever: a large-scale dataset for fact extraction and verification. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal, arXiv:1803.0535579arXiv preprintJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018. 7, 9\n\nLearning to Learn: Introduction and Overview. Sebastian Thrun, Lorien Pratt, 0792380479. 8Kluwer Academic PublishersUSASebastian Thrun and Lorien Pratt. Learning to Learn: Introduction and Overview, pp. 3-17. Kluwer Academic Publishers, USA, 1998. ISBN 0792380479. 8\n\nMatching networks for one shot learning. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Kavukcuoglu Koray, Daan Wierstra, Advances in Neural Information Processing Systems. D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. GarnettCurran Associates, Inc29Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https: //proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf. 8\n\nThe TREC-8 question answering track report. M Ellen, Voorhees, TREC. Ellen M Voorhees et al. The TREC-8 question answering track report. In TREC, 1999. 7\n\nMulti-passage BERT: A globally normalized BERT model for open-domain question answering. Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, Bing Xiang, 10.18653/v1/D19-1599Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. Multi-passage BERT: A globally normalized BERT model for open-domain question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5878-5882, 2019. doi: 10.18653/v1/D19-1599. URL https://aclanthology.org/D19-1599. 7\n\nEmergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. URL https: //arxiv.org/abs/2206.07682. 8\n\nCCNet: Extracting high quality monolingual datasets from web crawl data. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, Edouard Grave, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the 12th Language Resources and Evaluation Conference, 2020. 10\n\nApproximate nearest neighbor negative contrastive learning for dense text retrieval. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk, arXiv:2007.00808arXiv preprintLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808, 2020. 7\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018. URL https://arxiv.org/abs/1809.09600. 9\n\nPretrained Transformers for Text Ranking: BERT and Beyond. Andrew Yates, Rodrigo Nogueira, Jimmy Lin, 10.1145/3437963.3441667Proceedings of the 14th ACM International Conference on Web Search and Data Mining, WSDM '21. the 14th ACM International Conference on Web Search and Data Mining, WSDM '21New York, NY, USA, 2021; IsraelAssociation for Computing MachineryAndrew Yates, Rodrigo Nogueira, and Jimmy Lin. Pretrained Transformers for Text Ranking: BERT and Beyond. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining, WSDM '21, pp. 1154-1156, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 978-1-4503-8297-7. doi: 10.1145/3437963.3441667. URL https://doi.org/10.1145/3437963.3441667. event-place: Virtual Event, Israel. 7\n\nLearning Discriminative Projections for Text Similarity Measures. Kristina Wen-Tau Yih, John C Toutanova, Christopher Platt, Meek, 978-1-932432-92-3Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL '11. the Fifteenth Conference on Computational Natural Language Learning, CoNLL '11USA; Portland, OregonAssociation for Computational LinguisticsWen-tau Yih, Kristina Toutanova, John C. Platt, and Christopher Meek. Learning Discriminative Projections for Text Similarity Measures. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL '11, pp. 247-256, USA, 2011. Association for Computational Linguistics. ISBN 978-1-932432-92-3. event-place: Portland, Oregon. 7\n\nAdaptive semiparametric language models. Dani Yogatama, Lingpeng Cyprien De Masson D&apos;autume, Kong, 10.1162/tacl_a_00371Transactions of the Association for Computational Linguistics. 9Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. Adaptive semiparametric language models. Transactions of the Association for Computational Linguistics, 9:362-373, 2021. doi: 10.1162/tacl_a_00371. URL https://aclanthology.org/2021.tacl-1.22. 1\n", "annotations": {"author": "[{\"end\":179,\"start\":69},{\"end\":286,\"start\":180},{\"end\":397,\"start\":287},{\"end\":491,\"start\":398},{\"end\":604,\"start\":492},{\"end\":709,\"start\":605},{\"end\":818,\"start\":710},{\"end\":926,\"start\":819},{\"end\":1037,\"start\":927},{\"end\":1144,\"start\":1038}]", "publisher": null, "author_last_name": "[{\"end\":84,\"start\":77},{\"end\":193,\"start\":188},{\"end\":299,\"start\":293},{\"end\":412,\"start\":404},{\"end\":505,\"start\":498},{\"end\":616,\"start\":610},{\"end\":725,\"start\":715},{\"end\":832,\"start\":826},{\"end\":943,\"start\":937},{\"end\":1051,\"start\":1046}]", "author_first_name": "[{\"end\":76,\"start\":69},{\"end\":187,\"start\":180},{\"end\":292,\"start\":287},{\"end\":403,\"start\":398},{\"end\":497,\"start\":492},{\"end\":609,\"start\":605},{\"end\":714,\"start\":710},{\"end\":825,\"start\":819},{\"end\":936,\"start\":927},{\"end\":1045,\"start\":1038}]", "author_affiliation": "[{\"end\":148,\"start\":102},{\"end\":178,\"start\":150},{\"end\":255,\"start\":209},{\"end\":285,\"start\":257},{\"end\":366,\"start\":320},{\"end\":396,\"start\":368},{\"end\":460,\"start\":414},{\"end\":490,\"start\":462},{\"end\":573,\"start\":527},{\"end\":603,\"start\":575},{\"end\":678,\"start\":632},{\"end\":708,\"start\":680},{\"end\":787,\"start\":741},{\"end\":817,\"start\":789},{\"end\":895,\"start\":849},{\"end\":925,\"start\":897},{\"end\":1006,\"start\":960},{\"end\":1036,\"start\":1008},{\"end\":1113,\"start\":1067},{\"end\":1143,\"start\":1115}]", "title": "[{\"end\":66,\"start\":1},{\"end\":1210,\"start\":1145}]", "venue": null, "abstract": "[{\"end\":2209,\"start\":1212}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2304,\"start\":2287},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2326,\"start\":2304},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2349,\"start\":2326},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3851,\"start\":3833},{\"end\":3885,\"start\":3851},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5403,\"start\":5379},{\"end\":10077,\"start\":10071},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":11139,\"start\":11119},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":13760,\"start\":13740},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":19090,\"start\":19067},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19108,\"start\":19090},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19133,\"start\":19108},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":19170,\"start\":19149},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19201,\"start\":19181},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19358,\"start\":19345},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":19381,\"start\":19358},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":19441,\"start\":19418},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19507,\"start\":19488},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":19649,\"start\":19631},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19668,\"start\":19649},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":19686,\"start\":19668},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":19997,\"start\":19977},{\"end\":20090,\"start\":20071},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20330,\"start\":20311},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20352,\"start\":20330},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":20370,\"start\":20352},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":20433,\"start\":20415},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":20456,\"start\":20433},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":20696,\"start\":20674},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20918,\"start\":20900},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21178,\"start\":21161},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21346,\"start\":21328},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":21452,\"start\":21432},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":21596,\"start\":21576},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21619,\"start\":21601},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":21867,\"start\":21844},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22069,\"start\":22045},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":22275,\"start\":22258},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22444,\"start\":22424},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22864,\"start\":22843},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":22916,\"start\":22891},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":23090,\"start\":23067},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":23493,\"start\":23472},{\"end\":23516,\"start\":23493},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":23537,\"start\":23516},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23609,\"start\":23586},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":23896,\"start\":23875},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23907,\"start\":23896},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":23928,\"start\":23907},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":24099,\"start\":24081},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":24257,\"start\":24236},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":24649,\"start\":24628},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":24666,\"start\":24649},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":24685,\"start\":24666},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24708,\"start\":24685},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":24727,\"start\":24708},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24751,\"start\":24729},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":25343,\"start\":25318},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":25366,\"start\":25343},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":25383,\"start\":25366},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":25400,\"start\":25383},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":25440,\"start\":25415},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25521,\"start\":25501},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":25539,\"start\":25521},{\"end\":25649,\"start\":25628},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25666,\"start\":25649},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":26975,\"start\":26949},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":27006,\"start\":26986},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":27039,\"start\":27020},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":27087,\"start\":27068},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27120,\"start\":27098},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27176,\"start\":27154},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":27228,\"start\":27208},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":27276,\"start\":27255},{\"end\":27572,\"start\":27548},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":28528,\"start\":28510},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28553,\"start\":28535},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":28592,\"start\":28569},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28787,\"start\":28761},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28822,\"start\":28802},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":28914,\"start\":28893},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29358,\"start\":29336},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":32338,\"start\":32317},{\"end\":41007,\"start\":41006},{\"end\":42033,\"start\":42009},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":44028,\"start\":44005},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49486,\"start\":49464},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":50545,\"start\":50522},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":51098,\"start\":51076},{\"end\":51425,\"start\":51402},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":53103,\"start\":53083},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":56123,\"start\":56104},{\"end\":60866,\"start\":60843},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":61009,\"start\":60986},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":61198,\"start\":61176}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":68395,\"start\":67970},{\"attributes\":{\"id\":\"fig_2\"},\"end\":68727,\"start\":68396},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":69619,\"start\":68728},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":70208,\"start\":69620},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":70685,\"start\":70209},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":71482,\"start\":70686},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":71815,\"start\":71483},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":72730,\"start\":71816},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":73189,\"start\":72731},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":73664,\"start\":73190},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":73765,\"start\":73665},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":73868,\"start\":73766},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":74259,\"start\":73869},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":74314,\"start\":74260},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":74593,\"start\":74315},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":79072,\"start\":74594},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":81873,\"start\":79073},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":82767,\"start\":81874},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":82858,\"start\":82768}]", "paragraph": "[{\"end\":3113,\"start\":2225},{\"end\":4128,\"start\":3115},{\"end\":4395,\"start\":4130},{\"end\":5556,\"start\":4397},{\"end\":5605,\"start\":5558},{\"end\":5757,\"start\":5607},{\"end\":5926,\"start\":5759},{\"end\":6065,\"start\":5928},{\"end\":6309,\"start\":6067},{\"end\":6464,\"start\":6311},{\"end\":6557,\"start\":6466},{\"end\":6736,\"start\":6559},{\"end\":6865,\"start\":6738},{\"end\":7834,\"start\":6907},{\"end\":8532,\"start\":7836},{\"end\":8700,\"start\":8594},{\"end\":9656,\"start\":8702},{\"end\":9942,\"start\":9732},{\"end\":10348,\"start\":9944},{\"end\":10947,\"start\":10396},{\"end\":11499,\"start\":10949},{\"end\":11566,\"start\":11526},{\"end\":11636,\"start\":11635},{\"end\":12108,\"start\":11638},{\"end\":12208,\"start\":12207},{\"end\":12647,\"start\":12210},{\"end\":13015,\"start\":12649},{\"end\":13180,\"start\":13033},{\"end\":13649,\"start\":13182},{\"end\":14232,\"start\":13651},{\"end\":14639,\"start\":14234},{\"end\":15679,\"start\":14675},{\"end\":16500,\"start\":15681},{\"end\":16760,\"start\":16502},{\"end\":16933,\"start\":16795},{\"end\":17168,\"start\":16954},{\"end\":17559,\"start\":17184},{\"end\":18157,\"start\":17586},{\"end\":18853,\"start\":18159},{\"end\":20134,\"start\":18913},{\"end\":20697,\"start\":20136},{\"end\":23235,\"start\":20721},{\"end\":23760,\"start\":23283},{\"end\":24100,\"start\":23782},{\"end\":24949,\"start\":24102},{\"end\":25861,\"start\":24951},{\"end\":26373,\"start\":25877},{\"end\":26508,\"start\":26388},{\"end\":27448,\"start\":26555},{\"end\":28635,\"start\":27503},{\"end\":29763,\"start\":28637},{\"end\":30034,\"start\":29785},{\"end\":30500,\"start\":30052},{\"end\":31726,\"start\":30502},{\"end\":32913,\"start\":31728},{\"end\":33155,\"start\":32945},{\"end\":33242,\"start\":33157},{\"end\":33332,\"start\":33244},{\"end\":35005,\"start\":33334},{\"end\":35288,\"start\":35007},{\"end\":36487,\"start\":35290},{\"end\":36696,\"start\":36503},{\"end\":37766,\"start\":36698},{\"end\":38404,\"start\":37800},{\"end\":39158,\"start\":38421},{\"end\":40421,\"start\":39189},{\"end\":41487,\"start\":40423},{\"end\":42591,\"start\":41489},{\"end\":43876,\"start\":42634},{\"end\":44300,\"start\":43878},{\"end\":45342,\"start\":44318},{\"end\":46393,\"start\":45359},{\"end\":47360,\"start\":46426},{\"end\":47960,\"start\":47362},{\"end\":49025,\"start\":47962},{\"end\":49989,\"start\":49027},{\"end\":50752,\"start\":49991},{\"end\":51513,\"start\":50795},{\"end\":52399,\"start\":51515},{\"end\":53104,\"start\":52401},{\"end\":54497,\"start\":53106},{\"end\":54827,\"start\":54519},{\"end\":55763,\"start\":54829},{\"end\":56242,\"start\":55765},{\"end\":56680,\"start\":56244},{\"end\":57676,\"start\":56695},{\"end\":57914,\"start\":57758},{\"end\":58142,\"start\":58050},{\"end\":58478,\"start\":58186},{\"end\":58810,\"start\":58501},{\"end\":61199,\"start\":58834},{\"end\":61734,\"start\":61209},{\"end\":63000,\"start\":61736},{\"end\":63697,\"start\":63002},{\"end\":64138,\"start\":63723},{\"end\":64357,\"start\":64161},{\"end\":64517,\"start\":64409},{\"end\":64634,\"start\":64564},{\"end\":64654,\"start\":64636},{\"end\":64849,\"start\":64656},{\"end\":65210,\"start\":64851},{\"end\":65682,\"start\":65212},{\"end\":66513,\"start\":65710},{\"end\":66788,\"start\":66526},{\"end\":66808,\"start\":66790},{\"end\":67024,\"start\":66810},{\"end\":67450,\"start\":67026},{\"end\":67913,\"start\":67452},{\"end\":67969,\"start\":67915}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8593,\"start\":8533},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9731,\"start\":9657},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10395,\"start\":10349},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11525,\"start\":11500},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11634,\"start\":11567},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12206,\"start\":12109},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16794,\"start\":16761},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16953,\"start\":16934},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17585,\"start\":17560}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33804,\"start\":33797},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35104,\"start\":35097},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":35554,\"start\":35547},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":36927,\"start\":36920},{\"end\":39338,\"start\":39331},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":41201,\"start\":41194},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":41770,\"start\":41763},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":42872,\"start\":42865},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":44399,\"start\":44392},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":45541,\"start\":45533},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":51798,\"start\":51790},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":52089,\"start\":52081},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":52757,\"start\":52749},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":53703,\"start\":53695},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":54050,\"start\":54042},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":60082,\"start\":60074},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":64356,\"start\":64348},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":65721,\"start\":65713},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":66400,\"start\":66393},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":66570,\"start\":66562},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":67968,\"start\":67960}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2223,\"start\":2211},{\"attributes\":{\"n\":\"2.2\"},\"end\":6905,\"start\":6868},{\"attributes\":{\"n\":\"2.3\"},\"end\":13031,\"start\":13018},{\"attributes\":{\"n\":\"2.4\"},\"end\":14673,\"start\":14642},{\"end\":17182,\"start\":17171},{\"attributes\":{\"n\":\"3\"},\"end\":18868,\"start\":18856},{\"attributes\":{\"n\":\"3.1\"},\"end\":18911,\"start\":18871},{\"end\":20719,\"start\":20700},{\"end\":23281,\"start\":23238},{\"attributes\":{\"n\":\"3.2\"},\"end\":23780,\"start\":23763},{\"attributes\":{\"n\":\"4\"},\"end\":25875,\"start\":25864},{\"attributes\":{\"n\":\"4.1\"},\"end\":26386,\"start\":26376},{\"end\":26553,\"start\":26511},{\"end\":27501,\"start\":27451},{\"attributes\":{\"n\":\"4.2\"},\"end\":29783,\"start\":29766},{\"end\":30050,\"start\":30037},{\"attributes\":{\"n\":\"4.3\"},\"end\":32943,\"start\":32916},{\"attributes\":{\"n\":\"4.4\"},\"end\":36501,\"start\":36490},{\"attributes\":{\"n\":\"4.5\"},\"end\":37798,\"start\":37769},{\"attributes\":{\"n\":\"4.5.1\"},\"end\":38419,\"start\":38407},{\"end\":39187,\"start\":39161},{\"attributes\":{\"n\":\"4.5.2\"},\"end\":42632,\"start\":42594},{\"attributes\":{\"n\":\"4.5.3\"},\"end\":44316,\"start\":44303},{\"attributes\":{\"n\":\"4.5.4\"},\"end\":45357,\"start\":45345},{\"attributes\":{\"n\":\"5.1\"},\"end\":46424,\"start\":46396},{\"attributes\":{\"n\":\"5.2\"},\"end\":50793,\"start\":50755},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":54517,\"start\":54500},{\"attributes\":{\"n\":\"6\"},\"end\":56693,\"start\":56683},{\"end\":57720,\"start\":57679},{\"end\":57731,\"start\":57723},{\"end\":57756,\"start\":57734},{\"end\":58048,\"start\":57917},{\"end\":58184,\"start\":58145},{\"end\":58499,\"start\":58481},{\"end\":58832,\"start\":58813},{\"end\":61207,\"start\":61202},{\"end\":63721,\"start\":63700},{\"end\":64159,\"start\":64141},{\"end\":64382,\"start\":64360},{\"end\":64407,\"start\":64385},{\"end\":64562,\"start\":64520},{\"end\":65708,\"start\":65685},{\"end\":66524,\"start\":66516},{\"end\":67981,\"start\":67971},{\"end\":68407,\"start\":68397},{\"end\":68738,\"start\":68729},{\"end\":69630,\"start\":69621},{\"end\":70219,\"start\":70210},{\"end\":70696,\"start\":70687},{\"end\":71493,\"start\":71484},{\"end\":71826,\"start\":71817},{\"end\":72741,\"start\":72732},{\"end\":73200,\"start\":73191},{\"end\":73676,\"start\":73666},{\"end\":73777,\"start\":73767},{\"end\":73880,\"start\":73870},{\"end\":74271,\"start\":74261},{\"end\":74326,\"start\":74316},{\"end\":74605,\"start\":74595},{\"end\":79084,\"start\":79074},{\"end\":81895,\"start\":81875},{\"end\":82779,\"start\":82769}]", "table": "[{\"end\":69619,\"start\":69104},{\"end\":70208,\"start\":69926},{\"end\":70685,\"start\":70377},{\"end\":71482,\"start\":70817},{\"end\":71815,\"start\":71670},{\"end\":72730,\"start\":72132},{\"end\":73189,\"start\":73145},{\"end\":73664,\"start\":73517},{\"end\":73765,\"start\":73726},{\"end\":73868,\"start\":73833},{\"end\":74259,\"start\":73911},{\"end\":74314,\"start\":74298},{\"end\":74593,\"start\":74378},{\"end\":79072,\"start\":74686},{\"end\":81873,\"start\":79183},{\"end\":82767,\"start\":82348}]", "figure_caption": "[{\"end\":68395,\"start\":67983},{\"end\":68727,\"start\":68409},{\"end\":69104,\"start\":68740},{\"end\":69926,\"start\":69632},{\"end\":70377,\"start\":70221},{\"end\":70817,\"start\":70698},{\"end\":71670,\"start\":71495},{\"end\":72132,\"start\":71828},{\"end\":73145,\"start\":72743},{\"end\":73517,\"start\":73202},{\"end\":73726,\"start\":73679},{\"end\":73833,\"start\":73780},{\"end\":73911,\"start\":73883},{\"end\":74298,\"start\":74274},{\"end\":74378,\"start\":74329},{\"end\":74686,\"start\":74608},{\"end\":79183,\"start\":79087},{\"end\":82348,\"start\":81900},{\"end\":82858,\"start\":82782}]", "figure_ref": "[{\"end\":3947,\"start\":3939},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46728,\"start\":46720},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":47503,\"start\":47495},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":56277,\"start\":56269}]", "bib_author_first_name": "[{\"end\":83975,\"start\":83968},{\"end\":83996,\"start\":83988},{\"end\":84015,\"start\":84008},{\"end\":84030,\"start\":84023},{\"end\":84045,\"start\":84036},{\"end\":84059,\"start\":84054},{\"end\":84330,\"start\":84321},{\"end\":84347,\"start\":84341},{\"end\":84362,\"start\":84356},{\"end\":84379,\"start\":84373},{\"end\":84390,\"start\":84385},{\"end\":84408,\"start\":84403},{\"end\":84425,\"start\":84419},{\"end\":84458,\"start\":84445},{\"end\":84474,\"start\":84468},{\"end\":84487,\"start\":84482},{\"end\":85005,\"start\":85000},{\"end\":85018,\"start\":85011},{\"end\":85269,\"start\":85266},{\"end\":85271,\"start\":85270},{\"end\":85287,\"start\":85279},{\"end\":85298,\"start\":85294},{\"end\":85313,\"start\":85306},{\"end\":85328,\"start\":85323},{\"end\":85345,\"start\":85337},{\"end\":85362,\"start\":85356},{\"end\":85382,\"start\":85376},{\"end\":85396,\"start\":85390},{\"end\":85411,\"start\":85405},{\"end\":85428,\"start\":85420},{\"end\":85443,\"start\":85438},{\"end\":85466,\"start\":85458},{\"end\":85479,\"start\":85476},{\"end\":85495,\"start\":85490},{\"end\":85509,\"start\":85503},{\"end\":85524,\"start\":85518},{\"end\":85526,\"start\":85525},{\"end\":85543,\"start\":85536},{\"end\":85555,\"start\":85548},{\"end\":85575,\"start\":85564},{\"end\":85587,\"start\":85583},{\"end\":85598,\"start\":85594},{\"end\":85614,\"start\":85607},{\"end\":85628,\"start\":85623},{\"end\":86334,\"start\":86328},{\"end\":86350,\"start\":86343},{\"end\":86369,\"start\":86360},{\"end\":86383,\"start\":86378},{\"end\":86726,\"start\":86718},{\"end\":86743,\"start\":86736},{\"end\":86756,\"start\":86752},{\"end\":86773,\"start\":86766},{\"end\":86790,\"start\":86785},{\"end\":86814,\"start\":86805},{\"end\":86824,\"start\":86820},{\"end\":86837,\"start\":86834},{\"end\":86839,\"start\":86838},{\"end\":86851,\"start\":86847},{\"end\":87374,\"start\":87369},{\"end\":87385,\"start\":87381},{\"end\":87398,\"start\":87393},{\"end\":87414,\"start\":87407},{\"end\":87625,\"start\":87616},{\"end\":87643,\"start\":87637},{\"end\":87657,\"start\":87652},{\"end\":87673,\"start\":87666},{\"end\":87687,\"start\":87681},{\"end\":87700,\"start\":87696},{\"end\":87714,\"start\":87710},{\"end\":87741,\"start\":87734},{\"end\":87758,\"start\":87749},{\"end\":87773,\"start\":87767},{\"end\":87790,\"start\":87784},{\"end\":87803,\"start\":87798},{\"end\":87815,\"start\":87809},{\"end\":87839,\"start\":87831},{\"end\":87854,\"start\":87848},{\"end\":87862,\"start\":87860},{\"end\":87875,\"start\":87871},{\"end\":87891,\"start\":87881},{\"end\":87906,\"start\":87901},{\"end\":87923,\"start\":87920},{\"end\":87933,\"start\":87930},{\"end\":87944,\"start\":87938},{\"end\":87962,\"start\":87957},{\"end\":87974,\"start\":87969},{\"end\":87992,\"start\":87985},{\"end\":88004,\"start\":88001},{\"end\":88021,\"start\":88012},{\"end\":88035,\"start\":88031},{\"end\":88047,\"start\":88041},{\"end\":88060,\"start\":88054},{\"end\":88077,\"start\":88071},{\"end\":88094,\"start\":88088},{\"end\":88106,\"start\":88100},{\"end\":88126,\"start\":88120},{\"end\":88140,\"start\":88135},{\"end\":88152,\"start\":88148},{\"end\":88168,\"start\":88163},{\"end\":88182,\"start\":88176},{\"end\":88194,\"start\":88189},{\"end\":88214,\"start\":88205},{\"end\":89829,\"start\":89818},{\"end\":89841,\"start\":89837},{\"end\":90069,\"start\":90064},{\"end\":90082,\"start\":90077},{\"end\":90095,\"start\":90091},{\"end\":90111,\"start\":90105},{\"end\":90124,\"start\":90118},{\"end\":90143,\"start\":90136},{\"end\":90161,\"start\":90155},{\"end\":90503,\"start\":90498},{\"end\":90520,\"start\":90512},{\"end\":90534,\"start\":90528},{\"end\":90548,\"start\":90540},{\"end\":90813,\"start\":90807},{\"end\":90829,\"start\":90823},{\"end\":90831,\"start\":90830},{\"end\":90844,\"start\":90838},{\"end\":90851,\"start\":90845},{\"end\":90871,\"start\":90865},{\"end\":90886,\"start\":90881},{\"end\":90906,\"start\":90899},{\"end\":90908,\"start\":90907},{\"end\":91402,\"start\":91397},{\"end\":91417,\"start\":91410},{\"end\":91430,\"start\":91426},{\"end\":91446,\"start\":91440},{\"end\":91459,\"start\":91452},{\"end\":91471,\"start\":91466},{\"end\":91893,\"start\":91888},{\"end\":91908,\"start\":91901},{\"end\":91917,\"start\":91914},{\"end\":91936,\"start\":91929},{\"end\":91951,\"start\":91944},{\"end\":91965,\"start\":91961},{\"end\":91986,\"start\":91978},{\"end\":92001,\"start\":91997},{\"end\":92868,\"start\":92864},{\"end\":92884,\"start\":92878},{\"end\":92904,\"start\":92898},{\"end\":92923,\"start\":92913},{\"end\":92941,\"start\":92933},{\"end\":92958,\"start\":92948},{\"end\":92974,\"start\":92969},{\"end\":93606,\"start\":93600},{\"end\":93621,\"start\":93615},{\"end\":93636,\"start\":93631},{\"end\":93650,\"start\":93645},{\"end\":94172,\"start\":94165},{\"end\":94657,\"start\":94651},{\"end\":94667,\"start\":94663},{\"end\":94680,\"start\":94675},{\"end\":95662,\"start\":95655},{\"end\":95677,\"start\":95670},{\"end\":95967,\"start\":95960},{\"end\":95984,\"start\":95975},{\"end\":95998,\"start\":95992},{\"end\":96244,\"start\":96237},{\"end\":96258,\"start\":96252},{\"end\":96274,\"start\":96267},{\"end\":96568,\"start\":96562},{\"end\":96580,\"start\":96574},{\"end\":96590,\"start\":96586},{\"end\":96605,\"start\":96597},{\"end\":96623,\"start\":96615},{\"end\":96967,\"start\":96960},{\"end\":96977,\"start\":96972},{\"end\":96988,\"start\":96983},{\"end\":97000,\"start\":96993},{\"end\":97010,\"start\":97006},{\"end\":97467,\"start\":97464},{\"end\":97485,\"start\":97479},{\"end\":97499,\"start\":97493},{\"end\":97512,\"start\":97508},{\"end\":97524,\"start\":97518},{\"end\":97538,\"start\":97534},{\"end\":97550,\"start\":97545},{\"end\":98037,\"start\":98029},{\"end\":98054,\"start\":98047},{\"end\":98059,\"start\":98055},{\"end\":98073,\"start\":98067},{\"end\":98088,\"start\":98083},{\"end\":98107,\"start\":98100},{\"end\":98120,\"start\":98116},{\"end\":98137,\"start\":98130},{\"end\":98152,\"start\":98146},{\"end\":98168,\"start\":98161},{\"end\":98743,\"start\":98737},{\"end\":98763,\"start\":98754},{\"end\":98780,\"start\":98774},{\"end\":98794,\"start\":98789},{\"end\":98814,\"start\":98808},{\"end\":98825,\"start\":98820},{\"end\":98843,\"start\":98838},{\"end\":98856,\"start\":98852},{\"end\":98861,\"start\":98857},{\"end\":98877,\"start\":98869},{\"end\":98894,\"start\":98889},{\"end\":98905,\"start\":98902},{\"end\":98917,\"start\":98913},{\"end\":98933,\"start\":98928},{\"end\":98948,\"start\":98942},{\"end\":98965,\"start\":98959},{\"end\":98992,\"start\":98985},{\"end\":99005,\"start\":99000},{\"end\":99016,\"start\":99011},{\"end\":99665,\"start\":99656},{\"end\":99684,\"start\":99678},{\"end\":99698,\"start\":99691},{\"end\":99711,\"start\":99706},{\"end\":100007,\"start\":100001},{\"end\":100023,\"start\":100015},{\"end\":100036,\"start\":100028},{\"end\":100044,\"start\":100042},{\"end\":100055,\"start\":100051},{\"end\":100068,\"start\":100063},{\"end\":100619,\"start\":100612},{\"end\":100636,\"start\":100629},{\"end\":100929,\"start\":100922},{\"end\":100946,\"start\":100939},{\"end\":101310,\"start\":101303},{\"end\":101325,\"start\":101320},{\"end\":101340,\"start\":101335},{\"end\":101357,\"start\":101351},{\"end\":101360,\"start\":101358},{\"end\":101375,\"start\":101366},{\"end\":101391,\"start\":101384},{\"end\":101713,\"start\":101706},{\"end\":101731,\"start\":101723},{\"end\":101744,\"start\":101739},{\"end\":101764,\"start\":101755},{\"end\":101778,\"start\":101773},{\"end\":101797,\"start\":101791},{\"end\":101813,\"start\":101806},{\"end\":102115,\"start\":102107},{\"end\":102128,\"start\":102123},{\"end\":102130,\"start\":102129},{\"end\":102138,\"start\":102135},{\"end\":102152,\"start\":102146},{\"end\":102590,\"start\":102585},{\"end\":102604,\"start\":102599},{\"end\":102868,\"start\":102862},{\"end\":102882,\"start\":102876},{\"end\":102895,\"start\":102889},{\"end\":102897,\"start\":102896},{\"end\":102908,\"start\":102904},{\"end\":103169,\"start\":103164},{\"end\":103185,\"start\":103177},{\"end\":103201,\"start\":103193},{\"end\":103565,\"start\":103560},{\"end\":103577,\"start\":103574},{\"end\":103593,\"start\":103590},{\"end\":103607,\"start\":103604},{\"end\":103609,\"start\":103608},{\"end\":103625,\"start\":103617},{\"end\":103638,\"start\":103633},{\"end\":103651,\"start\":103646},{\"end\":103662,\"start\":103658},{\"end\":103679,\"start\":103672},{\"end\":103689,\"start\":103684},{\"end\":103997,\"start\":103989},{\"end\":104015,\"start\":104009},{\"end\":104027,\"start\":104022},{\"end\":104039,\"start\":104033},{\"end\":104050,\"start\":104044},{\"end\":104064,\"start\":104059},{\"end\":104078,\"start\":104071},{\"end\":104398,\"start\":104391},{\"end\":104415,\"start\":104411},{\"end\":104425,\"start\":104422},{\"end\":104440,\"start\":104436},{\"end\":104458,\"start\":104454},{\"end\":104853,\"start\":104847},{\"end\":104869,\"start\":104864},{\"end\":104881,\"start\":104875},{\"end\":104894,\"start\":104888},{\"end\":104912,\"start\":104906},{\"end\":104927,\"start\":104922},{\"end\":104943,\"start\":104935},{\"end\":105604,\"start\":105598},{\"end\":105623,\"start\":105614},{\"end\":105639,\"start\":105632},{\"end\":105882,\"start\":105879},{\"end\":105906,\"start\":105896},{\"end\":105923,\"start\":105917},{\"end\":105941,\"start\":105934},{\"end\":105956,\"start\":105951},{\"end\":105970,\"start\":105965},{\"end\":105988,\"start\":105980},{\"end\":106003,\"start\":105998},{\"end\":106021,\"start\":106016},{\"end\":106036,\"start\":106030},{\"end\":106050,\"start\":106042},{\"end\":106067,\"start\":106062},{\"end\":106082,\"start\":106075},{\"end\":106099,\"start\":106091},{\"end\":106113,\"start\":106107},{\"end\":106115,\"start\":106114},{\"end\":106126,\"start\":106121},{\"end\":106142,\"start\":106138},{\"end\":106151,\"start\":106147},{\"end\":106820,\"start\":106814},{\"end\":106831,\"start\":106826},{\"end\":106844,\"start\":106837},{\"end\":106856,\"start\":106850},{\"end\":106869,\"start\":106863},{\"end\":107607,\"start\":107599},{\"end\":107624,\"start\":107619},{\"end\":107646,\"start\":107638},{\"end\":107665,\"start\":107658},{\"end\":107958,\"start\":107956},{\"end\":107975,\"start\":107966},{\"end\":108792,\"start\":108786},{\"end\":108803,\"start\":108798},{\"end\":108818,\"start\":108811},{\"end\":108830,\"start\":108824},{\"end\":108853,\"start\":108842},{\"end\":108855,\"start\":108854},{\"end\":108874,\"start\":108865},{\"end\":109147,\"start\":109140},{\"end\":109159,\"start\":109153},{\"end\":109172,\"start\":109166},{\"end\":109184,\"start\":109179},{\"end\":110127,\"start\":110121},{\"end\":110141,\"start\":110133},{\"end\":110157,\"start\":110149},{\"end\":110420,\"start\":110415},{\"end\":110433,\"start\":110429},{\"end\":110447,\"start\":110443},{\"end\":110693,\"start\":110689},{\"end\":110707,\"start\":110700},{\"end\":110719,\"start\":110713},{\"end\":110730,\"start\":110726},{\"end\":111276,\"start\":111269},{\"end\":111289,\"start\":111284},{\"end\":111308,\"start\":111297},{\"end\":111322,\"start\":111317},{\"end\":111340,\"start\":111332},{\"end\":111357,\"start\":111352},{\"end\":111373,\"start\":111365},{\"end\":111387,\"start\":111383},{\"end\":111402,\"start\":111395},{\"end\":111411,\"start\":111408},{\"end\":111802,\"start\":111798},{\"end\":111815,\"start\":111810},{\"end\":112721,\"start\":112716},{\"end\":112732,\"start\":112730},{\"end\":112746,\"start\":112741},{\"end\":112757,\"start\":112753},{\"end\":113024,\"start\":113023},{\"end\":113041,\"start\":113036},{\"end\":113053,\"start\":113049},{\"end\":113077,\"start\":113072},{\"end\":113093,\"start\":113087},{\"end\":113112,\"start\":113103},{\"end\":113467,\"start\":113462},{\"end\":113483,\"start\":113478},{\"end\":113497,\"start\":113491},{\"end\":113510,\"start\":113504},{\"end\":114175,\"start\":114170},{\"end\":114187,\"start\":114181},{\"end\":114205,\"start\":114197},{\"end\":114222,\"start\":114218},{\"end\":114556,\"start\":114547},{\"end\":114570,\"start\":114565},{\"end\":114580,\"start\":114579},{\"end\":114593,\"start\":114589},{\"end\":114602,\"start\":114598},{\"end\":114620,\"start\":114611},{\"end\":114637,\"start\":114626},{\"end\":114653,\"start\":114645},{\"end\":114666,\"start\":114660},{\"end\":114684,\"start\":114677},{\"end\":114697,\"start\":114695},{\"end\":114709,\"start\":114705},{\"end\":114721,\"start\":114717},{\"end\":114740,\"start\":114732},{\"end\":115299,\"start\":115293},{\"end\":115315,\"start\":115311},{\"end\":115336,\"start\":115325},{\"end\":115349,\"start\":115344},{\"end\":115370,\"start\":115359},{\"end\":115372,\"start\":115371},{\"end\":115674,\"start\":115669},{\"end\":115691,\"start\":115684},{\"end\":115709,\"start\":115699},{\"end\":115721,\"start\":115718},{\"end\":115742,\"start\":115735},{\"end\":115756,\"start\":115747},{\"end\":115758,\"start\":115757},{\"end\":115776,\"start\":115767},{\"end\":116083,\"start\":116078},{\"end\":116099,\"start\":116093},{\"end\":116121,\"start\":116111},{\"end\":116137,\"start\":116130},{\"end\":116152,\"start\":116145},{\"end\":116167,\"start\":116162},{\"end\":116182,\"start\":116178},{\"end\":116200,\"start\":116195},{\"end\":116213,\"start\":116209},{\"end\":116237,\"start\":116222},{\"end\":116252,\"start\":116246},{\"end\":116268,\"start\":116261},{\"end\":116285,\"start\":116276},{\"end\":116694,\"start\":116684},{\"end\":116708,\"start\":116703},{\"end\":116726,\"start\":116718},{\"end\":116744,\"start\":116738},{\"end\":116760,\"start\":116754},{\"end\":116779,\"start\":116772},{\"end\":116796,\"start\":116789},{\"end\":116810,\"start\":116804},{\"end\":116824,\"start\":116817},{\"end\":116835,\"start\":116832},{\"end\":116854,\"start\":116845},{\"end\":117233,\"start\":117229},{\"end\":117250,\"start\":117243},{\"end\":117260,\"start\":117255},{\"end\":117273,\"start\":117268},{\"end\":117285,\"start\":117280},{\"end\":117298,\"start\":117294},{\"end\":117588,\"start\":117584},{\"end\":117590,\"start\":117589},{\"end\":117605,\"start\":117596},{\"end\":117622,\"start\":117616},{\"end\":117633,\"start\":117628},{\"end\":117650,\"start\":117644},{\"end\":117668,\"start\":117661},{\"end\":117679,\"start\":117675},{\"end\":117696,\"start\":117691},{\"end\":117713,\"start\":117708},{\"end\":117728,\"start\":117720},{\"end\":117741,\"start\":117736},{\"end\":117757,\"start\":117754},{\"end\":117773,\"start\":117768},{\"end\":117787,\"start\":117782},{\"end\":117805,\"start\":117798},{\"end\":117820,\"start\":117814},{\"end\":117844,\"start\":117840},{\"end\":117849,\"start\":117845},{\"end\":117869,\"start\":117861},{\"end\":117882,\"start\":117876},{\"end\":117896,\"start\":117890},{\"end\":117913,\"start\":117905},{\"end\":117928,\"start\":117921},{\"end\":117947,\"start\":117940},{\"end\":117963,\"start\":117955},{\"end\":117976,\"start\":117972},{\"end\":117990,\"start\":117985},{\"end\":118007,\"start\":118000},{\"end\":118021,\"start\":118018},{\"end\":118035,\"start\":118032},{\"end\":118045,\"start\":118040},{\"end\":118061,\"start\":118053},{\"end\":118078,\"start\":118073},{\"end\":118097,\"start\":118092},{\"end\":118110,\"start\":118106},{\"end\":118128,\"start\":118123},{\"end\":118146,\"start\":118139},{\"end\":118164,\"start\":118157},{\"end\":118176,\"start\":118172},{\"end\":118191,\"start\":118186},{\"end\":118200,\"start\":118192},{\"end\":118213,\"start\":118205},{\"end\":118227,\"start\":118223},{\"end\":118245,\"start\":118240},{\"end\":118266,\"start\":118259},{\"end\":118283,\"start\":118275},{\"end\":118301,\"start\":118295},{\"end\":118323,\"start\":118310},{\"end\":118338,\"start\":118333},{\"end\":120393,\"start\":120388},{\"end\":120406,\"start\":120402},{\"end\":120420,\"start\":120416},{\"end\":120439,\"start\":120430},{\"end\":120451,\"start\":120445},{\"end\":120467,\"start\":120460},{\"end\":120481,\"start\":120476},{\"end\":120491,\"start\":120488},{\"end\":120503,\"start\":120496},{\"end\":120861,\"start\":120855},{\"end\":120877,\"start\":120873},{\"end\":120895,\"start\":120885},{\"end\":120910,\"start\":120905},{\"end\":121143,\"start\":121140},{\"end\":121153,\"start\":121149},{\"end\":121171,\"start\":121163},{\"end\":121182,\"start\":121178},{\"end\":122037,\"start\":122030},{\"end\":122634,\"start\":122629},{\"end\":122661,\"start\":122656},{\"end\":122679,\"start\":122670},{\"end\":122681,\"start\":122680},{\"end\":122693,\"start\":122689},{\"end\":122978,\"start\":122970},{\"end\":122997,\"start\":122993},{\"end\":123012,\"start\":123005},{\"end\":123028,\"start\":123023},{\"end\":123039,\"start\":123035},{\"end\":123372,\"start\":123368},{\"end\":123382,\"start\":123381},{\"end\":123653,\"start\":123649},{\"end\":123669,\"start\":123662},{\"end\":124441,\"start\":124437},{\"end\":124457,\"start\":124450},{\"end\":125136,\"start\":125130},{\"end\":125151,\"start\":125143},{\"end\":125164,\"start\":125156},{\"end\":125172,\"start\":125170},{\"end\":125187,\"start\":125179},{\"end\":125760,\"start\":125754},{\"end\":125774,\"start\":125767},{\"end\":125790,\"start\":125784},{\"end\":125792,\"start\":125791},{\"end\":125801,\"start\":125800},{\"end\":125803,\"start\":125802},{\"end\":125810,\"start\":125806},{\"end\":125826,\"start\":125820},{\"end\":126591,\"start\":126587},{\"end\":126608,\"start\":126601},{\"end\":126619,\"start\":126615},{\"end\":126631,\"start\":126626},{\"end\":126644,\"start\":126639},{\"end\":126941,\"start\":126937},{\"end\":126958,\"start\":126951},{\"end\":126975,\"start\":126968},{\"end\":126992,\"start\":126985},{\"end\":127007,\"start\":127001},{\"end\":127009,\"start\":127008},{\"end\":127022,\"start\":127017},{\"end\":127400,\"start\":127394},{\"end\":127415,\"start\":127408},{\"end\":127432,\"start\":127425},{\"end\":127448,\"start\":127441},{\"end\":127466,\"start\":127460},{\"end\":127485,\"start\":127480},{\"end\":127498,\"start\":127494},{\"end\":127511,\"start\":127504},{\"end\":127530,\"start\":127524},{\"end\":127545,\"start\":127540},{\"end\":127564,\"start\":127559},{\"end\":127577,\"start\":127572},{\"end\":127589,\"start\":127585},{\"end\":127597,\"start\":127590},{\"end\":127614,\"start\":127609},{\"end\":127628,\"start\":127625},{\"end\":127643,\"start\":127635},{\"end\":127660,\"start\":127653},{\"end\":127672,\"start\":127665},{\"end\":127689,\"start\":127682},{\"end\":127703,\"start\":127698},{\"end\":128277,\"start\":128272},{\"end\":128288,\"start\":128283},{\"end\":128313,\"start\":128305},{\"end\":128327,\"start\":128322},{\"end\":128522,\"start\":128517},{\"end\":128540,\"start\":128534},{\"end\":128543,\"start\":128541},{\"end\":128558,\"start\":128553},{\"end\":128569,\"start\":128565},{\"end\":128571,\"start\":128570},{\"end\":128587,\"start\":128581},{\"end\":128618,\"start\":128612},{\"end\":128632,\"start\":128626},{\"end\":128644,\"start\":128638},{\"end\":128652,\"start\":128650},{\"end\":128667,\"start\":128660},{\"end\":128679,\"start\":128672},{\"end\":128691,\"start\":128684},{\"end\":128701,\"start\":128697},{\"end\":128716,\"start\":128709},{\"end\":128734,\"start\":128727},{\"end\":128750,\"start\":128745},{\"end\":128764,\"start\":128758},{\"end\":128778,\"start\":128773},{\"end\":128794,\"start\":128789},{\"end\":128809,\"start\":128800},{\"end\":128823,\"start\":128816},{\"end\":128832,\"start\":128828},{\"end\":128846,\"start\":128839},{\"end\":128861,\"start\":128856},{\"end\":128880,\"start\":128869},{\"end\":128888,\"start\":128887},{\"end\":128890,\"start\":128889},{\"end\":128905,\"start\":128898},{\"end\":128911,\"start\":128906},{\"end\":128926,\"start\":128922},{\"end\":128942,\"start\":128934},{\"end\":128944,\"start\":128943},{\"end\":130512,\"start\":130507},{\"end\":130528,\"start\":130521},{\"end\":130546,\"start\":130538},{\"end\":130572,\"start\":130567},{\"end\":130858,\"start\":130849},{\"end\":130872,\"start\":130866},{\"end\":131117,\"start\":131112},{\"end\":131134,\"start\":131127},{\"end\":131152,\"start\":131145},{\"end\":131175,\"start\":131164},{\"end\":131187,\"start\":131183},{\"end\":131767,\"start\":131766},{\"end\":131972,\"start\":131966},{\"end\":131986,\"start\":131979},{\"end\":131998,\"start\":131991},{\"end\":132009,\"start\":132003},{\"end\":132025,\"start\":132021},{\"end\":132870,\"start\":132865},{\"end\":132878,\"start\":132876},{\"end\":132889,\"start\":132884},{\"end\":132906,\"start\":132901},{\"end\":132921,\"start\":132915},{\"end\":132937,\"start\":132928},{\"end\":132952,\"start\":132948},{\"end\":132970,\"start\":132963},{\"end\":132983,\"start\":132978},{\"end\":132996,\"start\":132990},{\"end\":133008,\"start\":133006},{\"end\":133010,\"start\":133009},{\"end\":133025,\"start\":133016},{\"end\":133042,\"start\":133037},{\"end\":133057,\"start\":133052},{\"end\":133069,\"start\":133065},{\"end\":133083,\"start\":133076},{\"end\":133497,\"start\":133488},{\"end\":133516,\"start\":133506},{\"end\":133532,\"start\":133526},{\"end\":133549,\"start\":133542},{\"end\":133570,\"start\":133561},{\"end\":133585,\"start\":133579},{\"end\":133601,\"start\":133594},{\"end\":134101,\"start\":134098},{\"end\":134116,\"start\":134109},{\"end\":134126,\"start\":134124},{\"end\":134140,\"start\":134131},{\"end\":134153,\"start\":134147},{\"end\":134163,\"start\":134159},{\"end\":134179,\"start\":134173},{\"end\":134193,\"start\":134187},{\"end\":134552,\"start\":134546},{\"end\":134563,\"start\":134559},{\"end\":134576,\"start\":134568},{\"end\":134590,\"start\":134584},{\"end\":134606,\"start\":134599},{\"end\":134608,\"start\":134607},{\"end\":134622,\"start\":134616},{\"end\":134649,\"start\":134638},{\"end\":134651,\"start\":134650},{\"end\":134969,\"start\":134963},{\"end\":134984,\"start\":134977},{\"end\":135000,\"start\":134995},{\"end\":135758,\"start\":135750},{\"end\":135776,\"start\":135772},{\"end\":135778,\"start\":135777},{\"end\":135801,\"start\":135790},{\"end\":136471,\"start\":136467},{\"end\":136490,\"start\":136482}]", "bib_author_last_name": "[{\"end\":83986,\"start\":83976},{\"end\":84006,\"start\":83997},{\"end\":84021,\"start\":84016},{\"end\":84034,\"start\":84031},{\"end\":84052,\"start\":84046},{\"end\":84067,\"start\":84060},{\"end\":84339,\"start\":84331},{\"end\":84354,\"start\":84348},{\"end\":84371,\"start\":84363},{\"end\":84383,\"start\":84380},{\"end\":84401,\"start\":84391},{\"end\":84417,\"start\":84409},{\"end\":84443,\"start\":84426},{\"end\":84466,\"start\":84459},{\"end\":84480,\"start\":84475},{\"end\":84493,\"start\":84488},{\"end\":85009,\"start\":85006},{\"end\":85024,\"start\":85019},{\"end\":85031,\"start\":85026},{\"end\":85277,\"start\":85272},{\"end\":85292,\"start\":85288},{\"end\":85304,\"start\":85299},{\"end\":85321,\"start\":85314},{\"end\":85335,\"start\":85329},{\"end\":85354,\"start\":85346},{\"end\":85374,\"start\":85363},{\"end\":85388,\"start\":85383},{\"end\":85403,\"start\":85397},{\"end\":85418,\"start\":85412},{\"end\":85436,\"start\":85429},{\"end\":85456,\"start\":85444},{\"end\":85474,\"start\":85467},{\"end\":85488,\"start\":85480},{\"end\":85501,\"start\":85496},{\"end\":85516,\"start\":85510},{\"end\":85534,\"start\":85527},{\"end\":85546,\"start\":85544},{\"end\":85562,\"start\":85556},{\"end\":85581,\"start\":85576},{\"end\":85592,\"start\":85588},{\"end\":85605,\"start\":85599},{\"end\":85621,\"start\":85615},{\"end\":85633,\"start\":85629},{\"end\":86341,\"start\":86335},{\"end\":86358,\"start\":86351},{\"end\":86376,\"start\":86370},{\"end\":86391,\"start\":86384},{\"end\":86734,\"start\":86727},{\"end\":86750,\"start\":86744},{\"end\":86764,\"start\":86757},{\"end\":86783,\"start\":86774},{\"end\":86803,\"start\":86791},{\"end\":86818,\"start\":86815},{\"end\":86832,\"start\":86825},{\"end\":86845,\"start\":86840},{\"end\":86856,\"start\":86852},{\"end\":87379,\"start\":87375},{\"end\":87391,\"start\":87386},{\"end\":87405,\"start\":87399},{\"end\":87421,\"start\":87415},{\"end\":87635,\"start\":87626},{\"end\":87650,\"start\":87644},{\"end\":87664,\"start\":87658},{\"end\":87679,\"start\":87674},{\"end\":87694,\"start\":87688},{\"end\":87708,\"start\":87701},{\"end\":87721,\"start\":87715},{\"end\":87732,\"start\":87723},{\"end\":87747,\"start\":87742},{\"end\":87765,\"start\":87759},{\"end\":87782,\"start\":87774},{\"end\":87796,\"start\":87791},{\"end\":87807,\"start\":87804},{\"end\":87829,\"start\":87816},{\"end\":87846,\"start\":87840},{\"end\":87858,\"start\":87855},{\"end\":87869,\"start\":87863},{\"end\":87879,\"start\":87876},{\"end\":87899,\"start\":87892},{\"end\":87918,\"start\":87907},{\"end\":87928,\"start\":87924},{\"end\":87936,\"start\":87934},{\"end\":87955,\"start\":87945},{\"end\":87967,\"start\":87963},{\"end\":87983,\"start\":87975},{\"end\":87999,\"start\":87993},{\"end\":88010,\"start\":88005},{\"end\":88029,\"start\":88022},{\"end\":88039,\"start\":88036},{\"end\":88052,\"start\":88048},{\"end\":88069,\"start\":88061},{\"end\":88086,\"start\":88078},{\"end\":88098,\"start\":88095},{\"end\":88118,\"start\":88107},{\"end\":88133,\"start\":88127},{\"end\":88146,\"start\":88141},{\"end\":88161,\"start\":88153},{\"end\":88174,\"start\":88169},{\"end\":88187,\"start\":88183},{\"end\":88203,\"start\":88195},{\"end\":88219,\"start\":88215},{\"end\":88224,\"start\":88221},{\"end\":89835,\"start\":89830},{\"end\":89849,\"start\":89842},{\"end\":90075,\"start\":90070},{\"end\":90089,\"start\":90083},{\"end\":90103,\"start\":90096},{\"end\":90116,\"start\":90112},{\"end\":90134,\"start\":90125},{\"end\":90153,\"start\":90144},{\"end\":90169,\"start\":90162},{\"end\":90510,\"start\":90504},{\"end\":90526,\"start\":90521},{\"end\":90538,\"start\":90535},{\"end\":90558,\"start\":90549},{\"end\":90821,\"start\":90814},{\"end\":90836,\"start\":90832},{\"end\":90863,\"start\":90852},{\"end\":90879,\"start\":90872},{\"end\":90897,\"start\":90887},{\"end\":90914,\"start\":90909},{\"end\":91408,\"start\":91403},{\"end\":91424,\"start\":91418},{\"end\":91438,\"start\":91431},{\"end\":91450,\"start\":91447},{\"end\":91464,\"start\":91460},{\"end\":91478,\"start\":91472},{\"end\":91899,\"start\":91894},{\"end\":91912,\"start\":91909},{\"end\":91927,\"start\":91918},{\"end\":91942,\"start\":91937},{\"end\":91959,\"start\":91952},{\"end\":91976,\"start\":91966},{\"end\":91995,\"start\":91987},{\"end\":92009,\"start\":92002},{\"end\":92876,\"start\":92869},{\"end\":92896,\"start\":92885},{\"end\":92911,\"start\":92905},{\"end\":92931,\"start\":92924},{\"end\":92946,\"start\":92942},{\"end\":92967,\"start\":92959},{\"end\":92982,\"start\":92975},{\"end\":92989,\"start\":92984},{\"end\":93613,\"start\":93607},{\"end\":93629,\"start\":93622},{\"end\":93643,\"start\":93637},{\"end\":93655,\"start\":93651},{\"end\":94177,\"start\":94173},{\"end\":94661,\"start\":94658},{\"end\":94673,\"start\":94668},{\"end\":94685,\"start\":94681},{\"end\":95668,\"start\":95663},{\"end\":95687,\"start\":95678},{\"end\":95973,\"start\":95968},{\"end\":95990,\"start\":95985},{\"end\":96005,\"start\":95999},{\"end\":96250,\"start\":96245},{\"end\":96265,\"start\":96259},{\"end\":96282,\"start\":96275},{\"end\":96572,\"start\":96569},{\"end\":96584,\"start\":96581},{\"end\":96595,\"start\":96591},{\"end\":96613,\"start\":96606},{\"end\":96629,\"start\":96624},{\"end\":96970,\"start\":96968},{\"end\":96981,\"start\":96978},{\"end\":96991,\"start\":96989},{\"end\":97004,\"start\":97001},{\"end\":97019,\"start\":97011},{\"end\":97477,\"start\":97468},{\"end\":97491,\"start\":97486},{\"end\":97506,\"start\":97500},{\"end\":97516,\"start\":97513},{\"end\":97532,\"start\":97525},{\"end\":97543,\"start\":97539},{\"end\":97561,\"start\":97551},{\"end\":98045,\"start\":98038},{\"end\":98065,\"start\":98060},{\"end\":98081,\"start\":98074},{\"end\":98098,\"start\":98089},{\"end\":98114,\"start\":98108},{\"end\":98128,\"start\":98121},{\"end\":98144,\"start\":98138},{\"end\":98159,\"start\":98153},{\"end\":98175,\"start\":98169},{\"end\":98752,\"start\":98744},{\"end\":98772,\"start\":98764},{\"end\":98787,\"start\":98781},{\"end\":98806,\"start\":98795},{\"end\":98818,\"start\":98815},{\"end\":98836,\"start\":98826},{\"end\":98850,\"start\":98844},{\"end\":98867,\"start\":98862},{\"end\":98887,\"start\":98878},{\"end\":98900,\"start\":98895},{\"end\":98911,\"start\":98906},{\"end\":98926,\"start\":98918},{\"end\":98940,\"start\":98934},{\"end\":98957,\"start\":98949},{\"end\":98983,\"start\":98966},{\"end\":98998,\"start\":98993},{\"end\":99009,\"start\":99006},{\"end\":99025,\"start\":99017},{\"end\":99035,\"start\":99027},{\"end\":99676,\"start\":99666},{\"end\":99689,\"start\":99685},{\"end\":99704,\"start\":99699},{\"end\":99718,\"start\":99712},{\"end\":100013,\"start\":100008},{\"end\":100026,\"start\":100024},{\"end\":100040,\"start\":100037},{\"end\":100049,\"start\":100045},{\"end\":100061,\"start\":100056},{\"end\":100073,\"start\":100069},{\"end\":100627,\"start\":100620},{\"end\":100642,\"start\":100637},{\"end\":100937,\"start\":100930},{\"end\":100952,\"start\":100947},{\"end\":101318,\"start\":101311},{\"end\":101333,\"start\":101326},{\"end\":101349,\"start\":101341},{\"end\":101364,\"start\":101361},{\"end\":101382,\"start\":101376},{\"end\":101397,\"start\":101392},{\"end\":101721,\"start\":101714},{\"end\":101737,\"start\":101732},{\"end\":101753,\"start\":101745},{\"end\":101771,\"start\":101765},{\"end\":101789,\"start\":101779},{\"end\":101804,\"start\":101798},{\"end\":101819,\"start\":101814},{\"end\":102121,\"start\":102116},{\"end\":102133,\"start\":102131},{\"end\":102144,\"start\":102139},{\"end\":102159,\"start\":102153},{\"end\":102597,\"start\":102591},{\"end\":102874,\"start\":102869},{\"end\":102887,\"start\":102883},{\"end\":102902,\"start\":102898},{\"end\":102920,\"start\":102909},{\"end\":103175,\"start\":103170},{\"end\":103191,\"start\":103186},{\"end\":103208,\"start\":103202},{\"end\":103572,\"start\":103566},{\"end\":103588,\"start\":103578},{\"end\":103602,\"start\":103594},{\"end\":103615,\"start\":103610},{\"end\":103631,\"start\":103626},{\"end\":103644,\"start\":103639},{\"end\":103656,\"start\":103652},{\"end\":103670,\"start\":103663},{\"end\":103682,\"start\":103680},{\"end\":103696,\"start\":103690},{\"end\":104007,\"start\":103998},{\"end\":104020,\"start\":104016},{\"end\":104031,\"start\":104028},{\"end\":104042,\"start\":104040},{\"end\":104057,\"start\":104051},{\"end\":104069,\"start\":104065},{\"end\":104082,\"start\":104079},{\"end\":104409,\"start\":104399},{\"end\":104420,\"start\":104416},{\"end\":104434,\"start\":104426},{\"end\":104452,\"start\":104441},{\"end\":104464,\"start\":104459},{\"end\":104862,\"start\":104854},{\"end\":104873,\"start\":104870},{\"end\":104886,\"start\":104882},{\"end\":104904,\"start\":104895},{\"end\":104920,\"start\":104913},{\"end\":104933,\"start\":104928},{\"end\":104954,\"start\":104944},{\"end\":105612,\"start\":105605},{\"end\":105630,\"start\":105624},{\"end\":105647,\"start\":105640},{\"end\":105894,\"start\":105883},{\"end\":105915,\"start\":105907},{\"end\":105932,\"start\":105924},{\"end\":105949,\"start\":105942},{\"end\":105963,\"start\":105957},{\"end\":105978,\"start\":105971},{\"end\":105996,\"start\":105989},{\"end\":106014,\"start\":106004},{\"end\":106028,\"start\":106022},{\"end\":106040,\"start\":106037},{\"end\":106060,\"start\":106051},{\"end\":106073,\"start\":106068},{\"end\":106089,\"start\":106083},{\"end\":106105,\"start\":106100},{\"end\":106119,\"start\":106116},{\"end\":106136,\"start\":106127},{\"end\":106145,\"start\":106143},{\"end\":106158,\"start\":106152},{\"end\":106824,\"start\":106821},{\"end\":106835,\"start\":106832},{\"end\":106848,\"start\":106845},{\"end\":106861,\"start\":106857},{\"end\":106874,\"start\":106870},{\"end\":107617,\"start\":107608},{\"end\":107636,\"start\":107625},{\"end\":107656,\"start\":107647},{\"end\":107674,\"start\":107666},{\"end\":107964,\"start\":107959},{\"end\":107980,\"start\":107976},{\"end\":107986,\"start\":107982},{\"end\":108796,\"start\":108793},{\"end\":108809,\"start\":108804},{\"end\":108822,\"start\":108819},{\"end\":108840,\"start\":108831},{\"end\":108863,\"start\":108856},{\"end\":108878,\"start\":108875},{\"end\":109151,\"start\":109148},{\"end\":109164,\"start\":109160},{\"end\":109177,\"start\":109173},{\"end\":109189,\"start\":109185},{\"end\":110131,\"start\":110128},{\"end\":110147,\"start\":110142},{\"end\":110167,\"start\":110158},{\"end\":110427,\"start\":110421},{\"end\":110441,\"start\":110434},{\"end\":110456,\"start\":110448},{\"end\":110698,\"start\":110694},{\"end\":110711,\"start\":110708},{\"end\":110724,\"start\":110720},{\"end\":110742,\"start\":110731},{\"end\":111282,\"start\":111277},{\"end\":111295,\"start\":111290},{\"end\":111315,\"start\":111309},{\"end\":111330,\"start\":111323},{\"end\":111350,\"start\":111341},{\"end\":111363,\"start\":111358},{\"end\":111381,\"start\":111374},{\"end\":111393,\"start\":111388},{\"end\":111406,\"start\":111403},{\"end\":111423,\"start\":111412},{\"end\":111808,\"start\":111803},{\"end\":111818,\"start\":111816},{\"end\":111825,\"start\":111820},{\"end\":112728,\"start\":112722},{\"end\":112739,\"start\":112733},{\"end\":112751,\"start\":112747},{\"end\":112764,\"start\":112758},{\"end\":113034,\"start\":113025},{\"end\":113047,\"start\":113042},{\"end\":113070,\"start\":113054},{\"end\":113085,\"start\":113078},{\"end\":113101,\"start\":113094},{\"end\":113118,\"start\":113113},{\"end\":113126,\"start\":113120},{\"end\":113476,\"start\":113468},{\"end\":113489,\"start\":113484},{\"end\":113502,\"start\":113498},{\"end\":113520,\"start\":113511},{\"end\":114179,\"start\":114176},{\"end\":114195,\"start\":114188},{\"end\":114216,\"start\":114206},{\"end\":114234,\"start\":114223},{\"end\":114563,\"start\":114557},{\"end\":114577,\"start\":114571},{\"end\":114587,\"start\":114581},{\"end\":114596,\"start\":114594},{\"end\":114609,\"start\":114603},{\"end\":114624,\"start\":114621},{\"end\":114643,\"start\":114638},{\"end\":114658,\"start\":114654},{\"end\":114675,\"start\":114667},{\"end\":114693,\"start\":114685},{\"end\":114703,\"start\":114698},{\"end\":114715,\"start\":114710},{\"end\":114730,\"start\":114722},{\"end\":114748,\"start\":114741},{\"end\":115309,\"start\":115300},{\"end\":115323,\"start\":115316},{\"end\":115342,\"start\":115337},{\"end\":115357,\"start\":115350},{\"end\":115380,\"start\":115373},{\"end\":115682,\"start\":115675},{\"end\":115697,\"start\":115692},{\"end\":115716,\"start\":115710},{\"end\":115733,\"start\":115722},{\"end\":115745,\"start\":115743},{\"end\":115765,\"start\":115759},{\"end\":115783,\"start\":115777},{\"end\":116091,\"start\":116084},{\"end\":116109,\"start\":116100},{\"end\":116128,\"start\":116122},{\"end\":116143,\"start\":116138},{\"end\":116160,\"start\":116153},{\"end\":116176,\"start\":116168},{\"end\":116193,\"start\":116183},{\"end\":116207,\"start\":116201},{\"end\":116220,\"start\":116214},{\"end\":116244,\"start\":116238},{\"end\":116259,\"start\":116253},{\"end\":116274,\"start\":116269},{\"end\":116292,\"start\":116286},{\"end\":116701,\"start\":116695},{\"end\":116716,\"start\":116709},{\"end\":116736,\"start\":116727},{\"end\":116752,\"start\":116745},{\"end\":116770,\"start\":116761},{\"end\":116787,\"start\":116780},{\"end\":116802,\"start\":116797},{\"end\":116815,\"start\":116811},{\"end\":116830,\"start\":116825},{\"end\":116843,\"start\":116836},{\"end\":116861,\"start\":116855},{\"end\":117241,\"start\":117234},{\"end\":117253,\"start\":117251},{\"end\":117266,\"start\":117261},{\"end\":117278,\"start\":117274},{\"end\":117292,\"start\":117286},{\"end\":117308,\"start\":117299},{\"end\":117594,\"start\":117591},{\"end\":117614,\"start\":117606},{\"end\":117626,\"start\":117623},{\"end\":117642,\"start\":117634},{\"end\":117659,\"start\":117651},{\"end\":117673,\"start\":117669},{\"end\":117689,\"start\":117680},{\"end\":117706,\"start\":117697},{\"end\":117718,\"start\":117714},{\"end\":117734,\"start\":117729},{\"end\":117752,\"start\":117742},{\"end\":117766,\"start\":117758},{\"end\":117780,\"start\":117774},{\"end\":117796,\"start\":117788},{\"end\":117812,\"start\":117806},{\"end\":117838,\"start\":117821},{\"end\":117859,\"start\":117850},{\"end\":117874,\"start\":117870},{\"end\":117888,\"start\":117883},{\"end\":117903,\"start\":117897},{\"end\":117919,\"start\":117914},{\"end\":117938,\"start\":117929},{\"end\":117953,\"start\":117948},{\"end\":117970,\"start\":117964},{\"end\":117983,\"start\":117977},{\"end\":117998,\"start\":117991},{\"end\":118016,\"start\":118008},{\"end\":118030,\"start\":118022},{\"end\":118038,\"start\":118036},{\"end\":118051,\"start\":118046},{\"end\":118071,\"start\":118062},{\"end\":118090,\"start\":118079},{\"end\":118104,\"start\":118098},{\"end\":118121,\"start\":118111},{\"end\":118137,\"start\":118129},{\"end\":118155,\"start\":118147},{\"end\":118170,\"start\":118165},{\"end\":118184,\"start\":118177},{\"end\":118203,\"start\":118201},{\"end\":118221,\"start\":118214},{\"end\":118238,\"start\":118228},{\"end\":118257,\"start\":118246},{\"end\":118273,\"start\":118267},{\"end\":118293,\"start\":118284},{\"end\":118308,\"start\":118302},{\"end\":118331,\"start\":118324},{\"end\":118351,\"start\":118339},{\"end\":120400,\"start\":120394},{\"end\":120414,\"start\":120407},{\"end\":120428,\"start\":120421},{\"end\":120443,\"start\":120440},{\"end\":120458,\"start\":120452},{\"end\":120474,\"start\":120468},{\"end\":120486,\"start\":120482},{\"end\":120494,\"start\":120492},{\"end\":120507,\"start\":120504},{\"end\":120871,\"start\":120862},{\"end\":120883,\"start\":120878},{\"end\":120903,\"start\":120896},{\"end\":120916,\"start\":120911},{\"end\":121147,\"start\":121144},{\"end\":121161,\"start\":121154},{\"end\":121176,\"start\":121172},{\"end\":121189,\"start\":121183},{\"end\":121200,\"start\":121191},{\"end\":122048,\"start\":122038},{\"end\":122654,\"start\":122635},{\"end\":122668,\"start\":122662},{\"end\":122687,\"start\":122682},{\"end\":122710,\"start\":122694},{\"end\":122719,\"start\":122712},{\"end\":122991,\"start\":122979},{\"end\":123003,\"start\":122998},{\"end\":123021,\"start\":123013},{\"end\":123033,\"start\":123029},{\"end\":123048,\"start\":123040},{\"end\":123379,\"start\":123373},{\"end\":123390,\"start\":123383},{\"end\":123660,\"start\":123654},{\"end\":123677,\"start\":123670},{\"end\":124448,\"start\":124442},{\"end\":124465,\"start\":124458},{\"end\":125141,\"start\":125137},{\"end\":125154,\"start\":125152},{\"end\":125168,\"start\":125165},{\"end\":125177,\"start\":125173},{\"end\":125194,\"start\":125188},{\"end\":125765,\"start\":125761},{\"end\":125782,\"start\":125775},{\"end\":125798,\"start\":125793},{\"end\":125818,\"start\":125811},{\"end\":125832,\"start\":125827},{\"end\":126599,\"start\":126592},{\"end\":126613,\"start\":126609},{\"end\":126624,\"start\":126620},{\"end\":126637,\"start\":126632},{\"end\":126651,\"start\":126645},{\"end\":126949,\"start\":126942},{\"end\":126966,\"start\":126959},{\"end\":126983,\"start\":126976},{\"end\":126999,\"start\":126993},{\"end\":127015,\"start\":127010},{\"end\":127029,\"start\":127023},{\"end\":127406,\"start\":127401},{\"end\":127423,\"start\":127416},{\"end\":127439,\"start\":127433},{\"end\":127458,\"start\":127449},{\"end\":127478,\"start\":127467},{\"end\":127492,\"start\":127486},{\"end\":127502,\"start\":127499},{\"end\":127522,\"start\":127512},{\"end\":127538,\"start\":127531},{\"end\":127557,\"start\":127546},{\"end\":127570,\"start\":127565},{\"end\":127583,\"start\":127578},{\"end\":127607,\"start\":127598},{\"end\":127623,\"start\":127615},{\"end\":127633,\"start\":127629},{\"end\":127651,\"start\":127644},{\"end\":127663,\"start\":127661},{\"end\":127680,\"start\":127673},{\"end\":127696,\"start\":127690},{\"end\":127713,\"start\":127704},{\"end\":128281,\"start\":128278},{\"end\":128303,\"start\":128289},{\"end\":128320,\"start\":128314},{\"end\":128338,\"start\":128328},{\"end\":128346,\"start\":128340},{\"end\":128532,\"start\":128523},{\"end\":128551,\"start\":128544},{\"end\":128563,\"start\":128559},{\"end\":128579,\"start\":128572},{\"end\":128600,\"start\":128588},{\"end\":128610,\"start\":128602},{\"end\":128624,\"start\":128619},{\"end\":128636,\"start\":128633},{\"end\":128648,\"start\":128645},{\"end\":128658,\"start\":128653},{\"end\":128670,\"start\":128668},{\"end\":128682,\"start\":128680},{\"end\":128695,\"start\":128692},{\"end\":128707,\"start\":128702},{\"end\":128725,\"start\":128717},{\"end\":128743,\"start\":128735},{\"end\":128756,\"start\":128751},{\"end\":128771,\"start\":128765},{\"end\":128787,\"start\":128779},{\"end\":128798,\"start\":128795},{\"end\":128814,\"start\":128810},{\"end\":128826,\"start\":128824},{\"end\":128837,\"start\":128833},{\"end\":128854,\"start\":128847},{\"end\":128867,\"start\":128862},{\"end\":128885,\"start\":128881},{\"end\":128896,\"start\":128891},{\"end\":128920,\"start\":128912},{\"end\":128932,\"start\":128927},{\"end\":128952,\"start\":128945},{\"end\":128969,\"start\":128954},{\"end\":130519,\"start\":130513},{\"end\":130536,\"start\":130529},{\"end\":130565,\"start\":130547},{\"end\":130579,\"start\":130573},{\"end\":130864,\"start\":130859},{\"end\":130878,\"start\":130873},{\"end\":131125,\"start\":131118},{\"end\":131143,\"start\":131135},{\"end\":131162,\"start\":131153},{\"end\":131181,\"start\":131176},{\"end\":131196,\"start\":131188},{\"end\":131773,\"start\":131768},{\"end\":131783,\"start\":131775},{\"end\":131977,\"start\":131973},{\"end\":131989,\"start\":131987},{\"end\":132001,\"start\":131999},{\"end\":132019,\"start\":132010},{\"end\":132031,\"start\":132026},{\"end\":132874,\"start\":132871},{\"end\":132882,\"start\":132879},{\"end\":132899,\"start\":132890},{\"end\":132913,\"start\":132907},{\"end\":132926,\"start\":132922},{\"end\":132946,\"start\":132938},{\"end\":132961,\"start\":132953},{\"end\":132976,\"start\":132971},{\"end\":132988,\"start\":132984},{\"end\":133004,\"start\":132997},{\"end\":133014,\"start\":133011},{\"end\":133035,\"start\":133026},{\"end\":133050,\"start\":133043},{\"end\":133063,\"start\":133058},{\"end\":133074,\"start\":133070},{\"end\":133089,\"start\":133084},{\"end\":133504,\"start\":133498},{\"end\":133524,\"start\":133517},{\"end\":133540,\"start\":133533},{\"end\":133559,\"start\":133550},{\"end\":133577,\"start\":133571},{\"end\":133592,\"start\":133586},{\"end\":133607,\"start\":133602},{\"end\":134107,\"start\":134102},{\"end\":134122,\"start\":134117},{\"end\":134129,\"start\":134127},{\"end\":134145,\"start\":134141},{\"end\":134157,\"start\":134154},{\"end\":134171,\"start\":134164},{\"end\":134185,\"start\":134180},{\"end\":134202,\"start\":134194},{\"end\":134557,\"start\":134553},{\"end\":134566,\"start\":134564},{\"end\":134582,\"start\":134577},{\"end\":134597,\"start\":134591},{\"end\":134614,\"start\":134609},{\"end\":134636,\"start\":134623},{\"end\":134659,\"start\":134652},{\"end\":134975,\"start\":134970},{\"end\":134993,\"start\":134985},{\"end\":135004,\"start\":135001},{\"end\":135770,\"start\":135759},{\"end\":135788,\"start\":135779},{\"end\":135807,\"start\":135802},{\"end\":135813,\"start\":135809},{\"end\":136480,\"start\":136472},{\"end\":136522,\"start\":136491},{\"end\":136528,\"start\":136524}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":84299,\"start\":83890},{\"attributes\":{\"id\":\"b1\"},\"end\":84932,\"start\":84301},{\"attributes\":{\"id\":\"b2\"},\"end\":85191,\"start\":84934},{\"attributes\":{\"id\":\"b3\"},\"end\":86293,\"start\":85193},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":222125277},\"end\":86663,\"start\":86295},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":229156229},\"end\":87316,\"start\":86665},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3618568},\"end\":87567,\"start\":87318},{\"attributes\":{\"id\":\"b7\"},\"end\":89756,\"start\":87569},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":223637},\"end\":89982,\"start\":89758},{\"attributes\":{\"doi\":\"abs/1803.05457\",\"id\":\"b9\",\"matched_paper_id\":3922816},\"end\":90414,\"start\":89984},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52967399},\"end\":90749,\"start\":90416},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00459\",\"id\":\"b11\",\"matched_paper_id\":235669861},\"end\":91333,\"start\":90751},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":53218829},\"end\":91800,\"start\":91335},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.98\",\"id\":\"b13\",\"matched_paper_id\":237568724},\"end\":92789,\"start\":91802},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4612975},\"end\":93536,\"start\":92791},{\"attributes\":{\"doi\":\"10.18653/v1/2021.findings-emnlp.73\",\"id\":\"b15\",\"matched_paper_id\":237439232},\"end\":94084,\"start\":93538},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9248748},\"end\":94588,\"start\":94086},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.295\",\"id\":\"b17\",\"matched_paper_id\":229923710},\"end\":95537,\"start\":94590},{\"attributes\":{\"id\":\"b18\"},\"end\":95885,\"start\":95539},{\"attributes\":{\"id\":\"b19\"},\"end\":96177,\"start\":95887},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8693672},\"end\":96560,\"start\":96179},{\"attributes\":{\"doi\":\"arXiv:2002.08909\",\"id\":\"b21\"},\"end\":96891,\"start\":96562},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207930212},\"end\":97410,\"start\":96893},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221516475},\"end\":97978,\"start\":97412},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6216506},\"end\":98687,\"start\":97980},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":247778764},\"end\":99582,\"start\":98689},{\"attributes\":{\"doi\":\"abs/2207.03030\",\"id\":\"b26\",\"matched_paper_id\":250334200},\"end\":99918,\"start\":99584},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":8384258},\"end\":100522,\"start\":99920},{\"attributes\":{\"doi\":\"arXiv:2007.01282\",\"id\":\"b28\"},\"end\":100850,\"start\":100524},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":227746078},\"end\":101237,\"start\":100852},{\"attributes\":{\"doi\":\"arXiv:2012.15156\",\"id\":\"b30\"},\"end\":101636,\"start\":101239},{\"attributes\":{\"id\":\"b31\"},\"end\":102061,\"start\":101638},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00324\",\"id\":\"b32\",\"matched_paper_id\":208513249},\"end\":102500,\"start\":102063},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2996187},\"end\":102770,\"start\":102502},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":26501419},\"end\":103112,\"start\":102772},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2010.57.18\",\"id\":\"b35\",\"matched_paper_id\":5850884},\"end\":103517,\"start\":103114},{\"attributes\":{\"id\":\"b36\"},\"end\":103927,\"start\":103519},{\"attributes\":{\"doi\":\"arXiv:2004.04906\",\"id\":\"b37\"},\"end\":104318,\"start\":103929},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":207870430},\"end\":104782,\"start\":104320},{\"attributes\":{\"doi\":\"10.18653/v1/2020.findings-emnlp.171\",\"id\":\"b39\",\"matched_paper_id\":218487109},\"end\":105533,\"start\":104784},{\"attributes\":{\"id\":\"b40\"},\"end\":105813,\"start\":105535},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00276\",\"id\":\"b41\",\"matched_paper_id\":86611921},\"end\":106745,\"start\":105815},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1082\",\"id\":\"b42\",\"matched_paper_id\":6826032},\"end\":107499,\"start\":106747},{\"attributes\":{\"doi\":\"abs/2203.05115\",\"id\":\"b43\",\"matched_paper_id\":247362809},\"end\":107913,\"start\":107501},{\"attributes\":{\"doi\":\"10.18653/v1/2021.naacl-main.208\",\"id\":\"b44\",\"matched_paper_id\":232233408},\"end\":108724,\"start\":107915},{\"attributes\":{\"id\":\"b45\"},\"end\":109086,\"start\":108726},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.518\",\"id\":\"b46\",\"matched_paper_id\":229363636},\"end\":110048,\"start\":109088},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":173990818},\"end\":110327,\"start\":110050},{\"attributes\":{\"id\":\"b48\"},\"end\":110630,\"start\":110329},{\"attributes\":{\"doi\":\"10.18653/v1/K17-1034\",\"id\":\"b49\",\"matched_paper_id\":793385},\"end\":111202,\"start\":110632},{\"attributes\":{\"doi\":\"arXiv:2005.11401\",\"id\":\"b50\"},\"end\":111735,\"start\":111204},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.353\",\"id\":\"b51\",\"matched_paper_id\":230433941},\"end\":112668,\"start\":111737},{\"attributes\":{\"id\":\"b52\"},\"end\":112934,\"start\":112670},{\"attributes\":{\"doi\":\"abs/2106.13353\",\"id\":\"b53\",\"matched_paper_id\":235652287},\"end\":113371,\"start\":112936},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1260\",\"id\":\"b54\",\"matched_paper_id\":52183757},\"end\":114168,\"start\":113373},{\"attributes\":{\"doi\":\"arXiv:2004.10645\",\"id\":\"b55\"},\"end\":114480,\"start\":114170},{\"attributes\":{\"doi\":\"abs/2112.09332\",\"id\":\"b56\",\"matched_paper_id\":245329531},\"end\":115204,\"start\":114482},{\"attributes\":{\"id\":\"b57\"},\"end\":115609,\"start\":115206},{\"attributes\":{\"doi\":\"arXiv:2005.04611\",\"id\":\"b58\"},\"end\":116033,\"start\":115611},{\"attributes\":{\"id\":\"b59\"},\"end\":116603,\"start\":116035},{\"attributes\":{\"id\":\"b60\"},\"end\":117174,\"start\":116605},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":160025533},\"end\":117506,\"start\":117176},{\"attributes\":{\"id\":\"b62\"},\"end\":120303,\"start\":117508},{\"attributes\":{\"doi\":\"arXiv:1910.10683\",\"id\":\"b63\"},\"end\":120792,\"start\":120305},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":11816014},\"end\":121087,\"start\":120794},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":245144844},\"end\":121949,\"start\":121089},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":2100831},\"end\":122581,\"start\":121951},{\"attributes\":{\"id\":\"b67\"},\"end\":122873,\"start\":122583},{\"attributes\":{\"id\":\"b68\"},\"end\":123283,\"start\":122875},{\"attributes\":{\"doi\":\"abs/2009.07118\",\"id\":\"b69\",\"matched_paper_id\":221703107},\"end\":123555,\"start\":123285},{\"attributes\":{\"doi\":\"10.18653/v1/2021.eacl-main.20\",\"id\":\"b70\",\"matched_paper_id\":210838924},\"end\":124374,\"start\":123557},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.32\",\"id\":\"b71\",\"matched_paper_id\":238260199},\"end\":125042,\"start\":124376},{\"attributes\":{\"doi\":\"10.1145/2567948.2577348\",\"id\":\"b72\",\"matched_paper_id\":207210855},\"end\":125661,\"start\":125044},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.346\",\"id\":\"b73\"},\"end\":126523,\"start\":125663},{\"attributes\":{\"id\":\"b74\"},\"end\":126830,\"start\":126525},{\"attributes\":{\"doi\":\"abs/2203.13224\",\"id\":\"b75\",\"matched_paper_id\":247627671},\"end\":127287,\"start\":126832},{\"attributes\":{\"id\":\"b76\"},\"end\":128192,\"start\":127289},{\"attributes\":{\"id\":\"b77\"},\"end\":128513,\"start\":128194},{\"attributes\":{\"id\":\"b78\"},\"end\":130438,\"start\":128515},{\"attributes\":{\"doi\":\"arXiv:1803.05355\",\"id\":\"b79\"},\"end\":130801,\"start\":130440},{\"attributes\":{\"doi\":\"0792380479. 8\",\"id\":\"b80\"},\"end\":131069,\"start\":130803},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":8909022},\"end\":131720,\"start\":131071},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":16944215},\"end\":131875,\"start\":131722},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1599\",\"id\":\"b83\",\"matched_paper_id\":201307832},\"end\":132818,\"start\":131877},{\"attributes\":{\"id\":\"b84\"},\"end\":133413,\"start\":132820},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":207870323},\"end\":134011,\"start\":133415},{\"attributes\":{\"doi\":\"arXiv:2007.00808\",\"id\":\"b86\"},\"end\":134469,\"start\":134013},{\"attributes\":{\"id\":\"b87\"},\"end\":134902,\"start\":134471},{\"attributes\":{\"doi\":\"10.1145/3437963.3441667\",\"id\":\"b88\",\"matched_paper_id\":222310837},\"end\":135682,\"start\":134904},{\"attributes\":{\"doi\":\"978-1-932432-92-3\",\"id\":\"b89\",\"matched_paper_id\":1965270},\"end\":136424,\"start\":135684},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00371\",\"id\":\"b90\",\"matched_paper_id\":231717977},\"end\":136870,\"start\":136426}]", "bib_title": "[{\"end\":84319,\"start\":84301},{\"end\":86326,\"start\":86295},{\"end\":86716,\"start\":86665},{\"end\":87367,\"start\":87318},{\"end\":89816,\"start\":89758},{\"end\":90062,\"start\":89984},{\"end\":90496,\"start\":90416},{\"end\":90805,\"start\":90751},{\"end\":91395,\"start\":91335},{\"end\":91886,\"start\":91802},{\"end\":92862,\"start\":92791},{\"end\":93598,\"start\":93538},{\"end\":94163,\"start\":94086},{\"end\":94649,\"start\":94590},{\"end\":96235,\"start\":96179},{\"end\":96958,\"start\":96893},{\"end\":97462,\"start\":97412},{\"end\":98027,\"start\":97980},{\"end\":98735,\"start\":98689},{\"end\":99654,\"start\":99584},{\"end\":99999,\"start\":99920},{\"end\":100920,\"start\":100852},{\"end\":102105,\"start\":102063},{\"end\":102583,\"start\":102502},{\"end\":102860,\"start\":102772},{\"end\":103162,\"start\":103114},{\"end\":104389,\"start\":104320},{\"end\":104845,\"start\":104784},{\"end\":105877,\"start\":105815},{\"end\":106812,\"start\":106747},{\"end\":107597,\"start\":107501},{\"end\":107954,\"start\":107915},{\"end\":109138,\"start\":109088},{\"end\":110119,\"start\":110050},{\"end\":110687,\"start\":110632},{\"end\":111796,\"start\":111737},{\"end\":113021,\"start\":112936},{\"end\":113460,\"start\":113373},{\"end\":114545,\"start\":114482},{\"end\":117227,\"start\":117176},{\"end\":120853,\"start\":120794},{\"end\":121138,\"start\":121089},{\"end\":122028,\"start\":121951},{\"end\":123366,\"start\":123285},{\"end\":123647,\"start\":123557},{\"end\":124435,\"start\":124376},{\"end\":125128,\"start\":125044},{\"end\":125752,\"start\":125663},{\"end\":126935,\"start\":126832},{\"end\":131110,\"start\":131071},{\"end\":131764,\"start\":131722},{\"end\":131964,\"start\":131877},{\"end\":133486,\"start\":133415},{\"end\":134961,\"start\":134904},{\"end\":135748,\"start\":135684},{\"end\":136465,\"start\":136426}]", "bib_author": "[{\"end\":83988,\"start\":83968},{\"end\":84008,\"start\":83988},{\"end\":84023,\"start\":84008},{\"end\":84036,\"start\":84023},{\"end\":84054,\"start\":84036},{\"end\":84069,\"start\":84054},{\"end\":84341,\"start\":84321},{\"end\":84356,\"start\":84341},{\"end\":84373,\"start\":84356},{\"end\":84385,\"start\":84373},{\"end\":84403,\"start\":84385},{\"end\":84419,\"start\":84403},{\"end\":84445,\"start\":84419},{\"end\":84468,\"start\":84445},{\"end\":84482,\"start\":84468},{\"end\":84495,\"start\":84482},{\"end\":85011,\"start\":85000},{\"end\":85026,\"start\":85011},{\"end\":85033,\"start\":85026},{\"end\":85279,\"start\":85266},{\"end\":85294,\"start\":85279},{\"end\":85306,\"start\":85294},{\"end\":85323,\"start\":85306},{\"end\":85337,\"start\":85323},{\"end\":85356,\"start\":85337},{\"end\":85376,\"start\":85356},{\"end\":85390,\"start\":85376},{\"end\":85405,\"start\":85390},{\"end\":85420,\"start\":85405},{\"end\":85438,\"start\":85420},{\"end\":85458,\"start\":85438},{\"end\":85476,\"start\":85458},{\"end\":85490,\"start\":85476},{\"end\":85503,\"start\":85490},{\"end\":85518,\"start\":85503},{\"end\":85536,\"start\":85518},{\"end\":85548,\"start\":85536},{\"end\":85564,\"start\":85548},{\"end\":85583,\"start\":85564},{\"end\":85594,\"start\":85583},{\"end\":85607,\"start\":85594},{\"end\":85623,\"start\":85607},{\"end\":85635,\"start\":85623},{\"end\":86343,\"start\":86328},{\"end\":86360,\"start\":86343},{\"end\":86378,\"start\":86360},{\"end\":86393,\"start\":86378},{\"end\":86736,\"start\":86718},{\"end\":86752,\"start\":86736},{\"end\":86766,\"start\":86752},{\"end\":86785,\"start\":86766},{\"end\":86805,\"start\":86785},{\"end\":86820,\"start\":86805},{\"end\":86834,\"start\":86820},{\"end\":86847,\"start\":86834},{\"end\":86858,\"start\":86847},{\"end\":87381,\"start\":87369},{\"end\":87393,\"start\":87381},{\"end\":87407,\"start\":87393},{\"end\":87423,\"start\":87407},{\"end\":87637,\"start\":87616},{\"end\":87652,\"start\":87637},{\"end\":87666,\"start\":87652},{\"end\":87681,\"start\":87666},{\"end\":87696,\"start\":87681},{\"end\":87710,\"start\":87696},{\"end\":87723,\"start\":87710},{\"end\":87734,\"start\":87723},{\"end\":87749,\"start\":87734},{\"end\":87767,\"start\":87749},{\"end\":87784,\"start\":87767},{\"end\":87798,\"start\":87784},{\"end\":87809,\"start\":87798},{\"end\":87831,\"start\":87809},{\"end\":87848,\"start\":87831},{\"end\":87860,\"start\":87848},{\"end\":87871,\"start\":87860},{\"end\":87881,\"start\":87871},{\"end\":87901,\"start\":87881},{\"end\":87920,\"start\":87901},{\"end\":87930,\"start\":87920},{\"end\":87938,\"start\":87930},{\"end\":87957,\"start\":87938},{\"end\":87969,\"start\":87957},{\"end\":87985,\"start\":87969},{\"end\":88001,\"start\":87985},{\"end\":88012,\"start\":88001},{\"end\":88031,\"start\":88012},{\"end\":88041,\"start\":88031},{\"end\":88054,\"start\":88041},{\"end\":88071,\"start\":88054},{\"end\":88088,\"start\":88071},{\"end\":88100,\"start\":88088},{\"end\":88120,\"start\":88100},{\"end\":88135,\"start\":88120},{\"end\":88148,\"start\":88135},{\"end\":88163,\"start\":88148},{\"end\":88176,\"start\":88163},{\"end\":88189,\"start\":88176},{\"end\":88205,\"start\":88189},{\"end\":88221,\"start\":88205},{\"end\":88226,\"start\":88221},{\"end\":89837,\"start\":89818},{\"end\":89851,\"start\":89837},{\"end\":90077,\"start\":90064},{\"end\":90091,\"start\":90077},{\"end\":90105,\"start\":90091},{\"end\":90118,\"start\":90105},{\"end\":90136,\"start\":90118},{\"end\":90155,\"start\":90136},{\"end\":90171,\"start\":90155},{\"end\":90512,\"start\":90498},{\"end\":90528,\"start\":90512},{\"end\":90540,\"start\":90528},{\"end\":90560,\"start\":90540},{\"end\":90823,\"start\":90807},{\"end\":90838,\"start\":90823},{\"end\":90865,\"start\":90838},{\"end\":90881,\"start\":90865},{\"end\":90899,\"start\":90881},{\"end\":90916,\"start\":90899},{\"end\":91410,\"start\":91397},{\"end\":91426,\"start\":91410},{\"end\":91440,\"start\":91426},{\"end\":91452,\"start\":91440},{\"end\":91466,\"start\":91452},{\"end\":91480,\"start\":91466},{\"end\":91901,\"start\":91888},{\"end\":91914,\"start\":91901},{\"end\":91929,\"start\":91914},{\"end\":91944,\"start\":91929},{\"end\":91961,\"start\":91944},{\"end\":91978,\"start\":91961},{\"end\":91997,\"start\":91978},{\"end\":92011,\"start\":91997},{\"end\":92878,\"start\":92864},{\"end\":92898,\"start\":92878},{\"end\":92913,\"start\":92898},{\"end\":92933,\"start\":92913},{\"end\":92948,\"start\":92933},{\"end\":92969,\"start\":92948},{\"end\":92984,\"start\":92969},{\"end\":92991,\"start\":92984},{\"end\":93615,\"start\":93600},{\"end\":93631,\"start\":93615},{\"end\":93645,\"start\":93631},{\"end\":93657,\"start\":93645},{\"end\":94179,\"start\":94165},{\"end\":94663,\"start\":94651},{\"end\":94675,\"start\":94663},{\"end\":94687,\"start\":94675},{\"end\":95670,\"start\":95655},{\"end\":95689,\"start\":95670},{\"end\":95975,\"start\":95960},{\"end\":95992,\"start\":95975},{\"end\":96007,\"start\":95992},{\"end\":96252,\"start\":96237},{\"end\":96267,\"start\":96252},{\"end\":96284,\"start\":96267},{\"end\":96574,\"start\":96562},{\"end\":96586,\"start\":96574},{\"end\":96597,\"start\":96586},{\"end\":96615,\"start\":96597},{\"end\":96631,\"start\":96615},{\"end\":96972,\"start\":96960},{\"end\":96983,\"start\":96972},{\"end\":96993,\"start\":96983},{\"end\":97006,\"start\":96993},{\"end\":97021,\"start\":97006},{\"end\":97479,\"start\":97464},{\"end\":97493,\"start\":97479},{\"end\":97508,\"start\":97493},{\"end\":97518,\"start\":97508},{\"end\":97534,\"start\":97518},{\"end\":97545,\"start\":97534},{\"end\":97563,\"start\":97545},{\"end\":98047,\"start\":98029},{\"end\":98067,\"start\":98047},{\"end\":98083,\"start\":98067},{\"end\":98100,\"start\":98083},{\"end\":98116,\"start\":98100},{\"end\":98130,\"start\":98116},{\"end\":98146,\"start\":98130},{\"end\":98161,\"start\":98146},{\"end\":98177,\"start\":98161},{\"end\":98754,\"start\":98737},{\"end\":98774,\"start\":98754},{\"end\":98789,\"start\":98774},{\"end\":98808,\"start\":98789},{\"end\":98820,\"start\":98808},{\"end\":98838,\"start\":98820},{\"end\":98852,\"start\":98838},{\"end\":98869,\"start\":98852},{\"end\":98889,\"start\":98869},{\"end\":98902,\"start\":98889},{\"end\":98913,\"start\":98902},{\"end\":98928,\"start\":98913},{\"end\":98942,\"start\":98928},{\"end\":98959,\"start\":98942},{\"end\":98985,\"start\":98959},{\"end\":99000,\"start\":98985},{\"end\":99011,\"start\":99000},{\"end\":99027,\"start\":99011},{\"end\":99037,\"start\":99027},{\"end\":99678,\"start\":99656},{\"end\":99691,\"start\":99678},{\"end\":99706,\"start\":99691},{\"end\":99720,\"start\":99706},{\"end\":100015,\"start\":100001},{\"end\":100028,\"start\":100015},{\"end\":100042,\"start\":100028},{\"end\":100051,\"start\":100042},{\"end\":100063,\"start\":100051},{\"end\":100075,\"start\":100063},{\"end\":100629,\"start\":100612},{\"end\":100644,\"start\":100629},{\"end\":100939,\"start\":100922},{\"end\":100954,\"start\":100939},{\"end\":101320,\"start\":101303},{\"end\":101335,\"start\":101320},{\"end\":101351,\"start\":101335},{\"end\":101366,\"start\":101351},{\"end\":101384,\"start\":101366},{\"end\":101399,\"start\":101384},{\"end\":101723,\"start\":101706},{\"end\":101739,\"start\":101723},{\"end\":101755,\"start\":101739},{\"end\":101773,\"start\":101755},{\"end\":101791,\"start\":101773},{\"end\":101806,\"start\":101791},{\"end\":101821,\"start\":101806},{\"end\":102123,\"start\":102107},{\"end\":102135,\"start\":102123},{\"end\":102146,\"start\":102135},{\"end\":102161,\"start\":102146},{\"end\":102599,\"start\":102585},{\"end\":102607,\"start\":102599},{\"end\":102876,\"start\":102862},{\"end\":102889,\"start\":102876},{\"end\":102904,\"start\":102889},{\"end\":102922,\"start\":102904},{\"end\":103177,\"start\":103164},{\"end\":103193,\"start\":103177},{\"end\":103210,\"start\":103193},{\"end\":103574,\"start\":103560},{\"end\":103590,\"start\":103574},{\"end\":103604,\"start\":103590},{\"end\":103617,\"start\":103604},{\"end\":103633,\"start\":103617},{\"end\":103646,\"start\":103633},{\"end\":103658,\"start\":103646},{\"end\":103672,\"start\":103658},{\"end\":103684,\"start\":103672},{\"end\":103698,\"start\":103684},{\"end\":104009,\"start\":103989},{\"end\":104022,\"start\":104009},{\"end\":104033,\"start\":104022},{\"end\":104044,\"start\":104033},{\"end\":104059,\"start\":104044},{\"end\":104071,\"start\":104059},{\"end\":104084,\"start\":104071},{\"end\":104411,\"start\":104391},{\"end\":104422,\"start\":104411},{\"end\":104436,\"start\":104422},{\"end\":104454,\"start\":104436},{\"end\":104466,\"start\":104454},{\"end\":104864,\"start\":104847},{\"end\":104875,\"start\":104864},{\"end\":104888,\"start\":104875},{\"end\":104906,\"start\":104888},{\"end\":104922,\"start\":104906},{\"end\":104935,\"start\":104922},{\"end\":104956,\"start\":104935},{\"end\":105614,\"start\":105598},{\"end\":105632,\"start\":105614},{\"end\":105649,\"start\":105632},{\"end\":105896,\"start\":105879},{\"end\":105917,\"start\":105896},{\"end\":105934,\"start\":105917},{\"end\":105951,\"start\":105934},{\"end\":105965,\"start\":105951},{\"end\":105980,\"start\":105965},{\"end\":105998,\"start\":105980},{\"end\":106016,\"start\":105998},{\"end\":106030,\"start\":106016},{\"end\":106042,\"start\":106030},{\"end\":106062,\"start\":106042},{\"end\":106075,\"start\":106062},{\"end\":106091,\"start\":106075},{\"end\":106107,\"start\":106091},{\"end\":106121,\"start\":106107},{\"end\":106138,\"start\":106121},{\"end\":106147,\"start\":106138},{\"end\":106160,\"start\":106147},{\"end\":106826,\"start\":106814},{\"end\":106837,\"start\":106826},{\"end\":106850,\"start\":106837},{\"end\":106863,\"start\":106850},{\"end\":106876,\"start\":106863},{\"end\":107619,\"start\":107599},{\"end\":107638,\"start\":107619},{\"end\":107658,\"start\":107638},{\"end\":107676,\"start\":107658},{\"end\":107966,\"start\":107956},{\"end\":107982,\"start\":107966},{\"end\":107988,\"start\":107982},{\"end\":108798,\"start\":108786},{\"end\":108811,\"start\":108798},{\"end\":108824,\"start\":108811},{\"end\":108842,\"start\":108824},{\"end\":108865,\"start\":108842},{\"end\":108880,\"start\":108865},{\"end\":109153,\"start\":109140},{\"end\":109166,\"start\":109153},{\"end\":109179,\"start\":109166},{\"end\":109191,\"start\":109179},{\"end\":110133,\"start\":110121},{\"end\":110149,\"start\":110133},{\"end\":110169,\"start\":110149},{\"end\":110429,\"start\":110415},{\"end\":110443,\"start\":110429},{\"end\":110458,\"start\":110443},{\"end\":110700,\"start\":110689},{\"end\":110713,\"start\":110700},{\"end\":110726,\"start\":110713},{\"end\":110744,\"start\":110726},{\"end\":111284,\"start\":111269},{\"end\":111297,\"start\":111284},{\"end\":111317,\"start\":111297},{\"end\":111332,\"start\":111317},{\"end\":111352,\"start\":111332},{\"end\":111365,\"start\":111352},{\"end\":111383,\"start\":111365},{\"end\":111395,\"start\":111383},{\"end\":111408,\"start\":111395},{\"end\":111425,\"start\":111408},{\"end\":111810,\"start\":111798},{\"end\":111820,\"start\":111810},{\"end\":111827,\"start\":111820},{\"end\":112730,\"start\":112716},{\"end\":112741,\"start\":112730},{\"end\":112753,\"start\":112741},{\"end\":112766,\"start\":112753},{\"end\":113036,\"start\":113023},{\"end\":113049,\"start\":113036},{\"end\":113072,\"start\":113049},{\"end\":113087,\"start\":113072},{\"end\":113103,\"start\":113087},{\"end\":113120,\"start\":113103},{\"end\":113128,\"start\":113120},{\"end\":113478,\"start\":113462},{\"end\":113491,\"start\":113478},{\"end\":113504,\"start\":113491},{\"end\":113522,\"start\":113504},{\"end\":114181,\"start\":114170},{\"end\":114197,\"start\":114181},{\"end\":114218,\"start\":114197},{\"end\":114236,\"start\":114218},{\"end\":114565,\"start\":114547},{\"end\":114579,\"start\":114565},{\"end\":114589,\"start\":114579},{\"end\":114598,\"start\":114589},{\"end\":114611,\"start\":114598},{\"end\":114626,\"start\":114611},{\"end\":114645,\"start\":114626},{\"end\":114660,\"start\":114645},{\"end\":114677,\"start\":114660},{\"end\":114695,\"start\":114677},{\"end\":114705,\"start\":114695},{\"end\":114717,\"start\":114705},{\"end\":114732,\"start\":114717},{\"end\":114750,\"start\":114732},{\"end\":115311,\"start\":115293},{\"end\":115325,\"start\":115311},{\"end\":115344,\"start\":115325},{\"end\":115359,\"start\":115344},{\"end\":115382,\"start\":115359},{\"end\":115684,\"start\":115669},{\"end\":115699,\"start\":115684},{\"end\":115718,\"start\":115699},{\"end\":115735,\"start\":115718},{\"end\":115747,\"start\":115735},{\"end\":115767,\"start\":115747},{\"end\":115785,\"start\":115767},{\"end\":116093,\"start\":116078},{\"end\":116111,\"start\":116093},{\"end\":116130,\"start\":116111},{\"end\":116145,\"start\":116130},{\"end\":116162,\"start\":116145},{\"end\":116178,\"start\":116162},{\"end\":116195,\"start\":116178},{\"end\":116209,\"start\":116195},{\"end\":116222,\"start\":116209},{\"end\":116246,\"start\":116222},{\"end\":116261,\"start\":116246},{\"end\":116276,\"start\":116261},{\"end\":116294,\"start\":116276},{\"end\":116703,\"start\":116684},{\"end\":116718,\"start\":116703},{\"end\":116738,\"start\":116718},{\"end\":116754,\"start\":116738},{\"end\":116772,\"start\":116754},{\"end\":116789,\"start\":116772},{\"end\":116804,\"start\":116789},{\"end\":116817,\"start\":116804},{\"end\":116832,\"start\":116817},{\"end\":116845,\"start\":116832},{\"end\":116863,\"start\":116845},{\"end\":117243,\"start\":117229},{\"end\":117255,\"start\":117243},{\"end\":117268,\"start\":117255},{\"end\":117280,\"start\":117268},{\"end\":117294,\"start\":117280},{\"end\":117310,\"start\":117294},{\"end\":117596,\"start\":117584},{\"end\":117616,\"start\":117596},{\"end\":117628,\"start\":117616},{\"end\":117644,\"start\":117628},{\"end\":117661,\"start\":117644},{\"end\":117675,\"start\":117661},{\"end\":117691,\"start\":117675},{\"end\":117708,\"start\":117691},{\"end\":117720,\"start\":117708},{\"end\":117736,\"start\":117720},{\"end\":117754,\"start\":117736},{\"end\":117768,\"start\":117754},{\"end\":117782,\"start\":117768},{\"end\":117798,\"start\":117782},{\"end\":117814,\"start\":117798},{\"end\":117840,\"start\":117814},{\"end\":117861,\"start\":117840},{\"end\":117876,\"start\":117861},{\"end\":117890,\"start\":117876},{\"end\":117905,\"start\":117890},{\"end\":117921,\"start\":117905},{\"end\":117940,\"start\":117921},{\"end\":117955,\"start\":117940},{\"end\":117972,\"start\":117955},{\"end\":117985,\"start\":117972},{\"end\":118000,\"start\":117985},{\"end\":118018,\"start\":118000},{\"end\":118032,\"start\":118018},{\"end\":118040,\"start\":118032},{\"end\":118053,\"start\":118040},{\"end\":118073,\"start\":118053},{\"end\":118092,\"start\":118073},{\"end\":118106,\"start\":118092},{\"end\":118123,\"start\":118106},{\"end\":118139,\"start\":118123},{\"end\":118157,\"start\":118139},{\"end\":118172,\"start\":118157},{\"end\":118186,\"start\":118172},{\"end\":118205,\"start\":118186},{\"end\":118223,\"start\":118205},{\"end\":118240,\"start\":118223},{\"end\":118259,\"start\":118240},{\"end\":118275,\"start\":118259},{\"end\":118295,\"start\":118275},{\"end\":118310,\"start\":118295},{\"end\":118333,\"start\":118310},{\"end\":118353,\"start\":118333},{\"end\":120402,\"start\":120388},{\"end\":120416,\"start\":120402},{\"end\":120430,\"start\":120416},{\"end\":120445,\"start\":120430},{\"end\":120460,\"start\":120445},{\"end\":120476,\"start\":120460},{\"end\":120488,\"start\":120476},{\"end\":120496,\"start\":120488},{\"end\":120509,\"start\":120496},{\"end\":120873,\"start\":120855},{\"end\":120885,\"start\":120873},{\"end\":120905,\"start\":120885},{\"end\":120918,\"start\":120905},{\"end\":121149,\"start\":121140},{\"end\":121163,\"start\":121149},{\"end\":121178,\"start\":121163},{\"end\":121191,\"start\":121178},{\"end\":121202,\"start\":121191},{\"end\":122050,\"start\":122030},{\"end\":122656,\"start\":122629},{\"end\":122670,\"start\":122656},{\"end\":122689,\"start\":122670},{\"end\":122712,\"start\":122689},{\"end\":122721,\"start\":122712},{\"end\":122993,\"start\":122970},{\"end\":123005,\"start\":122993},{\"end\":123023,\"start\":123005},{\"end\":123035,\"start\":123023},{\"end\":123050,\"start\":123035},{\"end\":123381,\"start\":123368},{\"end\":123392,\"start\":123381},{\"end\":123662,\"start\":123649},{\"end\":123679,\"start\":123662},{\"end\":124450,\"start\":124437},{\"end\":124467,\"start\":124450},{\"end\":125143,\"start\":125130},{\"end\":125156,\"start\":125143},{\"end\":125170,\"start\":125156},{\"end\":125179,\"start\":125170},{\"end\":125196,\"start\":125179},{\"end\":125767,\"start\":125754},{\"end\":125784,\"start\":125767},{\"end\":125800,\"start\":125784},{\"end\":125806,\"start\":125800},{\"end\":125820,\"start\":125806},{\"end\":125834,\"start\":125820},{\"end\":126601,\"start\":126587},{\"end\":126615,\"start\":126601},{\"end\":126626,\"start\":126615},{\"end\":126639,\"start\":126626},{\"end\":126653,\"start\":126639},{\"end\":126951,\"start\":126937},{\"end\":126968,\"start\":126951},{\"end\":126985,\"start\":126968},{\"end\":127001,\"start\":126985},{\"end\":127017,\"start\":127001},{\"end\":127031,\"start\":127017},{\"end\":127408,\"start\":127394},{\"end\":127425,\"start\":127408},{\"end\":127441,\"start\":127425},{\"end\":127460,\"start\":127441},{\"end\":127480,\"start\":127460},{\"end\":127494,\"start\":127480},{\"end\":127504,\"start\":127494},{\"end\":127524,\"start\":127504},{\"end\":127540,\"start\":127524},{\"end\":127559,\"start\":127540},{\"end\":127572,\"start\":127559},{\"end\":127585,\"start\":127572},{\"end\":127609,\"start\":127585},{\"end\":127625,\"start\":127609},{\"end\":127635,\"start\":127625},{\"end\":127653,\"start\":127635},{\"end\":127665,\"start\":127653},{\"end\":127682,\"start\":127665},{\"end\":127698,\"start\":127682},{\"end\":127715,\"start\":127698},{\"end\":128283,\"start\":128272},{\"end\":128305,\"start\":128283},{\"end\":128322,\"start\":128305},{\"end\":128340,\"start\":128322},{\"end\":128348,\"start\":128340},{\"end\":128534,\"start\":128517},{\"end\":128553,\"start\":128534},{\"end\":128565,\"start\":128553},{\"end\":128581,\"start\":128565},{\"end\":128602,\"start\":128581},{\"end\":128612,\"start\":128602},{\"end\":128626,\"start\":128612},{\"end\":128638,\"start\":128626},{\"end\":128650,\"start\":128638},{\"end\":128660,\"start\":128650},{\"end\":128672,\"start\":128660},{\"end\":128684,\"start\":128672},{\"end\":128697,\"start\":128684},{\"end\":128709,\"start\":128697},{\"end\":128727,\"start\":128709},{\"end\":128745,\"start\":128727},{\"end\":128758,\"start\":128745},{\"end\":128773,\"start\":128758},{\"end\":128789,\"start\":128773},{\"end\":128800,\"start\":128789},{\"end\":128816,\"start\":128800},{\"end\":128828,\"start\":128816},{\"end\":128839,\"start\":128828},{\"end\":128856,\"start\":128839},{\"end\":128869,\"start\":128856},{\"end\":128887,\"start\":128869},{\"end\":128898,\"start\":128887},{\"end\":128922,\"start\":128898},{\"end\":128934,\"start\":128922},{\"end\":128954,\"start\":128934},{\"end\":128971,\"start\":128954},{\"end\":130521,\"start\":130507},{\"end\":130538,\"start\":130521},{\"end\":130567,\"start\":130538},{\"end\":130581,\"start\":130567},{\"end\":130866,\"start\":130849},{\"end\":130880,\"start\":130866},{\"end\":131127,\"start\":131112},{\"end\":131145,\"start\":131127},{\"end\":131164,\"start\":131145},{\"end\":131183,\"start\":131164},{\"end\":131198,\"start\":131183},{\"end\":131775,\"start\":131766},{\"end\":131785,\"start\":131775},{\"end\":131979,\"start\":131966},{\"end\":131991,\"start\":131979},{\"end\":132003,\"start\":131991},{\"end\":132021,\"start\":132003},{\"end\":132033,\"start\":132021},{\"end\":132876,\"start\":132865},{\"end\":132884,\"start\":132876},{\"end\":132901,\"start\":132884},{\"end\":132915,\"start\":132901},{\"end\":132928,\"start\":132915},{\"end\":132948,\"start\":132928},{\"end\":132963,\"start\":132948},{\"end\":132978,\"start\":132963},{\"end\":132990,\"start\":132978},{\"end\":133006,\"start\":132990},{\"end\":133016,\"start\":133006},{\"end\":133037,\"start\":133016},{\"end\":133052,\"start\":133037},{\"end\":133065,\"start\":133052},{\"end\":133076,\"start\":133065},{\"end\":133091,\"start\":133076},{\"end\":133506,\"start\":133488},{\"end\":133526,\"start\":133506},{\"end\":133542,\"start\":133526},{\"end\":133561,\"start\":133542},{\"end\":133579,\"start\":133561},{\"end\":133594,\"start\":133579},{\"end\":133609,\"start\":133594},{\"end\":134109,\"start\":134098},{\"end\":134124,\"start\":134109},{\"end\":134131,\"start\":134124},{\"end\":134147,\"start\":134131},{\"end\":134159,\"start\":134147},{\"end\":134173,\"start\":134159},{\"end\":134187,\"start\":134173},{\"end\":134204,\"start\":134187},{\"end\":134559,\"start\":134546},{\"end\":134568,\"start\":134559},{\"end\":134584,\"start\":134568},{\"end\":134599,\"start\":134584},{\"end\":134616,\"start\":134599},{\"end\":134638,\"start\":134616},{\"end\":134661,\"start\":134638},{\"end\":134977,\"start\":134963},{\"end\":134995,\"start\":134977},{\"end\":135006,\"start\":134995},{\"end\":135772,\"start\":135750},{\"end\":135790,\"start\":135772},{\"end\":135809,\"start\":135790},{\"end\":135815,\"start\":135809},{\"end\":136482,\"start\":136467},{\"end\":136524,\"start\":136482},{\"end\":136530,\"start\":136524}]", "bib_venue": "[{\"end\":83966,\"start\":83890},{\"end\":84508,\"start\":84495},{\"end\":84998,\"start\":84934},{\"end\":85264,\"start\":85193},{\"end\":86445,\"start\":86393},{\"end\":86883,\"start\":86858},{\"end\":87432,\"start\":87423},{\"end\":87614,\"start\":87569},{\"end\":89860,\"start\":89851},{\"end\":90190,\"start\":90185},{\"end\":90571,\"start\":90560},{\"end\":90997,\"start\":90936},{\"end\":91532,\"start\":91480},{\"end\":92127,\"start\":92041},{\"end\":93092,\"start\":92991},{\"end\":93760,\"start\":93691},{\"end\":94228,\"start\":94179},{\"end\":94878,\"start\":94716},{\"end\":95653,\"start\":95539},{\"end\":95958,\"start\":95887},{\"end\":96336,\"start\":96284},{\"end\":96701,\"start\":96647},{\"end\":97102,\"start\":97021},{\"end\":97650,\"start\":97563},{\"end\":98263,\"start\":98177},{\"end\":99069,\"start\":99037},{\"end\":99739,\"start\":99734},{\"end\":100165,\"start\":100075},{\"end\":100610,\"start\":100524},{\"end\":101006,\"start\":100954},{\"end\":101301,\"start\":101239},{\"end\":101704,\"start\":101638},{\"end\":102242,\"start\":102181},{\"end\":102631,\"start\":102607},{\"end\":102931,\"start\":102922},{\"end\":103296,\"start\":103234},{\"end\":103558,\"start\":103519},{\"end\":103987,\"start\":103929},{\"end\":104518,\"start\":104466},{\"end\":105060,\"start\":104991},{\"end\":105596,\"start\":105535},{\"end\":106241,\"start\":106180},{\"end\":106982,\"start\":106896},{\"end\":107695,\"start\":107690},{\"end\":108161,\"start\":108019},{\"end\":108784,\"start\":108726},{\"end\":109382,\"start\":109220},{\"end\":110178,\"start\":110169},{\"end\":110413,\"start\":110329},{\"end\":110841,\"start\":110764},{\"end\":111267,\"start\":111204},{\"end\":112018,\"start\":111856},{\"end\":112714,\"start\":112670},{\"end\":113147,\"start\":113142},{\"end\":113628,\"start\":113542},{\"end\":114302,\"start\":114252},{\"end\":114769,\"start\":114764},{\"end\":115291,\"start\":115206},{\"end\":115667,\"start\":115611},{\"end\":116076,\"start\":116035},{\"end\":116682,\"start\":116605},{\"end\":117333,\"start\":117310},{\"end\":117582,\"start\":117508},{\"end\":120386,\"start\":120305},{\"end\":120929,\"start\":120918},{\"end\":121344,\"start\":121202},{\"end\":122148,\"start\":122050},{\"end\":122627,\"start\":122583},{\"end\":122968,\"start\":122875},{\"end\":123411,\"start\":123406},{\"end\":123828,\"start\":123708},{\"end\":124583,\"start\":124497},{\"end\":125285,\"start\":125219},{\"end\":125959,\"start\":125865},{\"end\":126585,\"start\":126525},{\"end\":127050,\"start\":127045},{\"end\":127392,\"start\":127289},{\"end\":128270,\"start\":128194},{\"end\":130505,\"start\":130440},{\"end\":130847,\"start\":130803},{\"end\":131247,\"start\":131198},{\"end\":131789,\"start\":131785},{\"end\":132228,\"start\":132053},{\"end\":132863,\"start\":132820},{\"end\":133677,\"start\":133609},{\"end\":134096,\"start\":134013},{\"end\":134544,\"start\":134471},{\"end\":135121,\"start\":135029},{\"end\":135925,\"start\":135832},{\"end\":136611,\"start\":136550},{\"end\":87437,\"start\":87434},{\"end\":89865,\"start\":89862},{\"end\":90578,\"start\":90573},{\"end\":92218,\"start\":92129},{\"end\":93180,\"start\":93094},{\"end\":95027,\"start\":94880},{\"end\":97170,\"start\":97104},{\"end\":97724,\"start\":97652},{\"end\":98336,\"start\":98265},{\"end\":99095,\"start\":99071},{\"end\":100242,\"start\":100167},{\"end\":102936,\"start\":102933},{\"end\":107074,\"start\":106984},{\"end\":108296,\"start\":108163},{\"end\":109531,\"start\":109384},{\"end\":110183,\"start\":110180},{\"end\":110905,\"start\":110843},{\"end\":112167,\"start\":112020},{\"end\":113718,\"start\":113630},{\"end\":120936,\"start\":120931},{\"end\":121495,\"start\":121346},{\"end\":122233,\"start\":122150},{\"end\":123935,\"start\":123830},{\"end\":124656,\"start\":124585},{\"end\":125338,\"start\":125287},{\"end\":126040,\"start\":125961},{\"end\":132390,\"start\":132230},{\"end\":133732,\"start\":133679},{\"end\":135231,\"start\":135123},{\"end\":136026,\"start\":135927}]"}}}, "year": 2023, "month": 12, "day": 17}