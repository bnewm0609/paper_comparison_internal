{"id": 239016164, "updated": "2023-10-05 21:26:26.243", "metadata": {"title": "Natural Attribute-based Shift Detection", "authors": "[{\"first\":\"Jeonghoon\",\"last\":\"Park\",\"middle\":[]},{\"first\":\"Jimin\",\"last\":\"Hong\",\"middle\":[]},{\"first\":\"Radhika\",\"last\":\"Dua\",\"middle\":[]},{\"first\":\"Daehoon\",\"last\":\"Gwak\",\"middle\":[]},{\"first\":\"Yixuan\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jaegul\",\"last\":\"Choo\",\"middle\":[]},{\"first\":\"Edward\",\"last\":\"Choi\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2021, "month": 10, "day": 18}, "abstract": "Despite the impressive performance of deep networks in vision, language, and healthcare, unpredictable behaviors on samples from the distribution different than the training distribution cause severe problems in deployment. For better reliability of neural-network-based classifiers, we define a new task, natural attribute-based shift (NAS) detection, to detect the samples shifted from the training distribution by some natural attribute such as age of subjects or brightness of images. Using the natural attributes present in existing datasets, we introduce benchmark datasets in vision, language, and medical for NAS detection. Further, we conduct an extensive evaluation of prior representative out-of-distribution (OOD) detection methods on NAS datasets and observe an inconsistency in their performance. To understand this, we provide an analysis on the relationship between the location of NAS samples in the feature space and the performance of distance- and confidence-based OOD detection methods. Based on the analysis, we split NAS samples into three categories and further suggest a simple modification to the training objective to obtain an improved OOD detection method that is capable of detecting samples from all NAS categories.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2110.09276", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2110-09276", "doi": null}}, "content": {"source": {"pdf_hash": "31fbe0427d0479ed59058d82e0a14041433d4a88", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2110.09276v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7e4107118fe9365c57d1a3f4ab6a1b52e678ab2a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/31fbe0427d0479ed59058d82e0a14041433d4a88.txt", "contents": "\nNATURAL ATTRIBUTE-BASED SHIFT DETECTION\n\n\nJeonghoon Park jeonghoonpark@kaist.ac.kr \nKAIST 1\nUniversity of Wisconsin-Madison\n\n\nJimin Hong \nKAIST 1\nUniversity of Wisconsin-Madison\n\n\nRadhika Dua radhikadua@kaist.ac.kr \nKAIST 1\nUniversity of Wisconsin-Madison\n\n\nDaehoon Gwak daehoon.gwak@kaist.ac.kr \nKAIST 1\nUniversity of Wisconsin-Madison\n\n\nYixuan Li \nKAIST 1\nUniversity of Wisconsin-Madison\n\n\nJaegul Choo jchoo@kaist.ac.kr \nKAIST 1\nUniversity of Wisconsin-Madison\n\n\nEdward Choi edwardchoi@kaist.ac.kr \nKAIST 1\nUniversity of Wisconsin-Madison\n\n\nNATURAL ATTRIBUTE-BASED SHIFT DETECTION\n\nDespite the impressive performance of deep networks in vision, language, and healthcare, unpredictable behaviors on samples from the distribution different than the training distribution cause severe problems in deployment. For better reliability of neural-network-based classifiers, we define a new task, natural attributebased shift (NAS) detection, to detect the samples shifted from the training distribution by some natural attribute such as age of subjects or brightness of images. Using the natural attributes present in existing datasets, we introduce benchmark datasets in vision, language, and medical for NAS detection. Further, we conduct an extensive evaluation of prior representative out-of-distribution (OOD) detection methods on NAS datasets and observe an inconsistency in their performance. To understand this, we provide an analysis on the relationship between the location of NAS samples in the feature space and the performance of distance-and confidence-based OOD detection methods. Based on the analysis, we split NAS samples into three categories and further suggest a simple modification to the training objective to obtain an improved OOD detection method that is capable of detecting samples from all NAS categories. * Equal contribution arXiv:2110.09276v1 [cs.LG]\n\nINTRODUCTION\n\nDeep learning has significantly improved the performance in various domains such as computer vision, natural language processing, and healthcare (Bojarski et al., 2016;Mikolov et al., 2013;Esteva et al., 2016). However, it has been reported that the deep classifiers make unreliable predictions on samples drawn from a different distribution than the training distribution (Amodei et al., 2016;Hendrycks & Gimpel, 2017;Nguyen et al., 2015). Especially, this problem can become severe when the test distribution is shifted from the training distribution by some attributes (i.e., age of subjects or brightness of images), as such shift could gradually degrade the classifier performance until its malfunction is explicitly visible. These shifts occur in the real-world as a result of a change in specific attribute. For example, a clinical text-based diagnosis classifier trained in 2021 will gradually encounter increasingly shifted samples as time flows, since writing styles change and new terms are introduced in time. Detection of such samples is a vital task especially in safety-critical systems, such as autonomous vehicle control or medical diagnosis, where wrong predictions can lead to dire consequences. To this end, we take a step forward by proposing a new task of detecting samples shifted by a natural attribute (e.g., age, time) that can easily be observed in the real-world setting. We refer to such shifts as Natural Attribute-based Shifts (NAS), and the task of detecting them as NAS detection.\n\nDetection of NAS is both different from, and also more challenging than out-of-distribution (OOD) detection (Hendrycks & Gimpel, 2017;Liang et al., 2018;Lee et al., 2018;Sastry & Oore, 2020), which typically evaluates the detection methods with a clearly distinguished in-distribution (ID) samples and OOD samples (e.g., CIFAR10 as ID and SVHN as OOD, which have disjoint labels). In contrast, we aim to detect samples from a natural attribute-based shift within the same label space. Since NAS samples share more features with the ID than the typical OOD samples do, identifying the former is expected to be more challenging than the latter. Although OOD detection has some relevance to NAS detection, comprehensive evaluation of the existing OOD detection methods on the natural attribute-based shift is an unexplored territory. Therefore, in this paper, we perform an extensive evaluation of representative OOD methods on NAS samples.\n\nDepending on the task environment, NAS detection can be pursued in parallel to domain generalization (Seo et al., 2020;Gulrajani & Lopez-Paz, 2020;Carlucci et al., 2019), which aims to overcome domain shifts (e.g., image classifier adapting to sketches, photos, art paintings, etc.). For example, an X-ray-based diagnosis model should detect images of unusual brightness so that the X-ray machine can be properly configured, and the diagnosis model can perform in the optimal setting. In other cases, domain generalization can be preferred, such as when we expect the classifier to be deployed in a less controlled environment (e.g., online image classifier) for non-safety critical tasks.\n\nIn this paper, we formalize NAS detection to enhance the reliability of real-world decision systems. Since there exists no standard dataset for this task, we create a new benchmark dataset in the vision, text, and medical domain by adjusting the natural attributes (e.g., age, time, and brightness) of the ID dataset. Then we conduct an extensive evaluation of representative confidence-and distance-based OOD methods on our datasets and observe that none of the methods perform consistently across all NAS datasets.\n\nAfter a careful analysis on where NAS samples reside in the feature space and its impact on the distance-and confidence-based OOD detection performance, we identify the root cause of the inconsistent performance. Following this observation, we define three general NAS categories based on two criteria: the distance between NAS samples and the decision boundary, the distance between NAS samples, and the ID data. Finally, we conduct an additional experiment to demonstrate that a simple modification to the negative log-likelihood training objective can dramatically help the Mahalanobis detector (Lee et al., 2018), a distance-based OOD detection method, generalize to all NAS categories. We also compare our results with various baselines and show that our proposed modification outperforms the baselines and is effective across the three NAS datasets.\n\nIn summary, the contributions of this paper are as follows:\n\n\u2022 We define a new task, Natural Attribute-based Shift detection (NAS detection), which aims to detect the samples from a distribution shifted by some natural attribute. We create a new benchmark dataset and provide them to encourage further research on evaluating NAS detection.\n\n\u2022 To the best of our knowledge, this is the first work to conduct a comprehensive evaluation of the OOD detection methods on shifts based on natural attributes, and discover that none of the OOD methods perform consistently across all NAS scenarios.\n\n\u2022 We provide novel analysis based on the location of shifted samples in the feature space and the performance of existing OOD detection methods. Based on the analysis, we split NAS samples into three categories.\n\n\u2022 We demonstrate that a simple yet effective modification to the training objective for deep classifiers enables consistent OOD detection performances for all NAS categories.\n\n\nNATURAL ATTRIBUTE-BASED SHIFT DETECTION\n\nWe now formalize a new task, NAS detection, which aims to enhance the reliability of real-world decision systems by detecting samples from NAS. We address this task in the classification problems. Let D I = {X , Y} denote the in-distribution data, which is composed of N training samples with inputs X = {x 1 , ..., x N } and labels Y = {y 1 , ..., y N }. Specifically, x i \u2208 R d represents a d-dimensional input vector, and y i \u2208 K represents its corresponding label where K = {1, ..., K} is a set of class labels. The discriminative model f \u03b8 : X \u2192 Y learns with ID dataset D I to assign label y i for each x i . In the NAS detection setting, we assume that an in-distribution sample consists of attributes, and some of the attributes can be shifted in the test time due to natural causes such as time, age, or brightness.\n\nWhen a particular attribute A (e.g., age), which has a value of a (e.g., 16), is shifted by the degree of \u03b4, the shifted distribution can be denoted as D A=a+\u03b4\nS = {X , Y }. X = {x 1 , ..\n., x M } and Y = {y 1 , ..., y M } represents the M shifted samples and labels respectively. Importantly, in the NAS setting, although the test distribution is changed from the ID, the label space is preserved as K, which is the set of class labels in D I . In the test time, the model f \u03b8 might encounter the sample x  Figure 1: Facial images from the UTKFace dataset to show the variation with age. X-ray images with different levels of brightness created from the RSNA Bone Age dataset.\n\nfrom a shifted data D A=a+\u03b4 S , and it should be able to identify that the attribute-shifted sample is not from the ID.\n\n\nNAS DATASET DESCRIPTION\n\nIn this section, we describe three benchmark datasets which have a controllable attribute for simulating realistic distribution shifts. Since there exists no standard dataset for NAS detection, we create new benchmark datasets using existing datasets by adjusting natural attributes in order to reflect real-world scenarios. We carefully select datasets from vision, language, and medical domains containing natural attributes (e.g., year, age, and brightness), which allows us to naturally split the samples. By grouping samples based on these attributes, we can induce natural attribute-based distribution shifts as described below.\n\n\nImage.\n\nWe use the UTKFace dataset (Zhang et al., 2017) which consists of over 20, 000 face images with annotations of age, gender, and ethnicity. As shown in Figure 1, we can visually observe that the facial images vary with age. Therefore, we set the 1, 282 facial images of 26 years old age as D I . For creating the NAS dataset, we vary the age of UTKFace dataset. To obtain an equal number of samples in the NAS dataset, the age groups that has less than 200 images are merged into one group until it has 200 samples. Finally, 15 groups D age S are produced for the NAS datasets, varying the ages from 25 to 1 (i.e., D age=25\nS , D age=24 S , . . . , D age=1 S ).\nText. We use the Amazon Review dataset (He & McAuley, 2016;McAuley et al., 2015) which contains product reviews from Amazon. We consider the product category \"book\" and group its reviews based on the year to reflect the distributional shift across time. We obtain 9 groups with each group containing reviews from the year between 2005 and 2014. Then, the group with 24, 000 reviews posted in 2005 is set as D I , and the groups with reviews after 2005 as D year S (i.e., D year=2006 S , D year=2007 S , . . . , D year=2014 S ). Each D year S group contains 1500 positive reviews and 1500 negative reviews. We observed that as we move ahead in time, the average length of a review gets shorter and it uses more adjectives than previous years. Due to the space constraint, we provide a detailed analysis of the dataset in the Section B of the Appendix.\n\n\nMedical.\n\nWe use the RSNA Bone Age dataset (Halabi et al., 2019), a real-world dataset that contains left-hand X-ray images of the patient, along with their gender and age (0 to 20 years). We consider patients in the age group of 10 to 12 years for our dataset. To reflect diverse X-ray imaging set-ups in the hospital, we varied the brightness factor between 0 and 4.5 and form 16 different dataset D brightness S (i.e., D brightness=0.0 S , D brightness=0.2 S , . . . , D brightness=4.5 S ), and each group contains Xray images of 200 males and 200 females. Figure 1 presents X-ray images with different levels of brightness with realistic and continuous distribution shifts. In-distribution data D I is composed of 3, 000 images of brightness factor 1.0 (unmodified images).\n\n\nCAN OOD DETECTION METHODS ALSO DETECT NAS?\n\nIn this section, we briefly discuss about OOD detection methods and conduct an extensive evaluation of OOD detection methods on our proposed benchmark datasets.\n\n\nOOD DETECTION METHODS\n\nIn this work, we use three widely-used post-hoc and modality-agnostic OOD detection methods. We use maximum of softmax probability (MSP) (Hendrycks & Gimpel, 2017) and ODIN (Liang et al., 2018) as confidence-based OOD detection baselines, and Mahalanobis detector (Lee et al., 2018) as distance-based OOD detection baseline. Note that ODIN and Mahalanobis detector assume the availability of OOD validation dataset to tune their hyperparameters. However, for all our experiments, we use variants of the above methods that do not access the OOD validation dataset as  Figure 2: Comparison of well-known distance-based and confidence-based OOD detection methods for the CE model on our benchmark datasets. Age '26', year '2005', and brightness '1.0' are indistribution data in UTKFace, Amazon Review, and RSNA Bone Age dataset, respectively. The NAS detection performance of these methods is inconsistent across different datasets. Hsu et al. (2020). The exact equations and details of how each OOD detection method assigns an OOD score to a given sample is provided in Section A of the Appendix.\n\n\nEXPERIMENTS AND RESULTS\n\nWe now systematically evaluate the performance of the three OOD detection methods under NAS. We report the AUROC of all OOD detection methods averaged across five random seeds, evaluated for all NAS datasets.\n\nExperimental Settings. In image domain, we train a gender classification model on our UTKFace NAS dataset using ResNet18 model and the cross-entropy loss. We use our Amazon Book Review NAS dataset in text domain and train a 4-layer Transformer with the cross-entropy loss for the sentiment classification task. Lastly, in medical domain, we use our RSNA Bone Age NAS dataset and train a ResNet18 with cross-entropy loss to predict the gender given the hand X-ray image of the patient. We then evaluate the trained model on the corresponding test set in image, text, medical domains, respectively. Further, we evaluate the NAS detection performance of representative OOD detection methods in image, text, and medical domains on their corresponding NAS datasets, which gradually shift with age, year, and brightness, respectively.\n\n\nResults.\n\nWe present the classification accuracy of the trained models on the ID test set in Table 2. We observe that the models trained using the cross-entropy loss obtain high accuracy and perform well on their corresponding tasks. We further demonstrate the effectiveness of the existing representative OOD detection methods on our benchmark datasets in Figure 2. We observe that in the UTKFace NAS dataset, samples are detected by ODIN and MSP, which are confidence-based methods, but not by Mahalanobis detector (Figure 2a). In the Amazon Review dataset, NAS samples are detected only by the Mahalanobis detector, while MSP and ODIN fail ( Figure 2b). Moreover, the scores of confidence-based methods are lower than 50 in AUROC. Lastly, Figure 2c shows that inputs from NAS in the RSNA Bone Age dataset are detected well by all three methods.\n\n\nANALYZING INCONSISTENCY OF OOD DETECTION METHODS\n\nIn this section, we first study the behavior of NAS samples in three datasets using PCA visualization. Then we analyze the inconsistent performance of the OOD detection methods by considering them in two categories, namely confidence-based and distance-based methods. Lastly, based on the analysis, we conclude this section by defining three NAS categories.\n\n\nANALYSIS OF THE LOCATION OF NAS SAMPLES\n\nAs illustrated in Figure 3, we apply principal component analysis (PCA) on the feature representations obtained from the penultimate layer of the models to visualize the movement of NAS samples as we monotonically increase the degree of attribute shift (i.e., age, year, and brightness). Further, Figure 4 presents the model's prediction confidence across varying degrees of the attribute shift.\n\n\nImage.\n\nBy gradually changing the age, NAS samples move toward the space between the two clusters of ID samples (i.e., the decision boundary) as can be seen in Figure 3 [Top]. Further, Figure  4a demonstrates that confidence decrease as we increase the degree of attribute shift, which indicates that NAS samples move close to the decision boundary. Note that the majority of the NAS samples still overlap with ID sample clusters as we change the age.\n\nText. As shown in Figure 3 [Middle], NAS samples gradually move away from the ID samples (and away from the decision boundary) as the year changes. In contrast to the UTKFace dataset, the confidence gradually increases, as shown in Figure 4b, since the NAS samples are getting far away from the decision boundary.  Medical. Figure 3 [Bottom] demonstrates that when we increase the brightness, NAS samples move to the middle of the two classes and also move towards the outer edge of the ID sample clusters. Furthermore, as shown in Figure 4c, the relatively decreased confidence indicates that NAS samples are placed near the decision boundary as we increase the brightness of the images.\n\n\nCOMPARISON BETWEEN CONFIDENCE-BASED AND DISTANCE-BASED OOD DETECTION\n\nConfidence-based methods. Figure 2 and Figure 3 illustrate that confidence-based methods achieve high AUROC when the NAS samples are near the decision boundary due to their low confidence. In the image and medical domain, we observed the NAS samples moving towards the decision boundary with increasing attribute shift, thus they are detected by the confidence-based methods. In contrast, in the text domain, the high prediction confidence of the shifted NAS samples causes the degradation in the AUROC of the confidence-based methods. Therefore, we conclude that to effectively utilize the confidence-based methods in all three NAS datasets, it is necessary to reduce the confidence of samples outside the ID, namely, enforce NAS samples to move near the decision boundary, which is not always possible (e.g., Amazon Review dataset).\n\nDistance-based methods. From Figure 2 and Figure 3, we observe that the distance-based OOD detection method (i.e., Mahalanobis Detector) achieves high AUROC when NAS samples are sufficiently away from the ID samples. In the text and medical domains, the Mahalanobis detector worked well since NAS samples moved sufficiently away from the ID samples as the shift increased. However, in the image domain, the method fails to detect NAS samples because instead of deviating from the ID, they move intermediately between the classes.\n\nPrior works (Luo et al., 2020;Liu et al., 2017) report that the cross-entropy loss cannot guarantee a sufficient inter-class distance. In other words, representations do not need to be far from the decision boundary to lower the cross-entropy loss. In this regard, we assume that the performance degradation of the Mahalanobis detector is caused by the cross-entropy loss learning latent features that are not separable enough to detect the NAS samples located between the classes (e.g., Figure 3 [Top]). Specifically, if some classes are located nearby in the feature space, samples moving between classes (i.e., the case of the UTKFace dataset) will not be far from the ID. Even though NAS samples move away from one of the ID class cluster, they will gradually get closer to another ID class cluster. Figure 5: ID score landscape (brighter region means higher ID score) of the existing OOD detection methods (left: MSP, middle: ODIN, right: Mahalanobis). We use a synthetic 2D dataset to train a 4-layer ResNet. The Red points represent the ID samples; Purple Stars, Gray Diamonds and Orange Triangles indicate samples from different NAS categories. A sample is regarded as NAS when it has a low ID score.  Considering the performance of confidence-based and distance-based detection methods, we now divided NAS into three categories based on two criteria: 1) whether the dataset is near the decision boundary or not; 2) whether they are close or far from the in-distribution dataset. Since a dataset (i.e., samples) far from the decision boundary and overlapping with the in-distribution dataset is not a distributionally shifted dataset, our work focuses on the remaining three cases that cover all possible scenarios of NAS. Without loss of generality, we use a classification task with three classes as a motivating example depicted by Figure 5.\n\n\nNAS CATEGORIZATION\n\n\u2022 NAS category 1: This category comprises of NAS samples that are located near the decision boundary, and between ID samples of different classes. Purple Stars in Figure 5 represents samples from this category. Such samples are easily detected by confidence-based methods, MSP and ODIN, but harder to be detected by Mahalanobis Detector, which is a distance-based method. \u2022 NAS category 2: This category consists of NAS samples that are placed away from the decision boundary and the ID data. For example, Gray Diamonds in Figure 5. Mahalanobis detector regards such samples as NAS, whereas confidence-based methods fail to detect them since NAS samples have a higher prediction confidence (i.e., higher ID score) than ID samples. \u2022 NAS category 3: This category mainly comprises of NAS samples located anywhere near the decision boundary but far away from ID data. For example, Orange Triangles in Figure 5. Such samples are easily detected by both distance-based and confidence-based OOD detection methods.\n\n\nMETHOD FOR CONSISTENT NAS DETECTION PERFORMANCE\n\nIn this section, we suggest a modification in the training objective for deep classifiers to encourage consistent NAS detection performance on all NAS categories. Then we provide experiment results where the proposed method was compared against diverse OOD detection methods on the three NAS datasets (UTKFace, Amazon Review, RSNA Bone Age).\n\n6.1 METHOD For a generally applicable OOD detection method to all NAS categories, we suggest a new training objective for deep classifiers comprised of classification loss (L CE ), distance loss (L dist ) and entropy loss (L entropy ). The proposed objective improves the performance of the Mahalanobis detector on NAS samples from category 1 without sacrificing performance on NAS samples from other categories. We focus on improving the distance-based OOD detection method rather than the confidence-based method since it is not always possible to enforce NAS samples to be embedded near the decision boundary, as discussed in Section 5.2.  Table 2: In-distribution classification accuracy on three datasets with cross-entropy loss and our proposed loss.\n\nThe proposed training loss is defined as:\nL total = L CE + L dist + L entropy .(1)\nNote that for classification loss, we use the standard cross-entropy loss, but other losses such as focal loss can also be used. The distance loss is used to increase the distance between distinct class distributions of ID samples so that NAS samples have larger space to move around without overlapping with ID clusters, especially in NAS category 1:\nL dist = \u03bb 1 1 K 2 l k =l \u00b5 l \u2212 \u00b5 k 2 \u221a D ,(2)\nwhere \u03bb 1 < 0 is a hyperparameter, K is the number of target classes, D is the dimension of feature representation in the latent space (typically the penultimate layer), and \u00b5 l is the mean vector of the features of samples with label l. Since the value of the vector norm increases as the dimension of the feature space increases, we normalize the distance by the square root of the feature dimension.\n\nAs will be discussed in Section D of Appendix, we discovered after some initial experiments that the distance loss often made the model use a very limited number of latent dimensions to increase the distance between class mean vectors, which degraded the NAS detection performance. In other words, adding only L dist to L CE caused the latent feature space to collapse into a very small number of dimensions (i.e. rank-deficient), which caused all NAS samples to be embedded near the ID samples. Therefore, we add entropy loss to increase the number of features used to represent samples.\n\nThe entropy loss is defined as:\nL entropy = \u03bb 2 D i Var(z i ) + \u03bb 3 1 D 2 i j =i C 2 ij ,(3)\nwhere \u03bb 2 > 0 and \u03bb 3 > 0 are hyperparameters, z \u2208 R D is the feature representation in the latent feature space, Var(\u00b7) is variance, and C ij is the correlation coefficient between ith and jth dimension of the feature space. Specifically, C ij is described as:\nC ij = Cov (z i , z j ) \u03c3(z i )\u03c3(z j ) ,(4)\nwhere Cov(\u00b7) and \u03c3(\u00b7) are covariance and the standard deviation, respectively.\n\nIntuitively, the first term in equation 3 encourages each latent dimension to have diverse values, therefore preventing the latent feature space from collapsing into a confined space. With the first term alone, however, all latent dimensions might learn correlated information, thus making the latent space rank-deficient. Therefore, we use the second term in equation 3 to minimize the correlation between different latent dimensions. Note that minimizing the feature correlation was also used in previous works under different contexts such as self-supervised learning (Zbontar et al., 2021).\n\n\nRESULTS AND DISCUSSION\n\nTo demonstrate the effectiveness of the suggested method, we train all classifiers using the standard cross-entropy loss and our modified loss and compare post-hoc OOD detection methods across three NAS datasets. Specifically, we present the results of the confidence-based (i.e., MSP and ODIN) and the distance-based methods (i.e., Mahalanobis distance). We also include a recently proposed OOD detection method (Sastry & Oore, 2020) which computes channel-wise correlations in CNN with the gram matrix and estimates the deviation of the test samples from the training samples to detect the OOD samples. 1 Although this method uses the distance in the channel correlation space, we expect  it to behave more similarly to the Mahalanobis detector than confidence-based methods, namely MSP and ODIN. We also compare with another recent baseline which exploits the energy score to detect OOD samples (Liu et al., 2020). As the method leverage the logit layer to calculate the energy score, and the softmax score is based on the logit values, we conjecture that the energy score demonstrates detection ability similar to the confidence-based methods.\n\nFor a fair comparison, we use only the penultimate layer for evaluating the Mahalanobis detector and Gram matrix since our method is developed based on the analysis of the NAS samples in the penultimate layer feature space. We also provide results of the ensemble version that utilize all layers in Section E of the Appendix. We describe the details of experiment setups and selecting the values for \u03bb 1 , \u03bb 2 and \u03bb 3 in the Appendix Section C, where the three values can be reasonably chosen without any explicit NAS validation datasets. We also present an ablation study to investigate the effect of different terms in the proposed loss in Section D of the Appendix. We also compare with other recent baselines that suggest other training objective for OOD detection in Section E of the Appendix.\n\nAs shown in Table 2, the in-distribution classification accuracy of the model trained with our suggested loss is comparable to that of the model trained with the cross-entropy loss. Further, we present the NAS detection performance of the baselines and our method in Table 3. As we expected above, the gram matrix achieves performance similar to the Mahalanobis detector, demonstrating the low AUROC in the UTKFace dataset. Interestingly, even though the energy score is calculated based on the logit values, which are more tempered than the softmax score, it shows NAS detection performance similar to that of MSP, which is a confidence-based method. These results show that similar to MSP, ODIN, and Mahalanobis, Gram matrix and Energy-based methods also have performance inconsistency on NAS datasets. We report the additional NAS detection performance on four other metrics, which are often used in the OOD detection community in Section F of Appendix.\n\nAlso, it is readily visible that our proposed training objective makes the Mahalanobis detector a robust NAS detection method for all NAS categories. In UTKFace, we can see the dramatic NAS detection performance increase for Mahalanobis detector. And for the other two datasets, the proposed loss does not decrease the NAS detection performance of Mahalanobis detector. Note that ODIN is more sensitive to OOD samples than Mahalanobis detector for some brightness levels in the RSNA Bone Age dataset, but it shows inconsistent performance across all three datasets. While these approaches are viable, it is not directly related with the downstream task but aims to detect features which is different from training distribution. In this paper, we mainly focus on methods that utilize the classification models to detect OOD samples since we are mainly interested in samples that affect decision-making systems.\n\n\nRELATED WORK\n\n\nModel uncertainty.\n\nA number of previous works measure model uncertainty using various methods such as Bayesian neural networks (Blundell et al., 2015), Monte Carlo dropout (Gal & Ghahramani, 2016), and deep ensembles (Lakshminarayanan et al., 2017). Note that technically, model uncertainty can be used to detect NAS samples, especially for NAS categories 1 and 3, since sampling model weights from the function space can be seen as redrawing the decision boundary, and NAS samples in categories 1 and 3 will be affected heavily by this process. Model uncertainty, however, aims to capture the uncertainty in the model weights rather than detecting OOD samples, making the two rather independent research directions.\n\n\nCONCLUSIONS\n\nTo enhance the reliability of decision-making systems, we define a new task, Natural Attributebased Shift (NAS) detection, that aims to detect the samples shifted by a natural attribute. We introduce NAS detection benchmark datasets by adjusting the natural attributes present in the existing datasets. Through extensive evaluation of existing OOD detection methods on NAS datasets, we observe inconsistent performance depending on the nature of NAS samples. Then, we analyze the inconsistency by probing the relationship between the location of NAS samples and the performance of existing OOD detection methods. Based on this observation, we suggest a simple remedy to help Mahalanobis OOD detection method to have consistent performance across all NAS categories. We hope our dataset and task inspire fellow researchers to investigate practical methods for identifying NAS, which is crucial for deploying the prediction models in real-world systems.  \n\n\nA DETAILS OF OOD DETECTION METHODS\n\nIn this section, we describe the three post-hoc and task-agnostic OOD detection methods in detail, focusing mainly on their formulation and how each method assigns an OOD score to an input sample.\n\n\nA.1 MAXIMUM OF SOFTMAX PROBABILITY (MSP)\n\nIn this method, the maximum of softmax probability is considered as confidence score (Hendrycks & Gimpel, 2017). Formally, we calculate the maximum softmax probability as follows:\nS MSP (x) = max c exp z (c) C j=1 exp z (j) ,\nwhere C is the number of target classes, c is the index of a class, and z (j) denotes j th attribute of the feature in the logit layer.\n\nA.2 ODIN ODIN (Liang et al., 2018) utilized two well established techniques, namely temperature scaling and input preprocessing to increase the difference between softmax scores of in-distribution and OOD samples. Temperature scaling was originally proposed in Hinton et al. (2015) to distill the knowledge in neural networks and was later adopted widely in classification tasks to calibrate confidence of prediction (Guo et al., 2017). In addition to temperature scaling, the input is preprocessed in order to increase the softmax score of given input by adding small perturbations which are obtained by back-propagating the gradient of the loss with respect to the input. More specifically, ODIN is computed as follows:\nS(x; T ) = max c exp z (c) /T C j=1 exp z (j) /T ,\nwhere T \u2208 R + is the temperature scaling parameter, C is the number of target classes, c is the index of class, and z (j) denotes j th attribute of the logit layer features of input x. During training, T is set to 1.\n\nFor OOD detection, the input is first pre-processed as follows:\nx = x \u2212 \u03b5 sign (\u2212\u2207 x log S(x; T )) ,\nwhere \u03b5 represents the magnitude of perturbation.\n\nNext, the network calculates the calibrated softmax score of the preprocessed input as follows:\nS ODIN (x; T ) = max c exp z (c) /T C j=1 exp z (j) /T\n, wherez (j) denotes j th attribute of the logit layer features of the preprocessed inputx .\n\nLastly, the modified softmax score is compared to a threshold value \u03b4. If the score is greater than the threshold, then the input is classified as ID sample and otherwise OOD. Originally, T , \u03b5, and \u03b4 are hyperparameters and are selected such that the false positive rate (FPR) at true positive rate (TPR) 95% is minimized on the validation OOD dataset. However, the performance saturates when T is greater than 1000 and therefore, in general, a large value of T is preferred. Following this, in this paper, we fix T = 1000 for our experiments.\n\n\nA.3 MAHALANOBIS DETECTOR\n\nTo obtain Mahalanobis distance-based OOD score (Lee et al., 2018) of a sample, we calculate the mahalanobis distance from the clusters of classes to the sample. Then, the distance from the closest class is chosen as the confidence score.\n\nSpecifically, the Mahalanobis score of an input x is defined as\nS mahala (x) = l \u03b1 l max c \u2212 f l \u03b8 (x) \u2212 \u00b5 c,l \u03a3 \u22121 l f l \u03b8 (x) \u2212 \u00b5 c,l ,\nwhere c and l are the class and layer index, respectively, f l \u03b8 is the l th layer's feature representation of an input x, \u00b5 c,l and \u03a3 l are their class mean vector and tied covariance of the training data, correspondingly.\n\nNote that ODIN and Mahalanobis Detector assume the availability of OOD validation dataset. However, some recent works (Shafaei et al., 2019;Techapanurak et al., 2020) report that this assumption limits the OOD detection generalizability since a model is biased towards an OOD validation set. In response, this paper validate the performance of OOD methods in the version that does not require to tune with OOD validation dataset. We perform the performance of ODIN as Hsu et al. (2020). We do not perform Mahalanobis Detector ensembling over on all layers with the optimal linear combination which requires explicit OOD data. Instead, we perform two version that use only the penultimate layer of hidden representation and sum uniformly over all layers. Therefore, for all our experiments, we use modified OOD detection methods that do not require the OOD validation dataset.\n\n\nB ANALYSIS ON THE TEXT DATASET\n\nIn this section, we provide a detailed analysis of the text dataset. We use the Amazon Review (He & McAuley, 2016;McAuley et al., 2015) dataset. We consider the product category \"book\" and conducted an analysis to see the impact of time on product reviews. We then performed an analysis to see the impact of time on the length of the reviews. Figure 6 presents a comparison between density plot of review length for each year from 2006 to 2014. We observe that as we move ahead in time, the length of the reviews gradually reduces. Further, Figure 7 presents the distribution of the average ratio of important words in a sequence by year. First, we train a model to classify sentiment polarity using Catboost (Prokhorenkova et al., 2017) using document term frequency vector. We then extract top-100 important words using feature importance. Lastly, we manually select important words over 100 words. We find that the distribution of ratio of important words in a sequence gradually increase over time. Based on this analysis, we figure out that the feature related with downtream task is shifted by time.  \n\n\nC IMPLEMENTATION DETAILS\n\nIn this section, we describe the training details. Followed by it, we describe the algorithm we used to select the hyperparameters in our suggested modification to the loss function. Then, we provide the links to download the datasets.\n\n\nC.1 TRAINING DETAILS AND COMPUTING INFRASTRUCTURE\n\nImage. We used ResNet18  that is pretrained with ImageNet (Deng et al., 2009) to train a gender classifier using a UTKFace NAS dataset. After the average pooling layer, we trained a fully-connected network composed of 2 hidden layers which have 128 and 2 units respectively with a relu activation. The network is trained for 100 epochs with a batch size of 64. We used stochastic gradient descent with Adam optimizer (Kingma & Ba, 2015) and set learning rate as 3e \u2212 5. For data augmentation, the technique of SimCLR (Chen et al., 2020) is used for all the experiments.\n\nText. We perform experiment with Transformer network with 4 layers. The network is trained for 10 epochs with batch size of 128. We used Adam as an optimizer and set the learning rate as 3e \u2212 6.\n\nMedical. We use a ResNet18  model, pretrained on ImageNet (Deng et al., 2009), and add two fully-connected layers containing 128 and 2 hidden units with a relu activation. We train the network to classify gender given the x-ray image. Each model is trained for 30 epochs using SGD optimizer with a learning rate of 0.01 and momentum of 0.9, using a batch size of 64.\n\nAll the experiments are conducted on a single GeForce RTX 3090 GPU with 24GB memory. The results are measured by computing mean and standard deviation across 5 trials upon randomly chosen seeds.\n\n\nC.2 HYPERPARAMETER TUNING\n\nOur suggested modification to the training loss mainly comprises of distance loss (L dist ) and a entropy loss (L entropy ). More formally, the training loss is given by:\nL total = L CE + L dist + L entropy .(5)\nThe entropy loss comprises of two terms and is defined as:\nL entropy = \u03bb 2 D i Var(z i ) + \u03bb 3 1 D 2 i j =i C 2 ij ,(6)\nwhere \u03bb 2 > 0 and \u03bb 3 > 0 are hyperparameters, z \u2208 R D is the feature representation in the latent feature space, and C ij is the correlation coefficient between ith and jth dimension of the feature space. For more details, please refer to Section 6.1.\n\nFor simplicity, in this section, we will refer the first term of entropy loss as variance loss, and the second term as correlation loss. To obtain the hyperparameters of different terms in our loss function, we explore the value of \u03bb 2 in [0.01, 0.1, 1.0, 10.] and \u03bb 3 in [0.0001, 0.001, 0.01, 0.1, 1.0].\n\nThe hyperparameter corresponding the distance loss, \u03bb 1 , is set as 0.1, and then the hyperparameters of the variance loss and the correlation loss are chosen by a simple algorithm.\n\nAlgorithm: We now describe the algorithm we used to find the hyperparameters of variance and correlation loss. First, we calculate the harmonic mean of the variance loss and correlation loss using our training dataset. Next, we select the hyperparameters with the lowest value of harmonic mean. Then, to ensure that entropy loss prevents the feature space collapsing problem, we apply the singular value decomposition (SVD) in the penultimate features and test if the sum of singular values except the two largest values is improved by the entropy loss or not. Concretely, to prove the enhancement, we compare the values with those of the model trained with the L CE + \u03bb 1 L dist . If the selected hyperparameters do not improve the values, we reject them and investigate the hyperparameters that have the next lowest harmonic mean. The hyperparameters satisfying the above steps are selected as the hyperparameters of our loss function.\n\nNote that we reject the hyperparameters if they significantly degrade the classification accuracy. In image domain, we applied the algorithm and obtained 0.1, 0.1, 0.0001 as hyperparameters of \u03bb 1 , \u03bb 2 , and \u03bb 3 , respectively. In the medical domain, we obtain the hyperparameter corresponding to \u03bb 3 as 1.0, and the other hyperparameters are the same as those of the image domain. In the text domain, 10. and 1.0 are used for the \u03bb 2 and \u03bb 3 . The distance loss is set by 0.1.\n\n\nC.3 LINKS FOR DATASETS\n\nIn our work, we use the openly available datasets, namely, UTKFace dataset 2 , Amazon Review dataset 3 , and RSNA Bone Age dataset 4 .  In this section, we conduct an ablation study to investigate the effect of each proposed term in the loss function in equation 5. Table 4 presents an ablation study to find the effect of the entropy loss and distance loss in the proposed modification to training loss. In UTKFace, when training with solely attaching L dist or L entropy to L CE , the NAS detection performance using mahalanobis OOD detection method improves compared to training with only L CE , but the performance does not monotonically increase in both cases along with the variation of Age. However, when both L dist and L entropy are used, there is a large improvement in performance with the results being monotonically increasing as we move from ID towards NAS, which implies that these two losses are mutually beneficial. In the text and medical domains, although there is a little degradation of AUROC when only L dist is added, the performance is improved by using L entropy in the training objective.\n\n\nD ABLATION STUDIES ON DISTANCE AND ENTROPY LOSS\n\nFurther, to analyze the impact of each loss term to the feature-level representations, we perform a singular value decomposition (SVD) in the penultimate layer of ResNet18 trained on the UTKFace dataset, similar to Verma et al. (2019). When trained with L CE only, the sum of the two largest singular values was 617.47, while the sum of the remaining singular values was 785.54. When trained with L CE + L dist , the sum of the two largest singular values increased (4481.88), while the sum of the remaining singular values decreased (371.86).\n\nAs discussed in Section 6.1 in the main paper, this indicates that the addition of the distance loss is likely to collapse the latent feature space (i.e. rank-deficient), thus possibly decreasing the model's sensitivity to NAS samples. For example, if a model is trained to classify apples and bananas based only on the color, it will not be able to tell that a firetruck is a NAS sample. When we train the classifier with L entropy added, the problem is effectively alleviated.   In this section, we investigate NAS detection performance using additional baselines, including recent work, and compare them with those of the model trained with our modified loss on three NAS datasets. Firstly, as we mentioned in the main paper, we evaluate the model trained with crossentropy loss with ensembled version of Mahalanobis detector (Lee et al., 2018) and the method that utilizes the gram matrix in OOD detection (Sastry & Oore, 2020).\n\nWe also consider the other recent baselines for the comparison. Recently, Sehwag et al. (2021) propose a unified framework to leverage self-supervised learning for OOD detection. We set the method as our baseline and denote this baseline as SSD + . The experiment on SSD + requires a data augmentation strategy. In the Image domain, we augment the training dataset as Chen et al. (2020). We construct an augmented training dataset using a back-translation model for text experiments as (Fang et al., 2020;Gunel et al., 2020). We train SSD + for 100 epochs for UTKFace and RSNA Bone age dataset and 50 epochs for Amazon Review dataset.\n\nSome recent methods (Hsu et al., 2020;Techapanurak et al., 2020) exploit the cosine similarity to detect OOD samples. Hsu et al. (2020) suggest to decompose confidence scores and to modify input preprocessing method in ODIN (Liang et al., 2018). For OOD detection, they calculate the cosine similarity between the features from the penultimate layer and the class specific weights and use the maximum value as an OOD score. Techapanurak et al. (2020) also propose to compute cosine similarity between the class weight vector and the penultimate layer feature of each input to obtain OOD scores. Specifically, they train the model with cross entropy loss as standard classification model, but logit value is set by the scaled cosine similarity value. After training, the cosine similarity values between the penultimate layer feature of the input and the weight vector for each class are calculated. The maximum cosine similarity value is used as the OOD score for the samples. We name the methods as GODIN and Scaled Cosine in Table 6, respectively.\n\nNAS detection performance of our method and other baselines are provided in Table 6. In UTKFace dataset, GODIN, Scaled Cosine, and ensembled version of Mahalanobis detector have relatively low detection performance than the other baselines. Although Gram matrix shows reasonable detection performance in UTKFace and RSNA Bone Age NAS groups, this method is hard to applied to the Amazon Book Review dataset because the notion of gram matrix is vague in the text domain. In three NAS categories, SSD+ has the most robust detection performance among the five baselines, however, our simple remedy in training loss help Mahalanobis detector to demonstrate improved performance particularly under the UTKFace NAS dataset. In conclusion, when compared with the five more recent baselines, our method shows robust and high detection performance in all the three NAS datasets, regardless of the domains.\n\n\nF QUANTITATIVE RESULTS WITH OTHER METRICS\n\nWe report the NAS detection performance of baselines and our method based on four other metrics: Detection Accuracy, AUPR-In, AUPR-Out, and TNR@95%TPR, which are often used in the OOD detection community. We present the detection performance on NAS datasets in three domain measured by Detection Accuracy, AUPR-In, AUPR-Out, and TNR@95%TPR in Tables 7,8,9, and 10 respectively. The results indicate that our simple modification in the training objective improves the performance of the Mahalanobis detector, thus making it a robust NAS detection method for the three NAS categories presented in the paper. Specifically, in UTKFace, we observe that using our suggested training loss results in a significant increase in NAS detection performance using the Mahalanobis detector. At the same time, the suggested method effectively detects the NAS samples in the other two datasets. In summary, based on the results obtained using five different metrics (AU-ROC, Detection Accuracy, AUPR-In, AUPR-Out, and TNR@95%TPR), our suggested modification makes the Mahalanobis detector a general NAS detection method for all three NAS categories.  \n\nFigure 3 :\n3PCA visualization to demonstrate the movement of NAS samples as we vary the age, year and brightness in UTKFace, Amazon Review and RSNA Bone Age dataset respectively.\n\nFigure 4 :\n4Impact on prediction confidence on varying age, year, and brightness in UTKFace, Amazon Review, and RSNA Bone Age dataset, respectively.\n\n\nevaluation on distribution shifts. Recent works have leveraged the distribution shifts for OOD detection and uncertainty estimation. Rabanser et al. (2019) primarily focus on detecting the entire shifted distribution, not on detecting a single shifted sample. Ovadia et al. (2019) quantify the predictive uncertainty and investigate the quality of the calibration under the dataset shift. Techapanurak & Okatani (2021) aims to predict the classification accuracy of a shifted distribution by utilizing the average OOD score of the distribution. Hsu et al. (2020) also consider the distribution shift with preserved label space. However, Hsu et al. (2020) focus on the shifted distribution from different domain (e.g., real images vs. sketches ). To the best of our knowledge, none of these works conduct an in-depth analysis on the performance of existing OOD detection methods on samples shifted based on natural attributes.OOD detection methods. Post-hoc OOD detection methods(Hendrycks & Gimpel, 2017; Liang  et al., 2018; Lee et al., 2018; Sastry & Oore, 2020) that utilize the classification models to obtain the OOD scores have achieved remarkable performance. All of these works aim to detect shifted sample which might affect decision making system in terms of confidence and hidden representation. Other OOD detection approach is to train the generative model(Ren et al., 2019;Choi et al., 2018;  Mahmood et al., 2020)  on the training distribution and estimate the density of OOD samples in test time.\n\nFigure 6 :\n6The density plot of sequence length per year. The x-axis represents the length with log scaling.\n\nFigure 7 :\n7The density plot of the ratio of important words per sequence.\n\nTable 1 :\n1Comparison between different categories of NAS based on the location of samples in the features space and performance of confidence-based and distance-based methods.\n\nTable 3 :\n3NAS detection performance on distributional shifts in three datasets measured by AUROC.\n\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proc. the International Conference on Learning Representations (ICLR), 2017. Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. In Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2020. Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR), 2017. Ahsan Mahmood, Junier Oliva, and Martin Andreas Styner. Multiscale score matching for out-ofdistribution detection. In Proc. the International Conference on Learning Representations (ICLR), 2020. Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2015. Stephan Rabanser, Stephan G\u00fcnnemann, and Zachary C Lipton. Failing loudly: an empirical study of methods for detecting dataset shift. In Proc. the Advances in Neural Information Processing Systems (NeurIPS), 2019. Engkarat Techapanurak, Masanori Suganuma, and Takayuki Okatani. Hyperparameter-free out-ofdistribution detection using cosine similarity. In Proc. of the Asian Conference on Computer Vision (ACCV), 2020. Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In Proc. the International Conference on Machine Learning (ICML), 2019.ArXiv, abs/1503.02531, 2015. \n\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-\ndistribution image without learning from out-of-distribution data. In Proc. of the IEEE conference \non computer vision and pattern recognition (CVPR), 2020. \n\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. the \nInternational Conference on Learning Representations (ICLR), 2015. \n\nBalaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-\ntive uncertainty estimation using deep ensembles. In Proc. the Advances in Neural Information \nProcessing Systems (NeurIPS), 2017. \n\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting \nout-of-distribution samples and adversarial attacks. In Proc. the Advances in Neural Information \nProcessing Systems (NeurIPS), 2018. \n\nShiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability of out-of-distribution \nimage detection in neural networks. In Proc. the International Conference on Learning Repre-\nsentations (ICLR), 2018. \n\nYan Luo, Yongkang Wong, Mohan Kankanhalli, and Qi Zhao. G-softmax: Improving intraclass \ncompactness and interclass separability of features. IEEE transactions on neural networks and \nlearning systems, 31(2):685-699, 2020. \n\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word repre-\nsentations in vector space. In Proc. the International Conference on Learning Representations \n(ICLR), 2013. \n\nAnh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confi-\ndence predictions for unrecognizable images. In Proc. of the IEEE conference on computer vision \nand pattern recognition (CVPR), 2015. \n\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon, \nBalaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating \npredictive uncertainty under dataset shift. In Proc. the Advances in Neural Information Processing \nSystems (NeurIPS), 2019. \n\nLiudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey \nGulin. Catboost: unbiased boosting with categorical features. arXiv preprint arXiv:1706.09516, \n2017. \nJie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark A DePristo, Joshua V Dillon, \nand Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. arXiv preprint \narXiv:1906.02845, 2019. \n\nChandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with gram \nmatrices. In Proc. the International Conference on Machine Learning (ICML), 2020. \n\nVikash Sehwag, Mung Chiang, and Prateek Mittal. {SSD}: A unified framework for self-supervised \noutlier detection. In Proc. the International Conference on Learning Representations (ICLR), \n2021. \n\nSeonguk Seo, Yumin Suh, Dongwan Kim, Geeho Kim, Jongwoo Han, and Bohyung Han. Learning \nto optimize domain specific normalization for domain generalization. In Proc. of the European \nConference on Computer Vision (ECCV), 2020. \n\nAlireza Shafaei, Mark Schmidt, and James J. Little. A less biased evaluation of out-of-distribution \nsample detectors. 2019. \n\nEngkarat Techapanurak and Takayuki Okatani. Practical evaluation of out-of-distribution detection \nmethods for image classification. arXiv preprint arXiv:2101.02447, 2021. \n\nJure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St\u00e9phane Deny. Barlow twins: Self-supervised \nlearning via redundancy reduction. arXiv preprint arXiv:2103.03230, 2021. \n\nZhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial \nautoencoder. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR), \n2017. \n\n\nTable of Contents\nofMaximum of Softmax Probability (MSP) . . . . . . . . . . . . . . . . . . . . . 13 A.2 ODIN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 A.3 Mahalanobis Detector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 Training Details and Computing Infrastructure . . . . . . . . . . . . . . . . . . 15 C.2 Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 C.3 Links for Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16A Details of OOD detection methods \n13 \nA.1 B Analysis on the Text Dataset \n14 \n\nC Implementation Details \n15 \nC.1 D Ablation Studies on Distance and Entropy Loss \n17 \n\nE Comparison with Additional Baselines \n19 \n\nF Quantitative Results with Other Metrics \n20 \n\n\n\n\nCE + L dist L CE + L entropy L CE + L dist + L entropyDistribution \nAge \nL CE \nL UTKFace \n\nID \n26 \n-\n-\n-\n-\n\nNAS \n\n25 \n50.0\u00b11.2 \n50.3\u00b11.0 \n56.8\u00b12.4 \n50.1\u00b11.7 \n24 \n50.6\u00b12.8 \n50.8\u00b10.8 \n54.6\u00b11.7 \n53.3\u00b11.3 \n23 \n47.9\u00b12.4 \n48.8\u00b11.2 \n58.1\u00b13.3 \n52.7\u00b11.2 \n22 \n51.2\u00b11.7 \n53.4\u00b11.0 \n61.5\u00b12.8 \n56.5\u00b11.3 \n21 \n49.5\u00b12.4 \n51.7\u00b12.3 \n56.0\u00b13.6 \n55.3\u00b11.0 \n19-20 \n52.2\u00b11.5 \n54.8\u00b12.0 \n56.6\u00b13.1 \n57.9\u00b11.2 \n17-18 \n52.9\u00b11.6 \n58.5\u00b12.4 \n62.0\u00b14.5 \n61.0\u00b11.1 \n15-16 \n59.9\u00b12.9 \n70.0\u00b13.4 \n66.6\u00b14.2 \n70.7\u00b10.7 \n12-14 \n57.7\u00b12.6 \n67.4\u00b15.5 \n65.2\u00b15.6 \n75.3\u00b10.9 \n9-11 \n56.0\u00b13.8 \n66.5\u00b17.6 \n67.7\u00b15.8 \n80.5\u00b10.9 \n7-8 \n53.3\u00b14.4 \n64.1\u00b17.9 \n69.3\u00b17.3 \n82.1\u00b11.9 \n5-6 \n53.2\u00b12.4 \n63.0\u00b18.0 \n64.3\u00b15.1 \n83.5\u00b11.4 \n3-4 \n56.4\u00b13.2 \n64.3\u00b110.1 \n65.8\u00b15.1 \n88.6\u00b11.0 \n2 \n53.3\u00b12.8 \n58.6\u00b110.1 \n57.7\u00b14.2 \n88.3\u00b11.6 \n1 \n51.2\u00b14.4 \n58.1\u00b110.2 \n55.6\u00b15.7 \n90.3\u00b12.0 \n\nDistribution \nYear \nL CE \nL CE + L dist L CE + L entropy L CE + L dist + L entropy \n\nAmazon Review \n\nID \n2005 \n-\n-\n-\n-\n\nNAS \n\n2006 \n51.5\u00b10.1 \n51.5\u00b10.1 \n51.4\u00b10.1 \n51.4\u00b10.1 \n2007 \n56.8\u00b10.2 \n56.3\u00b10.1 \n56.6\u00b10.1 \n56.5\u00b10.1 \n2008 \n55.2\u00b10.3 \n54.8\u00b10.1 \n54.9\u00b10.1 \n54.9\u00b10.1 \n2009 \n53.7\u00b10.2 \n53.6\u00b10.1 \n53.6\u00b10.1 \n53.6\u00b10.1 \n2010 \n54.0\u00b10.5 \n53.6\u00b10.1 \n53.6\u00b10.1 \n53.6\u00b10.1 \n2011 \n55.6\u00b10.7 \n54.8\u00b10.1 \n54.9\u00b10.0 \n54.9\u00b10.0 \n2012 \n63.3\u00b10.8 \n62.5\u00b10.1 \n62.6\u00b10.1 \n62.6\u00b10.1 \n2013 \n75.5\u00b10.5 \n74.7\u00b10.1 \n75.1\u00b10.1 \n75.1\u00b10.0 \n2014 \n76.8\u00b10.2 \n76.3\u00b10.1 \n76.8\u00b10.1 \n76.7\u00b10.1 \n\nDistribution Brightness \nL CE \nL CE + L dist L CE + L entropy L CE + L dist + L entropy \n\nRSNA Bone Age \n\nNAS \n\n0.0 \n99.9\u00b10.2 \n99.9\u00b10.2 \n100.0\u00b10.0 \n100.0\u00b10.0 \n0.2 \n93.7\u00b11.8 \n94.0\u00b12.3 \n96.9\u00b11.2 \n98.0\u00b11.0 \n0.4 \n79.5\u00b14.5 \n74.4\u00b16.3 \n88.4\u00b13.6 \n89.8\u00b12.2 \n0.6 \n64.5\u00b15.5 \n60.9\u00b14.1 \n74.5\u00b13.7 \n74.6\u00b12.5 \n0.8 \n53.3\u00b12.4 \n51.9\u00b11.6 \n56.7\u00b12.3 \n56.3\u00b11.8 \nID \n1.0 \n-\n-\n-\n-\n\nNAS \n\n1.2 \n58.5\u00b11.9 \n57.8\u00b12.8 \n57.4\u00b11.8 \n55.0\u00b12.3 \n1.4 \n69.0\u00b13.1 \n67.1\u00b14.4 \n68.6\u00b11.7 \n65.1\u00b13.1 \n1.6 \n76.9\u00b14.0 \n75.4\u00b15.0 \n78.5\u00b11.9 \n75.5\u00b13.9 \n1.8 \n82.1\u00b15.1 \n81.4\u00b14.8 \n84.3\u00b11.9 \n82.3\u00b14.3 \n2.0 \n85.8\u00b15.5 \n86.3\u00b13.8 \n88.2\u00b11.6 \n87.0\u00b14.0 \n2.5 \n91.9\u00b15.0 \n93.3\u00b13.1 \n94.0\u00b11.3 \n93.7\u00b12.9 \n3.0 \n95.4\u00b13.3 \n96.8\u00b11.6 \n96.3\u00b11.0 \n96.6\u00b11.8 \n3.5 \n96.8\u00b12.7 \n98.4\u00b11.0 \n97.4\u00b10.8 \n97.8\u00b11.3 \n4.0 \n97.7\u00b12.1 \n99.0\u00b10.7 \n97.9\u00b10.8 \n98.4\u00b10.9 \n4.5 \n98.2\u00b11.6 \n99.2\u00b10.6 \n98.2\u00b10.8 \n98.7\u00b10.8 \n\n\n\nTable 4 :\n4NAS detection performance on three NAS datasets measured with Mahalanobis OOD detection method using AUROC.\n\nTable 5 :\n5Classification accuracy of the in-distribution in three NAS datasets.Distribution \nAge \nBaselines \nOurs \n\nMahalanobis* Gram matrix* GODIN Scaled Cosine \nSSD+ \nMahalanobis Mahalanobis* \n\nUTKFace \n\nID \n26 \n-\n-\n-\n-\n-\n-\n-\n\nNAS \n\n25 \n50.3\u00b10.9 \n49.9\u00b10.3 \n50.5\u00b11.8 \n51.9\u00b12.9 \n48.7\u00b11.0 \n50.1\u00b11.7 \n49.8\u00b11.0 \n24 \n52.4\u00b10.8 \n50.9\u00b10.5 \n48.9\u00b11.8 \n50.9\u00b13.0 \n52.8\u00b11.2 \n53.3\u00b11.3 \n54.4\u00b11.0 \n23 \n50.2\u00b11.2 \n51.7\u00b10.6 \n48.1\u00b11.3 \n49.3\u00b12.2 \n49.3\u00b11.0 \n52.7\u00b11.2 \n54.3\u00b10.5 \n22 \n55.1\u00b11.2 \n56.6\u00b10.5 \n49.2\u00b11.9 \n53.6\u00b12.2 \n51.2\u00b10.7 \n56.5\u00b11.3 \n58.2\u00b10.7 \n21 \n51.8\u00b11.3 \n51.3\u00b10.9 \n51.8\u00b11.3 \n51.9\u00b13.1 \n50.3\u00b10.5 \n55.3\u00b11.0 \n56.8\u00b10.8 \n19-20 \n55.7\u00b11.0 \n56.6\u00b10.3 \n47.9\u00b11.2 \n54.8\u00b11.8 \n56.7\u00b10.7 \n57.9\u00b11.2 \n58.8\u00b10.3 \n17-18 \n56.9\u00b11.2 \n61.3\u00b10.7 \n46.6\u00b12.0 \n58.1\u00b12.7 \n58.2\u00b11.0 \n61.0\u00b11.1 \n61.8\u00b10.6 \n15-16 \n65.7\u00b11.2 \n63.3\u00b10.5 \n45.9\u00b11.9 \n66.5\u00b12.5 \n67.1\u00b11.8 \n70.7\u00b10.7 \n70.7\u00b11.5 \n12-14 \n61.6\u00b11.1 \n67.7\u00b10.7 \n45.1\u00b12.8 \n69.5\u00b14.3 \n71.5\u00b11.4 \n75.3\u00b10.9 \n75.9\u00b11.9 \n9-11 \n63.7\u00b11.0 \n70.8\u00b11.2 \n43.8\u00b12.2 \n71.3\u00b15.7 \n73.4\u00b11.6 \n80.5\u00b10.9 \n80.8\u00b11.4 \n7-8 \n62.8\u00b11.4 \n73.1\u00b11.0 \n45.0\u00b11.6 \n69.4\u00b17.3 \n72.5\u00b11.4 \n82.1\u00b11.9 \n82.6\u00b11.3 \n5-6 \n61.8\u00b11.3 \n70.6\u00b10.5 \n44.1\u00b13.0 \n68.9\u00b15.9 \n74.7\u00b11.4 \n83.5\u00b11.4 \n83.0\u00b11.8 \n3-4 \n68.4\u00b11.0 \n75.3\u00b10.6 \n44.3\u00b11.7 \n68.1\u00b19.4 \n80.8\u00b11.3 \n88.6\u00b11.0 \n89.4\u00b11.2 \n2 \n63.5\u00b11.0 \n74.4\u00b11.1 \n44.2\u00b11.9 \n65.4\u00b110.9 \n79.5\u00b11.2 \n88.3\u00b11.6 \n88.9\u00b11.3 \n1 \n65.8\u00b11.9 \n80.3\u00b10.9 \n43.3\u00b12.7 \n63.2\u00b110.4 \n79.7\u00b11.8 \n90.3\u00b12.0 \n91.5\u00b11.1 \n\nDistribution \nYear \nBaselines \nOurs \n\nMahalanobis* Gram matrix* GODIN Scaled Cosine \nSSD+ \nMahalanobis Mahalanobis* \n\nAmazon Review \n\nID \n2005 \n-\n-\n-\n-\n-\n-\n-\n\nNAS \n\n2006 \n51.4\u00b10.0 \n-\n50.5\u00b10.1 \n49.4\u00b10.5 \n51.3\u00b10.3 \n51.4\u00b10.1 \n51.4\u00b10.0 \n2007 \n56.6\u00b10.1 \n-\n50.9\u00b10.5 \n49.2\u00b10.6 \n55.4\u00b11.7 \n56.5\u00b10.1 \n56.6\u00b10.0 \n2008 \n54.9\u00b10.0 \n-\n50.8\u00b10.1 \n49.4\u00b10.5 \n53.8\u00b10.2 \n54.9\u00b10.1 \n54.8\u00b10.1 \n2009 \n53.6\u00b10.1 \n-\n50.9\u00b10.3 \n49.3\u00b10.6 \n52.5\u00b11.2 \n53.6\u00b10.1 \n53.6\u00b10.1 \n2010 \n53.6\u00b10.1 \n-\n50.5\u00b10.3 \n48.8\u00b10.6 \n52.6\u00b11.3 \n53.6\u00b10.1 \n53.6\u00b10.1 \n2011 \n55.0\u00b10.1 \n-\n50.5\u00b10.5 \n47.4\u00b10.7 \n53.0\u00b11.5 \n54.9\u00b10.0 \n54.8\u00b10.0 \n2012 \n62.4\u00b10.1 \n-\n50.8\u00b10.6 \n45.9\u00b11.3 \n58.2\u00b12.6 \n62.6\u00b10.1 \n62.5\u00b10.1 \n2013 \n75.0\u00b10.1 \n-\n51.2\u00b11.1 \n45.2\u00b11.8 \n67.4\u00b17.7 \n75.1\u00b10.0 \n75.0\u00b10.1 \n2014 \n76.5\u00b10.1 \n-\n51.1\u00b10.6 \n45.8\u00b11.6 \n69.1\u00b17.2 \n76.7\u00b10.1 \n76.7\u00b10.1 \n\nDistribution Brightness \nBaselines \nOurs \n\nMahalanobis* Gram matrix* GODIN Scaled Cosine \nSSD+ \nMahalanobis Mahalanobis* \n\nRSNA Bone Age \n\nNAS \n\n0.0 \n100.0\u00b10.1 \n100.0\u00b10.0 \n97.8\u00b10.6 \n97.5\u00b12.1 \n100.0\u00b10.0 \n100.0\u00b10.0 \n100.0\u00b10.0 \n0.2 \n93.9\u00b12.5 \n100.0\u00b10.0 \n48.9\u00b11.0 \n72.9\u00b19.3 \n96.1\u00b15.9 \n98.0\u00b11.0 \n89.3\u00b112.3 \n0.4 \n77.8\u00b19.6 \n99.0\u00b10.3 \n51.8\u00b11.2 \n59.9\u00b13.4 \n92.1\u00b17.9 \n89.8\u00b12.2 \n67.2\u00b118.2 \n0.6 \n65.2\u00b18.9 \n61.9\u00b13.5 \n50.7\u00b11.6 \n54.1\u00b13.2 \n81.9\u00b17.4 \n74.6\u00b12.5 \n58.4\u00b112.9 \n0.8 \n54.2\u00b13.6 \n37.9\u00b11.5 \n51.1\u00b11.3 \n50.8\u00b12.2 \n62.3\u00b13.6 \n56.3\u00b11.8 \n52.0\u00b16.1 \nID \n1.0 \n-\n-\n-\n-\n-\n-\n-\n\nNAS \n\n1.2 \n65.8\u00b11.8 \n58.6\u00b10.9 \n48.9\u00b11.0 \n53.5\u00b12.6 \n66.6\u00b13.2 \n55.0\u00b12.4 \n61.2\u00b11.5 \n1.4 \n79.3\u00b11.9 \n63.5\u00b11.0 \n49.2\u00b10.6 \n58.4\u00b14.5 \n83.8\u00b15.1 \n65.1\u00b13.1 \n73.4\u00b12.6 \n1.6 \n88.4\u00b12.4 \n69.1\u00b11.0 \n49.1\u00b11.4 \n62.9\u00b15.6 \n92.7\u00b15.4 \n75.5\u00b13.9 \n83.1\u00b15.1 \n1.8 \n93.5\u00b11.9 \n74.9\u00b11.1 \n49.7\u00b11.4 \n66.6\u00b17.3 \n96.2\u00b14.4 \n82.3\u00b14.3 \n89.4\u00b16.0 \n2.0 \n96.6\u00b11.1 \n80.8\u00b11.6 \n50.1\u00b10.7 \n69\u00b110.2 \n97.6\u00b13.2 \n87.0\u00b14.0 \n93.2\u00b15.4 \n2.5 \n99.5\u00b10.2 \n91.3\u00b11.4 \n48.2\u00b12.0 \n75.4\u00b113.4 \n99.3\u00b11.1 \n93.7\u00b12.9 \n98.1\u00b12.6 \n3.0 \n99.9\u00b10.1 \n95.7\u00b11.0 \n49.1\u00b12.7 \n83.0\u00b19.4 \n99.8\u00b10.5 \n96.6\u00b11.8 \n99.3\u00b11.0 \n3.5 \n99.9\u00b10.1 \n97.6\u00b10.5 \n50.1\u00b13.0 \n87.3\u00b15.2 \n99.9\u00b10.2 \n97.8\u00b11.3 \n99.7\u00b10.3 \n4.0 \n100.0\u00b10.1 \n98.4\u00b10.4 \n51.2\u00b13.5 \n89.3\u00b12.7 \n100.0\u00b10.1 \n98.4\u00b10.9 \n99.9\u00b10.1 \n4.5 \n100.0\u00b10.0 \n98.9\u00b10.2 \n51.6\u00b13.3 \n90.2\u00b12.5 \n100.0\u00b10.1 \n98.7\u00b10.8 \n99.9\u00b10.1 \n\n\n\nTable 6 :\n6NAS detection performance on distributional shifts in three NAS datasets measured by AUROC. Mahalanobis and Mahalanobis* indicate using penultimate layer only or ensembling all layer respectively. Gram matrix* also denotes the ensembled version.\n\nTable 8 :\n8NAS detection performance on three NAS datasets measured by detection accuracy.\nHowever, in the text domain, the notion of gram matrix is vague, and hence we do not compare against this baseline.\nhttps://susanqq.github.io/UTKFace/ 3 https://jmcauley.ucsd.edu/data/amazon/ 4 https://www.rsna.org/education/ai-resources-and-training/ ai-image-challenge/rsna-pediatric-bone-age-challenge-2017\n\nConcrete problems in ai safety. Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Man\u00e9, arXiv:1606.06565arXiv preprintDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Con- crete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.\n\nWeight uncertainty in neural network. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra, Proc. the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In Proc. the International Conference on Machine Learning (ICML), 2015.\n\nEnd to end learning for self-driving cars. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, Karol Zieba, Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. End to end learning for self-driving cars, 2016.\n\nDomain generalization by solving jigsaw puzzles. M Fabio, Antonio D&apos; Carlucci, Silvia Innocente, Barbara Bucci, Tatiana Caputo, Tommasi, Proc. of the IEEE conference on computer vision and pattern recognition (CVPR). of the IEEE conference on computer vision and pattern recognition (CVPR)Fabio M Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Do- main generalization by solving jigsaw puzzles. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR), 2019.\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, Proc. the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2020Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proc. the International Conference on Learning Representations (ICLR), 2020.\n\nHyunsun Choi, Eric Jang, Alexander A Alemi, arXiv:1810.01392Waic, but why? generative ensembles for robust anomaly detection. arXiv preprintHyunsun Choi, Eric Jang, and Alexander A Alemi. Waic, but why? generative ensembles for robust anomaly detection. arXiv preprint arXiv:1810.01392, 2018.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Proc. of the IEEE conference on computer vision and pattern recognition (CVPR). of the IEEE conference on computer vision and pattern recognition (CVPR)Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR), 2009.\n\nDermatologist-level classification of skin cancer with deep neural networks. Andre Esteva, Brett Kuprel, Roberto Novoa, Justin Ko, Swetter Susan, Helen Balu, M , Sebastian Thrun, Andre Esteva, Brett Kuprel, Roberto Novoa, Justin Ko, Swetter Susan, Helen Balu, M, and Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks. 2016.\n\nCert: Contrastive self-supervised learning for language understanding. Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, Pengtao Xie, arXiv:2005.12766arXiv preprintHongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. Cert: Contrastive self-supervised learning for language understanding. arXiv preprint arXiv:2005.12766, 2020.\n\nDropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, Proc. the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Proc. the International Conference on Machine Learning (ICML), 2016.\n\nIn search of lost domain generalization. Ishaan Gulrajani, David Lopez-Paz, Proc. the International Conference on Learning Representations (ICLR. the International Conference on Learning Representations (ICLR2020Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In Proc. the Inter- national Conference on Learning Representations (ICLR), 2020.\n\nSupervised contrastive learning for pre-trained language model fine-tuning. Beliz Gunel, Jingfei Du, Alexis Conneau, Ves Stoyanov, arXiv:2011.01403arXiv preprintBeliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised contrastive learning for pre-trained language model fine-tuning. arXiv preprint arXiv:2011.01403, 2020.\n\nOn calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, Proc. the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In Proc. the International Conference on Machine Learning (ICML), 2017.\n\nThe rsna pediatric bone age machine learning challenge. S Safwan, Luciano M Halabi, Jayashree Prevedello, Artem B Kalpathy-Cramer, Alexander Mamonov, Mark Bilbily, Ian Cicero, Lucas Ara\u00fajo Pan, Rafael Teixeira Pereira, Nitamar Sousa, Felipe Campos Abdala, Hans H Kitamura, Leon Thodberg, George Chen, Katherine Shih, Marc D Andriole, Bradley J Kohli, Adam E Erickson, Flanders, 10.1148/radiol.2018180736Radiology. 2903Safwan S. Halabi, Luciano M. Prevedello, Jayashree Kalpathy-Cramer, Artem B. Mamonov, Alexander Bilbily, Mark Cicero, Ian Pan, Lucas Ara\u00fajo Pereira, Rafael Teixeira Sousa, Nita- mar Abdala, Felipe Campos Kitamura, Hans H. Thodberg, Leon Chen, George Shih, Kather- ine Andriole, Marc D. Kohli, Bradley J. Erickson, and Adam E. Flanders. The rsna pediatric bone age machine learning challenge. Radiology, 290(3):498-503, 2019. ISSN 0033-8419. doi: 10.1148/radiol.2018180736.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proc. of the IEEE conference on computer vision and pattern recognition (CVPR). of the IEEE conference on computer vision and pattern recognition (CVPR)Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR), 2016.\n\nUps and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. Ruining He, Julian Mcauley, Proc. the International Conference on World Wide Web (WWW). the International Conference on World Wide Web (WWW)Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In Proc. the International Conference on World Wide Web (WWW), 2016.\n\nTable 10: NAS detection performance on three NAS datasets measured by AUPR-Out. Table 10: NAS detection performance on three NAS datasets measured by AUPR-Out.\n", "annotations": {"author": "[{\"end\":126,\"start\":43},{\"end\":180,\"start\":127},{\"end\":258,\"start\":181},{\"end\":339,\"start\":259},{\"end\":392,\"start\":340},{\"end\":465,\"start\":393},{\"end\":543,\"start\":466}]", "publisher": null, "author_last_name": "[{\"end\":57,\"start\":53},{\"end\":137,\"start\":133},{\"end\":192,\"start\":189},{\"end\":271,\"start\":267},{\"end\":349,\"start\":347},{\"end\":404,\"start\":400},{\"end\":477,\"start\":473}]", "author_first_name": "[{\"end\":52,\"start\":43},{\"end\":132,\"start\":127},{\"end\":188,\"start\":181},{\"end\":266,\"start\":259},{\"end\":346,\"start\":340},{\"end\":399,\"start\":393},{\"end\":472,\"start\":466}]", "author_affiliation": "[{\"end\":125,\"start\":85},{\"end\":179,\"start\":139},{\"end\":257,\"start\":217},{\"end\":338,\"start\":298},{\"end\":391,\"start\":351},{\"end\":464,\"start\":424},{\"end\":542,\"start\":502}]", "title": "[{\"end\":40,\"start\":1},{\"end\":583,\"start\":544}]", "venue": null, "abstract": "[{\"end\":1877,\"start\":585}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2061,\"start\":2038},{\"end\":2082,\"start\":2061},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2102,\"start\":2082},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2287,\"start\":2266},{\"end\":2312,\"start\":2287},{\"end\":2332,\"start\":2312},{\"end\":3542,\"start\":3516},{\"end\":3561,\"start\":3542},{\"end\":3578,\"start\":3561},{\"end\":3598,\"start\":3578},{\"end\":4466,\"start\":4448},{\"end\":4494,\"start\":4466},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4516,\"start\":4494},{\"end\":6172,\"start\":6154},{\"end\":9780,\"start\":9760},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10453,\"start\":10433},{\"end\":10474,\"start\":10453},{\"end\":12420,\"start\":12394},{\"end\":12450,\"start\":12425},{\"end\":13204,\"start\":13187},{\"end\":18730,\"start\":18712},{\"end\":18747,\"start\":18730},{\"end\":26228,\"start\":26210},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29297,\"start\":29274},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":29343,\"start\":29319},{\"end\":29395,\"start\":29364},{\"end\":31223,\"start\":31197},{\"end\":31509,\"start\":31484},{\"end\":31756,\"start\":31736},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31910,\"start\":31892},{\"end\":34178,\"start\":34156},{\"end\":34204,\"start\":34178},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35062,\"start\":35042},{\"end\":35083,\"start\":35062},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36450,\"start\":36431},{\"end\":36809,\"start\":36790},{\"end\":36909,\"start\":36890},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37217,\"start\":37198},{\"end\":41651,\"start\":41632},{\"end\":42809,\"start\":42791},{\"end\":42990,\"start\":42960},{\"end\":43282,\"start\":43264},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43401,\"start\":43382},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":43420,\"start\":43401},{\"end\":43570,\"start\":43552},{\"end\":43596,\"start\":43570},{\"end\":43667,\"start\":43650},{\"end\":43776,\"start\":43751},{\"end\":43982,\"start\":43956},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":48397,\"start\":48379}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":46840,\"start\":46661},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46990,\"start\":46841},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48504,\"start\":46991},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48614,\"start\":48505},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48690,\"start\":48615},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":48868,\"start\":48691},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":48968,\"start\":48869},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":54585,\"start\":48969},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":55396,\"start\":54586},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":57547,\"start\":55397},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":57667,\"start\":57548},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":61125,\"start\":57668},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":61383,\"start\":61126},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":61475,\"start\":61384}]", "paragraph": "[{\"end\":3406,\"start\":1893},{\"end\":4345,\"start\":3408},{\"end\":5036,\"start\":4347},{\"end\":5554,\"start\":5038},{\"end\":6411,\"start\":5556},{\"end\":6472,\"start\":6413},{\"end\":6752,\"start\":6474},{\"end\":7003,\"start\":6754},{\"end\":7216,\"start\":7005},{\"end\":7392,\"start\":7218},{\"end\":8260,\"start\":7436},{\"end\":8421,\"start\":8262},{\"end\":8939,\"start\":8450},{\"end\":9060,\"start\":8941},{\"end\":9722,\"start\":9088},{\"end\":10355,\"start\":9733},{\"end\":11244,\"start\":10394},{\"end\":12024,\"start\":11257},{\"end\":12231,\"start\":12071},{\"end\":13351,\"start\":12257},{\"end\":13587,\"start\":13379},{\"end\":14417,\"start\":13589},{\"end\":15267,\"start\":14430},{\"end\":15677,\"start\":15320},{\"end\":16116,\"start\":15721},{\"end\":16570,\"start\":16127},{\"end\":17260,\"start\":16572},{\"end\":18167,\"start\":17333},{\"end\":18698,\"start\":18169},{\"end\":20552,\"start\":18700},{\"end\":21583,\"start\":20575},{\"end\":21976,\"start\":21635},{\"end\":22734,\"start\":21978},{\"end\":22777,\"start\":22736},{\"end\":23170,\"start\":22819},{\"end\":23620,\"start\":23218},{\"end\":24210,\"start\":23622},{\"end\":24243,\"start\":24212},{\"end\":24566,\"start\":24305},{\"end\":24689,\"start\":24611},{\"end\":25285,\"start\":24691},{\"end\":26459,\"start\":25312},{\"end\":27259,\"start\":26461},{\"end\":28217,\"start\":27261},{\"end\":29128,\"start\":28219},{\"end\":29863,\"start\":29166},{\"end\":30832,\"start\":29879},{\"end\":31067,\"start\":30871},{\"end\":31291,\"start\":31112},{\"end\":31473,\"start\":31338},{\"end\":32196,\"start\":31475},{\"end\":32464,\"start\":32248},{\"end\":32529,\"start\":32466},{\"end\":32616,\"start\":32567},{\"end\":32713,\"start\":32618},{\"end\":32861,\"start\":32769},{\"end\":33407,\"start\":32863},{\"end\":33673,\"start\":33436},{\"end\":33738,\"start\":33675},{\"end\":34036,\"start\":33813},{\"end\":34913,\"start\":34038},{\"end\":36055,\"start\":34948},{\"end\":36319,\"start\":36084},{\"end\":36942,\"start\":36373},{\"end\":37138,\"start\":36944},{\"end\":37506,\"start\":37140},{\"end\":37702,\"start\":37508},{\"end\":37902,\"start\":37732},{\"end\":38002,\"start\":37944},{\"end\":38316,\"start\":38064},{\"end\":38622,\"start\":38318},{\"end\":38805,\"start\":38624},{\"end\":39744,\"start\":38807},{\"end\":40224,\"start\":39746},{\"end\":41365,\"start\":40251},{\"end\":41960,\"start\":41417},{\"end\":42894,\"start\":41962},{\"end\":43530,\"start\":42896},{\"end\":44581,\"start\":43532},{\"end\":45479,\"start\":44583},{\"end\":46660,\"start\":45525}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8449,\"start\":8422},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10393,\"start\":10356},{\"attributes\":{\"id\":\"formula_2\"},\"end\":22818,\"start\":22778},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23217,\"start\":23171},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24304,\"start\":24244},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24610,\"start\":24567},{\"attributes\":{\"id\":\"formula_6\"},\"end\":31337,\"start\":31292},{\"attributes\":{\"id\":\"formula_7\"},\"end\":32247,\"start\":32197},{\"attributes\":{\"id\":\"formula_8\"},\"end\":32566,\"start\":32530},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32768,\"start\":32714},{\"attributes\":{\"id\":\"formula_10\"},\"end\":33812,\"start\":33739},{\"attributes\":{\"id\":\"formula_11\"},\"end\":37943,\"start\":37903},{\"attributes\":{\"id\":\"formula_12\"},\"end\":38063,\"start\":38003}]", "table_ref": "[{\"end\":22628,\"start\":22621},{\"end\":27280,\"start\":27273},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27535,\"start\":27528},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":40524,\"start\":40517},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":44566,\"start\":44559},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":44666,\"start\":44659},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":45880,\"start\":45868}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1891,\"start\":1879},{\"attributes\":{\"n\":\"2\"},\"end\":7434,\"start\":7395},{\"attributes\":{\"n\":\"3\"},\"end\":9086,\"start\":9063},{\"end\":9731,\"start\":9725},{\"end\":11255,\"start\":11247},{\"attributes\":{\"n\":\"4\"},\"end\":12069,\"start\":12027},{\"attributes\":{\"n\":\"4.1\"},\"end\":12255,\"start\":12234},{\"attributes\":{\"n\":\"4.2\"},\"end\":13377,\"start\":13354},{\"end\":14428,\"start\":14420},{\"attributes\":{\"n\":\"5\"},\"end\":15318,\"start\":15270},{\"attributes\":{\"n\":\"5.1\"},\"end\":15719,\"start\":15680},{\"end\":16125,\"start\":16119},{\"attributes\":{\"n\":\"5.2\"},\"end\":17331,\"start\":17263},{\"attributes\":{\"n\":\"5.3\"},\"end\":20573,\"start\":20555},{\"attributes\":{\"n\":\"6\"},\"end\":21633,\"start\":21586},{\"attributes\":{\"n\":\"6.2\"},\"end\":25310,\"start\":25288},{\"attributes\":{\"n\":\"7\"},\"end\":29143,\"start\":29131},{\"end\":29164,\"start\":29146},{\"attributes\":{\"n\":\"8\"},\"end\":29877,\"start\":29866},{\"end\":30869,\"start\":30835},{\"end\":31110,\"start\":31070},{\"end\":33434,\"start\":33410},{\"end\":34946,\"start\":34916},{\"end\":36082,\"start\":36058},{\"end\":36371,\"start\":36322},{\"end\":37730,\"start\":37705},{\"end\":40249,\"start\":40227},{\"end\":41415,\"start\":41368},{\"end\":45523,\"start\":45482},{\"end\":46672,\"start\":46662},{\"end\":46852,\"start\":46842},{\"end\":48516,\"start\":48506},{\"end\":48626,\"start\":48616},{\"end\":48701,\"start\":48692},{\"end\":48879,\"start\":48870},{\"end\":54604,\"start\":54587},{\"end\":57558,\"start\":57549},{\"end\":57678,\"start\":57669},{\"end\":61136,\"start\":61127},{\"end\":61394,\"start\":61385}]", "table": "[{\"end\":54585,\"start\":50793},{\"end\":55396,\"start\":55134},{\"end\":57547,\"start\":55453},{\"end\":61125,\"start\":57749}]", "figure_caption": "[{\"end\":46840,\"start\":46674},{\"end\":46990,\"start\":46854},{\"end\":48504,\"start\":46993},{\"end\":48614,\"start\":48518},{\"end\":48690,\"start\":48628},{\"end\":48868,\"start\":48703},{\"end\":48968,\"start\":48881},{\"end\":50793,\"start\":48971},{\"end\":55134,\"start\":54607},{\"end\":55453,\"start\":55399},{\"end\":57667,\"start\":57560},{\"end\":57749,\"start\":57680},{\"end\":61383,\"start\":61138},{\"end\":61475,\"start\":61396}]", "figure_ref": "[{\"end\":8778,\"start\":8770},{\"end\":9892,\"start\":9884},{\"end\":11815,\"start\":11807},{\"end\":12832,\"start\":12824},{\"end\":14785,\"start\":14777},{\"end\":14947,\"start\":14937},{\"end\":15074,\"start\":15065},{\"end\":15171,\"start\":15162},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15747,\"start\":15739},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16026,\"start\":16018},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16287,\"start\":16279},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16314,\"start\":16304},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16598,\"start\":16590},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16813,\"start\":16804},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16904,\"start\":16896},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":17113,\"start\":17104},{\"end\":17367,\"start\":17359},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17380,\"start\":17372},{\"end\":18206,\"start\":18198},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18219,\"start\":18211},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19196,\"start\":19188},{\"end\":19512,\"start\":19504},{\"end\":20551,\"start\":20543},{\"end\":20746,\"start\":20738},{\"end\":21106,\"start\":21098},{\"end\":21482,\"start\":21474},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":35299,\"start\":35291},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":35497,\"start\":35489}]", "bib_author_first_name": "[{\"end\":61824,\"start\":61819},{\"end\":61838,\"start\":61833},{\"end\":61850,\"start\":61845},{\"end\":61867,\"start\":61863},{\"end\":61884,\"start\":61880},{\"end\":61898,\"start\":61895},{\"end\":62144,\"start\":62137},{\"end\":62161,\"start\":62155},{\"end\":62178,\"start\":62173},{\"end\":62196,\"start\":62192},{\"end\":62560,\"start\":62553},{\"end\":62577,\"start\":62571},{\"end\":62581,\"start\":62578},{\"end\":62595,\"start\":62589},{\"end\":62617,\"start\":62609},{\"end\":62630,\"start\":62626},{\"end\":62645,\"start\":62638},{\"end\":62661,\"start\":62653},{\"end\":62663,\"start\":62662},{\"end\":62678,\"start\":62672},{\"end\":62691,\"start\":62688},{\"end\":62706,\"start\":62700},{\"end\":62717,\"start\":62714},{\"end\":62729,\"start\":62725},{\"end\":62741,\"start\":62736},{\"end\":63050,\"start\":63049},{\"end\":63073,\"start\":63058},{\"end\":63090,\"start\":63084},{\"end\":63109,\"start\":63102},{\"end\":63124,\"start\":63117},{\"end\":63600,\"start\":63596},{\"end\":63612,\"start\":63607},{\"end\":63632,\"start\":63624},{\"end\":63650,\"start\":63642},{\"end\":64023,\"start\":64016},{\"end\":64034,\"start\":64030},{\"end\":64050,\"start\":64041},{\"end\":64052,\"start\":64051},{\"end\":64366,\"start\":64363},{\"end\":64376,\"start\":64373},{\"end\":64390,\"start\":64383},{\"end\":64405,\"start\":64399},{\"end\":64413,\"start\":64410},{\"end\":64420,\"start\":64418},{\"end\":64878,\"start\":64873},{\"end\":64892,\"start\":64887},{\"end\":64908,\"start\":64901},{\"end\":64922,\"start\":64916},{\"end\":64934,\"start\":64927},{\"end\":64947,\"start\":64942},{\"end\":64955,\"start\":64954},{\"end\":64967,\"start\":64958},{\"end\":65243,\"start\":65235},{\"end\":65257,\"start\":65250},{\"end\":65268,\"start\":65264},{\"end\":65282,\"start\":65275},{\"end\":65296,\"start\":65289},{\"end\":65605,\"start\":65600},{\"end\":65617,\"start\":65611},{\"end\":65987,\"start\":65981},{\"end\":66004,\"start\":65999},{\"end\":66395,\"start\":66390},{\"end\":66410,\"start\":66403},{\"end\":66421,\"start\":66415},{\"end\":66434,\"start\":66431},{\"end\":66697,\"start\":66692},{\"end\":66708,\"start\":66703},{\"end\":66719,\"start\":66717},{\"end\":66733,\"start\":66725},{\"end\":67094,\"start\":67093},{\"end\":67110,\"start\":67103},{\"end\":67112,\"start\":67111},{\"end\":67130,\"start\":67121},{\"end\":67148,\"start\":67143},{\"end\":67150,\"start\":67149},{\"end\":67177,\"start\":67168},{\"end\":67191,\"start\":67187},{\"end\":67204,\"start\":67201},{\"end\":67218,\"start\":67213},{\"end\":67225,\"start\":67219},{\"end\":67237,\"start\":67231},{\"end\":67246,\"start\":67238},{\"end\":67263,\"start\":67256},{\"end\":67277,\"start\":67271},{\"end\":67284,\"start\":67278},{\"end\":67297,\"start\":67293},{\"end\":67299,\"start\":67298},{\"end\":67314,\"start\":67310},{\"end\":67331,\"start\":67325},{\"end\":67347,\"start\":67338},{\"end\":67358,\"start\":67354},{\"end\":67360,\"start\":67359},{\"end\":67378,\"start\":67371},{\"end\":67380,\"start\":67379},{\"end\":67392,\"start\":67388},{\"end\":67394,\"start\":67393},{\"end\":67982,\"start\":67975},{\"end\":67994,\"start\":67987},{\"end\":68010,\"start\":68002},{\"end\":68020,\"start\":68016},{\"end\":68481,\"start\":68474},{\"end\":68492,\"start\":68486}]", "bib_author_last_name": "[{\"end\":61831,\"start\":61825},{\"end\":61843,\"start\":61839},{\"end\":61861,\"start\":61851},{\"end\":61878,\"start\":61868},{\"end\":61893,\"start\":61885},{\"end\":61903,\"start\":61899},{\"end\":62153,\"start\":62145},{\"end\":62171,\"start\":62162},{\"end\":62190,\"start\":62179},{\"end\":62205,\"start\":62197},{\"end\":62569,\"start\":62561},{\"end\":62587,\"start\":62582},{\"end\":62607,\"start\":62596},{\"end\":62624,\"start\":62618},{\"end\":62636,\"start\":62631},{\"end\":62651,\"start\":62646},{\"end\":62670,\"start\":62664},{\"end\":62686,\"start\":62679},{\"end\":62698,\"start\":62692},{\"end\":62712,\"start\":62707},{\"end\":62723,\"start\":62718},{\"end\":62734,\"start\":62730},{\"end\":62747,\"start\":62742},{\"end\":63056,\"start\":63051},{\"end\":63082,\"start\":63074},{\"end\":63100,\"start\":63091},{\"end\":63115,\"start\":63110},{\"end\":63131,\"start\":63125},{\"end\":63140,\"start\":63133},{\"end\":63605,\"start\":63601},{\"end\":63622,\"start\":63613},{\"end\":63640,\"start\":63633},{\"end\":63657,\"start\":63651},{\"end\":64028,\"start\":64024},{\"end\":64039,\"start\":64035},{\"end\":64058,\"start\":64053},{\"end\":64371,\"start\":64367},{\"end\":64381,\"start\":64377},{\"end\":64397,\"start\":64391},{\"end\":64408,\"start\":64406},{\"end\":64416,\"start\":64414},{\"end\":64428,\"start\":64421},{\"end\":64885,\"start\":64879},{\"end\":64899,\"start\":64893},{\"end\":64914,\"start\":64909},{\"end\":64925,\"start\":64923},{\"end\":64940,\"start\":64935},{\"end\":64952,\"start\":64948},{\"end\":64973,\"start\":64968},{\"end\":65248,\"start\":65244},{\"end\":65262,\"start\":65258},{\"end\":65273,\"start\":65269},{\"end\":65287,\"start\":65283},{\"end\":65300,\"start\":65297},{\"end\":65609,\"start\":65606},{\"end\":65628,\"start\":65618},{\"end\":65997,\"start\":65988},{\"end\":66014,\"start\":66005},{\"end\":66401,\"start\":66396},{\"end\":66413,\"start\":66411},{\"end\":66429,\"start\":66422},{\"end\":66443,\"start\":66435},{\"end\":66701,\"start\":66698},{\"end\":66715,\"start\":66709},{\"end\":66723,\"start\":66720},{\"end\":66744,\"start\":66734},{\"end\":67101,\"start\":67095},{\"end\":67119,\"start\":67113},{\"end\":67141,\"start\":67131},{\"end\":67166,\"start\":67151},{\"end\":67185,\"start\":67178},{\"end\":67199,\"start\":67192},{\"end\":67211,\"start\":67205},{\"end\":67229,\"start\":67226},{\"end\":67254,\"start\":67247},{\"end\":67269,\"start\":67264},{\"end\":67291,\"start\":67285},{\"end\":67308,\"start\":67300},{\"end\":67323,\"start\":67315},{\"end\":67336,\"start\":67332},{\"end\":67352,\"start\":67348},{\"end\":67369,\"start\":67361},{\"end\":67386,\"start\":67381},{\"end\":67403,\"start\":67395},{\"end\":67413,\"start\":67405},{\"end\":67985,\"start\":67983},{\"end\":68000,\"start\":67995},{\"end\":68014,\"start\":68011},{\"end\":68024,\"start\":68021},{\"end\":68484,\"start\":68482},{\"end\":68500,\"start\":68493}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1606.06565\",\"id\":\"b0\"},\"end\":62097,\"start\":61787},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":39895556},\"end\":62508,\"start\":62099},{\"attributes\":{\"id\":\"b2\"},\"end\":62998,\"start\":62510},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":81978372},\"end\":63523,\"start\":63000},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":211096730},\"end\":64014,\"start\":63525},{\"attributes\":{\"doi\":\"arXiv:1810.01392\",\"id\":\"b5\"},\"end\":64308,\"start\":64016},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":57246310},\"end\":64794,\"start\":64310},{\"attributes\":{\"id\":\"b7\"},\"end\":65162,\"start\":64796},{\"attributes\":{\"doi\":\"arXiv:2005.12766\",\"id\":\"b8\"},\"end\":65512,\"start\":65164},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":160705},\"end\":65938,\"start\":65514},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":220347682},\"end\":66312,\"start\":65940},{\"attributes\":{\"doi\":\"arXiv:2011.01403\",\"id\":\"b11\"},\"end\":66648,\"start\":66314},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":28671436},\"end\":67035,\"start\":66650},{\"attributes\":{\"doi\":\"10.1148/radiol.2018180736\",\"id\":\"b13\",\"matched_paper_id\":53768272},\"end\":67927,\"start\":67037},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":68369,\"start\":67929},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1964279},\"end\":68816,\"start\":68371},{\"attributes\":{\"id\":\"b16\"},\"end\":68977,\"start\":68818}]", "bib_title": "[{\"end\":62135,\"start\":62099},{\"end\":63047,\"start\":63000},{\"end\":63594,\"start\":63525},{\"end\":64361,\"start\":64310},{\"end\":65598,\"start\":65514},{\"end\":65979,\"start\":65940},{\"end\":66690,\"start\":66650},{\"end\":67091,\"start\":67037},{\"end\":67973,\"start\":67929},{\"end\":68472,\"start\":68371}]", "bib_author": "[{\"end\":61833,\"start\":61819},{\"end\":61845,\"start\":61833},{\"end\":61863,\"start\":61845},{\"end\":61880,\"start\":61863},{\"end\":61895,\"start\":61880},{\"end\":61905,\"start\":61895},{\"end\":62155,\"start\":62137},{\"end\":62173,\"start\":62155},{\"end\":62192,\"start\":62173},{\"end\":62207,\"start\":62192},{\"end\":62571,\"start\":62553},{\"end\":62589,\"start\":62571},{\"end\":62609,\"start\":62589},{\"end\":62626,\"start\":62609},{\"end\":62638,\"start\":62626},{\"end\":62653,\"start\":62638},{\"end\":62672,\"start\":62653},{\"end\":62688,\"start\":62672},{\"end\":62700,\"start\":62688},{\"end\":62714,\"start\":62700},{\"end\":62725,\"start\":62714},{\"end\":62736,\"start\":62725},{\"end\":62749,\"start\":62736},{\"end\":63058,\"start\":63049},{\"end\":63084,\"start\":63058},{\"end\":63102,\"start\":63084},{\"end\":63117,\"start\":63102},{\"end\":63133,\"start\":63117},{\"end\":63142,\"start\":63133},{\"end\":63607,\"start\":63596},{\"end\":63624,\"start\":63607},{\"end\":63642,\"start\":63624},{\"end\":63659,\"start\":63642},{\"end\":64030,\"start\":64016},{\"end\":64041,\"start\":64030},{\"end\":64060,\"start\":64041},{\"end\":64373,\"start\":64363},{\"end\":64383,\"start\":64373},{\"end\":64399,\"start\":64383},{\"end\":64410,\"start\":64399},{\"end\":64418,\"start\":64410},{\"end\":64430,\"start\":64418},{\"end\":64887,\"start\":64873},{\"end\":64901,\"start\":64887},{\"end\":64916,\"start\":64901},{\"end\":64927,\"start\":64916},{\"end\":64942,\"start\":64927},{\"end\":64954,\"start\":64942},{\"end\":64958,\"start\":64954},{\"end\":64975,\"start\":64958},{\"end\":65250,\"start\":65235},{\"end\":65264,\"start\":65250},{\"end\":65275,\"start\":65264},{\"end\":65289,\"start\":65275},{\"end\":65302,\"start\":65289},{\"end\":65611,\"start\":65600},{\"end\":65630,\"start\":65611},{\"end\":65999,\"start\":65981},{\"end\":66016,\"start\":65999},{\"end\":66403,\"start\":66390},{\"end\":66415,\"start\":66403},{\"end\":66431,\"start\":66415},{\"end\":66445,\"start\":66431},{\"end\":66703,\"start\":66692},{\"end\":66717,\"start\":66703},{\"end\":66725,\"start\":66717},{\"end\":66746,\"start\":66725},{\"end\":67103,\"start\":67093},{\"end\":67121,\"start\":67103},{\"end\":67143,\"start\":67121},{\"end\":67168,\"start\":67143},{\"end\":67187,\"start\":67168},{\"end\":67201,\"start\":67187},{\"end\":67213,\"start\":67201},{\"end\":67231,\"start\":67213},{\"end\":67256,\"start\":67231},{\"end\":67271,\"start\":67256},{\"end\":67293,\"start\":67271},{\"end\":67310,\"start\":67293},{\"end\":67325,\"start\":67310},{\"end\":67338,\"start\":67325},{\"end\":67354,\"start\":67338},{\"end\":67371,\"start\":67354},{\"end\":67388,\"start\":67371},{\"end\":67405,\"start\":67388},{\"end\":67415,\"start\":67405},{\"end\":67987,\"start\":67975},{\"end\":68002,\"start\":67987},{\"end\":68016,\"start\":68002},{\"end\":68026,\"start\":68016},{\"end\":68486,\"start\":68474},{\"end\":68502,\"start\":68486}]", "bib_venue": "[{\"end\":61817,\"start\":61787},{\"end\":62268,\"start\":62207},{\"end\":62551,\"start\":62510},{\"end\":63220,\"start\":63142},{\"end\":63728,\"start\":63659},{\"end\":64140,\"start\":64076},{\"end\":64508,\"start\":64430},{\"end\":64871,\"start\":64796},{\"end\":65233,\"start\":65164},{\"end\":65691,\"start\":65630},{\"end\":66084,\"start\":66016},{\"end\":66388,\"start\":66314},{\"end\":66807,\"start\":66746},{\"end\":67449,\"start\":67440},{\"end\":68104,\"start\":68026},{\"end\":68560,\"start\":68502},{\"end\":68896,\"start\":68818},{\"end\":62325,\"start\":62270},{\"end\":63294,\"start\":63222},{\"end\":63793,\"start\":63730},{\"end\":64582,\"start\":64510},{\"end\":65748,\"start\":65693},{\"end\":66148,\"start\":66086},{\"end\":66864,\"start\":66809},{\"end\":68178,\"start\":68106},{\"end\":68614,\"start\":68562}]"}}}, "year": 2023, "month": 12, "day": 17}