{"id": 2928248, "updated": "2023-09-28 17:12:59.563", "metadata": {"title": "Deep Hashing Network for Unsupervised Domain Adaptation", "authors": "[{\"first\":\"Hemanth\",\"last\":\"Venkateswara\",\"middle\":[]},{\"first\":\"Jose\",\"last\":\"Eusebio\",\"middle\":[]},{\"first\":\"Shayok\",\"last\":\"Chakraborty\",\"middle\":[]},{\"first\":\"Sethuraman\",\"last\":\"Panchanathan\",\"middle\":[]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2017, "month": 6, "day": 22}, "abstract": "In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1706.07522", "mag": "2951111165", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/VenkateswaraECP17", "doi": "10.1109/cvpr.2017.572"}}, "content": {"source": {"pdf_hash": "563253ea8bc6db95eec13d129a2ad820760f4053", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1706.07522v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1706.07522", "status": "GREEN"}}, "grobid": {"id": "861ee3688cafc03bdf35da1e6d2b77f9efc469e1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/563253ea8bc6db95eec13d129a2ad820760f4053.txt", "contents": "\nDeep Hashing Network for Unsupervised Domain Adaptation\n22 Jun 2017\n\nHemanth Venkateswara hemanthv@asu.edu \nCenter for Cognitive Ubiquitous Computing\nArizona State University\nTempeAZUSA\n\nJose Eusebio jeusebio@asu.edu \nCenter for Cognitive Ubiquitous Computing\nArizona State University\nTempeAZUSA\n\nShayok Chakraborty shayok.chakraborty@asu.edu \nCenter for Cognitive Ubiquitous Computing\nArizona State University\nTempeAZUSA\n\nSethuraman Panchanathan \nCenter for Cognitive Ubiquitous Computing\nArizona State University\nTempeAZUSA\n\nDeep Hashing Network for Unsupervised Domain Adaptation\n22 Jun 2017\nIn recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efficiency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we first introduce a new dataset, Office-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the first research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efficient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.\n\nIntroduction\n\nDeep learning algorithms automatically learn a discriminating set of features and have depicted commendable performance in a variety of computer vision applications. Unfortunately, training a deep model necessitates a large volume of labeled data, which can be time consuming and expensive to acquire. However, labeled data from a different, but related domain is often available, which has motivated the development of algorithms which can leverage labeled data in a source domain to develop a machine learning model for the target domain. Learning a discriminative model in the presence of the shift between training and test distributions is known as transfer learning or domain adaptation [17]. Unsupervised domain adaptation is a challenging setting, where labeled data is available only in the source domain; no labeled data is available in the target domain. Conventional shallow transfer learning methods develop their models in two stages, feature extraction followed by domain adaptation. The features are fixed and then a model is trained to align the source and target domains [16,20,33,38,42,43,44]. On the other hand, deep transfer learning procedures exploit the feature learning capabilities of deep networks to learn transferable feature representations for domain adaptation and have demonstrated impressive empirical performance [17,18,31,34,46].\n\nThe explosive growth of digital data in the modern era has posed fundamental challenges regarding their storage, retrieval and computational requirements. Against this backdrop, hashing has emerged as one of the most popular and effective techniques due to its fast query speed and low memory cost [48]. Hashing techniques transform high dimensional data into compact binary codes and generate similar binary codes for similar data items. Motivated by this fact, we propose to train a deep neural network to output binary hash codes (instead of probability values), which can be used for classification. We see two advantages to estimating a hash value instead of a standard probability vector in the final layer of the network: (i) the hash values are used to develop a unique loss function for target data in the absence of labels and (ii) during prediction, the hash value of a test sample can be compared against the hash values of the training samples to arrive at a more robust category prediction.\n\nIn this paper, we first introduce a new dataset, Office-Home, which we use to evaluate our algorithm. The Office-Home dataset is an object recognition dataset which contains images from 4 domains. It has around 15, 500 images organized into 65 categories. We further propose a novel deep learning framework called Domain Adaptive Hash-ing (DAH) to learn informative hash codes to address the problem of unsupervised domain adaptation. We propose a unique loss function to train the deep network with the following components: (i) supervised hash loss for labeled source data, which ensures that source samples belonging to the same class have similar hash codes; (ii) unsupervised entropy loss for unlabeled target data, which imposes each target sample to align closely with exactly one of the source categories and be distinct from the other categories and (iii) a loss based on multi-kernel Maximum Mean Discrepancy (MK-MMD), which seeks to learn transferable features within the layers of the network to minimize the distribution difference between the source and target domains. Figure 1 illustrates the different layers of the DAH and the components of the loss function.\n\n\nRelated Work\n\nThere have been many approaches to address the problem of domain-shift in unsupervised domain adaptation. One straightforward approach is, to modify a classifier trained for the source data by adapting it to classify target data [1,4] or learn a transformation matrix to linearly transform the source data, so that it is aligned with the target [27,42]. Some other procedures re-weight the data points in the source domain, to select source data that is similar to the target, when training a domain adaptive classifier, [9,10,19]. A standard procedure to reduce domain discrepancy is, to project the source and target data to a common subspace, thereby aligning their principal axes [16,44]. Reducing domain disparity through nonlinear alignment of data has been possible with Maximum Mean Discrepancy (MMD) -a measure that provides the distribution difference between two datasets in a reproducing-kernel Hilbert space [13]. Kernel-PCA based methods apply the MMD to achieve nonlinear alignment of domains [32,33,38]. Manifold based approaches are also popular in domain adaptation for computer vision, where the subspace of a domain is treated as a point on the manifold and transformations are learned to align two domains [20,23]. A survey of popular domain adaptation techniques for computer vision is provided in [41] and a more generic survey of transfer learning approaches can be found in [39].\n\nAll of the above techniques can be termed as shallow learning procedures, since the models are learned using predetermined features. In recent years deep learning has become very successful at learning highly discriminative features for computer vision applications [8]. Deep learning systems like deep CNNs learn representations of data that capture underlying factors of variation between different tasks in a multi-task transfer learning setting [3]. These representations also disentangle the factors of variation allowing for the transfer of knowledge between tasks [12,18,37]. Yosinski et al. [49] demonstrated how the lower layers of a network produce generic features and the upper layers output task specific features. Based on this, deep learning procedures for domain adaptation train networks to learn transferable features in the fully connected final layers of a network [31,46]. In other approaches to deep domain adaptation, Ganin et al. [17] trained domain adversarial networks to learn features that make the source and target domain indistinguishable and Long et al. [34], trained a network to do both feature adaptation and classifier adaptation using residual transfer networks.\n\nUnsupervised hashing techniques have been developed to extract unique hash codes for efficient storage and retrieval of data [22,25]. Neural network based hashing has led the way in state-of-the-art unsupervised hashing techniques [7,11,14]. The closest work incorporating hashing and adaptation appears in cross-modal hashing, where deep hashing techniques embed multi-modal data and learn hash codes for two related domains, like text and images [5,6,29]. However, these algorithms are not unsupervised and they are mainly applied to extract common hash codes for multi-modal data for retrieval purposes. To the best of our knowledge, there has been no work in unsupervised domain adaptation using deep hashing networks. We now present the Domain Adaptive Hashing (DAH) network for unsupervised domain adaptation through deep hashing.\n\n\nDomain Adaptive Hashing Networks\n\nIn unsupervised domain adaptation, we consider data from two domains; source and target. The source consists of labeled data, D s = {x s i , y s i } ns i=1 and the target has only unlabeled data D t = {x t i } nt i=1 . The data points x * i belong to X, where X is some input space. The corresponding labels are represented by y * i \u2208 Y := {1, . . . , C}. The paradigm of domain adaptive learning attempts to address the problem of domain-shift in the data, where the data distributions of the source and target are different, i.e. P s (X, Y ) = P t (X, Y ). The domain-shift notwithstanding, our goal is to train a deep neural network classifier \u03c8(.), that can predict the labels {\u0177 t i } nt i=1 , for the target data. We implement the neural network as a deep CNN which consists of 5 convolution layers conv1 -conv5 and 3 fully connected layers fc6 -fc8 followed by a loss layer. In our model, we introduce a hashing layer hash-fc8 in place of the standard fc8 layer to learn a binary code h i , for every data point x i , where h i \u2208 {\u22121, +1} d . The hash-fc8 layer is driven by two loss functions, (i) supervised hash loss for the source data, (ii) unsupervised entropy loss for the target data. The supervised hash loss ensures hash values that are distinct and discriminatory, i.e. if x i and x j belong to the same category, their hash values h i and h j are similar and different otherwise. The unsupervised entropy loss aligns the target hash values with source hash values based on the similarity of their feature representations. The output of the network is represented as \u03c8(x), where \u03c8(x) \u2208 R d , which we convert to a hash code h = sgn(\u03c8(x)), where sgn(.) is the sign function. Once the network has been trained, the probability of x being assigned a label y is given by f (x) = p(y|h). We train the network using D s and D t and predict the target data labels\u0177 t * using f (.). In order to address the issue of domain-shift, we need to align the feature representations of the target and the source. We do that by reducing the domain discrepancy between the source and target feature representations at multiple layers of the network. In the following subsections, we discuss the design of the domain adaptive hash (DAH) network in detail.\n\n\nReducing Domain Disparity\n\nDeep learning methods have been very successful in domain adaptation with state-of-the-art algorithms [17,31,34,46] in recent years. The feature representations transition from generic to task-specific as one goes up the layers of a deep CNN [49]. The convolution layers conv1 to conv5 have been shown to be generic and so, readily transferable, whereas the fully connected layers are more task-specific and need to be adapted before they can be transferred. In the DAH algorithm, we attempt to minimize the MK-MMD loss to reduce the domain difference between the source and target feature representations for fully connected layers, F = {fc6, fc7, fc8}. Such a loss function has been used in previous research [31,34]. The multi-layer MK-MMD loss is given by,\nM(U s , U t ) = l\u2208F d 2 k (U l s , U l t ),(1)\nwhere, U l s = {u s,l i } ns i=1 and U l t = {u t,l i } nt i=1 are the set of output representations for the source and target data at layer l, where u * ,l i is the output representation of x * i for the l th layer. The final layer outputs are denoted as U s and U t . The MK-MMD measure d 2 k (.) is the multi-kernel maximum mean discrepancy between the source and target representations, [24]. For a nonlinear mapping \u03c6(.) associated with a reproducing kernel Hilbert space H k and kernel k(.), where k(x, y) = \u03c6(x), \u03c6(y) , the MMD is defined as,\nd 2 k (U l s , U l t ) = E[\u03c6(u s,l )] \u2212 E[\u03c6(u t,l )] 2 H k .(2)\nThe characteristic kernel k(.), is determined as a convex combination of \u03ba PSD kernels, {k m } \u03ba m=1 , K := k : k = \u03ba m=1 \u03b2 m k m , \u03ba m=1 \u03b2 m = 1, \u03b2 m \u2265 0, \u2200m . We set \u03b2 m = 1/\u03ba according to [34] and it works well in practice.\n\n\nSupervised Hashing for Source Data\n\nThe Hamming distance for a pair of hash values h i and h j has a unique relationship with the dot product h i , h j ,\ngiven by: dist H (h i , h j ) = 1 2 (d \u2212 h \u22a4 i h j ),\nwhere d is the hash length. The dot product h i , h j can be treated as a similarity measure for the hash codes. Larger the value of the dot product (high similarity), smaller is the distance dist H and smaller the dot product (low similarity), larger is the distance dist H . Let s ij \u2208 {0, 1} be the similarity between x i and x j . If x i and x j belong to the same category, s ij = 1 and 0, otherwise. The probability of similarity between x i and x j given the corresponding hash values h i and h j , can be expressed as a likelihood function, given by,\np(s ij |h i , h j ) = \u03c3(h \u22a4 i h j ), s ij = 1 1 \u2212 \u03c3(h \u22a4 i h j ), s ij = 0,(3)\nwhere, \u03c3(x) = 1 1+e \u2212x is the sigmoid function. As the dot product h i , h j increases, the probability of p(s ij = 1|h i , h j ) also increases, i.e., x i and x j belong to the same category. As the dot product decreases, the probability p(s ij = 1|h i , h j ) also decreases, i.e., x i and x j belong to different categories. We construct the (n s \u00d7 n s ) similarity matrix S = {s ij }, for the source data with the provided labels, where s ij = 1 if x i and x j belong to the same category and 0, otherwise. Let H = {h i } ns i=1 be the set of source data hash values. If the elements of H are assumed to be i.i.d., the negative log likelihood of the similarity matrix S given H can be written as,\nmin H L(H) = \u2212log p(S|H) = \u2212 sij \u2208S s ij h \u22a4 i h j \u2212 log 1 + exp(h \u22a4 i h j ) .(4)\nBy minimizing Equation (4), we can determine hash values H for the source data which are consistent with the similarity matrix S. The hash loss has been used in previous research for supervised hashing [30,50]. Equation (4) is a discrete optimization problem that is challenging to solve. We introduce a relaxation on the discrete constraint\nh i \u2208 {\u22121, +1} d by instead solving for u i \u2208 R d , where U s = {u i } ns i=1\nis the output of the network and u i = \u03c8(x i ) (the superscript denoting the domain has been dropped for ease of representation). However, the continuous relaxation gives rise to (i) approximation error, when h i , h j is substituted with u i , u j and, (ii) quantization error, when the resulting real codes u i are binarized [50]. We account for the approximation error by having a tanh(.) as the final activation layer of the neural network, so that the components of u i are bounded between \u22121 and +1. In addition, we also introduce a quantization loss ||u i \u2212 sgn(u i )|| 2 2 along the lines of [22], where sgn(.) is the sign function. The continuous optimization problem for supervised hashing can now be outlined;\nmin U s L(U s ) = \u2212 sij \u2208S s ij u \u22a4 i u j \u2212 log 1 + exp(u \u22a4 i u j ) + ns i=1 u i \u2212 sgn(u i ) 2 2 .(5)\n\nUnsupervised Hashing for Target Data\n\nIn the absence of target data labels, we use the similarity measure u i , u j , to guide the network to learn discriminative hash values for the target data. An ideal target output u t i , needs to be similar to many of the source outputs from the j th category {u sj k } K k=1 . We assume without loss of generality, K source data points for every category j where, j \u2208 {1, . . . , C} and u sj k is the k th source output from category j. In addition, u t i must be dissimilar to most other source outputs u s l k belonging to a different category (j = l). Enforcing similarity with all the K data points makes for a more robust target data category assignment. We outline a probability measure to capture this intuition. Let p ij be the probability that input target data point x i is assigned to category j where,\np ij = K k=1 exp(u t i \u22a4 u sj k ) C l=1 K k=1 exp(u t i \u22a4 u s l k )(6)\nThe exp(.) has been introduced for ease of differentiability and the denominator ensures j p ij = 1. When the target data point output is similar to one category only and dissimilar to all the other categories, the probability vector p i = [p i1 , . . . , p iC ] T tends to be a one-hot vector. A one-hot vector can be viewed as a low entropy realization of p i . We can therefore envisage all the p i to be one-hot vectors (low entropy probability vectors), where the target data point outputs are similar to source data point outputs in one and only one category. To this end we introduce a loss to capture the entropy of the target probability vectors. The entropy loss for the network outputs is given by,\nH(U s , U t ) = \u2212 1 n t nt i=1 C j=1 p ij log(p ij )(7)\nMinimizing the entropy loss gives us probability vectors p i that tend to be one-hot vectors, i.e., the target data point outputs are similar to source data point outputs from any one category only. Enforcing similarity with K source data points from a category, guarantees that the hash values are determined based on a common similarity between multiple source category data points and the target data point.\n\n\nDomain Adaptive Hash Network\n\nWe propose a model for deep unsupervised domain adaptation based on hashing (DAH) that incorporates unsupervised domain adaptation between the source and the target (1), supervised hashing for the source (5) and unsupervised hashing for the target (7) in a deep convolutional neural network. The DAH network is trained to minimize (1) and target entropy loss (7) respectively. The hash values H are obtained from the output of the network using H = sgn(U). The loss terms (5) and (7) are determined in the final layer of the network with the network output U . The MK-MMD loss (1) is determined between layer outputs {U l s , U l t } at each of the fully connected layers F = {fc6, fc7, fc8}, where we adopt the linear time estimate for the unbiased MK-MMD as described in [24] and [31]. The DAH is trained using standard backpropagation. The detailed derivation of the derivative of (8) w.r.t. U is provided in the supplementary material. Network Architecture: Owing to the paucity of images in a domain adaptation setting, we circumvent the need to train a deep CNN with millions of images by adapting the pre-trained VGG-F [8] network to the DAH. The VGG-F has been trained on the ImageNet 2012 dataset and it consists of 5 convolution layers (conv1 -conv5) and 3 fully connected layers (fc6, fc7, fc8). We introduce the hashing layer hash-fc8 that outputs vectors in R d in the place of fc8.\nmin U J = L(U s ) + \u03b3M(U s , U t ) + \u03b7H(U s , U t ), (8) where, U := {U s \u222a U t } and (\u03b3, \u03b7) control the importance of domain adaptation\nTo account for the hashing approximation, we introduced a tanh() layer. However, we encounter the issue of vanishing gradients [26] when using tanh() as it saturates with large inputs. We therefore preface the tanh() with a batch normalization layer which prevents the tanh() from saturating. In effect, hash-fc8\n:= {fc8 \u2192 batch-norm \u2192 tanh()}.\nThe hash-fc8 provides greater stability when fine-tuning the learning rates than the deep hashing networks [30,50].   [35] and MMI [40], digit datasets SVHN [36], USPS and MNIST [28], head pose recognition datasets PIE [33], object recognition datasets COIL [33], Office [42] and Office-Caltech [20]. These datasets were created before deep-learning became popular and are insufficient for training and evaluating deep learning based domain adaptation approaches. For instance, the object-recognition dataset Office has 4110 images across 31 categories and Office-Caltech has 2533 images across 10 categories.\n\nWe release the Office-Home dataset for domain adaptation based object recognition, that can be used to evaluate deep learning algorithms for domain adaptation. The Office-Home dataset consists of 4 domains, with each domain containing images from 65 categories of everyday objects and a total of around 15, 500 images. The domains include, Art: artistic depictions of objects in the form of sketches, paintings, ornamentation, etc.; Clipart: collection of clipart images; Product: images of objects without a background, akin to the Amazon category in Office dataset; Real-World: images of objects captured with a regular camera.\n\nPublic domain images were downloaded from websites like www.deviantart.com and www.flickr.com to create the Art and Real-World domains. Clipart images were gathered from multiple clipart websites. The Product domain images were exclusively collected from www.amazon.com using web-crawlers. The collected images were manually filtered on the basis of quality, size and content. The dataset has an average of around 70 images per category and a maximum of 99 images in a category. The primary challenge in creating this dataset was acquiring sufficient number of public domain images across all the 4 domains. Figure 2 depicts a sampling of 16 categories from the Office-Home dataset and Table 1 outlines some meta data for the dataset. The Acc. column in the Table  1 refers to classification accuracies using the LIBLINEAR SVM [15] classifier (5-fold cross validation) with deep fea-tures extracted using the VGG-F network. The dataset is publicly available for research 1 .\n\n\nExperiments\n\nIn this section we conduct extensive experiments to evaluate the DAH algorithm. Since we propose a domain adaptation technique based on hashing, we evaluate objection recognition accuracies for unsupervised domain adaptation and also study the discriminatory capability of the learned hash codes for unsupervised domain adaptive hashing. The implementation details are available at https://github.com/hemanthdv/da-hash\n\n\nDatasets\n\nOffice [42]: This is currently the most popular benchmark dataset for object recognition in the domain adaptation computer vision community. The dataset consists of images of everyday objects in an office environment. It has 3 domains; Amazon (A), Dslr (D) and Webcam (W). The dataset has around 4, 100 images with a majority of the images (2816 images) in the Amazon domain. We adopt the common evaluation protocol of different pairs of transfer tasks for this dataset [31,34]. We consider 6 transfer tasks for all combinations of source and target pairs for the 3 domains. Office-Home: We introduce this new dataset and evaluate it in a similar manner to the Office dataset. We consider 12 transfer tasks for the Art (Ar), Clipart (Cl), Product (Pr) and Real-World (Rw) domains for all combinations of source and target for the 4 domains. Considering all the different pairs of transfer enables us to evaluate the inherent bias between the domains in a comprehensive manner [45].\n\n\nImplementation Details\n\nWe implement the DAH using the MatConvnet framework [47]. Since we train a pre-trained VGG-F, we finetune the weights of conv1-conv5, fc6 and fc7. We set their learning rates to 1/10 th the learning rate of hash-fc8.\n\nWe vary the learning rate between 10 \u22124 to 10 \u22125 over 300 epochs with a momentum 0.9 and weight decay 5 \u00d7 10 \u22124 . We set K = 5 (number of samples from a category). Since we have 31 categories in the Office dataset, we get a source batch size of 31 \u00d7 5 = 155. For the target batch, we randomly select 155 samples. The total batch size turns out to be 310. For the Office-Home dataset, with K = 5 and 65 categories, we get a batch size of 650. We set d = 64 (hash code length) for all our experiments. Since there is imbalance in the number of like and unlike pairs in S, we set the values in similarity matrix S i,j \u2208 {0, 10}. Increasing the similarity weight of like-pairs improves the performance of DAH. For the entropy loss, we set \u03b7 = 1. For the MK-MMD loss, we follow the heuristics mentioned in [24], to determine the parameters. We estimate \u03b3, by validating a binary domain classifier to distinguish between source and target data points and select \u03b3 which gives largest error on a validation set. For MMD, we use a Gaussian kernel with a bandwidth \u03c3 given by the median of the pairwise distances in the training data. To incorporate the multi-kernel, we vary the bandwidth \u03c3 m \u2208 [2 \u22128 \u03c3, 2 8 \u03c3] with a multiplicative factor of 2. We define the target classifier f (x t i ) = p(y|h t i ) in terms of 6. The target data point is assigned to the class with the largest probability, with\u0177 i = max j (p ij ) using the hash codes for the source and the target.\n\n\nUnsupervised Domain Adaptation\n\nIn this section, we study the performance of the DAH for unsupervised domain adaptation, where labeled data is available only in the source domain and no labeled data is available in the target domain. We compare the DAH with state-of-the-art domain adaptation methods: (i) Geodesic Flow Kernel (GFK) [20], (ii) Transfer Component Analysis (TCA) [38], (iii) Correlation Alignment (CORAL) [44] and (iv) Joint Distribution Adaptation (JDA) [33]. We also compare the DAH with state-of-the-art deep learning methods for domain adaptation: (v) Deep Adaptation Network (DAN) [31] and (vi) Domain Adversarial Neural Network (DANN) [17]. For all of the shallow learning methods, we extract and use deep features from the fc7 layer of the VGG-F network that was pre-trained on the ImageNet 2012 dataset. We also evaluate the effect of the entropy loss on hashing for the DAH. The DAH-e is the DAH algorithm where \u03b7 is set to zero, which implies that the target hash values are not driven to align with the source categories. We follow the standard protocol for unsupervised domain adaptation, where all the labeled source data and all the unlabeled target data is used for training. Results and Discussion: The results are reported for the target classification in each of the transfer tasks in Tables 2  and 3, where accuracies denote the percentage of correctly Office-Home, DAH does best at extracting transferable features to achieve higher accuracies. We also note that DAH delivers better performance than DAH-e; thus, minimizing the entropy on the target data through 7 aids in improved alignment of the source and target samples, which boosts the accuracy.\n\nFeature Analysis: We also study the feature representations of the penultimate layer (fc7) outputs using t-SNE embeddings as in [12]. Figure 3a depicts the A-distance between domain pairs using Deep (VGG-F), DAN and DAH features. Ben-David et al. [2] defined A-distance as the distance between two domains that can be viewed as the discrepancy between two domains. Although it is difficult to estimate its exact value, an approximate distance measure is given by 2(1 \u2212 2\u01eb), where \u01eb is the generalization error for a binary classifier trained to distinguish between the two domains. We used a LIBLINEAR SVM [15] clas- \n\n\nUnsupervised Domain Adaptive Hashing\n\nIn this section, we study the performance of our algorithm to generate compact and efficient hash codes from the data for classifying unseen test instances, when no labels are available. This problem has been addressed in the literature, with promising empirical results [7,11,21]. However, in a real-world setting, labels may be available from a different, but related (source) domain; a strategy to utilize the labeled data from the source domain to learn representative hash codes for the target domain is therefore of immense practical importance. Our work is the first to identify and address this problem. We consider the following scenarios to address this real-world challenge: (i) No labels are available for a given dataset and the hash codes need to be learned in a completely unsupervised manner. We evaluate against baseline unsupervised hashing methods (ITQ) [22] and (KMeans) [25] and also state-of-the-art methods for unsupervised hashing (BA) [7] and (BDNN) [11]. (ii) Labeled data is available from a different, but related source domain. A hashing model is trained on the labeled source data and is used to learn hash codes for the target data. We refer to this method as NoDA, as no domain adaptation is performed. We used the deep pairwise-supervised hashing (DPSH) algorithm [30] to train a deep network with the source data and applied the network to generate hash codes for the target data. (iii) Labeled data is available from a different, but related source domain and we use our DAH formulation to learn hash codes for the target domain by reducing domain disparity. (iv) Labeled data is available in the target domain. This method falls under supervised hashing (SuH) (as it uses labeled data in the target domain to learn hash codes in the same domain) and denotes the upper bound on the performance. It is included to compare the performance of unsupervised hashing algorithms relative to the supervised algorithm. We used the DPSH algorithm [30] to train a deep network on the target data and used it to generate hash codes on a validation subset.\n\n\nResults and Discussion:\n\nWe applied the precision-recall curves and the mean average precision (mAP) measures to evaluate the efficacy of the hashing methods, similar to previous research [7,11,21]. The results are depicted in Figures 4 and 5 (precision-recall curves) and Table 4 (mAP values), where we present hashing with code length d = 64 bits. Hashing performance with d = 16 bits also follows a similar trend and is presented in the supplementary material. For the sake of brevity, we drop the results with Dslr as it is very similar to Webcam, with little domain difference. We note that the NoDA has the poorest performance due to domain mismatch. This demonstrates that domain disparity needs to be considered before deploying a hashing network to extract hash codes. The unsupervised hashing methods ITQ, KMeans, BA and BDNN perform slightly better compared to NoDA. The proposed DAH algorithm encompasses hash code learning and domain adaptation in a single integrated framework. It is thus able to leverage the labeled data in the source domain in a meaningful manner to learn efficient hash codes for the target domain. This accounts for its improved performance, as is evident in Figures 4 and 5 and Table 4. The supervised hashing technique (SuH) uses labels from the target and therefore depicts the best performance. The proposed DAH framework consistently delivers the best performance relative to SuH when compared with the other hashing procedures. This demonstrates the merit of our framework in learning representative hash codes by utilizing labeled data from a different domain. Such a framework will be immensely useful in a real-world setting.\n\n\nConclusions\n\nIn this paper, we have proposed a novel domain adaptive hashing (DAH) framework which exploits the feature learning capabilities of deep neural networks to learn efficient hash codes for unsupervised domain adaptation. The DAH framework solves two important practical problems: category assignment with weak supervision or insufficient labels (through domain adaptation) and the estimation of hash codes in an unsupervised setting (hash codes for target data). Thus, two practical challenges are addressed through a single integrated framework. This research is the first of its kind to integrate hash code learning with unsupervised domain adaptation. We also introduced a new dataset, Office-Home, which can be used to further research in domain adaptation.\n\n\nSupplementary Material\n\n\nLoss Function Derivative\n\nIn this section we outline the derivative of Equation 8 for the backpropagation algorithm;\nmin U J = L(U s ) + \u03b3M(U s , U t ) + \u03b7H(U s , U t ),(8)\nwhere, U := {U s \u222a U t } and (\u03b3, \u03b7) control the importance of domain adaptation (1) and target entropy loss (7) respectively. In the following subsections, we outline the derivative of the individual terms w.r.t. the input U .\n\n\nDerivative for MK-MMD\nM(U s , U t ) = l\u2208F d 2 k (U l s , U l t ),(1)d 2 k (U l s , U l t ) = E[\u03c6(u s,l )] \u2212 E[\u03c6(u t,l )] 2 H k .(2)\nWe implement the linear MK-MMD loss according to [24]. For this derivation, we consider the loss at just one layer. The derivative for the MK-MMD loss at every other layer can be derived in a similar manner. The output of i th source data point at layer l is represented as u i and the output of the i th target data point is represented as v i . For ease of representation, we drop the superscripts for the source (s), the target (t) and the layer (l). Unlike the conventional MMD loss which is O(n 2 ), the MK-MMD loss outlined in [24] is O(n) and can be estimated online (does not require all the data). The loss is calculated over every batch of data points during the back-propagation. Let n be the number of source data points U := {u i } n i=1 and the number of target data points V := {v i } n i=1 in the batch. We assume equal number of source and target data points in a batch and that n is even. The MK-MMD is defined over a set of 4 data points w i = [u 2i\u22121 , u 2i , v 2i\u22121 , v 2i ], \u2200i \u2208 {1, 2, . . . , n/2}. The MK-MMD is given by,\nM(U, V) = \u03ba m=1 \u03b2 m 1 n/2 n/2 i=1 h m (w i ),(9)\nwhere, \u03ba is the number of kernels and \u03b2 m = 1/\u03ba is the weight for each kernel and,\nh m (w i ) = k m (u 2i\u22121 , u 2i ) + k m (v 2i\u22121 , v 2i ) \u2212 k m (u 2i\u22121 , v 2i ) \u2212 k m (u 2i , v 2i\u22121 ),(10)\nwhere, k m (x, y) = exp \u2212 ||x\u2212y|| 2 2 \u03c3m . Re-writing the MK-MMD in terms of the kernels, we have,\nM(U, V) = 2 n\u03ba \u03ba m=1 n/2 i=1 k m (u 2i\u22121 , u 2i ) + k m (v 2i\u22121 , v 2i ) \u2212 k m (u 2i\u22121 , v 2i ) \u2212 k m (u 2i , v 2i\u22121 ) ,(11)\nWe now outline the derivative of 11 w.r.t. source output u q and target output v q . The derivative is,\n\u2202M \u2202u q = 2 n\u03ba \u03ba m=1 n/2 i=1 2 \u03c3 m k m (u 2i\u22121 , u 2i ).(u 2i\u22121 \u2212 u 2i ).(I{q = 2i} \u2212 I{q = 2i \u2212 1}) + 2 \u03c3 m k m (u 2i\u22121 , v 2i ).(u 2i\u22121 \u2212 v 2i ).I{q = 2i \u2212 1} + 2 \u03c3 m k m (u 2i , v 2i\u22121 ).(u 2i \u2212 v 2i\u22121 ).I{q = 2i} ,(12)\nwhere, I{.} is the indicator function which is 1 if the condition is true, else it is false. The derivative w.r.t. the target data output v q is,\n\u2202M \u2202v q = 2 n\u03ba \u03ba m=1 n/2 i=1 2 \u03c3 m k m (v 2i\u22121 , v 2i ).(v 2i\u22121 \u2212 v 2i ).(I{q = 2i} \u2212 I{q = 2i \u2212 1}) \u2212 2 \u03c3 m k m (u 2i\u22121 , v 2i ).(u 2i\u22121 \u2212 v 2i ).I{q = 2i} \u2212 2 \u03c3 m k m (u 2i , v 2i\u22121 ).(u 2i \u2212 v 2i\u22121 ).I{q = 2i \u2212 1} ,(13)\n\nDerivative for Supervised Hash Loss\n\nThe supervised hash loss is given by,\nmin U s L(U s ) = \u2212 sij \u2208S s ij u \u22a4 i u j \u2212 log 1 + exp(u \u22a4 i u j ) + ns i=1 u i \u2212 sgn(u i ) 2 2 .(5)\nThe partial derivative of 5 w.r.t. source data output u p is given by,\n\u2202L \u2202u q = sij \u2208S I{i = q} \u03c3(u \u22a4 i u j ) \u2212 s ij u j + I{j = q} \u03c3(u \u22a4 i u j ) \u2212 s ij u i + 2(u q \u2212 sgn(u q ))(14)\nwhere, \u03c3(x) = 1 1+exp(\u2212x) . We assume sgn(.) to be a constant and avoid the differentiability issues with sgn(.) at 0. Since the S is symmetric, we can reduce the derivative to,\n\u2202L \u2202u q = ns j=1 2 \u03c3(u \u22a4 q u j ) \u2212 s qj u j + 2 u q \u2212 sgn(u q ) .(15)\n\nDerivative for Unsupervised Entropy Loss\n\nWe outline the derivative of dH dU in the following section, where H is defined as,\nH(U s , U t ) = \u2212 1 n t nt i=1 C j=1 p ij log(p ij )(7)\nand p ij is the probability of target data output u t i belonging to category j, given by\np ij = K k=1 exp(u t i \u22a4 u sj k ) C l=1 K k \u2032 =1 exp(u t i \u22a4 u s l k \u2032 )(6)\nFor ease of representation, we will denote the target output u t i as v i and drop the superscript t. Similarly, we will denote the k th source data point in the j th category u sj k as u j k , by dropping the domain superscript. We define the probability p ij with the news terms as,\np ij = K k=1 exp(v i \u22a4 u j k ) C l=1 K k \u2032 =1 exp(v i \u22a4 u l k \u2032 )(16)\nFurther, we simplify by replacing exp(v \u22a4 i u j k ) with exp(i, jk). Equation 16 can now be represented as,\np ij = K k=1 exp(i, jk) C l=1 K k \u2032 =1 exp(i, lk \u2032 )(17)\nWe drop the outer summations (along with the -ve sign) and will reintroduce it at a later time. The entropy loss can be re-phrased using log( a b ) = log(a) -log(b) as,\nH ij = K k=1 exp(i, jk) C l=1 K k \u2032 =1 exp(i, lk \u2032 ) log K k=1 exp(i, jk) (18) \u2212 K k=1 exp(i, jk) C l=1 K k \u2032 =1 exp(i, lk \u2032 ) log C l=1 K k \u2032 =1 exp(i, lk \u2032 ) (19) k \u2032 (29) = log(p ij ) + 1 kp ijk u j k \u2212 log(p ij ) + 1 p ij l,k \u2032pijk \u2032 u l k \u2032 (30) = log(p ij ) + 1 kp ijk u j k \u2212 p ij l,k \u2032pijk \u2032 u l k \u2032(31)\nThe derivative of H w.r.t. target output v q is given by,\n\u2202H \u2202v q = \u2212 1 n t C j=1 log(p qj ) + 1 kp qjk u j k \u2212 p qj l,k \u2032pqjk \u2032 u l k \u2032(32)\nThe derivative of H w.r.t. the source outputs is given by 25 and w.r.t. the target outputs is given by 32.\n\n\nUnsupervised Domain Adaptation: Additional Results\n\nIn the main paper we had presented results for unsupervised domain adaptation based object recognition with d = 64 bits. Here, we outline the classification results with d = 16 (DAH-16) and d = 128 (DAH-128) bits for the Office-Home dataset in Table 5. We also present the (DAH-64), DAN and DANN results for comparison. There is an increase in the average recognition accuracy for d = 128 bits compared to d = 64 bits because of the increased capacity in representation. As expected, d = 16 has a lower recognition accuracy. \n\n\nUnsupervised Domain Adaptive Hashing: Additional Results\n\nWe provide the unsupervised domain adaptive hashing results for d = 16 and d = 128 bits in Figures 6 and 7 respectively. In Tables 6 and 7, we outline the corresponding mAP values. The notations are along the lines outlined in the main paper. We observe similar trends for both d = 16 and d = 128 bits compared to d = 64 bits. It is interesting to note that with increase in bit size d, the mAP does not necessarily increase. Table 7 (d = 64) has its mAP values lower than those for d = 64 (see main paper) for all the hashing methods. This indicates that merely increasing the hash code length does not always improve mAP scores. Also, the mAP values for Real-World for d = 128 bits has DAH performing better than SuH. This indicates that in some cases domain adaptation helps in learning a better generalized model.   \n\nFigure 1 :\n1The Domain Adaptive Hash (DAH) network that outputs hash codes for the source and the target. The network is trained with a batch of source and target data. The convolution layers conv1 -conv5 and the fully connected layers fc6 and fc7 are fine tuned from the VGG-F network. The MK-MMD loss trains the DAH to learn feature representations which align the source and the target. The hash-fc8 layer is trained to output vectors of d dimensions. The supervised hash loss drives the DAH to estimate a unique hash value for each object category. The unsupervised entropy loss aligns the target hash values to their corresponding source categories. Best viewed in color.\n\n\nFigure 1 illustrates the proposed DAH network.\n\nFigure 2 :\n2Sample images from the Office-Home dataset. The dataset consists of images of everyday objects organized into 4 domains; Art: paintings, sketches and/or artistic depictions, Clipart: clipart images, Product: images without background and Real-World: regular images captured with a camera. The figure displays examples from 16 of the 65 categories.\n\nFigure 3 :Figure 4 :Figure 5 :\n345Deep Features (Ar,Cl) (c) DAN Features (Ar,Cl) (d) DAH Features (Ar,Cl) Feature analysis of fc7 layer. (a) A-distances for Deep, DAN and DAH, (b), (c) and (d) t-SNE embeddings for 10 categories from Art (\u2022) and Clipart(+) domains. Best viewed in color. Precision-Recall curves @64 bits for the Office-Home dataset. Comparison of hashing without domain adaptation (NoDA), shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive hashing (DAH) and supervised hashing (SuH). Best viewed in color. Precision-Recall curves @64 bits for the Office dataset. Comparison of hashing without domain adaptation (NoDA), shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive hashing (DAH) and supervised hashing (SuH). Best viewed in color.\n\nFigure 6 :Figure 7 :\n67Precision-Recall curves @16 bits for the Office-Home dataset. Comparison of hashing without domain adaptation (NoDA), shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive hashing (DAH) and supervised hashing (SuH). Best viewed in color. Precision-Recall curves @128 bits for the Office-Home dataset. Comparison of hashing without domain adaptation (NoDA), shallow unsupervised hashing (ITQ, KMeans), state-of-the-art deep unsupervised hashing (BA, BDNN), unsupervised domain adaptive hashing (DAH) and supervised hashing (SuH). Best viewed in color.\n\nTable 1 :\n1Statistics for the Office-Home dataset. Min: # is the minimum number of images amongst all the categories, Min: Size and Max: Size are the minimum and maximum image sizes across all categories and Acc. is the classification accuracy. Supervised deep learning models require a large volume of labeled training data. Unfortunately, existing datasets for vision-based domain adaptation are limited in their size and are not suitable for validating deep learning algorithms. The standard datasets for vision based domain adaptation are, facial expression datasets CKPlusDomain. \nMin: # \nMin: Size \nMax: Size \nAcc \n\nArt \n15 \n117\u00d785 pix. 4384\u00d72686 pix. 44.99\u00b11.85 \nClipart \n39 \n18\u00d718 pix. 2400\u00d72400 pix. 53.95\u00b11.45 \nProduct \n38 \n75\u00d763 pix. 2560\u00d72560 pix. 66.41\u00b11.18 \nReal-World \n23 \n88\u00d780 pix. 6500\u00d74900 pix. 59.70\u00b11.04 \n\n4. The Office-Home Dataset \n\n\n\nTable 2 :\n2Recognition accuracies (%) for domain adaptation experiments on the Office dataset. {Amazon (A), Dslr (D), Webcam (W)}. A\u2192W implies A is source and W is target.classified target data samples. We present results with hash length d = 64 bits. The DAH algorithm consistently outperforms the baselines across all the domains for the Office-Home dataset. However, DANN marginally surpasses DAH for the Office dataset, prompting us to reason that domain adversarial training is more effective than DAH when the categories are fewer in number. Since domain alignment is category agnostic, it is possible that the aligned domains are not classification friendly in the presence of large number of categories. When the number of categories is large, as inExpt. \nA\u2192D A\u2192W D\u2192A D\u2192W W\u2192A W\u2192D Avg. \nGFK \n48.59 \n52.08 \n41.83 \n89.18 \n49.04 \n93.17 62.32 \nTCA \n51.00 \n49.43 \n48.12 \n93.08 \n48.83 \n96.79 64.54 \nCORAL \n54.42 \n51.70 \n48.26 \n95.97 \n47.27 \n98.59 66.04 \nJDA \n59.24 \n58.62 \n51.35 \n96.86 \n52.34 \n97.79 69.37 \nDAN \n67.04 \n67.80 \n50.36 \n95.85 \n52.33 \n99.40 72.13 \nDANN \n72.89 \n72.70 \n56.25 \n96.48 \n53.20 \n99.40 75.15 \nDAH-e \n66.27 \n66.16 \n55.97 \n94.59 \n53.91 \n96.99 72.31 \nDAH \n66.47 \n68.30 \n55.54 \n96.10 \n53.02 \n98.80 73.04 \n\n\n\nTable 3 :\n3Recognition accuracies (%) for domain adaptation experiments on the Office-Home dataset. {Art (Ar), Clipart (Cl), Product (Pr), Real-World (Rw)}. Ar\u2192Cl implies Ar is source and Cl is target.Expt. \nAr\u2192Cl Ar\u2192Pr Ar\u2192Rw Cl\u2192Ar Cl\u2192Pr Cl\u2192Rw Pr\u2192Ar Pr\u2192Cl Pr\u2192Rw Rw\u2192Ar Rw\u2192Cl Rw\u2192Pr Avg. \nGFK \n21.60 \n31.72 \n38.83 \n21.63 \n34.94 \n34.20 \n24.52 \n25.73 \n42.92 \n32.88 \n28.96 \n50.89 \n32.40 \nTCA \n19.93 \n32.08 \n35.71 \n19.00 \n31.36 \n31.74 \n21.92 \n23.64 \n42.12 \n30.74 \n27.15 \n48.68 \n30.34 \nCORAL \n27.10 \n36.16 \n44.32 \n26.08 \n40.03 \n40.33 \n27.77 \n30.54 \n50.61 \n38.48 \n36.36 \n57.11 \n37.91 \nJDA \n25.34 \n35.98 \n42.94 \n24.52 \n40.19 \n40.90 \n25.96 \n32.72 \n49.25 \n35.10 \n35.35 \n55.35 \n36.97 \nDAN \n30.66 \n42.17 \n54.13 \n32.83 \n47.59 \n49.78 \n29.07 \n34.05 \n56.70 \n43.58 \n38.25 \n62.73 \n43.46 \nDANN \n33.33 \n42.96 \n54.42 \n32.26 \n49.13 \n49.76 \n30.49 \n38.14 \n56.76 \n44.71 \n42.66 \n64.65 \n44.94 \nDAH-e \n29.23 \n35.71 \n48.29 \n33.79 \n48.23 \n47.49 \n29.87 \n38.76 \n55.63 \n41.16 \n44.99 \n59.07 \n42.69 \nDAH \n31.64 \n40.75 \n51.73 \n34.69 \n51.93 \n52.79 \n29.91 \n39.63 \n60.71 \n44.99 \n45.13 \n62.54 \n45.54 \n\nsifier with 5-fold cross-validation to estimate \u01eb. Figure 3a \nindicates that the DAH features have the least discrepancy \nbetween the source and target compared to DAN and Deep \nfeatures. This is also confirmed with the t-SNE embeddings \nin Figures 3b-3d. The Deep features show very little over-\nlap between the domains and the categories depict minimal \nclustering. Domain overlap and clustering improves as we \nmove to DAN and DAH features, with DAH providing the \nbest visualizations. This corroborates the efficacy of the \nDAH algorithm to exploit the feature learning capabilities \nof deep neural networks to learn representative hash codes \nto address domain adaptation. \n\n\n\nTable 4 :\n4Mean average precision @64 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and Real-World and Clipart is the source domain for Art. Similarly, Amazon and Webcam are source target pairs.Expt. \nNoDA ITQ KMeans \nBA \nBDNN DAH SuH \n\nAmazon \n0.324 0.465 \n0.403 \n0.367 0.491 0.582 0.830 \nWebcam \n0.511 0.652 \n0.558 \n0.480 0.656 0.717 0.939 \nArt \n0.155 0.191 \n0.170 \n0.156 0.193 0.302 0.492 \nClipart \n0.160 0.195 \n0.178 \n0.179 0.206 0.333 0.622 \nProduct \n0.239 0.393 \n0.341 \n0.349 0.407 0.414 0.774 \nReal-World \n0.281 0.323 \n0.279 \n0.273 0.336 0.533 0.586 \nAvg. \n0.278 0.370 \n0.322 \n0.301 0.382 0.480 0.707 \n\n\n\nTable 5 :\n5Recognition accuracies (%) for domain adaptation experiments on the Office-Home dataset. {Art (Ar), Clipart (Cl), Product (Pr), Real-World (Rw)}. Ar\u2192Cl implies Ar is source and Cl is target. . Ar\u2192Cl Ar\u2192Pr Ar\u2192Rw Cl\u2192Ar Cl\u2192Pr Cl\u2192Rw Pr\u2192Ar Pr\u2192Cl Pr\u2192Rw Rw\u2192Ar Rw\u2192Cl Rw\u2192Pr Avg.ExptDAN \n30.66 \n42.17 \n54.13 \n32.83 \n47.59 \n49.78 \n29.07 \n34.05 \n56.70 \n43.58 \n38.25 \n62.73 \n43.46 \nDANN \n33.33 \n42.96 \n54.42 \n32.26 \n49.13 \n49.76 \n30.49 \n38.14 \n56.76 \n44.71 \n42.66 \n64.65 \n44.94 \nDAH-16 \n23.83 \n30.32 \n40.14 \n25.67 \n38.79 \n33.26 \n20.11 \n27.72 \n40.90 \n32.63 \n25.54 \n37.46 \n31.36 \nDAH-64 \n31.64 \n40.75 \n51.73 \n34.69 \n51.93 \n52.79 \n29.91 \n39.63 \n60.71 \n44.99 \n45.13 \n62.54 \n45.54 \nDAH-128 \n32.58 \n40.64 \n52.40 \n35.72 \n52.80 \n52.12 \n30.94 \n41.31 \n59.31 \n45.65 \n46.67 \n64.97 \n46.26 \n\n\n\nTable 6 :\n6Mean average precision @16 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and Real-World and Clipart is the source domain for Art.Expt. \nNoDA ITQ KMeans \nBA \nBDNN DAH SuH \n\nArt \n0.102 0.147 \n0.133 \n0.131 0.151 0.207 0.381 \nClipart \n0.110 0.120 \n0.116 \n0.123 0.138 0.211 0.412 \nProduct \n0.134 0.253 \n0.241 \n0.253 0.313 0.257 0.459 \nReal-World \n0.193 0.225 \n0.195 \n0.216 0.248 0.371 0.400 \nAvg. \n0.135 0.186 \n0.171 \n0.181 0.212 0.262 0.413 \n\n\n\nTable 7 :\n7Mean average precision @128 bits. For the NoDA and DAH results, Art is the source domain for Clipart, Product and Real-World and Clipart is the source domain for Art.Expt. \nNoDA ITQ KMeans \nBA \nBDNN DAH SuH \n\nArt \n0.154 0.202 \n0.175 \n0.148 0.207 0.314 0.444 \nClipart \n0.186 0.210 \n0.196 \n0.187 0.213 0.350 0.346 \nProduct \n0.279 0.416 \n0.356 \n0.336 0.432 0.424 0.792 \nReal-World \n0.308 0.343 \n0.289 \n0.258 0.348 0.544 0.458 \nAvg. \n0.232 0.293 \n0.254 \n0.232 0.300 0.408 0.510 \n\n\nhttps://hemanthdv.github.io/officehome-dataset/\nAcknowledgements: This material is based upon work supported by the National Science Foundation (NSF) under Grant No:1116360. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF.We need to estimate both, \u2202Hij \u2202vi for the target and \u2202Hij \u2202u p q for the source. We refer to \u2202u p q for a consistent reference to source data. The derivative \u2202Hij \u2202u p q for 18 is,where, I{.} is an indicator function which is 1 only when both the conditions within are true, else it is 0. The derivative \u2202Hij \u2202u p q for 19 is,19, and definingp ijk =The derivative of H w.r.t the source output u p q is given by,We now outline the derivative \u2202H \u2202vi for 18 as,and the derivative \u2202H \u2202vi for 19 as,= log k exp(i, jk) kp ijk u j k \u2212 log l,k \u2032 exp(i, lk \u2032 ) kp ijk u j k + kp ijk u j k \u2212 p ij l,k \u2032pijk \u2032 u l k \u2032 \u2212 p ij log k exp(i, jk) l,k \u2032pijk \u2032 u l k \u2032 + p ij log l,k \u2032 exp(i, lk \u2032 ) l,k \u2032pijk \u2032 u l\nTabula rasa: Model transfer for object category detection. Y Aytar, A Zisserman, IEEE ICCV. Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for object category detection. In IEEE ICCV, 2011. 2\n\nA theory of learning from different domains. S Ben-David, J Blitzer, K Crammer, A Kulesza, F Pereira, J W Vaughan, Machine learning. 791-2S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of learning from different do- mains. Machine learning, 79(1-2):151-175, 2010. 6\n\nRepresentation learning: A review and new perspectives. Y Bengio, A Courville, P Vincent, IEEE transactions on pattern analysis and machine intelligence. 35Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798- 1828, 2013. 2\n\nDomain adaptation problems: A dasvm classification technique and a circular validation strategy. L Bruzzone, M Marconcini, IEEE. 325L. Bruzzone and M. Marconcini. Domain adaptation prob- lems: A dasvm classification technique and a circular valida- tion strategy. IEEE, PAMI, 32(5):770-787, 2010. 2\n\nDeep visual-semantic hashing for cross-modal retrieval. Y Cao, M Long, J Wang, Q Yang, P S Yu, ACM-SIGKDD. Y. Cao, M. Long, J. Wang, Q. Yang, and P. S. Yu. Deep visual-semantic hashing for cross-modal retrieval. In ACM- SIGKDD, 2016. 2\n\nTransitive hashing network for heterogeneous multimedia retrieval. Z Cao, M Long, Q Yang, AAAI. Z. Cao, M. Long, and Q. Yang. Transitive hashing network for heterogeneous multimedia retrieval. In AAAI, 2016. 2\n\nHashing with binary autoencoders. M A Carreira-Perpin\u00e1n, R Raziperchikolaei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition27M. A. Carreira-Perpin\u00e1n and R. Raziperchikolaei. Hashing with binary autoencoders. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 557-566, 2015. 2, 7\n\nReturn of the devil in the details: Delving deep into convolutional nets. K Chatfield, K Simonyan, A Vedaldi, A Zisserman, BMVC. 24K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep into convo- lutional nets. In BMVC, 2014. 2, 4\n\nMultisource domain adaptation and its application to early detection of fatigue. R Chattopadhyay, Q Sun, W Fan, I Davidson, S Panchanathan, J Ye, ACM Transactions on Knowledge Discovery from Data (TKDD). 6418R. Chattopadhyay, Q. Sun, W. Fan, I. Davidson, S. Pan- chanathan, and J. Ye. Multisource domain adaptation and its application to early detection of fatigue. ACM Transac- tions on Knowledge Discovery from Data (TKDD), 6(4):18, 2012. 2\n\nSelective transfer machine for personalized facial action unit detection. W.-S Chu, F De La Torre, J F Cohn, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionW.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer machine for personalized facial action unit detection. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3515-3522, 2013. 2\n\nLearning to hash with binary deep neural network. T.-T Do, A.-D Doan, N.-M Cheung, European Conference on Computer Vision. Springer27T.-T. Do, A.-D. Doan, and N.-M. Cheung. Learning to hash with binary deep neural network. In European Conference on Computer Vision, pages 219-234. Springer, 2016. 2, 7\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, ICML. 26J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti- vation feature for generic visual recognition. In ICML, pages 647-655, 2014. 2, 6\n\nDomain transfer multiple kernel learning. L Duan, I W Tsang, D Xu, IEEE PAMI. 343L. Duan, I. W. Tsang, and D. Xu. Domain transfer multiple kernel learning. IEEE PAMI, 34(3):465-479, 2012. 2\n\nDeep hashing for compact binary codes learning. V Erin Liong, J Lu, G Wang, P Moulin, J Zhou, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionV. Erin Liong, J. Lu, G. Wang, P. Moulin, and J. Zhou. Deep hashing for compact binary codes learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2475-2483, 2015. 2\n\nLiblinear: A library for large linear classification. R.-E Fan, K.-W Chang, C.-J Hsieh, X.-R Wang, C.-J Lin, Journal of machine learning research. 96R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.- J. Lin. Liblinear: A library for large linear classification. Journal of machine learning research, 9(Aug):1871-1874, 2008. 5, 6\n\nUnsupervised visual domain adaptation using subspace alignment. B Fernando, A Habrard, M Sebban, T Tuytelaars, CVPR. 1B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars. Un- supervised visual domain adaptation using subspace align- ment. In CVPR, pages 2960-2967, 2013. 1, 2\n\nDomainadversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, Journal of Machine Learning Research. 17596Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domain- adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1-35, 2016. 1, 2, 3, 6\n\nDomain adaptation for large-scale sentiment classification: A deep learning approach. X Glorot, A Bordes, Y Bengio, Proceedings of the 28th International Conference on Machine Learning (ICML-11). the 28th International Conference on Machine Learning (ICML-11)1X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for large-scale sentiment classification: A deep learning ap- proach. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 513-520, 2011. 1, 2\n\nConnecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. B Gong, K Grauman, F Sha, ICML (1). B. Gong, K. Grauman, and F. Sha. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. In ICML (1), pages 222-230, 2013. 2\n\nGeodesic flow kernel for unsupervised domain adaptation. B Gong, Y Shi, F Sha, K Grauman, IEEE CVPR. 6B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In IEEE CVPR, 2012. 1, 2, 5, 6\n\nIterative quantization: A procrustean approach to learning binary codes. Y Gong, S Lazebnik, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEEY. Gong and S. Lazebnik. Iterative quantization: A pro- crustean approach to learning binary codes. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Confer- ence on, pages 817-824. IEEE, 2011. 7\n\nIterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval. Y Gong, S Lazebnik, A Gordo, F Perronnin, IEEE Transactions on Pattern Analysis and Machine Intelligence. 35127Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin. Itera- tive quantization: A procrustean approach to learning binary codes for large-scale image retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12):2916- 2929, 2013. 2, 4, 7\n\nDomain adaptation for object recognition: An unsupervised approach. R Gopalan, R Li, R Chellappa, 2011 international conference on computer vision. R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised approach. In 2011 in- ternational conference on computer vision, pages 999-1006. IEEE, 2011. 2\n\nOptimal kernel choice for large-scale two-sample tests. A Gretton, D Sejdinovic, H Strathmann, S Balakrishnan, M Pontil, K Fukumizu, B K Sriperumbudur, Advances in neural information processing systems. 511A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, K. Fukumizu, and B. K. Sriperumbudur. Optimal kernel choice for large-scale two-sample tests. In Advances in neural information processing systems, pages 1205-1213, 2012. 3, 4, 5, 11\n\nK-means hashing: An affinitypreserving quantization method for learning binary compact codes. K He, F Wen, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition7K. He, F. Wen, and J. Sun. K-means hashing: An affinity- preserving quantization method for learning binary compact codes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2938-2945, 2013. 2, 7\n\nGradient flow in recurrent nets: the difficulty of learning long-term dependencies. S Hochreiter, Y Bengio, P Frasconi, J Schmidhuber, S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. 4\n\nEfficient learning of domain-invariant image representations. J Hoffman, E Rodner, J Donahue, K Saenko, T Darrell, ICLR. J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Dar- rell. Efficient learning of domain-invariant image represen- tations. In ICLR, 2013. 2\n\nWhat is the best multi-stage architecture for object recognition?. K Jarrett, K Kavukcuoglu, Y Lecun, 2009 IEEE 12th International Conference on Computer Vision. IEEEK. Jarrett, K. Kavukcuoglu, Y. Lecun, et al. What is the best multi-stage architecture for object recognition? In 2009 IEEE 12th International Conference on Computer Vision, pages 2146-2153. IEEE, 2009. 5\n\nQ.-Y Jiang, W.-J Li, arXiv:1602.02255Deep cross-modal hashing. arXiv preprintQ.-Y. Jiang and W.-J. Li. Deep cross-modal hashing. arXiv preprint arXiv:1602.02255, 2016. 2\n\nFeature learning based deep supervised hashing with pairwise labels. W.-J Li, S Wang, W.-C Kang, IJCAI. 47W.-J. Li, S. Wang, and W.-C. Kang. Feature learning based deep supervised hashing with pairwise labels. In IJCAI, 2016, 2016. 4, 7\n\nLearning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, M Jordan, ICML. 56M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transfer- able features with deep adaptation networks. In ICML, pages 97-105, 2015. 1, 2, 3, 4, 5, 6\n\nTransfer joint matching for unsupervised domain adaptation. M Long, J Wang, G Ding, J Sun, P Yu, CVPR. M. Long, J. Wang, G. Ding, J. Sun, and P. Yu. Transfer joint matching for unsupervised domain adaptation. In CVPR, pages 1410-1417, 2014. 2\n\nTransfer feature learning with joint distribution adaptation. M Long, J Wang, G Ding, J Sun, P S Yu, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision6M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu. Transfer feature learning with joint distribution adaptation. In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 2200-2207, 2013. 1, 2, 5, 6\n\nUnsupervised domain adaptation with residual transfer networks. M Long, H Zhu, J Wang, M I Jordan, NIPS. M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks. In NIPS, 2016. 1, 2, 3, 5\n\nThe extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. P Lucey, J F Cohn, T Kanade, J Saragih, Z Ambadar, I Matthews, CVPR. IEEEP. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews. The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified ex- pression. In CVPR, pages 94-101. IEEE, 2010. 5\n\nReading digits in natural images with unsupervised feature learning. Y Netzer, T Wang, A Coates, A Bissacco, B Wu, A Y Ng, NIPS Workshop on Deep Learning and Unsupervised Feature Learning. Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised fea- ture learning. In NIPS Workshop on Deep Learning and Un- supervised Feature Learning 2011, 2011. 5\n\nLearning and transferring mid-level image representations using convolutional neural networks. M Oquab, L Bottou, I Laptev, J Sivic, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionM. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations using convolu- tional neural networks. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 1717-1724, 2014. 2\n\nDomain adaptation via transfer component analysis. Neural Networks. S J Pan, I W Tsang, J T Kwok, Q Yang, IEEE Trans. on. 2226S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. Neural Net- works, IEEE Trans. on, 22(2):199-210, 2011. 1, 2, 6\n\nA survey on transfer learning. S J Pan, Q Yang, IEEE TKDE. 2210S. J. Pan and Q. Yang. A survey on transfer learning. IEEE TKDE, 22(10):1345-1359, 2010. 2\n\nWebbased database for facial expression analysis. M Pantic, M Valstar, R Rademaker, L Maat, ICME. IEEEM. Pantic, M. Valstar, R. Rademaker, and L. Maat. Web- based database for facial expression analysis. In ICME. IEEE, 2005. 5\n\nVisual domain adaptation: A survey of recent advances. V M Patel, R Gopalan, R Li, R Chellappa, IEEE signal processing magazine. 323V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual do- main adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53-69, 2015. 2\n\nAdapting visual category models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, ECCV. K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting vi- sual category models to new domains. In ECCV, 2010. 1, 2, 5\n\nGeneralized domain-adaptive dictionaries. S Shekhar, V M Patel, H V Nguyen, R Chellappa, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS. Shekhar, V. M. Patel, H. V. Nguyen, and R. Chellappa. Generalized domain-adaptive dictionaries. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 361-368, 2013. 1\n\nReturn of frustratingly easy domain adaptation. B Sun, J Feng, K Saenko, ICCV, TASK-CV. 6B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In ICCV, TASK-CV, 2015. 1, 2, 6\n\nUnbiased look at dataset bias. A Torralba, A A Efros, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEEA. Torralba and A. A. Efros. Unbiased look at dataset bias. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1521-1528. IEEE, 2011. 5\n\nSimultaneous deep transfer across domains and tasks. E Tzeng, J Hoffman, T Darrell, K Saenko, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision13E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultane- ous deep transfer across domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4068-4076, 2015. 1, 2, 3\n\nMatconvnet -convolutional neural networks for matlab. A Vedaldi, K Lenc, Proceeding of the ACM Int. Conf. on Multimedia. eeding of the ACM Int. Conf. on MultimediaA. Vedaldi and K. Lenc. Matconvnet -convolutional neural networks for matlab. In Proceeding of the ACM Int. Conf. on Multimedia, 2015. 5\n\nHashing for similarity search: A survey. J Wang, H T Shen, J Song, J Ji, arXiv:1408.2927arXiv preprintJ. Wang, H. T. Shen, J. Song, and J. Ji. Hashing for similarity search: A survey. arXiv preprint arXiv:1408.2927, 2014. 1\n\nHow transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, Advances in neural information processing systems. 23J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans- ferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320-3328, 2014. 2, 3\n\nDeep hashing network for efficient similarity retrieval. H Zhu, M Long, J Wang, Y Cao, Thirtieth AAAI Conference on Artificial Intelligence. H. Zhu, M. Long, J. Wang, and Y. Cao. Deep hashing net- work for efficient similarity retrieval. In Thirtieth AAAI Con- ference on Artificial Intelligence, 2016. 4\n", "annotations": {"author": "[{\"end\":187,\"start\":70},{\"end\":297,\"start\":188},{\"end\":423,\"start\":298},{\"end\":527,\"start\":424}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":78},{\"end\":200,\"start\":193},{\"end\":316,\"start\":305},{\"end\":447,\"start\":435}]", "author_first_name": "[{\"end\":77,\"start\":70},{\"end\":192,\"start\":188},{\"end\":304,\"start\":298},{\"end\":434,\"start\":424}]", "author_affiliation": "[{\"end\":186,\"start\":109},{\"end\":296,\"start\":219},{\"end\":422,\"start\":345},{\"end\":526,\"start\":449}]", "title": "[{\"end\":56,\"start\":1},{\"end\":583,\"start\":528}]", "venue": null, "abstract": "[{\"end\":2138,\"start\":596}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2851,\"start\":2847},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3247,\"start\":3243},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3250,\"start\":3247},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3253,\"start\":3250},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3256,\"start\":3253},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3259,\"start\":3256},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3262,\"start\":3259},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3265,\"start\":3262},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3506,\"start\":3502},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3509,\"start\":3506},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3512,\"start\":3509},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3515,\"start\":3512},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3518,\"start\":3515},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3823,\"start\":3819},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5953,\"start\":5950},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5955,\"start\":5953},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6070,\"start\":6066},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6073,\"start\":6070},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6245,\"start\":6242},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6248,\"start\":6245},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6251,\"start\":6248},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6409,\"start\":6405},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6412,\"start\":6409},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6646,\"start\":6642},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6733,\"start\":6729},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6736,\"start\":6733},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6739,\"start\":6736},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6952,\"start\":6948},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6955,\"start\":6952},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7045,\"start\":7041},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7124,\"start\":7120},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7396,\"start\":7393},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7579,\"start\":7576},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7702,\"start\":7698},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7705,\"start\":7702},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7708,\"start\":7705},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7730,\"start\":7726},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8016,\"start\":8012},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8019,\"start\":8016},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8085,\"start\":8081},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8217,\"start\":8213},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8457,\"start\":8453},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8460,\"start\":8457},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8562,\"start\":8559},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8565,\"start\":8562},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8568,\"start\":8565},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8779,\"start\":8776},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8781,\"start\":8779},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8784,\"start\":8781},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11591,\"start\":11587},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11594,\"start\":11591},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11597,\"start\":11594},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11600,\"start\":11597},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11731,\"start\":11727},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12200,\"start\":12196},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12203,\"start\":12200},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12688,\"start\":12684},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13102,\"start\":13098},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14970,\"start\":14966},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14973,\"start\":14970},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":15515,\"start\":15511},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15788,\"start\":15784},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18626,\"start\":18623},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18920,\"start\":18916},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18929,\"start\":18925},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19272,\"start\":19269},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19807,\"start\":19803},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20132,\"start\":20128},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20135,\"start\":20132},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20143,\"start\":20139},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20156,\"start\":20152},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20182,\"start\":20178},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20203,\"start\":20199},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20244,\"start\":20240},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20283,\"start\":20279},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20296,\"start\":20292},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20320,\"start\":20316},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":22094,\"start\":22090},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22695,\"start\":22691},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23158,\"start\":23154},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":23161,\"start\":23158},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23664,\"start\":23660},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23748,\"start\":23744},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24715,\"start\":24711},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25712,\"start\":25708},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":25757,\"start\":25753},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":25799,\"start\":25795},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25849,\"start\":25845},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":25980,\"start\":25976},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":26035,\"start\":26031},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27196,\"start\":27192},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27314,\"start\":27311},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27674,\"start\":27670},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27996,\"start\":27993},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27999,\"start\":27996},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28002,\"start\":27999},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28599,\"start\":28595},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28617,\"start\":28613},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28685,\"start\":28682},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28701,\"start\":28697},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29023,\"start\":29019},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":29698,\"start\":29694},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29994,\"start\":29991},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29997,\"start\":29994},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30000,\"start\":29997},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32863,\"start\":32859},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33347,\"start\":33343},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36410,\"start\":36408}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39362,\"start\":38685},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39411,\"start\":39363},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39772,\"start\":39412},{\"attributes\":{\"id\":\"fig_3\"},\"end\":40679,\"start\":39773},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41328,\"start\":40680},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42186,\"start\":41329},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43412,\"start\":42187},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45154,\"start\":43413},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45802,\"start\":45155},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46580,\"start\":45803},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":47068,\"start\":46581},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47557,\"start\":47069}]", "paragraph": "[{\"end\":3519,\"start\":2154},{\"end\":4525,\"start\":3521},{\"end\":5704,\"start\":4527},{\"end\":7125,\"start\":5721},{\"end\":8326,\"start\":7127},{\"end\":9164,\"start\":8328},{\"end\":11455,\"start\":9201},{\"end\":12245,\"start\":11485},{\"end\":12842,\"start\":12293},{\"end\":13133,\"start\":12907},{\"end\":13289,\"start\":13172},{\"end\":13902,\"start\":13344},{\"end\":14681,\"start\":13981},{\"end\":15105,\"start\":14764},{\"end\":15904,\"start\":15184},{\"end\":16862,\"start\":16046},{\"end\":17643,\"start\":16934},{\"end\":18110,\"start\":17700},{\"end\":19538,\"start\":18143},{\"end\":19988,\"start\":19676},{\"end\":20630,\"start\":20021},{\"end\":21261,\"start\":20632},{\"end\":22237,\"start\":21263},{\"end\":22671,\"start\":22253},{\"end\":23665,\"start\":22684},{\"end\":23908,\"start\":23692},{\"end\":25372,\"start\":23910},{\"end\":27062,\"start\":25407},{\"end\":27681,\"start\":27064},{\"end\":29800,\"start\":27722},{\"end\":31473,\"start\":29828},{\"end\":32248,\"start\":31489},{\"end\":32392,\"start\":32302},{\"end\":32675,\"start\":32449},{\"end\":33856,\"start\":32810},{\"end\":33988,\"start\":33906},{\"end\":34195,\"start\":34097},{\"end\":34424,\"start\":34321},{\"end\":34793,\"start\":34648},{\"end\":35092,\"start\":35055},{\"end\":35265,\"start\":35195},{\"end\":35555,\"start\":35378},{\"end\":35752,\"start\":35669},{\"end\":35898,\"start\":35809},{\"end\":36259,\"start\":35975},{\"end\":36437,\"start\":36330},{\"end\":36663,\"start\":36495},{\"end\":37033,\"start\":36976},{\"end\":37223,\"start\":37117},{\"end\":37803,\"start\":37278},{\"end\":38684,\"start\":37864}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12292,\"start\":12246},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12906,\"start\":12843},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13343,\"start\":13290},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13980,\"start\":13903},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14763,\"start\":14682},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15183,\"start\":15106},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16006,\"start\":15905},{\"attributes\":{\"id\":\"formula_7\"},\"end\":16933,\"start\":16863},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17699,\"start\":17644},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19675,\"start\":19539},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20020,\"start\":19989},{\"attributes\":{\"id\":\"formula_11\"},\"end\":32448,\"start\":32393},{\"attributes\":{\"id\":\"formula_12\"},\"end\":32746,\"start\":32700},{\"attributes\":{\"id\":\"formula_13\"},\"end\":32809,\"start\":32746},{\"attributes\":{\"id\":\"formula_14\"},\"end\":33905,\"start\":33857},{\"attributes\":{\"id\":\"formula_15\"},\"end\":34096,\"start\":33989},{\"attributes\":{\"id\":\"formula_16\"},\"end\":34320,\"start\":34196},{\"attributes\":{\"id\":\"formula_17\"},\"end\":34647,\"start\":34425},{\"attributes\":{\"id\":\"formula_18\"},\"end\":35016,\"start\":34794},{\"attributes\":{\"id\":\"formula_19\"},\"end\":35194,\"start\":35093},{\"attributes\":{\"id\":\"formula_20\"},\"end\":35377,\"start\":35266},{\"attributes\":{\"id\":\"formula_21\"},\"end\":35625,\"start\":35556},{\"attributes\":{\"id\":\"formula_22\"},\"end\":35808,\"start\":35753},{\"attributes\":{\"id\":\"formula_23\"},\"end\":35974,\"start\":35899},{\"attributes\":{\"id\":\"formula_24\"},\"end\":36329,\"start\":36260},{\"attributes\":{\"id\":\"formula_25\"},\"end\":36494,\"start\":36438},{\"attributes\":{\"id\":\"formula_26\"},\"end\":36975,\"start\":36664},{\"attributes\":{\"id\":\"formula_27\"},\"end\":37116,\"start\":37034}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":21956,\"start\":21949},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22029,\"start\":22021},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26708,\"start\":26693},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30083,\"start\":30076},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31025,\"start\":31018},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":37529,\"start\":37522},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":38002,\"start\":37988},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":38297,\"start\":38290}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2152,\"start\":2140},{\"attributes\":{\"n\":\"2.\"},\"end\":5719,\"start\":5707},{\"attributes\":{\"n\":\"3.\"},\"end\":9199,\"start\":9167},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11483,\"start\":11458},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13170,\"start\":13136},{\"attributes\":{\"n\":\"3.3.\"},\"end\":16044,\"start\":16008},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18141,\"start\":18113},{\"attributes\":{\"n\":\"5.\"},\"end\":22251,\"start\":22240},{\"attributes\":{\"n\":\"5.1.\"},\"end\":22682,\"start\":22674},{\"attributes\":{\"n\":\"5.2.\"},\"end\":23690,\"start\":23668},{\"attributes\":{\"n\":\"5.3.\"},\"end\":25405,\"start\":25375},{\"attributes\":{\"n\":\"5.4.\"},\"end\":27720,\"start\":27684},{\"end\":29826,\"start\":29803},{\"attributes\":{\"n\":\"6.\"},\"end\":31487,\"start\":31476},{\"end\":32273,\"start\":32251},{\"attributes\":{\"n\":\"7.\"},\"end\":32300,\"start\":32276},{\"attributes\":{\"n\":\"7.1.\"},\"end\":32699,\"start\":32678},{\"attributes\":{\"n\":\"7.2.\"},\"end\":35053,\"start\":35018},{\"attributes\":{\"n\":\"7.3.\"},\"end\":35667,\"start\":35627},{\"attributes\":{\"n\":\"8.\"},\"end\":37276,\"start\":37226},{\"attributes\":{\"n\":\"9.\"},\"end\":37862,\"start\":37806},{\"end\":38696,\"start\":38686},{\"end\":39423,\"start\":39413},{\"end\":39804,\"start\":39774},{\"end\":40701,\"start\":40681},{\"end\":41339,\"start\":41330},{\"end\":42197,\"start\":42188},{\"end\":43423,\"start\":43414},{\"end\":45165,\"start\":45156},{\"end\":45813,\"start\":45804},{\"end\":46591,\"start\":46582},{\"end\":47079,\"start\":47070}]", "table": "[{\"end\":42186,\"start\":41907},{\"end\":43412,\"start\":42945},{\"end\":45154,\"start\":43615},{\"end\":45802,\"start\":45386},{\"end\":46580,\"start\":46084},{\"end\":47068,\"start\":46758},{\"end\":47557,\"start\":47247}]", "figure_caption": "[{\"end\":39362,\"start\":38698},{\"end\":39411,\"start\":39365},{\"end\":39772,\"start\":39425},{\"end\":40679,\"start\":39808},{\"end\":41328,\"start\":40704},{\"end\":41907,\"start\":41341},{\"end\":42945,\"start\":42199},{\"end\":43615,\"start\":43425},{\"end\":45386,\"start\":45167},{\"end\":46084,\"start\":45815},{\"end\":46758,\"start\":46593},{\"end\":47247,\"start\":47081}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5619,\"start\":5611},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21879,\"start\":21871},{\"end\":27207,\"start\":27198},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27536,\"start\":27524},{\"end\":30039,\"start\":30030},{\"end\":31013,\"start\":30998},{\"end\":37970,\"start\":37955}]", "bib_author_first_name": "[{\"end\":48654,\"start\":48653},{\"end\":48663,\"start\":48662},{\"end\":48841,\"start\":48840},{\"end\":48854,\"start\":48853},{\"end\":48865,\"start\":48864},{\"end\":48876,\"start\":48875},{\"end\":48887,\"start\":48886},{\"end\":48898,\"start\":48897},{\"end\":48900,\"start\":48899},{\"end\":49162,\"start\":49161},{\"end\":49172,\"start\":49171},{\"end\":49185,\"start\":49184},{\"end\":49547,\"start\":49546},{\"end\":49559,\"start\":49558},{\"end\":49806,\"start\":49805},{\"end\":49813,\"start\":49812},{\"end\":49821,\"start\":49820},{\"end\":49829,\"start\":49828},{\"end\":49837,\"start\":49836},{\"end\":49839,\"start\":49838},{\"end\":50054,\"start\":50053},{\"end\":50061,\"start\":50060},{\"end\":50069,\"start\":50068},{\"end\":50232,\"start\":50231},{\"end\":50234,\"start\":50233},{\"end\":50255,\"start\":50254},{\"end\":50686,\"start\":50685},{\"end\":50699,\"start\":50698},{\"end\":50711,\"start\":50710},{\"end\":50722,\"start\":50721},{\"end\":50978,\"start\":50977},{\"end\":50995,\"start\":50994},{\"end\":51002,\"start\":51001},{\"end\":51009,\"start\":51008},{\"end\":51021,\"start\":51020},{\"end\":51037,\"start\":51036},{\"end\":51418,\"start\":51414},{\"end\":51425,\"start\":51424},{\"end\":51440,\"start\":51439},{\"end\":51442,\"start\":51441},{\"end\":51871,\"start\":51867},{\"end\":51880,\"start\":51876},{\"end\":51891,\"start\":51887},{\"end\":52200,\"start\":52199},{\"end\":52211,\"start\":52210},{\"end\":52218,\"start\":52217},{\"end\":52229,\"start\":52228},{\"end\":52240,\"start\":52239},{\"end\":52249,\"start\":52248},{\"end\":52258,\"start\":52257},{\"end\":52516,\"start\":52515},{\"end\":52524,\"start\":52523},{\"end\":52526,\"start\":52525},{\"end\":52535,\"start\":52534},{\"end\":52713,\"start\":52712},{\"end\":52718,\"start\":52714},{\"end\":52727,\"start\":52726},{\"end\":52733,\"start\":52732},{\"end\":52741,\"start\":52740},{\"end\":52751,\"start\":52750},{\"end\":53168,\"start\":53164},{\"end\":53178,\"start\":53174},{\"end\":53190,\"start\":53186},{\"end\":53202,\"start\":53198},{\"end\":53213,\"start\":53209},{\"end\":53511,\"start\":53510},{\"end\":53523,\"start\":53522},{\"end\":53534,\"start\":53533},{\"end\":53544,\"start\":53543},{\"end\":53773,\"start\":53772},{\"end\":53782,\"start\":53781},{\"end\":53794,\"start\":53793},{\"end\":53804,\"start\":53803},{\"end\":53815,\"start\":53814},{\"end\":53829,\"start\":53828},{\"end\":53843,\"start\":53842},{\"end\":53855,\"start\":53854},{\"end\":54222,\"start\":54221},{\"end\":54232,\"start\":54231},{\"end\":54242,\"start\":54241},{\"end\":54755,\"start\":54754},{\"end\":54763,\"start\":54762},{\"end\":54774,\"start\":54773},{\"end\":55042,\"start\":55041},{\"end\":55050,\"start\":55049},{\"end\":55057,\"start\":55056},{\"end\":55064,\"start\":55063},{\"end\":55290,\"start\":55289},{\"end\":55298,\"start\":55297},{\"end\":55700,\"start\":55699},{\"end\":55708,\"start\":55707},{\"end\":55720,\"start\":55719},{\"end\":55729,\"start\":55728},{\"end\":56134,\"start\":56133},{\"end\":56145,\"start\":56144},{\"end\":56151,\"start\":56150},{\"end\":56461,\"start\":56460},{\"end\":56472,\"start\":56471},{\"end\":56486,\"start\":56485},{\"end\":56500,\"start\":56499},{\"end\":56516,\"start\":56515},{\"end\":56526,\"start\":56525},{\"end\":56538,\"start\":56537},{\"end\":56540,\"start\":56539},{\"end\":56959,\"start\":56958},{\"end\":56965,\"start\":56964},{\"end\":56972,\"start\":56971},{\"end\":57439,\"start\":57438},{\"end\":57453,\"start\":57452},{\"end\":57463,\"start\":57462},{\"end\":57475,\"start\":57474},{\"end\":57704,\"start\":57703},{\"end\":57715,\"start\":57714},{\"end\":57725,\"start\":57724},{\"end\":57736,\"start\":57735},{\"end\":57746,\"start\":57745},{\"end\":57976,\"start\":57975},{\"end\":57987,\"start\":57986},{\"end\":58002,\"start\":58001},{\"end\":58284,\"start\":58280},{\"end\":58296,\"start\":58292},{\"end\":58524,\"start\":58520},{\"end\":58530,\"start\":58529},{\"end\":58541,\"start\":58537},{\"end\":58752,\"start\":58751},{\"end\":58760,\"start\":58759},{\"end\":58767,\"start\":58766},{\"end\":58775,\"start\":58774},{\"end\":59005,\"start\":59004},{\"end\":59013,\"start\":59012},{\"end\":59021,\"start\":59020},{\"end\":59029,\"start\":59028},{\"end\":59036,\"start\":59035},{\"end\":59251,\"start\":59250},{\"end\":59259,\"start\":59258},{\"end\":59267,\"start\":59266},{\"end\":59275,\"start\":59274},{\"end\":59282,\"start\":59281},{\"end\":59284,\"start\":59283},{\"end\":59696,\"start\":59695},{\"end\":59704,\"start\":59703},{\"end\":59711,\"start\":59710},{\"end\":59719,\"start\":59718},{\"end\":59721,\"start\":59720},{\"end\":59981,\"start\":59980},{\"end\":59990,\"start\":59989},{\"end\":59992,\"start\":59991},{\"end\":60000,\"start\":59999},{\"end\":60010,\"start\":60009},{\"end\":60021,\"start\":60020},{\"end\":60032,\"start\":60031},{\"end\":60346,\"start\":60345},{\"end\":60356,\"start\":60355},{\"end\":60364,\"start\":60363},{\"end\":60374,\"start\":60373},{\"end\":60386,\"start\":60385},{\"end\":60392,\"start\":60391},{\"end\":60394,\"start\":60393},{\"end\":60782,\"start\":60781},{\"end\":60791,\"start\":60790},{\"end\":60801,\"start\":60800},{\"end\":60811,\"start\":60810},{\"end\":61282,\"start\":61281},{\"end\":61284,\"start\":61283},{\"end\":61291,\"start\":61290},{\"end\":61293,\"start\":61292},{\"end\":61302,\"start\":61301},{\"end\":61304,\"start\":61303},{\"end\":61312,\"start\":61311},{\"end\":61536,\"start\":61535},{\"end\":61538,\"start\":61537},{\"end\":61545,\"start\":61544},{\"end\":61710,\"start\":61709},{\"end\":61720,\"start\":61719},{\"end\":61731,\"start\":61730},{\"end\":61744,\"start\":61743},{\"end\":61943,\"start\":61942},{\"end\":61945,\"start\":61944},{\"end\":61954,\"start\":61953},{\"end\":61965,\"start\":61964},{\"end\":61971,\"start\":61970},{\"end\":62230,\"start\":62229},{\"end\":62240,\"start\":62239},{\"end\":62249,\"start\":62248},{\"end\":62258,\"start\":62257},{\"end\":62438,\"start\":62437},{\"end\":62449,\"start\":62448},{\"end\":62451,\"start\":62450},{\"end\":62460,\"start\":62459},{\"end\":62462,\"start\":62461},{\"end\":62472,\"start\":62471},{\"end\":62879,\"start\":62878},{\"end\":62886,\"start\":62885},{\"end\":62894,\"start\":62893},{\"end\":63064,\"start\":63063},{\"end\":63076,\"start\":63075},{\"end\":63078,\"start\":63077},{\"end\":63385,\"start\":63384},{\"end\":63394,\"start\":63393},{\"end\":63405,\"start\":63404},{\"end\":63416,\"start\":63415},{\"end\":63811,\"start\":63810},{\"end\":63822,\"start\":63821},{\"end\":64099,\"start\":64098},{\"end\":64107,\"start\":64106},{\"end\":64109,\"start\":64108},{\"end\":64117,\"start\":64116},{\"end\":64125,\"start\":64124},{\"end\":64339,\"start\":64338},{\"end\":64351,\"start\":64350},{\"end\":64360,\"start\":64359},{\"end\":64370,\"start\":64369},{\"end\":64679,\"start\":64678},{\"end\":64686,\"start\":64685},{\"end\":64694,\"start\":64693},{\"end\":64702,\"start\":64701}]", "bib_author_last_name": "[{\"end\":48660,\"start\":48655},{\"end\":48673,\"start\":48664},{\"end\":48851,\"start\":48842},{\"end\":48862,\"start\":48855},{\"end\":48873,\"start\":48866},{\"end\":48884,\"start\":48877},{\"end\":48895,\"start\":48888},{\"end\":48908,\"start\":48901},{\"end\":49169,\"start\":49163},{\"end\":49182,\"start\":49173},{\"end\":49193,\"start\":49186},{\"end\":49556,\"start\":49548},{\"end\":49570,\"start\":49560},{\"end\":49810,\"start\":49807},{\"end\":49818,\"start\":49814},{\"end\":49826,\"start\":49822},{\"end\":49834,\"start\":49830},{\"end\":49842,\"start\":49840},{\"end\":50058,\"start\":50055},{\"end\":50066,\"start\":50062},{\"end\":50074,\"start\":50070},{\"end\":50252,\"start\":50235},{\"end\":50272,\"start\":50256},{\"end\":50696,\"start\":50687},{\"end\":50708,\"start\":50700},{\"end\":50719,\"start\":50712},{\"end\":50732,\"start\":50723},{\"end\":50992,\"start\":50979},{\"end\":50999,\"start\":50996},{\"end\":51006,\"start\":51003},{\"end\":51018,\"start\":51010},{\"end\":51034,\"start\":51022},{\"end\":51040,\"start\":51038},{\"end\":51422,\"start\":51419},{\"end\":51437,\"start\":51426},{\"end\":51447,\"start\":51443},{\"end\":51874,\"start\":51872},{\"end\":51885,\"start\":51881},{\"end\":51898,\"start\":51892},{\"end\":52208,\"start\":52201},{\"end\":52215,\"start\":52212},{\"end\":52226,\"start\":52219},{\"end\":52237,\"start\":52230},{\"end\":52246,\"start\":52241},{\"end\":52255,\"start\":52250},{\"end\":52266,\"start\":52259},{\"end\":52521,\"start\":52517},{\"end\":52532,\"start\":52527},{\"end\":52538,\"start\":52536},{\"end\":52724,\"start\":52719},{\"end\":52730,\"start\":52728},{\"end\":52738,\"start\":52734},{\"end\":52748,\"start\":52742},{\"end\":52756,\"start\":52752},{\"end\":53172,\"start\":53169},{\"end\":53184,\"start\":53179},{\"end\":53196,\"start\":53191},{\"end\":53207,\"start\":53203},{\"end\":53217,\"start\":53214},{\"end\":53520,\"start\":53512},{\"end\":53531,\"start\":53524},{\"end\":53541,\"start\":53535},{\"end\":53555,\"start\":53545},{\"end\":53779,\"start\":53774},{\"end\":53791,\"start\":53783},{\"end\":53801,\"start\":53795},{\"end\":53812,\"start\":53805},{\"end\":53826,\"start\":53816},{\"end\":53840,\"start\":53830},{\"end\":53852,\"start\":53844},{\"end\":53865,\"start\":53856},{\"end\":54229,\"start\":54223},{\"end\":54239,\"start\":54233},{\"end\":54249,\"start\":54243},{\"end\":54760,\"start\":54756},{\"end\":54771,\"start\":54764},{\"end\":54778,\"start\":54775},{\"end\":55047,\"start\":55043},{\"end\":55054,\"start\":55051},{\"end\":55061,\"start\":55058},{\"end\":55072,\"start\":55065},{\"end\":55295,\"start\":55291},{\"end\":55307,\"start\":55299},{\"end\":55705,\"start\":55701},{\"end\":55717,\"start\":55709},{\"end\":55726,\"start\":55721},{\"end\":55739,\"start\":55730},{\"end\":56142,\"start\":56135},{\"end\":56148,\"start\":56146},{\"end\":56161,\"start\":56152},{\"end\":56469,\"start\":56462},{\"end\":56483,\"start\":56473},{\"end\":56497,\"start\":56487},{\"end\":56513,\"start\":56501},{\"end\":56523,\"start\":56517},{\"end\":56535,\"start\":56527},{\"end\":56554,\"start\":56541},{\"end\":56962,\"start\":56960},{\"end\":56969,\"start\":56966},{\"end\":56976,\"start\":56973},{\"end\":57450,\"start\":57440},{\"end\":57460,\"start\":57454},{\"end\":57472,\"start\":57464},{\"end\":57487,\"start\":57476},{\"end\":57712,\"start\":57705},{\"end\":57722,\"start\":57716},{\"end\":57733,\"start\":57726},{\"end\":57743,\"start\":57737},{\"end\":57754,\"start\":57747},{\"end\":57984,\"start\":57977},{\"end\":57999,\"start\":57988},{\"end\":58008,\"start\":58003},{\"end\":58290,\"start\":58285},{\"end\":58299,\"start\":58297},{\"end\":58527,\"start\":58525},{\"end\":58535,\"start\":58531},{\"end\":58546,\"start\":58542},{\"end\":58757,\"start\":58753},{\"end\":58764,\"start\":58761},{\"end\":58772,\"start\":58768},{\"end\":58782,\"start\":58776},{\"end\":59010,\"start\":59006},{\"end\":59018,\"start\":59014},{\"end\":59026,\"start\":59022},{\"end\":59033,\"start\":59030},{\"end\":59039,\"start\":59037},{\"end\":59256,\"start\":59252},{\"end\":59264,\"start\":59260},{\"end\":59272,\"start\":59268},{\"end\":59279,\"start\":59276},{\"end\":59287,\"start\":59285},{\"end\":59701,\"start\":59697},{\"end\":59708,\"start\":59705},{\"end\":59716,\"start\":59712},{\"end\":59728,\"start\":59722},{\"end\":59987,\"start\":59982},{\"end\":59997,\"start\":59993},{\"end\":60007,\"start\":60001},{\"end\":60018,\"start\":60011},{\"end\":60029,\"start\":60022},{\"end\":60041,\"start\":60033},{\"end\":60353,\"start\":60347},{\"end\":60361,\"start\":60357},{\"end\":60371,\"start\":60365},{\"end\":60383,\"start\":60375},{\"end\":60389,\"start\":60387},{\"end\":60397,\"start\":60395},{\"end\":60788,\"start\":60783},{\"end\":60798,\"start\":60792},{\"end\":60808,\"start\":60802},{\"end\":60817,\"start\":60812},{\"end\":61288,\"start\":61285},{\"end\":61299,\"start\":61294},{\"end\":61309,\"start\":61305},{\"end\":61317,\"start\":61313},{\"end\":61542,\"start\":61539},{\"end\":61550,\"start\":61546},{\"end\":61717,\"start\":61711},{\"end\":61728,\"start\":61721},{\"end\":61741,\"start\":61732},{\"end\":61749,\"start\":61745},{\"end\":61951,\"start\":61946},{\"end\":61962,\"start\":61955},{\"end\":61968,\"start\":61966},{\"end\":61981,\"start\":61972},{\"end\":62237,\"start\":62231},{\"end\":62246,\"start\":62241},{\"end\":62255,\"start\":62250},{\"end\":62266,\"start\":62259},{\"end\":62446,\"start\":62439},{\"end\":62457,\"start\":62452},{\"end\":62469,\"start\":62463},{\"end\":62482,\"start\":62473},{\"end\":62883,\"start\":62880},{\"end\":62891,\"start\":62887},{\"end\":62901,\"start\":62895},{\"end\":63073,\"start\":63065},{\"end\":63084,\"start\":63079},{\"end\":63391,\"start\":63386},{\"end\":63402,\"start\":63395},{\"end\":63413,\"start\":63406},{\"end\":63423,\"start\":63417},{\"end\":63819,\"start\":63812},{\"end\":63827,\"start\":63823},{\"end\":64104,\"start\":64100},{\"end\":64114,\"start\":64110},{\"end\":64122,\"start\":64118},{\"end\":64128,\"start\":64126},{\"end\":64348,\"start\":64340},{\"end\":64357,\"start\":64352},{\"end\":64367,\"start\":64361},{\"end\":64377,\"start\":64371},{\"end\":64683,\"start\":64680},{\"end\":64691,\"start\":64687},{\"end\":64699,\"start\":64695},{\"end\":64706,\"start\":64703}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8406263},\"end\":48793,\"start\":48594},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8577357},\"end\":49103,\"start\":48795},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":393948},\"end\":49447,\"start\":49105},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14739721},\"end\":49747,\"start\":49449},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207239085},\"end\":49984,\"start\":49749},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8872397},\"end\":50195,\"start\":49986},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6261591},\"end\":50609,\"start\":50197},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7204540},\"end\":50894,\"start\":50611},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52308023},\"end\":51338,\"start\":50896},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6600989},\"end\":51815,\"start\":51340},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5564100},\"end\":52118,\"start\":51817},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6161478},\"end\":52471,\"start\":52120},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":8548594},\"end\":52662,\"start\":52473},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206592854},\"end\":53108,\"start\":52664},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3116168},\"end\":53444,\"start\":53110},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9440223},\"end\":53723,\"start\":53446},{\"attributes\":{\"id\":\"b16\"},\"end\":54133,\"start\":53725},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":18235792},\"end\":54628,\"start\":54135},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":16169580},\"end\":54982,\"start\":54630},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6742009},\"end\":55214,\"start\":54984},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":52800221},\"end\":55592,\"start\":55216},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2605321},\"end\":56063,\"start\":55594},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10337178},\"end\":56402,\"start\":56065},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":9067102},\"end\":56862,\"start\":56404},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2361503},\"end\":57352,\"start\":56864},{\"attributes\":{\"id\":\"b25\"},\"end\":57639,\"start\":57354},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16496273},\"end\":57906,\"start\":57641},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206769720},\"end\":58278,\"start\":57908},{\"attributes\":{\"doi\":\"arXiv:1602.02255\",\"id\":\"b28\"},\"end\":58449,\"start\":58280},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8383593},\"end\":58687,\"start\":58451},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":556999},\"end\":58942,\"start\":58689},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":407822},\"end\":59186,\"start\":58944},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13798326},\"end\":59629,\"start\":59188},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":745350},\"end\":59869,\"start\":59631},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3329621},\"end\":60274,\"start\":59871},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16852518},\"end\":60684,\"start\":60276},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":206592191},\"end\":61211,\"start\":60686},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":788838},\"end\":61502,\"start\":61213},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":740063},\"end\":61657,\"start\":61504},{\"attributes\":{\"id\":\"b39\"},\"end\":61885,\"start\":61659},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":918513},\"end\":62179,\"start\":61887},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7534823},\"end\":62393,\"start\":62181},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":7955708},\"end\":62828,\"start\":62395},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":16439870},\"end\":63030,\"start\":62830},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":2777306},\"end\":63329,\"start\":63032},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":2655115},\"end\":63754,\"start\":63331},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":207224096},\"end\":64055,\"start\":63756},{\"attributes\":{\"doi\":\"arXiv:1408.2927\",\"id\":\"b47\"},\"end\":64280,\"start\":64057},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":362467},\"end\":64619,\"start\":64282},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":11321040},\"end\":64925,\"start\":64621}]", "bib_title": "[{\"end\":48651,\"start\":48594},{\"end\":48838,\"start\":48795},{\"end\":49159,\"start\":49105},{\"end\":49544,\"start\":49449},{\"end\":49803,\"start\":49749},{\"end\":50051,\"start\":49986},{\"end\":50229,\"start\":50197},{\"end\":50683,\"start\":50611},{\"end\":50975,\"start\":50896},{\"end\":51412,\"start\":51340},{\"end\":51865,\"start\":51817},{\"end\":52197,\"start\":52120},{\"end\":52513,\"start\":52473},{\"end\":52710,\"start\":52664},{\"end\":53162,\"start\":53110},{\"end\":53508,\"start\":53446},{\"end\":53770,\"start\":53725},{\"end\":54219,\"start\":54135},{\"end\":54752,\"start\":54630},{\"end\":55039,\"start\":54984},{\"end\":55287,\"start\":55216},{\"end\":55697,\"start\":55594},{\"end\":56131,\"start\":56065},{\"end\":56458,\"start\":56404},{\"end\":56956,\"start\":56864},{\"end\":57701,\"start\":57641},{\"end\":57973,\"start\":57908},{\"end\":58518,\"start\":58451},{\"end\":58749,\"start\":58689},{\"end\":59002,\"start\":58944},{\"end\":59248,\"start\":59188},{\"end\":59693,\"start\":59631},{\"end\":59978,\"start\":59871},{\"end\":60343,\"start\":60276},{\"end\":60779,\"start\":60686},{\"end\":61279,\"start\":61213},{\"end\":61533,\"start\":61504},{\"end\":61940,\"start\":61887},{\"end\":62227,\"start\":62181},{\"end\":62435,\"start\":62395},{\"end\":62876,\"start\":62830},{\"end\":63061,\"start\":63032},{\"end\":63382,\"start\":63331},{\"end\":63808,\"start\":63756},{\"end\":64336,\"start\":64282},{\"end\":64676,\"start\":64621}]", "bib_author": "[{\"end\":48662,\"start\":48653},{\"end\":48675,\"start\":48662},{\"end\":48853,\"start\":48840},{\"end\":48864,\"start\":48853},{\"end\":48875,\"start\":48864},{\"end\":48886,\"start\":48875},{\"end\":48897,\"start\":48886},{\"end\":48910,\"start\":48897},{\"end\":49171,\"start\":49161},{\"end\":49184,\"start\":49171},{\"end\":49195,\"start\":49184},{\"end\":49558,\"start\":49546},{\"end\":49572,\"start\":49558},{\"end\":49812,\"start\":49805},{\"end\":49820,\"start\":49812},{\"end\":49828,\"start\":49820},{\"end\":49836,\"start\":49828},{\"end\":49844,\"start\":49836},{\"end\":50060,\"start\":50053},{\"end\":50068,\"start\":50060},{\"end\":50076,\"start\":50068},{\"end\":50254,\"start\":50231},{\"end\":50274,\"start\":50254},{\"end\":50698,\"start\":50685},{\"end\":50710,\"start\":50698},{\"end\":50721,\"start\":50710},{\"end\":50734,\"start\":50721},{\"end\":50994,\"start\":50977},{\"end\":51001,\"start\":50994},{\"end\":51008,\"start\":51001},{\"end\":51020,\"start\":51008},{\"end\":51036,\"start\":51020},{\"end\":51042,\"start\":51036},{\"end\":51424,\"start\":51414},{\"end\":51439,\"start\":51424},{\"end\":51449,\"start\":51439},{\"end\":51876,\"start\":51867},{\"end\":51887,\"start\":51876},{\"end\":51900,\"start\":51887},{\"end\":52210,\"start\":52199},{\"end\":52217,\"start\":52210},{\"end\":52228,\"start\":52217},{\"end\":52239,\"start\":52228},{\"end\":52248,\"start\":52239},{\"end\":52257,\"start\":52248},{\"end\":52268,\"start\":52257},{\"end\":52523,\"start\":52515},{\"end\":52534,\"start\":52523},{\"end\":52540,\"start\":52534},{\"end\":52726,\"start\":52712},{\"end\":52732,\"start\":52726},{\"end\":52740,\"start\":52732},{\"end\":52750,\"start\":52740},{\"end\":52758,\"start\":52750},{\"end\":53174,\"start\":53164},{\"end\":53186,\"start\":53174},{\"end\":53198,\"start\":53186},{\"end\":53209,\"start\":53198},{\"end\":53219,\"start\":53209},{\"end\":53522,\"start\":53510},{\"end\":53533,\"start\":53522},{\"end\":53543,\"start\":53533},{\"end\":53557,\"start\":53543},{\"end\":53781,\"start\":53772},{\"end\":53793,\"start\":53781},{\"end\":53803,\"start\":53793},{\"end\":53814,\"start\":53803},{\"end\":53828,\"start\":53814},{\"end\":53842,\"start\":53828},{\"end\":53854,\"start\":53842},{\"end\":53867,\"start\":53854},{\"end\":54231,\"start\":54221},{\"end\":54241,\"start\":54231},{\"end\":54251,\"start\":54241},{\"end\":54762,\"start\":54754},{\"end\":54773,\"start\":54762},{\"end\":54780,\"start\":54773},{\"end\":55049,\"start\":55041},{\"end\":55056,\"start\":55049},{\"end\":55063,\"start\":55056},{\"end\":55074,\"start\":55063},{\"end\":55297,\"start\":55289},{\"end\":55309,\"start\":55297},{\"end\":55707,\"start\":55699},{\"end\":55719,\"start\":55707},{\"end\":55728,\"start\":55719},{\"end\":55741,\"start\":55728},{\"end\":56144,\"start\":56133},{\"end\":56150,\"start\":56144},{\"end\":56163,\"start\":56150},{\"end\":56471,\"start\":56460},{\"end\":56485,\"start\":56471},{\"end\":56499,\"start\":56485},{\"end\":56515,\"start\":56499},{\"end\":56525,\"start\":56515},{\"end\":56537,\"start\":56525},{\"end\":56556,\"start\":56537},{\"end\":56964,\"start\":56958},{\"end\":56971,\"start\":56964},{\"end\":56978,\"start\":56971},{\"end\":57452,\"start\":57438},{\"end\":57462,\"start\":57452},{\"end\":57474,\"start\":57462},{\"end\":57489,\"start\":57474},{\"end\":57714,\"start\":57703},{\"end\":57724,\"start\":57714},{\"end\":57735,\"start\":57724},{\"end\":57745,\"start\":57735},{\"end\":57756,\"start\":57745},{\"end\":57986,\"start\":57975},{\"end\":58001,\"start\":57986},{\"end\":58010,\"start\":58001},{\"end\":58292,\"start\":58280},{\"end\":58301,\"start\":58292},{\"end\":58529,\"start\":58520},{\"end\":58537,\"start\":58529},{\"end\":58548,\"start\":58537},{\"end\":58759,\"start\":58751},{\"end\":58766,\"start\":58759},{\"end\":58774,\"start\":58766},{\"end\":58784,\"start\":58774},{\"end\":59012,\"start\":59004},{\"end\":59020,\"start\":59012},{\"end\":59028,\"start\":59020},{\"end\":59035,\"start\":59028},{\"end\":59041,\"start\":59035},{\"end\":59258,\"start\":59250},{\"end\":59266,\"start\":59258},{\"end\":59274,\"start\":59266},{\"end\":59281,\"start\":59274},{\"end\":59289,\"start\":59281},{\"end\":59703,\"start\":59695},{\"end\":59710,\"start\":59703},{\"end\":59718,\"start\":59710},{\"end\":59730,\"start\":59718},{\"end\":59989,\"start\":59980},{\"end\":59999,\"start\":59989},{\"end\":60009,\"start\":59999},{\"end\":60020,\"start\":60009},{\"end\":60031,\"start\":60020},{\"end\":60043,\"start\":60031},{\"end\":60355,\"start\":60345},{\"end\":60363,\"start\":60355},{\"end\":60373,\"start\":60363},{\"end\":60385,\"start\":60373},{\"end\":60391,\"start\":60385},{\"end\":60399,\"start\":60391},{\"end\":60790,\"start\":60781},{\"end\":60800,\"start\":60790},{\"end\":60810,\"start\":60800},{\"end\":60819,\"start\":60810},{\"end\":61290,\"start\":61281},{\"end\":61301,\"start\":61290},{\"end\":61311,\"start\":61301},{\"end\":61319,\"start\":61311},{\"end\":61544,\"start\":61535},{\"end\":61552,\"start\":61544},{\"end\":61719,\"start\":61709},{\"end\":61730,\"start\":61719},{\"end\":61743,\"start\":61730},{\"end\":61751,\"start\":61743},{\"end\":61953,\"start\":61942},{\"end\":61964,\"start\":61953},{\"end\":61970,\"start\":61964},{\"end\":61983,\"start\":61970},{\"end\":62239,\"start\":62229},{\"end\":62248,\"start\":62239},{\"end\":62257,\"start\":62248},{\"end\":62268,\"start\":62257},{\"end\":62448,\"start\":62437},{\"end\":62459,\"start\":62448},{\"end\":62471,\"start\":62459},{\"end\":62484,\"start\":62471},{\"end\":62885,\"start\":62878},{\"end\":62893,\"start\":62885},{\"end\":62903,\"start\":62893},{\"end\":63075,\"start\":63063},{\"end\":63086,\"start\":63075},{\"end\":63393,\"start\":63384},{\"end\":63404,\"start\":63393},{\"end\":63415,\"start\":63404},{\"end\":63425,\"start\":63415},{\"end\":63821,\"start\":63810},{\"end\":63829,\"start\":63821},{\"end\":64106,\"start\":64098},{\"end\":64116,\"start\":64106},{\"end\":64124,\"start\":64116},{\"end\":64130,\"start\":64124},{\"end\":64350,\"start\":64338},{\"end\":64359,\"start\":64350},{\"end\":64369,\"start\":64359},{\"end\":64379,\"start\":64369},{\"end\":64685,\"start\":64678},{\"end\":64693,\"start\":64685},{\"end\":64701,\"start\":64693},{\"end\":64708,\"start\":64701}]", "bib_venue": "[{\"end\":48684,\"start\":48675},{\"end\":48926,\"start\":48910},{\"end\":49257,\"start\":49195},{\"end\":49576,\"start\":49572},{\"end\":49854,\"start\":49844},{\"end\":50080,\"start\":50076},{\"end\":50351,\"start\":50274},{\"end\":50738,\"start\":50734},{\"end\":51098,\"start\":51042},{\"end\":51526,\"start\":51449},{\"end\":51938,\"start\":51900},{\"end\":52272,\"start\":52268},{\"end\":52549,\"start\":52540},{\"end\":52835,\"start\":52758},{\"end\":53255,\"start\":53219},{\"end\":53561,\"start\":53557},{\"end\":53903,\"start\":53867},{\"end\":54329,\"start\":54251},{\"end\":54788,\"start\":54780},{\"end\":55083,\"start\":55074},{\"end\":55380,\"start\":55309},{\"end\":55803,\"start\":55741},{\"end\":56211,\"start\":56163},{\"end\":56605,\"start\":56556},{\"end\":57055,\"start\":56978},{\"end\":57436,\"start\":57354},{\"end\":57760,\"start\":57756},{\"end\":58068,\"start\":58010},{\"end\":58341,\"start\":58317},{\"end\":58553,\"start\":58548},{\"end\":58788,\"start\":58784},{\"end\":59045,\"start\":59041},{\"end\":59356,\"start\":59289},{\"end\":59734,\"start\":59730},{\"end\":60047,\"start\":60043},{\"end\":60463,\"start\":60399},{\"end\":60896,\"start\":60819},{\"end\":61333,\"start\":61319},{\"end\":61561,\"start\":61552},{\"end\":61707,\"start\":61659},{\"end\":62014,\"start\":61983},{\"end\":62272,\"start\":62268},{\"end\":62561,\"start\":62484},{\"end\":62916,\"start\":62903},{\"end\":63157,\"start\":63086},{\"end\":63492,\"start\":63425},{\"end\":63875,\"start\":63829},{\"end\":64096,\"start\":64057},{\"end\":64428,\"start\":64379},{\"end\":64760,\"start\":64708},{\"end\":50415,\"start\":50353},{\"end\":51590,\"start\":51528},{\"end\":52899,\"start\":52837},{\"end\":54394,\"start\":54331},{\"end\":57119,\"start\":57057},{\"end\":59410,\"start\":59358},{\"end\":60960,\"start\":60898},{\"end\":62625,\"start\":62563},{\"end\":63546,\"start\":63494},{\"end\":63919,\"start\":63877}]"}}}, "year": 2023, "month": 12, "day": 17}